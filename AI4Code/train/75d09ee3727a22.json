{"cell_type":{"47cd8d0f":"code","4d88a84b":"code","7a995475":"code","2be030f5":"code","563a377f":"code","ad4104a7":"code","52f9f125":"code","534cb5d0":"code","f870fce1":"code","55b650ec":"code","060a2294":"code","6d49b1e9":"code","a5f0293a":"code","962d4ae3":"code","06d020ff":"code","426332aa":"code","b3e9fe2e":"code","d709af82":"code","d3e32341":"code","c0509810":"code","62f0943e":"code","a365bbb8":"code","2615cc9b":"code","da158163":"code","ac202702":"code","f1a7220a":"code","0d38d963":"code","726d90ac":"code","323739f2":"code","a9bd6cad":"code","947f53f4":"code","8709c492":"code","47d8ee6b":"code","974e2579":"code","d19a9793":"code","ec83f8e2":"code","037ca546":"code","ce78a8c0":"code","0babae55":"code","16f1d054":"markdown","54b44264":"markdown","4a6a9d21":"markdown","d52505af":"markdown","87baab8d":"markdown","6cf2d7ad":"markdown","8b95b273":"markdown","75af2378":"markdown","203dff8f":"markdown","34ed1652":"markdown","7aab270d":"markdown"},"source":{"47cd8d0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4d88a84b":"import datetime\nimport matplotlib.pyplot as plt\nimport re\nfrom string import punctuation\nimport time\nimport tqdm","7a995475":"from kaggle.competitions import nflrush\n\n# You can only call make_env() once, so don't lose it!\nenv = nflrush.make_env()","2be030f5":"start_time = time.time()\noriginal_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)\ntrain_df = original_df\ntrain_df.head()","563a377f":"# Some Rough Plots for Empirical Distributions, will skip when submit\n\n# Each play only has one yards\nplay_yards_list = train_df.groupby([\"GameId\", \"PlayId\"])[\"Yards\"].unique().values.astype(int)\n\nfig, (hist1, hist2) = plt.subplots(1, 2)\nfig.set_figwidth(15)\nrough_density_hist = hist1.hist(play_yards_list, bins=100, range=(-25, 75), density=True)\ntitle1 = hist1.set_title(\"pmf\")\nrough_cumulative_hist = hist2.hist(play_yards_list, bins=100, range=(-25, 75), density=True, cumulative=True)\ntitle2 = hist2.set_title(\"cdf\")","ad4104a7":"# Precomputed New Features\n\n# From https:\/\/www.kaggle.com\/prashantkikani\/nfl-starter-lgb-feature-engg\ntrain_df['DefendersInTheBox_vs_Distance'] = train_df['DefendersInTheBox'] \/ train_df['Distance']","52f9f125":"# Categorical features\n\ncat_features = []\nfor col in train_df.columns:\n    if train_df[col].dtype =='object':\n        cat_features.append((col, len(train_df[col].unique())))\n        \ncat_features","534cb5d0":"### StadiumType ###\n\n\n# Fix Typos\ndef clean_StadiumType(txt):\n    if pd.isna(txt):\n        return np.nan\n    txt = txt.lower()\n    txt = ''.join([c for c in txt if c not in punctuation])\n    txt = re.sub(' +', ' ', txt)\n    txt = txt.strip()\n    txt = txt.replace('outside', 'outdoor')\n    txt = txt.replace('outdor', 'outdoor')\n    txt = txt.replace('outddors', 'outdoor')\n    txt = txt.replace('outdoors', 'outdoor')\n    txt = txt.replace('oudoor', 'outdoor')\n    txt = txt.replace('indoors', 'indoor')\n    txt = txt.replace('ourdoor', 'outdoor')\n    txt = txt.replace('retractable', 'rtr.')\n    return txt\n\n\n# We are just going to focus on the words: outdoor, indoor, closed and open\ndef transform_StadiumType(txt):\n    if pd.isna(txt):\n        return np.nan\n    if 'outdoor' in txt or 'open' in txt:\n        return 1\n    if 'indoor' in txt or 'closed' in txt:\n        return 0\n    return np.nan\n\n\n# Run functions\ntrain_df[\"StadiumType\"] = train_df[\"StadiumType\"].apply(clean_StadiumType)\ntrain_df['StadiumType'] = train_df['StadiumType'].apply(transform_StadiumType)","f870fce1":"### Truf ###\n\n#from https:\/\/www.kaggle.com\/c\/nfl-big-data-bowl-2020\/discussion\/112681#latest-649087\n\n\nTurf = {'Field Turf':'Artificial', 'A-Turf Titan':'Artificial', 'Grass':'Natural', 'UBU Sports Speed S5-M':'Artificial', \n        'Artificial':'Artificial', 'DD GrassMaster':'Artificial', 'Natural Grass':'Natural', \n        'UBU Speed Series-S5-M':'Artificial', 'FieldTurf':'Artificial', 'FieldTurf 360':'Artificial', 'Natural grass':'Natural', 'grass':'Natural', \n        'Natural':'Natural', 'Artifical':'Artificial', 'FieldTurf360':'Artificial', 'Naturall Grass':'Natural', 'Field turf':'Artificial', \n        'SISGrass':'Artificial', 'Twenty-Four\/Seven Turf':'Artificial', 'natural grass':'Natural'} \n\ntrain_df['Turf'] = train_df['Turf'].map(Turf)\n\n# Change to binary feature: is Natural\ntrain_df['Turf'] = train_df['Turf'] == 'Natural'","55b650ec":"### PossessionTeam ###\n\n# We have some problem with the enconding of the teams such as BLT and BAL or ARZ and ARI.\n# Uncomment this code to show the issue\n# train_df[(train_df['PossessionTeam']!=train_df['HomeTeamAbbr']) & (train_df['PossessionTeam']!=train_df['VisitorTeamAbbr'])][['PossessionTeam', 'HomeTeamAbbr', 'VisitorTeamAbbr']]\n\n# Fix them manually\n\nmap_abbr = {'ARI': 'ARZ', 'BAL': 'BLT', 'CLE': 'CLV', 'HOU': 'HST'}\nfor abb in train_df['PossessionTeam'].unique():\n    map_abbr[abb] = abb\n\ntrain_df['PossessionTeam'] = train_df['PossessionTeam'].map(map_abbr)\ntrain_df['HomeTeamAbbr'] = train_df['HomeTeamAbbr'].map(map_abbr)\ntrain_df['VisitorTeamAbbr'] = train_df['VisitorTeamAbbr'].map(map_abbr)\n\n\n# New Features: HomePossesion, Field_eq_Possession, HomeField\ntrain_df['HomePossesion'] = train_df['PossessionTeam'] == train_df['HomeTeamAbbr']\ntrain_df['Field_eq_Possession'] = train_df['FieldPosition'] == train_df['PossessionTeam']\ntrain_df['HomeField'] = train_df['FieldPosition'] == train_df['HomeTeamAbbr']","060a2294":"### OffensiveFormation ###\n\noff_form = train_df['OffenseFormation'].unique()\ntrain_df = pd.concat([train_df.drop(['OffenseFormation'], axis=1), pd.get_dummies(train_df['OffenseFormation'], prefix='Formation')], axis=1)\n\n# Here the `dummy_col` is actually including df.columns and dummy columns, will be used in further function\ndummy_col = train_df.columns","6d49b1e9":"### GameClock ###\n\n# Change `m:s:ms` string type to seconds\ndef strtoseconds(txt):\n    txt = txt.split(':')\n    ans = int(txt[0])*60 + int(txt[1]) + int(txt[2])\/60\n    return ans\n\n\ntrain_df['GameClock'] = train_df['GameClock'].apply(strtoseconds)","a5f0293a":"### PlayerHeight ### \n\n# Change `feet-inch` string type to inches, 1ft = 12 in\ndef strtoinches(txt):\n    txt = txt.split('-')\n    ans = int(txt[0]) * 12 + int(txt[1])\n    return ans\n\n\ntrain_df['PlayerHeight'] = train_df['PlayerHeight'].apply(strtoinches)\n\n# New Feature: PlayerBMI\ntrain_df['PlayerBMI'] = 703 * (train_df['PlayerWeight']\/(train_df['PlayerHeight'])**2)","962d4ae3":"### TimeHandoff & TimeSnap ###\n\ntrain_df['TimeHandoff'] = train_df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\ntrain_df['TimeSnap'] = train_df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n\n# New Feature: TimeDelta\ntrain_df['TimeDelta'] = train_df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)","06d020ff":"### PlayerBirthDate ###\n\ntrain_df['PlayerBirthDate'] = train_df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))\n\n# New Feature: PlayerAge\nseconds_in_year = 60*60*24*365.25\ntrain_df['PlayerAge'] = train_df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds() \/ seconds_in_year, axis=1)\n\ntrain_df = train_df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate'], axis=1)","426332aa":"### WindSpeed & WindDirection ###\n\ntrain_df['WindSpeed'] = train_df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n\n# Replace the ones that has x-y by (x+y)\/2\n# And also the ones with x gusts up to y\ntrain_df['WindSpeed'] = train_df['WindSpeed'].apply(lambda x: (int(x.split('-')[0]) + int(x.split('-')[1])) \/ 2 if not pd.isna(x) and '-' in x else x)\ntrain_df['WindSpeed'] = train_df['WindSpeed'].apply(lambda x: (int(x.split()[0]) + int(x.split()[-1])) \/ 2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\ntrain_df['WindSpeed'] = train_df['WindSpeed'].apply(lambda x: float(x) if str(x).isnumeric() else -1)\n\n\n# Clean WindDirection\ndef clean_WindDirection(txt):\n    if pd.isna(txt):\n        return np.nan\n    txt = txt.lower()\n    txt = ''.join([c for c in txt if c not in punctuation])\n    txt = txt.replace('from', '')\n    txt = txt.replace(' ', '')\n    txt = txt.replace('north', 'n')\n    txt = txt.replace('south', 's')\n    txt = txt.replace('west', 'w')\n    txt = txt.replace('east', 'e')\n    return txt\n\ndef transform_WindDirection(txt):\n    if pd.isna(txt):\n        return np.nan\n    if txt=='n':\n        return 0\n    if txt=='nne' or txt=='nen':\n        return 1\/8\n    if txt=='ne':\n        return 2\/8\n    if txt=='ene' or txt=='nee':\n        return 3\/8\n    if txt=='e':\n        return 4\/8\n    if txt=='ese' or txt=='see':\n        return 5\/8\n    if txt=='se':\n        return 6\/8\n    if txt=='ses' or txt=='sse':\n        return 7\/8\n    if txt=='s':\n        return 8\/8\n    if txt=='ssw' or txt=='sws':\n        return 9\/8\n    if txt=='sw':\n        return 10\/8\n    if txt=='sww' or txt=='wsw':\n        return 11\/8\n    if txt=='w':\n        return 12\/8\n    if txt=='wnw' or txt=='nww':\n        return 13\/8\n    if txt=='nw':\n        return 14\/8\n    if txt=='nwn' or txt=='nnw':\n        return 15\/8\n    return np.nan\n\n\ntrain_df['WindDirection'] = train_df['WindDirection'].apply(clean_WindDirection)\ntrain_df['WindDirection'] = train_df['WindDirection'].apply(transform_WindDirection)","b3e9fe2e":"### PlayDirection ###\n\ntrain_df['PlayDirection'] = train_df['PlayDirection'].apply(lambda x: x.strip() == 'right')","d709af82":"### Team ###\n\ntrain_df['Team'] = train_df['Team'].apply(lambda x: x.strip()=='home')","d3e32341":"### GameWeather ###\n\n# TODO:\n# Lower case\n# N\/A Indoor, N\/A (Indoors) and Indoor => indoor Let's try to cluster those together.\n# coudy and clouidy => cloudy\n# party => partly\n# sunny and clear => clear and sunny\n# skies and mostly => \"\"\n\ntrain_df['GameWeather'] = train_df['GameWeather'].str.lower()\nindoor = \"indoor\"\ntrain_df['GameWeather'] = train_df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\ntrain_df['GameWeather'] = train_df['GameWeather'].apply(lambda x: x.replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly') if not pd.isna(x) else x)\ntrain_df['GameWeather'] = train_df['GameWeather'].apply(lambda x: x.replace('clear and sunny', 'sunny and clear') if not pd.isna(x) else x)\ntrain_df['GameWeather'] = train_df['GameWeather'].apply(lambda x: x.replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n\n# TODO:\n# climate controlled or indoor => 3, sunny or sun => 2, clear => 1, cloudy => -1, rain => -2, snow => -3, others => 0\n# partly => multiply by 0.5\n\ndef map_weather(txt):\n    ans = 1\n    if pd.isna(txt):\n        return 0\n    if 'partly' in txt:\n        ans*=0.5\n    if 'climate controlled' in txt or 'indoor' in txt:\n        return ans*3\n    if 'sunny' in txt or 'sun' in txt:\n        return ans*2\n    if 'clear' in txt:\n        return ans\n    if 'cloudy' in txt:\n        return -ans\n    if 'rain' in txt or 'rainy' in txt:\n        return -2*ans\n    if 'snow' in txt:\n        return -3*ans\n    return 0\n\ntrain_df['GameWeather'] = train_df['GameWeather'].apply(map_weather)","c0509810":"### NflId & NflIdRusher ###\n\n# New Feature: IsRusher\ntrain_df['IsRusher'] = train_df['NflId'] == train_df['NflIdRusher']\ntrain_df.drop(['NflId', 'NflIdRusher'], axis=1, inplace=True)","62f0943e":"### X & Orientation & Dir ###\n\ntrain_df['X'] = train_df.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n\n# From https:\/\/www.kaggle.com\/scirpus\/hybrid-gp-and-nn\ndef new_orientation(angle, play_direction):\n    if play_direction == 0:\n        new_angle = 360.0 - angle\n        if new_angle == 360.0:\n            new_angle = 0.0\n        return new_angle\n    else:\n        return angle\n\ntrain_df['Orientation'] = train_df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\ntrain_df['Dir'] = train_df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)","a365bbb8":"### YardsLeft ###\n\n# New Feature: YardsLeft\n# Compute how many yards are left to the end-zone\ntrain_df['YardsLeft'] = train_df.apply(lambda row: 100 - row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\ntrain_df['YardsLeft'] = train_df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100 - row['YardsLeft'], axis=1)\n\n# Yards<=YardsLeft and YardsLeft-100<=Yards, thus we are going to drop those wrong lines\ntrain_df.drop(train_df.index[(train_df['YardsLeft'] < train_df['Yards']) | (train_df['YardsLeft'] - 100 > train_df['Yards'])], inplace=True)","2615cc9b":"### Drop the categorical features and fillna ###\n    \ntrain_df = train_df.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\ntrain_df = train_df.drop(['index', 'IsRusher', 'Team'], axis=1)\ntrain_df = train_df.set_index(['GameId', 'PlayId'])\ncat_features = []\nfor col in train_df.columns:\n    if train_df[col].dtype =='object':\n        cat_features.append(col)\ntrain_df = train_df.drop(cat_features, axis=1)\ntrain_df.fillna(-999, inplace=True)\n\nclean_time = time.time()\nprint(\"Finish Data Cleaning, session time: {}s\".format(clean_time - start_time))","da158163":"def clean_data(df):\n    df['StadiumType'] = df['StadiumType'].apply(clean_StadiumType)\n    df['StadiumType'] = df['StadiumType'].apply(transform_StadiumType)\n    df['DefendersInTheBox_vs_Distance'] = df['DefendersInTheBox'] \/ df['Distance']\n    df['OffenseFormation'] = df['OffenseFormation'].apply(lambda x: x if x in off_form else np.nan)\n    df = pd.concat([df.drop(['OffenseFormation'], axis=1), pd.get_dummies(df['OffenseFormation'], prefix='Formation')], axis=1)\n    missing_cols = set( dummy_col ) - set( df.columns )-set('Yards')\n    for c in missing_cols:\n        df[c] = 0\n    df = df[dummy_col]\n#     df.drop(['Yards'], axis=1, inplace=True)\n    df['Turf'] = df['Turf'].map(Turf)\n    df['Turf'] = df['Turf'] == 'Natural'\n    df['PossessionTeam'] = df['PossessionTeam'].map(map_abbr)\n    df['HomeTeamAbbr'] = df['HomeTeamAbbr'].map(map_abbr)\n    df['VisitorTeamAbbr'] = df['VisitorTeamAbbr'].map(map_abbr)\n    df['HomePossesion'] = df['PossessionTeam'] == df['HomeTeamAbbr']\n    df['Field_eq_Possession'] = df['FieldPosition'] == df['PossessionTeam']\n    df['HomeField'] = df['FieldPosition'] == df['HomeTeamAbbr']\n    df['GameClock'] = df['GameClock'].apply(strtoseconds)\n    df['PlayerHeight'] = df['PlayerHeight'].apply(lambda x: 12*int(x.split('-')[0])+int(x.split('-')[1]))\n    df['PlayerBMI'] = 703*(df['PlayerWeight']\/(df['PlayerHeight'])**2)\n    df['TimeHandoff'] = df['TimeHandoff'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeSnap'] = df['TimeSnap'].apply(lambda x: datetime.datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\"))\n    df['TimeDelta'] = df.apply(lambda row: (row['TimeHandoff'] - row['TimeSnap']).total_seconds(), axis=1)\n    df['PlayerBirthDate'] = df['PlayerBirthDate'].apply(lambda x: datetime.datetime.strptime(x, \"%m\/%d\/%Y\"))\n    seconds_in_year = 60*60*24*365.25\n    df['PlayerAge'] = df.apply(lambda row: (row['TimeHandoff']-row['PlayerBirthDate']).total_seconds()\/seconds_in_year, axis=1)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: x.lower().replace('mph', '').strip() if not pd.isna(x) else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split('-')[0])+int(x.split('-')[1]))\/2 if not pd.isna(x) and '-' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: (int(x.split()[0])+int(x.split()[-1]))\/2 if not pd.isna(x) and type(x)!=float and 'gusts up to' in x else x)\n    df['WindSpeed'] = df['WindSpeed'].apply(lambda x: float(x) if str(x).isnumeric() else -1)\n    df['WindDirection'] = df['WindDirection'].apply(clean_WindDirection)\n    df['WindDirection'] = df['WindDirection'].apply(transform_WindDirection)\n    df['PlayDirection'] = df['PlayDirection'].apply(lambda x: x.strip() == 'right')\n    df['Team'] = df['Team'].apply(lambda x: x.strip()=='home')\n    indoor = \"indoor\"\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: indoor if not pd.isna(x) and indoor in x else x)\n    df['GameWeather'] = df['GameWeather'].apply(lambda x: x.lower().replace('coudy', 'cloudy').replace('clouidy', 'cloudy').replace('party', 'partly').replace('clear and sunny', 'sunny and clear').replace('skies', '').replace(\"mostly\", \"\").strip() if not pd.isna(x) else x)\n    df['GameWeather'] = df['GameWeather'].apply(map_weather)\n    df['IsRusher'] = df['NflId'] == df['NflIdRusher']\n    df['X'] = df.apply(lambda row: row['X'] if row['PlayDirection'] else 120-row['X'], axis=1)\n    df['Orientation'] = df.apply(lambda row: new_orientation(row['Orientation'], row['PlayDirection']), axis=1)\n    df['Dir'] = df.apply(lambda row: new_orientation(row['Dir'], row['PlayDirection']), axis=1)\n    df['YardsLeft'] = df.apply(lambda row: 100-row['YardLine'] if row['HomeField'] else row['YardLine'], axis=1)\n    df['YardsLeft'] = df.apply(lambda row: row['YardsLeft'] if row['PlayDirection'] else 100-row['YardsLeft'], axis=1)\n    \n    # Drop the categorical features and fillna\n    \n    df = df.sort_values(by=['PlayId', 'Team', 'IsRusher', 'JerseyNumber']).reset_index()\n    df = df.drop(['TimeHandoff', 'TimeSnap', 'PlayerBirthDate', 'NflId', 'NflIdRusher', 'index', 'IsRusher', 'Team'], axis=1)\n    df = df.set_index(['GameId', 'PlayId'])\n    cat_features = []\n    for col in df.columns:\n        if df[col].dtype =='object':\n            cat_features.append(col)\n    df = df.drop(cat_features, axis=1)\n    df.fillna(-999, inplace=True)\n    return df\n\n\n# Run Function\n# original_df = pd.read_csv('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv', low_memory=False)\n# clean_df = clean_data(original_df)","ac202702":"from sklearn.preprocessing import StandardScaler\nimport torch\nimport torch.nn as nn\nimport torchvision","f1a7220a":"# Split Design Matrix\n\nplay_cols = []\nplayer_cols = []\nfor col in train_df.columns:\n    if train_df[col][:22].std() == 0:\n        play_cols.append(col)\n    else:\n        player_cols.append(col)\n\ntry:\n    player_cols.remove(\"level_0\")\nexcept:\n    pass","0d38d963":"print(play_cols)\nprint(player_cols)","726d90ac":"# Transform Function\n\n# The train_df will be transformed into (play_matrix(2D), player_cube(3D), targets), also get standardized\n\ndef transform_df(df, is_train=True, col1=play_cols, col2=player_cols):\n     \n    if is_train:\n        \n        play_cols.remove('Yards')\n        play_df = df[play_cols]\n        player_df = df[player_cols]\n        targets = df['Yards']\n        \n        # Transform play\n        play_df = play_df[::22]\n        x1 = torch.tensor(StandardScaler().fit_transform(play_df).astype(float), dtype=torch.float32)\n\n        # Transform player -> 3D Cube\n        player_df = StandardScaler().fit_transform(player_df)\n        player_df = player_df.reshape(play_df.shape[0], player_df.shape[1], 22)\n        x2 = torch.tensor(player_df.astype(float), dtype=torch.float32)\n\n        # Transform targets\n        index = play_df.reset_index(\"GameId\").index\n\n        y = torch.tensor(targets[::22].values, dtype=torch.float32)\n\n        y_dis = torch.zeros([y.shape[0], 199])\n        y_dis[torch.arange(y.shape[0]), (y + 99).long()] = 1\n\n        return x1, x2, y_dis, index\n\n    else:\n        \n        play_df = df[play_cols]\n        player_df = df[player_cols]\n        \n        # Transform play\n        play_df = play_df[::22]\n        x1 = torch.tensor(StandardScaler().fit_transform(play_df).astype(float), dtype=torch.float32)\n\n        # Transform player -> 3D Cube\n        player_df = StandardScaler().fit_transform(player_df)\n        player_df = player_df.reshape(play_df.shape[0], player_df.shape[1], 22)\n        x2 = torch.tensor(player_df.astype(float), dtype=torch.float32)\n        \n        index = play_df.reset_index(\"GameId\").index\n\n        return x1, x2, index\n\n\n# Run Function\nx1, x2, y, index = transform_df(train_df)","323739f2":"x1.shape, x2.shape, y.shape, index.shape","a9bd6cad":"# Define Customized mixture nn\n\n\"\"\"\nNeural Network Structure:\n\nPlay Data -> Dense Layer -> \n                            Mixture Matrix -> Dense Layers -> Softmax -> NN output\nPlayer Data -> CNN Layer -> \n\nThe NN output stands for the probabilities in yards range [-99, 99]\n\"\"\"\n\nclass MixtureNN(nn.Module):\n    def __init__(self):\n        super(MixtureNN, self).__init__()\n        \n        self.x1_dense = nn.Sequential(\n            nn.Linear(len(play_cols), 200),\n            nn.ReLU(),\n            nn.Linear(200, 100),\n            nn.ReLU(),\n        )\n        \n        #################\n        # params tuning #\n        # MaxPooling ?  #\n        # kernel size   #\n        #################\n        \n        self.x2_conv = nn.Sequential(\n            nn.Conv1d(in_channels=len(player_cols), out_channels=22, kernel_size=5, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv1d(in_channels=22, out_channels=11, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n        )\n        self.x_dense = nn.Sequential(\n            nn.Linear(320, 480),\n            nn.BatchNorm1d(480),\n#             nn.Dropout(0.05),\n            nn.ReLU(),\n            nn.Linear(480, 240),\n            nn.BatchNorm1d(240),\n#             nn.Dropout(0.05),\n            nn.ReLU(),\n            nn.Linear(240, 199),\n            nn.Softmax(dim=1)\n        )\n    \n    def forward(self, x1, x2):\n        x1 = self.x1_dense(x1)\n        x2 = self.x2_conv(x2).reshape(x1.shape[0], 220)\n        x = torch.cat((x1, x2), dim=1)\n#         print(x.shape, x1.shape, x2.shape)\n        x = self.x_dense(x)\n        return x","947f53f4":"MixtureNN()(x1,x2).shape","8709c492":"\"\"\"\nLoss = (pred_dis - actual_dist)**2.sum(dim=1).mean()\nSimilar to Cross Entrophy\n\"\"\"\n\nclass CRPS(nn.Module):\n    def __init__(self):\n        super(CRPS, self).__init__()\n        \n    def forward(self, x, y):\n#         y_index = (y + 99).long()\n#         loss = (x**2).sum(dim=1) - 2 * x[torch.arange(y_index.shape[0]), y_index] + 1\n#         loss = -torch.log(x[torch.arange(y_index.shape[0]), y_index] + 1)\n        loss = ((torch.cumsum(x, dim=1) - torch.cumsum(y, dim=1))**2).mean(1)\n        return loss.sum()","47d8ee6b":"# V10\n\nbefore_training_time = time.time()\nloss_func = CRPS()\n\n\n################\n# split method #\n################\n\n# Define train validation split function\ndef random_split(val_rate=0.2, batch=512):\n    total_index = np.arange(len(y))\n    val_index = np.random.choice(total_index, int(len(total_index) * 0.2), replace=False)\n    train_index = np.delete(total_index, val_index)\n\n    # Generate DataLoader\n    train_set = torch.utils.data.TensorDataset(x1[train_index], x2[train_index], y[train_index])\n    val_set = torch.utils.data.TensorDataset(x1[val_index], x2[val_index], y[val_index])\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch, shuffle=True, num_workers=8)\n    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch, shuffle=True, num_workers=8)\n    return train_loader, val_loader\n\n\n# Define function to calculate average loss for all batches\ndef calculate_average_loss(dataloader: torch.utils.data.DataLoader):\n    running_loss = 0\n    for x1_batch, x2_batch, y_batch in dataloader:\n        prediction = model(x1_batch, x2_batch)\n        loss = loss_func(prediction, y_batch)\n        running_loss += float(loss)\n    return running_loss \/ len(dataloader.dataset)\n\n\n\nprint(\"======== START TRAINING......\")\ncv_times = 0\nmodels_params = []\nsum_best_loss = 0\n\nwhile cv_times < 20:\n    \n    stable_step = 0 \n    best_val_loss = 100\n    epoch = 0\n    session_time = time.time()\n    \n    # Define Network\n    model = MixtureNN()\n    model.train()\n    # Define Optimizer\n#     optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.01)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n    \n    print(\"======== Start Model {} ========\".format(cv_times))\n    train_loader, val_loader = random_split(val_rate=0.3, batch=512)\n    \n    while stable_step < 50:\n        for x1_batch, x2_batch, y_batch in train_loader:\n\n            output = model(x1_batch, x2_batch)\n            loss_val = loss_func(output, y_batch)\n            loss_val.backward()\n            optimizer.step()\n\n        train_ave_loss = calculate_average_loss(train_loader)\n        val_ave_loss = calculate_average_loss(val_loader)\n\n        if val_ave_loss < best_val_loss:\n            best_val_loss = val_ave_loss\n            best_params = model.state_dict()\n            stable_step = 0\n        else:\n            stable_step += 1\n\n        epoch += 1\n        print(\"epoch: {}, train_loss: {}, val_loss: {}, best_val_loss: {}, stable_step: {}\".format(\n            epoch, train_ave_loss, val_ave_loss, best_val_loss, stable_step))\n\n        # MAX epoch\n        if epoch >= 500:\n            break\n    \n    # Save model \n    print(\"======== Finish Model {} ========\".format(cv_times))\n    print(\"Model training time: {}s. The best evaluation loss is {}.\".format(time.time() - session_time, best_val_loss))\n    models_params.append(best_params)\n    sum_best_loss += best_val_loss\n    cv_times += 1\n\ntraining_time = time.time()\nprint(\"======== END, total training time: {}s\".format(training_time - before_training_time))\nprint(\"average_best_loss: {}\".format(sum_best_loss \/ cv_times))","974e2579":"# V20\n\n# before_training_time = time.time()\n# loss_func = CRPS()\n\n\n# ################\n# # split method #\n# ################\n\n# # Define random train validation split function\n# def random_split(val_rate=0.2, batch=512):\n#     total_index = np.arange(len(y))\n#     val_index = np.random.choice(total_index, int(len(total_index) * 0.2), replace=False)\n#     train_index = np.delete(total_index, val_index)\n\n#     # Generate DataLoader\n#     train_set = torch.utils.data.TensorDataset(x1[train_index], x2[train_index], y[train_index])\n#     val_set = torch.utils.data.TensorDataset(x1[val_index], x2[val_index], y[val_index])\n#     train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch, shuffle=True, num_workers=8)\n#     val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch, shuffle=True, num_workers=8)\n#     return train_loader, val_loader\n\n\n# # Define time series train validation split function\n# def time_series_split(total_folds=10, train_folds=4, val_folds=1, step=1, start=0, batch=512):\n    \n#     if start > total_folds - train_folds - val_folds:\n#         raise ValueError('invalid start')\n        \n#     fold_len = int(len(y) \/ total_folds)\n#     fold_index = np.arange(0, len(y), fold_len)\n#     train_index = np.arange(fold_index[start], fold_index[start + train_folds])\n#     val_index = np.arange(fold_index[start + train_folds], fold_index[start + train_folds + val_folds])\n    \n#     # Generate DataLoader\n#     train_set = torch.utils.data.TensorDataset(x1[train_index], x2[train_index], y[train_index])\n#     val_set = torch.utils.data.TensorDataset(x1[val_index], x2[val_index], y[val_index])\n#     train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch, shuffle=True, num_workers=8)\n#     val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch, shuffle=True, num_workers=8)\n#     return train_loader, val_loader\n    \n    \n# # Define function to calculate average loss for all batches\n# def calculate_average_loss(dataloader: torch.utils.data.DataLoader):\n#     running_loss = 0\n#     for x1_batch, x2_batch, y_batch in dataloader:\n#         prediction = model(x1_batch, x2_batch)\n#         loss = loss_func(prediction, y_batch)\n#         running_loss += float(loss)\n#     return running_loss \/ len(dataloader.dataset)\n\n\n# train_time = 10\n# train_result = []\n# init_time = time.time()\n# for i in range(train_time):\n    \n#     print(\"TRAINING TIME {}\".format(i))\n#     print(\"======== START TRAINING......\")\n#     cv_times = 0\n#     models_params = []\n#     sum_best_loss = 0\n\n#     while cv_times < 2:\n\n#         stable_step = 0 \n#         best_val_loss = 100\n#         epoch = 0\n#         session_time = time.time()\n\n#         # Define Network\n#         model = MixtureNN()\n#         model.train()\n#         # Define Optimizer\n#     #     optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.01)\n#         optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n\n#         print(\"======== Start Model {} ========\".format(cv_times))\n#     #     train_loader, val_loader = random_split(val_rate=0.3, batch=512)\n#         train_loader, val_loader = time_series_split(total_folds=5, train_folds=3, val_folds=1, step=1, start=cv_times, batch=512)\n\n#         while stable_step < 50:\n#             for x1_batch, x2_batch, y_batch in train_loader:\n\n#                 output = model(x1_batch, x2_batch)\n#                 loss_val = loss_func(output, y_batch)\n#                 loss_val.backward()\n#                 optimizer.step()\n\n#             train_ave_loss = calculate_average_loss(train_loader)\n#             val_ave_loss = calculate_average_loss(val_loader)\n\n#             if val_ave_loss < best_val_loss:\n#                 best_val_loss = val_ave_loss\n#                 best_params = model.state_dict()\n#                 stable_step = 0\n#             else:\n#                 stable_step += 1\n\n#             epoch += 1\n#             print(\"epoch: {}, train_loss: {}, val_loss: {}, best_val_loss: {}, stable_step: {}\".format(\n#                 epoch, train_ave_loss, val_ave_loss, best_val_loss, stable_step))\n\n#             # MAX epoch\n#             if epoch >= 500:\n#                 break\n\n#         # Save model \n#         print(\"======== Finish Model {} ========\".format(cv_times))\n#         print(\"Model training time: {}s. The best evaluation loss is {}.\".format(time.time() - session_time, best_val_loss))\n#         models_params.append(best_params)\n#         sum_best_loss += best_val_loss\n#         cv_times += 1\n\n#     training_time = time.time()\n#     print(\"======== END, total training time: {}s\".format(session_time - before_training_time))\n#     print(\"average_best_loss: {}\".format(sum_best_loss \/ cv_times))\n#     train_result.append(models_params)\n    \n# print(\"\\nTOTAL: {}s\".format(time.time() - init_time))","d19a9793":"# V10\n\ndef make_pred(df, sample, env, models_params):\n    \n    clean_df = df\n    clean_df = clean_data(clean_df)\n    x1, x2, index = transform_df(clean_df, is_train=False)\n    y_preds = []\n#     print(x1.shape, x2.shape)\n    \n    for params in models_params:\n        model.load_state_dict(params)\n        model.eval()\n        y_preds.append(torch.cumsum(model(x1, x2), dim=1).detach().data.numpy())\n        \n    y_ave = np.array(y_preds).mean(0)\n    yardsleft = np.array(clean_df[\"YardsLeft\"][::22])\n\n    for i in range(len(yardsleft)):\n        y_ave[i, :yardsleft[i] - 1] = 0\n        y_ave[i, yardsleft[i] + 99:] = 1\n    env.predict(pd.DataFrame(data=y_ave.clip(0, 1), columns=sample.columns))\n    \n    return y_ave","ec83f8e2":"# V20\n\n# def make_pred(df, sample, env, train_result):\n    \n#     clean_df = df\n#     clean_df = clean_data(clean_df)\n#     x1, x2, index = transform_df(clean_df, is_train=False)\n#     final_pred = []\n# #     print(x1.shape, x2.shape)\n    \n#     for params_list in train_result:\n#         y_preds = []\n#         for params in params_list:\n#             model.load_state_dict(params)\n#             model.eval()\n#             y_preds.append(torch.cumsum(model(x1, x2), dim=1).detach().data.numpy())\n#         y_ave = np.array(y_preds).mean(0)\n#         final_pred.append(y_ave)\n    \n#     final_ave = np.array(final_pred).mean(0)\n#     yardsleft = np.array(clean_df[\"YardsLeft\"][::22])\n\n#     for i in range(len(yardsleft)):\n#         final_ave[i, :yardsleft[i] - 1] = 0\n#         final_ave[i, yardsleft[i] + 99:] = 1\n#     env.predict(pd.DataFrame(data=final_ave.clip(0, 1), columns=sample.columns))\n    \n#     return final_pred","037ca546":"for test, sample in tqdm.tqdm(env.iter_test()):\n    make_pred(test, sample, env, models_params)\n","ce78a8c0":"env.write_submission_file()","0babae55":"total_time = time.time()\nprint('Total running time is: {}s'.format(total_time - start_time))","16f1d054":"# Build NN Model","54b44264":"# Prediction","4a6a9d21":"### Data Cleaning Aggregate Function","d52505af":"### Define Mixture Neural Network","87baab8d":"### Data Cleaning & New Features","6cf2d7ad":"### Define Loss Function","8b95b273":"# Feature Engineer","75af2378":"### Split Design Matrix and Transformation","203dff8f":"# Environment Setting","34ed1652":"### Build Pipeline and Train","7aab270d":"# Overview"}}