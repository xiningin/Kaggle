{"cell_type":{"b1a5d76c":"code","1ffafd0b":"code","a8876d40":"code","3ec40cf3":"code","76656cfe":"code","c9c4ea3f":"code","c72f2c21":"code","a7a8f11c":"code","5d73df9c":"code","85a36a7c":"code","88cf88ac":"code","83f34eca":"code","563a23dd":"code","1ad78663":"code","fdb87dd7":"code","028f97fe":"code","35534089":"code","a5324308":"code","965ba02c":"code","7d4a80dd":"code","bc947c84":"code","05f54ca0":"code","0187f644":"code","da866cda":"code","3980a3c7":"code","75d23047":"code","154ad3ed":"code","39420c73":"code","f84546a0":"code","f2864671":"code","84dd5786":"code","881fcf51":"code","a5b11542":"code","dfb89ff8":"code","2e310fdc":"code","269c7e4f":"code","a5387609":"code","d71793ab":"code","072d5ce3":"code","11ea54d1":"code","924a3260":"code","5fb18c57":"code","e3b17a72":"markdown","8d779d6e":"markdown","e375d1a3":"markdown","d5719982":"markdown","3bc541ff":"markdown","74ebbbdc":"markdown","9e79fb76":"markdown","dfc22154":"markdown","4956c353":"markdown","39162415":"markdown","566a9fa0":"markdown","645a8e15":"markdown","01f5e5fa":"markdown","589c5e64":"markdown","7358f1b7":"markdown","00a201d2":"markdown","d0342a58":"markdown","2db4d6a0":"markdown","4f25d427":"markdown","4375c295":"markdown","704acbed":"markdown","faebb027":"markdown","3f914d18":"markdown","59198f7a":"markdown","2f6b4532":"markdown","dfcbeff9":"markdown","c8aec625":"markdown","324480b0":"markdown","113d81d7":"markdown","1a826bab":"markdown","2c0817e7":"markdown","5131e7b2":"markdown","15f07f40":"markdown","f72ae576":"markdown","da895fe4":"markdown","cf072acc":"markdown"},"source":{"b1a5d76c":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport featuretools as ft\nfrom featuretools.primitives import *\nfrom featuretools.variable_types import Numeric\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel, SelectKBest, RFE, chi2\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1ffafd0b":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')\ndf = pd.concat([traindf, testdf], axis=0, sort=False)","a8876d40":"df.head(5)","3ec40cf3":"#Thanks to:\n# https:\/\/www.kaggle.com\/mauricef\/titanic\n# https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n#\ndf = pd.concat([traindf, testdf], axis=0, sort=False)\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['Title'] = df.Name.str.split(',').str[1].str.split('.').str[0].str.strip()\ndf['IsWomanOrBoy'] = ((df.Title == 'Master') | (df.Sex == 'female'))\ndf['LastName'] = df.Name.str.split(',').str[0]\nfamily = df.groupby(df.LastName).Survived\ndf['WomanOrBoyCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).count())\ndf['WomanOrBoyCount'] = df.mask(df.IsWomanOrBoy, df.WomanOrBoyCount - 1, axis=0)\ndf['FamilySurvivedCount'] = family.transform(lambda s: s[df.IsWomanOrBoy].fillna(0).sum())\ndf['FamilySurvivedCount'] = df.mask(df.IsWomanOrBoy, df.FamilySurvivedCount - \\\n                                    df.Survived.fillna(0), axis=0)\ndf['WomanOrBoySurvived'] = df.FamilySurvivedCount \/ df.WomanOrBoyCount.replace(0, np.nan)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.replace(np.nan, 0)\ndf['Alone'] = (df.WomanOrBoyCount == 0)\n\n#Thanks to https:\/\/www.kaggle.com\/kpacocha\/top-6-titanic-machine-learning-from-disaster\n#\"Title\" improvement\ndf['Title'] = df['Title'].replace('Ms','Miss')\ndf['Title'] = df['Title'].replace('Mlle','Miss')\ndf['Title'] = df['Title'].replace('Mme','Mrs')\n# Embarked\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Cabin, Deck\ndf['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\ndf.loc[(df['Deck'] == 'T'), 'Deck'] = 'A'\n\n# Thanks to https:\/\/www.kaggle.com\/erinsweet\/simpledetect\n# Fare\nmed_fare = df.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndf['Fare'] = df['Fare'].fillna(med_fare)\n#Age\ndf['Age'] = df.groupby(['Sex', 'Pclass', 'Title'])['Age'].apply(lambda x: x.fillna(x.median()))\n# Family_Size\ndf['Family_Size'] = df['SibSp'] + df['Parch'] + 1\n\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\ncols_to_drop = ['Name','Ticket','Cabin']\ndf = df.drop(cols_to_drop, axis=1)\n\ndf.WomanOrBoySurvived = df.WomanOrBoySurvived.fillna(0)\ndf.WomanOrBoyCount = df.WomanOrBoyCount.fillna(0)\ndf.FamilySurvivedCount = df.FamilySurvivedCount.fillna(0)\ndf.Alone = df.Alone.fillna(0)\ndf.Alone = df.Alone*1","76656cfe":"df.head(5)","c9c4ea3f":"df_optimum = pd.concat([df.WomanOrBoySurvived.fillna(0), df.Alone, df.Sex.replace({'male': 0, 'female': 1})], axis=1)","c72f2c21":"target = df.Survived.loc[traindf.index]\ndf = df.drop(['SibSp','Parch','IsWomanOrBoy','WomanOrBoyCount','FamilySurvivedCount','WomanOrBoySurvived','Alone'], axis=1)\ndf['PassengerId'] = df.index\ndf.head()","a7a8f11c":"es = ft.EntitySet(id = 'titanic_data')\nes = es.entity_from_dataframe(entity_id = 'df', dataframe = df.drop(['Survived'], axis=1), \n                              variable_types = \n                              {\n                                  'Embarked': ft.variable_types.Categorical,\n                                  'Sex': ft.variable_types.Boolean,\n                                  'Title': ft.variable_types.Categorical,\n                                  'Family_Size': ft.variable_types.Numeric,\n                                  'LastName': ft.variable_types.Categorical\n                              },\n                              index = 'PassengerId')","5d73df9c":"es = es.normalize_entity(base_entity_id='df', new_entity_id='Pclass', index='Pclass')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Age', index='Age')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Fare', index='Fare')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Embarked', index='Embarked')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title', index='Title')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='LastName', index='LastName')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Deck', index='Deck')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Family_Size', index='Family_Size')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title_Sex', index='Sex')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Sex_LastName', index='LastName')\nes = es.normalize_entity(base_entity_id='df', new_entity_id='Title_LastName', index='LastName')\nes","85a36a7c":"primitives = ft.list_primitives()\npd.options.display.max_colwidth = 500\nprimitives[primitives['type'] == 'aggregation'].head(primitives[primitives['type'] == 'aggregation'].shape[0])","88cf88ac":"pd.set_option('max_columns',500)\npd.set_option('max_rows',500)","83f34eca":"features, feature_names = ft.dfs(entityset = es, \n                                 target_entity = 'df', \n                                 max_depth = 2)\nlen(feature_names)","563a23dd":"feature_names","1ad78663":"features","fdb87dd7":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\ncols = features.columns.values.tolist()\nfor col in cols:\n    if features[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","028f97fe":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in features.columns:\n        le = LabelEncoder()\n        le.fit(list(features[col].astype(str).values))\n        features[col] = le.transform(list(features[col].astype(str).values))","35534089":"features.head(3)","a5324308":"train, test = features.loc[traindf.index], features.loc[testdf.index]\nX_norm = MinMaxScaler().fit_transform(train)","965ba02c":"# Threshold for removing correlated variables\nthreshold = 0.9\n\ndef highlight(value):\n    if value > threshold:\n        style = 'background-color: pink'\n    else:\n        style = 'background-color: palegreen'\n    return style\n\n# Absolute value correlation matrix\ncorr_matrix = features.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.style.applymap(highlight)","7d4a80dd":"# Select columns with correlations above threshold\ncollinear_features = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures_filtered = features.drop(columns = collinear_features)\n#features_positive = features_filtered.loc[:, features_filtered.ge(0).all()]\nprint('The number of features that passed the collinearity threshold: ', features_filtered.shape[1])","bc947c84":"FE_option0 = features.columns\nFE_option1 = features_filtered.columns\nprint(len(FE_option0), len(FE_option1))","05f54ca0":"lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train, target)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])\nX_selected_df.shape","0187f644":"FE_option2 = X_selected_df.columns\nFE_option2","da866cda":"lasso = LassoCV(cv=5).fit(train, target)\nmodel = SelectFromModel(lasso, prefit=True)\nX_new = model.transform(train)\nX_selected_df = pd.DataFrame(X_new, columns=[train.columns[i] for i in range(len(train.columns)) if model.get_support()[i]])","3980a3c7":"FE_option3 = X_selected_df.columns\nFE_option3","75d23047":"# Visualization from https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e\n# but to k='all'\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(train, target)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(train.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(len(dfcolumns),'Score')) ","154ad3ed":"FE_option4 = featureScores[featureScores['Score'] > 1000]['Feature']\nlen(FE_option4)","39420c73":"FE_option5 = featureScores[featureScores['Score'] > 100]['Feature']\nlen(FE_option5)","f84546a0":"rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=50, step=10, verbose=5)\nrfe_selector.fit(X_norm, target)","f2864671":"rfe_support = rfe_selector.get_support()\nrfe_feature = train.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","84dd5786":"FE_option6 = rfe_feature","881fcf51":"embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=200), threshold='1.25*median')\nembeded_rf_selector.fit(train, target)","a5b11542":"embeded_rf_support = embeded_rf_selector.get_support()\nembeded_rf_feature = train.loc[:,embeded_rf_support].columns.tolist()\nprint(str(len(embeded_rf_feature)), 'selected features')","dfb89ff8":"FE_option7 = embeded_rf_feature","2e310fdc":"test_rule = df_optimum.loc[testdf.index]\n# The one line of the code for prediction : LB = 0.83253 (Titanic Top 3%) \ntest_rule['Survived'] = (((test_rule.WomanOrBoySurvived <= 0.238) & (test_rule.Sex > 0.5) & (test_rule.Alone > 0.5)) | \\\n          ((test_rule.WomanOrBoySurvived > 0.238) & \\\n           ~((test_rule.WomanOrBoySurvived > 0.55) & (test_rule.WomanOrBoySurvived <= 0.633))))\n\n# Saving the result\npd.DataFrame({'Survived': test_rule['Survived'].astype(int)}, \\\n             index=testdf.index).reset_index().to_csv('survived.csv', index=False)","269c7e4f":"acc_simple_rule = 92.7\nLB_simple_rule = 0.83253","a5387609":"def RF (features_set,file):\n    # Tuning Random Forest model for features \"features_set\", makes prediction and save it into file  \n    train_fe = train[features_set]\n    test_fe = test[features_set]\n    random_forest = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [100, 500]}, cv=5).fit(train_fe, target)\n    random_forest.fit(train_fe, target)\n    Y_pred = random_forest.predict(test_fe).astype(int)\n    random_forest.score(train_fe, target)\n    acc_random_forest = round(random_forest.score(train_fe, target) * 100, 2)\n    pd.DataFrame({'Survived': Y_pred}, index=testdf.index).reset_index().to_csv(file, index=False)\n    return acc_random_forest","d71793ab":"acc0 = RF(FE_option0, 'survived_FT.csv')\nacc1 = RF(FE_option1, 'survived_FE1_Pearson.csv')\nacc2 = RF(FE_option2, 'survived_FE2_LinSVC.csv')\nacc3 = RF(FE_option3, 'survived_FE3_Lasso.csv')\nacc4 = RF(FE_option4, 'survived_FE4_Chi2_1000.csv')\nacc5 = RF(FE_option5, 'survived_FE5_Chi2_100.csv')\nacc6 = RF(FE_option6, 'survived_FE6_RFE_LogR.csv')\nacc7 = RF(FE_option7, 'survived_FE7_RFE_RF.csv')","072d5ce3":"# After download solutions in Kaggle competition:\n# 2019:\n# LB0 = 0.74641\n# LB1 = 0.73684\n# LB2 = 0.75119\n# LB3 = 0.75598\n# LB4 = 0.76076\n# LB5 = 0.74641\n# LB6 = 0.74641\n# LB7 = 0.74162\n# 2020:\nLB0 = 0.74162\nLB1 = 0.73444\nLB2 = 0.74401\nLB3 = 0.74401\nLB4 = 0.75358\nLB5 = 0.73923\nLB6 = 0.75358\nLB7 = 0.73444","11ea54d1":"models = pd.DataFrame({\n    'Model': ['Simple rule',\n              'FT',\n              'FT + Pearson correlation', \n              'FT + SelectFromModel with LinearSVC',\n              'FT + SelectFromModel with Lasso', \n              'FT + SelectKBest with Chi-2 with Score > 1000',\n              'FT + SelectKBest with Chi-2 with Score > 100',\n              'FT + RFE with Logistic Regression',\n              'FT + RFE with Random Forest'],\n    \n    'acc':  [acc_simple_rule, acc0, acc1, acc2, acc3, acc4, acc5, acc6, acc7],\n\n    'LB':   [LB_simple_rule, LB0, LB1, LB2, LB3, LB4, LB5, LB6, LB7]})","924a3260":"models.sort_values(by=['acc', 'LB'], ascending=False)","5fb18c57":"models.sort_values(by=['LB', 'acc'], ascending=False)","e3b17a72":"### 5.1. The simple rule - very accurate model <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","8d779d6e":"## 3. Preparing to modeling with manual FE <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","e375d1a3":"## 6. Comparison of 28 models <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","d5719982":"### 4.1. FS with the Pearson correlation <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","3bc541ff":"### 4.6. FS by the Recursive Feature Elimination (RFE) with Random Forest<a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Back to Table of Contents](#0.1)","74ebbbdc":"Thanks to https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection","9e79fb76":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","dfc22154":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [Preparing to modeling with manual FE](#3)\n -  [Clearning data and manual FE](#3.1)\n -  [Automatic FE with Featuretools](#3.2)\n -  [Encoding categorical features](#3.3)\n1. [Automatic feature selection (FS)](#4)\n -  [FS with the Pearson correlation](#4.1)\n -  [FS by the SelectFromModel with LinearSVC](#4.2) \n -  [FS by the SelectFromModel with Lasso](#4.3) \n -  [FS by the SelectKBest with Chi-2](#4.4)\n -  [FS by the Recursive Feature Elimination (RFE) with Logistic Regression](#4.5) \n -  [FS by the Recursive Feature Elimination (RFE) with Random Forest](#4.6) \n1. [Modeling](#5)\n -  [The simple rule - very accurate model](#5.1)\n -  [The Random Forest Classifiers for 8 options of selected feature sets](#5.2)\n1. [Comparison of 28 models](#6)\n1. [Conclusions](#7)","4956c353":"Thanks to https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset","39162415":"Thanks to https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection","566a9fa0":"### 4.4. FS by the SelectKBest with Chi-2 <a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","645a8e15":"### 4.2. FS by the SelectFromModel with LinearSVC <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","01f5e5fa":"### 4.5. FS by the Recursive Feature Elimination (RFE) with Logistic Regression<a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","589c5e64":"[Go to Top](#0.0)","7358f1b7":"### 3.3. Encoding categorical features <a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","00a201d2":"### 3.1. Clearning data and manual FE <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","d0342a58":"### 3.2. Automatic FE with Featuretools <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","2db4d6a0":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6e\/St%C3%B6wer_Titanic.jpg)","4f25d427":"### 5.2. The Random Forest Classifiers for 8 options of selected feature sets <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","4375c295":"Thanks to:\n\n* FE from the https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset\n\n* Visualization from the https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20","704acbed":"## 5. Modeling <a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","faebb027":"<a class=\"anchor\" id=\"0.0\"><\/a>\n# Titanic : Comparison of automatic FE&FS efficiency with Featuretools and tradicional approaches","3f914d18":"**Comparison of 9 models, including 8 new models**","59198f7a":"Select the main features from which the optimum rule with LB = 0.83253 (see [Titanic Top 3% : one line of the prediction code](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code)) for features were manually selected and **look if the Featuretools library can find them yourself**","2f6b4532":"## 7. Conclusions <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","dfcbeff9":"Thanks to https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset","c8aec625":"From my kernel: [Titanic Top 3% : one line of the prediction code](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code)","324480b0":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html","113d81d7":"I hope you find this kernel useful and enjoyable.","1a826bab":"Your comments and feedback are most welcome.","2c0817e7":"The analysis makes the following conclusions:\n\n- The results of the work of the methods of the **Featuretools** library and their post-processing in different ways **did not allow us to come close to the optimal set** of features done manually (from kernel [Titanic Top 3% : one line of the prediction code](https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code))\n\n\u00a0- Among **the methods of post-processing** the Featuretools results in this competition, **the best is SelectKBest with strong filtering** (to only 23 features), the less effective one is the **SelectFromModel** method\n \n- Lack of post-processing of Featuretools results or application of RFE methods is the least effective to predict\n\n- **the Featuretools library has many possibility** - this kernel has not opened them all, so it is advisable to conduct a more in-depth study of this task (other max_depth in *ft.dfs*, other constraints for feature filtering, etc.)","5131e7b2":"## 4. Automatic feature selection (FS)<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","15f07f40":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","f72ae576":"My kernels outline traditional approaches to FE:\n\n1) the consolidated result of EDA and FE optimization from many authors:\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-cluster-analysis\n* https:\/\/www.kaggle.com\/vbmokin\/titanic-top-3-one-line-of-the-prediction-code\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-15\n* https:\/\/www.kaggle.com\/vbmokin\/three-lines-of-code-for-titanic-top-20\n\n2) the result of the formation of many features and their processing by 20 models (boosting, regression, simple neural networks, etc.):\n\n* [Titanic (0.83253) - Comparison 20 popular models](https:\/\/www.kaggle.com\/vbmokin\/titanic-0-83253-comparison-20-popular-models)\n\n\nThe kernel [Automated feature engineering for Titanic dataset](https:\/\/www.kaggle.com\/liananapalkova\/automated-feature-engineering-for-titanic-dataset) provides an example of using library Featuretools for automatic FE. Let us analyze whether this application will produce comparable results.","da895fe4":"Thanks to:\n* https:\/\/www.kaggle.com\/sz8416\/6-ways-for-feature-selection\n* https:\/\/towardsdatascience.com\/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e","cf072acc":"### 4.3. FS by the SelectFromModel with Lasso <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}