{"cell_type":{"b1e5e4cc":"code","23835bdd":"code","11018e77":"code","4c2a8da0":"code","2ed1d0b7":"code","956f570c":"code","e022a272":"code","c7ae75cc":"code","aa1e0193":"code","6d51eac0":"code","e7100f6d":"code","b016d649":"code","be75d52c":"code","5ca2d4c0":"code","a4fd10b0":"code","1b46733f":"code","727a278f":"code","f73f8ffa":"code","cd47903b":"code","843a622d":"code","10c89dc4":"code","fc55cc02":"code","58137da7":"code","1cfb5e28":"code","2a7d3a9f":"code","2ab20729":"code","cb7e246d":"code","818c1517":"code","21b56159":"code","9527a9a2":"code","e331035a":"code","15eaa2bc":"code","0eb2520c":"code","032b4281":"code","6c6b5ba8":"code","ba521234":"code","88125cc4":"code","1545450f":"code","09a773e8":"code","aa86fa15":"code","54796558":"code","4f66f9ac":"code","1630aacb":"code","4cb49e15":"code","a099f764":"code","d4791306":"code","d4c63837":"code","df3b6332":"code","dd1f897d":"code","74cc9d4f":"code","6890e64f":"markdown","2023bcd7":"markdown","91405b7f":"markdown","26669418":"markdown","df524253":"markdown","2fb155e5":"markdown","e50cf9bf":"markdown","7eb59478":"markdown","54ad0c0c":"markdown","a6133376":"markdown","2d561a13":"markdown","32c7e185":"markdown","c7bffb77":"markdown","4f9ddfc8":"markdown","3cb7c3a2":"markdown","f4d6f6ef":"markdown","9f168127":"markdown","1812b23a":"markdown","2e58575e":"markdown","adc39e2b":"markdown","08022478":"markdown","5466f6ac":"markdown","5415456b":"markdown","b73b9e28":"markdown","94dff2d4":"markdown","3d264783":"markdown","e59a70ab":"markdown","67b911e5":"markdown","b137d543":"markdown","b41f5eaf":"markdown","1a1c6edf":"markdown","6bab7267":"markdown","495a32f0":"markdown","cfe0d46b":"markdown","6721e71a":"markdown","0b35c580":"markdown","a02e515d":"markdown","d0ae3c40":"markdown","6627a3eb":"markdown","d44c6b9b":"markdown","4bf019de":"markdown"},"source":{"b1e5e4cc":"import pandas as pd\nimport numpy as np\nimport keras\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nfrom collections import Counter # counter takes values returns value_counts dictionary\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\n\nnp.random.seed(2)","23835bdd":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","11018e77":"df.head()","4c2a8da0":"print(\"Credit Card Fraud Detection data -  rows:\",df.shape[0],\" columns:\", df.shape[1])","2ed1d0b7":"df.describe()","956f570c":"#Let's check if there's any missing data\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","e022a272":"# get class distribution\nprint (\"Normal transaction:\", df['Class'][df['Class']==0].count()) #class = 0\nprint (\"Fraudulent transaction:\", df['Class'][df['Class']==1].count()) #class = 1","c7ae75cc":"temp = df[\"Class\"].value_counts()\ndf_class = pd.DataFrame({'Class': temp.index,'values': temp.values})","aa1e0193":"#visualizing the class label\ntrace = go.Bar(\n    x = df_class['Class'],y = df_class['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df_class['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=800\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='class')","6d51eac0":"# separate classes into different datasets\nnormal_class = df.query('Class == 0')\nfraudulent_class = df.query('Class == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=69)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=69)","e7100f6d":"#plot the time feature\nplt.figure(figsize=(10,8), )\nplt.title('Time Distribution (Seconds)')\nsns.distplot(df['Time'],color='red');","b016d649":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Time of transaction vs Amount by class')\n\nax1.scatter(fraudulent_class.Time, fraudulent_class.Amount)\nax1.set_title('Fraud')\n\nax2.scatter(normal_class.Time, normal_class.Amount)\nax2.set_title('Normal')\n\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","be75d52c":"#plot the amount feature\nplt.figure(figsize=(10,8))\nplt.title('Distribution of Amount')\nsns.distplot(df['Amount'],color='blue');","5ca2d4c0":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15,9))\nf.suptitle('Amount per transaction by class')\n\nbins = 50\n\nax1.hist(fraudulent_class.Amount, bins = bins)\nax1.set_title('Fraud')\n\nax2.hist(normal_class.Amount, bins = bins)\nax2.set_title('Normal')\n\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","a4fd10b0":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","1b46733f":"df['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))","727a278f":"target = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']","f73f8ffa":"X = df[predictors]\ny = df[target]","cd47903b":"# Use stratify to ensure samples of fraud label are in the test set\n#stratify split is used when there is misclassified data\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, random_state=1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","843a622d":"y_train.value_counts(normalize=True)","10c89dc4":"y_test.value_counts(normalize=True)","fc55cc02":"from sklearn.linear_model import LogisticRegression # Importing Classifier Step","58137da7":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train) ","1cfb5e28":"y_pred = logreg.predict(X_test)","2a7d3a9f":"from sklearn.metrics import confusion_matrix, classification_report, r2_score, roc_auc_score, roc_curve, auc","2ab20729":"print(classification_report(y_test, y_pred))","cb7e246d":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","818c1517":"print('Accuracy :{0:0.5f}'.format(accuracy_score(y_pred , y_test))) \nprint('AUC : {0:0.5f}'.format(roc_auc_score(y_test , y_pred)))\nprint('Precision : {0:0.5f}'.format(precision_score(y_test , y_pred)))\nprint('Recall : {0:0.5f}'.format(recall_score(y_test , y_pred)))\nprint('F1 : {0:0.5f}'.format(f1_score(y_test , y_pred)))\n# print('Confusion Matrix : \\n', cnf_matrix)\nprint(\"\\n\")","21b56159":"# Predicted values counts for fraud and genuine of test dataset\n# Our model predicted 96 transactions as fraud and 71106 transactions as genuine from the test dataset.\npd.Series(y_pred).value_counts()","9527a9a2":"# Actual values counts for fraud and genuine of test dataset\npd.Series(y_test).value_counts()","e331035a":"# There are originally 123 fraud transactions and our model predicted only 96 fraud transaction. \n# So the accuracy of our model should be  96\/123 , right?\n96\/123","15eaa2bc":"#confusion matrix\ncnf_matrix = confusion_matrix(y_test,y_pred)\nprint(cnf_matrix)","0eb2520c":"#heatmap for confusion matrix\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"winter\" ,fmt='g')\n\nplt.title('Confusion matrix', y=1.1, fontsize = 22)\nplt.ylabel('Actual',fontsize = 18)\nplt.xlabel('Predicted',fontsize = 18)\n\nplt.show()","032b4281":"## We already know that we have 123 fraud transaction in our test dataset, but our model predicted only 87 fraud transaction. So the real accuracy of our model is:\n87\/123","6c6b5ba8":"#ROC-AUC SCORE\nroc_auc_score(y_test , y_pred) ","ba521234":"y_pred_proba = logreg.predict_proba(X_test)\ny_pred_proba","88125cc4":"# plot ROC Curve\n\nplt.figure(figsize=(8,6))\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nauc = roc_auc_score(y_test, y_pred)\nprint(\"AUC - \",auc,\"\\n\")\n\nplt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\n\nplt.plot([0,1], [0,1], 'k--' )\n\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for Predicting a credit card fraud detection')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.show()","1545450f":"print('Original dataset shape %s' % Counter(y))\n\nrus = RandomUnderSampler(random_state=42)\nX_under, y_under = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_under))\n\n# Slit into train and test datasets\nX_train_under, X_test_under, y_train_under, y_test_under = train_test_split(X_under, y_under, shuffle=True, test_size=0.3, random_state=0)","09a773e8":"print('Original dataset shape %s' % Counter(y))\n\nros = RandomOverSampler(random_state=42)\nX_over, y_over = ros.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_over))\n\n# Slit into train and test datasets\nX_train_over, X_test_over, y_train_over, y_test_over = train_test_split(X_over, y_over, test_size=0.3, shuffle=True, random_state=0)","aa86fa15":"print('Original dataset shape %s' % Counter(y))\n\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_smote))\n\n# Slit into train and test datasets\nX_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.3, shuffle=True, random_state=0)","54796558":"print('Original dataset shape %s' % Counter(y))\n\nadasyn = ADASYN(random_state=42)\nX_adasyn, y_adasyn = adasyn.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_adasyn))\n\n# Slit into train and test datasets\nX_train_adasyn, X_test_adasyn, y_train_adasyn, y_test_adasyn = train_test_split(X_adasyn, y_adasyn, test_size=0.3, shuffle=True, random_state=0)","4f66f9ac":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\nRANDOM_STATE = 2018\n\nMAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result\n\nIS_LOCAL = False","1630aacb":"names_lst = []\n\n# Empty list to capture performance matrix for train set\naucs_train_lst = []\naccuracy_train_lst = []\nprecision_train_lst = []\nrecall_train_lst = []\nf1_train_lst = []\n\n# Empty list to capture performance matrix for test set\naucs_test_lst = []\naccuracy_test_lst = []\nprecision_test_lst = []\nrecall_test_lst = []\nf1_test_lst = []\n\n# Function for model building and performance measure\n\ndef build_measure_model(models):\n    plt.figure(figsize=(12,6))\n\n    for name, model,Xdata,ydata in models:\n        \n        names_lst.append(name)\n\n        # split data in train test set\n        X_train, X_test, y_train, y_test = train_test_split(Xdata, ydata, test_size=0.3, shuffle=True, random_state=0)\n        \n        # Build model\n        model.fit(X_train, y_train)\n        \n        # Predict\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        # calculate accuracy\n        Accuracy_train = accuracy_score(y_train, y_train_pred)\n        accuracy_train_lst.append(Accuracy_train)\n        \n        Accuracy_test = accuracy_score(y_test, y_test_pred)\n        accuracy_test_lst.append(Accuracy_test)\n\n        # calculate auc\n        Aucs_train = roc_auc_score(y_train, y_train_pred)\n        aucs_train_lst.append(Aucs_train)\n        \n        Aucs_test = roc_auc_score(y_test , y_test_pred)\n        aucs_test_lst.append(Aucs_test)\n\n        # calculate precision\n        PrecisionScore_train = precision_score(y_train , y_train_pred)\n        precision_train_lst.append(PrecisionScore_train)\n        \n        PrecisionScore_test = precision_score(y_test , y_test_pred)\n        precision_test_lst.append(PrecisionScore_test)\n\n        # calculate recall\n        RecallScore_train = recall_score(y_train , y_train_pred)\n        recall_train_lst.append(RecallScore_train)\n        \n        RecallScore_test = recall_score(y_test , y_test_pred)\n        recall_test_lst.append(RecallScore_test)\n\n        # calculate f1 score\n        F1Score_train = f1_score(y_train , y_train_pred)\n        f1_train_lst.append(F1Score_train)\n        \n        F1Score_test = f1_score(y_test , y_test_pred)\n        f1_test_lst.append(F1Score_test)\n\n        #print('F1 Score of '+ name +' model : {0:0.5f}'.format(F1Score_test))\n\n        # draw confusion matrix\n        cnf_matrix = confusion_matrix(y_test , y_test_pred)\n        #cnf_matrix_lst.append(cnf_matrix)\n\n        print(\"Model Name :\", name)\n        \n        print('Train Accuracy :{0:0.5f}'.format(Accuracy_train)) \n        print('Test Accuracy :{0:0.5f}'.format(Accuracy_test))\n        \n        print('Train AUC : {0:0.5f}'.format(Aucs_train))\n        print('Test AUC : {0:0.5f}'.format(Aucs_test))\n        \n        print('Train Precision : {0:0.5f}'.format(PrecisionScore_train))\n        print('Test Precision : {0:0.5f}'.format(PrecisionScore_test))\n        \n        print('Train Recall : {0:0.5f}'.format(RecallScore_train))\n        print('Test Recall : {0:0.5f}'.format(RecallScore_test))\n        \n        print('Train F1 : {0:0.5f}'.format(F1Score_train))\n        print('Test F1 : {0:0.5f}'.format(F1Score_test))\n        \n        print('Confusion Matrix : \\n', cnf_matrix)\n        print(\"\\n\")\n\n\n        # plot ROC Curve\n        fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n        auc = roc_auc_score(y_test, y_test_pred)\n        plt.plot(fpr,tpr,linewidth=2, label=name + \", auc=\"+str(auc))\n    \n        #---------- For loops ends here--------#\n\n\n    plt.legend(loc=4)\n    plt.plot([0,1], [0,1], 'k--' )\n    plt.rcParams['font.size'] = 12\n    plt.title('ROC curve for Predicting a credit card fraud detection')\n    plt.xlabel('False Positive Rate (1 - Specificity)')\n    plt.ylabel('True Positive Rate (Sensitivity)')\n    plt.show()","4cb49e15":"#------------------ Logistic Regression (LR) ------------------#\nLRmodels = []\n\nLRmodels.append(('LR imbalance', LogisticRegression(solver='liblinear', multi_class='ovr'),X,y))\nLRmodels.append(('LR Undersampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_under,y_under))\nLRmodels.append(('LR Oversampling', LogisticRegression(solver='liblinear', multi_class='ovr'),X_over,y_over))\nLRmodels.append(('LR SMOTE', LogisticRegression(solver='liblinear', multi_class='ovr'),X_smote,y_smote))\nLRmodels.append(('LR ADASYN', LogisticRegression(solver='liblinear', multi_class='ovr'),X_adasyn,y_adasyn))\n\n# Call function to create model and measure its performance\nprint(build_measure_model(LRmodels))","a099f764":"#-----------------Decision Tree (DT)------------------#\nDTmodels = []\n\ndt = DecisionTreeClassifier()\n\nDTmodels.append(('DT imbalance', dt,X,y))\nDTmodels.append(('DT Undersampling', dt,X_under,y_under))\nDTmodels.append(('DT Oversampling', dt,X_over,y_over))\nDTmodels.append(('DT SMOTE', dt,X_smote,y_smote))\nDTmodels.append(('DT ADASYN', dt,X_adasyn,y_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(DTmodels)","d4791306":"#-----------------Random Forest (RF) ------------------#\nRFmodels = []\n\nclf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)\n\nRFmodels.append(('RF imbalance', clf,X,y))\nRFmodels.append(('RF Undersampling', clf,X_under,y_under))\nRFmodels.append(('RF Oversampling', clf,X_over,y_over))\nRFmodels.append(('RF SMOTE', clf,X_smote,y_smote))\nRFmodels.append(('RF ADASYN', clf,X_adasyn,y_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(RFmodels)","d4c63837":"#-----------------XG BOOST(XG) ------------------#\nXGmodels = []\n\n# Prepare the train and valid datasets\n#data_dmatrix = xgb.DMatrix(data=X,label=y)\n\nxgb_model = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 10, eval_metric ='auc', random_state = RANDOM_STATE)\n\nXGmodels.append(('XG imbalance', xgb_model,X,y))\nXGmodels.append(('XG Undersampling', xgb_model,X_under,y_under))\nXGmodels.append(('XG Oversampling', xgb_model,X_over,y_over))\nXGmodels.append(('XG SMOTE', xgb_model,X_smote,y_smote))\nXGmodels.append(('XG ADASYN', xgb_model,X_adasyn,y_adasyn))\n\n# Call function to create model and measure its performance\nbuild_measure_model(XGmodels)","df3b6332":"data = {'Model':names_lst,\n       'Accuracy_Train':accuracy_train_lst,\n       'Accuracy_Test':accuracy_test_lst,\n       'AUC_Train':aucs_train_lst,\n       'AUC_Test':aucs_test_lst,\n       'PrecisionScore_Train':precision_train_lst,\n       'PrecisionScore_Test':precision_test_lst,\n       'RecallScore_Train':recall_train_lst,\n       'RecallScore_Test':recall_test_lst,\n       'F1Score_Train':f1_train_lst,\n       'F1Score_Test':f1_test_lst}","dd1f897d":"print(\"Performance measures of various classifiers: \\n\")\nperformance_df = pd.DataFrame(dict([ (k,pd.Series(v)) for k,v in data.items() ])) \nperformance_df.sort_values(['AUC_Test','RecallScore_Test','F1Score_Test'],ascending=False)","74cc9d4f":"df = pd.DataFrame(performance_df,columns=['Model','AUC_Test','RecallScore_Test','F1Score_Test'])\ndf.plot(x ='Model', kind = 'bar', figsize=(15, 8))\nplt.show()","6890e64f":"## PREDICTIVE MODELS","2023bcd7":"## CREDIT CARD FRAUD DETECTION\n\n### Context\n#### What is Credit Card Fraud?\nCredit card fraud is when someone uses another person's credit card or account information to make unauthorized purchases or access funds through cash advances. Credit card fraud doesn\u2019t just happen online; it happens in stores, too. As a business owner, you can avoid serious headaches \u2013 and unwanted publicity \u2013 by recognizing potentially fraudulent use of credit cards in your payment environment.\n\n### Three challenges surrounding credit card fraud\n1. It's not always easy to agree on ground truth for what \"fraud\" means.\n2. Regardless of how you define ground truth, the vast majority of charges are not fraudulent.\n3. Most merchants aren't experts at evaluating the business impact of fraud.\n\n### Problem Statement:\nThe Credit Card Fraud Detection Problem includes modeling past credit card transactions with the knowledge of the ones that turned out to be a fraud. This model is then used to identify whether a new transaction is fraudulent or not. Our aim here is to detect 100% of the fraudulent transactions while minimizing the incorrect fraud classifications.\n\n### Why does class imbalanced affect model performance?\nIn general, we want to maximize the recall while capping FPR (False Positive Rate), but you can classify a lot of charges wrong and still maintain a low FPR because you have a large number of true negatives.\nThis is conducive to picking a relatively low threshold, which results in the high recall but extremely low precision.\n\n### What is the catch?\nTraining a model on a balanced dataset optimizes performance on validation data.\nHowever, the goal is to optimize performance on the imbalanced production dataset. You ultimately need to find a balance that works best in production.\nOne solution to this problem is: Use all fraudulent transactions, but subsample non-fraudulent transactions as needed to hit our target rate.\n\n### Business questions to brainstorm:\nSince all features are anonymous, we will focus our analysis on non-anonymized features: Time, Amount\nHow different is the amount of money used in different transaction classes?\nDo fraudulent transactions occur more often during a certain frames?\n\n### Dataset\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. \n\nDue to confidentiality issues, there are not provided the original features and more background information about the data\n\n1. Features V1, V2, ... V28 are the principal components obtained with PCA;\n2. The only features which have not been transformed with PCA are Time and Amount. Feature Time contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature Amount is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n3. Feature Class is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n#### Acknowledgements\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http:\/\/mlg.ulb.ac.be\/BruFence and http:\/\/mlg.ulb.ac.be\/ARTML\n\nPlease cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015","91405b7f":"## FUNCTION FOR MODEL BUILDING AND PERFORMANCE MEASURE","26669418":"### The above graph shows that Time is irrelevent for detecting fraudulent transactions","df524253":"#### So 78.048% should be our accuracy rate","2fb155e5":"## 3. RANDOM FOREST(RF)","e50cf9bf":"## TRAIN THE MODELS","7eb59478":"### Let's Build different models with different balanced datasets, first by creating multiple datasets for undersampled , oversampled and SMOTE sampled","54ad0c0c":"## 1. LOGISTIC REGRESSION(LR)","a6133376":"## CHECK THE DATA","2d561a13":"## DATA IMBALANCE","32c7e185":"### Given that this distribution is two days\u2019 worth of data, it would follow the trend I\u2019d expect to see for normal consumers. Most purchases are made during the daylight hours, and as people get out of work\/school and head home, purchasing dwindles down until the next day.","c7bffb77":"## DATA EXPLORATION","4f9ddfc8":"### Receiver Operating Characteristics (ROC)\nThe ROC is a performance measurement for classification problems at various thresholds. It is essentially a probability curve, and the higher the Area Under the Curve (AUC) score the better the model is at predicting fraudulent\/non-fraudulent transactions.","3cb7c3a2":"### Model Evaluation","f4d6f6ef":"#### Looking at the Time feature, we can confirm that the data contains 284,807 transactions, during 2 consecutive days (or 172792 seconds).\n#### Looking at the Amount feature, we can say that there's a wide range of amount, so scaling the data prior to running the model will provide better results.","9f168127":"## IMPORTING REQUIRED LIBRARIES","1812b23a":"### Define predictors and target values\nLet's define the predictor features and the target features. Categorical features, if any, are also defined. In our case, there are no categorical feature.","2e58575e":"### TRANSACTIONS IN TIME","adc39e2b":"#### As expected, there is no notable correlation between features V1-V28. There are certain correlations between some of these features and Time (inverse correlation with V3) and Amount (direct correlation with V7 and V20, inverse correlation with V1 and V5).","08022478":"## OverSampled Data","5466f6ac":"## SMOTE Data","5415456b":"#### Split data in train and test set\nLet's define train and test sets.","b73b9e28":"## CHECKING MISSING DATA","94dff2d4":"### Performance measures of various classifiers","3d264783":"#### As we can observe, the dataset is highly imbalanced with only 492 cases i.e 0.17% of the observations being in the Fraudulent category.","e59a70ab":"#### We are aware that our dataset is highly imbalanced, however, we check the performance of imbalance dataset first and later we implement some techniques to balance the dataset and again check the performance of balanced dataset. Finally, we will compare each models performance.\n### Let's start with a Logistic Regression model.","67b911e5":"## CORRELATION MATRIX","b137d543":"### STANDARDSCALAR ON TIME AND AMOUNT","b41f5eaf":"### AMOUNT DISTRIBUTION","1a1c6edf":"### Hightlights\n\nAfter training each of the models, these are the final results. All of the scores for Random Forest with Oversampling technique and the Random Forest with SMOTE technique models are very promising for our dataset! Each model has a high true positive rate and a low false-positive rate, which is exactly what we\u2019re looking for.\n\nIn the ROC graph above, the AUC scores for Random Forest with Oversampling technique is pretty high, which is what we\u2019d like to see. As we move further right along the curve, we both capture more True Positives but also incur more False Positives. This means we capture more fraudulent transactions, but also flag even more normal transactions as fraudulent.   \n#### So Random Forest with Oversampling technique  is our final model, as this gives highest Recall score of 100% on both train and test datasets.","6bab7267":"## 4. XG BOOST","495a32f0":"#### MODEL EVALUATION MATRIX\nEvery problem is different and derives a different set of values for a particular business use case , thus every model must be evaluated differently.","cfe0d46b":"However, this not the case. Actually there are originally 123 fraud transactions and 71079 genuine transactions in the test dataset. However, our model predicted only 96 fraud transaction. Also, it should be kept in mind that these 96 predicted fraud transaction may not be identified correctly. It means that these predicted 96 fraud transactions are NOT only from 123 originally fraud transaction, but they may also be from genuine transactions as well.\n\nWe will see our real accuracy in below cells.","6721e71a":"### While the vast majority of transactions are very low, this distribution is also expected. Most daily transactions aren\u2019t extremely expensive (most are < 50), but it\u2019s likely where most fraudulent transactions are occurring as well.","0b35c580":"## 2. DECISION TREE(DT)","a02e515d":"#### There are no missing values in the entire dataset","d0ae3c40":"## UnderSampled Data","6627a3eb":"### The above graph shows that most of the fraudulent transactions are of very low amount","d44c6b9b":"#### So, 70.73% is the real accuracy of our model, which is nothing but the Recall Score. So we have the emphasis on Recall score and F1 score to measure the performance of our model, not the accuracy.","4bf019de":"## ADASYN Data"}}