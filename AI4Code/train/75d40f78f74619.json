{"cell_type":{"759ac3a8":"code","a2f1fae7":"code","d6513b32":"code","c750100e":"code","94837b44":"code","41faf395":"code","7779bf4c":"code","d78b661a":"code","866d964f":"code","132c26c2":"code","885c65c5":"code","9b69b2b1":"code","a6245ec3":"code","d7fccc59":"code","cfd4031c":"code","923914bb":"code","a83ef414":"code","1694d9ab":"code","62d347b4":"code","9bf41271":"code","6aa1c4ae":"code","a0189056":"code","d566d978":"code","1b56a9c8":"code","3ce904f5":"code","d3da366a":"code","a2632044":"code","b28cc22f":"code","d9cd2b4b":"code","62975f97":"code","25f8e19f":"code","12e09d93":"code","3d401b9c":"code","28c1fbf6":"code","871d9056":"code","41a70789":"code","aad49dcb":"code","10048f4d":"code","2552d86e":"code","d9801917":"code","8271d9c1":"code","87950eb5":"code","47901b08":"code","cdb9d3db":"code","c0484b5f":"code","941b10ab":"code","25d8d423":"code","98953049":"code","8306a367":"code","c93dcf68":"code","ffd8dc0f":"code","2035bc8f":"code","b14d62c1":"code","f645c5ef":"markdown","c5c602d2":"markdown","d3e64c3f":"markdown","6971c616":"markdown","41930bdf":"markdown","952e23c7":"markdown","6200eba0":"markdown","c7d16dd5":"markdown","7172a77a":"markdown","1c25ae30":"markdown","3a5e331b":"markdown","63bf4b8a":"markdown","8007ee9f":"markdown","c76a9cf1":"markdown","bab1a252":"markdown","5cf945ad":"markdown","9a00eacd":"markdown","2d7f89bc":"markdown","e75ae4d9":"markdown","d9d26f62":"markdown","fcc9b531":"markdown","4f7aa1d4":"markdown","931139f6":"markdown","2ad8b050":"markdown","26ea3a2c":"markdown","769841cf":"markdown","5a37238b":"markdown"},"source":{"759ac3a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2f1fae7":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport os\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\n\n# 'Statsmodels' is used to build and analyze various statistical models\nimport statsmodels\nimport statsmodels.api as sm\nimport statsmodels.stats.api as sms\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.compat import lzip\nfrom statsmodels.graphics.gofplots import ProbPlot\n\n# 'SciPy' is used to perform scientific computations\nfrom scipy.stats import f_oneway\nfrom scipy.stats import jarque_bera\nfrom scipy import stats","d6513b32":"df = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","c750100e":"df.shape","94837b44":"df.dtypes","41faf395":"df.describe()","7779bf4c":"Total = df.isnull().sum().sort_values(ascending=False) \n\nPercent = (df.isnull().sum()*100\/df.isnull().count()).sort_values(ascending=False)   \n\nmissing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])\n\n# print the missing data\nmissing_data","d78b661a":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df.isnull(), cbar=False)\n\n# display the plot\nplt.show()","866d964f":"num_col = df.select_dtypes(include=np.number)\nnum_col.columns","132c26c2":"corr = num_col.corr()\n\n# print the correlation matrix\ncorr","885c65c5":"plt.figure(figsize=(15, 8))\n\nsns.heatmap(corr, cmap='YlGnBu', vmax=1.0, vmin=-1.0,annot = True, annot_kws={\"size\": 15}, )\n\n# specify name of the plot using plt.title()\nplt.title('Correlation between numeric features')\n\n# display the plot\nplt.show()","9b69b2b1":"from sklearn.linear_model import LinearRegression\nX = df.drop('alcohol', axis=1)\n\n# extract the target variable from the data set\ny = df['alcohol']\n\n# split data into train subset and test subset for predictor and target variables\n# random_state: the seed used by the random number generator\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","a6245ec3":"model = LinearRegression()\nmodel.fit(X_train, y_train)","d7fccc59":"y_train_pred= model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nr2_train = r2_score(y_train, y_train_pred)\nr2_test = r2_score(y_test, y_test_pred)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\nrmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint(r2_train, r2_test)\nprint(rmse_train, rmse_test)","cfd4031c":"sns.pairplot(df)","923914bb":"# set the plot size\nplt.rcParams['figure.figsize']=(18,8)\n\n# create a boxplot for all numeric features\n# column: selects the specified columns\ndf.boxplot()\n# to display the plot\nplt.show()","a83ef414":"Q1 = df.drop(['alcohol'], axis=1).quantile(0.25)\n\n# compute the first quartile using quantile(0.75)\n# use .drop() to drop the target variable \n# axis=1: specifies that the labels are dropped from the columns\nQ3 = df.drop(['alcohol'], axis=1).quantile(0.75)\n\n# calculate of interquartile range \nIQR = Q3 - Q1\n\n# print the IQR values for numeric variables\nprint(IQR)","1694d9ab":"# filter out the outlier values\n# ~ : selects all rows which do not satisfy the condition\n# |: bitwise operator OR in python\n# any() : returns whether any element is True over the columns\n# axis : \"1\" indicates columns should be altered (use \"0\" for 'index')\ndf = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]","62d347b4":"df.shape","9bf41271":"# set figure size \nplt.rcParams['figure.figsize']=(15,8)\n\n# recheck for outliers\n# column: selects the specifies columns\ndf.boxplot()\n# display only the plot\nplt.show()","6aa1c4ae":"# generate the correlation matrix \ncorr =  df.corr()\n\n# print the correlation matrix\ncorr","a0189056":"sns.heatmap(corr, cmap='YlGnBu', vmax=1.0, vmin=-1.0, annot = True, annot_kws={\"size\": 15})\n\n# specify name of the plot\nplt.title('Correlation between numeric features')\n\n# display the plot\nplt.show()","d566d978":"Xc=sm.add_constant(X)\nol = sm.OLS(y,Xc).fit()\n\n# print the summary output\nprint(ol.summary())","1b56a9c8":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\n\nvf=[vif(Xc.values,i) for i in range(Xc.shape[1])]\n\npd.DataFrame(vf,index=Xc.columns,columns=['vif'])","3ce904f5":"cols=list(X.columns)\n\n\nfor col in cols:\n    X[col+'_2']=X[col]**2\nX.head()","d3da366a":"X.shape\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nlir=LinearRegression()\n\nrfe = RFE(lir,n_features_to_select=10)\nrfe.fit(X,y)\n\npd.DataFrame(rfe.ranking_,index=X.columns,columns=['select']).sort_values(by='select')","a2632044":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nno_of_cols=12\ntrain_score=[]\ntest_score=[]\n\nfor n in range(no_of_cols):\n    lir = LinearRegression()\n    rfe = RFE(lir, n_features_to_select=n+1)\n    rfe.fit(X_train, y_train)\n    \n    score1 = rfe.score(X_train, y_train)\n    train_score.append(score1)\n\n    score2 = rfe.score(X_test, y_test)\n    test_score.append(score2)\n","b28cc22f":"plt.plot(train_score,'g')\nplt.plot(test_score,'r')\nplt.show()","d9cd2b4b":"idx=np.linspace(1,12,12)\nrf=pd.DataFrame(test_score,columns=['r-sq']).sort_values(by='r-sq',ascending=False)\nrf","62975f97":"from sklearn.feature_selection import RFECV","25f8e19f":"lir=LinearRegression()\nrfecv=RFECV(lir,cv=3,scoring='r2')\nrfecv.fit(X,y)","12e09d93":"rfecv.grid_scores_","3d401b9c":"plt.plot(range(1,23),rfecv.grid_scores_)","28c1fbf6":"rf=pd.DataFrame(rfecv.grid_scores_,index=range(1,23),columns=['scores'])\nrf.sort_values(by='scores',ascending=False)","871d9056":"pd.DataFrame(rfecv.ranking_, index=X.columns, columns=['select']).sort_values(by='select')\n","41a70789":"y = df['alcohol']\nX1 =df.drop(['alcohol','total sulfur dioxide'], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n\nlir = LinearRegression(fit_intercept=True)\n\nlir.fit(X_train, y_train)\ny_train_pred = lir.predict(X_train)\n\nr2_Train = r2_score(y_train, y_train_pred)\nrmse_Train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n\nprint('r2-Train: ', r2_Train, 'rmse_Train: ', rmse_Train)\n\ny_test_pred = lir.predict(X_test)\n\nr2_Test = r2_score(y_test, y_test_pred)\nrmse_Test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint('r2-Test: ', r2_Test, 'rmse_Test: ', rmse_Test)\n","aad49dcb":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\ny=df['alcohol']\nX=df.drop('alcohol',axis=1)\n\ncols=list(X.columns)\n\nfor col in cols:\n    X[col+'_2']=X[col]**2\nX.head()","10048f4d":"lir = LinearRegression()\nsfs1=sfs(lir,k_features=22,forward=True,scoring='r2',cv=3,verbose=2)\nsfs1=sfs1.fit(X,y)","2552d86e":"sf=pd.DataFrame(sfs1.subsets_).T\nsf","d9801917":"plt.figure(figsize=(10,5))\nplt.plot(sf.index,sf['avg_score'])\nplt.xlabel('number of features')\nplt.ylabel('r-square')\nplt.show()","8271d9c1":"sf[sf['avg_score']==sf['avg_score'].max()]['feature_names']","87950eb5":"sfs1=sfs(lir,k_features=13,forward=True,scoring='r2',cv=3)\nsfs1=sfs1.fit(X,y)","47901b08":"selected_features=list(sfs1.k_feature_names_)\nselected_features","cdb9d3db":"y = df['alcohol']\nX1 = X[selected_features]\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y, test_size=0.3, random_state=42)\n\nlir = LinearRegression(fit_intercept=True)\n\nlir.fit(X_train, y_train)\ny_train_pred = lir.predict(X_train)\n\nr2_Train = r2_score(y_train, y_train_pred)\nrmse_Train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n\nprint('r2-Train: ', r2_Train, 'rmse_Train: ', rmse_Train)\n\ny_test_pred = lir.predict(X_test)\n\nr2_Test = r2_score(y_test, y_test_pred)\nrmse_Test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n\nprint('r2-Test: ', r2_Test, 'rmse_Test: ', rmse_Test)\n","c0484b5f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,LeaveOneOut\nfrom sklearn.datasets import load_boston\nboston= load_boston()\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.linear_model import Lasso,LassoCV,Ridge,RidgeCV,ElasticNet,ElasticNetCV","941b10ab":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nXs=ss.fit_transform(X)\nXs=pd.DataFrame(Xs,columns=X.columns)\n\nlasso=Lasso(alpha=0.1,max_iter=10000)\nlasso.fit(X,y)\n\npd.DataFrame(lasso.coef_,index=X.columns,columns=['coef'])","25d8d423":"y_pred=lasso.predict(Xs)\nr2_score(y,y_pred)","98953049":"np.linspace(1,10,10)\n\nnp.logspace(-4,1,10)\n\nalphas=np.logspace(-3,-1,10)\ncoefs=[]\nfor a in alphas:\n    lasso=Lasso(alpha=a,max_iter=10000)\n    lasso.fit(Xs,y)\n    coefs.append(lasso.coef_)\n    \n    \nplt.figure(figsize=(10, 5))   \nplt.plot(alphas,coefs)\nplt.xlabel('alphas')\nplt.ylabel('coeffecients')","8306a367":"alphas=np.logspace(-3,0,50)\n\nlassocv=LassoCV(alphas=alphas,cv=3,max_iter=10000,random_state=5)\nlassocv.fit(Xs,y)","c93dcf68":"lassocv.alpha_","ffd8dc0f":"lasso=Lasso(alpha=lassocv.alpha_,max_iter=10000)\nlasso.fit(X,y)\npd.DataFrame(lasso.coef_,index=X.columns,columns=['coef'])","2035bc8f":"y_pred=lasso.predict(Xs)\nr2_score(y,y_pred)","b14d62c1":"Xc=sm.add_constant(X)\nol = sm.OLS(y,Xc).fit()\n\n# print the summary output\nprint(ol.summary())","f645c5ef":"### RFE Cv","c5c602d2":"#### Simulation to understand the impact of alpha on coeffecient","d3e64c3f":"### Using IQR Method","6971c616":"### Finally I got a better model as compair to the base model which I created .\n### R2 value now 76% with the above Linear Regression prcocess. which is a good fit model.","41930bdf":"### Forward Selection Approaches \n","952e23c7":"### Lassso regression\n","6200eba0":"### Recheck the Correlation\n##### Recheck the correlation after treating outliers. An outlier might either decrease or increase a correlation coefficient, depending on where it is in relation to the other points","c7d16dd5":"### Summary Statistics\n","7172a77a":"### Missing Value","1c25ae30":"### Understand the Dataset","3a5e331b":"## Rechecking The Linear OlS Model","63bf4b8a":"#### Visualize the Null Values\n","8007ee9f":"### Rechecking after the Outlair detection\n","c76a9cf1":"### Data preparation is the process of cleaning and transforming raw data prior to building predictive models.\n\n##### Here we will analyze and prepare data to perform regression analysis:\n1. Check dimensions of the dataframe in terms of rows and columns\n2. Check the data types. Refer data definition to ensure your data types are correct\n3. If data types are not as per business context, change the data types as per requirement\n4. Study summary statistics\n5. Check for missing values\n6. Study correlation\n7. Perform feature engineering\n8. Detect outliers\n9. Recheck the correlation\n\n##### Note: It is an art to explore data and one will need more and more practice to gain expertise in this area.","bab1a252":"### Tunning to find out the best alpha","5cf945ad":"# Data Analysis and Preparation","9a00eacd":"# 1. Import Libraries\nLet us import the required libraries and functions","2d7f89bc":"# Base Linear Model","e75ae4d9":"### RFE -Recursive Feature Elimination","d9d26f62":"# Feature Engineering\n","fcc9b531":"## Checking the correlation\n","4f7aa1d4":"## Discover Outliers\n","931139f6":"###### Importance of detecting an outlier\nAn outlier is an observation that appears to deviate distinctly from other observations in the data. If the outliers are not removed, the model accuracy may decrease.\n\n###### Recollect that one of the assumptions of Linear Regression is there should be no outliers present in the data","2ad8b050":"# Read Data","26ea3a2c":"### VIF ","769841cf":"# Regularition\n","5a37238b":"## Linear Regression (OLS)"}}