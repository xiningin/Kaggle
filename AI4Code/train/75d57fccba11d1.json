{"cell_type":{"3bae034d":"code","b59d4b36":"code","2fc5cf0d":"code","2c4f0c4e":"code","d1b1e6c7":"code","a6981891":"code","d0105c86":"code","483f3bbd":"code","e9839f26":"code","9b733f7a":"code","6b67f99e":"code","51ec3e3c":"code","e9d6d4ff":"code","3b515bc6":"code","b217873a":"markdown","2869055d":"markdown","fa16a5ac":"markdown","daf51760":"markdown","0bc1a16f":"markdown","8e4b03ca":"markdown","1f93dca9":"markdown","d1d07f0a":"markdown","3cf4038f":"markdown","479047b1":"markdown"},"source":{"3bae034d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b59d4b36":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport re\nfrom tqdm import tqdm\n\n\ndirs=[\"pmc_json\",\"pdf_json\"]\ndocs=[]\ncounts=0\nfor d in dirs:\n    print(d)\n    counts = 0\n    for file in tqdm(os.listdir(f\"..\/input\/CORD-19-research-challenge\/document_parses\/{d}\")):#What is an f string?\n        file_path = f\"..\/input\/CORD-19-research-challenge\/document_parses\/{d}\/{file}\"\n        j = json.load(open(file_path,\"rb\"))\n        #Taking last 7 characters. it removes the 'PMC' appended to the beginning\n        #also paperid in pdf_json are guids and hard to plot in the graphs hence the substring\n        paper_id = j['paper_id']\n        paper_id = paper_id[-7:]\n        title = j['metadata']['title']\n\n        try:#sometimes there are no abstracts\n            abstract = j['abstract'][0]['text']\n        except:\n            abstract = \"\"\n            \n        full_text = \"\"\n        bib_entries = []\n        for text in j['body_text']:\n            full_text += text['text']\n                \n        docs.append([paper_id, title, abstract, full_text])\n        #comment this below block if you want to consider all files\n        #comment block start\n        counts = counts + 1\n        if(counts >= 25000):\n            break\n        #comment block end    \ndf=pd.DataFrame(docs,columns=['paper_id','title','abstract','full_text']) \n","2fc5cf0d":"print(df.shape)\ndf.head()","2c4f0c4e":"# installing haystack\n\n! pip install git+https:\/\/github.com\/deepset-ai\/haystack.git","d1b1e6c7":"# importing necessary dependencies\n\nfrom haystack import Finder\nfrom haystack.indexing.cleaning import clean_wiki_text\nfrom haystack.indexing.utils import convert_files_to_dicts, fetch_archive_from_http\nfrom haystack.reader.farm import FARMReader\nfrom haystack.reader.transformers import TransformersReader\nfrom haystack.utils import print_answers","a6981891":"! wget https:\/\/artifacts.elastic.co\/downloads\/elasticsearch\/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n! chown -R daemon:daemon elasticsearch-7.6.2\n\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\nes_server = Popen(['elasticsearch-7.6.2\/bin\/elasticsearch'],\n                   stdout=PIPE, stderr=STDOUT,\n                   preexec_fn=lambda: os.setuid(1)  # as daemon\n                  )\n# wait until ES has started\n! sleep 30","d0105c86":"from haystack.database.elasticsearch import ElasticsearchDocumentStore\ndocument_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\")","483f3bbd":"# Now, let's write the dicts containing documents to our DB.\ndocument_store.write_documents(df[['title', 'full_text']].rename(columns={'title':'name','full_text':'text'}).to_dict(orient='records'))","e9839f26":"from haystack.retriever.sparse import ElasticsearchRetriever\nretriever = ElasticsearchRetriever(document_store=document_store)","9b733f7a":"reader = FARMReader(model_name_or_path=\"deepset\/roberta-base-squad2-covid\", use_gpu=True, context_window_size=1000)","6b67f99e":"finder = Finder(reader, retriever)","51ec3e3c":"question = \"What is the impact of coronavirus on babies?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","e9d6d4ff":"question = \"What is the impact of coronavirus on pregnant women?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","3b515bc6":"question = \"which organ does coronavirus impact?\"\nnumber_of_answers_to_fetch = 2\n\nprediction = finder.get_answers(question=question, top_k_retriever=10, top_k_reader=number_of_answers_to_fetch)\nprint(f\"Question: {prediction['question']}\")\nprint(\"\\n\")\nfor i in range(number_of_answers_to_fetch):\n    print(f\"#{i+1}\")\n    print(f\"Answer: {prediction['answers'][i]['answer']}\")\n    print(f\"Research Paper: {prediction['answers'][i]['meta']['name']}\")\n    print(f\"Context: {prediction['answers'][i]['context']}\")\n    print('\\n\\n')","b217873a":"## Welcome Haystack!\n\n![](https:\/\/raw.githubusercontent.com\/deepset-ai\/haystack\/master\/docs\/img\/sketched_concepts_white.png)\n\nThe secret sauce behind scaling up is `Haystack`. It lets you scale QA models to large collections of documents! You can read more about this amazing library here https:\/\/github.com\/deepset-ai\/haystack\n\nFor installation: ! `pip install git+https:\/\/github.com\/deepset-ai\/haystack.git`\n\nBut just to give a background, there are 3 major components to `Haystack`.\n\n**Document Store**: Database storing the documents for our search. We recommend `Elasticsearch`, but have also more light-weight options for fast prototyping (SQL or In-Memory).\n\n**Retriever**: Fast, simple algorithm that identifies candidate passages from a large collection of documents. Algorithms include `TF-IDF` or `BM25`, custom `Elasticsearch` queries, and embedding-based approaches. The Retriever helps to narrow down the scope for Reader to smaller units of text where a given question could be answered.\n\n**Reader**: Powerful neural model that reads through texts in detail to find an answer. Use diverse models like `BERT`, `RoBERTa` or `XLNet` trained via `FARM` or `Transformers` on SQuAD like tasks. The Reader takes multiple passages of text as input and returns top-n answers with corresponding confidence scores. You can just load a pretrained model from Hugging Face's model hub or fine-tune it to your own domain data.\n\nAnd then there is **Finder** which glues together a **Reader** and a **Retriever** as a pipeline to provide an easy-to-use question answering interface.","2869055d":"## Data\n\n - We will take 25,000 articles from `pmc_json` directory and 25000 articles from `pdf_json` - So that it fits into Kaggle's compute limits.\n -  We will extract `paper_id`,`title`,`abstract`,`full_text` and put it in an easy to use `pandas.DataFrame`. ","fa16a5ac":"And finally: The **Finder** sticks together reader and retriever in a pipeline to answer our actual questions.","daf51760":"## Voila! We're done!\n\nLet's see, how well our search engine works!\n\nAs a response to our query - \n - we get the `answer`\n - a 1000 words `context` around the answer \n - and the `name` of the research paper","0bc1a16f":"**Let's prepare Retriever, Reader, & Finder\u00b6**\n\nRetrievers help narrowing down the scope for the Reader to smaller units of text where a given question could be answered. They use some simple but fast algorithm.\n\nHere: We use Elasticsearch's default BM25 algorithm","8e4b03ca":"A **Reader** scans the texts returned by retrievers in detail and extracts the k best answers. They are based on powerful, but slower deep learning models.\n\n`Haystack` currently supports Readers based on the frameworks `FARM` and `Transformers`. With both you can either load a local model or one from `Hugging Face's` model hub (https:\/\/huggingface.co\/models).\n\nHere: a medium sized `RoBERTa QA` model using a Reader based on `FARM` (https:\/\/huggingface.co\/deepset\/roberta-base-squad2)","1f93dca9":"We have 50,000 articles and columns like `paper_id`, `title`, `abstract` and `full_text`\n\nWe will be interested in `title` and `full_text` columns as these columns will be used to build the engine. Let's setup a Search Engine on top `full_text` - which contains the full content of the research papers.","d1d07f0a":"Once `ElasticsearchDocumentStore` is setup, we will write our documents\/texts to the `DocumentStore`.\n\nWriting documents to `ElasticsearchDocumentStore` requires a format - List of dictionaries The default format here is: \n`[{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"},`\n`{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"}`\n`{\"name\": \"<some-document-name>, \"text\": \"<the-actual-text>\"}]`\n\n(Optionally: you can also add more key-value-pairs here, that will be indexed as fields in `Elasticsearch` and can be accessed later for filtering or shown in the responses of the Finder)\n\nWe will use `title` column to pass as `name` and `full_text` column to pass as the `text`","3cf4038f":"## Setting up DocumentStore\n`Haystack` finds answers to queries within the documents stored in a `DocumentStore`. The current implementations of `DocumentStore` include `ElasticsearchDocumentStore`, `SQLDocumentStore`, and `InMemoryDocumentStore`.\n\nBut they recommend `ElasticsearchDocumentStore` because as it comes preloaded with features like full-text queries, BM25 retrieval, and vector storage for text embeddings.\n\nSo - Let's set up a `ElasticsearchDocumentStore`","479047b1":"## Introduction\n\nThis notebook will let you build a faster and accurate CORD Search Engine using Transformers\ud83e\udd17 - on top of the research articles. \n\nIn this notebook, we will:\n - First, structure the input data `(.json)` into easy to use `pandas.DataFrame`\n - Then, use `RoBERTa` model to build Q&A model\n - and Finally, use `Haystack` to scale QA model to thousands of documents and build a search engine."}}