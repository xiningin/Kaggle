{"cell_type":{"d9e23dc3":"code","5769f6e7":"code","e5694c87":"code","69007dbe":"code","c6eb9ab4":"code","424c6a7d":"code","d291bfdf":"code","81bb02fe":"code","cdc40524":"code","ceb717f1":"code","e60e049f":"code","e342418a":"code","a12aea62":"code","8bc4016e":"code","67212dc3":"code","94f5a252":"code","07251a93":"code","c3dff0c9":"code","ce179a24":"code","21d18063":"code","37626091":"code","9162c558":"code","b936581d":"code","abda8f50":"code","3fb56964":"code","3cad9f3d":"code","41ede106":"code","c1f03efe":"code","424262e9":"code","d334bdcf":"code","1ea8ad0d":"code","cea3fea1":"code","8df2f89f":"code","1bddf76f":"code","7336c527":"code","bd83bea8":"code","d8464542":"code","e96d68e2":"code","d499766c":"code","7f24e130":"code","5cd71516":"code","73de45ad":"code","5edbb826":"code","95722f09":"code","5df85af3":"code","fc5d59d2":"code","e83e9b97":"code","29990bd4":"code","6b4e6abc":"code","10956d0d":"markdown","dba8be16":"markdown","5482bb92":"markdown","345f756d":"markdown","d5d0a088":"markdown","12ac2ee3":"markdown","bcfd4b85":"markdown","76a48f38":"markdown","aae7c4ca":"markdown"},"source":{"d9e23dc3":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\n\n# import the library to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5769f6e7":"# import the train dataset\ntrain_original = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')","e5694c87":"# view the train dataset\ntrain_original.head()","69007dbe":"# verify the shape of the train dataset\ntrain_original.shape","c6eb9ab4":"# Check if there are any missing values\ntrain_original.info()","424c6a7d":"# Describe the dataset\ntrain_original.describe().T","d291bfdf":"#import the test dataset\ntest_original = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","81bb02fe":"# View the test dataset\ntest_original.head()","cdc40524":"# Check the null values\ntest_original.info()","ceb717f1":"# Check the distribution of the features in the train dataset using histogram\n# Since there are more features, randomly chose the features out from 50 features \n\n# Histogram for 'feature_0'\nplt.figure(figsize=(20, 10))\nplt.subplot(2,5,1)\nplt.title('Histogram for feature_0')\nplt.xlabel('feature_0')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_0, kde=False, color=['red'])\n\n# Histogram for 'feature_10'\nplt.subplot(2,5,2)\nplt.title('Histogram for feature_10')\nplt.xlabel('feature_10')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_10, kde=False, color=['green'])\n\n# Histogram for 'feature_20'\nplt.subplot(2,5,3)\nplt.title('Histogram for feature_20')\nplt.xlabel('feature_20')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_20, kde=False, color=['blue'])\n\n# Histogram for 'feature_35'\nplt.subplot(2,5,4)\nplt.title('Histogram for feature_35')\nplt.xlabel('feature_35')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_35, kde=False, color=['black'])\n\n\n# Histogram for 'feature_49'\nplt.subplot(2,5,5)\nplt.title('Histogram for feature_49')\nplt.xlabel('feature_49')\nplt.ylabel('Frequency')\nsns.distplot(train_original.feature_49, kde=False, color=['magenta'])","e60e049f":"# Check the balancing of the classes in the 'target'variable\ntrain_original['target'].value_counts()","e342418a":"# Check the balance of the data by plotting count of the target by their values\nplt.figure(figsize=(5,2))\nplt.title('Count plot for target')\nsns.countplot(train_original['target'])","a12aea62":"# store the 'ID' values into the new object train_ID and test_ID and verify the shape\n\n# Train dataset\ntrain_ID = train_original.id\nprint('The shape of the ID feature in train dataset is:',train_ID.shape)\n\n# Test dataset\ntest_ID = test_original.id\nprint('\\nThe shape of the ID feature in test dataset is:',test_ID.shape)","8bc4016e":"# store the remaining values into the new object train_X and test_X and verify the shape\n\n# Train dataset\ntrain_X = train_original.drop(columns = ['id','target'])\nprint('The shape of the final train dataset is:',train_X.shape)\n\n# Test dataset\ntest_X = test_original.drop(columns = ['id'])\nprint('\\nThe shape of the final test dataset is:',test_X.shape)","67212dc3":"# store the 'target' values into the new object train_y and verify the shape\ntrain_y=train_original.target\nprint('The shape of the target feature of train dataset is:',train_y.shape)","94f5a252":"# save these datasets to csv for future use if required\ntrain_ID.to_csv('train_ID.csv',index=False)\ntest_ID.to_csv('test_ID.csv',index=False)\ntrain_X.to_csv('train_X.csv',index=False)\ntest_X.to_csv('test_X.csv',index=False)\ntrain_y.to_csv('train_y.csv',index=False)","07251a93":"# Perform correlation analysis between the features of train dataset and visualize using a heat map.\nplt.figure(figsize=(20,10))\nsns.heatmap(train_X.corr(),annot=True,cmap='RdYlGn');\n##\nplt.xticks(rotation=90, color='indigo', size=10)\nplt.yticks(rotation=0, color='indigo', size=10)","c3dff0c9":"# import the required library\nfrom sklearn.decomposition import PCA","ce179a24":"# create a PCA object (instantiate)\npca=PCA()","21d18063":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X = pca.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X = pca_fit_train_X.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X = pca.transform(test_X)","37626091":"# Calculate the percentage of variation of each principal components\n# Assuming we have to principal components PC1 and PC2, then  \n# explained_variance_ratio for PC1 = (Variation for PC1\/ (Total variation ie (PC1+PC2)))*100\n##\npca_train_X_variation = np.round(pca_fit_train_X.explained_variance_ratio_.cumsum()*100,decimals=1)\n##\n# .cumsum() is used to display PC's variation with respect to cumulative percentage","9162c558":"# Print the cumulative percentage of expalined variance\npca_train_X_variation","b936581d":"# assign labels for each PC's as PC1,2,etc., for visulaization in scree plot \n##\nlabels = ['PC' + str(x) for x in range(1, len(pca_train_X_variation)+1)]","abda8f50":"# generate the scree plot\n## \nplt.figure(figsize=(25,5))\n##\nplt.bar(x=range(1, len(pca_train_X_variation)+1), height=pca_train_X_variation,tick_label=labels)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\n##################\nplt.title('Scree Plot',color='tab:orange', fontsize=25)\n###################\n##\nplt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\nplt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n##","3fb56964":"# create a PCA object again (instantiate) by considering first 28 principal components\npca_28=PCA(n_components=28)","3cad9f3d":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X_28 = pca_28.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X_28 = pca_fit_train_X_28.transform(train_X)\n\n#transform the test dataset\npca_transform_test_X_28 = pca_28.transform(test_X)","41ede106":"# From PCA, the final train and test datasets are as follows\n\n#train data\npca_fit_transform_train_X_28.shape","c1f03efe":"#test data\npca_transform_test_X_28.shape","424262e9":"#train label\ntrain_y.shape","d334bdcf":"# import the library SMOTE to deal with Imbalanced Multiclass Classification\nfrom imblearn.over_sampling import SMOTE\n\n#instantiate SMOTE\nsmote = SMOTE(random_state=42)","1ea8ad0d":"# sample and fit the dataset to balance the classes,thereby increasing the observations\nX_smote, y_smote = smote.fit_resample(pca_fit_transform_train_X_28,train_y)\n\nprint('Shape of independent features(X) before SMOTE :', pca_fit_transform_train_X_28.shape)\nprint('Shape of independent features(X) after SMOTE :' , X_smote.shape)\n\nprint('Shape of dependent feature(y) before SMOTE :', train_y.shape)\nprint('Shape of dependent feature(y) after SMOTE :' , y_smote.shape)\n\nprint('Count of the classes before SMOTE:\\n',train_y.value_counts())\nprint('Count of the classes before SMOTE:\\n',y_smote.value_counts())","cea3fea1":"# I will use only XGBoost Classifier for the second submission. \n# Later, will check how i can improve the score by trying with different algorithms\n\n# import the required libraries\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier","8df2f89f":"# Parameters\nparams={ 'learning_rate' : [0.01, 0.05, 0.1] ,\n'max_depth' : [3,4,5],\n'min_child_weight': [ 0, 1, 3],\n'gamma' : [ 0, 0.25,1],\n'colsample_bytree': [ 0.2, 0.4, 0.6],\n'reg_lambda' : [0,1,10]\n}\n## Explainations of the parameters\n# 'max_depth' - Maximum depth of trees (default = 6, range: [0,\u221e])\n# 'Learning rate'(eta) - scaling the tree by learning rate predicts the output in smaller steps close to the\n# 'reg_lambda' - L2 regularization parameter on weights which estimate the mean of the data to avoid overfit\n# 'gamma' - Minimum loss reduction required to make a further partition on a leaf node of the tree (pruning)\n# 'min_child_weight' - default =1. If the weights of each leaf is less than the min_child weight, then the t\n# So weights of the each leaf is > min_child_weight\n# 'colsample_bytree': It is the subsample ratio of columns when constructing each tree.(default=1, range 0.5","1bddf76f":"# Optimize the Hyperparameter using RandomizedSearchCV\n\n#import the required libraries\nfrom sklearn.model_selection import RandomizedSearchCV","7336c527":"#Instantiate the classifier\nXGB=xgb.XGBClassifier(missing=1)\n# Objective will be automatically set to 'multi:softprob' # Alternate is 'multi:softmax'\n# 'multi:softmax' returns predicted class\n# 'multi:softprob' returns predicted probabilities","bd83bea8":"# Using Random search of parameters with 5 fold cross validation\n# Improve the predictions using cross validation to optimize the parameters\nRandom_Search=RandomizedSearchCV (XGB,param_distributions=params,n_iter=5,n_jobs=-1,cv=5,verbose=-1)\n# cv=5 - Number of folds in a `(Stratified)KFold`","d8464542":"# Fit the train dataset to the Random_Search to obtain the best estimators and parameters.\nRandom_Search.fit(X_smote,y_smote)","e96d68e2":"# Print the best estimator\nRandom_Search.best_estimator_","d499766c":"#Print the best parameters\nRandom_Search.best_params_","7f24e130":"# Instantiate the XGBoost classifier with the best estimators and parameters\nXGB=xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=4,\n              min_child_weight=1, missing=1, monotone_constraints='()',\n              n_estimators=100, n_jobs=2, num_parallel_tree=1,\n              objective='multi:softprob', random_state=0, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n# leave the other parameters to default values","5cd71516":"# Check the accuracy of the model using Number of folds in a `(Stratified)KFold` cv=5\n# import the library\nfrom sklearn.model_selection import cross_val_score\n\nAccuracy=cross_val_score(XGB,X_smote,y_smote,cv=5)\nAccuracy","73de45ad":"#Print the mean accuracy of each k-fold\nprint(\"Accuracy of XGBoost Model with Cross Validation is:\",Accuracy.mean() * 100)","5edbb826":"# Fit the training data\nXGB.fit(X_smote,y_smote)","95722f09":"#predict the test dataset\ny_predict_xgb = XGB.predict(pca_transform_test_X_28)\ny_predict_xgb","5df85af3":"# predict the probabality,the id of test dataset belongs to each class \nprobability_xgb=XGB.predict_proba(pca_transform_test_X_28)\nprobability_xgb","fc5d59d2":"# read the sample submission file\nsample_submit=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","e83e9b97":"# create a dataframe similar to sample submission file\npred_prob_xgb = pd.DataFrame(probability_xgb, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4'])\npred_prob_xgb['id'] = sample_submit['id']","29990bd4":"#Set the index to 'id'\npred_prob_xgb=pred_prob_xgb.set_index('id')\npred_prob_xgb","6b4e6abc":"# Export the predicted probability, the id of test dataset belongs to each class \npred_prob_xgb.to_csv('submission_20210510_V2.csv')","10956d0d":"**Comments**\n> There are 100000 rows and 52 columns in train dataset including 'target' variable. ","dba8be16":"**Comments**\n> There are no null values and all the independent features are of integer datatype and 'target' is of object datatype","5482bb92":"**Comments**\n> It is evident that most of the features distribution are not normal (Right skewed). So, normalization of the data is required before training the model. ","345f756d":"**Comments**\n> 1. There are 50000 rows and 51 columns in test dataset which excludes 'target' variable.\n> 2. There are no null values in the test dataset and all the independent features are of integer datatype","d5d0a088":"**Comments**\n> 1. After SMOTE, the number of observation got increased to 229988 rows from 100000 rows.\n> 2. After SMOTE, the number of majority class_2 remains unchanged (57497), whereas number of minority classes 1,3 and 4 are increased to 57497. This ensures, balancing of the classes is done perfectly.","12ac2ee3":"**Comments**: \n> By looking into the array of elements and scree plot, It is observed that among 50 Principal components, ~90% of variation of the data in train_X dataset is explained by only first 28 principal components.","bcfd4b85":"**Comments** \n> It is observed that correlation between the features are not crossing 0.25. So, it is likely the features are independent of each other","76a48f38":"**Comments**\n> 1. Count of Class2 category is more compared to other class categories. Hence, the problem is imbalanced muticlass classification problem.\n\n> 2. Resampling technique should be used to deal with imbalanced multiclass classification problem. It consists of removing samples from the majority class 'Class2' (under-sampling) and \/ or adding more samples from the minority classes 'Class_1,Class_4,Class_3' (over-sampling).\n\n> 3. Removing samples from the majority class will lead to data loss and adding more samples from the minority class will lead to overfitting.\n\n> 4. Hence, we will use SMOTE (Synthetic Minority Oversampling Technique). Its an oversampling technique where the synthetic samples are generated for the minority class. This algorithm helps to overcome the overfitting problem.","aae7c4ca":"# **Competition Overview:** [Tabular_Playground_Series](https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/overview)\n\n## **Objective:** To predict the probability, the id of test dataset belongs to each class\n\n## **Dataset:** [Synthetic Dataset](https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/data)"}}