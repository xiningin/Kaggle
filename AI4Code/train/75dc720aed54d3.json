{"cell_type":{"1a8260af":"code","33af36bd":"code","cf278b4e":"code","dda6c44a":"code","a703dd50":"code","845ee76b":"code","713f36dc":"code","186283d8":"code","1e2e4335":"code","ffa003a4":"code","5538c283":"code","bc0a3fd9":"code","da67a360":"code","143d9197":"code","f55827e8":"code","a93383b0":"code","26baa8ee":"code","3221e7f4":"code","87abd904":"code","49e9885c":"code","e831f5a5":"code","873cc846":"code","16983d98":"code","96c130a4":"code","ec2d890a":"code","dfb1a2b3":"code","a4e531c8":"code","a2e21b9c":"code","de6bf0a5":"code","2372aaf5":"code","1b1c79bd":"code","c01310f0":"code","2fca644a":"code","aea29bd8":"code","9b821c17":"code","31970390":"code","533eb8dd":"code","397195a7":"code","aa2078e5":"code","c5c8af74":"code","442c8e32":"code","cbf12023":"code","426b977b":"code","ebd8562d":"code","74a8438b":"code","47c3af2d":"code","61307b06":"markdown","be604060":"markdown","6ac2afdb":"markdown","1a90856b":"markdown","e4b202a1":"markdown","91c139f3":"markdown","bd9488f2":"markdown","2a4bd79d":"markdown","f76282e3":"markdown","5027d1ce":"markdown","cffb09de":"markdown","90a4cf7d":"markdown","d14af921":"markdown","ae56c477":"markdown","1136c3af":"markdown","8b1d76e3":"markdown","f4645284":"markdown","6c84ab66":"markdown","dbfd4185":"markdown","1e5b016d":"markdown","edade8eb":"markdown"},"source":{"1a8260af":"#importing input files\nimport numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","33af36bd":"fake_df = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\nreal_df = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")","cf278b4e":"fake_df.head()","dda6c44a":"real_df.head()","a703dd50":"#pandas profiling to observe the distributions in the data\nimport pandas_profiling\nfake_report = pandas_profiling.ProfileReport(fake_df)","845ee76b":"fake_report","713f36dc":"fake_df.drop_duplicates(inplace = True)","186283d8":"real_report = pandas_profiling.ProfileReport(real_df)","1e2e4335":"real_report","ffa003a4":"real_df.drop_duplicates(inplace = True)","5538c283":"fake_df['label'] = 1\nreal_df['label'] = 0","bc0a3fd9":"#concat and shuffle real and fake dataset\ndata = pd.concat([fake_df,real_df],axis = 0)\ndata = data.sample(frac = 1)\ndata.reset_index(drop=True,inplace = True)\ndata.head()","da67a360":"import nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\n\nimport regex as re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.preprocessing import text\n\nfrom textblob import TextBlob, Word","143d9197":"subj = []\nfor sub in data['subject']:\n    sub = sub.lower()\n    sub = re.sub(\" \", '',sub)\n    sub = re.sub(\"-\",'',sub)\n    sub = re.sub(\"_\",'',sub)\n    subj.append(sub)\ndata['subject'] = subj","f55827e8":"# generate sentiment subjectivity, polarity \ndef sentiment_polarity(data,clean_col):\n    subjectivity = []\n    polarity = []\n    sense = []\n    for text in data[clean_col]:\n        subjectivity.append(TextBlob(text).sentiment.subjectivity)\n        pol = TextBlob(text).sentiment.polarity\n        polarity.append(pol)\n        if pol>0:\n            sense.append(1)\n        elif pol<0:\n            sense.append(-1)\n        else:\n            sense.append(0)\n    return subjectivity, polarity, sense\n\nsub,pol,sentiment = sentiment_polarity(data,'title')","a93383b0":"data['subjectivity'] = sub\ndata['polarity'] = pol\ndata['sentiment_polarity'] = sentiment","26baa8ee":"import plotly.express as px\nfig = px.box(data, x=\"sentiment_polarity\", y=\"subjectivity\",color=\"label\")\nfig.update_layout(plot_bgcolor='rgba(0,0,0,0)')\nfig.show()","3221e7f4":"# removing stopwords, punctuations and obtain stemmed and lemmatized form of data\nstopwords_en = set(stopwords.words('english'))\ndef clean_data(data,col):\n    stemmed_col = []\n    lemmatized_col = []\n    for text in data[col]:\n        sent = text.lower()\n        sent = re.sub(r'((http|https)\\:\\\/\\\/)?[a-zA-Z0-9\\.\\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\\/\\?\\:@\\-_=#])*', \n                    '', sent, flags=re.MULTILINE)\n        sent = re.sub(r'[^\\w\\s]', '', sent) \n        ps = PorterStemmer()\n        stemmed_text = \" \".join([ps.stem(word) for word in sent.split() if word not in stopwords_en and not word.isdigit()])\n        stemmed_col.append(stemmed_text)\n        \n        sent = TextBlob(sent)\n        lemmatized_text = \" \".join([word.lemmatize() for word in sent.words if word not in stopwords_en and not word.isdigit()])\n        lemmatized_col.append(lemmatized_text)\n        \n    return stemmed_col, lemmatized_col","87abd904":"stemmed_col, lemmatized_col = clean_data(data,'title')","49e9885c":"data['stemmed_title'] = stemmed_col\ndata['lemmatized_title'] = lemmatized_col","e831f5a5":"data.tail()","873cc846":"# create vocabulary of words used in titles\nimport wordcloud\nimport matplotlib.pyplot as plt\n\ndef gen_vocab(data,col,label):\n    vocab =  {}\n    data = data.loc[data['label'] == label]\n    for text in data[col]:\n      sen = text.split()\n      for word in sen:\n        try:\n          vocab[word] += 1\n        except KeyError:\n          vocab[word] = 1\n    vocab = dict(sorted(vocab.items(), key=lambda item: item[1]))\n    vals = \" \".join(w for w in vocab.keys())\n    w = wordcloud.WordCloud().generate(vals)\n    plt.imshow(w, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()\n    return vocab","16983d98":"vocab_fake_title_unclean = gen_vocab(data,'title',1)","96c130a4":"vocab_real_title_unclean = gen_vocab(data,'title',0)","ec2d890a":"vocab_fake_title_clean = gen_vocab(data,'lemmatized_title',1)","dfb1a2b3":"vocab_real_title_clean = gen_vocab(data,'lemmatized_title',0)","a4e531c8":"def rare_words(vocab):\n    rare_words = []\n    for key,value in vocab.items():\n      if value<=5:\n        rare_words.append(key)\n    return rare_words","a2e21b9c":"rare_words_fake_title = rare_words(vocab_fake_title_clean)\nrare_words_real_title = rare_words(vocab_real_title_clean)","de6bf0a5":"## word count, sentence count, average lengths\ndata['word_count_body'] = data[\"text\"].apply(lambda x: len(str(x).split(\" \")))\ndata['char_count_body'] = data[\"text\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndata['sentence_count_body'] = data[\"text\"].apply(lambda x: len(str(x).split(\".\")))\ndata['avg_word_length_body'] = data['char_count_body'] \/ data['word_count_body']\ndata['avg_sentence_lenght_body'] = data['word_count_body'] \/ data['sentence_count_body']\ndata.head()","2372aaf5":"## plot word counts of fake and real news data\nfig = px.box(data, x=\"sentiment_polarity\", y=\"word_count_body\",color=\"label\")\nfig.update_layout(plot_bgcolor='rgba(0,0,0,0)')\nfig.show()","1b1c79bd":"##stemming and lemmatizing body text\n## This is a very time consuming method, any optimized solution is most welcome in the comments below!\nstemmed_col , lemmatized_col = clean_data(data,'text')","c01310f0":"data['stemmed_body'] = stemmed_col\ndata['lemmatized_body'] = lemmatized_col","2fca644a":"## TF-IDF vectorizer on body text to obtain bi-grams and tri- grams of the dataset.\nvectorizer = TfidfVectorizer(ngram_range=(2, 3),max_features=20000,smooth_idf=True)\ntfidif_matrix = vectorizer.fit_transform(data['lemmatized_body'])\nprint(tfidif_matrix.shape)\nprint(vectorizer.get_feature_names()[:50])","aea29bd8":"## Load word vectors\nembeddings_index = dict()\nf = open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt')\nfor line in f:\n  values = line.split()\n  word = values[0]\n  try:\n    coefs = np.asarray(values[1:], dtype='float32')\n  except ValueError:\n    pass\n  embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","9b821c17":"## obtain sequnece of tokens from data\ntokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(data['lemmatized_body'])\nbody_sequence = tokenizer.texts_to_sequences(data['lemmatized_body'])\nbody_sequence = sequence.pad_sequences(body_sequence,maxlen=200)","31970390":"## This generates a huge dictionary, hence I have commented it out, you could have a look if you want!\n##print(tokenizer.word_counts)","533eb8dd":"## obtain word vectors for tokens from our dataset\ntokens = len(tokenizer.word_index) + 2\nembedding_matrix = np.zeros((tokens, 200))\ncount = 0\nunknown = []\nfor word, i in tokenizer.word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector\n  else:\n    unknown.append(word)\n    count += 1","397195a7":"print(f'Percentage of obtained word vectors for our dataset : {100*(1-(count\/len(tokenizer.word_index)))} %')","aa2078e5":"#!pip install tokenizers","c5c8af74":"## Token generation by the BERT tokeniser\nfrom tokenizers import BertWordPieceTokenizer\ntokenizer = BertWordPieceTokenizer(\"\/kaggle\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")\n\ndef bert_tokenizer(data,col):\n    encoded_output = []\n    for i in data[col]:\n        encoded = tokenizer.encode(i)\n        encoded_output.append(encoded)\n    return encoded_output\ndata['encoded_body'] = bert_tokenizer(data,'lemmatized_body')","442c8e32":"data['encoded_body'][:2]","cbf12023":"encoded_sent = data['encoded_body'][0]\nencoded_sent.offsets\nencoded_sent.tokens","426b977b":"import transformers\nfrom transformers import pipeline","ebd8562d":"## Obtain sentiment Analysis of the news body\nsentimentAnalysis = pipeline(\"sentiment-analysis\")\n\n## The following loop isn't an optimised way to \n## obtain analysis of a large amount of data hence \n## i have commented it out.\n# sentiment_analysis = []\n# for body in data['lemmatized_title']:\n#     sentiment = sentimentAnalysis(body)\n#     sentiment_analysis.append(sentiment)\n# print (sentiment_analysis[:5])\n\nbody = data['lemmatized_body'][0]\nprint(sentimentAnalysis(body))","74a8438b":"## Name entity recognizers \nnamedEntityRecgnition = pipeline(\"ner\")\n\n## The following loop isn't an optimised way to \n## obtain analysis of a large amount of data hence \n## i have commented it out.\n# ner_body = []\n# for body in data['lemmatized_body']:\n#     neR = namedEntityRecgnition(body)\n#     ner_body.append(neR)\n# print (sentiment_analysis[:5])\n\nprint(namedEntityRecgnition(body))","47c3af2d":"## Extract summary of the data\nsummarizer = pipeline('summarization')\n## The following loop isn't an optimised way to \n## obtain analysis of a large amount of data hence \n## i have commented it out.\n# summary = []\n# for body in data['lemmatized_body']:\n#     sum = summarizer(body)\n#     summary.append(sum)\n# print (summary[:5])\n\nprint(summarizer(body))","61307b06":"# Feature Engineering on News Title:","be604060":"I have used the GloVe model 200 dimensional embeddings, to generate vectors for this dataset.","6ac2afdb":"The title of a news article is very useful to detect fake news, few of the reasons such as due to their style of casing (first letter of every word capitalised), polarity of the language, use of informal or obnoxious style of writing. Hence, the title can be used to generate many useful insights about the news articles, some of which I have explored below:","1a90856b":"The HuggingFace\ud83e\udd17  library provides a range of state of art transformer models, pre-processing methods for natural language processsing, understanding, text generation, intent classification,question answering and much more. <br>You can read more about these models on their website: [https:\/\/huggingface.co\/transformers\/index.html](http:\/\/)<br>In this notebook, I am exploring their tokenizers, transformer pipelines, processors etc. to learn and tune to get the most optimised results. My work is still under progress, so stay tuned for more techniques!","e4b202a1":"Subjectivity of the title is more for fake news, irrespective of their sentiment polarity, it is so, because probably the writers of these fake news try to include a lot of weird facts to make it a clickbait for readers. Real news, which generally are neutral reports try making it as concise as possible!","91c139f3":"This embedding matrix can now be used as weights for an embedding layer to train a deep learning model!:)","bd9488f2":"Lemmatized titles highlight the common verbs occuring in the titles. For the fake titles, We see words like politicize, shatter, berate, incriminate, 6 year old(wonder why!?)etc. We can make somewhat sense the kind of language fake news makers use.","2a4bd79d":"Now we can use these list of words to remove these words from the titles, with the help of regex or replace.","f76282e3":"Stemming and Lemmatizing a chunk of data, makes it easier for the machine to identify numerous words from their root form, which is essential for semantic analysis of the data, generating tokens and embeddings for words in the doc, which are very useful to perform deep learning on the data. <br>\nRaw form of data should also be cleaned to remove stopwords, punctuations and at times, if nessacary, rare words to prevent crowding by regular pronouns, models, verbs and spell errors which do not really contribute to understanding the subject of the sentence.<br>\nBut, a point to be noted, for generating embeddings (for a sequence of words in a sentence) which will further be used for training deep learning models, removal of stop words is not a good practice because doing so breaks the flow of the language which is nessaccary for the deep learning model to learn.","5027d1ce":"The collection of words from the unfiltered titles of real news data. We can see that they mostly focus on countries in relation with the United States, such as Russia, Saudi, UK,other than the common occurence of the word Trump. We can also see words like Anti, Ex and Post, probably focusing on the relations with the country or the politicians maybe. They don't touch social issues much compared to fake news.","cffb09de":"We can obtain embedding vectors from pre-trained embedding models, which can be used to create an embedding matrix, which is a good parameter to feed into the embedding layer of a sequential deep learning model. Embeddings are useful to study similarities of different words, hence obtain corresponding vectors which are essential for studying sequences and sequence classifcation by deep learning models.","90a4cf7d":"The above generated features of the news body can be used for analysis and gaining useful insights. So far we have generated a large number of features, which can be used for modelling and predictions. Stay tuned for more ideas on feature engineering in NLP!","d14af921":"We can see that fake news have significantly more number of words than real news. Now that's a good discovery, though not very dependable!","ae56c477":"HuggingFace transformers provide a set of pipelines which are pre-trained to undergo complex functions such as sentiment analysis, name entity recognizers, summaries and even answer questions! to name a few. I have explored a few of these piplines to gain insight and features from our dataset. <br>\nSince these pipelines take up a lot of RAM, it is advised to use them as and when required only!","1136c3af":"For the real news titles we can spot words like endanger, scrutinize, swear, refute and nouns like william, chahed(politician), barca etc.","8b1d76e3":"Textual data, is said to be the most available and the most unorganised form of data, and most organisations fail to manipulate its raw form, hence this huge resource for very useful insights gets wasted! In this notebook, I have explored various preprocessing techniques, ranging from basic TF-IDF matrix to tokenizers provided by the state-of-art NLP models- \nGoogle's BERT(Bidirectional Encoder Representations from Transformers) on the data to generate useful features.","f4645284":"# Feature Engineering on News Body:","6c84ab66":"We can use the vocabularies we generated to filter out rare words from texts, which is completely optional!","dbfd4185":"# Work Under Progress!\n\nPlease Leave an upvote if you like my notebook! Any suggestions or corrections are welcome in the comments below, thanks!","1e5b016d":"A collection of most frequent words from the unfiltered titles of fake data. There is Trump,Obama,Hillary, and also we see words like Gun, Cop, Black, Muslim, Racist, American, Right, Lie, which shows they always attack sensitve and delicate social and political issues to attract readers and influence them!","edade8eb":"News body is a big chunk of insightful information, which can be used to summarise the content of the article, generate name entity recognizers which can be used to identify imortant information or terms in the article, answer questions about the news, ofcourse with the help of new state of art models. \n\nFor statistical analysis, we can count the number of words, sentences, approx lengths, to see if there is any significant difference between the real and fake news on the basis of these features. We can also obtain n-grams with the help of tf-idf vectorizers, which can be further used for semantic analysis on the data. I have explored a few of these techniques to obtain useful features!"}}