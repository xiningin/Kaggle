{"cell_type":{"f5216eef":"code","b3db97ce":"code","75be22f5":"code","1982935c":"code","24c6fbb4":"code","fe9aeaa3":"code","6e4df3ee":"code","7c07086f":"code","8caac229":"code","da808fea":"code","24559420":"code","83039878":"code","6a2a0a1e":"code","6eea19c1":"code","f6f18072":"code","47444e46":"code","fa1bd04a":"markdown","ddb650f0":"markdown","5ada77ad":"markdown","f43627c7":"markdown","77021739":"markdown","1af97966":"markdown","a78eca15":"markdown","a26feae8":"markdown","57e36b43":"markdown","bfe45062":"markdown","44dd4bff":"markdown","e904a990":"markdown","ac507f74":"markdown","ecf4e331":"markdown","8d47ba4b":"markdown","47576a75":"markdown","5baec92a":"markdown","8606d97a":"markdown"},"source":{"f5216eef":"import json\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nnp.random.seed(123)\nsns.set_style('darkgrid')\npd.set_option('display.max_colwidth', None)","b3db97ce":"# -- Uncomment and run the shell commands below to download and unzip\n\n# # Create a data folder (if it does not exist)\n# !mkdir -p .\/data \n# # Download the competition data as a zip\n# !kaggle competitions download -c moscow-housing-tdt4173 -p .\/data\/ \n# # Unzip the competition data (alternatively use another unzipping software you have installed)\n# !unzip -o -d data\/ data\/moscow-housing-tdt4173.zip\n\n# -- Here we just create a symlink because this code is being run on a kaggle \n#    machine where the data is already downloaded to \/kaggle\/input\/...\n\n# Note: Don't run this command if you run this notebook locally\n!ln -s \/kaggle\/input\/moscow-housing-tdt4173 .\/data","75be22f5":"!ls .\/data | sort","1982935c":"def describe_column(meta):\n    \"\"\"\n    Utility function for describing a dataset column (see below for usage)\n    \"\"\"\n    def f(x):\n        d = pd.Series(name=x.name, dtype=object)\n        m = next(m for m in meta if m['name'] == x.name)\n        d['Type'] = m['type']\n        d['#NaN'] = x.isna().sum()\n        d['Description'] = m['desc']\n        if m['type'] == 'categorical':\n            counts = x.dropna().map(dict(enumerate(m['cats']))).value_counts().sort_index()\n            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n        elif m['type'] == 'real' or m['type'] == 'integer':\n            stats = x.dropna().agg(['mean', 'std', 'min', 'max'])\n            d['Statistics'] = ', '.join(f'{s}={v :.1f}' for s, v in stats.items())\n        elif m['type'] == 'boolean':\n            counts = x.dropna().astype(bool).value_counts().sort_index()\n            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n        else:\n            d['Statistics'] = f'#unique={x.nunique()}'\n        return d\n    return f\n\ndef describe_data(data, meta):\n    desc = data.apply(describe_column(meta)).T\n    desc = desc.style.set_properties(**{'text-align': 'left'})\n    desc = desc.set_table_styles([ dict(selector='th', props=[('text-align', 'left')])])\n    return desc ","24c6fbb4":"apartments = pd.read_csv('data\/apartments_train.csv')\nprint(f'Loaded {len(apartments)} apartments')\nwith open('data\/apartments_meta.json') as f: \n    apartments_meta = json.load(f)\ndescribe_data(apartments, apartments_meta)","fe9aeaa3":"buildings = pd.read_csv('data\/buildings_train.csv')\nprint(f'Loaded {len(buildings)} buildings')\nwith open('data\/buildings_meta.json') as f: \n    buildings_meta = json.load(f)\nbuildings.head()\ndescribe_data(buildings, buildings_meta)","6e4df3ee":"print(f'All apartments have an associated building: {apartments.building_id.isin(buildings.id).all()}')\ndata = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\ndata.head()","7c07086f":"def root_mean_squared_log_error(y_true, y_pred):\n    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n    assert (y_true >= 0).all() \n    assert (y_pred >= 0).all()\n    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n    return np.mean(log_error ** 2) ** 0.5","8caac229":"apartments_test = pd.read_csv('data\/apartments_test.csv')\nbuildings_test = pd.read_csv('data\/buildings_test.csv')\nprint(f'All test apartments have an associated building: {apartments_test.building_id.isin(buildings_test.id).all()}')\ndata_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\nprint(f'Number of train samples: {len(data)}')\nprint(f'Number of test samples:  {len(data_test)}')\ndata_test.head()","da808fea":"fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2, dpi=100)\nprint(f'Number of missing price entries in train data: {data.price.isna().sum()}')\nprint(f'Training set features with any missing value: {data.isna().any().sum()}\/{data.shape[1]}')\nprint(f'Testing set features with any missing value: {data_test.isna().any().sum()}\/{data_test.shape[1]}')\ndata.isna().mean().plot.bar(ax=ax1, title='Fraction of NaN values in the training set')\ndata_test.isna().mean().plot.bar(ax=ax2, title='Fraction of NaN values in the testing set');","24559420":"fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2, dpi=100)\nsns.histplot(data.price.rename('price \/ rubles'), ax=ax1)\nax1.set_title('Distribution of raw train set prices');\nsns.histplot(np.log10(data.price).rename('log10(price)'), ax=ax2)\nax2.set_title('Distribution of train set prices after log transform');","83039878":"def plot_map(data, ax=None, s=5, a=0.75, q_lo=0.0, q_hi=0.9, cmap='autumn', column='price', title='Moscow apartment price by location'):\n    data = data[['latitude', 'longitude', column]].sort_values(by=column, ascending=True)\n    backdrop = plt.imread('data\/moscow.png')\n    backdrop = np.einsum('hwc, c -> hw', backdrop, [0, 1, 0, 0]) ** 2\n    if ax is None:\n        plt.figure(figsize=(12, 8), dpi=100)\n        ax = plt.gca()\n    discrete = data[column].nunique() <= 20\n    if not discrete:\n        lo, hi = data[column].quantile([q_lo, q_hi])\n        hue_norm = plt.Normalize(lo, hi)\n        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(lo, hi))\n        sm.set_array([])\n    else:\n        hue_norm = None \n    ax.imshow(backdrop, alpha=0.5, extent=[37, 38, 55.5, 56], aspect='auto', cmap='bone', norm=plt.Normalize(0.0, 2))\n    sns.scatterplot(x='longitude', y='latitude', hue=data[column].tolist(), ax=ax, s=s, alpha=a, palette=cmap,linewidth=0, hue_norm=hue_norm, data=data)\n    ax.set_xlim(37, 38)    # min\/max longitude of image \n    ax.set_ylim(55.5, 56)  # min\/max latitude of image\n    if not discrete:\n        ax.legend().remove()\n        ax.figure.colorbar(sm)\n    ax.set_title(title)\n    return ax, hue_norm\n\nplot_map(data);","6a2a0a1e":"import sklearn.model_selection as model_selection\n\ndata_train, data_valid = model_selection.train_test_split(data, test_size=0.33, stratify=np.log(data.price).round())\nfig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, dpi=100)\nprint(f'Split dataset into {len(data_train)} training samples and {len(data_valid)} validation samples')\n\nsns.histplot(np.log10(data_train.price).rename('log10(price)'), ax=ax1);\nsns.histplot(np.log10(data_valid.price).rename('log10(price)'), ax=ax2);\nax1.set_title('Training set log prices'); ax2.set_title('Validation set log prices');","6eea19c1":"y_train = data_train.price\ny_valid = data_valid.price\n\nmean = y_train.mean()\ny_train_hat = np.full(len(y_train), mean)\ny_valid_hat = np.full(len(y_valid), mean)\n\nprint(f'Train rmsle: {root_mean_squared_log_error(y_true=y_train, y_pred=y_train_hat) :.4f}')\nprint(f'Valid rmsle: {root_mean_squared_log_error(y_true=y_valid, y_pred=y_valid_hat) :.4f}')","f6f18072":"import sklearn.tree as tree\n\nX_train = data_train[['latitude', 'longitude']]\ny_train = data_train.loc[X_train.index].price\nX_valid = data_valid[['latitude', 'longitude']]\ny_valid = data_valid.loc[X_valid.index].price\n\nmodel = tree.DecisionTreeRegressor().fit(X_train, y_train)\n\ny_train_hat = model.predict(X_train)\ny_valid_hat = model.predict(X_valid)\nprint(f'Train RMSLE: {root_mean_squared_log_error(y_true=y_train, y_pred=y_train_hat) :.4f}')\nprint(f'Valid RMSLE: {root_mean_squared_log_error(y_true=y_valid, y_pred=y_valid_hat) :.4f}')\n\nlats, lngs = np.meshgrid(np.linspace(55.5, 56, 50), np.linspace(37, 38, 50))\npreds = model.predict(np.stack([lats, lngs], axis=-1).reshape(-1, 2)).reshape(lats.shape)\nax, norm = plot_map(data, a=0.25)\nax.set_title(ax.get_title() + ' with model prediction overlay')\nax.imshow(preds.T[::-1, :], extent=(37, 38, 55.5, 56), alpha=0.3, aspect='auto', cmap='autumn', norm=norm);","47444e46":"# Fit model to the full dataset \nX_train = data[['latitude', 'longitude']]\ny_train = data['price']\nprint(f'Num nans in train {X_train.isna().any(axis=1).sum()}')\nmodel = tree.DecisionTreeRegressor(max_depth=20).fit(X_train, y_train)\n\n# Generate predictions for test set \nX_test = data_test[['latitude', 'longitude']]\nX_test_nan = X_test.isna().any(axis=1)\nprint(f'Num nans in test: {X_test_nan.sum()}')\ny_test_hat = model.predict(X_test[~X_test_nan])\n\n# Construct submission dataframe\nsubmission = pd.DataFrame()\nsubmission['id'] = data_test.id\nsubmission.loc[~X_test_nan, 'price_prediction'] = y_test_hat # Predict on non-nan entries\nsubmission['price_prediction'].fillna(y_train.mean(), inplace=True) # Fill missing entries with mean predictor\nprint(f'Generated {len(submission)} predictions')\n\n# Export submission to csv with headers\nsubmission.to_csv('sample_submission.csv', index=False)\n\n# Look at submitted csv\nprint('\\nLine count of submission')\n!wc -l sample_submission.csv\n\nprint('\\nFirst 5 rows of submission')\n!head -n 5 sample_submission.csv","fa1bd04a":"### Directory structure\n\nThe dataset comes as 6 files plus a street map of moscow. We will have a closer look at the data later, but a brief rundown of the contents is given below.\n\n- `apartments_(train|test).csv` contains information about specific apartments. Each row in this table corresponds to one datapoint that you will make a prediction for. The train file also contains a column for the listed apartment price, i.e. the ground truth to the variable you will be predicting.\n- `buildings_(train|test).csv` contains suplementary information about the building each apartment is located in. \n- `(apartments|buildings)_meta.json` contains metadata about the columns found in the apartment and building tables, including a brief description, the datatype, and categories (where applicable).","ddb650f0":"### Geographical Considerations\n\nHere we will have a rudimentary look at correlations between features and the target variable. Specifically, we will investigate the relationship between an apartment's location and its price. Anyone who's ever been out apartment shopping knows that proximity to popular facilities often plays a huge role in what prices you should expect. We visualize the relationship by creating a scatterplot between the latitude and longitude coordinate features and colorcode the dots based on the apartment price. To further contextualize the data, we also add a backdrop of Moscow exported from [Open Street Map](https:\/\/www.openstreetmap.org\/#map=10\/55.7515\/37.4998).","5ada77ad":"## 1: Data\n\nWe begin this tutorial by showing how you can download the material and giving a description of the data you will be working with.\n\n### Download (with Kaggle API)\n\nThe data can be download through the Kaggle API. If you've obtained the data in some other fashion (e.g. by downloading it through the web page) then you can skip this step. Just ensure that all the data is located in a sub-directory of the directory this notebook is located in called `data`. \n\nTo download the data with the Kaggle API, you'll need the _API client_ and an _API key_. Full documentation on how to use it is available from [the official github repository](https:\/\/github.com\/Kaggle\/kaggle-api), but an abriged version is given below.\n\nThe client can be installed through pip\/anaconda.\n\n```bash\npip install kaggle\nconda install -c conda-forge kaggle\n```\n\nThe API key can be generated from the kaggle website. Go to `Your Profile` (in sidebar accessible from icon in the top right corner), navigate to the `Account` tab, scroll down to the `API` section, and click the `Create New API Token` button. This should generate an API token and start a download of a json file called `kaggle.json`. \n\nOnce the download is complete, you must move the file to a special kaggle folder.\n\n\n```bash\nmkdir -p ~\/.kaggle   # Creates a '.kaggle' directory in your homefolder if it doesn't exist\nmv ~\/Downloads\/kaggle.json ~\/.kaggle   # Moves 'kaggle.json' (API key) to ~\/.kaggle folder\nchmod 600 ~\/.kaggle\/kaggle.json  # Sets read\/write access to API key file to owner only\n```\n\n**Note**: on Windows, the default location for the kaggle home folder might look something like `C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json`.\n\nOnce the API is set up you can download the competition data.","f43627c7":"## 3: Basic Exploratory Data Analysis (EDA)\n\nIn the sections below, we provide brief analyses of some of the more pertient aspects of the data. Doing good EDA is an artform in itself and we expect you to go beyond what is provided here.","77021739":"### Regression Target\n\nNext we examine the target variable; the listed apartment price. As previously mentioned, there's a big spread in prices. Plotting a histogram of the raw value yields a sharp peak (relatively) close to zero with a long tail stretching past 2.5 billion rubles. A log transform makes it a bit easier to tell what is going on. We use a `log10` transform so that the number on the x-axis correspond to the number of digits in the price. A distinct mode can be observed around 7.0 (10 million rubles) with the distribution tapering off quickly to the left and more slowly to the right.","1af97966":"### A very simple Baseline\n\nIt is useful to establish where the floor is, performance-wise, before starting to hypothesising about what the best features and models should look like. Here we create a simple mean predictor and obtain an RMSLE of around 1.02, both in- and out-of-sample. Any model that is conditioned on features should perform better than this benchmark.\n\n**Challenge**: Can you make a different \"constant value predictor\" that performs better than the sample mean with respect the the RMSLE metric used? The solution to this problem could be useful when defining the objective\/loss of more complex models.","a78eca15":"### Sample Submission\n\nLastly, we'll demonstrate how to make a submission. We begin by training another Decision Tree Regressor just like in the section above, except this time over the entire training set. \nThe trained model is then used to generate predictions for the test set. Note that we invoke a little bit of trickery here. The test set is missing latitude\/longitude coordinates for two datapoints, so we take the easy route and only generate predictions for the remaining 9935 entries. The two remaining predictions are simply filled with the mean price.\nIn the end, we export the predictions to a .csv file. Its format is pretty simple; one row for each datapoint and each row contains the id of the predicted datapoint along with the predicted price. \n\n**Important**: make sure that the id column of your submission file matches the id of `apartments_test.csv` and that the header used is `id,price_prediction`.\n\nThe exported .csv file can then be submitted to kaggle and bundled with your final delivery on blackboard.","a26feae8":"# Moscow Housing Dataset\n\nIn this challenge you will predict the prices of apartments in Moscow. \n\nReal estate is a popular investment vehicle and can be pretty lucurative. Whether you're an individual looking for a place to call home or an investor looking for a profitable opportunity, being able to appraise assets accurately - or at least better than the next guy - can be of huge value.\n\nWe have procured a dataset consisting of information about over 33,000 apartments in Moscow. It contains around 30 variables that encode data such as location, facilities, and building information. Your task is to do analysis, feature engineering, and ultimately create models that can reliably predict the listed price of both low- and high-end apartments.","57e36b43":"### Missing Data\n\nWe begin by taking a closer look at the missing values. As can be seen in the plots below, about half of the features have almost no missing values, but the remainig half is a mixed bag ranging from around 20% missing values to more than 70%. For [various reasons](https:\/\/en.wikipedia.org\/wiki\/Missing_data#Types), this is a common problem setting in machine learning that can be tackled in several ways. Perhaps the easiest way to go about it is to simply ignore all features with missing values. However, that naturally comes with the risk of missing out on useful information that could benefit the model. You can also drop rows with missing values from the training set, but remember that you still have to make predictions for all the rows in the test set.\n\nThe process of filling in missing data entries is referred to as _imputation_. A simple way to do this for real-valued data is to replace missing values by the sample mean (or median) of the data that is present. For categorical features, you can create an extra category for \"missing\", and boolean features can be converted to a categorical True\/False\/DontKnow. However, there are also more sophisticated approaches out there that you might want to look into. For more information on the topic, [Scikit Learn's documentation](https:\/\/scikit-learn.org\/stable\/modules\/impute.html) is a good place to start.","bfe45062":"### Train-Test Split\n\nWe have already made a train\/test split that will be used to evaluate your submission. Concretely, you are given:\n- ~23k training samples **with** price data that you can use to select and fit models\n- ~10k testing samples **without** price data that we will use to evaluate your models\n\nThe split have been desiged so that you can expect roughly the same distribution of data during evaluation. Specifically, we have made sure to stratify it with respect to the price range and location of apartments. In the cell below, we load the test split in an equivalent manner to how the training data was loaded.","44dd4bff":"### Appartments\n\nWe first load the training split of the apartment data. This file contains information that is specific to each apartment. As previously stated, each row in this table correspond to a unique datapoint and the price column is the variable you will be trying to predict in this challenge. We use the `describe_data` function decleared in the cell above to map the apartment data to its metadata and generate summary statistics. Note that several of the features have a substantial amount of missing values (`#NaN`). We will get back to this later in this notebook.","e904a990":"## 4: Getting Started\n\nTo conclude this demo, we will give quick rundown of how you can split the dataset to do your own evaluations, train simple models, and create a submission with predictions that can be uploaded to Kaggle\/Blackboard.\n\n\n### Validation Split\n\nEven though you can get feedback on how well you're doing by uploading submissions to Kaggle, it a lot quicker to do it yourself. Creating an evaluation split allows you to estimate the out-of-sample performance of your models and is particularly useful when (e.g.) testing out a new preprocessing pipeline or optimizing hyperparameters. In the following cell, we split the training data into two subpartitions with 67% data in the training set and 33% data in the validation set. \n\nNote that we stratify based on (rounded) log price. This ensures that we get roughly the same price distributions in both the training and validation set (see figures).","ac507f74":"### Uploading restults\n\nOnce a submission csv has been created it can be uploaded to Kaggle. You can upload sumbissions manually through the competition web page ([as explained here](https:\/\/www.kaggle.com\/docs\/competitions#submitting-predictions)). Alternatively, you can use the Kaggle API and do it from the terminal with the following command template:\n\n```bash\nkaggle competitions submit moscow-housing-tdt4173 -f <filepath> -m \"<message>\"\n```\n\nWhere `<filepath>` in this case would be `.\/sample_submission.csv` and `<message>` is your own comment for the submission.\n\nThe submission created here was actually used as the sample submission in the Kaggle competition. You should be able to find it in the leaderboard under the team name `sample_submission.csv`. It obtained a **public**$^1$ score of 0.61666. Note that this is quite a bit higher than the 0.4326 validation set score we obtained above, but it is actually not that surprising given the way this model work (hint: analyze building IDs across train\/test).\n\n$^1$) The kaggle leaderboard will display your public score. For this competition, this constitutes RMLSE calcluated over a sub-sample of the test set (50% of the data). Your score for the remaining test set datapoints will remain hidden until the end of the competition. The reason for this is the same reason that you typically want to do a train\/valid\/test split in machine learning. If complete feedback on all the test samples were constantly available, then you could more easily overfit you models (in terms of hyperparamters, ensemble composition, etc).","ecf4e331":"### Spatial Decision Tree\n\nNext we will try to make a slightly more sophisticated model. The EDA above suggests that you can make a nontrivial guess about prices based on geographical location alone. We, therefore, train a model to make price predictions conditioned on the latitude and longitude of the listed apartment. We use a [Regression Tree](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html) (Decision Tree for real valued output) and fit it with default hyperparameters.\n\nIt can be observed that this leads to a substantial improvement in performance. Although we overfit a bit, the validation RMSLE is more than halved compared to the mean-predicting baseline. ","8d47ba4b":"### Evaluation Metric\n\nIt is common to evaluate regression problems according to some deviation measure of the error (difference) between the predictions and the ground truth values. Typical choices are Mean Squared Error (MSE) and its square root, the Root Mean Squared Error (RMSE). \n\nHowever, both of these measures are quite sensitive to extreme values and work best if the typical scale of prediction errors are consistent across the dataset. This is not likely to be the case here because the price variable ranges from around 1 million rubles to over 2 billion rubles. This means that a, say 10%, prediction error would matter a lot more if it is for one of the expensive apartments than if it is for one of the cheaper ones. Consequently, we will use a variation that takes a log transform of the target variable before computing prediction errors.\n\n**TL;DR**: submissions for this problem will be evaluated according to the `Root Mean Squared Log Error` (RMSLE):\n\n- $\\text{RMSLE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}{(\\log(1 + \\hat{y_i}) - \\log(1 + y_i))^2}}$\n\nIn the equation above, ${y_i}$ corresponds to the ground truth for datapoint $i$, $\\hat{y_i}$ corresponds to the predicted value for datapoint $i$, and $n$ denotes the total number of datapoints (size of $y$, $\\hat{y}$). See the cell below for an implementation.","47576a75":"## 2: Objective\n\nYour objective is to predict the value of the `price` column based on all the other columns. You are free to approach this regression problem with all the data science tricks you know or learn about during the project, just make sure that your process is well-documented and explained in your deliverables.","5baec92a":"## Combined\n\nMost datascience tools and pipelines for tabular data assumes a single table (sometimes referred to as a design matrix) to work with. To supplement the apartment datapoints with building information, we map building rows to apartment rows based on the `apartment.building_id == building.id` relationship with the `pd.merge` function. We use a `left` join to preserve all the apartment rows, while possibly duplicating building rows that correspond to more than one apartment.","8606d97a":"### Buildings\n\nNext we load the training split of the building data. In it, you will find additional building-level information to supplement the apartment data. This includes its physical location, its current state and construction details, as well as shared facilities such as elevators and parking. This file is a bit smaller than the apartment file because several apartments can map to the same building. As we will see in the next section, you can use the `id` column to map buildings to their respective apartments."}}