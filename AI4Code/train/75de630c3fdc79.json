{"cell_type":{"3ce15fb9":"code","082361aa":"code","da9333bf":"code","b7491769":"code","b2f8c227":"code","bc2a2c38":"code","c06f764c":"code","3b9eab12":"code","7ab62b2f":"code","a3ddc3f6":"code","63a96ce0":"code","67087b47":"code","5302ea72":"code","e8a15bde":"code","f0b3215e":"code","28b2a3b0":"code","0d60e00a":"code","43085fa9":"code","c32ddd8a":"code","03e57cd4":"code","b834cf86":"code","f778c0c8":"markdown","c5ad4847":"markdown","8f1331da":"markdown"},"source":{"3ce15fb9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\nfrom tensorflow.keras.callbacks import Callback\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import BatchNormalization\nimport math\nfrom tensorflow.keras import optimizers\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","082361aa":"train = pd.read_csv(\"\/kaggle\/input\/kaggle-club-stockpriceprediction\/train.csv\")\ntrain.head()","da9333bf":"sample = pd.read_csv(\"\/kaggle\/input\/kaggle-club-stockpriceprediction\/sample_submission.csv\")\nsample.head()","b7491769":"train.tail(10)","b2f8c227":"train_y = train[(train[\"date\"] == \"2020-08-07\")|(train[\"date\"] == \"2020-07-31\")].groupby(\"name\")[\"end\"].diff().dropna().map(lambda x: 1 if x >0 else 0)\ntest_y = train[(train[\"date\"] == \"2020-08-14\")|(train[\"date\"] == \"2020-08-07\")].groupby(\"name\")[\"end\"].diff().dropna().map(lambda x: 1 if x >0 else 0)","bc2a2c38":"train_X = train[train[\"date\"] < \"2020-07-31\"].iloc[:,1:]\ntest_X = train[(train[\"date\"] < \"2020-08-07\")&(train[\"date\"] > \"2018-01-12\")].iloc[:,1:]\nsub_X = train[(train[\"date\"] < \"2020-08-14\")&(train[\"date\"] > \"2018-01-17\")].iloc[:,1:]","c06f764c":"le = LabelEncoder()\nle = le.fit(train_X['market'])\ntrain_X['market'] = le.transform(train_X['market'])\ntest_X['market'] = le.transform(test_X['market'])\nsub_X['market'] = le.transform(sub_X['market'])","3b9eab12":"train_X","7ab62b2f":"using_col = ['name', 'open', 'high', 'low', 'end', 'volume', 'end_fixed','market','open_diff', 'high_diff', 'low_diff', 'end_diff', 'volume_diff', 'end_fixed_diff']\ndiff_col = ['open', 'high', 'low', 'end', 'volume', 'end_fixed']\ndiff_col_new = ['open_diff', 'high_diff', 'low_diff', 'end_diff', 'volume_diff', 'end_fixed_diff']","a3ddc3f6":"train_X_diff = train_X.groupby(\"name\")[diff_col].diff()\ntrain_X_diff.columns = diff_col_new\ntest_X_diff = test_X.groupby(\"name\")[diff_col].diff()\ntest_X_diff.columns = diff_col_new\nsub_X_diff = sub_X.groupby(\"name\")[diff_col].diff()\nsub_X_diff.columns = diff_col_new","63a96ce0":"train_X = pd.concat([train_X,train_X_diff],axis=1).dropna()\ntest_X = pd.concat([test_X,test_X_diff],axis=1).dropna()\nsub_X = pd.concat([sub_X,sub_X_diff],axis=1).dropna()","67087b47":"train_X","5302ea72":"len(train_X[\"date\"].unique())","e8a15bde":"train_X = train_X[using_col].values.reshape([-1,len(train_X[\"date\"].unique()),14])\ntest_X = test_X[using_col].values.reshape([-1,len(test_X[\"date\"].unique()),14])\nsub_X = sub_X[using_col].values.reshape([-1,len(sub_X[\"date\"].unique()),14])","f0b3215e":"sv = ModelCheckpoint(\n        'model.h5', monitor='val_loss', verbose=2, save_best_only=True,\n        save_weights_only=True, mode='max', save_freq='epoch')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=10, min_lr=0.00001)","28b2a3b0":"model = Sequential()\nmodel.add(Dense(128,input_shape=(train_X.shape[1:]),activation=\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dense(128,input_shape=(train_X.shape[1:]),activation=\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Dense(128,input_shape=(train_X.shape[1:]),activation=\"relu\"))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(axis=-1))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))","0d60e00a":"model.compile(optimizer=\"adam\", loss='binary_crossentropy',metrics=[\"AUC\"])\nmodel.summary()","43085fa9":"train_X = train_X.astype(np.float64)\ntrain_y = train_y.astype(np.float64)\nsub_X = sub_X.astype(np.float64)\ntest_X = test_X.astype(np.float64)\ntest_y = test_y.astype(np.float64)","c32ddd8a":"history = model.fit(train_X, train_y,\n                    epochs= 10,\n                    validation_data=(test_X,test_y),\n                    batch_size=128,callbacks=[sv,reduce_lr])","03e57cd4":"roc_auc_score(test_y,model.predict(test_X).reshape(-1))","b834cf86":"#predict\nsample[\"target\"] = model.predict(sub_X).reshape(-1)\nsample.to_csv(\"owari.csv\",index=False)","f778c0c8":"\u5b66\u7fd2\u3059\u308b\u305f\u3081\u306b\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\u305e\uff01\n8\u670814\u65e5\u306e\u682a\u4fa1\u30928\u67087\u307e\u3067\u306e\u30c7\u30fc\u30bf\u3067\u4e88\u6e2c\u3059\u308b\u30e2\u30c7\u30eb\u3092\u4f5c\u308d\u3046\u3002\n\u307e\u305a\u306f8\/14\u306e8\/7\u306e\u7d42\u5024\u306e\u5dee\u5206\u3092\u53d6\u308d\u3046\uff01","c5ad4847":"\u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4f5c\u308d\u3046","8f1331da":"\u30c7\u30fc\u30bf\u3092\u898b\u3066\u307f\u3088\u3046"}}