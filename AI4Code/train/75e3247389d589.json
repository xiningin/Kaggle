{"cell_type":{"d70cdc38":"code","a0641244":"code","d3959327":"code","0042be40":"code","0b1d6431":"code","d3f1ccf2":"markdown","43d7c002":"markdown","becc7c52":"markdown","129e1e14":"markdown","bc3d3b53":"markdown"},"source":{"d70cdc38":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nimport re","a0641244":"##preprocess the texts using nltk\ndef text_preprocess(text):\n    text = str(text)\n    token_texts = nltk.word_tokenize(text,language = 'english')\n    st = SnowballStemmer('english', ignore_stopwords = False)\n    english_stopwords = stopwords.words(\"english\")\n    token_stem_stopword=[]\n\t\n    ##remove punctuation and stopwords do the stemming\n    for each in token_texts:\n        if each not in english_stopwords and each.isalpha():\n\t        token_stem_stopword.append(st.stem(each))\n\t\t    \n    new_texts = ' '.join(token_stem_stopword)\n    \n    ##remove url in the texts\n    new_texts = re.sub(r'https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+','',new_texts)\n    #new_texts = re.sub(r'[a-z]*[:.]+\\S+','',new_texts)\n    \n    return new_texts\n\n#a=text_preprocess('forest fire http:\/\/t.co\/Y8IcF89H6w http:\/\/t.co\/t9MSnZV1Kb forest fire')\n#print(a)\n\n\n##preprocess the keyword using nltk\ndef text_preprocess2(text):\n    \n    if text is not np.nan:\n        text = str(text)\n        token_texts = text.split('%20')\n        st = SnowballStemmer('english', ignore_stopwords = False)\n        #st = PorterStemmer()\n        english_stopwords = stopwords.words(\"english\")\n        token_stem_stopword=[]\n        \n        #remove punctuation and stopwords do the stemming\n        for each in token_texts:\n            if each not in english_stopwords:\n                token_stem_stopword.append(st.stem(each))\n        \n        ##bomber will not be stemmed using SnowballStemmer so that add it manually\n        token_stem_stopword_2 = ['bomb' if x=='bomber' else x for x in token_stem_stopword]\n        new_texts = ' '.join(token_stem_stopword_2)\n\n        return new_texts   \n    \n    else:\n        return np.nan\n\n\n#a=text_preprocess2('suicide%20bomber')\n#print(a)\n","d3959327":"def fetch_data():\n    raw_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n    raw_data = pd.DataFrame(raw_data, columns =['keyword','text','target'])    \n    \n    raw_data['keyword'] = raw_data['keyword'].apply(lambda x: text_preprocess2(x))\n    raw_data['text'] = raw_data['text'].apply(lambda x: text_preprocess(x))\n    keyword_list = list(set(list(raw_data['keyword'])))\n    #print(keyword_list)\n    \n    ##Some texts without keyword actually contain keyword so that \n    ##I extract the keyword from these texts according to existed keyword list\n    row, column = np.shape(raw_data)\n    for i in range(row):\n        for j in range(1,len(keyword_list)):\n            if raw_data.loc[i,'text'].find(keyword_list[j]) != -1 and raw_data.loc[i,'keyword'] is np.nan:\n                raw_data.loc[i,'keyword'] = keyword_list[j]                  \n    #print(raw_data.isnull().sum())\n    \n    raw_data = raw_data.dropna(axis=0)\n    \n    ##Assign weight based on the average target of each keyword\n    keyword_target = raw_data['target'].groupby(raw_data['keyword']).mean()\n    keyword_weight = keyword_target.to_dict()\n    #print(len(keyword_weight))\n    raw_data['keyword'] = raw_data['keyword'].apply(lambda x: keyword_weight[x])\n    \n    \n    features = pd.DataFrame(raw_data, columns =['keyword','text'])\n    labels = pd.DataFrame(raw_data, columns =['target'])\n    \n    ##Also add words from texts in test data to the corpus\n    predict_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n    predict_data = pd.DataFrame(predict_data, columns =['id','keyword','text'])\n    predict_data['text'] = predict_data['text'].apply(lambda x: text_preprocess(x))\n    corpus = list(features['text'])+list(predict_data['text'])\n    \n    \n    ##Generate TF-IDF term matrix and use LSA to reduce dimension\n    vectorizer = TfidfVectorizer()   \n    vector = vectorizer.fit_transform(corpus)\n    \n    ##Recommending reserved number of feature is 100\n    svd_model = TruncatedSVD(n_components=100, algorithm='randomized', n_iter=10, random_state=1)\n    result_vector = svd_model.fit_transform(vector) \n    #print(np.shape(result_vector))\n    term_matrix = np.array(result_vector[:7597,:])\n    keyword_matrix = np.array(features['keyword']).reshape(-1, 1)\n    \n    ##Concatenate the term matrix and keyword weight\n    new_features = np.concatenate((term_matrix, keyword_matrix), axis=1)\n    labels = np.array(labels)\n    #print(np.shape(new_features))\n    term_matrix_predict = np.array(result_vector[7597:,:])\n    train_x, test_x, train_y, test_y = train_test_split(new_features,labels,test_size=0.2, random_state=1)\n    #print(np.shape(train_x))\n    return train_x, test_x, train_y, test_y, term_matrix_predict, predict_data, keyword_weight, new_features, labels\n\n#fetch_data()","0042be40":"##trian on partial dataset\ndef SVM_classifier():\n    train_x, test_x, train_y, test_y = fetch_data()[:4]\n    train_y = np.array(train_y).ravel()\n    test_y = np.array(test_y).ravel()\n    #print(train_y)\n    my_SVM = SVC(C =10, kernel='rbf',gamma ='scale', random_state=10)\n    my_SVM = my_SVM.fit(train_x, train_y)\n    result = my_SVM.score(test_x, test_y)\n    print(result)\n    score_train = my_SVM.score(train_x, train_y)\n    print(score_train)\n\n    return my_SVM \n\n\nSVM_classifier()\n\n\n#train on whole dataset\ndef SVM_classifier_2():\n    train_x, train_y = fetch_data()[7:9]\n    train_y = np.array(train_y).ravel()\n    #print(train_y)\n    my_SVM = SVC(C =10, kernel='rbf',gamma ='scale', random_state=10)\n    my_SVM = my_SVM.fit(train_x, train_y)\n    score_train = my_SVM.score(train_x, train_y)\n    print(score_train)\n\n    return my_SVM","0b1d6431":"def SVM_predict_tweet():\n    SVM = SVM_classifier_2()\n    term_matrix_predict, predict_data, keyword_weight = fetch_data()[4:7]\n    predict_data['keyword'] = predict_data['keyword'].apply(lambda x: text_preprocess2(x))\n    \n    ##Assign weight for each keyword according to keyword dict generated in fetch data\n    row, column = np.shape(predict_data)\n    for i in range(row):\n        for key in keyword_weight.keys():\n            if predict_data.loc[i,'text'].find(key) != -1 and predict_data.loc[i,'keyword'] is np.nan:\n                predict_data.loc[i,'keyword'] = key\n    #print(predict_data.isnull().sum())\n    predict_data['keyword'] = predict_data['keyword'].apply(lambda x: keyword_weight[x] if x is not np.nan else x)\n    predict_data = predict_data.fillna(int(0))\n    #print(predict_data.isnull().sum())\n    predict_keyword = np.array(predict_data['keyword']).reshape(-1, 1)\n    predict_matrix = np.concatenate((term_matrix_predict, predict_keyword),axis =1)\n    results = SVM.predict(predict_matrix)\n    output = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n    output['target'] = results\n    output.to_csv('submission.csv',index=False, header=True)\n\n\n    \nSVM_predict_tweet()","d3f1ccf2":"****In this notebook, keyword is also taken into consideration (supplement null values in attribute keyword and convert keyword to weight). Although the model is SVM, but the score is approaching to those modeles using Neural Network.****","43d7c002":"Preprocess the texts and keywords","becc7c52":"In this fetch data function. \n\n1. First I find those texts actually contain keyword but show no keyword and add keyword based on existed keyword list. \n\n2. Then I take keyword into consideration as another feature by grouping target by keyword and caculate the average. The average    target for each keyword is considered as a new feature as keyword weight.\n\n3. Then I generate  TF-IDF term matrix and reduce dimension of this matrix using LSA(SVD)\n\n4. I concatenate the term matrix and keyword weights","129e1e14":"Import all the libraries needed","bc3d3b53":"Use Support Vector Machine to train. The model performs best when C==10."}}