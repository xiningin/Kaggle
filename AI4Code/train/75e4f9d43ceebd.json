{"cell_type":{"14549be8":"code","851e9746":"code","ff3ef6d4":"code","f189746e":"code","fb850ce7":"code","951c455c":"code","7299edd1":"code","25768b19":"code","902ab1e1":"code","71279f59":"code","7aaf0a3e":"code","2ff46200":"code","3c379bab":"code","d873de5b":"code","7862eec2":"code","5277d7dd":"code","9c3c9234":"code","0479285d":"code","fce106af":"code","5e42bfe9":"code","f1e58d25":"code","6999b999":"code","43f9e33c":"code","1f7213ff":"code","2bf15414":"code","205c81a6":"code","7083cd14":"code","f0d1c9f0":"code","cac59b5c":"code","6c122013":"code","aaa16136":"code","9ae98d05":"code","55fc3296":"code","73f63f0e":"code","da18991f":"code","398c0666":"code","3496d092":"code","711948db":"code","bb3fa26e":"code","9e6d6ebb":"code","effc66a2":"code","9338410b":"code","0fe15475":"code","5759b21e":"code","ac96e0b5":"code","312ad44f":"code","6263555c":"code","d95ff9ae":"code","27e56526":"code","8b60941c":"code","d9a9893f":"code","9992d521":"code","2072f9a4":"code","65f747cc":"code","61cee49c":"code","7bfd20f4":"code","3f9b78ef":"code","7ed6a140":"markdown","7a080d3d":"markdown","00f7078c":"markdown","b3e8dd3f":"markdown","eb4b30f8":"markdown","c0e4c389":"markdown","75b659fa":"markdown","00666a03":"markdown","2a36c98a":"markdown","d7a35f85":"markdown","3ef5082d":"markdown","0ed30d72":"markdown","101aafb2":"markdown","374b4016":"markdown","4be6012b":"markdown","aab71865":"markdown","299f1b38":"markdown","ba6af50e":"markdown","24699cf0":"markdown","fb547ed2":"markdown","8d4ea5c2":"markdown","45d630f9":"markdown","6f3c9fef":"markdown","e2c3dd0f":"markdown","71125478":"markdown","1d64c7f5":"markdown","a1e3e257":"markdown","c6c55778":"markdown","5d5245d6":"markdown","ea3a0c20":"markdown","cea690dd":"markdown","33ab3720":"markdown","c5308237":"markdown"},"source":{"14549be8":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgb","851e9746":"PATH=\"..\/input\/\"\nos.listdir(PATH)","ff3ef6d4":"train_df = pd.read_csv(PATH+\"train.csv\")\ntest_df = pd.read_csv(PATH+\"test.csv\")","f189746e":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"Test:  rows:{} cols:{}\".format(test_df.shape[0], test_df.shape[1]))","fb850ce7":"train_df.head()","951c455c":"test_df.head()","7299edd1":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return np.transpose(pd.concat([total, percent], axis=1, keys=['Total', 'Percent']))","25768b19":"missing_data(train_df)","902ab1e1":"missing_data(test_df)","71279f59":"train_df.info()","7aaf0a3e":"train_df.describe()","2ff46200":"categorical_columns = ['waterfront', 'view', 'condition', 'grade']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,2,figsize=(16,10))\nfor col in categorical_columns:\n    i += 1\n    plt.subplot(2,2,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'])\n    plt.xlabel(col, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","3c379bab":"def plot_stats(feature):\n    temp = train_df[feature].dropna().value_counts().head(50)\n    df1 = pd.DataFrame({feature: temp.index,'Number of samples': temp.values})\n    temp = test_df[feature].dropna().value_counts().head(50)\n    df2 = pd.DataFrame({feature: temp.index,'Number of samples': temp.values})    \n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,6))\n    s = sns.barplot(x=feature,y='Number of samples',data=df1, ax=ax1)\n    s.set_title(\"Train set\")\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    s = sns.barplot(x=feature,y='Number of samples',data=df2, ax=ax2)\n    s.set_title(\"Test set\")\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()    ","d873de5b":"categorical_columns = ['waterfront', 'view', 'condition', 'grade']\n\nfor col in categorical_columns:\n    plot_stats(col)","7862eec2":"numerical_columns = ['bedrooms', 'bathrooms', 'floors']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(1,3,figsize=(18,4))\nfor col in numerical_columns:\n    i += 1\n    plt.subplot(1,3,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'])\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","5277d7dd":"area_columns = ['sqft_living','sqft_lot','sqft_above', 'sqft_basement', 'sqft_living15', 'sqft_lot15']\n\ni = 0\nplt.figure()\nfig, ax = plt.subplots(3,2,figsize=(16,15))\nfor col in area_columns:\n    i += 1\n    plt.subplot(3,2,i)\n    plt.scatter(x=train_df[col],y=train_df['price'],c='magenta', alpha=0.2)\n    plt.xlabel(col, fontsize=12)\n    plt.ylabel('price', fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","9c3c9234":"geo_columns = ['lat','long']\n\ni = 0\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(16,6))\nfor col in geo_columns:\n    i += 1\n    plt.subplot(1,2,i)\n    plt.scatter(x=train_df[col],y=train_df['price'], c=train_df['zipcode'], alpha=0.2)\n    plt.xlabel(col, fontsize=10)\n    plt.ylabel('price', fontsize=10)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=10)\nplt.show();","0479285d":"from mpl_toolkits.mplot3d import Axes3D\nfig=plt.figure(figsize=(16,16))\nax=fig.add_subplot(1,1,1, projection=\"3d\")\nax.scatter(train_df['lat'],train_df['long'],train_df['price'],c=train_df['zipcode'],alpha=.8)\nax.set(xlabel='\\nLatitude',ylabel='\\nLongitude',zlabel='\\nPrice')","fce106af":"print(\"There are {} unique zipcodes.\".format(train_df['zipcode'].nunique()))","5e42bfe9":"plt.figure(figsize=(18,4))\nsns.boxplot(x=train_df['zipcode'],y=train_df['price'])\nplt.xlabel('zipcode', fontsize=8)\nlocs, labels = plt.xticks()\nplt.tick_params(axis='x', labelsize=8, rotation=90)\nplt.show();","f1e58d25":"plt.figure(figsize=(16,16))\nplt.scatter(x=train_df['lat'],y=train_df['long'], c=train_df['zipcode'], cmap='Spectral')\nplt.xlabel('lat', fontsize=12); plt.ylabel('long', fontsize=12)\nplt.show();","6999b999":"for df in [train_df, test_df]:\n    df['date'] = pd.to_datetime(df['date'])\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['quarter'] = df['date'].dt.quarter\n    df['is_month_start'] = pd.to_numeric(df['date'].dt.is_month_start)\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['is_weekend'] = pd.to_numeric(df['dayofweek']>=5)","43f9e33c":"for df in [train_df, test_df]:\n    df['med_lat'] = np.round(df['lat'],1) \n    df['med_long'] = np.round(df['long'],1) \n    df['build_old'] = 2019 - df['yr_built']\n    df['sqft_living_diff'] = df['sqft_living'] - df['sqft_living15']\n    df['sqft_lot_diff'] = df['sqft_lot'] - df['sqft_lot15']\n    df['bedroom_bathroom_ratio'] = df['bedrooms'] \/ df['bathrooms']","1f7213ff":"train_df.head()","2bf15414":"date_columns = ['year', 'month', 'dayofweek', 'quarter']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,2,figsize=(12,12))\nfor col in date_columns:\n    i += 1\n    plt.subplot(2,2,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=False)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","205c81a6":"date_columns = ['year', 'month', 'dayofweek', 'quarter']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,2,figsize=(16,12))\nfor col in date_columns:\n    i += 1\n    plt.subplot(2,2,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=True)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","7083cd14":"date_columns = ['dayofyear', 'weekofyear']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,1,figsize=(18,12))\nfor col in date_columns:\n    i += 1\n    plt.subplot(2,1,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=False)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","f0d1c9f0":"date_columns = ['dayofyear', 'weekofyear']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,1,figsize=(18,12))\nfor col in date_columns:\n    i += 1\n    plt.subplot(2,1,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=True)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","cac59b5c":"date_columns = ['is_month_start', 'is_weekend']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nfor col in date_columns:\n    i += 1\n    plt.subplot(1,2,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=False)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","6c122013":"date_columns = ['is_month_start', 'is_weekend']\ni = 0\nplt.figure()\nfig, ax = plt.subplots(1,2,figsize=(12,6))\nfor col in date_columns:\n    i += 1\n    plt.subplot(1,2,i)\n    sns.boxplot(x=train_df[col],y=train_df['price'],showfliers=True)\n    plt.xlabel(col, fontsize=8)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=8)\nplt.show();","aaa16136":"features = ['bedrooms','bathrooms','floors',\n            'waterfront','view','condition','grade',\n            'sqft_living','sqft_lot','sqft_above','sqft_basement','sqft_living15','sqft_lot15',\n            'yr_built','yr_renovated',\n            'lat', 'long','zipcode', \n            'date', 'dayofweek', 'weekofyear', 'dayofyear', 'quarter', \n            'is_month_start', 'month', 'year', 'is_weekend',\n            'price']\n\nmask = np.zeros_like(train_df[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(18,18))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(train_df[features].corr(),linewidths=0.25,vmax=1.0,square=True,cmap=\"Blues\", \n            linecolor='w',annot=True,mask=mask,cbar_kws={\"shrink\": .75});\n","9ae98d05":"#We are using 80-20 split for train-test\nVALID_SIZE = 0.2\n#We also use random state for reproducibility\nRANDOM_STATE = 2019\n\ntrain, valid = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","55fc3296":"predictors = ['sqft_living', 'grade']\ntarget = 'price'","73f63f0e":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","da18991f":"RFC_METRIC = 'mse'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier","398c0666":"model = RandomForestRegressor(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","3496d092":"model.fit(train_X, train_Y)","711948db":"preds = model.predict(valid_X)","bb3fa26e":"def plot_feature_importance():\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': model.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","9e6d6ebb":"plot_feature_importance()","effc66a2":"print(\"RF Model score: \", model.score(train_X, train_Y))","9338410b":"def rmse(preds, y):\n    return np.sqrt(mean_squared_error(preds, y))","0fe15475":"print(\"Root mean squared error (valid set):\",round(rmse(preds, valid_Y),2))","5759b21e":"predictors = ['sqft_living', 'grade', 'sqft_above']\ntarget = 'price'\ntrain_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values\nmodel = RandomForestRegressor(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)\nmodel.fit(train_X, train_Y)\npreds = model.predict(valid_X)","ac96e0b5":"plot_feature_importance()","312ad44f":"print(\"RF Model score: \", model.score(train_X, train_Y))\nprint(\"Root mean squared error (valid set):\",round(rmse(preds, valid_Y),2))","6263555c":"predictors = ['sqft_living', 'sqft_lot',\n              'sqft_above', 'sqft_living15',\n              'waterfront', 'view', 'condition', 'grade',\n             'bedrooms', 'bathrooms', 'floors',\n             'zipcode', \n              'month', 'dayofweek', \n              'med_lat', 'med_long',\n              'build_old', 'sqft_living_diff', 'sqft_lot_diff',\n             ]\ntarget = 'price'\ntrain_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values\nmodel = RandomForestRegressor(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)\nmodel.fit(train_X, train_Y)\npreds = model.predict(valid_X)\nplot_feature_importance()\nprint(\"RF Model score: \", model.score(train_X, train_Y))\nprint(\"Root mean squared error (valid set):\",round(rmse(preds, valid_Y),2))","d95ff9ae":"test_X = test_df[predictors] \npredictions_RF = model.predict(test_X)\nsubmission = pd.read_csv(PATH+\"sample_submission.csv\")\nsubmission['price'] = predictions_RF\nsubmission.to_csv('submission.csv', index=False)","27e56526":"param = {'num_leaves': 51,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"metric\": 'rmse',\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 42}","8b60941c":"predictors = ['sqft_living', 'sqft_lot',\n              'sqft_above', 'sqft_living15',\n              'waterfront', 'view', 'condition', 'grade',\n             'bedrooms', 'bathrooms', 'floors',\n             'zipcode', \n              'month', 'dayofweek', \n              'med_lat', 'med_long',\n              'build_old', 'sqft_living_diff', 'sqft_lot_diff',\n             ]\ntarget = 'price'","d9a9893f":"#prepare fit model with cross-validation\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\noof = np.zeros(len(train_df))\npredictions_lgb_cv = np.zeros(len(test_df))\nfeature_importance_df = pd.DataFrame()","9992d521":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df,train_df['price'].values)):\n    strLog = \"fold {}\".format(fold_)\n    print(strLog)\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][predictors], label=train_df.iloc[trn_idx][target])#, categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train_df.iloc[val_idx][predictors], label=train_df.iloc[val_idx][target])#, categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][predictors], num_iteration=clf.best_iteration)\n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = predictors\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    #predictions\n    predictions_lgb_cv += clf.predict(test_df[predictors], num_iteration=clf.best_iteration) \/ folds.n_splits\n    \nstrRMSE = \"RMSE: {}\".format(rmse(oof, train_df[target]))\nprint(strRMSE)","2072f9a4":"def plot_feature_importance_cv():\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False).index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n    plt.figure(figsize=(12,6))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n    plt.title('LightGBM Features (averaged over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","65f747cc":"plot_feature_importance_cv()","61cee49c":"submission = pd.read_csv(PATH+\"sample_submission.csv\")\nsubmission['price'] = predictions_lgb_cv\nsubmission.to_csv('submission_cv.csv', index=False)","7bfd20f4":"predictions_blending = predictions_RF * 0.55 + predictions_lgb_cv * 0.45","3f9b78ef":"submission = pd.read_csv(PATH+\"sample_submission.csv\")\nsubmission['price'] = predictions_blending\nsubmission.to_csv('submission_blending.csv', index=False)","7ed6a140":"Let's evaluate the `rmse` score for training set and for valid set.","7a080d3d":"# <a id='10'>Submission (3)<\/a>  ","00f7078c":"## <a id='21'>Load packages<\/a>\n\nWe load the packages used for the analysis. There are packages for data manipulation, visualization and models.","b3e8dd3f":"#  <a id='7'>Model with cross-validation<\/a> \n\nLet's use a slightly improved model with cross-validation.","eb4b30f8":"Let's plot the zipcode on lat\/long.","c0e4c389":"# <a id='1'>Introduction<\/a>  \n\nThis Kernel will take you through the process of **analyzing the data** to understand the **predictive values** of various **features** and the possible correlation between different features, **selection of features** with predictive value, **features engineering** to create features with higher predictive value, creation of a **baseline model**, succesive **refinement** of the model  through selection of features and, at the end, **submission** of the best solution found. \n\n\nThe dataset used for this tutorial contains sales information for houses in a certain US region.\n\nThe objective of the competition is to predict with good accuracy the sale **price** for each house in the test data.\n\nThe metric used for the competition is **RMSE**.\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n","75b659fa":"# <a id='5'>Model refinement<\/a>  \n\nWe will now add succesivelly features and verify if the validation `rmse` error improves.","00666a03":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n# <a id='3'>Data exploration<\/a>  \n\nWe check the shape of train and test dataframes and also show a selection of rows, to have an initial image of the data.\n\n","2a36c98a":"# <a id='11'>Suggestions for further improvement<\/a>  \n\nTo further develop this Kernel and obtain better performance in the competition, try the followings:\n* experiment with different algorithms (linear regressor, catbost, xgb, lgb etc.)\n* add more features to your model;\n* after adding each new feature, test if the cross-validation score is improving (your rmse for cross-valid should decrease);  \n* start to experiment more with engineered features; see the Reference section of this Kernel for suggestions;  \n* use hyperparameter optimization for your model to set the best hyperparameters;  \n\n**Important to remember**: before submitting a new solution, try to get better score (or better cross-validation score) for your model;  do not use the public score on the leaderboard as the unique criteria for evaluating your model performance, since this is calculated with only 30% of the test set.\n\n**Happy Kaggling**!\n","d7a35f85":"Let's represent now the Pearson corelation matrix for all the numerical features.","3ef5082d":"Let's add one more feature.","0ed30d72":"# <a id='2'>Prepare the data analysis<\/a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n","101aafb2":"Both **train** and **test** files contains the following values:  \n\n* **id** - the index of the transaction (in the dataset);  \n* **date** - the date of the transaction;  \n* **bedrooms** - the number of bedrooms;  \n* **bathrooms** - the number of the bathrooms;    \n* **sqft_living** - the number of square feet of the living area;    \n* **sqft_lot** - the number of square feet of the lot area;  \n* **floors** - the number of floors of the house;  \n* **waterfront** - flag to indicate if the house is on the waterfront;  \n* **view** - flag to indicate if the house has a view;  \n* **condition** - categorical value for the condition of the house;  \n* **grade** - categorical value for the grade of the property;  \n* **sqft_above** - the number of square feet of the living area above the ground;  \n* **sqft_basement** - the number of square feet of the living area below the ground;  \n* **yr_built** - year when the property was built;  \n* **yr_renovated** - year when the property was renovated; \n* **zipcode** - zipcode for the property location;  \n* **lat** - latitude of the property location;  \n* **long** - longitude of the property location;  \n* **sqft_living15** - the number of square feet of the living area '15';  \n* **sqft_lot15** - the number of square feet of the lot area '15';  \n\n\nThe **train** data has as well the target value, **price**. This is the sale price of the property. \n\nThe competition objective is to estimate with highest accuracy the price of properties in the test set.\n\nIt is important, before going to create a model, to have a good understanding of the data. We will therefore explore the various features.","374b4016":"And let's build some more engineering features.","4be6012b":"# <a id='12'>References<\/a>  \n\n[1]  Root mean square error,  https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation   \n[2]  Example of using cross validation and scalling, https:\/\/www.kaggle.com\/gpreda\/lanl-earthquake-eda-and-prediction   \n[3] Tutorial for classification, https:\/\/www.kaggle.com\/gpreda\/tutorial-for-classification   \n[4] Regression models, https:\/\/www.kaggle.com\/toraaglobal\/soilpropertyprediction   \n[5] Regression models, https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard  \n[6] Regression models, https:\/\/www.kaggle.com\/gpreda\/elo-world-high-score-without-blending  \n","aab71865":"# <a id='4'>Build a baseline model<\/a>  \n\n\nWe start with a very basic model, with only two features.\n\nBut first, we will separate train dataset in train and valid sets.\n","299f1b38":"Let's check the geographical info now.","ba6af50e":"Let's look into more details about the date feature.","24699cf0":"Let's plot the features importance. This shows the relative importance of the predictors features for the current model. With this information, we are able to select the features we will use for our gradually refined models.","fb547ed2":"## <a id='31'>Check for missing data<\/a>  \n\nLet's create a function that check for missing data in the two datasets (train and test).","8d4ea5c2":"# <a id='6'>Submission<\/a>  \n\nWe prepare now the submission file.","45d630f9":"There are no missing data in the train and test sets.","6f3c9fef":"# <a id='8'>Submission (2)<\/a>  \n\nWe prepare now the submission file.","e2c3dd0f":"We plot now the relative distribution of price and square feet (area).","71125478":"We plot then the price distribution grouped by each numerical feature.","1d64c7f5":"We plot first the price distribution grouped by each categorical feature.","a1e3e257":"Let's plot the distribution of price by zipcode.\n","c6c55778":"Besides the date (in string format), all other data is numeric (either float or integer).  Also, there is no missing data. \nFloat values are the bathrooms (that have also fractional values), the floors (same as previous), lat\/long (real numbers) and the price (same as previous).\nThe rest, categorical or numeral, are integers.\n\nLet's check now the distribution of each numeric column.","5d5245d6":"Let's see also the comparison between train and test set distribution of categorical columns.","ea3a0c20":"## <a id='32'>Features visualization<\/a>  \n\nLet's explore each individual feature.  First, let's show the info about each feature.","cea690dd":"<h1><center><font size=\"6\">House Sales EDA and Prediction<\/font><\/center><\/h1>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n - <a href='#21'>Load packages<\/a>  \n - <a href='#22'>Load the data<\/a>   \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check for missing data<\/a>  \n - <a href='#32'>Features visualization<\/a>   \n- <a href='#4'>Build a baseline model<\/a>  \n- <a href='#5'>Model refinement<\/a> \n- <a href='#6'>Submission<\/a>  \n- <a href='#7'>Model with cross-validation<\/a> \n- <a href='#8'>Submission (2)<\/a> \n- <a href='#9'>Blending<\/a>\n- <a href='#10'>Submission (3)<\/a>\n- <a href='#11'>Suggestions for further improvement<\/a>\n- <a href='#12'>References<\/a>      ","33ab3720":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='22'>Load the data<\/a>  \n\nLet's see first what data files do we have in the root directory. ","c5308237":"# <a id='9'>Blending<\/a>  \n\nLet's combine the predictions obtained until now, using a weighted sum of the two models predictions."}}