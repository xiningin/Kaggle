{"cell_type":{"dca11e79":"code","e1953609":"code","bce71114":"code","e6975015":"code","ae994df9":"code","910caee3":"code","358c6927":"code","77989590":"code","4879f6b2":"code","33fb30a5":"code","16ca3724":"code","ebb3207c":"code","49cd6589":"code","ae727e79":"code","e6967a79":"code","12c51003":"code","6ca52b9f":"code","f97df426":"code","e6b94b42":"code","fd95a47d":"code","360b7c0e":"code","cbf7686e":"code","553957bb":"code","ba25c1d3":"code","f119beac":"code","2f9ed4a0":"code","0eb212aa":"code","3acde22c":"code","7c68ba0d":"code","693c074a":"code","f2ebc3a4":"code","e48dd793":"markdown"},"source":{"dca11e79":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n%matplotlib inline","e1953609":"boston = load_boston()","bce71114":"# boston is a dictionary, let's check what it contains\nprint(boston.keys())\nprint(boston.DESCR)","e6975015":"bos = pd.DataFrame(boston.data, columns=boston.feature_names)\n\nbos['MEDV'] = boston.target","ae994df9":"print(\"Dataframe type : {}\".format(type(bos)))\nprint(\"Dataframe shape: {}\".format(bos.shape))\nprint(\"Dataframe features: {}\".format(list(bos.columns.values)))","910caee3":"bos.head()","358c6927":"bos.tail()","77989590":"bos.describe()","4879f6b2":"# check for missing values in all the columns\nprint(\"[INFO] df isnull():\\n {}\".format(bos.isnull().sum()))","33fb30a5":"# set the size of the figure\nsns.set(rc={'figure.figsize':(12, 8)})\n\ng = sns.PairGrid(bos, vars=['LSTAT', 'RM', 'CRIM', 'NOX', 'MEDV'], height=1.5, aspect=1.5)\ng = g.map_diag(plt.hist)\ng = g.map_lower(sns.regplot, lowess=True, scatter_kws={'s': 15, 'alpha':0.3}, \n                line_kws={'color':'red', 'linewidth': 2})\ng = g.map_upper(sns.kdeplot, n_levels=15, cmap='coolwarm')\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \nplt.show()","16ca3724":"fig, axes = plt.subplots(2, 2)\nfig.suptitle(\"Scatterplot and Boxplot for LSTAT and RM\")\n\nsns.regplot(x=bos['LSTAT'], y=bos['MEDV'], lowess=True, scatter_kws={'s': 25, 'alpha':0.3},\n            line_kws={'color':'purple', 'linewidth': 2}, ax=axes[0, 0])\n\nsns.boxplot(x=bos['LSTAT'], ax=axes[0, 1])\n\nsns.regplot(x=bos['RM'], y=bos['MEDV'], lowess=True, scatter_kws={'s': 25, 'alpha':0.3},\n            line_kws={'color':'purple', 'linewidth': 2}, ax=axes[1, 0])\n\nsns.boxplot(x=bos['RM'], ax=axes[1, 1]).set(xlim=(3, 9))\n\nplt.show()","ebb3207c":"fig, axes = plt.subplots(2, 2)\nfig.suptitle(\"Scatterplot and Boxplot for CRIM and NOX\")\n\nsns.regplot(x=bos['CRIM'], y=bos['MEDV'], lowess=True, scatter_kws={'s': 25, 'alpha':0.3},\n            line_kws={'color':'purple', 'linewidth': 2}, ax=axes[0, 0])\n\nsns.boxplot(x=bos['CRIM'], ax=axes[0, 1])\n\nsns.regplot(x=bos['NOX'], y=bos['MEDV'], lowess=True, scatter_kws={'s': 25, 'alpha':0.3},\n            line_kws={'color':'purple', 'linewidth': 2}, ax=axes[1, 0]).set(xlim=(0.35, 0.9))\n            \nsns.boxplot(x=bos['NOX'], ax=axes[1, 1]).set(xlim=(0.35, 0.9))\n\nplt.show()","49cd6589":"fig, axes = plt.subplots(2, 2)\nfig.suptitle(\"Histogram of Key Features\")\n\nsns.distplot(bos['LSTAT'], bins=30, ax=axes[0, 0])\nsns.distplot(bos['RM'], bins=30, ax=axes[0, 1])\nsns.distplot(bos['CRIM'], bins=30, ax=axes[1, 0])\nsns.distplot(bos['NOX'], bins=30, ax=axes[1, 1])\n\nplt.show()","ae727e79":"# plot a histogram showing the distribution of the target variable\nsns.distplot(bos['MEDV'], bins=30)\n\nplt.show()","e6967a79":"# compute the pair wise correlation for all columns  \ncorrelation_matrix = bos.corr(method='pearson').round(2)\n \n# use the heatmap function from seaborn to plot the correlation matrix\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)\n\nplt.title('Pearson pair-wise Correlation Matrix')\nplt.show()","12c51003":"X = pd.DataFrame(np.c_[bos['LSTAT'], bos['RM'], bos['CRIM'], bos['NOX']], columns=['LSTAT', 'RM', 'CRIM', 'NOX'])\nY = bos['MEDV']\n\nprint(X.shape)\nprint(Y.shape)","6ca52b9f":"# splits the training and test data set in 75% : 25%\n# assign random_state to any value.This ensures consistency.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=5)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","f97df426":"lm = LinearRegression()\nlm.fit(X_train, Y_train)\n\nprint('Linear Regression coefficients: {}'.format(lm.coef_))\nprint('Linear Regression intercept: {}'.format(lm.intercept_))\n\n# model evaluation for training set\ny_train_predict = lm.predict(X_train)\n\n# plt.plot(np.unique(Y_train), np.poly1d(np.polyfit(Y_train, y_train_predict, 1))(np.unique(Y_train)), \n#         linewidth=2, color='r')\n\n# calculating the intercept and slope for the regression line\nb, m = np.polynomial.polynomial.polyfit(Y_train, y_train_predict, 1)","e6b94b42":"sns.scatterplot(Y_train, y_train_predict, alpha=0.4)\nsns.regplot(Y_train, y_train_predict, truncate=True, scatter_kws={'s': 20, 'alpha':0.3}, line_kws={'color':'green', 'linewidth': 2})\nsns.lineplot(np.unique(Y_train), np.unique(np.poly1d(b + m * np.unique(Y_train))), linewidth=0.5, color='r')\n\nplt.xlabel(\"Actual Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Actual Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$ [Training Set]\")\n \nplt.show()","fd95a47d":"rmse = (np.sqrt(mean_squared_error(Y_train, y_train_predict)))\nr2 = r2_score(Y_train, y_train_predict)\n \nprint(\"The linear model performance for training set\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","360b7c0e":"# model evaluation for testing set\ny_test_predict = lm.predict(X_test)","cbf7686e":"sns.scatterplot(Y_test, y_test_predict, alpha=0.4)\nsns.regplot(Y_test, y_test_predict, truncate=True, scatter_kws={'s': 20, 'alpha':0.3}, line_kws={'color':'green', 'linewidth': 2})\n \nplt.xlabel(\"Actual Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Actual Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$ [Test Set]\")\n \nplt.show()","553957bb":"# root mean square error of the model\nrmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n \n# r-squared score of the model\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"\\nThe linear model performance for testing set\")\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","ba25c1d3":"y_train_residual = y_train_predict - Y_train\ny_test_residual = y_test_predict - Y_test\n\nplt.subplot(1, 2, 1)\nsns.distplot(y_train_residual, bins=15)\nplt.title('Residual Histogram for Training Set')\n\nplt.subplot(1, 2, 2)\nsns.distplot(y_test_residual, bins=15)\nplt.title('Residual Histogram for Test Set')\n\nplt.show()","f119beac":"fig, axes = plt.subplots()\nfig.suptitle('Residual plot of Training and Test set')\n\n# Plot the residuals after fitting a linear model\nsns.residplot(y_train_predict, y_train_residual, lowess=True, color=\"b\", ax=axes, label='Training Set', \n              scatter_kws={'s': 25, 'alpha':0.3})\n\nsns.residplot(y_test_predict, y_test_residual, lowess=True, color=\"g\", ax=axes, label='Test Set',\n              scatter_kws={'s': 25})\n\nlegend = axes.legend(loc='upper left', shadow=True, fontsize='large')\nlegend.get_frame().set_facecolor('#f9e79f')\n\nplt.xlabel('Predicted')\nplt.ylabel('Residual')\nplt.show()","2f9ed4a0":"\"Creates a polynomial regression model for the given degree\"\npoly_features = PolynomialFeatures(degree=2)\n   \n# transform the features to higher degree features.\nX_train_poly = poly_features.fit_transform(X_train)\n   \n# fit the transformed features to Linear Regression\npoly_model = LinearRegression()\n\npoly_model.fit(X_train_poly, Y_train)\n     \n# predicting on training data-set\ny_train_predicted = poly_model.predict(X_train_poly)\n   \n# predicting on test data-set\ny_test_predicted = poly_model.predict(poly_features.fit_transform(X_test))","0eb212aa":"y_train_residual = y_train_predicted - Y_train\ny_test_residual = y_test_predicted - Y_test\n\nplt.subplot(1, 2, 1)\nsns.distplot(y_train_residual, bins=15)\nplt.title('Residual Histogram for Training Set [Polynomial Model]')\n\nplt.subplot(1, 2, 2)\nsns.distplot(y_test_residual, bins=15)\nplt.title('Residual Histogram for Test Set [Polynomial Model]')\n\nplt.show()","3acde22c":"sns.scatterplot(Y_train, y_train_predicted, alpha=0.4)\nsns.regplot(Y_train, y_train_predicted, scatter_kws={'s': 20, 'alpha':0.3}, line_kws={'color':'green', 'linewidth': 2}, order=2)\n \nplt.xlabel(\"Actual Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Actual Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$ [Training Set]\")\n \nplt.show()","7c68ba0d":"sns.scatterplot(Y_test, y_test_predicted, alpha=0.4)\nsns.regplot(Y_test, y_test_predicted, scatter_kws={'s': 20, 'alpha':0.3}, line_kws={'color':'green', 'linewidth': 2}, order=2)\n \nplt.xlabel(\"Actual Prices: $Y_i$\")\nplt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\nplt.title(\"Actual Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$ [Test Set]\")\n \nplt.show()","693c074a":"# evaluating the model on training data-set\nrmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\nr2_train = r2_score(Y_train, y_train_predicted)\n     \nprint(\"The polynomial model performance for the training set\")\nprint(\"RMSE of training set is {}\".format(rmse_train))\nprint(\"R2 score of training set is {}\".format(r2_train))","f2ebc3a4":"# evaluating the model on test data-set\nrmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predicted))\nr2_test = r2_score(Y_test, y_test_predicted)\n\nprint(\"The polynomial model performance for the test set\")\nprint(\"RMSE of test set is {}\".format(rmse_test))\nprint(\"R2 score of test set is {}\".format(r2_test))","e48dd793":"We can conclude that the straight regression line is unable to capture the patterns in the data. This is an example of <i>underfitting<\/i>. To overcome underfitting, we need to increase the complexity of the model.  This could be done by converting the original features into their higher order polynomial terms by using the <b>PolynomialFeatures<\/b> class provided by scikit-learn. Next, we train the model using Linear Regression."}}