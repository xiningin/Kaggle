{"cell_type":{"2a286f71":"code","11300b19":"code","aa6a152a":"code","cef59585":"code","9baedbe4":"code","b8a82258":"code","591035b2":"code","603c7e2c":"code","92d920fa":"code","f76160fa":"code","72264fcf":"code","0e9a2037":"code","11c5677a":"code","7c7d5eab":"code","39fa73ff":"code","0ec98776":"code","970ae45e":"code","473f6d11":"code","ba7a1f5a":"code","bb91b0ef":"code","8293328d":"code","53aef80a":"code","756f7394":"code","5f71faf0":"code","e7a4668e":"code","aa6b4597":"code","3a77978b":"code","43d984e3":"code","dafd5a25":"code","a79e5cff":"code","39d39e0c":"code","37f0a76f":"code","4c61bad2":"code","ea28fba0":"code","5cfa1e2b":"code","132a02de":"code","9665d09b":"code","1c5527ad":"code","b978b3bf":"code","8016baf6":"code","eb943e31":"code","be250ccd":"code","6a90998d":"code","781d7dce":"code","d2fc09df":"code","7cb5dc22":"code","d74f2575":"code","d3bff3b9":"code","de2c81cc":"code","42247e2b":"code","370b3bba":"code","694122a6":"code","db243bc3":"code","5d43e122":"code","376e002b":"code","4892a8e6":"code","919f8dbf":"code","4b9803bc":"code","f310e163":"code","d34991d2":"code","a32c0f04":"code","0627ab1f":"code","9ada673b":"code","fc916079":"code","54ff44f4":"code","3cc32ce0":"code","bd677fff":"code","f5c937fd":"code","90abca71":"code","fbc5af8d":"markdown","14739353":"markdown","d0b2f17f":"markdown","e2d402c3":"markdown","7920ff28":"markdown","16cf5a23":"markdown","3a787cad":"markdown","829c3eb4":"markdown","c670fd11":"markdown","621f76ca":"markdown","d913ee68":"markdown","6110d3df":"markdown","ec60510d":"markdown","4a2c7c89":"markdown","234ee31e":"markdown","4dcd1480":"markdown","4ee84ca5":"markdown","b2babced":"markdown","fbaee967":"markdown","3465d692":"markdown","b0c1eebd":"markdown","87b0fbaa":"markdown","e8512e80":"markdown","7dbab38b":"markdown","7ab49e6d":"markdown","eab77de7":"markdown","a05bc85f":"markdown","228da321":"markdown","908e23c5":"markdown","57c11065":"markdown","cf102643":"markdown","2def48b3":"markdown","c41e93db":"markdown","7a8c986b":"markdown","36a0c60f":"markdown","72c70671":"markdown","da5da57e":"markdown","3abd02f6":"markdown","41808b25":"markdown","b8b4775d":"markdown","33ac5e00":"markdown","0e541a59":"markdown","e6159066":"markdown","be89c5f9":"markdown","fab90369":"markdown","8c47cf5e":"markdown","dfa682ac":"markdown","2ad58786":"markdown","dfc38575":"markdown","9ad3280c":"markdown","eb52db9e":"markdown","e75febf1":"markdown","877ce678":"markdown"},"source":{"2a286f71":"#Make Necessary Import\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom imblearn import over_sampling\n\nsns.set(rc={'figure.figsize':(16,8)})\nsns.set_style(\"whitegrid\")\nsns.color_palette(\"dark\")\nplt.style.use(\"fivethirtyeight\")\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)","11300b19":"data = pd.read_csv('\/kaggle\/input\/from-trending-youtube-video-statistics\/youtube_statistics.csv', sep=';')\ndata.head()","aa6a152a":"print('There are',str(len(data)), 'rows in this dataset')","cef59585":"data.info()","9baedbe4":"cats = ['trending_date','title','channel_title','publish_time','tags','comments_disabled','ratings_disabled','video_error_or_removed',\n       'description','publish_date']\nnums = ['category_id','views','likes','dislikes','comment_count','No_tags','desc_len','len_title']","b8a82258":"data[nums].describe()","591035b2":"data[cats].describe()","603c7e2c":"# assign the numerical data into nums object\nnumerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ndisplay(data.select_dtypes(include=numerics).columns)\nprint(data.select_dtypes(include=numerics).shape)\ndata_num = data.select_dtypes(include=numerics)\ndata_num.head(3)","92d920fa":"# assign the categorical data into cats object\ndisplay(data.select_dtypes(include=['object']).columns)\nprint(data.select_dtypes(include=object).shape)\ndata_cat = data.select_dtypes(include=['object'])\ndata_cat.head(3)","f76160fa":"# look at the distribution of data with boxplot\nfeatures = ['category_id','views','likes','dislikes','comment_count','No_tags','desc_len','len_title']\nplt.figure(figsize=(15, 7))\nfor i in range(0, len(features)):\n    plt.subplot(1, 8, i+1)\n    sns.boxplot(y=data[features[i]],color='green',orient='v')\n    plt.tight_layout();\n#plt.savefig('fig\/boxplot.png')","72264fcf":"#View data distribution\ndata_num = data[features]\nk = len(data_num.columns)\nn = 3\nm = (k - 1) \/\/ n + 1\nfig, axes = plt.subplots(m, n, figsize=(n * 5, m * 3))\nfor i, (name, col) in enumerate(data_num.iteritems()):\n    r, c = i \/\/ n, i % n\n    ax = axes[r, c]\n    col.hist(ax=ax, color='green')\n    ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name, color='red')\n    ax2.set_ylim(0)\n\nfig.tight_layout();\n#plt.savefig('fig\/distribusi data.png')","0e9a2037":"#look for unique values from each categorical data\ndata_cat_unique = data_cat.nunique().reset_index()\ndata_cat_unique.columns = ['fitur', 'unik nilai']\ndata_cat_unique = data_cat_unique.sort_values('unik nilai', ascending=False)\ndata_cat_unique","11c5677a":"#create a correlation matrix from each numeric data\nfeatures = ['category_id','views','likes','dislikes','comment_count','No_tags','desc_len','len_title']\ncorr_= data[features].corr()\nplt.figure(figsize=(16,10))\nsns.heatmap(corr_, annot=True, fmt = \".2f\", cmap = \"BuPu\");\n#plt.savefig('fig\/heatmap.png');","7c7d5eab":"#create a pairplot graph from each numeric data\nplt.figure(figsize=(10,8))\nsns.pairplot(data=data, x_vars=['views','category_id','likes','dislikes','comment_count','No_tags','desc_len','len_title'], y_vars=['views'], height=5, aspect=0.75);\nfig.tight_layout();\n#plt.savefig('fig\/pairplot.png')","39fa73ff":"#see the number of missing values from the data frame\ndata_missing_value = data.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value","0ec98776":"#see the percentage of missing value for each feature\ndata_missing_value['percentage'] = round((data_missing_value['missing_value']\/len(data))*100,2)\ndata_missing_value = data_missing_value.sort_values('percentage', ascending=False).reset_index(drop=True)\ndata_missing_value = data_missing_value[data_missing_value['percentage']>0]\ndata_missing_value","970ae45e":"#View the distribution of missing values using the barplot\nfig, ax = plt.subplots(figsize=(15,7))\n\ng = sns.barplot(x = 'feature',y='percentage',data=data_missing_value,ax=ax, \n               palette=sns.color_palette(\"Blues_d\", n_colors=13, desat=1))\n\nx = np.arange(len(data_missing_value['feature']))\ny = data_missing_value['percentage']\n\nfor i, v in enumerate(y):\n    ax.text(x[i]- 0.1, v+0.5, str(v)+'%', fontsize = 15, color='gray', fontweight='bold')\n    \ntitle = '''\nThe distribution of missing values\n'''\nax.text(0.05,10,title,horizontalalignment='left',color='black',fontsize=22,fontweight='bold')    \n\ntext = '''\nThere are 17 features that have missing value\n8 numeric and 9 non-numeric\n\nlen_title, desc_len, and No_tags with numeric data types have a 3.86% missing value\npublish_date with categorical non-numeric data type has a 3.86% missing value\ncomment_count, likes, dislikes and views with numeric data type have 2.99% missing value\ncomments_disabled, description, video_error, ratings_disabled, and tags with categorical data types have 2.99% missing value\ntitle, publish_title, and channel_title with categorical non-numeric data types have a 2.12% missing value\ncategory_id with numeric data type has 3.86% missing value\n\n'''\nax.text(0.05,5.5,text,horizontalalignment='left',color='black',fontsize=16,fontweight='normal')\n\nax.set_ylim(0,10)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=90)\nplt.tight_layout;\n#plt.savefig('fig\/distribusi nilai hilang.png');","473f6d11":"#dropping category_id, publish_date, description, tags, title, channel_title\ndata_clean = data.drop(['publish_date','description','title','channel_title','tags','publish_time'], axis=1)","ba7a1f5a":"data_clean['comments_disabled'].value_counts()","bb91b0ef":"#fill in the comments_disabled value with mode\ndata_clean['comments_disabled'] = data_clean['comments_disabled'].fillna(data_clean['comments_disabled'].mode()[0])","8293328d":"data_clean['comments_disabled'].value_counts()","53aef80a":"data_clean['video_error_or_removed'].value_counts()","756f7394":"#fill in the video_error_or_removed value with mode\ndata_clean['video_error_or_removed'] = data_clean['video_error_or_removed'].fillna(data_clean['video_error_or_removed'].mode()[0])","5f71faf0":"data_clean['video_error_or_removed'].value_counts()","e7a4668e":"data_clean['ratings_disabled'].value_counts()","aa6b4597":"#fill in the ratings_disabled value with mode\ndata_clean['ratings_disabled'] = data_clean['ratings_disabled'].fillna(data_clean['ratings_disabled'].mode()[0])","3a77978b":"data_clean['ratings_disabled'].value_counts()","43d984e3":"#fill in the missing value with median\ndata_clean.fillna(data_clean.median(), inplace=True)\ndata_clean.head(3)","dafd5a25":"data_clean.isnull().sum()","a79e5cff":"#see the number of duplicate data\ndata_clean.duplicated().sum()","39d39e0c":"#dropping duplicate data\ndata_clean = data_clean.drop_duplicates()","37f0a76f":"#see the number of duplicate data\ndata_clean.duplicated().sum()","4c61bad2":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['likes'],kde=True, ax=ax[0,0])\nax[0,0].set_title('likes - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['likes'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('likes - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['likes']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('likes - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['likes']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('likes - log transformation')\nax[1,1].set_xlabel('')","ea28fba0":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['dislikes'],kde=True, ax=ax[0,0])\nax[0,0].set_title('dislikes - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['dislikes'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('dislikes - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['dislikes']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('dislikes - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['dislikes']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('dislikes - log transformation')\nax[1,1].set_xlabel('')","5cfa1e2b":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['comment_count'],kde=True, ax=ax[0,0])\nax[0,0].set_title('comment_count - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['comment_count'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('comment_count - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['comment_count']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('comment_count - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['comment_count']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('comment_count - log transformation')\nax[1,1].set_xlabel('')","132a02de":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['desc_len'],kde=True, ax=ax[0,0])\nax[0,0].set_title('desc_len - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['desc_len'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('desc_len - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['desc_len']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('desc_len - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['desc_len']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('desc_len - log transformation')\nax[1,1].set_xlabel('')","9665d09b":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['No_tags'],kde=True, ax=ax[0,0])\nax[0,0].set_title('No_tags - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['No_tags'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('No_tags - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['No_tags']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('No_tags - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['No_tags']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('No_tags - log transformation')\nax[1,1].set_xlabel('')","1c5527ad":"f,ax = plt.subplots(2,2,figsize=(18,15))\n\ng = sns.distplot(data_clean['views'],kde=True, ax=ax[0,0])\nax[0,0].set_title('views - Original')\nax[0,0].set_xlabel('')\n\ng = sns.boxplot(data_clean['views'],color='green',orient='h', ax=ax[0,1])\nax[0,1].set_title('views - Original')\nax[0,1].set_xlabel('')\n\ng = sns.distplot(np.log1p(data_clean['views']+1),kde=True, ax=ax[1,0])\nax[1,0].set_title('views - log transformation')\nax[1,0].set_xlabel('')\n\ng = sns.boxplot(np.log1p(data_clean['views']+1),color='green',orient='h', ax=ax[1,1])\nax[1,1].set_title('views - log transformation')\nax[1,1].set_xlabel('')","b978b3bf":"features = ['category_id','views','likes','dislikes','comment_count','No_tags','desc_len','len_title']\n\ndf_pre = data_clean.copy()\nfor var in features:\n    df_pre['log_'+var]= (data_clean[var]+1).apply(np.log)","8016baf6":"for var in features:\n    df_pre['std_'+var]= MinMaxScaler().fit_transform(df_pre[var].values.reshape(len(df_pre), 1))","eb943e31":"data_clean.describe()","be250ccd":"df_pre.head()","6a90998d":"sns.set(rc={'figure.figsize':(10,4)})\nsns.heatmap(df_pre[['std_category_id','std_views','std_likes','std_dislikes','std_comment_count','std_No_tags','std_desc_len','std_len_title']].corr(), annot=True)\n#plt.savefig('fig\/split train test.png')","781d7dce":"x = df_pre[['std_category_id','std_likes','std_dislikes','std_comment_count','std_No_tags','std_desc_len','std_len_title']] # menggunakan semua feature kecuali target\ny = df_pre['std_views']","d2fc09df":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2, random_state=42) #Splitting the data into Train and Test","7cb5dc22":"#Fitting simple linear regression to the Training Set\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(xtrain, ytrain)","d74f2575":"pred = regressor.predict(xtest) #Predicting the views","d3bff3b9":"np.exp(pred).round() # inverse log transform","de2c81cc":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\ndef eval_regression(model, pred, xtrain, ytrain, xtest, ytest):\n    print(\"MAE: %.2f\" % mean_absolute_error(ytest, pred)) # The MAE\n    print(\"RMSE: %.2f\" % mean_squared_error(ytest, pred, squared=False)) # The RMSE\n    print('R2 score: %.2f' % r2_score(ytest, pred)) # Explained variance score: 1 is perfect prediction","42247e2b":"pred = regressor.predict(xtest)\n\nprint('Coefficients: \\n', regressor.coef_) # The slope\nprint('Intercept: \\n', regressor.intercept_) # The Intercept\n\neval_regression(regressor, pred, xtrain, ytrain, xtest,ytest)","370b3bba":"from sklearn.linear_model import Ridge\nridge_model = Ridge()\nridge_model.fit(xtrain, ytrain)","694122a6":"pred = ridge_model.predict(xtest)\n\nprint('Coefficients: \\n', ridge_model.coef_) # The slope\nprint('Intercept: \\n', ridge_model.intercept_) # The Intercept\n\neval_regression(ridge_model, pred, xtrain, ytrain, xtest, ytest)","db243bc3":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nalpha = [200, 230, 250,265, 270, 275, 290, 300, 500] # alpha\nhyperparameters = dict(alpha=alpha)\n\nfrom sklearn.linear_model import Ridge\nridge_model = Ridge()\nclf = RandomizedSearchCV(ridge_model, hyperparameters, cv=5, random_state=42, scoring='r2')\n\n#Fitting Model\nbest_model = clf.fit(xtrain, ytrain)","5d43e122":"pred = best_model.predict(xtest)\neval_regression(best_model, pred, xtrain, ytrain, xtest, ytest)","376e002b":"from sklearn.linear_model import Lasso\nlasso_model = Lasso()\nlasso_model.fit(xtrain, ytrain)","4892a8e6":"pred = lasso_model.predict(xtest)\neval_regression(lasso_model, pred, xtrain, ytrain, xtest, ytest)","919f8dbf":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nalpha = [0.02, 0.024, 0.025, 0.026, 0.03] # alpha or lambda\n\nhyperparameters = dict(alpha=alpha)\n\nfrom sklearn.linear_model import Lasso\nlasso_model = Lasso()\nclf = RandomizedSearchCV(lasso_model, hyperparameters, cv=5, random_state=42, scoring='r2')\n\n#Fitting Model\nbest_model = clf.fit(xtrain, ytrain)","4b9803bc":"pred = best_model.predict(xtest)\neval_regression(best_model, pred, xtrain, ytrain, xtest, ytest)","f310e163":"from sklearn.linear_model import ElasticNet\n\nelasticnet_model = ElasticNet()\nelasticnet_model.fit(xtrain, ytrain)","d34991d2":"pred = elasticnet_model.predict(xtest)\neval_regression(elasticnet_model, pred, xtrain, ytrain, xtest, ytest)","a32c0f04":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nalpha = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\nl1_ratio = np.arange(0, 1, 0.01)\n\nhyperparameters = dict(alpha=alpha, l1_ratio=l1_ratio, normalize=[True,False])\n\nfrom sklearn.linear_model import ElasticNet\nelasticnet_model = ElasticNet()\nclf = RandomizedSearchCV(elasticnet_model, hyperparameters, cv=5, random_state=42, scoring='r2')\n\n#Fitting Model\nbest_model = clf.fit(xtrain, ytrain)","0627ab1f":"pred = best_model.predict(xtest)\neval_regression(best_model, pred, xtrain, ytrain, xtest, ytest)","9ada673b":"from sklearn.tree import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor()\ndt.fit(xtrain, ytrain)\npred = dt.predict(xtest)\neval_regression(dt, pred, xtrain, ytrain, xtest, ytest)","fc916079":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=104)\nbest_model1 = rf.fit(xtrain, ytrain)\npred = rf.predict(xtest)\neval_regression(rf, pred, xtrain, ytrain, xtest, ytest)","54ff44f4":"#make feature importance graph\nX=data[['category_id','likes','dislikes','comment_count','No_tags','desc_len','len_title']]\nfeat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(10).plot(kind='barh')\nax.invert_yaxis()\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","3cc32ce0":"from sklearn.svm import SVR\n\nsvr = SVR()\nsvr.fit(xtrain, ytrain)\npred = svr.predict(xtest)\neval_regression(svr, pred, xtrain, ytrain, xtest, ytest)","bd677fff":"import pickle\npickle.dump(best_model1, open('RandomForest_model.pkl', 'wb'))","f5c937fd":"loaded_model = pickle.load(open('RandomForest_model.pkl', 'rb'))\nresult = loaded_model.score(xtest,ytest)\nprint('Model Score : ', str(round(result,2)*100), '%')","90abca71":"print('train accuracy : ', str(round(loaded_model.score(xtrain, ytrain),2)*100), '%')\nprint('test accuracy : ', str(round(loaded_model.score(xtest, ytest),2)*100), '%')","fbc5af8d":"**The model we chose has very little tendency to overfitting, but we think it can be tolerated within normal reasonable limits, where the train accuracy has a value of 99,38% and the test accuracy has a value of 96,26%.**","14739353":"### Separation of Categorical and Numerical Data","d0b2f17f":"## Graphical Approaching","e2d402c3":"**Objective**<br>\n* Build model to predict video views based on statistical numbers and other attributes.","7920ff28":"# Save Model","16cf5a23":"# Data Preparation","3a787cad":"## Randomized Search","829c3eb4":"**Data Preprocessing**\n\n**we take several steps in preprocessing<\/br>**\n\n* Dropping the `publish_date`, `publish_time`, `description`, `tags`, `title`, and `channel_title` features because these features have their own unique value for each existing video\n* Then we fill in the features that have missing values by using the mode of the feature (the `comments_disabled`, `video_error_or_removed`, and `ratings_disabled` features. This is done because this feature has categorical data so it's better to use the fill with the values that appear the most frequently.\n* Features with numerical data are filled in with the median value of each feature, we do this because the median value is more representative of the entire data distribution and is also more robust even though there are outliers.\n* After filling in the missing values, we drop 5067 duplicated data rows\n* The transformation log is carried out on features with numerical data values to convert them to normal \/ almost normal distributions, this is done because there are several features that have a skewed data distribution.\n* Finally, we normalized so that the scale of each numeric feature has the same scale and it is hoped that it can simplify the process of learning the machine learning model data that we created.","c670fd11":"Fitting model to train set","621f76ca":"## Describe Data","d913ee68":"**Simple EDA**\n\n**In some boxplot charts it can be seen that the data distribution is uneven and has lots of outliers and it can also be seen that the data distribution on the `No_Tags`, `Desc_len`, and `len_title` features is quite skewed.**\n\n**Then when we look at the heatmap, we can see that there are several features that are positively correlated with the views of a video, such as `likes`, `dislikes`, `comment_count`, `No_tags`, and `desc_len`, so maybe some of these features are of feature importance for doing machine learning modeling**","6110d3df":"# Machine Learning-Youtube Views Prediction","ec60510d":"## Normalization","4a2c7c89":"**Team Behind This Project**<br>\n* As the project from Data Science in [Rakamin_Academy](https:\/\/rakamin.com\/), this model was built from a team called **Hi5**<br>\n\n**The Team Member**<br>\n**1. Herdin Surya Dwi Putra**<br>\nemail: herdinsurya@gmail.com<br>\nphone: +6281272243710<br>\nLinkedin: www.linkedin.com\/in\/herdinsurya<br>\n\n**2. Jomen Pardede (me)**<br>\nemail: jomenpardede@gmail.com<br>\nphone: +6282272055285<br>\nLinkedin: www.linkedin.com\/in\/jomen-pardede<br>\n\n**3. Mia Maryasha**<br>\nemail: maryashamia@gmail.com<br>\nphone: +6281285566246<br>\nLinkedin: www.linkedin.com\/in\/mia-maryasha-738723173\/<br>\n\n**4. M. Jayus Abror**<br>\nemail: mhmdabror1994@gmail.com<br>\nphone: +6282211101091<br>\nLinkedin: www.linkedin.com\/in\/muhammad-abror-b42809119<br>\n\n**5. Moh. Ardiansyah Gonti**<br>\nemail: @gmail.com<br>\nphone: +6285717368356<br>\nLinkedin: www.linkedin.com\/in\/mohammad-ardiansyah-gonti-266706110\/<br>\n\n**Our Friendly Mentor: Ade Irawan**<br>\nemail: @gmail.com<br>\nphone: +6281906486000<br>\nLinkedin: <br>\n","234ee31e":"### Multivariate Analysis","4dcd1480":"# Split Train & Test\n","4ee84ca5":"# Conclusion","b2babced":"**Main Instructions**\n* Exploratory analysis from the data, create some visualization to describe the data\n* Describe the pre-processing steps, also the reason behind them\n* Split the data into training and testing with optional portion\n* Build the models with matching hyperparameter tune, choose the best model, also the reason behind that\n* Test the model with data test","fbaee967":"### Statistical Numerical Data","3465d692":"### Univariate Analysis","b0c1eebd":"### Statistical Categorical Data","87b0fbaa":"# Fit Elastic Net Regularization Model","e8512e80":"## Load Data","7dbab38b":"## Filling Missing Column with Mode","7ab49e6d":"# Fit Ridge Regularization Model","eab77de7":"**Based on the experiments that have been carried out, there are several models that are good for determining the views of Youtube videos** <\/br>\n* Random Forest with MAE 0.00, RMSE 0.01, and R2 of 0.96 makes Random Forest the best model so far\n* Decision Tree is the second best sequence model after Random Forest with a slightly smaller R2 value of 0.92\n* Elastic Net is the third best model with MAE 0.01, RMSE 0.01, and R2 0.80 values\n* Ridge Regularization is the next best model with MAE values of 0.01, RMSE 0.01, and R2 of 0.79\n* a very influential feature is the number of likes and dislikes of a video","a05bc85f":"In the data distribution of numerical features, it can be seen that there are several features that are positively skewed and `len_title` feature that is negatively skewed.","228da321":"**Conclusion**<\/br>\n* Dataframe have 18 columns\n* Dataframe have 37924 rows\n* Total categorical columns are 10 columns\n* Total numerical columns are 8 columns\n* `views` column is the target for this dataset","908e23c5":"**The model's performance is very bad because the R2 value is below the 0 value, which is -6.85**","57c11065":"## Randomized Search\n","cf102643":"# Load and Describe Data","2def48b3":"**Categorical Data Conclusion**<\/br>\n* Data in `trending_date` has 429 unique values\n* Data in `title` has 16466 unique values\n* Data in `channel_title` has 1548 unique values\n* Data in `publish_time` has 12494 unique values\n* Data in `tags` has 12463 unique values\n* Data in `comments_disabled` has 2 unique values with majority is False value\n* Data in `ratings_disabled` has 2 unique values with majority is False value\n* Data in `video_error_or_removed` has 2 unique values with majority is False value\n* Data in `description` has 13992 unique values\n* Data in `publish_date` has 219 unique values","c41e93db":"There are 3 features that have a strong positive correlation to `views`, namely the `likes`, `dislikes`, and `comment_count` features","7a8c986b":"# Exploratory Data Analysis","36a0c60f":"It can be seen in the boxplot graph above that the `view`, `likes`, `dislikes`, `comment_count`, `o_tags`, and `desc_len` features have many outliers so that logarithmic transformations are needed for these features.","72c70671":"the `publish_time`, `publish_date`, `description`, `tags`, `title`, `channel_title` features are removed because they have a very large number of unique values, so it is assumed that each row of data has a different value.","da5da57e":"## Evaluation","3abd02f6":"## Duplicate Values","41808b25":"## Statistical Summary","b8b4775d":"# Fit Random Forest Model","33ac5e00":"**Conclusion from Statistical Numerical Data**<br>\n\n* The distribution of data for feature `views`, `likes`, `dislikes`, `comment_count`, `desc_len`, and `len_title` looks skewed *(mean & median are not close enough)*","0e541a59":"**About dataset**\n* This dataset is obtained from [Trending YouTube Video Statistics | Kaggle](https:\/\/www.kaggle.com\/datasnaek\/youtube-new)","e6159066":"# Team","be89c5f9":"## Randomized Search","fab90369":"# Fit Support Vector Regressor Model","8c47cf5e":"# Fit Decision Tree Model","dfa682ac":"## Outliers","2ad58786":"## Predict","dfc38575":"# Fit Lasso Regularization Model","9ad3280c":"Predict the test set","eb52db9e":"## Drop Column \n","e75febf1":"## Fit Simple Linear Regression Model","877ce678":"## Filling Missing Column with Median\n"}}