{"cell_type":{"b4aa0f54":"code","1c602f07":"code","9c5ec56d":"code","899dd110":"code","7bd1b19a":"code","067fa21b":"code","46f3ad73":"code","590330cd":"code","ecdd34cf":"code","900abd5c":"code","9bbe347c":"code","14f0673a":"code","ff6234ad":"markdown","e87f6517":"markdown","585b32a8":"markdown","50ae3622":"markdown","4c32f506":"markdown","6d4fc004":"markdown","c43fe607":"markdown","7d8db10f":"markdown","c1dd2298":"markdown","4b92405a":"markdown","7ff0bfd1":"markdown"},"source":{"b4aa0f54":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\nprint(df.head())\nprint(f\"Shape of data: {df.shape}\")","1c602f07":"#check for null values\ndf.isnull().sum()","9c5ec56d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nplt.figure();\n\ndf[['age', 'bmi', 'children', 'charges']].diff().hist(color=\"r\", alpha=0.8, bins=50, figsize=(12, 6));","899dd110":"import plotly.express as px\nfig = px.box(df['charges'], color = df['sex'],points=\"all\")\nfig.show()","7bd1b19a":"fig = px.box(df['charges'], color = df['smoker'],points=\"all\")\nfig.show()","067fa21b":"fig = px.box(df['charges'], color = df['region'],points=\"all\")\nfig.show()","46f3ad73":"fig = px.scatter_matrix(df, color = 'charges')\nfig.show()","590330cd":"from sklearn.preprocessing import LabelEncoder\n\nfor c in df.columns:\n    if df[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)\n        \n        \ndisplay(df.head())","ecdd34cf":"X = df.drop(['charges'], axis = 1)\ny = df['charges']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","900abd5c":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n","9bbe347c":"lr = LinearRegression()\n\nknn = KNeighborsRegressor(n_neighbors=10)\n\ndt = DecisionTreeRegressor(max_depth = 3)\n\nrf = RandomForestRegressor(max_depth = 3, n_estimators=500)\n\nada = AdaBoostRegressor( n_estimators=50, learning_rate =.01)\n\ngbr = GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate =.2)\n\nxgb = XGBRegressor(max_depth = 3, n_estimators=50, learning_rate =.2)\n\ncb = CatBoostRegressor(learning_rate =.01, max_depth =5, verbose = 0)\n\nregressors = [('Linear Regression', lr), ('K Nearest Neighbours', knn),\n               ('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n              ('Gradient Boosting Regressor', gbr), ('XGBoost', xgb), ('catboost', cb)]\n","14f0673a":"from sklearn.metrics import r2_score\n\nfor regressor_name, regressor in regressors:\n \n    # Fit regressor to the training set\n    regressor.fit(X_train, y_train)    \n   \n    # Predict \n    y_pred = regressor.predict(X_test)\n    accuracy = round(r2_score(y_test,y_pred),1)*100\n    \n\n   \n    # Evaluate  accuracy on the test set\n    print('{:s} : {:.0f} %'.format(regressor_name, accuracy))\n    plt.rcParams[\"figure.figsize\"] = (20,8)\n    plt.bar(regressor_name,accuracy)\n    ","ff6234ad":"**Median charges for female is 9412.963k dollars, Median charges for male is 9369.616k dollars**\n\n**The upper fence charges for male is 40.27k dollars which is much higher than the upper fence charges for female which is 28.47k dollars**\n\n**Outliers exist both in male and female charges**","e87f6517":"Scatter plot matrix of the dataframe","585b32a8":"**Median charges for smoker is significantly high 34.45k dollars, and that of non smoker is 7345.40k dollars**","50ae3622":"# Model training & Testing","4c32f506":"**Highest accuracies given by are Gradient Boosting Regressor, XGBoost, CatBoost, RandomForest, Decision Tree models**\n\n# **Upvote if you like it, this motivates us to produce more notebooks for the community**","6d4fc004":"# Spliting and scaling","c43fe607":"There is no significant visible discrimination in charges based on region.","7d8db10f":"# We need to encode the categoricals.","c1dd2298":"_Columns children, charges, children, bmi follows fairly normal distribution._","4b92405a":"No null values,proceed!","7ff0bfd1":"![](http:\/\/pioneerinstitute.org\/wp-content\/uploads\/healthcare_costs_scrabble.jpg)\n\n**Column Descriptions**\n\nage: age of primary beneficiary\n\nsex: insurance contractor gender, female, male\n\nbmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n\nchildren: Number of children covered by health insurance \/ Number of dependents\n\nsmoker: Smoking\n\nregion: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n\ncharges: Individual medical costs billed by health insurance"}}