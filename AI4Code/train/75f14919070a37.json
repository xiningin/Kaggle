{"cell_type":{"6a22efa6":"code","8870ac82":"code","5454d8ee":"code","fe9351b4":"code","6747a732":"code","bbf5628c":"code","ab7c7cd3":"code","c88f2ebe":"code","72dab2ce":"code","16356e86":"code","cf595ff3":"code","c3f34db2":"code","e93b7143":"code","dbe4438a":"code","dc0e498c":"code","11df0786":"code","fb883a72":"code","132b586e":"code","1daee84c":"code","f0ec9ece":"code","634d76ca":"code","aa90ab13":"code","55636d63":"code","15f86957":"code","311a33d2":"code","f9a73da4":"code","6f0f6f42":"code","470e4e03":"code","59900197":"code","96546ea6":"code","ee4539d8":"code","9a329c48":"code","60344b36":"code","2e2a0524":"code","1de02eef":"code","29d15214":"code","20b28b5c":"markdown","ff5b0249":"markdown","62091386":"markdown","6a35d8e9":"markdown","bf3006b0":"markdown","c4954d4b":"markdown","9edc3ee7":"markdown","a3566946":"markdown","e1c2fbff":"markdown","14021629":"markdown","0d527efc":"markdown","4fdb4d85":"markdown","7acc1f1f":"markdown","40bbb979":"markdown","935df79f":"markdown","9df2c302":"markdown","d42c4a99":"markdown","cd1b76da":"markdown","38670c77":"markdown","ac54db52":"markdown","86a872b5":"markdown","feba5d18":"markdown","0427870c":"markdown","c7a3e594":"markdown","bdd268ce":"markdown","25b75e94":"markdown","84f9a86d":"markdown","aa5a9a67":"markdown","6c987b9c":"markdown","efe42c63":"markdown","6b228faa":"markdown","96a74aa6":"markdown","4dcaa6af":"markdown","d48f38ce":"markdown","9394d413":"markdown","d22aff57":"markdown","ce2738bf":"markdown","dd46541e":"markdown","dc017a89":"markdown","909f190f":"markdown"},"source":{"6a22efa6":"# the following line will make plots appear with out calling plt.show()\n%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # I\/O of data sets\nfrom zipfile import ZipFile # woking with zip archives\nimport os # directory operations\n\n# packages for visualization\nimport ipywidgets as iw\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\nimport matplotlib.image as im \nfrom keras.preprocessing import image\n\n# keras for deep learning\nfrom keras import layers, models, optimizers, metrics \nfrom keras.preprocessing.image import ImageDataGenerator ","8870ac82":"zp_filenames = ['Testing.zip','Training.zip','Validation.zip']","5454d8ee":"# using list comprehension to build a list for zipped file with their path names.\nzp_filenamesWithpath = [os.path.join('..\/input\/kaggle-for-deep-learning-p-1-getting-data-ready',k) for k in zp_filenames]","fe9351b4":"for k in zp_filenamesWithpath: # looping over the files that need to be unzipped\n    \n    # extracting the zipfiles to the current directory\n    with ZipFile(k,'r') as zp: \n        zp.extractall()","6747a732":"# using dict comprehension to store paths to a dictionary\npath = {i: os.path.join('working', i) for i in os.listdir('working')} \n\npath","bbf5628c":"# Reading the '.csv' file to a dataframe.\nCnnP = pd.read_csv('..\/input\/kaggle-for-deep-learning-p-2-visualization-eda\/CNNParameters.csv', index_col = 0)","ab7c7cd3":"# Converting the data frame to a dictionary and printing it's values.\nCnnP = CnnP.to_dict()['0']\nprint(CnnP)","c88f2ebe":"# Initializing a sequential model\nmodel = models.Sequential()\n\n# Adding convolution and maxpooling layers\nmodel.add(layers.Conv2D(32, (3, 3), \n                        activation='relu', \n                        input_shape=(100, 100, 3))) # convolution layer with an output of 32\nmodel.add(layers.MaxPooling2D((2, 2))) # max pooling layer\n\nmodel.add(layers.Conv2D(64, (3, 3), \n                        activation='relu')) # convolution later\nmodel.add(layers.MaxPooling2D((2, 2))) # maxpooling layer\n\nmodel.add(layers.Conv2D(128, (3, 3), \n                        activation='relu')) # convolution layer\nmodel.add(layers.MaxPooling2D((2, 2))) # maxpooling layer\n\nmodel.add(layers.Conv2D(128, (3, 3), \n                        activation='relu')) # convolution layer\nmodel.add(layers.MaxPooling2D((2, 2))) # max pooling layer\n\n# adding a flatten and a dense layer\nmodel.add(layers.Flatten()) # flatten layer and a dropout layer\nmodel.add(layers.Dense(512, \n                       activation='relu')) # dense layer\n\n# output layer with 'softmax' activation as there are multiple classes.\nmodel.add(layers.Dense(CnnP['No classes'], \n                       activation='softmax'))","72dab2ce":"# Checking the model architecture\nmodel.summary()","16356e86":"model.compile(loss = 'categorical_crossentropy',\n              optimizer = optimizers.Adam(lr=1e-4),\n              metrics = [metrics.categorical_accuracy])","cf595ff3":"# Initializing the ImageDataGenerator object. We set rescale to 1\/255 to normalize pixel values to be in range 0,1\ndatagen = ImageDataGenerator(rescale=1.\/255)","c3f34db2":"# Using the generator to get training images ready for the network\ntrain_generator = datagen.flow_from_directory(path['Training'],\n                                              target_size = (100,100),\n                                              batch_size = CnnP['Training batch size'],\n                                              class_mode = 'categorical')","e93b7143":"# Using the generator to get validation images ready for the network\nvalidation_generator = datagen.flow_from_directory(path['Validation'],\n                                                   target_size = (100,100),\n                                                   batch_size = CnnP['Validation batch size'],\n                                                   class_mode = 'categorical')","dbe4438a":"# Fitting the model\nhistory = model.fit_generator(train_generator,\n                              steps_per_epoch = CnnP['Training steps\/epoch'],\n                              epochs = 50,\n                              validation_data = validation_generator,\n                              validation_steps = CnnP['Validation steps\/epoch'])","dc0e498c":"model_json = model.to_json() # serialize model to JSON\n# writing the model as a json file\nwith open(\"fruit_classification_model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save_weights(\"fruit_classification_weights.h5\") # saving model weights as HDF5\nprint(\"Model Saved\")","11df0786":"def plotModel(history):\n    fg, (ax1, ax2) = plt.subplots(2,1,figsize = (15, 15)) # creating subplots\n    # calculating percentage accuracy and loss for training and validation\n    acc = np.array(history.history['categorical_accuracy'])*100\n    val_acc = np.array(history.history['val_categorical_accuracy'])*100\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n \n    epochs = range(1, len(acc) + 1) # calculating x-axis values\n    fnt = {'fontsize': 20,\n           'fontweight' : 'medium',\n           'verticalalignment': 'baseline'} # dictionary to control font for labels & title\n\n    # plotting accuracy\n    ax1.plot(epochs, acc, 'bo', label='Training acc')\n    ax1.plot(epochs, val_acc, 'b', label='Validation acc')\n    ax1.set_title('Training and validation accuracy', fnt)\n    ax1.set_xlabel('Epoch number', fnt, labelpad = 20)\n    ax1.set_ylabel('Accuracy (%)', fnt)\n    ax1.legend(fontsize = 20)\n\n    # plotting loss\n    ax2.plot(epochs, loss, 'bo', label='Training loss')\n    ax2.plot(epochs, val_loss, 'b', label='Validation loss')\n    ax2.set_title('Training and validation loss', fnt)\n    ax2.set_xlabel('Epoch number', fnt, labelpad = 20)\n    ax2.set_ylabel('Loss', fnt)\n    ax2.legend(fontsize = 20)\n\n    # changing font size of tick labels\n    for tick in ax1.xaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    for tick in ax2.xaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    for tick in ax1.yaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    for tick in ax2.yaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    return fg\n    \nfg = plotModel(history)    \nfg.savefig('modelResults.png')# saving the figure","fb883a72":"# Creating the test_generator\ntest_generator = datagen.flow_from_directory(path['Test'],\n                                             target_size = (100,100),\n                                             batch_size = 1,\n                                             class_mode = 'categorical') # testing in bathches of 5","132b586e":"# Calculating the test loss and accuracy\ntest_loss, test_acc = model.evaluate_generator(test_generator, steps = 3155)","1daee84c":"# printing the test loss and accuracy\nprint(\"Test accuracy for the model is %0.2f %s. \\n\" % (test_acc*100, '%'))\nprint('Test loss for the model is %0.2f .' % test_loss)","f0ec9ece":"model2 = models.Sequential()\n\nmodel2.add(layers.Conv2D(32, (3, 3), \n                        activation='relu', \n                        input_shape=(100, 100, 3)))\nmodel2.add(layers.MaxPooling2D((2, 2)))\nmodel2.add(layers.Conv2D(64, (3, 3), \n                        activation='relu'))\nmodel2.add(layers.MaxPooling2D((2, 2)))\nmodel2.add(layers.Conv2D(128, (3, 3), \n                        activation='relu'))\nmodel2.add(layers.MaxPooling2D((2, 2)))\nmodel2.add(layers.Conv2D(128, (3, 3), \n                        activation='relu'))\nmodel2.add(layers.MaxPooling2D((2, 2)))\n\nmodel2.add(layers.Flatten()) \n\nmodel2.add(layers.Dropout(0.5)) # adding a drop out layer\n\nmodel2.add(layers.Dense(512, \n                       activation='relu'))\nmodel2.add(layers.Dense(CnnP['No classes'], \n                       activation='softmax')) ","634d76ca":"# Making an image data generator object with augmentation \ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   rotation_range=40,\n                                   width_shift_range=0.2,\n                                   height_shift_range=0.2,\n                                   shear_range=0.2,\n                                   zoom_range=0.2,\n                                   horizontal_flip=True,\n                                   fill_mode='nearest')","aa90ab13":"# This list contains names of images in folder named orange\nimgs_orange = [os.path.join(path['Test'],'Orange',img) for img in os.listdir(os.path.join(path['Test'],'Orange'))]","55636d63":"img = image.load_img(imgs_orange[0])","15f86957":"img_ar = image.img_to_array(img) # convert image to a numpy array\nimg_ar = img_ar.reshape((1,) + img_ar.shape) # reshape the image","311a33d2":"# Creating a figure with 4 subplots\nfg, ax = plt.subplots(4, 1, figsize = (20,20))\n\n# variable for referring to an axis\nk = 0 \n\n# Using train_datagen to generate randomly transformed images\nfor batch in train_datagen.flow(img_ar, batch_size=1):\n    \n    ax[k].imshow(image.array_to_img(batch[0]))\n    ax[k].axis('off')\n    k += 1 # updating value of k\n    if k == 4: # we only want to see 4 images (we have 4 subplots so lets break if k is equal to 4)\n        break\n        \nfg.savefig('augmentedImages.png')# saving the figure","f9a73da4":"datagen = ImageDataGenerator(rescale=1.\/255) ","6f0f6f42":"# Using the generator for training directory\ntrain_generator = train_datagen.flow_from_directory(path['Training'],\n                                              target_size = (100,100),\n                                              batch_size = CnnP['Training batch size'],\n                                              class_mode='categorical')","470e4e03":"# Using the generator validation directory\nvalidation_generator = datagen.flow_from_directory(path['Validation'],\n                                                   target_size = (100,100),\n                                                   batch_size = CnnP['Validation batch size'],\n                                                   class_mode = 'categorical')","59900197":"model2.compile(loss = 'categorical_crossentropy',\n              optimizer = optimizers.Adam(lr=1e-4),\n              metrics = [metrics.categorical_accuracy])","96546ea6":"# Fitting the model\nhistory2 = model2.fit_generator(train_generator,\n                              steps_per_epoch = CnnP['Training steps\/epoch'],\n                              epochs = 50,\n                              validation_data = validation_generator,\n                              validation_steps = CnnP['Validation steps\/epoch'])","ee4539d8":"model_json = model2.to_json() # serialize model to JSON\n# writing the model as a json file\nwith open(\"fruit_classification_model_dropout.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel2.save_weights(\"fruit_classification_weights_augmented.h5\") # saving model weights as HDF5\nprint(\"Model Saved\")","9a329c48":"fg = plotModel(history2)    \nfg.savefig('modelResultsAugmentation.png')# saving the figure","60344b36":"# Calculating the test loss and accuracy\ntest_loss2, test_acc2 = model2.evaluate_generator(test_generator, steps = 3155)","2e2a0524":"# printing the test loss and accuracy\nprint(\"Test accuracy for the model is %0.2f %s. \\n\" % (test_acc2*100, '%'))\nprint('Test loss for the model is %0.2f .' % test_loss2)","1de02eef":"import shutil\nshutil.rmtree('working')","29d15214":"fg, (ax1, ax2) = plt.subplots(2,1,figsize = (15, 15)) # creating subplots\n\n# calculating percentage accuracy and loss for training and validation\nacc1 = np.array(history.history['categorical_accuracy'])*100\nval_acc1 = np.array(history.history['val_categorical_accuracy'])*100\n\nloss1 = history.history['loss']\nval_loss1 = history.history['val_loss']\n\nacc2 = np.array(history2.history['categorical_accuracy'])*100\nval_acc2 = np.array(history2.history['val_categorical_accuracy'])*100\n\nloss2 = history2.history['loss']\nval_loss2 = history2.history['val_loss']\n \nepochs = range(1, len(acc1) + 1) # calculating x-axis values\n\nfnt = {'fontsize': 20,\n        'fontweight' : 'medium',\n        'verticalalignment': 'baseline'} # dictionary to control font for labels & title\n\n# plotting accuracy\nax1.plot(epochs, acc1, 'bo', label='Training acc')\nax1.plot(epochs, val_acc1, 'b', label='Validation acc')\n\nax1.plot(epochs, acc2, 'ro', label='Training acc, augmented')\nax1.plot(epochs, val_acc2, 'r', label='Validation acc, augmented')\n\nax1.set_title('Training and validation accuracy', fnt)\nax1.set_xlabel('Epoch number', fnt, labelpad = 20)\nax1.set_ylabel('Accuracy (%)', fnt)\nax1.legend(fontsize = 20)\n\n# plotting loss\nax2.plot(epochs, loss1, 'bo', label='Training loss')\nax2.plot(epochs, val_loss1, 'b', label='Validation loss')\n\nax2.plot(epochs, loss2, 'ro', label='Training loss, augmented')\nax2.plot(epochs, val_loss2, 'r', label='Validation loss, augmented')\n\nax2.set_title('Training and validation loss', fnt)\nax2.set_xlabel('Epoch number', fnt, labelpad = 20)\nax2.set_ylabel('Loss', fnt)\nax2.legend(fontsize = 20)\n\n# changing font size of tick labels\nfor tick in ax1.xaxis.get_major_ticks():\n    tick.label.set_fontsize(20)\n    for tick in ax2.xaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    for tick in ax1.yaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    for tick in ax2.yaxis.get_major_ticks():\n        tick.label.set_fontsize(20)\n    ","20b28b5c":"Creating image generator to use for validation and test data set. Here, we only rescale the images.","ff5b0249":"1. ** The accuracy increase from 96.7 % to 98 .6 %, and the loss decreases from 0.20 to 0.03**. \nAt the end, we just need to remove the extracted files from the working directory.","62091386":"## Suplimentary figure\nTo observe the effect of data augmentation and addition of a drop out layer, we can plot loss and accuracy for both models on the same plot. This will highlight the change. ","6a35d8e9":"## Getting data ready\nTo use the image data we need transform the data in a way which can be input to the netwrok. In brief we will normalize pixel values to be between 0 and 1 and slit data into batches. We will use ImageDataGenerator class for doing this. ","bf3006b0":"Now, we need to perform two steps: \n1. Convert the image to a numpy array and,\n2. reshape the image to 4 dimensions. These steps are required for keras image data generator to be able to generate images.","c4954d4b":"## Building and compiling the convolution neural network\nWe now build and compile the network.","9edc3ee7":"We start with building the model. The model will be sequencial and will have 4 convolution and maximum pooling layers. At the end we will have a flatten layer. The output of the final layer should be equal to the total number of classes. This value is stored in the dictionary CnnP and has the key 'No classes'. Also, the final layer will have a 'softmax' activation as there are multiple classes.","a3566946":"First, let's make a list of the names of the zip files that we need to unzip.","e1c2fbff":"# Using convolution neural network with to classify fruit images\nThis is **Part 3** of the fruit classification project. In [**Part 1**](https:\/\/www.kaggle.com\/rehanali88\/kaggle-for-deep-learning-p-1-getting-data-ready) we split the data into training, validation and test sets. We saved these folders in zip format. In [**Part 2**](https:\/\/www.kaggle.com\/rehanali88\/kaggle-for-deep-learning-p-2-visualization-eda), we visually explored the data and calculated parameters to use in building convolution neural network.   \nthis part we will use the split data and the calculated parameters from the previous part to build a convolution neural network for fruit classification. ","14021629":"Now we plot loss and accuracy vs epoch number for validation and training data. As we will be using this code again, it will be helpfull if we define a function for this.","0d527efc":"## Why data augmentation\nBefore moving forward and building our model let's first understand the purpose of data augmentation. We want to overcome overfitting. To do this, one approach we can take is to generate new images from the same dataset that have been transformed in a certain way. This transformation involves image rotation, image shear, image shift etc. Summed up in a word, this process is called 'data augmentation'.    \nThe following ImageDataGenerator object will create a generator object which will perform data augmentation for us.","4fdb4d85":"## Testing the model\nNow let's test the model on the test data set. As before we will prepare the images by generating batches of images for the test data. Then we will use 'evaluate_generator' method to calculate loss and accuracy for the test data.","7acc1f1f":"## Summarizing important results\n1. Validation loss shows a slight but steady increase as epoch number increases along with high variability.\n2. Validation loss and accuracy never reach the values close to training loss and accuracy.   \n3. *We might be overfitting the data.*\n4. The accuracy on the training data set and validation data set is around 97.59 %.\n      \nNow we will tweak the model architecture and we will use image augmentation to see if we are able to overcome overfitting. We will also see if this improves  model accuracy on test data.","40bbb979":"We have extracted the compressed folders to our working directory and stored their paths to a dictionary. We will use these paths when we build the network.","935df79f":"We can check the model architecture by printing the model summary.","9df2c302":"Now, we extract the files in zipped folders to this directory.","d42c4a99":"## Moving forward and generating images for training and validation dataset\nNow we can use our created 'train_datagen' to generate batches of images for the training data set. But ** we can not use this generator for validation and testing images. These images should not be transformed in any way**. We only need to rescale them so that pixel values are between 0 and 1.","cd1b76da":"## Why a drop out layer\nThe problem that we are trying to solve is our model memorizing patterns. Essesntially, a drop out layer randomly removes a sub set of neurons. We are basically introducing noise to the data so that our model doesn't learn patterns that are not significant.","38670c77":"## Summarizing important results\n1. Validation loss ** DOES NOT** show a steady increase as epoch number increases.Alsom the variability is low.\n2. Validation loss and accuracy reach the values close to training loss and accuracy.   \n3. *We are not overfitting the data.*\n      \nBy tweaking the model architecture and using image augmentation we have overcome overfitting. Now let's evaluate model accuracy on test data.","ac54db52":"## Getting values for different parameters that will be used in building the network\nIn part 2 we calculated and saved the parameters that we need to build the model. Now we will use read that file and print it's contents.","86a872b5":"## Visualizing data augmentation\nWe can see data augmentation in action. For this we will chose an image from our data and use 'train_datagen'  to generate augmented images which we will then plot.","feba5d18":"First let's save the model and it's weights.","0427870c":"## Extracting zipped files\nWe need to extract the images from test, training and validation sets. This part is exactly the same as 'Extracting zipped files' in part 2.","c7a3e594":"## Fitting the model\nNow, we can fit the model using the method 'fit_generator'. We already have all the parameters that we need to fitting the model.","bdd268ce":"First let's use an image to augment. For this example, we will chose an orange from the training data set.","25b75e94":"## ConvNet with data augmentation and drop outlayer\nNow we will build a new model with two changes:\n1. We will modify the model architechture, with the major change being the addition of a dropout layer.\n2. We will use data augmentation to transform the training data.","84f9a86d":"To get our data ready we need to know the path to the directory where the images are stored. We have already stored the paths to a directory named path. We also need batch_size values which we have stored in CnnP dictionary. Using these value's we get the data ready for input to the network.","aa5a9a67":"The images above show data augmentation in action. It is just generation of a number of images by performing random transformations on a single image. ","6c987b9c":"## Importing required packages\nLet's first import the packages we will need for our analysis. The comments show the usage of every package we are importing.","efe42c63":"Let's select the first image in the 'Orange' directory' and load it.","6b228faa":"## Insights from the plots\nThe following points are to be noted from the plots above:\n1. After epoch number 10, both the training accuracy and loss do not change alot.\n2. The validation loss and accuracy never reach the traing loss and accuracy.\n3. After epoch number 10, the validation loss seems to increase as the epoch number increases. **What this shows is that we might be overfitting the data. In other terms, the model might be memorizing instead of learning!. We will solve this later using image augmentation and by modifying the model architecture**.","96a74aa6":"Let's assign the path's to the different sets to a dictionary.","4dcaa6af":"As before we save the model and it's weights and plot the results.","d48f38ce":"## Compile and fit the model","9394d413":"Now we compile the model. Here we need to define a loss function. This function provides a value which is a measure of disparity between the output of the network and the original labels of the data. As we have multiple classes we will use categorical crossentropy. \nWe also need to provide an optimizer. This changes the weights of different layers in the network based on the input it gets from the loss function. The goal of the optimizer is to move the weights in a direction that minimizes loss. We will use Adam as the optimizer and start at a low learning rate of 0.0001.\nFinally, the model will also output a measure of model performance. We will use categorical accuracy as a measure of model performance.","d22aff57":"Neural network is now ready to be used. We just need to get our data ready.","ce2738bf":"## Saving and plotting\nWe will now save the model. This is important as we can use this model to train related images, for example images of vegetables. As the features for fruits and vegetables are similar we can freeze some convolution and max pooling layers and train the model on vegetables. More importantly the same model can be used to train new classes of images. This will save alot of training time and will require a smaller number of samples. We also need to plot loss and accuracy vs epoch number to test for overfitting.","dd46541e":"Using this list, we make a list of zip folders names including their path.","dc017a89":"Now, we can see data augmentation in action.","909f190f":"We will use these parameters, along with the extracted files to build and compile the neural network."}}