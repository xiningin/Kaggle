{"cell_type":{"93786e30":"code","a74fbd9f":"code","a1dba6b2":"code","8889a0ff":"code","2347c2a2":"code","ec63b1e2":"code","918f6723":"code","38df8757":"code","7b8de940":"code","ee1add65":"code","bcf76dda":"code","a81b7aee":"code","dce09018":"code","805d5508":"code","45932401":"code","691cd656":"code","8104087f":"code","940f67e2":"code","c66fd6cf":"code","370fa744":"code","ef601836":"code","e1047867":"code","7497a313":"code","bba7c074":"code","22b1fa20":"code","93e49673":"code","37cb5cb5":"code","a2a4d383":"code","2b422c33":"code","0a4b90ba":"code","2cbdfe71":"code","1dbdb3b7":"code","ed3d6a1a":"code","5df9d3b7":"code","dee570e9":"code","59943be9":"code","6755f017":"code","ba8bbe94":"code","5c5fa7f3":"code","2048c9c3":"code","1a965575":"code","79288a02":"code","a04de15b":"code","f27cc200":"code","306ad72e":"code","014e79eb":"code","60cd5739":"code","c8ed1d3b":"code","cd8904eb":"code","f9285564":"code","db1b206f":"code","3bef848c":"markdown","c48a3e56":"markdown","99d47957":"markdown","7c60e8f9":"markdown","6ed667d2":"markdown","7bb4cf53":"markdown","38b8d5c4":"markdown","c8adbeb6":"markdown","6c77836b":"markdown","f24e3268":"markdown","d2aef3f5":"markdown","275938a9":"markdown","084b0cc1":"markdown","cdee6729":"markdown","03e4df3e":"markdown","19d18987":"markdown","3abe7244":"markdown","491fe73b":"markdown","bca07b79":"markdown","bf8d6be7":"markdown","a8b979b6":"markdown","e5c7308b":"markdown","1df7eedc":"markdown","e01e43bd":"markdown","8a0f8712":"markdown","70276e69":"markdown","0c497901":"markdown","8cd98c36":"markdown","364ccdf9":"markdown","4f3e84aa":"markdown","4b190791":"markdown","51ed9794":"markdown","81fc78a9":"markdown","c1e13878":"markdown","98f47ee0":"markdown","f374f462":"markdown","a718ae53":"markdown","ecc4c8de":"markdown","cd72af7a":"markdown","0d111840":"markdown","b5f5b3b8":"markdown","f82ab134":"markdown","5aeb967a":"markdown","b10087e9":"markdown","d7ccddaf":"markdown","36ea7eb1":"markdown","f4b92733":"markdown","f8b95140":"markdown","ac9cec23":"markdown","f4ac4bab":"markdown","68ce82e6":"markdown","4a427c96":"markdown","578f8d99":"markdown","cb459fff":"markdown","f10d563d":"markdown","b295af78":"markdown","dc0671ab":"markdown","63a0fbe8":"markdown","4a7334a2":"markdown","09368f67":"markdown","9034cb89":"markdown","7586cbfd":"markdown","33607ed6":"markdown","03f3de2b":"markdown","acf823d4":"markdown","6ffe589b":"markdown","69c411b3":"markdown","a00ef0c9":"markdown","214186a6":"markdown","098a8206":"markdown","8ed51f35":"markdown","6fd2c5f4":"markdown"},"source":{"93786e30":"# Import Packages\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization tool\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\n#independence test\nimport scipy\n\n# Data Processing\nfrom sklearn.preprocessing import MinMaxScaler\n\n#imbalanced data\nfrom imblearn.combine import SMOTETomek\n\n# evaluation models\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.feature_selection import RFE\nfrom yellowbrick.classifier import DiscriminationThreshold\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n","a74fbd9f":"#Load Data \ndataset = pd.read_csv('..\/input\/data_v1.0.csv', delimiter=',')\ndataset = dataset.drop(\"Unnamed: 0\",1) #Not Useful Column","a1dba6b2":"print ('\\033[1m' +\"Rows     : \" +'\\033[0m' ,dataset.shape[0])\nprint ('\\033[1m' + \"Columns  : \"  +'\\033[0m',dataset.shape[1])\n\nprint('\\033[1m'+\"Types of variables :\\n\" +'\\033[0m',dataset.dtypes)\nprint( '\\033[1m'+\"Non Missing Values :\\n\" +'\\033[0m',dataset.count())","8889a0ff":"dataset.head(3) # preview the data","2347c2a2":"dataset.isnull().sum() #Delete Missing Values (<1% of Total Obs.)","ec63b1e2":"data_wrangling= dataset.dropna()\nonly_na = dataset[~dataset.index.isin(data_wrangling.index)] # Check Observations with Missing Values deleted \ndata_wrangling.count()","918f6723":"only_na.head(4)","38df8757":"data_wrangling = data_wrangling.drop_duplicates() #No Duplicates\ndata_wrangling.count()","7b8de940":"data_wrangling.describe()","ee1add65":"# Delete if Age is less than 18, Experience < 0 , \ndata_wrangling = data_wrangling[data_wrangling[\"age\"] > 16] # 454 obs. deleted\ndata_wrangling = data_wrangling[data_wrangling[\"age\"] <= 70] #  obs. deleted\ndata_wrangling = data_wrangling[data_wrangling[\"exp\"] > 0] #2 obs. deleted\ndata_wrangling.count()","bcf76dda":"# If the Note is higher than 100 than 100\ndata_wrangling.loc[(data_wrangling['note'] > 100), 'note'] = 100","a81b7aee":"#Change Date Type\ndata_wrangling['date'] = pd.to_datetime(data_wrangling['date'],format=\"%Y-%m-%d\")\ndata_wrangling['year_candidature']=  data_wrangling['date'].dt.year\ndata_wrangling['month_candidature']=  data_wrangling['date'].dt.month\ndata_wrangling['day_candidature']=  data_wrangling['date'].dt.day","dce09018":"data_wrangling['c_exp'] = pd.qcut(data_wrangling['exp'],3, precision =0)\ndata_wrangling['c_age'] = pd.qcut(data_wrangling['age'], 3, precision =0)\ndata_wrangling['c_note'] = pd.qcut(data_wrangling['note'],4, precision =0)\ndata_wrangling['c_salaire'] = pd.qcut(data_wrangling['salaire'],5, precision =0)\ndata_wrangling.tail(2)","805d5508":"# Copy the dataframe : keep indexes\nscaled_features = data_wrangling.copy()\n \n#Choose the variables to standardize\nfeatures = scaled_features[['age', 'exp','note','salaire']]\n#Standardized\nscaler = MinMaxScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\n\nscaled_features[['age', 'exp','note','salaire']] = features\nscaled_features.rename(columns={'age':'norm_age', 'exp':'norm_exp', 'salaire':'norm_salaire', 'note':'norm_note' }, inplace=True)\n\ndata_cleaning = pd.merge(data_wrangling, scaled_features[['norm_age','norm_exp','norm_salaire','norm_note','index']],\n                 left_on='index',\n                 right_on='index',\n                 how='inner')","45932401":"#Pie Plot\n#labels\nlab = data_cleaning[\"embauche\"].value_counts().keys().tolist()\n#values\nval = data_cleaning[\"embauche\"].value_counts().values.tolist()\n\nfig1, ax1 = plt.subplots()\nax1.pie(val, labels=lab, shadow=True, autopct='%1.0f%%', startangle=90, colors=['#00bf81','#d900b3'])\nax1.axis('equal')\nplt.show()\n# Unbalanced Data","691cd656":"data_cleaning[data_cleaning.embauche==1].describe()","8104087f":"data_cleaning[data_cleaning.embauche==0].describe()","940f67e2":"# Draw a nested boxplot per embauche\nimport warnings; warnings.simplefilter('ignore')\n\nnumeric_features = ['age','exp','salaire','note']    \ntarget = ['embauche']\n\ndef sns_boxplot(data,target,numeric_feature):  \n    fig, ax = plt.subplots(figsize=(7,5))\n    sns.boxplot(x=target, y=numeric_feature,\n                hue=target, palette=[\"#00bf81\", \"#d900b3\"],\n                data=data, showmeans=True)\n    plt.show()\n\nfor i in numeric_features:\n    sns_boxplot(data_cleaning,\"embauche\",i)","c66fd6cf":"# plot Histogram\ndata_embauche1= data_cleaning[data_cleaning[\"embauche\"] == 1]\ndata_embauche0= data_cleaning[data_cleaning[\"embauche\"] == 0]\n\ndef histogram(data_target,data_nntarget,numeric_feature):  \n    f, axes = plt.subplots(1,2, figsize=(11, 5), sharex=True)\n    sns.distplot( data_nntarget[numeric_feature] , color=\"#00bf81\", ax=axes[0],bins=15,kde=True).set_title(\"Embauche = 0\")\n    sns.distplot( data_target[numeric_feature] , color=\"#d900b3\", ax=axes[1],bins=15,kde=True).set_title(\"Embauche = 1\")\n\nfor i in numeric_features:\n    histogram(data_embauche1,data_embauche0,i)","370fa744":"categorical_features = ['c_age','c_exp','c_note','c_salaire'] \n\ndef cat_distribution(data,target,categorical_features):\n    x, y, hue = categorical_features, \"prop\", target\n\n\n    prop_df = (data[categorical_features]\n               .groupby(data_cleaning[hue])\n               .value_counts(normalize=True)\n               .rename(y)\n               .reset_index())\n    prop_df = prop_df.sort_values([categorical_features]).reset_index(drop=True)\n\n    f, ax = plt.subplots(figsize=(8, 6))\n    ax = sns.barplot(x=x, y=y, hue=target, data=prop_df,palette=['#4cd2a6','#e032c2']).set_title(x + \" distribution\")\n\nfor i in categorical_features:\n    cat_distribution(data_cleaning,\"embauche\",i)","ef601836":"categorical_features = ['cheveux','sexe','diplome','specialite','dispo','year_candidature','month_candidature','day_candidature'] \n\ndef cat_distribution(data,target,categorical_features):\n    x, y, hue = categorical_features, \"prop\", target\n\n\n    prop_df = (data[categorical_features]\n               .groupby(data_cleaning[hue])\n               .value_counts(normalize=True)\n               .rename(y)\n               .reset_index())\n\n    f, ax = plt.subplots(figsize=(7, 5))\n    ax = sns.barplot(x=x, y=y, hue=target, data=prop_df,palette=['#4cd2a6','#e032c2']).set_title(x + \" distribution\")\n\nfor i in categorical_features:\n    cat_distribution(data_cleaning,\"embauche\",i)","e1047867":"categorical_variables = ['cheveux','sexe','diplome','specialite','dispo','c_age','c_note','c_salaire','c_exp']\nfinal_result = pd.DataFrame(columns=['statchisq','pvalue','cat_variable'])\n\nfor i in categorical_variables:\n    # Chi-square test\n    ct1= pd.crosstab(data_cleaning['embauche'],data_cleaning[i])\n    colsum=ct1.sum(axis=0)\n    colpct =ct1\/colsum\n    cs1 =scipy.stats.chi2_contingency(ct1)\n\n    # Add the statchisq and pvalue\n    statchisq = cs1[0]\n    pvalue = cs1[1]\n\n    result = [statchisq,pvalue,i]\n    final_result.loc[len(final_result)]=result\n    \n\nfinal_result.sort_values(by=['pvalue'])\n","7497a313":"categorical_variables = ['cheveux','sexe','diplome','specialite','dispo','c_age','c_note','c_salaire','c_exp']\nmfinal_result = pd.DataFrame(columns=['statchisq','pvalue','variable1','variable2'])\n\nfor i in categorical_variables:\n    for j in categorical_variables:\n        # Chi-square test\n        if i != j:\n            ct1= pd.crosstab(data_cleaning[j],data_cleaning[i])\n            colsum=ct1.sum(axis=0)\n            colpct =ct1\/colsum\n            cs1 =scipy.stats.chi2_contingency(ct1)\n\n            # Add the statchisq and pvalue\n            statchisq = cs1[0]\n            pvalue = cs1[1]\n\n            result = [statchisq,pvalue,i,j]\n            mfinal_result.loc[len(mfinal_result)]=result\n\n\nmfinal_result.sort_values(by=['variable1']).head(30) # multicollin\u00e9arit\u00e9 sexe & cheveux \/ sexe& specialite \/ diplome & dispo \/ ","bba7c074":"#The Kruskal-Wallis H-test tests the null hypothesis that the population median of all of the groups are equal. \n#It is a non-parametric version of ANOVA. Since ANOVA has strong assumptions\nnumerical_variables =['age','exp','note','salaire','day_candidature','month_candidature','year_candidature']\n\nfor i in numerical_variables:\n    # compare samples\n    stat, p = scipy.stats.kruskal(data_embauche0[i], data_embauche1[i])\n    print('Statistics=%.3f, p=%.3f' % (stat, p) +\", variable:\" +i)\n    # interpret\n    alpha = 0.05\n    if p > alpha:\n        print('Same distributions (fail to reject H0)')\n    else:\n        print('Different distributions (reject H0)')","22b1fa20":"data_blond =data_cleaning[data_cleaning[\"cheveux\"]==\"blond\"]\ndata_brun = data_cleaning[data_cleaning[\"cheveux\"]==\"brun\"]\ndata_roux =data_cleaning[data_cleaning[\"cheveux\"]==\"roux\"]\ndata_chatain =data_cleaning[data_cleaning[\"cheveux\"]==\"chatain\"]\nstat, p = scipy.stats.kruskal(data_blond[\"salaire\"], data_brun[\"salaire\"],data_roux[\"salaire\"] ,data_chatain[\"salaire\"])\n\nprint('Statistics=%.3f, p=%.3f' % (stat, p) +\" \" +i)\n# interpret\nalpha = 0.001\nif p > alpha:\n    print('Same distributions (fail to reject H0)')\nelse:\n    print('Different distributions (reject H0)')","93e49673":"# calculate the correlation matrix (Spearman-Correlation)\n# Only on Numeric Features\ndata_numeric = data_cleaning[['exp','note','age','salaire','year_candidature','month_candidature','day_candidature']]\n\ndef heatMap(df):\n   #Create Correlation df\n    corr = df.corr(method='spearman')\n    #Plot figsize\n    fig, ax = plt.subplots(figsize=(7, 7))\n    #Generate Color Map, red & blue\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n    #Generate Heat Map, allow annotations and place floats in map\n    sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n    #Apply xticks\n    plt.xticks(range(len(corr.columns)), corr.columns);\n    #Apply yticks\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    #show plot\n    plt.show()\n\nheatMap(data_numeric)\n","37cb5cb5":"#Spearman, Pearson, Kendall-tau correlation\nvariables =data_numeric.columns\nmethod = ['pearson','spearman','kendalltau']\ncorr_result = pd.DataFrame(columns=['variable1','variable2','corr_spearman','pvalue_spearman','corr_pearson',\n                                    'pvalue_pearson','corr_kendalltau','pvalue_kendalltau'])\n\nfor i in variables:\n    for j in variables:\n        if j!= i:\n            corr1 =scipy.stats.spearmanr(data_numeric[i], data_numeric[j]) \n            corr2 =scipy.stats.pearsonr(data_numeric[i], data_numeric[j])\n            corr3 =scipy.stats.kendalltau(data_numeric[i], data_numeric[j])\n            # Add the corr and pvalue per method\n            corrspearman = corr1[0]\n            pvaluespearman = corr1[1]\n            corrpearson = corr2[0]\n            pvaluepearson = corr2[1]\n            corrkendalltau = corr3[0]\n            pvaluekendalltau = corr3[1]\n            \n            result = [i,j,corrspearman,pvaluespearman,corrpearson,pvaluepearson,corrkendalltau,pvaluekendalltau]\n            corr_result.loc[len(corr_result)]=result\n           \ncorr_result.drop_duplicates(subset=['pvalue_spearman'], keep='first').sort_values(by='pvalue_spearman')\n","a2a4d383":"data_modelling = pd.get_dummies(data_cleaning, prefix_sep='_', drop_first=True)\ndata_modelling.columns\n","2b422c33":"#Splitting train and test data \ntrain,test = train_test_split(data_modelling,test_size = .25 ,random_state = 111)\n\n##seperating dependent and independent variables\ncols    = [i for i in data_modelling.columns if i not in 'index' + 'embauche' + 'date']\ntrain_X = train[cols]\ntrain_Y = train['embauche']\ntest_X  = test[cols]\ntest_Y  = test['embauche']\nindices_train=train_Y.index.values\nindices_test=test_Y.index.values\n\ntarget_count = train.embauche.value_counts()\ntest_count=test.embauche.value_counts()\nprint('Class 0 (Training):', target_count[0],'\/Class 0 (Test):',test_count[0])\nprint('Class 1 (Training):', target_count[1],'\/Class 1 (Test):',test_count[1])\n","0a4b90ba":"#Oversampling minority class using smote and Undersampling using Tomek links\nsmotetomek_X = train[cols]\nsmotetomek_Y = train['embauche']\n\nsmote_tomek = SMOTETomek(random_state=42, ratio=0.5) #Ratio !=1 the objective is to predict class 0 AND 1\nX_resampled, y_resampled = smote_tomek.fit_resample(train_X, train_Y)\n\nsmotetomek_X = pd.DataFrame(data = X_resampled,columns=cols)\nsmotetomek_Y = pd.DataFrame(data = y_resampled,columns=['embauche'])\nprint ((smotetomek_Y['embauche'] == 1).sum())\nprint ((smotetomek_Y['embauche'] == 0).sum())\n","2cbdfe71":"from sklearn import preprocessing\n\n#Define Variables\ncol_num=['age','exp','note','salaire']\ncol_norm_num = ['norm_age','norm_exp', 'norm_salaire', 'norm_note']\ncol_cat_num = ['c_exp_(8.0, 11.0]', 'c_exp_(11.0, 23.0]', 'c_age_(32.0, 39.0]',\n               'c_age_(39.0, 69.0]', 'c_note_(64.0, 75.0]', 'c_note_(75.0, 87.0]',\n               'c_note_(87.0, 100.0]', 'c_salaire_(30757.0, 33701.0]',\n               'c_salaire_(33701.0, 36220.0]', 'c_salaire_(36220.0, 39172.0]',\n               'c_salaire_(39172.0, 53977.0]']\ncol_cat =['year_candidature', 'month_candidature', 'day_candidature','cheveux_brun', 'cheveux_chatain', 'cheveux_roux', 'sexe_M',\n       'diplome_doctorat', 'diplome_licence', 'diplome_master','specialite_detective', 'specialite_forage', 'specialite_geologie',\n       'dispo_oui']\ncol_index= ['index']\ntarget = ['embauche']","1dbdb3b7":"# Function created to tune the parameters using AUC \ndef hyperparameters_def(parameter_grid, kfold, algorithm,X,Y):   \n    acc_scorer = metrics.make_scorer(metrics.accuracy_score) #Choose AUC score\n    \n    grid_search_algo = GridSearchCV(algorithm, param_grid = parameter_grid,\n                              cv = kfold,scoring=acc_scorer) #K-Folds: 10\n\n    grid_search_algo.fit(X, Y)\n\n    print (\"Best Score: {}\".format(grid_search_algo.best_score_)) \n    print (\"Best params: {}\".format(grid_search_algo.best_params_)) ","ed3d6a1a":"def threshold_def(best_classifier,X,Y):\n    visualizer = DiscriminationThreshold(best_classifier)\n\n    visualizer.fit(X, Y)  # Fit the training data to the visualizer\n    visualizer.poof()     # Draw\/show\/poof the data","5df9d3b7":"py.init_notebook_mode(connected=True)\n\ndef evaluation(algorithm,cols,cf,X,Y):\n\n    algorithm.fit(X,Y)\n    predictions   = algorithm.predict(test_X[cols])\n    probabilities = algorithm.predict_proba(test_X[cols])\n\n    #confusion matrix\n    conf_matrix = metrics.confusion_matrix(test_Y,predictions)\n    #roc_auc_score\n    model_roc_auc = metrics.roc_auc_score(test_Y,predictions) \n\n    print (\"Area under curve on Test Set: \",model_roc_auc,\"\\n\")\n    print(\"F1 on Test Set\", metrics.f1_score(test_Y, predictions),\"\\n\")\n    print (\"Accuracy   Score on Test Set : \",metrics.accuracy_score(test_Y,predictions))\n    \n    #coeffs\n    if   cf == \"coefficients\" :\n        coefficients  = pd.DataFrame(algorithm.coef_.ravel())\n    elif cf == \"features\" :\n        coefficients  = pd.DataFrame(algorithm.feature_importances_)\n    \n    column_df     = pd.DataFrame(cols)\n    coef_sumry    = (pd.merge(coefficients,column_df,left_index= True,\n                              right_index= True, how = \"left\"))\n    \n    coef_sumry.columns = [\"coefficients\",\"features\"]\n    coef_sumry    = coef_sumry.sort_values(by = \"coefficients\",ascending = False)\n    #FPR,TPR\n    fpr,tpr,thresholds = metrics.roc_curve(test_Y,probabilities[:,1])\n    \n    #plot confusion matrix\n    trace1 = go.Heatmap(z = conf_matrix ,\n                        x = [\"Not churn\",\"Churn\"],\n                        y = [\"Not churn\",\"Churn\"],\n                        showscale  = False,colorscale = \"Picnic\",\n                        name = \"matrix\")\n\n    #plot roc curve\n    trace2 = go.Scatter(x = fpr,y = tpr,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2))\n    trace3 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n\n    #plot coeffs\n    trace4 = go.Bar(x = coef_sumry[\"features\"],y = coef_sumry[\"coefficients\"],\n                    name = \"coefficients\",\n                    marker = dict(color = coef_sumry[\"coefficients\"],\n                                  colorscale = \"Picnic\",\n                                  line = dict(width = .6,color = \"black\")))\n\n    #subplots\n    fig = tls.make_subplots(rows=2, cols=2, specs=[[{}, {}], [{'colspan': 2}, None]],\n                            subplot_titles=('Confusion Matrix',\n                                            'Receiver operating characteristic',\n                                            'Feature Importances'),print_grid=False)\n\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,1,2)\n    fig.append_trace(trace4,2,1)\n\n    fig['layout'].update(showlegend=False, title=\"Model performance\" ,\n                         autosize = False,height = 900,width = 800,\n                         plot_bgcolor = 'rgba(240,240,240, 0.95)',\n                         paper_bgcolor = 'rgba(240,240,240, 0.95)',\n                         margin = dict(b = 195))\n    fig[\"layout\"][\"xaxis2\"].update(dict(title = \"false positive rate\"))\n    fig[\"layout\"][\"yaxis2\"].update(dict(title = \"true positive rate\"))\n    fig[\"layout\"][\"xaxis3\"].update(dict(showgrid = True,tickfont = dict(size = 10),\n                                        tickangle = 90))\n    py.iplot(fig)","dee570e9":"parameter_grid = {'Cs': [1, 2, 3, 4, 5, 6 ,7 ,8 ,9 ,10]\n                  }\nX_lr = smotetomek_X[['norm_age','norm_salaire','year_candidature', 'month_candidature', 'day_candidature','sexe_M','diplome_doctorat', \n                     'diplome_licence', 'diplome_master','specialite_detective', 'specialite_forage', 'specialite_geologie']]\n\nlogistic = LogisticRegressionCV(random_state= 0,cv=10)\n\nhyperparameters_def(parameter_grid,10,logistic,X_lr,smotetomek_Y)","59943be9":"logit= LogisticRegressionCV(random_state=0 ,Cs=1)\nthreshold_def(logit,X_lr,smotetomek_Y)","6755f017":"cols = ['norm_age','norm_salaire','year_candidature', 'month_candidature', 'day_candidature','sexe_M','diplome_doctorat', \n                     'diplome_licence', 'diplome_master','specialite_detective', 'specialite_forage', 'specialite_geologie']\n\nevaluation (logit, cols,\"coefficients\", X_lr,smotetomek_Y)","ba8bbe94":"#Tuning the parameters using AUC\n\ndecision_tree_classifier = DecisionTreeClassifier(random_state=0)\nX_decisiontree = smotetomek_X[col_num + col_cat]\nparameter_grid = {'max_depth': [1, 2, 3, 4, 5, 6, 7], #Higher value will overfit the model\n                  'max_features': [1, 2, 3, 4, 5],\n                  'criterion': ['gini','entropy'],\n                  'splitter': ['best','random'],\n                  }\n    \nhyperparameters_def(parameter_grid,10,decision_tree_classifier,X_decisiontree,smotetomek_Y.values.ravel())","5c5fa7f3":"decision_tree= DecisionTreeClassifier(random_state=0 ,max_features= 5, criterion='gini',splitter= 'best', max_depth= 8)\nthreshold_def(decision_tree,X_decisiontree,smotetomek_Y.values.ravel())\n","2048c9c3":"cols= col_num + col_cat\n\nevaluation (decision_tree, cols, \"features\", X_decisiontree,smotetomek_Y.values.ravel())","1a965575":"X_randomforest = smotetomek_X[col_num + col_cat]\nparam_grid = {\n    'min_samples_split': [3, 5, 10], \n    #'max_depth': [2, 3, 5, 15, 25],\n    'max_features': ['auto', 'sqrt'],\n    'bootstrap': [True, False],\n    'criterion': ['gini','entropy']   \n}\n\n# Create a based model\n#To obtain a deterministic behaviour during fitting\nrf = RandomForestClassifier(random_state=0, n_estimators=10)#n_estimators = 100 really slow\n\nhyperparameters_def(param_grid,10,rf,X_randomforest,smotetomek_Y.values.ravel())","79288a02":"rf= RandomForestClassifier(random_state=0, n_estimators=10,bootstrap= False, max_features= 'auto', criterion='entropy',min_samples_split= 5)\nthreshold_def(rf,X_randomforest,smotetomek_Y.values.ravel())\n","a04de15b":"cols= col_num + col_cat\n\nevaluation (rf, cols, \"features\", X_randomforest,smotetomek_Y.values.ravel())","f27cc200":"from sklearn.svm import SVC\nX_svm = smotetomek_X[col_norm_num + col_cat]\nsvc_lin  = SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n               decision_function_shape='ovr', degree=3, gamma=1.0, kernel='linear',\n               max_iter=-1, probability=True, random_state=None, shrinking=True,\n               tol=0.001, verbose=False)\nsvc_lin.fit(X_svm,smotetomek_Y) ","306ad72e":"cols= col_norm_num + col_cat\n\nevaluation(svc_lin, cols, \"coefficients\", X_svm,smotetomek_Y)","014e79eb":"xgc = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                    colsample_bytree=1, gamma=0, learning_rate=0., max_delta_step=0,\n                    max_depth = 8, min_child_weight=1, missing=None, n_estimators=1000,\n                    n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n                    silent=True, subsample=1)\n\nX_Xgboost = smotetomek_X[col_num + col_cat]\n\n#hyperparameters_def(param_grid,10,xgc,X_Xgboost,smotetomek_Y)","60cd5739":"#threshold_def(xgc,X_Xgboost,smotetomek_Y)","c8ed1d3b":"cols= col_num + col_cat\n\nevaluation (xgc, cols, \"features\", X_Xgboost,smotetomek_Y)","cd8904eb":"cols= col_norm_num + col_cat\nX_KNN = smotetomek_X[col_norm_num + col_cat]\n\nparam_grid = {\n    'leaf_size':[5,10,20,30], \n    'n_neighbors':[3,4,5,6,7]\n}\n\n# Create a based model\n#To obtain a deterministic behaviour during fitting\nknn_test = KNeighborsClassifier(algorithm='auto')\n\nhyperparameters_def(param_grid,10,knn_test,X_KNN,smotetomek_Y)","f9285564":"knn = KNeighborsClassifier(algorithm='auto', leaf_size=10, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=4, p=2,\n           weights='uniform')\nthreshold_def(knn,X_KNN,smotetomek_Y)","db1b206f":"cols= col_norm_num + col_cat\n #model\nknn.fit(X_KNN,smotetomek_Y)\npredictions   = knn.predict(test_X[cols])\nprobabilities = knn.predict_proba(test_X[cols])\n    \nprint (\"Accuracy Score   : \",metrics.accuracy_score(test_Y,predictions))\n#confusion matrix\nconf_matrix = metrics.confusion_matrix(test_Y,predictions)\n#roc_auc_score\nmodel_roc_auc = metrics.roc_auc_score(test_Y,predictions) \nprint (\"Area under curve : \",model_roc_auc)\nfpr,tpr,thresholds = metrics.roc_curve(test_Y,probabilities[:,1])\n   \n#plot roc curve\ntrace1 = go.Scatter(x = fpr,y = tpr,\n                    name = \"Roc : \" + str(model_roc_auc),\n                    line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                   )\ntrace2 = go.Scatter(x = [0,1],y=[0,1],\n                    line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                    dash = 'dot'))\n\n#plot confusion matrix\ntrace3 = go.Heatmap(z = conf_matrix ,x = [\"Not churn\",\"Churn\"],\n                    y = [\"Not churn\",\"Churn\"],\n                    showscale  = False,colorscale = \"Blues\",name = \"matrix\",\n                    xaxis = \"x2\",yaxis = \"y2\"\n                   )\n\nlayout = go.Layout(dict(title=\"Model performance\" ,\n                        autosize = False,height = 500,width = 800,\n                        showlegend = False,\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(title = \"false positive rate\",\n                                     gridcolor = 'rgb(255, 255, 255)',\n                                     domain=[0, 0.6],\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(title = \"true positive rate\",\n                                     gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        margin = dict(b=200),\n                        xaxis2=dict(domain=[0.7, 1],tickangle = 90,\n                                    gridcolor = 'rgb(255, 255, 255)'),\n                        yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                       )\n              )\ndata = [trace1,trace2,trace3]\nfig = go.Figure(data=data,layout=layout)\n\npy.iplot(fig)","3bef848c":"#### 6.6.1 Hyperparameters Tuning (memory space )","c48a3e56":"<a id='Significance of the correlation coefficient'><\/a>\n#### 4.3.1 Significance of the correlation coefficient","99d47957":"##### Histogram (Numeric Features)","7c60e8f9":"<a id='Evaluation'><\/a>\n#### 6.1.3 Evaluation","6ed667d2":"#### 6.5.2 Evaluation","7bb4cf53":"<a id='Exploratory Analysis'><\/a>\n## 3. Exploratory Analysis\n","38b8d5c4":"<a id='Standardizing numeric features'><\/a>\n###  2.6 Standardizing numeric features","c8adbeb6":"<a id='Training, validation and test sets'><\/a>\n#### 5.2 Training, validation and test sets","6c77836b":"__Data wrangling__:\n- All missing values are excluded from the analysis. Some algorithms don't know how-to handle null values and replacing missing values could biased the model. \n- Since it's a candidature to work at Orf\u00e9e we exclude all individuals having less than 16 years old and more than 70 years old (legal age in France).\n- Furthermore we delete individuals if the experience is less than 0 (3 observations).   \n*Finally 1555 obs. are excluded from the analysis to avoid biased estimations.*  \nWe create three features year, month and day of the candidature.     <br \/>\n\n\n__Exploratory analysis__:<br \/>\n- Note higher than 64 seems to have a discriminant impact \n- People less than 32 y\/o seems to be more recruted\n- Low salary and High Salary are less recruted\n- Sexe (M), Diplome (Master and Licence) and speciality (archeology) seems to have an impact on Embauche = 1\n- The date of Candidature such as the year or the month seems to have an impact on Embauche = 1\n","f24e3268":"<a id='First Conclusion'><\/a>\n### 3.5 First Conclusion","d2aef3f5":"####  (This is Python 3 environment please check your version and packages installation) ","275938a9":"# Table of contents\n1. [Data](#Data)  \n    1.1 [Data Overview](#Data Overview)   \n       \n2. [Data Wrangling](#Data Wrangling)  \n    2.1 [Missing Values](#Missing Values)  \n    2.2 [Duplicate Values](#Duplicate Values)  \n    2.3 [Outliers](#Outliers)     \n    2.4 [Create time features](#Create time features)  \n    2.5 [Categorize numeric features](#Categorize numeric features)    \n    2.6 [Standardizing numeric features](#Standardizing numeric features)\n3. [Exploratory Analysis](#Exploratory Analysis)  \n    3.1 [Analysis of Target](#Analysis of Target)  \n    3.2 [Numeric features distribution](#Numeric features distribution)   \n    3.3 [Categorical features distribution: age, exp, salaire, note](#Categorical features: age, exp, salaire, note)    \n    3.4 [Categorical features distribution](#Categorical features distribution)     \n    3.5 [First Conclusion](#First Conclusion)    \n    \n4. [Independence tests](#Independent tests)  \n    4.1 [Chi-square test of Independence](#Chi-square test of Independence)   \n    4.2 [Kruskal-Wallis H-test](#Kruskal-Wallis H-test)  \n    4.3 [Correlation Matrix](#Correlation Matrix)           \n      4.3.1 [Significance of the correlation coefficient](#Significance of the correlation coefficient)  \n    4.4 [Conclusion of Independent Tests](#Conclusion of Independent Tests)        \n    \n5. [Data pre-processing](#Data pre-processing)  \n    5.1 [Encoding Categorical Features](#Encoding Categorical Features)  \n    5.2 [Training, validation and test sets](#Training, validation and test sets)   \n    5.3 [Imbalanced Data](#Imbalanced Data)     \n\n6. [Model Building](#Model Building)        \n    6.1 [Baseline Model](#Baseline Model)   \n    6.1.1 [Hyperparameters tuning](#Hyperparameters tuning)  \n    6.1.2 [Threshold tuning](#Threshold tuning)  \n    6.1.3 [Evaluation](#Evaluation)    \n    6.2 [Logistic Regression](#Logistic Regression)   \n    6.3 [Decision Tree](#Decision Tree)  \n    6.4 [Random Forests Classifier](#Random Forests Classifier)  \n    6.5 [Support Vector Machine](#Support Vector Machine)  \n    6.6 [XGBoost Classifier](#XGBoost Classifier)  \n    6.7 [KNN Classifier](#KNN Classifier)   \n7. [Conclusion](#Conclusion)  \n","084b0cc1":"CART (Classification and Regression Trees) algorithm.","cdee6729":"<a id='Test on variables: Cheveux & Salaire'><\/a>\n#### 4.2.2 Test on variables: Cheveux & Salaire","03e4df3e":"#### 6.7.1 Hyperparameters Tuning (memory space)","19d18987":"<a id='Baseline Model'><\/a>\n### 6.1 Baseline Model","3abe7244":"<a id='XGBoost Classifier'><\/a>\n### 6.6 XGBoost Classifier","491fe73b":"<a id='Encoding Categorical Features'><\/a>\n#### 5.1 Encoding Categorical Features","bca07b79":"<a id='Data pre-processing'><\/a>\n### 5. Data pre-processing","bf8d6be7":"<a id='Independent tests'><\/a>\n### 4. Independent tests","a8b979b6":"<a id='Correlation Matrix'><\/a>\n### 4.3 Correlation Matrix","e5c7308b":"<a id='Categorize numeric features'><\/a>\n###  2.5 Categorize numeric features","1df7eedc":"<a id='Decision Tree'><\/a>\n### 6.3 Decision Tree","e01e43bd":"<a id='Model Building'><\/a>\n### 6. Model Building","8a0f8712":"<a id='6.3.1 Hyperparameters tuning'><\/a>\n#### 6.3.1 Hyperparameters tuning","70276e69":"<a id='Data Wrangling'><\/a>\n## 2. Data Wrangling","0c497901":"Standardizing the features. In this way we are comparing measurements that have different units plus for some machine learning algorithms it is a general requirement. ","8cd98c36":"<a id='Create time features'><\/a>\n###  2.4 Create time features","364ccdf9":"<a id='Kruskal-Wallis H-test'><\/a>\n### 4.2 Kruskal-Wallis H-test","4f3e84aa":"<a id='KNN Classifier'><\/a>\n### 6.7 KNN Classifier","4b190791":"- Stratified K-fold cross-validation is used to train\/validate the model.   \n- All the models are tuned with the AUC metrics.\n- To avoid overfitting we test our prediction on the test set.","51ed9794":"<a id='Conclusion of Independent Tests'><\/a>\n### 4.4 Conclusion of Independent Tests","81fc78a9":"#### 6.2.1 Hyperparameters tuning","c1e13878":"The objective is to train the model on validation Test to tune the hyperparameters and test the results on the Test set.","98f47ee0":"#### 6.4.3 Evaluation","f374f462":"<a id='Analysis of Target'><\/a>\n### 3.1 Analysis of Target","a718ae53":"__Variables Dependent to the Target__:\n- Sexe, specialit\u00e9, diplome, year_candidature    \n\n__Variables Independent to the Target__:\n- exp, note, salaire, cheveux, dispo\n\nSome dependent variables are correlated multicollinearity could biased the model. ","ecc4c8de":"<a id='Dependence with the Target'><\/a>\n#### 4.2.1 Dependence with the Target","cd72af7a":"<a id='Data Overview'><\/a>\n### 1.1 Data Overview","0d111840":"<a id='6.3.3 Evaluation'><\/a>\n#### 6.3.3 Evaluation","b5f5b3b8":"<a id='Categorical features distribution'><\/a>\n###  3.4 Categorical features distribution","f82ab134":"<a id='Conclusion'><\/a>\n### 7. Conclusion","5aeb967a":"##### Box Plots (Numeric Features)","b10087e9":"If we predicted only Embauche =0 we could have an accuracy of 90%, the objective is to also predict Embauche = 1.  \nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class,\nbased on those that already exist. It works randomly picking a point from the minority class and computing the \nk-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.   \nWe will do a combination of over-sampling and under-sampling, using the SMOTE and Tomek links techniques. [More info](https:\/\/imbalanced-learn.org\/en\/stable\/auto_examples\/combine\/plot_comparison_combine.html)","d7ccddaf":"<a id='Outliers'><\/a>\n###  2.3 Outliers","36ea7eb1":"#### 6.7.2 Threshold Tuning (memory space)","f4b92733":"\n#### 6.5.1 Hyperparameters Tuning  (memory space)","f8b95140":"<a id='Random Forests Classifier'><\/a>\n### 6.4 Random Forests Classifier","ac9cec23":"<a id='Multicollinearity'><\/a>\n#### 4.1.2 Multicollinearity","f4ac4bab":"<a id='Categorical features: age, exp, salaire, note'><\/a>\n###  3.3 Categorical features distribution: age, exp, salaire, note","68ce82e6":"<a id='Chi-square test of Independence'><\/a>\n### 4.1 Chi-square test of Independence","4a427c96":"#### 6.4.1 Hyperparameters Tuning ","578f8d99":"<a id='Test with the Target'><\/a>\n#### 4.1.1 Test with the Target","cb459fff":"<a id='Data'><\/a>\n## 1. Data ","f10d563d":"#### 6.6.2 Threshold Tuning","b295af78":"The objective was to find the best prediction for the target Embauche on the data set candidature.   \nThe exploratory analysis and independance tests confirm the results of features importance in the majority of the models. \nBy treating the imbalanced data, the prediction on Embauche = 1 is better.   \nThe XGBoost model seems to be the best model (based on the AUC, F1 and Accuracy).","dc0671ab":"<a id='Numeric features distribution'><\/a>\n###  3.2 Numeric features distribution ","63a0fbe8":"#### 6.4.2  Threshold Tuning","4a7334a2":"#### 6.2.2 Threshold tuning","09368f67":"#### 6.2.3 Evaluation","9034cb89":"<a id='Threshold tuning'><\/a>\n#### 6.1.2 Threshold tuning","7586cbfd":"<a id='Imbalanced Data'><\/a>\n#### 5.3 Imbalanced Data","33607ed6":"#### 6.7.3 Evaluation","03f3de2b":"# Test Technique\nObjective: \n           The aim is to predict the success or failure to a candidature.","acf823d4":"<a id='Logistic Regression'><\/a>\n### 6.2 Logistic Regression","6ffe589b":"<a id='Missing Values'><\/a>\n### 2.1 Missing Values","69c411b3":"<a id='Hyperparameters tuning'><\/a>\n#### 6.1.1 Hyperparameters tuning","a00ef0c9":"<a id='Support Vector Machine'><\/a>\n### 6.5 Support Vector Machine","214186a6":"1. GridSearchCV to tune your model by searching the best hyperparameters and keeping the classifier with the highest AUC score.\n2. Adjust the decision threshold using the precision-recall curve and the roc curve.","098a8206":"#### 6.6.2 Evaluation","8ed51f35":"<a id='Duplicate Values'><\/a>\n### 2.2 Duplicate Values","6fd2c5f4":"<a id='6.3.2 Threshold tuning'><\/a>\n#### 6.3.2 Threshold tuning"}}