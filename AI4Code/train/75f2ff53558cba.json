{"cell_type":{"20adee23":"code","864aeee5":"code","5ef42e4e":"code","80b27587":"code","157d91dc":"code","ba44ffee":"code","2eaf0592":"code","9172d4dc":"code","9bc09065":"code","abe2f2a2":"code","e6cb130f":"code","c1a225a7":"code","eae6d98f":"code","a3c3920b":"code","9e74e8f1":"code","de301cea":"code","5fa72c8f":"code","31106e39":"code","c670299b":"code","76ffebd2":"code","71793e11":"code","0a0e6bc2":"code","3fa129f9":"code","587c79b3":"code","87813e04":"code","7f59b458":"code","94bb861e":"code","6bc9acc2":"code","a7ba68e4":"code","b1f674b2":"code","e5d177ec":"code","c5057cc2":"code","96cfa525":"code","694b6248":"code","d6fc0563":"code","c930e39a":"code","e04355b0":"code","6f89659a":"code","23974d85":"code","bd0d97e3":"code","55b7f7c2":"code","67a265a1":"code","81b42262":"code","a0339bf9":"code","f63d958a":"code","2b74f823":"code","84ea55ae":"code","a9d7da1d":"code","d9244416":"code","904fcc39":"code","c873d78b":"code","11da04ff":"code","75932ba9":"code","93e17e83":"code","33de5145":"code","d719e2b6":"code","70924d9c":"code","8465feab":"code","be254b94":"code","a5fe3d9a":"code","4231b272":"code","f0c9f467":"code","059c998c":"code","a8d32396":"code","04a2ad17":"code","05298aec":"code","2016a9c0":"code","1aac8237":"code","623090a1":"code","c6e175d4":"code","679cad9c":"code","38cf56bd":"code","4ae07723":"code","95fcc7c1":"code","09393702":"code","a4b04ecf":"code","85d89d0c":"code","0db634da":"code","5eb95316":"code","5f5a4000":"code","22d008ab":"code","226916be":"code","a269b9c3":"code","94721c17":"code","a0c2ef72":"code","2a9624a0":"code","54a284a0":"code","ed21a7ef":"code","c2859175":"code","1abcdfc4":"code","f928c13c":"code","dab43402":"code","131d71e5":"code","209ab785":"code","2b629804":"code","e5cdbf2f":"code","df93fffc":"code","70550ea5":"code","0cb192f3":"code","c82d60a3":"code","327784ae":"code","a78b79e6":"code","daa2b72c":"code","686b68b9":"code","a2f97b71":"code","110b84f3":"code","47163238":"code","523b4fd3":"code","d9cf4970":"code","5ab2f0c1":"code","1bb02a55":"code","03200daf":"code","a6d86445":"markdown","c411583f":"markdown","78b67cea":"markdown","76c4a182":"markdown","b73df493":"markdown","5034290d":"markdown","55732370":"markdown","464d9c1d":"markdown","31c0c4b8":"markdown","d5567146":"markdown","4d00ccef":"markdown","973500e9":"markdown","19df2cec":"markdown","198c1ced":"markdown","e96ee486":"markdown","8975e744":"markdown","f2dbf689":"markdown","d67c22d2":"markdown","76a22d9d":"markdown","18debfed":"markdown","ab62803b":"markdown","3abaf056":"markdown","ffaad2c2":"markdown","14c019d8":"markdown","2093c2de":"markdown","ada631fb":"markdown","5946ba9a":"markdown","18f1355e":"markdown","c6687abf":"markdown","891b4c56":"markdown","fa3b795a":"markdown","8f21c93b":"markdown","c66e0c93":"markdown","06ebe39a":"markdown","b059b0ff":"markdown","5caa1310":"markdown","c61a648e":"markdown","7a098869":"markdown","5cd3b90b":"markdown","9f47a2d9":"markdown","43e57904":"markdown","20e4be18":"markdown","d278a913":"markdown","c8172303":"markdown","b53861cc":"markdown","8536a9b6":"markdown","1c9b75c6":"markdown","0abb6a72":"markdown","87edca14":"markdown","081c309a":"markdown","8e07a401":"markdown","7e76b392":"markdown"},"source":{"20adee23":"from PIL import Image\nImage.open('..\/input\/images\/diabet.jpg')","864aeee5":"import numpy as np\nimport pandas as pd\n\n# Visualization\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# Metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score,f1_score,recall_score,mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.metrics import classification_report\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n# !pip install catboost\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","5ef42e4e":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","80b27587":"data = pd.read_csv(\"..\/input\/diabetes\/diabetic_data.csv\")\ndef display_all(data):\n    with pd.option_context(\"display.max_row\", 100, \"display.max_columns\", 100):\n        display(data)\ndisplay_all(data.head())","157d91dc":"data.shape","ba44ffee":"data.info()","2eaf0592":"data.describe().T","9172d4dc":"IDs_mapping = pd.read_csv(\"..\/input\/id-mapping\/IDs_mapping.csv\")\n        \ndisplay_all(IDs_mapping.head(67))","9bc09065":"data.readmitted = [1 if each=='<30' else 0 for each in data.readmitted]","abe2f2a2":"fig, ax =plt.subplots(nrows=1,ncols=2, figsize=(12,5))\nlabels=['0','1']\nsns.countplot(x=data.readmitted, data=data, palette=\"pastel\",ax=ax[0], edgecolor=\".3\")\ndata.readmitted.value_counts().plot.pie(autopct=\"%1.2f%%\", ax=ax[1], colors=['#66a3ff','#facc99'], \n                                        labels=labels, explode = (0, 0.05), startangle=120,\n                                        textprops={'fontsize': 12, 'color':'#0a0a00'})\nplt.show()","e6cb130f":"data.replace('?', np.nan , inplace=True)","c1a225a7":"msno.matrix(data)\nplt.show()","eae6d98f":"msno.bar(data,sort='descending',color='#66a9bc')\nplt.show()","a3c3920b":"def Missing_Values(data):\n    variable_name = []\n    total_value = []\n    total_missing_value = []\n    missing_value_rate = []\n    unique_value_list = []\n    total_unique_value = []\n    data_type = []\n    \n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()\/data[col].shape[0],4))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n        \n    missing_data=pd.DataFrame({\"Variable\":variable_name,\\\n                               \"#_Total_Value\":total_value,\\\n                               \"#_Total_Missing_Value\":total_missing_value,\\\n                               \"%_Missing_Value_Rate\":missing_value_rate,\\\n                               \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value\n                              })\n    \n    missing_data = missing_data.set_index(\"Variable\")\n    return missing_data.sort_values(\"#_Total_Missing_Value\",ascending=False)","9e74e8f1":"data_info = Missing_Values(data)\ndata_info","de301cea":"data_dictionary = pd.read_csv('..\/input\/dataset\/var.csv', sep=';')\ndata_dictionary = data_dictionary.set_index(\"variable_name\")\ndata_dictionary.head()","5fa72c8f":"data_info['Variable_Structure'] = np.array(data_dictionary[\"Variable_Structure\"])\ndata_info","31106e39":"drop_list = ['examide' , 'citoglipton', 'weight','encounter_id','patient_nbr','payer_code','medical_specialty']  \ndata.drop(drop_list,axis=1, inplace=True)\ndata_info.drop(drop_list, axis=0,inplace=True)","c670299b":"numerical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Structure\"]==\"numeric\")].index)\nlen(numerical_columns), numerical_columns","76ffebd2":"categorical_columns = list(data_info.loc[(data_info.loc[:,\"Variable_Structure\"]==\"nominal\")].index)\nlen(categorical_columns), categorical_columns","71793e11":"def boxplot_for_outlier(df,columns):\n    count = 0\n    fig, ax =plt.subplots(nrows=2,ncols=4, figsize=(16,8))\n    for i in range(2):\n        for j in range(4):\n            sns.boxplot(x = df[columns[count]], palette=\"Wistia\",ax=ax[i][j])\n            count = count+1","0a0e6bc2":"boxplot_for_outlier(data,numerical_columns)","3fa129f9":"f,ax = plt.subplots(figsize=(8, 6))\nsns.heatmap(data[numerical_columns].corr(), annot=True, linewidths=0.5,linecolor=\"black\", fmt= '.2f',ax=ax,cmap=\"coolwarm\")\nplt.show()","587c79b3":"data.gender.replace('Unknown\/Invalid', np.nan , inplace=True)\ndata.dropna(subset=['gender'], how='all', inplace = True)","87813e04":"data.gender.value_counts()","7f59b458":"fig, ax =plt.subplots(nrows=1,ncols=2, figsize=(12,5))\nlabels=['Female','Male']\nsns.countplot(x=data.gender, data=data, palette=\"pastel\",ax=ax[0], edgecolor=\".3\")\ndata.gender.value_counts().plot.pie(autopct=\"%1.2f%%\", ax=ax[1], colors=['#66a3ff','#facc99'], \n                                        labels=labels, explode = (0, 0.05), startangle=120,\n                                        textprops={'fontsize': 12, 'color':'#0a0a00'})\nplt.show()","94bb861e":"visual_list = ['gender','age','race']\nfig, ax =plt.subplots(nrows=1,ncols=3,figsize=(24,8))\ncount =0\nfor i in visual_list:\n    sns.countplot(data[i], hue=data.readmitted, palette='YlOrBr', ax=ax[count]);\n    count = count+1","6bc9acc2":"data.groupby(by = \"insulin\").readmitted.mean()","a7ba68e4":"fig, ax =plt.subplots(figsize=(10,4))\nsns.countplot(x=\"insulin\", hue=\"readmitted\", data=data, palette=\"YlOrBr\")\nplt.show()","b1f674b2":"age_list = list(data.age.unique())\nsns.catplot(x=\"insulin\", hue=\"age\", data=data, kind=\"count\", height=6, aspect=2, palette=\"gnuplot\");","e5d177ec":"data[\"race\"].fillna(data[\"race\"].mode()[0], inplace = True)","c5057cc2":"data[\"race\"].isnull().sum()","96cfa525":"data = data.loc[~data.discharge_disposition_id.isin([11,13,14,19,20,21])]","694b6248":"diag_list = ['diag_1','diag_2','diag_3']\n\nfor col in diag_list:\n    data[col].fillna('NaN', inplace=True)","d6fc0563":"import re\ndef transformFunc(value):\n    value = re.sub(\"V[0-9]*\", \"0\", value) # V \n    value = re.sub(\"E[0-9]*\", \"0\", value) # E \n    value = re.sub('NaN', \"-1\", value) # Nan \n    return value\n\ndef transformCategory(value):\n    if value>=390 and value<=459 or value==785:\n        category = 'Circulatory'\n    elif value>=460 and value<=519 or value==786:\n        category = 'Respiratory'\n    elif value>=520 and value<=579 or value==787:\n        category = 'Digestive'\n    elif value==250:\n        category = 'Diabetes'\n    elif value>=800 and value<=999:\n        category = 'Injury'          \n    elif value>=710 and value<=739:\n        category = 'Musculoskeletal'   \n    elif value>=580 and value<=629 or value==788:\n        category = 'Genitourinary'\n    elif value>=140 and value<=239 :\n        category = 'Neoplasms'\n    elif value==-1:\n        category = 'NAN'\n    else :\n        category = 'Other'\n\n    return category","c930e39a":"for col in diag_list:\n    data[col] = data[col].apply(transformFunc)\n    data[col] = data[col].astype(float)","e04355b0":"for col in diag_list:\n    data[col] = data[col].apply(transformCategory)","6f89659a":"fig, ax =plt.subplots(nrows=3,ncols=1,figsize=(15,12))\ncount =0\nfor i in diag_list:\n    sns.countplot(data[i], hue=data.readmitted, palette='Spectral', ax=ax[count], order = data[i].value_counts().index);\n    count = count+1","23974d85":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors = 2 , contamination = 0.1)\nclf.fit_predict(data[numerical_columns])","bd0d97e3":"df_scores = clf.negative_outlier_factor_\ndf_scores[0:30]","55b7f7c2":"np.sort(df_scores)[0:30]","67a265a1":"threshold_value = np.sort(df_scores)[2]","81b42262":"outlier_tf = df_scores > threshold_value\noutlier_tf","a0339bf9":"new_df = data[df_scores > threshold_value]","f63d958a":"data[df_scores < threshold_value]","2b74f823":"# Custom encoding for the 21 Drug Features\ndrugs = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', 'pioglitazone',\n        'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', 'metformin-pioglitazone',\n        'metformin-rosiglitazone', 'glimepiride-pioglitazone', 'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\n\nfor col in drugs:\n    data[col] = data[col].replace(['No','Steady','Up','Down'],[0,1,1,1])\n    data[col] = data[col].astype(int)","84ea55ae":"# A1Cresult and max_glu_serum\ndata['A1Cresult'] = data['A1Cresult'].replace(['>7','>8','Norm','None'],[1,1,0,-99])\ndata['max_glu_serum'] = data['max_glu_serum'].replace(['>200','>300','Norm','None'],[1,1,0,-99])","a9d7da1d":"# One hot Encoding Race and Id's \none_hot_data = pd.get_dummies(data, columns=['race'], prefix=[\"enc\"])\n\ncolumns_ids = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']\n\none_hot_data[columns_ids] = one_hot_data[columns_ids].astype('str')\none_hot_data = pd.get_dummies(one_hot_data, columns=columns_ids)","d9244416":"df = one_hot_data.copy()\nX = df.drop(columns=\"readmitted\", axis=1)\nY = df.readmitted","904fcc39":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)","c873d78b":"ordinal_enc = OrdinalEncoder()\nX_train.age = ordinal_enc.fit_transform(X_train.age.values.reshape(-1, 1))\nX_test.age = ordinal_enc.transform(X_test.age.values.reshape(-1, 1))","11da04ff":"for col in diag_list:\n    label_enc = LabelEncoder()\n    X_train[col] = label_enc.fit_transform(X_train[col])\n    X_test[col] = label_enc.fit_transform(X_test[col]) ","75932ba9":"binary = ['change', 'diabetesMed', 'gender']\n\nfrom category_encoders import BinaryEncoder\nbinary_enc = BinaryEncoder(cols=binary)\nbinary_enc.fit_transform(X_train)\nX_train = binary_enc.fit_transform(X_train)\nX_test = binary_enc.transform(X_test)","93e17e83":"from sklearn.utils import resample\n\nX = pd.concat([X_train, y_train], axis=1)\n\nnot_readmitted = X[X.readmitted==0]\nreadmitted = X[X.readmitted==1]\n\nnot_readmitted_sampled = resample(not_readmitted,\n                                replace = False, \n                                n_samples = len(readmitted),\n                                random_state = 42)\n\ndownsampled = pd.concat([not_readmitted_sampled, readmitted])\n\ndownsampled.readmitted.value_counts()","33de5145":"y_train = downsampled.readmitted\nX_train = downsampled.drop('readmitted', axis=1)","d719e2b6":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 42)","70924d9c":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix as cm","8465feab":"def calc_specificity(y_actual, y_pred, thresh):\n    # calculates specificity\n    return sum((y_pred < thresh) & (y_actual == 0)) \/sum(y_actual ==0)\n\ndef print_report(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    precision = precision_score(y_actual, (y_pred > thresh))\n    fscore = f1_score(y_actual,(y_pred > thresh) )\n    specificity = calc_specificity(y_actual, y_pred, thresh)\n    print('AUC:%.3f'%auc)\n    print('accuracy:%.3f'%accuracy)\n    print('recall:%.3f'%recall)\n    print('precision:%.3f'%precision)\n    print('fscore:%.3f'%fscore)\n    print('specificity:%.3f'%specificity)\n    print(' ')\n    return auc, accuracy, recall, precision,fscore, specificity","be254b94":"thresh = 0.5","a5fe3d9a":"log_model = LogisticRegression(solver = \"liblinear\",class_weight=\"balanced\",random_state = 42).fit(X_train, y_train)","4231b272":"y_train_preds = log_model.predict_proba(X_train)[:,1]\ny_val_preds = log_model.predict_proba(X_val)[:,1]\n\nprint(\"Logistic Regression\")\nprint('Training:')\nlr_train_auc, lr_train_accuracy, lr_train_recall, \\\n    lr_train_precision, lr_train_fscore, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlr_val_auc, lr_val_accuracy, lr_val_recall, \\\n    lr_val_precision,lr_val_fscore, lr_val_specificity = print_report(y_val,y_val_preds, thresh)","f0c9f467":"# Confusion Matrix\n\npredictions = log_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = log_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","059c998c":"random_forest_model = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=100, max_depth=3)\nrandom_forest_model.fit(X_train, y_train) ","a8d32396":"y_train_preds = random_forest_model.predict_proba(X_train)[:,1]\ny_val_preds = random_forest_model.predict_proba(X_val)[:,1]\n\nprint(\"Random Forest\")\nprint('Training:')\nrf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision,rf_train_fscore, rf_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nrf_val_auc, rf_val_accuracy, rf_val_recall, rf_val_precision,rf_val_fscore, rf_val_specificity = print_report(y_val,y_val_preds, thresh)","04a2ad17":"# Confusion Matrix\n\npredictions = random_forest_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = random_forest_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","05298aec":"gradient_model = GradientBoostingClassifier(random_state=42)\ngradient_model.fit(X_train, y_train)","2016a9c0":"y_train_preds = gradient_model.predict_proba(X_train)[:,1]\ny_val_preds = gradient_model.predict_proba(X_val)[:,1]\n\nprint(\"Gradient Boosing\")\nprint('Training:')\ngbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision,gbc_train_fscore, gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ngbc_val_auc, gbc_val_accuracy, gbc_val_recall, gbc_val_precision, gbc_val_fscore, gbc_val_specificity = print_report(y_val,y_val_preds, thresh)","1aac8237":"# Confusion Matrix\n\npredictions = gradient_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = gradient_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","623090a1":"xgb_model = XGBClassifier(random_state=42, n_jobs=-1,max_depth=3)\nxgb_model.fit(X_train, y_train)","c6e175d4":"y_train_preds = xgb_model.predict_proba(X_train)[:,1]\ny_val_preds = xgb_model.predict_proba(X_val)[:,1]\n\nprint(\"XGBOOST\")\ny_train_preds = gradient_model.predict_proba(X_train)[:,1]\ny_val_preds = gradient_model.predict_proba(X_val)[:,1]\n\nprint(\"Gradient Boosing\")\nprint('Training:')\nxgb_train_auc, xgb_train_accuracy, xgb_train_recall, xgb_train_precision, xgb_train_fscore, xgb_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nxgb_val_auc, xgb_val_accuracy, xgb_val_recall, xgb_val_precision,xgb_val_fscore, xgb_val_specificity = print_report(y_val,y_val_preds, thresh)","679cad9c":"# Confusion Matrix\n\npredictions = xgb_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = xgb_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","38cf56bd":"from lightgbm import LGBMClassifier\nlgbm_model = LGBMClassifier(random_state = 42,max_depth=3)\nlgbm_model.fit(X_train, y_train)","4ae07723":"y_train_preds = lgbm_model.predict_proba(X_train)[:,1]\ny_val_preds = lgbm_model.predict_proba(X_val)[:,1]\n\nprint(\"LGBM\")\nprint('Training:')\nlgbm_train_auc, lgbm_train_accuracy,lgbm_train_recall, lgbm_train_precision,lgbm_train_fscore,lgbm_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlgbm_val_auc, lgbm_val_accuracy, lgbm_val_recall, lgbm_val_precision,lgbm_val_fscore,lgbm_val_specificity = print_report(y_val,y_val_preds, thresh)","95fcc7c1":"# Confusion Matrix\n\npredictions = lgbm_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = lgbm_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","09393702":"cat_model = CatBoostClassifier(random_state = 42, max_depth=3)\ncat_model.fit(X_train, y_train,verbose=False)","a4b04ecf":"y_train_preds = cat_model.predict_proba(X_train)[:,1]\ny_val_preds = cat_model.predict_proba(X_val)[:,1]\n\nprint(\"CATBOOST\")\nprint('Training:')\ncatb_train_auc, catb_train_accuracy,catb_train_recall, catb_train_precision,catb_train_fscore,catb_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ncatb_val_auc,catb_val_accuracy, catb_val_recall, catb_val_precision,catb_val_fscore,catb_val_specificity = print_report(y_val,y_val_preds, thresh)","85d89d0c":"# Confusion Matrix\n\npredictions = cat_model.predict(X_train)\ntrain_score = round(accuracy_score(y_train, predictions), 3)\ncm_train = cm(y_train, predictions)\n\npredictions = cat_model.predict(X_val)\nval_score = round(accuracy_score(y_val, predictions), 3)\ncm_val = cm(y_val, predictions)\n\nfig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(15,5)) \nsns.heatmap(cm_train, annot=True, fmt=\".0f\",ax=ax1)\nax1.set_xlabel('Predicted Values')\nax1.set_ylabel('Actual Values')\nax1.set_title('Train Accuracy Score: {0}'.format(train_score), size = 15)\nsns.heatmap(cm_val, annot=True, fmt=\".0f\",ax=ax2)\nax2.set_xlabel('Predicted Values')\nax2.set_ylabel('Actual Values')\nax2.set_title('Validation Accuracy Score: {0}'.format(val_score), size = 15)\nplt.show()","0db634da":"base_models_results = pd.DataFrame({'classifier':['LOJ','LOJ','RF','RF','GBM','GBM','XGB','XGB','LGBM','LGBM','CATB','CATB'],\n                           'data_set':['train','val']*6,\n                          'auc':[lr_train_auc, lr_val_auc,rf_train_auc,rf_val_auc,gbc_train_auc,gbc_val_auc,xgb_train_auc,xgb_val_auc,lgbm_train_auc,lgbm_val_auc,catb_train_auc,catb_val_auc,],\n                          'accuracy':[lr_train_accuracy, lr_val_accuracy,rf_train_accuracy,rf_val_accuracy,gbc_train_accuracy,gbc_val_accuracy,xgb_train_accuracy,xgb_val_accuracy,lgbm_train_accuracy,lgbm_val_accuracy,catb_train_accuracy,catb_val_accuracy,],\n                          'recall':[lr_train_recall, lr_val_recall,rf_train_recall,rf_val_recall,gbc_train_recall,gbc_val_recall,xgb_train_recall,xgb_val_recall,lgbm_train_recall,lgbm_val_recall,catb_train_recall,catb_val_recall,],\n                          'precision':[lr_train_precision, lr_val_precision,rf_train_precision,rf_val_precision,gbc_train_precision,gbc_val_precision,xgb_train_precision,xgb_val_precision,lgbm_train_precision,lgbm_val_precision,catb_train_precision,catb_val_precision,],\n                          'fscore':[lr_train_fscore, lr_val_fscore,rf_train_fscore,rf_val_fscore,gbc_train_fscore,gbc_val_fscore,xgb_train_fscore,xgb_val_fscore,lgbm_train_fscore,lgbm_val_fscore,catb_train_fscore,catb_val_fscore,],\n                          'specificity':[lr_train_specificity, lr_val_specificity,rf_train_specificity,rf_val_specificity,gbc_train_specificity,gbc_val_specificity,xgb_train_specificity,xgb_val_specificity,lgbm_train_specificity,lgbm_val_specificity,catb_train_specificity,catb_val_specificity,]})","5eb95316":"base_models_results","5f5a4000":"fig, ax = plt.subplots(figsize=(10,6)) \nax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=base_models_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\nplt.show()","22d008ab":"import lightgbm as lgb\n\nplt.rcParams[\"figure.figsize\"] = (18, 10)\nlgb.plot_importance(lgbm_model)\n\nfeature_imp = pd.Series(lgbm_model.feature_importances_, index = X_train.columns)\nbest_features = feature_imp.nlargest(25)","226916be":"best_features.index","a269b9c3":"X_train_importance = X_train[best_features.index]\nX_val_importance = X_val[best_features.index]\nX_test_importance = X_test[best_features.index]","94721c17":"rf = RandomForestClassifier()","a0c2ef72":"rf_params = {\"max_depth\": [2,5,8],\n             \"n_estimators\": [100,200,500,700],\n             \"max_features\": [3,5,8],\n             \"min_samples_split\":[2,5,10]}","2a9624a0":"rf_cv_model = GridSearchCV(rf, rf_params, cv=3, n_jobs=-1, verbose=2).fit(X_train_importance, y_train)","54a284a0":"rf_cv_model.best_params_","ed21a7ef":"rf_tuned =RandomForestClassifier(max_depth=5,\n                                 max_features=5,\n                                 min_samples_split=5,\n                                 n_estimators=500).fit(X_train_importance, y_train)","c2859175":"y_train_preds = random_forest_model.predict_proba(X_train)[:,1]\ny_val_preds = random_forest_model.predict_proba(X_val)[:,1]\n\nprint('Baseline Random Forest')\nrf_train_auc_base = roc_auc_score(y_train, y_train_preds)\nrf_val_auc_base = roc_auc_score(y_val, y_val_preds)\n\nprint('Training AUC:%.3f'%(rf_train_auc_base))\nprint('Validation AUC:%.3f'%(rf_val_auc_base))\n\nprint('Optimized Random Forest')\ny_train_preds_random = rf_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_random = rf_tuned.predict_proba(X_val_importance)[:,1]\n\nrf_train_auc = roc_auc_score(y_train, y_train_preds_random)\nrf_val_auc = roc_auc_score(y_val, y_val_preds_random)\n\nprint('Training AUC:%.3f'%(rf_train_auc))\nprint('Validation AUC:%.3f'%(rf_val_auc))","1abcdfc4":"lgbm=LGBMClassifier()","f928c13c":"lgbm_params = {\"learning_rate\":[0.01,0.1,0.05],\n              \"n_estimators\": [100,200,500],\n               \"subsample\":[0.1,0.2],\n              \"max_depth\":[2,3,5,8]}","dab43402":"lgbm_cv_model=GridSearchCV(lgbm,lgbm_params,cv=3,n_jobs=-1,verbose=2).fit(X_train_importance,y_train)","131d71e5":"lgbm_cv_model.best_params_","209ab785":"lgbm_tuned=LGBMClassifier(learning_rate=0.1,max_depth=2,n_estimators=200,subsample= 0.1).fit(X_train_importance, y_train)","2b629804":"y_train_preds = lgbm_model.predict_proba(X_train)[:,1]\ny_val_preds = lgbm_model.predict_proba(X_val)[:,1]\n\nprint('Baseline LGBM')\nlgbm_train_auc_base = roc_auc_score(y_train, y_train_preds)\nlgbm_val_auc_base = roc_auc_score(y_val, y_val_preds)\n\nprint('Training AUC:%.3f'%(lgbm_train_auc_base))\nprint('Validation AUC:%.3f'%(lgbm_val_auc_base))\n\nprint('Optimized LGBM')\ny_train_preds_lgbm = lgbm_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_lgbm = lgbm_tuned.predict_proba(X_val_importance)[:,1]\n\nlgbm_train_auc = roc_auc_score(y_train, y_train_preds_lgbm)\nlgbm_val_auc = roc_auc_score(y_val, y_val_preds_lgbm)\n\nprint('Training AUC:%.3f'%(lgbm_train_auc))\nprint('Validation AUC:%.3f'%(lgbm_val_auc))","e5cdbf2f":"catb=CatBoostClassifier()","df93fffc":"catb_params={\"iterations\":[200,500,1000],\n            \"learning_rate\":[0.05,0.1],\n            \"depth\":[4,5,8]}","70550ea5":"catb_cv_model=GridSearchCV(catb,catb_params, cv=3, n_jobs=-1,  verbose=2).fit(X_train_importance,y_train)","0cb192f3":"catb_cv_model.best_params_","c82d60a3":"catb_tuned =CatBoostClassifier(depth=5,iterations=200,learning_rate=0.05).fit(X_train_importance, y_train)","327784ae":"y_train_preds = cat_model.predict_proba(X_train)[:,1]\ny_valid_preds = cat_model.predict_proba(X_val)[:,1]\n\nprint('Baseline CATBOOST')\ncatb_train_auc_base = roc_auc_score(y_train, y_train_preds)\ncatb_val_auc_base = roc_auc_score(y_val, y_val_preds)\n\nprint('Training AUC:%.3f'%(catb_train_auc_base))\nprint('Validation AUC:%.3f'%(catb_val_auc_base))\n\nprint('Optimized CATBOOST')\ny_train_preds_catb = catb_tuned.predict_proba(X_train_importance)[:,1]\ny_val_preds_catb = catb_tuned.predict_proba(X_val_importance)[:,1]\n\ncatb_train_auc = roc_auc_score(y_train, y_train_preds_catb)\ncatb_val_auc = roc_auc_score(y_val, y_val_preds_catb)\n\nprint('Training AUC:%.3f'%(catb_train_auc))\nprint('Validation AUC:%.3f'%(catb_val_auc))","a78b79e6":"data_results = pd.DataFrame({'classifier':['RF','RF','LGBM','LGBM','CATB','CATB'],\n                           'data_set':['base','optimized']*3,\n                          'auc':[rf_val_auc_base,rf_val_auc,\n                                 lgbm_val_auc_base,lgbm_val_auc,\n                                 catb_val_auc_base,catb_val_auc,],\n                          })","daa2b72c":"data_results","686b68b9":"ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=data_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\n\nplt.show()","a2f97b71":"classifiers = [ rf_tuned,\n                lgbm_tuned,\n                catb_tuned]\n\n# Define a result table as a DataFrame\nresult_table = pd.DataFrame(columns=['classifiers', 'fpr','tpr','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    yproba = cls.predict_proba(X_test_importance)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    result_table = result_table.append({'classifiers':cls.__class__.__name__,\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc}, ignore_index=True)\n\n# Set name of the classifiers as index labels\nresult_table.set_index('classifiers', inplace=True)\nresult_table.sort_values('auc',ascending=False,inplace=True)","110b84f3":"fig = plt.figure(figsize=(10,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='black', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=14)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=14)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':10}, loc='lower right')\n\nplt.show()","47163238":"def test_scores(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    \n    return auc, accuracy, recall\n\n\nclassifiers = [ rf_tuned,\n                lgbm_tuned,\n                catb_tuned]\n\n# Define a result table as a DataFrame\ntest_result = pd.DataFrame(columns=['classifiers', 'accuracy','recall','auc'])\n\n# Train the models and record the results\nfor cls in classifiers:\n    y_test_preds = cls.predict_proba(X_test_importance)[:,1]\n    \n    test_auc, test_accuracy, test_recall = test_scores(y_test,y_test_preds, 0.5) # thresh = 0.5\n    \n    test_result = test_result.append({'classifiers':cls.__class__.__name__,\n                                        'accuracy':test_accuracy, \n                                        'recall':test_recall, \n                                        'auc':test_auc}, ignore_index=True)\n\n\n# Set name of the classifiers as index labels\ntest_result.set_index('classifiers', inplace=True)\ntest_result.sort_values('auc',ascending=False,inplace=True)","523b4fd3":"test_result","d9cf4970":"import plotly.graph_objs as go\n\ntrace1=go.Bar(\n                x=test_result.index,\n                y=test_result.accuracy,\n                name=\"Accuracy\",\n                marker= dict(color = 'rgba(100, 20, 30, 0.7)',\n                            line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.accuracy,3),textposition='auto')\ntrace2=go.Bar(\n                x=test_result.index,\n                y=test_result.recall,\n                name=\"Recall\",\n                marker=dict(color = 'rgba(56, 140, 200, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.recall,3),textposition='auto')\ntrace3=go.Bar(\n                x=test_result.index,\n                y=test_result.auc,\n                name=\"AUC\",\n                marker=dict(color = 'rgba(120, 180, 20, 0.7)',\n                           line=dict(color='rgb(0,0,0)',width=1.9)),\n                text=round(test_result.auc,3),textposition='auto')\n\nedit_df=[trace1,trace2,trace3]\nlayout = { 'barmode':'group',\n           'title_text':'Accuracy, Recall and AUC Plot Readmitted' }\n\nfig= go.Figure(data=edit_df,layout=layout)\n#plt.savefig('graph.png')\nfig.show()","5ab2f0c1":"best_model = catb_tuned\n\ny_train_preds = best_model.predict_proba(X_train_importance)[:,1]\ny_valid_preds = best_model.predict_proba(X_val_importance)[:,1]\ny_test_preds = best_model.predict_proba(X_test_importance)[:,1]","1bb02a55":"thresh = 0.5\n\nprint('Training:')\ntrain_auc, train_accuracy, train_recall, train_precision, train_fscore, train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nval_auc, val_accuracy, val_recall, val_precision, val_fscore,val_specificity = print_report(y_val,y_val_preds, thresh)\nprint('Test:')\ntest_auc, test_accuracy, test_recall, test_precision, test_fscore, test_specificity = print_report(y_test,y_test_preds, thresh)","03200daf":"from sklearn.metrics import roc_curve \n\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\nauc_train = roc_auc_score(y_train, y_train_preds)\n\nfpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_val_preds)\nauc_val = roc_auc_score(y_val, y_val_preds)\n\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\nauc_test = roc_auc_score(y_test, y_test_preds)\n\nfig, ax = plt.subplots(figsize=(10,6)) \nplt.plot(fpr_train, tpr_train, 'r-',label ='Train AUC:%.3f'%auc_train)\nplt.plot(fpr_val, tpr_val, 'b-',label ='Valid AUC:%.3f'%auc_val)\nplt.plot(fpr_test, tpr_test, 'g-',label ='Test AUC:%.3f'%auc_test)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","a6d86445":"## Local Outlier Factor","c411583f":"## Loading Data","78b67cea":"### Examination and visualization of the effect of the target variable on insulin variable","76c4a182":"Columns that would not give information were removed","b73df493":"## Analyze results baseline models","5034290d":"# <a id=\"8\"><\/a>8. Prediction Result","55732370":"## Representation of missing values, unique values, etc.","464d9c1d":"# <a id=\"10\"><\/a>10. References\n\n* https:\/\/www.kaggle.com\/iabhishekofficial\/prediction-on-hospital-readmission\n* https:\/\/github.com\/andrewwlong\/diabetes_readmission","31c0c4b8":"Diabetes, which is at the forefront of diseases of the age, is a disease that plays a leading role in the formation of many deadly diseases and is very common all over the world.\n\nIt is important to know whether a patient can be readmitted in a hospital. In this project, we tried predict whether diabetes patients will return to the hospital or not by using machine learning algorithms.","d5567146":"## Variable Description\n\n<span style='font-weight:bold;color:#561225'>Encounter ID:<\/span> Unique identifier of an encounter\n    \n<span style='font-weight:bold;color:#561225'>Patient number:<\/span> Unique identifier of a patient\n    \n<span style='font-weight:bold;color:#561225'>Race Values:<\/span> Caucasian, Asian, African American, Hispanic, and other\n    \n<span style='font-weight:bold;color:#561225'>Gender Values:<\/span> male, female, and unknown\/invalid\n    \n<span style='font-weight:bold;color:#561225'>Age:<\/span> Grouped in 10-year intervals: 0, 10), 10, 20), \u2026, 90, 100)\n   \n<span style='font-weight:bold;color:#561225'>Weight:<\/span> Weight in pounds\n  \n<span style='font-weight:bold;color:#561225'>Admission type:<\/span> Integer identifier corresponding to 9 distinct values, for example, emergency, urgent, elective, newborn, and not available\n \n<span style='font-weight:bold;color:#561225'>Discharge disposition:<\/span> Integer identifier corresponding to 29 distinct values, for example, discharged to home, expired, and not available\n\n<span style='font-weight:bold;color:#561225'>Admission source:<\/span> Integer identifier corresponding to 21 distinct values, for example, physician referral, emergency room, and transfer from a hospital\n\n<span style='font-weight:bold;color:#561225'>Time in hospital:<\/span> Integer number of days between admission and discharge\n\n<span style='font-weight:bold;color:#561225'>Payer code :<\/span> Integer identifier corresponding to 23 distinct values, for example, Blue Cross\/Blue Shield, Medicare, and self-pay Medical\n\n<span style='font-weight:bold;color:#561225'>Medical specialty:<\/span> Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family\/general practice, and surgeon\n\n<span style='font-weight:bold;color:#561225'>Number of lab procedures:<\/span> Number of lab tests performed during the encounter\n\n<span style='font-weight:bold;color:#561225'>Number of procedures:<\/span> Numeric Number of procedures (other than lab tests) performed during the encounter\n\n<span style='font-weight:bold;color:#561225'>Number of medications:<\/span> Number of distinct generic names administered during the encounter\n\n<span style='font-weight:bold;color:#561225'>Number of outpatient visits:<\/span> Number of outpatient visits of the patient in the year preceding the encounter\n\n<span style='font-weight:bold;color:#561225'>Number of emergency visits:<\/span> Number of emergency visits of the patient in the year preceding the encounter\n\n<span style='font-weight:bold;color:#561225'>Number of inpatient visits:<\/span> Number of inpatient visits of the patient in the year preceding the encounter\n\n<span style='font-weight:bold;color:#561225'>Diagnosis 1:<\/span> The primary diagnosis (coded as first three digits of ICD9); 848 distinct values\n\n<span style='font-weight:bold;color:#561225'>Diagnosis 2:<\/span> Secondary diagnosis (coded as first three digits of ICD9); 923 distinct values\n\n<span style='font-weight:bold;color:#561225'>Diagnosis 3:<\/span> Additional secondary diagnosis (coded as first three digits of ICD9); 954 distinct values\n\n<span style='font-weight:bold;color:#561225'>Number of diagnoses :<\/span> Number of diagnoses entered to the system 0%\n\n<span style='font-weight:bold;color:#561225'>Glucose serum test :<\/span> result Indicates the range of the result or if the test was not taken. Values: \u201c>200,\u201d \u201c>300,\u201d \u201cnormal,\u201d and \u201cnone\u201d if not measured\n\n<span style='font-weight:bold;color:#561225'>A1c test result :<\/span> Indicates the range of the result or if the test was not taken. Values: \u201c>8\u201d if the result was greater than 8%, \u201c>7\u201d if the result was greater than 7% but less than 8%, \u201cnormal\u201d if the result was less than 7%, and \u201cnone\u201d if not measured.\n\n<span style='font-weight:bold;color:#561225'>Change of medications :<\/span> Indicates if there was a change in diabetic medications (either dosage or generic name). Values: \u201cchange\u201d and \u201cno change\u201d\n\n<span style='font-weight:bold;color:#561225'>Diabetes medications :<\/span> Indicates if there was any diabetic medication prescribed. Values: \u201cyes\u201d and \u201cno\u201d 24 features for medications For the generic names: <span style='font-weight:bold'>metformin, repaglinide, nateglinide, chlorpropamide, glimepiride, acetohexamide, glipizide, glyburide, tolbutamide, pioglitazone, rosiglitazone, acarbose, miglitol, troglitazone, tolazamide, examide, sitagliptin, insulin, glyburide-metformin, glipizide-metformin, glimepiride- pioglitazone, metformin-rosiglitazone, and metformin- pioglitazone, <\/span> the feature indicates whether the drug was prescribed or there was a change in the dosage. Values: \u201cup\u201d if the dosage was increased during the encounter, \u201cdown\u201d if the dosage was decreased, \u201csteady\u201d if the dosage did not change, and \u201cno\u201d if the drug was not prescribed\n\n<span style='font-weight:bold;color:#123456'>Readmitted:<\/span> Days to inpatient readmission. Values: \u201c<30\u201d if the patient was readmitted in less than 30 days, \u201c>30\u201d if the patient was readmitted in more than 30 days, and \u201cNo\u201d for no record of readmission","4d00ccef":"## Model Selection","973500e9":"## Model Selection: Best Classifier","19df2cec":"### <p style='font-weight:bold;color:#123456'><i>I hope you find this kernel useful. If you like it please do an upvote.<\/i><p>","198c1ced":"## Target Distribution (Readmitted)","e96ee486":"## CATBOOST Classifier","8975e744":"## Determination of numerical columns:","f2dbf689":"## Gender Distribution","d67c22d2":"### Resampling techniques \u2014 Undersample majority class\n\nSince we have an unbalanced dataset, We will use sampling technique.","76a22d9d":"## Train-Validation Splitting","18debfed":"## Gradient Boosting Classifier","ab62803b":"# <a id=\"3\"><\/a>3. Feature Engineering","3abaf056":"## Information about Missing values","ffaad2c2":"# <a id=\"9\"><\/a>9. Conclusion\n\nBased on the Auc results observed in the best classifier train, validation and test set results. It was observed that the best model was Catboost. The test result was almost %67 succesful. it was concluded that %67 of the patients who returned to the patient within 30 days returned and predicted correctly.","14c019d8":"## Encoding","2093c2de":"# <a id=\"1\"><\/a> 1. Exploratory Data Analysis\n\n## Importing Libraries","ada631fb":"# <a id=\"4\"><\/a>4. Train-Test Splitting","5946ba9a":"## CATBOOST Classifier Model Tuning","18f1355e":"## Random Forest Classifier","c6687abf":"## Model Selection: Baseline Models","891b4c56":"### Diagnostics 1-2-3 Transform","fa3b795a":"# <a id=\"5\"><\/a>5. Modelling","8f21c93b":"## Hyperparameter Tuning Results","c66e0c93":"## Roc-Auc Comparison of Models","06ebe39a":"# Hospital Readmissions Prediction","b059b0ff":"## Missing Value Filling","5caa1310":"### Gender, Age and Race Visualization","c61a648e":"## Light-GBM Classifier","7a098869":"# <a id=\"7\"><\/a>7. Hyperparameter Tuning","5cd3b90b":"## XGBOOST Classifier","9f47a2d9":"## Determination of categorical columns:","43e57904":"# <a id=\"2\"><\/a>2. Visualization","20e4be18":"## Random Forest Classifier Model Tuning","d278a913":"## Logistic Regression","c8172303":"### Distribution of Diag_1, Diag_2 and Diag_3 Variables by Target Variable:","b53861cc":"## Ligth-GBM Classifier Model Tuning","8536a9b6":"## Feature \u0130mportance with Light-GBM Classifier","1c9b75c6":"Target content changed to 1-0\n\nThe outcome we are looking at is whether the patient gets readmitted to the hospital within 30 days or not.\n\nThe variable actually has <30, >30 and No Readmission categories. To reduce our problem to a binary classification, we combined the readmission after 30 days and no readmission into a single category:\n\nNO and >30: 0 <br>\n<30 : 1","0abb6a72":"### Visualization of the insulin variable according to the age variable:","87edca14":"# <a id=\"6\"><\/a>6. Feature Importance","081c309a":"# Content\n\nThe data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks.\n\nThe following steps were followed in this project;\n<ol> \n    <li><a href='#1'>Exploratory Data Analysis<\/a><\/li>\n    <li><a href='#2'>Visualization<\/a><\/li>\n    <li><a href='#3'>Feature Engineering<\/a><\/li>\n        <ul>     \n         <li>Missing Value Handling<\/li>\n         <li>Outlier Handling<\/li>\n         <li>Encoding<\/li>\n        <\/ul> \n    <li><a href='#4'>Splitting Train-Validation-Test<\/a><\/li>\n    <li><a href='#5'>Modelling<\/a><\/li>\n        <ul>     \n         <li> Logistic Regression<\/li>\n         <li> Random Forest Classifier<\/li>\n         <li> GradientBoosting Classifier<\/li>\n         <li> XGboost Classifier<\/li>\n         <li> Light-GBM Classifier<\/li>\n         <li> CatBoost Classifier<\/li>\n        <\/ul>\n    <li><a href='#6'>Feature Importance<\/a><\/li>\n    <li><a href='#7'>Hyperparameter Tuning<\/a><\/li>\n    <li><a href='#8'>Predict Results<\/a><\/li>\n    <li><a href='#9'>Conclusion<\/a><\/li>\n    <li><a href='#10'>References<\/a><\/li>\n<\/ol> ","8e07a401":"## Types of data measurement scales","7e76b392":"## Outlier Visualization With BoxPlot:"}}