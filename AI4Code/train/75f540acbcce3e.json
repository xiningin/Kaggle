{"cell_type":{"a1fd7f86":"code","c1242111":"code","fb200743":"code","bd32bcdb":"code","26e8d0fe":"code","34d3a332":"code","8cceb9d8":"code","c7663a1f":"code","e005674e":"code","fa602748":"code","87bab118":"code","45ad5e53":"code","8b62e6cb":"code","6551b7c2":"code","318b581e":"code","f43ee2fa":"code","9a63ddd6":"code","30a161b1":"code","f2b23cfd":"code","0dd1dfcd":"code","5b8b283f":"code","61ffaf5c":"code","4d2e49b0":"code","572e2a19":"code","b72654f9":"code","4798c95b":"code","d1842182":"code","a83f2e17":"code","084ae56e":"code","2d12a3d1":"code","9c3a60ed":"code","5908ae11":"code","35bf498e":"code","c44a575c":"code","157e30ce":"markdown","648187c4":"markdown","7392b19e":"markdown","36273e8e":"markdown","2c79a1eb":"markdown","86d131b1":"markdown","edd5a54b":"markdown","ca2e57c5":"markdown","ef5c4e67":"markdown","1acffe10":"markdown","a79f8f45":"markdown","bea429fd":"markdown","2cb5d29c":"markdown","55e1bed7":"markdown","34c9e543":"markdown","a418108e":"markdown","b2577d00":"markdown","9ed0ba7d":"markdown"},"source":{"a1fd7f86":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\n\nimport math\nimport gc\nimport copy\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom lightgbm import LGBMRegressor\nimport os\n# os.listdir('..\/input\/imputed')","c1242111":"DATA_PATH = '..\/input'\nSUBMISSIONS_PATH = '.\/'\n# use atomic numbers to recode atomic names\nATOMIC_NUMBERS = {\n    'H': 1,\n    'C': 6,\n    'N': 7,\n    'O': 8,\n    'F': 9\n}","fb200743":"pd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_rows', 120)\npd.set_option('display.max_columns', 120)","bd32bcdb":"train_dtypes = {\n    'molecule_name': 'category',\n    'atom_index_0': 'int8',\n    'atom_index_1': 'int8',\n    'type': 'category',\n    'scalar_coupling_constant': 'float32'\n}\ntrain_csv = pd.read_csv('..\/input\/champs-scalar-coupling\/train.csv', index_col='id', dtype=train_dtypes)\ntrain_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\ntrain_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\ntrain_csv.head(10)","26e8d0fe":"imputed_train = pd.read_csv('..\/input\/imputed\/imputed_4_correlated_train.csv',dtype = train_dtypes)\n\nimputed_test = pd.read_csv('..\/input\/imputed\/imputed_4_correlated_test.csv',dtype = train_dtypes)\n# imputed_test['molecule_index'] = imputed_test.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')","34d3a332":"print('Shape: ', train_csv.shape)\nprint('Total: ', train_csv.memory_usage().sum())\ntrain_csv.memory_usage()","8cceb9d8":"submission_csv = pd.read_csv('..\/input\/champs-scalar-coupling\/sample_submission.csv', index_col='id')","c7663a1f":"test_csv = pd.read_csv('..\/input\/champs-scalar-coupling\/test.csv', index_col='id', dtype=train_dtypes)\ntest_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\ntest_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\ntest_csv.head(10)","e005674e":"structures_dtypes = {\n    'molecule_name': 'category',\n    'atom_index': 'int8',\n    'atom': 'category',\n    'x': 'float32',\n    'y': 'float32',\n    'z': 'float32'\n}\nstructures_csv = pd.read_csv('..\/input\/champs-scalar-coupling\/structures.csv', dtype=structures_dtypes)\nstructures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\nstructures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\nstructures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')\nstructures_csv.head(10)","fa602748":"print('Shape: ', structures_csv.shape)\nprint('Total: ', structures_csv.memory_usage().sum())\nstructures_csv.memory_usage()","87bab118":"def build_type_dataframes(base, structures, coupling_type):\n    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n    base = base.reset_index()\n    base['id'] = base['id'].astype('int32')\n    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n    return base, structures","45ad5e53":"def add_coordinates(base, structures, index):\n    df = pd.merge(base, structures, how='inner',\n                  left_on=['molecule_index', f'atom_index_{index}'],\n                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n    df = df.rename(columns={\n        'atom': f'atom_{index}',\n        'x': f'x_{index}',\n        'y': f'y_{index}',\n        'z': f'z_{index}'\n    })\n    return df","8b62e6cb":"def add_atoms(base, atoms):\n    df = pd.merge(base, atoms, how='inner',\n                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n    return df","6551b7c2":"def merge_all_atoms(base, structures):\n    df = pd.merge(base, structures, how='left',\n                  left_on=['molecule_index'],\n                  right_on=['molecule_index'])\n    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n    return df","318b581e":"def add_center(df):\n    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n\ndef add_distance_to_center(df):\n    df['d_c'] = ((\n        (df['x_c'] - df['x'])**np.float32(2) +\n        (df['y_c'] - df['y'])**np.float32(2) + \n        (df['z_c'] - df['z'])**np.float32(2)\n    )**np.float32(0.5))\n\ndef add_distance_between(df, suffix1, suffix2):\n    df[f'd_{suffix1}_{suffix2}'] = ((\n        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n    )**np.float32(0.5))","f43ee2fa":"def add_distances(df):\n    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n    \n    for i in range(1, n_atoms):\n        for vi in range(min(4, i)):\n            add_distance_between(df, i, vi)","9a63ddd6":"def add_n_atoms(base, structures):\n    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)","30a161b1":"def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n    base = add_coordinates(base, structures, 0)\n    base = add_coordinates(base, structures, 1)\n    \n    base = base.drop(['atom_0', 'atom_1'], axis=1)\n    atoms = base.drop('id', axis=1).copy()\n    if 'scalar_coupling_constant' in some_csv:\n        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n        \n    add_center(atoms)\n    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n\n    atoms = merge_all_atoms(atoms, structures)\n    \n    add_distance_to_center(atoms)\n    \n    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n    atoms['num'] = atom_groups.cumcount() + 2\n    atoms = atoms.drop(['d_c'], axis=1)\n    atoms = atoms[atoms['num'] < n_atoms]\n\n    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n    atoms = atoms.reset_index()\n    \n    # downcast back to int8\n    for col in atoms.columns:\n        if col.startswith('atom_'):\n            atoms[col] = atoms[col].fillna(0).astype('int8')\n            \n    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n    \n    full = add_atoms(base, atoms)\n    add_distances(full)\n    \n    full.sort_values('id', inplace=True)\n    \n    return full","f2b23cfd":"def take_n_atoms(df, n_atoms, four_start=4):\n    labels = []\n    for i in range(2, n_atoms):\n        label = f'atom_{i}'\n        labels.append(label)\n\n    for i in range(n_atoms):\n        num = min(i, 4) if i < four_start else 4\n        for j in range(num):\n            labels.append(f'd_{i}_{j}')\n    if 'scalar_coupling_constant' in df:\n        labels.append('scalar_coupling_constant')\n    return df[labels]","0dd1dfcd":"%%time\nfull = build_couple_dataframe(train_csv, structures_csv, '1JHN', n_atoms=10)\nprint(full.shape)","5b8b283f":"full.columns","61ffaf5c":"df = take_n_atoms(full, 7)\n# LightGBM performs better with 0-s then with NaN-s\ndf = df.fillna(0)\ndf.columns","4d2e49b0":"X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\ny_data = df['scalar_coupling_constant'].values.astype('float32')\n\nX_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=128)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","572e2a19":"# configuration params are copied from @artgor kernel:\n# https:\/\/www.kaggle.com\/artgor\/brute-force-feature-engineering\nLGB_PARAMS = {\n    'objective': 'regression',\n    'metric': 'mae',\n    'verbosity': -1,\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.2,\n    'num_leaves': 128,\n    'min_child_samples': 79,\n    'max_depth': 9,\n    'subsample_freq': 1,\n    'subsample': 0.9,\n    'bagging_seed': 11,\n    'reg_alpha': 0.1,\n    'reg_lambda': 0.3,\n    'colsample_bytree': 1.0\n}","b72654f9":"model = LGBMRegressor(**LGB_PARAMS, n_estimators=6000, n_jobs = -1)\nmodel.fit(X_train, y_train, \n        eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n        verbose=100, early_stopping_rounds=200)\n\ny_pred = model.predict(X_val)\nnp.log(mean_absolute_error(y_val, y_pred))","4798c95b":"cols = list(df.columns)\ncols.remove('scalar_coupling_constant')\ncols\ndf_importance = pd.DataFrame({'feature': cols, 'importance': model.feature_importances_})\nsns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));","d1842182":"def build_x_y_data(some_csv, coupling_type, n_atoms):\n    full = build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=n_atoms)\n    \n    df = take_n_atoms(full, n_atoms)\n    df = df.fillna(0)\n    print(df.columns)\n    \n    if 'scalar_coupling_constant' in df:\n        X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n        y_data = df['scalar_coupling_constant'].values.astype('float32')\n    else:\n        X_data = df.values.astype('float32')\n        y_data = None\n    \n    return X_data, y_data","a83f2e17":"def train_and_predict_for_one_coupling_type(coupling_type, submission, n_atoms, n_folds=5, n_splits=5, random_state=128):\n    print(f'*** Training Model for {coupling_type} ***')\n    \n    X_data, y_data = build_x_y_data(train_csv, coupling_type, n_atoms)\n    X_test, _ = build_x_y_data(test_csv, coupling_type, n_atoms)\n    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n\n    cv_score = 0\n    \n    if n_folds > n_splits:\n        n_splits = n_folds\n    \n    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n\n    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n        if fold >= n_folds:\n            break\n\n        X_train, X_val = X_data[train_index], X_data[val_index]\n        y_train, y_val = y_data[train_index], y_data[val_index]\n\n        model = LGBMRegressor(**LGB_PARAMS, n_estimators=6000, n_jobs = -1)\n        model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n            verbose=100, early_stopping_rounds=200)\n\n        y_val_pred = model.predict(X_val)\n        val_score = np.log(mean_absolute_error(y_val, y_val_pred))\n        print(f'{coupling_type} Fold {fold}, logMAE: {val_score}')\n        \n        cv_score += val_score \/ n_folds\n        y_pred += model.predict(X_test) \/ n_folds\n        \n        \n    submission.loc[test_csv['type'] == coupling_type, 'scalar_coupling_constant'] = y_pred\n    return cv_score","084ae56e":"model_params = {\n    '1JHN': 7,\n    '1JHC': 10,\n    '2JHH': 9,\n    '2JHN': 9,\n    '2JHC': 9,\n    '3JHH': 9,\n    '3JHC': 10,\n    '3JHN': 10\n}\nN_FOLDS = 5\nsubmission = submission_csv.copy()\n\ncv_scores = {}\nfor coupling_type in model_params.keys():\n    cv_score = train_and_predict_for_one_coupling_type(\n        coupling_type, submission, n_atoms=model_params[coupling_type], n_folds=N_FOLDS)\n    cv_scores[coupling_type] = cv_score","2d12a3d1":"pd.DataFrame({'type': list(cv_scores.keys()), 'cv_score': list(cv_scores.values())})","9c3a60ed":"np.mean(list(cv_scores.values()))","5908ae11":"submission[submission['scalar_coupling_constant'] == 0].shape","35bf498e":"submission.head(10)","c44a575c":"submission.to_csv(f'{SUBMISSIONS_PATH}\/submission.csv')","157e30ce":"## Load Everything","648187c4":"There are many steps, how to improve the score for this kernel:\n* Tune LGB hyperparameters - I did nothing for this\n* Tune number of atoms for each type\n* Try to add other features\n* Play with categorical features for atom types (one-hot-encoding, CatBoost?)\n* Try other tree libraries\n\nAlso, this representation fails badly on `*JHC` coupling types. The main reason for this is that 3rd and 4th atoms are usually located on the same distance and representation starts \"jittering\" randomly picking one of them. So, two similar configurations will have different representation due to usage of 3\/4 of 4\/3 distances.\n\nThe biggest challenge would be to implement handcrafted KNN with some compiled language(Rust, C++, C).\n\nWould be cool to see this kernel forked and addressed some of the issues with higher LB score.","7392b19e":"By default all data is read as `float64` and `int64`. We can trade this uneeded precision for memory and higher prediction speed. So, let's read with Pandas all the data in the minimal representation: ","36273e8e":"## Submission Model","2c79a1eb":"It's funny, but looks like atom types aren't used a lot in the final decision. Quite a contrary to what a man would do.","86d131b1":"For experiments, full dataset can be built with higher number of atoms, and for building a training\/validation sets we can trim them:","edd5a54b":"## Room for improvement","ca2e57c5":"## Core Idea\n\nDespite a lot of creeping Physics and Chemistry knowledge introduced in the description, this competition is more about Geometry and pattern matching.\n\nThe hypothesis of this kernel is next:\n1. If we have two similar sets of atoms with the same distances between them and the same types - the scalar coupling constant should be very close.\n2. More closest atoms to the pair of atoms under prediction have higher influence on scalar coupling constant then those with higher distance\n\nSo, basically, this problem could be dealt with some kind of K-Nearest Neighbor algorithm or any tree-based - e.g. LightGBM, in case we can find some representation which would describe similar configurations with similar feature sets.\n\nEach atom is described with 3 cartesian coordinates. This representation is not stable. Each coupling pair is located in a different point in space and two similar coupling sets would have very different X,Y,Z.\n\nSo, instead of using coordinates let's consider next system:\n1. Take each pair of atoms as two first core atoms\n2. Calculate the center between the pair\n3. Find all n-nearest atoms to the center (excluding first two atoms)\n4. Take two closest atoms from step 3 - they will be 3rd and 4th core atoms\n5. Calculate the distances from 4 core atoms to the rest of the atoms and to the core atoms as well\n\nUsing this representation each atom position can be described by 4 distances from the core atoms. This representation is stable to rotation and translation. And it's suitable for pattern-matching. So, we can take a sequence of atoms, describe each by 4 distances + atom type(H,O,etc) and looking up for the same pattern we can find similar configurations and detect scalar coupling constant.\n\nHere I used LightGBM, because sklearn KNN can't deal with the amount of data. My blind guess is that hand-crafted KNN can outperform LightGBM.\n\nLet's code the solution!","ef5c4e67":"## Build Distance Dataset","1acffe10":"We don't calculate distances for `d_0_x`, `d_1_1`, `d_2_2`, `d_2_3`, `d_3_3` because we already have them in later atoms(`d_0_1` == `d_1_0`) or they are equal to zeros(e.g. `d_1_1`, `d_2_2`).","a79f8f45":"Thanks to Sergii for this kernel:\n\nhttps:\/\/www.kaggle.com\/criskiev\/distance-is-all-you-need-lb-1-481. \n\nI just changed Folds to 5 from 3 and number of iterations to 6000 from 1500. The lb improved to -1.643 from -1.481","bea429fd":"Checking cross-validation scores for each type:","2cb5d29c":"And cv mean score:","55e1bed7":"## Check LightGBM with the smallest type","34c9e543":"Sanity check for all cells to be filled with predictions:","a418108e":"Not a bad score for such a simple set of features.","b2577d00":"## Load Dataset","9ed0ba7d":"Let's build a separate model for each type of coupling. Dataset is split into 5 pieces and in this kernel we will use only 3 folds for speed up.\n\nMain tuning parameter is the number of atoms. I took good numbers, but accuracy can be improved a bit by tuning them for each type."}}