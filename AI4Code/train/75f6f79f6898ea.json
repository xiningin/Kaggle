{"cell_type":{"80366e6c":"code","e5d18282":"code","3ce58ad1":"code","319a67b9":"code","548fce07":"code","0af03968":"code","ec6bf03c":"code","575b84c6":"code","4b320df1":"code","aea9d5d7":"code","e314b3e2":"code","ac078e49":"code","db18876a":"code","f4fd37d3":"code","7c6cdf9d":"code","fd97315d":"code","9334c1c8":"code","8c7db692":"code","0890a09e":"code","8b631583":"code","19baab39":"code","5c1340ba":"code","d387506b":"code","d7ca5455":"code","556e1dc6":"code","e795b143":"code","6dcb06be":"code","efe0c5b3":"code","e5c8498e":"code","66cae7d9":"code","753d35fb":"code","4ddf65ec":"code","0b066153":"code","c810b350":"code","cdbeb004":"code","c7480991":"code","73d73925":"code","3dc82db7":"code","3e5041c3":"code","029e2bfc":"code","abfb9c05":"code","96e46b2d":"code","8c08a48d":"code","85fc8a59":"code","13581904":"code","daddbb7d":"code","a5413027":"code","ba2a3ff0":"markdown","1b055c4f":"markdown","30c3dfeb":"markdown","95fe1de0":"markdown","c1757e1e":"markdown","9f00e670":"markdown","2b7ad898":"markdown","fcc04657":"markdown","ceec39b0":"markdown","ac320cb4":"markdown","ca0d5e81":"markdown","c6ebd456":"markdown","de1f6007":"markdown","93e2500e":"markdown","b052a2d1":"markdown","c189ca72":"markdown","0233978a":"markdown","133bd568":"markdown","86a1170c":"markdown","fd4bc3fb":"markdown","8aa44933":"markdown","17c24b56":"markdown","b2d15cca":"markdown","abf74f86":"markdown","e39a81b1":"markdown","4d84c182":"markdown","378bb23e":"markdown","36be2c0b":"markdown","dbc287b4":"markdown","b013b9bd":"markdown"},"source":{"80366e6c":"print('Hello My name is Bashir Abubakar and welcome to this exploration!')","e5d18282":"# import necessary libraries\n# data cleaning and manipulation \nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install chart_studio\n!pip install cufflinks\nfrom chart_studio.plotly import plot, iplot\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly\nimport chart_studio\nchart_studio.tools.set_credentials_file(username='bashman18', api_key='\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022')\ninit_notebook_mode(connected=True)\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nimport itertools\nimport time\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\nimport sklearn.linear_model as skl_lm\nimport sklearn.metrics as metrics\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_score\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\n\n# initialize some package settings\nsns.set(style=\"whitegrid\", color_codes=True, font_scale=1.3)\n\n%matplotlib inline\n\nprint('All modules imported')","3ce58ad1":"# read in the data and check the first 10 rows\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head(10)","319a67b9":"# general summary of the dataframe\ndf.info()","548fce07":"# check number of missing values\nnull_feat = pd.DataFrame(len(df['id']) - df.isnull().sum(), columns = ['Count'])\nnull_feat","0af03968":"# remove the 'Unnamed: 32' column\ndf = df.drop('Unnamed: 32', axis=1)\n# Reassign target\ndf.diagnosis.replace(to_replace = dict(M = 1, B = 0), inplace = True)","ec6bf03c":"# check the data type of each column\ndf.dtypes","575b84c6":"# drop the id column as well and check the dataframe\ndf=df.drop(\"id\",axis=1)\ndf.head()","4b320df1":"# assign our categorical variables to a dataframe\nM = df[(df['diagnosis'] != 0)]\nB = df[(df['diagnosis'] == 0)]","aea9d5d7":"# check what the dataframe looks like\ndf.head()","e314b3e2":"trace = go.Bar(x = (len(M), len(B)), y = ['malignant', 'benign'], orientation = 'h', opacity = 0.8, marker=dict(\n        color=[ 'gold', 'black'],\n        line=dict(color='#000000',width=1.0)))\n\nlayout = dict(title =  'Count of diagnosis variable')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)\n\ntrace = go.Pie(labels = ['benign','malignant'], values = df['diagnosis'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['black', 'gold'], \n                           line=dict(color='#000000', width=1.5)))\n\nlayout = dict(title =  'Distribution of diagnosis variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","ac078e49":"benign, malignant = df['diagnosis'].value_counts()\nprint('Number of cells labeled Benign: ', benign)\nprint('Number of cells labeled Malignant : ', malignant)\nprint('')\nprint('% of cells labeled Benign', round(benign \/ len(df) * 100, 2), '%')\nprint('% of cells labeled Malignant', round(malignant \/ len(df) * 100, 2), '%')","db18876a":"def plot_distribution(df_f, size_bin) :  \n    tmp1 = M[df_f]\n    tmp2 = B[df_f]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['malignant', 'benign']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = df_f)\n\n    py.iplot(fig, filename = 'Density plot')","f4fd37d3":"plot_distribution('radius_mean', .5)","7c6cdf9d":"plot_distribution('texture_mean', .5)","fd97315d":"plot_distribution('perimeter_mean', 5)","9334c1c8":"plot_distribution('area_mean', 10)\n#plot_distribution('smoothness_mean', .5)\n#plot_distribution('compactness_mean' .5)\n#plot_distribution('concavity_mean' .5)\n#plot_distribution('concave points_mean' .5)\n#plot_distribution('symmetry_mean' .5)\n#plot_distribution('fractal_dimension_mean' .5)","8c7db692":"#correlation\ncorrelation = df.corr()\n#tick labels\nmatrix_cols = correlation.columns.tolist()\n#convert to array\ncorr_array  = np.array(correlation)","0890a09e":"trace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   xgap = 2,\n                   ygap = 2,\n                   colorscale='Viridis',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 9)),\n                        xaxis   = dict(tickfont = dict(size = 9)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","8b631583":"def plot_ft1_ft2(ft1, ft2) :  \n    trace0 = go.Scatter(\n        x = M[ft1],\n        y = M[ft2],\n        name = 'malignant',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = B[ft1],\n        y = B[ft2],\n        name = 'benign',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = ft1 +\" \"+\"vs\"+\" \"+ ft2,\n                  yaxis = dict(title = ft2,zeroline = False),\n                  xaxis = dict(title = ft1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","19baab39":"plot_ft1_ft2('perimeter_mean','radius_worst')\nplot_ft1_ft2('area_mean','radius_worst')\nplot_ft1_ft2('texture_mean','texture_worst')\nplot_ft1_ft2('area_worst','radius_worst')","5c1340ba":"plot_ft1_ft2('smoothness_mean','texture_mean')\nplot_ft1_ft2('radius_mean','fractal_dimension_worst')\nplot_ft1_ft2('texture_mean','symmetry_mean')\nplot_ft1_ft2('texture_mean','symmetry_se')","d387506b":"plot_ft1_ft2('area_mean','fractal_dimension_mean')\nplot_ft1_ft2('radius_mean','fractal_dimension_mean')\nplot_ft1_ft2('area_mean','smoothness_se')\nplot_ft1_ft2('smoothness_se','perimeter_mean')","d7ca5455":"df.head()","556e1dc6":"# define X, y functions for our model\nX=df.drop('diagnosis',axis=1)\nX.head()","e795b143":"y=df['diagnosis']\ny.head()","6dcb06be":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 1)","efe0c5b3":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e5c8498e":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=3) \n\nstart_time=time.time()\n\nclf.fit(X_train, y_train)\n\nend_time=time.time()\n\nprint(\"---%s seconds ---\" % (end_time - start_time))\n\nprint(clf.score(X_test, y_test))","66cae7d9":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,roc_auc_score\n# Validating the train on the model\ny_train_pred =clf.predict(X_train)\ny_train_prob =clf.predict_proba(X_train)[:,1]\n\nprint(\"Accuracy Score of train\", accuracy_score(y_train,y_train_pred))\nprint(\"AUC of the train \", roc_auc_score(y_train,y_train_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_train,y_train_pred))","753d35fb":"# Validating the test on the model\ny_test_pred =clf.predict(X_test)\ny_test_prob =clf.predict_proba(X_test)[:,1]\n\nprint(\"Accuracy Score of test\", accuracy_score(y_test,y_test_pred))\nprint(\"AUC of the test \", roc_auc_score(y_test,y_test_prob))\nprint(\" confusion matrix \\n\" , confusion_matrix(y_test,y_test_pred))","4ddf65ec":"y_pred_proba =clf.predict_proba(X_test)[:,1]","0b066153":"roc = roc_auc_score(y_train,y_train_prob)\nroc","c810b350":"cm_2 = confusion_matrix(y_train,y_train_pred)\ncm_2","cdbeb004":" confusion_mat = confusion_matrix(y_test,y_test_pred)","c7480991":"import plotly.figure_factory as ff\nfig = ff.create_annotated_heatmap(cm_2)\n\n# add title\nfig.update_layout(title_text='<i><b>Confusion matrix<\/b><\/i>')\n\n# adjust margins to make room for yaxis title\nfig.update_xaxes(side=\"top\")\n\n# add colorbar\nfig['data'][0]['showscale'] = True\nfig.show()","73d73925":"y_test_prob","3dc82db7":"#Find Optimum K value\nscores = []\nfor each in range(1,15):\n    KNNfind = KNeighborsClassifier(n_neighbors = each)\n    KNNfind.fit(X_train,y_train)\n    scores.append(KNNfind.score(X_test,y_test))\n    \nplt.plot(range(1,15),scores,color=\"black\")\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Score(Accuracy)\")\nplt.show()","3e5041c3":"y_test_pred","029e2bfc":"tst = df.corr()['diagnosis'].copy()\ntst = tst.drop('diagnosis')\ntst.sort_values(inplace=True)\ntst.iplot(kind='bar',title='Feature Importances',xaxis_title=\"Features\",\n    yaxis_title=\"Correlation\")","abfb9c05":"def model_performance(clf,X_train,X_test,\n                                 y_train,y_test) :\n    \n    #model\n    clf.fit(X_train, y_train)\n    y_test_pred =clf.predict(X_test)\n    y_test_prob =clf.predict_proba(X_test)[::,1]\n\n\n    \n    print (clf)\n    print (\"\\n Classification report : \\n\",classification_report(y_test,y_test_pred))\n    print (\"Accuracy Score   : \",accuracy_score(y_test,y_test_pred))\n    #confusion matrix\n    conf_matrix = confusion_matrix(y_test,y_test_pred)\n    #roc_auc_score\n    model_roc_auc = roc_auc_score(y_test,y_test_pred) \n    print (\"Area under curve : \",model_roc_auc)\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n     \n    #plot roc curve\n    trace1 = go.Scatter(x = fpr,y =tpr ,\n                        name = \"Roc : \" + str(model_roc_auc),\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2),\n                       )\n    trace2 = go.Scatter(x = [0,1],y=[0,1],\n                        line = dict(color = ('rgb(205, 12, 24)'),width = 2,\n                        dash = 'dot'))\n    \n    #plot confusion matrix\n    trace3 = go.Heatmap(z = conf_matrix ,x = [\"Accurate\",\"Inaccurate\"],\n                        y = [\"Accurate\",\"Inaccurate\"],\n                        showscale  = False,colorscale = \"Blues\",name = \"matrix\",\n                        xaxis = \"x2\",yaxis = \"y2\"\n                       )\n    \n    layout = go.Layout(dict(title=\"Model performance\" ,\n                            autosize = False,height = 500,width = 1000,\n                            showlegend = False,\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(title = \"false positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         domain=[0, 0.6],\n                                         ticklen=5,gridwidth=2),\n                            yaxis = dict(title = \"true positive rate\",\n                                         gridcolor = 'rgb(255, 255, 255)',\n                                         zerolinewidth=1,\n                                         ticklen=5,gridwidth=2),\n                            margin = dict(b=200),\n                            xaxis2=dict(domain=[0.7, 1],tickangle = 90,\n                                        gridcolor = 'rgb(255, 255, 255)'),\n                            yaxis2=dict(anchor='x2',gridcolor = 'rgb(255, 255, 255)')\n                           )\n                  )\n    data = [trace1,trace2,trace3]\n    fig = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)","96e46b2d":"model_performance(clf,X_train,X_test,y_train,y_test)","8c08a48d":"X_train","85fc8a59":"X_test","13581904":"y_test_pred[1:6]","daddbb7d":"y_test_pred = [\"M\" if x < 0.5 else \"B\" for x in y_test_pred]","a5413027":"y_test_pred[1:6]","ba2a3ff0":"**Since KNN is a distance based Algorithm - we need to do standardization of values with standard scaler**","1b055c4f":"Looking at the matrix, we can immediately verify the presence of multicollinearity between some of our variables. For instance, the radius_mean column has a correlation of 1 and 0.99 with perimeter_mean and area_mean columns, respectively. This is probably because the three columns essentially contain the same information, which is the physical size of the observation (the cell). Therefore we should only pick one of the three columns when we go into further analysis.","30c3dfeb":"# Let's Connect on LinkedIn!\nIf anybody would like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on **LinkedIn:**\nhttps:\/\/www.linkedin.com\/in\/bashir-abubakar-61935417b\/","95fe1de0":"<img src=\"https:\/\/content.linkedin.com\/content\/dam\/brand\/site\/img\/logo\/logo-tm.png\"\/>","c1757e1e":"**We have finally developed our KNN classifier model, now lets print the scores to determine the accuracy**","9f00e670":"We have successfully developed a KNN Classifier model. This model can take some unlabeled data and effectively assign each observation a probability ranging from 0 to 1. However, for us to evaluate whether the predictions are accurate, the predictions must be encoded so that each instance can be compared directly with the labels in the test data. In other words, instead of numbers between 0 or 1, the predictions should show \"M\" or \"B\", denoting malignant and benign respectively. In our model, a probability of 1 corresponds to the \"Benign\" class, whereas a probability of 0 corresponds to the \"Malignant\" class. Therefore, we can apply a threshhold value of 0.5 to our predictions, assigning all values closer to 0 a label of \"M\" and assigniing all values closer to 1 a label of \"B\".","2b7ad898":"### Feature Importances\n**Let's determine the features that are relevant to our analysis using the correlation technique with a bar chart as seen below**","fcc04657":"It's finally time to develop our model! We will start by first splitting our dataset into two parts; one as a training set for the model, and the other as a test set to validate the predictions that the model will make. If we omit this step, the model will be trained and tested on the same dataset, and it will underestimate the true error rate, a phenomenon known as overfitting. It is like writing an exam after taking a look at the questions and answers beforehand. We want to make sure that our model truly has predictive power and is able to accurately label unseen data. We will set the test size to 0.3; i.e., 70% of the data will be assigned to the training set, and the remaining 30% will be used as a test set. In order to obtain consistent results, we will set the random state parameter to a value of 40.","ceec39b0":"This is the end of our exploration.","ac320cb4":"Determine the optimum value of K","ca0d5e81":"### Model Performance","c6ebd456":"### The Model\n---","de1f6007":"Now that we have split our data into appropriate sets, let's write the code to be used for the KNN Classifier.","93e2500e":"We can confirm that probabilities closer to 0 have been labeled as \"M\", while the ones closer to 1 have been labeled as \"B\".","b052a2d1":"Lets check how the model performs on all features when we make predictions ","c189ca72":"It looks like our data does not contain any missing values, except for our suspect column **Unnamed: 32**, which is full of missing values. Let's go ahead and remove this column entirely. After that, let's check for the data type of each column.","0233978a":"### KNN Classification on all Features","133bd568":"### Positive correlated features","86a1170c":"We will generate a matrix similar to the one above, but this time displaying the correlations between the variables. Let's find out if our hypothesis about the multicollinearity has any statistical support.","fd4bc3fb":"Another place where multicollienartiy is apparent is between the \"mean\" columns and the \"worst\" column. For instance, the radius_mean column has a correlation of 0.97 with the radius_worst column. In fact, each of the 10 key attributes display very high (from 0.7 up to 0.97) correlations between its \"mean\" and \"worst\" columns. This is somewhat inevitable, because the \"worst\" columns are essentially just a subset of the \"mean\" columns; the \"worst\" columns are also the \"mean\" of some values (the three largest values among all observations). Therefore, I think we should discard the \"worst\" columns from our analysis and only focus on the \"mean\" columns when training our model.","8aa44933":"# Predicting Breast Cancer - KNN Classification \n---\n**Bashir Abubakar**\n\n# Introduction\n\nThe contents of this notebook:\n1. **The Data** - *Exploratory Data Analysis*\n2. **The Variables** - *Feature Selection*\n3. **The Model** - *Building a Logistic Regression Model*\n4. **The Prediction** - *Making Predictions with the Model*\n\n**Let's explore the Breast Cancer dataset and develop a KNN Classifier model to predict classification of suspected cells to Benign or Malignant.**\n\n# Data\n---\n*Extracted from the popular [UCI ML repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29)*\n\n### Attribute Information:\n\n* **id** \n* **diagnosis**: M = malignant, B = benign\n\n*Columns 3 to 32* \n\nTen real-valued features are computed for each cell nucleus: \n\n* **radius**: distances from center to points on the perimeter \n* **texture**: standard deviation of gray-scale values\n* **perimeter** \n* **area** \n* **smoothness**: local variation in radius lengths \n* **compactness**: perimeter^2 \/ area - 1.0 \n* **concavity**: severity of concave portions of the contour\n* **concave points**: number of concave portions of the contour\n* **symmetry** \n* **fractal dimension**: \"coastline approximation\" - 1\n\nThe mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\n---","17c24b56":"The result is telling us that we have 108+57 correct predictions and 6+0 incorrect predictions.","b2d15cca":"Out of the 569 observations, 357 (or 62.7%) have been labeled malignant, while the rest 212 (or 37.3%) have been labeled benign. Later when we develop a predictive model and test it on unseen data, we should expect to see a similar proportion of labels.\n\nAlthough our dataset has 30 columns excluding the **id** and the **diagnosis** columns, they are all in fact very closely related since they all contain information on the same 10 key attributes but only differ in terms of their perspectives (i.e., the mean, standard errors, and the mean of the three largest values denoted as \"worst\"). \n\nIn this sense, we could attempt to dig out some quick insights by analyzing the data in only one of the three perspectives. For instance, we could choose to check out the relationship between the 10 key attributes and the **diagnosis** variable by only choosing the \"mean\" columns.\n\nLet's quickly scan for any interesting patterns between our 10 \"mean\" columns and the response variable by generating a scatter plot matrix as shown below:","abf74f86":"Our response variable 'diagnosis', is categorical and has two classes, 'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n\nLet's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes.","e39a81b1":"The last column, **Unnamed:32**, seems like it has a lot of missing values. Let's quickly check for any missing values for other columns as well.","4d84c182":"Let's check the correlation between few features by pair","378bb23e":"Our response variable, **diagnosis**, is categorical and has two classes,  'B' (Benign) and 'M' (Malignant). All explanatory variables are numerical, so we can skip data type conversion.\n\nLet's now take a closer look at our response variable, since it is the main focus of our analysis. We begin by checking out the distribution of its classes.","36be2c0b":"#### Uncorrelated features","dbc287b4":"There are some interesting patterns visible. For instance, the almost perfectly linear patterns between the radius, perimeter and area attributes are hinting at the presence of multicollinearity between these variables. Another set of variables that possibly imply multicollinearity are the concavity, concave_points and compactness.","b013b9bd":"**Check the confusion matrix array to determine the number of accurate samples predicted**"}}