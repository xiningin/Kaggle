{"cell_type":{"adcfdc1a":"code","e9c12db1":"code","1f4dbfff":"code","13cd5225":"code","d98c619a":"code","043de766":"code","e0239975":"code","951d991a":"code","aeade940":"code","6cb962be":"code","c0db327f":"code","ce8aafbe":"code","2c40f7cf":"code","d691bbf2":"code","15fa3457":"code","0bbd843d":"code","b9ff0d68":"code","c99e5354":"code","9509f453":"code","7b237c5a":"code","829b5cac":"code","5108a9b9":"code","fb248591":"code","ed6544bc":"code","dcd196c1":"code","67242873":"code","da951660":"code","22c6272a":"code","594d8eaf":"code","9c973855":"code","3374ecd4":"code","5e50d8e7":"code","3ebd8e11":"code","0cb58701":"code","13464683":"code","aa17d65f":"code","6c3aae7f":"code","778119b0":"code","df1a45cf":"code","49d9aef0":"code","0c7aef4e":"code","f7a306d9":"code","a87bc3fd":"code","19ed577b":"code","d7b09de4":"code","7bdb08bb":"code","12e127bb":"code","9d8953c1":"code","b0666ac0":"code","be484b4f":"code","01a8d995":"code","84a3caec":"code","c0f1ebe7":"code","e2c754a1":"code","d285caf8":"code","17df17d1":"code","ae2533e3":"code","e3417d4c":"code","b508d1ae":"code","4bf4178e":"code","497408f2":"code","21a50a6d":"code","bd56cb17":"code","8c02b740":"code","aa41d179":"code","c580130d":"code","51341fc8":"code","26817c5d":"code","6b361076":"code","e1a35b06":"code","7b7d3de4":"code","53b2a275":"code","654a61a1":"code","8e177467":"code","0fe83481":"code","b9f2f101":"code","7bee2609":"code","e81b3364":"code","1a1f7929":"code","29da600e":"code","fb8c8c29":"code","39664ede":"code","6c6b1203":"code","5fec7a82":"code","d1812678":"code","e17acc73":"code","c4edd602":"code","14f88770":"code","bfae58ea":"code","a5aa28a6":"code","c1c8156b":"code","dfd73ffb":"code","b36402e6":"code","5b6064bf":"code","f46622d0":"code","e9d19d7a":"code","9c233c35":"code","067327e2":"code","b26353a1":"code","a011c1d9":"code","af55ea20":"code","dc27c922":"code","fe867ac9":"code","88232009":"code","602b70cf":"code","a04aafc0":"code","92ef2a1b":"code","e31c830f":"code","53f1d454":"code","d4b3d3a1":"code","baf2d242":"code","25657213":"code","a3a43e1d":"code","e7a17de2":"code","8dbe9c6e":"code","21b687e3":"code","7eec96d4":"code","90fb4dc8":"code","8992a0da":"code","8b5a344e":"code","77346559":"code","1dc2e995":"code","d1be5348":"code","af929963":"code","cbcc8e06":"code","4041dcbe":"code","a4e35431":"code","4d9cfcc8":"code","d4705799":"code","eb874f7c":"code","4cc6ffed":"markdown","4bc80928":"markdown","1904adde":"markdown","e97fc24b":"markdown","b078cac1":"markdown","2fc64f78":"markdown","1aa13327":"markdown","85b0eaed":"markdown","ac96f9e7":"markdown","6ecec168":"markdown","56a83eb2":"markdown","a791570a":"markdown","aaae4f47":"markdown","917b3227":"markdown","789ce11c":"markdown","8f314484":"markdown","fa20e637":"markdown","c7d57e53":"markdown","80bdb5ae":"markdown","0bfdae9a":"markdown","7814e850":"markdown","7c682d77":"markdown","248a0517":"markdown","3d257444":"markdown","023cc8a4":"markdown","4301f702":"markdown","3fe43208":"markdown","865b5ae4":"markdown","8e911730":"markdown","0287990f":"markdown","91683989":"markdown","27deea45":"markdown","ccd44499":"markdown","b833edaa":"markdown","c9327ea5":"markdown","dd3f5a72":"markdown","0090241c":"markdown","dbbc279b":"markdown","0b354824":"markdown","c780983a":"markdown","ff5956b8":"markdown","4d493fc6":"markdown","ad2a8c77":"markdown","8af6a2c8":"markdown","987cb1f0":"markdown","a0567449":"markdown","fe1efe25":"markdown","e7533209":"markdown","713cb746":"markdown","4bdbdb17":"markdown","9b03ac8c":"markdown","2ef9aba1":"markdown","b39d5b3f":"markdown","c263ff03":"markdown","24c2d91a":"markdown","73ba9666":"markdown","6453d55f":"markdown","016f8fc2":"markdown","55c36701":"markdown","aa1eccc7":"markdown","832180c7":"markdown","c6cc9ee5":"markdown","48fc50c4":"markdown","bda6e928":"markdown","32302cbc":"markdown","8eee1911":"markdown","d9b4de24":"markdown","40209107":"markdown","951ff238":"markdown","df784765":"markdown","cd060b6d":"markdown","06b2c500":"markdown","7177d969":"markdown","6f107aa2":"markdown","58392d4c":"markdown","add0f2e3":"markdown","8e2dc7ca":"markdown","99957420":"markdown","92f1ca82":"markdown","2dee2729":"markdown","cf86c632":"markdown","dc100d44":"markdown","f35c8fd7":"markdown","0df2bb91":"markdown","f3914268":"markdown","62cc8b91":"markdown","3c41c125":"markdown","69361d7a":"markdown","a2a37909":"markdown"},"source":{"adcfdc1a":"!pip install sktime\n!pip install delayed\n!pip install sklearn\n!pip install kats","e9c12db1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport re\nimport gc\nfrom tqdm.auto import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport matplotlib.pyplot as plt\nfrom typing import Tuple \nimport warnings\nwarnings.filterwarnings('ignore')\n\nrandom.seed(2021)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.model_selection import train_test_split\nfrom random import randint\n\n# Feature Extraction\nfrom kats.tsfeatures.tsfeatures import TsFeatures\n\n# Modelling\nfrom sklearn.pipeline import Pipeline\nfrom sktime.classification.compose import ColumnEnsembleClassifier\nfrom sktime.classification.dictionary_based import BOSSEnsemble\nfrom sktime.classification.interval_based import TimeSeriesForestClassifier\nfrom sktime.classification.shapelet_based import MrSEQLClassifier\nfrom sktime.transformations.panel.compose import ColumnConcatenator\n\n# Evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.metrics import log_loss  \nfrom sklearn.metrics import roc_auc_score\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f4dbfff":"# Constants\nfigsize = (18, 2)\nfigsize_md = (12,4)\nfigsize_lg = (18, 4)","13cd5225":"fienames = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        fienames.append(filename)","d98c619a":"print(fienames[:10])","043de766":"rejected_filenames = []\nsample_files = random.sample(filenames, 10)\n\nfor filename in tqdm(filenames):\n    try:\n        tmp_df = pd.read_csv(os.path.join(dirname, filename), delimiter=\",\", header=None)\n        \n        if filename in sample_files:\n            display(tmp_df.head(4))\n            display(tmp_df.describe())\n        \n    except:\n        rejected_filenames.append(filename)\n\nprint(f\"Number of rejected data files: {len(rejected_filenames)}\")","e0239975":"print(rejected_filenames)","951d991a":"for filename in rejected_filenames:\n    print(filename)\n    display(pd.read_csv(os.path.join(dirname, filename), delimiter=\"\\t\", header=None).head(2))","aeade940":"# Check eye closed for con42\n# display(pd.read_csv(os.path.join(dirname, 'OpenBCI-RAW-con42_ec.txt'), delimiter=\"\\t\", header=None).head(2)) ## Console: No such file","6cb962be":"df = []\nerror_dict = {}\n\n\nfor filename in tqdm(filenames):\n    key_part = \"_\".join(filename.split('-')[2:])\n    \n    subject_id = \"_\".join(key_part.split('_')[:-1]).lower()\n    label_class = key_part.split('_')[-1].lower().split('.')[0]\n    \n    # Remove non-letters from label_class\n    subject_id = subject_id.replace(\" \", \"\")\n    label_class = re.findall('[a-z]+', label_class)[0]\n\n    try:\n        tmp_df = pd.read_csv(os.path.join(dirname, filename), delimiter=\",\", header=None)\n        \n        if tmp_df.shape[1] <= 10:\n            \n            if tmp_df.mean(axis = 1)[0] == 0:\n                tmp_df = tmp_df[1:].reset_index(drop = True)\n            \n            tmp_df['class'] = label_class\n            tmp_df['subject_id'] = subject_id\n            df.append(tmp_df)\n        \n        else:\n            error_dict[filename] = 'Invalid count of columns.'\n            break           \n            \n    except Exception as e:\n        error_dict[filename] = e\n        \n\ngc.collect()    \n    \nif len(df) > 0:\n    df = pd.concat(df, axis=0).reset_index(drop=True)\nelse:\n    print('Empty dataframe loaded.')\n    \n# Check if any filename that failed\nprint(\"Error instance:\\n\", error_dict)\n\n# Display data head\ndisplay(df.head())","c0db327f":"# Confirmed that there is no data available in column '8' -> dropping it\ndf[8].unique()","ce8aafbe":"df = df.drop([8], axis = 1)","2c40f7cf":"# Inspected the aggregated dataset\ndf.describe()","d691bbf2":"df.groupby([\"subject_id\"])[\"class\"].unique()","15fa3457":"df['subject_id'] = df['subject_id'].replace({'con19_mubasira': 'con19_mubashira'})","0bbd843d":"# Create subject_id <-> integers mapping\nsubject_id_map = {subject_id: idx for idx, subject_id in enumerate(tqdm(df['subject_id'].unique()))}\nclass_map = {'ec': 0, 'eo': 1}\n\n# Replace the subject_id with their assigned integer\ndf['subject_id'] = df['subject_id'].replace(subject_id_map)\ndf['class'] = df['class'].replace(class_map)\n\n# Now subject ids of the dataset are numerical ids\nprint(df['subject_id'].unique())\nprint(df['class'].unique())","b9ff0d68":"class_distribution = {\n                        k: df[df['subject_id'] == k]['class'].value_counts() \n                        for k in df['subject_id'].unique()\n                     }\n\n\nprint('Number of subjects with data loaded:\\n', len(class_distribution), end=\"\\n\\n\")\nprint('Sequence legnth distribution for each subject:')\nclass_distribution = pd.DataFrame(\n                                    class_distribution.values(), \n                                    index=class_distribution.keys()\n                                 )\n\ndisplay(class_distribution.T)\ndisplay(class_distribution.describe())\n\nclass_distribution.hist(figsize = (14, 4))\nplt.show()","c99e5354":"distribution_all = class_distribution[0].to_list() + class_distribution[1].to_list()\npd.Series(distribution_all).hist(figsize = (14, 3))\nplt.title(\"Sequence length distribution of both classes\")\nplt.show()","9509f453":"def tukey_fences(values: list) -> Tuple[float, float]:\n    LQ = np.quantile(values, 0.25)\n    UQ = np.quantile(values, 0.75)\n    lower_bound = LQ - 1.5 * (UQ - LQ)\n    upper_bound = UQ + 1.5 * (UQ - LQ)\n\n    return lower_bound, upper_bound","7b237c5a":"seq_bounds = tukey_fences(distribution_all)\nprint(seq_bounds)","829b5cac":"print(f\"Acceptance rate of using {seq_bounds[1]} as sequence lengths upper-cutoff\")\nrejected_data_points = [sample_size - seq_bounds[1] for sample_size in distribution_all if sample_size > seq_bounds[1]]\naccepted_data_points = 100 * (sum(distribution_all) - sum(rejected_data_points)) \/ sum(distribution_all)\nprint(accepted_data_points)\n\nsequence_bound = seq_bounds[1] # To be used in padding","5108a9b9":"distribution_all = class_distribution[0].to_list() + class_distribution[1].to_list()\ntmp_df = pd.Series(distribution_all)\ntmp_df.hist(figsize = (18, 3))\nplt.vlines(seq_bounds[1], ymin = 0, ymax = 50, color='r', linestyles = \"dotted\")\nplt.title(\"Sequence length distribution of both classes\")\nplt.show()","fb248591":"print('Subject id with missing data')\nsubject_missing = class_distribution[class_distribution.isna().any(axis = 1)].index\ndisplay(class_distribution.iloc[subject_missing])\n\nprint(f\"Number of subjects BEFORE dropping: {len(df['subject_id'].unique())}\")\n# Drop the subjects with missing data\ndf = df[~df[\"subject_id\"].isin(subject_missing)].reset_index(drop=True)\n\nprint(f\"Number of subjects after dropping: {len(df['subject_id'].unique())}\")","ed6544bc":"sample_sizes = {}\nfor k in tqdm(df['subject_id'].unique()):\n    sample_sizes[k] = df[df['subject_id'] == k]['class'].value_counts()\n    \ngc.collect()","dcd196c1":"sample_sizes = pd.DataFrame(sample_sizes).T\n\n# Convert class counts to percentage for comparison\nsample_sizes['total'] = sample_sizes[0] + sample_sizes[1]\nsample_sizes = sample_sizes.assign(ec_percent=lambda df: round(df[0] \/ df['total'] * 100, 3))\nsample_sizes = sample_sizes.assign(eo_percent=lambda df: round(df[1] \/ df['total'] * 100, 3))","67242873":"display(sample_sizes.T)\n\ndisplay(sample_sizes.describe())","da951660":"sample_sizes[['ec_percent', 'eo_percent']].plot(kind='barh', figsize=figsize_lg)\nplt.title('Sequence length distribution across subjects', fontsize=16)\n\nplt.ylabel('Subject id', fontsize=14)\nplt.yticks(rotation=0, fontsize=14)\n\nplt.xlabel('Percetage', fontsize=12)\nplt.xticks(rotation=0, fontsize=12)\n\nplt.grid()\nplt.show()","22c6272a":"sample_sizes = sample_sizes.assign(percent_diff = lambda x: abs(x['ec_percent'] - x['eo_percent']))\n\nprint(\"Statistics of the absoulte class differences\")\nsample_sizes['percent_diff'].describe()","594d8eaf":"class_map = {'eo': 1, 'ec': 0}\ndf['class'] = df['class'].replace(class_map)","9c973855":"test_ratio = 0.2\nunique_subject_id = list(df[\"subject_id\"].unique())\ntest_subject_id = random.sample(unique_subject_id, int(len(unique_subject_id) * 0.2))","3374ecd4":"test_df = df[df[\"subject_id\"].isin(test_subject_id)].reset_index(drop=True)","5e50d8e7":"df = df[~df[\"subject_id\"].isin(test_subject_id)].reset_index(drop=True)","3ebd8e11":"def filter_wrapper(func, df, **kwargs):\n    \"\"\"\n    A wrapper function for any filter or transformation functions to apply over the entire df.\n    \"\"\"\n    \n    res_df = pd.DataFrame()\n    \n    for subject_id in tqdm(df[\"subject_id\"].unique()):\n        for class_id in df[\"class\"].unique():\n            tmp_df = df[(df[\"subject_id\"] == subject_id) & (df[\"class\"] == class_id)]\n            tmp_df = tmp_df.drop([\"subject_id\", \"class\"], axis = 1)\n            tmp_df = func(tmp_df, **kwargs)\n            \n            tmp_df[\"class\"] = class_id\n            tmp_df[\"subject_id\"] = subject_id\n\n            if res_df.empty:\n                res_df = tmp_df\n\n            else:\n                res_df = pd.concat([res_df, tmp_df], axis = 0)\n\n    return res_df.reset_index(drop=True)\n","0cb58701":"subject_id = 0\nclass_id = 0\n\ntmp_df = df[(df['subject_id'] == subject_id) & (df['class'] == class_id)]","13464683":"def normalize_df(df: pd.DataFrame, window_size: int = 100) -> pd.DataFrame:\n    return (df-df.mean())\/df.std()\n\n\ndef standardize_df(df: pd.DataFrame) -> pd.DataFrame:\n    return (df - df.min()) \/ (df.max() - df.min())\n\n\ndef norm_standard_window(df: pd.DataFrame, window_size: int = 500) -> pd.DataFrame:\n    tmp_df = df.reset_index(drop=True)\n    res_df = df.copy()\n    \n    for idx in range(0, len(tmp_df), window_size):\n        upper_idx = min(len(tmp_df), idx+window_size)\n        window_df = tmp_df[idx: upper_idx]\n        norm_window_df = normalize_df(window_df)\n        norm_window_df = standardize_df(norm_window_df)\n        res_df.iloc[idx:upper_idx] = norm_window_df\n    \n    return res_df\n","aa17d65f":"tmp_df = df[(df['subject_id'] == 0) & (df['class'] == 0)]","6c3aae7f":"tmp_df.plot(figsize = figsize)\nplt.show()","778119b0":"tmp_df[0].plot(figsize = figsize)\nplt.show()","df1a45cf":"tmp_df_norm = norm_standard_window(tmp_df, window_size = 500)","49d9aef0":"tmp_df_norm.plot(figsize = figsize)\nplt.show()","0c7aef4e":"tmp_df_norm[0].plot(figsize = figsize)\nplt.show()","f7a306d9":"args_dict = {\"window_size\": 500}\ndf_norm = filter_wrapper(norm_standard_window, df, **args_dict)","a87bc3fd":"print(\"Before windowed normalization\")\ndf[(df['subject_id'] == 0) & (df['class'] == 0)][0].plot(figsize = figsize)\nplt.show()\n\nprint(\"After windowed normalization\")\ndf_norm[(df_norm['subject_id'] == 0) & (df_norm['class'] == 0)][0].plot(figsize = figsize)\nplt.show()","19ed577b":"def seq_convolution(\n                    df: pd.DataFrame, \n                    feature_cols: list,\n                    kernel_size = 10,\n                    kernel_type = \"mean\",\n                    step_size = 5,\n                    verbose: str = False\n                    ) -> pd.DataFrame:\n\n    static_cols = [col for col in df.columns if col not in feature_cols]\n    index_bitmap = np.arange(len(df)) % step_size == 1\n    \n    if kernel_type == \"max\":\n        res_df =  df[feature_cols].rolling(window=kernel_size).max()\n        \n    elif kernel_type == \"mean\":\n        res_df =  df[feature_cols].rolling(window=kernel_size).mean()\n        \n    res_df = res_df[index_bitmap].dropna().reset_index(drop=True)\n    \n    for col in static_cols:\n        res_df[col] = df[str(col)].iloc[0]\n        \n    if verbose:\n        print(f\"Data points reduced by: {len(df) - len(res_df)} or {round(100 * (len(df) - len(res_df)) \/ len(df), 3)} %\")\n        \n    return res_df\n    ","d7b09de4":"feature_cols = [i for i in range(8)]\nargs = {\"kernel_size\": 5, \"step_size\": 2, \"feature_cols\": feature_cols, \"verbose\": True}\ndf_conv = filter_wrapper(seq_convolution, df_norm, **args)","7bdb08bb":"# Check effects\nsubject_id = 38\nclass_id = 0\n\ntmp_df_norm = df_norm[(df_norm['subject_id'] == subject_id) & (df_norm['class'] == class_id)]\ntmp_conv_norm = df_conv[(df_conv['subject_id'] == subject_id) & (df_conv['class'] == class_id)]\n\nprint(\"After normalization\")\ntmp_df_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"After GAP\")\ntmp_conv_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()","12e127bb":"df_processing = df_conv\n\nsubject_id = 0\nchannels = [i for i in range(8)]\nclass_dict = {0: 'eo', 1: 'ec'}\n\n\nfor channel in channels:\n    for k, v in class_dict.items():\n        tmp_df2 = df_processing[(df_processing['subject_id'] == subject_id) & (df_processing['class'] == k)][channel]\n        tmp_df2.reset_index(drop=True).plot(figsize=(16, 1), label=v)\n    \n    plt.title(f\"EEG activities of {subject_id} for channel {channel}\"); plt.grid(); plt.legend(); plt.show()","9d8953c1":"n = 3\ndf_processing = df_norm\nsample_subject_id = random.sample(list(df_processing['subject_id'].unique()), n)\n\nfor subj_id in tqdm(sample_subject_id):\n    for k, v in class_dict.items():\n        tmp_df = df_processing[(df_processing['subject_id'] == subj_id) & (df_processing['class'] == k)][channels]\n        tmp_df.reset_index(drop=True).plot(figsize=(20, 2))\n        plt.title(f\"EEG activities of subject {subj_id} with condition {v}\")\n        plt.grid()\n        plt.show()","b0666ac0":"def moving_average(\n                    df: pd.DataFrame, \n                    feature_cols: list,\n                    kernel_size = 50\n                    ) -> pd.DataFrame:\n\n    res_df =  df[feature_cols].rolling(window=kernel_size).mean()\n    return res_df.dropna().reset_index(drop=True)\n    ","be484b4f":"args = {\"feature_cols\": feature_cols, \"kernel_size\": 10}\ndf_norm_ma = filter_wrapper(moving_average, df_norm, **args)","01a8d995":"args = {\"feature_cols\": feature_cols, \"kernel_size\": 10}\ndf_conv_ma = filter_wrapper(moving_average, df_conv, **args)","84a3caec":"# Check effects\nsubject_id = 38\nclass_id = 0\n\ntmp_df = df[(df['subject_id'] == subject_id) & (df['class'] == class_id)]\ntmp_df_norm = df_norm[(df_norm['subject_id'] == subject_id) & (df_norm['class'] == class_id)]\ntmp_df_conv = df_conv[(df_conv['subject_id'] == subject_id) & (df_conv['class'] == class_id)]\ntmp_df_norm_ma = df_norm_ma[(df_norm_ma['subject_id'] == subject_id) & (df_norm_ma['class'] == class_id)]\ntmp_df_conv_ma = df_conv_ma[(df_conv_ma['subject_id'] == subject_id) & (df_conv_ma['class'] == class_id)]\n\nprint(\"Original\")\ntmp_df[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization\")\ntmp_df_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + Global Average Pooling (GAP)\")\ntmp_conv_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + Moaving Average (MA)\")\ntmp_df_norm_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + Global Average Pooling (GAP) + Moaving Average (MA)\")\ntmp_df_conv_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()","c0f1ebe7":"n = 3\nchannel = 0\ndf_before = df_norm\ndf_after = df_norm_ma\nsample_subject_id = random.sample(list(df_processing['subject_id'].unique()), n)\n\nfor subj_id in tqdm(sample_subject_id):\n    for k, v in class_dict.items():\n        print(\"BEFORE MA processing\")\n        tmp_df = df_before[(df_before['subject_id'] == subj_id) & (df_before['class'] == k)][channel]\n        tmp_df.reset_index(drop=True).plot(figsize=figsize)\n        plt.title(f\"EEG activities of subject {subj_id} with condition {v}\"); plt.grid(); plt.show()\n        \n        print(\"AFTER MA processing\")\n        tmp_df = df_after[(df_after['subject_id'] == subj_id) & (df_after['class'] == k)][channel]\n        tmp_df.reset_index(drop=True).plot(figsize=figsize)\n        plt.title(f\"EEG activities of subject {subj_id} with condition {v}\"); plt.grid(); plt.show()","e2c754a1":"import scipy\nfrom scipy.signal import butter, lfilter\nfrom scipy.signal import freqz\n\ndef butter_bandpass(lowcut, highcut, fs, order=5):\n    nyq = 0.5 * fs\n    low = lowcut \/ nyq\n    high = highcut \/ nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\n\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n","d285caf8":"lowcut = 1.0\nhighcut = 100.0\nfs = 250.0\n\nplt.figure(figsize = (16, 3))\nfor order in [3, 6, 9]:\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n    w, h = freqz(b, a, worN=2000)\n    plt.plot((fs * 0.5 \/ np.pi) * w, abs(h), label=\"order = %d\" % order)\n    \nplt.plot([0, 0.5 * fs], [np.sqrt(0.5), np.sqrt(0.5)],\n         '--', label='sqrt(0.5)')\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Gain')\nplt.grid(True)\nplt.legend(loc='best')\n\nplt.legend()\nplt.title(\"The frequency response for different orders with Butterworth filter\")\nplt.show()","17df17d1":"# Sample rate and desired cutoff frequencies (in Hz).\nfs = 5000.0\nlowcut = 300.0\nhighcut = 1250.0\n\n# Plot the frequency response for a few different orders.\nplt.figure(1, figsize = (18, 4))\nplt.clf()\nfor order in [3, 6, 9]:\n    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n    w, h = freqz(b, a, worN=2000)\n    plt.plot((fs * 0.5 \/ np.pi) * w, abs(h), label=\"order = %d\" % order)\n\nplt.plot(\n        [0, 0.5 * fs], [np.sqrt(0.5), np.sqrt(0.5)],\n         '--', \n         label='sqrt(0.5)'\n        )\nplt.xlabel('Frequency (Hz)')\nplt.ylabel('Gain')\nplt.grid(True)\nplt.legend(loc='best')\n\n# Filter a noisy signal.\nT = 0.05\nnsamples = T * fs\nt = np.linspace(0, T, int(nsamples), endpoint=False)\na = 0.02\nf0 = 600.0\n\n# Sample signal construction \n# x = np.ones(int(nsamples))\nx = 0.1 * np.sin(2 * np.pi * 1.2 * np.sqrt(t))\nx += 0.01 * np.cos(2 * np.pi * 312 * t + 0.1)\nx += a * np.cos(2 * np.pi * f0 * t + .11)\nx += 0.03 * np.cos(2 * np.pi * 2000 * t)\n\n\nplt.figure(2, figsize = (18, 4))\nplt.clf()\nplt.plot(t, x, label='Noisy signal')\n\ny = butter_bandpass_filter(x, lowcut, highcut, fs, order=6)\nplt.plot(t, y, label=f\"Filtered signal ({lowcut} Hz - {highcut}Hz)\")\nplt.xlabel('time (seconds)')\nplt.hlines([-a, a], 0, T, linestyles='--')\nplt.grid(True)\nplt.axis('tight')\nplt.legend(loc='upper left')\n\nplt.show()","ae2533e3":"def passband_filter(\n                    signal, \n                    lowcut = 1.0, # Min frequency\n                    highcut = 100.0, # Max frequency\n                    fs = 250.0, # Sampling rate\n                    plot = False \n                    ) -> np.array:\n    \n    # Sample rate and desired cutoff frequencies (in Hz).    \n#     fs = len(signal)\n    for order in [3, 6, 9]:\n        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n        w, h = freqz(b, a, worN=2000)\n        \n        if plot:\n            plt.figure(1, figsize = figsize); plt.clf()\n            plt.plot((fs * 0.5 \/ np.pi) * w, abs(h), label=\"order = %d\" % order)\n\n    if plot:\n        plt.plot([0, 0.5 * fs], [np.sqrt(0.5), np.sqrt(0.5)], '--', label='sqrt(0.5)')\n        plt.xlabel('Frequency (Hz)'); plt.ylabel('Gain'); plt.grid(True); plt.legend(loc='best')\n\n    # Filter a noisy signal.\n#     T = 0.05\n    T = len(signal) \/ fs\n#     nsamples = T * fs\n    nsamples = len(signal)\n    t = np.linspace(0, T, int(nsamples), endpoint=False)\n    a = 0.02\n    \n    if plot:\n        plt.figure(figsize = figsize)\n        plt.plot(t, signal, label='Noisy signal')\n    \n    # Butter bandpass filter    \n    filtered_signal = butter_bandpass_filter(signal, lowcut, highcut, fs, order=6)\n    \n    # Plot filtered results\n    if plot:\n        plt.plot(t, filtered_signal, label=f\"Filtered signal ({lowcut} Hz - {highcut} Hz)\")\n        plt.xlabel('time (seconds)')\n        plt.hlines([-a, a], 0, T, linestyles='--')\n        plt.grid(True); plt.axis('tight'); plt.legend(loc='upper left'); plt.show()\n        \n    return filtered_signal\n\n\ndef passband_filter_wrapper(\n                            df, \n                            feature_cols: list, \n                            lowcut: float = 1.0, # Min frequency\n                            highcut: float = 100.0, # Max frequency\n                            fs: float = 250.0, # Sampling rate\n                            plot: bool = False \n                            ) -> np.array:\n    \n    for col in feature_cols:\n        df[col] = passband_filter(\n                                df[col], \n                                lowcut, # Min frequency\n                                highcut, # Max frequency\n                                fs, # Sampling rate\n                                plot\n                                )\n    return df","e3417d4c":"# Check effects\nsubject_id = 38\nclass_id = 0\n\ntmp_df_ma = df_norm_ma[(df_norm_ma['subject_id'] == subject_id) & (df_norm_ma['class'] == class_id)]\ntmp_df_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()","b508d1ae":"filtered_sample_data = passband_filter(tmp_df_ma[1], lowcut = 1.0, highcut = 100.0, plot=True)","4bf4178e":"filtered_sample_data = passband_filter(tmp_df_ma[1], lowcut = 0.1, highcut = 95.0, plot=True)","497408f2":"filtered_sample_data = passband_filter(tmp_df_ma[1], lowcut = 0.1, highcut = 100.0, plot=True)","21a50a6d":"args = {\"lowcut\": 0.1, \"feature_cols\": feature_cols, \"highcut\": 100.0, \"plot\": False}\ndf_norm_bp = filter_wrapper(passband_filter_wrapper, df_norm, **args)\n\nargs = {\"lowcut\": 0.1, \"feature_cols\": feature_cols, \"highcut\": 100.0, \"plot\": False}\ndf_norm_ma_bp = filter_wrapper(passband_filter_wrapper, df_norm_ma, **args)\n\nargs = {\"lowcut\": 0.1, \"feature_cols\": feature_cols, \"highcut\": 100.0, \"plot\": False}\ndf_norm_conv_ma_bp = filter_wrapper(passband_filter_wrapper, df_conv_ma, **args)","bd56cb17":"# Check effects\nsubject_id = 2\nclass_id = 0\n\ntmp_df = df[(df['subject_id'] == subject_id) & (df['class'] == class_id)]\ntmp_df_norm = df_norm[(df_norm['subject_id'] == subject_id) & (df_norm['class'] == class_id)]\ntmp_df_norm_ma = df_norm_ma[(df_norm_ma['subject_id'] == subject_id) & (df_norm_ma['class'] == class_id)]\ntmp_df_conv_ma = df_conv_ma[(df_conv_ma['subject_id'] == subject_id) & (df_conv_ma['class'] == class_id)]\ntmp_df_norm_bp = df_norm_bp[(df_norm_bp['subject_id'] == subject_id) & (df_norm_bp['class'] == class_id)]\ntmp_df_norm_ma_bp = df_norm_ma_bp[(df_norm_ma_bp['subject_id'] == subject_id) & (df_norm_ma_bp['class'] == class_id)]\ntmp_df_norm_conv_ma_bp = df_norm_conv_ma_bp[(df_norm_conv_ma_bp['subject_id'] == subject_id) & (df_norm_conv_ma_bp['class'] == class_id)]\n\nprint(\"Original\")\ntmp_df[feature_cols].plot(figsize = (16, 2)); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization\")\ntmp_df_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP\")\ntmp_conv_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + MA\")\ntmp_df_norm_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP + MA\")\ntmp_df_conv_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + BP\")\ntmp_df_norm_bp[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + MA + BP\")\ntmp_df_norm_ma_bp[feature_cols][1000:].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP + BP\")\ntmp_df_norm_conv_ma_bp[feature_cols][1000:].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n","8c02b740":"args = {\"feature_cols\": feature_cols, \"kernel_size\": 10}\ndf_bp_norm_ma = filter_wrapper(moving_average, df_norm_bp, **args)","aa41d179":"# Check effects\nsubject_id = 2\nclass_id = 0\n\ntmp_df = df[(df['subject_id'] == subject_id) & (df['class'] == class_id)]\ntmp_df_norm = df_norm[(df_norm['subject_id'] == subject_id) & (df_norm['class'] == class_id)]\ntmp_df_norm_ma = df_norm_ma[(df_norm_ma['subject_id'] == subject_id) & (df_norm_ma['class'] == class_id)]\ntmp_df_conv_ma = df_conv_ma[(df_conv_ma['subject_id'] == subject_id) & (df_conv_ma['class'] == class_id)]\ntmp_df_norm_bp = df_norm_bp[(df_norm_bp['subject_id'] == subject_id) & (df_norm_bp['class'] == class_id)]\ntmp_df_norm_ma_bp = df_norm_ma_bp[(df_norm_ma_bp['subject_id'] == subject_id) & (df_norm_ma_bp['class'] == class_id)]\ntmp_df_norm_conv_ma_bp = df_norm_conv_ma_bp[(df_norm_conv_ma_bp['subject_id'] == subject_id) & (df_norm_conv_ma_bp['class'] == class_id)]\ntmp_df_bp_norm_ma = df_bp_norm_ma[(df_bp_norm_ma['subject_id'] == subject_id) & (df_bp_norm_ma['class'] == class_id)].reset_index(drop=True)\n\nprint(\"Original\")\ntmp_df[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization\")\ntmp_df_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP\")\ntmp_conv_norm[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + MA\")\ntmp_df_norm_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP + MA\")\ntmp_df_conv_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + BP\")\ntmp_df_norm_bp[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + MA + BP\")\ntmp_df_norm_ma_bp[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP + BP\")\ntmp_df_norm_conv_ma_bp[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()\n\nprint(\"Normalization + GAP + BP + Normalization + MA\")\ntmp_df_bp_norm_ma[feature_cols].plot(figsize = figsize); plt.legend(loc=\"right\"); plt.grid(); plt.show()","c580130d":"from scipy.fft import fft, fftfreq\n\n#### Test\n# Number of sample points\nN = 600\n# sample spacing\nT = 1.0 \/ 800.0\nx = np.linspace(0.0, N*T, N, endpoint=False)\ny = np.sin(50.0 * 2.0*np.pi*x) + 0.5*np.sin(80.0 * 2.0*np.pi*x)\n# y = np.ones(N) * 3\n\nprint(\"Original waveform\")\nplt.figure(figsize=figsize)\nplt.plot(x, y)\nplt.show()\n\nprint(\"Frequency domain\")\nyf = fft(y)\nxf = fftfreq(N, T)[:N\/\/2]\nplt.figure(figsize=figsize)\nplt.plot(xf, 2.0\/N * np.abs(yf[0:N\/\/2]))\nplt.grid()\nplt.show()","51341fc8":"#### Apply\n\ndef plot_fft(\n            signal: np.array, \n            show_original = True, \n            sample_rate = 250\n            ) -> None:\n    # Number of sample points\n    N = len(signal)\n    # sample spacing\n    T = 1.0\/sample_rate\n    x = np.linspace(0.0, N * T, len(signal), endpoint=False)\n    y = signal\n\n    print(\"Original waveform\")\n    plt.figure(figsize = (14, 2))\n    plt.plot(x, y)\n    plt.grid()\n    plt.show()\n\n    print(\"Frequency domain\")\n    yf = fft(y)\n    xf = fftfreq(N, T)[:N\/\/2]\n    plt.figure(figsize = (14, 2))\n    plt.plot(xf, 2.0\/N * np.abs(yf[0:N\/\/2]))\n    plt.grid()\n    plt.show()","26817c5d":"subject_id = 0\nclass_id = 0\n\nprint(\"After normalization\")\nprocessing_df = df_norm\ntmp_df = processing_df[(processing_df[\"subject_id\"] == subject_id) & (processing_df[\"class\"] == class_id)] \ntest_df = tmp_df\nplot_fft(test_df.values)\n\nprint(\"After Global Average Pooling (GAP)\")\nprocessing_df = df_conv\ntmp_df = processing_df[(processing_df[\"subject_id\"] == subject_id) & (processing_df[\"class\"] == class_id)] \ntest_df = tmp_df\nplot_fft(test_df.values)\n\nprint(\"After Moaving Average (MA)\")\nprocessing_df = df_norm_ma\ntmp_df = processing_df[(processing_df[\"subject_id\"] == subject_id) & (processing_df[\"class\"] == class_id)] \ntest_df = tmp_df\nplot_fft(test_df.values)\n\nprint(\"After Band Pass (BP)\")\nprocessing_df = df_norm_bp\ntmp_df = processing_df[(processing_df[\"subject_id\"] == subject_id) & (processing_df[\"class\"] == class_id)] \ntest_df = tmp_df\nplot_fft(test_df.values)\n\nprint(\"After Band Pass (BP) with normalization and moving average\")\nprocessing_df = df_bp_norm_ma\ntmp_df = processing_df[(processing_df[\"subject_id\"] == subject_id) & (processing_df[\"class\"] == class_id)] \ntest_df = tmp_df\nplot_fft(test_df.values)","6b361076":"class_status = [0, 1]\ndt = .01\nreverse_class_map = {v: k for k, v in class_map.items()}\ndf_processing = df_norm\n\nfor channel_id in tqdm(feature_cols):\n    for class_id in class_status:\n        plt.figure(figsize = (20, 2))\n        plt.legend(fontsize=20)\n        for subject_id in set(df_processing['subject_id'].unique()):\n            snippet = df_processing[(df_processing['subject_id'] == subject_id) \n                                  & (df_processing['class'] == class_id)][channel_id]\n\n            # plt.psd(snippet, Fs = len(snippet))\n            plt.psd(snippet**2, 512, 1.\/dt, label = f'{subject_id}')\n            title_txt = f\"PSD of the data for channel {channel_id}\"\n            title_txt += \" \" + f\"with class {reverse_class_map[class_id]}\"\n            plt.title(title_txt)\n\n    if channel_id == 7:\n        plt.legend(loc='upper center', \n                   bbox_to_anchor=(0.5, -0.25),\n                   ncol = 13, \n                   fontsize=12)\n\n    plt.show()\n    ","e1a35b06":"class_status = [0, 1]\ndt = .01\nreverse_class_map = {v: k for k, v in class_map.items()}\ndf_processing = df_conv\n\nfor channel_id in tqdm(feature_cols):\n    for class_id in class_status:\n        plt.figure(figsize = (20, 2))\n        plt.legend(fontsize=20)\n        for subject_id in set(df_processing['subject_id'].unique()):\n            snippet = df_processing[(df_processing['subject_id'] == subject_id) \n                                  & (df_processing['class'] == class_id)][channel_id]\n\n            # plt.psd(snippet, Fs = len(snippet))\n            plt.psd(snippet**2, 512, 1.\/dt, label = f'{subject_id}')\n            title_txt = f\"PSD of the data for channel {channel_id}\"\n            title_txt += \" \" + f\"with class {reverse_class_map[class_id]}\"\n            plt.title(title_txt)\n\n    if channel_id == 7:\n        plt.legend(loc='upper center', \n                   bbox_to_anchor=(0.5, -0.25),\n                   ncol = 13, \n                   fontsize=12)\n\n    plt.show()\n    ","7b7d3de4":"class_status = [0, 1]\ndt = .01\nreverse_class_map = {v: k for k, v in class_map.items()}\ndf_processing = df_norm_bp\n\nfor channel_id in tqdm(feature_cols):\n    for class_id in class_status:\n        plt.figure(figsize = figsize)\n        plt.legend(fontsize=20)\n        for subject_id in set(df_processing['subject_id'].unique()):\n            snippet = df_processing[(df_processing['subject_id'] == subject_id) \n                                  & (df_processing['class'] == class_id)][channel_id]\n\n            # plt.psd(snippet, Fs = len(snippet))\n            plt.psd(snippet**2, 512, 1.\/dt, label = f'{subject_id}')\n            title_txt = f\"PSD of the data for channel {channel_id}\"\n            title_txt += \" \" + f\"with class {reverse_class_map[class_id]}\"\n            plt.title(title_txt)\n\n    if channel_id == 7:\n        plt.legend(loc='upper center', \n                   bbox_to_anchor=(0.5, -0.25),\n                   ncol = 13, \n                   fontsize=12)\n\n    plt.show()\n    ","53b2a275":"class_status = [0, 1]\ndt = .01\nreverse_class_map = {v: k for k, v in class_map.items()}\ndf_processing = df_bp_norm_ma\n\nfor channel_id in tqdm(feature_cols):\n    for class_id in class_status:\n        plt.figure(figsize = figsize)\n        plt.legend(fontsize=20)\n        for subject_id in set(df_processing['subject_id'].unique()):\n            snippet = df_processing[(df_processing['subject_id'] == subject_id) \n                                  & (df_processing['class'] == class_id)][channel_id]\n\n            # plt.psd(snippet, Fs = len(snippet))\n            plt.psd(snippet**2, 512, 1.\/dt, label = f'{subject_id}')\n            title_txt = f\"PSD of the data for channel {channel_id}\"\n            title_txt += \" \" + f\"with class {reverse_class_map[class_id]}\"\n            plt.title(title_txt)\n\n    if channel_id == 7:\n        plt.legend(loc='upper center', \n                   bbox_to_anchor=(0.5, -0.25),\n                   ncol = 13, \n                   fontsize=12)\n\n    plt.show()\n    ","654a61a1":"import seaborn as sns\n\ndef visualize_corr(corr: np.array) -> None:\n    \n    print('Correlation Matrix')\n    \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 40, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(\n                corr, \n                mask=mask, \n                cmap=cmap, \n                vmax=corr.max().max(), \n                vmin=corr.min().min(), \n    #             center=0,\n                annot=True,\n                square=True, \n                linewidths=.5, \n                cbar_kws={\"shrink\": .5}\n               )\n\n    plt.show()\n    \n","8e177467":"df_processing = df_norm\n#Calculate correlation coefficients matrix across all EEG channels.\ncorr = df_processing[feature_cols + [\"class\"]].corr()\nvisualize_corr(corr)\n","0fe83481":"df_processing = df_conv\n#Calculate correlation coefficients matrix across all EEG channels.\ncorr = df_processing[feature_cols + [\"class\"]].corr()\nvisualize_corr(corr)\n","b9f2f101":"df_processing = df_norm_ma\n#Calculate correlation coefficients matrix across all EEG channels.\ncorr = df_processing[feature_cols + [\"class\"]].corr()\nvisualize_corr(corr)\n","7bee2609":"df_processing = df_bp_norm_ma\n#Calculate correlation coefficients matrix across all EEG channels.\ncorr = df_processing[feature_cols + [\"class\"]].corr()\nvisualize_corr(corr)\n","e81b3364":"from tsfresh import extract_features\nfrom tsfresh.feature_extraction import EfficientFCParameters, MinimalFCParameters\nfrom kats.tsfeatures.tsfeatures import TsFeatures\nfrom kats.consts import TimeSeriesData\n\n\ndef flatten_df(\n                df: pd.DataFrame\n                ) -> pd.DataFrame:\n    res_df = pd.DataFrame()\n    original_df_cols = df.columns \n\n    for i in tqdm(range(len(df))):\n        tmp_cols = [f\"{col}_{i}\" for col in original_df_cols]\n        tmp_row = pd.DataFrame(df.iloc[i]).T.reset_index(drop=True)\n        tmp_row.columns = tmp_cols\n\n        res_df = pd.concat([res_df, tmp_row], axis = 1)\n    return res_df\n\n\ndef convert_col_df(\n                    df: pd.DataFrame, \n                    feature_cols: list\n                    ) -> pd.DataFrame:\n    \n    col_df_res = pd.DataFrame()\n    \n    for col_id in tqdm(feature_cols):\n        col_df = df[[col_id]]\n        col_df.columns = [\"value\"]\n        col_df[\"col_id\"] = str(col_id)\n        col_df_res = col_df_res.append(col_df)\n        \n    return col_df_res\n        \n\ndef extract_kats_features(\n                    df: pd.DataFrame, \n                    time_col: str=\"\"\n                    ) -> dict:\n    \n    if time_col == \"\":\n        df.index.name = \"time\"\n        df = df.reset_index()\n        time_col = \"time\"\n    \n    # Construct TimeSeriesData object\n    ts = TimeSeriesData(df, time_col_name=time_col)\n    model = TsFeatures()\n    output_features = model.transform(ts)\n    output_features = pd.DataFrame(output_features, index = [0])\n    \n    return output_features\n\n    \ndef get_statistical_features(\n                            df: pd.DataFrame, \n                            feature_cols: list,\n                            window_size: int = 500,\n                            subset_percentiles: list = [],\n                            model: str = \"tsfresh\",\n                            n_jobs: int = 10,\n                            disable_progressbar: bool = True,\n                            ) -> pd.DataFrame:\n    \n    tf = pd.DataFrame()\n    df = df.reset_index(drop=True)\n    \n    if len(subset_percentiles) > 0:\n        subset_begin_idx, subset_end_idx = [int(len(df) * subset_percentiles[0]), int(len(df) * subset_percentiles[1] + 1)]\n        df = df.iloc[subset_begin_idx:subset_end_idx, :].reset_index(drop=True)\n    \n    df = convert_col_df(df, feature_cols)\n\n    for col_id in tqdm(df[\"col_id\"].unique()):\n        processing_df = df[df[\"col_id\"] == col_id].reset_index(drop=True)\n\n        for window_idx in tqdm(range(0, len(processing_df), window_size)):\n            if model == \"tsfresh\":\n                tmp_res = extract_features(df[window_idx:window_idx+window_size], \n                                                column_id=\"col_id\", \n                                                n_jobs = n_jobs,\n                                                show_warnings=False, \n                                                disable_progressbar=disable_progressbar,\n                                                default_fc_parameters=EfficientFCParameters()\n                                               )\n            elif model == \"kats\":\n                tmp_res = extract_kats_features(processing_df[window_idx : window_idx + window_size][\"value\"])\n                \n            else:\n                print(\"No model specified.\")\n                return None\n\n            tmp_res.columns = [f\"{col_name}_{col_id}_{window_idx}\" for col_name in tmp_res.columns]\n            tf = tf.append(tmp_res)\n\n\n    tf = flatten_df(tf.reset_index(drop=True))\n    \n    return tf","1a1f7929":"test_df","29da600e":"tf = get_statistical_features(test_df, feature_cols, window_size = 1000, subset_percentiles = [0, 0.1])","fb8c8c29":"tf","39664ede":"tf = get_statistical_features(test_df, feature_cols, window_size = 1000, model=\"kats\")","6c6b1203":"tf","5fec7a82":"tf = get_statistical_features(test_df, feature_cols, window_size = 20000, model=\"kats\")","d1812678":"tf","e17acc73":"del tf\ngc.collect()","c4edd602":"df_processing = df_norm_ma\nargs_list = {\"window_size\": 1000, \"feature_cols\": feature_cols, \"subset_percentiles\": [0, 0.01]}\n\nstats_feature_df = filter_wrapper(get_statistical_features, df_processing, **args_list)","14f88770":"stats_feature_df","bfae58ea":"df_processing = df_norm_ma\nargs_list = {\"window_size\": 1000, \"feature_cols\": feature_cols, \"subset_percentiles\": [0, 0.01], \"model\": \"kats\"}\n\nstats_feature_df_k = filter_wrapper(get_statistical_features, df_processing, **args_list)","a5aa28a6":"stats_feature_df_k","c1c8156b":"from sklearn.preprocessing import MinMaxScaler","dfd73ffb":"add_features = stats_feature_df_k.columns.drop([\"subject_id\", \"class\"])\ntarget_feature = [\"class\"]","b36402e6":"stats_feature_df_k.replace([np.inf, -np.inf, np.nan], 0, inplace=True)","5b6064bf":"stats_feature_df_k.shape","f46622d0":"stats_feature_df_k = stats_feature_df_k.loc[:, (stats_feature_df_k != 0).any(axis=0)]","e9d19d7a":"stats_feature_df_k.shape","9c233c35":"add_features = stats_feature_df_k.columns.drop([\"subject_id\", \"class\"])","067327e2":"scaler = MinMaxScaler()\nstats_feature_df_k[add_features] = scaler.fit_transform(stats_feature_df_k[add_features])","b26353a1":"add_features = stats_feature_df_k.columns.drop([\"subject_id\", \"class\"])\nX = stats_feature_df_k[add_features]\nY = stats_feature_df_k[target_feature]","a011c1d9":"from sklearn.feature_selection import SelectKBest, chi2","af55ea20":"print(\"As a rule of thumb, feature number should be less than the number of samples to avoid overfitting \/ feature memorisation.\")\nprint(\"Therefore, the feature number should be less than: \", X.shape[0])","dc27c922":"# feature ranking\nfeature_selector = SelectKBest(score_func=chi2, k=min(X.shape))\nfeature_selection = feature_selector.fit(X, Y)","fe867ac9":"print(\"The most explanable features according to the univariate statistical tests: \")\nfeature_selection = add_features[feature_selection.get_support()]\nprint(feature_selection)","88232009":"tuncated_X = X[feature_selection]\ntuncated_X","602b70cf":"from sklearn.decomposition import PCA\n\ndef get_feature_explains(tuncated_X: pd.DataFrame) -> list:\n    # For it to includes all the features (must be less than no. samples due to covariance matrix calculation), n_components need to be set explicitly.\n    pca = PCA(n_components = tuncated_X.shape[1]) # if n_component parameter is not set, it does n_components == min(n_samples, n_features). \n    pca.fit(tuncated_X)\n\n    percent_var_explains = pca.explained_variance_ratio_\n    cum_sum_var_explains = np.cumsum(percent_var_explains)\n    return cum_sum_var_explains\n","a04aafc0":"cum_sum_var_explains = get_feature_explains(tuncated_X)","92ef2a1b":"def get_feature_by_var_exp(cum_sum_var_explains: list, best_explain_percentage: float = 0.95) -> Tuple[int, dict]:\n    \n    best_num_features = len(cum_sum_var_explains)\n    feature_explain_dict = {}\n\n    for min_percent in [0.8, 0.9, 0.95]:\n        percent_th_list = [i for i in range(len(cum_sum_var_explains)) if cum_sum_var_explains[i] >= min_percent]\n        if len(percent_th_list) > 0:\n            num_features = min(percent_th_list)\n            feature_explain_dict[num_features] = min_percent\n\n            if min_percent == best_explain_percentage:\n                best_num_features = num_features\n            \n    return best_num_features, feature_explain_dict","e31c830f":"best_num_features, feature_explain_dict = get_feature_by_var_exp(cum_sum_var_explains, best_explain_percentage = 0.95)","53f1d454":"best_explain_percentage = 0.95\nbest_num_features = len(cum_sum_var_explains)\nfeature_explain_dict = {}\n\nfor min_percent in [0.8, 0.9, 0.95]:\n    num_features = min([i for i in range(len(cum_sum_var_explains)) if cum_sum_var_explains[i] >= min_percent])\n    feature_explain_dict[num_features] = min_percent\n    \n    if min_percent == best_explain_percentage:\n        best_num_features = num_features","d4b3d3a1":"plt.figure(figsize=figsize_md)\nplt.plot(cum_sum_var_explains, color = 'r', label='explain curve')\n\nfor num_features, min_percent in feature_explain_dict.items():\n    plt.vlines(num_features, 0, 1, color='b', linestyles=\"dashed\", label = f\"Explained % {min_percent} with {num_features} features\")\n\nplt.title(\"Accumulated % variance explained by the number of features\")\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Variance Explained\")\nplt.grid()\nplt.legend(loc='right')\nplt.show()","baf2d242":"topk_feature_selection = feature_selection[: best_num_features]\nprint(\"Selected features for modelling for this sample batch (features could varied from batch to batch)\", end=\"\\n\\n\")\nprint(topk_feature_selection)","25657213":"def get_key_features(stats_feature_df: pd.DataFrame, best_explain_percentage: float = 0.95) -> list:\n    \n    # Feature processing     \n    ## Ensure values can be processed\n    stats_feature_df.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n    \n    stats_feature_df[\"subject_id\"] = stats_feature_df[\"subject_id\"].apply(lambda x: str(x))\n    ## Drop 0s columns\n    stats_feature_df = stats_feature_df.loc[:, (stats_feature_df != 0).any(axis=0)]\n    \n    # Define feaute search space \n    add_features = stats_feature_df.columns.drop([\"subject_id\", \"class\"])\n    target_feature = [\"class\"]\n    \n    ## Min-max scaling of feature values     \n    scaler = MinMaxScaler()\n    stats_feature_df[add_features] = scaler.fit_transform(stats_feature_df[add_features])\n    \n    # Feature selection\n    ## Update feature space and get X, Y for feature     \n    add_features = stats_feature_df.columns.drop([\"subject_id\", \"class\"])\n    X = stats_feature_df[add_features]\n    Y = stats_feature_df[target_feature]\n    \n    ## Features ranking     \n    feature_selector = SelectKBest(score_func=chi2, k=min(X.shape))\n    feature_selection = feature_selector.fit(X, Y)\n    feature_selection = add_features[feature_selection.get_support()]\n    \n    ## Update featuree space for further selection     \n    tuncated_X = X[feature_selection]\n    cum_sum_var_explains = get_feature_explains(tuncated_X)\n    ## Define the best number of features to be feature set that explain 95% of variance based on the ranking of univariate statistical tests.      \n    best_num_features, feature_explain_dict = get_feature_by_var_exp(cum_sum_var_explains, best_explain_percentage = best_explain_percentage)\n    topk_feature_selection = feature_selection[: best_num_features]\n    \n    return list(topk_feature_selection)","a3a43e1d":"topk_feature_selection = get_key_features(stats_feature_df_k, best_explain_percentage = 0.95)\nprint(topk_feature_selection)","e7a17de2":"def extract_select_features_wrapper(\n                                    df: pd.DataFrame, \n                                    batch_percentage: int = 1, \n                                    best_explain_percentage: float = 0.95,\n                                    model: str = \"kats\",\n                                    sample_percentages: int = [0, 100],\n                                    n_jobs: int = 10\n                                   ) -> pd.DataFrame:\n    \n    res_df = pd.DataFrame()\n    feature_selection_log = {}\n    step_size = batch_percentage \/ 100\n    for i in tqdm(range(sample_percentages[0], sample_percentages[1], batch_percentage)):\n        args_list = {\"window_size\": 1000, \n                     \"feature_cols\": feature_cols, \n                     \"model\": model,\n                     \"n_jobs\": n_jobs,\n                     \"subset_percentiles\": [i \/ 100, i + step_size]}\n        print(\"Extracting statistical features:\")\n        stats_feature_df = filter_wrapper(get_statistical_features, df, **args_list)\n\n        # Feature selections     \n        print(\"Running automated feature seleciton\")\n        selected_features = get_key_features(stats_feature_df, best_explain_percentage = best_explain_percentage)\n        feature_selection_log[i] = (len(selected_features), len(stats_feature_df))\n        selected_features = selected_features + [\"subject_id\", \"class\"]\n        stats_feature_df = stats_feature_df[selected_features]\n        \n        res_df = pd.concat([res_df, stats_feature_df], axis = 1)\n        \n    return res_df","8dbe9c6e":"df_processing = df_norm_ma\nfeatures_df_k = extract_select_features_wrapper(df_processing, \n                                                batch_percentage = 1, \n                                                sample_percentages = [0, 10],\n                                                model = \"kats\",\n                                                best_explain_percentage=0.95)","21b687e3":"def format_model_input(df, feature_cols):\n    \"\"\" Adjust data structure for modelling\n    \"\"\"\n    sample_size = 5000\n    model_df = pd.DataFrame()\n    \n    for subject_id in tqdm(df[\"subject_id\"].unique()):\n        for class_id in df[\"class\"].unique():\n            tmp_df = df[(df[\"subject_id\"] == subject_id) & (df[\"class\"] == class_id)][feature_cols]\n\n            tmp_list = []\n            for col in tmp_df.columns:\n                tmp_list.append(pd.Series(tmp_df[col].values[:sample_size]))\n\n            tmp_df = pd.DataFrame([tmp_list], columns = tmp_df.columns)\n            tmp_df[\"class\"] = class_id\n\n            if model_df.empty:\n                model_df = tmp_df\n\n            else:\n                model_df = pd.concat([model_df, tmp_df], axis = 0)\n\n\n    model_df = model_df.reset_index(drop=True)\n\n    gc.collect()\n    \n    return model_df","7eec96d4":"model_df = {}\nfeature_cols = [i for i in range(8)]\n\npreprocess_df = df_norm\nmodel_df[\"norm\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_conv\nmodel_df[\"conv\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_norm_ma\nmodel_df[\"norm_ma\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_conv_ma\nmodel_df[\"conv_ma\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_norm_bp\nmodel_df[\"norm_bp\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_norm_ma_bp\nmodel_df[\"norm_ma_bp\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_norm_conv_ma_bp\nmodel_df[\"norm_conv_ma_bp\"] = format_model_input(preprocess_df, feature_cols)\n\npreprocess_df = df_bp_norm_ma\nmodel_df[\"norm_bp_norm_ma\"] = format_model_input(preprocess_df, feature_cols)","90fb4dc8":"from sklearn.model_selection import train_test_split\n\n\ndef train_val_split(df, feature_cols, verbose=False):\n    X = df[feature_cols]\n    y = df['class']\n\n    X_train, X_val, y_train, y_val = train_test_split(\n                                                    X, \n                                                    y, \n                                                    test_size=0.2, \n                                                    random_state=42\n                                                    )\n\n\n    if verbose:\n        print(f\"Size of  dataset (unit = one of the conditions of one of the subject ids): \")\n        print(f\"Train: {len(y_train)}\")\n        print(f\"Test: {len(y_val)}\")\n    \n    return X_train, X_val, y_train, y_val","8992a0da":"def pred_random_guess(X: pd.DataFrame) -> list:        \n    return [randint(0, 1) for i in range(len(X))]\n","8b5a344e":"def eval_results(\n                y_true: pd.Series, \n                y_pred: pd.Series\n                ) -> dict:\n    eval_res = {}\n\n    # Accuracy\n    eval_res[\"Accuracy\"] = accuracy_score(y_true, y_pred)\n    # Balanced accuracy\n    # When true, the result is adjusted for chance, so that random performance would score 0, while keeping perfect performance at a score of 1.\n    eval_res[\"Balanced_Accuracy\"] = balanced_accuracy_score(y_true, y_pred, adjusted=False) \n    # Confusion matrix - tn, fp, fn, tp\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    eval_res[\"tn\"] = tn\n    eval_res[\"fp\"] = fp\n    eval_res[\"fn\"] = fn\n    eval_res[\"tp\"] = tp\n    eval_res[\"Confusion_Matrix\"] = [confusion_matrix(y_true, y_pred).ravel()]\n    # Precision\n    eval_res[\"Precision\"] = precision_score(y_true, y_pred)\n    # Recall\n    eval_res[\"Recall\"] = recall_score(y_true, y_pred)\n    # F1 score that summarise precision and recall with 1:1 importance ratio.\n    eval_res[\"F1\"] = f1_score(y_true, y_pred)\n    # Where the beta let us tune the weights on precision-recall ratio for score.\n    # The higher the beta, the higher importance placed on the precision of the predictions.    \n    eval_res[\"F1_b\"] = fbeta_score(y_true, y_pred,beta=0.5)\n    # Log Loss\/Binary Crossentropy \n    #     - where y_pred = probabilities and y_true are binary class labels\n    eval_res[\"Log_loss\"] = log_loss(y_true, y_pred, eps=1e-15)\n    # the measure of the ability of a classifier to distinguish between classes\n    #     - used as a summary of the ROC curve.\n    eval_res[\"AUC\"] = roc_auc_score(y_true, y_pred)\n    \n    return eval_res\n","77346559":"model_df.keys()","1dc2e995":"X_train, X_val, y_train, y_val = train_val_split(model_df['norm'], feature_cols)\n\neval_res = {}\ndict_id = str(len(eval_res))\neval_res[dict_id] = {}\neval_res[dict_id][\"model\"] = \"random\"\neval_res[dict_id][\"preprocess\"] = \"NA\"\ny_pred = pred_random_guess(X_val)\neval_res[dict_id].update(eval_results( \n                                y_true = list(y_val),\n                                y_pred = y_pred\n                                ))\n\neval_summary = pd.DataFrame(eval_res).T\ndisplay(eval_summary)","d1be5348":"def fit_concat_forest_model(X_train, y_train):\n    \"\"\" Time series concatenation\n    \"\"\"\n    steps = [\n        (\"concatenate\", ColumnConcatenator()),\n        (\"classify\", TimeSeriesForestClassifier(n_estimators=100)),\n    ]\n    classifer = Pipeline(steps)\n    classifer.fit(X_train, y_train)\n    \n    return classifer\n\n\ndef fit_concat_shaplet_model(X_train, y_train):\n    \"\"\" Time series concatenation\n    \"\"\"\n    steps = [\n        (\"concatenate\", ColumnConcatenator()),\n        (\"classify\", ShapeletTransformClassifier(n_estimators=100)),\n    ]\n    classifer = Pipeline(steps)\n    classifer.fit(X_train, y_train)\n    \n    return classifer\n    \n    \ndef fit_columns_ensemble_model(X_train, y_train):\n    \"\"\" Column ensembling\n    \"\"\"\n    classifier = ColumnEnsembleClassifier(\n                        estimators=[\n                            (\"TSF0\", TimeSeriesForestClassifier(n_estimators=100), [0]),\n                            (\"BOSSEnsemble3\", BOSSEnsemble(max_ensemble_size=5), [3]),\n                        ]\n                    )\n    classifier.fit(X_train, y_train)\n    \n    return classifier\n\n\ndef fit_mr_seql_model(X_train, y_train):\n    \"\"\" Bespoke classification algorithms\n    \"\"\"\n    classifier = MrSEQLClassifier()\n    classifier.fit(X_train, y_train)\n    \n    return classifier\n\n","af929963":"classifiers = {}\neval_res = {}","cbcc8e06":"for k, v in tqdm(model_df.items()):\n    dict_id = str(len(eval_res))\n    eval_res[dict_id] = {}\n    eval_res[dict_id][\"model\"] = \"concat_forest\"\n    eval_res[dict_id][\"preprocess\"] = k\n    \n    X_train, X_val, y_train, y_val = train_val_split(v, feature_cols)\n    classifiers[f\"concat_forest\"] = fit_concat_forest_model(X_train, y_train)\n    y_pred = classifiers[\"concat_forest\"].predict(X_val)\n    \n    eval_res[dict_id].update(eval_results(y_true = y_val, y_pred = y_pred))\n    ","4041dcbe":"eval_summary = pd.DataFrame(eval_res).T\ndisplay(eval_summary)","a4e35431":"plt.bar(x = eval_summary[\"model\"], y = eval_summary[\"AUC\"])","4d9cfcc8":"# classifiers[\"ensemble_forest\"] = fit_columns_ensemble_model(X_train, y_train)\n# res[\"ensemble_forest\"] = classifiers[\"ensemble_forest\"].predict(X_test)\n\n# classifiers[\"mr_seql\"] = fit_mr_seql_model(X_train, y_train)\n# res[\"mr_seql\"] = classifiers[\"mr_seql\"].predict(X_test)","d4705799":"eval_summary = pd.DataFrame(eval_res).T\ndisplay(eval_summary)","eb874f7c":"# import pickle\n\n# model_folder = \"\"\n# version = \"0_1_0\"\n\n# # Save models\n# for model_name, model in classifiers.items():\n#     filename = f\"{model_name}_{version}.sav\"\n#     pickle.dump(model, open(filename, \"wb\"))","4cc6ffed":"### ","4bc80928":"#### Check file naming convention\n- There can be typo or inappropriate files being shared across.","1904adde":"## Step 3. Data quality check and cleaning","e97fc24b":"## References\n\n- sktime: https:\/\/github.com\/alan-turing-institute\/sktime","b078cac1":"### Standardisation of added features","2fc64f78":"#### Fast Fourier Transform (FFT) for main frequencies profile","1aa13327":"#### For 'Eyes Opened' and 'Eyes Closed' conditions across all channels","85b0eaed":"#### Class value encoding","ac96f9e7":"#### Comment:\n- There is no 'ec' complement for 'OpenBCI-RAW-con42_eo.txt'. \n- Its data format is different to others (\"\\t\" delimited instead of \",\" and 19 columns instead of 8). \n- We treat the con42 data as corrupted and ignore it for now.","6ecec168":"### Passband Filter to get frequency between 1 Hz and 120 Hz","56a83eb2":"#### subject_id encoding\n- The actual name of subjects (even just the first name) are personal identifiable information which should always be masked \/ dropped \/ or encoded before sharing across or processing.","a791570a":"#### 'Eyes Opened' condition vs 'Eye Closed condition","aaae4f47":"#### Test","917b3227":"#### Time series standardisation and normalization","789ce11c":"## Step 1: Inspect sample\n- Using top 4 rows and describe to undertstand the expected scale of each column","8f314484":"Check filenames. Each subject should have two datasets (con(id) _ (name) _ (eo\/ec)).","fa20e637":"## Step 2: Load datasets\n- Loading with the insights from the previosu step.\n- If there is a memory problem in loading all data at once, we can opt for batch processing strategy.","c7d57e53":"#### Apply","80bdb5ae":"#### Replace inf or -inf with 0s","0bfdae9a":"#### Comment:\n- Due to the explosion of features and the lack of flexibiity of adjusting the number of features, with the limited computing resources (specifically RAM), here are some of the options to approach this:\n1. An effective feature selection method.\n2. A less intensive alternative is needed for the feature extraction stage. \n- However, the speed is concerning for this feature generation method, since a lot of redundant features have to be generated first. It could be helpful as a thorough and more exhausive search. For our purpose, we would look at a more lightweight feature generation package.","7814e850":"#### After BP + NORM + MA ","7c682d77":"#### Test","248a0517":"### High-level diagram of the steps\n![image.png](attachment:3fa2f2ac-0677-42c6-8660-39bc7b31acd7.png)","3d257444":"### Feature importance and selection","023cc8a4":"#### After convolution","4301f702":"## Step 4. Data Preprocessing and Feature Extraction","3fe43208":"#### Comment\n- There are outliers in sequence lenghts over subject_id for both classes. For modelling, we need to define input sequence length.\n- To think about this decision, we can think about the end cases, and some options. If:\n    - Taking the longest sequence as input size, we ensure the full utilization of available data, but the padding of all other shorter sequences mean sparse data.\n    - Taking the shortest sequence as input size, we are potentially missing a lot of valuable training data for model improvements.\n    - Taking a certain quantile while keeping > 90% of data. \n    \n- How about using Tukey's fences (Lower Quartile - 1.5 * IQR, Upper Quartile + 1.5 * IQR) for outlier removal of sequence lenghts? We don't need the lower bound, but we can use the upper bound. Statistically, the upper bound by Tukey's rule uses Q3 + 1.5 * IQR (If Q3 is ~ 0.675\ud835\udf0e from mean, then it is using ~ 675\ud835\udf0e + 1.5 x 1.35\ud835\udf0e = 2.7\ud835\udf0e) as cutoff.\n","865b5ae4":"## Evaluation\n- Confusion matrix with Accuracy, Precision, Recall, F1 score\n- Log Loss\/Binary Crossentropy\n- Area Under the Curve (AUC)","8e911730":"#### Comment:\n- As more filters have been applied, the correlation between channel(3, 0), channel(3, 2) and channel (5, 4) decreased. The original correlation might be induced by background noise. \n","0287990f":"### Results Comparison","91683989":"### Comment:\n- Some extremely long-tailed sequences are trimmed to the Tukey's 1.5k upper bound for standardisation.","27deea45":"#### Apply to channel 1, because it has more fluctuations","ccd44499":"### Apply feature extraction and selection pipeline over the entire dataset","b833edaa":"#### Plot PSD for amplitudes of different frequencies\n\nRef: \n- https:\/\/matplotlib.org\/stable\/gallery\/lines_bars_and_markers\/psd_demo.html\n- https:\/\/matplotlib.org\/stable\/gallery\/lines_bars_and_markers\/psd_demo.html\n- https:\/\/www.geeksforgeeks.org\/matplotlib-pyplot-psd-in-python\/","c9327ea5":"### 3.1. Tukey fences (for univariate outliers)\n- John Tukey suggested an intuitive statistically proven test for outliers, with the following guidelines:\n    - k = 1.5 indicates an \"outlier\".\n    - k = 3 indicates data that is \"far out\". \n- Nonparametric Tukey Fences are robust methods in detecting outliers.","dd3f5a72":"### Cleaning of extra features","0090241c":"### Wrapper function for feature extraction + feature selection","dbbc279b":"### Model Candidates\n\n#### Multivariate time-series classification\n- sktime \n    - Offers three main ways of solving multivariate time series classification problems:\n\n        1. Concatenation of time series columns into a single long time series column via ColumnConcatenator and apply a classifier to the concatenated data,\n\n        2. Column-wise ensembling via ColumnEnsembleClassifier in which one classifier is fitted for each time series column and their predictions aggregated,\n    \n        3. Bespoke estimator-specific methods for handling multivariate time series data, e.g. finding shapelets in multidimensional spaces (still work in progress).","0b354824":"#### Comment\n- Some signals contain a lot of spikes and constant segments.\n- Some signals contain trends and offsets.","c780983a":"### 3.3. High-level statistics","ff5956b8":"#### Install the required packages and libraries (Restart kernel after installing for updated import namespace)","4d493fc6":"#### Drop 0s columns","ad2a8c77":"<hr style=\"border:1px solid gray\"> <\/hr>\n","8af6a2c8":"### 4.2 Context background\n\nEEG: The first known neurophysiologic recordings of animals were performed by Richard Caton in 1875. The advent of recording the electrical activity of human beings took another half century to occur. Hans Berger, a German psychiatrist, pioneered the EEG in humans in 1924. The EEG is an electrophysiological technique for the recording of electrical activity arising from the human brain. Given its exquisite temporal sensitivity, the main utility of EEG is in the evaluation of dynamic cerebral functioning. [1]\n\n\n#### Literature Review (Due to time constrait, it is not in great depth.):\n1. .\n2. \n3. \n4. \n5. \n6. \n\n\n\n#### Conventions in EEG data modelling:\n- Normalization should be done at individual level before aggregation of data from different subjects.\n\n\n\nRef:\n[1] Electroencephalography (EEG): An Introductory Text and Atlas of Normal and Abnormal Findings in Adults, Children, and Infants [Internet]. https:\/\/www.ncbi.nlm.nih.gov\/books\/NBK390346\/ ","987cb1f0":"### 3.5. Summary:\n- Now we have data from 39 subjects.\n- Each with different class ration and sample sizes.\n- Average sample size \n    - Total: 161,745\n    - ec: 79,789\n    - eo: 81,956\n- On Average it is 49.5% : 50.5% to ec : eo.\n- The eye opened data has larger sample size than eye closed, with more significant difference in max values. \n- Each subject has data for both ec and eo, and there is no missing data.\n- Class imbalanced condition is not significant for class imputation to be beneficial. ","a0567449":"## Conclusion","fe1efe25":"### 3.2. Missing data handling","e7533209":"### Baseline - Random guess ","713cb746":"### Step 4.3. Data Cleaning \n\n#### Description:\n- As part of the preprocessing, the aim of data cleaning is to maximise the signal to noise (S\/N) ratio. There are 3 main sources of noise and artifects for EEG data: \n    1. Biological noise: Frequency flutuations that are originated by heartbeats, blinkings, body temperature, and mood, etc.\n    2. Environmental noise: The effects of the surround stimulants to our brain wave, such as sounds of someone walking by, the movement of shadows or reflection on window or screen when someone is moving nearby, the sound of air conditioner. Basically any change in the surroundings that trigger any micro-reactions.  \n    3. White noise: (high frequency background noise that is much higher than our normal brain frequency of 0.5 Hz to 120 Hz (referring to delta wave to gamma wave, with gamma wave conventionally having 40 Hz as the experimental signal upper-bound.))\n\n#### Implementation Plan\n- Normalization and Standardisation\n- Convolution: Global Average Pooling \/ Global Max Pooling\n- Moving Average \n- Passband Filter","4bdbdb17":"#### Comment\n- The 63 features above are selected based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.\n    - Reference: \n        - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.SelectKBest.html\n        - https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html#univariate-feature-selection","9b03ac8c":"#### Check class distribution over subject_id","2ef9aba1":"# Pipeline design:\n#### 1. Inspect sample\n#### 2. Load datasets \n#### 3. Data quality check and cleaning\n    3.1. Tukey fences (for univariate outliers)\n    3.2. Missing data handling\n    3.3. High-level statistics\n    3.4. Class distirbution check and handling\n    3.5. Summary of the above\n    3.6. Splitting the dataset (hold out test dataset)\n#### 4. Data Preprocessing and Feature Extraction\n    4.1. Project Background\n    4.2. Context Background\n    4.3. Data Cleaning (Not in the following order, applying the following with different combinations.)\n        4.3.1. Normalization & Standardization\n        4.3.2. Convolution\n        4.3.3. Moving Average\n        4.3.4. Passband Filter (With domain heuristics)\n    4.4. Signal Characterization\n        4.4.1. Signal Entropy\n        4.4.2. Fast Fourier Transform (FFT)\n        4.4.3. Power Spectral Density (PSD)\n        4.4.4. Channels correlations\n    4.5. Features Extraction\n        4.5.1. Get statistical features\n        4.5.2. Autoencoder\n    \n#### 5. Modelling\n    5.1. Baseline Setting \n    5.2. Evaluation Pipeline Setting\n    5.3. Building Model Candidates \n    5.4. Results Aggregation\n    \n#### 6. Reaults & Discussion\n    6.1 Results Comparisons\n    6.2 Discussion \n\n#### 7. Conclusion\n    7.1. Summary of Project\n    7.2. Further Improvement and Investigations\n    \n#### 8. References and Appendix\n    8.1 References\n    8.2 Appendix\n","b39d5b3f":"#### Comment\n- Based on PCA variance explained, using 12 features could explain >= 95% of variance. \n- Use the first 12 features of the feature set found by SelectKBest feature selector (since the feature set was sorted by importance). \n- Further details about using PCA for finding feature number to explain variance.\n    - Refernce:\n        - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\n        - https:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_pca_iris.html\n        ","c263ff03":"## Implementation Plan","24c2d91a":"### sktime","73ba9666":"#### After bandpass filter","6453d55f":"<hr style=\"border:1px solid gray\"> <\/hr>","016f8fc2":"### 4.1. Project Background\n\n#### Description\n\nThis is the eight channel EEG raw data collected from human brain under controlled environment having no internal or external noise. The data was collected from COMSATS university Islamabad. Data is composed of 90 % students and 10 % teachers. For every subject, two set of data was made. First set of data was taken when the subject had eyes open, and second set was taken when eyes of the subject were closed. The hardware collecting the data was CYTON 8th channel device.\n\n- Each Column is a channel value - so there are 8 channels - Use them as variables\n- The End of file name tell us if eye is close or open - Use this as class\n- For example EO means eye open and EC means eye closed\n\n##### Note (According to the data provider):\n- The sampling rate is 250 records per second.\n- The sample size depends on the duration of collecting data (250 records\/second), for each subject and condition duration was not same.\n\n#### Goal: Detect whether a person is in deep sleep or not.\n\n#### Objectives targets (following the iterative CRISP-DM data exploration framework):\n\n#### Without previous experience in handling EEG data, the targets and estimated time cost are listed below:\n- T1: Input data formatting (15m)\n- T2: 1st stage EDA & visualisation (30m)\n    - Background review for context and options (2h)\n- T3: Initial preprocessing (30m)\n- T4: 2nd stage EDA & visualisation (30m)\n- T5: Design Evaluation Metrics (45m)\n- T6: Baseline Model and Pipeline (30m)\n- T7: Models Investigation (1h30m - 2h)\n- T8: Models Building and Comparison (1h45m)\n\nTotal time estimated: (8h15m - 9h45m)\n","55c36701":"#### Confusion matrix with Accuracy, Precision, Recall, (weighted) F1 score","aa1eccc7":"#### Test","832180c7":"### Feature selection with univariate statistical tests","c6cc9ee5":"### 3.6 Splitting the dataset (Hold out test dataset)","48fc50c4":"#### Test","bda6e928":"### Summary of data cleaning","32302cbc":"#### Inspect the files failed to be loaded","8eee1911":"#### Demo","d9b4de24":"#### After normalisation","40209107":"### Global Average Pooling (GAP) for Convolution","951ff238":"### Apply at Scale","df784765":"#### Comment:\n- From the 39 subjects each with both eo and ec classes, the average class differences is 4.3%, with a std of ~6%. \n- The highest class imbalanced subject has class difference of 32.8%, whereas the subject with the lowest imbalanced has only 0.04% difference.\n\nConclusion:\n- Imputation is not needed but could be an option for further optimisation.","cd060b6d":"### Step 5. Modelling\nChecklist\n- Data formatting for models\n- Baseline\n- Evaluation\n- Model Candidates\n- Results Comparison\n- Summary","06b2c500":"#### Correlations between channel","7177d969":"#### Test","6f107aa2":"#### Batching and processing segmentations separately","58392d4c":"#### Shape of the filter","add0f2e3":"#### Save models","8e2dc7ca":"<hr style=\"border:1px solid gray\"> <\/hr>\n","99957420":"### 3.4. Class distirbution check and handling","92f1ca82":"### 4.5. Features Extraction\n\nIntro:\n- Feature extraction is an important step in the process of electroencephalogram (EEG) signal classification.\n- Here we conduct experiments on features extraction by:\n    - statistical appraoch with tsfresh.\n    - \u201cpattern recognition\u201d approach.\n    - neural network approach with autoencoder. \n\n##### Plan for the experiments:\n- tsfresh - A python package that automates the extraction of empirically tested to be relevant features. \n    - Documentation for the calculated features: https:\/\/tsfresh.readthedocs.io\/en\/latest\/api\/tsfresh.feature_extraction.html\n    \n##### Suggested alternatives:\n- Autoencoder","2dee2729":"#### Comment:\n- Most of the data samples are matched with subject_id as keys, each with ec and eo as samples. \n- For the \"con19_mubasira\" and \"con19_mubashira\", the names are similar with complementary 'ec' and 'eo'. With this assumption in mind, let's put them together for analysis.","cf86c632":"### Comment:\n- A test set is hold out for downstream results without accidentally being used in training. ","dc100d44":"#### Comment:\n- We sampled 10 files to inspect their first 4 rows and the high-level statistics of their columns. \n- The files have varied number of of sample sizes, which means the duraction of recording varied.\n- From the samples, the column '8' is an empty column, it is expected because we only have 8 channels in placed (columns 0 - 7).\n- Some files contain mostly negative values across the channels.\n- Some samples have first row with 0s across the channels. \n- 2 files were not loaded, further inpsection of them is needed.","f35c8fd7":"### 4.5.2. Facebook's Kats package for time-series feature extraction","0df2bb91":"### Step 4.4. Signal Characterization\n\n#### Implementation Plan:\n1. Signal Entropy: To evaluate the randomness of the signal from each channel\n    - Reason: Applying the concept of entropy to time series like electroencephalography (EEG) is a way to quantify, in a statistical sense, the amount of uncertainty or randomness in the pattern, which is also roughly equivalent to the amount of information contained in the signal.\n2. Fast Fourier Transform (FFT): To set cut-off frequency or passband filter to remove noisy signal by frequencies.\n    - Reason: As one of the traditional and verified techniques capable of extracting features from EEG signals. If an EEG signal is recorded at a sampling frequency of 100 Hz, the FFT can separate the signal into features in the range of 0\u2013100 Hz.It is great at analyzing vibration when there are a finite number of dominant frequency components. Therefore, we can lay out the spectrum of the signals that combine to form the resultant signals.\n3. Power Spectral Density (PSD): Especially for the characterisation random vibration signals. It describes how the power of your signal is distributed over frequency.\n    - Reason: To gain a better physical understanding of how much the frequencies contribute to the resultant signals in terms of power.\n4. Correlations between channels: Finding the correlations between channels to observe the existance of collinearities. \n    \n    ","f3914268":"### Build Feature selection \n- Using 1% subset as a trial run to test feature selection mechanism.","62cc8b91":"#### 4.5.1. Extract statistical features with tsfresh","3c41c125":"#### Apply to our sample data","69361d7a":"##### Using subject id 0 and class 0 as sample to inspect effects","a2a37909":"#### Moving Average for spikes smoothing"}}