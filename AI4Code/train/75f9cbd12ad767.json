{"cell_type":{"16714b24":"code","402c7d02":"code","b469583f":"code","5d6bbc24":"code","fb0b1d50":"code","c819826e":"code","92fc87ba":"code","ca6575ff":"code","4694318a":"code","7bdf047c":"code","fa2e10ca":"code","5a0c530d":"code","eed80526":"code","b0a4fba6":"code","b18e56e9":"code","0e3d6552":"code","64f0c4ad":"code","31089d82":"code","19935b9d":"code","efdb1644":"code","7d8ab41e":"code","5d2f1eda":"code","f79c7437":"code","6d7fba10":"code","3e4a4935":"code","0cca4fb6":"code","113af9c9":"markdown","3e426ccb":"markdown","b59a5c42":"markdown","22453cb3":"markdown","4553e1d8":"markdown","f3e8c4b3":"markdown","60b340a0":"markdown","94f4eb46":"markdown","be70c3ee":"markdown","7b393837":"markdown","34056779":"markdown","74fdda2e":"markdown","8b346eab":"markdown","0b7f6101":"markdown","916a6db9":"markdown"},"source":{"16714b24":"import itertools\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pylab as py\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline","402c7d02":"!pip install pydotplus","b469583f":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree","5d6bbc24":"import collections\nimport pydotplus\nimport matplotlib.image as mpimg","fb0b1d50":"df = pd.read_csv(\"..\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")\ndf.head()","c819826e":"test_data = pd.read_csv(\"..\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ntest_data.head()","92fc87ba":"df = df.dropna()\ndf.head()","ca6575ff":"df['breed_category'].value_counts()","4694318a":"X = df[['length(m)', 'height(cm)',\"X1\",\"X2\"]]\nX[0:5]","7bdf047c":"y = df[\"pet_category\"]\ny[0:5]","fa2e10ca":"X = preprocessing.StandardScaler().fit(X).transform(X.astype(float))","5a0c530d":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3, random_state = 123)","eed80526":"print(\"Training Set: \",X_train.shape,y_train.shape)\nprint(\"Testing Set: \",X_test.shape,y_test.shape)","b0a4fba6":"from sklearn.neighbors import KNeighborsClassifier\nk = 9\nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh","b18e56e9":"y_hat = neigh.predict(X_test)\ny_hat[0:5]\nnp.unique(y_hat)","0e3d6552":"from sklearn import metrics\nprint(\"Accuracy Score is : \", metrics.accuracy_score(y_test,y_hat))","64f0c4ad":"ks = 10\nmean_acc = np.zeros((ks - 1))\nstd_acc = np.zeros((ks - 1))\nconfusion_matrix = []\n\nfor n in range(1,ks):\n    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat = neigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test,yhat)\n    std_acc[n-1] = np.std(yhat == y_test)\/np.sqrt(yhat.shape[0])\n    \nmean_acc","31089d82":"plt.plot(range(1,ks),mean_acc,'g')\nplt.fill_between(range(1,ks),mean_acc - 1*std_acc,mean_acc + 1*std_acc,alpha = 1)\nplt.legend([\"Accuracy\", \"+\/- 3xstd\"])\nplt.tight_layout()\nprint(\"The best accuracy for the model is :\",mean_acc.max(),\"With k=\",mean_acc.argmax()+1)\nplt.show()","19935b9d":"PetTree = DecisionTreeClassifier(criterion =\"entropy\", max_depth = 5)\nPetTree.fit(X_train,y_train)","efdb1644":"yhat1 = PetTree.predict(X_test) \nprint(yhat1[0:5])\nprint(y_test[0:5])","7d8ab41e":"print(\"Accuracy Score is : \", metrics.accuracy_score(y_test,yhat1))","5d2f1eda":"Xtest = test_data[['length(m)', 'height(cm)', 'X1', 'X2']]\nXtest.head()","f79c7437":"pred = PetTree.predict(Xtest)","6d7fba10":"data_feature_names = ['length(m)', 'height(cm)', 'X1', 'X2']","3e4a4935":"# Visualize data\ndot_data = tree.export_graphviz(PetTree,\n                                feature_names=data_feature_names,\n                                out_file=None,\n                                filled=True,\n                                rounded=True)\ngraph = pydotplus.graph_from_dot_data(dot_data)\n\ncolors = ('turquoise', 'orange')\nedges = collections.defaultdict(list)\n\nfor edge in graph.get_edge_list():\n    edges[edge.get_source()].append(int(edge.get_destination()))\n\nfor edge in edges:\n    edges[edge].sort()    \n    for i in range(2):\n        dest = graph.get_node(str(edges[edge][i]))[0]\n        dest.set_fillcolor(colors[i])\nfilename = \"tree.png\"\ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(100,200))\nplt.imshow(img,interpolation = 'nearest')\nplt.show()","0cca4fb6":"output = pd.DataFrame({'PetId': test_data.pet_id, 'Pet Category': pred})\noutput.to_csv('Output.csv', index=False)\nprint(\"Your submission was successfully saved!\")","113af9c9":"# Reading Data for Model and Prediction","3e426ccb":"# <u><center>Adopt a Pet<\/center><\/u>\n<center>Authored by: Pratham Tripathi<\/center>\n\n## <u>Aim:<\/u> \nTo predict which pet is accurate when prerequisites are given (like length,Breadth etc). \n\n## <u>Approach:<\/u>\nThe Approach was Supervised Machine Learning Classification model. Here, we used some basic algorithms like K- Nearest Neighbors, Decision Tree Classifier.\n### 1. <u>K-Nearest-Neighbors (KNN) :<\/u> \nThe k-nearest neighbors algorithm (k-NN) is a non-parametric method proposed by Thomas Cover used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n- In kNN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n- In kNN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\n### 2. <u>Decision Tree :<\/u> \nA decision tree is a flowchart-like structure in which each internal node represents a \u201ctest\u201d on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).","b59a5c42":"# Cleaning The Data","22453cb3":"# Pre-Processing Data","4553e1d8":"# K-nearest Neighbor Algorithm","f3e8c4b3":"# Final Output in a csv file","60b340a0":"# Feature and Target Sets","94f4eb46":"# Prediction and Evaluation of KNN Model","be70c3ee":"# Prediction of test.csv\n\n<p>Since <u>Decision Tree<\/u> has a better Accuracy here than the KNN model, we are going to choose it as our main model for prediction.<\/p>","7b393837":"# Prediction and Evaluation of Decision Tree","34056779":"# Decision Tree ","74fdda2e":"# Visualization of The Decision Tree","8b346eab":"# Main Prediction using Decision Tree","0b7f6101":"# Importing Required Libraries","916a6db9":"# Spliting Data for Testing and Training"}}