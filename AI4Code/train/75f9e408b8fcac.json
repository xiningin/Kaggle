{"cell_type":{"f0d43dc9":"code","f6fb587f":"code","2eff279f":"code","65b39ade":"code","fc8862fb":"code","599a0dcf":"code","82ecd752":"code","9d034ea3":"code","ceff590b":"code","975b94ee":"code","93bcea8e":"markdown","a0f1f49d":"markdown","35a75faa":"markdown","d28da589":"markdown","fb89e76c":"markdown","d02dea3c":"markdown","87aeca5b":"markdown"},"source":{"f0d43dc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f6fb587f":"import sys, os\nimport numpy as np\nimport matplotlib.pyplot as plt","2eff279f":"train_df  = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\") \ntest_df = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\") ","65b39ade":"class TwoLayerNet:\n\n    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n        # weight initialize\n        self.params = {}\n        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n        self.params['b2'] = np.zeros(output_size)\n\n    def predict(self, x):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n    \n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        return y\n        \n    # x:input data, t:teaching data\n    def loss(self, x, t):\n        y = self.predict(x)\n        \n        return cross_entropy_error(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        t = np.argmax(t, axis=1)\n        \n        accuracy = np.sum(y == t) \/ float(x.shape[0])\n        return accuracy\n        \n    # x:input data, t:teaching data\n    def numerical_gradient(self, x, t):\n        loss_W = lambda W: self.loss(x, t)\n        \n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        \n        return grads\n        \n    def gradient(self, x, t):\n        W1, W2 = self.params['W1'], self.params['W2']\n        b1, b2 = self.params['b1'], self.params['b2']\n        grads = {}\n        \n        batch_num = x.shape[0]\n        \n        # forward\n        a1 = np.dot(x, W1) + b1\n        z1 = sigmoid(a1)\n        a2 = np.dot(z1, W2) + b2\n        y = softmax(a2)\n        \n        # backward\n        dy = (y - t) \/ batch_num\n        grads['W2'] = np.dot(z1.T, dy)\n        grads['b2'] = np.sum(dy, axis=0)\n        \n        dz1 = np.dot(dy, W2.T)\n        da1 = sigmoid_grad(a1) * dz1\n        grads['W1'] = np.dot(x.T, da1)\n        grads['b1'] = np.sum(da1, axis=0)\n\n        return grads","fc8862fb":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))    \n\n\ndef sigmoid_grad(x):\n    return (1.0 - sigmoid(x)) * sigmoid(x)\n    \n\ndef relu(x):\n    return np.maximum(0, x)\n\n\ndef relu_grad(x):\n    grad = np.zeros_like(x)\n    grad[x>=0] = 1\n    return grad\n    \n\ndef softmax(x):\n    x = x - np.max(x, axis=-1, keepdims=True)   # \u30aa\u30fc\u30d0\u30fc\u30d5\u30ed\u30fc\u5bfe\u7b56\n    return np.exp(x) \/ np.sum(np.exp(x), axis=-1, keepdims=True)\n\n\ndef sum_squared_error(y, t):\n    return 0.5 * np.sum((y-t)**2)\n\n\ndef cross_entropy_error(y, t):\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n        \n    # When teaching data is one-hot-vecotr, change label to index.\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n             \n    batch_size = y.shape[0]\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size\n\n\ndef softmax_loss(X, t):\n    y = softmax(X)\n    return cross_entropy_error(y, t)","599a0dcf":"def numerical_gradient(f, x):\n    h = 1e-4 # 0.0001\n    grad = np.zeros_like(x)\n    \n    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n    while not it.finished:\n        idx = it.multi_index\n        tmp_val = x[idx]\n        x[idx] = tmp_val + h\n        fxh1 = f(x) # f(x+h)\n        \n        x[idx] = tmp_val - h \n        fxh2 = f(x) # f(x-h)\n        grad[idx] = (fxh1 - fxh2) \/ (2*h)\n        \n        x[idx] = tmp_val\n        it.iternext()   \n        \n    return grad","82ecd752":"class SGD:\n\n    \"\"\"\u78ba\u7387\u7684\u52fe\u914d\u964d\u4e0b\u6cd5\uff08Stochastic Gradient Descent\uff09\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        \n    def update(self, params, grads):\n        for key in params.keys():\n            params[key] -= self.lr * grads[key] \n\n\nclass Momentum:\n\n    \"\"\"Momentum SGD\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():                                \n                self.v[key] = np.zeros_like(val)\n                \n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n            params[key] += self.v[key]\n\n\nclass Nesterov:\n\n    \"\"\"Nesterov's Accelerated Gradient (http:\/\/arxiv.org\/abs\/1212.0901)\"\"\"\n\n    def __init__(self, lr=0.01, momentum=0.9):\n        self.lr = lr\n        self.momentum = momentum\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.v[key] *= self.momentum\n            self.v[key] -= self.lr * grads[key]\n            params[key] += self.momentum * self.momentum * self.v[key]\n            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n\n\nclass AdaGrad:\n\n    \"\"\"AdaGrad\"\"\"\n\n    def __init__(self, lr=0.01):\n        self.lr = lr\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] += grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] \/ (np.sqrt(self.h[key]) + 1e-7)\n\n\nclass RMSprop:\n\n    \"\"\"RMSprop\"\"\"\n\n    def __init__(self, lr=0.01, decay_rate = 0.99):\n        self.lr = lr\n        self.decay_rate = decay_rate\n        self.h = None\n        \n    def update(self, params, grads):\n        if self.h is None:\n            self.h = {}\n            for key, val in params.items():\n                self.h[key] = np.zeros_like(val)\n            \n        for key in params.keys():\n            self.h[key] *= self.decay_rate\n            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n            params[key] -= self.lr * grads[key] \/ (np.sqrt(self.h[key]) + 1e-7)\n\n\nclass Adam:\n\n    \"\"\"Adam (http:\/\/arxiv.org\/abs\/1412.6980v8)\"\"\"\n\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.iter = 0\n        self.m = None\n        self.v = None\n        \n    def update(self, params, grads):\n        if self.m is None:\n            self.m, self.v = {}, {}\n            for key, val in params.items():\n                self.m[key] = np.zeros_like(val)\n                self.v[key] = np.zeros_like(val)\n        \n        self.iter += 1\n        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) \/ (1.0 - self.beta1**self.iter)         \n        \n        for key in params.keys():\n            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n            \n            params[key] -= lr_t * self.m[key] \/ (np.sqrt(self.v[key]) + 1e-7)\n            \n            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n            #params[key] += self.lr * unbias_m \/ (np.sqrt(unbisa_b) + 1e-7)","9d034ea3":"def split_dataframe(dataframe=None, fraction=0.9, rand_seed=1):\n    df_1 = dataframe.sample(frac=fraction, random_state=rand_seed)\n    df_2 = dataframe.drop(df_1.index)\n    return df_1, df_2","ceff590b":"def _change_one_hot_label(X):\n    T = np.zeros((X.size, 10))\n    for idx, row in enumerate(T):\n        row[X[idx]] = 1\n\n    return T","975b94ee":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\n#(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\ntrain_df_new, val_df = split_dataframe(dataframe=train_df, fraction=0.9, rand_seed=1)\nx_train = train_df_new.iloc[:,1:].values.astype(np.uint8)\nt_train = _change_one_hot_label(train_df_new.iloc[:,0].values)\nx_test  = val_df.iloc[:,1:].values.astype(np.uint8)\nt_test  = _change_one_hot_label(val_df.iloc[:,0].values)\n\n\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n\niters_num = 10000  # \u7e70\u308a\u8fd4\u3057\u306e\u56de\u6570\u3092\u9069\u5b9c\u8a2d\u5b9a\u3059\u308b\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niter_per_epoch = max(train_size \/ batch_size, 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = x_train[batch_mask]\n    t_batch = t_train[batch_mask]\n    \n    # \u52fe\u914d\u306e\u8a08\u7b97\n    #grad = network.numerical_gradient(x_batch, t_batch)\n    grad = network.gradient(x_batch, t_batch)\n    \n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u66f4\u65b0\n    for key in ('W1', 'b1', 'W2', 'b2'):\n        network.params[key] -= learning_rate * grad[key]\n    \n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(x_train, t_train)\n        test_acc = network.accuracy(x_test, t_test)\n        train_acc_list.append(train_acc)\n        test_acc_list.append(test_acc)\n        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n\n# \u30b0\u30e9\u30d5\u306e\u63cf\u753b\nmarkers = {'train': 'o', 'test': 's'}\nx = np.arange(len(train_acc_list))\nplt.plot(x, train_acc_list, label='train acc')\nplt.plot(x, test_acc_list, label='test acc', linestyle='--')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.ylim(0, 1.0)\nplt.legend(loc='lower right')\nplt.show()","93bcea8e":"## \n\n\u53c2\u7167\u5143\n\u30bc\u30ed\u304b\u3089\u4f5c\u308bDeepLearning\n\nhttps:\/\/github.com\/oreilly-japan\/deep-learning-from-scratch","a0f1f49d":"## Neural Net Layers","35a75faa":"## Optimizer","d28da589":"## Read Data","fb89e76c":"## Specify Neural Network","d02dea3c":"## Ditail of back propagation\n\n\u8a08\u7b97\u30b0\u30e9\u30d5\u3092\u4f7f\u3063\u3066\u8aac\u660e\n\n<img src=\"https:\/\/cdn-ak.f.st-hatena.com\/images\/fotolife\/t\/taxa_program\/20180606\/20180606214834.png\" width=\"600\">\n\nback propagation\u3067\u306f\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5206\u5c90\u3067\u5fae\u5206\u3092\u623b\u3057\u3066\u3044\u3063\u3066\u3044\u308b\u3002<br>\n\u3053\u306e\u3053\u3068\u306b\u3088\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u624b\u524d\u5074\u306e\u5909\u5316\u304c\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u53e3\u306b\u3069\u308c\u3050\u3089\u3044\u306e\u5f71\u97ff\u304c\u3042\u308b\u304b\u304c\u308f\u304b\u308b\u3002<br>\n\u3053\u308c\u306b\u3088\u308a\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u51fa\u53e3\u3067\u8a08\u7b97\u3057\u305floss\u3092\u3001\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u624b\u524d\u5074\u306eWeight\u3092\u3069\u308c\u3060\u3051\u5909\u5316\u3055\u305b\u308c\u3070\u3001\u51fa\u53e3\u306eloss\u3092\u6e1b\u3089\u305b\u308b\u304b\u304c\u89e3\u308b\u3002<br>","87aeca5b":"## Training"}}