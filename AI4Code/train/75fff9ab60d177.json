{"cell_type":{"0efe3f7f":"code","2a44a40b":"code","e4c2a72a":"code","3fb9469f":"code","cfdba9b7":"code","a95f9fdc":"code","e3caad0f":"code","6df123df":"code","eac8b025":"code","9204263d":"code","a6770192":"code","dd69ea80":"markdown","82500639":"markdown","2cebc573":"markdown","6a53ddb6":"markdown","75fdd2a9":"markdown","c5cfc9f5":"markdown","9f9721a0":"markdown","bc7e4254":"markdown","893d7e2d":"markdown"},"source":{"0efe3f7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\ndata = pd.read_csv('..\/input\/heart.csv')\ndata.head()","2a44a40b":"import seaborn as sns\nimport pandas as pd\n\ndims = (11, 5)\nfig, ax = plt.subplots(1,2, figsize=dims)\n\ng = sns.kdeplot(data[\"age\"][(data[\"target\"] == 0) & (data[\"age\"].notnull())], ax =ax[0], color=\"Red\", shade = True)\ng = sns.kdeplot(data[\"age\"][(data[\"target\"] == 1) & (data[\"age\"].notnull())], ax =ax[0], color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Sick\",\"None\"])\n\n\naverage = data[\"age\"].mean()\nstd = data[\"age\"].std()\nage_normalized = (data[\"age\"] - average) \/ std\n\nh = sns.kdeplot(age_normalized[(data[\"target\"] == 0) & (age_normalized.notnull())], ax =ax[1], color=\"Red\", shade = True)\nh = sns.kdeplot(age_normalized[(data[\"target\"] == 1) & (age_normalized.notnull())], ax =ax[1], color=\"Blue\", shade= True)\nh.set_xlabel(\"Age\")\nh.set_ylabel(\"Frequency\")\nh = h.legend([\"Sick\",\"None\"])\n\nfig.show()","e4c2a72a":"from sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import FeatureUnion\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n\n# Use One-Hot Encoding\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"sex\", \"fbs\", \"exang\", \"cp\", \"restecg\", \"slope\", \"ca\", \"thal\"])),\n        (\"cat_encoder\", OneHotEncoder(sparse=False, categories='auto')),\n    ])\n\n# Use Z-score normalization\nstandard_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"age\", \"trestbps\", \"chol\", \"thalach\", \"oldpeak\"])),\n        ('scale', StandardScaler()),\n    ])\n\n# Merge pipelines\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"standard_pipeline\", standard_pipeline),\n    ])","3fb9469f":"# Get the target value\ntarget = np.array(data[\"target\"].values)\ndata.drop('target', axis=1, inplace=True)\n\n# Use our pipeline on our data\ndata = preprocess_pipeline.fit_transform(data)\n\n# Split our data in train and test samples\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.20, random_state=42)","cfdba9b7":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\n","a95f9fdc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\nforest_clf = RandomForestClassifier(n_estimators=100, max_depth=5)\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=10)\nforest_scores.mean()","e3caad0f":"forest_clf.fit(X_train, y_train)\ny_pred = forest_clf.predict(X_test)","6df123df":"from sklearn.metrics import confusion_matrix\n\nclass_names = np.array([\"Healthy\", \"Sick\"])\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\nplt.show()\n\nfrom sklearn.metrics import precision_score\nprint (\"Accuracy\")\nprint(precision_score(y_test, y_pred, average='macro'))\n\nfrom sklearn.metrics import recall_score\nprint (\"Recall\")\nprint(recall_score(y_test, y_pred, average='macro'))","eac8b025":"from sklearn.svm import SVC\nsvm_clf = SVC(gamma=\"auto\")\nsvm_clf.fit(X_train, y_train)\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\nprint(svm_scores.mean())\n\nsvm_clf.fit(X_train, y_train)\ny_pred = svm_clf.predict(X_test)\n\nprint (\"Accuracy\")\nprint(precision_score(y_test, y_pred, average='macro'))\n\nprint (\"Recall\")\nprint(recall_score(y_test, y_pred, average='macro'))\n\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\nplt.show()","9204263d":"from sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier(n_neighbors=50)\nneigh.fit(X_train, y_train)\nneigh_scores = cross_val_score(neigh, X_train, y_train, cv=10)\nprint(neigh_scores.mean())\n\nneigh.fit(X_train, y_train)\ny_pred = neigh.predict(X_test)\n\nprint (\"Accuracy\")\nprint(precision_score(y_test, y_pred, average='macro'))\n\nprint (\"Recall\")\nprint(recall_score(y_test, y_pred, average='macro'))\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\nplt.show()","a6770192":"prm = 50\n\nforest_scores = cross_val_score(forest_clf, X_train, y_train, cv=prm, scoring='recall')\nsvm_scores = cross_val_score(svm_clf, X_train, y_train, cv=prm, scoring='recall')\nneigh_scores = cross_val_score(neigh, X_train, y_train, cv=prm, scoring='recall')\n\n%matplotlib inline\n\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\nplt.figure(figsize=(8, 4))\nplt.plot([1]*prm, svm_scores, \".\")\nplt.plot([2]*prm, forest_scores, \".\")\nplt.plot([3]*prm, neigh_scores, \".\")\nplt.boxplot([svm_scores, forest_scores, neigh_scores], labels=(\"SVM\",\"Random Forest\", \"K Neighbors\"))\nplt.ylabel(\"Recall\", fontsize=14)\nplt.show()","dd69ea80":"# Heart Disease UCI - Classifications","82500639":"As you can see normalization with Z-score change the value of our variable but keep the distribution of our data.","2cebc573":"## K-nearest neighbors","6a53ddb6":"## Classification systems","75fdd2a9":"## Support-vector machine","c5cfc9f5":"## Compare the different systems based on the recall","9f9721a0":"## Normalization effect\nWe are going to see the normalization effect on the age variable.","bc7e4254":"### Random Forest","893d7e2d":"## Transformation of our data and creation of our pipeline\n\nNow, we are going to create our pipeline which will take our data and apply transformations on it. We are going to use One-Hot encoding and Z-score normalization on our data."}}