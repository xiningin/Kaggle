{"cell_type":{"eac92004":"code","9eb72d31":"code","d2bf758d":"code","98ccd625":"code","aff7dbb3":"code","1e9323f1":"code","a4b880a7":"code","58224f10":"code","a0d5d545":"code","42c5fe6c":"code","f8689440":"code","30b39cbe":"code","d2fc8ef2":"code","9aa75084":"code","94ec8b84":"code","0e18e063":"code","40a4d3a8":"code","30759aaa":"code","9145aa05":"code","f7ccb194":"code","a5f06697":"code","8225b0ee":"code","919aae44":"code","75b1c814":"code","84667828":"code","ae7a1759":"code","07d8e5c7":"code","108aa7b4":"code","59adb83f":"code","8e4326f7":"code","57949473":"code","dae41405":"code","59947b59":"code","d3cc5e55":"code","4d45e949":"code","341c1892":"code","68f7e227":"code","7f0cbd0d":"code","220cee92":"code","847011df":"code","1b089998":"code","bd29bddf":"code","bff22582":"code","9964ec3c":"code","2c4f4382":"code","8d33ed7b":"code","742bd934":"code","606b7d6d":"code","8c0c21ef":"code","32ae5c2e":"code","957c5ed2":"code","d117a245":"code","026028b3":"code","fb4d09aa":"code","54a5477c":"code","bb870e99":"code","e1ec765f":"code","ca4683a9":"markdown","8cd9dc3b":"markdown","7089567b":"markdown","b4f385c7":"markdown","de23d408":"markdown","8d9a4a12":"markdown","983fd708":"markdown","04c0d295":"markdown","d0de92da":"markdown","921fe989":"markdown","7e388dc0":"markdown","4b1d59fc":"markdown","ee781214":"markdown","2693995b":"markdown","3e185b41":"markdown","696d9aac":"markdown","90363ca7":"markdown","79d09a59":"markdown","fa923749":"markdown","1c1f43ad":"markdown","accd006b":"markdown","eb165b1e":"markdown","4181a5c2":"markdown","c61145ee":"markdown","ee768d22":"markdown","dbd56556":"markdown","f4b04ac4":"markdown","3ebc37ed":"markdown","1e6727da":"markdown","d3daddbd":"markdown","f26a53d8":"markdown","37dd8f24":"markdown"},"source":{"eac92004":"#importing all usefull lib\n\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold, cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression, RidgeCV, Lasso, ElasticNetCV, BayesianRidge, LassoLarsIC\nfrom sklearn.metrics import mean_squared_error, make_scorer \nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nimport math\nfrom sklearn.preprocessing import StandardScaler\nimport warnings as wr\nwr.filterwarnings(\"ignore\")","9eb72d31":"#uploading training and test data\n\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","d2bf758d":"train_data.head()","98ccd625":"#saving outcome in Sale_Price\n\nSale_Price=train_data.iloc[:,80]\nSale_Price.shape","aff7dbb3":"train_data.shape","1e9323f1":"#droping SalePrice column\ntrain=train_data.drop([\"SalePrice\"],axis=1)\ntrain.head()","a4b880a7":"\ntest.head()","58224f10":"test.shape","a0d5d545":"data= pd.concat([train,test], keys=['x', 'y'])#here X is training data and Y testing data\ndata=data.drop([\"Id\"],axis=1)","42c5fe6c":"data.shape","f8689440":"plt.figure(figsize=(20,6))\nsns.heatmap(data.isnull(),yticklabels=False,cbar=True,cmap='mako')","30b39cbe":"total_null = data.isnull().sum().sort_values(ascending=False) #First sum and order all null values for each variable\npercentage = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False) #Get the percentage\nmissing_data = pd.concat([total_null, percentage], axis=1, keys=['Total', 'Percentage'])\nmissing_data.head(20)","d2fc8ef2":"\ndata = data.drop((missing_data[missing_data[\"Percentage\"] > 0.05]).index,1) #Drop All Var. with null values > 1\n\ndata.isnull().sum()","9aa75084":"num_col=data._get_numeric_data().columns.tolist()\nnum_col","94ec8b84":"\ncat_col=set(data.columns)-set(num_col)\ncat_col","0e18e063":"\nfor col in num_col:\n    data[col].fillna(data[col].mean(),inplace=True)","40a4d3a8":"for col in cat_col:\n\n    data[col].fillna(data[col].mode()[0],inplace=True)","30759aaa":"#count total value in every catgorical feature\nfor i in cat_col:\n    print(data[i].value_counts())","9145aa05":"#droping some unnecessary cat_features bcoz they have 80% + same value and 20% - defertnt values so they can't effect score\ndf=data.drop([\"RoofMatl\",\"Heating\",\"Condition2\",\"BsmtCond\",\"CentralAir\",\"Functional\",\"Electrical\",\n              \"LandSlope\",\"ExterCond\",\"Condition1\",\"GarageArea\",\"BsmtUnfSF\",\"3SsnPorch\",\"MiscVal\",\n              \"BsmtFinType2\",\"Utilities\",\"Street\",\"Exterior2nd\",\"Neighborhood\"],axis=1) ","f7ccb194":"corrmat = train_data.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(9,9))\ng = sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","a5f06697":"var = train_data[train_data.columns[1:]].corr()['SalePrice'][:]\nvar.sort_values(ascending=False)","8225b0ee":"#droping low version feature\ndf=df.drop([\"MoSold\",\"BsmtFinSF2\",\"BsmtHalfBath\",\"OverallCond\",\"YrSold\",\n            \"MSSubClass\",\"EnclosedPorch\",\"KitchenAbvGr\",\"ScreenPorch\",\"2ndFlrSF\",\"OverallQual\",\"GrLivArea\"],axis=1)","919aae44":"df.shape\n","75b1c814":"#here we checking data summury\ndf.describe()","84667828":"#sale price analysis\n\nsns.distplot(train_data['SalePrice']);\nprint(\"Skewness coeff. is: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis coeff. is: %f\" % train_data['SalePrice'].kurt())","ae7a1759":"sns.kdeplot(data=train_data,x='SalePrice',hue=\"MoSold\",fill=True,common_norm=False,palette=\"husl\")","07d8e5c7":"data_year_trend = pd.concat([train_data['SalePrice'], train_data['YearBuilt']], axis=1)\ndata_year_trend.plot.scatter(x='YearBuilt', y='SalePrice', ylim=(0,800000));","108aa7b4":"data_bsmt_trend = pd.concat([train_data['SalePrice'], train_data['TotalBsmtSF']], axis=1)\ndata_bsmt_trend.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0,800000));","59adb83f":"data_PoolArea_trend = pd.concat([train_data['SalePrice'], train_data['PoolArea']], axis=1)\ndata_PoolArea_trend.plot.scatter(x='PoolArea', y='SalePrice', ylim=(0,800000));","8e4326f7":"data = pd.concat([train_data['SalePrice'], train_data['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","57949473":"corr_matrix = df.corr()\nf, ax1 = plt.subplots(figsize=(12,9)) \nax1=sns.heatmap(corr_matrix,vmax = 0.9); ","dae41405":"df.shape","59947b59":"#Here we extract the numerical variables, this will come in handy later on\n\nn_features = df.select_dtypes(exclude = [\"object\"]).columns","d3cc5e55":"#for i in df[n_features]:\n    #sns.boxplot(x=df[i])\n    #plt.show()","4d45e949":"\nX=pd.get_dummies(df)\nX.shape","341c1892":"\n#scalerX = MinMaxScaler(feature_range=(0, 1))\n#X[X.columns] = scalerX.fit_transform(X[X.columns])\nscaler=StandardScaler()\nX[X.columns] = scaler.fit_transform(X[X.columns])\n\n\n\n","68f7e227":"#Training data after preproscing\n\nTrain_data=X.loc[\"x\"]\nTrain_data.shape","7f0cbd0d":"#Testing data after preproscing\nTest_data=X.loc[\"y\"]\nTest_data.shape","220cee92":"#here we add salePrice column in traning data\n\nTrain_data.insert(2,column=\"SalePrice\",value=Sale_Price)\nTrain_data.head()","847011df":"x=Train_data.drop([\"SalePrice\"],axis=True)\ny=Train_data[\"SalePrice\"]","1b089998":" \nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.25,random_state=40)","bd29bddf":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators = 50,random_state=40,\n                          min_impurity_decrease=0.002,min_weight_fraction_leaf=0.001,min_samples_split=5)\nrfr.fit(x_train,y_train)\ny_predictrfr = rfr.predict(x_test)\n\n#here we can check our model score\nprint(rfr.score(x_test,y_test))","bff22582":"\nrmse = math.sqrt(mean_squared_error(y_test, rfr.predict(x_test)))\n\nprint(\"mear squares error :\",rmse)","9964ec3c":"\nfrom sklearn.tree import DecisionTreeRegressor\ndtr=DecisionTreeRegressor(random_state=140,min_samples_split=5,min_impurity_decrease=0.002,min_weight_fraction_leaf=0.001)\ndtr.fit(x_train,y_train)\ny_predictdtr = dtr.predict(x_test)\n#u can also use GridSearchCV \/ random Searchcv for hyperperameter tuning\nprint(dtr.score(x_test,y_test))","2c4f4382":"rmse = math.sqrt(mean_squared_error(y_test, dtr.predict(x_test)))\nprint(\"RMSE:\",rmse)","8d33ed7b":"GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.04,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n#RMSE estimated through the partition of the train set\nGBoost.fit(x_train, y_train)\ny_predictGB = GBoost.predict(x_test)\nprint(\"RMSE: %.4f\" % rmse)","742bd934":"print(GBoost.score(x_test,y_test))","606b7d6d":"import numpy as np\nred = plt.scatter(np.arange(0,80,5),y_predictGB[0:80:5],color = \"red\")\ngreen = plt.scatter(np.arange(0,80,5),y_predictrfr[0:80:5],color = \"green\")\nblue = plt.scatter(np.arange(0,80,5),y_predictdtr[0:80:5],color = \"blue\")\nblack = plt.scatter(np.arange(0,80,5),y_test[0:80:5],color = \"black\")\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Home Price\")\nplt.legend((red,green,blue,black),('GBoost', 'RFR', 'DTR', 'REAL'))\nplt.show()","8c0c21ef":"#here we see test data here one column is missing that is Saleprice bcoz that is need to predict\nTest_data.head()","32ae5c2e":"Test_data.shape","957c5ed2":"#here we predict SalePrice using RFR model\ny_model_prerfc = GBoost.predict(Test_data)","d117a245":"#Here we can See predict Sale Price\ny_model_prerfc=np.around(y_model_prerfc,2)\ny_model_prerfc\n","026028b3":"prediction=np.array(y_model_prerfc).tolist()\ntest.head()","fb4d09aa":"test.insert(1,column=\"SalePrice\",value=prediction)\ntest.head()","54a5477c":"predict_sub=test.drop(test.iloc[:,2:],axis=1)\npredict_sub.head()\n","bb870e99":"predict_sub.shape","e1ec765f":"predict_sub.to_csv('Home_predictionsGB.csv',index=False)\n\n","ca4683a9":"**By these analysis** \n\nwe discovered that our previsions were quite correct.\n\n__Year Built__ seems to have a slight relation with our main variable, and people, as we thought, tend to buy newer houses. \n\nInstead, for __TotalBsmtSF__ and __GrLivArea__ there seems be a stronger relation with __SalePrice__. ","8cd9dc3b":"# *Welcome Kagglers*","7089567b":"# *EDA  , Feature Engineering and Prediction*","b4f385c7":"# *filling numrical missing value using fillna*","de23d408":"Sale Price analysis on PoolArea","8d9a4a12":"These measures of symmetry are useful in order to understand the symmetry of the distribution of our main variable.\nOur distribution is highly skewed and present a longer tail on the right. \nThe high value of kurtosis can determine an higher probability of outliers values.","983fd708":"**Sale Price Analysis on YearBuilt**","04c0d295":"**spliting Training data for traning model and cheak score**","d0de92da":"# here we use Random Forest Regressor for model building","921fe989":"We have to do some considerations. \nLet's divide our null values into 2 groups:\n - __PoolQC__, __MiscFeature__, __Alley__, __Fence__, __FireplaceQu__ and __LotFrontage__.\nThese are all variables which presents many null values. In general, by common opinion, we can discourage variables which have more than 15% of missing values. \nThese are not vital information for someone who wants to buy an house, such as __FireplaceQu__ or, for example, many houses doesn't have an __Alley__ access. We can drop them.\n\nThe second group:\n - __GarageX__ properties\nIf we look carefully, all of these variables have the same number of null values! Maybe this can be a strange coincidence, or just that they all refer to the same variable Garage, in which \"Na\" means \"There is no Garage\". The same occurs for __BsmtX__ and MasVnr__, which means that we will have to deal with them afterwards.","7e388dc0":"by the above chart be can essly find out the outliers","4b1d59fc":"# here we split data in input(x) and output(y)","ee781214":"**It will helpfull for beginners & Intermediate**\n**By this notebook you will get a idia how things work**","2693995b":"# Please do an up vote if you find useful\n### Thank you","3e185b41":"**finding numeric column from data**","696d9aac":"# Data Preposesing \n\n**combining training & testing data for preposesing after that we do not write same code for test**\n","90363ca7":"# Outliers","79d09a59":"# Our initial considerations \nLooking forward to our columns, we found some variables which can have an high correlation with our main variable SalePrice:\n- __Year Built__\n- __TotalBsmtSF__\n- __GrLivArea__\n- __PoolArea__\n\nThese are variables related to the conditions of the building, its age and some \"extra luxury\" features such as __PoolArea__. \nIn principle they are all characteristics which can rise the price of an abitation. \nAnother theory we suggested was to consider mainly the \"inner\" part of the house, such as __KitchenQual__ or __CentralAir__, but these could be too general features which mainly all the houses can have.\n\nNow, with these prior hypotesis, let's dive into the \"__SalePrice__\" analysis.","fa923749":"**finding catogorical features**","1c1f43ad":"# *filling catgorical missing value*","accd006b":"# Model Building using train Data","eb165b1e":"The aim of this project is to build a Machine Learning model in order to predict the appropriate price of a house given a set of features. We decided to divide our analysis into 5 parts:\n\n\n   * First look at the problem and general understanding of the variables;\n   * Study the main variable (\"SalePrice\");\n   * Study how the main variable is related to the other feature;\n   * Data Preprocessing: make some cleaning on our training data set in order to better visualize and estimate;\n   * Build a model in order to predict SalePrice\n   * Explorty data Analysis\n","4181a5c2":"**Sale price Analysis on TotalBsmtSf**","c61145ee":"saleprice analysis on QverallQual","ee768d22":"# Heatmap Correlation Matrix","dbd56556":"# G Boosting","f4b04ac4":"**here we use one hot encoading to encoad cat_features**","3ebc37ed":"**here we use minmax scaler for scaling numeric fields**","1e6727da":"# here we use Decision Tree algo","d3daddbd":"# Prediction On Testing Data","f26a53d8":"# Dealing with null values\n\n**Now our goal is to deal with null values and try to understand for each one what can we do: maybe we can replace them or maybe we can just skip them.**","37dd8f24":"# EDA"}}