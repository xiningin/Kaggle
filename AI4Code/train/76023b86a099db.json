{"cell_type":{"cae5520b":"code","a8ee86ea":"code","82c700ab":"code","c1b488b6":"code","9c01dc16":"code","e68b1b5b":"code","0db1d1f0":"code","e77c19b0":"code","04faea45":"code","d9387978":"code","ad7a5e45":"code","39526d82":"markdown","f60442c4":"markdown","3b27b1c4":"markdown"},"source":{"cae5520b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8ee86ea":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical","82c700ab":"train = pd.read_csv(filepath_or_buffer=\"\/kaggle\/input\/titanic\/train.csv\", index_col=\"PassengerId\")\ntest = pd.read_csv(filepath_or_buffer=\"\/kaggle\/input\/titanic\/test.csv\", index_col=\"PassengerId\")\n\ntrain.tail()","c1b488b6":"def clean(df):\n    # Drop unwanted variables [\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"]\n    df = df.drop(columns=[\"Name\", \"Cabin\", \"Ticket\"])\n\n    # Categorical into one hot: \"Sex\", \"Embarked\" then drop original columns\n    x = pd.get_dummies(df[\"Sex\"])\n    df = df.join(x)\n    df = df.drop(columns=[\"Sex\"])\n    y = pd.get_dummies(df[\"Embarked\"])\n    df = df.join(y)\n    df = df.drop(columns=[\"Embarked\"])\n    \n    # NaN values\n    #df[\"Age\"] = df[\"Age\"].fillna(30)\n    df = df.dropna()\n    df[\"Age\"]=(df[\"Age\"]-df[\"Age\"].mean())\/df[\"Age\"].std()\n    df[\"Fare\"]=(df[\"Fare\"]-df[\"Fare\"].mean())\/df[\"Fare\"].std()\n    return df\n    #print(df)\n    \n    \ndef clean2(df):\n    # Drop unwanted variables [\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"]\n    df = df.drop(columns=[\"Name\", \"Cabin\", \"Ticket\"])\n\n    # Categorical into one hot: \"Sex\", \"Embarked\" then drop original columns\n    x = pd.get_dummies(df[\"Sex\"])\n    df = df.join(x)\n    df = df.drop(columns=[\"Sex\"])\n    y = pd.get_dummies(df[\"Embarked\"])\n    df = df.join(y)\n    df = df.drop(columns=[\"Embarked\"])\n    \n    # NaN values\n    #df[\"Age\"] = df[\"Age\"].fillna(30)\n    df = df.fillna(0)\n    df[\"Age\"]=(df[\"Age\"]-df[\"Age\"].mean())\/df[\"Age\"].std()\n    df[\"Fare\"]=(df[\"Fare\"]-df[\"Fare\"].mean())\/df[\"Fare\"].std()\n    return df\n    #print(df)\n    \n    \ntrain = clean(train)\ntest = clean2(test)","9c01dc16":"train.head()","e68b1b5b":"class TitanicData(Dataset):\n    def __init__(self):\n        y = torch.tensor(train[\"Survived\"].values.astype(np.float32))\n        self.y = torch.unsqueeze(y, 1)\n        self.x = torch.tensor(train.drop(columns=[\"Survived\"]).values.astype(np.float32))\n\n    def __getitem__(self, item):\n        return self.x[item], self.y[item]\n\n    def __len__(self):\n        return len(train)\n    \n        \ntrain_loader = DataLoader(TitanicData(), batch_size=10, shuffle=False)","0db1d1f0":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.linearReLU = nn.Sequential(\n        nn.Linear(10, 128),\n        nn.ReLU(),\n        nn.Linear(128, 128),\n        nn.ReLU(),\n        nn.Linear(128, 128),\n        nn.ReLU(),\n        nn.Linear(128, 1),\n        nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        logits = self.linearReLU(x)\n        return logits\n\n# class Net(nn.Module):\n#     def __init__(self):\n#         super(Net, self).__init__()\n#         self.fc1 = nn.Linear(10,64)\n#         self.fc2 = nn.Linear(64,64)\n#         self.fc3 = nn.Linear(64,2)\n                \n#     def forward(self, x):\n#         z = nn.ReLU(self.fc1(x))\n#         z = nn.ReLU(self.fc2(x))\n#         z = self.fc3(x)\n#         return z\n    \nmodel = Net()\nprint(model)    \n\n\ndevice = \"cpu\"\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)","e77c19b0":"batch_size = 20\nn_epochs = 100\nloss_plot = []\n\ndef train_model(n_epochs):\n    for epoch in range(n_epochs):\n        model.train()\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            z = model(x)\n            loss = criterion(z, y)\n            loss_plot.append(loss)\n            loss.backward()\n            optimizer.step()\n            \ntrain_model(n_epochs)\n","04faea45":"plot_data = pd.DataFrame(loss_plot).astype(\"float\")\n#plot_data.tail()\nplot_data.plot.line()\n\n#model(TitanicData().x)","d9387978":"test1 = torch.tensor(test.values.astype(np.float32))\n\nx = model(test1)\n","ad7a5e45":"test0 = pd.read_csv(filepath_or_buffer = \"\/kaggle\/input\/titanic\/test.csv\")\nprint(len(test0))\nx_np = x.detach().numpy()\nx_df = pd.DataFrame(x_np)\nx_df = x_df.round()\nx_df = x_df.fillna(0)\nx_df = x_df.astype(int)\n\ntest0 = test0.join(x_df)\n \nprediction = test0[[\"PassengerId\", 0]]\nprediction = prediction.rename(columns={\"PassengerId\":\"PassengerId\", 0: \"Survived\"})\n\nprediction.to_csv('.\/submission.csv', index = False)","39526d82":"# 1. Load data, EDA\n\n* Deal with NaN values\n* Normalise numeric values\n\n","f60442c4":"Export Predictions","3b27b1c4":"Training Loop\n\n"}}