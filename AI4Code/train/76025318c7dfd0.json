{"cell_type":{"f61fc058":"code","72032eec":"code","4556c7f6":"code","f8810ce5":"code","fc86fb5c":"code","b5426fb6":"code","d1021543":"code","9cbd8ed6":"code","41030d9d":"code","4e878162":"code","2cedb379":"code","5a9caeb8":"code","bdd7d814":"code","02fe46c9":"code","d0ee4f27":"code","a6eec0a5":"code","bfa01a83":"code","6abd89a0":"code","3d88c438":"code","c8b51301":"code","2eefe76d":"code","f3fc559d":"code","b65f51e8":"code","6dfff3bc":"code","52dfeb72":"code","a950e7c0":"code","fd593396":"code","4b27520f":"code","196c83c5":"code","05034f7b":"code","5ade7108":"code","d8375922":"code","70cb7ba3":"code","5fd48e4a":"code","8783e033":"code","5c116470":"code","0138d086":"code","02dc89fb":"code","7dd4c135":"code","b6a816c5":"markdown","af516f62":"markdown","5f953765":"markdown","1e9e6f20":"markdown","942a782b":"markdown","cd6e2a71":"markdown","00563d02":"markdown","08fb9ec0":"markdown","fb525540":"markdown","700129bd":"markdown","9c7d4ed5":"markdown","dd0141ab":"markdown","0f64a312":"markdown","dbff3242":"markdown","80b92dd0":"markdown","92520e5f":"markdown","ed9e4237":"markdown","b363e8a9":"markdown","2978414b":"markdown","776bd7ce":"markdown","6e95b36e":"markdown","816c10b9":"markdown"},"source":{"f61fc058":"# Instalation of the package, necessary on Kaggle\n!pip install ftfy\n","72032eec":"##General imports\nimport random\nimport numpy as np\nimport pandas as pd\nimport copy \nimport time\nfrom scipy.stats import uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nimport io\n\n\nfrom ftfy import fix_text\nimport string\nimport re\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.model_selection import train_test_split\n","4556c7f6":"#Defining locations and opening data files\n\nfolder_location = '..\/input\/sentiment-analysis-pmr3508\/'\ntrain_filename = 'data_train.csv'\ntest_filename = 'data_test1.csv'\nsub_x_filename = 'data_test2_X.csv'\ntrain = pd.read_csv(folder_location + train_filename, na_values = \"?\", header=0)\ntest = pd.read_csv(folder_location + test_filename, na_values = \"?\", header=0)\nsub = pd.read_csv(folder_location + sub_x_filename, na_values = \"?\", header=0)\n\nd2v_filename = 'doc2vec'","f8810ce5":"'''\nSimple function to plot some basic info about a dataset\n'''\ndef ds_eval(ds):\n    print(\"Data looks like this:\")\n    print(ds.head())\n    print(\"\\nDuplicated values: \", ds.duplicated().sum())\n    print(\"\\nMissing values:\\n \", ds.isnull().sum())\n    print(\"\\nValues distribution:\\n\" ,ds.positive.value_counts())","fc86fb5c":"ds_eval(train)","b5426fb6":"train = train.drop_duplicates(subset='review')","d1021543":"def clearReviewText(text):\n    txt=text.replace(\"<br \/>\",\" \") #retirando tags\n    txt=fix_text(txt) #consertando Mojibakes (Ver https:\/\/pypi.org\/project\/ftfy\/)\n    txt=txt.lower() #passando tudo para min\u00fasculo\n    txt=txt.translate(str.maketrans('', '', string.punctuation)) #retirando toda pontua\u00e7\u00e3o\n    txt=txt.replace(\" \u2014 \", \" \") #retirando h\u00edfens\n    txt=re.sub(\"\\d+\", ' <number> ', txt) #colocando um token especial para os n\u00fameros\n    txt=re.sub(' +', ' ', txt) #deletando espa\u00e7os extras\n    return txt","9cbd8ed6":"# Uses the clearing function on each line of the dataset\ntrain_X = [clearReviewText(i).split() for i in train.review]","41030d9d":"# Loading of the doc2vec model\nd2v = Doc2Vec.load(folder_location +d2v_filename)  ","4e878162":"# Function that applies the text to vector transform\ndef emb(txt, model, normalize=False): \n    x=model.infer_vector(txt, steps=20)    \n    if normalize: return(x\/np.sqrt(x@x))\n    else: return(x)","2cedb379":"# Use of the above function\ntrain_X = [emb(i, d2v) for i in train_X] \ntrain_X = np.array(train_X)\n","5a9caeb8":"# While debugging locally, we can save the vectorized model to save some time on later runs\n\nnp.save(\"vectorized_data\", train_X) #Vectorized data can be saved for later testing, as it takes some time to get it again","bdd7d814":"# Loading of a already vectorized data\ntrain_X = np.load(\"vectorized_data.npy\")#Loader for the saved vectorized data","02fe46c9":"# Labels for the reviews\ntrain_Y = train.positive","d0ee4f27":"train_X","a6eec0a5":"## Repeat everything for the test data\nds_eval(test)\ntest = test.drop_duplicates(subset='review')\ntest_X = [clearReviewText(i).split() for i in test.review]\ntest_X = [emb(i, d2v) for i in test_X] \ntest_X = np.array(test_X)\ntest_Y = test.positive","bfa01a83":"# Test labels\ntest_Y = test.positive","6abd89a0":"#train_X, train_Y, test_x and test_Y are Go!","3d88c438":"##More imports\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential, regularizers\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.python.framework import ops\nfrom tensorflow.keras.callbacks import EarlyStopping","c8b51301":"## Model building function\ndef buildTFNN(neurons, regularizers_, n_features, no_summary = False):\n    layers = len(neurons)\n    ops.reset_default_graph()\n    #Instancing of the model\n    TFNN = Sequential() \n    TFNN.add(Dense(neurons[0], activation='relu', input_shape=(n_features,),\n                   kernel_regularizer=regularizers.l1_l2(l1=regularizers_[0], l2=regularizers_[1]),\n                   bias_regularizer=regularizers.l1_l2(l1=regularizers_[0], l2=regularizers_[1])))  \n    for i in range(1,layers):\n            TFNN.add(Dense(neurons[i], activation='relu', \n                   kernel_regularizer=regularizers.l1_l2(l1=regularizers_[0], l2=regularizers_[1]),\n                   bias_regularizer=regularizers.l1_l2(l1=regularizers_[0], l2=regularizers_[1]))) \n    \n    TFNN.add(Dense(1, activation='sigmoid'))\n    if no_summary == False: \n        TFNN.summary()\n    TFNN.compile(optimizer='adam', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n    return TFNN","2eefe76d":"# Use the building function to instance the model\nnn1 = buildTFNN([50],[0,0],train_X.shape[1])\nX_train_t, X_train_v, y_train_t, y_train_v = train_test_split(train_X,\n                                                            train_Y,\n                                                            test_size=0.25)\n\n# Early stop allows the train process to stop as it's preformance deteriorates\nearlystop_callback = EarlyStopping(monitor='val_auc', patience=20)\nhistory = nn1.fit(X_train_t, y_train_t,\n                        epochs=30,\n                        validation_data=(X_train_v,y_train_v), \n                        batch_size=100, \n                        shuffle=True, \n                        verbose=True,\n                        callbacks=[earlystop_callback]\n                        ) \n","f3fc559d":"#Plot of the train history \nplt.title('Loss Functions')\nplt.xlabel('Epoch')\nplt.ylabel('Cross Entropy')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.legend()\nplt.show()\n\n","b65f51e8":"#AUC\nplt.title('AUC (ROC)')\nplt.xlabel('Epoch')\nplt.ylabel('AUC')\nplt.plot(history.history['auc'], label='train')\nplt.plot(history.history['val_auc'], label='val')\nplt.legend()\nplt.show()","6dfff3bc":"# Scoring for the first NN\nnn1.evaluate(test_X,  test_Y, verbose=2)","52dfeb72":"iterations = 100\n\nneuron_combinations = []\nregularizer_combinations=[]\n\n#Random values generation\nfor i in range(iterations):\n    h1=random.randrange(25, 100, 1)\n    h2=random.randrange(20, h1, 1)\n    neuron_combinations.append([h1,h2])\n    \n    l1=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    l2=random.choice([0, 1e-15, 1e-10, 1e-5, 1e-3, 1e-2, 1e-1])\n    regularizer_combinations.append([l1, l2])\n    \n#DataFrame\nparameters = {'neurons': neuron_combinations, 'reg': regularizer_combinations, 'epochs': iterations*[None], 'auc': iterations*[None]}\nparameters = pd.DataFrame(parameters)\nparameters = parameters[['neurons', 'reg', 'epochs', 'auc']]\nparameters.head()","a950e7c0":"\nfor i in range(iterations):\n    earlystop_callback = EarlyStopping(monitor='val_auc', patience=3)\n    \n    nn = buildTFNN(parameters.neurons[i],parameters.reg[i],train_X.shape[1], True)\n    history = nn.fit(X_train_t, y_train_t,\n                            epochs=30,\n                            validation_data=(X_train_v,y_train_v), \n                            batch_size=100, \n                            shuffle=True, \n                            verbose=0,\n                            callbacks=[earlystop_callback]\n                            ) \n    parameters.loc[i,'epochs'] = len(history.history['val_auc'])\n    parameters.loc[i,'auc'] = history.history['val_auc'][-1]\n    print(parameters.neurons[i], 'Val Auc: ',history.history['val_auc'][-1])\n","fd593396":"parameters = parameters.iloc[np.argsort(parameters.loc[:,'auc']),:]\nparameters.tail(10)","4b27520f":"# New model instancing and training\nnn2 = buildTFNN([86,73],[1e-15,1e-5],train_X.shape[1])\n\nearlystop_callback = EarlyStopping(monitor='val_auc', patience=10)\nhistory = nn2.fit(X_train_t, y_train_t,\n                        epochs=4,\n                        validation_data=(X_train_v,y_train_v), \n                        callbacks = [earlystop_callback], \n                        shuffle=True, \n                        verbose=True,\n                        \n                        ) ","196c83c5":"nn2.evaluate(test_X,  test_Y, verbose=2)","05034f7b":"# Prepares the evaluation data and aplly the classification\nsub_X = [clearReviewText(i).split() for i in sub.review]\nsub_X = [emb(i, d2v) for i in sub_X] \nsub_X = np.array(sub_X)\nsub_pred1 = nn2.predict(sub_X)\nsubmission = pd.DataFrame()\nsubmission[0] = sub_pred1[:,0]\nsubmission.columns = ['positive']\nsubmission.to_csv(\"submission_Tf.csv\", index = True, index_label = 'Id')","5ade7108":"#Sklearn imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score","d8375922":"#Creation of a first model\nsknn1 = MLPClassifier(solver='lbfgs', alpha=10, hidden_layer_sizes=(25,10), random_state=1, verbose = 1 )\nsknn1.fit(train_X, train_Y)\nsknn1_pred = sknn1.predict_proba(test_X)[:, 1]\nroc_auc_score(test_Y, sknn1_pred)\n","70cb7ba3":"sknn_ = MLPClassifier(early_stopping=True)\nhyperparams = {\n    'hidden_layer_sizes': [(i,j) for i in np.linspace(1, 100, 100).astype('int') for j in np.linspace(1, 100, 100).astype('int')],\n    'alpha': np.logspace(-5, -2, 100)\n}\nsknn = RandomizedSearchCV(sknn_, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1)\nsknn.fit(train_X, train_Y)\nsknn.best_params_, sknn.best_score_","5fd48e4a":"sknn_pred = sknn.predict_proba(test_X)[:,1]\nroc_auc_score(test_Y, sknn_pred)","8783e033":"# Some more imports\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score","5c116470":"# Instancing and cross validation\nknn1 = KNeighborsClassifier(n_neighbors=20)\nscore_numeric = cross_val_score(knn1, train_X, train_Y, cv=10)\nprint(score_numeric)","0138d086":"# Fitting and prediction on the test data\nknn1.fit(train_X, train_Y)\nknn1_pred = knn1.predict_proba(test_X)[:,1]\nroc_auc_score(test_Y, knn1_pred)","02dc89fb":"logreg = LogisticRegression()\nhyperparams = dict(C=np.linspace(0,10,100), \n                     penalty=['l2', 'l1'])\nclf = RandomizedSearchCV(logreg, hyperparams, scoring='roc_auc', n_iter=50, cv=2, n_jobs=-1, random_state=0, verbose=2)\nsearch_logreg = clf.fit(train_X, train_Y)\nsearch_logreg.best_params_, search_logreg.best_score_ \n\n","7dd4c135":"logreg = LogisticRegression(C=search_logreg.best_params_['C'], penalty=search_logreg.best_params_['penalty'])\nlogreg.fit(train_X, train_Y)\nlog1_pred = logreg.predict_proba(test_X)[:,1]\nroc_auc_score(test_Y, log1_pred)","b6a816c5":"Lets proceed to a 2 layer network, with optmized parameters. \nA table is created to store the performance and parameter for each iteration","af516f62":"Going back to SKLearn, we try the Multi Layer Perceptron Classifier ","5f953765":"The best parameters and their scores are shown bellow","1e9e6f20":"We observe a sligh improvement. Enough to test the model with the test dataset","942a782b":"Now, we use the trained model on the test data to get a score that will be used for comparison later","cd6e2a71":"That is a pretty good result for the few lines of code necessary.\n\nWe can also use the built in method to optimize a model with sklearn. Since it is a random search, we define the ranges for the number of neurons and the alpha value.","00563d02":"Next, all the scenarios created above are trainned and tested ","08fb9ec0":"Since it is a pretty good score, lets prepare a submission file","fb525540":"Inspired by the sample program on this compettition, we use a logistic regression for the last model. Again, a random search is used to optimize everything.","700129bd":"# Neural networks for sentiment analysis with the Movie Review Dataset","9c7d4ed5":"There are some duplicated entries on the train data. Lets remove those...","dd0141ab":"This wraps up the Neural Networks models.\nLets also try out two more classifiers, starting by our old friend KNN CLassifier","0f64a312":"It is curious to think how a KNN relates (initially) textual data. It was much clearer for the California Housing problem, for example.","dbff3242":"On the last activities, SKlearn was used to create classifiers and regressors.  With the will to use Neural Networks, lets start righ away with a new library: TensorFlow \/ Keras\nBesides having the coolest name, this one is well known by it's good performance","80b92dd0":"As usual, it is useful to have a glance of the data we are working with. A function can be written to do so.","92520e5f":"Time to define and train a first Neural Network with 1 layer of 50 neurons","ed9e4237":"Since we will test a good amount of models, lets create a function to prepare those","b363e8a9":"Before putting the data through doc-2-vec model, it is mandatory to clear it from any symbols or weird chars. This step has to be concistent between all the data (train and test) and the pre-trained model.","2978414b":"We start by installing ftfy module on the virtual machine. Note it is necessary to **enable 'Internet'** on the Settings tab, at the right menu bar ->","776bd7ce":"Eventhough the score looks fine, it is not better than the neural network model","6e95b36e":"With this result, we build a new model to make the predictions on the test data","816c10b9":"This work is a first experience with neural network classification. Initially, we will use a pre-trainned text to vector model. Then, we can use neural networks in order to classify movie reviews into positive or negative.\n"}}