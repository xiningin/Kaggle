{"cell_type":{"7c1792b9":"code","75d822ee":"code","a0c8fe66":"code","08bf2120":"code","83bb2cac":"code","38044a06":"code","c071b1d0":"code","998ebc5c":"code","9da016a0":"code","f44b6e45":"code","9369c142":"code","331214c9":"code","5a1eb878":"code","61d6c185":"code","b682ab60":"code","5cb24d16":"code","bb4ca297":"code","3b59df9d":"code","0bcdc32a":"code","f53568ff":"code","72ebdcab":"code","22859492":"code","b994f1bd":"code","896ee074":"code","523a1357":"code","7525da4d":"markdown","033d9c96":"markdown","72aa31eb":"markdown","ceecdece":"markdown","8ab49f60":"markdown"},"source":{"7c1792b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","75d822ee":"# importing necessary libraries, required for analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport math","a0c8fe66":"# Reading the dataset as a dataframe\nfile_name = '..\/input\/breast-cancer-wisconsin-data\/data.csv'\ndata_df = pd.read_csv(file_name)","08bf2120":"data_df.head()","83bb2cac":"data_df.info()","38044a06":"data_df.drop('Unnamed: 32', axis=1, inplace=True)","c071b1d0":"data_df.head()","998ebc5c":"print(data_df.shape)","9da016a0":"data_df['diagnosis'].unique()#.iloc[0]","f44b6e45":"diagnosis_mapping = {\"M\": 0, \"B\": 1}\n\ndata_df['diagnosis'] = data_df['diagnosis'].map(diagnosis_mapping)","9369c142":"sns.countplot(x = 'diagnosis', data=data_df)","331214c9":"sns.pairplot(data_df)","5a1eb878":"plt.figure(figsize=(24,8))\nsns.heatmap(data_df.corr())","61d6c185":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE # Oversampling\nfrom sklearn.metrics import accuracy_score, f1_score, fbeta_score, make_scorer\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nimport re\n\nrandom_state= 101\n","b682ab60":"X = data_df.drop(['id', 'diagnosis'], axis=1)\ny = data_df['diagnosis']","5cb24d16":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n","bb4ca297":"# Since dataset is inbalanced, we need to apply SMOTE - it will include new dummy rows for analysis. This needs to be done only on train dataset \nrandom_state = random_state\nfeatures = X_train.columns\nsm = SMOTE(random_state=random_state)#, ratio=1.0)\nX_train, y_train = sm.fit_resample(X_train, y_train)\nX_train = pd.DataFrame(X_train, columns=features)","3b59df9d":"# sns.countplot(x='diagnosis', data=y_train)\n#y_train['diagnosis'].value_counts()\na = pd.DataFrame(y_train)\na['diagnosis'].value_counts()","0bcdc32a":"# Appling MinMaxScaler (fit and transform) on train data and only transform on test data to avoid data leakages \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","f53568ff":"naive_predictor_accuracy = accuracy_score(y_train,np.ones(len(y_train)))\nnaive_predictor_f1score = f1_score(y_train, np.ones(len(y_train)))\n\nprint(\"Naive predictor accuracy: %.3f\" % (naive_predictor_accuracy))\nprint(\"Naive predictor f1-score: %.3f\" % (naive_predictor_f1score))","72ebdcab":"model_performance = []\n\nclassifier_type = [LogisticRegression,\n                    KNeighborsClassifier,\n                    DecisionTreeClassifier,\n                    RandomForestClassifier,\n                    GaussianNB,\n                    SVC, \n                    XGBClassifier]\n\n\ndf = pd.DataFrame(columns=['Model Name', 'Accuracy', 'F1 Score', 'Recall', 'Precision'])\n\nfor mName in classifier_type:\n    model_name = mName\n    model_name = str(model_name)\n    model = mName()\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(mName,':- ') \n    print(classification_report(y_test,predictions))\n    print('-------------------------------------------------------------')\n    clf_accuracy = accuracy_score(y_test,predictions)\n    clf_f1_score = f1_score(y_test,predictions)\n    clf_recall_score = recall_score(y_test,predictions)\n    clf_precision_score = precision_score(y_test,predictions)\n    \n    #print(\"%s model accuracy-score: %.3f\" % (mName, clf_accuracy))\n    #print(\"%s model f1-score: %.3f\" % (mName, clf_f1_score))\n    #print(\"%s model recall-score: %.3f\" % (mName, clf_recall_score))\n    #print(\"%s model precision-score: %.3f\" % (mName, clf_precision_score))\n    \n    nameLen = len(model_name.split('.'))\n    model_name = model_name.split('.')[nameLen-1]\n    model_name= re.sub('[^A-Za-z0-9]+', '', model_name)\n        \n    df = df.append({'Model Name': model_name, 'Accuracy': clf_accuracy, 'F1 Score': clf_f1_score, 'Recall': clf_recall_score, 'Precision': clf_precision_score }, ignore_index=True)\n\ndf = df.sort_values('Accuracy', ascending=False)\ndf = df.reset_index(drop=True)\ndf","22859492":"# Working model\n\ngbm = XGBClassifier(max_depth=3, n_estimators=400, learning_rate=0.05).fit(X_train, y_train)\npredictions = gbm.predict(X_test)\n\nprint(classification_report(y_test,predictions))","b994f1bd":"confusion_matrix(y_test, predictions)","896ee074":"clf_xgb = XGBClassifier(objective = 'binary:logistic')\nparam_dist = {'n_estimators': [400],\n              'learning_rate': [.10, .01, .001, .001],\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'min_child_weight': [1, 2, 3, 4]\n             }\n\nclf = RandomizedSearchCV(clf_xgb, \n                         param_distributions = param_dist,\n                         n_iter = 5, \n                         scoring = 'roc_auc', \n                         error_score = 0, \n                         verbose = 3, \n                         n_jobs = -1)\n\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\nprint(classification_report(y_test,predictions))","523a1357":"relative_importance = clf.best_estimator_.feature_importances_\nrelative_importance = relative_importance \/ np.sum(relative_importance)\n\nfeature_importance =\\\n    pd.DataFrame(list(zip(features,\n                          relative_importance)),\n                 columns=['feature', 'relativeimportance'])\n\nfeature_importance = feature_importance.sort_values('relativeimportance',\n                                                    ascending=False)\n\nfeature_importance = feature_importance.reset_index(drop=True)\n\npalette = sns.color_palette(\"coolwarm\", feature_importance.shape[0])\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x='relativeimportance',\n            y='feature',\n            data=feature_importance,\n            palette=palette)\nplt.xlabel('XGBClassifier')\nplt.ylabel('Feature')\nplt.title('XGBClassifier Estimated Feature Importance')","7525da4d":"# End","033d9c96":"# Applying ML with RandomizedSearchCV - Hyperparameter","72aa31eb":"# Model Baselining","ceecdece":"With Naive predictor baselining model is baselined\n\n# Execute different ML model to find the best fit","8ab49f60":"# XGBClassifier model looks to be best fit, lets try some basic modeling "}}