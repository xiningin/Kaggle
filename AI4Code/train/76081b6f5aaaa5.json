{"cell_type":{"f8e91828":"code","1bb0daed":"code","a46d93b2":"code","70b74003":"code","016dff7b":"code","c71f6a92":"code","215d641b":"code","439ddae2":"code","17bdea27":"code","99e62eba":"code","0795d1aa":"code","533f4195":"code","8a4c8d85":"code","8a60efb5":"code","08daa190":"code","3e1abe71":"code","456763f5":"code","911aed5f":"code","eaa7d331":"code","fce0c806":"code","9be07cf9":"code","0eeaa596":"code","ed327755":"code","4ff8fe0e":"code","bcfe9cfd":"code","3bc1361d":"code","43e9c029":"code","3570c4fd":"markdown","ccc49311":"markdown","5479dfcb":"markdown","8ebadaaf":"markdown","2b3fb725":"markdown","211d2e21":"markdown","ff962e0e":"markdown","7c82ce9f":"markdown","6ddfe033":"markdown","d11c72c3":"markdown","49218498":"markdown","0ea0dd4d":"markdown","9c142635":"markdown","663e95b1":"markdown","1b331697":"markdown","ae72a2fa":"markdown","b14f0e2c":"markdown","3377fed4":"markdown","8aba8b59":"markdown","51aa2cdd":"markdown"},"source":{"f8e91828":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport plotly.express as px\nfrom mpl_toolkits import mplot3d \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,plot_confusion_matrix\n\n","1bb0daed":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")","a46d93b2":"df.head()","70b74003":"df.describe()","016dff7b":"df.isna().any()","c71f6a92":"df.dtypes","215d641b":"hm = df.corr()\nsns.heatmap(hm, xticklabels=hm.columns, yticklabels=hm.columns , cmap='Blues')","439ddae2":"fig = px.box(df, x='sex', y='age', points=\"all\",color_discrete_sequence =['green']*len(df))\nfig.update_layout(title_text=\"Male = 1 Female =0\")\nfig.show()","17bdea27":"fig = px.box(df, x='target', y='age', color_discrete_sequence =['red']*len(df))\nfig.show()","99e62eba":"fig = plt.figure(figsize =(14, 9)) \nax = plt.axes(projection ='3d') \n\nax.scatter(df['age'], df['sex'], df['chol'], c='orange', marker='o') \nax.set_xlabel('Age')\nax.set_ylabel('Sex')\nax.set_zlabel('Cholestrol')  \nplt.show() ","0795d1aa":"X=df.values\nx=X[:,0:13]\ny=X[:,13]\n","533f4195":"x=preprocessing.StandardScaler().fit(x).transform(x)\nxtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.1,random_state=4)","8a4c8d85":"from sklearn.neighbors import KNeighborsClassifier\nkmax=22\nmean_acc=np.zeros((kmax-1))\n\nfor i in range(1,kmax):\n    kn=KNeighborsClassifier(n_neighbors=i).fit(xtrain,ytrain)\n    yhat=kn.predict(xtest)\n    n1=1000\n    mean_acc[i-1]=metrics.accuracy_score(ytest,yhat)\n    \nplt.plot(range(1,kmax),mean_acc,'r')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of neighbors')\nplt.tight_layout()\nplt.show()","8a60efb5":"print(\"The best accuracy of KNN was\", mean_acc.max(),\"with k=\",mean_acc.argmax()+1)\nkn=KNeighborsClassifier(n_neighbors=19).fit(xtrain,ytrain)\nyhat=kn.predict(xtest)\n\nplot_confusion_matrix(kn,xtest,ytest,cmap=plt.cm.Blues)\nplt.show()\nprint(classification_report(ytest,yhat))\na1=metrics.accuracy_score(ytest,yhat)\n\n","08daa190":"from sklearn.linear_model import LogisticRegression\n","3e1abe71":"LR1=LogisticRegression(C=0.01,solver=\"liblinear\").fit(xtrain,ytrain)\nyhat1=LR1.predict(xtest)\n\nplot_confusion_matrix(LR1,xtest,ytest,cmap=plt.cm.Reds)\nplt.show()\n\nprint(classification_report(ytest,yhat1))\na2=metrics.accuracy_score(ytest,yhat1)\n","456763f5":"LR2=LogisticRegression(C=0.01,solver=\"newton-cg\").fit(xtrain,ytrain)\nyhat2=LR2.predict(xtest)\n\nplot_confusion_matrix(LR2,xtest,ytest,cmap=plt.cm.Greys)\nplt.show()\n\nprint(classification_report(ytest,yhat2))\na3=metrics.accuracy_score(ytest,yhat2)\n","911aed5f":"LR3=LogisticRegression(C=0.01,solver=\"sag\").fit(xtrain,ytrain)\nyhat3=LR3.predict(xtest)\n\nplot_confusion_matrix(LR3,xtest,ytest,cmap=plt.cm.YlOrBr)\nplt.show()\n\nprint(classification_report(ytest,yhat3))\na4=metrics.accuracy_score(ytest,yhat3)\n","eaa7d331":"from sklearn import svm\n","fce0c806":"svmM1=svm.SVC(C=0.2,kernel='rbf')\nsvmM1.fit(xtrain,ytrain)\nyhatsvm1=svmM1.predict(xtest)\n\nplot_confusion_matrix(svmM1,xtest,ytest,cmap=plt.cm.Greens)\nplt.show()\n\nprint(classification_report(ytest,yhatsvm1))\na5=metrics.accuracy_score(ytest,yhatsvm1)\n","9be07cf9":"svmM2=svm.SVC(C=0.1,kernel='linear')\nsvmM2.fit(xtrain,ytrain)\nyhatsvm2=svmM2.predict(xtest)\n\nplot_confusion_matrix(svmM2,xtest,ytest,cmap=plt.cm.Blues)\nplt.show()\n\nprint(classification_report(ytest,yhatsvm2))\na6=metrics.accuracy_score(ytest,yhatsvm2)\n","0eeaa596":"svmM3=svm.SVC(C=1.5,kernel='poly')\nsvmM3.fit(xtrain,ytrain)\nyhatsvm3=svmM3.predict(xtest)\n\nplot_confusion_matrix(svmM3,xtest,ytest,cmap=plt.cm.Oranges)\nplt.show()\n\nprint(classification_report(ytest,yhatsvm3))\na7=metrics.accuracy_score(ytest,yhatsvm3)\n","ed327755":"from sklearn.tree import DecisionTreeClassifier\n\ndt=DecisionTreeClassifier(criterion=\"entropy\")\ndt.fit(xtrain,ytrain)\nyhatdt=dt.predict(xtest)\n\nplot_confusion_matrix(dt,xtest,ytest,cmap=plt.cm.Reds)\nplt.show()\n\nprint(classification_report(ytest,yhatdt))\na8=metrics.accuracy_score(ytest,yhatdt)\n","4ff8fe0e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=n1)\nrf = rf.fit(xtrain, ytrain)\nyhatrf=rf.predict(xtest)\n\nplot_confusion_matrix(rf,xtest,ytest,cmap=plt.cm.Greys)\nplt.show()\n\nprint(classification_report(ytest,yhatrf))\na9=metrics.accuracy_score(ytest,yhatrf)\n","bcfe9cfd":"from sklearn.ensemble import AdaBoostClassifier\nab = AdaBoostClassifier(n_estimators=17)\nab = ab.fit(xtrain, ytrain)\nyhatab=ab.predict(xtest)\n\nplot_confusion_matrix(ab,xtest,ytest,cmap=plt.cm.Greens)\nplt.show()\n\nprint(classification_report(ytest,yhatab))\na10=metrics.accuracy_score(ytest,yhatab)\n","3bc1361d":"print(\"KNN Accuracy                            : %.5f\"%metrics.accuracy_score(ytest,yhat))\nprint(\"Logistic Regression(liblinear) Accuracy : %.5f\"%metrics.accuracy_score(ytest,yhat1))\nprint(\"Logistic Regression(newton-cg) Accuracy : %.5f\"%metrics.accuracy_score(ytest,yhat2))\nprint(\"Logistic Regression(sag) Accuracy       : %.5f\"%metrics.accuracy_score(ytest,yhat2))\nprint(\"SVM(rbf Kernel) Accuracy                : %.5f\"%metrics.accuracy_score(ytest,yhatsvm1))\nprint(\"SVM(linear Kernel) Accuracy             : %.5f\"%metrics.accuracy_score(ytest,yhatsvm2))\nprint(\"SVM(polynomial Kernel)                  : %.5f\"%metrics.accuracy_score(ytest,yhatsvm3))\nprint(\"Decision Tree Accuracy                  : %.5f\"%metrics.accuracy_score(ytest,yhatdt))\nprint(\"Random Forest Accuracy                  : %.5f\"%metrics.accuracy_score(ytest,yhatrf))\nprint(\"Ada Boost Accuracy                      : %.5f\"%metrics.accuracy_score(ytest,yhatab))\n\n","43e9c029":"\n\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nclassifiers = ['KNN', 'LR1', 'LR2', 'LR3', 'SVM1','SVM2','SVM3','DT','RF','AB']\naccuracies = [a1,a2,a3,a4,a5,a6,a7,a8,a9,a10]\nax.bar(classifiers,accuracies,align='center', width=0.4)\nplt.ylim([0.7, 0.95])\nplt.show()","3570c4fd":"**Ada Boost Classifier :-**","ccc49311":"**Support Vector Machine Classifier :-**","5479dfcb":"**K Nearest Neighbour Classifier :-**","8ebadaaf":"**Visulizations of the data :-**","2b3fb725":"**Standardizing all the feature values and spliiting the data into train and test set:-**","211d2e21":"Using the rbf kernel:","ff962e0e":"**Decison Tree Classifier :-**","7c82ce9f":"Using the ploynomial kernel :","6ddfe033":"**Making sure all the data is in numeric form**","d11c72c3":"Using Lib Linear solver :","49218498":"Using SAG solver :","0ea0dd4d":"Finding the optimal number of neighbors","9c142635":"Using Newton-cg solver :","663e95b1":"**Viewing the data contents and its summary :-**","1b331697":"**Checking if any feature column has any null values**","ae72a2fa":"**Logisitic Regression Classifier:-**","b14f0e2c":"**Accuracies of the different classifier models used :-**","3377fed4":"**Random Forest Classifier :-**","8aba8b59":"Using the linear kernel:","51aa2cdd":"We can see that for the given data, Logisitic Regression(with Linear solver), SVM(with a Linear Kernel) and Random Forest Classifier gave the highest accuracies. Whereas SVM(with a Ploynomial Kernel) and Decision Tree Classifier gave the least accuracy.\nI hope this insight, comparions and the various models helped.\nCheers"}}