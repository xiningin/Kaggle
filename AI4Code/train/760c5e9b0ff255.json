{"cell_type":{"61f906ff":"code","6c1bd64f":"code","bb29e050":"code","0d08627b":"code","219b50f6":"code","2a9fe853":"markdown"},"source":{"61f906ff":"import pandas as pd\nimport category_encoders as ce\nfrom sklearn import linear_model\nfrom sklearn.model_selection import StratifiedKFold","6c1bd64f":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\n\ntrain.sort_index(inplace=True)\ntrain_y = train['target']\ntest_id = test['id']\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","bb29e050":"cat_feat_to_encode = train.columns.tolist()\nsmoothing=50.0\n\noof = pd.DataFrame([])\nfor tr_idx, oof_idx in StratifiedKFold(\n    n_splits=5, random_state=1, shuffle=True).split(\n        train, train_y):\n    ce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\n    ce_target_encoder.fit(train.iloc[tr_idx, :], train_y.iloc[tr_idx])\n    oof = oof.append(ce_target_encoder.transform(train.iloc[oof_idx, :]), ignore_index=False)\n\nce_target_encoder = ce.TargetEncoder(cols = cat_feat_to_encode, smoothing=smoothing)\nce_target_encoder.fit(train, train_y)\ntrain = oof.sort_index() \ntest = ce_target_encoder.transform(test)","0d08627b":"glm = linear_model.LogisticRegression(\n  random_state=1, solver='lbfgs', max_iter=5000, fit_intercept=True, \n  penalty='none', verbose=0)\n\nglm.fit(train, train_y)","219b50f6":"from datetime import datetime\npd.DataFrame({'id': test_id, 'target': glm.predict_proba(test)[:,1]}).to_csv(\n    'sub_' + str(datetime.now().strftime('%Y-%m-%d_%H-%M-%S')) + '.csv', \n    index=False)","2a9fe853":"Target encoding&mdash;as implemented in [contrib.scikit-learn.org\/categorical-encoding](https:\/\/contrib.scikit-learn.org\/categorical-encoding\/targetencoder.html)&mdash;can prove powerful especially to encode high cardinality categorical features. This implementation assumes that the target is ordinal (which is the case here as it is a binary outcome, but for many multiclass classification that is often not the case).\n\nHere we use it for all features as a starting point, but many of those features might better contribute to the overall predictive power when encoded with alternative techniques.\n\nWe use *k-fold* to mitigate data leaks that would otherwise almost certainly lead to overfitting. Alternatively, we could split the train set, but given the small size of it, a resampling technique sounds preferable."}}