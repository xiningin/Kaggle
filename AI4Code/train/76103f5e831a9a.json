{"cell_type":{"bc8ccbbd":"code","88fa1670":"code","54b84c46":"code","65efb0bf":"code","fbc7be35":"code","d8bf568d":"code","6c61bddc":"code","fce1d25a":"code","4d8052b8":"code","2a2efb86":"code","02fa52a2":"code","5e563557":"code","55f4d4f1":"code","c1032e39":"code","0753a94b":"code","56a84719":"code","d8e012fc":"code","f26901ce":"code","1c6224c4":"code","a5724917":"code","9fd10354":"code","4fbdc495":"markdown","a3fdd6a9":"markdown","4801a4aa":"markdown","237aa685":"markdown","5045afbc":"markdown","3a663da8":"markdown","a3c022de":"markdown","0b903061":"markdown","113dede3":"markdown","d219622d":"markdown","feb38d1c":"markdown","78b3f3bc":"markdown","81a6c50c":"markdown","1685a067":"markdown","a30e270b":"markdown","e67ac805":"markdown","5cdd91d8":"markdown","af21cb81":"markdown","df02169b":"markdown","68b09bc2":"markdown","97d8ddb5":"markdown"},"source":{"bc8ccbbd":"import random as r\nimport math as m\ninside = 0\ntotal = 100000\n# Iterate for the number of darts.\nfor i in range(0, total):\n    # Generate random x, y in [0, 1].\n    x2 = r.random()**2\n    y2 = r.random()**2\n    # Increment if inside unit circle.\n    if m.sqrt(x2 + y2) < 1.0:\n        inside += 1\nmystery = (float(inside) \/ total) * 4\nprint(mystery)","88fa1670":"text_file = open(\"Output.txt\", \"w\")\ntext_file.write(\"Number: %s\" % mystery)\ntext_file.close()\n\n#Lets download the file we have just created.\n#add a new markdown cell with the following content:\n#<a href=\"Output.txt\"> Download File <\/a>\n#Run the cell.","54b84c46":"\n##Don't forget to turn the internet on. Right menu->settings->Internet. Might ask for phone number\n!pip install git+https:\/\/github.com\/goolig\/dsClass.git","65efb0bf":"from dsClass.path_helper import get_file_path\n%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#very important, let talk about it\nf_name = get_file_path('fruit_data_with_colors.csv')\n\n","fbc7be35":"fruits = pd.read_csv()#TODO, what is missing? To figure out use \"?pd.read_csv\" in a new code cell\nfruits.head()\n","d8bf568d":"#How many samples and features?\nfruits.shape","6c61bddc":"#Which fruits?\nfruits['fruit_name'].unique()","fce1d25a":"fruits.describe()","4d8052b8":"#Is the data balanced?\nfruits.groupby('fruit_name').size()\n","2a2efb86":"import seaborn as sns\n#We can also visualize it:\nsns.countplot(fruits['fruit_name'],label=\"Count\")\nplt.show()","02fa52a2":"fruits.drop('fruit_label', axis=1).plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False, figsize=(9,9), \n                                        title='Box Plot for each input variable')\nplt.savefig('fruits_box')\nplt.show()\n#Discuss box and whiskers plots","5e563557":"import pylab as pl\nfruits.drop('fruit_label' ,axis=1).hist(bins=30, figsize=(9,9))\npl.suptitle(\"Histogram for each numeric input variable\")\nplt.savefig('fruits_hist')\nplt.show()\n","55f4d4f1":"\nimport matplotlib.pyplot as plt\ncorr = fruits.corr()\ncorr.style.background_gradient()\n","c1032e39":"from sklearn.model_selection import train_test_split\nfeature_names = ['mass', 'width', 'height', 'color_score']\n\nX = fruits[feature_names]\nX.loc[:,'volume'] = #How can we calcualte the volume? Google is your best friend.\ny = fruits['fruit_name']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","0753a94b":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit() #TODO: complete the fit. Try Shift + Tab. Try pressing it twice, 3 and 4 times.\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))\n","56a84719":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit()#TODO\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))\n","d8e012fc":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit()#TODO\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test, y_test)))\n","f26901ce":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit()#TODO\nprint('Accuracy of GNB classifier on training set: {:.2f}'\n     .format(gnb.score(X_train, y_train)))\nprint('Accuracy of GNB classifier on test set: {:.2f}'\n     .format(gnb.score(X_test, y_test)))\n","1c6224c4":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nclf.fit()  #TODO\nprint('Accuracy of rf classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of rf classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","a5724917":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\npred = knn.predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))\n","9fd10354":"ans = pd.DataFrame(X_test)\nans['prediction'] = #TODO\nans","4fbdc495":"# Evaluation","a3fdd6a9":"# Welcome \nMistery code, practice simple syntax, what does it do?","4801a4aa":"# Preprocessing","237aa685":"Let's have a look at correlation:","5045afbc":"Logistic regression: https:\/\/en.wikipedia.org\/wiki\/Logistic_regression\nAssumes linear relation between X and a transformation of y (log odds).","3a663da8":"Ensemble on many small deicion trees","a3c022de":"What can you say about each of the features? let's invistigate","0b903061":"# Classification:","113dede3":"# Data exploration\nLet\u2019s read it and have a look the first a few rows of the data.","d219622d":"# Random Forest","feb38d1c":"# Naive Bayes\nAn algorithm based on bayes thorem:\n![image.png](attachment:image.png)","78b3f3bc":"# What can we do to improve our performance? \n1. Feature engineering: sclaing. \n2. Let's add some features. After evaluating the results we can \n3. Are the results consistent?","81a6c50c":"# KNN\nk nearst nieghbors classifiers asisngs the class of the closest samples.\nDistance in the basic algorithm is euclidean.","1685a067":"Adding our prediction to the test set:","a30e270b":"# Source for the following code: https:\/\/towardsdatascience.com\/solving-a-simple-classification-problem-with-python-fruits-lovers-edition-d20ab6b071d2","e67ac805":"# Decision Tree\n\nAdd code to train a deicsion tree.\nDecision tree tries to create a tree such as this in a greedy fashion:\n![image.png](attachment:image.png)","5cdd91d8":"Print to file:","af21cb81":"Tips for using jupyter: https:\/\/www.dataquest.io\/blog\/jupyter-notebook-tips-tricks-shortcuts\/\n\nNew to python? Here is a short tutorial for python: https:\/\/www.tutorialspoint.com\/python3\/python_basic_syntax.htm","df02169b":"# Data\nThe fruits dataset was created by Dr. Iain Murray from University of Edinburgh. He bought a few dozen oranges, lemons and apples of different varieties, and recorded their measurements in a table. And then the professors at University of Michigan formatted the fruits data slightly and it can be downloaded from the linke above.\n","68b09bc2":"Installing our package","97d8ddb5":"We split our data. We let the prediction model to observe the train set, and we ask him to predict the observations in the test set.\nWe can later see if he is correct."}}