{"cell_type":{"ab96cbb4":"code","9ce2c110":"code","1b246389":"code","2022311d":"code","4c5a5b76":"code","756c9703":"code","269a5aa8":"code","b7b6d745":"code","a90c430b":"code","393ae540":"code","f7972edc":"code","5cb9dece":"code","4c153729":"code","bd502811":"code","5d41d8e9":"code","87675918":"code","d6d94eec":"code","f7704556":"code","b0ff76b7":"code","5febe38d":"code","83e46f16":"code","93dee17a":"code","d002b1c6":"code","d29abeab":"code","3cde50d7":"code","8e3b19db":"code","6b53c560":"code","822a6190":"code","23c7f2b1":"code","b465eda2":"code","a31c5151":"code","695e40ec":"code","8a21d729":"code","7945f6c4":"code","ce31ec4e":"code","eb0dc7ea":"code","a63d37f1":"code","22ece2f6":"code","8a388003":"code","53a27248":"code","f3a4f336":"code","f0cc7d99":"code","b57f5afd":"code","d3a1e89b":"code","add7422b":"code","120505b1":"code","3253b650":"code","f85f7fa6":"code","70d55a3b":"code","a729b533":"code","d7bcc431":"code","e00789ef":"code","585e40c4":"code","b35c5b70":"code","3fb48ee6":"code","6c997f05":"code","6cfc4f73":"code","02dec6d4":"code","9639708b":"code","13050fb0":"code","db120304":"code","8bc6f930":"code","d93b53e4":"code","1c1d347b":"code","5984f9b5":"code","6106d55e":"code","04026547":"code","0412b8ab":"code","78f9de57":"code","37630771":"code","68a26562":"code","87ac02f0":"code","22fc8111":"code","734761e6":"markdown","765c88da":"markdown","75ede783":"markdown","651e173e":"markdown","c6170087":"markdown","98324a2e":"markdown","af315141":"markdown","2c7ab4ee":"markdown","04d70af0":"markdown","a01aa102":"markdown","a9cbf33a":"markdown","5739f24e":"markdown","6722ba8c":"markdown","92ed96c5":"markdown","ac93b1e8":"markdown","213f9978":"markdown","72b6008c":"markdown","10b33f6b":"markdown","18eb194b":"markdown","bf6800be":"markdown","8c24be4c":"markdown","1eb79477":"markdown","a20dc822":"markdown","f4318a4a":"markdown","3e0446a8":"markdown","988d0582":"markdown","f3c9f542":"markdown","aa21be50":"markdown","b2dcfe97":"markdown","dcc8fce8":"markdown","8062ca2e":"markdown","30c0de73":"markdown","9cbd9de3":"markdown"},"source":{"ab96cbb4":"from google.colab import drive\ndrive.mount('\/content\/gdrive\/')","9ce2c110":"import pandas as pd \nimport numpy as np\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain_data_path = '\/content\/gdrive\/My Drive\/Colab Notebooks\/Kaggle Project\/Housing Price\/train.csv'\ntest_data_path = '\/content\/gdrive\/My Drive\/Colab Notebooks\/Kaggle Project\/Housing Price\/test.csv'\nsaved_file_path = '\/content\/gdrive\/My Drive\/Colab Notebooks\/Kaggle Project\/Housing Price\/submission.csv'","1b246389":"# Read the data\nX_full = pd.read_csv(train_data_path,index_col='Id')\nX_test_full = pd.read_csv(test_data_path,index_col='Id')\nprint(\"Shape of the Training Set:\",X_full.shape)\nprint(\"Shape of the Test Set\",X_test_full.shape)","2022311d":"col_with_missing = (X_full.isnull().sum())\nlist_features_with_na = col_with_missing[col_with_missing>0]\nprint(list_features_with_na)","4c5a5b76":"#find the column names which have missing values\nfeatures_with_na = [feature for feature in X_full.columns if X_full[feature].isnull().sum()>0]\n#print(features_with_na)\n\n# Find the percentage of missing values in each feature\nfor feature in features_with_na:\n  print(\"{} ----- {:,.4f}\".format(feature,X_full[feature].isnull().mean()))","756c9703":"for feature in features_with_na:\n  data = X_full.copy()\n  # make a variable that indicates '1' if the observation is missing otherwise '0'\n  data[feature] = np.where(data[feature].isnull(),1,0)\n  # Calculate the mean sales price where the information is missing\n  data.groupby(feature).SalePrice.median().plot.bar()\n  plt.title(feature)\n  plt.show()","269a5aa8":"numerical_features = [cname for cname in X_full.columns if X_full[cname].dtype in ['int64','float64']]\nprint(\"Number of numerical Features: \",len(numerical_features))\nX_full[numerical_features].head()","b7b6d745":"year_features = [cname for cname in numerical_features if 'Yr' in cname or 'Year' in cname]\n#print(temp_features)\n# Let's explore the contents of these features\nfor feature in year_features:\n  print(feature,X_full[feature].unique())","a90c430b":"X_full.groupby(\"YrSold\")['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Sale Price')\nplt.title(\"Price vs YrSold \")\n","393ae540":"## Here we will compare the difference between All years feature with SalePrice\n\nfor feature in year_features:\n    if feature!='YrSold':\n        data=X_full.copy()\n        ## We will capture the difference between year variable and year the house was sold for\n        data[feature]=data['YrSold']-data[feature]\n\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.show()","f7972edc":"## Numerical variables are usually of 2 type\n## 1. Continous variable and Discrete Variables\n\ndiscrete_feature=[feature for feature in numerical_features if len(X_full[feature].unique())<25 and feature not in year_features]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))\nX_full[discrete_feature].head()","5cb9dece":"## Lets Find the realtionship between them and Sale PRice\n\nfor feature in discrete_feature:\n    data=X_full.copy()\n    data.groupby(feature).SalePrice.median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","4c153729":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature+year_features]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))\nX_full[continuous_feature].head()","bd502811":"## Lets analyse the continuous values by creating histograms to understand the distribution\n\nfor feature in continuous_feature:\n    data=X_full.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()\n","5d41d8e9":"for feature in continuous_feature:\n  data = X_full.copy()\n  if 0 in data[feature].unique():\n    pass\n  else:\n    data[feature] = np.log(data[feature])  \n    data['SalePrice'] = np.log(data['SalePrice'])\n    plt.scatter(x = data[feature],y = data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","87675918":"for feature in continuous_feature:\n  data = X_full.copy()\n  if 0 in data[feature].unique():\n    pass\n  else:\n    data[feature] = np.log(data[feature])  \n    data.boxplot(column=feature)\n    plt.title(feature)\n    plt.show()","d6d94eec":"for feature in continuous_feature:\n  data = X_full.copy()\n  if 0 in data[feature].unique():\n    pass\n  else:\n    Q1 = data[feature].quantile(0.25)\n    Q3 = data[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_limit = Q1 - 1.5*IQR\n    upper_limit = Q3 + 1.5*IQR\n    df_no_outlier = data[(data[feature]>lower_limit)&(data[feature]<upper_limit)]\n    plt.boxplot(df_no_outlier[feature])\n    #data.boxplot(column=feature)\n    plt.title(\"After Removing Outlier \"+feature)\n    plt.show()","f7704556":"#X_full['GrLivArea'] = np.log(X_full['GrLivArea'])  \nX_full.boxplot(column='GrLivArea')\nplt.title('GrLivArea')\nplt.show()","b0ff76b7":"Q1 = X_full['GrLivArea'].quantile(0.25)\nQ3 = X_full['GrLivArea'].quantile(0.75)\nQ1, Q3\n","5febe38d":"IQR = Q3 - Q1\nIQR","83e46f16":"lower_limit = Q1 - 1.5*IQR\nupper_limit = Q3 + 1.5*IQR\nlower_limit, upper_limit","93dee17a":"df_no_outlier = X_full[(X_full['GrLivArea']>lower_limit)&(X_full['GrLivArea']<upper_limit)]\nplt.boxplot(df_no_outlier.GrLivArea)\nplt.show()","d002b1c6":"categorical_features = [cname for cname in X_full.columns if X_full[cname].dtype =='object']\nprint(\"Number of Categorical Features:\",len(categorical_features))","d29abeab":"# we have to find the cardinality in each categorical features\nfor feature in categorical_features:\n  print(\"{}--Cardinality: {}\".format(feature,len(X_full[feature].unique())))","3cde50d7":"# The relationship between the categorical features and SalePrice\nfor feature in categorical_features:\n  data = X_full.copy()\n  #data.groupby(feature).SalePrice.median().plot.bar()\n  sns.swarmplot(x= data[feature],y = data['SalePrice'])\n  plt.title(feature)\n  plt.xlabel(feature)\n  plt.ylabel('SalePrice')\n  plt.show()","8e3b19db":"droped_columns = [cname for cname in X_test_full.columns if X_test_full[cname].isnull().sum()>1000 ]\ndroped_columns","6b53c560":"# Always remember there must be some data leakage so do split the dataset first\n# Remove rows with missing sale price\nX_full.dropna(subset=['SalePrice'],axis = 0,inplace = True)\ny = X_full.SalePrice\nX_full.drop(columns=droped_columns,axis =1,inplace=True )\nX_test_full.drop(columns=droped_columns,axis =1,inplace=True )\nX_full.drop(columns=['SalePrice'],axis =1,inplace=True )\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X_full, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint(\"After Removing size of Train Set:\",X_train.shape)\nprint(\"After Remving Size of Validation Set:\",X_valid.shape)\nprint(\"After Removing Size of Test Set:\",X_test_full.shape)","822a6190":"X_train_full =X_train\nX_train_full['SalePrice'] = y_train \nX_valid_full =X_valid\nX_valid_full['SalePrice'] = y_valid ","23c7f2b1":"## Let us capture all the nan values\n## First lets handle Categorical features which are missing\nfeatures_nan=[feature for feature in X_full.columns if X_full[feature].isnull().sum()>0 and X_full[feature].dtypes=='O']\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(X_full[feature].isnull().mean(),4)))","b465eda2":"categorical_features=[feature for feature in X_full.columns if X_full[feature].dtypes=='O']\n## Replace missing value with the most-frequent value\ndef replace_cat_feature(dataset,features_nan):\n    data=dataset.copy()\n    for feature in features_nan:\n      value = data[feature].value_counts().to_dict()\n        # use most frequent value\n      most_frequent = max(value,key=value.get)\n      #data[feature+' nan']=np.where(data[feature].isnull(),1,0)\n      data[feature]=data[feature].fillna(most_frequent)\n    return data\n\nprint(\"Number of Categorical Features: \",len(categorical_features))\nX_train_full=replace_cat_feature(X_train_full,categorical_features)\nX_valid_full=replace_cat_feature(X_valid_full,categorical_features)\nX_test_full=replace_cat_feature(X_test_full,categorical_features)\nprint(\"After Removing Missing Values:\\n\")\nprint(\"Size of Train Set:\",X_train_full.shape)\nprint(\"Size of Validation Set:\",X_valid_full.shape)\nprint(\"Size of Test Set:\",X_test_full.shape)","a31c5151":"#X_train_full[categorical_features].isnull().sum()\nX_test_full.head()","695e40ec":"## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in X_full.columns if X_full[feature].isnull().sum()>1 and X_full[feature].dtypes!='O']\n\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(X_full[feature].isnull().mean(),4)))","8a21d729":"numerical_features = [feature for feature in X_full.columns if X_full[feature].dtypes!='O']\n# Replace nan values with the median value\ndef replace_num_feature(dataset,numerical_with_nan):\n  data = dataset.copy()\n  for feature in numerical_with_nan:\n    ## We will replace by using median since there are outliers\n    median_value=data[feature].median()\n    ## create a new feature to capture nan values\n    #data[feature+' nan']=np.where(data[feature].isnull(),1,0)\n    data[feature].fillna(median_value,inplace=True)\n  return data\n\nprint(\"Number of Numerical Features: \",len(numerical_features))\nX_train_full=replace_num_feature(X_train_full,numerical_features)\nX_valid_full=replace_num_feature(X_valid_full,numerical_features)\nX_test_full=replace_num_feature(X_test_full,numerical_features)  \nprint(\"After Removing Missing Values:\\n\")\nprint(\"Size of Train Set:\",X_train_full.shape)\nprint(\"Size of Validation Set:\",X_valid_full.shape)\nprint(\"Size of Test Set:\",X_test_full.shape)","7945f6c4":"X_valid_full[numerical_features].isnull().sum()","ce31ec4e":"X_test_full[numerical_features].isnull().sum()    ","eb0dc7ea":"# Temporal Varaibles Handling\ntemp_features = ['YearBuilt','YearRemodAdd','GarageYrBlt']\ndef temp_variables_handling(dataset,temp_features):\n  data = dataset.copy()\n  for feature in temp_features:\n    data[feature] = data['YrSold']-data[feature]\n  return data  \n\n###\nX_train_full = temp_variables_handling(X_train_full,temp_features)\nX_valid_full = temp_variables_handling(X_valid_full,temp_features)\nX_test_full = temp_variables_handling(X_test_full,temp_features)  \nprint(\"After Adding Temporal Variables:\\n\")\nprint(\"Size of Train Set:\",X_train_full.shape)\nprint(\"Size of Validation Set:\",X_valid_full.shape)\nprint(\"Size of Test Set:\",X_test_full.shape)","a63d37f1":"correlated_features = set()\ncorrelation_matrix = X_train_full.corr()\n#plt.figure(figsize=(30,20))\n#sns.heatmap(correlation_matrix, annot=True, cmap=plt.cm.Reds)\n#plt.show()","22ece2f6":"for i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)\n\nprint(\"Number of Correlated Independent Features:\",len(correlated_features))\nprint(correlated_features)            ","8a388003":"#Correlation with output variable\ncor_target = abs(correlation_matrix[\"SalePrice\"])\n#Selecting low correlated features\nnon_relevant_features = cor_target[cor_target < 0.045]\nnon_relevant_features = [feature for feature,value in non_relevant_features.to_dict().items()]\nprint(\"Number of non correlated features with Output:\",len(non_relevant_features))\nprint(non_relevant_features)","53a27248":"non_correlated_features = non_relevant_features + list(correlated_features)\nprint(\"Total Non Correlated Features are:\",len(non_correlated_features))\nprint(non_correlated_features)","f3a4f336":"X_train_full_dropped = X_train_full.drop(non_correlated_features,axis = 1)\nX_valid_full_dropped = X_valid_full.drop(non_correlated_features,axis = 1)\n#non_correlated_features.remove('SalePrice')\nX_test_full_dropped = X_test_full.drop(non_correlated_features,axis = 1)\nprint(\"After Removing Non Correlated Variables:\\n\")\nprint(\"Size of Train Set:\",X_train_full_dropped.shape)\nprint(\"Size of Validation Set:\",X_valid_full_dropped.shape)\nprint(\"Size of Test Set:\",X_test_full_dropped.shape)","f0cc7d99":"import numpy as np\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\ndef log_transformation(dataset,num_features):\n  data = dataset.copy()\n  for feature in num_features:\n      data[feature]=np.log(data[feature])\n  return data    \nX_train_full = log_transformation(X_train_full_dropped,num_features)\nX_valid_full = log_transformation(X_valid_full_dropped,num_features)\nX_test_full = log_transformation(X_test_full_dropped,num_features)","b57f5afd":"X_test_full.head(10)","d3a1e89b":"## Bring it all together in a function\ncategorical_features=[feature for feature in X_full.columns if X_full[feature].dtype=='O']\ndef rare_category_handling(dataset,categorical_features):\n  data = dataset.copy()\n  for feature in categorical_features:\n      temp=X_train_full.groupby(feature)['SalePrice'].count()\/len(data)\n      temp_df=temp[temp>0.01].index\n      data[feature]=np.where(data[feature].isin(temp_df),data[feature],'Rare_var')\n  return data    \n\n\nX_valid_full = rare_category_handling(X_valid_full,categorical_features)\nX_test_full = rare_category_handling(X_test_full,categorical_features)\nX_train_full = rare_category_handling(X_train_full,categorical_features)\nX_train_full.head(20)  ","add7422b":"# All categorical columns\nobject_cols = [col for col in X_train_full_dropped.columns if X_train_full_dropped[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train_full_dropped[col]) == set(X_valid_full_dropped[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","120505b1":"from sklearn.preprocessing import LabelEncoder\ndef label_encoding(X_train,X_valid,X_test,good_label_cols,bad_label_cols):\n  \n# Drop categorical columns that will not be encoded\n  label_X_train = X_train.drop(bad_label_cols, axis=1)\n  label_X_valid = X_valid.drop(bad_label_cols, axis=1)\n  label_X_test  = X_test.drop(bad_label_cols, axis=1)\n\n  # Apply label encoder only to good_label_cols \n  \n  l_encoder = LabelEncoder()\n  for col in good_label_cols:\n      label_X_train[col] = l_encoder.fit_transform(X_train[col])\n      label_X_valid[col] = l_encoder.transform(X_valid[col])\n      label_X_test[col] = l_encoder.fit_transform(X_test[col])\n      \n  return label_X_train,label_X_valid,label_X_test   \n","3253b650":"X_t_full,X_va_full,X_tes_full = label_encoding(X_train_full_dropped,X_valid_full_dropped,X_test_full_dropped,good_label_cols,bad_label_cols)\nprint(\"After Label Encoding:\\n\")\nprint(\"Size of Train Set:\",X_t_full.shape)\nprint(\"Size of Validation Set:\",X_va_full.shape)\nprint(\"Size of Test Set:\",X_tes_full.shape)","f85f7fa6":"# All categorical columns\nobject_cols = [col for col in X_train_full_dropped.columns if X_train_full_dropped[col].dtype == \"object\"]\n\n# Taking the columns with low cardinality\nlow_cardinality_cols = [cname for cname in X_train_full_dropped.columns if X_train_full_dropped[cname].nunique() < 10 and\n                        X_train_full_dropped[cname].dtype =='object']\n\nhigh_cardinality_cols = [cname for cname in X_train_full_dropped.columns if X_train_full_dropped[cname].nunique() >10 and\n                        X_train_full_dropped[cname].dtype =='object']\n                        \nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","70d55a3b":"from sklearn.preprocessing import OneHotEncoder\n\ndef onehot_encoding(X_train,X_valid,X_test,low_cardinality_cols,object_cols):\n  OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n  OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[low_cardinality_cols]))\n  OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[low_cardinality_cols]))\n  OH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[low_cardinality_cols]))\n\n  # One-hot encoding removed index; put it back\n  OH_cols_train.index = X_train.index\n  OH_cols_valid.index = X_valid.index\n  OH_cols_test.index = X_test.index\n\n  # Remove categorical columns (will replace with one-hot encoding)\n  num_X_train = X_train.drop(object_cols, axis=1)\n  num_X_valid = X_valid.drop(object_cols, axis=1)\n  num_X_test = X_test.drop(object_cols, axis=1)\n\n  # Add one-hot encoded columns to numerical features\n  OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n  OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n  OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\n  return  OH_X_train,OH_X_valid,OH_X_test\n\n","a729b533":"X_t_full,X_va_full,X_tes_full = onehot_encoding(X_train_full_dropped,X_valid_full_dropped,X_test_full_dropped,low_cardinality_cols,object_cols)\nprint(\"After One Hot Encoding:\\n\")\nprint(\"Size of Train Set:\",X_t_full.shape)\nprint(\"Size of Validation Set:\",X_va_full.shape)\nprint(\"Size of Test Set:\",X_tes_full.shape)","d7bcc431":"X_t_full.head()","e00789ef":"scaling_feature=[feature for feature in X_train_full.columns if feature not in ['SalePrice'] ]\nlen(scaling_feature)","585e40c4":"from sklearn.preprocessing import MinMaxScaler\nfeature_scale=[feature for feature in X_t_full.columns if feature not in ['SalePrice']]\nscaler=MinMaxScaler()\nscaler.fit(X_t_full[feature_scale])\n# Bring it all together in a function\ndef feature_scaling(dataset,feature_scale):\n  data = dataset.copy()\n  scaled_data = pd.concat([data[['SalePrice']].reset_index(drop=True),\n                    pd.DataFrame(scaler.transform(data[feature_scale]), columns=feature_scale)],\n                    axis=1)\n  return scaled_data","b35c5b70":"X_train_scaled = feature_scaling(X_t_full,feature_scale)\nX_valid_scaled = feature_scaling(X_va_full,feature_scale)","3fb48ee6":"X_tes_scaled = pd.DataFrame(scaler.transform(X_tes_full[feature_scale]), columns=feature_scale,index=X_tes_full.index)\nX_tes_scaled","6c997f05":"X_tes_full.isnull().sum()","6cfc4f73":"## for feature slection\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","02dec6d4":"y_train=X_t_full[['SalePrice']]\n## drop dependent feature from dataset\nX_train=X_t_full.drop(['SalePrice'],axis=1)","9639708b":"### Apply Feature Selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.1, random_state=0)) # remember to set the seed, the random state in this function\nfeature_sel_model.fit(X_train, y_train)\n","13050fb0":"feature_sel_model.get_support()","db120304":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = X_train.columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n    np.sum(feature_sel_model.estimator_.coef_ == 0)))","8bc6f930":"selected_feat","d93b53e4":"X_train=X_train[selected_feat]\nX_train.head()\n","1c1d347b":"X_valid = X_va_full[selected_feat]\ny_valid = X_va_full[['SalePrice']]","5984f9b5":"# this function will determine the appropriate n_estimator\ndef find_estimator(n_estimator):\n  model =  XGBRegressor(n_estimators=n_estimator,learning_rate=0.05,random_state=0)\n                    \n  scores = -1 * cross_val_score(model, X_train, y_train,\n                                  cv=5,\n                                  scoring='neg_mean_absolute_error')\n  return scores.mean()\n","6106d55e":"results = {i:find_estimator(i) for i in [700,705,710,715,720,725,730,735,740,745,750]}","04026547":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.plot(list(results.keys()), list(results.values()))\nplt.show()\n\nprint(\"Optimum estimator number:{}\".format(min(results, key= results.get )))","0412b8ab":"y_train = np.log(y_train)","78f9de57":"y_train = X_t_full['SalePrice']\ny_valid = X_va_full['SalePrice']\nX_train = X_t_full.drop('SalePrice',axis =1)\nX_valid = X_va_full.drop('SalePrice',axis = 1)\n#X_train = X_t_full\n#X_valid = X_va_full\n\n","37630771":"X_train.head()","68a26562":"# Final Model\nmodel = XGBRegressor(n_estimators=600,learning_rate=0.05,random_state=0)\n\n# Preprocessing of training data, fit model \nmodel.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = model.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","87ac02f0":"X_train.head()","22fc8111":"# Submission\npreds_test = model.predict(X_tes_full)\noutput = pd.DataFrame({'Id': X_tes_full.index,\n                       'SalePrice': preds_test})\noutput.to_csv(saved_file_path, index=False)","734761e6":"**Findings:**\n\n1. Sales Price exponentially increase with Overall Quality\n2. Sales Price also increase as Full Bath increases\n\n","765c88da":"**Dropped columns those have maximum nan values in test set**","75ede783":"####Numerical Features","651e173e":"##Temporal Variables\nFrom the dataset we have four datetime variables and we have extract information from these variables.","c6170087":"#Numerical Featurs","98324a2e":"##Categorical Features","af315141":"**Remove these features from all the distributions set** ","2c7ab4ee":"**Since there are lot of missing values in our dataset a relationship have to be established between these missing values with our target price.** ","04d70af0":"##Feature Scaling","a01aa102":"**We can not perform scaler fit in all the set. Only fit and transform the train data and all the other data only be transformed.**","a9cbf33a":"**When we have continous features we must need to use logarithmic distribution for plotting -**","5739f24e":"### Label Encoding","6722ba8c":"**From the relation plot it is clearly visible that where we have maximum number of nan values at that time the sale price is also maximum. So we have to do something.**","92ed96c5":"##Handling Rare Categorical Variables\n**We will remove categorical variables that are present less than 1% of the observations**","ac93b1e8":"#Feature Engineering","213f9978":"##Libraries and Data Importing","72b6008c":"#Exploratory Data Analysis","10b33f6b":"##Numerical Variables (Normalization)\n\n**Since, some of the numerical variables are skewed thus we have to perform log normal distribution on those features. Performed the operation on those features those doesn't have any `0` value.**","18eb194b":"**we will loop through all the columns in the `correlation_matrix` and will add the columns with a correlation value of `0.8 `to the correlated_features set as shown below.**","bf6800be":"##Model Selection","8c24be4c":"###Missing Values Handling ","1eb79477":"##Outliers Removal Using IQR","a20dc822":"In this plots the black dot points are actually outliers.","f4318a4a":"**Try to find a relation between the year the house was sold and the sale price**","3e0446a8":"##Feature Selection","988d0582":"**using only features_nan is not seem correct as we are only taken the train data feattures that have nan values .**","f3c9f542":"###Outliers Detection\n**outliers can only be checked on continous variables and it can be done by using box plot**","aa21be50":"**Before doing Lable Encoding it is good to know in which columns we can perform the label Encoding and in which columns we ca not.**","b2dcfe97":"####Categorical Features","dcc8fce8":"##One Hot Encoding","8062ca2e":"###Continous Variable","30c0de73":"The plot shows as the the year increasing the price goes down but it seems bit confusing.","9cbd9de3":"###Feature Selection Using Correlation"}}