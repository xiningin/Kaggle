{"cell_type":{"a79265e4":"code","c8285d10":"code","e738b735":"code","6ff84115":"code","ac78cd3b":"code","7dfcb343":"code","430732ca":"code","e77e4b23":"code","7102ab57":"code","c32dac1b":"code","57164f1c":"code","04ffc497":"code","222aed41":"code","29f279c2":"code","64e14590":"code","edf1de38":"code","751f0a10":"code","ef13289c":"code","c36cf1ff":"code","497d3b5d":"code","ab37fec2":"code","f87dfdae":"code","59c69af2":"code","f4aa8dfe":"markdown","9b731c23":"markdown","d5ecfa88":"markdown","bd2c0e84":"markdown","dd7f399d":"markdown","3e7c7438":"markdown","5be4894e":"markdown","7612bfc2":"markdown","92d11da1":"markdown","ed4feb08":"markdown","b5d26d73":"markdown","9c41cc8f":"markdown","0e3d41fd":"markdown","09706bdd":"markdown","50db04e0":"markdown","379880fc":"markdown","91523f01":"markdown","d6e61a7a":"markdown","4eb530d3":"markdown","2fa4ad50":"markdown","b2d55278":"markdown","a219f682":"markdown","74ce1be0":"markdown","6e8dd2b8":"markdown","86cd6e65":"markdown","50391ea3":"markdown","4da3abc8":"markdown","9c14ece3":"markdown","04a329fd":"markdown","ed459394":"markdown","c07c9da8":"markdown","a684279e":"markdown","19454390":"markdown","0567ecc3":"markdown","0921fca3":"markdown"},"source":{"a79265e4":"!pip install -U spacy","c8285d10":"!python -m spacy download en","e738b735":"# importing the spacy library and creating the spacy object\nimport spacy\nspacy_obj = spacy.load('en_core_web_sm')","6ff84115":"spacy_obj.pipeline","ac78cd3b":"spacy_obj.pipe_names","7dfcb343":"# Reading a text in spaCy\ntext = '''The Taj Mahal was constructed under the reign and guidance of the famous Mughal Emperor Shah Jahan. This monument was built as an ode to his beautiful wife Mumtaz mahal and the love between them. The construction of Taj Mahal started in the year of 1632.'''\n\ndoc = spacy_obj(text)","430732ca":"print(doc)","e77e4b23":"for sentences in doc.sents:\n    print(sentences)","7102ab57":"for token in doc:\n    print(token.text)","c32dac1b":"# Print individual tokens by using the slicing notation:\nprint(doc[0])\nprint(doc[1])\nprint(doc[10])","57164f1c":"test_text = spacy_obj(\"Ram said to John, \\\"I'll be travelling to U.S.A. next week.\\\"\")\nfor token in test_text:\n    print(token.text)          ","04ffc497":"print(doc[0:7])\nprint(doc[7:15])\nprint(doc[15:21])","222aed41":"print(doc[0].is_sent_start)\nprint(doc[1].is_sent_start)","29f279c2":"for token in doc:\n    print(token.text,\"------->\",token.pos_, \"------->\",token.lemma_)","64e14590":"print(len(spacy_obj.Defaults.stop_words))\nprint(spacy_obj.Defaults.stop_words)","edf1de38":"textual_data = spacy_obj(\"Hello, My name is Saurabh Gupta. Hmmm! Btw I'm writing this notebook. Hmmm! My hobbies is to write blogs and read books. I work as a freelancer also.\")\nprint('Original data is: \\n',textual_data)\n\nall_stopwords = spacy_obj.Defaults.stop_words\n\n# Defining a remove stopwords function to remove all the stopwords from the text\ndef remove_stopwords(data):\n    # Creating an empty list that will store all individuals tokens\n    all_tokens =[]\n\n    for token in textual_data:\n        all_tokens.append(token.text.lower())\n\n    # printing all the tokens\n    print(\"\\nOriginal token list: \\n\",all_tokens)\n\n\n    tokens_left = [word for word in all_tokens if not word in all_stopwords]\n    print(\"\\nToken list after stop word removal:\\n\",tokens_left)\n    #text_data_tokens\n\n    refined_textual_data = (' ').join(tokens_left)\n    \n    return refined_textual_data \n\nprint(\"\\nActual data after stop word removal is : \\n\",remove_stopwords(textual_data))","751f0a10":"spacy_obj.Defaults.stop_words.add('hmmm')\nspacy_obj.vocab['hmmm'].is_stop = True\nspacy_obj.Defaults.stop_words.add('btw')\nspacy_obj.vocab['btw'].is_stop = True\n\n# Verifying that our custom stopwords are added into the default list\nprint(len(spacy_obj.Defaults.stop_words))\n\n# Calling the remove_stopwords function again on the textual_data\nprint(\"\\nActual data after stop word removal is : \\n\",remove_stopwords(textual_data))","ef13289c":"spacy_obj.Defaults.stop_words.remove('hmmm')\nspacy_obj.vocab['hmmm'].is_stop = False\nspacy_obj.Defaults.stop_words.remove('btw')\nspacy_obj.vocab['btw'].is_stop = False\n\n# Verifying that our custom stopwords we added are removed from the default list\nprint(len(spacy_obj.Defaults.stop_words))","c36cf1ff":"text_1 = \"Ram is a tall boy. He loves playing basketball.\"\n\ndoc_1 = spacy_obj(text_1)\n\nfor token in doc_1:\n    print( \"Token '\"+token.text + \"'  its coarse tag is:   \"+ token.pos_ + \"  and it's fine-grained POS tag is: \" + token.tag_)","497d3b5d":"print(spacy.explain('ADP'))\nprint(spacy.explain('NNP'))","ab37fec2":"# Printing the hash value of both the coarse POS tag and fine-grained POS tag for the very first token in doc_1.\n\nprint(doc_1[0].text + ' has coarse POS hash value: ' + str(doc_1[0].pos))\nprint(doc_1[0].text + ' has fine-grained POS hash value: '+ str(doc_1[0].tag))","f87dfdae":"sent_1 = spacy_obj('Ram loves to read books on Computer Vision.')\nsent_2 = spacy_obj('Ram read a book yesterday.')\n\nword_1 = sent_1[3] # Assigning word_1 = read\nword_2 = sent_2[1] # Assigning word_2 = read\n\n#printing POS tags for read token in the first sentence\nprint(word_1.text,word_1.pos_, word_1.tag_, spacy.explain(word_1.tag_))\n\n#printing POS tags for read token in the second sentence\nprint(word_2.text,word_2.pos_, word_2.tag_, spacy.explain(word_2.tag_))","59c69af2":"## Visualizing the Parts Of Speech using the displaCy\n\nfrom spacy import displacy\n\ndata = spacy_obj('Ram loves to read books on Computer Vision.')\ndisplacy.render(data, style='dep', jupyter=True, options={'distance': 100})","f4aa8dfe":"If we analyze the result of the tagging, we can see spaCy was able to identify correct form of the common word in both the sentence correctly. It is possible because it uses morphology to determine the correct essence of the text. ","9b731c23":"\nSee, how beautifully spaCy tags each token to its approriate parts-of-speech. Also, we can use the spacy.explain() function to see the full information about the tag used.","d5ecfa88":"## 1. Tokenization\n\nThe first step in the preprocessing pipeline is of tokenizing the text present in the document into small tokens (either into sentences or in individual words). spaCy library is really efficient at tokenization and it intelligetly identifies words, sentences, punctuation marks, etc. to tokenize given text.\n\n**(A) Tokenization into sentences**\n\nWe can make use of the 'sents' attribute to tokenize the text document into sentences. This could be done as:","bd2c0e84":"Thus, we can notice all the additional stopwords can also be removed after adding them to the by default stopword set.","dd7f399d":"Also, we want to disable few pipeline compnents on my own. So, we can use the **disable_pipes()** method and we ca pass the name of the components that we want to disable in form of comma -separated values. This can be done as:\n\n**spacy_obj.disable_pipes('attribute_ruler','tagger')**\n\nOuput: ['attribute_ruler', 'tagger']\n\nNow, we can see the list of all active pipeline components, To verify that our task was successful we can again check out for the names of the pipeline components as:\n\n**spacy_obj.pipe_names**\n\nOutput: ['tok2vec', 'parser', 'ner', 'lemmatizer']","3e7c7438":"There are two very important additional components that you should be aware about before proceeding forward. These are as follows:\n\n**1. Statistical models**\n\nThese models are the main power engines behind spaCy and they are used to perform the NLP main NLP tasks like named entity recognition, parts-of-speech tagging, dependency parsing, etc.\n\n(a) en_core_web_sm\n\n(b) en_core_web_md\n\n(c) en_core_web_lg\n\n(d) en_core_web_trf\n\nYou can read more about these statistical models at: https:\/\/spacy.io\/models\/en\n\n**2. Processing pipeline**\n\nTo perform operations on a piece of text data, we need to pass it through a spaCy object. When the data is passed through the object it undergoes through a pipeline of several processes that are shown in the figure below:\n![Screenshot (191).png](attachment:dc3f86f4-703d-4d6d-ba11-09cc2879a488.png)\n\nWe will discuss the operations performed in this pipeline later in the same notebook.","5be4894e":"## 2. Lemmatization\n\nLemmatization is the process of grouping inflected words from a common root word, so that they can be grouped into one single term for analysing purpose. It's main emphasis is to look beyond the scope of word reduction. It considers the language's full vocabulary so that it can apply morphological analysis on words.\n\nThe point to be noted is that spaCy library does not have stemming feature, because it prefers lemmitization as it is considerd to be more informative than stemming.","7612bfc2":"## 4.  Parts of Speech tagging (POS tagging)\n\nThe next step in the preprocessing pipeline after tokenisation is to assign appropriate parts-of-speech to each token. This step can be useful in many NLP tasks for information extraction, feature engineering, language understanding, etc. As discussed earlier spaCy library is already trained with statistical models that allows it to achieve the objective of POS tagging efficiently. The statistical model contains binary data and is already trained over a lot of examples that enables it to make generalized predictions.\n\nHowever, predicting the correct parts-of-speech for a word is a really challenging task. It is because of the reasons that are listed below:\n\n- (i) It is not easy to make a machine understand and process the raw text.\n- (ii) There are some words which give different meaning when they are used in different context.\n- (iii) There are cases possible when relatively different words can give same meaning.\n\nIn this way, it is not easier for a machine\/model to understand the context of the text and infer proper meaning out of it. To overcome this, we can add\/use linguistic models to train our machine\/model and add useful knowledge to it so that our model can learn from it and give better accurate results.\n\nspaCy alreads is trained on statistical models that fulfills its need of adding useful information to it.","92d11da1":"- spaCy identifies each quotation mark, commas, question mark and other punctuation marks that are present in form of prefix, suffix, infix and separates them into an indvidual token. (In above example, I'll be was separated into 'I & 'll' two different tokens.)\n- Punctuation marks that exists as part of a known abbereviation will not be separated. (In above example, U.S.A is kept as U.S.A.)\n- Also, punctuation marks used as infixes will be exempted from tokenisation in cases of email address, website or some numerical figures. ","ed4feb08":"It should be noted down that spaCy encodes all the strings token to a unique hash value in order to reduce memory usage and improve its efficiency. So, we can use the **pos** tag and **tag** tag to view the hash values. By using these tags, we can get the hash values of he corresponsding token. Also, these are the short hand notations of the two tags we read earlier that are **pos_** and **tag_**. We add an **underscore (_)** sign with these tag names so that we can get the hash values in readable string format. ","b5d26d73":"To check whether a particular word is the starting of a sentence we can use the 'is_sent_start' attribute with the doc object. It returns a Boolean value, True in case the word is the starting of any sentence otherwise it returns False.","9c41cc8f":"**II. Installing the spaCy library onto our system.**\n\n(A) To install spaCy library on our system we can use the Anaconda command promt and run either of these two commands:\n\n**pip install -U spacy**\n\n**conda install -c conda-forge spacy**\n\n(B) If we directly want to install the library via the Jupyter Notebook, we can use the command:\n\n**!pip install -U spacy**\n\nWe will be using English language to perform our NLP tasks, so we all need to install the language model specific to English language which can be done by using the following command:\n\n**Python -m spacy download en** (when using the Anaconda command prompt) or\n\n**!Python -m spacy download en** (when directly insalling on Jupyter Notebook)\n\nIt will successfully get installed.\n\n\n","0e3d41fd":"![0001.jpg](attachment:96ecf53d-a7e7-4d13-aeb0-3b1ed7856062.jpg)","09706bdd":"**Removing a stopword from the stopword set**\n\nIf we want to remove some stopwords from the by default stopword set, we can also do it. It can be done as follows:","50db04e0":"Now that we have gained some knowledge about various POS tags available in spaCy. Let's get into skin of it by working on some hands-on examples. For the text that we read earlier, we will be generating the POS tags for each token in that text data. We can make use of the **pos_ tag** and **tag_ tag** along with each token to view its respective coarse and fine-grained POS tag.  ","379880fc":"Let's begin with setting up a virtual environment and installation of the spaCy library onto our system.**\n\nNote - I'm here explaining how to install spaCy on the Anaconda application. So, you need to have Anaconda installed on your system.\n\n**I. Setting up a virtual environment**\n\n1. We will start with creating a virtual environment on our computer. For this, we need to run the following commandd into the Anaconda prompt:\n\n**conda create -n [Name_of_environment] [Python_Version]**\n\nFor Eg: **conda create -n SPACY Python=3.9**\n\nhere, SPACY is the name of the newly created environment and Python version we are installing is 3.9\n\n2. Now, we can check out the list of all available environments on our system using the command:\n\n**conda info -e**\n\n3. We need to activate our newly created environment using the command:\n\n**conda activate SPACY**\n\nFor using Jupyter Notebook in Anaconda for our newly created virtual environment, we have to manually install a IPython kernel that can be installed after activating our virtual environment (we did it in the last step). We sholud run the command:\n\n**pip install --user ipykernel**\n\n4. Now, we need to add our newly created virtual environment to the Jupyter\n\n**python -m ipykernell install --user --name=SPACY**","91523f01":"## Features of spaCy","d6e61a7a":"**Adding a custom stopword for removal from the text**\n\nSuppose there are certain cases where our text data consists of some irrelevant words that are of no use to us and we want to remove those words from the text data and these words do not exist in the in-built stopword set. So, for such cases spaCy gives us the freedom to add our custom stopwords to the default set.\n\nIn our last example, we can observe after stopword removal there are still words such as 'Hmmm', 'Btw', that are of no use and we can remove them from our text too. This can be done as follows:","4eb530d3":"1. It supports 64+ languages and has been trained on 55 pipelines for 17 different languages.\n2. It operates flawlessly with other frameworks such as PyTorch, Tensorflow, Gensim, NLTK, Scikit-Learn and other Python libraries.\n3. The library is super-fast and efficient and productive to use.\n4. It allows easy model packaging, deployment and workflow management.\n5. The ecosystem around spaCy has really developed a lot in the past 6 years. We can choose a variety of plugins to integrate in our machine learning stack and build custom component and workflows.  \n6. spaCy gives us the power to build lingusitically sophisticated statistical models that can be based on variety of NLP problems.\n7. We can perform various NLP tasks such as named entity recognition, parts-of-speech tagging, dependency parsing, text classification, lemmitization, sentance segmentation, morphological analysis, entity linking and much more with the help of the spaCy library.\n8. The library is easy to install and is robust in nature. It gives us rigourously evaluated accuracy.","2fa4ad50":"## Everything you need to know about the spaCy library to use it for NLP","b2d55278":"**(B) Tokenization into individual words**\n\nThis divides the entire sentence into word by word including the punctuation marks and escape sequences. This could be done as:","a219f682":"It is possible that the same words in two different sentences can give different meanings. To determine the real essence of each sentence spaCy library uses something known as morphology.\n\nLet's have two sentences and analyze their results after doing POS tagging:","74ce1be0":"**Removal of stopwards from the text data**\n","6e8dd2b8":"#### More concepts like Named Entity Recognition, Pattern Matching will be covered in the next part of the notebook. Do follow and upvote if you like this notebook.","86cd6e65":"![nlp-python-spacy.jpg](attachment:985764b6-5a17-472a-a0bf-86ce2554b234.jpg)\n\n## What is spaCy?\nSpacy is an open-source Python library that has become the most goto libraries for performing natural language processing tasks in the last 6 years since its realease in the year 2015. It was developed by Matthew Honnibal at Explosion AI. Since, it is implemented using Cython programming language. It was built for production use-cases and can help an individual to build applications that can process large volumes of text and also understand it. We can use the spaCy library to preprocess large volume of textual data for accomplishing our deep learning purposes. It can also be used to build information processing and natural language understanding systems.","50391ea3":"**(C) Spanning\/Slicing of words in the text**\nWe can slice the text document as per our requirements using the slicing notation that we use to slice other sequences like lists and strings.","4da3abc8":"We can use the **pipeline** attribute associated with the 'spacy_obj' to get information about all the pipeline components. In case, we just want the name of the pipeline component, we can use the **pipe_names** attribute associated woth the 'spacy_obj' object. The code for this is shown below:","9c14ece3":"## 3. Stopwards in spaCy\n\nThere are many words in the English dictionary that are very common and are of no important use to us for finding some useful information from them. These can be words like- is, am ,are, a, an, the, etc. If we keep them as it is in the text, they will tend to increase the vocabulary size and making use of these large size vocab to train our model can be really time taking and also our model can become inefficient as these gie no useful information.\n\n- spaCy holds an in-built set of 326 stopwords in the English by default, that are removed from our text document in the preproceesing pipeline. These stopwards are-","04a329fd":"After downloading the required spacy library and the statistical model, we are now good to go for our further NLP tasks. We can begin with importing the installed spaCy library.","ed459394":"**(D) Assignment of tokens is not allowed in spaCy.**\n\ndoc[0] = doc[1], willl throw an error.","c07c9da8":"Points to be noted:\n- Lemma for a particular word is determined by keeping in mind the part-of-speech. To notice in the above example,we have two tokens constructed and construction. In the first case, constructed is a Verb, so its lemmatized form is given as construct, but the token construction is treated as a Noun so it is still given as construction in the lemmatized form. Thus, we can verify Lemmitization of a word also depends upon the parts-of-speech.\n\n- Also, Lemmatization doesn't reduce words to their most basic synonym.","a684279e":"spaCy follows a specific rule for tokenization of text.\n\n1. Initially, it starts preprocessing the raw text into tokens from left to right based on whitespaces.\n\n2. Then, it performs splitting tokens into sub-tokens by performing two different checks that are:\n- **(a) Exception rule check:** Punctuation marks in between tokens (in words such as U.S.A) are looked over and are left untokenised.\n- **(b) Prefix-Suffix and Infix check:** Punctuation marks like commas, hyphens, quotation marks, periods, etc. are identified and made as a separate token.\n\nThis rule checking is applied iteratively on the tokens from left to right. Thus, with the help of these two intuitive checking methodology spaCy tokenizes words inteliigently. For more better understanding, have a look at the below given code:","19454390":"## 5. Dependency Parsing\n\nEach sentence in a textual data follows a particular structure that gives proper meaning to the sentence. Dependency Parsing is a metod using which we can extract the dependencies of a sentence that helps to represent the grammatical structure of the sentence. The dependency usually exists between the root word of the sentence and all the other existing words. Most of the times the 'Verb' inside the sentence is treated as a root word.\n\nThe dependency amongst the words in a sentence can be represented using a directed graph. Various components that a directed graph represents are listed below:\n\n- Each token (word) is represented using a node.\n- The existing grammatical relation between two tokens is represented using edges.","0567ecc3":"Now, that we have known this much information about the spaCy library. We should see some implementation of how this library works.\n\nIn this kernel, we will start with installing the spaCy library.","0921fca3":"### Different types of POS tags available in spaCy\n\nThere are two types of POS tags available in spaCy that are-\n\n**(a) coarse POS tags**\n\nThese are ordinary POS tags that we know. Each token is assigned a its own coarse POS tag. Complete list of coarse POS tags can be seen on the official documetation page. Some of them are given below:\n\n![Screenshot (194).png](attachment:da089c66-c2ca-4aab-a3f4-e1c1d01d3fa4.png)\n\n**(b) fine-grained tags**\nIn this each token is assigned a more detailed POS tag based on the morphology. Given below is a list of few fine-gained POS tags. Complete list of fine-grained POS tags can be seen on the official documetation page\n\n![Screenshot (195).png](attachment:3dae5772-5d8b-4cea-a069-480c80030640.png)"}}