{"cell_type":{"910f490c":"code","e32e1374":"code","f9885e1d":"code","642b287e":"code","25e7df5d":"code","73878fff":"code","8838f56d":"code","0e29020c":"code","2c5c867c":"code","818c923d":"code","2c874155":"code","b22e67c7":"code","6f297d47":"code","af58f71c":"code","e16b08d8":"code","98d97608":"code","a5208fb0":"code","1e462e23":"code","5593fafd":"code","e7d74c29":"code","7dcd2afe":"code","508db34b":"code","4c8ac67d":"code","a3f4748b":"code","3ff89e68":"code","271691d0":"code","4f38a84e":"code","933871f4":"code","0e59727e":"code","9b79f18e":"code","644c30ee":"code","97791cd7":"code","14acad7b":"code","09d8beef":"code","78b43a88":"code","e5e7967f":"code","d7988ecd":"code","1b56f1a6":"code","30c34c3f":"code","aa061b1e":"code","6ee75e56":"code","7f6cf1bc":"code","a28ffbe0":"code","2aba72d3":"code","c2ba8b15":"code","008c782d":"code","1cb987c9":"code","bb5402da":"code","fd16e7ee":"code","484fb521":"code","644493d1":"code","4d913a04":"code","43505301":"code","ee595206":"code","7a0c0eac":"code","93920f4a":"code","f8ad9cd4":"code","7fd53013":"code","bc140881":"code","23054cb4":"code","a1a40fce":"markdown","56ea1a70":"markdown","192725bf":"markdown","1cf7bfe1":"markdown","46d037f9":"markdown","0301041f":"markdown","2cf22f2c":"markdown","ecde0841":"markdown","5316e1f7":"markdown","9ae24623":"markdown","f3db8a7e":"markdown","de6e13d0":"markdown","faa24f3c":"markdown","b23f8b4a":"markdown","140127df":"markdown","879a3cb6":"markdown","8640226e":"markdown","664c1eae":"markdown","41fe5aa4":"markdown","cee66621":"markdown","a6cb8f07":"markdown","2addba49":"markdown","fc98ce9a":"markdown","1600d75d":"markdown","a8f79019":"markdown","479377e2":"markdown","a846d567":"markdown","d4fa3bd9":"markdown","14ee3756":"markdown","67951a67":"markdown","c9671469":"markdown","130b90ff":"markdown","a58bc48d":"markdown","76ec8e14":"markdown","6c289842":"markdown","ccfc676a":"markdown","49f204ff":"markdown","b1d24f5c":"markdown","ada71565":"markdown","4e0dac81":"markdown","9a6a337f":"markdown","ea0a4c3e":"markdown"},"source":{"910f490c":"from IPython.core.display import display, HTML, Javascript\n\ndef nb():\n    styles = open(\"..\/input\/intermediate-notebooks-data\/custom-orange.css\", \"r\").read()\n    return HTML(\"<style>\"+styles+\"<\/style>\")\nnb()","e32e1374":"import os\nimport numpy as np \nimport pandas as pd \nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cuml, cudf, cupy\nimport nltk\nimport tensorflow as tf\nimport wandb\n\nfrom pandas import DataFrame\nfrom sklearn.feature_extraction.text import CountVectorizer as CV\nfrom nltk.corpus import stopwords\nfrom cuml.feature_extraction.text import CountVectorizer\nfrom cuml.neighbors import NearestNeighbors\nfrom colorama import Fore, Back, Style\nfrom wordcloud import WordCloud,STOPWORDS\nfrom tensorflow.keras.applications import ResNet101\nfrom PIL import Image\n\nnltk.download('stopwords')\n\n# colored output\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA","f9885e1d":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"api_key\")\n\nos.environ[\"WANDB_SILENT\"] = \"true\"","642b287e":"! wandb login $api_key","25e7df5d":"train_df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")","73878fff":"train_df.head()","8838f56d":"test_df","0e29020c":"# specifying directory paths\n\ntrain_jpg_directory = '..\/input\/shopee-product-matching\/train_images\/'\ntest_jpg_directory = '..\/input\/shopee-product-matching\/test_images\/'","2c5c867c":"# function to get image paths from train and test directory\n\ndef getImagePaths(path):\n    image_names = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            fullpath = os.path.join(dirname, filename)\n            image_names.append(fullpath)\n    return image_names","818c923d":"train_images_path = getImagePaths(train_jpg_directory)\ntest_images_path = getImagePaths(test_jpg_directory)","2c874155":"print(f\"{y_}Number of train images: {g_} {len(train_images_path)}\\n\")\nprint(f\"{y_}Number of test images: {g_} {len(test_images_path)}\\n\")","b22e67c7":"def getShape(images_paths):\n    shape = cv2.imread(images_paths[0]).shape\n    for image_path in images_paths:\n        image_shape=cv2.imread(image_path).shape\n        if (image_shape!=shape):\n            return \"Different image shape\"\n        else:\n            return \"Same image shape \" + str(shape)","6f297d47":"getShape(train_images_path)","af58f71c":"getShape(test_images_path)","e16b08d8":"# function to display multiple images\n\ndef display_multiple_img(images_paths, rows, cols,title):\n    \n    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(16,8))\n    plt.suptitle(title, fontsize=20)\n    for ind,image_path in enumerate(images_paths):\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \n        try:\n            ax.ravel()[ind].imshow(image)\n            ax.ravel()[ind].set_axis_off()\n        except:\n            continue;\n    plt.tight_layout()\n    plt.show()","98d97608":"display_multiple_img(train_images_path[0:25], 5, 5,\"Train images\")","a5208fb0":"display_multiple_img(test_images_path, 1, 3,\"Test images\")","1e462e23":"def styling():\n    for spine in plt.gca().spines.values():\n        spine.set_visible(False)\n        plt.xticks([])\n        plt.yticks([])","5593fafd":"def hist(image_path):\n    plt.figure(figsize=(16, 3))\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    \n    plt.subplot(1, 5, 1)\n    plt.imshow(img)\n    styling()\n    \n    custom_colors = [\"#ef233c\", \"#76da71\", \"#2667ff\",\"#aea3b0\"]\n    labels = ['Red Channel', 'Green Channel', 'Blue Channel','Total']\n    \n    for i in range(1,4):\n        plt.subplot(1, 5, i+1)\n        plt.hist(img[:, :, i-1].reshape(-1),bins=64,color=custom_colors[i-1],alpha = 0.6)\n        plt.xlabel(labels[i-1],fontsize=10)\n        styling()\n        \n    plt.subplot(1, 5, 5)\n    plt.hist(img.reshape(-1),bins=128,color=custom_colors[3],alpha = 0.6)\n    plt.xlabel(labels[3],fontsize=10)\n    styling()\n    plt.show()","e7d74c29":"def display_hist(images_paths):\n        for ind,image_path in enumerate(images_paths):\n            if (ind<6):\n                hist(image_path)","7dcd2afe":"display_hist(train_images_path[5:10])","508db34b":"display_hist(test_images_path)","4c8ac67d":"# initializing the run\nrun = wandb.init(project=\"shopee\",\n                 job_type=\"upload\",\n                 config={\n                     \"num_examples\" : 8\n                 })\n\n# creating an artifact \nartifact = wandb.Artifact(name=\"histograms\", type=\"raw_data\")\n\n# setting up a WandB Table object to hold the dataset\ncolumns=[\"id\", \"raw image\", \"red channel\",\"green channel\",\"blue channel\",\"label\"]\n\ntable = wandb.Table(\n    columns=columns\n)\n\n# filling up the table\nimages_train = [f for f in train_images_path[5:10]]\nimages_test = test_images_path\n\nall_images = images_train + images_test\nlabels = [\"train\",\"train\",\"train\",\"train\",\"train\",\"test\",\"test\",\"test\"]\n\nfor ndx in range(wandb.config.num_examples):\n    img_file = all_images[ndx]\n    train_id = img_file.split(\"\/\")[4].split(\".\")[0]\n    \n    img = cv2.imread(img_file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n\n  # raw image\n    raw_img = wandb.Image(img_file)\n    \n  # plotting histograms \n    wb_color = [\"#ef233c\",\"#76da71\",\"#2667ff\"]\n    def wb_hist(i):\n        plt.figure(figsize=(16, 10))\n        plt.hist(img[:, :, i-1].reshape(-1),bins=64,color=wb_color[i-1],alpha = 0.6)\n        return wandb.Image(plt)\n    red = wb_hist(1)\n    green = wb_hist(2)\n    blue = wb_hist(3)\n    \n    # adding an artifact file\n    artifact.add_file(img_file, os.path.join(\"images\", train_id + \"_train_id.png\"))\n\n  # adding a row to the table\n    row = [train_id, raw_img, red,green,blue,labels[ndx]]\n    table.add_data(*row)\n    \n# adding the table to the artifact\nartifact.add(table, \"raw_examples\")\n    \n# logging the artifact\nrun.log_artifact(artifact)\n\nrun.finish()","a3f4748b":"train_df['label_group'].nunique()","3ff89e68":"train_labels_count = train_df['label_group'].value_counts()\n\n# getting count for most frequent and least frequent label groups\nmost_freq = train_labels_count[train_labels_count == train_labels_count.max()]\nless_freq = train_labels_count[train_labels_count == train_labels_count.min()]\n\n# getting most frequent and least frequent label groups\nm_label = np.unique(train_df['label_group'][train_df['label_group'].isin(most_freq.index)].values)\nl_label = np.unique(train_df['label_group'][train_df['label_group'].isin(less_freq.index)].values)\n\nprint(f\"{m_} Most frequent label group: \", m_label)\nprint(f\"{y_} Less frequent label group: \", l_label)","271691d0":"run = wandb.init(project='shopee', name='count')\n\nmw = train_labels_count.max()\nlw = train_labels_count.min()\nuw = train_df['label_group'].nunique()\n\nwandb.log({'Unique label groups': uw, \n           'Most frequent label groups': mw, \n           'Least frequent label groups': lw})\n\nrun.finish()","4f38a84e":"def path(group,m):\n    PATH = \"..\/input\/shopee-product-matching\/train_images\/\"\n    \n    #label\n    if m=='l':\n        z = train_df['image'][train_df['label_group']==group].values\n    \n    #title\n    if m=='t':\n        z = train_df['image'][train_df['title']==group].values\n   \n    image_names = []\n    for filename in z:\n        fullpath = os.path.join(PATH, filename)\n        image_names.append(fullpath)\n    return image_names","933871f4":"lg = 159351600\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","0e59727e":"lg = 994676122\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","9b79f18e":"lg = 562358068\ndisplay_multiple_img(path(lg,'l'), 3, 3,lg)","644c30ee":"lg = 297977\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","97791cd7":"lg = 887886\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","14acad7b":"lg = 4293276364\ndisplay_multiple_img(path(lg,'l'), 1, 2,lg)","09d8beef":"train_df.shape","78b43a88":"train_df['title'].nunique()","e5e7967f":"t = train_df['title'].value_counts().sort_values(ascending=False).reset_index()\nt.columns = ['title','count']\nt","d7988ecd":"img_title = \"Koko syubbanul muslimin koko azzahir koko baju\"\ndisplay_multiple_img(path(img_title,'t'), 3, 3,img_title)","1b56f1a6":"img_title = \"Baju Koko Pria Gus Azmi Syubbanul Muslimin Kombinasi Hadroh Azzahir Hilw HO187 KEMEJA KOKO PRIA BAJU\"\ndisplay_multiple_img(path(img_title,'t'), 4, 2, img_title)","30c34c3f":"img_title = \"Monde Boromon Cookies 1 tahun+ 120gr\"\ndisplay_multiple_img(path(img_title,'t'), 2, 3, img_title)","aa061b1e":"# color function for the wordcloud\ndef color_wc(word=None,font_size=None,position=None, orientation=None,font_path=None, random_state=None):\n    h = int(360.0 * 21.0 \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(80, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n\n\nrun = wandb.init(project='shopee', job_type='image-visualization',name='wordCloud')\n\nfig = plt.gcf()\nfig.set_size_inches(16, 8)\n\nwc = WordCloud(stopwords=STOPWORDS,background_color=\"white\", contour_width=2, contour_color='orange',width=1500, height=750,color_func=color_wc,max_words=150, max_font_size=256,random_state=42)\nwc.generate(' '.join(train_df['title']))\nfig = plt.imshow(wc, interpolation=\"bilinear\")\nfig = plt.axis('off')\n\nwandb.log({\"wordcloud\": [wandb.Image(plt, caption=\"Wordcloud\")]})\nrun.finish()\n\nrun","6ee75e56":"def get_top_n_words(corpus, n=None):\n    vec = CV().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CV(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CV(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7f6cf1bc":"def plot_bt(x,w,p):\n    common_words = x(train_df['title'], 20)\n    common_words_df = DataFrame (common_words,columns=['word','freq'])\n\n    plt.figure(figsize=(16, 10))\n    sns.barplot(x='freq', y='word', data=common_words_df,palette=p)\n    plt.title(\"Top 20 \"+ w , fontsize=16)\n    plt.xlabel(\"Frequency\", fontsize=14)\n    plt.yticks(fontsize=13)\n    plt.xticks(rotation=45, fontsize=13)\n    plt.ylabel(\"\");\n    return common_words_df","a28ffbe0":"common_words = get_top_n_words(train_df['title'], 20)\ncommon_words_df1 = DataFrame(common_words,columns=['word','freq'])\nplt.figure(figsize=(16, 8))\nax = sns.barplot(x='freq', y='word', data=common_words_df1,palette='Oranges')\n\nplt.title(\"Top 20 unigrams\", fontsize=16)\nplt.xlabel(\"Frequency\", fontsize=14)\nplt.yticks(fontsize=13)\nplt.xticks(rotation=45, fontsize=13)\nplt.ylabel(\"\");\n\ncommon_words_df2 = plot_bt(get_top_n_bigram,\"bigrams\",'BuGn')\ncommon_words_df3 = plot_bt(get_top_n_trigram,\"trigrams\",'RdPu')","2aba72d3":"def plot_wb(df, name, title): \n    run = wandb.init(project='shopee', job_type='image-visualization',name=name)\n\n    labels = df.sort_values('freq', ascending=False).word\n    values = df.sort_values('freq', ascending= False).freq\n    dt = [[label, val] for (label, val) in zip(labels, values)]\n    table = wandb.Table(data=dt, columns = [\"Word\", \"Frequency\"])\n    wandb.log({name : wandb.plot.bar(table, \"Word\", \"Frequency\",title=title)})\n\n    run.finish()\n    \nplot_wb(common_words_df1, \"unigrams\",\"Top 20 unigrams\")\nplot_wb(common_words_df2, \"bigrams\",\"Top 20 bigrams\")\nplot_wb(common_words_df3, \"trigrams\",\"Top 20 trigrams\")","c2ba8b15":"train_df_c = cudf.from_pandas(train_df)","008c782d":"train_df_c","1cb987c9":"STOPWORDS = nltk.corpus.stopwords.words('english')\n\npunctuation = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '\/',  '\\\\', ':', ';', '<', '=', '>',\n           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\t','\\n',\"'\",\",\",'~' , '\u2014']\n\ndef text_preprocessing(input_text, filters=None, stopwords=STOPWORDS):\n    # filter punctuation \n    translation_table = {ord(char): ord(' ') for char in filters}\n    input_text = input_text.str.translate(translation_table)\n    \n    #convert to lower case\n    input_text = input_text.str.lower()\n        \n    # remove stopwords \n    stopwords_gpu = cudf.Series(stopwords)\n    input_text =  input_text.str.replace_tokens(stopwords_gpu, ' ')\n        \n    # normalize spaces\n    input_text = input_text.str.normalize_spaces( )\n    \n    # strip leading and trailing spaces\n    input_text = input_text.str.strip(' ')\n    \n    return input_text\n\ndef preprocess_df(df, col, **kwargs):\n    df[col] = text_preprocessing(df[col], **kwargs)\n    return  df\n\n%time \ndf = preprocess_df(train_df_c,'title', filters=punctuation)\n\ntrain_df_c.head(5)","bb5402da":"train_df_c.to_csv(\"title_preprocessed_dataset.csv\")","fd16e7ee":"# run = wandb.init(project='shopee', name='title_preprocessed')\n\n# artifact = wandb.Artifact('title_preprocessed_dataset', type='dataset')\n\n# add a file to the artifact's contents\n# artifact.add_file(\"title_preprocessed_dataset.csv\")\n\n# save the artifact version to W&B and mark it as the output of this run\n# run.log_artifact(artifact)\n\n# run.finish()","484fb521":"vec = CountVectorizer(stop_words='english', binary=True)\n%time X = vec.fit_transform(train_df_c.title).toarray()","644493d1":"n = 50\nknn = NearestNeighbors(n_neighbors=n)\nknn.fit(X)\ndistances, indices = knn.kneighbors(X)","4d913a04":"for k in range(5):\n    plt.figure(figsize=(20,3))\n    plt.plot(np.arange(50),cupy.asnumpy(distances[k,]),'o-',color='#f48c06')\n    plt.title('Text Distance From Train Row %i to Other Train Rows'%k,fontsize=15, fontweight='bold',horizontalalignment='center',fontfamily='serif')\n    plt.ylabel('Distance to Train Row %i'%k,fontsize=13, fontweight='bold',fontfamily='serif')\n    plt.xlabel('Index Sorted by Distance to Train Row %i'%k,fontsize=13, fontweight='bold',fontfamily='serif')\n    plt.show()\n    \n    print( train_df_c.loc[cupy.asnumpy(indices[k,:10]),['title','label_group']] )","43505301":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, df, img_size=256, batch_size=32, path=train_jpg_directory): \n        self.df = df\n        self.img_size = img_size\n        self.batch_size = batch_size\n        self.path = path\n        self.indexes = np.arange(len(self.df))\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        ct = len(self.df) \/\/ self.batch_size\n        ct += int(((len(self.df)) % self.batch_size)!=0)\n        return ct\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X = self.__data_generation(indexes)\n        return X\n            \n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' \n        X = np.zeros((len(indexes),self.img_size,self.img_size,3),dtype='float32')\n        df = self.df.iloc[indexes]\n        for i,(index,row) in enumerate(df.iterrows()):\n            img = cv2.imread(self.path+row.image)\n            X[i,] = cv2.resize(img,(self.img_size,self.img_size))\n        return X","ee595206":"model = ResNet101(weights='imagenet',include_top=False, pooling='avg', input_shape=None)\ntrain_gen = DataGenerator(train_df, batch_size=128)","7a0c0eac":"# ie = model.predict(train_gen,verbose=1)\n# np.save(\"image_embedding_val.npy\", ie)","93920f4a":"# run = wandb.init(project='shopee', name='image_embedding_val')\n\n# artifact = wandb.Artifact(name='image_embedding_val', type='dataset')\n\n# Add a file to the artifact's contents\n# artifact.add_file(\"image_embedding_val.npy\")\n\n# Save the artifact version to W&B and mark it as the output of this run\n# run.log_artifact(artifact)\n\n# run.finish()","f8ad9cd4":"run = wandb.init()\n\n# query W&B for an artifact and mark it as input to this run\nartifact = run.use_artifact('ruchi798\/shopee\/image_embedding_val:v0', type='dataset')\n\n# download the artifact's contents\nartifact_dir = artifact.download()","7fd53013":"path = os.path.join(artifact_dir, \"image_embedding_val.npy\")\nimg_embeddings = np.load(path)","bc140881":"n = 50\nknn = NearestNeighbors(n_neighbors=n)\nknn.fit(img_embeddings)\ndistances, indices = knn.kneighbors(img_embeddings)","23054cb4":"ROWS=2\nCOLS=4\nfor c in range(75,85):\n    print(\"Cluster \",c)  \n    t = train_df.loc[cupy.asnumpy(indices[c,:8])]   \n    for k in range(ROWS):\n        plt.figure(figsize=(20,5))\n        for j in range(COLS):\n            row = COLS*k + j\n            name = t.iloc[row,1]\n            img = cv2.imread(train_jpg_directory+name)\n            \n            #converting from BGR to RGB\n            img = img[:, :, ::-1]\n            \n            plt.subplot(1,COLS,j+1)\n            plt.axis('off')\n            plt.imshow(img)\n        plt.show()","a1a40fce":"<center><h1>Introduction \ud83d\udcdd<\/h1><\/center>\n\n> \ud83c\udfafGoal: To build a model that predicts which items are the same products\n> \n> As a shopaholic\ud83d\udecd\ufe0f , I admit getting the best deals for products is a very rewarding experience. Scanning through multiple shopping websites to get the perfect deal and keeping an eye on upcoming sales is one manual way to go about.\n> \n> We often find retail companies offering recommendations in which they promote their products in such a way that customers tend to get swayed and pick a similar product that is priced lower. Product matching \ud83d\udccb\ud83d\udccb is one of these strategies wherein a company to offers products at rates that are competitive to the same product sold by another retailer. \n> \n> These matches can be performed automatically with the help of machine learning and that is the goal of this competition. We have been provided with data of **Shopee**, which is the leading e-commerce platform in Southeast Asia and Taiwan. ","56ea1a70":"**Visualizing and querying the dataset** with W&B \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\n[Documentation](https:\/\/docs.wandb.ai\/datasets-and-predictions)","192725bf":"**Logging** the image embeddings as **an artifact**\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\nThis helps me to save on time since the model need not be trained over and over again\ud83e\udd73","1cf7bfe1":"<center><h1>Least frequent label groups \ud83d\udcc9<\/h1><\/center>","46d037f9":"<img src=\"https:\/\/i.imgur.com\/1bEOBR1.png\">","0301041f":"**Logging an image** of the wordcloud of image titles\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","2cf22f2c":"**Logging** the preprocessed title dataset as **an artifact**\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","ecde0841":"\n<center><h3>Pre-processing title \u2702\ufe0f<\/h3><\/center>","5316e1f7":"**Logging a dictionary of custom objects** \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","9ae24623":"<center><h1>Reading csv files \ud83d\udcd6<\/h1><\/center>","f3db8a7e":"<div>\n    <center><img src=\"https:\/\/i.imgur.com\/mqPVRT5.png\"><\/center>\n    <\/div>","de6e13d0":"We have 11014 unique label groups for products.","faa24f3c":"Checking if images in each directory have the same shape","b23f8b4a":"References \ud83d\udcdc\n- [RAPIDS cuML TfidfVectorizer and KNN](https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-tfidfvectorizer-and-knn)\n- [A very detailed explanation for data generation](https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly)\n\nInspiration \ud83d\udca1\n- [Custom Jupyter Notebook Theme with plain CSS](https:\/\/medium.com\/@formigone\/my-first-custom-theme-for-jupyter-notebook-a9c1e69efdfe) \ud83c\udfa8\n\nIllustrations tools \u26a1\n- [Canva](https:\/\/www.canva.com\/en_gb\/) \ud83d\udd8c\ufe0f","140127df":"**Different versions** of the artifacts can be stored in W&B.\n\n**Comparison of any two artifact versions** in the table is possible. \n\nHere I'm comparing the ```highlighted versions``` in the left sidebar, i.e., ```v1``` with ```v2``` in a split panel view \u2b07\ufe0f\n\nWe can see values from both artifact versions in a single table. \n\n![](https:\/\/i.imgur.com\/9AVDPjQ.png)","879a3cb6":"Since the shape of the training dataframe and number of unique titles differ, we can infer that we have images with the same title.","8640226e":"<center><h1>Most frequent label groups \ud83d\udcc8<\/h1><\/center>","664c1eae":"<center><h1>Unigrams, bigrams and trigrams \ud83d\udd22 <\/h1><\/center>","41fe5aa4":"<h1><center>Evaluation metric: <b>F1-score \ud83e\uddea<\/b> <\/center><\/h1>\n\n> The evaluation metric for this competition is F1-Score or F-Score.\n> \n> <center><img src=\"https:\/\/www.gstatic.com\/education\/formulas2\/355397047\/en\/f1_score.svg\"><\/center>\n> \n>  It finds the balance between precision and recall.\n>  <center><img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/d37e557b5bfc8de22afa8aad1c187a357ac81bdb\"><\/center>\n>  <center><img src=\"https:\/\/miro.medium.com\/max\/560\/1*AEV3TE67ahMn3NVpU0ov4g.png\" height=10><\/center>\n>  \n>  where-\n>  - TP = True Positive\n>  - FP = False Positive\n>  - TN = True Negative\n>  - FN = False Negative","cee66621":"<center><h3>Titles with similar text \ud83e\udd89\ud83e\udd89<\/h3><\/center>","a6cb8f07":"<center><h1>Getting image paths from the directory \ud83d\udee3\ufe0f<\/h1><\/center>","2addba49":"<img src=\"https:\/\/i.imgur.com\/pl3FhXV.png\">","fc98ce9a":"<center><h3>Similar Images\ud83e\udd89\ud83e\udd89<\/h3><\/center>","1600d75d":"![](https:\/\/i.imgur.com\/sn9xTWx.png)","a8f79019":"<center><img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\"><\/center>\n\nI will be integrating ```W&B``` for ```visualizations``` and ```logging artifacts```!\n\n[Shopee Project on W&B Dashboard](https:\/\/wandb.ai\/ruchi798\/shopee?workspace=user-ruchi798) \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f\n\n* To get the API key, an account is to be created on the website first.\n* Next, use secrets to use API Keys more securely\ud83e\udd2b","479377e2":"[Documentation](https:\/\/docs.rapids.ai\/api\/cuml\/nightly\/api.html#cuml.feature_extraction.text) \ud83d\udcd6","a846d567":"This is a snapshot of the table I just created and added to an artifact.\n\n![](https:\/\/i.imgur.com\/oF7CloS.png)","d4fa3bd9":"Since I have already logged the image embeddings artifact, I can directly use it in this manner \u2b07\ufe0f","14ee3756":"A snapshot of the newly created artifacts \u2b07\ufe0f","67951a67":"Number of images in each directory","c9671469":"Here's a snapshot of my [project](https:\/\/wandb.ai\/ruchi798\/shopee?workspace=user-ruchi798) \u2b07\ufe0f\n\n![](https:\/\/i.imgur.com\/PYEnRRo.png)","130b90ff":"<center><h1>Diving into the Data \ud83e\udd3f <\/h1><\/center>\n\n> **train\/test.csv** - Each row contains the data for a single posting. \n> \n> \u2139\ufe0fMultiple postings might have the exact same image ID, but with different titles or vice versa.\n> \n> - posting_id : the ID code for the posting\n> - image : the image id\/md5sum\n> - image_phash : a perceptual hash of the image\n> - title : the product description for the posting\n> - label_group : ID code for all postings that map to the same product. Not provided for the test set\n> - matches - **Space delimited** list of all posting IDs that match a particular posting. \n> \n> \ud83d\udcccPosts always self-match. \n> \n> \ud83d\udccc**Group sizes were capped at 50**, so we need not predict more than 50 matches for a posting.","a58bc48d":"We can even specify filters on any column to **limit the visible rows down to only rows that match**. \n\nHere I've filtered the table to see only the ```test images```.\n\n![](https:\/\/i.imgur.com\/8eTOBbX.png)","76ec8e14":"> **Observations from EDA**\ud83d\udcdd:\n> \n> * Visually similar images in different label groups\n> * Same images with different titles\n> * Same titles have different images","6c289842":"<center><h1>Displaying images \ud83d\udcf7 <\/h1><\/center>","ccfc676a":"<center><h3>CountVectorizer for Feature Extraction \ud83d\udcd0<\/h3><\/center>","49f204ff":"<center><h1>Colour Histograms \ud83c\udfa8<\/h1><\/center>","b1d24f5c":"<center><h1>Wordcloud of image titles \u2601\ufe0f<\/h1><\/center>","ada71565":"<center><h1>Plugging in RAPIDS \ud83c\udfc3\u200d\u2640\ufe0f <\/h1><\/center>\n<center><img src=\"https:\/\/i.imgur.com\/qWulN0F.jpg\" height=40><\/center>","4e0dac81":"<center><h1>Images with the same title \ud83e\udd89\ud83e\udd89<\/h1><\/center>","9a6a337f":"<center><h1>Import Libraries \ud83d\udcda<\/h1><\/center>","ea0a4c3e":"**Logging custom bar charts** for unigrams, bigrams and trigrams\ud83c\udfcb\ufe0f\u200d\u2640\ufe0f"}}