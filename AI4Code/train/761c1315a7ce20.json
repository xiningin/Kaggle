{"cell_type":{"a84e6676":"code","7ece66c5":"code","444a5c3f":"code","ccce2059":"markdown","4ff41df1":"markdown","90717240":"markdown","dfb73784":"markdown"},"source":{"a84e6676":"# uninstall\n!pip uninstall -y wandb\n\n# download\n!pip install transformers\n!pip install simplet5\n\n# import\nimport re\nimport json\nimport torch\n\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nfrom simplet5 import SimpleT5\nfrom torch.utils.data import Dataset\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split","7ece66c5":"# Data load function\ndef load_sentiment_dataset(random_seed = 1, file_path=\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"):\n    # load dataset and sample 10k reviews.\n    df = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n    df = df[[0, 5]]\n    df.columns = ['label', 'text']\n    df = df.sample(10000, random_state=1)\n    \n    def pick_first_n_words(string, max_words=250): # tried a few max_words, kept 250 as max tokens was < 512\n        split_str = string.split()\n        return \" \".join(split_str[:min(len(split_str), max_words)])\n\n    df['text'] = df['text'].apply(lambda x: pick_first_n_words(x))\n    map_label = {0:'negative', 4: 'positive'}\n    df['label'] = df['label'].apply(lambda x: map_label[x])\n    \n    # divide into test and train\n    X_train, X_test, y_train, y_test = \\\n              train_test_split(df['text'].tolist(), df['label'].tolist(),\n              shuffle=True, test_size=0.05, random_state=random_seed, stratify=df['label'])\n    \n    # transform to pandas dataframe\n    train_data = pd.DataFrame({'source_text': X_train, 'target_text': y_train})    \n    test_data = pd.DataFrame({'source_text': X_test, 'target_text': y_test})    \n\n    # return\n    return train_data, test_data","444a5c3f":"from glob import glob\n\nfor trial_no in range(3):\n    # create data\n    train_df, test_df = load_sentiment_dataset(trial_no)    \n    # load model\n    model = SimpleT5()\n    model.from_pretrained(model_type=\"t5\", model_name=\"t5-base\")\n    # train model\n    model.train(train_df=train_df,\n                eval_df=test_df, \n                source_max_token_len=300, \n                target_max_token_len=200, \n                batch_size=8, \n                max_epochs=2, \n                outputdir = \"outputs\",\n                use_gpu=True\n               )\n    # fetch the path to last model\n    last_epoch_model = None \n    for file in glob(\".\/outputs\/*\"):\n        if 'epoch-1' in file:\n            last_epoch_model = file\n    # load the last model\n    model.load_model(\"t5\", last_epoch_model, use_gpu=True)\n    # test and save\n    # for each test data perform prediction\n    predictions = []\n    for index, row in test_df.iterrows():\n        prediction = model.predict(row['source_text'])[0]\n        predictions.append(prediction)\n    df = test_df.copy()\n    df['predicted'] = predictions\n    df['original'] = df['target_text']\n    print(f1_score(df['original'], df['predicted'], average='macro'))\n    df.to_csv(f\"result_run_{trial_no}.csv\", index=False)\n    # clean the output\n    !rm -rf .\/outputs","ccce2059":"## T5 Finetuning on Sentiment Classification\n\n### Overview\n\n- Compare performance of different text generation model on a sentiment detection task.\n- For this, we will fine the text generation model T5 on train data and report performance on the test data.\n- Hence, we will also learn how to fine tune the TG models along wth how to apply these model to an example NLP task.\n\n### Model\n\n- Huggingface\n\n### Dataset\n\n- Tweet Sentiment","4ff41df1":"### Download and import packages","90717240":"### Load model and tokenizer; Call data Prep","dfb73784":"### Dataset load and prep functions"}}