{"cell_type":{"a5529df8":"code","a36d2889":"code","03e0c740":"code","8e185bba":"code","4e60d0c6":"code","04c97289":"code","b9d80478":"code","a15b0e14":"code","1f210bde":"code","d8ead472":"code","a93ea9e5":"code","815249d9":"code","fc988ddf":"code","4165c757":"code","cddbe918":"code","4f1cc49a":"code","87c3003f":"code","8eeb4ec9":"code","1009ef08":"code","eafefe80":"code","b3e2037a":"code","cbccb0ef":"code","6a2abb20":"code","cad81db9":"code","285fef9c":"code","578db856":"code","8edee243":"code","2477100f":"code","e8759b5e":"code","a7692903":"code","d6135bf0":"code","6093cc74":"code","42358b0c":"code","0a71aec8":"code","0eae2fc4":"code","f095715b":"code","ae8154e1":"code","33d1baed":"code","d9ac249e":"code","8bf93c7c":"code","b2eec72d":"code","f6f98ac6":"code","58fbf33d":"code","9b7cd0a2":"code","c936b451":"code","2be6fda5":"code","8f63bd25":"code","a5ea6bc1":"code","e8eafea8":"code","b94c2e79":"code","0e3f321a":"code","5a7c82de":"code","cda2ecee":"markdown","2b3a3d9c":"markdown","ce798cfc":"markdown","5e86006e":"markdown","0d579292":"markdown","ad199270":"markdown","c9904c43":"markdown","8d31b587":"markdown","fc3918ee":"markdown","e865313e":"markdown","38a79767":"markdown","f52c11b2":"markdown","beb871f2":"markdown","18edf7ee":"markdown","ee4d2051":"markdown","7734771e":"markdown","e3c32d9d":"markdown","01bfe0e1":"markdown","bf6f2fb7":"markdown","2cef2bfe":"markdown","79047008":"markdown","188db71f":"markdown","c726d239":"markdown","30bcf7c7":"markdown","3f976923":"markdown","0c6e0c58":"markdown","5a8317dd":"markdown","95a449ed":"markdown","2d8b826b":"markdown","52171b72":"markdown","12966927":"markdown","e57c2882":"markdown","82275cb7":"markdown","af9ffdbd":"markdown","e5cf7d20":"markdown","6761bb19":"markdown","26f972ce":"markdown"},"source":{"a5529df8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy import arange\n\nfrom scipy import stats","a36d2889":"RS = 42","03e0c740":"#\/kaggle\/input\/miami-housing-dataset\/miami-housing.csv\ndf=pd.read_csv('\/kaggle\/input\/miami-housing-dataset\/miami-housing.csv')\ndf.rename(columns = {'age': 'AGE', 'avno60plus': 'AVNO_60_PLUS', 'month_sold': 'MONTH_SOLD', 'structure_quality': 'STRUCTURE_QUALITY'}, inplace = True)","8e185bba":"df.head()","4e60d0c6":"df.shape","04c97289":"df.drop(['LATITUDE', 'LONGITUDE','PARCELNO'], axis = 1, inplace = True)","b9d80478":"from sklearn.model_selection import train_test_split\n\nX = df.drop('SALE_PRC', axis = 1)\nY = df['SALE_PRC']\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = RS, test_size = 0.20)","a15b0e14":"Y.shape","1f210bde":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","d8ead472":"df = X_train.join(y_train, how = 'inner')\ndf.sort_index(axis = 0, inplace = True)","a93ea9e5":"df.info()","815249d9":"Duplicate = df[df.duplicated(keep='first')]\nprint(\"Duplicate Entries :\", Duplicate.shape)","fc988ddf":"discrete_feature = [feature for feature in df.columns if len(df[feature].unique())<25]\nprint('Discrete variables count: {}'.format(len(discrete_feature)))","4165c757":"discrete_feature","cddbe918":"for feature in discrete_feature:\n    plt.figure(figsize=(10,8))\n    plot=sns.countplot(df[feature])\n    \n    total = len(df[feature])\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        plot.annotate(percentage, (x, y), size = 10)\n    \n    plt.show() ","4f1cc49a":"for feature in discrete_feature:\n    plt.figure(figsize=(8,6))\n    data = df.copy()\n    data.groupby(feature)['SALE_PRC'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SALE_PRC')\n    plt.title(feature)\n    plt.show()","87c3003f":"continuous_feature = [feature for feature in df.columns if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","8eeb4ec9":"continuous_feature","1009ef08":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","eafefe80":"from scipy.stats import shapiro\n\nfor feature in continuous_feature:\n    data = df.copy()\n    DataToTest = data[feature]\n    stat, p = shapiro(DataToTest)\n    print(feature)\n    print('stat = %.2f, p = %.30f' % (stat, p))\n    \n    if p > 0.05:\n        print('Normal distribution')\n        print()\n    else:\n        print('Not a normal distribution')\n        print()","b3e2037a":"for feature in continuous_feature:\n    \n    data = df.copy()\n    \n    if feature != 'SALE_PRC':\n        data[feature]=data[feature]    \n        plt.scatter(data[feature],data['SALE_PRC'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","cbccb0ef":"for feature in continuous_feature:\n    \n    data = df.copy()\n    data.boxplot(column=feature)\n    plt.ylabel(feature)\n    plt.title(feature)\n    plt.show()","6a2abb20":"def IQR_outliers(data,limit=1.5):\n    numColumns = data.select_dtypes(include=np.number).columns.tolist(); # extract list of numeric columns\n    Q1 = data.quantile(0.25)\n    Q3 = data.quantile(0.75)\n    IQR = Q3-Q1;\n    outliers=((data[numColumns] < (Q1 - limit*IQR)) | (data[numColumns] > (Q3 + limit*IQR))).sum()*100\/data.shape[0]\n    return outliers ","cad81db9":"outliers = IQR_outliers(df)\noutliers","285fef9c":"corr = df[continuous_feature].corr()\nf, ax = plt.subplots(figsize=(12, 10))\nmask = np.triu(np.ones_like(corr, dtype=bool))[1:, :-1]\ncorr = corr.iloc[1:,:-1].copy()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)\nplt.yticks(rotation=0)\nplt.show()","578db856":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)","8edee243":"from sklearn.feature_selection import VarianceThreshold\n\nconstant_filter = VarianceThreshold(threshold = 0.01)\nconstant_filter.fit(X_train_scaled)","2477100f":"constant_filter.get_support().sum()","e8759b5e":"X_train_T = X_train_scaled.T\nX_train_T.duplicated().sum()","a7692903":"df = X_train_scaled.copy()\nvif = pd.Series(np.linalg.inv(df.corr().values).diagonal(),index=df.columns,\n          name='VIF').abs().sort_values(ascending=False).round(2)\ndf = pd.cut(vif.round(1),[0,1,5,10,20,30,40,float('inf')]).value_counts().sort_index()\ndf.index = df.index.map(str)\n\nplt.subplots(figsize=(20, 10))\nplt.bar(x=df.index, height=df)","d6135bf0":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE\n\nsel = RFE(RandomForestRegressor(n_estimators = 100, random_state = 0, n_jobs = -1))\nsel.fit(X_train_scaled, y_train)\nsel.get_support()","6093cc74":"features = X_train_scaled.columns[sel.get_support()]\nfeatures","42358b0c":"X_train_sel = pd.DataFrame(sel.transform(X_train_scaled), columns = features)","0a71aec8":"X_train_sel.shape, y_train.shape","0eae2fc4":"X_train_final, X_validation, y_train_final, y_validation = train_test_split(X_train_sel, y_train,  random_state = RS, test_size = 0.10)","f095715b":"X_train_final.shape, y_train_final.shape, X_validation.shape, y_validation.shape","ae8154e1":"def fit_n_print(model, X_train, X_test, y_train, y_test):  # take the model, train data and test data as input\n    \n    start = time.time()  # note the start time \n\n    model.fit(X_train, y_train)   # fit the model using the train data\n\n    pred = model.predict(X_test)     # model predictions on the test data\n\n    r2 = metrics.r2_score(y_test, pred)  # calculate the r squared value on the test data\n    \n    rmse = sqrt(metrics.mean_squared_error(y_test, pred))   # Root mean squared error\n        \n    scorer = {'r2' : metrics.make_scorer(metrics.r2_score),\n              'mse' : metrics.make_scorer(metrics.mean_squared_error)\n               }    # make scorers to be used in cross validation\n    \n    cv = cross_validate(model, X_train, y_train, cv=10, scoring = scorer)   # perform cross validation accross 3 metrics\n    \n    r2_cv = cv['test_r2'].mean()# mean r squared value\n    \n    rmse_cv = np.mean([sqrt(mse) for mse in cv['test_mse']]) # mean RMSE value(take root of individual mse value and then mean)\n\n    end = time.time()  #note the end time\n    \n    \n    duration = end - start  # calculate the total duration\n    \n    \n    return r2, rmse, r2_cv, rmse_cv, duration, pred  # return all the metrics along with predictions","33d1baed":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn import metrics\nfrom math import sqrt\nfrom sklearn.model_selection import cross_validate\nimport time\n\nlr = LinearRegression()        \ndt = DecisionTreeRegressor(random_state=RS)   \nbr = BaggingRegressor(random_state=RS)\nrf = RandomForestRegressor(random_state=RS, n_estimators=100)  # specifying n_estimators to avoid \"future warnings\"\ngb = GradientBoostingRegressor(random_state=RS, n_estimators=100)\nxgb = XGBRegressor(random_state=RS, n_estimators=100)\ncb = CatBoostRegressor(random_state=RS, n_estimators=100)\nlbr = LGBMRegressor(random_state=RS, n_estimators=100)\n\n\nresult = {}   # Create an empty dictionary to later use to store metrics of each of the models\n\n# putting all 5 models in a for loop and appending the results of each of the models to the 'result' dictionary\nfor model, name  in zip([lr, dt, br, rf, gb, xgb, cb, lbr], ['Linear Regression', 'Decision Tree',\n                                               'Bagging Ensemble', 'Random Forest', 'Gradient Boost Ensemble',\n                                               'XGB Regressor', 'CatBoostRegressor', 'LGBMRegressor']):\n    result[name] = fit_n_print(model,X_train_final, X_validation,y_train_final, y_validation)  ","d9ac249e":"result1 = pd.DataFrame(np.array(list(result.values()))[:,:-1],    # make a dataframe out of the metrics from result dictionary \n                       columns= ['R Squared', 'RMSE', 'R2 CV', 'RMSE CV', 'Elapsed'],\n                      index= result.keys())   # use the model names as index\n\nresult1.index.name = 'Model'   # name the index of the result1 dataframe as 'Model'\n\nresult1 ","8bf93c7c":"from sklearn.model_selection import GridSearchCV\n\nmodel = CatBoostRegressor(random_state=RS)\nparameters = {'depth' : [6,8,10],\n              'learning_rate' : [0.01, 0.05, 0.1],\n              'iterations'    : [30, 50, 100]\n              }\n\ngrid = GridSearchCV(estimator=model, param_grid = parameters, cv = 10, n_jobs=-1, verbose=0)\ngrid.fit(X_train_sel, y_train)","b2eec72d":"grid.best_params_","f6f98ac6":"model_final = CatBoostRegressor(depth = 10, iterations = 100, learning_rate = 0.1, random_state=RS, verbose=None)\nmodel_final.fit(X_train_final, y_train_final)","58fbf33d":"pred = model_final.predict(X_validation)\n\nr2 = metrics.r2_score(y_validation, pred)\nprint('r2 score: ', r2)\n    \nrmse = sqrt(metrics.mean_squared_error(y_validation, pred))\nprint(\"rmse: \", rmse)","9b7cd0a2":"X_test.shape, y_test.shape","c936b451":"X_test = scaler.transform(X_test)\nX_test = sel.transform(X_test)\n\nX_test.shape, y_test.shape","2be6fda5":"pred = model_final.predict(X_test)\n\nr2 = metrics.r2_score(y_test, pred)\nprint('r2 score: ', r2)\n    \nrmse = sqrt(metrics.mean_squared_error(y_test, pred))\nprint(\"rmse: \", rmse)","8f63bd25":"X = scaler.transform(X)\nX = sel.transform(X)","a5ea6bc1":"X.shape, Y.shape","e8eafea8":"from sklearn.model_selection import cross_validate\n\nmodel_to_pickle = CatBoostRegressor(depth = 10, iterations = 100, learning_rate = 0.1, random_state=RS)\nall_accuracies = cross_validate(estimator=model_to_pickle, X = X, y = Y, cv=10, scoring = 'r2')","b94c2e79":"print(all_accuracies)","0e3f321a":"avg_score = all_accuracies['test_score'].mean()\nstd = all_accuracies['test_score'].std()\nstd_2 = std * 2\nproduction_acc_min = avg_score - std_2\nproduction_acc_max = avg_score + std_2\nprint(\"In production the accuracy of our model ranges from {} to {} (with 95% confidence)\".format(production_acc_min, production_acc_max))","5a7c82de":"import pickle\n# save the model to disk\nfilename = 'finalized_model.sav'\npickle.dump(model_to_pickle, open(filename, 'wb'))","cda2ecee":"## MODEL PICKLING","2b3a3d9c":"#### Information about the dataset","ce798cfc":"***Feature selection by Recursive Feature Elimination***","5e86006e":"## FEATURE ENGINEERING","0d579292":"#### Discrete feature analysis","ad199270":"We cannot see any monotonic relationship between the independent and dependent features, this maybe beacause of skewness or outliers.\n\nSo now let us check for outliers in independent features.","c9904c43":"#### Checking for duplicate entries","8d31b587":"- There are no duplicate entries in the dataset.","fc3918ee":"***Feature scaling***","e865313e":"We got the r2 score of 0.88 by using just 6 features(Which is half of the original feature count)","38a79767":"There are few multicollinear features. Generally VIF>10 is considered as high, None of the features have VIF value greater than 10","f52c11b2":"There are no constant and quasi constant features in the dataset.","beb871f2":"## EXPLORATORY DATA ANALYSIS","18edf7ee":"- This dataset has skewed independent\/ input features. We can prove this by some statistical tests.","ee4d2051":"***Checking for constant, quasi constant features***","7734771e":"'LND_SQFOOT', 'TOT_LVG_AREA', 'SPEC_FEAT_VAL', 'RAIL_DIST', 'WATER_DIST', 'SUBCNTR_DI', 'HWY_DIST', 'AVNO_60_PLUS' - These features have high percentage of outliers.","e3c32d9d":"#### Continuous feature analysis","01bfe0e1":"Remember that we got this accuracy by using just 6 features. Definitely we can get better accuracy with more features.\n\nI wanted to demonstrate Feature selection, Model tuning, Cross validation and production accuracy in this dataset. So accuracy is secondary to me.","bf6f2fb7":"### Thankyou for reading my notebook :)\n### Happy learning!!!","2cef2bfe":"***Checking for duplicate features***","79047008":"Please correct me if u find any mistakes in this notebook, i am pretty much new to this domain.","188db71f":"***Checking for multicollinearity***","c726d239":"- There are no missing values in the dataset.\n\n- All the features in the dataset are numerical.","30bcf7c7":"## FEATURE SELECTION","3f976923":"- AVNO_60_PLUS and MONTH_SOLD seems like they do not add any value in predicting the SALE_PRC, because all of its classes median values are almost equal, Let's keep an eye on these two features and test for its significance on SALE_PRC by using some statistical tests(FEATURE SELECTION).\n\n- class 3 of STRUCTURE_QUALITY has the highest SALE_PRC.","0c6e0c58":"#### Discrete features","5a8317dd":"- None of the features are normally distributed. We need to make these features normally distributed before building the model by applying some Tranformations(FEATURE ENGINEERING)","95a449ed":"- Since 'LATITUDE','LONGITUDE','PARCELNO' are unique for each records we can drop them.","2d8b826b":"There are no duplicate features in the dataset","52171b72":"As we can see the model performs well even on unseen\/new data.\n\nThis shows that our model is pretty much generalized :)","12966927":"***OBSERVATIONS***\n\n- Dataset has no missing values.\n- AVNO_60_PLUS and MONTH_SOLD seems like they do not add any value in predicting the SALE_PRC(let us conform this through feature selection techniques)\n- None of the input\/independent features are normally distributed and some features have outliers in them.\n- No multicollinearity","e57c2882":"- AVNO_60_PLUS is dominated by class 0\n- Month 6 has the maximum number of sales, Almost all months have equal percentage of sales.\n- 54.8% of houses have Structure quality rating of 5","82275cb7":"#### Train test split","af9ffdbd":"As we can see the dataset has large number of outliers in them.\n\nOutliers can be handled using some outlier handling techniques orelse we can use Machine learning models that are robust to outliers.","e5cf7d20":"CatBoostRegressor performs well, let us tune its hyperparameters.","6761bb19":"## PREDICTION ON NEW DATA","26f972ce":"## ACCURACY RANGE IN PRODUCTION"}}