{"cell_type":{"171f0df5":"code","6caece4c":"code","5e757bf9":"code","7afac52d":"code","2226d243":"code","941fed26":"code","632ff72b":"code","24ae6dda":"code","b704801c":"code","8fcfa843":"code","67e8cfe3":"code","34266cbb":"code","81d2f3af":"code","684ac84b":"code","9cecab5b":"code","448571c8":"code","8711b5a9":"code","5d9ab679":"code","34f077a1":"code","55625484":"code","83cd6269":"code","251260b6":"code","35a15e88":"code","e6205889":"code","80ced3a5":"code","8f977ab7":"code","a149f37f":"code","50621132":"code","faec4c36":"markdown","c3e1b559":"markdown","667b6d84":"markdown","4817d149":"markdown","bf14f890":"markdown","f7a8d5f7":"markdown","f1a01eb4":"markdown","fe2ab5ce":"markdown","f19c91c0":"markdown","72d8bae9":"markdown","557e228d":"markdown","f2534c5f":"markdown","39c64786":"markdown","69961b64":"markdown","80f57633":"markdown","e8a1ef40":"markdown","f0a776be":"markdown","c0fd60e2":"markdown","8ca395e0":"markdown","8e6650b7":"markdown","7a71a7f8":"markdown","21031aab":"markdown","df41d013":"markdown"},"source":{"171f0df5":"# Version 2 + Bug fix - thanks to @chinhuic\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nfrom IPython.display import Image\n# Any results you write to the current directory are saved as output.","6caece4c":"'''\n# Declare model and optimizer as usual, with default (FP32) precision\nmodel = ...\noptimizer = ...\n\n# Allow Amp to perform casts as required by the opt_level\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n\n# loss.backward() becomes (for dynamic loss scaling in amp):\nwith amp.scale_loss(loss, optimizer) as scaled_loss:\n    scaled_loss.backward()\n    \n'''","5e757bf9":"Image('..\/input\/kaggledaysdata\/amp_dynamic_loss_scaling.PNG')","7afac52d":"os.listdir('..\/input\/nvidia-apex\/repository\/NVIDIA-apex-880ab92\/')","2226d243":"! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidia-apex\/repository\/NVIDIA-apex-880ab92","941fed26":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\nimport gc\nimport re\nimport operator \nimport sys\nfrom sklearn import metrics\nfrom sklearn import model_selection\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom nltk.stem import PorterStemmer\nfrom sklearn.metrics import roc_auc_score\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\nfrom tqdm import tqdm, tqdm_notebook\nimport os\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport warnings\nwarnings.filterwarnings(action='once')\nimport pickle\nfrom apex import amp\nimport shutil","632ff72b":"device=torch.device('cuda')","24ae6dda":"MAX_SEQUENCE_LENGTH = 220\nSEED = 1234\nEPOCHS = 1\nData_dir=\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\"\nInput_dir = \"..\/input\"\nWORK_DIR = \"..\/working\/\"\nnum_to_load= 10000 #100000                         #Train size to match time limit\nvalid_size= 10000 #100000                          #Validation Size\nTOXICITY_COLUMN = 'target'","b704801c":"# Add the Bart Pytorch repo to the PATH\n# using files from: https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\npackage_dir_a = \"..\/input\/ppbert\/pytorch-pretrained-bert\/pytorch-pretrained-BERT\"\nsys.path.insert(0, package_dir_a)\n\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n","8fcfa843":"# Translate model from tensorflow to pytorch\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\nBERT_MODEL_PATH + 'bert_config.json',\nWORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","67e8cfe3":"os.listdir(\"..\/working\")","34266cbb":"# This is the Bert configuration file\nfrom pytorch_pretrained_bert import BertConfig\n\nbert_config = BertConfig('..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'+'bert_config.json')\n","81d2f3af":"# Converting the lines to BERT format\n# Thanks to https:\/\/www.kaggle.com\/httpwwwfszyc\/bert-in-keras-taming\ndef convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm_notebook(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    print(longer)\n    return np.array(all_tokens)","684ac84b":"BERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\n","9cecab5b":"%%time\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None,do_lower_case=True)\ntrain_df = pd.read_csv(os.path.join(Data_dir,\"train.csv\")).sample(num_to_load+valid_size,random_state=SEED)\nprint('loaded %d records' % len(train_df))\n\n# Make sure all comment_text values are strings\ntrain_df['comment_text'] = train_df['comment_text'].astype(str) \n\nsequences = convert_lines(train_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"),MAX_SEQUENCE_LENGTH,tokenizer)\ntrain_df=train_df.fillna(0)\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\ny_columns=['target']\n\ntrain_df = train_df.drop(['comment_text'],axis=1)\n# convert target to 0,1\ntrain_df['target']=(train_df['target']>=0.5).astype(float)","448571c8":"\nX = sequences[:num_to_load]                \ny = train_df[y_columns].values[:num_to_load]\nX_val = sequences[num_to_load:]                \ny_val = train_df[y_columns].values[num_to_load:]\n","8711b5a9":"test_df=train_df.tail(valid_size).copy()\ntrain_df=train_df.head(num_to_load)","5d9ab679":"\n\ntrain_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))\n","34f077a1":"# From baseline kernel\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]>0.5\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\n\n\nSUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]>0.5]\n    return compute_auc((subgroup_examples[label]>0.5), subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[(df[subgroup]>0.5) & (df[label]<=0.5)]\n    non_subgroup_positive_examples = df[(df[subgroup]<=0.5) & (df[label]>0.5)]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[(df[subgroup]>0.5) & (df[label]>0.5)]\n    non_subgroup_negative_examples = df[(df[subgroup]<=0.5) & (df[label]<=0.5)]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label]>0.5, examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]>0.5])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)","55625484":"output_model_file = \"bert_pytorch_no_ba.bin\"\n\nlr=2e-5\nbatch_size = 32\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nmodel = BertForSequenceClassification.from_pretrained(\"..\/working\",cache_dir=None,num_labels=len(y_columns))\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)\/batch_size)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()\n\ntq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n    for i,(x_batch, y_batch) in tk0:\n#        optimizer.zero_grad()\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device)) # compute loss\n        with amp.scale_loss(loss, optimizer) as scaled_loss:                  # update the gradients by the loss (the loss should be scaled since we are using AMP)\n            scaled_loss.backward()\n        #loss.backward()                                                       \n        optimizer.step()                                                      # update NN parameters by the optimizer policy and gradients calculated\n        optimizer.zero_grad()                                                 # reset gradient tensors\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf)\n        avg_loss += loss.item() \/ len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()\/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n\ntorch.save(model.state_dict(), output_model_file)","83cd6269":"# Run validation\n# The following 2 lines are not needed but show how to download the model for prediction\nmodel = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\n_ = model.load_state_dict(torch.load(output_model_file ))\nmodel = model.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel = model.eval()\nvalid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n\n\n\nMODEL_NAME = 'model_without_ba'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n#bias_metrics_df\n'Validation Score', get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","251260b6":"output_model_file = \"bert_pytorch_ba.bin\"\n\nlr=2e-5\nbatch_size = 32\naccumulation_steps=4\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\nmodel = BertForSequenceClassification.from_pretrained(\"..\/working\",cache_dir=None,num_labels=len(y_columns))\nmodel.zero_grad()\nmodel = model.to(device)\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\ntrain = train_dataset\n\nnum_train_optimization_steps = int(EPOCHS*len(train)\/batch_size\/accumulation_steps)\n\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=lr,\n                     warmup=0.05,\n                     t_total=num_train_optimization_steps)\n\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\nmodel=model.train()\n\ntq = tqdm_notebook(range(EPOCHS))\nfor epoch in tq:\n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    avg_loss = 0.\n    avg_accuracy = 0.\n    lossf=None\n    tk0 = tqdm_notebook(enumerate(train_loader),total=len(train_loader),leave=False)\n    optimizer.zero_grad()   # Bug fix - thanks to @chinhuic\n    for i,(x_batch, y_batch) in tk0:\n#        optimizer.zero_grad()\n        y_pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n        loss =  F.binary_cross_entropy_with_logits(y_pred,y_batch.to(device))\n        with amp.scale_loss(loss, optimizer) as scaled_loss:  # update the gradients by the loss (the loss should be scaled since we are using AMP)\n            scaled_loss.backward()\n        if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n            optimizer.step()                            # Now we can do an optimizer step to update NN parameters\n            optimizer.zero_grad()\n        if lossf:\n            lossf = 0.98*lossf+0.02*loss.item()\n        else:\n            lossf = loss.item()\n        tk0.set_postfix(loss = lossf)\n        avg_loss += loss.item() \/ len(train_loader)\n        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()\/len(train_loader)\n    tq.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n\n\ntorch.save(model.state_dict(), output_model_file)","35a15e88":"# Run validation\n# The following 2 lines are not needed but show how to download the model for prediction\nmodel = BertForSequenceClassification(bert_config,num_labels=len(y_columns))\nmodel.load_state_dict(torch.load(output_model_file ))\nmodel = model.to(device)\nfor param in model.parameters():\n    param.requires_grad=False\nmodel = model.eval()\nvalid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(X_val,dtype=torch.long))\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask=(x_batch>0).to(device), labels=None)\n    valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n\n\nMODEL_NAME = 'model_with_ba'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n#bias_metrics_df\n'Validation Score', get_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","e6205889":"X_val","80ced3a5":"# Step 1. Sort tensors by length\ndef get_sort_keys(arr):\n    try:\n        arr = arr.numpy()\n    except:\n        pass\n    key = np.argsort( np.sum(arr == 0, axis = 1))\n    return key\n\nkeys = get_sort_keys(X_val)\nkeys","8f977ab7":"# Step 1. Sort tensors by length\ndef sort_ds_by_key(X, keys):\n    new_X = []\n    for i, _ in enumerate(X):    \n        new_X.append(X[keys[i]])\n        \n    return np.array(new_X)\n\nsorted_X_val = sort_ds_by_key(X_val, keys)\nsorted_X_val","a149f37f":"def trim_tensors(tsrs):\n    max_len = torch.max(torch.sum(tsrs != 0, 1))\n    if max_len > 2: \n        tsrs = tsrs[:, :max_len+1]\n    return tsrs ","50621132":"# Step 2. Trim tensors per batch to the maximum length within that batch\nvalid_preds = np.zeros((len(X_val)))\nsorted_valid_preds = np.zeros((len(X_val)))\nvalid = torch.utils.data.TensorDataset(torch.tensor(sorted_X_val, dtype=torch.long)) # change from X_val to sorted_X_val\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=32, shuffle=False)\n\ntk0 = tqdm_notebook(valid_loader)\nfor i,(x_batch,)  in enumerate(tk0):\n    trimmed_x_batch = trim_tensors(x_batch)\n    pred = model(trimmed_x_batch.to(device), attention_mask=(trimmed_x_batch>0).to(device), labels=None)\n    sorted_valid_preds[i*32:(i+1)*32]=pred[:,0].detach().cpu().squeeze().numpy()\n\nvalid_preds[keys] = sorted_valid_preds # restore to the original order\n\nMODEL_NAME = 'model_with_ba_tensor_trim'\ntest_df[MODEL_NAME]=torch.sigmoid(torch.tensor(valid_preds)).numpy()\nTOXICITY_COLUMN = 'target'\nbias_metrics_df = compute_bias_metrics_for_model(test_df, identity_columns, MODEL_NAME, 'target')\n#bias_metrics_df\nget_final_metric(bias_metrics_df, calculate_overall_auc(test_df, MODEL_NAME))","faec4c36":"### AMP Initialization Comparison\n\n* O0: FP32 training\n* O1: Mixed Precision (recommended for typical use)\n    * Whitelist ops (for example, convolutions) are performed in FP16\n    * Blacklist ops that benefit from FP32 precision (for example, softmax) are performed in FP32\n* O2: \u201cAlmost FP16\u201d Mixed Precision (not recommended, could be harder to converge and stabilize)\n* O3: FP16 training (for performance comparison)","c3e1b559":"### Pytorch Coding Example","667b6d84":"# Batch Accumulation (BA)\n\n* When to use BA: Train model with larger batch size but limited memory.\n* How to implement BA: Accumulate gradients and backpropagate them all at once.\n* Why BA saves memory: Gradients are accumulated batch by batch instead of calculated all at once. \n* Why larger batch size: Might stablize the training especially when finetuning models with a few epochs only.","4817d149":"# How to train\/inference BERT models fast with limited GPU RAM (Pytorch).\n## 1. Automatic Mixed Precision\n## 2. Batch Accumulation\n## 3. Tensor Trimming","bf14f890":"## Reference: https:\/\/nvidia.github.io\/apex\/amp.html\n## Reference: https:\/\/docs.nvidia.com\/deeplearning\/sdk\/mixed-precision-training\/index.html","f7a8d5f7":"## Mixed Precision Training","f1a01eb4":"## Train with Batch Accumulation (with AMP)","fe2ab5ce":"### Tokenize Strings for BERT","f19c91c0":"* Note: Higher scores mean better in this competition. For more details, please reference https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview\/evaluation","72d8bae9":"## Steps\n1. Sort tensors by length\n2. Trim tensors per batch to the maximum length within that batch","557e228d":"# Automatic Mixed Precision (AMP)","f2534c5f":"## Train without Batch Accumulation (with AMP)","39c64786":"## Automatic Mixed Precision (AMP) Training (Pytorch)\n* Automatic loss scaling and master weights integrated into optimizer classses\n* Automatic casting between FP16 and FP32 to maximize speed while ensuring no loss in task-specific accuracy.","69961b64":"## More tensor trimming related to-do in training time\n1. Try to sample train data with similar sequence lengths (to maximize the benefit of next step)\n    * Example: Bucketize the training data by sequence legnths, but might need to take care the number of the buckets. Too much buckets might affect the randomness and NN's performance.\n2. Trim tensors per batch","80f57633":"### Benefits\n* Decrease the required amount of memory: Enable larger model or larger mini-batches\n* Shorten the training and inference time.","e8a1ef40":"* Data is from the competition \"Jigsaw Unintended Bias in Toxicity Classification\": https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview\/description\n    * Efficiency is the key to win this competition ==> Why I select these three topics to introduce in this workshop\n        * Train efficiently to iterate ideas and tune models fast.\n        * Inference efficiently to add more models in the inference kernel to do a better model ensembling.\n* Most BERT training codes are from https:\/\/www.kaggle.com\/yuval6967\/toxic-bert-plain-vanila by @yuval6967 ","f0a776be":"### Install APEX for AMP","c0fd60e2":"# Tensor Trimming (Inference)","8ca395e0":"### AMP Dynamic Loss Scaling","8e6650b7":"### Mixed Precision Training Steps\n* Porting the model to use the FP16 data type where appropriate\n* Adding loss scaling to preserve small gradient values","7a71a7f8":"## Validation Utilities","21031aab":"* We could observe the inference speed increases with time, since our max. sequence length per batch goes from large to small.\n* Should not affect the score much (still some due to some 0's are trimmed)","df41d013":"### Precision Definition\n* Half Precision: FP16\n* Single Precision: FP32\n* Double Precision: FP64\n* Mixed Precision (here):  FP16 + FP32[](http:\/\/)"}}