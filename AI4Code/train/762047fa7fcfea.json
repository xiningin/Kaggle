{"cell_type":{"667d3bc4":"code","61d680ac":"code","381641d6":"code","2e565ff5":"code","5f20004e":"code","b69b90db":"code","5d939fd3":"code","526c1ce4":"code","ccfd3b06":"code","47d775a8":"code","e330303f":"code","e16995c0":"code","3866325e":"code","6987ad7a":"markdown","3a4f87b5":"markdown","6ea30442":"markdown","8682966e":"markdown","9777ba36":"markdown"},"source":{"667d3bc4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport sys\nimport math\nimport time\nimport pickle\nimport psutil\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom datetime import datetime\nfrom contextlib import contextmanager\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\n\nimport warnings\n\nimport optuna\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import (StandardScaler, PowerTransformer, MinMaxScaler,\n                                   QuantileTransformer, LabelEncoder,\n                                   OneHotEncoder, OrdinalEncoder)\nimport category_encoders as ce\nplt.style.use('fivethirtyeight')\nwarnings.filterwarnings('ignore')","61d680ac":"original_train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ntest_target = pd.read_csv('..\/input\/tabular-feb-submission-files\/sub45_47_48_(5).csv')","381641d6":"test['target'] = test_target.target","2e565ff5":"train = pd.concat([original_train, test], axis=0)\ntrain","5f20004e":"N_FOLDS = 10\nN_ESTIMATORS = 30000\nSEED = 2021\nBAGGING_SEED = 48","b69b90db":"@contextmanager\ndef timer(name: str):\n    t0 = time.time()\n    p = psutil.Process(os.getpid())\n    m0 = p.memory_info()[0] \/ 2. ** 30\n    try:\n        yield\n    finally:\n        m1 = p.memory_info()[0] \/ 2. ** 30\n        delta = m1 - m0\n        sign = '+' if delta >= 0 else '-'\n        delta = math.fabs(delta)\n        print(f\"[{m1:.1f}GB({sign}{delta:.1f}GB): {time.time() - t0:.3f}sec] {name}\", file=sys.stderr)\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n\n\ndef score_log(df: pd.DataFrame, seed: int, num_fold: int, model_name: str, cv: float):\n    score_dict = {'date': datetime.now(), 'seed': seed, 'fold': num_fold, 'model': model_name, 'cv': cv}\n    # noinspection PyTypeChecker\n    df = pd.concat([df, pd.DataFrame.from_dict([score_dict])])\n    df.to_csv(LOG_PATH \/ f\"model_score_{model_name}.csv\", index=False)\n    return df","5d939fd3":"with timer(\"Read data\"):\n    train_df = train\n    test_df = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\n    sub_df = pd.read_csv( \"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\n\n    all_df = pd.concat([train_df, test_df])\n    all_df = all_df.sort_values('id').reset_index(drop=True)","526c1ce4":"cont_features = [f'cont{i}' for i in range(14)]\ncat_features = [f'cat{i}' for i in range(10)]\nall_features = cat_features + cont_features\ntarget_feature = 'target'\n\ntarget = train_df[target_feature]\ntrain_df = train_df[all_features]\ntest_df = test_df[all_features]","ccfd3b06":"# LabelEncoder()\nwith timer(\"LabelEncoder\"):\n    le = LabelEncoder()\n    for col in cat_features:\n        le.fit(train_df[col])\n        train_df[col] = le.transform(train_df[col])\n        test_df[col] = le.transform(test_df[col])","47d775a8":"params = {'random_state': SEED,\n          'metric': 'rmse',\n          'n_estimators': N_ESTIMATORS,\n          'n_jobs': -1,\n          'cat_feature': [x for x in range(len(cat_features))],\n          'bagging_seed': SEED,\n          'feature_fraction_seed': SEED,\n          'learning_rate': 0.003899156646724397,\n          'max_depth': 99,\n          'num_leaves': 63,\n          'reg_alpha': 9.562925363678952,\n          'reg_lambda': 9.355810045480153,\n          'colsample_bytree': 0.2256038826485174,\n          'min_child_samples': 290,\n          'subsample_freq': 1,\n          'subsample': 0.8805303688019942,\n          'max_bin': 882,\n          'min_data_per_group': 127,\n          'cat_smooth': 96,\n          'cat_l2': 19\n          }","e330303f":"oof = np.zeros(train_df.shape[0])\npreds = 0\ntrain_preds = 0\nscore_df = pd.DataFrame()\nfeature_importances = pd.DataFrame()\n\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","e16995c0":"for fold, (train_idx, valid_idx) in enumerate(kf.split(X=train_df)):\n    X_train, X_valid = train_df.iloc[train_idx], train_df.iloc[valid_idx]\n    y_train, y_valid = target.iloc[train_idx], target.iloc[valid_idx]\n\n    with timer(f\"fold {fold}: fit\"):\n        model = LGBMRegressor(**params)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_valid, y_valid)],\n                  eval_metric='rmse',\n                  early_stopping_rounds=100,\n                  verbose=0)\n\n    fi_tmp = pd.DataFrame()\n    fi_tmp['feature'] = model.feature_name_\n    fi_tmp['importance'] = model.feature_importances_\n    fi_tmp['fold'] = fold\n    fi_tmp['seed'] = SEED\n    feature_importances = feature_importances.append(fi_tmp)\n\n    oof[valid_idx] = model.predict(X_valid)\n    preds += model.predict(test_df)\/N_FOLDS\n    train_preds += model.predict(train_df)\/N_FOLDS\n    rmse = mean_squared_error(y_valid, oof[valid_idx], squared=False)\n    score_df = score_log(score_df, SEED, fold, 'lgb', rmse)\n    print(f\"rmse {rmse}\")\n\nrmse = mean_squared_error(target, oof, squared=False)\nscore_df = score_log(score_df, SEED, 999, 'lgb', rmse)\nprint(\"+-\"*40)\nprint(f\"rmse {rmse}\")","3866325e":"pred_train_df = pd.DataFrame([])\nsub_df.target = preds\nsub_df.to_csv(f\"pseudo_labelling_cv{rmse:.6f}.csv\", index=False)\nsub_df.head()\n\npred_train_df.target = train_preds\npred_train_df.to_csv(\"train_pred_v0.csv\", index=False)\n\nnp.save(LOG_PATH \/ 'train_oof', oof)\nnp.save(LOG_PATH \/ 'test_preds', preds)","6987ad7a":"**Please upvote it was helpful for you! Thank you!!!**","3a4f87b5":"# **6th place solution : Tabular Feb. Competition**\n\nIt is my first time to reach this high ranking. So I am really happy!\nThis is my code I used to improve my score. \nI refer to lots of discussion and codes, and It was really helpful. Thanks you for everybody who wrote it. \nAnd special thanks to @hiro5299834! Wrote amazing LGBM codes and fine-tuned params, it became our code baseline. [His Code](https:\/\/www.kaggle.com\/hiro5299834\/tps-feb-2021-with-single-lgbm-tuned)\n","6ea30442":"# **Pseudo Labelling**\nJust concat well-trained submission file, test file, and train file.","8682966e":"# **Training with LGBM**\n\nThis code is from BIZEN's codes, but pseudo labelling always worked with any LGBM models. So you can use any fine-tuned LGBM models. Actually, I reached 0.84245 private score with single LGBM model and pseudo labelling. (0.84193 public LB) ","9777ba36":"# **Abstract**\nI used pseudo labelling for improving score. Pseudo Labelling is kind of semi-supervised learning. I labelled all test data, and trained again with original train data and pseudo-labelled test data. [Detail](https:\/\/analyticsindiamag.com\/pseudo-labelling-a-guide-to-semi-supervised-learning\/) \nIt is really simple trick, but score improved a lot. Original code was 0.84198 LB, but pseudo labelling code reached 0.84185 LB (0.84249 private score) \n\nAfter this, I ensembled a lot. And I can reached 0.84246 private score. But I couldn't see the public score improved, so I couldn't select that for final submission, unfortunately. \n\nBelow is my code for training 6th place model."}}