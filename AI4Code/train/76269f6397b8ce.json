{"cell_type":{"8bfe09e6":"code","0e972ef5":"code","02149809":"code","d2591ee6":"code","a57caaed":"code","f5613d0b":"code","ff013978":"code","11fb6f1c":"code","d5bb7d01":"code","34b49a68":"code","e10d9011":"code","f764a431":"code","b2c870be":"code","e98c9cc6":"code","25b122ab":"code","bcae1b8a":"code","c79c03ca":"code","6ee3ccfb":"code","3733fc82":"code","f2e6837d":"code","e4c995e3":"code","3d98ec27":"code","70949a5c":"code","0b2d3ecf":"code","0752b66e":"code","a596414a":"code","6eb5e838":"code","98a9e97a":"code","ff16d908":"code","b169828a":"code","3815a902":"code","b107e36d":"code","5f380820":"code","355fae52":"code","0944a192":"code","383cf11f":"code","2eb26560":"code","f616ea36":"code","7a41f871":"code","69f45ce0":"code","4d7308df":"code","3373751a":"code","90b951a3":"code","3eec41d3":"code","5ca22366":"code","4367e57b":"code","c51ff912":"code","9f3647a7":"code","298138c6":"code","487a45b6":"code","40419a33":"code","a60e781f":"code","b027512c":"code","b370762b":"code","e4687ebe":"code","4a6facc0":"code","fa902a00":"markdown","dbe025c5":"markdown","86940ba9":"markdown","0975fbe3":"markdown","0073e3f5":"markdown","73f74079":"markdown","ef6d2167":"markdown","0335b7d8":"markdown","03a8d71d":"markdown","944ced16":"markdown","f6cdce73":"markdown","19f63204":"markdown","05ee5095":"markdown","94d27cb9":"markdown","73a76d2d":"markdown","cc81d098":"markdown"},"source":{"8bfe09e6":"view = 0\nbatch_sz = 4\nepochs = 1\nsteps_per_epoch = 1000\nvalidation_steps = 100","0e972ef5":"!git clone https:\/\/github.com\/GeorgeSeif\/Semantic-Segmentation-Suite.git\n  ","02149809":"import os","d2591ee6":"from pathlib import Path\ndata_path = Path('Semantic-Segmentation-Suite\/CamVid')","a57caaed":"print('Number of train frames: ' + str(len(os.listdir(data_path\/'train'))))\nprint('Number of train labels: ' + str(len(os.listdir(data_path\/'train_labels'))))\nprint('Number of val frames: ' + str(len(os.listdir(data_path\/'val'))))\nprint('Number of val labels: ' + str(len(os.listdir(data_path\/'val_labels'))))\nprint('Number of test frames: ' + str(len(os.listdir(data_path\/'test'))))\nprint('Number of test labels: ' + str(len(os.listdir(data_path\/'test_labels'))))\nprint('Total frames: ' + str(len(os.listdir(data_path\/'train')) + len(os.listdir(data_path\/'val')) + len(os.listdir(data_path\/'test'))))","f5613d0b":"import pandas as pd\nclasses = pd.read_csv(data_path \/ 'class_dict.csv', index_col =0)","ff013978":"classes","11fb6f1c":"n_classes = len(classes)\nn_classes","d5bb7d01":"cls2rgb = {cl:list(classes.loc[cl, :]) for cl in classes.index}","34b49a68":"%matplotlib inline\nimport cv2\nimport matplotlib.pyplot as plt\n#from google.colab.patches import  cv2_imshow\n\n#img = cv2.imread(data_path\/'train\/0001TP_006690.png')\nimg = cv2.imread(str(data_path) + '\/train\/0001TP_006690.png')\nplt.imshow(img)\n","e10d9011":"import numpy as np\nmask = cv2.imread(str(data_path) + '\/train_labels\/0001TP_006690_L.png')\nmask = cv2.cvtColor((mask).astype(np.uint8), cv2.COLOR_BGR2RGB)# If you want to get the same order as in the color mapping of CAMVID, use the cv converted","f764a431":"plt.imshow(mask)","b2c870be":"from keras.preprocessing.image import load_img\nmask = load_img(str(data_path) + '\/train_labels\/0001TP_006690_L.png')\nmask","e98c9cc6":"mask = np.array(mask)# Now colors are the same as in the dict, since keras load_img uses RGB order.","25b122ab":"mask.shape","bcae1b8a":"mask.shape","c79c03ca":"def adjust_mask(mask, flat=False):\n    \n    semantic_map = []\n    for colour in list(cls2rgb.values()):        \n        equality = np.equal(mask, colour)# 256x256x3 with True or False\n        class_map = np.all(equality, axis = -1)# 256x256 If all True, then True, else False\n        semantic_map.append(class_map)# List of 256x256 arrays, map of True for a given found color at the pixel, and False otherwise.\n    semantic_map = np.stack(semantic_map, axis=-1)# 256x256x32 True only at the found color, and all False otherwise.\n    if flat:\n      semantic_map = np.reshape(semantic_map, (-1,256*256))\n\n    return np.float32(semantic_map)# convert to numbers","6ee3ccfb":"new_mask = adjust_mask(mask)","3733fc82":"new_mask.shape","f2e6837d":"idx2rgb={idx:np.array(rgb) for idx, (cl, rgb) in enumerate(cls2rgb.items())}\n","e4c995e3":"# Map the idx back to rgb\ndef map_class_to_rgb(p):\n  \n  return idx2rgb[p[0]]\n\nrgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(np.argmax(new_mask, axis=-1), -1))","3d98ec27":"plt.imshow(rgb_mask)","70949a5c":"import numpy as np \nimport os\n#import skimage.io as io\n#import skimage.transform as trans\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom keras import backend as keras\n\n\ndef unet(n_classes, pretrained_weights = None,input_size = (256,256,3), flat=False, ohe=True):\n    inputs = Input(input_size)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n    drop4 = Dropout(0.5)(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n    drop5 = Dropout(0.5)(conv5)\n\n    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n    merge6 = concatenate([drop4,up6], axis = 3)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv1,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    #conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n    #conv10 = Conv2D(n_classes, (1,1), activation = 'softmax')(conv9)\n    conv10 = Conv2D(n_classes, (1,1), padding='same')(conv9)\n    if flat:\n      output_layer = Reshape((256*256,n_classes))(conv10)\n    else:\n      output_layer = conv10\n    output_layer = Activation('softmax')(output_layer)\n     \n\n    model = Model( inputs,output_layer)\n\n    if ohe:\n      model.compile(optimizer = Adam(lr = 1e-4), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    else:\n      model.compile(optimizer = Adam(lr = 1e-4), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n    \n    #model.summary()\n\n    if(pretrained_weights):\n        model.load_weights(pretrained_weights)\n\n    return model\n\n","0b2d3ecf":"\nmodel = unet(n_classes)\nmodel.summary()","0752b66e":"def load_CAMVID(data_type='train', enc='ohe', shape='normal'):\n  img_path = str(data_path) + '\/' + data_type + '\/'\n  labels_path = str(data_path) + '\/' + data_type + '_labels\/'\n  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n  x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1.\/255 for file in sorted(os.listdir(img_path))])\n  if(enc=='ohe'):\n    \n    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=(256,256)))) for file in sorted(os.listdir(labels_path))])\n  elif(enc=='sparse_cat'):\n    y = np.array([adjust_mask(np.array(load_img(str(labels_path) + file, target_size=(256,256)))) for file in sorted(os.listdir(labels_path))])\n  if(shape == 'flat'):\n    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n    y = np.expand_dims(y, axis=-1)\n  return x, y\n  ","a596414a":"import time\nstart = time.time()\nx_train, y_train = load_CAMVID(data_type='train')\n#x_test, y_test = load_CAMVID(data_type='test')# Don't load test for RAM consumption\nx_val, y_val = load_CAMVID(data_type='val')\nend = time.time()\nprint('Time elapsed: ', end-start)","6eb5e838":"print(x_train.shape)\nprint(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","98a9e97a":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit(x=x_train, \n              y=y_train,\n              validation_data=(x_val, y_val),\n              batch_size=batch_sz,# 32 gives OOM sometimes\n              epochs=epochs,\n              callbacks=[model_checkpoint])","ff16d908":"# img (256,256,3)\n# gt_mask: gt_mode=sparse--> (256,256) or ohe --> (256,256,32)\ndef visualize_seg(img, gt_mask, shape='normal', gt_mode='sparse'):\n  plt.figure(1)\n  \n  # Img\n  plt.subplot(311)\n  plt.imshow(img)\n  \n  # Predict\n  pred_mask = model.predict(np.expand_dims(img, 0))\n  pred_mask = np.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[0]\n  if shape=='flat':\n    pred_mask = np.reshape(pred_mask, (256,256)) # Reshape only if you use the flat model. O.w. you dont need\n  \n  rgb_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(pred_mask, -1))\n  \n  # Prediction\n  plt.subplot(312)\n  plt.imshow(rgb_mask)\n              \n  # GT mask\n  if gt_mode == 'ohe':\n    gt_img_ohe = np.argmax(gt_mask, axis=-1)\n    gt_mask = np.apply_along_axis(map_class_to_rgb, -1, np.expand_dims(gt_img_ohe, -1))              \n  \n  plt.subplot(313)\n  plt.imshow((gt_mask).astype(np.uint8))\n                \n  \n  ","b169828a":"visualize_seg(x_val[100], y_val[100], gt_mode='ohe')","3815a902":"from keras.utils import Sequence\nclass CAMVID_Dataset(Sequence):\n\n\n    def __init__(self, data_path, batch_size=4, dim=(256,256), n_classes=32, data_type='train', shape='normal'):\n\n        self.images_dir = str(data_path) + '\/' + data_type + '\/'\n        self.masks_dir = str(data_path) + '\/' + data_type + '_labels\/'\n        assert len(os.listdir(self.images_dir)) == len(os.listdir(self.masks_dir))\n        self.data_type = data_type\n        self.shape = shape\n        self.batch_size = batch_size\n        self.dim = dim\n        self.n = len(os.listdir(self.images_dir))\n        self.n_batches = int(np.floor(self.n  \/ self.batch_size))\n        self.indexes = np.arange(self.n)\n\n    def __len__(self):\n        return  self.n_batches\n\n    def __getitem__(self, index):\n        X = np.empty((self.batch_size, *self.dim, 3))\n        Y = np.zeros((self.batch_size, *self.dim, n_classes))\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        # Generate data\n        for i, ID in enumerate(indexes):\n          idx = ID\n\n          file = sorted(os.listdir(self.images_dir))[idx]\n\n          # Load image\n          image = np.array(load_img(str(self.images_dir) + file, target_size=(256,256)))*1.\/255\n\n\n          # Load mask\n          file = sorted(os.listdir(self.masks_dir))[idx]\n          mask = adjust_mask(np.array(load_img(str(self.masks_dir) + file, target_size=(256,256))))\n\n          if(self.shape == 'flat'):\n            mask = np.reshape(mask.shape[0], mask.shape[1]*mask.shape[2])\n            mask = np.expand_dims(mask, axis=-1)        \n          X[i,:] = image\n          Y[i,:] = mask\n          \n        return X, Y\n      ","b107e36d":"train_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='train')\nvalid_gen = CAMVID_Dataset(str(data_path), batch_size=batch_sz, n_classes=n_classes, data_type='val')","5f380820":"x,y = next(enumerate(train_gen))[1]\nprint(x.shape)\nprint(y.shape)","355fae52":"n_train_samples = len(os.listdir(str(data_path) + '\/train\/'))\nn_train_samples","0944a192":"\nmodel = unet(n_classes)","383cf11f":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_gen,\n                    validation_data=valid_gen,\n                    steps_per_epoch=n_train_samples,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","2eb26560":"# Data generator\n#https:\/\/keras.io\/preprocessing\/image\/\n# Data generator\n#batch_sz = 4\n\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rescale=1.\/255)\n\n# So our usage here is as data loader instead of loading everything in RAM, not data augmentation\nmask_gen_args = dict()\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['train'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nmask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['train_labels'],\n    class_mode=None,\n    seed=seed,\n    color_mode='rgb',\n    batch_size=batch_sz,\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\n      \n\nval_image_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['val'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nval_mask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['val_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)\n\n      ","f616ea36":"\ndef train_generator_fn():\n\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)        ","7a41f871":"\ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  ","69f45ce0":"n_train_samples = len(os.listdir(str(data_path) + '\/train\/'))\nn_train_samples","4d7308df":"\nmodel = unet(n_classes)","3373751a":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=n_train_samples,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","90b951a3":"visualize_seg(next(val_image_generator)[0], next(val_mask_generator)[0], gt_mode='sparse')","3eec41d3":"# Data generator\n#batch_sz = 4\n#https:\/\/keras.io\/preprocessing\/image\/\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest',\n                    rescale=1.\/255)\n\nmask_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    \n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['train'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nmask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['train_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\ndef train_generator_fn():\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  \n        \nval_image_generator = image_datagen.flow_from_directory(\n    data_path,\n    class_mode=None,\n    classes=['val'],\n    seed=seed,\n    batch_size=batch_sz,\n    target_size=(256,256))\n\nval_mask_generator = mask_datagen.flow_from_directory(\n    data_path,\n    classes=['val_labels'],\n    class_mode=None,\n    seed=seed,\n    batch_size=batch_sz,\n    color_mode='rgb',\n    target_size=(256,256))\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)        \n        \ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)         \n","5ca22366":"img = load_img(str(data_path) + '\/train\/0001TP_006690.png', target_size=(256,256))\nimg","4367e57b":"mask = load_img(str(data_path) + '\/train_labels\/0001TP_006690_L.png', target_size=(256,256))\nmask","c51ff912":"# The .flow() command below generates batches of randomly transformed images.\n# It will loop indefinitely, so we need to `break` the loop at some point!\nfrom keras.preprocessing.image import array_to_img, img_to_array\ni = 0\nimg = img_to_array(img)\nmask = img_to_array(mask)\nfor aug_img, aug_mask in zip(image_datagen.flow(np.expand_dims(img, 0), batch_size=1), mask_datagen.flow(np.expand_dims(mask, 0), batch_size=1)):\n    plt.figure(i)\n    plt.subplot(221)\n    imgplot = plt.imshow(array_to_img(aug_img[0]))\n    plt.subplot(222)\n    imgplot = plt.imshow(array_to_img(aug_mask[0]))\n    i += 1\n    if i > 10:\n        break\n\nplt.show()","9f3647a7":"model = unet(n_classes)\n","298138c6":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=steps_per_epoch,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","487a45b6":"img = next(val_image_generator)[0]\ngt_img = next(val_mask_generator)[0]\nvisualize_seg(img, gt_img, gt_mode='sparse')\n","40419a33":"def load_raw_CAMVID(data_type='train', enc='ohe', shape='normal'):\n  img_path = str(data_path) + '\/' + data_type + '\/'\n  labels_path = str(data_path) + '\/' + data_type + '_labels\/'\n  # without adding target_size=(256,256) in load_img we get Out of mem: 421x960x720x32x4bytes is around 34GB!\n  x = np.array([np.array(load_img(str(img_path) + file, target_size=(256,256)))*1.\/255 for file in sorted(os.listdir(img_path))])\n  if(enc=='ohe'):\n    \n    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n  elif(enc=='sparse_cat'):\n    y = np.array([np.array(load_img(str(labels_path) + file, target_size=(256,256))) for file in sorted(os.listdir(labels_path))])\n  if(shape == 'flat'):\n    y = np.reshape(y.shape[0], y.shape[1]*y.shape[2])\n    y = np.expand_dims(y, axis=-1)\n  return x, y","a60e781f":"import time\nstart = time.time()\nx_train, y_train = load_raw_CAMVID(data_type='train')\n#x_test, y_test = load_raw_CAMVID(data_type='test')# Don't load test for RAM consumption\nx_val, y_val = load_raw_CAMVID(data_type='val')\nend = time.time()\nprint('Time elapsed: ', end-start)\n\nprint(x_train.shape)\nprint(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\nprint(x_val.shape)\nprint(y_val.shape)","b027512c":"# Data generator\n#batch_sz = 4\n#https:\/\/keras.io\/preprocessing\/image\/\nfrom keras.preprocessing.image import ImageDataGenerator\n# we create two instances with the same arguments\n\n# VI Note: use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    #rescale=1.\/255)# Data is already scaled when loaded\n\nmask_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='nearest')\n                    #preprocessing_function=adjust_mask)# This is not possible since the preprocessing_function can only return the same shape as image\n\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen  = ImageDataGenerator(**mask_gen_args) \n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\n#image_datagen.fit(images, augment=True, seed=seed)\n#mask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow(\n    x_train,\n    seed=seed,\n    batch_size=batch_sz)\n\nmask_generator = mask_datagen.flow( \n    y_train,\n    seed=seed,\n    batch_size=batch_sz)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\ndef train_generator_fn():\n\n    for (img,mask) in train_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)  \n        \nval_image_generator = image_datagen.flow(\n    x_val,\n    seed=seed,\n    batch_size=batch_sz)\n\nval_mask_generator = mask_datagen.flow(\n    y_val,\n    seed=seed,\n    batch_size=batch_sz)\n\n# combine generators into one which yields image and masks\nval_generator = zip(val_image_generator, val_mask_generator)        \n        \ndef val_generator_fn():\n\n    for (img,mask) in val_generator:\n        new_mask = adjust_mask(mask)\n        yield (img,new_mask)         \n","b370762b":"model = unet(n_classes)","e4687ebe":"model_checkpoint = ModelCheckpoint('unet_camvid.hdf5', monitor='val_loss',verbose=1, save_best_only=True)\nmodel.fit_generator(train_generator_fn(),\n                    validation_data=val_generator_fn(),\n                    steps_per_epoch=steps_per_epoch,\n                    validation_steps=validation_steps,\n                    epochs=epochs,\n                    callbacks=[model_checkpoint])","4a6facc0":"img = next(val_image_generator)[0]\ngt_img = next(val_mask_generator)[0]\nvisualize_seg(img, gt_img, gt_mode='sparse')","fa902a00":"\n\nLet's take a look on some augmented images and masks","dbe025c5":"Now if you plot the mask again, you will see different colors. For example the red and blue are reversed than before:","86940ba9":"Let's have a look on the masks (the ground truth)","0975fbe3":"__Since we scaled while loading the data, we don't need to scale in the generator__","0073e3f5":"Another solution is to use load_image from keras which uses RGB (it uses PIL under the hood) unlike cv2.imread","73f74079":"# Train","ef6d2167":"## Now let's visualize and explore some samples:","0335b7d8":"Now, let's see which classes we have. This can be found in the original CAMVID [text file](http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamVid\/data\/label_colors.txt). However, under the same repo, the author has dumped it into csv which we will use.","03a8d71d":"This data frame maps the class names to colors.\n\nTo access the colors, we can index the dataframe with its row index name using the .loc operation.\n","944ced16":"# Let's try some samples","f6cdce73":"# Let's try on some samples","19f63204":"As you can see the masks are just colors (L,W,3).\nWhat we actually want is a (L,W) matrix, with each value is from 0 to 31 representing the 32 class labels.","05ee5095":"## Let's test the model on sample images","94d27cb9":"# Try some samples","73a76d2d":"Colors are different from the colors in cls2rgb! Because the order is BGR not RGB when using cv2.imread: https:\/\/stackoverflow.com\/questions\/46898979\/how-to-check-the-channel-order-of-an-image\n\nIf you want to get the same order as in the color mapping of CAMVID, use the cv converted","cc81d098":"Now we are ready to create a map from class name to color"}}