{"cell_type":{"ff218670":"code","ce8b0fda":"code","d67dd619":"code","b5c15c56":"code","1e262c41":"code","b5f6dce3":"code","13218c01":"code","702364f4":"code","5ff7a698":"code","b11b2d67":"code","69d4bf19":"code","cbfcebcf":"code","aa847087":"code","c3b8f717":"code","46701e94":"code","a5c0f92e":"code","d1730827":"code","c5b7a160":"code","88b1b2a1":"code","63deed81":"code","338b200e":"code","8e7ba68a":"code","ec06caeb":"code","31c129e1":"code","e0a82b68":"code","0d2ba89b":"code","452588c2":"code","101f0c86":"code","17c37d72":"code","47089db6":"code","1add3dd5":"code","b6ab16a8":"code","ac61182e":"markdown","8b9c1b74":"markdown","5b7761a1":"markdown","747b35bc":"markdown","45596564":"markdown","dcc47093":"markdown","76138cc9":"markdown","9c380e52":"markdown","e84a7fd0":"markdown","13a89d20":"markdown","445cc2d2":"markdown","e9bc1109":"markdown","77aba899":"markdown","4dc756fa":"markdown","c3fd6708":"markdown","f570c9a4":"markdown","6c16a492":"markdown","0f29eef7":"markdown","596a9761":"markdown","e4028ac1":"markdown","04981d70":"markdown","28235e38":"markdown","c6897e2f":"markdown","5386d0a5":"markdown","830db256":"markdown","d45c7981":"markdown","bdfff17b":"markdown","da317db7":"markdown","294717ba":"markdown","03afb262":"markdown","8165aa81":"markdown","d26f73ca":"markdown","0061cb3f":"markdown","bceb4bcb":"markdown","9b31f223":"markdown","7a8abc97":"markdown","e82a5ea7":"markdown","8f8fa3ca":"markdown","2cdb35d7":"markdown"},"source":{"ff218670":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport itertools\nimport scipy.stats as ss\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","ce8b0fda":"df = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\nprint('Name of the attributes: \\n{}'.format(df.columns))","d67dd619":"print('Size of the dataset: {}'.format(df.shape))","b5c15c56":"description_df = df.describe()\ndescription_df","1e262c41":"for col in description_df:\n    if description_df[col].loc['unique'] < 2:\n        print(('Deleted column: {}'.format(col)))\n        df = df.drop([col], axis=1)","b5f6dce3":"col_names = list(df.columns)\nnum_missing = (df[col_names].isnull()).sum()\nprint('There are {} columns with missing values.'.format(len([i for i in num_missing if i > 0])))","13218c01":"le = LabelEncoder()\nlabel_df = pd.DataFrame()\nfor col in df.columns:\n    label_df[col] = le.fit_transform(df[col])","702364f4":"onehot_df = pd.get_dummies(df)\nonehot_df.head()","5ff7a698":"print(label_df.info())\nprint(onehot_df.info())","b11b2d67":"binary_class = pd.Series(df['class']).value_counts().sort_index()\nsns.barplot(binary_class.index, binary_class.values)\nplt.ylabel(\"Count\")\nplt.xlabel(\"Class\")\nplt.title('Number of poisonous\/edible mushrooms (e = edible, p = poisonous)');","69d4bf19":"plt.figure(figsize=(14,12))\nsns.heatmap(label_df.corr(),linewidths=.1,cmap=\"BrBG\", annot=True)\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nplt.yticks(rotation=0);","cbfcebcf":"df_long = pd.melt(label_df, 'class', var_name='Characteristics')\nfig, ax = plt.subplots(figsize=(12,6))\nplot = sns.violinplot(ax=ax, x='Characteristics', y='value', hue='class', split=True, data=df_long, inner='quartile')\ndf_no_class = label_df.drop(['class'], axis=1)\nplot.set_xticklabels(rotation=90, labels=list(df_no_class.columns));","aa847087":"def cramers_corrected_stat(x,y):\n    \n    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    conf_matrix=pd.crosstab(x, y)\n    chi2 = ss.chi2_contingency(conf_matrix)[0]\n    n = sum(conf_matrix.sum())\n    phi2 = chi2\/n\n    r,k = conf_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    result = np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1)))\n    return round(result,6)\n\ncorrM = np.zeros((len(col_names), len(col_names)))\nfor col1, col2 in itertools.combinations(col_names, 2):\n    idx1, idx2 = col_names.index(col1), col_names.index(col2)\n    corrM[idx1, idx2] = cramers_corrected_stat(label_df[col1], label_df[col2])\n    corrM[idx2, idx1] = corrM[idx1, idx2]\n\ncorr = pd.DataFrame(corrM, index=col_names, columns=col_names)\ncorr['class'].plot(kind='bar', figsize=(8,6), title=\"Association between the binary class and each variable\");","c3b8f717":"new_var = df[['class', 'gill-color']]\nsns.catplot('class', col='gill-color', data=new_var, kind='count', height=2.5, aspect=.8, col_wrap=4, order=['e', 'p']);","46701e94":"new_var = df[['class', 'spore-print-color']]\nsns.catplot('class', col='spore-print-color', data=new_var, kind='count', height=2.5, aspect=.8, col_wrap=4, order=['e', 'p']);","a5c0f92e":"new_var = df[['class', 'odor']]\nsns.catplot('class', col='odor', data=new_var, kind='count', height=2.5, aspect=.8, col_wrap=4, order=['e', 'p']);","d1730827":"X = onehot_df.drop(['class_e', 'class_p'], axis=1)\ny = df['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","c5b7a160":"# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n# evaluate the model\ny_pred = model.predict(X_test)\n# evaluate predictions\nprint('Accuracy (all features): %.2f' % (accuracy_score(y_test, y_pred)))","88b1b2a1":"# feature selection\ndef select_features(X_train, y_train, X_test, method):\n    fs = SelectKBest(score_func=method, k=25)\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs","63deed81":"# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, chi2)\n# what are scores for the features\nfeature_scores = [tup for tup in zip(fs.scores_, X.columns)]\nprint('Top 10 chi-squared features: \\n')\nfor i, j in sorted(feature_scores, key=lambda tup: tup[0], reverse=True)[:10]:\n    print('%s: %f' % (j, i))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.title('Chi-square Feature Importance')\nplt.show()","338b200e":"# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train_fs, y_train)\n# evaluate the model\ny_pred = model.predict(X_test_fs)\n# evaluate predictions\nprint('Accuracy (chi-squared features): %.2f' % (accuracy_score(y_test, y_pred)))","8e7ba68a":"# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test, mutual_info_classif)\n# what are scores for the features\nfeature_scores = [tup for tup in zip(fs.scores_, X.columns)]\nprint('Top 10 Mutual Information features: \\n')\nfor i, j in sorted(feature_scores, key=lambda tup: tup[0], reverse=True)[:10]:\n    print('%s: %f' % (j, i))\n# plot the scores\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.title('Mutual Information Feature Importance')\nplt.show()","ec06caeb":"# fit the model\nmodel = LogisticRegression()\nmodel.fit(X_train_fs, y_train)\n# evaluate the model\ny_pred = model.predict(X_test_fs)\n# evaluate predictions\nprint('Accuracy (Mutual Information features): %.2f' % (accuracy_score(y_test, y_pred)))","31c129e1":"descr = label_df.describe()\ndescr.loc['std'].sort_values(ascending=False)","e0a82b68":"models = [SVC(kernel='rbf', random_state=0), SVC(kernel='linear', random_state=0), XGBClassifier(), LogisticRegression(), KNeighborsClassifier()]\nmodel_names = ['SVC_rbf', 'SVC_linear', 'xgboost', 'Logistic Regression', 'KNeighbors Classifier']\nfor i, model in enumerate(models):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print ('Accurancy of ' + model_names[i] + ': ' + str(accuracy_score(y_test, y_pred)))","0d2ba89b":"feature_scores = [tup for tup in zip(models[3].coef_[0], X.columns)]\nprint('Top 10 Logistic Regression features: \\n')\nfor i, j in sorted(feature_scores, key=lambda tup: tup[0], reverse=True)[:10]:\n    print('%s: %f' % (j, i))","452588c2":"feature_scores = [tup for tup in zip(models[2].feature_importances_, X.columns)]\nprint('Top 10 xgboost features: \\n')\nfor i, j in sorted(feature_scores, key=lambda tup: tup[0], reverse=True)[:10]:\n    print('%s: %f' % (j, i))","101f0c86":"tree_clf = DecisionTreeClassifier().fit(X_train, y_train)\nprint ('Accuracy of Decision Tree Classifier: ' + str(accuracy_score(y_test, tree_clf.predict(X_test))))\n\nimportances = tree_clf.feature_importances_\nfeature_scores = [tup for tup in zip(importances, X.columns)]\nprint('Top 10 Decision Tree features: \\n')\nsorted_scores = sorted(feature_scores, key=lambda tup: tup[0], reverse=True)\nfor i, j in sorted_scores[:10]:\n    print('%s: %f' % (j, i))\n    \n# plot the scores\nplt.bar([j for i, j in sorted_scores[:13]], sorted(importances, reverse=True)[:13])\nplt.title('Decision Tree Feature Importance')\nplt.xticks(rotation=90)\nplt.show()","17c37d72":"print('Double-click on the image to enlarge it')\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6,6), dpi=600)\nplot_tree(tree_clf,\n            feature_names=list(X_train.columns), \n            class_names=['poisonous', 'edible'],\n            filled=True);","47089db6":"RR_model = RandomForestClassifier()\n\nparameters = {'min_samples_leaf': range(10,100,10), \n                    'n_estimators' : range(10,100,10),\n                    'criterion' : ['gini', 'entropy'],\n                    'max_features':['auto','sqrt','log2']\n                    }\n\nRR_model = RandomizedSearchCV(RR_model, parameters, cv=10, scoring='accuracy', n_iter=20, n_jobs=-1)\nRR_model.fit(X_train,y_train)","1add3dd5":"y_pred = RR_model.predict(X_test)\nprint(classification_report(y_test, y_pred))","b6ab16a8":"scores_df = pd.DataFrame(RR_model.cv_results_)\nscores_df = scores_df.sort_values(by=['rank_test_score']).reset_index(drop='index')\nscores_df.head()","ac61182e":"spore-print-color: black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y","8b9c1b74":"Make predictions and display scoring metrics","5b7761a1":"Let's plot the three most correlated features according to each individual characteristics and see how they relate to real-world characteristics. This might come in handy the next time you are going shroom hunting.","747b35bc":"Let's see how some of the most common classification models perform on this task.","45596564":"We can display the results report using pandas. This comprehensive report may be useful to inspect all the parameter settings and do a narrower grid search using the most optimal combinations of parameters.","dcc47093":"# Conclusion\n\nWe have surveyed the various steps of a data science project and how to deal with categorical variables. While this dataset is rather old and modern predictors easily reach 100\\% accuracy, it still serves as an interesting basis for exploratory analysis.\n\nIf you do decide to go mushroom hunting next season, please only pick those you can identify with confidence.\n\nThat's it, thanks for reading!","76138cc9":"## Binary Classification Problem: Is this mushroom poisonous or not? \n\nUsing the [mushroom dataset], we will study how a machine learning model can predict whether a mushroom is poisonous or not given its features.\nWhile this dataset  was originally curated more than 30 years ago by the UCI Machine Learning repository, mushroom hunting is still relevant to this day and the data provides for a good case-study of modeling with categorical variables only.\n\nIn this notebook, we will survey the various steps of a data science project:\n- Data Preparation\n- Exploratory Data Analysis\n- Feature Selection\n- Data Modeling\n- Hyperparameter Tuning \n\nBy doing so we will find out which features are most indicative of a poisonous mushroom and see which machine learning models perform best for this task.\n\n[mushroom dataset]: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mushroom","9c380e52":"# Data Modeling\n\nWhen it comes to machine learning algorithms, there is no one-size-fits-all. Depending on the task and the data, different models might perform better. Thus, the best approach is to try various models, tune them and compare how they perform.","e84a7fd0":"Checking for missing values","13a89d20":"### Imports","445cc2d2":"# Hyperparameter Tuning\n\nFor the sake of demonstration, we will perform hyperparameter tuning, the final step that would help bring up the performance of the predictor. We can find which combination of hyperparameters performs best by using a randomized search. RandomizedSearchCV samples from a set of pre-defined hyperparameters and fits various models with different combinations of paramaters using k-fold cross-validation. The most accurate model can then be retrieved.\n\nWe will use a Random Forest Classifier, an ensemble method that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control overfitting.","e9bc1109":"We convert the data from categorical to ordinal using LabelEncoder from scikit-learn. We store the mappings of each value with its ordinal representation in a dictionary for later use.","77aba899":"# Exploratory Data Analysis \n\nThis process can be likened to getting to know our data. By analysing how the features relate to each other, we gain crucial knowledge about the data which will helps us train efficient models.","4dc756fa":"We can observe that different models use the data in different ways, though there are some similarities in what features are considered most important.","c3fd6708":"Model built using Mutual Information features:","f570c9a4":"To be more systematic, we can check for correlation between the categorical features. Cramer's V statistical test allows us to measure the strength of the association between categorical variables. It is based on Pearson's chi-squared statistic and has the advantage of giving good norming from 0 to 1 regardless of table size. (Code taken from https:\/\/stackoverflow.com\/questions\/20892799\/using-pandas-calculate-cram%C3%A9rs-coefficient-matrix)","6c16a492":"This plot confirms our intuition and reveals other relevant features that were less obvious such as 'odor'.","0f29eef7":"One important thing to consider first is whether the dataset is balanced, i.e. whether the dataset contains the same number of samples from the positive and negative class.","596a9761":"# Dataset\n\nThis dataset is known as the mushroom dataset. It includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. The field guide from which the mushroom records were drawn clearly states that there is no simple rule for determining the edibility of a mushroom, and it is common knowledge that if you are not able to identify a mushroom with confidence, you should not pick it. However, for experimentation purposes, we will study how well various machine learning models can predict whether a mushroom is poisonous or not given its physical characteristics. \n\nThere are 22 attributes, which are all categorical, and 8124 training instances. The goal is to predict the binary class 'poisonous' or 'edible'. ","e4028ac1":"By the looks of this plot, two variables seem to have a strong influence on whether a mushroom is poisonous or not: 'gill-color', and 'spore-print-color'.","04981d70":"# Beware of foul-smelling mushrooms","28235e38":"Now let's see how a simple Decision Tree Classifier does. This model has the advantage of being highly interpretable and is well-suited for categorical variables. In this tree, each node represents a feature and the leaves correspond to the predicted outcome.","c6897e2f":"gill-color: black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y \n\nWatch out for buff (light brownish yellow) gills!","5386d0a5":"Splitting training and testing data","830db256":"Using the describe() method, we can retrieve the number of unique values for each column in a dataframe. We delete columns with only one value, which add no information for this classification task.","d45c7981":"Model built using all features","bdfff17b":"Are there any correlated features? We could consider deleting one of two features that are strongly correlated, as they contain similar information. Correlated features in general don't improve models, but they affect specific models in different ways and to varying extents:\n- For linear models (e.g., linear regression or logistic regression), multicolinearity can yield solutions that are wildly varying and possibly numerically unstable.\n- Random forests can be good at detecting interactions between different features, but highly correlated features can mask these interactions.\n\nMore generally, a simpler model is preferable, and, in some sense, a model with fewer features is simpler.\n\nPlease note that this is just an indication and might not be entirely reliable as we are using categorical variables encoded as labels.","da317db7":"Model built using chi-squared features","294717ba":"Double checking our cleaned data","03afb262":"In this case, 'gill-attachment' is strongly correlated with 'veil-color' (0.9).","8165aa81":"It turns out this problem is easily solved by modern predictors. For more interpretability, let's find out which features are given more importance by various models.","d26f73ca":"odor: almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s\n\nYou know what to do if a mushroom is foul-smelling...","0061cb3f":"Such integer representation can, however, not be used directly with estimators that expect continuous input and would interpret the categories as being ordered, which is not desired (i.e. the set of browsers was ordered arbitrarily).\n\nAnother possibility to convert categorical features to features is to use a one-hot encoding. This type of encoding can be obtained by transforming each categorical feature with n_categories possible values into n_categories binary features, with one of them 1, and all others 0.","bceb4bcb":"Both methods are selecting the same features as most relevant with some variations. However they both result in a worsened accuracy. This can be explained by the fact that the number of features in the dataset is relatively low to begin with.  ","9b31f223":"# Data Preparation\n\nFirst, we need to load the data and clean it. The cleaning process in our case will include:\n- Deleting columns that contain a single value\n- Checking for missing values\n- Converting categorical values to ordinal values","7a8abc97":"Plotting the data using a violin plot allows us to identify which characteristics might be relevant for classification.","e82a5ea7":"# Feature Selection\n\nFeature selection is the automatic selection of features that are most relevant to a predictive modeling problem.\n\nThe objective of feature selection is three-fold:\n- improving the prediction performance of the models\n- providing faster and more cost-effective models\n- providing a better understanding of the underlying process that generated the data\n\nThere are three general classes of feature selection algorithms: filter methods, wrapper methods and embedded methods. Here we will focus on filter methods and show how they can be implemented using scikit-learn.\n\nHaving irrelevant features in the data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression. Let's see how using filter methods such as the Chi-squared test and information gain and selecting the top 25 features affects the prediction performance.","8f8fa3ca":"Another filtering method is to ignore features with low variance. These features can be dismissed as they do not contribute to the model's accuracy. Here we retrieve the standard deviation from the ordinally encoded features. We could then set a threshold below which features would be dismissed and test if the model performs better with fewer features.","2cdb35d7":"Interestingly, only three features account for about 90% of the information the Decision Tree Classifier needs to achieve 100% accuracy. The feature 'odor-n' (n = none) is a key deciding factor for this task.\n\nWe can visualize and interpret how the tree classifier makes decisions by plotting it."}}