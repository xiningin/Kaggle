{"cell_type":{"9d7c007e":"code","06ebe2db":"code","fd49c291":"code","27a7e7db":"code","b5abbacd":"code","dc32fda8":"code","ba032702":"code","ccaa14fc":"code","b2af49ac":"markdown","c4ad179c":"markdown","69caeef3":"markdown","aa059dad":"markdown","e66a39b6":"markdown","4b6f4665":"markdown","15be4853":"markdown","c26568bf":"markdown"},"source":{"9d7c007e":"# Imports\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport tensorflow as tf\nimport keras\n\nfrom keras.models import Model\nfrom keras.layers import Input, Layer, Activation, Dense, Flatten, Dropout, Lambda, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, SpatialDropout2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras import losses\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfrom PIL import Image\nfrom matplotlib.pyplot import imshow\nfrom random import randrange","06ebe2db":"os.listdir('..\/input\/pokemon-mugshots-from-super-mystery-dungeon')","fd49c291":"# Load data.\ndata = []\npaths = []\nfor r, d, f in os.walk(r'..\/input\/pokemon-mugshots-from-super-mystery-dungeon\/smd\/'):\n    for file in f:\n        if '.png' in file:\n            paths.append(os.path.join(r, file))\n            \nfor path in paths:\n    img = Image.open(path)\n    x = np.array(img)\n    x = x[...,:3]\n    if(x.shape == (64,64,3)):\n        data.append(x)\n\nsrc = np.array(data)\nsrc = src.reshape(len(data),64,64,3)\n\n\n#splitting data into train and test sets\nx_train,x_test,y_train,y_test = train_test_split(src, src, test_size=0.2, shuffle=True, random_state=69)\n\n# Normalize data.\nx_test = x_test \/ np.max(x_train)\nx_train = x_train \/ np.max(x_train)","27a7e7db":"# VQ layer.\nclass VQVAELayer(Layer):\n    def __init__(self, embedding_dim, num_embeddings, commitment_cost,\n                 initializer='uniform', epsilon=1e-10, **kwargs):\n        self.embedding_dim = embedding_dim\n        self.num_embeddings = num_embeddings\n        self.commitment_cost = commitment_cost\n        self.initializer = initializer\n        super(VQVAELayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        # Add embedding weights.\n        self.w = self.add_weight(name='embedding',\n                                  shape=(self.embedding_dim, self.num_embeddings),\n                                  initializer=self.initializer,\n                                  trainable=True)\n\n        # Finalize building.\n        super(VQVAELayer, self).build(input_shape)\n\n    def call(self, x):\n        # Flatten input except for last dimension.\n        flat_inputs = K.reshape(x, (-1, self.embedding_dim))\n\n        # Calculate distances of input to embedding vectors.\n        distances = (K.sum(flat_inputs**2, axis=1, keepdims=True)\n                     - 2 * K.dot(flat_inputs, self.w)\n                     + K.sum(self.w ** 2, axis=0, keepdims=True))\n\n        # Retrieve encoding indices.\n        encoding_indices = K.argmax(-distances, axis=1)\n        encodings = K.one_hot(encoding_indices, self.num_embeddings)\n        encoding_indices = K.reshape(encoding_indices, K.shape(x)[:-1])\n        quantized = self.quantize(encoding_indices)\n\n        return quantized\n\n    @property\n    def embeddings(self):\n        return self.w\n\n    def quantize(self, encoding_indices):\n        w = K.transpose(self.embeddings.read_value())\n        return tf.nn.embedding_lookup(w, encoding_indices, validate_indices=False)","b5abbacd":"# Calculate vq-vae loss.\ndef vq_vae_loss_wrapper(data_variance, commitment_cost, quantized, x_inputs):\n    def vq_vae_loss(x, x_hat):\n        recon_loss = losses.mse(x, x_hat) \/ data_variance\n        \n        e_latent_loss = K.mean((K.stop_gradient(quantized) - x_inputs) ** 2)\n        q_latent_loss = K.mean((quantized - K.stop_gradient(x_inputs)) ** 2)\n        loss = q_latent_loss + commitment_cost * e_latent_loss\n        \n        return recon_loss + loss #* beta\n    return vq_vae_loss","dc32fda8":"# Hyper Parameters.\nepochs = 2 # MAX\nbatch_size = 500\nvalidation_split = 0.2\n\n# VQ-VAE Hyper Parameters.\nembedding_dim = 32 # Length of embedding vectors.\nnum_embeddings = 128 # Number of embedding vectors (high value = high bottleneck capacity).\ncommitment_cost = 0.25 # Controls the weighting of the loss terms.","ba032702":"# Encoder\ninput_img = Input(shape=(64, 64, 3))\nx = Conv2D(32, (3, 3), activation='relu')(input_img)\nx = Conv2D(32, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\nx = MaxPooling2D(pool_size=(2, 2))(x)\nx = Conv2D(64, (3, 3), activation='relu')(x)\n# VQVAELayer.\nenc = Conv2D(embedding_dim, kernel_size=(1, 1), strides=(1, 1), name=\"pre_vqvae\")(x)\nenc_inputs = enc\n\n#uncomment these lines when you run locally\n#enc = VQVAELayer(embedding_dim, num_embeddings, commitment_cost, name=\"vqvae\")(enc)\n#x = Lambda(lambda enc: enc_inputs + K.stop_gradient(enc - enc_inputs), name=\"encoded\")(enc)\n\ndata_variance = np.var(x_train)\nloss = vq_vae_loss_wrapper(data_variance, commitment_cost, enc, enc_inputs)\n\n# Decoder.\nx = Conv2DTranspose(64, (3, 3), activation='relu')(x)\nx = UpSampling2D()(x)\nx = Conv2DTranspose(32, (3, 3), activation='relu')(x)\nx = UpSampling2D()(x)\nx = Conv2DTranspose(32, (3, 3), activation='relu')(x)\nx = Conv2DTranspose(3, (3, 3))(x)\n\n# Autoencoder.\nvqvae = Model(input_img, x)\nvqvae.compile(loss=loss, optimizer='adam')\n#vqvae.summary()\n# this model maps an input to its encoded representation. Saving it as it's own model\nencoder = Model(input_img, enc)\n#encoder.summary()","ccaa14fc":"history = vqvae.fit(x_train, x_train,batch_size=batch_size, epochs=epochs,validation_data=(x_test, x_test))","b2af49ac":"# Training\nTrained locally for 250 epochs\n![training](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/training.JPG)","c4ad179c":"# Data Preprocessing\nThe images are all pngs so I am using `x = x[...,:3]` to get rid of the alpha channel","69caeef3":"The training is too time consuming for the online notebook. The results from training done locally are posted below","aa059dad":"# Model Summary\n\nI couldn't get the custom layer to work on a Kaggle notebook so here is the local version\n\n![model](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/model.JPG)","e66a39b6":"# Custom VQVAE Keras Layer\nVery nicely written custom layer provided by Github user [HenningBuhl](https:\/\/github.com\/HenningBuhl\/VQ-VAE_Keras_Implementation\/blob\/master\/VQ_VAE_Keras_MNIST_Example.ipynb)","4b6f4665":"# Pokemon-VQVAE\n* Encoding and Recreating Images using Vector Quantized Variational AutoEncoders\n* `64x64x3` images are encoded into `12x12x32`==`64x24x3` latent space and transformed back into `64x64x3` images\n* The Vector Quantized representation is `37.5%` the size of the original image\n\n# Work Based on\n* [Neural Discrete Representation Learning](https:\/\/arxiv.org\/abs\/1711.00937)\n* [VQ-VAE Keras Implementation](https:\/\/github.com\/HenningBuhl\/VQ-VAE_Keras_Implementation)\n* [Kaggle Dataset](https:\/\/www.kaggle.com\/brilja\/pokemon-mugshots-from-super-mystery-dungeon)\n","15be4853":"# Results\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/label0.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/label1.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_2.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_3.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_4.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_5.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_6.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_7.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_8.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_9.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_0.png)\n![r](https:\/\/raw.githubusercontent.com\/vee-upatising\/Pokemon-VQVAE\/master\/results\/image_11.png)\n\n# [View Jupyter Notebook](https:\/\/nbviewer.jupyter.org\/github\/vee-upatising\/Pokemon-VQVAE\/blob\/master\/VQVAE.ipynb)\n\n","c26568bf":"Model will take a `64x64x3` image and encode it in into a tensor of size `12x12x32`, then upsample back to `64x64x3`"}}