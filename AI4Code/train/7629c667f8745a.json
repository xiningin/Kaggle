{"cell_type":{"d91bf433":"code","b77818e2":"code","b62522f2":"code","1977b294":"code","c6d8d1b9":"code","1a592141":"code","5638a49c":"code","7818ec1d":"code","94b950ea":"code","ff87cdda":"code","1b57bf42":"code","8f3f09a5":"code","ff54cfd8":"code","d40737c3":"code","524426d0":"code","786ea3d0":"code","aeb1dc2b":"code","cbf71a2e":"code","c72b37e3":"code","9b7c7c5c":"code","b064ae24":"code","39d7563a":"code","5d907ab7":"code","241d2187":"code","15aa1386":"code","75fcc8ec":"code","1f012b2a":"code","5e989810":"code","29bb6075":"code","7d6f9a98":"code","e9d37ef7":"code","8fcb55c6":"code","5a758a54":"code","ec004a7d":"code","36ac88d8":"code","80dbedab":"code","bc2c0b3b":"code","aa4a6c55":"code","b7002f33":"code","b1f2e34b":"code","17f485d5":"code","ccd5b37e":"code","8805fe01":"code","9777c925":"code","41cdb90c":"code","0c502903":"code","aaca3e49":"code","5b93282a":"code","a8fb7c16":"code","f3f630ec":"code","434c19d2":"code","c8b07352":"code","8ef5df3a":"code","15013adf":"code","eb522a31":"code","26b6122a":"code","d99d240a":"code","076a2d56":"code","ebcec0d1":"code","885456b0":"code","1f979dc3":"code","f4dd32c6":"code","eb068d91":"code","d4ba2825":"code","e6c668ce":"markdown","43d3ad1b":"markdown","dcb65a55":"markdown","aa4f4c96":"markdown","9a058ee9":"markdown","4e030c03":"markdown","338964f6":"markdown","e6f4df29":"markdown","c117b769":"markdown","02c55a81":"markdown","cf9be76f":"markdown","c5629aa6":"markdown","67c31593":"markdown"},"source":{"d91bf433":"%pylab inline\n\nimport seaborn as sns\nimport pandas as pd\nimport lifetimes","b77818e2":"np.random.seed(42)\n\nimport random\nrandom.seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (12, 8) \nplt.rcParams[\"figure.dpi\"] = 60 \n\n# sns.set(style=\"ticks\")\n# sns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 5})","b62522f2":"orders = pd.read_csv('..\/input\/olist_orders_dataset.csv')\ncustomers = pd.read_csv('..\/input\/olist_customers_dataset.csv')\npayments = pd.read_csv('..\/input\/olist_order_payments_dataset.csv')\norder_items = pd.read_csv(\"..\/input\/olist_order_items_dataset.csv\")\n\ncols = ['customer_id', 'order_id', 'order_purchase_timestamp']\norders = orders[cols]\norders = orders.set_index('customer_id')\norders.drop_duplicates(inplace=True)\norders[\"order_purchase_timestamp\"] = pd.to_datetime(orders[\"order_purchase_timestamp\"])\n\n# aggregate cost of items\ncosts = order_items.groupby(\"order_id\")[\"price\"].sum()\n\ncols = ['customer_id', 'customer_unique_id']\ncustomers = customers[cols]\ncustomers = customers.set_index('customer_id')\n\ntransactions = pd.concat([orders,customers], axis=1, join='inner')\ntransactions.reset_index(inplace=True)\n\ncols = ['customer_unique_id', 'order_id','order_purchase_timestamp']\ntransactions = transactions[cols]\n\ntransactions['order_purchase_timestamp'] = pd.to_datetime(transactions['order_purchase_timestamp'])\ntransactions['order_date'] = transactions.order_purchase_timestamp.dt.date\ntransactions['order_date'] = pd.to_datetime(transactions['order_date'])\n\ncols = ['customer_unique_id', 'order_id', 'order_date']\ntransactions = transactions[cols]","1977b294":"products = pd.read_csv(\"..\/input\/olist_products_dataset.csv\")\nsellers = pd.read_csv(\"..\/input\/olist_sellers_dataset.csv\")","c6d8d1b9":"transactions = transactions.merge(costs.to_frame(\"total_cost\").reset_index(), on='order_id')\ntransactions.columns = [\"customer_id\", \"order_id\", \"order_date\", \"total_cost\"]","1a592141":"transactions.order_date.value_counts().plot()","5638a49c":"vc_num_purchases_by_customer = transactions.groupby(\"customer_id\")[\"order_id\"].nunique().value_counts()\nvc_num_purchases_by_customer = vc_num_purchases_by_customer.sort_index()\nnunique_items_per_customer = transactions.merge(order_items).groupby(\"customer_id\")[\"product_id\"].nunique()\nvc_num_items_per_customer = nunique_items_per_customer.value_counts()\nvc_num_items_per_customer = vc_num_items_per_customer.sort_index()\n\nprint(\"Average number of purchases: {:.2f}\".format(transactions.groupby(\"customer_id\")[\"order_id\"].nunique().mean()))\nprint(\"Average number of items bought per customer: {:.2f}\".format(\n    nunique_items_per_customer.mean()))\n\n\n# plot of number of orders per customer\nax = plt.figure().add_subplot(211)\nvc_num_purchases_by_customer.plot.bar(color='dodgerblue', ax=ax)\n\npct_cumsum_vc = vc_num_purchases_by_customer.cumsum() \/ vc_num_purchases_by_customer.sum()\n\nax2=ax.twinx()\nax.set_ylabel(\"Number of customers\")\nax.set_xlabel(\"Number of purchases\")\nax2.set_ylabel(\"Cumulative percent\")\nax2.plot(range(len(vc_num_purchases_by_customer)), pct_cumsum_vc, linestyle='--', color='salmon')\n\n# plot of number of items bought per customer\nax = plt.figure().add_subplot(212)\nvc_num_items_per_customer.plot.bar(color='dodgerblue', ax=ax)\n\npct_cumsum_vc = vc_num_items_per_customer.cumsum() \/ vc_num_items_per_customer.sum()\n\nax2=ax.twinx()\nax.set_ylabel(\"Number of customers\")\nax.set_xlabel(\"Number of items bought\")\nax2.set_ylabel(\"Cumulative percent\")\nax2.plot(range(len(vc_num_items_per_customer)), pct_cumsum_vc, linestyle='--', color='salmon')","7818ec1d":"np.max(transactions.order_date) - np.min(transactions.order_date)","94b950ea":"from lifetimes.utils import calibration_and_holdout_data\n\ncalibration_period_ends = '2018-06-30'\n\nsummary_cal_holdout = calibration_and_holdout_data(transactions, \n                                                   customer_id_col = 'customer_id', \n                                                   datetime_col = 'order_date', \n                                                   freq = 'D',\n                                        calibration_period_end=calibration_period_ends,\n                                        observation_period_end='2018-09-28', monetary_value_col='total_cost' )","ff87cdda":"summary_cal_holdout[:2]","1b57bf42":"from lifetimes import BetaGeoFitter, ModifiedBetaGeoFitter\nmbgf = ModifiedBetaGeoFitter()\nmbgf.fit(summary_cal_holdout[\"frequency_cal\"], summary_cal_holdout[\"recency_cal\"], summary_cal_holdout[\"T_cal\"], \n         iterative_fitting=3, verbose=True)","8f3f09a5":"from lifetimes.plotting import plot_period_transactions\nax = plot_period_transactions(mbgf, max_frequency=7)\nax.set_yscale('log')\nsns.despine();","ff54cfd8":"from lifetimes.plotting import plot_frequency_recency_matrix, plot_probability_alive_matrix, plot_expected_repeat_purchases\n\n\nplt.figure(figsize=(10, 10))\nplot_frequency_recency_matrix(mbgf, T=120, );\n\nplt.figure(figsize=(10, 10))\nplot_probability_alive_matrix(mbgf);","d40737c3":"n = 7\n\nsummary = summary_cal_holdout.copy()\nduration_holdout = summary.iloc[0]['duration_holdout']\n\nsummary['model_predictions'] = summary.apply(lambda r: mbgf.conditional_expected_number_of_purchases_up_to_time(\n    duration_holdout, r['frequency_cal'], r['recency_cal'], r['T_cal']), axis=1)\nagg_data = summary.groupby(\"frequency_cal\")[['frequency_holdout', 'model_predictions']].mean()\nax = agg_data.iloc[:n].plot(title=\"Pearson correlation (calibration & holdout): {:.4f}\".format(agg_data.corr().min().min()))\nax.set_xticks(agg_data.iloc[:n].index);","524426d0":"t = 120\npredicted_purchases = mbgf.conditional_expected_number_of_purchases_up_to_time(t, \n                                                                               summary_cal_holdout['frequency_cal'], \n                                                                               summary_cal_holdout['recency_cal'],\n                                                                               summary_cal_holdout['T_cal'])\npredicted_purchases.sort_values().tail(4)","786ea3d0":"from lifetimes.plotting import plot_history_alive\n\nfig = plt.figure(figsize=(15, 10))\n\nfor idx, customer_id in enumerate(predicted_purchases.sort_values().tail(4).index, 1):\n    # all days\n    days_since_birth = (max(transactions.order_date - min(transactions.order_date))).days\n    sp_trans = transactions.loc[transactions['customer_id'] == customer_id]\n    \n    plot_history_alive(mbgf, days_since_birth, sp_trans, 'order_date', ax=fig.add_subplot(2, 2, idx))","aeb1dc2b":"# weak correlation between monetary and frequency\nreturning_customers_summary = summary_cal_holdout[summary_cal_holdout[\"frequency_cal\"] > 0]\nprint(returning_customers_summary[[\"frequency_cal\", \"monetary_value_cal\"]].corr())\n\nfig, axes = plt.subplots(1,2,figsize=(12, 5))\nsns.distplot(returning_customers_summary[\"monetary_value_cal\"], ax=axes[0], )\nsns.distplot(np.log(returning_customers_summary[\"monetary_value_cal\"] + 1), ax=axes[1], axlabel='$log(monetary\\_value)$')","cbf71a2e":"from lifetimes import GammaGammaFitter\n\ngg = GammaGammaFitter()\ngg.fit(returning_customers_summary[\"frequency_cal\"], \n       returning_customers_summary[\"monetary_value_cal\"], verbose=True)","c72b37e3":"expected_average_profit_validation = gg.conditional_expected_average_profit(\n    summary_cal_holdout['frequency_holdout'], summary_cal_holdout['monetary_value_holdout'])\n\nexpected_average_profit = gg.conditional_expected_average_profit(\n    summary_cal_holdout['frequency_cal'], summary_cal_holdout['monetary_value_cal'])\n\nprint(\"With non-repeat buyers\")\nprint(\"Train correlation\")\nprint(pd.Series.corr(summary_cal_holdout[\"monetary_value_cal\"], expected_average_profit).round(4))\n\nprint(\"Validation correlation\")\nprint(pd.Series.corr(summary_cal_holdout[\"monetary_value_holdout\"], expected_average_profit_validation).round(4))","9b7c7c5c":"expected_average_profit_validation = gg.conditional_expected_average_profit(\n    returning_customers_summary['frequency_holdout'], returning_customers_summary['monetary_value_holdout'])\n\nexpected_average_profit = gg.conditional_expected_average_profit(\n    returning_customers_summary['frequency_cal'], returning_customers_summary['monetary_value_cal'])\n\nprint(\"Repeat buyers only\")\nprint(\"Train correlation\")\nprint(pd.Series.corr(returning_customers_summary[\"monetary_value_cal\"], expected_average_profit).round(4))\n\nprint(\"Validation correlation\")\nprint(pd.Series.corr(returning_customers_summary[\"monetary_value_holdout\"], expected_average_profit_validation).round(4))","b064ae24":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nmean_absolute_error(summary_cal_holdout[\"monetary_value_holdout\"], expected_average_profit_validation)","39d7563a":"print(\"Expected average profit validation: {:.2f}, Average profit validation: {:.2f}\".format(\n    gg.conditional_expected_average_profit(\n        summary_cal_holdout['frequency_cal'],\n        summary_cal_holdout['monetary_value_cal']\n    ).mean(),\n    summary_cal_holdout[summary_cal_holdout['frequency_cal']>0]['monetary_value_cal'].mean()))","5d907ab7":"returning_customers_summary[:2]","241d2187":"from sklearn.model_selection import train_test_split\n\ntrain_cols = [\"frequency_cal\", \"recency_cal\", \"T_cal\", \"monetary_value_cal\"]\nX_train = returning_customers_summary[train_cols].values\ny_train = returning_customers_summary[\"frequency_cal\"].values\n\ntest_cols = [\"frequency_cal\", \"recency_cal\", \"T_cal\", \"monetary_value_holdout\"]\nX_test = returning_customers_summary[test_cols].values\ny_test = returning_customers_summary[\"frequency_holdout\"].values","15aa1386":"import statsmodels.api as sm\n\nX_train_constant = sm.add_constant(X_train, prepend=False)\n\nmod = sm.OLS(y_train, X_train_constant)\nres = mod.fit()\nprint(res.summary())","75fcc8ec":"from sklearn.svm import SVR\n\nsvr_model = SVR()\n\nsvr_model.fit(X_train, y_train)\npreds = svr_model.predict(X_test)\n\nn = 30\n\nsummary = returning_customers_summary.copy()\nduration_holdout = summary.iloc[0]['duration_holdout']\n\nsummary['model_predictions'] = preds\nagg_data = summary.groupby(\"frequency_cal\")[['frequency_holdout', 'model_predictions']].mean()\nax = agg_data.iloc[:n].plot(title=\"Pearson correlation (calibration & holdout): {:.4f}\".format(agg_data.corr().min().min()))\nax.set_xticks(agg_data.iloc[:n].index);","1f012b2a":"orders = pd.read_csv('..\/input\/olist_orders_dataset.csv')\ncustomers = pd.read_csv('..\/input\/olist_customers_dataset.csv')\npayments = pd.read_csv('..\/input\/olist_order_payments_dataset.csv')\norder_items = pd.read_csv(\"..\/input\/olist_order_items_dataset.csv\")\n","5e989810":"print(\"Orders\")\ndisplay(orders[:2])\nprint(\"Customers\")\ndisplay(customers[:2])\nprint(\"Payments\")\ndisplay(payments[:2])\nprint(\"Orders-Items\")\ndisplay(order_items[:2])\nprint(\"Items\")\ndisplay(products[:2])\nprint(\"Sellers\")\ndisplay(sellers[:2])","29bb6075":"import featuretools as ft\n\nes = ft.EntitySet(id = 'customers')\n\nes = es.entity_from_dataframe(entity_id = 'customers', dataframe = customers[[\"customer_id\", \"customer_city\", \"customer_state\"]], \n                              index = 'customer_id',)\n\nes = es.entity_from_dataframe(entity_id = 'orders', dataframe = orders.reset_index(),\n                              index = 'order_id', \n                              time_index = 'order_purchase_timestamp')\n\nes = es.entity_from_dataframe(entity_id = 'order_products', \n                              dataframe = order_items[[\"order_id\", \"product_id\", \"seller_id\", \"price\", \"freight_value\"]],\n                              index = 'order_product_id', make_index=True)\n\nes = es.entity_from_dataframe(entity_id = 'products', \n                              dataframe = products,\n                              index = 'product_id', )\n\nes = es.entity_from_dataframe(entity_id = 'sellers', \n                              dataframe = sellers[[\"seller_id\", \"seller_city\", \"seller_state\"]],\n                              index = 'seller_id', )\n\nes = es.entity_from_dataframe(entity_id = 'payments', \n                              dataframe = payments[[\"order_id\", \"payment_type\", \"payment_installments\", \"payment_value\"]],\n                              index = 'payment_id', make_index=True)","7d6f9a98":"# Add the relationship to the entity set\n# customer to orders\nes = es.add_relationship(ft.Relationship(es['customers']['customer_id'],\n                                    es['orders']['customer_id']))\n# orders to order products\nes = es.add_relationship(ft.Relationship(es['orders']['order_id'],\n                                    es['order_products']['order_id']))\n# products to order products\nes = es.add_relationship(ft.Relationship(es['products']['product_id'], \n                                         es['order_products']['product_id']))\n# sellers to order products\nes = es.add_relationship(ft.Relationship(es['sellers']['seller_id'],\n                                         es['order_products']['seller_id']))\n\n# orders to payments\nes = es.add_relationship(ft.Relationship(es['orders']['order_id'],\n                                         es['payments']['order_id']))\n","e9d37ef7":"es.add_last_time_indexes()","8fcb55c6":"es","5a758a54":"# We want to know if the customer will buy anything again after cutoff\ncutoff_time = pd.Timestamp('July 1, 2018')","ec004a7d":"# training window of only 2 months, then experiment with 4 months, 6 months\n# find out if the customer will buy anything after \n# turns out, sliding windows are not supported yet\nfeatures, feature_names = ft.dfs(entityset = es, target_entity = 'customers', verbose=True, \n                                 cutoff_time=cutoff_time,\n                                 training_window = ft.Timedelta(\"60 days\"))","36ac88d8":"import missingno as msno\nfeatures_null_filtered = msno.nullity_filter(features, p=0.75)","80dbedab":"features_encoded, features_names_encoded = ft.encode_features(features_null_filtered, feature_names)","bc2c0b3b":"orders[\"order_purchase_timestamp\"] = pd.to_datetime(orders[\"order_purchase_timestamp\"])\nlast_timestamp_per_customer = orders.reset_index().groupby(\"customer_id\")[\"order_purchase_timestamp\"].max()\ncustomer_bought_after_cutoff = last_timestamp_per_customer > cutoff_time","aa4a6c55":"from sklearn.model_selection import train_test_split\n\nX = features_encoded\ny = customer_bought_after_cutoff\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","b7002f33":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import make_pipeline\n\nimputer = Imputer(strategy='most_frequent')\nt_svd = TruncatedSVD(n_components=100)\nlog_res = LogisticRegression(C=0.1)\n\npipeline = make_pipeline(imputer, t_svd, log_res)\npipeline.fit(X_train, y_train)","b1f2e34b":"from sklearn.metrics import log_loss, accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\ndef complete_evaluation(y_test, y_probs, y_preds, positive_class=1):\n    \"\"\"\n    Complete evaluation\n    \"\"\"\n    acc = accuracy_score(y_test, y_preds)\n    average_prec = average_precision_score(y_test, y_probs[:, positive_class])\n    precs, recs, fscore, support = precision_recall_fscore_support(y_test, y_preds)\n    prec_0, prec_1 = precs\n    rec_0, rec_1 = recs\n    fscore_0, fscore_1 = fscore \n    support_0, support_1 = support\n    \n    balanced_accuracy = (rec_0 + rec_1) \/ 2\n\n    log_loss_value = log_loss(y_test, y_probs)\n    auc_value = roc_auc_score(y_test, y_probs[:, positive_class])\n\n    return pd.DataFrame([{\"accuracy\" : acc, \"loss\" : log_loss_value, \"auc\" : auc_value, \"average_precision\" : average_prec,\n                  \"precision_0\" : prec_0, \"precision_1\" : prec_1, \n                  \"recall_0\" : rec_0, \"recall_1\" : rec_1, \"fscore_0\" : fscore_0, \"fscore_1\" : fscore_1,\n                  \"support_0\" : support_0, \"support_1\" : support_1, \"balanced_accuracy\" : balanced_accuracy}])","17f485d5":"y_proba = pipeline.predict_proba(X_test)\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","ccd5b37e":"df_results_thresholds","8805fe01":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","9777c925":"# 120 days training window\nfeatures, feature_names = ft.dfs(entityset = es, target_entity = 'customers', verbose=True, \n                                 cutoff_time=cutoff_time,\n                                 training_window = ft.Timedelta(\"120 days\"))","41cdb90c":"import missingno as msno\nfeatures_null_filtered = msno.nullity_filter(features, p=0.75)","0c502903":"features_encoded, features_names_encoded = ft.encode_features(features_null_filtered, feature_names)","aaca3e49":"last_timestamp_per_customer = orders.reset_index().groupby(\"customer_id\")[\"order_purchase_timestamp\"].max()\ncustomer_bought_after_cutoff = last_timestamp_per_customer > cutoff_time","5b93282a":"from sklearn.model_selection import train_test_split\n\nX = features_encoded\ny = customer_bought_after_cutoff\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","a8fb7c16":"imputer = Imputer(strategy='most_frequent')\nt_svd = TruncatedSVD(n_components=100)\nlog_res = LogisticRegression(C=0.1)\n\npipeline = make_pipeline(imputer, t_svd, log_res)\npipeline.fit(X_train, y_train)","f3f630ec":"y_proba = pipeline.predict_proba(X_test)\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","434c19d2":"df_results_thresholds","c8b07352":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","8ef5df3a":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nimputer = Imputer(strategy='most_frequent')\nrandom_forest = RandomForestClassifier(n_estimators=50, max_depth=3, min_samples_split=10, \n                                       class_weight='balanced_subsample')\n# gbt = GradientBoostingClassifier(subsample=0.5, n_iter_no_change=3, verbose=True,)\n\npipeline = make_pipeline(imputer, random_forest)\npipeline.fit(X_train, y_train)","15013adf":"def name_scores(featurecoef, col_names, label=\"Score\", sort=False):\n    \"\"\"\n    Generates a DataFrame with all the independent variables used in the model with their corresponding coefficient\n    :param featurecoef: model.coef_ | model.feature_importances_\n    :param column: string to be anonymized\n    :param label: Name of the column where coefficients will be added\n    :param sort: False = Decending, True = Ascending\n    :return: pandas DataFrame\n    \"\"\"\n    \n    df_feature_importance = pd.DataFrame([dict(zip(col_names, featurecoef))]).T.reset_index()\n    df_feature_importance.columns = [\"Feature\", label]\n    if sort:\n        return df_feature_importance.sort_values(ascending=False, by=label)\n    return df_feature_importance","eb522a31":"pd.set_option('display.width', 1000)\npd.set_option('display.max_colwidth', -1)","26b6122a":"name_scores(random_forest.feature_importances_, X_train.columns, sort=True)[:20]","d99d240a":"y_proba = pipeline.predict_proba(X_test)\nsns.distplot(y_proba[:, 1])\n\nlist_results = []\nfor threshold in [0.5, 0.525, 0.55, 0.575, 0.6, 0.625, 0.65]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","076a2d56":"df_results_thresholds","ebcec0d1":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","885456b0":"imputer = Imputer(strategy='most_frequent')\ngbt = GradientBoostingClassifier(verbose=True,)\n\npipeline = make_pipeline(imputer, gbt)\npipeline.fit(X_train, y_train)","1f979dc3":"name_scores(gbt.feature_importances_, X_train.columns, sort=True)[:20]","f4dd32c6":"y_proba = pipeline.predict_proba(X_test)\nsns.distplot(y_proba[:, 1])\n\nlist_results = []\nfor threshold in [0.05, 0.075, 0.1, 0.125, 0.15, 0.175, 0.2, 0.225, 0.25]:\n    y_preds = y_proba[:, 1] > threshold\n    result = complete_evaluation(y_test, y_proba, y_preds)\n    result[\"threshold\"] = threshold\n    result[\"number_predictions\"] = (y_preds == 1).sum()\n    result[\"average f-score\"] = ((result[\"fscore_0\"] + result[\"fscore_1\"])\/2).iloc[0]\n    list_results.append(result)\n    \ndf_results_thresholds = pd.concat(list_results)","eb068d91":"df_results_thresholds","d4ba2825":"width = 0.3\nplt.figure(figsize=(15,5))\ndf_results_thresholds.plot(y=['number_predictions', \"precision_1\"], x=\"threshold\", position=0, kind='bar',\n                                    width=width, secondary_y= 'number_predictions', rot=0, )","e6c668ce":"# NBD Model","43d3ad1b":"# Objective:\n1. Predict the number of purchases a customer will do in 2018 H2.\n2. Predict the total cash credited by the customer in 2018 H2.","dcb65a55":"# 120 days","aa4f4c96":"# 60 days","9a058ee9":"# Conclusions:\n- Repeat purchases could be predicted (holdout set's pearson correlation with predictions) although the probability is very low (expected number of future purchases).\n- Monetary value predictions has good correlation although this includes the non-repeat buyers. \n- For the repeat buyers, monetary value predictions' correlation is low. The exact monetary value is hard to predict since for an e-commerce site, the variety of items is very large.","4e030c03":"# ModifiedBetaGeoFitter\n- With this technique, we are abstracting:\n  - the nature of the items in the order\n  - the ratings of the users in the order\n  - the credit payment history of the items in the user\n  - the freight rate","338964f6":"# Regression, finally","e6f4df29":"# Regression with item types\n## Target: Frequency Holdout\nNot good!","c117b769":"# Adding Monetary, GammaGamma Filter","02c55a81":"## Gradient Boosted Trees","cf9be76f":"## A more complex model -- Random Forests","c5629aa6":"# Regression, finally","67c31593":"# Adding the nature of items\n- X attributes: \n - number of items purchased, type of items purchased, city, payment history, type of seller, cost of items, \n- y attributes: how many times the customer returned to purchase an order"}}