{"cell_type":{"a3fdf57a":"code","bc1a52df":"code","58c73a54":"code","2f2aa3a5":"code","61fc5cfc":"code","b4cca02b":"code","faaa96cd":"code","1ba71941":"code","a56ee0aa":"code","26d441d5":"code","fe1eb9f1":"code","5d554115":"code","54c81e4d":"code","1692800f":"code","fea0736e":"code","5dbf2309":"code","2ebee4ca":"code","9ce610ea":"code","af8fdc73":"code","c79977eb":"code","dd9e5777":"code","5c5d6025":"code","093d7f34":"code","729df035":"code","78eaca75":"code","11f6e285":"code","ed5c7e76":"markdown","73af183d":"markdown","877fc89a":"markdown","d4d6c13c":"markdown","1f1b452b":"markdown","df515e18":"markdown","9100bf55":"markdown","57c6d381":"markdown","dece684b":"markdown","866fc17b":"markdown","47b4c8c3":"markdown","b4398624":"markdown","6afed734":"markdown","3ed9c2c8":"markdown","e524e33b":"markdown"},"source":{"a3fdf57a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc1a52df":"import pandas as pd\nimport sklearn as skl\nimport plotly as plt\nimport csv\n\nplt.offline.init_notebook_mode(connected=True)\npd.options.mode.chained_assignment = None\nnp.set_printoptions(linewidth = 95)","58c73a54":"data_path = \"\/kaggle\/input\/pokemon-database\/Pokemon Database.csv\"\n\ndf_raw = pd.read_csv(data_path, encoding='cp1252')\ndf_raw = df_raw.set_index('Pokemon Id')\ndf_raw.loc[df_raw['Original Pokemon ID'].notna(),'Legendary Type'] = \\\n    list(df_raw.loc()[df_raw[df_raw['Original Pokemon ID'].notna()]['Original Pokemon ID']]['Legendary Type'])\ndf_raw.head()","2f2aa3a5":"df_raw.columns","61fc5cfc":"column_name_dict = {\n    'Pokedex Number': 'nid', \n    'Pokemon Name': 'name', \n    'Alternate Form Name': 'form',\n    'Legendary Type': 'legendary', \n    'Pokemon Height': 'height', \n    'Pokemon Weight': 'weight', \n    'Primary Type': 'type_1', \n    'Secondary Type': 'type_2',\n    'Health Stat': 'hp', \n    'Attack Stat': 'atk', \n    'Defense Stat': 'def', \n    'Special Attack Stat': 'satk', \n    'Special Defense Stat': 'sdef', \n    'Speed Stat': 'spd', \n    'EV Yield Total': 'ev_total', \n    'Pre-Evolution Pokemon Id': 'prev_id'\n}\n\ndf = df_raw[column_name_dict.keys()]\ndf.columns = column_name_dict.values()\ndf = df.fillna(value={'form': '', 'legendary': ''})\ndf.type_2[df.type_2.isna()] = df.type_1[df.type_2.isna()] \ndf = df[df.form==''].drop(columns=['form'])\ndf = df[df.prev_id.isin(df.index) | df.prev_id.isna()]\ndf.head(10)","b4cca02b":"sorted(list(set(df.legendary)))","faaa96cd":"SIZES = ['height', 'weight']\nTYPES = ['type_1', 'type_2']\nTYPE_LIST = sorted(list(set(df.type_1)))\nLEGENDARY_TYPE_LIST = sorted(list(set(df.legendary)))\nSTATS = ['hp', 'atk', 'def', 'satk', 'sdef', 'spd']\ndisplay(df[SIZES].describe().T.drop(columns=['count']).style.set_caption('Stats for heights and weights'))\ndisplay(df[STATS].describe().T.drop(columns=['count']).style.set_caption('Stats for base stats'))","1ba71941":"def log_normalize(arr, mid, r_scale):\n    return np.log(arr\/mid)\/r_scale\n\ndef log_rev_normalize(arr, mid, r_scale):\n    return np.exp(arr*r_scale)*mid\n\ndisplay(log_normalize(df[['height']], 1,3).describe().T.drop(columns=['count']).style.set_caption('Stats for heights'))\ndisplay(log_normalize(df[['weight']],10,5).describe().T.drop(columns=['count']).style.set_caption('Stats for weights'))","a56ee0aa":"import re\ndf.name = df.name.apply(lambda n: n.lower())\nname_is_special = [len(re.sub('[a-z]', '', n.lower())) > 0 for n in df.name]\ndf.name[name_is_special]","26d441d5":"df.name[name_is_special] = df.name[name_is_special].apply(lambda n: \n                                                          re.sub('_\\((.).+\\)', '_\\g<1>', \n                                                          re.sub(\"[-\\ 2:']+\", '_', \n                                                                 n.replace('.', '').replace('\u00e9', 'e')))\n                                                         )\ndf.name[name_is_special]","fe1eb9f1":"df.name.reindex(df.name.str.len().sort_values(ascending=True).index)","5d554115":"df.name.str.len().describe().drop(['count'])","54c81e4d":"def df_to_arrays(df_in): \n    stats = df_in[STATS]\n    types = df_in[TYPES]\n    legendary = df_in['legendary']\n    \n    height = log_normalize(df_in[['height']], 1,3)\n    weight = log_normalize(df_in[['weight']],10,5)\n    stats_norm = np.asarray(stats)\/256\n    types_onehot = np.equal.outer(np.asarray(types), TYPE_LIST).astype(np.float)\n    legendaty_onehot = np.equal.outer(np.asarray(legendary), LEGENDARY_TYPE_LIST).astype(np.float)\n    \n    return (stats_norm, np.concatenate([height, weight], axis=1), \n            types_onehot[:,0,:], types_onehot[:,1,:],\n            legendaty_onehot)\n\ndef arrays_to_df(stats_norm, sizes, \n                 type1_onehot, type2_onehot, legendaty_onehot): \n    stats = np.round(stats_norm*256).astype(np.int)\n    type1 = np.array(TYPE_LIST)[np.argmax(type1_onehot,axis=1)]\n    type2 = np.array(TYPE_LIST)[np.argmax(type2_onehot,axis=1)]\n    height = log_rev_normalize(sizes[:,0], 1,3)\n    weight = log_rev_normalize(sizes[:,1],10,5)\n    legendary = np.array(LEGENDARY_TYPE_LIST)[np.argmax(legendaty_onehot,axis=1)]\n    \n    return pd.concat([pd.DataFrame(stats, columns=STATS),\n                      pd.DataFrame(np.stack([type1,type2],axis=1), columns=TYPES),\n                      pd.DataFrame(np.stack([height,weight],axis=1), columns=SIZES),\n                      pd.DataFrame(legendary, columns=['legendary']),\n                     ], \n                     axis=1)\n\ndf_in = df[df.name.isin(['skarmory', 'lugia', 'rayquaza'])]\ndisplay(df_in.style.set_caption('before normalization'))\n\narrays = df_to_arrays(df_in)\nprint('after normalization')\ndisplay(arrays)\nprint('shapes: ', [a.shape for a in arrays])\n\ndisplay(arrays_to_df(*arrays).style.set_caption('after reverse normalization'))","1692800f":"data_all_array = df_to_arrays(df)\ndisplay(data_all_array)\nprint([a.shape for a in data_all_array])","fea0736e":"from keras.layers import Input\nfrom keras.layers import Dense, Concatenate\nfrom keras.models import Model\n\nstats_num = len(STATS)\nsizes_num = len(SIZES)\ntypes_num = len(TYPE_LIST)\nlegendary_num = len(LEGENDARY_TYPE_LIST)\nhidden_dim = [64, 32, 16]\nencode_dim = 8\n\n# encoder\ninput_stats = Input(shape=(stats_num,), name='input_st')\ninput_sizes = Input(shape=(sizes_num,), name='input_sz')\ninput_type1 = Input(shape=(types_num,), name='input_t1')\ninput_type2 = Input(shape=(types_num,), name='input_t2')\ninput_legend = Input(shape=(legendary_num,), name='input_lg')\n\ninputs = Concatenate(name='concat_in')([input_stats, input_sizes, input_type1, input_type2, input_legend])\n\nfor i, dim in enumerate(hidden_dim):\n    if i==0:\n        enc_hidden = Dense(dim, activation='elu', name='hidden_1_en')(inputs)\n    else:\n        enc_hidden = Dense(dim, activation='elu', name=f'hidden_{i+1}_en')(enc_hidden)\n\nenc_latent = Dense(encode_dim, activation='softsign', name='output_en')(enc_hidden)\n\nencoder_model = Model(inputs=(input_stats, input_sizes, \n                              input_type1, input_type2, input_legend), \n                      outputs=enc_latent, \n                      name='encoder')\nencoder_model.summary()\nprint()\n\n# decoder\ninput_latent = Input(shape=(encode_dim,), name='input_lt')\n\nfor i, dim in enumerate(hidden_dim[::-1]):\n    if i==0:\n        dec_hidden = Dense(dim, activation='elu', name='hidden_1_de')(input_latent)\n    else:\n        dec_hidden = Dense(dim, activation='elu', name=f'hidden_{i+1}_de')(dec_hidden)\n\ndec_stats = Dense(stats_num, activation='sigmoid', name='output_st')(dec_hidden)\ndec_sizes = Dense(sizes_num, activation='sigmoid', name='output_sz')(dec_hidden)\ndec_type1 = Dense(types_num, activation='softmax', name='output_t1')(dec_hidden)\ndec_type2 = Dense(types_num, activation='softmax', name='output_t2')(dec_hidden)\ndec_legend = Dense(legendary_num, activation='softmax', name='output_lg')(dec_hidden)\n\ndecoder_model = Model(inputs=input_latent,\n                      outputs=(dec_stats, dec_sizes, \n                               dec_type1, dec_type2, dec_legend), \n                      name='decoder')\ndecoder_model.summary()\nprint()\n\n# autoencoder\nautoencoder_model = Model(inputs=encoder_model.input, outputs=decoder_model(encoder_model.output), name='autonencoder')\nautoencoder_model.summary()\nprint()\n\noutput_names = ['stats', 'sizes', 'type1', 'type2', 'legendary']\noutput_names_dict = dict(zip(output_names, autoencoder_model.output_names))\noutput_names_rev_dict = dict(zip(autoencoder_model.output_names, output_names))\n\ndef output_name_map(output_names_dict, mapped):\n    return {output_names_dict[k]:mapped[k] for k in mapped.keys()}\n\nlosses = {'stats': 'mean_absolute_error',\n          'sizes': 'mean_absolute_error',\n          'type1': 'categorical_crossentropy',\n          'type2': 'categorical_crossentropy',\n          'legendary': 'categorical_crossentropy',}\nlosses = output_name_map(output_names_dict, losses)\n\nloss_weights = {'stats': 50.,\n                'sizes': 30.,\n                'type1': .5,\n                'type2': .5,\n                'legendary': .2}\nloss_weights = output_name_map(output_names_dict, loss_weights)\n\nmetrics = {'type1': 'categorical_accuracy',\n           'type2': 'categorical_accuracy',\n           'legendary': 'categorical_accuracy'}\nmetrics = output_name_map(output_names_dict, metrics)\n\nautoencoder_model.compile(optimizer='adam', \n                          loss=losses, loss_weights=loss_weights, \n                          metrics=metrics)","5dbf2309":"from tqdm.keras import TqdmCallback\n\nload_if_avalible = True\nweight_path = '\/kaggle\/working\/model_1_weight.h5'\n\nif (not load_if_avalible) or (not os.path.exists(weight_path)): \n    train_history = autoencoder_model.fit(data_all_array, data_all_array,\n                                          epochs=20_000+1, batch_size=512, shuffle=True, \n                                          verbose=0, callbacks=[TqdmCallback(verbose=0)])\n    autoencoder_model.save_weights(weight_path)\nelse: \n    autoencoder_model.load_weights(weight_path)","2ebee4ca":"import plotly.express as px\n\nhistory = {}\nfor k in train_history.history.keys():\n    if 'loss' in k:\n        if k == 'loss':\n            history[k] = train_history.history[k]\n        else:\n            nk = output_names_rev_dict[k[:-len('_loss')]]+'_loss'\n            history[nk] = train_history.history[k]\n    if 'categorical_accuracy' in k:\n        nk = output_names_rev_dict[k[:-len('_categorical_accuracy')]]+'_categorical_accuracy'\n        history[nk] = train_history.history[k]\n\ndisplay(\n    px.line({m: history[m][::100] \n              for m in filter(lambda s: 'loss' in s, history.keys())})\\\n                .update_layout(xaxis_title='epoch\/100', yaxis_title='loss')\n    )\n\ndisplay(\n    px.line({m: history[m][::100] \n              for m in filter(lambda s: 'accuracy' in s, history.keys())})\\\n                .update_layout(xaxis_title='epoch\/100', yaxis_title='accuracy')\n    )","9ce610ea":"df_in = df[df.name.isin(['skarmory', 'lugia', 'rayquaza', 'archeops', 'latias', 'latios', 'arceus', 'keldeo'])]\n\ndisplay(df_in[STATS+TYPES+SIZES+['legendary']]\n        .style.set_caption('input data').format({'height': '{:.1f}', 'weight': '{:.1f}'}))\n\nlatent_vector = encoder_model.predict(df_to_arrays(df_in))\ndisplay(pd.DataFrame(latent_vector).style.set_caption('latent vector'))\n                                          \ndisplay(arrays_to_df(*decoder_model.predict(latent_vector))\n       .style.set_caption('output data').format({'height': '{:.1f}', 'weight': '{:.1f}'}))","af8fdc73":"recons_df = arrays_to_df(*autoencoder_model.predict(data_all_array))\nrecons_df.index = df.index\nrecons_df","c79977eb":"latent_vector_all = encoder_model.predict(data_all_array)\n\nlatent_df = pd.DataFrame(latent_vector_all, \n                         columns=[f'lt{i}' for i in range(encode_dim)],\n                        index=df.index)\ndisplay(latent_df.describe().T.drop(columns=['count']))","dd9e5777":"lt_df_plot = pd.concat([df[['nid', 'name']+STATS+SIZES+TYPES+['ev_total', 'legendary']], \n                        latent_df],\n                        axis=1).copy()\n\nlt_df_plot.legendary[lt_df_plot.legendary == ''] = 'None'","5c5d6025":"import ipywidgets as widgets\n\nTYPE_COLOR_MAP = {\n    'Bug': 'lightgreen', \n    'Dark': 'black', \n    'Dragon': 'blue', \n    'Electric': 'yellow', \n    'Fairy': 'fuchsia', \n    'Fighting': 'orange', \n    'Fire': 'red', \n    'Flying': 'skyblue', \n    'Ghost': 'midnightblue', \n    'Grass': 'green', \n    'Ground': 'brown', \n    'Ice': 'aqua', \n    'Normal': 'gray', \n    'Poison': 'purple', \n    'Psychic': 'violet', \n    'Rock': 'teal', \n    'Steel': 'silver', \n    'Water': 'navy', \n}\n\ndef show_pcs_fig(df):\n    def show_pcs_fig_df(x_axis, y_axis, color):\n        fig = px.scatter(df, x=x_axis, y=y_axis, \n                         color=color, size='ev_total', \n                         hover_data=['name','legendary'],\n                         size_max=6, \n                         color_discrete_map=TYPE_COLOR_MAP,\n                         category_orders={'type_1': TYPE_LIST,\n                                          'type_2': TYPE_LIST})\n        return fig\n    return show_pcs_fig_df\n\nlatent_str = [f'lt{i}' for i in range(encode_dim)]+STATS+SIZES\nlt_x_dropdown = widgets.Dropdown(options=latent_str, value=latent_str[0])\nlt_y_dropdown = widgets.Dropdown(options=latent_str, value=latent_str[1])\nclass_dropdown = widgets.Dropdown(options=['type_1', 'type_2', 'legendary'], value='type_1')\n\n_ = widgets.interact(show_pcs_fig(lt_df_plot), x_axis=lt_x_dropdown, y_axis=lt_y_dropdown, color=class_dropdown)","093d7f34":"latent_vector_all = encoder_model.predict(data_all_array)\n\nlatent_df = pd.DataFrame(latent_vector_all, columns=[f'lt{i}' for i in range(encode_dim)])\nlatent_df.describe().T.drop(columns=['count'])","729df035":"from sklearn.decomposition import PCA\n\ndef normalize(df, population=None):\n    if population is None:\n        population = df\n    df_desc = population.describe().loc()[['mean', 'std']]\n    return (df-df_desc.loc['mean'])\/df_desc.loc['std']\n\ndef rev_normalize(df, population=None):\n    if population is None:\n        population = df\n    df_desc = population.describe().loc()[['mean', 'std']]\n    return (df*df_desc.loc['std'])+df_desc.loc['mean']\n\npca = PCA(random_state=227)\npca.fit(normalize(latent_df))\npcs = pca.components_\n\nlatent_var_r = pd.DataFrame(pca.explained_variance_ratio_[:,np.newaxis], columns=['var_r'])\nlatent_var_r.index = [f'pc{i}' for i in range(len(pcs))]\n\nlatent_pc = pd.DataFrame(pca.components_, columns=[f'lt{i}' for i in range(encode_dim)])\nlatent_pc.index = [f'pc{i}' for i in range(len(pcs))]\n\ndisplay(\n    pd.concat([latent_pc, latent_var_r],axis=1).style\\\n        .background_gradient(cmap='bwr_r', subset=[f'lt{i}' for i in range(encode_dim)], axis=0)\\\n        .background_gradient(cmap='Blues', subset=['var_r'], axis=0)\\\n        .format('{:.3}')\n)","78eaca75":"pc_df_plot = pd.concat([df[['nid', 'name']+STATS+SIZES+TYPES+['ev_total', 'legendary']], \n                        pd.DataFrame(pca.transform(normalize(latent_df)), \n                                     columns=[f'pc{i}' for i in range(len(pcs))],\n                                     index=df.index)],\n                        axis=1).copy()\n\npc_df_plot.legendary[pc_df_plot.legendary == ''] = 'None'\n\npcs_str = [f'pc{i}' for i in range(len(pcs))]+STATS+SIZES\npc_x_dropdown = widgets.Dropdown(options=pcs_str, value=pcs_str[0])\npc_y_dropdown = widgets.Dropdown(options=pcs_str, value=pcs_str[1])\nclass_dropdown = widgets.Dropdown(options=['type_1', 'type_2', 'legendary'], value='type_1')\n\n_ = widgets.interact(show_pcs_fig(pc_df_plot), x_axis=pc_x_dropdown, y_axis=pc_y_dropdown, color=class_dropdown)","11f6e285":"import random\n\ndef pca_components_to_df(pca_latent_in):\n    latent_in = rev_normalize(pca.inverse_transform(pca_latent_in), latent_df)[np.newaxis, :]\n    return arrays_to_df(*decoder_model.predict(latent_in))\n\npca_latent_sliders = [widgets.FloatSlider(value=0, min=-4.0, max=4.0, step=0.01,\n                                          description=f'pc{i}:', orientation='vertical', continuous_update=False, \n                                          readout=True, readout_format='.2f', ) for i in range(encode_dim)]\npca_latent_hbox = widgets.HBox(pca_latent_sliders)\n\ndf_out = widgets.Output(layout={'border': '1px solid black', 'height': '80px'})\n\n\ndef on_slider_update(change):\n    df_out.clear_output()\n    slider_values = [slider.value for slider in pca_latent_sliders]\n    df_gen = pca_components_to_df(slider_values)\n    df_gen.insert(6, 'bst', df_gen[STATS].sum(axis=1))\n    \n    with df_out: \n        display(df_gen.style.format('{:.1f}', subset=SIZES))\n        \nfor slider in pca_latent_sliders:\n    slider.observe(on_slider_update, names='value')\n    \n\nrandomize_button = widgets.Button(description='Randomize')\n\ndef on_click_randomize(b):\n    for slider in pca_latent_sliders:\n        rand_norm = random.gauss(0, 1.0)\n        slider.value = round(min(4.0, max(-4.0, rand_norm)), 2)\n        \nrandomize_button.on_click(on_click_randomize)   \n\n\nset_as_button = widgets.Button(description='Set as: ')\nset_as_text = widgets.Text(value='',\n                           placeholder='Enter name', disabled=False)\n\ndef on_click_set_as(b):\n    if set_as_text.value not in list(pc_df_plot.name):\n        return\n    pcs_values = list(np.asarray(pc_df_plot[pc_df_plot.name == set_as_text.value][[f'pc{i}' for i in range(encode_dim)]]))[0]\n    for slider, value in zip(pca_latent_sliders, pcs_values):\n        slider.value = round(min(4.0, max(-4.0, value)), 2)\n        \nset_as_button.on_click(on_click_set_as)\n\nbuttons_hbox = widgets.HBox([randomize_button, set_as_button, set_as_text])\n\non_slider_update(_)\ndisplay(pca_latent_hbox, buttons_hbox, df_out)","ed5c7e76":"## Data normalization","73af183d":"And then to make the names easier to deal with, I turn the name of all Pokemon to lowercase, but I still need to deal names with some non-alphabetic characters: ","877fc89a":"## Model definition","d4d6c13c":"## Apply PCA to the latent space vectors","1f1b452b":"For the sake of simplicity I only keep the base forms i.e. no regional forms, alternate forms etc. and for I'm going to do later I also need to remove the Pokemon after those Pokemon, Sirfetch'd for example. \nBefore doing normalization, we calculate the statistics (especially min and max) to determine how to normalize the data:  ","df515e18":"# Model 1: Autoencoder on stats, sizes, types, and legendary type","9100bf55":"First, we make the functions that can go back and forth between the raw data and the training data for the network: ","57c6d381":"Since the height and weight have more log-like distribution, here I introduce some function for normalizing and reverse normalizing: ","dece684b":"## Train the model","866fc17b":"After that, now we observe the length of the modified names: ","47b4c8c3":"## Evaluate the fitting result","b4398624":"## Data preparation","6afed734":"## Inspect the latent space results","3ed9c2c8":"# Loading and preprocessing the data","e524e33b":"First, we load the data, doing some preprocessing, preserve the columns we are interested, and rename them in the process:"}}