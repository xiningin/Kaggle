{"cell_type":{"e518ea42":"code","778a1b27":"code","a613e2f4":"code","7de6fcb7":"code","1690b416":"code","98d96ac8":"code","5c47545b":"code","f7ae610e":"code","20192399":"code","5c7d4310":"code","a26b70e0":"code","38a1ef9d":"code","d4bf59d3":"code","3d4c97f5":"code","7dbfc4bf":"code","9fecd534":"code","39f2939c":"code","3288e9b3":"code","7a92df6c":"code","553b4e7e":"code","cd6e4c1a":"code","31cbc452":"code","ee413446":"code","def12784":"code","2dd24ff0":"code","9ddc3368":"code","d13eef66":"code","877c7b5b":"code","5171dd82":"code","ea08513b":"code","c7dab7ac":"code","8f3022eb":"markdown","da9b0f27":"markdown","29bcd480":"markdown","e3bd4a63":"markdown","6304ae66":"markdown","c4ca674d":"markdown","82af3443":"markdown","3860e176":"markdown","55d7fb0f":"markdown","230f256e":"markdown","b228fdb5":"markdown","e556da53":"markdown"},"source":{"e518ea42":"!pip install joblib","778a1b27":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\nfrom xgboost import XGBRegressor #ML\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom joblib import dump, load\nimport warnings\nwarnings.filterwarnings(action='ignore')","a613e2f4":"features = ['NU_NOTA_MT','NU_NOTA_COMP1','NU_NOTA_COMP2','NU_NOTA_COMP4','NU_NOTA_COMP5','NU_NOTA_COMP3','NU_NOTA_REDACAO','NU_NOTA_LC','NU_NOTA_CH','NU_NOTA_CN']\ndata = pd.read_csv('..\/input\/enem-2016\/microdados_enem_2016_coma.csv', encoding='latin-1', sep=',', usecols=features, nrows=200000)","7de6fcb7":"data.head()","1690b416":"data[['NU_NOTA_MT']].info()","98d96ac8":"data.describe()","5c47545b":"col = data.columns       # .columns gives columns names in data \nprint(col)","f7ae610e":"features = ['NU_NOTA_MT','NU_NOTA_COMP1','NU_NOTA_COMP2','NU_NOTA_COMP4','NU_NOTA_COMP5','NU_NOTA_COMP3','NU_NOTA_REDACAO','NU_NOTA_LC','NU_NOTA_CH','NU_NOTA_CN']\ntarget = \"NU_NOTA_MT\"","20192399":"total = data[features].isnull().sum().sort_values(ascending = False)\npercent = (data[features].isnull().sum()\/data[features].isnull().count()*100).sort_values(ascending = False)\nmissing  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing","5c7d4310":"data_map = data[[target]]\ndata_map[features] = data[features]\nplt.figure(figsize=(15,15))\nsns.heatmap(data_map.corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()","a26b70e0":"for column in features:\n    plt.figure(figsize = (20, 3))\n    data.plot(kind='scatter', x=column, y=target)","38a1ef9d":"duplicated_data = data.duplicated()\ndata[duplicated_data]","d4bf59d3":"train = data.copy()\ntrain = train.loc[:, features]\ntrain.dropna(subset=[target], inplace=True)","3d4c97f5":"y = train[target]\nX = train.drop([target], axis=1)","7dbfc4bf":"numerical_columns = list(X._get_numeric_data().columns)\ncategorical_columns = list(set(X.columns) - set(numerical_columns))","9fecd534":"numerical_pipeline = Pipeline([\n        ('data_filler', SimpleImputer(strategy=\"median\")),\n        ('std_scaler', StandardScaler()),\n    ])","39f2939c":"categorical_pipeline = Pipeline([\n        ('data_filler', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ])","3288e9b3":"transformer = ColumnTransformer([\n    (\"numerical\", numerical_pipeline, numerical_columns),\n    (\"categorical\", categorical_pipeline, categorical_columns)\n])","7a92df6c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","553b4e7e":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","cd6e4c1a":"def train_ensemble_models(X, y):\n    clf1 = LinearRegression()\n    clf2 = Lasso(alpha=.5)\n    clf3 = Ridge(alpha=.1)\n    clf4 = LassoLars(alpha=.1)\n    clf5 = AdaBoostRegressor()\n    clf6 = SVR(kernel='rbf',gamma='scale',C=100)\n    clf7 = GradientBoostingRegressor()\n\n    for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6, clf7], ['Linear Regression', 'Lasso', 'Ridge','Lasso Lars','Ada Boost Regressor','SVR', 'Gradient Boosting Regressor']):\n        execute_pipeline(clf, X, y, label)","31cbc452":"def execute_pipeline(clf, X, y, title):\n    \n    pipe = Pipeline([\n        ('transformer', transformer),\n        ('reduce_dim', 'passthrough'),\n        ('classify', clf)\n    ])\n\n    N_FEATURES_OPTIONS = [2, 4, 8]\n    \n    param_grid = [\n        {\n            'reduce_dim': [PCA()],\n            'reduce_dim__n_components': N_FEATURES_OPTIONS\n        },\n        {\n            'reduce_dim': [SelectKBest()],\n            'reduce_dim__k': N_FEATURES_OPTIONS\n        },\n    ]\n    reducer_labels = ['PCA', 'KBest']\n\n    grid = GridSearchCV(pipe,  param_grid=param_grid, scoring='r2', cv=10, verbose=1, n_jobs=-1, return_train_score=True)\n    grid.fit(X, y)\n\n    mean_train_scores = np.array(grid.cv_results_['mean_train_score'])\n    mean_scores = np.array(grid.cv_results_['mean_test_score'])\n    mean_scores = mean_scores.reshape(2, len(N_FEATURES_OPTIONS))\n    bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) * (len(reducer_labels) + 1) + .5)\n\n    plt.figure()\n    COLORS = 'bgrcmyk'\n    for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n        plt.bar(bar_offsets + i, mean_train_scores[i], label='{} train'.format(label),alpha=.7)\n        plt.bar(bar_offsets + i, reducer_scores, label='{} test'.format(label), color=COLORS[i])\n\n    plt.title(title)\n    plt.xlabel('Number of features')\n    plt.xticks(bar_offsets + len(reducer_labels) \/ 2, N_FEATURES_OPTIONS)\n    plt.ylabel('Classification accuracy')\n    plt.ylim((0, 1))\n    plt.legend(loc='upper left')\n\n    plt.show()","ee413446":"grid_result = train_ensemble_models(X_train, y_train)","def12784":"transformer = transformer\nreduction = SelectKBest(k=8)\nmodel = GradientBoostingRegressor()\n\nX_train_transformer = transformer.fit_transform(X_train)\nX_test_transformer = transformer.transform(X_test)\n\nX_train_reduction_transformer = reduction.fit_transform(X_train_transformer, y_train)\nX_test_reduction_transformer = reduction.transform(X_test_transformer)\n\nmodel.fit(X_train_reduction_transformer, y_train)\n\ny_predict = model.predict(X_test_reduction_transformer)\n\nrmse = (np.sqrt(mean_squared_error(y_test, y_predict)))\nr2 = r2_score(y_test, y_predict)\nprint('RMSE is {}'.format(rmse))\nprint('R2 score is {}'.format(r2))","2dd24ff0":"cols = reduction.get_support(indices=True)\nnew_features = []\nfor bool, feature in zip(cols, X_train.columns):\n    if bool:\n        new_features.append(feature)\n        \ndataframe = pd.DataFrame(X_train, columns=new_features)\ndataframe","9ddc3368":"dataframe['target'] = y_train","d13eef66":"plt.figure(figsize=(15,15))\nsns.heatmap(dataframe.corr(), annot=True, square=True, cmap='coolwarm')\nplt.show()","877c7b5b":"persistence = {}\npersistence['transformer'] = transformer\npersistence['reduction'] = reduction\npersistence['model']  = model\ndump(persistence, 'persist.joblib')","5171dd82":"persistence = load('persist.joblib')\n\ntransformer = persistence['transformer']\nreduction = persistence['reduction']\nmodel = persistence['model']\n\ndataset_test_transformer = transformer.transform(dataset_test)\ndataset_test_reduction_transformer = reduction.transform(dataset_test_transformer)\n\npredictions = model.predict(dataset_test_reduction_transformer)","ea08513b":"output = pd.DataFrame({'NU_INSCRICAO': nuInscricao, 'NU_NOTA_MT': predictions})","c7dab7ac":"output.to_csv('answer.csv', index=False)\nprint(\"Your submission was successfully saved!\")","8f3022eb":"## Data Analysis","da9b0f27":"Data from ENEM 2016, the Brazilian High School National Exam.","29bcd480":"## Clean Dataset","e3bd4a63":"## Conclusion","6304ae66":"# ENEM Challenge","c4ca674d":"### Context\nThis dataset was downloaded from INEP, a department from the Brazilian Education Ministry. It contains data from the applicants for the 2016 National High School Exam.\n\n### Content\nInside this dataset there are not only the exam results, but the social and economic context of the applicants.\n\n### Acknowledgements\nThe original dataset is provided by INEP (http:\/\/portal.inep.gov.br\/microdados).\n\n### Inspiration\nThe objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.","82af3443":"## Save models and results","3860e176":"### Validated Model","55d7fb0f":"## Models","230f256e":"## Data Preprocessing","b228fdb5":"### Imports","e556da53":"## Read dataset"}}