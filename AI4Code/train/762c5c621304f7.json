{"cell_type":{"8555a1c6":"code","a1d63a83":"code","8cc70bd6":"code","fe26db1f":"code","43a9f733":"code","21360fcd":"code","e3f8a8bd":"code","ad7bb507":"code","e0a9e0f6":"code","f2094da9":"code","8e5db6b2":"code","419d4004":"code","d8bb627c":"code","2fbd253b":"code","4c143ccb":"code","590fd6f1":"code","78b50dea":"code","73ce8242":"code","d6e406e8":"code","ba7a5f51":"code","e1ab39c9":"code","6f86d7fd":"code","59b4219f":"code","4301c11d":"code","30d41247":"code","68951c05":"code","173e1ae8":"code","ff0530a8":"code","196b75f8":"markdown"},"source":{"8555a1c6":"# Importing necessary packages\nimport pandas as pd","a1d63a83":"#importing data\nfile_url = 'https:\/\/raw.githubusercontent.com\/PacktWorkshops\/The-Data-Science-Workshop\/master\/Chapter03\/bank-full.csv'\nbankData = pd.read_csv(file_url, sep=\";\")","8cc70bd6":"# Getting the total counts under each job category\njobTot = bankData.groupby('job')['y'].agg(jobTot='count').reset_index()\njobTot","fe26db1f":"# Getting all the details in one place\njobProp = bankData.groupby(['job', 'y'])['y'].agg(jobCat='count').reset_index()","43a9f733":"# Merging both the data frames\njobComb = pd.merge(jobProp, jobTot, on=['job'])\njobComb['catProp'] = (jobComb.jobCat\/jobComb.jobTot)*100\n\njobComb.head()","21360fcd":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Create seperate data frames for Yes and No\njobcombYes = jobComb[jobComb['y'] == 'yes']\njobcombNo = jobComb[jobComb['y'] == 'no']\n\n# Get the length of the xaxis labels \nxlabels = jobTot['job'].nunique()\n\n# Get the proportion values \njobYes = jobcombYes['catProp'].unique()\njobNo = jobcombNo['catProp'].unique()\n\n# Arrange the indexes of x asix\nind = np.arange(xlabels)\n\n# Get the width of each bar\nwidth = 0.35  \n\n# Getting the plots\np1 = plt.bar(ind, jobYes, width)\np2 = plt.bar(ind, jobNo, width,bottom=jobYes)\n\nplt.ylabel('Propensity Proportion')\nplt.title('Propensity of purchase by Job')\n\n# Defining the x label indexes and y label indexes\nplt.xticks(ind, jobTot['job'].unique())\nplt.yticks(np.arange(0, 100, 10))\n\n# Defining the legends\nplt.legend((p1[0], p2[0]), ('Yes', 'No'))\n\n# To rotate the axis labels \nplt.xticks(rotation=90)\nplt.show()","e3f8a8bd":"# Normalising data\nfrom sklearn import preprocessing\nx = bankData[['balance']].values.astype(float)\n# Creating the scaling function\nminmaxScaler = preprocessing.MinMaxScaler()\n# Transforming the balance data by normalising it with minmaxScalre\nbankData['balanceTran'] = minmaxScaler.fit_transform(x)\n# Printing the head of the data\nbankData.head()","ad7bb507":"# Adding a small numerical constant to eliminate 0 values\n\nbankData['balanceTran'] = bankData['balanceTran'] + 0.00001","e0a9e0f6":"# Let us transform values for loan data\nbankData['loanTran'] = 1\n# Giving a weight of 5 if there is no loan\nbankData.loc[bankData['loan'] == 'no', 'loanTran'] = 5\nbankData.head()","f2094da9":"# Let us transform values for Housing data\nbankData['houseTran'] = 5\n# Giving a weight of 1 if the customer has a house\nbankData.loc[bankData['housing'] == 'no', 'houseTran'] = 1\n\nbankData.head()","8e5db6b2":"# Let us now create the new variable which is a product of all these\nbankData['assetIndex'] = bankData['balanceTran'] * bankData['loanTran'] * bankData['houseTran']\nbankData.head()","419d4004":"# Finding the quantile\nnp.quantile(bankData['assetIndex'],[0.25,0.5,0.75])","d8bb627c":"# Creating quantiles from the assetindex data\nbankData['assetClass'] = 'Quant1'\n\nbankData.loc[(bankData['assetIndex'] > 0.38) & (bankData['assetIndex'] < 0.57), 'assetClass'] = 'Quant2'\n\nbankData.loc[(bankData['assetIndex'] > 0.57) & (bankData['assetIndex'] < 1.9), 'assetClass'] = 'Quant3'\n\nbankData.loc[bankData['assetIndex'] > 1.9, 'assetClass'] = 'Quant4'\n\nbankData.head()","2fbd253b":"# Calculating total of each asset class\nassetTot = bankData.groupby('assetClass')['y'].agg(assetTot='count').reset_index()\n# Calculating the category wise counts\nassetProp = bankData.groupby(['assetClass', 'y'])['y'].agg(assetCat='count').reset_index()","4c143ccb":"# Merging both the data frames\nassetComb = pd.merge(assetProp, assetTot, on=['assetClass'])\nassetComb['catProp'] = (assetComb.assetCat \/ assetComb.assetTot)*100\nassetComb","590fd6f1":"# Categorical variables, removing loan and housing\nbankCat1 = pd.get_dummies(bankData[['job','marital','education','default','contact','month','poutcome']])","78b50dea":"bankNum1 = bankData[['age','day','duration','campaign','pdays','previous','assetIndex']]\nbankNum1.head()","73ce8242":"# Normalise some of the numerical variables\nfrom sklearn import preprocessing","d6e406e8":"# Creating the scaling function\nminmaxScaler = preprocessing.MinMaxScaler()","ba7a5f51":"# Creating the transformation variables\nageT1 = bankNum1[['age']].values.astype(float)\ndayT1 = bankNum1[['day']].values.astype(float)\ndurT1 = bankNum1[['duration']].values.astype(float)","e1ab39c9":"# Transforming the balance data by normalising it with minmaxScalre\nbankNum1['ageTran'] = minmaxScaler.fit_transform(ageT1)\nbankNum1['dayTran'] = minmaxScaler.fit_transform(dayT1)\nbankNum1['durTran'] = minmaxScaler.fit_transform(durT1)","6f86d7fd":"# Let us create a new numerical variable by selecting the transformed variables\nbankNum2 = bankNum1[['ageTran','dayTran','durTran','campaign','pdays','previous','assetIndex']]\n\n# Printing the head of the data\nbankNum2.head()","59b4219f":"# Preparing the X variables\nX = pd.concat([bankCat1, bankNum2], axis=1)\nprint(X.shape)\n# Preparing the Y variable\nY = bankData['y']\nprint(Y.shape)\nX.head()","4301c11d":"from sklearn.model_selection import train_test_split\n# Splitting the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n","30d41247":"from sklearn.linear_model import LogisticRegression\n# Defining the LogisticRegression function\nbankModel = LogisticRegression()\nbankModel.fit(X_train, y_train)","68951c05":"pred = bankModel.predict(X_test)\nprint('Accuracy of Logisticr regression model prediction on test set: {:.2f}'.format(bankModel.score(X_test, y_test)))","173e1ae8":"# Confusion Matrix for the model\nfrom sklearn.metrics import confusion_matrix\nconfusionMatrix = confusion_matrix(y_test, pred)\nprint(confusionMatrix)","ff0530a8":"#accuracy on model\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","196b75f8":"# Here is my work on bank term deposit."}}