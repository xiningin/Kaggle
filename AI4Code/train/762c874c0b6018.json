{"cell_type":{"5625bf76":"code","ba5d5260":"code","39978a3f":"code","f2e7c5c2":"code","b3b2a207":"code","084b7102":"code","08c47c54":"code","504ad566":"code","e185bef1":"code","7423bace":"code","e150146d":"code","9bb064e8":"code","f2b8a4bb":"code","b44c275d":"code","46cffbb4":"code","2bb3d219":"code","78106b39":"code","a916b45b":"code","c4c74017":"code","f6e55d58":"code","80efe0ad":"code","75565e62":"code","c94039aa":"code","5065b4e3":"code","e93be031":"code","0176b7db":"code","c1290196":"code","3239d81c":"code","cee50115":"markdown"},"source":{"5625bf76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ba5d5260":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","39978a3f":"data = pd.read_csv('\/kaggle\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv')\ndata.head()","f2e7c5c2":"data.shape","b3b2a207":"data.describe()","084b7102":"data.isnull().describe()","08c47c54":"data['pixels'] = data['pixels'].map(lambda x: np.array(x.split(' '), dtype=np.float32).reshape(48, 48))","504ad566":"## normalizing pixels data\ndata['pixels'] = data['pixels'].apply(lambda x: x\/255)\n\n## calculating distributions\nage_dist = data['age'].value_counts()\nethnicity_dist = data['ethnicity'].value_counts()\ngender_dist = data['gender'].value_counts().rename(index={0:'Male',1:'Female'})\n\ndef ditribution_plot(x,y,name):\n    fig = go.Figure([\n        go.Bar(x=x, y=y)\n    ])\n\n    fig.update_layout(title_text=name)\n    fig.show()","e185bef1":"import plotly.graph_objects as go\nimport plotly.express as px\nditribution_plot(x=age_dist.index, y=age_dist.values, name='Age Distribution')","7423bace":"ditribution_plot(x=ethnicity_dist.index, y=ethnicity_dist.values, name='Ethnicity Distribution')","e150146d":"ditribution_plot(x=gender_dist.index, y=gender_dist.values, name='Gender Distribution')","9bb064e8":"# Plot some pictures\nfig, axes = plt.subplots(1, 5, figsize=(20, 10))\n\nfor i in range(5):\n    random_face = np.random.choice(len(data))\n    \n    age = data['age'][random_face]\n    ethnicity = data['ethnicity'][random_face]\n    gender = data['gender'][random_face]\n    \n    axes[i].set_title('Age: {0}, Ethnicity: {1}, Sex: {2}'.format(age, ethnicity, gender))\n    axes[i].imshow(data['pixels'][random_face])\n    axes[i].imshow(data['pixels'][random_face])\n    axes[i].axis('off')","f2b8a4bb":"X = np.array(data['pixels'].tolist())\n\n## Converting pixels from 1D to 3D\nX = X.reshape(X.shape[0],48,48,1)","b44c275d":"# Normalise images\nif np.max(X) > 1: \n    X = X\/255","46cffbb4":"# Set some useful variables\ninput_shape = X.shape[1:] \n\nepochs = 20\nbatch_size = 64\nrandom_seeds = 42","2bb3d219":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D,InputLayer, Dropout, BatchNormalization, Flatten, Dense, MaxPooling2D\nfrom tensorflow.keras import utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","78106b39":"# split the data into train ad test\ny = data['age'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=random_seeds)","a916b45b":"AgeModel = Sequential()\n\nAgeModel.add(Conv2D(64, kernel_size=(3,3), input_shape=input_shape, activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(Dropout(0.2))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Conv2D(256, kernel_size=(3,3), activation='relu'))\nAgeModel.add(MaxPooling2D(pool_size=(2,2)))\nAgeModel.add(Dropout(0.5))\nAgeModel.add(BatchNormalization())\n\nAgeModel.add(Flatten())\nAgeModel.add(Dense(128, activation='relu'))\nAgeModel.add(Dropout(0.4))\nAgeModel.add(Dense(1))\n\nAgeModel.compile(optimizer='adam',\n              loss='mean_squared_error',\n              metrics=['mse'])\n\n\n# Callbacks for age model\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience=4, monitor='val_loss', mode='min'), \n             tf.keras.callbacks.ReduceLROnPlateau(patience=2, verbose=1)]\n\nAgeModel.summary()","c4c74017":"history = AgeModel.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=batch_size, callbacks=callbacks)","f6e55d58":"pd.DataFrame(history.history).plot()","80efe0ad":"valid_score = AgeModel.evaluate(X_test, y_test, verbose=1)","75565e62":"y_pred = AgeModel.predict(X_test)\n","c94039aa":"y = data['gender'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=random_seeds)\n","5065b4e3":"gender_model = Sequential([\n    InputLayer(input_shape=(48,48,1)),\n    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n    BatchNormalization(),\n    MaxPooling2D((2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D((2, 2)),\n    Flatten(),\n    Dense(64, activation='relu'),\n    Dropout(rate=0.5),\n    Dense(1, activation='sigmoid')\n])\n\ngender_model.compile(optimizer='sgd',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\n\n\n## Stop training when validation loss reach 0.2700\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_loss')<0.2000):\n            print(\"\\nReached 0.2000 val_loss so cancelling training!\")\n            self.gender_model.stop_training = True\n        \ncallback = myCallback()\n\ngender_model.summary()","e93be031":"history = gender_model.fit(\n    X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), batch_size=batch_size, callbacks=[callback])\n","0176b7db":"pd.DataFrame(history.history).plot();","c1290196":"valid_score = gender_model.evaluate(X_test, y_test, verbose=1)","3239d81c":"y_pred = gender_model.predict(X_test)\n","cee50115":"# Model for Age and Gender Prediction"}}