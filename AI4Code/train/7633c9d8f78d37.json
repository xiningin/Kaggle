{"cell_type":{"ab85ba6a":"code","a4c0547f":"code","63b0cfb9":"code","d12c1cc1":"code","4bdcb222":"code","d7e7a449":"code","48a637b8":"code","34e76a0b":"code","6cc8af8d":"code","06263531":"code","3c17b566":"code","8380f91e":"code","2010f104":"code","e50b5ee0":"code","2407de60":"code","6d34cd89":"code","b8ddae45":"code","ca1299fd":"code","b789ff70":"code","bd1fbfc9":"code","60201a1e":"code","d2599e0b":"code","885d7770":"code","7546d516":"code","3fec21e4":"code","2635ca84":"code","3a31b128":"code","ce468dac":"code","9dfddced":"code","1e0f7d98":"code","1aa96bc6":"code","693310c9":"code","6d5402b6":"code","f393d0ed":"code","2e5c1fcd":"code","5ad8901f":"code","14590f37":"code","f749e713":"code","16c9bf6a":"code","8c117dae":"code","35080fdb":"code","30585861":"code","c67a3ef2":"code","fc9f1680":"code","e4d3f42b":"code","ac114acf":"code","362bf3c7":"code","f9f3d5d3":"code","04c4b9e8":"code","119c2fb2":"code","50f3df50":"code","fc41bc90":"code","bda92177":"code","4b02f9e6":"code","289c731d":"code","3aff1b61":"code","74751dbb":"code","6dd966ba":"code","746b32ba":"code","ac517a55":"code","e1574ee5":"code","fe3319ea":"code","74f1fa76":"markdown","15fa7a9a":"markdown","37deb890":"markdown","8d5c90bc":"markdown","328af0e8":"markdown","e6b9b9b5":"markdown","791ef04e":"markdown","ab3ef6bf":"markdown","aaab2edf":"markdown","2c28321e":"markdown","ded3ae72":"markdown","b395fb7f":"markdown","d9a4bb28":"markdown","50a60ed4":"markdown","71950b54":"markdown","1c96d2cd":"markdown","96f18fbf":"markdown","8db4a9e3":"markdown","f9cfdb74":"markdown","7b4fb32d":"markdown","ead1929c":"markdown","40403bbb":"markdown","a4228c73":"markdown","13504afc":"markdown","d5a3913f":"markdown","cd88d57c":"markdown","f7b8a205":"markdown","e02adade":"markdown","9b1e9ebe":"markdown","ecbe3535":"markdown","e568cb10":"markdown","d52ade93":"markdown","ecf6b167":"markdown","e77f6653":"markdown","577fe34e":"markdown","23dbe615":"markdown","b63d0d31":"markdown","ea8c8303":"markdown","529c246c":"markdown"},"source":{"ab85ba6a":"import warnings\nwarnings.filterwarnings(\"ignore\")","a4c0547f":"!pip install auto-sklearn==0.12.0\n!pip install scikit-learn==0.23.2","63b0cfb9":"# processamento de dados, algebra linear\nimport numpy as np \nimport pandas as pd","d12c1cc1":"# imprime os arquivos\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4bdcb222":"df = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')\ndf.sample(3)","d7e7a449":"columns_to_remove = ['id', 'date', 'yr_renovated', 'zipcode']\ndf = df.drop(columns_to_remove, axis=1)\ndf.sample(3)","48a637b8":"df.describe()","34e76a0b":"lines, columns = df.shape\nprint('linhas :', lines)\nprint('colunas:', columns)","6cc8af8d":"# recupera os valores (X), e as classes (Y)\nX = df.drop('price', axis=1)\nY = df['price']","06263531":"# normalizador\nfrom sklearn.preprocessing import StandardScaler","3c17b566":"# normaliza\u00e7\u00e3o dos dados\nmin_max_scaler = StandardScaler()\nX = min_max_scaler.fit_transform(X)","8380f91e":"# treinamento, test split\nfrom sklearn.model_selection import train_test_split","2010f104":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=26)","e50b5ee0":"print('treinamento:', len(y_train))\nprint('teste      :', len(y_test))","2407de60":"# visualiza\u00e7\u00e3o de dados\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6d34cd89":"# extra\u00ed a correla\u00e7\u00e3o dos dados\ncorr = df.corr(method='pearson')\n\n# heatmap - gr\u00e1fico de calor\nplt.figure(figsize=(11,8))\nsns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')\nplt.show()\n# print(corr)","b8ddae45":"df.hist(figsize=(10,8))\nplt.tight_layout()\nplt.show()","ca1299fd":"# m\u00e9tricas\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","b789ff70":"# vari\u00e1vel de resultado final\n# ser\u00e1 armazenado o resultado de todos experimentos\nexperiment = {}","bd1fbfc9":"# regressor\nfrom sklearn.neighbors import KNeighborsRegressor","60201a1e":"model1 = KNeighborsRegressor(n_neighbors=3,metric='euclidean')\nmodel1.fit(X_train,y_train)","d2599e0b":"y_pred = model1.predict(X_test)","885d7770":"# R Square Error\nr2 = r2_score(y_test,y_pred)\n\n# Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_test,y_pred)\n\n# Mean Square Error (MSE)\nmse = mean_squared_error(y_test,y_pred, squared=True)\n\n# Root Mean Square Error (RMSE)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","7546d516":"experiment['KNN'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","3fec21e4":"# regressor\nfrom sklearn.linear_model import LinearRegression","2635ca84":"model2 = LinearRegression()\nmodel2.fit(X_train, y_train)","3a31b128":"y_pred = model2.predict(X_test)","ce468dac":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","9dfddced":"experiment['Linear Regression'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","1e0f7d98":"# regressor\nfrom sklearn.svm import SVR","1aa96bc6":"model3 = SVR()\nmodel3.fit(X_train, y_train)","693310c9":"y_pred = model3.predict(X_test)","6d5402b6":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","f393d0ed":"experiment['SVM'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","2e5c1fcd":"# regressor\nfrom sklearn.tree import DecisionTreeRegressor","5ad8901f":"model4 = DecisionTreeRegressor(random_state=26)\nmodel4.fit(X_train, y_train)","14590f37":"y_pred = model4.predict(X_test)","f749e713":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","16c9bf6a":"experiment['Decision Tree'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","8c117dae":"# regressor\nfrom sklearn.ensemble import RandomForestRegressor","35080fdb":"model5 = RandomForestRegressor(n_estimators=100, random_state=26)\nmodel5.fit(X_train, y_train)","30585861":"y_pred = model5.predict(X_test)","c67a3ef2":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","fc9f1680":"experiment['Random Forest'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","e4d3f42b":"# ensemble\nfrom sklearn.ensemble import BaggingRegressor","ac114acf":"model_base = DecisionTreeRegressor(random_state=26)\nmodel6 = BaggingRegressor(base_estimator=model_base, n_estimators=10, random_state=26)\nmodel6.fit(X_train, y_train)","362bf3c7":"y_pred = model6.predict(X_test)","f9f3d5d3":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","04c4b9e8":"experiment['Bagging'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","119c2fb2":"# ensemble\nfrom sklearn.ensemble import VotingRegressor","50f3df50":"r1 = LinearRegression()\nr2 = RandomForestRegressor(n_estimators=10, random_state=26)\n\nmodel7 = VotingRegressor([('LR', r1), ('RF', r2)])\nmodel7.fit(X_train, y_train)","fc41bc90":"y_pred = model7.predict(X_test)","bda92177":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","4b02f9e6":"experiment['Ensemble'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","289c731d":"# automl\nimport autosklearn.regression","3aff1b61":"automl = autosklearn.regression.AutoSklearnRegressor(\n    time_left_for_this_task=120,\n    per_run_time_limit=30,\n    tmp_folder='\/automl\/tmp\/',\n    output_folder='\/automl\/output\/',\n)\nautoml.fit(X_train, y_train, dataset_name='housesalesprediction')","74751dbb":"y_pred = automl.predict(X_test)","6dd966ba":"r2 = r2_score(y_test,y_pred)\nmae = mean_absolute_error(y_test,y_pred)\nmse = mean_squared_error(y_test,y_pred, squared=True)\nrmse = mean_squared_error(y_test,y_pred, squared=False)","746b32ba":"experiment['AutoML'] = {'R2':r2, 'MAE':mae, 'MSE':mse, 'RMSE':rmse}\n\nprint('R2  :',r2)\nprint('MAE :',mae)\nprint('MSE :',mse)\nprint('RMSE:',rmse)","ac517a55":"automl.show_models()","e1574ee5":"# palheta de cores\nimport seaborn as sns","fe3319ea":"cm = sns.color_palette('Blues_r', as_cmap=True)\npd.DataFrame(experiment).T.style.background_gradient(subset=['R2'], cmap=cm).highlight_max(subset=['R2'], axis=0)","74f1fa76":"### Avalia\u00e7\u00e3o","15fa7a9a":"**Discuss\u00e3o Bagging**   \n\nBagging obteve bons resultados, pr\u00f3ximo ao Random Forest.   \n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","37deb890":"<a id=\"reg\"><\/a>\n\n## Regress\u00e3o Linear","8d5c90bc":"### Avalia\u00e7\u00e3o","328af0e8":"**Discuss\u00e3o KNN Regressor**   \n\nk-NN obteve um R\u00b2 pr\u00f3ximo de 79%, ou seja, representou bem a fun\u00e7\u00e3o de pre\u00e7os.\n\n-----","e6b9b9b5":"### Conjuntos de Dados","791ef04e":"**Discuss\u00e3o \u00c1rvore de Decis\u00e3o**   \n\n\u00c1rvore de Decis\u00e3o obteve um resultado similar \u00e0 Regress\u00e3o Linear, com R\u00b2 de 71%.\n\n-----","ab3ef6bf":"<a id=\"regression\"><\/a>\n\n-----\n\n# Regress\u00e3o\n\nEsta se\u00e7\u00e3o re\u00fane um conjunto de experimentos. Cada subse\u00e7\u00e3o \u00e9 um algoritmo de Aprendizado de M\u00e1quina.\n\n\n[Voltar para o Topo](#top)","aaab2edf":"<a id=\"ensemble\"><\/a>\n\n## Ensemble\n\nRegress\u00e3o com estrat\u00e9gia de Ensemble, utilizando os algoritmos Linear Regression e Random Forest.","2c28321e":"<a id=\"knn\"><\/a>\n\n## K-NN Regressor\n\n_(k-Nearest Neighbors)_","ded3ae72":"### Avalia\u00e7\u00e3o","b395fb7f":"## Sele\u00e7\u00e3o dos Dados\n\nNesta se\u00e7\u00e3o observamos os dados e selecionamos apenas aqueles que s\u00e3o interessantes para os modelos de regress\u00e3o. Al\u00e9m disso, n\u00e3o ser\u00e1 proposto nenhum _feature engineer_ para enriquecimento dos dados ou tratamendo dos dados.","d9a4bb28":"### Avalia\u00e7\u00e3o","50a60ed4":"### Avalia\u00e7\u00e3o","71950b54":"# Regress\u00e3o - Pre\u00e7o de Venda da Casa\n\nEste notebook realiza um estudo, um conjunto de experimentos, de algoritmos de regress\u00e3o sobre o dataset [House Sales in King County, USA](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction). Um conjunto de dados que re\u00fane mais de 21 mil casas e 21 atributos, tais como pre\u00e7o, n\u00famero de quartos, n\u00famero de banheiros, andares, nota da casa, entre outros. Nosso objetivo \u00e9 predizer o valor de uma casa baseado nas caracter\u00edsticas da casa.\n\n> Conte\u00fado voltado para iniciantes na \u00e1rea de Aprendizado de M\u00e1quina e Ci\u00eancia de Dados!\n\n<a id=\"top\"><\/a>\n\n## Conte\u00fado\n\n> **Nota**. Alguns c\u00f3digos foram ocultados a fim de facilitar a leitura e dar destaque para os conte\u00fados mais importantes.\n\nO notebook est\u00e1 organizado como segue:\n\n- [Dados](#data) - Carregamento dos dados, pr\u00e9-processamento.\n- [Visualiza\u00e7\u00e3o](#visual) - An\u00e1lise explorat\u00f3ria dos dados.\n- [Regress\u00e3o](#regression) - Aplica\u00e7\u00e3o de algoritmos de Aprendizado de M\u00e1quina.\n    - [KNN Regressor](#knn) - Regress\u00e3o com k-NN.\n    - [Regress\u00e3o Linear](#reg) - Regress\u00e3o com Regress\u00e3o Linear.\n    - [Support Vector Machines](#svm) - Regress\u00e3o com Support Vector Machines.\n    - [\u00c1rvore de Decis\u00e3o](#decision) - Regress\u00e3o com Decision Tree.\n    - [Random Forest](#forest) - Regress\u00e3o com Random Forest.\n    - [Bagging](#bagging) - Regress\u00e3o com estrat\u00e9gia de Bagging.\n    - [Ensemble](#ensemble) - Regress\u00e3o com estrat\u00e9gia de Ensemble.\n    - [AutoML](#automl) - Regress\u00e3o com Automated Machine Learning.","1c96d2cd":"# Conclus\u00e3o\n\nPor fim, o melhor algoritmo foi o Random Forest com R\u00b2 de 88%.   \nAs demais estrat\u00e9gias de ensemble, Bagging e Ensemble, tamb\u00e9m apresentaram bons resultados.","96f18fbf":"### Visualiza\u00e7\u00e3o\n\nN\u00f3s conseguimos visualizar a \u00e1rvore de decis\u00e3o, como as ramifica\u00e7\u00f5es ocorreram.   \n\u00c9 muito \u00fatil para uma apresenta\u00e7\u00e3o de neg\u00f3cio, em que voc\u00ea consegue explicar a intelig\u00eancia induzida.\n\n> N\u00e3o ser\u00e1 apresentado neste notebook, pois a \u00e1rvore aqui constru\u00edda \u00e9 muito grande e demora para ser executada.\n\n- Refer\u00eancia: [Visualize a Decision Tree in 4 Ways with Scikit-Learn and Python](https:\/\/mljar.com\/blog\/visualize-decision-tree\/)","8db4a9e3":"<a id=\"automl\"><\/a>\n\n## AutoML\n\nAutomated Machine Learning.","f9cfdb74":"### Qual a correla\u00e7\u00e3o dos atributos?\n\n`DataFrame.corr()` calcula a correla\u00e7\u00e3o de pares de colunas, excluindo `NaN` e valores nulos. Por padr\u00e3o \u00e9 computado a [Correla\u00e7\u00e3o de Pearson](https:\/\/www.statisticssolutions.com\/pearsons-correlation-coefficient\/), seu coeficiente de correla\u00e7\u00e3o mede a rela\u00e7\u00e3o estat\u00edstica, ou associa\u00e7\u00e3o, entre duas vari\u00e1veis cont\u00ednuas. ","7b4fb32d":"### Visualiza\u00e7\u00e3o\n\nPodemos ver o modelo ou o ensemble de modelos utilizado no AutoML.\n\n> Para isto, utilize o comando `automl.show_models()`.","ead1929c":"<a id=\"decision\"><\/a>\n\n## \u00c1rvore de Decis\u00e3o","40403bbb":"Visualizando a estat\u00edstica descritiva dos im\u00f3veis a venda.","a4228c73":"### Avalia\u00e7\u00e3o","13504afc":"<a id=\"bagging\"><\/a>\n\n## Bagging\n\nRegress\u00e3o com estrat\u00e9gia de Bagging, com algoritmo base Decision Tree.","d5a3913f":"<a id=\"svm\"><\/a>\n\n## Support Vector Machines (SVM)","cd88d57c":"## Carregamento dos Dados","f7b8a205":"**Descri\u00e7\u00e3o dos Dados**\n\n[Column defintions - Nova19](https:\/\/www.kaggle.com\/harlfoxem\/housesalesprediction\/discussion\/207885)\n\n- id - ID unico para cada casa. _(remover)_\n- date - Data da casa a venda. _(remover)_\n- price - Pre\u00e7o da cada.\n- bedrooms - N\u00famero de quartos.\n- bathrooms - N\u00famero de banheiros, no qual .5 conta como lavabo.\n- sqft_living - M2 do espa\u00e7o interior.\n- sqft_lot - M2 do espa\u00e7o do terreno.\n- floors - N\u00famero de andares.\n- waterfront - Tem vista para o mar (1) ou n\u00e3o (0). (categ\u00f3rico)\n- view - Valor de 0 a 4 informando se a vista \u00e9 boa. (categ\u00f3rico)\n- condition - Valor de 1 a 5 sobre a condi\u00e7\u00e3o da casa. (categ\u00f3rico)\n- grade - Nota de 1 a 13, no qual 1-3 pequenas constru\u00e7\u00f5es, 7 constru\u00e7\u00e3o e desing mediano, e 11-13 para constru\u00e7\u00f5es de alto n\u00edvel.\n- sqft_above - M2 do interior da casa, acima do n\u00edvel do solo.\n- sqft_basement - M2 do interior da casa, abaixo do n\u00edvel do solo.\n- yr_built - Ano de constru\u00e7\u00e3o da casa.\n- yr_renovated - \u00daltimo ano de renova\u00e7\u00e3o da casa. _(remover)_\n- zipcode - CEP da resid\u00eancia. _(remover)_\n- lat - Latitude.\n- long - Longitude.\n- sqft_living15 - M2 do espa\u00e7o interno para os 15 vizinhos mais pr\u00f3ximos.\n- sqft_lot15 - M2 do terreno para os 15 vizinhos mais pr\u00f3ximos.","e02adade":"**Discuss\u00e3o Ensemble**   \n\nEnsemble obteve bons resultados, pr\u00f3ximo ao Bagging.   \n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","9b1e9ebe":"### Avalia\u00e7\u00e3o","ecbe3535":"<a id=\"data\"><\/a>\n\n-----\n\n# Dados\n\nEsta se\u00e7\u00e3o re\u00fane um conjunto de c\u00f3digo para carregamento e pr\u00e9-processamento sobre os dados.\n\n[Voltar para o Topo](#top)\n","e568cb10":"### Histograma dos Valores por Atributo","d52ade93":"## Conjunto de Treinamento e Teste\n\nNesta se\u00e7\u00e3o vamos separa os valores de `X` e `Y`, em seguida normalizar os valores de `X`, por fim, separar entre conjunto de dados de treinamento e teste.\n\n> A normaliza\u00e7\u00e3o se faz necess\u00e1ria, pois alguns algoritmos se beneficiam de valores normalizados, tal como o K-NN.","ecf6b167":"**Discuss\u00e3o Random Forest**   \n\nRandom Forest obteve o melhor resultado at\u00e9 o momento, R\u00b2 de 88%.   \n\nAl\u00e9m disso, Random Forests s\u00e3o um dos algoritmos mais utilizados em competi\u00e7\u00f5es de Aprendizado de M\u00e1quina.\n\n> **Nota**. Possui alto custo computacional, pois tem que treinar v\u00e1rios modelos.\n\n-----","e77f6653":"<a id=\"visual\"><\/a>\n\n-----\n\n# Visualiza\u00e7\u00e3o dos Dados\n\nEsta se\u00e7\u00e3o re\u00fane um conjunto de visualiza\u00e7\u00f5es sobre os dados.\n\n[Voltar para o Topo](#top)\n","577fe34e":"**Discuss\u00e3o Regress\u00e3o Linear**   \n\nRegress\u00e3o Linear obteve um resultado inferior ao k-NN, com R\u00b2 de 70%.\n\n-----","23dbe615":"### Avalia\u00e7\u00e3o","b63d0d31":"<a id=\"forest\"><\/a>\n\n## Random Forest","ea8c8303":"**Discuss\u00e3o Support Vector Machines (SVM)**   \n\nSVM n\u00e3o conseguiu nem atinguir o caso m\u00e9dio, pois seu R\u00b2 est\u00e1 negativo.\n\n-----","529c246c":"### Normaliza\u00e7\u00e3o dos Dados\n\nNesta se\u00e7\u00e3o vamos utilizar a normaliza\u00e7\u00e3o [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html). Esta fun\u00e7\u00e3o de preprocessamento normaliza os dados conforme segue: \n\n$$x_{new} = \\frac{(x - \\overline{x})}{\\sigma}$$\n\nOu seja, o novo valor $x_{new}$ \u00e9 resultado da normaliza\u00e7\u00e3o do $x$, utilizando a m\u00e9dia $\\overline{x}$ e o desvio padr\u00e3o $\\sigma$."}}