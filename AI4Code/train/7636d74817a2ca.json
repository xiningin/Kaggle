{"cell_type":{"1ba6d7ba":"code","9d7f0eea":"code","6e9031ab":"code","114ab16a":"code","f4e88cb7":"code","cd8e4ce5":"code","b084a42d":"code","c34b4297":"code","f73792da":"code","db2f1a69":"code","698d0371":"code","d15ded81":"code","f9d5f1d7":"code","b7ed6ff3":"code","d59e0763":"code","864e559b":"markdown","5c070cf8":"markdown","3463bcfb":"markdown","ea4f668d":"markdown","fcf45578":"markdown","d740c607":"markdown","03d9d99b":"markdown","e5a9a1fa":"markdown","a3367fb1":"markdown","2836575a":"markdown","efc98a3f":"markdown","8564f903":"markdown","1a44f0e6":"markdown","f8a0433c":"markdown","f74e2d3a":"markdown","10b8734a":"markdown","442ce98a":"markdown"},"source":{"1ba6d7ba":"!pip install rank_bm25 nltk\n\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nimport torch\nimport numpy as np\n\nimport os\nimport json\nfrom tqdm import tqdm\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path, PurePath\nimport pandas as pd\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom rank_bm25 import BM25Okapi\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nimport re\n\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nimport pandas as pd\n\ndef set_column_width(ColumnWidth, MaxRows):\n    pd.options.display.max_colwidth = ColumnWidth\n    pd.options.display.max_rows = MaxRows\n    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n    \ninteract(set_column_width, \n         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n         MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));","9d7f0eea":"def getListOfFiles(dirName):\n    # create a list of file and sub directories \n    # names in the given directory \n    listOfFile = os.listdir(dirName)\n    allFiles = list()\n    # Iterate over all the entries\n    for entry in listOfFile:\n        # Create full path\n        fullPath = os.path.join(dirName, entry)\n        # If entry is a directory then get the list of files in this directory \n        if os.path.isdir(fullPath):\n            allFiles = allFiles + getListOfFiles(fullPath)\n        else:\n            if fullPath[-5:] == '.json': allFiles.append(fullPath)\n                \n    return allFiles\n\n#extracts the abstract and body text of a .json file\ndef clean_json(path):\n    #loads json file into a string\n    with open(path) as f:\n        dic = json.load(f)\n        \n    try:    \n        paper_id = dic['paper_id']\n    \n        abstract = dic['abstract'][0]['text']\n    except:\n        abstract= \"\"\n    txt= \"\"\n    for i in range(len(dic['body_text'])):\n        par = dic['body_text'][i]['text']\n        txt = txt + ' ' + par\n    return paper_id, abstract, txt\n\n#getting all the paths into a list\npaths = getListOfFiles('\/kaggle\/input\/CORD-19-research-challenge')\n\n\nids = []\nabstracts = []\ntexts = []\nfor path in tqdm(paths):\n    paper_id, abstract, txt = clean_json(path)\n    ids.append(paper_id)\n    abstracts.append(abstract)\n    texts.append(txt)\n        \nids_texts = pd.DataFrame(data=list(zip(ids, texts)),columns=['sha', 'text'])","6e9031ab":"# Where are all the files located\ninput_dir = PurePath('..\/input\/CORD-19-research-challenge')\nmetadata_path = input_dir \/ 'metadata.csv'\nmetadata = pd.read_csv(metadata_path, dtype={'Microsoft Academic Paper ID': str,\n                                              'pubmed_id': str})\n\n# Set the abstract to the paper title if it is null\nmetadata.abstract = metadata.abstract.fillna(metadata.title)\n\n#all articles with sha\nwith_sha = metadata.loc[~metadata.sha.isnull()]\n\n#all articles without sha (doesn't have the full text)\nnot_sha = metadata.loc[metadata.sha.isnull()]\n\n#If no sha, then use the abstract as the text of the article.\nnot_sha['text'] = not_sha['abstract']\n\n#If the article doesn't have the abstract,then use the title\nidx = not_sha.loc[not_sha.text.isnull()].index\nnot_sha.text.loc[idx] = not_sha.title.loc[idx]\n\n#if it doesn't have a title, then drop it\nnot_sha.drop(not_sha.text.isnull().index,inplace=True)\n\n#now merging the metadata with the dataframe that has the full texts\nmerged = with_sha.merge(ids_texts, on=['sha'], how='inner')\nmerged = pd.concat((merged, not_sha))\nmerged = merged.reset_index()\nmerged.head()","114ab16a":"def get(url, timeout=6):\n    try:\n        r = requests.get(url, timeout=timeout)\n        return r.text\n    except ConnectionError:\n        print(f'Cannot connect to {url}')\n        print(f'Remember to turn Internet ON in the Kaggle notebook settings')\n    except HTTPError:\n        print('Got http error', r.status, r.text)\n\n# Convert the doi to a url\ndef doi_url(d): \n    return f'http:\/\/{d}' if d.startswith('doi.org') else f'http:\/\/doi.org\/{d}'\n\n\n# class AllPaperData:\n#     def __init__(self, metadata: pd.DataFrame):\n#         self.metadata = metadata\n        \n#     def __getitem__(self, item):\n#         return Paper(self.metadata.iloc[item])\n    \n#     def __len__(self):\n#         return len(self.metadata)\n    \n#     def head(self, n):\n#         return AllPaperData(self.metadata.head(n).copy().reset_index(drop=True))\n    \n#     def tail(self, n):\n#         return AllPaperData(self.metadata.tail(n).copy().reset_index(drop=True))\n\n#     def abstracts(self):\n#         return self.metadata.abstract.dropna()\n    \n#     def titles(self):\n#         return self.metadata.title.dropna()\n        \n#     def _repr_html_(self):\n#         return self.metadata._repr_html_()\n    \nclass Paper:\n    def __init__(self, item):\n        self.paper = item.to_frame().fillna('')\n        self.paper.columns = ['Value']\n    \n    def doi(self):\n        return self.paper.loc['doi'].values[0]\n    \n    def html(self):\n        '''\n        Load the paper from doi.org and display as HTML. Requires internet to be ON\n        '''\n        if self.doi():\n            url = doi_url(self.doi())\n            text = get(url)\n            return widgets.HTML(text)\n    \n    def text(self):\n        '''\n        Load the paper from doi.org and display as text. Requires Internet to be ON\n        '''\n#         text = get(self.doi())\n        return self.paper.loc['text'].values[0]\n    \n    def abstract(self):\n        return self.paper.loc['abstract'].values[0]\n    \n    def title(self):\n        return self.paper.loc['title'].values[0]\n    \n    def authors(self, split=False):\n        '''\n        Get a list of authors\n        '''\n        authors = self.paper.loc['authors'].values[0]\n        if not authors:\n            return []\n        if not split:\n            return authors\n        if authors.startswith('['):\n            authors = authors.lstrip('[').rstrip(']')\n            return [a.strip().replace(\"\\'\", \"\") for a in authors.split(\"\\',\")]\n        \n        # Todo: Handle cases where author names are separated by \",\"\n        return [a.strip() for a in authors.split(';')]\n        \n    def _repr_html_(self):\n        return self.paper._repr_html_()\n    \n\n# papers = AllPaperData(merged)","f4e88cb7":"english_stopwords = list(set(stopwords.words('english')))\nSEARCH_DISPLAY_COLUMNS = ['title', 'abstract', 'doi', 'authors', 'journal', 'text']\n\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\ndef clean(text):\n    t = text.lower()\n    t = strip_characters(t)\n    return t\n\ndef tokenize(text):\n    words = nltk.word_tokenize(text)\n    return list(set([word for word in words \n                     if len(word) > 1\n                     and not word in english_stopwords\n                     and not (word.isnumeric() and len(word) is not 4) # don't remove years\n                     and (not word.isnumeric() or word.isalpha())] )\n               )\n\ndef preprocess(text):\n    t = clean(text)\n    tokens = tokenize(t)\n    return tokens","cd8e4ce5":"class SearchResults:\n    def __init__(self,\n                 data: pd.DataFrame,\n                 columns = None):\n        self.results = data\n        if columns:\n            self.results = self.results[columns]\n            \n    def __getitem__(self, item):\n        return Paper(self.results.loc[item])\n    \n    def __len__(self):\n        return len(self.results)\n        \n    def _repr_html_(self):\n        return self.results._repr_html_()\n    \n    def set_ans(self, ans):\n        col_name = self.results.columns.tolist()\n        col_name.insert(2,'Answer')\n        self.results = self.results.reindex(columns=col_name)\n        self.results['Answer'] = ans\n        \nclass WordTokenIndex:\n    def __init__(self, \n                 corpus: pd.DataFrame, \n                 columns=SEARCH_DISPLAY_COLUMNS):\n        self.corpus = corpus\n        # Use abstart and title to create index\n        raw_search_str = self.corpus.abstract.fillna('') + ' ' + self.corpus.title.fillna('')\n        self.index = raw_search_str.apply(preprocess).to_frame()\n        self.index.columns = ['terms']\n        self.index.index = self.corpus.index\n        self.columns = columns\n    \n    def search(self, search_string):\n        search_terms = preprocess(search_string)\n        # get indexs that include the search_string\n        result_index = self.index.terms.apply(lambda terms: any(i in terms for i in search_terms))\n        # get searched papers\n        results = self.corpus[result_index].copy().reset_index().rename(columns={'index':'paper'})\n        return SearchResults(results, self.columns + ['paper'])\n    \nclass RankBM25Index(WordTokenIndex):\n    ''' Rank '''\n    def __init__(self, corpus: pd.DataFrame, columns=SEARCH_DISPLAY_COLUMNS):\n        super().__init__(corpus, columns)\n        self.bm25 = BM25Okapi(self.index.terms.tolist())\n        \n    def search(self, search_string, n=4):\n        search_terms = preprocess(search_string)\n        doc_scores = self.bm25.get_scores(search_terms)\n        \n        ind = np.argsort(doc_scores)[::-1][:n]\n        results = self.corpus.iloc[ind][self.columns]\n        results['Score'] = doc_scores[ind]\n        results = results[results.Score > 0]\n        return SearchResults(results.reset_index(), self.columns + ['Score'])","b084a42d":"# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('\/kaggle\/input\/bert-covid\/output_squad')\nmodel = BertForQuestionAnswering.from_pretrained('\/kaggle\/input\/bert-covid\/output_squad')","c34b4297":"max_question_len = 100\ndef getAnswer(question, text):\n    input_ids = tokenizer.encode(question, text, max_length=512)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n#     print(len(input_ids))\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    start_scores = start_scores[0, input_ids.index(102):]\n    end_scores = end_scores[0, input_ids.index(102):]\n    if torch.argmax(start_scores).item() >= torch.argmax(end_scores).item():\n        return \"No Answer\"\n#     print(input_ids.index(102) + torch.argmax(start_scores))\n#     print(input_ids.index(102) + torch.argmax(end_scores) + 1)\n    all_tokens = all_tokens[: input_ids.index(102) + torch.argmax(start_scores)] + [\"\u3010\"] + all_tokens[input_ids.index(102) + torch.argmax(start_scores):]\n    all_tokens = all_tokens[: input_ids.index(102) + torch.argmax(end_scores) + 1] + [\"\u3011\"] + all_tokens[input_ids.index(102) + torch.argmax(end_scores) + 1:]\n    start_span = max(input_ids.index(102) + torch.argmax(start_scores) - 5, input_ids.index(102))\n    end_span = min(input_ids.index(102) + torch.argmax(end_scores) + 6, len(all_tokens) + 1)\n\n    answer = tokenizer.convert_tokens_to_string(all_tokens[start_span : end_span])\n    answer = answer.replace(\"[SEP]\", \"\")\n    return answer\n\n\ndef searchAndGetAnswer(question):\n    # Filter out relevant papers\n    results = bm25_index.search(question, 50)\n\n#     print(\"Search item num: %d\" % len(results))\n\n    # Get answers\n    ans_list = []\n    ans_index = []\n    for i in range(len(results)):\n        text = results[i].text()\n        ans = getAnswer(question, text)\n        if ans == \"No Answer\":\n            continue\n        ans_list.append(ans)\n        ans_index.append(i)\n        print(\"{}\/{}\".format(len(ans_index), 10))\n        if len(ans_index) >= 10:\n            break\n    results.results = results.results.loc[ans_index, :].reset_index()\n\n    # Append to metadata\n    results.set_ans(ans_list)\n    \n    # Change the order\n    cols=['title','abstract','Answer','Score','doi','authors','journal']\n    res = results.results.loc[:,cols]\n    return res","f73792da":"# Build index on coprus\nbm25_index = RankBM25Index(merged)","db2f1a69":"p = \"Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\"\nres = searchAndGetAnswer(p)\nres","698d0371":"p = \"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\"\nres = searchAndGetAnswer(p)\nres","d15ded81":"p = '''\nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\nEvidence of whether farmers are infected, and whether farmers could have played a role in the origin.\nSurveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\nExperimental infections to test host range for this pathogen.\n'''\nres = searchAndGetAnswer(p)\nres","f9d5f1d7":"p = '''\nAnimal host(s) and any evidence of continued spill-over to humans\n'''\nres = searchAndGetAnswer(p)\nres","b7ed6ff3":"p = '''\nSocioeconomic and behavioral risk factors for this spill-over\n'''\nres = searchAndGetAnswer(p)\nres","d59e0763":"p = '''\nSustainable risk reduction strategies\n'''\nres = searchAndGetAnswer(p)\nres","864e559b":"## Bert furtuer pre-train and fine-tune\nUse transformers bert implementation and load the pre-trained model 'bert-base-uncased'. We use all papers as corpus to further pre-train the bert with language model task. After further pre-train, we use SQuAD-2.0 dataset to fine-tune the bert model. Finally, we save the trained model which named as \"output_squad\".","5c070cf8":"### Q5: *Socioeconomic and behavioral risk factors for this spill-over*","3463bcfb":"### Q4: *Animal host(s) and any evidence of continued spill-over to humans*","ea4f668d":"### Q1: *Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.*","fcf45578":"## QA function\nCreate QA function which receive a question and a piece of text, feed them in to bert model and output the possible answer in the text. If model outputs invalid answer, the function will output \"No Answer\".","d740c607":"Create class Paper to make it easier to get some field like abstract, full text, doi and so on.","03d9d99b":"### Q3: *Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.*\n* Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n* Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n* Experimental infections to test host range for this pathogen.","e5a9a1fa":"## BM25 Search Engine\nDefine class to handle index building and string searching.","a3367fb1":"## Data Preprocess\n\nLoad full-text paper and clean them. Read json files and build dataframe with two columns: sha, text. We will merge this with metadata in the following cell.","2836575a":"Read metadata file and build a dataframe to store. Drop items which don't have full text.","efc98a3f":"![%E4%B8%8B%E8%BD%BD.jfif](attachment:%E4%B8%8B%E8%BD%BD.jfif)\n# Bert-QA with BM25 search engine\nThis project use Bert model that further pre-trained on paper corpus and fine-tuned on SQuAD dataset to extract possiable answers from given papers. We use BM25 search engine to filter several most relative papers and rank them by relation score. Then we feed these papers and the questions to Bert which has been fine-tuned for question answering. Finally, we list each answer that produces by Bert in a table.","8564f903":"## Install and import packages\nInstall and import packages","1a44f0e6":"Some preprocess functions to clean the origin data. We tokenize the text, remove punctuation and some special characters.","f8a0433c":"## Build pipeline and show results\nWe show the model output in tables and enclosed them with \"\u3010\u3011\" to represent the answer in the paper.","f74e2d3a":"### Q2: *Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.*","10b8734a":"## Background\n### Bert for QA\n![bert.png](attachment:bert.png)\n\nBERT (Bidirectional Encoder Representations from Transformers) is a recent paper published by researchers at Google AI Language. It has caused a stir in the Machine Learning community by presenting state-of-the-art results in a wide variety of NLP tasks, including Question Answering (SQuAD v1.1), Natural Language Inference (MNLI), and others.\n\nBERT\u2019s key technical innovation is applying the bidirectional training of Transformer, a popular attention model, to language modelling. This is in contrast to previous efforts which looked at a text sequence either from left to right or combined left-to-right and right-to-left training. The paper\u2019s results show that a language model which is bidirectionally trained can have a deeper sense of language context and flow than single-direction language models. In the paper, the researchers detail a novel technique named Masked LM (MLM) which allows bidirectional training in models in which it was previously impossible.\n\nQuestion answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP), which is concerned with building systems that automatically answer questions posed by humans in a natural language\n\n\n### BM25\n![image.png](attachment:image.png)\nIn information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Sp\u00e4rck Jones, and others.\n\nThe name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London's City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent state-of-the-art TF-IDF-like retrieval functions used in document retrieval.","442ce98a":"### Q6: *Sustainable risk reduction strategies*"}}