{"cell_type":{"3b7952d0":"code","62dbcbb4":"code","da4d406c":"code","425e1603":"code","ddc63680":"code","5c2ae426":"code","b4b8de87":"code","6f4454ae":"code","6148c955":"code","663e6e2d":"code","4c298512":"code","e91716dc":"code","8ee744ce":"code","101a7c7f":"code","1cafe562":"code","cbdb82ff":"code","4233aad8":"code","fe905788":"code","79dbbe5c":"code","94d041b9":"code","2619f6dd":"code","a5f3c18b":"code","54d08344":"code","fc09844e":"code","95554e10":"code","6145032c":"code","0c90b412":"code","9782ccf8":"code","59d81029":"code","2f0289ab":"code","8a6a3e1b":"code","6732bb11":"code","7a4e57b7":"code","174f5fdb":"code","c7ca910c":"code","496ebd0f":"code","1e396c88":"code","cde68440":"code","6dc82888":"code","4f7c4900":"code","20facd82":"code","04b67673":"code","cd9f1bd2":"code","39f71ba9":"code","e5adf772":"markdown","96331f0a":"markdown","7754c476":"markdown","409f7161":"markdown","294b606b":"markdown","3c682185":"markdown","f0f82081":"markdown","186d5c5b":"markdown","7576a538":"markdown","447da0a8":"markdown","fdb1f42d":"markdown","ac11bf48":"markdown","54df3f7e":"markdown","26500a99":"markdown","7ca06adc":"markdown","af46f2bc":"markdown","99888a2d":"markdown","44f9c2bc":"markdown","28eb62ce":"markdown"},"source":{"3b7952d0":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn import preprocessing\n\n\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport nltk\nfrom nltk.tokenize import word_tokenize,RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom keras.optimizers import Adam\nimport os\n\n\nfrom textblob import TextBlob\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","62dbcbb4":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","da4d406c":"train","425e1603":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","ddc63680":"train.isnull().sum()","5c2ae426":"test.isnull().sum()","b4b8de87":"train['target'].value_counts()","6f4454ae":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","6148c955":"disaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","663e6e2d":"non_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","4c298512":"\ndef remove_nick_name(text):\n    \n    return re.sub(r\"\\@\\S+\", \"\", text)\n\ntrain['text']=train['text'].apply(lambda x : remove_nick_name(x))\ntest['text']=test['text'].apply(lambda x : remove_nick_name(x))\ntrain[500:700]\n","e91716dc":"def remove_numbers(self):\n    result = ''.join(i for i in self if not i.isdigit())\n    return(result)\n\n\ntrain['text']=train['text'].apply(lambda x : remove_numbers(x))\ntest['text']=test['text'].apply(lambda x : remove_numbers(x))\ntrain\n","8ee744ce":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))\ntest['text']=test['text'].apply(lambda x : remove_URL(x))\ntest\n","101a7c7f":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))\n\ntrain.head()\n","1cafe562":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ntrain['text']=train['text'].apply(lambda x: remove_emoji(x))\ntest['text']=test['text'].apply(lambda x: remove_emoji(x))\ntrain.head()\n","cbdb82ff":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\ntrain.head()\n","4233aad8":"train = train.applymap(lambda s:s.lower() if type(s) == str else s)\ntest = test.applymap(lambda s:s.lower() if type(s) == str else s)","fe905788":"!pip install textblob","79dbbe5c":"\ndef polarity_check_sayi(self):\n    polarity_list = []\n    for el in self['text']:\n        result = TextBlob(el).sentiment.polarity\n        polarity_list.append(result)\n\n    self['polarity'] = polarity_list\n    \n    \n\ndef polarity_check_label(self):\n    polarity_list = []\n    for el in self['text']:\n        result = TextBlob(el).sentiment.polarity\n        if result < 0:\n            check = 'Negative'\n        elif result == 0:\n            check = 'Neutral'\n        else:\n            check = 'Positive'\n            \n        polarity_list.append(check)\n\n    self['polarity'] = polarity_list\n    \n\n    \npolarity_check_label(test)\npolarity_check_label(train)\n\ntest","94d041b9":"loc_dict={'United States':'USA','New York':'USA',\"London\":'UK',\"Los Angeles, CA\":'USA',\"Washington, D.C.\":'USA',\n          \"California\":'USA',\"Chicago, IL\":'USA',\"Chicago\":'USA',\"New York, NY\":'USA',\"California, USA\":'USA',\n          \"FLorida\":'USA',\"Nigeria\":'Africa',\"Kenya\":'Africa',\"Everywhere\":'Worldwide',\"San Francisco\":'USA',\n          \"Florida\":'USA',\"United Kingdom\":'UK',\"Los Angeles\":'USA',\"Toronto\":'Canada',\"San Francisco, CA\":'USA',\n          \"NYC\":'USA',\"Seattle\":'USA',\"Earth\":'Worldwide',\"Ireland\":'UK',\"London, England\":'UK',\"New York City\":'USA',\n          \"Texas\":'USA',\"London, UK\":'UK',\"Atlanta, GA\":'USA',\"Mumbai\":\"India\"}\n\ntrain['location'].replace(loc_dict,inplace=True)\n\n#Create barchart for top 10 locations using seaborn\nsns.barplot(y=train['location'].value_counts()[:10].index,x=train['location'].value_counts()[:10],\n            orient='h');","2619f6dd":"train = train.replace(np.nan, 'usa', regex=True)\ntest = test.replace(np.nan, 'usa', regex=True)\ntrain","a5f3c18b":"text = \"Benim ad\u0131m Ba\u015fak.\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\nprint(\"-\"*100)\nprint(\"Example Text: \",text)\nprint(\"-\"*100)\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","54d08344":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))","fc09844e":"train['text'].head()","95554e10":"test['text'].head()","6145032c":"def remove_stopwords(text):\n    \n\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","0c90b412":"train['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))","9782ccf8":"train.text.head()","59d81029":"test.text.head()","2f0289ab":"def combine_text(text):\n    \n    '''\n    Input-text= list cleand and tokenized text\n    Output- Takes a list of text and returns combined one large chunk of text.\n    \n    '''\n    all_text = ' '.join(text)\n    return all_text\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain.head()","8a6a3e1b":"count_vectorizer=CountVectorizer()\ntrain_cv=count_vectorizer.fit_transform(train[\"text\"])\ntest_cv=count_vectorizer.transform(test[\"text\"])\nprint(train_cv[0].todense())","6732bb11":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf=TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2))\ntrain_tf=tfidf.fit_transform(train[\"text\"])\ntest_tf=tfidf.transform(test[\"text\"])","7a4e57b7":"\n#Split the CountVector vectorized data into train and test datasets for model training and testing\nX_train_cv, X_test_cv, y_train_cv, y_test_cv =train_test_split(train_cv,train.target,test_size=0.2,random_state=2020)","174f5fdb":"def fit_and_predict(model,X_train,y_train,X_test,y_test):\n    \n    '''Input- model=model to be trained\n              X_train, y_train= traing data set\n              X_test,  y_test = testing data set\n       Output- Print accuracy of model for training and test data sets   \n    '''\n    \n    # Fitting a simple Logistic Regression on Counts\n    clf = model\n    clf.fit(X_train, y_train)\n    predictions=clf.predict(X_test)\n    confusion_matrix(y_test,predictions)\n    print(classification_report(y_test,predictions))\n    print('-'*50)\n    print(\"{}\" .format(model))\n    print('-'*50)\n    print('Accuracy of classifier on training set:{}%'.format(round(clf.score(X_train, y_train)*100)))\n    print('-'*50)\n    print('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test,predictions)*100)))\n    print('-'*50)","c7ca910c":"models=[LogisticRegression(C=1.0),SVC(),MultinomialNB(),DecisionTreeClassifier(),\n        KNeighborsClassifier(n_neighbors=5),RandomForestClassifier()]","496ebd0f":"\n# Loop through the list of models and use 'fit_and_predict()' function to trian and make predictions\nfor model in models:\n    fit_and_predict(model,X_train_cv, y_train_cv,X_test_cv,y_test_cv)","1e396c88":"# Split the TFDIF vectorized data into train and test datasets for model training and testing\nX_train_tf, X_test_tf, y_train_tf, y_test_tf =train_test_split(train_tf,train.target,test_size=0.2,random_state=2020)","cde68440":"# Loop through the list of models and use 'fit_and_predict()' function to train and make predictions on the TFDIF vectororized data\nfor model in models:\n    fit_and_predict(model,X_train_tf, y_train_tf,X_test_tf,y_test_tf)","6dc82888":"\n# Printing model performance results.\nresults_dict={'Classifier':['Logistic regression', 'SVC', 'MultinomialNB', 'DecisionTreeClassifier',\n                            'KNeighborsClassifier','RandomForestClassifier'],\n              'F1-Score':[0.81, 0.40, .80, .75,0.65,0.76],'Accuracy':['81%', '56%', '80%','75%','69%','77%']} \nresults=pd.DataFrame(results_dict)\nresults","4f7c4900":"\n# Printing model performance results.\nresults_dict={'Classifier':['Logistic regression', 'SVC', 'MultinomialNB', 'DecisionTreeClassifier',\n                            'KNeighborsClassifier','RandomForestClassifier'],\n              'F1-Score':[0.81, 0.40, .80, .75,0.65,0.76],'Accuracy':['81%', '56%', '80%','75%','69%','77%']} \nresults=pd.DataFrame(results_dict)\nresults","20facd82":"#LogisticRegression(C=1.0)","04b67673":"# Fitting 'LogisticRegression()' with CountVectorizer() fit dataset\nclf_logreg = MultinomialNB()\nclf_logreg.fit(X_train_cv, y_train_cv)\npred=clf_logreg.predict(X_test_cv)\nconfusion_matrix(y_test_cv,pred)\nprint(classification_report(y_test_cv,pred))\nprint('Accuracy of classifier on training set:{}%'.format(round(clf_logreg.score(X_train_cv, y_train_cv)*100)))\nprint('Accuracy of classifier on test set:{}%' .format(round(accuracy_score(y_test_cv,pred)*100)))","cd9f1bd2":"clf_logreg","39f71ba9":"sub_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsub_df[\"target\"] = clf_logreg.predict(test_cv)\nsub_df.to_csv(\"submission.csv\", index=False)","e5adf772":"Train ve Test verimizdeki b\u00fct\u00fcn harfleri, k\u00fc\u00e7\u00fck harf yapal\u0131m.","96331f0a":"Text olarak verilen tweetlerin, sentiment(olumluluk\/ olumsuzluk) durumlar\u0131n\u0131 inceleyip, polarity sutunu olu\u015fturup buraya kaydeder.","7754c476":"Afet ve afet d\u0131\u015f\u0131 tweetlerden bir \u00f6rne\u011fi inceleyelim.","409f7161":"train.dropna(subset=['keyword', 'location'], how='all', inplace=True)\ntest.dropna(subset=['keyword', 'location'], how='all', inplace=True)\n#test.dropna(inplace=True)\n\ntrain","294b606b":"Yukar\u0131da dizin olarak ayr\u0131lan kelimeleri pre processing sonras\u0131 tekrar bir araya getirdik.","3c682185":"Test ve Train datalar\u0131nda location verisinde bir\u00e7ok de\u011fer eksik. Hedef s\u00fctunumuza bir g\u00f6z atal\u0131m.","f0f82081":"**Train ve Test verisi preprocessing**\n\ntrain ve test verisi i\u00e7erisindeki @<kullanici_adi> ibareleri temizleyelim.\n\n","186d5c5b":"train ve test verisi i\u00e7erisindeki url temizleyelim.","7576a538":"Location s\u00fctunumuzdaki bo\u015f olan verilere en \u00e7ok kullan\u0131lan veriyi ekleyelim.","447da0a8":"sub_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsub_df[\"target\"] = model.predict(test_cv)\nsub_df.to_csv(\"submission.csv\", index=False)\n","fdb1f42d":"train = train.applymap(lambda s:s.lower() if type(s) == str else s)\ntest = test.applymap(lambda s:s.lower() if type(s) == str else s)","ac11bf48":"train ve test verisi i\u00e7erisindeki noktalama i\u015faretlerini temizleyelim.","54df3f7e":"train ve test verisi i\u00e7erisindeki numaralar\u0131 temizleyelim.","26500a99":"train ve test verisi i\u00e7erisindeki emojileri temizleyelim.","7ca06adc":"train ve test verisi i\u00e7erisindeki html temizleyelim.","af46f2bc":"Train ve Test datam\u0131zdaki  \"keyword\" ve \"locations\" sut\u00fcnlar\u0131n\u0131n her ikisinin de bo\u015f oldu\u011fu verileri silelim.","99888a2d":"Tokenization: Bir c\u00fcmleyi kelimelerine ay\u0131rmaya yarar. Kelimeler bir liste olu\u015fturur.","44f9c2bc":"\u0130ngilizce deki anlams\u0131z kelimeleri \u00e7\u0131kartal\u0131m \"the gibi\"","28eb62ce":"Veri setindeki locations sutununda en \u00e7ok kullan\u0131lan de\u011ferleri g\u00f6relim."}}