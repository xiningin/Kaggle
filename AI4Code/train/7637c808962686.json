{"cell_type":{"ae2a192e":"code","31a3a1fb":"code","a2d40fef":"code","b5a9f876":"code","5aaade9a":"code","c23cd3ca":"code","4bea4549":"code","4c1bef5f":"code","1398b42b":"code","36b8b667":"code","b331d5cd":"code","51a5d347":"code","541f93a6":"code","f486a545":"code","f42a0910":"code","1ea57c66":"code","14df17cb":"code","533d4ce0":"code","b411e0cf":"code","63f71bef":"code","16a77a7b":"code","8faa0ff2":"code","d3b7dde1":"markdown","fc95e49f":"markdown","524cc911":"markdown","c774f091":"markdown","890b1dcf":"markdown","f8d37593":"markdown","5e5f8083":"markdown","11df3217":"markdown","212626f2":"markdown"},"source":{"ae2a192e":"!pip install ..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\/","31a3a1fb":"# import glob\nimport os\nimport random\nimport warnings\nfrom functools import partial\n\n# import colorednoise as cn\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport scipy as sp\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.optim as optim\nfrom pytorch_lightning import LightningDataModule, callbacks\n\n# from pytorch_lightning.utilities import rank_zero_info\nfrom sklearn import model_selection\nfrom sklearn.metrics import f1_score\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchaudio.transforms import AmplitudeToDB, MelSpectrogram\n","a2d40fef":"import cv2\nimport audioread\nimport logging\nimport os\nimport random\nimport time\nimport warnings\n\nimport librosa\nimport numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport timm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data as torchdata\n\nfrom contextlib import contextmanager\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom albumentations.core.transforms_interface import ImageOnlyTransform\n# from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n# from torchlibrosa.augmentation import SpecAugmentation\nfrom tqdm import tqdm","b5a9f876":"def set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)  # type: ignore\n    torch.backends.cudnn.deterministic = True  # type: ignore\n    torch.backends.cudnn.benchmark = True  # type: ignore\n    \n    \ndef get_logger(out_file=None):\n    logger = logging.getLogger()\n    formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    logger.handlers = []\n    logger.setLevel(logging.INFO)\n\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    handler.setLevel(logging.INFO)\n    logger.addHandler(handler)\n\n    if out_file is not None:\n        fh = logging.FileHandler(out_file)\n        fh.setFormatter(formatter)\n        fh.setLevel(logging.INFO)\n        logger.addHandler(fh)\n    logger.info(\"logger set up\")\n    return logger\n    \n    \n@contextmanager\ndef timer(name: str, logger: Optional[logging.Logger] = None):\n    t0 = time.time()\n    msg = f\"[{name}] start\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)\n    yield\n\n    msg = f\"[{name}] done in {time.time() - t0:.2f} s\"\n    if logger is None:\n        print(msg)\n    else:\n        logger.info(msg)","5aaade9a":"logger = get_logger(\"main.log\")\nset_seed(1213)","c23cd3ca":"class CFG:\n    ######################\n    # Globals #\n    ######################\n    seed = 1213\n    epochs = 35\n    train = True\n    folds = [0]\n    img_size = 224\n    main_metric = \"epoch_f1_at_05\"\n    minimize_metric = False\n\n    ######################\n    # Data #\n    ######################\n    train_datadir = Path(\"..\/input\/birdclef-2021\/train_short_audio\")\n    train_csv = \"..\/input\/birdclef-2021\/train_metadata.csv\"\n    train_soundscape = \"..\/input\/birdclef-2021\/train_soundscape_labels.csv\"\n\n    ######################\n    # Dataset #\n    ######################\n    transforms = {\n        \"train\": [{\"name\": \"Normalize\"}],\n        \"valid\": [{\"name\": \"Normalize\"}],\n        \"test\": [{\"name\": \"Normalize\"}]\n    }\n    period = 30\n#     period = 5\n    n_mels = 128\n    \n    sample_rate = 32000\n\n    target_columns = [\n        'acafly', 'acowoo', 'aldfly', 'ameavo', 'amecro',\n        'amegfi', 'amekes', 'amepip', 'amered', 'amerob',\n        'amewig', 'amtspa', 'andsol1', 'annhum', 'astfly',\n        'azaspi1', 'babwar', 'baleag', 'balori', 'banana',\n        'banswa', 'banwre1', 'barant1', 'barswa', 'batpig1',\n        'bawswa1', 'bawwar', 'baywre1', 'bbwduc', 'bcnher',\n        'belkin1', 'belvir', 'bewwre', 'bkbmag1', 'bkbplo',\n        'bkbwar', 'bkcchi', 'bkhgro', 'bkmtou1', 'bknsti', 'blbgra1',\n        'blbthr1', 'blcjay1', 'blctan1', 'blhpar1', 'blkpho',\n        'blsspa1', 'blugrb1', 'blujay', 'bncfly', 'bnhcow', 'bobfly1',\n        'bongul', 'botgra', 'brbmot1', 'brbsol1', 'brcvir1', 'brebla',\n        'brncre', 'brnjay', 'brnthr', 'brratt1', 'brwhaw', 'brwpar1',\n        'btbwar', 'btnwar', 'btywar', 'bucmot2', 'buggna', 'bugtan',\n        'buhvir', 'bulori', 'burwar1', 'bushti', 'butsal1', 'buwtea',\n        'cacgoo1', 'cacwre', 'calqua', 'caltow', 'cangoo', 'canwar',\n        'carchi', 'carwre', 'casfin', 'caskin', 'caster1', 'casvir',\n        'categr', 'ccbfin', 'cedwax', 'chbant1', 'chbchi', 'chbwre1',\n        'chcant2', 'chispa', 'chswar', 'cinfly2', 'clanut', 'clcrob',\n        'cliswa', 'cobtan1', 'cocwoo1', 'cogdov', 'colcha1', 'coltro1',\n        'comgol', 'comgra', 'comloo', 'commer', 'compau', 'compot1',\n        'comrav', 'comyel', 'coohaw', 'cotfly1', 'cowscj1', 'cregua1',\n        'creoro1', 'crfpar', 'cubthr', 'daejun', 'dowwoo', 'ducfly', 'dusfly',\n        'easblu', 'easkin', 'easmea', 'easpho', 'eastow', 'eawpew', 'eletro',\n        'eucdov', 'eursta', 'fepowl', 'fiespa', 'flrtan1', 'foxspa', 'gadwal',\n        'gamqua', 'gartro1', 'gbbgul', 'gbwwre1', 'gcrwar', 'gilwoo',\n        'gnttow', 'gnwtea', 'gocfly1', 'gockin', 'gocspa', 'goftyr1',\n        'gohque1', 'goowoo1', 'grasal1', 'grbani', 'grbher3', 'grcfly',\n        'greegr', 'grekis', 'grepew', 'grethr1', 'gretin1', 'greyel',\n        'grhcha1', 'grhowl', 'grnher', 'grnjay', 'grtgra', 'grycat',\n        'gryhaw2', 'gwfgoo', 'haiwoo', 'heptan', 'hergul', 'herthr',\n        'herwar', 'higmot1', 'hofwoo1', 'houfin', 'houspa', 'houwre',\n        'hutvir', 'incdov', 'indbun', 'kebtou1', 'killde', 'labwoo', 'larspa',\n        'laufal1', 'laugul', 'lazbun', 'leafly', 'leasan', 'lesgol', 'lesgre1',\n        'lesvio1', 'linspa', 'linwoo1', 'littin1', 'lobdow', 'lobgna5', 'logshr',\n        'lotduc', 'lotman1', 'lucwar', 'macwar', 'magwar', 'mallar3', 'marwre',\n        'mastro1', 'meapar', 'melbla1', 'monoro1', 'mouchi', 'moudov', 'mouela1',\n        'mouqua', 'mouwar', 'mutswa', 'naswar', 'norcar', 'norfli', 'normoc', 'norpar',\n        'norsho', 'norwat', 'nrwswa', 'nutwoo', 'oaktit', 'obnthr1', 'ocbfly1',\n        'oliwoo1', 'olsfly', 'orbeup1', 'orbspa1', 'orcpar', 'orcwar', 'orfpar',\n        'osprey', 'ovenbi1', 'pabspi1', 'paltan1', 'palwar', 'pasfly', 'pavpig2',\n        'phivir', 'pibgre', 'pilwoo', 'pinsis', 'pirfly1', 'plawre1', 'plaxen1',\n        'plsvir', 'plupig2', 'prowar', 'purfin', 'purgal2', 'putfru1', 'pygnut',\n        'rawwre1', 'rcatan1', 'rebnut', 'rebsap', 'rebwoo', 'redcro', 'reevir1',\n        'rehbar1', 'relpar', 'reshaw', 'rethaw', 'rewbla', 'ribgul', 'rinkin1',\n        'roahaw', 'robgro', 'rocpig', 'rotbec', 'royter1', 'rthhum', 'rtlhum',\n        'ruboro1', 'rubpep1', 'rubrob', 'rubwre1', 'ruckin', 'rucspa1', 'rucwar',\n        'rucwar1', 'rudpig', 'rudtur', 'rufhum', 'rugdov', 'rumfly1', 'runwre1',\n        'rutjac1', 'saffin', 'sancra', 'sander', 'savspa', 'saypho', 'scamac1',\n        'scatan', 'scbwre1', 'scptyr1', 'scrtan1', 'semplo', 'shicow', 'sibtan2',\n        'sinwre1', 'sltred', 'smbani', 'snogoo', 'sobtyr1', 'socfly1', 'solsan',\n        'sonspa', 'soulap1', 'sposan', 'spotow', 'spvear1', 'squcuc1', 'stbori',\n        'stejay', 'sthant1', 'sthwoo1', 'strcuc1', 'strfly1', 'strsal1', 'stvhum2',\n        'subfly', 'sumtan', 'swaspa', 'swathr', 'tenwar', 'thbeup1', 'thbkin',\n        'thswar1', 'towsol', 'treswa', 'trogna1', 'trokin', 'tromoc', 'tropar',\n        'tropew1', 'tuftit', 'tunswa', 'veery', 'verdin', 'vigswa', 'warvir',\n        'wbwwre1', 'webwoo1', 'wegspa1', 'wesant1', 'wesblu', 'weskin', 'wesmea',\n        'westan', 'wewpew', 'whbman1', 'whbnut', 'whcpar', 'whcsee1', 'whcspa',\n        'whevir', 'whfpar1', 'whimbr', 'whiwre1', 'whtdov', 'whtspa', 'whwbec1',\n        'whwdov', 'wilfly', 'willet1', 'wilsni1', 'wiltur', 'wlswar', 'wooduc',\n        'woothr', 'wrenti', 'y00475', 'yebcha', 'yebela1', 'yebfly', 'yebori1',\n        'yebsap', 'yebsee1', 'yefgra1', 'yegvir', 'yehbla', 'yehcar1', 'yelgro',\n        'yelwar', 'yeofly1', 'yerwar', 'yeteup1', 'yetvir']\n\n    ######################\n    # Loaders #\n    ######################\n    loader_params = {\n        \"train\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": True\n        },\n        \"valid\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": False\n        },\n        \"test\": {\n            \"batch_size\": 64,\n            \"num_workers\": 20,\n            \"shuffle\": False\n        }\n    }\n\n    ######################\n    # Model #\n    ######################\n    models_cfg = [\n        {\"resnet34\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp004\/fold0__20210417-17-00-45\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",\n},\n        {\"efficientnet_b0\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp004\/fold0__20210418-10-44-57\/best_loss.ckpt\",},\n        {\"resnet18\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210420-06-34-46\/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210421-00-17-35\/best_loss.ckpt\",},\n        {\"mixnet_m\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210421-11-01-38\/best_loss.ckpt\",},\n        {\"repvgg_b0\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210421-21-55-49\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210422-11-13-45\/best_loss.ckpt\"},\n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp005\/fold0__20210423-04-29-31\/best_loss.ckpt\",},\n        \n        {\"repvgg_b0\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp008\/repvgg_b0\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        \n        {\"resnet34\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnet34\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"efficientnet_b0\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/efficientnet_b0\/fold0\/lightning_logs\/version_1\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnest50d_1s4x24d\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnest50d_1s4x24d\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet18\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnet18\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/ecaresnet26t\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/ecaresnet26t\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"mixnet_m\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/mixnet_m\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnest26d\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnest26d\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnest26d\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"repvgg_b0\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/repvgg_b0\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"repvgg_b0\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/repvgg_b0\/fold3\/lightning_logs\/version_0\/checkpoints\/checkpoints\/best_loss.ckpt\"},\n        {\"repvgg_b2\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/repvgg_b2\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"repvgg_b2\":\"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/repvgg_b2\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        \n        {\"resnet34\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/resnet34\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"efficientnet_b0\": \"..\/input\/birdclef20201-checkpoints\/checkpoints\/exp009\/efficientnet_b0\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnest50d_1s4x24d\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet18\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnet18\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/ecaresnet26t\/fold0\/lightning_logs\/version_1\/checkpoints\/best_loss.ckpt\",},\n        {\"mixnet_m\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/mixnet_m\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnest26d\/fold2\/lightning_logs\/version_1\/checkpoints\/best_loss.ckpt\"},\n        {\"repvgg_b0\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/repvgg_b0\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"repvgg_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/repvgg_b2\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        \n        {\"efficientnet_b0\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/efficientnet_b0_ft30\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest50d_1s4x24d_ft30\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"ecaresnet26t\":\"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/ecaresnet26t_ft30\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest26d_ft30\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"repvgg_b2\":\"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/repvgg_b2_ft30\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        \n        {\"efficientnet_b0\":\"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/efficientnet_b0_ft30\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest50d_1s4x24d_ft30\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"ecaresnet26t\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/ecaresnet26t_ft30\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest26d_ft30\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnest26d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest26d_ft30\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"repvgg_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/repvgg_b2_ft30\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        \n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnest50d_1s4x24d\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\":\"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnet50d\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\":\"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/resnet50d\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/efficientnet_b2\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/efficientnet_b2\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        \n        {\"resnest50d_1s4x24d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnest50d_1s4x24d_ft30\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnet50d_ft30\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/resnet50d_ft30\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\", },\n        {\"efficientnet_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/efficientnet_b2_ft30\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"efficientnet_b2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp010\/efficientnet_b2_ft30\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        \n        {\"wide_resnet50_2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/wide_resnet50_2\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"wide_resnet50_2\": \"..\/input\/birdclef20201-checkpoints2\/checkpoints\/exp009\/wide_resnet50_2\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        \n        {\"repvgg_b2\": \"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/repvgg_b2\/fold0\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"repvgg_b2\": \"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/repvgg_b2\/fold1\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\":\"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/resnet50d\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"resnet50d\":\"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/resnet50d\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\",},\n        {\"wide_resnet50_2\":\"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/wide_resnet50_2\/fold2\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n        {\"wide_resnet50_2\":\"..\/input\/birdclef20201-checkpoints3\/checkpoints\/exp011\/wide_resnet50_2\/fold3\/lightning_logs\/version_0\/checkpoints\/best_loss.ckpt\"},\n    ]\n    pretrained = False\n    num_classes = 397\n    in_channels = 1\nprint(f\"model_num: {len(CFG.models_cfg)}\")","4bea4549":"TARGET_SR = 32000\nTEST = (len(list(Path(\"..\/input\/birdclef-2021\/test_soundscapes\/\").glob(\"*.ogg\"))) != 0)\nif TEST:\n    DATADIR = Path(\"..\/input\/birdclef-2021\/test_soundscapes\/\")\nelse:\n    DATADIR = Path(\"..\/input\/birdclef-2021\/train_soundscapes\/\")","4c1bef5f":"all_audios = list(DATADIR.glob(\"*.ogg\"))\nall_audio_ids = [\"_\".join(audio_id.name.split(\"_\")[:2]) for audio_id in all_audios]\nsubmission_df = pd.DataFrame({\n    \"row_id\": all_audio_ids\n})\nsubmission_df","1398b42b":"def gem_freq(x, p=3, eps=1e-6):\n    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), 1)).pow(1.0 \/ p)\n\n\nclass GeMFreq(nn.Module):\n    def __init__(self, p=3, eps=1e-6):\n        super().__init__()\n        self.p = torch.nn.Parameter(torch.ones(1) * p)\n        self.eps = eps\n\n    def forward(self, x):\n        return gem_freq(x, p=self.p, eps=self.eps)\n\n\nclass NormalizeMelSpec(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n\n    def forward(self, X):\n        mean = X.mean((1, 2), keepdim=True)\n        std = X.std((1, 2), keepdim=True)\n        Xstd = (X - mean) \/ (std + self.eps)\n        norm_min, norm_max = Xstd.min(-1)[0].min(-1)[0], Xstd.max(-1)[0].max(-1)[0]\n        fix_ind = (norm_max - norm_min) > self.eps * torch.ones_like(\n            (norm_max - norm_min)\n        )\n        V = torch.zeros_like(Xstd)\n        if fix_ind.sum():\n            V_fix = Xstd[fix_ind]\n            norm_max_fix = norm_max[fix_ind, None, None]\n            norm_min_fix = norm_min[fix_ind, None, None]\n            V_fix = torch.max(\n                torch.min(V_fix, norm_max_fix),\n                norm_min_fix,\n            )\n            # print(V_fix.shape, norm_min_fix.shape, norm_max_fix.shape)\n            V_fix = (V_fix - norm_min_fix) \/ (norm_max_fix - norm_min_fix)\n            V[fix_ind] = V_fix\n        return V\n\n\nclass AttHead(nn.Module):\n    def __init__(\n        self, in_chans, p=0.5, num_class=397, train_period=15.0, infer_period=5.0\n    ):\n        super().__init__()\n        self.train_period = train_period\n        self.infer_period = infer_period\n        self.pooling = GeMFreq()\n\n        self.dense_layers = nn.Sequential(\n            nn.Dropout(p \/ 2),\n            nn.Linear(in_chans, 512),\n            nn.ReLU(),\n            nn.Dropout(p),\n        )\n        self.attention = nn.Conv1d(\n            in_channels=512,\n            out_channels=num_class,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n        self.fix_scale = nn.Conv1d(\n            in_channels=512,\n            out_channels=num_class,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True,\n        )\n\n    def forward(self, feat):\n        feat = self.pooling(feat).squeeze(-2).permute(0, 2, 1)  # (bs, time, ch)\n\n        feat = self.dense_layers(feat).permute(0, 2, 1)  # (bs, 512, time)\n        time_att = torch.tanh(self.attention(feat))\n        assert self.train_period >= self.infer_period\n        if self.training or self.train_period == self.infer_period:\n\n            clipwise_pred = torch.sum(\n                torch.sigmoid(self.fix_scale(feat)) * torch.softmax(time_att, dim=-1),\n                dim=-1,\n            )  # sum((bs, 24, time), -1) -> (bs, 24)\n            logits = torch.sum(\n                self.fix_scale(feat) * torch.softmax(time_att, dim=-1),\n                dim=-1,\n            )\n        else:\n            framewise_pred_long = torch.sigmoid(self.fix_scale(feat))\n            clipwise_pred_long = torch.sum(framewise_pred_long * torch.softmax(time_att, dim=-1), dim=-1) \n            \n            feat_time = feat.size(-1)\n            start = (\n                feat_time \/ 2 - feat_time * (self.infer_period \/ self.train_period) \/ 2\n            )\n            end = start + feat_time * (self.infer_period \/ self.train_period)\n            start = int(start)\n            end = int(end)\n            feat = feat[:, :, start:end]\n            att = torch.softmax(time_att[:, :, start:end], dim=-1)\n#             print(feat_time, start, end)\n#             print(att_a.sum(), att.sum(), time_att.shape)\n            framewise_pred = torch.sigmoid(self.fix_scale(feat))\n            clipwise_pred = torch.sum(framewise_pred * att, dim=-1) \n            logits = torch.sum(\n                self.fix_scale(feat) * att,\n                dim=-1,\n            )\n            time_att = time_att[:, :, start:end]\n        return (\n            logits,\n            clipwise_pred,\n            self.fix_scale(feat).permute(0, 2, 1),\n            time_att.permute(0, 2, 1),\n            clipwise_pred_long,\n        )\n\n\nclass AttModel(nn.Module):\n    def __init__(\n        self,\n        backbone=\"resnet34\",\n        p=0.5,\n        n_mels=224,\n        num_class=397,\n        train_period=15.0,\n        infer_period=5.0,\n        in_chans=1,\n    ):\n        super().__init__()\n        self.n_mels = n_mels\n        self.logmelspec_extractor = nn.Sequential(\n            MelSpectrogram(\n                32000,\n                n_mels=n_mels,\n                f_min=20,\n                n_fft=2048,\n                hop_length=512,\n                normalized=True,\n            ),\n            AmplitudeToDB(top_db=80.0),\n            NormalizeMelSpec(),\n        )\n\n        self.backbone = timm.create_model(\n            backbone, features_only=True, pretrained=False, in_chans=in_chans\n        )\n        encoder_channels = self.backbone.feature_info.channels()\n        dense_input = encoder_channels[-1]\n        self.head = AttHead(\n            dense_input,\n            p=p,\n            num_class=num_class,\n            train_period=train_period,\n            infer_period=infer_period,\n        )\n\n    def forward(self, input):\n#         img = self.logmelspec_extractor(input)[\n#             :, None\n#         ]  # (batch_size, 1, mel_bins, time_steps)\n        feats = self.backbone(input)\n        return self.head(feats[-1])\n    \nclass Model(nn.Module):\n    def __init__(\n        self,\n        backbone=\"resnet34\",\n        p=0.5,\n        n_mels=224,\n        num_class=397,\n        train_period=15.0,\n        infer_period=5.0,\n        in_chans=1,\n    ):\n        super().__init__()\n        self.model = AttModel(backbone, p, n_mels, num_class, train_period, infer_period, in_chans)\n","36b8b667":"class TestDataset(torchdata.Dataset):\n    def __init__(self, df: pd.DataFrame, clip: np.ndarray, train_period=30, \n                 waveform_transforms=None):\n        self.df = df\n        self.clip = np.concatenate([clip, clip, clip])\n        self.train_period = train_period\n        self.waveform_transforms=waveform_transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx: int):\n        SR = 32000\n        sample = self.df.loc[idx, :]\n        row_id = sample.row_id\n\n        end_seconds = int(sample.seconds)\n        start_seconds = int(end_seconds - 5)\n        \n        end_index = int(SR * (end_seconds + (self.train_period - 5) \/ 2) + len(self.clip) \/\/ 3)\n        start_index = int(SR * (start_seconds - (self.train_period - 5) \/ 2) + len(self.clip) \/\/ 3)\n        \n        y = self.clip[start_index:end_index].astype(np.float32)\n\n        y = np.nan_to_num(y)\n\n        if self.waveform_transforms:\n            y = self.waveform_transforms(y)\n\n        y = np.nan_to_num(y)\n        \n        return y, row_id","b331d5cd":"def get_transforms(phase: str):\n    transforms = CFG.transforms\n    if transforms is None:\n        return None\n    else:\n        if transforms[phase] is None:\n            return None\n        trns_list = []\n        for trns_conf in transforms[phase]:\n            trns_name = trns_conf[\"name\"]\n            trns_params = {} if trns_conf.get(\"params\") is None else \\\n                trns_conf[\"params\"]\n            if globals().get(trns_name) is not None:\n                trns_cls = globals()[trns_name]\n                trns_list.append(trns_cls(**trns_params))\n\n        if len(trns_list) > 0:\n            return Compose(trns_list)\n        else:\n            return None\n\n\ndef get_waveform_transforms(config: dict, phase: str):\n    return get_transforms(config, phase)\n\n\ndef get_spectrogram_transforms(config: dict, phase: str):\n    transforms = config.get('spectrogram_transforms')\n    if transforms is None:\n        return None\n    else:\n        if transforms[phase] is None:\n            return None\n        trns_list = []\n        for trns_conf in transforms[phase]:\n            trns_name = trns_conf[\"name\"]\n            trns_params = {} if trns_conf.get(\"params\") is None else \\\n                trns_conf[\"params\"]\n            if hasattr(A, trns_name):\n                trns_cls = A.__getattribute__(trns_name)\n                trns_list.append(trns_cls(**trns_params))\n            else:\n                trns_cls = globals().get(trns_name)\n                if trns_cls is not None:\n                    trns_list.append(trns_cls(**trns_params))\n\n        if len(trns_list) > 0:\n            return A.Compose(trns_list, p=1.0)\n        else:\n            return None\n\n\nclass Normalize:\n    def __call__(self, y: np.ndarray):\n        max_vol = np.abs(y).max()\n        y_vol = y * 1 \/ max_vol\n        return np.asfortranarray(y_vol)\n\n\nclass NewNormalize:\n    def __call__(self, y: np.ndarray):\n        y_mm = y - y.mean()\n        return y_mm \/ y_mm.abs().max()\n\n\nclass Compose:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray):\n        for trns in self.transforms:\n            y = trns(y)\n        return y\n\n\nclass AudioTransform:\n    def __init__(self, always_apply=False, p=0.5):\n        self.always_apply = always_apply\n        self.p = p\n\n    def __call__(self, y: np.ndarray):\n        if self.always_apply:\n            return self.apply(y)\n        else:\n            if np.random.rand() < self.p:\n                return self.apply(y)\n            else:\n                return y\n\n    def apply(self, y: np.ndarray):\n        raise NotImplementedError\n\n\nclass NoiseInjection(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.noise_level = (0.0, max_noise_level)\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        noise_level = np.random.uniform(*self.noise_level)\n        noise = np.random.randn(len(y))\n        augmented = (y + noise * noise_level).astype(y.dtype)\n        return augmented\n\n\nclass GaussianNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal \/ (10 ** (snr \/ 20))\n\n        white_noise = np.random.randn(len(y))\n        a_white = np.sqrt(white_noise ** 2).max()\n        augmented = (y + white_noise * 1 \/ a_white * a_noise).astype(y.dtype)\n        return augmented\n\n\nclass PinkNoise(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20, sr=32000):\n        super().__init__(always_apply, p)\n\n        self.min_snr = min_snr\n        self.max_snr = max_snr\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        snr = np.random.uniform(self.min_snr, self.max_snr)\n        a_signal = np.sqrt(y ** 2).max()\n        a_noise = a_signal \/ (10 ** (snr \/ 20))\n\n        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n        a_pink = np.sqrt(pink_noise ** 2).max()\n        augmented = (y + pink_noise * 1 \/ a_pink * a_noise).astype(y.dtype)\n        return augmented\n\n\nclass PitchShift(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_range=5, sr=32000):\n        super().__init__(always_apply, p)\n        self.max_range = max_range\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        n_steps = np.random.randint(-self.max_range, self.max_range)\n        augmented = librosa.effects.pitch_shift(y, self.sr, n_steps)\n        return augmented\n\n\nclass TimeStretch(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, max_rate=1, sr=32000):\n        super().__init__(always_apply, p)\n        self.max_rate = max_rate\n        self.sr = sr\n\n    def apply(self, y: np.ndarray, **params):\n        rate = np.random.uniform(0, self.max_rate)\n        augmented = librosa.effects.time_stretch(y, rate)\n        return augmented\n\n\ndef _db2float(db: float, amplitude=True):\n    if amplitude:\n        return 10**(db \/ 20)\n    else:\n        return 10 ** (db \/ 10)\n\n\ndef volume_down(y: np.ndarray, db: float):\n    \"\"\"\n    Low level API for decreasing the volume\n    Parameters\n    ----------\n    y: numpy.ndarray\n        stereo \/ monaural input audio\n    db: float\n        how much decibel to decrease\n    Returns\n    -------\n    applied: numpy.ndarray\n        audio with decreased volume\n    \"\"\"\n    applied = y * _db2float(-db)\n    return applied\n\n\ndef volume_up(y: np.ndarray, db: float):\n    \"\"\"\n    Low level API for increasing the volume\n    Parameters\n    ----------\n    y: numpy.ndarray\n        stereo \/ monaural input audio\n    db: float\n        how much decibel to increase\n    Returns\n    -------\n    applied: numpy.ndarray\n        audio with increased volume\n    \"\"\"\n    applied = y * _db2float(db)\n    return applied\n\n\nclass RandomVolume(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, limit=10):\n        super().__init__(always_apply, p)\n        self.limit = limit\n\n    def apply(self, y: np.ndarray, **params):\n        db = np.random.uniform(-self.limit, self.limit)\n        if db >= 0:\n            return volume_up(y, db)\n        else:\n            return volume_down(y, db)\n\n\nclass OneOf:\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, y: np.ndarray):\n        n_trns = len(self.transforms)\n        trns_idx = np.random.choice(n_trns)\n        trns = self.transforms[trns_idx]\n        y = trns(y)\n        return y\n\n\nclass CosineVolume(AudioTransform):\n    def __init__(self, always_apply=False, p=0.5, limit=10):\n        super().__init__(always_apply, p)\n        self.limit = limit\n\n    def apply(self, y: np.ndarray, **params):\n        db = np.random.uniform(-self.limit, self.limit)\n        cosine = np.cos(np.arange(len(y)) \/ len(y) * np.pi * 2)\n        dbs = _db2float(cosine * db)\n        return y * dbs\n\n\ndef drop_stripes(image: np.ndarray, dim: int, drop_width: int, stripes_num: int):\n    total_width = image.shape[dim]\n    lowest_value = image.min()\n    for _ in range(stripes_num):\n        distance = np.random.randint(low=0, high=drop_width, size=(1,))[0]\n        begin = np.random.randint(\n            low=0, high=total_width - distance, size=(1,))[0]\n\n        if dim == 0:\n            image[begin:begin + distance] = lowest_value\n        elif dim == 1:\n            image[:, begin + distance] = lowest_value\n        elif dim == 2:\n            image[:, :, begin + distance] = lowest_value\n    return image\n\n\nclass TimeFreqMasking(ImageOnlyTransform):\n    def __init__(self,\n                 time_drop_width: int,\n                 time_stripes_num: int,\n                 freq_drop_width: int,\n                 freq_stripes_num: int,\n                 always_apply=False,\n                 p=0.5):\n        super().__init__(always_apply, p)\n        self.time_drop_width = time_drop_width\n        self.time_stripes_num = time_stripes_num\n        self.freq_drop_width = freq_drop_width\n        self.freq_stripes_num = freq_stripes_num\n\n    def apply(self, img, **params):\n        img_ = img.copy()\n        if img.ndim == 2:\n            img_ = drop_stripes(\n                img_, dim=0, drop_width=self.freq_drop_width, stripes_num=self.freq_stripes_num)\n            img_ = drop_stripes(\n                img_, dim=1, drop_width=self.time_drop_width, stripes_num=self.time_stripes_num)\n        return img_","51a5d347":"from torch.nn.modules.batchnorm import _BatchNorm\n\ndef prepare_model_for_inference(model, path: Path):\n    if not torch.cuda.is_available():\n        ckpt = torch.load(path, map_location=\"cpu\")\n    else:\n        ckpt = torch.load(path)\n    model.load_state_dict(ckpt[\"state_dict\"])\n    model.eval()\n\n    return model","541f93a6":"def prediction_for_clip(test_df: pd.DataFrame, \n                        clip: np.ndarray, \n                        models, \n                        threshold=0.05, \n                        threshold_long=None):\n\n    dataset = TestDataset(df=test_df, \n                          clip=clip,\n                          train_period = CFG.period, \n                          waveform_transforms=get_transforms(phase=\"test\"))\n    loader = torchdata.DataLoader(dataset, batch_size=1, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n#     [model.eval() for model in models]\n    prediction_dict = {}\n    for image, row_id in tqdm(loader):\n        row_id = row_id[0]\n        image = image.to(device)\n\n        with torch.no_grad():\n            image = models[0].logmelspec_extractor(image)[:, None]\n            probas = []\n            probas_long = []\n            for model in models:\n                with torch.cuda.amp.autocast():\n                    _, clipwise_pred, _, _, clipwise_pred_long = model(image)\n                probas.append(clipwise_pred.detach().cpu().numpy().reshape(-1))\n                probas_long.append(clipwise_pred_long.detach().cpu().numpy().reshape(-1))\n            probas = np.array(probas)\n            probas_long = np.array(probas_long)\n#             probas = np.array([model(image)[1].detach().cpu().numpy().reshape(-1) for model in models])\n        if threshold_long is None:\n            events = probas.mean(0) >= threshold\n        else:\n            events = ((probas.mean(0) >= threshold).astype(int) \\\n                      + (probas_long.mean(0) >= threshold_long).astype(int)) >= 2\n        labels = np.argwhere(events).reshape(-1).tolist()\n#         labels = labels[:2]\n        if len(labels) == 0:\n            prediction_dict[row_id] = \"nocall\"\n        else:\n            labels_str_list = list(map(lambda x: CFG.target_columns[x], labels))\n            label_string = \" \".join(labels_str_list)\n            prediction_dict[row_id] = label_string\n    return prediction_dict","f486a545":"def load_model(backbone_name, weight_path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = Model(\n        backbone_name,\n        p=0.5,\n        n_mels=CFG.n_mels,\n        num_class=CFG.num_classes,\n        train_period=CFG.period,\n        infer_period=5,\n    )\n    model = prepare_model_for_inference(model, weight_path).to(device)\n    model = model.model\n    return model\n\ndef prediction(test_audios,\n               models_cfg,\n               threshold=0.05, \n               threshold_long=None):\n    \n    models = [load_model(list(models_cfg.keys())[0], list(models_cfg.values())[0]) for models_cfg in models_cfg]\n    warnings.filterwarnings(\"ignore\")\n    prediction_dfs = []\n    for audio_path in test_audios:\n        with timer(f\"Loading {str(audio_path)}\", logger):\n            clip, _ = sf.read(audio_path)\n\n        seconds = []\n        row_ids = []\n        for second in range(5, 605, 5):\n            row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n            seconds.append(second)\n            row_ids.append(row_id)\n            \n        test_df = pd.DataFrame({\n            \"row_id\": row_ids,\n            \"seconds\": seconds\n        })\n        with timer(f\"Prediction on {audio_path}\", logger):\n            prediction_dict = prediction_for_clip(test_df,\n                                                  clip=clip,\n                                                  models=models,\n                                                  threshold=threshold, threshold_long=threshold_long)\n        row_id = list(prediction_dict.keys())\n        birds = list(prediction_dict.values())\n        prediction_df = pd.DataFrame({\n            \"row_id\": row_id,\n            \"birds\": birds\n        })\n        prediction_dfs.append(prediction_df)\n    \n    prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n    return prediction_df","f42a0910":"threshold = 0.025\nthreshold_long = 0.05","1ea57c66":"# # site_mask = pd.read_csv(\"..\/input\/site-month-mask\/site_mask.csv\")\n# # site_mask[\"all\"] = site_mask.drop('other', axis=1).T.sum()\n# # rare_threshold = 10\n# # common_threshold = 200\n# # print(\"common, rare: \", site_mask[\"all\"][site_mask[\"all\"].values>common_threshold].sum(), site_mask[\"all\"][site_mask[\"all\"].values<rare_threshold].sum())\n# # print(f\"common classes:\", (site_mask[\"all\"].values>common_threshold).sum(), \"rare classes: \", (site_mask[\"all\"].values<rare_threshold).sum())\n# # threshold = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold, np.ones_like(site_mask[\"all\"].values) * threshold * 4)\n# # threshold = np.where((site_mask[\"all\"].values>common_threshold), np.ones_like(site_mask[\"all\"].values) * threshold \/ 2, np.ones_like(site_mask[\"all\"].values) * threshold)\n\n# # threshold_long = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long, np.ones_like(site_mask[\"all\"].values) * threshold_long * 4)\n# # threshold_long = np.where((site_mask[\"all\"].values>common_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long \/ 2, np.ones_like(site_mask[\"all\"].values) * threshold_long)\n\n# site_mask = pd.read_csv(\"..\/input\/site-month-mask\/site_mask.csv\")\n# site_mask[\"all\"] = site_mask.drop('other', axis=1).T.sum()\n# rare_threshold = 10\n# print(\"common, rare: \", site_mask[\"all\"][site_mask[\"all\"].values>rare_threshold].sum(), site_mask[\"all\"][site_mask[\"all\"].values<rare_threshold].sum())\n# print(\"common classes: \", (site_mask[\"all\"].values>rare_threshold).sum())\n# threshold = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold, np.ones_like(site_mask[\"all\"].values) * threshold * 4)\n# threshold_long = np.where((site_mask[\"all\"].values>rare_threshold), np.ones_like(site_mask[\"all\"].values) * threshold_long, np.ones_like(site_mask[\"all\"].values) * threshold_long * 4)","14df17cb":"# month_mask = pd.read_csv(\"..\/input\/site-month-mask\/month_mask.csv\")\n\n# month_mask[\"all\"] = month_mask.T.sum()\n# rare_threshold = 100\n# print(\"common, rare: \", month_mask[\"all\"][month_mask[\"all\"].values>rare_threshold].sum(), month_mask[\"all\"][month_mask[\"all\"].values<rare_threshold].sum())\n# print(\"common classes: \", (month_mask[\"all\"].values>rare_threshold).sum())\n# threshold = np.where((month_mask[\"all\"].values>rare_threshold), np.ones_like(month_mask[\"all\"].values) * threshold, np.ones_like(month_mask[\"all\"].values) * 0.1)\n# threshold_long = np.where((month_mask[\"all\"].values>rare_threshold), np.ones_like(month_mask[\"all\"].values) * threshold_long, np.ones_like(month_mask[\"all\"].values) * 0.2)\n# month_mask[\"all\"].plot()","533d4ce0":"submission = prediction(test_audios=all_audios,\n                        models_cfg=CFG.models_cfg,\n                        threshold=threshold, \n                        threshold_long=threshold_long)\nsubmission.to_csv(\"submission.csv\", index=False)","b411e0cf":"# birds2id = {b : i for i, b in enumerate(CFG.target_columns)}\n# month_mask = pd.read_csv(\"..\/input\/site-month-mask\/month_mask.csv\")\n# site_mask = pd.read_csv(\"..\/input\/site-month-mask\/site_mask.csv\")\n# index = 0\n# remove_num = 0\n# for audio_path in all_audios:\n#     site = audio_path.name.split(\"_\")[1]\n#     month = audio_path.name.split(\"_\")[2][4:6]\n#     m = month_mask[month]\n#     for second in range(5, 605, 5):\n#         row_id = \"_\".join(audio_path.name.split(\"_\")[:2]) + f\"_{second}\"\n#         birds = submission.iloc[index, 1].split(\" \")\n#         birds_removed = []\n#         for b in birds:\n#             if b != \"nocall\":\n#                 if site == \"COR\" or site == \"COL\":\n#                     if site_mask.loc[birds2id[b], \"COR\"] == 0 and site_mask.loc[birds2id[b], \"COL\"] == 0:\n#                         remove_num += 1\n#                     else:\n#                         if month_mask.loc[birds2id[b], month] == 0:\n#                             remove_num += 1\n#                         else:\n#                             birds_removed.append(b)\n#                 else:\n#                     if site_mask.loc[birds2id[b], site] == 0:\n#                         remove_num += 1\n#                     else:\n#                         if month_mask.loc[birds2id[b], month] == 0:\n#                             remove_num += 1\n#                         else:\n#                             birds_removed.append(b)\n#         birds_removed = list(set(birds_removed))\n#         if len(birds_removed) == 0:\n#             birds_removed.append(\"nocall\")\n#         submission.iloc[index, 1] = \" \".join(birds_removed)\n#         index += 1\n# print(remove_num)\n# # print(submission)\n# submission.to_csv(\"submission.csv\", index=False)","63f71bef":"pd.read_csv(\"submission.csv\")","16a77a7b":"def get_metrics(s_true, s_pred):\n    s_true = set(s_true.split())\n    s_pred = set(s_pred.split())\n    n, n_true, n_pred = len(s_true.intersection(s_pred)), len(s_true), len(s_pred)\n    \n    prec = n\/n_pred\n    rec = n\/n_true\n    f1 = 2*prec*rec\/(prec + rec) if prec + rec else 0\n    \n    return {\"f1\": f1, \"prec\": prec, \"rec\": rec, \"n_true\": n_true, \"n_pred\": n_pred, \"n\": n}","8faa0ff2":"TARGET_PATH = None\nTEST_AUDIO_ROOT = Path(\"..\/input\/birdclef-2021\/test_soundscapes\")\nSAMPLE_SUB_PATH = \"..\/input\/birdclef-2021\/sample_submission.csv\"\nif not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n    TEST_AUDIO_ROOT = Path(\"..\/input\/birdclef-2021\/train_soundscapes\")\n    SAMPLE_SUB_PATH = None\n    # SAMPLE_SUB_PATH = \"..\/input\/birdclef-2021\/sample_submission.csv\"\n    TARGET_PATH = Path(\"..\/input\/birdclef-2021\/train_soundscape_labels.csv\")\n    \nif TARGET_PATH:\n    sub_target = pd.read_csv(TARGET_PATH)\n    sub_target = sub_target.merge(submission, how=\"left\", on=\"row_id\")\n    \n    print(sub_target[\"birds_x\"].notnull().sum(), sub_target[\"birds_x\"].notnull().sum())\n    assert sub_target[\"birds_x\"].notnull().all()\n    assert sub_target[\"birds_y\"].notnull().all()\n    \n    df_metrics = pd.DataFrame([get_metrics(s_true, s_pred) for s_true, s_pred in zip(sub_target.birds_x, sub_target.birds_y)])\n    \n    print(df_metrics.mean())","d3b7dde1":"## Get model","fc95e49f":"## Dependencies","524cc911":"## Libraries","c774f091":"## Config","890b1dcf":"## Dataset","f8d37593":"## Utilities","5e5f8083":"## Prediction","11df3217":"## Define Model","212626f2":"## Data Loading"}}