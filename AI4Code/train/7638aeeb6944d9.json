{"cell_type":{"2aca1771":"code","ca80b218":"code","badcea35":"code","f9594e38":"code","2ce938b3":"code","f71be582":"code","1282c6b3":"code","950f4f63":"code","0960e04e":"code","b2031026":"code","96d769f4":"code","a62111dd":"code","f4ca972e":"code","ce467c41":"code","c4224d4d":"code","cb00686a":"code","23ff4039":"code","32f9ca69":"code","080a712c":"code","6846f064":"code","91b8a58e":"markdown","6722139f":"markdown","844f9a61":"markdown","cb97e71b":"markdown","e7279605":"markdown","efa8f4e3":"markdown","d3edbfb3":"markdown","c2d56634":"markdown","ede40755":"markdown","135fdd37":"markdown","567f676e":"markdown","0b98e825":"markdown","1235e390":"markdown","a4629124":"markdown","2db1a894":"markdown","02d5f89d":"markdown","80dd300e":"markdown","799b01c3":"markdown","f88993be":"markdown"},"source":{"2aca1771":"%matplotlib inline\nimport theano\nimport pymc3 as pm\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nsns.set_style('white')\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles, make_blobs","ca80b218":"X, Y = make_circles(n_samples=2000, random_state=42, noise=0.2)#make_circles(n_samples=1000, random_state=42, noise=0.2)\n#make_moons(noise=0.2, random_state=0, n_samples=1000)#\n#centers=2, n_features=2,\nX = scale(X)\nX = X.astype(float)\nY = Y.astype(float)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.7)","badcea35":"fig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\nax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\nsns.despine(); ax.legend()\nax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');","f9594e38":"# just to see the dimensions o four data\nprint(X_train.shape)\nprint(Y_train.shape)","2ce938b3":"def construct_nn(ann_input, ann_output):\n    n_hidden = 4 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    #init_3 = np.random.randn(n_hidden,n_hidden).astype(float)\n   # init_4 = np.random.randn(n_hidden,n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        '''\n         # Weights from 1st to 2nd layer\n        weights_2_3 = pm.Normal('w_2_3', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n         # Weights from 1st to 2nd layer\n        weights_3_4 = pm.Normal('w_3_4', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        '''\n        # Weights from hidden layer to output\n        weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n       # act_3 = pm.math.tanh(pm.math.dot(act_2, weights_2_3))\n       # act_4 = pm.math.tanh(pm.math.dot(act_3, weights_3_4))\n        act_out = pm.math.sigmoid(pm.math.dot(act_2, \n                                              weights_2_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)","f71be582":"from pymc3.theanof import set_tt_rng, MRG_RandomStreams\nset_tt_rng(MRG_RandomStreams(42)) #random number generator and like setting seed","1282c6b3":"%%time\n\nwith neural_network:\n    inference = pm.ADVI()\n    approx = pm.fit(n=50000, method=inference)","950f4f63":"trace = approx.sample(draws=1000)","0960e04e":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","b2031026":"# Replace arrays our NN references with the test data\nann_input.set_value(X_test)\nann_output.set_value(Y_test)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)\n\n# Use probability of > 0.5 to assume prediction of class 1\npred = ppc['out'].mean(axis=0) > 0.5","96d769f4":"fig, ax = plt.subplots()\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\nsns.despine()\nax.set(title='Predicted labels in testing set', xlabel='X', ylabel='Y');","a62111dd":"print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))","f4ca972e":"grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\ngrid_2d = grid.reshape(2, -1).T\ndummy_out = np.ones(grid.shape[1], dtype=np.int8)","ce467c41":"ann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)","c4224d4d":"cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');","cb00686a":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');","23ff4039":"import numpy as np\nnp.max( ppc['out'].std(axis=0))","32f9ca69":"minibatch_x = pm.Minibatch(X_train, batch_size=32)\nminibatch_y = pm.Minibatch(Y_train, batch_size=32)\n\nneural_network_minibatch = construct_nn(minibatch_x, minibatch_y)\nwith neural_network_minibatch:\n    inference = pm.ADVI()\n    approx = pm.fit(40000, method=inference)","080a712c":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","6846f064":"trace_VI = approx.sample(draws=10000)\npm.traceplot(trace_VI);","91b8a58e":"## Mini-batch ADVI\n\nSo far, we have trained our model on all data at once. Obviously this won't scale to something like ImageNet. Moreover, training on mini-batches of data (stochastic gradient descent) avoids local minima and can lead to faster convergence.\n\nFortunately, ADVI can be run on mini-batches as well. It just requires some setting up:","6722139f":"# Part a: Probability surface\n## Include and discuss the probability results for your data.\n\n**Since the neural network we have is not perfect for learning circualr kind of data the accuracy is only 51%. The probability surface is also demonstrting the same and the scale is restricted to 40-60%. We can see light od light pink which is between 50\/52% which is our accuracy. I did try to improve the layers and neurons i nthe code above i nthe newural network function. It did nto make much difference at all. From research it says using tensorflow for this kind of data works better with accuracy but that is out of scope for this class. Therefore it is hard with this data unlike the moons to show the clear boundaries.**","844f9a61":"Performance wise that's pretty good considering that NUTS is having a really hard time. Further below we make this even faster. To make it really fly, we probably want to run the Neural Network on the GPU.\n\nAs samples are more convenient to work with, we can very quickly draw samples from the variational approximation using the `sample` method (this is just sampling from Normal distributions, so not at all the same like MCMC):","cb97e71b":"# Changing the dataset from make_moons used in class to make_circles for the assignemnt as asked in the question 2 HW4","e7279605":"# Part b - Uncertainty in predicted value\n## Include and discuss the uncertainty results derived from the standard deviation in the posterior predictive\n\n**All shown above with probabilities we can see the same with standard deviation. There is no clarity in the uncertainity bands at all. Generally we want to train the data in the uncertainity regions to improve accuracy. Therefore the model needs to be improved.**","efa8f4e3":"# Variational Inference: Bayesian Neural Networks\n\n**This was adapted from an excellent notebook here:**\n(c) 2016-2018 by Thomas Wiecki, updated by Maxim Kochurov\nOriginal blog post: https:\/\/twiecki.github.io\/blog\/2016\/06\/01\/bayesian-deep-learning\/\n\n**Also inspired from Dr.Baesener's DS6040 BML Class** <br>\n**Shilpa Narayan (smn7ba)** <br>\n**DS 6040 Question 2 - HW4** <br>\n**University of Virginia** <br>","d3edbfb3":"# Part c - ELBOW plot\n## include and discuss the plot of the ELBOW, and what that tells about convergence to maximizing the likelihood.\n\n**Simialr to the above ELBOW plot thsi plot is also clearly showing converging to maximizing the likelihood between 20000 and 50000 but it also shows extra converging between 0 and 20000. As the author of the blog describes training on mini batches of data makes a difference in faster and better convergence. Though thsi is not as great as general MCMC sampling, it is still close and faster which is better for larger data and neural networks with many layers.**","c2d56634":"## Bayesian Neural Networks in PyMC3","ede40755":"Let's look at our predictions:","135fdd37":"### Variational Inference: Scaling model complexity\n\nWe could now just run a MCMC sampler like NUTS which works pretty well in this case, but as I already mentioned, this will become very slow as we scale our model up to deeper architectures with more layers.\n\nInstead, we will use [ADVI](https:\/\/arxiv.org\/abs\/1603.00788) variational inference algorithm which was recently added to `PyMC3`, and updated to use the operator variational inference (OPVI) framework. This is much faster and will scale better. Note, that this is a mean-field approximation so we ignore correlations in the posterior.","567f676e":"### Generating data\n\nFirst, lets generate some toy data -- a simple binary classification problem that's not linearly separable.","0b98e825":"Plotting the objective function (ELBO) we can see that the optimization is convergin well.","1235e390":"Hey, our neural network did great!","a4629124":"As you can see, mini-batch ADVI's running time is much lower. It also seems to converge faster.\n\nFor fun, we can also look at the trace. The point is that we also get uncertainty of our Neural Network weights.","2db1a894":"## Lets look at what the classifier has learned\n\nFor this, we evaluate the class probability predictions on a grid over the whole input space.","02d5f89d":"### Model specification\n\nA neural network is quite simple. The basic unit is a [perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) which is nothing more than [logistic regression](http:\/\/pymc-devs.github.io\/pymc3\/notebooks\/posterior_predictive.html#Prediction). We use many of these in parallel and then stack them up to get hidden layers. Here we will use 2 hidden layers with 7 neurons each which is sufficient for such a simple problem.","80dd300e":"# Part d - Traceplot\n## Include and discuss the traceplot, focusing on what this tells you about your network.\n\n**w_in_1 is our layer into hidden layer, w_1_2 is first to second layer and w_2_out is second layer to the output. We can see we have 4 neurons and the uncertainity in all layers and neurons is almost constant. However all of them have zero in them. Since the mdoel is not great for thsi data there is nothing much we can infer but in general we get uncertainty estimates of our weights which could inform us about the stability of the learned representations of the network and in this case seems constant and there is not much being learned between layers.**","799b01c3":"Now that we trained our model, lets predict on the hold-out set using a posterior predictive check (PPC). ","f88993be":"That's not so bad. The `Normal` priors help regularize the weights. Usually we would add a constant `b` to the inputs but I omitted it here to keep the code cleaner."}}