{"cell_type":{"21fdfc80":"code","d9ccdfbf":"code","f8a4fd6c":"code","c0ae5c2d":"code","227677db":"code","8a004339":"code","ffb51e4f":"code","3a2d7560":"code","468b5293":"code","fd316f94":"code","23e225bd":"code","be9dd1a3":"code","3f8ec510":"code","97d14e67":"code","357a3bfc":"code","2622d937":"code","37d0db92":"code","18aec924":"code","4211368c":"code","d892dbd8":"code","b65b1265":"code","73eae325":"code","f0e1b5c2":"code","d79a0edb":"code","60488e44":"code","4651e411":"code","93f4abd6":"code","b1f30ce0":"code","5b494f17":"code","98ddce6e":"code","625a26be":"code","c564c17f":"code","51279a67":"code","353022cd":"code","af391224":"code","2d4823e8":"code","2acccdbb":"code","0ec23cba":"code","f5881158":"code","bc355ec8":"code","875a77ae":"code","384b0156":"code","cbbacd4c":"code","2e5f161c":"code","f8e844b2":"code","a3d3e837":"code","39754b26":"code","f6fff20d":"code","d91b57f6":"code","d20e43df":"code","5086079a":"code","7f60228b":"code","1febd460":"code","4085e35a":"code","0daf48bf":"code","d25067ff":"code","e995ead7":"code","26470f88":"code","4793dadd":"code","a96aaaec":"code","2ee0b22a":"code","82403459":"code","97ca6edb":"code","893820c8":"code","242132d2":"code","bfbf970d":"code","fddca012":"code","c096a160":"code","990be9b5":"code","cfa9c169":"code","b8b76bee":"code","c24ed929":"code","f4c64fac":"code","b068d7ef":"code","c0a28965":"code","1741d053":"code","979e87ca":"markdown","389b7897":"markdown","6e62c5ae":"markdown","c7471359":"markdown","276abaab":"markdown","ca3a2822":"markdown","244d1865":"markdown","9963e28e":"markdown","f6175fa3":"markdown","a34bcafa":"markdown","e34b1c67":"markdown","461049bb":"markdown","f4c3c992":"markdown","ff361c7f":"markdown","a2c6b07b":"markdown","d9baf080":"markdown","2262bef2":"markdown","5200e03c":"markdown","ac399fe0":"markdown","e8645ec7":"markdown","41cab853":"markdown","ac659b68":"markdown","9fe268ec":"markdown"},"source":{"21fdfc80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d9ccdfbf":"from sklearn.feature_extraction import DictVectorizer\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 16, 10 ","f8a4fd6c":"df = pd.read_csv('\/kaggle\/input\/laptop-price\/laptop_price.csv', encoding = \"ISO-8859-1\")\ndf.head()","c0ae5c2d":"df.info()","227677db":"df.columns = df.columns.str.lower().str.replace(' ', '_')\ndf.columns","8a004339":"# drop `laptop_id`\ndf = df.drop('laptop_id', axis=1)\ndf.head()","ffb51e4f":"categorical_columns = list(df.dtypes[df.dtypes == 'object'].index)\n\nfor c in categorical_columns:\n    df[c] = df[c].str.lower()\ndf.head()","3a2d7560":"# Check missing values.\ndf.isnull().sum()","468b5293":"# check on duplicates on all columns\ndf[df.duplicated(keep=False)].sort_values('product')","fd316f94":"# drop duplicates\nprint('shape before drop_duplicates =', df.shape)\ndf = df.drop_duplicates(keep='first')\nprint('shape after drop_duplicates =', df.shape)","23e225bd":"# we need check situation when we have the same characteristics, but price isn`t the same.\nprint('shape before drop_duplicates =', df.shape)\ncolumns_without_price = df.columns.drop('price_euros')\ndf = df.drop_duplicates(columns_without_price, keep='first')\nprint('shape after drop_duplicates =', df.shape)\ndf = df.reset_index(drop=True)\ndf.head()","be9dd1a3":"# Count unique number for each categorical features.\ndf[categorical_columns].nunique()","3f8ec510":"print(df.shape[0])\n(df[categorical_columns].nunique() \/ len(df) * 100).round(2)","97d14e67":"sns.distplot(df['price_euros'], hist_kws={'rwidth':0.85, 'edgecolor':'black', 'alpha':0.75});","357a3bfc":"sns.distplot(np.log(df['price_euros']), hist_kws={'rwidth':0.85, 'edgecolor':'black', 'alpha':0.75});","2622d937":"np.percentile(df['price_euros'].values, 95)","37d0db92":"df = df[df.price_euros < np.percentile(df['price_euros'].values, 95)]","18aec924":"df['log_price'] = np.log(df['price_euros'])","4211368c":"print(df['typename'].value_counts())\nsns.countplot(x='typename', data=df);","d892dbd8":"print(df['company'].value_counts())\nsns.countplot(x='company', data=df);","b65b1265":"th_hold = 6\nreraly_company = df['company'].value_counts()[(df['company'].value_counts() < th_hold)].keys()\nfor company in reraly_company:\n    df = df[df['company'] != company]","73eae325":"df['company'].value_counts()","f0e1b5c2":"print(df['ram'].value_counts())\nsns.countplot(x='ram', data=df);","d79a0edb":"df = df[df['ram'] != '64gb']\ndf = df[df['ram'] != '24gb']","60488e44":"print(df['opsys'].value_counts())\nsns.countplot(x='opsys', data=df);","4651e411":"df = df[df['opsys'] != 'android']\n\n\ndf['opsys'] = df['opsys'].str.replace('mac os x', 'macos')\ndf['opsys'] = df['opsys'].str.replace('windows 10 s', 'windows 10')","93f4abd6":"# convert `weight` to numerical values.\ndf.loc[:, 'weight'] = df.loc[:, 'weight'].apply(lambda x: float(x[:-2]))\ndf.head()","b1f30ce0":"def split_resolution(x):\n    width = int(x.split('x')[0])\n    height = int(x.split('x')[1])\n    return [width, height]","5b494f17":"# lets extract resolution from `screenresolution`.\ndf['resolution'] = df['screenresolution'].str.extract(r'(\\d+x\\d+)')\ndf['screenresolution'] = df['screenresolution'].replace(r'(\\d+x\\d+)', '', regex=True)\ndf['width'] = df['resolution'].apply(lambda x: split_resolution(x)[0])\ndf['height'] = df['resolution'].apply(lambda x: split_resolution(x)[1])","98ddce6e":"df['screenresolution'].value_counts()","625a26be":"df['touchscreen'] = (df['screenresolution'].str.extract(r'(touchscreen)') == 'touchscreen').astype(int)\ndf['ips'] = (df['screenresolution'].str.extract(r'(ips)') == 'ips').astype(int)\ndf['full_hd'] = (df['screenresolution'].str.extract(r'(full hd)') == 'full hd').astype(int)\ndf['4k_ultra_hd'] = (df['screenresolution'].str.extract(r'(4k ultra hd)') == '4k ultra hd').astype(int)\ndf['quad_hd'] = (df['screenresolution'].str.extract(r'(quad hd+)') == 'quad hd').astype(int)","c564c17f":"df[['touchscreen', 'ips', 'full_hd', '4k_ultra_hd', 'quad_hd']].sum()","51279a67":"df.head()","353022cd":"df['ghz'] = df['cpu'].str.extract(r'(\\d+(?:\\.\\d+)?ghz)') \ndf['ghz'] = df['ghz'].apply(lambda x: x[:-3]).astype(float)\ndf['cpu'] = df['cpu'].replace(r'(\\d+(?:\\.\\d+)?ghz)', '', regex=True)","af391224":"df.head()","2d4823e8":"def extract_brand(x):\n    return x.split(' ')[0]","2acccdbb":"df['cpu'].apply(lambda x: extract_brand(x)).value_counts()","0ec23cba":"df = df[~df['cpu'].str.startswith('samsung')]","f5881158":"df['cpu_intel'] = (df['cpu'].str.extract(r'(intel)') == 'intel').astype(int)\ndf['cpu_amd'] = (df['cpu'].str.extract(r'(amd)') == 'amd').astype(int)","bc355ec8":"df['gpu'].apply(lambda x: extract_brand(x)).value_counts()","875a77ae":"df['gpu_intel'] = (df['gpu'].str.extract(r'(intel)') == 'intel').astype(int)\ndf['gpu_nvidia'] = (df['gpu'].str.extract(r'(nvidia)') == 'nvidia').astype(int)\ndf['gpu_amd'] = (df['gpu'].str.extract(r'(amd)') == 'amd').astype(int)","384b0156":"df.head()","cbbacd4c":"df['memory'].value_counts()","2e5f161c":"df['memory_ssd'] = (df['memory'].str.extract(r'(ssd)') == 'ssd').astype(int)\ndf['memory_hdd'] = (df['memory'].str.extract(r'(hdd)') == 'hdd').astype(int)\ndf['memory_flash'] = (df['memory'].str.extract(r'(flash storage)') == 'flash storage').astype(int)","f8e844b2":"def split_memory(x, type_memory):\n    \"\"\"\n    x: str\n    type_memory: str\n    \n    split `str` by \"+\"\n    and extract storege size for each type_memory \n    \"\"\"\n    \n    res = x.split('+')\n    \n    ssd_value = 0\n    hdd_value = 0\n    flash_value = 0 \n    for _ in res:\n        \n        count_gb = int(re.findall(r'\\d+', _)[0])\n        if 'tb' in _:\n            count_gb = count_gb * 1024\n        \n        if 'ssd' in _:\n            ssd_value += count_gb\n        elif 'hdd' in _:\n            hdd_value += count_gb\n        else:\n            flash_value += count_gb\n            \n    if type_memory == 'ssd':\n        return ssd_value\n    elif type_memory == 'hdd':\n        return hdd_value\n    else:\n        return flash_value","a3d3e837":"df['ssd_value'] = df['memory'].apply(lambda x: split_memory(x, 'ssd'))\ndf['hdd_value'] = df['memory'].apply(lambda x: split_memory(x, 'hdd'))\ndf['flash_value'] = df['memory'].apply(lambda x: split_memory(x, 'flash'))","39754b26":"df.head(10)","f6fff20d":"df.columns","d91b57f6":"df = df.drop(['product', 'screenresolution', 'cpu', 'gpu', 'memory', 'resolution'], axis=1)\ndf.head()","d20e43df":"heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':16}, pad=12);","5086079a":"df.shape","7f60228b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\n\nimport sklearn\n\n\nimport xgboost as xgb","1febd460":"df_train, df_test = train_test_split(df, test_size=0.2, random_state=7)","4085e35a":"y_train = df_train[['price_euros', 'log_price']]\ny_test = df_test[['price_euros', 'log_price']]\n\ndf_train = df_train.drop(['price_euros', 'log_price'], axis=1)\ndf_test = df_test.drop(['price_euros', 'log_price'], axis=1)","0daf48bf":"dv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train.to_dict(orient='records')\nX_train = dv.fit_transform(train_dict)\n\ntest_dict = df_test.to_dict(orient='records')\nX_test = dv.transform(test_dict)","d25067ff":"# sorted(sklearn.metrics.SCORERS.keys())","e995ead7":"rf = RandomForestRegressor(random_state=7, n_jobs=-1)","26470f88":"folds = KFold(n_splits=5, shuffle=True, random_state=7)\n\nparameters = {'n_estimators':[50, 100, 150, 200, 250], 'max_depth':[3, 4, 5, 6, 7, 8, 9, 10],\n             'max_features': [\"auto\", \"sqrt\", \"log2\"]}\n\n\nmodel_rf = GridSearchCV(rf, \n                        param_grid=parameters, \n                        scoring='neg_mean_absolute_error', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True, \n                        n_jobs=-1)","4793dadd":"# model_rf.fit(X_train, y_train['log_price'].values.ravel())\n# model_rf.best_params_\n\n\n# {'max_depth': 10, 'max_features': 'auto', 'n_estimators': 200}","a96aaaec":"rf = RandomForestRegressor(n_estimators=200, max_depth=10, max_features='auto', random_state=7, n_jobs=-1)\n\nrf.fit(X_train, y_train['log_price'].values.ravel())\ny_pred_rf = rf.predict(X_test)","2ee0b22a":"mean_absolute_error(y_test['log_price'].values.ravel(), y_pred_rf).round(3)","82403459":"y_test['y_pred_rf'] = y_pred_rf\ny_test['y_pred_rf_price'] = np.exp(y_test['y_pred_rf'])\ny_test['abs_error_rf'] = abs(y_test['price_euros'] - y_test['y_pred_rf_price'])\ny_test.sort_values('abs_error_rf', ascending=False).head(20)","97ca6edb":"# mean and median errors in euro.\nround(np.mean(y_test['abs_error_rf']), 3), round(np.median(y_test['abs_error_rf']), 3)","893820c8":"y_test['abs_error_rf'].describe()","242132d2":"sorted(list(zip(rf.feature_importances_, dv.feature_names_)), key=lambda x: x[0], reverse=True)","bfbf970d":"from xgboost import XGBRegressor","fddca012":"model = XGBRegressor(n_estimators=100, max_depth=5, objective='reg:squarederror')","c096a160":"folds = KFold(n_splits=3, shuffle=True, random_state=7)\n\nparameters = {'n_estimators': [150, 200, 250], \n              'max_depth': [4, 5, 6], \n              'learning_rate': [0.1, 0.01],\n              'colsample_bytree': [0.5], \n              'subsample': [0.6], \n              'eta': [.3, .2, .1, .05, .01, .005]}\n\nmodel_xgb = GridSearchCV(model, \n                        param_grid=parameters, \n                        scoring='neg_mean_absolute_error', \n                        cv=folds, \n                        verbose=1,\n                        return_train_score=True, n_jobs=-1)","990be9b5":"# model_xgb.fit(X_train, y_train['log_price'].values.ravel())\n# model_xgb.best_params_\n\n\n# {'colsample_bytree': 0.5,\n#  'eta': 0.3,\n#  'learning_rate': 0.1,\n#  'max_depth': 5,\n#  'n_estimators': 250,\n#  'subsample': 0.6}","cfa9c169":"model = XGBRegressor(n_estimators=250, max_depth=5, objective='reg:squarederror',\n                    learning_rate=0.1, subsample=0.6, colsample_bytree=0.5, eta=0.3)","b8b76bee":"model.fit(X_train, y_train['log_price'].values.ravel())","c24ed929":"y_hat = model.predict(X_test)","f4c64fac":"mean_absolute_error(y_test['log_price'].values.ravel(), y_hat).round(3)","b068d7ef":"y_test['y_pred_xgb'] = y_hat\ny_test['y_pred_xgb_price'] = np.exp(y_test['y_pred_xgb'])\ny_test['abs_error_xgb'] = abs(y_test['price_euros'] - y_test['y_pred_xgb_price'])\ny_test.sort_values('abs_error_xgb', ascending=False).head(20)","c0a28965":"# mean and median errors in euro.\nround(np.mean(y_test['abs_error_xgb']), 3),  round(np.median(y_test['abs_error_xgb']), 3)","1741d053":"r2_score(y_test['log_price'].values.ravel(), y_hat).round(3)","979e87ca":"We have a few `company`, that meets very rarely.\n\nDelete `company` with lower counts in datasets.","389b7897":"### memory","6e62c5ae":"Split our data to train and test.\n\nI doesn't use validation dataset, because I will use another technick for validation. ","c7471359":"Drop unnessery columns.","276abaab":"## XGBRegressor","ca3a2822":"## EDA","244d1865":"Rows count in our dataset `df` are 1303. We can see that `product` has big percent unique values. We will delete their.","9963e28e":"### RAM","f6175fa3":"### CPU","a34bcafa":"Transform our target columns.","e34b1c67":"Lets extract resolution from `screenresolution`.","461049bb":"## RandomForestRegressor","f4c3c992":"## Feature generation","ff361c7f":"Delete `opsys` with rarely counts in datasets and replace `mac os x` to `macos`,  `windows 10 s` to `windows 10 s`, because they are very similar. ","a2c6b07b":"## preparation dataset to one style","d9baf080":"Delete `samsung` cpu brand with lower counts in datasets.","2262bef2":"Use `KFold` and `GridSearchCV` for tuning hyperparameters for RandomForestRegressor","5200e03c":"We have a few `ram`, that meets very rarely.\n\nDelete `ram` with lower counts in datasets.","ac399fe0":"### screenresolution","e8645ec7":"Delete the rows where `price_euros` is greater than 95 quintiles(this means that laptops more expensive than ~2,500 \u20ac will not be in the training dataset).","41cab853":"Extracy type of memory.","ac659b68":"### GPU","9fe268ec":"## BUILD MODELS"}}