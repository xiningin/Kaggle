{"cell_type":{"c98d401c":"code","302db6b1":"code","2b33ee2b":"code","463b53b5":"code","23a5811c":"code","aeea58c7":"code","53453f62":"code","31250b91":"code","574ea442":"code","42f0ab2f":"code","4ec1c094":"code","d105950c":"code","59e186f9":"code","18e368d6":"code","7f88baaf":"code","7d8feaa6":"code","7c7c3f8b":"code","dd2ef378":"code","1bce6d57":"code","241b0c10":"code","d8ab32fd":"code","318b2f10":"code","9585bc18":"code","905a5675":"code","a7caa946":"code","15935a4c":"code","855176b9":"code","fc9c8ae1":"code","895dcf41":"code","3a2756e2":"code","210b378c":"code","ffd43dfe":"code","ce8b363c":"code","ebf85568":"code","b477a92b":"code","31f6f3b4":"code","0fd9bb34":"code","23cd282a":"markdown","a09b9c68":"markdown","c0c63072":"markdown","2c2891f2":"markdown","def1ce03":"markdown","823df92f":"markdown","192115c6":"markdown","2d4c9174":"markdown","a14bd7ac":"markdown","4437c0ce":"markdown"},"source":{"c98d401c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","302db6b1":"import warnings\nwarnings.filterwarnings(action='ignore')","2b33ee2b":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","463b53b5":"train.shape","23a5811c":"# Check the missing values and types by data column.\ntrain.info()","aeea58c7":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split","53453f62":"# Seperate the X and y of the data.\ny = train.pop('Survived')\nX = train","31250b91":"# Drop columns\nX.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis=1, inplace=True)","574ea442":"# Binning column 'Age' in 10 units.\nbins = list(np.arange(0, 100, 10))\nX['Age'] = np.digitize(X['Age'], bins)","42f0ab2f":"# 'Fare' column applies StandardScaler.\nfare_scaler = StandardScaler()\nX['Fare'] = fare_scaler.fit_transform(X['Fare'].to_numpy().reshape(len(X), 1))","4ec1c094":"# Grants a unique number to change the Categorical column to a numeric value. This is the same as Label Encoding.\ndef fit_lb(column, lb_enc_dict, df):\n    if column not in lb_enc_dict.keys():\n        sub_enc_dict = dict()\n        for idx, e in enumerate(set(df[column])):\n            if e is not np.nan:\n                sub_enc_dict[e] = float(idx)\n        lb_enc_dict[column] = sub_enc_dict\n    return lb_enc_dict","d105950c":"# Function to convert to Label Encoding.\ndef transform_lb(data, column, lb_enc_dict):\n    transform_res = []\n    for e in data:\n        if e in lb_enc_dict[column].keys():\n            transform_res.append(lb_enc_dict[column][e])\n        else:\n            if e is not np.nan:\n                max_idx = max(lb_enc_dict[column].values()) + 1.0\n                lb_enc_dict[column][e] = max_idx\n                transform_res.append(max_idx)\n            else:\n                transform_res.append(np.nan)\n    return transform_res","59e186f9":"# Make(Fit) Encoding Dictionary\nlb_enc_dict = dict()\nlb_enc_dict = fit_lb('Sex', lb_enc_dict, X)\nlb_enc_dict = fit_lb('Embarked', lb_enc_dict, X)\nlb_enc_dict = fit_lb('Age', lb_enc_dict, X)","18e368d6":"# Convert to numeric value using Dictionary. \nX['Sex'] = transform_lb(list(X['Sex']), 'Sex', lb_enc_dict)\nX['Embarked'] = transform_lb(list(X['Embarked']), 'Embarked', lb_enc_dict)\nX['Age'] = transform_lb(list(X['Age']), 'Age', lb_enc_dict)","7f88baaf":"# Use the most frequent value to fill in the missing value.\nimputor = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nX = imputor.fit_transform(X)","7d8feaa6":"# Separate the Train and Test data.\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3)\nprint(f\"train_X Shape : {train_X.shape}\")\nprint(f\"train_y Shape : {train_y.shape}\")\nprint(f\"test_X Shape : {test_X.shape}\")\nprint(f\"test_y Shape : {test_y.shape}\")","7c7c3f8b":"from sklearn.metrics import accuracy_score\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier","dd2ef378":"rf_clf = RandomForestClassifier(criterion='entropy', n_estimators=50)\nrf_clf.fit(train_X, train_y)\nrf_pred_y = rf_clf.predict(test_X)\nrf_pred_proba = rf_clf.predict_proba(test_X)\nrf_accuracy = round(accuracy_score(test_y, rf_pred_y), 4)\nprint(f\"[RandomForestClassifier] Accuracy : {rf_accuracy}\")","1bce6d57":"lg_clf = LogisticRegression(C=1.0, penalty='l2', tol=1e-07)\nlg_clf.fit(train_X, train_y)\nlg_pred_y = lg_clf.predict(test_X)\nlg_pred_proba = lg_clf.predict_proba(test_X)\nlg_accuracy = accuracy_score(test_y, lg_pred_y)\nprint(f\"[LogisticRegression] Accuracy : {lg_accuracy}\")","241b0c10":"xgb_clf = XGBClassifier(colsample_bynode=0.8, learning_rate=1, max_depth=3, num_parallel_tree=100, objective='binary:logistic', subsample=0.8)\nxgb_clf.fit(train_X, train_y)\nxgb_pred_y = xgb_clf.predict(test_X)\nxgb_pred_proba = xgb_clf.predict_proba(test_X)\nxgb_accuracy = accuracy_score(test_y, xgb_pred_y)\nprint(f\"[XGBClassifier] Accuracy : {xgb_accuracy}\")","d8ab32fd":"svc_clf = SVC(C=4.0, kernel='rbf', tol=0.01, probability=True)\nsvc_clf.fit(train_X, train_y)\nsvc_pred_y = svc_clf.predict(test_X)\nsvc_pred_proba = svc_clf.predict_proba(test_X)\nsvc_accuracy = accuracy_score(test_y, svc_pred_y)\nprint(f\"[SVC] Accuracy : {svc_accuracy}\")","318b2f10":"ada_clf = AdaBoostClassifier(n_estimators=50)\nada_clf.fit(train_X, train_y)\nada_pred_y = ada_clf.predict(test_X)\nada_pred_proba = ada_clf.predict_proba(test_X)\nada_accuracy = accuracy_score(test_y, ada_pred_y)\nprint(f\"[AdaBoostClassifier] Accuracy : {ada_accuracy}\")","9585bc18":"total_proba = rf_pred_proba * 0.2 + lg_pred_proba * 0.2 + xgb_pred_proba * 0.2 + ada_pred_proba * 0.2 + svc_pred_proba * 0.2\ntotal_pred_y = np.argmax(total_proba, axis=1)\ntotal_accuracy = accuracy_score(test_y, total_pred_y)\nprint(f\"[Total] Accuracy : {total_accuracy}\")","905a5675":"test.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis=1, inplace=True)","a7caa946":"test['Age'] = np.digitize(test['Age'], bins)","15935a4c":"# Transform the test dataset using the MinMaxScaler, which is fitted with the train dataset.\ntest['Fare'] = fare_scaler.transform(test['Fare'].to_numpy().reshape(len(test), 1))","855176b9":"# Convert to numeric value using Dictionary. \ntest['Sex'] = transform_lb(list(test['Sex']), 'Sex', lb_enc_dict)\ntest['Embarked'] = transform_lb(list(test['Embarked']), 'Embarked', lb_enc_dict)\ntest['Age'] = transform_lb(list(test['Age']), 'Age', lb_enc_dict)","fc9c8ae1":"# Transform the test dataset using the SimpleImputer, which is fitted with the train dataset.\ntest = imputor.transform(test)","895dcf41":"rf_pred_proba = rf_clf.predict_proba(test)\nprint(f\"[RandomForestClassifier] Inference Result Shape : {rf_pred_proba.shape}\")","3a2756e2":"lg_pred_proba = lg_clf.predict_proba(test)\nprint(f\"[LogisticRegression] Inference Result Shape : {lg_pred_proba.shape}\")","210b378c":"xgb_pred_proba = xgb_clf.predict_proba(test)\nprint(f\"[XGBClassifier] Inference Shape : {xgb_pred_proba.shape}\")","ffd43dfe":"svc_pred_proba = svc_clf.predict_proba(test)\nprint(f\"[SVC] Inference Shape : {svc_pred_proba.shape}\")","ce8b363c":"ada_pred_proba = ada_clf.predict_proba(test)\nprint(f\"[SVC] AdaBoostClassifier Shape : {ada_pred_proba.shape}\")","ebf85568":"total_proba = rf_pred_proba * 0.2 + lg_pred_proba * 0.2 + xgb_pred_proba * 0.2 + ada_pred_proba * 0.2 + svc_pred_proba * 0.2\ntotal_pred_y = np.argmax(total_proba, axis=1)\nprint(f\"[Total] Inference Shape : {total_pred_y.shape}\")","b477a92b":"print(f\"Inference Result (Sample): {total_pred_y[:10]}\")","31f6f3b4":"submission['Survived'] = total_pred_y","0fd9bb34":"submission.to_csv('sampleSubmission.csv', index=False)","23cd282a":"# 1. Train and Test","a09b9c68":"### 1.1. Load and Check Data","c0c63072":"# Summary\n1. Data Preprocessing : \n   - Drop : Name, PassengerId, Cabin, Ticket\n   - Transform (Binning) : Age\n   - Transform (Scaling) : Fare (MinMax)\n   - Transform (Encoding) : Sex, Embarked, Age\n   - Missing Value : SimpleImputer (most frequent)\n2. Model \n   - RandomForestClassifier\n   - LogisticRegression\n   - XGBClassifier\n   - SVC\n   - AdaBoostClassifier\n3. Summary\n   - The probabilities extracted from the five models were weighted equally, summed, and then the label was extracted.","2c2891f2":"### 2.3. Summary (Ensemble)","def1ce03":"### 2.2. Inference","823df92f":"### 1.4. Summary (Ensemble)","192115c6":"### 2.1. Data Preprocessing","2d4c9174":"### 1.3. Modeling","a14bd7ac":"# 2. Submission Task","4437c0ce":"### 1.2. Data Preprocessing"}}