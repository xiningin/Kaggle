{"cell_type":{"2d45dcd6":"code","3e86d7a2":"code","65e1eb88":"code","0fc29203":"code","1fcb1078":"code","64bb22e5":"code","3ca3b7cb":"code","cff411be":"code","9fff310e":"code","4655292d":"code","3512ce5f":"code","ab08f9fe":"code","0913dd88":"code","23099782":"code","ccb344a9":"code","3ba3e676":"code","954c2681":"code","a6c1169f":"code","73884da8":"code","c94a0a8b":"code","00f8e3ae":"code","78a4a9b0":"code","b9f52f82":"code","98061275":"code","6cd8d1a0":"code","cf584406":"code","a9224842":"code","1923543b":"code","347cbd83":"code","c592e2c0":"code","2c3a14df":"code","2e353ffe":"markdown","bad8c34b":"markdown","e22ff871":"markdown","ea195e64":"markdown","cbb3dcf7":"markdown","4ba2b5a7":"markdown","4e38fe2e":"markdown","bfe25a9e":"markdown","847877ff":"markdown","13cb5a87":"markdown","54423d5b":"markdown","05ef1295":"markdown"},"source":{"2d45dcd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e86d7a2":"df = pd.read_csv('..\/input\/besedo\/train_trimmed_encod.csv', encoding='utf8')\npd.set_option('display.max_columns', None)\ndf.head()","65e1eb88":"# Filling out all the null values using median \n# More appropriate strategies might be required depending on the context\ndf.fillna(df.median(), inplace=True)","0fc29203":"(1e2*df['ad_customerSpecific_moderationDecision'].value_counts().sort_index()\/len(df)).plot(kind='barh')\nplt.title('Customer Decision Distribution')\nplt.xlabel('% Distribution');","1fcb1078":"sns.boxplot(x=\"ad_customerSpecific_moderationDecision\", y=\"ad_content_price_amount\", data=df)\nplt.ylim(-10**3, 10**4)\nplt.title('Price Distribution vs. Customer Decision');","64bb22e5":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df.columns:\n    if df[c].dtype=='float16' or  df[c].dtype=='float32' or  df[c].dtype=='float64':\n        df[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf = df.fillna(-999)\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n        \nprint('Labelling done.')","3ca3b7cb":"df = pd.get_dummies(df)","cff411be":"from sklearn.cluster import KMeans\nk = 5\nkmeans = KMeans(n_clusters=k, random_state=1)\ndf['k_5_label'] = kmeans.fit_predict(df)","9fff310e":"kmeans.inertia_","4655292d":"profile = df.groupby('k_5_label').mean().T","3512ce5f":"round(profile)","ab08f9fe":"round(pd.DataFrame(kmeans.cluster_centers_.T))","0913dd88":"from sklearn.cluster import MiniBatchKMeans\n\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5, random_state=1)\ndf['k_5_batch'] = minibatch_kmeans.fit_predict(df)","23099782":"pd.crosstab(df['k_5_label'], df['k_5_batch'])","ccb344a9":"from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score","3ba3e676":"def evaluate_metrics(df, min_clust=2, max_clust=10, rand_state=1):\n    inertias = []\n    silhouette = []\n    ch_score = []\n    db_score = []\n    for n_clust in range(min_clust, max_clust):\n        kmeans = KMeans(n_clusters=n_clust, random_state=rand_state)\n        y_label = kmeans.fit_predict(df)\n        inertias.append(kmeans.inertia_)\n        silhouette.append(silhouette_score(df, y_label))\n        ch_score.append(calinski_harabasz_score(df, y_label))\n        db_score.append(davies_bouldin_score(df, y_label))        \n\n    fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n    ax[0][0].plot(range(min_clust, max_clust), inertias, '-x', linewidth=2)\n    ax[0][0].set_xlabel('No. of clusters')\n    ax[0][0].set_ylabel('Inertia')\n    \n    ax[0][1].plot(range(min_clust, max_clust), silhouette, '-x', linewidth=2)\n    ax[0][1].set_xlabel('No. of clusters')\n    ax[0][1].set_ylabel('Silhouette Score')\n    \n    ax[1][0].plot(range(min_clust, max_clust), ch_score, '-x', linewidth=2)\n    ax[1][0].set_xlabel('No. of clusters')\n    ax[1][0].set_ylabel('Calinski Harabasz Score')\n    \n    ax[1][1].plot(range(min_clust, max_clust), db_score, '-x', linewidth=2)\n    ax[1][1].set_xlabel('No. of clusters')\n    ax[1][1].set_ylabel('Davies Bouldin Score')\n    fig.suptitle('Metrics to evaluate the number of clusters')\n    plt.show()","954c2681":"#evaluate_metrics(df.iloc[:, :-2], min_clust=2, max_clust=15, rand_state=0)","a6c1169f":"#df = df_original.copy()\n#df.fillna(df.median(), inplace=True)","73884da8":"from sklearn.preprocessing import StandardScaler\ndf_scaled = StandardScaler().fit_transform(df)","c94a0a8b":"#evaluate_metrics(df_scaled, min_clust=2, max_clust=15, rand_state=0)","00f8e3ae":"from yellowbrick.cluster.silhouette import SilhouetteVisualizer","78a4a9b0":"#plt.style.use('seaborn-paper')\n#fig, axs = plt.subplots(2, 3, figsize=(20, 15))\n#axs = axs.reshape(6)\n#for i, k in enumerate(range(7, 13)):\n   # ax = axs[i]\n    #sil = SilhouetteVisualizer(KMeans(n_clusters=k, random_state=1), ax=ax)\n    #sil.fit(df_scaled)\n    #sil.finalize()","b9f52f82":"#plt.style.use('fivethirtyeight')","98061275":"#df.T","6cd8d1a0":"#kmeans = MiniBatchKMeans(n_clusters=8, random_state=1)\n#df['k_8_label'] = kmeans.fit_predict(df)","cf584406":"#round(1e2 * df['k_8_label'].value_counts().sort_index()\/len(df), 2)","a9224842":"#round(df.groupby('k_8_label').mean().T, 2)","1923543b":"#fig, ax = plt.subplots(figsize=(6, 4))\n#df.mean()","347cbd83":"#round(1e2 * df['k_8_label'].value_counts().sort_index()\/len(df))","c592e2c0":"#(df[['ad_content_price_amount', 'ad_content_customerSpecific_autoCarOrder', 'ad_location_city', 'ad_content_customerSpecific_autoCarMm', 'ad_customerSpecific_moderationDecision', 'k_8_label']]\n# .groupby('k_8_label').mean().plot.bar(figsize=(15, 5)))\n#plt.title('Content Behavior of various segments')\n#plt.xlabel('SEGMENTS');","2c3a14df":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Be MORE than patient. Mar\u00edlia Prata, @mpwolke was Here failing again.')","2e353ffe":"# **<span style=\"color:#346888;\">Segmentation and Clustering<\/span>**\n\n\nSegmenting is the process of putting customers into groups based on similarities, and clustering is the process of finding similarities in customers so that they can be grouped, and therefore segmented. They seem quite similar, but they are not quite the same.\nhttps:\/\/www.acquia.com\/blog\/difference-between-segmentation-and-clustering\n\nClustering techniques are used to group data\/observations in a few segments so that data within any segment are similar while data across segments are different. Defining what we mean when we say \u201csimilar\u201d or \u201cdifferent\u201d observations is a key part of cluster analysis which often requires a lot of contextual knowledge and creativity beyond what statistical tools can provide.\nhttps:\/\/inseaddataanalytics.github.io\/INSEADAnalytics\/CourseSessions\/Sessions45\/ClusterAnalysisReading.html","bad8c34b":"Small segments:\n\nCluster 2: This group of customers is in a dire need of a credit limit increase. They also have the highest activities among all the clusters.\n\nCluster 3: This group of customers on the other hand are not completely utilizing the credit line assigned to them. Additional investigations are needed to understand why this particular set of consumers are not utilizing their lines or if their credit lines could in the future be assigned to a different set of consumers.","e22ff871":"#Evaluations of clustering metrics\n\nTo figure out the number of clusters that can be found out in our datasets, we can evaluate a set of indices or scores.\n\nSilhoutte score, Calinski Harabasz score and Davies Bouldin score","ea195e64":"#Codes by Amol Deshmukh https:\/\/www.kaggle.com\/des137\/customer-segmentation-credit-cards\/notebook","cbb3dcf7":"Inertia is one measure of understanding the behaviors of clusters.","4ba2b5a7":"#With the general intuition obtained from various methods above, we conclude that 8 seems to be an appropriate number for clustering? I don't know. The programm stopped after evaluating metrics","4e38fe2e":"The snippet above stopped the Notebook that exceeded the time. Then I commented all the rest.","bfe25a9e":"Look at the distribution of the population within the cluster.","847877ff":"#Scaling of features","13cb5a87":"Observations:\n\nLarge segments\n\nCluster 6: This cluster shows low balances but average activity. This cluster will be an approprite cluster for spend campaign targetting.\n\nCluster 0: This cluster shows slightly higher balances and purchase activities, but higher one-off purchase behavior.\n\nCluster 4: This cluster has the highest activity, balances, and purchases. This group of customers interestingly also have a higher set of credit lines, indicating that an increasing credit limit increases leads to an increase in the purchase activitis. (A rigourous testing of this hypothesis should be carries out.)","54423d5b":"#Minibatch Clustering","05ef1295":"![](https:\/\/data-flair.training\/blogs\/wp-content\/uploads\/sites\/2\/2019\/07\/R-project-customer-segmentation.png)data-flair.training"}}