{"cell_type":{"55c8a061":"code","edba4a0f":"code","b373fdb3":"code","9da9912c":"code","afb669c1":"code","bc6031f5":"code","123e6d83":"code","44b5d2c0":"code","bd76434b":"code","928ef0be":"code","ae8d1fc1":"code","0d24eaa3":"code","976ab591":"code","cc04ee99":"code","cc186bea":"code","22ffda14":"code","412719e7":"code","722861a3":"code","9882e7fe":"markdown","1ef40239":"markdown","0b41ae48":"markdown","42ec09b3":"markdown","8c45b0f7":"markdown","ed0c38ea":"markdown","7733d5b5":"markdown","bc644815":"markdown","8148c616":"markdown","153de5a8":"markdown","67ad5e5e":"markdown"},"source":{"55c8a061":"%%capture\n!pip3 install box2d-py\n!pip3 install gym[Box_2D]\n#remove \" > \/dev\/null 2>&1\" to see what is going on under the hood\n!pip install gym pyvirtualdisplay > \/dev\/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg > \/dev\/null 2>&1\n!pip install PyVirtualDisplay\n!pip install gym[all]\nimport torch\nimport os\nimport io\nimport random\nimport base64\nimport glob\nimport time\nfrom gym.wrappers import Monitor, FrameStack, GrayScaleObservation,NormalizeReward\nimport torch.nn as nn\nimport gym\nfrom gym.spaces.box import Box\nfrom IPython.display import HTML\nfrom IPython import display as ipythondisplay\nfrom PIL import Image\nimport numpy as np\nimport cv2\nfrom pyvirtualdisplay import Display\nimport matplotlib.pyplot as plt\nfrom torchvision.transforms import Resize as Resize\nimport torch.nn.functional as F","edba4a0f":"!pip install gym-minigrid\nimport gym_minigrid","b373fdb3":"display = Display(visible=0, size=(100, 100))\ndisplay.start()","9da9912c":"def make_env(env_name, seed=1, rank=1, grayscale=False, num_stack=-1, *args, **kwargs):\n    if 'CarRacing' in env_name:\n        env = gym.make(env_name, verbose=0, *args, **kwargs)\n    elif 'MiniGrid' in env_name:\n        env = gym.make(env_name, *args, **kwargs)\n        from gym_minigrid.wrappers import FlatObsWrapper, ReseedWrapper\n        env = FlatObsWrapper(env)\n        env = ReseedWrapper(env)\n    else:\n        env = gym.make(env_name, *args, **kwargs)\n    env.seed(seed + rank)\n    # If the inputs are images, i.e have shape (W,H,3), transpose it for pytorch cnn\n    obs_shape = env.observation_space.shape\n    if len(obs_shape) == 3 and obs_shape[2] in [1, 3]:\n        if grayscale:\n            env = GrayScaleObservation(env, keep_dim=False)\n        else:\n            env = TransposeImageWrapper(env, op=[2, 0, 1])\n    if num_stack > 0:\n        env = FrameStack(env, num_stack, lz4_compress=False)\n    # normalize reward\n    env = NormalizeReward(env)\n    return env\n    \ndef make_envs(env_name, num_env, seed, *args, **kwargs):\n    envs = [make_env(env_name, seed, i, *args, **kwargs) for i in range(num_env)]\n    return envs\n    \nclass Envs(gym.Wrapper):\n    '''Vectorized version of the environment '''\n    def __init__(self, env_name, num_process, seed, max_len=1000, *args, **kwargs):\n        print('Environment name:', env_name)\n        print('Total number of simulations: {}'.format(num_process))\n        self.envs = make_envs(env_name, num_process, seed, *args, **kwargs)\n        super().__init__(self.envs[0])\n        self.num_process = num_process\n        self.max_len = max_len\n        self.current_reward = np.zeros(num_process, dtype=np.float32)\n        self.current_length = np.zeros(num_process, dtype=np.int)\n        self.epsilon = 1e-8\n        \n    def reset(self):\n        self.current_reward = np.zeros(self.num_process, dtype=np.float32)\n        self.current_length = np.zeros(self.num_process, dtype=np.int)\n        return np.asarray([env.reset() for env in self.envs])\n    \n    def reset_at(self, env_index):\n        assert env_index < self.num_process\n        self.current_reward[env_index] = 0\n        self.current_length[env_index] = 0\n        return self.envs[env_index].reset()\n    \n    def step(self, actions):\n        assert len(actions) == len(self.envs), 'action dimension and num_env dont match'\n        next_states, rewards, dones, infos = [], [], [], []\n        for env, action in zip(self.envs, actions):\n            next_state, reward, done, info = env.step(action)\n            next_states.append(next_state)\n            rewards.append(reward*np.sqrt(env.return_rms.var + self.epsilon))\n            dones.append(done)\n            infos.append(info)\n            \n        self.current_reward = self.current_reward + np.asarray(rewards)\n        self.current_length += 1\n        \n        self.update_terminal(dones, infos)\n        return np.asarray(next_states), np.asarray(rewards), \\\n            np.asarray(dones), infos\n    \n    def close(self):\n        for env in self.envs:\n            env.close()\n            \n    def update_terminal(self, dones, infos):\n        for i in range(len(dones)):\n            if dones[i] or self.current_length[i] > self.max_len:\n                infos[i].update(episode_reward=self.current_reward[i],\n                                episode_length=self.current_length[i])\n                dones[i] = True\n        \n        \nclass TransposeImageWrapper(gym.ObservationWrapper):\n    '''Transpose img dimension before being fed to neural net'''\n    def __init__(self, env, op):\n        super().__init__(env)\n        assert len(op) == 3, \"Error: Operation, \" + str(op) + \", must be dim3\"\n        self.op = op\n        obs_shape = self.observation_space.shape\n        self.observation_space = Box(\n            self.observation_space.low[0, 0, 0],\n            self.observation_space.high[0, 0, 0], [\n                obs_shape[self.op[0]], obs_shape[self.op[1]],\n                obs_shape[self.op[2]]\n            ],\n            dtype=self.observation_space.dtype)\n\n    def observation(self, ob):\n        return ob.transpose(self.op[0], self.op[1], self.op[2])\n","afb669c1":"# Custom four-room classes\n\nclass CustomFourRoom(gym_minigrid.envs.fourrooms.FourRoomsEnv):\n    \"\"\"\n    Classic 4 rooms gridworld environment.\n    Can specify agent and goal position, if not it set by default.\n    \"\"\"\n\n    def __init__(self, agent_pos=(4, 16), goal_pos=(15, 15), max_steps=1000):\n        self._agent_default_pos = agent_pos\n        self._goal_default_pos = goal_pos\n        super().__init__(agent_pos=agent_pos, goal_pos=goal_pos)\n\n        self.max_steps = max_steps\n\nclass SRoom(CustomFourRoom):\n    \"\"\"\n    Goal and start point lie in the same room.\n    \"\"\"\n    def __init__(self, goal_pos=(5, 15)):\n        super().__init__(goal_pos=goal_pos)\n\nclass LRoom(CustomFourRoom):\n    \"\"\"\n    Goal position at the room in the left.\n    \"\"\"\n    def __init__(self, goal_pos=(5, 5)):\n        super().__init__(goal_pos=goal_pos)\n    \nclass RRoom(CustomFourRoom):\n    \"\"\"\n    Goal position at the room in the right.\n    \"\"\"\n    def __init__(self, goal_pos=(15, 15)):\n        super().__init__(goal_pos=goal_pos)\n        \nclass ORoom(CustomFourRoom):\n    \"\"\"\n    Goal position at the opposite room.\n    \"\"\"\n    def __init__(self, goal_pos=(15, 5)):\n        super().__init__(goal_pos=goal_pos)\n    \n        \nfrom gym.envs.registration import register\nregister(\n    id='MiniGrid-MyEnv-v0',\n    entry_point=__name__ +':CustomFourRoom',\n)\nregister(\n    id='MiniGrid-MyEnv-Same-v0',\n    entry_point=__name__ +':SRoom',\n)\nregister(\n    id='MiniGrid-MyEnv-Left-v0',\n    entry_point=__name__ +':LRoom',\n)\nregister(\n    id='MiniGrid-MyEnv-Right-v0',\n    entry_point=__name__ +':RRoom',\n)\nregister(\n    id='MiniGrid-MyEnv-Opposite-v0',\n    entry_point=__name__ +':ORoom',\n)","bc6031f5":"class ReplayBuffer(object):\n    def __init__(self, num_steps, num_processes, obs_shape, action_space):\n        \n        self.obs = torch.zeros(num_steps + 1, num_processes, *obs_shape) # one additional observation at the beginning: (s, a, s')\n        self.rewards = torch.zeros(num_steps, num_processes, 1)\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n        \n        if action_space.__class__.__name__ == 'Discrete':\n            action_shape = 1\n        else:\n            action_shape = action_space.shape[0]\n            \n        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n        if action_space.__class__.__name__ == 'Discrete':\n            self.actions = self.actions.long()\n            \n        # if the episode is done or not\n        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n        \n        self.num_steps = num_steps\n        self.num_processes = num_processes\n        self.step = 0\n        self.obs_shape = obs_shape\n    \n    def insert(self, ob, action, action_log_probs, reward, value_pred, done):\n        '''Insert new tuple (s, a, s', r) to the replay buffer'''\n        self.obs[self.step+1] = torch.tensor(ob).view(self.obs.shape[1:])\n        self.actions[self.step] = action.view(self.actions.shape[1:])\n        self.rewards[self.step] = torch.tensor(reward).view(self.rewards.shape[1:])\n        self.value_preds[self.step] = value_pred.view(self.value_preds.shape[1:])\n        self.action_log_probs[self.step] = action_log_probs.view(self.action_log_probs.shape[1:])\n        self.masks[self.step + 1] = done.view(self.masks.shape[1:])\n        \n        self.step = (self.step + 1) % self.num_steps\n        \n    def reset(self):\n        self.obs[0].copy_(self.obs[-1])\n        self.masks[0].copy_(self.masks[-1])\n        self.step=0\n        \n    def compute_return(self, next_vals, gamma):\n        self.returns[-1] = next_vals\n        for step in reversed(range(self.rewards.size(0))):\n            self.returns[step] = self.returns[step + 1] * \\\n                gamma * self.masks[step + 1] + self.rewards[step]\n        \n    def sample_minibatch(self, minibatch_size, advantage):\n        '''Sample minibatches for PPO updates'''\n        \n        batch_size = self.num_steps * self.num_processes # equivalent to M*N in the paper\n        assert minibatch_size <= batch_size\n        \n        # random indices\n        minibatch_indices = torch.utils.data.BatchSampler(\n                torch.utils.data.SubsetRandomSampler(range(batch_size)), \n                minibatch_size, drop_last=False)\n        \n        for minibatch_indice in minibatch_indices:\n            obs = self.obs[:-1].view(-1, *self.obs_shape)[minibatch_indice]\n            act = self.actions.view(-1, self.actions.shape[-1])[minibatch_indice]\n            ret = self.returns[:-1].view(-1, 1)[minibatch_indice]\n            mask = self.masks[:-1].view(-1, 1)[minibatch_indice]\n            log_prob = self.action_log_probs.view(-1,1)[minibatch_indice]\n            value_preds = self.value_preds[:-1].view(-1, 1)[minibatch_indice]\n            adv = advantage.view(-1, 1)[minibatch_indice]\n            yield obs, act, log_prob, value_preds, ret, adv, mask\n            \n    def to(self, device):\n        self.obs = self.obs.to(device)\n        self.rewards = self.rewards.to(device)\n        self.value_preds = self.value_preds.to(device)\n        self.returns = self.returns.to(device)\n        self.action_log_probs = self.action_log_probs.to(device)\n        self.actions = self.actions.to(device)\n        self.masks = self.masks.to(device)","123e6d83":"class Base(nn.Module):\n    def __init__(self, \n              output_size: int,\n              **kwargs):\n        super().__init__(**kwargs)\n        self.net = None\n    \n    def forward(self, obs):\n        return self.net(obs)\n\n\nclass CNNModel(Base):\n    def __init__(self, \n              input_size: tuple,\n              output_size: int, \n              **kwargs):\n        super().__init__(output_size, **kwargs)\n        self.net = nn.Sequential(Resize((64, 64)),   # input image is resized to 64x64\n                                nn.Conv2d(input_size[0], 32, 2, stride=2), nn.ReLU(),\n                                nn.Conv2d(32, 64, 2, stride=2), nn.ReLU(),\n                                nn.Conv2d(64, 32, 2, stride=2), nn.ReLU(), nn.Flatten(),\n                                nn.Linear(32*8*8, output_size))\n\n    def forward(self, obs):\n        obs = (obs-128)\/256.\n        if len(obs.shape) == 3: \n            obs = obs.unsqueeze(0)\n        return super().forward(obs)\n\nclass MLPModel(Base):\n    def __init__(self, \n              input_size: int,\n              output_size: int, \n              **kwargs):\n        super().__init__(output_size, **kwargs)\n        self.net = nn.Sequential(nn.Linear(input_size, 64), nn.Tanh(),\n                                 nn.Linear(64, 64), nn.Tanh(),\n                                 nn.Linear(64, output_size))\n","44b5d2c0":"class NormalDistribution(torch.distributions.Normal):\n    def log_prob(self, act):\n        log_prob = super().log_prob(act).sum(-1, keepdim=True)\n        return log_prob\n    \n    def entropy(self):\n        return super().entropy().sum(-1)\n\nclass Normal(nn.Module):\n    def __init__(self, output_size):\n        super().__init__()\n        self.logstd = nn.Parameter(torch.zeros(output_size, dtype=torch.float32))\n    \n    def forward(self, x):\n        return NormalDistribution(x, self.logstd.exp())\n\nclass Categorical(nn.Module):\n    def forward(self, x):\n        return torch.distributions.Categorical(logits=x)","bd76434b":"class MLPPolicy(nn.Module):\n    def __init__(self, obs_shape, output_size, **kwargs):\n        super().__init__()\n        self.actor = MLPModel(obs_shape, output_size)\n        self.critic = MLPModel(obs_shape, 1)\n        self.train()\n        \n    def forward(self, x):\n        return self.actor(x), self.critic(x)\n    \nclass CNNPolicy(nn.Module):\n    def __init__(self, obs_shape, output_size, **kwargs):\n        super().__init__()\n        self.hidden = 128\n        self.main = CNNModel(obs_shape, self.hidden) # actor and critic share the same feature extractor\n        self.actor = nn.Linear(self.hidden, output_size)\n        self.critic = nn.Linear(self.hidden, 1)\n        self.train();\n        \n    def forward(self, x):\n        x = self.main(x)\n        return self.actor(x), self.critic(x)\n    ","928ef0be":"class Policy(nn.Module):\n    def __init__(self, \n                 observation_space, \n                 action_space, \n                 **kwargs):\n        super().__init__()\n        \n        discrete = isinstance(action_space, gym.spaces.Discrete)\n        output_size = action_space.n if discrete else action_space.shape[0]\n        obs_shape = observation_space.shape \n        print('Observation shape', obs_shape)\n        print('Action shape', output_size)\n        # check if inputs are images or numbers\n        if len(obs_shape) == 3:\n            self.base = CNNPolicy(obs_shape, output_size)\n        else:\n            self.base = MLPPolicy(obs_shape[0], output_size)\n        \n        print('Action space: {}'.format('Discrete' if discrete else 'Continuous'))\n        if discrete:\n            self.dist = Categorical()\n        else:\n            self.dist = Normal(output_size)\n            \n    def forward(self, x):\n        pass\n    \n    def act(self, x):\n        act_logits, values = self.base(x)\n        dist = self.dist(act_logits)\n        act = dist.sample()\n        log_prob = dist.log_prob(act)\n        return act, values, log_prob\n    \n    def log_prob(self, ob, act):\n        act_logits, val = self.base(ob)\n        dist = self.dist(act_logits)\n        log_prob = dist.log_prob(act.squeeze()).view(-1, 1)\n        return log_prob, val, dist.entropy()","ae8d1fc1":"class PPO():\n    def __init__(self,\n                 actor_critic,\n                 clip_param,\n                 ppo_epoch,\n                 minibatch_size,\n                 value_loss_coef,\n                 entropy_coef,\n                 lr,\n                 eps=None,\n                 max_grad_norm=.5,\n                 **kwargs):\n        self.actor_critic = actor_critic\n        \n        self.clip_param = clip_param\n        self.ppo_epoch = ppo_epoch\n        self.minibatch_size = minibatch_size\n        \n        self.value_loss_coef = value_loss_coef\n        self.entropy_coef = entropy_coef\n        \n        self.max_grad_norm = max_grad_norm\n        self.optimizer = torch.optim.Adam(actor_critic.parameters(), lr=lr, eps=eps)\n        \n    def update(self,\n               rollouts: ReplayBuffer, \n               ppo_epochs,\n               clip_param,\n               vf_coef,\n               entropy_coef):\n        old_advantage = rollouts.returns[:-1]-rollouts.value_preds[:-1]\n        old_advantage = (old_advantage - old_advantage.mean()) \/ (\n                old_advantage.std() + 1e-5)\n        \n        value_loss_epoch = 0\n        action_loss_epoch = 0\n        dist_entropy_epoch = 0\n        loss_epoch = 0\n\n        for k in range(ppo_epochs):\n            for sample in rollouts.sample_minibatch(self.minibatch_size, old_advantage):\n                obs, act, old_log_prob, value_preds, ret, adv, mask = sample\n                log_prob, values, entropy = self.actor_critic.log_prob(obs, act)\n                assert log_prob.shape == old_log_prob.shape, '{} {}'.format(log_prob.shape, old_log_prob.shape)\n                assert old_log_prob.shape == adv.shape, '{} {}'.format(adv.shape, old_log_prob.shape)\n                \n                ratio = torch.exp(log_prob-old_log_prob)\n                surr1 = ratio * adv\n                surr2 = torch.clamp(ratio, 1-clip_param, 1+clip_param)*adv\n                \n                loss_clip  = -torch.min(surr1, surr2).mean()\n                loss_vf    = .5*(ret - values).pow(2).mean()\n                loss_entrp = -entropy.mean()\n                loss = loss_clip + vf_coef * loss_vf + entropy_coef * loss_entrp\n                \n                self.optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.actor_critic.parameters(),\n                                         self.max_grad_norm)\n                self.optimizer.step()\n                \n                value_loss_epoch += loss_vf.item()\n                action_loss_epoch += loss_clip.item()\n                dist_entropy_epoch += loss_entrp.item()\n                loss_epoch += loss.item()\n                \n        num_updates = ppo_epochs * self.minibatch_size\n\n        value_loss_epoch \/= num_updates\n        action_loss_epoch \/= num_updates\n        dist_entropy_epoch \/= num_updates\n        loss_epoch \/= num_updates\n        \n        return loss_epoch, action_loss_epoch, value_loss_epoch, dist_entropy_epoch","0d24eaa3":"class Logger():\n    def __init__(self):\n        self.loss = []\n        self.vf_loss = []\n        self.clip_loss = []\n        self.entrp_loss = []\n        self.reward_episode = []\n        self.len_episode = []\n        \n    def report(self, i, n_update): \n        print('='*20,'\\nUpdates: {}\/{} \\nTraining reward: {}\\nEpisode length: {}'.format(i, n_update, \n                        np.mean(self.reward_episode[-10:]) if len(self.reward_episode) > 10 else 'na', \n                        np.mean(self.len_episode[-10:]) if len(self.len_episode) > 10 else 'na'))\n        print('Actor loss: {}\\nCritic loss: {}\\n'.format(np.mean(self.clip_loss[-10:]), np.mean(self.vf_loss[-10:])))\n        print('Max\/min reward: {}\/{}\\n'.format(np.max(self.reward_episode[-min(10, len(self.reward_episode)):]) if len(self.reward_episode) > 0 else 'na', \n                                               np.min(self.reward_episode[-min(len(self.reward_episode), 10):]) if len(self.reward_episode) > 0 else 'na'))\n        \n    def add_loss(self, loss, clip_loss, vf_loss, entrp_loss):\n        self.loss.append(loss)\n        self.vf_loss.append(vf_loss)\n        self.clip_loss.append(clip_loss)\n        self.entrp_loss.append(entrp_loss)\n        \n    def add_reward(self, r):\n        self.reward_episode.append(r)\n        \n    def add_length(self, l):\n        self.len_episode.append(l)\n        \n    def plot(self):\n        plt.figure(figsize=(30,6))\n        plt.subplot(1,3,1) \n        plt.plot(self.vf_loss, label='critic loss')\n        plt.plot(self.entrp_loss, label='entropy loss')\n        plt.plot(self.clip_loss, label='actor loss')\n        plt.legend()\n        plt.title('Training loss')\n        plt.subplot(1, 3, 2)\n        plt.plot(self.reward_episode)\n        plt.title('Reward')\n        plt.subplot(1, 3, 3)\n        plt.plot(self.len_episode)\n        plt.title('Episode length')\n        plt.show()\n        \n    def play_and_display(self, env, agent):\n        env = Monitor(env, '.\/video', force=True)\n        ob = env.reset()\n        img_obs = []\n        step, max_step = 0, int(2e3)\n        while True:\n            if hasattr(env, 'sim'):\n                img_obs.append(env.sim.render(camera_name='track', height=500, width=500)[::-1])\n            else:\n                img_obs.append(env.render(mode='rgb_array'))\n                \n            with torch.no_grad():\n                ac, _, _ = agent.act(torch.tensor(np.array(ob, dtype=np.float32)).unsqueeze(0))\n            ob, rew, done, _ = env.step(ac.cpu().squeeze(0).numpy())\n            step += 1\n            if done or step >= max_step:\n                break\n        env.close()\n        def show_video():\n            mp4list = glob.glob('video\/*.mp4')\n            if len(mp4list) > 0:\n                mp4 = mp4list[0]\n                video = io.open(mp4, 'r+b').read()\n                encoded = base64.b64encode(video)\n                ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                      loop controls style=\"height: 300px;\">\n                      <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n                      <\/video>'''.format(encoded.decode('ascii'))))\n            else: \n                print(\"Could not find video\")\n        show_video()\n        %rm -r video","976ab591":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n#     torch.backends.cudnn.deterministic = True # uncomment this line will slow down the code","cc04ee99":"def main(env_name: 'Environment name', \n         num_processes: 'Number of simulators'=16, \n         seed=1, \n         device='cpu', \n         num_steps: 'Number of forward steps '=5, \n         num_updates: 'Number of training steps'=1e5, \n         log_freq=200, \n         clip_param: 'PPO clip parameter'=0.2, \n         ppo_epochs=4,\n         minibatch_size=32,\n         vf_coef: 'c1 hyperparam in PPO'=.5,\n         entropy_coef: 'c2 hyperparam in PPO'=.01,\n         lr=1e-3,\n         eps: 'eps in optimizer'=1e-5,\n         gamma: 'Discounted factor'=.99,\n         grayscale: 'Convert input images to grayscale'=False,\n         num_stack: 'Number of stacking frames'=-1, # if input is image, it need to be converted to grayscale first\n         verbose: 'Print result'=True,\n         max_len: 'Max length of each training episode'=1000,\n         log_vid_freq=-1,\n         num_game_demo=2):\n    \n    seed_everything(seed)\n    envs = Envs(env_name, num_processes, seed, max_len, grayscale, num_stack)\n    actor_critic = Policy(envs.observation_space, envs.action_space)\n    agent = PPO(actor_critic,clip_param,ppo_epochs,minibatch_size,vf_coef,entropy_coef,lr,eps)\n    rollouts = ReplayBuffer(num_steps, num_processes, envs.observation_space.shape, envs.action_space)\n    logger = Logger()\n    \n    obs = envs.reset()\n    rollouts.obs[0] = torch.tensor(obs).view(rollouts.obs.shape[1:])\n    rollouts.to(device)\n    \n    start = time.time()\n    \n    for i in range(num_updates):\n        for step in range(num_steps):\n            with torch.no_grad():\n                act, vals, log_prob = actor_critic.act(rollouts.obs[step])\n            \n            obs, rewards, dones, infos = envs.step(act.cpu().numpy())\n            for j in range(len(dones)):\n                if dones[j]:\n                    logger.add_reward(infos[j]['episode_reward'])\n                    logger.add_length(infos[j]['episode_length'])\n                    obs[j] = envs.reset_at(j)\n\n            dones = 1-torch.tensor(dones, dtype=torch.float32, device=device)\n            rollouts.insert(obs, act, log_prob, rewards, vals, dones)\n        \n        with torch.no_grad():\n            _, next_val, _ = actor_critic.act(rollouts.obs[-1])\n        \n        rollouts.compute_return(next_val, gamma)\n        loss = agent.update(rollouts, ppo_epochs, clip_param,vf_coef,entropy_coef)\n        \n        logger.add_loss(*loss)\n        rollouts.reset()\n\n        if verbose and (i+1)%log_freq==0:\n            end = time.time()\n            total_num_step = num_steps*num_processes*(i+1)\n            logger.report(i+1, num_updates)\n            print('FPS:{:.1f}'.format(total_num_step \/ (end-start) ))\n        if log_vid_freq > 0 and (i+1)%log_vid_freq==0:\n            print('Game play of current policy:')\n            logger.play_and_display(make_env(env_name, grayscale=grayscale, num_stack=num_stack), actor_critic)\n    \n    envs.close()\n    logger.plot()\n    # play four different times\n    for _ in range(num_game_demo):\n        logger.play_and_display(make_env(env_name, grayscale=grayscale, num_stack=num_stack), actor_critic)\n    \n    # save model\n    path = env_name + '_' + time.strftime(\"%d-%m-%Y_%H-%M-%S\")+'.pt'\n    torch.save(actor_critic.state_dict(), path)\n    print('Model is saved at', path)","cc186bea":"args = { \n    'env_name': 'MiniGrid-MyEnv-Same-v0',\n    'num_updates': int(1e2),\n    'lr': 1e-4,\n    'log_freq': 33,\n    'num_steps': 512,\n    'max_len': 500,\n    'vf_coef': 1,\n    'minibatch_size': 128,\n    'ppo_epochs': 5,\n    'num_processes': 1,\n    'num_game_demo': 1\n#     'grayscale': True,\n#     'num_stack': 1, # for stacking frames\n}\nmain(**args)","22ffda14":"args = { \n    'env_name': 'MiniGrid-MyEnv-Left-v0',\n    'num_updates': int(6e2),\n    'lr': 1e-4,\n    'log_freq': 160,\n    'num_steps': 512,\n    'max_len': 500,\n    'vf_coef': 1,\n    'minibatch_size': 128,\n    'ppo_epochs': 5,\n    'num_processes': 2,\n    'num_game_demo': 1\n#     'grayscale': True,\n#     'num_stack': 1, # for stacking frames\n}\nmain(**args)","412719e7":"args = { \n    'env_name': 'MiniGrid-MyEnv-Right-v0',\n    'num_updates': int(6e2),\n    'lr': 1e-4,\n    'log_freq': 150,\n    'num_steps': 512,\n    'max_len': 500,\n    'vf_coef': 1,\n    'minibatch_size': 128,\n    'ppo_epochs': 5,\n    'num_processes': 2,\n    'num_game_demo': 1\n#     'grayscale': True,\n#     'num_stack': 1, # for stacking frames\n}\nmain(**args)","722861a3":"args = { \n    'env_name': 'MiniGrid-MyEnv-Opposite-v0',\n    'num_updates': int(6e2),\n    'lr': 1e-4,\n    'log_freq': 150,\n    'num_steps': 512,\n    'max_len': 500,\n    'vf_coef': 1,\n    'minibatch_size': 128,\n    'ppo_epochs': 5,\n    'num_processes': 2,\n    'num_game_demo': 1\n#     'grayscale': True,\n#     'num_stack': 1, # for stacking frames\n}\nmain(**args)","9882e7fe":"# Distribution","1ef40239":"# Envs","0b41ae48":"# Logger","42ec09b3":"# Replay buffer","8c45b0f7":"This code is based on [this repo](https:\/\/github.com\/ikostrikov\/pytorch-a2c-ppo-acktr-gail)","ed0c38ea":"# Test with different environments","7733d5b5":"# Main","bc644815":"# Policy","8148c616":"# Model","153de5a8":"# PPO","67ad5e5e":"# Seed\n"}}