{"cell_type":{"105cb204":"code","f3b550e3":"code","32dd8d99":"code","7023dab3":"code","1b16461c":"code","84d53696":"code","ab9fa197":"code","58a588f4":"code","6b4bc52f":"code","e357fce3":"code","2644714e":"code","3fc280cd":"code","90ae131f":"code","9466f44f":"code","40eebb26":"code","cdae0572":"code","e0a345ce":"code","b96421c5":"code","d1efc115":"code","856adc79":"code","cc006b75":"code","21a250e1":"code","dfe770cd":"code","f03ec564":"code","41c4703d":"markdown","044d31fd":"markdown","81f820f8":"markdown","b3352f42":"markdown","76d0dd5b":"markdown","2eaa205c":"markdown"},"source":{"105cb204":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3b550e3":"from fastai import *\nfrom fastai.tabular import *","32dd8d99":"train = pd.read_csv('\/kaggle\/input\/electrical-consumption\/train_6BJx641.csv')\ntest =  pd.read_csv('\/kaggle\/input\/electrical-consumption\/test_pavJagI.csv')\nprint(train.shape)\nprint(test.shape)","7023dab3":"comb = pd.concat([train,test])\nprint(comb.shape)","1b16461c":"comb['datetime'] = pd.to_datetime(comb['datetime'])","84d53696":"comb.head()","ab9fa197":"from pykalman import KalmanFilter\ndef Kalman1D(observations,damping=1):\n    # To return the smoothed time series data\n    observation_covariance = damping\n    initial_value_guess = observations[0]\n    transition_matrix = 1\n    transition_covariance = 0.1\n    initial_value_guess\n    kf = KalmanFilter(\n            initial_state_mean=initial_value_guess,\n            initial_state_covariance=observation_covariance,\n            observation_covariance=observation_covariance,\n            transition_covariance=transition_covariance,\n            transition_matrices=transition_matrix\n        )\n    pred_state, state_cov = kf.smooth(observations)\n    return pred_state\n\n# Kalman Filter\nobservation_covariance = .0015\ncomb['temperature'] = Kalman1D(comb.temperature.values,observation_covariance)\ncomb['var1'] = Kalman1D(comb.var1.values,observation_covariance)\ncomb['pressure'] = Kalman1D(comb.pressure.values,observation_covariance)\ncomb['windspeed'] = Kalman1D(comb.windspeed.values,observation_covariance)\n#test['signal'] = Kalman1D(test.signal.values,observation_covariance)\n","58a588f4":"def booleancon(x):\n    if x == True:\n        return 1\n    else:\n        return 0\n    \n\n\ndef daypart(x):\n    if x >= 0 and x<=4:\n        return 101\n    elif x>=5 and x<=8:\n        return 102\n    elif x>=9 and x<=12:\n        return 103\n    elif x>=13 and x<=16:\n        return 104\n    elif x>=17 and x<=19:\n        return 105\n    elif x>=20 and x<=23:\n        return 106\n    else:\n        return 0\n    \ndef seasons(x):\n    if x>=3 and x<5:\n        return 0\n    elif x>=6 and x<=8:\n        return 1\n    elif x>=9 and x<=11:\n        return 2\n    elif x>=12 and x<=2:\n        return 3\n    else:\n        return 0\n    \ndef var2(x):\n    if x=='A':\n        return 1\n    elif x=='B':\n        return 2\n    else:\n        return 3\n\ndef time_pr(train):\n    train = add_datepart(train,'datetime',drop=False,time=True)\n    #train.drop(['datetimeIs_month_end', 'datetimeIs_quarter_end','datetimeIs_year_start','datetimeIs_year_end'], axis=1, inplace=True)\n    train['datetimeIs_month_end'] = train['datetimeIs_month_end'].apply(booleancon)\n    train['datetimeIs_month_start']   = train['datetimeIs_month_start'].apply(booleancon)\n    train['datetimeIs_quarter_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_quarter_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['datetimeIs_year_start'] = train['datetimeIs_quarter_start'].apply(booleancon)\n    train['datetimeIs_year_end'] = train['datetimeIs_quarter_end'].apply(booleancon)\n    train['var2'] = train['var2'].apply(var2)\n    train['daypart'] = train['datetimeHour'].apply(daypart)\n    train['season'] = train['datetimeMonth'].apply(seasons)\n    train['year_month'] = train['datetimeYear'].astype(str)+'_'+train['datetimeMonth'].astype(str)\n    train['MonthCat'] = 'M'+train['datetimeMonth'].astype(str)\n    train['HourCat'] = 'H'+train['datetimeHour'].astype(str)\n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        d = {}\n        d['mean'+c] = train.groupby(['year_month'])[c].mean()\n        d['median'+c] = train.groupby(['year_month'])[c].median()\n        d['max'+c] = train.groupby(['year_month'])[c].max()\n        d['min'+c] = train.groupby(['year_month'])[c].min()\n        d['std'+c] = train.groupby(['year_month'])[c].std()\n        d['mean_abs_chg'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        d['abs_max'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.max(np.abs(x)))\n        d['abs_min'+c] = train.groupby(['year_month'])[c].apply(lambda x: np.min(np.abs(x)))\n        for v in d:\n            train[v] = train['year_month'].map(d[v].to_dict())\n        train['range'+c] = train['max'+c] - train['min'+c]\n        train['maxtomin'+c] = train['max'+c] \/ train['min'+c]\n        train['abs_avg'+c] = (train['abs_min'+c] + train['abs_max'+c]) \/ 2\n    for c in ['temperature','var1','pressure','datetimeElapsed']:\n        train['signal_shift_+1'+c] = train.groupby(['year_month'])[c].shift(1)\n        train['signal_shift_-1'+c] = train.groupby(['year_month'])[c].shift(-1)\n        train['signal_shift_+2'+c] = train.groupby(['year_month'])[c].shift(2)\n        train['signal_shift_-2'+c] = train.groupby(['year_month'])[c].shift(-2)\n        train['signal_shift_+3'+c] = train.groupby(['year_month'])[c].shift(3)\n        train['signal_shift_-3'+c] = train.groupby(['year_month'])[c].shift(-3)\n        train['signal_shift_+4'+c] = train.groupby(['year_month'])[c].shift(4)\n        train['signal_shift_-4'+c] = train.groupby(['year_month'])[c].shift(-4)\n        train['signal_shift_+5'+c] = train.groupby(['year_month'])[c].shift(5)\n        train['signal_shift_-5'+c] = train.groupby(['year_month'])[c].shift(-5)\n        \n        train['signal_shift_+5'+c] = train.groupby(['year_month'])[c].shift(5)\n        train['signal_shift_-5'+c] = train.groupby(['year_month'])[c].shift(-5)\n    return train\n","6b4bc52f":"pd.set_option('display.max_columns', 1000)  # or 1000\npd.set_option('display.max_rows', 1000)  # or 1000\npd.set_option('display.max_colwidth', 199)  # or 199\ncomb = time_pr(comb)","e357fce3":"comb.head()","2644714e":"dummy_train = comb[comb['datetimeDay']<=16].fillna(method='bfill')\ndummy_test = comb[(comb['datetimeDay']>16) & (comb['datetimeDay']<=23)].fillna(method='bfill')\n","3fc280cd":"col = []\nfor i in comb.columns:\n    if i!= 'electricity_consumption' and i!='ID' and i!='datetime' and i!='year_month' and i!='MonthCat' and i!='HourCat':\n        col.append(i)\n","90ae131f":"actual_data = comb[comb['datetimeDay']<=23].fillna(method='bfill')","9466f44f":"X = actual_data[col].values\nY = actual_data['electricity_consumption'].values","40eebb26":"x_train = dummy_train[col].values\ny_train = dummy_train['electricity_consumption'].values\nx_test = dummy_test[col].values\ny_test = dummy_test['electricity_consumption'].values","cdae0572":"import lightgbm as lgb\nd_train = lgb.Dataset(x_train, label=y_train)\nd_test = lgb.Dataset(x_test, label=y_test)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 1000\nparams['learning_rate'] = 0.015\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = -1\nparams['num_threads'] = 10\nparams['max_bin'] = 217\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=42\nclf = lgb.train(params, d_train, 2000,d_test,verbose_eval=200, early_stopping_rounds=200)","e0a345ce":"# 42  [995]\tvalid_0's rmse: 91.8676","b96421c5":"from xgboost import XGBRegressor\np=XGBRegressor(n_estimators=30000,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# max_depth=5,0.018\np.fit(x_train,y_train,eval_set=[(x_test, y_test)],eval_metric='rmse',early_stopping_rounds=500,verbose=200)\n#p.fit(X,Y)\n","d1efc115":"from catboost import CatBoostRegressor\ncb_model = CatBoostRegressor(n_estimators = 1000,\n    loss_function = 'RMSE',\n    eval_metric = 'RMSE',random_state=1729)\ncb_model.fit(x_train, y_train, use_best_model=True, eval_set=(x_test, y_test), early_stopping_rounds=50)","856adc79":"d_train = lgb.Dataset(X, label=Y)\nparams = {}\nparams['application']='root_mean_squared_error'\nparams['num_boost_round'] = 1000\nparams['learning_rate'] = 0.015\nparams['boosting_type'] = 'gbdt'\nparams['metric'] = 'rmse'\nparams['sub_feature'] = 0.833\nparams['num_leaves'] = 15\nparams['min_split_gain'] = 0.05\nparams['min_child_weight'] = 27\nparams['max_depth'] = -1\nparams['num_threads'] = 10\nparams['max_bin'] = 217\nparams['lambda_l2'] = 0.10\nparams['lambda_l1'] = 0.30\nparams['feature_fraction']= 0.833\nparams['bagging_fraction']= 0.979\nparams['seed']=42\nclf = lgb.train(params, d_train, 2000)","cc006b75":"# p=XGBRegressor(n_estimators=30000,random_state=1729,learning_rate=0.017,max_depth=4,n_jobs=4)\n# # max_depth=5,0.018\n# p.fit(X,Y)\n","21a250e1":"test = comb[comb['datetimeDay']>23]\nx_test = test[col].values\npred = clf.predict(x_test)\ntest['electricity_consumption'] =[round(i) for i in pred]\ntest[['ID','electricity_consumption']].to_csv('result.csv',header=True,index = None)","dfe770cd":"col_dict ={}\nz=0\nfor i in col:\n    col_dict[i]=z\n    z=z+1\n    ","f03ec564":"lgb.plot_importance(clf,importance_type='split', max_num_features=25)","41c4703d":"LGBM","044d31fd":"XGBOOST","81f820f8":"Modeling","b3352f42":"WINDOWS=[3,5]\ndef create_rolling_features(df):\n    for window in WINDOWS:\n        df[\"rolling_mean_temp_\" + str(window)] = df['temperature'].rolling(window=window).mean()\n        df[\"rolling_std_temp_\" + str(window)] = df['temperature'].rolling(window=window).std()\n        df[\"rolling_var_temp_\" + str(window)] = df['temperature'].rolling(window=window).var()\n        df[\"rolling_min_temp_\" + str(window)] = df['temperature'].rolling(window=window).min()\n        df[\"rolling_max_temp_\" + str(window)] = df['temperature'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_temp_\" + str(window)] = df[\"rolling_min_temp_\" + str(window)] \/ df[\"rolling_max_temp_\" + str(window)]\n        df[\"rolling_min_max_diff_temp_\" + str(window)] = df[\"rolling_max_temp_\" + str(window)] - df[\"rolling_min_temp_\" + str(window)]\n        \n        df[\"rolling_mean_var1_\" + str(window)] = df['var1'].rolling(window=window).mean()\n        df[\"rolling_std_var1_\" + str(window)] = df['var1'].rolling(window=window).std()\n        df[\"rolling_var_var1_\" + str(window)] = df['var1'].rolling(window=window).var()\n        df[\"rolling_min_var1_\" + str(window)] = df['var1'].rolling(window=window).min()\n        df[\"rolling_max_var1_\" + str(window)] = df['var1'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_var1_\" + str(window)] = df[\"rolling_min_var1_\" + str(window)] \/ df[\"rolling_max_var1_\" + str(window)]\n        df[\"rolling_min_max_diff_var1_\" + str(window)] = df[\"rolling_max_var1_\" + str(window)] - df[\"rolling_min_var1_\" + str(window)]\n        \n        df[\"rolling_mean_pressure_\" + str(window)] = df['pressure'].rolling(window=window).mean()\n        df[\"rolling_std_pressure_\" + str(window)] = df['pressure'].rolling(window=window).std()\n        df[\"rolling_var_pressure_\" + str(window)] = df['pressure'].rolling(window=window).var()\n        df[\"rolling_min_pressure_\" + str(window)] = df['pressure'].rolling(window=window).min()\n        df[\"rolling_max_pressure_\" + str(window)] = df['pressure'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_pressure_\" + str(window)] = df[\"rolling_min_pressure_\" + str(window)] \/ df[\"rolling_max_pressure_\" + str(window)]\n        df[\"rolling_min_max_diff_pressure_\" + str(window)] = df[\"rolling_max_pressure_\" + str(window)] - df[\"rolling_min_pressure_\" + str(window)]\n        \n        df[\"rolling_mean_windspeed\" + str(window)] = df['windspeed'].rolling(window=window).mean()\n        df[\"rolling_std_windspeed\" + str(window)] = df['windspeed'].rolling(window=window).std()\n        df[\"rolling_var_windspeed\" + str(window)] = df['windspeed'].rolling(window=window).var()\n        df[\"rolling_min_windspeed\" + str(window)] = df['windspeed'].rolling(window=window).min()\n        df[\"rolling_max_windspeed\" + str(window)] = df['windspeed'].rolling(window=window).max()\n        df[\"rolling_min_max_ratio_windspeed\" + str(window)] = df[\"rolling_min_windspeed\" + str(window)] \/ df[\"rolling_max_windspeed\" + str(window)]\n        df[\"rolling_min_max_diff_windspeed\" + str(window)] = df[\"rolling_max_windspeed\" + str(window)] - df[\"rolling_min_windspeed\" + str(window)]    \n    \n    \n    df = df.replace([np.inf, -np.inf], np.nan)    \n    df.fillna(0, inplace=True)\n    return df\n\ncomb = create_rolling_features(comb)\n#test = create_rolling_features(test)","76d0dd5b":"PRE-PROCESSING","2eaa205c":"CatBoost"}}