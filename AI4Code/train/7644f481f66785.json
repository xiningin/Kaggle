{"cell_type":{"f95b5d11":"code","10f9385c":"code","da7b65c7":"code","33987eb4":"code","3ead6876":"code","46309bfc":"code","0865a754":"code","328d3a2e":"code","06be85b7":"code","f4c7f34b":"code","102da547":"code","687db93d":"code","74861bf8":"code","470e5d07":"code","7908aefa":"code","7e42b339":"code","50067aca":"code","cbff34c0":"code","13a7ee76":"markdown","07c97f68":"markdown","55d5f270":"markdown","ed021dfa":"markdown","3c71bad1":"markdown","07ccbb2a":"markdown","bc5232d7":"markdown","46717829":"markdown","1739a9db":"markdown","a8052e15":"markdown","6bb3988f":"markdown"},"source":{"f95b5d11":"import numpy as np\nimport pandas as pd\nfrom keras import Model\nfrom keras.applications.mobilenet import MobileNet, preprocess_input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\nfrom keras.layers import Conv2D, Reshape\nfrom keras.utils import Sequence\nfrom keras.backend import epsilon\nimport tensorflow as tf\n\nfrom PIL import Image\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nnp.random.seed(1)","10f9385c":"train = pd.read_csv(\"..\/input\/racoon-detection\/train_labels_.csv\")","da7b65c7":"train.head()","33987eb4":"train.shape","3ead6876":"row = train.sample()\n\nimg = cv2.imread(f'..\/input\/racoon-detection\/Racoon Images\/images\/{row.filename.values[0]}')\nxmin = row.xmin.values[0]\nymin = row.ymin.values[0]\nxmax = row.xmax.values[0]\nymax = row.ymax.values[0]\n\nimg = cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color=(255, 0, 0), thickness=4)\nplt.imshow(img)\nplt.show()\n","46309bfc":"IMAGE_SIZE = 128","0865a754":"coords = train[[\"width\",\"height\",\"xmin\",\"ymin\",\"xmax\",\"ymax\"]]\n\ncoords[\"xmin\"] = coords[\"xmin\"] * IMAGE_SIZE \/ coords[\"width\"]\ncoords[\"xmax\"] = coords[\"xmax\"] * IMAGE_SIZE \/ coords[\"width\"]\ncoords[\"ymin\"] = coords[\"ymin\"] * IMAGE_SIZE \/ coords[\"height\"]\ncoords[\"ymax\"] = coords[\"ymax\"] * IMAGE_SIZE \/ coords[\"height\"]\n\ncoords.drop([\"width\",\"height\"], axis=1, inplace=True)\ncoords.head()","328d3a2e":"paths = train[\"filename\"]\nlen(paths)","06be85b7":"images = \"..\/input\/racoon-detection\/Racoon Images\/images\/\"\n\nbatch_images = np.zeros((len(paths), IMAGE_SIZE, IMAGE_SIZE,3), dtype=np.float32)\n\nfor i, f in enumerate(paths):\n    img = Image.open(images + f)\n    img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n    img = img.convert('RGB')\n    batch_images[i] = preprocess_input(np.array(img, dtype=np.float32))","f4c7f34b":"model = MobileNet(input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3), include_top=False)","102da547":"for layers in model.layers:\n    layers.trainable = False\n\nx = model.layers[-1].output\nx = Conv2D(4, kernel_size=4, name=\"coords\")(x)\nx = Reshape((4,))(x)\n\nmodel = Model(inputs=model.inputs, outputs=x)\nmodel.summary()","687db93d":"def loss(gt, pred):\n    intersections = 0\n    unions = 0\n    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n    intersection = diff_width * diff_height\n    \n    # Compute union\n    area_gt = gt[:,2] * gt[:,3]\n    area_pred = pred[:,2] * pred[:,3]\n    union = area_gt + area_pred - intersection\n\n    # Compute intersection and union over multiple boxes\n    for j, _ in enumerate(union):\n        if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n            intersections += intersection[j]\n            unions += union[j]\n\n    # Compute IOU. Use 1e-8 to prevent division by zero\n    iou = np.round(intersections \/ (unions + 1e-8), 4)\n    iou = iou.astype(np.float32)\n    return iou\n\n\ndef IoU(y_true, y_pred):\n    iou = tf.py_function(loss, [y_true, y_pred], tf.float32)\n    return iou","74861bf8":"gt = coords\n\nPATIENCE=10\n\nmodel.compile(optimizer=\"Adam\",\n              loss=\"mse\",\n              metrics=[IoU])\n\nstop = EarlyStopping(monitor='val_iou', patience=PATIENCE, mode=\"max\")\n\nreduce_lr = ReduceLROnPlateau(monitor='val_iou', factor=0.2, patience=PATIENCE,\n                              min_lr=1e-7, verbose=1, mode=\"max\")\n\nmodel.fit(batch_images, gt,\n          epochs=100,\n          callbacks=[stop, reduce_lr],\n          verbose=1)","470e5d07":"test_img = random.choice(paths)\nfilename = images + test_img\nunscaled = cv2.imread(filename)\nunscaled = cv2.cvtColor(unscaled, cv2.COLOR_BGR2RGB)","7908aefa":"image_height, image_width, _ = unscaled.shape\nimage = cv2.resize(unscaled, (IMAGE_SIZE, IMAGE_SIZE))\nfeat_scaled = preprocess_input(np.array(image, dtype=np.float32))","7e42b339":"region = model.predict(x=np.array([feat_scaled]))[0]","50067aca":"x0 = int(region[0] * image_width \/ IMAGE_SIZE) \ny0 = int(region[1] * image_height \/ IMAGE_SIZE)\n\nx1 = int((region[2]) * image_width \/ IMAGE_SIZE)\ny1 = int((region[3]) * image_height \/ IMAGE_SIZE)","cbff34c0":"unscaled = cv2.rectangle(unscaled, (x0, y0), (x1, y1), color=(255, 0, 0), thickness=2)\n\n# Display the image\nplt.imshow(unscaled)\nplt.show()","13a7ee76":"## Preprocessing of Test Image\nResizing the image to 128 * 128 and preprocess the image for the MobileNet model","07c97f68":"## Compiling the model","55d5f270":"## Pick a test image from the given data","ed021dfa":"## Define a custom loss function IoU which calculates Intersection Over Union","3c71bad1":"## Reading the training data from train.csv file","07ccbb2a":"* Scaling the BBox","bc5232d7":"## Plotting the predicted bounding box","46717829":"## Model Building\n* Building the model using transfer learning","1739a9db":"## Importing the necessary libraries","a8052e15":"* Predict the coordinates of the bounding box for the given test image","6bb3988f":"* Create a list variable known as 'path' which has all the path for all the training images\n* Create an array 'coords' which has the resized coordinates of the bounding box for the training images"}}