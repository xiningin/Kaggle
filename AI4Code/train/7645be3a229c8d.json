{"cell_type":{"a841e16e":"code","4e7cddf3":"code","7b001b18":"code","de85c918":"code","4023085e":"code","dce7a90a":"code","f84a8b25":"code","d6aad468":"code","67c746f1":"code","90a33d29":"code","5610f899":"code","7cbdf176":"code","93db9665":"code","a2ec13ee":"code","1c1f0a13":"code","ae4de3c7":"code","7b0c209d":"code","96f58ad9":"code","3b60eb7d":"code","f506e087":"code","e63becbf":"code","2c5e0330":"code","f2ed61d6":"code","ff807a77":"code","add085a6":"code","cbbc91eb":"code","3fd2dcf7":"markdown","0e78b984":"markdown","6d1eb7ae":"markdown","da35cc3c":"markdown","a1f36e2d":"markdown","aa3f31e3":"markdown","7c7ed49c":"markdown","0a45c976":"markdown","a17b83fc":"markdown","fb14aece":"markdown","bf3e3251":"markdown","03679962":"markdown","2c70ed45":"markdown","bf421d4e":"markdown","3f03ab19":"markdown","08375e09":"markdown","20d6cf7b":"markdown","d941cb31":"markdown","c5cb00dc":"markdown","a4881303":"markdown"},"source":{"a841e16e":"import pandas as pd\nimport numpy as np","4e7cddf3":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain","7b001b18":"train.isnull().sum()","de85c918":"from scipy import stats\n\nsurvived = train[train['Survived']==1]\ndid_not_survive = train[train['Survived']==0]\n\n# libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.rcParams.update({'font.size': 13})\n# set width of bar\nbarWidth = 0.25\n\n\nmale = [len(survived[survived['Sex']=='male']), \\\n        len(did_not_survive[did_not_survive['Sex']=='male'])]\nfemale = [len(survived[survived['Sex']=='female']), \\\n         len(did_not_survive[did_not_survive['Sex']=='female'])]\n\n# set height of bar\nbars1 = [len(survived[survived['Sex']=='male']), \\\n         len(did_not_survive[did_not_survive['Sex']=='male'])]\nbars2 = [len(survived[survived['Sex']=='female']), \\\n         len(did_not_survive[did_not_survive['Sex']=='female'])]\n \n# Set position of bar on X axis\nr1 = np.arange(len(bars1))\nr2 = [x + barWidth for x in r1]\n \n# Make the plot\nplt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='Survived')\nplt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='Did not survive')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Sex', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(bars1))], ['Male', 'Female'])\n \n# Create legend & Show graphic\nplt.legend()\nplt.show()\n\np_value = stats.chi2_contingency([male, female])[1]\nprint(\"Chi-Square P-Value: \" + str(p_value))","4023085e":"\n# libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n \n# set width of bar\nbarWidth = 0.25\n\nclass1 = [len(survived[survived['Pclass']==1]), \\\n        len(survived[survived['Pclass']==2]), \\\n        len(survived[survived['Pclass']==3]),]\nclass2 = [len(did_not_survive[did_not_survive['Pclass']==1]), \\\n        len(did_not_survive[did_not_survive['Pclass']==2]), \\\n        len(did_not_survive[did_not_survive['Pclass']==3])]\n \n# Set position of bar on X axis\nr1 = np.arange(len(class1))\nr2 = [x + barWidth for x in r1]\n\n# Make the plot\nplt.bar(r1, class1, color='#7f6d5f', width=barWidth, edgecolor='white', label='Survived')\nplt.bar(r2, class2, color='#557f2d', width=barWidth, edgecolor='white', label='Did not survive')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Class', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(class1))], ['Class 1', 'Class 2', 'Class 3'])\n \n# Create legend & Show graphic\nplt.legend()\nplt.show()\n\n\nclass1 = [len(survived[survived['Pclass']==1]), \\\n        len(did_not_survive[did_not_survive['Pclass']==1])]\nclass2 = [len(survived[survived['Pclass']==2]), \\\n        len(did_not_survive[did_not_survive['Pclass']==2])]\nclass3 = [len(survived[survived['Pclass']==3]), \\\n        len(did_not_survive[did_not_survive['Pclass']==3])]\n\np_value = stats.chi2_contingency([class1, class2, class3])[1]\nprint(\"Chi-Square P-Value: \" + str(p_value))","dce7a90a":"\n# libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n \n# set width of bar\nbarWidth = 0.25\n\nclass1 = [len(survived[survived['Embarked']=='Q']), \\\n        len(survived[survived['Embarked']=='C']), \\\n        len(survived[survived['Embarked']=='S']),]\nclass2 = [len(did_not_survive[did_not_survive['Embarked']=='Q']), \\\n        len(did_not_survive[did_not_survive['Embarked']=='C']), \\\n        len(did_not_survive[did_not_survive['Embarked']=='S'])]\n \n# Set position of bar on X axis\nr1 = np.arange(len(class1))\nr2 = [x + barWidth for x in r1]\n\n# Make the plot\nplt.bar(r1, class1, color='#7f6d5f', width=barWidth, edgecolor='white', label='Survived')\nplt.bar(r2, class2, color='#557f2d', width=barWidth, edgecolor='white', label='Did not survive')\n \n# Add xticks on the middle of the group bars\nplt.xlabel('Embarked', fontweight='bold')\nplt.xticks([r + barWidth for r in range(len(class1))], ['Queenstown', 'Cherbourg', 'Southampton'])\n \n# Create legend & Show graphic\nplt.legend()\nplt.show()\n\nQ_embark = [len(survived[survived['Embarked']=='Q']), \\\n        len(did_not_survive[did_not_survive['Embarked']=='Q'])]\nC_embark = [len(survived[survived['Embarked']=='C']), \\\n        len(did_not_survive[did_not_survive['Embarked']=='C'])]\nS_embark = [len(survived[survived['Embarked']=='S']), \\\n        len(did_not_survive[did_not_survive['Embarked']=='S'])]\n\np_value = stats.chi2_contingency([Q_embark, C_embark, S_embark])[1]\nprint(\"Chi-Square P-Value: \" + str(p_value))","f84a8b25":"import matplotlib.pyplot as plt\nimport numpy as np\nimport random\n\nplt.rcParams[\"figure.figsize\"] = (20,3)\nplt.rcParams.update({'font.size': 13})\n\ndata = {}\ndata['Age'] = {\n    'Survived': list(survived.dropna()['Age']),\n    'Did not Survive': list(did_not_survive.dropna()['Age']),\n}\ndata['SibSp'] = {\n    'Survived': list(survived.dropna()['SibSp']),\n    'Did not Survive': list(did_not_survive.dropna()['SibSp']),\n}\ndata['Parch'] = {\n    'Survived': list(survived.dropna()['Parch']),\n    'Did not Survive': list(did_not_survive.dropna()['Parch']),\n}\ndata['Fare'] = {\n    'Survived': list(survived.dropna()['Fare']),\n    'Did not Survive': list(did_not_survive.dropna()['Fare']),\n}\n\nfig, axes = plt.subplots(ncols=4)\nfig.subplots_adjust(wspace=0)\n\nfor ax, name in zip(axes, ['Age', 'SibSp', 'Parch', 'Fare']):\n    ax.boxplot([data[name][item] for item in ['Survived', 'Did not Survive']])\n    ax.set(xticklabels=['Survived', 'Did not Survive'], xlabel=name)\n    ax.margins(0.05) # Optional\n\nplt.show()","d6aad468":"train = pd.get_dummies(train, columns=['Sex', 'Pclass', 'Embarked'])","67c746f1":"train.corr()","90a33d29":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\n\nX = train[['Fare', 'Sex_male', 'Pclass_1', 'Pclass_3', 'Embarked_C', 'Embarked_S']]\ny = train['Survived']\nX = my_imputer.fit_transform(X)","5610f899":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\nclf = LogisticRegression(solver='liblinear')\n\n# Create regularization hyperparameter space\nC = [0.001, 0.01, 0.1, 0.3, 0.5, 0.7, 1, 10, 100]\npenalty = ['l1','l2']\n\nhyperparameters = dict(C=C, penalty=penalty)\n\ngrid_clf_acc = GridSearchCV(clf, param_grid=hyperparameters, cv=5, verbose=0)\ngrid_clf_acc.fit(X_train, y_train)","7cbdf176":"grid_clf_acc.best_params_","93db9665":"clf = LogisticRegression(solver='liblinear', **grid_clf_acc.best_params_)","a2ec13ee":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nclf.fit(X_train, y_train)\n\nX_test = scaler.transform(X_test)\n\ny_pred = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))","1c1f0a13":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred, target_names=['Survived', 'Did not survive']))","ae4de3c7":"lr = clf.fit(X, y)","7b0c209d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n\nclf = RandomForestClassifier()\n\nhyperparameters = {\n    'bootstrap': [True, False],\n    'max_depth': [80, 90, 100, 110],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\ngrid_clf_acc = GridSearchCV(clf, param_grid=hyperparameters, cv=5, verbose=0)\ngrid_clf_acc.fit(X_train, y_train)","96f58ad9":"clf = RandomForestClassifier(**grid_clf_acc.best_params_)","3b60eb7d":"clf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\n\nprint(accuracy_score(y_test, y_pred))","f506e087":"print(classification_report(y_test, y_pred, target_names=['Survived', 'Did not survive']))","e63becbf":"rf = clf.fit(X, y)","2c5e0330":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","f2ed61d6":"test = pd.get_dummies(test, columns=['Sex', 'Pclass', 'Embarked'])\nX = test[['Fare', 'Sex_male', 'Pclass_1', 'Pclass_3', 'Embarked_C', 'Embarked_S']]\nX = my_imputer.fit_transform(X)","ff807a77":"pred = rf.predict(X)","add085a6":"submission = pd.DataFrame({'PassengerId': test['PassengerId'],'Survived': pred})","cbbc91eb":"submission.to_csv('\/kaggle\/working\/titanic',index=False)","3fd2dcf7":"There's plenty of missing values for the Age column and Cabin column. We'll check later if it's worth imputing values into those columns.\nLet's then check the relationship between the categorical variables and the dependent variable.\n\nLet's check the Sex variable which indicates the passenger's sex.","0e78b984":"The variables that noticeably correlated with Survived were Fare, Sex, Pclass and Embarked. Knowing that, we'll choose those variables as our predictors for our model.\n\nSince one of our predictors is Embarked, and we have observed earlier that there were 2 missing values for that. Let's create a simple imputer that will impute the missing values.","6d1eb7ae":"We can see that the model performs better when predicting passengers that Survived than passengers that didn't survive. Let's train the Logistic Regression model using the entirety of our data.","da35cc3c":"Embarked seems to also tell us something interesting. Passengers that have embarked from Southampton had a noticeable difference in survival rate which leans more towards not surviving. Passengers coming from Cherbourg were a bit more likely to survive and passengers embarking from Queenstown is less likely to survive. Doing another Chi-square test tells us to reject the Null Hypothesis as well.\n\nLet's now take a look at the numerical variables by using a barplot.","a1f36e2d":"## 3.3 - Making the Submission\nWe'll be using our random forest model for submitting our results. Since this is a competition, we'd be choosing the more powerful model that performed better despite the sacrifice in computational power and in simplicity.","aa3f31e3":"# 4 - Last Remarks\nThanks for allowing me to present this work to you guys! If you liked the notebook, please give it an upvote!","7c7ed49c":"## 2.2 - Data Manipulation and Feature Selection\n\nNow that we've taken a look at the data, let's now start to manipulate the data for modeling and choose the features from our dataset. First, let's one-hot our categorical variables.","0a45c976":"Let's take a look at the correlation table to check what variables correlate to \"Survived\".","a17b83fc":"We can see that the model actually does better for both classes when we compare it to our Logistic Regression model from before. It still does better when it predicts the class of passengers that survived.\n\n\n## 3.3 - So which one's better?\nSo in the end, can we conclude that a random forest model is much better than a logistic regression model? Well, that depends on what you mean by better. Sometimes when resources are constrained and less computational power is better, the Logistic Regression model would win on that scenario. A Logistic Regression model is much simpler and is easily understandable by many practitioners. However, its simplicity comes with the sacrifice of a bit of accuracy. If that extra increase in predictive power is of utmost importance, like in competitions. Then our random forest model would be the better choice. So in the end, it depends on the scenario.\n\nLet's train our random forest model as well on the entire dataset.","fb14aece":"Based on the charts above, we can see that there's a noticeable difference between survival depending on the sex of the passenger. We can observe that females were more likely to survive than males. This is backed up by the Chi-Square's P-value, which indicates that we reject the Null Hypothesis that knowing Sex does not help in predicting Survival.\n\nLet's then check the class variable which indicates the seat class of a passenger. (Class 1 being the best seat)","bf3e3251":"The median age of the passengers have a bit of a difference, with the median age being higher for passengers who did not survive. SibSp and Parch does not have a noticeable difference. Fare for the passengers that did not survive has a lower median as compared to passengers that survived. Passengers that survived had noticeably more outliers that passenger that did not survive.","03679962":"# Titanic - EDA, Logistic Regression, Random Forest\n\nWe'll be approaching the Titanic survival classification competition using two well-known modeling techniques and look at the pros and cons of choosing either one of the models over the other.\n\n# 1 - Introduction\n\nThe titanic classification competition is the starter competition here in Kaggle for new members. I've chosen Logistic Regression as my primary choice for the model. Afterwards, I'll test out Random Forest as well and check the difference in results between the two models.\n\n## 1.1 - Why Logistic Regression\nLogistic Regression is a popular traditional machine learning algorithm. Since our model predicts whether the passenger survives or not (binary classifier), then a logistic regression model would be a perfect approach for this problem as it is effective and also very simple. Training time is also very fast!\n\nAnother reason as to why Logistic Regression was chosen is because its loss function is convex, meaning that finding the global minimum is certain as long as your learning rate is not too high.\n\n#### Note: Logistic Regression is usable as well for multi-class problems by doing Softmax Regression.\n\n## 1.2 - Why Random Forest\nRandom Forest is another popular algorithm for creating powerful classifiers by utilizing an ensemble of decision trees. Although it is very powerful, it is not my primary choice as it trades off interpretability for the sake of accuracy. Another sacrifice that would be made here as well is computational power. Random Forest models take a longer time to train, hyperparameter tuning can become a problem because of this.\n\n# 2 - Exploratory Data Analysis\n\nWe'll first do an exploratory data analysis so we can get a feel on how the data looks like and have an idea as to what features to select. The primary steps that we'll have would be:\n\n- Get a feel of the dataset by examining the variables\n- Data Manipulation and Feature Selection\n\n## 2.1 - Get a feel of the dataset\n\nLet's first import the dependencies that we'll be using then load the data.","2c70ed45":"We achieved an accuracy of ~79% using the Logistic Regression model. It's satisfactory. However, accuracy isn't always the best way to assess the model's performence. Let's look at the confusion matrix.","bf421d4e":"Let's take a look at the grid search results.","3f03ab19":"## 3.2 - Random Forest\nThis time we'll be using training a Random Forest model with out dataset. A random forest model utilizes an ensemble of trees to create a prediction. Since we're using a tree-based algorithm, we won't have to standardize our data. This model is well-known due to its power, but we are trading interpretability for it (when compared to our Logistic Regression model). Let's see if this model performs better than our Logistic Regression model.\n\nLet's repeat the same steps that we've done previously.","08375e09":"Let's check how many missing values there are for each column.","20d6cf7b":"Let's create the model using our grid search results as the hyperparameters. We'll then fit the model with the training data and test it out on the test set.","d941cb31":"We've achieved roughly ~81% accuracy using our random forest model, which is a 2% bump from our previous Logistic Regression model. Let's check the confusion matrix as well.","c5cb00dc":"# 3 - Modeling\nLet's start the modeling process. We'll go for the primary model of choice, the Logistic Regression model.\n\n## 3.1 - Logistic Regression\nLogistic Regression is a common choice for binary classification. It is also much more simple and interpretable when compared to most models. First, we'll split our dataset into test and train. Then we'll add a standard scaler to standardize our data so our model would converge faster. We'll then use grid search to find the optimal hyperparameters for our model.","a4881303":"We can see that passengers who belonged to Class 3 were less likely to survive than Class 1 and 2 passengers. Class 1 passengers had the highest chance of survival as compared to the other two classes. This may indicate that Class 1 passengers were given more priority in rescue operations than Class 3 passengers. The Chi-square P-value also tells us that the Null Hypothesis(knowing the class variable does not help in predicting survival) is rejected.\n\nLet's now take a look at the Embarked variable which tells us where the port of embarkation is for the passengers."}}