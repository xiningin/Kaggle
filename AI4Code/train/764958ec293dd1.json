{"cell_type":{"dd20d782":"code","5037227b":"code","0ca8a763":"code","1ecc38a0":"code","4f4eee60":"code","416a588d":"code","a9f9017c":"code","d6672e8f":"code","4841d3b7":"code","a21e3fec":"code","b7c3aa04":"code","65022519":"code","5b38cc68":"code","aa55566a":"code","fdb7293e":"code","a0dccc08":"code","2b870980":"code","fdf56ec1":"code","f4462ef8":"code","7d6307f6":"code","efa877d0":"code","0e8d268d":"code","60353d4b":"code","9bb69c03":"code","b6a9f6c2":"code","f415a0f5":"code","a24aa08f":"code","e9b732ab":"code","ff0d6a2d":"code","b3a15423":"code","116a8fa6":"code","6520f45e":"code","22a3de8f":"code","1b572192":"code","88c3acbe":"code","ca077d58":"code","48967e54":"code","9632df04":"code","bd92e4b3":"code","d31ee1a4":"code","82613258":"code","840adc4f":"code","1805a85e":"code","03312470":"code","2843f0d4":"code","7497d227":"code","48873ac2":"code","f2e3d9c0":"code","6ddac26e":"code","4c398992":"code","235838d1":"code","bcb0ebaf":"code","f257da62":"code","47c20a32":"code","d40749f1":"code","561fabbb":"markdown","a98f356b":"markdown","83d84377":"markdown","603eef6d":"markdown"},"source":{"dd20d782":"! pip install jieba","5037227b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ca8a763":"df = pd.read_csv('\/kaggle\/input\/baidu-qianyan-text-sim-lcqmc-train\/train.tsv', names = ['q1', 'q2', 'label'], sep = '\\t')\ndf.head(2)","1ecc38a0":"print (df.isna().sum())\nprint (df.groupby('label')['label'].count())","4f4eee60":"import requests\nimport os \nimport hashlib\nimport torch\nfrom torch import nn\nimport math\nimport sys\nimport random\nimport collections\nimport zipfile\nimport time\nimport multiprocessing\nimport jieba\nfrom matplotlib import pyplot as plt\nfrom IPython import display","416a588d":"def get_tokens_and_segments(tokens_a, tokens_b=None):\n    \"\"\"\u83b7\u53d6\u8f93\u5165\u5e8f\u5217\u7684\u8bcd\u5143\u53ca\u5176\u7247\u6bb5\u7d22\u5f15\u3002\"\"\"\n    tokens = ['<cls>'] + tokens_a + ['<sep>']\n    # 0\u548c1\u5206\u522b\u6807\u8bb0\u7247\u6bb5A\u548cB\n    segments = [0] * (len(tokens_a) + 2)\n    if tokens_b is not None:\n        tokens += tokens_b + ['<sep>']\n        segments += [1] * (len(tokens_b) + 1)\n    return tokens, segments","a9f9017c":"def transpose_output(X, num_heads):\n    \"\"\"Reverse the operation of `transpose_qkv`.\"\"\"\n    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.permute(0, 2, 1, 3)\n    return X.reshape(X.shape[0], X.shape[1], -1)\n\ndef masked_softmax(X, valid_lens):\n    \"\"\"\u901a\u8fc7\u5728\u6700\u540e\u4e00\u4e2a\u8f74\u4e0a\u906e\u853d\u5143\u7d20\u6765\u6267\u884c softmax \u64cd\u4f5c\"\"\"\n    # `X`: 3D\u5f20\u91cf, `valid_lens`: 1D\u62162D \u5f20\u91cf\n    # print (\"X.shape: {} valid_lens: {}\".format(X.shape, type(valid_lens)))\n#     if valid_lens is None:\n#         print ('valid_lens is None')\n#     else:\n#         print ('valid_lens shape: {}'.format(valid_lens.shape))\n    if valid_lens is None:\n        return nn.functional.softmax(X, dim=-1)\n    else:\n        shape = X.shape\n        if valid_lens.dim() == 1:\n            valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n        else:\n            valid_lens = valid_lens.reshape(-1)\n        # \u5728\u6700\u540e\u7684\u8f74\u4e0a\uff0c\u88ab\u906e\u853d\u7684\u5143\u7d20\u4f7f\u7528\u4e00\u4e2a\u975e\u5e38\u5927\u7684\u8d1f\u503c\u66ff\u6362\uff0c\u4ece\u800c\u5176 softmax (\u6307\u6570)\u8f93\u51fa\u4e3a 0\n        X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n                              value=-1e6)\n        return nn.functional.softmax(X.reshape(shape), dim=-1)\n\ndef transpose_qkv(X, num_heads):\n    \"\"\"Transposition for parallel computation of multiple attention heads.\"\"\"\n    # Shape of input `X`:\n    # (`batch_size`, no. of queries or key-value pairs, `num_hiddens`).\n    # Shape of output `X`:\n    # (`batch_size`, no. of queries or key-value pairs, `num_heads`,\n    # `num_hiddens` \/ `num_heads`)\n    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n\n    # Shape of output `X`:\n    # (`batch_size`, `num_heads`, no. of queries or key-value pairs,\n    # `num_hiddens` \/ `num_heads`)\n    X = X.permute(0, 2, 1, 3)\n\n    # Shape of `output`:\n    # (`batch_size` * `num_heads`, no. of queries or key-value pairs,\n    # `num_hiddens` \/ `num_heads`)\n    return X.reshape(-1, X.shape[2], X.shape[3])","d6672e8f":"class PositionWiseFFN(nn.Module):\n    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,\n                 **kwargs):\n        super(PositionWiseFFN, self).__init__(**kwargs)\n        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n        self.relu = nn.ReLU()\n        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n\n    def forward(self, X):\n        return self.dense2(self.relu(self.dense1(X)))\n\nclass AddNorm(nn.Module):\n    def __init__(self, normalized_shape, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.ln = nn.LayerNorm(normalized_shape)\n\n    def forward(self, X, Y):\n        return self.ln(self.dropout(Y) + X)\n\nclass DotProductAttention(nn.Module):\n    \"\"\"\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\"\"\"\n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # `queries` \u7684\u5f62\u72b6\uff1a(`batch_size`, \u67e5\u8be2\u7684\u4e2a\u6570, `d`)\n    # `keys` \u7684\u5f62\u72b6\uff1a(`batch_size`, \u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570, `d`)\n    # `values` \u7684\u5f62\u72b6\uff1a(`batch_size`, \u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570, \u503c\u7684\u7ef4\u5ea6)\n    # `valid_lens` \u7684\u5f62\u72b6: (`batch_size`,) \u6216\u8005 (`batch_size`, \u67e5\u8be2\u7684\u4e2a\u6570)\n    def forward(self, queries, keys, values, valid_lens=None):\n        d = queries.shape[-1]\n        # \u8bbe\u7f6e `transpose_b=True` \u4e3a\u4e86\u4ea4\u6362 `keys` \u7684\u6700\u540e\u4e24\u4e2a\u7ef4\u5ea6\n        # print ('DotProductAttention shape: {}\\t{}\\t{}\\t\\n\\n'.format(queries.shape, keys.shape, values.shape))\n        scores = torch.bmm(queries, keys.transpose(1,2)) \/ math.sqrt(d)\n#         if valid_lens is not None:\n#             print ('score: {} valid_lens: {}'.format(scores.shape, valid_lens.shape))\n        self.attention_weights = masked_softmax(scores, valid_lens)\n        # print ('weight: {}\\n\\n'.format(self.attention_weights.shape))\n        return torch.bmm(self.dropout(self.attention_weights), values)\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 num_heads, dropout, bias=False, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = DotProductAttention(dropout)\n        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n\n    def forward(self, queries, keys, values, valid_lens):\n        # `queries`, `keys`, or `values` \u7684\u5f62\u72b6:\n        # (`batch_size`, \u67e5\u8be2\u6216\u8005\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570, `num_hiddens`)\n        # `valid_lens`\u3000\u7684\u5f62\u72b6:\n        # (`batch_size`,) or (`batch_size`, \u67e5\u8be2\u7684\u4e2a\u6570)\n        # \u7ecf\u8fc7\u53d8\u6362\u540e\uff0c\u8f93\u51fa\u7684 `queries`, `keys`, or `values`\u3000\u7684\u5f62\u72b6:\n        # (`batch_size` * `num_heads`, \u67e5\u8be2\u6216\u8005\u201c\u952e\uff0d\u503c\u201d\u5bf9\u7684\u4e2a\u6570,\n        # `num_hiddens` \/ `num_heads`)\n        # print ('MultiHeadAttention shape: {}\\t{}\\t{}\\t'.format(queries.shape, keys.shape, values.shape))\n        # print ('MultiHeadAttention shape: {}\\t{}\\t{}\\t'.format(self.W_q(queries).shape, self.W_k(keys).shape, self.W_v(values).shape))\n        # print ('self.num_heads: {}'.format(self.num_heads))\n        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n        values = transpose_qkv(self.W_v(values), self.num_heads)\n        # print ('MultiHeadAttention shape: {}\\t{}\\t{}\\t'.format(queries.shape, keys.shape, values.shape))\n#         if valid_lens is not None:\n#             print (\"valid_lens shape: {}\".format(valid_lens.shape))\n        if valid_lens is not None:\n            # \u5728\u8f74 0\uff0c\u5c06\u7b2c\u4e00\u9879\uff08\u6807\u91cf\u6216\u8005\u77e2\u91cf\uff09\u590d\u5236 `num_heads` \u6b21\uff0c\n            # \u7136\u540e\u5982\u6b64\u590d\u5236\u7b2c\u4e8c\u9879\uff0c\u7136\u540e\u8bf8\u5982\u6b64\u7c7b\u3002\n            valid_lens = torch.repeat_interleave(\n                valid_lens, repeats=self.num_heads, dim=0)\n\n        # `output` \u7684\u5f62\u72b6: (`batch_size` * `num_heads`, \u67e5\u8be2\u7684\u4e2a\u6570,\n        # `num_hiddens` \/ `num_heads`)\n#         if valid_lens is not None:\n#             print (\"valid_lens shape: {}\".format(valid_lens.shape))\n        output = self.attention(queries, keys, values, valid_lens)\n\n        # `output_concat` \u7684\u5f62\u72b6: (`batch_size`, \u67e5\u8be2\u7684\u4e2a\u6570, `num_hiddens`)\n        output_concat = transpose_output(output, self.num_heads)\n        return self.W_o(output_concat)\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, key_size, query_size, value_size, num_hiddens,\n                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n                 dropout, use_bias=False, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = MultiHeadAttention(\n            key_size, query_size, value_size, num_hiddens, num_heads, dropout,\n            use_bias)\n        self.addnorm1 = AddNorm(norm_shape, dropout)\n        self.ffn = PositionWiseFFN(\n            ffn_num_input, ffn_num_hiddens, num_hiddens)\n        self.addnorm2 = AddNorm(norm_shape, dropout)\n\n    def forward(self, X, valid_lens):\n        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n        return self.addnorm2(Y, self.ffn(Y))\n\nclass BERTEncoder(nn.Module):\n    \"\"\"BERT encoder.\"\"\"\n    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\n                 **kwargs):\n        super(BERTEncoder, self).__init__(**kwargs)\n        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n        self.segment_embedding = nn.Embedding(2, num_hiddens)\n        self.blks = nn.Sequential()\n        for i in range(num_layers):\n            self.blks.add_module(f\"{i}\", EncoderBlock(\n                key_size, query_size, value_size, num_hiddens, norm_shape,\n                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n        # \u5728BERT\u4e2d\uff0c\u4f4d\u7f6e\u5d4c\u5165\u662f\u53ef\u5b66\u4e60\u7684\uff0c\u56e0\u6b64\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u8db3\u591f\u957f\u7684\u4f4d\u7f6e\u5d4c\u5165\u53c2\u6570\n        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n                                                      num_hiddens))\n\n    def forward(self, tokens, segments, valid_lens):\n        # \u5728\u4ee5\u4e0b\u4ee3\u7801\u6bb5\u4e2d\uff0c`X`\u7684\u5f62\u72b6\u4fdd\u6301\u4e0d\u53d8\uff1a\uff08\u6279\u91cf\u5927\u5c0f\uff0c\u6700\u5927\u5e8f\u5217\u957f\u5ea6\uff0c`num_hiddens`\uff09\n        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n        for blk in self.blks:\n            X = blk(X, valid_lens)\n        return X","4841d3b7":"vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\nnorm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\nencoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                      ffn_num_hiddens, num_heads, num_layers, dropout)\n\ntokens = torch.randint(0, vocab_size, (2, 8))\nsegments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\nencoded_X = encoder(tokens, segments, None)\nencoded_X.shape","a21e3fec":"class MaskLM(nn.Module):\n    \"\"\"BERT\u7684\u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\"\"\"\n    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n        super(MaskLM, self).__init__(**kwargs)\n        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n                                 nn.ReLU(),\n                                 nn.LayerNorm(num_hiddens),\n                                 nn.Linear(num_hiddens, vocab_size))\n\n    def forward(self, X, pred_positions):\n        num_pred_positions = pred_positions.shape[1]\n        pred_positions = pred_positions.reshape(-1)\n        batch_size = X.shape[0]\n        batch_idx = torch.arange(0, batch_size)\n        # \u5047\u8bbe`batch_size=2\uff0c`num_pred_positions`=3\n        # \u90a3\u4e48`batch_idx`\u662f`np.array\uff08[0,0,0,1,1]\uff09`\n        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n        masked_X = X[batch_idx, pred_positions]\n        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n        mlm_Y_hat = self.mlp(masked_X)\n        return mlm_Y_hat","b7c3aa04":"mlm = MaskLM(vocab_size, num_hiddens)\nmlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\nmlm_Y_hat = mlm(encoded_X, mlm_positions)\nmlm_Y_hat.shape","65022519":"mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\nloss = nn.CrossEntropyLoss(reduction='none')\nmlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\nmlm_l.shape","5b38cc68":"class NextSentencePred(nn.Module):\n    \"\"\"BERT\u7684\u4e0b\u4e00\u53e5\u9884\u6d4b\u4efb\u52a1\"\"\"\n    def __init__(self, num_inputs, **kwargs):\n        super(NextSentencePred, self).__init__(**kwargs)\n        self.output = nn.Linear(num_inputs, 2)\n\n    def forward(self, X):\n        # `X`\u7684\u5f62\u72b6\uff1a (batch size, `num_hiddens`)\n        return self.output(X)","aa55566a":"# \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0cPyTorch\u4e0d\u4f1a\u50cfmxnet\u4e2d\u90a3\u6837\u5c55\u5e73\u5f20\u91cf\n# \u5982\u679cflatten=True\uff0c\u5219\u9664\u7b2c\u4e00\u4e2a\u8f93\u5165\u6570\u636e\u8f74\u5916\uff0c\u6240\u6709\u8f93\u5165\u6570\u636e\u8f74\u90fd\u6298\u53e0\u5728\u4e00\u8d77\nencoded_X = torch.flatten(encoded_X, start_dim=1)\n# NSP\u7684\u8f93\u5165\u5f62\u72b6: (batch size, `num_hiddens`)\nnsp = NextSentencePred(encoded_X.shape[-1])\nnsp_Y_hat = nsp(encoded_X)\nnsp_Y_hat.shape","fdb7293e":"nsp_y = torch.tensor([0, 1])\nnsp_l = loss(nsp_Y_hat, nsp_y)\nnsp_l.shape","a0dccc08":"class BERTModel(nn.Module):\n    \"\"\"BERT\u6a21\u578b\"\"\"\n    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n                 ffn_num_hiddens, num_heads, num_layers, dropout,\n                 max_len=1000, key_size=768, query_size=768, value_size=768,\n                 hid_in_features=768, mlm_in_features=768,\n                 nsp_in_features=768):\n        super(BERTModel, self).__init__()\n        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n                    dropout, max_len=max_len, key_size=key_size,\n                    query_size=query_size, value_size=value_size)\n        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n                                    nn.Tanh())\n        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n        self.nsp = NextSentencePred(nsp_in_features)\n\n    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n        encoded_X = self.encoder(tokens, segments, valid_lens)\n        if pred_positions is not None:\n            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n        else:\n            mlm_Y_hat = None\n        # \u7528\u4e8e\u4e0b\u4e00\u53e5\u9884\u6d4b\u7684\u591a\u5c42\u611f\u77e5\u673a\u5206\u7c7b\u5668\u7684\u9690\u85cf\u5c42\u30020\u662f\u201c<cls>\u201d\u6807\u8bb0\u7684\u7d22\u5f15\u3002\n        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n        return encoded_X, mlm_Y_hat, nsp_Y_hat","2b870980":"def download(name, cache_dir=os.path.join('\/kaggle\/working', 'data')):\n    \"\"\"\u4e0b\u8f7d\u4e00\u4e2aDATA_HUB\u4e2d\u7684\u6587\u4ef6\uff0c\u8fd4\u56de\u672c\u5730\u6587\u4ef6\u540d\u3002\n    Defined in :numref:`sec_kaggle_house`\"\"\"\n    assert name in DATA_HUB, f\"{name} \u4e0d\u5b58\u5728\u4e8e {DATA_HUB}.\"\n    # print (\"download name: {} cache_dir: {}\".format(name, cache_dir))\n    url, sha1_hash = DATA_HUB[name]\n    os.makedirs(cache_dir, exist_ok=True)\n    fname = os.path.join(cache_dir, url.split('\/')[-1])\n    # fname = '\/kaggle\/working\/wikitext'\n    if os.path.exists(fname):\n        sha1 = hashlib.sha1()\n        with open(fname, 'rb') as f:\n            while True:\n                data = f.read(1048576)\n                if not data:\n                    break\n                sha1.update(data)\n        if sha1.hexdigest() == sha1_hash:\n            return fname  # Hit cache\n    print(f'\u6b63\u5728\u4ece{url}\u4e0b\u8f7d{fname}...')\n    r = requests.get(url, stream=True, verify=True)\n    with open(fname, 'wb') as f:\n        f.write(r.content)\n    return fname","fdf56ec1":"#@save\nDATA_HUB = dict()\nDATA_HUB['wikitext-2'] = (\n    'https:\/\/s3.amazonaws.com\/research.metamind.io\/wikitext\/'\n    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n\n#@save\ndef _read_wiki_demo(data_dir):\n    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n    with open(file_name, 'r') as f:\n        lines = f.readlines()\n    # \u5927\u5199\u5b57\u6bcd\u8f6c\u6362\u4e3a\u5c0f\u5199\u5b57\u6bcd\n    paragraphs = [line.strip().lower().split(' . ')\n                  for line in lines if len(line.split(' . ')) >= 2]\n    random.shuffle(paragraphs)\n    return paragraphs","f4462ef8":"def _read_wiki(data_dir):\n    file_name = '\/kaggle\/input\/baidu-qianyan-text-sim-lcqmc-train\/train.tsv'\n    with open(file_name, 'r') as f:\n        lines = f.readlines()\n    # \u5927\u5199\u5b57\u6bcd\u8f6c\u6362\u4e3a\u5c0f\u5199\u5b57\u6bcd\n    paragraphs = [line.strip().lower().split('\\t')[: 2]\n                  for line in lines if len(line.split('\\t')) >= 3]\n    random.shuffle(paragraphs)\n    return paragraphs","7d6307f6":"# paragraphs = _read_wiki('wikitext-2\/wikitext-2')\n# print (len(paragraphs))\n# print (len(paragraphs[0]), len(paragraphs[20]))\n# print (paragraphs[0], paragraphs[20])\n# print (paragraphs[0][0], '\\n', paragraphs[20][0])\n# print (type(paragraphs[0][0]), '\\n', type(paragraphs[20][0]))","efa877d0":"def _get_next_sentence(sentence, next_sentence, paragraphs):\n    if random.random() < 0.5:\n        is_next = True\n    else:\n        # `paragraphs`\u662f\u4e09\u91cd\u5217\u8868\u7684\u5d4c\u5957\n        next_sentence = random.choice(random.choice(paragraphs))\n        is_next = False\n    return sentence, next_sentence, is_next","0e8d268d":"def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n    nsp_data_from_paragraph = []\n    for i in range(len(paragraph) - 1):\n        tokens_a, tokens_b, is_next = _get_next_sentence(\n            paragraph[i], paragraph[i + 1], paragraphs)\n        # \u8003\u86511\u4e2a'<cls>'\u8bcd\u5143\u548c2\u4e2a'<sep>'\u8bcd\u5143\n        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n            continue\n        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n        nsp_data_from_paragraph.append((tokens, segments, is_next))\n    return nsp_data_from_paragraph","60353d4b":"def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n                        vocab):\n    # \u4e3a\u906e\u853d\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u521b\u5efa\u65b0\u7684\u8bcd\u5143\u526f\u672c\uff0c\u5176\u4e2d\u8f93\u5165\u53ef\u80fd\u5305\u542b\u66ff\u6362\u7684\u201c<mask>\u201d\u6216\u968f\u673a\u8bcd\u5143\n    mlm_input_tokens = [token for token in tokens]\n    pred_positions_and_labels = []\n    # \u6253\u4e71\u540e\u7528\u4e8e\u5728\u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u83b7\u53d615%\u7684\u968f\u673a\u8bcd\u5143\u8fdb\u884c\u9884\u6d4b\n    random.shuffle(candidate_pred_positions)\n    for mlm_pred_position in candidate_pred_positions:\n        if len(pred_positions_and_labels) >= num_mlm_preds:\n            break\n        masked_token = None\n        # 80%\u7684\u65f6\u95f4\uff1a\u5c06\u8bcd\u66ff\u6362\u4e3a\u201c<mask>\u201d\u8bcd\u5143\n        if random.random() < 0.8:\n            masked_token = '<mask>'\n        else:\n            # 10%\u7684\u65f6\u95f4\uff1a\u4fdd\u6301\u8bcd\u4e0d\u53d8\n            if random.random() < 0.5:\n                masked_token = tokens[mlm_pred_position]\n            # 10%\u7684\u65f6\u95f4\uff1a\u7528\u968f\u673a\u8bcd\u66ff\u6362\u8be5\u8bcd\n            else:\n                masked_token = random.randint(0, len(vocab) - 1)\n        mlm_input_tokens[mlm_pred_position] = masked_token\n        pred_positions_and_labels.append(\n            (mlm_pred_position, tokens[mlm_pred_position]))\n    return mlm_input_tokens, pred_positions_and_labels","9bb69c03":"def _get_mlm_data_from_tokens(tokens, vocab):\n    candidate_pred_positions = []\n    # `tokens`\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\n    for i, token in enumerate(tokens):\n        # \u5728\u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u4e0d\u4f1a\u9884\u6d4b\u7279\u6b8a\u8bcd\u5143\n        if token in ['<cls>', '<sep>']:\n            continue\n        candidate_pred_positions.append(i)\n    # \u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u9884\u6d4b15%\u7684\u968f\u673a\u8bcd\u5143\n    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n    pred_positions_and_labels = sorted(pred_positions_and_labels,\n                                       key=lambda x: x[0])\n    pred_positions = [v[0] for v in pred_positions_and_labels]\n    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]","b6a9f6c2":"def download_extract(embedding_name, pretrained_file_name, folder=\"wikitext-2\"):\n    \"\"\"Download and extract a zip\/tar file.\"\"\"\n    print (\"download_extract ing...\")\n    fname = download(embedding_name, pretrained_file_name)\n    print ('download done fname: {:}'.format(fname))\n    base_dir = os.path.dirname(fname) \n    data_dir, ext = os.path.splitext(fname)\n    print ('data_dir: {}'.format(data_dir))\n    if ext == '.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    elif ext in ('.tar', '.gz'):\n        fp = tarfile.open(fname, 'r')\n    else:\n        assert False, 'Only zip\/tar files can be extracted'\n    fp.extractall(base_dir)\n    # print ('base_dir: {} data_dir: {}'.format(base_dir, data_dir))\n    if folder:\n        return os.path.join(base_dir, folder)\n    else:\n        return data_dir","f415a0f5":"def _get_mlm_data_from_tokens(tokens, vocab):\n    candidate_pred_positions = []\n    # `tokens`\u662f\u4e00\u4e2a\u5b57\u7b26\u4e32\u5217\u8868\n    for i, token in enumerate(tokens):\n        # \u5728\u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u4e0d\u4f1a\u9884\u6d4b\u7279\u6b8a\u8bcd\u5143\n        if token in ['<cls>', '<sep>']:\n            continue\n        candidate_pred_positions.append(i)\n    # \u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e2d\u9884\u6d4b15%\u7684\u968f\u673a\u8bcd\u5143\n    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n    pred_positions_and_labels = sorted(pred_positions_and_labels,\n                                       key=lambda x: x[0])\n    pred_positions = [v[0] for v in pred_positions_and_labels]\n    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n\ndef _pad_bert_inputs(examples, max_len, vocab):\n    max_num_mlm_preds = round(max_len * 0.15)\n    all_token_ids, all_segments, valid_lens,  = [], [], []\n    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n    nsp_labels = []\n    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n         is_next) in examples:\n        all_token_ids.append(torch.tensor(token_ids + [vocab['<pad>']] * (\n            max_len - len(token_ids)), dtype=torch.long))\n        all_segments.append(torch.tensor(segments + [0] * (\n            max_len - len(segments)), dtype=torch.long))\n        # `valid_lens` \u4e0d\u5305\u62ec'<pad>'\u7684\u8ba1\u6570\n        valid_lens.append(torch.tensor(len(token_ids), dtype=torch.float32))\n        all_pred_positions.append(torch.tensor(pred_positions + [0] * (\n            max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n        # \u586b\u5145\u8bcd\u5143\u7684\u9884\u6d4b\u5c06\u901a\u8fc7\u4e58\u4ee50\u6743\u91cd\u5728\u635f\u5931\u4e2d\u8fc7\u6ee4\u6389\n        all_mlm_weights.append(\n            torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n                max_num_mlm_preds - len(pred_positions)),\n                dtype=torch.float32))\n        all_mlm_labels.append(torch.tensor(mlm_pred_label_ids + [0] * (\n            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n            all_mlm_weights, all_mlm_labels, nsp_labels)\nclass _WikiTextDataset(torch.utils.data.Dataset):\n    def __init__(self, paragraphs, max_len):\n        # \u8f93\u5165`paragraphs[i]`\u662f\u4ee3\u8868\u6bb5\u843d\u7684\u53e5\u5b50\u5b57\u7b26\u4e32\u5217\u8868\uff1b\u800c\u8f93\u51fa`paragraphs[i]`\u662f\u4ee3\u8868\u6bb5\u843d\u7684\u53e5\u5b50\u5217\u8868\uff0c\u5176\u4e2d\u6bcf\u4e2a\u53e5\u5b50\u90fd\u662f\u8bcd\u5143\u5217\u8868\n        # print (type(paragraphs[0]), type(paragraphs[0]), paragraphs[0], type(paragraphs[0][0]))\n        paragraphs = [tokenize(\n            paragraph, token='ch') for paragraph in paragraphs]\n        # print (type(paragraphs[0]), type(paragraphs[0]), paragraphs[0], type(paragraphs[0][0]), '\\n\\n')\n        sentences = [sentence for paragraph in paragraphs\n                     for sentence in paragraph]\n        self.vocab = Vocab(sentences, min_freq=5, reserved_tokens=[\n            '<pad>', '<mask>', '<cls>', '<sep>'])\n        # \u83b7\u53d6\u4e0b\u4e00\u53e5\u5b50\u9884\u6d4b\u4efb\u52a1\u7684\u6570\u636e\n        examples = []\n        for paragraph in paragraphs:\n            examples.extend(_get_nsp_data_from_paragraph(\n                paragraph, paragraphs, self.vocab, max_len))\n        # \u83b7\u53d6\u906e\u853d\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u7684\u6570\u636e\n        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n                      + (segments, is_next))\n                     for tokens, segments, is_next in examples]\n        # \u586b\u5145\u8f93\u5165\n        (self.all_token_ids, self.all_segments, self.valid_lens,\n         self.all_pred_positions, self.all_mlm_weights,\n         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n            examples, max_len, self.vocab)\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx], self.all_pred_positions[idx],\n                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n                self.nsp_labels[idx])\n\n    def __len__(self):\n        return len(self.all_token_ids)","a24aa08f":"data_dir = 'wikitext-2\/wikitext-2'\nparagraphs = _read_wiki(data_dir)","e9b732ab":"print (type(paragraphs[0]), type(paragraphs[0]), paragraphs[0], type(paragraphs[0][0]))","ff0d6a2d":"# batch_size, max_len = 512, 64\n# train_set = _WikiTextDataset(paragraphs, max_len)","b3a15423":"def get_dataloader_workers():  #@save\n    \"\"\"\u4f7f\u75284\u4e2a\u8fdb\u7a0b\u6765\u8bfb\u53d6\u6570\u636e\u3002\"\"\"\n    return 4\n\ndef load_data_wiki(batch_size, max_len):\n    \"\"\"\u52a0\u8f7dWikiText-2\u6570\u636e\u96c6\u3002\"\"\"\n    num_workers = get_dataloader_workers()\n    data_dir = download_extract('wikitext-2', 'wikitext-2')\n    paragraphs = _read_wiki(data_dir)\n    print ('paragraphs', len(paragraphs), type(paragraphs))\n    train_set = _WikiTextDataset(paragraphs, max_len)\n    print (\"train_set: \", len(train_set), batch_size)\n    train_iter = torch.utils.data.DataLoader(train_set, batch_size,\n                                        shuffle=True, num_workers=num_workers)\n    return train_iter, train_set.vocab","116a8fa6":"def tokenize(lines, token='ch'):  #@save\n    \"\"\"\u5c06\u6587\u672c\u884c\u62c6\u5206\u4e3a\u5355\u8bcd\u6216\u5b57\u7b26\u8bcd\u5143\u3002\"\"\"\n    if isinstance(lines, list):\n        # print ('lis')\n        return [jieba.lcut(line) for line in lines]\n    elif isinstance(lines, str):\n        # print ('str')\n        return jieba.lcut(lines)\n    else:\n        print ('error')","6520f45e":"def count_corpus(tokens):\n    \"\"\"\u7edf\u8ba1\u8bcd\u5143\u7684\u9891\u7387\u3002\n    Defined in :numref:`sec_text_preprocessing`\"\"\"\n    # \u8fd9\u91cc\u7684 `tokens` \u662f 1D \u5217\u8868\u6216 2D \u5217\u8868\n    if len(tokens) == 0 or isinstance(tokens[0], list):\n        # \u5c06\u8bcd\u5143\u5217\u8868\u5c55\u5e73\u6210\u4f7f\u7528\u8bcd\u5143\u586b\u5145\u7684\u4e00\u4e2a\u5217\u8868\n        tokens = [token for line in tokens for token in line]\n    return collections.Counter(tokens)","22a3de8f":"class Vocab:\n    \"\"\"\u6587\u672c\u8bcd\u6c47\u8868\"\"\"\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        \"\"\"Defined in :numref:`sec_text_preprocessing`\"\"\"\n        if tokens is None:\n            tokens = []\n        if reserved_tokens is None:\n            reserved_tokens = []\n        # \u6309\u51fa\u73b0\u9891\u7387\u6392\u5e8f\n        counter = count_corpus(tokens)\n        self.token_freqs = sorted(counter.items(), key=lambda x: x[1],\n                                  reverse=True)\n        # \u672a\u77e5\u8bcd\u5143\u7684\u7d22\u5f15\u4e3a0\n        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n        uniq_tokens += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in uniq_tokens]\n        self.idx_to_token, self.token_to_idx = [], dict()\n        for token in uniq_tokens:\n            self.idx_to_token.append(token)\n            self.token_to_idx[token] = len(self.idx_to_token) - 1\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]","1b572192":"batch_size, max_len = 512, 64\ntrain_iter, vocab = load_data_wiki(batch_size, max_len)\n\nfor (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n     mlm_Y, nsp_y) in train_iter:\n    # print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n#           pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n#           nsp_y.shape)\n    break","88c3acbe":"print (torch.arange((10), dtype=torch.float32)[None, :])\nprint (torch.arange((10), dtype=torch.float32)[: , None])\nprint (torch.arange((10), dtype=torch.float32))\nvalid_len_tmp = torch.full((10, 1), 5)\nprint (valid_len_tmp)\nprint (torch.arange((10), dtype=torch.float32)[None, :] < valid_len_tmp[:, None])","ca077d58":"def try_all_gpus():  #@save\n    \"\"\"\u8fd4\u56de\u6240\u6709\u53ef\u7528\u7684GPU\uff0c\u5982\u679c\u6ca1\u6709GPU\uff0c\u5219\u8fd4\u56de[cpu(),]\u3002\"\"\"\n    devices = [torch.device(f'cuda:{i}')\n             for i in range(torch.cuda.device_count())]\n    return devices if devices else [torch.device('cpu')]\n\nclass Accumulator:\n    \"\"\"\u5728`n`\u4e2a\u53d8\u91cf\u4e0a\u7d2f\u52a0\u3002\"\"\"\n    def __init__(self, n):\n        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n        self.data = [0.0] * n\n\n    def add(self, *args):\n        self.data = [a + float(b) for a, b in zip(self.data, args)]\n\n    def reset(self):\n        self.data = [0.0] * len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\nclass Timer:\n    \"\"\"\u8bb0\u5f55\u591a\u6b21\u8fd0\u884c\u65f6\u95f4\u3002\"\"\"\n    def __init__(self):\n        \"\"\"Defined in :numref:`subsec_linear_model`\"\"\"\n        self.times = []\n        self.start()\n\n    def start(self):\n        \"\"\"\u542f\u52a8\u8ba1\u65f6\u5668\u3002\"\"\"\n        self.tik = time.time()\n\n    def stop(self):\n        \"\"\"\u505c\u6b62\u8ba1\u65f6\u5668\u5e76\u5c06\u65f6\u95f4\u8bb0\u5f55\u5728\u5217\u8868\u4e2d\u3002\"\"\"\n        self.times.append(time.time() - self.tik)\n        return self.times[-1]\n\n    def avg(self):\n        \"\"\"\u8fd4\u56de\u5e73\u5747\u65f6\u95f4\u3002\"\"\"\n        return sum(self.times) \/ len(self.times)\n\n    def sum(self):\n        \"\"\"\u8fd4\u56de\u65f6\u95f4\u603b\u548c\u3002\"\"\"\n        return sum(self.times)\n\n    def cumsum(self):\n        \"\"\"\u8fd4\u56de\u7d2f\u8ba1\u65f6\u95f4\u3002\"\"\"\n        return np.array(self.times).cumsum().tolist()\n    \ndef use_svg_display():\n    \"\"\"\u4f7f\u7528svg\u683c\u5f0f\u5728Jupyter\u4e2d\u663e\u793a\u7ed8\u56fe\u3002\n    Defined in :numref:`sec_calculus`\"\"\"\n    display.set_matplotlib_formats('svg')\n\ndef set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n    \"\"\"\u8bbe\u7f6ematplotlib\u7684\u8f74\u3002\n    Defined in :numref:`sec_calculus`\"\"\"\n    axes.set_xlabel(xlabel)\n    axes.set_ylabel(ylabel)\n    axes.set_xscale(xscale)\n    axes.set_yscale(yscale)\n    axes.set_xlim(xlim)\n    axes.set_ylim(ylim)\n    if legend:\n        axes.legend(legend)\n    axes.grid()\n    \nclass Animator:\n    \"\"\"\u5728\u52a8\u753b\u4e2d\u7ed8\u5236\u6570\u636e\u3002\"\"\"\n    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n                 ylim=None, xscale='linear', yscale='linear',\n                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n                 figsize=(3.5, 2.5)):\n        \"\"\"Defined in :numref:`sec_softmax_scratch`\"\"\"\n        # \u589e\u91cf\u5730\u7ed8\u5236\u591a\u6761\u7ebf\n        if legend is None:\n            legend = []\n        use_svg_display()\n        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n        if nrows * ncols == 1:\n            self.axes = [self.axes, ]\n        # \u4f7f\u7528lambda\u51fd\u6570\u6355\u83b7\u53c2\u6570\n        self.config_axes = lambda: set_axes(\n            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n        self.X, self.Y, self.fmts = None, None, fmts\n\n    def add(self, x, y):\n        # \u5411\u56fe\u8868\u4e2d\u6dfb\u52a0\u591a\u4e2a\u6570\u636e\u70b9\n        if not hasattr(y, \"__len__\"):\n            y = [y]\n        n = len(y)\n        if not hasattr(x, \"__len__\"):\n            x = [x] * n\n        if not self.X:\n            self.X = [[] for _ in range(n)]\n        if not self.Y:\n            self.Y = [[] for _ in range(n)]\n        for i, (a, b) in enumerate(zip(x, y)):\n            if a is not None and b is not None:\n                self.X[i].append(a)\n                self.Y[i].append(b)\n        self.axes[0].cla()\n        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n            self.axes[0].plot(x, y, fmt)\n        self.config_axes()\n        display.display(self.fig)\n        display.clear_output(wait=True)\n\ndef sequence_mask(X, valid_len, value=0):\n    \"\"\"Mask irrelevant entries in sequences.\"\"\"\n    maxlen = X.size(1)\n    # print ('sequence_mask: {} {} {}'.format(X.shape, valid_len.shape, value))\n    # print (valid_len[: 20], valid_len[-20: ])\n    mask = torch.arange((maxlen), dtype=torch.float32,\n                        device=X.device)[None, :] < valid_len[:, None]\n#     print (mask[0, : 10])\n#     print (mask[5, : 10])\n#     print ('mask: {}'.format(mask.shape))\n    X[~mask] = value\n    return X","48967e54":"batch_size, max_len = 512, 64\ntrain_iter, vocab = load_data_wiki(batch_size, max_len)","9632df04":"net = BERTModel(len(vocab), num_hiddens=128, norm_shape=[128],\n                    ffn_num_input=128, ffn_num_hiddens=256, num_heads=2,\n                    num_layers=2, dropout=0.2, key_size=128, query_size=128,\n                    value_size=128, hid_in_features=128, mlm_in_features=128,\n                    nsp_in_features=128)\ndevices = try_all_gpus()\nloss = nn.CrossEntropyLoss()","bd92e4b3":"def _get_batch_loss_bert(net, loss, vocab_size, tokens_X,\n                         segments_X, valid_lens_x,\n                         pred_positions_X, mlm_weights_X,\n                         mlm_Y, nsp_y):\n    # \u524d\u5411\u4f20\u64ad\n    _, mlm_Y_hat, nsp_Y_hat = net(tokens_X, segments_X,\n                                  valid_lens_x.reshape(-1),\n                                  pred_positions_X)\n    # \u8ba1\u7b97\u906e\u853d\u8bed\u8a00\u6a21\u578b\u635f\u5931\n    mlm_l = loss(mlm_Y_hat.reshape(-1, vocab_size), mlm_Y.reshape(-1)) *\\\n    mlm_weights_X.reshape(-1, 1)\n    mlm_l = mlm_l.sum() \/ (mlm_weights_X.sum() + 1e-8)\n    # \u8ba1\u7b97\u4e0b\u4e00\u53e5\u5b50\u9884\u6d4b\u4efb\u52a1\u7684\u635f\u5931\n    nsp_l = loss(nsp_Y_hat, nsp_y)\n    l = mlm_l + nsp_l\n    return mlm_l, nsp_l, l","d31ee1a4":"def train_bert(train_iter, net, loss, vocab_size, devices, num_steps):\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    trainer = torch.optim.Adam(net.parameters(), lr=1e-3)\n    step, timer = 0, Timer()\n    animator = Animator(xlabel='step', ylabel='loss',\n                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n    # \u906e\u853d\u8bed\u8a00\u6a21\u578b\u635f\u5931\u7684\u548c\uff0c\u4e0b\u4e00\u53e5\u9884\u6d4b\u4efb\u52a1\u635f\u5931\u7684\u548c\uff0c\u53e5\u5b50\u5bf9\u7684\u6570\u91cf\uff0c\u8ba1\u6570\n    metric = Accumulator(4)\n    num_steps_reached = False\n    while step < num_steps and not num_steps_reached:\n        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n            tokens_X = tokens_X.to(devices[0])\n            segments_X = segments_X.to(devices[0])\n            valid_lens_x = valid_lens_x.to(devices[0])\n            pred_positions_X = pred_positions_X.to(devices[0])\n            mlm_weights_X = mlm_weights_X.to(devices[0])\n            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n            trainer.zero_grad()\n            timer.start()\n            mlm_l, nsp_l, l = _get_batch_loss_bert(\n                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n            l.backward()\n            trainer.step()\n            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n            timer.stop()\n#             animator.add(step + 1,\n#                          (metric[0] \/ metric[3], metric[1] \/ metric[3]))\n            step += 1\n            if step == num_steps:\n                num_steps_reached = True\n                break\n\n    print(f'MLM loss {metric[0] \/ metric[3]:.3f}, '\n          f'NSP loss {metric[1] \/ metric[3]:.3f}')\n    print(f'{metric[2] \/ timer.sum():.1f} sentence pairs\/sec on '\n          f'{str(devices)}')\n\ntrain_bert(train_iter, net, loss, len(vocab), devices, 1)","82613258":"def train_bert_with_plot(train_iter, net, loss, vocab_size, devices, num_steps):\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    trainer = torch.optim.Adam(net.parameters(), lr=1e-3)\n    step, timer = 0, Timer()\n    animator = Animator(xlabel='step', ylabel='loss',\n                            xlim=[1, num_steps], legend=['mlm', 'nsp'])\n    # \u906e\u853d\u8bed\u8a00\u6a21\u578b\u635f\u5931\u7684\u548c\uff0c\u4e0b\u4e00\u53e5\u9884\u6d4b\u4efb\u52a1\u635f\u5931\u7684\u548c\uff0c\u53e5\u5b50\u5bf9\u7684\u6570\u91cf\uff0c\u8ba1\u6570\n    metric = Accumulator(4)\n    num_steps_reached = False\n    while step < num_steps and not num_steps_reached:\n        for tokens_X, segments_X, valid_lens_x, pred_positions_X,\\\n            mlm_weights_X, mlm_Y, nsp_y in train_iter:\n            tokens_X = tokens_X.to(devices[0])\n            segments_X = segments_X.to(devices[0])\n            valid_lens_x = valid_lens_x.to(devices[0])\n            pred_positions_X = pred_positions_X.to(devices[0])\n            mlm_weights_X = mlm_weights_X.to(devices[0])\n            mlm_Y, nsp_y = mlm_Y.to(devices[0]), nsp_y.to(devices[0])\n            trainer.zero_grad()\n            timer.start()\n            mlm_l, nsp_l, l = _get_batch_loss_bert(\n                net, loss, vocab_size, tokens_X, segments_X, valid_lens_x,\n                pred_positions_X, mlm_weights_X, mlm_Y, nsp_y)\n            l.backward()\n            trainer.step()\n            metric.add(mlm_l, nsp_l, tokens_X.shape[0], 1)\n            timer.stop()\n            animator.add(step + 1,\n                         (metric[0] \/ metric[3], metric[1] \/ metric[3]))\n            step += 1\n            if step == num_steps:\n                num_steps_reached = True\n                break\n\n    print(f'MLM loss {metric[0] \/ metric[3]:.3f}, '\n          f'NSP loss {metric[1] \/ metric[3]:.3f}')\n    print(f'{metric[2] \/ timer.sum():.1f} sentence pairs\/sec on '\n          f'{str(devices)}')\n\ntrain_bert(train_iter, net, loss, len(vocab), devices, 50)","840adc4f":"def get_bert_encoding(net, tokens_a, tokens_b=None):\n    tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n    token_ids = torch.tensor(vocab[tokens], device=devices[0]).unsqueeze(0)\n    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n    valid_len = torch.tensor(len(tokens), device=devices[0]).unsqueeze(0)\n    encoded_X, _, _ = net(token_ids, segments, valid_len)\n    return encoded_X\n\ntokens_a = ['\u6211', '\u660e\u5929', '\u53ef\u80fd', '\u4e0b\u96e8']\nencoded_text = get_bert_encoding(net, tokens_a)\n# \u8bcd\u5143\uff1a '<cls>', 'a', 'crane', 'is', 'flying', '<sep>'\nencoded_text_cls = encoded_text[:, 0, :]\nencoded_text_crane = encoded_text[:, 2, :]\nencoded_text.shape, encoded_text_cls.shape, encoded_text_crane[0][:3]","1805a85e":"tokens_a, tokens_b = ['\u660e\u5929', '\u53ef\u80fd', '\u4e0b\u96e8'], ['\u660e\u5929', '\u5e94\u8be5' '\u6709\u96e8']\nencoded_pair = get_bert_encoding(net, tokens_a, tokens_b)\n# \u8bcd\u5143\uff1a '<cls>', 'a', 'crane', 'driver', 'came', '<sep>', 'he', 'just',\n# 'left', '<sep>'\nencoded_pair_cls = encoded_pair[:, 0, :]\nencoded_pair_crane = encoded_pair[:, 2, :]\nencoded_pair.shape, encoded_pair_cls.shape, encoded_pair_crane[0][:3]","03312470":"def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n                          num_heads, num_layers, dropout, max_len, devices):\n    data_dir = download_extract(pretrained_model)\n    # Define an empty vocabulary to load the predefined vocabulary\n    vocab = Vocab()\n    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n    vocab.token_to_idx = {token: idx for idx, token in enumerate(\n        vocab.idx_to_token)}\n    bert = BERTModel(len(vocab), num_hiddens, norm_shape=[256],\n                         ffn_num_input=256, ffn_num_hiddens=ffn_num_hiddens,\n                         num_heads=4, num_layers=2, dropout=0.2,\n                         max_len=max_len, key_size=256, query_size=256,\n                         value_size=256, hid_in_features=256,\n                         mlm_in_features=256, nsp_in_features=256)\n    # Load pretrained BERT parameters\n    bert.load_state_dict(torch.load(os.path.join(data_dir,\n                                                 'pretrained.params')))\n    return bert, vocab","2843f0d4":"train_set = _WikiTextDataset(paragraphs, max_len)\nvocab = train_set.vocab\nprint (type(vocab))","7497d227":"def download_extract(name, folder=None):\n    \"\"\"\u4e0b\u8f7d\u5e76\u89e3\u538bzip\/tar\u6587\u4ef6\u3002\n    Defined in :numref:`sec_kaggle_house`\"\"\"\n    fname = download(name)\n    base_dir = os.path.dirname(fname)\n    data_dir, ext = os.path.splitext(fname)\n    if ext == '.zip':\n        fp = zipfile.ZipFile(fname, 'r')\n    elif ext in ('.tar', '.gz'):\n        fp = tarfile.open(fname, 'r')\n    else:\n        assert False, '\u53ea\u6709zip\/tar\u6587\u4ef6\u53ef\u4ee5\u88ab\u89e3\u538b\u7f29\u3002'\n    fp.extractall(base_dir)\n    return os.path.join(base_dir, folder) if folder else data_dir\n\ndevices = try_all_gpus()\n# bert, vocab = load_pretrained_model(\n#     'bert.small', num_hiddens=256, ffn_num_hiddens=512, num_heads=4,\n#     num_layers=2, dropout=0.1, max_len=512, devices=devices)\nbert = net\nprint (type(bert), type(vocab))","48873ac2":"#@tab pytorch\nclass qianyan_Dataset(torch.utils.data.Dataset):\n    def __init__(self, dataset, max_len, vocab=None):\n        self.all_token_ids, self.all_segments, self.valid_lens, self.labels = [], [], [], []\n        self.vocab = vocab\n        self.max_len = max_len  \n        cnt = len(dataset[0])\n        for index in range(cnt):\n            q1, q2, lebel = dataset[0][index], dataset[1][index], dataset[2][index] \n            q1 = tokenize(q1)\n            q2 = tokenize(q2)\n            l1, l2 = len(q1), len(q2)\n            while len(q1) + len(q2) > max_len - 3:\n                if len(q1) > len(q2):\n                    q1.pop()\n                else:\n                    q2.pop()\n            tokens, segments = get_tokens_and_segments(q1, q2)\n            token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n                                 * (self.max_len - len(tokens))\n            segments = segments + [0] * (self.max_len - len(segments))\n            self.all_token_ids.append([int(x) for x in token_ids])\n            self.all_segments.append([int(x) for x in segments])\n            self.valid_lens.append(len(tokens))\n            self.labels.append(lebel)\n        self.all_token_ids, self.all_segments, self.valid_lens = torch.tensor(self.all_token_ids, dtype=torch.long), \\\n                                                                torch.tensor(self.all_segments, dtype=torch.long), \\\n                                                                torch.tensor(self.valid_lens)\n        print('read ' + str(len(self.all_token_ids)) + ' examples')\n\n    def __getitem__(self, idx):\n        return (self.all_token_ids[idx], self.all_segments[idx],\n                self.valid_lens[idx]), self.labels[idx]\n\n    def __len__(self):\n        return len(self.all_token_ids)","f2e3d9c0":"def read_my_data():\n    lines = open('..\/input\/baidu-qianyan-text-sim-lcqmc-train\/train.tsv', 'r').read().split('\\n') # [: 10000]\n    # print (lines[: 3])\n    random.shuffle(lines)\n    # print (lines[: 3])\n    # print (len(lines))\n    print (len(lines))\n    q1 = [x.split('\\t')[0] for x in lines if len(x.split('\\t')) == 3]\n    q2 = [x.split('\\t')[1] for x in lines if len(x.split('\\t')) == 3]\n    labels = [int(x.split('\\t')[2]) for x in lines if len(x.split('\\t')) == 3]\n    return q1, q2, labels\nbatch_size, max_len, num_workers = 512, 128, get_dataloader_workers()\nq1, q2, labels = read_my_data()\nprint ('all len', len(q1))\ntrain_length = int(len(q1) * 0.7)","6ddac26e":"train_data = [q1[: train_length], q2[: train_length], labels[: train_length]]\ntest_data = [q1[train_length: ], q2[train_length: ], labels[train_length: ]]","4c398992":"%%time\n# 5.9G\ntrain_set = qianyan_Dataset(train_data, max_len, vocab)","235838d1":"%%time\n# \ntest_set = qianyan_Dataset(test_data, max_len, vocab)\ntrain_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n                                   num_workers=num_workers) \ntest_iter = torch.utils.data.DataLoader(test_set, batch_size,\n                                  num_workers=num_workers)","bcb0ebaf":"# batch_size, max_len, num_workers = 512, 128, get_dataloader_workers()\n# data_dir = download_extract('SNLI')\n# train_set = SNLIBERTDataset(d2l.read_snli(data_dir, True), max_len, vocab)\n# test_set = SNLIBERTDataset(d2l.read_snli(data_dir, False), max_len, vocab)\n# train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True,\n#                                    num_workers=num_workers)\n# test_iter = torch.utils.data.DataLoader(test_set, batch_size,\n#                                   num_workers=num_workers)","f257da62":"size = lambda x, *args, **kwargs: x.numel(*args, **kwargs)\ndef evaluate_accuracy_gpu(net, data_iter, device=None):\n    \"\"\"\u4f7f\u7528GPU\u8ba1\u7b97\u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u5ea6\u3002\n    Defined in :numref:`sec_lenet`\"\"\"\n    if isinstance(net, nn.Module):\n        net.eval()  # \u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\n        if not device:\n            device = next(iter(net.parameters())).device\n    # \u6b63\u786e\u9884\u6d4b\u7684\u6570\u91cf\uff0c\u603b\u9884\u6d4b\u7684\u6570\u91cf\n    metric = Accumulator(2)\n    with torch.no_grad():\n        for X, y in data_iter:\n            if isinstance(X, list):\n                # BERT\u5fae\u8c03\u6240\u9700\u7684\uff08\u4e4b\u540e\u5c06\u4ecb\u7ecd\uff09\n                X = [x.to(device) for x in X]\n            else:\n                X = X.to(device)\n            y = y.to(device)\n            metric.add(accuracy(net(X), y), size(y))\n    return metric[0] \/ metric[1]\n\nargmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\nastype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\nreduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\ndef accuracy(y_hat, y):\n    \"\"\"\u8ba1\u7b97\u9884\u6d4b\u6b63\u786e\u7684\u6570\u91cf\u3002\n    Defined in :numref:`sec_softmax_scratch`\"\"\"\n    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n        y_hat = argmax(y_hat, axis=1)\n    cmp = astype(y_hat, y.dtype) == y\n    return float(reduce_sum(astype(cmp, y.dtype)))\n\ndef train_batch_ch13(net, X, y, loss, trainer, devices):\n    \"\"\"\u7528\u591aGPU\u8fdb\u884c\u5c0f\u6279\u91cf\u8bad\u7ec3\"\"\"\n    if isinstance(X, list):\n        # \u5fae\u8c03BERT\u4e2d\u6240\u9700\uff08\u7a0d\u540e\u8ba8\u8bba\uff09\n        X = [x.to(devices[0]) for x in X]\n    else:\n        X = X.to(devices[0])\n    y = y.to(devices[0])\n    net.train()\n    trainer.zero_grad()\n    pred = net(X)\n#     print (\"pred type\", type(pred), type(y))\n#     print ('pred shape', pred.shape, y.shape)\n    l = loss(pred, y)\n    l.sum().backward()\n    trainer.step()\n    train_loss_sum = l.sum()\n    train_acc_sum = accuracy(pred, y)\n    return train_loss_sum, train_acc_sum\n\ndef train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n               devices=try_all_gpus()):\n    \"\"\"\u7528\u591aGPU\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\"\"\"\n    timer, num_batches = Timer(), len(train_iter)\n    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 1],\n                            legend=['train loss', 'train acc', 'test acc'])\n    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n    for epoch in range(num_epochs):\n        # 4\u4e2a\u7ef4\u5ea6\uff1a\u50a8\u5b58\u8bad\u7ec3\u635f\u5931\uff0c\u8bad\u7ec3\u51c6\u786e\u5ea6\uff0c\u5b9e\u4f8b\u6570\uff0c\u7279\u70b9\u6570\n        metric = Accumulator(4)\n        for i, (features, labels) in enumerate(train_iter):\n            timer.start()\n            l, acc = train_batch_ch13(\n                net, features, labels, loss, trainer, devices)\n            metric.add(l, acc, labels.shape[0], labels.numel())\n            timer.stop()\n            if (i + 1) % (num_batches \/\/ 5) == 0 or i == num_batches - 1:\n                animator.add(epoch + (i + 1) \/ num_batches,\n                             (metric[0] \/ metric[2], metric[1] \/ metric[3],\n                              None))\n        test_acc = evaluate_accuracy_gpu(net, test_iter)\n        animator.add(epoch + 1, (None, None, test_acc))\n    print(f'loss {metric[0] \/ metric[2]:.3f}, train acc '\n          f'{metric[1] \/ metric[3]:.3f}, test acc {test_acc:.3f}')\n    print(f'{metric[2] * num_epochs \/ timer.sum():.1f} examples\/sec on '\n          f'{str(devices)}')","47c20a32":"%%time\nclass BERTClassifier(nn.Module):\n    def __init__(self, bert):\n        super(BERTClassifier, self).__init__()\n        self.encoder = bert.encoder\n        self.hidden = bert.hidden\n        self.output = nn.Linear(128, 3)\n\n    def forward(self, inputs):\n        tokens_X, segments_X, valid_lens_x = inputs\n        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n        # print ('encoded_X shape', encoded_X.shape)\n        return self.output(self.hidden(encoded_X[:, 0, :]))\n\nnet = BERTClassifier(bert)\nlr, num_epochs = 1e-4, 50\ntrainer = torch.optim.Adam(net.parameters(), lr=lr)\nloss = nn.CrossEntropyLoss(reduction='none')\ntrain_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)","d40749f1":"print (devices)","561fabbb":"## \u51c6\u5907 mlm nsp \u6570\u636e","a98f356b":"# 14.10. \u9884\u8bad\u7ec3BERT\nhttps:\/\/zh-v2.d2l.ai\/chapter_natural-language-processing-pretraining\/bert-pretraining.html\n\n","83d84377":"# bert model from d2l \nhttps:\/\/zh-v2.d2l.ai\/chapter_natural-language-processing-pretraining\/bert.html","603eef6d":"# fine tune "}}