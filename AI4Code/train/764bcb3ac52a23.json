{"cell_type":{"965fa746":"code","b737cfbb":"code","56d0cad3":"code","c428bc9a":"code","58e06403":"code","831dbe51":"code","3515f4c5":"code","74085a42":"code","93a410d4":"code","dfc93d60":"markdown","eefd6172":"markdown","d690b993":"markdown","6f9c6b95":"markdown","a553349b":"markdown"},"source":{"965fa746":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b737cfbb":"import numpy as np\nimport pandas as pd\ndf=pd.read_csv(\"..\/input\/latest-covid19-india-statewise-data\/Latest Covid-19 India Status.csv\")","56d0cad3":"df","c428bc9a":"df.info()","58e06403":"\n\n\ndf['Deaths'].replace(np.NaN,df['Deaths'].mean)","831dbe51":"from sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()  #Creating instance\ndf[\"State_No\"]=labelencoder.fit_transform(df[\"State\/UTs\"]) #new numerical column\ndf","3515f4c5":"ode_df=pd.get_dummies(df['State\/UTs'])\node_df\n","74085a42":"from sklearn.preprocessing import StandardScaler\ndf2 = df.copy()\nnum_cols = ['Total Cases','Active','Discharged','Deaths'] # numerical features\nfor i in num_cols:  # apply standardization on numerical features\n    scale = StandardScaler().fit(df2[[i]])     # fit on training data column\n    df2[i] = scale.transform(df2[[i]]) # transform the training data column\ndf2  #standardized dataframe\n    \n    ","93a410d4":"# data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\ndf3=df[['Total Cases','Active','Discharged','Deaths','State_No']]\n\nnorm = MinMaxScaler().fit(df3)    # fit scaler on data\n# transform data\ndf_norm = norm.transform(df3)\ndf_norm\n\n","dfc93d60":"#Now Performimg Label Encoding\n","eefd6172":"# Now Performing One-Hot Encoding","d690b993":"# Performimg Standardization","6f9c6b95":"#now Performing Normalization","a553349b":"#Since the dataset doesn\u2019t have null values or missing places no need of handling the\nYet the way to do so is as follows:\n"}}