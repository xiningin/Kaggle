{"cell_type":{"7f50dff3":"code","856c2388":"code","a4be406c":"code","9520e64e":"code","2ed2d14c":"code","300de31b":"code","66234794":"code","3781968c":"code","4670ee29":"code","cef03896":"code","822f62a4":"code","4ed99660":"code","ec6d43df":"code","2b5277b3":"code","d6a313b7":"code","b60e74f7":"code","752b3d46":"code","531fc489":"code","c214275a":"code","30914d7e":"code","7eaf55c2":"code","e5a1e3c4":"code","2607c291":"code","a86bfc2e":"code","4b0271a6":"code","90dd3377":"code","bfd1e4e4":"code","5d866b80":"code","781150af":"code","db3c561f":"code","c98582a8":"code","e292ead7":"code","b5a0c8c9":"code","3b2245f7":"code","ce93ae3f":"code","e796d903":"code","c651ef29":"code","e42c3a13":"code","7379d2db":"code","5f341cc3":"code","567c46fa":"code","f039f079":"code","a494233f":"markdown","a28af29e":"markdown","e612fad0":"markdown","423e7d66":"markdown","bde79bee":"markdown","01262b72":"markdown","4ee1fe90":"markdown","fc403d67":"markdown","8af2d16e":"markdown","064f9d68":"markdown","8a4293d6":"markdown","41a034dd":"markdown","94bb2acc":"markdown","56f4bcc7":"markdown","484610fe":"markdown","68301a2a":"markdown","d411fe45":"markdown","96583772":"markdown","45974afe":"markdown","34c7ff7f":"markdown","f29a06ff":"markdown","41513a0e":"markdown","70d0a143":"markdown","7db321b6":"markdown","54a252fd":"markdown","6f98512b":"markdown","a9989903":"markdown"},"source":{"7f50dff3":"!pip install catboost\n!pip install hyperopt \n!pip install mlxtend","856c2388":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport functools\nfrom hyperopt import hp, fmin, Trials, tpe\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.metrics import classification_report, average_precision_score, accuracy_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_score, recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","a4be406c":"# import data \npath = '..\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx'\ndf = pd.read_excel(path)\ndf.head()","9520e64e":"# shape\nprint('Rows: {} | Columns: {}'.format(df.shape[0], df.shape[1]))","2ed2d14c":"# data types \ndf.dtypes","300de31b":"#  categorical features\ncat_cols = df.select_dtypes(include=['object']).columns\nnum_cols = df.select_dtypes(exclude=['object']).columns\nprint(df[cat_cols].columns)","66234794":"# Label ICU \nplt.figure(figsize=(10,6))\nplt.title('ICU Class count', fontsize=14)\nsns.countplot(df['ICU'], palette='GnBu')\nplt.tight_layout()\n\n# count class \n\nclass_1 = len(df[df['ICU']==1])\nclass_0 = len(df[df['ICU']==0])\n\n# show \nprint('\\n')\nprint('Outside the ICU: {}'.format(class_0))\nprint('Inside the ICU: {}'.format(class_1))","3781968c":"# category aggregation function\n\ndef aggregate(df, col, arg, title):\n  \n  # category values \n  range_1 = len(df[df[col]==arg])\n  show = print('{}: {}'.format(title,range_1))\n  return show","4670ee29":"# age distribution\nplt.figure(figsize=(12,7))\nplt.title('Age Distribution', fontsize=14)\nsns.countplot(df['AGE_PERCENTIL'], palette='Purples')\nplt.tight_layout()\n\n\n# Summary ages \nprint('----- AGE COUNT -----')\nprint('\\n')\naggregate(df, 'AGE_PERCENTIL', '10th', 'AGE 10th')\naggregate(df, 'AGE_PERCENTIL', '20th', 'AGE 20th')\naggregate(df, 'AGE_PERCENTIL', '30th', 'AGE 30th')\naggregate(df, 'AGE_PERCENTIL', '40th', 'AGE 40th')\naggregate(df, 'AGE_PERCENTIL', '50th', 'AGE 50th')\naggregate(df, 'AGE_PERCENTIL', '60th', 'AGE 60th')\naggregate(df, 'AGE_PERCENTIL', '70th', 'AGE 70th')\naggregate(df, 'AGE_PERCENTIL', '80th', 'AGE 80th')\naggregate(df, 'AGE_PERCENTIL', '90th', 'AGE 90th')\naggregate(df, 'AGE_PERCENTIL', 'Above 90th', 'AGE above 90th')","cef03896":"# Patient event window \nplt.figure(figsize=(10,6))\nplt.title('Patient event window | ICU ', fontsize=14)\nsns.countplot(df['WINDOW'], hue='ICU', data=df, palette='GnBu')\nplt.tight_layout()","822f62a4":"# Does age influence taking the patient to an ICU bed?\nplt.figure(figsize=(12,10))\nplt.subplot(2,1,1)\nplt.title('Does age influence taking the patient to an ICU bed? ', fontsize=14)\nsns.countplot(df['AGE_PERCENTIL'], hue='ICU', data=df, palette='GnBu')\n\nprint('\\n')\n\n# Ages with Disease Grouping 1\nplt.subplot(2,1,2)\nplt.title('Ages with Disease grouping ', fontsize=14)\nsns.barplot(df['AGE_PERCENTIL'], y='DISEASE GROUPING 1', hue='ICU', data=df, palette='GnBu')\nplt.tight_layout()","4ed99660":"# view mising values \n\ndef missing_values(data):\n    \n    # Null total \n    missing_values = data.isnull().sum()\n    \n    # Order nulls \n    total = missing_values.sort_values(ascending=True)\n    \n    # Percentage  \n    percent = (missing_values \/ len(data.index)*100).round(2).sort_values(ascending=True)\n\n    table_missing = pd.concat([total, percent], axis=1, keys=['Number of Nulls', 'Percentagem of Nulls'])\n  \n    return table_missing.tail(10)\n\n\nmissing_values(df)","ec6d43df":"# input missing values\n\nimp_numeric = SimpleImputer(missing_values=np.nan, strategy='mean')\nnum_cols = df.select_dtypes(exclude=['object']).columns \n\n\nfor col in num_cols:\n  if df[col].isnull().sum() > 0:\n    df[col] = imp_numeric.fit_transform(df[[col]])\n  else:\n    pass","2b5277b3":"# handling categorical AGE_PERCENTIL \n\ndef cat_percentil(percentil):\n    if percentil == \"Above 90th\":\n        return(100)\n    else:\n        return(int(\"\".join(c for c in str(percentil) if c.isdigit())))\n\n\n# checking AGE_PERCENTIL \ndf['AGE_PERCENTIL'] = df['AGE_PERCENTIL'].apply(lambda x: cat_percentil(x))\ndf['AGE_PERCENTIL'].isnull().sum()","d6a313b7":"# handling categorical WINDOW \n\ndef cat_window(window):\n    if window == \"ABOVE_12\":\n        return(13)\n    else:\n        return(int((window.split(\"-\")[1])))\n\n\ndf['WINDOW'] = df['WINDOW'].apply(lambda x: cat_window(x))\ndf['WINDOW'].isnull().sum()","b60e74f7":"# remove ID \ndf.drop('PATIENT_VISIT_IDENTIFIER', axis=1, inplace=True)","752b3d46":"# features | label\nX_baseline = df.drop('ICU', axis=1)\ny_baseline = df['ICU']\n\n\n# spliting \nX_train_baseline, X_test_baseline, y_train_baseline, y_test_baseline = train_test_split(X_baseline,y_baseline, test_size=0.30, random_state=42)\n\n\n# Standard \nscaler = StandardScaler()\nX_train_baseline = scaler.fit_transform(X_train_baseline)\nX_test_baseline = scaler.transform(X_test_baseline)\n\n\n# Logistic regression \nbaseline = LogisticRegression(random_state=42)\nbaseline.fit(X_train_baseline, y_train_baseline)\ny_pred_baseline = baseline.predict(X_test_baseline)\ny_proba_baseline = baseline.predict_proba(X_test_baseline)[:,1]\nprint(classification_report(y_test_baseline, y_pred_baseline))\n\n\n# Summary \nprint('\\n')\nprint('--------- Baseline Summary ------------')\nprint('\\n')\nprint('Precision: {}'.format(round(precision_score(y_test_baseline, y_pred_baseline),2)))\nprint('Recall: {}'.format(round(recall_score(y_test_baseline, y_pred_baseline),2)))\nprint('AUC: {}'.format(round(roc_auc_score(y_test_baseline, y_proba_baseline),2)))\nprint('Precision-Recall: {}'.format(round(average_precision_score(y_test_baseline, y_proba_baseline),2)))","531fc489":"# feature selector function \n\ndef feature_selection(k_value=2, name='RandomForest', model=XGBClassifier(random_state=42)):\n\n  # number features \n  k=k_value \n  f_class = functools.partial(f_classif)\n\n\n  # features | label \n  X = df.drop('ICU', axis=1)\n  y = df['ICU']\n\n\n  # spliting \n  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n\n\n  # filter  \n  seletor = SelectKBest(score_func=f_class, k=k)\n  X_train2 = seletor.fit_transform(X_train, y_train)\n  X_test2 = seletor.transform(X_test)\n\n\n  # Random Forest  \n  mdl = model\n  mdl.fit(X_train2, y_train)\n  y_pred = mdl.predict(X_test2)\n  y_proba = mdl.predict_proba(X_test2)[:,1]\n\n\n  # metrics \n  accuracy = round(accuracy_score(y_test, y_pred),2)\n  precision = round(precision_score(y_test, y_pred),2)\n  recall = round(recall_score(y_test, y_pred),2)\n  precision_recall = round(average_precision_score(y_test, y_proba),2)\n  auc = round(roc_auc_score(y_test, y_proba),2)\n  \n\n  print('K: {} | Acur\u00e1cia: {}'.format(k, accuracy))\n  print('K: {} | Precision: {}'.format(k, precision))\n  print('K: {} | Recall: {}'.format(k, recall))\n  print('K: {} | Precision-Recall: {}'.format(k, precision_recall))\n  print('K: {} | AUC: {}'.format(k, auc))\n  print('\\n')\n  print('\\n')\n\n  mask = seletor.get_support()\n  best_features = X.columns[mask]\n  print('----- Best {} features {} --------'.format(k, name))\n  print('\\n')\n  print(best_features)","c214275a":"# Features for XGBoost \nfeature_selection(k_value=50, name='XGBoost', model=XGBClassifier(random_state=42))","30914d7e":"# Features for LightGBM\nfeature_selection(k_value=50, name='LightGBM', model=XGBClassifier(random_state=42))","7eaf55c2":"# Features for CatBoost\nfeature_selection(k_value=50, name='CatBoost', model=CatBoostClassifier(iterations=100,\n                             loss_function='Logloss',\n                             task_type='CPU',\n                             random_state=42,\n                             verbose=False))","e5a1e3c4":"# TOP 50 features XGBoost \nxgb_columns = df[['AGE_ABOVE65', 'AGE_PERCENTIL', 'ALBUMIN_MIN', 'ALBUMIN_MAX',\n       'BE_ARTERIAL_MEDIAN', 'BE_ARTERIAL_MEAN', 'BE_ARTERIAL_MIN',\n       'BE_ARTERIAL_MAX', 'BE_VENOUS_MEDIAN', 'BE_VENOUS_MEAN',\n       'BE_VENOUS_MIN', 'BE_VENOUS_MAX', 'HEMATOCRITE_MEDIAN',\n       'HEMATOCRITE_MEAN', 'HEMATOCRITE_MIN', 'HEMATOCRITE_MAX',\n       'HEMOGLOBIN_MEDIAN', 'HEMOGLOBIN_MEAN', 'HEMOGLOBIN_MIN',\n       'HEMOGLOBIN_MAX', 'LACTATE_MEDIAN', 'LACTATE_MEAN', 'LACTATE_MIN',\n       'LACTATE_MAX', 'LEUKOCYTES_MEDIAN', 'LEUKOCYTES_MEAN', 'LEUKOCYTES_MIN',\n       'LEUKOCYTES_MAX', 'BLOODPRESSURE_DIASTOLIC_MEAN',\n       'RESPIRATORY_RATE_MEAN', 'BLOODPRESSURE_DIASTOLIC_MEDIAN',\n       'RESPIRATORY_RATE_MEDIAN', 'BLOODPRESSURE_DIASTOLIC_MIN',\n       'TEMPERATURE_MIN', 'BLOODPRESSURE_SISTOLIC_MAX', 'RESPIRATORY_RATE_MAX',\n       'OXYGEN_SATURATION_MAX', 'BLOODPRESSURE_DIASTOLIC_DIFF',\n       'BLOODPRESSURE_SISTOLIC_DIFF', 'HEART_RATE_DIFF',\n       'RESPIRATORY_RATE_DIFF', 'TEMPERATURE_DIFF', 'OXYGEN_SATURATION_DIFF',\n       'BLOODPRESSURE_DIASTOLIC_DIFF_REL', 'BLOODPRESSURE_SISTOLIC_DIFF_REL',\n       'HEART_RATE_DIFF_REL', 'RESPIRATORY_RATE_DIFF_REL',\n       'TEMPERATURE_DIFF_REL', 'OXYGEN_SATURATION_DIFF_REL', 'WINDOW', 'ICU']]\n\n\nX = xgb_columns.drop('ICU', axis=1)\ny = df['ICU']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n\nmdl = XGBClassifier(random_state=42)\nmdl.fit(X_train, y_train)\ny_pred = mdl.predict(X_test)\ny_proba = mdl.predict_proba(X_test)[:,1]\n\n\n# objective function \ndef objective(params):\n  return -average_precision_score(y_test, y_proba)","2607c291":"#probability of distribution of parameters\n\nspaces_xgboost = {'n_estimators': hp.randint('n_estimators', 1200),\n          'learning_rate': hp.loguniform('learning_rate', 1e-5, 1e-1),\n          'max_depth': hp.randint('max_depth', 30),\n          'min_child_weight': hp.uniform('min_child_weight', 0,20),\n          'reg_lambda': hp.uniform('reg_lambda', 0.001, 3),\n          'reg_alpha': hp.uniform('reg_alpha', 0.001, 3),\n          'gamma': hp.uniform('gamma', 0,10),\n          'max_delta_step': hp.uniform('max_delta_step', 0, 10),\n          'max_leaves': hp.randint('max_leaves', 30),\n          'colsample_bytree': hp.uniform('colsample_bytree', 0.1,1.0),\n          'colsample_bylevel': hp.uniform('colsample_bylevel', 0.1, 1.0),\n          'scale_pos_weight': hp.randint('scale_pos_weight', 10)}","a86bfc2e":"# Bayesian Optimization \ntrials = Trials()\nxgboost_tuning = fmin(objective, spaces_xgboost, algo=tpe.suggest, max_evals=100, trials=trials, verbose=1)","4b0271a6":"# best params \nxgboost_tuning","90dd3377":"# XGBoost tuning\nxgboost = XGBClassifier(**xgboost_tuning, random_state=42)\nxgboost.fit(X_train, y_train)\ny_pred = xgboost.predict(X_test)\ny_proba = xgboost.predict_proba(X_test)[:,1]\nprint(classification_report(y_test, y_pred))\n\n\n# XGBoost summary \nprint('\\n')\nprint('AUC: {}'.format(round(roc_auc_score(y_test, y_proba),2)))\nprint('Precision-Recall: {}'.format(round(average_precision_score(y_test, y_proba),2)))","bfd1e4e4":"# TOP 50 features LightGBM \nlightgbm_columns = df[['AGE_ABOVE65', 'AGE_PERCENTIL', 'ALBUMIN_MIN', 'ALBUMIN_MAX',\n       'BE_ARTERIAL_MEDIAN', 'BE_ARTERIAL_MEAN', 'BE_ARTERIAL_MIN',\n       'BE_ARTERIAL_MAX', 'BE_VENOUS_MEDIAN', 'BE_VENOUS_MEAN',\n       'BE_VENOUS_MIN', 'BE_VENOUS_MAX', 'HEMATOCRITE_MEDIAN',\n       'HEMATOCRITE_MEAN', 'HEMATOCRITE_MIN', 'HEMATOCRITE_MAX',\n       'HEMOGLOBIN_MEDIAN', 'HEMOGLOBIN_MEAN', 'HEMOGLOBIN_MIN',\n       'HEMOGLOBIN_MAX', 'LACTATE_MEDIAN', 'LACTATE_MEAN', 'LACTATE_MIN',\n       'LACTATE_MAX', 'LEUKOCYTES_MEDIAN', 'LEUKOCYTES_MEAN', 'LEUKOCYTES_MIN',\n       'LEUKOCYTES_MAX', 'BLOODPRESSURE_DIASTOLIC_MEAN',\n       'RESPIRATORY_RATE_MEAN', 'BLOODPRESSURE_DIASTOLIC_MEDIAN',\n       'RESPIRATORY_RATE_MEDIAN', 'BLOODPRESSURE_DIASTOLIC_MIN',\n       'TEMPERATURE_MIN', 'BLOODPRESSURE_SISTOLIC_MAX', 'RESPIRATORY_RATE_MAX',\n       'OXYGEN_SATURATION_MAX', 'BLOODPRESSURE_DIASTOLIC_DIFF',\n       'BLOODPRESSURE_SISTOLIC_DIFF', 'HEART_RATE_DIFF',\n       'RESPIRATORY_RATE_DIFF', 'TEMPERATURE_DIFF', 'OXYGEN_SATURATION_DIFF',\n       'BLOODPRESSURE_DIASTOLIC_DIFF_REL', 'BLOODPRESSURE_SISTOLIC_DIFF_REL',\n       'HEART_RATE_DIFF_REL', 'RESPIRATORY_RATE_DIFF_REL',\n       'TEMPERATURE_DIFF_REL', 'OXYGEN_SATURATION_DIFF_REL', 'WINDOW', 'ICU']]\n\n\nX = lightgbm_columns.drop('ICU', axis=1)\ny = df['ICU']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n\n\n# objective function \ndef objective(params):\n  return -average_precision_score(y_test, y_proba)","5d866b80":"#probability of distribution of parameters\n\nspaces_lightgbm = {'n_estimators': hp.randint('n_estimators', 1000),\n          'num_leaves': hp.randint('num_leaves', 100),\n          'learning_rate': hp.loguniform('learning_rate', 1e-5, 1e-1),\n          'max_depth': hp.randint('max_depth', 30),\n          'min_child_samples': hp.uniform('min_child_samples', 0,20),\n          'lambda_l1': hp.uniform('lambda_l1', 0.001, 3),\n          'lambda_l2': hp.uniform('lambda_l2', 0.001, 3),\n          'min_data_in_leaf': hp.randint('min_data_in_leaf', 40)}","781150af":"# Bayesian Optimization \ntrials = Trials()\nlightgbm_tuning = fmin(objective, spaces_lightgbm, algo=tpe.suggest, max_evals=100, trials=trials, verbose=1)","db3c561f":"# best params LightGBM \nlightgbm_tuning","c98582a8":"# LightGBM tuning\nlightgbm = LGBMClassifier(**lightgbm_tuning, seed=42)\nlightgbm.fit(X_train, y_train)\ny_pred = xgboost.predict(X_test)\ny_proba = lightgbm.predict_proba(X_test)[:,1]\nprint(classification_report(y_test, y_pred))\n\n\n# LightGBM summary \nprint('\\n')\nprint('AUC: {}'.format(round(roc_auc_score(y_test, y_proba),2)))\nprint('Precision-Recall: {}'.format(round(average_precision_score(y_test, y_proba),2)))","e292ead7":"# TOP 50 features CatBoost  \ncatboost_columns = df[['AGE_ABOVE65', 'AGE_PERCENTIL', 'ALBUMIN_MIN', 'ALBUMIN_MAX',\n       'BE_ARTERIAL_MEDIAN', 'BE_ARTERIAL_MEAN', 'BE_ARTERIAL_MIN',\n       'BE_ARTERIAL_MAX', 'BE_VENOUS_MEDIAN', 'BE_VENOUS_MEAN',\n       'BE_VENOUS_MIN', 'BE_VENOUS_MAX', 'HEMATOCRITE_MEDIAN',\n       'HEMATOCRITE_MEAN', 'HEMATOCRITE_MIN', 'HEMATOCRITE_MAX',\n       'HEMOGLOBIN_MEDIAN', 'HEMOGLOBIN_MEAN', 'HEMOGLOBIN_MIN',\n       'HEMOGLOBIN_MAX', 'LACTATE_MEDIAN', 'LACTATE_MEAN', 'LACTATE_MIN',\n       'LACTATE_MAX', 'LEUKOCYTES_MEDIAN', 'LEUKOCYTES_MEAN', 'LEUKOCYTES_MIN',\n       'LEUKOCYTES_MAX', 'BLOODPRESSURE_DIASTOLIC_MEAN',\n       'RESPIRATORY_RATE_MEAN', 'BLOODPRESSURE_DIASTOLIC_MEDIAN',\n       'RESPIRATORY_RATE_MEDIAN', 'BLOODPRESSURE_DIASTOLIC_MIN',\n       'TEMPERATURE_MIN', 'BLOODPRESSURE_SISTOLIC_MAX', 'RESPIRATORY_RATE_MAX',\n       'OXYGEN_SATURATION_MAX', 'BLOODPRESSURE_DIASTOLIC_DIFF',\n       'BLOODPRESSURE_SISTOLIC_DIFF', 'HEART_RATE_DIFF',\n       'RESPIRATORY_RATE_DIFF', 'TEMPERATURE_DIFF', 'OXYGEN_SATURATION_DIFF',\n       'BLOODPRESSURE_DIASTOLIC_DIFF_REL', 'BLOODPRESSURE_SISTOLIC_DIFF_REL',\n       'HEART_RATE_DIFF_REL', 'RESPIRATORY_RATE_DIFF_REL',\n       'TEMPERATURE_DIFF_REL', 'OXYGEN_SATURATION_DIFF_REL', 'WINDOW', 'ICU']]\n\n\n\nX = catboost_columns.drop('ICU', axis=1)\ny = df['ICU']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=42)\n\n\n\n# objective function \ndef objective(params):\n  return -average_precision_score(y_test, y_proba)","b5a0c8c9":"#probability of distribution of parameters\n\nspaces_catboost = {'iterations': hp.randint('iterations', 1000),\n                  'learning_rate': hp.loguniform('learning_rate', 1e-5, 1e-1),\n                  'depth': hp.randint('depth', 10),\n                  'l2_leaf_reg': hp.uniform('l2_leaf_reg', 0.01, 3),\n                  'bagging_temperature': hp.randint('bagging_temperature', 20),\n                  'random_strength': hp.loguniform('random_strength', 1e-9, 10),\n                  'scale_pos_weight': hp.uniform('scale_pos_weight', 0.01, 1.0)\n                  }","3b2245f7":"# Bayesian Optimization \ntrials = Trials()\ncatboost_tuning = fmin(objective, spaces_catboost, algo=tpe.suggest, max_evals=100, trials=trials, verbose=1)","ce93ae3f":"# best params Catboost \ncatboost_tuning","e796d903":"# Catboost Tuning \ncatboost = CatBoostClassifier(**catboost_tuning,\n                         random_seed=42,\n                         eval_metric='Precision',\n                         task_type= 'CPU',\n                         verbose=False)\ncatboost.fit(X_train, y_train)\ny_pred = catboost.predict(X_test)\ny_proba = catboost.predict_proba(X_test)[:,1]\nprint(classification_report(y_test, y_pred))\n\n# CatBoost summary \nprint('\\n')\nprint('AUC: {}'.format(round(roc_auc_score(y_test, y_proba),2)))\nprint('Precision-Recall: {}'.format(round(average_precision_score(y_test, y_proba),2)))","c651ef29":"# Cross validation function \n\ndef cross_validation(X, model, name, cv=5):\n  \n  # features | class \n  X = X\n  y = df['ICU']\n\n  # results metrics \n  accuracy = []\n  precision = []\n  recall = []\n  auc = []\n  precision_recall = []\n\n  # StratifiedKFold \n  KFold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n\n  for train_index, test_index in KFold.split(X,y):\n    # Train and Test \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    print('Train: {}'.format(X_train.shape))\n    print('Test: {}'.format(X_test.shape))\n    print('\\n')\n\n    # Gradient boosting \n    mdl = model\n    mdl.fit(X_train, y_train)\n    y_pred = mdl.predict(X_test)\n    y_proba = mdl.predict_proba(X_test)[:,1]\n\n\n    # metrics \n    acc_results = round(accuracy_score(y_test, y_pred),5)\n    precision_results = round(precision_score(y_test, y_pred),2)\n    recall_results = round(recall_score(y_test, y_pred),2)\n    auc_results = round(roc_auc_score(y_pred, y_proba),2)\n    pr_results = round(average_precision_score(y_test, y_proba),2)\n\n\n\n\n  # append results \n  accuracy.append(acc_results)\n  precision.append(precision_results)\n  recall.append(recall_results)\n  auc.append(auc_results)\n  precision_recall.append(pr_results)\n\n\n  # show metrics \n  print('\\n')\n  print('----- Validation {} -----'.format(name))\n  print('Accuracy: {}'.format(np.mean(accuracy)))\n  print('Precision: {}'.format(np.mean(precision)))\n  print('Recall: {}'.format(np.mean(recall)))\n  print('AUC: {}'.format(np.mean(auc)))\n  print('Precision-Recall: {}'.format(np.mean(precision_recall)))  ","e42c3a13":"# CV XGBoost \ncross_validation(X=xgb_columns.drop('ICU', axis=1), model=xgboost, name='XGBoost',cv=5)","7379d2db":"# CV LightGBM \ncross_validation(X=lightgbm_columns.drop('ICU', axis=1), model=lightgbm, name='LightGBM',cv=5)","5f341cc3":"# CV CatBoost \ncross_validation(X=catboost_columns.drop('ICU', axis=1), model=catboost, name='CatBoost',cv=5)","567c46fa":"# Three models \nclf1 = xgboost\nclf2 = lightgbm\nclf3 = catboost \n\n# meta-classifier \nmeta_clf = LogisticRegression(random_state=42)\n\n# Stacking \nstack = StackingClassifier(classifiers=[clf1, clf2, clf3],\n                           use_probas=True,\n                           average_probas=False,\n                          meta_classifier=meta_clf)","f039f079":"stack.fit(X_train, y_train)\ny_pred = stack.predict(X_test)\ny_proba = stack.predict_proba(X_test)[:,1]\n\nprint(classification_report(y_test, y_pred))\nprint('\\n')\n\nprint('AUC: {}'.format(round(roc_auc_score(y_test, y_proba),2)))\nprint('Precision-Recall: {}'.format(round(average_precision_score(y_test, y_proba),4)))","a494233f":"#### Tuning XGBoost\n\n<br>","a28af29e":"There are 1925 records in the data set, among the numbers <b> 1410 <\/b> are outside the ICU and <b> 515 <\/b> are in ICU beds\nThere is a certain imbalance of the classes, which we can keep an eye on in the modeling later\n\n<br>","e612fad0":"<br>\n\nOur baseline was made with a very simple logistic regression model and presented the results above, starting from here we will guide our next models comparing the results with this baseline.\n\n\n\n<hr>\n<br>\n","423e7d66":"<br>\n<hr>\n<br>\n<br>\n\n\n### 8. Stacking Gradient Boosting\n\n\nEnsemble methods are commonly used to boost predictive accuracy by combining the predictions of multiple machine learning models. The traditional wisdom has been to combine so-called \u201cweak\u201d learners. However, a more modern approach is to create an ensemble of a well-chosen collection of strong yet diverse models.\n\n<br>\n\nLet's create a stacking set with three Gradient Boosting algorithms as an experiment.\n\n<b> Stacking<\/b>:\n\n* XGBoost \n* LightGBM\n* CatBoost \n\n<br>\n<hr>\n<br>","bde79bee":"<br>\n<hr>\n<br>","01262b72":"<br>\n\n### <b>Goal<\/b> \n\nPredict admission to the ICU of confirmed COVID-19 cases.\nBased on the data available, is it feasible to predict which patients will need intensive care unit support?\nThe aim is to provide tertiary and quarternary hospitals with the most accurate answer, so ICU resources can be arranged or patient transfer can be scheduled.\n\n\nWe will seek a solution that can, segregate patients who MUST in the ICU, in terms of measures, we will look at <b> Precision-Recall <\/b>\n\n\n<br>\n<hr>\n\n","4ee1fe90":"### S\u00edrio-Liban\u00eas COVID-19 - Clinical Data to assess diagnosis\n<hr>\n<br>\n\n Authors of this notebook: \n* <b>Felipe Oliveira <\/b>\n* <b> R\u00f4mulo Amaral <\/b> \n<br>\n\n\n\n<p align=center>\n<img src=\"https:\/\/brasiliaempresas.com.br\/wp-content\/uploads\/2019\/02\/hospital-sirio-libanes.jpg\" width=\"60%\"><\/p>\n\n\n\n#### <b>Context<\/b>\nCOVID-19 pandemic impacted the whole world, overwhelming healthcare systems - unprepared for such intense and lengthy request for ICU beds, professionals, personal protection equipment and healthcare resources.\nBrazil recorded first COVID-19 case on February 26 and reached community transmission on March 20\n\n\n<br>\n<hr>","fc403d67":"The dataset description contained the information that there are two categorical features (removing the label) in the set:\n\n* 'AGE_PERCENTIL'\n* 'WINDOW'\n\n<br>\n","8af2d16e":"<br>\n<hr>\n<br>\n<br>\n\n\n### 6. Tuning Hyperparameters \n\n\nThe approach that we are going to apply for Optimizing hyperparameters is <b> Bayesian Optimization <\/b> a robust technique that has greater gains in converging the search for the best value of a hyperparameter.\n\n\nFramework that provides the technique is Hyperopt\n\nFor a bigger study, about Bayesian Optimization and what is the mathematics behind:\n\n* https:\/\/towardsdatascience.com\/an-introductory-example-of-bayesian-optimization-in-python-with-hyperopt-aae40fff4ff0\n\n* https:\/\/medium.com\/spikelab\/hyperparameter-optimization-using-bayesian-optimization-f1f393dcd36d\n\n\n\n\n\n<br>\n<hr>\n<br>","064f9d68":"<br>\n<hr>\n<br>","8a4293d6":"As we have already seen, we have many features making the set with a high dimension, something that we must deal with in the feature engineering stage.\n\n<br>","41a034dd":"Ages are formatted on a percentile scale, it is a variable that follows a uniform trend. ","94bb2acc":"### Tuning CatBoost ","56f4bcc7":"<br>\n<hr>\n<br>","484610fe":"Crossing the <b> DISEASE GROUPING 1 <\/b> feature with age, traits are also identified, that patients over 90 years old with this first group of diseases, have a high probability of being referred to ICU\n\n<br>","68301a2a":"<br>\n\n### 5. Feature Selection\n\nWe will apply a Feature selection technique, which will help to reduce the number of features helping to contain the complexity of the model.\n\n\nThe criteria for selecting the features used will be through the <b> ANOVA F-Value <\/b> is a way to see which features are most statistically relevant.\n\n\n\n\n<br>\n\n<b> What is ANOVA? <\/b>\n\n\nAnalysis of Variance or ANOVA is a procedure used to compare the distribution of three or more groups in independent samples.\n\n\nits associated estimation procedure (such as \"variation\" between groups) used to analyze differences between group means in a sample.\n\n  ANOVA is based on the law of total variation, in which the variation observed in a specific variable is divided into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test to determine whether two or more population averages are equal and, therefore, generalizes the t-test in addition to two means.\n\n\nmore about ANOVA: \n* https:\/\/www.youtube.com\/watch?v=CS_BKChyPuc\n* https:\/\/www.statisticshowto.com\/probability-and-statistics\/hypothesis-testing\/anova\/\n\n\n\n<br> \n\nIn the Scikit-learn package it is available in the feature_selection module and is called <b> f_classif <\/b>\n\n<br>\n<hr>\n<br>","d411fe45":"<br>\n<hr>\n<br>\n\n\n### <b>Conclusion<\/b> \n\nThis notebook from the beginning we elaborated the modeling, aiming to maximize the <b> Precision-Recall <\/b> of the model that we consider to be the most important task of this problem, to identify patients who need a bed in the ICU.\n\n<br>\n\nAs we saw that there were many features, we tried to reduce them at the discretion of an ANOVA test for the features that the test considered statistically significant, and we selected only 50 to reduce the complexity of the model (in another notebook that we did the part with only the feature selection , the best number of features for the models was 165 features, we chose to use only 50 to reduce complexity)\n\n<br>\n\nApplying three boosting algorithms, we expected better results in relation to the metrics, tuning the hyperparameters and using precision-recall as an objective function, there was also little impact on the predictive power of the models.\n\n<br>\n\n#### <b> Hypothesis suggestions <\/b>\n\n\nWe formulate some hypotheses to be tested in the future as possible best solutions.\n\n<br>\n\n* <b>Feature engineering:<\/b> Investing more time in this step, certainly because they are largely anonymous, makes it difficult to understand, and ends up limiting some attempts due to knowledge about the data, a better treatment with features can be promising (let's explore) the amount of missing values and how to allocate them is very delicate.\n\n<br>\n\n* <b>Amount of data:<\/b>analyzing the dataset carefully we find that there are 1925 lines, but that a patient is present in 5 lines and grouping them all, the base has about <b> 385 <\/b> patients that less than 250 of them went to the ICU, in in addition to the low amount of data, we can also explore resampling techniques to balance the base.\n\n\n<br>\n\n\n* <b> H2o AutoML: <\/b> Finally, another solution, which we can also try is to use the AutoML tool called <b> H2o <\/b> which has high performance in several solutions on the market, it had the Python library and also the <b> H2o AI driverless platform <\/b> where everything is done automatically, is a possibility to be considered.\n\n\n<br> \n\n\nWe thank you for opening and making the data available, and we will continue looking for other ways to solve this problem, immediately our ML model Stacking has <b> Precision-Recall: 81% <\/b> and it is our solution so far.\n\n\nAbove all we want this notebook to help other colleagues, if you have doubts you can write, we had some decisions and considerations throughout this solution that should not have been clear, so you can ask!\n\n<br>\n<hr>\n<br>","96583772":"<br>\n<hr>\n<br>\n<br>\n\n### 7. Validation \n\n\nIn this step we will create a robust cross-validation for each Gradient Boosting.\n\nWe chose to apply the <b> StratifiedKFold <\/b> which, unlike Sklearn's standard KFold, it maintains the proportion of the classes in each fold allowing no class imbalance in the validation.\n\n\n<br>\n<hr>\n<br>","45974afe":"<br>\n<hr>\n<br>\n<br>\n\n\n### 2. Exploratory Data analysis\n\nLet's explore the variables that most impact, the event that we are trying to predict.\n\nThere are 231 features composed in this dataset, we have a high dimension included.\n\nAs it is a set of medical data, the data are considered sensitive, because of this many features are already pre-processed and with their names changed, to prevent the data.\n\n\n\n\n<br>\n<hr>","34c7ff7f":"<br>\n<hr>\n<br>","f29a06ff":"<br>\n<hr>\n\n\n### 3. Feature engineering \n\nWe will use a simple approach in this step of transforming the features, to later create a general baseline of a single model.\n\n* Missing Values \n* Correct data type\n\n<br>\n<hr>\n<br>","41513a0e":"Looking at the patient's windows (observation) over time in hours, it follows something that we predicted that over time, if the patient already has a strong suspicion on his admission to the hospital, he can be inserted into the ICU on his arrival, or maybe in 0-2 hours, etc., the sooner we can predict whether the patient will need a bed or not, the better.","70d0a143":"<br>\n<hr>\n<br>","7db321b6":"<br>\n<hr>\n<br>\n\n\n### Conclusion Exploratory analysis \n\n<b> An analysis carried out, including a lot of knowledge from doctors and health professionals, already had on the effects of covid-19 in patients. <\/b>\n\n* Patients over 60 years old have a high probability of reaching the ICU after 12h\n\n* The age points a lot that we must observe much more the patients of high age, that it contains results in the examinations that proved that to some high discrepancy in the examinations.\n\n\n<b> Warning: As it is a dataset that contains many anonymous features, in a way we do not infer more conclusions due to the health knowledge factor, we made assumptions based on what we classify as the simplest to be analyzed, an analysis done alongside a doctor or professional of health would be of greater gain and strength. <\/b> \n\n\n\n<hr>\n<br>","54a252fd":"<br>\n<hr>\n<br>\n\n#### Tuning LightGBM\n\n<br>","6f98512b":"### 4. Baseline \n\n\nWe will develop a baseline, a model that will serve as a parameter in relation to the metrics, through it we will interpret how the later models will be performing compared to it.\n\nLet's apply a simple logistic regression, and see the performance of the model.\n\n<br>\n\n<b>WARNING:<\/b> As already seen in the exploratory analysis part, the metrics with the greatest focus will be <b> Precision, Recall and AUC <\/b> as we have already verified that there is a certain class imbalance, and we want the model to learn to identify a separation in patients who need ICU or not and those who in fact need immediate ICU referral.\n\n<br>\n<hr>\n<br>","a9989903":"### 1. Frameworks\n\n<br>"}}