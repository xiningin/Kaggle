{"cell_type":{"b9543a11":"code","009044d3":"code","3a3c05b0":"code","e966df9e":"code","ed1187f0":"code","4f8f2983":"code","9b9591f3":"code","435e43d4":"code","82b4adef":"code","1022ffca":"code","095772a4":"code","67268372":"code","8558fcfe":"code","280b3bc7":"code","2471d34d":"code","2289c1a3":"code","f30188e1":"code","8f5ebe14":"code","52ca6c6b":"code","6928eb91":"code","d41fd9d1":"code","5673e28b":"code","cebb270a":"code","8da2a99e":"code","598c0881":"code","4bef49f5":"code","06e4f2db":"code","aabee523":"code","df25df88":"code","b3e1400c":"code","484065c2":"code","8d169524":"code","a72e7fb6":"code","ddebd68d":"code","a70fcb64":"code","e89b66cf":"code","d196dca8":"code","745ce90e":"code","ab4d52bb":"code","159ced84":"code","ac4aba00":"code","08beb193":"code","d51eaa53":"code","bf991abe":"code","eb8ed76c":"code","b4eac12a":"code","a6288f99":"code","de904937":"code","c8e5550e":"code","86f42ee6":"code","185fa3b4":"code","21afab01":"code","d7b5b84d":"code","970131b9":"code","5740ee78":"code","0a86ffda":"code","dbe19437":"code","9774e6e7":"code","2a662db4":"code","244fd656":"code","019d5ca2":"code","58603f97":"code","e9aa3761":"code","0b36ee7c":"markdown","a425c967":"markdown","de436826":"markdown","3ec739f9":"markdown","64dfea53":"markdown","be0aff96":"markdown","88378d84":"markdown","f51a5f5f":"markdown","cb4a0b5b":"markdown","e4ff3556":"markdown","0b59f00e":"markdown","abc93146":"markdown","576a0806":"markdown","17e09c0f":"markdown","c2396e43":"markdown","2c9ac2db":"markdown","053a0bc9":"markdown","a958dc77":"markdown","ed3d8c9e":"markdown","b3a2adf7":"markdown","ae476766":"markdown","e39983da":"markdown","93ff53dc":"markdown","7dadbd45":"markdown","d420d8ae":"markdown","d86db4b2":"markdown","99d8a811":"markdown","8f37e9ab":"markdown","bc574874":"markdown","cbcc6d70":"markdown","3b9df088":"markdown","2a5f64c3":"markdown","a942f7e8":"markdown","ae951996":"markdown","51974cce":"markdown","9c3ba564":"markdown","f5367fdb":"markdown","b425d956":"markdown","11712fce":"markdown","2e1bdcab":"markdown","251963e9":"markdown","a40460e1":"markdown","5db07a48":"markdown"},"source":{"b9543a11":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nimport warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('ggplot')\nsns.set(style=\"ticks\", context = 'talk', palette = 'bright', rc={'figure.figsize':(11,8.27)})","009044d3":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ngen = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","3a3c05b0":"train.info()","e966df9e":"train.describe()","ed1187f0":"train.head()","4f8f2983":"print(\"Unique values in PassengerId Column:\", len(train['PassengerId'].unique()))\nprint(\"Unique values in Name Column:\", len(train['Name'].unique()))\nprint(\"Unique values in Ticket Column:\", len(train['Ticket'].unique()))","9b9591f3":"train.isnull().sum().sort_values(ascending=False)","435e43d4":"train['Survived'].value_counts(normalize=True).plot(kind = 'bar')","82b4adef":"sns.countplot(x='Sex', hue='Survived', data=train)\nsns.catplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train, height=9)","1022ffca":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train[train['Sex']=='female']\nmen = train[train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","095772a4":"sns.countplot(x='Embarked', hue='Survived', data=train)","67268372":"emb = train.groupby(['Embarked', 'Survived']).size()\nemb_pct = emb.groupby(level=0).apply(lambda x: 100*x\/float(x.sum()))\nemb_pct.to_frame().unstack().plot(kind='bar', stacked=True)\nplt.ylabel('Survival Chance in %')\ncurrent_handles, _ = plt.gca().get_legend_handles_labels()\nreversed_handles = reversed(current_handles)\n\nlabels = reversed(train['Survived'].unique())\n\nplt.legend(reversed_handles,labels,loc='lower right')\nplt.show()","8558fcfe":"sns.countplot(x='Pclass', hue=\"Survived\", data=train)","280b3bc7":"pc = train.groupby(['Pclass', 'Survived']).size()\npc_pct = pc.groupby(level=0).apply(lambda x: 100*x\/float(x.sum()))\npc_pct.to_frame().unstack().plot(kind='bar', stacked=True)\nplt.ylabel('Survival Chance in %')\ncurrent_handles, _ = plt.gca().get_legend_handles_labels()\nreversed_handles = reversed(current_handles)\n\nlabels = reversed(train['Survived'].unique())\n\nplt.legend(reversed_handles,labels,loc='lower right')\nplt.show()","2471d34d":"g = sns.FacetGrid(train, row='Embarked', size = 7)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex')\ng.add_legend()","2289c1a3":"train['Family'] = train['SibSp'] + train['Parch']\ntrain['Alone'] = 0\ntrain['Alone'] = np.where(train['Family']>0, 0, 1)\ntrain['Alone'].value_counts()","f30188e1":"test['Family'] = test['SibSp'] + test['Parch']\ntest['Alone'] = 0\ntest['Alone'] = np.where(test['Family']>0, 0, 1)\ntest['Alone'].value_counts()","8f5ebe14":"ax = pd.crosstab(train['Family'], train['Survived']).apply(lambda row: row\/row.sum(), axis=1).plot(kind='bar')\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.yticks([])\n\n# Add this loop to add the annotations\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x, y + height + 0.01))","52ca6c6b":"sns.catplot('Survived', 'Fare', data = train)","6928eb91":"sns.catplot('Pclass', 'Fare', data = train)","d41fd9d1":"train['Fare_range'] = pd.qcut(train['Fare'], 4)\nsns.barplot(x='Fare_range', y='Survived', data = train)","5673e28b":"train.isna().sum()","cebb270a":"train['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","8da2a99e":"train['Age'].fillna(round(train['Age'].mean(), 1), inplace=True)","598c0881":"train.drop(['Cabin', 'PassengerId', 'Ticket', 'Name', 'Fare_range', 'SibSp', 'Parch'], axis = 1, inplace = True)","4bef49f5":"test['Age'].fillna(round(test['Age'].mean(), 1), inplace=True)\ntest['Fare'].fillna(round(test['Fare'].mean(), 1), inplace=True)\ntest.drop(['Cabin', 'PassengerId', 'Ticket', 'Name', 'SibSp', 'Parch'], axis = 1, inplace = True)","06e4f2db":"test.isna().sum()","aabee523":"train['Sex'].replace('male', 1, inplace=True)\ntrain['Sex'].replace('female', 0, inplace=True)\n\ntrain['Embarked'].replace('S', 0, inplace=True)\ntrain['Embarked'].replace('Q', 1, inplace=True)\ntrain['Embarked'].replace('C', 2, inplace=True)","df25df88":"test['Sex'].replace('male', 1, inplace=True)\ntest['Sex'].replace('female', 0, inplace=True)\n\ntest['Embarked'].replace('S', 0, inplace=True)\ntest['Embarked'].replace('Q', 1, inplace=True)\ntest['Embarked'].replace('C', 2, inplace=True)","b3e1400c":"print('\\nTrain')\nprint(train.dtypes)\nprint('\\nTest')\nprint(test.dtypes)","484065c2":"X = train.drop('Survived', axis = 1)\nY = train['Survived']","8d169524":"from sklearn.model_selection import train_test_split\nX_train, X_cv, y_train, y_cv = train_test_split(X, Y, test_size = 0.2, random_state = 0)","a72e7fb6":"print('Training set {}, {}'.format(X_train.shape, y_train.shape))\nprint('Cross Validation set {}, {}'.format(X_cv.shape, y_cv.shape))","ddebd68d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_cv = sc.transform(X_cv)","a70fcb64":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense , Dropout , Lambda, Flatten\nfrom keras.optimizers import Adam ,RMSprop\nimport keras.backend as K\n\nfrom sklearn.model_selection import GridSearchCV, validation_curve\nfrom sklearn.metrics import f1_score, roc_auc_score, confusion_matrix","e89b66cf":"def plot_confusion_matrix(y_true, y_pred):\n    cf_matrix = confusion_matrix(y_true, y_pred)\n    group_names = ['True Neg', 'False Pos', 'False Neg', 'True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')","d196dca8":"report = pd.DataFrame(columns = ['Models', 'Train Accuracy', 'CV Accuracy', 'CV F1-Score']) ","745ce90e":"model = LogisticRegression(random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'Logistic Regression', \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","ab4d52bb":"plot_confusion_matrix(y_cv, y_pred)","159ced84":"model = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'KNN',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","ac4aba00":"plot_confusion_matrix(y_cv, y_pred)","08beb193":"model = SVC(kernel='linear', random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'SVC Linear',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","d51eaa53":"plot_confusion_matrix(y_cv, y_pred)","bf991abe":"model = SVC(kernel='rbf', random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'SVC RBF',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","eb8ed76c":"plot_confusion_matrix(y_cv, y_pred)","b4eac12a":"model = GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'Gaussian Naive Bayes',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","a6288f99":"plot_confusion_matrix(y_cv, y_pred)","de904937":"model = DecisionTreeClassifier(criterion='entropy', random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'Decision Tree',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","c8e5550e":"plot_confusion_matrix(y_cv, y_pred)","86f42ee6":"model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_cv)\n\nreport = report.append({'Models': 'Random Forests',  \n                        'Train Accuracy': round(model.score(X_train, y_train)*100, 2), 'CV Accuracy': round(model.score(X_cv, y_cv)*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","185fa3b4":"plot_confusion_matrix(y_cv, y_pred)","21afab01":"model = Sequential()\nmodel.add(Dense(12, input_dim=7, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=150, batch_size = 10, verbose=0)\n\n_, train_acc = model.evaluate(X_train, y_train)\n_, cv_acc = model.evaluate(X_cv, y_cv)\n\ny_pred = model.predict_classes(X_cv)\ny_pred.reshape(len(y_pred),)\n\nreport = report.append({'Models': 'Neural Network',  \n                        'Train Accuracy': round(train_acc*100, 2), 'CV Accuracy': round(cv_acc*100, 2), \n                        'CV F1-Score': f1_score(y_cv, y_pred)}, ignore_index=True)","d7b5b84d":"plot_confusion_matrix(y_cv, y_pred)","970131b9":"report.sort_values('CV F1-Score', ascending=False)","5740ee78":"model = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nmodel.fit(X, Y)\ny_pred = model.predict(test)","0a86ffda":"submission = pd.read_csv('..\/input\/titanic\/test.csv')","dbe19437":"submission = pd.concat([submission['PassengerId'], pd.DataFrame(y_pred)], axis=1)\nsubmission.columns = ['PassengerId', 'Survived']","9774e6e7":"submission","2a662db4":"submission.to_csv('submission.csv', index=False)","244fd656":"model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nmodel.fit(X, Y)\ny_pred = model.predict(test)","019d5ca2":"submission = pd.concat([submission['PassengerId'], pd.DataFrame(y_pred)], axis=1)\nsubmission.columns = ['PassengerId', 'Survived']\nsubmission.to_csv('submission.csv', index=False)","58603f97":"model = Sequential()\nmodel.add(Dense(12, input_dim=7, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, Y, epochs=150, batch_size = 10, verbose=0)\n\ny_pred = model.predict_classes(test)\ny_pred.reshape(len(y_pred),)","e9aa3761":"submission = pd.concat([submission['PassengerId'], pd.DataFrame(y_pred)], axis=1)\nsubmission.columns = ['PassengerId', 'Survived']\nsubmission.to_csv('submission.csv', index=False)","0b36ee7c":"The fare of ticket for class 3 and 2 is between 0 and 100 while for class 1 it is ranging from 0 to 500. There are outliers in this case which are in class 1. \n\nA much better understanding can be found by dividing the fare into different ranges and checking the survival chances.","a425c967":"Here we can notice that column Cabin is also having Nan values, which are supposed to treated before training the machine learning model. Also there are certain columns with categorical variables, for example Sex, Ticket, Embarked, Cabin. ","de436826":"Seperating the X and Y from our dataset. Also splitting the training set for training and cross validation","3ec739f9":"## Data Pre-processing\n\nAfter completing the analysis of the features we now move towards data pre-processing where we treat the missing values and outliers (if any). Conversion of categorical variables to numerical variables.","64dfea53":"### Support Vector Machine with Linear Kerneal","be0aff96":"Now for the Age column","88378d84":"With this plot we found that the passenger paying a higher fare would have higher survival chances as compared to others. So we might require creating seperate bins for Fare column","f51a5f5f":"* Gaussian Naive Bayes has an excellent True Positive Rate as compared to other models. But, it also has the worst True Negative Rate as compared to other models. Also it doesn't have a good training accuracy.\n* Decision Tree got an accuracy of 98.17% on training set but on the cross validation set it performs worst. Clearly the decision tree seems to be overfitting the training set.\n* Out of the other models Neural Networks, KNN and Random Forests seem to give interesting results. So we will move ahead with those models.","cb4a0b5b":"This submission did well and scored 76.08%","e4ff3556":"This is a better plot showing us the survival probability for each embarkment. Here we can see that passengers embarked at port C have the highest survival chance followed by port Q and lastly port S. \n\nBut only this knowledge is not sufficient we have to check the class and gender of the passengers. So then we can have a better understanding.","0b59f00e":"Converting categorical variables to numbers","abc93146":"From this plot we come to know that passenger travelling in class 3 have highest chance of not being able to survive. While passenger in class 1 have good chances of surviving. But lets see the probability of survival in each class","576a0806":"### Logistic Regression","17e09c0f":"Here there seems no direct relation between fare and survival, this can be thought of as the fare of the class in which the passenger is travelling.","c2396e43":"Here since the survived column has 2 values 0 and 1 the mean of the column i.e. 0.38384 tells us the % of passenger that survived. Also there are columns which have missing values, for example the Age column which has 714 entries filled out of 891.\n\nLets have a glimpse at the data","2c9ac2db":"Firstly we will replace the null values in emabrked column with the mode.","053a0bc9":"## Conclusion: \n\n* Performed EDA and model selection on titanic dataset to predict the survival of a passenger. \n* Found that features like SibSp and Parch are better when combined together. \n* Feature Cabin which had 77% missing values was neglected for the classification task. But one should do analysis of the feature and treat the missing values, for example cabin feature had alpha numeric values and if these values are some how changed to numbers using regular expression, then this might work as well\n* Performed feature engineering and created 2 new features from SibSp and Parch features.\n* Lastly for model selection I tried various models and then came up with 3 models on the basis on accuracy on training and cross validation accuracy, f1 score.\n* Surely these results are not the best results so for further improvements one can try k fold cross validation, grid search to get the best hyper parameters. Other than this one can look after feature engineering i.e. create more features with the help of Fare and Pclass. Also the Cabin feature can be used","a958dc77":"So here if the person is alone his survival chances are around 30%. Moreover if the family size is more than 4 then survival chances becomes very less.\n\nNow lets analyse the ticket fare","ed3d8c9e":"### K Nearest Neighbours","b3a2adf7":"So now we come to know that women at port S and Q have higher chances of surviving as compared to port C. The inverse is also true where men at port C have higher chances of survivng as compared to men at port S and Q.","ae476766":"Ticket, Name and PassengerId won't be useful in model training as they are unique values and moreover a passenger's name or id can not determine whether he will survive or not\n\nLets have a look at the total null values in each column","e39983da":"### Neural Network","93ff53dc":"Training KNN on the whole train set","7dadbd45":"Now lets do the same preprocessing for test dataset","d420d8ae":"This submission did well and scored 74.16%","d86db4b2":"Now since everything is fine we can start with model training","99d8a811":"So there are a lot of null values in the cabin column, so we might end up droping that column after doing further investogation. The Embarked column has only 2 null values which can easily be replaced by the mode of that column.\n\nBefore filling the null values lets analyse the features and identify which of these features would contribute to high survival rate.","8f37e9ab":"From the first graph it is found that women have a higher chance of survival than men. From the second graph it was found that children also have a higher survival chance as compared to men. Also women having age in 20-50 have a good survival chance. As the age increases men's survival chance decreases. For a better representation we will plot a distribution of passengers according to the age and sex","bc574874":"Now lets drop the redundant columns","cbcc6d70":"### Gaussian Naive Bayes","3b9df088":"Since the data is in a 60\/40 split we won't require balancing the labels. Also accuracy can be a decent measure for model training ","2a5f64c3":"So there are a total of 891 samples in train set with 11 features and 1 target variable (Survived).\n\nSo what does each feature mean?\n\n* Name: Passenger name\n* Pclass: passenger class i.e. the class in which the passenger is travelling\n* Survived: Survival of the passenger (1 if the passenger survives else 0) \n* PassengerId: Unique Id of a passenger. \n* pclass: Ticket class     \n* Sex: Sex     \n* Age: Age in years     \n* sibsp: # of siblings \/ spouses aboard the Titanic     \n* parch: # of parents \/ children aboard the Titanic     \n* ticket: Ticket number     \n* fare: Passenger fare     \n* cabin: Cabin number     \n* embarked: Port of Embarkation","a942f7e8":"So probability of survival in class 3 is minimum and for class 1 it is maximum. It can be very much understood that passenger travelling in higher class are given more preference as compared to the other classes. So as the passenger class degrades the chances of survival also decreases.","ae951996":"### Support Vector Machine with Radial Basis Function","51974cce":"### Random Forests","9c3ba564":"This submission could only score upto 64.83%","f5367fdb":"So from this plot there is clear idea that women having age above 20 have the highest survival chance. This can be proven as the old women are saved first then the young women and children are saved and at the end men are saved before the Titanic sank. So beacuse of the variation in age the survival chances vary therefore we might end up creating different age groups.","b425d956":"### 1. Age of Passenger","11712fce":"### Decision Tree Classifier","2e1bdcab":"## Model Selection","251963e9":"The majority of passengers are embarked at port S. At port S the survival chance is less.","a40460e1":"## Exploratory Data Analysis","5db07a48":"Now lets check for the sibsp and parch features. These features basically give us the size of a family, so they make more sense when combined together. Also we will check if the passenger is travelling alone, this will give us the chances of a lone passenger."}}