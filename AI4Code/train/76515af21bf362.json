{"cell_type":{"cf6eb31c":"code","0cd47793":"code","1469f0c9":"code","efa106ee":"code","38ece312":"code","05443af0":"code","7ca133fc":"code","c25458bc":"code","2eca5668":"code","736bacf3":"code","195bf009":"code","f0d38566":"code","44c47ae2":"code","f0e02063":"code","26b7169a":"code","2a7000e6":"code","2820407f":"code","3b04ac43":"code","620e0101":"code","31b74fe0":"code","9f321e87":"code","586363c9":"code","ac704545":"markdown","8eba07c5":"markdown","47c60a0e":"markdown","5c260c80":"markdown","e0a04952":"markdown","443487a7":"markdown"},"source":{"cf6eb31c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport scipy.stats as st\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\n\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nSEED = 0","0cd47793":"DATA_FPATH = '\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv'\ndf = pd.read_csv(DATA_FPATH, parse_dates=['date'])","1469f0c9":"def weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)","efa106ee":"# delete all negative and outliers values\n# probaply even better delete also zero values\ndf.drop(df.loc[(df['CPM'] < 0) | (df['CPM'] > df['CPM'].quantile(q=0.95))].index, inplace=True)\n\n# delete all datafields related with CPM: 'total_revenue', 'measurable_impressions', 'viewable_impressions', 'revenue_share_percent', 'total_impressions', 'integration_type_id'\n# delete all datafields with poor value representation : 'integration_type_id', 'ad_type_id'\ndf.drop(['total_revenue', 'measurable_impressions', 'viewable_impressions', 'revenue_share_percent', 'total_impressions', 'integration_type_id', 'ad_type_id'], axis=1, inplace=True)\n","38ece312":"# split dataset to train and test based on date\n\ndf_train = df.loc[df.date < datetime(2019, 6, 22)].copy()\ndf_train.sort_values(by='date', inplace=True)\ndf_train.drop('date', axis=1, inplace=True)\n\ndf_test = df.loc[df.date >= datetime(2019, 6, 22)].copy()\ndf_test.drop('date', axis=1, inplace=True)","05443af0":"sns.distplot(df_train['CPM'])","7ca133fc":"sns.distplot(df_train['CPM'].loc[df_train['CPM'] > 0], kde=False)","c25458bc":"zero_cnt = df_train['CPM'].loc[df_train['CPM'] == 0].count()\nnot_zero_cnt = df_train['CPM'].loc[df_train['CPM'] > 0].count()\nprint(f'{zero_cnt} items with CMP equal 0') \nprint(f'{not_zero_cnt} items with CMP more than 0') ","2eca5668":"df_train.info()","736bacf3":"df_train.nunique()","195bf009":"fig, ax = plt.subplots(3, 3)\nfig.set_size_inches(25, 16)\nfig.subplots_adjust(wspace=0.2, hspace=0.3)\n\nfor i, column in enumerate(df_train.columns):\n    if column == 'CPM':\n        continue\n    sns.countplot(df_train[column], ax=ax[i % 3][i \/\/ 3])\n    if (i % 3 == 1 and i \/\/ 3 in {0, 1}) or (i % 3 == 2 and i \/\/ 3 == 2):\n        ax[i % 3][i \/\/ 3].tick_params(axis=\"x\", rotation=90, labelsize=4)\n    if i % 3 == 0 and i \/\/ 3 == 1:\n        ax[i % 3][i \/\/ 3].tick_params(axis=\"x\", rotation=90)","f0d38566":"class DataPipeline:\n    def fit(self, df):\n\n        # target encoding on geo_id\n        geo_id_grouped = df.groupby('geo_id').agg({'order_id' : 'count', 'CPM': 'mean'}).reset_index().rename(columns={'order_id': 'cnt', 'CPM': 'avr_CPM_geo_id'})\n        geo_id_grouped.drop(geo_id_grouped.loc[geo_id_grouped.cnt < 50].index, inplace=True)\n        geo_id_grouped.drop('cnt', axis=1, inplace=True)\n        self.geo_id_grouped = geo_id_grouped\n        \n        # target encoding on advertiser_id\n        advertiser_id_grouped = df.groupby('advertiser_id').agg({'order_id' : 'count', 'CPM': 'mean'}).reset_index().rename(columns={'order_id': 'cnt', 'CPM': 'avr_CPM_advertiser_id'})\n        advertiser_id_grouped.drop(advertiser_id_grouped.loc[advertiser_id_grouped.cnt < 50].index, inplace=True)\n        advertiser_id_grouped.drop('cnt', axis=1, inplace=True)\n        self.advertiser_id_grouped = advertiser_id_grouped\n        \n        # target encoding on order_id\n        order_id_grouped = df.groupby('order_id').agg({'advertiser_id' : 'count', 'CPM': 'mean'}).reset_index().rename(columns={'advertiser_id': 'cnt', 'CPM': 'avr_CPM_order_id'})\n        order_id_grouped.drop(order_id_grouped.loc[order_id_grouped.cnt < 50].index, inplace=True)\n        order_id_grouped.drop('cnt', axis=1, inplace=True)\n        self.order_id_grouped = order_id_grouped\n        \n        # target encoding on ad_unit_id\n        ad_unit_id_grouped = df.groupby('ad_unit_id').agg({'advertiser_id' : 'count', 'CPM': 'mean'}).reset_index().rename(columns={'advertiser_id': 'cnt', 'CPM': 'avr_CPM_ad_unit_id'})\n        ad_unit_id_grouped.drop(ad_unit_id_grouped.loc[ad_unit_id_grouped.cnt < 50].index, inplace=True)\n        ad_unit_id_grouped.drop('cnt', axis=1, inplace=True)\n        self.ad_unit_id_grouped = ad_unit_id_grouped\n        \n        # modes calculation\n        self.modes = df.mode()\n        \n        # device_category_id_val_cnt\n        self.rare_values = {}\n        \n        for x in df.columns:\n            if x == 'CPM':\n                continue\n            \n            val_cnt = df[x].value_counts()\n            self.rare_values[x] = val_cnt[val_cnt < 0.05 * val_cnt.sum()].index\n        \n        \n    def transform(self, df):\n        df = df.copy()\n        \n        # target encoding on geo_id\n        df = df.merge(self.geo_id_grouped, on='geo_id', how='left')\n        \n        # target encoding on advertiser_id\n        df = df.merge(self.advertiser_id_grouped, on='advertiser_id', how='left')\n        \n        # target encoding on order_id\n        df = df.merge(self.order_id_grouped, on='order_id', how='left')\n        \n        # target encoding on ad_unit_id\n        df = df.merge(self.ad_unit_id_grouped, on='ad_unit_id', how='left')\n        \n        # fill rare (with representation less than 5 percentages) values with corresponding mode\n        for x in self.rare_values.keys():\n            if x == 'CPM':\n                continue\n            \n            outliers_column_name = f'{x}_outliers'\n            df[outliers_column_name] = 0\n            \n            for y in self.rare_values[x]:\n                df.loc[df[x] == y, [x, outliers_column_name]] = self.modes[x], 1\n        \n        # separate target\n        y = df['CPM']\n        df.drop('CPM', axis=1, inplace=True)\n        \n        return df, y","44c47ae2":"preprocessor = DataPipeline()\npreprocessor.fit(df_train)\ndf_train_preprocessed, y = preprocessor.transform(df_train)\nX_test, y_test = preprocessor.transform(df_test)","f0e02063":"X_train, X_valid, y_train, y_valid = train_test_split(df_train_preprocessed, y, test_size=0.25, shuffle=False)","26b7169a":"params = {\n    'cat_features': df_train_preprocessed.select_dtypes('int64').columns,\n    'verbose': 200,\n    'random_seed': SEED,\n    'iterations': 10000,\n    'task_type': 'GPU',\n    'border_count': 32,\n    'learning_rate': 0.75,\n    'eval_metric': 'RMSE',\n    'l2_leaf_reg': 6,\n    'early_stopping_rounds': 200,\n}\n\ncbr = CatBoostRegressor(**params)","2a7000e6":"cbr.fit(X_train, y_train, eval_set=(X_valid, y_valid))","2820407f":"print(f'valid mse {mean_squared_error(y_valid, cbr.predict(X_valid))}')\nprint(f'test mse {mean_squared_error(y_test, cbr.predict(X_test))}')","3b04ac43":"def plot_feature_importances(importances, X):\n    \n    indices = np.argsort(importances)[:-50:-1]\n\n    plt.figure(figsize = (20, 8))\n    plt.title(\"Feature importances\", fontsize=16)\n    plt.bar(range(len(indices)), importances[indices] \/ importances.sum(),\n           color=\"darkblue\", align=\"center\")\n    plt.xticks(range(len(indices)), X.columns[indices], rotation = 90, fontsize=14)\n    plt.xlim([-1, len(indices)])\n\n    plt.tight_layout()\n    # plt.savefig('fe.jpg')\n    plt.show()\n    \n    return indices\n    \nind = plot_feature_importances(importances = cbr.get_feature_importance(), X=X_train)","620e0101":"# use only first 12 features\nn_features = 12\nX_train_chopped = X_train[X_train.columns[ind][:n_features]]\nX_valid_chopped = X_valid[X_valid.columns[ind][:n_features]]\nX_test_chopped = X_test[X_test.columns[ind][:n_features]]","31b74fe0":"params = {\n    'cat_features': X_train_chopped.select_dtypes('int64').columns,\n    'verbose': 200,\n    'random_seed': SEED,\n    'iterations': 10000,\n    'task_type': 'GPU',\n    'border_count': 32,\n    'learning_rate': 0.75,\n    'eval_metric': 'RMSE',\n    'l2_leaf_reg': 6,\n    'early_stopping_rounds': 200,\n}\n\ncbr = CatBoostRegressor(**params)","9f321e87":"cbr.fit(X_train_chopped, y_train, eval_set=(X_valid_chopped, y_valid))","586363c9":"mean_squared_error(y_test, cbr.predict(X_test_chopped))","ac704545":"# Short EDA","8eba07c5":"# Load dataset","47c60a0e":"# Model","5c260c80":"# Features selection","e0a04952":"## CPM","443487a7":"# Data Preprocessing"}}