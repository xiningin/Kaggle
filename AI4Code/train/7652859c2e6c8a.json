{"cell_type":{"11a07998":"code","a358aee1":"code","6a5401a7":"code","2f77d3da":"code","2d221aaf":"code","365b5811":"code","f7d4d108":"code","5e6c913c":"code","1b56d0cf":"code","6c12ddf7":"code","13756b7d":"code","b0db6e7d":"code","2aa6ec4b":"code","8bdd0b5d":"code","df2f77b6":"code","a0a5f415":"code","66c1fa27":"code","85bb8ae7":"code","58fcf55c":"code","7ed985d2":"code","31b7f33a":"code","b69d0f0c":"code","aadcba26":"code","4c98bc13":"code","6fd4ade6":"code","d2474838":"code","958d2138":"code","6f68fd0e":"code","9e26bb34":"code","bc6f4f98":"markdown","2db124bc":"markdown","2975282c":"markdown","06da2cae":"markdown","c26cc817":"markdown","d46094cc":"markdown","ddc4cb1e":"markdown","24141b2d":"markdown","c4f024aa":"markdown","dce6b82d":"markdown","bcb40833":"markdown","e2a1f983":"markdown","5417b765":"markdown","ee5bf814":"markdown"},"source":{"11a07998":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","a358aee1":"#1. Data collection\n\nimport pandas as pd \n\n# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/nida-competition1\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/nida-competition1\/test.csv\")\n\n# preview train data\ntrain_df.head()","6a5401a7":"train_data = train_df.copy()\ntrain_data['y'] = np.where(train_df['y']=='yes',1,0)","2f77d3da":"train_data = train_data.replace('unknown',np.nan)\nprint(train_data.isnull().sum())\n\n#missing data\nprint('Percent of missing \"job\" records is %.2f%%' %((train_data['job'].isnull().sum()\/train_data.shape[0])*100))\nprint('Percent of missing \"marital\" records is %.2f%%' %((train_data['marital'].isnull().sum()\/train_data.shape[0])*100))\nprint('Percent of missing \"education\" records is %.2f%%' %((train_data['education'].isnull().sum()\/train_data.shape[0])*100))\nprint('Percent of missing \"default\" records is %.2f%%' %((train_data['default'].isnull().sum()\/train_data.shape[0])*100))\nprint('Percent of missing \"housing\" records is %.2f%%' %((train_data['housing'].isnull().sum()\/train_data.shape[0])*100))\nprint('Percent of missing \"loan\" records is %.2f%%' %((train_data['loan'].isnull().sum()\/train_data.shape[0])*100))","2d221aaf":"# Explore Job Data\nprint(train_df['job'].value_counts())\njob = sns.countplot(x='job', data=train_df, palette='Set2')\njob.set_xticklabels(train_df['job'],rotation=90)\nplt.show()","365b5811":"# impute job with the most job \nprint('The most common job is %s.' %train_data['job'].value_counts().idxmax())","f7d4d108":"# Explore marital Data\nprint(train_df['marital'].value_counts())\nmarital = sns.countplot(x='marital', data=train_df, palette='Set1')\nplt.show()","5e6c913c":"# impute marital with the most job \nprint('The most common marital is %s.' %train_data['marital'].value_counts().idxmax())","1b56d0cf":"# Explore education Data\nprint(train_df['education'].value_counts())\neducation = sns.countplot(x='education', data=train_df, palette='Set1')\neducation.set_xticklabels(train_df['education'],rotation=90)\nplt.show()","6c12ddf7":"# impute education with the most job \nprint('The most common education is %s.' %train_data['education'].value_counts().idxmax())","13756b7d":"# Explore default Data\nprint(train_df['default'].value_counts())\neducation = sns.countplot(x='default', data=train_df, palette='Set3')\nplt.show()","b0db6e7d":"# impute education with the most job \nprint('The most common default is %s.' %train_data['default'].value_counts().idxmax())","2aa6ec4b":"# Explore housing Data\nprint(train_df['housing'].value_counts())\neducation = sns.countplot(x='housing', data=train_df, palette='Set3')\nplt.show()","8bdd0b5d":"# impute education with the most job \nprint('The most common housing is %s.' %train_data['housing'].value_counts().idxmax())","df2f77b6":"# Explore loan Data\nprint(train_df['loan'].value_counts())\neducation = sns.countplot(x='loan', data=train_df, palette='Set3')\nplt.show()","a0a5f415":"# impute loan with the most job \nprint('The most common loan is %s.' %train_data['loan'].value_counts().idxmax())","66c1fa27":"train_data[\"job\"].fillna(train_df['job'].value_counts().idxmax(), inplace=True)\ntrain_data[\"marital\"].fillna(train_df['marital'].value_counts().idxmax(), inplace=True)\ntrain_data[\"education\"].fillna(train_df['education'].value_counts().idxmax(), inplace=True)\ntrain_data[\"default\"].fillna(train_df['default'].value_counts().idxmax(), inplace=True)\ntrain_data[\"housing\"].fillna(train_df['housing'].value_counts().idxmax(), inplace=True)\ntrain_data[\"loan\"].fillna(train_df['loan'].value_counts().idxmax(), inplace=True)","85bb8ae7":"sns.barplot('poutcome', 'y', data=train_data, color=\"darkturquoise\")\nplt.show()","58fcf55c":"train_data['default'] = np.where(train_df['default']=='yes',1,0)\ntrain_data['housing'] = np.where(train_df['housing']=='yes',1,0)\ntrain_data['loan'] = np.where(train_df['loan']=='yes',1,0)\n\n#divide poutcome to 2 group\ntrain_data['poutcome_new'] = np.where(train_df['poutcome']=='success',1,0)\n\ntraining = pd.get_dummies(train_data, columns=[\"job\",\"marital\",\"education\",\"contact\",])\ntraining.drop('month', axis=1, inplace=True)\ntraining.drop('day_of_week', axis=1, inplace=True)\n\ntraining.drop('pdays', axis=1, inplace=True)\ntraining.drop('poutcome', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()","7ed985d2":"test_data = test_df.copy()\ntest_data = test_data.replace('unknown',np.nan)\ntest_data.isnull().sum()","31b7f33a":"# impute test data\ntest_data[\"job\"].fillna(train_df['job'].value_counts().idxmax(), inplace=True)\ntest_data[\"marital\"].fillna(train_df['marital'].value_counts().idxmax(), inplace=True)\ntest_data[\"education\"].fillna(train_df['education'].value_counts().idxmax(), inplace=True)\ntest_data[\"default\"].fillna(train_df['default'].value_counts().idxmax(), inplace=True)\ntest_data[\"housing\"].fillna(train_df['housing'].value_counts().idxmax(), inplace=True)\ntest_data[\"loan\"].fillna(train_df['loan'].value_counts().idxmax(), inplace=True)","b69d0f0c":"test_data['default'] = np.where(test_df['default']=='yes',1,0)\ntest_data['housing'] = np.where(test_df['housing']=='yes',1,0)\ntest_data['loan'] = np.where(test_df['loan']=='yes',1,0)\n\n#divide poutcome to 2 group\ntest_data['poutcome_new'] = np.where(test_df['poutcome']=='success',1,0)\n\ntesting = pd.get_dummies(test_data, columns=[\"job\",\"marital\",\"education\",\"contact\"])\ntesting.drop('month', axis=1, inplace=True)\ntesting.drop('day_of_week', axis=1, inplace=True)\n\ntesting.drop('pdays', axis=1, inplace=True)\ntesting.drop('poutcome', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","aadcba26":"#Over Sampling\nmax_size = final_train['y'].value_counts().max()\nlst = [final_train]\nfor class_index, group in final_train.groupby('y'):\n    lst.append(group.sample(max_size-len(group), replace=True))\nframe_new = pd.concat(lst)\nfinal_train = frame_new\nfinal_train['y'].value_counts()","4c98bc13":"# Explore Data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(20,8))\navg_y_byage = train_data[[\"age\", \"y\"]].groupby(['age'], as_index=False).mean()\ng = sns.barplot(x='age', y='y', data=avg_y_byage, color=\"LightSeaGreen\")\ng.set_xticklabels(avg_y_byage[\"age\"],rotation=90)\nplt.show()","6fd4ade6":"final_train['IsMajor']=np.where(final_train['age']>60, 1, 0)\nfinal_test['IsMajor']=np.where(final_test['age']>60, 1, 0)\nfinal_train.head()","d2474838":"final_test.head()","958d2138":"train_res = final_train['y']\nfinal_train.drop('y',axis =1,inplace=True)","6f68fd0e":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(final_train, train_res, test_size=0.1, random_state=42)\nns_probs = [0 for _ in range(len(y_test))]\n# clf3 = xgb.DMatrixr(n_estimators=100,learning_rate=1.0)\nxg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\nmodel = xgb.DMatrix(X_train, y_train)\nlr_probs = model.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\nns_auc = roc_auc_score(y_test, ns_probs)\nlr_auc = roc_auc_score(y_test, lr_probs)\n\n\nprint('No Skill: ROC AUC=%.3f' % (ns_auc))\nprint('RandomForest: ROC AUC=%.3f' % (lr_auc))\n\n\nns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n\n\npyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\npyplot.plot(lr_fpr, lr_tpr, marker='.', label='RandomForest')\n\n\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\n\n\npyplot.legend()\n\n\npyplot.show()","9e26bb34":"predicty = model.predict_proba(final_test)\ndf = pd.DataFrame(predicty)\ndf.to_csv(\".\/submission.csv\",index=False)","bc6f4f98":"## 1. Data Collection","2db124bc":"### 2.2 marital","2975282c":"### 2.4 default","06da2cae":"## 2. Data cleaning","c26cc817":"### 2.3 education","d46094cc":"### 2.6 loan","ddc4cb1e":"## MEMBER\n\n### 1. \u0e19\u0e32\u0e22\u0e18\u0e19\u0e31\u0e15\u0e16\u0e4c\u0e01\u0e23\u0e13\u0e4c \u0e0a\u0e37\u0e48\u0e19\u0e1a\u0e23\u0e23\u0e25\u0e37\u0e2d\u0e2a\u0e38\u0e02  6310422031\n### 2. \u0e19\u0e32\u0e22\u0e28\u0e38\u0e20\u0e13\u0e31\u0e10 \u0e44\u0e17\u0e22\u0e1b\u0e23\u0e30\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e4c  6310422034\n### 3. \u0e19\u0e32\u0e22\u0e2d\u0e19\u0e38\u0e27\u0e31\u0e15 \u0e21\u0e32\u0e25\u0e34\u0e19\u0e35  6310422036\n### 4. \u0e19\u0e32\u0e22\u0e01\u0e27\u0e34\u0e19 \u0e2a\u0e34\u0e07\u0e2b\u0e40\u0e02\u0e15\u0e15\u0e4c  6310422040\n### 5. \u0e19\u0e32\u0e22\u0e1e\u0e35\u0e23\u0e13\u0e31\u0e10 \u0e40\u0e0a\u0e37\u0e49\u0e2d\u0e2a\u0e27\u0e22 6310422042","24141b2d":"## Private Score 0.94460","c4f024aa":"### 2.7 Final Impute","dce6b82d":"### 2.5 housing","bcb40833":"### 2.1 Job Data","e2a1f983":"#### 2.8.1 Age","5417b765":"## 3. Model Training","ee5bf814":"### 2.8 Additional Variable"}}