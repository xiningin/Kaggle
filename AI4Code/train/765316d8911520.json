{"cell_type":{"030918bf":"code","b7d147fa":"code","72ff27f2":"code","7cb310f5":"code","713ed3e8":"code","91fe9f60":"code","b514a86a":"code","d1b466c6":"code","448e5f2a":"code","8068e7a3":"code","a8adcdbb":"code","d202c088":"code","d22d6ad5":"code","0d38ba23":"code","05a74628":"code","41b12b74":"code","fa83406d":"code","bfbce285":"code","64086886":"code","64aad106":"code","bf4d3cc7":"code","cfa372bf":"code","3feeee29":"code","0dcfb89b":"code","363850d6":"code","58f92be9":"code","c999636b":"code","442df49e":"code","cb921f5d":"code","a36f6504":"markdown","53b455bb":"markdown","9964d002":"markdown","7fe0010a":"markdown","84e0a879":"markdown","1185107e":"markdown","f0a40d49":"markdown","835dc16a":"markdown","5e06f1dd":"markdown","6c785ecc":"markdown","6b527dba":"markdown","13cb30ac":"markdown","06d78761":"markdown","f507f67e":"markdown","96d3ddf9":"markdown","448d9a02":"markdown","10f334d3":"markdown","36596f5e":"markdown","50a75172":"markdown","ce736bb4":"markdown","d76b39c8":"markdown","4778b6de":"markdown","fdc2a5a2":"markdown","88790dc8":"markdown","750bc919":"markdown","c12d0299":"markdown","614bd013":"markdown","98290dfd":"markdown","ce8b4661":"markdown"},"source":{"030918bf":"from datetime import datetime\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 200)\nfrom scipy.stats import probplot\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 42\nPATH = '..\/input\/'","b7d147fa":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:1459], all_data.loc[1460:].drop(['SalePrice'], axis=1)\n\ndf_train = pd.read_csv(PATH + 'train.csv')\ndf_test = pd.read_csv(PATH + 'test.csv')\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['SalePrice'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))","72ff27f2":"print(df_all.info())\ndf_all.sample(5)","7cb310f5":"def display_missing(df):    \n    for col in df.columns.tolist():\n        if df[col].isnull().sum():\n            print('{} column missing values: {}\/{}'.format(col, df[col].isnull().sum(), len(df)))\n    print('\\n')\n    \nfor df in dfs:\n    print('{}'.format(df.name))\n    display_missing(df)","713ed3e8":"# Filling masonry veneer features\ndf_all['MasVnrArea'] = df_all['MasVnrArea'].fillna(0)\ndf_all['MasVnrType'] = df_all['MasVnrType'].fillna('None')\n\n# Filling continuous basement features\nfor feature in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']:\n    df_all[feature] = df_all[feature].fillna(0)\n\n# Filling categorical basement features\nfor feature in ['BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual']:\n    df_all[feature] = df_all[feature].fillna('None')\n\n# Filling continuous garage features\nfor feature in ['GarageArea', 'GarageCars', 'GarageYrBlt']:\n    df_all[feature] = df_all[feature].fillna(0)\n\n# Filling categorical garage features\nfor feature in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    df_all[feature] = df_all[feature].fillna('None')\n    \n# Filling other categorical features\nfor feature in ['Alley', 'Fence', 'FireplaceQu', 'MiscFeature', 'PoolQC']:\n    df_all[feature] = df_all[feature].fillna('None')\n\ndisplay_missing(df_all)","91fe9f60":"# Filling missing values in categorical features with the mode value of neighborhood and house type\nfor feature in ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual', 'MSZoning', 'SaleType', 'Utilities']:\n    df_all[feature] = df_all.groupby(['Neighborhood', 'MSSubClass'])[feature].apply(lambda x: x.fillna(x.mode()[0]))\n\n# Filling the missing values in LotFrontage with the median of neighborhood\ndf_all['LotFrontage'] = df_all.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))\n\ndisplay_missing(df_all)","b514a86a":"print('Training Set SalePrice Skew: {}'.format(df_train['SalePrice'].skew()))\nprint('Training Set SalePrice Kurtosis: {}'.format(df_train['SalePrice'].kurt()))\nprint('Training Set SalePrice Mean: {}'.format(df_train['SalePrice'].mean()))\nprint('Training Set SalePrice Median: {}'.format(df_train['SalePrice'].median()))\nprint('Training Set SalePrice Max: {}'.format(df_train['SalePrice'].max()))\n\nfig, axs = plt.subplots(nrows=2, figsize=(16, 16))\nplt.subplots_adjust(left=None, bottom=5, right=None, top=6, wspace=None, hspace=None)\n\nsns.distplot(df_train['SalePrice'], hist=True, ax=axs[0])\nprobplot(df_train['SalePrice'], plot=axs[1])\n\naxs[0].set_xlabel('Sale Price', size=12.5, labelpad=12.5)\naxs[1].set_xlabel('Theoretical Quantiles', size=12.5, labelpad=12.5)\naxs[1].set_ylabel('Ordered Values', size=12.5, labelpad=12.5)\n\nfor i in range(2):\n    axs[i].tick_params(axis='x', labelsize=12.5)\n    axs[i].tick_params(axis='y', labelsize=12.5)\n\naxs[0].set_title('Distribution of Sale Price in Training Set', size=15, y=1.05)\naxs[1].set_title('Sale Price Probability Plot', size=15, y=1.05)\n\nplt.show()","d1b466c6":"df_train, df_test = divide_df(df_all)\n# Dropping categorical features\ncols = ['GarageYrBlt', 'Id', 'MSSubClass', 'MoSold', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n\ndf_train_corr = df_train.drop(cols, axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\ndf_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n\ndf_test_corr = df_test.drop(cols, axis=1).corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\ndf_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\ndf_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)","448e5f2a":"# Features correlated with target\ndf_train_corr_nd[df_train_corr_nd['Feature 1'] == 'SalePrice']","8068e7a3":"# Training set high correlations\ndf_train_corr_nd.head(10)","a8adcdbb":"fig, axs = plt.subplots(nrows=2, figsize=(50, 50))\n\nsns.heatmap(df_train.drop(cols, axis=1).corr().round(2), ax=axs[0], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})\nsns.heatmap(df_test.drop(cols, axis=1).corr().round(2), ax=axs[1], annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})\n\nfor i in range(2):    \n    axs[i].tick_params(axis='x', labelsize=13)\n    axs[i].tick_params(axis='y', labelsize=13)\n    \naxs[0].set_title('Training Set Correlations', size=15)\naxs[1].set_title('Test Set Correlations', size=15)\n\nplt.show()","d202c088":"num_features = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2', \n                'BsmtUnfSF', 'EnclosedPorch', 'GarageArea', 'GarageYrBlt', 'GrLivArea', \n                'LotArea', 'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal', \n                'OpenPorchSF', 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF', \n                'YearBuilt', 'YearRemodAdd']\n\nfig, axs = plt.subplots(ncols=2, nrows=11, figsize=(12, 80))\nplt.subplots_adjust(right=1.5)\ncmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(11, 2, i)\n    sns.scatterplot(x=feature, y='SalePrice', hue='SalePrice', size='SalePrice', palette=cmap, data=df_train)\n        \n    plt.xlabel('{}'.format(feature), size=15)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 12})\n        \nplt.show()","d22d6ad5":"cat_features = ['Alley', 'BedroomAbvGr', 'BldgType', 'BsmtCond', 'BsmtExposure', \n                'BsmtFinType1', 'BsmtFinType2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtQual', \n                'CentralAir', 'Condition1', 'Condition2', 'Electrical', 'ExterCond', \n                'ExterQual', 'Exterior1st', 'Exterior2nd', 'Fence', 'FireplaceQu', \n                'Fireplaces', 'Foundation', 'FullBath', 'Functional', 'GarageCars', \n                'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'HalfBath', \n                'Heating', 'HeatingQC', 'KitchenAbvGr', 'KitchenQual', 'LandContour', \n                'LandSlope', 'LotConfig', 'LotShape', 'MSSubClass', 'MSZoning', \n                'MasVnrType', 'MiscFeature', 'MoSold', 'Neighborhood', 'OverallCond', \n                'OverallQual', 'PavedDrive', 'PoolQC', 'RoofMatl', 'RoofStyle', \n                'SaleCondition', 'SaleType', 'Street', 'TotRmsAbvGrd', 'Utilities', 'YrSold']\n\nfig, axs = plt.subplots(ncols=2, nrows=28, figsize=(18, 120))\nplt.subplots_adjust(right=1.5, top=1.5)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(28, 2, i)\n    sns.swarmplot(x=feature, y='SalePrice', data=df_train, palette='Set3')\n        \n    plt.xlabel('{}'.format(feature), size=25)\n    plt.ylabel('SalePrice', size=25, labelpad=15)\n    \n    for j in range(2):\n        if df_train[feature].value_counts().shape[0] > 10:        \n            plt.tick_params(axis='x', labelsize=7)\n        else:\n            plt.tick_params(axis='x', labelsize=20)\n        plt.tick_params(axis='y', labelsize=20)\n            \nplt.show()","0d38ba23":"fig, axs = plt.subplots(ncols=2, nrows=11, figsize=(12, 80))\nplt.subplots_adjust(right=1.5)\n\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(11, 2, i)\n    sns.kdeplot(df_train[feature], bw='silverman', label='Training Set', shade=True)\n    sns.kdeplot(df_test[feature], bw='silverman', label='Test Set', shade=True)\n        \n    plt.xlabel('{}'.format(feature), size=15)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(loc='best', prop={'size': 15})\n        \nplt.show()","05a74628":"df_train['Dataset'] = 'Training Set'\ndf_test['Dataset'] = 'Test Set'\ndf_all = concat_df(df_train, df_test)\n\nfig, axs = plt.subplots(ncols=2, nrows=28, figsize=(18, 120))\nplt.subplots_adjust(right=1.5, top=1.5)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(28, 2, i)\n    sns.countplot(x=feature, hue='Dataset', data=df_all, palette='Set2')\n        \n    plt.xlabel('{}'.format(feature), size=25)\n    plt.ylabel('Count', size=25)\n    \n    for j in range(2):\n        if df_train[feature].value_counts().shape[0] > 10:        \n            plt.tick_params(axis='x', labelsize=7)\n        else:\n            plt.tick_params(axis='x', labelsize=20)\n        plt.tick_params(axis='y', labelsize=20)\n        \n    plt.legend(loc='upper right', prop={'size': 15})\n            \nplt.show()","41b12b74":"df_all['YearBuiltRemod'] = df_all['YearBuilt'] + df_all['YearRemodAdd']\ndf_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\ndf_all['TotalSquareFootage'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\ndf_all['TotalBath'] = df_all['FullBath'] + (0.5 * df_all['HalfBath']) + df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath'])\ndf_all['TotalPorchSF'] = df_all['OpenPorchSF'] + df_all['3SsnPorch'] + df_all['EnclosedPorch'] + df_all['ScreenPorch'] + df_all['WoodDeckSF']\ndf_all['OverallRating'] = df_all['OverallQual'] + df_all['OverallCond']\n\ndf_all['HasPool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['Has2ndFloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasGarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasBsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasFireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\ndf_all['NewHouse'] = 0\nidx = df_all[df_all['YrSold'] == df_all['YearBuilt']].index\ndf_all.loc[idx, 'NewHouse'] = 1","fa83406d":"fig = plt.figure(figsize=(12, 6))\ncmap = sns.color_palette('Set1', n_colors=10)\n\nsns.scatterplot(x=df_all['GrLivArea'], y='SalePrice', hue='OverallQual', palette=cmap, data=df_all)\n\nplt.xlabel('GrLivArea', size=15)\nplt.ylabel('SalePrice', size=15)\nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12) \n    \nplt.title('GrLivArea & OverallQual vs SalePrice', size=15, y=1.05)\n\nplt.show()","bfbce285":"df_all.drop(df_all[np.logical_and(df_all['OverallQual'] < 5, df_all['SalePrice'] > 200000)].index, inplace=True)\ndf_all.drop(df_all[np.logical_and(df_all['GrLivArea'] > 4000, df_all['SalePrice'] < 300000)].index, inplace=True)\ndf_all.reset_index(drop=True, inplace=True)","64086886":"bsmtcond_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4}\nbsmtexposure_map = {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\nbsmtfintype_map = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\nbsmtqual_map = {'None': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\ncentralair_map = {'Y': 1, 'N': 0}\nextercond_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nexterqual_map = {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\nfireplacequ_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfunctional_map = {'Typ': 0, 'Min1': 1, 'Min2': 1, 'Mod': 2, 'Maj1': 3, 'Maj2': 3, 'Sev': 4}\ngaragecond_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\ngaragefinish_map = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\ngaragequal_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nheatingqc_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nkitchenqual_map = {'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}\nlandslope_map = {'Gtl': 1, 'Mod': 2, 'Sev': 3}\nlotshape_map = {'Reg': 0, 'IR1': 1, 'IR2': 2, 'IR3': 3}\npaveddrive_map = {'N': 0, 'P': 1, 'Y': 2}\n\ndf_all['BsmtCond'] = df_all['BsmtCond'].map(bsmtcond_map)\ndf_all['BsmtExposure'] = df_all['BsmtExposure'].map(bsmtexposure_map)\ndf_all['BsmtFinType1'] = df_all['BsmtFinType1'].map(bsmtfintype_map)\ndf_all['BsmtFinType2'] = df_all['BsmtFinType2'].map(bsmtfintype_map)\ndf_all['BsmtQual'] = df_all['BsmtQual'].map(bsmtqual_map)\ndf_all['CentralAir'] = df_all['CentralAir'].map(centralair_map)\ndf_all['ExterCond'] = df_all['ExterCond'].map(extercond_map)\ndf_all['ExterQual'] = df_all['ExterQual'].map(exterqual_map)\ndf_all['FireplaceQu'] = df_all['FireplaceQu'].map(fireplacequ_map)\ndf_all['Functional'] = df_all['Functional'].map(functional_map)\ndf_all['GarageCond'] = df_all['GarageCond'].map(garagecond_map)\ndf_all['GarageFinish'] = df_all['GarageFinish'].map(garagefinish_map)\ndf_all['GarageQual'] = df_all['GarageQual'].map(garagequal_map)\ndf_all['HeatingQC'] = df_all['HeatingQC'].map(heatingqc_map)\ndf_all['KitchenQual'] = df_all['KitchenQual'].map(kitchenqual_map)\ndf_all['LandSlope'] = df_all['LandSlope'].map(landslope_map)\ndf_all['LotShape'] = df_all['LotShape'].map(lotshape_map)\ndf_all['PavedDrive'] = df_all['PavedDrive'].map(paveddrive_map)\n\ndf_all.drop(columns=['Street', 'Utilities', 'PoolQC'], inplace=True)","64aad106":"nominal_features = ['Alley', 'BldgType', 'Condition1', 'Condition2', 'Electrical', \n                    'Exterior1st', 'Exterior2nd', 'Fence', 'Foundation', 'GarageType', \n                    'Heating', 'HouseStyle', 'LandContour', 'LotConfig', 'MSSubClass',\n                    'MSZoning', 'MasVnrType', 'MiscFeature', 'MoSold', 'Neighborhood',\n                    'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType', 'YrSold']\n\nencoded_features = []\n\nfor feature in nominal_features:\n    encoded_df = pd.get_dummies(df_all[feature])\n    n = df_all[feature].nunique()\n    encoded_df.columns = ['{}_{}'.format(feature, col) for col in encoded_df.columns]\n    encoded_features.append(encoded_df)\n\ndf_all = pd.concat([df_all, *encoded_features], axis=1)\ndf_all.drop(columns=nominal_features, inplace=True)","bf4d3cc7":"fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\nplt.subplots_adjust(top=1.5, right=1.5)\n\nsns.distplot(df_all['SalePrice'].dropna(), hist=True, ax=axs[0][0])\nprobplot(df_all['SalePrice'].dropna(), plot=axs[0][1])\n\ndf_all['SalePrice'] = np.log1p(df_all['SalePrice'])\n\nsns.distplot(df_all['SalePrice'].dropna(), hist=True, ax=axs[1][0])\nprobplot(df_all['SalePrice'].dropna(), plot=axs[1][1])\n\naxs[0][0].set_xlabel('Sale Price', size=20, labelpad=12.5)\naxs[1][0].set_xlabel('Sale Price', size=20, labelpad=12.5)\naxs[0][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\naxs[0][1].set_ylabel('Ordered Values', size=20)\naxs[1][1].set_xlabel('Theoretical Quantiles', size=20, labelpad=12.5)\naxs[1][1].set_ylabel('Ordered Values', size=20)\n\nfor i in range(2):\n    for j in range(2):\n        axs[i][j].tick_params(axis='x', labelsize=15)\n        axs[i][j].tick_params(axis='y', labelsize=15)\n        \naxs[0][0].set_title('Distribution of Sale Price', size=25, y=1.05)\naxs[0][1].set_title('Sale Price Probability Plot', size=25, y=1.05)\naxs[1][0].set_title('Distribution of Sale Price After log1p Transformation', size=25, y=1.05)\naxs[1][1].set_title('Sale Price Probability Plot After log1p Transformation', size=25, y=1.05)\n\nplt.show()\n\nprint('Training Set SalePrice Skew: {}'.format(df_all['SalePrice'].skew()))\nprint('Training Set SalePrice Kurtosis: {}'.format(df_all['SalePrice'].kurt()))","cfa372bf":"cont_features = ['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BsmtFinSF1', 'BsmtFinSF2',\n                 'BsmtUnfSF', 'EnclosedPorch', 'GarageArea', 'GrLivArea', 'LotArea', \n                 'LotFrontage', 'LowQualFinSF', 'MasVnrArea', 'MiscVal', 'OpenPorchSF', \n                 'PoolArea', 'ScreenPorch', 'TotalBsmtSF', 'WoodDeckSF']\n\nskewed_features = {feature: df_all[feature].skew() for feature in cont_features if df_all[feature].skew() >= 0.5}\ntransformed_skews = {}\n\nfor feature in skewed_features.keys():\n    df_all[feature] = boxcox1p(df_all[feature], boxcox_normmax(df_all[feature] + 1))\n    transformed_skews[feature] = df_all[feature].skew()\n    \ndf_skew = pd.DataFrame(index=skewed_features.keys(), columns=['Skew', 'Skew after boxcox1p'])\ndf_skew['Skew'] = skewed_features.values()\ndf_skew['Skew after boxcox1p'] = transformed_skews.values()\n\nfig = plt.figure(figsize=(24, 12))\n\nsns.pointplot(x=df_skew.index, y='Skew', data=df_skew, markers=['o'], linestyles=['-'])\nsns.pointplot(x=df_skew.index, y='Skew after boxcox1p', data=df_skew, markers=['x'], linestyles=['--'], color='#bb3f3f')\n\nplt.xlabel('Skewed Features', size=20, labelpad=12.5)\nplt.ylabel('Skewness', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=11)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.title('Skewed Features Before and After boxcox1p Transformation', size=20)\n\nplt.show()","3feeee29":"sparse = []\n\nfor feature in df_all.columns:\n    counts = df_all[feature].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(df_all) * 100 > 99.94:\n        sparse.append(feature)\n        \ndf_all.drop(columns=sparse, inplace=True)\n\ndf_train, df_test = df_all.loc[:1456], df_all.loc[1457:].drop(['SalePrice'], axis=1)\ndrop_cols = ['Id', 'Dataset']\nX_train = df_train.drop(columns=drop_cols + ['SalePrice']).values\ny_train = df_train['SalePrice'].values\nX_test = df_test.drop(columns=drop_cols).values\n\nprint('X_train shape: {}'.format(X_train.shape))\nprint('y_train shape: {}'.format(y_train.shape))\nprint('X_test shape: {}'.format(X_test.shape))","0dcfb89b":"def rmse(y_train, y_pred):\n     return np.sqrt(mean_squared_error(y_train, y_pred))\n\ndef cv_rmse(model, X=X_train, y=y_train):    \n    return np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kf))\n\nK = 10\nkf = KFold(n_splits=K, shuffle=True, random_state=SEED)","363850d6":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=np.arange(14.5, 15.6, 0.1), cv=kf))\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=np.arange(0.0001, 0.0009, 0.0001), random_state=SEED, cv=kf))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(alphas=np.arange(0.0001, 0.0008, 0.0001), l1_ratio=np.arange(0.8, 1, 0.025), cv=kf))\nsvr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=SEED)\nlgbmr = LGBMRegressor(objective='regression', \n                      num_leaves=4,\n                      learning_rate=0.01, \n                      n_estimators=5000,\n                      max_bin=200, \n                      bagging_fraction=0.75,\n                      bagging_freq=5, \n                      bagging_seed=SEED,\n                      feature_fraction=0.2,\n                      feature_fraction_seed=SEED,\n                      verbose=0)\nxgbr = XGBRegressor(learning_rate=0.01,\n                    n_estimators=3500,\n                    max_depth=3,\n                    gamma=0.001,\n                    subsample=0.7,\n                    colsample_bytree=0.7,\n                    objective='reg:squarederror',\n                    nthread=-1,\n                    seed=SEED,\n                    reg_alpha=0.0001)\nstack = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, svr, gbr, lgbmr, xgbr), meta_regressor=xgbr, use_features_in_secondary=True)\n\nmodels = {'RidgeCV': ridge,\n          'LassoCV': lasso, \n          'ElasticNetCV': elasticnet,\n          'SupportVectorRegressor': svr, \n          'GradientBoostingRegressor': gbr, \n          'LightGBMRegressor': lgbmr, \n          'XGBoostRegressor': xgbr, \n          'StackingCVRegressor': stack}\npredictions = {}\nscores = {}\n\nfor name, model in models.items():\n    start = datetime.now()\n    print('[{}] Running {}'.format(start, name))\n    \n    model.fit(X_train, y_train)\n    predictions[name] = np.expm1(model.predict(X_train))\n    \n    score = cv_rmse(model, X=X_train, y=y_train)\n    scores[name] = (score.mean(), score.std())\n    \n    end = datetime.now()\n    \n    print('[{}] Finished Running {} in {:.2f}s'.format(end, name, (end - start).total_seconds()))\n    print('[{}] {} Mean RMSE: {:.6f} \/ Std: {:.6f}\\n'.format(datetime.now(), name, scores[name][0], scores[name][1]))","58f92be9":"def blend_predict(X):\n    return ((0.1 * elasticnet.predict(X)) + \n            (0.05 * lasso.predict(X)) +\n            (0.1 * ridge.predict(X)) +\n            (0.1 * svr.predict(X)) +\n            (0.1 * gbr.predict(X)) +\n            (0.15 * xgbr.predict(X)) +\n            (0.1 * lgbmr.predict(X)) +\n            (0.3 * stack.predict(X)))\n\nblended_score = rmse(y_train, blend_predict(X_train))\nprint('Blended Prediction RMSE: {}'.format(blended_score))","c999636b":"fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(18, 36))\nplt.subplots_adjust(top=1.5, right=1.5)\n\nfor i, model in enumerate(models, 1):\n    plt.subplot(4, 2, i)\n    plt.scatter(predictions[model], np.expm1(y_train))\n    plt.plot([0, 800000], [0, 800000], '--r')\n\n    plt.xlabel('{} Predictions (y_pred)'.format(model), size=20)\n    plt.ylabel('Real Values (y_train)', size=20)\n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n\n    plt.title('{} Predictions vs Real Values'.format(model), size=25)\n    plt.text(0, 700000, 'Mean RMSE: {:.6f} \/ Std: {:.6f}'.format(scores[model][0], scores[model][1]), fontsize=25)\n\nplt.show()","442df49e":"scores['Blender'] = (blended_score, 0)\n\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='medium', color='black', weight='semibold')\n\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=11)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","cb921f5d":"submission_df = pd.DataFrame(columns=['Id', 'SalePrice'])\nsubmission_df['Id'] = df_test['Id']\nsubmission_df['SalePrice'] = np.expm1(blend_predict(X_test))\nsubmission_df.to_csv('submissions.csv', header=True, index=False)\nsubmission_df.head(10)","a36f6504":"#### **1.2.1 Systematically Missing Data**\n`MasVnrArea` and `MasVnrType` are defined as masonry veneer area in square feet and masonry veneer type. Missing values in those features mean that there is no masonry veneer in those houses. Missing values in `MasVnrArea` are filled with **0** and missing values in `MasVnrType` are filled with **None**.\n\n`BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF` and `TotalBsmtSF` are basement area in square feet. There is only **1** missing value in those features and it is the same house. That house has missing values in other basement features as well and it most likely doesn't have basement. That's why missing values in `BsmtFinSF1`, `BsmtFinSF2`, `BsmtUnfSF` and `TotalBsmtSF` are filled with **0**. The other basement features are categorical. `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2` and `BsmtQual` are missing in houses without basements, so they are filled with **None**. There are also ordinal basement features with missing values such as `BsmtFullBath` and `BsmtHalfBath`. Missing values in those features are filled with **0**.\n\nThere are **7** garage features and all of them have missing values. `GarageArea` is defined as garage area in square feet. There is only **1** house with missing `GarageArea`. That house also has missing value in `GarageCars` feature. Both `GarageArea` and `GarageCars` features are filled with **0** because that house has no garage. `GarageType`, `GarageYrBlt`, `GarageFinish`, `GarageQual` and `GarageCond` are categorical garage features. All of them except `GarageYrBlt` are filled with **None**. `GarageYrBlt` is filled with **0** because it is a numerical type.\n\nOther systematically missing categorical features are `Alley`, `Fence`, `FireplaceQu`, `MiscFeature` and `PoolQC`. There are missing values in them because those features doesn't exist in those houses. Missing values in those features are also filled with **None**.","53b455bb":"### **2.3 Encoding**","9964d002":"## **0. Introduction**","7fe0010a":"### **3.5 Submission**","84e0a879":"### **2.4 Dealing with the Skewness**\nThe skewed and long tailed distribution of the `SalePrice` is solved by applying **log(1 + x)** transformation. This transformation reduced skewness from **1.88** to **0.12** and reduced kurtosis from **6.53** to **0.80**. Target is normally distributed after this transformation. Probability is a straight line rather than a convex curve, which is an indicator of the reduced skewness.","1185107e":"### **1.6 Training vs Test Set**","f0a40d49":"### **3.4 Evaluation & Blending**\nAll of the models individually achieved scores between **0.10** and **0.12**, but when the predictions of those models are blended, they get **0.058**. That's because those models are actually overfitting to certain degree. They are very good at predicting a subset of houses, and they fail at predicting the rest of the dataset. When their predictions are blended, they complement each other. ","835dc16a":"### **1.5 Target vs Features**","5e06f1dd":"### **3.1 Feature Selection**\nSparse features are dropped because they are tend to be ignored by tree algorithms. **99.94** is the threshold for the sparse features. If **99.94%** of the values of a feature are zeros, the feature is dropped. Other useless features like `Id` and `Dataset` are also dropped. Finally, `X_train`, `y_train` and `X_test` are separated and ready for the machine learning models.","6c785ecc":"### **2.2 Outliers**\nThere are two houses larger than **4500** square feet and sold for less than **300000**. They are too large for their prices. Those two houses are dropped because they are affecting the regression coefficient of `GrLivArea` drastically. There is a house with less than **5** `OverallQual`, but sold for more than **200000**. That is an overly paid price for a low quality house, so it is dropped as well.","6b527dba":"The other highly skewed features are also transformed, but **boxcox1p** transformation is used for those features. **boxcox1p** is defined as $((1 + x)^{\\lambda} - 1)$ if $\\lambda$ is not **0**, and $log(1 + x)$ if $\\lambda$ is **0**.\n\n**0.5** is used as a skewness threshold, and transformation is applied to features which have higher skew than the threshold. However, this doesn't work on every feature because some of them are sparse.","13cb30ac":"#### **1.5.1 Numerical Features**\nData points of `1stFlrSF`, `BsmtUnfSF`, `GarageArea`, `GrLivArea`, `LotArea`, `LotFrontage` and `TotalBsmtSF` features are not stacked at **0** as much as other numerical features. Those features exist in almost every single house. Fitting the regression line is easier for those features. In addition to that, outliers are very visible in those features.\n\nData points of `2ndFlrSF`, `3SsnPorch`, `BsmtFinSF1`, `BsmtFinSF2`, `EnclosedPorch`, `LowQualFinSF`, `MasVnrArea`, `MiscVal`, `OpenPorchSF`, `PoolArea`, `ScreenPorch` and `WoodDeckSF` features are heavily stacked at **0**. Those features are rarer than the previous ones and they don't exist in every house, so they are sparse features. Those sparse features may not be reliable as the previous features when they are used as continuous features, because they are going to introduce bias to the regression function.\n\n`GarageYrBlt`, `YearBuilt` and `YearRemodAdd` are ordinal features, but a linear relationship can be seen from their plots. Houses with recent dates are more likely to be sold at higher prices.","06d78761":"### **1.7 Conclusion**\nThere are features with ambiguous types. `GarageYrBlt`, `MoSold`, `YearBuilt`, `YearRemodAdd` and `YrSold` are date features. Those are numerical features by default and they imply linear relationship. It might be better to use some of them as categorical features. \n\nThere were **24** features with systematically missing data. Those features are filled with **None** and **0** depending on their types which made them sparse. Converting those features from multi class or continuous to binary, could give better results.\n\nTarget (`SalePrice`) distribution is highly skewed and long tailed because of the outliers. It requires a transformation in order to perform better in models. Dealing with the outliers could also achieve better model performance.\n\nMany features are strongly correlated with each other and target. This relationship can be used to create new features with feature interaction in order to overcome multicollinearity issue.\n\n**12** continuous features are heavily stacked at **0**. They are also sparse features. Those features may add bias to the model. Some categorical features are not informative for two reasons. The feature is either too homogenous like `Utilities` feature, or all of the values have the same characteristics like `MoSold` feature. Those features can be conbined with other features or dropped completely.\n\nThere are some numerical feature distributions that are too noisy. Their distributions in training and test set are quite different. They may require grouping to overcome this problem. The same problem occurs for categorical features as well. There are some categorical feature values that doesn't exist in both training set and test set. They need to be grouped with other values.","f507f67e":"### **1.3 Target Distribution**\nTarget is not normally distributed. Training set `SalePrice` skew is **1.88** which clearly shows that the target has high positive skew. The distribution is asymmetrical because of the extremely high outliers. \n\nTraining set `SalePrice` kurtosis is **6.53** which is an indicator of the tail extremity. Mean `SalePrice` is **180921.2**, however, median is **163000**. This huge gap between mean and median is also the effect of outliers.\n\nProbability plot clearly illustrates that outliers will strongly affect the regression models since a single outlier may result in all predictor coefficients being biased. The probability being a convex curve rather than a straight line is the result of the skewness.","96d3ddf9":"### **1.1 Overview**\n* Training set has **1460** samples and test set has **1459** samples\n* Training set have **81** features and test set have **80** features\n* One extra feature in training set is `SalePrice` which is the target to predict","448d9a02":"### **1.4 Correlations**\nFeatures are strongly correlated with target. **8** features have more than **0.3** correlation coefficient with `SalePrice` and **3** of them are higher than **0.6**. It looks like multicollinearity occurs in the dataset. \n\nThe other features are also strongly correlated with each other and dependent to each other. There are more than **25** correlations with at least **0.5** coefficient. The highest among them is between `GarageArea` and `GarageCars` which is **0.88**.\n\nThe correlation coefficients of training set and test set are very close.","10f334d3":"#### **2.3.1 Label Encoding Ordinal Features**\nThere are **45** categorical features, and **23** of them are ordinal. Different set of integers are mapped to those ordinal features depending on their values.\n\n**3** of the **23** ordinal features are dropped because they are not informative. Those features are `Street`, `Utilities` and `PoolQC`. The `PoolQC` and `Street` features have only different values in **10** houses, and `Utilities` feature has only **1** different value in one house. That's why those features don't provide any useful information.\n\nThere are also partial ordinal features that are not label encoded because all of their values are not ordered.","36596f5e":"#### **1.6.1 Numerical Features**\n`1stFloorSF`, `2ndFloorSF`, `BsmtFinSF1`, `BsmtUnfSF`, `GarageArea`, `GarageYrBlt`, `GrLivArea`, `LotArea`, `LotFrontage`, `MasVnrArea`, `OpenPorchSF`, `TotalBsmtSF`, `WoodDeckSF`, `YearBuilt` and `YearRemodAdd` features have similar distributions in training set and test set. Models using these features, are less likely to overfit.\n\n`3SsnPorch`, `BsmtFinSF2`, `EnclosedPorch`, `LowQualFinSF`, `MiscVal`, `PoolArea` and `ScreenPorch` are too noisy. The distributions of those features in training set and test set doesn't match, so they might introduce bias to the models.","50a75172":"## **3. Models**","ce736bb4":"#### **1.5.2 Categorical Features**\nCategorical features are not strongly correlated with `SalePrice`. There are only **2** categorical features that have significant correlation with `SalePrice`, and they are `OverallQual` and `TotRmsAbvGrd`. A linear relationship can easily be seen from their plots. When the number of `OverallQual` and `TotRmsAbvGrd` increases, `SalePrice` tends to increase as well.\n\nData points of `MoSold` and `YrSold` are uniformly distributed between classes. Those two features might have the least information about `SalePrice` among other categorical features.\n\nThe other categorical features don't have significant correlation with `SalePrice`. However, values in some of those features have very distinct `SalePrice` maximums, minimums and interquartile ranges. Those features could be useful in tree based algorithms.  ","d76b39c8":"#### **2.3.2 One-Hot Encoding Nominal Features**\nThe rest of the categorical features are nominal, and there are **25** of them. Those features are one-hot encoded because there is no order in their values. Partial ordinal features are also one-hot encoded along with nominal features.","4778b6de":"#### **1.2.2 Randomly Missing Data**\nAfter filling systematically missing data, there are few features left with missing values. Those are the remaining randomly missing data. A house can't exist without those features, so they can't be filled with **0** or **None**. The amount of randomly missing data in a feature is also smaller which makes it possible to fill them with descriptive statistical measures. The statistical measures are based on the groups of neighborhoods and building classes because a house would most likely look like its neighbors.\n*  `Electrical`, `Exterior1st`, `Exterior2nd`, `Functional`, `KitchenQual`, `MSZoning`, `SaleType` and `Utilities` are categorical features with randomly missing data and missing values in those features are filled with mode of building class and neighborhood groups\n* `LotFrontage` is the only randomly missing continuous feature and missing values in `LotFrontage` are filled with the median values of neighborhoods","fdc2a5a2":"## **2. Feature Engineering**","88790dc8":"#### **1.6.2 Categorical Features**\nValues in some categorical features exist in one set, but doesn't exist in another set. This problem exists in `BedroomAbvGr`, `Condition2`, `Electrical`, `Exterior1st`, `Exterior2nd`, `Fireplaces`, `FullBath`, `GarageCars`, `GarageQual`, `Heating`, `KitchenAbvGr`, `MSSubClass`, `MiscFeature`, `PoolQC`, `RoofMatl` and `Utilities` features. This is a potential problem because some of the categorical features are going to be one-hot encoded, and because of that process, the feature counts of training set and test set might not match.\n\nOther categorical feature value counts in training set and test set are close to each other. Those features have similar distributions in both data sets, so they are more reliable than the previous ones.","750bc919":"### **2.1 Feature Interactions**\nCreated **12** new features in this section. **6** of them are continuous, and **6** of them are categorical. `YearBuiltRemod`, `TotalSF`, `TotalSquare`, `TotalBath`,`TotalPorch` and `OverallRating` are the new continuous features. They are the total number of some related features. `HasPool`, `Has2ndFloor`, `HasGarage`, `HasBsmt` and `HasFireplace` are the new categorical features. They are the binary forms of some rare features. `NewHouse` is a categorical feature to separate houses which were built and sold at the same year.","c12d0299":"### **1.2 Missing Values**\nThere are many features with missing values in both training set and test set. `display_missing` function shows the count of missing values in every feature if they have at least 1 missing value.\n* Training set has missing values in **19** features\n* Test set has missing values in **32** features\n\nThere are two types of missing data in this dataset; **systematically** missing data and **randomly** missing data. Majority of the missing data are **systematically** missing which is easier to fix, but **randomly** missing data requires extra effort. It is better to deal with those types of missing data separately.","614bd013":"### **3.3 Models & Stacking**\nCreated **8** models and one of them is a stack of those models. \n* `RidgeCV`, `LassoCV` and `ElasticNetCV` are linear models with built-in cross validation\n* `SVR` is a linear support vector machine algorithm\n* `GradientBoostingRegressor`, `LGBMRegressor` and `XGBRegressor` are tree based regression model\n* Lastly, `StackingCVRegressor` is the stack of those models\n\n\n**Stacking** is an ensemble learning technique to combine multiple regression models via a meta-regressor to give improved prediction accuracy. In the standard stacking procedure, the first-level regressors are fit to the same training set that is used prepare the inputs for the second-level regressor, which may lead to overfitting. The `StackingCVRegressor`, however, uses the concept of **out-of-fold predictions** the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level regressor. After the training of the `StackingCVRegressor`, the first-level regressors are fit to the entire dataset for optimal predicitons.\n\nAll of the models are stacked in `StackingCVRegressor`, and `XGBRegressor` is used as the meta-regressor. \n\n`use_features_in_secondary` parameter is set to **True**, which means that the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. (If it is set to **False**, the meta-regressor will be trained only on the predictions of the original regressors.)\n\n![alt](https:\/\/i.ibb.co\/71k3JR0\/stacking-cv-regressor-overview.png)","98290dfd":"### **3.2 Cost Function & Cross Validation**\n`rmse` calculates the root of **mean squared error**, and since the target variable is already at log space, this function calculates **root mean squared log error** which is the competition score metric.\n\n`cv_rmse` has to be implemented with `cross_val_score` function, which returns a vector of scores of the specified cost function (`rmse`) for every fold. Square root of that vector is the `rmse` of every fold. A **10** fold shuffled cross validation is used for validation sets.","ce8b4661":"## **1. Exploratory Data Analysis**"}}