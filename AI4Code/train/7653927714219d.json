{"cell_type":{"2f32f4a1":"code","7e6879db":"code","7b257e8e":"code","d9f47ada":"code","c7715fd0":"code","ae78595b":"code","ce70b951":"code","61a22728":"code","7d583180":"code","97bc8f01":"code","10be9402":"code","47241c85":"code","905b5d56":"code","e3c4ecb3":"code","261c8a7a":"code","005cdd35":"code","ad2b6dd8":"code","a6ccc0cf":"code","c853631b":"code","00326430":"code","27ac7ed5":"code","d2bc6e47":"code","4d20d0a8":"code","e2a10d2e":"code","400d23c8":"code","33bbfea1":"code","3193a890":"code","6a686801":"code","3dcf92f4":"code","fc6ecf0b":"code","66664ee1":"code","63aab393":"code","a44de90a":"code","b7542b70":"code","15ff62e0":"code","b34d7861":"code","dae32d8b":"code","78c74fda":"code","e0acbfa3":"code","c84a5b89":"code","938123cd":"code","31a21866":"code","a7a3ae53":"code","d6c80f10":"code","0479e7d1":"code","1c7740a1":"code","e6677901":"code","1cd941c7":"code","8ce27ec5":"code","6224279a":"markdown","8d6ce713":"markdown","7c6daf01":"markdown","2c21488d":"markdown","842820f6":"markdown","e403a85f":"markdown","a0d8253c":"markdown","44e0ab7f":"markdown","3f5e8bf9":"markdown","b3f7d7c9":"markdown","35b0b873":"markdown","517e3015":"markdown"},"source":{"2f32f4a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e6879db":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7b257e8e":"Heart_Disease =pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","d9f47ada":"Heart_Disease.head()","c7715fd0":"Heart_Disease.describe()","ae78595b":"Heart_Disease.info()","ce70b951":"Heart_Disease.columns","61a22728":"Heart_Disease.shape ","7d583180":"Heart_Disease.isnull().sum()","97bc8f01":"Heart_Disease.isnull().values.any()","10be9402":"#checking for Outlier's\nsns.boxplot(x=Heart_Disease)","47241c85":"#Discover outliers with mathematical function\n#Z-Score\n#if the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers.\nfrom scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(Heart_Disease))\nprint(z)","905b5d56":"plt.figure(figsize=(10,10))\nsns.heatmap(Heart_Disease.corr(),annot=True,fmt='.1f')\nplt.show()","e3c4ecb3":"sns.pairplot(Heart_Disease)\nplt.show()","261c8a7a":"sns.barplot(x=Heart_Disease.age.value_counts()[:10].index,y=Heart_Disease.age.value_counts()[:10].values)\nplt.xlabel('Age')\nplt.ylabel('Age Counter')\nplt.title('Age Analysis')\nplt.show()","005cdd35":"minAge=min(Heart_Disease.age)\nmaxAge=max(Heart_Disease.age)\nmeanAge=Heart_Disease.age.mean()\nprint('Min Age :',minAge)\nprint('Max Age :',maxAge)","ad2b6dd8":"young_ages=Heart_Disease[(Heart_Disease.age>=29)&(Heart_Disease.age<40)]\nmiddle_ages=Heart_Disease[(Heart_Disease.age>=40)&(Heart_Disease.age<55)]\nelderly_ages=Heart_Disease[(Heart_Disease.age>55)]\nprint('Young Ages :',len(young_ages))\nprint('Middle Ages :',len(middle_ages))\nprint('Elderly Ages :',len(elderly_ages))\n","a6ccc0cf":"sns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(young_ages),len(middle_ages),len(elderly_ages)])\nplt.xlabel('Age Range')\nplt.ylabel('Age Counts')\nplt.title('Ages State in Dataset')\nplt.show()","c853631b":"colors = ['blue','green','yellow']\nexplode = [0,0,0.1]\nplt.figure(figsize = (5,5))\n#plt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')\nplt.pie([len(young_ages),len(middle_ages),len(elderly_ages)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')\nplt.title('Age States',color = 'blue',fontsize = 15)\nplt.show()","00326430":"#Sex (1 = male; 0 = female)\nsns.countplot(Heart_Disease.sex)\nplt.show()","27ac7ed5":"sns.countplot(Heart_Disease.cp)\nplt.xlabel('Chest Type')\nplt.ylabel('Count')\nplt.title('Chest Type vs Count State')\nplt.show()\n#0 status at least\n#1 condition slightly distressed\n#2 condition medium problem\n#3 condition too bad","d2bc6e47":"# Show the results of a linear regression within each dataset\nsns.lmplot(x=\"trestbps\", y=\"chol\",data=Heart_Disease,hue=\"cp\")\nplt.show()","4d20d0a8":"sns.barplot(x=Heart_Disease.thalach.value_counts()[:20].index,y=Heart_Disease.thalach.value_counts()[:20].values)\nplt.xlabel('Thalach')\nplt.ylabel('Count')\nplt.title('Thalach Counts')\nplt.xticks(rotation=45)\nplt.show()","e2a10d2e":"sns.countplot(Heart_Disease.thal)\nplt.show()","400d23c8":"#Let's see the correlation values between them\nHeart_Disease.corr()","33bbfea1":"X=Heart_Disease.drop(['target','slope'],axis=1)\n#removing 'slope' to reduce the strong negative multicollinearity between 'slope' and 'oldpeak'\nY=Heart_Disease['target']","3193a890":"import statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\nlogit = sm.Logit(y_train, X_train).fit()\nprint(logit.summary())\n# attributes with p value less than 0.05 are statistically significant","6a686801":"X=X.drop(['restecg','fbs','chol','trestbps','age'],axis=1)","3dcf92f4":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\nlogit = sm.Logit(y_train, X_train).fit()\nprint(logit.summary())","fc6ecf0b":"y_pred = logit.predict(X_test)\nprediction = list(map(round, y_pred)) ","66664ee1":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\ncm1= confusion_matrix(y_test,prediction)\nprint('Confusion Matrix : ')\nprint(cm1)\nfrom sklearn.metrics import accuracy_score\nprint (\"Accuracy Score : \", accuracy_score(y_test, prediction))\nprint ('Report : ')\nprint (classification_report(y_test, prediction))","63aab393":"#Sensitivity and Specificity\n#A test with a sensitivity and specificity of around 90% would be considered to have good diagnostic performance)\n\nsensitivity = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\nprint('Specificity : ', specificity)","a44de90a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)","b7542b70":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)","15ff62e0":"y_pred = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc\nacclog = accuracy_score(y_test, y_pred)*100\nreclog = recall_score(y_test, y_pred)*100\npreclog = precision_score(y_test, y_pred)*100\nfprlog, tprlog, _ = roc_curve(y_test, y_pred)\nauclog=auc(fprlog, tprlog)*100","b34d7861":"from sklearn.metrics import confusion_matrix\ncm1= confusion_matrix(y_test,y_pred)\nprint('Confusion Matrix :' )\nprint(cm1)\nfrom sklearn.metrics import accuracy_score\nprint (\"Accuracy Score: \", accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","dae32d8b":"#Sensitivity and Specificity\n#A test with a sensitivity and specificity of around 90% would be considered to have good diagnostic performance)\n\nsensitivity = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\nprint('Specificity : ', specificity)","78c74fda":"yl = model.predict_proba(X_test)","e0acbfa3":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')\nknn.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = knn.predict(X_test)\n\nyk = knn.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc\naccknn = accuracy_score(y_test, y_pred)*100\nrecknn = recall_score(y_test, y_pred)*100\nprecknn = precision_score(y_test, y_pred)*100\nfprknn, tprknn, _ = roc_curve(y_test, y_pred)\naucknn=auc(fprknn, tprknn)*100\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(y_test, y_pred)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","c84a5b89":"sensitivity = cm2[0,0]\/(cm2[0,0]+cm2[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm2[1,1]\/(cm2[1,0]+cm2[1,1])\nprint('Specificity : ', specificity)","938123cd":"# Fitting Kernel SVM to the Training set\nfrom sklearn.svm import SVC\nsvm = SVC(kernel = 'rbf', random_state = 0, probability=True)\nsvm.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = svm.predict(X_test)\n\nys = svm.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc\naccsvm = accuracy_score(y_test, y_pred)*100\nrecsvm = recall_score(y_test, y_pred)*100\nprecsvm = precision_score(y_test, y_pred)*100\nfprsvm, tprsvm, _ = roc_curve(y_test, y_pred)\naucsvm=auc(fprsvm, tprsvm)*100\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(y_test, y_pred)\n\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","31a21866":"sensitivity = cm3[0,0]\/(cm3[0,0]+cm3[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm3[1,1]\/(cm3[1,0]+cm3[1,1])\nprint('Specificity : ', specificity)","a7a3ae53":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = gnb.predict(X_test)\n\nyg = gnb.predict_proba(X_test)\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_curve, auc\naccgnb = accuracy_score(y_test, y_pred)*100\nrecgnb = recall_score(y_test, y_pred)*100\nprecgnb = precision_score(y_test, y_pred)*100\nfprgnb, tprgnb, _ = roc_curve(y_test, y_pred)\naucgnb=auc(fprgnb, tprgnb)*100\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(y_test, y_pred)\n\nresults = confusion_matrix(y_test, y_pred)\nprint ('Confusion Matrix :')\nprint(results)\nprint ('Accuracy Score :',accuracy_score(y_test, y_pred))\nprint ('Report : ')\nprint (classification_report(y_test, y_pred))","d6c80f10":"sensitivity = cm4[0,0]\/(cm4[0,0]+cm4[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = cm4[1,1]\/(cm4[1,0]+cm4[1,1])\nprint('Specificity : ', specificity)","0479e7d1":"algos=[\"Logistic Regression\",\"K Nearest Neighbor\",\"Support Vector Machine\",\"Gaussian Naive Bayes\"]\nacc=[acclog,accknn,accsvm,accgnb]\nauc=[auclog,aucknn,aucsvm,aucgnb]\nrecall=[reclog,recknn,recsvm,recgnb]\nprec=[preclog,precknn,precsvm,precgnb]\ncomp={\"Algorithms\":algos,\"Accuracies\":acc,\"Area Under the Curve\":auc,\"Recall\":recall,\"Precision\":prec}\ncompdf=pd.DataFrame(comp)\ndisplay(compdf)\n#display(compdf.sort_values(by=[\"Accuracies\",\"Area Under the Curve\",\"Recall\",\"Precision\"], ascending=False))","1c7740a1":"import sklearn.metrics as metrics\nroc_auc1=metrics.auc(fprlog,tprlog)\nroc_auc2=metrics.auc(fprknn,tprknn)\nroc_auc3=metrics.auc(fprsvm,tprsvm)\nroc_auc4=metrics.auc(fprgnb,tprgnb)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title(\"Performance Comparison of Classification Models (ROC Curve)\", fontsize=25)\nplt.plot(fprlog,tprlog,\"b\",label=\"AUC of Logistic Regression = %0.2f\" % roc_auc1)\nplt.plot(fprknn,tprknn,\"g\",label=\"AUC of K Nearest Neighbor = %0.2f\" % roc_auc2)\nplt.plot(fprsvm,tprsvm,\"m\",label=\"AUC of Support Vector Machine = %0.2f\" % roc_auc3)\nplt.plot(fprgnb,tprgnb,\"c\",label=\"AUC of Gaussian Naive Bayes = %0.2f\" % roc_auc4)\nplt.rcParams.update({'font.size': 16})\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1],[0, 1],\"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\", fontsize = 18)\nplt.xlabel(\"False Positive Rate\", fontsize = 18)\n\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=22)","e6677901":"from sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1cd941c7":"accuracy_log1 = cross_val_score(model, X, Y, scoring='accuracy', cv = 10)\n#print('CVS for log1 : ', accuracy_svc)\nprint(\"Accuracy of LOG with Cross Validation is:\",accuracy_log1.mean() * 100)\n#accuracy_log = cross_val_score(log, X, Y, cv = 10)\n#print('CVS for LOG : ', accuracy_svc)\n#print(\"Accuracy of LOG with Cross Validation is:\",accuracy_log.mean() * 100)\naccuracy_svc = cross_val_score(svm, X, Y, cv = 10)\n#print('CVS for SVC : ', accuracy_svc)\nprint(\"Accuracy of SVC with Cross Validation is:\",accuracy_svc.mean() * 100)\naccuracy_gnb = cross_val_score(gnb, X, Y, scoring='accuracy', cv = 10)\n#print('CVS for GNB : ', accuracy_gnb)\nprint(\"Accuracy of GNB with Cross Validation is:\",accuracy_gnb.mean() * 100)\naccuracy_knn = cross_val_score(knn, X, Y, scoring='accuracy', cv = 10)\n#print('CVS for knn : ', accuracy_gnb)\nprint(\"Accuracy of KNN with Cross Validation is:\",accuracy_knn.mean() * 100)","8ce27ec5":"algos=[\"Logistic Regression\",\"K Nearest Neighbor\",\"Support Vector Machine\",\"Gaussian Naive Bayes\"]\nacc1=[acclog,accknn,accsvm,accgnb]\nacc2=[accuracy_log1.mean() * 100, accuracy_knn.mean() * 100, accuracy_svc.mean() * 100, accuracy_gnb.mean() * 100]\ncomp={\"Algorithms\":algos,\"Accuracies without Cross Validation\":acc1,\"Accuracies with Cross Validation\":acc2}\ncompdf=pd.DataFrame(comp)\ndisplay(compdf)","6224279a":"# Import necessary Python modules and Read the data","8d6ce713":"# Splitting Data into train and test with 70% and 20% respectively \n","7c6daf01":"# Comparison of all the Machine Learning Algorithms by Comparing some Evaluation Metrics\n","2c21488d":"# 3.Support Vector Machine Algorithm\n","842820f6":"# All Classification Algorithms with Default Parameters\n","e403a85f":"# Exploratory Data Analysis (EDA)","a0d8253c":"# 4. Gaussian Naive Bayes Algorithm\n","44e0ab7f":"# Cross Validation Score ","3f5e8bf9":"# 1. Logistic Regression Algorithm\n","b3f7d7c9":"# 2. K Nearest Neighbor Algorithm\n","35b0b873":"## DATASET COLUMNS \n1. Age (age in years)\n2. Sex (1 = male; 0 = female)\n3. CP (chest pain type)\n4. TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))\n5. CHOL (serum cholestoral in mg\/dl)\n6. FPS (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n7. RESTECH (resting electrocardiographic results)\n8. THALACH (maximum heart rate achieved)\n9. EXANG (exercise induced angina (1 = yes; 0 = no))\n10. OLDPEAK (ST depression induced by exercise relative to rest)\n11. SLOPE (the slope of the peak exercise ST segment)\n12. CA (number of major vessels (0-3) colored by flourosopy)\n13. THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)\n14. TARGET (1 or 0)","517e3015":"# ROC of all the Machine Learning Algorithms on default parameters"}}