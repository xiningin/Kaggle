{"cell_type":{"79bf64ac":"code","2f23cd40":"code","0dda583e":"code","6694dec4":"code","78316423":"code","27e8afee":"code","74ea2ab4":"code","d115781d":"code","5565e3d6":"code","290ab0e8":"code","1ab0197d":"code","5b886776":"code","1e040bca":"code","a2539149":"code","8395c8ea":"code","4d7c0731":"code","ee341828":"code","e26ee935":"markdown","421ce03d":"markdown","d6b6c786":"markdown","e2b28a5d":"markdown","f5fd4bc1":"markdown","2df833ee":"markdown","99b6e1a6":"markdown","f1b3d64e":"markdown","bc14c141":"markdown","b4d5d0e4":"markdown"},"source":{"79bf64ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f23cd40":"path = '\/kaggle\/input\/digit-recognizer\/'\n\ntrain_df = pd.read_csv(path + 'train.csv')\ntest_df = pd.read_csv(path + 'test.csv')\ntrain_df = train_df.sample(frac=1, random_state=42)\n","0dda583e":"train_df.head()","6694dec4":"train_df.info()","78316423":"y_train = train_df.pop('label')\ny_train.head()","27e8afee":"train_df.shape","74ea2ab4":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(train_df, y_train, test_size=0.1, random_state=42)","d115781d":"#The images are of size 28 x 28 and are single channel i.e black & white images\nx_train_np = x_train.to_numpy().reshape(x_train.shape[0], 28, 28, 1)\nx_val_np = x_val.to_numpy().reshape(x_val.shape[0], 28, 28, 1)\n\ntest_df_np = test_df.to_numpy().reshape(test_df.shape[0], 28, 28, 1)\n\n#print(x_train_np[0])","5565e3d6":"print(x_train_np.shape[0])\nprint(x_train.shape[0])","290ab0e8":"from keras.utils.np_utils import to_categorical\n\ny_train_np = to_categorical(y_train, dtype='int32')\ny_val_np = to_categorical(y_val, dtype='int32')\n\n#print(y_train_np)","1ab0197d":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop, Adam","5b886776":"train_datagen  = ImageDataGenerator(rescale = 1.0\/255\n                                    ,rotation_range=20,\n                                    width_shift_range=0.03,\n                                    height_shift_range=0.03,\n                                    shear_range=0.2,\n                                    zoom_range=0.05,\n                                    fill_mode='nearest')\n\nval_datagen = ImageDataGenerator(rescale = 1.0\/255)\n\ntrain_gen = train_datagen.flow(x_train_np, y_train_np, batch_size=64)\nval_gen = val_datagen.flow(x_val_np, y_val_np, batch_size=64)\n","1e040bca":" model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation=tf.nn.relu, padding='same', input_shape=(28,28,1)),\n    tf.keras.layers.MaxPooling2D (2,2),\n\n    tf.keras.layers.Conv2D(64, (3,3), activation=tf.nn.relu, padding='same'),\n    tf.keras.layers.MaxPooling2D (2,2),    \n\n    tf.keras.layers.Conv2D(128, (3,3), activation=tf.nn.relu, padding='same'),\n    tf.keras.layers.MaxPooling2D (2,2),        \n\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(.25),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n    #tf.keras.layers.Dropout(.2),\n    #tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation='softmax' )\n])\n\nmodel.compile(optimizer=Adam(.001), loss='categorical_crossentropy', metrics=['accuracy'])","a2539149":"es_callback = tf.keras.callbacks.EarlyStopping(monitor ='val_accuracy', patience = 10, mode = 'max', restore_best_weights = True)\nreducelr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, min_lr=0.00000001)\n\nhistory = model.fit(train_gen,\n                              epochs = 100,\n                              verbose = 1,\n                              validation_data = val_gen,\n                            callbacks = [es_callback, reducelr_callback])","8395c8ea":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nhistory_df = pd.DataFrame(history.history)\nhistory_df.head(100)","4d7c0731":"history_df[['accuracy','val_accuracy']].plot()\nhistory_df[['loss','val_loss']].plot()\nplt.show()\n","ee341828":"mnist_predict = model.predict_classes(test_df_np, verbose=0)\n\nsubmissions=pd.DataFrame({\"ImageId\": list(range(1,len(mnist_predict)+1)),\n                         \"Label\": mnist_predict})\nsubmissions.to_csv(\"mnist_pred.csv\", index=False, header=True)","e26ee935":"Split the training data into training and validation set. Let us keep aside 10% records for validation","421ce03d":"Use image generator for the training set to get wider variation of images for training","d6b6c786":"Now let us check the shape of the dataset","e2b28a5d":"Now the time to build the model","f5fd4bc1":"Read the training and test data","2df833ee":"Predictions for the test set","99b6e1a6":"Now let me convert the data into numpy array","f1b3d64e":"Now let me convert the labels to categorical variables","bc14c141":"Define the callbacks","b4d5d0e4":"Plot the training history"}}