{"cell_type":{"ce724ffb":"code","018d84a9":"code","518141a8":"code","1d89567c":"code","198e615a":"code","5387bd7c":"code","a0a2e64f":"code","83db6199":"code","002cb7a7":"code","cd50f515":"code","216c37e4":"code","b96e0ea2":"code","5a34c271":"code","85f5e5e9":"code","e001803e":"code","29595e44":"code","163a6a20":"code","5fd3b940":"code","cdbb275b":"code","3d16c41c":"code","21709ac8":"code","e2147091":"code","4ad0dc6e":"code","bbab290e":"code","1008e5cc":"code","aee24e73":"code","93fec9ce":"code","12fe5301":"code","340e76a9":"code","8c9bf3fd":"code","a09f6604":"code","f3824734":"code","68b5b554":"code","c8010a58":"code","dc883635":"code","d11456ab":"code","c876fb2d":"code","e1630836":"code","ac3c6954":"code","801153ca":"markdown","bc531f48":"markdown","90b0002d":"markdown","9767f0da":"markdown","4bfe6312":"markdown","9e3ab627":"markdown","1fd32472":"markdown","a0a8c722":"markdown","79004eb6":"markdown","cb721706":"markdown","cf142805":"markdown","e8e8aec8":"markdown","5930faa2":"markdown","78e5478d":"markdown","a3aed36c":"markdown","7a383f87":"markdown","f1a42955":"markdown","5a5a37b3":"markdown","fa474482":"markdown","50cc039d":"markdown","00bce070":"markdown","31bf5592":"markdown","d100f11f":"markdown","aa67d0d2":"markdown","0c4cbcd4":"markdown","0859a103":"markdown","42c86078":"markdown","ec0a5b5c":"markdown","5ad6c36f":"markdown","4c9d3b7b":"markdown","b9f0cef5":"markdown","310bc3e4":"markdown","f2eb10fd":"markdown","628c952a":"markdown","5576a4a0":"markdown","518f5b6f":"markdown","474766f0":"markdown","40b490c0":"markdown","9ca71bba":"markdown","99863a5f":"markdown","c56af6b4":"markdown","ec11c022":"markdown","eac6ef33":"markdown","f17fc883":"markdown","fabfb85e":"markdown","e3e3021d":"markdown","50397636":"markdown","654577b3":"markdown","b97b89dd":"markdown"},"source":{"ce724ffb":"# EDA\n#------------------\nimport numpy as np\nimport pandas as pd\n\n\n\n# Visualization\n#------------------\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport seaborn as sns\n\n\n\n# Preprocessing\n#------------------\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Models\n#------------------\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n\n# Evaluation\nfrom sklearn import metrics","018d84a9":"data = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')","518141a8":"data.shape","1d89567c":"data.info()","198e615a":"data.describe()","5387bd7c":"data.head()","a0a2e64f":"print(\"In this dataset from {all} customers, {num} of them are unique.\".format(all = len(data) ,\n                                                                               num = len(data.CustomerId.unique())))","83db6199":"data.drop(labels = ['RowNumber' , 'CustomerId' , 'Surname'] , axis = 1 , inplace = True)","002cb7a7":"data.isnull().sum()","cd50f515":"print(\"Geography unique values :\" , '-'*25 , sep = '\\n')\nprint(data.Geography.value_counts())","216c37e4":"data = pd.concat([pd.get_dummies(data['Geography']) , data] , axis = 1)\ndata.drop(labels = 'Geography' , axis = 1 , inplace = True)","b96e0ea2":"data['Gender'].replace({'Male':1 , 'Female':0} , inplace = True)","5a34c271":"data.info()","85f5e5e9":"plt.figure(figsize = (12,7))\nsns.heatmap(data = data.corr() , cmap = 'rocket' , annot = True , fmt = '.2f')\nplt.show()","e001803e":"plt_fnt = {'family':'serif' , 'size':16}\n\nplt.subplots_adjust(left = 1 , bottom = 1 , right = 2.8 , top = 2 , wspace = 0.5 , hspace = None)\nplt.subplot(1,3,1)\nplt.pie(x = [len(data[data['Exited'] == 1]) , len(data[data['Exited'] == 0])]  , labels = ['Exited' , 'Stayed'],\n       autopct = '%.2f %%' , shadow = True , explode = [0.3,0] , colors = ['tab:cyan' , 'tab:olive'])\nplt.title(\"Churn Rate\" , fontdict = plt_fnt)\n\nplt.subplot(1,3,2)\nplt.pie(x = [ len(data[(data.Exited == 1) & (data.Gender == 1)]) , len(data[(data.Exited == 1) & (data.Gender == 0)]) ]\n       , labels = ['Male' , 'Female'] , shadow = True , explode = [0.2,0] , startangle = -40 , autopct = '%.2f %%'\n       , colors = ['darkblue','deeppink'])\nplt.title('Exited customers\\nMale VS Female' , fontdict = plt_fnt)\n\nplt.subplot(1,3,3)\nplt.pie(x = [len(data[(data.Exited == 1) & (data.IsActiveMember == 0)]),len(data[(data.Exited == 1)&(data.IsActiveMember==1)])]\n       , labels = ['Inactive' , 'Active'] , autopct = '%.2f %%' , shadow = True , explode = [0.1,0] \n       , startangle = 40)\nplt.title('Exited customers\\nActive VS Inactive' , fontdict = plt_fnt)\n\n\nplt.show()","29595e44":"age_kernel_exited = data[data.Exited == 1]['Age'].mode()[0]\nage_kernel_stayed = data[data.Exited == 0]['Age'].mode()[0]\nsns.displot(data = data , x = 'Age' , hue = 'Exited' , fill = True , kind = 'kde' , legend = False , alpha = 0.1)\nplt.legend(labels = ['Exited' , 'Stayed'])\nplt.plot([49 , 49] , [0.001,0.015] , linestyle = '--' , c = 'tab:red' , alpha = 0.8)\nplt.plot([59 , 59] , [0.001,0.015] , linestyle = '--' , c = 'tab:red' , alpha = 0.8)\nplt.title('Customers age distribution' , fontdict = plt_fnt)\nplt.text(53 , 0.007 ,'Dangerous range' , rotation = 90 , color ='tab:red')\nplt.show()","163a6a20":"# by num_german --> i mean total german customers\nnum_german = len(data[data.Germany == 1])\nnum_french = len(data[data.France == 1])\nnum_spanish = len(data[data.Spain == 1])\n\n# by ex_german --> i mean exited german customers\nex_german = len(data[(data.Germany == 1) & (data.Exited == 1)])\nex_french = len(data[(data.France == 1) & (data.Exited == 1)])\nex_spanish = len(data[(data.Spain == 1) & (data.Exited == 1)])\n\n# by german_ex_rate --> i mean exit rate of german customers\ngerman_ex_rate = ex_german\/num_german\nfrench_ex_rate = ex_french\/num_french\nspanish_ex_rate = ex_spanish\/num_spanish\n\nplt.ylim(0,6000)\nplt.bar(x = ['German' , 'French' , 'Spanish'] , height = [num_german , num_french , num_spanish])\nplt.bar(x = ['German' , 'French' , 'Spanish'] , height = [ex_german , ex_french , ex_spanish])\n\ncolor_legend = {'Total':'tab:blue' , 'Exited':'tab:orange'}\nhandles = [Line2D([0], [0], marker='s', color='w', markerfacecolor=v, label=k, markersize=8) for k, v in color_legend.items()]\nplt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.title('Customer exit rate with respect of their region' , fontdict = plt_fnt)\nplt.ylabel('Count')\n\nsmall_font = {'family':'serif' , 'size':12}\nplt.text('German' , 1000 , \"{:.1f}%\".format(german_ex_rate*100) , fontdict = small_font)\nplt.text('French' , 1000 , \"{:.1f} %\".format(french_ex_rate*100) , fontdict = small_font)\nplt.text('Spanish' , 600 , \"{:.1f} %\".format(spanish_ex_rate*100) , fontdict = small_font)\n\nplt.show()","5fd3b940":"sns.catplot(data = data , x = 'Exited'  , y = 'Balance' , kind = 'bar' , color = 'tab:green')\nplt.plot([0,1.4],[data[data.Exited == 0]['Balance'].mean(),data[data.Exited == 0]['Balance'].mean()],linestyle = '--' , c ='k'\n         , alpha = 0.8)\nplt.show()","cdbb275b":"data.NumOfProducts.value_counts()","3d16c41c":"# num_p1 --> number of customers with 1 purchased product\nnum_p1 = len(data[data.NumOfProducts == 1])\nnum_p2 = len(data[data.NumOfProducts == 2])\nnum_p3 = len(data[data.NumOfProducts == 3])\nnum_p4 = len(data[data.NumOfProducts == 4])\n\n# ex_p1 --> number of exited customers who purchased 1 product\nex_p1 = len(data[(data.NumOfProducts == 1) & (data.Exited == 1)])\nex_p2 = len(data[(data.NumOfProducts == 2) & (data.Exited == 1)]) \nex_p3 = len(data[(data.NumOfProducts == 3) & (data.Exited == 1)])\nex_p4 = len(data[(data.NumOfProducts == 4) & (data.Exited == 1)])\n\np1_ex_rate = ex_p1\/num_p1\np2_ex_rate = ex_p2\/num_p2\np3_ex_rate = ex_p3\/num_p3\np4_ex_rate = ex_p4\/num_p4\n\nplt.barh(y = ['1_product','2_products','3_products','4_products'] , width = [num_p1,num_p2,num_p3,num_p4], color = 'tab:green')\nplt.barh(y = ['1_product','2_products','3_products','4_products'] , width = [ex_p1,ex_p2,ex_p3,ex_p4] , color = 'darkred')\n\nplt.xlim(0,6000)\n\nplt.text(200 , '4_products','{:.1f} %'.format(p4_ex_rate*100),fontdict = small_font)\nplt.text(400 , '3_products','{:.1f} %'.format(p3_ex_rate*100),fontdict = small_font)\nplt.text(4700 , '2_products','{:.1f} %'.format(p2_ex_rate*100),fontdict = small_font)\nplt.text(5200 , '1_product','{:.1f} %'.format(p1_ex_rate*100), fontdict = small_font)\n\nplt.title('Churn rate VS number of purchased products' , fontdict = plt_fnt)\nplt.xlabel('Number of customers')\n\nplt.show()","21709ac8":"x_set = data.iloc[:,:-1].values\ny_set = data.iloc[:,-1]\n\nx_train , x_test , y_train , y_test = train_test_split(x_set , y_set , test_size = 0.2 \n                                                       , random_state = 1)\n\nprint(\"x_train shape -->\", x_train.shape)\nprint(\"y_train shape -->\",y_train.shape)\nprint(\"x_test shape  -->\",x_test.shape)\nprint(\"y_test shape  -->\",y_test.shape)","e2147091":"sc = StandardScaler()\n\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","4ad0dc6e":"oversample = SMOTE(random_state = 1 , k_neighbors = 5)\nx_train_resample , y_train_resample = oversample.fit_resample(x_train , y_train)","bbab290e":"# org_true --> original true class (1)\norg_true = 0\norg_false = 0\n\n# ovr_true --> over_sampled true class\novr_true = 0\novr_false = 0\n\nfor item in y_train:\n    if item == 1:\n        org_true += 1\n    elif item == 0:\n        org_false += 1\n\nfor item in y_train_resample:\n    if item == 1:\n        ovr_true += 1\n    elif item == 0:\n        ovr_false += 1\n\nplt.subplots_adjust(left = 1 , bottom = 1 , right = 2.3 , top = 2 , wspace = 0.5 , hspace = None)\nplt.subplot(1,2,1)\nplt.bar(x = ['Exited' , 'Stayed'] , height = [org_true , org_false] , color = 'tab:olive')\nplt.title('y_train Before SMOTE',fontdict = plt_fnt)\n\nplt.subplot(1,2,2)\nplt.bar(x = ['Exited' , 'Stayed'] , height = [ovr_true , ovr_false] , color = 'tab:red')\nplt.title('y_train After SMOTE' , fontdict = plt_fnt)\nplt.show()","1008e5cc":"# lr_org --> logistic regression with original trainset\nlr_org = LogisticRegression()\nlr_org.fit(x_train , y_train)\npred_lr_org = lr_org.predict(x_test)\n\n# lr_ovr --> Logistic regression with oversampled trainset\nlr_ovr = LogisticRegression()\nlr_ovr.fit(x_train_resample , y_train_resample)\npred_lr_ovr = lr_ovr.predict(x_test)\n\nlr_org_acc = metrics.accuracy_score(y_test , pred_lr_org)\nlr_ovr_acc = metrics.accuracy_score(y_test , pred_lr_ovr)\nprint(\"Logistic regression trained with original trainset --> accuracy of prediction on test = {:.3f}\".format(lr_org_acc))\nprint(\"Logistic regression trained with oversampled trainset --> accuracy of prediction on test = {:.3f}\".format(lr_ovr_acc))","aee24e73":"plt.subplots_adjust(left = 1 , bottom = 1 , right = 2.3 , top = 2 , wspace = 0.5 , hspace = None)\n\nplt.subplot(1,2,1)\nplt.title('Trained with original trainset' , fontdict = small_font)\nsns.heatmap(data = metrics.confusion_matrix(y_test , pred_lr_org) , annot = True , fmt = '.0f' , cmap = 'viridis'\n            , cbar = False , square = True)\n\nplt.subplot(1,2,2)\nplt.title('Trained with oversampled trainset' , fontdict = small_font)\nsns.heatmap(data = metrics.confusion_matrix(y_test , pred_lr_ovr) , annot = True , fmt = '.0f' , cmap = 'viridis'\n           , cbar = False , square = True)\n\n\n\nplt.show()","93fec9ce":"print(\"In logistic regression:\" , end = '\\n'+'-'*20+'\\n')\nprint(\"Precision with original trainset = {:.3f}\".format(metrics.precision_score(y_test , pred_lr_org , average = None)[0]))\nprint(\"Precision with oversampled trainset = {:.3f}\".format(metrics.precision_score(y_test , pred_lr_ovr , average = None)[0]))","12fe5301":"lr_acc = lr_ovr_acc\nlr_pr = metrics.precision_score(y_test , pred_lr_ovr , average = None)[0]","340e76a9":"knc = KNeighborsClassifier() # the default hyper parameters\nknc.fit(x_train_resample , y_train_resample)\npred_knc = knc.predict(x_test)\n\nknc_acc = metrics.accuracy_score(y_test , pred_knc)\n# knc_pr --> k nearest classifer precision\nknc_pr = metrics.precision_score(y_test , pred_knc , average = None)[0]\n\nprint(\"KNN classifier accuracy =\" , knc_acc)\nprint(\"KNN precision =\" , knc_pr)","8c9bf3fd":"gnb = GaussianNB() # the default hyper parameters\ngnb.fit(x_train_resample , y_train_resample)\npred_gnb = gnb.predict(x_test)\n\ngnb_acc = metrics.accuracy_score(y_test , pred_gnb)\ngnb_pr = metrics.precision_score(y_test , pred_gnb , average = None)[0]\n\nprint(\"Gaussian naive bayes accuracy =\" , gnb_acc)\nprint(\"Gaussian naive bayes precision =\" , gnb_pr)","a09f6604":"best_acc = 0\nbest_k = 0\ntest_acc_arr = []\ntrain_acc_arr = []\n\nfor k in range(1,250):\n    dt = DecisionTreeClassifier(min_samples_leaf = k)\n    dt.fit(x_train_resample , y_train_resample)\n    temp_acc = metrics.accuracy_score(y_test , dt.predict(x_test))\n    test_acc_arr.append(temp_acc)\n    \n    train_acc_arr.append(metrics.accuracy_score(y_train_resample , dt.predict(x_train_resample)))\n    if temp_acc > best_acc :\n        best_acc = temp_acc\n        best_k = k\n\nprint(\"best accuracy = {:.2f} , in k = {:}\".format(best_acc , best_k))\n\nplt.plot(np.arange(1,250) , train_acc_arr , color = 'tab:red')\nplt.plot(np.arange(1,250) , test_acc_arr , color = 'tab:green')\nplt.plot([87 , 87] , [0.75 , 0.95] , c = 'k' , linestyle = '--' ,alpha = 0.7)\nplt.title('Accuracy per min_sample_leaf', fontdict = plt_fnt)\nplt.ylabel('Accuracy'  , fontdict = small_font)\nplt.xlabel('Minimum samples per leaf' , fontdict = small_font)\nplt.text(90 , 0.9 , 'best_k = 87' , fontdict = small_font)\nplt.show()","f3824734":"dt = DecisionTreeClassifier(min_samples_leaf = 87)\ndt.fit(x_train_resample , y_train_resample)\npred_dt = dt.predict(x_test)\n\ndt_acc = metrics.accuracy_score(y_test , pred_dt)\ndt_pr = metrics.precision_score(y_test , pred_dt , average = None)[0]\n\nprint(\"Decision tree classifier accuracy =\" , dt_acc)\nprint(\"Decision tree classifier precision =\" , dt_pr)","68b5b554":"svc = SVC(C = 1 , gamma = 0.9 , kernel = 'rbf' , random_state = 1)\nsvc.fit(x_train_resample , y_train_resample)\npred_svc = svc.predict(x_test)\n\nsvc_acc = metrics.accuracy_score(y_test , pred_svc) \nsvc_pr = metrics.precision_score(y_test , pred_svc , average = None)[0]\n\nprint(\"Support vector classifier accuracy =\" , svc_acc)\nprint(\"Support vector classifier precision =\" , svc_pr)","c8010a58":"rfc = RandomForestClassifier(random_state = 1 , max_depth = 15 , n_estimators = 1000)\nrfc.fit(x_train_resample , y_train_resample)\n\npred_rfc_test = rfc.predict(x_test)\npred_rfc_train = rfc.predict(x_train_resample)\n\nrfc_acc = metrics.accuracy_score(y_test , pred_rfc_test)\nrfc_pr = metrics.precision_score(y_test , pred_rfc_test , average = None)[0]\nrfc_acc_train = metrics.accuracy_score(y_train_resample , pred_rfc_train)\n\nprint(\"-\"*10+\"Test\"+'-'*10)\nprint(\"Random forest accuracy =\" , rfc_acc)\nprint(\"Random forest precision =\" , rfc_pr)\nprint(\"-\"*10+\"Train\"+'-'*10)\nprint(\"Random forest accuracy on trainset =\",rfc_acc_train)","dc883635":"ada = AdaBoostClassifier(learning_rate = 0.25 , n_estimators = 1200 , random_state = 1)\nada.fit(x_train_resample , y_train_resample)\npred_ada = ada.predict(x_test)\n\nada_acc = metrics.accuracy_score(y_test , pred_ada) \nada_pr = metrics.precision_score(y_test , pred_ada , average = None)[0]\n\nprint(\"Ada boost accuracy =\" , ada_acc)\nprint(\"Ada boost precision =\" , ada_pr)","d11456ab":"xgb = XGBClassifier(subsample = 0.9 , random_state = 1 , n_estimators = 1200 , max_depth = 15 , use_label_encoder = False\n                   , learning_rate = 0.01 , gamma = 0.1)\nxgb.fit(x_train_resample , y_train_resample)\npred_xgb = xgb.predict(x_test)\n\nxgb_acc = metrics.accuracy_score(y_test , pred_xgb) \nxgb_pr = metrics.precision_score(y_test , pred_xgb , average = None)[0]\n\nprint(\"XGBoost classifier accuracy =\" , xgb_acc)\nprint(\"XGBoost classifier precision =\" , xgb_pr)","c876fb2d":"# x --> accuracies\n# y --> precisions\nlabels = [\"LR\" , \"KNN\" , \"GNB\" , \"DT\" , \"SVC\" , \"RF\" , \"Ada\" , \"XGB\"]\nx = [lr_acc , knc_acc , gnb_acc , dt_acc , svc_acc , rfc_acc , ada_acc , xgb_acc]\ny = [lr_pr , knc_pr , gnb_pr , dt_pr , svc_pr , rfc_pr , ada_pr , xgb_pr]\n\nplt.xlim(0.711,0.869)\nplt.ylim(0.851,0.915)\n\nplt.title('Models comparison' , fontdict = plt_fnt)\nplt.xlabel('Accuracy' , fontdict = plt_fnt)\nplt.ylabel('Precision' , fontdict = plt_fnt)\n\nplt.scatter(x , y , alpha = 0.5)\nfor i in range(len(labels)):\n    plt.annotate(labels[i] , (x[i] , y[i]))","e1630836":"eval_frame = pd.DataFrame()\neval_frame['Model'] = labels\neval_frame['Accuracy'] = x\neval_frame['Precision'] = y\neval_frame.sort_values('Accuracy' , ascending = False)","ac3c6954":"plt.figure(figsize = (5,5))\nplt.title('Random forest evaluation on testset' , fontdict = small_font)\nsns.heatmap(data = metrics.confusion_matrix(y_test , pred_rfc_test) , annot = True , fmt = '.0f' , cmap = 'viridis'\n            , cbar = False , square = True)\n\nplt.show()","801153ca":"<a id = \"5_5\"><\/a>","bc531f48":"<h2 style=\"color:#005b96\">5-1. Heat map results :<\/h2>\n\n- <h3 style=\"color:#009b96\">There is not a strong <u>linear relation<\/u> between target and input features.<\/h3>\n- <h3 style=\"color:#009b96\">But between all the input features, age, belonging to Germany, being an active member and balance are more linearly related to the target.<\/h3>","90b0002d":"<h1 style=\"color:#88bddb\">What will i do in this notebook?<\/h1>\n\n- 1.<a href=\"#1\">Importing the libraries<\/a>\n\n\n- 2.<a href=\"#2\">Load the data<\/a>\n\n\n- 3.<a href=\"#3\">EDA<\/a>\n\n\n- 4.<a href=\"#4\">Data Preprocessing :<\/a>\n- - 4-0.<a href=\"#4_0\">Duplicate customers?<\/a>\n- - 4-1.<a href=\"#4_1\">Dropping useless columns<\/a>\n- - 4-2.<a href=\"#4_2\">Missing values<\/a>\n- - 4-3.<a href=\"#4_3\">Encoding<\/a>\n\n\n- 5.<a href=\"#5\">Strorytelling - Visualization :<\/a>\n- - 5_1.<a href=\"#5_1\">Linear correlations<\/a>\n- - 5_2.<a href=\"#5_2\">Exited_Gender_IsActive<\/a>\n- - 5_3.<a href=\"#5_3\">Dangerous range !<\/a>\n- - 5_4.<a href=\"#5_4\">Customer regions<\/a>\n- - 5_5.<a href=\"#5_5\">Balances<\/a>\n- - 5_6.<a href=\"#5_6\">Number of purchased products<\/a>\n\n\n- 6.<a href=\"#6\">Prepare data for machine learning<\/a>\n- - 6_1.<a href=\"#6_1\">Train_test split<\/a>\n- - 6_2.<a href=\"#6_2\">Scaling<\/a>\n- - 6_3.<a href=\"#6_3\">SMOTE (a.k.a. synthetic minority over-sampling technique)<\/a>\n\n\n- 7.<a href=\"#7\">Train the models<\/a>\n- - 7-1.<a href=\"#7_1\">Logistic regression<\/a>\n- - 7-1-2.<a href=\"#7_1_2\">What's the effect of SMOTE ?<\/a>\n- - 7-2.<a href=\"#7_2\">KNN classifier<\/a>\n- - 7-3.<a href=\"#7_3\">Gaussian Naive Bayes<\/a>\n- - 7-4.<a href=\"#7_4\">Classification tree<\/a>\n- - 7-5.<a href=\"#7_5\">Support vector classifier<\/a>\n- - 7-6.<a href=\"#7_6\">Random forest<\/a>\n- - 7-7.<a href=\"#7_7\">AdaBoost<\/a>\n- - 7-8.<a href=\"#7_8\">XGBoost<\/a>\n\n\n- 8.<a href=\"#8\">Final step<\/a>\n- - 8-1.<a href=\"#8_1\">Models comparison<\/a>\n- - 8-2.<a href=\"#8_2\">Final model confusion matrix<\/a>","9767f0da":"- <h2 style=\"color:#009b96\">As you see in the above cell, XGBoost has the best accuracy among the other models.<\/h2>\n- <h2 style=\"color:tomato\">Remember that, not only the accuracy but also the precision is important for us in this problem, i mentioned that in part 7_1_2 .<\/h2>\n- <h2 style=\"color:#009b96\">So because <u>random forest<\/u> has a better precision, i will choose that as the best model among others.<\/h2>","4bfe6312":"<a id = \"2\"><\/a>\n## 2. Load and Prepare Data","9e3ab627":"<h2 style=\"color:#005b96\">5-6. Bar plot results:<\/h2>\n\n- <h3 style=\"color:tomato\">It's too weird, that we losed all 60 customers who have purchased 4 products.<\/h3>\n- <h3 style=\"color:#009b96\">Also we have a big churn rate in customers who have have purchased 3 products.<\/h3>\n- <h3 style=\"color:#009b96\">More products are not satisfying the customers.<\/h3>\n- <h3 style=\"color:#009b96\">Maybe there is a problem in services of the products.<\/h3>","1fd32472":"<h1 style=\"color:#a56bb5\">We are done...<\/h1>\n<h1 style=\"color:#a56bb5\">Thanks for reading<\/h1>","a0a8c722":"<a id = \"7_2\"><\/a>\n<h2 style=\"color:#005b96\">7_2. KNN classifier<\/h2>","79004eb6":"<a id = \"8_2\"><\/a>\n<h1 style=\"color:tomato\">8_2. Random forest confusion matrix<\/h1>","cb721706":"<a id = \"7_5\"><\/a>\n<h2 style=\"color:#005b96\">7_5. Support vector classifier<\/h2>","cf142805":"<h2 style=\"color:#005b96\">5-2. Pie plot results:<\/h2>\n\n- <h3 style=\"color:#009b96\">Simply out of every 10 customers, 2 of them have exited.<\/h3>\n- <h3 style=\"color:#009b96\">The second plot is showing that the churn rate is higher between female customers.<\/h3>\n- <h3 style=\"color:#009b96\">So its more probable to lose a female customer.<\/h3> \n- <h3 style=\"color:#009b96\">Also from the third plot it's obvious that the probability of losing an inactive customer is higher.<\/h3> ","e8e8aec8":"<a id = \"7_1_2\"><\/a>\n<h2 style=\"color:#005b96\">7_1_2. What's the effect of SMOTE ?<\/h2>\n\n- <h2 style=\"color:#009b96\">As you see we have got better accuracy without SMOTE.<\/h2>\n- <h2 style=\"color:tomato\">So is SMOTE making the result worse ???<\/h2>\n- <h2 style=\"color:tomato\">The answer is NO!<\/h2>\n<h2 style=\"color:#009b96\">Let's take a look at the confusion matrices :<\/h2>","5930faa2":"<a id = \"4_0\"><\/a>\n<h2 style=\"color:#005b96\">4-0. Do we have any duplicate customers?<\/h2>","78e5478d":"<a id = \"4_1\"><\/a>\n<h2 style=\"color:#005b96\">4-1. Dropping useless columns:<\/h2>\n\n- RowNumber\n- CustomerId\n- Surname","a3aed36c":"<h2 style=\"color:tomato\">**No missing values so no need to dropna or impute**<\/h2>","7a383f87":"<a id = \"5_3\"><\/a>","f1a42955":"<h2 style=\"color:#005b96\">5-4. Bar plot results:<\/h2>\n\n- <h3 style=\"color:#009b96\">Most of our customers are French.<\/h3>\n- <h3 style=\"color:#009b96\">German customers have the highest exit rate (32.4%).<\/h3>\n- <h3 style=\"color:#009b96\">So there is a higher probability to lose a German customer.<\/h3> ","5a5a37b3":"<a id = \"1\"><\/a>\n## 1. Importing the libraries","fa474482":"<h2 style=\"color:#005b96\">5-3. KDE results:<\/h2>\n\n- <h3 style=\"color:#009b96\">The plot is showing that approximately between 49 and 59 there is a <u>dangerous range.<\/u> <\/h3>\n- <h3 style=\"color:#009b96\">Which the number of exiting customers is more than the number of customers who stayed.<\/h3>\n- <h3 style=\"color:#009b96\">So there is a higher probability to lose a customer with age from 49 to 59 years old.<\/h3> ","50cc039d":"<a id = \"6_1\"><\/a>\n<h2 style=\"color:#005b96\">6-1. Train_test split:<\/h2>","00bce070":"<h2 style=\"color:#005b96\">5-5. Bar plot results:<\/h2>\n\n- <h3 style=\"color:#009b96\">It seems that exited customers are richer(due to their accounts not the estimated salary) !<\/h3>","31bf5592":"<a id = \"7_1\"><\/a>\n<h2 style=\"color:#005b96\">7_1. Logistic regression<\/h2>","d100f11f":"<a id = \"5\"><\/a>\n## 5. Strorytelling - Visualization","aa67d0d2":"<a id = \"5_6\"><\/a>","0c4cbcd4":"<a id = \"7\"><\/a>\n## 7. Train the model","0859a103":"<a id = \"7_8\"><\/a>\n<h2 style=\"color:#005b96\">7_8. XGBoost<\/h2>","42c86078":"- <h3 style=\"color:#009b96\">Until now we have our train and test sets ready.<\/h3>\n- <h3 style=\"color:#009b96\">But the distribution of y_train is imbalanced (you will see it a few steps later).<\/h3>\n- <h3 style=\"color:#009b96\">This makes machine learning classifier tends to be more biased towards the majority class.<\/h3>\n- <h3 style=\"color:#009b96\">There are some techniques to solve this problem, here i will use SMOTE.<\/h3>\n\n<h2 style=\"color:tomato\">Note :<\/h2>\n<h3 style=\"color:tomato\">For a robust evaluation, there isn't any synthetic sample in the test set !<\/h3>","ec0a5b5c":"- <h2 style=\"color:#009b96\">Imagine that we are the owner of this business, we want to find exiting customers,\nit's not really important for us to predict a customer that will stay (in real scenario), as an exiting customer (a.k.a. FN or false negative).<\/h2>\n- <h2 style=\"color:#009b96\">But it will be costly for us, if we predict a customer that  will exit ( in real scenario), as an staying customer (a.k.a. FP or false positive).<\/h2>\n- <h2 style=\"color:tomato\">So we conclude that in this problem FP is more important for us.<\/h2>\n- <h3 style=\"color:tomato\">So we should also use another evaluation parameter named precision ( TP \/ (TP + FP) )<\/h3>\n<h2 style=\"color:tomato\">Let's compare the precisions :<\/h2>","5ad6c36f":"<a id = \"7_7\"><\/a>\n<h2 style=\"color:#005b96\">7_7. AdaBoost<\/h2>","4c9d3b7b":"<a id = \"8_1\"><\/a>\n<h1 style=\"color:tomato\">8_1. Models comparison<\/h1>","b9f0cef5":"<a id = \"4_2\"><\/a>\n<h2 style=\"color:#005b96\">4-2. Missing values:<\/h2>","310bc3e4":"<a id = \"5_2\"><\/a>","f2eb10fd":"<a id = \"4_3\"><\/a>\n<h2 style=\"color:#005b96\">4-3. Encoding:<\/h2>\n\n- Geography --> OneHotEncoding\n- Gender    --> LabelEncoding","628c952a":"<a id = \"6_3\"><\/a>\n<h2 style=\"color:#005b96\">6_3. SMOTE (a.k.a. synthetic minority over-sampling technique):<\/h2>","5576a4a0":"- <h2 style=\"color:tomato\">As you see in the above cell, the precision is better with oversampling (that's what we wanted !).<\/h2>\n<h3 style = \"color: #009b96\">So from here i will continue training models with the oversampled trainset . . .<\/h3>","518f5b6f":"<a id = \"4\"><\/a>\n## 4. Data Preprocessing","474766f0":"<a id = \"8\"><\/a>\n## 8. Final step","40b490c0":"<a id = \"5_1\"><\/a>","9ca71bba":"<a id = \"7_4\"><\/a>\n<h2 style=\"color:#005b96\">7_4. Classification tree<\/h2>","99863a5f":"<a id = \"6\"><\/a>\n## 6. Prepare Data for Machine learning","c56af6b4":"- <h2 style=\"color:#009b96\">Every thing seems to be good.<\/h2>\n- <h2 style=\"color:tomato\">What about scaling ?!<\/h2>\n- <h2 style = \"color:#009b96\">I will do that in part 6-1, after the visualiaztion and storytelling.<\/h2>","ec11c022":"- <h2 style=\"color:#009b96\">With the above plot we can compare models with respect of accuracy and precision at the same time.<\/h2>\n- <h2 style=\"color:#009b96\">whatever the model is closer to the <u>top right corner<\/u>, its a better model for this problem.<\/h2>","eac6ef33":"<a id = \"6_2\"><\/a>\n<h2 style=\"color:#005b96\">6-2. Scaling:<\/h2>","f17fc883":"<a id = \"7_6\"><\/a>\n<h2 style=\"color:#005b96\">7_6. Random forest<\/h2>","fabfb85e":"<a id = \"5_4\"><\/a>","e3e3021d":"- <h2 style=\"color:##009b96\">As you see when we put small values for min_samples_leaf, we face to overfitting.<\/h2>\n- <h2 style=\"color:##009b96\">So best min_samples_leaf = 87<\/h2>","50397636":"<a id = \"7_3\"><\/a>\n<h2 style=\"color:#005b96\">7_3. Gaussian Naive Bayes<\/h2>","654577b3":"<h2 style=\"color:#009b96\">Our data until now:<\/h2>","b97b89dd":"<a id = \"3\"><\/a>\n## 3. EDA"}}