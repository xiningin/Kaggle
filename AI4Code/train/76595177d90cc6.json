{"cell_type":{"66d275db":"code","657512b8":"code","9d1a4ffa":"code","63dfb366":"code","48fe1cd3":"code","ba99dafb":"code","2cc86e47":"code","a304f8e1":"code","ced683b6":"code","b1f3abdd":"code","692dcfb4":"code","90e0ce45":"code","3e5d2538":"markdown","bc591dac":"markdown","7628d8cb":"markdown","8d1f6ad3":"markdown","ffac791f":"markdown","0a0aaf76":"markdown","b9927474":"markdown","d16cf952":"markdown"},"source":{"66d275db":"import numpy as np\nimport pandas as pd\n\n# Porject modules\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom collections import OrderedDict\nimport time\n\n# PyTorch modules\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models, datasets","657512b8":"# Useful project constants\nBATCH_SIZE = 24\nTYPES_OF_DATASETS = ['train', 'valid'] # Order matters her for train_model function","9d1a4ffa":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'valid': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = '..\/input\/flower_data\/flower_data\/'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in TYPES_OF_DATASETS}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE,\n                                             shuffle=False)\n              for x in TYPES_OF_DATASETS}\ndataset_sizes = {x: len(image_datasets[x]) for x in TYPES_OF_DATASETS}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","63dfb366":"model = models.densenet161(pretrained=True)\nNUM_CLASSES = len(class_names)\nIN_SIZE = model.classifier.in_features  # Expected in_features for desnsenet161 model\nNUM_EPOCHS = 10\n\n# Freeze feature block since we're using this model for feature extraction\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Prep for model training\ncriterion = nn.CrossEntropyLoss()\n# only classifier parameters are being optimized the rest are frozen\noptimizer = optim.SGD(model.classifier.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","48fe1cd3":"# Fit data to model\ndef fit(model, data_loader, criterion, optimizer=None, train=False):\n    if train:\n        model.train()\n    else:\n        model.eval()\n    running_loss = 0.0\n    running_acc = 0.0\n    for data, target in data_loader:\n        data, target = data.to(device), target.to(device)\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the loss\n        loss = criterion(output, target)\n        if train:\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n        # update running training loss\n        running_loss += loss.item() * data.size(0)\n\n        # Calculate accuracy\n        ps = torch.exp(output)\n        top_p, top_class = ps.topk(1, dim=1)\n        correct = top_class == target.view(*top_class.shape)\n        running_acc += torch.mean(correct.type(torch.FloatTensor))\n        \n    loss = running_loss \/ len(data_loader.dataset)\n    acc = 100. * running_acc \/ len(data_loader)\n    \n    return loss, acc","ba99dafb":"# Function for initiating model training\ndef train_model(model, train_loader, valid_loader, criterion, optimizer,\n               scheduler, num_epochs=10):\n    \"\"\"\n    Trains and validates the data using the specified model and parameters.\n    \"\"\"\n    model.to(device)\n\n    start_train_timer = time.time()\n    for epoch in range(num_epochs):\n        # Start timer\n        start = time.time()\n        # Pass forward through the model\n        scheduler.step()\n        train_loss, train_accuracy = fit(model, train_loader, criterion=criterion,\n                                         optimizer=optimizer, train=True)\n        valid_loss, valid_accuracy = fit(model, valid_loader, criterion=criterion,\n                                         train=False)\n\n        # calculate average loss over an epoch\n        elapshed_epoch = time.time() - start\n\n        # print training\/validation statistics \n        print('Epoch: {} - completed in: {:.0f}m {:.0f}s'.format(\n            epoch + 1, elapshed_epoch \/\/ 60, elapshed_epoch % 60))\n        print('\\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n            train_loss, valid_loss))\n        print('\\tTraining accuracy: {:.3f} \\tValidation accuracy: {:.3f}'.format(\n            train_accuracy, valid_accuracy))\n            \n    training_time = time.time() - start_train_timer\n    hours = training_time \/\/ (60 * 60)\n    training_time -= hours * 60 * 60\n    print('Model training completed in: {:.0f}h {:.0f}m {:.0f}s'.format(\n            hours, training_time \/\/ 60, training_time % 60))","2cc86e47":"# Get dataloaders for training and validation sets\ntrain_loader = dataloaders['train']\nvalid_loader = dataloaders['valid']","a304f8e1":"%%time\n# Run training model without precalculating frozen features\ntrain_model(model, train_loader, valid_loader, criterion,\n            optimizer, scheduler, num_epochs=NUM_EPOCHS)","ced683b6":"# Function for generating convoluted features and labels for given dataset and model\ndef preconvfeat(dataloader, model):\n    model.to(device)\n    conv_features = []\n    labels_list = []\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        output = model.features(inputs)  # calculate values for features block only\n        conv_features.extend(output.data.cpu()) # save to CPU since it has much larger RAM\n        labels_list.extend(labels.data.cpu())\n\n    return (conv_features, labels_list)","b1f3abdd":"# Convoluated feature dataset class for retrieving datasets\nclass ConvDataset(Dataset):\n    def __init__(self, feats, labels):\n        self.conv_feats = feats\n        self.labels = labels\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        return self.conv_feats[idx], self.labels[idx]","692dcfb4":"# Custom fc class for densenet161 that uses precalculated feature values\nclass FullyConnectedModel(nn.Module):\n\n    def __init__(self,in_size,out_size):\n        super().__init__()\n        self.fc = nn.Linear(in_size,out_size)\n\n    def forward(self, features):\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1)).view(features.size(0), -1)\n        out = self.fc(out)\n        return out\n\nfc = FullyConnectedModel(IN_SIZE, NUM_CLASSES)","90e0ce45":"%%time\n# Precalculate feature values for the training and validation data\nconv_train_data, labels_train_data = preconvfeat(dataloaders['train'], model)\nconv_valid_data, labels_valid_data = preconvfeat(dataloaders['valid'], model)\n# Convert the calculated data into Dataset opbject\nfeat_train_dataset = ConvDataset(conv_train_data, labels_train_data)\nfeat_valid_dataset = ConvDataset(conv_valid_data, labels_valid_data)\n# Generate DataLoader objects for calculated Datasets\ntrain_dataloader = DataLoader(feat_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nvalid_dataloader = DataLoader(feat_valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n# run training model\ntrain_model(fc, train_dataloader, valid_dataloader, criterion,\n            optimizer, scheduler, num_epochs=NUM_EPOCHS)","3e5d2538":"## Benchmark Running Time\nLet's first begin by timing the typical transfer learning process where the data is repeated passed through both the features and full connected blocks. ","bc591dac":"## Summary\n\nAs you can see, precalculating the features block and iterating only over the fully connected block, even for a small datasets, has significant improvements in training run time. This process is valuable for working with larger datasets and models that require large number of epochs to train. Though memory constraints need to be evaluated when considering this process, especially for very large datasets, since the calculated features data can be very large and thus may exceed memory capacity.","7628d8cb":"### Project Imports\nLet's start with importing modules needed for this project.","8d1f6ad3":"## Configure Model\nWill be using densenet161 pretrained model for the kernal. For this model, the features and fully connected blocks are named \"features\" and \"classifier\" respectively.","ffac791f":"## Introduction\nTransfer learning is a popular method in computer vision because it allows for quickly building accurate models. With transfer learning, instead of starting the learning process from scratch, you leverage patterns that have been learned from pre-trained models as features extraction. A pre-trained model is a model that was trained on a large benchmark dataset, typically ImageNet, to solve a problem similar to the one that we want to solve - image classification.\n\nThough less time than building from scratching, using transfer learning to train image classification models can still take a considerable amount of time. In my experience, models can take up to 30-45 minutes per epoch to train when involving large datasets and\/or complicated models. This means the more epochs needed to improve the model, the longer overall runtime is needed. So how can we improve the training runtime when implementing transfer learning? We can precalculate the frozen layers then pass those values as the dataset into the training model.\n\nPre-trained models can typically be broken into two parts, a features and fully connected block. The features block is typically frozen and used as features extraction and the fully connected block is reconfigured for the specific dataset being applied and the respective number for classes to be predicted. During the forward and back-propagation steps of the training phase, the data passes through both blocks and the model is updated accordingly decreasing the model's loss function.\n\nThe process of passing all the data through both the feature and fully connected blocks can be improved by precalculcating the data values for the features block. With the calculated features data values, we then need to only iterate through the fully connected blocks to train our model. This kernal provides a walkthrough of how to implement such a strategy using PyTorch.","0a0aaf76":"## Define Model Training Functions","b9927474":"## Load Data\nConfigure data augmentation requirements and load image data into memory.","d16cf952":"## Precalculated Features Running Time"}}