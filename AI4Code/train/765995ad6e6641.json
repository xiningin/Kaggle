{"cell_type":{"c1f4bc31":"code","180ac700":"code","7e2a610c":"code","d583216c":"code","7353f4a7":"code","b740fd58":"code","57d99e27":"code","36c8d969":"code","e837b43c":"code","51c196eb":"code","d408e928":"code","a00e29f8":"code","99aad13d":"code","dd0d8244":"code","dcec25da":"code","77b271af":"code","c2796cc1":"code","bd05a0ca":"code","862dea2a":"code","216c0278":"code","5b5e3c53":"code","d32401c6":"code","fab1fe6a":"code","3738f364":"code","c3a02cc7":"code","4be7048e":"code","0f8beb7f":"code","b11888e7":"code","c0211152":"code","d07a5ae0":"code","ce354e90":"code","2a0b91f8":"code","4b0f7e8a":"code","a7de7e10":"code","dc96ed27":"code","af284efd":"code","79381998":"code","d2394ca6":"code","085704c6":"code","9ff4ad7b":"markdown","a7de7358":"markdown","7340aeb8":"markdown","e8dd7bec":"markdown","fd472025":"markdown","d62f229c":"markdown","b19f702e":"markdown","e3fb9f92":"markdown","6424f9e5":"markdown","d24ab3c4":"markdown","f8773bd9":"markdown","215dd86d":"markdown","0bf38881":"markdown","26cec6c9":"markdown","2d0dd010":"markdown","e072a4e8":"markdown","7431c30f":"markdown","b7905494":"markdown"},"source":{"c1f4bc31":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","180ac700":"titanic = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndisplay(titanic.head())","7e2a610c":"print(titanic.isna().sum())\n#AGE, CABIN and EMBARKED have some missing values\nprint(titanic.shape)","d583216c":"men_age = titanic[titanic[\"Sex\"] == \"male\"] #take men apart \nmean_men_age = np.mean(men_age[\"Age\"]) #taking the mean age of the mens in the ship\nmen_age = men_age.fillna({\"Age\":mean_men_age}) #replacing missing values with the mean\nprint(men_age[\"Age\"].isna().sum()) # No more missing values","7353f4a7":"women_age = titanic[titanic[\"Sex\"] == \"female\"]\nmean_age_women = np.mean(women_age[\"Age\"])\nwomen_age = women_age.fillna({\"Age\":mean_age_women})\nprint(women_age[\"Age\"].isna().sum())","b740fd58":"titanic_ok = pd.concat([men_age,women_age])\ntitanic_ok.sort_values(\"PassengerId\")\n\n#We can confirmn that we havent losed any important information","57d99e27":"assert titanic_ok[\"Age\"].isna().sum() == 0","36c8d969":"col_to_del = [\"PassengerId\",\"Name\",\"Cabin\",\"Ticket\"]\nfor col in col_to_del:\n    del(titanic_ok[col])","e837b43c":"most_common_port = titanic_ok[\"Embarked\"].mode()[0] #Port S, Southampton\ntitanic_ok = titanic_ok.fillna({\"Embarked\":most_common_port})\n#print(titanic_ok.isna().sum()) #not missing values","51c196eb":"assert titanic_ok[\"Embarked\"].isna().sum() == 0","d408e928":"titanic_ok","a00e29f8":"sns.set_style(\"whitegrid\")\nplt.style.use(\"ggplot\")","99aad13d":"sns.catplot(x=\"Sex\",data=titanic_ok,kind=\"count\",hue=\"Survived\",col=\"Pclass\",col_wrap=2)\nplt.show()","dd0d8244":"titanic_ok.groupby([\"Pclass\",\"Sex\"])[\"Survived\"].sum()","dcec25da":"to_int = lambda x : int(x)\ntitanic_ok[\"Age\"] = titanic_ok[\"Age\"].apply(to_int)","77b271af":"sns.catplot(x=\"Sex\",y=\"Age\",data=titanic_ok,kind=\"box\",sym=\"\",hue=\"Survived\")\nsns.swarmplot(x=\"Sex\",y=\"Age\",data=titanic_ok,hue=\"Survived\",size=4,dodge=True,color=\".2\")\nplt.show()","c2796cc1":"#Final date set to model.\ntitanic_ok","bd05a0ca":"copy_titanic = titanic_ok.copy()","862dea2a":"def sex_dum(sex):\n    if sex == \"male\":\n        return 1\n    else:\n        return 0\ncopy_titanic[\"Sex\"] = copy_titanic[\"Sex\"].apply(sex_dum)","216c0278":"def embarked_dum(type):\n    if type == \"S\":\n        return 0\n    elif type == \"Q\":\n        return 1\n    else:\n        return 2\ncopy_titanic[\"Embarked\"] = copy_titanic[\"Embarked\"].apply(embarked_dum)","5b5e3c53":"copy_titanic #Ready to modelling","d32401c6":"#labels - Target variables\ny = copy_titanic[\"Survived\"].values\n#Features\nX = copy_titanic.drop(\"Survived\",axis=1).values","fab1fe6a":"#lets bring the new data set\ntest_titanic = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nto_predict = test_titanic[[\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Embarked\"]]\nto_predict = to_predict.fillna(method=\"pad\")\nassert to_predict[\"Age\"].isna().sum() == 0\n\nto_predict[\"Age\"] = to_predict[\"Age\"].apply(to_int)\nto_predict[\"Sex\"] = to_predict[\"Sex\"].apply(sex_dum)\nto_predict[\"Embarked\"] = to_predict[\"Embarked\"].apply(embarked_dum)\n\nto_predict #READT TO USE\n#On this cell, I organized the data and left it ready to use. I just used methods previously defined","3738f364":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline","c3a02cc7":"SEED = 42","4be7048e":"X_scale = scale(X)\nX_train,X_test,y_train,y_test = train_test_split(X_scale,y,test_size=0.3,random_state=SEED,stratify=y)","0f8beb7f":"k_range = range(1,30)\nscores = list()\nfor k in k_range:\n    knn =  KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train,y_train)\n    y_pred = knn.predict(X_test)\n    scores.append(accuracy_score(y_test,y_pred))\nplt.plot(k_range,scores,marker=\"o\")\nplt.ylabel(\"acurracy\")\nplt.xlabel(\"N#_neighbors\")\nplt.show() ","b11888e7":"knn =  KNeighborsClassifier(n_neighbors = 8)\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\nprint(accuracy_score(y_test,y_pred))","c0211152":"X_to_predict = to_predict.values\npredictions = knn.predict(X_to_predict)\nto_report = pd.DataFrame({'PassengerId': test_titanic.PassengerId, 'Survived': predictions})\n#to_report.to_csv('submission.csv', index=False)","d07a5ae0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","ce354e90":"log_reg = LogisticRegression()\n\nparams_log = {\"C\":np.logspace(-5,30)}\ngrid_log = GridSearchCV (estimator = log_reg, param_grid = params_log , cv = 10, n_jobs = -1)\ngrid_log.fit(X_train,y_train)\nC_log = grid_log.best_estimator_ # Best parameter","2a0b91f8":"y_pred_log = grid_log.best_estimator_.predict(X_test)\nprint(accuracy_score(y_test,y_pred_log))","4b0f7e8a":"predictions_log = grid_log.best_estimator_.predict(X_to_predict)\nto_report_log = pd.DataFrame({'PassengerId': test_titanic.PassengerId, 'Survived': predictions_log})\n#to_report_log.to_csv('submission2.csv', index=False)","a7de7e10":"from sklearn.tree import DecisionTreeClassifier","dc96ed27":"d_tree = DecisionTreeClassifier(max_depth = 6, random_state = SEED)\nd_tree.fit(X_train,y_train)\ny_pred_dtree = d_tree.predict(X_test)\nprint(accuracy_score(y_test,y_pred_dtree))","af284efd":"predictions_dtree = d_tree.predict(X_to_predict)\nto_report_dtree = pd.DataFrame({'PassengerId': test_titanic.PassengerId, 'Survived': predictions_dtree})\n#to_report_dtree.to_csv('submission3.csv', index=False)","79381998":"from sklearn.ensemble import RandomForestClassifier","d2394ca6":"r_forest = RandomForestClassifier(n_estimators=100, max_depth=4, random_state=SEED)\nr_forest.fit(X_train,y_train)\ny_pred_rforest = r_forest.predict(X_test)\nprint(accuracy_score(y_test,y_pred_rforest))","085704c6":"predictions_rforest = r_forest.predict(X_to_predict)\nto_report_rforest = pd.DataFrame({'PassengerId': test_titanic.PassengerId, 'Survived': predictions_rforest})\n#to_report_rforest.to_csv('submission4.csv', index=False)","9ff4ad7b":"As is seeing above, I can notice that the Pclass with the most survivers is the first class, followed by the third and finally the second. this is not a surprise, the priority is to save the \"rich\" people but what do is surprised is that I found more survivors on the third class than in the second class.\n\nThe Famale sex has more chances to survive.","a7de7358":"### EDA (Exploratoy Data Analysis)","7340aeb8":"Above I can see how the age is distributed through sex and its hue by either survived or not.\ninteresting things to notice: \n      1. The oldest person to survived was a female of approximately 80 years old.\n      2. the majority of the survivors are women between 20 and 35 years old. \n      ","e8dd7bec":"checking for missing values","fd472025":"# To conclude, I must keep working on my models and improve the ways to aboard the problem","d62f229c":"#### 3. Decision tree","b19f702e":"The first thing to do is to import and read the dataset into our notebook","e3fb9f92":"SUBMISSION IN KEGGLE = 0.77990","6424f9e5":"#### 2. Logisctic Regression","d24ab3c4":"### Modelling. \n","f8773bd9":"#### 1. KNN (K - nearest neigthbors)","215dd86d":"FIRST SUBMISSION IN KEGGLE = 0.61961","0bf38881":"same logic for women that was used in men ","26cec6c9":"8 is the best number of neighbors for this case lets use it","2d0dd010":"for the two missing values in \"Embarked\", I am going to replace with the most common port of embarkation","e072a4e8":"### Cleaning procces","7431c30f":"For the models we wont be ussing this columns","b7905494":"#### 4. Random Forest"}}