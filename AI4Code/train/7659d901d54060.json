{"cell_type":{"6522a77a":"code","91a56803":"code","aadb77f0":"code","31f28f73":"code","7def34e8":"code","6051c7b2":"code","297b4810":"code","333c5be2":"markdown","4881243d":"markdown","6fa7b4d4":"markdown","91a9fead":"markdown"},"source":{"6522a77a":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n","91a56803":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","aadb77f0":"cont_features = [col for col in train.columns if col.startswith(\"cont\")]\nlen(cont_features)","31f28f73":"#X = X.abs()\ny = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n        \n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","7def34e8":"print(score_list)\nprint(np.mean(score_list))","6051c7b2":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 200,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.01,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train[cont_features], y_train)\n        dvalid = lgbm.Dataset(X_val[cont_features], y_val)\n        params = {\"objective\": \"regression\",\n              \"metric\": \"rmse\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 300,\n              \"lambda_l1\":2,\n              \"lambda_l2\":2,\n              \"learning_rate\":0.003,\n              'min_child_samples': 50,\n              \"bagging_fraction\":0.7,\n              \"bagging_freq\":1}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1000,\n                           early_stopping_rounds=100,\n                           init_model = model\n                        )\n\n    \n    \n        y_pred_list.append(model.predict(X_val[cont_features]))\n        test_preds.append(model.predict(test[cont_features]))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","297b4810":"print(score_list)\nprint(np.mean(score_list))","333c5be2":"### We can see that results have improved.\n* 0.6957978243963224 -> 0.695761435448342\n* 3 points improvement in 5th decimal. Not so bad for this competition :)","4881243d":"## What can be done more with this method?\n1. Now you can also try tuning your extra boosting model's parameters :)\n2. You can try stopping your first model earlier (without early stopping) and running extra boosting model longer. \n3. You can try 3rd extra boosting model on top of the 2nd.\n","6fa7b4d4":"## Now we'll run 2nd models on top of 1st models with a lower learning rate thanks to lgbm init_model method.","91a9fead":"In this notebook I want to share init_model parameter of a lightgbm model. Since we compete in 4th and even 5th decimals in this competition, we need to be able to squeeze from the data as much as we can. So I wanted to share this trick, which I also used for the first time."}}