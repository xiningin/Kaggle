{"cell_type":{"f9168ada":"code","403aea27":"code","34d45918":"code","edacb322":"code","493e582c":"code","2d0d801f":"code","782d82d4":"code","3d0677c6":"code","16291437":"code","a01167e9":"code","9e202a64":"code","76d6d638":"code","8a1aeeb8":"code","bf089b53":"code","434bca05":"code","9b95b410":"code","2e3df80e":"code","165a113c":"code","e509038f":"code","3b286622":"code","f6e89f33":"code","1f7efe4e":"code","9c6b05b3":"code","5edf8654":"code","f7b315aa":"code","1d3d7399":"code","62f4492e":"code","cba8c29a":"code","1dbf9151":"code","9747ec44":"code","60f281eb":"code","6770fec6":"code","827edb4b":"code","6606da8b":"code","52401380":"code","ae0a8476":"code","ae5718ec":"code","763ffe11":"code","79eb528e":"code","6fd008d0":"code","76c1b725":"code","0fc6c13d":"code","6d957e6c":"code","63ab278d":"code","7c35100d":"code","9ccc562c":"code","3399be9b":"code","579bd293":"code","99e66a93":"code","0d8310a8":"code","f6379e4f":"code","0fa6eb8a":"code","bfa5899b":"code","a9f9fe09":"code","4423c160":"code","bfd0b7c6":"code","8a4bdc13":"code","5fa14474":"code","91453666":"code","31cbdecb":"code","e5b572d7":"code","ae11a24c":"code","5fc3be47":"code","1dcefdde":"code","47f63fb7":"code","c9cc1719":"code","9b2e2840":"code","be43e208":"code","e6d0284d":"code","93e86b7c":"code","2c48aa64":"code","e093e2c8":"code","fe772da5":"code","3fe042f6":"code","1a74f87e":"code","be2c73ed":"code","b39e8f28":"code","30d75496":"code","8549bf7a":"code","11caf72b":"code","63d2cef6":"code","fd2a77b6":"code","5601c9d9":"code","aa86c57d":"code","363f1111":"code","25158c38":"code","70ede610":"code","83b95d39":"code","2cdfad1a":"code","32a91d5a":"code","c5ec987d":"code","d495d865":"code","27b76cf2":"code","7096b61d":"code","f4d2b509":"code","78922149":"code","951e8909":"code","70f0e37f":"code","e24e5c04":"code","7d1fb976":"code","56313b06":"code","51138d0b":"code","5bacbb8e":"code","349563b5":"code","ea61ef8a":"code","ee94390c":"code","09b722c7":"code","b1a9332f":"code","6c165ce4":"code","9ccef057":"code","3dcc1a8e":"markdown","0d30ca7e":"markdown","46696b53":"markdown","000b3886":"markdown","0946d3b2":"markdown","3ae29863":"markdown","f651981d":"markdown","b9997b26":"markdown","a93d7615":"markdown","c77e74fe":"markdown","d4e6142e":"markdown","8e68ece8":"markdown","88e998ef":"markdown","c0817eab":"markdown","2f2a2632":"markdown","04bd5c30":"markdown","9bcf0fa3":"markdown","f2efdbe9":"markdown","6ae64b97":"markdown","bf0923d7":"markdown","81c32e48":"markdown","26980bb2":"markdown","9e5be04d":"markdown","434ef0d7":"markdown","ba8e666d":"markdown","c77eb656":"markdown","1ddbc153":"markdown","e2c3d2b8":"markdown","a9f455fd":"markdown","3a6b4f02":"markdown","4ca674e5":"markdown","597dc5ad":"markdown","e37ccadb":"markdown","95b5f39a":"markdown","5817d1d0":"markdown","a1cca6c7":"markdown","2defa938":"markdown","d18289ac":"markdown","19c79e91":"markdown","eae49f9b":"markdown","125ce0f5":"markdown","964c657a":"markdown","195cc344":"markdown","df380a6c":"markdown","5f9c4a6c":"markdown","1db75fa9":"markdown","ec67272b":"markdown","218831e5":"markdown","48e8a6a6":"markdown","cf046151":"markdown","a9b7ffdd":"markdown","e0f7bf87":"markdown","264c1dc1":"markdown","dcf04cdf":"markdown","c5ae94f1":"markdown","6d3951e7":"markdown","9c38962b":"markdown","579129ff":"markdown","1cf49ae4":"markdown","b88d782a":"markdown"},"source":{"f9168ada":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nfrom sklearn.metrics import auc, f1_score, roc_auc_score, roc_curve, confusion_matrix, accuracy_score, precision_score\nimport seaborn as sns\nimport time\npd.set_option('display.max_rows', 90)\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 1000)","403aea27":"in_file = '\/kaggle\/input\/datasets-for-churn-telecom\/cell2celltrain.csv'","34d45918":"df = pd.read_csv(in_file)","edacb322":"df.head()","493e582c":"df.shape","2d0d801f":"df.Churn.value_counts()","782d82d4":"df.Churn.value_counts()\/df.shape[0]","3d0677c6":"# number of missing values in dataset\ndf.isnull().sum().values.sum()","16291437":"missing = list()\nfor x in df.columns:\n    if df[x].isnull().sum() != 0:\n        print(x, df[x].isnull().sum())\n        missing.append(x)","a01167e9":"# First of, what on earthe is this feature name? is the Director assisting calls?\ndf.DirectorAssistedCalls.describe()","9e202a64":"plt.figure(figsize=(7,7))\nplt.grid(True)\nsns.distplot(df.DirectorAssistedCalls.fillna(0))\nplt.xlim(right=25)","76d6d638":"df.Churn[df.DirectorAssistedCalls != 0].value_counts()","8a1aeeb8":"# ratios against all customer population\ndf.Churn[df.DirectorAssistedCalls != 0].value_counts()\/df.shape[0]","bf089b53":"df.AgeHH1.describe()","434bca05":"df.AgeHH2.describe()","9b95b410":"df.Churn[(df.AgeHH1.fillna(0) == 0)&(df.AgeHH2.fillna(0) == 0)].value_counts()","2e3df80e":"df = df.fillna(0)","165a113c":"#get a list of categoricals\ncategoricals = list()\nfor x in df.columns:\n    if df[x].dtype == 'object':\n        categoricals.append(x)","e509038f":"df[categoricals].nunique()","3b286622":"def plot_val_counts(df, col=''):\n    plt.figure(figsize=(5,5))\n    plt.grid(True)\n    plt.bar(df[col][df.Churn=='Yes'].value_counts().index, \n            df[col][df.Churn=='Yes'].value_counts().values)\n    plt.title(f'{col}')\n    plt.xticks(rotation=-90)","f6e89f33":"plot_val_counts(df, col='HandsetPrice')","1f7efe4e":"plot_val_counts(df, col='CreditRating')","9c6b05b3":"plot_val_counts(df, col='Occupation')","5edf8654":"plot_val_counts(df, col='PrizmCode')","f7b315aa":"def plot_distro(df, col = '', y_limit=None, x_limit_r=None, x_limit_l = None):\n    plt.figure(figsize=(10,10))\n    plt.grid(True)\n    sns.distplot(df[col][df.Churn == 'Yes'])\n    sns.distplot(df[col][df.Churn == 'No'])\n    plt.legend(['churn_flag_yes', 'churn_flag_no'])\n    if y_limit:\n        plt.ylim(top=y_limit)\n    if x_limit_r:\n        plt.xlim(right=x_limit_r)\n    if x_limit_l:\n        plt.xlim(left=x_limit_l)","1d3d7399":"plot_distro(df, col='PercChangeMinutes', x_limit_r=1200, x_limit_l=-1200)","62f4492e":"plot_distro(df, col='TotalRecurringCharge', x_limit_r=180)","cba8c29a":"plot_distro(df, col='DirectorAssistedCalls', y_limit=.3, x_limit_r=10)","1dbf9151":"plot_distro(df, col='MonthlyRevenue', x_limit_r=200)","9747ec44":"plt.figure(figsize=(10,10))\nplt.grid(True)\nsns.boxplot(x=df.Occupation[df.Churn == 'Yes'], y=df.MonthlyRevenue[df.Churn == 'Yes'])\n#sns.boxplot(x=df.Occupation[df.Churn == 'No'], y=df.MonthlyRevenue[df.Churn == 'No'])","60f281eb":"plt.figure(figsize=(10,10))\nplt.grid(True)\nsns.boxplot(x=df.Occupation, y=df.MonthlyRevenue, hue=df.Churn)\nplt.ylim(top=100)","6770fec6":"plt.figure(figsize=(10,10))\nplt.grid(True)\nsns.boxplot(x=df.ChildrenInHH, y=df.MonthlyRevenue, hue=df.Churn)\n#sns.boxplot(x=df.ChildrenInHH[df.Churn == 'No'], y=df.MonthlyRevenue[df.Churn == 'No'])\nplt.ylim(top=150)","827edb4b":"df.MonthsInService.describe()","6606da8b":"tenure_churn = df.MonthsInService[df.Churn == 'Yes'].value_counts()\ntenure_no_churn = df.MonthsInService[df.Churn == 'No'].value_counts()","52401380":"tenure = pd.merge(tenure_churn.reset_index(), tenure_no_churn.reset_index(), on='index')","ae0a8476":"tenure = tenure.sort_values(by='index')","ae5718ec":"tenure = tenure.reset_index().drop(columns='level_0')","763ffe11":"tenure.columns","79eb528e":"plt.figure(figsize=(10,10))\nplt.grid(True)\nsns.pointplot(x=tenure.index, y=tenure.MonthsInService_x, color='red')\nsns.pointplot(x=tenure.index, y=tenure.MonthsInService_y, color='green')\nplt.xticks(rotation=90)\nplt.title('When the churn picks')","6fd008d0":"def get_lists_of_dtypes(df):\n    \"\"\"\n    Helper function to create list of features by type and by number of unique\n    values they consist of.\n    \"\"\"\n    strings = list()\n    integers = list()\n    floats = list()\n    # Checking for partial string match to append accordingly value type\n    # As here we might have different type of ints and floats\n    # Note that strings we're returning as dictionary, to have number of unique vals for each feature\n    for x in df.columns[2:]:\n        if str(df[x].dtype)[:3] in 'obj':\n            strings.append({x:len(df[x].unique())})\n        elif str(df[x].dtype)[:3] in 'int':\n            integers.append(x)\n        elif str(df[x].dtype)[:3] in 'flo':\n            floats.append(x)\n        else:\n            continue\n    return strings,integers, floats","76c1b725":"s, i, f = get_lists_of_dtypes(df)","0fc6c13d":"s","6d957e6c":"def prep_categorical_features(s):\n    \"\"\"\n    helper function to return features that we want to one hot encode\n    \"\"\"\n    one_hot = list()\n    binary = list()\n    for x in s:\n        for k, v in x.items():\n            if v > 2:\n                one_hot.append(k)\n            else:\n                binary.append(k)\n    return one_hot, binary","63ab278d":"one_hot, binary = prep_categorical_features(s)","7c35100d":"def pairwise(col_1, col_2):\n    \"\"\"\n    calculates pairwise features\n    for given two dataframe columns\n    \"\"\"\n    tot = col_1 + col_2\n    diff = col_1 - col_2\n    ratio = col_1\/col_2\n    return tot, diff, ratio","9ccc562c":"def stats(col):\n    \"\"\"\n    calculates stats for given\n    dataframe column\n    \"\"\"\n    mini = col.min()\n    maxi = col.max()\n    avg = col.mean()\n    return mini, maxi, avg","3399be9b":"def feature_engine_numericals(dff, i, f):\n    \"\"\"\n    Expands dataframe based on current lists of\n    numerical features (int, floats)\n    \"\"\"\n    numericals = i + f\n    df = dff.copy()\n    for x in numericals:\n        df[f'{x}_min'], df[f'{x}_max'], df[f'{x}_mean'] = stats(df[x])\n        for e in numericals:\n            if e==x:\n                pass\n            else:\n                df[f'sum_{x}_{e}'], df[f'diff_{x}_{e}'], df[f'ratio_{x}_{e}'] = pairwise(df[x], df[e])\n    return df","579bd293":"%%time\npair_df = feature_engine_numericals(df, i, f)","99e66a93":"def feature_engine_categoricals(dff, binary, one_hot):\n    \"\"\"\n    Function to expand dataframe by one-hot encoding\n    categorical variables also, changes datatype to float\n    \"\"\"\n    df = dff.copy()\n    lb = LabelBinarizer()\n    for b in binary:\n        df[f'{b}_tr'] = lb.fit_transform(df[b]).astype(np.float64)\n        df = df.drop(columns=b)\n    df = pd.get_dummies(df, columns=one_hot, dtype=float)\n    return df","0d8310a8":"%%time\npair_df = feature_engine_categoricals(pair_df, binary, one_hot)","f6379e4f":"pair_df.shape, df.shape","0fa6eb8a":"pair_df.head()","bfa5899b":"le = LabelEncoder()","a9f9fe09":"lab = le.fit_transform(pair_df.Churn).astype(np.float64)","4423c160":"l = pd.DataFrame({'lbls':pair_df.Churn, 'l_tr':lab})","bfd0b7c6":"l.head()","8a4bdc13":"feats = pair_df.iloc[:,2:]","5fa14474":"x_train, x_test, y_train, y_test = train_test_split(feats, lab, test_size = .25, random_state = 7)","91453666":"x_tr, x_ev, y_tr, y_ev = train_test_split(x_train, y_train, test_size = .05, random_state = 7)","31cbdecb":"x_tr.shape, x_ev.shape, y_tr.dtype, y_ev.dtype","e5b572d7":"train_data = lgbm.Dataset(data=x_tr, label=y_tr)\nval_data = lgbm.Dataset(data=x_ev, label=y_ev)","ae11a24c":"# tuning copied from https:\/\/www.kaggle.com\/avanwyk\/a-lightgbm-overview\n# Note that there is no param search here, as this is meant to be a base line model.\n\nadvanced_params = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'auc',\n    \n    'learning_rate': 0.1,\n    'num_leaves': 141, # more leaves increases accuracy, but may lead to overfitting.\n    \n    'max_depth': 7, # the maximum tree depth. Shallower trees reduce overfitting.\n    'min_split_gain': 0, # minimal loss gain to perform a split\n    'min_child_samples': 21, # or min_data_in_leaf: specifies the minimum samples per leaf node.\n    'min_child_weight': 5, # minimal sum hessian in one leaf. Controls overfitting.\n    \n    'lambda_l1': 0.5, # L1 regularization\n    'lambda_l2': 0.5, # L2 regularization\n    \n    'feature_fraction': 0.7, # randomly select a fraction of the features before building each tree.\n    # Speeds up training and controls overfitting.\n    'bagging_fraction': 0.5, # allows for bagging or subsampling of data to speed up training.\n    'bagging_freq': 0, # perform bagging on every Kth iteration, disabled if 0.\n    \n    'scale_pos_weight': 99, # add a weight to the positive class examples (compensates for imbalance).\n    \n    'subsample_for_bin': 200000, # amount of data to sample to determine histogram bins\n    'max_bin': 1000, # the maximum number of bins to bucket feature values in.\n    # LightGBM autocompresses memory based on this value. Larger bins improves accuracy.\n    \n    'nthread': 4, # number of threads to use for LightGBM, best set to number of actual cores.\n}","5fc3be47":"# train function from https:\/\/www.kaggle.com\/avanwyk\/a-lightgbm-overview\ndef train_gbm(params, training_set, validation_set, init_gbm=None, boost_rounds=100, early_stopping_rounds=0, metric='auc'):\n    evals_result = {} \n\n    gbm = lgbm.train(params, # parameter dict to use\n                    training_set,\n                    init_model=init_gbm, # initial model to use, for continuous training.\n                    num_boost_round=boost_rounds, # the boosting rounds or number of iterations.\n                    early_stopping_rounds=early_stopping_rounds, # early stopping iterations.\n                    # stop training if *no* metric improves on *any* validation data.\n                    valid_sets=validation_set,\n                    evals_result=evals_result, # dict to store evaluation results in.\n                    verbose_eval=True) # print evaluations during training.\n    \n    return gbm, evals_result","1dcefdde":"gbm, evals_result = train_gbm(advanced_params, training_set=train_data, validation_set=val_data,\n                             boost_rounds=1000, early_stopping_rounds=50)","47f63fb7":"y_hat = gbm.predict(x_test)","c9cc1719":"test_res = pd.DataFrame({'y_true':y_test, 'y_hat':y_hat})","9b2e2840":"test_res.y_hat[test_res.y_true == 0].shape, test_res.y_hat[test_res.y_true == 1].shape","be43e208":"roc_auc_score(test_res.y_true, test_res.y_hat)","e6d0284d":"test_res.y_hat[test_res.y_true == 0].describe()","93e86b7c":"test_res.y_hat[test_res.y_true == 1].describe()","2c48aa64":"def plot_distro(df, col = '', fiter_col = '', y_limit=None, x_limit_r=None, x_limit_l = None):\n    plt.figure(figsize=(10,10))\n    plt.grid(True)\n    sns.distplot(df[col][df[fiter_col] == 1])\n    sns.distplot(df[col][df[fiter_col] == 0])\n    plt.legend(['churn_flag_yes', 'churn_flag_no'])\n    if y_limit:\n        plt.ylim(top=y_limit)\n    if x_limit_r:\n        plt.xlim(right=x_limit_r)\n    if x_limit_l:\n        plt.xlim(left=x_limit_l)","e093e2c8":"plot_distro(test_res, col = 'y_hat', fiter_col = 'y_true', y_limit=None, x_limit_r=None, x_limit_l = .8)","fe772da5":"plt.figure(figsize=(10,10))\nplt.grid(True)\nsns.distplot(np.exp(test_res.y_hat[test_res.y_true == 0]), color='green')\nsns.distplot(np.exp(test_res.y_hat[test_res.y_true == 1]), color='red')\nplt.title('Distribution of the results, for two classes')\nplt.legend(['no_churn', 'churn'])","3fe042f6":"plt.figure(figsize=(12,12))\nplt.grid(True)\nsns.distplot(1\/np.log(test_res.y_hat[test_res.y_true == 0]), color='green')\nsns.distplot(1\/np.log(test_res.y_hat[test_res.y_true == 1]), color='red')\nplt.plot([-36.3, -36.3], [0, 0.024], 'bo--', linewidth=2.5)\nplt.plot([-45, -45], [0, 0.024], 'go--', linewidth=2.5)\nplt.title('Distribution of the results, for two classes with upper thresholds')\n\nplt.legend(['best_auc_threshold','threshold_business', 'no_churn', 'churn'])","1a74f87e":"1\/np.log(test_res['y_hat'][test_res['y_true'] == 1]).describe()","be2c73ed":"test_res['y_transformed'] = 1\/np.log(test_res['y_hat'])","b39e8f28":"def plot_roc_curve(test_res, threshold = -39):\n    ns_probs = [0 for _ in range(len(test_res))]\n    fpr, tpr, threshold = roc_curve(test_res.y_true, np.where(test_res.y_transformed < threshold, 1, 0))\n    _fpr_, _tpr_, _threshold_ = roc_curve(test_res.y_true, ns_probs)\n    roc_auc = auc(fpr, tpr)\n    plt.figure(figsize=(10,10))\n    plt.grid(True)\n    plt.title(\"ROC Curve. Area under Curve: {:.3f}\".format(roc_auc))\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    _ = plt.plot(fpr, tpr, 'r')\n    __ = plt.plot(_fpr_, _tpr_, 'b', ls = '--' )","30d75496":"plot_roc_curve(test_res, -36.3)","8549bf7a":"plot_roc_curve(test_res, -45)","11caf72b":"test_res.y_transformed","63d2cef6":"lgbm.plot_importance(gbm, figsize=(10,12), max_num_features=15,importance_type='split' )","fd2a77b6":"lgbm.plot_importance(gbm, figsize=(10,12), max_num_features=15,importance_type='gain' )","5601c9d9":"def plot_conf_mat(cm):\n    \"\"\"\n    Helper function to plot confusion matrix.\n    With text centerred.\n    \"\"\"\n    plt.figure(figsize=(8,8))\n    ax = sns.heatmap(cm, annot=True,fmt=\"d\",annot_kws={\"size\": 16})\n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)","aa86c57d":"# AUC\ncm_auc = confusion_matrix(test_res.y_true, np.where(test_res.y_transformed < -36.3, 1, 0), labels=[0, 1])","363f1111":"plot_conf_mat(cm_auc)","25158c38":"# Business\ncm_bus = confusion_matrix(test_res.y_true, np.where(test_res.y_transformed < -43, 1, 0), labels=[0, 1])","70ede610":"plot_conf_mat(cm_bus)","83b95d39":"# Our testing population\ntest_res.y_true.value_counts()","2cdfad1a":"test_res.head()","32a91d5a":"# first threshold to explore is AUC oriented -36.3\nauc_based = test_res[test_res.y_transformed <= -36.3]\nauc_based.y_true.value_counts()","c5ec987d":"auc_based.y_true.value_counts()\/auc_based.shape[0]","d495d865":"# second threshold to explore would be Business oriented -43\nbus_based = test_res[test_res.y_transformed <= -43]\nbus_based.y_true.value_counts()","27b76cf2":"bus_based.y_true.value_counts()\/bus_based.shape[0]","7096b61d":"df.MonthlyRevenue.describe()","f4d2b509":"# average per month and retention rate\navg_pm = 58.5\nret_rate = .6","78922149":"def clv(avg_pm, ret_rate):\n    \"\"\"\n    Example calculation of CLV per year\n    with assumed retention rate of customers.\n    \"\"\"\n    clv = 12 * avg_pm \/ (ret_rate\/(1-ret_rate))\n    return clv","951e8909":"cust_lv = clv(avg_pm, ret_rate)","70f0e37f":"cust_lv","e24e5c04":"# so we will have 4000 customers with auc model\n# and 2650 from business model\n# retention rate would be the same\nnew_avg = 58.5 * .9","7d1fb976":"new_avg","56313b06":"campaign_clv = clv(new_avg, ret_rate)","51138d0b":"campaign_clv","5bacbb8e":"auc_campaing = 4000 * campaign_clv\nbus_campaign = 2650 * campaign_clv","349563b5":"auc_campaing, bus_campaign","ea61ef8a":"pr = precision_score(test_res.y_true, np.where(test_res.y_transformed < -43, 1, 0))\npr","ee94390c":"acc = accuracy_score(test_res.y_true, np.where(test_res.y_transformed < -43, 1, 0))\nacc","09b722c7":"precision_score(test_res.y_true, np.where(test_res.y_transformed < -36.3, 1, 0))","b1a9332f":"accuracy_score(test_res.y_true, np.where(test_res.y_transformed < -36.3, 1, 0))","6c165ce4":"f1_score(test_res.y_true, np.where(test_res.y_transformed < -43, 1, 0))","9ccef057":"f1_score(test_res.y_true, np.where(test_res.y_transformed < -36.3, 1, 0))","3dcc1a8e":"### Split for train test","0d30ca7e":"### Customer tenure and churn","46696b53":"### Here we visualise matrices. \n> First from auc view point\n\n> Second what I call business view point","000b3886":"### Feature engineering on categoricals transformations","0946d3b2":"#### From above list we can remove straight away: MonthlyRevenue, MonthlyMinutes, TotalRecurringCharge, RoamingCalls, PercChangeMinutes, PercChangeRevenues, Handsets, HandsetModels, CurrentEquipmentDays, OverageMinutes.\n#### As, above features simply could have zero value or be not present at all. We'll set them to zero 'safely'. We'll investigate the remaining lot!","3ae29863":"### <a id='1.2'>1.2. Missing Values exploration<\/a>","f651981d":"... hmmmm interesting, recall from above that our churn population in data set is 14711, and with this feature we're capturing a lot of churners. However, way more non-churnes to be fair.","b9997b26":"### <a id='4'>4. The consequences of our model<\/a>","a93d7615":"> 1 - customers who have churned\n\n> 0 - customers retained","c77e74fe":"### <a id='4.1'>4.1 Feature Importance<\/a> ","d4e6142e":"### Boom! We have missing vals but why? Let's roll up sleeves and check.\n#### Despite number of techniques available, we will use common sense most of all, to determine whether or not missing value is there for a reason or missing at random","8e68ece8":"### In this example we looked at the churn problem with somewhat in-depth analysis. Moreover, this example should show you the journey that most of the models built in industry are going through. Before you educate business to the terms that Data Scientists are using, you might want to simplify your solution to one that you can fully explain. Very few business units would sing-up for solution they don\u2019t trust and understand. Building trustworthy solution is rather lengthy process, so gear-up some patience and keep going it will work eventually.\n### I would like to stress, that this is not a full solution. I got inspired by this excellent example of analysis, visualisation and multiple models in this kernel https:\/\/www.kaggle.com\/pavanraj159\/telecom-customer-churn-prediction. \n\n## Next steps:\n### Build multiple models to improve precision or accuracy, this you can always get from the business, they know their costs, once you explain the upside and downside of each solution. Setup A\/B test to see how well campaigns are doing against treatment and control groups. Consider lift modelling to optimise specific KPI or business target. You also could like my github repo \ud83d\ude0a https:\/\/github.com\/jackall\/churn_analysis, and upvote this kernel \ud83d\ude0a if you found it useful.","88e998ef":"### Model params","c0817eab":"### <a id='5'>5. Conclusions:<\/a>","2f2a2632":"### <a id='1'>1. EDA<\/a>","04bd5c30":"### The usual suspects missing values check","9bcf0fa3":"### <a id='3.2'>3.2. Threshold selection<\/a>","f2efdbe9":"### <a id='2.1'>2.1. Feature engineering<\/a>","6ae64b97":"# Yet another look at Telco customer churn problem","bf0923d7":"### Not much of a difference visible from this perspective, let's zoom in","81c32e48":"### <a id='1.4'>1.4. Data distributions<\/a>\n### Data distributions","26980bb2":"### We know now that business-oriented campaign will be smaller targeting 3.5k customers of which 55% wouldn't churn and 45% with higher churn propensity.","9e5be04d":"### Split train for train and eval","434ef0d7":"### From above feature importance plots, we see that almost all most important features are engineered.","ba8e666d":"### <a id='1.1'>1.1. Exploratory data analysis<\/a>","c77eb656":"does it have an impact on churn?","1ddbc153":"### Let's look at type of features and how we could categorize them","e2c3d2b8":"### If we assume that retention campaign would offer and extra 10% discount per month, we can see the impact right away for both models auc and business one. \n### To view that let's assume that campaign retains 50% of customers with high churn propensity","a9f455fd":"### I think we can zero-out this feature","3a6b4f02":"### Transform results accordingly","4ca674e5":"### <a id='4.3'>4.3 Campaigning the model<\/a>","597dc5ad":"### Model train","e37ccadb":"### Some missing values conclusions.\nIn my opinion is safe to 0 substitute all missing variables in this data set.\nAs we showing above, there could be some data missing at random, however vast majority is due to the processes like data gathering, data entering, choice for customer that results in missing information.","95b5f39a":"### Ratios\nWe take a note of class imbalance","5817d1d0":"### Now we can investigate how to select the threshold for setting churn flag = 1\n### Above plot implies that there's no perfect separation, as this is always the case in real life\n### Therefore, we have to look for so called 'swee tspot' where we maximise impact of the model\n### In the industry treshold is usually determined by business who uses model\n### In Kaggle we're looking for best AUC threshold, if that's the metric on leader board\n### I will focus on business case here. You all know how to do it the other way ;)\n#### from below descriptive stats we could start at -39","a1cca6c7":"### <a id='3'>3. Modelling<\/a>","2defa938":"## TL;DR\n### This notebook aims at showing the difference between Kaggle world and the real-world using telco churn dataset. An example, how you could look at the problem and build applicable solution, that might be acceptable by the business. \n### How to do some of the feature engineering and how to create training dataset that makes sense\n### How to build a baseline model - I am not focusing on tuning the model or building multiple models, you all know how to do that better than me.\n### I am trying to focus on consequences of the model and how important it is to put modelling into right business context and be able to show model value. ","d18289ac":"### <a id='2.1'>2.1. Feature preparation<\/a>","19c79e91":"### <a id='4.2'>4.2 Unconfusing confusion matrices<\/a>","eae49f9b":"### Each model deployed in production or considered for deployment has its consequences. Rarely spoken in ML literature in these terms. As we, Data Scientists, tend to talk in True Positive, True Negatives, False Positives, False Negatives terms. We need to bring these terms to business in their jargon, especially to be able to explain the impact and put Dollar figure on top of the results. In my experience the simpler these terms are the more successful you will be with productionizing the model, that's what we aim for, right?\n### So, here we will focus a bit on how business see this problem and what we Data Scientist might do to maximise chance of success","125ce0f5":"### We've left with missing vals in Age let's look at stats first","964c657a":"### <a id='1.3'>1.3. Churn breakdown<\/a>","195cc344":"### One of the ways to look at this problem from business point of view is consider Customer Lifetime Value (CLV). \n\n### Where CLV = Avg. revenue per period x [ Retention Rate \/ (1 - Retention Rate) ] as per http:\/\/www.customer-lifetimevalue.com\/\n\n### Here we will make the following assumptions. period 12 months, retention rate 0.6","df380a6c":"### Let's transform the results to separate two distributions as far as possible","5f9c4a6c":"#### Stats are showing that the most values are zero, so this could be calls to call centre, where customer said \n### 'I want to speak to your MANAGER!'","1db75fa9":"### AUC campaign discounts 3k customers at 10%, BUS campaign discounts 1.9k customers at 10%. So, during the next retention cycle, customers would expect another discount for loyalty. In my opinion is better to have campaigns optimized for precision","ec67272b":"### Prep numerical pairwise features\n### as in many feature engineering techniques we will use some of the well known tricks\n> pairwise sum, diff, ratio\n\n> min, max, mean\n\n### Let's write a function to do just that","218831e5":"### So, our model would target over 5k customers in a campaign 60% of which wouldn't churn, 40% with higher churn propensity. Here the model would offer incentives to 3k customers that don't need them and 2k that might need them. The question here is, how many of 2k customers we would retain and how many would churn anyway?\n\n### Unfortunately, there's no ideal solution we'd have to run campaign and study the results with A\/B tests","48e8a6a6":"## Let's take a look at churn and how it's reflected in some of the features","cf046151":"## Let's investigate strings and number of unique values\n### Below we can see that features with 2 unique values are already encoded\n### as binary type [0,1] - so we can leave them as they are\n### remaining features we will one hot encode including ServiceArea despite having 748 unique values\n### as this feature might have an impact on model, due to larger population density\n### therefore, might have higher\/lower churn propensity","a9b7ffdd":"### Here we'll look at confusion matrices, where we can see how change in the threshold impacts the results. Having the results in this form we have one foot in the door. But still we need more explanation.\n\n### So, here is a little bit of industry context. Most of the companies these days have models that have customer retention as a main goal. These models are run by Marketing teams or CVM teams (Customer Value Management) or perhaps some other function. As Data Scientist you need to gain their trust. They won't buy in to Blackbox solution right away i.e. Deep Learning model, they will have to understand what's going on, under the hood. This we can achieve in multiple ways, some examples:\n> Feature importance that make sense to the business, tree-based models are good for that, but sometimes results are funny\n\n> Confusion matrix explanation and business impact of the model","e0f7bf87":"### Most of categoricals are binary so let's see how the churn is reflected in some other variables","264c1dc1":"### <a id='3.1'>3.1. Analysing and transforming results<\/a>","dcf04cdf":"- <a href='#1'>1. EDA<\/a>\n    - <a href='#1.1'>1.1. Exploratory data analysis<\/a>\n    - <a href='#1.2'>1.2. Missing Values exploration<\/a>\n    - <a href='#1.3'>1.3. Churn breakdown<\/a>\n    - <a href='#1.4'>1.4. Data distributions against Churn flag<\/a>\n- <a href='#2'>2. Feature preparation<\/a>\n    - <a href='#2.1'>2.1. Feature engineering<\/a>\n- <a href='#3'>3. Modelling<\/a>\n    - <a href='#3.1'>3.1. Analysing and transforming results<\/a>\n    - <a href='#3.2'>3.2. Threshold selection<\/a>\n- <a href='#4'>4. The consequences of our model<\/a>\n    - <a href='#4.1'>4.1. Feature Importance<\/a>\n    - <a href='#4.2'>4.2. Unconfusing confusion matrices<\/a>\n    - <a href='#4.3'>4.3. Campaigning the model<\/a>\n    - <a href='#4.4'>4.4. Customer Lifetime Value<\/a>\n- <a href='#5'>5. Conclusions<\/a>","c5ae94f1":"### So now, let's see how the model would be campaigned, and what would be an impact of the model","6d3951e7":"### Encode labels and separate features and labels","9c38962b":"### <a id='4.4'>4.4 Customer Lifetime Value<\/a>","579129ff":"### How many records are impacted by 0 age\nThis could be due to many reasons:\n> customers given option not to report age\n\n> 'Clerical' error\n\n> recent change in system\n\n> ETL process crashed\n","1cf49ae4":"### Let's note that differences are just subtle, there are no obvious relationships between Occupation and churn","b88d782a":"### Class sizes"}}