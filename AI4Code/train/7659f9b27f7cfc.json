{"cell_type":{"31c00960":"code","a1fb21f7":"code","0402638c":"code","ad9cb839":"code","54ee4b4e":"code","08f410d0":"code","71449d54":"code","8b264478":"code","7147f098":"code","78a8e127":"code","6146d4e0":"code","6cbc5223":"code","6bf67ef6":"code","e6f943f9":"code","b326ac14":"code","a974aa6d":"code","61c4befa":"code","3c971dde":"code","b16b9e25":"code","ef3be2d1":"code","0fe43f5f":"code","7593e18b":"code","9645e18a":"code","01024507":"code","3c3b107c":"code","9ef17eea":"code","f7ad665c":"code","4e471192":"code","7270117c":"code","2da4e561":"code","8eb565d4":"code","869e3f1d":"code","c53d3bcc":"code","f2de4a51":"code","e806fc01":"code","505df75a":"code","290e0ac6":"code","b1d8c10d":"code","b1fce4fe":"code","8f57ea5d":"code","5eec8bd7":"code","487ddf8b":"code","398ced81":"code","ae5b0184":"code","ffbaa5c7":"code","34a960c3":"code","232c099f":"code","e530a49f":"code","f48f9c6a":"code","d1386870":"code","66f17edd":"code","d95f6b24":"code","f0cdac1c":"markdown","d35e5f39":"markdown","6ef4b81b":"markdown","c4990517":"markdown","39c3b8ce":"markdown","c768566a":"markdown","6c023038":"markdown","0c393a99":"markdown","2743da16":"markdown","1d3492e6":"markdown","21c74480":"markdown","b1bdc57d":"markdown","b814ab42":"markdown","1fbb21f1":"markdown","e8987b17":"markdown","c90c790b":"markdown","5e480475":"markdown","7765b896":"markdown","d73dab6f":"markdown","d4fca57d":"markdown","2e67361b":"markdown","b20c7152":"markdown","212e15e8":"markdown","bd051a8c":"markdown","e8399002":"markdown","d3278065":"markdown","05bcdd55":"markdown","f47e38c6":"markdown","0e0b14cf":"markdown","260419f5":"markdown","aba59c98":"markdown","8d19d745":"markdown","c39cadf3":"markdown","a80ab1a7":"markdown","7b313e76":"markdown"},"source":{"31c00960":"import numpy as py\nimport pandas as pd\nimport os\nimport math\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import plot\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","a1fb21f7":"############################\n######## Reading Data\n############################\n# Path of the file to read. \n\niowa_file_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_data_path = '..\/input\/home-data-for-ml-course\/test.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\ntest_home_data = pd.read_csv(test_data_path)","0402638c":"list(home_data.columns[1:80])","ad9cb839":"for col in list(home_data.columns):\n    if sum(pd.isna(home_data[col]))>0:\n        print(col, sum(pd.isna(home_data[col])), \"Total Number\", len(home_data[col]))","54ee4b4e":"plt.hist(home_data.SalePrice, bins = 200)\nplt.ylabel('Counts')\nplt.xlabel('Sale Price (USD)')\nplt.ylim(0,52)\nx = [py.mean(home_data.SalePrice) + 3*py.std(home_data.SalePrice) for i in range(60)]\nh = plt.plot(x, range(60), color='g', alpha=1)\nplt.annotate('Outlier Limit',xy = (py.mean(home_data.SalePrice) + 3*py.std(home_data.SalePrice),45), xycoords='data',\n            xytext=(500000,45), arrowprops=dict(arrowstyle=\"->\",facecolor='black'))    \nplt.show()\n","08f410d0":"len(home_data.SalePrice[home_data.SalePrice>=py.mean(home_data.SalePrice) + 3*py.std(home_data.SalePrice)])","71449d54":"X = pd.read_csv(iowa_file_path)# Our full training dataset\nX_test_full = pd.read_csv(test_data_path)# Our testing dataset, that we will make predictions on\n\n# Remove rows with missing target, and separating the target from the predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny_true = X.SalePrice ## keeping to compare fit\ny_train = py.log(X.SalePrice)              \nX.drop(['SalePrice'], axis=1, inplace=True)","8b264478":"def root_mean_square(X,y):\n    val = mean_squared_error(X,y)\n    return val**(1\/2)\nbasic_score = make_scorer(root_mean_square)\ndef root_mean_square_X(X,y):\n    val = mean_squared_error(py.log(X),py.log(y))\n    return val**(1\/2)\nbasic_score_X = make_scorer(root_mean_square_X)","7147f098":"all_categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\nordinal_categorical_cols = []\nord_vals = ['Gd', 'RFn', 'Mod', 'ALQ', 'IR1', 'Bnk', 'ELO']\nfor col in all_categorical_cols:\n    sety = set(X[col])\n    if sety.intersection(ord_vals) != set():\n        ordinal_categorical_cols.append(col)\n\ndummy_categorical_cols = list(set(all_categorical_cols)-set(ordinal_categorical_cols))","78a8e127":"home_data.MSSubClass.dtype","6146d4e0":"vars_to_remove = 'MSSubClass'\ndummy_categorical_cols.append('MSSubClass')","6cbc5223":"#### First lets separate out the numeric columns\nnumeric_cols = [cname for cname in X.columns if \n                X[cname].dtype in ['int64', 'float64']]\n### we also have to make sure to take out the MSSubClass variable\nnumeric_cols.remove(vars_to_remove)\n\n#### Now we need to make our new encoded datasets\nmy_cols = dummy_categorical_cols + ordinal_categorical_cols + numeric_cols\nX_train = X[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","6bf67ef6":"#### This should encode the ordinal categorical varibles ordinally\n## We have to be careful to set the paraticular encoding (e.g. 'Good': 2), so that\n## the automatic coding that LabelEncoder does won't misclassify our ordinal variables\n##\n## We will use Pandas Factorize\nord_set1 = ['LotShape']\nord_set2 = ['LandContour']\nord_set3 = ['LandSlope']\nord_set4 = ['Functional']\nord_set5 = ['GarageFinish']\nord_set6 = ['PoolQC']\nord_set7 = ['BsmtExposure']\nord_set8 = ['ExterQual','ExterCond','HeatingQC','KitchenQual']\nord_set9 = ['BsmtQual','BsmtCond','FireplaceQu','GarageQual','GarageCond']\nord_set10 = ['BsmtFinType1','BsmtFinType2']\nOrd_Set_All = [ord_set1,ord_set2,ord_set3,ord_set4,ord_set5,ord_set6,ord_set7,ord_set8,ord_set9,ord_set10]\n##\nord_para1 = {'Reg':3, 'IR1':2, 'IR2':1, 'IR3':0}\nord_para2 = {'Lvl':3, 'Bnk':2, 'HLS':1, 'Low':0}\nord_para3 = {'Gtl':2, 'Mod':1, 'Sev':0}\nord_para4 = {'Typ':7, 'Min1':6, 'Min2':5, 'Mod':4, 'Maj1':3, 'Maj2':2, 'Sev':1, 'Sal':0}\nord_para5 = {'Fin':3, 'RFn':2, 'Unf':1, 'NA':0}\nord_para6 = {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'NA':0}\nord_para7 = {'Gd':4, 'Av':3, 'Mn':2, 'No':1, 'NA':0}\nord_para8 = {'Ex':4, 'Gd':3, 'TA':2, 'Fa':1, 'Po':0}\nord_para9 = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\nord_para10 = {'GLQ':6, 'ALQ':5, 'BLQ':4, 'Rec':3, 'LwQ':2, 'Unf':1, 'NA':0}\nOrd_Para_All = [ord_para1,ord_para2,ord_para3,ord_para4,ord_para5,ord_para6,ord_para7,ord_para8,ord_para9,ord_para10]\n##\n#### we need to set the parameters for each kind of variable\nfor i in range(len(Ord_Set_All)):\n    ordy = Ord_Set_All[i]\n    ord_mapper = Ord_Para_All[i]\n    for col in ordy:\n        ### For the training data\n        hld = X_train[col].replace(ord_mapper)\n        hld = hld.fillna(0) ### dealing with missing values (as discussed above)\n        X_train[col] = hld\n        ### For the testing data\n        hld = X_test[col].replace(ord_mapper)\n        hld = hld.fillna(0)\n        X_test[col] = hld\n","e6f943f9":"#### Finally we one-hot encode the remaining data using pandas again\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\nX_train, X_test = X_train.align(X_test, join='right', axis=1)","b326ac14":"for col in X_train:\n    if sum(pd.isna(X_train[col]))>0:\n        print(col,sum(pd.isna(X_train[col])))","a974aa6d":"print(home_data.GarageYrBlt[pd.isna(home_data.GarageType)==True])\nprint(len(home_data.GarageYrBlt[pd.isna(home_data.GarageType)==True]))\nprint(sum(pd.isna(home_data.GarageYrBlt)==True))","61c4befa":"print(home_data.MasVnrArea[home_data.MasVnrType=='None'])\nprint(home_data.MasVnrType[pd.isna(home_data.MasVnrArea)==True])\nprint(len(home_data.MasVnrArea[home_data.MasVnrType=='None']))\nprint(sum(pd.isna(home_data.MasVnrArea)==True))","3c971dde":"print(sum(home_data.MasVnrArea==0),len(home_data.MasVnrArea))","b16b9e25":"print(py.mean(home_data.LotFrontage))\nprint(sum(home_data.LotFrontage<75))\nprint(sum(home_data.LotFrontage<=1))\n","ef3be2d1":"numeric_missing = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\nfor col in numeric_missing:\n    if col == 'MasVnrArea' or col == 'GarageYrBlt':\n    ### For the training data\n        hld = X_train[col]\n        hld = hld.fillna(0) ### dealing with missing values (as discussed above)\n        X_train[col] = hld\n    ### For the testing data\n        hld = X_test[col]\n        hld = hld.fillna(0)\n        X_test[col] = hld\n    elif col == 'LotFrontage':\n        lot_mean = py.mean(home_data[col])\n    ### For the training data\n        hld = X_train[col]\n        hld = hld.fillna(lot_mean) ### dealing with missing values (as discussed above)\n        X_train[col] = hld\n    ### For the testing data\n        hld = X_test[col]\n        hld = hld.fillna(lot_mean)\n        X_test[col] = hld\n        \n        ","0fe43f5f":"for col in X_test:\n    if sum(pd.isna(X_test[col]))>0:\n        print(col,sum(pd.isna(X_test[col])))","7593e18b":"print(X_test['BsmtFinSF2'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtUnfSF'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['TotalBsmtSF'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtFullBath'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtHalfBath'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtExposure'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtFinType1'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtFinType2'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtQual'][pd.isna(X_test['BsmtFinSF1'])==True])\nprint(X_test['BsmtCond'][pd.isna(X_test['BsmtFinSF1'])==True])\n\nprint(X_test['BsmtQual'][pd.isna(X_test['BsmtFullBath'])==True])\nprint(X_test['BsmtCond'][pd.isna(X_test['BsmtFullBath'])==True])\nprint(X_test['BsmtQual'][pd.isna(X_test['BsmtHalfBath'])==True])\nprint(X_test['BsmtCond'][pd.isna(X_test['BsmtHalfBath'])==True])\n","9645e18a":"print(X_test['GarageArea'][pd.isna(X_test['GarageCars'])==True])\nprint(X_test['GarageQual'][pd.isna(X_test['GarageCars'])==True])\nprint(X_test['GarageYrBlt'][pd.isna(X_test['GarageCars'])==True])\nprint(X_test['GarageCond'][pd.isna(X_test['GarageCars'])==True])\nprint(X_test['GarageFinish'][pd.isna(X_test['GarageCars'])==True])\nprint(X_test['GarageType_Detchd'][pd.isna(X_test['GarageCars'])==True])\n","01024507":"test_missing = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF',\n                   'BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea']\nfor col in test_missing:\n        hld = X_test[col]\n        hld = hld.fillna(0)\n        X_test[col] = hld","3c3b107c":"X_train['Overall_Score'] = X_train['OverallQual'] + X_train['OverallCond']\nX_train['Overall_Score_sq'] = X_train['Overall_Score']**2\nX_train['Overall_Score_cu'] = X_train['Overall_Score']**3\nX_train['Overall_Score_sqrt'] = X_train['Overall_Score']**(1\/2)\nX_train['Overall_Score_log'] = py.log(X_train['Overall_Score'])\n\nX_train['Total_Living_SF'] = X_train['TotalBsmtSF'] + X_train['1stFlrSF'] + X_train['2ndFlrSF']\nX_train['Total_Living_SF_log'] = py.log(X_train['Total_Living_SF'])\nX_train['Total_Living_SF_sq'] = X_train['Total_Living_SF']**2\nX_train['Total_Living_SF_cu'] = X_train['Total_Living_SF']**3\nX_train['Total_Living_SF_sqrt'] = X_train['Total_Living_SF']**(1\/2)\n\nX_train['GrLivArea_log'] = py.log(X_train['GrLivArea'])\nX_train['GrLivArea_sq'] = (X_train['GrLivArea'])**2\nX_train['GrLivArea_cu'] = (X_train['GrLivArea'])**3\nX_train['GrLivArea_sqrt'] = (X_train['GrLivArea'])**(1\/2)\n\nX_train['OverallQual_log'] = py.log(X_train['OverallQual'])\nX_train['OverallQual_sq'] = (X_train['OverallQual'])**2\nX_train['OverallQual_cu'] = (X_train['OverallQual'])**3\nX_train['OverallQual_sqrt'] = (X_train['OverallQual'])**(1\/2)\n\nX_train['GarageCars_sq'] = (X_train['GarageCars'])**2\nX_train['GarageCars_cu'] = (X_train['GarageCars'])**3\nX_train['GarageCars_sqrt'] = (X_train['GarageCars'])**(1\/2)\n\nX_train['GarageArea_sq'] = (X_train['GarageArea'])**2\nX_train['GarageArea_cu'] = (X_train['GarageArea'])**3\nX_train['GarageArea_sqrt'] = (X_train['GarageArea'])**(1\/2)\n\nX_train['YearBuilt_diff'] = X_train['YrSold']-X_train['YearBuilt']\nX_train['YearBuilt_diff_sq'] = X_train['YearBuilt_diff']**2\nX_train['YearBuilt_diff_cu'] = X_train['YearBuilt_diff']**3\n\nX_train['YearRemod_diff'] = X_train['YrSold']-X_train['YearRemodAdd']\nX_train['YearRemod_diff_sq'] = X_train['YearRemod_diff']**2\nX_train['YearRemod_diff_cu'] = X_train['YearRemod_diff']**3\n\nX_train['Total_Outside_SF'] = (X_train['WoodDeckSF'] + X_train['OpenPorchSF'] + X_train['EnclosedPorch'] \n                                 + X_train['3SsnPorch'] +X_train['ScreenPorch'] + X_train['PoolArea']\n                                + X_train['GarageArea'])\n\nX_train['Total_Bathrooms'] = X_train['BsmtFullBath'] + (1\/2)*X_train['BsmtHalfBath'] + X_train['FullBath'] + (1\/2)*X_train['HalfBath']\nX_train['Total_Bathrooms_log'] = py.log(X_train['Total_Bathrooms'])\nX_train['Total_Bathrooms_sq'] = (X_train['Total_Bathrooms'])**2\nX_train['Total_Bathrooms_cu'] = (X_train['Total_Bathrooms'])**3\nX_train['Total_Bathrooms_sqrt'] = (X_train['Total_Bathrooms'])**(1\/2)\n\nX_train['Total_Rooms'] = X_train['TotRmsAbvGrd'] + X_train['Total_Bathrooms']\nX_train['Total_Rooms_log'] = py.log(X_train['Total_Rooms'])\nX_train['Total_Rooms_sq'] = (X_train['Total_Rooms'])**2\nX_train['Total_Rooms_cu'] = (X_train['Total_Rooms'])**3\nX_train['Total_Rooms_sqrt'] = (X_train['Total_Rooms'])**(1\/2)","9ef17eea":"# For X_test\nX_test['Overall_Score'] = X_test['OverallQual'] + X_test['OverallCond']\nX_test['Overall_Score_sq'] = X_test['Overall_Score']**2\nX_test['Overall_Score_cu'] = X_test['Overall_Score']**3\nX_test['Overall_Score_sqrt'] = X_test['Overall_Score']**(1\/2)\nX_test['Overall_Score_log'] = py.log(X_test['Overall_Score'])\n\nX_test['Total_Living_SF'] = X_test['TotalBsmtSF'] + X_test['1stFlrSF'] + X_test['2ndFlrSF']\nX_test['Total_Living_SF_log'] = py.log(X_test['Total_Living_SF'])\nX_test['Total_Living_SF_sq'] = X_test['Total_Living_SF']**2\nX_test['Total_Living_SF_cu'] = X_test['Total_Living_SF']**3\nX_test['Total_Living_SF_sqrt'] = X_test['Total_Living_SF']**(1\/2)\n\nX_test['GrLivArea_log'] = py.log(X_test['GrLivArea'])\nX_test['GrLivArea_sq'] = (X_test['GrLivArea'])**2\nX_test['GrLivArea_cu'] = (X_test['GrLivArea'])**3\nX_test['GrLivArea_sqrt'] = (X_test['GrLivArea'])**(1\/2)\n\nX_test['OverallQual_log'] = py.log(X_test['OverallQual'])\nX_test['OverallQual_sq'] = (X_test['OverallQual'])**2\nX_test['OverallQual_cu'] = (X_test['OverallQual'])**3\nX_test['OverallQual_sqrt'] = (X_test['OverallQual'])**(1\/2)\n\nX_test['GarageCars_sq'] = (X_test['GarageCars'])**2\nX_test['GarageCars_cu'] = (X_test['GarageCars'])**3\nX_test['GarageCars_sqrt'] = (X_test['GarageCars'])**(1\/2)\n\nX_test['GarageArea_sq'] = (X_test['GarageArea'])**2\nX_test['GarageArea_cu'] = (X_test['GarageArea'])**3\nX_test['GarageArea_sqrt'] = (X_test['GarageArea'])**(1\/2)\n\nX_test['YearBuilt_diff'] = X_test['YrSold']-X_test['YearBuilt']\nX_test['YearBuilt_diff_sq'] = X_test['YearBuilt_diff']**2\nX_test['YearBuilt_diff_cu'] = X_test['YearBuilt_diff']**3\n\nX_test['YearRemod_diff'] = X_test['YrSold']-X_test['YearRemodAdd']\nX_test['YearRemod_diff_sq'] = X_test['YearRemod_diff']**2\nX_test['YearRemod_diff_cu'] = X_test['YearRemod_diff']**3\n\nX_test['Total_Outside_SF'] = (X_test['WoodDeckSF'] + X_test['OpenPorchSF'] + X_test['EnclosedPorch'] \n                                 + X_test['3SsnPorch'] +X_test['ScreenPorch'] + X_test['PoolArea']\n                                + X_test['GarageArea'])\n\nX_test['Total_Bathrooms'] = X_test['BsmtFullBath'] + (1\/2)*X_test['BsmtHalfBath'] + X_test['FullBath'] + (1\/2)*X_test['HalfBath']\nX_test['Total_Bathrooms_log'] = py.log(X_test['Total_Bathrooms'])\nX_test['Total_Bathrooms_sq'] = (X_test['Total_Bathrooms'])**2\nX_test['Total_Bathrooms_cu'] = (X_test['Total_Bathrooms'])**3\nX_test['Total_Bathrooms_sqrt'] = (X_test['Total_Bathrooms'])**(1\/2)\n\nX_test['Total_Rooms'] = X_test['TotRmsAbvGrd'] + X_test['Total_Bathrooms']\nX_test['Total_Rooms_log'] = py.log(X_test['Total_Rooms'])\nX_test['Total_Rooms_sq'] = (X_test['Total_Rooms'])**2\nX_test['Total_Rooms_cu'] = (X_test['Total_Rooms'])**3\nX_test['Total_Rooms_sqrt'] = (X_test['Total_Rooms'])**(1\/2)","f7ad665c":"for col in numeric_cols:\n    hld = X_train[col]\n    hld = (hld-py.mean(hld))\/py.std(hld) ### normalizing\n    X_train[col] = hld\n    hld = X_test[col]\n    hld = (hld-py.mean(hld))\/py.std(hld) ### normalizing\n    X_test[col] = hld","4e471192":"modelOLS = LinearRegression()","7270117c":"CV_RMSE = py.mean(cross_val_score(modelOLS,X_train,y_train,scoring = basic_score,cv = 5))\nprint('Cross-Validated RMSE:',CV_RMSE)#0.3567548888921507","2da4e561":"modelOLS.fit(X_train, y_train)\n\npredictions_X = modelOLS.predict(X_train)\n\nMAE_OLS = mean_squared_error(predictions_X, y_train)**(1\/2)\n\nprint(\"Root Mean Square Error:\", MAE_OLS)#0.09609467307102444","8eb565d4":"resid_v = ((predictions_X)-y_train)**2\nplt.ylabel('Squared Residuals')\nplt.xlabel('Index')\nplt.xlim(0,len(y_train)+10)\nplt.plot(range(len(resid_v)),resid_v, \"o\")\nplt.plot([-10, 2000], [MAE_OLS, MAE_OLS], 'k-', lw=2)\n\nplt.annotate('Root Mean Squared Error',xy = (1400,MAE_OLS), xycoords='data',\n            xytext=(1500,max(resid_v)\/2), arrowprops=dict(arrowstyle=\"->\",facecolor='black'))    \nplt.show()","869e3f1d":"modelOLS.fit(X_train, y_train)\n\nprint(modelOLS.score(X_train, y_train))#0.9420881714862639\n\nmodelOLS.fit(X_train, y_true)\n\nprint(modelOLS.score(X_train, y_true))#0.9326219998440954","c53d3bcc":"DT_model = DecisionTreeRegressor(min_samples_split = 44, min_samples_leaf=2, \n                                 criterion = 'mae',max_features = 57, \n                                 random_state=31415)","f2de4a51":"paraDT = {'min_samples_split':list(range(40,50)),'min_samples_leaf':[1,2],\n          'max_features':list(range(50,60))}\nDT_search = GridSearchCV(DT_model,param_grid = paraDT,\n                   scoring = basic_score, n_jobs = -1, cv = 5,refit = False)","e806fc01":"DT_search.fit(X_train,y_train)\nSearch_Hld = pd.DataFrame(DT_search.cv_results_)\nprint(dict(Search_Hld.params[Search_Hld.rank_test_score==max(Search_Hld.rank_test_score)]))\nprint(min(Search_Hld.mean_test_score))","505df75a":"DT_model = DecisionTreeRegressor(min_samples_split = 45, min_samples_leaf=2, \n                                 criterion = 'mae',max_features = 52, \n                                 random_state=31415)","290e0ac6":"full_CV = cross_val_score(DT_model,X_train,y_train,scoring = basic_score,\n                          cv = 5, n_jobs = -1)\nCV_MAE = py.mean(full_CV)\nprint(full_CV)\nprint('Root Mean Squared Error:',CV_MAE)#0.1774702304319352","b1d8c10d":"DT_model.fit(X_train, y_train)\nDT_val_predictions = DT_model.predict(X_train)\nDT_val_mae = mean_squared_error(DT_val_predictions, y_train)**(1\/2)\nprint(\"Root Mean Squared Error:\", DT_val_mae)# 0.14423140509255775","b1fce4fe":"rf_model = RandomForestRegressor(n_estimators = 1050, min_samples_split = 2, \n                                 min_samples_leaf=1, max_features = 12, \n                                 criterion='mse', n_jobs=-1,random_state=31415)","8f57ea5d":"full_CV = cross_val_score(rf_model,X_train,y_train,scoring = basic_score,\n                          cv = 5, n_jobs = -1)\nCV_RMSE = py.mean(full_CV)\nprint(full_CV)\nprint('Cross-Validated RMSE:',CV_RMSE)#0.13474172228451703","5eec8bd7":"rf_model.fit(X_train, y_train)\nrf_val_predictions = rf_model.predict(X_train)\nrf_val_mae = mean_squared_error(rf_val_predictions, y_train)**(1\/2)\nprint(\"Root Mean Squared Error:\", rf_val_mae)#0.04864128166289313","487ddf8b":"resid_v = (rf_val_predictions-y_train)**2\nplt.ylabel('Squared Residuals')\nplt.xlabel('Index')\nplt.xlim(0,len(y_train)+10)\nplt.plot(range(len(resid_v)),resid_v, \"o\")\nplt.plot([-10, 2000], [rf_val_mae, rf_val_mae], 'k-', lw=2)\n\nplt.annotate('Root Mean Squared Error',xy = (1400,rf_val_mae), xycoords='data',\n            xytext=(1500,max(resid_v)\/2), arrowprops=dict(arrowstyle=\"->\",facecolor='black'))    \nplt.show()","398ced81":"XGBModel2 = XGBRegressor(n_estimators=2000, learning_rate=0.02, \n                         max_depth = 3, subsample = 0.8, colsample_bytree = 0.2, \n                         gamma = 0, n_jobs=-1, random_state = 31415)","ae5b0184":"full_CV = cross_val_score(XGBModel2,X_train,y_train,scoring = basic_score,\n                          cv = 5, n_jobs = -1)\nCV_RMSE = py.mean(full_CV)\nprint(full_CV)\nprint('Cross-Validated RMSE:',CV_RMSE)#0.11686018766610913","ffbaa5c7":"# Fit the model\nXGBModel2.fit(X_train, y_train, \n              eval_metric = 'rmse',\n              verbose=False)\n\n# Get predictions\npredictions_X = XGBModel2.predict(X_train) # Your code here\n\n# Calculate MAE\nmae_X = mean_squared_error(predictions_X, y_train)**(1\/2) # Your code here\nprint(\"Root Mean Square Error:\", (mae_X)) \n#0.048673194399949625","34a960c3":"resid_v = ((predictions_X)-y_train)**2\nplt.ylabel('Squared Residuals')\nplt.xlabel('Index')\nplt.xlim(0,len(y_train)+10)\nplt.plot(range(len(resid_v)),resid_v, \"o\")\nplt.plot([-10, 2000], [mae_X, mae_X], 'k-', lw=2)\nplt.annotate('Root Mean Squared Error',xy = (1400,mae_X), xycoords='data',\n            xytext=(1550,max(resid_v)\/2), arrowprops=dict(arrowstyle=\"->\",facecolor='black'))    \nplt.show()","232c099f":"XGBModel3 = XGBRegressor(n_estimators=2000, learning_rate=1.151, n_jobs=-1, random_state = 31415) # Your code here\n\n# Fit the model\nXGBModel3.fit(X_train, y_train,\n              eval_metric = 'rmse',\n              verbose=True)\n\n\n# Get predictions\npredictions_X = XGBModel3.predict(X_train) # Your code here\n\n# Calculate MAE\nmae_X = mean_squared_error(predictions_X, y_train)**(1\/2) # Your code here\nprint(\"Root Mean Squared Error:\", mae_X) #0.00045737980164642415","e530a49f":"full_CV = cross_val_score(XGBModel3,X_train,y_train,scoring = basic_score,\n                          cv = 5, n_jobs = -1)\nCV_RMSE = py.mean(full_CV)\nprint(full_CV)\nprint('Cross-Validated RMSE:',CV_RMSE)#0.2131412227630994","f48f9c6a":"XGBModel1 = XGBRegressor(n_estimators=2000, learning_rate=0.02, max_depth = 3, \n                         subsample = 0.95, colsample_bytree = 0.05, gamma = 0, \n                         n_jobs=-1, random_state = 31415)\nfull_CV = cross_val_score(XGBModel1,X_train,y_true,scoring = basic_score_X,\n                          cv = 5, n_jobs = -1)\nCV_RMSE = py.mean(full_CV)\nprint(full_CV)\nprint('Cross-Validated RMSE:',CV_RMSE)#0.11916145928557147","d1386870":"XGBModel1.fit(X_train, y_true, eval_metric = 'rmse',verbose=False)\nXGBpredictions_Test = XGBModel1.predict(X_test)","66f17edd":"predictions_Test = XGBpredictions_Test","d95f6b24":"output = pd.DataFrame({'Id': test_home_data.Id,\n                       'SalePrice': predictions_Test})\noutput.to_csv('submission.csv', index=False)","f0cdac1c":"Additionally, a review of the variable coding in the dataset documentation reveals that the variable 'MSSubClass' should be treated as categorical, even though after reading in the training data, it is automatically treated as an integer.","d35e5f39":"#### Missing Data","6ef4b81b":"## Models\n\nHere we will compare the results of a variety of common regression models.\n\n### Multivariate Regression\n\nFirst, we can consider a basic multivariate regression model.  This is a simple model that minimizes the residual sum of squares using linear algebra.","c4990517":"Thus, we can conclude there is no basement for the properties corresponding to these missing values (BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath), and set them all to zero.","39c3b8ce":"Looking at the variable coding, we can see that there are no columns with an extremely high number of categories.  There may be some computing concerns about having a couple hundred of coding varaibles, but this notebook should be able to easily handle that.  Additionally, there could be some concerns about using this full model if we wanted to either make some statement about the association between certain variables and housing prices, or predict housing prices from new data (as mentioned earlier). Here our only goal is the make a model that predicts the test dataset well.  So, there should be no issue with using the full dataset.  \n\n### Encoding the Categorical Variables\n\nBelow we will apply one-hot encoding and ordinal encoding to the categorical variables.  Ordinal encoding would preserve the relationship between the categories (e.g., coding 'poor,' 'fair,' 'good,' to '1,' '2,' '3.'), while one-hot encoding just creates a dummy variable for each category (e.g., 'poor,' 'fair,' 'good,' would each get their own variable that had a value of '1', wherever the original category reported 'poor.').","c768566a":"## Exporting Predictions\nWe will use our ensemble model to determine the predictions.  Tedious testing has allowed me to put together the below model.","6c023038":"## Exploratory Data Analysis\n\nBelow we can see all the explanatory variables in the set.  Note that some of these variables probably could not be collected until a sale had completed or was imminment; such as, 'MoSold,' 'YrSold,' 'SaleType,' 'SaleCondition'.  Additionally, the vague variables like 'MiscFeature,' and 'MiscVal' may be difficult to accommodate if one were to try to predict their values for a new house.  \n","0c393a99":"## Building the Dataset\nBelow we cover building the datasets we will use in the final models.   \n\n### Creating the 'Full' Datasets\nFirst we read in, and create the full datasets 'X,' 'y_train,' and 'X_test_full.'  Since, the y_train variable is skewed and has a number of outliers, taking the logarithm may improve our results.  Additionally, in the description of this compeition, it states that the evaluation will be based on the root-mean-squared-error ('RMSE') of the logarithm of the predictions and the true values.","2743da16":"Here, we can see that a XGBoost model is a great improvement on even the random forest model.  The cross-validated RMSE here is about 0.117 compared to the random forest score of 0.135.  The residual plot also shows that there are still a few outliers.  \n\nAlthough it is interesting to note that the XGBoost model's fit to the full training data is slightly worse than the random forest model's, indicating that the random forest may be overfitting the data. Below we can see an even more extreme example of overfitting.  ","1d3492e6":"Next, to control for over-fitting, we will use k-fold stratified cross-validation.  To do this we will use the 'cross_val_score' function from sklearn.  To compare using RMSE we will need to make our own function.","21c74480":"Again, with 'LotFrontage', there does not appear to be any clear reason for the missing values.  But, there are also no zero values, and given the variables description ('Linear feet of street connected to property') zero does not appear to be an appropriate value.  Instead, we will apply the mean value (the mean and median values are fairly close together regardless).  ","b1bdc57d":"We can see that a basic multivariate regression model gives a RMSE of about 0.357, and has fairly decent R-squared values.  So, this model is a decent fit.  We can also see that the overall fit improves when we use the log-transformed target variable, since this helps control fitting to the extreme outliers in SalePrice. \n\nThe plot above also shows us that the residuals have a few extreme outliers.  While it is possible that further modification of the linear model could improve the fit (e.g., addressing skewness in the variables, or adding additional variables), instead we will apply some machine learning models to this regression, and see if these more complex models yield any improvements over this basic model. ","b814ab42":"Thus, are final datasets are: X_train, y_train, y_true, and X_test.\n\nFinally, we have to consider the missing values in the numeric columns.","1fbb21f1":"Looking at this residual plot, we can see some similar patterns to the previous plot for the OLS model, suggesting that there are a few outliers that this model is not accommodating.  ","e8987b17":"We also need to consider missing values in the test data.","c90c790b":"## Loading the Data\n\nHere we are just reading in the data.  ","5e480475":"Here we see that while where MasVnrArea is missing MasVnrType is missing as well, there is a separate category for there not being a masonry veneer.  So, it is unclear whether or not zeroes would be appropriate to fill the missing values. ","7765b896":"Additionally, it may be beneficial to create a few additional variables using combinations of existing variables.  These variables can help account for non-linear relationships between the target variable and the explanatory variable, or allow us to add additional data, which should or could have been in the original dataset, but were missing for whatever reason (such as the total square-footage of the house.).","d73dab6f":"Alternatively, you could look at the R-squared value.  To get a feel for the fit of the model.","d4fca57d":"### Random Forest\nA random forest is an ensemble that fits multiple decision trees on various sub-samples of the dataset. So, we would expect it to perform better than the decision tree model.","2e67361b":"But, since it appears that the median of MasVnrArea is zero, it should be an appropriate value as well.  ","b20c7152":"Here we can see that while there are a number of variables that have a few missing values, some are nearly entirely missing.  For the variables with the highest missing values ('Alley,' 'FireplaceQU,' 'PoolQC,' 'Fence,' and 'MiscFeature') the missing values indicate that that feature does not exist (e.g., that there is no fence).  So these missing values should be easy to accommodate with one-hot encoding. A review of the description of these variables shows that this is explicitly the case for all the variables with missing values except for 'Electrical.'  But, it is likely safe to assume that this is the case for Electrical as well, or at least that 1 missing value will not impact the overall model much.  \n\n### Review of Sale Price\n","212e15e8":"### Decision Tree\n\nA decision tree is a non-parametric supervised learning method that uses a series of branching if-then-else decisions to predict the value of a target variable.  For example, if you observe higher-value houses all having a pool, then when you have a new observation that has a pool you would predict its sale value as being higher than it would be otherwise.  ","bd051a8c":"# Purpose\n\nThis Notebook analyzes the Ames Housing dataset (\"Ames Dataset\") with a variety of basic regression models.  \n\nThe Ames Dataset is comprised of data regarding houses that were sold in Ames, Iowa, from 2006 to 2010.  It consists of 79 explanatory variables for the target variable of Sale Price.  \n\n## Setting Up the Necessary Packages\n\nHere we just read in the various Python packages we will use in the notebook.  ","e8399002":"Here, we can see that it is easy to make a model that can fit the training data extremely well, but that this same model does not perform well on cross-validation, because it is overfitting the training data.   ","d3278065":"This suggests that either the missing value is for a detached garage, that wasn't fully recorded.  Or, that there was no garage, but it was mislabeled as a detached one.  Either way the data was mislabeled, as the varible description file explicitly states that NAs correspond to no garage.  Given that four variables indicate that there is no garage, while one indicates that there is a garage, I am going to fill these variables with zeroes as well.  ","05bcdd55":"MSSubClass has different integer encoding for the type of dwelling involved, but the integer values are not ordinally related.  For example, a two family conversion is encoded as '190' while a two-story home that is dated 1946 or newer is encoding as '60'.  But, it would be incorrect for the dataset to interpret this as two family conversions somehow being better than two-story homes.  Thus, this variable should be re-encoded with dummy variables instead.  ","f47e38c6":"Based on this we can conclude that replacing the missing values with zeroes is appropriate since every instance of a missing value in Garage Year Built corresponds to there not being a garage.","0e0b14cf":"Here, we can see that a Random Forest model is a great improvement on the multivariate model.  The cross-validated RMSE here is about 0.135 compared to the multivariate score of 0.357.  It is also an improvment over the single decision tree's score of 0.177. However, this model is much more complicated and computationally expensive.  ","260419f5":"As of the time of this writing, this model placed in the top 2% with a mean absolute error ('MAE') (despite the competition description using RMSE the actual evaluation is based on MAE) of about 13,236 on the test data.  So, there is still room for improvement.  I would suggest: explore ensemble methods and other linear regression machine learning procedures, create further interaction variables (e.g., the ratio of rooms to bathrooms, or remodel year to sale year, etc.), then further hyperparameter optimization (as the models I used were not fully optimized, and adding new variables will necessitate further optimization). To that end I would suggest using Lasso, or ElasticNet, and a stacked regression or voting ensemble, the Pandas corr method, and the Sklearn RandomizedSearchCV or GridSearchCV from model_selection.","aba59c98":"Finally, there may be some issues of skewness or non-normality of the explanatory variables.  So, it may be benefical to standardize the data.  Alternatively, we could do a deeper review of skewness and transform those variables that are highly skewed.  ","8d19d745":"For these three columns, there are a few ways we could potnetially fill, or resolve missing values.  First, we could fill them with a specific value. For example, zero, or the mean, or median for that variable.  Alternatively, we could fill it with some sort of predicted estimation based on the values in the other columns.  To determine the appropriate fill, we must analyze the data.  Observe:","c39cadf3":"In the plot above we can see that the data is right skewed, and has a number of outliers.  In fact we have 22 outliers in the training dataset, as seen below.  Note that I am using the common rule of thumb of three standard deviations above the mean as the threshold for outliers.  ","a80ab1a7":"### XGBoost\nXGBoost, which stands for extreme gradient boosting, is an implementation of gradient boosting. Gradient boosting is an ensemble method that iteratively adds models in cycles. That is to say, gradient boosting forms a group of models to use as regressors or classifiers by starting with an initial model and using that to fit new models each cycle. The cycle uses the current models in the ensemble to generate predictions for each observation in the dataset. These predictions, via a loss function, are used to fit a new model to add to the ensemble.  I performed the hyper-parameter optimization by hand.","7b313e76":"Now, we will look at the categorical columns, and separate them based on whether or not they are ordinal.  A reveiw of the categories of each variable reveals that all the ordinal categorical variables contain one of 'Gd,' 'GdWo,' 'RFn,' 'Mod,' 'ALQ,' 'IR1,' 'Bnk,' or 'ELO.' We separate out these variables below."}}