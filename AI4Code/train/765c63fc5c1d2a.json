{"cell_type":{"f27e85e2":"code","7842da58":"code","5ed4aaf2":"code","dbc8d969":"code","c77b20de":"code","3c164919":"code","4d6904b3":"code","d0e0884b":"code","50382f97":"code","a7c4a881":"code","9507cd31":"code","e56d0c9c":"code","645da64e":"code","48aea5a1":"code","8f890317":"code","527cce8c":"code","23d8c64d":"code","ec130eaf":"code","a620db5f":"code","04d01895":"code","26e8917e":"code","d61976bc":"code","ed552c7a":"code","beb851fb":"code","ceecdf20":"code","3ea72cac":"code","a3c81b40":"code","b67c7af2":"code","36efc995":"code","76e7fe2d":"code","6ef8898c":"code","21e1877a":"code","42bd10ce":"code","e79ec4d6":"code","c22dc321":"code","51907580":"code","31f9ae09":"code","53edd92a":"code","5f0c6bf7":"code","83c075a1":"code","99c68f6d":"code","2c680ace":"code","6548aa7d":"code","c22a0364":"code","f0d8fc59":"code","aeae45d1":"code","1169f3cd":"markdown","2813272b":"markdown","d5f9c895":"markdown","aa6d383d":"markdown"},"source":{"f27e85e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7842da58":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\n#import numpy as np\n#import pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom datetime import datetime\n\nfrom scipy.stats import skew, norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom xgboost import XGBRegressor\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')","5ed4aaf2":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/30-days-of-ml\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test","dbc8d969":"data_dir = Path(\"..\/input\/30-days-of-ml\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"id\")\n\ndf.head()","c77b20de":"df.columns","3c164919":"def clean(df):\n    return df\ndef encode(df):\n    return df\ndef impute(df):\n    return df","4d6904b3":"df_train, df_test = load_data()","d0e0884b":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","50382f97":"X = df_train.copy()\ny = X.pop(\"target\")\n\n# Label encoding for categoricals\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int","a7c4a881":"# Peek at the values\ndisplay(X)\n\n# Display information about dtypes and missing values\ndisplay(X.info())\n","9507cd31":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores","e56d0c9c":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","645da64e":"X1 = df_train.copy()\n# Label encoding for categoricals\nfor colname in X1.select_dtypes(\"object\"):\n    X1[colname], _ = X1[colname].factorize()\nsns.relplot(x=\"cont12\", y=\"target\", data=X1);\nsns.relplot(x=\"cont9\", y=\"target\", data=X1);\nsns.relplot(x=\"cont10\", y=\"target\", data=X1);","48aea5a1":"sns.jointplot(x=\"target\", y=\"cont10\", kind=\"reg\", data=X1)","8f890317":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(df['target'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(df['target'], kde = True, hist=True, fit = norm)\nplt.title('Target distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"Target in \", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","527cce8c":"from scipy import stats\n\nshap_t,shap_p = stats.shapiro(df['target'])\n\nprint(\"Skewness: %f\" % abs(df['target']).skew())\nprint(\"Kurtosis: %f\" % abs(df['target']).kurt())\nprint(\"Shapiro_Test: %f\" % shap_t)\nprint(\"Shapiro_Test: %f\" % shap_p)","23d8c64d":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = df.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.set(font_scale=1.6)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .9})\nplt.show()","ec130eaf":"def encode(df):\n    cat_cols = [col for col in df.columns if 'cat' in col]\n    enc = OrdinalEncoder()\n    df[cat_cols] = enc.fit_transform(df[cat_cols])\n    df.head()\n    return df","a620db5f":"df_train, df_test = load_data()","04d01895":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","26e8917e":"X = df_train.copy()\ny = X.pop(\"target\")\nX.pop(\"cat2\")\nX.pop(\"cat6\")\nX.pop(\"cat4\")\nX.pop(\"cat3\")\n\n#baseline_score = score_dataset(X, y)\n#print(f\"Baseline score: {baseline_score:.5f} RMSLE\")","d61976bc":"print(X.shape)","ed552c7a":"print('START ML', datetime.now(), )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","beb851fb":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\nSEED = 7770777\n\nlgb_params = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.007899156646724397,\n    \"num_leaves\": 77,\n    \"max_depth\": 77,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 9.562925363678952,\n    \"reg_lambda\": 9.355810045480153,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    \"bagging_seed\": SEED,\n    \"feature_fraction_seed\": SEED,\n    \"seed\": SEED\n}\n\nlightgbm = LGBMRegressor(**lgb_params)\n\nlightgbm1 = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# changed objective from reg:linear to reg:squarederror\n#xgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n#                                     max_depth=3, min_child_weight=0,\n#                                     gamma=0, subsample=0.7,\n#                                     colsample_bytree=0.7,\n#                                     objective='reg:squarederror', nthread=-1,\n#                                     scale_pos_weight=1, seed=27,\n#                                     reg_alpha=0.00006, random_state=42)\n\nxgb_params = {\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 80,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\n\n\nBest_trial= {'objective': 'reg:tweedie', \n             'tree_method': 'hist', \n             'reg_lambda': 6.323815225685991, \n             'reg_alpha': 2.498166499808977, \n             'colsample_bytree': 0.2, \n             'subsample': 0.8, \n             'learning_rate': 0.02, \n             'n_estimators': 6000, \n             'max_depth': 7, \n             'random_state': 40, \n             'min_child_weight': 99, \n             'use_label_encoder': False, \n             'booster': 'gbtree'}\n\nxgboost = XGBRegressor(**xgb_params)\n\n#xgboost1 = XGBRegressor(**Best_trial)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","ceecdf20":"print(datetime.now(), 'xgbBoosting')\nxgboost_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'xgbBoosting end')","3ea72cac":"def xgboost_models_predict(X):\n    return ((1 * xgboost_model_full_data.predict(X)))\n\ny_gbr_pred = xgboost_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred))","a3c81b40":"print(datetime.now(), 'lightgbmBoosting')\nlightgbm_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'lightgbmBoosting end')","b67c7af2":"def lightgbm_models_predict(X):\n    return ((1 * lightgbm_model_full_data.predict(X)))\n\ny_gbr_pred1 = lightgbm_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred1))","36efc995":"print(datetime.now(), 'lightgbm1Boosting')\nlightgbm1_model_full_data = lightgbm1.fit(X, y)\nprint(datetime.now(), 'lightgbm1Boosting end')","76e7fe2d":"def lightgbm1_models_predict(X):\n    return ((1 * lightgbm1_model_full_data.predict(X)))\n\ny_gbr_pred11 = lightgbm1_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred11))","6ef8898c":"print(datetime.now(), 'gbrBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'gbrgbmBoosting end')","21e1877a":"def gbr_models_predict(X):\n    return ((1 * gbr_model_full_data.predict(X)))\n\ny_gbr_pred2 = gbr_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred2))","42bd10ce":"def blend_models_predict(X):\n    return ((0.15 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * lightgbm1_model_full_data.predict(X)) +\\\n            (0.7 * xgboost_model_full_data.predict(X))\n           )\n\ny_gbr_pred3 = blend_models_predict(X)\nprint('RMSLE score on train data:')\nprint(rmsle(y, y_gbr_pred3))","e79ec4d6":"X_test = df_test.copy()\ny_test = X_test.pop(\"target\")\nX_test.pop(\"cat2\")\nX_test.pop(\"cat6\")\nX_test.pop(\"cat4\")\nX_test.pop(\"cat3\")\n\nprint(X_test.shape, X.shape)","c22dc321":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission.iloc[:,1] = xgboost_models_predict(X_test)\nsubmission.head()","51907580":"print('Predict submission', datetime.now(),)\nsubmission1 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission1.iloc[:,1] = lightgbm1_models_predict(X_test)\nsubmission1.head()","31f9ae09":"print('Predict submission', datetime.now(),)\nsubmission2 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission2.iloc[:,1] = gbr_models_predict(X_test)\nsubmission2.head()","53edd92a":"print('Predict submission', datetime.now(),)\nsubmission3 = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission3.iloc[:,1] = blend_models_predict(X_test)\nsubmission3.head()","5f0c6bf7":"submission.to_csv(\"30days_ML_Submission_v28_orig parameters.csv\", index=False)\nprint('Save submission', datetime.now(),)","83c075a1":"submission1.to_csv(\"30days_ML_Submission_v28_lightgbm.csv\", index=False)\nprint('Save submission', datetime.now(),)","99c68f6d":"submission2.to_csv(\"30days_ML_Submission_v28_gbr.csv\", index=False)\nprint('Save submission', datetime.now(),)","2c680ace":"submission3.to_csv(\"30days_ML_Submission_v28_blended.csv\", index=False)\nprint('Save submission', datetime.now(),)","6548aa7d":"#Setting the kfold parameters\nkf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\nmean_rmse = 0\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    X_train, X_valid = X.iloc[train_id], X.iloc[valid_id]\n    y_train, y_valid = y.iloc[train_id], y.iloc[valid_id]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n    \n    #Mean of the predictions\n    preds += model.predict(X_test) \/ 10 # Splits\n    \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 10 #splits\n    \n    #Out of Fold predictions\n    oof_preds[valid_id] = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_id]))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    \n    mean_rmse += fold_rmse \/ 10\n    \nprint(f\"\\nOverall RMSE: {mean_rmse}\")","c22a0364":"print('kFold xgboost Predict submission', datetime.now(),)\nsubmission_kFold = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsubmission_kFold.iloc[:,1] = preds\nsubmission_kFold.head()","f0d8fc59":"submission_kFold.to_csv(\"30days_ML_Submission_v28_kFold.csv\", index=False)\nprint('Save submission', datetime.now(),)","aeae45d1":"final_blend_total = submission['target'] * 0.55 + submission_kFold['target'] * 0.15 + submission1['target'] * 0.15 \n+ submission2['target'] * 0.15\nsubmission_blended_kFold = submission_kFold.copy()\nsubmission_blended_kFold['target'] = final_blend_total\nsubmission_blended_kFold.to_csv(\"30days_ML_Submission_v28_blended_kFold.csv\", index=False)\nprint('Save submission', datetime.now(),)","1169f3cd":"# Correlation Matrix\n\nThe correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.\n","2813272b":"# Introduction\n\nThis notebook uses a blended model consisting XGBost, Gradient Boost Regressor and Light GBM. In addition, I have also used kFolds for XGBoost and used the prediction in the final blend.","d5f9c895":"## KFolds","aa6d383d":"# EDA - Exploratory Data Analysis #"}}