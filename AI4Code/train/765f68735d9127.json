{"cell_type":{"f6d0efb1":"code","a8911c27":"code","b5c42959":"code","91546cd0":"code","ea8074ab":"code","c8a2ed4e":"code","18525e73":"code","a050655d":"code","66478464":"code","ae12871b":"code","ff73dcc7":"code","a967a198":"code","39ac431d":"code","5339a7ff":"code","ddf15031":"code","4cc16389":"code","9553fe0e":"code","546dc04d":"code","910e7600":"code","ed7aa907":"code","82537c76":"code","7fda3f17":"code","7889a132":"code","6d1c2d9a":"code","6cd70b61":"code","cad54654":"code","6ca1b746":"code","5266faff":"code","d2233fb9":"code","c5f50b4c":"code","13832bd4":"code","fb9fa854":"code","7b6bfb02":"code","023ba498":"code","0d2ab972":"code","3c3a93ea":"code","8561ba41":"code","ce80e6d9":"code","9a32b290":"code","627d0aac":"code","49947c66":"code","7fa4e9b5":"markdown","911bb6de":"markdown","be6facb4":"markdown","e4b34b94":"markdown","ae386ac0":"markdown","50497701":"markdown","fe5305e5":"markdown","450a7bb6":"markdown","ede4091a":"markdown","71e7b231":"markdown","199737fb":"markdown","f998313b":"markdown","80ff8fdc":"markdown","a615e4aa":"markdown"},"source":{"f6d0efb1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a8911c27":"# import the required packages\nimport pandas as pd # used of data wrangling also known as MS Excel for Python\nimport numpy as np # used for large datasets in array\nimport seaborn as sns # package to visualize data\nimport matplotlib.pyplot as plt # another package to visualize data\nfrom sklearn.preprocessing import StandardScaler, normalize # to use preprocessing of data\nfrom sklearn.cluster import KMeans # The main package: The ML itself\nfrom sklearn.decomposition import PCA # package for dimentionality reductions","b5c42959":"# load the data in pandas DataFrame and check the 1st head (1st 5 rows)\ncreditcard_df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ncreditcard_df.head()","91546cd0":"# CUSTID: Identification of Credit Card holder \n# BALANCE: Balance amount left in customer's account to make purchases\n# BALANCE_FREQUENCY: How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n# PURCHASES: Amount of purchases made from account\n# ONEOFFPURCHASES: Maximum purchase amount done in one-go\n# INSTALLMENTS_PURCHASES: Amount of purchase done in installment\n# CASH_ADVANCE: Cash in advance given by the user\n# PURCHASES_FREQUENCY: How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n# ONEOFF_PURCHASES_FREQUENCY: How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n# PURCHASES_INSTALLMENTS_FREQUENCY: How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n# CASH_ADVANCE_FREQUENCY: How frequently the cash in advance being paid\n# CASH_ADVANCE_TRX: Number of Transactions made with \"Cash in Advance\"\n# PURCHASES_TRX: Number of purchase transactions made\n# CREDIT_LIMIT: Limit of Credit Card for user\n# PAYMENTS: Amount of Payment done by user\n# MINIMUM_PAYMENTS: Minimum amount of payments made by user  \n# PRC_FULL_PAYMENT: Percent of full payment paid by user\n# TENURE: Tenure of credit card service for user","ea8074ab":"# check the datatypes and null values in columns using the \"info()\"\ncreditcard_df.info()","c8a2ed4e":"# get more statistical insights using \"describe()\"\ncreditcard_df.describe()","18525e73":"# lets revist the missing data\nimport seaborn as sns\nsns.heatmap(creditcard_df.isnull(), yticklabels=False, cbar=False, cmap='Blues')","a050655d":"# This gives more clear sense of missing data in a tablular form\ncreditcard_df.isnull().sum()","66478464":"# lets fill up the missing elements with mean of the \"MINIMUM_PAYMENTS\"\n# But first lets try out the parts of the code before fully write it in single line\n# The below gives the True\/False against each row based on \"MINIMUM_PAYMENTS\" is null(TRUE) or not(FALSE)\ncreditcard_df['MINIMUM_PAYMENTS'].isnull() == True","ae12871b":"# In order to replace the null values, we have to identity the locations of these row\n# the below gives only the rows where \"MINIMUM_PAYMENTS\" are missing values or where the above code gave FALSE\ncreditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS']","ff73dcc7":"# Finally, lets replace the above row values with mean()\ncreditcard_df.loc[(creditcard_df['MINIMUM_PAYMENTS'].isnull() == True), 'MINIMUM_PAYMENTS'] = creditcard_df['MINIMUM_PAYMENTS'].mean()","a967a198":"#let's check again for any missing value\ncreditcard_df.isnull().sum()","39ac431d":"# Lets do the same again for the \"CREDIT_LIMIT\" column\ncreditcard_df.loc[(creditcard_df['CREDIT_LIMIT'].isnull() == True), 'CREDIT_LIMIT'] = creditcard_df['CREDIT_LIMIT'].mean()","5339a7ff":"# Probably the last check for missing values\ncreditcard_df.isnull().sum()","ddf15031":"# lets check if we have any duplicate rows\/entries in the dataset\ncreditcard_df.duplicated().sum()","4cc16389":"# lets drop the column \"CUST_ID\" as it is just an ID and not a feature. In 99% cases, we drop the ID unless they can be used to derive otehr features\ncreditcard_df.drop('CUST_ID', axis=1, inplace=True)\ncreditcard_df.head()","9553fe0e":"# lets check out the column names and count\ncreditcard_df.columns","546dc04d":"n=len(creditcard_df.columns)\nn","910e7600":"creditcard_df.info()","ed7aa907":"# distplot with KDE\nplt.figure(figsize=(10,50))\nfor i in range(len(creditcard_df.columns)):\n    plt.subplot(17,1,i+1)\n    sns.distplot(creditcard_df[creditcard_df.columns[i]],kde_kws={'color':'b', 'lw':3, 'label':'KDE', 'bw':0.1}, hist_kws={'color':'g'})\n    #sns.distplot(creditcard_df[creditcard_df.columns[i]],kde_kws={'color':'b', 'lw':3, 'label':'KDE'})\n    plt.title(creditcard_df.columns[i])\n    \nplt.tight_layout()\n# Few observations\n# Mean of balance is somewhere between $1000 and $2000\n# 'Balance_Frequency' for most customers is updated frequently at 1\n# For 'PURCHASES_FREQUENCY', there are two distinct group of customers at 0 and 1\n# For 'ONEOFF_PURCHASES_FREQUENCY' and 'PURCHASES_INSTALLMENT_FREQUENCY' most users don't do one off puchases or installment purchases frequently \n# Very small number of customers pay their balance in full 'PRC_FULL_PAYMENT'~0\n# Average credit limit is around $5000\n# Most customers have tenure between 11 and 12","82537c76":"# Correlation is used to see the relation between features.\n# Positive correlation mean the features are direcltly proportional and negetive means inversely proportional\ncorrelations = creditcard_df.corr()\nf, ax = plt.subplots(figsize=(20,10))\nsns.heatmap(correlations,annot=True)","7fda3f17":"#TODO: Add details","7889a132":"scaler = StandardScaler()\ncreditcard_df_scaled = scaler.fit_transform(creditcard_df)\ncreditcard_df_scaled.shape","6d1c2d9a":"creditcard_df_scaled","6cd70b61":"scores_1 = []\nrange_values = range(1,20)\n\nfor i in range_values:\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(creditcard_df_scaled)\n    scores_1.append(kmeans.inertia_)\n    \nplt.plot(scores_1,'bx-')","cad54654":"kmeans = KMeans(7)\nkmeans.fit(creditcard_df_scaled)\nlabels = kmeans.labels_","6ca1b746":"kmeans.cluster_centers_.shape","5266faff":"cluster_centers = pd.DataFrame(data=kmeans.cluster_centers_, columns=[creditcard_df.columns])\ncluster_centers","d2233fb9":"# To understand the data better, perform the inverse transformation\ncluster_centers = np.round(scaler.inverse_transform(cluster_centers),4)\ncluster_centers = pd.DataFrame(data=cluster_centers, columns=[creditcard_df.columns])\ncluster_centers","c5f50b4c":"labels","13832bd4":"labels.shape","fb9fa854":"labels.max()","7b6bfb02":"labels.min()","023ba498":"y_kmeans = kmeans.fit_predict(creditcard_df_scaled)\ny_kmeans # this should be same as labels???","0d2ab972":"creditcard_df_cluster = pd.concat([creditcard_df, pd.DataFrame({'cluster':labels})], axis=1)\ncreditcard_df_cluster.head()","3c3a93ea":"# plot the histogram of various clusters\nfor i in creditcard_df.columns:\n    plt.figure(figsize=(35,5))\n    for j in range(7):\n        plt.subplot(1,7,j+1)\n        cluster = creditcard_df_cluster[creditcard_df_cluster['cluster'] == j]\n        cluster[i].hist(bins = 20)\n        plt.title('{}   \\nCluster {}'.format(i,j))\n        \nplt.show()","8561ba41":"# TODO: Add details","ce80e6d9":"# Obtain the principal component\npca = PCA(n_components=2)\nprincipal_comp = pca.fit_transform(creditcard_df_scaled)\nprincipal_comp","9a32b290":"pca_df = pd.DataFrame(data=principal_comp, columns=['pca1','pca2'])\npca_df.head()","627d0aac":"# concatenate the clusters labels to the dataframe\npca_df = pd.concat([pca_df,pd.DataFrame({'cluster':labels})], axis=1)\npca_df.head()","49947c66":"plt.figure(figsize=(10,10))\nax = sns.scatterplot(x='pca1', y='pca2', hue='cluster', data=pca_df, palette=['red','green','blue','pink','yellow','gray','purple'])","7fa4e9b5":"Great, no missing values in \"MINIMUM_PAYMENTS\" now but we still have a missing data for \"CREDIT_LIMIT\" column","911bb6de":"The above helps for the \"MINIMUM_PAYMENTS\" column, but what about the other missing data columns, they not very much visible. Lets try a different method","be6facb4":"## Problem Statement:\n### We have data of about 9000 credit card holders for last 6 months. Our job is to group these customers based on their credit card usage.","e4b34b94":"## Theory behind K-Means\nThe objective of K-means is simple, identify patterns in data points and group (k) similar data points together. The \"k\" is the number of clusters to be define. In other words, K-means algorithm identify k number of centroids and then allocates all the data point to each of these centroid to their nearest cluster maintaining the distance as small as possible. The steps are as follows:\n1. Choose number of clusters \"K\".\n2. Select random \"K\" points in the data hyperspace.\n3. Assign each data point to its nearest centroid, hence creating \"K\" number of clusters.\n4. Sum the distance between each data point and its nearest\/assign centroid.\n5. Re-arrange the centroid so that the sum is moving towards minimum.\n6. Go to step 4 and repeat until condition 7 or 8 is met.\n7. There is no change in the sum of distance.\n8. Pre-defined number of iterations are reached","ae386ac0":"### Note that we have null values in columns CREDIT_LIMIT and MINIMUM_PAYMENTS, we will address this later","50497701":"# Visualize and Explore Dataset","fe5305e5":"## Visualize the data.\nNow its to plot graphs and see what we can derive just by looking at different features. We will do this by using Distribution Plot (distplot) from matplotlib.hist and KDE Plot (kdeplot) from seaborn library.\n* KDE Plot represents the Kernel Density Estimate\n* KDE is used for visualizing the Probability Density of a continuous variable. \n* KDE demonstrates the probability density at different values in a continuous variable. ","450a7bb6":"# Implement Elbow Method","ede4091a":"# Principal Component Analysis (PCA)","71e7b231":"This returns 0 rows meaning our dataset do not have any duplicate entries","199737fb":"Great, no missing values anywhere","f998313b":"## WOAH, hold on a minute. Where are the labels??\n### Suprise, we don't have labels because this is Unsupervised Learning. Unsupervised Learning is a kind of machine learning where the goal is to looks for patterns and segregate them based on their features. It's like how a baby differentiate between a cat and dog without actually knowing which one is called cat or which one is dog. The algorithm look for \"features\" to distinguish the groups.","80ff8fdc":"# Apply K-Means method","a615e4aa":" # Elbow Methods\n![](http:\/\/)"}}