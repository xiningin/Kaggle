{"cell_type":{"b5fff11b":"code","b0f8d881":"code","84157556":"code","41dcaad5":"code","eb770ecf":"code","7adeb7cc":"code","9b66e84a":"code","1c2b046f":"code","b5f303c9":"code","7b5e010a":"code","39fec4c3":"code","098ee39e":"code","7eb1c9e1":"code","67d17a37":"code","c64b3b78":"code","ee18cf94":"code","1ecd28bf":"code","3ed7a0a7":"code","52935065":"code","e50c6e08":"code","39980dd1":"code","dafbd14c":"code","d1081a89":"code","3a2dde6f":"code","998e4a99":"code","80aa777a":"code","1935f73b":"code","46540595":"code","f29a60de":"code","aaedd7e8":"code","4d0a889a":"markdown","0d45e735":"markdown","ce329e05":"markdown","a7d3a070":"markdown","8ec8e4b7":"markdown","dead4ade":"markdown","5038cbe5":"markdown"},"source":{"b5fff11b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b0f8d881":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nsns.set()","84157556":"train = pd.read_csv('..\/input\/train.csv')\ntrain.head()","41dcaad5":"train.describe()","eb770ecf":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","7adeb7cc":"test.describe()","9b66e84a":"sns.heatmap(train.isnull(), cbar=False, cmap='plasma', yticklabels=False)","1c2b046f":"sns.countplot(x='Survived', data=train, hue='Sex')\n# It seems more woman survied than man","b5f303c9":"sns.countplot(x='Survived', data=train, hue='Pclass')\n# there is correlation between Passnger class and survival \n# majority of people died are from Pclass 3","7b5e010a":"sns.boxplot(x='Pclass', y='Age', data=train)\n# older people could effort better class\n# Therefore we will replace missing age values with average age of each passnger class ","39fec4c3":"mean_ages = train.groupby('Pclass')['Age'].mean()\nmean_ages","098ee39e":"def impute_age(cols):\n    age = cols['Age']\n    pclass = cols['Pclass']\n\n    if pd.isnull(age):\n        if pclass == 1:\n            return 38\n        elif pclass == 2:\n            return 29\n        else:\n            return 25\n    else:\n        return age\n\ndef impute_fare(cols):\n    fare = cols['Fare']\n    pclass = cols['Pclass']\n\n    if pd.isnull(fare):\n        if pclass == 1:\n            return 84\n        elif pclass == 2:\n            return 20\n        else:\n            return 13\n    else:\n        return fare","7eb1c9e1":"train['Age'] = train[['Age', 'Pclass']].apply(impute_age, axis=1)\ntest['Age'] = test[['Age', 'Pclass']].apply(impute_age, axis=1)\ntest['Fare'] = test[['Fare', 'Pclass']].apply(impute_fare, axis=1)","67d17a37":"gender = pd.get_dummies(train['Sex'], drop_first=True)\ngender_test = pd.get_dummies(test['Sex'], drop_first=True)","c64b3b78":"embark = pd.get_dummies(train['Embarked'], drop_first=True)\nembark_test = pd.get_dummies(test['Embarked'], drop_first=True)","ee18cf94":"pass_class = pd.get_dummies(train['Pclass'], drop_first=True)\npass_class_test = pd.get_dummies(test['Pclass'], drop_first=True)\ntrain.head()","1ecd28bf":"train.drop(['Cabin','Sex', 'Embarked','Name','Ticket', 'PassengerId', 'Pclass'], axis=1, inplace=True)\npass_id = test['PassengerId']\ntest.drop(['Cabin', 'Sex', 'Embarked','Name','Ticket', 'PassengerId', 'Pclass'], axis=1, inplace=True)\ntrain.head()","3ed7a0a7":"train = pd.concat([train, gender, embark, pass_class], axis=1)\ntest = pd.concat([test, gender_test, embark_test, pass_class_test], axis=1)\ntrain.head(10)","52935065":"sns.heatmap(train.isnull(), cmap='plasma')","e50c6e08":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","39980dd1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nk_fold = KFold(n_splits=10, shuffle=True, random_state=42)","dafbd14c":"# train = train.dropna(inplace=True)\n# test = test.dropna(inplace=True)\nX = train.drop(['Survived', 'Fare'], axis=1)\ny = train['Survived']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nX.head()","d1081a89":"# sns.pairplot(train)\nprint()","3a2dde6f":"clf = KNeighborsClassifier(n_neighbors=5)\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (score.mean(), score.std() * 2))","998e4a99":"clf = LogisticRegressionCV(cv=5, max_iter=125)\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, scoring='accuracy')\nprint(score)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (score.mean(), score.std() * 2))","80aa777a":"clf = RandomForestClassifier(n_estimators=10)\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (score.mean(), score.std() * 2))","1935f73b":"clf = GaussianNB()\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (score.mean(), score.std() * 2))","46540595":"clf = SVC(gamma='auto')\nscore = cross_val_score(clf, X_train, y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(score)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (score.mean(), score.std() * 2))","f29a60de":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\nlogmodel.fit(X_train, y_train)\n\npredictions = logmodel.predict(test.drop('Fare', axis=1))\nsubmission = pd.DataFrame({'PassangerId': pass_id, 'Survived':predictions})\nsubmission.to_csv('submission_logreg.csv', index=False)\nsubmission.head()","aaedd7e8":"rfmodel = RandomForestClassifier(n_estimators=100)\nrfmodel.fit(X_train, y_train)\n\npredictions = rfmodel.predict(test.drop('Fare', axis=1))\nsubmission = pd.DataFrame({'PassangerId': pass_id, 'Survived':predictions})\nsubmission.to_csv('submission_rf.csv', index=False)\nsubmission.head()","4d0a889a":"## SVM ","0d45e735":"## Naive Bayes","ce329e05":"## DECISION TREE","a7d3a070":"## Highest accuracy achieved by Random Forest and Logistic Regression\n## TESTING","8ec8e4b7":"## RANDOM FOREST","dead4ade":"# Let's see if we have missing data","5038cbe5":"We can replace missing age with averaga age. for that we need to learn more about our data  \n\nAs for cabin we will need to drop it since we can't do much about it"}}