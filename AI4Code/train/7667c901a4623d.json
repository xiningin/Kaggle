{"cell_type":{"62d956a2":"code","25be1496":"code","05accd18":"code","1d8a09d1":"code","fdc6d7ec":"code","e1fcf8f8":"code","7f6aea05":"code","7917f42c":"code","deb5e4c3":"code","06dbc36b":"code","71e963c3":"code","20504728":"code","e8c312b0":"code","b5099ed1":"code","9a719daf":"code","a1c673e4":"code","36897f58":"code","2c5ca63b":"code","7e03f894":"code","614c5c23":"code","c56662f1":"code","d0e82e6d":"code","fa076d6a":"code","d8792c75":"code","b3a2bd64":"code","3539fbdf":"code","5e76fd68":"code","cba0403a":"code","ea1bc7b1":"code","a474f832":"code","b204be05":"code","29e8fa6d":"code","05492b6b":"code","f082a581":"code","e9f9c007":"code","812195b2":"code","8022f177":"code","3d24e0d9":"code","ebe476b0":"code","11b8a656":"code","0bdc081e":"code","df15dfe8":"code","db0b2303":"code","2bfda433":"code","7639cb2c":"code","08733ab6":"code","8a075c1d":"code","3ef806c1":"code","345cd803":"code","8b10bd27":"code","8afed52f":"code","d4855ff1":"code","c301e391":"code","d684a59a":"code","b7d5e5df":"code","e156d10b":"code","cca426cd":"code","2e237eb9":"code","fd46d66d":"code","a32aaf96":"code","30e11e4a":"code","7ea2de81":"code","b9064b11":"code","8ec97ff8":"markdown","e40abd79":"markdown","e3bf5d45":"markdown","1cde9621":"markdown","4c950586":"markdown","aaa4fa68":"markdown","21cad54c":"markdown","14de47c6":"markdown","3919ed75":"markdown","e01dffa0":"markdown","933f3a8d":"markdown","b2c8d237":"markdown","a02713d4":"markdown","9a3f1e1e":"markdown","0e39971e":"markdown","454330f6":"markdown","4d4346b9":"markdown","4cbf4d07":"markdown","71b5c390":"markdown","6e15ec32":"markdown","5add6163":"markdown","941756d0":"markdown","471c5bf3":"markdown","b42286f1":"markdown","71908a79":"markdown","dbd8b03a":"markdown","e9a75a00":"markdown","53c6b006":"markdown","3fe1b26a":"markdown","2dcd36dd":"markdown","cb6186e6":"markdown","d2e72cce":"markdown","cfe16198":"markdown","489210de":"markdown","35ddb3aa":"markdown","c0cf10de":"markdown"},"source":{"62d956a2":"import numpy as np\nimport pandas as pd\nfrom sklearn.utils import shuffle\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\nfrom importlib import reload\n%matplotlib inline","25be1496":"df = pd.read_csv(\"..\/input\/ph-data.csv\")\ndf = shuffle(df)\ndf.head()","05accd18":"df.info()","1d8a09d1":"def label(row):\n    if row['label'] < 7:\n        return 0 # acid\n    else: \n        return 1 # base\ndf['class'] = df.apply(lambda row: label(row), axis=1)\ndf.drop('label', axis=1, inplace=True)\ndf.head()","fdc6d7ec":"df['class'].value_counts()","e1fcf8f8":"n = df['class'].value_counts()[0] + df['class'].value_counts()[1]\nn0 = df['class'].value_counts()[0]\nn1 = df['class'].value_counts()[1]\n\np_0 = n0 \/ n \np_1 = n1 \/ n\n\nprint(\"Probliblity a priori for class\\t 0 : {}\\t 1: {}\".format(round(p_0, 3), round(p_1, 3)))","7f6aea05":"# acid \nm_blue_acid = df[df['class'] == 0]['blue'].mean()\nm_green_acid = df[df['class'] == 0]['green'].mean()\nm_red_acid = df[df['class'] == 0]['red'].mean()\n# base \nm_blue_base = df[df['class'] == 1]['blue'].mean()\nm_green_base = df[df['class'] == 1]['green'].mean()\nm_red_base = df[df['class'] == 1]['red'].mean()\n\n# mean acid vector\nm_acid = np.array([m_red_acid, m_green_acid, m_blue_acid]).T\nm_acid_matrix = np.matrix(m_acid)\n# mean base vector\nm_base = np.array([m_red_base, m_green_base, m_blue_base]).T\nm_base_matrix = np.matrix(m_base)\n\n# only for plotly\nm_acid_df = pd.DataFrame(data=m_acid).T\nm_acid_df.columns = ['red', 'green', 'blue']\nm_base_df = pd.DataFrame(data=m_base).T\nm_base_df.columns = ['red', 'green', 'blue']\n\n\nprint(\"Mean for acid:\\n red - {}\\n green - {}\\n blue - {}\".\n      format(round(m_red_acid,3), round(m_green_acid,3), round(m_blue_acid,3)))\nprint(\"Mean for base:\\n red - {}\\n green - {}\\n blue - {}\".\n      format(round(m_red_base,3), round(m_green_base,3), round(m_blue_base,3)))","7917f42c":"acid_i = np.matrix(df[df['class'] == 0].drop('class', axis=1).values)\nbase_i = np.matrix(df[df['class'] == 1].drop('class', axis=1).values)\nacid_cov_matrix = np.zeros((3, 3))\nbase_cov_matrix = np.zeros((3, 3))\nfor i in range(n0):\n    acid_cov_matrix += np.dot((acid_i[i].T - m_acid_matrix.T),(acid_i[i] - m_acid_matrix))\nfor i in range(n1):\n    base_cov_matrix += np.dot((base_i[i].T - m_base_matrix.T),(base_i[i] - m_base_matrix))\nacid_cov_matrix = acid_cov_matrix \/ (n0 - 1)\nbase_cov_matrix = base_cov_matrix \/ (n1 - 1)\nprint('Acid covariance matrix: \\n{} \\nBase covariance matrix: \\n{}'.format(acid_cov_matrix, base_cov_matrix))","deb5e4c3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# for acid\ncov_acid = df[df['class'] == 0].drop('class', axis=1).cov()\n# for base\ncov_base = df[df['class'] == 1].drop('class', axis=1).cov()\n\nfig, axn = plt.subplots(1, 2, figsize=(15,5))\nsns.heatmap(cov_acid, \n            xticklabels=cov_acid.columns.values,\n            yticklabels=cov_acid.columns.values,annot=True,ax=axn[0])\naxn[0].set_title('Covariance matrix - acid')\nsns.heatmap(cov_base, \n            xticklabels=cov_base.columns.values,\n            yticklabels=cov_base.columns.values,annot=True,ax=axn[1])\naxn[1].set_title('Covariance matrix - base')\nplt.show()","06dbc36b":"import plotly\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\nacid = go.Scatter3d(\n    x=df[df['class'] == 0]['blue'],\n    y=df[df['class'] == 0]['red'],\n    z=df[df['class'] == 0]['green'],\n    mode='markers',\n    marker = dict(size=3,\n                  color='rgb(255,0,0)',\n                  line=dict(width=0)),\n    name ='ACID'\n)\nbase = go.Scatter3d(\n    x=df[df['class'] == 1]['blue'],\n    y=df[df['class'] == 1]['red'],\n    z=df[df['class'] == 1]['green'],\n    mode='markers',\n    marker = dict(size=3,\n                  color='rgb(0,0,255)',\n                  line=dict(width=0)),\n    name ='BASE'\n)\n\nacid_mean = go.Scatter3d(\n    x = m_acid_df['blue'],\n    y = m_acid_df['green'],\n    z = m_acid_df['red'],\n    mode='markers',\n    marker = dict(size=10,\n                  color='rgb(255,20,0)',\n                  line=dict(width=3)),\n    name = \"ACID_MEAN\"\n)\n\nbase_mean = go.Scatter3d(\n    x = m_base_df['blue'],\n    y = m_base_df['green'],\n    z = m_base_df['red'],\n    mode='markers',\n    marker = dict(size=10,\n                  color='rgb(0,20,255)',\n                  line=dict(width=3)),\n    name = \"BASE_MEAN\"\n)\n\ndata = [acid, base, acid_mean, base_mean]\nlayout = go.Layout(\n    title='PH-scale',\n    scene = dict(\n        xaxis = dict(title='blue'),\n        yaxis = dict(title='red'),\n        zaxis = dict(title='green'),)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='ph-scale')","71e963c3":"W = ((n0 - 1) * acid_cov_matrix + (n1 - 1) * base_cov_matrix) \/ (n - 2)\nW","20504728":"W_pd = ((n0 - 1) * cov_acid + (n1 - 1) * cov_base) \/ (n - 2)\nW_pd","e8c312b0":"hat_a = np.linalg.pinv(W)\nhat_a = hat_a.dot(m_acid_matrix.T - m_base_matrix.T)\nprint(hat_a)","b5099ed1":"b = - 0.5 * hat_a.T\nb = b.dot(m_acid_matrix.T - m_base_matrix.T)\nprint(b)","9a719daf":"hat_a_pd = pd.DataFrame(np.linalg.pinv(W_pd.values), W_pd.columns, W_pd.index).values\nhat_a_pd = hat_a_pd.dot(m_acid_matrix.T - m_base_matrix.T)\nprint(hat_a_pd)","a1c673e4":"b_pd = - 0.5 * hat_a_pd.T\nb_pd = b_pd.dot(m_acid_matrix.T - m_base_matrix.T)\nprint(b_pd)","36897f58":"hyper_d = (0.5 * ((m_base_matrix - m_acid_matrix) * np.linalg.inv(W)) * (m_acid_matrix.T + m_base_matrix.T)).item(0)\nhyper_a = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(W)).item(0)\nhyper_b = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(W)).item(1)\nhyper_c = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(W)).item(2)\nprint(\"a : {}\\tb : {}\\tc : {}\\td : {}\".format(hyper_a, hyper_b, hyper_c, hyper_d))","2c5ca63b":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.animation as animation\nimport os\n\ndef rotate(angle):\n    ax.view_init(azim=angle)\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111,projection='3d')\n\nx = np.linspace(0, 250)\ny = np.linspace(0, 250)\n\nX,Y = np.meshgrid(x,y)\nZ = (-hyper_a*X - hyper_b*Y + hyper_d) * (1. \/ hyper_c)\n\n# plot the surface\nax.plot_surface(X, Y, Z) # use in animation\nax.scatter(df[df['class'] == 1]['red'],\n    df[df['class'] == 1]['green'],\n    df[df['class'] == 1]['blue'])\nax.scatter(df[df['class'] == 0]['red'],\n    df[df['class'] == 0]['green'],\n    df[df['class'] == 0]['blue'])\n# plt.axis('off')\n# rot_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0,362,2),interval=100)\n# rot_animation.save('rotation1.gif', dpi=80, writer='imagemagick')\nplt.show()","7e03f894":"# naive Bayes \n# LDA or QDA \n# random forest + graphs ","614c5c23":"X = np.array(df.drop('class', axis=1))\ny = np.array(df['class'])\nX.shape, y.shape","c56662f1":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(store_covariance=True)\nlda.fit(X, y)","d0e82e6d":"lda_matrix = lda.covariance_\nprint(lda_matrix)","fa076d6a":"lda_d = (0.5 * ((m_base_matrix - m_acid_matrix) * np.linalg.inv(lda_matrix)) *(m_acid_matrix.T + m_base_matrix.T)).item(0)\nlda_a = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(lda_matrix)).item(0)\nlda_b = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(lda_matrix)).item(1)\nlda_c = ((m_base_matrix - m_acid_matrix) * np.linalg.inv(lda_matrix)).item(2)\nprint(\"a : {}\\tb : {}\\tc : {}\\td : {}\".format(lda_a, lda_b, lda_c, lda_d))","d8792c75":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nqda = QuadraticDiscriminantAnalysis(store_covariance=True)\nqda.fit(X, y)","b3a2bd64":"qda_cov = qda.covariance_ \nqda_cov","3539fbdf":"acid = go.Scatter3d(\n    x=df[df['class'] == 0]['blue'],\n    y=df[df['class'] == 0]['red'],\n    z=df[df['class'] == 0]['green'],\n    mode='markers',\n    marker = dict(size=3,\n                  color='rgb(255,0,0)',\n                  line=dict(width=0)),\n    name ='ACID'\n)\nbase = go.Scatter3d(\n    x=df[df['class'] == 1]['blue'],\n    y=df[df['class'] == 1]['red'],\n    z=df[df['class'] == 1]['green'],\n    mode='markers',\n    marker = dict(size=3,\n                  color='rgb(0,0,255)',\n                  line=dict(width=0)),\n    name ='BASE'\n)\n\nx_line = np.linspace(0,200)\ny_line = np.linspace(0,200)\nX_surface, Y_surface = np.meshgrid(x_line, y_line)\nZ = (-hyper_a*X_surface - hyper_b*Y_surface + hyper_d) * (1. \/ hyper_c)\nZ_lda = (-lda_a*X_surface - lda_b*Y_surface + lda_d) * (1. \/ lda_c)\n\n# something wrong with W matrix\nsurface = go.Surface(z=X_surface, x=Z, y=Y_surface, opacity=0.9, name ='SURFACE')\nsurface_lda = go.Surface(z=Y_surface, x=Z_lda, y=X_surface, opacity=0.9, name ='SURFACE_lda')\n\ndata_surface = [acid, base, surface, surface_lda]\nlayout = go.Layout(\n    title='Comparation between surfaces',\n    scene = dict(\n        xaxis = dict(title='blue'),\n        yaxis = dict(title='red'),\n        zaxis = dict(title='green'),)\n)\nfig = go.Figure(data=data_surface, layout=layout)\npy.iplot(fig,filename='ph-surface')","5e76fd68":"lda.predict_proba([[255,0,0]]) # only blue -> base ","cba0403a":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\nX_reduced = pca.fit_transform(X)\nX_reduced[:10]","ea1bc7b1":"pca.explained_variance_ratio_","a474f832":"1 - pca.explained_variance_ratio_.sum()","b204be05":"def plot_after_dim_reduce(X_reduced):\n    for i in range(len(y)):\n        if y[i] == 0:\n            plt.scatter(X_reduced[i, 0], X_reduced[i, 1], c='b')\n        elif y[i] == 1:\n            plt.scatter(X_reduced[i, 0], X_reduced[i, 1], c='r')\n    plt.grid(True)\n    plt.show()","29e8fa6d":"plot_after_dim_reduce(X_reduced)","05492b6b":"from sklearn.decomposition import IncrementalPCA\n\nn_batches = 50\ninc_pca = IncrementalPCA(n_components=2)\nfor X_batch in np.array_split(X, n_batches):\n    inc_pca.partial_fit(X_batch)\n\nX_reduced_inc_pca = inc_pca.fit_transform(X)","f082a581":"plot_after_dim_reduce(X_reduced_inc_pca)","e9f9c007":"X_recoverd_inc_pca = inc_pca.inverse_transform(X_reduced_inc_pca)\nprint(X_recoverd_inc_pca[:5])\nprint(X[:5])","812195b2":"from sklearn.decomposition import KernelPCA\n\nrbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.03)\nX_reduced_kernel_rbf = rbf_pca.fit_transform(X)","8022f177":"plot_after_dim_reduce(X_reduced_kernel_rbf)","3d24e0d9":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nclf = Pipeline([\n        (\"kpca\", KernelPCA(n_components=2)),\n        (\"log_reg\", LogisticRegression())\n    ])\n\nparam_grid = [{\n        \"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n        \"kpca__kernel\": [\"rbf\", \"sigmoid\", \"poly\", \"linear\"]\n    }]\n\ngrid_search = GridSearchCV(clf, param_grid, cv=10)\ngrid_search.fit(X, y)","ebe476b0":"print(grid_search.best_params_)","11b8a656":"lin_pca = KernelPCA(n_components=2, kernel='linear', gamma=0.03)\nX_reduced_kernel_lin = lin_pca.fit_transform(X)","0bdc081e":"plot_after_dim_reduce(X_reduced_kernel_lin)","df15dfe8":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, random_state=42)\nX_reduced_lle = lle.fit_transform(X)","db0b2303":"plot_after_dim_reduce(X_reduced_lle)","2bfda433":"from sklearn.manifold import MDS\n\nmds = MDS(n_components=2, random_state=42)\nX_reduced_mds = mds.fit_transform(X)","7639cb2c":"plot_after_dim_reduce(X_reduced_mds)","08733ab6":"from sklearn.manifold import Isomap\n\nisomap = Isomap(n_components=2)\nX_reduced_isomap = isomap.fit_transform(X)","8a075c1d":"plot_after_dim_reduce(X_reduced_isomap)","3ef806c1":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_reduced_tsne = tsne.fit_transform(X)","345cd803":"plot_after_dim_reduce(X_reduced_tsne)","8b10bd27":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","8afed52f":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train, y_train)","d4855ff1":"from sklearn.metrics import accuracy_score, auc, confusion_matrix, roc_curve\n\ny_pred = log_reg.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprint('Accuracy : {}'.format(acc))","c301e391":"confusion_matrix(y_test, y_pred)","d684a59a":"from sklearn.metrics import roc_auc_score\nimport scikitplot as skplt\n\ny_pred_proba = log_reg.predict_proba(X_test)\nskplt.metrics.plot_precision_recall(y_test, y_pred_proba)\nplt.show()","b7d5e5df":"X_train_dr, X_test_dr, y_train_dr, y_test_dr = train_test_split(X_reduced_kernel_lin, y, test_size=0.2)\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train_dr, y_train_dr)","e156d10b":"y_pred_dr = log_reg.predict(X_test_dr)\nacc_dr = accuracy_score(y_test_dr, y_pred_dr)\nprint('Accuracy : {}'.format(acc_dr))","cca426cd":"confusion_matrix(y_test_dr, y_pred_dr)","2e237eb9":"X_train_mds, X_test_mds, y_train_mds, y_test_mds = train_test_split(X_reduced_mds, y, test_size=0.2)\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train_mds, y_train_mds)","fd46d66d":"y_pred_mds = log_reg.predict(X_test_mds)\nacc_mds = accuracy_score(y_test_mds, y_pred_mds)\nprint('Accuracy : {}'.format(acc_mds))","a32aaf96":"confusion_matrix(y_test_mds, y_pred_mds)","30e11e4a":"X_train_tsne, X_test_tsne, y_train_tsne, y_test_tsne = train_test_split(X_reduced_tsne, y, test_size=0.2)\nlog_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_train_tsne, y_train_tsne)","7ea2de81":"y_pred_tsne = log_reg.predict(X_test_tsne)\nacc_tsne = accuracy_score(y_test_tsne, y_pred_tsne)\nprint('Accuracy : {}'.format(acc_tsne))","b9064b11":"confusion_matrix(y_test_tsne, y_pred_tsne)","8ec97ff8":"**We loss 9.7% information**","e40abd79":"### Isomap","e3bf5d45":"*the surface is supposed to split the data. I have to find a mistake*","1cde9621":"### W matrix\n\n*** \nW is an intragroup covariance matrix \n$$\n    \\begin{align}\n    \\large W = \\frac{1}{n-2} \\sum_{k=1}^{2} \\left(n_k - 1\\right) S_k\n    \\end{align}\n$$\n$S_k$ is a covariance matrix of each class, $n$ is a sum of every instances. ","4c950586":"**calculating by hand**","aaa4fa68":"## Split data ","21cad54c":"### Split data into two bins <0,7) - acid, <7,14> - base","14de47c6":"### PCA","3919ed75":"## Split data and predict ","e01dffa0":"### t-SNE","933f3a8d":"**with *Pandas* function**","b2c8d237":"## Linear discriminant analysis","a02713d4":"### MDS","9a3f1e1e":"### Probability","0e39971e":"**with grid search**","454330f6":"### Visualization","4d4346b9":"**ROC curve**","4cbf4d07":"**try to use this matrix to create surface**","71b5c390":"## Quadratic Discriminant Analysis","6e15ec32":"**Logistic Regression with dimensinality reduction (t-SNE)**","5add6163":"### LLE","941756d0":"**using calculated by hand matrixes**","471c5bf3":"**using calculated by hand matrixes**","b42286f1":"### Calculate mean of each class ","71908a79":"### Kernel PCA","dbd8b03a":"### Calculating covariance","e9a75a00":"## Dimensionality reduction","53c6b006":"### Calculate hypersurface parametrs","3fe1b26a":"# Classify whether the image show acid or base","2dcd36dd":"**Logistic Regression with dimensinality reduction (MDS)**","cb6186e6":"### Incremental PCA","d2e72cce":"**using *Pandas* matrixes**","cfe16198":"**Confusion matrix**","489210de":"### Logistic Regression","35ddb3aa":"**Logistic Regression with dimensinality reduction (Linear PCA)**","c0cf10de":"### $\\hat{a}$ vector and $b$\n\n\\begin{align}\n\\large \\hat{a} &= W^{-1} \\left(mean_{2} - mean_{1}\\right) \\\\\n\\large b &= - 0.5 \\cdot \\hat{a}^T \\cdot \\left(mean_{2} - mean_{1}\\right)\n\\end{align}\n"}}