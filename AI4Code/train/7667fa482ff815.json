{"cell_type":{"6aff6888":"code","d2bae610":"code","6086a180":"code","202c8bba":"code","c20072ff":"code","4b6b75fc":"code","2847d5ed":"code","c76f414c":"code","40441ca6":"code","3b9fe1d9":"code","e6d82383":"markdown"},"source":{"6aff6888":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","d2bae610":"import gc\nimport os\nimport random\nimport wandb\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\n\ndevice = torch.device(\"cuda\")","6086a180":"class config:\n    EXP_NAME = \"exp138_classifiy\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SEED = 0\n    \n    LR = 5e-3\n    N_EPOCHS = 50\n    EMBED_SIZE = 64\n    HIDDEN_SIZE = 256\n    BS = 512\n    WEIGHT_DECAY = 1e-5\n    T_MAX = 50\n    MIN_LR = 1e-6\n    \n    NOT_WATCH_PARAM = ['INPUT']","202c8bba":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","c20072ff":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df, label_dic=None):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        self.label_dic = label_dic\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        \n        X = df[['R_cate', 'C_cate', 'u_in', 'u_out']].values\n        y = df['pressure'].values\n        \n        if self.label_dic is None:\n            label = [-1]\n        else:\n            label = [self.label_dic[i] for i in y]\n\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\" : torch.tensor(label).long(),\n        }\n        return d","4b6b75fc":"class VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        self.seq_emb = nn.Sequential(\n            nn.Linear(4, config.EMBED_SIZE),\n            nn.LayerNorm(config.EMBED_SIZE),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        \n        self.cnn_1_1 = nn.Conv1d(config.EMBED_SIZE, config.HIDDEN_SIZE, kernel_size=1, padding=0)\n        self.cnn_1_2 = nn.Conv1d(config.HIDDEN_SIZE, config.HIDDEN_SIZE, kernel_size=1, padding=0)\n        self.cnn_2_1 = nn.Conv1d(config.EMBED_SIZE, config.HIDDEN_SIZE, kernel_size=2, padding=1)\n        self.cnn_2_2 = nn.Conv1d(config.HIDDEN_SIZE, config.HIDDEN_SIZE, kernel_size=2, padding=0)\n        self.cnn_3_1 = nn.Conv1d(config.EMBED_SIZE, config.HIDDEN_SIZE, kernel_size=3, padding=2)\n        self.cnn_3_2 = nn.Conv1d(config.HIDDEN_SIZE, config.HIDDEN_SIZE, kernel_size=3, padding=0)\n        self.cnn_4_1 = nn.Conv1d(config.EMBED_SIZE, config.HIDDEN_SIZE, kernel_size=4, padding=3)\n        self.cnn_4_2 = nn.Conv1d(config.HIDDEN_SIZE, config.HIDDEN_SIZE, kernel_size=4, padding=0)\n        \n        self.lstm1 = nn.LSTM(config.HIDDEN_SIZE * 4, config.HIDDEN_SIZE, batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE, batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE, batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE, batch_first=True, bidirectional=True)\n        self.head = nn.Sequential(\n            nn.Linear(config.HIDDEN_SIZE * 2, config.HIDDEN_SIZE * 2),\n            nn.LayerNorm(config.HIDDEN_SIZE * 2),\n            nn.ReLU(),\n            nn.Dropout(0.),\n            nn.Linear(config.HIDDEN_SIZE * 2, 950),\n        )\n        \n        for n, m in self.named_modules():\n            if isinstance(m, nn.LSTM):\n                print(f'init {m}')\n                for param in m.parameters():\n                    if len(param.shape) >= 2:\n                        nn.init.orthogonal_(param.data)\n                    else:\n                        nn.init.normal_(param.data)\n\n    def forward(self, X, y=None):\n        h = self.seq_emb(X)\n        # CNN\n        h = h.permute(0, 2, 1)\n        \n        h1 = F.relu(self.cnn_1_1(h))\n        h1 = F.relu(self.cnn_1_2(h1))\n        h2 = F.relu(self.cnn_2_1(h))\n        h2 = F.relu(self.cnn_2_2(h2))\n        h3 = F.relu(self.cnn_3_1(h))\n        h3 = F.relu(self.cnn_3_2(h3))\n        h4 = F.relu(self.cnn_4_1(h))\n        h4 = F.relu(self.cnn_4_2(h4))\n        h = torch.cat((h1, h2, h3, h4), 1)\n        \n        h = h.permute(0, 2, 1)\n        # LSTM\n        out, (hn, cn) = self.lstm1(h, None) \n        out, (hn, cn) = self.lstm2(out, (hn, cn)) \n        out, (hn, cn) = self.lstm3(out, (hn, cn)) \n        h, _ = self.lstm4(out, (hn, cn))\n        # Head\n        logits = self.head(h)\n        \n        if y is None:\n            loss = None\n        else:\n            mask = X[:, :, 3] == 0\n            loss = self.loss_fn(logits, y, mask)\n            \n        return logits, loss\n    \n    def loss_fn(self, y_pred, y_true, mask):\n        criterion = nn.CrossEntropyLoss()\n\n        loss_u_out_0 = criterion(y_pred[mask].reshape(-1, 950), y_true[mask].reshape(-1))\n        loss_u_out_1 = criterion(y_pred[mask==0].reshape(-1, 950), y_true[mask==0].reshape(-1))\n\n        for lag, w in [(1, 0.4), (2, 0.2), (3, 0.1), (4, 0.1)]:\n            # negative\n            loss_u_out_0 += criterion(y_pred[mask].reshape(-1, 950), F.relu(y_true[mask].reshape(-1) - lag).long()) * w\n            loss_u_out_1 += criterion(y_pred[mask==0].reshape(-1, 950), F.relu(y_true[mask==0].reshape(-1) - lag).long()) * w\n            # positive\n            loss_u_out_0 += criterion(y_pred[mask].reshape(-1, 950), (949 - F.relu((949 - (y_true[mask].reshape(-1) + lag)))).long()) * w\n            loss_u_out_1 += criterion(y_pred[mask==0].reshape(-1, 950), (949 - F.relu((949 - (y_true[mask==0].reshape(-1) + lag)))).long()) * w\n\n        loss = loss_u_out_0 + loss_u_out_1 * 0.5\n        return loss","2847d5ed":"def train_loop(model, optimizer, scheduler, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        scheduler.step()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\ndef valid_loop(model, loader, target_dic_inv):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        losses.append(loss.item())\n        predicts.append(out.argmax(2).cpu())\n\n    return np.array(losses).mean(), target_dic_inv[torch.vstack(predicts).reshape(-1)].numpy()\n\ndef test_loop(model, loader, target_dic_inv):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        predicts.append(out.argmax(2).cpu())\n\n    return target_dic_inv[torch.vstack(predicts).reshape(-1)].numpy()","c76f414c":"def main():\n    \n    train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n    \n    target_dic = {v:i for i, v in enumerate(sorted(train_df['pressure'].unique().tolist()))}\n    target_dic_inv = torch.tensor(list(target_dic.keys()))\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df['C_cate'] = train_df['C'].map({10: 0, 20: 1, 50:2})\n    train_df['R_cate'] = train_df['R'].map({5: 0, 20: 1, 50:2})\n    test_df['C_cate'] = test_df['C'].map({10: 0, 20: 1, 50:2})\n    test_df['R_cate'] = test_df['R'].map({5: 0, 20: 1, 50:2})\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"), target_dic)\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"), target_dic)\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        model = VentilatorModel()\n        model.to(device)\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n        num_train_steps = int(len(train_loader) * config.N_EPOCHS)\n        num_warmup_steps = int(num_train_steps \/ 10)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_train_steps)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n        wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}\/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}\/{config.EXP_NAME}\/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_score = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n            train_loss, lrs = train_loop(model, optimizer, scheduler, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader, target_dic_inv)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            if valid_score < valid_best_score:\n                valid_best_score = valid_score\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_score\": valid_score,\n                \"valid_best_score\": valid_best_score,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader, target_dic_inv)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/sub_f{fold}.csv\", index=None)\n        \n    train_df['oof'] = oof\n    train_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/oof.csv\", index=None)\n    \n    sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n    sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission.csv\", index=None)\n    \n    cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n    print(\"CV:\", cv_score)","40441ca6":"if __name__ == \"__main__\":\n    main()","3b9fe1d9":"wandb.finish()","e6d82383":"# Ventilator 1dCNN LSTM\n\nIn most sharing code, the feature values to input LSTM are made by humans.\nI use 1d-CNN to extract features.\n\nBy only using the basic 4 features (R, C, u_in, u_out), I got OOF score = 0.387 and LB score = 0.559.\n\n## Update\n### 2021.10.27 (Version\u00a06)\nmultiple kernel(1, 2, 3, 4) CNN and classification training\n\nI got OOF score = 0.4126, LB score = 0.5946.\nIt's a huge overfit.\n\n<img src=\"https:\/\/raw.githubusercontent.com\/trtd56\/RFCX\/main\/tmp\/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88%202021-10-27%2015.39.51.png\" alt=\"drawing\" width=\"400\"\/>\n"}}