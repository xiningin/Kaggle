{"cell_type":{"9dc93341":"code","8ced6609":"code","843d6589":"code","6c46ae9c":"code","239961b0":"code","96b865d8":"code","19410525":"code","8b1b3bc7":"code","439abecb":"code","85d4665f":"code","ea914524":"code","e8ce3a51":"code","e65c8555":"code","f80386b0":"code","d8979cd8":"code","f993bd06":"code","9658b95a":"code","f6b64be4":"code","d7a3ce6e":"code","0998af72":"code","7dabcc67":"code","69fd48f7":"code","33db531f":"code","978e82f9":"code","8b08fb83":"code","b802e51b":"code","40e9897a":"code","9a1a1bb6":"code","857e623a":"code","3ef25a1f":"code","f6f04f2c":"code","301cb0ce":"code","1f349108":"code","1ce8b422":"code","e000d14e":"code","551294f9":"markdown","087f4a0c":"markdown","cc600ea6":"markdown","25647edf":"markdown","250bb6f3":"markdown","9e8b7b83":"markdown","697574a6":"markdown","bd47b883":"markdown","dc7c8f24":"markdown","25bc307d":"markdown","b5ca7f72":"markdown","366e4b18":"markdown","751a7afe":"markdown","7a041faf":"markdown","d5b1641c":"markdown","5df7872a":"markdown","9e224725":"markdown","c4324f09":"markdown","0c07c2e6":"markdown"},"source":{"9dc93341":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('bmh')\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import MaxAbsScaler,PowerTransformer,MinMaxScaler,RobustScaler, StandardScaler, Normalizer, QuantileTransformer\n\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,GridSearchCV\nfrom sklearn.decomposition import PCA\n\n# models\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,VotingClassifier,\\\nGradientBoostingClassifier,StackingClassifier,VotingClassifier,HistGradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression,Perceptron,RidgeClassifier,RidgeClassifierCV,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, make_scorer\nfrom scipy import stats\nfrom imblearn.over_sampling import SMOTE, ADASYN,BorderlineSMOTE,KMeansSMOTE,SVMSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.mixture import GaussianMixture\n\nimport time","8ced6609":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain=pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv',index_col='ID')\ntest=pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv',index_col='ID')\ntest_ID=test.index","843d6589":"train.head()","6c46ae9c":"train.describe()","239961b0":"df1=train[train['y']=='HOROZ']\ndf2=train[train['y']=='SEKER']\ndf3=train[train['y']=='DERMASON']\ndf4=train[train['y']=='SIRA']\ndf5=train[train['y']=='BARBUNYA']\ndf6=train[train['y']=='CALI']\ndf7=train[train['y']=='BOMBAY']","96b865d8":"dfs=[df1,df2,df3,df4,df5,df6,df7]\nfor i in dfs:\n    print(i['y'].unique())\n    display(i.describe())","19410525":"train.info()","8b1b3bc7":"train.y.unique()","439abecb":"print(train.duplicated().sum())\nprint(test.duplicated().sum())","85d4665f":"sns.pairplot(train,hue='y')","ea914524":"train.skew(axis=0)","e8ce3a51":"plt.figure(figsize=(10,7))\nsns.heatmap(train.corr(),annot=True)","e65c8555":"train['y'] = train['y'].map({'HOROZ':0, 'SEKER':1, 'DERMASON':2, 'SIRA':3, 'BARBUNYA':4, 'CALI':5, 'BOMBAY':6})","f80386b0":"train.y.unique()","d8979cd8":"print(train.shape)\nprint(test.shape)","f993bd06":"plt.figure(figsize=(25, 25))\nfor i, col in enumerate(list(train.columns)):\n    plt.subplot(7, 4, i+1)\n    sns.histplot(train[col], kde=True, bins=10)","9658b95a":"plt.figure(figsize=(25, 25))\nfor i, col in enumerate(list(train.columns)):\n    plt.subplot(7, 4, i+1)\n    sns.boxplot(train[train['y'] == 0][col])","f6b64be4":"# #remove all outliers\n# from scipy import stats\n# z_scores = stats.zscore(train)\n# abs_z_scores = np.abs(z_scores)\n# filtered_entries = (abs_z_scores < 2).all(axis=1)\n# train = train[filtered_entries]","d7a3ce6e":"Y=train['y']\ntrain.drop('y',axis='columns',inplace=True)","0998af72":"# #boxcox outliers handling\n# for col in train.columns:\n#     train[col],fitted_lambda= stats.boxcox(train[col] ,lmbda=None)\n#     test[col],fitted_lambda= stats.boxcox(test[col] ,lmbda=None)","7dabcc67":"# #imputing outliers using median\n# for col in train.columns:\n#     q1 = train[col].quantile(0.25)\n#     q3 = train[col].quantile(0.75)\n#     iqr = q3-q1\n#     Lower_tail = q1 - 1.5 * iqr\n#     Upper_tail = q3 + 1.5 * iqr\n#     m = np.median(train[col])\n#     for i in train[col]:\n#         if i > Upper_tail or i < Lower_tail:\n#                 train[col] = train[col].replace(i, m)\n    \n#     q1 = test[col].quantile(0.25)\n#     q3 = test[col].quantile(0.75)\n#     iqr = q3-q1\n#     Lower_tail = q1 - 1.5 * iqr\n#     Upper_tail = q3 + 1.5 * iqr\n#     m = np.median(test[col])\n#     for i in test[col]:\n#         if i > Upper_tail or i < Lower_tail:\n#                 test[col] = test[col].replace(i, m)","69fd48f7":"X_train, X_valid, y_train, y_valid = train_test_split(train,Y, train_size=0.8,random_state=465,stratify=Y)","33db531f":"#balancing Dataset\nprint(train.shape)\nprint(Y.value_counts())\noversample = BorderlineSMOTE(sampling_strategy={2:3000,6:1300,4:1300,1:2000,5:1900},random_state = 465)\ntrain, Y = oversample.fit_resample(train, Y)\nprint(train.shape)\nprint(Y.value_counts())","978e82f9":"scaler=PowerTransformer()\ntrain=pd.DataFrame(scaler.fit_transform(train),columns=train.columns)\ntest=pd.DataFrame(scaler.transform(test),columns=test.columns)","8b08fb83":"# rf=RandomForestClassifier()\n# ada=AdaBoostClassifier()\n# et=ExtraTreesClassifier()\n# gbc=GradientBoostingClassifier()  #excluded due to very long training time and not getting best results\n# hgbc=HistGradientBoostingClassifier()\n# per=Perceptron()\n# rc=RidgeClassifier()\n# rcv=RidgeClassifierCV()\n# sgd=SGDClassifier()\n# dt=DecisionTreeClassifier()\n# svm=SVC()\n# xgb=XGBClassifier()\n# catb=CatBoostClassifier(verbose=None)\n# knn=KNeighborsClassifier(7)\n# mlp=MLPClassifier()\n\n\n# models=[rf,ada,et,hgbc,per,rc,rcv,sgd,dt,svm,xgb,knn,mlp] \n# # models=[svm,ada]\n\n# for model in models:\n#     start=time.time()\n#     grid=GridSearchCV(estimator=model,param_grid={},scoring='f1_micro',cv=5,verbose=1)\n#     grid.fit(train,Y)\n#     end = time.time()\n#     print(model, '\\n', grid.best_score_,'\\n', round(end-start))","b802e51b":"train.drop(['AspectRation','Eccentricity'],axis=1,inplace=True)\ntest.drop(['AspectRation','Eccentricity'],axis=1,inplace=True)","40e9897a":"# params={\n#     'C':[5.3],\n#     'kernel' : ['poly'],\n#     'decision_function_shape':['ovo'],\n#     'coef0' : [3.1],\n#        }\n# model = GridSearchCV(estimator=SVC(random_state = 465), param_grid=params, scoring='f1_micro', cv=3,verbose=3)\n# model.fit(train,Y)\n# print(model.best_params_)\n# print(model.best_estimator_)\n# print(model.best_score_)\n# bestmodel=model.best_estimator_","9a1a1bb6":"# params={#'hidden_layer_sizes':[45,50,55,60],\n#     'hidden_layer_sizes':[24],\n# #        'activation':['identity', 'logistic', 'tanh', 'relu'],\n# #        'early_stopping':[True],\n#         'beta_1' :[.7],\n#        }\n# model = GridSearchCV(estimator=MLPClassifier(random_state = 158), param_grid=params, scoring='f1_micro', cv=StratifiedKFold(10),verbose=3)\n# model.fit(train,Y)\n# print(model.best_params_)\n# print(model.best_estimator_)\n# print(model.best_score_)\n# bestmodel=model.best_estimat","857e623a":"# class CatBoostClassifierInt(CatBoostClassifier):\n#     def predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=1, verbose=None,parent_method_name=None):\n#         predictions = self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose,parent_method_name)\n     \n#         # This line is the only change I did\n#         return np.asarray(predictions, dtype=np.int64).ravel()","3ef25a1f":"# catboost=CatBoostClassifierInt(CatBoostClassifier(depth= 8, iterations= 90, learning_rate= 0.2))","f6f04f2c":"voting_est=[('SVC',SVC(C=5.3, coef0=3.1, decision_function_shape='ovo', kernel='poly',random_state=0)),\n            ('MultiLayer Perceptron',MLPClassifier(beta_1=0.7, hidden_layer_sizes=24, random_state=158)),\n           ('XGBoost',XGBClassifier(booster='dart',eta=0.03,max_depth=15,subsample=0.8,gamma=0,n_estimators=230,colsample_bytree=0.6,colsample_bylevel=0.6,colsample_bynode=0.6))]\nvc=VotingClassifier(estimators=voting_est)\n\n\nmodels=[vc] \n# models=[svm,ada]\n\nfor mod in models:\n    start=time.time()\n    model=GridSearchCV(estimator=mod,param_grid={},scoring='f1_micro',cv=StratifiedKFold(5),verbose=0)\n    model.fit(train,Y)\n    end = time.time()\n    print(mod, '\\n', model.best_score_,'\\n', round(end-start))\n    bestmodel=model.best_estimator_","301cb0ce":"pred=bestmodel.predict(test)\npredictions = pd.DataFrame({'ID':test_ID,\n                       'y': pred})","1f349108":"predictions['y']=predictions['y'].map({0:'HOROZ', 1:'SEKER', 2:'DERMASON', 3:'SIRA', 4:'BARBUNYA', 5:'CALI',6:'BOMBAY'})","1ce8b422":"predictions","e000d14e":"predictions.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","551294f9":"#  \u2728 Detecting outliers","087f4a0c":"## Mapping The Target Classes","cc600ea6":"# \u2728 Data Scaling","25647edf":"# \u2728 Grid Search","250bb6f3":"# \u2728 Generating Submission File","9e8b7b83":"## Checking Duplicates","697574a6":"# \u2728 Treating Imbalanced Data","bd47b883":"# \u2728 Gathering Insights\n","dc7c8f24":"# \u2728 Making Predictions","25bc307d":"## Correlation","b5ca7f72":"## Checking the Skeweness of the data","366e4b18":"# \u2728 Hi There \n#  We're Serial Kernels \ud83d\udc32","751a7afe":"# \u2728 EDA","7a041faf":"# \u2728 Loading Data","d5b1641c":"* The Data Suffers from High Skeweness\n* The data is Imblanaced there are different distributions in each class\n* There is High correlation between columns\n* No Duplicates \n* No missing Values","5df7872a":"# \u2728 Splitting the data","9e224725":"## Visualization","c4324f09":"# \u2728 Model","0c07c2e6":"# \u2728 Importing Libraries"}}