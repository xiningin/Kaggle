{"cell_type":{"62d5985d":"code","c8c0589c":"code","475467b7":"code","d739bc01":"code","6955a9a5":"code","4f38496d":"code","4d80e24b":"code","0bee987e":"code","633dd52e":"code","f30adb08":"code","5911f61e":"code","ac591d82":"code","178347a6":"code","94c25e78":"code","ef426647":"code","5deeeee7":"code","f5608d61":"code","b7207f89":"code","981c7898":"code","25b78979":"code","12707a62":"code","00f5495c":"code","52e96b2e":"code","a3bb19ab":"code","1be92cc1":"code","21fcadbd":"code","76700e60":"code","428f69be":"code","74491d95":"code","fc790674":"code","8f1d2aa6":"code","8f80efb6":"code","828ef435":"markdown","d4cd293f":"markdown","0be340a4":"markdown","22d78512":"markdown","fd55e06d":"markdown","12512bde":"markdown","5fed1cc7":"markdown","7c8defec":"markdown","e5c07afc":"markdown","0275374f":"markdown","e967fa80":"markdown","c4bf0cd4":"markdown","ab504a84":"markdown","a463ae6d":"markdown","37f692f0":"markdown","2b614078":"markdown","102e3b50":"markdown","b49589c7":"markdown","9cf3beae":"markdown","3409a445":"markdown","b2be8aae":"markdown","41c903d8":"markdown","d323d972":"markdown"},"source":{"62d5985d":"!pip install pyspark","c8c0589c":"import os\nimport sys\nimport pyspark\nfrom pyspark.rdd import RDD\nfrom pyspark.sql import Row\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql import functions\nfrom pyspark.sql.functions import lit, desc, col, size\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nfrom IPython.core.interactiveshell import InteractiveShell\nimport matplotlib\nfrom pylab import *\nimport scipy.stats as stats\n\n# This helps auto print out the items without explixitly using 'print'\nInteractiveShell.ast_node_interactivity = \"all\" \n\n# Initialize a spark session.\n\nconf = pyspark.SparkConf().setMaster(\"local[*]\")\ndef init_spark():\n    spark = SparkSession \\\n        .builder \\\n        .appName(\"Statistical Inferences with Pyspark\") \\\n        .config(conf=conf) \\\n        .getOrCreate()\n    return spark\n\nspark = init_spark()\nfilename_data = '..\/input\/fitrec-data-pyspark\/endomondoHR_proper.json'\ndf = spark.read.json(filename_data, mode=\"DROPMALFORMED\")\n\n# Load meta data file into pyspark data frame as well\nprint('Data frame type: {}'.format(type(df)))","475467b7":"print('Columns & datatypes:')\nDataFrame(df.dtypes, columns =['Column Name','Data type'])","d739bc01":"print('Data frame describe (string and numeric columns only):')\ndf.describe().toPandas()\n\nprint('\\nFisrt 2 data rows:')\ndf.limit(2).toPandas()","6955a9a5":"# Summary function\ndef user_activity_workout_summarize(df):\n    user_count = format(df.select('userId').distinct().count(), ',d')\n    workout_count = format(df.select('id').distinct().count(), ',d')\n    activity_count = str(df.select('sport').distinct().count())\n    seqOp = (lambda x,y: x+y)\n    sum_temp = df.rdd.map(lambda x: len(x.timestamp)).aggregate(0, seqOp, seqOp)\n    total_records_count = format(sum_temp, ',d')\n    columns=['Users count', 'Activity types count','Workouts count', 'Total records count']\n    data = [[user_count], [activity_count], [workout_count], [total_records_count]]\n    sum_dict = {column: data[i] for i, column in enumerate(columns)}\n    sum_df = pd.DataFrame.from_dict(sum_dict)[columns]\n    gender_user_count = df.select('gender','userId').distinct().groupBy('gender').count().toPandas()\n    gender_activities_count = df.groupBy('gender').count().toPandas()\n    gender_user_activity_count = gender_user_count.join(\n        gender_activities_count.set_index('gender'), on='gender'\n        , how='inner', lsuffix='_gu'\n    )\n    gender_user_activity_count.columns = ['Gender', '# of users', 'Activities (workouts) count']\n    \n    return sum_df, gender_user_activity_count\n\nsum_dfs = user_activity_workout_summarize(df)\nprint('\\nOverall data set summary on users, activities(workouts) and number of fitness records:')\nsum_dfs[0]\nprint('\\nSummarize on genders:')\nsum_dfs[1]","4f38496d":"rdd = df.rdd\ndef avgHeartRate(row):\n    if row['heart_rate']!='':\n        ht = np.mean(row['heart_rate'])\n        return Row(id=int(row['id']), gender=str(row['gender'])\n                   , sport=str(row['sport']), userId=row['userId'], avg_heart_rate=float(ht))\nprint('Top 3 rows of average heart rate per workout:')\nrdd_avgHR = rdd.map(avgHeartRate)\nrdd_avgHR.toDF().limit(3).toPandas()\nprint('\\nDescribe:')\ndf1 = spark.createDataFrame(rdd_avgHR)\nDataFrame(df1.dtypes, columns =['Column Name','Data type'])","4d80e24b":"df2 = df1.groupBy(['gender','sport']).avg('avg_heart_rate')\ndf2.createOrReplaceTempView(\"table1\")\ndf2_Male = spark.sql(\"SELECT * FROM table1 WHERE gender='male'\")\ndf2_Female = spark.sql(\"SELECT * FROM table1 WHERE gender='female'\")","0bee987e":"df2A=df2_Male.join(df2_Female, 'sport','outer')","633dd52e":"df2_Male = df2_Male.withColumnRenamed('avg(avg_heart_rate)','maleAvgHR')\ndf2_Female = df2_Female.withColumnRenamed('avg(avg_heart_rate)','femaleAvgHR')\ndf2AInner = df2_Male.join(df2_Female, 'sport','inner')\ndf2AInner = df2AInner.withColumn('diffAvg',df2AInner.maleAvgHR-df2AInner.femaleAvgHR)\ndf2AInner.limit(5).toPandas()","f30adb08":"rddAInner = df2AInner.rdd\nsportType = rddAInner.map(lambda row: row['sport']).collect()\ndiffAvg = rddAInner.map(lambda row: row['diffAvg']).collect()\nxticks = plt.xticks(rotation=90)\nxlabel = plt.xlabel('Sport')\nylabel = plt.ylabel('Average heart rate(male) - Average heart rate(female) (bpm)')\nplot = plt.bar(sportType, diffAvg, facecolor='#558866', edgecolor='white')\ntitle = plt.title('The difference in average heart rate between male and female')","5911f61e":"InteractiveShell.ast_node_interactivity = \"all\"\nrdd2 = df.rdd\ndef covHeartRate(row):\n    if row['heart_rate'] != '' and row['speed'] != '' and row['altitude'] != '':   \n        if size(row['heart_rate']) == size(row['speed']) and size(row['heart_rate']) == size(row['altitude']):\n            pearson_hr_al = stats.pearsonr(row['heart_rate'],row['altitude'])[0]\n            pearson_hr_speed = stats.pearsonr(row['heart_rate'],row['speed'])[0] \n            abs_pearson_hr_al = abs(stats.pearsonr(row['heart_rate'],row['altitude'])[0])\n            abs_pearson_hr_speed = abs(stats.pearsonr(row['heart_rate'],row['speed'])[0]) \n            return Row(id=int(row['id']), pearson_hr_al=float(pearson_hr_al)\n                       , pearson_hr_speed=float(pearson_hr_speed), abs_pearson_hr_al=float(abs_pearson_hr_al)\n                       , abs_pearson_hr_speed=float(abs_pearson_hr_speed)\n                       , gender=str(row['gender']), sport=str(row['sport']),userId=row['userId'])\nrddaHR2 = rdd2.filter(lambda row: row['speed'] is not None).map(covHeartRate)\ndf5 = spark.createDataFrame(rddaHR2).dropna()\nprint('\\nSummary of coefficients table:')\ndf5.describe().toPandas()","ac591d82":"# Aggregate by gender & sport\ndf6 = df5.groupBy(['gender', 'sport']).agg({'abs_pearson_hr_al':'mean', 'abs_pearson_hr_speed':'mean'})\ndf6.createOrReplaceTempView(\"table3\")\ndf6M = spark.sql(\"SELECT * FROM table3 WHERE gender = 'male'\")\ndf6M = df6M.withColumnRenamed('avg(abs_pearson_hr_al)','mAvgPerHtAt') \\\n    .withColumnRenamed('avg(abs_pearson_hr_speed)','mAvgPerHtSp')\n\ndf6FM = spark.sql(\"SELECT * FROM table3 WHERE gender = 'female'\")\ndf6FM = df6FM.withColumnRenamed('avg(abs_pearson_hr_al)','fmAvgPerHtAt') \\\n    .withColumnRenamed('avg(abs_pearson_hr_speed)','fmAvgPerHtSp')","178347a6":"rdd6MInner = df6M.rdd\nrdd6FMInner = df6FM.rdd\nsportTypeM = rdd6MInner.map(lambda row: row['sport']).collect()\nsportTypeFM = rdd6FMInner.map(lambda row: row['sport']).collect()\nperMhtat = rdd6MInner.map(lambda row: row['mAvgPerHtAt']).collect()\nperMhtsp = rdd6MInner.map(lambda row: row['mAvgPerHtSp']).collect()\nperFMhtat = rdd6FMInner.map(lambda row: row['fmAvgPerHtAt']).collect()\nperFMhtsp = rdd6FMInner.map(lambda row: row['fmAvgPerHtSp']).collect()\n\nmeanMhtat, varMhtat = mean(perMhtat), var(perMhtat)\nmeanMhtsp, varMhtsp = mean(perMhtsp), var(perMhtsp)\nmeanFMhtat, varFMhtat = mean(perFMhtat), var(perFMhtat)\nmeanFMhtsp, varFMhtsp = mean(perFMhtsp), var(perFMhtsp)\ncolumn_list = [{'gender': 'male', 'correlation': '(heart rate, altitude)'\n      , 'mean': float(meanMhtat), 'variance': float(varMhtat)}\n     , {'gender': 'male','correlation': '(heart rate, speed)'\n        ,'mean': float(meanMhtsp), 'variance': float(varMhtsp) }\n     , {'gender': 'female', 'correlation':'(heart rate, altitude)'\n      , 'mean': float(meanFMhtat), 'variance': float(varFMhtat)}\n        , {'gender':'female', 'correlation':'(heart rate, speed)'\n           , 'mean': float(meanFMhtsp), 'variance': float(varFMhtsp)}]\nshowdf6 = pd.DataFrame(column_list)\nprint(\"Average correlation coefficient of different sports for male and female:\")\nshowdf6\n\n# Take the differences\ndiffMatsp = np.array(perMhtat) - np.array(perMhtsp)\ndiffFMatsp = np.array(perFMhtat) - np.array(perFMhtsp)\n\nfig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=plt.figaspect(0.35))\nxstick_labels0 = axs[0].set_xticklabels(sportTypeM, rotation=90)\nxlabel0 = axs[0].set_xlabel('Sport')\nylabel0 = axs[0].set_ylabel(\n    'coe(heart rate, altitude) - coe(heart rate, altitude))'\n)\ntitle0 = axs[0].set_title('Male')\nplot0 = axs[0].bar(sportTypeM, diffMatsp, facecolor='#9999ff', edgecolor='white')\nxstick1 = plt.xticks(rotation=90)\nxlabel1 = axs[1].set_xlabel('Sport')\nylabel1 = axs[1].set_ylabel('coe(heart rate, altitude) - coe(heart rate, altitude))')\ntitle1 = axs[1].set_title('Female')\nplot1 = axs[1].bar(sportTypeFM, diffFMatsp, facecolor='#ff9999', edgecolor='white')\ntitle = fig.text(\n    0.5, 1.02, 'Pearson coefficient difference between (heart rate, altitude) vs. (heart rate, speed)'\n    , ha='center', va='top', transform=fig.transFigure\n)","94c25e78":"from datetime import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import udf\n\n# Apply filter to select workouts that have all records occuring in the sameday (cross out the ones that lasted\n# more than one day).\nrdd3=df.rdd\ndef same_day(row):\n    if row['heart_rate'] != '' and row['speed'] != '' and row['altitude'] != '':   \n        if size(row['heart_rate']) == size(row['speed']) and size(row['heart_rate']) == size(row['altitude']):\n            dateValue = [datetime.fromtimestamp(t) - timedelta(hours=7) for t in row['timestamp']]\n            return Row(\n                id=int(row['id']), gender=row['gender'], timestamp=row['timestamp']\n                , heartrate=row['heart_rate'], speed=row['speed'], altitude=row['altitude']\n                , year=dateValue[0].year, month=dateValue[0].month, day=dateValue[0].day\n                , yearl=dateValue[-1].year, monthl=dateValue[-1].month, dayl=dateValue[-1].day\n                , sport=str(row['sport']), userId=row['userId']\n            )\n#  return Row(ratiohtsp=float(vecsp))\nrddaHR3 = rdd3.filter(lambda row: row['speed'] is not None) \\\n    .map(same_day) \\\n    .filter(lambda row: row['year'] == row['yearl'] and row['month'] == row['monthl'] and row['day'] == row['dayl'])\n\ndf7 = spark.createDataFrame(rddaHR3).drop('year').drop('yearl').drop('month') \\\n    .drop('day').drop('monthl').drop('dayl')\n\n# Group and workouts by 4 time ranges of the day based on workout start hour \n# (0: start hour from 0 - 5)\n# (1: start hour from 6 - 11)\n# (2: start hour from 12 - 17)\n# (3: start hour from 18 - 24)\ndef markWorkout(row):\n    hours = [(datetime.fromtimestamp(t) - timedelta(hours=7)).hour for t in row['timestamp']]\n    mark = -1\n    upIndex = -1\n    if hours[0] >= 6 and hours[0] < 12:\n        upIndex = [i for i in range(len(hours)) if hours[i] < 12][-1]\n        mark = 1\n    elif hours[0] >= 12 and hours[0] < 18:\n        upIndex=[i for i in range(len(hours)) if hours[i] < 19][-1]\n        mark = 2\n    elif hours[0] >= 18 and hours[0] < 24:\n        upIndex = [i for i in range(len(hours)) if hours[i] < 24][-1]\n        mark = 3\n    elif hours[0] >= 0 and hours[0] < 6:\n        upIndex = [i for i in range(len(hours)) if hours[i] < 8][-1]\n        mark = 0\n    if mark !=- 1 and upIndex !=- 1:\n        reTimestamp = row['timestamp'][:upIndex]\n        dateValue = [datetime.fromtimestamp(t) for t in reTimestamp]\n        reHeartRate = row['heartrate'][:upIndex]\n        reAltitude = row['altitude'][:upIndex]\n        reSpeed = row['speed'][:upIndex]\n        count = upIndex + 1\n        return Row(\n            id=row['id'], gender=row['gender'], mark=mark, countTimestamp=count\n            , reTimestamp=reTimestamp, dateValue=dateValue, reHeartRate=reHeartRate\n            , reAltitude=reAltitude, reSpeed=reSpeed, sport=str(row['sport']), userId=row['userId']\n        )\n    \nrdd31 = df7.rdd.map(markWorkout).filter(lambda row: row['countTimestamp']>10)\ndf8 = spark.createDataFrame(rdd31)\nDataFrame(df8.dtypes, columns=['Column Name','Data type'])\ndf8.describe().toPandas()\ndf8.limit(3).toPandas()","ef426647":"df8Count = df8.groupBy(['mark','sport','gender']).count().orderBy(['mark','sport','gender'])\nrdd8Count = df8Count.rdd\ndf8Count.createOrReplaceTempView(\"table4\")\ndf8CountM = spark.sql(\"SELECT * from table4 where gender='male'\")\n#df8CountM.toPandas()\ndf8CountFM = spark.sql(\"SELECT * from table4 where gender='female'\")\n#df8CountFM.toPandas()\n\nsportTypeM0 = rdd8Count.filter(\n    lambda row: row['mark'] == 0 and row['gender'] == 'male'\n).map(lambda row: row['sport']).collect()\nsportTypeFM0 = rdd8Count.filter(\n    lambda row: row['mark'] == 0 and row['gender'] == 'female'\n).map(lambda row: row['sport']).collect()\ncountM0 = rdd8Count.filter(\n    lambda row: row['mark'] == 0 and row['gender'] == 'male'\n).map(lambda row: row['count']).collect()\ncountFM0 = rdd8Count.filter(\n    lambda row: row['mark'] == 0 and row['gender'] == 'female'\n).map(lambda row: row['count']).collect()\n\nsportTypeM1 = rdd8Count.filter(\n    lambda row: row['mark'] == 1 and row['gender'] == 'male'\n).map(lambda row: row['sport']).collect()\nsportTypeFM1 = rdd8Count.filter(\n    lambda row: row['mark'] == 1 and row['gender'] == 'female'\n).map(lambda row: row['sport']).collect()\ncountM1 = rdd8Count.filter(\n    lambda row: row['mark'] == 1 and row['gender'] == 'male'\n).map(lambda row: row['count']).collect()\ncountFM1 = rdd8Count.filter(\n    lambda row: row['mark'] == 1 and row['gender'] == 'female'\n).map(lambda row: row['count']).collect()\n\nsportTypeM2 = rdd8Count.filter(\n    lambda row: row['mark'] == 2 and row['gender'] == 'male'\n).map(lambda row: row['sport']).collect()\nsportTypeFM2 = rdd8Count.filter(\n    lambda row: row['mark'] == 2 and row['gender'] == 'female'\n).map(lambda row: row['sport']).collect()\n\ncountM2 = rdd8Count.filter(\n    lambda row: row['mark'] == 2 and row['gender'] == 'male'\n\n).map(lambda row: row['count']).collect()\ncountFM2 = rdd8Count.filter(\n    lambda row: row['mark'] == 2 and row['gender'] == 'female'\n).map(lambda row: row['count']).collect()\n\nsportTypeM3 = rdd8Count.filter(\n    lambda row: row['mark'] == 3 and row['gender'] == 'male'\n).map(lambda row: row['sport']).collect()\nsportTypeFM3 = rdd8Count.filter(\n    lambda row: row['mark'] == 3 and row['gender'] == 'female'\n).map(lambda row: row['sport']).collect()\ncountM3 = rdd8Count.filter(\n    lambda row: row['mark'] == 3 and row['gender'] == 'male'\n).map(lambda row: row['count']).collect()\ncountFM3 = rdd8Count.filter(\n    lambda row: row['mark'] == 3 and row['gender'] == 'female'\n).map(lambda row: row['count']).collect()","5deeeee7":"fig,axes = plt.subplots(4,2,figsize=(10,25))\nsubplot_adj = plt.subplots_adjust(wspace=0.3, hspace=0.7)\n\nfor i in range(8):\n    ax = plt.subplot(4, 2, i+1)\n    sca_x = plt.sca(ax)\n    xticks = plt.xticks(rotation=90)\n    xlabel = plt.xlabel('Sport')\n    ylabel = plt.ylabel('Workouts count')\n    if i%2 == 0:\n        title = plt.title('Time:' + str(int(i \/ 2 * 6)) + ' ~ ' + str(int(i \/ 2 * 6) + 6) + 'h___Male')\n        sportType = locals()['sportTypeM' + str(int(i \/ 2))]\n        count = locals()['countM' + str(int(i\/2))]\n        facecolor = '#9999ff'\n        plot = bar(sportType, count, facecolor=facecolor, edgecolor='white')\n    else:\n        title = plt.title(\n            'Time:' + str(int((i - 1) \/ 2 * 6)) + ' ~ ' + str(int((i - 1) \/ 2 * 6)+ 6) + 'h___Female'\n        )\n        sportType = locals()['sportTypeFM'+str(int(i\/2))]\n        count = locals()['countFM'+str(int(i\/2))]\n        facecolor = '#ff9999'\n        plot = bar(sportType, count, facecolor=facecolor, edgecolor='white')\na = fig.tight_layout()\nchart_title =fig.text(0.5, 1, 'Workout count per time range by sport & gender', \n            ha='center', va='top', fontsize='medium', transform=fig.transFigure)","f5608d61":"fig,axes = plt.subplots(4,2, figsize=(15, 35))\n\n\nfor i in range(8):\n    ax = plt.subplot(4, 2 , i + 1)\n    plt.sca(ax)\n    if i%2 == 0:\n        subplot_title = plt.title('Time:' + str(int(i\/2*6)) + ' ~ ' + str(int(i\/2 * 6) + 6) + 'h___Male ')\n        sportType = locals()['sportTypeM' + str(int(i\/2))]\n        count = locals()['countM' + str(int(i\/2))]\n        explode = tuple([0.1 for i in range(len(sportType))])\n        \n        facecolor = '#9999ff'\n        plot = plt.pie(x=count, autopct='%.2f%%'#, shadow=True\n                       , labels=sportType, explode=explode)\n    else:\n        subplot_title = plt.title('Time:' + str(int((i-1)\/2*6)) + ' ~ ' + str(int((i-1)\/2*6) + 6) +'h___Female ')\n        sportType = locals()['sportTypeFM' + str(int(i\/2))]\n        count = locals()['countFM'+str(int(i\/2))]\n        explode = tuple([0.1 for i in range(len(sportType))])\n        \n        facecolor = '#ff9999'\n        plot = plt.pie(x=count, labels=sportType#, shadow=True\n                       , autopct='%.2f%%', explode=explode)\n","b7207f89":"InteractiveShell.ast_node_interactivity = \"all\"\nrdd8 = df8.rdd\ndef avgReHeartRate(row):\n    if row['reHeartRate'] != '':\n        reAvgHeartRate = np.mean(row['reHeartRate'])\n        reVarHeartRate = np.var(row['reHeartRate'])\n        reAvgAltitude = np.mean(row['reAltitude'])\n        reVarAltitude = np.var(row['reAltitude'])\n        reAvgSpeed = np.mean(row['reSpeed'])\n        reVarSpeed = np.var(row['reSpeed'])\n        return Row(\n            id = int(row['id'])\n            , mark = row['mark']\n            , gender = str(row['gender'])\n            , sport = str(row['sport'])\n            , userId = int(row['userId'])\n            , reAvgHeartRate = float(reAvgHeartRate)\n            , reVarHeartRate = float(reVarHeartRate)\n            , reAvgAltitude = float(reAvgAltitude)\n            , reVarAltitude = float(reVarAltitude)\n            , reAvgSpeed = float(reAvgSpeed)\n            , reVarSpeed = float(reVarSpeed)\n        )\n   \nrdda8HR = rdd8.map(avgReHeartRate)\n\ndf9 = spark.createDataFrame(rdda8HR)\nDataFrame(df9.dtypes, columns = ['Column Name','Data type'])\n\n\ndf9AvgHearRate=df9.groupBy(['mark','sport','gender']) \\\n    .avg(\n        'reAvgHeartRate', 'reVarHeartRate'\n        , 'reAvgAltitude', 'reVarAltitude'\n        , 'reAvgSpeed', 'reVarSpeed'\n    ).orderBy(['mark','sport','gender'])\n\ndf9AvgHearRate.limit(5).toPandas()","981c7898":"rdd9AvgHearRate = df9AvgHearRate.rdd\nrddtimeM0 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 0 and row['gender'] == 'male')\nrddtimeM1 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 1 and row['gender'] == 'male')\nrddtimeM2 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 2 and row['gender'] == 'male')\nrddtimeM3 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 3 and row['gender'] == 'male')\nrddtimeFM0 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 0 and row['gender'] == 'female')\nrddtimeFM1 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 1 and row['gender'] == 'female')\nrddtimeFM2 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 2 and row['gender'] == 'female')\nrddtimeFM3 = rdd9AvgHearRate.filter(lambda row: row['mark'] == 3 and row['gender'] == 'female')\n\ndftimeM0 = spark.createDataFrame(rddtimeM0) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'MreAvgSptAvgHeartRate0') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'MreAvgSptVarHeartRate0') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'MreAvgSptAvgAltitude0') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'MreAvgSptVarAltitude0') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'MreAvgSptAvgSpeed0') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'MreAvgSptVarSpeed0')\n    \ndftimeM1 = spark.createDataFrame(rddtimeM1) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'MreAvgSptAvgHeartRate1') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'MreAvgSptVarHeartRate1') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'MreAvgSptAvgAltitude1') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'MreAvgSptVarAltitude1') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'MreAvgSptAvgSpeed1') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'MreAvgSptVarSpeed1')\n\ndftimeM2 = spark.createDataFrame(rddtimeM2) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'MreAvgSptAvgHeartRate2') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'MreAvgSptVarHeartRate2') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'MreAvgSptAvgAltitude2') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'MreAvgSptVarAltitude2') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'MreAvgSptAvgSpeed2') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'MreAvgSptVarSpeed2')\n\ndftimeM3 = spark.createDataFrame(rddtimeM3) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'MreAvgSptAvgHeartRate3') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'MreAvgSptVarHeartRate3') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'MreAvgSptAvgAltitude3') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'MreAvgSptVarAltitude3') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'MreAvgSptAvgSpeed3') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'MreAvgSptVarSpeed3')\n\ndftimeFM0 = spark.createDataFrame(rddtimeFM0) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'FMreAvgSptAvgHeartRate0') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'FMreAvgSptVarHeartRate0') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'FMreAvgSptAvgAltitude0') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'FMreAvgSptVarAltitude0') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'FMreAvgSptAvgSpeed0') \\\n    .withColumnRenamed('avg(reVarSpeed)','FMreAvgSptVarSpeed0')\n\ndftimeFM1 = spark.createDataFrame(rddtimeFM1) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'FMreAvgSptAvgHeartRate1') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'FMreAvgSptVarHeartRate1') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'FMreAvgSptAvgAltitude1') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'FMreAvgSptVarAltitude1') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'FMreAvgSptAvgSpeed1') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'FMreAvgSptVarSpeed1')\n\ndftimeFM2 = spark.createDataFrame(rddtimeFM2) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'FMreAvgSptAvgHeartRate2') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'FMreAvgSptVarHeartRate2') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'FMreAvgSptAvgAltitude2') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'FMreAvgSptVarAltitude2') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'FMreAvgSptAvgSpeed2') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'FMreAvgSptVarSpeed2')\n\ndftimeFM3 = spark.createDataFrame(rddtimeFM3) \\\n    .withColumnRenamed('avg(reAvgHeartRate)', 'FMreAvgSptAvgHeartRate3') \\\n    .withColumnRenamed('avg(reVarHeartRate)', 'FMreAvgSptVarHeartRate3') \\\n    .withColumnRenamed('avg(reAvgAltitude)', 'FMreAvgSptAvgAltitude3') \\\n    .withColumnRenamed('avg(reVarAltitude)', 'FMreAvgSptVarAltitude3') \\\n    .withColumnRenamed('avg(reAvgSpeed)', 'FMreAvgSptAvgSpeed3') \\\n    .withColumnRenamed('avg(reVarSpeed)', 'FMreAvgSptVarSpeed3')\n\ndftimeMAll = dftimeM0.join(dftimeM1, 'sport', 'outer') \\\n    .join(dftimeM2, 'sport', 'outer') \\\n    .join(dftimeM3, 'sport', 'outer')\ndftimeMAll.limit(3).toPandas()\ndftimeFMAll = dftimeFM0.join(dftimeFM1, 'sport', 'outer') \\\n    .join(dftimeFM2, 'sport', 'outer').join(dftimeFM3, 'sport', 'outer')\ndftimeFMAll.limit(3).toPandas()","25b78979":"fig, axes = plt.subplots(3, 2, figsize=(17, 27))\nplot = plt.subplots_adjust(wspace=0.3, hspace=0.5)\n\nbar_width = 0.2\ncolors = ['blue', 'lightgreen', 'yellow', 'grey']\n        \nfor j in range(6):\n    ax = plt.subplot(3, 2, j+1)\n    plt.sca(ax)\n    if int(j\/2) == 1:\n        nameStr1 = 'reAvgSptAvgAltitude'\n        nameStr3 = 'Altitude'\n        nameStr5 = 'altitude (m)'\n\n    elif int(j\/2) == 2:\n        nameStr1 = 'reAvgSptAvgSpeed'\n        nameStr3 = 'Speed'\n        nameStr5 = 'speed (MPH)'\n\n    elif int(j\/2) == 0:\n        nameStr1 = 'reAvgSptAvgHeartRate'\n        nameStr3 = 'HeartRate'\n        nameStr5 = 'heart rate (bpm)'        \n    if j % 2 == 0:\n        nameStr2 = 'M'\n        nameStr4 = 'Male'\n    else:\n        nameStr2 = 'FM'\n        nameStr4 = 'Female'\n    dftimeAll = locals()['dftime' + nameStr2 + 'All']\n    dftimeAllSportType = dftimeAll.rdd.map(lambda row: row['sport']).collect() \n    for k in range(4):\n        rowName = nameStr2 + nameStr1 + str(k)\n        dfTimeLi = dftimeAll.rdd.map(lambda row: row[rowName]).collect()\n        dfTimeLi = [0 if i == None else i for i in dfTimeLi]\n        plot = plt.bar(x=np.arange(len(dftimeAllSportType)) + k * bar_width\n                , height=dfTimeLi, label='Time:' + str(int(k*6)) + ' ~ ' + str(int(k*6)+6) + 'h'\n                , color=colors[k], alpha=0.8, width=bar_width)\n    xsticks = plt.xticks(np.arange(len(dftimeAllSportType)) + 1.5 * bar_width\n                         , dftimeAllSportType, rotation=90)\n    title = plt.title(\"Comparison of average \" + nameStr3 + \" between different periods for \" + nameStr4)\n    legend = plt.legend()\n    xlabel = plt.xlabel(\"Sport\")\n    ylabel = plt.ylabel(\"Average \" + nameStr5)\n","12707a62":"InteractiveShell.ast_node_interactivity = \"all\"\ndftimeMIN = dftimeM0.join(dftimeM1, 'sport', 'inner') \\\n    .join(dftimeM2, 'sport', 'inner') \\\n    .join(dftimeM3, 'sport', 'inner')\ndftimeMIN.toPandas()\ndftimeFMIN = dftimeFM0.join(dftimeFM1, 'sport', 'inner') \\\n    .join(dftimeFM2, 'sport', 'inner') \\\n    .join(dftimeFM3, 'sport', 'inner')\ndftimeFMIN.limit(5).toPandas()","00f5495c":"MaverageHeartRate = []\nMaverageAltitude = []\nMaverageSpeed = []\nfor i in range(4):\n    MaverageHeartRate.append(\n        mean(dftimeMIN.rdd.map(\n            lambda row: row['MreAvgSptAvgHeartRate' + str(i)]).collect()\n            )\n    )\n    \n    MaverageAltitude.append(\n        mean(dftimeMIN.rdd.map(\n            lambda row: row['MreAvgSptAvgAltitude' + str(i)]).collect()\n            )\n    )\n    MaverageSpeed.append(\n        mean(dftimeMIN.rdd.map(\n            lambda row: row['MreAvgSptAvgSpeed' + str(i)]).collect()))\n    \nFMaverageHeartRate = []\nFMaverageAltitude = []\nFMaverageSpeed = []\nfor i in range(4):\n    FMaverageHeartRate.append(\n        mean(dftimeFMIN.rdd.map(\n            lambda row: row['FMreAvgSptAvgHeartRate' + str(i)]).collect()\n            )\n    )\n    \n    FMaverageAltitude.append(\n        mean(dftimeFMIN.rdd.map(\n            lambda row: row['FMreAvgSptAvgAltitude' + str(i)]).collect()\n            )\n    )\n    FMaverageSpeed.append(mean(dftimeFMIN.rdd.map(lambda row: row['FMreAvgSptAvgSpeed' + str(i)]).collect()))","52e96b2e":"fig,axes = plt.subplots(2, 1,figsize=plt.figaspect(0.35))\nsubplot_adj = plt.subplots_adjust(wspace =0.3, hspace =0.5)\n\nbar_width = 0.2\ncolors = ['blue', 'lightgreen', 'yellow']\ntimesP = ['0~6', '6~12', '12~18', '18~24']\n\nfor j in range(2):\n    ax = plt.subplot(1, 2, j + 1)\n    plotsca = plt.sca(ax)       \n    if j % 2 == 0:\n        nameStr2 = 'M'\n        nameStr4 = 'Male'\n    else:\n        nameStr2 = 'FM'\n        nameStr4 = 'Female'    \n    for k in range(3):\n        if k == 0:\n            nameStr6 = 'averageHeartRate'\n            nameStr7 = 'average heartrate (bpm)'\n        elif k == 1:\n            nameStr6 = 'averageAltitude'\n            nameStr7 = 'average altitude (m)'\n        elif k == 2:\n            nameStr6 = 'averageSpeed'\n            nameStr7 = 'average speed (MPH)'\n        showY = locals()[nameStr2 + nameStr6]\n        plot = plt.bar(\n            x=np.arange(len(timesP)) + k * bar_width\n            , height=showY, label=nameStr7, color=colors[k]\n            , alpha=0.8, width=bar_width\n        )\n    xsticks = plt.xticks(np.arange(len(timesP))+ 0.7 * bar_width, timesP)\n    title = plt.title(\"Average values for different period for \" + nameStr4)\n    legend = plt.legend()\n    xlabel = plt.xlabel(\"Period (hour)\")\n    ylabel = plt.ylabel(\"Average measurement\")","a3bb19ab":"InteractiveShell.ast_node_interactivity = \"all\"\nrdd10 = df.rdd\n\n# same_day function already defined in the previous section.\nrddSd10=rdd10.filter(lambda row: row['speed'] is not None).map(same_day) \\\n    .filter(\n    lambda row: row['year'] == row['yearl'] and row['month'] == row['monthl'] and row['day'] == row['dayl'])\ndfSd10=spark.createDataFrame(rddSd10) \\\n    .drop('year').drop('yearl').drop('month').drop('day').drop('monthl').drop('dayl')\n\ndef markWorkout1(row):\n    hours = [(datetime.fromtimestamp(t) - timedelta(hours=7)).hour for t in row['timestamp']]\n    mark = -1\n    upIndex = -1\n    if hours[0] >= 6 and hours[0] < 12:\n        mark = 1\n    elif hours[0] >= 12 and hours[0] < 18:\n        mark=2\n    elif hours[0] >= 18 and hours[0] < 24:\n        mark = 3\n    elif hours[0] >= 0 and hours[0] < 6:\n        mark = 0\n    if mark != -1:\n        perAvgHeatRate = mean(np.array(row['heartrate']))\n        perAverageAltitude = mean(np.array(row['altitude']))\n        perAverageSpeed = mean(np.array(row['speed']))       \n        return Row(\n            id = row['id'] \\\n            , sport = str(row['sport']) \\\n            , userId = row['userId'] \\\n            , gender = row['gender'] \\\n            , mark = mark \\\n            , perAvgHeatRate = float(perAvgHeatRate) \\\n            , perAverageAltitude = float(perAverageAltitude) \\\n            , perAverageSpeed = float(perAverageSpeed)\n        )\n\n    \nrddMAvg10 = dfSd10.rdd.map(markWorkout1)\ndfMAvg10 = spark.createDataFrame(rddMAvg10)\nprint('Data set preparation:')\nDataFrame(dfMAvg10.dtypes, columns = ['Column Name','Data type'])\ndfMAvg10.describe().toPandas()\ndfMAvg10.limit(3).toPandas()","1be92cc1":"print('Aggregate values:')\ndftmp=dfMAvg10.groupBy(['userId', 'sport']) \\\n    .agg({'sport':'count', 'perAverageAltitude':'mean'\n            , 'perAverageSpeed':'mean', 'perAvgHeatRate':'mean'})\ndftmp.describe().toPandas()\ndftmp.limit(3).toPandas()\ndf10MC=dfMAvg10.groupBy(['userId', 'mark']).count()\ndf10MC.describe().toPandas()\ndf10MC.limit(3).toPandas()","21fcadbd":"InteractiveShell.ast_node_interactivity = \"all\"\nall_marks = [0, 1, 2, 3]\ndef mark_count(valueN): \n    '''\n    valueN: tuple of (mark, count)\n    '''\n    markL = []\n    dic = {}\n    for item in valueN:\n        markL.append(item[0])\n    # Find the list of marks not belong to the current mark list in valueN \n    li = list(set(all_marks).difference(set(markL)))\n    \n    # Update count for the existed marks, and assign non-existed marks with 0\n    for item in valueN:\n        dic[item[0]] = item[1]\n    for m in li:\n        dic[m] = 0    \n    dicSt = sorted(dic.items(), key=lambda d:d[0])\n    markL = [value for key,value in dicSt]   \n    return markL\n\n# Generate the list of counts per hour mark for each userId\nrdd10MCTF = df10MC.rdd.map(\n    lambda row: (row['userId'], (row['mark'], row['count']))\n).groupByKey().mapValues(mark_count) \\\n    .map(lambda row: Row(userId=row[0], markCt=row[1]))\n\ndf10MCTF = spark.createDataFrame(rdd10MCTF)\n\nall_sports = sorted(\n    dfMAvg10[['sport']].distinct().rdd.map(lambda row: row['sport']).collect()\n)\n\n# Generate workout count,  average speed, heart rate & altitude vectors for each user\ndef f1(valueN):\n    sportDic = {}\n    averageSpeed = {}\n    averageHeartRate = {}\n    averageAltitude = {}\n    \n    sportType = []\n    for item in valueN:\n        sportType.append(item[0])\n    li = list(set(all_sports).difference(set(sportType)))\n        \n    for item in valueN:\n        sportDic[item[0]] = item[4]\n        averageSpeed[item[0]] = round(item[1])\n        averageHeartRate[item[0]] = round(item[2])\n        averageAltitude[item[0]] = round(item[3])\n    \n    for sp in li:\n        sportDic[sp] = 0\n        averageSpeed[sp] = 0\n        averageHeartRate[sp] = 0\n        averageAltitude[sp] = 0\n    \n    sportDicSt=sorted(sportDic.items(), key=lambda d:d[0])\n    sportL=[value for key,value in sportDicSt]\n    averageSpeedSt=sorted(averageSpeed.items(), key=lambda d:d[0])\n    averageSpeedL=[value for key,value in averageSpeedSt]\n    averageHeartRateSt=sorted(averageHeartRate.items(), key=lambda d:d[0])\n    averageHeartRateL=[value for key,value in averageHeartRateSt]\n    averageAltitudeSt=sorted(averageAltitude.items(), key=lambda d:d[0])\n    averageAltitudeL=[value for key,value in averageAltitudeSt]\n    \n    return sportL, averageSpeedL, averageHeartRateL, averageAltitudeL  \n    \nrdd10AvgTF = dftmp.rdd.map(\n    lambda row: (\n        row['userId'], (\n                        row['sport'], row['avg(perAverageSpeed)']\n                        , row['avg(perAvgHeatRate)'], row['avg(perAverageAltitude)']\n                        , row['count(sport)']\n                        )\n                )\n).groupByKey().mapValues(f1).map(\n                    lambda row: Row(userId=row[0], sportCt=row[1][0], averageSpeed=row[1][1]\n                    , averageHeartRate=row[1][2], averageAltitude=row[1][3])\n)\n\ndf10AvgTF = spark.createDataFrame(rdd10AvgTF)\n\n\nrddgender = dfMAvg10[['userId','gender']].distinct().rdd \\\n    .filter(lambda row: row['gender'] == 'male' or row['gender'] == 'female') \\\n    .map(\n        lambda row: Row(userId = row['userId'], gender = 0) if row['gender'] == 'male' \\\n        else Row(userId=row['userId'], gender = 1)\n    )\ndfgender = spark.createDataFrame(rddgender)\ndf10GMCAvgTF = df10MCTF.join(df10AvgTF, 'userId', 'inner').join(dfgender,'userId', 'inner')\ndf10GMCAvgTF.describe().toPandas()\nprint('Final co-ordinate vectors for users, take 3:')\ndf10GMCAvgTF.limit(3).toPandas()","76700e60":"def marginM(row):\n    maxMarkC = max(row['markCt'])\n    maxAverageAltitude = max(row['averageAltitude'])\n    maxAverageHeartRate = max(row['averageHeartRate'])\n    maxAverageSpeed = max(row['averageSpeed'])\n    maxSportC = max(row['sportCt'])\n    return Row(\n        maxMarkC=maxMarkC, maxAverageAltitude=maxAverageAltitude\n        , maxAverageHeartRate=maxAverageHeartRate, maxAverageSpeed=maxAverageSpeed\n        , maxSportC=maxSportC\n    )  \n\nmaxRecord = spark.createDataFrame(\n    df10GMCAvgTF.rdd.map(marginM)\n).groupBy().max(\n    'maxMarkC', 'maxAverageAltitude', 'maxAverageHeartRate', 'maxAverageSpeed', 'maxSportC'\n    ).rdd.map(lambda row: (row[0], row[1], row[2], row[3], row[4])).collect()\n\nmaxMarkC = maxRecord[0][0]\nmaxAverageAltitude = maxRecord[0][1]\nmaxAverageHeartRate = maxRecord[0][2]\nmaxAverageSpeed = maxRecord[0][3]\nmaxSportC = maxRecord[0][4]\n\n# Define scaling factors to calculate distance for each feature later\nmarkCM = round(maxAverageAltitude \/ maxMarkC)\nAverageHeartRateM = round(maxAverageAltitude \/ maxAverageHeartRate)\nAverageSpeedM = round(maxAverageAltitude \/ maxAverageSpeed)\nSportCM = round(maxAverageAltitude \/ maxSportC)\ngenderM = maxAverageAltitude","428f69be":"import random\nrandom.seed(50)\nrdd10GMCAvgTF = df10GMCAvgTF.rdd\nuserId = rdd10GMCAvgTF.map(lambda row: row['userId']).collect()\ninitUsers = random.sample(userId, 5)\ncentroids = rdd10GMCAvgTF.filter(lambda row: row['userId'] in initUsers).collect()","74491d95":"# Apply different weights for different features\nweightMark = 1\nweightAltitude = 1\nweightHeartRate = 5\nweightSpeed = 1.2\nweightSport = 1\nweightGender = 3\n\n# Function to assign user to the closest centroid the first time\ndef assigCent(row):\n    distDic = {}\n    for centroid in centroids:\n        # calculate distances:\n        disMark = np.sum(\n            np.square(np.array(row['markCt']) - np.array(centroid['markCt']))\n        )\n        disAltitude = np.sum(\n            np.square(np.array(row['averageAltitude']) - np.array(centroid['averageAltitude']))\n        )\n        disHeartRate = np.sum(\n            np.square(np.array(row['averageHeartRate']) - np.array(centroid['averageHeartRate']))\n        )\n        disSpeed = np.sum(\n            np.square(np.array(row['averageSpeed']) - np.array(centroid['averageSpeed']))\n        )\n        disSport = np.sum(\n            np.square(np.array(row['sportCt']) - np.array(centroid['sportCt']))\n        )\n        disGender = np.sum(np.square(np.array(row['gender']) - np.array(centroid['gender'])))\n        # Calculate final distances based on weighted value per feature defined above:\n        distDic[centroid['userId']] = weightMark * markCM * disMark \\\n            + weightAltitude * disAltitude \\\n            + weightHeartRate * AverageHeartRateM * disHeartRate \\\n            + weightSpeed * AverageSpeedM * disSpeed \\\n            + weightSport * SportCM * disSport \\\n            + weightGender * genderM * disGender\n        \n    selectedCentroid = min(distDic, key = distDic.get)\n    return selectedCentroid\n\nassignedGroup = rdd10GMCAvgTF.map(\n    lambda row: (assigCent(row), row['userId'])\n).groupByKey().mapValues(list).map(lambda row: row[1]).collect()\n\nfirstAssignGroup = assignedGroup\n\ndef cluster_summarize_df(assignedGroup):\n    display_list = []\n    for index, centroid in enumerate(assignedGroup):\n          display_list.append({'Group': index, 'Users count': len(centroid)})\n    display_df = pd.DataFrame(display_list)\n    return display_df\n\nprint('After initializing centroid:')\ncluster_summarize_df(firstAssignGroup)","fc790674":"# Function to re-calculate co-ordinates for new centroids\ndef new_coordinates(bList):\n    countSum = []\n    for i in range(len(bList)):\n        if i == 0:\n            countSum = np.array(bList[i])\n        else:\n            countSum += np.array(bList[i])\n    return countSum \/ len(bList)\n\n# Function to assign users to clusters from second time\ndef assigLCent(row):\n    distDic = {}\n    for key, value in centroids.items():\n        disMark = np.sum(\n            np.square(np.array(row['markCt']) - np.array(value['markCt']))\n        )\n        disAltitude = np.sum(\n            np.square(np.array(row['averageAltitude']) - np.array(value['averageAltitude']))\n        )\n        disHeartRate = np.sum(\n            np.square(np.array(row['averageHeartRate']) - np.array(value['averageHeartRate']))\n        )\n        disSpeed = np.sum(\n            np.square(np.array(row['averageSpeed']) - np.array(value['averageSpeed']))\n        )\n        disSport = np.sum(\n            np.square(np.array(row['sportCt']) - np.array(value['sportCt']))\n        )\n        disGender = np.sum(\n            np.square(np.array(row['gender']) - np.array(value['gender']))\n        )\n        distDic[key] = weightMark * markCM * disMark \\\n            + weightAltitude * disAltitude \\\n            + weightHeartRate * AverageHeartRateM * disHeartRate \\\n            + weightSpeed * AverageSpeedM * disSpeed \\\n            + weightSport * SportCM * disSport \\\n            + weightGender * genderM * disGender\n    selectedCentroid = min(distDic, key=distDic.get)\n    return selectedCentroid\n\n# Function to compare 2 clusters\ndef compare(r1, r2):\n    if len(r1) != len(r2):\n        return False\n    for i in range(len(r1)):\n        list_found = False\n        for j in range(len(r2)):\n            res_list = r1[i]\n            a_list = r2[j]            \n            if res_list == a_list:\n                list_found = True                  \n                break\n        if not list_found:\n            return False\n    return True   \n\n\ndef sortedgroup(groups):\n    sortedG = []\n    for group in groups:\n        sortedG.append(sorted(group))\n    return sortedG\n        \ntimes = 0\nmaxIter = 20\ncompResult = False\n\n# Converge \/ loop break condition\nwhile compResult == False and times <= maxIter:\n    \n    newCentroid={}\n    i = 0\n    # Re-calculate co-ordinates for new centroids\n    for group in assignedGroup:\n        markList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['markCt']).collect()\n        )\n        \n        altitudeList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['averageAltitude']).collect()\n        )\n        \n        heartRateList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['averageHeartRate']).collect()\n        )\n        \n        speedList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['averageSpeed']).collect()\n        )\n        sportList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['sportCt']).collect()\n        )\n        \n        genderList = new_coordinates(\n            rdd10GMCAvgTF.filter(\n                lambda row: row['userId'] in group\n            ).map(lambda row: row['gender']).collect()\n        )\n        \n        newCentroid[i] = {\n            'markCt':markList, 'averageAltitude':altitudeList\n            , 'averageHeartRate':heartRateList, 'averageSpeed':speedList\n            , 'sportCt':sportList, 'gender':genderList\n        }\n        \n        i += 1\n\n    centroids = newCentroid \n    \n    # Assign users to new groups\n    newAssignedGroup = rdd10GMCAvgTF.map(\n        lambda row: (assigLCent(row), row['userId'])\n    ).groupByKey().mapValues(list).map(lambda row: row[1]).collect()\n    \n    print('\\nIteration #{} - Group summary:'.format(times))\n    cluster_summarize_df(newAssignedGroup)\n    \n    # Check if new clusters are the same with the preivous iteration\n    compResult = compare(sortedgroup(assignedGroup), sortedgroup(newAssignedGroup))\n    \n    if compResult == True or times == maxIter:\n        print(\"---------\")\n        print('Clusters converged after {} iterations'.format(times + 1))\n        \n    assignedGroup=newAssignedGroup\n        \n    times+=1\n    ","8f1d2aa6":"InteractiveShell.ast_node_interactivity = \"all\"\n\ndef reRow(li):\n    count = 0\n    sumV = 0\n    for i in li:\n        if i != 0:\n            sumV += i\n            count += 1\n    if count != 0:\n        value = sumV \/ count\n        return value\n    else:\n        return 0        \n\ndef countAverage(row):\n    avgAltitudeSP = reRow(row['averageAltitude'])\n    avgHeartRateSP = reRow(row['averageHeartRate'])\n    avgSpeedSP = reRow(row['averageSpeed'])\n    return Row(\n        useId = row['userId'], markCt = row['markCt']\n        , avgAltitudeSP = float(avgAltitudeSP), avgHeartRateSP = float(avgHeartRateSP)\n        , avgSpeedSP = float(avgSpeedSP), sportCt = row['sportCt'], gender=row['gender']\n    )\n    \ntemprdds = []\nfor group in firstAssignGroup:\n    temprdds.append(\n        spark.createDataFrame(\n            rdd10GMCAvgTF.filter(lambda row: row['userId'] in group).map(countAverage)\n        )\n    )\nprint('Summary for first assigned clusters:')\nfor i, temprdd in enumerate(temprdds):\n    print('\\nGroup #{}:'.format(i))\n    temprdd.toPandas().describe()\n\nfor group in assignedGroup:\n    temprdds.append(\n        spark.createDataFrame(\n            rdd10GMCAvgTF.filter(lambda row: row['userId'] in group).map(countAverage)\n        )\n    )\nprint('Summary for final clusters:')    \nfor i, temprdd in enumerate(temprdds[5:]):\n    print('\\nGroup #{}:'.format(i))\n    temprdd.toPandas().describe()\n\n","8f80efb6":"fig, axes = plt.subplots(1, 2, figsize=plt.figaspect(0.3))\nsubplot_adj = plt.subplots_adjust(wspace =0.3, hspace =0.5)\n\nbar_width = 0.2\ncolor = ['blue', 'lightgreen', 'yellow']\nxShow = ['Group {}'.format(i+1) for i in range(5)]\n\navgAltitude = []\navgHeartrate = []\navgSpeed = []\navgAltitude1 = []\navgHeartrate1 = []\navgSpeed1 = []\nfor i in range(len(temprdds)):\n    if i < 5:\n        avgAltitude.append(\n            np.mean(\n                np.array(temprdds[i].rdd.map(lambda row: row['avgAltitudeSP']).collect())\n            )\n        )\n        \n        avgHeartrate.append(\n            np.mean(np.array(temprdds[i].rdd.map(lambda row: row['avgHeartRateSP']).collect())\n                   )\n        )\n        \n        avgSpeed.append(\n            np.mean(np.array(temprdds[i].rdd.map(lambda row: row['avgSpeedSP']).collect())\n                   )\n        )\n    else:\n        avgAltitude1.append(\n            np.mean(np.array(temprdds[i].rdd.map(lambda row: row['avgAltitudeSP']).collect())\n                   )\n        )\n        avgHeartrate1.append(\n            np.mean(np.array(temprdds[i].rdd.map(lambda row: row['avgHeartRateSP']).collect())\n                   )\n        )\n        avgSpeed1.append(\n            np.mean(np.array(temprdds[i].rdd.map(lambda row: row['avgSpeedSP']).collect())\n                   )\n        )\nax = plt.subplot(1,2,1)\nsca = plt.sca(ax)    \nplot = plt.bar(\n    x=np.arange(len(xShow)), height=avgHeartrate\n    , label='Heart rate (bpm)', color=color[0], alpha=0.8, width=bar_width\n)\n\nplot = plt.bar(\n    x=np.arange(len(xShow)) + 1 * bar_width, height=avgAltitude\n    , label='Altitude (m)', color=color[1], alpha=0.8, width=bar_width\n)\nplot = plt.bar(\n    x=np.arange(len(xShow)) + 2 * bar_width, height=avgSpeed\n    , label='Speed (mph)', color=color[2], alpha=0.8, width=bar_width\n)\nxsticks = plt.xticks(np.arange(len(xShow)) + 0.7 * bar_width, xShow, rotation=90)\ntitle = plt.title(\"First users assignment \/ classification\")\nlegend = plt.legend()\nxlabel = plt.xlabel(\"User Group\")\nylabel = plt.ylabel(\"Average measurement\")\n\nax=plt.subplot(1,2,2)\nsca = plt.sca(ax) \nplot = plt.bar(\n    x=np.arange(len(xShow)), height=avgHeartrate1\n    , label='Heart rate (bpm)', color=color[0], alpha=0.8, width=bar_width\n)\nplot = plt.bar(\n    x=np.arange(len(xShow)) + 1 * bar_width, height=avgAltitude1\n    , label='Altitude (m)', color=color[1], alpha=0.8, width=bar_width\n)\nplot = plt.bar(\n    x=np.arange(len(xShow)) + 2 * bar_width, height=avgSpeed1\n    , label='Speed (mph)', color=color[2], alpha=0.8, width=bar_width\n)\nxsticks = plt.xticks(np.arange(len(xShow)) + 0.7 * bar_width, xShow, rotation=90)\ntitle = plt.title(\"Classification result after {} times\".format(times))\nlegend = plt.legend()\nxlabel = plt.xlabel(\"User Group\")\nylabel = plt.ylabel(\"Average measurement\")\nfig_title = fig.text(\n    0.5, 1.01, 'Comparison between first and final iteration of k-means clustering (k = 5)'\n    , ha='center', va='top', transform=fig.transFigure, fontsize='medium'\n)","828ef435":"### C.3) K-means result:\nPrepare data for plotting:","d4cd293f":"Hi everyone,  \n\nIn this notebook I will do the <font color=\"red\"><b> statistical inferences<\/b><\/font> to the dataset which we have already processed at the previous notebook ( Pyspark for EDA ).\nStatistical inferences is one of the most important task in any data science project.\n\nI hope you enjoy the notebook,\n\nThank you","0be340a4":"This plot showed that in most of the sports having both genders participated, average heart rates of female are higher than male's.","22d78512":"### B.3) Choice of sports in different periods of a day between male and female\nWe group the workouts into 4 groups base on started hour and try to see which kinds of sport people prefer at a specific time range of the day.","fd55e06d":"\n## B. Statistical Inferences\n\n### B.1) Heat rate difference between different genders and sports\n\nWe take average of heart rate per workout and compare the different between male, female per sport. ","12512bde":"As we can see once again, the percentage in the pie charts show the dorminance of bike and run compared to other sport. This analysis is quite similar to one in the Exploratory Notebook, just in a different view angle :3","5fed1cc7":"#### Prepare the cordinates for each userId before starting k-mean implementation. \nUsers' coordinates (pre-scaling) will be built based on:\n- Marks (hour period) count per period by 4 marks (0, 1, 2, 3)\n- Gender: 0 (male) or 1 (female)\n- Average speed, heart rate, altitude per workout, per sport to create vectors of n sports (dimentions)","7c8defec":"For average heart rate, there is no much difference within a sport among different time periods of a day, but the overall difference bewtween sports can be observed.   \nNext, find the overall average measurements per time period across all sports:","e5c07afc":"## Features standardization\nSince all of the features are at different scales, we need to apply standardization on them before measuring distances by apply scaling factors for all features by max altitude.","0275374f":"### B.4) Average heartrate, altitude, speed change for each sport during different periods of the day","e967fa80":"From the plots, it can be seen that bike and run are the dominant activities in term of participation, regardless of time of the day and gender. Another observation is that based on the scale of the y-axes, males exercised much more than female in all period of times.","c4bf0cd4":"### C.2) K-means Implementation\n#### - Initialize centroids:\nStart initialize central centroids with k = 5. There are some assumptions on different impacts of hour period, altitude, heart rate, speed, workout count and gender so weights are applied.","ab504a84":"Look at the overall statistic and first 2 data rows:","a463ae6d":"Again, for overall regardless of the sports, average speed and heart rate are not different among time periods.","37f692f0":"The 2 charts shows that for both males and females, the average correlation between heart rate and altitude vs heart rate and speed are not so much different.","2b614078":"## A. Overview about data set size and summary","102e3b50":"Thank you for reading my work","b49589c7":"#### - Next iterations","9cf3beae":"### B.2) Correlations of heart rate vs. speed, heart rate vs. altitude\n\nWe calculate the Pearson Correlation Coefficients between heart rate vs altitude, heart rate vs speed, the the average of those coefficients by sport and gender, then take the difference of `average_coe(heart rate, altitude)` and `average_coe(heart rate, speed)`.we only look at workouts with non-empty speed values.\n\nYou can visit the https:\/\/www.statisticshowto.com\/probability-and-statistics\/correlation-coefficient-formula\/ to deep into more detail of Correlation concept","3409a445":"#### - Assign users  to their closest centroids:","b2be8aae":"## C. Classification - Finding similar users\nWe will apply k-means clustering technique to group similar users, based on users' gender, workout counts per period, sports count, average heart rate, average speed and average altitude per workout.\n\n### C.1) Data preparation","41c903d8":"#### - Plot results for first and final clusters iterations:\nWe will plot mean measurements of the groups \/ clusters to plot and compare between first and final iterations. ","d323d972":"Columns of the data set:"}}