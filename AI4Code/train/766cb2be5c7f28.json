{"cell_type":{"315a581d":"code","20a22efe":"code","89ee4518":"code","28857f88":"code","7c72d347":"code","68b77d26":"code","cb6fc1cd":"code","6068aa4a":"code","88be6559":"code","31482df8":"code","082baa67":"code","9a076ffe":"code","1f53cb86":"code","08ec88eb":"code","f6dca4c6":"code","6781ff3e":"code","bd2dbec6":"code","9d8dd0aa":"code","50decd59":"code","5945465a":"code","f2b31f94":"code","7f87464f":"code","3d0472c4":"code","b120651d":"code","30a3a682":"code","d4dd6ad7":"code","05954c3f":"code","c8559cf4":"code","4b91c3a4":"code","18be4a0b":"code","66d9fc3f":"code","f5c00ee3":"code","c0bcd08b":"code","17fb0381":"code","1a60caf6":"code","b07c9d0a":"code","fcfa20b1":"markdown","fc7248f0":"markdown","433ca035":"markdown","7fe6a22f":"markdown","48e1bad4":"markdown","52cabc94":"markdown","ee855acb":"markdown","265bb60a":"markdown","ad366214":"markdown","a64936a6":"markdown","3eb2ef69":"markdown","8a320f40":"markdown","250efe02":"markdown","9cb68c3c":"markdown","898c6d89":"markdown","4d8f1f10":"markdown"},"source":{"315a581d":"\nimport os\nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nfrom transformers import AutoTokenizer\nfrom transformers import TFAutoModel\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow.keras.backend as K\nimport logging\n","20a22efe":"config = {\n    'seed' : 42,\n    'model': '..\/input\/murilbertcased',\n    'group': 'MURIL',\n    \n    'batch_size': 16,\n    'max_length': 384,\n    'doc_stride': 128,\n    \n    'device' : 'GPU',\n    'epochs' : 10,\n    'n_best_scores': 20,\n    'max_answer_length': 75,\n    'test_size' : 0.15,\n    'lr': 3e-5,\n    'use_transfer_learning' : False,\n    \n    'use_wandb': True,\n    'wandb_mode' : 'offline',\n}","89ee4518":"def seed_everything(seed = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \n# Creating a logger \ud83d\udcc3\ndef init_logger(log_file:str ='training.log'):\n    \n    # Specify the format \n    formatter = logging.Formatter('%(levelname)s:%(name)s:%(message)s')\n    \n    # Create a StreamHandler Instance\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.DEBUG)\n    stream_handler.setFormatter(formatter)\n    \n    # Create a FileHandler Instance\n    file_handler = logging.FileHandler(log_file)\n    file_handler.setFormatter(formatter)\n    \n    # Create a logging.Logger Instance\n    logger = logging.getLogger('Chaii-MURIL')\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n    \n    return logger\n\n\nLOGGER = init_logger()\nLOGGER.info(\"Logger Initialized\")\n\nseed_everything()\nLOGGER.info(\"Seed Setting done\")\n","28857f88":"# To make this work with TPU\n\ndef get_device(device):\n    if device == 'TPU':\n        try: # detect TPUs\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n            strategy = tf.distribute.TPUStrategy(tpu)\n        except ValueError: # detect GPUs\n            print('Cannot initialize TPU')\n    if device == 'GPU':\n        strategy = tf.distribute.MirroredStrategy() \n\n    print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    return strategy\n\nstrategy= get_device(config['device'])\nconfig['batch_size'] = config['batch_size'] * strategy.num_replicas_in_sync\n\nLOGGER.info(\"Effective batch size is \" + str(config['batch_size']))\n","7c72d347":"#Intialize wandb run\n\nif config['use_wandb']:\n    import wandb\n    from wandb.keras import WandbCallback\n    from kaggle_secrets import UserSecretsClient\n    os.environ[\"WANDB_MODE\"] = \"offline\"\n\n    if config['wandb_mode'] == 'offline':\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n        key='X'*40\n        wandb.login(key=key)\n    else:\n        user_secrets = UserSecretsClient()\n        wandb_api = user_secrets.get_secret(\"wandb_api\")\n        wandb.login(key=wandb_api)\n\n    run = wandb.init(project='chaii', \n                     group =config['group'], \n                     job_type='train',\n                     config = config)\n\n    LOGGER.info(\"Wandb is initialized\")\n","68b77d26":"df = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv', encoding='utf-8')\ndf.head()","cb6fc1cd":"df.language.value_counts(normalize=True)*100","6068aa4a":"from sklearn.model_selection import train_test_split\n\ndf_train, df_valid = train_test_split(df, \n                                      shuffle=True, \n                                      random_state=config['seed'], \n                                      test_size=config['test_size'], \n                                      stratify=df['language'].values)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\ndf_train.to_csv('df_train.csv', index=False)\ndf_valid.to_csv('df_valid.csv', index=False)\n\nLOGGER.info(\"Data read from disk\")","88be6559":"if config['use_wandb']:\n    artifact =  wandb.Artifact(name=\"folds\", type=\"dataset\")\n    artifact.add_file('.\/df_train.csv')\n    artifact.add_file('.\/df_valid.csv')\n\n    LOGGER.info(\"Logging folds.csv to W&B Artifacts\")\n    wandb.log_artifact(artifact)\n","31482df8":"tokenizer = AutoTokenizer.from_pretrained(config['model'])\nprint(tokenizer)\n\nLOGGER.info('Tokenizer loaded')","082baa67":"AUTOTUNE = tf.data.experimental.AUTOTUNE","9a076ffe":"\nclass ChaiiDataset:\n    def __init__(self, max_length, stride, tokenizer):\n        self.max_length = max_length\n        self.doc_stride = stride\n        self.pad_on_right = tokenizer.padding_side == \"right\"\n        self.tokenizer = tokenizer\n    \n\n    def run_tokenizer(self, data):\n        tokenized_data = self.tokenizer(\n        list(data[\"question\" if self.pad_on_right else \"context\"].values),\n        list(data[\"context\" if self.pad_on_right else \"question\"].values),\n        truncation=\"only_second\" if self.pad_on_right else \"only_first\",\n        max_length=self.max_length,\n        stride=self.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n        )\n\n        return tokenized_data\n\n    def prepare_train_features(self, train_data):\n        tokenized_train_data = self.run_tokenizer(train_data)\n\n        sample_mapping = tokenized_train_data.pop(\"overflow_to_sample_mapping\")\n        offset_mapping = tokenized_train_data.pop(\"offset_mapping\")\n\n        tokenized_train_data[\"start_positions\"] = []\n        tokenized_train_data[\"end_positions\"] = []\n\n\n        for i, offsets in tqdm(enumerate(offset_mapping)):\n            # We will label impossible answers\n            # with the index of the CLS token.\n            input_ids = tokenized_train_data[\"input_ids\"][i]\n            cls_index = input_ids.index(self.tokenizer.cls_token_id)\n\n            # Grab the sequence corresponding \n            # to that example (to know what is \n            # the context and what is the question).\n            sequence_ids = tokenized_train_data.sequence_ids(i)\n\n            # One example can give \n            # several spans, this is the \n            # index of the example containing \n            # this span of text.\n            \n            sample_index = sample_mapping[i]\n            answers = list(train_data[\"answer_text\"].values)[sample_index]\n            \n            # If no answers are given,\n            # set the cls_index as answer.\n            if len(train_data[\"answer_start\"]) == 0:\n                tokenized_train_data[\"start_positions\"].append(cls_index)\n                tokenized_train_data[\"end_positions\"].append(cls_index)\n            else:\n                # Start\/end character index \n                # of the answer in the text.\n                start_char = list(train_data[\"answer_start\"].values)[sample_index]\n                end_char = start_char + len(answers)\n                \n                # Start token index of the \n                # current span in the text.\n                token_start_index = 0\n                while sequence_ids[token_start_index] != (1 if self.pad_on_right else 0):\n                    token_start_index += 1\n\n                # End token index of the \n                # current span in the text.\n                token_end_index = len(input_ids) - 1\n                while sequence_ids[token_end_index] != (1 if self.pad_on_right else 0):\n                    token_end_index -= 1\n\n                # Detect if the answer is out of span \n                # (in which case this \n                # feature is labeled with the CLS index).\n                if not (offsets[token_start_index][0] <= start_char \n                        and offsets[token_end_index][1] >= end_char):\n                    tokenized_train_data[\"start_positions\"].append(cls_index)\n                    tokenized_train_data[\"end_positions\"].append(cls_index)\n                else:\n                    # Otherwise move the token_start_index \n                    # and token_end_index to the two ends of the answer.\n                    # Note: we could go after the last offset \n                    # if the answer is the last word (edge case).\n                    while token_start_index < len(offsets) \\\n                        and offsets[token_start_index][0] <= start_char:\n                        token_start_index += 1\n                    tokenized_train_data[\"start_positions\"].append(token_start_index - 1)\n                    while offsets[token_end_index][1] >= end_char:\n                        token_end_index -= 1\n                    tokenized_train_data[\"end_positions\"].append(token_end_index + 1)\n\n        return tokenized_train_data\n\n\n    def prepare_eval_features(self, eval_data):\n        tokenized_eval_data = self.run_tokenizer(eval_data)\n        sample_mapping = tokenized_eval_data.pop(\"overflow_to_sample_mapping\")\n\n        # We keep the example_id that gave \n        # us this feature and we will \n        # store the offset mappings.\n        tokenized_eval_data[\"example_id\"] = []\n\n        for i in tqdm(range(len(tokenized_eval_data[\"input_ids\"]))):\n            # Grab the sequence corresponding to that \n            # example (to know what is the context \n            # and what is the question).\n            sequence_ids = tokenized_eval_data.sequence_ids(i)\n            context_index = 1 if self.pad_on_right else 0\n\n            # One example can give several spans,\n            # this is the index of the example \n            # containing this span of text.\n            sample_index = sample_mapping[i]\n            tokenized_eval_data[\"example_id\"].append(eval_data[\"id\"].values[sample_index])\n\n            # Set to None the offset_mapping \n            # that are not part of the context \n            # so it's easy to determine if a token\n            # position is part of the context or not.\n            tokenized_eval_data[\"offset_mapping\"][i] = [\n                (o if sequence_ids[k] == context_index else None)\n                for k, o in enumerate(tokenized_eval_data[\"offset_mapping\"][i])\n            ]\n\n        return tokenized_eval_data\n\n    def prepare_tf_data_pipeline(self, data, batch_size = 16, type='train'):\n        \n        if type=='train':\n            tokenized_data = self.prepare_train_features(data)\n        else:\n            tokenized_data = self.prepare_eval_features(data)\n\n        input_ids = tokenized_data['input_ids']\n        token_type_ids = tokenized_data['token_type_ids']\n        attention_mask = tokenized_data['attention_mask']\n\n        def map_func(X_ids, \n                     X_tts, \n                     X_att, \n                     labels_s, \n                     labels_e):\n            return {'input_ids':X_ids, \n                    'token_type_ids':X_tts, \n                    'attention_mask': X_att}, \\\n                    {'start_positions': labels_s, \n                     'end_positions': labels_e}\n\n        def map_func_eval(X_ids, \n                          X_tts, \n                          X_att):\n            return {'input_ids':X_ids, \n                    'token_type_ids':X_tts, \n                    'attention_mask': X_att}\n\n        \n        if type=='train':\n\n            start_positions = tokenized_data['start_positions']\n            end_positions =  tokenized_data['end_positions']\n\n            dataset_train_raw = tf.data.Dataset.from_tensor_slices((input_ids,token_type_ids,\n                                                                    attention_mask, \n                                                                    start_positions,  \n                                                                    end_positions))\n            \n            dataset_train = dataset_train_raw.map(map_func) \\\n                            .shuffle(1000) \\\n                            .batch(batch_size, drop_remainder=True) \\\n                            .prefetch(buffer_size=AUTOTUNE) \n            return dataset_train\n\n\n        if type == 'eval':\n            dataset_eval_raw = tf.data.Dataset.from_tensor_slices((input_ids,\n                                                    token_type_ids,  \n                                                    attention_mask))\n            dataset_eval = dataset_eval_raw.map(map_func_eval) \\\n                            .batch(batch_size)\n                \n            return dataset_eval, tokenized_data       \n\n","1f53cb86":"        \ndef postprocess(raw_dataframe, features, pred_start, pred_end):\n    postprocessed_predictions = {}\n    dict_local = {}\n\n    for index, item in enumerate(features['example_id']):\n        if item not in dict_local:\n            dict_local[item] = [index]\n        else:\n            dict_local[item].append(index)\n\n\n    for key, value in dict_local.items():\n        valid_answers = []\n        min_null_score = None\n        context = raw_dataframe[raw_dataframe['id'] == key]['context'].values[0]\n\n        for indx in value:\n            start = pred_start[indx]\n            end = pred_end[indx]\n\n            offset_mapping = features[\"offset_mapping\"][indx]\n\n            # Update minimum null prediction.\n            cls_index = features[\"input_ids\"][indx] \\\n                        .index(tokenizer.cls_token_id)\n\n\n            feature_null_score = start[cls_index] + end[cls_index]\n            if min_null_score is None \\\n                or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n\n            # Go through all possibilities \n            # for the `n_best_size` greater start and end.\n\n            start_indexes = np.argsort(start)[-1 : -config['n_best_scores'] - 1 : -1].tolist()\n            end_indexes = np.argsort(end)[-1 : -config['n_best_scores'] - 1 : -1].tolist()\n\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers,\n                    # either because the indices are \n                    # out of bounds or correspond\n                    # to part of the input_ids \n                    # that are not in the context.\n                    \n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers \n                    # with a length that is \n                    # either < 0 or > max_answer_length.\n                    if end_index < start_index \\\n                        or end_index - start_index + 1 > config['max_answer_length']:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    local_context = features[\"input_ids\"][indx]\n                    valid_answers.append(\n                        {\n                            \"score\": start[start_index] + end[end_index],\n                            \"text\": context[start_char: end_char],\n                            \"token\": tokenizer.decode(local_context[start_char: end_char])\n                        }\n                    )\n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, \n                                 key=lambda x: x[\"score\"], \n                                 reverse=True)[0]\n        else:\n            best_answer = {\"text\": \"\", \n                           \"score\": 0.0}\n\n        postprocessed_predictions[key] = best_answer['text']\n\n    return postprocessed_predictions\n","08ec88eb":"dataset = ChaiiDataset(config['max_length'], config['doc_stride'], tokenizer)","f6dca4c6":"train_dataset = dataset.prepare_tf_data_pipeline(df_train)\nLOGGER.info(\"tf.data pipeline for training dataset is created\")\n","6781ff3e":"valid_dataset, valid_tokenized = dataset.prepare_tf_data_pipeline(df_valid, type='eval')\nLOGGER.info(\"tf.data pipeline for validation dataset is created\")\n","bd2dbec6":"def get_keras_model():\n    pretrained_model = TFAutoModel.from_pretrained(config['model'])\n    \n    input_ids = layers.Input(shape=(config['max_length'],),\n                             name='input_ids', \n                             dtype=tf.int32)\n    token_type_ids = layers.Input(shape=(config['max_length'],),\n                                  name='token_type_ids', \n                                  dtype=tf.int32)\n    attention_mask = layers.Input(shape=(config['max_length'],),\n                                  name='attention_mask', \n                                  dtype=tf.int32)\n    embedding = pretrained_model(input_ids, \n                     token_type_ids=token_type_ids, \n                     attention_mask=attention_mask)[0]\n\n   \n\n    x1 = tf.keras.layers.Dropout(0.1)(embedding) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax', \n                                    name='start_positions')(x1)\n\n    x2 = tf.keras.layers.Dropout(0.1)(embedding) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax', \n                                    name='end_positions')(x2)\n    \n    model = keras.Model(inputs=[input_ids, \n                                token_type_ids, \n                                attention_mask],\n                        outputs=[x1, x2])\n    \n    return model","9d8dd0aa":"model = get_keras_model()\nLOGGER.info(\"Model Loaded\")","50decd59":"if config['use_transfer_learning']:\n    for layer in model.layers:\n        if 'tf_bert_model' in layer.name:\n            layer.trainable = False\n    Logger.info(\"Transfer learning is enabled\")\n","5945465a":"loss = keras.losses.CategoricalCrossentropy()\noptimizer = keras.optimizers.Adam(lr= config['lr'])\nmodel.compile(loss = 'sparse_categorical_crossentropy', optimizer=optimizer)","f2b31f94":"model.summary()","7f87464f":"tf.keras.utils.plot_model(model, show_shapes=True,show_dtype=True)","3d0472c4":"wandb.log({\"model\": wandb.Image('model.png')})","b120651d":"class JaccardScore(keras.callbacks.Callback):\n    def __init__(self, eval_dataframe, eval_data, eval_tokenized):\n        self.eval_dataframe = eval_dataframe\n        self.eval_data = eval_data\n        self.eval_tokenized = eval_tokenized\n        \n\n    def jaccard(self, str1, str2):\n        a = set(str1.split()) \n        b = set(str2.split())\n        if (len(a)==0) & (len(b)==0): return 0.5\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    \n    \n    def get_jaccard_score(self, y_true, y_pred):\n        score=0.0\n        for indx, item in enumerate(y_true):\n            score += self.jaccard(y_true[indx], y_pred[indx])\n        return score\n\n    def on_epoch_end(self, epoch, logs=None):\n\n        pred_start, pred_end = self.model.predict(self.eval_data)\n        \n        predictions = postprocess(self.eval_dataframe, \n                                  self.eval_tokenized, \n                                  pred_start, pred_end)\n\n        y_true = []\n        y_pred = []\n        \n        for indx, item in self.eval_dataframe.iterrows():\n            id = item['id']\n            y_pred.append(predictions[id])\n            y_true.append(item['answer_text'])\n            \n        \n        score = self.get_jaccard_score(y_true,y_pred)\n        epoch_jaccard = score\/len(predictions)\n        \n        wandb.log({\"epoch\": epoch+1, \"jaccard\": epoch_jaccard})\n        print(f\"Epoch: {epoch+1}, jaccard score={epoch_jaccard:.2f}\")","30a3a682":"def get_callbacks():\n    bm = tf.keras.callbacks.ModelCheckpoint('best_model.h5',\n                                            verbose=1, \n                                            monitor='val_loss', \n                                            mode='min', \n                                            save_best_only=True, \n                                            save_weights_only=True)\n    lm = tf.keras.callbacks.ModelCheckpoint('last_model.h5',\n                                            verbose=1, \n                                            save_best_only=False, \n                                            save_weights_only=True)\n    jaccard_score_callback = JaccardScore(df_valid, \n                                          valid_dataset, \n                                          valid_tokenized)\n    callbacks = [bm , lm,  jaccard_score_callback]\n    \n    if config['use_wandb']:\n        callbacks.append( WandbCallback(save_model=False) )\n    return callbacks","d4dd6ad7":"LOGGER.debug(\"Model Training is starting\")\nhistory = model.fit(train_dataset, \n          epochs=config['epochs'], \n          callbacks=get_callbacks(), \n          verbose = 1, \n          validation_data = dataset.prepare_tf_data_pipeline(df_valid))","05954c3f":"def plot_hist(hist):\n    plt.figure(figsize=(15,5))\n    local_epochs = len(hist.history[\"loss\"])\n    plt.plot(np.arange(local_epochs, step=1), hist.history[\"loss\"], '-o', label='Train Loss',color='#ff7f0e')\n    plt.plot(np.arange(local_epochs, step=1), hist.history[\"val_loss\"], '-o',label='Val Loss',color='#1f77b4')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Loss',size=14)\n    plt.legend(loc=2)\n    \n    plt.savefig('loss.png')\n    plt.show()\n    \nplot_hist(history)","c8559cf4":"K.clear_session()\ndel train_dataset\n\nimport gc\ngc.collect()","4b91c3a4":"## Load best weights\n\nmodel.load_weights('best_model.h5')\nLOGGER.info(\"Best weights loaded\")","18be4a0b":"df_test = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\ndf_test.head()","66d9fc3f":"test_dataset, test_tokenized = dataset.prepare_tf_data_pipeline(df_test, type='eval')\nLOGGER.info(\"Test dataset loaded\")\n","f5c00ee3":"pred_test_start, pred_test_end = model.predict(test_dataset, verbose = 1, workers=4)\ntest_preds = postprocess(df_test, test_tokenized, pred_test_start, pred_test_end)\n\nLOGGER.info(\"Done with predictions on test set\")","c0bcd08b":"pred_test_df = pd.DataFrame(test_preds.keys(), \n                            test_preds.values()). \\\n                            reset_index()\npred_test_df.columns = ['PredictionString', 'id']\npred_test_df = pred_test_df[['id', 'PredictionString']]\npred_test_df.to_csv('submission.csv', index=False)\nLOGGER.info(\"Saving of predictions done\")","17fb0381":"pred_test_df","1a60caf6":"if config['use_wandb']:\n    run.finish()","b07c9d0a":"LOGGER.info(\"Done everything. Finished\")","fcfa20b1":"## Stack\n\n1. Pandas for Data Preprocessing\n\n2. tf.data from Tensorflow for Data Pipelines\n\n3. Hugging Face for pretrained model and tokenizer\n\n4. MURIL as the pretrained model\n\n5. Keras for Model training\n\n6. Weights and biases for experiment tracking\n\n![kaggle.png](attachment:d3c64f59-55c8-4f4f-9eff-d82c4e64779e.png)","fc7248f0":"## Further Reading\n\n1. I am going to use other postprocessing methods to improve the score.\n\n2. Generally for MURIL a goood approach would be to finetune on squad 2 first and then finetune on the competition dataset.\n\n3. Use Learning Rate Scheduler\n\n\n### If you learnt something from this kernel please don't forget to upvote :) ","433ca035":"<a id=\"section1\"><\/a>\n## Imports\n","7fe6a22f":"## References\n\n\n\n1. This wonderful [notebook](https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb) by HuggingFace \n    \n2. Keras Text extraction with Bert [Link](https:\/\/keras.io\/examples\/nlp\/text_extraction_with_bert\/)\n","48e1bad4":"<a id=\"section8\"><\/a>\n## Creating Model","52cabc94":"<a id=\"section10\"><\/a>\n## Make predictions on test set","ee855acb":"# MURIL (Multilingual Representations for Indic Languages)\n\n\n<font size=\"3\">\nIn this notebook I try to train model using MURIL pretrained model combined with Hugging Face and Keras.\n\nBut what is MURIL? MURIL (Khanuja et al)(@simrankhanuja) is a multilingual LM specifically built for Indic Languages. Read the full paper [here](https:\/\/arxiv.org\/pdf\/2103.10730.pdf)\n<\/font>\n","265bb60a":"<a id=\"section2\"><\/a>\n## Configuration","ad366214":"<a id=\"section9\"><\/a>\n## Model Training","a64936a6":"# Table of Contents\n\n1. [Imports](#section1)\n2. [Configuration](#section2)\n3. [Dataset split](#section3)\n4. [Setting Tokenizer](#section4)\n5. [Dataset Pipeline](#section5)\n6. [Postprocessing Pipeline](#section6)\n7. [Creating Dataset](#section7)\n8. [Creating Model](#section8)\n9. [Model Training](#section9)\n10. [Make Predictions on Test Set](#section10)","3eb2ef69":"![Screenshot from 2021-09-05 15-27-50.png](attachment:a91d27e6-acff-4189-abf6-24314847de65.png)","8a320f40":"<a id=\"section3\"><\/a>\n## Dataset Split","250efe02":"<a id=\"section4\"><\/a>\n## Setting Tokenizer","9cb68c3c":"<a id=\"section5\"><\/a>\n## Dataset Pipelines","898c6d89":"<a id=\"section6\"><\/a>\n## Postprocessing Pipeline","4d8f1f10":"<a id=\"section7\"><\/a>\n## Creating Dataset"}}