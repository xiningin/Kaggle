{"cell_type":{"ad5edd59":"code","bf57a5cb":"code","362a8a1b":"code","bcd97338":"code","1fc7e0b6":"code","2c75e695":"code","5fad9303":"code","ca3abeb7":"code","1df7eb3f":"code","a96ed429":"code","424380de":"code","15e267b9":"code","8558b8bd":"code","c54aa3bf":"code","9361987c":"code","f163750a":"code","d0ebc2b2":"code","7fef06b0":"code","7704fd19":"code","4abed700":"code","b3c7953e":"code","4b00dd7b":"code","c8bdb0a8":"code","ef79d8a3":"markdown","cdd70c49":"markdown","bc112aba":"markdown","1b3b0b32":"markdown","1c12294a":"markdown","8a11d58b":"markdown","cce2c37e":"markdown","5c5f6fba":"markdown","1f94f167":"markdown","8d539f52":"markdown","86e7ecd5":"markdown","4d933439":"markdown","59380a59":"markdown","e9c8a83a":"markdown","8746af1e":"markdown","465fbd87":"markdown","c46265cf":"markdown","655ebcf2":"markdown","a838a9c1":"markdown","d8d4f063":"markdown","b997a5cf":"markdown","a11de190":"markdown","20dc2a8d":"markdown"},"source":{"ad5edd59":"!pip install dataprep\nimport pandas as pd\nfrom functools import reduce\nimport numpy as np\nfrom dataprep.clean import clean_country\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.metrics import explained_variance_score, max_error, mean_absolute_error, median_absolute_error\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import SVR\nfrom numpy import mean\nfrom sklearn.tree import export_graphviz\nfrom subprocess import check_call\nfrom sklearn.inspection import permutation_importance","bf57a5cb":"# Return Pandas Series with the number of NaNs per column in DataFrame\ndef nans_per_column(df):\n    return df.isna().sum()\n\n\n# Compare Country Code\/Country Name MultiIndex in two DataFrames\ndef compare_multiindex(wb_df, res_df):\n    wb_df = wb_df.set_index(['Country Code', 'Country Name'])\n    res_df = res_df.set_index(['Country Code', 'Country Name'])\n    index1, index2 = wb_df.index, res_df.index\n    diff1 = index1.difference(index2).to_frame(index=False)\n    diff2 = index2.difference(index1).to_frame(index=False)\n    diff1['Belongs to'] = 'Worldbank data'\n    diff2['Belongs to'] = 'Resource data'\n    diff = pd.concat([diff1, diff2])\n    return diff.sort_values(by='Country Code')","362a8a1b":"# Load worldbank data\ndef load_wb():\n    wb_data = pd.read_csv('.\/data\/world_bank_data.csv', skipfooter=5, engine='python') # Load world bank data, skip footer to avoid non-data lines in end of file\n    return wb_data\n\n\n# Load resource data (cement, coal, gas, and oil)\ndef load_resources():\n    resources = ['cement', 'coal', 'gas', 'oil']\n    \n    # Create DFs from all files and merge in an outer join on Entity, Code and Year\n    dfs = [pd.read_csv('data\/{}.csv'.format(f)) for f in resources]\n    res_df = reduce(lambda left,right: pd.merge(left, right, on=['Entity', 'Code', 'Year'], how='outer'), dfs)\n    return res_df","bcd97338":"# Format worldbank data\ndef format_wb(wb_data):\n    df = pd.DataFrame(wb_data)\n\n    # Remove invalid values\n    df = df.replace('..', np.nan) \n    \n    # Change datatype to numerical\n    df[\"2016 [YR2016]\"] = df[\"2016 [YR2016]\"].astype(\"float\")\n    df[\"2017 [YR2017]\"] = df[\"2017 [YR2017]\"].astype(\"float\")\n    df[\"2018 [YR2018]\"] = df[\"2018 [YR2018]\"].astype(\"float\")\n    df[\"2019 [YR2019]\"] = df[\"2019 [YR2019]\"].astype(\"float\")\n    \n    # Using the last avaiable value for 2019 from the last years\n    df.iloc[: , -4:] = df.iloc[: , -4:].interpolate(method='linear', axis='columns')\n\n    # Reformat rows and columns\n    df = df.pivot(\n        index=['Country Name', 'Country Code'],\n        columns='Series Name',\n        values='2019 [YR2019]'\n    )\n    \n    # Remove MultiIndex and \"Series Name\" from df.pivot()\n    df = df.reset_index().rename_axis(None, axis=1) \n    return df\n\n# Format resource data\ndef format_res(res_data):\n    df = pd.DataFrame(res_data)\n    df = df.rename(columns={\"Entity\": \"Country Name\", \"Code\": \"Country Code\"})\n\n    # Remove not needed data\n    df.drop(df[df.Year < 2016].index, inplace=True)\n    df.drop(df[df.Year == 2020].index, inplace=True)\n    \n    # Change datatype to numerical\n    df[\"Annual CO2 emissions from cement (per capita)\"] = df[\"Annual CO2 emissions from cement (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from coal (per capita)\"] = df[\"Annual CO2 emissions from coal (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from gas (per capita)\"] = df[\"Annual CO2 emissions from gas (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from oil (per capita)\"] = df[\"Annual CO2 emissions from oil (per capita)\"].astype(\"float\")\n\n    # Using the last avaiable value for 2019 from the last years\n    for i in range(len(df[\"Country Name\"].unique())):\n        mask = df.loc[:,\"Country Name\"]==df[\"Country Name\"].unique()[i]\n        df[mask] = df[mask].interpolate(method='linear')\n    \n    # Remove not needed data\n    df.drop(df[df.Year < 2019].index, inplace=True)\n    df = df.drop(columns=['Year'])\n    \n    return df\n\n\n# Standardize country names according to Dataprep library: https:\/\/docs.dataprep.ai\/user_guide\/clean\/clean_country.html\ndef standardize_countries(unstandard_df):\n    df = pd.DataFrame(unstandard_df)\n    standard_df = clean_country(df, 'Country Code', report=False)\n    standard_df = standard_df[standard_df['Country Code_clean'].notna()]\n    standard_df['Country Name'] = standard_df['Country Code_clean']\n    standard_df = standard_df.drop(columns='Country Code_clean')\n    standard_df = standard_df.set_index('Country Code')\n    return standard_df\n\n\n# # Combine worldbank and resource data\ndef combine_dfs(wb_df, res_df):\n    df = wb_df.merge(res_df, on=['Country Code', 'Country Name'], how='left')\n    df.dropna(thresh=20, inplace=True)\n    df.fillna(value=0, inplace=True)\n    return df.sort_values(by='Country Code')","1fc7e0b6":"# Load worldbank data\ndef load_wb():\n    wb_data = pd.read_csv('..\/input\/income-level-country\/world_bank_data.csv', skipfooter=5, engine='python') # Load world bank data, skip footer to avoid non-data lines in end of file\n    return wb_data\n\nwb_data = load_wb()\nwb_data.head()","2c75e695":"# Load resource data (cement, coal, gas, and oil)\ndef load_resources():\n    resources = ['cement', 'coal', 'gas', 'oil']\n    \n    # Create DFs from all files and merge in an outer join on Entity, Code and Year\n    dfs = [pd.read_csv('..\/input\/income-level-country\/{}.csv'.format(f)) for f in resources]\n    res_df = reduce(lambda left,right: pd.merge(left, right, on=['Entity', 'Code', 'Year'], how='outer'), dfs)\n    return res_df\n\nres_data = load_resources()\nres_data.head()","5fad9303":"# Format worldbank data\ndef format_wb(wb_data):\n    df = pd.DataFrame(wb_data)\n\n    df = df.replace('..', np.nan) \n    df[\"2016 [YR2016]\"] = df[\"2016 [YR2016]\"].astype(\"float\")\n    df[\"2017 [YR2017]\"] = df[\"2017 [YR2017]\"].astype(\"float\")\n    df[\"2018 [YR2018]\"] = df[\"2018 [YR2018]\"].astype(\"float\")\n    df[\"2019 [YR2019]\"] = df[\"2019 [YR2019]\"].astype(\"float\")\n    \n    df.iloc[: , -4:] = df.iloc[: , -4:].interpolate(method='linear', axis='columns')\n\n    # Reformat rows and columns\n    df = df.pivot(\n        index=['Country Name', 'Country Code'],\n        columns='Series Name',\n        values='2019 [YR2019]'\n    )\n    \n    # Remove MultiIndex and \"Series Name\" from df.pivot()\n    df = df.reset_index().rename_axis(None, axis=1) \n    return df\n\nwb_df = format_wb(wb_data)\nwb_df.head()","ca3abeb7":"# Format resource data\ndef format_res(res_data):\n    df = pd.DataFrame(res_data)\n    df = df.rename(columns={\"Entity\": \"Country Name\", \"Code\": \"Country Code\"})\n    df.drop(df[df.Year < 2016].index, inplace=True)\n    df.drop(df[df.Year == 2020].index, inplace=True)\n    \n    df[\"Annual CO2 emissions from cement (per capita)\"] = df[\"Annual CO2 emissions from cement (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from coal (per capita)\"] = df[\"Annual CO2 emissions from coal (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from gas (per capita)\"] = df[\"Annual CO2 emissions from gas (per capita)\"].astype(\"float\")\n    df[\"Annual CO2 emissions from oil (per capita)\"] = df[\"Annual CO2 emissions from oil (per capita)\"].astype(\"float\")\n\n    for i in range(len(df[\"Country Name\"].unique())):\n        mask = df.loc[:,\"Country Name\"]==df[\"Country Name\"].unique()[i]\n        df[mask] = df[mask].interpolate(method='linear')\n    \n    df.drop(df[df.Year < 2019].index, inplace=True)\n    \n    df = df.drop(columns=['Year'])\n    \n    return df\n\nres_df = format_res(res_data)\nres_df.head()","1df7eb3f":"# Compare Country Code\/Country Name MultiIndex in two DataFrames\ndef compare_multiindex(wb_df, res_df):\n    wb_df = wb_df.set_index(['Country Code', 'Country Name'])\n    res_df = res_df.set_index(['Country Code', 'Country Name'])\n    index1, index2 = wb_df.index, res_df.index\n    diff1 = index1.difference(index2).to_frame(index=False)\n    diff2 = index2.difference(index1).to_frame(index=False)\n    diff1['Belongs to'] = 'Worldbank data'\n    diff2['Belongs to'] = 'Resource data'\n    diff = pd.concat([diff1, diff2])\n    return diff.sort_values(by='Country Code')\n\ndiff = compare_multiindex(wb_df, res_df)\ndiff.head()","a96ed429":"# Standardize country names according to Dataprep library: https:\/\/docs.dataprep.ai\/user_guide\/clean\/clean_country.html\ndef standardize_countries(unstandard_df):\n    df = pd.DataFrame(unstandard_df)\n    standard_df = clean_country(df, 'Country Code', report=False)\n    standard_df = standard_df[standard_df['Country Code_clean'].notna()]\n    standard_df['Country Name'] = standard_df['Country Code_clean']\n    standard_df = standard_df.drop(columns='Country Code_clean')\n    standard_df = standard_df.set_index('Country Code')\n    return standard_df\n\nstd_wb_df = standardize_countries(wb_df)\nstd_wb_df.head()","424380de":"std_res_df = standardize_countries(res_df)\nstd_res_df.head()","15e267b9":"# Combine worldbank and resource data\ndef combine_dfs(wb_df, res_df):\n    df = wb_df.merge(res_df, on=['Country Code', 'Country Name'], how='left')\n    df.dropna(thresh=20, inplace=True)\n    df.fillna(value=0, inplace=True)\n    return df.sort_values(by='Country Code')\n\ncomb_df = combine_dfs(std_wb_df, std_res_df)\ncomb_df.head()","8558b8bd":"# Return Pandas Series with the number of NaNs per column in DataFrame\ndef nans_per_column(df):\n    return df.isna().sum()\n\nnans_per_column(comb_df)","c54aa3bf":"outliers=comb_df.select_dtypes(exclude=['object'])[['Employment in agriculture (% of total employment) (modeled ILO estimate)']]\n\nfor column in outliers:\n        plt.figure(figsize=(15,1))\n        sns.boxplot(data=outliers, x=column)\n        plt.close","9361987c":"Q1 = comb_df['Employment in agriculture (% of total employment) (modeled ILO estimate)'].quantile(0.25)\nQ3 = comb_df['Employment in agriculture (% of total employment) (modeled ILO estimate)'].quantile(0.75)\nIQR = Q3 - Q1\n \nprint(comb_df[['Employment in agriculture (% of total employment) (modeled ILO estimate)']].loc[comb_df['Employment in agriculture (% of total employment) (modeled ILO estimate)'] >= Q3 + 1.5*IQR])","f163750a":"plt.hist(comb_df[\"GNI per capita, Atlas method (current US$)\"])\nplt.title(\"Distribution of the Output Variable\")\nplt.xlabel(\"GNI per Capita\")\nplt.ylabel(\"Frequeny\")\nplt.show()\n\nfor i in comb_df.columns:\n    if i == \"Country Name\":\n        continue\n    if skew(comb_df[i]) >= 4:\n        print(\"Skewness:\",skew(comb_df[i]))\n        plt.hist(comb_df[i])\n        plt.xlabel(i)\n        plt.ylabel(\"Frequency\")\n        plt.title(\"Distribution of {}\".format(i))\n        plt.show()","d0ebc2b2":"cor = comb_df.corr()\n\nhigh_cor = []\nfor i in range(0,cor.shape[0]):\n    for j in range(0,cor.shape[1]):\n        # get correlation > 0.75\n        if cor.iloc[i, j] >= 0.75 and i != j:\n            if i not in high_cor:\n                high_cor.append(i)\n            if j not in high_cor:\n                high_cor.append(j)\n\ndf = pd.DataFrame()\nfor i in high_cor:\n    df = df.append(comb_df[cor.columns[i]])\n\n# annotate only correlation > 0.75\nax = sns.heatmap(df.transpose().corr(), annot=True)\nfor text in ax.texts:\n    if float(text.get_text())>=0.75:\n        text.set_text(text.get_text())\n    else:\n        text.set_text(\"\")","7fef06b0":"del comb_df[\"Country Name\"]\n\ndel comb_df[\"Trade (% of GDP)\"]\ndel comb_df[\"Oil rents (% of GDP)\"]\n\ny = comb_df[\"GNI per capita, Atlas method (current US$)\"]\nX = comb_df.loc[:, comb_df.columns != \"GNI per capita, Atlas method (current US$)\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=987, shuffle=True)\n\nscaler = MinMaxScaler()\nX_train_min_max = scaler.fit_transform(X_train)\nX_test_min_max = scaler.transform(X_test)\n\nscaler = StandardScaler()\nX_train_standard = scaler.fit_transform(X_train)\nX_test_standard = scaler.transform(X_test)","7704fd19":"datasets = {\"\":[X_train,X_test], \"MinMax Scaling\":[X_train_min_max,X_test_min_max], \"Standard Scaling\":[X_train_standard,X_test_standard]}\ncoefs = []\n\nfor label, data in datasets.items():\n    model = LinearRegression()\n    model.fit(data[0], y_train)\n\n    pred = model.predict(data[1])\n\n    coefs.append(model.coef_)\n\n    print(\"RMSE:\", mean_squared_error(y_test, pred, squared=False))\n\n    x = range(0,int(max(y_test)))\n    plt.scatter(pred, y_test)\n    plt.title(\"Linear Regression {}\".format(label))\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.plot(x,x, color=\"red\")\n    plt.show()\n\ncoefs = pd.DataFrame(coefs).transpose()\ncoefs.index = X_train.columns.str[0:30]\ncoefs.columns = [\"No Scaling\",\"MinMax Scaling\",\"Standard Scaling\"]\nprint(coefs)","4abed700":"kf = KFold(n_splits=5, random_state=123, shuffle=True)\n\nrf_min_max = []\nknn_min_max = []\nlasso_min_max = []\nsvr_min_max = []\n\nrf_standard = []\nknn_standard = []\nlasso_standard = []\nsvr_standard = []\n\nfor train_index, test_index in kf.split(X_train):\n    X_train_kf_min_max, X_test_kf_min_max = X_train_min_max[train_index,], X_train_min_max[test_index,]\n    X_train_kf_standard, X_test_kf_standard = X_train_standard[train_index,], X_train_standard[test_index,]\n    y_train_kf, y_test_kf = y_train[train_index], y_train[test_index]\n\n    params = {\"n_estimators\":[25,50,75,100,125],\n              \"min_samples_leaf\":[1,3,5,7]}\n\n    model = GridSearchCV(estimator=RandomForestRegressor(), param_grid=params)\n    model.fit(X_train_kf_min_max, y_train_kf)\n    pred = model.predict(X_test_kf_min_max)\n    rf_min_max.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    model = GridSearchCV(estimator=RandomForestRegressor(), param_grid=params)\n    model.fit(X_train_kf_standard, y_train_kf)\n    pred = model.predict(X_test_kf_standard)\n    rf_standard.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    params = {\"n_neighbors\":[1,3,5,7,9,11], \"weights\":[\"uniform\",\"distance\"],\"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"]}\n\n    model = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=params)\n    model.fit(X_train_kf_min_max, y_train_kf)\n    pred = model.predict(X_test_kf_min_max)\n    knn_min_max.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    model = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=params)\n    model.fit(X_train_kf_standard, y_train_kf)\n    pred = model.predict(X_test_kf_standard)\n    knn_standard.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    params = {\"alpha\":[1,10,100,1000,10000, 100000]}\n\n    model = GridSearchCV(estimator=Lasso(max_iter=10000), param_grid=params)\n    model.fit(X_train_kf_min_max, y_train_kf)\n    pred = model.predict(X_test_kf_min_max)\n    lasso_min_max.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    model = GridSearchCV(estimator=Lasso(max_iter=10000), param_grid=params)\n    model.fit(X_train_kf_standard, y_train_kf)\n    pred = model.predict(X_test_kf_standard)\n    lasso_standard.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    params = [\n            {\"kernel\":\n                 [\"linear\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"poly\"],\n             \"degree\": [2, 3, 4, 5, 6, 7, 8],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"rbf\"],\n             \"gamma\": [\"auto\", \"scale\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]}\n        ]\n\n    model = GridSearchCV(estimator=SVR(), param_grid=params)\n    model.fit(X_train_kf_min_max, y_train_kf)\n    pred = model.predict(X_test_kf_min_max)\n    svr_min_max.append(mean_squared_error(y_test_kf, pred, squared=False))\n\n    model = GridSearchCV(estimator=SVR(), param_grid=params)\n    model.fit(X_train_kf_standard, y_train_kf)\n    pred = model.predict(X_test_kf_standard)\n    svr_standard.append(mean_squared_error(y_test_kf, pred, squared=False))\n\nprint(\"Mean RMSE Random Forest with MinMaxScaler:\",mean(rf_min_max))\nprint(\"Mean RMSE Random Forest with StandardScaler:\",mean(rf_standard))\nprint(\"Mean RMSE K-Nearest Neighbors with MinMaxScaler:\",mean(knn_min_max))\nprint(\"Mean RMSE K-Nearest Neighbors with StandardScaler:\",mean(knn_standard))\nprint(\"Mean RMSE SVR with MinMaxScaler:\",mean(svr_min_max))\nprint(\"Mean RMSE SVR with StandardScaler:\",mean(svr_standard))\nprint(\"Mean RMSE Lasso with MinMaxScaler:\",mean(lasso_min_max))\nprint(\"Mean RMSE Lasso with StandardScaler:\",mean(lasso_standard))","b3c7953e":"if mean(rf_min_max) < min(mean(rf_standard),mean(svr_min_max),mean(svr_standard),mean(lasso_min_max),mean(lasso_standard),\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Random Forest Regressor\")\n    params = {\"n_estimators\":[25,50,75,100,125],\n              \"min_samples_leaf\":[1,3,5,7]}\n\n    model = GridSearchCV(estimator=RandomForestRegressor(random_state=123), param_grid=params)\n    model.fit(X_train_min_max, y_train)\n    pred = model.predict(X_test_min_max)\nelif mean(rf_standard) < min(mean(svr_min_max),mean(svr_standard),mean(lasso_min_max),mean(lasso_standard),\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Random Forest Regressor\")\n    params = {\"n_estimators\":[25,50,75,100,125],\n              \"min_samples_leaf\":[1,3,5,7]}\n\n    model = GridSearchCV(estimator=RandomForestRegressor(random_state=123), param_grid=params)\n    model.fit(X_train_standard, y_train)\n    pred = model.predict(X_test_standard)\nelif mean(svr_min_max) < min(mean(svr_standard),mean(lasso_min_max),mean(lasso_standard),\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Support Vector Regression\")\n    params = [\n            {\"kernel\":\n                 [\"linear\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"poly\"],\n             \"degree\": [2, 3, 4, 5, 6, 7, 8],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"rbf\"],\n             \"gamma\": [\"auto\", \"scale\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]}\n        ]\n\n    model = GridSearchCV(estimator=SVR(), param_grid=params)\n    model.fit(X_train_min_max, y_train)\n    pred = model.predict(X_test_min_max)\nelif mean(svr_standard) < min(mean(lasso_min_max),mean(lasso_standard),\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Support Vector Regression\")\n    params = [\n            {\"kernel\":\n                 [\"linear\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"poly\"],\n             \"degree\": [2, 3, 4, 5, 6, 7, 8],\n             \"C\": [0.001, 0.1, 1, 10, 100]},\n            {\"kernel\":\n                 [\"rbf\"],\n             \"gamma\": [\"auto\", \"scale\"],\n             \"C\": [0.001, 0.1, 1, 10, 100]}\n        ]\n\n    model = GridSearchCV(estimator=SVR(), param_grid=params)\n    model.fit(X_train_standard, y_train)\n    pred = model.predict(X_test_standard)\nelif mean(lasso_min_max) < min(mean(lasso_standard),\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Lasso\")\n    params = {\"alpha\":[1,10,100,1000,10000, 100000]}\n    model = GridSearchCV(estimator=Lasso(max_iter=10000), param_grid=params)\n    model.fit(X_train_min_max, y_train)\n    pred = model.predict(X_test_min_max)\nelif mean(lasso_standard) < min(\n                          mean(knn_min_max),mean(knn_standard)):\n    print(\"Best Model:\",\"Lasso\")\n    params = {\"alpha\":[1,10,100,1000,10000, 100000]}\n    model = GridSearchCV(estimator=Lasso(max_iter=10000), param_grid=params)\n    model.fit(X_train_standard, y_train)\n    pred = model.predict(X_test_standard)\nelif mean(knn_min_max) < mean(knn_standard):\n    print(\"Best Model:\",\"K-Nearest Neighbor Regressor\")\n    params = {\"n_neighbors\":[1,3,5,7,9,11], \"weights\":[\"uniform\",\"distance\"],\"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"]}\n    model = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=params)\n    model.fit(X_train_min_max, y_train)\n    pred = model.predict(X_test_min_max)\nelse:\n    print(\"Best Model:\",\"K-Nearest Neighbor Regressor\")\n    params = {\"n_neighbors\":[1,3,5,7,9,11], \"weights\":[\"uniform\",\"distance\"],\"algorithm\":[\"ball_tree\",\"kd_tree\",\"brute\"]}\n    model = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=params)\n    model.fit(X_train_standard, y_train)\n    pred = model.predict(X_test_standard)\n\n\nprint(\"Final Hyper-Parameters:\", model.best_params_)\nprint(\"RMSE:\", mean_squared_error(y_test, pred, squared=False))\nprint(\"Exlained Variance:\", explained_variance_score(y_test,pred))\nprint(\"Max Error:\", max_error(y_test, pred))\nprint(\"Mean Absolute Error:\", mean_absolute_error(y_test, pred))\nprint(\"Median Absolute Error:\", median_absolute_error(y_test, pred))\n\nx = range(0,int(max(y_test)))\nplt.scatter(pred, y_test)\nplt.title(\"Final Model Predictions\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.plot(x,x, color=\"red\")\nplt.show()","4b00dd7b":"rf = model.best_estimator_\nimportances = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=123, n_jobs=-1).importances_mean\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_])\n\nplt.figure(figsize=(15,10))\nplt.bar(X.columns, importances, yerr=std)\nplt.title(\"Feature Importances according to Feature Permutation\")\nplt.ylabel(\"Mean Accuracy Decrease\")\nplt.xticks(rotation=90)\nplt.show()","c8bdb0a8":"for i in [1,3,5,7,9,13,19,21,25,28,33,45]:\n    export_graphviz(model.best_estimator_.estimators_[i], out_file=\".\/vis_{}.dot\".format(i), feature_names=X_train.columns, rounded=True,\n                    precision=True, filled=True)\n    check_call(['dot', '-Tpng', '.\/vis_{}.dot'.format(i), '-o', '.\/vis_{}.png'.format(i)],shell=True)\n\n    #img = image.imread(\"trees\/vis_{}.png\".format(i))\n    #plt.imshow(img)\n    #plt.show()\n\nplt.scatter(comb_df[\"Employment in agriculture (% of total employment) (modeled ILO estimate)\"],\n            comb_df[\"GNI per capita, Atlas method (current US$)\"])\nplt.title(\"Employment in Agriculture vs. GNI\")\nplt.xlabel(\"Employment in Agriculture\")\nplt.ylabel(\"GNI per Capita\")\nplt.show()\n\nplt.scatter(comb_df[\"Employment in services (% of total employment) (modeled ILO estimate)\"],\n            comb_df[\"GNI per capita, Atlas method (current US$)\"])\nplt.title(\"Employment in Services vs. GNI\")\nplt.xlabel(\"Employment in Services\")\nplt.ylabel(\"GNI per Capita\")\nplt.show()\n\nplt.scatter(comb_df[\"Annual CO2 emissions from oil (per capita)\"],\n            comb_df[\"GNI per capita, Atlas method (current US$)\"])\nplt.title(\"Annual CO2 emissions from oil vs. GNI\")\nplt.xlabel(\"Annual CO2 emissions from oil\")\nplt.ylabel(\"GNI per Capita\")\nplt.show()","ef79d8a3":"### Explore the Distributions\nIn the following, the distribution of the output variable is plotted. It can be seen that the distribution is very skewed. We might therefore have problems correctly prediction higher GNI values as the majority of values is between 0 and 20 000.\n\nWe also took the time to plot the different attributes. We used the *skew()* function of scipy, which measures the skewness of the data. We plotted the histograms for data with a skewness of >= 4.\nIt can be seen that a lot of attribute values are skewed as well and only few resemble somehting close to a normal distribution.","cdd70c49":"### Format data and Standardize Country Names with Dataprep\n#### Worldbank data\n\nMissing values from 2019 are interpolated using the values from 2016-2018, then the DataFrame is pivoted to create columns out of the Series Names. Each column value will correspond to the 2019 data for each country.  ","bc112aba":"## Loading and Preprocessing of Data\n### Load data from CSV files\nWorld bank data is loaded into a DataFrame from world_bank_data.csv\\\nResource data is loaded and combined into a DataFrame from cement.csv, coal.csv, gas.csv, and oil.csv","1b3b0b32":"#### Standardization of Country Name\n\nCompare country code\/name combinations to explore data and manually detect potential inconsistencies.","1c12294a":"### Combine datasets\nWorldbank and Resource DataFrames are merged on Country Name and Country Code. Entities with a large number of NaNs are removed, as these belong to countries with too few values to work with.","8a11d58b":"### Loading functions","cce2c37e":"### Search for outliers\nDuring the exploration phase we examined all the different attributes in the following way. The outliers were searched in the first step with boxplots. An outlier is defined as a data point that is located outside the whiskers of the boxplot. There was no negative or left-hand outliers. The maximum of the box plot is defined as Q3+1.5\\*IQR. For visualisation purposes we only generate the boxplot for one attribute in the following.","5c5f6fba":"### Linear Regression\nIn the following, we use linear regression to assess whether the data is linearly separable or not. We use all input datasets (not scaled, MinMax scaled and Standard scaled) to train the models. For each model, we save the regression coefficients to analyse the realtive importance of the variables.\n\nAltogether, it can be seen that the models return very similar RMSE values. Therfore, scaling the data did not make a big difference in this setting. We can also conclude that the data does not seem to be linear separable and we have to turn to different algorithms to find better model fits.\n\nMoreover, we printed the respective model coefficients, which led to the following insights. All three models returned completely different regression coefficients, which is why they are hard to compare. It is hardly possible to make up assumptions about rather insignificant variables throughout all models. Therefore, this had to be decided for each model separately.\n\nNevetheless, the following three variables have the least impact on the output throughout all models.\n- Expense (% of GDP)\n- ICT goods imports\n- Annual CO2 emissions from gas","1f94f167":"### Visualisation of Some Decision Trees\nWe generated some visualisations of the decision trees. They are too big, however, to include them in the notebook. You can find the generated .png files in the *trees* directory.\n\nWe believe that it is interesting to take a further look at certain decision boundaries to check, which values for the respective attributes give an idea on the GNI value of the country.\n- What can be observed when looking at the different decision boundaries for Employment in Agriculture is that 0.1 seems to play an important role. We plotted the two variables against each other below. This explains the high importance of the attribute.\n- When looking at the decision boundary for Employment in services, the line is not as clear as for agriculture, but the decision boundaries usually range from 0.7-0.9 (70% - 90%). When looking at the scatterplot, this is also not surprising.\n- Another interesting insights is that the CO2 emissions from oil decision boundary is usually about 0.0 to 0.1. The scatterplot reveals that countries with a lower CO2 emission from oil value have a much lower GNI value as well.\n\nThe other variables do not provide a very clear separation between low- and high GNI value countries.","8d539f52":"### Helper functions","86e7ecd5":"## Analysis of Results\n### Feature Importance\nBelow, we used feature permutation to retrieve the feature importances of the attributes.\n\nIt can be observed that there are a lot of attributes, which hardly important and only few highly contributing ones. In the following, we rank the features based on their importance.\n1. Employment in Agriculture\n2. Employment in Services\n3. Annual CO2 emissions from coal\n4. Phyisicians per 1000 people\n5. Annual CO2 emissions from oil\n6. Medium and high-tech manufacturing value added\n7. Expense\n8. ICT goods exports\n9. Annual CO2 emissions from gas","4d933439":"## Conclusion\nOne of initial decision for us was which year we wanted to consider for the predictions. As most of the data for 2021 was not available when we first started looking (December 2021), we decided to not go with 2020 because it was the first COVID year, where a lot of attributes could have deviated. We therefore used the data from 2019.\n\nIn the beginning, we spent a lot of time thinking about which attributes we wanted to consider and focused on using attributes which were not directly related to the income level of a country (e.g. expenditure on health care, infrastructure or education depends on the available financial resources of a country). We therefore chose to use attributes related to working sectors, natural resources and import and export.\n\nAfter deciding on attributes to consider for the task, finding and downloading the data was a straightforward task and done very quickly. Loading and merging the datasets was also an easy task as both data sources provided a three character country code. However, we used the package *dataprep* to unify the **Country Name** column as well.\n\nWe recognised, however, that the data from World Bank had a lot of NaN values, which was why we decided to interpolate missing values based on data from 2016 to 2018. We then deleted all other rows with more than 20 NaN values. Afterwards, we plotted a correlation matrix for the data frame and looked at columns with a correlation of more than 75\\%. Because of the high correlation we deleted the columns **Trade (% of GDP)** and **Oil rents (% of GDP)**.\n\nAs already mentioned when plotting the histogram of the output variable, the model had difficulties predicting GNI values for countries with a higher income. For the countries with lower ones, the prediciton was quite accurate, as can be seen from the Median Absolute Error. The high outliers for higher income countries changed RMSE value a lot.\n\nStill, a main objective of the task was to find more and less important attributes when trying to predict the GNI value of a country. As the Random Forest Regressor worked best for our dataset, we were able to use the scikit-learn function *permuation_importance* to retrieve the importance of each attribute for predicting the output variable.\n\nThis showed us that there were few variables, which were highly contributing to explaining the GNI value (Employment in Agriculture being the most important one) and a lot of variables which did not contribute too much to the GNI value. Therefore, if the goal is to make more accurate predictions, it should be considered to take more diverse attributes into consideration (again, while trying to not use attributes directly related to the income level of a country).","59380a59":"After examining the values we ageed on the \"outliers\" are explainable with common sense, or the values could be (apprx.) verified by other sources too. Therefore, we accepted them since they are not sensor \/ measured, but more \"reliable\" data.","e9c8a83a":"From comparing country codes\/names, it is clear that:\n- Resource data: Non-country entities have Country Code set to NaN\n- Worldbank data: Non-country entities have Country Code set to an inofficial country code\n- Country name spelling differs between the two data sets\n\nTo standardize the country names and remove non-country entities, the Dataprep library is used. This will filter the NaN\/invalid Country Codes and use standardized country name spelling.","8746af1e":"## Machine Learning\nIn the following, we first delete the non-predictive attribute *Country Name*. As described in the previous steps, due to high correlation, we have also decided to trop the *Trade* and *Oil rents* variable.\n\nWe then split the data into an input and output data frame (the output variable being *GNI per capita*). Afterwards we split the data again into a training and test set. The training set will be used to fit a ML model and the test set will be used in the end to again test the configuration with a completely independent test set.\n\nAdditionally, we scale the data using two different approaches, namely MinMaxScaling and  Standard Scaling. The two approaches will be compared later on to find the optimal scaling technique.","465fbd87":"### Formatting functions","c46265cf":"## Data Exploration\n### Check for null values\nThe goal of data preparation was to fill or remove the null values. This was made to enable further processing and, ultimately, training of the model.","655ebcf2":"### Cross-Validation\nAs Linear Regression did not lead to satisfactory results, we decided to go with different Regression models. In the following, we implement a 5-Fold Cross-Validation to test and compare a RandomForestRegressor, KNeighborsRegressor, Lasso Regression and a Support Vector Regression.\n\nWe split the data into another training and validation set in the beginning and train the model (and fit the Grid Search) on the training set. We then store the RMSE on the validation set.\n\nIn the end, the best model is determined by the mean RMSE from the Cross-Validation.","a838a9c1":"### Final Fitting of the Model\nIn the following, we determine the best model and scaling and train it once again with the training data. The configuration is then tested on the independent test set.\n\nFinally, the results are printed. Here, we used multiple different evaluation methods to be able further analyse the results.\n\nWhat can evidently be observed is that (as already discussed in the exloration section with the distribution of the output variable) the model is able to accurately predict lower GNI values. In contrast, it has a lot more difficulties predicting the GNI values of countries with a higher income level.\n\nBased on the results of the final model, we can make the following observations.\n- The RMSE value of the Model is very high, given that the predictions for higher GNI values were quite inaccurate\n- Therefore, the maximum error is very high as well, as well as the mean error\n- Still, it can be seen that the median error was not too high and the predictions where, at least for smaller GNI values, quite accurate","d8d4f063":"### Correlation of Variables\nIn the following, a correlation plot is generated, using all attributes with a correlation bigger or equal 0.75.\n\nThe following can be observed:\n- Coal Rents and Annual CO2 emissions from coal highly correlate (0.76) with each other. This can, however, be expected as they describe a similar variable.\n- Exports and Imports of Goods have a high correlation with each other (which can be expected too). **However, there is an even higher correlation with the trade variable, which is why we removed it ultimately (with a correlation of 0.98 and 0.97 respectively, there is hardly any added information)**.\n- Fuel exports and Oil rents correlate with each other. This was expected too as fuel exports depend on oil.\n- Moreover, Oil rents and total natural resources rents highly correlate with each other (0.92). **This and the high correlation with Fuel exports led to us removing the Oil rents from the dataset**.\n- ICT (Information and communications technology) exports and imports correlate highly.\n- And lastly, the export and manufacturing of medium and high-tec products correlate with a factor of 0.78 too.","b997a5cf":"### Check the outliers\nAfter examining the boxplots the \"outliers\" were printed (and controlled) with the \"value > Q3+1.5\\*IQR\" formula. In case there were still too many datapoints we used a bigger multiplier, if there weren't any datapoint a subtraction was used with a smaller multiplier (e.g. \"value > Q3-0.5\\*IQR\") to still be able to check the biggest values.\n","a11de190":"#### Resource data\nColumn names are changed to match Worldbank DataFrame, old data is dropped, and missing values from 2019 are interpolated using the values from 2016-2018.","20dc2a8d":"# Which characteristics are predictive for the income level of a country?\n\n## Data\n### World Bank\nWe took the dataset `world_bank_data.csv` and the corresponding metadata (`world_bank_metadata`) from World Bank.\n\nThe data can be found on their website:\n- World Bank. World Development Indicators. Publishes online at databank.worldbank.org. Retrieved from: https:\/\/databank.worldbank.org\/source\/world-development-indicators. [Online Resource]. Accessed on 12\/26\/2021.\n\n### Our World in Data\nWe took the datasets `cement.csv`, `coal.csv`, `gas.csv` and `oil.csv` from the organisation *Our World in Data*.\n\nThe corresponding data can be found in:\n- Hannah Ritchie and Max Roser (2020). CO\u2082 and Greenhouse Gas Emissions. Published online at OurWorldInData.org. Retrieved from: https:\/\/ourworldindata.org\/co2-and-other-greenhouse-gas-emissions [Online Resource]. Accessed on 12\/26\/2021.\n- Global Carbon Project. (2021). Supplemental data of Global Carbon Budget 2021 (Version 1.0) [Data set]. Global Carbon Project. https:\/\/doi.org\/10.18160\/gcp-2021\n\n## Preparation\n### Imports"}}