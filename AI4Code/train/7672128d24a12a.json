{"cell_type":{"1939fd3c":"code","6cdfc8a6":"code","5f717525":"code","e753381a":"code","ec379942":"code","cede44aa":"code","da6f6f48":"code","2e37ac2f":"code","92ec88b0":"code","372c35bd":"code","76edae95":"code","fe827342":"markdown","41d336c5":"markdown","c10b73db":"markdown","60c7ca1e":"markdown"},"source":{"1939fd3c":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","6cdfc8a6":"import gc\nimport os\nimport random\nimport wandb\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\ndevice = torch.device(\"cuda\")","5f717525":"class config:\n    EXP_NAME = \"exp010_transformer\"\n    \n    INPUT = \"\/kaggle\/input\/ventilator-pressure-prediction\"\n    OUTPUT = \"\/kaggle\/working\"\n    N_FOLD = 5\n    SKIP_FOLDS = [1, 2, 3, 4]  # only fold-0\n    SEED = 0\n    \n    LR = 2.5e-2\n    N_EPOCHS = 50\n    HIDDEN_SIZE = 64\n    BS = 2048\n    WEIGHT_DECAY = 1e-5\n    \n    NOT_WATCH_PARAM = ['INPUT']","e753381a":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","ec379942":"class VentilatorDataset(Dataset):\n    \n    def __init__(self, df):\n        self.dfs = [_df for _, _df in df.groupby(\"breath_id\")]\n        \n    def __len__(self):\n        return len(self.dfs)\n    \n    def __getitem__(self, item):\n        df = self.dfs[item]\n        \n        X = df[['R_cate', 'C_cate', 'u_in', 'u_out']].values\n        y = df['pressure'].values\n        d = {\n            \"X\": torch.tensor(X).float(),\n            \"y\": torch.tensor(y).float(),\n        }\n        return d","cede44aa":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) \/ d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n    \nclass VentilatorModel(nn.Module):\n    \n    def __init__(self):\n        super(VentilatorModel, self).__init__()\n        # This embedding method from: https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter-simple-lstm\n        self.r_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.c_emb = nn.Embedding(3, 2, padding_idx=0)\n        self.seq_emb = nn.Sequential(\n            nn.Linear(9, config.HIDDEN_SIZE),\n            nn.LayerNorm(config.HIDDEN_SIZE),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        self.pos_encoder = PositionalEncoding(d_model=config.HIDDEN_SIZE, dropout=0.2)\n        encoder_layers = nn.TransformerEncoderLayer(d_model=config.HIDDEN_SIZE, nhead=8, dim_feedforward=2048, dropout=0.2, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=2)\n        self.head = nn.Linear(config.HIDDEN_SIZE, 1)\n        \n        # Encoder\n        initrange = 0.1\n        self.r_emb.weight.data.uniform_(-initrange, initrange)\n        self.c_emb.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, X, y=None):\n        bs = X.shape[0]\n        r_emb = self.r_emb(X[:,:,0].long()).view(bs, 80, -1)\n        c_emb = self.c_emb(X[:,:,1].long()).view(bs, 80, -1)\n        seq_x = torch.cat((r_emb, c_emb, X[:, :, 2:]), 2)\n        h = self.seq_emb(seq_x)\n        h = self.pos_encoder(h)\n        h = self.transformer_encoder(h)\n        regr = self.head(h)\n        \n        if y is None:\n            loss = None\n        else:\n            loss = self.loss_fn(regr.squeeze(2), y)\n            \n        return regr, loss\n    \n    def loss_fn(self, y_pred, y_true):\n        loss = nn.L1Loss()(y_pred, y_true)\n        return loss","da6f6f48":"def train_loop(model, optimizer, loader):\n    losses, lrs = [], []\n    model.train()\n    optimizer.zero_grad()\n    for d in loader:\n        out, loss = model(d['X'].to(device), d['y'].to(device))\n        \n        losses.append(loss.item())\n        step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n        lrs.append(step_lr)\n        \n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return np.array(losses).mean(), np.array(lrs).mean()\n\ndef valid_loop(model, loader):\n    losses, predicts = [], []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, loss = model(d['X'].to(device), d['y'].to(device))\n        losses.append(loss.item())\n        predicts.append(out.cpu())\n\n    return np.array(losses).mean(), torch.vstack(predicts).squeeze(2).numpy().reshape(-1)\n\ndef test_loop(model, loader):\n    predicts = []\n    model.eval()\n    for d in loader:\n        with torch.no_grad():\n            out, _ = model(d['X'].to(device))\n        predicts.append(out.cpu())\n\n    return torch.vstack(predicts).squeeze(2).numpy().reshape(-1)","2e37ac2f":"from torch.optim.optimizer import Optimizer\nclass Lamb(Optimizer):\n    # Reference code: https:\/\/github.com\/cybertronai\/pytorch-lamb\n\n    def __init__(\n        self,\n        params,\n        lr: float = 1e-3,\n        betas = (0.9, 0.999),\n        eps: float = 1e-6,\n        weight_decay: float = 0,\n        clamp_value: float = 10,\n        adam: bool = False,\n        debias: bool = False,\n    ):\n        if lr <= 0.0:\n            raise ValueError('Invalid learning rate: {}'.format(lr))\n        if eps < 0.0:\n            raise ValueError('Invalid epsilon value: {}'.format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 0: {}'.format(betas[0])\n            )\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\n                'Invalid beta parameter at index 1: {}'.format(betas[1])\n            )\n        if weight_decay < 0:\n            raise ValueError(\n                'Invalid weight_decay value: {}'.format(weight_decay)\n            )\n        if clamp_value < 0.0:\n            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.clamp_value = clamp_value\n        self.adam = adam\n        self.debias = debias\n\n        super(Lamb, self).__init__(params, defaults)\n\n    def step(self, closure = None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    msg = (\n                        'Lamb does not support sparse gradients, '\n                        'please consider SparseAdam instead'\n                    )\n                    raise RuntimeError(msg)\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(\n                        p, memory_format=torch.preserve_format\n                    )\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                # m_t\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n                # v_t\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n\n                # Paper v3 does not use debiasing.\n                if self.debias:\n                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n                    bias_correction \/= 1 - beta1 ** state['step']\n                else:\n                    bias_correction = 1\n\n                # Apply bias to lr to avoid broadcast.\n                step_size = group['lr'] * bias_correction\n\n                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n\n                adam_step = exp_avg \/ exp_avg_sq.sqrt().add(group['eps'])\n                if group['weight_decay'] != 0:\n                    adam_step.add_(p.data, alpha=group['weight_decay'])\n\n                adam_norm = torch.norm(adam_step)\n                if weight_norm == 0 or adam_norm == 0:\n                    trust_ratio = 1\n                else:\n                    trust_ratio = weight_norm \/ adam_norm\n                state['weight_norm'] = weight_norm\n                state['adam_norm'] = adam_norm\n                state['trust_ratio'] = trust_ratio\n                if self.adam:\n                    trust_ratio = 1\n\n                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n\n        return loss","92ec88b0":"def main():\n    train_df = pd.read_csv(f\"{config.INPUT}\/train.csv\")\n    test_df = pd.read_csv(f\"{config.INPUT}\/test.csv\")\n    sub_df = pd.read_csv(f\"{config.INPUT}\/sample_submission.csv\")\n    oof = np.zeros(len(train_df))\n    test_preds_lst = []\n\n    gkf = GroupKFold(n_splits=config.N_FOLD).split(train_df, train_df.pressure, groups=train_df.breath_id)\n    for fold, (_, valid_idx) in enumerate(gkf):\n        train_df.loc[valid_idx, 'fold'] = fold\n\n    train_df['C_cate'] = train_df['C'].map({10: 0, 20: 1, 50:2})\n    train_df['R_cate'] = train_df['R'].map({5: 0, 20: 1, 50:2})\n    test_df['C_cate'] = test_df['C'].map({10: 0, 20: 1, 50:2})\n    test_df['R_cate'] = test_df['R'].map({5: 0, 20: 1, 50:2})\n\n    test_df['pressure'] = -1\n    test_dset = VentilatorDataset(test_df)\n    test_loader = DataLoader(test_dset, batch_size=config.BS,\n                             pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n    \n    for fold in range(config.N_FOLD):\n        if fold in []\n        print(f'Fold-{fold}')\n        train_dset = VentilatorDataset(train_df.query(f\"fold!={fold}\"))\n        valid_dset = VentilatorDataset(train_df.query(f\"fold=={fold}\"))\n\n        set_seed()\n        train_loader = DataLoader(train_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n        valid_loader = DataLoader(valid_dset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=False, drop_last=False, num_workers=os.cpu_count())\n\n        model = VentilatorModel()\n        model.to(device)\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n\n        uniqe_exp_name = f\"{config.EXP_NAME}_f{fold}\"\n        wandb.init(project='Ventilator', entity='trtd56', name=uniqe_exp_name, group=config.EXP_NAME)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        \n        os.makedirs(f'{config.OUTPUT}\/{config.EXP_NAME}', exist_ok=True)\n        model_path = f\"{config.OUTPUT}\/{config.EXP_NAME}\/ventilator_f{fold}_best_model.bin\"\n        \n        valid_best_loss = float('inf')\n        for epoch in tqdm(range(config.N_EPOCHS)):\n\n            train_loss, lrs = train_loop(model, optimizer, train_loader)\n            valid_loss, valid_predict = valid_loop(model, valid_loader)\n            valid_score = np.abs(valid_predict - train_df.query(f\"fold=={fold}\")['pressure'].values).mean()\n\n            if valid_loss < valid_best_loss:\n                valid_best_loss = valid_loss\n                torch.save(model.state_dict(), model_path)\n                oof[train_df.query(f\"fold=={fold}\").index.values] = valid_predict\n\n            wandb.log({\n                \"train_loss\": train_loss,\n                \"valid_loss\": valid_loss,\n                \"valid_best_loss\": valid_best_loss,\n                \"valid_score\": valid_score,\n                \"learning_rate\": lrs,\n            })\n            \n            torch.cuda.empty_cache()\n            gc.collect()\n        \n        model.load_state_dict(torch.load(model_path))\n        test_preds = test_loop(model, test_loader)\n        test_preds_lst.append(test_preds)\n        \n        sub_df['pressure'] = test_preds\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/sub_f{fold}.csv\", index=None)\n        \n    train_df['oof'] = oof\n    train_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/oof.csv\", index=None)\n    \n    if len(config.SKIP_FOLDS) == 0:\n        sub_df['pressure'] = np.stack(test_preds_lst).mean(0)\n        sub_df.to_csv(f\"{config.OUTPUT}\/{config.EXP_NAME}\/submission.csv\", index=None)\n    \n        cv_score = train_df.apply(lambda x: abs(x['oof'] - x['pressure']), axis=1).mean()\n        print(\"CV:\", cv_score)","372c35bd":"if __name__ == \"__main__\":\n    main()","76edae95":"wandb.finish()","fe827342":"## Optimizer\n\nI use Lamb Optimizer. (cf: https:\/\/arxiv.org\/abs\/1904.00962)\n\nThis optimizer can treat large batch sizes efficiently.","41d336c5":"## Dataset Class","c10b73db":"## Transformer Model","60c7ca1e":"# Ventilator Train Transformer\n\n![](https:\/\/user-images.githubusercontent.com\/544269\/27125686-0ccd1b82-5130-11e7-8004-44e357dc9b25.png)\n\nIn this notebook, I train the transformer model.\n\nIf you want to know Transformer, you can see [this paper](https:\/\/arxiv.org\/abs\/1706.03762)."}}