{"cell_type":{"958dcd5c":"code","35e63aed":"code","b6b181bb":"code","9ae84168":"code","577a3721":"code","7578c3e0":"code","01e05df8":"code","de7e3c05":"code","cd775ef6":"code","44af53a7":"code","d1bddd56":"code","7a0f204a":"code","b906c484":"code","b1d3dd3a":"code","e56b6f79":"code","2b640bb0":"markdown","9248853e":"markdown","b57a9f93":"markdown","fff0296e":"markdown","dc29b20e":"markdown","4facfcf6":"markdown","3b21b0f9":"markdown","aefa5db0":"markdown","436d7d63":"markdown","0fc882c1":"markdown","dc7d8f67":"markdown","26c305aa":"markdown","be46b91b":"markdown"},"source":{"958dcd5c":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport xgboost as xgb\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt","35e63aed":"df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain_cutoff = len(df)\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nprint(len(df), len(df_test))\ndf = pd.concat([df, df_test])","b6b181bb":"f = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt', 'r')\nprint(f.read())","9ae84168":"plt.bar(df[\"LandContour\"].fillna(\"0\"),df[\"SalePrice\"])","577a3721":"#replacing categorical hierarchical scales by values\nfor column in [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"PoolQC\"]:\n    df.replace({column : { 'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1 , 'NA' : 0}}, inplace = True)\n\ndf.replace( {\"LotShape\" : { 'IR3' : 1, 'IR2' : 2, 'IR1' : 3, 'Reg' : 4 },\n            \"BsmtExposure\" : { 'Gd' : 3, 'Av' : 2, 'Mn' : 1, 'No' : 0, 'NA' : 0 },\n            \"BsmtFinType1\" : { 'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NA' : 0},\n            \"BsmtFinType2\" : { 'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NA' : 0},\n            \"CentralAir\" : { 'Y' : 1, 'N' : 0},\n            \"PavedDrive\" : { 'Y' : 2, 'P' : 1, 'N' : 0},\n            \"Electrical\" : { 'SBrkr' : 5, 'FuseA' : 4, 'FuseF' : 3, 'FuseP' : 2, 'Mix' : 1},\n            \"Functional\" : { 'Typ' : 7, 'Min1' : 6, 'Min2' : 5, 'Mod' : 4, 'Maj1' : 3, 'Maj2' : 2, 'Sev' : 1, 'Sal' : 0},\n            \"GarageType\" : { '2Types' : 4, 'Attchd' : 3, 'Basment' : 2, 'BuiltIn' : 2, 'CarPort' : 2, 'Detchd' : 1, 'NA' : 0},\n            \"GarageFinish\" : { 'Fin' : 3, 'RFn' : 2, 'Unf' : 1, 'NA' : 0}},\n            inplace = True)\n\n\n\ndf[\"Unrenovated_since\"] = 2021 - df[\"YearRemodAdd\"]","7578c3e0":"dummy_df = pd.get_dummies(df)\n\n#Imputing\ncol_names = dummy_df.columns\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\ndummy_df_vals = imp.fit_transform(dummy_df)\ndummy_df = pd.DataFrame(dummy_df_vals, columns = col_names)\n\ndummy_df.head()","01e05df8":"X = dummy_df.drop(\"SalePrice\", axis = 1).values[:train_cutoff]\ny =  dummy_df[\"SalePrice\"].values[:train_cutoff]","de7e3c05":"def rmse(y, y_hat):\n     return np.sqrt(sum((y-y_hat)**2 \/ len(y)))","cd775ef6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","44af53a7":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X, y)\ny_hat = lr.predict(X)\n\nrmse_lr = rmse(y, y_hat)\nr2 = sklearn.metrics.r2_score(y, y_hat)\nr2, rmse_lr","d1bddd56":"subsample_lst = [0.5, 0.75, 1]\nn_estimators_lst = [100, 200, 300]\nlearning_rate_lst = [0.2, 0.1, 0.05]\ncolsample_bytree_lst = [0.3, 0.5, 0.8]\nreg_alpha_lst = [.2, .5, .8]\nreg_lambda_lst = [.2, .5, .8]","7a0f204a":"'''\nperformance_lst = []\n\nfor subsample in subsample_lst:\n    for n_estimators in n_estimators_lst:\n        for learning_rate in learning_rate_lst:\n            for colsample_bytree in colsample_bytree_lst:\n                for reg_alpha in reg_alpha_lst:\n                    for reg_lambda in reg_lambda_lst:\n                        \n                        \n                        \n                        xgbr = xgb.XGBRegressor(verbosity=0, \n                                                subsample = subsample,\n                                                n_estimators = n_estimators,\n                                                learning_rate = learning_rate,\n                                                colsample_bytree = colsample_bytree,\n                                                reg_alpha = reg_alpha,\n                                                reg_lambda = reg_lambda)\n\n                        xgbr.fit(X_train, y_train)\n\n                        y_hat = xgbr.predict(X_train)\n                        rmse_tr = rmse(y_train, y_hat)\n                        #print(f\"Train: {xgbr.score(X_train, y_train), rmse_tr}\")\n\n                        y_hat_test = xgbr.predict(X_test)\n                        rmse_te = rmse(y_test, y_hat_test)\n\n                        #print(f\"Test: {xgbr.score(X_test, y_test), rmse_te}\")\n                        \n                        performance_lst.append([rmse_te, rmse_tr, subsample, n_estimators, learning_rate, colsample_bytree, reg_alpha, reg_lambda, xgbr.score(X_test, y_test)])\n                        \n    \nperformance_lst = sorted(performance_lst, key = itemgetter(0))\n\nperformance_lst[0]\n'''","b906c484":"performance_lst = [[18393.655464095325,\n 5392.206565641212,\n 0.5,\n 300,\n 0.05,\n 0.8,\n 0.8,\n 0.8,\n 0.9355624485460461]]","b1d3dd3a":"xgbr_best_params = xgb.XGBRegressor(verbosity=0, \n                                    subsample = performance_lst[0][2],\n                                    n_estimators = performance_lst[0][3],\n                                    learning_rate = performance_lst[0][4],\n                                    colsample_bytree = performance_lst[0][5],\n                                    reg_alpha = performance_lst[0][6],\n                                    reg_lambda = performance_lst[0][7])\n\nxgbr_best_params.fit(X, y)","e56b6f79":"X_test = dummy_df.drop(\"SalePrice\", axis = 1).values[train_cutoff:]\nindex = np.arange(train_cutoff+1, train_cutoff+len(X_test))\n\nfinal_pred = xgbr_best_params.predict(X_test)\n#final_pred = lr.predict(X_test)\n\nsubmission_df = pd.DataFrame(list(zip(df_test[\"Id\"].values, final_pred)), columns = [\"Id\",\"SalePrice\"])\nsubmission_df.to_csv(\"house_sale_prediction_2.csv\", index = False)","2b640bb0":"# XGBoost","9248853e":"### Import of data\nData is concatenated for parallel preprocessing and split later for predictions","b57a9f93":"# House Prices Prediction\nThis notebook aims to get decent results in the House Prices Kaggle Competitions.  \nThere is room for improvement in various areas, including\n- Feature engineering\n- Hyperparameter tuning\n- Imputing\n\n**Suggestions are appreciated!**","fff0296e":"### Grid search for hyperparameter tuning","dc29b20e":"# Simple linear regression","4facfcf6":"### Training on the entire dataset","3b21b0f9":"### Basic RMSE function","aefa5db0":"### Train test split","436d7d63":"### Creation of Dummy Variables, Imputing","0fc882c1":"### Import of variable descriptions","dc7d8f67":"### This is the hyperparameter combination which led to best performance","26c305aa":"### Prediction of test.csv-data","be46b91b":"### Some replaces for turning categoricals into numerics"}}