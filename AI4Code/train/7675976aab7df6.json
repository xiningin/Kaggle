{"cell_type":{"85725235":"code","2f9ae5c7":"code","2c2d47b0":"code","cb358071":"code","5ed64a56":"code","8f65f920":"code","beb3675d":"code","1cb52a78":"code","a432721d":"code","28bc1e45":"code","8e87afcd":"code","8f7d0d80":"code","cad7c44b":"code","d4477d5a":"code","097e81e7":"code","2229f7fe":"code","3bfb5fc3":"code","1bd60815":"code","5d1b1a46":"code","1c27e10e":"code","b3dedd8f":"code","47c29440":"code","f086d1f5":"code","ee8f16b4":"code","6e6ecff2":"code","5a947298":"code","9e1049f4":"code","1a346264":"code","008d3759":"code","8d2af0a1":"code","9dc45f6a":"code","c0a955f4":"code","149d31c7":"code","2fa5e597":"code","ac7a5531":"code","1d17053c":"code","81b12375":"code","23f4f382":"code","16e4fd9d":"code","87d3b0fa":"code","a0b48856":"code","256f78ea":"code","b5418198":"markdown","b817a301":"markdown","b9a5a8dd":"markdown","192f2580":"markdown","d4028e21":"markdown","ca2c4aed":"markdown","3ca5891e":"markdown","a06090e9":"markdown","d2144199":"markdown","25c6642a":"markdown","5e04e597":"markdown","415e1fed":"markdown","57d82734":"markdown","05d36b98":"markdown","2bde5e4c":"markdown","9337bf16":"markdown","a10473bb":"markdown","fc898b64":"markdown","cd9071f7":"markdown"},"source":{"85725235":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport warnings \n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f9ae5c7":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\nconcat_df = pd.concat([test_df, train_df], ignore_index = True)\nconcat_df = concat_df.sample(frac=1, ignore_index = True)","2c2d47b0":"concat_df","cb358071":"concat_df.sample(5)","5ed64a56":"concat_df.info()","8f65f920":"concat_df.describe()","beb3675d":"concat_df.duplicated().sum()","1cb52a78":"null = pd.DataFrame({'null':concat_df.isna().sum(), 'pct_null': round(concat_df.isna().sum() \/ len(concat_df), 5) * 100})\nnull = null.sort_values('pct_null', ascending=False)\nnull","a432721d":"fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(20,6))\nplt.suptitle('Null Counts', fontsize=20)\n\nax1.bar(null.index, null.null)\nax2.bar(null.index, null.pct_null)\n\nplt.tight_layout()\nplt.show()","28bc1e45":"targets = ['0.0', '1.0', 'null']\ntarget_count = [i for i in concat_df.target.value_counts()]\ntarget_count.append(concat_df.target.isna().sum())\n\ntarget_count","8e87afcd":"fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(20,6))\nplt.suptitle('Target Counts', fontsize=20)\n\nax1.bar(targets, target_count)\nax2.pie(target_count, labels=targets, explode=[0.0, 0.0, 0.1], startangle=90, shadow=True, autopct='%1.1f%%')\n\nplt.tight_layout()\nplt.show()","8f7d0d80":"class FE:\n    def __init__(self, df):\n        self.df = df\n        \n    def add_column(self):\n        column = Column(self.df)\n        column.add_all()\n    \n\nclass Column(FE):\n    \n    def __init__(self, df):\n        super().__init__(df)\n    \n    def add_len(self):\n        self.df['text_len'] = self.df.text.apply(lambda x: len(x))\n        \n    def add_tags(self):\n        regex = \"#(\\w+)\"\n        for index, text in enumerate(self.df.text):\n            tags = re.findall(regex, text)\n            if len(tags) == 0:\n                self.df.loc[index, 'hashtags'] = None\n                self.df.loc[index, 'hashtags_count'] = 0\n            else:\n                self.df.loc[index, 'hashtags'] = ', '.join(tags)\n                self.df.loc[index, 'hashtags_count'] = len(tags)\n        \n    def add_mention(self):\n        regex = \"@(\\w+)\"\n        for index, text in enumerate(self.df.text):\n            mention = re.findall(regex, text)\n            if len(mention) == 0:\n                self.df.loc[index, 'mention'] = None\n                self.df.loc[index, 'mention_count'] = 0\n            else:\n                self.df.loc[index, 'mention'] = ', '.join(mention)\n                self.df.loc[index, 'mention_count'] = len(mention)\n\n    def add_clean_text(self):\n        for index, text in enumerate(self.df.text):\n            clean_text = re.sub(r'http\\S+', '', text)\n            clean_text = re.sub(r'[^A-Za-z0-9]+', ' ', clean_text)\n            clean_text = re.sub(r'@(\\w+)', ' ', clean_text)\n            clean_text = re.sub(r'\\w*\\d\\w*', '', clean_text)\n            clean_text = clean_text.strip()\n            clean_text = re.sub(\"\\s\\s+\", \" \", clean_text)\n            self.df.loc[index, 'clean_text'] = clean_text.lower()\n            \n    def add_has_location(self):\n        for index, location in enumerate(self.df.location):\n            if isinstance(location, str):\n                self.df.loc[index, 'has_location'] = 1\n            else:\n                self.df.loc[index, 'has_location'] = 0\n                \n    def add_has_key(self):\n        for index, location in enumerate(self.df.keyword):\n            if isinstance(location, str):\n                self.df.loc[index, 'has_key'] = 1\n            else:\n                self.df.loc[index, 'has_key'] = 0\n                \n    def add_all(self):\n        self.add_len()\n        self.add_tags()\n        self.add_mention()\n        self.add_clean_text()\n        self.add_has_location()\n        self.add_has_key()\n        \n        return self.df","cad7c44b":"fe = FE(train_df)\nfe.add_column()\n\nfe = FE(test_df)\nfe.add_column()","d4477d5a":"fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(20,6))\n\nplt.suptitle('Tweet Text Length', fontsize=20)\nsns.boxplot(ax=ax1, data=train_df, x='text_len')\nsns.histplot(ax=ax2, data=train_df, x='text_len', hue='target', multiple='stack')\n\nplt.tight_layout()\nplt.show()","097e81e7":"train_df = train_df[['id', 'keyword','location', 'has_key', 'has_location', 'hashtags', 'hashtags_count', 'mention', 'mention_count', 'text_len', 'target', 'text', 'clean_text']]","2229f7fe":"sns.countplot(data=train_df, x='has_key', hue='target', edgecolor='black')\nplt.title('Has keywords count', fontsize=16)\nplt.tight_layout()","3bfb5fc3":"sns.countplot(data=train_df, x='has_location', hue='target', edgecolor='black')\nplt.title('Has location count', fontsize=16)\nplt.tight_layout()","1bd60815":"fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(18,5))\nplt.suptitle('Hashtags count', fontsize=20)\n\nsns.histplot(ax=ax1, data=train_df, x='hashtags_count', hue='target', multiple='stack')\nsns.boxplot(ax=ax2, data=train_df, x='hashtags_count')\n\nplt.tight_layout()","5d1b1a46":"fig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(18,5))\nplt.suptitle('Mention count', fontsize=20)\n\nsns.countplot(ax=ax1, data=train_df, x='mention_count', edgecolor='black')\nsns.boxplot(ax=ax2, data=train_df, x='mention_count')\n\nplt.tight_layout()","1c27e10e":"top5_keys = train_df.keyword.value_counts()[:50].index.tolist()\ntop5_count = train_df.keyword.value_counts()[:50].tolist()","b3dedd8f":"plt.figure(figsize=(20,16))\nplt.title('Top 50 Keywords', fontsize=20)\n\nsns.barplot(x=top5_count, y=top5_keys, palette='rocket')\n\nplt.tight_layout()","47c29440":"target1_words = ''\ntarget0_words = ''\n\nstopwords = set(STOPWORDS)\n\nfor i in train_df[train_df.target == 1.0].clean_text:\n    target1_words += i\n\nfor i in train_df[train_df.target == 0.0].clean_text:\n    target0_words += i\n\n\nfig, (ax1, ax2) = plt.subplots(ncols=2, nrows=1, figsize=(16,7))    \n\nwordcloud1 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(target1_words)\n\nwordcloud0 = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(target0_words)\n \n    \nax1.imshow(wordcloud1)\nax1.set_title('Target 1 Word Cloud', fontsize=18)\n\nax2.imshow(wordcloud0)\nax2.set_title('Target 0 Word Cloud', fontsize=18)\n    \nplt.show()","f086d1f5":"print(train_df.target.value_counts())","ee8f16b4":"class Data:\n    def __init__(self, data):\n        self.data = data\n        \n    def fix(self):\n        minimum = min(train_df.target.value_counts().tolist())\n        \n        fixed_data = pd.concat([self.data[self.data.target == 1.0][:minimum],\n                               self.data[self.data.target == 0.0][:minimum]])\n        \n        fixed_data = fixed_data.sample(frac=1).reset_index()\n        return fixed_data","6e6ecff2":"data = Data(train_df)\ntrain_df = data.fix()\n\ntrain_df.target.value_counts()","5a947298":"X = train_df.clean_text\ny = train_df.target\n\nfrom sklearn.model_selection import train_test_split\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.25, random_state=1)","9e1049f4":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\n\ntrain_x_vectors = vectorizer.fit_transform(train_x)\ntest_x_vectors = vectorizer.transform(test_x)","1a346264":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","008d3759":"class Model:\n    def __init__(self, model):\n        self.model = model\n    \n    def predict(self):\n        self.model.fit(train_x_vectors, train_y)\n        model_pred = self.model.predict(test_x_vectors)\n        self.get_score(model_pred)\n        \n    def get_score(self, model_pred):\n        model_acc = accuracy_score(model_pred, test_y)\n        model_report = classification_report(model_pred, test_y)        \n        model_f1 = f1_score(model_pred, test_y)        \n        model_cv = cross_val_score(self.model, train_x_vectors, train_y, cv=5)\n        \n        print(f'Accuracy Score: {model_acc}\\nMean Cross Validation: {np.mean(model_cv)}\\n\\n{model_report}\\nf1_score: {model_f1}')","8d2af0a1":"from sklearn.svm import LinearSVC\nmodel = Model(LinearSVC())\nmodel.predict()","9dc45f6a":"from sklearn.svm import SVC\nmodel = Model(SVC())\nmodel.predict()","c0a955f4":"from sklearn.ensemble import RandomForestClassifier\nmodel = Model(RandomForestClassifier())\nmodel.predict()","149d31c7":"from sklearn.tree import DecisionTreeClassifier\nmodel = Model(DecisionTreeClassifier())\nmodel.predict()","2fa5e597":"from sklearn.neighbors import KNeighborsClassifier\nmodel = Model(KNeighborsClassifier())\nmodel.predict()","ac7a5531":"from sklearn.linear_model import LogisticRegression\nmodel = Model(LogisticRegression())\nmodel.predict()","1d17053c":"from sklearn.naive_bayes import MultinomialNB\nmodel = Model(MultinomialNB())\nmodel.predict()","81b12375":"model = LogisticRegression()\nparams = {\n    'solver':['newton-cg', 'lbfgs', 'liblinear'],\n    'penalty':['l2'],\n    'C':[100, 10, 1.0, 0.1, 0.01]\n}\n\nlogistic = GridSearchCV(estimator=model, param_grid=params, cv=5)\n\nlogistic.fit(train_x_vectors, train_y)\nlogistic_pred = logistic.predict(test_x_vectors)\n\nscore = pd.DataFrame(logistic.cv_results_)","23f4f382":"print(f'Accuracy Score: {accuracy_score(logistic_pred, test_y)}')","16e4fd9d":"score[['param_solver', 'param_C', 'param_penalty', 'mean_test_score']].sort_values('mean_test_score', ascending=False)","87d3b0fa":"from sklearn.feature_extraction.text import TfidfVectorizer\nvec = TfidfVectorizer()\n\ntrain_vec = vec.fit_transform(train_df.clean_text)\ntest_vec = vec.transform(test_df.clean_text)","a0b48856":"final_model = LogisticRegression()\nfinal_model.fit(train_vec, train_df.target)\n\nmodel_pred = final_model.predict(test_vec)","256f78ea":"output = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\noutput['target'] = model_pred\n\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","b5418198":"### **Naive Bayesian**","b817a301":"# **Creating Models**","b9a5a8dd":"### **Support Vector Machine**","192f2580":"### **Logistic Regression**","d4028e21":"# **Data Preprocessing**","ca2c4aed":"### **Balance the number of target 0 and 1**","3ca5891e":"### **Decision Tree Classifier**","a06090e9":"### **Linear SVC**","d2144199":"## Null values count","25c6642a":"### **Vectorize Text**","5e04e597":"# **Exploratory Data Analysis**","415e1fed":"### **Random Forest Classifier**","57d82734":"# **Hyperparameter Tuning**\nI'll be using logistic regression since it has the highest cross validation score","05d36b98":"# **Feature Engineering**","2bde5e4c":"### **Train Test Split**","9337bf16":"### **K-NN**","a10473bb":"# **Submit Result**","fc898b64":"# **Data Visualization**","cd9071f7":"## Target distribution"}}