{"cell_type":{"3f906b0f":"code","bc3eb7ca":"code","c5c0ec50":"code","a0d58459":"code","78e03d6d":"code","3a5e56f4":"code","1c3e2880":"code","d9d2941e":"code","b7de4f12":"code","f6887bc9":"code","1072cde7":"code","b9953842":"code","3e89c27e":"code","3d73154e":"code","4043e9e8":"code","ead8ea1e":"code","158d7b81":"code","4de2c49c":"code","5aac4434":"code","e183d405":"code","8028dea7":"code","b904d8b1":"code","97fd2b5f":"code","ad2cba33":"code","c7dc190b":"code","cdeb544b":"code","693e3759":"code","02af9329":"code","161e4c7e":"markdown","437bf30c":"markdown"},"source":{"3f906b0f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport cv2\nimport pickle\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, LSTM, Input, Embedding, Conv2D,Concatenate,Flatten,Add,Dropout,GRU\nimport random\nimport datetime\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom math import log\nimport warnings\nwarnings.filterwarnings('ignore')","bc3eb7ca":"train_dataset = pd.read_csv('Train_Data.csv')\ncv_dataset = pd.read_csv('CV_Data.csv')\ntest_dataset = pd.read_csv('Test_Data.csv')","c5c0ec50":"X_train, X_test, X_cv = train_dataset['Person_id'], test_dataset['Person_id'], cv_dataset['Person_id'][:546]\ny_train, y_test, y_cv = train_dataset['Report'], test_dataset['Report'], cv_dataset['Report'][:546]","a0d58459":"max_capt_len = 153\npad_size = max_capt_len ","78e03d6d":"tokenizer = Tokenizer(filters='!\"#$%&()*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(y_train.values)\n\ntrain_rep_tok = tokenizer.texts_to_sequences(y_train)\ncv_rep_tok = tokenizer.texts_to_sequences(y_cv)\ntest_rep_tok = tokenizer.texts_to_sequences(y_test)\n\ntrain_rep_padded = pad_sequences(train_rep_tok, maxlen=153, padding='post')\ncv_rep_padded = pad_sequences(cv_rep_tok, maxlen=153, padding='post')\ntest_rep_padded = pad_sequences(test_rep_tok, maxlen=153, padding='post')\n\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","3a5e56f4":"f = open('Image_features_attention.pickle','rb') # contains the features from chexNet\nXnet_Features = pickle.load(f)\nf.close()","1c3e2880":"f = open('GLOVE_VECTORS.pickle','rb') # 300d glove vectors  \nglove_vectors = pickle.load(f)\nf.close()","d9d2941e":"a = Xnet_Features['Scanned Images\/CXR1_1_IM-0001_0'][0]\na.shape","b7de4f12":"BATCH_SIZE = 14\nBUFFER_SIZE = 500","f6887bc9":"def load_image(id_, report):\n    '''Loads the Image Features with their corresponding Ids'''\n    img_feature = Xnet_Features[id_.decode('utf-8')][0]\n    return img_feature, report","1072cde7":"def create_dataset(img_name_train,reps):\n  \n    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, reps))\n\n  # Use map to load the numpy files in parallel\n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(load_image, [item1, item2], [tf.float32, tf.int32]),\n                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n  # Shuffle and batch\n    dataset = dataset.shuffle(500).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset","b9953842":"train_generator = create_dataset(X_train.values, train_rep_padded)\ncv_generator = create_dataset(X_cv.values, cv_rep_padded)","3e89c27e":"vocab_size = len(tokenizer.word_index.keys()) + 1\n\nembedding_matrix = np.zeros((vocab_size,300))\nfor word, i in tokenizer.word_index.items():\n    if word in glove_vectors.keys():\n        vec = glove_vectors[word]\n        embedding_matrix[i] = vec\n    else:\n        continue","3d73154e":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Encoder, self).__init__()\n        self.units = units\n       # self.bs = batch_size\n        \n    def build(self, input_shape):\n        self.maxpool = tf.keras.layers.MaxPool1D()\n        self.dense = Dense(self.units, kernel_initializer=tf.keras.initializers.glorot_uniform(seed = 56), name='dense_encoder')\n        \n    def call(self, input_, training=True):\n        \n        x = self.maxpool(input_)\n        x = self.dense(x)\n        \n        return x\n    \n    def get_states(self, bs):\n        \n        return tf.zeros((bs, self.units))","4043e9e8":"class OneStepDecoder(tf.keras.layers.Layer):\n    def __init__(self, vocab_size, att_units, dec_units):\n        super(OneStepDecoder, self).__init__()\n        self.vocab_size = vocab_size\n       # self.emb_dim = emb_dim\n        self.att_units = att_units\n        self.dec_units = dec_units\n        \n    def build(self, input_shape):\n        self.embedding = Embedding(self.vocab_size, output_dim=300, input_length=max_capt_len, mask_zero=True,\n                                   weights = [embedding_matrix],\n                                   name=\"embedding_layer_decoder\")\n        self.gru = GRU(self.dec_units, return_sequences=True, return_state=True, name=\"Decoder_GRU\")\n        self.fc = Dense(self.vocab_size)\n        \n        self.V = Dense(1)\n        self.W = Dense(self.att_units)\n        self.U = Dense(self.att_units)\n        \n    def call(self, dec_input, hidden_state, enc_output):\n       \n\n        hidden_with_time = tf.expand_dims(hidden_state, 1)\n        attention_weights = self.V(tf.nn.tanh(self.U(enc_output) + self.W(hidden_with_time)))        \n        attention_weights = tf.nn.softmax(attention_weights, 1)\n        context_vector = attention_weights * enc_output\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n       \n\n        x = self.embedding(dec_input)\n        x = tf.concat([tf.expand_dims(context_vector, axis=1),x], axis=-1)\n        output, h_state = self.gru(x, initial_state = hidden_state)\n        output = tf.reshape(output, (-1, output.shape[2]))\n        x = self.fc(output)\n        \n        return x, h_state, attention_weights","ead8ea1e":"class Decoder(tf.keras.layers.Layer):\n    \n    def __init__(self, vocab_size, input_length, dec_units, att_units):\n        super(Decoder, self).__init__()\n        self.vocab_size = vocab_size\n    #    self.embedding_dim = embedding_dim\n        self.input_length = input_length\n        self.dec_units = dec_units\n        self.att_units = att_units\n        self.onestep_decoder = OneStepDecoder(self.vocab_size, self.att_units, self.dec_units)\n    @tf.function    \n    def call(self, dec_input, hidden_state, enc_output):\n        all_outputs = tf.TensorArray(tf.float32, dec_input.shape[1], name='output_arrays')\n        \n        for timestep in range(dec_input.shape[1]):\n            \n            output, hidden_state, attention_weights = self.onestep_decoder(dec_input[:, timestep:timestep+1], \n                                                                           hidden_state, enc_output)\n            \n            all_outputs = all_outputs.write(timestep, output)\n            \n        all_outputs = tf.transpose(all_outputs.stack(), [1,0,2])\n        return all_outputs","158d7b81":"class Attention_Model(tf.keras.Model):\n    def __init__(self, vocab, units, max_capt_len, att_units, batch_size):\n        super(Attention_Model, self).__init__()\n        self.batch_size = batch_size\n        self.encoder = Encoder(units)\n        self.decoder = Decoder(vocab_size, max_capt_len, units, att_units)\n        \n    def call(self, data):\n        enc_input, dec_input = data[0], data[1]\n    \n        enc_output = self.encoder(enc_input)\n        enc_state = self.encoder.get_states(self.batch_size)\n        dec_output = self.decoder(dec_input, enc_state, enc_output)\n\n        return dec_output","4de2c49c":"units = 256\natt_units = 10","5aac4434":"model1 = Attention_Model(vocab_size, units, max_capt_len, att_units, BATCH_SIZE)","e183d405":"optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nloss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='auto')\n\ndef maskedLoss(y_true, y_pred):\n    #getting mask value\n    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n    \n    #calculating the loss\n    loss_ = loss_function(y_true, y_pred)\n    \n    #converting mask dtype to loss_ dtype\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    \n    #applying the mask to loss\n    loss_ = loss_*mask\n    \n    #getting mean over all the values\n    loss_ = tf.reduce_mean(loss_)\n    return loss_","8028dea7":"model1.compile(optimizer=optimizer, loss=maskedLoss)","b904d8b1":"EPOCHS = 10","97fd2b5f":"current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntrain_log_dir = 'Tensorboard\/attention_OneStep\/fit2\/' + current_time + '\/train'\nval_log_dir = 'Tensorboard\/attention_OneStep\/fit2\/' + current_time + '\/test'\ntrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\nval_summary_writer = tf.summary.create_file_writer(val_log_dir)","ad2cba33":"epoch_train_loss = []\nepoch_val_loss = []\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    print(\"EPOCH: \", epoch+1)\n    batch_loss_tr = 0\n    batch_loss_val = 0\n#    print('Training...')\n    for img, rep in train_generator:\n        res = model1.train_on_batch([img, rep[:,:-1]], rep[:,1:])\n        batch_loss_tr += res\n        \n    train_loss = batch_loss_tr\/(X_train.shape[0]\/BATCH_SIZE)\n\n    with train_summary_writer.as_default():\n        tf.summary.scalar('loss', train_loss, step = epoch)\n    \n#    print(\"VALIDATING..\")\n    for img, rep in cv_generator:\n        res = model1.test_on_batch([img, rep[:,:-1]], rep[:,1:])\n        batch_loss_val += res\n        \n    val_loss = batch_loss_val\/(X_cv.shape[0]\/BATCH_SIZE)\n\n    with val_summary_writer.as_default():\n        tf.summary.scalar('loss', val_loss, step = epoch)    \n        \n    epoch_train_loss.append(train_loss)\n\n    epoch_val_loss.append(val_loss)\n    \n    print('Training Loss: {},  Validation Loss: {}'.format(train_loss, val_loss))\n    print('Time Taken for this Epoch : {} sec'.format(time.time()-start))   \n    model1.save_weights('Weights\/Attention\/OneStep\/epoch_'+ str(epoch+1) + '.h5')","c7dc190b":"def inference_concat(inputs):\n    \n    in_ = len(inputs.split()) - 1\n    inputs = Xnet_Features[inputs]\n    enc_state = tf.zeros((1, 256))\n    enc_output = model1.layers[0](inputs)\n    input_state = enc_state\n    pred = []\n    cur_vec = np.array([tokenizer.word_index['startseq']]).reshape(-1,1)\n\n    for i in range(153):\n\n        inf_output, input_state, attention_weights = model1.layers[1].onestep_decoder(cur_vec, input_state, enc_output)\n\n        cur_vec = np.reshape(np.argmax(inf_output), (1, 1))\n        if cur_vec[0][0] != 0:\n            pred.append(cur_vec)\n        else:\n            break\n\n    final = ' '.join([tokenizer.index_word[e[0][0]] for e in pred if e[0][0] != 0 and e[0][0] != 7])\n    return final#, att_weights","cdeb544b":"a = inference_concat(X_cv.values[72])","693e3759":"y_cv.values[72]  # original","02af9329":"a  # predicted","161e4c7e":"The attention model is already giving decent outputs within just 10 epochs of training!!","437bf30c":"# Medical Report generation using DL (Classifying into 14 diseases)\nLink to image data(contains X-rays): https:\/\/academictorrents.com\/details\/5a3a439df24931f410fac269b87b050203d9467d<br>\nLink to report data(contains corresponding reports): https:\/\/academictorrents.com\/details\/66450ba52ba3f83fbf82ef9c91f2bde0e845aba9"}}