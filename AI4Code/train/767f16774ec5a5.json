{"cell_type":{"e69159b5":"code","2dab23f6":"code","2f9c2465":"code","6be06917":"code","5a71ea9a":"markdown","653cec66":"markdown"},"source":{"e69159b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2dab23f6":"#Define data\ntrain = pd.read_csv('\/kaggle\/input\/elte-photometric-redshift-estimation-2020\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/elte-photometric-redshift-estimation-2020\/testX.csv')\nsample = pd.read_csv('\/kaggle\/input\/elte-photometric-redshift-estimation-2020\/sample.csv')","2f9c2465":"import logging \n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\n\n\nclass BasePipeLine:\n    def __init__(self, train, test, sample):\n        self.train = train\n        self.test = test\n        self.sample = sample\n        self.KNNR = KNeighborsRegressor()\n        self.DTR = DecisionTreeRegressor()\n        self.SVR = SVR()\n        self.SGDR = SGDRegressor()\n        self.algorithms = [self.KNNR, self.DTR, self.SVR, self.SGDR]\n#         self.indexes = [alg.__class__.__name__ for alg in self.algorithms]\n#         self.columns = ['score', 'best_params']\n        self.params_dict = {\n            'KNeighborsRegressor': {'n_neighbors':list(range(3, 11))},\n            'SVR' : {'kernel': ('linear', 'rbf','poly'), 'C':[1.5, 10],'gamma': [1e-7, 1e-4],'epsilon':[0.1,0.2,0.3]},\n            'DecisionTreeRegressor' : {\"min_samples_split\": [10, 20], \"max_depth\": [2, 6]},\n            'SGDRegressor': {'alpha': 10.0 ** -np.arange(1, 7), 'penalty': ['l2', 'l1', 'elasticnet'], 'learning_rate': ['constant', 'optimal', 'invscaling']}\n        }\n            \n            \n    def data_transform(self):\n        X = self.train.drop(['redshift'], axis=1)\n        y = self.train['redshift']\n        X_test = self.test.drop(['ID'], axis=1)\n        y_test = self.sample.drop(['ID'], axis=1).values\n        return X, y, X_test, y_test\n    \n    \n    def grid_search_cv(self, X_train, y_train):\n        best_models = [] \n        for algorithm in self.algorithms:\n            clf = GridSearchCV(estimator=algorithm, param_grid=self.params_dict['{}'.format(algorithm.__class__.__name__)], cv=3, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n            clf.fit(X_train, y_train)\n            best_models.append(clf.best_estimator_)\n        return best_models\n    \n    \n    def make_prediction(self, best_models, X_test, y_test):\n        evaluation = []\n        for model in best_models:\n            predictions = model.predict(X_test)\n            evaluation.append(mean_squared_error(y_test, predictions))\n        return evaluation\n        #return predictions \n    \n    \n    def transform_to_submission_csv(self, predictions):\n        submission_df = pd.DataFrame(columns=['ID', 'redshift'])\n        submission_df['redshift'] = predictions\n        submission_df['ID'] = submission_df['redshift'].index\n        submission_df.to_csv('\/kaggle\/working\/submission.csv', index = False)\n    \n    \n    def run(self):\n        X, y, X_test, y_test = BasePipeLine(train, test, sample).data_transform()\n        logging.info('Transformation finished')\n        \n        best_models = BasePipeLine.grid_search_cv(self, X, y)\n        logging.info('Fitting models finished')\n        \n        BasePipeLine.make_prediction(best_models, X_test, y_test)\n        #predictions = BasePipeLine.make_prediction(best_model, X_test, y_test)\n        logging.info('Evaluating Finished')\n        \n#         BasePipeLine.transform_to_submission_csv(predictions)\n#         logging.info('BasePipeLine Calculations Finished')","6be06917":"#BasePipeLine(train[:100], test[:100], sample[:100]).run()","5a71ea9a":"### BaseLine\n\nThe main part is ready as well as parameters dictionary.<br>\nThe ```output``` will return list of 4 values based on 4 best param_grid of algorithms. One of them will perform better than others. <br>\nActually ```this baseline can be freely used by you```. In days I will finish rest part with output and transform_to_csv.<br> \n\n<strong>Be considerable the calculations can take time, especially with full data capacity. To decrease it, customize params_dict with less hyper parameters<\/strong>\n\n<h3>Happy Coding<\/h3>","653cec66":"### Brief explanation of data:\n - Target column (variable): ```train.redshift```, can be found in train data.\n - Column ```test.ID``` in test data actually is replication of the index of DataFrame length and can be ignored for a while. We can add it in submission made dublicating index value.\n \n### Beating baselines\n - Basically we have usual ```Regression``` task, cause our target value is continuous. So we can firstly try to deploy all the standart algorithms for Regression.  \n \n### Algorithms. Simple Ones.\n\n1. SVM \n2. Stochastic Gradient Descent\n3. KNN\n4. Dicision Trees"}}