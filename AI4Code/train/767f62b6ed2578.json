{"cell_type":{"dc2baf96":"code","a2d0f7c2":"code","8eba8af1":"code","efe947c5":"code","6bd66ceb":"code","24a5c9ca":"code","3982ec90":"code","4b69dd78":"code","acc11286":"code","d9cae743":"code","9296c821":"code","6c129bd6":"code","258ca14b":"code","d2ae85ef":"code","d3e9c377":"code","9b97b51e":"code","905dfaab":"code","1dcbc821":"code","3e922fcb":"code","a835398a":"code","f26d2006":"code","3e9d7398":"code","e9f6c5ea":"code","e6461b51":"code","6d8bbef8":"code","bc7944c8":"code","c23f91e5":"code","25c2be2c":"code","9e2eaa25":"code","b83a89ad":"code","ad86a284":"code","b13ec831":"code","c2c205f1":"code","41608457":"code","36bd023c":"code","9d5299d8":"code","9b997972":"markdown","fb11644b":"markdown","6c44c4f3":"markdown","7a83f7c6":"markdown","20eea951":"markdown","8f7f39dc":"markdown","504e9567":"markdown","5160cc8d":"markdown","fa981efb":"markdown","ef0a716d":"markdown","709a7a62":"markdown","10a6b0ac":"markdown","88a8c149":"markdown","f3a1d3df":"markdown","30eda14c":"markdown","46db7e37":"markdown","aa097acc":"markdown","3e018777":"markdown","b6fb20b9":"markdown","3bae22c3":"markdown","d71e73b0":"markdown","7f219e9d":"markdown","b1d30bf6":"markdown","648deaee":"markdown","70b0994f":"markdown","592609ed":"markdown","1f43faf1":"markdown","22060c80":"markdown","21fbe53b":"markdown","85e56a8c":"markdown","88ea0c1c":"markdown","b9bfd441":"markdown","61ca526d":"markdown","a2c09f1b":"markdown","52d37f67":"markdown","e3948d89":"markdown","d77ffdba":"markdown","8913a36d":"markdown","489e5279":"markdown","c503d56c":"markdown","9a8c2669":"markdown","2c46c88e":"markdown","d15afad3":"markdown","5da83afc":"markdown","ba36b294":"markdown","a4692c82":"markdown","bd87615a":"markdown","f7e455ea":"markdown","168532b3":"markdown","64fd7a6f":"markdown","e95f71d7":"markdown","2b034adb":"markdown","52d7a894":"markdown","9c615af7":"markdown","f22c8ff6":"markdown","4130a735":"markdown"},"source":{"dc2baf96":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport lightgbm as lgb\npd.options.mode.chained_assignment = None","a2d0f7c2":"df = pd.read_csv('https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00510\/Grisoni_et_al_2016_EnvInt88.csv')\ndf","8eba8af1":"df.SMILES.value_counts()","efe947c5":"import re\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ndf['SMILES'] = df['SMILES'].apply(lambda x:re.sub('[^A-Za-z0-9_]+', '', x))","6bd66ceb":"df.SMILES.value_counts()","24a5c9ca":"df.head()","3982ec90":"df.Set.value_counts(100)","4b69dd78":"x = df.drop(['Class','Set'],axis=1)\ny = df[['Class']]","acc11286":"x,y","d9cae743":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,\n                                                    y,\n                                                    test_size = 0.25,\n                                                    random_state=29)","9296c821":"pd.DataFrame(np.arange(12).reshape((4,3)),columns=['a', 'b', 'c'])","6c129bd6":"xgb.DMatrix(pd.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c']))","258ca14b":"xgb.DMatrix(X_train)","d2ae85ef":"xgb.DMatrix(pd.get_dummies(df['Set']))","d3e9c377":"lgb.LGBMClassifier(boosting_type='gbdt',n_jobs=-1,).fit(X_train.select_dtypes('float'),np.ravel(y_train))","9b97b51e":"lgb.LGBMClassifier(n_jobs=-1).fit(X_train,np.ravel(y_train))","905dfaab":"X_train[['CAS','SMILES']] = X_train[['CAS','SMILES']].apply(lambda x: x.astype('category'),axis=0)\nX_test[['CAS','SMILES']] = X_test[['CAS','SMILES']].apply(lambda x: x.astype('category'),axis=0)","1dcbc821":"X_train.dtypes","3e922fcb":"lgb.LGBMClassifier(n_jobs=-1).fit(X_train,np.ravel(y_train))","a835398a":"X_train_na = X_train.copy()","f26d2006":"X_train_na.isna().sum().sum()","3e9d7398":"X_train_na = X_train_na.replace(to_replace=0,value=np.nan)","e9f6c5ea":"X_train_na.isna().sum().sum()","e6461b51":"xgb.DMatrix(X_train_na['N072'].replace(to_replace=0,value=np.nan), missing=np.nan)","6d8bbef8":"xgb.DMatrix(X_train_na['N072'], missing=np.nan)","bc7944c8":"xgb.XGBRFClassifier().fit(pd.get_dummies(X_train_na,columns=['CAS','SMILES']),np.ravel(y_train))","c23f91e5":"lgb.LGBMClassifier(n_jobs=-1,use_missing=True).fit(X_train,np.ravel(y_train))","25c2be2c":"from xgboost import plot_importance\n\nplot_importance(xgb.XGBRFClassifier().fit(pd.get_dummies(X_train_na,\n                                                         columns=['CAS','SMILES']),np.ravel(y_train)));","9e2eaa25":"lgb.plot_importance(lgb.LGBMClassifier(n_jobs=-1,\n                                       use_missing=True).fit(X_train,np.ravel(y_train)));","b83a89ad":"%%time\nxgb.XGBRegressor(colsample_bytree = 0.3, \n                 learning_rate = 0.1,max_depth = 5, alpha = 10, \n                 n_estimators = 10).fit(pd.get_dummies(X_train_na,columns=['CAS','SMILES']),np.ravel(y_train))","ad86a284":"%%time\nlgb.LGBMClassifier(n_jobs=-1,use_missing=True,categorical_feature=True,max_depth=1,learning_rate=0.1).fit(X_train_na,np.ravel(y_train),verbose=True)","b13ec831":"from sklearn.metrics import f1_score,precision_score,recall_score,mean_squared_error,classification_report","c2c205f1":"X_train_xgboost, X_test_xgboost, y_train, y_test = train_test_split(pd.get_dummies(x,columns=['SMILES','CAS']),\n                                                    y,\n                                                    test_size = 0.25,\n                                                    random_state=29)","41608457":"X_train_xgboost.shape","36bd023c":"%%time\nxgb_c = xgb.XGBRFClassifier(n_jobs=-1,).fit(X_train_xgboost,np.ravel(y_train))\nxgb_predict = xgb_c.predict(X_test_xgboost)\nprint(\"\\n Model Parameters: \",xgb_c)\nprint(\"\\nF1 Score: \",f1_score(y_test, xgb_predict,average='weighted'))\nprint(\"\\nPrecision Score : \",precision_score(y_test, xgb_predict,average='weighted'))\nprint(\"\\nMSE SCORE : \",mean_squared_error(y_test, xgb_predict))\nprint(\"\\nclassification report: \\n\",classification_report(y_test,xgb_predict))","9d5299d8":"%%time\nlgb_c = lgb.LGBMClassifier(n_jobs=-1).fit(X_train,np.ravel(y_train),verbose=True)\nlgb_predict = lgb_c.predict(X_test)\nprint(\"\\nModel Parameters: \",lgb_c)\nprint(\"\\nF1 Score: \",f1_score(y_test, lgb_predict,average='weighted'))\nprint(\"\\nPrecision Score : \",precision_score(y_test, lgb_predict,average='weighted'))\nprint(\"\\nMSE SCORE : \",mean_squared_error(y_test, lgb_predict))\nprint(\"\\nclassification report: \\n\",classification_report(y_test,lgb_predict))","9b997972":"## XGBoost","fb11644b":"```python\nxgboost.plot_importance(booster,ax=None, height=0.2, xlim=None, ylim=None, \n                           title='Feature importance', xlabel='F score',\n                           ylabel='Features', fmap='', importance_type='weight',\n                           max_num_features=None, \n                           grid=True, show_values=True, **kwargs)\n```","6c44c4f3":"## LightGBM","7a83f7c6":"## XGBoost","20eea951":"# Performance metrics (is it possible to use a user-defined metric?)\n","8f7f39dc":"# SUMMARY & RECAP\n\nWe can see the on model default settings that Lightgbm has outperformed XGboost in terms on performance and accuracy whithout any parameter tunnings, XGBoost has spent in total total: 8.09 s to classify 25% of test data with 68% as an F1 score, while LightGBM spent total: 216 ms, with 74% relative F1 score, this shows how lightgbm works better one the dimension of features are enncoded within the model, and how that cann be generlized on test data.","504e9567":"## XGBoost","5160cc8d":"# Overfitting","fa981efb":"```python\nlightgbm.plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance',\n                         xlabel='Feature importance', ylabel='Features', importance_type='split', \n                         max_num_features=None, ignore_zero=True, figsize=None, dpi=None, grid=True, \n                         precision=3,**kwargs)\n\n```","ef0a716d":"## LightGBM","709a7a62":"# WHICH ONE IS BETTER\n\nHere we viewed a test on perfomance annd speed, which replicate a relative smiliar results to the our test.\n\n- According to Catboost testing methodology.\n\nMethodology: We measured mean tree construction time one can achieve without using feature subsampling and\/or bagging. For XGBoost and CatBoost we use default tree depth equal to 6, for LightGBM we set leafs count to 64 to have more comparable results. We set bin to 15 for all 3 methods. Such bin count gives the best performance and the lowest memory usage for LightGBM and CatBoost (128-255 bin count usually leads both algorithms to run 2-4 times slower). For XGBoost we could use even smaller bin count but performance gains compared to 15 bins are too small to account for. All algorithms were run with 16 threads, which is equal to hardware core count.","10a6b0ac":"Their are several builtin metrics can be user to evaluate performace for XGBoost estimator where it imputed in ```eval_metric``` object : ('eval_metric': 'logloss'), the evals_result returns logloss.\n\nAs an example the follwoing metrics are built-in evaluating object.\n\n- rmse: root mean square error\n\n- rmsle: root mean square log error.\n\n- mae: mean absolute error\n\n- mphe: mean Pseudo Huber error.\n\n- logloss: negative log-likelihood\n\n- error: Binary classification error rate. It is calculated as #(wrong cases)\/#(all cases).\n\n- error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through \u2018t\u2019.\n\nand many others an be seen here: https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#learning-task-parameters","88a8c149":"# Important Hyperparameters and Tunning:\n>- Grid Search\n>- Random Search\n>- Other on your choice","f3a1d3df":"## LightGBM","30eda14c":"```python\n\nxgboost.train(params, dtrain, num_boost_round=10, evals=(), \n              obj=None, feval=None, maximize=False, early_stopping_rounds=None, \n              evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None)\n\n```","46db7e37":"In this tutorial we investigate Xgboost and lightgbm python modules and their functions and methods on the main section task, we hope that you like it. ","aa097acc":"Feature importance is only defined when the decision tree model is chosen as base learner (booster=gbtree). It is not defined for other base learner types, such as linear learners (booster=gblinear).\n\n- \u201dweight\u201d is the number of times a feature appears in a tree\n\n- \u201dgain\u201d is the average gain of splits which use the feature\n\n- \u201dcover\u201d is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split.\n\navailbe parameters for importance_type are: \n\n[\u2018weight\u2019, \u2018gain\u2019, \u2018cover\u2019, \u2018total_gain\u2019, \u2018total_cover\u2019]","3e018777":"REACHED HERE\n---","b6fb20b9":"# Feature Importance","3bae22c3":"## XGBoost (Yes)\n\nXGBoost is designed to be an extensible library. One way to extend it is by providing thier own objective function for training and corresponding metric for performance monitoring + support for user predifend custom metrics.","d71e73b0":"LightGBM can use categorical features as input directly. It doesn\u2019t need to convert to one-hot coding, and is much faster than one-hot coding, categorical features should be encoded as type category, for example```astype.('category')```. Categorical features must be encoded as non-negative integers (int)","7f219e9d":"**TASKS**:\n\nYou should prepare the Tutorial on XGBoost vs LightGBM algorithms comparison. The goal is to investigate the main features of each algorithm, compare and highlight important features. As we want to compare the core performance of algorithms the feature engineering will be limited.\n\n\n* **<font color='red'>Note that all thge error cells are produced on purpose for the sake of demostration<\/font>.**","b1d30bf6":"We can see here how the the encoded caegorical features + the zero values has been encoded in the model object.","648deaee":"LightGBM enables the missing value handle by default. Disable it by setting use_missing=false.\n\nLightGBM uses NA (NaN) to represent missing values by default. Change it to use zero by setting zero_as_missing=true.\n\nWhen zero_as_missing=false (default), the unshown values in sparse matrices (and LightSVM) are treated as zeros.\n\nWhen zero_as_missing=true, NA and zeros (including unshown values in sparse matrices (and LightSVM)) are treated as missing.\n\n**NOTE** :  All negative values in categorical features will be treated as missing values.","70b0994f":"LightGBM has accepted the categorical feature, as the estimator encode the features while training.","592609ed":"## XGBoost\n\n\nThere are  two ways that can control overfitting in XGBoost:\n\nControl model complexity by ```max_depth, min_child_weight and gamma```.\n\nAdd randomness to make training robust to noise by  ```subsample and colsample_bytree```.\n\nReduce stepsize eta. Remember to increase ``num_round``` when you do so\n\nhttps:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html?highlight=overfitting#control-overfitting","1f43faf1":"# Main sections\n\n- Algorithm overview\n- Categorical features handling\n- Missing values handling\n- Feature Importance\n- Important Hyperparameters and Tunning:\n- Performance metrics (is it possible to use a user-defined metric?)\n- Overfitting\n- Cross-Validation\n- Other important library features (you can add any feature description on your decision)\n- Performance and Speed\n- Advantages and Disadvantages\n- Which algorithm to use?","22060c80":"XGBoost supports missing value by default. In tree algorithms, branch directions for missing values are learned during training. Note that the gblinear booster treats missing values as zeros.\n\n","21fbe53b":"**Note:**\n\nCategorical features not supported\n\nNote that XGBoost does not provide specialization for categorical features; if your data contains categorical features, load it as a NumPy array first and then perform corresponding preprocessing steps like ```one-hot encoding``` .","85e56a8c":"## LightGBM (Yes)\n\nLightgbm support custom metrics by encoding the parameter ```metric``` in the evaluating object.\n\n* metric, default = ```\"\"```, type = multi-enum, aliases: metrics, metric_types\n\nmetric(s) to be evaluated on the evaluation set(s)\n\n\n```\"\"``` (empty string or not specified) means that metric corresponding to specified objective will be used (this is possible only for pre-defined objective functions, otherwise no evaluation metric will be added)\n\n\n```\"None\"``` (string, not a None value) means that no metric will be registered, aliases: na, null, custom\n\n\nand many others an be seen here: https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html#metric-parameters","88ea0c1c":"![](https:\/\/avatars.mds.yandex.net\/get-yablogs\/41243\/file_1530623946729\/orig)\n![](https:\/\/avatars.mds.yandex.net\/get-yablogs\/38241\/file_1530623961886\/orig)","b9bfd441":"Categorical features are not supported\n\nNote that XGBoost does not provide specialization for categorical features; if your data contains categorical features, load it as a NumPy array first and then perform corresponding preprocessing steps like one-hot encoding.","61ca526d":"### Data Set Information:\n\n*A dataset of manually-curated BCF for 779 chemicals was used to determine the mechanisms of bioconcentration, i.e. to predict whether a chemical: (1) is mainly stored within lipid tissues, (2) has additional storage sites (e.g. proteins), or (3) is metabolized\/eliminated. Data were randomly split into a training set of 584 compounds (75%) and a test set of 195 compounds (25%), preserving the proportion between the classes. Two QSAR classification trees were developed using CART (Classification and Regression Trees) machine learning technique coupled with Genetic Algorithms. The file contains the selected Dragon descriptors (9) along with CAS, SMILES, experimental BCF, experimental\/predicted KOW and mechanistic class (1, 2, 3). Further details on model development and performance along with descriptor definitions and interpretation are provided in the original manuscript (Grisoni et al., 2019).*\n\n\nDatasets link: https:\/\/archive.ics.uci.edu\/ml\/datasets\/QSAR+Bioconcentration+classes+dataset","a2c09f1b":"We can see that LightGBM supports only a data that must be int, float or bool only, unless we encoded ```CAS, SMILES, Set``` as a categorical variables as below.","52d37f67":"We can see here how the zero values has been encoded in the XGBoost object.","e3948d89":"# Categorical features handling","d77ffdba":"# XGBOOST VS LIGHTGBM","8913a36d":"# Performance and Speed\n\nAs mention on Lightgbm paper, LightGBM improves on XGBoost. LightGBM uses XGBoost as a baseline and outperforms it in training speed and the dataset sizes it can handle.","489e5279":"Hyper-parameters are parameters that are not directly learnt within estimators. which can be used by scikit-learn models that is dependents on the created estimators kernel:\n\nfor examples ```model_selection``` support the following that can be applied to lightgbm and XGBoost.\n\n\n- Exhaustive search over specified parameter values for an estimator: ```model_selection.GridSearchCV(estimator, \u2026)```\n\n\n- Grid of parameters with a discrete number of values for each: ```model_selection.ParameterGrid(param_grid)```\n\n- Generator on parameters sampled from given distributions: ```model_selection.ParameterSampler(\u2026[, \u2026])```\n\n\n- Generator on parameters sampled from given distributions.```model_selection.RandomizedSearchCV(\u2026[, \u2026])```","c503d56c":"We set such learning rate that algorithms start to overfit approximately after 8000 rounds (learning curves are displayed at figure above, quality of obtained models differs by approximately 0.5%). We measured time to train ensembles of 8000 trees. Mean tree construction time for CatBoost was 17.9ms, for XGBoost 488ms, for LightGBM 40ms. As you can see CatBoost 2 times faster then LightGBM and 20 times faster then XGBoost.","9a8c2669":"### DATA CLEANING\n","2c46c88e":"## XGBoost\n\nWe can see here that we should enncode the categorical feature by one hot encoding method to enforce the feature the the model, which increase the dimensionality on features to **584** in total.","d15afad3":"## XGBoost\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient  machine learning algorithms under the Gradient Boosting framework\n\n**The XGBoost python module is able to load data from:**\n\n- LibSVM text format file\n\n- Comma-separated values (CSV) file\n\n- NumPy 2D array\n\n- SciPy 2D sparse array\n\n- cuDF DataFrame\n\n- Pandas data frame, and\n\n- XGBoost binary buffer file.\n\n[XGBoost paper](https:\/\/arxiv.org\/pdf\/1603.02754.pdf)","5da83afc":"importance_type (string, optional (default=\"split\")) \u2013 How the importance is calculated. If \u201csplit\u201d, result contains numbers of times the feature is used in a model. If \u201cgain\u201d, result contains total gains of splits which use the feature.","ba36b294":"## LightGBM\n\nA fast, distributed, high performance gradient boosting (GBT, GBDT, GBRT, GBM or MART) framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. LightGBM is a gradient boosting framework that uses tree based learning algorithms.\n    \n**The LightGBM Python module can load data from:**\n\n- LibSVM (zero-based) \/ TSV \/ CSV \/ TXT format file\n\n- NumPy 2D array(s), pandas DataFrame, H2O DataTable\u2019s Frame, SciPy sparse matrix\n\n- LightGBM binary file\n    \n[LightGBM paper](https:\/\/papers.nips.cc\/paper\/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)\n","a4692c82":"Now we can see that it has been saved as an object.","bd87615a":"## LightGBM","f7e455ea":"## Third-Party integration hyperparameter tunning (Optuna)\n\n![](https:\/\/optuna.org\/assets\/img\/optuna-logo.png)\n\n\n**compatible with xgboost and lightgbm**\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. Thanks to our define-by-run API, the code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\n\nhttps:\/\/github.com\/optuna\/optuna","168532b3":"## XGBoost","64fd7a6f":"# Missing Value Handling","e95f71d7":"# REFRENCES\n\n- https:\/\/catboost.ai\/news\/extremely-fast-learning-on-gpu-has-arrived\n- https:\/\/github.com\/szilard\/GBM-perf#results\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/index.html\n- https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html","2b034adb":"# ALGORITHMS OVERVIEW","52d7a894":"# Categorical features handling","9c615af7":"## LightGBM\n\n\nDeal with Over-fitting by several ways such as : \n\n>- Use small max_bin\n\n>- Use small num_leaves\n\n>- Use min_data_in_leaf and min_sum_hessian_in_leaf\n\n>- Use bagging by set bagging_fraction and bagging_freq\n\n>- Use feature sub-sampling by set feature_fraction\n\nUse bigger training data\n\n>- Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n\n>- Try max_depth to avoid growing deep tree\n\n>- Try extra_trees\n\n>- Try increasing path_smooth\n\n\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html#deal-with-over-fitting","f22c8ff6":"### TRAININNG\/TESTINNG SPLIT","4130a735":"```python\n\nfit(X, y, sample_weight=None, init_score=None, eval_set=None, eval_names=None, eval_sample_weight=None,\n   eval_class_weight=None, eval_init_score=None, eval_metric=None, early_stopping_rounds=None, verbose=True, \n   feature_name='auto', categorical_feature='auto', callbacks=None, init_model=None)\n\n```"}}