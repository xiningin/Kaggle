{"cell_type":{"0a310f49":"code","e8ae729d":"code","ba13dc74":"code","3b28c4d5":"code","e4f8ac5e":"code","5e748ae7":"code","4d8be458":"code","6ce27456":"code","6ba6f3ef":"markdown","ccc5a444":"markdown","32798ec4":"markdown","e8a69d76":"markdown","6766d986":"markdown","51456c3c":"markdown","9ce3c0cc":"markdown","e4faf124":"markdown","f9d535af":"markdown"},"source":{"0a310f49":"import pandas as pd\nimport lightgbm as lgb","e8ae729d":"import os\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv', index_col='Id')\ndataX = train[ train.columns[train.columns.str.contains('Cm$')] ]\ndatay = OrdinalEncoder().fit_transform(train[['Species']]).flatten().astype(int)\ntrainSet = lgb.Dataset(dataX, datay)\n\nparam = {'objective'       : 'multiclass',\n         'metric'          : 'multi_logloss',\n         'num_class'       : train['Species'].nunique(),\n         'num_leaves'      : 3,\n         'min_data_in_leaf': 36, \n         'learning_rate'   : .15,\n         'num_boost_round' : 30}\nmodel = lgb.train(param, trainSet)","ba13dc74":"def grabdict(tisdict, tree_index, split_index, depth, splits, leaves):\n# recursive function to unravel nested dictionaries\n    depth += 1\n    if 'split_index' in tisdict.keys():\n        tis = tisdict.copy()\n        del tis['left_child']\n        del tis['right_child']\n        tis['tree_index'] = tree_index\n        split_index = tis['split_index']\n        splits = pd.concat([splits, pd.DataFrame(tis, index=[len(splits)])])\n        splits, leaves = grabdict(tisdict['left_child'], tree_index, split_index, depth, splits, leaves)\n        splits, leaves = grabdict(tisdict['right_child'], tree_index, split_index, depth, splits, leaves)\n    else:\n        tis = tisdict.copy()\n        tis['tree_index'] = tree_index\n        tis['split_index'] = split_index\n        tis['depth'] = depth\n        leaves = pd.concat([leaves, pd.DataFrame(tis, index=[len(leaves)])])\n    return splits, leaves\n\ndef grabtrees(model):\n# wrapper function to call the two functions above\n    splits, leaves = pd.DataFrame(), pd.DataFrame()\n    tree_info = model.dump_model()['tree_info']\n    for tisdict in tree_info:\n        splits, leaves = grabdict(tisdict['tree_structure'], tisdict['tree_index'], 0, 0, splits, leaves)\n    leaves = leaves.merge(splits, left_on=['tree_index', 'split_index'], right_on=['tree_index', 'split_index'], how='left')\n    return tree_info, leaves\n\ntree_info, leaves = grabtrees(model)\nleaves   # all leaves in a single df: one leaf per row","3b28c4d5":"imp_split = model.feature_importance('split')\nfor tt in range(len(imp_split)):\n    print(imp_split[tt], model.feature_name()[tt])\n# So PetalLengthCm is found to be most important (by 'split' sense)\n# followed by PetalWidthCm, SepalWidthCm and lastly SepalLengthCm.","e4f8ac5e":"# Recall that DataFrame *leaves* contain one leaf per row.\n# Each split produces 2 leaves sharing a common splitting criterion.\n# We should therefore first drop the duplicating partner-leaves.\none_split_per_row = leaves[['tree_index', 'split_index', 'split_feature', 'split_gain']].drop_duplicates()\none_split_per_row","5e748ae7":"one_split_per_row['split_feature'].value_counts(sort=False)\n# voila; we reproduce the exact same numbers of model.feature_importance('split')","4d8be458":"imp_gain = model.feature_importance('gain')\nfor tt in range(len(imp_gain)):\n    print(imp_gain[tt], model.feature_name()[tt])\n# So PetalLengthCm is found to be most important (by 'gain' sense)\n# followed by PetalWidthCm, SepalWidthCm and lastly SepalLengthCm.\n# In this problem the order of feature importance happens to agree in both 'split' and 'gain' senses.\n# This is not always the case.","6ce27456":"one_split_per_row.groupby('split_feature')['split_gain'].sum()\n# Voila: perfect tally!","6ba6f3ef":"# 2. Toy data & toy model\nBorrowing data from https:\/\/www.kaggle.com\/uciml\/iris.\n*Replace next cell with your own data and model.*","ccc5a444":"# 1. Initiation rite\nInvocations we can't go on without.","32798ec4":"### Where do these values come from?","e8a69d76":"# 5. importance_type='gain'","6766d986":"### Where do these floating-point values come from?","51456c3c":"# 6. Sister notebooks: the Leaf-by-leaf series\nDecision trees: a leaf-by-leaf demo\n\nhttps:\/\/www.kaggle.com\/marychin\/decision-trees-a-leaf-by-leaf-demo\n\n**num_leaves** and **min_data_in_leaf**: a LightGBM demo\n\nhttps:\/\/www.kaggle.com\/marychin\/num-leaves-min-data-in-leaf-a-lightgbm-demo\n\nmin_sum_hessian: a LightGBM demo\n\nhttps:\/\/www.kaggle.com\/marychin\/min-sum-hessian-a-lightgbm-demo\n\nfeature_importances split vs gain: a demo (we are here)\n\nhttps:\/\/www.kaggle.com\/marychin\/feature-importances-split-vs-gain-a-demo\n\n# 7. Cheers, Kagglers & Kaggle!\nTogether we democratise learning and skills.","9ce3c0cc":"# 3. Put trees in Pandas DataFrame\nSkip the next cell unless you are particularly interested! The original notebook where I wrote these[](http:\/\/) functions is available from https:\/\/www.kaggle.com\/marychin\/lightgbm-trees-to-pandas-dataframe.","e4faf124":"# 0. Context\nFeature importance is the *feature* that gets me hooked on tree-based methods.\n\nTwo types of feature importance can be extracted from LightGBM boosters: *split* and *gain*. Referring to \nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.Booster.html#lightgbm.Booster.feature_importance:\n> importance_type (string, optional (default=\"split\")) \u2013 How the importance is calculated. If \u201csplit\u201d, resultcontains numbers of times the feature is used in a model. If \u201cgain\u201d, result contains total gains of splits which use the feature.\n\nThere is often a confusion between the two importance types: *split* vs *gain*. Here is a look-inside, back-of-envelop demo of the two:\n1. start with output from *booster.dump_model()['tree_info']*;\n2. calculate feature importance of type *split*;\n3. prove perfect tally with output from *booster.feature_importance()*;\n4. repeat the same steps for feature importance of type *gain*.","f9d535af":"# 4. importance_type='split'"}}