{"cell_type":{"2222b14d":"code","48d25cbe":"code","7b52d400":"code","1b4ca6b1":"code","ef7b6aba":"code","3ed1542c":"code","a2b60a1f":"code","748bcdb8":"code","74936cbc":"markdown","edaa4dfc":"markdown","fd6df4e2":"markdown","1a6621b6":"markdown","673deac8":"markdown","dd68786b":"markdown"},"source":{"2222b14d":"!pip install mlxtend","48d25cbe":"from mlxtend.evaluate import bias_variance_decomp\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.data import boston_housing_data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nimport warnings\nwarnings.filterwarnings('ignore') ","7b52d400":"def get_bias_var(tree, X_train, y_train, X_test, y_test, loss_type):\n    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(tree, X_train, y_train, X_test, y_test, loss=loss_type, random_seed=123)\n\n    print('Average expected loss: %.3f' % avg_expected_loss)\n    print('Average bias: %.3f' % avg_bias)\n    print('Average variance: %.3f' % avg_var)\n    return","1b4ca6b1":"X, y = boston_housing_data()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, shuffle=True)\ntree = DecisionTreeRegressor(random_state=123)\nerror_dt, bias_dt, var_dt = bias_variance_decomp(tree, X_train, y_train, X_test, y_test,'mse', random_seed=123)\n\ntree_pruned = DecisionTreeRegressor(random_state=123, max_depth=2)\nerror_dt_pruned, bias_dt_pruned, var_dt_pruned = bias_variance_decomp(tree_pruned, X_train, y_train, X_test, y_test,'mse', random_seed=123)\n\nprint(\"variance Reduction:\", str(np.round((var_dt_pruned\/var_dt-1)*100,2)) + '%')\nprint(\"At the expense of introducing bias:\", str(np.round((bias_dt_pruned\/bias_dt-1)*100, 2)) + '%')","ef7b6aba":"tree = DecisionTreeRegressor(random_state=123)\nerror_dt, bias_dt, var_dt = bias_variance_decomp(tree, X_train, y_train, X_test, y_test,'mse', random_seed=123)\n\ntree_rf = RandomForestRegressor(random_state=123)\nerror_rf, bias_rf, var_rf = bias_variance_decomp(tree_rf, X_train, y_train, X_test, y_test,'mse', random_seed=123)\n\nprint(\"variance Reduction:\", str(np.round((var_rf\/var_dt-1)*100,2))+ '%')\nprint(\"At the expense of introducing bias:\", str(np.round((bias_rf\/bias_dt-1)*100, 2)) + '%')","3ed1542c":"bias_knn, var_knn, error_knn = [], [], []\nfor k in range(1, 15):\n    clf_knn = KNeighborsRegressor(n_neighbors=k)\n    avg_expected_loss, avg_bias, avg_var  = avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(clf_knn, X_train, y_train, X_test, y_test, loss='mse', random_seed=123)\n    bias_knn.append(avg_bias)\n    var_knn.append(avg_var)\n    error_knn.append(avg_expected_loss)\nplt.plot(range(1,15), error_knn, 'b', label = 'total_error')\nplt.plot(range(1,15), bias_knn, 'k', label = 'bias')\nplt.plot(range(1,15), var_knn, 'y', label = 'variance')\nplt.legend()","a2b60a1f":"tree = DecisionTreeRegressor(random_state=123)\ntree.fit(X_train, y_train)\nfeat_array = tree.feature_importances_\n\ndf_feat = pd.DataFrame()\nname_list, importance_list = [], []\nfor name, importance in zip(range(X_train.shape[1]), feat_array):\n    name_list.append(name)\n    importance_list.append(importance)\ndf_feat['name'] = name_list\ndf_feat['importance'] = importance_list\n\nprint(\"Top 6 features controbute 95% of total feature importance:\", df_feat.sort_values(by = 'importance', ascending = False).head(6)['importance'].sum())\n\nerror_fs, bias_fs, var_fs  = [], [], []\nfeat_count = [2,3,4,5,6,7,8,9,10,11,12]\nfor feat in feat_count:\n    top_feat = df_feat.sort_values(by = 'importance', ascending = False).head(feat)['name'].values\n    X_train_fs = X_train[:, top_feat]\n    X_test_fs = X_test[:, top_feat]\n    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(tree, X_train_fs, y_train, X_test_fs, y_test, loss='mse', random_seed=123)\n    error_fs.append(avg_expected_loss)\n    bias_fs.append(avg_bias)\n    var_fs.append(avg_var)\n    \nplt.plot(feat_count, error_fs, 'b', label = 'total_error')\nplt.plot(feat_count, bias_fs, 'k', label = 'bias')\nplt.plot(feat_count, var_fs, 'y', label = 'variance')\nplt.legend()","748bcdb8":"linear_model = LinearRegression()\nlasso_model = Lasso(alpha=0.05)\nerror_reg, bias_reg, var_reg = bias_variance_decomp(lasso_model, X_train, y_train, X_test, y_test, loss='mse', random_seed=123)\nerror_no_reg, bias_no_reg, var_no_reg = bias_variance_decomp(linear_model, X_train, y_train, X_test, y_test, loss='mse', random_seed=123)\nprint(\"variance Reduction:\", (var_reg\/var_no_reg-1)*100)\nprint(\"At the expense of introducing bias:\", (bias_reg\/bias_no_reg-1)*100)","74936cbc":"### KNN -- a high value of k leads to high bias and low variance","edaa4dfc":"### Pruning DT reduces the variance","fd6df4e2":"### An implementation to demonstrate bias and variance trade off using mlxtend library","1a6621b6":"### Adding features brings down the bias","673deac8":"### Using Ensemble\/Random Forest to reduce variance","dd68786b":"### Regularization to control the variance"}}