{"cell_type":{"1b8d0a84":"code","d0024a85":"code","79f40789":"code","500a3858":"code","6ad5aa37":"code","bfc8213c":"code","dd228c6a":"code","53f9d470":"code","fe9b83cc":"code","66098e4b":"code","af7d301d":"code","19c0aec6":"code","a6fa7ac7":"code","225009fe":"code","0f0022ac":"code","c173c29c":"code","2323933f":"code","8c7d96b9":"code","7b9ddcb1":"code","52a98ed3":"code","ba851f8e":"code","3a59984d":"code","5cafc38f":"code","2992c736":"code","9016bdeb":"code","d9e6a4e4":"code","41ae828f":"code","fd77b849":"code","0fabb4bb":"code","d73b7544":"code","6eaa8b50":"code","4a007890":"code","19e93b8c":"code","acf1840c":"code","abefd63e":"code","9f222f87":"code","aacc2dca":"code","e32bd351":"code","eaca1f59":"code","78a719ac":"code","710bfb22":"code","bb1fbd27":"code","13b977b4":"code","ee85476e":"code","5e6f4c13":"code","8fd5f84e":"code","5f0ec0f1":"code","32589338":"code","444fe7ac":"code","0f2e688a":"code","b3f21c9b":"code","03508d16":"code","f8d7f825":"code","e6707921":"markdown","70b7a4a7":"markdown","e90feaad":"markdown","577994ea":"markdown","45aec2bc":"markdown","b9cda6ce":"markdown","17a08541":"markdown","d125b294":"markdown","f3fcd08d":"markdown","9bee5135":"markdown","812a5a78":"markdown","04ba928c":"markdown","7bb3d62b":"markdown","b2e43c12":"markdown","7e172c50":"markdown","f17bf982":"markdown","57a35cee":"markdown","785dc598":"markdown","ab2b92c8":"markdown","4d671c1c":"markdown","7825054d":"markdown","a95dfdb6":"markdown","c7f2f98b":"markdown"},"source":{"1b8d0a84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d0024a85":"# Import libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder","79f40789":"recruitment = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\nrecruitment.head(5)","500a3858":"recruitment.shape","6ad5aa37":"recruitment.info()","bfc8213c":"recruitment.isnull().sum()","dd228c6a":"# Making null value as zero.\nrecruitment.fillna(0,inplace=True)\nrecruitment.head(5)","53f9d470":"## Datatypes of columns\nrecruitment.dtypes","fe9b83cc":"## Drop \"sl.no.\" as it will not help in model learning.\n\nrecruitment.drop('sl_no', axis=1, inplace=True)","66098e4b":"recruitment.drop(['hsc_b','ssc_b'], axis=1, inplace=True)","af7d301d":"recruitment.head()","19c0aec6":"plt.figure(figsize=(15,10))\n\nax = plt.subplot(331)\nplt.boxplot(recruitment['ssc_p'])\nax.set_title('Secondary School Percentage')\n\nax = plt.subplot(332)\nplt.boxplot(recruitment['hsc_p'])\nax.set_title('Higher Secondary School Percentage')\n\nax = plt.subplot(333)\nplt.boxplot(recruitment['degree_p'])\nax.set_title('Degree Percentage')\n\nax = plt.subplot(334)\nplt.boxplot(recruitment['mba_p'])\nax.set_title('MBA Percentage')\n\nax = plt.subplot(335)\nplt.boxplot(recruitment['etest_p'])\nax.set_title('Employibility Percentage')","a6fa7ac7":"Q1 = recruitment['hsc_p'].quantile(0.25)\nQ3 = recruitment['hsc_p'].quantile(0.75)\nIQR = Q3 - Q1\n\nrecruitment_processed= recruitment.loc[(recruitment['hsc_p'] >= Q1 - 1.5 * IQR) & (recruitment['hsc_p'] <= Q3 + 1.5 *IQR)]","225009fe":"plt.figure(figsize=(8,5))\n\n\nplt.boxplot(recruitment_processed['hsc_p'])\nplt.title('Higher Secondary School Percentage')\n","0f0022ac":"categorical_columns = recruitment_processed.select_dtypes(\"object\").columns\ncategorical_columns","c173c29c":"plt.figure(figsize = (15, 7))\n\n\n#Gender\nplt.subplot(231)\nax=sns.countplot(x=\"gender\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Higher secondary specialisation\nplt.subplot(232)\nax=sns.countplot(x=\"hsc_s\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Degree type\nplt.subplot(233)\nax=sns.countplot(x=\"degree_t\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Specialisation\nplt.subplot(234)\nax=sns.countplot(x=\"specialisation\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Work experience\nplt.subplot(235)\nax=sns.countplot(x=\"workex\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)\n\n#Status\nplt.subplot(236)\nax=sns.countplot(x=\"status\", data=recruitment_processed)\nax.set_xticklabels(ax.get_xticklabels(),fontsize=12)","2323933f":"sns.pairplot(recruitment_processed,vars=['ssc_p','hsc_p','degree_p','mba_p','etest_p'],hue=\"status\")","8c7d96b9":"## Check categorical columns\ncategorical_columns","7b9ddcb1":"recruitment_processed[categorical_columns].head()","52a98ed3":"column_to_be_encoded = ['gender','workex','status']","ba851f8e":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in column_to_be_encoded:\n    recruitment_processed[col] = label_encoder.fit_transform(recruitment_processed[col])\nrecruitment_processed.head()","3a59984d":"## Creating dummies\n\ndummies=pd.get_dummies(recruitment_processed[['hsc_s','degree_t','specialisation']])\nrecruitment_final = pd.concat([recruitment_processed,dummies],axis=1)\nrecruitment_final.drop(['hsc_s','degree_t','specialisation'],axis=1, inplace=True)\nrecruitment_final.head()","5cafc38f":"X = recruitment_final.drop(['status','salary'], axis=1)\ny = recruitment_final['status']","2992c736":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7, test_size=0.3, random_state=100)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","9016bdeb":"# Import the StandardScaler()\nfrom sklearn.preprocessing import StandardScaler\n\n# Create a scaling object\nscaler = StandardScaler()\n\n# Create a list of the variables that you need to scale\nvarlist = ['ssc_p', 'hsc_p', 'degree_p','etest_p','mba_p']#, #'Asymmetrique Activity Score',\n       #'Asymmetrique Profile Score']\n\n# Scale these variables using 'fit_transform'\nX_train[varlist] = scaler.fit_transform(X_train[varlist])","d9e6a4e4":"\nimport statsmodels.api as sm\n\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","41ae828f":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","fd77b849":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 10)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","0fabb4bb":"col = X_train.columns[rfe.support_]\ncol","d73b7544":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","6eaa8b50":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4a007890":"## drop column whose VIF is more than 5\ncol =col.drop(['degree_t_Comm&Mgmt'])\ncol","19e93b8c":"X_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres2 = logm3.fit()\nres2.summary()","acf1840c":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","abefd63e":"\n# Getting the predicted values on the train set\ny_train_pred = res2.predict(X_train_sm)\ny_train_pred[:10]","9f222f87":"\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","aacc2dca":"y_train_pred_final = pd.DataFrame({'status':y_train.values, 'status_Prob':y_train_pred})\ny_train_pred_final['ID'] = y_train.index\ny_train_pred_final.head()","e32bd351":"y_train_pred_final['Status_predicted'] = y_train_pred_final.status_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head(20)","eaca1f59":"\nfrom sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.status, y_train_pred_final.Status_predicted )\nprint(confusion)","78a719ac":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.status, y_train_pred_final.Status_predicted))","710bfb22":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","bb1fbd27":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","13b977b4":"# Let us calculate specificity\nTN \/ float(TN+FP)","ee85476e":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","5e6f4c13":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.status, y_train_pred_final.status_Prob, drop_intermediate = False )","8fd5f84e":"draw_roc(y_train_pred_final.status, y_train_pred_final.status_Prob)","5f0ec0f1":"\n# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.status_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","32589338":" #Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.status, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","444fe7ac":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","0f2e688a":"y_train_pred_final['Status_predicted'] = y_train_pred_final.status_Prob.map(lambda x: 1 if x > 0.7 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","b3f21c9b":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.status, y_train_pred_final.Status_predicted)","03508d16":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","f8d7f825":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.status, y_train_pred_final.Status_predicted )\nprint(confusion)","e6707921":"### Inference:\n\n1. There are 67 null values in our data, which means 67 unplaced students.\n2. We can't drop these values as this will provide a valuable information on why candidates failed to get hired.\n3. We can't impute it with mean\/median values and it will go against the context of this dataset and it will show unhired candidates got salary.\n4. Our best way to deal with these null values is to impute it with '0'.","70b7a4a7":"#### From the curve above, 0.7 is the optimum point to take it as a cutoff probability","e90feaad":"### 7. Model Building","577994ea":"### 3. Outliers\n\nUsing boxplot to check for outliers.","45aec2bc":"### Inferece:\n1. Candidates who scored good in SSC and HSC got placed more.\n2. MBA Percentage did not make a good contributor for Hiring Process as number of Students unplaced are more than Placed who completed MBA.","b9cda6ce":"### 4. Visualizations\n\n#### 1. Categorical Columns: ","17a08541":"### Using RFE for feature selection","d125b294":"### Finding Optimal Cutoff Point","f3fcd08d":"### 2.Data Cleaning and Processing","9bee5135":"\"ssc_b\" and \"hsc_b\" Features are providing information about the board in which candidate pursued his\/her 10th and 12th. These features will not weigh in model learning, Hence dropping these features. ","812a5a78":"P values for all the features is less than 0.5 and VIF is less than 3 which is ideal case.Hence we will be using this model for predictions. ","04ba928c":"### Inference:\n1. There are twice number of Males compared to Female, which may show inferene that more Males are hired than Female\n2. More candidates are from \"commerce\" background.\n3. A large number of candidates have no prior work experience.\n4. Count of placed candidates is more than Unplaced.","7bb3d62b":"### Plotting the ROC Curve\nAn ROC curve demonstrates several things:\n1. It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n2. The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n3. The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","b2e43c12":"#### 7A. Logistic Regression","7e172c50":"### 6. Train-Test Split","f17bf982":"### 5. Preprocessing data for Model Building","57a35cee":"From the ROC curve we can infer that our logistic model has classified the placed students correctly rather than predicting false positive. \nThe more the ROC curve(blue) lies towards the top left side (upper side) the better our model is. \nTo imporve positive prediction We can choose 0.8 or 0.9 for the threshold value which can reap us true positive result. ","785dc598":"### 1.Import Dataset","ab2b92c8":"We have removed the outliers from data, now there are no outliers in the hsc_p data.","4d671c1c":"Now we will create dummies for remaining columns with their name as prefix which will help in identifying the field.","7825054d":"\"Higher Secondary Percentage\" (hsc_p) has most outliers. Any other features do not have outliers. \n\nRemoving these outliers from the data.","a95dfdb6":"Creating a dataframe with the actual converted flag and the predicted probabilities","c7f2f98b":"If we apply label encoding to \"hsc_s\",\"degree_t\" and \"specialiation\" columns, it will not make any sense as they will be numbered and then they can not be distinguished unless we know exact code for that field, hence excluding these column from label encoding. "}}