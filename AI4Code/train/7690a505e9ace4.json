{"cell_type":{"c5943234":"code","be9e49e6":"code","4aacfa33":"code","65491962":"code","38e9bddd":"code","b3d66ba9":"code","f6dd7ff7":"code","ca21b667":"code","e1c36a4d":"code","5aabe72c":"code","d21e9b83":"code","7d1ff0b3":"code","64e786c6":"markdown","d76c2633":"markdown","15bd3609":"markdown","662e0cbc":"markdown"},"source":{"c5943234":"import nltk    \nimport random  \nimport string\n\nimport bs4 as bs  \nimport urllib.request  \nimport re \n\nimport os\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nimport os\n\ndirectory = '\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/'\n\nfor dirname, _, filenames in os.walk(directory):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be9e49e6":"comm_use_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv\")","4aacfa33":"comm_use_df.head()","65491962":"comm_use_df.columns\n","38e9bddd":"def format_corpus(corpus):  \n    for i in range(len(corpus)):\n        corpus [i] = corpus [i].lower()\n        corpus [i] = re.sub(r'\\W',' ',corpus [i])\n        corpus [i] = re.sub(r'\\s+',' ',corpus [i])\n    return corpus","b3d66ba9":"def word_frequencies(corpus):\n    wordfreq = {}\n    for sentence in corpus:\n        tokens = nltk.word_tokenize(sentence)\n        for token in tokens:\n            if token not in wordfreq.keys():\n                wordfreq[token] = 1\n            else:\n                wordfreq[token] += 1\n    return wordfreq","f6dd7ff7":"def make_bow_csv(csv_df,df_name): \n    all_rows = []\n    for i, row in csv_df.iterrows():\n        row = []\n        df = comm_use_df.loc[i]\n        row.append(df[\"paper_id\"])\n        abstract = str(df[\"abstract\"])[8:] ## 1st 8 chars contain abstract word\n        abs_corpus = format_corpus(nltk.sent_tokenize(abstract))\n        abs_wordfreq = word_frequencies(abs_corpus)\n#         row.append(abs_wordfreq)\n        \n        csv_df[\"BOW abstract\"] = str(abs_wordfreq)\n        text = str(df[\"text\"]) ## 1st 8 chars contain abstract word\n        text_corpus = format_corpus(nltk.sent_tokenize(text))\n        text_wordfreq = word_frequencies(text_corpus)\n#         row.append(text_wordfreq)\n        csv_df[\"BOW text\"] = str(text_wordfreq)\n    print(csv_df)","ca21b667":"comm_use_df[\"BOW abstract\"] = \"\"\ncomm_use_df[\"BOW text\"] = \"\"\nmake_bow_csv(comm_use_df,\"filename\")\n","e1c36a4d":"comm_use_df.to_csv('BOW_comm_use.csv', index=False)","5aabe72c":"non_comm_use_df = pd.read_csv(\"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv\")\n","d21e9b83":"non_comm_use_df[\"BOW abstract\"] = \"\"\nnon_comm_use_df[\"BOW text\"] = \"\"\nmake_bow_csv(non_comm_use_df,\"filename\")","7d1ff0b3":"non_comm_use_df.to_csv('BOW_non_comm_use.csv', index=False)","64e786c6":"**tokenize the sentences in the corpus and create a dictionary that contains words and their corresponding frequencies in the corpus**","d76c2633":"**tokenize the sentences in the corpus and create a dictionary that contains words and their corresponding frequencies in the corpus**","15bd3609":"## About this notebook\nIn this notebook, I applied bag of word for abstract and text columns and generated new files. I hope this will help some one who try to solve this tasks\nI used output files from [xhlulu's kernel output](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv),Go and check it and give credits.\n","662e0cbc":"**corpus into individual sentences**\"\nconvert the sentence to lower case, and then remove the punctuation and empty spaces from the text. and remove abstract key in every corpus"}}