{"cell_type":{"c36276ef":"code","2d12e482":"code","5c63ac17":"code","7c242dd1":"code","073d300d":"code","85395013":"code","1480e4b5":"code","2db25860":"code","ece72236":"code","d8228200":"code","ac26e735":"code","75cd086a":"code","8ca517ea":"code","adf9c84a":"code","f6025c0a":"code","75b96d05":"code","eaed1c8b":"code","9be8996d":"markdown","4e86638b":"markdown","712dec89":"markdown"},"source":{"c36276ef":"#Importing all the necessary packages\n\nimport numpy as np\nimport tensorflow as tf\nimport keras\nimport librosa as lb        \nimport cv2 \nimport os\nfrom keras import backend as K\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom numpy import save\nimport matplotlib\n%matplotlib inline\nimport seaborn as sns\nmatplotlib.rcParams['figure.figsize'] = (20, 20)\n\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras import regularizers\nfrom keras.engine.topology import Layer\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix\nfrom keras.callbacks import ReduceLROnPlateau\nmatplotlib.use(\"Agg\")\nimport itertools\n","2d12e482":"#functon for converting all labels in to one hot vectors\ndef one_hot_encode(filename):\n  label_dict = {\"classical\":[1,0,0,0,0,0],\n                \"hiphop\":[0,1,0,0,0,0],\"jazz\":[0,0,1,0,0,0],\n                \"metal\":[0,0,0,1,0,0],\"pop\":[0,0,0,0,1,0],\"reggae\":[0,0,0,0,0,1]}\n  return label_dict[filename] ","5c63ac17":"SR = 22050\nN_FFT = 512\nHOP_LENGTH = N_FFT \/\/ 2\nN_MELS = 64  \n\n#taking audio input and generating mel-spectograms out of it\n\ndef mel_spectogram(file):\n  y, sr = lb.load(file)\n  melspec = lb.feature.melspectrogram(y = y,sr = sr, hop_length = HOP_LENGTH, n_fft = N_FFT, n_mels = N_MELS)\n  melspec_db = lb.power_to_db(melspec,ref = np.max)\n  return melspec_db\n\ndef list_mel_spectogram(data_file_location,dirname1):\n  mel_spect_list = []\n  label = []\n  for dirname, _, filenames in os.walk(data_file_location):\n\n    if dirname == dirname1:\n      \n      for filename in filenames:\n        \n        label.append(np.array(one_hot_encode(filename.split('.')[0])))\n        x = np.array(mel_spectogram(os.path.join(dirname, filename)))\n        x = cv2.resize(x.reshape(x.shape[0],x.shape[1],1), (2584,64),interpolation = cv2.INTER_CUBIC)\n        x = x \/ np.linalg.norm(x)\n        mel_spect_list.append((x.reshape(x.shape[0],x.shape[1],1)))\n\n  return mel_spect_list,label \n\ndef prepare_data():\n\n  data_file_location = '..\/input\/automatic-music-tagging-gtzan-dataset'\n  training_dirname = '..\/input\/automatic-music-tagging-gtzan-dataset\/_train'\n  validation_dirname = '..\/input\/automatic-music-tagging-gtzan-dataset\/_validation'\n  test_dirname = '..\/input\/automatic-music-tagging-gtzan-dataset\/_test'\n\n  train_mel_im, train_label = list_mel_spectogram(data_file_location, training_dirname)\n  valid_mel_im, valid_label = list_mel_spectogram(validation_dirname, validation_dirname)\n  test_mel_im, test_label = list_mel_spectogram(test_dirname, test_dirname)\n  \n\n  return train_mel_im, train_label, valid_mel_im, valid_label, test_mel_im, test_label\n","7c242dd1":"\n#making our dataset ready\ntrain_mel_im, train_label, valid_mel_im, valid_label, test_mel_im, test_label = prepare_data()\n\n#saving datasets into drive\nsave('.\/train_mel_im.npy',train_mel_im)\nsave('.\/train_label.npy',train_label)\nsave('.\/valid_mel_im.npy',valid_mel_im)\nsave('.\/valid_label.npy',valid_label)\nsave('.\/test_mel_im.npy',test_mel_im)\nsave('.\/test_label.npy',test_label)\n\n","073d300d":"#loading them from drive in case runtime get restart\ntrain_mel_im = np.load('.\/train_mel_im.npy')\ny_train = np.load('.\/train_label.npy')\nvalid_mel_im = np.load('.\/valid_mel_im.npy')\ny_val = np.load('.\/valid_label.npy')\ntest_mel_im = np.load('.\/test_mel_im.npy')\ny_test = np.load('.\/test_label.npy')","85395013":"\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.1f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n    plt.show()\n\n#making a cnn model\ndef cnn(num_genres=6, input_shape=(64,2584,1)):\n    model = Sequential()\n    model.add(Conv2D(64, kernel_size=(4, 4),\n                     activation='relu', #kernel_regularizer=regularizers.l2(0.04),\n                     input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 4)))\n    model.add(Conv2D(64, (3, 5), activation='relu'\n                    , kernel_regularizer=regularizers.l2(0.04)\n                    ))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Conv2D(64, (2, 2), activation='relu'\n       # , kernel_regularizer=regularizers.l2(0.04)\n        ))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.04)))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.04)))\n    model.add(Dense(num_genres, activation='softmax'))\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n                  metrics=['accuracy'])\n    return model\n ","1480e4b5":"model = cnn(num_genres=6, input_shape=(64,2584,1))\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","2db25860":"history = model.fit(np.array(train_mel_im),np.array(y_train), validation_data=(np.array(valid_mel_im),np.array(y_val)),  \n                epochs=250, steps_per_epoch=100, validation_steps=10, batch_size=32,callbacks = [learning_rate_reduction])","ece72236":"#checking performance of our model\n\nvalidation_accuracy = model.evaluate(np.array(valid_mel_im), np.array(y_val))\ntraining_accuracy = model.evaluate(np.array(train_mel_im), np.array(y_train))\ntesting_accuracy = model.evaluate(np.array(test_mel_im), np.array(y_test))\n# print of test error used only after development of the model\nprint(\"\\nTraining accuracy: %f\\t Validation accuracy: %f\\t Testing Accuracy: %f\" %\n      (training_accuracy[1], validation_accuracy[1], testing_accuracy[1]))\nprint(\"\\nTraining loss: %f    \\t Validation loss: %f    \\t Testing Loss: %f \\n\" %\n      (training_accuracy[0], validation_accuracy[0], testing_accuracy[0]))\nprint( )","d8228200":"%matplotlib inline\nlabels = ['classical','hiphop','jazz','metal','pop','reggae']\n\n# plotting confusion matrix\nY_pred = model.predict(np.array(valid_mel_im))\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(np.array(y_val),axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = labels,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n\n","ac26e735":"#Saving model and model weights\n\n# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")\n","75cd086a":"!pip install ffmpeg-python","8ca517ea":"\"\"\"\nTo write this piece of code I took inspiration\/code from a lot of places.\nIt was late night, so I'm not sure how much I created or just copied o.O\nHere are some of the possible references:\nhttps:\/\/blog.addpipe.com\/recording-audio-in-the-browser-using-pure-html5-and-minimal-javascript\/\nhttps:\/\/stackoverflow.com\/a\/18650249\nhttps:\/\/hacks.mozilla.org\/2014\/06\/easy-audio-capture-with-the-mediarecorder-api\/\nhttps:\/\/air.ghost.io\/recording-to-an-audio-file-using-html5-and-js\/\nhttps:\/\/stackoverflow.com\/a\/49019356\n\"\"\"\nfrom IPython.display import HTML, Audio\nfrom google.colab.output import eval_js\nfrom base64 import b64decode\nimport numpy as np\nfrom scipy.io.wavfile import read as wav_read\nimport io\nimport ffmpeg\n\nAUDIO_HTML = \"\"\"\n<script>\nvar my_div = document.createElement(\"DIV\");\nvar my_p = document.createElement(\"P\");\nvar my_btn = document.createElement(\"BUTTON\");\nvar t = document.createTextNode(\"Press to start recording\");\n\nmy_btn.appendChild(t);\n\/\/my_p.appendChild(my_btn);\nmy_div.appendChild(my_btn);\ndocument.body.appendChild(my_div);\n\nvar base64data = 0;\nvar reader;\nvar recorder, gumStream;\nvar recordButton = my_btn;\n\nvar handleSuccess = function(stream) {\n  gumStream = stream;\n  var options = {\n    \/\/bitsPerSecond: 8000, \/\/chrome seems to ignore, always 48k\n    mimeType : 'audio\/webm;codecs=opus'\n    \/\/mimeType : 'audio\/webm;codecs=pcm'\n  };            \n  \/\/recorder = new MediaRecorder(stream, options);\n  recorder = new MediaRecorder(stream);\n  recorder.ondataavailable = function(e) {            \n    var url = URL.createObjectURL(e.data);\n    var preview = document.createElement('audio');\n    preview.controls = true;\n    preview.src = url;\n    document.body.appendChild(preview);\n\n    reader = new FileReader();\n    reader.readAsDataURL(e.data); \n    reader.onloadend = function() {\n      base64data = reader.result;\n      \/\/console.log(\"Inside FileReader:\" + base64data);\n    }\n  };\n  recorder.start();\n  };\n\nrecordButton.innerText = \"Recording... press to stop\";\n\nnavigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n\n\nfunction toggleRecording() {\n  if (recorder && recorder.state == \"recording\") {\n      recorder.stop();\n      gumStream.getAudioTracks()[0].stop();\n      recordButton.innerText = \"Saving the recording... pls wait!\"\n  }\n}\n\n\/\/ https:\/\/stackoverflow.com\/a\/951057\nfunction sleep(ms) {\n  return new Promise(resolve => setTimeout(resolve, ms));\n}\n\nvar data = new Promise(resolve=>{\n\/\/recordButton.addEventListener(\"click\", toggleRecording);\nrecordButton.onclick = ()=>{\ntoggleRecording()\n\nsleep(2000).then(() => {\n  \/\/ wait 2000ms for the data to be available...\n  \/\/ ideally this should use something like await...\n  \/\/console.log(\"Inside data:\" + base64data)\n  resolve(base64data.toString())\n\n});\n\n}\n});\n      \n<\/script>\n\"\"\"\n\ndef get_audio():\n  display(HTML(AUDIO_HTML))\n  data = eval_js(\"data\")\n  binary = b64decode(data.split(',')[1])\n  \n  process = (ffmpeg\n    .input('pipe:0')\n    .output('pipe:1', format='wav')\n    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n  )\n  output, err = process.communicate(input=binary)\n  \n  riff_chunk_size = len(output) - 8\n  # Break up the chunk size into four bytes, held in b.\n  q = riff_chunk_size\n  b = []\n  for i in range(4):\n      q, r = divmod(q, 256)\n      b.append(r)\n\n  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n  riff = output[:4] + bytes(b) + output[8:]\n\n  sr, audio = wav_read(io.BytesIO(riff))\n\n  return audio, sr","adf9c84a":"audio, sr = get_audio() \n#music is taken from this --> https:\/\/www.youtube.com\/watch?v=312Sb-2PovA","f6025c0a":"def mel_spectogram(audio,sr):\n  #y, sr = lb.load(file)\n  melspec = lb.feature.melspectrogram(y = audio.astype(float),sr = sr, hop_length = HOP_LENGTH, n_fft = N_FFT, n_mels = N_MELS)\n  melspec_db = lb.power_to_db(melspec,ref = np.max)\n  return melspec_db","75b96d05":"\nx = np.array(mel_spectogram((audio),float(sr)))\nx = x \/ np.linalg.norm(x)\nx = cv2.resize(x.reshape(x.shape[0],x.shape[1],1), (2584,64),interpolation = cv2.INTER_CUBIC)\npred = model.predict((x.reshape(1,x.shape[0],x.shape[1],1)))","eaed1c8b":"%matplotlib inline \nsns.set_style(style = 'darkgrid')\nplt.figure(figsize = (15,10))\nsns.barplot(x = ['classical','hiphop','jazz','metal','pop','reggae'],y = pred[0])\nplt.show()\nplt.savefig('random_music_test3.jpg')","9be8996d":"**Taking Audio input and performing classification on that**\n\n# Below code is to check on real world music","4e86638b":"problem is with reggae only model considering reggae as hip hop","712dec89":"# Some times Deep Learning doesn't teaches you deep learning\n\n- yeah you heard right some times deep learning doesn't teaches you deep learning but teaches much more than that in my case this was the project which taught me that model building is not everything in ML or DL the basic thing is that you have to extact features and train them over your model so features are relatively more important than model for the quality of the result.\n\n- You will find that our model is overfitting than also I uploaded because the way how we can extract features from sound that too in computer vision friendly way is worth noticing.\n\n- The thing is music is sound and for computer vision we need images so we will convert them into mel-spectograms \n  spectogram - fourier transformation of audio data and by taking log on y- axis will make it mel-spectogram this is required because than only features will be distinguable.\n  \n- now this will make our image data than we will make a shallow model and fit this converted data on it and will see the results unfortunately model overfitted So I will leave this to you why model is overfitting do comment below:)\n\n- I'm also providing a link on which this project is based on - http:\/\/cs229.stanford.edu\/proj2018\/report\/21.pdf\n\n- If you like this than don't forget to upvote:)"}}