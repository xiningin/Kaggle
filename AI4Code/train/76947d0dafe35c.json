{"cell_type":{"3fff4da6":"code","d94c3a1a":"code","9cfc5c36":"code","ba4b6e47":"code","36412b8c":"code","a0ed9f66":"code","e6dfdeaf":"code","c99be13d":"code","a6bb6477":"code","b77163ee":"code","dfa2d3c5":"code","b1819eb2":"code","8803a2cb":"code","1332518a":"markdown"},"source":{"3fff4da6":"# IMPORT MODULES\n# TURN ON the GPU !!!\n# If importing dataset from outside - like this IMDB - Internet must be \"connected\"\n\nimport os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils, to_categorical\n \nfrom keras.datasets import imdb\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\nprint(\"Files in current directory:\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory","d94c3a1a":"# LOAD IMDB DATA\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\nnum_words=10000)","9cfc5c36":"print(\"train_data \", train_data.shape)\nprint(\"train_labels \", train_labels.shape)\nprint(\"_\"*100)\nprint(\"test_data \", test_data.shape)\nprint(\"test_labels \", test_labels.shape)\nprint(\"_\"*100)\nprint(\"Maximum value of a word index \")\nprint(max([max(sequence) for sequence in train_data]))\nprint(\"Maximum length num words of review in train \")\nprint(max([len(sequence) for sequence in train_data]))","ba4b6e47":"# See an actual review in words\n# Reverse from integers to words using the DICTIONARY (given by keras...need to do nothing to create it)\n\nword_index = imdb.get_word_index()\n\nreverse_word_index = dict(\n[(value, key) for (key, value) in word_index.items()])\n\ndecoded_review = ' '.join(\n[reverse_word_index.get(i - 3, '?') for i in train_data[123]])\n\nprint(decoded_review)","36412b8c":"# VECTORIZE as one cannot feed integers into a NN \n# Encoding the integer sequences into a binary matrix - one hot encoder basically\n# From integers representing words, at various lengths - to a normalized one hot encoded tensor (matrix) of 10k columns\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results","a0ed9f66":"x_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nprint(\"x_train \", x_train.shape)\nprint(\"x_test \", x_test.shape)","e6dfdeaf":"# VECTORIZE the labels too - NO INTEGERS only floats into a tensor...(rare exceptions)\n\ny_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')\nprint(\"y_train \", y_train.shape)\nprint(\"y_test \", y_test.shape)","c99be13d":"# Set a VALIDATION set\n\nx_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]\n\nprint(\"x_val \", x_val.shape)\nprint(\"partial_x_train \", partial_x_train.shape)\nprint(\"y_val \", y_val.shape)\nprint(\"partial_y_train \", partial_y_train.shape)","a6bb6477":"# NN MODEL\n\n# Use of DROPOUT\nmodel = models.Sequential()\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(16, kernel_regularizer=regularizers.l1(0.001),activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\n# Use of REGULARIZATION\n#model = models.Sequential()\n#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu', input_shape=(10000,)))\n#model.add(layers.Dense(16, kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),activation='relu'))\n#model.add(layers.Dense(1, activation='sigmoid'))\n\n# REGULARIZERS L1 L2\n#regularizers.l1(0.001)\n#regularizers.l2(0.001)\n#regularizers.l1_l2(l1=0.001, l2=0.001)\n\n# OPTIMIZERS\n#model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])","b77163ee":"# FIT \/ TRAIN model\n\nNumEpochs = 10\nBatchSize = 512\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n\nresults = model.evaluate(x_test, y_test)\nprint(\"_\"*100)\nprint(\"Test Loss and Accuracy\")\nprint(\"results \", results)\nhistory_dict = history.history\nhistory_dict.keys()","dfa2d3c5":"# VALIDATION LOSS curves\n\nplt.clf()\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history_dict['loss']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","b1819eb2":"# VALIDATION ACCURACY curves\n\nplt.clf()\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\nepochs = range(1, (len(history_dict['acc']) + 1))\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8803a2cb":"# PREDICT\n\nmodel.predict(x_test)","1332518a":"* **This kernel is based on one of the exercises in the excellent book: [Deep Learning with Python by Francois Chollet](https:\/\/www.amazon.com\/Deep-Learning-Python-Francois-Chollet\/dp\/1617294438)\n**\n* The kernel imports the IMDB reviews (originally text - already transformed by Keras to integers using a dictionary)\n* Vectorizes and normalizes the data\n* Compiles a multi layers NN\n* Monitors the learning \/ validation curves for loss and accuracy\n* Try and error with different layers and hidden units\n* Employs L1 and L2 weight regularization\n* Implements a DROPOUT layer\n\n* The above mentioned book is a **MUST READ**.\n* *Thanks Francois for an amazing book !*"}}