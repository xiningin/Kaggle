{"cell_type":{"6311910d":"code","eaf9676a":"code","de3b175f":"code","529b2d37":"code","d585010c":"code","f6eb877d":"code","b506946f":"code","cc9b7e66":"code","843541dc":"code","46aeca08":"code","f8893c1a":"code","0e70d448":"code","32370ef0":"code","acb538c5":"code","ba69fdc7":"code","f4cbe526":"code","32ed289b":"code","9715ac81":"code","895ec1e6":"code","fe166d9f":"code","2e3c0d60":"code","d749e9a2":"code","5428192a":"code","9135a6b1":"code","2d6dae99":"code","a3ec0019":"code","53683936":"code","feea9389":"code","7542b99d":"code","be4e6b3c":"code","99b55d5a":"code","9916e4cc":"code","93de4e4a":"code","d3e96ef1":"code","76d6c174":"markdown","4b4075bc":"markdown","c19cea7c":"markdown","2603155c":"markdown","be15f439":"markdown","0e7b5103":"markdown","7f1327b3":"markdown","3853db19":"markdown","fff9f923":"markdown","64e390b8":"markdown","bd52e7d1":"markdown","e74a11c3":"markdown","1587b462":"markdown","b1881af2":"markdown","c4339914":"markdown","d5a414e1":"markdown","e186b412":"markdown","85556046":"markdown","0f169cad":"markdown","3d2918af":"markdown","73f0854f":"markdown","0540b09f":"markdown","503261ff":"markdown"},"source":{"6311910d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eaf9676a":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n# from jupytertehmes import jtplot\n\nimport umap\nfrom sklearn.decomposition import TruncatedSVD, PCA, NMF, LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.datasets import fetch_20newsgroups\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim import corpora\nfrom gensim.models.ldamodel import LdaModel","de3b175f":"# set plot rc parameters\n\n# jtplot.style(grid=False)\nplt.rcParams['figure.facecolor'] = 'white'\nplt.rcParams['axes.facecolor'] = '#464646'\n#plt.rcParams['axes.edgecolor'] = '#FFFFFF'\nplt.rcParams['figure.figsize'] = 10, 7\nplt.rcParams['text.color'] = '#666666'\nplt.rcParams['axes.labelcolor'] = '#666666'\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['xtick.color'] = '#666666'\nplt.rcParams['xtick.labelsize'] = 14\nplt.rcParams['ytick.color'] = '#666666'\nplt.rcParams['ytick.labelsize'] = 14\n\n# plt.rcParams['font.size'] = 16\n\nsns.color_palette('dark')\n%matplotlib inline","529b2d37":"# Load news data set\n# remove meta data headers footers and quotes from news dataset\ndataset = fetch_20newsgroups(shuffle=True,\n                            random_state=32,\n                            remove=('headers', 'footers', 'qutes'))","d585010c":"# sneak peek of the news articles\nfor idx in range(10):\n    print(dataset.data[idx],'\\n\\n','#'*100, '\\n\\n')","f6eb877d":"# put your data into a dataframe\nnews_df = pd.DataFrame({'News': dataset.data,\n                       'Target': dataset.target})\n\n# get dimensions of data \nnews_df.shape","b506946f":"news_df.head()","cc9b7e66":"# replace target names from target numbers in our news data frame\nnews_df['Target_name'] = news_df['Target'].apply(lambda x: dataset.target_names[x])","843541dc":"news_df.head()","46aeca08":"# plot distribution of topics in news data\nfig = plt.figure(figsize=[10,7])\nax = sns.countplot(news_df['Target_name'], color=sns.xkcd_rgb['greenish cyan'])\nplt.title('Distribution of Topics')\nplt.xlabel('Topics')\nplt.ylabel('Count of topics')\nplt.xticks(rotation=90)","f8893c1a":"# clean text data\n# remove non alphabetic characters\n# remove stopwords and lemmatize\n\ndef clean_text(sentence):\n    # remove non alphabetic sequences\n    pattern = re.compile(r'[^a-z]+')\n    sentence = sentence.lower()\n    sentence = pattern.sub(' ', sentence).strip()\n    \n    # Tokenize\n    word_list = word_tokenize(sentence)\n    \n    # stop words\n    stopwords_list = set(stopwords.words('english'))\n    # puctuation\n    # punct = set(string.punctuation)\n    \n    # remove stop words\n    word_list = [word for word in word_list if word not in stopwords_list]\n    # remove very small words, length < 3\n    # they don't contribute any useful information\n    word_list = [word for word in word_list if len(word) > 2]\n    # remove punctuation\n    # word_list = [word for word in word_list if word not in punct]\n    \n    # stemming\n    # ps  = PorterStemmer()\n    # word_list = [ps.stem(word) for word in word_list]\n    \n    # lemmatize\n    lemma = WordNetLemmatizer()\n    word_list = [lemma.lemmatize(word) for word in word_list]\n    # list to sentence\n    sentence = ' '.join(word_list)\n    \n    return sentence\n\n# we'll use tqdm to monitor progress of data cleaning process\n# create tqdm for pandas\ntqdm.pandas()\n# clean text data\nnews_df['News'] = news_df['News'].progress_apply(lambda x: clean_text(str(x)))","0e70d448":"news_df.head()","32370ef0":"# plot word count for news text\nwordcloud = WordCloud(background_color='black',\n                      max_words=200).generate(str(news_df['News']))\nfig = plt.figure(figsize=[16,16])\nplt.title('WordCloud of News')\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","acb538c5":"# vectorize text data\ntfid_vec = TfidfVectorizer(tokenizer=lambda x: str(x).split())\nX = tfid_vec.fit_transform(news_df['News'])\nX.shape","ba69fdc7":"# # PCA\n# pca = PCA(n_components=2)\n# pca.fit(X)\n# pc1, pc2 = pca.transform(X)\n# # plot news vectors\n# ax = sns.scatterplot(pc1, pc2, hue=news_df['Target_name'])\n# plt.show()\n\n# pca doesn't take sparse input","f4cbe526":"# t-SNE\ntsne = TSNE(n_components=2,\n           perplexity=50,\n           learning_rate=300,\n           n_iter=800,\n           verbose=1)\n# tsne to our document vectors\ncomponets = tsne.fit_transform(X)","32ed289b":"# plot news vectors\ndef plot_embeddings(embedding, title):\n    fig = plt.figure(figsize=[15,12])\n    ax = sns.scatterplot(embedding[:,0], embedding[:,1], hue=news_df['Target_name'])\n    plt.title(title)\n    plt.xlabel('axis 0')\n    plt.ylabel('axis 1')\n    plt.legend(bbox_to_anchor=(1.05,1), loc=2)\n    plt.show()\n    return\n\nplot_embeddings(componets, 'Visualizing news vectors (t-SNE)')","9715ac81":"# # get umap embeddings\n# engine = umap.UMAP(n_components=2,\n#                    n_neighbors=150,\n#                    min_dist=0.7)\n# # fit data\n# embedding = engine.fit_transform(X)\n\n# # plot umap embeddings\n# plot_embeddings(embedding, 'Visualizing news vectors UMAP')","895ec1e6":"# create svd instance\nsvd_model = TruncatedSVD(n_components=20,\n                         random_state=12,\n                         n_iter=100,\n                         algorithm='randomized')\n\n# fit model to data\nsvd_model.fit(X)","fe166d9f":"# topic word mapping martrix\nsvd_model.components_.shape","2e3c0d60":"# document topic mapping matrix\ndoc_topic = svd_model.fit_transform(X)\ndoc_topic.shape","d749e9a2":"terms = tfid_vec.get_feature_names()\nlen(terms)","5428192a":"# function to map words to topics\ndef map_word2topic(components, terms):\n    # create output series\n    word2topics = pd.Series()\n    \n    for idx, component in enumerate(components):\n        # map terms (words) with topic\n        # which is probability of word given a topic P(w|t)\n        term_topic = pd.Series(component, index=terms)\n        # sort values based on probability\n        term_topic.sort_values(ascending=False, inplace=True)\n        # put result in series output\n        word2topics['topic '+str(idx)] = list(term_topic.iloc[:10].index)\n        \n    return word2topics","9135a6b1":"word2topics = map_word2topic(svd_model.components_, terms)\n\n# print topic results\nprint('Topics\\t\\tWords')\nfor idx, item in zip(word2topics.index, word2topics):\n    print(idx,'\\t',item)","2d6dae99":"# get top3 topics for a news document\ndef get_top3_topics(x):\n    top3 = list(x.sort_values(ascending=False).head(3).index) + list(x.sort_values(ascending=False).head(3).values)\n    return top3\n\n# map top3 topic words to news document\ndef map_topicword2doc(model, X):\n    # output data frame column list\n    cols = ['topic_'+str(i+1)+'_name' for i in range(3)] + ['topic_'+str(i+1)+'_prob' for i in range(3)]\n    # doc to topic mapping\n    doc_topic = model.fit_transform(X)\n    # list of topics\n    topics = ['topic'+str(i) for i in range(20)]\n    # doc topic data frame\n    doc_topic_df = pd.DataFrame(doc_topic, columns=topics)\n    # map top 3 topics to doc\n    outdf = doc_topic_df.progress_apply(lambda x: get_top3_topics(x), axis=1)\n    # outdf is a series of list\n    # convert it to a data frame\n    outdf = pd.DataFrame(dict(zip(outdf.index, outdf.values))).T\n    outdf.columns = cols\n    \n    return outdf","a3ec0019":"top_topics = map_topicword2doc(svd_model, X)\nnews_topics = pd.concat([news_df, top_topics], axis=1)","53683936":"top_topics.shape, news_topics.shape","feea9389":"# convert probability from string to float\nnews_topics = news_topics.infer_objects()","7542b99d":"news_topics.head(10)","be4e6b3c":"# plot boxplot of top 3 topic scores to check their distribution\ncols = ['topic_1_prob','topic_2_prob','topic_3_prob']\ncolors = [sns.xkcd_rgb['greenish cyan'], sns.xkcd_rgb['cyan'], sns.xkcd_rgb['reddish pink']]\nfig = plt.figure(figsize=[15,8])\nnews_topics.boxplot(column=cols,\n                   grid=False)\nplt.show()","99b55d5a":"# lda instance\nlda_model = LatentDirichletAllocation(n_components=20,\n                                     random_state=12,\n                                     learning_method='online',\n                                     max_iter=5,\n                                     learning_offset=50)\n# fit model\nlda_model.fit(X)","9916e4cc":"lda_model.components_.shape","93de4e4a":"doc_topic_lda = lda_model.transform(X)\ndoc_topic_lda.shape","d3e96ef1":"word2topics_lda = map_word2topic(lda_model.components_, terms)\n\n# print topic results\nprint('Topics\\t\\tWords')\nfor idx, item in zip(word2topics_lda.index, word2topics_lda):\n    print(idx,'\\t',item)","76d6c174":"#### Umap","4b4075bc":"*  This Exercise is for understanding Topic modeling and we wont be needing Target and Target names for that\n*  You guys can try other problems like multilabel classification on this dataset; plenty of resources are available for that\n*  Some of the example problems are available on [sklearns website](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.fetch_20newsgroups.html)","c19cea7c":"## Text preprocessing","2603155c":"*  well If you compare topics assigned to an article with target names, it's really not that impressive","be15f439":"### Latent Semantic Analysis (LSA)","0e7b5103":"### Distribution of Topics","7f1327b3":"*  Few topics have some kind pattern in the most likey words for that perticular topic but its not somthing amazing\n*  we can say LSA does a decent job but not great job\n*  it looks like we will do fairly well by reducing number of topics\n*  may be checking target topics once again to find a good number topics will do the trick, may be I'll work on that in later version of this notebook","3853db19":"# Topic modeling on 20 news group data set","fff9f923":"### Visualize news vectors","64e390b8":"#### PCA","bd52e7d1":"#### map topics to terms","e74a11c3":"*  PCA can't be used for sparse data\n*  There is some issue with umap implementation\n*  so we'll use t-SNE for our analysis","1587b462":"## Import libraries","b1881af2":"## Load data","c4339914":"*  not very useful, isn't it?","d5a414e1":"#### map document to topics and terms","e186b412":"*  All the articles are almost uniformly ditributed among 20 topics","85556046":"*  we'll use TF-IDF vectorizer\n*  it is also sometimes reffered as document-term matrix","0f169cad":"#### t-SNE","3d2918af":"### WordCloud of processed text","73f0854f":"### Featurize News article","0540b09f":"## Topic model","503261ff":"### Latent Dirichlet Allocation (LDA) "}}