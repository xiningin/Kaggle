{"cell_type":{"8107e31b":"code","65987781":"code","961a0232":"code","c7ec24c3":"code","85f2f1d9":"code","ed393b5d":"code","25381d96":"code","bc70acfb":"code","dcb62128":"code","81580198":"code","4db7e51b":"code","db36df83":"code","7e386127":"code","8f3fc727":"code","179479a6":"markdown","424999dd":"markdown","1b79ba4d":"markdown","ab635287":"markdown","cc13697f":"markdown","736bf1d9":"markdown","9d068993":"markdown","d103f8ad":"markdown","ac4fd40b":"markdown","97b95d3a":"markdown","9aa9654b":"markdown","c5a11c8b":"markdown","a99f9aeb":"markdown","a0fc0fa0":"markdown","72cdfcc8":"markdown","ebbae424":"markdown","e73eeb19":"markdown","92facb99":"markdown","b94a1b9d":"markdown","bce8f318":"markdown","04db1820":"markdown","b981d751":"markdown","c540594f":"markdown"},"source":{"8107e31b":"#Importing libraries\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pylab as plt\nimport PIL\nimport PIL.Image\nimport pathlib","65987781":"# DATA_URL = 'https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz'\n# data_dir = tf.keras.utils.get_file(\"flower_photos\", DATA_URL, untar=True)","961a0232":"data_dir = \"..\/input\/flowers-recognition\/flowers\"","c7ec24c3":"def display_image_in_actual_size(im_path):\n    dpi = 100\n    im_data = plt.imread(im_path)\n    height, width, depth = im_data.shape\n    figsize = width \/ float(dpi), height \/ float(dpi)\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0, 0, 1, 1])\n    # Hide spines, ticks, etc.\n    ax.axis('off')\n    # Display the image.\n    ax.imshow(im_data, cmap='gray')\n    plt.show()","85f2f1d9":"IMAGE_PATH = \"..\/input\/flowers-recognition\/flowers\/rose\/10503217854_e66a804309.jpg\"\ndisplay_image_in_actual_size(IMAGE_PATH)","ed393b5d":"IMAGE_PATH = \"..\/input\/flowers-recognition\/flowers\/rose\/11944957684_2cc806276e.jpg\"\ndisplay_image_in_actual_size(IMAGE_PATH)","25381d96":"# Let's create keyword argument options.\npixels = 224\nBATCH_SIZE = 32\nIMAGE_SIZE = (pixels, pixels)\n# And then I am going to pass them into their function calls:\ndatagen_kwargs = dict(rescale=1.\/255, \n                      validation_split = .20)\ndataflow_kwargs = dict(target_size = IMAGE_SIZE, \n                       batch_size = BATCH_SIZE,\n                      interpolation = \"bilinear\")\n# Now that Create a generator object:\nvalid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(**datagen_kwargs)","bc70acfb":"valid_generator = valid_datagen.flow_from_directory(data_dir, \n                                                    subset=\"validation\", \n                                                    shuffle=False, \n                                                    **dataflow_kwargs)","dcb62128":"# Let's use same generator object for training data:\ntrain_datagen = valid_datagen\ntrain_generator = train_datagen.flow_from_directory(data_dir, \n                                                    subset = \"training\",\n                                                    shuffle = True, \n                                                    **dataflow_kwargs)","81580198":"# Let's take a look shapes of batches image and label.\nfor image_batch, labels_batch in train_generator:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","4db7e51b":"# First of all import tensorflow hub.\nimport tensorflow_hub as hub\n# Built the model.\nNUM_CLASSES = 5\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n    hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_101\/feature_vector\/4\", trainable = False),\n    tf.keras.layers.Dense(NUM_CLASSES, activation = \"softmax\")\n])\nmodel.build([None, 224, 224, 3])","db36df83":"# Let's take a look summary of the model. \nmodel.summary()","7e386127":"model.compile(optimizer=tf.keras.optimizers.SGD(lr = 0.005, momentum = 0.9),\n             loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True,label_smoothing = 0.1),\n             metrics = [\"accuracy\"])","8f3fc727":"steps_per_epoch = train_generator.samples \/\/ train_generator.batch_size\nvalidation_steps = valid_generator.samples \/\/ valid_generator.batch_size\nmodel.fit(train_generator,\n         epochs=5,\n         steps_per_epoch = steps_per_epoch,\n         validation_data = valid_generator,\n         validation_steps = validation_steps)","179479a6":"## Exploring the Dataset","424999dd":"## Conclusion","1b79ba4d":"Now you can determine the source directory. This generator will only stream 20% of the data, and this is designated as a validation dataset:","ab635287":"I am going to load dataset in kaggle input.","cc13697f":"Don't forget to follow on Tirendaz Academy [YouTube-Tr](https:\/\/youtube.com\/c\/tirendazakademi), [YouTube-Eng](https:\/\/www.youtube.com\/channel\/UCFU9Go20p01kC64w-tmFORw), [Twitter](https:\/\/twitter.com\/TirendazAcademy), [Medium](https:\/\/tirendazacademy.medium.com), [GitHub](https:\/\/github.com\/TirendazAcademy) and [LinkedIn](https:\/\/www.linkedin.com\/in\/tirendaz-academy)","736bf1d9":"## Training the Model","9d068993":"In this tutorial, I am going to show to how to preparing image data using the flower dataset provided by TensorFlow. The flower dataset consists of five types of flowers and diverse image dimensions. \n\nFirst of all, let me import libraries.","d103f8ad":"To use the ResNet model, I am going to convert image sizes into 224\\*224 with three color channels (RGB). To do this, I am going to use ImageDataGenerator class and the flow_from_directory method.\n ImageDataGenerator creates a generator object, which generates streaming data from the directory as specified by flow_from_directory.","ac4fd40b":"That is all. In this tutorial, I showed how to prepar image data for preprocessing. I used flower photos dataset and ResNet, which is a prebuild model. This model usually is used for image dataset. ","97b95d3a":"- [KC Tung, 2021, TensorFlow 2 Pocket Reference](https:\/\/www.amazon.com\/TensorFlow-Pocket-Reference-Building-Deploying\/dp\/1492089184)\n- [TensorFlow Tutorial](https:\/\/www.tensorflow.org\/tutorials)","9aa9654b":"I have dataset consist of images arranged right format. It's ready the dataset to train model. I am going to use ResNet model in TensorFlow Hub, which is a library for loading trained models from TFHub in a single line of code. ","c5a11c8b":"## Compiling the model","a99f9aeb":"## Training the model","a0fc0fa0":"## Preparing Image Data for Processing with TensorFlow 2.x","72cdfcc8":"## Preprocessing the data","ebbae424":"Let me create a function to take a look pictures in dataset.","e73eeb19":"## Loading Dataset","92facb99":"As you can see, the dimensions and aspect ratios of these images are different.","b94a1b9d":"![rose](https:\/\/images.unsplash.com\/photo-1579053778004-3a4d3f0fae19?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=747&q=80)","bce8f318":"Let me display a few picture of dataset.","04db1820":"If you want you can directly load flower_photos dataset using get_file() method. This method downloads a file from a URL if it not already in the cache. You can find this method as follow:","b981d751":"## Resources","c540594f":"Note that the output is NumPy arrays. For a batch of images, the sample size is 32, with 224 pixels in height and width and three channels representing RGB color space. For the label batch, there are likewise 32 samples. Each row is one-hot encoded to represent which of the five classes it belongs to."}}