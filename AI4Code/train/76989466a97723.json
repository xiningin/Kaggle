{"cell_type":{"c05cd497":"code","124026d3":"code","863c33e7":"code","285b4492":"code","2296c9de":"code","08a2809f":"code","a2f3d056":"code","6fa29a09":"code","eec4aa6e":"code","f0701611":"code","083bd76f":"markdown","014979dc":"markdown"},"source":{"c05cd497":"import numpy as np\nimport pandas as pd\nimport torch\nimport transformers\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')","124026d3":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.metrics import accuracy_score\nimport torch","863c33e7":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self,df,out,max_len=96):\n        \n        self.df = df\n        self.out = out\n        self.max_len = max_len\n        self.tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n        \n    def __getitem__(self,index):\n        data = {}\n        row = self.df.iloc[index]\n        ids,masks,labels = self.get_input_data(row)\n        data['ids'] = torch.tensor(ids)\n        data['masks'] = masks\n        data['labels'] = torch.tensor(self.out.iloc[index].values[0],dtype=torch.float32)\n        return data\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self,row):\n        ids = self.tokenizer.encode(row[0],add_special_tokens=True)\n        pad_len = self.max_len - len(ids)\n        if pad_len > 0 :\n            ids += [0]*pad_len\n        ids = torch.tensor(ids)    \n        masks = torch.where(ids != 0 , torch.tensor(1),torch.tensor(0))\n        return ids,masks,self.out.iloc[0].values\n    \n        ","285b4492":"df = pd.read_csv('https:\/\/github.com\/clairett\/pytorch-sentiment-classification\/raw\/master\/data\/SST2\/train.tsv',delimiter='\\t',header=None)\ntrain_set,val_set = train_test_split(df,test_size = 0.2)\ntrain_loader = torch.utils.data.DataLoader(Dataset(pd.DataFrame(train_set[0]),pd.DataFrame(train_set[1]),max_len=80),batch_size = 32, shuffle = True, num_workers = 2)\nval_loader = torch.utils.data.DataLoader(Dataset(pd.DataFrame(val_set[0]),pd.DataFrame(val_set[1]),max_len=80),batch_size = 32,num_workers = 2)","2296c9de":"def binary_acc(y_pred, y_test):\n    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n\n    correct_results_sum = (y_pred_tag == y_test).sum().float()\n    acc = correct_results_sum\/y_test.shape[0]\n    acc = torch.round(acc * 100)\n    \n    return acc.item()","08a2809f":"class Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        config = transformers.DistilBertConfig.from_pretrained('distilbert-base-uncased')\n        self.distilBert = transformers.DistilBertModel.from_pretrained('distilbert-base-uncased',config=config)\n        self.fc0 = nn.Linear(768,512)\n        self.d0 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(512,256)\n        self.d1 = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(256,1)\n        nn.init.normal_(self.fc0.weight,std= 0.1)\n        nn.init.normal_(self.fc0.bias ,0.)\n        nn.init.normal_(self.fc1.weight,std =0.1)\n        nn.init.normal_(self.fc1.bias, 0.)\n        nn.init.normal_(self.fc2.weight,std=0.1)\n        nn.init.normal_(self.fc2.bias , 0.)\n\n        \n    def forward(self,input_ids,attention_mask):\n        hid= self.distilBert(input_ids,attention_mask = attention_mask)\n        hid= hid[0][:,0]\n        x = self.fc0(hid)\n        x = F.relu(x)\n        x = self.d0(x)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.d1(x)\n        return self.fc2(x)\n","a2f3d056":"criterion = nn.BCEWithLogitsLoss(reduction='mean').to('cuda')\nmodel = Model().to('cuda')\nfor params in model.distilBert.parameters():\n    params.require_grad = False\n    params._trainable = False\noptimizer = torch.optim.AdamW(model.parameters(),lr=2e-5)","6fa29a09":"\nfor epoch in range(3):\n        \n        epoch_loss = 0\n        val_loss = 0\n        correct = 0\n        accuracy = 0\n\n        for data in train_loader:\n            ids = data['ids'].cuda()\n            masks = data['masks'].cuda()\n            labels = data['labels'].cuda()\n            labels = labels.unsqueeze(1)\n            model.train()\n            optimizer.zero_grad()\n            outputs = model(ids,masks)\n            loss = criterion(outputs,labels)\n            loss.backward()\n            optimizer.step()\n           \n            epoch_loss += loss.item()\n            \n        print(f'Train Epoch {epoch} : Loss {epoch_loss\/len(train_loader)}')\n        print(\"Train Accuracy : \",binary_acc(outputs,labels))\n        model.eval()\n        correct = 0\n        for data in val_loader:\n            ids = data['ids'].cuda()\n            masks = data['masks'].cuda()\n            labels = data['labels'].cuda()\n            outputs = model(ids,masks)\n            labels = labels.unsqueeze(1)\n            loss = criterion(outputs,labels)\n            val_loss += loss.item()\n            \n        print(f'Val Epoch {epoch} : Loss {val_loss\/len(val_loader)}')\n        print(\"Val Accuracy : \",binary_acc(outputs,labels))\n        \n","eec4aa6e":"def test(sentence):\n    tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    ids = tokenizer.encode(sentence,add_special_tokens=True)\n    padded = ids + [0]*(80 - len(ids))\n    padded = torch.tensor(padded,dtype=torch.int64).unsqueeze(0)\n    masks = torch.where(padded != 0 , torch.tensor(1), torch.tensor(0)).cuda()\n    padded = padded.cuda()\n    model.eval()\n    output = model(padded,masks)\n    return torch.round(F.sigmoid(output)).item()","f0701611":"test(\"Kaggle is a very nice platform to start Deep Learning.\")","083bd76f":"<h1> Neural Networks <\/h1>","014979dc":"<h1> Sentiment Classification Using DistilBert and Pytorch<\/h1>\n\nIn this notebook, we will see a great overview of how to classify sentences using DistilBert pretrained by HuggingFace and a fine-tuned Neural Network to classify the sentiment.\n\n\n"}}