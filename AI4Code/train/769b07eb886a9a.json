{"cell_type":{"49876425":"code","f893ccae":"code","b51c413d":"code","ace607c2":"code","799a2788":"code","b9655b44":"code","8cb4c8b3":"code","ef864ce8":"code","3717da4e":"code","b39ab9f6":"code","63f2f940":"code","37d36a13":"code","d80b7617":"code","e45c4707":"code","8e590a49":"code","27470801":"code","78b5c0df":"code","479997bf":"code","fb80151f":"code","bf81a989":"code","dba8b6ef":"code","cbc93535":"code","443a8e24":"code","532b80f4":"code","d788c92e":"code","4c9fea64":"code","02c54ac0":"code","20374d50":"code","5faadc42":"code","38603373":"code","2fc7ffc4":"code","416643a1":"code","29050616":"code","abfba3bf":"code","365ec928":"code","b9b5957c":"code","03355f8b":"code","e8c36bdf":"code","81e59721":"code","1ac982e2":"code","83fb0163":"code","acbaa1a4":"code","8c6a282d":"code","380b9bd5":"code","9c810b9f":"code","7055b7a5":"code","d6cd93ed":"code","eecef4aa":"code","6d0b2fa9":"code","1c55b217":"code","736e2143":"code","93d5148a":"code","2905321e":"code","3e10a111":"code","fcc495c9":"code","2fc1ccda":"code","d6bcb910":"code","336172bb":"code","28db8b57":"code","26fc5497":"code","6e43d2ea":"code","c4b8c09f":"code","91363851":"code","ca084817":"code","53bfcf51":"code","8c0c36c4":"code","20f7e9d3":"code","0dfb818f":"code","5396fd02":"code","64ebf6a4":"code","64a9bdf5":"code","3dc0d7af":"code","65e79cf1":"code","8c9b5b35":"code","e65becbd":"code","c784fb89":"markdown","9bb4fcf5":"markdown","7a39db0a":"markdown","0d78af1c":"markdown","928abfd5":"markdown","92f1958f":"markdown","5f5388f0":"markdown","d94dbf5b":"markdown","b7c783b4":"markdown","01e89e3a":"markdown","092d7852":"markdown","a73fabdb":"markdown","99e5d43f":"markdown","e28af16d":"markdown","5ec2581a":"markdown","e6406dff":"markdown","18927b04":"markdown","561d80a5":"markdown","443318ac":"markdown","47580df5":"markdown","9389bb7b":"markdown","490a0c17":"markdown","28fdf642":"markdown","7a70f049":"markdown","190bcd29":"markdown","f0eb9acb":"markdown","07b5c9b0":"markdown","4b44a141":"markdown","172a8fb4":"markdown","b026b86e":"markdown","b29aa8a7":"markdown","4edf4ea6":"markdown","98ce10cd":"markdown","3bd0b5b0":"markdown","f27bd914":"markdown","9494728c":"markdown","3a77a42e":"markdown"},"source":{"49876425":"!pip install opendatasets --upgrade --quiet","f893ccae":"import os\nimport torch\nimport torchvision\nimport tarfile\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tt\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nfrom torchvision.transforms import ToTensor\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nmatplotlib.rcParams['figure.facecolor'] = '#ffffff'","b51c413d":"import opendatasets as od\n\ndataset_url = 'https:\/\/www.kaggle.com\/jutrera\/stanford-car-dataset-by-classes-folder'\nod.download(dataset_url)","ace607c2":"from torch.utils.data import Dataset","799a2788":"import os\n\nDATA_DIR_TRAIN = '.\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/train'\ntrain_classes = os.listdir(DATA_DIR_TRAIN)\n\nDATA_DIR_TEST = '.\/stanford-car-dataset-by-classes-folder\/car_data\/car_data\/test'\ntest_classes = os.listdir(DATA_DIR_TEST)\n\ntrain_classes[:5], test_classes[:5]","b9655b44":"# Tie the class indices to their names\n\ndef find_classes(dir):\n    train_classes = os.listdir(dir)\n    train_classes.sort()\n    train_class_to_idx = {train_classes[i]: i for i in range(len(train_classes))}\n    return train_classes, train_class_to_idx\n    test_class_to_idx = {test_classes[i]: i for i in range(len(test_classes))}\n    return test_classes, test_class_to_idx\n\ntrain_classes, train_c_to_idx = find_classes(DATA_DIR_TRAIN)\ntest_classes, test_c_to_idx = find_classes(DATA_DIR_TEST)","8cb4c8b3":"train_classes[:5], test_classes[:5]","ef864ce8":"len(train_classes), len(test_classes)","3717da4e":"train_classes == test_classes","b39ab9f6":"# Tie the class indices to their names\n\ndef find_classes(dir):\n    test_classes = os.listdir(dir)\n    test_classes.sort()\n    test_class_to_idx = {test_classes[i]: i for i in range(len(test_classes))}\n    return test_classes, test_class_to_idx\ntest_classes, test_c_to_idx = find_classes(DATA_DIR_TEST)","63f2f940":"def extract_class(Datasets):\n  for vals in os.listdir(Datasets):\n    print(vals)","37d36a13":"extract_class(DATA_DIR_TRAIN)","d80b7617":"train_dataset = ImageFolder(DATA_DIR_TRAIN, transform = ToTensor())\ntest_dataset = ImageFolder(DATA_DIR_TEST, transform = ToTensor())","e45c4707":"len(train_dataset), len(test_dataset)","8e590a49":"image, label = train_dataset[0]\nprint('image_shape:', image.shape, 'Label:',label)","27470801":"image, label = train_dataset[1]\nprint('image_shape:', image.shape, 'Label:',label)","78b5c0df":"def show_example(img, label):\n    print('Label: ', train_dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))","479997bf":"show_example(*train_dataset[0])","fb80151f":"show_example(*test_dataset[7030])","bf81a989":"show_example(*test_dataset[0])","dba8b6ef":"train_tfms = tt.Compose([tt.Resize((256, 256)),\n                         tt.RandomRotation(0),\n                         tt.ToTensor(),\n                         tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\ntest_tfms = tt.Compose([tt.Resize((256, 256)),\n                        tt.ToTensor(),\n                        tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])","cbc93535":"train_dataset = ImageFolder(DATA_DIR_TRAIN, transform = train_tfms)\ntest_dataset = ImageFolder(DATA_DIR_TEST, transform = test_tfms)","443a8e24":"# Pick the last 4 from the training dataset\ntrain_dataset.classes[-5:-1]","532b80f4":"# Lift index 5: 9 in the test dataset\ntest_dataset.classes[5:9]","d788c92e":"len(train_dataset), len(test_dataset)","4c9fea64":"image, label = train_dataset[0]\nprint('image_shape:', image.shape, 'Label:',label)","02c54ac0":"image, label = test_dataset[7030]\nprint('image_shape:', image.shape, 'Label:',label)","20374d50":"show_example(*train_dataset[0])","5faadc42":"show_example(*test_dataset[7030])","38603373":"show_example(*test_dataset[0]);","2fc7ffc4":"random_seed = 42\ntorch.manual_seed(random_seed);","416643a1":"val_percent = 0.1\nval_size = int(val_percent * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\n\ntrain_ds, val_ds = random_split(train_dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","29050616":"!pip install jovian --upgrade --quiet","abfba3bf":"import jovian","365ec928":"jovian.log_dataset(dataset_url = dataset_url, val_size = val_size, random_seed = random_seed)","b9b5957c":"batch_size = 128","03355f8b":"train_dl = DataLoader(train_ds, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\nval_dl = DataLoader(val_ds, batch_size * 2, num_workers = 4, pin_memory=True)","e8c36bdf":"def show_batch(dl):\n  for images, labels in dl:\n    fig, ax = plt.subplots(figsize = (12,6))\n    ax.set_xticks([]), ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1,2,0))\n    break","81e59721":"show_batch(train_dl);","1ac982e2":"show_batch(val_dl); ","83fb0163":"# Execute this to save new versions of the notebook\n# jovian.commit(project=\"stanford-cars\")","acbaa1a4":"simple_model = nn.Sequential(\n    nn.Conv2d(3,8, kernel_size = 3, stride = 1, padding = 1),\n    nn.MaxPool2d(2,2)\n)","8c6a282d":"for images, labels in train_dl:\n  print('images.shape:', images.shape)\n  out = simple_model(images)\n  print('out.shape:', out.shape)\n  break","380b9bd5":"class ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n        \ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","9c810b9f":"class StanfordCarsModel(ImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 16 x 128 x 128\n\n            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 32 x 64 x 64\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2), # output: 64 x 32 x 32\n\n            nn.Flatten(), \n            nn.Linear(64*32*32, 1024),\n            nn.ReLU(),\n            nn.Linear(1024, 512),\n            nn.ReLU(),\n            nn.Linear(512, 196))\n        \n    def forward(self, xb):\n        return self.network(xb)","7055b7a5":"model = StanfordCarsModel()\nmodel","d6cd93ed":"for images, labels in train_dl:\n  print('images.shape:', images.shape)\n  out = model(images)\n  print('out.shape:', out.shape)\n  print('out[0]:', out[0])\n  break","eecef4aa":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","6d0b2fa9":"device = get_default_device()\ndevice","1c55b217":"train_dl = DeviceDataLoader(train_dl, device)\nval_dl = DeviceDataLoader(val_dl, device)\nto_device(model, device);","736e2143":"#jovian.commit()","93d5148a":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","2905321e":"model = to_device(StanfordCarsModel(), device)","3e10a111":"evaluate(model, val_dl)","fcc495c9":"num_epochs = 10\nopt_func = torch.optim.Adam\nlr = 5e-2","2fc1ccda":"jovian.reset()\njovian.log_hyperparams({\n    'num_epochs': num_epochs,\n    'opt_func': opt_func.__name__,\n    'batch_size': batch_size,\n    'lr': lr,\n})","d6bcb910":"history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)","336172bb":"jovian.log_metrics(train_loss=history[-1]['train_loss'], \n                   val_loss=history[-1]['val_loss'], \n                   val_acc=history[-1]['val_acc'])","28db8b57":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');","26fc5497":"plot_accuracies(history)","6e43d2ea":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","c4b8c09f":"plot_losses(history)","91363851":"#jovian.commit()","ca084817":"test_dataset = ImageFolder(DATA_DIR_TEST, transform = test_tfms)","53bfcf51":"def predict_image(img, model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return test_dataset.classes[preds[0].item()]","8c0c36c4":"img, label = test_dataset[0]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","20f7e9d3":"img, label = test_dataset[1002]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","0dfb818f":"img, label = test_dataset[6153]\nplt.imshow(img.permute(1, 2, 0))\nprint('Label:', test_dataset.classes[label], ', Predicted:', predict_image(img, model))","5396fd02":"test_loader = DeviceDataLoader(DataLoader(test_dataset, batch_size*2), device)\nresult = evaluate(model, test_loader)\nresult","64ebf6a4":"jovian.log_metrics(test_loss=result['val_loss'], test_acc=result['val_acc'])","64a9bdf5":"torch.save(model.state_dict(), 'stanfordcars-cnn.pth')","3dc0d7af":"model2 = to_device(StanfordCarsModel(), device)","65e79cf1":"model2.load_state_dict(torch.load('stanfordcars-cnn.pth'))","8c9b5b35":"evaluate(model2, test_loader)","e65becbd":"#jovian.commit()","c784fb89":"The `.state_dict` method returns an `OrderedDict` containing all the weights and bias matrices mapped to the right attributes of the model. To load the model weights, we can redefine the model with the same structure, and use the `.load_state_dict` method.","9bb4fcf5":"Just as we have recorded the hyperparameters, we can also record the final metrics achieved by the model using `jovian.log_metrics` for reference, analysis and comparison.","7a39db0a":"# Stanford-cars\n\n![](https:\/\/www.motortrend.com\/uploads\/sites\/5\/2019\/12\/MotorTrend-Most-Important-Cars-of-the-Decade.jpg?fit=around%7C875:492)\n\nThe objective of this notebook is to show generative modelling of `Stanford Cars` and to display how powerful Neural Networks are","0d78af1c":"The training and test datasets have 8144 and 8041 elements respectively. Let's look at a sample element from each. Each element is a tuple, containing an image tensor and a label. Since the data consists of X * Y (different sized) px color images with 3 channels (RGB), each image tensor has the shape `(3, X, Y)`.","928abfd5":"Let's verify that the model produces the expected output on a batch of training data. The 196 outputs for each image can be interpreted as probabilities for the 196 target classes (after applying softmax), and the class with the highest probability is chosen as the label predicted by the model for the input image. Check out [Part 3 (logistic regression)](https:\/\/jovian.ml\/aakashns\/03-logistic-regression#C50) for a more detailed discussion on interpeting the outputs, applying softmax and identifying the predicted labels.","92f1958f":"## Training the Model\n\nWe'll define two functions: `fit` and `evaluate` to train the model using gradient descent and evaluate its performance on the validation set. For a detailed walkthrough of these functions, check out the [previous tutorial](https:\/\/jovian.ai\/aakashns\/03-logistic-regression).","5f5388f0":"Once again, let's save and commit the notebook before we proceed further.","d94dbf5b":"## Saving and loading the model\n\nSince we've trained our model for a long time and achieved a resonable accuracy, it would be a good idea to save the weights of the model to disk, so that we can reuse the model later and avoid retraining from scratch. Here's how you can save the model.","b7c783b4":"We can look at batches of images from the dataset using the `make_grid` method from `torchvision`. Each time the following code is run, we get a different bach, since the sampler shuffles the indices before creating batches.","01e89e3a":"We can view the images using `matplotlib`, but we need to change the tensor dimensions to `(256,256,3)`. ","092d7852":"Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)","a73fabdb":"\nWe'll use `nn.Sequential` to chain the layers and activations functions into a single network architecture.","99e5d43f":"You can see the difference between the standardized and non-standardized images earlier due to normalization","e28af16d":"Just as a sanity check, let's verify that this model has the same loss and accuracy on the test set as before.","5ec2581a":"Let's define a helper function `predict_image`, which returns the predicted label for a single image tensor.","e6406dff":"Before continuing, let us save our work to the cloud using `jovian.commit`.","18927b04":"We can also plot the valdation set accuracies to study how the model improves over time.","561d80a5":"## Training and Validation Datasets\n\nWhile building real world machine learning models, it is quite common to split the dataset into 3 parts:\n\n1. **Training set** - used to train the model i.e. compute the loss and adjust the weights of the model using gradient descent.\n2. **Validation set** - used to evaluate the model while training, adjust hyperparameters (learning rate etc.) and pick the best version of the model.\n3. **Test set** - used to compare different models, or different types of modeling approaches, and report the final accuracy of the model.\n\nSince there's no predefined validation set, we can set aside a small portion (10 percent of images) of the training set to be used as the validation set. We'll use the `random_split` helper method from PyTorch to do this. To ensure that we always create the same validation set, we'll also set a seed for the random number generator.","443318ac":"## Downloading the Dataset\n\nWe'll use the `Stanford Cars` dataset from https:\/\/www.kaggle.com\/jutrera\/stanford-car-dataset-by-classes-folder .We have 16,185 images of 196 classes of cars. The data is split into 8,144 training images and 8,041 testing images, where each class has been split roughly in a 50-50 split. Classes are typically at the level of Make, Model, Year.\n\nWe can use the [`opendatasets`](https:\/\/github.com\/JovianML\/opendatasets) library to download the [dataset](https:\/\/www.kaggle.com\/splcher\/animefacedataset) from Kaggle. `opendatasets` uses the [Kaggle Official API](https:\/\/github.com\/Kaggle\/kaggle-api) for downloading datasets from Kaggle. ","47580df5":"To display batches of data in a grid, we need to normalize the dimensions of the images. Here we are using 256 x 256px images with normalization to yield `3 * 256 * 256`","9389bb7b":"We can now create data loaders for training and validation, to load the data in batches","490a0c17":"Before we begin training, let's instantiate the model once again and see how it performs on the validation set with the initial set of parameters.","28fdf642":"To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU as required. ","7a70f049":"Our model reaches an accuracy of around 2.26%, and by looking at the graph, it seems unlikely that the model will achieve an accuracy higher than 2.26% even after training for a long time. This suggests that we might need to use a more powerful model to capture the relationship between the images and the labels more accurately. This can be done by adding more convolutional layers to our model, or incrasing the no. of channels in each convolutional layer, or by using regularization techniques.\n\nWe can also plot the training and validation losses to study the trend.","190bcd29":"## Testing with individual images\n\nWhile we have been tracking the overall accuracy of a model so far, it's also a good idea to look at model's results on some sample images. Let's test out our model with some images from the predefined test dataset of 10000 images. We begin by creating a test dataset using the `ImageFolder` class.\n","f0eb9acb":"Let's look at the standardized\/normalized\/transformed datasets. The list of classes is stored in the `.classes` property of the dataset. The numeric label for each element corresponds to index of the element's label in the list of classes.","07b5c9b0":"Identifying where our model performs poorly can help us improve the model, by collecting more training data, increasing\/decreasing the complexity of the model, and changing the hypeparameters.\n\nAs a final step, let's also look at the overall loss and accuracy of the model on the test set, and record using `jovian`. We expect these values to be similar to those for the validation set. If not, we might need a better validation set that has similar data and distribution as the test set (which often comes from real world data).","4b44a141":"The standardized training and test datasets have 8144 and 8041 elements respectively. Each element is a tuple, containing an image tensor and a label. Since the data consists of 256 * 256px color images with 3 channels (RGB), each image tensor has the shape `(3, 256, 256)`.\n\n","172a8fb4":"The initial accuracy is around 0.4%, which is what one might expect from a randomly intialized model (since it has a 1 in 196 chance of getting a label right by guessing randomly).\n\nWe'll use the following *hyperparmeters* (learning rate, no. of epochs, batch_size etc.) to train our model. As an exercise, you can try changing these to see if you have achieve a higher accuracy in a shorter time. ","b026b86e":"The above directory structure (one folder per class) is used by many computer vision datasets, and most deep learning libraries provide utilites for working with such datasets. We can use the `ImageFolder` class from `torchvision` to load the data as PyTorch tensors.","b29aa8a7":"## Convolutional Neural Networks\n\nWe will use a convolutional neural network, using the `nn.Conv2d` class from PyTorch","4edf4ea6":"The `jovian` library also provides a simple API for recording important parameters related to the dataset, model training, results etc. for easy reference and comparison between multiple experiments. Let's record `dataset_url`, `val_pct` and `rand_seed` using `jovian.log_dataset`.","98ce10cd":"The image tensors of element 0 and 1 from the train dataset are of different shapes i.e. `3 * 523 * 700` v\/s `3 * 64 * 85` . \n\nWe can view the images using `matplotlib`, but we need to change the tensor dimensions to `(X,Y,3)`. Let's create a helper function to display an image and its label.\n","3bd0b5b0":"It's important to record the hyperparameters of every experiment you do, to replicate it later and compare it against other experiments. We can record them using `jovian.log_hyperparams`.","f27bd914":"Let's make one final commit using `jovian`.","9494728c":"The `Conv2d` layer transforms a 3-channel image to an 8-channel *feature map*, and the `MaxPool2d` layer halves the height and width. The feature map gets smaller as we add more layers, until we are finally left with a small feature map, which can be flattened into a vector. We can then add some fully connected layers at the end to get vector of size 10 for each image.\n\n<img src=\"https:\/\/i.imgur.com\/KKtPOKE.png\" style=\"max-width:540px\">\n\nLet's define the model by extending an `ImageClassificationBase` class which contains helper methods for training & validation.","3a77a42e":"We can now wrap our training and validation data loaders using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available), and use `to_device` to move our model to the GPU (if available)."}}