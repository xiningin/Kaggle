{"cell_type":{"2447dbc6":"code","07ceaaa4":"code","1ebfe60e":"code","2831d52f":"code","63705571":"code","2c498069":"code","c9a6c9af":"code","19ee5d2b":"code","87d27118":"code","1fee13e0":"code","f3fd390b":"code","c4b00145":"code","d2bff65b":"code","0a66c5e6":"code","00c28c99":"code","6d50f508":"code","3196d7c2":"code","fe1eccd1":"code","f8803346":"code","5c6bcf74":"code","c49c3ab7":"code","7f5df855":"code","796e947a":"code","d25836a2":"code","4dfaf1f2":"code","ae0cb817":"code","f5c69545":"code","c6f43d73":"code","689a0e2f":"code","b7d577ac":"code","b3b01d32":"code","65d764fc":"code","c14fe8d0":"code","9fdc9d65":"code","fc7fdecd":"code","1de9bc6a":"code","6cfdd53f":"code","322f87f7":"code","1ec70aaa":"code","9ce9359a":"markdown","a8567874":"markdown","37f3b97a":"markdown","56921cc0":"markdown","7d72bc7a":"markdown","5f74c862":"markdown","80c00e58":"markdown","e8393ab0":"markdown","ec6f0ea4":"markdown","6bc17744":"markdown","eaec4497":"markdown","1a83c76e":"markdown","338e5709":"markdown","55f186a3":"markdown"},"source":{"2447dbc6":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom xgboost import XGBRegressor","07ceaaa4":"train = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\ntrain","1ebfe60e":"test = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\ntest","2831d52f":"# the SalePrice is right skewed\nplt.hist(train.SalePrice)\nplt.xlabel(\"$\")\nplt.ylabel(\"Count\")\nplt.title(\"Sale Price\")\nplt.show()","63705571":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"]) # Normalising target variable","2c498069":"plt.hist(train.SalePrice)\nplt.xlabel(\"log1($)\")\nplt.ylabel(\"Count\")\nplt.title(\"Sale Price Post-Transform\")\nplt.show()","c9a6c9af":"list(train.columns)","19ee5d2b":"train.describe()","87d27118":"train.info()","1fee13e0":"for c in train.columns:\n    print(\"\\n---- %s ---\" % c)\n    print(train[c].value_counts())","f3fd390b":"cat_feat = train.select_dtypes(include=[np.object])\ncat_feat.info()","c4b00145":"cat_feat.nunique()","d2bff65b":"num_feat = train.select_dtypes(include=[np.number])\nnum_feat.info()","0a66c5e6":"num_feat.hist(figsize = (30, 30))","00c28c99":"correlation_matrix = num_feat.corr()\n\nplt.figure(figsize=(40, 40))\n\nax = sns.heatmap(\n    correlation_matrix,\n    vmax=1,\n    square=True,\n    annot=True,\n    fmt='.2f',\n    cmap='BuPu',\n    cbar_kws={\"shrink\": .5},\n    annot_kws={\"size\": 15},\n    robust=True\n)\n\nplt.title('Correlation Matrix of features', fontsize=20)","6d50f508":"correlation_matrix[['SalePrice']].sort_values(['SalePrice'], ascending = False)","3196d7c2":"temp_df_num = train.select_dtypes(include = [np.number])\nfig = plt.figure(figsize = (20, 25))\nfor i in range(len(temp_df_num.columns)):\n    plt.subplot(13, 3, i + 1)\n    sns.scatterplot(x = temp_df_num.columns[i], \n                    y = temp_df_num['SalePrice'], \n                    data = temp_df_num)\n    plt.title(temp_df_num.columns[i])\nfig.tight_layout(pad = 1.0)","fe1eccd1":"pd.set_option('display.max_rows', 500)\ndisplay(train.isnull().sum())\npd.reset_option('display.max_rows')","f8803346":"cols_with_missing = [col for col in train.columns\n                     if train[col].isnull().any()]\ncols_with_missing","5c6bcf74":"msno.matrix(train[cols_with_missing])","c49c3ab7":"msno.bar(train[cols_with_missing])","7f5df855":"for col_name in cols_with_missing:\n    print(f'{col_name}:\\n{train[col_name].unique()}\\n')\n    col_isna = train[col_name].isna().sum()\n    col_count = train[col_name].count()\n    print(f'isna: {col_isna}\\n')\n    print(f'count: {col_count}\\n')\n    print(f'%: {col_isna\/(col_isna + col_count)*100}\\n\\n----------------------------------------\\n')","796e947a":"# save the 'Id' column\ntest_id = test.Id","d25836a2":"# features to remove due to collinearity\nredun = ['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars']\n\ntrain.drop(redun, axis = 1, inplace = True)\ntest.drop(redun, axis = 1, inplace = True)\n\nnum_feat = list(set(num_feat)-set(redun))\ncat_feat = list(set(cat_feat)-set(redun))","4dfaf1f2":"useless = ['YrSold','MoSold', 'Id']\n\ntrain.drop(useless, axis = 1, inplace = True)\ntest.drop(useless, axis = 1, inplace = True)\n\nnum_feat = list(set(num_feat)-set(useless))\ncat_feat = list(set(cat_feat)-set(useless))","ae0cb817":"# too sparse\nsparse = ['PoolQC', 'MiscFeature', 'Alley']\n\ntrain.drop(sparse, axis = 1, inplace = True)\ntest.drop(sparse, axis = 1, inplace = True)\n\nnum_feat = list(set(num_feat)-set(sparse))\ncat_feat = list(set(cat_feat)-set(sparse))","f5c69545":"# Removing outliers\ntrain = train[train.GrLivArea < 4500] \n# train = train.drop(train[(train['OverallQual'] > 9) & (train['SalePrice'] < 220000)].index)","c6f43d73":"# Removing SalePrice from features\nnum_feat.remove(\"SalePrice\")","689a0e2f":"# Specifying input variables\nX_cols = num_feat + cat_feat","b7d577ac":"# Preprocessing for numerical data\n# Replace missing numerical variables with 0\nnumerical_transformer = SimpleImputer(strategy='constant', fill_value = 0)","b3b01d32":"# Preprocessing for categorical data\n# Replaces missing categorical variables with mode\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","65d764fc":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_feat),\n        ('cat', categorical_transformer, cat_feat)\n    ])","c14fe8d0":"# specifying target variable\ny_train = train.SalePrice\ny_train","9fdc9d65":"X_train = train[X_cols]\nX_train","fc7fdecd":"X_test = test[X_cols]\nX_test","1de9bc6a":"\"\"\"\nn_estimators: The model creates decision trees, this parameter specifies the number of trees (default = 100). Higher = more granular, but greater chance of overfitting.\n\nmax_depth = The maximum tree depth (if confused, look up decision trees)\n\nlearning_rate: Used in gradient descent, controls how much the models weights change in response to errors. Too low = overfitting, too high = underfitting.\n\nsubsample: How much of the training data to randomly sample. 0.7 = 70% of training data is randomly sample each iteration.\n\nseed: A Random Seed, set to a value to be able to replicate the same random numbers each time. Useful for testing, can observe changes.\n\nearly_stopping_rounds: Each time the model iterates it either gets better or it doesn't. In our case if it iterates 5 times and doesn't improve, the model stops training. It helps prevent overfitting.\n\neval_set: Selects your evaluation data. The model runs on the training data and evaluates how accurately it makes predictions during training.\n\nVerbose: If you set it to True, it prints the evaluation metric at each boosting stage.\n\n\n\nA lot of parameter tuning is trial and error, these are the best settings that I happened to find. It could be possible to iteratively find the optimal parameters but it would take a considerable amount of time and you may end up overfitting your model, which would result in it performing poorly when used on other data outside of the training and tesing sets.\n\"\"\"\n\nmodel = XGBRegressor(n_estimators = 3460,\n                     max_depth = 3,\n                     learning_rate = 0.01,\n                     subsample = 0.7,\n                     seed=1,\n                     early_stopping_rounds=5,\n                     eval_set=[(X_train, y_train)],\n                     verbose=False)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, \n                y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_test)","6cfdd53f":"# Validate model score\n# Do this to compare changes\nscores = cross_val_score(my_pipeline, X_train, y_train)\nprint(\"Model Cross-val Score: \", scores.mean())","322f87f7":"n_top = 30\n# Combining column names and column scores\nzipped = zip(X_train.columns, my_pipeline['model'].feature_importances_)\ndf = pd.DataFrame(zipped, columns = [\"feature\", \"value\"])\n\n# Sort the features by the absolute value of their coefficient\ndf[\"abs_value\"] = df[\"value\"].apply(lambda x: abs(x))\ndf[\"colors\"] = df[\"value\"].apply(lambda x: \"green\" if x > 0 else \"red\")\ndf = df.sort_values(\"abs_value\", ascending=False)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 7))\nsns.barplot(x=\"feature\",\n            y=\"value\",\n            data=df.head(n_top),\n            palette=df.head(n_top)[\"colors\"])\nax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=20)\nax.set_title(\"Top features\", fontsize=25)\nax.set_ylabel(\"Coefficient\", fontsize=22)\nax.set_xlabel(\"Feature Name\", fontsize=22)","1ec70aaa":"# Creates a dataframe with our results for submission\noutput = pd.DataFrame({'Id': test_id,\n                       'SalePrice': preds})\noutput[\"SalePrice\"] = np.expm1(output[\"SalePrice\"])\noutput.to_csv('submission.csv', index=False)","9ce9359a":"### Outliners","a8567874":"### Most Important Features","37f3b97a":"## Read in data","56921cc0":"## Modelling & Predicting","7d72bc7a":"#### Categorical","5f74c862":"`GrLivArea` has outliners.","80c00e58":"## Evaluating Results","e8393ab0":"#### Numerical","ec6f0ea4":"## Import necessary packages","6bc17744":"## Cleaning the data","eaec4497":"## Exploratory Data Analysis","1a83c76e":"### Missing values","338e5709":"# [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course)","55f186a3":"### Categorical & Numerical features"}}