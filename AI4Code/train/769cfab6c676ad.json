{"cell_type":{"810e858a":"code","6d678f19":"code","00a740ef":"code","69a33b0d":"code","4e3bdda7":"code","0e876e05":"code","0dfa78b7":"code","39dc0b6d":"code","e9ae5271":"code","5a369dd0":"code","aedc3c0c":"code","67b9f0ec":"code","31cfcf91":"code","21974db3":"code","d4efd0b8":"code","b6e65a74":"code","e753100c":"code","22976b00":"code","d7a7a8c0":"code","052f4b48":"code","29c1f0a2":"code","4dfdd17a":"code","0fea07f6":"code","4764f84e":"code","7d0a9a92":"code","a9394903":"code","0e8c2072":"code","1e388b40":"code","d10bf601":"code","292b8c90":"code","9ab2a8a0":"code","aca4825c":"markdown","2194dc77":"markdown","151f4260":"markdown","2aad2fc8":"markdown","17dc20c8":"markdown","32063634":"markdown","0d41ad28":"markdown","3a0eca5e":"markdown","f1e314da":"markdown"},"source":{"810e858a":"from __future__ import print_function, division\nfrom builtins import range, input\n# keras libraries\nimport keras\nfrom keras.layers import *\nfrom keras.models import Model,Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n# sklearn and matplotlibg for visualisation\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom glob import glob\nimport cv2","6d678f19":"train_path='\/kaggle\/input\/fruit-images-for-object-detection\/data\/train'\nval_path='\/kaggle\/input\/fruit-images-for-object-detection\/data\/test'","00a740ef":"image_files = glob(train_path + '\/*\/*.jp*g')\nvalid_image_files = glob(val_path + '\/*\/*.jp*g')\nprint(\"Number of Images for Training: \",len(image_files))\nprint(\"Number of Images for validating: \",len(glob(val_path + '\/*\/*.jp*g')))\n\n# useful for getting number of classes\nfolders = glob(train_path + '\/*')\nprint(\"Number of classes: \",len(folders))\n\n# look at a random image \nplt.imshow(image.load_img(np.random.choice(image_files)))\n\nplt.show()","69a33b0d":"# re-size all the images to 100x100\nIMAGE_SIZE = [224, 224] \n\n# using the VGG16 model but not including the final output layer by using the command (include_top=False).\nvgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n\n# don't train existing weights\nfor layer in vgg.layers:\n    layer.trainable = False\n\n# additional layers\nx = Flatten()(vgg.output)\n# we can add additional fully connected layers like this.\n# x = Dense(1000, activation='relu')(x)\nprediction = Dense(len(folders), activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=vgg.input, outputs=prediction)\n\n# view the structure of the model\nmodel.summary()","4e3bdda7":"# compliling the model.\nmodel.compile(\n  loss='categorical_crossentropy',\n  optimizer='rmsprop',\n  metrics=['accuracy']\n)\n\n# create an instance of ImageDataGenerator\ngen = ImageDataGenerator(\n  rotation_range=20,\n  width_shift_range=0.1,\n  height_shift_range=0.1,\n  shear_range=0.1,\n  zoom_range=0.2,\n  horizontal_flip=True,\n  vertical_flip=True,\n  preprocessing_function=preprocess_input\n)","0e876e05":"# training config:\nepochs = 10\nbatch_size = 32\n\n# create generators\ntrain_generator = gen.flow_from_directory(\n  train_path,\n  target_size=IMAGE_SIZE,\n  shuffle=False,\n  batch_size=batch_size,\n)\nvalid_generator = gen.flow_from_directory(\n  val_path,\n  target_size=IMAGE_SIZE,\n  shuffle=False,\n  batch_size=batch_size,\n)","0dfa78b7":"# fit the model\nr = model.fit_generator(\n  train_generator,\n  validation_data=valid_generator,\n  epochs=epochs,\n  steps_per_epoch=len(image_files) \/\/ batch_size,\n  validation_steps=len(valid_image_files) \/\/ batch_size,\n)","39dc0b6d":"# accuracies\nplt.plot(r.history['accuracy'], label='train acc')\nplt.plot(r.history['val_accuracy'], label='val acc')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()","e9ae5271":"# loss\nplt.plot(r.history['loss'], label='train loss')\nplt.plot(r.history['val_loss'], label='val loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","5a369dd0":"import os \nimport pandas as pd\nimport cv2\nfrom PIL import Image\ndata=[]\nlabels=[]\nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/banana\")\nfor filename in filenames:\n    labels.append(0)\n    img=cv2.imread('\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/banana\/'+filename)\n    img = Image.fromarray(img, 'RGB')\n    size_image = img.resize((224, 224))\n    data.append(np.array(size_image))\n    files.append(filename)\n\nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/mixed\")\nfor filename in filenames:\n    labels.append(1)\n    img=cv2.imread('\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/mixed\/'+filename)\n    img = Image.fromarray(img, 'RGB')\n    size_image = img.resize((224, 224))\n    data.append(np.array(size_image))\n    files.append(filename)\n    \nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/apple\")\nfor filename in filenames:\n    labels.append(2)  \n    img=cv2.imread('\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/apple\/'+filename)\n    img = Image.fromarray(img, 'RGB')\n    size_image = img.resize((224, 224))\n    data.append(np.array(size_image))\n    files.append(filename)\n    \nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/orange\")\nfor filename in filenames:\n    labels.append(3)\n    img=cv2.imread('\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/orange\/'+filename)\n    img = Image.fromarray(img, 'RGB')\n    size_image = img.resize((224, 224))\n    data.append(np.array(size_image))\n    files.append(filename)\n","aedc3c0c":"data=np.array(data)\nlabels=np.array(labels)\ndata1=data","67b9f0ec":"s=np.arange(data.shape[0])\nnp.random.shuffle(s)\ndata=data[s]\nlabels=labels[s]","31cfcf91":"num_classes=len(np.unique(labels))\nlen_data=len(data)\n(x_train,x_test)=data[(int)(0.1*len_data):],data[:(int)(0.1*len_data)]\nx_train = x_train.astype('float32')\/255 # As we are working on image data we are normalizing data by divinding 255.\nx_test = x_test.astype('float32')\/255\ntrain_len=len(x_train)\ntest_len=len(x_test)","21974db3":"(y_train,y_test)=labels[(int)(0.1*len_data):],labels[:(int)(0.1*len_data)]","d4efd0b8":"y_train=keras.utils.to_categorical(y_train,num_classes)\ny_test=keras.utils.to_categorical(y_test,num_classes)","b6e65a74":"model1 = Sequential()\nmodel1.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=IMAGE_SIZE + [3]))\nmodel1.add(MaxPooling2D(2,2))\nmodel1.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\nmodel1.add(MaxPooling2D(2,2))\nmodel1.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\nmodel1.add(MaxPooling2D(2,2))\nmodel1.add(Flatten())\nmodel1.add(Dense(100, activation='relu'))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(4, activation='softmax'))\n\nmodel1.summary()","e753100c":"model1.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])","22976b00":"model1.fit(x_train,y_train,batch_size=20,epochs=20,verbose=1)","d7a7a8c0":"# accuracies\nplt.plot(model1.history.history['accuracy'], label='train acc')\nplt.plot(model1.history.history['loss'], label='train loss')\nplt.xlabel('epoch')\nplt.legend()\nplt.show()","052f4b48":"out=model.predict(train_generator)\nout1=model1.predict(train_generator)","29c1f0a2":"out","4dfdd17a":"out1","0fea07f6":"rows=240\ncolumns=4\ny_predicted1=[]\nfor i in range(rows):\n    max=-1\n    for j in range(columns):\n        if(max<out1[i][j]):\n            max=out1[i][j]\n            max1=j\n    y_predicted1.append(max1)","4764f84e":"data1=data1.astype('float32')\/255","7d0a9a92":"y_predicted1=model1.predict_classes(data1)","a9394903":"y_predicted1","0e8c2072":"rows=240\ncolumns=4\ny_predicted=[]\nfor i in range(rows):\n    max=-1\n    for j in range(columns):\n        if(max<out[i][j]):\n            max=out[i][j]\n            max1=j\n    y_predicted.append(max1)","1e388b40":"y_predicted1=np.asarray(y_predicted)\ny_predicted1","d10bf601":"import os\nimport pandas as pd\nfiles=[]\ncategories = []\nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/banana\")\nfor filename in filenames:\n    categories.append(0)\n    files.append(filename)\n\nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/mixed\")\nfor filename in filenames:\n    categories.append(1)\n    files.append(filename)\n    \nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/apple\")\nfor filename in filenames:\n    categories.append(2)    \n    files.append(filename)\n    \nfilenames = os.listdir(\"\/kaggle\/input\/fruit-images-for-object-detection\/data\/train\/orange\")\nfor filename in filenames:\n    categories.append(3)\n    files.append(filename)\n\n\ndf = pd.DataFrame({\n    'filename': files,\n    'category': categories\n})","292b8c90":"# from the VGG16 model\nreport = classification_report(df['category'], y_predicted)\nprint(report)","9ab2a8a0":"# from the simple model.\nreport = classification_report(df['category'], y_predicted1)\nprint(report)","aca4825c":"## CLASSIFICATION REPORT\nThe classification report visualizer displays the precision, recall, F1, and support scores for the model. \nThe classification report shows a representation of the main classification metrics on a per-class basis. This gives a deeper intuition of the classifier behavior over global accuracy which can mask functional weaknesses in one class of a multiclass problem. ","2194dc77":"IMPORTING LIBRARIES","151f4260":"IMAGE DATAGENERATORS\nFor loading large size datasets directly from the directory we use datagenerators which load the data directly into the model.The main benefit of using this class to load the data is that images are loaded for a single dataset in batches, meaning that it can be used for loading both small datasets as well as very large image datasets with thousands or millions of images. The pattern for using the ImageDataGenerator class is used as follows: 1.Construct and configure an instance of the ImageDataGenerator class. 2.Retrieve an iterator by calling the flow_from_directory() function. 3.Use the iterator in the training or evaluation of a model.","2aad2fc8":"### LOADING THE DATA ","17dc20c8":"# CONVOLUTIONAL NETWORKS\n\nThe dataset used is relatively small in size. Each example in the dataset has been resized to a (224,224) RGB image, associated with a label from 4 classes(apple,banana,orange and mixed). The dataset serves as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n\nIn this work, I will use the pre-trained model VGG16, developed by Karen Simonyan and Andrew Zisserman in 2014, a simple and widely used convnet architecture for ImageNet,[here is the link to the paper](https:\/\/arxiv.org\/pdf\/1409.1556.pdf),.The model achieves 92.7% top-5 test accuracy in ImageNet.\nThe dataset used in this work shares the same image size and structure of training and testing splits. The model is trained for 10 epochs with batch size of 32, compiled with categorical_crossentropy loss function and rmsprop optimizer.\nAt the end I have also implemented a small CNN network as well consisting of 3 convolutional layers and 2 dense layers by increasing number of epoch cycles to 30.","32063634":"### VGG16 MODEL ","0d41ad28":"### ANALYSING THE RESULTS FROM THE MODELS ","3a0eca5e":"### SIMPLE 3 LAYERED CNN ","f1e314da":"We can clearly see from the classification report that VGG16 model performed better as compared to a comparatively less dense CNN on the initial epochs but as the number of epochs increased the performance was more or less balanced. So adding layers does help improve the performance but after the saturation point it also deteoriates the model's accuracy very steeply."}}