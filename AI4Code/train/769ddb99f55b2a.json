{"cell_type":{"82438848":"code","d97b99f2":"code","4c2f9caf":"code","5cc1968b":"code","dbb0770f":"code","e21c1f5a":"code","f5d1eefb":"code","7e36969a":"code","b3ea5a05":"code","85d04c25":"code","cfc4c7f9":"code","83a44144":"code","182b82be":"code","85a539ef":"code","8b791c95":"code","1af92784":"code","fd05863b":"code","05444c3b":"code","593baf1e":"code","ce8d4066":"code","0a352303":"code","9cd1e4c7":"code","52087e0f":"code","7979f788":"code","2ab8ed45":"code","0dbf6847":"code","c9ddafa8":"code","2cc8b23b":"code","76db1b2b":"code","851c8adb":"code","00a75ba2":"code","1fcdd44b":"code","c90d0691":"code","89183a61":"code","12e1cb7f":"code","1aec3381":"code","5381a336":"code","9a9ec63b":"code","7a0d3bf7":"code","a4074e0e":"code","284c4aa9":"code","772890a2":"code","c6856ecd":"code","b74af29b":"code","730f35b6":"code","353d5e8b":"code","fd45d77e":"code","e1d53be5":"code","b1712209":"code","34189786":"code","26480ebc":"code","63422a37":"code","7209a7d5":"code","ef86642f":"code","2a0f15e0":"code","4c3c4400":"code","6235cd1a":"code","658655c0":"markdown","c0671a1b":"markdown","79c9a53d":"markdown","5bb808f9":"markdown","82fbd38c":"markdown","73124331":"markdown","1443ebdf":"markdown","c55a51fe":"markdown","9097501d":"markdown","b9602ef8":"markdown","b3aa3a23":"markdown","8d72dd64":"markdown","7dd9bebf":"markdown","a3578e88":"markdown","8d851f50":"markdown","456261b4":"markdown","4925aff3":"markdown","5ff5b1d1":"markdown","ce249254":"markdown","feedaf8e":"markdown","b0080ba1":"markdown","3dbfa9e2":"markdown","63250a67":"markdown","6cf4153e":"markdown","10648eb1":"markdown","3000109a":"markdown","97832717":"markdown","5b3b8d2a":"markdown","5da573c7":"markdown","20dfc322":"markdown"},"source":{"82438848":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d97b99f2":"import pandas as pd\nimport matplotlib.pyplot as plt\ndataset = pd.read_csv('..\/input\/covid19-corona-virus-india-dataset\/complete.csv')\ndataset.head()","4c2f9caf":"dataset.tail()","5cc1968b":"print(dataset.shape)","dbb0770f":"print(dataset.columns)","e21c1f5a":"print(dataset.info())","f5d1eefb":"dataset.describe()","7e36969a":"dataset.describe(include=['object', 'bool'])","b3ea5a05":"dataset.sort_values(by='Total Confirmed cases', ascending=False).head()","85d04c25":"dataset.sort_values(by=['Death', 'Total Confirmed cases'],\n        ascending=[True, False]).head()","cfc4c7f9":"dataset.sort_values(by=['Death', 'Total Confirmed cases'],\n        ascending=[False, False]).head()","83a44144":"dataset['New cases'].mean()","182b82be":"dataset[dataset['New cases'] == 1].mean()","85a539ef":"dataset[dataset['New cases'] == 1]['New deaths'].mean()","8b791c95":"\n\ndataset.loc[0:5, 'Date':'Total Confirmed cases']","1af92784":"dataset.iloc[0:5, 0:6]","fd05863b":"dataset[-1:]","05444c3b":"import numpy as np\npd.set_option(\"display.precision\", 2)\ndataset.apply(np.max)","593baf1e":"a=dataset['Name of State \/ UT'].unique()\nprint(a)","ce8d4066":"b=dataset['Name of State \/ UT'].nunique()\nprint(b)","0a352303":"c=dataset['Name of State \/ UT'].nunique(dropna=False)\nprint(c)","9cd1e4c7":"d=dataset.nunique()\nprint(d)","52087e0f":"dataset[dataset['Name of State \/ UT'].apply(lambda state: state[0] == 'K')].head()","7979f788":"kerala=dataset.loc[dataset['Name of State \/ UT']== \"Kerala\"]\nprint(kerala)\nsumofkerala=kerala.sum()\nprint(sumofkerala)","2ab8ed45":"kerala.hist(column='New deaths')","0dbf6847":"columns_to_show = ['New cases','New deaths','New recovered']\n\ndataset.groupby(['Death'])[columns_to_show].describe(percentiles=[])","c9ddafa8":"columns_to_show = ['New cases','New deaths','New recovered']\n\ndataset.groupby(['Death'])[columns_to_show].agg([np.mean, np.std, np.min, \n                                            np.max])","2cc8b23b":"pd.crosstab(dataset['Death'], dataset['New cases'])","76db1b2b":"pd.crosstab(dataset['Death'], dataset['New deaths'])","851c8adb":"pd.crosstab(dataset['Death'], dataset['New deaths'], normalize=True)","00a75ba2":"dataset.pivot_table(['New cases', 'New deaths', 'New recovered'],\n               ['Date'], aggfunc='mean')","1fcdd44b":"dataset.pivot_table(['New cases', 'New deaths','New recovered'],\n               ['Name of State \/ UT'], aggfunc='mean')","c90d0691":"dataset['Death'].value_counts()\n ","89183a61":"dataset['Death'] = pd.to_numeric(dataset['Death'],errors='coerce')\n","12e1cb7f":"total_death = dataset['Death'] + dataset['New deaths']           \ndataset.insert(loc=len(dataset.columns), column='Total Death', value=total_death) \ndataset.head()","1aec3381":"dataset['Total_Death'] = dataset['Death'] + dataset['New deaths'] \ndataset.head()\ndataset.tail()","5381a336":"dataset.drop(['Total_Death'], axis=1)","9a9ec63b":"# and here\u2019s how you can delete rows\ndataset.drop([1, 2]).head() ","7a0d3bf7":"dataset.hist('Total_Death')","a4074e0e":"Data1 = dataset.pivot_table(['New cases','New deaths'],['Name of State \/ UT'],aggfunc=np.sum)","284c4aa9":"print(Data1)","772890a2":"print(l)","c6856ecd":"data = Data1['New cases']     \ndf = pd.DataFrame(data=data)\nlines = df.plot.line()","b74af29b":"data = Data1['New deaths']   \ndf = pd.DataFrame(data=data)\nlines = df.plot.line()","730f35b6":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\nbar = df.plot.bar()","353d5e8b":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\nbarh = df.plot.barh()","fd45d77e":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\ndensity = df.plot.density()","e1d53be5":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\narea = df.plot.area()","b1712209":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\nhist = df.plot.hist()","34189786":"data = Data1['New cases']   \ndf = pd.DataFrame(data=data)\npie = df.plot.pie(subplots=True, figsize=(11, 6))","26480ebc":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","63422a37":"data=pd.read_csv('..\/input\/titanic-machine-learning-from-disaster\/train.csv')","7209a7d5":"data.head()","ef86642f":"data.isnull().sum()","2a0f15e0":"data.info()","4c3c4400":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","6235cd1a":"data.groupby(['Sex','Survived'])['Survived'].count()","658655c0":"Part1: Exploratory Data Analysis(EDA):\n1)Analysis of the features.\n\n2)Finding any relations or trends considering multiple features.\n\nPart2: Feature Engineering and Data Cleaning:\n1)Adding any few features.\n\n2)Removing redundant features.\n\n3)Converting features into suitable form for modeling.\n\nPart3: Predictive Modeling\n1)Running Basic Algorithms.\n\n2)Cross Validation.\n\n3)Ensembling.\n\n4)Important Features Extraction.","c0671a1b":"int64, float64 and object are the data types of our features. We see that one feature is  3 features are of type object, and 7 features are numeric. With this same method, we can easily see if there are any missing values. Here, there are none because each column contains 4692 observations, the same number of rows we saw before with shape.","79c9a53d":"The Age, Cabin and Embarked have null values. I will try to fix them.","5bb808f9":"It is possible to add a column more easily without creating an intermediate Series instance:","82fbd38c":"The apply method can also be used to apply a function to each row. To do this, specify axis=1. Lambda functions are very convenient in such scenarios. For example, if we need to select all states starting with K, we can do it like this:","73124331":"To delete columns or rows, use the drop method, passing the required indexes and the axis parameter (1 if you delete columns, and nothing or 0 if you delete rows). The inplace argument tells whether to change the original DataFrame. With inplace=False, the drop method doesn't change the existing DataFrame and returns a new one with dropped rows or columns. With inplace=True, it alters the DataFrame.","1443ebdf":"Applying Functions to Cells, Columns and Rows\nTo apply functions to each column, use apply():","c55a51fe":"If we need the first or the last line of the data frame, we can use the df[:1] or df[-1:] construct:","9097501d":"DataFrame transformations\nLike many other things in Pandas, adding columns to a DataFrame is doable in many ways.\n\nFor example, if we want to calculate the total number of deaths , let's create the total_deaths Series and paste it into the DataFrame:","b9602ef8":"Let\u2019s do the same thing, but slightly differently by passing a list of functions to agg():\n","b3aa3a23":"Indexing and retrieving data\nA DataFrame can be indexed in a few different ways.\n\nTo get a single column, you can use a DataFrame['Name'] construction. Let's use this to answer a question about that column alone: what is the proportion of death rate in our dataframe?","8d72dd64":"For categorical (type object) and boolean (type bool) features we can use the value_counts method. Let's have a look at the distribution of Churn:","7dd9bebf":"It is evident that not many passengers survived the accident.\n\nOut of 891 passengers in training set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn't.\n\nWe will try to check the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age,etc.\n\nFirst let us understand the different types of features.","a3578e88":"The describe method shows basic statistical characteristics of each numerical feature (int64 and float64 types): number of non-missing values, mean, standard deviation, range, median, 0.25 and 0.75 quartiles.\n\n\n\nIn order to see statistics on non-numerical features, one has to explicitly indicate data types of interest in the include parameter.","8d851f50":"1. How many new cases and new death in india?","456261b4":"df.plot.area     df.plot.barh     df.plot.density  df.plot.hist     df.plot.line     df.plot.scatter\ndf.plot.bar      df.plot.box      df.plot.hexbin   df.plot.kde      df.plot.pie","4925aff3":"columns_to_show = ['Cured','Confirmed']\n\ndataset.groupby(['Deaths'])[columns_to_show].describe(percentiles=[])\n\n\n\n\n\ncolumns_to_show = ['Cured','Confirmed']\n\ndataset.groupby(['Deaths'])[columns_to_show].agg([np.mean, np.std, np.min, \n                                            np.max])\n                                            \n                                            \ncolumns_to_show = ['Cured','Confirmed']\n\ndataset.groupby(['Deaths'])[columns_to_show].nunique()\n\n\n\npd.crosstab(dataset['Deaths'], dataset['Cured'])\n\n\npd.crosstab(dataset['Deaths'], dataset['Cured'], normalize='index')\n\npd.crosstab(dataset['Deaths'], dataset['Cured'], normalize='columns')\n\npd.crosstab(dataset['Deaths'], dataset['Cured'], normalize=True)\n\n\ndataset.pivot_table(['Cured', 'Deaths'],\n               ['Date'], aggfunc='mean')\n\n\ndataset.pivot_table(['Cured', 'Deaths'],\n               ['State\/UnionTerritory'], aggfunc=np.sum)","5ff5b1d1":"We can also sort by multiple columns:","ce249254":"DataFrames can be indexed by column name (label) or row name (index) or by the serial number of a row. The loc method is used for indexing by name, while iloc() is used for indexing by number.\n\nIn the first case below, we say \"give us the values of the rows with index from 0 to 5 (inclusive) and columns labeled from Date to Total Confirmed cases (inclusive)\". In the second case, we say \"give us the values of the first five rows in the first three columns\" (as in a typical Python slice: the maximal value is not included).","feedaf8e":"Sorting\nA DataFrame can be sorted by the value of one of the variables (i.e columns). For example, we can sort by Total Confirmed cases (use ascending=False to sort in descending order):","b0080ba1":"419 is actually quite bad for our country, That much of person have covid virus","3dbfa9e2":"From the output, we can see that the table contains 4692 rows and 10 columns.\n\nNow let's try printing out column names using columns:","63250a67":"We can use the info() method to output some general information about the dataframe:","6cf4153e":"Boolean indexing with one column is also very convenient. The syntax is df[P(df['Name'])], where P is some logical condition that is checked for each element of the Name column. The result of such indexing is the DataFrame consisting only of rows that satisfy the P condition on the Name column.\n\nLet's use it to answer the question:\n\nWhat are average values of numerical features for New cases?","10648eb1":"Summary tables\nSuppose we want to see how the observations in our sample are distributed in the context of two variables - Death and New Cases. To do so, we can build a contingency table using the crosstab method:","3000109a":"pivot tables are implemented in Pandas: the pivot_table method takes the following parameters:\n\nvalues \u2013 a list of variables to calculate statistics for,\nindex \u2013 a list of variables to group data by,\naggfunc \u2013 what statistics we need to calculate for groups, ex. sum, mean, maximum, minimum or something else.\n\nLet's take a look at the average New cases, New deaths, and New recovered by Date and State \/UT:\n\n","97832717":"Grouping\nIn general, grouping data in Pandas works as follows:\n\ndf.groupby(by=grouping_columns)[columns_to_show].function()\n\nFirst, the groupby method divides the grouping_columns by their values. They become a new index in the resulting dataframe.\nThen, columns of interest are selected (columns_to_show). If columns_to_show is not included, all non groupby clauses will be included.\nFinally, one or several functions are applied to the obtained groups per selected columns.\n\n\n\nHere is an example where we group the data according to the values of the death and display statistics of three columns in each group","5b3b8d2a":"DataFrame transformations\nLike many other things in Pandas, adding columns to a DataFrame is doable in many ways.\n\nFor example, if we want to calculate the total number of deaths, let's create the total_deaths Series and paste it into the DataFrame:","5da573c7":"For categorical (type object) and boolean (type bool) features we can use the value_counts method.","20dfc322":"Types Of Features\nCategorical Features:\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.\n\nCategorical Features in the dataset: Sex,Embarked.\n\nOrdinal Features:\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable.\n\nOrdinal Features in the dataset: PClass\n\nContinous Feature:\nA feature is said to be continous if it can take values between any two points or between the minimum or maximum values in the features column.\n\nContinous Features in the dataset: Age\n\nAnalysing The Features\nSex--> Categorical Feature"}}