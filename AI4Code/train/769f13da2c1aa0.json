{"cell_type":{"df119f19":"code","a264017e":"code","70857716":"code","6af209e7":"code","d581fe63":"code","420a5bfb":"code","08ec3d81":"code","d7b1f8da":"code","430c8fcd":"code","8df28b47":"code","4d99f838":"code","84437176":"code","d8a52dd0":"code","8655f7b0":"code","b297e3c5":"code","0ae7fb6e":"code","6d69760a":"code","442bd354":"code","da292a90":"code","5ffa5b46":"code","5676d5e7":"code","a925d66c":"code","ab2be2c5":"code","5806fd6e":"code","4771e6d6":"code","48435e5d":"code","6d456f9a":"code","76258b57":"code","0cdb962c":"code","1cc88fef":"code","87e8f2c2":"code","cde06d1b":"code","eeebe06d":"code","be4db784":"markdown","71c20bf4":"markdown","fb345186":"markdown","130e28e3":"markdown","57ef9507":"markdown","0e79e056":"markdown","34964b06":"markdown","fa51071c":"markdown","dee4776d":"markdown","c325f33e":"markdown","ffa33fb4":"markdown","092cd1ab":"markdown","2502ab3e":"markdown","fd1aee72":"markdown","d02876b2":"markdown","c6f9e8bf":"markdown","e937f5e0":"markdown","101913c3":"markdown","bd4d941c":"markdown","baab805a":"markdown","305954bc":"markdown","cfe26d7c":"markdown","eea78480":"markdown","c25f7385":"markdown","f499c0ab":"markdown","d58945a3":"markdown","2dfe22ff":"markdown","edc73be4":"markdown","c6056443":"markdown","cf68fb9a":"markdown"},"source":{"df119f19":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, Input, Activation, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.metrics import accuracy_score, classification_report","a264017e":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","70857716":"training_data = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')\ntesting_data = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')","6af209e7":"training_data.head()","d581fe63":"targets = { 0: 'T-shirt\/top',\n            1: 'Trouser',\n            2: 'Pullover',\n            3: 'Dress',\n            4: 'Coat',\n            5: 'Sandal',\n            6: 'Shirt',\n            7: 'Sneaker',\n            8: 'Bag',\n            9: 'Ankle boot'}","420a5bfb":"print(f\"No.of null values in testing_data = {training_data.isnull().values.sum()}\")\nprint(f\"No.of null values in testing_data = {testing_data.isnull().values.sum()}\")","08ec3d81":"fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = True, figsize = (16,6))\n\nsns.countplot(x = 'label', data = training_data, ax = ax[0])\nsns.countplot(x = 'label', data = testing_data, ax = ax[1])\n","d7b1f8da":"X = training_data.drop('label', axis = 1)\ny = training_data.loc[:, 'label']\n\nX_test = testing_data.drop('label', axis = 1)\ny_test = testing_data.loc[:, 'label']","430c8fcd":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8, random_state = 50)","8df28b47":"fig, ax = plt.subplots(nrows = 1, ncols = 2, sharey = True, figsize = (16,6))\n\nsns.countplot(y_train, ax = ax[0])\nsns.countplot(y_val, ax = ax[1])\n","4d99f838":"def standardize(df):\n    df = df \/ 255.0\n    return np.array(df)\n\n\nX_train = standardize(X_train)\nX_test = standardize(X_test)\nX_val = standardize(X_val)","84437176":"X_train = X_train.reshape(-1,28,28,1)\nX_test = X_test.reshape(-1,28,28,1)\nX_val = X_val.reshape(-1,28,28,1)","d8a52dd0":"print(f\"Train_set - {X_train.shape}\\nTest_set - {X_test.shape}\\nVal_set - {X_val.shape}\")","8655f7b0":"i = 0\n\nplt.imshow(X_train[i].reshape(28,28))\nplt.title(targets[y_train[i]])","b297e3c5":"datagen = ImageDataGenerator(horizontal_flip = True)\ndatagen.fit(X_train)","0ae7fb6e":"def functional_api_model_architecture(inputs):\n    \n    x = inputs\n    x = Conv2D(filters = 256, kernel_size = (3,3),input_shape = (28,28,1))(x)\n    x = LeakyReLU()(x)\n    x = Conv2D(filters = 256, kernel_size = (3,3))(x)\n    x = LeakyReLU()(x)\n    x = BatchNormalization(axis = -1)(x)\n    x = MaxPooling2D(pool_size = (2,2))(x)\n    x = Dropout(rate = 0.3)(x)\n\n    x = Conv2D(filters = 256,kernel_size = (3,3))(x)\n    x = LeakyReLU()(x)\n    x = Conv2D(filters = 256,kernel_size = (3,3))(x)\n    x = LeakyReLU()(x)\n    x = BatchNormalization(axis = -1)(x)\n    x = MaxPooling2D(pool_size = (2,2))(x)\n    x = Dropout(rate = 0.3)(x)\n    \n    x = Conv2D(filters = 256,kernel_size = (3,3))(x)\n    x = LeakyReLU()(x)\n    x = BatchNormalization(axis = -1)(x)\n    x = MaxPooling2D(pool_size = (2,2))(x)\n    x = Dropout(rate = 0.3)(x)\n\n    x = Flatten()(x)\n    x = Dense(units = 64)(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate = 0.5)(x)\n\n    x = Dense(units = 128)(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate = 0.5)(x)\n\n    x = Dense(units = 10)(x)\n    x = Activation('softmax', name = 'output_cat')(x)\n\n    return x","6d69760a":"def functional_api_model_init():\n    \n    inputs = Input(shape = (28,28,1))\n    model = Model(inputs = inputs, outputs = functional_api_model_architecture(inputs))\n    model.summary()\n    plot_model(model, \"my_model.png\", show_shapes=True)\n    print(\"Your Model was successfully created\")\n    \n    return model","442bd354":"model = functional_api_model_init()","da292a90":"from IPython.display import Image\nImage(\"\/kaggle\/working\/my_model.png\")","5ffa5b46":"adam = Adam(learning_rate = 0.001)\nloss = { 'output_cat' : 'sparse_categorical_crossentropy'}","5676d5e7":"model.compile(optimizer = adam, loss = loss, metrics = ['accuracy'])","a925d66c":"earlystopping = EarlyStopping(patience = 7, monitor='val_loss')\nreduce_LR_plateau = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 4)","ab2be2c5":"epochs = 50\nbatch_size = 256","5806fd6e":"hist = model.fit(datagen.flow(X_train, y_train), epochs = epochs, validation_data = (X_val, y_val),\n          batch_size = batch_size, callbacks = [earlystopping, reduce_LR_plateau], verbose = 1)","4771e6d6":"fig, ax = plt.subplots(nrows = 2, ncols = 1, sharex = True, figsize = (10,18))\n\nax[0].plot(hist.history['val_accuracy'], c = 'b', label = 'Validation_accuracy')\nax[0].plot(hist.history['accuracy'], c = 'r', label = 'Training accuracy')\nax[0].set_title('Validation Accuracy vs Training Accuracy')\n\nax[1].plot(hist.history['val_loss'], c = 'b', label = 'Validation Loss')\nax[1].plot(hist.history['loss'], c = 'r', label = 'Training Loss')\nax[1].set_title('Validation Loss vs Training Loss')\n\nplt.legend()","48435e5d":"predictions = model.predict(X_test)\npredictions = np.argmax(predictions, axis = 1)","6d456f9a":"print(f\"Accuracy Score : {accuracy_score(y_test, predictions)}\")\nprint(f\"\\n\\n\\n{classification_report(y_test, predictions)}\")","76258b57":"false_predictions = [i for i in range(y_test.shape[0]) if predictions[i] != y_test[i]]","0cdb962c":"rows = 5 \ncols = 5\nindex = 0\n\nfig, ax = plt.subplots(nrows = rows, ncols = cols, figsize = (15,20))\n\nfor i in range(5):\n    for j in range(5):\n        false_index = false_predictions[index]\n        ax[i][j].imshow(X_test[false_index, :,:,-1])\n        ax[i][j].set_title(f\"P : {targets[predictions[false_index]]}\\nA : {targets[y_test[false_index]]}\")\n        index += 1\n        ax[i][j].axis(\"off\")\n        \n\nplt.show()\nplt.tight_layout()","1cc88fef":"import cv2 \nimport requests\nfrom PIL import Image\nfrom io import BytesIO","87e8f2c2":"def preprocess_image(url):\n    \n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    \n    fig, ax = plt.subplots(nrows = 1, ncols = 3,figsize = (15,20))\n    ax[0].imshow(img)\n    ax[0].set_title(\"Image\")\n    \n    \n    ## Grayscale and Normalization\n    img = np.array(img)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)   ## From 3 channel RGB to 1 channel Grayscale\n    print(f\"Shape of image array after converting it to Grayscale : {img.shape}\")\n    img = img \/ 255.0\n    ax[1].imshow(img)\n    ax[1].set_title(\"Grayscale Image\")\n    \n    ## Resizing\n    img = cv2.resize(img, (28,28))\n    print(f\"Shape of image after Grayscale and Resizing : {img.shape}\")\n    ax[2].imshow(img)\n    ax[2].set_title(\"Grayscale & Resized\")   \n    \n    plt.tight_layout()\n    \n    ## Making it model ready\n    img = np.expand_dims(img, axis = [0,3])\n    print(f\"At last ready to be predicted by model, shape - {img.shape}\")\n    \n    return img\n\n\ndef predict_image(url):\n    \n    img = preprocess_image(url)\n    predicted_label = model.predict(img)\n    predicted_label = np.argmax(predicted_label, axis = 1)[0]\n    return targets[predicted_label]\n","cde06d1b":"predict_image(\"https:\/\/images.all-free-download.com\/images\/graphicthumb\/blank_black_tshirt_stock_photo_168263.jpg\")","eeebe06d":"predict_image(\"https:\/\/dievca.files.wordpress.com\/2014\/06\/victoria-secret-goddess-maxi-dress-white-black-background.jpg\")","be4db784":"Lets map the labels, eventually this will make few things easier to understand for us","71c20bf4":"<a id = \"Viz\"><\/a><h2> Vizualization<\/h2>","fb345186":"* We'll use Adam optimizer, you can try others as well like RMSProp and more\n* Kept loss as sparse_categorical_crossentropy because our targets has values one vector corresponding to the different categories ( 0,5,1,2, ....). Had it been hot encoded , like - each row having 10 cols corresponding to each category - eg : 1st row - (1,0,0,0,0,0,0,0,0) - Tshirt","130e28e3":"We'll take only first 25 images into consideration","57ef9507":"Let's take a look where our model actually failed. ","0e79e056":"In our dataset, we've images of 28 * 28 dimensions which are flattened into 784 pixels. We will resize into 28 x 28.","34964b06":"Neural networks process inputs using small weight values, and inputs with large integer values can disrupt or slow down the learning process. As such it is good practice to normalize the pixel values so that each pixel value has a value between 0 and 1.","fa51071c":"<a id = \"Data-Aug\"><\/a><h2>Data Augmentation<\/h2>","dee4776d":"Thanks for going through patiently through my kernel. Do hit upvote if you think it was simple and easy to learn. I'm Living my life on feedbacks and areas of improvement. Please give feedbacks. Thanks already. \n","c325f33e":"<a id = \"Split\"><\/a><h2>Splitting<\/h2>","ffa33fb4":"On Visualization of the Training,it looks something like this","092cd1ab":"Recent advances in deep learning models have been largely attributed to the quantity and diversity of data gathered in recent years. Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks.\n\nThere are many augmentation techniques we can use given by Keras, check out -\n\nhttps:\/\/keras.io\/api\/preprocessing\/image\/\n\nhere we only use Horizontal Flips because the model need to understand a shoe from left to right and also from right to left. Not only left to right.\nAnd so onnn....","2502ab3e":"Looks pretty good. No imbalances","fd1aee72":"<a id = \"Train\"><\/a><h2> Training the Model <h2>","d02876b2":"<a id = \"Import\"><\/a><h2> Import the required libraries and Dataset <\/h2>","c6f9e8bf":"Change the value of i and browse through other images","e937f5e0":"<a id = \"Evaluate\"><\/a><h2> Testing<\/h2>","101913c3":"1 column for labels and 784 pixels for the images of dimension (28 x 28)","bd4d941c":"While testing on real images from internet, do keep the training data in mind. \nHave images with properties same as the Zalando images. Like , most of the Zalando images had black background and in portait mode. \nSo images not having such properties won't have correct labels. So keep in mind the properties of the data on which our model is trained.","baab805a":"Let's make this clear that we'll use separate validation data for validation. I'm stressing on this point because you might have seen many people validating on the Test data and at last evaluating on the same test data. Eventually that gives high accuracy scores which is actually wrong. Keep in mind that the test data is completely a new data seen by the model.","305954bc":"Resizing -  .reshape(no.of images, height, width, no.channel)\nThe images are in Grayscale and therefore has 1 channel. Thats why we've mentioned 1.\nHad it been a RGB, we should've mentioned 3.\nA CMYK image has 4 channels: cyan, magenta, yellow, and key (black). And so on....","cfe26d7c":"Have a good day!","eea78480":"No Null values, No imputations!","c25f7385":"* We'll have EarlyStopping with patience 7 and monitor validation Loss.\n* ReduceLRonPlateau for reducing the Learning Rate while there's plateauing on val_loss, with factor of 0.1 and patience of 4","f499c0ab":"<a id = \"Tryout\"><\/a><h2>Playground<\/h2>","d58945a3":"<a id = \"Modelling\"><\/a><h2>Modelling<\/h2>","2dfe22ff":"That was all about the architecture of simple CNN Model and lets start its Training","edc73be4":"<a id = \"Prepro1\"><\/a><h2> Standardization<\/h2>","c6056443":"This kernel is to showcase a functional api approach of creating a Image Classification Model.\nBy knowing this approach, we'll be ready to make advanced Vision problems in a better way. This will help in using the concept of Transfer Learning and applying it. \n\nThe Dataset used is Fashion-Mnist by Zalando with 60,000 training data and 10,000 test data. It has grayscale images of 28 x 28. It has 10 clothes categories labelled like -\n\n0 T-shirt\/top\n1 Trouser\n2 Pullover\n3 Dress\n4 Coat\n5 Sandal\n6 Shirt\n7 Sneaker\n8 Bag\n9 Ankle boot\n\nWe'll follow the basic ML project steps - \n\n\n1) [Importing the Libraries and dataset](#Import)<br>\n2) [Splitting the Dataset into Validation, Train and Test data](#Split)<br>\n3) [Preprocessing](#Prepro1)<br>\n     i)  [Standardization](#Prepro1)<br>\n     ii) [Converting the images into matrix or tensors of 28 x 28 with single channel(Grayscale)](#Prepro2)<br>\n4) [Vizualization](#Viz)<br>\n5) [Data Augmentation](#Data-Aug)<br>\n6) [Modeling](#Modelling)<br>\n7) [Training of model using train and validation data](#Train)<br>\n8) [Metric Evaluation on test data](#Evaluate)<br>\n9) [Maybe playaround with the model](#Tryout)<br>\n\nLet's go through these steps -","cf68fb9a":"<a id = \"Prepro2\"><\/a><h2>Resizing<\/h2>"}}