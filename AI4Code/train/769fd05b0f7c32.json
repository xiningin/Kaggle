{"cell_type":{"6ecbfa8a":"code","19def918":"code","50065962":"code","2495cdd4":"code","572eea48":"code","fd187d3b":"code","31c83899":"code","9d71f5e4":"code","6b60fac4":"code","bff2cc0d":"code","bdea7539":"code","f034cb22":"code","c8147891":"code","9a467271":"code","85e7fc69":"code","541cbd02":"code","8520207d":"code","b4f089e8":"code","f74ee308":"code","85c6882e":"code","e4b3f75b":"code","ba4a3108":"code","a26bcf80":"markdown","f7bbf7ea":"markdown","746a58ae":"markdown","84d7056d":"markdown","36a5334f":"markdown"},"source":{"6ecbfa8a":"import numpy as np \nimport pandas as pd\npd.set_option('display.width', 100000) # Extend the display width to prevent split functions to not cover full text\nimport matplotlib.pyplot as plt\nimport time\nimport warnings \nfrom sklearn.manifold import TSNE\nwarnings.filterwarnings('ignore')\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \n%matplotlib inline\n\n# NLP libraries\nimport spacy\nfrom spacy.lang.en import English\nfrom nltk.tokenize import word_tokenize \nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\nfrom multiprocessing import cpu_count\nimport gensim.downloader as api\nimport re, string, unicodedata\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nfrom tqdm.notebook import tqdm\n\n\n#I will load output files from biorxiv clean outout kernel, which contains a useful transformation of the json files in dictionaries to csv readable format. \n\nbiorxiv = pd.read_csv(\"..\/input\/biorxiv_clean.csv\")\n\nbiorxiv['paper_text'] = biorxiv['text'].map(lambda x: re.sub('\\[[^]],?#=*\\]', '', x))# Convert the titles to lowercase\nbiorxiv['paper_text'] = biorxiv['paper_text'].map(lambda x: x.lower())\n\nbiorxiv.head(5)\n","19def918":"biorxiv.shape","50065962":"biorxiv.isnull().sum()","2495cdd4":"# Filter papers containing all words in list\ndef filter_papers_word_list(word_list):\n    papers_id_list = []\n    text = \" \"\n    for idx, paper in biorxiv.iterrows():\n        text += paper.paper_text\n        if all(x in paper.paper_text for x in word_list):\n            papers_id_list.append(paper.paper_id)\n            #text += paper.paper_text\n            \n\n    return paperTextProcessing(text, word_list)\n","572eea48":"def remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = remove_punctuation(words)\n    words = lemmatize_verbs(words)\n    return words\n\n\ndef paperTextProcessing(paper_text, word_list):\n    \n    relevant_sentences = []\n    paper_text = paper_text.split(\".\")\n    for sentence in paper_text:\n        for word in word_list:\n            if word in sentence:\n                sentence1 = sentence.split()\n                relevant_sentences.append(sentence1)\n\n    stop_words = set(stopwords.words('english')) \n    \n    for sentence in relevant_sentences:\n        for word in stop_words:\n            while(word in sentence):\n                sentence.remove(word)\n    \n    final_corpus = []\n    \n    for sentence in relevant_sentences:\n        final_corpus.append(normalize(sentence))\n        \n        \n    return final_corpus","fd187d3b":"def dataSegmentation(corpus, vocabulary):\n    sentences = []\n    for sentence in corpus:\n        num_sentence = []\n        for word in sentence:\n            if word in model:\n                num_sentence.append(vocabulary[word])\n            else:\n                print(word)\n\n        sentences.append(num_sentence)\n        \n    return sentences\n\n\n\ndef tsnePlotSimilarWords(title, labels, embedding_clusters, word_clusters, a, filename=None):\n    plt.figure(figsize=(16, 9))\n    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n        x = embeddings[:, 0]\n        y = embeddings[:, 1]\n        plt.scatter(x, y, c=color, alpha=a, label=label)\n        for i, word in enumerate(words):\n            plt.annotate(word, alpha=0.5, xy=(x[i], \n                        y[i]), xytext=(5, 2),\n                         textcoords='offset points', \n                         ha='right', \n                         va='bottom', size=12)\n    plt.legend(loc=1)\n    plt.title(title)\n    plt.grid(True)\n    if filename:\n        plt.savefig(filename, format='png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n\ndef wordClustering(model, keys):\n    \n\n    embedding_clusters = []\n    word_clusters = []\n    for word in keys:\n        embeddings = []\n        words = []\n        for similar_word, _ in model.most_similar(word, topn=30):\n            words.append(similar_word)\n            embeddings.append(model[similar_word])\n        embedding_clusters.append(embeddings)\n        word_clusters.append(words)\n        \n    \n    embedding_clusters = np.array(embedding_clusters)\n    n, m, k = embedding_clusters.shape\n    tsne_model_en_2d = TSNE(perplexity=25, n_components=2, init='pca', n_iter=3500, random_state=32)\n    embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n    \n    tsnePlotSimilarWords('COVID-19 Word Clustering', keys, embeddings_en_2d, word_clusters, 0.7,\n                        'similar_words.png')\n    ","31c83899":"from gensim.test.utils import common_texts, get_tmpfile\n\ncorpus = filter_papers_word_list([\"transmission\", \"incubation\", \"propagation\", \"infection\", \"environment\"])\n\nfrom gensim.models import Word2Vec\nmodel = Word2Vec(corpus, min_count=2, workers=20, window=2, alpha=0.02, hs=1)\nnewCorpus = filter_papers_word_list([\"coronavirus\", \"virus\", \"risk\", \"disease\", \"infection\", \"transmission\", \"prevention\", \"SARS\", \"outbreak\", \"covid-19\", \"ethical\", \"social\", \"government\", \"symptoms\", \"vaccines\"])\nnewCorpus.reverse()\nmodel.train(newCorpus, epochs=20, total_examples=model.corpus_count, compute_loss=True)\nmodel.save(\"word2vec.model\")","9d71f5e4":"style.use('seaborn-poster')\nstyle.use('ggplot')\n\nkeys = ['transmissions', 'infection', 'vaccine']\n\nwordClustering(model, keys)","6b60fac4":"from gensim.models import FastText\n\ncorpus = filter_papers_word_list([\"transmission\", \"incubation\", \"propagation\", \"infection\", \"environment\"])\n\n\nmodel2 = FastText(corpus, min_count=2, workers=20, window=2, word_ngrams=1, alpha=0.02, hs=1)\nnewCorpus = filter_papers_word_list([\"coronavirus\", \"virus\", \"risk\", \"disease\", \"infection\", \"transmission\", \"prevention\", \"SARS\", \"outbreak\", \"covid-19\", \"ethical\", \"social\", \"government\", \"symptoms\", \"vaccines\"])\nnewCorpus.reverse()\nmodel2.train(newCorpus, epochs=20, total_examples=model.corpus_count, compute_loss=True)\n","bff2cc0d":"style.use('seaborn-poster')\nstyle.use('ggplot')\n\nkeys = ['transmissions', 'infection', 'vaccine']\n\nwordClustering(model2, keys)","bdea7539":"model.most_similar('vaccine', topn=30)","f034cb22":"model2.most_similar('vaccine', topn=30)","c8147891":"model.most_similar('origin', topn=20)","9a467271":"model2.most_similar('origin', topn=20)","85e7fc69":"import plotly.io as pio\n#import plotly.plotly as py \nimport plotly.graph_objects as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\ninit_notebook_mode(connected=True)\n\ndef tsne_plot(model):\n    \n    labels = []\n    tokens = []\n\n    for word in model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=30, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n       \n\n    data = [\n        go.Scatter(\n            x=[i[0] for i in new_values],\n            y=[i[1] for i in new_values],\n            mode='markers',\n            text=[i for i in model.wv.vocab],\n            marker=dict(\n            size=4,\n            color = [len(i) for i in model.wv.vocab], #set color equal to a variable\n            opacity= 0.8,\n            colorscale='Viridis',\n            showscale=False\n        )\n        )\n    ]\n    layout = go.Layout()\n    layout = dict(\n              yaxis = dict(zeroline = False),\n              xaxis = dict(zeroline = False)\n             )\n    \n    \n    \n    fig = go.Figure(data=data, layout=layout)\n    \n    fig.update_layout(title=\n    {\n        'text': \"Semantic word representations by word2vec model\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n    \n    \n    file = plot(fig)\n    pio.show(fig)\n\ntsne_plot(model)","541cbd02":"from gensim.models import Word2Vec\n\n# read the evaluation file, get it at:\n# https:\/\/word2vec.googlecode.com\/svn\/trunk\/questions-words.txt\nquestions = \"..\/input\/biorxiv_clean.csv\"\nevals = open(questions, 'r').readlines()\nnum_sections = len([l for l in evals if l.startswith(':')])\nprint('total evaluation sentences: {} '.format(len(evals) - num_sections))\n#total evaluation sentences: 19544\n\n# load the pre-trained model of GoogleNews dataset (100 billion words), get it at:\n# https:\/\/code.google.com\/p\/word2vec\/#Pre-trained_word_and_phrase_vectors \ngoogle = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n# test the model accuracy*\nw2v_model_accuracy(google)\n#Total sentences: 7614, Correct: 74.26%, Incorrect: 25.74%\n\n\ndef w2v_model_accuracy(model):\n\n    accuracy = model.accuracy(questions)\n    \n    sum_corr = len(accuracy[-1]['correct'])\n    sum_incorr = len(accuracy[-1]['incorrect'])\n    total = sum_corr + sum_incorr\n    percent = lambda a: a \/ total * 100\n    \n    print('Total sentences: {}, Correct: {:.2f}%, Incorrect: {:.2f}%'.format(total, percent(sum_corr), percent(sum_incorr)))","8520207d":"book_layer = model.get_layer('book_embedding')\nbook_weights = book_layer.get_weights()[0]\nbook_weights.shape","b4f089e8":"class SimilarPaper:\n    \n    def __init__(self, biorxiv, comparison_col):\n        self.biorxiv = biorxiv.dropna(subset=[comparison_col])\n        self.comparison_col = comparison_col\n\n    def calculate_similarity(self, anchor_index):\n        \"\"\"\n        Calculate the cosine distance of every embedding\n        to a given 'anchor' embedding.\n        \"\"\"\n        anchor_emb = self.biorxiv.loc[anchor_index, self.comparison_col]\n        self.biorxiv[\"cos_sim\"] = self.biorxiv[self.comparison_col].apply(\n            lambda x: metrics.pairwise.cosine_similarity(\n                [x],\n                [anchor_emb]\n            )[0][0]\n        )\n\n    def get_top_similar(self, n_top=10):\n        \"\"\"\n        Return the n_top papers that are most similar\n        to the anchor paper.\n        \"\"\"\n        top_list = (\n            self.biorxiv\n            .sort_values(\"cos_sim\", ascending=False)\n            [[\"title\", \"cos_sim\"]]\n            .drop_duplicates()\n            [:n_top]\n        )\n        return top_list","f74ee308":"class DocModel(object):\n\n\tdef __init__(self, docs, **kwargs):\n\t\t\"\"\"\n\t\t:param docs: list of TaggedDocument\n\t\t:param kwargs: dictionary of (key,value) for Doc2Vec arguments\n\t\t\"\"\"\n\t\tself.model = Doc2Vec(**kwargs)\n\t\tself.docs = docs\n\t\tself.model.build_vocab([x for x in self.docs])\n\n\tdef custom_train(self, fixed_lr=False, fixed_lr_epochs=None):\n\t\t\"\"\"\n\t\tTrain Doc2Vec with two options, without fixed learning rate(recommended) or with fixed learning rate.\n\t\tFixed learning rate also includes implementation of shuffling training dataset.\n\t\t:param fixed_lr: boolean\n\t\t:param fixed_lr_epochs: num of epochs for fixed lr training\n\t\t\"\"\"\n\t\tif not fixed_lr:\n\t\t\tself.model.train([x for x in self.docs],\n\t\t\t\t\t total_examples=len(self.docs),\n\t\t\t\t\t epochs=self.model.epochs)\n\t\telse:\n\t\t\tfor _ in range(fixed_lr_epochs):\n\t\t\t\tself.model.train(utils.shuffle([x for x in self.docs]),\n\t\t\t\t\t\t total_examples=len(self.docs),\n\t\t\t\t\t\t epochs=1)\n\t\t\t\tself.model.alpha -= 0.002\n\t\t\t\tself.model.min_alpha = self.model.alpha  # fixed learning rate\n\n\n\tdef test_orig_doc_infer(self):\n\t\t\"\"\"\n\t\tUse the original doc as input for model's vector inference,\n\t\tand then compare using most_similar()\n\t\tto see if model finds the original doc id be the most similar doc to the input.\n\t\t\"\"\"\n\t\tidx = np.random.randint(len(self.docs))\n\t\tprint('idx: ' + str(idx))\n\t\tdoc = [doc for doc in self.docs if doc.tags[0] == idx]\n\t\tinferred_vec = self.model.infer_vector(doc[0].words)\n\t\tprint(self.model.docvecs.most_similar([inferred_vec]))  # wrap vec in a list\n\n","85c6882e":"# Failed Attempt (Not achieving better result.)\nfor _ in range(fixed_lr_epochs):\n   self.model.train(utils.shuffle([x for x in self.docs]),\n                total_examples=len(self.docs),\n                epochs=1)\n   self.model.alpha -= 0.002\n   self.model.min_alpha = self.model.alpha  # fixed learning rate","e4b3f75b":"!pip UtilWordEmbedding\nfrom UtilWordEmbedding import DocModel\n# Configure keyed arguments for Doc2Vec model.\ndm_args = {\n    'dm': 1,\n    'dm_mean': 1,\n    'vector_size': 100,\n    'window': 5,\n    'negative': 5,\n    'hs': 0,\n    'min_count': 2,\n    'sample': 0,\n    'workers': workers,\n    'alpha': 0.025,\n    'min_alpha': 0.025,\n    'epochs': 100,\n    'comment': 'alpha=0.025'\n}\n# Instantiate a pv-dm model.\ndm = DocModel(docs=all_docs.tagdocs, **dm_args)\ndm.custom_train()","ba4a3108":"paper_recom = SimilarPaper(biorxiv=biorxiv, comparison_col=\"paper_text\")\npaper_recom.calculate_similarity(anchor_index=ANCHOR_PAPER)\npaper_recom.get_top_similar()","a26bcf80":"#  1. Filtering papers","f7bbf7ea":"# **COvid-19: Context words embading**\nThe purpose of this kernel is to pre-process the papers of interest to build insightful models. I am far from done, but the first version includes a little Word2vec to vectorize the words and the proper visualization with t-SNE and matplotlib. I might change the models in the future implementations as my main intention is to display meaningful clues along the key words. Therefore, my plan is to implement a Word Sense Disambiguation Algorithm and some wrapper methods.\n\nDisclaimer: This kernel is still under construction.","746a58ae":"# 2.  Word Clustering","84d7056d":"# 4. Word2vec Embedding Visualization","36a5334f":"# 3. Modeling with Word2vec"}}