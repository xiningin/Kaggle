{"cell_type":{"1ca6d100":"code","827beaf1":"code","3bb5a6b5":"code","c46c1382":"code","07518cf1":"code","e17f7282":"code","9ca07c5d":"code","9a737b83":"code","43f8f545":"code","aff7a01a":"code","cbc32f7c":"code","7f848a60":"code","c2ebbbf6":"code","a810b908":"code","df7b265f":"code","92329a04":"code","ec91df8b":"code","80ffa3c9":"code","604bac09":"code","88ebdfe2":"code","d7d83194":"code","1d054308":"code","2ecab6d8":"code","bc6078e2":"code","36f8af73":"code","e7c1d12f":"code","4f0c58ae":"code","2b1e7858":"code","301ec98b":"code","d408574b":"code","a5683cdf":"code","0e35be79":"code","c1eb5bbf":"code","3e167a71":"code","82d479fb":"markdown","df4923b1":"markdown","4d3a71c4":"markdown","505c7e14":"markdown","e2001a97":"markdown","e8b54aad":"markdown","46723210":"markdown","9ee97a9b":"markdown","a26b07cc":"markdown","f162df45":"markdown","fd2f296b":"markdown","265d6047":"markdown","897638dc":"markdown","a8f128d3":"markdown","6aea0d76":"markdown"},"source":{"1ca6d100":"import warnings\nwarnings.filterwarnings(\"ignore\")","827beaf1":"import numpy as np\nimport pandas as pd \nimport os\nprint(os.listdir(\"..\/input\"))\n['train.csv', 'gender_submission.csv', 'test.csv']\n# Step 1 is to import both data sets\ntraining_data = pd.read_csv(\"..\/input\/train.csv\")\ntesting_data = pd.read_csv(\"..\/input\/test.csv\")\n\n# Step two is to create columns which I will add to the respective datasets, in order to know which row came from which dataset when I combine the datasets\ntraining_column = pd.Series([1] * len(training_data))\ntesting_column = pd.Series([0] * len(testing_data))\n\n# Now we append them by creating new columns in the original data. We use the same column name\ntraining_data['is_training_data'] = training_column\ntesting_data['is_training_data'] = testing_column","3bb5a6b5":"# Now we can merge the datasets while retaining the key to split them later\ncombined_data = training_data.append(testing_data, ignore_index=True, sort=False)\n\n# Encode gender (if == female, True)\ncombined_data['female'] = combined_data.Sex == 'female'\n\n# Split out Title\ntitle = []\nfor i in combined_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ncombined_data['title'] = title\n\n# Replace the title values with an aliased dictionary\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    'Mr' : 'Mr', \n    'Mrs' : 'Mrs',\n    'Miss' : 'Miss',\n    'Master' : 'Master',\n    'Don' : 'Formal',\n    'Dona' : 'Formal',\n    'Rev' : 'Religious',\n    'Dr' : 'Academic',\n    'Mme' : 'Mrs',\n    'Ms' : 'Miss',\n    'Major' : 'Formal',\n    'Lady' : 'Formal',\n    'Sir' : 'Formal',\n    'Mlle' : 'Miss',\n    'Col' : 'Formal',\n    'Capt' : 'Formal',\n    'the Countess' : 'Formal',\n    'Jonkheer' : 'Formal',\n}\ncleaned_title = title_arr.map(title_dict)\ncombined_data['cleaned_title'] = cleaned_title\n\n# Fill NaN of Age - first create groups to find better medians than just the overall median and fill NaN with the grouped medians\ngrouped = combined_data.groupby(['female','Pclass', 'cleaned_title']) \ncombined_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))\n\n#add an age bin\nage_bin_conditions = [\n    combined_data['Age'] == 0,\n    (combined_data['Age'] > 0) & (combined_data['Age'] <= 16),\n    (combined_data['Age'] > 16) & (combined_data['Age'] <= 32),\n    (combined_data['Age'] > 32) & (combined_data['Age'] <= 48),\n    (combined_data['Age'] > 48) & (combined_data['Age'] <= 64),\n    combined_data['Age'] > 64\n]\nage_bin_outputs = [0, 1, 2, 3, 4, 5]\ncombined_data['age_bin'] = np.select(age_bin_conditions, age_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Embarked\ncombined_data['Embarked'] = combined_data['Embarked'].fillna(\"S\") \n\n# Fill NaN of Fare, adding flag for boarded free, binning other fares\ncombined_data['Fare'] = combined_data['Fare'].fillna(combined_data['Fare'].mode()[0]) \ncombined_data['boarded_free'] = combined_data['Fare'] == 0 \nfare_bin_conditions = [\n    combined_data['Fare'] == 0,\n    (combined_data['Fare'] > 0) & (combined_data['Fare'] <= 7.9),\n    (combined_data['Fare'] > 7.9) & (combined_data['Fare'] <= 14.4),\n    (combined_data['Fare'] > 14.4) & (combined_data['Fare'] <= 31),\n    combined_data['Fare'] > 31\n]\nfare_bin_outputs = [0, 1, 2, 3, 4]\ncombined_data['fare_bin'] = np.select(fare_bin_conditions, fare_bin_outputs, 'Other').astype(int)\n\n# Fill NaN of Cabin with a U for unknown. Not sure cabin will help.\ncombined_data['Cabin'] = combined_data['Cabin'].fillna(\"U\") \n\n# Counting how many people are riding on a ticket\nfrom collections import Counter\ntickets_count = pd.DataFrame([Counter(combined_data['Ticket']).keys(), Counter(combined_data['Ticket']).values()]).T\ntickets_count.rename(columns={0:'Ticket', 1:'ticket_riders'}, inplace=True)\ntickets_count['ticket_riders'] = tickets_count['ticket_riders'].astype(int)\ncombined_data = combined_data.merge(tickets_count, on='Ticket')\n\n# Finding survival rate for people sharing a ticket\n# Note that looking at the mean automatically drops NaNs, so we don't have an issue with using the combined data to calculate survival rate as opposed to just the training data\ncombined_data['ticket_rider_survival'] = combined_data['Survived'].mean()\n\n# # Finding survival rate for people sharing a ticket (cont'd)\n# This groups the data by ticket\n# And then if the ticket group is greater than length 1 (aka more than 1 person rode on the ticket)\n# it looks at the max and min of the _other_ rows in the group (by taking the max\/min after dropping the current row)\n# and if the max is 1, it replaces the default survival rate of .3838383 (the mean) with 1. This represents there being\n# at least one known member of the ticket group which survived. If there is no known survivor on that ticket, but there  \n# is a known fatality, the value is replaced with 0, representing there was at least one known death in that group. If\n# neither, then the value remains the mean. \nfor ticket_group, ticket_group_df in combined_data[['Survived', 'Ticket', 'PassengerId']].groupby(['Ticket']):\n    if (len(ticket_group_df) != 1):\n        for index, row in ticket_group_df.iterrows():\n            smax = ticket_group_df.drop(index)['Survived'].max()\n            smin = ticket_group_df.drop(index)['Survived'].min()\n            if (smax == 1.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'ticket_rider_survival'] = 1\n            elif (smin==0.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'ticket_rider_survival'] = 0\n\n# Finding survival rate for people with a shared last name (same method as above basically)\ncombined_data['last_name'] = combined_data['Name'].apply(lambda x: str.split(x, \",\")[0])  \ncombined_data['last_name_group_survival'] = combined_data['Survived'].mean()\n\nfor last_name_group, last_name_group_df in combined_data[['Survived', 'last_name', 'PassengerId']].groupby(['last_name']):\n    if (len(last_name_group_df) != 1):\n        for index, row in last_name_group_df.iterrows():\n            smax = last_name_group_df.drop(index)['Survived'].max()\n            smin = last_name_group_df.drop(index)['Survived'].min()\n            if (smax == 1.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'last_name_group_survival'] = 1\n            elif (smin==0.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'last_name_group_survival'] = 0\n\n# Finding survival rate for people with a shared last name _and_ fare\ncombined_data['last_name_fare_group_survival'] = combined_data['Survived'].mean()\n\nfor last_name_fare_group, last_name_fare_group_df in combined_data[['Survived', 'last_name', 'Fare', 'PassengerId']].groupby(['last_name', 'Fare']):\n    if (len(last_name_fare_group_df) != 1):\n        for index, row in last_name_fare_group_df.iterrows():\n            smax = last_name_fare_group_df.drop(index)['Survived'].max()\n            smin = last_name_fare_group_df.drop(index)['Survived'].min()\n            if (smax == 1.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'last_name_fare_group_survival'] = 1\n            elif (smin==0.0):\n                combined_data.loc[combined_data['PassengerId'] == row['PassengerId'], 'last_name_fare_group_survival'] = 0\n                \n# Finding cabin group\ncabin_group = []\nfor i in combined_data['Cabin']:\n    cabin_group.append(i[0])\ncombined_data['cabin_group'] = cabin_group\n\n# Adding a family_size feature as it may have an inverse relationship to either of its parts\ncombined_data['family_size'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Mapping ports to passenger pickup order\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ncombined_data['pickup_order'] = combined_data['Embarked'].map(port)\n\n# Encode childhood\ncombined_data['child'] = combined_data.Age < 16\n\n# One-Hot Encoding the titles\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cleaned_title'], prefix=\"C_T\")], axis = 1)\n\n# One-Hot Encoding the Pclass\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Pclass'], prefix=\"PClass\")], axis = 1)\n\n# One-Hot Encoding the  cabin group\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['cabin_group'], prefix=\"C_G\")], axis = 1)\n\n# One-Hot Encoding the ports\ncombined_data = pd.concat([combined_data, pd.get_dummies(combined_data['Embarked'], prefix=\"Embarked\")], axis = 1)","c46c1382":"new_train_data=combined_data.loc[combined_data['is_training_data']==1]\nnew_test_data=combined_data.loc[combined_data['is_training_data']==0]\n# here is the expanded model set and metric tools\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\nk_fold = KFold(n_splits = 10, shuffle=True, random_state=0) \nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n","07518cf1":"# Here are the features\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'C_T_Master', 'C_T_Miss', 'C_T_Mr', 'C_T_Mrs',\n            'C_T_Formal','C_T_Academic', 'C_T_Religious','C_G_A', 'C_G_B', 'C_G_C', 'C_G_D', 'C_G_E', 'C_G_F', 'C_G_G', \n            'C_G_T', 'C_G_U', 'family_size', 'ticket_riders', 'ticket_rider_survival', 'last_name_group_survival', 'last_name_fare_group_survival']\ntarget = 'Survived'\ncvs_train_data = new_train_data[features]\ncvs_test_data = new_test_data[features]\ncvs_target = new_train_data['Survived']\ncvs_train_data.shape","e17f7282":"# Define the classifiers I will use\nclassifiers = [\n    RandomForestClassifier(n_estimators=10, random_state=0),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=2, min_samples_split=6,\n            min_weight_fraction_leaf=0.0, n_estimators=35, n_jobs=None,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=9, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=10,\n            min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=1,\n            oob_score=False, random_state=0, verbose=0, warm_start=False),\n    DecisionTreeClassifier(random_state=0),\n    LogisticRegression(solver='liblinear'),\n    KNeighborsClassifier(n_neighbors=15),\n    SVC(gamma='auto'),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    XGBClassifier(),\n    GaussianNB(),\n    LinearSVC()]","9ca07c5d":"# Fit and use cross_val_score and k_fold to score accuracy\nclf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, cvs_train_data, cvs_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(cvs_train_data, cvs_target)\n    prediction = clf.predict(cvs_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 'run1_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1","9a737b83":"clf_scores_temp = clf_scores.copy()\nclf_scores_1 = pd.DataFrame(clf_scores_temp, columns=clf_scores_temp.pop(0))\nclf_scores_1","43f8f545":"from sklearn.preprocessing import StandardScaler\nstd_scaler = StandardScaler()\nscaled_train_data = std_scaler.fit_transform(cvs_train_data)\nscaled_test_data = std_scaler.transform(cvs_test_data)\nscaled_target = cvs_target","aff7a01a":"clf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, scaled_train_data, scaled_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(scaled_train_data, scaled_target)\n    prediction = clf.predict(scaled_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 'run2_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1\n","cbc32f7c":"clf_scores_temp = clf_scores.copy()\nclf_scores_2 = pd.DataFrame(clf_scores_temp, columns=clf_scores_temp.pop(0))\nclf_scores_2","7f848a60":"from sklearn.preprocessing import MinMaxScaler\nmm_scaler = MinMaxScaler()\nmms_train_data = mm_scaler.fit_transform(cvs_train_data)\nmms_test_data = mm_scaler.transform(cvs_test_data)\nmms_target = cvs_target","c2ebbbf6":"clf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, mms_train_data, mms_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(mms_train_data, mms_target)\n    prediction = clf.predict(mms_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 'run3_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1\n","a810b908":"clf_scores_temp = clf_scores.copy()\nclf_scores_3 = pd.DataFrame(clf_scores_temp, columns=clf_scores_temp.pop(0))\nclf_scores_3","df7b265f":"from sklearn.feature_selection import chi2, SelectKBest","92329a04":"cvs_train_data.shape","ec91df8b":"fit_test = SelectKBest(score_func=chi2, k='all')\nfit_test.fit(\n    mms_train_data, \n    mms_target)\nnp.set_printoptions(precision=3)\n\nfit_test_scores_1 = pd.DataFrame(fit_test.scores_)\nfit_test_scores_1['feature_name'] = cvs_train_data.columns\nfit_test_scores_1.columns = ['chi_squared', 'feature_name']\nfit_test_scores_1 = fit_test_scores_1.sort_values('chi_squared', ascending=False)\nfit_test_scores_1","80ffa3c9":"from scipy.stats import chi2\nchi2.ppf(0.95, len(fit_test_scores_1)-1)","604bac09":"# Here are the features\nfeatures_2 = ['female', 'C_T_Mr', 'C_T_Mrs', 'C_T_Miss', 'Pclass',\n       'ticket_rider_survival', 'C_G_B', 'C_G_U', 'C_G_D', 'C_G_E',\n       'child', 'last_name_fare_group_survival',\n       'last_name_group_survival', 'C_G_C', 'fare_bin']\ntarget = 'Survived'\nlcs_train_data = new_train_data[features_2]\nlcs_test_data = new_test_data[features_2]\nlcs_target = new_train_data['Survived']\nlcs_train_data.shape, lcs_test_data.shape, lcs_target.shape","88ebdfe2":"mm_scaler = MinMaxScaler()\nmms_lcs_train_data = mm_scaler.fit_transform(lcs_train_data)\nmms_lcs_test_data = mm_scaler.transform(lcs_test_data)\nmms_lcs_target = new_train_data['Survived']\nmms_lcs_train_data.shape, mms_lcs_test_data.shape, mms_lcs_target.shape","d7d83194":"from sklearn.feature_selection import chi2, SelectKBest\nfit_test2 = SelectKBest(score_func=chi2, k='all')\nfit_test2.fit(\n    mms_lcs_train_data, \n    mms_lcs_target)\nnp.set_printoptions(precision=3)\n\nfit_test_scores_2 = pd.DataFrame(fit_test2.scores_)\nfit_test_scores_2['feature_name'] = lcs_train_data.columns\nfit_test_scores_2.columns = ['chi_squared', 'feature_name']\nfit_test_scores_2 = fit_test_scores_2.sort_values('chi_squared', ascending=False)\nfit_test_scores_2","1d054308":"from scipy.stats import chi2\nchi2.ppf(0.95, len(fit_test_scores_2)-1)","2ecab6d8":"from sklearn.preprocessing import StandardScaler\n\nttrd = new_train_data[features_2]\nttsd = new_test_data[features_2]\n\nstd_scaler = StandardScaler()\nssca_train_data = std_scaler.fit_transform(ttrd)\nssca_test_data = std_scaler.transform(ttsd)\n\nf2_target = new_train_data['Survived']","bc6078e2":"clf_scores = [['Score', 'Classifier']]\ni = 0\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    acc_score = round(np.mean(cross_val_score(clf, ssca_train_data, f2_target, cv=k_fold, n_jobs=1, scoring='accuracy'))*100,2)\n    print(acc_score, \"%\", name);\n    clf_scores.append([acc_score, name])\n    clf.fit(ssca_train_data, f2_target)\n    prediction = clf.predict(ssca_test_data)\n    submission = pd.DataFrame({\n        \"PassengerId\" : new_test_data['PassengerId'],\n        \"Survived\" : prediction.astype(int)\n    })\n    submission_name = 'run4_{}_{}.csv'.format(i, name)\n    submission.to_csv(submission_name, index=False)\n    i += 1\n","36f8af73":"clf_scores_temp = clf_scores.copy()\nclf_scores_4 = pd.DataFrame(clf_scores_temp, columns=clf_scores_temp.pop(0))\nclf_scores_4","e7c1d12f":"clf_score_temp = clf_scores_1.copy()\nclf_score_cons = pd.DataFrame(clf_score_temp['Classifier'])\nclf_score_cons['run4'] = clf_scores_4['Score']\nclf_score_cons['run3'] = clf_scores_3['Score']\nclf_score_cons['run2'] = clf_scores_2['Score']\nclf_score_cons['run1'] = clf_scores_1['Score']","4f0c58ae":"clf_score_cons","2b1e7858":"import matplotlib.pyplot as plt\nimport seaborn as sns\n","301ec98b":"cm = sns.light_palette('green', as_cmap=True)\n\ns = clf_score_cons.style.background_gradient(cmap=cm, low=0, high=1, axis=1)\ns","d408574b":"# since I started scaling the variables, KNN seems to be functioning real nice. Let's see if GridSearchCV can help us there.\nfrom sklearn.model_selection import GridSearchCV","a5683cdf":"n_neighbors = [14, 16, 17, 18, 19, 20, 22]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = list(range(10,30,1))\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n                cv=k_fold, scoring = \"accuracy\")\ngd.fit(mms_train_data, mms_target)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","0e35be79":"gd.best_estimator_.fit(mms_train_data, mms_target)\nprediction = gd.best_estimator_.predict(mms_test_data)\nsubmission = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : prediction.astype(int)\n})\nsubmission_name = 'run5_knn.csv'\nsubmission.to_csv(submission_name, index=False)","c1eb5bbf":"# New record! .80382! ","3e167a71":"forest_params = dict(     \n    max_depth = [n for n in range(5, 20)],     \n    min_samples_split = [n for n in range(3, 13)], \n    min_samples_leaf = [n for n in range(1, 7)],     \n    n_estimators = [n for n in range(10, 60, 2)],\n)\n\n# # Going to reduce the number of attempts for a while to see if this works better\n# forest_params = dict(     \n#     max_depth = [n for n in range(8, 12)],     \n#     min_samples_split = [n for n in range(5, 9)], \n#     min_samples_leaf = [n for n in range(2, 4)],     \n#     n_estimators = [n for n in range(10, 60, 25)],\n# )\n\n\nforest_gscv = GridSearchCV(estimator=RandomForestClassifier(), param_grid=forest_params, cv=k_fold) \nforest_gscv.fit(ssca_train_data, cvs_target)\n\n\nprint(\"Best score: {}\".format(forest_gscv.best_score_))\nprint(\"Optimal params: {}\".format(forest_gscv.best_estimator_))\n\n\nforest_gscv.best_estimator_.fit(ssca_train_data, cvs_target)\nprediction = forest_gscv.best_estimator_.predict(ssca_test_data)\nsubmission = pd.DataFrame({\n    \"PassengerId\" : new_test_data['PassengerId'],\n    \"Survived\" : prediction.astype(int)\n})\nsubmission_name = 'run6_forest_gscv.csv'\nsubmission.to_csv(submission_name, index=False)","82d479fb":"# Import Classifiers","df4923b1":"# So it looks like the tree random forest classifiers I have, plus the extratees, seem to be the best.\nI'll try submitting them.","4d3a71c4":"# 4 - running again std scaler new features","505c7e14":"# GridSearchCV for Random Forest","e2001a97":"# 3 - Min Max Scale the values and run the classifiers on the new values","e8b54aad":"# Load data","46723210":"# Drop some low-chi^2 features and run chi^2 again","9ee97a9b":"# Combine and process","a26b07cc":"# Run chi^2 values to determine feature importance","f162df45":"# GridSearchCV for KNN","fd2f296b":"# 1 - Run Classifiers ","265d6047":"The extra trees set a record at 79904. Everything else was lower.","897638dc":"# 2 - Standard scale the values and run classifiers on scaled values","a8f128d3":"# define features","6aea0d76":"# Define Classifiers"}}