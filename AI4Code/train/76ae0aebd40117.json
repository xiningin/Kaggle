{"cell_type":{"e28ab7e7":"code","102c6e43":"code","fc452d71":"code","e543c934":"code","6280e34e":"code","096bad15":"code","0e41479e":"code","531d60be":"code","a3bfe6cc":"code","9a700183":"code","ab3f74b1":"code","8e604ebd":"code","3fb63cc7":"code","40012a5b":"code","38862252":"code","609c66c2":"code","2948034a":"code","39f5b119":"code","36454f36":"code","398984cd":"code","f78bdbf3":"code","acbabea9":"code","feca1941":"code","388b0027":"code","fb64a77a":"code","193a7d25":"code","2de334e6":"code","c882231f":"code","cc3f47e2":"code","d3450a6e":"code","ecb6bf32":"code","5bc9195e":"code","feddab8b":"code","60c4014f":"markdown","2c0e0832":"markdown","c4833d06":"markdown","340de50d":"markdown","8c53b2b7":"markdown","20a9e9dc":"markdown","ec63dcb5":"markdown","75005078":"markdown","1eb9c949":"markdown","5f2ebc0f":"markdown","418a0df9":"markdown","9a7e6090":"markdown","cfca2157":"markdown","d4ca691a":"markdown","40305e54":"markdown","87677950":"markdown"},"source":{"e28ab7e7":"# libraries\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef title(string, icon='-'):\n    print(string.center(100,icon))\n\ndef setJupyterNotebook():\n    import pandas as pd;import numpy as np\n    np.set_printoptions(precision=3)\n    pd.set_option('display.float_format', lambda x: '%.3f' % x)\n    np.random.seed(8)\n    import warnings\n    warnings.filterwarnings('ignore')\n\n\ndef Split(df,target='target',test_size=0.3,random_state=8):\n    '''\n    input: pandas dataframe, target='target', test_size=0.3,random_state=8\n    output: tuple of X_train, X_test, y_train, y_test\n    '''\n    X,y = df.drop([target], axis=1),df[target]\n    from sklearn.model_selection import train_test_split\n    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n\ndef OHE(data,non_features,cat_features=None): # Use later OneHotEncoder of sklearn and fit_transform(X_train) and transform (X_test)\n    X_train, X_test, y_train, y_test = data\n    if cat_features is None:\n        cat_features = [col for col in X_train.select_dtypes('object').columns if col not in non_features]\n    X_train_cat, X_test_cat = tuple([pd.concat([pd.get_dummies(X_cat[col],drop_first=False,prefix=col,prefix_sep='_',)\\\n               for col in cat_features],axis=1) for X_cat in data[:2]])\n    X_train = pd.concat([X_train,X_train_cat],axis=1).drop(cat_features,axis=1)\n    X_test = pd.concat([X_test,X_test_cat],axis=1).drop(cat_features,axis=1)\n    OHE_features = list(X_train_cat.columns)\n    return (X_train, X_test, y_train, y_test), OHE_features\n\ndef Balance(data):\n    '''\n    input: data = tuple of X_train, X_test, y_train, y_test\n           target='target' # column name of the target variable\n    output: data = the balanced version of data\n    => FUNCTION DOES BALANCING ONLY ON TRAIN DATASET\n    '''\n    X_train, X_test, y_train, y_test = data\n    target=y_train.name #if else 'target'\n    print('Checking Imbalance');print(y_train.value_counts(normalize=True))\n    Input = 'y'#input('Do You Want to Treat Data?\\nPress \"y\" or \"n\" \\n')\n    if Input.strip() == \"y\":\n        print('Treating Imbalance on Train Data')\n        from imblearn.over_sampling import SMOTE\n        from imblearn.under_sampling import NearMiss\n        SM = SMOTE(random_state=8, ratio=1.0)\n        X_train_SM, y_train_SM = SM.fit_sample(X_train, y_train)\n        X_train_SM = pd.DataFrame(X_train_SM, columns=X_train.columns)\n        y_train_SM = pd.Series(y_train_SM,name=target);\n        print('After Balancing')\n        print(y_train_SM.value_counts(normalize=True));\n        print('*',\"*\");plt.figure(figsize=(8,3));\n        plt.subplot(1,2,1);sns.countplot(y_train);plt.title('before Imbalance');\n        plt.subplot(1,2,2);sns.countplot(y_train_SM);plt.title('after Imbalance Treatment');plt.show()\n        data = X_train_SM,X_test, y_train_SM, y_test\n    \n    elif Input.strip()=='n':\n        sns.countplot(y_train);plt.print('BEFORE');\n        data = data\n    \n    return data\n\n\ndef SetIndex(data, index = 'ID'):\n    '''\n    setting index  before puting to ML algorithms and manual label encoding of y_train and y_test\n    '''\n    X_train, X_test, y_train, y_test = data\n    y_train = y_train.map({'Yes':1,'No':0}); y_test = y_test.map({'Yes':1,'No':0})\n\n    try:X_train = X_train.set_index(index)\n    except:X_train=X_train\n    y_train.index=X_train.index\n    try:X_test = X_test.set_index(index)\n    except:X_test=X_test\n    y_test.index=X_test.index\n    data = X_train, X_test, y_train, y_test\n    return data\n\ndef FeatureScale(data,OHE_features,scaler='MinMaxScaler'):\n    '''\n        Feature Scaling only numerical_feaures. and not on OHE features\n        input   data = X_train, X_test, y_train, y_test\n                OHE_features = list of One Encoded categorical feature columns\n                scaler = either 'StandardScaler' or 'MinMaxScaler'\n        output data = X_train, X_test, y_train, y_test\n    '''\n    X_train, X_test, y_train, y_test = data\n    X_train_num = X_train[[col for col in X_train.columns if col not in OHE_features]]\n    X_train_cat = X_train[[col for col in X_train.columns if col in OHE_features]]\n    X_test_num = X_test[[col for col in X_test.columns if col not in OHE_features]]\n    X_test_cat = X_test[[col for col in X_test.columns if col in OHE_features]]\n\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n    scalers = {'StandardScaler':StandardScaler(),'MinMaxScaler':MinMaxScaler()}\n    sc = scalers[scaler]\n    print('Applying',scaler)\n    sc_X_train= pd.DataFrame(sc.fit_transform(X_train_num),columns=X_train_num.columns,index=X_train_num.index)\n    sc_X_test = pd.DataFrame(sc.transform(X_test_num),columns=X_test_num.columns,index=X_test_num.index)\n\n    X_train_scale = pd.concat([sc_X_train,X_train_cat],axis=1)\n    X_test_scale = pd.concat([sc_X_test,X_test_cat],axis=1)\n    \n    data = X_train_scale, X_test_scale, y_train, y_test \n    return data\n\ndef FeatureScaleAll(data,scaler='MinMaxScaler'):\n    '''\n    FeaturesScaling Both on OHE columns and numerical columns\n    input   data = X_train, X_test, y_train, y_test\n            scaler = either 'StandardScaler' or 'MinMaxScaler'\n    output data = X_train, X_test, y_train, y_test\n    '''\n    X_train, X_test, y_train, y_test = data\n    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n    scalers = {'StandardScaler':StandardScaler(),'MinMaxScaler':MinMaxScaler()}\n    sc = scalers[scaler]\n    print('Applying',scaler)\n    sc_X_train= pd.DataFrame(sc.fit_transform(X_train),columns=X_train.columns,index=X_train.index)\n    sc_X_test = pd.DataFrame(sc.transform(X_test),columns=X_test.columns,index=X_test.index)\n    data = sc_X_train, sc_X_test, y_train, y_test \n    return data\n\n\n#importing Algorithms\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\n\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier # inspired by LGBM\n\nfrom lightgbm import LGBMClassifier\n\ndef ClassificationModelDictionary():\n    LR = dict(name ='LogisticRegression',model = LogisticRegression(),\n         parameters = {\"penalty\": ['l1', 'l2'],'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n         best_parameters = {},\n         cv_params={'penalty': ['l1', 'l2'],'random_state':[0,8]})\n    DT = dict(name ='DecisionTreeClassifier',model = DecisionTreeClassifier(),\n         parameters = {'criterion': ['gini', 'entropy'],'splitter': ['best', 'random'],\n                       'max_depth': [None,2,3,4,5,6,7,8,9,10], 'max_features': ['auto', 'log2',None],\n                       'random_state': [8],'min_samples_leaf' : [1,2,3,4,5]},\n         best_parameters = {},\n         cv_params= {'criterion': ['gini', 'entropy'],'splitter': ['best'],\n                    'max_features': ['auto', 'log2', None],'random_state': [0,8]}\n)\n\n    KNN= dict(name = 'KNeighborsClassifier',\n              model = KNeighborsClassifier(),\n              parameters = {'n_neighbors': [i for i in range(1,25)],\n                            'p':[1,2]}, # 1=manhattan, 2, euclidean\n             best_parameters = {},\n             cv_params={'priors': [None], 'var_smoothing': [1e-09]})\n\n    GNB= dict(name = 'GaussianNB',\n              model = GaussianNB(),\n              parameters = {'priors':[None,],'var_smoothing':[1e-09,]},\n              best_parameters = {},\n              cv_params={'priors': [None], 'var_smoothing': [1e-09]})\n    BNB= dict(name = 'BernoulliNB',\n              model = BernoulliNB(),\n              parameters = {'alpha':[1.0,],\n                            'binarize':[0.0,],\n                            'fit_prior':[True,False],\n                            'class_prior':[None]},\n              best_parameters = {},\n              cv_params={'alpha': [1.0],'binarize': [0.0],\n                        'fit_prior': [True, False],'class_prior': [None]})\n\n    RF= dict(name = 'RandomForestClassifier',\n              model = RandomForestClassifier(),\n              parameters = {'max_depth': [2, 3, 4],\n                            'bootstrap': [True, False],\n                            'max_features': ['auto', 'sqrt', 'log2', None],\n                            'criterion': ['gini', 'entropy'],\n                            'random_state': [8]},\n             best_parameters = {},\n             cv_params= {'max_depth': [2, 3, 4],'bootstrap': [True, False],\n                         'max_features': ['auto', 'sqrt', 'log2', None],'criterion': ['gini', 'entropy'],\n                         'random_state': [8]})\n    SVM= dict(name = 'SVC',\n              model = SVC(),\n              parameters = {'C': [1, 10, 100,500, 1000], 'kernel': ['linear','rbf'],\n                            'C': [1, 10, 100,500, 1000], 'gamma': [1,0.1,0.01,0.001, 0.0001], 'kernel': ['rbf'],\n                            #'degree': [2,3,4,5,6] , 'C':[1,10,100,500,1000] , 'kernel':['poly']\n                                    },\n              best_parameters = {},\n              cv_params={'C': [1, 10, 100, 500, 1000],'kernel': ['rbf'],\n                        'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n)\n\n\n    BAG_params={'base_estimator': [DecisionTreeClassifier(),\n                                   DecisionTreeClassifier(max_depth=2),\n                                   DecisionTreeClassifier(max_depth=4),\n                                   BernoulliNB(),\n                                   LogisticRegression(penalty='l1'),\n                                   LogisticRegression(penalty='l2'),\n                                   ], #GaussianNB(),],\n                'n_estimators': [10,], \n                'max_samples': [1.0], 'max_features': [1.0], \n                'bootstrap': [True,], 'bootstrap_features': [False], \n                'oob_score': [False], #'warm_start': [False], \n                'n_jobs': [None], 'random_state': [8], 'verbose': [0]}\n\n    BAG= dict(name = 'BaggingClassifier',\n              model= BaggingClassifier(),\n              parameters = BAG_params,\n              best_parameters = {},\n              cv_params={'base_estimator': [DecisionTreeClassifier(criterion='gini'),\n                                            DecisionTreeClassifier(criterion='entropy'),\n                                            BernoulliNB(),\n                                            LogisticRegression(penalty='l1'),\n                                            LogisticRegression(penalty='l2')],\n                            'bootstrap': [True],\n                            'random_state':[0,8] }\n             )\n\n    GB = dict(name = 'GradientBoostingClassifier',\n              model = GradientBoostingClassifier(),\n              parameters = {\n                  'loss':['deviance','exponential'],\n                  'learning_rate':[0.1,0.01,1.0],\n                  'n_estimators':[100,200,25,50,75],\n                  'subsample':[1.0,0.75,0.5,0.25,0.01], # < 1.0 leads to reduction of variance and increase in bias\n                                    #  < 1.0 results in Stochastic Gradient Boosting\n                  'random_state':[8],\n                  #'ccp_alpha': [0.0,0.0001,0.001,0.01,0.1,1.0]# only in version 0.22\n                                  #cost-complexity pruning algorithm to prune tree to avoid over fitting\n                  #'min_samples_split':[2,3,4],\n                  #'min_samples_leaf':[1,2,3],\n                  #'min_weight_fraction_leaf':[0],\n                  #'max_depth':[3,4,5],\n                  #'min_impurity_decrease':[0],\n                  #'init':[None],\n                  #'max_features':[None],\n                  #'verbose':[0],\n\n              },\n              best_parameters = {},\n              cv_params = {'loss': ['deviance', 'exponential'],\n                            'n_estimators': [100],'random_state': [0,8]}\n              )\n    ADA= dict(name = 'AdaBoostClassifier',\n              model = AdaBoostClassifier(),\n              parameters = {'base_estimator':[DecisionTreeClassifier(max_depth=1),\n                                                 DecisionTreeClassifier(max_depth=2),\n                                                 DecisionTreeClassifier(max_depth=3),\n                                                 DecisionTreeClassifier(max_depth=4),\n                                                 BernoulliNB(),\n                                                 #GaussianNB(),\n                                             ],\n                            'n_estimators':[25,50,75,100],# ,100\n                            'learning_rate':[1.0,0.1],\n                            #'alogorithm':['SAMME', 'SAMME.R'],\n                            'random_state':[8],\n                           },\n              best_parameters = {},\n              cv_params = {'base_estimator': [None,DecisionTreeClassifier(criterion='gini'),\n                                                    DecisionTreeClassifier(criterion='entropy'),\n                                                    BernoulliNB(),\n                                                    LogisticRegression(penalty='l1'),\n                                                    LogisticRegression(penalty='l2')],\n                            'random_state':[0,8] })\n\n    XGB_params = {'max_depth': [3],'learning_rate': [0.1],'n_estimators': [100,],#50,150,200],\n                  'verbosity': [1],'objective': ['binary:logistic'],\n                  'booster': ['gbtree', 'gblinear','dart'], # IMPORTANT\n                  'tree_method': ['auto', 'exact', 'approx', 'hist'],#, 'gpu_hist' # IMPORTANT\n                  'n_jobs': [1],'gamma': [0],\n                  'min_child_weight': [1],'max_delta_step': [0],\n                  'subsample': [1],\n                  'colsample_bytree': [1],'colsample_bylevel': [1],'colsample_bynode': [1],\n                  'reg_alpha': [0],'reg_lambda': [1],'scale_pos_weight': [1],'base_score': [0.5],\n                  'random_state': [8],'missing': [None]}\n\n    XGB= dict(name = 'XGBClassifier',\n              model= XGBClassifier(),\n              parameters = XGB_params,\n              best_parameters = {},\n              cv_params = {'tree_method': ['auto', 'exact', 'approx', 'hist'],\n                              'booster': ['gbtree', 'gblinear', 'dart'],\n                              'random_state':[0,8]}\n             )\n\n    LBGM_params={'boosting_type': ['gbdt','goss'], # ,'dart','rf'\n                 'num_leaves': [31], 'max_depth': [-1], 'learning_rate': [0.1], \n                 'n_estimators': [100], 'subsample_for_bin': [200000], 'objective': [None],\n                 'class_weight': [None], 'min_split_gain': [0.0], 'min_child_weight': [0.001],\n                 'min_child_samples': [20], 'subsample': [1.0], 'subsample_freq': [0], \n                 'colsample_bytree': [1.0], 'reg_alpha': [0.0], 'reg_lambda': [0.0], \n                 'random_state': [8], 'n_jobs': [-1], 'silent': [True], 'importance_type': ['split']}\n\n    LGBM= dict(name = 'LGBMClassifier',\n              model= LGBMClassifier(),\n              parameters = LBGM_params,\n              best_parameters = {},\n              cv_params = {'boosting_type': ['gbdt', 'goss'],\n                               'random_state':[0,8]}\n             )\n\n    HGB_params={'loss': ['auto','binary_crossentropy',], # 'categorical_crossentropy'\n                'learning_rate': [0.1], 'max_iter': [100], 'max_leaf_nodes': [31],\n                'max_depth': [None], 'min_samples_leaf': [20], \n                'l2_regularization': [0,1,2], # for no-regulaiziation, 1 regulztn\n                'max_bins': [255], \n                #'warm_start': [False],\n                'scoring': [None], 'validation_fraction': [0.1],\n                'n_iter_no_change': [None], 'tol': [1e-07], 'verbose': [0],\n                'random_state': [8]}\n\n    HGB= dict(name = 'HistGradientBoostingClassifier',\n              model= HistGradientBoostingClassifier(),\n              parameters = HGB_params,\n              best_parameters = {},\n              cv_params = {'loss': ['auto', 'binary_crossentropy'],\n                                                'l2_regularization': [0, 1, 2],\n                                                'random_state':[0,8]}\n             )\n\n    models = {i:mod for i,mod in enumerate([LR,DT,KNN,GNB,BNB,RF,SVM,BAG,GB,ADA,XGB,LGBM,HGB],start=1)}\n    return models\n\n\n\ndef MODEL(model_dict,data,phase='',scores=None,use_params=False):\n    '''\n    input =>\n        model_dict  : Each Individual Models in a dictionary, \n        data        : (X_train,X_test,y_train,y_test) \n        phase       : '' (default) [options like base, final, HPO, etc...]\n        scores      : None (default) scores id data frame with cols: 'Model','Phase','AUC_ROC','TrainingAccuracy\n                            'TestingAccuracy','Recall','Precision','F1_Score','FalsePositives','FalseNegatives'\n        use_params  : False (default) -- uses best_parameters from model_dict\n    output =>\n        tuple of dictionary{model_name:model} and scores (DataFrame)\n    '''\n    X_train, X_test, y_train, y_test = data\n    if scores is None: scores=pd.DataFrame(columns=['Model','Phase','AUC_ROC','TrainingAccuracy',\n                                             'TestingAccuracy','Recall','Precision','F1_Score',\n                                             'FalsePositives','FalseNegatives'])\n    model = model_dict['model']\n    if use_params:model.set_params(**model_dict['best_parameters'])\n    algorithm_name = model_dict['name']\n    model_name = algorithm_name+phase\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score,confusion_matrix\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    record = [{'Model':algorithm_name,'Phase':phase,\n               'AUC_ROC':roc_auc_score(y_test,y_pred),\n               'TrainingAccuracy':accuracy_score(y_train,model.predict(X_train)),\n               'TestingAccuracy':accuracy_score(y_test,y_pred),\n               'Recall':recall_score(y_test,y_pred),\n               'Precision':precision_score(y_test,y_pred),\n               'F1_Score':f1_score(y_test,y_pred),\n               'FalsePositives':fp,\n               'FalseNegatives':fn,\n              }]\n    scores =scores.append(pd.DataFrame(record),sort=False)\n    return {model_name:model}, scores\n\n\ndef RunAll(models,data,phase='',scores=None,trained_models = {},use_params=False):\n    '''\n    Run All Alogithms:\n    input =>\n        models      : a dictionary of All Models \n        data        : (X_train,X_test,y_train,y_test) \n        phase       : '' (default) [options like base, final, HPO, etc...]\n        scores      : None (default) scores id data frame with cols: 'Model','Phase','AUC_ROC','TrainingAccuracy\n                            'TestingAccuracy','Recall','Precision','F1_Score','FalsePositives','FalseNegatives'\n        trained_models : {} (default)     -- a dictionary of all trained models and it's unique names\n        use_params  : False (default) -- uses best_parameters from model_dict\n    output =>\n        tuple of trained_models (dictionary) and scores (DataFrame)\n    '''\n    if scores is None: scores=pd.DataFrame(columns=['Model','Phase','AUC_ROC','TrainingAccuracy',\n                                             'TestingAccuracy','Recall','Precision','F1_Score',\n                                                   'FalsePositives','FalseNegatives'])\n    for i in range(1,len(models)+1):\n        trained_model, scores = MODEL(model_dict=models[i],data=data,phase=phase,scores=scores,use_params=use_params)\n        trained_models.update(trained_model)\n    return trained_models, scores\n\n\n","102c6e43":"# SHAPIRO TEST (STATISTICAL WAY of NORMAL DISTRIBUTION CHECK)\ndef ShapiroTest(data_name,data,verbose=True):\n    '''\n    input column name and data in single dimension\n    return Boolean (if shapiro test passed then True, else False)\n    '''\n    from scipy.stats import shapiro; n_icon = 123;\n    H0 = f'H0: {data_name}  = normally distributed'\n    Ha = f'Ha: {data_name} != normally distributed'\n    if verbose:print(n_icon*'=');print('\\t|',H0,'|\\t[Null Hypothesis]');print('\\t|',Ha,'|\\t[Alternate Hypothesis]')\n    _, pval = shapiro(data)\n    if verbose:print('The P-value(Probability of Null Hypothesis being True) is',pval);print(n_icon*'-')\n    if pval < 0.05:\n        if verbose:print(f'Since the P-Value({pval}) is less than 0.05, \\n\\tWe reject the Null Hypothesis(\"{H0}\")\\\n    \\nHence, Our Alternate Hyposthesis \"{Ha}\" holds Good')\n        return False\n    elif pval > 0.05:\n        if verbose:print(f'Since the P-Value({pval}) is more than 0.05, \\n\\tWe fail to reject the Null Hypothesis(\"{H0}\")\\\n    \\nTherefore \"{data_name}\" is normally distributed');\n        return True","fc452d71":"setJupyterNotebook()\ndf=pd.read_excel('\/kaggle\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx',\n              sheet_name='Data',index_col = 0)","e543c934":"df.head()","6280e34e":"df.info()","096bad15":"index = 'ID';target = 'Personal Loan'\nnon_features = [index,target]","0e41479e":"df['Education_cat'] = df['Education'].astype('object')\nfor col in ['ZIP Code','Personal Loan']:df[col] = df[col].astype('object')# Converting to category \n# Identifying type of columns\ntarget = 'Personal Loan'\nnon_features = [target,]\nnum_features = [col for col in df.select_dtypes(np.number).columns if col not in non_features]\ncat_features = [col for col in df.select_dtypes('object').columns if col not in non_features]\ndf[target] = df[target].map({1:'Yes',0:'No'})\ndf['Education_cat']=df['Education_cat'].map({1: 'Undergrad', 2: 'Graduate', 3: 'Advanced\/Professional'})","531d60be":"plt.figure(figsize=(20,5));sns.heatmap(df.isnull(),yticklabels=False,cbar=False)\nprint('TotalNullValues',df.isnull().sum().sum())","a3bfe6cc":"df[num_features].plot(kind='box',subplots=True, layout=(4,4), fontsize=8, figsize=(14,14));","9a700183":"print(df[target].value_counts(normalize=True));sns.countplot(df[target]);plt.title('Checking Imabalance');plt.show()","ab3f74b1":"QualityDict = {'normal':[],'not_normal':[],'categorical':[]}\nfor col in num_features:\n    if ShapiroTest(data_name = col,data = df[col], verbose=False):QualityDict['normal'].append(col)\n    else:QualityDict['not_normal'].append(col)\nprint(QualityDict['not_normal']);print(QualityDict['normal'])","8e604ebd":"skewed = ['Family'];withOutliers = ['Income', 'CCAvg', 'Mortgage']\nIQR=df[withOutliers].describe().T['75%']-df[withOutliers].describe().T['25%']\nLW,UW = df[withOutliers].describe().T['25%']-(IQR*1.5),df[withOutliers].describe().T['75%']+(IQR*1.5)\nfor i in withOutliers:df[i][df[i]>UW[i]]=UW[i];df[i][df[i]<LW[i]]=LW[i]\ndf[withOutliers].plot(kind='box',subplots=True, layout=(4,4), fontsize=8, figsize=(14,14));","3fb63cc7":"# Feature Scaling or Log\nlen(num_features)","40012a5b":"plt.figure(figsize=(20,10))\nfor i,col in enumerate(num_features,start=1):plt.subplot(3,4,i);sns.boxplot(y=df[col],x=df[target]);\nplt.show()","38862252":"non_features = [target,'ZIP Code']# removing ZIP Codel colum from features\nnum_features = [col for col in df.select_dtypes(np.number).columns if col not in non_features]\ncat_features = [col for col in df.select_dtypes('object').columns if col not in non_features]\nprint('Numerical Features:',num_features); print('Categorical Features:',cat_features)","609c66c2":"# dropping ZIP Code\ndf =df.drop(['ZIP Code'], axis=1)","2948034a":"df.head()","39f5b119":"data = Split(df,target=target)","36454f36":"data,OHE_features=OHE(data=data,non_features=non_features,cat_features=None)","398984cd":"data = Balance(data = data)","f78bdbf3":"data = SetIndex(data=data, index=index) # setting index before modeling","acbabea9":"data[0].info()","feca1941":"models = ClassificationModelDictionary()\n# modelling without scaling\nunscaled_data = data\ntrained_models, unscaled = RunAll(models=models,data=unscaled_data,phase='unscaled',\n                                  scores=None,trained_models = {},use_params=False)\n# modelling after the MINMAX scaling all columns irrespective of numerical or OHE columns\nminmaxAll_data = FeatureScaleAll(data,scaler='MinMaxScaler')\ntrained_models, MinMaxAll = RunAll(models=models,data=minmaxAll_data,phase='MinMaxAll',\n                                scores=None,trained_models = {},use_params=False)\n# modelling after the Z-Score scaling all columns irrespective of numerical or OHE columns\nzcoreAll_data = FeatureScaleAll(data,scaler='StandardScaler')\ntrained_models, ZscoreAll = RunAll(models=models,data=zcoreAll_data,phase='ZscoreAll',\n                                scores=None,trained_models = {},use_params=False)\n# modelling after the MINMAX scaling on numerical columns only discarding OHE columns to scale\nminmax_data = FeatureScale(data,OHE_features=OHE_features,scaler='MinMaxScaler')\ntrained_models, MinMax = RunAll(models=models,data=minmax_data,phase='MinMax',\n                                scores=None,trained_models = {},use_params=False)\n# modelling after the Z-Score scaling on numerical columns only discarding OHE columns to scale\nzcore_data = FeatureScale(data,OHE_features=OHE_features,scaler='StandardScaler')\ntrained_models, Zscore = RunAll(models=models,data=zcore_data,phase='Zscore',\n                                scores=None,trained_models = {},use_params=False)","388b0027":"scores = pd.concat([unscaled,MinMaxAll,ZscoreAll,MinMax,Zscore],axis=0).sort_values('AUC_ROC', ascending=False)\nscores.head()","fb64a77a":"from datetime import datetime; import pickle\nDataModelScores = dict(data=data,trained_models=trained_models,modelsDict=models,scores=scores)\ntime = datetime.now().strftime('%Y_%b_%d_%H_%M_%S')\npickle.dump(DataModelScores, open('{}_DataModelScores.pkl'.format(time), 'wb'))\nDataModelScores = pickle.load(open(f'{time}_DataModelScores.pkl', 'rb'))","193a7d25":"scores_selected = scores.head(10) # selecting top 10 with maxinum ROC_AUC\nscores_selected","2de334e6":"for_CV = {phase:list(scores_selected[scores_selected.Phase==phase].Model.unique())\\\n                  for phase in scores_selected.Phase.unique()}","c882231f":"models = ClassificationModelDictionary()\n\nmodels = {models[i]['name']:[models[i]['model'],models[i]['cv_params'],models[i]['best_parameters']]for i in models}\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer,accuracy_score, recall_score\nscoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n\nfor phase in for_CV.keys():\n    if phase == 'unscaled':\n        print(f'Selecting {phase} data');print(123*'*')\n        unscaled_data = data\n        for name in for_CV[phase]:\n            model = models[name][0]\n            parameters = models[name][1]\n            print('Selecting Model:',name,'\\nWith Param Dict:',parameters)\n\n            cv = GridSearchCV(estimator=model, param_grid=parameters,cv=10,scoring=scoring,\n                              refit='AUC',return_train_score=True)\n            cv.fit(unscaled_data[0],unscaled_data[2])\n            models[name][2][phase]=cv.best_params_\n            print('Best Parameters: ------->',models[name][2]);print(123*'-')\n    elif phase == 'ZscoreAll':\n        print(f'Selecting {phase} data');print(123*'*')\n        zcoreAll_data = FeatureScaleAll(data,scaler='StandardScaler')\n        for name in for_CV[phase]:\n            model = models[name][0]\n            parameters = models[name][1]\n            print('Selecting Model:',name,'\\nWith Param Dict:',parameters)\n\n            cv = GridSearchCV(estimator=model, param_grid=parameters,cv=10,scoring=scoring,\n                              refit='AUC',return_train_score=True)\n            cv.fit(zcoreAll_data[0],zcoreAll_data[2])\n            models[name][2][phase]=cv.best_params_\n            print('Best Parameters: ------->',models[name][2]);print(123*'-')\n    elif phase == 'Zscore':\n        print(f'Selecting {phase} data');print(123*'*')\n        zcore_data = FeatureScale(data,OHE_features=OHE_features,scaler='StandardScaler')\n        for name in for_CV[phase]:\n            model = models[name][0]\n            parameters = models[name][1]\n            print('Selecting Model:',name,'\\nWith Param Dict:',parameters)\n\n            cv = GridSearchCV(estimator=model, param_grid=parameters,cv=10,scoring=scoring,\n                              refit='AUC',return_train_score=True)\n            cv.fit(zcore_data[0],zcore_data[2])\n            models[name][2][phase]=cv.best_params_\n            print('Best Parameters: ------->',models[name][2]);print(123*'-')\n    elif phase == 'MinMaxAll':\n        print(f'Selecting {phase} data');print(123*'*')\n        minmaxAll_data = FeatureScaleAll(data,scaler='MinMaxScaler')\n        for name in for_CV[phase]:\n            model = models[name][0]\n            parameters = models[name][1]\n            print('Selecting Model:',name,'\\nWith Param Dict:',parameters)\n\n            cv = GridSearchCV(estimator=model, param_grid=parameters,cv=10,scoring=scoring,\n                              refit='AUC',return_train_score=True)\n            cv.fit(minmaxAll_data[0],minmaxAll_data[2])\n            models[name][2][phase]=cv.best_params_\n            print('Best Parameters: ------->',models[name][2]);print(123*'-')\n    elif phase == 'MinMax':\n        print(f'Selecting {phase} data');print(123*'*')\n        minmax_data = FeatureScale(data,OHE_features=OHE_features,scaler='MinMaxScaler')\n        for name in for_CV[phase]:\n            model = models[name][0]\n            parameters = models[name][1]\n            print('Selecting Model:',name,'\\nWith Param Dict:',parameters)\n\n            cv = GridSearchCV(estimator=model, param_grid=parameters,cv=10,scoring=scoring,\n                              refit='AUC',return_train_score=True)\n            cv.fit(minmax_data[0],minmax_data[2])\n            models[name][2][phase]=cv.best_params_\n            print('Best Parameters: ------->',models[name][2]);print(123*'-')","cc3f47e2":"cv_selected_models = {i:[models[i][0],models[i][2]] for i in models.keys() if models[i][2] != {}}\nprint('Best Models are'.center(100,'-'))\nfor key in cv_selected_models.keys():print(key,'with data',*cv_selected_models[key][1])","d3450a6e":"def Classify(algorithm,model,data,phase='',scores=None):\n    '''\n    input: algorithm=alogorithm Name,model=alogorithm with set params,data,phase='',scores=None\n    output: scores dataframe\n    '''\n    X_train, X_test, y_train, y_test = data\n    if scores is None: scores=pd.DataFrame(columns=['Model','Phase','AUC_ROC','TrainingAccuracy',\n                                             'TestingAccuracy','Recall','Precision','F1_Score',\n                                             'FalsePositives','FalseNegatives'])\n    model_name = algorithm+phase\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    from sklearn.metrics import precision_score,recall_score,f1_score,roc_auc_score,accuracy_score,confusion_matrix\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n    record = [{'Model':algorithm,'Phase':phase,\n               'AUC_ROC':roc_auc_score(y_test,y_pred),\n               'TrainingAccuracy':accuracy_score(y_train,model.predict(X_train)),\n               'TestingAccuracy':accuracy_score(y_test,y_pred),\n               'Recall':recall_score(y_test,y_pred),\n               'Precision':precision_score(y_test,y_pred),\n               'F1_Score':f1_score(y_test,y_pred),\n               'FalsePositives':fp,\n               'FalseNegatives':fn,\n              }]\n    scores =scores.append(pd.DataFrame(record),sort=False)\n    return scores.sort_values('AUC_ROC',ascending=False)","ecb6bf32":"scores = pd.DataFrame(columns=['Model','Phase','AUC_ROC','TrainingAccuracy','TestingAccuracy',\n                               'Recall','Precision','F1_Score','FalsePositives','FalseNegatives'])\nfor key in cv_selected_models.keys():\n    phases = list(cv_selected_models[key][1].keys())\n    for phase in phases:\n        if phase == 'unscaled':data_phase = data\n        elif phase == 'ZscoreAll':data_phase = FeatureScaleAll(data,scaler='StandardScaler')\n        elif phase == 'Zscore':data_phase = FeatureScale(data,OHE_features=OHE_features,scaler='StandardScaler')\n        elif phase == 'MinMaxAll':data_phase = FeatureScaleAll(data,scaler='MinMaxScaler')\n        elif phase == 'MinMax':data_phase = FeatureScale(data,OHE_features=OHE_features,scaler='MinMaxScaler')\n        parameters = cv_selected_models[key][1][phase]\n        model =  cv_selected_models[key][0]\n        model.set_params(**parameters)\n        scores = Classify(algorithm=key,model=model,data=data_phase,phase=phase,scores=scores)\nscores.sort_values('AUC_ROC',ascending=False)","5bc9195e":"cv_selected_models['GradientBoostingClassifier'][1]","feddab8b":"plt.figure(figsize=(20,5))\nsns.barplot(x=scores.AUC_ROC,y=scores.Model,hue=scores.Phase);","60c4014f":"## One Hot Encoding","2c0e0832":"## GRID SEARCH HYPER PARAMETER TUNING on to 10 models","c4833d06":"> NOTES:\n* Gradient Boosting classifer is stable in predicing even scaled or non-scaled data. It can reduce Computation cost. \n* Therefore selecting Gradient Boosting Model with Unscaled data, with parameters {'loss': 'deviance', 'n_estimators': 100, 'random_state': 8}","340de50d":"## MODELING","8c53b2b7":"> Observation\n* Income, CCAvg, Mortgage columsn have ouliers\n* Family column data is right skewed as well","20a9e9dc":"## Check for defects in the data such as missing values, null, outliers, and class imbalance","ec63dcb5":"> OBSERVATION\n* High Class Imbalance\n* Since Dataset is comparetviely smaller better to go with Over_sampling","75005078":"> income, family, CCAvg, Education filds show some siginificant influence on target","1eb9c949":"## Splitting Dataset","5f2ebc0f":"> Notes\n* Except Income, Family, CCAvg, education shows some influence. So we use and probably remove other numerical features.\n* But I have to wait to see how algorithm finally says\n* Will remove Zip code also","418a0df9":"## Data Preparation","9a7e6090":"### Bivariate plot to check if variables which are correlated with Target","cfca2157":"### Treating Outliers","d4ca691a":"> OBSERVATIONS\n1. Zip code has 467\tunique ids\n2. Education_cat has 3 unique Features - 'Undergrad', 'Graduate', 'Advanced\/Professional'\n3. Most Frequeent is underGraduate\n4. Target column has More 'No' values, unique are 'yes', 'no'","40305e54":"## Treating Imbalance","87677950":"## Split dataset into train and test\u00b6"}}