{"cell_type":{"fb23f43f":"code","672acaaa":"code","384ac43c":"code","b40a09aa":"code","ea599df8":"code","12701dc0":"code","a7fb295e":"code","3433e603":"code","98e022de":"code","1a61c459":"code","08e728db":"code","d7a07c33":"code","15ee766f":"code","5547ca74":"code","5fbd12da":"code","42294a68":"code","b70daada":"code","7895ca3c":"code","d6ce9c2d":"code","32ad0670":"code","a0b946fd":"markdown","32819b62":"markdown","6dbbc94f":"markdown","70a344a2":"markdown","4ed00867":"markdown","f5db9d58":"markdown","fc0b98c7":"markdown","821ac23b":"markdown","829bb9ab":"markdown"},"source":{"fb23f43f":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","672acaaa":"data = pd.read_csv('\/kaggle\/input\/heart-disease\/heart.csv')\ndata.head()","384ac43c":"data.describe()","b40a09aa":"data.isnull().values.any()","ea599df8":"import seaborn as sns","12701dc0":"_ = data.corr()\nsns.heatmap(_, cmap=sns.diverging_palette(150, 275, s=80, l=55, n=9))\n#Not the prettiest plot but I'm using a diverging colour pallette so \n#that its easier to pick out the values closest to 0 around the target","a7fb295e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nX = data.drop(['target'], axis=1)\ny = data['target']","3433e603":"train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=.2)","98e022de":"from sklearn.ensemble import GradientBoostingRegressor\n\nest_range = range(5, 105, 5)\nscore_graph = {'ROC AUC Score': [], 'n_estimators': []}\nest_list = []\nscore_list = []\nfor num in est_range:\n    gbr = GradientBoostingRegressor(n_estimators=num, random_state=0)\n    gbr.fit(train_X, train_y)\n    ls_preds = gbr.predict(val_X)\n    acc = roc_auc_score(val_y, ls_preds)\n    print('ROC AUC Score with',num ,'estimators is: ', acc)\n    score_graph['ROC AUC Score'] = score_graph['ROC AUC Score'] + [acc]\n    score_graph['n_estimators'] = score_graph['n_estimators'] + [num]","1a61c459":"sns.lineplot(x=score_graph['n_estimators'], y=score_graph['ROC AUC Score'])\n\n# When selecting an optimal parameter where there is a plateau\n# I tend to pick the side of the plateau that has the lowest complexity.\n# This helps prevent overfitting","08e728db":"import warnings\nwarnings.filterwarnings(\"ignore\")","d7a07c33":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nmy_model = GradientBoostingRegressor(n_estimators=35).fit(train_X, train_y)\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","15ee766f":"feat = ['ca', 'oldpeak', 'thal']\nX = data[feat]\ny = data['target']\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 1, test_size=.2)","5547ca74":"gbr = GradientBoostingRegressor(n_estimators=35, random_state=0)\ngbr.fit(train_X, train_y)\nls_preds = gbr.predict(val_X)\nprint('ROC AUC Scoree is: ', roc_auc_score(val_y, ls_preds))","5fbd12da":"est_range = range(5, 105, 5)\nscore_graph = {'ROC AUC Score': [], 'n_estimators': []}\nest_list = []\nscore_list = []\n# A coder with some forsight might have just created a function for this instead of copy pasting my prior code\nfor num in est_range:\n    gbr = GradientBoostingRegressor(n_estimators=num, random_state=0)\n    gbr.fit(train_X, train_y)\n    ls_preds = gbr.predict(val_X)\n    acc = roc_auc_score(val_y, ls_preds)\n    print('ROC AUC Score with',num ,'estimators is: ', acc)\n    score_graph['ROC AUC Score'] = score_graph['ROC AUC Score'] + [acc]\n    score_graph['n_estimators'] = score_graph['n_estimators'] + [num]\nsns.lineplot(x=score_graph['n_estimators'], y=score_graph['ROC AUC Score'])","42294a68":"gbr = GradientBoostingRegressor(n_estimators=5, random_state=0)\ngbr.fit(train_X, train_y)\nls_preds = gbr.predict(val_X)\nprint('ROC AUC Scoree is: ', roc_auc_score(val_y, ls_preds))","b70daada":"from sklearn.metrics import confusion_matrix\n\ndef rounder(num, thresh=0.5):\n    if num >= thresh:\n        return 1\n    else:\n        return 0","7895ca3c":"rounding = pd.Series([rounder(x,.2) for x in ls_preds])\nconfuse = confusion_matrix(val_y, rounding)\nprint(confuse[[1],[0]])","d6ce9c2d":"import matplotlib\nfrom matplotlib.pyplot import figure\nfalse_negitives = []\nthreshholds = []\nfor num in range(1, 10, 1):\n    n = num\/10\n    rounding = pd.Series([rounder(x,n) for x in ls_preds])\n    print('Accuracy with threshold set to',n ,'is: ', accuracy_score(val_y, rounding))\n    confuse = confusion_matrix(val_y, rounding)\n    false_negitives = false_negitives + [int(confuse[[1],[0]])]\n    threshholds = threshholds + [n]\n    confuse = pd.DataFrame(confuse)\n    figure(num=None, figsize=(5, 5))\n    sns.heatmap(confuse, linewidths=1,annot=True, fmt='.5g', annot_kws={\"size\": 12},cmap=\"YlGnBu\", \n                yticklabels=['Negitive', 'Positive'], xticklabels=['Negitive Predicted', 'Positive Predicted']).set_title('Threshhold set to: '+str(n))","32ad0670":"# this plots the rate of false negitives as we increase the \n# threshholds for when a person is determined to have heart deasease \nsns.lineplot(y=false_negitives, x=threshholds)","a0b946fd":"# Step 5: Classification thresholds\n\nUp to this point we've been using the area under curve for assessing the probabilities that the model assessed to each individual. Now to change tracks to predict which patients will have heart disease. This time I'm going to loop through and see how high we can set the threshold before we start seeing false negatives (where someone with heart disease would be told they are fine).\n\nWhile not strictly speaking the goal of the initial task, one of the metrics that matters most to me is if the model would tell someone they are fine, only to have a heart attack occur (basically I'm looking for an accurate yet pessimistic model) ","32819b62":"\n\n# Step Three: feature importance\n\nWorking backwards from a sufficient model now we will use Permutation Importance to determine which features are adding signal and which are adding noise.","6dbbc94f":"# Task: Predict Patient Illness\n\nThe purpose of this Task is to predict if a patient has heart disease based on a set of symptoms. As a secondary objective highlight the features which have the greatest impact. As it turns out, this data set was uploaded to Kaggle twice. This submission was initially composed for VolodymyrGavrysh's task, but since it is applicable to another task on the other dataset I will submit it there as well.\n\n\n\nSpecial thanks to:\n* VolodymyrGavrysh for both the task and for uploading the dataset to Kaggle: https:\/\/www.kaggle.com\/volodymyrgavrysh\/heart-disease\n* And also Ronit for posting the dataset on Kaggle as well, https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n* Shoumik for the Task https:\/\/www.kaggle.com\/shoumikgoswami\n\nAnd the principal investigator responsible for the data collection at each institution:\n* Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n* University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n* University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n* V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D.","70a344a2":"As we can see above there are three strong features (ca, oldpeak, and thal). Slope, age, and sex have some relevence as well but not a high enough relevence to confirm that it isn't due to random chance.\n\nPermeation importance works by shuffling the data in a single column, then rerunning the model and evaluating its performance. a higher number indicates a larger positive impact to the model by leaving the datapoint intact. A negative permeation importance means that the model actually improved as a result of the shuffle (so random chance was a better predictor than the original datapoint). If we look at the largest negative of the set, 'exang', its value is -0.03, so that is where I'm drawing my line of feature value.","4ed00867":"# Second Step: model selection\n\nBecause the dataset is this small I'll be experimenting with model complexity before feature selection\/importance. Avoiding overfitting will be the main hurdle for this task.","f5db9d58":"The following I copy\/pasted from the data source\n\n\"\n# Attribute Information:\n\n    Age: Age\n    Sex: Sex (1 = male; 0 = female)\n    ChestPain: Chest pain (typical, asymptotic, nonanginal, nontypical)\n    RestBP: Resting blood pressure\n    Chol: Serum cholestoral in mg\/dl\n    Fbs: Fasting blood sugar > 120 mg\/dl (1 = true; 0 = false)\n    RestECG: Resting electrocardiographic results\n    MaxHR: Maximum heart rate achieved\n    ExAng: Exercise induced angina (1 = yes; 0 = no)\n    Oldpeak: ST depression induced by exercise relative to rest\n    Slope: Slope of the peak exercise ST segment\n    Ca: Number of major vessels colored by flourosopy (0 - 3)\n    Thal: (3 = normal; 6 = fixed defect; 7 = reversable defect)\n    target: AHD - Diagnosis of heart disease (1 = yes; 0 = no)\n\"","fc0b98c7":"As a side note: at this point since we are using fewer features we also need to prune the complexity of the model to prevent overfitting.","821ac23b":"# First steps: simple EDA\n\nCheck the data to make sure its cleaned, encoded, and not containing missing values.","829bb9ab":"# Final Thoughts:\n\nWith the threshold for predicting heart disease set to .5 (so the model initially predicted a 50% chance or greater to produce a positive outcome) we have 0 false positive cases and an accuracy of 88%. It's not perfect, but I'm absolutely satisfied with those results.\n\nI would have liked to get a more accurate model, but due to the limited number of cases I probably was overly cautious of overfitting. I also probably could have used grid search to squeeze a bit more performance out of the model. Furthermore, the data that's needed to supply this model, and effort needed to prepare the data, almost makes it easier just to have a physician evaluate the patient. The original data set has a few more features in it, and I wonder if it would have been worth while to source the added data or if it would have symply created too much noise.\n\nAs one other side note, many of the techniques I chose to use in this notebook scale poorly. So having a small dataset to start with isn't always a bad thing. Alternatively, some of the loops I used could be used on a subset of a larger dataset.\n\nThis year I've challenged myself to complete one task on Kaggle per week, in order to develop a larger Data Science portfolio. If you found this notebook useful or interesting please give it an upvote. I'm always open to constructive feedback. If you have any questions, comments, concerns, or if you would like to collaborate on a future task of the week feel free to leave a comment here or message me directly.\n"}}