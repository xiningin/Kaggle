{"cell_type":{"733b2751":"code","bb9c1232":"code","a3a32a9c":"code","e7a7ce03":"code","40c65479":"code","82627706":"code","b23b463a":"code","210deafc":"code","5ef0a333":"code","b42a12b6":"code","901af8aa":"code","ee9a8c5b":"code","a5add1dd":"code","a1615a14":"code","e749a98d":"code","3c31f893":"code","5c54f5b8":"markdown","b5e9d3b5":"markdown","577457ec":"markdown","220b07ea":"markdown"},"source":{"733b2751":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport numpy as np\nimport pandas as pd\nimport spacy\nimport string\nnlp = spacy.load(\"en_core_web_lg\")","bb9c1232":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn import metrics, svm","a3a32a9c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e7a7ce03":"full_train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nfull_submission_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nfull_train_data.head()","40c65479":"train_data_shape = full_train_data.shape[0]","82627706":"sns.countplot(full_train_data['target'])","b23b463a":"df = pd.concat([full_train_data, full_submission_data])\ndf.shape","210deafc":"def clean_text(text):\n    # remove_URL\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text =  url.sub(r'', text)\n\n    # remove_html\n    html = re.compile(r'<.*?>')\n    text = html.sub(r'', text)\n\n    # remove_emoji\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags = re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n\n    # remove_punct\n    table = str.maketrans('', '', string.punctuation)\n    text = text.translate(table)\n    \n    return text\n\n\ndf['text'] = df['text'].apply(lambda x : clean_text(x))","5ef0a333":"train_data = df.copy().iloc[:train_data_shape]\nsubmission_data = df.copy().iloc[train_data_shape:]","b42a12b6":"X = train_data.copy().drop(['target'], axis=1)\ny = train_data.copy()['target']\nX_sub = submission_data.copy().drop(['target'], axis=1)","901af8aa":"class SpacyVectorTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, nlp):\n        self.nlp = nlp\n        self.dim = 300\n\n    def fit(self, X, y):\n        return self\n\n    def transform(self, X):\n        return np.array([self.nlp(text).vector for text in X])","ee9a8c5b":"column_preprocessor = ColumnTransformer(\n    [\n        ('text_glove', SpacyVectorTransformer(nlp), 'text'),\n    ],\n    remainder='drop',\n    n_jobs=1\n)","a5add1dd":"pipeline = Pipeline([\n    ('column_preprocessor', column_preprocessor),\n    ('svm', svm.SVC(kernel='rbf', C=1.2, gamma=0.2))\n])","a1615a14":"print(\"start pipeline fit\")\npipeline.fit(X, y)","e749a98d":"# Make predictions using the trained model\npredictions = pipeline.predict(X_sub)\npredictions = [int(i) for i in predictions]","3c31f893":"# # Generate results\nresults = pd.Series(predictions, name=\"target\")\ntweet_ids = full_submission_data['id']\n\nsubmission = pd.concat([tweet_ids, results], axis=1)\nprint(submission.head())\nsubmission.to_csv(\"submission.csv\",index=False)","5c54f5b8":"## Data Cleaning","b5e9d3b5":"## Data Preparation","577457ec":"## Training","220b07ea":"This is a simple solution that uses the Scikit-learn library and a Custom Class of Glove Embeddings to achieve quite good results. I have also created a [companion tutorial](https:\/\/www.sunnyville.ai\/posts\/custom-class-glove-embeddings-sklearn-pipeline\/) at my website to explain some of the details of the solution, which you may find helpful. "}}