{"cell_type":{"8445738f":"code","356df8bf":"code","2a269963":"code","110b1f25":"code","2e7b6409":"code","35aa101c":"code","84a38bcf":"code","91f6ccf0":"code","99f6c3bb":"code","98519502":"code","c3dc27fc":"code","d5c6d62b":"code","b200a3aa":"code","2962ad4a":"code","85be2210":"code","3f478f95":"code","93ddff83":"code","acb46fe3":"code","cda6157d":"code","b94a3750":"code","2630301a":"code","bec4a723":"code","9cbd6a34":"code","11d26907":"code","6e55032c":"code","b5c62cf9":"code","d7fa299d":"code","5d780ed0":"code","31950011":"code","243fc50e":"code","b4043431":"code","0b1df5df":"code","0ad4db68":"code","cfad02fb":"code","0d062c76":"code","f5975c89":"code","26b535c5":"code","76604734":"code","51755e24":"code","610a64df":"code","d8a0ef6c":"code","0d355d27":"code","e2412c58":"code","1f530a3e":"code","7e5ae687":"code","efbb0dee":"code","234f3602":"code","85467d15":"code","4ed62ad4":"code","3996f334":"code","21cf8072":"markdown","4c5def27":"markdown","b1741a40":"markdown","8c10e033":"markdown","ac7e900e":"markdown","481d0adc":"markdown","ec48e710":"markdown","7cfadaaa":"markdown","9dd22da4":"markdown","e3aab1c8":"markdown","f8a47d64":"markdown","9c09f457":"markdown","9fa4896b":"markdown","9a49fdde":"markdown","8f8227fa":"markdown","be473057":"markdown","b6535279":"markdown","ee82e13a":"markdown","5b5d120a":"markdown","8d511254":"markdown","5ce4c092":"markdown","2f243393":"markdown","8bc14c30":"markdown","1aa69683":"markdown","df918d38":"markdown","852378e2":"markdown","76d5f235":"markdown","2daf914e":"markdown","352e8202":"markdown","d6564f16":"markdown","3c1eb375":"markdown","ebab9b41":"markdown","851c91ea":"markdown","a43657bd":"markdown","558e67d7":"markdown","e03a6693":"markdown","20b01ac1":"markdown","45229225":"markdown","cb0783ef":"markdown","49f232ca":"markdown","9f1aec3a":"markdown","87f541e1":"markdown","51fba4f0":"markdown","844f92d9":"markdown","293fd726":"markdown"},"source":{"8445738f":"import sys\nfrom os import walk\nfrom os.path import join\nimport numpy as np\nimport pandas as pd\nimport itertools","356df8bf":"import matplotlib.pyplot as plt\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud\nfrom PIL import Image","2a269963":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score,accuracy_score","110b1f25":"DATA_URL = \"\/kaggle\/input\/\"\nDATA_FILE = DATA_URL + \"clickbait-dataset\/clickbait_data.csv\"\nBAIT_IMG = DATA_URL + \"bait-img\/bait_inv.png\"\nMODEL_FILE = \"\/kaggle\/working\/clickbait-dataset\/saved_model.csv\"\nTEST_SAMPLE = 0.15","2e7b6409":"data = pd.read_csv(DATA_FILE)\nn_entries = len(data)\nprint(f\"The dataset consists of {n_entries} entries\")","35aa101c":"# entries are null?\ndata[\"headline\"].isnull().values.any()","84a38bcf":"# empty entries\n(data[\"headline\"].str.len() == 0).any()","91f6ccf0":"data = shuffle(data)\ndata.head()","99f6c3bb":"data[\"clickbait\"].value_counts()","98519502":"target = data[\"clickbait\"]\nfeatures = data.drop(\"clickbait\", axis=1)\nx_train, x_test, y_train, y_test = train_test_split(features, target, test_size=TEST_SAMPLE)","c3dc27fc":"print(f\"Length of the test dataset: {len(x_test)}\")\nprint(f\"Length of the train dataset: {len(x_train)}\")","d5c6d62b":"y_train.value_counts()","b200a3aa":"nltk.download(\"stopwords\")\nnltk.download('punkt')\ndef clean_message(message, stemmer=PorterStemmer(), stop_words=set(stopwords.words())):\n  #lowercase\n  message = message.lower()\n  #tokenize\n  token_msg = word_tokenize(message)\n  #remove stop words, punctuation, stemming\n  filtered_words = []\n  for word in token_msg:\n    if (word not in stop_words) and word.isalpha():\n      filtered_words.append(stemmer.stem(word))\n  return filtered_words","2962ad4a":"print(f\"Headline: {x_train.iloc[4].headline}\")\nprint(f\"Word list: {clean_message(x_train.iloc[4].headline)}\")","85be2210":"%%time\nnested_list = x_train.headline.apply(clean_message)","3f478f95":"nested_list.head()","93ddff83":"doc_ids_clickbait = x_train[y_train == 1].index\ndoc_ids_legit = x_train[y_train == 0].index\nnested_list_clickbait = nested_list.loc[doc_ids_clickbait]\nnested_list_legit = nested_list.loc[doc_ids_legit]","acb46fe3":"flat_list_clickbait = [item for sublist in nested_list_clickbait for item in sublist]\nflat_list_legit = [item for sublist in nested_list_legit for item in sublist]\nlegit_words = pd.Series(flat_list_legit).value_counts()\nclickbait_words = pd.Series(flat_list_clickbait).value_counts()","cda6157d":"print(f\"Unique words in clickbait headlines {clickbait_words.shape[0]}\")\nprint(f\"Unique words in legit headlines {legit_words.shape[0]}\")","b94a3750":"print(\"Most common words in clickbait headlines\")\nclickbait_words[:10]","2630301a":"print(\"Most common words in legit headlines\")\nlegit_words[:10]","bec4a723":"icon = Image.open(BAIT_IMG)\nimage_mask = Image.new(mode=\"RGB\", size=icon.size, color=(255, 255, 255))\nimage_mask.paste(icon, box=icon)\nrgb_array = np.array(image_mask)\nbait_str = ' '.join(flat_list_clickbait).upper()\nword_cloud =  WordCloud(mask=rgb_array, background_color=\"black\", max_words=1000, colormap=\"Accent\")\nword_cloud.generate(bait_str)\nplt.figure(figsize=[26,16])\nplt.axis(\"off\")\nplt.imshow(word_cloud, interpolation=\"bilinear\")\nplt.show()","9cbd6a34":"EXAMPLE_HL = \"Doctors hate him! Lose weight with 10 easy steps.\"","11d26907":"clean_message(EXAMPLE_HL)","6e55032c":"doctor_in_clickbait = clickbait_words[\"doctor\"]\ndoctor_in_legit = legit_words[\"doctor\"]\np_doctor_if_bait = doctor_in_clickbait \/ y_train.value_counts(ascending=True)[1]\nprint(f\"P(doctor|bait) = {p_doctor_if_bait}\")","b5c62cf9":"p_clickbait = y_train.value_counts(ascending=True)[1] \/ len(x_train)\nprint(f\"P(bait) = {p_clickbait}\")","d7fa299d":"p_doctor = (doctor_in_clickbait + doctor_in_legit) \/ len(x_train)\nprint(f\"P(doctor) = {p_doctor}\")","5d780ed0":"p_bait_if_doctor = p_doctor_if_bait * p_clickbait \/ p_doctor\nprint(f\"P(bait|doctor) = {p_bait_if_doctor}\")","31950011":"compound_prob = 1\nfor word in clean_message(EXAMPLE_HL):\n    try: \n        word_in_clickbait = clickbait_words[word]\n    except KeyError:\n        word_in_clickbait = 0\n    try: \n        word_in_legit = legit_words[word]\n    except KeyError:\n        word_in_legit = 0\n    p_word_if_bait = word_in_clickbait \/ y_train.value_counts(ascending=True)[1]\n    p_word = (word_in_clickbait + word_in_legit) \/ len(x_train)\n    p_bait_if_word = p_word_if_bait * p_clickbait \/ p_word\n    compound_prob = compound_prob * p_bait_if_word\n    print(f\"P(bait|{word}) =  \\t{p_bait_if_word}\")\nprint(f\"\\nP(this is bait) =  \\t{compound_prob}\")","243fc50e":"compound_prob = 1\nfor word in clean_message(EXAMPLE_HL):\n    try: \n        word_in_clickbait = clickbait_words[word]\n    except KeyError:\n        word_in_clickbait = 0\n    try: \n        word_in_legit = legit_words[word]\n    except KeyError:\n        word_in_legit = 0\n    p_word_if_legit = word_in_legit \/ y_train.value_counts(ascending=True)[1]\n    p_word = (word_in_clickbait + word_in_legit) \/ len(x_train)\n    p_legit_if_word = p_word_if_legit * (1-p_clickbait) \/ p_word\n    compound_prob = compound_prob * p_legit_if_word\n    print(f\"P(legit|{word}) =  \\t{p_legit_if_word}\")\nprint(f\"\\nP(this is legit) =  \\t{compound_prob}\")","b4043431":"clickbait_score = 0\nfor word in clean_message(EXAMPLE_HL):\n    try: \n        word_in_clickbait = clickbait_words[word]\n    except KeyError:\n        word_in_clickbait = 0\n    try: \n        word_in_legit = legit_words[word]\n    except KeyError:\n        word_in_legit = 0\n    p_word_if_bait = word_in_clickbait \/ y_train.value_counts(ascending=True)[1]\n    p_word = (word_in_clickbait + word_in_legit) \/ len(x_train)\n    p_bait_if_word = p_word_if_bait * p_clickbait \/ p_word\n    clickbait_score = clickbait_score + p_bait_if_word\n    print(f\"P(bait|{word}) =  \\t{p_bait_if_word}\")\nprint(f\"\\nclickbait score =  \\t{clickbait_score}\")","0b1df5df":"legit_score = 0\nfor word in clean_message(EXAMPLE_HL):\n    try: \n        word_in_clickbait = clickbait_words[word]\n    except KeyError:\n        word_in_clickbait = 0\n    try: \n        word_in_legit = legit_words[word]\n    except KeyError:\n        word_in_legit = 0\n    p_word_if_legit = word_in_legit \/ y_train.value_counts(ascending=True)[1]\n    p_word = (word_in_clickbait + word_in_legit) \/ len(x_train)\n    p_legit_if_word = p_word_if_legit * (1-p_clickbait) \/ p_word\n    legit_score = legit_score + p_legit_if_word\n    print(f\"P(legit|{word}) =  \\t{p_legit_if_word}\")\nprint(f\"\\nlegit score =  \\t{legit_score}\")","0ad4db68":"certainty = round(clickbait_score \/ (clickbait_score + legit_score) * 100, 3)\nprint(f\"The headline is a clickbait with a {certainty}% confidence\")","cfad02fb":"clickbait_words_set = set(clickbait_words.index)\nlegit_words_set = set(legit_words.index)\nall_words_list = list(set.union(clickbait_words_set, legit_words_set))\nprint(f\"Unique words: {len(all_words_list)}\")","0d062c76":"clickbait_occur = []\nlegit_occur = []\ntotal_occur = []\nfor word in all_words_list:\n\n    a = legit_words.get(key=word)\n    b = clickbait_words.get(key=word)\n\n    if a == None:\n        a = 0\n    legit_occur.append(a)\n    if b == None:\n        b = 0\n    clickbait_occur.append(b)\n    total_occur.append(a + b)\n\ntoken_df = pd.DataFrame(list(zip(all_words_list, clickbait_occur, legit_occur, total_occur)), columns =['word', 'clickbait_occur', 'legit_occur', 'total_occur'])","f5975c89":"token_df.head()","26b535c5":"clickbait_occur_total = token_df[\"clickbait_occur\"].sum()\nlegit_occur_total = token_df[\"legit_occur\"].sum()\ntotal_occur_total = clickbait_occur_total + legit_occur_total","76604734":"token_df[\"P_word_if_bait\"] = token_df[\"clickbait_occur\"] \/ clickbait_occur_total\ntoken_df[\"P_word_if_legit\"] = token_df[\"legit_occur\"] \/ legit_occur_total\ntoken_df[\"P_word\"] = token_df[\"total_occur\"] \/ total_occur_total","51755e24":"token_df.head()","610a64df":"token_df = token_df.drop([\"clickbait_occur\", \"legit_occur\", \"total_occur\"], axis=1)\ntoken_df.head()","d8a0ef6c":"# token_df.to_csv(MODEL_FILE)","0d355d27":"def is_clickbait(headline):\n    words = clean_message(headline)\n    clickbait_percentage = legit_percentage = 1\n    for word in words:\n        slice_df = token_df[token_df[\"word\"] == word]\n        if slice_df.empty == False:\n            P_word_if_bait = float(slice_df[\"P_word_if_bait\"])\n            P_word_if_legit\t= float(slice_df[\"P_word_if_legit\"])\n            P_word = float(slice_df[\"P_word\"])\n            clickbait_percentage += P_word_if_bait * p_clickbait \/ P_word\n            legit_percentage += P_word_if_legit * (1 - p_clickbait) \/ P_word\n    clickbait_chance = round(clickbait_percentage \/ (clickbait_percentage + legit_percentage) * 100, 3)\n    return int(clickbait_percentage > legit_percentage), clickbait_chance","e2412c58":"verdict, confidence = is_clickbait(EXAMPLE_HL)\nprint(f\"Is it clickbait? {bool(verdict)} \\t Confidence level: {confidence}%\")","1f530a3e":"y_test = np.array(y_test)","7e5ae687":"%%time\nprediction = np.array([])\nfor index, row in x_test.iterrows():\n    headline = row[\"headline\"]\n    verdict, certainty = is_clickbait(headline)\n    prediction = np.append(prediction, verdict)","efbb0dee":"test_match_answer = np.equal(prediction, y_test)\nunique_counts = np.unique(test_match_answer, return_counts=True)","234f3602":"result_df = pd.DataFrame(unique_counts[0], columns=[\"match\"])\nresult_df[\"count\"] = unique_counts[1]\nresult_df","85467d15":"guessed_right = int(result_df[result_df[\"match\"] == True][\"count\"])\nguessed_wrong = int(result_df[result_df[\"match\"] == False][\"count\"])\ncategory_names = [\"Guessed right\", \"Guessed wrong\"]\nsizes = [guessed_right, guessed_wrong]\ncustom_colors = [\"#70ef63\", \"#ff7675\"]\nplt.figure(figsize=[3,3], dpi=150)\nplt.pie(sizes, labels=category_names, textprops={\"fontsize\": 8}, \n        startangle=90, autopct=\"%1.0f%%\", colors=custom_colors, pctdistance=0.8)\n# draw circle\ncentre_circle = plt.Circle((0, 0), radius=0.6, fc=\"white\")\nplt.gca().add_artist(centre_circle)\nplt.title(\"Test dataset prediction results\")\nplt.show()","4ed62ad4":"print(f\"F1 score of the model {f1_score(y_test, prediction)}\")","3996f334":"conf_matrix = confusion_matrix(y_true=y_test, y_pred=prediction)\nplt.figure(figsize=[4, 4])\nplt.imshow(conf_matrix, cmap=\"bone_r\")\nplt.title(\"Confusion matrix\", fontsize=16)\nplt.ylabel(\"Actual classes\")\nplt.xlabel(\"Predicted classes\")\ntick_marks = np.arange(2)\nplt.yticks(tick_marks, [0,1])\nplt.xticks(tick_marks, [0,1])\nplt.colorbar()\nfor i, j in itertools.product(range(2), range(2)):\n    plt.text(j, i, conf_matrix[i][j], horizontalalignment=\"center\", \n             color=\"white\" if conf_matrix[i][j] > 200 else \"black\")\nplt.show()","21cf8072":"But this is just valid for a single word, if we want to consider all the key words in the headline, we can multiply the single word probabilities.","4c5def27":"# Exploring the dataset","b1741a40":"Now we can pick a random headline from the train dataset and extract the stemmed key words as a list","8c10e033":"We've seen clickbait headlines promising us to lose weight, earn easy money or listing us 10 easy tricks that will help us immensely in our daily lives, thanks to the sacrifice of somebody who's now hated by doctors.\n\nIt's the presence of certain words and topics what makes possible that a shallow learning algorithm such as Naive-Bayes a perfectly capable way of clasifying a news feed between serious, legitimate journalism and attention-grabbing clickbaits.\n\nThis notebook uses the [Clickbait Dataset](https:\/\/www.kaggle.com\/amananandrai\/clickbait-dataset) by [Aman Anand](https:\/\/www.kaggle.com\/amananandrai)","ac7e900e":"The clickbait score is higher than the legit, the verdict is that this made-up headline would be a clickbait.","481d0adc":"Likewise, the probability of the headline being legit can be calculated the same way","ec48e710":"## Saving the model","7cfadaaa":"# Conclussions","9dd22da4":"## $$P(word_1\\cap word_2) = P(word_1)P(word_2)$$","e3aab1c8":"### Basic libraries","f8a47d64":"We've checked there are words more often present in clickbait newslines, in the top ten for each category there's not a single match, it's the frequency of these words what makes an algorithm like Naive-Bayes very accurate.","9c09f457":"## Using nltk to remove stopwords, stem and tokenize words","9fa4896b":"# Importing the data and libraries","9a49fdde":"## Classifying a single example headline","8f8227fa":"## $$P{(bait|word)} = \\frac{P(word|bait)\\,P(bait)}{P(word)}$$","be473057":"From this data we can calculate the different probababilities and add them as extra columns","b6535279":"The Bayes Theorem sets the probability of a headline being a clickbait considering it contains a certain word as the product of the probability of encountering that word in a clickbait headline by the probability of any headline of being a clickbait, and divided by the raw probability of encountering that word in any headline.","ee82e13a":"We can see legit headlines often contain words associated with the most grim topics, by the high frequency of \"kill\", \"dead\", \"bomb\" and \"crash\".\n\nOn the other hand clickbait headlines have a higher presence of verbs trying to make the reader obtain something, e.g. \"time\", \"make\", \"need\", \"thing\".","5b5d120a":"### ML libraries","8d511254":"For this step we'll choose an stereotypical headline not necessarily present in the dataset","5ce4c092":"## Batch the probability of every word to make the complete model","2f243393":"# Batch the test data results","8bc14c30":"### Visualization and word processing libraries","1aa69683":"After the split the distribution of clickbait and legit headlines is still approximately 50-50","df918d38":"## Explore the tokenized dataset","852378e2":"# Classifying news headlines as clickbait or not using a Naive-Bayes algorithm","76d5f235":"First let's gather all occurences of each word and count them in the different categories","2daf914e":"The key words we need to focus on are:","352e8202":"With this new approach, let's repeat the example and analyze the results","d6564f16":"### Batching the entire train dataset","3c1eb375":"Optionally, we can remove the colums `clickbait_occur`, `legit_occur` and `total_occur` before saving the model, to reduce storage space.\n","ebab9b41":"Now for the entire headline words we can encase the word list in a loop. Multiplying all the individual probabilities will yield the total probability of the headline being clickbait","851c91ea":"The disparity between scores can be used to extract a confidence lever for the verdict.","a43657bd":"But there's a workaround and it consists of treating all single probabilities as a score, and summing them instead of multiplying, the score for both cases is compared, and the less equal they are, the higher the certainty the bigger one is correct.","558e67d7":"Clickbait headlines are formulaic enough to classify them easily using a simple algorithm like Naive-Bayes\n\nWether they're selling us money, time, or hate, the vocabulary pool clickbait headlines use is very different from the vocabulary pool used in legit news headlines. In a way, their intentions are their own demise, as a model like this one could be used in a browser extension capable of automatically hiding clickbait articles solely based on the used vocabulary","e03a6693":"The dataset does not contain null or empty entries so there's not cleaning to be done","20b01ac1":"However, this is only valid when the probabilities of word 1 and word 2 are completely independent for each other, which is in most cases not true in a gramatically correct sentence. This is where the \"naive\" part comes in the \"Naive-Bayes\" Algorithm, as it considers all words independent of each other.\n\nThis, which might be seen as a huge limitation is actually a valid assumption, considering the resultic accuracy of the model, as we'll see.","45229225":"## Splitting the dataset into test and train datasets","cb0783ef":"There's an almost equal number of clickbait and legit headlines in the dataset, which is ideal.","49f232ca":"# Using the full model","9f1aec3a":"The final Bayes-Naive model will consist of a table with the following probabilities for each word present in the headlines dataset:\n * $P(word|bait)$\n * $P(word|legit)$\n * $P(word)$","87f541e1":"So the probability of a given headline containing the word \"doctor\" is around 58.62% ","51fba4f0":"### Constants and paths","844f92d9":"Here we can see a limitation of the algorithm with the Naive-Bayes algorithm with the current dataset. The word \"weight\" only appears in clickbait headlines, so the algorithm result for this word to be in a legit headline is 0, and that will inevitably make the whole probability always 0.","293fd726":"# Training a model to classify new headlines into clickbait or not"}}