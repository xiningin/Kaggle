{"cell_type":{"e8fa2ae9":"code","c9ff5f83":"code","74abb76a":"code","d5a1a859":"code","f09261e6":"code","fc141923":"code","a923fd5b":"code","db8eab9d":"code","a7449400":"code","471d155f":"code","14056cfc":"code","6e3058e2":"code","4f606858":"code","501990f1":"code","db19c2c5":"code","056a1741":"code","7e4c8d58":"code","f9cb457a":"code","4862ee16":"code","88b58538":"code","0283a096":"code","f1daa796":"code","d2f52fe6":"code","61479949":"code","135b3a35":"code","13f09b36":"code","80041b03":"code","d6ea5cd6":"code","da02c2b3":"code","aad968bf":"code","d93e16f0":"code","bf8b666b":"code","7755a620":"code","4baefb43":"code","a20c8c45":"code","b5bde932":"code","a5d93a42":"code","35c1608e":"code","950bc12a":"code","f376fa23":"code","ef25bbd1":"markdown","329805ca":"markdown","6839f071":"markdown","fd41e649":"markdown","dda88d08":"markdown","637e6759":"markdown","2da95aa3":"markdown","8ec021d1":"markdown","afe54ab3":"markdown","ba4f014e":"markdown","6e1eaf0d":"markdown","192dc5ee":"markdown","ce8ba821":"markdown","83b9c899":"markdown","f7106dae":"markdown","277ccff6":"markdown","315ff6fb":"markdown","b012caa3":"markdown","f5a9d2a5":"markdown","535134a0":"markdown"},"source":{"e8fa2ae9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9ff5f83":"df=pd.read_csv('\/kaggle\/input\/brain-tumor\/data.csv')","74abb76a":"df.head()","d5a1a859":"df.isnull().sum()","f09261e6":"df.dropna(inplace=True)","fc141923":"df.info()","a923fd5b":"df.columns","db8eab9d":"df['y'].unique()","a7449400":"df.drop(columns=['Unnamed: 0'],axis=1,inplace=True)","471d155f":"from sklearn import preprocessing","14056cfc":"label_encoder = preprocessing.LabelEncoder()\ndf['y']= label_encoder.fit_transform(df['y'])","6e3058e2":"df['y'].unique()","4f606858":"for i in df.columns:\n    for each in df[i].values:\n        \n        if each > 1 or each < -1:\n            df[i] = (df[i] - np.min(df[i]))\/(np.max(df[i]) - np.min(df[i]))\n        else:\n            pass","501990f1":"y = df[\"y\"]\nX = df.drop([\"y\"],axis = 1)","db19c2c5":"from sklearn.model_selection import train_test_split","056a1741":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3,random_state = 0)","7e4c8d58":"print(X_train.shape)\nprint(y_train.shape)","f9cb457a":"from sklearn.metrics import r2_score, mean_squared_error, confusion_matrix, roc_curve, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom collections import Counter\nfrom sklearn.metrics import r2_score","4862ee16":"log_reg=LogisticRegression(random_state=0,max_iter=900)","88b58538":"log_reg.fit(X_train,y_train)","0283a096":"y_pred=log_reg.predict(X_test)","f1daa796":"log_reg.score(X_test,y_test)","d2f52fe6":"params = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\nlr_model = LogisticRegression(random_state = 0)\nlr_cv = GridSearchCV(lr_model,params,cv = 5).fit(X_train,y_train)\nlr_cv.best_params_","61479949":"log_reg2 = LogisticRegression(C = .01,random_state = 0,penalty= 'l2')\nlog_reg2.fit(X_train,y_train)","135b3a35":"log_reg2.predict(X_test)","13f09b36":"log_reg2.score(X_test,y_test)","80041b03":"knn = KNeighborsClassifier(n_neighbors = 2).fit(X_train,y_train)\nknn.score(X_test,y_test)","d6ea5cd6":"tree = DecisionTreeClassifier(random_state = 0).fit(X_train,y_train)\ntree.score(X_test,y_test)","da02c2b3":"params = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50))}\ntree_model = DecisionTreeClassifier(random_state = 0)\ntree_cv = GridSearchCV(tree_model, params, cv = 10, n_jobs = -1).fit(X_train,y_train)\ntree_cv.best_params_","aad968bf":"tree_tuned = DecisionTreeClassifier(max_depth = 1, min_samples_leaf = 7, min_samples_split = 2,random_state = 42).fit(X_train,y_train)\ntree_tuned.score(X_test,y_test)","d93e16f0":"rf = RandomForestClassifier(random_state = 0).fit(X_train,y_train)\nrf.score(X_test,y_test)","bf8b666b":"svm = SVC(random_state = 42).fit(X_train,y_train)\nsvm.score(X_test,y_test)","7755a620":"gbm = GradientBoostingClassifier(random_state = 42).fit(X_train,y_train)\ngbm.score(X_test,y_test)","4baefb43":"gbm_tuned = GradientBoostingClassifier(max_depth = 2, learning_rate = 0.01, min_samples_split = 2, n_estimators = 100, random_state = 42).fit(X_train,y_train)\ngbm_tuned.score(X_test,y_test)","a20c8c45":"ada = AdaBoostClassifier(random_state = 42).fit(X_train,y_train)\nada.score(X_test,y_test)","b5bde932":"ada_tuned = AdaBoostClassifier(learning_rate = 0.01,n_estimators = 1000,random_state = 42).fit(X_train,y_train)\nada_tuned.score(X_test,y_test)","a5d93a42":"bag = BaggingClassifier(random_state = 42).fit(X_train,y_train)\nbag.score(X_test,y_test)","35c1608e":"bag_tuned = BaggingClassifier(n_estimators = 45,random_state = 42).fit(X_train,y_train)\nbag_tuned.score(X_test,y_test)","950bc12a":"\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","f376fa23":"pred_list = [log_reg2,knn,rf,gbm_tuned,svm,ada_tuned,bag_tuned]\n\nfor i in pred_list:\n    print(\"Score : \",i.score(X_test,y_test))\n    y_pred = i.predict(X_test)\n    sns.heatmap(confusion_matrix(y_test,y_pred),annot = True)\n    plt.xlabel(\"Y_pred\")\n    plt.ylabel(\"Y_test\")\n    plt.title(i)\n    plt.show()","ef25bbd1":"# Decision Tree","329805ca":"# Now Drop y column","6839f071":"# Now Model Buildings","fd41e649":"# Random Forest","dda88d08":"# KNN (K Neighbors)","637e6759":"# Bagging","2da95aa3":"# Now Check The Shape of the X_train and y_train","8ec021d1":"# Adaboost","afe54ab3":"# Now Label Encoding for y column","ba4f014e":"# GBM","6e1eaf0d":"# Now Train Test Split","192dc5ee":"# Machine Learning (Classification)","ce8ba821":"1. Logistic Regression\n2. KNN(K Neighbors)\n3. Decision Tree\n4. Random Forests\n5. SVM\n6. GBM (Gradient Boosting Machine)\n7. AdaBoost\n8. Bagging","83b9c899":"# Now Check Unique Value","f7106dae":"# Now Normalization For our dataset","277ccff6":"# Compare Algorithms with Plots","315ff6fb":"# Now Check Nan values","b012caa3":"# SVM","f5a9d2a5":"# Now Drop Unnamed Col","535134a0":"# GridSearchCV"}}