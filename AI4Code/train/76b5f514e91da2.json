{"cell_type":{"3b6c16f6":"code","cdbd3451":"code","fa6c5ec9":"code","6afca8b3":"code","f4fffad1":"code","d188976d":"code","6189c95e":"code","15363dee":"code","1673b2c3":"code","6e8b8cc3":"code","e0852bf1":"code","eb25bf82":"code","cfa4692f":"code","216d91e7":"code","78711f13":"code","338e09fc":"code","004fc211":"code","40cff32d":"code","f3bea239":"code","7fe7b74c":"code","3acbc095":"markdown","5c028b33":"markdown","cdebd8db":"markdown","ac23b53d":"markdown","590565e7":"markdown","2c76897f":"markdown","efc4ac65":"markdown","eaa2ddb1":"markdown","8b1f715d":"markdown"},"source":{"3b6c16f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdbd3451":"data = pd.read_csv(\"\/kaggle\/input\/star-type-classification\/Stars.csv\")","fa6c5ec9":"data.head()","6afca8b3":"data.isnull().sum()","f4fffad1":"data['Color'].value_counts()","d188976d":"top_10 = data['Color'].value_counts()[:9]","6189c95e":"labels_10 = top_10.index\n\nfor label in labels_10:\n    data[label] = np.where(data['Color'] == label,1, 0)\n    \nspectral = data['Spectral_Class'].value_counts().index\n\nfor label in spectral:\n    data[label] = np.where(data['Spectral_Class'] == label, 1, 0)","15363dee":"data = data.drop(['Color'], axis = 1)\ndata = data.drop(['Spectral_Class'], axis = 1)","1673b2c3":"data","6e8b8cc3":"import matplotlib.pyplot as plt\n\nplt.figure(figsize = (15,15))\n\ncolumns = data.columns\n\ndef plot(columns, target):\n    \n    for i in range(len(columns)):\n        plt.subplot(len(columns) \/\/ 3, 3, i+1)\n        plt.scatter(data[columns[i]], data[target])\n        plt.xlabel(columns[i])\n        plt.ylabel(target)\n        plt.title(columns[i] + \"vs\" + target)\n    plt.show()\n    \nplt.subplots_adjust(bottom = 0.01, top = 1.3)\n    \nplot(columns, 'Type')\n    \n    ","e0852bf1":"import tensorflow as tf\nfrom tensorflow import keras","eb25bf82":"all_columns = data.columns\n\nall_columns = all_columns.drop(\"Type\")\n\nprint(all_columns)\n\nnew_data = data[all_columns]\n\nlabels = data['Type']\n\nlabels_array = np.array(labels)\n\ntrain_data = new_data","cfa4692f":"import sklearn\n\nfrom sklearn.model_selection import train_test_split","216d91e7":"train_data, test_data, train_labels, test_labels = train_test_split(new_data, labels_array)\n\ntrain_data","78711f13":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nnorm_train_data = scaler.fit_transform(train_data)\n\nnorm_test_data = scaler.fit_transform(test_data)\n\nnorm_train_data.shape","338e09fc":"model = tf.keras.models.Sequential([\n    \n    tf.keras.layers.Flatten(input_shape = (1, 20)),\n    tf.keras.layers.Dense(20, activation = 'relu'),\n    tf.keras.layers.Dense(6, activation = 'softmax')\n])","004fc211":"model.summary()","40cff32d":"model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.optimizers.Adam(), metrics = ['acc'])","f3bea239":"history = model.fit(norm_train_data, train_labels, epochs = 90, validation_data = (norm_test_data, test_labels))","7fe7b74c":"def plot_history(history, string):\n    \n    plt.plot( history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('string')\n    plt.legend([string, 'val_' + string])\n    plt.show()\n    \nplot_history(history, 'acc')\nplot_history(history, 'loss')","3acbc095":"Now that the categorical variable has been one-hot-encoded, we can drop the 'Color' variable","5c028b33":"First, lets make sure there are no null values","cdebd8db":"lovely! after training for 90 epochs, our validation accuracy is 95%! Thats pretty good!","ac23b53d":"great! Now we only have numerical variables! With this, lets now look at the relationship between each variable and 'type' to determine which variables are actually useful!","590565e7":"don't forget to normalize the data!","2c76897f":"ok, now lets split this data into training and test sets","efc4ac65":"wonderful! we can now see the relationships between different variables and their respective star types!","eaa2ddb1":"lets now one-hot-encode and get the top 10 most frequent Colors in order to remove some of the 'noise'","8b1f715d":"Since there are no null values, we can now map out the 'color' column so that it is a numerical \"float\" type variable"}}