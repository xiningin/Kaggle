{"cell_type":{"8cbd0059":"code","fdfe6419":"code","cec7d5c2":"code","86981e57":"code","727370e4":"code","6e3f394c":"code","afc95213":"code","50590c22":"code","d437dad9":"code","98a0c9c9":"code","0fad1cb0":"code","7fbe7461":"code","0fb69dba":"code","c4d801c5":"code","0c15a818":"code","a9ad5bec":"code","7c2391b9":"code","76e82165":"code","9f8bf611":"code","3462b4c7":"code","02a979b7":"code","f3f34108":"code","ff096848":"code","741e1e0c":"code","5b640034":"code","9fd106ef":"code","2d962aff":"code","dcc33bb0":"code","ed42c3fa":"code","b0c46d73":"code","6288188f":"code","96c3cb21":"code","fc0c455d":"code","0b871007":"code","796046de":"code","25452a22":"code","a1a147e5":"code","bfb8f5e5":"code","83fab42b":"code","0fc56504":"code","3b7a2de9":"code","166a189f":"code","fb197014":"code","9899270b":"markdown","1ced5084":"markdown","07b6d4d5":"markdown","cda5c524":"markdown","b6285202":"markdown","8a081ae8":"markdown","1a81d007":"markdown","b2d80891":"markdown","1f9dc53f":"markdown","9ea056e3":"markdown"},"source":{"8cbd0059":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom time import time\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdfe6419":"train = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv\")","cec7d5c2":"print(\"Train size dataset:\", train.shape)\nprint(\"Test size dataset:\", test.shape)","86981e57":"train.head(5)","727370e4":"test.head(5)","6e3f394c":"# Getting the right index column\n\ntrain = train.set_index(\"id\")\ntest = test.set_index(\"id\")","afc95213":"train[[\"Age\", \"Annual_Premium\"]].describe()","50590c22":"for col in list(train.columns):\n    print(\"Column:\", col, \"- NA value:\", train[col].isna().unique())","d437dad9":"for col in list(test.columns):\n    print(\"Column:\", col, \"- NA value:\", test[col].isna().unique())","98a0c9c9":"train.groupby(\"Response\").size()","0fad1cb0":"train.skew()","7fbe7461":"# Import my favorite visualization lib\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\nimport seaborn as sns","0fb69dba":"grid = sns.FacetGrid(train, col=\"Response\", palette=\"tab20c\", height=6)\ngrid.map_dataframe(sns.countplot, \"Gender\")\ngrid.add_legend()","c4d801c5":"grid = sns.FacetGrid(train, col=\"Vehicle_Age\", row=\"Gender\", hue=\"Response\",  palette=\"tab20c\", height=6, legend_out=True)\ngrid.map_dataframe(sns.countplot, \"Age\")\ngrid.add_legend()","0c15a818":"grid = sns.FacetGrid(train, col=\"Previously_Insured\", row=\"Gender\", hue=\"Response\", palette=\"tab20c\", height=6, legend_out=True)\ngrid.map_dataframe(sns.countplot, \"Age\")\ngrid.add_legend()","a9ad5bec":"sns.jointplot(data=train, x=\"Age\", y=\"Annual_Premium\", kind=\"scatter\")","7c2391b9":"# Taking a look on the categorical values on train dataset\n\nfor col in train[[\"Gender\", \"Vehicle_Age\", \"Vehicle_Damage\"]]:\n    print(\"Column:\",col, \"\\nItems number:\", len(train[col].unique()), \"\\nItems:\", train[col].unique().tolist(), \"\\n\")","76e82165":"# Taking a look on the categorical values on test dataset\n\nfor col in test[[\"Gender\", \"Vehicle_Age\", \"Vehicle_Damage\"]]:\n    print(\"Column:\",col, \"\\nItems number:\", len(test[col].unique()), \"\\nItems:\", test[col].unique().tolist(), \"\\n\")","9f8bf611":"# Transforming string data into numeric data on train dataset\n\ntrain[\"Gender\"] = train[\"Gender\"].map(lambda s: 1 if (s == \"Male\") else 0)\n\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].map(lambda s: 1 if (s == \"Yes\") else 0)\n\ntrain[\"Vehicle_Age\"] = train[\"Vehicle_Age\"].map(lambda s: 3 if (s == \"> 2 Years\") else (2 if (s == \"1-2 Year\") else 1))\n\ntrain.head(5)","3462b4c7":"# Transforming string data into numeric data on test dataset\n\ntest[\"Gender\"] = test[\"Gender\"].map(lambda s: 1 if (s == \"Male\") else 0)\n\ntest[\"Vehicle_Damage\"] = test[\"Vehicle_Damage\"].map(lambda s: 1 if (s == \"Yes\") else 0)\n\ntest[\"Vehicle_Age\"] = test[\"Vehicle_Age\"].map(lambda s: 3 if (s == \"> 2 Years\") else (2 if (s == \"1-2 Year\") else 1))\n\ntest.head(5)","02a979b7":"correlations_train = train.corr()\n#correlations_train.head(15)\n\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(correlations_train, annot=True, linewidths=.5, fmt=\".2g\")","f3f34108":"# Normalization on train dataset\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nselected_cols = [\"Age\", \"Annual_Premium\", \"Vintage\"]\n\nnormalize = MinMaxScaler(feature_range = (0, 1))\n\nfor col in selected_cols:\n    train[col] = normalize.fit_transform(train[[col]])\n    \ntrain.head(5)","ff096848":"# Normalization on test dataset\nfor col in selected_cols:\n    test[col] = normalize.fit_transform(test[[col]])\n    \ntest.head(5)","741e1e0c":"# Feature engineering\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfeature_selection = ExtraTreesClassifier()\nfeature_selection.fit(train.iloc[:,0:10], train.iloc[:,10])\n\n#print(train.columns[0:10])\n#print(feature_selection.feature_importances_)\n\nfor item in range(len(feature_selection.feature_importances_)):\n    print(\"Feature:\", train.columns[item], \"- Score:\", feature_selection.feature_importances_[item])","5b640034":"# Droping low score correlation columns\n\ntrain = train.drop(columns=[\"Driving_License\"])\ntest = test.drop(columns=[\"Driving_License\"])","9fd106ef":"from sklearn.model_selection import RandomizedSearchCV\n\n# Setting the X and Y variables\nx_train = train.iloc[:,0:9]\ny_train = train.iloc[:,9]\n\nx_test = test.iloc[:,0:9]\n\n# Func to print the best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results[\"rank_test_score\"] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(results[\"mean_test_score\"][candidate], results[\"std_test_score\"][candidate]))\n            print(\"Parameters: {0}\".format(results[\"params\"][candidate]))","2d962aff":"# Setting up the cross-validation\n\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(10, shuffle=True)","dcc33bb0":"# Logistic Regression\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\nlog_regress_model = LogisticRegression()\n\nlr_result_cross_val = cross_val_score(log_regress_model, x_train, y_train, cv = kfold, scoring = \"accuracy\")\n\nprint(\"Logistic Regression accuracy: %.2f%%\" % (lr_result_cross_val.mean() * 100))","ed42c3fa":"from sklearn.model_selection import RandomizedSearchCV\n\nrandom_lg_params = {\n    \"penalty\":[\"l1\", \"l2\"],\n    \"C\":[0.1, 1, 2, 5, 10, 15, 25, 50, 75, 100, 150, 500, 1000],\n    \"class_weight\":[\"balanced\", None],\n    \"solver\":[\"newton-cg\", \"lbfgs\", \"sag\", \"saga\"],\n    \"max_iter\":[50, 100, 150, 500, 1000],\n    \"multi_class\":[\"ovr\", \"multinomial\"]\n}\n\nlog_regress_best_params = RandomizedSearchCV(estimator = log_regress_model, param_distributions = random_lg_params, n_iter = 10, cv = kfold, verbose= 1, random_state= 101, n_jobs = -1)","b0c46d73":"log_regress_best_params.fit(x_train, y_train)","6288188f":"report(results=log_regress_best_params.cv_results_)","96c3cb21":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\n\nknn_cross_val = cross_val_score(knn_model, x_train, y_train, cv = kfold, scoring = \"accuracy\")\n\nprint(\"KNN accuracy: %.2f\" % (knn_cross_val.mean() * 100))","fc0c455d":"random_knn_params = {\n    \"n_neighbors\":[3,4,5,8,10,15,20,30],\n    \"weights\":[\"uniform\", \"distance\"],\n    \"algorithm\":[\"ball_tree\", \"kd_tree\", \"brute\"],\n}\n\nknn_best_params = RandomizedSearchCV(estimator = knn_model, param_distributions = random_knn_params, n_iter = 10, cv = kfold, verbose= 1, random_state= 101, n_jobs = -1)","0b871007":"knn_best_params.fit(x_train, y_train)","796046de":"report(results=knn_best_params.cv_results_)","25452a22":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier()","a1a147e5":"random_rf_params = {\n    \"n_estimators\":[5, 10, 100, 300, 500, 1000],\n    \"criterion\":[\"gini\", \"entropy\"],\n    \"max_features\":[\"auto\", \"sqrt\", \"log2\", None],\n    \"min_samples_split\":[2, 4, 6, 8, 10],\n    \"min_samples_leaf\":[1, 2, 4, 6, 8],\n    \"min_weight_fraction_leaf\":[0, 1, 2, 4, 6, 8, 10],\n    \"max_leaf_nodes\":[1, 2, 4, 8, 16, 32],\n    \"max_depth\": [2,3,4,5,6,7,10]\n}\n\nrf_best_params = RandomizedSearchCV(estimator = rfc_model, param_distributions = random_rf_params, n_iter = 10, cv = kfold, verbose = 1, random_state = 101, n_jobs = -1)","bfb8f5e5":"rf_best_params.fit(x_train, y_train)","83fab42b":"report(results=rf_best_params.cv_results_)","0fc56504":"from xgboost import XGBClassifier\n\nxgb_model = XGBClassifier()","3b7a2de9":"xgb_best_params = {\n    \"max_depth\":[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n    \"n_estimators\":[10, 50, 100, 250, 500, 850, 1000],\n    \"subsample\":[i for i in np.arange(0.1, 1.1, 0.1)],\n    \"eta\":[0.0001, 0.001, 0.01, 0.1, 1.0],\n    \"colsample_bytree\":[i for i in np.arange(0.1, 1.1, 0.1)]\n}\n\nxgb_best_params = RandomizedSearchCV(estimator = xgb_model, param_distributions = xgb_best_params, n_iter = 10, cv = kfold, verbose = 1, random_state = 101, n_jobs = -1)","166a189f":"xgb_best_params.fit(x_train, y_train)","fb197014":"report(results=xgb_best_params.cv_results_)","9899270b":"# ML Models","1ced5084":"**Data Dictionary**\n\n* Gender -  Male: 1 | Female: 0\n* Vehicle_Damage -  Yes: 1 | No: 0\n* Vehicle_Age -  > 2 Years: 3 | 1-2 Year: 2 | < 1 Year: 1","07b6d4d5":"**KNN model**","cda5c524":"**Random Forest Classifier model**","b6285202":"Let's take a look on the datasets...","8a081ae8":"**Cross-Validation approach and Randomized Search parameters**","1a81d007":"**XGBoostClassifier model**\n\n*Working in progress here*","b2d80891":"**Logistic Regression Model**","1f9dc53f":"# Data Pre Processing\n\n**Feature transformation and selection:**","9ea056e3":"# Viewing the data..."}}