{"cell_type":{"7709f741":"code","2412c154":"code","20fa8624":"code","2671e42a":"code","b5cb1434":"code","3e3c56d9":"code","45a66387":"code","256aed68":"code","abb04c13":"code","3121a839":"code","81f7d709":"code","3025161c":"code","53dc181e":"code","d7755709":"code","4d09b453":"code","0f021353":"code","befd4a8b":"code","83b4922d":"code","b55b37db":"code","fb5c718e":"code","f66baf38":"code","ea4c2696":"code","80923af4":"code","3624c479":"code","6de38169":"code","6f61a8ec":"code","4abc8245":"code","049c24ba":"code","4e1cfd36":"code","8bfad93c":"code","e1359658":"markdown","f98a50c8":"markdown","b9ce2a33":"markdown","0ed90a7c":"markdown","2a358558":"markdown","4df70987":"markdown","b4d4a813":"markdown","65ea1ea8":"markdown","358f4aeb":"markdown","89a1336b":"markdown","01a4c475":"markdown","1c835b20":"markdown","c7d6acc6":"markdown","7844994d":"markdown","fc506142":"markdown","ae4ffd1f":"markdown","b4d7a438":"markdown","479bb65d":"markdown","7e304504":"markdown","17931e84":"markdown","cc79b276":"markdown","d5344852":"markdown","eb9da16f":"markdown","96f8566e":"markdown","e861b572":"markdown","0fc7a0b1":"markdown","68dc8d8d":"markdown"},"source":{"7709f741":"import os\nimport gc\ngc.enable()\n\nimport sys\nsys.path.append('..\/input\/sentence-transformers\/sentence-transformers-master')","2412c154":"from tqdm import tqdm, trange\nimport pandas as pd\nimport numpy as np\n\nimport xgboost as xgb\nfrom sklearn.linear_model import BayesianRidge\nfrom lightgbm import LGBMRegressor\n\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import (\n    Dataset, DataLoader, \n    SequentialSampler, RandomSampler\n)\nfrom transformers import RobertaConfig\nfrom transformers import (\n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup\n)\nfrom transformers import RobertaTokenizer\nfrom transformers import RobertaModel\n","20fa8624":"FOLDS = 5\nTRAIN_FILE = '..\/input\/commonlitreadabilityprize\/train.csv'\nTEST_FILE = '..\/input\/commonlitreadabilityprize\/test.csv'\nSAMPLE_SUB_FILE = '..\/input\/commonlitreadabilityprize\/sample_submission.csv'\nROBERTA_LARGE_PATH = '..\/input\/robertalarge\/'\nSENTENCE_EMBEDDINGS_MODEL_PATH = '..\/input\/finetuned-model1\/checkpoint-568'","2671e42a":"train = pd.read_csv(TRAIN_FILE)\ntrain.head()","b5cb1434":"test = pd.read_csv(TEST_FILE)\ntest.head()","3e3c56d9":"def rmse(targets, preds):\n    return round(np.sqrt(mean_squared_error(targets, preds)), 4)","45a66387":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2021)\n    for f, (t_, v_) in enumerate(kf.split(X=data)):\n        data.loc[v_, 'kfold'] = f\n    return data","256aed68":"train = create_folds(train, num_splits=5)","abb04c13":"class DatasetRetriever(Dataset):\n    def __init__(self, data, tokenizer, max_len, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.values.tolist()\n        if not is_test:\n            self.targets = self.data.target.values.tolist()\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = max_len\n        \n    def convert_examples_to_features(self, data, tokenizer, max_len, is_test=False):\n        data = data.replace('\\n', '')\n        tok = tokenizer.encode_plus(\n            data, \n            max_length=max_len, \n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=True\n        )\n        curr_sent = {}\n        padding_length = max_len - len(tok['input_ids'])\n        curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n        curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n            ([0] * padding_length)\n        curr_sent['attention_mask'] = tok['attention_mask'] + \\\n            ([0] * padding_length)\n        return curr_sent\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n            excerpt, label = self.excerpts[item], self.targets[item]\n            features = self.convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.double),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = self.convert_examples_to_features(\n                excerpt, self.tokenizer, \n                self.max_len, self.is_test\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","3121a839":"class CommonLitModel(nn.Module):\n    def __init__(\n        self, \n        model_name, \n        config,  \n        multisample_dropout=False,\n        output_hidden_states=False\n    ):\n        super(CommonLitModel, self).__init__()\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            model_name, \n            output_hidden_states=output_hidden_states\n        )\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        self.regressor = nn.Linear(config.hidden_size, 1)\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.regressor)\n \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n \n    def forward(\n        self, \n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        labels=None\n    ):\n        outputs = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        sequence_output = outputs[1]\n        sequence_output = self.layer_norm(sequence_output)\n \n        # multi-sample dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.regressor(dropout(sequence_output))\n            else:\n                logits += self.regressor(dropout(sequence_output))\n        \n        logits \/= len(self.dropouts)\n \n        # calculate loss\n        loss = None\n        if labels is not None:\n            loss_fn = torch.nn.MSELoss()\n            logits = logits.view(-1).to(labels.dtype)\n            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n        \n        output = (logits,) + outputs[1:]\n        return ((loss,) + output) if loss is not None else output","81f7d709":"def make_model(model_name='roberta-large', num_labels=1):\n    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n    config = RobertaConfig.from_pretrained(model_name)\n    config.update({'num_labels':num_labels})\n    model = CommonLitModel(model_name, config=config)\n    return model, tokenizer\n\ndef make_loader(\n    data, \n    tokenizer, \n    max_len,\n    batch_size,\n    fold=0,\n    is_test = False\n):\n    if is_test:\n        test_dataset = DatasetRetriever(data, tokenizer, max_len, is_test=True)\n        test_sampler = SequentialSampler(test_dataset)\n        test_loader = DataLoader(\n            test_dataset, \n            batch_size=batch_size \/\/ 2, \n            sampler=test_sampler, \n            pin_memory=False, \n            drop_last=False, \n            num_workers=0\n        )\n\n        return test_loader\n    else:\n        train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n        train_dataset = DatasetRetriever(train_set, tokenizer, max_len)\n        valid_dataset = DatasetRetriever(valid_set, tokenizer, max_len)\n\n        train_sampler = RandomSampler(train_dataset)\n        train_loader = DataLoader(\n            train_dataset, \n            batch_size=batch_size, \n            sampler=train_sampler, \n            pin_memory=True, \n            drop_last=False, \n            num_workers=4\n        )\n\n        valid_sampler = SequentialSampler(valid_dataset)\n        valid_loader = DataLoader(\n            valid_dataset, \n            batch_size=batch_size \/\/ 2, \n            sampler=valid_sampler, \n            pin_memory=True, \n            drop_last=False, \n            num_workers=4\n        )\n\n        return train_loader, valid_loader\n","3025161c":"class Evaluator:\n    def __init__(self, model, scalar=None):\n        self.model = model\n        self.scalar = scalar\n\n    def evaluate(self, data_loader, tokenizer):\n        preds = []\n        self.model.eval()\n        total_loss = 0\n        with torch.no_grad():\n            for batch_idx, batch_data in enumerate(data_loader):\n                input_ids, attention_mask, token_type_ids = batch_data['input_ids'], \\\n                    batch_data['attention_mask'], batch_data['token_type_ids']\n                input_ids, attention_mask, token_type_ids = input_ids.cuda(), \\\n                    attention_mask.cuda(), token_type_ids.cuda()\n                \n                if self.scalar is not None:\n                    with torch.cuda.amp.autocast():\n                        outputs = self.model(\n                            input_ids=input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids\n                        )\n                else:\n                    outputs = self.model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        token_type_ids=token_type_ids\n                    )\n                \n                logits = outputs[0].detach().cpu().numpy().squeeze().tolist()\n                preds += logits\n        return preds","53dc181e":" def config(fold):\n    torch.manual_seed(2021)\n    torch.cuda.manual_seed(2021)\n    torch.cuda.manual_seed_all(2021)\n    \n    max_len = 250\n    batch_size = 8\n\n    model, tokenizer = make_model(\n        model_name=ROBERTA_LARGE_PATH, \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'..\/input\/roberta-large-itptfit\/model{fold}.bin')\n    )\n    test_loader = make_loader(\n        test, tokenizer, max_len=max_len,\n        batch_size=batch_size, is_test = True\n    )\n\n    if torch.cuda.device_count() >= 1:\n        model = model.cuda() \n    else:\n        raise ValueError('CPU training is not supported')\n\n    # scaler = torch.cuda.amp.GradScaler()\n    scaler = None\n    return (\n        model, tokenizer, \n        test_loader, scaler\n    )","d7755709":"def run(fold=0):\n    model, tokenizer, \\\n        test_loader, scaler = config(fold)\n    \n    import time\n\n    evaluator = Evaluator(model, scaler)\n\n    test_time_list = []\n\n    torch.cuda.synchronize()\n    tic1 = time.time()\n\n    preds = evaluator.evaluate(test_loader, tokenizer)\n\n    torch.cuda.synchronize()\n    tic2 = time.time() \n    test_time_list.append(tic2 - tic1)\n    \n    del model, tokenizer, test_loader, scaler\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return preds","4d09b453":"oof_roberta = np.zeros(len(train))\nfor fold in tqdm(range(5), total=5):\n    model, tokenizer = make_model(\n        model_name='..\/input\/robertalarge\/', \n        num_labels=1\n    )\n    model.load_state_dict(\n        torch.load(f'..\/input\/roberta-large-itptfit\/model{fold}.bin')\n    )\n    model.cuda()\n    model.eval()\n    val_index = train[train.kfold==fold].index.tolist()\n    train_loader, val_loader = make_loader(train, tokenizer, 250, 16, fold=fold)\n    # scalar = torch.cuda.amp.GradScaler()\n    scalar = None\n    val_preds = []\n    for index, data in enumerate(val_loader):\n        input_ids, attention_mask, token_type_ids, labels = data['input_ids'], \\\n            data['attention_mask'], data['token_type_ids'], data['label']\n        input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n            attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n        if scalar is not None:\n            with torch.cuda.amp.autocast():\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    token_type_ids=token_type_ids,\n                    labels=labels\n                )\n        else:\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                labels=labels\n            )\n        \n        loss, logits = outputs[:2]\n        val_preds += logits.cpu().detach().numpy().tolist()\n    oof_roberta[val_index] = val_preds\n    \n    del model, tokenizer, train_loader, val_loader\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint('Roberta: Mean OOF RMSE = {}'.format(rmse(train.target.values, oof_roberta)))","0f021353":"pred_df_roberta = pd.DataFrame()\nfor fold in tqdm(range(5)):\n    pred_df_roberta[f'fold{fold}'] = run(fold)\ntest_preds_roberta = pred_df_roberta.mean(axis=1).values.tolist()\ntest_preds_roberta","befd4a8b":"# setting model path for fine-tuned roberta weights\nmodel_path = SENTENCE_EMBEDDINGS_MODEL_PATH\nword_embedding_model = models.Transformer(model_path, max_seq_length=275)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","83b4922d":"X_train = model.encode(train.excerpt, device='cuda')\ndisplay(X_train.shape)","b55b37db":"X_test = model.encode(test.excerpt, device='cuda')\ndisplay(X_test.shape)","fb5c718e":"%%time\n\nxgb_params = {\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    \n    'eta': 0.05,\n    'max_depth': 3,\n    \n    'gamma': 1,\n    'subsample': 0.8,\n    \n    'nthread': 2\n}\n\nbest_iterations = []\noof_rmses = []\noof_xgboost = np.zeros(len(train))\n\ntest_preds_xgboost = []\n\nfor fold in range(FOLDS):\n    print(f'\\nTraining Fold {fold + 1} \/ {FOLDS}')\n    \n    train_idx, val_idx = train.index[train['kfold']!=fold].tolist(), train.index[train['kfold']==fold].tolist()\n    \n    dtrain = xgb.DMatrix(X_train[train_idx], train.target[train_idx])\n    dvalid = xgb.DMatrix(X_train[val_idx], train.target[val_idx])\n    evals_result = dict()\n    booster = xgb.train(xgb_params,\n                        dtrain,\n                        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n                        num_boost_round=300,\n                        early_stopping_rounds=20,\n                        evals_result=evals_result,\n                        verbose_eval=False)\n    \n    best_iteration = np.argmin(evals_result['valid']['rmse'])\n    best_iterations.append(best_iteration)\n    oof_rmse = evals_result['valid']['rmse'][best_iteration]\n    oof_rmses.append(oof_rmse)\n    print(f\"Fold {fold+1}: train OOF RMSE: {oof_rmse}\")\n    \n    val_pred = booster.predict(dvalid, ntree_limit=int(best_iteration+1))\n    oof_xgboost[val_idx] = val_pred\n    test_preds_xgboost.append(booster.predict(xgb.DMatrix(X_test), ntree_limit=int(best_iteration+1)))\n    \nprint('XGBoost: Mean OOF RMSE = {}'.format(rmse(train.target.values, oof_xgboost)))","f66baf38":"test_preds_xgboost = np.mean(test_preds_xgboost,0)\ntest_preds_xgboost","ea4c2696":"%%time\n\noof_ridge = np.zeros(len(train))\ntest_preds_ridge = []\n\nfor fold in range(FOLDS):\n    print(f'\\nTraining Fold {fold + 1} \/ {FOLDS}')\n    \n    train_idx, val_idx = train.index[train['kfold']!=fold].tolist(), train.index[train['kfold']==fold].tolist()\n\n    reg = BayesianRidge(n_iter=300, verbose=True)\n    reg.fit(X_train[train_idx],train.target[train_idx])\n    \n    \n    val_pred = reg.predict(X_train[val_idx])\n    oof_ridge[val_idx] = val_pred\n    oof_rmse = rmse(val_pred, train.target[val_idx].values)\n    print(f\"Fold {fold+1}: train OOF RMSE: {oof_rmse}\\n\")\n    \n    test_preds_ridge.append(reg.predict(X_test))\n\nprint('BayesianRidge: Mean OOF RMSE = {}'.format(rmse(train.target.values, oof_ridge)))","80923af4":"test_preds_ridge = np.mean(test_preds_ridge,0)\ntest_preds_ridge","3624c479":"oof_train = pd.DataFrame()\noof_train['roberta'] = oof_roberta\noof_train['xgboost'] = oof_xgboost\noof_train['ridge'] = oof_ridge\noof_train['target'] = train.target.values\noof_train = create_folds(oof_train, num_splits=5)\ndisplay(oof_train.shape)\noof_train.head()","6de38169":"x_oof_train = oof_train[['roberta', 'xgboost', 'ridge']]\nx_oof_train.head()","6f61a8ec":"final_test = pd.DataFrame()\nfinal_test['roberta'] = test_preds_roberta\nfinal_test['xgboost'] = test_preds_xgboost\nfinal_test['ridge'] = test_preds_ridge\ndisplay(final_test.shape)\nfinal_test.head()","4abc8245":"%%time\n\nstacking_preds = []\noof_rmses = []\n\n\nfor fold in range(FOLDS):\n    print(f'\\nTraining Fold {fold + 1} \/ {FOLDS}')\n    \n    train_idx, val_idx = oof_train.index[oof_train['kfold']!=fold].tolist(), oof_train.index[oof_train['kfold']==fold].tolist()\n    x_train, y_train = x_oof_train.iloc[list(train_idx)], oof_train.target.iloc[train_idx]\n    x_val, y_val = x_oof_train.iloc[list(val_idx)], oof_train.target.iloc[val_idx]\n\n    reg = LGBMRegressor(max_depth=5, n_estimators=40)\n    reg.fit(x_train, y_train)\n    \n    val_pred = reg.predict(x_val)\n    oof_rmse = rmse(val_pred, oof_train.target[val_idx].values)\n    oof_rmses.append(oof_rmse)\n    print(f\"Fold {fold+1} train OOF RMSE: {oof_rmse}\")\n    \n    stacking_preds.append(reg.predict(final_test))\n\nprint('Stacking LGBMRegressor: Mean OOF RMSE = {}'.format(np.mean(oof_rmses)))","049c24ba":"sub = pd.read_csv(SAMPLE_SUB_FILE)","4e1cfd36":"sub['target'] = np.mean(stacking_preds,0)\nsub.to_csv('submission.csv', index=False)","8bfad93c":"sub","e1359658":"#### Evaluator","f98a50c8":"#### Test Run","b9ce2a33":"### Loading Dataset","0ed90a7c":"Train set with the columns that will be required for training.","2a358558":"#### Model","4df70987":"Taking mean across the 5 folds for the test set prediction","b4d4a813":"## Stacking Ensemble of 3 Models\n1. Roberta Large model from - https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-2 \n2. Ridge Regression on Sentence Embeddings from Sentence Transformers \n3. XGBoost Regression on Sentence Embeddings from Sentence Transformers\nThe last 2 models have been referred from - \n\nI have gone through multiple notebooks published in the competition that tried out weighted-average while ensembling models and have got some good results. I also wanted to try out stacking with a meta model and see how it performs.\n\nBelow, i provide a basic indroduction to Stacking Ensemble method and provide the code to implement the same. Do note that i have not done any fine-tuning on the hyperparameters for the regression models or the meta models. Further fine-tuning or selection of a better model may help you improve your score. \n\n**I am starting out with notebooks on Kaggle and this is one of my first attempts at it. If you find anything useful, please UPVOTE and also leave out SUGGESTIONS in the comments on how i can improve this and my future notebooks.**\n","65ea1ea8":"#### Config","358f4aeb":"### Submission","89a1336b":"### Creating the test set for generating the final prediction from the meta-model.\nThe mean of the predictions across all the 5 folds on the test set by the base models is used to create this final test set.","01a4c475":"#### Dataset class","1c835b20":"### CONFIG","c7d6acc6":"### RMSE helper method","7844994d":"### That's it from this notebook. Hope the beginners on Kaggle find something useful here. ","fc506142":"#### Utils","ae4ffd1f":"### Intermediate Train set built from the OOF predictions of all the 3 models\nYou can experiment with your own models and add the OOF predictions from them to the this train set to be used for trianing the meta-model.","b4d7a438":"### Create Folds helper method","479bb65d":"Taking mean across the 5 folds for the test set prediction","7e304504":"#### OOF Roberta Large Predictions","17931e84":"### Sentence Embeddings model using offline Sentence Transformers Libary.\nCode referred from - https:\/\/www.kaggle.com\/datafan07\/eda-simple-bayesian-ridge-with-sentence-embeddings\nThe model is a Roberta model used in the linked notebook.","cc79b276":"### XGBoost model on sentence embeddings along with OOF and test set predictions","d5344852":"### Stacking Ensemble\n![Stacking Emseble - TowardsDataScience](https:\/\/miro.medium.com\/max\/1400\/1*1ArQEf8OFkxVOckdWi7mSA.png)\n\n[Image Source](https:\/\/towardsdatascience.com\/the-power-of-ensembles-in-deep-learning-a8900ff42be9)\n\nStacking is a technique for ensembling multiple models. It helps us generalise the predictions from different models that might perform well on a subset of the data but not the whole set. Hence, by building different models that perform well on part of the data and using a meta model to generalize from their predictions, we can aim to achieve a better overall score. \n\n* The base models are trained on the original train dataset and the predictions from all the 3 models are combined and used as the intermediate train dataset which is used to train the final meta-model. \n* The best way to generate predictions to create the intermediate train dataset is to train the base models using K-Fold cross validation and use their Out-Of-Fold predictions to create the training dataset for the meta-model.\n* Here i have used 5-fold cross-validation for all the 3 models to generate the predictions.\n\n#### Meta Model selected - LGBMRegressor\n\nOthe experiments that can tried out - \n1. Providing Sentence Embeddings as input to the final meta-model along with the predictions from the base models.\n2. Trying out a different meta model for ensembling.\n\nIf you guys, have any other experiments that can be tried out, please comment below. That will be really helpful.\n\n","eb9da16f":"#### Test Set Roberta Large Predictions for Final Test set\nWe take mean across the predictions from the 5 folds.","96f8566e":"### Bayesian Ridge model on sentence embeddings along with OOF and test set predictions","e861b572":"### Stacking Ensemble Model - LGBMRegressor\nI haven't performed any hyperparameter tuning or experimented with any other models for the meta-model. Please leave comments below if want to share your experiments.","0fc7a0b1":"### Importing Libraries","68dc8d8d":"### Robeta Large Inference\nThe below code for the Roberta Inference has been taken from the notebook - https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-infer-2. "}}