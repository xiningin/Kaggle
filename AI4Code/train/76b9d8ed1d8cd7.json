{"cell_type":{"6fab37f8":"code","c681968b":"code","35987293":"code","3f1bfbd9":"code","44c0ea51":"code","f58b73ba":"code","dace5b81":"code","6baed309":"code","f50d22b3":"code","87352251":"code","f43d3ea3":"code","4cd1f002":"code","ad9eafb3":"code","2674ff19":"code","a1132a0d":"code","ee8019b1":"code","12382715":"code","5d6fd1e2":"code","bb8d8021":"code","83c2fee6":"code","92d14b98":"code","61987b5a":"code","3d0419b2":"code","ec75c38b":"code","de8139ce":"code","513c6397":"code","7b069164":"code","a402017e":"code","1bf5a248":"code","2b6ab335":"code","bb194027":"code","bdc913d1":"code","53d26eda":"code","3dafcdf0":"code","0d67ab8d":"code","9f0d5852":"code","f48a6bd6":"code","372b5182":"code","a01c4b94":"code","8576034a":"code","da4eee9f":"markdown","14b4184b":"markdown","44503444":"markdown","0f7dba22":"markdown","c7439d28":"markdown","bda7ba67":"markdown","fb01c6aa":"markdown","3fac5102":"markdown","b6db5c8f":"markdown","6b69d8f3":"markdown","f3f679c4":"markdown","686e34da":"markdown","913fc5b4":"markdown","a9faf687":"markdown","a52f26c1":"markdown","aadcfd10":"markdown","dba18a9f":"markdown","df7d85f7":"markdown","ded30a7c":"markdown","c9fd981a":"markdown","019fb667":"markdown","6b91b35d":"markdown","fb31a476":"markdown","a59f74ba":"markdown","155586fc":"markdown","d46e07fd":"markdown","48201e0b":"markdown","3d273cb4":"markdown","1e01f06e":"markdown","0e165862":"markdown","92145b10":"markdown","492aa6a5":"markdown","2cec92e7":"markdown","e0976a14":"markdown","b1977622":"markdown","c261fcea":"markdown","08632f56":"markdown","65b8288a":"markdown","be5aeab7":"markdown"},"source":{"6fab37f8":"import pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import boxcox\nimport matplotlib.cm as cm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.cluster import DBSCAN","c681968b":"# Read in raw data and have a peak\nula_raw = pd.read_csv(\"..\/input\/ulabox_orders_with_categories_partials_2017.csv\")\n#ula_raw = pd.read_csv(\".\/data\/ulabox_orders_with_categories_partials_2017.csv\")\nula_raw.head()","35987293":"ula_pivot = pd.pivot_table(ula_raw, values='order', index='customer', aggfunc='count')\n\nplt.hist(ula_pivot['order'], bins=50)\n#plt.xlim(0, 105)\nplt.show()\nmsg = \"There are %d unique customers in the dataset of %d orders\" % (len(ula_pivot), len(ula_raw))\nprint(msg)\nula_pivot.order.value_counts().head()","3f1bfbd9":"# Add product category percentages as a check\nula_raw['Total%'] = ula_raw['Food%'] + ula_raw['Fresh%'] + ula_raw['Drinks%'] + ula_raw['Home%'] + ula_raw['Beauty%'] + ula_raw['Health%'] + ula_raw['Baby%'] + ula_raw['Pets%']\nula_raw.describe()","44c0ea51":"# Examine outliers\nula_check = ula_raw[(ula_raw['Total%']<99.0) | (ula_raw['Total%']>101)]\nplt.hist(ula_check['Total%'], bins=100)\nplt.xlim(0, 105)\nplt.title(\"Observations with total % below 99% or above 101%\")\nplt.show()\nmsg = \"There are %d potential outliers in the dataset of %d samples\" % (len(ula_check), len(ula_raw))\nprint(msg)","f58b73ba":"#Remove records that may have errors in the product category percentages\nula = ula_raw[(ula_raw['Total%']>=99.0) & (ula_raw['Total%']<=101)]","dace5b81":"freebies = len(ula_raw[(ula_raw['discount%']==100.0)])\n#print(\"There are \"  freebies \" samples with 100% discount.\")\nmsg = \"There are %d samples with 100 percent discount.\" % freebies\nprint(msg)\n\nrecharge = len(ula_raw[(ula_raw['discount%']<0)])\n#print(\"There are \"  freebies \" samples with 100% discount.\")\nmsg = \"There are %d samples with negative discount.\" % recharge\nprint(msg)\n\n#Remove records that have 100% discount\nula = ula_raw[(ula_raw['discount%']!=100.0)]","6baed309":"# Review distributions of potential predictor variables\nprod = ula[['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%',\n            'total_items', 'discount%']]\n\nfig = plt.figure()\n\nnum=1\nfor column in prod:\n    # Find the right spot on the plot\n    ax = plt.subplot(5, 2, num)\n    fig.set_size_inches(8, 18, forward=True)\n    \n    plot_var = ula[column]\n    plt.hist(plot_var, bins=50)\n    #sns.kdeplot(plot_var, shade=True, linewidth=1, legend=False, color='g') #\n    title = column #+ \"(log): \" + v_normal\n    plt.title(title, fontsize=9)\n    ax.set_ylim(0, 29000)\n   \n    num+=1\n      \nplt.show()      ","f50d22b3":"# Create dataset for transformed features\nula_tran = ula.copy()\nmin_disc = np.min(ula_tran['discount%'])\nula_tran['discount%'] = ula_tran['discount%'] - min_disc","87352251":"# Create dummy product category indicators\nprod = ['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%']\nnames = ['Food', 'Fresh', 'Drinks', 'Home', 'Beauty', 'Health', 'Baby', 'Pets']\nfor index in range(len(prod)):\n    ula_tran[names[index]] = (ula_tran[prod[index]]>0).astype(int)","f43d3ea3":"# Create aggregate product measure\ncat_per = (ula_tran[['Food', 'Fresh', 'Drinks', 'Home', 'Beauty', 'Health', 'Baby', 'Pets']].aggregate('mean'))*100\ncat_avg = ula_tran[['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%']].aggregate('mean')","4cd1f002":"# What % of orders contain at east one product category?\ncat_per.sort_values(ascending=False)","ad9eafb3":"# What is mean % share of monetary value of orders?\ncat_avg.sort_values(ascending=False)","2674ff19":"# Plot distribution of product categories across orders\nx=['Food', 'Fresh', 'Drinks', 'Home', 'Beauty', 'Health', 'Baby', 'Pets']\n\nfig, axs = plt.subplots(1, 2, figsize=(10, 3))\naxs[0].bar(x, cat_per, color='b')\naxs[0].set_xlabel(\"Product Category\")\naxs[0].set_ylabel(\"% of Orders\")\naxs[0].set_ylim([0, 100])\naxs[0].set_title(\"Percent of Orders Containing Product Category\")\n\naxs[1].bar(x, cat_avg, color='g')\naxs[1].set_xlabel(\"Product Category\")\naxs[1].set_ylabel(\"Avg. Value of Order\")\naxs[1].set_ylim([0, 100])\naxs[1].set_title(\"Avg. Percent of Order Value\")\n\nfig.tight_layout()\nplt.show()","a1132a0d":"ula_tran['cat_count'] = ula_tran['Food'] + ula_tran['Fresh'] + ula_tran['Drinks'] + ula_tran['Home'] + ula_tran['Beauty'] + ula_tran['Health'] + ula_tran['Baby'] + ula_tran['Pets']","ee8019b1":"plt.hist(ula_tran.cat_count, bins=8)\nplt.title(\"Number of Product Categories per Order\")\nplt.xlabel(\"Product Categories\")\nplt.ylabel(\"Orders\")\nplt.show()","12382715":"#Save identifier and numeric columns for later\nula_id = ula_tran[['order', 'customer', 'hour', 'total_items', 'discount%', 'cat_count']]","5d6fd1e2":"# Transform all the numeric columns in input data, due to skewness\nfrom scipy.stats import boxcox\n\nvars = ['total_items', 'discount%', 'cat_count']\n\nfig = plt.figure()\n\nnum=1\nfor col in vars:\n    # use boxcox square root\n    ula_tran[col] = boxcox(ula_tran[col] + 0.1)[0]\n        # Find the right spot on the plot\n    plt.subplot(2, 2, num)\n    fig.set_size_inches(8, 8, forward=True)\n    \n    plot_var = ula_tran[col]\n    plt.hist(plot_var, bins=20)\n    #sns.kdeplot(plot_var, shade=True, linewidth=1, legend=False, color='g') #\n    title = col \n    plt.title(title, fontsize=9)\n    \n    num+=1\n      \nplt.show()      \n ","bb8d8021":"#Plot orders over time\n\ndays = pd.pivot_table(ula, values='order', index='weekday', aggfunc='count')\nhours = pd.pivot_table(ula, values='order', index='hour', aggfunc='count')\nfig, axs = plt.subplots(2, 1, figsize=(7, 5))\naxs[0].plot(days['order'], 'b')\naxs[0].set_xlabel(\"weekday (1=Monday)\")\naxs[0].set_ylabel(\"orders\")\naxs[1].plot(hours['order'], 'g')\naxs[1].set_xlabel(\"hour\")\naxs[1].set_ylabel(\"orders\")\nfig.tight_layout()\n#fig.suptitle('Number of Orders by Day and Hour')\nplt.show()","83c2fee6":"heatmap = pd.pivot_table(ula, values='order', index='weekday', columns='hour', aggfunc='count')\n\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 5)\n#plt.gca().invert_yaxis()\n#plt.ylim(1999, 2016) \nplt.title('Ulabox Orders by Day and Time') \nsns.heatmap(heatmap, cmap = 'Blues')\n#ax.set_xlabel('Counties (Sorted by Mean Drug Mortality)')\nax = plt.gca()\nplt.show()","92d14b98":"# Recode hours to categories\ndef recode(x): \n    \"\"\"Recode hours to categories\"\"\"\n    if (x >= 1) & (x <= 7): \n        return 1\n    elif (x >= 8) & (x <= 10): \n        return  2\n    elif (x >= 11) & (x <= 13): \n        return  3\n    elif (x >= 14) & (x <= 16): \n        return  4\n    elif (x >= 17) & (x <= 19): \n        return  5\n    elif (x >= 20) | (x == 0):  \n        return  6 \n\nula_tran['time_cat'] = ula_tran['hour'].apply(recode)   ","61987b5a":"#Create dataset for cluster analysis\nula_cluster = ula_tran.drop(columns=['order', 'customer', 'hour', 'Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%'])\n#ula_cluster.info()","3d0419b2":"# list numeric features for pipeline\nnum_vars = ['discount%', 'total_items', 'cat_count']\n# list categorical features for pipeline\ncat_vars = ['time_cat', 'weekday']\n\n# Create numeric and categorical transformations\nnum_pipeline = Pipeline(steps=[('scaler', StandardScaler())])\ncat_pipeline = Pipeline(steps=[('onehot', OneHotEncoder(categories='auto'))])\n\npreprocessor = ColumnTransformer(transformers=[\n        ('num', num_pipeline, num_vars),\n        ('cat', cat_pipeline, cat_vars)],\n        remainder='passthrough')","ec75c38b":"# Create transformed dataset\nX = preprocessor.fit_transform(ula_cluster)","de8139ce":"pca = sklearn.decomposition.PCA(random_state=58)\npca.fit(X)","513c6397":"pca_expl_var = pd.DataFrame(pca.explained_variance_ratio_).reset_index()\npca_expl_var.columns = [\"PC\", \"expl_var\"]\n#pca_expl_var.head()","7b069164":"fig, ax = plt.subplots()\nfig.set_size_inches(12, 4, forward=True)\n\nplt.title('Explained Variance for Principal Components of Ulabox Orders') \nplt.plot(pca_expl_var.PC, pca_expl_var.expl_var, color = 'tab:red')\n\nax.set_xlabel('Principal Component')\nax.set_ylabel('Explained Variance')\nax.set_xticks(np.arange(0, 32, step=1))\n#plt.xticks(np.arange(0, n_clusters, step=1))\n#plt.ylim(0, 0.2) \n\nplt.show()","a402017e":"pca = sklearn.decomposition.PCA(n_components=4, random_state=58)\npca.fit(X)","1bf5a248":"X_pca = pca.transform(X)","2b6ab335":"# evaluate K-means models of PCA-reduced data\nk_values = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n\nsse_score = []\nsil_score = []\nfor k in k_values:\n    model = KMeans(n_clusters=k, random_state=58)\n    km = model.fit(X_pca)\n    sse_score.append(km.inertia_)\n    cluster_labels = km.fit_predict(X_pca)\n    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n    sil_score.append(silhouette_avg)\n\n    msg = \"Clusters:%s SSE:%f.2 Sil: %f.3\" % (k, km.inertia_, silhouette_avg)\n    print(msg)\n    print(pd.Series(cluster_labels).value_counts())","bb194027":"# plot the K-means SSE and Silhouette scores\n\nfig, ax1 = plt.subplots()\nax1.plot(k_values, sse_score, color = 'tab:red')\nax1.set_xlabel('K')\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel('SSE', color='tab:red')\nax1.tick_params('y', colors='tab:red')\nax1.set_xticks([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])\n\nax2 = ax1.twinx()\nax2.plot(k_values, sil_score, color = 'blue')\nax2.set_ylabel('Silhouette Score', color='blue')\nax2.tick_params('y', colors='blue')\n\nfig.tight_layout()\nplt.show()","bdc913d1":"# Make silhouette plots\nseed=58\n\nmodel_list = []\nmodel_list.append((KMeans(n_clusters=3, random_state=seed)))\nmodel_list.append((KMeans(n_clusters=4, random_state=seed)))\nmodel_list.append((KMeans(n_clusters=5, random_state=seed)))\nmodel_list.append((KMeans(n_clusters=6, random_state=seed)))\n\nscore=[]\n\nfor model in model_list:\n    # Create a subplot \n    fig, ax1 = plt.subplots()\n    fig.set_size_inches(18, 6)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, .5]\n    ax1.set_xlim([-0.3, .5])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    #ax1.set_ylim([0, X_pca_sample_pca.shape[0] + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value \n    clusterer =  model.fit(X_pca) \n    cluster_labels = clusterer.labels_\n    cluster_dist = pd.DataFrame(cluster_labels, columns=['Cluster'])\n    \n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n    print(\"The average silhouette_score is: \", silhouette_avg)\n    msg = \"Clusters:%s SSE:%f.2 Sil: %f.3\" % ((model.get_params()[\"n_clusters\"]),\n                                              clusterer.inertia_, silhouette_avg)\n    print(msg)\n    print(pd.Series(cluster_labels).value_counts())\n    score.append(silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)\n\n    y_lower = 10\n    for i in range(model.get_params()[\"n_clusters\"]):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/model.get_params()[\"n_clusters\"])\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(model)\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.3, -0.1, 0, 0.2, 0.4, 0.6, ])\n\n    #plt.suptitle((\"Comparison of Clustering Models\"), fontsize=14, fontweight='bold')\n\nplt.show()","53d26eda":"# Add PCA loadings to dataset\nula_tran['x'] = pca.fit_transform(X)[:,0]\nula_tran['y'] = pca.fit_transform(X)[:,1]","3dafcdf0":"# Recreate best K-means model\nn_clusters=5\nseed=58\nmodel = KMeans(n_clusters=n_clusters, random_state=seed)\nkm5 = model.fit(X_pca)\nclusters = km5.predict(X_pca)\nula_tran['cluster5'] = km5.predict(X_pca)","0d67ab8d":"cluster_list = np.arange(km5.get_params()['n_clusters'])\n\n# Plot clusters on PCA axes\nfig, ax = plt.subplots()\nfig.set_size_inches(8, 8, forward=True)\nplt.title('Assigned clusters and first two principal components') \n\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\n\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n#for cluster, group in groups:\nfor cluster in cluster_list:\n    plot_data = ula_tran[ula_tran['cluster5']==cluster]\n    ax.plot(plot_data.x, plot_data.y, marker='.', linestyle='', alpha=0.3, ms=12, label=cluster)\nax.legend(loc=1)\n\nplt.show()","9f0d5852":"# Plot clusters on PCA axes\nfig, ax = plt.subplots()\nfig.set_size_inches(6, 6, forward=True)\nplt.title('Cluster 0 and first two principal components') \n\nax.set_xlabel('PC 1')\nax.set_ylabel('PC 2')\nax.set_xlim(0, 80)\nax.set_ylim(-3, 6)\n\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\nplot_data = ula_tran[ula_tran['cluster5']==0]\nax.plot(plot_data.x, plot_data.y, marker='.', linestyle='', alpha=0.2, ms=12, label=\"0\")\n#color=['skyblue', 'gold', 'slateblue', 'yellowgreen'])\nax.legend(loc=1)\n\nplt.show()","f48a6bd6":"ula_labelled = ula_tran.drop(columns=['total_items', 'discount%', 'cat_count'])\nula_labelled['total_items'] = ula_id['total_items']\nula_labelled['discount%'] = ula_id['discount%'] + min_disc\nula_labelled['cat_count'] = ula_id['cat_count']\nula_labelled['hour'] = ula_id['hour']\nula_labelled['order'] = ula_id['order']\nula_labelled['customer'] = ula_id['customer']","372b5182":"ula_labelled.cluster5.value_counts()","a01c4b94":"# Review distributions of potential predictor variables\nvar_list = ['Food%', 'Fresh%', 'Drinks%', 'Home%', 'Beauty%', 'Health%', 'Baby%', 'Pets%',\n            'total_items', 'discount%', 'cat_count']\n\nfig = plt.figure()\n\nnum=1\nfor var in var_list:\n    # Find the right spot on the plot\n    ax = plt.subplot(7, 2, num)\n    fig.set_size_inches(14, 30, forward=True)\n    \n    plot_var = ula_labelled[var]\n    sns.boxplot(x=plot_var, y='cluster5', data=ula_labelled, linewidth=1.5, orient='h')\n    #plt.title('Mean Drug Mortality by Ubanization Code')\n    ax.set_xlabel(var)\n    ax.set_ylabel(\"Cluster\")\n    ax.set_yticklabels(('0 n=8634', '1  n=30', '2 n=3896', '3 n=16769', '4 n=275'))\n   \n    num+=1\nfig.tight_layout()      \nplt.show()      ","8576034a":"colors = ['Blues', 'Oranges', 'Greens', 'Reds', 'Purples']\n\nfor cluster in cluster_list:\n    heatmap = pd.pivot_table(ula_tran[ula_tran['cluster5']==cluster], values='order', index='weekday', columns='time_cat', aggfunc='count')\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(12, 5)\n    title = (\"Cluster \" + str(cluster) + \": Ulabox Orders by Day and Time\")\n    plt.title(title) \n    sns.heatmap(heatmap, cmap = colors[cluster])\n    #ax.set_xlabel('Counties (Sorted by Mean Drug Mortality)')\n    ax = plt.gca()\n    plt.show()","da4eee9f":"** Cluster 3: n=16734 **\n- strong tendency to order in late evening, especially Sunday or Monday nights.\n- most likely to include food or fresh products\n- consistently includes drinks, but never 100%\n- 2nd most likely to inclue home or beauty products\n- highest total items and highest mean category count\n- modest discounts\n\nSummary: Cluster 3 orders (shown in red above) are the largest category of orders (57%), and they are also the largest in terms of total items and number of product categories included. These orders are most often placed on Sunday or Monday evening. They orders are the most likely to contain products from the 'Fresh' category. These orders have the hallmarks of planned weekly grocery shopping for a family or other household. \n\nSince 'Fresh' is only available in Barcelona and Madrid, the presence of 'Fresh' products may be more of an indicator of geography than of preference. Adding geography data to the model could help clear this up.\n\nLet's call these orders ** Fresh Family Shopping **","14b4184b":"The above plot shows that the first two principal components do a good job of differentialting the clusters. Clusters 2 and 3 (green and red) are differentiated on PC2 but are identical on PC1. PC1 serves to differentiate Clusters 1 and 4 from the rest. But where is Cluster 0, which accounts for 29% (n=8625) of the order samples?","44503444":"The heatmaps above show the distribution of the 5 order clusters over weekday and time.","0f7dba22":"## Compare clusters according to original, untransformed features","c7439d28":"## PCA Feature Reduction","bda7ba67":"** Table of Contents **\n\n1. Introduction\n\n2. Data Cleaning\n\n3. Exploratory Data Analysis\n\n4. Data Preparation\n\n5. Principal Components Analysis\n\n6. Cluster Analysis\n\n7. Discussion\n\n## Introduction\n\nThis is an exploratory project using data from Ulabox, Spain's leading online grocery retailier. The dataset has been made public on Kaggle with this description:\n\n> ** Context: **\n>Ulabox is the most successful pure-player online grocery in Spain. It picks up more than \u20ac1 million in monthly revenue and asserts a customer satisfaction above 95%. It currently serves Madrid and Barcelona with fresh food and the rest of Spain's peninsula with non perishable items.\n\n> ** Content **\nThe ulabox_orders_with_categories_partials_2017 dataset includes a subset of anonymized 30k orders from the beginning of 2017. All kind of customers (around 10k) are represented in this dataset: from urban and rural areas, from first-timers to loyal customers.\n\nData Source:\nThe Ulabox Online Supermarket Dataset 2017, accessed from https:\/\/www.github.com\/ulabox\/datasets\n\n\n","fb01c6aa":"The next step will be to see if we can simplify the data through principal components analysis.","3fac5102":"Here is a plot of Cluster 0 all by itself on the same axes as above. We can see that it must be similar to Cluster 2 since it was hiding underneath. We can assume that Cluster 0 is differentiated from Cluster 2 by either PC3 or PC4, which are not shown in 2D.","b6db5c8f":"## Exploratory Data Analysis","6b69d8f3":"Since there are only 416 potential outliers in Total%, I will delete them from the dataset. Without knowing how the dataset was created I can't really guess how these values occurred. The dataset creator did comment that none of the orders involved cancellations or returns.","f3f679c4":"These charts show how the product categories are distributed across orders. More than half of all orders contain a product from the Drinks category (84%), and the same is true for the Food, Home, and Fresh categories. Less than 10% of all orders contain products from the Pets or Health categories.\n\nThe share of order value largely follows these patterns. On average, Drinks, Food, and Fresh products make up over 20% of each order. On average, Pet and Health products account for just 1% of order value.","686e34da":"## Plot clusters on PC1 and PC2","913fc5b4":"The number of orders per customer has an exponential distribution, with about 44% of customers having just one order in the dataset. Depending on how this dataset was extracted, this may or may not be all of each customer's order records. The dataset creators did say that these orders are all from early 2017 and that they represent many kinds of customers. For the sake of this analysis, I will assume that we do have all orders for each customer in the dataset.\n\nCustomer segmentation is typically performed with customer-level data. But since we have rich data at the order level, I will first look for patterns there.","a9faf687":"Cluster 1: n=275\n - Wednesday evening\n - most likely to include beauty products\n - but lower than others on all other product categories\n - orer avg. amount of items but can be from many categories (or not)\n \nCluster 1 (shown in purple above) is a small cluster, accounting for 1% of orders. These orders are the most likely to contain beauty products. They tend to be placed on Wednesday evenings, which seems unusual but could be a result of weekly advertisements, discounts, or messages sent to loyalty club members. On the other hand the discounts for these orders are just average. These orders tend to have an average number of items, but can have a large category count. \n\nA cluster this small may not be very meaningful, but let's call it ** Beauty Shopping **\n","a52f26c1":"These histograms have the same y axis to show that for several categories (Beauty, Health, Baby, and Pets) most orders do not contain any products from those categories. Zero percent is the mode for all of the product categories. This data is difficult to transform to Gaussian approximation. Therefore I will instead dichotomize these measures into dummy variable that indicate whether or not the category was prsent in the order.\n\nThe distribution of discount% is skewed, and the rest of the numeric variables appear to have exponential distributions. I will use a Boxcox transformation on these variables to better approximate a normal distribution. For the discount% I will first add a constant to all observations to eliminate negative numbers. These features will then be standardized in the next step.","aadcfd10":"The dataset creator provided this information about the discount data:\n>Ulabox applies recharges like: a sugar tax for processed food and drinks, or a difficult-to-transport recharge for very heavy items. As the normal discounts and recharges are processed at the same time, sometimes the value in this field is negative.\n\nThis is unfortunate since this type of recharge is not really related to discounting. A sugar tax, for example, is a required cost for the consumer, not an offset to a discount. It would be better if the discount and recharge amounts had been kept separate. However, these recharges are applied in the same manner to all orders, not just those that ended up with a negative discount. There is really nothing we can do about this as this it just the way the business has calculated this measure.\n\n>And by the way, on the other side of the spectrum: a 100% discount is a \"free order\" discount that is usually dispatched to influencers.\n\nGive-aways to influencers are a different kind of marketing expense than a discount on retail customer orders, so we can go ahead and delete these from the dataset.","dba18a9f":"### Next Steps and Recommendations\nCluster analysis identified 5 types of orders in the dataset. The next step in the anlaysis will be to aggegate the data to the customer level and see if we can use the order types to segment the customers.\n\nThere are some issues with the data that Ulabox has provided that make the analysis more difficult.\nOne thing that would improve the analysis would be to separate the data on discounts and recharges to get a better sense of the impact of discounting on orders. It would also be helpful to add data on order location to the models. This would help clarify whether orders that do not contain 'Fresh' products are reflective of consumer preference or simple geographic availability of 'Fresh' products. ","df7d85f7":"In this dataset 1=Monday and 7=Sunday, so we can see that the most orders occur on Monday and Sunday while orders are much lower on Friday and Saturday. The most popular time for ordering is between 10-11pm, followed by 10am-noon.","ded30a7c":"The above plot shows that by 4 principal components we have captured over 95% of the variance in the dataset. I will reduce the original data to these 4 components.","c9fd981a":"The 5-cluster model, which has the highest average Silhouette score, has several advantages. It produces three good-size clusters, with two small clusters that may or may not  be useful. And the 5-cluster model contains few samples with negative silhouette scores, indicating consistency within the clusters.","019fb667":"Cluster 2: n=3940\n- Orders tend to happen late evening Sunday or Monday.\n- highest discount % nad most likely to contain baby products or home products.\n- small total items and cat count\n- avg drinks, fresh, and food\n\nCluster 2 (shown in green above) is the third largest cluster of orders, accounting for 13% of orders. These orders tend to have a low total number of items and low category count. They most likley to contain baby or home products. These orders tend to occur at the beginning of the week. These orders have the hallmarks of being part of regular weekly shopping, but they may be for younger families with small children who are more sensitive to pricing and motivated by discounts.\n\nLet's call these orders ** Small Children Family Shopping **\n","6b91b35d":"Above are the transformed numeric features. Next we will explore the time and day features of the dataset.","fb31a476":"The boxplots above show the distibutions of original, untransformed features as well as calculated numeric fields within each cluster.","a59f74ba":"## Clustering Order Data\n\nNext I use K-means, which is a good general purpose clustering technique. Since I don't know how many clusters we might find, I will test the range 3-20 clusters and compare both the sums of squares and the silhouette score for the models.","155586fc":"Next we will take a closer look at the numeric variables.","d46e07fd":"When I add a column summing all the product categories, the summary of the numeric data raises two issues:\n1. We can see that there are some samples in the dataset that do not have total=100%.\n2. There are negative discounts and discounts of 100%","48201e0b":"## Data Preparation","3d273cb4":"# Ulabox Online Retail Order Segmentation","1e01f06e":"Next we will use the first two princiapl components to illustrate our cluster model.","0e165862":"There does not seem to be a clear 'elbow' in the SSE curve, though there are slight breaks in the rate of SSE improvement at 4, 5, and 6 clusters. The silhouette scores for these models are highest for 2 and 5 clusters, although the 2-cluster model is highly imbalanced and therefore not practical. We can use silhouette plots to further explore the most promising models. ","92145b10":"## Discussion","492aa6a5":"This heatmap shows that most of the late evening orders are placed on Sunday. Overall, Sunday evening and Monday midday are the most common times for orders to be placed.\n\nAlthough time is generally a continuous variable, in this case we do not have the date of the order and so we will not be able to treat time as continuous. If we tried to do so, distance-based clustering methods would interpret hour 23 as very far from hour 0, which it is not. Since we have over 29,000 samples and not too many features, I will treat both weekday and hour as categorical variables and one-hot encode them prior to clustering. The hour of the day will first be grouped into categories as follows:\n\n| Hour | Code | Time of Day   |\n|------|------|---------------|\n| 1-7  | 1    | Overnight     |\n| 8-10 | 2    | Early Morning |\n| 11-13 | 3    | Midday        |\n| 14-16  | 4    | Afternoon     |\n| 17-19  | 5    | Evening       |\n| 20-0 | 6    | Late Evening  |","2cec92e7":"The number of product categories represented in individual orders is distributed fairly normally, although single-category orders are overrepresented.","e0976a14":"Currently, each row of the dataset represents an order. Some customers have placed multiple orders, as seen above. Let's see what that distribution looks like.","b1977622":"Cluster 4: n=30\n- smallest orders in terms of items\n- typically few categories\n- lowest beauty, health, pets, and home products\n- low fresh\n- avg. food\n- orders tend to be placed Monday evening or Tuesday midday\n\nThis cluster (shown in orange above) is the smallest of all, and may not be meaningful. These are small orders that tend to contain food products. Let's call them ** Afterthoughts & Emergencies **","c261fcea":"We can use the information above to describe and label the 5 clusters identified by K-means. I will review the clusters in the order of their frequency.","08632f56":"** Cluster 0: n=8625 **\n - avg. on food & fresh\n - above avg. drinks (though not as much as Cluster3)\n - avg home, moderate baby\n - low beauty products\n - low total items\n - low cat count (2-3)\n - negative discounts (probably due to drinks recharge)\n - tend to be placed Sunday night, Monday night, or Monday morning\n \nCluster 0 (shown in blue above) is the second largest cluster of orders, accounting for 29% of orders. These orders tend to have a low total number of items and low category county. They are a bit less likley to contain food or fresh items,and can contain a large share of drinks. These orders tend to occur at the beginning of the week, but are less concentrated in time. These orders have the hallmarks of being part of regular weekly shopping, but they may be for smaller families or for families that also regularly buy groceries at a store.\n\nLet's call the orders ** Small Family Shopping **\n","65b8288a":"## Data Cleaning","be5aeab7":"The data is prepared for cluster analysis by dropping unneeded features, scaling numeric variables and one-hot encoding the day and time features."}}