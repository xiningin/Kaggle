{"cell_type":{"9bcf7a7b":"code","6f04f383":"code","c824a678":"code","4b3132a4":"code","601d10c5":"code","a1dcbbef":"code","8dd53f8b":"code","56465b74":"code","d9350f58":"code","fc32f8da":"code","2fdcde7e":"code","f539b6be":"code","d399f98e":"code","2cd7b7e5":"code","2c85443e":"code","74d455ef":"code","2f79e2bd":"code","9042f3d6":"code","27b8d7d9":"code","82971e7a":"code","ecb4a3e1":"code","dac67e35":"code","2baaf953":"code","21f9419f":"code","b92af30e":"code","73ca05ea":"code","4f6b9c31":"code","20a46253":"code","9e4a8cd4":"code","c46ec455":"code","ed60689b":"code","261f1e4d":"code","cdcb4c81":"code","e0fcf2a0":"code","3878be9d":"code","77387c86":"code","a9c4a56b":"code","7d1cd17c":"code","d931c62e":"code","57985e95":"code","e124aa34":"code","6a66f312":"markdown","d318f5bd":"markdown","c6c045e6":"markdown","c9995b01":"markdown","80a128dd":"markdown","a42cdf63":"markdown"},"source":{"9bcf7a7b":"# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport re\nimport string\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport nltk\nimport torchtext\nimport random\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nfrom tqdm import tqdm\nfrom torchtext.data import BucketIterator\nfrom IPython.display import clear_output\nfrom nltk import word_tokenize\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","6f04f383":"data = pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv')\ndata.head()","c824a678":"data.info()","4b3132a4":"data['Body'][0]","601d10c5":"# \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043a\u043e\u043b\u043e\u043d\u043a\u0438 \u0441 \u043d\u0435\u043d\u0443\u0436\u043d\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0435\u0439\ndf = data.drop(['Id', 'CreationDate'], axis=1)\ndf.head()","a1dcbbef":"# \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u043a\u043e\u043b\u043e\u043a\u043d\u043a\u0438 \u0441 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u043c\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u0432 \u043e\u0434\u043d\u0443\ndf['text']= df['Title'] + ' ' + df['Body'] + ' ' + df['Tags']\ndf = df.drop(['Title', 'Body', 'Tags'], axis=1)","8dd53f8b":"# \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 \u0441 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439\ndf['Y'] = df['Y'].replace(['HQ'], 0)\ndf['Y'] = df['Y'].replace(['LQ_EDIT'], 1)\ndf['Y'] = df['Y'].replace(['LQ_CLOSE'], 2)\ndf.head()","56465b74":"df['text'][0]","d9350f58":"df['Y'].value_counts()","fc32f8da":"# \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0438 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u0432\u0435\u043d\u043d\u0443\u044e \u0432 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u043f\u0438\u0441\u043a\u0438\ntext, target = list(df['text']), list(df['Y'])","2fdcde7e":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0442\u0435\u043a\u0441\u0442\u0430\ndef preprocess(doc):\n    prepdoc = []\n    \n    lemmatizer = WordNetLemmatizer()\n    \n    urlptr = r'((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)'\n    alhptr = '[^a-zA-Z0-9]'\n    sqcptr = r'(.)\\1\\1+'\n    rplptr = r'\\1\\1'\n    \n    for text in doc:\n        # \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0432\u0435\u0441\u044c \u0442\u0435\u043a\u0441\u0442 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443\n        text = text.lower()\n        # \u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c \u0441\u0441\u044b\u043b\u043a\u0438 \u043d\u0430 'URL'\n        text = re.sub(urlptr, ' URL', text)      \n        # \u0443\u0431\u0438\u0440\u0430\u0435\u043c \u0432\u0441\u0435 \u0441\u0438\u043c\u0432\u043e\u043b\u044b, \u043e\u0442\u043b\u0438\u0447\u043d\u044b\u0435 \u043e\u0442 \u0431\u0443\u043a\u0432\u0435\u043d\u043d\u044b\u0445 \u0438\u043b\u0438 \u0446\u0438\u0444\u0440\u043e\u0432\u044b\u0445\n        text = re.sub(alhptr, ' ', text)\n        # \u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0438\u0437 \u0442\u0440\u0451\u0445 \u0438 \u0431\u043e\u043b\u0435\u0435 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0445 \u0431\u0443\u043a\u0432\n        text = re.sub(sqcptr, rplptr, text)\n        \n        words = ' '\n        for word in text.split():\n            # \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043a\u043e\u0440\u043e\u0442\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0438 \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u043e\u0444\u043e\u0440\u043c\u044b \u043a \u043b\u0435\u043c\u043c\u0435 (\u0441\u043b\u043e\u0432\u0430\u0440\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435)\n            if len(word) > 1:\n                word = lemmatizer.lemmatize(word)\n                words += (word + ' ')\n            \n        prepdoc.append(words)\n        \n    return prepdoc","f539b6be":"# \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430 'text' \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u0447\u0430\u0441\u0442\u044c \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0439\n%time preptext = preprocess(text)\npreptext[:2]","d399f98e":"# \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0432 Word2Vec\ndf_w2v = [sentence.split() for sentence in preptext]\ndf_w2v[:2]","2cd7b7e5":"%%time\n# \u043e\u0431\u0443\u0447\u0438\u043c Word2Vec \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043d\u0430\u0448\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\nw2v = Word2Vec(sentences=df_w2v, size=100, window=5, \n               min_count=5, workers=2, sg=1, iter=50)     ","2c85443e":"# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0438 \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u043c\u043e\u0434\u0435\u043b\u044c Word2Vec\nw2v.save('..\/working\/w2v.model')\nw2v = Word2Vec.load('..\/working\/w2v.model')","74d455ef":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u043e\u0445\u043e\u0436\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\nw2v.wv.most_similar('javascript')[:5]","2f79e2bd":"w2v.wv.most_similar('console')[:5]","9042f3d6":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438 \u0441\u0445\u043e\u0436\u0438\u0445 \u0441\u043b\u043e\u0432\ndef display_closestwords_tsnescatterplot(model, word, size):\n    sns.set_style('darkgrid')\n    mpl.rcParams.update({'font.size': 15})\n    \n    arr = np.empty((0, size), dtype='f')\n    word_labels = [word]\n    \n    close_words = model.wv.most_similar([word])\n    arr = np.append(arr, model.wv.__getitem__([word]), axis=0)\n    \n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, wrd_vector, axis=0)\n        \n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    plt.figure(figsize=(16, 10))\n    plt.scatter(x_coords, y_coords)\n    \n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()-50, x_coords.max()+50)\n    plt.ylim(y_coords.min()-50, y_coords.max()+50)\n    plt.show()","27b8d7d9":"display_closestwords_tsnescatterplot(w2v,'git', 100)","82971e7a":"display_closestwords_tsnescatterplot(w2v,'access', 100)","ecb4a3e1":"# \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\nw2v.wv['git']","dac67e35":"# \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0439\u0441\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u043b\u043e\u0432\n# w2v.wv.vocab","2baaf953":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0437\u0430\u043d\u043e\u0432\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0438\u0437 \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u043d\u0435\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0439 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438\ndict = {'Text': preptext, 'Target': target}    \ndf_rnn = pd.DataFrame(dict)\ndf_rnn.head()","21f9419f":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u0438\u0437 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043d\u0430 \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0435 \ndf_rnn['Text'].map(len).hist(bins=100);","b92af30e":"# \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0432 .csv \u0444\u0430\u0439\u043b\ndf.to_csv('df')","73ca05ea":"# \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0443\u0440\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u0440\u0438 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0439 \u0437\u0430\u0433\u0443\u0440\u0437\u043a\u0438 \u0432 torch\ndescription = torchtext.data.Field(tokenize=word_tokenize, lower=True, batch_first=True)\ny = torchtext.data.Field(sequential=False, is_target=True, use_vocab=False)","4f6b9c31":"%%time\n# \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 torch \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e TabularDataset\ndata = torchtext.data.TabularDataset(path='..\/working\/df', format='csv', \n                                     fields={\n                                         'text': ('text', description),\n                                         'Y': ('target', y)\n                                     })","20a46253":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\ndescription.build_vocab(data)","9e4a8cd4":"# \u0437\u0430\u0433\u0440\u0443\u0437\u0438\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 Word2Vec\n!wget https:\/\/dl.fbaipublicfiles.com\/fasttext\/vectors-english\/wiki-news-300d-1M.vec.zip","c46ec455":"# \u0440\u0430\u0441\u043f\u0430\u043a\u0443\u0435\u043c \u0437\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u043d\u044b\u0439 Word2Vec\n!unzip '..\/working\/wiki-news-300d-1M.vec.zip' -d '..\/working'","ed60689b":"# \u0437\u0430\u0433\u0443\u0440\u0437\u0438\u043c \u0432\u0435\u0442\u043e\u0440\u0430 \u0438\u0437 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0433\u043e Word2Vec\ndescription.vocab.load_vectors(torchtext.vocab.Vectors('..\/working\/wiki-news-300d-1M.vec'))","261f1e4d":"description.vocab.vectors.shape","cdcb4c81":"# \u0440\u0430\u0437\u043e\u0431\u044c\u0451\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443\ntrain, val = data.split(split_ratio=0.8)","e0fcf2a0":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u0435\u0442\u044c LSTM\nclass lstm(nn.Module):\n    def __init__(self, w2v, padding_inx, dropout, hidden_size):\n        super(lstm, self).__init__()\n        \n        self.embedding = nn.Embedding.from_pretrained(w2v)\n        self.embedding.padding_inx = padding_inx\n\n        self.embedding.weight.requires_grad = True\n\n        self.dropout = nn.Dropout(p = dropout)\n        self.lstm = nn.LSTM(input_size = self.embedding.embedding_dim,\n                            hidden_size = hidden_size,\n                            num_layers = 2,\n                            dropout = dropout,\n                            bidirectional = True)\n        self.label = nn.Linear(hidden_size*2*2, 1)\n\n    def forward(self, sentence):\n        x = self.embedding(sentence)\n        x = torch.transpose(x, dim0 = 1, dim1 = 0)\n        out, (hidden, c) = self.lstm(x)\n        x = self.dropout(torch.cat([c[i,:,:] for i in range(c.shape[0])], dim=1))\n        x = self.label(x)\n        return x","3878be9d":"# \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e BucketIterator \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u044b \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0432 \u0441\u0435\u0442\u044c\nbatch_size = 16\ntrain_i = torchtext.data.BucketIterator(dataset=train,\n                                        batch_size=batch_size,\n                                        shuffle=True,\n                                        sort = False,\n                                        train =True)\n\n\nval_i = torchtext.data.BucketIterator(dataset=val,\n                                      batch_size=batch_size,\n                                      shuffle=True,\n                                      sort = False,\n                                      train = False)","77387c86":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u043d\u0430\u0448\u0443 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u0443\u0442\u044c \u0441\u0435\u0442\u044c LSTM\nmodel = lstm(description.vocab.vectors, description.vocab.stoi[description.pad_token], \n             dropout=0.2, hidden_size=128).cuda()","a9c4a56b":"# \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nloss = nn.BCEWithLogitsLoss()","7d1cd17c":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438\ndef train(epochs, model, eval_time, loss_f, optimizer, train_i, val_i):\n    sns.set_style('white')\n    mpl.rcParams.update({'font.size': 10})\n    \n    step = 0\n    losses = []\n    val_losses = []\n    accuracy = []\n    val_accuracy = []\n    train_i.init_epoch()\n    \n    for epoch in range(epochs):\n        for batch in iter(train_i):\n            step += 1\n            model.train()\n            x = batch.text.cuda()\n            y = batch.target.type(torch.Tensor).cuda()\n            model.zero_grad()\n            preds = model.forward(x).view(-1)\n            loss = loss_f(preds, y)\n            losses.append(loss.cpu().data.numpy())\n            accuracy.append(accuracy_score(batch.target.data.numpy().tolist(), \n                                           np.round(np.array(torch.sigmoid(preds).cpu().data.numpy().tolist()))\n                                          ))\n            loss.backward()\n            optimizer.step()\n\n            if step % eval_time == 0:\n                clear_output(True)\n                model.eval()\n                model.zero_grad()\n\n                for batch in iter(val_i):\n                    x = batch.text.cuda()\n                    y = batch.target.type(torch.Tensor).cuda()\n                    preds = model.forward(x).view(-1)\n                    val_losses.append(loss_f(preds, y).cpu().data.numpy())\n                    val_accuracy.append(accuracy_score(batch.target.data.numpy().tolist(), \n                                                   np.round(np.array(torch.sigmoid(preds).cpu().data.numpy().tolist()))\n                                                      ))\n                    \n                fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n                fig.suptitle('Accuracy & Loss')\n                \n                axs[0, 0].set_title('train cross-entropy loss')\n                axs[0, 1].set_title('test cross-entropy loss')\n                axs[1, 0].set_title('train accuracy')\n                axs[1, 1].set_title('test accuracy')\n\n                axs[0, 0].plot(losses)\n                axs[0, 0].plot(pd.Series(losses).rolling(400).mean().values)\n                axs[0, 1].plot(val_losses)\n                axs[0, 1].plot(pd.Series(val_losses).rolling(400).mean().values)\n                axs[1, 0].plot(accuracy)\n                axs[1, 0].plot(pd.Series(accuracy).rolling(400).mean().values)\n                axs[1, 1].plot(val_accuracy)\n                axs[1, 1].plot(pd.Series(val_accuracy).rolling(400).mean().values)\n            \n                for ax in axs.flat:\n                    ax.set(xlabel='step')\n\n                plt.show()","d931c62e":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU\ndevice = torch.device(\"cuda:0\")\nmodel.to(device)","57985e95":"# \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a\u0438 \u0441 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438 loss \u0438 accurancy\ntrain(3, model, 250, loss, optimizer, train_i, val_i)","e124aa34":"# \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0432\u044b\u0432\u043e\u0434\u0438\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430\nmodel.eval()\n\nreal = []\npreds = []\n\nfor batch in iter(val_i):\n    x = batch.text.cuda()\n    real += batch.target.data.numpy().tolist()\n    preds += torch.sigmoid(model.forward(x).view(-1)).cpu().data.numpy().tolist()\n\nprint(classification_report(real, np.round(np.array(preds))))","6a66f312":"## Word2Vec & words clasterization","d318f5bd":"## Training LSTM with pre-trained Word2Vec","c6c045e6":"## Dowloading pre-trained Word2Vec","c9995b01":"## Download, explore and preprocess data","80a128dd":"# LSTM with pre-trained Word2Vec for Stack Overflow questions ","a42cdf63":"* \u0412 \u0446\u0435\u043b\u043e\u043c \u0432\u0438\u043d\u043e, \u0447\u0442\u043e \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043a\u0430 \u043d\u0435\u0443\u0434\u043e\u0432\u043b\u0435\u0442\u0432\u043e\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f, \u0438 \u043e\u0434\u0438\u043d \u0438\u0437 \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043d\u0435 \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0451\u0442\u0441\u044f \u0432\u043e\u0432\u0441\u0435. \u0422\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0435\u0449\u0451 \u043f\u043e\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438 \u0441\u0430\u043c\u043e\u0439 \u0441\u0435\u0442\u0438 (\u0432 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0435)..."}}