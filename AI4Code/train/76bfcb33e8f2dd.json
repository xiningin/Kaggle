{"cell_type":{"f92fa2ca":"code","b52c1809":"code","3e102e1a":"code","57c6e6fa":"code","024cb821":"code","4251402e":"code","6b1298d2":"code","f4774fb3":"code","28656f0c":"code","19bf6d6a":"code","f37eb729":"code","36582917":"code","d75ac8ad":"code","19daca9e":"code","70af3d5e":"code","8cda803a":"code","7bad3ec4":"code","4edfeb8b":"code","3f5cea49":"code","110a90d7":"code","ceaf267b":"code","79b233b7":"code","04227e59":"code","82719ebc":"code","412ac59e":"code","e1f7eb58":"code","08f8b088":"code","d9d0da4b":"code","a8fda03c":"code","1ecd0a4f":"code","f6898ad0":"code","1ccd639f":"code","b196631e":"code","de2606e4":"code","0a21cd96":"code","102300ff":"code","ed18d916":"code","cc18308c":"code","02f3edd0":"code","7a3933ed":"markdown","4c8255bf":"markdown","a2ab1052":"markdown","618043bb":"markdown","6c9bf86f":"markdown","1c89fb78":"markdown","9cef4bff":"markdown","a3e2d7e5":"markdown","f0a0e127":"markdown","f1234fe1":"markdown","10bab680":"markdown"},"source":{"f92fa2ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b52c1809":"!pip install -qq transformers","3e102e1a":"import transformers\nimport torch\nimport torch.nn as nn","57c6e6fa":"from tqdm import tqdm\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","024cb821":"DEVICE = \"cuda\"\nMAX_LEN=64\nTRAIN_BATCH_SIZE=64\nVALID_BATCH_SIZE=64\nTEST_BATCH_SIZE=64\nEPOCHS=2\nTRAINING_FILE = \"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\"\nMODEL_PATH = \".\/model_BERT.bin\"\nBERT_PRE_TRAINED_MODEL = \"bert-base-uncased\"\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PRE_TRAINED_MODEL)","4251402e":"dfx = pd.read_csv(TRAINING_FILE).fillna(\"none\")","6b1298d2":"print(dfx.head())\nprint(dfx.shape)","f4774fb3":"class_names = list(np.unique(dfx.sentiment))\nprint(class_names)\nprint(len(class_names))","28656f0c":"import seaborn as sns\nsns.countplot(dfx.sentiment)","19bf6d6a":"dfx.sentiment = dfx.sentiment.apply(lambda x: 1 if x == \"positive\" else 0)","f37eb729":"print(dfx.info())","36582917":"dfx_sample1, dfx_sample2 = model_selection.train_test_split(dfx, test_size=0.8,random_state=42, \n                                                           stratify=dfx.sentiment.values)\nprint(dfx_sample1.shape)\nprint(dfx_sample2.shape)","d75ac8ad":"df_train, df_valid = model_selection.train_test_split(dfx_sample1, test_size=0.2,random_state=42,stratify=dfx_sample1.sentiment.values)\n#df_train, df_valid = model_selection.train_test_split(dfx, test_size=0.2,random_state=42,stratify=dfx.sentiment.values)\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\nprint(df_train.shape)\nprint(df_valid.shape)","19daca9e":"df_valid, df_test = model_selection.train_test_split(df_valid, test_size=0.5,random_state=42, \n                                                      stratify=df_valid.sentiment.values)\ndf_valid = df_valid.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\nprint(df_valid.shape)\nprint(df_test.shape)","70af3d5e":"class IMDBDataset:\n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = TOKENIZER\n        self.max_len = MAX_LEN\n\n    def __len__(self):\n        return len(self.reviews)\n\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        \n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n        }","8cda803a":"train_dataset = IMDBDataset(reviews=df_train.review.values, targets=df_train.sentiment.values,tokenizer=TOKENIZER,\n                            max_len=MAX_LEN)\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4)","7bad3ec4":"len(train_dataset)","4edfeb8b":"len(train_data_loader)","3f5cea49":"valid_dataset = IMDBDataset(reviews=df_valid.review.values, targets=df_valid.sentiment.values,tokenizer=TOKENIZER,\n                            max_len=MAX_LEN)\nvalid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1)","110a90d7":"len(valid_dataset)","ceaf267b":"len(valid_data_loader)","79b233b7":"class BERTBaseUncasedClassifier(nn.Module):\n    def __init__(self,n_classes):\n        super(BERTBaseUncasedClassifier, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PRE_TRAINED_MODEL)\n        self.bert_drop = nn.Dropout(0.3) #Regularization\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) #Fully connected Layer\n\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            return_dict=False\n        )\n        output = self.bert_drop(pooled_output)\n        return self.out(output)","04227e59":"device = torch.device(DEVICE)\nmodel = BERTBaseUncasedClassifier(len(class_names))\nmodel.to(device)","82719ebc":"optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nprint(\"Length of Train Data Loader: \",len(train_data_loader))\nprint(\"Total Steps: \",total_steps)\nprint(\"Epochs: \",EPOCHS)\n\nscheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\nloss_fn = nn.CrossEntropyLoss().to(device)","412ac59e":"def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    for bi, d in tqdm(enumerate(data_loader),total=len(data_loader)):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","e1f7eb58":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader),total=len(data_loader)):\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n              )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n\n    return correct_predictions.double() \/ n_examples, np.mean(losses)","08f8b088":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom collections import defaultdict\n\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}\/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n    model,\n    valid_data_loader,\n    loss_fn,\n    device,\n    len(df_valid)\n    )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    checkpoint = {'model': BERTBaseUncasedClassifier(len(class_names)),\n              'state_dict': model.state_dict(),\n              'optimizer' : optimizer.state_dict()}\n    \n    torch.save(checkpoint, 'checkpoint.pth')\n           \n    if val_acc > best_accuracy:\n        #torch.save(model.state_dict(), 'best_model_state.bin')\n        torch.save(model.state_dict(), MODEL_PATH)\n        best_accuracy = val_acc","d9d0da4b":"import matplotlib.pyplot as plt\nplt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","a8fda03c":"test_dataset = IMDBDataset(reviews=df_test.review.values, targets=df_test.sentiment.values,tokenizer=TOKENIZER,\n                            max_len=MAX_LEN)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=4)","1ecd0a4f":"print(len(test_dataset))\nprint(len(test_data_loader))","f6898ad0":"def get_predictions(model, data_loader):\n    model = model.eval()\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"review_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            real_values.extend(targets)\n        predictions = torch.stack(predictions).cpu()\n        prediction_probs = torch.stack(prediction_probs).cpu()\n        real_values = torch.stack(real_values).cpu()\n    return review_texts, predictions, prediction_probs, real_values","1ccd639f":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(model,test_data_loader)","b196631e":"print(len(y_review_texts),len(y_pred),len(y_pred_probs),len(y_test))","de2606e4":"from sklearn.metrics import confusion_matrix, classification_report\nprint(classification_report(y_test, y_pred, target_names=class_names))","0a21cd96":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","102300ff":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model_load = checkpoint['model']\n    model_load.load_state_dict(checkpoint['state_dict'])\n    for parameter in model_load.parameters():\n        parameter.requires_grad = False\n    \n    #model.eval()\n    \n    return model_load","ed18d916":"model_load = load_checkpoint('checkpoint.pth')\nprint(model_load)","cc18308c":"model_load = model_load.to(device)","02f3edd0":"review_text = \"I hate love working on BERT!!\"\n\nencoded_review = TOKENIZER.encode_plus(\n  review_text,\n  max_length=MAX_LEN,\n  add_special_tokens=True,\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',\n)\n\nraw_input_ids = encoded_review['input_ids'].to(device)\nraw_attention_mask = encoded_review['attention_mask'].to(device)\nraw_output = model_load(raw_input_ids, raw_attention_mask)\n_, raw_prediction = torch.max(raw_output, dim=1)\nprint(f'Review text: {review_text}')\nprint(f'Sentiment  : {class_names[raw_prediction]}')","7a3933ed":"# Training Function","4c8255bf":"# BERT Training process","a2ab1052":"# DataLoader","618043bb":"# BERT Preprocessing of Input dataset - Tokenization of data into Input IDs and Attention Mask","6c9bf86f":"# Define Cofigurations ","1c89fb78":"# Evaluation Function","9cef4bff":"# Test Dataset Loader","a3e2d7e5":"# AdamW Optimizer","f0a0e127":"# Convert sentiments into integer 1 and 0","f1234fe1":"# BERT Classifier Transformer","10bab680":"# Splitting dataset into Train-Validation-Test"}}