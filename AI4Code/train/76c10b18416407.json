{"cell_type":{"3c9608a2":"code","71659070":"code","f73604c0":"code","d875eb45":"code","798e134c":"code","57a03594":"code","65a86d2d":"code","7554ebd7":"code","974d759d":"code","f58e454b":"code","98d011f9":"code","e442dbb6":"code","ed772a18":"code","7666da4f":"code","19295e45":"code","b2f195f3":"code","265e20cb":"code","0d3f0ab7":"code","cf6854d2":"code","0786b4d6":"code","985496c2":"code","a3e05063":"code","7ad71ff3":"markdown","e27ed7c7":"markdown","31371521":"markdown","3505c860":"markdown","e9718fcb":"markdown","4d939c94":"markdown","245d9655":"markdown","d808c46e":"markdown","7c5676fd":"markdown"},"source":{"3c9608a2":"import torch\ncuda_version_major = int(torch.version.cuda.split('.')[0])\n\n!git clone --depth=1 https:\/\/github.com\/airctic\/icevision.git\n%cd icevision\n!pip install -e .[all,dev]\n!pre-commit install\n    \nimport IPython\nIPython.Application.instance().kernel.do_shutdown(True)","71659070":"import IPython\nIPython.Application.instance().kernel.do_shutdown(True)","f73604c0":"# print(cuda_version_major)\nimport torch\nprint(torch.__version__)","d875eb45":"!pip install mmcv-full==\"1.3.17\" -f https:\/\/download.openmmlab.com\/mmcv\/dist\/11\/1.9.1\/index.html --upgrade\n!pip install mmdet","798e134c":"from icevision.all import *\n\nimport pandas as pd\nimport os\nimport ast\nimport numpy as np\n\nimport greatbarrierreef","57a03594":"INPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nINPUT_DIR_IMG = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'\nsys.path.append(INPUT_DIR)\nsys.path.append(INPUT_DIR_IMG)","65a86d2d":"TRAINING_RATIO = 0.8\n\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(data_df))\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\n# Shuffle both the training and validation datasets.\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', \n      float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df)))\nprint('Training ratio (positive samples):', \n      float(train_positive_count) \/ (train_positive_count + val_positive_count))","7554ebd7":"# Take only the positive images for training and validation\ntrain_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))\n\ntrain_data_df[\"annotations\"] = train_data_df[\"annotations\"].map(lambda x : ast.literal_eval(x))\n\ntrain_data_df[\"filepath\"] = train_data_df.apply(lambda x : f\"video_{x.video_id}\/{x.video_frame}.jpg\", axis=1)\n\ntrain_data_df.head(3)","974d759d":"train_data_df.head(3)","f58e454b":"HEIGHT, WIDTH = 720, 1280\npresize = 512\nsize = 384\n\ndf = train_data_df\ndf = df.explode(\"annotations\")\n\ndf[\"width\"] = [WIDTH]*len(df)\ndf[\"height\"] = [HEIGHT]*len(df)\ndf[\"label\"] = [\"starfish\"]*len(df)\n\ndf[\"xmin\"] = df.apply(lambda x : x.annotations[\"x\"], axis=1)\ndf[\"ymin\"] = df.apply(lambda x : x.annotations[\"y\"], axis=1)\ndf[\"xmax\"] = df.apply(lambda x : x.annotations[\"x\"]+x.annotations[\"width\"], axis=1)\ndf[\"ymax\"] = df.apply(lambda x : x.annotations[\"y\"]+x.annotations[\"height\"], axis=1)\n\ndf.loc[df[\"xmax\"] > 1280, \"xmax\"] = 1280\ndf.loc[df[\"ymax\"] > 720, \"ymax\"] = 720\n\ndf = df.drop([\"video_id\",\"sequence\",\"video_frame\",\"sequence_frame\",\n              \"image_id\",\"annotations\"], axis=1)\n\ndf = df.reset_index(drop=True)\ndf.head(3)","98d011f9":"df.head(10)","e442dbb6":"#Class template for creating custom parser in Icevision\ntemplate_record = ObjectDetectionRecord()\nParser.generate_template(template_record)","ed772a18":"class COTSParser(Parser):\n    def __init__(self, template_record, data_dir, df):\n        super().__init__(template_record=template_record)\n        \n        self.data_dir = data_dir\n        self.df = df\n        self.class_map = ClassMap(list(self.df['label'].unique()))\n        \n    def __iter__(self) -> Any:\n        for o in self.df.itertuples():\n            yield o\n            \n    def __len__(self) -> int:\n        return len(self.df)\n    \n    def record_id(self, o) -> Hashable:\n        return o.filepath\n    \n    def parse_fields(self, o, record, is_new):\n        if is_new:\n            record.set_filepath(os.path.join(self.data_dir,o.filepath))\n            record.set_img_size(ImgSize(width=o.width, height=o.height))\n            record.detection.set_class_map(self.class_map)\n\n        record.detection.add_bboxes([BBox.from_xyxy(o.xmin, o.ymin, o.xmax, o.ymax)])\n        record.detection.add_labels([o.label])","7666da4f":"parser = COTSParser(template_record, INPUT_DIR_IMG, df)\n\ntrain_records, valid_records = parser.parse()\nprint(parser.class_map)","19295e45":"#Augmentations using albumentation\ntrain_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()])\n\nvalid_tfms = tfms.A.Adapter([*tfms.A.resize_and_pad(size), tfms.A.Normalize()])\n\ntrain_ds = Dataset(train_records, train_tfms)\nvalid_ds = Dataset(valid_records, valid_tfms)\n\nsamples = [train_ds[0] for _ in range(5)]\nshow_samples(samples, ncols=3)","b2f195f3":"PATH = \"..\/input\/yolo-x-training-using-icevision\/COTS_yolo-sx.pth\"\nfine_tune = True #set false if running this for the first time\n\nmodel_type = models.mmdet.yolox #selecting the model\nbackbone = model_type.backbones.yolox_s_8x8(pretrained=True) #selecting backbone for the model\nmodel = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map)) #instantiate the model\n\nif fine_tune:\n    state_dict = torch.load(PATH)\n    model.load_state_dict(state_dict)","265e20cb":"train_dl = model_type.train_dl(train_ds, batch_size=1, shuffle=True)\nvalid_dl = model_type.valid_dl(valid_ds, batch_size=4, shuffle=False)","0d3f0ab7":"metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]","cf6854d2":"#finding the best prabable learning rate\nlearn = model_type.fastai.learner(dls=[train_dl, valid_dl], model=model, metrics=metrics)\nlearn.lr_find() ","0786b4d6":"#begin training\nlearn.fine_tune(5, 6.309573450380412e-07, freeze_epochs=1)","985496c2":"infer_dl = model_type.infer_dl([valid_ds[0],valid_ds[4],valid_ds[7],valid_ds[9]], batch_size=1, shuffle=False)\npreds = model_type.predict_from_dl(model, infer_dl, keep_images=True)\nshow_preds(preds=preds)","a3e05063":"torch.save(model.state_dict(), \"COTS_yolo-sx2.pth\")","7ad71ff3":"### Creating [custom parser](https:\/\/airctic.com\/dev\/custom_parser\/) for the COTS dataset","e27ed7c7":"# **Imports**","31371521":"# **Data Loader**","3505c860":"**GitHub:[IceVision Framework](https:\/\/github.com\/airctic\/icevision)**\n\n**Documentation: [IceVision Docs](https:\/\/airctic.com\/dev\/)**","e9718fcb":"# **Data Parser**","4d939c94":"# **Prepare Training dataset**","245d9655":"# **Defining Model**","d808c46e":"# **Installation**","7c5676fd":"# **Model Training**"}}