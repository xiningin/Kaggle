{"cell_type":{"76c67349":"code","02ffbf81":"code","8eed36fb":"code","22a5f8fa":"code","39254461":"code","45191aa6":"code","8aeed20f":"code","93f29f00":"code","9bc2aa81":"code","ab8d82b0":"code","d45609d5":"code","024a6322":"code","a49b0ed0":"code","0c7fb58a":"code","8867abf8":"code","9616335c":"code","6d2c5a26":"code","d85a7baf":"code","de9661b8":"code","08ec478d":"code","ccc8fe8c":"code","e2387dc3":"code","209a7b4e":"code","e7ba9047":"code","bd6d3e06":"code","91ddc3d3":"markdown","43866479":"markdown","04c74d03":"markdown","33f182aa":"markdown","8119f656":"markdown","c4d76d47":"markdown","acb4c27f":"markdown","dcd565c4":"markdown","7d2f3013":"markdown","881224ea":"markdown","62c99eed":"markdown","8d2146ac":"markdown","bf49dfdb":"markdown","586b8ff5":"markdown","263e2cc1":"markdown","0a8302e9":"markdown","30b7b0f5":"markdown","c88dc1dd":"markdown","20021d18":"markdown","46ee856f":"markdown","84a69791":"markdown","3ebafcf6":"markdown","398e7e1f":"markdown","1c6253f9":"markdown"},"source":{"76c67349":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, confusion_matrix","02ffbf81":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\n\ncount_row = df.shape[0]  # gives number of row count\ncount_column = df.shape[1] # gives number of column count\n\nprint('Number of rows: {}'.format(count_row))\nprint('Number of columns: {}'.format(count_column))\n\ndf.head()","8eed36fb":"df.describe()","22a5f8fa":"plt.figure(figsize=(10,10))\np=sns.heatmap(df.corr(), annot=True,cmap='RdYlGn',square=True, fmt='.2f')","39254461":"# Check for missing data\ndf.isnull().sum() # gives number of null value by column","45191aa6":"from sklearn.preprocessing import MinMaxScaler\n\nX, y = df.drop(['target'], axis=1), df['target']\n\nscaler = MinMaxScaler()\n#scaler = StandardScaler()\n\n# Optionnal\nX = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)\nX.head()","8aeed20f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","93f29f00":"from sklearn.neighbors import KNeighborsClassifier\n\nn = 5\n\nclf = KNeighborsClassifier(n)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\nprint('Test set predictions: {}'.format(preds))\nprint('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","9bc2aa81":"training_accuracy = []\ntest_accuracy = []\n\nn_range = range(1, 11)\n\nfor n in n_range:\n    clf = KNeighborsClassifier(n)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    test_accuracy.append(clf.score(X_test, y_test))\n    \nplt.plot(n_range, training_accuracy, label='training accuracy')\nplt.plot(n_range, test_accuracy, label='test accuracy')\nplt.xlabel('n neighbors')\nplt.ylabel('accuracy')\nplt.legend()","ab8d82b0":"from sklearn.svm import SVC\n\nclf = SVC(kernel=\"linear\", C=1)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)\n\nplt.bar(range(X_train.shape[1]), clf.coef_[0], align = 'center', alpha = 0.5)\nplt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\nxlims = plt.xlim()\nplt.hlines(0, xlims[0], xlims[1])","d45609d5":"training_accuracy = []\ntest_accuracy = []\n\nC_range = np.logspace(-3, 3, num=7)\n\nfor C in C_range:\n    clf = SVC(kernel=\"linear\", C=C)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    test_accuracy.append(clf.score(X_test, y_test))\n    \nplt.plot(C_range, training_accuracy, label='training accuracy')\nplt.plot(C_range, test_accuracy, label='test accuracy')\nplt.xlabel('C')\nplt.xscale(\"log\")\nplt.ylabel('accuracy')\nplt.legend()","024a6322":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nclf = LinearDiscriminantAnalysis()\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","a49b0ed0":"from sklearn.naive_bayes import GaussianNB\n\nclf = GaussianNB()\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","0c7fb58a":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C=1)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)\n\nplt.bar(range(X_train.shape[1]), clf.coef_[0], align = 'center', alpha = 0.5)\nplt.xticks(range(X_train.shape[1]), X_train.columns, rotation=90)\nxlims = plt.xlim()\nplt.hlines(0, xlims[0], xlims[1])","8867abf8":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier(max_depth=3, min_samples_leaf=11)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","9616335c":"training_accuracy = []\ntest_accuracy = []\n\nmax_depth_range = range(1, 10)\n\nfor max_depth in max_depth_range:\n    clf = DecisionTreeClassifier(max_depth=max_depth)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    test_accuracy.append(clf.score(X_test, y_test))\n    \nplt.plot(max_depth_range, training_accuracy, label='training accuracy')\nplt.plot(max_depth_range, test_accuracy, label='test accuracy')\nplt.xlabel('max depth')\nplt.ylabel('accuracy')\nplt.legend()","6d2c5a26":"training_accuracy = []\ntest_accuracy = []\n\nmin_samples_leaf_range = range(15, 1, -1)\n\nfor min_samples_leaf in min_samples_leaf_range:\n    clf = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    test_accuracy.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\n\nplt.plot(min_samples_leaf_range, training_accuracy, label='training accuracy')\nplt.plot(min_samples_leaf_range, test_accuracy, label='test accuracy')\nplt.xlabel('min samples leaf')\nplt.ylabel('accuracy')\nax.set_xlim(15, 0)\nplt.legend()","d85a7baf":"from sklearn.tree import export_graphviz\n\nexport_graphviz(clf, out_file='tree.dot', class_names=['0', '1'], feature_names=df.columns[:-1], impurity=False, filled=True)","de9661b8":"import graphviz\n\nwith open('tree.dot') as f:\n    dot_graph = f.read()\ndisplay(graphviz.Source(dot_graph))","08ec478d":"def plot_feature_importances(model):\n    n_features = X_train.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), X_train.columns)\n    plt.xlabel('Feature importance')\n    plt.ylabel('Feature')\n    plt.ylim(-1, n_features)\n    \nplot_feature_importances(clf)","ccc8fe8c":"from sklearn.linear_model import Perceptron\n\nclf = Perceptron(tol=1e-3)\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","e2387dc3":"class MyPerceptron():  \n    \"\"\"My own implementation of the perceptron\"\"\"\n\n    def __init__(self, n_epochs=5, lr=0.01):\n        \"\"\"\n        Called when initializing the classifier\n        \"\"\"\n        self.n_epochs = n_epochs\n        self.lr = lr\n\n    def fit(self, X, Y=None):\n        \"\"\"\n        Fit classifier\n        \"\"\"\n        assert (type(self.n_epochs) == int), \"n_epochs parameter must be integer\"\n        assert (type(self.lr) == float), \"lr parameter must be float\"\n\n        self.weights = np.random.rand(X.shape[1] + 1)\n        \n        for epoch in range(self.n_epochs):\n            for x, y in zip(X.values, Y.values):\n                pred = self.predict(x)\n                self.weights[1:] += self.lr * (y - pred) * x\n                self.weights[0] += self.lr * (y - pred)\n\n        return self\n\n    def predict(self, X, Y=None):\n        \"\"\"\n        Make predictions\n        \"\"\"\n        try:\n            getattr(self, \"weights\")\n        except AttributeError:\n            raise RuntimeError(\"You must train classifer before making predictions!\")\n\n        return(np.heaviside(np.dot(X, self.weights[1:]) + self.weights[0], 1))\n\n    def score(self, X, Y=None):\n        \"\"\"\n        Compute accuracy score\n        \"\"\"\n        return(accuracy_score(Y, self.predict(X))) ","209a7b4e":"clf = MyPerceptron()\n\nclf.fit(X_train, y_train)\n\npreds = clf.predict(X_test)\n#print('Test set predictions: {}'.format(preds))\n#print('\\n')\n\n#print('Train set accuracy: {:.2f}'.format(accuracy_score(y_train, clf.predict(X_train))))\n#print('Test set accuracy: {:.2f}'.format(accuracy_score(y_test, clf.predict(X_test))))\nprint('Train set accuracy: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Test set accuracy: {:.2f}'.format(clf.score(X_test, y_test)))\nprint('\\n')\n\nconfusion = confusion_matrix(y_test, preds)\n\nprint('Confusion matrix:\\n', confusion)","e7ba9047":"training_accuracy = []\ntest_accuracy = []\n\nn_epochs_range = range(1, 50)\n\nfor n_epochs in n_epochs_range:\n    clf = MyPerceptron(n_epochs=n_epochs)\n    clf.fit(X_train, y_train)\n    training_accuracy.append(clf.score(X_train, y_train))\n    test_accuracy.append(clf.score(X_test, y_test))\n\nfig, ax = plt.subplots()\n\nplt.plot(n_epochs_range, training_accuracy, label='training accuracy')\nplt.plot(n_epochs_range, test_accuracy, label='test accuracy')\nplt.xlabel('n epochs')\nplt.ylabel('accuracy')\nplt.legend()","bd6d3e06":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(clf, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","91ddc3d3":"Find the best value of C","43866479":"## Split train\/test","04c74d03":"## Missing data handling","33f182aa":"### Own implementation","8119f656":"## Naive Bayes","c4d76d47":"## Decision tree\n\n### Predictive model","acb4c27f":"### Inflence of _max tree depth_","dcd565c4":"### Influence of _n epochs_","7d2f3013":"Basic statistics","881224ea":"Find the best value for _n_","62c99eed":"# Train models\n\n## k-NN","8d2146ac":"# Get the data","bf49dfdb":"Heatmap of correlations","586b8ff5":"## Normalize\n\nUseful for:\n- all distance-based classifiers: k-NN, SVM\n- LDA (which assumes that each input variable has the same variance)","263e2cc1":"# Permutation importance\n\n(experimental)","0a8302e9":"## Logistic regression","30b7b0f5":"## Neural Network\n\n### Scikit-learn implementation","c88dc1dd":"# Preprocess the data","20021d18":"## LDA","46ee856f":"## Linear SVM","84a69791":"### Display of tree","3ebafcf6":"### Features importance","398e7e1f":"### Influence of _min samples leaf_","1c6253f9":"Imports"}}