{"cell_type":{"bcd03ea3":"code","aac165fe":"code","256edaaa":"code","aa51bd2b":"code","cacf0100":"code","600975ea":"code","4dd2b674":"code","ed13a94a":"code","12edcc83":"code","a9d26774":"code","66476ce4":"code","e3e02525":"code","f151af67":"code","cd56d682":"code","dabc44b0":"code","a52e9cb7":"code","4f87d866":"code","a575c0cb":"code","eed4512a":"code","e579f870":"code","e0a6d460":"code","303fc03c":"code","4e1052d0":"code","1dc1d40f":"code","e8661577":"code","e0d685e7":"code","86cc56b7":"code","f183d4a6":"code","179f0311":"code","50e93706":"code","2ab2d985":"code","24f0749d":"code","e5882781":"code","5329bd81":"code","791b37a0":"code","c5a7c4c3":"code","f2eb7b41":"code","015119ae":"code","fecf7f51":"code","cb4434a9":"code","7d8ed2e6":"code","9b1a55c0":"code","b0a6e595":"code","d6c8ff01":"code","ed662dca":"code","20afd17d":"code","7521a8a3":"code","2a2eaf6b":"code","5d916553":"code","7319a70d":"code","6a75d2eb":"code","8ede235f":"code","3e356faa":"code","2fead8dc":"code","0c4a5418":"code","b4c3b15d":"code","40f8516d":"code","57585c61":"code","1605ff1e":"markdown","6b271de7":"markdown","1a8afe35":"markdown","a878a58d":"markdown","a0903848":"markdown","f00e419a":"markdown","d3bae8a2":"markdown","a8cab0cf":"markdown","e5552a13":"markdown","17322eca":"markdown","a513e820":"markdown","ad9fac5a":"markdown","e23bd4b4":"markdown","fd6117fe":"markdown"},"source":{"bcd03ea3":"\n\n# Update necessary packages first\n!pip3 uninstall --yes fbprophet\n!pip3 install fbprophet --no-cache-dir --no-binary :all:\n!pip3 install pydotplus --no-cache-dir --no-binary :all:","aac165fe":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as mdates \nimport datetime\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing\nfrom scipy.spatial import distance\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom sklearn.metrics import mean_squared_error\nimport bq_helper\nimport warnings\nfrom fbprophet.plot import add_changepoints_to_plot\nfrom sklearn.linear_model import LinearRegression\nfrom numpy import mean,absolute\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import t\nfrom fbprophet import Prophet\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\nwarnings.filterwarnings(\"ignore\")","256edaaa":"#get weather data for O'Hare airport in Chicago\nnoaa_data_set = bq_helper.BigQueryHelper(active_project= \"bigquery-public-data\",\n                                        dataset_name= \"noaa_gsod\")\n\nbase_query = \"\"\"\nSELECT\n    CAST(CONCAT(year,'-',mo,'-',da) AS date) AS date,\n    temp,\n    wdsp,\n    max AS max_temp,\n    min AS min_temp,\n    prcp,\n    sndp AS snow_depth,\n    fog,\n    rain_drizzle,\n    snow_ice_pellets,\n    hail,\n    thunder,\n    tornado_funnel_cloud\nFROM\n\"\"\"\n\nwhere_clause = \"\"\"\nWHERE stn='725300'\n\"\"\"\ntables=[\n    \"`bigquery-public-data.noaa_gsod.gsod2019`\",\n    \"`bigquery-public-data.noaa_gsod.gsod2018`\",\n    \"`bigquery-public-data.noaa_gsod.gsod2017`\",\n    \"`bigquery-public-data.noaa_gsod.gsod2016`\",\n    \"`bigquery-public-data.noaa_gsod.gsod2015`\",\n    \"`bigquery-public-data.noaa_gsod.gsod2014`\"]\n\nfor t in range(len(tables)):\n    if t==0:\n        query = \"{0} {1} {2} \\n\".format(base_query,tables[t],where_clause)\n    else:\n        query+=\"UNION ALL \\n {0} {1} {2}\".format(base_query,tables[t],where_clause)\n\nweather_data= noaa_data_set.query_to_pandas_safe(query, max_gb_scanned=2.0)\nweather_data['date']=pd.to_datetime(weather_data['date'])","aa51bd2b":"dfWeather = weather_data.copy()\ndfViolations = pd.read_csv('..\/input\/chicago-red-light-and-speed-camera-data\/red-light-camera-violations.csv')","cacf0100":"#Create Ohio University color dictionary\nouColors = {'Cutler Green': '#00694E',\n            'Cupola White': '#ffffff',\n            'Putnam': '#756e65',\n            'Under the Elms': '#024230',\n            'Gateway': '#514c47',\n            'Wheat': '#fccd06',\n            'PawPaw': '#e0dc4b'}","600975ea":"#remove null values from the violations data set\noriginalViolations = len(dfViolations)\ndfViolations = dfViolations.dropna()\nremaining = len(dfViolations)\nrecordsWithMissingVal = originalViolations - remaining\n\nremoveNaText = f'{recordsWithMissingVal:,} of the original {originalViolations:,} records were removed because at least one value was missing. \\\n                \\n{remaining:,} records remaining.'\n\nprint(removeNaText)","4dd2b674":"#remove duplicated rows from the dataframe\ndfViolations[dfViolations.duplicated()]","ed13a94a":"#remove the 6 duplicated rows from the dataframe\ndfViolations = dfViolations.drop(dfViolations[dfViolations.duplicated()].index)\n#make sure that all duplicated rows are gone\ndfViolations[dfViolations.duplicated()]","12edcc83":"#remove neighborhood identifier columns because they are not unique for each intersection\ncols = ['INTERSECTION', 'VIOLATION DATE', 'VIOLATIONS']\ndfViolations = dfViolations.groupby(['INTERSECTION','VIOLATION DATE']).sum()\ndfViolations = dfViolations.reset_index()\ndfViolations = dfViolations[cols]","a9d26774":"dfViolations.head()","66476ce4":"#Convert violation date to datetime format and add year, month and day of week columns\ndfViolations['VIOLATION DATE'] = pd.to_datetime(dfViolations['VIOLATION DATE'])\ndfViolations['year'] = dfViolations['VIOLATION DATE'].dt.year\ndfViolations['month'] = dfViolations['VIOLATION DATE'].dt.month\n#week starts on Monday\ndfViolations['dow'] = dfViolations['VIOLATION DATE'].dt.dayofweek\ndfViolations.set_index('VIOLATION DATE', inplace=True)","e3e02525":"#merge weather into the violations data frame\ndfWeather['date'] = pd.to_datetime(dfWeather['date'])\ndfWeather = dfWeather.set_index('date')\n\ndfViolationsWithWeather = dfViolations.merge(dfWeather, right_index=True, left_index=True)","f151af67":"#if the snow depth is equal to 999.9 replace it with 0\ndfViolationsWithWeather['snow_depth'] = dfViolationsWithWeather['snow_depth'].replace(999.9,0)\n\n#replace min_temp and max_temp = 9999.9 with temp\nmask = (dfViolationsWithWeather['min_temp']==9999.9) & (dfViolationsWithWeather['temp']!=9999.9)\ndfViolationsWithWeather['min_temp'][mask]=dfViolationsWithWeather['temp']\n\nmask = (dfViolationsWithWeather['max_temp']==9999.9) & (dfViolationsWithWeather['temp']!=9999.9)\ndfViolationsWithWeather['max_temp'][mask]=dfViolationsWithWeather['temp']\n\n#no tornado's were reported in during this timeframe by the NOAA dataset so the tornado column is dropped\ndfViolationsWithWeather.drop(columns=['tornado_funnel_cloud']\n                            , inplace=True)\ndfViolationsWithWeather = dfViolationsWithWeather.astype({'fog': 'int',\n                               'rain_drizzle': 'int',\n                               'snow_ice_pellets': 'int',\n                               'hail': 'int',\n                               'thunder': 'int',\n                                'wdsp': 'float64'})","cd56d682":"#create a multi index by Intersection and date\nsummaryColumns = ['VIOLATION DATE','year', 'month', 'dow', 'temp', 'wdsp', 'max_temp',\n       'min_temp', 'prcp', 'snow_depth', 'fog', 'rain_drizzle',\n       'snow_ice_pellets', 'hail', 'thunder']\n\ndfViolationsWithWeather = dfViolationsWithWeather.reset_index()\ndfViolationsWithWeather.rename(columns={\"index\": \"VIOLATION DATE\"}, inplace=True)\n\n#Create a consolidated dataframe for high level exploratory analysis\ndfGroupedViolations = dfViolationsWithWeather.groupby(summaryColumns)[['VIOLATIONS']].sum()\ndfGroupedViolations = dfGroupedViolations.reset_index()\ndfGroupedViolations = dfGroupedViolations.set_index(['VIOLATION DATE'])\n\ndfViolationsWithWeather = dfViolationsWithWeather.sort_values(by=['INTERSECTION', 'VIOLATION DATE'])\ndfViolationsWithWeather = dfViolationsWithWeather.set_index(['INTERSECTION','VIOLATION DATE'])","dabc44b0":"#add 7 day and 365 day moving averages to the summarized and full dataframes\ndfViolationsWithWeather['7Day'] = dfViolationsWithWeather.groupby(level=0)['VIOLATIONS'].rolling(window=7, center=False).mean().values\ndfViolationsWithWeather['365Day'] = dfViolationsWithWeather.groupby(level=0)['VIOLATIONS'].rolling(window=365, center=False).mean().values\ndfGroupedViolations['7Day'] = dfGroupedViolations['VIOLATIONS'].rolling(window=7, center=False).mean().values\ndfGroupedViolations['365Day'] = dfGroupedViolations['VIOLATIONS'].rolling(window=365, center=False).mean().values","a52e9cb7":"#unique values in the data frame\npd.DataFrame(dfViolationsWithWeather.nunique(), columns=['Count'])","4f87d866":"#data types in the data frame\npd.DataFrame(dfViolationsWithWeather.dtypes, columns=['DataType'])","a575c0cb":"pd.DataFrame(pd.DataFrame(list(dfViolationsWithWeather.index), columns=['Intersection', 'Date'])['Intersection'].value_counts())","eed4512a":"#how many unique intersections are in the dataset?\nnumIntersections = len(set(dfViolationsWithWeather.index.get_level_values(0)))\nnumRecords = len(dfViolationsWithWeather)\nstartDate = dfViolationsWithWeather.index.get_level_values(1).min().strftime('%m\/%d\/%Y')\nendDate = dfViolationsWithWeather.index.get_level_values(1).max().strftime('%m\/%d\/%Y')\n\nsummary = f'The cleaned dataset includes {numRecords:,} records for {numIntersections}'\nsummary += f' unique intersections collected between {startDate} and {endDate}'\n\nprint(summary)","e579f870":"#which cameras\/locations had the most violations?\ntop5Intersections = dfViolationsWithWeather.groupby(level=0)['VIOLATIONS'].sum().sort_values(ascending=False).head(5).index\npd.DataFrame(list(top5Intersections),columns=['Intersection'])","e0a6d460":"#Intersection share of total violations\ndfIntersections =(dfViolationsWithWeather.groupby(level=0)['VIOLATIONS'].sum().sort_values(ascending=False)\/\n                dfViolationsWithWeather['VIOLATIONS'].sum())*100\n\n#Violations are spread out between the intersections in Chicago, there isn't a small group that represents most of the violations\ndfIntersections.cumsum()[:90].reset_index()","303fc03c":"corrCols = ['temp', 'wdsp', 'prcp', 'snow_depth']\n\nfig, axs = plt.subplots(4, figsize=(25,30))\nfor f, feature in enumerate(corrCols):\n    corr = dfGroupedViolations['VIOLATIONS'].corr(dfGroupedViolations[feature])\n\n    axs[f].plot(dfGroupedViolations['VIOLATIONS'],dfGroupedViolations[feature],'o', color=ouColors['Cutler Green'])\n    axs[f].set_xlabel('Violations')\n    axs[f].set_ylabel(feature)\n    axs[f].set_title(f'Correlation between violations and {feature} is: {corr:.2}')\n\nplt.show()\nplt.savefig('correlation.png')","4e1052d0":"boxplotCols = ['month','dow']\n\nfig, ax = plt.subplots(2, figsize=(12,10))\n\nfor f,feature in enumerate(boxplotCols):\n    sns.boxplot(x=feature, y='VIOLATIONS', data = dfGroupedViolations, ax=ax[f])\n    ax[f].set_title(f'Box Plot of violations by {feature}')\nplt.show()","1dc1d40f":"boxplotCols = ['fog','rain_drizzle','snow_ice_pellets','hail','thunder']\nfig, ax = plt.subplots(5, figsize=(12,20))\n\nfor f,feature in enumerate(boxplotCols):\n    sns.boxplot(y=feature, x='VIOLATIONS', data = dfGroupedViolations, orient='h', ax=ax[f])\n    ax[f].set_title(f'Box Plot of violations by {feature}')\nplt.subplots_adjust(bottom=0.08, hspace=0.25)\nplt.show()","e8661577":"#Plot the summary of 7 day rolling average and 365 day rolling average against the daily violations\nfig, ax = plt.subplots(figsize = (20,7))# plotting daily data\nax.plot(dfGroupedViolations['VIOLATIONS'], marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\nax.plot(dfGroupedViolations['7Day'], color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\nax.plot(dfGroupedViolations['365Day'], color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\nax.xaxis.set_major_locator(mdates.YearLocator())\nax.legend()\nax.set_xlabel('Year')\nax.set_ylabel('Violations')\nax.set_title('Trends in Red Light Violations')\n\nplt.show()","e0d685e7":"fig, axs = plt.subplots(5, figsize=(20,20))\nfor i, intersection in enumerate(top5Intersections):\n    axs[i].plot(dfViolationsWithWeather['VIOLATIONS'][intersection], marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\n    axs[i].plot(dfViolationsWithWeather['7Day'][intersection], color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\n    axs[i].plot(dfViolationsWithWeather['365Day'][intersection], color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\n    axs[i].xaxis.set_major_locator(mdates.YearLocator())\n    axs[i].legend()\n    axs[i].set_xlabel('Year')\n    axs[i].set_ylabel('Violations')\n    axs[i].set_title(f'{top5Intersections[i].title()} Red Light Violations')\n    \nplt.subplots_adjust(bottom=0.08, hspace=0.25)\nplt.show()","86cc56b7":"cols = ['year','month','dow','snow_ice_pellets','fog','snow_depth','wdsp','temp','VIOLATIONS']\nX = preprocessing.normalize(dfViolationsWithWeather[cols], norm='l1', axis=0)\nX_Grouped = preprocessing.normalize(dfGroupedViolations[cols], norm='l1', axis=0)","f183d4a6":"distortions = [] \ninertias = [] \nmapping1 = {} \nmapping2 = {} \nK = range(1,11) \n  \nfor k in K: \n    #Building and fitting the model \n    kmeanModel = KMeans(init='random',n_clusters=k, random_state=0).fit(X) \n    kmeanModel.fit(X)     \n      \n    distortions.append(sum(np.min(distance.cdist(X, kmeanModel.cluster_centers_, \n                      'euclidean'),axis=1)) \/ X.shape[0]) \n    inertias.append(kmeanModel.inertia_) \n  \n    mapping1[k] = sum(np.min(distance.cdist(X, kmeanModel.cluster_centers_, \n                 'euclidean'),axis=1)) \/ X.shape[0] \n    mapping2[k] = kmeanModel.inertia_ ","179f0311":"plt.figure(figsize=(10,5))\n\nplt.subplot(121)\nplt.plot(K, distortions, 'o-') \nplt.xlabel('Values of K') \nplt.ylabel('Distortion') \nplt.title('The Elbow Method using Distortion') \n\nplt.subplot(122)\nplt.plot(K, inertias, 'o-') \nplt.xlabel('Values of K') \nplt.ylabel('Inertia') \nplt.title('The Elbow Method using Inertia') \nplt.tight_layout()\n\n","50e93706":"cluster = KMeans(n_clusters=4, random_state=0).fit(X)\n\ndfViolationsWithWeather['clusters']=cluster.labels_\n\ncluster1 = dfViolationsWithWeather[dfViolationsWithWeather['clusters']==1]['VIOLATIONS'].reset_index()\ncluster1 = cluster1.groupby('VIOLATION DATE')['VIOLATIONS'].sum()\ncluster2 = dfViolationsWithWeather[dfViolationsWithWeather['clusters']==2]['VIOLATIONS'].reset_index()\ncluster2 = cluster2.groupby('VIOLATION DATE')['VIOLATIONS'].sum() #Winter Cluster\ncluster3 = dfViolationsWithWeather[dfViolationsWithWeather['clusters']==3]['VIOLATIONS'].reset_index()\ncluster3 = cluster3.groupby('VIOLATION DATE')['VIOLATIONS'].sum()\ncluster0 = dfViolationsWithWeather[dfViolationsWithWeather['clusters']==0]['VIOLATIONS'].reset_index()\ncluster0 = cluster0.groupby('VIOLATION DATE')['VIOLATIONS'].sum() #Seasonal","2ab2d985":"groupedCluster = KMeans(n_clusters=4, random_state=0).fit(X_Grouped)\ndfGroupedViolations['clusters']=groupedCluster.labels_","24f0749d":"fig, axs = plt.subplots(figsize=(25,7))\n\naxs.plot(cluster0, marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\naxs.plot(cluster0, color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\naxs.plot(cluster0, color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\naxs.xaxis.set_major_locator(mdates.YearLocator())\naxs.legend()\naxs.set_xlabel('Year')\naxs.set_ylabel('Violations')\naxs.set_title(f'Cluster 0: Summer Good Weather')\n\nplt.show()","e5882781":"fig, axs = plt.subplots(figsize=(25,7))\n\naxs.plot(cluster1, marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\n#axs.plot(cluster1, color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\n#axs.plot(cluster1, color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\naxs.xaxis.set_major_locator(mdates.YearLocator())\naxs.legend()\naxs.set_xlabel('Year')\naxs.set_ylabel('Violations')\naxs.set_title(f'Cluster 1: Warm')\n\nplt.show()","5329bd81":"fig, axs = plt.subplots(figsize=(25,7))\n\naxs.plot(cluster2, marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\n#axs.plot(cluster2, color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\n#axs.plot(cluster2, color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\naxs.xaxis.set_major_locator(mdates.YearLocator())\naxs.legend()\naxs.set_xlabel('Year')\naxs.set_ylabel('Violations')\naxs.set_title(f'Cluster 2: Winter no weather')\n\nplt.show()","791b37a0":"fig, axs = plt.subplots(figsize=(25,7))\n\naxs.plot(cluster3, marker='.', markersize=2, color=ouColors['Putnam'],linestyle='None', label='Daily')\n#axs.plot(cluster3, color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\n#axs.plot(cluster3, color=ouColors['Gateway'], linewidth=3, label='Trend (365DayRolling)')\n\n\naxs.xaxis.set_major_locator(mdates.YearLocator())\naxs.legend()\naxs.set_xlabel('Year')\naxs.set_ylabel('Violations')\naxs.set_title(f'Cluster 3: Winter Bad Weather')\n\nplt.show()","c5a7c4c3":"# boxplotCols = ['month','dow']\n#'fog','rain_drizzle','snow_ice_pellets','hail','thunder']\nfig, ax = plt.subplots(2, figsize=(10,4))\n\nplt.subplot(121)\nsns.boxplot(x='clusters', y='temp', data = dfViolationsWithWeather)\nplt.title('Clusters by average temperature')\n\nplt.subplot(122)\nsns.boxplot(x='clusters', y='month', data = dfViolationsWithWeather)\nplt.title('Clusters by month')\n#plt.tight_layout()\nplt.show()\n","f2eb7b41":"#get the mean squared error for the simple average to compare each model too. Models should be better than the simplest form of prediction.\nmeanViolations = dfGroupedViolations['VIOLATIONS'].mean()\nbaselineError = mean_squared_error(dfGroupedViolations['VIOLATIONS'].values, np.full((1970,),meanViolations))\nbaselineError","015119ae":"#Create functions for determining the accuracy of the models\ndef model_performance(X_train,y_train,X_test,y_test,model):\n    trainingScores = pd.DataFrame(y_train.copy(), columns=['Actual'])\n    trainingScores['Prediction'] = model.predict(X_train)\n    trainingScores['Residual'] = trainingScores['Actual']-trainingScores['Prediction']\n\n    validationScores = pd.DataFrame(y_test.copy(), columns=['Actual'])\n    validationScores['Prediction'] = model.predict(X_test)\n    validationScores['Residual'] = validationScores['Actual']-validationScores['Prediction']\n\n    data = {'SSE': [sum(trainingScores['Residual']**2),sum(validationScores['Residual']**2)],\n            'MSE': [mean_squared_error(trainingScores['Actual'], trainingScores['Prediction']), mean_squared_error(validationScores['Actual'], validationScores['Prediction'])],\n            'RMSE': [mean_squared_error(trainingScores['Actual'], trainingScores['Prediction'])**.5, mean_squared_error(validationScores['Actual'], validationScores['Prediction'])**.5],\n            'MAD': [mean(absolute(trainingScores['Residual'])),mean(absolute(validationScores['Residual']))],\n            'R2': [r2_score(trainingScores['Actual'], trainingScores['Prediction']), r2_score(validationScores['Actual'], validationScores['Prediction'])]}\n\n    df_summary = pd.DataFrame.from_dict(data, orient='index', columns=['Training', 'Validation'])\n    df_summary['%Difference'] = (df_summary['Training']-df_summary['Validation'])\/df_summary['Training']*100\n    return df_summary\n\ndef steyx(y,x):\n    \"\"\"Determines the standard error of the predicted y value for each actual y or\n    the measure of the amount of error in the prediction of y for an individual x\"\"\"\n    y_mean = mean(y)\n    x_mean = mean(x)\n\n    sumYSquare = sum((y-mean(y))**2)\n    sumXSquare = sum((x-mean(x))**2)\n\n    sumDiffSquare = sum((y-mean(y))*(x-mean(x)))**2\n\n    dof = len(y)-2\n\n    return ((1\/dof)*(sumYSquare-(sumDiffSquare\/sumXSquare)))**.5","fecf7f51":"#60\/40 train test split\n#determine number of records for a 60\/40 train\/test split\nsplit = int(len(dfGroupedViolations)*.6)\n#create dummy variables for any categorical variables left in dataset\nxCols = ['year','month','dow','temp','wdsp','max_temp','min_temp','prcp','snow_depth','fog','rain_drizzle','snow_ice_pellets','hail','thunder','clusters']\nX = dfGroupedViolations[xCols]\ny = dfGroupedViolations['VIOLATIONS']\n\nX_train = X[0:split]\ny_train = y[0:split].ravel()\nX_test = X[split:]\ny_test = y[split:].ravel()\n\nregr = LinearRegression()\nregr.fit(X_train,y_train)\npred = regr.predict(X_test)\n\n#Display the coefficients for the linear regression model\npredictor = ['Intercept']+ xCols\nlrCoefficient = pd.DataFrame(zip(predictor,regr.coef_.reshape(15)),columns=['Predictor','Estimate'])\nlrCoefficient","cb4434a9":"lr_forecast = regr.predict(X)\nlr_residual = y - lr_forecast\n\nfull_lr_performance = model_performance(X_train, y_train, X_test, y_test, regr)\nfull_lr_performance.round(2)","7d8ed2e6":"#step forward feature selection\nsfsl = sfs(regr,\n          k_features=7,\n          forward=True,\n          floating=False,\n          scoring='r2',\n          cv=0)\n\nsfsl = sfsl.fit(X_train, y_train)\nsf_Cols = list(sfsl.k_feature_names_)\nsf_Cols","9b1a55c0":"#Linear Regression with reduced columns\nX_reduced = dfGroupedViolations[sf_Cols]\ny_reduced = dfGroupedViolations['VIOLATIONS']\n\nX_train_reduced = X_reduced[0:1182]\ny_train_reduced = y_reduced[0:1182].ravel()\nX_test_reduced = X_reduced[1182:]\ny_test_reduced = y_reduced[1182:].ravel()\n\nregr_reduced = LinearRegression()\nregr_reduced.fit(X_train_reduced,y_train_reduced)\npred_reduced = regr_reduced.predict(X_test_reduced)\n\n#Display the coefficients for the linear regression model\npredictor_reduced = ['Intercept']+ sf_Cols\nlr_reduced_coefficient = pd.DataFrame(zip(predictor_reduced,regr_reduced.coef_.reshape(7)),columns=['Predictor','Estimate'])\nlr_reduced_coefficient","b0a6e595":"lr_forecast_reduced = regr_reduced.predict(X_reduced)\nlr_residual_reduced = y_reduced - lr_forecast\n\nreduced_lr_performance = model_performance(X_train_reduced, y_train_reduced, X_test_reduced, y_test_reduced, regr_reduced)\nreduced_lr_performance.round(2)","d6c8ff01":"#Visualization of linear regression full model\ntrain_vals = pd.DataFrame(regr_reduced.predict(X_train_reduced), index=X_train_reduced.index, columns=['pred'])\ntest_vals = pd.DataFrame(regr_reduced.predict(X_test_reduced), index=X_test_reduced.index, columns=['pred'])\n# visualization\nplt.figure(figsize=(20,7))\nplt.plot(dfGroupedViolations['VIOLATIONS'], color=ouColors['Cutler Green'], linewidth=1, label='actual')\nplt.plot(train_vals,color=ouColors['Wheat'], linewidth=2, label = \"train\")\nplt.plot(test_vals,color=ouColors['Gateway'], linewidth=2, label = \"test\")\nplt.title(\"Time Series Forecast\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Violations\")\nplt.legend()\nplt.show()","ed662dca":"firstOrderDiff = dfGroupedViolations['VIOLATIONS'].diff()\nfirstOrderDiff.dropna(inplace=True)\n\nfig, axs = plt.subplots(figsize=(20,7))\n\naxs.plot(firstOrderDiff, color=ouColors['Cutler Green'], linewidth=2, label='7DayRolling')\n\naxs.xaxis.set_major_locator(mdates.YearLocator())\naxs.legend()\naxs.set_xlabel('Year')\naxs.set_ylabel('Violations')\naxs.set_title(f'First Order Differencing')\n\nplt.show()","20afd17d":"result = adfuller(firstOrderDiff)\n\nprint(f'ADF Statistic: {result[0]}')\nprint(f'p-value: {result[1]}')\nprint(f'Critical Values: ')\nfor key, value in result[4].items():\n    print(f'\\t{key}: {value}')","7521a8a3":"lag_acf = acf(firstOrderDiff, nlags=20)\nlag_pacf = pacf(firstOrderDiff, nlags=20, method='ols')\n\n# ACF\nplt.figure(figsize=(10,4))\n\nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(firstOrderDiff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(firstOrderDiff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\n\n# PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(firstOrderDiff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(firstOrderDiff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.tight_layout()","2a2eaf6b":"# predict all path\nstart_index = '2019-11-22'\nend_index = '2019-12-31'\n# fit model\nmodel = ARIMA(dfGroupedViolations['VIOLATIONS'], order=(5,0,6))\nmodel_fit = model.fit(disp=0)\nforecast = model_fit.predict()\nfutureforecast = model_fit.predict(start=start_index, end=end_index)\nerror = mean_squared_error(dfGroupedViolations['VIOLATIONS'], forecast)\nprint(\"error: \" ,error)\n\n# visualization\nplt.figure(figsize=(22,10))\nplt.plot(dfGroupedViolations['VIOLATIONS'],color=ouColors['Cutler Green'], linewidth=1, label = \"original\")\nplt.plot(forecast,alpha=.7,color=ouColors['Wheat'], label = \"predicted\")\nplt.plot(futureforecast, color=ouColors['Gateway'],linewidth=2,label = \"forecast\")\nplt.title(\"Time Series Forecast\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Violations\")\nplt.legend()\n\nplt.show()","5d916553":"error = mean_squared_error(dfGroupedViolations['VIOLATIONS'], forecast)\narima_residual = dfGroupedViolations['VIOLATIONS'] - forecast\n\n\ndata = {'SSE': [sum(arima_residual**2)],\n        'MSE': [mean_squared_error(dfGroupedViolations['VIOLATIONS'], forecast)],\n        'RMSE': [mean_squared_error(dfGroupedViolations['VIOLATIONS'], forecast)**.5],\n        'MAD': [mean(absolute(arima_residual))],\n        'R2': [r2_score(dfGroupedViolations['VIOLATIONS'], forecast)]}\n\ndf_summary = pd.DataFrame.from_dict(data, orient='index', columns=['ARIMA'])\ndf_summary.round(2)","7319a70d":"dfGroupedViolations.head()\ndailyViolations = dfGroupedViolations['VIOLATIONS'].reset_index()\ndailyViolations.columns = ['ds', 'y']\ndailyViolations.head()","6a75d2eb":"m=Prophet(yearly_seasonality=True, weekly_seasonality=True) \nm.add_seasonality(name='weekly', period=7, fourier_order=3)\n\nm.fit(dailyViolations)\n\nfuture = m.make_future_dataframe(freq='D', periods=39)  #Predict violations through the end of 2020\n\nfuture.tail()","8ede235f":"fb_forecast = m.predict(future)\nfig = m.plot(fb_forecast, xlabel='Date', ylabel='Violations')\nax = fig.gca()\na = add_changepoints_to_plot(ax,m, fb_forecast)\nax.set_title(\"Facebook Prophet Forecast\")\n","3e356faa":"fb_forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(39)\n","2fead8dc":"fig2 = m.plot_components(fb_forecast)","0c4a5418":"actuals = dailyViolations['y']\nfb_est = fb_forecast.loc[:1969]['yhat']\nfb_residual = actuals - fb_est\n\n\ndata = {'SSE': [sum(fb_residual**2)],\n        'MSE': [mean_squared_error(actuals, fb_est)],\n        'RMSE': [mean_squared_error(actuals, fb_est)**.5],\n        'MAD': [mean(absolute(fb_residual))],\n        'R2': [r2_score(actuals, fb_est)]}\n\ndf_summary = pd.DataFrame.from_dict(data, orient='index', columns=['FBProphet'])\ndf_summary.round(2)","b4c3b15d":"from scipy.stats import t\nT = t.ppf(1-(.05\/2),(len(actuals)-1))\n\n\nlr_preds = list(regr.predict(X_train))+list(regr.predict(X_test))\nlr_residual = actuals-lr_preds","40f8516d":"#1 to 1 plots for each model\n#Linear Regression\n\nplt.figure(3, figsize=(25,7))\n\nplt.subplot(131)\na = plt.plot(lr_preds,actuals,'ro',alpha=0.5, label='actuals')\na1 = plt.plot(lr_preds,lr_preds,label='predictions')\na2 = plt.plot(lr_preds,lr_preds-T*steyx(actuals,lr_preds),label='-95% PI')\na3 = plt.plot(lr_preds,lr_preds+T*steyx(actuals,lr_preds),label='+95% PI')\nplt.xlabel('Actual')\nplt.ylabel('Estimate')\nplt.title('Estimate vs Estimate Plot: Linear Regression')\n\nplt.subplot(132)\na = plt.plot(forecast,actuals,'ro',alpha=0.5, label='actuals')\na1 = plt.plot(forecast,forecast,label='predictions')\na2 = plt.plot(forecast,forecast-T*steyx(actuals,forecast.values),label='-95% PI')\na3 = plt.plot(forecast,forecast+T*steyx(actuals,forecast.values),label='+95% PI')\nplt.xlabel('Actual')\nplt.ylabel('Estimate')\nplt.title('Estimate vs Estimate Plot: ARIMA')\n\nplt.subplot(133)\na = plt.plot(fb_est,actuals,'ro',alpha=0.5, label='actuals')\na1 = plt.plot(fb_est,fb_est,label='predictions')\na2 = plt.plot(fb_est,fb_est-T*steyx(actuals,fb_est.values),label='-95% PI')\na3 = plt.plot(fb_est,fb_est+T*steyx(actuals,fb_est.values),label='+95% PI')\nplt.xlabel('Actual')\nplt.ylabel('Estimate')\nplt.title('Estimate vs Estimate Plot: FB Prophet')","57585c61":"data = {'SSE': [sum((lr_residual)**2),sum(arima_residual**2),sum(fb_residual**2)],\n       'MSE': [mean_squared_error(actuals, lr_preds),  mean_squared_error(actuals, forecast), mean_squared_error(actuals, fb_est)],\n       'RMSE': [mean_squared_error(actuals, lr_preds)**.5, mean_squared_error(actuals, forecast)**.5,mean_squared_error(actuals, fb_est)**.5],\n       'MAD': [mean(absolute(lr_residual)), mean(absolute(arima_residual)),mean(absolute(fb_residual))],\n       'R2': [r2_score(actuals, lr_preds), r2_score(actuals, forecast),r2_score(actuals, fb_est)]}\n\ndfSummary = pd.DataFrame.from_dict(data, orient='index', columns=['Logistic Regression', 'ARIMA', 'FB Prophet'])\ndfSummary.round(2)","1605ff1e":" ## <a id=\"1.1\">1.1 Export weather data from noaa<\/a> \n[top](#top)","6b271de7":"## Time Series trends for the top 5 intersections","1a8afe35":"<a id=\"top\">#top<\/a>\n# Predicting Red Light Violations at Chicago Intersections\n<p>This project will attempt to predict red light violations at Chicago intersections with the goal of creating a sliding scale of fines based on intersections with the highest risk of violations under given conditions such as season, day of the week and weather conditions. \n\nTwo datasets were used in this analysis, [Chicago Red Light and Speed Camera Data](https:\/\/www.kaggle.com\/chicago\/chicago-red-light-and-speed-camera-data) and weather data from the Chicago O'Hare airport [NOAA GSO](https:\/\/www.kaggle.com\/noaa\/gsod). Both of these data sets are available on Kaggle and as files in this project.<\/p>\n\n## Table of Contents\n- [1. Preprocessing](#1)\n    - [1.1 Load violations and temperature data](#1.1)\n    - [1.2 Clean data and feature engineering](#1.2)\n- [2. Exploratory Analysis](#2)\n    - [2.1 Time series trends](#2.1)\n    - [2.2 Correlation between features and violations](#2.2)\n    - [2.3 Kmeans clustering with categorical features](#2.3)\n- [3. Prediction](#3)\n    - [3.1 Linear Regression](#3.1)\n    - [3.2 ARIMA](#3.2)\n    - [3.3 FAcebook Prophet](#3.3)\n- [4. Conclusions](#4)","a878a58d":"### <a id=\"3.3\">3.3 Facebook Prophet<\/a>  \n[top](#top)","a0903848":"## Time Series Trends","f00e419a":"### <a id=\"4\">#4 Conclusions<\/a>\n[top](#top)\n\n","d3bae8a2":"### <a id=\"2.3\">2.3 Kmeans Clustering<\/a>  \n[top](#top)","a8cab0cf":"The timeseries is now stationary because the P value is below the cutoff of .05 and the ADF statistic is less than the 1% critical value.","e5552a13":"## <a id=\"3\">3 Prediction<\/a>  \n[top](#top)","17322eca":"## <a id=\"1.2\">1.2 Clean data and feature engineering<\/a> \n[top](#top)","a513e820":"### <a id=\"2.2\">2.2 Correlations<\/a>  \n[top](#top)","ad9fac5a":"## <a id=\"2\">2. Exploratory Analysis<\/a>  \n\n### <a id=\"2.1\">2.1 Time Series Trends<\/a>\n\n[top](#top)","e23bd4b4":"### <a id=\"3.2\">3.2 ARIMA<\/a>\n[top](#top)","fd6117fe":"### <a id=\"3.1\">3.1 Linear Regression<\/a>\n[top](#top)"}}