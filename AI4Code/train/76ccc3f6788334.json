{"cell_type":{"40424427":"code","fd751824":"code","fa242760":"code","34894b74":"code","e03b8645":"code","66e4705c":"code","6066038d":"code","f38fc5d7":"code","d7691d1c":"code","7e01cff6":"code","2b91ee71":"code","d3d7638c":"code","c1e34287":"code","51788c83":"code","9e58d26d":"code","172fbd3c":"code","dbf75716":"code","70275b87":"code","1b99004e":"code","7953d6b4":"code","83f0181c":"code","e907b739":"code","9da6f833":"code","6cfd9b7b":"code","dfad0758":"code","8a397a1e":"code","fb1c0440":"code","7c73b4d0":"code","0fde9e0b":"code","a70288e4":"code","da73acf5":"code","c8b1e1c8":"code","4a27e979":"code","590948e7":"code","756aae1b":"code","86c9ceaa":"code","e5c0aa1a":"code","9928e9b2":"code","ed4ca2de":"code","f864e6eb":"code","c88c0dac":"code","3645d510":"code","4e9ccf38":"code","7b428ff8":"code","a37ff88a":"code","d8f8247d":"markdown","3978de1a":"markdown","44820bf1":"markdown","9fab3ffa":"markdown","0c8efcfe":"markdown","30d73230":"markdown","a89457bf":"markdown","d61e8ea3":"markdown","821bb285":"markdown","83831c94":"markdown","513461fe":"markdown","6fe19c8c":"markdown","3e2e8fb2":"markdown","9bad9bfe":"markdown","4419cd71":"markdown","a072a64e":"markdown","efe4cdad":"markdown","54a6ea35":"markdown","1dde588e":"markdown","c4575155":"markdown","d601461c":"markdown","0c4ed149":"markdown","9391f867":"markdown","ded3d3f1":"markdown","9aa0d953":"markdown","2477c7ca":"markdown","645d9fc0":"markdown","7ddeeddf":"markdown","944a58c7":"markdown","679479ed":"markdown","941ede9e":"markdown","a1ab4542":"markdown","fcfee4b1":"markdown","bdca193d":"markdown","86ad154d":"markdown","6b30a87e":"markdown","c0da8c68":"markdown","85b21891":"markdown","bb879a46":"markdown","b1c6f8ac":"markdown","45a183e9":"markdown","c6d65ad9":"markdown","f7d9a1f5":"markdown","3b4bfc5f":"markdown","7f4301cf":"markdown","8981fde2":"markdown","459086ab":"markdown","2c081b5b":"markdown","24635c20":"markdown","df91d707":"markdown","4928e816":"markdown","7fbb3aaf":"markdown","bf467684":"markdown","2daf4464":"markdown","be5126cd":"markdown","36a2ced2":"markdown","ba4eadda":"markdown","7fabdfe1":"markdown","22562107":"markdown","958657ea":"markdown"},"source":{"40424427":"from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom pandas import get_dummies\nimport plotly.graph_objs as go\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport scipy\nimport numpy\nimport json\nimport sys\nimport csv\nimport os","fd751824":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('sklearn: {}'.format(sklearn.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","fa242760":"sns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline","34894b74":"# import Dataset to play with it\ndataset = pd.read_csv('..\/input\/Iris.csv')","e03b8645":"type(dataset)","66e4705c":"# Generate data\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nX, y = make_blobs(n_samples=1000, centers=20, random_state=123)\nlabels = [\"b\", \"r\"]\ny = np.take(labels, (y < 10))\nprint(X) \nprint(y[:5])","6066038d":"# X is a 2 dimensional array, with 1000 rows and 2 columns\nprint(X.shape)\n \n# y is a vector of 1000 elements\nprint(y.shape)","f38fc5d7":"# Rows and columns can be accessed with lists, slices or masks\nprint(X[[1, 2, 3]])     # rows 1, 2 and 3\nprint(X[:5])            # 5 first rows\nprint(X[500:510, 0])    # values from row 500 to row 510 at column 0\nprint(X[y == \"b\"][:5])  # 5 first rows for which y is \"b\"","d7691d1c":"# Plot\nfrom matplotlib import pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.max_open_warning\"] = -1\nplt.figure()\nfor label in labels:\n    mask = (y == label)\n    plt.scatter(X[mask, 0], X[mask, 1], c=label)\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.show()","7e01cff6":"from sklearn.datasets import load_wine\ndata = load_wine()\ndata.target[[10, 80, 140]]\nlist(data.target_names)\n","2b91ee71":"from sklearn.cluster import DBSCAN\nimport numpy as np\nX = np.array([[1, 2], [2, 2], [2, 3],[8, 7], [8, 8], [25, 80]])\nclustering = DBSCAN(eps=3, min_samples=2).fit(X)\n","d3d7638c":"clustering.labels_","c1e34287":"clustering ","51788c83":"\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","9e58d26d":"class Estimator(object):\n    def fit(self, X, y=None):\n        \"\"\"Fits estimator to data.\"\"\"\n        # set state of ``self``\n        return self","172fbd3c":"# Import the nearest neighbor class\nfrom sklearn.neighbors import KNeighborsClassifier  # Change this to try \n                                                    # something else\n\n# Set hyper-parameters, for controlling algorithm\nclf = KNeighborsClassifier(n_neighbors=5)\n\n# Learn a model from training data\nclf.fit(X, y)","dbf75716":"# Estimator state is stored in instance attributes\nclf._tree","70275b87":"# Make predictions  \nprint(clf.predict(X[:5])) ","1b99004e":"# Compute (approximate) class probabilities\nprint(clf.predict_proba(X[:5]))","7953d6b4":"# K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","83f0181c":"from sklearn.neighbors import  RadiusNeighborsClassifier\nModel=RadiusNeighborsClassifier(radius=8.0)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n#summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accouracy score\nprint('accuracy is ', accuracy_score(y_test,y_pred))","e907b739":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","9da6f833":"from sklearn.linear_model import PassiveAggressiveClassifier\nModel = PassiveAggressiveClassifier()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","6cfd9b7b":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nModel = GaussianNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","dfad0758":"# BernoulliNB\nfrom sklearn.naive_bayes import BernoulliNB\nModel = BernoulliNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","8a397a1e":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nModel = SVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","fb1c0440":"# Support Vector Machine's \nfrom sklearn.svm import NuSVC\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","7c73b4d0":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nModel = LinearSVC()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\n\nprint('accuracy is',accuracy_score(y_pred,y_test))","0fde9e0b":"# Decision Tree's\nfrom sklearn.tree import DecisionTreeClassifier\n\nModel = DecisionTreeClassifier()\n\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","a70288e4":"# ExtraTreeClassifier\nfrom sklearn.tree import ExtraTreeClassifier\n\nModel = ExtraTreeClassifier()\n\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","da73acf5":"import numpy as np\n\ndef sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))","c8b1e1c8":"w = np.array([0.2, 0.3, 0.8])\nb = 0.5\nx = np.array([0.5, 0.6, 0.1])","4a27e979":"z = w.dot(x) + b\nprint(\"z:\", z)\nprint(\"a:\", sigmoid(z))","590948e7":"def activation(z):\n    if z > 0:\n        return 1\n    return 0","756aae1b":"w = np.array([1, 1])\nb = -1\nx = np.array([0, 0])\nprint(\"0 AND 0:\", activation(w.dot(x) + b))\nx = np.array([1, 0])\nprint(\"1 AND 0:\", activation(w.dot(x) + b))\nx = np.array([0, 1])\nprint(\"0 AND 1:\", activation(w.dot(x) + b))\nx = np.array([1, 1])\nprint(\"1 AND 1:\", activation(w.dot(x) + b))","86c9ceaa":"w = np.array([1, 1])\nb = 0\nx = np.array([0, 0])\nprint(\"0 OR 0:\", activation(w.dot(x) + b))\nx = np.array([1, 0])\nprint(\"1 OR 0:\", activation(w.dot(x) + b))\nx = np.array([0, 1])\nprint(\"0 OR 1:\", activation(w.dot(x) + b))\nx = np.array([1, 1])\nprint(\"1 OR 1:\", activation(w.dot(x) + b))","e5c0aa1a":"from sklearn.neural_network import MLPClassifier\nModel=MLPClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n# Summary of the predictions\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","9928e9b2":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","ed4ca2de":"from sklearn.ensemble import BaggingClassifier\nModel=BaggingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","f864e6eb":"from sklearn.ensemble import AdaBoostClassifier\nModel=AdaBoostClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","c88c0dac":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","3645d510":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","4e9ccf38":"from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nModel=QuadraticDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","7b428ff8":"from sklearn.cluster import KMeans\niris_SP = dataset[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n# k-means cluster analysis for 1-15 clusters                                              \nfrom scipy.spatial.distance import cdist\nclusters=range(1,15)\nmeandist=[]\n\n# loop through each cluster and fit the model to the train set\n# generate the predicted cluster assingment and append the mean \n# distance my taking the sum divided by the shape\nfor k in clusters:\n    model=KMeans(n_clusters=k)\n    model.fit(iris_SP)\n    clusassign=model.predict(iris_SP)\n    meandist.append(sum(np.min(cdist(iris_SP, model.cluster_centers_, 'euclidean'), axis=1))\n    \/ iris_SP.shape[0])\n\n\"\"\"\nPlot average distance from observations from the cluster centroid\nto use the Elbow Method to identify number of clusters to choose\n\"\"\"\nplt.plot(clusters, meandist)\nplt.xlabel('Number of clusters')\nplt.ylabel('Average distance')\nplt.title('Selecting k with the Elbow Method') \n# pick the fewest number of clusters that reduces the average distance\n# If you observe after 3 we can see graph is almost linear","a37ff88a":"print(__doc__)\n\n# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD 3 clause\n\n\niris = datasets.load_iris()\nX = iris.data[:, 0:2]  # we only take the first two features for visualization\ny = iris.target\n\nn_features = X.shape[1]\n\nC = 10\nkernel = 1.0 * RBF([1.0, 1.0])  # for GPC\n\n# Create different classifiers.\nclassifiers = {\n    'L1 logistic': LogisticRegression(C=C, penalty='l1',\n                                      solver='saga',\n                                      multi_class='multinomial',\n                                      max_iter=10000),\n    'L2 logistic (Multinomial)': LogisticRegression(C=C, penalty='l2',\n                                                    solver='saga',\n                                                    multi_class='multinomial',\n                                                    max_iter=10000),\n    'L2 logistic (OvR)': LogisticRegression(C=C, penalty='l2',\n                                            solver='saga',\n                                            multi_class='ovr',\n                                            max_iter=10000),\n    'Linear SVC': SVC(kernel='linear', C=C, probability=True,\n                      random_state=0),\n    'GPC': GaussianProcessClassifier(kernel)\n}\n\nn_classifiers = len(classifiers)\n\nplt.figure(figsize=(3 * 2, n_classifiers * 2))\nplt.subplots_adjust(bottom=.2, top=.95)\n\nxx = np.linspace(3, 9, 100)\nyy = np.linspace(1, 5, 100).T\nxx, yy = np.meshgrid(xx, yy)\nXfull = np.c_[xx.ravel(), yy.ravel()]\n\nfor index, (name, classifier) in enumerate(classifiers.items()):\n    classifier.fit(X, y)\n\n    y_pred = classifier.predict(X)\n    accuracy = accuracy_score(y, y_pred)\n    print(\"Accuracy (train) for %s: %0.1f%% \" % (name, accuracy * 100))\n\n    # View probabilities:\n    probas = classifier.predict_proba(Xfull)\n    n_classes = np.unique(y_pred).size\n    for k in range(n_classes):\n        plt.subplot(n_classifiers, n_classes, index * n_classes + k + 1)\n        plt.title(\"Class %d\" % k)\n        if k == 0:\n            plt.ylabel(name)\n        imshow_handle = plt.imshow(probas[:, k].reshape((100, 100)),\n                                   extent=(3, 9, 1, 5), origin='lower')\n        plt.xticks(())\n        plt.yticks(())\n        idx = (y_pred == k)\n        if idx.any():\n            plt.scatter(X[idx, 0], X[idx, 1], marker='o', c='w', edgecolor='k')\n\nax = plt.axes([0.15, 0.04, 0.7, 0.05])\nplt.title(\"Probability\")\nplt.colorbar(imshow_handle, cax=ax, orientation='horizontal')\n\nplt.show()","d8f8247d":"<a id=\"7\"><\/a> <br>\n# 4- Applications\n\n- **Classifying** signal from background events; \n- **Diagnosing** disease from symptoms;\n- **Recognising** cats in pictures;\n- **Identifying** body parts with Kinect cameras;\n- ...\n  ###### [Go to top](#top)","3978de1a":"<a id=\"17\"><\/a> <br>\n##  7-7 Radius Neighbors Classifier\nClassifier implementing a **vote** among neighbors within a given **radius**\n\nIn scikit-learn **RadiusNeighborsClassifier** is very similar to **KNeighborsClassifier** with the exception of two parameters. First, in RadiusNeighborsClassifier we need to specify the radius of the fixed area used to determine if an observation is a neighbor using radius. Unless there is some substantive reason for setting radius to some value, it is best to treat it like any other hyperparameter and tune it during model selection. The second useful parameter is outlier_label, which indicates what label to give an observation that has no observations within the radius - which itself can often be a useful tool for identifying outliers.\n###### [Go to top](#top)","44820bf1":"<a id=\"35\"><\/a> <br>\n## 7-23 Quadratic Discriminant Analysis\nA classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n\nThe model fits a **Gaussian** density to each class.\n###### [Go to top](#top)","9fab3ffa":"<a id=\"31\"><\/a> <br>\n## 7-19 Bagging classifier \nA Bagging classifier is an ensemble **meta-estimator** that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting . If samples are drawn with replacement, then the method is known as Bagging . When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces . Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches .[http:\/\/scikit-learn.org]\n###### [Go to top](#top)","0c8efcfe":"<a id=\"27\"><\/a> <br>\n## 7-17 Neural network\n\nI have used multi-layer Perceptron classifier.\nThis model optimizes the log-loss function using **LBFGS** or **stochastic gradient descent**.\n###### [Go to top](#top)","30d73230":"<a id=\"19\"><\/a> <br>\n##  7-9 Passive Aggressive Classifier","a89457bf":"After loading the data via **pandas**, we should checkout what the content is, description and via the following:","d61e8ea3":">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","821bb285":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Import](#2)\n    1. [Version](#3)\n1. [Algorithms](#4)\n    1. [Data Collection](#5)\n1. [Framework](#6)\n1. [Applications](#7)\n1. [How to use Sklearn Data Set? ](#8)\n1. [Loading external data](#9)\n1. [Model Deployment](#10)\n    1. [Families of ML algorithms](#11)\n    1. [Prepare Features & Targets](#12)\n    1. [Accuracy and precision](#13)\n    1. [Estimators](#14)\n    1. [Predictors](#15)\n    1. [K-Nearest Neighbours](#16)\n    1. [Radius Neighbors Classifier](#17)\n    1. [Logistic Regression](#18)\n    1. [Passive Aggressive Classifier](#19)\n    1. [Naive Bayes](#20)\n    1. [BernoulliNB](#21)\n    1. [SVM](#22)\n    1. [Nu-Support Vector Classification](#23)\n    1. [Linear Support Vector Classification](#24)\n    1. [Decision Tree](#25)\n    1. [ExtraTreeClassifier](#26)\n    1. [Neural network](#27)\n        1. [What is a Perceptron?](#28)\n        1. [The XOR Problem](#29)\n    1. [RandomForest](#30)\n    1. [Bagging classifier ](#31)\n    1. [AdaBoost classifier](#32)\n    1. [Gradient Boosting Classifier](#33)\n    1. [Linear Discriminant Analysis](#34)\n    1. [Quadratic Discriminant Analysis](#35)\n    1. [Kmeans](#36)\n1. [conclusion](#37)\n1. [References](#38)","83831c94":"We can define this activation function in Python as:","513461fe":"<a id=\"15\"><\/a> <br>\n## 7-5- Predictors","6fe19c8c":"<a id=\"26\"><\/a> <br>\n## 7-16 ExtraTreeClassifier\nAn extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the **max_features** randomly selected features and the best split among those is chosen. When max_features is set 1, this amounts to building a totally random decision tree.\n\n**Warning**: Extra-trees should only be used within ensemble methods.\n###### [Go to top](#top)","3e2e8fb2":"<a id=\"20\"><\/a> <br>\n## 7-10 Naive Bayes\nIn machine learning, naive Bayes classifiers are a family of simple \"**probabilistic classifiers**\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.","9bad9bfe":"<a id=\"33\"><\/a> <br>\n## 7-21 Gradient Boosting Classifier\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.","4419cd71":"## 1-3 Setup\n\nA few tiny adjustments for better **code readability**","a072a64e":"<a id=\"8\"><\/a> <br>\n# 5- How to use Sklearn Data Set? \n\n- Input data = Numpy arrays or Scipy sparse matrices ;\n- Algorithms are expressed using high-level operations defined on matrices or vectors (similar to MATLAB) ;\n    - Leverage efficient low-leverage implementations ;\n    - Keep code short and readable. ","efe4cdad":"<a id=\"3\"><\/a> <br>\n## 1-2 Version","54a6ea35":"<a id=\"4\"><\/a> <br>\n# 2- Algorithms","1dde588e":"There are many online examples and tutorials on perceptrons and learning. Here is a list of some articles:\n- [Wikipedia on Perceptrons](https:\/\/en.wikipedia.org\/wiki\/Perceptron)\n- Jurafsky and Martin (ed. 3), Chapter 8\n###### [Go to top](#top)","c4575155":"The goal of supervised classification is to build an estimator $\\varphi: {\\cal X} \\mapsto {\\cal Y}$ minimizing\n\n$$\nErr(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}\n$$\n\nwhere $\\ell$ is a loss function, e.g., the zero-one loss for classification $\\ell_{01}(Y,\\hat{Y}) = 1(Y \\neq \\hat{Y})$.","d601461c":"<a id=\"32\"><\/a> <br>\n##  7-20 AdaBoost classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\nThis class implements the algorithm known as **AdaBoost-SAMME** .","0c4ed149":"This is an example that I have taken from a draft of the 3rd edition of Jurafsky and Martin, with slight modifications:\nWe import *numpy* and use its *exp* function. We could use the same function from the *math* module, or some other module like *scipy*. The *sigmoid* function is defined as in the textbook:\n","9391f867":"<a id=\"12\"><\/a> <br>\n## 7-2 Prepare Features & Targets\nFirst of all seperating the data into dependent(**Feature**) and independent(**Target**) variables.\n\n**<< Note 4 >>**\n* X==>>Feature\n* y==>>Target\n##  Test error\n\nIssue: the training error is a __biased__ estimate of the generalization error.\n\nSolution: Divide ${\\cal L}$ into two disjoint parts called training and test sets (usually using 70% for training and 30% for test).\n- Use the training set for fitting the model;\n- Use the test set for evaluation only, thereby yielding an unbiased estimate.","ded3d3f1":"no see our prediction for iris","9aa0d953":"<a id=\"34\"><\/a> <br>\n## 7-22 Linear Discriminant Analysis\nLinear Discriminant Analysis (discriminant_analysis.LinearDiscriminantAnalysis) and Quadratic Discriminant Analysis (discriminant_analysis.QuadraticDiscriminantAnalysis) are two classic classifiers, with, as their names suggest, a **linear and a quadratic decision surface**, respectively.\n\nThese classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no **hyperparameters** to tune.\n###### [Go to top](#top)","2477c7ca":"<a id=\"37\"><\/a> <br>\n# 8- conclusion\nThis kernel is a simple tutorial for machine learning with sklearn and it is not completed yet!","645d9fc0":"For OR we could implement a perceptron as:","7ddeeddf":"<a id=\"22\"><\/a> <br>\n## 7-12 SVM\n\nThe advantages of support vector machines are:\n* Effective in high dimensional spaces.\n* Still effective in cases where number of dimensions is greater than the number of samples. \n* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation\n###### [Go to top](#top)","944a58c7":" # 9- Read more\n \n you can start to learn and review your knowledge about ML with a perfect dataset and try to learn and memorize the workflow for your journey in Data science world with read more sources, here I want to give some courses, e-books and cheatsheet:\n ## 9-1 Courses\n \nThere are a lot of online courses that can help you develop your knowledge, here I have just  listed some of them:\n\n1. [Machine Learning Certification by Stanford University (Coursera)](https:\/\/www.coursera.org\/learn\/machine-learning\/)\n\n2. [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science (Udemy)](https:\/\/www.udemy.com\/machinelearning\/)\n\n3. [Deep Learning Certification by Andrew Ng from deeplearning.ai (Coursera)](https:\/\/www.coursera.org\/specializations\/deep-learning)\n\n4. [Python for Data Science and Machine Learning Bootcamp (Udemy)](Python for Data Science and Machine Learning Bootcamp (Udemy))\n\n5. [Mathematics for Machine Learning by Imperial College London](https:\/\/www.coursera.org\/specializations\/mathematics-machine-learning)\n\n6. [Deep Learning A-Z\u2122: Hands-On Artificial Neural Networks](https:\/\/www.udemy.com\/deeplearning\/)\n\n7. [Complete Guide to TensorFlow for Deep Learning Tutorial with Python](https:\/\/www.udemy.com\/complete-guide-to-tensorflow-for-deep-learning-with-python\/)\n\n8. [Data Science and Machine Learning Tutorial with Python \u2013 Hands On](https:\/\/www.udemy.com\/data-science-and-machine-learning-with-python-hands-on\/)\n\n9. [Machine Learning Certification by University of Washington](https:\/\/www.coursera.org\/specializations\/machine-learning)\n\n10. [Data Science and Machine Learning Bootcamp with R](https:\/\/www.udemy.com\/data-science-and-machine-learning-bootcamp-with-r\/)\n11. [Creative Applications of Deep Learning with TensorFlow](https:\/\/www.class-central.com\/course\/kadenze-creative-applications-of-deep-learning-with-tensorflow-6679)\n12. [Neural Networks for Machine Learning](https:\/\/www.class-central.com\/mooc\/398\/coursera-neural-networks-for-machine-learning)\n13. [Practical Deep Learning For Coders, Part 1](https:\/\/www.class-central.com\/mooc\/7887\/practical-deep-learning-for-coders-part-1)\n14. [Machine Learning](https:\/\/www.cs.ox.ac.uk\/teaching\/courses\/2014-2015\/ml\/index.html)\n\n## 9-2 Ebooks\n\nSo you love reading , here is **10 free machine learning books**\n1. [Probability and Statistics for Programmers](http:\/\/www.greenteapress.com\/thinkstats\/)\n2. [Bayesian Reasoning and Machine Learning](http:\/\/web4.cs.ucl.ac.uk\/staff\/D.Barber\/textbook\/091117.pdf)\n2. [An Introduction to Statistical Learning](http:\/\/www-bcf.usc.edu\/~gareth\/ISL\/)\n2. [Understanding Machine Learning](http:\/\/www.cs.huji.ac.il\/~shais\/UnderstandingMachineLearning\/index.html)\n2. [A Programmer\u2019s Guide to Data Mining](http:\/\/guidetodatamining.com\/)\n2. [Mining of Massive Datasets](http:\/\/infolab.stanford.edu\/~ullman\/mmds\/book.pdf)\n2. [A Brief Introduction to Neural Networks](http:\/\/www.dkriesel.com\/_media\/science\/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf)\n2. [Deep Learning](http:\/\/www.deeplearningbook.org\/)\n2. [Natural Language Processing with Python](https:\/\/www.researchgate.net\/publication\/220691633_Natural_Language_Processing_with_Python)\n2. [Machine Learning Yearning](http:\/\/www.mlyearning.org\/)\n\n## 9-3 Cheat Sheets\n\nData Science is an ever-growing field, there are numerous tools & techniques to remember. It is not possible for anyone to remember all the functions, operations and formulas of each concept. That\u2019s why we have cheat sheets. But there are a plethora of cheat sheets available out there, choosing the right cheat sheet is a tough task. So, I decided to write this article.\n\nHere I have selected the cheat sheets on the following criteria: comprehensiveness, clarity, and content [26]:\n1. [Quick Guide to learn Python for Data Science ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Data-Science-in-Python.pdf)\n1. [Python for Data Science Cheat sheet ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/beginners_python_cheat_sheet.pdf)\n1. [Python For Data Science Cheat Sheet NumPy](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Numpy_Python_Cheat_Sheet.pdf)\n1. [Exploratory Data Analysis in Python]()\n1. [Data Exploration using Pandas in Python](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Data-Exploration-in-Python.pdf)\n1. [Data Visualisation in Python](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/data-visualisation-infographics1.jpg)\n1. [Python For Data Science Cheat Sheet Bokeh](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Python_Bokeh_Cheat_Sheet.pdf)\n1. [Cheat Sheet: Scikit Learn ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/Scikit-Learn-Infographic.pdf)\n1. [MLalgorithms CheatSheet](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/MLalgorithms-.pdf)\n1. [Probability Basics  Cheat Sheet ](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist\/blob\/master\/cheatsheets\/probability_cheatsheet.pdf)","679479ed":"<a id=\"14\"><\/a> <br>\n## 7-4- Estimators","941ede9e":"<a id=\"16\"><\/a> <br>\n## 7-6 K-Nearest Neighbours\nIn **Machine Learning**, the **k-nearest neighbors algorithm** (k-NN) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\n###### [Go to top](#top)","a1ab4542":"<a id=\"36\"><\/a> <br>\n## 7-24 Kmeans \nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). \n\nThe goal of this algorithm is **to find groups in the data**, with the number of groups represented by the variable K. The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided.\n\n###### [Go to top](#top)","fcfee4b1":"<a id=\"2\"><\/a> <br>\n##   1-1 Import","bdca193d":"<a id=\"30\"><\/a> <br>\n## 7-18 RandomForest\nA random forest is a meta estimator that **fits a number of decision tree classifiers** on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \n\nThe sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).\n###### [Go to top](#top)","86ad154d":"<a id=\"25\"><\/a> <br>\n## 7-15 Decision Tree\nDecision Trees (DTs) are a non-parametric supervised learning method used for **classification** and **regression**. The goal is to create a model that predicts the value of a target variable by learning simple **decision rules** inferred from the data features.\n###### [Go to top](#top)","6b30a87e":"<a id=\"38\"><\/a> <br>\n# 9- References\n1. [Coursera](https:\/\/www.coursera.org\/specializations\/data-science-python)\n1. [GitHub](https:\/\/github.com\/mjbahmani)\n###### [Go to top](#top)","c0da8c68":"<a id=\"10\"><\/a> <br>\n# 7- Model Deployment\nAll learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n\n- an `estimator` interface for building and fitting models;\n- a `predictor` interface for making predictions;\n- a `transformer` interface for converting data.\n\nGoal: enforce a simple and consistent API to __make it trivial to swap or plug algorithms__. \n\nIn this section have been applied more than **20 learning algorithms** that play an important rule in your experiences and improve your knowledge in case of using sklearn.\n\n> **<< Note 3 >>** : The results shown here may be slightly different for your analysis because, for example, the neural network algorithms use random number generators for fixing the initial value of the weights (starting points) of the neural networks, which often result in obtaining slightly different (local minima) solutions each time you run the analysis. Also note that changing the seed for the random number generator used to create the train, test, and validation samples can change your results.\n\n  ###### [Go to top](#top)","85b21891":"For AND we could implement a perceptron as:","bb879a46":"## <div style=\"text-align: center\">A Journey with Scikit-Learn + 20 ML Algorithms<\/div>\n\n<div style=\"text-align: center\">There are plenty of <b>courses and tutorials<\/b> that can help you learn Scikit-Learn from scratch but here in <b>Kaggle<\/b>, After reading, you can use this workflow to solve other real problems and use it as a template to deal with <b>machine learning<\/b> problems.<\/div>\n<div style=\"text-align:center\">last update: <b>11\/13\/2018<\/b><\/div>\n\n<img src=\"http:\/\/scikit-learn.org\/stable\/_images\/scikit-learn-logo-notext.png\">\n\n\n>###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","b1c6f8ac":"Our example data, **weights** $w$, **bias** $b$, and **input** $x$ are defined as:","45a183e9":"<a id=\"18\"><\/a> <br>\n## 7-8 Logistic Regression\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is **dichotomous** (binary). Like all regression analyses, the logistic regression is a **predictive analysis**.\n\nIn statistics, the logistic model (or logit model) is a widely used statistical model that, in its basic form, uses a logistic function to model a binary dependent variable; many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model; it is a form of binomial regression. Mathematically, a binary logistic model has a dependent variable with two possible values, such as pass\/fail, win\/lose, alive\/dead or healthy\/sick; these are represented by an indicator variable, where the two values are labeled \"0\" and \"1\"\n###### [Go to top](#top)","c6d65ad9":"## 6-1 what is new?\nA new clustering algorithm: cluster.**OPTICS**: an algoritm related to cluster.**DBSCAN**, that has hyperparameters easier to set and that scales better\n","f7d9a1f5":"<a id=\"23\"><\/a> <br>\n## 7-13 Nu-Support Vector Classification\n\n> Similar to SVC but uses a parameter to control the number of support vectors.","3b4bfc5f":"<a id=\"21\"><\/a> <br>\n##  7-11 BernoulliNB\nLike MultinomialNB, this classifier is suitable for **discrete data**. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary\/boolean features.","7f4301cf":"<a id=\"6\"><\/a> <br>\n# 3- Framework\n\nData comes as a finite learning set ${\\cal L} = (X, y)$ where\n* Input samples are given as an array $X$ of shape `n_samples` $\\times$ `n_features`, taking their values in ${\\cal X}$;\n* Output values are given as an array $y$, taking _symbolic_ values in ${\\cal Y}$.\n###### [Go to top](#top)","8981fde2":"Our neural unit would compute $z$ as the **dot-product** $w \\cdot x$ and add the **bias** $b$ to it. The sigmoid function defined above will convert this $z$ value to the **activation value** $a$ of the unit:","459086ab":"**Unsupervised learning**:\n\n* Clustering (KMeans, Ward, ...)\n* Matrix decomposition (PCA, ICA, ...)\n* Density estimation\n* Outlier detection","2c081b5b":"There is no way to implement a perceptron for XOR this way.","24635c20":"<a id=\"29\"><\/a> <br>\n### 7-17-2 The XOR Problem\nThe power of neural units comes from combining them into larger networks. Minsky and Papert (1969): A single neural unit cannot compute the simple logical function XOR.\n\nThe task is to implement a simple **perceptron** to compute logical operations like AND, OR, and XOR.\n\n- Input: $x_1$ and $x_2$\n- Bias: $b = -1$ for AND; $b = 0$ for OR\n- Weights: $w = [1, 1]$\n\nwith the following activation function:\n\n$$\ny = \\begin{cases}\n    \\ 0 & \\quad \\text{if } w \\cdot x + b \\leq 0\\\\\n    \\ 1 & \\quad \\text{if } w \\cdot x + b > 0\n  \\end{cases}\n$$\n###### [Go to top](#top)","df91d707":"#### This Kernel is not completed and will be updated soon!!!","4928e816":"**<< Note 1 >>**\n\n* Each row is an observation (also known as : sample, example, instance, record)\n* Each column is a feature (also known as: Predictor, attribute, Independent Variable, input, regressor, Covariate)","7fbb3aaf":"<a id=\"28\"><\/a> <br>\n## 7-17-1 What is a Perceptron?","bf467684":"<a id=\"11\"><\/a> <br>\n## 7-1 Families of ML algorithms\nThere are several categories for machine learning algorithms, below are some of these categories:\n* Linear\n    * Linear Regression\n    * Logistic Regression\n    * Support Vector Machines\n* Tree-Based\n    * Decision Tree\n    * Random Forest\n    * GBDT\n* KNN\n* Neural Networks\n\n-----------------------------\nAnd if we  want to categorize ML algorithms with the type of learning, there are below type:\n* Classification\n\n    * k-Nearest \tNeighbors\n    * LinearRegression\n    * SVM\n    * DT \n    * NN\n    \n* clustering\n\n    * K-means\n    * HCA\n    * Expectation Maximization\n    \n* Visualization \tand\tdimensionality \treduction:\n\n    * Principal \tComponent \tAnalysis(PCA)\n    * Kernel PCA\n    * Locally -Linear\tEmbedding \t(LLE)\n    * t-distributed\tStochastic\tNeighbor\tEmbedding \t(t-SNE)\n    \n* Association \trule\tlearning\n\n    * Apriori\n    * Eclat\n* Semisupervised learning\n* Reinforcement Learning\n    * Q-learning\n* Batch learning & Online learning\n* Ensemble  Learning\n\n**<< Note >>**\n> Here is no method which outperforms all others for all tasks\n\n###### [Go to top](#top)","2daf4464":"<a id=\"9\"><\/a> <br>\n# 6- Loading external data\n\n- Numpy provides some [simple tools](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/routines.io.html) for loading data from files (CSV, binary, etc);\n\n- For structured data, Pandas provides more [advanced tools](http:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html) (CSV, JSON, Excel, HDF5, SQL, etc);\n###### [Go to top](#top)","be5126cd":"__Model selection and evaluation:__\n\n* Cross-validation\n* Grid-search\n* Lots of metrics\n\n_... and many more!_ (See our [Reference](http:\/\/scikit-learn.org\/dev\/modules\/classes.html))\n<a id=\"5\"><\/a> <br>\n## 2-1 Data Collection\n**Data collection** is the process of gathering and measuring data, information or any variables of interest in a standardized and established manner that enables the collector to answer or test hypothesis and evaluate outcomes of the particular collection.[techopedia]\n\n**Iris dataset**  consists of 3 different types of irises\u2019 (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n\nThe rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.[6]\n###### [Go to top](#top)","36a2ced2":"<a id=\"1\"><\/a> <br>\n# 1-Introduction\n\n- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including: [NumPy](http:\/\/numpy.org), [SciPy](http:\/\/scipy.org), [IPython](http:\/\/ipython.org), [Matplotlib](http:\/\/matplotlib.org), [Pandas](http:\/\/pandas.pydata.org\/), _and many others..._\n\n\n\n- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries;\n- Core algorithms are implemented in low-level languages.","ba4eadda":"<a id=\"36\"><\/a> <br>\n## 7-25 Plot classification probability","7fabdfe1":"<a id=\"24\"><\/a> <br>\n## 7-14 Linear Support Vector Classification\n\nSimilar to **SVC** with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n###### [Go to top](#top)","22562107":"<a id=\"13\"><\/a> <br>\n## 7-3 Accuracy and precision\n- Recall that we want to learn an estimator $\\varphi$ minimizing the generalization error $Err(\\varphi) = \\mathbb{E}_{X,Y}\\{ \\ell(Y, \\varphi(X)) \\}$.\n\n- Problem: Since $P_{X,Y}$ is unknown, the generalization error $Err(\\varphi)$ cannot be evaluated.\n\n- Solution: Use a proxy to approximate $Err(\\varphi)$.\n* **precision** : \n\nIn pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n* **recall** : \n\nrecall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n* **F-score** :\n\nthe F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n**What is the difference between accuracy and precision?**\n\"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning\/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance.\n###### [Go to top](#top)","958657ea":"**Supervised learning**:\n\n* Linear models (Ridge, Lasso, Elastic Net, ...)\n* Support Vector Machines\n* Tree-based methods (Random Forests, Bagging, GBRT, ...)\n* Nearest neighbors \n* Neural networks (basics)\n* Gaussian Processes\n* Feature selection"}}