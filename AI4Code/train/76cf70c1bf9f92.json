{"cell_type":{"5456977d":"code","a3b9aa12":"code","7d8a8a9e":"code","d840056a":"code","f6f66a90":"code","cc8bda63":"code","6bc4bdc5":"code","acb8157c":"code","c8a3e898":"code","c6bef8f4":"code","daf8e61e":"code","45bedc1f":"code","023d9b80":"code","d34cf366":"code","e4a91e17":"code","86c693db":"code","86d0cb6b":"code","5b7cf09a":"code","3cf4e5cc":"markdown","c32083d9":"markdown","7be47e7c":"markdown"},"source":{"5456977d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom matplotlib.pyplot import hist, scatter\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier","a3b9aa12":"raw_data = load_iris()\nX = raw_data['data']\nY = raw_data['target']","7d8a8a9e":"#Target names \ntarget_names = list(raw_data.target_names)\n\n#Features names \nfeatures=list(raw_data.feature_names)\n\nfeatures","d840056a":"target_names","f6f66a90":"# Visualisation avec seaborn\ndf = pd.DataFrame(X, columns=raw_data.feature_names)\ndf['target'] = Y\ndf['species'] = df['target'].map({0:raw_data.target_names[0], 1:raw_data.target_names[1], 2:raw_data.target_names[2]})\n\n# Pair plot with seaborn\nsns.pairplot(df.drop('target', axis=1), height=3, hue=\"species\");","cc8bda63":"# Train\/Test split\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.5)\n\n# Shape\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","6bc4bdc5":"#arbre de d\u00e9cision avec l'entropie comme crit\u00e8re de puret\u00e9 des noeuds \nclf =tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=5,\n                                  min_samples_split=2, min_samples_leaf=1, \n                                  min_weight_fraction_leaf=0.0, max_features=None, \n                                  random_state=None, max_leaf_nodes=None, \n                                  min_impurity_split=1e-07, class_weight=None)\n\nclf=clf.fit(X_train,Y_train)","acb8157c":"#evaluation du mod\u00e8le \nscore = clf.score(X_test, Y_test)\nprint(\"Acuracy  = \", score)\nY_true, Y_pred = Y_test, clf.predict(X_test)\n\nprint(\"Confusion matrix : \")\n\ncm = confusion_matrix(Y_true, Y_pred)\n\nsns.heatmap(cm, annot=True, annot_kws={'size':10},\n            cmap=plt.cm.Greens, linewidths=0.2)\n\n\ntick_marks = np.arange(len(target_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, target_names, rotation=25)\nplt.yticks(tick_marks2, target_names, rotation=0)\nplt.xlabel('Valeurs Pr\u00e9dites')\nplt.ylabel('Valeurs Observ\u00e9es')\nplt.title('Matrice de Confusion')\nplt.show()","c8a3e898":"plt.figure(figsize=(7,10))\ntree.plot_tree(clf,filled =True)\n","c6bef8f4":"#Bagging avec 10 instances de mod\u00e8les \n#model=DecisionTreeClassifier()\n\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.5)\n\nfrom sklearn.ensemble import  BaggingClassifier\nmodel=BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=5,\n                                  min_samples_split=2, min_samples_leaf=1, \n                                  min_weight_fraction_leaf=0.0, max_features=None, \n                                  random_state=None, max_leaf_nodes=None, \n                                  min_impurity_split=1e-07, class_weight=None),n_estimators=10)\nmodel.fit(X_train,Y_train)\n\nScores =[]\nfor i in range(10):\n    out = cross_validate(model.estimators_[i], X=X_train, y=Y_train, cv = 3, scoring =('accuracy'),return_train_score=True) \n    Scores.append(out['test_score'])\n\nfor i in range(10):\n    print(\"Model\",i+1,\": \",Scores[i])","daf8e61e":"for i in range(10):\n    print(\"Model\",i+1,\": \",Scores[i])","45bedc1f":"#Afficher quelques arbres appris \nfor i in range(2,6):\n    plt.figure(figsize=(7,10))\n    tree.plot_tree(model.estimators_[i],filled=True)","023d9b80":"#matrice qui associe \u00e0 chaque exemple les diff\u00e9rentes pr\u00e9dictions des diff\u00e9rents estimateurs appris\ny_pred = np.zeros((len(Y_test), 10))\nfor j in range(10):\n  Y_temp= model.estimators_[j].predict(X_test)\n  for i in range(75):\n    y_pred[i,j]=Y_temp[i]\n    \n    \nfrom collections import Counter\nimport operator\ny_true =[]\nfor i in range(75):\n  y_temp=y_pred[i,:]\n  compteur=Counter(y_temp) #Compter le nombre de features ayant la valeur de chaque classe\n  #Target = 0,1,2\n  y_true.append(max(compteur.items(), key=operator.itemgetter(1))[0])   #R\u00e9ccup\u00e9rer la classe qui a le compteur le plus elev\u00e9\ny_true=np.array(y_true) \n\ny_true","d34cf366":"Y_test","e4a91e17":"print(accuracy_score(y_true,Y_test))","86c693db":"#instance et entrainement d'arbre al\u00e9atoire avec 10 arbres , crit\u00e8re entropy\n\nmodel =RandomForestClassifier(n_estimators=10,criterion='entropy')\nmodel.fit(X_train, Y_train)\nscore = model.score(X_test, Y_test)\n\nprint(\"Acuracy  = \", score)\n#Y_true, Y_pred = Y_test, model.predict(X_test)\nprint(\"Confusion matrix : \")\n\ncm = confusion_matrix(Y_true, Y_pred)\n\nsns.heatmap(cm, annot=True, annot_kws={'size':10},\n            cmap=plt.cm.Greens, linewidths=0.2)\n\n\ntick_marks = np.arange(len(target_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, target_names, rotation=25)\nplt.yticks(tick_marks2, target_names, rotation=0)\nplt.xlabel('Valeurs Pr\u00e9dites')\nplt.ylabel('Valeurs Observ\u00e9es')\nplt.title('Matrice de Confusion')\nplt.show()\n","86d0cb6b":"fig, axes = plt.subplots(nrows = 1,ncols = 10,figsize = (10,2), dpi=900)\nfor index in range(len(model.estimators_)):\n  tree.plot_tree(model.estimators_[index],\n\n                    filled = True,\n                    ax = axes[index]);\n\n  axes[index].set_title('Model ' + str(index +1), fontsize = 5)\nfig.savefig('random.png')\n\n#Cliquer sur l'image pour zoomer","5b7cf09a":"# La variable la plus discriminante selon le mod\u00e8le\nplt.bar([\"x0\", \"x1\", \"x2\", \"x3\"], list(model.feature_importances_), orientation = 'vertical')\nplt.ylabel('Weight'); \nplt.title('Variable weight');\n","3cf4e5cc":"<b> Model <\/b>","c32083d9":"<h5>  par<b> Vada ZAMBLE | vadazamble@yahoo.fr <\/b> <h5>\n\nDonn\u00e9es IRIS => <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_iris.html\">load_iris<\/a>\n    \nDescription du jeu de donn\u00e9es => <a href=\"https:\/\/scikit-learn.org\/stable\/datasets\/index.html#iris-plants-dataset\"> Iris_Dataset <\/a>. ","7be47e7c":"Cr\u00e9ation des matrices X_train, X_test, Y_train, Y_test avec un ratio 50%"}}