{"cell_type":{"72c427a2":"code","9859ac0a":"code","23740a14":"code","74573e37":"code","9901d87f":"code","5b52dd6c":"code","39353dd6":"code","2da4fc27":"code","77029661":"code","1bc047a5":"code","ceb3d825":"code","f0b93df1":"code","2641d34f":"code","83acb675":"code","792a6c95":"code","c95feb92":"code","2465592a":"code","0db3067a":"code","4f346571":"code","1684f882":"code","22e34265":"code","e0b758c6":"code","8bfbe1d9":"code","093a0825":"code","3fa11fc7":"code","1f0e8980":"code","8ec5bd7f":"code","d1eca29b":"code","1968fdd9":"code","c6f83965":"code","5eebda78":"code","58a7b995":"code","e3462b98":"code","e265d856":"code","03dfa8d3":"code","d36ed204":"code","437e2990":"code","7f1d1608":"code","41f97046":"code","d628b38e":"code","0f0538a2":"code","270f9342":"code","b009c2a2":"code","a2329c74":"code","9f653183":"code","e326b41e":"code","dcc395a4":"code","d19fe59a":"code","90e011d1":"code","b12bb63b":"code","e14c4476":"code","9c58969d":"code","154e585f":"code","13886c0a":"code","652a8876":"code","ce3a16b1":"code","5174e510":"code","5bf8898d":"code","c250218a":"code","cb78139e":"code","17183370":"code","5ced48eb":"code","fa1cbcfa":"code","d6dc7168":"code","55215cd9":"code","112c47c7":"markdown","a3dcfb5d":"markdown","6241a72b":"markdown","8a8b38e5":"markdown","5caaeb51":"markdown","97de3bc6":"markdown","f4a7fbc8":"markdown","a874bf4e":"markdown","5eed8f17":"markdown","ed20a2fa":"markdown","1366f253":"markdown","523567a6":"markdown","3aecaa24":"markdown","d6e243bd":"markdown","ebb6383a":"markdown","7f9bdb04":"markdown","eeec3d8e":"markdown","a2dd38ea":"markdown","8e3ea6cc":"markdown","114b5ff7":"markdown","592cda64":"markdown","283aa26b":"markdown","03a2ee78":"markdown","b9a758f1":"markdown","51de3ffc":"markdown","20521239":"markdown","33e0a4df":"markdown","2579c7ea":"markdown","c2840122":"markdown","21832b6a":"markdown","87169bbd":"markdown","53f54143":"markdown","662a6599":"markdown","5802c390":"markdown","0ad438d0":"markdown","b035654b":"markdown","4cf724f5":"markdown","e53326b9":"markdown","05573501":"markdown","fdf1c4a8":"markdown","8cc9c86f":"markdown","d83812c8":"markdown","66e459ff":"markdown","202c54e5":"markdown","ff9467c8":"markdown","4a0a7c27":"markdown","945bc871":"markdown","91b71405":"markdown","6b72090b":"markdown"},"source":{"72c427a2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\n","9859ac0a":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint('The size of the training set: ', train.shape)\nprint('The size of the test set is: ' ,test.shape)","23740a14":"test.head()","74573e37":"train.describe()","9901d87f":"# To get more information about the data you are working with, it is good to use the info() method\n\ntrain.info()","5b52dd6c":"Passenger_Id = test['PassengerId']","39353dd6":"#we keep this for when we will be separating DATA back into train and test \n\ntrain_rowsize = train.shape[0]\ntest_rowsize = test.shape[0]","2da4fc27":"#The different data types \ntrain.dtypes.value_counts()","77029661":"test.dtypes.value_counts()","1bc047a5":"data = pd.concat((train, test))\n\n# we drop the preditor from the data \ndata.drop('Survived', axis = 1, inplace = True)\ndata.drop('PassengerId', axis = 1, inplace = True)","ceb3d825":"data.hist(figsize=(10,10));","f0b93df1":"#we find out the percentage of survivers by gender\ndata.Sex.value_counts(normalize = True)","2641d34f":"embarked_counts = data.Embarked.value_counts(normalize = True)\nembarked_counts","83acb675":"embarked_counts.plot(kind='bar')\nplt.title(\"Passengers per boarding gates\");","792a6c95":"train['Died'] = 1 - train['Survived']\ntrain.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind = 'bar', figsize = (10, 5), stacked = True);","c95feb92":"ax = plt.subplot()\nax.set_ylabel('Average fare')\ndata.groupby('Pclass').mean()['Fare'].plot(kind='bar', figsize=(10, 5), ax = ax);","2465592a":"plt.figure(figsize=(10, 5))\nplt.hist([train[train['Survived'] == 1]['Fare'], train[train['Survived'] == 0]['Fare']], \n         stacked=True,bins = 50, label = ['Survived','Dead'])\nplt.xlabel('Fare')\nplt.ylabel('Number of passengers')\nplt.legend();","0db3067a":"#Here is a list of all the features with Nans and the number of null for each features\nnull_values = data.columns[data.isnull().any()]\nnull_features = data[null_values].isnull().sum().sort_values(ascending = False)\nmissing_data = pd.DataFrame({'No of Nulls' :null_features})\nmissing_data","4f346571":"test.isnull().sum()","1684f882":"train.isnull().sum()","22e34265":"#Fare, embarked, Cabin and Age all have missing values. Lets plot the missing values\nimport warnings\nwarnings.filterwarnings('ignore')            #to silence warnings\n\n%matplotlib inline\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nplt.figure(figsize= (10, 5))\nplt.xticks(rotation='90')\nax = plt.axes()\nsns.barplot(null_features.index, null_features)\nax.set(xlabel = 'Features', ylabel = 'Number of missing values', title = 'Features with Missing values');","e0b758c6":"data[\"Embarked\"] = data[\"Embarked\"].fillna('S')","8bfbe1d9":"data[data['Fare'].isnull()]","093a0825":"def fill_missing_fare(df):\n    median_fare=df[(df['Pclass'] == 3) & (df['Embarked'] == 'S')]['Fare'].median()\n#'S'\n       #print(median_fare)\n    df[\"Fare\"] = df[\"Fare\"].fillna(median_fare)\n    return df\n\ndata=fill_missing_fare(data)","3fa11fc7":"%matplotlib inline\n\nsns.set_context('notebook')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nplt.figure(figsize= (10, 5))\n\nax = sns.distplot(data[\"Age\"].dropna(),   #plot only the numerical data\n                  color = 'green',\n                 kde = False)    \nax.grid(True)\n\nax.set(xlabel = 'Age', ylabel = 'Number of people', title = 'Age range of Passengers');","1f0e8980":"# we will genrate a set of random values from 0 to 80 and missing ages with any one of these values\n\nsizeof_null = data[\"Age\"].isnull().sum()\nrand_age = np.random.randint(0, 80, size = sizeof_null)","8ec5bd7f":" # fill NaN values in Age column with random values generated\n    \nage_slice = data[\"Age\"].copy()\nage_slice[np.isnan(age_slice)] = rand_age\ndata[\"Age\"] = age_slice\ndata[\"Age\"] = data[\"Age\"].astype(int)","d1eca29b":"data['Age'] = data['Age'].astype(int)\ndata.loc[ data['Age'] <= 18, 'Age'] = 0\ndata.loc[(data['Age'] > 18) & (data['Age'] <= 35), 'Age'] = 1\ndata.loc[(data['Age'] > 35) & (data['Age'] <= 60), 'Age'] = 2\ndata.loc[(data['Age'] > 60) & (data['Age'] <= 80), 'Age'] = 3\n\ndata['Age'].value_counts()","1968fdd9":"data.sample(10)","c6f83965":"data['Cabin'].dropna().sample(10)","5eebda78":"data[\"Deck\"]=data['Cabin'].str[0]\n\ndata['Deck'].unique()","58a7b995":"data['Deck'] = data['Deck'].fillna('H')   # replacing the nan with H","e3462b98":"data['Deck'].unique()","e265d856":"# we include a new feauture, the Familysize including the passengers\ndata[\"FamilySize\"] = data[\"SibSp\"] + data[\"Parch\"]+ 1\ndata['FamilySize'].value_counts()","03dfa8d3":"data.loc[ data['FamilySize'] == 1, 'FSize'] = 'Single family'\ndata.loc[(data['FamilySize'] > 1) & (data['FamilySize'] <= 5), 'FSize'] = 'Small Family'\ndata.loc[(data['FamilySize'] > 5), 'FSize'] = ' Extended Family'","d36ed204":"data.head()","437e2990":"le = LabelEncoder()\n\ndata['Sex'] = le.fit_transform(data['Sex'])\ndata['Embarked'] = le.fit_transform(data['Embarked'])\ndata['Deck'] = le.fit_transform(data['Deck'])\ndata['FSize'] = le.fit_transform(data['FSize'])","7f1d1608":"data['Sex'].unique()","41f97046":"data.dtypes.value_counts()","d628b38e":"data = data.drop(['Name', 'Ticket','Cabin',], axis = 1)","0f0538a2":"#we check for skewness in  data\n\nskew_limit = 0.75\nskew_vals = data.skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skewness'})\n            .query('abs(Skewness) > {0}'.format(skew_limit)))\n\nskew_cols","270f9342":"print(\"There are {} skewed numerical features to  transform\".format(skew_cols.shape[0]))","b009c2a2":"tester = 'Deck'\nfig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(16,5))\n#before normalisation\ndata[tester].hist(ax = ax_before)\nax_before.set(title = 'Before nplog1p', ylabel = 'Frequency', xlabel = 'Value')\n\n#After normalisation\ndata[tester].apply(np.log1p).hist(ax = ax_after)\nax_after.set(title = 'After nplog1p', ylabel = 'Frequency', xlabel = 'Value')\n\nfig.suptitle('Field \"{}\"'.format(tester));","a2329c74":"skewed = skew_cols.index.tolist()\ndata[skewed] = data[skewed].apply(np.log1p)","9f653183":"# Correlation between the features and the predictor- Survived\npredictor = train['Survived']\ncorrelations = data.corrwith(predictor)\ncorrelations = correlations.sort_values(ascending = False)\n# correlations\ncorrs = (correlations\n            .to_frame()\n            .reset_index()\n            .rename(columns={'level_0':'feature1',\n                                0:'Correlations'}))\ncorrs","e326b41e":"plt.figure(figsize= (10, 5))\nax = correlations.plot(kind = 'bar')\nax.set(ylabel = 'Pearson Correlation', ylim = [-0.4, 0.4]);","dcc395a4":"#importing libraries\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier\n","d19fe59a":"# First i need to break up my data into train and test\ntrain_new = data[:train_rowsize]\ntest_new = data[train_rowsize:]\ntest_new.shape","90e011d1":"train_new.dtypes.value_counts()","b12bb63b":"train_new.head()","e14c4476":"test_new.dtypes.value_counts()","9c58969d":"test_new.head()","154e585f":"n_folds = 5\n\nkf = KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(train_new)\n","13886c0a":"y_train = train.Survived\nn_folds = 5\n    \ndef f1_score (model): \n    kf = KFold(n_folds, shuffle = True, random_state = 42).get_n_splits(train_new)\n    rmse = np.sqrt(cross_val_score(model, train_new, y_train, scoring = 'f1', cv = kf))\n    # f1 because it is the sweet spot between recall and precision\n    return (rmse)","652a8876":"logreg = LogisticRegression()\nrf = RandomForestClassifier()\ngboost = GradientBoostingClassifier()\nxgb = XGBClassifier()\nlgbm = LGBMClassifier()","ce3a16b1":"# from sklearn.metrics import SCORERS\n# print(SCORERS.keys())","5174e510":"score = f1_score(logreg)\nprint(\"\\nLogistic regression score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","5bf8898d":"score = f1_score(rf)\nprint(\"\\nRandom Forest score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","c250218a":"score = f1_score(gboost)\nprint(\"\\nGradient Boosting Classifier score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","cb78139e":"score = f1_score(xgb)\nprint(\"\\neXtreme Gradient BOOSTing score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","17183370":"score = f1_score(lgbm)\nprint(\"\\nLight Gradient Boosting score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","5ced48eb":"all_classifier = VotingClassifier(estimators=[('logreg', logreg), ('rf', rf), \n                                              ('gboost', gboost), ('xgb', xgb),\n                                             ('lgbm', lgbm)], voting='soft')\n\nVC = all_classifier.fit(train_new, y_train)","fa1cbcfa":"score = f1_score(VC)\nprint(\"\\nVoting Classifier score: mean = {:.4f}   std = ({:.4f}) \\n\".format(score.mean(), score.std()))","d6dc7168":"prediction = VC.predict(test_new)","55215cd9":"titanic_submission = pd.DataFrame ({\"PassengerId\": test[\"PassengerId\"],\n                             \"Survived\": prediction})\ntitanic_submission.to_csv('Titanic_Submission.csv', index = False)\n\ntitanic_submission.sample(10)\n","112c47c7":"Looks like males were more likely to Succumb than female. :|","a3dcfb5d":"I will be using  a nplog1p transformation to standardise the skew columns.","6241a72b":"The chart above may not shown any values for the fare and embarked but they do have some missing values.\n\nHaving missing values or NaNs in your dataset can couse error with come machine learning algorithms. We could try to replace the missing values with\n\n* A constant value that has meaning within the domain like a 0, or\n* We could replace them with the mean, median or mode of values\n* With another values selected from a random record or another predicted model\n\nWe will start imputing the missing values in our dataset\n","8a8b38e5":"Actually the fare price is correlated with the Passenger class. Those who paid more where in Pclass 1 and as we can see in the chart below, passengers with lower fare were more likely to succumb.","5caaeb51":"# Visualisation:","97de3bc6":"In general, to do feature enginnering easily, i like to merge the train and the test data. This facilitates working on both datasets once instead of having to do one thing on the train data and the same thing on the test data. But it is totally up to you to merge the 2 datasets or not, either ways work.","f4a7fbc8":"# Converting Categoricals to numericals","a874bf4e":"# Correlations:","5eed8f17":"# Introduction:","ed20a2fa":"Why do we normalise datasets:\n    \nThe skewed data have to be normalise because many of the algorithms in data assume that the data science is normal and calculate various stats assuming this. So the more the data is close to normal the more it fits the assumption.\nif you want to read more:\nhttps:\/\/stats.stackexchange.com\/questions\/189652\/is-it-a-good-practice-to-always-scale-normalize-data-for-machine-learning","1366f253":"* ** How does our Base models score **","523567a6":"Lets creat age groups. we will divide the passemgers into children(0 - 18), young adults (19 - 35), late adult (36 - 60) and the elderly (61 - 80)","3aecaa24":"* ** Voting Classifier Ensembling **","d6e243bd":"Let's visualize survival based on gender:","ebb6383a":"\n**Title: TITANIC SURVIVAL PREDICTION**\n\nMikel Kengni \/ December 2018\n\nPlan:\n\n**First things First: Snacks and Chips checked, Coffee checked**\n\n* **Introduction**\n\n 1. Import the libraries\n \n 2. Explore the dataset\n \n 3. Feature exploration and Cleaning\n \n \n* **Correlation with predictor**\n\n\n* **Feature Engineering**\n\n 1. Encoding the categorical data\n\n 2. Feature scaling and Normalization\n\n 3. Checking for skewness in the numerical data\n\n\n* **Prediction**\n\n 1. Split the data into train and Validation\n\n 2. Build the model\n\n 3. Feature importance\n\n 4. Predictions\n\n 5. Stacking\n    ","7f9bdb04":"# 3- Feature exploration and Cleaning","eeec3d8e":"Stacking is a mechanism that uses the 'Unity is Strength' principle. Basically, it uses multiple machine learning algorithms at the same time, to obtain a better performance and thus a better prediction\n\nStacking Technique involves the following Steps:-\n\n* Split the training data into 2 disjoint sets\n* Train several Base Learners on the first part\n* Test the Base Learners on the second part and make predictions\n* Using the predictions from (3) as inputs,the correct responses from the output,train the higher level learner or meta level Learner\n\nTo learn more about stackin, You can read this: https:\/\/medium.com\/@gurucharan_33981\/stacking-a-super-learning-technique-dbed06b1156d ","a2dd38ea":"**EMBARKED:**\n\nWe observed that there was only missing data for the embarked feature  and the most used \ngate for embarkement is was 'S' so i will fill the missing embarked with the median of all boarding gates i.e 'S'","8e3ea6cc":"The before normalisation shows a right skewed distribution or positive skewed feature. After nplog1p, the distribution is more symmetrical.\n\n**BTW: You can change the tester and see the different features before and after nplog1p normalization**\n","114b5ff7":"We can see that features like fare, sibsp and parch are very much skewed to the left. We will have to normalise them at some point. It is always better to have them symmetrcal or centered rather than have them tilting ton one side as this may affect your final results.","592cda64":"I will be using a numpy  log1p to transform  the very skewed data. Just to give a sense of what numpy log1p tranformation does in a skewed dataset, We are going to design a before and after log1p .","283aa26b":" * ** Define the Error method: **\n        \nI will be using the Root mean square error method from sklearn ","03a2ee78":"**FARE:**\n\nlets take a look at the person with the missing fare    ","b9a758f1":"we can see that the passenger is a male in his 60s who embarked at gate 'S', with a pclass = 3. Lets fill his fare with the median of all passengers of pclass == 3 who embarked at gate 'S'. ","51de3ffc":"* ** Submission **","20521239":"We observe that the train set has 5 categorical features(the object features) and 7 numerical features while the test data has 4 categorical features and 7 numerical features. We will have to separate the categorical feature from the numerical features and then use and encoding method to convert the categorical features to numerical features. ","33e0a4df":"* **Defining a 5 fold cross validation split Strategy for the train_new set **","2579c7ea":"observation:\n* Age groups range from 0 to 80\n* The largest number of age group was 20 ~ 22\n* About 100 children( ages from 0  - 18), we shall find the exact number later\n* less that 100 elderly people(ages from 60 -80)","c2840122":"The Survived column is the target variable and is the feature we are going to predict. If Suvival = 1 the passenger survived, otherwise he's dead.\n\n**The other variables describe the passengers are:**\n\n* PassengerId: id given to each traveler on the boat\n* Pclass: The passenger class. It has three possible values: 1,2,3 (first, second and third class)\n* Name of the passeger:\n* Sex: Either  Male or Female\n* Age:\n* SibSp: number of siblings and spouses traveling with the passenger\n* Parch: number of parents and children traveling with the passenge\n* Ticket number: \n* Ticket Fare\n* Cabin number\n* Embarkation Gate. This describe three possible gates on the Titanic from which the people embarked(gates: S,C,Q)","21832b6a":"We can observe that 64 percent of all the titenic passengers were male. and 36 percent were females. Of the 64 percent of mlae, lets find out how what percentage survived.","87169bbd":"**Cabin:**","53f54143":"Observation:\nWe can see from the describe method obove that:\n* there were about 891 passengers on board(count = 891 for most of the columns)\n* There may be about 177(891 - 714) missing data in Age column ","662a6599":"** Age: **\n    \nlets plot the age distribution to get an idea of what we are working with","5802c390":"We can put all our classifiers togeter into one classifier and use it to predict our test set, provided it score better than all the other individual models. As shown below, the voting classifier scores far better than the other classifiers. :)","0ad438d0":"For the newcomers: This is basically how to find the missing data. since i merged both train and test data. The number of missing data above is the sum of the number of missing data in the train and in the test sets.","b035654b":"# Modelling ","4cf724f5":"Is there any relation ship between the fare tickets and the Passenger Vlass?","e53326b9":"# Ensembling- Stacking","05573501":"# How Skewed is our dataset?","fdf1c4a8":"Lets take alook at how skewed our dataset is. Just a little background:\n\n* In statistics, skewness is a measure of the asymmetry of the probability distribution of a random variable about its mean. In other words, skewness tells you the amount and direction of skew (departure from horizontal symmetry). The skewness value can be positive or negative, or even undefined. If skewness is 0, the data are perfectly symmetrical, although it is quite unlikely for real-world data. As a general rule of thumb:\n\n* If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n* If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n* If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n\nreferences:https:\/\/help.gooddata.com\/display\/doc\/Normality+Testing+-+Skewness+and+Kurtosis","8cc9c86f":"# 1- Importing Libraries","d83812c8":"kernels used\/ References:\n    \n* https:\/\/www.kaggle.com\/poonaml\/titanic-survival-prediction-end-to-end-ml-pipeline?scriptVersionId=1124380\n* https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n* https:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html\n    ","66e459ff":"# Imputing missing values","202c54e5":"* ** Defining My Base models **","ff9467c8":"# 2- Loading and Reading data","4a0a7c27":"If we can extract the letters in front of every cabin, which refers to the **Deck** on the ship, we can have the position of a passenger on the titanic. ","945bc871":"Lets take a look at the number of missing data in both the train and the test sets","91b71405":"Since we will need the passenger ID in the submission file, we save it for easy access later","6b72090b":"Here we can observe that there are some missing data in the Cabin column( about 687 missing data) and the enbarked column has 2 missing data. "}}