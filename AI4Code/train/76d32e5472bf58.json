{"cell_type":{"1de85225":"code","57088b47":"code","5d0f1d77":"code","61b6a20d":"code","173e446b":"code","aa9d586a":"code","f2bb0b1a":"code","f5dac7a5":"code","4a21ac56":"code","467f4289":"code","b66e871d":"code","66fdd875":"code","81dfc1f8":"code","ebc52b5d":"code","92e0661c":"code","64d5b66b":"code","f74f9222":"code","f04156b8":"code","e7aee890":"code","e93314a6":"code","9fa3f701":"code","f4b3cd05":"code","6f923622":"code","9889e96b":"code","e135f6ef":"code","56d89c24":"code","3bc9e719":"code","b3ab5c23":"code","ead385c5":"code","b91363eb":"code","220c1dde":"code","3c84f94e":"code","d427f164":"code","9deb88d8":"code","72e84497":"code","6ad99f45":"code","310f0726":"code","61bd85d5":"markdown","2eafd227":"markdown","041f227b":"markdown","0d50b2e9":"markdown","779ed792":"markdown","87bb8e5b":"markdown","f51fb03c":"markdown","78c3df8f":"markdown","d9bf37d6":"markdown","4249c3bd":"markdown","150b9024":"markdown","7e3db17c":"markdown"},"source":{"1de85225":"%reset -sf","57088b47":"import os, collections, random, itertools\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","5d0f1d77":"for dirname, _, filenames in os.walk('\/kaggle\/input'): \n    for filename in filenames: print(os.path.join(dirname, filename))","61b6a20d":"# load data\ndf = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")\ndf[\"question1\"] = df[\"question1\"].astype(str)  # resolve nan\ndf[\"question2\"] = df[\"question2\"].astype(str)\ndf[\"qid1\"] = df[\"qid1\"] - 1\ndf[\"qid2\"] = df[\"qid2\"] - 1\nmaxidx = max(max(df[\"qid1\"]), max(df[\"qid2\"])) + 1","173e446b":"df.sample(10)","aa9d586a":"# all questions are identified with its qid\nqid_to_question = {}\nfor qid1, qid2, question1, question2 in zip(df[\"qid1\"], df[\"qid2\"], df[\"question1\"], df[\"question2\"]):\n    qid_to_question[qid1] = question1\n    qid_to_question[qid2] = question2","f2bb0b1a":"print(\"Number of questions\", len(qid_to_question))\nprint(\"Number of duplicate pairs\", sum(df[\"is_duplicate\"]))\nprint(\"Percentage of pairs that are duplicate {:.3f}%\".format(sum(df[\"is_duplicate\"])\/len(qid_to_question)*100))","f5dac7a5":"qid_to_labelled_qids = collections.defaultdict(set)\nqid_to_duplicate_qids = collections.defaultdict(set)\nfor qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n    qid_to_labelled_qids[qid1].add(qid2)\n    qid_to_labelled_qids[qid2].add(qid1)\n    if is_duplicate:\n        qid_to_duplicate_qids[qid1].add(qid2)\n        qid_to_duplicate_qids[qid2].add(qid1)","4a21ac56":"plt.figure(figsize=(14,4))\nplt.title(\"Number of labels for each question, and how many of which are duplicate\")\n\nlabel_sizes = [len(qid_to_labelled_qids[qid]) for qid in qid_to_question]\ncount, bins = np.histogram(label_sizes, bins=range(max(label_sizes)+2))\ncount = count*bins[:-1]   # convert number of groups to population\nplt.bar(bins[1:-1], count[1:], width=1, label=\"total\")\n\nduplicate_sizes = [len(qid_to_duplicate_qids[qid]) for qid in qid_to_question]\ncount, bins = np.histogram(duplicate_sizes, bins=range(max(duplicate_sizes)+2))\ncount = count*bins[:-1]   # convert number of groups to population\nplt.bar(bins[1:-1], count[1:], width=1, label=\"duplicate\")\nplt.xlim(0,50)\nplt.xlabel(\"Count\")\nplt.ylabel(\"Dataset Frequency\")\nplt.legend()\nplt.show()","467f4289":"print(\"Largest label sizes:\", sorted(label_sizes)[-20:])\nprint(\"Largest duplicate sizes:\", sorted(duplicate_sizes)[-20:])","b66e871d":"import typing\n\n\nclass DisjointSet:\n    # https:\/\/github.com\/not522\/ac-library-python\/blob\/master\/atcoder\/dsu.py\n    def __init__(self, n: int = 0) -> None:\n        self._n = n\n        self.parent_or_size = [-1] * n\n\n    def union(self, a: int, b: int) -> int:\n        assert 0 <= a < self._n\n        assert 0 <= b < self._n\n\n        x = self.leader(a)\n        y = self.leader(b)\n\n        if x == y:\n            return x\n\n        if -self.parent_or_size[x] < -self.parent_or_size[y]:\n            x, y = y, x\n\n        self.parent_or_size[x] += self.parent_or_size[y]\n        self.parent_or_size[y] = x\n\n        return x\n\n    def same(self, a: int, b: int) -> bool:\n        assert 0 <= a < self._n\n        assert 0 <= b < self._n\n\n        return self.leader(a) == self.leader(b)\n\n    def find(self, a: int) -> int:\n        return self.leader(a)\n    \n    def leader(self, a: int) -> int:\n        assert 0 <= a < self._n\n\n        parent = self.parent_or_size[a]\n        while parent >= 0:\n            if self.parent_or_size[parent] < 0:\n                return parent\n            self.parent_or_size[a], a, parent = (\n                self.parent_or_size[parent],\n                self.parent_or_size[parent],\n                self.parent_or_size[self.parent_or_size[parent]]\n            )\n\n        return a\n\n    def size(self, a: int) -> int:\n        assert 0 <= a < self._n\n\n        return -self.parent_or_size[self.leader(a)]\n\n    def groups(self) -> typing.List[typing.List[int]]:\n        leader_buf = [self.leader(i) for i in range(self._n)]\n\n        result: typing.List[typing.List[int]] = [[] for _ in range(self._n)]\n        for i in range(self._n):\n            result[leader_buf[i]].append(i)\n\n        return list(filter(lambda r: r, result))","66fdd875":"# all questions are identified with its qid\ndisjoint_set = DisjointSet(maxidx)\nfor qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n    if is_duplicate:\n        disjoint_set.union(qid1, qid2)","81dfc1f8":"cnt = 0\nfor qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n    if not is_duplicate:\n        if disjoint_set.find(qid1) == disjoint_set.find(qid2):\n            cnt += 1\n            if cnt < 10:\n                print(qid_to_question[qid1], \"\\n\", qid_to_question[qid2], \"\\n\")\nprint(cnt)","ebc52b5d":"group_sizes = np.array([len(group) for group in disjoint_set.groups()])\ncount, bins = np.histogram(group_sizes, bins=range(max(group_sizes)+2))\ncount = count*bins[:-1]   # convert number of groups to population\nplt.figure(figsize=(14,4))\nplt.title(\"Group size of similar questions\")\nplt.bar(bins[2:-1], count[2:], width=1)\nplt.xlim(0,50)\nplt.xlabel(\"Group Size\")\nplt.ylabel(\"Dataset Frequency\")\nplt.show()","92e0661c":"print(\"Largest group sizes:\", sorted(group_sizes)[-20:])","64d5b66b":"initial_connection_count = sum(duplicate_sizes)\nfinal_connection_count = sum(group_sizes)\ninitial_connection_count, final_connection_count, final_connection_count - initial_connection_count","f74f9222":"# define tokenisation process\n\nimport pickle, functools\nqid_to_tokens_preprocessed_filename = \"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_processed_token_list_tokenise_then_spellcheck.pkl\"\nwith open(qid_to_tokens_preprocessed_filename, \"rb\") as f:\n    qid_to_tokens_preprocessed = pickle.load(f)\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstopword_set = set(stopwords.words())\nstopword_set.update([\"?\"])\n\n@functools.lru_cache(maxsize=None)\ndef tokenise_qid(qid, qid_to_tokens_preprocessed=qid_to_tokens_preprocessed):\n    if qid_to_tokens_preprocessed:\n        return qid_to_tokens_preprocessed[qid]\n    \n    sentence = qid_to_question[qid]\n    return word_tokenize(sentence.lower())","f04156b8":"groups = disjoint_set.groups()\noverlap_count_duplicate = []\nfor group in tqdm.tqdm(groups):\n    for qid1,qid2 in itertools.combinations(group, r=2):\n        overlapping_tokens = set(tokenise_qid(qid1)) & set(tokenise_qid(qid2))\n        overlapping_tokens = list(token for token in overlapping_tokens if token not in stopword_set)\n        overlap_count_duplicate.append(len(overlapping_tokens))\n\n\noverlap_count_random = []\nsample1 = random.sample(qid_to_question.keys(), 20000)\nsample2 = random.sample(qid_to_question.keys(), 20000)\nfor qid1, qid2 in zip(sample1, sample2):\n    overlapping_tokens = set(tokenise_qid(qid1)) & set(tokenise_qid(qid2))\n    overlapping_tokens = list(token for token in overlapping_tokens if token not in stopword_set)\n    overlap_count_random.append(len(overlapping_tokens))","e7aee890":"plt.figure(figsize=(14,4))\nplt.hist(overlap_count_duplicate, bins=range(15), density=True, alpha=0.5, label=\"duplicate pair\")\nplt.hist(overlap_count_random, bins=range(15), density=True, alpha=0.5, label=\"random pair\")\nplt.title(\"Distribution of overlapping non-root word tokens for duplicate pairs and random pairs\")\nplt.legend()\nplt.show()","e93314a6":"from nltk.corpus import stopwords\nstopword_set = set(stopwords.words())","9fa3f701":"import pickle\nimport random\n\nfrom nltk.corpus import stopwords\nstopword_set = set(stopwords.words())\n\nwith open(\"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_processed_token_list_spellcheck_then_tokenise.pkl\", \"rb\") as f:\n    qid_to_tokens = pickle.load(f)\n\n# with open(\"..\/input\/quora-question-pairs-tokenise-pipeline\/token_to_qid_tokenise_then_spellcheck.pkl\", \"rb\") as f:\n#     token_to_qids = pickle.load(f)\n\ntoken_to_qids = collections.defaultdict(set)\nfor qid, tokens in qid_to_tokens.items():\n    for token in tokens:\n        token_to_qids[token].add(qid)","f4b3cd05":"# most common non-stop words, the question mark has been excluded\nsorted([(len(v),k) for k,v in token_to_qids.items() if k not in stopword_set], reverse=True)[:20]","6f923622":"token_length_sizes = []\nconsidered_set_sizes = []\nfor qid in tqdm.tqdm(random.sample(qid_to_tokens.keys(), 10000)):\n    considered_set = set()\n    for token in qid_to_tokens[qid]:\n        if token in stopword_set:\n            continue\n        if token in token_to_qids:  # some tokens are not found in the token_to_qids (probably from test set)\n            for considered_qid in token_to_qids[token]:\n                considered_set.add(considered_qid)\n    token_length_sizes.append(len(set(qid_to_tokens[qid])))\n    considered_set_sizes.append(len(considered_set))","9889e96b":"plt.figure(figsize=(14,4))\nplt.hist(considered_set_sizes, bins=np.arange(0,70000,1000), density=True)\nplt.title(\"How many other questions has at least one common non-rootword token\")\nplt.xlabel(\"Query comparison size\")\nplt.legend()\nplt.show()","e135f6ef":"plt.figure(figsize=(14,4))\nplt.scatter(considered_set_sizes, token_length_sizes, alpha=0.1)\nplt.title(\"Relationship between number of unique tokens and query comparison size\")\nplt.ylabel(\"Number of unique tokens\")\nplt.xlabel(\"Query comparison size\")\nplt.xlim(None,70000)\nplt.show()","56d89c24":"# model_name = \"bert-base-nli-stsb-mean-tokens\"\n# sentence_vectors = np.load(f\"..\/input\/quora-question-pairs-bert-sentence-vectors\/sentence_vectors_{model_name}.npy\")","3bc9e719":"with open(\"..\/input\/quora-question-pairs-tokenise-pipeline\/qid_to_vec_trf.pkl\", 'rb') as f:\n    qid_to_vec = pickle.load(f)\nsentence_vectors = []\nfor idx in sorted(qid_to_vec.keys()):\n    sentence_vectors.append(qid_to_vec[idx])\nsentence_vectors = np.array(sentence_vectors)","b3ab5c23":"NUM_LARGEST_GROUPS = 1\nlargest_groups = sorted(disjoint_set.groups(), key=len)[-NUM_LARGEST_GROUPS:]\nqids_of_largest_groups = np.array(sum(largest_groups, []))  # flatten","ead385c5":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nsentence_pca = pca.fit_transform(sentence_vectors)","b91363eb":"from matplotlib import collections  as mc\n\ndef plot_2d_distribution(vectors_2d, qids_to_connect, title=\"\"):\n    qids_to_connect = set(qids_to_connect)\n    plt.figure(figsize=(10,8))\n    plt.scatter(*list(zip(*vectors_2d))[:2], s=1, alpha=0.1)\n    lines = []\n    for qid1, qid2, is_duplicate in zip(df[\"qid1\"], df[\"qid2\"], df[\"is_duplicate\"]):\n        if is_duplicate and qid1 in qids_to_connect and qid2 in qids_to_connect:\n            lines.append([vectors_2d[qid1][:2], vectors_2d[qid2][:2]])\n    lc = mc.LineCollection(lines, color=\"red\", alpha=0.2,\n                           linewidths=[1\/(0.1 + (a-c)**2 + (b-d)**2)**0.5 for (a,b),(c,d) in lines])\n    plt.gca().add_collection(lc)\n    plt.title(title)\n\n    mx, my = np.nanmean(vectors_2d, axis=0)\n    sx, sy = np.nanstd(vectors_2d, axis=0)\n    plt.xlim(mx - 4*sx, mx + 4*sx)\n    plt.ylim(my - 4*sy, mx + 4*sy)\n    plt.show()","220c1dde":"plot_2d_distribution(sentence_pca, qids_of_largest_groups, \n                     \"Plot of PCA projection of all word embeddings, {} largest group(s) highlighted\".format(NUM_LARGEST_GROUPS))","3c84f94e":"!git clone https:\/\/github.com\/DmitryUlyanov\/Multicore-TSNE.git > \/dev\/null && cd Multicore-TSNE\/ && pip install . > \/dev\/null && rm -rf .\/Multicore-TSNE ","d427f164":"NUM_LARGEST_GROUPS = 20\nlargest_groups = sorted(disjoint_set.groups(), key=len)[-NUM_LARGEST_GROUPS:]\nqids_of_largest_groups = np.array(sum(largest_groups, []))  # flatten","9deb88d8":"from MulticoreTSNE import MulticoreTSNE as TSNE\n\n# it was recommended by scipy that we first reduce the dimensions\nqids_to_fit_tsne = np.array(list(set(qids_of_largest_groups) | set(random.sample(qid_to_question.keys(), 20000))))\nsentence_pca = PCA(n_components=50).fit_transform(sentence_vectors[qids_to_fit_tsne])\ntsne = TSNE(n_jobs=4)\nsentence_tsne = np.empty((sentence_vectors.shape[0], 2))\nsentence_tsne[:] = np.nan\nsentence_tsne[qids_to_fit_tsne] = tsne.fit_transform(sentence_pca)","72e84497":"plot_2d_distribution(sentence_tsne, qids_of_largest_groups,\n                     \"Plot of T-SNE projection of {} largest groups and 20000 other questions\".format(NUM_LARGEST_GROUPS))","6ad99f45":"import itertools\nfrom scipy.spatial.distance import cosine\n\ndistances = []\nfor group in tqdm.tqdm(disjoint_set.groups()):\n    for qid1,qid2 in itertools.combinations(group, r=2):\n        distance = cosine(sentence_vectors[qid1], sentence_vectors[qid2])\n        distances.append((distance,qid1,qid2))\n\ndistances = sorted(distances)","310f0726":"for distance,qid1,qid2 in distances[-10:]:\n    print(f\"Distance: {distance:.2f}\\n{qid_to_question[qid1]}\\n{qid_to_question[qid2]}\\n\")","61bd85d5":"### Simple Analysis of the dataset","2eafd227":"### Visualise distribution of overlapping word count for duplicate pairs","041f227b":"### Counting the number of inconsistent nonduplicate labels","0d50b2e9":"### Visualise distribution of sentence vectors","779ed792":"### Visualise distribution of the number of questions to compare against","87bb8e5b":"### Understand the most frequent non rootword tokens","f51fb03c":"### Counting the number of augmented connections","78c3df8f":"### Connecting similar questions","d9bf37d6":"### Visualising the group size of similar questions","4249c3bd":"### Indexing the questions","150b9024":"#### Duplicate questions with largest cosine distance of sentence vectors","7e3db17c":"# Dataset Visualisation\n\nThis notebook is solely use to generate visualisations on the dataset."}}