{"cell_type":{"b635aa7f":"code","04e8abd4":"code","759a035c":"code","b0929a9e":"code","3467763f":"code","94888722":"code","bb92b7af":"code","e5f72ec1":"code","783b6f23":"code","12c0ab87":"code","51ca0b1b":"code","3cac9383":"code","c1f28cc8":"code","8b75173d":"code","e71c7f41":"code","ccfe22dd":"code","e49c141d":"code","f90e6b82":"code","215b4e4a":"code","b986f178":"code","fcf4c1bc":"code","9a8c0c0a":"code","8e3db463":"code","19850840":"code","c5087510":"code","271067c7":"code","1bb77bf2":"code","e7eee07a":"code","1a779150":"code","d0a342f2":"code","b5c020a3":"code","bee02770":"code","c35d5576":"code","879d7301":"code","63a4ff6f":"code","511229ef":"code","a6573e51":"code","fddf3457":"code","36e1b522":"code","a44b0955":"code","4a7b9af0":"code","f0deef2a":"code","6b3ac39f":"code","9fcad680":"code","f1aee34f":"code","72278361":"markdown","44719d1a":"markdown","7e991c6b":"markdown","ac8bd7c5":"markdown","37570726":"markdown","71670792":"markdown","8175a42c":"markdown","3a3fec27":"markdown","88925e16":"markdown","3b34af57":"markdown","6c36e02a":"markdown","18cf1010":"markdown","62a8b6f6":"markdown","8aedc39b":"markdown","bf665d6c":"markdown","d8669d22":"markdown","5a6c656a":"markdown","2759578e":"markdown","f9325aa0":"markdown","739867e7":"markdown","4276952a":"markdown"},"source":{"b635aa7f":"import pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\n%matplotlib inline","04e8abd4":"#Medical Cost Personal Datasets\ndata = pd.read_csv(r'..\/input\/vehicle-dataset-from-cardekho\/CAR DETAILS FROM CAR DEKHO.csv')\ndata","759a035c":"data.head()","b0929a9e":"data.shape","3467763f":"#removing duplicate entries\ndata.drop_duplicates(keep='first',inplace=True)\ndata.shape","94888722":"#check missing or null values\ndata.isnull().sum()","bb92b7af":"data.info()","e5f72ec1":"data.describe(include=\"all\")","783b6f23":"del data[\"name\"]","12c0ab87":"data.columns","51ca0b1b":"data=data[['year', 'km_driven', 'fuel', 'seller_type',\n       'transmission', 'owner', 'selling_price']]","3cac9383":"#checking unique values for the categorical data\nprint(data[\"fuel\"].unique())\nprint(data[\"seller_type\"].unique())\nprint(data[\"transmission\"].unique())\nprint(data[\"owner\"].unique())\n","c1f28cc8":"datac=data.copy(deep=True)","8b75173d":"#Converting the categorical to indicator variables.\ndata=pd.get_dummies(data,drop_first =True)\ndata.head()","e71c7f41":"from datetime import date\nyear=date.today().year\nyear\ndata.year = year-data.year","ccfe22dd":"data.columns","e49c141d":"data.head()","f90e6b82":"sns.jointplot(x='year', y='selling_price',data = data)","215b4e4a":"sns.jointplot(x='year', y='selling_price',data = data, kind= 'hex')","b986f178":"sns.set(color_codes=True)\nsns.lmplot(x='km_driven', y='selling_price',data = data)\n","fcf4c1bc":"sns.pairplot(datac[['year', 'km_driven','owner', 'selling_price']],hue='owner', markers=[\"o\", \"s\", \"D\",\"p\",\"*\"])  \n#\"Owner\" can be replaced with 'Fuel_Type_Diesel','Fuel_Type_Petrol','Seller_Type_Individual','Transmission_Manual' for more plots.","9a8c0c0a":"sns.catplot(data=datac, kind=\"swarm\", x=\"owner\", y=\"selling_price\", col=\"seller_type\")","8e3db463":"correlations=datac.corr()\ncorrelations","19850840":"correlations=data.corr()\ncorrelations","c5087510":"data","271067c7":"y = data[\"selling_price\"].values\nX = data.drop(columns=\"selling_price\").values","1bb77bf2":"#Train-test split of the data\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)\n\n","e7eee07a":"columns = data.columns.tolist()","1a779150":"columns.remove(\"selling_price\")","d0a342f2":"columns","b5c020a3":"#columns to be standardized : km_driven,year\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\ny_train = sc.fit_transform(y_train.reshape(-1, 1))\ny_test = sc.fit_transform(y_test.reshape(-1, 1))","bee02770":"X_train","c35d5576":"X_test","879d7301":"y_train","63a4ff6f":"y_test","511229ef":"#Importing libraries\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error","a6573e51":"#linear regression\nmodel=linear_model.LinearRegression()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nprint('Coefficients: \\n', model.coef_)","fddf3457":"ridgeregr = linear_model.Ridge(alpha=30, normalize =True)\n\n# Train the model using the training sets\nridgeregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\nridge_y_pred = ridgeregr.predict(X_test)\nprint('Coefficients: \\n', ridgeregr.coef_)","36e1b522":"lasso =linear_model.Lasso(alpha=50, normalize = False) \nlasso.fit(X_train,y_train)\n\n# Make predictions using the testing set\nlasso_y_pred = lasso.predict(X_test)\nprint('Coefficients: \\n', lasso.coef_)","a44b0955":"plt.pyplot.scatter(y_test, y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, y_pred))","4a7b9af0":"sns.distplot(y_test-y_pred)","f0deef2a":"plt.pyplot.scatter(y_test, ridge_y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, ridge_y_pred))","6b3ac39f":"sns.distplot(y_test-ridge_y_pred)","9fcad680":"plt.pyplot.scatter(y_test, lasso_y_pred)\nplt.pyplot.ylabel('Predicted')\nplt.pyplot.xlabel('Actual')\nprint('Mean squared error: %.4f' % mean_squared_error(y_test, lasso_y_pred))","f1aee34f":"sns.distplot(y_test-lasso_y_pred)","72278361":"# Training the regression models.\n","44719d1a":"Here we have a dataset of car details on which we try to fit some regression models to predict the selling prices of the car.","7e991c6b":"# DATA PREPROCESSING","ac8bd7c5":"Let's emphasize the last assumption, the normal distribution of the error wich is nothing but the difference in values between the actual target value and the predicted target value. Let's create a visualization of this difference and verify our implementation of the regression model on our data.","37570726":"# Vizualisation","71670792":"# Performance Evaluation. ","8175a42c":"We now use pairplot to detect any multicollinearity between the predictors. While the column of charges in the plot will give us the dependence of the response variable on the predictor variable.","3a3fec27":"# Training and Testing Data.\nWe now go ahead and split the data into training and testing sets. We define a variable X that will contain all the columns except the target column and store the target column, i.e, \"Selling_Price\" in another variable, say y.\n ","88925e16":"  For Lasso, the coefficients can be evaluated to zero. Hence by the coefficients we can say that no feature has any significant impact on the target variable.\n  This is the reason why Lasso regression is used for feature selection.","3b34af57":"CONCLUSION:\nIn this case, it is clear from the figures that Lasso performs the worst.","6c36e02a":"Enhancing a scatterplot by including a linear regression model (and its uncertainty) using lmplot().","18cf1010":"We use seaborn to create a jointplot to compare various columns to vizualise the correlation between them [to vaguely estimate the strength of the correlation or presence of multicollinearity(Multicollinearity generally occurs when there are high correlations between two or more predictor variables.)].","62a8b6f6":"# Feature Scaling","8aedc39b":"The distribution of error follows normal distribution indeed. Though for plain regression model, the distribution is slightly left(negatively) skewed,i.e, it has more negative values compared to the number of positive values.","bf665d6c":"Hence we have 5 categorical columns. If we do not drop \"Car_Name\" before converting the categorical data to indicators, the resultant data will give 98+ features making the data redundant. Hence:","d8669d22":"Before we evaluate our performance, I would like to mention the four principle assumptions which justify the use of linear regression models for purposes of inference or prediction, i.e,\n\"(i) linearity and additivity of the relationship between dependent and independent variables:\n\n    (a) The expected value of dependent variable is a straight-line function of each independent variable, holding the others fixed.\n\n    (b) The slope of that line does not depend on the values of the other variables.\n\n    (c)  The effects of different independent variables on the expected value of the dependent variable are additive.\n\n(ii) statistical independence of the errors (in particular, no correlation between consecutive errors in the case of time series data)\n\n(iii) homoscedasticity (constant variance) of the errors\n\n    (a) versus time (in the case of time series data)\n\n    (b) versus the predictions\n\n    (c) versus any independent variable\n\n(iv) normality of the error distribution.\"","5a6c656a":"lmplot returns the FacetGrid object with the plot on it for further tweaking.FacetGrid class helps in visualizing distribution of one variable as well as the relationship between multiple variables separately within subsets of the dataset using multiple panels.\n\nA FacetGrid can be drawn with up to three dimensions \u2212 row, col, and hue. The first two have obvious correspondence with the resulting array of axes; thinking of the hue variable as a third dimension along a depth axis, where different levels are plotted with different colors.\n\nFacetGrid object takes a dataframe as input and the names of the variables that will form the row, column, or hue dimensions of the grid.","2759578e":"# Importing libraries","f9325aa0":"Hence there are no null values.","739867e7":"The column \"Year\" is meaningless unless it is in terms of the number of years after which the selling price is being estimated. Hence:","4276952a":"    The distribution for error in case of normalized Ridge regression model is right(positively) skewed for the data used here."}}