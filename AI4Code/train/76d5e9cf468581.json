{"cell_type":{"45595887":"code","e5e55af2":"code","4567bfdb":"code","46df71a3":"code","4f27a68d":"code","850dc8b5":"code","8d8d5ae3":"code","155ee723":"code","313e5c09":"code","959f866f":"code","ebdfa87c":"code","798ab545":"code","3f2f4ced":"code","c5bfae26":"code","236e27c2":"code","8a86fc04":"code","e9e7f2a0":"code","e91b3a84":"code","01582676":"code","c68574e9":"code","e0a6198b":"code","684430dd":"code","7dc76896":"code","474bd842":"code","816dd623":"code","5412b7c8":"code","229bdc19":"code","9e5f8936":"code","4a789999":"code","cf3d8c73":"code","97fbc679":"code","9550c5d0":"code","0919ccab":"code","9405f0d5":"code","78fef4fd":"code","adbf9768":"code","b8f48f8b":"markdown","0c919666":"markdown","427078bb":"markdown","ca97ee07":"markdown","71ed0b65":"markdown","61e924cf":"markdown","e3e96295":"markdown","36d1aee6":"markdown","96f0359d":"markdown","54ee61bd":"markdown","70e2a1e8":"markdown"},"source":{"45595887":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\n%matplotlib inline\n\nimport time\n\n\n# due to Kaggle memory limitations and the enormous dataset size, a sample from the whole\n# trainset will be used for ML modeling\ntrain_sample_fraction = 0.2\n\n\n# another global variable that must be defined is the NA values rate \/ theshold to ommit columns with\n# NA values that pass this rate\nna_rate_threshold = 0.9\n\n# theshold to remove columns with unbalanced features to their values \nunbalanced_feature_rate_threshold = 0.9\n\n# Any results you write to the current directory are saved as output.","e5e55af2":"# I am grateful for the help of author of this kernel for the main idea to load the dataset and save memory space!!\n# https:\/\/www.kaggle.com\/theoviel\/load-the-totality-of-the-data\n\ndtypes = {\n        'MachineIdentifier':                                    'category',\n        'ProductName':                                          'category',\n        'EngineVersion':                                        'category',\n        'AppVersion':                                           'category',\n        'AvSigVersion':                                         'category',\n        'IsBeta':                                               'int8',\n        'RtpStateBitfield':                                     'float16',\n        'IsSxsPassiveMode':                                     'int8',\n        'DefaultBrowsersIdentifier':                            'float16',\n        'AVProductStatesIdentifier':                            'float32',\n        'AVProductsInstalled':                                  'float16',\n        'AVProductsEnabled':                                    'float16',\n        'HasTpm':                                               'int8',\n        'CountryIdentifier':                                    'int16',\n        'CityIdentifier':                                       'float32',\n        'OrganizationIdentifier':                               'float16',\n        'GeoNameIdentifier':                                    'float16',\n        'LocaleEnglishNameIdentifier':                          'int8',\n        'Platform':                                             'category',\n        'Processor':                                            'category',\n        'OsVer':                                                'category',\n        'OsBuild':                                              'int16',\n        'OsSuite':                                              'int16',\n        'OsPlatformSubRelease':                                 'category',\n        'OsBuildLab':                                           'category',\n        'SkuEdition':                                           'category',\n        'IsProtected':                                          'float16',\n        'AutoSampleOptIn':                                      'int8',\n        'PuaMode':                                              'category',\n        'SMode':                                                'float16',\n        'IeVerIdentifier':                                      'float16',\n        'SmartScreen':                                          'category',\n        'Firewall':                                             'float16',\n        'UacLuaenable':                                         'float32',\n        'Census_MDC2FormFactor':                                'category',\n        'Census_DeviceFamily':                                  'category',\n        'Census_OEMNameIdentifier':                             'float16',\n        'Census_OEMModelIdentifier':                            'float32',\n        'Census_ProcessorCoreCount':                            'float16',\n        'Census_ProcessorManufacturerIdentifier':               'float16',\n        'Census_ProcessorModelIdentifier':                      'float16',\n        'Census_ProcessorClass':                                'category',\n        'Census_PrimaryDiskTotalCapacity':                      'float32',\n        'Census_PrimaryDiskTypeName':                           'category',\n        'Census_SystemVolumeTotalCapacity':                     'float32',\n        'Census_HasOpticalDiskDrive':                           'int8',\n        'Census_TotalPhysicalRAM':                              'float32',\n        'Census_ChassisTypeName':                               'category',\n        'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float16',\n        'Census_InternalPrimaryDisplayResolutionHorizontal':    'float16',\n        'Census_InternalPrimaryDisplayResolutionVertical':      'float16',\n        'Census_PowerPlatformRoleName':                         'category',\n        'Census_InternalBatteryType':                           'category',\n        'Census_InternalBatteryNumberOfCharges':                'float32',\n        'Census_OSVersion':                                     'category',\n        'Census_OSArchitecture':                                'category',\n        'Census_OSBranch':                                      'category',\n        'Census_OSBuildNumber':                                 'int16',\n        'Census_OSBuildRevision':                               'int32',\n        'Census_OSEdition':                                     'category',\n        'Census_OSSkuName':                                     'category',\n        'Census_OSInstallTypeName':                             'category',\n        'Census_OSInstallLanguageIdentifier':                   'float16',\n        'Census_OSUILocaleIdentifier':                          'int16',\n        'Census_OSWUAutoUpdateOptionsName':                     'category',\n        'Census_IsPortableOperatingSystem':                     'int8',\n        'Census_GenuineStateName':                              'category',\n        'Census_ActivationChannel':                             'category',\n        'Census_IsFlightingInternal':                           'float16',\n        'Census_IsFlightsDisabled':                             'float16',\n        'Census_FlightRing':                                    'category',\n        'Census_ThresholdOptIn':                                'float16',\n        'Census_FirmwareManufacturerIdentifier':                'float16',\n        'Census_FirmwareVersionIdentifier':                     'float32',\n        'Census_IsSecureBootEnabled':                           'int8',\n        'Census_IsWIMBootEnabled':                              'float16',\n        'Census_IsVirtualDevice':                               'float16',\n        'Census_IsTouchEnabled':                                'int8',\n        'Census_IsPenCapable':                                  'int8',\n        'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',\n        'Wdft_IsGamer':                                         'float16',\n        'Wdft_RegionIdentifier':                                'float16',\n        'HasDetections':                                        'int8'\n        }\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage(deep=True).sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","4567bfdb":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype=dtypes)","46df71a3":"good_cols = list(train.columns)\n\nfor col in train.columns:\n    \n    # remove columns with high NA rate\n    na_rate = train[col].isnull().sum() \/ train.shape[0]\n    \n    # remove columns with high Unbalanced values rate\n    unbalanced_rate = train[col].value_counts(normalize=True, dropna=False).values[0]\n    \n    if na_rate > na_rate_threshold:\n        good_cols.remove(col)\n    elif unbalanced_rate > unbalanced_feature_rate_threshold:\n        good_cols.remove(col)","4f27a68d":"good_cols","850dc8b5":"train = train[good_cols]","8d8d5ae3":"import gc\n\ngc.collect()","155ee723":"categorical_columns = list(train.loc[:, train.dtypes ==\"category\"].columns)\nnumerical_and_binary_columns = list(train.loc[:, train.dtypes !=\"category\"].columns)\nnumerical_columns = numerical_and_binary_columns\n\ncategorical_columns.remove(\"MachineIdentifier\")\n\nbinary_columns = []\nfor col in (numerical_and_binary_columns):\n    if train[col].nunique() == 2:\n        binary_columns.append(col)\n        numerical_columns.remove(col)","313e5c09":"train_sample = train.sample(frac=train_sample_fraction, random_state=42)\ndel train\ngc.collect()","959f866f":"train_sample.shape","ebdfa87c":"test_dtypes = {k: v for k, v in dtypes.items() if k in good_cols}\n\n# get all columns except\ntest = pd.read_csv('..\/input\/test.csv', dtype=test_dtypes, usecols=good_cols[:-1])\n\n#test = reduce_mem_usage(test)","798ab545":"test.head()","3f2f4ced":"test.shape","c5bfae26":"train_sample = train_sample.drop(['MachineIdentifier'], axis=1)\ntest = test.drop(['MachineIdentifier'], axis=1)","236e27c2":"train_sample = train_sample.reset_index(drop=True)","8a86fc04":"modes = train_sample.mode()\n\nfor col in train_sample.columns:\n    train_sample[col] = np.where(train_sample[col].isnull(), modes[col], train_sample[col])\n\ndel modes","e9e7f2a0":"modes_test = test.mode()\n\nfor col in test.columns:\n    test[col] = np.where(test[col].isnull(), modes_test[col], test[col])\n\n#train_sample.shape\ndel modes_test","e91b3a84":"train_shape = train_sample.shape\ntest_shape = test.shape\n\ntrain_and_test = pd.concat([train_sample,test], axis=\"rows\", sort=False)\n\ndel train_sample\ndel test\ngc.collect()","01582676":"train_and_test.head()","c68574e9":"train_and_test.tail()","e0a6198b":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef MultiLabelEncoder(columnlist,dataframe):\n    for i in columnlist:\n        #print(i)\n        labelencoder_X=LabelEncoder()\n        dataframe[i]=labelencoder_X.fit_transform(dataframe[i])\n\nMultiLabelEncoder(categorical_columns, train_and_test)","684430dd":"gc.collect()","7dc76896":"train_sample = train_and_test[0:train_shape[0]]\ntest = train_and_test[(train_shape[0]):(train_and_test.shape[0]+1)]","474bd842":"del train_and_test","816dd623":"test = test.drop([\"HasDetections\"], axis = 1)","5412b7c8":"y = train_sample['HasDetections']\nX = train_sample.drop(['HasDetections'], axis=1)","229bdc19":"del train_sample\ngc.collect()","9e5f8936":"# main idea:\n# https:\/\/www.kaggle.com\/infinitewing\/k-fold-cv-xgboost-example-0-28?scriptVersionId=1553202\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nimport time\nimport random\n\nK = 5\nindex = 0\npredictions_proba_test_list = np.zeros(len(test))\nfold_auc_list = []\nfold_accuracy_list = []\n\nkf = KFold(n_splits = K, random_state = 42, shuffle = True)\n\nfor train_index, test_index in kf.split(X):\n    \n    print(\"Fold:\", index)\n    index = index + 1\n    \n    train_X, valid_X = X.iloc[train_index, :], X.iloc[test_index, :]\n    train_y, valid_y = y[train_index], y[test_index]\n    \n    new_seed =  random.randint(1, 2000)\n    \n    clf_xgb = xgb.XGBClassifier(learning_rate=0.03, \n                            n_estimators=1300, \n                            max_depth=8,\n                            min_child_weight=4,\n                            gamma=0,\n                            subsample=0.8,\n                            colsample_bytree=0.7,\n                            objective= 'binary:logistic',\n                            nthread=-1,\n                            scale_pos_weight=1,\n                            reg_alpha = 0.1,\n                            reg_lambda = 1,\n                            seed=new_seed)\n\n    clf_xgb.fit(train_X, train_y, eval_set=[(train_X, train_y), (valid_X, valid_y)], \n            early_stopping_rounds=100, eval_metric='auc', verbose=100)\n    \n    temp_predictions_proba_test_list = []\n\n    # read test set in chunks\n    chunck = 400000\n    test_times = test.shape[0] \/\/ chunck\n    test_rest = test.shape[0] % chunck\n\n    for i in  np.arange(0,(chunck * (test_times+1)), chunck):\n        \n        # create predictions in chunks due ot memory limitations\n        predictions_proba_test = list(clf_xgb.predict_proba(test[i:(i+chunck)])[:,1])\n        temp_predictions_proba_test_list.append(predictions_proba_test)\n        #print(\"times:\", i)\n\n\n    # flatten the list of lists\n    temp_predictions_proba_test_list = [y for x in temp_predictions_proba_test_list for y in x]\n    \n    \n    #print(np.shape(predictions_proba_test_list))\n    predictions_proba_test_list = [sum(x) for x in zip(predictions_proba_test_list, temp_predictions_proba_test_list)]\n    #print(test.shape)\n    #print(np.shape(predictions_proba_test_list))\n\n    \n    predictions = clf_xgb.predict(valid_X, ntree_limit=clf_xgb.n_estimators)\n\n    print()\n    print(classification_report(valid_y, predictions))\n\n    print()\n    print(\"accuracy_score\", accuracy_score(valid_y, predictions))\n    \n    predictions_probas = clf_xgb.predict_proba(valid_X)[:,1]\n    print(\"auc score\", roc_auc_score(valid_y, predictions_probas))\n    print()\n    \n    fold_accuracy_list.append(accuracy_score(valid_y, predictions))\n    fold_auc_list.append(roc_auc_score(valid_y, predictions_probas))\n\nprint()\nprint(\"Mean auc:\", np.mean(fold_auc_list))\nprint(\"Std auc:\", np.std(fold_auc_list))\n\nprint(\"Mean accuracy:\", np.mean(fold_accuracy_list))\nprint(\"Std accuracy:\", np.std(fold_accuracy_list))\n\ngc.collect()","4a789999":"predictions_proba_test_list = [x \/ kf.n_splits for x in predictions_proba_test_list]","cf3d8c73":"from sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(valid_y, predictions, cmap=\"BrBG\")","97fbc679":"sns.set(rc={'figure.figsize':(8,8)})\npredictions_probas = clf_xgb.predict_proba(valid_X)\nskplt.metrics.plot_roc(valid_y, predictions_probas)","9550c5d0":"sns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(valid_y, predictions_probas)","0919ccab":"sns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(valid_y, predictions_probas)","9405f0d5":"# I may implement tuning in the future but I am afraid of variance - bias tradeoff\n'''\n#idea and a big thank you to https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\nfrom sklearn.model_selection import GridSearchCV   #Perforing grid search\n\ngc.collect()\n\nparam_test1 = {\n    # based on previous personal kernels both parameters show better result having high numbers \n 'max_depth':[3, 5, 7, 9, 11],\n 'min_child_weight':[1, 3, 5, 7, 9]\n}\ngsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.05, n_estimators=70, gamma=0, subsample=0.8, colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1, reg_alpha = 0, \n                                                reg_lambda =1, seed=42), \n                        param_grid = param_test1, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)\n\ngsearch1.fit(xtrain, ytrain)\ngsearch1.best_params_, gsearch1.best_score_\n'''","78fef4fd":"del X\ndel y\ndel train_X\ndel train_y\ndel valid_X\ndel valid_y\ndel predictions\ndel predictions_probas\ndel temp_predictions_proba_test_list\ndel clf_xgb\ngc.collect()","adbf9768":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['HasDetections'] = predictions_proba_test_list\nsubmission.to_csv('xgboost.csv', index=False)","b8f48f8b":"## Machine Learning Modeling and Tuning","0c919666":"### Remove the HasDetections columns from test set, it has been added during dataframe concatenation.","427078bb":"### Encode the Categorical features before machine learning modeling","ca97ee07":"### Back to train and test set after Label Encoding","71ed0b65":"### Tuning\n#### The following part is commented, I have run tuning in previous versions of this kernel and figured out the optimal (so far) values of the xgboost parameters.\n\n### XGBoost Grid Search Part 1\n#### Tuning parameters:\n- 'max_depth'\n- 'min_child_weight'","61e924cf":"### Concatenate both train_sample and test sets before label encoding","e3e96295":"# Microsoft Malware Detection XGBoost Blending CV predictions\n\n![](https:\/\/winbuzzer.com\/wp-content\/uploads\/2016\/02\/Microsoft-Logo-1-1.jpg)\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRbuYMzpgoRh2tLUj_EVV0z7gtIKwfJfZ7G-DP5dscAvSqcgSR_OQ)\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/dc\/Cog-scripted-svg-green.svg\/537px-Cog-scripted-svg-green.svg.png)\n\n\nThe Goal For this Kernel is to blend \/ mix predictions from different folds when training the XGBoost classifier. The idea stems from different Kernels which I want to credit for such as:\n\n- https:\/\/www.kaggle.com\/artgor\/is-this-malware-eda-fe-and-lgb-updated\n\n","36d1aee6":"### Filling NA values with the statistical Mode","96f0359d":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRB40XiqXJooyGCgeV5-6DLkKEdDgFbvefkDgLMFG7_f4Sl0VQt)\n\n![](http:\/\/www.pevco.com\/wp-content\/uploads\/2015\/05\/gears-clear.png)","54ee61bd":"### XGBoost Baseline model and blending Folds","70e2a1e8":"### Prepare for submission"}}