{"cell_type":{"b68e0abf":"code","86cc6514":"code","dbd92f0b":"code","cb046c69":"code","2be7d6e6":"code","27da7c2b":"code","9aeab075":"code","26df4c69":"code","e6610989":"code","33451d9c":"code","679cfaf5":"code","0fcc0631":"code","7f747e9a":"code","4f049d73":"code","f00fad6b":"code","b4be6f37":"code","db02fb9e":"code","65ca1b05":"code","768c6242":"code","77905963":"code","e4e51ad1":"code","804a24fe":"code","7cb91872":"code","b07bc359":"code","6925e306":"code","23dbceb9":"code","e9bfdc9b":"code","f19d9f2c":"code","1443be65":"code","d0880217":"code","4537243b":"code","c7de3c00":"code","3c68191d":"code","aac70bcd":"code","3cd0e728":"code","978e9ca8":"code","0882e89d":"code","9ebcd338":"code","a0ca6f7a":"code","4191b8f5":"code","c63518e9":"code","0cbb84b1":"code","7d75e227":"code","a0784dbb":"code","cbb1c4e2":"markdown","5549fe57":"markdown","fe3bbf93":"markdown","895438fb":"markdown","aa9ce394":"markdown","b111e1e5":"markdown","b58a4f2e":"markdown","fe87f425":"markdown","4b70dc3d":"markdown","a0654da1":"markdown","54049b26":"markdown","462c7894":"markdown","f2edc1e6":"markdown","90b42eaf":"markdown","6986833b":"markdown","09af78a5":"markdown","222b6987":"markdown","c1f00981":"markdown","d99af6d9":"markdown","65b62f3c":"markdown"},"source":{"b68e0abf":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score,precision_score,recall_score","86cc6514":"df = pd.read_csv(\"\/kaggle\/input\/facebook-recruiting-iii-keyword-extraction\/Train.zip\")\ndf.head()","dbd92f0b":"print(\"Dataframe shape : \", df.shape)","cb046c69":"df = df.iloc[:10000, :]\nprint(\"Shape of Dataframe after subsetting : \", df.shape)","2be7d6e6":"duplicate_pairs = df.sort_values('Title', ascending=False).duplicated('Title')\nprint(\"Total number of duplicate questions : \", duplicate_pairs.sum())\ndf = df[~duplicate_pairs]\nprint(\"Dataframe shape after duplicate removal : \", df.shape)","27da7c2b":"df[\"tag_count\"] = df[\"Tags\"].apply(lambda x : len(x.split()))","9aeab075":"df[\"tag_count\"].value_counts()","26df4c69":"print( \"Maximum number of tags in a question: \", df[\"tag_count\"].max())\nprint( \"Minimum number of tags in a question: \", df[\"tag_count\"].min())\nprint( \"Average number of tags in a question: \", df[\"tag_count\"].mean())","e6610989":"sns.countplot(df[\"tag_count\"])\nplt.title(\"Number of tags in questions \")\nplt.xlabel(\"Number of Tags\")\nplt.ylabel(\"Frequency\")","33451d9c":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split())\ntag_bow = vectorizer.fit_transform(df['Tags'])","679cfaf5":"print(\"Number of questions :\", tag_bow.shape[0])\nprint(\"Number of unique tags :\", tag_bow.shape[1])","0fcc0631":"tags = vectorizer.get_feature_names()\nprint(\"Few tags :\", tags[:10])","7f747e9a":"freq = tag_bow.sum(axis=0).A1\ntag_to_count_map = dict(zip(tags, freq))","4f049d73":"list = []\nfor key, value in tag_to_count_map.items():\n  list.append([key, value]) ","f00fad6b":"tag_df = pd.DataFrame(list, columns=['Tags', 'Counts'])\ntag_df.head()","b4be6f37":"tag_df_sorted = tag_df.sort_values(['Counts'], ascending=False)\nplt.plot(tag_df_sorted['Counts'].values)\nplt.grid()\nplt.title(\"Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","db02fb9e":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.grid()\nplt.title(\"Top 100 tags : Distribution of frequency of tags based on appeareance\")\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")","65ca1b05":"plt.plot(tag_df_sorted['Counts'][0:100].values)\nplt.scatter(x=np.arange(0,100,5), y=tag_df_sorted['Counts'][0:100:5], c='g', label=\"quantiles with 0.05 intervals\")\nplt.scatter(x=np.arange(0,100,25), y=tag_df_sorted['Counts'][0:100:25], c='r', label = \"quantiles with 0.25 intervals\")\nfor x,y in zip(np.arange(0,100,25), tag_df_sorted['Counts'][0:100:25]):\n    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.01, y+30))\n\nplt.title('first 100 tags: Distribution of frequency of tags based on appeareance')\nplt.grid()\nplt.xlabel(\"Tag numbers for most frequent tags\")\nplt.ylabel(\"Frequency\")\nplt.legend()","768c6242":"print(\"{} tags are used more than 25 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>25].shape[0]))\nprint(\"{} tags are used more than 50 times\".format(tag_df_sorted[tag_df_sorted[\"Counts\"]>50].shape[0]))","77905963":"tag_to_count_map\ntupl = dict(tag_to_count_map.items())\nword_cloud = WordCloud(width=1600,height=800,).generate_from_frequencies(tupl)\nplt.figure(figsize = (12,8))\nplt.imshow(word_cloud)\nplt.axis('off')\nplt.tight_layout(pad=0)","e4e51ad1":"i=np.arange(20)\ntag_df_sorted.head(20).plot(kind='bar')\nplt.title('Frequency of top 20 tags')\nplt.xticks(i, tag_df_sorted['Tags'])\nplt.xlabel('Tags')\nplt.ylabel('Counts')\nplt.show()","804a24fe":"stop_words = set(stopwords.words('english'))\nstemmer = SnowballStemmer(\"english\")","7cb91872":"qus_list=[]\nqus_with_code = 0\nlen_before_preprocessing = 0 \nlen_after_preprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        qus_with_code+=1\n    len_before_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)<\/code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(body)\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    qus_list.append(question)\n    len_after_preprocessing += len(question)\ndf[\"question\"] = qus_list\navg_len_before_preprocessing=(len_before_preprocessing*1.0)\/df.shape[0]\navg_len_after_preprocessing=(len_after_preprocessing*1.0)\/df.shape[0]\nprint( \"Avg. length of questions(Title+Body) before preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) after preprocessing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (qus_with_code*100.0)\/df.shape[0])","b07bc359":"preprocessed_df = df[[\"question\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","6925e306":"vectorizer = CountVectorizer(tokenizer = lambda x: x.split(), binary='true')\ny_multilabel = vectorizer.fit_transform(preprocessed_df['Tags'])","23dbceb9":"def tags_to_consider(n):\n    tag_i_sum = y_multilabel.sum(axis=0).tolist()[0]\n    sorted_tags_i = sorted(range(len(tag_i_sum)), key=lambda i: tag_i_sum[i], reverse=True)\n    yn_multilabel=y_multilabel[:,sorted_tags_i[:n]]\n    return yn_multilabel\n\ndef questions_covered_fn(numb):\n    yn_multilabel = tags_to_consider(numb)\n    x= yn_multilabel.sum(axis=1)\n    return (np.count_nonzero(x==0))","e9bfdc9b":"questions_covered = []\ntotal_tags=y_multilabel.shape[1]\ntotal_qus=preprocessed_df.shape[0]\nfor i in range(100, total_tags, 100):\n    questions_covered.append(np.round(((total_qus-questions_covered_fn(i))\/total_qus)*100,3))","f19d9f2c":"plt.plot(np.arange(100,total_tags, 100),questions_covered)\nplt.xlabel(\"Number of tags\")\nplt.ylabel(\"Number of questions covered partially\")\nplt.grid()\nplt.show()\nprint(questions_covered[9],\"% of questions covered by 1000 tags\")\nprint(\"Number of questions that are not covered by 100 tags : \", questions_covered_fn(1000),\"out of \", total_qus)","1443be65":"yx_multilabel = tags_to_consider(1000)\nprint(\"Number of tags in the subset :\", y_multilabel.shape[1])\nprint(\"Number of tags considered :\", yx_multilabel.shape[1],\"(\",(yx_multilabel.shape[1]\/y_multilabel.shape[1])*100,\"%)\")","d0880217":"X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.2,random_state = 42)\nprint(\"Number of data points in training data :\", X_train.shape[0])\nprint(\"Number of data points in test data :\", X_test.shape[0])","4537243b":"vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\nX_train_multilabel = vectorizer.fit_transform(X_train['question'])\nX_test_multilabel = vectorizer.transform(X_test['question'])","c7de3c00":"print(\"Training data shape X : \",X_train_multilabel.shape, \"Y :\",y_train.shape)\nprint(\"Test data shape X : \",X_test_multilabel.shape,\"Y:\",y_test.shape)","3c68191d":"clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2'))\nclf.fit(X_train_multilabel, y_train)\ny_pred = clf.predict(X_test_multilabel)","aac70bcd":"print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\nprint(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\nprint(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\nprint(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))\n#print(\"Precision recall report :\\n\",metrics.classification_report(y_test, y_pred))","3cd0e728":"qus_list=[]\nqus_with_code = 0\nlen_before_preprocessing = 0 \nlen_after_preprocessing = 0 \nfor index,row in df.iterrows():\n    title, body, tags = row[\"Title\"], row[\"Body\"], row[\"Tags\"]\n    if '<code>' in body:\n        qus_with_code+=1\n    len_before_preprocessing+=len(title) + len(body)\n    body=re.sub('<code>(.*?)<\/code>', '', body, flags=re.MULTILINE|re.DOTALL)\n    body = re.sub('<.*?>', ' ', str(body.encode('utf-8')))\n    title=title.encode('utf-8')\n    question=str(title)+\" \"+str(title)+\" \"+str(title)+\" \"+ body\n    question=re.sub(r'[^A-Za-z]+',' ',question)\n    words=word_tokenize(str(question.lower()))\n    question=' '.join(str(stemmer.stem(j)) for j in words if j not in stop_words and (len(j)!=1 or j=='c'))\n    qus_list.append(question)\n    len_after_preprocessing += len(question)\ndf[\"question_with_more_wt_title\"] = qus_list\navg_len_before_preprocessing=(len_before_preprocessing*1.0)\/df.shape[0]\navg_len_after_preprocessing=(len_after_preprocessing*1.0)\/df.shape[0]\nprint( \"Avg. length of questions(Title+Body) before preprocessing: \", avg_len_before_preprocessing)\nprint( \"Avg. length of questions(Title+Body) after preprocessing: \", avg_len_after_preprocessing)\nprint (\"% of questions containing code: \", (qus_with_code*100.0)\/df.shape[0])\n","978e9ca8":"preprocessed_df = df[[\"question_with_more_wt_title\",\"Tags\"]]\nprint(\"Shape of preprocessed data :\", preprocessed_df.shape)","0882e89d":"X_train, X_test, y_train, y_test = train_test_split(preprocessed_df, yx_multilabel, test_size = 0.2,random_state = 42)\nprint(\"Number of data points in training data :\", X_train.shape[0])\nprint(\"Number of data points in test data :\", X_test.shape[0])","9ebcd338":"vectorizer = TfidfVectorizer(min_df=0.00009, max_features=200000, tokenizer = lambda x: x.split(), ngram_range=(1,3))\nX_train_multilabel = vectorizer.fit_transform(X_train['question_with_more_wt_title'])\nX_test_multilabel = vectorizer.transform(X_test['question_with_more_wt_title'])","a0ca6f7a":"print(\"Training data shape X : \",X_train_multilabel.shape, \"Y :\",y_train.shape)\nprint(\"Test data shape X : \",X_test_multilabel.shape,\"Y:\",y_test.shape)","4191b8f5":"clf = OneVsRestClassifier(SGDClassifier(loss='log', alpha=0.00001, penalty='l2'))\nclf.fit(X_train_multilabel, y_train)\ny_pred = clf.predict(X_test_multilabel)","c63518e9":"print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred))\nprint(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred, average = 'macro'))\nprint(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred, average = 'micro'))\nprint(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred))","0cbb84b1":"#using direct implementation of Logistic Regression\nclf2 = OneVsRestClassifier(LogisticRegression(penalty='l1'))\nclf2.fit(X_train_multilabel, y_train)\ny_pred2 = clf2.predict(X_test_multilabel)","7d75e227":"print(\"Accuracy :\",metrics.accuracy_score(y_test,y_pred2))\nprint(\"Macro f1 score :\",metrics.f1_score(y_test, y_pred2, average = 'macro'))\nprint(\"Micro f1 scoore :\",metrics.f1_score(y_test, y_pred2, average = 'micro'))\nprint(\"Hamming loss :\",metrics.hamming_loss(y_test,y_pred2))","a0784dbb":"clf2.predict()","cbb1c4e2":"### 1. Importing necessary libraries","5549fe57":"**Observations:**\n\n\"c#\", \"java\", \"php\", \"android\", \"javascript\", \"jquery\", \"C++\" are some of the most frequent tags.","fe3bbf93":"### 2. Loading data","895438fb":"**Observations:**\n\n1. Maximum number of tags in a question: 5\n2. Minimum number of tags in a question: 1\n3. Average number of tags per question: 2.92\n4. Most of the questions have either 2 or 3 tags","aa9ce394":"#### 4.3 Frequency of each tag","b111e1e5":"### 6. Text preprocessing","b58a4f2e":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data**](#2.-Loading-data) \n3. [**Data preprocessing**](#3.-Data-preprocessing)  \n    3.1 [**Checking for duplicates**](#3.1-Checking-for-duplicates)\n4. [**Basic Data Analysis on Tags**](#4.-Basic-Data-Analysis-on-Tags)  \n    4.1 [**Frequency of tag_count**](#4.1-Frequency-of-tag_count)  \n    4.2 [**Total number of unique tags**](#4.2-Total-number-of-unique-tags)  \n    4.3 [**Frequency of each tag**](#4.3-Frequency-of-each-tag)  \n5. [**Word map for most frequent Tags**](#5.-Word-map-for-most-frequent-Tags)  \n6. [**Text preprocessing**](#6.-Text-preprocessing)   \n7. [**Machine learning models**](#7.-Machine-learning-models)  \n    7.1 [**Multilabel problem - Handling tags**](#7.1-Multilabel-problem---Handling-tags)  \n    7.2 [**Splitting into train and test set with 80:20 ratio**](#7.2-Splitting-into-train-and-test-set-with-80:20-ratio)  \n    7.3 [**Featurization of Training Data**](#7.3-Featurization-of-Training-Data)  \n    7.4 [**Fitting Logistic Regression with OneVsRest Classifier**](#7.4-Fitting-Logistic-Regression-with-OneVsRest-Classifier)  \n    7.5 [**Modelling by assigning more weightage to Title**](#7.5-Modelling-by-assigning-more-weightage-to-Title)  ","fe87f425":"### 3. Data preprocessing\n\n#### 3.1 Checking for duplicates","4b70dc3d":"Since the number of records in the data is very large(6034195) so let's consider a small subset of data for faster computing.","a0654da1":"### 5. Word map for most frequent Tags","54049b26":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","462c7894":"###  7.3 Featurization of Training Data","f2edc1e6":"## 7. Machine learning models\n\n### 7.1 Multilabel problem - Handling tags","90b42eaf":"* ### 7.5 Modelling by assigning more weightage to Title","6986833b":"### 7.4 Fitting Logistic Regression with OneVsRest Classifier","09af78a5":"**Observations:**\n\n1. 144 tags are used more than 25 times.\n2. 59 tags are used more than 50 times.\n3. **C#** is most frequently used tag 778 times.\n4. Since some tags occur much more frequenctly than others, Micro-averaged F1-score is the appropriate metric for this probelm.","222b6987":"### Bar plot of top 20 tags","c1f00981":"### 4. Basic Data Analysis on Tags \n\n#### 4.1. Frequency of tag_count","d99af6d9":"### 7.2 Splitting into train and test set with 80:20 ratio","65b62f3c":"#### 4.2 Total number of unique tags"}}