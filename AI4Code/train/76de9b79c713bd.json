{"cell_type":{"f9296434":"code","06eb3d13":"code","283b1eed":"code","3c110a74":"code","b75701a0":"code","fec701eb":"code","19466b01":"code","e31ac103":"code","5f2a1869":"code","9d5b7b17":"code","e38edcdb":"code","5794a4a9":"code","19dce720":"code","fce188e1":"code","ecdd47a8":"code","d8e57ee6":"code","719e3793":"code","a6746bd9":"code","91033df3":"code","15855317":"code","9e900ceb":"code","7eff270d":"code","6b518f43":"code","300e6ccb":"code","d2839ba0":"code","49a33868":"code","3797f6b6":"code","edf4dc9a":"markdown","f9f6c4da":"markdown","ece62988":"markdown","ed0afd76":"markdown","5faee977":"markdown","68286f75":"markdown","e624eda7":"markdown","a725a10b":"markdown","320bcc39":"markdown"},"source":{"f9296434":"import os\nimport shutil\nimport random\nfrom tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport PIL\nimport seaborn as sns\nimport matplotlib.pyplot as plt","06eb3d13":"DATASET = \"..\/input\/EUROSAT\"\n\nLABELS = os.listdir(DATASET)\nprint(LABELS)","283b1eed":"# plot class distributions of whole dataset\ncounts = {}\n\nfor l in LABELS:\n    counts[l] = len(os.listdir(os.path.join(DATASET, l)))\n\n    \nplt.figure(figsize=(10, 5))\n\nplt.bar(range(len(counts)), list(counts.values()), align='center')\nplt.xticks(range(len(counts)), list(counts.keys()), fontsize=12, rotation=80)\nplt.xlabel('class label', fontsize=13)\nplt.ylabel('class size', fontsize=13)\nplt.title('EUROSAT Class Distribution', fontsize=15);","3c110a74":"img_paths = [os.path.join(DATASET, l, l+'_1000.jpg') for l in LABELS]\n\nimg_paths = img_paths + [os.path.join(DATASET, l, l+'_2000.jpg') for l in LABELS]\n\ndef plot_sat_imgs(paths):\n    plt.figure(figsize=(15, 8))\n    for i in range(20):\n        plt.subplot(4, 5, i+1, xticks=[], yticks=[])\n        img = PIL.Image.open(paths[i], 'r')\n        plt.imshow(np.asarray(img))\n        plt.title(paths[i].split('\/')[-2])\n\nplot_sat_imgs(img_paths)","b75701a0":"from skimage import io\n\ndef plot_img_histogram(img_path):\n    \n    image = io.imread(img_path)\n    plt.hist(image[:, :, 0].ravel(), bins = 256, color = 'red', alpha = 0.5)\n    plt.hist(image[:, :, 1].ravel(), bins = 256, color = 'Green', alpha = 0.5)\n    plt.hist(image[:, :, 2].ravel(), bins = 256, color = 'Blue', alpha = 0.5)\n    plt.xlabel('Intensity')\n    plt.ylabel('Count')\n    plt.title(img_path.split('\/')[-2])\n    plt.show()","fec701eb":"#for l in LABELS:\n#    path = os.path.join(DATASET, l, l+'_1000.jpg')\n#    plot_img_histogram(path)","19466b01":"import re\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom keras.preprocessing.image import ImageDataGenerator\n\nTRAIN_DIR = '..\/working\/training'\nTEST_DIR = '..\/working\/testing'\nBATCH_SIZE = 128\nNUM_CLASSES=len(LABELS)\nINPUT_SHAPE = (64, 64, 3)\nCLASS_MODE = 'categorical'\n\n# create training and testing directories\nfor path in (TRAIN_DIR, TEST_DIR):\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n# create class label subdirectories in train and test\nfor l in LABELS:\n    \n    if not os.path.exists(os.path.join(TRAIN_DIR, l)):\n        os.mkdir(os.path.join(TRAIN_DIR, l))\n\n    if not os.path.exists(os.path.join(TEST_DIR, l)):\n        os.mkdir(os.path.join(TEST_DIR, l))","e31ac103":"# map each image path to their class label in 'data'\ndata = {}\n\nfor l in LABELS:\n    for img in os.listdir(DATASET+'\/'+l):\n        data.update({os.path.join(DATASET, l, img): l})\n\nX = pd.Series(list(data.keys()))\ny = pd.get_dummies(pd.Series(data.values()))\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=69)\n\n# split the list of image paths\nfor train_idx, test_idx in split.split(X, y):\n    \n    train_paths = X[train_idx]\n    test_paths = X[test_idx]\n\n    # define a new path for each image depending on training or testing\n    new_train_paths = [re.sub('\\.\\.\\\/input\\\/EUROSAT', '..\/working\/training', i) for i in train_paths]\n    new_test_paths = [re.sub('\\.\\.\\\/input\\\/EUROSAT', '..\/working\/testing', i) for i in test_paths]\n\n    train_path_map = list((zip(train_paths, new_train_paths)))\n    test_path_map = list((zip(test_paths, new_test_paths)))\n    \n    # move the files\n    print(\"moving training files..\")\n    for i in tqdm(train_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])\n    \n    print(\"moving testing files..\")\n    for i in tqdm(test_path_map):\n        if not os.path.exists(i[1]):\n            if not os.path.exists(re.sub('training', 'testing', i[1])):\n                shutil.copy(i[0], i[1])","5f2a1869":"# training generator - create one for validation subset\ntrain_gen = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=60,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    validation_split=0.3\n)\n\ntrain_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    subset='training',\n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n\nvalid_generator = train_gen.flow_from_directory(\n    directory=TRAIN_DIR,\n    target_size=(64, 64),\n    batch_size=BATCH_SIZE,\n    class_mode=CLASS_MODE,\n    subset='validation',    \n    color_mode='rgb',\n    shuffle=True,\n    seed=69\n)\n\n# test generator for evaluation purposes - no augmentations, just rescaling\ntest_gen = ImageDataGenerator(\n    rescale=1.\/255\n)\n\ntest_generator = test_gen.flow_from_directory(\n    directory=TEST_DIR,\n    target_size=(64, 64),\n    batch_size=1,\n    class_mode=None,\n    color_mode='rgb',\n    shuffle=False,\n    seed=69\n)","9d5b7b17":"print(train_generator.class_indices)","e38edcdb":"np.save('class_indices', train_generator.class_indices)","5794a4a9":"import tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adagrad\n\n\nfrom keras.applications.vgg16 import VGG16\n\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, fbeta_score","19dce720":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")    \n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)\n    \ntf.config.set_soft_device_placement(True)","fce188e1":"def compile_model(input_shape, n_classes, optimizer, fine_tune=None):\n    \n    conv_base = VGG16(include_top=False,\n                     weights='imagenet', \n                     input_shape=input_shape,\n                     pooling='avg')\n    \n    top_model = conv_base.output\n    top_model = Dense(2048, activation='relu')(top_model)\n    output_layer = Dense(n_classes, activation='softmax')(top_model)\n    \n    model = Model(inputs=conv_base.input, outputs=output_layer)\n        \n    if type(fine_tune) == int:\n        for layer in conv_base.layers[fine_tune:]:\n            layer.trainable = True\n    else:\n        for layer in conv_base.layers:\n            layer.trainable = False\n\n    model.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                 metrics=['categorical_accuracy'])\n    \n    return model\n\ndef plot_history(history):\n       \n    acc = history.history['categorical_accuracy']\n    val_acc = history.history['val_categorical_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(acc)\n    plt.plot(val_acc)\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(loss)\n    plt.plot(val_loss)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    \n    plt.show();\n\ndef display_results(y_true, y_preds, class_labels):\n    \n    results = pd.DataFrame(precision_recall_fscore_support(y_true, y_preds),\n                          columns=class_labels).T\n    results.rename(columns={0: 'Precision',\n                           1: 'Recall',\n                           2: 'F-Score',\n                           3: 'Support'}, inplace=True)\n    \n    conf_mat = pd.DataFrame(confusion_matrix(y_true, y_preds), \n                            columns=class_labels,\n                            index=class_labels)    \n    f2 = fbeta_score(y_true, y_preds, beta=2, average='micro')\n    print(f\"Global F2 Score: {f2}\")    \n    return results, conf_mat\n\ndef plot_predictions(y_true, y_preds, test_generator, class_indices):\n\n    fig = plt.figure(figsize=(20, 10))\n    for i, idx in enumerate(np.random.choice(test_generator.samples, size=20, replace=False)):\n        ax = fig.add_subplot(4, 5, i + 1, xticks=[], yticks=[])\n        ax.imshow(np.squeeze(test_generator[idx]))\n        pred_idx = np.argmax(y_preds[idx])\n        true_idx = y_true[idx]\n                \n        plt.tight_layout()\n        ax.set_title(\"{}\\n({})\".format(class_indices[pred_idx], class_indices[true_idx]),\n                     color=(\"green\" if pred_idx == true_idx else \"red\"))    ","ecdd47a8":"optim = Adagrad()\n\nmodel = compile_model(INPUT_SHAPE, NUM_CLASSES, optim, fine_tune=None)\nmodel.summary()","d8e57ee6":"N_STEPS = train_generator.samples\/\/BATCH_SIZE\nN_VAL_STEPS = valid_generator.samples\/\/BATCH_SIZE\nN_EPOCHS = 100\n\n    # model callbacks\ncheckpoint = ModelCheckpoint(filepath='..\/working\/model.weights.best.hdf5',\n                        monitor='val_categorical_accuracy',\n                        save_best_only=True,\n                        verbose=1)\n\nearly_stop = EarlyStopping(monitor='val_categorical_accuracy',\n                           patience=10,\n                           restore_best_weights=True,\n                           mode='max')","719e3793":"history = model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=valid_generator,\n                             validation_steps=N_VAL_STEPS)","a6746bd9":"plot_history(history)","91033df3":"model.load_weights('..\/working\/model.weights.best.hdf5')\n\nclass_indices = train_generator.class_indices\nclass_indices = dict((v,k) for k,v in class_indices.items())\n\ntest_generator.reset()\n\npredictions = model.predict_generator(test_generator, steps=len(test_generator.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator.classes\n\nprf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())\nprf","15855317":"# re-train with fine-tuning\nmodel = compile_model(INPUT_SHAPE, NUM_CLASSES, optim, fine_tune=14)\n\ntrain_generator.reset()\nvalid_generator.reset()\n\nhistory = model.fit_generator(train_generator,\n                             steps_per_epoch=N_STEPS,\n                             epochs=N_EPOCHS,\n                             callbacks=[early_stop, checkpoint],\n                             validation_data=valid_generator,\n                             validation_steps=N_VAL_STEPS)","9e900ceb":"plot_history(history)","7eff270d":"model.load_weights('..\/working\/model.weights.best.hdf5')\n\ntest_generator.reset()\n\npredictions = model.predict_generator(test_generator, steps=len(test_generator.filenames))\npredicted_classes = np.argmax(np.rint(predictions), axis=1)\ntrue_classes = test_generator.classes\n","6b518f43":"prf, conf_mat = display_results(true_classes, predicted_classes, class_indices.values())","300e6ccb":"prf","d2839ba0":"conf_mat","49a33868":"plot_predictions(true_classes, predictions, test_generator, class_indices)","3797f6b6":"# Save the model and the weights\n\nmodel.save('..\/working\/vgg16_eurosat.h5')","edf4dc9a":"## i. Fine-tuning the Model\nHere I'm going to fine tune the trained model. This involves re-compiling the model - but this time I won't freeze all VGG16 convolutional layers. Here, I've decided that only the first 14 layers will be frozen from training. The last remaining layers will be trained on the dataset and hopefully, enhance the model's prediction performance on the test data.\nIn the previous section, the model achieved a Global F2-Score of ~0.79. Fine-tuning should hopefully beat that.","f9f6c4da":"In this project, I'll be exploring the EUROSAT dataset. The EUROSAT dataset is composed of images taken from the Sentinel-2 satellite. This dataset lists images of the earth's surface into 10 different land cover labels. For this project, I will build an image classification model for predicting a land cover label, given an image. ","ece62988":"# II. Preprocessing","ed0afd76":"We can see how the classes differ by inspecting the intensity of RGB values in each class using histograms","5faee977":"I'd like to evaluate the performance of the model later on after training, so I'll perform a stratified shuffle-split using Scikit-learn to maintain class proportions. 30% of the dataset will be held for evaluation purposes. I'll be loading my data into the Keras model using the ImageDataGenerator class. I'll need the images to be in their own respective land cover directories. \n\nAfter splitting the dataset, I'll create some image augmentations using the generator and also denote a subset of the training data to be used as validation data during training. ","68286f75":"The dataset is split into 10 classes of land cover. Each class varies in size, so I'll have to stratify later on when splitting the data into training, testing and validation sets. ","e624eda7":"# III. Training a Model\n\nTransfer Learning involves the loading of a pre-trained model and using its architecture for deriving new weights off of our own data. Using the VGG16 model, which has been trained using the Imagenet dataset, I can perform a 'freeze' on it's convolutional layers and train a model on my own dataset to achieve a high performance. I'll connect the VGG16 architecture with an output layer that is appropriate for the EuroSat dataset, and train from there. \n\nAfter an initial training, I'll re-compile the model with the newly-learned weights and fine-tune the model on the same training data as before. The minimum requirement I've set for myself is to achieve a Global F-Score of at least 0.80. \n\nAn F-Score is a weighted balance between a class' precision and recall during classification. An F-beta score allows you to prioritize precision or recall and indicates how well classification of a given class is performing. For this task, I've chosen an F-beta score that prioritizes Recall of information (an 'F2' score). ","a725a10b":"Looking at the preview of the different classes, we can see some similarities and stark differences between the classes. \n\nUrban environments such as Highway, Residential and Industrial images all contain structures and some roadways. \n\nAnnualCrops and PermanentCrops both feature agricultural land cover, with straight lines dilineating different crop fields. \n\nFinally, HerbaceaousVegetation, Pasture, and Forests feature natural land cover; Rivers also could be categorized as natural land cover as well, but may be easier to distinguish from the other natural classes.\n\nIf we consider the content of each image, we might be able to estimate which classes might be confused for each other. For example, an image of a river might be mistaken for a highway. Or an image of a highway junction, with surrounding buildings, could be mistaken for an Industrial site. We'll have to train a classifier powerful enough to differentiate these nuances. \n\nSentinel-2 satellite images could also be downloaded with 10+ additional bands. Near-Infrared Radiation bands, for example, is a band of data that is available for this dataset. NIR can be used to create an index, visualising the radiation that is present (or not present) in a picture. This dataset does not contain the NIR wavelength bands, so this option will not be explored. But it's worth pointing out that this classification task could be addressed in another way using NIR data. ","320bcc39":"# I. Data Exploration"}}