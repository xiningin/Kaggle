{"cell_type":{"2d759759":"code","f04f517d":"code","2c985206":"code","ca0b04da":"code","5f7afad0":"code","d5cfc83e":"code","b68234f0":"code","e4d89768":"code","3882ca15":"code","34f55082":"code","830b5643":"code","f88c43ca":"code","2e12250c":"code","0aa8dae8":"code","519aec1d":"code","f43182d9":"code","61999626":"code","e07cfdfb":"code","b6c03673":"code","1bf0509e":"code","29b99aeb":"code","87d61c2b":"code","161142ef":"code","fa9438cd":"code","7d4d63bf":"code","93272707":"code","e275b13d":"code","5147ef32":"code","9b8b1654":"code","171c7983":"code","2f3b8db5":"code","6d1d534c":"code","659d7b3c":"code","bd53b2b4":"code","f8b5e16a":"code","ad7746ad":"code","4aa9a7c2":"code","ec4064b7":"code","748eaedb":"code","d87c1e88":"code","6a9af503":"code","5248e97f":"code","fff13145":"code","e8b99f1a":"code","217c442e":"code","44697a16":"code","df65e6d4":"code","49d3e7d5":"code","0e649838":"code","9386d84d":"code","7e0d2136":"code","c46347b7":"code","e910ce07":"code","7099fabc":"code","d9316b73":"code","cec42c25":"code","669b0e5e":"code","97fab4e2":"code","c6f8b86f":"code","a7fc2059":"code","fb9c97b0":"code","dcf335bc":"code","f1858327":"code","fa68038f":"markdown","7fb69935":"markdown","fb9591b4":"markdown","8adffe4f":"markdown","d5ebdce7":"markdown","6508e5d0":"markdown","178cb4c1":"markdown","db638902":"markdown","5573f81d":"markdown","e011a5df":"markdown","a6ae563c":"markdown","5f362f0d":"markdown","3e8cb488":"markdown","8fbd50bd":"markdown","b33ec563":"markdown","4fc8e62c":"markdown","0dcf9d04":"markdown","50a7cabc":"markdown","c63d8726":"markdown"},"source":{"2d759759":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport re\nimport os\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler","f04f517d":"plt.style.use('ggplot')\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"Survived\"\nTRAIN_SET = \"..\/input\/titanic\/train.csv\"\nTEST_SET = '..\/input\/titanic\/test.csv'","2c985206":"df_train = pd.read_csv(TRAIN_SET)\ndf_test = pd.read_csv(TEST_SET)\ndf_train.head()","ca0b04da":"df_train.info()","5f7afad0":"df_train.describe()","d5cfc83e":"numerical_variables = [var for var in df_train.columns if df_train[var].dtypes != 'O']\nnumerical_variables.pop(numerical_variables.index(TARGET))\nnumerical_variables_num = len(numerical_variables)\nprint('Number of numerical variables: ', numerical_variables_num)\nfor var in numerical_variables:\n    print(var, end=' - ')","b68234f0":"discrete_vars = [var for var in numerical_variables if len(df_train[var].unique()) < 100]\ndiscrete_vars_num = len(discrete_vars)\nprint('Number of discrete variables: ', discrete_vars_num)\nfor var in discrete_vars:\n    print(var, end=' - ')","e4d89768":"continuous_vars = [var for var in numerical_variables if var not in discrete_vars+['PassengerId']]\nprint('Number of continuous variables: ', len(continuous_vars))\nfor var in continuous_vars:\n    print(var, end=' - ')","3882ca15":"categorical_variables = [var for var in df_train.columns if df_train[var].dtypes == 'O']\ncategorical_variables_num = len(categorical_variables)\nprint('Number of categorical variables: ', categorical_variables_num)\nfor var in categorical_variables:\n    print(var, end=' - ')","34f55082":"print(\"Number of complete duplicates: \", df_train.duplicated().sum())","830b5643":"print(f\"Number of nulls in features: {df_train.isnull().sum().sum()}\")","f88c43ca":"null_variables_in_numerics = [var for var in numerical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of null features: {len(null_variables_in_numerics)}\")\nprint(f\"Number of nulls in numerical features: {df_train[null_variables_in_numerics].isnull().sum().sum()}\")\nnum_nulls = df_train[numerical_variables].isnull().sum().sort_values(ascending=False)\nfor row in num_nulls.iteritems():\n    print(f\"{row[0]}    \\t{row[1] }nulls \\t{row[1]\/df_train.shape[0]:.3f}%\")","2e12250c":"null_variables_in_categoric = [var for var in categorical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of null features: {len(null_variables_in_categoric)}\")\nprint(f\"Number of nulls in categorical features: {df_train[null_variables_in_categoric].isnull().sum().sum()}\")\nnum_nulls = df_train[categorical_variables].isnull().sum().sort_values(ascending=False)\nfor row in num_nulls.iteritems():\n    print(f\"{row[0]}   \\t{row[1]} nulls \\t{row[1]\/df_train.shape[0]:.3f}%\")","0aa8dae8":"def get_title_from_name(passenger_name):\n    line = passenger_name\n    if re.search('Mrs', line):\n        return 'Mrs'\n    elif re.search('Mr', line):\n        return 'Mr'\n    elif re.search('Miss', line):\n        return 'Miss'\n    elif re.search('Master', line):\n        return 'Master'\n    else:\n        return 'Other'\n    \ndf_train['Title'] = df_train['Name'].apply(get_title_from_name)\ndf_test['Title'] = df_test['Name'].apply(get_title_from_name)\n\ncategorical_variables.pop(categorical_variables.index('Name'));\ncategorical_variables += ['Title']","519aec1d":"df_train[TARGET].describe()","f43182d9":"df_train[TARGET].hist();","61999626":"for var in continuous_vars:\n    sns.barplot(data=df_train, y=var, x=TARGET)\n    plt.show();","e07cfdfb":"plt.hist(df_train['Age'])\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Age Distribution');","b6c03673":"g = sns.FacetGrid(df_train, col=TARGET,height=5, aspect=1)\ng.map(plt.hist, 'Age', bins=5);","1bf0509e":"df_train_copy = df_train.copy()\ndf_train_copy['AgeBand'] = pd.cut(df_train['Age'], 5)\ntmp = df_train_copy.groupby('AgeBand')['AgeBand'].count().sort_values(ascending=False)\npd.DataFrame(columns=['Count'], data=tmp.values, index=tmp.keys())","29b99aeb":"df_train_copy[['AgeBand', TARGET]].groupby('AgeBand').mean().sort_values(by=TARGET, ascending=False)","87d61c2b":"for i, var in enumerate(discrete_vars):\n    if var == 'Age':\n        continue\n        \n    df = df_train.groupby(var)[TARGET].mean().sort_values(ascending=False)\n    df.plot.bar()\n    plt.ylabel('Survival Rate')\n    plt.title(f'{var} and surviving')\n    plt.show();","161142ef":"for i, var in enumerate(categorical_variables):\n    if var in ['Name', 'Ticket', 'Cabin']:\n        continue\n        \n    df = df_train.groupby(var)[TARGET].mean().sort_values(ascending=False)\n    df.plot.bar()\n    plt.ylabel('Survival Rate')\n    plt.title(f'{var} and surviving')\n    plt.show();","fa9438cd":"df_survived = df_train[df_train[TARGET] == 1]\ndf_survived.groupby('Sex')['Sex'].count().sort_values(ascending=False).plot.bar()\nplt.ylabel('Count')\nplt.title('Survived');","7d4d63bf":"df_unsurvived = df_train[df_train[TARGET] == 0]\ndf_unsurvived.groupby('Sex')['Sex'].count().plot.bar();\nplt.ylabel('Count')\nplt.title('unSurvived');","93272707":"g = sns.catplot(data=df_train, x=\"Pclass\", y=TARGET, hue=\"Sex\",\n                   height=6, kind=\"bar\", palette=['blue', 'red'])\ng.despine(left=True)\ng = g.set_ylabels(\"Survival Rate\")","e275b13d":"def get_ticket_prefix(data):\n    Ticket = []\n    for i in list(data.Ticket):\n        if not i.isdigit() :\n            Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n        else:\n            Ticket.append(\"X\")\n    return Ticket\n        \ndf_train['Ticket'] = get_ticket_prefix(df_train)\ndf_test['Ticket'] = get_ticket_prefix(df_test)","5147ef32":"plt.figure(figsize=(15, 5))\ndf_train.groupby('Ticket')[TARGET].mean().sort_values(ascending=False).plot.bar()\nplt.ylabel('Survival Rate')\nplt.title('Ticket and Surviving');","9b8b1654":"cols = categorical_variables + ['Pclass'] + [TARGET]\nn = len(cols)","171c7983":"def get_chi_test(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = stats.chi2_contingency(confusion_matrix, correction=True)\n    return chi2","2f3b8db5":"df_corr = pd.DataFrame(data=np.zeros(shape=(n, n)), columns=cols, index=cols)\n\np_value_index = 1\n\nfor row in cols:\n    for col in cols:\n        df_corr.loc[row, col] = get_chi_test(df_train[row], df_train[col])[p_value_index]\n\nalpha = 0.05\ncmap = sns.cm.rocket_r\nsns.heatmap(df_corr, vmin=1, vmax=0, annot=True, fmt='.3f')\nplt.title('p-value');","6d1d534c":"# Reference\n# https:\/\/stackoverflow.com\/questions\/46498455\/categorical-features-correlation\/46498792#46498792\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","659d7b3c":"df_corr = pd.DataFrame(data=np.zeros(shape=(n, n)), columns=cols, index=cols)\n\np_value_index = 1\n\nfor row in cols:\n    for col in cols:\n        df_corr.loc[row, col] = cramers_v(df_train[row], df_train[col])\n\nalpha = 0.05\ncmap = sns.cm.rocket_r\nsns.heatmap(df_corr, vmin=0, vmax=1, annot=True, fmt='.3f')\nplt.title('Cram\u00e9r\\'s V');","bd53b2b4":"from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler, LabelEncoder","f8b5e16a":"df_train = df_train.drop(['Name', 'Cabin', 'PassengerId'], axis=1)\n\ntest_id = df_test['PassengerId']\ndf_test = df_test.drop(['Name', 'Cabin', 'PassengerId'], axis=1)","ad7746ad":"df_train.isnull().sum()","4aa9a7c2":"df_test.isnull().sum()","ec4064b7":"def fill_with_mode(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    for feature in features:\n        mode = train[feature].mode()[0]\n        \n        train[feature] = train[feature].fillna(mode)\n        test[feature] = test[feature].fillna(mode)\n        \n    return train, test\n\nfeatures_to_fill_with_mode = ['Embarked']\ndf_train, df_test = fill_with_mode(df_train, df_test, features_to_fill_with_mode)","748eaedb":"def fill_with_median(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    for feature in features:\n        median = train[feature].dropna().median()\n        \n        train[feature] = train[feature].fillna(median)\n        test[feature] = test[feature].fillna(median)\n        \n    return train, test\n\nfeatures_to_fill_with_median = ['Age', 'Fare']\ndf_train, df_test = fill_with_median(df_train, df_test, features_to_fill_with_median)","d87c1e88":"data_combine = [df_train, df_test]\nfor dataset in data_combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n\n    \ndf_train = df_train.drop(['Parch', 'SibSp'], axis=1)\ndf_test = df_test.drop(['Parch', 'SibSp'], axis=1)","6a9af503":"# df_train['Age'] = pd.qcut(df_train['Age'], 4)\n# df_test['Age'] = pd.qcut(df_test['Age'], 4) \n\n# df_train['Fare'] = pd.qcut(df_train['Fare'], 5)\n# df_test['Fare'] = pd.qcut(df_test['Fare'], 5) ","5248e97f":"def to_ordinal_encoding(train, test, var):\n    train = train.copy()\n    test = test.copy()\n\n    orderd_labels = train.groupby(var)[TARGET].mean().sort_values().index.tolist()\n    ordinal_encoder = OrdinalEncoder(categories=[orderd_labels], handle_unknown='ignore')\n    \n    train[var] = ordinal_encoder.fit_transform(train[var].values.reshape(-1, 1))\n    test[var] = ordinal_encoder.transform(test[var].values.reshape(-1, 1))\n    \n    return train, test\n\ndf_train['Pclass'] = df_train['Pclass'].astype(str)\ndf_test['Pclass'] = df_test['Pclass'].astype(str)\n\ncateg_feats = [var for var in df_train.columns if df_train[var].dtypes == 'O' and var != TARGET]\nfor var in categ_feats:\n    df_train, df_test = to_ordinal_encoding(df_train, df_test, var)","fff13145":"# df_train.drop([ 'Ticket', 'Embarked', 'Sex', 'FamilySize'], axis=1, inplace=True)\n# df_test.drop([ 'Ticket', 'Embarked', 'Sex', 'FamilySize'], axis=1, inplace=True)\n\n# df_train.drop([ 'Embarked' ], axis=1, inplace=True)\n# df_test.drop([ 'Embarked' ], axis=1, inplace=True)","e8b99f1a":"df_train","217c442e":"def transform_with_MinMax(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    scaler = MinMaxScaler()\n    scaler.fit(train[features])\n    \n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    \n    return train, test\n\ndef transform_with_Standard(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    scaler = StandardScaler()\n    scaler.fit(train[features])\n    \n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    \n    return train, test","44697a16":"features = [var for var in df_train.columns if var not in [TARGET]]\ndf_train, df_test = transform_with_Standard(df_train, df_test, features)\n# df_train, df_test = transform_with_MinMax(df_train, df_test, features)","df65e6d4":"from sklearn.model_selection import cross_validate, GridSearchCV\n\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","49d3e7d5":"X_train = df_train.drop([TARGET], axis=1)\ny_train = df_train[TARGET]","0e649838":"#Validation function\nn_folds =  12\n# n_folds =  342\n\n\ndef cross_validation(model):\n    results = cross_validate(model, X_train, y_train, scoring='accuracy', cv=n_folds, return_train_score=True, n_jobs=-1)\n    return results","9386d84d":"MLA_columns = ['Parameters', 'Train Accuracy Mean', \"Test Accuracy Mean\", \"Test Std\", 'Time']\ndf_scores = pd.DataFrame(columns = MLA_columns)\ndf_scores.index.name = 'Model'","7e0d2136":"model_version = \"_V1\"","c46347b7":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(random_state=SEED),\n    ensemble.BaggingClassifier(random_state=SEED),\n    ensemble.ExtraTreesClassifier(random_state=SEED),\n    ensemble.GradientBoostingClassifier(random_state=SEED),\n    ensemble.RandomForestClassifier(random_state=SEED),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(random_state=SEED),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(random_state=SEED),\n    linear_model.PassiveAggressiveClassifier(random_state=SEED),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(random_state=SEED),\n    linear_model.Perceptron(random_state=SEED),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(random_state=SEED),\n    tree.ExtraTreeClassifier(random_state=SEED),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    XGBClassifier(random_state=SEED)    \n    ]","e910ce07":"# Reference\n# https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook#Step-5:-Model-Data\n\n#index through MLA and save performance to table\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__ + model_version\n#     df_scores.loc[row_index, 'Model'] = MLA_name\n    df_scores.loc[MLA_name, 'Parameters'] = str(alg.get_params())\n    \n    cv_results = cross_validation(alg)\n\n    df_scores.loc[MLA_name, 'Time'] = cv_results['fit_time'].mean()\n    df_scores.loc[MLA_name, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\n    df_scores.loc[MLA_name, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\n    df_scores.loc[MLA_name, 'Test Std'] = cv_results['test_score'].std()   \n\n    \ndf_scores.sort_values(by = ['Test Accuracy Mean'], ascending=False, inplace=True)\ndf_scores","7099fabc":"plt.figure(figsize=(15, 10))\nsns.barplot(x='Test Accuracy Mean', y=df_scores.index, data=df_scores, color='r')\nplt.title('Machine Learning Algorithm Test Accuracy \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm');","d9316b73":"model_version = \"_V2\"","cec42c25":"%%time\n\nparam_grid = {'n_neighbors': [5, 6,7,8,9, 11,12,14,16,18, 19, 20, 21],\n             }\nclf = KNeighborsClassifier()\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=n_folds, scoring=\"accuracy\", return_train_score=True, n_jobs=-1, verbose=True)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n\nprint(best_params) \n\n\nclf = KNeighborsClassifier(**best_params, n_jobs=-1)\nmodel_name = clf.__class__.__name__ + model_version\n\ncv_results = cross_validation(clf)\n\n\ndf_scores.loc[model_name, 'Parameters'] = str(clf.get_params())\ndf_scores.loc[model_name, 'Time'] = cv_results['fit_time'].mean()\ndf_scores.loc[model_name, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\ndf_scores.loc[model_name, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\ndf_scores.loc[model_name, 'Test Std'] = cv_results['test_score'].std()\ndf_scores.loc[model_name].to_frame()","669b0e5e":"%%time\n\nparam_grid = {'max_depth': [4, 8, 16, 32]\n             }\n\nclf = DecisionTreeClassifier(random_state=SEED)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=n_folds, scoring=\"accuracy\", return_train_score=True, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n\nprint(best_params) \n\nclf = DecisionTreeClassifier(**best_params, random_state=SEED)\nmodel_name = clf.__class__.__name__ + model_version\n\ncv_results = cross_validation(clf)\n\n\ndf_scores.loc[model_name, 'Parameters'] = str(clf.get_params())\ndf_scores.loc[model_name, 'Time'] = cv_results['fit_time'].mean()\ndf_scores.loc[model_name, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\ndf_scores.loc[model_name, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\ndf_scores.loc[model_name, 'Test Std'] = cv_results['test_score'].std()\ndf_scores.loc[model_name].to_frame()","97fab4e2":"clf.fit(X_train, y_train)\nf = pd.DataFrame(columns=['Feature', 'Importance'])\nf['Feature'] =  X_train.columns\nf['Importance'] = clf.feature_importances_\nf.sort_values('Importance', ascending=False)","c6f8b86f":"%%time\n\nparam_grid = {'n_estimators': [100, 300, 500],\n              'max_depth': [4, 8, 16],\n             }\n\nclf = RandomForestClassifier(random_state=SEED, n_jobs=5)\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid, cv=n_folds, scoring=\"accuracy\", return_train_score=True, n_jobs=5, verbose=2)\ngrid_search.fit(X_train, y_train)\nbest_params = grid_search.best_params_\n\nprint(best_params)\n\nclf = RandomForestClassifier(**best_params, n_jobs=-1, random_state=SEED)\nmodel_name = clf.__class__.__name__ + model_version\n\ncv_results = cross_validation(clf)\n\n\ndf_scores.loc[model_name, 'Parameters'] = str(clf.get_params())\ndf_scores.loc[model_name, 'Time'] = cv_results['fit_time'].mean()\ndf_scores.loc[model_name, 'Train Accuracy Mean'] = cv_results['train_score'].mean()\ndf_scores.loc[model_name, 'Test Accuracy Mean'] = cv_results['test_score'].mean()\ndf_scores.loc[model_name, 'Test Std'] = cv_results['test_score'].std()\ndf_scores.loc[model_name].to_frame()","a7fc2059":"clf.fit(X_train, y_train)\nf = pd.DataFrame(columns=['Feature', 'Importance'])\nf['Feature'] =  X_train.columns\nf['Importance'] = clf.feature_importances_\nf.sort_values('Importance', ascending=False)","fb9c97b0":"df_scores.sort_values('Test Accuracy Mean', ascending=False, inplace=True)\ndf_scores.to_csv('model_scores.csv', index=True)\ndf_scores","dcf335bc":"clf = RandomForestClassifier(**{'max_depth': 8, 'n_estimators': 100}, random_state=SEED)\nclf.fit(X_train, y_train)\npreds = clf.predict(df_test)","f1858327":"sub = pd.DataFrame()\nsub['PassengerId'] = test_id\nsub[TARGET] = preds\nsub.to_csv(\"submission.csv\", index=False)","fa68038f":"## Summary\n\n* Rich and upper class people survived more than medium or lower class people.\n* Women survived more than men.\n* Children and adult people survived more than the rest.","7fb69935":"Who paid more has more chance to survive than he who doesn't. ","fb9591b4":"### First Intuition Summary\n* 891 records. \n* 12 feature + target.\n* 7 numerical feature include (id + target).\n* 5 categorical features.\n* 0 complete duplicates.\n* 866 null value in all features.\n* 177 nulls in numerical features.\n* 689 nulls in categorical features.","8adffe4f":"**chi square test**","d5ebdce7":"**Cram\u00e9r\\'s V**","6508e5d0":"# Submission","178cb4c1":"# Feature Engineering","db638902":"**Decision Tree Classifier**","5573f81d":"## Categoricals Association","e011a5df":"## Target vs All","a6ae563c":"### Adding Features","5f362f0d":"# Modeling","3e8cb488":"**Random Forest Classifier**","8fbd50bd":"Lets explore more KNN, RandomForest, GradientBoosting and XGboost classifiers for the better preformance.","b33ec563":"**K Nearest Neighbor**","4fc8e62c":"## First Intuition","0dcf9d04":"## Summary\n* Not surprisingly ('Sex', 'Embarked', 'Title', 'Pclass') are correlated with the survival.\n* **'Ticket'** is correlated with the survival but 'Cabin' (p-value was 0.184) not. Tickets with same prefixes may have a similar class and survival.","50a7cabc":"First we extract the meaningfull information from the 'Name'.","c63d8726":"# EDA"}}