{"cell_type":{"b9db4cd4":"code","b5cf26f3":"code","47cf4532":"code","f737b004":"code","85b5def6":"code","fb3c3c39":"code","7275e957":"code","2f5ab1af":"code","bfca81fb":"code","018ef2fc":"code","d693e69e":"code","6597f44a":"code","a2ecb26e":"code","99ceadfb":"code","f6ec8238":"code","0678b8c9":"code","a534e086":"code","4ca62367":"code","cf879702":"code","41e291f3":"code","6ba1b032":"code","b15bcc2a":"code","026dd2f0":"code","4504abc0":"code","2e7dadd8":"code","e8f2a2a5":"code","6305b3c9":"code","9f5e2ef2":"code","e6b8d337":"code","0d6e2973":"code","79cdd274":"code","e2775688":"code","f3904e9e":"code","9561c4a8":"code","e0da15d1":"code","46baa102":"code","cdbd6bec":"code","09b1d4d6":"code","24a1ef9a":"code","1332f2d5":"code","37f413d7":"code","3939092b":"code","9dd3876f":"code","5d4614a6":"code","1bc2aa8c":"code","af0f4875":"code","953cea1c":"code","704d53f7":"code","4de7728b":"code","214767d5":"code","430fb7ab":"code","d7d7518f":"code","88f32948":"code","acf83b10":"code","4679f8c9":"code","e498aebc":"code","f393aab2":"code","ebd383bc":"code","264666f7":"code","8f8dbac9":"code","4b360722":"code","38d92814":"code","d7b7c5a9":"code","d8c29f18":"code","5295d9c5":"markdown","fe02cb69":"markdown","66c498cd":"markdown","4d69ec8e":"markdown","3c9328d2":"markdown","4fe1af59":"markdown","cfcc998b":"markdown","ca4458c8":"markdown","2793c227":"markdown","45ef9c23":"markdown","8515a493":"markdown","2add0faf":"markdown","9a7deb03":"markdown","335f47e2":"markdown","dee80e30":"markdown","dddf4965":"markdown","7bbb269d":"markdown","fbead61e":"markdown","d23c812d":"markdown","0d16bb79":"markdown","7bb1b214":"markdown","601f8824":"markdown","4c0aced5":"markdown","65e3c3b2":"markdown","7fad2718":"markdown","80fc5aed":"markdown","cf145d18":"markdown","17eff77d":"markdown","5f34eb45":"markdown","f8337932":"markdown","124f64a6":"markdown","5dc06752":"markdown","d84d4d3d":"markdown","4816071a":"markdown","3d1e9e4f":"markdown","591a592b":"markdown","1936b8fc":"markdown","2b69566f":"markdown","cdea7e8e":"markdown","c0f43b25":"markdown","713bec85":"markdown","b347b5ef":"markdown","9b853c29":"markdown","c32a4c9f":"markdown","a2dada3d":"markdown","ce9025bb":"markdown","2df519e9":"markdown","055a121e":"markdown","1d2c0f10":"markdown","a87d92aa":"markdown","fb47ff0b":"markdown"},"source":{"b9db4cd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5cf26f3":"import numpy as np # linear algerbra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # plotting\nimport json # data processing(.json)\n\nfrom sklearn.preprocessing import LabelEncoder # Creates placeholders for categorical variables\nfrom sklearn.feature_extraction.text import CountVectorizer # converts text into vector matrix\nfrom sklearn.model_selection import train_test_split # split data into training and testing sets\nfrom sklearn.naive_bayes import MultinomialNB # ML model for naive bayes\nfrom sklearn.metrics import accuracy_score, confusion_matrix # measure the accuracy of the model\nfrom sklearn.metrics import classification_report # classification report of the model\n \nimport re # NLP\nimport nltk # natural langauge processing\nfrom nltk.tokenize import word_tokenize # tokenizer\nfrom nltk.stem import PorterStemmer # stemmer\nfrom nltk.corpus import stopwords # stopwords\n\nfrom wordcloud import WordCloud # create word cloud images of text\n\nimport tensorflow as tf # create neural networks\nfrom tensorflow.keras import Sequential # create squential NN model\nfrom tensorflow.keras.layers import Dense # implements the operation: output = activation(dot(input, kernel) + bias)\nfrom tensorflow.keras.utils import plot_model # plot model architecture\nfrom tensorflow.keras.callbacks import EarlyStopping # early stopping of training\nfrom tensorflow.keras.models import load_model # load saved model\n\nfrom sklearn.model_selection import GridSearchCV # hyperparameter optimization\nfrom sklearn.model_selection import RandomizedSearchCV # hyperparameter optimization\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier # linking keras model to sklearn\n\n%matplotlib inline \n# With this backend, the output of plotting commands is displayed inline within frontends\n# like the Jupyter notebook, directly below the code cell that produced it. The resulting \n# plots will then also be stored in the notebook document","47cf4532":"# read dataset which in .csv format\n\ndata = pd.read_csv('\/kaggle\/input\/language-identification-datasst\/dataset.csv', encoding='utf-8').copy() # creates a dataframe of a copy of the dataset\n                                                           # utf-8 encoding use to be able to read text in other langauge\nprint(data.shape)  # shape of the dataset\ndata.head()        # first 5 rows of the dataset","f737b004":"data.tail() # last 5 rows of the dataset","85b5def6":"# categories in our target variable\n\ndata['language'].unique() ","fb3c3c39":"# checking for null values\n\ndata.isnull().sum()","7275e957":"# checking the datatype of the features\n\ndata.dtypes","2f5ab1af":"# number of of samples per language (category)\n\ndata['language'].value_counts()","bfca81fb":"# dropping duplicate samples\n\ndata = data.drop_duplicates(subset='Text')\ndata = data.reset_index(drop=True)","018ef2fc":"# rechecking the number of samples per language\n\ndata['language'].value_counts()","d693e69e":"# languages stopwords supported by NLTK\n\nprint(stopwords.fileids())","6597f44a":"# adding nonalphanumeric char to stopwords\n\nnonalphanumeric = ['\\'', '.', ',', '\\\"', ':', ';', '!', '@', '#', '$', '%', '^', '&',\n                 '*', '(', ')', '-', '_', '+', '=', '[', ']', '{', '}', '\\\\', '?', \n                 '\/','>', '<', '|', ' '] \n\nstopwords = nonalphanumeric","a2ecb26e":"# total stop words\n\nlen(stopwords)","99ceadfb":"def clean_text(text):\n    \"\"\"\n    takes text as input and returns cleaned text after tokenization, \n    stopwords removal and stemming\n    \"\"\"\n    tokens = word_tokenize(text) # creates text into list of words\n    words = [word.lower() for word in tokens if word not in stopwords] # creates a list with words which are not stopwords\n    words = [PorterStemmer().stem(word) for word in words] # stems(remove suffixes and prefixes)  words\n    return \" \".join(words) # joins the list of cleaned words into a sentence string","f6ec8238":"# applying clean_text function to all rows in 'Text' column \n\ndata['clean_text'] = data['Text'].apply(clean_text)","0678b8c9":"# using LabelEncoder to get placeholder number values for categorical variabel 'language'\n\nle = LabelEncoder()\ndata['language_encoded'] = le.fit_transform(data['language'])\ndata.head()","a534e086":"# list of languages encoded with thier respective indices representing their placeholder numbers\n\nlang_list = [i for i in range(22)]\nlang_list = le.inverse_transform(lang_list)\nlang_list = lang_list.tolist()\nlang_list","4ca62367":"# plotting a language-wise freqeuncy distribtion for number of samples in each language \n\nplt.figure(figsize=(10,10))\nplt.title('Language Counts')\nax = sns.countplot(y=data['language'], data=data)\nplt.show()","cf879702":"def remove_english(text):\n    \"\"\"\n    function that takes text as input and returns text without english words\n    \"\"\"\n    pat = \"[a-zA-Z]+\"\n    text = re.sub(pat, \"\", text)\n    return text","41e291f3":"data_Chinese = data[data['language']=='Chinese'] # Chinese data in dataset\n\nclean_text = data.loc[data.language=='Chinese']['clean_text']\nclean_text = clean_text.apply(remove_english) # removing english words\n\ndata_Chinese.loc[:,'clean_text'] = clean_text","6ba1b032":"# removing old chinese text and appending new cleaned chinese text\n\ndata.drop(data[data['language']=='Chinese'].index, inplace=True, axis=0)\ndata = data.append(data_Chinese)","b15bcc2a":"# shuffling dataframe and resetting index\n\ndata =data.sample(frac=1).reset_index(drop=True)","026dd2f0":"# defining input variable\n# vectorizing input varible 'clean_text' into a matrix \n\nx = data['clean_text']\n\ncv = CountVectorizer() # ngram_range=(1,2)\nx = cv.fit_transform(x)\n\n# changing the datatype of the number into uint8 to consume less memory\nx = x.astype('uint8') # uint8 and float32","4504abc0":"# defining target variable\n\ny = data['language_encoded']","2e7dadd8":"# splitting data into training and testing datasets\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)","e8f2a2a5":"# fitting the Multinomial Naive Bayes model\n\nNB_model = MultinomialNB()\nNB_model.fit(x_train, y_train)","6305b3c9":"# predicting using the naive bayes model\n\ny_pred = NB_model.predict(x_test)","9f5e2ef2":"# accuracy of the naive bayes model\n\naccuracy_score(y_test, y_pred)","e6b8d337":"# creating confusion matrix heatmap \n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(12,10))\nplt.title('Confusion Matrix - NB_model', Fontsize=20)\nsns.heatmap(cm, xticklabels=lang_list, yticklabels=lang_list, cmap='rocket_r', linecolor='white', linewidth=.005)\nplt.xlabel('Predicted Language', fontsize=15)\nplt.ylabel('True Language', fontsize=15)\nplt.show()","0d6e2973":"print(classification_report(y_test, y_pred))","79cdd274":"# converting csr matrix into np.ndarray supported by tensorflow\n\nx_train = x_train.toarray()\nx_test = x_test.toarray()","e2775688":"# shapes of the various datasets\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","f3904e9e":"# input size hyperparameter\n\nINPUT_SIZE = x_train.shape[1]\nINPUT_SIZE","9561c4a8":"# outputsize hyperparatmeter\n\nOUTPUT_SIZE = len(data['language_encoded'].unique())\nOUTPUT_SIZE","e0da15d1":"# epochs and batch_size hyperparameters\n\nEPOCHS = 10\nBATCH_SIZE = 128","46baa102":"# creating the MLP model\n\nmodel = Sequential([\n    Dense(100, activation='relu', kernel_initializer='he_normal', input_shape=(INPUT_SIZE,)),\n    Dense(80, activation='relu', kernel_initializer='he_normal'),\n    Dense(50, activation='relu', kernel_initializer='he_normal'),\n    Dense(OUTPUT_SIZE, activation='softmax')\n])","cdbd6bec":"# compiling the MLP model\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","09b1d4d6":"# fitting the MLP model\n\nhist = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.3, verbose=2)","24a1ef9a":"# summary of the MLP model\n\nmodel.summary()","1332f2d5":"# architetcure of the MLP model\n\nplot_model(model, show_shapes=True)","37f413d7":"# creating loss vs epochs plot\n\nplt.title('Learning Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Categorical Crossentropy')\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.legend()\nplt.show()","3939092b":"# creating accuracy vs epochs plot\n\nplt.title('Learning Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='val')\nplt.legend()\nplt.show()","9dd3876f":"# evaluating the loss and accuracy of the model\n\nloss, accuracy = model.evaluate(x_test, y_test, verbose=2)\nprint('Accuracy %.3f'%accuracy)","5d4614a6":"# defining input variable\n# vectorizing input varible 'clean_text' into a matrix \n\nx = data['clean_text']\n\ncv = CountVectorizer() # ngram_range=(1,2)\nx = cv.fit_transform(x)\n\n# changing the datatype of the number into uint8 to consume less memory\nx = x.astype('uint8') # uint8 and float32","1bc2aa8c":"# defining target variable\n\ny = data['language_encoded']","af0f4875":"# splitting data into training and testing datasets\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)","953cea1c":"# converting csr matrix into np.ndarray supported by tensorflow\n\nx_train = x_train.toarray()\nx_test = x_test.toarray()","704d53f7":"# shapes of the various datasets\n\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","4de7728b":"# input size hyperparameter\n\nINPUT_SIZE = x_train.shape[1]\nINPUT_SIZE\n\n# outputsize hyperparatmeter\n\nOUTPUT_SIZE = len(data['language_encoded'].unique())\nOUTPUT_SIZE","214767d5":"BATCH_SIZE = 256\nEPOCHS = 8","430fb7ab":"# configuring early stopping\n\nes = EarlyStopping(monitor='accuracy', patience=1)","d7d7518f":"# creating the MLP model\n\nmodel = Sequential([\n    Dense(100, activation='softsign', kernel_initializer='glorot_uniform', input_shape=(INPUT_SIZE,)),\n    Dense(80, activation='softsign', kernel_initializer='glorot_uniform'),\n    Dense(50, activation='softsign', kernel_initializer='glorot_uniform'),\n    Dense(OUTPUT_SIZE, activation='softmax')\n])","88f32948":"# compiling the MLP model\n\nmodel.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","acf83b10":"# fitting the model with earlystopping callback to avoid overfitting \n\nhist = model.fit(x_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.3, callbacks=[es], verbose=2)","4679f8c9":"# summary of the MLP model\n\nmodel.summary()","e498aebc":"# architetcure of the MLP model\n\nplot_model(model, show_shapes=True)","f393aab2":"# creating loss vs epochs plot\n\nplt.title('Learning Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Categorical Crossentropy')\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='val')\nplt.legend()\nplt.show()","ebd383bc":"# creating accuracy vs epochs plot\n\nplt.title('Learning Curve')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='val')\nplt.legend()\nplt.show()","264666f7":"# evaluating the loss and accuracy of the model\n\nloss, accuracy = model.evaluate(x_test, y_test, verbose=2)\nprint('Accuracy %.3f'%accuracy)","8f8dbac9":"y_pred_prob = model.predict(x_test) # returns an array containing probability for each category being output\ny_pred = []\nfor i in y_pred_prob:\n    out = np.argmax(i) # taking the highest probability category as output\n    y_pred.append(out)\ny_pred = np.array(y_pred)\n\ncm = confusion_matrix(y_test, y_pred) # confusion matrix\n\n# heat map of confusion matrix\nplt.figure(figsize=(12,10))\nplt.title('Confusion Matrix - MLP Model', Fontsize=20)\nsns.heatmap(cm, xticklabels=lang_list, yticklabels=lang_list, cmap='rocket_r', linecolor='white', linewidth=.005)\nplt.xlabel('Predicted Language', fontsize=15)\nplt.ylabel('True Language', fontsize=15)\nplt.show()","4b360722":"print(classification_report(y_test, y_pred))","38d92814":"# saving the model\n\nmodel.save('language_identifcation_model.h5')","d7b7c5a9":"# loading the model\n\nmodel = load_model('language_identifcation_model.h5')","d8c29f18":"# using the model for prediction\n\nsent = \"\"\"\u0906\u092a \u0915\u093f\u0924\u0928\u093e \u0938\u094b\u091a\u0924\u0947 \u0939\u094b\n\u0905\u0917\u0930 \u0906\u092a \u0920\u093f\u0920\u0941\u0930\u0924\u0940 \u0930\u093e\u0924\u094b\u0902 \u0915\u094b \u0917\u093f\u0928\u0947\u0902\n\u0905\u0930\u0947 \u0915\u094d\u092f\u093e \u0906\u092a \u092e\u093f\u0932 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902 (\u0905\u0930\u0947, \u0915\u094d\u092f\u093e \u0906\u092a \u092e\u093f\u0932 \u0938\u0915\u0924\u0947 \u0939\u0948\u0902?)\n\u0915\u094d\u092f\u093e \u0924\u0941\u092e \u092e\u093f\u0932\u094b\u0917\u0947 (\u0915\u094d\u092f\u093e \u0924\u0941\u092e \u092e\u093f\u0932\u094b\u0917\u0947?)\n\u0938\u0930\u094d\u0926\u093f\u092f\u094b\u0902 \u0915\u093e \u0905\u0902\u0924 \u092c\u0924\u093e\u0913\n\u090f\u0915 \u0915\u094b\u092e\u0932 \u0935\u0938\u0902\u0924 \u0915\u0947 \u0926\u093f\u0928 \u0924\u0915\n\u092e\u0948\u0902 \u091a\u093e\u0939\u0924\u093e \u0939\u0942\u0902 \u0915\u093f \u0924\u0941\u092e \u0924\u092c \u0924\u0915 \u0930\u0939\u094b \u091c\u092c \u0924\u0915 \u092b\u0942\u0932 \u0916\u093f\u0932 \u0928 \u091c\u093e\u090f\u0902\n\u091c\u094d\u092f\u094b\u0902 \u0915\u093e \u0924\u094d\u092f\u094b\u0902\"\"\"\n\n\nsent = cv.transform([sent])\nans = model.predict(sent)\nans = np.argmax(ans)\nle.inverse_transform([ans])","5295d9c5":"## Loading the data","fe02cb69":"## Creating a Naive Bayes model","66c498cd":"Our stopwords list contains a total of 16023 stopwords that will be reomoved form the dataset.","4d69ec8e":"We see that are model is overfitting which can lead our accuracy to get lower for new data.","3c9328d2":"## Creating a Multilayer Perceptron model","4fe1af59":"Tensorflow does not take sparse matrices as input and hence we have to convert our sparse matrix into an array. This requires alot of memory and hence uit8 data type helps.","cfcc998b":"We split 30% of the training data into validation data to check if the model is overfitting.","ca4458c8":"We use the Adaptive Moment Estimation (Adam) algorithm for optimization as it is good with sparse data. We use Sparse Categorical Crossentropy as the loss\/objective function as it performs well for muliclassification problems.","2793c227":"We see that some of the languages had text that had been repeated and hence we dropped those samples.","45ef9c23":"We configure early stopping to avoid overfitting.","8515a493":"The frequency plot of each language shows us the amount of samples we have for each language. All languages except Estonian, Russian, Arabic, Portugese, Dutch, Spanish, Pushto, Swedish, Hindi, French, Tamil, Indonesian and Latin have 1000 samples. ","2add0faf":"We see the 22 different categories that we will be classifiying the dataset into.","9a7deb03":"WiLI-2018, the Wikipedia language identification benchmark dataset, contains 235000 paragraphs of 235 languages.\nEach language in this dataset contains 1000 rows\/paragraphs.\n\nWe use a dataset that contains 22 selective languages from the original dataset which includes the following Languages\n\n- English\n- Arabic\n- French\n- Hindi\n- Urdu\n- Portuguese\n- Persian\n- Pushto\n- Spanish\n- Korean\n- Tamil\n- Turkish\n- Estonian\n- Russian\n- Romanian\n- Chinese\n- Swedish\n- Latin\n- Indonesian\n- Dutch\n- Japanese\n- Thai\n\nDataset: https:\/\/www.kaggle.com\/zarajamshaid\/language-identification-datasst","335f47e2":"We obtain an accuracy of 95.4% this is the accuracy that we have to beat after optimization.","dee80e30":"We create a Sequential model with activation function as Rectified Linear Unit(ReLU) and intializer as HE Normal as it works well with ReLU.\n\nOur Neural Network is 5 layers deep and 261254 layers wide.\n\nIt contains 3 hidden layers:\n- Hidden Layer 1 : 100 nodes \n- Hidden Layer 2 : 80 nodes\n- Hidden Layer 3 : 50 nodes\n\nThe output layer uses the Soft Max activation function which returns an array of lenght 22 contianing probablity of an element's index being the output.","dddf4965":"We split 30% of the training data into validation data to check if the model is overfitting.","7bbb269d":"We re-transform the data to obtain the entire dataset.","fbead61e":"## Preprocessing the data","d23c812d":"The dataset contains 1000 samples for each language, hence are dataset has a total of 22000 rows.","0d16bb79":"EPOCHS indicates the number of passes of the entire training dataset the machine learning algorithm has completed.\n\nWe divide the dataset into batches of BATCH_SIZE for ease of computation.","7bb1b214":"We use Count Vectorizer to transform the input text into a sparse matrix on the basis of the frequency(count) of each word that occurs in the entire text. \n\nCountVectorizer (also known as Bag of Words) creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample. \n\n","601f8824":"lang_list contains a list of all encoded languages with thier indices as thier placeholder values.","4c0aced5":"## Predicting using the model","65e3c3b2":"Computers cannot understand text data, they only understand numbers. Hence we provide placeholder numbers to each language category using the label encoder.","7fad2718":"Our model is a 5 layer Neural Network with the following architecture.","80fc5aed":"## Creating the optimized model","cf145d18":"## Saving the model","17eff77d":"We create a baseline model to compare our Multilayer Perceptron Neural Network against.","5f34eb45":"Looking at the confusion matrix we see that the model predicted Japanese as Chinese an unsual number of times, this could be due to the similarity between the Hanzi Script (Chinese) and the Kanji Script (Japanese).","f8337932":"We save the model for deployment in the future.","124f64a6":"Our model MLP obtains an accuracy of 96.8% which beats are MultinomialNB model (96.8%).","5dc06752":"We use the Adaptive Moment Estimation (Adam) algorithm for optimization as it is good with sparse data. We use Sparse Categorical Crossentropy as the loss\/objective function as it performs well for muliclassification problems.","d84d4d3d":"Hence we see that the Chinese text is now cleaned","4816071a":"We load the model and deploy it to predict the language of unknown text.","3d1e9e4f":"## Exploring the data","591a592b":"We split the dataset into X (independent) variable or inputs and y (dependent) variable or targets.","1936b8fc":"We split the dataset into training and testing dataset, using 20% of the dataset for testing and rest for training.","2b69566f":"We re-shuffle the dataset to have train and test samples representative of the entire dataset.","cdea7e8e":"# NLP DL Lang Identification \n","c0f43b25":"We create an optimized model using the hyperparameters found in Grid Search and Randomized Search.","713bec85":"We obtain an accuracy of 95.61%. \n\nLooking at the confusion matrix we see that the model predicted Chinese as Japanese an unsual number of times, this could be due to the similarity between the Hanzi Script (Chinese) and the Kanji Script (Japanese).","b347b5ef":"We see that there are english words present in the chinsese text and hence we will remove them.","9b853c29":"## Splitting into inputs and targets","c32a4c9f":"We convert the datatype from int64 to uint8 which consumes 8 times less bytes in comparison.\n\nA UINT8 is an 8-bit unsigned integer (range: 0 through 255 decimal). ","a2dada3d":"MultinomialNB is suitable for classification with discrete features and hence works well with our given dataset.","ce9025bb":"We create a function that takes text as input and preprocesses it to give clean text as output.\n\nThe function clean_text():\n- tokenizes the text into a list of words\n- lowers all words into lowercase\n- removes stopwords from the list of lowered words\n- stems the words\n- returns them in form of a string seperated by \" \"","2df519e9":"We create a function that takes text as input and removes all words in latin script from it.","055a121e":"## Importing Dependencies","1d2c0f10":"We create a Sequential model with activation function as softsign and intializer as glorot_uniform. \nOur Neural Network is 5 layers deep and 261254 layers wide.\n\nIt contains 3 hidden layers:\n- Hidden Layer 1 : 100 nodes \n- Hidden Layer 2 : 80 nodes\n- Hidden Layer 3 : 50 nodes\n\nThe output layer uses the Soft Max activation function which returns an array of lenght 22 contianing probablity of an element's index being the output.","a87d92aa":"Our model is a 5 layer Neural Network with the following architecture.","fb47ff0b":"It is convention to define hyperparameters separately with capital letters."}}