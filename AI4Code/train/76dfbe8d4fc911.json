{"cell_type":{"3047735e":"code","5dae56fe":"code","3af02efb":"code","fbdb4e47":"code","fc4cd2e1":"code","8294ac82":"code","9dff4958":"code","99b3d2bb":"code","f615e512":"code","e1b49507":"code","f787625a":"code","b8029584":"code","b4445cc6":"code","839e379d":"code","2dc44b20":"code","8f4cfae8":"code","d35327fe":"code","2f3ff45c":"code","4671d6f6":"code","9061254a":"code","ac806ab6":"code","6bc49648":"code","ce0527b4":"code","69612454":"code","04692ed8":"code","824e55fb":"code","dc4e6996":"code","086bae0d":"code","fa23e7ed":"code","7f75f976":"code","6806d1f2":"code","0e2ce2e8":"code","c8b0e4d7":"code","ff3f9c76":"code","67fa4ef1":"code","c56ba994":"code","43b84804":"code","28eaa7c0":"code","b735db42":"code","6fe4d4f4":"code","17a9dab8":"code","0a7d09af":"code","29369943":"code","33079616":"code","bb734f4e":"code","8f38f3a2":"code","e33f7918":"code","793c4120":"code","d624214c":"code","8689e9ac":"code","3f14b547":"code","2ef41919":"code","0b0c2e8d":"code","0c6f7e1b":"code","3cdf7f99":"code","d0cdb6f7":"code","92d633fd":"code","912c8b82":"code","a86af41a":"code","ef8ad8f3":"code","9a163d07":"code","8ff7dd8a":"code","b2902d42":"code","b6493456":"code","e5b284d0":"code","58e7d327":"code","c0f9c5ce":"code","e1985ddd":"code","5b92faac":"code","eb0b9dc7":"code","d19d2fdc":"code","eb45bb84":"code","9c193c5b":"code","087a5f88":"code","ba25d217":"code","841707a2":"code","0e78ce6c":"code","9b6a73be":"code","08455ed8":"code","e9455769":"code","6aa14e13":"code","2546458f":"code","79a40554":"code","5c5e6f19":"code","d9af6530":"code","eb78a0d7":"code","8c92180c":"code","df80494d":"code","5030ccc4":"code","99b3d680":"code","29f0a7da":"code","687e876d":"code","6e226079":"code","ecc9ec87":"code","3e6b568f":"code","6ccb5d83":"code","529554d4":"code","b92f58f4":"code","3fd0d1a3":"code","f9c33d5f":"code","e316070f":"code","5b7c1701":"code","1f945b8a":"code","6cbf177b":"markdown","0bad0bf3":"markdown","f0030697":"markdown","43c27728":"markdown","3654d4ce":"markdown","2d1ab884":"markdown","6c6ec56e":"markdown","556ec65e":"markdown","df1814e6":"markdown","a38656c2":"markdown","3bba29ed":"markdown","7200e462":"markdown","2244221b":"markdown","2c98d733":"markdown","f005e822":"markdown","876dd667":"markdown","f9d8a05a":"markdown","08e877a4":"markdown","ef068596":"markdown","66a22de7":"markdown","88de54f9":"markdown","db83d717":"markdown","5981db33":"markdown","5a2d71f4":"markdown","aab37257":"markdown"},"source":{"3047735e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5dae56fe":"# Loading and checking data\nraw_data = pd.read_csv(\"..\/input\/employee-absenteeism-prediction\/Absenteeism-data.csv\")\nraw_data.head()","3af02efb":"# Creating checkpoint\ndf = raw_data.copy()\ndf.head()","fbdb4e47":"# Checking the shape of dataset\ndf.shape","fc4cd2e1":"# Checking info\ndf.info()","8294ac82":"# Checking the statistical summary of dataset\ndf.describe()","9dff4958":"#We can make a simple EDA with \"pandasprofiling\".\nfrom pandas_profiling import ProfileReport\ndf_profile = ProfileReport(df, title='Pandas Profiling Report', html={'style':{'full_width':True}})\ndf_profile","99b3d2bb":"# Outlier detection\ndef detect_outlier(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indices\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","f615e512":"df.columns.values","e1b49507":"from collections import Counter\ndf.loc[detect_outlier(df,['Transportation Expense','Distance to Work', 'Age', 'Daily Work Load Average',\n                              'Body Mass Index', 'Children', 'Pets'])]","f787625a":"# To check the missing values\ndf.isnull().sum()","b8029584":"# ID variable is a number that is there to distinguish the individuals from one another, not to carry any numeric information.\n# We should drop it. \ndf = df.drop([\"ID\"], axis = 1) # axis=1 for columns, axis=0 for rows.\ndf.head()","b4445cc6":"# Checking Reason for Absence feature.\nprint(df[\"Reason for Absence\"].max())\nprint(df[\"Reason for Absence\"].min())\nprint(df[\"Reason for Absence\"].unique())","839e379d":"len(df[\"Reason for Absence\"].unique())","2dc44b20":"sorted(df[\"Reason for Absence\"].unique())","8f4cfae8":"# Convert categorical variable into dummy variables.\nreason_columns = pd.get_dummies(df[\"Reason for Absence\"])\nreason_columns","d35327fe":"reason_columns[\"Check\"] = reason_columns.sum(axis=1)\nreason_columns","2f3ff45c":"reason_columns[\"Check\"].sum(axis=0)","4671d6f6":"reason_columns[\"Check\"].unique()","9061254a":"reason_columns = reason_columns.drop([\"Check\"], axis=1)\nreason_columns","ac806ab6":"# To avoid potencial multicollinearity issues\nreason_columns = pd.get_dummies(df[\"Reason for Absence\"], drop_first = True)\nreason_columns","6bc49648":"df.columns.values","ce0527b4":"reason_columns.columns.values","69612454":"df = df.drop([\"Reason for Absence\"], axis=1)","04692ed8":"reason_type_1 = reason_columns.loc[:,1:14].max(axis = 1)\nreason_type_2 = reason_columns.loc[:,15:17].max(axis = 1)\nreason_type_3 = reason_columns.loc[:,18:21].max(axis = 1)\nreason_type_4 = reason_columns.loc[:,22:28].max(axis = 1)","824e55fb":"# Concatenate column values\ndf = pd.concat([df, reason_type_1, reason_type_2, reason_type_3, reason_type_4], axis = 1)","dc4e6996":"df.head()","086bae0d":"# Rename columns\ndf.columns.values","fa23e7ed":"column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1',\n       'Reason_2', 'Reason_3', 'Reason_4']","7f75f976":"df.columns = column_names","6806d1f2":"df.head()","0e2ce2e8":"# Reorder columns\ncolumn_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4',\n                          'Date', 'Transportation Expense', 'Distance to Work', 'Age',\n                          'Daily Work Load Average', 'Body Mass Index', 'Education',\n                          'Children', 'Pets', 'Absenteeism Time in Hours']","c8b0e4d7":"df = df[column_names_reordered]","ff3f9c76":"df.head()","67fa4ef1":"# Create a checkpoint\ndf_reason_mod = df.copy()","c56ba994":"# Analysis of Date feature\ndf_reason_mod[\"Date\"]","43b84804":"# Checking type of Date variable\ntype(df_reason_mod[\"Date\"][0])","28eaa7c0":"# Converting datatype to datetime\ndf_reason_mod[\"Date\"] = pd.to_datetime(df_reason_mod[\"Date\"], format = \"%d\/%m\/%Y\")","b735db42":"type(df_reason_mod[\"Date\"][0])","6fe4d4f4":"df_reason_mod.info()","17a9dab8":"# Extracting month value\ndf_reason_mod[\"Date\"][0].month","0a7d09af":"# Creating month variable\nlist_months = []\n\nfor i in range(df_reason_mod.shape[0]):\n    list_months.append(df_reason_mod[\"Date\"][i].month)","29369943":"len(list_months)","33079616":"df_reason_mod[\"Month Value\"] = list_months","bb734f4e":"df_reason_mod.head()","8f38f3a2":"# Extract the day of the week\ndf_reason_mod[\"Date\"][699].weekday()","e33f7918":"def date_to_weekday(date_value):\n    return date_value.weekday()","793c4120":"# Creating Day of the Week variable\ndf_reason_mod[\"Day of the Week\"] = df_reason_mod[\"Date\"].apply(date_to_weekday)","d624214c":"df_reason_mod.head()","8689e9ac":"df_reason_mod = df_reason_mod.drop([\"Date\"], axis=1)","3f14b547":"df_reason_mod.head()","2ef41919":"# Analysis other columns\ntype(df_reason_mod[\"Transportation Expense\"][0])","0b0c2e8d":"type(df_reason_mod[\"Distance to Work\"][0])","0c6f7e1b":"type(df_reason_mod[\"Age\"][0])","3cdf7f99":"type(df_reason_mod[\"Daily Work Load Average\"][0])","d0cdb6f7":"type(df_reason_mod[\"Body Mass Index\"][0])","92d633fd":"df_reason_mod[\"Education\"].unique()","912c8b82":"df_reason_mod[\"Education\"].value_counts()","a86af41a":"# Converting Education Feature\ndf_reason_mod[\"Education\"] = df_reason_mod[\"Education\"].map({1:0, 2:1, 3:1, 4:1})","ef8ad8f3":"df_reason_mod[\"Education\"].value_counts()","9a163d07":"# Final checkpoint\ndf_preprocessed = df_reason_mod.copy()","8ff7dd8a":"# Creating the targets\ndf_preprocessed[\"Absenteeism Time in Hours\"].median()","b2902d42":"targets = np.where(df_preprocessed[\"Absenteeism Time in Hours\"] > 3,1,0)","b6493456":"targets","e5b284d0":"df_preprocessed[\"Excessive Absenteeism\"] = targets","58e7d327":"df_preprocessed.head()","c0f9c5ce":"# Checking imbalance\ntargets.sum()\/targets.shape[0]","e1985ddd":"# create a checkpoint by dropping the unnecessary variables\n# also drop the variables we 'eliminated' after exploring the weights.\n# after implementing the ml model we also add 3 more columns to remove list because they have very less affect on target feature.\ndf_with_targets = df_preprocessed.drop(['Absenteeism Time in Hours','Day of the Week',\n                                        'Daily Work Load Average','Distance to Work'],axis=1)","5b92faac":"df_with_targets is df_preprocessed","eb0b9dc7":"df_with_targets.head()","d19d2fdc":"# Selecting input for the regression\ndf_with_targets.shape","eb45bb84":"unscaled_inputs = df_with_targets.iloc[:,:-1]","9c193c5b":"# import the libraries needed to create the Custom Scaler\n# note that all of them are a part of the sklearn package\n# moreover, one of them is actually the StandardScaler module, \n# so you can imagine that the Custom Scaler is build on it\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# create the Custom Scaler class\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    # init or what information we need to declare a CustomScaler object\n    # and what is calculated\/declared as we do\n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        \n        # scaler is nothing but a Standard Scaler object\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        # with some columns 'twist'\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    # the fit method, which, again based on StandardScale\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    # the transform method which does the actual scaling\n\n    def transform(self, X, y=None, copy=None):\n        \n        # record the initial order of the columns\n        init_col_order = X.columns\n        \n        # scale all features that you chose when creating the instance of the class\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        \n        # declare a variable containing all information that was not scaled\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        \n        # return a data frame which contains all scaled features and all 'not scaled' features\n        # use the original order (that you recorded in the beginning)\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","087a5f88":"# check what are all columns that we've got\nunscaled_inputs.columns.values","ba25d217":"# create the columns to scale, based on the columns to omit\n# use list comprehension to iterate over the list\ncolumns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']\ncolumns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","841707a2":"# declare a scaler object, specifying the columns you want to scale\nabsenteeism_scaler = CustomScaler(columns_to_scale)","0e78ce6c":"# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \nabsenteeism_scaler.fit(unscaled_inputs)","9b6a73be":"# standardizes the data, using the transform method \n# in the last line, we fitted the data - in other words\n# we found the internal parameters of a model that will be used to transform data. \n# transforming applies these parameters to our data\n# note that when you get new data, you can just call 'scaler' again and transform it in the same way as now\nscaled_inputs = absenteeism_scaler.transform(unscaled_inputs)","08455ed8":"# the scaled_inputs are now an ndarray, because sklearn works with ndarrays\nscaled_inputs","e9455769":"scaled_inputs","6aa14e13":"# check the shape of the inputs\nscaled_inputs.shape","2546458f":"# Split the data\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, train_size = 0.8, random_state = 42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","79a40554":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression","5c5e6f19":"reg = LogisticRegression()","d9af6530":"reg.fit(x_train, y_train)","eb78a0d7":"reg.score(x_train, y_train)","8c92180c":"# Manually check the accuracy\nmodel_outputs = reg.predict(x_train)\nnp.sum(model_outputs == y_train)\/(model_outputs.shape)","df80494d":"# Finding the intercept and coefficients\nreg.intercept_","5030ccc4":"reg.coef_","99b3d680":"unscaled_inputs.columns.values","29f0a7da":"feature_name = unscaled_inputs.columns.values","687e876d":"summary_table = pd.DataFrame(columns=[\"feature_name\"], data=feature_name)\nsummary_table[\"Coefficient\"] = np.transpose(reg.coef_)","6e226079":"summary_table","ecc9ec87":"summary_table.index = summary_table.index +1","3e6b568f":"summary_table","6ccb5d83":"summary_table.loc[0] = [\"Intercept\", reg.intercept_[0]]","529554d4":"summary_table = summary_table.sort_index()\nsummary_table","b92f58f4":"summary_table[\"Odds_ratio\"] = np.exp(summary_table.Coefficient)","3fd0d1a3":"summary_table","f9c33d5f":"summary_table.sort_values(\"Odds_ratio\", ascending=False)","e316070f":"# Testing the model\nreg.score(x_test, y_test)","5b7c1701":"predicted_proba = reg.predict_proba(x_test)\npredicted_proba","1f945b8a":"predicted_proba[:,1]","6cbf177b":"## <a id='5.'> 5. Preprocessing<\/a>","0bad0bf3":"- 0 : missing value, \n- 1: single value, \n- 2,3,4...: Not possible because there can only be one reason for absence.","f0030697":"## <a id='5.3.2.'> 5.3.2. Checking Date Feature<\/a>","43c27728":"## <a id='5.3.4.'> 5.3.4. Creating Target Variable<\/a>","3654d4ce":"- 20 is missing in \"Reason for Absence\".\n- \"Reason for Absence\" feature is categorical. We can get dummies for this feature but first we must be sure that every empoyee has only one \"Reason for Absence\". ","2d1ab884":"There is no outliers.","6c6ec56e":"## <a id='7.'> 7. References<\/a>\n\n* https:\/\/www.udemy.com\/course\/the-data-science-course-complete-data-science-bootcamp\/\n* https:\/\/365datascience.com\/","556ec65e":"There are no missing values in the dataset.","df1814e6":"## <a id='5.2.'> 5.2. Dropping Useless Variables<\/a>","a38656c2":"## <a id='1.'> 1. Importing Libraries<\/a>","3bba29ed":"One number is missing.","7200e462":"## <a id='3.'> 3. Variable Description<\/a>","2244221b":"- Classifying Reason for Absence\n- 1-14: Desease\n- 15-17: Pregnancy\n- 18-21: Emergency issues\n- 22-28: Light reasons ","2c98d733":"## <a id='4.'> 4. Data Analysis<\/a>","f005e822":"* ID: Identification number of the employees (categorical)  \n* Reason for Absence: Reason for the absenteeism of employees (categorical)  \n* Date: Date of the absenteeism time of the employees (categorical) \n* Transportation Expense: The amount of transportaion expense of the employees  (numerical)\n* Distance to Work: Distance to work for every employee (numerical)  \n* Age: Age of the employees (numerical)  \n* Daily Work Load Average:  Daily work load average of the employees (numerical)\n* Body Mass Index: Body index of the employees (numerical)  \n* Education: Education levels (categorical)   \n* Children: Children number (numerical)  \n* Pets: Pet number (numerical)                      \n* Absenteeism Time in Hours:  Daily absenteeism time of the employees \n \ndtypes: float64(1), int64(10), object(1)","876dd667":"## <a id='5.3.3.'> 5.3.3. Checking Other Features<\/a>","f9d8a05a":"## <a id='5.3.'> 5.3. Feature Engineering<\/a>","08e877a4":"# Content:\n\n- <a href='#1.'> 1. Importing Libraries<\/a>\n- <a href='#2.'> 2. Loading and Checking Data<\/a>\n- <a href='#3.'> 3. Variable Description<\/a>\n- <a href='#4.'> 4. Data Analysis<\/a>\n- <a href='#5.'> 5. Preprocessing<\/a>\n    - <a href='#5.1.'> 5.1. Outlier Detection and Missing Values<\/a>\n    - <a href='#5.2.'> 5.2. Dropping Useless Variables<\/a>\n    - <a href='#5.3.'> 5.3. Feature Engineering<\/a>\n        - <a href='#5.3.1.'> 5.3.1. Checking Reason for Absence Feature<\/a>\n        - <a href='#5.3.2.'> 5.3.2. Checking Date Feature<\/a>\n        - <a href='#5.3.3.'> 5.3.3. Checking Other Features<\/a>\n        - <a href='#5.3.4.'> 5.3.4. Creating Target Variable<\/a>\n    - <a href='#5.4.'> 5.4. Scaling and Splitting<\/a>\n- <a href='#6.'> 6. Single Logistic Regression and Evaluation<\/a>\n- <a href='#7.'> 7. References<\/a>","ef068596":"## <a id='5.3.1.'> 5.3.1. Checking Reason for Absence Feature<\/a>","66a22de7":"## <a id='6.'> 6. Single Logistic Regression and Evaluation<\/a>","88de54f9":"## <a id='5.4.'> 5.4. Scaling and Splitting<\/a>","db83d717":"## <a id='5.1.'> 5.1. Outlier Detection and Missing Values<\/a>","5981db33":"Group the Reasons for Absence","5a2d71f4":"## <a id='2.'> 2. Loading and Checking Data<\/a>","aab37257":"- Absenteeism Time in Hours =<3, Moderately absent\n- Absenteeism Time in Hours >3, Excessively absent"}}