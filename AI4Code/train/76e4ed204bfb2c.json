{"cell_type":{"5cc80f1b":"code","66beff0d":"code","66e344e8":"code","3371395b":"code","8d847f49":"code","769bfaf7":"code","379721c8":"code","95482c81":"markdown"},"source":{"5cc80f1b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","66beff0d":"import pandas as pd\nimport numpy as np\nimport json\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport time\n\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom functools import partial\nfrom sklearn.model_selection import KFold","66e344e8":"train = pd.read_csv('\/kaggle\/input\/gamma-log-facies\/CAX_LogFacies_Train_File.csv')\nprint(train.shape)\ntrain.head()","3371395b":"X_train = pd.pivot_table(train, values='GR', index=['well_id'], columns=['row_id'])\ny_train = pd.pivot_table(train, values='label', index=['well_id'], columns=['row_id'])\nX_train.shape, y_train.shape","8d847f49":"class EarlyStopping:\n    def __init__(self, patience=5, delta=0, checkpoint_path='checkpoint.pt', is_maximize=True):\n        self.patience, self.delta, self.checkpoint_path = patience, delta, checkpoint_path\n        self.counter, self.best_score = 0, None\n        self.is_maximize = is_maximize\n\n    def load_best_weights(self, model):\n        model.load_state_dict(torch.load(self.checkpoint_path))\n\n    def __call__(self, score, model):\n        if self.best_score is None or \\\n        (score > self.best_score + self.delta if self.is_maximize else score < self.best_score - self.delta):\n            torch.save(model.state_dict(), self.checkpoint_path)\n            self.best_score, self.counter = score, 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n    \nclass TabularDataset(Dataset):\n\n    def __init__(self, df_x, df_y, is_test=False):\n        self.x = df_x\n        self.y = df_y\n        self.n = df_x.shape[0]\n        self.is_test=is_test\n\n    def __len__(self): return self.n\n\n    def __getitem__(self, idx): \n        if not self.is_test:\n            return [self.x[idx].astype(np.float32), self.y[idx].astype(np.int64)]\n        else:\n            return [self.x[idx].astype(np.float32), self.y[idx]]\n            \n\n    \nclass Seq2SeqRnn(nn.Module):\n    def __init__(self, input_size, seq_len, hidden_size, output_size, num_layers=1, bidirectional=False, dropout=.3,\n            hidden_layers = [100, 200]):\n        \n        super().__init__()\n        self.input_size = input_size\n        self.seq_len = seq_len\n        self.hidden_size = hidden_size\n        self.num_layers=num_layers\n        self.bidirectional=bidirectional\n        self.output_size=output_size\n        \n        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, \n                           bidirectional=bidirectional, batch_first=True,dropout=.3)\n        \n         # Input Layer\n        if hidden_layers and len(hidden_layers):\n            first_layer  = nn.Linear(hidden_size*2 if bidirectional else hidden_size, hidden_layers[0])\n\n            # Hidden Layers\n            self.hidden_layers = nn.ModuleList(\n                [first_layer]+[nn.Linear(hidden_layers[i], hidden_layers[i+1]) for i in range(len(hidden_layers) - 1)]\n            )\n            for layer in self.hidden_layers: nn.init.kaiming_normal_(layer.weight.data)   \n\n            # output layers\n            self.output_layer = nn.Linear(hidden_layers[-1], output_size)\n            nn.init.kaiming_normal_(self.output_layer.weight.data) \n           \n        else:\n            self.hidden_layers = []\n            self.output_layer = nn.Linear(hidden_size*2 if bidirectional else hidden_size, output_size)\n            nn.init.kaiming_normal_(self.output_layer.weight.data) \n            \n        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(seq_len) for size in hidden_layers])\n        self.activation_fn = torch.relu\n            \n        self.dropout = nn.Dropout(dropout)\n        self.output_activation_fn = partial(torch.softmax, dim=1)\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        x = x.view(batch_size, self.seq_len, self.input_size) \n        outputs, (hidden, cell) = self.rnn(x)        \n\n        x = self.dropout(self.activation_fn(outputs))\n        for hidden_layer, bn_layer in zip(self.hidden_layers, self.bn_layers):\n            x = self.activation_fn(hidden_layer(x))\n#             x = bn_layer(x)\n            x = self.dropout(x)\n        \n        x = self.output_layer(x)\n#         x = self.output_activation_fn(x)\n        return x","769bfaf7":"folds = KFold(n_splits=5, random_state=100, shuffle=True)\nindices= [(train_index, test_index) for (train_index, test_index) in folds.split(X_train.index)]\ntrain_index, val_index = indices[2]\nlen(train_index), len(val_index)","379721c8":"for index, (train_index, val_index ) in enumerate(indices):\n    print(\"Fold : {}\".format(index))\n    \n    batchsize = 16\n    train_dataset = TabularDataset(df_x=X_train.iloc[train_index].values,  df_y=y_train.iloc[train_index].values)\n    train_dataloader = DataLoader(train_dataset, batchsize, shuffle=True, num_workers=1)\n\n    valid_dataset = TabularDataset(df_x=X_train.iloc[val_index].values,  df_y=y_train.iloc[val_index].values)\n    valid_dataloader = DataLoader(valid_dataset, batchsize, shuffle=True, num_workers=1)\n\n    # test_dataset = TabularDataset(df_x=X_test.values, df_y=X_test_ids.values, is_test=True)\n    # test_dataloader = DataLoader(test_dataset, batchsize, shuffle=False, num_workers=1)\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model=Seq2SeqRnn(input_size=1, seq_len=1100, hidden_size=256, output_size=5, num_layers=3, hidden_layers=[1024],\n                     bidirectional=True).to(device)\n\n    no_of_epochs = 100\n    early_stopping = EarlyStopping(patience=10, is_maximize=True, checkpoint_path=\"checkpoint_{}.pt\".format(index))\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    schedular = torch.optim.lr_scheduler.OneCycleLR(optimizer=optimizer, max_lr=0.001, epochs=no_of_epochs,\n                                            steps_per_epoch=len(train_dataloader))\n    avg_train_losses, avg_valid_losses = [], [] \n\n\n    for epoch in range(no_of_epochs):\n        start_time = time.time()\n\n        print(\"Epoch : {}\".format(epoch))\n\n        train_losses, valid_losses = [], []\n\n        model.train() # prep model for training\n        train_preds, train_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n\n        for x, y in train_dataloader:          \n            x = x.to(device)\n            y = y.to(device)\n\n            optimizer.zero_grad()\n            predictions = model(x)\n\n            predictions_ = predictions.view(-1, predictions.shape[-1]) \n            y_ = y.view(-1)\n\n            loss = criterion(predictions_, y_)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            schedular.step()\n            # record training loss\n            train_losses.append(loss.item())\n\n            train_true = torch.cat([train_true, y_], 0)\n            train_preds = torch.cat([train_preds, predictions_], 0)\n\n\n\n        model.eval() # prep model for evaluation\n        val_preds, val_true = torch.Tensor([]).to(device), torch.LongTensor([]).to(device)\n        for x, y in valid_dataloader:\n            x = x.to(device)\n            y = y.to(device)\n\n            predictions = model(x)\n            predictions_ = predictions.view(-1, predictions.shape[-1]) \n            y_ = y.view(-1)\n\n            loss = criterion(predictions_, y_)\n            valid_losses.append(loss.item())\n\n            val_true = torch.cat([val_true, y_], 0)\n            val_preds = torch.cat([val_preds, predictions_], 0)\n\n\n        # calculate average loss over an epoch\n        train_loss = np.average(train_losses)\n        valid_loss = np.average(valid_losses)\n        avg_train_losses.append(train_loss)\n        avg_valid_losses.append(valid_loss)\n\n        print( \"train_loss: {}, valid_loss: {}\".format(train_loss, valid_loss))\n\n        train_score = accuracy_score(train_preds.cpu().detach().numpy().argmax(1), train_true.cpu().detach().numpy())\n\n        val_score = accuracy_score(val_preds.cpu().detach().numpy().argmax(1), val_true.cpu().detach().numpy())\n        print( \"train_acc: {}, valid_acc: {}\".format(train_score, val_score))\n\n        if early_stopping(val_score, model):\n            print(\"Early Stopping...\")\n            print(\"Best Val Score: {}\".format(early_stopping.best_score))\n            break\n\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n    ","95482c81":"**Gamma Log Facies Type Prediction - Sequence Labeling**\n\n\n*https:\/\/www.crowdanalytix.com\/contests\/gamma-log-facies-type-prediction*\n"}}