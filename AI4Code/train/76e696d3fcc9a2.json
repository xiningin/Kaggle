{"cell_type":{"2080d48e":"code","eb96bef6":"code","3df8928c":"code","4bd45123":"code","eb32e659":"code","6cf08635":"code","6886a3ef":"code","5cf4a364":"code","3db51b1f":"code","af58ebcd":"code","94f5a7e1":"code","dce7f1d7":"code","65740fde":"markdown","2b823db2":"markdown","f11068af":"markdown","32f71c85":"markdown"},"source":{"2080d48e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb96bef6":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.mixture import GaussianMixture\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nimport optuna\n\ntrain = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n","3df8928c":"cat_features = [col for col in train.columns if train[col].dtype == object]\ngood_cat_features = [\n    col for col in cat_features if set(X_test[col]).issubset(set(train[col]))\n]\nbad_cat_features = list(set(cat_features) - set(good_cat_features))\n\nnum_features = [\n    col for col in train.columns if train[col].dtype != object and\n    col != 'id' and col != 'target'\n]\n\nmissing_val_count_by_column = (train.isnull().sum())\n\nprint(\n    'Cat', cat_features, 'Bad Cat', bad_cat_features,\n    'Num', num_features,\n    'Missing', missing_val_count_by_column[missing_val_count_by_column > 0]\n)","4bd45123":"#Plotting correlation\nplt.figure(figsize=(12,12))\ncorr_heatmap = sns.heatmap(\n    train[num_features + ['target']].corr(), \n    annot=True, cmap=\"GnBu\"\n)","eb32e659":"fig, ax = plt.subplots(5, 3, figsize=(14, 24))\nfor i, feature in enumerate(num_features):\n    plt.subplot(5, 3, i + 1)\n    #sns.histplot(train[feature], kde=True, bins=100)\n    sns.kdeplot(train[feature], color='Green')\n    sns.kdeplot(X_test[feature], color='Red')\n    ax[i \/\/ 3, i % 3].legend(['Train', 'Test'], facecolor='White')\n    # ax[i \/\/ 3, i % 3].set_title(ft +' ks stat :' +str(np.round(ks_score, 3)))\n    plt.xlabel(feature, fontsize=9)\nplt.show()","6cf08635":"def add_gmm(\n    train_df, test_df, numerics, dists\n):\n    gmms = []\n    for feature, dist in zip(numerics, dists):\n        X_ = np.array(\n            train_df[[feature]].append(test_df[[feature]])[feature].tolist()\n        ).reshape(-1, 1)\n        gmm_ = GaussianMixture(\n            n_components=dist,\n            init_params='kmeans',\n            random_state=0\n        ).fit(X_)\n        gmms.append(gmm_)\n        preds = gmm_.predict(X_)\n        clus = pd.get_dummies(preds).values\n        clus_feats = [f'{feature}_gmm_{i}'for i in range(clus.shape[1])]\n        train_df[f'{feature}_gmm'] = preds[:len(train_df)]\n        train_df[clus_feats] = clus[:len(train_df), :]\n        test_df[f'{feature}_gmm'] = preds[len(train_df):]\n        test_df[clus_feats] = clus[len(train_df):, :]\n        clus_cols = [feature, f'{feature}_gmm']\n        mu = train_df[clus_cols].append(test_df[clus_cols]).groupby(\n            f'{feature}_gmm'\n        )[feature].transform(\"mean\")\n        sigma = train_df[clus_cols].append(test_df[clus_cols]).groupby(\n            f'{feature}_gmm'\n        )[feature].transform(\"std\")\n        train_df[f'{feature}_gmm_dev'] = (\n            train_df[feature] - mu[:len(train_df)]\n        ) \/ sigma[:len(train_df)]\n        X_test[f'{feature}_gmm_dev'] = (\n            X_test[feature] - mu[len(train_df):]\n        ) \/ sigma[len(train_df):]\n\n\nadd_gmm(\n    train, X_test, num_features,\n    [8, 10, 6, 9, 5, 10, 8, 8, 9, 8, 9, 10, 9, 6]\n)","6886a3ef":"# Add PCA features\ndef add_pca(\n    train_df, test_df, cols, n_comp=20, prefix='pca_'\n):    \n    pca = PCA(n_components=n_comp, random_state=42)\n    pca_titles = [prefix + '_pca_' + str(x) for x in range(n_comp)]\n    \n    #create copies to fill nas as needed\n    temp_train = train_df.copy()\n    temp_test = test_df.copy()\n    \n    for c in cols:\n        fv = temp_train[c].mean()\n        temp_train[c] = temp_train[c].fillna(value=fv)\n        \n        fv = temp_test[c].mean()\n        temp_test[c] = temp_test[c].fillna(value=fv)\n    \n    for p in pca_titles:\n        # we update the actual original dsf\n        train_df[p] = 0.0\n        test_df[p] = 0.0\n    \n    pca_data = pd.concat([temp_train[cols], temp_test[cols]], axis=0)\n    # fit and transform on the cleaned data with no NANs\n    pca_data = pca.fit_transform(pca_data)\n        \n    train_df.loc[:, pca_titles] = pca_data[0:len(train_df)]\n    test_df.loc[:, pca_titles] = pca_data[len(train_df):]\n    return pca_titles\n\n\npca_titles = add_pca(train, X_test, num_features, n_comp=2, prefix='extra')\n\nfig,axes=plt.subplots(nrows=1, ncols=3, figsize=(20,7))\naxes[0].scatter(\n    x=train[pca_titles[0]], y=train[pca_titles[1]], c=train['target'],\n    s=1, alpha=0.8, cmap='seismic', vmax=9,vmin=6\n)\naxes[1].scatter(\n    x=train[pca_titles[0]], y=train['target'], s=1, alpha=0.8, color='Green'\n)\naxes[2].scatter(\n    x=train[pca_titles[1]], y=train['target'], s=1, alpha=0.8, color='Green'\n)\n\naxes[0].set_title('PCA1 vs PCA2 (color=Target)')\naxes[1].set_title('PCA1 vs Target')\naxes[2].set_title('PCA2 vs Target')\n\nplt.tight_layout()\n","5cf4a364":"for feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    X_test[feature] = le.transform(X_test[feature])\n    \nX = train.drop(columns='target')\ny = train['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, train_size=0.8, test_size=0.2\n)\nX.head()","3db51b1f":"def objective(trial):\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"RMSE\", \"MultiRMSE\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"8gb\",\n    }\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    gbm = CatBoostRegressor(**param)\n    gbm.fit(\n        X_train, y_train,\n        cat_features=cat_features,\n        eval_set=[(X_valid, y_valid)],\n        verbose=500, early_stopping_rounds=100\n    )\n    preds = gbm.predict(X_valid)\n    return np.sqrt(MSE(y_train, gbm.predict(X_train)))\n\n\n#study = optuna.create_study(direction=\"minimize\")\n#study.optimize(objective, n_trials=100, timeout=600)\n\n#print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nmodel_cat = CatBoostRegressor(\n    #**study.best_trial.params\n)\nmodel_cat.fit(\n    X_train, y_train,\n    cat_features=cat_features,\n    eval_set=(X_valid, y_valid),\n    verbose=500\n)\nprint(\n    'Train rmse for CatBoost:',\n    np.sqrt(MSE(y_train, model_cat.predict(X_train))),\n    'Validation rmse for CatBoost:',\n    np.sqrt(MSE(y_valid, model_cat.predict(X_valid)))\n    \n)","af58ebcd":"params = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1023,\n    'min_data_in_leaf':10,\n    'feature_fraction': 0.7,\n    'learning_rate': 0.01,\n    'num_rounds': 2000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\nmodel_lgbm = lgbm.train(\n    params=params,\n    train_set=lgbm.Dataset(X_train, y_train),\n    valid_sets=(lgbm.Dataset(X_train, y_train), lgbm.Dataset(X_valid, y_valid)),\n    verbose_eval=50, categorical_feature=cat_features\n)\nprint(\n    'Train rmse for CatBoost:',\n    np.sqrt(MSE(y_train, model_lgbm.predict(X_train))),\n    'Validation rmse for CatBoost:',\n    np.sqrt(MSE(y_valid, model_lgbm.predict(X_valid)))\n)","94f5a7e1":"model_xgb = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42\n)\nmodel_xgb.fit(\n    X_train, y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_valid, y_valid)], \n    verbose=50, \n    early_stopping_rounds=10\n)\nprint(\n    'Train rmse for XGBoost:',\n    np.sqrt(MSE(y_train, model_xgb.predict(X_train))),\n    'Validation rmse for XGBoost:',\n    np.sqrt(MSE(y_valid, model_xgb.predict(X_valid)))\n)","dce7f1d7":"# Fill in the line below: get test predictions\npreds_test = model_cat.predict(X_test)\n\noutput = pd.DataFrame({'id': X_test.id,\n                       'target': preds_test})\noutput.to_csv('submission_cat.csv', index=False)","65740fde":"## Prepare data for training and validation","2b823db2":"## CatBoost","f11068af":"## Check missing and unknown cat values in test","32f71c85":"## Imports and input"}}