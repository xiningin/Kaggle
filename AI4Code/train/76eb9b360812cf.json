{"cell_type":{"01f381fa":"code","34e42027":"code","3c4434f5":"code","17bb08ba":"code","54dd2d55":"code","362d4c11":"code","c201ef2e":"code","283a04a4":"code","310061fc":"code","567270bd":"code","f2cb27ce":"code","2bca0fba":"code","b8d49fda":"code","c6c8c9ec":"code","8fbd1b1d":"code","a4651de4":"code","682b6f12":"code","4fdcdf2e":"code","da762e91":"code","09ef569e":"code","aa2e33f6":"code","8dbcee69":"code","4b96ef10":"code","e9d1e826":"code","8a053b39":"code","39c3ad47":"code","ce510d6a":"code","f319cec4":"code","d6ffde1d":"code","36c0728e":"code","e927f309":"code","1af75028":"code","a02256af":"code","b085718a":"code","05f85322":"code","ee62424f":"markdown","30d1e371":"markdown","7ac8d876":"markdown","57594bdb":"markdown","4ef506c3":"markdown","e4daed71":"markdown","3bef3de5":"markdown","4a253cce":"markdown","1fceb0dd":"markdown","6a25a1b4":"markdown","f549b4e4":"markdown","d89bbdd6":"markdown","aca538c1":"markdown","ee56f9ba":"markdown","5241409e":"markdown","895cbd28":"markdown","10e36296":"markdown","adc7a112":"markdown","a0246018":"markdown","ec411514":"markdown","4ad8f6a4":"markdown","4395cae8":"markdown","5b1c2af1":"markdown"},"source":{"01f381fa":"import warnings; warnings.filterwarnings(\"ignore\")\nimport numpy as np,pandas as pd,keras as ks\nfrom time import time\nfrom IPython.display import display\nimport pylab as pl,seaborn as sn\npl.style.use('seaborn-bright')\nfw='weights.titanic.hdf5'\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import accuracy_score,f1_score,make_scorer\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV,train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Activation\nfrom keras.layers import Flatten,BatchNormalization\nfrom keras.layers import Convolution1D,MaxPooling1D\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau","34e42027":"train_data=pd.read_csv('..\/input\/train.csv')\ntest_data=pd.read_csv('..\/input\/test.csv')\nfull_data=train_data.append(test_data,ignore_index=True)\nprint (\"The Titanic dataset has {} data points with {} variables.\"\\\n       .format(*full_data.shape))\nprint (\"Examples of Data Points: \")\ndisplay(test_data[:3].T)","3c4434f5":"outcomes=train_data['Survived']\nall_features=train_data.drop('Survived',axis = 1)\nprint (\"The percentage of survived passengers: {:.2f}%\"\\\n       .format(100*outcomes.mean()))","17bb08ba":"def accuracy_score_0(truth, pred):         \n    if len(truth)==len(pred):         \n        return (\"Predictions have an accuracy of {:.2f}%.\"\\\n                .format((truth == pred).mean()*100))    \n    else:\n        return (\"Number of predictions does not match number of outcomes!\")","54dd2d55":"def predictions_0(data):\n    \"\"\" Model with no features. Always predicts a passenger did not survive. \"\"\"\n    predictions=[]\n    for _, passenger in data.iterrows():        \n        predictions.append(0)    \n    return pd.Series(predictions)\npredictions=predictions_0(all_features) \naccuracy_score_0(outcomes,predictions)","362d4c11":"number_by_sex=\\\npd.Series(train_data.groupby('Sex').count()['PassengerId'])\nsurvived_by_sex=\\\npd.Series(train_data.groupby('Sex').sum()['Survived'])\ndef percent_xy(x,y):\n    return round(100.*x\/y,2)\nnumber_by_sex_in_per=\\\npd.Series(percent_xy(number_by_sex,len(train_data)))\nsurvived_by_sex_in_per1=\\\npd.Series(percent_xy(survived_by_sex,outcomes.sum()))\nsurvived_by_sex_in_per2=\\\npd.Series(percent_xy(survived_by_sex,number_by_sex))\ndd={'Number by sex':number_by_sex,\n    'Number by sex in percentages':number_by_sex_in_per,\n    'Survived by sex':survived_by_sex,\n    'Survived by sex in percentages I':survived_by_sex_in_per1,\n    'Survived by sex in percentages II':survived_by_sex_in_per2}\nsex_data=pd.DataFrame(dd); sex_data","c201ef2e":"fig=pl.figure(figsize=(12,6)); ax=fig.add_subplot(111)\nind=np.array([1,2]); width=.2\nrects1=ax.bar(ind,number_by_sex_in_per,width,alpha=.7)\nrects2=ax.bar(ind+width,survived_by_sex_in_per1,width,alpha=.7)\nrects3=ax.bar(ind+2*width,survived_by_sex_in_per2,width,alpha=.7)\nax.set_xlim(-width+1,len(ind)+width+.6); ax.set_ylim(0,90)\nax.set_ylabel('Values by sex in percentages')\nax.set_title('Passengers, Statistics by Sex in Percentages',\n             fontsize=20)\nxTickMarks=['female','male']; ax.set_xticks(ind+width)\nxtickNames=ax.set_xticklabels(xTickMarks)\npl.setp(xtickNames,rotation=0,fontsize=30)\nax.legend((rects1[0],rects2[0],rects3[0]), \n          ('Number by sex in percentages', \n           'Survived by sex in percentages I', \n           'Survived by sex in percentages II'));","283a04a4":"def predictions_1(data):\n    \"\"\" Model with one feature: \n            - Predict a passenger survived if they are female. \"\"\"    \n    predictions=[]\n    for _, passenger in data.iterrows():\n        if passenger['Sex']=='female':\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return pd.Series(predictions)\npredictions1=predictions_1(all_features) \naccuracy_score_0(outcomes,predictions1)","310061fc":"male=train_data[train_data.Sex=='male']\nmale.Age.hist(alpha=.7)\nmale[male.Survived==1].Age.hist(figsize=(12,6),alpha=.7)\npl.xlabel(\"Age\"); pl.ylabel(\"Number by age\")\npl.title(\"Male Passengers, Statistics by Age\",fontsize=20)\npl.legend([\"Number of Male Passengers\", \n           \"Survived Male Passengers by Age\"]);","567270bd":"def predictions_2(data):\n    \"\"\" Model with two features: \n            - Predict a passenger survived if they are female.\n            - Predict a passenger survived if they are male and younger than 10. \"\"\"    \n    predictions=[]\n    for _, passenger in data.iterrows():\n        if passenger['Sex']=='female':\n            predictions.append(1)\n        elif passenger['Sex']=='male' and passenger['Age']<10:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return pd.Series(predictions)\npredictions2=predictions_2(all_features) \naccuracy_score_0(outcomes,predictions2)","f2cb27ce":"young_male=male[male.Age<15]\nsurvived_young_male=young_male[young_male.Survived==1]\npl.figure(figsize=(12,6))\npl.hist((young_male.Pclass,survived_young_male.Pclass), \n        bins=range(1,5),rwidth=.7,alpha=.7,align='left')\npl.xticks([1,2,3]); pl.xlabel(\"Plass\")\npl.ylabel(\"Number by Pclass\")\npl.title(\"Young Male Passengers, Statistics by Pclass\",fontsize=20)\npl.legend([\"Young Male Passengers\",\n            \"Survived Young Male Passengers\"]);","2bca0fba":"pl.figure(figsize=(12,6))\npl.hist((young_male.SibSp,survived_young_male.SibSp), \n         bins=range(0,7),rwidth=.5,align='left',alpha=.7)\npl.xlabel(\"SibSb\"); pl.ylabel(\"Number by SibSp\")\npl.title(\"Young Male Passengers, Statistics by SibSp\",fontsize=20)\npl.legend([\"Young Male Passengers\",\n           \"Survived Male Young Passengers\"]);","b8d49fda":"def predictions_final(data):\n    \"\"\" Model with multiple features. Makes a prediction with an accuracy of at least 80%. \"\"\"    \n    predictions=[]\n    for _, passenger in data.iterrows():\n        if (passenger['Sex']=='female'):\n            predictions.append(1)                \n        elif passenger['Pclass'] in [1,2] and \\\n        (passenger['Age']<16 or passenger['Age']>75):\n            predictions.append(1)            \n        elif passenger['Age']<15 and passenger['SibSp']<3:\n            predictions.append(1)            \n        else:\n            predictions.append(0)\n    return pd.Series(predictions)\npredictions_final=predictions_final(all_features) \naccuracy_score_0(outcomes,predictions_final)","c6c8c9ec":"print (\"Predictions have an accuracy of {:.2f}%.\"\\\n       .format(accuracy_score(outcomes,predictions_final)*100))\nprint (\"Predictions have an recall score equal to {:.2f}%.\"\\\n       .format(recall_score(outcomes,predictions_final)*100))\nprint (\"Predictions have an precision score equal to {:.2f}%.\"\\\n       .format(precision_score(outcomes,predictions_final)*100))","8fbd1b1d":"n_passengers=len(train_data)\nn_features=len(list(train_data.T.index))\nn_survived=len(train_data[train_data['Survived']==1])\nn_nonsurvived=len(train_data[train_data['Survived']==0])\nprint (\"Total number of passengers: {}\".format(n_passengers))\nprint (\"Number of features: {}\".format(n_features))\nprint (\"Number of passengers who survived: {}\"\\\n       .format(n_survived))\nprint (\"Number of passengers who did not survive: {}\"\\\n       .format(n_nonsurvived))\nprint (\"Persentage of survived passengers: {:.2f}%\"\\\n       .format(100*n_survived\/n_passengers))\nprint (\"\\nFeature columns:\\n{}\"\\\n       .format(list(all_features.columns)))\nprint (\"\\nTarget column: 'Survived'\")","a4651de4":"n_nan_cabin=pd.isnull(all_features['Cabin']).sum()\nprint (\" Persentage of NaN values in 'Cabin': {:.2f}%\"\\\n       .format(100.0*n_nan_cabin\/n_passengers))\nfeature_list2=['Pclass','Sex','Age','SibSp',\n               'Parch','Fare','Embarked']\npre_features=all_features[feature_list2]\ntest_features=test_data[feature_list2]\npre_features.head(11)","682b6f12":"print (\"Number of missing values in the train set\\n\")\nfor feature in feature_list2:\n    print(\"{}:{}\".format(feature,\n                         pd.isnull(pre_features[feature]).sum()))\nprint('')\nprint (\"Number of missing values in the test set\\n\")\nfor feature in feature_list2:\n    print(\"{}:{}\".format(feature,\n                         pd.isnull(test_features[feature]).sum()))","4fdcdf2e":"pre_features2=pre_features.copy()\ntest_features2=test_features.copy()\npre_features2['Sex'].replace({'male':1,'female':0},inplace=True)\ntest_features2['Sex'].replace({'male':1,'female':0},inplace=True)\nage_mean=pd.concat([pre_features2.Age,test_features2.Age],\n                   ignore_index=True).mean()\npre_features2['Age']=pre_features2['Age'].fillna(age_mean)\ntest_features2['Age']=test_features2['Age'].fillna(age_mean)\nfare_mean=pd.concat([pre_features2.Fare,test_features2.Fare],\n                    ignore_index=True).mean()\ntest_features2['Fare']=test_features2['Fare'].fillna(fare_mean)\nmost_common=pd.get_dummies(pre_features2['Embarked'])\\\n            .sum().sort_values(ascending=False).index[0]\npre_features2['Embarked']=pre_features2['Embarked'].fillna(most_common)\npre_features2.head(8).T","da762e91":"pre_features2=pd.get_dummies(pre_features2,prefix=['Embarked'])\ntest_features2=pd.get_dummies(test_features2,prefix=['Embarked'],\n                              columns=['Embarked'])","09ef569e":"pd.isnull(pre_features2).sum(),pd.isnull(test_features2).sum()","aa2e33f6":"def train_classifier(clf,X_train,y_train):\n    ''' Fits a classifier to the training data. '''\n    start=time(); clf.fit(X_train,y_train); end=time()\n    print(\"Trained model in {:.4f} seconds\".format(end-start))\ndef predict_labels(clf,features,target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    start=time(); y_pred=clf.predict(features); end=time()\n    print(\"Made predictions in {:.4f} seconds.\".format(end-start))\n    return f1_score(target.values,y_pred,pos_label=1)\ndef train_predict(clf,X_train,y_train):\n    ''' Train and predict using a classifer based on F1 score. '''\n    print(\"Training a {} using a training set size of {}. . .\"\\\n          .format(clf.__class__.__name__,len(X_train)))\n    train_classifier(clf,X_train,y_train)\n    print (\"F1 score for training set: {:.4f}.\"\\\n           .format(predict_labels(clf,X_train,y_train)))","8dbcee69":"clf_A=AdaBoostClassifier()\nclf_B=GradientBoostingClassifier()\nclf_C=RandomForestClassifier()\nX_train_200,y_train_200=pre_features2[:200],outcomes[:200]\nX_train_400,y_train_400=pre_features2[:400],outcomes[:400]\nX_train,y_train=pre_features2,outcomes\nX_test=test_features2\nfor clf in [clf_A,clf_B,clf_C]:\n    for (X_train,y_train) in [(X_train_200,y_train_200), \n                              (X_train_400,y_train_400), \n                              (X_train,y_train)]:\n        train_predict(clf,X_train,y_train)","4b96ef10":"clf=RandomForestClassifier()\nclf.fit(X_train,y_train)\ny_pred=clf.predict(test_features2); y_pred","e9d1e826":"submission=pd.DataFrame({\"PassengerId\":test_data[\"PassengerId\"],\"Survived\": y_pred})","8a053b39":"pre_features_array=pre_features2.values\noutcomes_array=outcomes.values\ntest_features_array=test_features2.values\nX_train2,X_test2,y_train2,y_test2=\\\ntrain_test_split(pre_features_array,outcomes_array,\n                 test_size=.2,random_state=1)\nn=int(len(X_test2)\/2)\nX_valid2,y_valid2=X_test2[:n],y_test2[:n]\nX_test2,y_test2=X_test2[n:],y_test2[n:]\nX_train2.shape,X_valid2.shape,X_test2.shape                                                       ","39c3ad47":"def mlp_model():\n    model=Sequential()    \n    model.add(Dense(36,input_dim=9,\n                    kernel_initializer='uniform',\n                    activation='relu'))\n    model.add(Dense(9,kernel_initializer='uniform',\n                    activation='relu'))\n    model.add(Dense(1, kernel_initializer='uniform',\n                    activation='sigmoid'))\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    return model\nmlp_model=mlp_model()","ce510d6a":"checkpointer=ModelCheckpoint(filepath=fw,verbose=2,save_best_only=True)\nlr_reduction=ReduceLROnPlateau(monitor='val_loss',patience=5,\n                               verbose=2,factor=.5)\nestopping=EarlyStopping(monitor='val_loss',patience=16,verbose=2)\nmlp_history=mlp_model.fit(X_train2,y_train2,\n                          validation_data=(X_valid2,y_valid2),\n                          epochs=100,batch_size=8,verbose=2,\n                          callbacks=[checkpointer,lr_reduction,estopping])","f319cec4":"def loss_plot(fit_history):\n    pl.figure(figsize=(12,6))\n    pl.plot(fit_history.history['loss'],label='train',c='g')\n    pl.plot(fit_history.history['val_loss'],label='valid',c='r')\n    pl.legend(); pl.title('Loss Function');      \ndef acc_plot(fit_history):\n    pl.figure(figsize=(12,6))\n    pl.plot(fit_history.history['accuracy'],label='train',c='g')\n    pl.plot(fit_history.history['val_accuracy'],label='valid',c='r')\n    pl.legend(); pl.title('Accuracy'); ","d6ffde1d":"loss_plot(mlp_history); acc_plot(mlp_history)","36c0728e":"mlp_scores=mlp_model.evaluate(X_test2,y_test2,verbose=0)\nprint(\"Accuracy: %.2f%%\" % (mlp_scores[1]*100))","e927f309":"y_pred_mlp=np.round(mlp_model.predict(test_features_array)).astype(int)\ny_pred_mlp[:,0]","1af75028":"submission_mlp=pd.DataFrame({\"PassengerId\":test_data[\"PassengerId\"],\n                             \"Survived\":y_pred_mlp[:,0]})","a02256af":"# submission_mlp.to_csv('submission.csv',index=False)","b085718a":"len(submission_mlp[\"Survived\"])","05f85322":"sum(submission_mlp[\"Survived\"]==submission[\"Survived\"])","ee62424f":"The evaluation terminology:\n\n- accuracy = number of  people that  are  correctly  predicted as survived or non-survived \/ number  of all  people  in  the dataset \n- recall = number of people that are predicted as survived and they are actually survived \/ number of  people are actually survived\n- precision =  number of people that  are predicted as survived  and they are actually survived \/ number of people that are predicted as survived\n\nIn this part of the project, I have used a manual implementation of a simple machine learning model, the decision tree which splits a set of data into smaller and smaller groups (called nodes), by one feature at a time. The predictions become more accurate if each of the resulting subsets is more homogeneous (contain similar labels) than before.\n\nA decision tree is just one of many models that come from supervised learning, i.e. learning a model from labeled training data to make predictions about unseen or future data in a set of samples the desired outputs are already known.","30d1e371":"The best result has the **RandomForestClassifier** and I will try to apply it for test predictions.","7ac8d876":"Combining the data in one histogram clearly shows the tendency: female passengers were more likely to survive in this instance.","57594bdb":"## Making   Predictions\n### Intuitive Predictions\nIf we were asked to make a prediction about any passenger aboard the RMS Titanic whom we knew nothing about, then the best prediction we could make would be that they did not survive. This is because we can assume that a majority of the passengers (more than 50%) did not survive the ship sinking.\n\nThe **predictions_0()** function below will always predict that a passenger did not survive. Let's check its accuracy.","4ef506c3":"As we can see, there are several non-numeric columns that need to be converted. Some of them have simply two values (male\/female, 1\/0, etc.). These features can be reasonably converted into binary values.\n\nOther columns, like **Embarked**, have more than two values and are known as categorical variables. The recommended way to handle such a column is to create as many columns as possible values and assign a 1 to one of them and 0 to all others.\n\nThese generated columns are sometimes called dummy variables, and I will use the **pandas.get_dummies()** function to perform this transformation.\n\nIn the preprocessing function, I will also replace found missing values in the features **Age** and **Fare** by the mean and in the feature **Embarked** by the most common value","e4daed71":"Let's evaluate the quality of the prediction.","3bef3de5":"### Accuracy Scores\nTo measure the performance of our predictions, we need metrics to score our predictions against the true outcomes of survival.\n\n- 1) The built function **accuracy_score_0()** calculates the proportion of passengers where our prediction of their survival is correct.\n- 2) Functions *sklearn.metrics*: \n- **recall_score**, \n- **accuracy_score**, \n- **precision_score**, \n- **f1_score**,\n- **make_scorer**.","4a253cce":"## Conclusion","1fceb0dd":"Using just the Sex feature for each passenger, we are able to increase the accuracy of our predictions by a significant margin.\nNow, let's consider using an additional feature to see if we can further improve our predictions. We will start by looking at the Age.","6a25a1b4":"Let's build on our previous prediction: If a passenger was female, then we will predict that they survived. Otherwise, we will predict the passenger did not survive.","f549b4e4":"## MLP Neural Networks","d89bbdd6":"### Survived\nThe next step is removing the Survived feature from the data and storing it separately as our prediction targets.\n\nThe passenger data and the outcomes of survival are now paired. That means for any passenger **all_features.loc[i]**, they have the survival **outcomes[i]**.","aca538c1":"## Classifiers\nI think this machine learning problem is in the classification field. It needs to predict the labels for the passengers: 'yes' or 'no' for the feature 'Survived'.\n\nFor simplicity, the border between regression and classification can be described in this way:\n\n- classification: predict the values of discrete or categorical targets;\n- regression: predict the values of continuous targets.\n\nLet's display some important information about the data training set.","ee56f9ba":"Now I should count missing values for the remained variables.","5241409e":"Now let us check the feature \"**Sex**\", the outcomes \"**Survived**\" and their possible dependence. \n\nHere is a special database for these categories including the indicators in percentages:\n\n- \"**Survived by sex in percentages I**\" determines the percentage of survived passengers of this sex in relation to the total number of survivors; \n- \"**Survived by sex in percentages II**\" - the percentage of survived passengers of this sex in relation to the total number of passengers of the same sex.","895cbd28":"Let's exclude the features that cannot have an influence on the target and the feature **Cabin** that has too many **NaN** values.","10e36296":"## Statistical Analysis and Data Exploration\n### Data\nLet's extract the data from the .csv file, create a  pandas DataFrame and look at the available indicators:\n\n- ***Survived***: Outcome of survival (0 = No; 1 = Yes)\n- ***Pclass***: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- ***Name***: Name of passenger\n- ***Sex***: Sex of the passenger\n- ***Age***: Age of the passenger (Some entries contain NaN)\n- ***SibSp***: Number of siblings and spouses of the passenger aboard\n- ***Parch***: Number of parents and children of the passenger aboard\n- ***Ticket***: Ticket number of the passenger\n- ***Fare***: Fare paid by the passenger\n- ***Cabin***: Cabin number of the passenger (Some entries contain NaN)\n- ***Embarked***: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)","adc7a112":"Adding the feature **Age** as a condition in conjunction with **Sex** improves the accuracy by a small margin more than with simply using the feature **Sex** alone. Now we can try to find a series of features and conditions to split the data on to obtain an outcome prediction accuracy of at least 80%. This may require multiple features and multiple levels of conditional statements to succeed. We can use the same feature multiple times with different conditions.\n\nThere are some experiments and the function **prediction_final()** as a result:","a0246018":"For predictions I have chosen the following models:\n\n- GradientBoostingClassifier();\n- RandomForestClassifier();\n- AdaBoostClassifier().\n\nLet's have a look at their applications and characteristics:\n\n1) **GradientBoostingClassifier**.\n\n- Applications: in the field of learning to rank (for example, web-seach), in ecology.\n  - Web-Search Ranking with Initialized Gradient Boosted Regression Trees: http:\/\/www.jmlr.org\/proceedings\/papers\/v14\/mohan11a\/mohan11a.pdf\n  - Gradient boosting machines, a tutorial: https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3885826\/\n- Strengths: natural handling of data of mixed type (= heterogeneous features), predictive power, robustness to outliers in output space (via robust loss functions).\n- Weaknesses: scalability, due to the sequential nature of boosting it can hardly be parallelized.\n\n2) **RandomForestClassifier**.\n\n- Applications: in ecology, bioinformatics.\n  - RANDOM FORESTS FOR CLASSIFICATION IN ECOLOGY: http:\/\/onlinelibrary.wiley.com\/doi\/10.1890\/07-0539.1\/abstract;jsessionid=AB1864A895F3244AF0699EB0317F2C99.f02t02\n  - Random Forest for Bioinformatics: http:\/\/www.cs.cmu.edu\/~qyj\/papersA08\/11-rfbook.pdf\n- Strengths: runs efficiently on large data bases; gives estimates of what variables are important in the classification; maintains accuracy when a large proportion of the data are missing; high prediction accuracy.\n- Weaknesses: difficult to interpret, can be slow to evaluate.\n\n3) **AdaBoostClassifier**.\n\n- Applications: the problem of face detection, text classification, etc.\n  - AdaBoost-based face detection for embedded systems: http:\/\/www.sciencedirect.com\/science\/article\/pii\/S1077314210000871\n  - Text Classification by Boosting Weak Learners based on Terms and Concepts: http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.443.8019&rep=rep1&type=pdf\n- Strengths: can be used with data that is textual, numeric, discrete, etc.; can be combined with any other learning algorithm, not prone to overfitting; simple to implement.\n- Weaknesses: can be sensitive to noisy data and outliers; the performance depends on data and weak learner (can fail if weak classifiers too complex).\n\nAll these classifiers will produce enough good predictions in this case. We should produce the result with the variant of ranking and it's a well-known fact that classification tends to be a better paradigm for ranking than regression.\n\nLet's initialize three helper functions which we can use for training and testing sets the three supervised learning models we've chosen above.\n\nThe functions are as follows:\n\n- **train_classifier** - takes as input a classifier and training data and fits the classifier to the data;\n- **predict_labels** - takes as input a fit classifier, features, and a target labeling and makes predictions using the F1 score;\n- **train_predict** - takes as input a classifier, and the training data, and performs train_clasifier and predict_labels.\n\nThis function will report the **F1 score** for the training data.","ec411514":"With the predefined functions above, I will now import the three supervised learning models and run the **train_predict** function for each one. It needs to train and predict on each classifier for three different training set sizes: 200, 400, and all data points. \n\nHence,  nine different outputs will be displayed below \u2014 three for each model using the varying training set sizes.","4ad8f6a4":"Overvaluation of meaning and application of machine learning is unlikely to succeed. And the particular supervised method has a special importance because of the possibility of a permanent correlation of the predictions with the result of real actions.\n\nThere are several natural ideas for applying the supervised learning.\n\nI. For every catastrophic situation, find out the exact sequence of steps and technical facilities which maximally decrease the damage. On the basis of the identified trends, it is possible to develop and check in practice clear guidelines to save lives and restore economic activities (for example, during and after the floods). Applying the scientific methods, in this case, means thousands of lives and quick recovering the economics. The useful features can evaluate disasters (areas, time period), damage (human lives, economic indicators) and recovery process (speed, effectiveness).\n\nII. The same techniques could be useful in the process of creating self-learning facilities of virtual reality in order to bring the process of their development to the real counterparts, predict it and make corrections in time. Here the set of concrete features is very individual and depends on the object. For example, it can be growth, flowering, etc. for the plant and its imitation.","4395cae8":"The final set of features **Sex**, **Age**, **SibSp** and **Pclass** are the most informative on my opinion.\n\nAs we noted the percentage of survivors of passengers is much higher among women than among men, and it was used in our predictions.\n\nNext, I proceed from the assumption that because of humanitarian reasons people rescue children and elders at first. Unfortunately, this was only valid for the passengers of the first and second classes in this dataset.\n\nAnd the latest clarification, which overcomes the border of 80% in prediction accuracy: if a family has more than three children, absolutely all the family may not be survived in catastrophic situations and in an atmosphere of panic.","5b1c2af1":"<h1>\ud83d\udcca Titanic Survival Exploration <\/h1>\n<h2>Collection of Classic Examples<\/h2>\n\n---\n[ => First Version](https:\/\/olgabelitskaya.github.io\/Data_Analyst_ND_Project2.html) [ => Second Version](https:\/\/olgabelitskaya.github.io\/MLE_ND_P0_V0.html) [ => Interactive Version](https:\/\/olgabelitskaya.github.io\/MLE_ND_P0_SMC.html)\n## References\n### Data\nIn 1912, the ship RMS Titanic struck an iceberg on its maiden voyage and sank, resulting in the deaths of most of its passengers and crew. In this project, we will explore a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive. \n\nThe data contains demographics and voyage information from 891 of the 2224 passengers and crew on board the ship.\n\nThis link allows reading the description of this dataset on the Kaggle website, where the data was obtained. https:\/\/www.kaggle.com\/c\/titanic\/data\n\n### Resources\n[Intro to Data Science. Udacity](https:\/\/www.udacity.com\/course\/intro-to-data-science--ud359)\n\n[Statistics in Python. Scipy Lecture Notes](http:\/\/www.scipy-lectures.org\/packages\/statistics\/index.html)\n\n[A Visual Introduction to Machine Learning](http:\/\/www.r2d3.us\/visual-intro-to-machine-learning-part-1\/)\n\n[The scikit-learn metrics](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.metrics)\n## Code Tools"}}