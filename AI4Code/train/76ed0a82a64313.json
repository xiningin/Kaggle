{"cell_type":{"cc2e7dfa":"code","331de884":"code","2ad31e6f":"code","a38cf024":"code","ed098bfe":"code","30096e8a":"code","6fb6331d":"code","499c5f6d":"code","37b56fc8":"code","396f923d":"code","bcd9a18d":"code","6f3b3e98":"code","6b7c8a74":"code","538bf0d6":"code","fed9c187":"code","aa41e08a":"code","79eedc4b":"code","01cca637":"code","efa9d604":"code","be9991e8":"code","b3776db7":"code","9f4eb98d":"code","1f3f3bbf":"code","660352fe":"code","9511ad2d":"code","2ebd8fea":"code","04bbe5a7":"code","6e6d77d8":"code","1bf52589":"code","6cab2cdd":"code","e7033549":"code","f071cda5":"code","31e63354":"code","cf25104a":"code","88cb2adb":"code","bdb4424a":"code","56b4f54e":"code","da4baad9":"code","ea249e07":"code","2e845704":"code","234cb5f2":"code","6a31030c":"code","32c8960b":"code","413475b2":"code","9bd3e6bb":"code","e462889c":"markdown","b1f1f558":"markdown","fc89a1f1":"markdown","b6406b57":"markdown","5a6b1a17":"markdown","46cfeffe":"markdown","f621187e":"markdown","c13c05bd":"markdown","a7d1d2c5":"markdown","29575aa8":"markdown","bf310433":"markdown","cab72841":"markdown","60f513bf":"markdown","1a154447":"markdown","20e379b4":"markdown","8bc9fd26":"markdown","ee94ffa3":"markdown","40dabda7":"markdown","a6dd6fb8":"markdown","576eece8":"markdown","e66c3890":"markdown","4576891d":"markdown","caf88b80":"markdown","58d407ea":"markdown","02a281b3":"markdown","4b79f200":"markdown"},"source":{"cc2e7dfa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as ply\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n# from sklearn import svm\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier","331de884":"titanic_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_data.head()","2ad31e6f":"titanic_data.head()","a38cf024":"titanic_data.shape","ed098bfe":"titanic_data.info()","30096e8a":"titanic_data.isnull().sum()","6fb6331d":"titanic_data = titanic_data.drop(columns = 'Cabin', axis=1)","499c5f6d":"titanic_data['Age'].fillna(titanic_data['Age'].mean(), inplace=True)\nage_mean = titanic_data['Age'].mean()\nfare_mean =  titanic_data['Fare'].mean()","37b56fc8":"titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)","396f923d":"titanic_data.isnull().sum()","bcd9a18d":"titanic_data.describe()","6f3b3e98":"titanic_data['Survived'].value_counts()","6b7c8a74":"sns.set()","538bf0d6":"# Count based on survival\nsns.countplot(x='Survived',data=titanic_data)","fed9c187":"titanic_data['Sex'].value_counts()","aa41e08a":"# Count based on gender\nsns.countplot(x='Sex',data=titanic_data)","79eedc4b":"# Count based on survival and gender\nsns.countplot(x='Sex', hue='Survived', data=titanic_data)","01cca637":"# Count based on pclass\nsns.countplot(x='Pclass', data=titanic_data)","efa9d604":"sns.countplot(x='Pclass',hue='Survived', data=titanic_data)","be9991e8":"titanic_data['Sex'].value_counts()","b3776db7":"titanic_data['Embarked'].value_counts()","9f4eb98d":"le1 = LabelEncoder()\ntitanic_data['Sex'] = le1.fit_transform(titanic_data['Sex'])\ntitanic_data.head()","1f3f3bbf":"titanic_data.head()","660352fe":"ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[-1])], remainder='passthrough' )\ntitanic_data = np.array(ct.fit_transform(titanic_data))\n# le2 = LabelEncoder()\n# titanic_data['Embarked'] = le2.fit_transform(titanic_data['Embarked'])\n# titanic_data.head()\ntitanic_data = pd.DataFrame(titanic_data, columns=['C','Q','S','PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare'])","9511ad2d":"X = titanic_data.drop(columns=['PassengerId','Ticket','Name','Survived'],axis=1)\ny = titanic_data.iloc[:,4].values\ny=y.astype('int')\nX.head()","2ebd8fea":"scalar = StandardScaler()\nX = scalar.fit_transform(X)\nprint(X.shape)","04bbe5a7":"y.shape\ny","6e6d77d8":"classifier = LogisticRegression(C = 0.01, penalty= 'l2', solver = 'newton-cg')\n#classifier = svm.SVC(kernel = 'linear')\n#classifier = KNeighborsClassifier(n_neighbors = 6,weights = 'uniform', metric = 'minkowski' , p=2)\n#classifier = svm.SVC(kernel = 'rbf', C=1, gamma =0.1)\n#classifier = GaussianNB()\n#classifier = DecisionTreeClassifier(criterion='entropy')\n#classifier = RandomForestClassifier(n_estimators=10,criterion = 'entropy')\n#from xgboost import XGBClassifier\n#classifier = XGBClassifier(use_label_encoder=False, eval_metric = 'error')\nclassifier.fit(X,y)","1bf52589":"# for competition test data\n# for competition test set\n# for competition\n# for competition\ntitanic_data_test = pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_data_test.isnull().sum()","6cab2cdd":"titanic_data_test.head()","e7033549":"titanic_data_test.columns","f071cda5":"titanic_data_test.describe()","31e63354":"titanic_data_test['Age'].fillna(age_mean, inplace=True)\ntitanic_data_test['Fare'].fillna(fare_mean, inplace=True)\ntitanic_data_test['Sex'] = le1.transform(titanic_data_test['Sex'])\ntitanic_data_test.head()\ntitanic_data_test = np.array(ct.fit_transform(titanic_data_test))\ntitanic_data_test = pd.DataFrame(titanic_data_test, columns=['C','Q','S','PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Ticket', 'Fare', 'Cabin'])\ntitanic_data_test.head()","cf25104a":"X_test_competition = titanic_data_test.drop(columns=['Name','PassengerId','Ticket','Cabin'], axis=1)\nX_test_competition.head()","88cb2adb":"X_test_competition.isnull().sum()","bdb4424a":"X_test_competition = scalar.fit_transform(X_test_competition)\nprint(X_test_competition)","56b4f54e":"# on entire train set\ny_pred = classifier.predict(X)\naccuracy_train = accuracy_score(y, y_pred)\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X,y= y , cv = 10)\nprint(\"Accuracy score on entire train data:\", accuracy_train)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))\n","da4baad9":"# from sklearn.model_selection import GridSearchCV\n# parameters = [{'C':[0.25,0.5,0.75,1], 'kernel' : ['linear']},\n#               {'C':[0.25,0.5,0.75,1], 'kernel' : ['rbf'], 'gamma' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}]\n# grid_search = GridSearchCV(estimator=classifier,\n#                           param_grid=parameters,\n#                           scoring='accuracy',\n#                           cv=10)\n# grid_search.fit(X,y)\n# print(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\n# print(\"Best Parameters: \", grid_search.best_params_)","ea249e07":"# on entire test set, Final result!!\ny_pred_competition = classifier.predict(X_test_competition)\ny_pred_competition","2e845704":"titanic_data_test_copy = pd.read_csv('..\/input\/titanic\/test.csv')\nresults = pd.DataFrame(titanic_data_test_copy['PassengerId'],columns=['PassengerId'])\nresults.head()","234cb5f2":"results['Survived'] = y_pred_competition","6a31030c":"results","32c8960b":"titanic_data_test.shape","413475b2":"results.to_csv('Results.csv',index=False)","9bd3e6bb":"!ls","e462889c":"Splitting the project into train and test set!","b1f1f558":"Evaluating accuracy for different models\n* logistic - 81%\n* svm linear kernel - 79%\n* KNN - 79%\n* svm rbf kernel - 81%\n* naive bayes - 80%\n* decision tree - 79%\n* Random forest - 79%\n* XGBoost Classifier - 80%","fc89a1f1":"Using KFold cross validation to check for mean of accuracies!\nChecking for **overall performance of model!**","b6406b57":"Model Training!","5a6b1a17":"For the **competition** we're gonna train the model on the entire train set!","46cfeffe":"Data Viualization!","f621187e":"Preparing submission file!","c13c05bd":"Preprocessing test set!","a7d1d2c5":"Standardizing the values!","29575aa8":"Importing the dataset!\nTitanic dataset from kaggle!","bf310433":"* 0 -> Didn't survive\n* 1 -> Survived","cab72841":"Now that we see logistic regressor performs better! Its good to tune the hyper parameters!","60f513bf":"Importing the test set!","1a154447":"# Titanic Survival Prediction Project!","20e379b4":"Encoding Categorical values into numerical values! **Sex** and **Embarked**","8bc9fd26":"We can see that many females have survived and it seems logical as in a situation like this, females and.  childerns are given priority!","ee94ffa3":"Data Analysis!","40dabda7":"Handling the mising values!\n* Since we have a lot of missing values in **cabin**, it's better to **drop** the column\n* For **age**, we cannot just drop the column as we have lot of useful data, hence we replace the missing values with the **mean value**!\n* for **embarked**, we find the **mode** and replace the missing data as it is a character data!","a6dd6fb8":"As we can see there are some Null values and hence we have to check how many total null values are there in the dataset!","576eece8":"Now let's convert the **sex** column and **embarked** column into **binary** form using the LabelEncoder","e66c3890":"Exporting the results","4576891d":"Finding number of people who survived and not!","caf88b80":"Now we split the dataset into **dependent** and **independent** variables. In independent variables we don't require the **name**, **passengerID** and the **ticket** column as they don't convey us much information!","58d407ea":"1. numpy - useful for arrays and mathematical calculations\n2. pandas - for creating data frames\n3. matplotlib - for creating graphs and plots\n4. seaborn - for creating graphs and plots\n5. sklearn - we use this library for training our model and for the dataset","02a281b3":"Impoting the libraries","4b79f200":"Hey everyone! In this project we're gonna be looking at the titanic data set and try to make some submissions to rank up into **kaggle contributer** !\nWe'll be using logistic classifier since we have to predict binary result i.e if the person survived or not! I have actually tested quite a number of classifer algorithms to see which gives the best accuracy and tried to tune the hyperparameters also! This project has a train set and a test sent and for the competition submission the model is trained on the entire train set and is used to predict the test!\nOverall accuracy of **78%** on the test set!\nHappy Learning!"}}