{"cell_type":{"ea5fe4e7":"code","bb13bd3b":"code","b84ea0d5":"code","97928cf8":"code","6141d7d9":"code","4474352f":"code","776cb9c8":"code","6218e308":"code","0a6fd0a8":"code","a7af268e":"code","152507e7":"code","ad4232ea":"code","0c2a4a06":"code","1d3e2c2d":"code","17bc49de":"code","e1372eb1":"code","1ad1b78c":"code","93fe8e2c":"code","f7c911d4":"code","3e8ef3c2":"code","6dc4d96f":"code","a555cbe9":"code","13673e83":"markdown","8acd87ad":"markdown","ad26242a":"markdown","99c8f868":"markdown","783f1bc0":"markdown","8bbfa0c9":"markdown","fc56a0c7":"markdown","50255b04":"markdown","c865b4df":"markdown","bb7b62da":"markdown"},"source":{"ea5fe4e7":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"..\/input\/stress-analysis-in-social-media\/dreaddit-train.csv\")\ntest = pd.read_csv(\"..\/input\/stress-analysis-in-social-media\/dreaddit-test.csv\")\n\ntrain.head()","bb13bd3b":"test.head()","b84ea0d5":"print(train.columns)\nprint(train.shape)","97928cf8":"print(test.columns)\nprint(test.shape)","6141d7d9":"#categorical columns\ntrain.select_dtypes(include=['object']).columns.tolist()\n","4474352f":"test.select_dtypes(include=['object']).columns.tolist()\n","776cb9c8":"train.subreddit.value_counts()","6218e308":"test.subreddit.value_counts()","0a6fd0a8":"train = train.drop(['post_id', 'sentence_range', 'id'], axis = 1)\ntest = test.drop(['post_id', 'sentence_range', 'id'], axis = 1)","a7af268e":"df = pd.concat([train,test],axis=0,ignore_index=True)\ndf = df.sample(frac = 1).reset_index(drop = True)\ndf.head()","152507e7":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['subreddit'] = le.fit_transform(df['subreddit'])\n","ad4232ea":"df.head()","0c2a4a06":"df.corr().abs()['label'].sort_values(ascending = False)[:30]","1d3e2c2d":"import tensorflow as tf\nimport transformers\nimport tqdm\nfrom keras.preprocessing import sequence\n\n#creating a function\ndef func_tokenizer(tokenizer_name, docs):\n    features = []\n    for doc in tqdm.tqdm(docs, desc = 'converting documents to features'):\n        tokens = tokenizer_name.tokenize(doc)\n        ids = tokenizer_name.convert_tokens_to_ids(tokens)\n        features.append(ids)\n    return features\nprint(\"The function is created successfully\")","17bc49de":"#Initialize bert tokenizer\nbert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')\n\nX,y = df[['text', 'lex_liwc_Tone', 'lex_liwc_negemo', 'lex_liwc_Clout','lex_liwc_i', 'sentiment' ]], df['label']\nbert_features = func_tokenizer(bert_tokenizer, X['text'])","e1372eb1":"bert_trg = sequence.pad_sequences(bert_features, maxlen = 500)\n","1ad1b78c":"X = pd.DataFrame(bert_trg)\n\nX = X.assign(lex_liwc_negemo = df['lex_liwc_negemo'].values)\nX = X.assign(lex_liwc_Tone= df['lex_liwc_Tone'].values)\nX = X.assign(lex_liwc_Clout = df['lex_liwc_Clout'].values)\nX = X.assign(lex_liwc_i = df['lex_liwc_i'].values)\nX = X.assign(sentiment = df['sentiment'].values)\nX = X.assign(lex_dal_min_pleasantness = df['lex_dal_min_pleasantness'].values)\nX = X.assign(lex_liwc_posemo = df['lex_liwc_posemo'].values)\nX = X.assign(lex_liwc_anx = df['lex_liwc_anx'].values)\nX = X.assign(lex_liwc_Authentic = df['lex_liwc_Authentic'].values)\nX = X.assign(lex_liwc_social = df['lex_liwc_social'].values)\nX = X.assign(lex_liwc_Analytic = df['lex_liwc_Analytic'].values)\nX = X.assign(lex_liwc_function = df['lex_liwc_function'].values)\nX = X.assign(lex_liwc_Dic = df['lex_liwc_Dic'].values)","93fe8e2c":"X","f7c911d4":"from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)","3e8ef3c2":"def get_models():\n    \n    models = dict()\n    models['lr'] = make_pipeline(StandardScaler(),LogisticRegression(solver = 'saga', C = 70.0))\n    models['knn'] = make_pipeline(StandardScaler(),KNeighborsClassifier())\n    models['cart'] = DecisionTreeClassifier(max_depth = 1)\n    models['svm'] = make_pipeline(StandardScaler(),SVC())\n    models['bayes'] = make_pipeline(StandardScaler(), GaussianNB())\n    models['xgboost'] = XGBClassifier(n_estimators = 11, max_depth = 1)\n    models['GBM'] = GradientBoostingClassifier(n_estimators = 10)\n    models['rf'] = RandomForestClassifier(n_estimators = 10)\n    models['adaboost'] = AdaBoostClassifier(n_estimators= 12)\n    \n    return models","6dc4d96f":"# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=4, random_state=1)\n\tscores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n\treturn scores","a555cbe9":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom numpy import mean\nfrom numpy import std\n\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\tscores = evaluate_model(model, X, y)\n\tresults.append(scores)\n\tnames.append(name)\n\tprint('>%s %.2f (%.2f)' % (name, scores.mean(), std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","13673e83":"# BERT tokenizer","8acd87ad":"# Logistic regression, XGBoost and SVM gave the highest accuracy!","ad26242a":"# Acknowledgements\n\nTurcan, E., & McKeown, K. (2019). Dreaddit: A Reddit dataset for stress analysis in social media. arXiv preprint arXiv:1911.00133.\n\n# The relevant research paper link can be found here: -\nhttps:\/\/aclanthology.org\/D19-6213.pdf","99c8f868":"# All 9 models to be applied","783f1bc0":"# Upvote if you like it.","8bbfa0c9":"![](https:\/\/data.whicdn.com\/images\/266750169\/original.jpg)","fc56a0c7":"# According to the research paper too, logistic regression gave the highest accuracy.","50255b04":"# The relevant research paper link can be found here: -\nhttps:\/\/aclanthology.org\/D19-6213.pdf","c865b4df":"Label ---> 1 (Stress)\n\nLabel ---> 0 (Not stress)","bb7b62da":"# Models to be applied with 10 fold repeated stratified K fold cross validation"}}