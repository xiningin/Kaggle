{"cell_type":{"d3575bd4":"code","8ae67a73":"code","2be0d0c1":"code","4fafdb39":"code","02a28849":"code","8acb7631":"code","8aebd5de":"code","9c1a00aa":"code","8cf8fed2":"code","e65ed24d":"code","3d4a51bc":"code","b09f91c9":"code","0ed743a0":"code","8846fb82":"code","baa0a7f8":"code","5f8fa6f3":"code","d170c18d":"code","9ce9b04f":"code","984ff5fa":"code","50f34838":"code","609dc6bf":"code","e16d8c26":"code","7ce49a3b":"code","bd566a97":"code","61833887":"code","52f25eff":"markdown","fd993f69":"markdown","132e1c9c":"markdown","0eeb66f8":"markdown","1de9bf55":"markdown"},"source":{"d3575bd4":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport tensorflow_datasets as tfds\n\nimport warnings\nwarnings.filterwarnings('ignore')","8ae67a73":"dataset, info = tfds.load(name='imdb_reviews', as_supervised=True, with_info=True)","2be0d0c1":"# info","4fafdb39":"# getting train and test data\n\ntrain_dataset = dataset['train']\ntest_dataset = dataset['test']","02a28849":"# checking dataset spec\n\ntrain_dataset.element_spec","8acb7631":"# loading training and testing data in list\n# so, that we can use the Tokenizer\n\ntrain_reviews = []\ntrain_labels = []\n\ntest_reviews = []\ntest_labels = []\n\nfor review, label in train_dataset:\n    review = review.numpy()\n    review = tf.compat.as_str_any(review) # convert bytes to string\n    train_reviews.append(review)\n    train_labels.append(label.numpy())\n\nfor review, label in test_dataset:\n    review = review.numpy()\n    review = tf.compat.as_str_any(review) # convert bytes to string\n    test_reviews.append(review)\n    test_labels.append(label.numpy())","8aebd5de":"len(train_reviews), len(train_labels)","9c1a00aa":"train_reviews[:2]","8cf8fed2":"tokenizer = Tokenizer(oov_token=\"<oov>\")\ntokenizer.fit_on_texts(train_reviews)","e65ed24d":"len(tokenizer.index_word)","3d4a51bc":"# defining maximum length of tokens and padding sequences\n\nmax_len = 150\n\ntrain_sequences = tokenizer.texts_to_sequences(train_reviews)\n\ntrain_sequences = pad_sequences(train_sequences, maxlen=150, \n                                padding='post', truncating='post', value=0)\n\ntest_sequences = tokenizer.texts_to_sequences(test_reviews)\n\ntest_sequences = pad_sequences(test_sequences, maxlen=150, \n                                padding='post', truncating='post', value=0)","b09f91c9":"train_sequences.shape","0ed743a0":"train_sequences[:2]","8846fb82":"# Creating final dataset in tf.data.Dataset form\n\nbatch_size = 100\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((train_sequences, train_labels))\ntrain_ds = train_ds.shuffle(1000).batch(batch_size).prefetch(1)\n\ntest_ds = tf.data.Dataset.from_tensor_slices((test_sequences, test_labels))\ntest_ds = test_ds.batch(batch_size)\n\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)","baa0a7f8":"def load_glove_vectors(filepath):\n    \n    model = {}\n    print(\"Loading Glove Model\")\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            words = line.split()\n            model[words[0]] = np.array([float(value) for value in words[1:]])\n    print(\"Total loaded words\", len(model))\n    \n    return model","5f8fa6f3":"glove_model = load_glove_vectors(\"..\/input\/glove6b50dtxt\/glove.6B.50d.txt\")","d170c18d":"embedding_dims = 50\nvocab_size = len(tokenizer.word_index) + 1\n\nempty_vector = np.zeros(embedding_dims) # default vector if word not in glove model\n\nembedding_matrix = np.zeros((vocab_size, embedding_dims))\n\nfor idx, word in tokenizer.index_word.items():\n    embedding_matrix[idx] = glove_model.get(word, empty_vector)","9ce9b04f":"embedding_matrix[12] # token = this","984ff5fa":"glove_model.get('this')","50f34838":"model = Sequential(\n    [\n     Embedding(vocab_size, embedding_dims, mask_zero=True, weights=[embedding_matrix], trainable=False),\n     Bidirectional(LSTM(64, return_sequences=True, dropout=0.5)),\n     Bidirectional(LSTM(64, dropout=0.25)),\n     Dense(1, 'sigmoid')\n    ]\n)\n\nmodel.summary()","609dc6bf":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n\nhistory = model.fit(train_ds, epochs=10)","e16d8c26":"model.evaluate(test_ds)","7ce49a3b":"model_2 = Sequential(\n    [\n     Embedding(vocab_size, embedding_dims, mask_zero=True, weights=[embedding_matrix], trainable=True),\n     Bidirectional(LSTM(64, return_sequences=True, dropout=0.5)),\n     Bidirectional(LSTM(64, dropout=0.25)),\n     Dense(1, 'sigmoid')\n    ]\n)\n\nmodel_2.summary()","bd566a97":"model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n\nhistory = model_2.fit(train_ds, epochs=10)","61833887":"model_2.evaluate(test_ds)","52f25eff":"## Fine-tuning Model","fd993f69":"## Loading and preparing data","132e1c9c":"## Loading Glove Model","0eeb66f8":"## Creating Pre-trained embedding matrix using Glove","1de9bf55":"## Feature Extraction Model"}}