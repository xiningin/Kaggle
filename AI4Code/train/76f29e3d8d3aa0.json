{"cell_type":{"4a403ab9":"code","1c92b744":"code","3aed4324":"code","0693def8":"markdown","9255be68":"markdown","87843b4a":"markdown"},"source":{"4a403ab9":"# Original code from https:\/\/github.com\/lyft\/l5kit\/blob\/20ab033c01610d711c3d36e1963ecec86e8b85b6\/l5kit\/l5kit\/evaluation\/metrics.py\n# Thanks for the additions from here: https:\/\/www.kaggle.com\/corochann\/lyft-training-with-multi-mode-confidence\nimport numpy as np\nimport torch\nfrom torch import Tensor\n\n\ndef pytorch_neg_multi_log_likelihood(gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor) -> Tensor:\n    \"\"\"\n    Compute a negative log-likelihood for the multi-modal scenario.\n    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n    https:\/\/en.wikipedia.org\/wiki\/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n    https:\/\/timvieira.github.io\/blog\/post\/2014\/02\/11\/exp-normalize-trick\/\n    https:\/\/leimao.github.io\/blog\/LogSumExp\/\n    Args:\n        gt (Tensor): array of shape (time)x(2D coords)\n        pred (Tensor): array of shape (modes)x(time)x(2D coords)\n        confidences (Tensor): array of shape (modes) with a confidence for each mode in each sample\n        avails (Tensor): array of shape (time) with the availability for each gt timestep\n    Returns:\n        Tensor: negative log-likelihood for this example, a single float number\n    \"\"\"\n    assert len(pred.shape) == 3, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n    num_modes, future_len, num_coords = pred.shape\n\n    assert gt.shape == (future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n    assert confidences.shape == (num_modes,), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n    assert abs(torch.sum(confidences).item() - 1.0) < 1e-6, \"confidences should sum to 1\"\n    assert avails.shape == (future_len,), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n    # assert all data are valid\n    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n\n    gt = torch.unsqueeze(gt, 0)  # add modes\n    avails = avails[None, :, None]  # add modes and cords\n\n    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n\n    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n\n    # use max aggregator on modes for numerical stability\n    max_value = error.max()  # error are negative at this point, so max() gives the minimum one\n    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1)) - max_value  # reduce modes\n    return error\n\n\ndef pytorch_neg_multi_log_likelihood_batch(\n    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n) -> Tensor:\n    \"\"\"\n    Compute a negative log-likelihood for the multi-modal scenario.\n    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n    https:\/\/en.wikipedia.org\/wiki\/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n    https:\/\/timvieira.github.io\/blog\/post\/2014\/02\/11\/exp-normalize-trick\/\n    https:\/\/leimao.github.io\/blog\/LogSumExp\/\n    Args:\n        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n    Returns:\n        Tensor: negative log-likelihood for this example, a single float number\n    \"\"\"\n    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n    batch_size, num_modes, future_len, num_coords = pred.shape\n\n    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n    # assert all data are valid\n    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n\n    # convert to (batch_size, num_modes, future_len, num_coords)\n    gt = torch.unsqueeze(gt, 1)  # add modes\n    avails = avails[:, None, :, None]  # add modes and cords\n\n    # error (batch_size, num_modes, future_len)\n    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n\n    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n        # error (batch_size, num_modes)\n        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n\n    # use max aggregator on modes for numerical stability\n    # error (batch_size, num_modes)\n    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n    # print(\"error\", error)\n    return torch.mean(error)\n\n\ndef pytorch_neg_multi_log_likelihood_single(\n    gt: Tensor, pred: Tensor, avails: Tensor\n) -> Tensor:\n    \"\"\"\n\n    Args:\n        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n    Returns:\n        Tensor: negative log-likelihood for this example, a single float number\n    \"\"\"\n    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n    # create confidence (bs)x(mode=1)\n    batch_size, future_len, num_coords = pred.shape\n    confidences = pred.new_ones((batch_size, 1))\n    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)\n","1c92b744":"import pandas as pd\nimport numpy as np\nfrom torch import Tensor\n\n# 'pixel_size': [0.50, 0.50],\n# so we must accept an error of up to 0.25 meter, mean error is 0.50 m \/ 4\n\nsubmission = pd.read_csv(\"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/single_mode_sample_submission.csv\")\n\ngt = Tensor(submission.iloc[0, 5:105].to_numpy().reshape((-1, 2)))\npred = Tensor(submission.iloc[0, 5:105].to_numpy().reshape((-1, 2)))\navails = Tensor(np.ones((50)))\n\ngt = gt.unsqueeze(0)\npred = pred.unsqueeze(0)\navails = avails.unsqueeze(0)\n\npred += (0.50 \/ 4)\n\nnll = pytorch_neg_multi_log_likelihood_single(gt, pred, avails)\nprint(f'Additional Expected Error for pixel_size = 0.50 (nll metric): {nll.item()}')","3aed4324":"gt = Tensor(submission.iloc[0, 5:105].to_numpy().reshape((-1, 2)))\npred = Tensor(submission.iloc[0, 5:105].to_numpy().reshape((-1, 2)))\navails = Tensor(np.ones((50)))\n\ngt = gt.unsqueeze(0)\npred = pred.unsqueeze(0)\navails = avails.unsqueeze(0)\n\npred += (0.25 \/ 4)\n\nnll = pytorch_neg_multi_log_likelihood_single(gt, pred, avails)\nprint(f'Additional Expected Error for pixel_size = 0.25 (nll metric): {nll.item()}')","0693def8":"### Seems like we have to expect, that 0.78125 of our metric is actually just the inaccuracy from the rasterizer","9255be68":"# Expected Loss: Inaccuracy from Rasterization\n\nAs we are using a rasterizer in most public Notebooks right now, I want to stress the importance of the raster size w.r.t. the expected loss that originates from the rasterization inaccuracy.\n\nEach history position, each lane, each other agent is encoded into a pixels and our net is only able to predict the next positions on the map with pixel accuracy. \nIn many notebooks, the raster has a size of 0.50 m per pixel (hyperparameter). Thus, the expected mean error will be a 0.50 \/ 4 for each direction for each predicted position.\n\nIn the following I will calculate the resulting error in the given metric.\n\n![expected_error.PNG](attachment:expected_error.PNG)\n![legend.PNG](attachment:legend.PNG)","87843b4a":"### We can reduce that value to 0.1953125 by creating a smaller grid (e.g. pixel_size = 0.25). Keep in mind, that you would need to double the raster_size to see the same region of the map."}}