{"cell_type":{"804bcb0f":"code","1440e640":"code","a5628e80":"code","f070a0fc":"code","e60068cc":"code","88c9d860":"code","1f668ef6":"code","bf2ad4a3":"code","687b6eaf":"code","82ba9e8b":"code","4777ccf9":"code","a7cd560c":"code","bcae4c53":"code","279a64c9":"code","78704782":"code","488d8034":"code","336b98e0":"code","cbf1256f":"code","c520c5c7":"code","380cdabb":"code","b303b60a":"code","f182a7b2":"code","68192e6f":"code","7d5bafce":"code","dc38c4b9":"code","03115a50":"code","357eced8":"code","512540e3":"markdown","05a9676d":"markdown","35a1c63c":"markdown","a1b9be57":"markdown","ffd7fac6":"markdown","e4719895":"markdown","2e341440":"markdown","d0d8b4a6":"markdown","5bc07846":"markdown","194a76fe":"markdown","9dd45cf4":"markdown","1596cec9":"markdown","264a37c0":"markdown","044d88ea":"markdown","2aeaa7bc":"markdown","5f6c08fc":"markdown","8c469b6f":"markdown","91c1368e":"markdown","bae3938b":"markdown","65295d8c":"markdown","773e9ffb":"markdown","a931e3ec":"markdown","a64fff27":"markdown","169cb436":"markdown","b96e0c3a":"markdown","fcf3c840":"markdown","51c445ed":"markdown","7fc4d509":"markdown"},"source":{"804bcb0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport warnings as wrn\nwrn.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1440e640":"# Importing Data Using Pandas\ndata = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","a5628e80":"data.head()","f070a0fc":"data.tail()","e60068cc":"data.info()","88c9d860":"data[\"class\"] = [1 if each == \"Normal\" else 0 for each in data[\"class\"]] ","1f668ef6":"data.head()","bf2ad4a3":"data.tail()","687b6eaf":"data = (data-np.min(data)) \/(np.max(data)-np.min(data))","82ba9e8b":"data.head()\n","4777ccf9":"from sklearn.model_selection import train_test_split\nx = data.drop(\"class\",axis=1)\ny = data[\"class\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=1,test_size=0.3)","a7cd560c":"from sklearn.metrics import confusion_matrix\ndef plot_confusionMatrix(y_true,y_pred):\n    cn = confusion_matrix(y_true=y_true,y_pred=y_pred)\n    \n    fig,ax = plt.subplots(figsize=(5,5))\n    sns.heatmap(cn,annot=True,linewidths=1.5)\n    plt.show()\n    return cn","bcae4c53":"score_list = {} # I've created this dict for saving score variables into it ","279a64c9":"from sklearn.neighbors import KNeighborsClassifier \nKNN = KNeighborsClassifier(n_neighbors=22) #I've tried more than 50 values. 22 is the best value\n\nKNN.fit(x_train,y_train)\nknn_score = KNN.score(x_test,y_test)\nscore_list[\"KNN Classifier\"] = knn_score\nprint(f\"Score is {knn_score}\")\n","78704782":"y_true = y_test\ny_pred = KNN.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","488d8034":"from sklearn.linear_model import LogisticRegression\n\nLR = LogisticRegression()\nLR.fit(x_train,y_train)\n\nlr_score = LR.score(x_test,y_test)\nscore_list[\"Logistic Regression\"] = lr_score\n\nprint(f\"Score is {lr_score}\")","336b98e0":"y_pred = LR.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","cbf1256f":"from sklearn.svm import SVC \n\nsvc = SVC()\nsvc.fit(x_train,y_train)\nsvc_score = svc.score(x_test,y_test)\nscore_list[\"SVC\"] = svc_score\nprint(f\"Score is {svc_score}\")","c520c5c7":"y_true = y_test\ny_pred = svc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","380cdabb":"from sklearn.naive_bayes import GaussianNB\n\nnbc = GaussianNB()\nnbc.fit(x_train,y_train)\nnbc_score = nbc.score(x_test,y_test)\nscore_list[\"GaussianNBC\"] = nbc_score\n\nprint(f\"Score is {nbc_score}\")","b303b60a":"y_true = y_test\ny_pred = nbc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","f182a7b2":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(random_state=1)\ndtc.fit(x_train,y_train)\n\ndtc_score = dtc.score(x_test,y_test)\nscore_list[\"DTC\"] = dtc_score\nprint(f\"Score is {dtc_score}\")","68192e6f":"y_true = y_test\ny_pred = dtc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","7d5bafce":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=50,random_state=1)\nrfc.fit(x_train,y_train)\nrfc_score = rfc.score(x_test,y_test)\nscore_list[\"RFC\"]=rfc_score\n\nprint(f\"Score is {rfc_score}\")","dc38c4b9":"y_true = y_test\ny_pred = rfc.predict(x_test)\nplot_confusionMatrix(y_true,y_pred)","03115a50":"score_list = list(score_list.items())","357eced8":"for alg,score in score_list:\n    print(f\"{alg} Score is {str(score)[:4]} \")\n","512540e3":"Our score is %78. Although our score is low, it is still better than Logistic Regression score","05a9676d":"Our function is ready","35a1c63c":"## Normalizing Data\n\nIn this section I am going to normalize data. In order to do this I am going to use this formula.\n\n#### normalized = (value - min value of the feature) \/ (max value of the feature - min value of the feature)\n\nLet's implement this in python!","a1b9be57":"* Our data does not have any NaN values so we will not fill NaN values\n* Our label is object, we have to convert it to int64\n* There are only 6 features in our dataset.","ffd7fac6":"## Random Forest Classification\n\nIn this section I am going to train our last model, Random Forest Classification.","e4719895":"## Decision Tree Classification\n\nIn this section I am going to train a decision tree classification model.","2e341440":"## Splitting Data\n\nIn this section I am going to split data into two pieces. Train and test. In order to do this I am going to use SKLearn library's Train Test Split Function. Let's do this!","d0d8b4a6":"* Our score is %81. It is very similar to our KNN score","5bc07846":"# Confusion Matrix Function\n\nIn this section I am going to define a function that creates a seaborn heatmap from a confusion matrix.","194a76fe":"# Result\n\nIn this section I am going to compare the scores of all the algorithms. At the first of Machine Learning Section. I've defined a dictionary that contains the scores of algorithms. In this section I am going to use that.","9dd45cf4":"# Machine Learning Algorithms\n\nFinally we've came our main section. In this section I am going to train some different machine learning algorithms and at the final of this section I am going to compare accuracy scores. Let's start with KNN","1596cec9":"Our first algorithm's score is %81. I think it is a bit low, but not bad.","264a37c0":"## Logistic Regression \n\nIn this section I am going to train a logistic regression model. I hope it will be better than KNN. ","044d88ea":"* Our SVC score is %80. This is better than Logistic Regression Score ","2aeaa7bc":"* Our logistic regression score is %74","5f6c08fc":"## Support Vector Machine Classification\nIn this section I am going to use SVM classification.","8c469b6f":"# Introduction\n\nHello people, welcome to my kernel! In this kernel I am going to compare different classification algorithms with **Biomechanical Features of Orthopedic Patients** dataset.\n\nIn this kernel I am going to use 6 different algorithm:\n\n* K-Nearest Neighbour\n* Logistic Regression\n* Support Vector Machine (SVC)\n* Naive Bayes Classification\n* Decision Tree Classification\n* Random Forest Classification\n\nLet's take a look at our schedule\n\n# Schedule\n1. Importing Libraries and Data\n2. Having Idea About Data\n1. Data Preprocessing for Machine Learning\n1. Confusion Matrix Function\n1. Machine Learning Algorithms\n    * KNN\n    * Logistic Regression\n    * SVM Classification\n    * Naive Bayes Classification\n    * Decision Tree Classification\n    * Random Forest Classification\n1. Result\n1. Conclusion\n","91c1368e":"I've imported two labeled data because I want to use Logistic Regression","bae3938b":"## Converting Label to Int64\n\nMaybe, you remember. There are two labels in our dataset. They are:\n* Normal\n* Abnormal\n\nI am going to convert *Normal* to 1 and *Abnormal* to 0. In order to do this I am going to use list comprehension\n\n(Python is really usefull language :D )","65295d8c":"- Our train and test splits are ready, we are ready to train some machine learning features!","773e9ffb":"# Data Preprocessing for Machine Learning\n\nIn this section I am going to prepare data for machine learning. In order to do this I am going to follow these steps.\n\n* Converting Label to Int64\n* Normalizing Data\n* Splitting Data \n\nLet's start.\n","a931e3ec":"## KNN Classification\n\nIn this section I am going to train a KNN model using sklearn library. Then I am going to save the score of this model in a variable. (I am going to save it because I want to compare the scores at the final)","a64fff27":"# Importing Libraries and Data\n\nIn this section I am going to only import the libraries that about the visualization and data processing. I am going to add machine learning libraries when I need them.","169cb436":"# Conclusion\n\nI am a beginner in Machine Learning so I might made some mistakes. If there is a problem please contact me.\n\nHowever if there is not any problems. If you upvote this kernel, I would be glad.","b96e0c3a":"We've saw our scores. Let's sort them.\n\n1. Random Forest Classification %87 Accuracy\n2. Naive Bayes and KNN Classification %81 Accuracy\n3. Support Vector Machine Classification %80 Accuracy\n4. Decision Tree Classification %78 Accuracy\n5. Logistic Regression %74 Accuracy\n\nAs we can see, for classification, the best algorithm is Random Forest Classification.\n\n## But do not forget that, in different datasets, it will be a different algorithm \n\n## So it does not show that, Random Forest Classification is not the best algorithm for classification - however for this dataset it is :) -","fcf3c840":"* Finally! Our score is %87. It is the best score of this kernel.\n* Our algorithms finished. We are ready to compare the scores.","51c445ed":"# Having Idea About Data\n\nIn this section I am going to examine dataset because before the preprocessing I have to have an idea about the data. In order to do this I am going to use head(),tail() and info() methods","7fc4d509":"## Naive Bayes Classification\n\nIn this section I am going to train a NBC model."}}