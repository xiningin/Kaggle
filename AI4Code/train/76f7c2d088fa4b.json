{"cell_type":{"e806c3ef":"code","940e9fff":"code","7c22a549":"code","72946529":"code","9aff6dc0":"code","e176acfe":"code","9a77b3f3":"code","625e689a":"code","91c59cac":"code","56701b68":"code","daf28b8b":"code","ae81df7c":"code","689346c2":"code","bc863a00":"code","cccade66":"code","9771a39e":"code","d9224469":"code","a6110fb3":"code","236ea281":"code","63adcd8d":"code","536269d9":"code","3b7ff208":"code","bca9a997":"code","b31b1e1f":"code","c5c5965c":"code","cfb2ab67":"code","d023d24a":"code","9f0c6f68":"code","1fb2f843":"code","edc1afbf":"code","dd90c994":"code","14d67a38":"code","34026137":"code","801bc5e9":"code","49d741ad":"code","e41bfd99":"code","483d1603":"code","d9d06735":"code","f36aeaa3":"code","0f56e2fc":"markdown","144574ea":"markdown","5e847c39":"markdown","e5dbec9a":"markdown","93a8c38d":"markdown","9bc77a6b":"markdown","59e031a3":"markdown","50a5bb76":"markdown","990ef06f":"markdown","ae1eaa8a":"markdown","53cfdd7e":"markdown","33b34070":"markdown","6bb82a6d":"markdown"},"source":{"e806c3ef":"import math, re, os, random\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom tensorflow import keras\nfrom functools import partial\nfrom sklearn.model_selection import train_test_split\nprint(\"Tensorflow version \" + tf.__version__)","940e9fff":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","7c22a549":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path('tfflair-records')\nBATCH_SIZE = 24 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [256, 256] # was 512\nEPOCHS = 5\nfolds = 3\n\n","72946529":"GCS_PATH","9aff6dc0":"df_gs = pd.DataFrame(tf.io.gfile.glob(GCS_PATH + '\/*.tfrec'),columns = ['gs'])\n\ndf_gs","e176acfe":"from sklearn.model_selection import GroupKFold\n\n\ngkf  = GroupKFold(n_splits = folds)\ndf_gs['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(df_gs, groups = df_gs.gs.tolist())):\n\n    df_gs.loc[val_idx, 'fold'] = fold\n\ndf_gs\n\n","9a77b3f3":"[*IMAGE_SIZE]","625e689a":"def decode_image(image):\n    image = tf.image.decode_png(image, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image,[*IMAGE_SIZE])\n    #image = tf.reshape(image, [*IMAGE_SIZE])\n    return image","91c59cac":"def read_tfrecord(example, labeled):\n    tfrecord_format = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n         \"label\": tf.io.FixedLenSequenceFeature([], tf.int64,allow_missing=True)\n    } if labeled else {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    label = tf.cast(example['label'], tf.int32)\n    #label = tf.reshape(label,(1,num_classes))\n  \n    return image, label","56701b68":"def filter_fn(image, label):\n    \n    im_size = tf.math.reduce_mean(image)\n    \n    return tf.math.greater(im_size, 0.025)","daf28b8b":"def load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(partial(read_tfrecord, labeled=labeled), num_parallel_calls=AUTOTUNE)\n    dataset = dataset.filter(filter_fn)\n    return dataset","ae81df7c":"#TRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/*.tfrec')","689346c2":"TRAINING_FILENAMES, VALID_FILENAMES = train_test_split(\n    tf.io.gfile.glob(GCS_PATH + '\/*.tfrec'),\n    test_size=0.2, random_state=15 ## was 0.35\n)\n\n#TEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/test_tfrecords\/ld_test*.tfrec')","bc863a00":"dataset = load_dataset(TRAINING_FILENAMES, labeled=True)","cccade66":"for i in dataset.take(2):\n    \n    print(i[0].shape)","9771a39e":"dataset = tf.data.TFRecordDataset(VALID_FILENAMES, num_parallel_reads=AUTOTUNE)","d9224469":"dataset = dataset.with_options(tf.data.Options())","a6110fb3":"import sys\nimport numpy\nnumpy.set_printoptions(threshold=sys.maxsize)\n\nlabeled = True\n\ntfrecord_format = {\n    \"image\": tf.io.FixedLenFeature([], tf.string),\n    \"label\": tf.io.FixedLenSequenceFeature([], tf.int64,allow_missing=True)\n} if labeled else {\n    \"image\": tf.io.FixedLenFeature([], tf.string),\n    \"image_name\": tf.io.FixedLenFeature([], tf.string)\n    }\n\nj = 0\nfor i in dataset:\n    image = tf.io.parse_single_example(i,tfrecord_format)\n    if j == 1:\n        break\n    else:\n        j = j +1\n        #print(tf.reshape(tf.reduce_max(tf.one_hot(image['image'], num_classes, dtype=tf.int32), axis=0),(1,19)))\n\n","236ea281":"image = decode_image(image['image'])","63adcd8d":"tf.math.reduce_mean(image)","536269d9":"dataset.take(1)","3b7ff208":"image.shape","bca9a997":"plt.imshow(image)","b31b1e1f":"def data_augment(image, label):\n    # Thanks to the dataset.prefetch(AUTO) statement in the following function this happens essentially for free on TPU. \n    # Data pipeline code is executed on the \"CPU\" part of the TPU while the TPU itself is computing gradients.\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    ##image = tf.image.random_crop(image, [int(IMAGE_SIZE[0]), int(IMAGE_SIZE[0]), 3])\n    image = tf.image.rot90(image, k= random.randint(1, 4))\n    #image = dropout(image)\n    #label = tf.one_hot(label, num_classes, dtype=tf.int32)\n    #image = tf.image.draw_bounding_boxes(image, [64, 1, 32] , colors)\n    #image = tfa.image.rotate(image, tf.constant(np.pi\/8))\n    #image = tf.image.random_brightness(image, 0.2)\n    #image = tf.image.random_hue(image, 0.2)\n    #image = tf.image.random_saturation(image, 5, 10)\n    return image, label","c5c5965c":"def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)  \n    #dataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE)  \n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(30233)\n    #dataset = dataset.unbatch()\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","cfb2ab67":"def get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALID_FILENAMES, labeled=True, ordered=ordered) \n    dataset = dataset.batch(BATCH_SIZE)\n    #dataset = dataset.cache()\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","d023d24a":"def get_test_dataset(ordered=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","9f0c6f68":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","1fb2f843":"NUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALIDATION_IMAGES = count_data_items(VALID_FILENAMES)\n#NUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training images, {} validation images '.format(\n    NUM_TRAINING_IMAGES, NUM_VALIDATION_IMAGES))","edc1afbf":"NUM_VALIDATION_IMAGES","dd90c994":"dataset = load_dataset(TRAINING_FILENAMES, labeled=True) \ndataset = dataset.map(data_augment, num_parallel_calls=AUTOTUNE) ","14d67a38":"for i in dataset.take(3):\n    print((i[0].shape))","34026137":"# load our validation dataset for EDA\nvalidation_dataset = get_validation_dataset()\nvalidation_dataset = validation_dataset.unbatch().batch(20)\nvalid_batch = iter(validation_dataset)","801bc5e9":"# load data\ntrain_dataset = get_training_dataset()\nvalid_dataset = get_validation_dataset()","49d741ad":"!pip install -U efficientnet","e41bfd99":"import efficientnet.keras as eff\nfrom keras.layers import Dropout\n","483d1603":"STEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALID_STEPS = NUM_VALIDATION_IMAGES \/\/ BATCH_SIZE","d9d06735":"def unfreeze_model(model):\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers:#[-18:]:\n        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n             layer.trainable = True","f36aeaa3":"num_folds = [i for i in range(folds)]\n\nlr_rates = [3e-5]\n\nepsilons = [1e-4]\n\nfor fold in num_folds:\n\n\n    for lr in lr_rates:\n\n        for e in epsilons:\n\n            my_callbacks = [\n                #tf.keras.callbacks.EarlyStopping(patience=5,verbose=1,monitor='val_binary_accuracy'),\n                tf.keras.callbacks.ModelCheckpoint(filepath=f'Brain_flair_model_effect_{lr}_{e}.h5',verbose=1,monitor='val_binary_accuracy',save_best_only=True),\n                #tf.keras.callbacks.LearningRateScheduler(lr_schedul,verbose=1)\n                tf.keras.callbacks.ReduceLROnPlateau(monitor='val_binary_accuracy',factor=0.1,patience=3,min_lr=1e-30,mode='min',verbose=1,)\n\n            ]\n\n            with strategy.scope():       \n                #img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.efficientnet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n                img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.densenet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n                #img_adjust_layer = tf.keras.layers.Lambda(tf.keras.applications.resnet.preprocess_input, input_shape=[*IMAGE_SIZE, 3])\n\n                #base_model = eff.EfficientNetB0(weights='noisy-student', include_top=False) #imagenet noisy-student\n                base_model = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False)\n                #base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n                base_model.trainable = False\n\n                model = tf.keras.Sequential([\n                    tf.keras.layers.BatchNormalization(renorm=True),\n                    img_adjust_layer,\n                    base_model,\n                    tf.keras.layers.GlobalAveragePooling2D(),  ## Average\n                    tf.keras.layers.Dense(512, activation='relu', \n                    bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)),\n                    Dropout(0.5),\n                    #tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)), # L2 normalize embeddings\n                    #tf.keras.layers.Dense(1024, activation='relu', \n                    #bias_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.001)),\n                    #Dropout(0.5),\n\n                    #tf.keras.layers.BatchNormalization(renorm=True),\n                    tf.keras.layers.Dense(1, activation='sigmoid')  \n                ])\n\n                model.compile(\n                    optimizer=tf.keras.optimizers.Adam(learning_rate=lr,epsilon=e),\n                    #loss='binary_crossentropy',#loss_func,  \n                    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                    metrics=tf.keras.metrics.BinaryAccuracy()\n                )\n\n\n                unfreeze_model(model)\n\n                epochs = EPOCHS  # @param {type: \"slider\", min:8, max:50}\n\n                history = model.fit(train_dataset, \n                                    steps_per_epoch=STEPS_PER_EPOCH, \n                                    epochs=epochs,\n                                    #validation_split=0.1,\n                                    validation_data=valid_dataset,\n                                    #validation_steps=VALID_STEPS,\n                                   callbacks=my_callbacks)\n\n                history_frame = pd.DataFrame(history.history)\n                history_frame.loc[:, ['loss', 'val_loss']].plot()\n                history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n\n\n\n","0f56e2fc":"# Train the model\nAs our model is training you'll see a printout for each epoch, and can also monitor TPU usage by clicking on the TPU metrics in the toolbar at the top right of your notebook.","144574ea":"# Set up environment","5e847c39":"## Decode the data\nIn the code chunk below we'll set up a series of functions that allow us to convert our images into tensors so that we can utilize them in our model. We'll also normalize our data. Our images are using a \"Red, Blue, Green (RBG)\" scale that has a range of [0, 255], and by normalizing it we'll set each pixel's value to a number in the range of [0, 1]. ","e5dbec9a":"## Building our model\nIn order to ensure that our model is trained on the TPU, we build it using `with strategy.scope()`.    \n\nThis model was built using transfer learning, meaning that we have a _pre-trained model_ (ResNet50) as our base model and then the customizable model built using `tf.keras.Sequential`. If you're new to transfer learning I recommend setting `base_model.trainable` to **False**, but _do_ encourage you to change which base model you're using (more options are available in the **[`tf.keras.applications` Module](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications)** documentation) as well iterate on the custom model. \n\nNote that we're using `sparse_categorical_crossentropy` as our loss function, because we did _not_ one-hot encode our labels.","93a8c38d":"# Model\n","9bc77a6b":"# Introduction\n\n**References**  \nThis notebook was built using the following amazing resources created by :\n- **Martin Gorner:** [Getting Started: TPUs + Cassava Leaf Disease](https:\/\/www.kaggle.com\/jessemostipak\/getting-started-tpus-cassava-leaf-disease)\n\nThe Output of the notbook can be used in the inference notebook proviede earlier:\n\nhttps:\/\/www.kaggle.com\/lucamtb\/brain-tumor-very-basice-inference","59e031a3":"# Set up variables\n","50a5bb76":"## A note on using train_test_split()\nWhile I used `train_test_split()` to create both a `training` and `validation` dataset, consider exploring **[cross validation instead](https:\/\/www.kaggle.com\/dansbecker\/cross-validation)**.","990ef06f":"You can also modify the above code to look at your `validation` and `test` data, like this:","ae1eaa8a":"## Adding in augmentations \n","53cfdd7e":"# Load the data\nIf you've primarily worked with notebooks in Learn, you've maybe noticed that data import and formatting is taken care of for you. But because we're working with competition data we'll have to handle this part of the pipeline ourselves.   \n\nThe data we're working with have been formatted into `TFRecords`, which are a format for storing a sequence of binary records. `TFRecords` work _really_ well with TPUs, and allow us to send a small number of large files across the TPU for processing.   \n\nIf you'd like to learn more about `TFRecords` and maybe even try creating them yourself, check out this **[TFRecords Basics notebook](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics)** and **[corresponding video](https:\/\/youtu.be\/KgjaC9VeOi8)** from Kaggle Data Scientist Ryan Holbrook.  \n\nBecause our data consists of `training` and `test` images only, we're going to split our `training` data into `training` and `validation` data using the `train_test_split()` function. ","33b34070":"We'll use the following function to load our dataset. One of the advantages of a TPU is that we can run multiple files across the TPU at once, and this accounts for the speed advantages of using a TPU. To capitalize on that, we want to make sure that we're using data as soon as it streams in, rather than creating a data streaming bottleneck.","6bb82a6d":"## Define data loading methods\nThe following functions will be used to load our `training`, `validation`, and `test` datasets, as well as print out the number of images in each dataset."}}