{"cell_type":{"bb8f0e27":"code","13c01dd3":"code","5577564a":"code","caed5f3e":"code","5e054da0":"code","2dca7aab":"code","2c37165c":"code","dbfcf647":"code","de1a6ebc":"code","461dea42":"code","a345ed3a":"code","1f0cd2cf":"code","4a54016c":"code","47f84c34":"code","81ee9fdf":"code","48f20b5b":"code","e25e31f5":"code","94c9d556":"code","0d17faff":"code","f7a09be7":"code","dcc15791":"code","6f8bfd5b":"code","7f764d88":"code","d9a693cd":"code","346bafde":"code","895d9939":"code","c5e3b269":"code","cb2bbe89":"code","2602cd4f":"code","7b8bdba8":"code","c25559af":"code","cb12aaed":"code","67efa4a8":"code","53e68502":"code","94c3809b":"code","a2a4ce37":"code","67f21001":"code","0017e4c0":"code","8aeec73c":"code","1a434dd6":"code","2eb550ff":"code","04dae050":"code","2367e017":"code","10a52f93":"code","994a7cae":"code","aa8645a9":"code","df4e6e20":"code","ea1fad03":"code","225de491":"code","dbf3e8dd":"code","93f61c42":"code","68b9cda1":"code","ae167e42":"code","9e7f4d0f":"code","41476187":"code","6a54b23d":"code","e959d504":"code","8d71c769":"code","280abe17":"code","d2e883cd":"code","b5c2dd60":"code","016e3732":"code","ee32a8e7":"code","8b255d13":"code","92c1372a":"code","be38169e":"code","8ef25040":"code","45472136":"code","67802bfe":"markdown","80a72cfc":"markdown","2529d559":"markdown","9d9c0661":"markdown","86886d5d":"markdown","effdb232":"markdown","b3cef940":"markdown","f7b85331":"markdown","71892cf5":"markdown","7b5c0fd6":"markdown","183fea35":"markdown","09a736f9":"markdown","c1d32d43":"markdown","e827c5e0":"markdown","406b745a":"markdown","b2f92c32":"markdown","1780f795":"markdown","de1134b6":"markdown","84dec248":"markdown","b90511d9":"markdown","985cd6ac":"markdown","8369dadf":"markdown","8369f906":"markdown","04c2362a":"markdown","a544d817":"markdown","e349f5bb":"markdown","e6c3d333":"markdown","09bc7cd0":"markdown","fe67d5b9":"markdown","91c3761d":"markdown","0f01847b":"markdown","2972c9bb":"markdown","3a8e6679":"markdown","12e9f801":"markdown","f0c78be9":"markdown","3e35ae46":"markdown","bf3e303e":"markdown","dc5b86e2":"markdown","8a544505":"markdown","a1464d11":"markdown","bc3db5fe":"markdown","2d734d42":"markdown"},"source":{"bb8f0e27":"#import the necessary libraries\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#import the necessary libraries\nimport numpy as np\nimport pandas as pd\n\n#Importing libraries for visulization\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Library for Data Pre-processing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Imputer\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n#Traditional Classification Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\n#Decision Tree and other Ensemble Techniques\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n\n#Library for Model Evaluation \nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix, classification_report\nfrom sklearn.metrics import roc_auc_score, auc\nfrom sklearn.metrics import roc_curve\n\n#Other Libraries\nfrom collections import Counter\nfrom scipy import stats\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.decomposition import PCA\nfrom scipy.stats import zscore","13c01dd3":"#load the csv file and make the data frame\nvehicle_df = pd.read_csv('\/kaggle\/input\/vehicle\/vehicle.csv')","5577564a":"#display the first 5 rows of dataframe\nvehicle_df.head()","caed5f3e":"print(\"The dataframe has {} rows and {} columns\".format(vehicle_df.shape[0],vehicle_df.shape[1]))","5e054da0":"#display the information of dataframe\nvehicle_df.info()","2dca7aab":"#display in each column how many null values are there\nvehicle_df.apply(lambda x: sum(x.isnull()))","2c37165c":"#display 5 point summary of dataframe\n#vehicle_df.describe().transpose()\nvehicle_df.describe().T","dbfcf647":"sns.pairplot(vehicle_df,diag_kind='kde', hue='class')\nplt.show()","de1a6ebc":"#Corelation Matrix of attributes \nvehicle_df.corr()","461dea42":"#Function for Null values treatment\n\ndef null_values(base_dataset):\n    print(\"Shape of DataFrame before null treatment\",base_dataset.shape)\n    print(\"Null values count before treatment\")\n    print(\"===================================\")\n    print(base_dataset.isna().sum(),\"\\n\")\n    ## null value percentage     \n    null_value_table=(base_dataset.isna().sum()\/base_dataset.shape[0])*100\n    ## null value percentage beyond threshold drop , else treat the columns    \n    retained_columns=null_value_table[null_value_table<30].index\n    # if any variable as null value greater than input(like 30% of the data) value than those variable are consider as drop\n    drop_columns=null_value_table[null_value_table>30].index\n    base_dataset.drop(drop_columns,axis=1,inplace=True)\n    len(base_dataset.isna().sum().index)\n    #cont=base_dataset.describe().columns\n    cont=[col for col in base_dataset.select_dtypes(np.number).columns ]\n    cat=[i for i in base_dataset.columns if i not in base_dataset.describe().columns]\n    for i in cat:\n        base_dataset[i].fillna(base_dataset[i].value_counts().index[0],inplace=True)\n    for i in cont:\n        base_dataset[i].fillna(base_dataset[i].mean(),inplace=True)\n    print(\"Null values counts after treatment\")\n    print(\"===================================\")\n    print(base_dataset.isna().sum())\n    print(\"\\nShape of DataFrame after null treatment\",base_dataset.shape)","a345ed3a":"null_values(vehicle_df)","1f0cd2cf":"#display 5 point summary of new dataframe\n#vehicle_df.describe().transpose()\nvehicle_df.describe().T","4a54016c":"#display the shape of dataframe\nprint(\"Shape of dataframe after missing values treatment:\",vehicle_df.shape)","47f84c34":"#Distribution of data\n\nvehicle_df.hist( figsize=(15,15), color='red')\nplt.show()","81ee9fdf":"num_features=[col for col in vehicle_df.select_dtypes(np.number).columns ]\n\nplt.figure(figsize=(20,20))\nfor i,col in enumerate(num_features,start=1):\n    plt.subplot(5,4,i);\n    sns.distplot(vehicle_df[col])\nplt.show()","48f20b5b":"num_features=[col for col in vehicle_df.select_dtypes(np.number).columns ]\n\nplt.figure(figsize=(20,20))\nfor i,col in enumerate(num_features,start=1):\n    plt.subplot(5,4,i);\n    sns.boxplot(vehicle_df[col]);\nplt.show()\n","e25e31f5":"num_features=[col for col in vehicle_df.select_dtypes(np.number).columns ]\n\nplt.figure(figsize=(20,20))\nfor i,col in enumerate(num_features,start=1):\n    plt.subplot(5,4,i);\n    sns.boxplot(vehicle_df['class'],vehicle_df[col]);\nplt.show()","94c9d556":"vehicle_df.skew()","0d17faff":"def outliers_transform_with_drop_record(base_dataset):\n    num_features=[col for col in base_dataset.select_dtypes(np.number).columns ]\n    print(\"Outliers in Dataset before Treatment\")\n    print(\"====================================\")\n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        count=(base_dataset[base_dataset[cols]>utv][cols].count())+(base_dataset[base_dataset[cols]<ltv][cols].count()) \n        print(\"Column \",cols,\"\\t has \",count,\" outliers\")\n        \n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        for p in x:\n            if p <ltv or p>utv:\n                base_dataset.drop(base_dataset[base_dataset[cols]>utv].index, axis=0, inplace=True)\n                base_dataset.drop(base_dataset[base_dataset[cols]<ltv].index, axis=0, inplace=True)\n    \n    print(\"\\nOutliers in Dataset after Treatment\")\n    print(\"====================================\")\n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        count=(base_dataset[base_dataset[cols]>utv][cols].count())+(base_dataset[base_dataset[cols]<ltv][cols].count()) \n        print(\"Column \",cols,\"\\t has \",count,\" outliers\")","f7a09be7":"#outliers_transform_with_drop_record(vehicle_df)","dcc15791":"def outliers_transform_with_replace_mean(base_dataset):\n    num_features=[col for col in base_dataset.select_dtypes(np.number).columns ]\n    print(\"Outliers in Dataset before Treatment\")\n    print(\"====================================\")\n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        count=(base_dataset[base_dataset[cols]>utv][cols].count())+(base_dataset[base_dataset[cols]<ltv][cols].count()) \n        print(\"Column \",cols,\"\\t has \",count,\" outliers\")\n        \n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        y=[]\n        for p in x:\n            if p <ltv or p>utv:\n                y.append(np.mean(x))\n            else:\n                y.append(p)\n        base_dataset[cols]=y\n                \n    print(\"\\nOutliers in Dataset after Treatment\")\n    print(\"====================================\")\n    for i,cols in enumerate(num_features,start=1):\n        x = base_dataset[cols]\n        qr3, qr1=np.percentile(x, [75,25])\n        iqr=qr3-qr1\n        utv=qr3+(1.5*(iqr))\n        ltv=qr1-(1.5*(iqr))\n        count=(base_dataset[base_dataset[cols]>utv][cols].count())+(base_dataset[base_dataset[cols]<ltv][cols].count()) \n        print(\"Column \",cols,\"\\t has \",count,\" outliers\")","6f8bfd5b":"outliers_transform_with_replace_mean(vehicle_df)","7f764d88":"#display how many are car,bus,van. \nnew_vehicle_df['class'].value_counts()","d9a693cd":"sns.countplot(new_vehicle_df['class'])\nplt.show()","346bafde":"#find the correlation between independent variables\nplt.figure(figsize=(20,5))\nsns.heatmap(vehicle_df.corr(),annot=True)\nplt.show()","895d9939":"corr = vehicle_df.drop('class', axis=1).corr() # We already examined SalePrice correlations\nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","c5e3b269":"vehicle_df.replace({'car':0,'bus':1,'van':2},inplace=True)","cb2bbe89":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        print(\"Train Result:\\n=============\")\n        print(f\"accuracy score: {accuracy_score(y_train, pred):.4f}\\n\")\n        #print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_train, pred,average=None)}\\n\\tRecall Score: {recall_score(y_train, pred,average=None)}\\n\\tF1 score: {f1_score(y_train, pred,average=None)}\\n\")\n        print(f\"Confusion Matrix:\\n=================\\n {confusion_matrix(y_train, clf.predict(X_train))}\\n\")\n        print(\"Classification Report:\\n======================\\n\",classification_report(y_train, pred))\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        print(\"Test Result:\\n============\")        \n        print(f\"accuracy score: {accuracy_score(y_test, pred)}\\n\")\n        #print(f\"Classification Report: \\n \\tPrecision: {precision_score(y_test, pred,average=None)}\\n\\tRecall Score: {recall_score(y_test, pred,average=None)}\\n\\tF1 score: {f1_score(y_test, pred,average=None)}\\n\")\n        print(f\"Confusion Matrix:\\n===============\\n {confusion_matrix(y_test, pred)}\\n\")\n        print(\"Classification Report:\\n======================\\n\",classification_report(y_test, pred))","2602cd4f":"#now separate the dataframe into dependent and independent variables\nX = vehicle_df.drop('class',axis=1)\nY = vehicle_df['class']\nprint(\"shape of X :\", X.shape)\nprint(\"shape of Y :\", Y.shape)","7b8bdba8":"from sklearn.model_selection import cross_val_score, train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=5)","c25559af":"from sklearn.svm import SVC\n\nlsvm = SVC(kernel='linear')\nlsvm.fit(X_train, y_train)\n\nprint_score(lsvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(lsvm, X_train, y_train, X_test, y_test, train=False)\n\n\nlsvm_accuracy=accuracy_score(y_test, lsvm.predict(X_test))","cb12aaed":"from sklearn.svm import SVC\n\npsvm = SVC(kernel='poly', degree=2, gamma='auto')\npsvm.fit(X_train, y_train)\n\nprint_score(psvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(psvm, X_train, y_train, X_test, y_test, train=False)\n\nlsvm_accuracy=accuracy_score(y_test, psvm.predict(X_test))","67efa4a8":"from sklearn.svm import SVC\n\nrsvm = SVC(kernel='rbf', gamma=1)\nrsvm.fit(X_train, y_train)\n\nprint_score(rsvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(rsvm, X_train, y_train, X_test, y_test, train=False)\n\nrsvm_accuracy=accuracy_score(y_test, rsvm.predict(X_test))","53e68502":"from sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler()\nX_std = sc.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_std, Y, test_size=0.3, random_state=5)","94c3809b":"print(\"=======================Linear Kernel SVM==========================\")\n\nfrom sklearn.svm import SVC\n\nlsvm = SVC(kernel='linear')\nlsvm.fit(X_train, y_train)\n\nprint_score(lsvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(lsvm, X_train, y_train, X_test, y_test, train=False)\n\nlsvm_accuracy=accuracy_score(y_test, lsvm.predict(X_test))\n\nprint(\"=======================Polynomial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\npsvm = SVC(kernel='poly', degree=2, gamma='auto')\npsvm.fit(X_train, y_train)\n\nprint_score(psvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(psvm, X_train, y_train, X_test, y_test, train=False)\n\npsvm_accuracy=accuracy_score(y_test, psvm.predict(X_test))\n\nprint(\"=======================Radial Kernel SVM==========================\")\nfrom sklearn.svm import SVC\n\nrsvm = SVC(kernel='rbf', gamma=1)\nrsvm.fit(X_train, y_train)\n\nprint_score(rsvm, X_train, y_train, X_test, y_test, train=True)\nprint_score(rsvm, X_train, y_train, X_test, y_test, train=False)\n\nrsvm_accuracy=accuracy_score(y_test, rsvm.predict(X_test))\n","a2a4ce37":"result = pd.DataFrame({'Model' : ['SVM Linear', 'SVM Polynomial', 'SVM Redial'], \n                       'Test Accuracy' : [lsvm_accuracy, psvm_accuracy, rsvm_accuracy],\n                      })\nresult","67f21001":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'C': [0.01, 0.1, 0.5, 1, 10, 100], \n              'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001], \n              'kernel': ['rbf', 'poly', 'linear']} \n\ngrid = GridSearchCV(SVC(), param_grid, refit=True, verbose=1, cv=5, iid=True)\n\ngrid.fit(X_train, y_train)\n\nprint_score(grid, X_train, y_train, X_test, y_test, train=True)\nprint_score(grid, X_train, y_train, X_test, y_test, train=False)","0017e4c0":"from sklearn.model_selection import KFold, cross_val_score\n\n\nkfold = KFold(n_splits= 10, random_state = 1)\n\n#instantiate the object\nsvc = SVC(kernel='linear') \n\n\n#now we will train the model with raw data\n\nresults = cross_val_score(estimator = svc, X = X_train, y = y_train, cv = kfold)\n\nprint(results,\"\\n\")\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean()*100, results.std()*100 * 2))\n\nkf_accuracy=results.mean()","8aeec73c":"from sklearn.model_selection import RepeatedKFold\n\nX = vehicle_df.drop('class',axis=1).values\ny = vehicle_df['class'].values\n\naccuracies = []\n#lr = LogisticRegression(random_state = 1)\nsvc = SVC(kernel='linear') \n\nrkf = RepeatedKFold(n_splits = 10, n_repeats= 3, random_state = 1)\n\nfor train_index, test_index in rkf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    svc.fit(X_train, y_train)\n    accuracies.append(accuracy_score(y_test, svc.predict(X_test)))\n\nprint(np.round(accuracies, 3),\"\\n\")\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (np.mean(accuracies)*100, np.std(accuracies)*100 * 2))\n\nrkf_accuracy=np.mean(accuracies)","1a434dd6":"result = pd.DataFrame({'Model' : ['Linear SVM', 'Linear SVM K-Fold', 'Linear SVM Repeated K-Fold'], \n                       'Accuracy' : [lsvm_accuracy, kf_accuracy, rkf_accuracy],\n                      })\nresult","2eb550ff":"#now sclaed the features attribute and replace the target attribute values with number\nX = vehicle_df.drop('class',axis=1)\ny = vehicle_df['class']\n\nX_scaled = X.apply(zscore)","04dae050":"#make the covariance matrix and we have 18 independent features so aur covariance matrix is 18*18 matrix\ncov_matrix = np.cov(X_scaled,rowvar=False)\nprint(\"cov_matrix shape:\",cov_matrix.shape)\nprint(\"Covariance_matrix\",cov_matrix)","2367e017":"#now with the help of above covariance matrix we will find eigen value and eigen vectors\npca = PCA(n_components=18)\npca.fit(X_scaled)","10a52f93":"#display explained variance ratio\npca_to_learn_variance.explained_variance_ratio_","994a7cae":"#display explained variance\npca_to_learn_variance.explained_variance_","aa8645a9":"#display principal components\npca_to_learn_variance.components_","df4e6e20":"plt.bar(list(range(1,19)),pca_to_learn_variance.explained_variance_ratio_)\nplt.xlabel(\"eigen value\/components\")\nplt.ylabel(\"variation explained\")\nplt.show()","ea1fad03":"plt.step(list(range(1,19)),np.cumsum(pca_to_learn_variance.explained_variance_ratio_))\nplt.xlabel(\"eigen value\/components\")\nplt.ylabel(\"cummalative of variation explained\")\nplt.show()","225de491":"#use first 8 principal components\npca_8c = PCA(n_components=8)\npca_8c.fit(X_scaled)","dbf3e8dd":"#transform the raw data which is in 18 dimension into 8 new dimension with pca\nX_scaled_pca_8c = pca_8c.transform(X_scaled)","93f61c42":"#display the shape of new_vehicle_df_pca_independent_attr\nX_scaled_pca_8c.shape","68b9cda1":"#now split the data into 80:20 ratio\nrawdata_X_train,rawdata_X_test,rawdata_y_train,rawdata_y_test = train_test_split(X_scaled,Y,test_size=0.20,random_state=1)\npca_X_train,pca_X_test,pca_y_train,pca_y_test = train_test_split(X_scaled_pca_8c,Y,test_size=0.20,random_state=1)","ae167e42":"print(\"shape of rawdata_X_train\",rawdata_X_train.shape)\nprint(\"shape of rawdata_y_train\",rawdata_y_train.shape)\nprint(\"shape of rawdata_X_test\",rawdata_X_test.shape)\nprint(\"shape of rawdata_y_test\",rawdata_y_test.shape)\nprint(\"--------------------------------------------\")\nprint(\"shape of pca_X_train\",pca_X_train.shape)\nprint(\"shape of pca_y_train\",pca_y_train.shape)\nprint(\"shape of pca_X_test\",pca_X_test.shape)\nprint(\"shape of pca_y_test\",pca_y_test.shape)","9e7f4d0f":"from sklearn.model_selection import KFold, cross_val_score\n\n\nkfold = KFold(n_splits= 10, random_state = 1)\n\nsvc = SVC() #instantiate the object\n\n#now we will train the model with raw data\n\nresults = cross_val_score(estimator = svc, X = rawdata_X_train, y = rawdata_y_train, cv = kfold)\n\nprint(results,\"\\n\")\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean()*100, results.std()*100 * 2))\n\nsns.boxplot(results)\nplt.show()","41476187":"svc.fit(rawdata_X_train,rawdata_y_train)\n\nprint(\"Raw Data Training Accuracy :\\t \", svc.score(rawdata_X_train, rawdata_y_train))\n\nraw_train_accuracy=svc.score(rawdata_X_train, rawdata_y_train)\n\n#Scoring the model on test_data\nprint(\"Raw Data Testing Accuracy :\\t  \",  svc.score(rawdata_X_test, rawdata_y_test))\n\nraw_test_accuracy=svc.score(rawdata_X_test, rawdata_y_test)\n\ny_pred = svc.predict(rawdata_X_test)","6a54b23d":"print(classification_report(rawdata_y_test, svc.predict(rawdata_X_test)))","e959d504":"#now fit the model on pca data with new dimension\n\nfrom sklearn.model_selection import KFold, cross_val_score\n\nkfold = KFold(n_splits= 10, random_state = 1)\n\nsvc = SVC() #instantiate the object\n\n#now train the model with pca data with new dimension\n\npca_results = cross_val_score(estimator = svc, X = pca_X_train, y = pca_y_train, cv = kfold)\n\nprint(pca_results,\"\\n\")\n\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (pca_results.mean()*100, pca_results.std()*100 * 2))\n\nsns.boxplot(pca_results)\nplt.show()","8d71c769":"svc.fit(pca_X_train,pca_y_train)\n\nprint(\"PCA data Training Accuracy :\\t \", svc.score(pca_X_train, pca_y_train))\n\npca_train_accuracy=svc.score(pca_X_train, pca_y_train)\n\n#Scoring the model on test_data\nprint(\"PCA data Testing Accuracy :\\t  \",  svc.score(pca_X_test, pca_y_test))\n\npca_test_accuracy=svc.score(pca_X_test, pca_y_test)\n","280abe17":"print(classification_report(pca_y_test, svc.predict(pca_X_test)))","d2e883cd":"#display confusion matrix of both models\nprint(\"Confusion matrix with raw data(18 dimension)\\n\",confusion_matrix(rawdata_y_test,rawdata_y_predict))\nprint(\"Confusion matrix with pca data(8 dimension)\\n\",confusion_matrix(pca_y_test,pca_y_predict))","b5c2dd60":"result = pd.DataFrame({'TrainTest' : ['raw_train_accuracy', 'raw_test_accuracy', 'pca_train_accuracy','pca_test_accuracy'], \n                       'Accuracy' : [raw_train_accuracy,raw_test_accuracy, pca_train_accuracy, pca_test_accuracy],\n                      })\nresult","016e3732":"#drop the columns\nX_scaled.drop(['max.length_rectangularity','scaled_radius_of_gyration','skewness_about.2','scatter_ratio','elongatedness','pr.axis_rectangularity','scaled_variance','scaled_variance.1'],axis=1,inplace=True)","ee32a8e7":"#display the shape of new dataframe\nX_scaled.shape","8b255d13":"dropcolumn_X_train,dropcolumn_X_test,dropcolumn_y_train,dropcolumn_y_test = train_test_split(X_scaled,Y,test_size=0.20,random_state=1)","92c1372a":"print(\"shape of dropcolumn_X_train\",dropcolumn_X_train.shape)\nprint(\"shape of dropcolumn_y_train\",dropcolumn_y_train.shape)\nprint(\"shape of dropcolumn_X_test\",dropcolumn_X_test.shape)\nprint(\"shape of dropcolumn_y_test\",dropcolumn_y_test.shape)","be38169e":"#fit the model on dropcolumn_X_train,dropcolumn_y_train\nsvc.fit(dropcolumn_X_train,dropcolumn_y_train)","8ef25040":"#predict the y value\ndropcolumn_y_predict = svc.predict(dropcolumn_X_test)","45472136":"#display the accuracy score and confusion matrix\nprint(\"Accuracy score with dropcolumn data(10 dimension)\",accuracy_score(dropcolumn_y_test,dropcolumn_y_predict))\nprint(\"Confusion matrix with dropcolumn data(10 dimension)\\n\",confusion_matrix(dropcolumn_y_test,dropcolumn_y_predict))","67802bfe":"#Overview\nWelcome to my kernel!\nData Description: The data contains features extracted from the silhouette of vehicles in different angles. Four \"Corgie\" model vehicles were used for the experiment: a double decker bus, Cheverolet van, Saab 9000 and an Opel Manta 400 cars. This particular combination of vehicles was chosen with the expectation that the bus, van and either one of the cars would be readily distinguishable, but it would be more difficult to distinguish between the cars.","80a72cfc":"From above we can see that 8 dimension are able to explain 95%variance of data. so we will use first 8 principal components","2529d559":"From above we can see that max null values is 6 which are in two columns 'radius_ratio', 'skewness_about'.\nso we have two options either we will drop those null values or we will impute those null values.\nDropping null values is not a good way because we will lose some information.but we will go with both options then we will see what's the effect on model.","9d9c0661":"From above we can see that by reducing 10 dimension we are achieving 94% accuracy","86886d5d":"#Dropping Missing Values","effdb232":"From above pair plots we can see that many columns are correlated and many columns have long tail so that is the indication of outliers.we will see down the line with the help of correlation matrix what's the strength of correlation and outliers are there or not.","b3cef940":"From above we can see that there are outliers in pr.axis_aspect_ratio column and there is right skewness because long tail is at right side(mean>median)","f7b85331":"Principal Component Analysis is an unsupervised learning class of statistical techniques used to explain data in high dimension using small number of variables called the principal components. Principal components are the linear combinations of the original variables in the dataset. As it will explain high dimension data with small number of variables. The big disadvantage is we cannot do interpretation with the model.In other words model with pca will become blackbox.   \nIn pca first we have to find the covariance matrix after that from that covariance matrix we have to find eigen vectors and eigen values. There is mathematical way to find eigen vectors and eigen values. i will attach the link of how to find the eigen value and eigen vector. Corresponding to each eigen vector there is eigen value. after that we have to sort the eigen vector by decreasing eigen values and choose k eigen vectors with the largest eigen value. ","71892cf5":"after seeing the max values of above outliers column. it's looks like outliers in above columns are natural not a typo mistake or artificial.\nNote: It's my assumption only. as there is no way to prove whether these outliers are natural or artificial.\nAs we know that mostly algorithms are affected by outliers and outliers may affect the model.as we will apply SVM on above data which is affected by outliers. so better to drop those outliers.","7b5c0fd6":"First let's create a new dataframe and then we will impute the missing values.","183fea35":"**With PCA**","09a736f9":"#Analysis of each column with the help of plots","c1d32d43":"Linear Kernel SVM","e827c5e0":"Ploynomial Kernel SVM","406b745a":"now, again we have two option we will drop those above eight columns manually or we will apply pca and let pca to be decided how it will explain above data which is in high dimension with smaller number of variables.\nwe will see both approaches.","b2f92c32":"#With dropping the above mentioned columns Manually","1780f795":"From above we can see that there are outliers in max.length_aspect_ratio and there is a right skewness because long tail is at right side(mean>median)","de1134b6":"**Repeated Kfold Cross Validation**","84dec248":"#Importing the Libraries and Basic EDA","b90511d9":"From above we can see that there are no outliers in compactness column and it's looks like normally distributed.","985cd6ac":"From above we can see that except 'class' column all columns are numeric type and there are null values in some columns.\nclass column is our target column.","8369dadf":"#Objective:\nThe objective is to classify a given silhouette as one of three types of vehicle, using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles.","8369f906":"From above we can see that there are no outliers in distance_circularity column but in distribution plot we can see that there are two peaks and we can see that there is right skewness because long tail is at the right side(mean>median)","04c2362a":"so now we have new dataframe called new_vehicle_df and we will make changes in this new dataframe.","a544d817":"SVM on Scaled Data","e349f5bb":"**K-Fold Cross Validation**","e6c3d333":"From above we can see that our data has missing values in some column. so before building any model we have to handle missing values. we have two option either we will drop those missing values or we will impute missing values. we will go with both options and see what's the effect on model. so first we will drop the missing values. Before dropping missing values we will create another dataframe and copy the original dataframe data into that. It's a good practice to keep the original dataframe as it is and make all modifications to the new dataframe.","09bc7cd0":"**Without PCA**","fe67d5b9":"SVM Classifier (Before PCA)","91c3761d":"#Conclusion:\nFrom above we can see that pca is doing a very good job.Accuracy with pca is approx 94% and with raw data approx 96% but note that pca 94% accuracy is with only 8 dimension where as rawdata has 18 dimension.But every thing has two sides, disadvantage of pca is we cannot do interpretation with the model.it's blackbox.","0f01847b":"From above we can see that cars are most followed by bus and then vans.","2972c9bb":"Vehicle Recognition","3a8e6679":"now before apply pca with 8 dimension which are explaining more than 95% variantion of data we will make model on raw data after that we will make model with pca and then we will compare both models.","12e9f801":"From above we can see that there are no outliers in circularity column and it's looks like normally distributed","f0c78be9":"so our objective is to reocgnize whether an object is a van or bus or car based on some input features.\nso our main assumption is there is little or no multicollinearity between the features.\nif two features is highly correlated then there is no use in using both features.in that case, we can drop one feature. \nso heatmap gives us the correlation matrix there we can see which features are highly correlated.\nFrom above correlation matrix we can see that there are many features which are highly correlated. if we see carefully then scaled_variance.1 and scatter_ratio has 1 correlation and many other features also there which having more than 0.9 correlation\nso we will drop those columns whose correlation is +-0.9 or above.\nso there are 8 such columns:\n->max.length_rectangularity\n->scaled_radius_of_gyration\n->skewness_about.2\n->scatter_ratio\n->elongatedness\n->pr.axis_rectangularity\n->scaled_variance\n->scaled_variance.1","3e35ae46":"**Support Vector Machine Hyperparameter tuning**","bf3e303e":"From above we can see that there are outliers in radius_ratio column and there is right skewness because long tail is at the right side(mean>median)","dc5b86e2":"#With Principal Component Analysis(PCA) ","8a544505":"#Fix Outliers after dropping missing values","a1464d11":"Redial Kernel SVM","bc3db5fe":"so by now we analyze each column and we found that there are outliers in some column. now our next step is to know whether these outliers are natural or artificial. if natural then we have to do nothing but if these outliers are artificial then we have to handle these outliers.\nwe have 8 columns in which we found outliers:\n->radius_ratio\n->pr.axis_aspect_ratio\n->max.length_aspect_ratio\n->scaled_variance\n->scaled_variance.1\n->scaled_radius_of_gyration.1\n->skewness_about\n->skewness_about.1","2d734d42":"Thanks for reading the kernel!\nHappy Learning:)"}}