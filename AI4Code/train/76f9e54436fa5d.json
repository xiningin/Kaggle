{"cell_type":{"cd3f14fa":"code","a9910e3b":"code","71be314c":"code","5e3c97f0":"code","71ea5940":"code","dc9a7a3e":"code","e54a5ec7":"code","e1ceb8fd":"code","4e2db773":"code","dbd50f57":"code","444f2938":"code","ca72352b":"code","42288b86":"code","c5412775":"code","13d66c15":"code","081e8bbd":"code","ee7abe0f":"code","edd78f55":"code","8d60fa52":"code","da4909d6":"code","a56c6d57":"code","a4c46af2":"code","7b132619":"code","852816f5":"code","2ed909fe":"code","e5e12678":"code","7c3b9f9f":"code","d30ba53e":"code","141793eb":"code","cadff910":"code","d48a3fa5":"code","ba03c028":"code","37662c0a":"code","64a46f50":"code","e4560160":"code","9126104c":"code","f3d43adb":"code","c6b9fa25":"code","b6fe5e8a":"code","3c0af7cd":"code","49af7b27":"code","f667cfea":"code","ba7d23fe":"markdown","794f9a52":"markdown","0901456c":"markdown","145557ba":"markdown","4e9b3f92":"markdown","21d2ae02":"markdown","12db5e5a":"markdown","6c438715":"markdown","c8ed64ab":"markdown","d01f2dd0":"markdown","947bf546":"markdown","e6ace4b6":"markdown","95758734":"markdown","6dcc4b06":"markdown","339d32d4":"markdown","f870d1af":"markdown","6322882b":"markdown","2ea88dfa":"markdown","5195bd50":"markdown","19a6c614":"markdown","5764bb5a":"markdown","79d63c04":"markdown","92dd605c":"markdown","0a050ecc":"markdown","97b19874":"markdown","2b55d3dd":"markdown","5428ea54":"markdown","2f55bc57":"markdown","06150b45":"markdown","a7daa043":"markdown","0a4b7a37":"markdown","02f7ca57":"markdown","345c6437":"markdown"},"source":{"cd3f14fa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns # plot grafics\nsns.set(color_codes=True)\n\nimport warnings # ignorar alertas desnecess\u00e1rios\nwarnings.filterwarnings('ignore')","a9910e3b":"df_treino = pd.read_csv('..\/input\/iris-train.csv')\ndf_teste = pd.read_csv(\"..\/input\/iris-test.csv\")","71be314c":"df_treino.shape, df_teste.shape","5e3c97f0":"df_treino.sample(10).T","71ea5940":"df_teste.sample(10).T","dc9a7a3e":"df_treino.describe().T","e54a5ec7":"df_treino.dtypes","e1ceb8fd":"df_treino.nunique()","4e2db773":"df_treino.isnull().sum()","dbd50f57":"# verificando correla\u00e7\u00e3o entre as vari\u00e1veis \n\ndf_treino.corr()","444f2938":"# gerar um gr\u00e1fico de dispers\u00e3o simples\ndf_treino.plot(kind=\"scatter\", x=\"SepalLengthCm\", y='SepalWidthCm')","ca72352b":"# gerar gr\u00e1fico para analisar pares de caracter\u00edsticas \nsns.pairplot(df_treino, hue='Species')","42288b86":"sns.heatmap(df_treino.corr(), annot=True, cmap='cubehelix_r')","c5412775":"df_treino.describe().T","13d66c15":"sns.boxplot(x=\"Species\", y='SepalLengthCm', data=df_treino)\n#sns.boxplot(x=\"Species\", y='SepalWidthCm', data=df_treino)\n#sns.boxplot(x=\"Species\", y='PetalLengthCm', data=df_treino)\n#sns.boxplot(x=\"Species\", y='PetalWidthCm', data=df_treino)\n","081e8bbd":"sns.violinplot(x=\"Species\", y=\"SepalLengthCm\", data=df_treino, hue='Species')\n#sns.violinplot(x=\"Species\", y=\"SepalWidthCm\", data=df_treino, hue='Species')\n#sns.violinplot(x=\"Species\", y=\"PetalLengthCm\", data=df_treino, hue='Species')\n#sns.violinplot(x=\"Species\", y=\"PetalWidthCm\", data=df_treino, hue='Species')","ee7abe0f":"feat = df_treino.iloc[:,0:-1]# Selecionando todas as linhas, da primeira coluna at\u00e9 a pen\u00faltima coluna.\ntarg = df_treino.iloc[:,-1] # Selecionando todas as linhas da \u00faltima coluna ['Class'].\n","edd78f55":"\nfrom sklearn.model_selection import train_test_split\n\nfeat_train, feat_test, targ_train, targ_test = train_test_split(feat, targ, random_state=42)\n","8d60fa52":"\nprint('feat treino',feat_train.shape)\nprint('feat test',feat_test.shape)\nprint('targ treino',targ_train.shape)\nprint('targ test',targ_test.shape)","da4909d6":"from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n# from xgboost import XGBClassifier - este caso tem que instalar antes via terminal\n\n\n# medi\u00e7\u00e3o da acur\u00e1cia do modelo\nfrom sklearn import metrics","a56c6d57":"\nlogreg = LogisticRegression(random_state=42) # Criando o modelo\n","a4c46af2":"feat_train.sample(5)","7b132619":"logreg.fit(feat_train, targ_train) # Treinando o modelo","852816f5":"feat_test.sample(5)","2ed909fe":"pred = logreg.predict(feat_test) # predizendo\n","e5e12678":"acc_logreg = round(metrics.accuracy_score(pred,targ_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(\"{}% de acur\u00e1cia\\n\".format(acc_logreg,))\nprint(metrics.confusion_matrix(targ_test, pred))\n\n\n#print(\"Gravando arquivo para submiss\u00e3o...\\n\")\n\n#df_teste_lg = df_teste.copy()\n\n#df_teste_lg['Species'] = logreg.predict(df_teste_lg)\n#df_teste_lg[['Id','Species']].to_csv('logreg_rodrigoaragao.csv', index=False)","7c3b9f9f":"# GaussianNB\n\ngaussian=GaussianNB() # Criando o modelo\ngaussian.fit(feat_train, targ_train) # Treinando o modelo\npred=gaussian.predict(feat_test) # predizendo\nacc_gaussian=round(metrics.accuracy_score(pred, targ_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_gaussian,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\n\n#print(\"Gravando arquivo para submiss\u00e3o...\\n\")\n\n#df_teste_gau = df_teste.copy()\n\n#df_teste_gau['Species'] = logreg.predict(df_teste_gau)\n#df_teste_gau[['Id','Species']].to_csv('gaussian_rodrigoaragao.csv', index=False)","d30ba53e":"# DecisionTreeClassifier\n\ntree=DecisionTreeClassifier(random_state=42) # Criando o modelo\ntree.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_tree=round(metrics.accuracy_score(pred,targ_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_tree,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_tree = df_teste.copy()\n\ndf_teste_tree['Species'] = logreg.predict(df_teste_tree)\ndf_teste_tree[['Id','Species']].to_csv('tree_rodrigoaragao.csv', index=False)","141793eb":"# RandomForestClassifier\n\nrfc=RandomForestClassifier(random_state=42) # Criando o modelo\nrfc.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_rfc=round(metrics.accuracy_score(pred,targ_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_rfc,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_rfc = df_teste.copy()\n\ndf_teste_rfc['Species'] = logreg.predict(df_teste_rfc)\ndf_teste_rfc[['Id','Species']].to_csv('rfc_rodrigoaragao.csv', index=False)","cadff910":"# ExtraTreesClassifier\n\netc=ExtraTreesClassifier(random_state=42) # Criando o modelo\netc.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_etc=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_etc,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_etc = df_teste.copy()\n\ndf_teste_etc['Species'] = logreg.predict(df_teste_etc)\ndf_teste_etc[['Id','Species']].to_csv('etc_rodrigoaragao.csv', index=False)","d48a3fa5":"# AdaBoostClassifier\n\nabc=AdaBoostClassifier(random_state=42) # Criando o modelo\nabc.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_abc=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_abc,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_abc = df_teste.copy()\n\ndf_teste_abc['Species'] = logreg.predict(df_teste_abc)\ndf_teste_abc[['Id','Species']].to_csv('abc_rodrigoaragao.csv', index=False)","ba03c028":"# SVC\n\nsvc=SVC(random_state=42) # Criando o modelo\nsvc.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_svc=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_svc,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_svc = df_teste.copy()\n\ndf_teste_svc['Species'] = logreg.predict(df_teste_svc)\ndf_teste_svc[['Id','Species']].to_csv('svc_rodrigoaragao.csv', index=False)","37662c0a":"# XGBClassifier\n\n# TEM QUE INSTALAR...\n\n#from xgboost import XGBClassifier\n\n#xgb=XGBClassifier(random_state=42) # Criando o modelo\n#xgb.fit(feat_train, targ_train) # Treinando o modelo\n#pred=tree.predict(feat_test) # predizendo\n#acc_xgb=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\n#print(acc_xgb,\"% de acur\u00e1cia\")\n#print(metrics.confusion_matrix(targ_test, pred))","64a46f50":"# GradientBoostingClassifier\n\ngbc=GradientBoostingClassifier(random_state=42) # Criando o modelo\ngbc.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_gbc=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_gbc,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_gbc = df_teste.copy()\n\ndf_teste_gbc['Species'] = logreg.predict(df_teste_gbc)\ndf_teste_gbc[['Id','Species']].to_csv('gbc_rodrigoaragao.csv', index=False)","e4560160":"# KNeighborsClassifier\n\nknn=KNeighborsClassifier() # Criando o modelo\nknn.fit(feat_train, targ_train) # Treinando o modelo\npred=tree.predict(feat_test) # predizendo\nacc_knn=round(metrics.accuracy_score(pred,targ_test)*100, 1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\nprint(acc_knn,\"% de acur\u00e1cia\")\nprint(metrics.confusion_matrix(targ_test, pred))\n\ndf_teste_knn = df_teste.copy()\n\ndf_teste_knn['Species'] = logreg.predict(df_teste_knn)\ndf_teste_knn[['Id','Species']].to_csv('knn_rodrigoaragao.csv', index=False)","9126104c":"# importando cross_val_score\nfrom sklearn.model_selection import cross_val_score","f3d43adb":"models = {'RandomForest': RandomForestClassifier(random_state=42),\n          'ExtraTrees': ExtraTreesClassifier(random_state=42),\n          'GBM': GradientBoostingClassifier(random_state=42),\n          'DecisionTree': DecisionTreeClassifier(random_state=42),\n          'AdaBoost': AdaBoostClassifier(random_state=42),\n          'KNN 1': KNeighborsClassifier(n_neighbors=1),\n          'KNN 3': KNeighborsClassifier(n_neighbors=3),\n          'KNN 11': KNeighborsClassifier(n_neighbors=11),\n          'SVC': SVC()}\n          #'LinearRegression': LinearRegression()}","c6b9fa25":"# feat_train, feat_test, targ_train, targ_test\n\ndef run_model(model, feat_train, feat_test, targ_train, targ_test):\n    model.fit(feat_train, targ_train) # comando '.fit' \u00e9 para treinar o modelo\n    preds = model.predict(feat_test) # comeando '.predict' \u00e9 para prever\n    return round(metrics.accuracy_score(preds, targ_test)*100,1) # avaliando a acur\u00e1cia. previs\u00f5es x resultados reais\n","b6fe5e8a":"scores = []\nfor name, model in models.items():\n    score = run_model(model, feat_train, feat_test, targ_train, targ_test)\n    scores.append(score)\n    print(name+' com a fun\u00e7\u00e3o:', score)\n    \n    score_cross = (cross_val_score(model, feat_train, targ_train, cv=10)) # 5 itera\u00e7\u00f5es\n    names.append(name)\n    scores.append(score_cross)\n    \n\n    print(name+' com cross_validation:', score_cross)","3c0af7cd":"# plotando gr\u00e1fico com os resultados\n\npd.Series(score, index=models.keys()).sort_values(ascending=False).plot.barh()","49af7b27":"# Preparando a base para execu\u00e7\u00e3o do modelo\n\nfeat_final = df_teste[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']]\n\n# feat_final","f667cfea":"# Executando a previs\u00e3o na base de teste\n\n\ndf_final = df_teste.copy()\n\ndf_final['Species'] = logreg.predict(df_final)\ndf_final[['Id','Species']].to_csv('sub_rodrigoaragao.csv', index=False)\n","ba7d23fe":"### - Pegar aleatoriamente 10 registros da base de treino, com o comando `sample` \n\nA vantagem de usar o comando `sample`, \u00e9 que retorna linhas aleat\u00f3rias da tabela e assim temos uma vis\u00e3o mais completa da variabilidade dos registros contidos. Utilizei o recurso `.T` para transpor os registros.","794f9a52":"### - Vendo quantos registros \u00fanicos existem por vari\u00e1vel","0901456c":"### - Analisando pelo boxplot por tipo","145557ba":"### - Testando Regress\u00e3o Log\u00edstica","4e9b3f92":"### - Testando ExtraTreesClassifier","21d2ae02":"### - Testando DecisionTreeClassifier","12db5e5a":"# Notebook para primeira competi\u00e7\u00e3o DSA\n### Autor: Rodrigo Arag\u00e3o Pinto\n#### An\u00e1lise da base de dados \u00cdris pra classifica\u00e7\u00e3o das esp\u00e9cies\n\n**Este notebook est\u00e1 dividido em 4 partes:**\n0. Preparando o Ambiente\n1. An\u00e1lise explorat\u00f3ria e conhecimendo dos dados\n2. Prepara\u00e7\u00e3o das bases de treino e teste para execu\u00e7\u00e3o dos modelos\n3. Execu\u00e7\u00e3o dos modelos e compara\u00e7\u00e3o dos resultados\n4. Fun\u00e7\u00e3o para executar todos os modelos de uma vez\n5. Execu\u00e7\u00e3o na base de teste e Envio para pontua\u00e7\u00e3o","6c438715":"### - Vendo os tipos de dados das vari\u00e1veis","c8ed64ab":"### - An\u00e1lise descritiva da base de treino","d01f2dd0":"# ### - Mapa de calor com a correla\u00e7\u00e3o das caracter\u00edsticas\n","947bf546":"### - Testando AdaBoostClassifier","e6ace4b6":"### - Analisando como ficou o tamanho das amostras","95758734":"### - Cria um dictionare com os 'alias' dos modelos - chave\/valor\n\n*A chave \u00e9 o nome e o modelo em si \u00e9 o valor*","6dcc4b06":"### - Executando o comando `sample`  na base de teste","339d32d4":"# 5. Execu\u00e7\u00e3o na base de teste e Envio para pontua\u00e7\u00e3o","f870d1af":" \n*Primeiro vamos dividir nossa base de dados entre features e target*","6322882b":"*- Carregar os arquivos texto (csv) que ser\u00e3o utilizados no modelo. Ser\u00e3o dois dataframes, um para treino e outro para valida\u00e7\u00e3o final do modelo.*","2ea88dfa":"## 2. Prepara\u00e7\u00e3o das bases de treino e teste para execu\u00e7\u00e3o dos modelos","5195bd50":"### - Testando o SVC","19a6c614":"*- Importar as bibliotecas para explora\u00e7\u00e3o e tratamento dos dados*","5764bb5a":"### - Testando o KNeighborsClassifier","79d63c04":"### - Cria uma fun\u00e7\u00e3o para executar os modelos de acordo com os dicts acima\n\n","92dd605c":"### - Separando em treino e valida\u00e7\u00e3o","0a050ecc":"### - Visualiza\u00e7\u00e3o gr\u00e1fica da base de treino por Esp\u00e9cie.\n","97b19874":"\n## 1. An\u00e1lise explorat\u00f3ria e conhecimendo dos dados\n\n\n*- Conhecer o tamanho das bases com o comando `shape`*","2b55d3dd":"# 4. Fun\u00e7\u00e3o para executar todos os modelos de uma vez\n\nUtilizando tamb\u00e9m o `cross_validation`","5428ea54":"### - Testando GaussianNB","2f55bc57":"### - Testando o GradientBoostingClassifier","06150b45":"## 0. Preparando o Ambiente","a7daa043":"# 3. Execu\u00e7\u00e3o dos modelos e compara\u00e7\u00e3o dos resultados\n\nImportando as bibliotecas com os modelos que ser\u00e3o utilizados","0a4b7a37":"### - Analisando pelo violinplot por tipo","02f7ca57":"### - Verificando de existem registros nulos para todas as vari\u00e1veis\n\nCaso exista algum valor nulo, este dever\u00e1 ser tratado","345c6437":"### - Testando RandomForestClassifier"}}