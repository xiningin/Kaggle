{"cell_type":{"6545619f":"code","283c83f3":"code","8ff10032":"code","e8998025":"code","22881932":"code","a1ab3986":"code","3487ac02":"code","5c35670f":"code","e3b0abe4":"code","700440cf":"code","1ffb445e":"code","4e2f7fec":"code","dfd2ce35":"code","05807735":"code","2af3f6df":"code","9c061214":"code","b679d5db":"code","d90e4af0":"code","0c534c63":"code","e3b0e238":"code","a10870b6":"code","b0a6e6c1":"code","6b94d82b":"code","3558eeba":"code","2e4bcfe0":"code","c4600f35":"code","5fe2b3a1":"code","73aa8f42":"code","bcdf388e":"code","bc3e1754":"code","0475192e":"code","2d781bf1":"code","05ae2bb0":"code","e9f1c852":"code","4f15a93a":"code","89d936c1":"code","377fc9ab":"code","fe39ad64":"code","97520276":"code","abbf13fc":"code","46c08133":"code","8161cf74":"code","9f1e1987":"code","b3a7c0b1":"code","3a38f4c1":"code","20552ae7":"markdown","b42ec0f6":"markdown","68f3d05e":"markdown","3779e993":"markdown","8c3634cd":"markdown","6b086e83":"markdown","4985d7a8":"markdown","30720c71":"markdown","24729846":"markdown","242a4a3f":"markdown","53535665":"markdown","6b1cd46b":"markdown","a2eff479":"markdown","506569f9":"markdown","bab072f9":"markdown","c18c3eab":"markdown","c63ffe93":"markdown","89ed663f":"markdown","418186ca":"markdown","68bea61e":"markdown","54ad0399":"markdown","ef062811":"markdown","9aa96e6a":"markdown","7efcec26":"markdown","63a24691":"markdown","0ab9fddb":"markdown","b02c9d65":"markdown","e98cb68b":"markdown","de8fce51":"markdown","32c85fbe":"markdown","0eb5fc81":"markdown","900dad28":"markdown","a8001c91":"markdown","0cc2e3c2":"markdown","92211e15":"markdown","1fbb64a7":"markdown","b82c2299":"markdown","85198d06":"markdown","1451c6f5":"markdown"},"source":{"6545619f":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","283c83f3":"def scale_feat(df,cont_feat) :\n    df1=df\n    scaler = preprocessing.RobustScaler()\n    df1[cont_feat] = scaler.fit_transform(df1[cont_feat])\n    return df1","8ff10032":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","e8998025":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","22881932":"df = pd.read_csv(\"..\/input\/telecom-churn-datasets\/churn-bigml-80.csv\")","a1ab3986":"df.head().T","3487ac02":"df.count()","5c35670f":"df.info()","e3b0abe4":"df.describe()","700440cf":"df['Churn'] = df['Churn'].map({ False: 0, True: 1 })","1ffb445e":"df.columns","4e2f7fec":"discr_feat = ['State', 'International plan', 'Voice mail plan', 'Customer service calls','Area code', 'Number vmail messages']\ncont_feat = list(set(df.columns) - set(discr_feat)-{'Churn'})","dfd2ce35":"for col in discr_feat :\n    df[col]=df[col].astype('category')\n    df[col] = df[col].cat.codes\n    df[col]=df[col].astype('int8')\n    ","05807735":"df.info()","2af3f6df":"df.head()","9c061214":"df.isnull().values.sum()","b679d5db":"df[cont_feat].describe()","d90e4af0":"df=scale_feat(df,cont_feat)","0c534c63":"df[cont_feat].describe()","e3b0e238":"for col in cont_feat :\n    plt.figure(figsize=[10,5])\n    sns.kdeplot(df[col])","a10870b6":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['Churn'], axis=1)\ny = df.Churn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","b0a6e6c1":"from sklearn import ensemble\n\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","6b94d82b":"print(classification_report(y_test, y_rf))","3558eeba":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","2e4bcfe0":"df.Churn.value_counts()","c4600f35":"from imblearn.under_sampling import RandomUnderSampler \n\nrus = RandomUnderSampler()\nX_train, y_train = rus.fit_sample(X_train, y_train)","5fe2b3a1":"y_train.value_counts()","73aa8f42":"rf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)\n\nprint(classification_report(y_test, y_rf))\n\ncm = confusion_matrix(y_test, y_rf)\nprint(cm)","bcdf388e":"X = df.drop(['Churn'], axis=1)\ny = df.Churn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","bc3e1754":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_sample(X_train, y_train)","0475192e":"y_train.value_counts()","2d781bf1":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","05ae2bb0":"print(classification_report(y_test, y_rf))","e9f1c852":"cm = confusion_matrix(y_test, y_rf)\nprint(cm)","4f15a93a":"plot_learning_curve(rf, X_train, y_train)","89d936c1":"plot_roc_curve(rf,X_test,y_test)","377fc9ab":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\nprint(xgb.score(X_test,y_test))","fe39ad64":"y_xgb = xgb.predict(X_test)\n\nprint(classification_report(y_test, y_xgb))\n\ncm = metrics.confusion_matrix(y_test, y_xgb)\nprint(cm)","97520276":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test)","abbf13fc":"print(classification_report(y_test, y_xgb))","46c08133":"X = df.drop(['Churn'], axis=1)\ny = df.Churn\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","8161cf74":"df.Churn.value_counts()","9f1e1987":"from xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=2278\/388)\n# xgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)","b3a7c0b1":"print(classification_report(y_test, y_xgb))\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)","3a38f4c1":"plot_learning_curve(xgb, X_train, y_train)\nplot_roc_curve(xgb,X_test,y_test)","20552ae7":"## Sous-\u00e9chantillonage","b42ec0f6":"On affiche les distributions des valeurs continues :","68f3d05e":"On v\u00e9rifie qu'on a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage :","3779e993":"On cr\u00e9e donc de \"fausses donn\u00e9es\" (mais \"vraisemblables\") pour l'apprentissage","8c3634cd":"On va utiliser une am\u00e9lioration de la m\u00e9thode XGBoost, sans sur\u00e9chantillonage","6b086e83":"Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation :","4985d7a8":"*Ces donn\u00e9es contiennent les transactions effectu\u00e9es par carte de cr\u00e9dit en septembre 2013 par les d\u00e9tenteurs de cartes europ\u00e9ennes.\nCet ensemble de donn\u00e9es pr\u00e9sente les transactions qui ont eu lieu en deux jours, o\u00f9 nous avons 492 fraudes sur 284 807 transactions. L'ensemble de donn\u00e9es est tr\u00e8s d\u00e9s\u00e9quilibr\u00e9, la classe positive (fraudes) repr\u00e9sente 0,172 % de toutes les transactions.*\n\n*Il ne contient que des variables d'entr\u00e9e num\u00e9riques qui sont le r\u00e9sultat d'une transformation ACP (analyse en composantes principales - une m\u00e9thode de r\u00e9duction de dimension). Malheureusement, pour des raisons de confidentialit\u00e9, nous ne pouvons pas fournir les caract\u00e9ristiques originales et plus d'informations sur le contexte des donn\u00e9es. Les caract\u00e9ristiques V1, V2, ... V28 sont les principales composantes obtenues avec l'ACP, les seules caract\u00e9ristiques qui n'ont pas \u00e9t\u00e9 transform\u00e9es avec l'ACP sont \"Temps\" et \"Montant\". La caract\u00e9ristique \"Temps\" contient les secondes \u00e9coul\u00e9es entre chaque transaction et la premi\u00e8re transaction de l'ensemble de donn\u00e9es. La caract\u00e9ristique \"Montant\" est le montant de la transaction, cette caract\u00e9ristique peut \u00eatre utilis\u00e9e par exemple pour l'apprentissage d\u00e9pendant des co\u00fbts. La caract\u00e9ristique \n\"Class\" est la variable de r\u00e9ponse et prend la valeur 1 en cas de fraude et 0 dans le cas contraire.*","30720c71":"La m\u00e9thode SMOTE (Synthetic Minority Oversampling TEchnique) consiste \u00e0 synth\u00e9tiser des \u00e9l\u00e9ments pour la classe minoritaire, \u00e0 partir de ceux qui existent d\u00e9j\u00e0. Elle fonctionne en choisissant au hasard un point de la classe minoritaire et en calculant les k-voisins les plus proches pour ce point. Les points synth\u00e9tiques sont ajout\u00e9s entre le point choisi et ses voisins.","24729846":"## For\u00eats al\u00e9atoires","242a4a3f":"## Librairies et fonctions utiles","53535665":"## Exercice : d\u00e9tection de fraude","6b1cd46b":"Pr\u00e9dire les fraudes \u00e0 la carte bancaire sur le dataset :  \nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud","a2eff479":"On applique les for\u00eats al\u00e9atoires sur le nouvel ensemble d'apprentissage","506569f9":"On a moins de donn\u00e9es d'apprentissage, mais les r\u00e9sultats sont plut\u00f4t meilleurs ...","bab072f9":"On mappe les valeurs de la colonne cible en 0\/1 :","c18c3eab":"# Churn (attrition) de clients Telecom","c63ffe93":"On teste les for\u00eats al\u00e9atoires :","89ed663f":"Fonction pour standardiser les donn\u00e9es quantitatives (cont_feat est une liste des colonnes correspondant \u00e0 des caract\u00e9ristiques quantitatives) :","418186ca":"On a bien \u00e9quilibr\u00e9 l'ensemble d'apprentissage (en \"ajoutant\" des donn\u00e9es) :","68bea61e":"Est-il n\u00e9cessaire d'appliquer une transformation sur les distributions ?","54ad0399":"On va garder autant de clients fid\u00e8les que de churners dans l'ensemble d'apprentissage (X_train), en tirant al\u00e9atoirement ceux qu'on va garder\nOn dit qu'on \"sous-\u00e9chantillonne la classe majoritaire\"","ef062811":"## Sur\u00e9chantillonnage","9aa96e6a":"On teste les for\u00eats al\u00e9atoires avec les donn\u00e9es sur\u00e9chantillonn\u00e9es :","7efcec26":"Que pensez-vous de cette matrice de confusion ?","63a24691":"## XGBoost pond\u00e9r\u00e9","0ab9fddb":"On va r\u00e9\u00e9quilibrer le dataset en sur-\u00e9chantillonnant la classe minoritaire :","b02c9d65":"Les valeurs num\u00e9riques ont des caract\u00e9ristiques tr\u00e8s diff\u00e9rentes :","e98cb68b":"On construit les ensembles d'apprentissage et de test :","de8fce51":"<img src=\"https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png\">","32c85fbe":"On v\u00e9rifie s'il y a des valeurs ind\u00e9termin\u00e9es dans le dataset :","0eb5fc81":"## Extreme Gradient Boosting : XGBoost avec sur\u00e9chantillonage SMOTE","900dad28":"## Traitement du dataset","a8001c91":"Il y a beaucoup moins de clients qui partent que de clients qui restent (heureusement pour l'op\u00e9rateur ...) :","0cc2e3c2":"On reconstitue les jeux de donn\u00e9es sans \u00e9chantillonnage :","92211e15":"On convertit les cat\u00e9gories en \u00e9tiquettes num\u00e9riques :","1fbb64a7":"Fonction pour tracer la courbe ROC :","b82c2299":"On normalise ces valeurs :","85198d06":"On utilise le param\u00e8tre *scale_pos_weight* pour donner plus d'impact aux erreurs commises sur la classe minoritaire :","1451c6f5":"On veut pr\u00e9dire le d\u00e9part de clients d'un op\u00e9rateur telecom \u00e0 partir de donn\u00e9es comme la formule d'abonnement, ou le temps de communication consomm\u00e9.\n\nOn peut trouver le dataset sur :  \nhttps:\/\/www.kaggle.com\/mnassrib\/telecom-churn-datasets \n"}}