{"cell_type":{"c9ebb6e6":"code","8fde95ef":"code","d1286ac5":"code","1110c832":"code","da076df5":"code","662ca41e":"code","9b89d432":"code","21f0525d":"code","37a3c808":"code","92b5e479":"code","e5986901":"code","254d6b2b":"code","289e8360":"code","6fcbe481":"code","cf7d56d1":"code","679c53cc":"code","e55bcf9b":"code","4d814705":"code","d8e55c4d":"code","163eb64b":"code","af81bae1":"code","48bb565b":"code","0b41c498":"code","3627a7d0":"code","cc6870ef":"code","407c0b52":"code","2b4b565a":"code","4e8c46cb":"code","38f4bbf3":"code","1b19accb":"code","661dabce":"code","cb17cdd2":"code","2cfd249e":"code","ce0af353":"code","17bf4f4d":"code","2af20551":"code","0f6c5d9d":"code","f014af24":"code","e3be6d00":"code","273860bd":"code","6ab71a5f":"code","78bfeb70":"markdown","d00b340c":"markdown","189f9f99":"markdown","c94f79aa":"markdown","f2ca90a6":"markdown","4ebceb0f":"markdown","4b28fb0e":"markdown","4e8375ff":"markdown","5409d0ff":"markdown","89ea0465":"markdown","ed4783cc":"markdown","ec8501fd":"markdown","06997380":"markdown","836bb828":"markdown","bd93cb59":"markdown","35823b9f":"markdown","b6c91cde":"markdown","3afe4ae0":"markdown","3a53cfd2":"markdown","dd15e7ab":"markdown","abdff4be":"markdown","83afee8a":"markdown","8e084268":"markdown","640c9690":"markdown","7d806e5b":"markdown"},"source":{"c9ebb6e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8fde95ef":"# https:\/\/www.kaggle.com\/timesler\/comparison-of-face-detection-packages\u3000\u3092\u53c2\u8003\u306b\u3057\u305f","d1286ac5":"%%capture\n!pip install \/kaggle\/input\/facenet-pytorch-vggface2\/facenet_pytorch-2.0.0-py3-none-any.whl","1110c832":"import cv2\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\nimport torch\nfrom tqdm.notebook import tqdm\nimport time\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\ndevice","da076df5":"sample = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/aagfhgtpmv.mp4'\n\nreader = cv2.VideoCapture(sample)\nimages_1080_1920 = []\nimages_720_1280 = []\nimages_540_960 = []","662ca41e":"print(reader.get(cv2.CAP_PROP_FRAME_WIDTH))\nprint(reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\nprint(reader.get(cv2.CAP_PROP_FRAME_COUNT))","9b89d432":"for i in tqdm(range(int(reader.get(cv2.CAP_PROP_FRAME_COUNT)))):\n    _, image = reader.read()\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    images_1080_1920.append(image)\n    images_720_1280.append(cv2.resize(image, (1280, 720)))\n    images_540_960.append(cv2.resize(image, (960, 540)))","21f0525d":"reader.release()","37a3c808":"images_1080_1920 = np.stack(images_1080_1920)\nimages_720_1280 = np.stack(images_720_1280)\nimages_540_960 = np.stack(images_540_960)\n\nprint('Shapes:')\nprint(images_1080_1920.shape)\nprint(images_720_1280.shape)\nprint(images_540_960.shape)","92b5e479":"def plot_faces(images, figsize=(10.8\/2, 19.2\/2), start_frame=0, end_frame=0):\n    shape = images[0].shape\n    if end_frame == 0:\n        end_frame = len(images) - 1\n    images = images[np.linspace(start_frame, end_frame, 16).astype(int)]\n    im_plot = []\n    for i in range(0, 16, 4):\n        im_plot.append(np.concatenate(images[i:i+4], axis=1))\n    im_plot = np.concatenate(im_plot, axis=0)\n    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    ax.imshow(im_plot)\n    ax.xaxis.set_visible(False)\n    ax.yaxis.set_visible(False)\n\n    ax.grid(False)\n    fig.tight_layout()\n\ndef timer(detector, detect_fn, images, *args):\n    start = time.time()\n    faces = detect_fn(detector, images, *args)\n    elapsed = time.time() - start\n    print(f', {elapsed:.3f} seconds')\n    return faces, elapsed","e5986901":"plot_faces(images_1080_1920, figsize=(10.8, 19.2), start_frame=0, end_frame=15)","254d6b2b":"from facenet_pytorch import MTCNN\ndetector = MTCNN(device=device, post_process=False)\n\ndef detect_facenet_pytorch(detector, images, batch_size):\n    faces = []\n    for lb in np.arange(0, len(images), batch_size):\n        imgs_pil = [Image.fromarray(image) for image in images[lb:lb+batch_size]]\n        faces.extend(detector(imgs_pil))\n    return faces\n\ntimes_facenet_pytorch = []    # batched","289e8360":"print('Detecting faces in 1080x1920 frames', end='')\nfaces, elapsed = timer(detector, detect_facenet_pytorch, images_1080_1920, 20)\ntimes_facenet_pytorch.append(elapsed)\n\nplot_faces(torch.stack(faces).permute(0, 2, 3, 1).int().numpy())","6fcbe481":"torch.stack(faces).shape","cf7d56d1":"plot_faces(torch.stack(faces).permute(0, 3, 2, 1).int().numpy())","679c53cc":"plot_faces(torch.stack(faces).permute(0, 2, 3, 1).int().numpy(), start_frame=0, end_frame=15)","e55bcf9b":"from IPython.display import HTML\nfrom base64 import b64encode\n\nDATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n\ndef play_video(video_file, subset=TRAIN_SAMPLE_FOLDER):\n    '''\n    Display video\n    param: video_file - the name of the video file to display\n    param: subset - the folder where the video file is located (can be TRAIN_SAMPLE_FOLDER or TEST_Folder)\n    '''\n    video_url = open(os.path.join(DATA_FOLDER, subset,video_file),'rb').read()\n    data_url = \"data:video\/mp4;base64,\" + b64encode(video_url).decode()\n    return HTML(\"\"\"<video width=500 controls><source src=\"%s\" type=\"video\/mp4\"><\/video>\"\"\" % data_url)","4d814705":"play_video(\"aagfhgtpmv.mp4\")","d8e55c4d":"plot_faces(torch.stack(faces).permute(0, 2, 3, 1).int().numpy(), start_frame=16, end_frame=31)","163eb64b":"plot_faces(torch.stack(faces).permute(0, 2, 3, 1).int().numpy(), start_frame=32, end_frame=48)","af81bae1":"faces2 = torch.stack(faces).permute(0, 2, 3, 1).int().numpy()","48bb565b":"plt.imshow(faces2[0])","0b41c498":"cv2.cvtColor(faces2[0], cv2.COLOR_RGB2GRAY)","3627a7d0":"faces2[0].dtype","cc6870ef":"tmp_gray_face = cv2.cvtColor(faces2[0].astype(\"uint16\"), cv2.COLOR_RGB2GRAY)\nplt.imshow(tmp_gray_face)","407c0b52":"plt.imshow(tmp_gray_face, cmap = \"gray\")\n","2b4b565a":"# https:\/\/tetlab117.hatenablog.com\/entry\/2017\/09\/28\/163638\nbf = cv2.BFMatcher(cv2.NORM_HAMMING)\n\n# ORB\u3068AKAZE\u306f\u7279\u5fb4\u70b9\u3084\u7279\u5fb4\u91cf\u3092\u62bd\u51fa\u3059\u308b\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\n# \u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3092\u8abf\u7bc0\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u3069\u3061\u3089\u3067\u3082\u884c\u3048\u308b\n\n# detector = cv2.ORB_create()\ndetector = cv2.AKAZE_create()","4e8c46cb":"# \u307e\u305a\u306f0\u30d5\u30ec\u30fc\u30e0\u76ee\u30681\u30d5\u30ec\u30fc\u30e0\u76ee\u306e\u985e\u4f3c\u5ea6\ntarget_img = cv2.cvtColor(faces2[0].astype(\"uint16\"), cv2.COLOR_RGB2GRAY)\ncomparing_img = cv2.cvtColor(faces2[1].astype(\"uint16\"), cv2.COLOR_RGB2GRAY)","38f4bbf3":"fig = plt.figure()\nax1 = fig.add_subplot(1, 2, 1)\nax1.imshow(target_img, cmap = \"gray\")\nax2 = fig.add_subplot(1, 2, 2)\nax2.imshow(comparing_img, cmap = \"gray\")\nplt.show()","1b19accb":"detector = cv2.AKAZE_create()\n(target_kp, target_des) = detector.detectAndCompute(target_img, None)\ntarget_kp","661dabce":"target_img = faces2[0].astype(\"uint16\")\ncomparing_img = faces2[1].astype(\"uint16\")","cb17cdd2":"target_hist = cv2.calcHist([target_img], [0], None, [256], [0, 256])\ncomparing_hist = cv2.calcHist([comparing_img], [0], None, [256], [0, 256])\nret = cv2.compareHist(target_hist, comparing_hist, 0)\nprint(ret)","2cfd249e":"hist_similarity = []\nfor i in range(0,299,1):\n    target_hist = cv2.calcHist([faces2[i].astype(\"uint16\")], [0], None, [256], [0, 256])\n    comparing_hist = cv2.calcHist([faces2[i+1].astype(\"uint16\")], [0], None, [256], [0, 256])\n    hist_similarity.append(cv2.compareHist(target_hist, comparing_hist, 0))","ce0af353":"plt.figure(figsize=(20, 4))\nplt.plot(hist_similarity)","17bf4f4d":"plot_faces(torch.stack(faces).permute(0, 2, 3, 1).int().numpy(), start_frame=125, end_frame=140)","2af20551":"def plot_histgram(file):\n    sample = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/' + file\n    images_1080_1920 = []\n\n    reader = cv2.VideoCapture(sample)\n\n    for i in tqdm(range(int(reader.get(cv2.CAP_PROP_FRAME_COUNT)))):\n        _, image = reader.read()\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        images_1080_1920.append(image)\n    reader.release()\n\n    images_1080_1920 = np.stack(images_1080_1920)\n\n    detector = MTCNN(device=device, post_process=False)\n    faces, elapsed = timer(detector, detect_facenet_pytorch, images_1080_1920, 20)\n    times_facenet_pytorch.append(elapsed)\n\n    faces2 = torch.stack(faces).permute(0, 2, 3, 1).int().numpy()\n\n\n    hist_similarity = []\n    for i in range(0,100,1):\n        target_hist = cv2.calcHist([faces2[i].astype(\"uint16\")], [0], None, [256], [0, 256])\n        comparing_hist = cv2.calcHist([faces2[i+1].astype(\"uint16\")], [0], None, [256], [0, 256])\n        hist_similarity.append(cv2.compareHist(target_hist, comparing_hist, 0))\n\n    plt.figure(figsize=(20, 4))\n    plt.plot(hist_similarity)","0f6c5d9d":"DATA_FOLDER = '..\/input\/deepfake-detection-challenge'\nTRAIN_SAMPLE_FOLDER = 'train_sample_videos'\n\ndef get_meta_from_json(path):\n    df = pd.read_json(os.path.join(DATA_FOLDER, path, \"metadata.json\"))\n    df = df.T\n    return df\n\nmeta_train_df = get_meta_from_json(TRAIN_SAMPLE_FOLDER)\nmeta_train_df.head()","f014af24":"for file in meta_train_df.loc[meta_train_df.label == \"FAKE\"].head(10).index.to_list():\n    plot_histgram(file)","e3be6d00":"canny_img = cv2.Canny(cv2.cvtColor(faces2[0].astype(\"uint8\"), cv2.COLOR_RGB2GRAY), 50, 110)\n# Canny\u306fuint16\u306f\u3060\u3081\u3089\u3057\u3044","273860bd":"plt.imshow(canny_img, cmap = \"gray\")","6ab71a5f":"canny_img = cv2.Canny(cv2.cvtColor(faces2[45].astype(\"uint8\"), cv2.COLOR_RGB2GRAY), 50, 110)\nplt.imshow(canny_img, cmap = \"gray\")","78bfeb70":"* x\u3068y\u3092\u5165\u308c\u66ff\u3048\u3066\u307f\u308b","d00b340c":"## \u524d\u6e96\u5099\u3068\u3057\u3066\u30b0\u30ec\u30fc\u30a2\u30a6\u30c8\u3059\u308b","189f9f99":"## \u7279\u5fb4\u70b9\u62bd\u51fa\u3092\u3057\u3066\u305d\u306e\u8ddd\u96e2\u3092\u51fa\u3057\u3066\u3001\u985e\u4f3c\u5ea6\u3068\u3059\u308b","c94f79aa":"## fake\u304b\u308910\u500b\u3001not fake\u304b\u308910\u500b\u53d6\u308a\u51fa\u3057\u3066\u3001\u30b0\u30e9\u30d5\u3092\u66f8\u3044\u3066\u307f\u308b","f2ca90a6":"* \u3053\u308c\u306ffake\u3002\u9854\u304c\u9014\u4e2d\u3067\u30ad\u30e2\u3044\u3053\u3068\u306b\u306a\u3063\u3066\u3044\u308b","4ebceb0f":"* 1\u30d5\u30ec\u30fc\u30e0\u76ee\u304b\u3089300\u30d5\u30ec\u30fc\u30e0\u76ee\u307e\u3067\u3084\u3063\u3066\u307f\u308b","4b28fb0e":"# \u9854\u691c\u51fa","4e8375ff":"* \u3046\u307e\u304f\u3044\u304b\u306a\u3044","5409d0ff":"# \u30a8\u30c3\u30b8\u691c\u51fa\u3057\u3066\u307f\u308b","89ea0465":"* \u305d\u308a\u3083\u9854\u3058\u3083\u306d\u30fc\u304b\u3089\u985e\u4f3c\u5ea6\u4e0b\u304c\u308b\u3088\u306d","ed4783cc":"* \u4f55\u304b\u3057\u3089\u306f\u51fa\u305d\u3046","ec8501fd":"* OpenCV\u306f\u753b\u50cf\u3092\u8aad\u3080\u6642\u3001BGR\u306b\u306a\u308b\u3089\u3057\u3044\u3002\u3078\u30fc\u3078\u30fc\u3078\u30fc\u3002","06997380":"* \u601d\u3063\u305f\u3068\u304a\u308a\u3067\u304d\u307e\u3057\u305f","836bb828":"* int32\u3060\u3068\u3060\u3081\u3089\u3057\u3044\uff08\u30de\u30a4\u30ca\u30b9\u304c\u5165\u3063\u3066\u3044\u306a\u304f\u3066\u3082\u3001\u5165\u308b\u53ef\u80fd\u6027\u304c\u3042\u308bdtype\u3060\u304b\u3089\u3060\u3081\uff09\n\n\u53c2\u8003\uff1a\u3000https:\/\/qiita.com\/nonbiri15\/items\/e8996bd157a6155a0db1","bd93cb59":"* gray\u306b\u306a\u308a\u307e\u3057\u305f","35823b9f":"# \u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u6bd4\u8f03\u3092\u3084\u3063\u3066\u307f\u308b","b6c91cde":"* 45-48\u30d5\u30ec\u30fc\u30e0\u3042\u305f\u308a\u306f\u53e3\u88c2\u3051\u611f\u304c\u5f37\u3044\u3093\u3060\u3051\u3069\u3071\u3063\u3068\u898b\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3088\u306d","3afe4ae0":"\u53c2\u8003\u306b\u3057\u305f\u306e\u306f\u3000https:\/\/qiita.com\/best_not_best\/items\/c9497ffb5240622ede01","3a53cfd2":"* \u3046\u307e\u304f\u3044\u304b\u306a\u3044\n* \u7406\u7531\u306f\u753b\u50cf\u304c\u5c0f\u3055\u3059\u304e\u308b\u304b\u3089\uff1f\n\n\u53c2\u8003\u306b\u3057\u305f\u306e\u306f\u3000https:\/\/tetlab117.hatenablog.com\/entry\/2017\/09\/28\/163638\u3000\u3060\u304c\u3001\u3053\u3053\u306f\u5927\u304d\u3044\u9854\u5199\u771f\u3057\u304b\u306a\u3044","dd15e7ab":"* \u767d\u9ed2\u3058\u3083\u306a\u304f\u3066\u30ab\u30e9\u30fc\u753b\u50cf\u306b\u623b\u3057\u3066\u304b\u3089\u3084\u308b","abdff4be":"* \u53e3\u88c2\u3051\u7537","83afee8a":"* tensor\u306e\u8ef8\u306e\u9806\u756a\u306e\u4e26\u3073\u66ff\u3048\u304c\u5fc5\u8981\u3089\u3057\u3044\u3002\u3044\u3061\u3044\u3061\u3081\u3093\u3069\u304f\u3055\u3044\u306d","8e084268":"# \u30d5\u30ec\u30fc\u30e0\u9593\u306e\u9854\u306e\u985e\u4f3c\u5ea6\u3092\u51fa\u3057\u3066\u307f\u308b","640c9690":"## The facenet-pytorch package","7d806e5b":"* \u3053\u308c\u306f\u4ffa\u306e\u30a4\u30e1\u30fc\u30b8\u3057\u3066\u3044\u308bgray\u3067\u306f\u306a\u3044\u3002\u3002\u3002"}}