{"cell_type":{"22909831":"code","f82223f2":"code","bcfc8ad1":"code","51e84d4a":"code","0a308708":"code","32f7e045":"code","4010b6ed":"code","09878d02":"code","77d8f3e0":"code","6f1b7534":"code","7e908217":"code","94d4227a":"code","be09f33c":"code","12d7a3a3":"code","009b1f19":"code","3ba12205":"code","ec9ec98c":"code","1d8c00c5":"code","9fb7b971":"code","cf376007":"code","a44dae9b":"code","208626d7":"code","068a58ba":"code","41cbac55":"code","e1f1e4b2":"code","bca82be0":"code","c5ed880b":"code","ef037778":"code","b2a947e8":"code","27a6d147":"code","4fbc07ab":"code","6885da8f":"code","ec1799f0":"code","f88246bc":"code","d8548cb7":"code","db3c5f4f":"code","abdda106":"code","d0e3d7d7":"code","0f599be0":"code","9b737829":"code","0c211850":"code","5fc20c64":"code","679b3e50":"code","6bb26f92":"code","09db0227":"code","495451d8":"code","7c447afd":"code","1817a92d":"code","a83c962b":"code","b2180abc":"code","371322a4":"code","4b239d92":"code","47132ce0":"code","dafe1d8b":"code","0bab00e6":"code","5f1c55d7":"code","ee2d2e88":"code","72d2a38c":"code","90405644":"code","8f40997b":"code","bc1e540f":"code","5bf0fcda":"code","891b61a3":"code","7348cae2":"code","943891f3":"code","b536ef01":"code","ae664ad8":"code","f88e76bb":"code","010a6c22":"code","985500fe":"code","2dbfc617":"code","3336811d":"code","261ad827":"code","de17a23d":"code","0d1b83e9":"code","c5b255a5":"code","d5753b8f":"code","46a3f94c":"code","3c861822":"code","31823035":"code","816e54c4":"code","c68c4ab6":"code","3d287b0b":"code","179b762a":"code","623e6fb7":"code","0b7f817a":"code","39d8b69e":"code","0a146c59":"markdown","bbc1bc29":"markdown","a68dc4a2":"markdown","4252baa6":"markdown","9a245ffc":"markdown","957967a8":"markdown","cd284035":"markdown","5a1e5818":"markdown","f65e1a6d":"markdown","83f14036":"markdown","92803500":"markdown","33af07ed":"markdown","fdecd39a":"markdown","56f3a26b":"markdown","ef034a59":"markdown","b179a32d":"markdown","6dba3262":"markdown","0383a28e":"markdown","ffefcd18":"markdown","3359bb21":"markdown","f9dc4173":"markdown","3e85a94e":"markdown","fbb8c41b":"markdown","86fd6e43":"markdown","9b52406a":"markdown","389dad29":"markdown","8cd82409":"markdown","94fe01a6":"markdown","bd3028fd":"markdown","c151fd6b":"markdown","3adf795c":"markdown","451b9a82":"markdown","2db7eba0":"markdown","d10bc837":"markdown","59d28b4a":"markdown","a92c528f":"markdown","fd23f167":"markdown","66055bab":"markdown","e940fe12":"markdown","a8a3d6d6":"markdown","86c8b7f3":"markdown","b5681754":"markdown","b26a8011":"markdown","dfc43926":"markdown","78baad5d":"markdown","d1e980e9":"markdown","4ea2d2f6":"markdown","f50344c6":"markdown","bb1e3773":"markdown","6ec0e5d2":"markdown","636d533d":"markdown","c7fb0443":"markdown","f31dac5c":"markdown","cf649a36":"markdown","86eeb021":"markdown","dfd3936f":"markdown","5327c890":"markdown","47bbd6c2":"markdown","f5f3ae67":"markdown","d5aa9c44":"markdown","384b8325":"markdown","cfb7f26e":"markdown"},"source":{"22909831":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f82223f2":"calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nsales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsales_train_evaluation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')","bcfc8ad1":"calendar.head(5)","51e84d4a":"calendar['d'].unique()","0a308708":"sales_train_validation.head(5)","32f7e045":"sales_train_validation.shape","4010b6ed":"sales_train_validation['cat_id'].unique()","09878d02":"sales_train_validation['dept_id'].unique()","77d8f3e0":"sales_train_validation['state_id'].unique()","6f1b7534":"sales_train_validation['store_id'].unique()","7e908217":"sell_prices.head()","94d4227a":"sell_prices['item_id'].nunique()","be09f33c":"sales_train_validation['store_id'].value_counts()","12d7a3a3":"import matplotlib.pyplot as plt \ndef msv_1(data, thresh = 20, color = 'black', edgecolor = 'black', height = 3, width = 15):\n    \n    plt.figure(figsize = (width, height))\n    percentage = (data.isnull().mean()) * 100\n    percentage.sort_values(ascending = False).plot.bar(color = color, edgecolor = edgecolor)\n    plt.axhline(y = thresh, color = 'r', linestyle = '-')\n    \n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    \n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, f'Columns with more than {thresh}% missing values', fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, f'Columns with less than {thresh}% missing values', fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv_1(calendar, 80, color=sns.color_palette('Reds',15))","009b1f19":"calendar.drop([\"event_name_2\", \"event_type_2\", \"event_name_1\", \"event_type_1\"], axis = 1, inplace = True)","3ba12205":"calendar.isnull().sum()","ec9ec98c":"df = sales_train_validation","1d8c00c5":"df.head()","9fb7b971":"from itertools import cycle\nimport matplotlib.pyplot as plt\n\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])\n\ndcols = [c for c in df.columns if 'd_' in c] ## Represents the days(d_1 -> d_1913)\n\ndf.loc[df['id'] == 'FOODS_3_090_CA_3_validation'].set_index('id')[dcols].T.plot(\nfigsize = (15, 5), title='FOODS_3_090_CA_3 sales by \"d\" number', color = next(color_cycle))\nplt.show()","cf376007":"calendar[['d','date','wm_yr_wk', 'weekday', 'wday', 'month', 'year']].head()","a44dae9b":"example = df.loc[df['id'] == 'FOODS_3_090_CA_3_validation'][dcols].T # Col name will be 8412\nexample = example.rename(columns={8412:'FOODS_3_090_CA_3'}) # Rename the column to FOODS_3_090_CA_3\nexample = example.reset_index().rename(columns={'index': 'd'}) # make the index d_1, d_2...d_1913\nexample = example.merge(calendar, how='left', validate='1:1')\nexample.head(5)","208626d7":"example.set_index('date')['FOODS_3_090_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_cycle),\n          title='FOODS_3_090_CA_3 sales by actual sale dates')\nplt.show()","068a58ba":"# Select more top selling examples\nexample2 = df.loc[df['id'] == 'HOBBIES_1_234_CA_3_validation'][dcols].T\nexample2 = example2.rename(columns={6324:'HOBBIES_1_234_CA_3'})\nexample2 = example2.reset_index().rename(columns={'index': 'd'}) \nexample2 = example2.merge(calendar, how='left', validate='1:1')\n\nexample2.set_index('date')['HOBBIES_1_234_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_cycle),\n          title='HOBBIES_1_234_CA_3 sales by actual sale dates')\nplt.show()\n\nexample3 = df.loc[df['id'] == 'HOUSEHOLD_1_118_CA_3_validation'][dcols].T\nexample3 = example3.rename(columns={6776:'HOUSEHOLD_1_118_CA_3'}) # Name it correctly\nexample3 = example3.reset_index().rename(columns={'index': 'd'}) # make the index \"d\"\nexample3 = example3.merge(calendar, how='left', validate='1:1')\n\nexample3.set_index('date')['HOUSEHOLD_1_118_CA_3'] \\\n    .plot(figsize=(15, 5),\n          color=next(color_cycle),\n          title='HOUSEHOLD_1_118_CA_3 sales by actual sale dates')\nplt.show()","41cbac55":"color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']","e1f1e4b2":"examples = ['FOODS_3_090_CA_3','HOBBIES_1_234_CA_3','HOUSEHOLD_1_118_CA_3']\nexample_df = [example, example2, example3]\n\nfor i in range(0, 3):\n    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15, 3))\n    \n    example_df[i].groupby('wday').mean()[examples[i]].plot(\n    kind = 'line', lw = 5, title = 'Average sale: day of the week', ax = ax1, \n        color = color_pal[0])\n    \n    example_df[i].groupby('month').mean()[examples[i]].plot(\n    kind = 'line', lw = 5, title = 'Average sale: month of the year', ax = ax2, \n        color = color_pal[4])\n    \n    example_df[i].groupby('year').mean()[examples[i]].plot(\n    kind = 'line', lw = 5, title = 'Average sale: year', ax = ax3, \n        color = color_pal[2])\nplt.show()","bca82be0":"twenty_examples = df.sample(20, random_state = 529).set_index('id')[dcols].T.merge(\ncalendar.set_index('d')['date'], left_index = True, right_index = True, validate = '1:1').set_index('date')","c5ed880b":"fig, axs = plt.subplots(10, 2, figsize = (20, 20))\naxs = axs.flatten()\n\nax_idx = 0\n\nfor item in twenty_examples.columns:\n    twenty_examples[item].plot(\n    title = item, color = next(color_cycle),\n    ax = axs[ax_idx])\n    ax_idx += 1\nplt.tight_layout()\nplt.show()","ef037778":"df.groupby('cat_id').count()['id'].sort_values().plot(kind = 'barh', figsize=(15, 5), title='Count of Items by Category')\nplt.show()","b2a947e8":"past_sales = df.set_index('id')[dcols].T.merge(calendar.set_index('d')['date'], left_index = True, right_index = True, validate = '1:1').set_index('date')","27a6d147":"past_sales.head(5)","4fbc07ab":"for i in df['cat_id'].unique():\n    items_col = [c for c in past_sales.columns if i in c]\n    past_sales[items_col].sum(axis = 1).plot(figsize = (15, 5), alpha = 0.8, title = 'Total Sales by Item Type')\nplt.legend(df['cat_id'].unique())\nplt.show()","6885da8f":"store_list = sell_prices['store_id'].unique()\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(90).mean() \\\n        .plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Rolling 90 Day Average Total Sales (10 stores)')\nplt.legend(store_list)\nplt.show()\n## Average sales of itmes in each store on 90 days average.","ec1799f0":"fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nfor s in store_list:\n    store_items = [c for c in past_sales.columns if s in c]\n    past_sales[store_items] \\\n        .sum(axis=1) \\\n        .rolling(7).mean() \\\n        .plot(alpha=1,\n              ax=axes[ax_idx],\n              title=s,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Weekly Sale Trends by Store ID')\nplt.tight_layout()\nplt.show()","f88246bc":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\ngreens = [\"mediumaquamarine\", \"mediumseagreen\", \"seagreen\", \"green\"]\nstore_list = sell_prices['store_id'].unique()\nfig = go.Figure()\nmeans = [] ## Mean sale of each store across all duration.\nstores = [] ## Unique Store ids\nfor i, s in enumerate(store_list):\n    if \"ca\" in s or \"CA\" in s:\n        store_items = [c for c in past_sales.columns if s in c]\n        data = past_sales[store_items].sum(axis=1).rolling(90).mean() ## Rolling 90 days average\n        means.append(np.mean(past_sales[store_items].sum(axis=1)))\n        stores.append(s)\n        fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=s, marker=dict(color=greens[i])))\nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (California)\")","d8548cb7":"calendar1 = calendar[['date', 'weekday', 'wday', 'month', 'year']]","db3c5f4f":"calendar1.head()","abdda106":"past_sales1 = past_sales.merge(calendar1.set_index('date'), left_index = True, right_index = True, validate = '1:1')","d0e3d7d7":"past_sales1.head()","0f599be0":"fig, axes = plt.subplots(5, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nstores = df['store_id'].unique()\nfor store in stores:\n    store_items = [c for c in past_sales1.columns if store in c]\n    past_sales1[store_items] \\\n        .sum(axis=1) \\\n        .plot(alpha=1,\n              kind = \"hist\",\n              ax=axes[ax_idx],\n              title=store,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n\nplt.suptitle('Sales Trend according to Stores')\nplt.tight_layout()\nplt.show()","9b737829":"fig, axes = plt.subplots(4, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\ndepts = df['dept_id'].unique()\nfor dept in depts:\n    dept_items = [c for c in past_sales1.columns if dept in c]\n    past_sales1[dept_items] \\\n        .sum(axis=1) \\\n        .plot(alpha=1,\n              kind = \"hist\",\n              ax=axes[ax_idx],\n              title=dept,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n\nplt.suptitle('Sales Trend according to Departments')\nplt.tight_layout()\nplt.show()","0c211850":"fig, axes = plt.subplots(2, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\ncats = df['cat_id'].unique()\nfor cat in cats:\n    cat_items = [c for c in past_sales1.columns if cat in c]\n    past_sales1[cat_items] \\\n        .sum(axis=1) \\\n        .plot(alpha=1,\n              kind = \"hist\",\n              ax=axes[ax_idx],\n              title=cat,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n\nplt.suptitle('Sales Trend according to Category')\nplt.tight_layout()\nplt.show()","5fc20c64":"fig, axes = plt.subplots(2, 2, figsize=(15, 10), sharex=True)\naxes = axes.flatten()\nax_idx = 0\nstates = df['state_id'].unique()\nfor state in states:\n    state_items = [c for c in past_sales1.columns if state in c]\n    past_sales1[state_items] \\\n        .sum(axis=1) \\\n        .plot(alpha=1,\n              kind = \"hist\",\n              ax=axes[ax_idx],\n              title=state,\n              lw=3,\n              color=next(color_cycle))\n    ax_idx += 1\n# plt.legend(store_list)\nplt.suptitle('Sales Trend according to States')\nplt.tight_layout()\nplt.show()","679b3e50":"salesSummer = past_sales1[(past_sales1['month'] == 6) | (past_sales1['month'] == 7) | (past_sales1['month'] == 8)]","6bb26f92":"states = df['state_id'].unique()\nfor state in states:\n    cols = [c for c in salesSummer.columns if state in c]\n    salesSummer[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items state wise in Summers')\nplt.legend(states)\nplt.show()    ","09db0227":"categories = df['cat_id'].unique()\nfor cat in categories:\n    cols = [c for c in salesSummer.columns if cat in c]\n    salesSummer[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items category wise in Summers')\nplt.legend(categories)\nplt.show()","495451d8":"depts = df['dept_id'].unique()\nfor dept in depts:\n    cols = [c for c in salesSummer.columns if dept in c]\n    salesSummer[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items Department wise in Summers')\nplt.legend(depts)\nplt.show()","7c447afd":"plt.hist(salesSummer[cols].sum(axis = 1))","1817a92d":"salesChristmas = past_sales1[past_sales1['month'] == 12]","a83c962b":"states = df['state_id'].unique()\nfor state in states:\n    cols = [c for c in salesChristmas.columns if state in c]\n    salesChristmas[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items state wise around Christmas')\nplt.legend(states)\nplt.show()    ","b2180abc":"categories = df['cat_id'].unique()\nfor cat in categories:\n    cols = [c for c in salesChristmas.columns if cat in c]\n    salesChristmas[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items category wise during Christmas')\nplt.legend(categories)\nplt.show()","371322a4":"depts = df['dept_id'].unique()\nfor dept in depts:\n    cols = [c for c in salesChristmas.columns if dept in c]\n    salesChristmas[cols] \\\n        .sum(axis=1).plot(figsize=(15, 5),\n              alpha=0.8,\n              title='Sales of items Department wise during Christmas')\nplt.legend(depts)\nplt.show()","4b239d92":"df['store_id'].unique() ## These are all the stores.","47132ce0":"df['dept_id'].unique() ## These are all the departments.","dafe1d8b":"stores = df['store_id'].unique()\ndepts = df['dept_id'].unique()\na = []\nconcat = []\nfor store in stores:\n    for dept in depts:\n        concat.append(store+'_'+dept)\n        cols = [c for c in past_sales1 if store in c and dept in c]\n        a.append(past_sales1[cols].sum(axis=1).values)","0bab00e6":"final = {}\nfor i in range(0, len(a)):\n    final[concat[i]] = a[i]","5f1c55d7":"final.keys()","ee2d2e88":"store_dept = pd.DataFrame(final)\nstore_dept.head()","72d2a38c":"store_dept['date'] = calendar['date']\nstore_dept['month'] = calendar['month']","90405644":"store_dept.head()","8f40997b":"store_dept['date'] = pd.to_datetime(store_dept['date'])","bc1e540f":"month_level = pd.DataFrame()","5bf0fcda":"store_dept.head()","891b61a3":"def store_cat_month(month_level):\n    for col in store_dept.drop(['date', 'month'], axis = 1).columns:\n        month_level[col] = store_dept.resample('M', on='date')[col].sum().values\n    return month_level","7348cae2":"month_level = store_cat_month(month_level)","943891f3":"month_level = month_level.reset_index()","b536ef01":"month_level.rename(columns = {'index': 'Month_'}, inplace = True)\nmonth_level['Month_'] += 1\nmonth_level.head()","ae664ad8":"month_level.shape","f88e76bb":"month_level = month_level.T","010a6c22":"month_level.head()","985500fe":"df.columns = df.iloc[0]\nmonth_level.columns = month_level.iloc[0]\nmonth_level.drop(month_level.index[0], inplace = True)","2dbfc617":"month_level.head()","3336811d":"month_level.drop([1], axis = 1, inplace = True)","261ad827":"month_level","de17a23d":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","0d1b83e9":"ss = StandardScaler()\nfeatures = ss.fit_transform(month_level)","c5b255a5":"features","d5753b8f":"kmeans_kwargs = {\n       \"init\": \"random\",\n       \"n_init\": 10,\n        \"max_iter\": 300, }\n# A list holds the SSE values for each k\nsse = []\nfor k in range(1, 20):\n    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n    kmeans.fit(features)\n    sse.append(kmeans.inertia_)","46a3f94c":"plt.plot(range(1, 20), sse)\nplt.xticks(range(1, 20))\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"SSE\")\nplt.show()","3c861822":"!pip install kneed","31823035":"from kneed import KneeLocator","816e54c4":"kl = KneeLocator(range(1, 20), sse, curve=\"convex\", direction=\"decreasing\")\nkl.elbow","c68c4ab6":"kmeans = KMeans(n_clusters=4, **kmeans_kwargs)\nkmeans.fit(features)","3d287b0b":"cluster_map = pd.DataFrame()\ncluster_map['data_index'] = month_level.index.values\ncluster_map['cluster'] = kmeans.labels_","179b762a":"cluster_map.head()","623e6fb7":"unique_clusters = cluster_map['cluster'].unique()\nclusters = {}\nfor cluster in unique_clusters:\n    similar_indices = cluster_map[cluster_map['cluster'] == cluster]['data_index']\n    clusters[cluster] = similar_indices\nclusters.keys()","0b7f817a":"fig, axes = plt.subplots(1, 4, figsize=(15, 5), sharey=True)\naxes = axes.flatten()\nax_idx = 0\nfor key in clusters.keys():\n    plt.axes(axes[ax_idx])\n    cluster = clusters[key].values\n    for i in range(0, len(cluster)):\n        plt.plot(month_level.loc[cluster[i]],\n                lw = 3,\n            )\n    ax_idx += 1\n    plt.title(\"C\" + str(key))\nplt.suptitle('Sales according to cluster number')\nplt.tight_layout()\nplt.show()","39d8b69e":"clusters[0]","0a146c59":"**From the sales_train_validation dataset given above we can observe that there are 30490 rows and 1919 columns, and if we observe the columns having 'd_' in it's name, we can conclude that sales data for 1913 days have been provided to us.**","bbc1bc29":"And sales_train_validation contains the sales of each of these 3049 items in each store.","a68dc4a2":"**This is the aggregation level we were talking about before, you can see that there are 70 keys formed by concatenating each store with each department i.e 10 (no of stores) * 7 (no of departments) = 70**\n","4252baa6":"It's very close to normal distribution.","9a245ffc":"**Let's now examine the sales of the above 3 items and weekly, monthly and yearly level.**","957967a8":"**The sales of the FOODS items is much much more than the HOUSEHOLD and HOBBIES category. You might be wondering why there is a certain dip in the sales for all the categories on a particular date, that's because it was a chritmas on those days.**","cd284035":"**The sales for three states: California, Texas and Wisconsin has been provided. And within each state, there are different stores for example: CA_1, CA_2, CA_3, and CA_4 in California.**","5a1e5818":"### Using 4 Clusters","f65e1a6d":"## Department Wise","83f14036":"**We can see above that this particular item had quite a lot of days where the sales of the item was zero**\n\n**We would also like to have the dates present on the x-axis instead of the day numbers, for that we will have to merge the sales_train_validation with the calendar dataset.**","92803500":"**Let's plot the sales for some more items say of HOBBIES_1_234_CA_3_validation and HOUSEHOLD_1_118_CA_3_validation**","33af07ed":"**Now let's randomly pick 20 more items and view their sales**","fdecd39a":"**We have created a new table named as 'past_sales' having number of columns as 30490, that has been formed by merging of 'sales_train_validation' and 'calendar' so that we can view the sales of all items for each date.**","56f3a26b":"**If we want to group together the stores according to the departments,we can club together the sales across stores for each category. Don't worry it will be clear going ahead.**","ef034a59":"#### Department Wise","b179a32d":"**Now we can observe above the sales of item 'FOODS_3_090_CA_3' for each date.**","6dba3262":"**past_sales1 represents the sales of all items for each date and containing the month, year, and wday as columns so that we can make some more slicing and dicing and analyze them**","0383a28e":"# Importing the Libraries","ffefcd18":"**There are 3049 unique items**","3359bb21":"# Sales Distribution","f9dc4173":"* Calendar dataset contains columns like 'date', 'weekday', 'wday', 'month' and 'year' etc which basically indicates the overall duration across which this dataset was made.\n\n* It also contains columns like 'event_name_1', 'event_name_2' which represents whether there was any holiday or event like christmas, father's day, etc that can lead to an increase in the sales of the items.","3e85a94e":"![walmart.jpeg](attachment:292a1cc7-2c87-4529-ac08-b9c03e7f4612.jpeg)","fbb8c41b":"**In US, June, July, and August are considered to be the summer months, therefore now we will aggregate data across these months.**","86fd6e43":"**Elbow method is a method that can be used for finding the right number of Clusters. By the look of the eye, it's difficult to judge the right cluster number, therefore we will use the 'kneed' library.**","9b52406a":"# Summers","389dad29":"Now there are no null values in the dataset.","8cd82409":"## Sales of Random Items","94fe01a6":"### Calendar","bd3028fd":"#### By States","c151fd6b":"# Exploratory Data Analysis on the Walmart Dataset\n\n**The M5 dataset contains hierarchical sales data, generously made available by Walmart, starting at the item level and aggregating to that of departments, product categories and stores in three geographical areas of the US: California, Texas, and Wisconsin.**","3adf795c":"#### FOOODS_3","451b9a82":"There are three different categories namely:\n* Hobbies\n\n* Household\n\n* Foods","2db7eba0":"**This means sales_train_validation represents the sale of each item in 10 different stores (3049 * 10 = 30490) for 1913 days**","d10bc837":"**Do keep in mind that:**\n\n**1 -> Saturday and 2 -> Sunday and so on**","59d28b4a":"**As only 3 days(29, 30, 31) are included in the first month, we will drop the first column.**","a92c528f":"**We can even view the stores_cat belonging to the clusters**","fd23f167":"### Monthly Level","66055bab":"**Therefore, month_level represents the sales at a monthly level for 64 months.**","e940fe12":"The sale was highest for CA_3 store","a8a3d6d6":"* Its surprising that for the items \"HOBBIES_1_234_CA_3\", the sale is peaking on the 4th day, instead of Saturday and Sunday, because for rest of the items the average sales is highest for 1st and 2nd day and then goes down for the rest of the days.\n\n* The sales of the item \"HOUSEHOLD_1_118_CA_3\" was very close to zero or was zero, maybe the sales of the product was discontinued.\n\n* There was a constant rise in the sales of the item \"HOBBIES_1_234_CA_3\" till 2015, after that it went down drastically.","86c8b7f3":"## Category","b5681754":"**The cluters has been differentiated based on the total number of items sold, each cluster has varying sales, and different lowest and highest sales. For example: C0 has stores_cat that have sales in range 50k to 100k, in contrast C3 have store_cat having sales in the range 0 to 20k.**","b26a8011":"### State Wise","dfc43926":"# Stores and Categories Clustering","78baad5d":"# Sales during Christmas or in December","d1e980e9":"We can observe that more than 80% of the values are missing for columns \"event_name_2\", \"event_type_2\", \"event_name_1\", \"event_type_1\". Therefore, we will remove thses columns.","4ea2d2f6":"**4 is the right number of clusters to be used.**","f50344c6":"**Let's analyze the sales of a random item say: FOODS_3_090_CA_3_validation. This id means the sales of 'FOODS_3_090' item in the store 'CA_3'.**","bb1e3773":"## Category Wise","6ec0e5d2":"**Christmas is a festival where we would expect the sales of the items to be higher as compared to all other days as people tend to buy a lot of stuff for their home as well as for gifting purposes. In order to analyze the sales during Christmas, we will take into account the whole December.**","636d533d":"### Dept wise","c7fb0443":"**In this notebook we are going to explore different features and time durations that plays an important role in the sale of the items across different stores. We will also try to cluster the stores_category that have similar sales.**\n\n**Do upvote the notebook if you liked it!**","f31dac5c":"# Exploratory Data Analysis","cf649a36":"### Category Wise","86eeb021":"**As expected the sales of the FOODS items was the highest among the all categories**","dfd3936f":"## Store Level","5327c890":"## State Wise","47bbd6c2":"#### Category Wise","f5f3ae67":"**And within each categories, there are different departments. For example: for Hobbies category we have departments like Hobbies_1 and Hobbies_2. Similarly for Household and Foods categories.**","d5aa9c44":"**Hope you liked the notebook, any suggestions would be highly appreciated.**\n\n**I will continue experimenting in future versions of the notebook.**\n\n**Please upvote if you liked it.**","384b8325":"**FOODS Category is the most sold product in Summers, and in particular FOODS_3 is sold the most, lets see it's distribution.**","cfb7f26e":"**We can see that the sales of the items follow a distribution that is very close to the normal distribution, therefore we can think of some methods like Power Transformation that can transform the distribution into a normal distributon.**"}}