{"cell_type":{"89c10ea3":"code","5a21c8d2":"code","cc5be145":"code","9dee8c03":"code","e932d510":"code","c00bf9e4":"code","925dfb86":"code","c331c3d1":"code","8bb457b4":"code","628419ae":"code","10ccaf73":"code","945a87ea":"code","ec152921":"code","354bcc3b":"code","96820159":"code","905b2bb0":"code","1414c331":"code","7f0b030f":"code","fc8ebf6e":"code","bd864cbf":"code","16a63c89":"code","15079a4c":"code","1ed781d2":"code","b5e23456":"code","2875748e":"code","ecdbeba5":"code","464dc943":"code","d7d32864":"code","15086b76":"code","1fc8429e":"code","0836cdf5":"code","e5388664":"code","7ab36d61":"code","7ce8d748":"code","52dd523a":"code","dc137ed1":"code","634f3724":"code","e501aa40":"code","e2bc2e5d":"markdown","47de9eef":"markdown","4d3c3b09":"markdown","597a15b3":"markdown"},"source":{"89c10ea3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport random\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom pandas import datetime\nimport math, time\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nimport datetime\nfrom operator import itemgetter\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os \nfor dirname, _, filenames in os.walk('\/kaggle\/input'):# \u041f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043f\u0440\u043e\u0439\u0442\u0438 \u043f\u043e \u0444\u0430\u0439\u043b\u0430\u043c \n    for i, filename in enumerate(filenames):\n        if i<5:\n            print(os.path.join(dirname,filename)) # \u041c\u0435\u0442\u043e\u0434 join \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0432\u0430\u043c \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u0438\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043f\u0443\u0442\u0435\u0439 \u043f\u0440\u0438 \u043f\u043e\u043c\u043e\u0449\u0438 \u043f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u0435\u043b\u044f\n# Any results you write to the current directory are saved as output.","5a21c8d2":"# symbols = ['aapl','goog','ibm']","cc5be145":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0434\u0435\u043b\u0430\u0435\u0442 \u0434\u0430\u0442\u0443 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0430\u043a\u0446\u0438\u044f\u043c\ndef stocks_data(symbols, dates):\n    df = pd.DataFrame(index=dates)\n    for symbol in symbols:\n        df_temp = pd.read_csv(\"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/{}.us.txt\".format(symbol), index_col='Date',\n                parse_dates=True, usecols=['Date', 'Close'], na_values=['nan'])\n        df_temp = df_temp.rename(columns={'Close': symbol})\n        df = df.join(df_temp)# \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e  join \u043c\u043e\u0436\u043d\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0440\u0430\u0431\u043e\u0447\u0438\u0439 \u043f\u0443\u0442\u044c \u0434\u043e \u0444\u0430\u0439\u043b\u0430\n    return df\n\ndates = pd.date_range('2015-01-02','2016-12-31',freq='B')\nsymbols = ['goog','ibm','aapl']\ndf = stocks_data(symbols, dates)\ndf.fillna(method='pad')\nprint(df)\n# \u0441\u0442\u043e\u0440\u043e\u0438\u043c \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0443\ndf.interpolate().plot()\nplt.show()","9dee8c03":"df.head()","e932d510":"# \u0441\u0442\u0440\u043e\u0438\u043c \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u043a\u0440\u0430\u0441\u043e\u0442\u0443\ndates = pd.date_range('2010-01-02','2017-10-11',freq='B')\ndf1=pd.DataFrame(index=dates)\ndf_ibm=pd.read_csv(\"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/ibm.us.txt\", parse_dates=True, index_col=0)\ndf_ibm=df1.join(df_ibm)\ndf_ibm[['Close']].plot()\nplt.ylabel(\"stock_price\")\nplt.title(\"IBM Stock\")\nplt.show()","c00bf9e4":"df_ibm=df_ibm[['Close']]\ndf_ibm.info()","925dfb86":"dates = pd.date_range('2010-01-02','2017-10-11',freq='B')\ndf1=pd.DataFrame(index=dates)\ndf_aapl=pd.read_csv(\"\/kaggle\/input\/price-volume-data-for-all-us-stocks-etfs\/Data\/Stocks\/aapl.us.txt\", parse_dates=True, index_col=0)\ndf_aapl=df1.join(df_aapl)\ndf_aapl[['Close']].plot()\nplt.ylabel(\"stock_price\")\nplt.title(\"Apple Stock\")\nplt.show()","c331c3d1":"df_aapl=df_aapl[['Close']]\ndf_aapl.info()","8bb457b4":"df_ibm = df_ibm.fillna(method = 'ffill')\ndf_aapl = df_aapl.fillna(method = 'ffill')\n\n# \u043f\u0435\u0440\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043e\u0442 -1 \u0434\u043e 1 (\u0442\u0438\u043f\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u0442 \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0443)\nscaler_1 = MinMaxScaler(feature_range=(-1, 1))\ndf_ibm['Close'] = scaler_1.fit_transform(df_ibm['Close'].values.reshape(-1,1))\n\nscaler_2 = MinMaxScaler(feature_range=(-1, 1))\ndf_aapl['Close'] = scaler_2.fit_transform(df_aapl['Close'].values.reshape(-1,1))","628419ae":"df_aapl=df1.join(df_aapl)\ndf_aapl[['Close']].plot()\nplt.ylabel(\"stock_price\")\nplt.title(\"Apple Stock\")\nplt.show()","10ccaf73":"plt.plot(df_aapl['Close'])\nplt.plot(df_ibm['Close'])\nplt.show()","945a87ea":"def load_data_2(stock_1 , stock_2, look_back):\n    data_raw_1= stock_1.as_matrix()\n    data_raw_2 = stock_2.as_matrix()\n    data_useful = []\n    \n    # \u0437\u0430\u0431\u0438\u0432\u0430\u0435\u043c \u0434\u0430\u0442\u0443 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438 \u0438\u043a\u0441-\u043e\u0432\n    for index in range(len(data_raw_1) - look_back - 21): \n        # \u0446\u0438\u043a\u043b \u0438\u0434\u0435\u0442 \u0434\u043e \u0447\u0438\u0441\u043b\u0430 \u0440\u0430\u0432\u043d\u043e\u0433\u043e \u0434\u043b\u0438\u043d\u043d\u0435 data_raw \u043c\u0438\u043d\u0443\u0441 look_back\n        # look_back \u044d\u0442\u043e \u043d\u0430 \u043a\u0430\u043a\u043e\u0439 \u0434\u0435\u043d\u044c \u043c\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c \u043d\u0430\u0437\u0430\u0434\n        data_1 = []\n        data_1.append(data_raw_1[index])\n        data_1.append(data_raw_1[index + 7])\n        data_1.append(data_raw_1[index + 14])\n        data_1.extend(data_raw_1[index+20: index+21+look_back])\n                \n        data_2 = []  \n        # \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0441\u043d\u0430\u0447\u0430\u043b\u0430 \u0432 \u043d\u043e\u0432\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u043d\u0443\u0436\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\n        # \u044d\u0442\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0442\u0438\u043f\u043e \u043e\u0442 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0445 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439\n        data_2.append(data_raw_2[index])\n        data_2.append(data_raw_2[index + 7])\n        data_2.append(data_raw_2[index + 14])\n        data_2.extend(data_raw_2[index+20: index+21+look_back])\n        # \u0434\u0435\u043b\u0430\u0435\u043c \u0438\u0437 \u0441\u043f\u0438\u0441\u043a\u0430  \u043c\u0430\u0441\u0441\u0438\u0432 numpy\n        data_2 = np.array(data_2)\n        # \u0434\u0435\u043b\u0430\u0435\u043c \u0435\u0433\u043e \u043d\u0430 \u0432\u0441\u044f\u043a\u0438\u0439 \u0441\u043b\u0443\u0447\u0430\u0439 -1 \u043d\u0430 1\n        data_2.reshape(-1,1)\n        data_1 = np.array(data_1)\n        data_1.reshape(-1,1)\n        data_3 = np.hstack((data_1 , data_2))\n        # \"\u043f\u0440\u0438\u043a\u043b\u0435\u0438\u0432\u0430\u0435\u043c\" \u0434\u0430\u0442\u0430 2 \u043a \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u0439 \u0434\u0430\u0442\u0435\n        data_useful.append(data_3)\n        if index == -1:\n            print(type(data_raw_1[index:index + look_back]))\n            print(data_raw_1[index:index + look_back].shape)\n    # \u0434\u0435\u043b\u0430\u0435\u043c \u0438\u0437 \u0441\u043f\u0438\u0441\u043a\u0430  \u043c\u0430\u0441\u0441\u0438\u0432 numpy\n    data_useful = np.array(data_useful);\n\n    test_set_size = int(np.round(0.3 * data_useful.shape[0]));\n    train_set_size = data_useful.shape[0] - (test_set_size);\n    \n    x_train_useful = data_useful[:train_set_size,:-1,:]\n    y_train_useful = data_useful[:train_set_size,-1,:]\n    \n    x_test_useful  = data_useful [train_set_size:,:-1]\n    y_test_useful  = data_useful [train_set_size:,-1,:]\n    \n    return [x_train_useful , y_train_useful , x_test_useful , y_test_useful]","ec152921":"look_back = 10\nx_train_useful, y_train_useful, x_test_useful, y_test_useful = load_data_2(df_aapl, df_ibm, look_back)\nprint('x_train_useful.shape = ',x_train_useful.shape)\nprint('y_train_useful.shape = ',y_train_useful.shape)\nprint('x_test_useful.shape = ',x_test_useful.shape)\nprint('y_test_useful.shape = ',y_test_useful.shape)\n","354bcc3b":"# \u0414\u0435\u043b\u0430\u0435\u043c \u0442\u0435\u043d\u0437\u043e\u0440\u044b \nx_train_useful = torch.from_numpy(x_train_useful).type(torch.Tensor)\nx_test_useful = torch.from_numpy(x_test_useful).type(torch.Tensor)\ny_train_useful = torch.from_numpy(y_train_useful).type(torch.Tensor)\ny_test_useful = torch.from_numpy(y_test_useful).type(torch.Tensor)\n\nprint(x_train_useful.size(), y_train_useful.size())\n\nn_steps = look_back-1\nbatch_size = 1606\nnum_epochs = 350\n#n_iters = 3000\n#n_iters \/ (len(train_X) \/ batch_size)\n#num_epochs = int(num_epochs)\n\n# train_useful = torch.utils.data.TensorDataset(x_train_useful,y_train_useful)\n# test_useful = torch.utils.data.TensorDataset(x_test_useful,y_test_useful)\n\n# train_loader_useful = torch.utils.data.DataLoader(dataset=train_useful, \n#                                            batch_size=batch_size, \n#                                            shuffle=False)\n\n# test_loader_useful = torch.utils.data.DataLoader(dataset=test_useful, \n#                                           batch_size=batch_size, \n#                                           shuffle=False)","96820159":"# Build model\n#####################\ninput_dim = 2\nhidden_dim = 32\nnum_layers = 2 \noutput_dim = 2\n\n# Here we define our model as a class\nclass LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.num_layers = num_layers\n\n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # Initialize cell state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        out = self.dropout(out)\n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        \n        # out.size() --> 100, 10\n        return out\n    \nmodel = LSTM(input_dim = input_dim, hidden_dim = hidden_dim, output_dim = output_dim, num_layers = num_layers)\n\nloss_fn = torch.nn.MSELoss(size_average=True)\n\noptimiser = torch.optim.Adam(model.parameters(), lr=0.01)\nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","905b2bb0":"# Train model\n#####################\n# Build model\n#####################\ninput_dim = 2\nhidden_dim = 32\nnum_layers = 2 \noutput_dim = 2\nnum_epochs = 100\nhist = np.zeros(num_epochs)\n\n# Number of steps to unroll\nseq_dim =look_back-1  \nmodel.train()\nfor t in range(num_epochs):\n    # Initialise hidden state\n    # Don't do this if you want your LSTM to be stateful\n    #model.hidden = model.init_hidden()\n    \n    #print(type(x_train_aapl))\n    # Forward pass\n    y_train_pred = model(x_train_useful)\n\n    loss = loss_fn(y_train_pred, y_train_useful)\n    \n    print(loss)\n    if t % 10 == 0 and t !=0:\n        print(\"Epoch \", t, \"MSE: \", loss.item())\n    hist[t] = loss.item()\n\n    # Zero out gradient, else they will accumulate between epochs\n    optimiser.zero_grad()\n\n    # Backward pass\n    loss.backward()\n\n    # Update parameters\n    optimiser.step()","1414c331":"plt.plot(hist, label=\"Training loss\")\nplt.legend()\nplt.show()","7f0b030f":"model.train(False)","fc8ebf6e":"\n# C\u0442\u0440\u043e\u0438\u043c \u0434\u043b\u044f apple train\nplt.plot(y_train_pred.detach().numpy()[:,0], label=\"Apple\")\nplt.plot(y_train_useful.detach().numpy()[:,0])\nplt.legend()\nplt.show()","bd864cbf":"plt.plot(y_train_pred.detach().numpy()[:,1], label=\"Ibm\")\nplt.plot(y_train_useful.detach().numpy()[:,1])\nplt.legend()\nplt.show()","16a63c89":"np.shape(y_train_pred)","15079a4c":"y_train_useful.detach().numpy().shape","1ed781d2":"y_train_pred = model(x_train_useful)\ny_test_pred = model(x_test_useful)","b5e23456":"print(x_train_pred.shape)\nprint(x_test_pred.shape)","2875748e":"# \u0421\u0442\u0440\u043e\u0438\u043c \u0434\u043b\u044f apple test\nplt.plot(y_test_pred.detach().numpy()[:,0], label=\"Apple\")\nplt.plot(y_test_useful.detach().numpy()[:,0] , label=\"True\")\nplt.legend()\nplt.show()","ecdbeba5":"# \u0421\u0442\u0440\u043e\u0438\u043c \u0434\u043b\u044f IBM test\nplt.plot(scaler_2.inverse_transform(x_test_pred.detach().numpy()[:,1].reshape(-1, 1)), label=\"Ibm\")\nplt.plot(scaler_2.inverse_transform(y_test_useful.detach().numpy()[:,1].reshape(-1, 1)))\nplt.legend()\nplt.show()","464dc943":"# \u0440\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 aapl \u0438 ibm\ny_train_pred_aapl = x_train_pred[:,1]\ny_train_pred_ibm = x_train_pred[:,0]\ny_test_pred_aapl = x_test_pred[:,1]\ny_test_pred_ibm = x_test_pred[:,0]\n\n# \u0434\u0435\u043b\u0430\u0435\u043c reshape\ny_train_useful[:,1].detach().numpy().reshape(-1, 1).shape\ny_train_pred_aapl.reshape(-1, 1).shape","d7d32864":"y_test_pred_aapl.size()","15086b76":"y_train_pred_aapl.shape","1fc8429e":"y_test_pred_aapl.shape","0836cdf5":"y_train_pred[:2]","e5388664":"y_train_useful[:2]","7ab36d61":"y_train_pred_aapl[:2]","7ce8d748":"########################### apple\ny_train_pred_aapl = scaler_2.inverse_transform(y_train_pred_aapl.detach().numpy().reshape(-1, 1))\ny_test_pred_aapl = scaler_2.inverse_transform(y_test_pred_aapl.detach().numpy().reshape(-1, 1))\n\n# \u0421\u0447\u0438\u0442\u0430\u0435\u043c RMSE \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u043e\u0439 \u043e\u0448\u0438\u0431\u043a\u0438\ntrainScore_aapl = math.sqrt(mean_squared_error(scaler_2.inverse_transform(y_train_useful[:,1].detach().numpy().reshape(-1, 1)) , y_train_pred_aapl.reshape(-1, 1)))\nprint('Train Score Apple: %.2f RMSE' % (trainScore_aapl))\ntestScore_aapl = math.sqrt(mean_squared_error(scaler_2.inverse_transform(y_test_useful[:,1].detach().numpy().reshape(-1, 1)), y_test_pred_aapl.reshape(-1, 1) ))\nprint('Test Score Apple: %.2f RMSE' % (testScore_aapl))\n\n########################### ibm\ny_train_pred_ibm = scaler_1.inverse_transform(y_train_pred_ibm.detach().numpy().reshape(-1, 1))\ny_test_pred_ibm = scaler_1.inverse_transform(y_test_pred_ibm.detach().numpy().reshape(-1, 1))\n\n# \u0421\u0447\u0438\u0442\u0430\u0435\u043c RMSE \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0441\u0440\u0435\u0434\u043d\u0435\u0439 \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0438\u0447\u043d\u043e\u0439 \u043e\u0448\u0438\u0431\u043a\u0438\ntrainScore_ibm = math.sqrt(mean_squared_error(scaler_1.inverse_transform(y_train_useful[:,1].detach().numpy().reshape(-1, 1)), y_train_pred_ibm.reshape(-1, 1)))\nprint('Train Score Ibm: %.2f RMSE' % (trainScore_ibm))\ntestScore_ibm = math.sqrt(mean_squared_error(scaler_1.inverse_transform(y_test_useful[:,1].detach().numpy().reshape(-1, 1)) , y_test_pred_ibm.reshape(-1, 1) ))\nprint('Test Score Ibm: %.2f RMSE' % (testScore_ibm))","52dd523a":"scaler_1.inverse_transform(y_test_useful[:,1].detach().numpy().reshape(-1, 1))[:5]","dc137ed1":"y_test_pred_ibm.reshape(-1, 1)[:5]","634f3724":"# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(df_aapl)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back + 21:len(y_train_pred_aapl)  + 21 +look_back, :] = y_train_pred_aapl\n\nprint(trainPredictPlot)\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df_aapl)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(y_train_pred_aapl)+ 21 +look_back-1:len(df_aapl)-1, :] = y_test_pred_aapl\n\n# plot baseline and predictions\nplt.figure(figsize=(16,8))\nplt.plot(scaler_2.inverse_transform(df_ibm))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()\n","e501aa40":"# shift train predictions for plotting\ntrainPredictPlot = np.empty_like(df_ibm)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[look_back + 21:len(y_train_pred_aapl)+look_back +21 , :] = y_train_pred_ibm\n\nprint(trainPredictPlot)\n# shift test predictions for plotting\ntestPredictPlot = np.empty_like(df_ibm)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(y_train_pred_ibm)+ 21 +look_back-1:len(df_ibm)-1, :] = y_test_pred_ibm\n\n# plot baseline and predictions\nplt.figure(figsize=(16,8))\nplt.plot(scaler_1.inverse_transform(df_aapl))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","e2bc2e5d":"In this notebook we will be building and training LSTM to predict IBM stock. We will use PyTorch.","47de9eef":"## 1. Libraries and settings","4d3c3b09":"## 3. Build the structure of model","597a15b3":"## 2. Analyze data"}}