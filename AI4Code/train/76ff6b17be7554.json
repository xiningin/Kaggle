{"cell_type":{"0c0c76cb":"code","35df8236":"code","be535260":"code","b79c0ada":"code","8f4ad041":"code","11e82c93":"code","39b43cdd":"code","f8e628b2":"code","1cefd5af":"code","eae7a8d5":"code","3eb87ce3":"code","9028e19a":"code","388f4378":"code","c133bb06":"code","9ebd4d0c":"markdown"},"source":{"0c0c76cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35df8236":"# Setting up Spark \n!pip install pyspark\n","be535260":"# Entery Point of Spark \nfrom pyspark.sql import SparkSession\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Exploratory Analysis\") \\\n    .getOrCreate()","b79c0ada":"# importing DataSet\nP_Data_Set = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load('\/kaggle\/input\/nyc-parking-tickets\/Parking_Violations_Issued_-_Fiscal_Year_2015.csv')\nP_Data_Set","8f4ad041":"# Exploring Data_Set, Display summary statistics\nP_Data_set.head(5)\nP_Data_Set.tail(2)\nP_Data_set.describe().show()","11e82c93":"# datatype of columns\nP_Data_Set.printSchema()","39b43cdd":"P_Data_Set=P_Data_Set.dropDuplicates()\nP_Data_Set = P_Data_Set.dropna()\nP_Data_Set.count()","f8e628b2":"# converting the Data set into a data frame\nP_Data_Set = P_Data_Set.toDF(*(c.replace(' ', '_') for c in parking.columns))\nparking.show(3)","1cefd5af":"# creating a local temporary view of the table, to avoid changed in the orginal data set \nparking.createOrReplaceTempView(\"Ptable\")\nspark.sql('Select * from Ptable') ","eae7a8d5":"# Total number of tickets for each year\nsql_ticket_year = spark.sql(\"select year(Issue_Date) as year, count(Summons_Number) as no_of_tickets from Ptable group by year order by year\")\nsql_ticket_year.show(50)\nsql_ticket_year.count()\n# error while counting the ticket number ","3eb87ce3":"# Specfying the scope of the analysis to 2015 data \nparking.createOrReplaceTempView(\"tble_view2015\")\nparking=spark.sql(\"select * from tble_view2015 where year(TO_DATE(CAST(UNIX_TIMESTAMP(Issue_Date,'MM\/dd\/yyyy') AS TIMESTAMP))) = 2015 \")\nparking.count()\n# error while filting the data  ","9028e19a":"P_Data_Set.createOrReplaceTempView(\"tble_view2015\")\n#Showing distribution of tickets over the year \nDistribution_on_years= spark.sql(\"SELECT year(Issue_Date) as year,month(Issue_Date) as month,count(*) as Ticket_Frequency FROM tble_view2015 GROUP BY year(Issue_Date),month(Issue_Date) order by Ticket_Frequency desc\")","388f4378":"Distribution_on_years.show()\nNumber_of_Violations_by_month = Distribution_on_years.toPandas()","c133bb06":"Number_of_Violations_by_month.plot(x= 'month', y='Ticket_Frequency', kind='line')\nplt.title(\"Violations on the basis of month in 2015\")\nplt.xlabel('month')\nplt.ylabel('Ticket_Frequency')\nplt.show()","9ebd4d0c":"#PreProcessing of Data such as Droping Duplicates, removing null values "}}