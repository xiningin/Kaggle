{"cell_type":{"a899909d":"code","d9ae02c3":"code","a8177ec9":"code","faed2eb6":"code","438cab43":"code","4b45a0e8":"code","0e68e3b6":"code","19697760":"markdown"},"source":{"a899909d":"import shap\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.ensemble import RandomForestClassifier","d9ae02c3":"# Get the dataset and fit a Random Forest on it\nX, y = load_wine(return_X_y=True, as_frame=True)\n\nrf = RandomForestClassifier()\nrf.fit(X, y)","a8177ec9":"# Runs the explainer on the model and the dataset to grab the Shap Values\nexplainer = shap.Explainer(rf)\nshap_values = explainer(X)","faed2eb6":"# The return of the explainer has three matrices, we will get the shap values one\nshap_values = shap_values.values[:, :, 0]","438cab43":"# Now, let's get the interaction values\np_all = explainer.shap_interaction_values(X)[0]","4b45a0e8":"def generate_SIR_SHAP_metrics(shap_values, shap_interaction_values):\n\n    # Define matrices to be filled\n    s = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))\n    a = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))\n    r = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))\n    i_ = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))\n\n    S = np.zeros((shap_values.shape[1], shap_values.shape[1]))\n    R = np.zeros((shap_values.shape[1], shap_values.shape[1]))\n    I = np.zeros((shap_values.shape[1], shap_values.shape[1]))\n\n    for i in range(shap_values.shape[1]):\n        # Selects the p_i vector -> Shap Values vector for feature i\n        pi = shap_values[:, i]\n        \n        for j in range(shap_values.shape[1]):\n            # Selects pij -> SHAP interaction vector between features i and j\n            pij = shap_interaction_values[:, i, j]\n            \n            # Other required vectors\n            pji = shap_interaction_values[:, j, i]\n            pj = shap_values[:, j]\n\n            # Synergy vector\n            s[i, j] = (np.inner(pi, pij) \/ np.linalg.norm(pij)**2) * pij\n            s[j, i] = (np.inner(pj, pji) \/ np.linalg.norm(pji)**2) * pji\n\n            # Autonomy vector\n            a[i,j] = pi - s[i, j]\n            a[j,i] = pj - s[j, i]\n\n            # Redundancy vector\n            r[i,j] = (np.inner(a[i, j], a[j, i]) \/ np.linalg.norm(a[j, i])**2) * a[j, i]\n            r[j,i] = (np.inner(a[j, i], a[i, j]) \/ np.linalg.norm(a[i, j])**2) * a[i, j]\n\n            # Independece vector\n            i_[i, j] = a[i, j] - r[i, j]\n            i_[j, i] = a[j, i] - r[j, i]\n\n            # Synergy value\n            S[i, j] = np.linalg.norm(s[i, j])**2 \/ np.linalg.norm(pi)**2\n\n            # Redundancy value\n            R[i, j] = np.linalg.norm(r[i, j])**2 \/ np.linalg.norm(pi)**2\n\n            # Independence value\n            I[i, j] = np.linalg.norm(i_[i, j])**2 \/ np.linalg.norm(pi)**2\n            \n    return S, I, R","0e68e3b6":"S, I, R = generate_SIR_SHAP_metrics(shap_values, p_all)","19697760":"# SHAP S-I-R Global Explanations\n\nOn this notebook we will review and implement the results from [Feature Synergy, Redundancy, and Independence in Global Model Explanations using SHAP Vector Decomposition](https:\/\/arxiv.org\/abs\/2107.12436), which proposes a new method of finding global explanations from Shap Values in a way we can improve our understanding of our datasets and models and also to generate a feature selection method.\n\nThe main interpretation here is the geometric position of the vectors grabbed from the paper:\n\n![image.png](attachment:b3b235f8-8f12-4c25-a861-c9c2922753cb.png)"}}