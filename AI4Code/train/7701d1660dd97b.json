{"cell_type":{"c7729438":"code","56956c1e":"code","c6deb18c":"code","8803388b":"code","ada8a0a1":"code","ffdd46dc":"markdown","5d25e325":"markdown","94e63386":"markdown","7216b126":"markdown"},"source":{"c7729438":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56956c1e":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle\nimport pandas as pd\nfrom tqdm import tqdm\nimport re\nimport json\nimport unicodedata\n\nclass baseTokenizer():\n    \"\"\"Base Tokenizer Class (Inherited by all subclasses) \"\"\"\n    def __init__(self):\n        self.PAD_WORD = '[PAD]'\n        self.UNK_WORD = '[UNK]'\n    \n    def unicodeToAscii(self, utterance):\n        \"\"\" Normalize strings\"\"\"\n        return ''.join(\n            c for c in unicodedata.normalize('NFD', utterance)\n            if unicodedata.category(c) != 'Mn'\n        )\n\n    def normalizeString(self, raw_utterance):\n        \"\"\"Remove nonalphabetics for each utterance\"\"\"\n        str = self.unicodeToAscii(raw_utterance.lower().strip())\n        str = re.sub(r\"([,.'!?])\", r\" \\1\", str)\n        str = re.sub(r\"[^a-zA-Z,.'!?]+\", r\" \", str)\n        return str\n    \n    def process(self, utterance):\n        pass\n\nclass gloveTokenizer(baseTokenizer):\n    \"\"\"Glove Tokenizer for Glove Embedding (End2End Model)\"\"\"\n    def __init__(self, vocab_path):\n        super(gloveTokenizer, self).__init__()\n        self.PAD = 0\n        self.UNK = 1\n        self.word2id = None\n        self.loadVocabFromJson(vocab_path)\n\n    def loadVocabFromJson(self, path):\n        self.word2id = json.load(open(path))\n\n    def process(self, utterance):\n        # baseTokenizer.normalizeString : remove nonalphabetics\n        utterance = self.normalizeString(utterance)\n        # transform into lower mode.\n        wordList = [word.lower() for word in utterance.split()]\n        indexes = [self.word2id.get(word, self.UNK) for word in wordList] # unk: 1\n        return indexes\n\nclass IEMOCAPDataset(Dataset):\n\n    def __init__(self, dataset_path, vocab_path, mode='train'):\n        self.tokenizer_ = gloveTokenizer(vocab_path)\n\n        self.videoIDs, self.videoSpeakers, self.videoLabels, self.videoText,\\\n        self.videoAudio, self.videoVisual, self.videoSentence, self.trainVid,\\\n        self.testVid = pickle.load(open(dataset_path, 'rb'), encoding='latin1')\n        '''\n        label index mapping = {'hap':0, 'sad':1, 'neu':2, 'ang':3, 'exc':4, 'fru':5}\n        '''\n        self.validVid, self.trainVid = self.trainVid[:12], self.trainVid[12:]\n\n\n        self.utterance_len = dict()\n        for dialogue_key in self.videoSentence.keys():\n            # word2ids & transform indexes into tensor to use pad_sequence\n            self.videoSentence[dialogue_key] = [torch.tensor(self.tokenizer_.process(utterance)).view(-1, 1)\n                                                    for utterance in self.videoSentence[dialogue_key]]\n            # get each utterance in a dialogue.\n            self.utterance_len[dialogue_key] = [len(utterance) for utterance in self.videoSentence[dialogue_key]]\n            # padding each utterance in a dialogue into same length. dict: key -> [utterance_num, ]\n            self.videoSentence[dialogue_key] = pad_sequence(self.videoSentence[dialogue_key],\n                                                            batch_first=True, padding_value=self.tokenizer_.PAD).squeeze()\n        \n        if mode == 'train':\n            self.keys = [x for x in self.trainVid]\n        elif mode == 'valid':\n            self.keys = [x for x in self.validVid]\n        elif mode == 'test':\n            self.keys = [x for x in self.testVid ]\n        self.keys.sort()\n        self.len = len(self.keys)\n\n    def __getitem__(self, index):\n        vid = self.keys[index]\n        return  self.videoSentence[vid],\\\n                torch.FloatTensor(self.utterance_len[vid]),\\\n                torch.FloatTensor(self.videoVisual[vid]),\\\n                torch.FloatTensor(self.videoAudio[vid]),\\\n                torch.FloatTensor([[1,0] if x=='M' else [0,1] for x in self.videoSpeakers[vid]]),\\\n                torch.FloatTensor([1]*len(self.videoLabels[vid])),\\\n                torch.LongTensor(self.videoLabels[vid]),\\\n                vid\n\n    def __len__(self):\n        return self.len\n\n\nclass IEMOCAPPadCollate:\n    \"\"\"How to process batch data into tensor. Used in ERCDataloader.\"\"\"\n    def __init__(self, dim=1):\n        self.dim = dim\n\n    def pad_tensor(self, vec, pad, dim):\n\n        pad_size = list(vec.shape)\n        pad_size[dim] = pad - vec.size(dim)\n        return torch.cat([vec, torch.zeros(*pad_size).type(torch.LongTensor)], dim=dim)\n\n    def pad_collate(self, batch):\n        \n        # find longest sequence\n        max_len = max(map(lambda x: x.shape[self.dim], batch))\n        \n        # pad according to max_len\n        batch = [self.pad_tensor(x, pad=max_len, dim=self.dim) for x in batch]\n        \n        # stack all\n        return pad_sequence(batch)\n    \n    def __call__(self, batch):\n        dat = pd.DataFrame(batch)\n        \n        return [self.pad_collate(dat[i]) if i==0 else \\\n                pad_sequence(dat[i], True) if i < 7 else \\\n                dat[i].tolist() for i in dat]\n\n\ndef ERCDataLoader(args):\n    \"\"\"\n    Returns: For End2End mode: [videoSentence], [utterance_len], [videoVisual], [videoAudio], [speaker_mask]\n                            [global_mask], [label], [vid] \n    \"\"\"\n    datasets = {\n        'train' : IEMOCAPDataset(dataset_path=args.data_path, vocab_path=args.vocabPath, mode='train'),\n        'valid' : IEMOCAPDataset(dataset_path=args.data_path, vocab_path=args.vocabPath, mode='valid'),\n        'test'  : IEMOCAPDataset(dataset_path=args.data_path, vocab_path=args.vocabPath, mode='test' ) \n    }\n\n    dataLoader = dict()\n\n    dataLoader['train'] = DataLoader(datasets['train'], batch_size=args.batch_size, \n                                            collate_fn=IEMOCAPPadCollate(dim=1), num_workers=args.num_workers)\n    dataLoader['valid'] = DataLoader(datasets['valid'], batch_size=args.batch_size,\n                                            collate_fn=IEMOCAPPadCollate(dim=1), num_workers=args.num_workers)\n    dataLoader['test' ] = DataLoader(datasets['test'], batch_size=args.batch_size,\n                                            collate_fn=IEMOCAPPadCollate(dim=1), num_workers=args.num_workers)\n    \n    return dataLoader","c6deb18c":"# Dataloader Unit Test.\nimport argparse\n\nargs = argparse.ArgumentParser()\n\nargs.data_path = '\/kaggle\/input\/iemocap\/IEMOCAP_features.pkl'\nargs.num_workers = 8\nargs.batch_size = 2\nargs.vocabPath = '\/kaggle\/input\/glove6b-iemocap\/IEMOCAP_vocab.json'\n\ndataloader = ERCDataLoader(args)\nwith tqdm(dataloader['train']) as td:\n    for index, batch_data in enumerate(td):\n        textf, text_len, visuf, acouf, party_mask, mask, label = batch_data[:-1]\n        \n        print(textf.shape)\n        print(text_len)\n        print(visuf.shape)\n        print(acouf.shape)\n        print(party_mask.shape)\n        print(mask.shape)\n        print(label.shape)\n        break","8803388b":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport pickle\n\nif torch.cuda.is_available():\n    FloatTensor = torch.cuda.FloatTensor\n    LongTensor = torch.cuda.LongTensor\n    ByteTensor = torch.cuda.ByteTensor\n\nelse:\n    FloatTensor = torch.FloatTensor\n    LongTensor = torch.LongTensor\n    ByteTensor = torch.ByteTensor\n\nclass MaskedNLLLoss(nn.Module):\n\n    def __init__(self, weight=None):\n        super(MaskedNLLLoss, self).__init__()\n        self.weight = weight\n        self.loss = nn.NLLLoss(weight=weight,\n                               reduction='sum')\n\n    def forward(self, pred, target, mask):\n        \"\"\"\n        pred -> batch * seq_len, n_classes\n        target -> batch * seq_len\n        mask -> batch, seq_len\n        \"\"\"\n        mask_ = mask.view(-1,1) # batch * seq_len, 1\n        if type(self.weight)==type(None):\n            loss = self.loss(pred*mask_, target)\/torch.sum(mask)\n        else:\n            loss = self.loss(pred*mask_, target)\\\n                            \/torch.sum(self.weight[target]*mask_.squeeze())\n        return loss\n\nclass gloveEmbedding(nn.Module):\n    \"\"\"w\/o finetune glove embedding.\n    \"\"\"\n    def __init__(self, glove_embedding_path):\n        super(gloveEmbedding, self).__init__()\n        embedding_matrix = pickle.load(open(glove_embedding_path, 'rb'))\n        self.embedding = nn.Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1], padding_idx=0)\n        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n        \n        self.embedding.weight.requires_grad = False\n    \n    def forward(self, token_seq):\n        return self.embedding(token_seq)\n\nclass CNNFeatureExtractor(nn.Module):\n    \"\"\"TextCNN feature exctractor (Conv1d + MaxPooling + linear)\"\"\"\n    \n    def __init__(self, glove_embedding_path, embedding_dim=300, output_size=100, filters=50, kernel_sizes=[3,4,5], dropout=0.5):\n        super(CNNFeatureExtractor, self).__init__()\n\n        self.embedding = gloveEmbedding(glove_embedding_path)\n\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels=embedding_dim, out_channels=filters, kernel_size=K) for K in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(len(kernel_sizes) * filters, output_size)\n        self.feature_dim = output_size\n\n\n    def forward(self, x, umask):\n        \n        num_utt, batch, num_words = x.size()\n        \n        x = x.type(LongTensor)  # (num_utt, batch, num_words)\n        x = x.view(-1, num_words) # (num_utt, batch, num_words) -> (num_utt * batch, num_words)\n        emb = self.embedding(x) # (num_utt * batch, num_words) -> (num_utt * batch, num_words, embedding_dim) \n        emb = emb.transpose(-2, -1).contiguous() # (num_utt * batch, num_words, embedding_dim)  -> (num_utt * batch, embedding_dim, num_words) \n        \n        convoluted = [F.relu(conv(emb)) for conv in self.convs] # [(num_utt * batch, out_channels, num_words-conv1d_kernel+1)]\n        pooled = [F.max_pool1d(c, c.size(2)).squeeze() for c in convoluted] # [(num_utt * batch, out_channels)]\n        concated = torch.cat(pooled, 1)\n        features = F.relu(self.fc(self.dropout(concated))) # (num_utt * batch, out_channels * len(kernel_size) ) -> (num_utt * batch, output_size)\n        features = features.view(num_utt, batch, -1) # (num_utt * batch, output_size) -> (num_utt, batch, output_size)\n        mask = umask.unsqueeze(-1).type(FloatTensor) # (batch, num_utt) -> (batch, num_utt, 1)\n        mask = mask.transpose(0, 1) # (batch, num_utt, 1) -> (num_utt, batch, 1)\n        mask = mask.repeat(1, 1, self.feature_dim) #  (num_utt, batch, 1) -> (num_utt, batch, output_size)\n        features = (features * mask) # (num_utt, batch, output_size) -> (num_utt, batch, output_size)\n\n        return features\n\nclass CnnModel(nn.Module):\n    def __init__(self, args):\n        super(CnnModel, self).__init__()\n\n\n        self.cnn_feat_extractor = CNNFeatureExtractor(args.glove_embedding_path, args.embedding_dim,\n         args.cnn_output_size, args.cnn_filters, args.cnn_kernel_sizes, args.cnn_dropout)\n        \n        self.classifier = nn.Linear(args.utterance_dim, args.n_classes)\n\n\n    def forward(self, text_f, seq_len, video, audio, party_mask, mask):\n\n        text_feature = self.cnn_feat_extractor(text_f, mask)\n\n        log_prob = F.log_softmax(self.classifier(text_feature), 2)\n        return log_prob","ada8a0a1":"import pandas as pd\nimport numpy as np, argparse, time, pickle, random, os, datetime\nimport torch.optim as optim\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n\ndef setup_seed(seed):\n    \"\"\" Manually Fix the random seed to get deterministic results.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\ndef train_or_eval_model(model, loss_function, dataloader, epoch, optimizer=None, train=False):\n    losses, preds, labels, masks, losses_sense  = [], [], [], [], []\n    \n    max_sequence_len = []\n\n    assert not train or optimizer!=None\n    if train:\n        model.train()\n    else:\n        model.eval()\n\n    with tqdm(dataloader) as td:\n        for data in td:\n\n            if train:\n                optimizer.zero_grad()\n                \n            textf, text_len, visuf, acouf, party_mask, mask, label = [d.cuda() for d in data[:-1]] if args.cuda else data[:-1]\n\n            log_prob = model(textf, text_len, visuf, acouf, party_mask, mask)\n\n            lp_ = log_prob.transpose(0,1).contiguous().view(-1, log_prob.size()[2]) # batch*seq_len, n_classes\n            labels_ = label.view(-1) # batch*seq_len\n            loss = loss_function(lp_, labels_, mask)\n\n            pred_ = torch.argmax(lp_,1) # batch*seq_len\n            preds.append(pred_.data.cpu().numpy())\n            labels.append(labels_.data.cpu().numpy())\n            masks.append(mask.view(-1).cpu().numpy())\n            losses.append(loss.item()*masks[-1].sum())\n\n            if train:\n                total_loss = loss\n                total_loss.backward()\n                \n                optimizer.step()\n\n    if preds!=[]:\n        preds  = np.concatenate(preds)\n        labels = np.concatenate(labels)\n        masks  = np.concatenate(masks)\n    else:\n        return float('nan'), float('nan'), float('nan'), [], [], [], float('nan'),[]\n\n    avg_loss = round(np.sum(losses)\/np.sum(masks), 4)\n    avg_sense_loss = round(np.sum(losses_sense)\/np.sum(masks), 4)\n\n    avg_accuracy = round(accuracy_score(labels,preds, sample_weight=masks)*100, 2)\n    avg_fscore = round(f1_score(labels,preds, sample_weight=masks, average='weighted')*100, 2)\n    \n    return avg_loss, avg_accuracy, labels, preds, masks, [avg_fscore]\n\n\nargs = argparse.ArgumentParser()\nargs.num_workers = 0\n\n# dataloader settings.\nargs.batch_size = 16\nargs.data_path = '\/kaggle\/input\/iemocap\/IEMOCAP_features.pkl'\nargs.vocabPath = '\/kaggle\/input\/glove6b-iemocap\/IEMOCAP_vocab.json'\n\n# model settings.\nargs.glove_embedding_path = '\/kaggle\/input\/glove6b-iemocap\/IEMOCAP_embedding.pkl'\nargs.embedding_dim = 300\nargs.cnn_output_size = 100\nargs.utterance_dim = 100\nargs.cnn_filters = 50\nargs.cnn_kernel_sizes = [3, 4, 5]\nargs.cnn_dropout = 0.5\nargs.n_classes = 6\n\n# train settings.\nargs.lr = 0.001\nargs.l2 = 0.0005\nargs.epochs = 100\n\n\nargs.cuda = torch.cuda.is_available()\nif args.cuda:\n    print('Running on GPU')\nelse:\n    print('Running on CPU')\n\nfor seed in [1, 11, 111, 1111, 11111]:\n    setup_seed(seed)\n    args.seed = seed\n\n    print(args)\n\n    model = CnnModel(args)\n    print('IEMOCAP CNN MODULE ...')\n\n    if args.cuda:\n        model.cuda()\n\n    loss_weights = torch.FloatTensor([1\/0.086747, 1\/0.144406, 1\/0.227883, 1\/0.160585, 1\/0.127711, 1\/0.252668])\n\n    loss_function  = MaskedNLLLoss(loss_weights.cuda() if args.cuda else loss_weights)\n\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.l2)\n    lf = open('\/kaggle\/working\/cnn_iemocap_logs.txt', 'a')\n\n    dataloader = ERCDataLoader(args)\n\n    valid_losses, valid_fscores = [], []\n    test_fscores, test_losses = [], []\n    best_loss, best_label, best_pred, best_mask = None, None, None, None\n\n    for e in range(args.epochs):\n        start_time = time.time()\n        train_loss, train_acc, _, _, _, train_fscore = train_or_eval_model(model, loss_function, dataloader['train'], e, optimizer, True)\n        valid_loss, valid_acc, _, _, _, valid_fscore = train_or_eval_model(model, loss_function, dataloader['valid'], e)\n        test_loss, test_acc, test_label, test_pred, test_mask, test_fscore = train_or_eval_model(model, loss_function, dataloader['test'], e)\n\n        valid_losses.append(valid_loss)\n        valid_fscores.append(valid_fscore)\n        test_losses.append(test_loss)\n        test_fscores.append(test_fscore)\n\n        x = 'epoch: {}, train_loss: {}, acc: {}, fscore: {}, valid_loss: {}, acc: {}, fscore: {}, test_loss: {}, acc: {}, fscore: {}, time: {} sec'.format(e+1, train_loss, train_acc, train_fscore, valid_loss, valid_acc, valid_fscore, test_loss, test_acc, test_fscore, round(time.time()-start_time, 2))\n\n        print (x)\n        lf.write(x + '\\n')\n\n    valid_fscores = np.array(valid_fscores).transpose()\n    test_fscores = np.array(test_fscores).transpose()\n\n    score1 = test_fscores[0][np.argmin(valid_losses)]\n    score2 = test_fscores[0][np.argmax(valid_fscores[0])]\n    scores = [score1, score2]\n    scores = [str(item) for item in scores]\n\n    print ('Test Scores: Weighted F1')\n    print('@Best Valid Loss: {}'.format(score1))\n    print('@Best Valid F1: {}'.format(score2))\n\n    rf = open('\/kaggle\/working\/textcnn_iemocap_results.txt', 'a')\n    rf.write('\\t'.join(scores) + '\\t' + str(args) + '\\n')\n    rf.close()","ffdd46dc":"## Train Process","5d25e325":"## Dataset Loader With Pytorch\n\n> Here, we use glove6b tokenizer, and process the raw text sequence into token index sequences.\n\n> baseTokenizer is the Base class which can be inherited by others tokenizer such as roberta etc.\n\n> gloveTokenizer is the Tokenizer we actually used.","94e63386":"> Unit Test for our dataloader to make sure dataloader process what we need.","7216b126":"## Construct Ours Model -- TextCNN Model."}}