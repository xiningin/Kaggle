{"cell_type":{"fc7946bb":"code","fee11e17":"code","de28d218":"code","bf14d3bb":"code","72f63aac":"code","dc894a0e":"code","305af10a":"code","c6622e66":"code","e093f74c":"code","af6b4c9d":"code","5c3e0bcb":"code","51a4cf47":"code","731e18df":"code","a202394f":"code","a26069df":"code","9f46556d":"code","1c3c90ad":"code","f50b0678":"code","62dae9ee":"code","59bdfeda":"code","bfd3a3b8":"code","1de37aa3":"code","576e2756":"code","cdcb6a59":"code","bf0df93d":"code","f9eb70f5":"code","db9b0f3c":"code","db98511b":"markdown","914a7052":"markdown","2ca52bc3":"markdown","497aa842":"markdown","65a0660b":"markdown","6eb77665":"markdown","0ca9ef25":"markdown","18ee67ee":"markdown","3c69324e":"markdown","d6855495":"markdown","a0a8d5ac":"markdown","16a5a820":"markdown","6a803083":"markdown","9bbb8192":"markdown","29b7f6d8":"markdown","cfc4e03c":"markdown","93d7e182":"markdown"},"source":{"fc7946bb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import GridSearchCV","fee11e17":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\nprint(train.shape)\nprint(test.shape)\n","de28d218":"print(train.head(4))\nprint(test.head(4))","bf14d3bb":"# found some NAN values,lets check \nprint(\"In the train Dataset the no. of missing\/NAN values in the columns are \\n\",train.isnull().sum())\nprint(\"\\n In the test Dataset the no. of missing\/NAN values in the columns are \\n\",test.isnull().sum())","72f63aac":"#lets check the missing value perecentage in the dataset\nprint(\"Total missing value the train and test detaset are:\",(train.isnull().sum().sum()+test.isnull().\n                                                             sum().sum()))\nprint(\"In the train dataset missing value percentage of the veriables are:\\n\",\n      train.isnull().sum()*100\/len(train))\n\nprint(\"In the test dataset missing value percentage of the veriables are:\\n\",\n      test.isnull().sum()*100\/len(test))","dc894a0e":"#Seperate the Survived veriabler then we will merge and drop the dataset\nSurvived = train[[\"Survived\"]]\ntrain[\"source\"] = \"train\"\ntest[\"source\"] = \"test\"\ndataset = pd.concat([train,test],axis=0)\nprint(dataset.shape)\nprint(dataset.head())\nprint(dataset.tail())\n","305af10a":"# drop the not required columns from the datasert\ndataset.drop([\"Cabin\",\"Ticket\",\"Survived\",\"Name\"],axis=1,inplace=True)\nprint(dataset.shape)\nprint(\"\\n datatypes:\\n\")\ndataset.info()","c6622e66":"dataset[\"Parch\"] = dataset[\"Parch\"].astype(\"object\")\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"object\")\ndataset[\"SibSp\"] = dataset[\"SibSp\"].astype(\"object\")\ndataset.info()","e093f74c":"#let's check how is dataset veriables are destributed and what we can do for Age missing values.\n# Basic descriptive analysis on Age Veriable to know the distribution\nprint(dataset[\"Age\"].describe())\nplt.rcParams[\"figure.figsize\"]=12,4\nsns.boxplot(\"Age\",data=dataset)\nplt.title(\"Age distribution Boxplot\")\n","af6b4c9d":"print(\"Missing value in Age:\",dataset[\"Age\"].fillna(30, inplace = True))\nprint(dataset[\"Age\"].isnull().sum())","5c3e0bcb":"# We have also seen some missing valus in Embarked and Fare veriables .\n#Because of the missing values are very less we can put mode for Embarked and mean for Fare.\nprint(\"\\n Embarked value count:\\n\" ,dataset[\"Embarked\"].value_counts())\nprint(\"Missing value in Embarked:\",dataset[\"Embarked\"].fillna(\"S\", inplace = True))\nprint(\"mean value of Fare is \",dataset[\"Fare\"].mean())\nprint(\"Missing value in Fare:\",dataset[\"Fare\"].fillna(dataset[\"Fare\"].mean(), inplace = True))\nprint(\"Total Missing Values in the dataset is:\",dataset.isnull().sum().sum())","51a4cf47":"train= dataset[dataset[\"source\"]==\"train\"]\ntest= dataset[dataset[\"source\"]==\"test\"]\ntrain.drop(\"source\",axis=1,inplace=True)\ntest.drop(\"source\",axis=1,inplace=True)\ntrain[\"Survived\"] = Survived[\"Survived\"]\nprint(train.shape)\nprint(test.shape)","731e18df":"#univariate data visualization for Numeric Data \"Age\",\"Fare\",\"Survived\":\nplt.rcParams[\"figure.figsize\"]=13,16\n\nplt.subplot(3,2,1)\nsns.countplot(train.Pclass)\nplt.title(\"Pclass Frequency Distribution\")\n\nplt.subplot(3,2,2)\nsns.countplot(train.Sex)\nplt.title(\"Sex Frequency Distribution\")\n\nplt.subplot(3,2,3)\nsns.distplot(train.Fare)\nplt.title(\"Fare Frequency Distribution\")\nplt.subplot(3,2,4)\nsns.boxplot(train.Fare)\nplt.title(\"Fare Boxplot Distribution\")\n\nplt.subplot(3,2,5)\nsns.countplot(train.Embarked)\nplt.title(\"Embarked Frequency Distribution\")\n\nplt.subplot(3,2,6)\nsns.countplot(train.Survived)\nplt.title(\"Survived Frequency Distribution\")","a202394f":"plt.rcParams[\"figure.figsize\"]=13,16\n\nplt.subplot(3,2,1)\nsns.countplot(train.Pclass,hue=train[\"Survived\"])\nplt.title(\"Pclass Frequency Distribution\")\n\nplt.subplot(3,2,2)\nsns.countplot(train.Sex,hue=train[\"Survived\"])\nplt.title(\"Pclass Frequency Distribution\")\n\nplt.subplot(3,2,3)\nsns.boxplot(x =\"Survived\",y=\"Fare\",data=train,hue=\"Sex\")\nplt.title(\"Fare Frequency Distribution\")\n\nplt.subplot(3,2,4)\nsns.countplot(train.Embarked,hue=train[\"Survived\"])\nplt.title(\"Embarked Frequency Distribution\")\n\nplt.subplot(3,2,5)\nsns.boxplot(x =\"Pclass\",y=\"Fare\",data=train,hue=\"Survived\")\nplt.title(\"Pcalss Vs Fare\")\nplt.subplot(3,2,6)\nsns.boxplot(x =\"Parch\",y=\"Fare\",data=train,hue=\"Survived\")\nplt.title(\"Pcalss Vs Fare\")\n","a26069df":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn import metrics\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","9f46556d":"X = train[[\"Age\",\"Fare\",\"Pclass\",\"SibSp\"]].values\nY = train[[\"Survived\"]].values\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.2,random_state=120)","1c3c90ad":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(xtrain,ytrain)\ny_pred = LR.predict(xtest)\nLR.score(xtrain, ytrain)","f50b0678":"# Support Vector Machines\nsvc = SVC()\nsvc.fit(xtrain, ytrain)\ny_pred = svc.predict(xtest)\nsvc.score(xtrain, ytrain)","62dae9ee":"#library\nfrom sklearn.model_selection import cross_val_score\nsvc = SVC()\nsvm_accuracy = cross_val_score(estimator=svc,X=xtrain,y=ytrain,cv=5)\nprint(\"SVM Accuracy:\",svm_accuracy)\nprint(\"Accuracy Mean:\",svm_accuracy.mean()*100)\nprint(\"Accuracy STD:\",svm_accuracy.std()*100)","59bdfeda":"'''svc = SVC()\n#from sklearn import svm, grid_search\nfrom sklearn.model_selection import GridSearchCV\ngrid_param = {'kernel':('linear', 'rbf','sigmoid'),\n              'C':(1,0.25,0.5,0.75),\n              'gamma': (0.001, 0.01, 0.1, 1),\n              'shrinking':(True,False)\n              }\ngd_sr = GridSearchCV(estimator=svc,  \n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=-1)\n\ngd_sr.fit(xtrain, ytrain) \nbest_parameters = gd_sr.best_params_  \nprint('Best Parameters',best_parameters) \nsvm_accuracy = gd_sr.best_score_  \nprint(\"Model Accuracy:\",svm_accuracy)'''","bfd3a3b8":"#------------------CV and Tunning with Random Forest---------------\nfrom sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(n_estimators=300, random_state=0)  \nRFC_Accuracy = cross_val_score(estimator=RFC, X=xtrain, y=ytrain, cv=5)\n\nprint(\"RFC Accuracy:\",RFC_Accuracy)\nprint(\"Accuracy Mean:\",RFC_Accuracy.mean()*100)\nprint(\"Accuracy STD:\",RFC_Accuracy.std()*100)","1de37aa3":"grid_param = {  \n    'n_estimators': [30,50,100],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False],\n    'max_features':[\"auto\",\"sqrt\"],\n    'max_depth': [2,3,5],\n    'max_leaf_nodes': [20,30,40]\n}\ngd_sr = GridSearchCV(estimator=RFC,  \n                     param_grid=grid_param,\n                     scoring='accuracy',\n                     cv=5,\n                     n_jobs=-1)\ngd_sr.fit(xtrain,ytrain)\nbest_parameters = gd_sr.best_params_  \nprint(best_parameters)\nbest_result = gd_sr.best_score_  \nprint(best_result)","576e2756":"#basic xgboost\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nXGB = XGBClassifier()\nXGB.fit(xtrain, ytrain)\ny_pred = XGB.predict(xtest)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(ytest, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","cdcb6a59":"'''#xgboost with Tuning\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold\n#basic xgboost\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier'''\n\n'''cv_params = {'max_depth': [3,5,7], 'min_child_weight': [1,3,5]}\nind_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n             'objective': 'binary:logistic'}\noptimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n                            cv_params, \n                             scoring = 'accuracy', cv = 5, n_jobs = -1) \noptimized_GBM.fit(xtrain, ytrain)\noptimized_GBM.cv_results_'''\n\n'''cv_params = {'learning_rate': [0.1, 0.01], 'subsample': [0.7,0.8,0.9]}\nind_params = {'n_estimators': 1000, 'seed':0, 'colsample_bytree': 0.8, \n             'objective': 'binary:logistic', 'max_depth': 3, 'min_child_weight': 1}\n\n\noptimized_GBM = GridSearchCV(xgb.XGBClassifier(**ind_params), \n                            cv_params, \n                             scoring = 'accuracy', cv = 5, n_jobs = -1)\noptimized_GBM.fit(xtrain, ytrain)\noptimized_GBM.cv_results_'''","bf0df93d":"# knn = KNeighborsClassifier(n_neighbors = 3)\n\n# knn.fit(X_train, Y_train)\n\n# Y_pred = knn.predict(X_test)\n\n# knn.score(X_train, Y_train)\n\n# Gaussian Naive Bayes\n\n# gaussian = GaussianNB()\n\n# gaussian.fit(X_train, Y_train)\n\n# Y_pred = gaussian.predict(X_test)\n\n# gaussian.score(X_train, Y_train)","f9eb70f5":"#------------------------------------------------------\nF_test = test[[\"Age\",\"Fare\",\"Pclass\",\"SibSp\"]].values\ny_pred = XGB.predict(F_test)\n#------------------------------------------------------\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","db9b0f3c":"y_pred","db98511b":"* We got 87% accuracy with LB 57% score. May be the model is overfitted and did not perform with the original test test. Lets apply 5 or 10 fold cross-velidation and some parameter tunnig to check whether the model performance imrpove or not.","914a7052":"Data is merged,Now see the Data Types,here some of are actually categorical but it is showing numeric and let's correct it!","2ca52bc3":"**Bvariate or Multivariate Data Vizualization**","497aa842":"It is clearly shown, in the given dataset around 20 percent of Age data is missing and 80 percent of Cabin data is missing.\n\nTo clean the dataset we can drop the Cabin column and also drop Ticket column(because Ticket is just a random unique number)\n\nBut before that,for easy data processing and visulization we can merge the train and test dataset and letter seperate into original train and test sets.","65a0660b":"Afetr tunning i got best fit parameters and model accuracy of 69.5 percentage,which is quite good.With this submission my Public Score is 66.5.Let do the CV and tunning with some other models and check the accuracy.","6eb77665":"**Feature Engg.**","0ca9ef25":"**One Hot Encoding**","18ee67ee":"**Thank you for visiting my kernel**\n\n**Please upvote if you find it useful !!!**","3c69324e":"**Data Visualization **","d6855495":"**Model Building**","a0a8d5ac":"**Let's Seperate dataset into original train and test sets**","16a5a820":"Yes, earlier the model was overfitted,Now we got the accuracy 63.9 with standard deviation of 1.9 percentage,which is very low.","6a803083":"As seen in the boxplot Age distribution is right skewed.Maximum data points are above the mean value and there are some outliers too.In the dataset 50 percent of age belongs to age group 20 to 40 age.so,to handle missing value we can use mean i.e 29.88 ~ 30.","9bbb8192":"**Cross Validation and Grid Search**","29b7f6d8":"model accuracy improve by 1% with randomforest.Lets try with xgBoost..","cfc4e03c":"Few Observations from train data set:\n\n1)There are more no. of passengers in the calss 3 as compare to class 1 and 2.\n\n2) No. of Male passengers are higher then the Female\n\n3)75 Percent of fare is in below 100$.\n\n4)Embarked Southampton has more no. of highest no. of passengers then Cherbourg and Queenstown.\n\n5)Around 341 passengers are live out of 891 passengers.","93d7e182":"**Normalization**"}}