{"cell_type":{"dc69659a":"code","3071df2c":"code","ef1ba5b3":"code","292434b3":"code","b8b9f02a":"code","836dd1d6":"code","e9c97912":"code","8e596c69":"code","51726bbf":"code","58eb76be":"markdown","10bd10ef":"markdown","baab7345":"markdown"},"source":{"dc69659a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3071df2c":"import glob, os\nimport re\nfrom dateutil.parser import parse\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport matplotlib.pyplot as plt","ef1ba5b3":"def readfiles(path): #Funcion para leer todos los archivos html que tengamos en el directorio\n    os.chdir(path)\n    pdfs = []\n    for file in glob.glob(\"*.html\"):\n        print(file)\n        pdfs.append(file)\n        \n    return pdfs\n\ndef dinero(x): #Funcion para cambiar el formato de los numeros EJ: 19,82 -> 19.82\n    x = x.replace(\",\",\"QWERTY\")\n    x = x.replace(\".\",\",\")\n    x = x.replace(\"QWERTY\",\".\")\n    \n    \n    return x\n\n\ndef is_date(string, fuzzy=False):#Funcion para comprobar si un string es una fecha o no\n    \"\"\"\n    Return whether the string can be interpreted as a date.\n\n    :param string: str, string to check for date\n    :param fuzzy: bool, ignore unknown tokens in string if True\n    \"\"\"\n    try: \n        parse(string, fuzzy=fuzzy)\n        return True\n\n    except ValueError:\n        return False","292434b3":"lista = readfiles(\"\/kaggle\/input\/contratos-menores-ayto-alcala-de-henares\")","b8b9f02a":"row = 0\ndf = pd.DataFrame()\n\nfor file in lista:\n    cont = 0\n\n    #print(file)\n    url = \"file:\/\/\/kaggle\/input\/contratos-menores-ayto-alcala-de-henares\/{}\".format(file)\n    \n    html = urlopen(url).read()\n    soup = BeautifulSoup(html, features=\"html.parser\") #Se lee el contenido del archivo html actual\n\n\n    for script in soup([\"script\", \"style\"]):\n        script.extract()    # Se eliminan los tags script y style para que nos quede unicamente el texto plano\n\n    # Se extrae el texto\n    text = soup.get_text()\n    lines = []\n    # Se separa y se crea una lista con cada linea\n    lines = [line.strip() for line in text.splitlines()]\n    \n    #Se itera cada linea y se a\u00f1aden los datos al dataframe\n    for i in lines: \n        if i == \"Aplicaci\u00f3n Presupuestaria\":\n            df.at[row,\"Empresa_Adjudicada\"] = lines[cont-1]\n\n\n        if  is_date(i) and i.count(\"\/\"):\n            df.at[row,\"Fecha\"] = lines[cont]\n            df.at[row,\"Concepto\"] = lines[cont+1]+lines[cont+3]\n            df.at[row,\"N_Contrato\"] = lines[cont-1]\n            df.at[row,\"Precio\"] = lines[cont+2]\n\n            row += 1\n\n        cont += 1\n    \n    #Debido al formato de los PDF algunas casillas con el nombre de la empresa quedaban como NA.\n    #Para solventar ese problema se utiliza este bucle\n    for i,rows in df.iterrows():\n    \n        try:\n            if rows.Empresa_Adjudicada.isupper() == False:\n                df.at[i,\"Empresa_Adjudicada\"] = df.at[i-1,\"Empresa_Adjudicada\"]\n        except:\n            df.at[i,\"Empresa_Adjudicada\"] = df.at[i-1,\"Empresa_Adjudicada\"]\n    \n            \ndf[\"Precio\"] = df[\"Precio\"].apply(dinero)\ndf = df[pd.to_numeric(df['Precio'], errors='coerce').notnull()] #Se eliminan contratos que se han cogido mal (unos 50)\n\ndf[\"Precio\"] = df[\"Precio\"].replace(\",\",\"\",regex=True) \ndf.Precio = df.Precio.astype(float)","836dd1d6":"df","e9c97912":"df = df.set_index(\"Fecha\",drop=True) #Se pone la fecha como indice y se transforma en datetime en la siguiente linea\n\ndf.index = pd.to_datetime(df.index)","8e596c69":"df = df.sort_values(\"Fecha\")\n\ndf = df.drop_duplicates() #Se ordenan por fecha y se eliminan los duplicados","51726bbf":"df[\"Precio_ac\"] = df.Precio.cumsum()#Se crea una columna adicional para obtener la suma acumulada y se cargan a un csv\n\n#df.to_csv(\"DB.csv\",encoding=\"UTF-8\") ","58eb76be":"Con este codigo extraemos los datos del documento html y los pasamos a un dataframe para posteriormente crear la base de datos en excel. \n\nComo podemos observar no somos capaces de extraerlos todos, pese a ello el numero de contratos no extraidos es residual (unos 50).\n\nComo no se trata de un estudio profesional descartaremos estos contratos, si se quisiera disponer de estos se podria o bien modificar el codigo o bien introducirlos manuelmente ya que no son demasiados.","10bd10ef":"Una vez preprocesados los datos he realizado un reporte con Power BI para su mejor analisis. Para ello se ha utilizado Power Query para transformar algunos datos y crear nuevas metricas. Este analisis sera adjuntado en el dataset igualmente.\n\nEste se divide en 3 pesta\u00f1as: Vista General, Empresas y Conceptos. Descargando el archivo podemos interactuar con el reporte y discriminar los datos que mas nos interesen.\n","baab7345":"Una vez descargados los archivos pdf de la web hay que pasarlos a HTML. Para ello yo he utlizado la siguiente web https:\/\/www.pdftohtml.net\/. Una vez echo esto estamos listos para trabajar con los datos y pasarlos a una BBDD\n"}}