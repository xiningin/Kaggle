{"cell_type":{"ffb36250":"code","adef195e":"code","e1974c51":"code","83bb5e87":"code","462bbc22":"code","68b1cdb1":"code","7788f7be":"code","e2b789c7":"code","67d0d033":"code","f746f52e":"code","12deecf8":"code","d2211bb1":"code","5c8694ae":"code","5727bccd":"code","a003530d":"code","785eea15":"code","285c06ad":"code","95af44e8":"code","aeb64826":"code","1b9ad79c":"code","a16a09b9":"code","dbdf7e4c":"code","030c9514":"code","af4d3719":"code","59a0f84a":"code","5595ccd8":"code","eaf9aee2":"code","2604e08c":"code","4f1e66ef":"code","5015f113":"code","74eeb467":"code","f1f31d04":"code","29be80a5":"code","9332504a":"code","372e304b":"code","73228fe7":"code","6f59925f":"code","ab5c87c2":"code","e4150d24":"code","07235711":"code","12aa3173":"code","acb168fb":"code","94394ca9":"code","0c33901b":"code","61b22f26":"code","55ea9510":"markdown","f9a6bcde":"markdown","581e400f":"markdown","c11b5370":"markdown","3bb36ca0":"markdown","f99de8ab":"markdown","0465e8aa":"markdown","38e1f64f":"markdown","bb9f676b":"markdown","72ffd209":"markdown","c97f71d6":"markdown","d5d4ef84":"markdown","213b19ff":"markdown","eecdc521":"markdown","a452aed6":"markdown","41bfaf35":"markdown","e420ee5b":"markdown","d4793fa9":"markdown","71954df8":"markdown","90e6465e":"markdown","3460c6e2":"markdown","fa1eea7d":"markdown"},"source":{"ffb36250":"!pip install arch","adef195e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom datetime import timedelta\nfrom sklearn.cluster import KMeans \nimport matplotlib.pyplot as plt \n\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.stattools import adfuller\nfrom arch import arch_model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Bidirectional,TimeDistributed, RepeatVector, Input\n\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1974c51":"bar_S = pd.read_csv('\/kaggle\/input\/stock-market-small-wide-dataset\/bar-S.csv')\nquote = pd.read_csv('\/kaggle\/input\/stock-market-small-wide-dataset\/quote-S.csv')","83bb5e87":"bar_S.head()","462bbc22":"quote.head()","68b1cdb1":"bar_S = bar_S.sort_values(by='time')\nbar_S['date'] = pd.to_datetime(bar_S['time']).apply(lambda x: x.strftime('%Y-%m-%d'))","7788f7be":"stock_list = list(set(bar_S['symbol']))\ndf_list = []\ndf_list_2 = []\nwindow = 30\nn_minutes = 10\ndate_list = list(set(bar_S['date']))\nfeatures = ['open_price','high_price','low_price','close_price',\n            'average_price','VWAP','volume','accumulated_volume','symbol','date','time']\nbar_S_window_time = 0\nfor i in stock_list:\n    new_bar_S = bar_S[bar_S['symbol']==i]\n    for j in date_list: \n        try:\n            lower_limit = list(new_bar_S[new_bar_S['date']==j].sort_values(by='time')['time'])[0]\n            lower_limit = pd.to_datetime(lower_limit)\n            upper_limit = lower_limit + timedelta(minutes=window*n_minutes)\n            df = new_bar_S[(new_bar_S['date']==j) & (new_bar_S['time']>=str(lower_limit)) & (new_bar_S['time']<str(upper_limit))]\n            sorted_df = df.sort_values(by='time')[features]\n            df_list_2.append(sorted_df)\n            sorted_df = sorted_df.groupby('date').mean()\n            sorted_df['symbol'] = i\n            df_list.append(sorted_df)\n        except Exception as e:\n            pass\nbar_S_windowed = pd.concat(df_list)\nbar_S_windowed = bar_S_windowed.sort_index()\nbar_S_window_time = pd.concat(df_list_2)\nbar_S_col_list = list(bar_S_windowed)\nbar_S_windowed.columns = ['mean '+col if col!='symbol' else 'symbol' for col in bar_S_col_list]\nbar_S_windowed.head()","e2b789c7":"price_features = ['mean open_price','mean high_price','mean low_price','mean close_price',\n                  'mean average_price','mean VWAP']\ncount = 0\ncm = plt.get_cmap('rainbow')\ncolors = cm(np.linspace(0, 1, 24))\nplt.rcParams['figure.figsize'] = (10,10)\nfor index in date_list: \n    bar_S_price_data = bar_S_windowed.loc[index,price_features]\n    cluster_list = range(1,21)\n    wss_list = []\n    for i in cluster_list:\n        km = KMeans(n_clusters=i)\n        km.fit(bar_S_price_data)\n        wss_list.append(km.inertia_)\n    plt.plot(cluster_list,wss_list,marker='o',color=colors[count])\n    plt.xlabel('No. of clusters')\n    plt.ylabel('Within sum of squares (WSS)')\n    plt.title('Elbow Curve')\n    plt.legend(date_list)\n    count+=1\n    for i, txt in enumerate(cluster_list):\n        plt.annotate(txt, (cluster_list[i], wss_list[i])) ","67d0d033":"km = KMeans(n_clusters=4)\nbar_S_price_data =  bar_S_windowed.loc[:,price_features]\nkm.fit(bar_S_price_data)\nlabel = km.predict(bar_S_price_data)","f746f52e":"stock_cluster = dict(zip(bar_S_windowed.loc[:,'symbol'],label))\ncluster_0_N1=[]\ncluster_1_N1=[]\ncluster_2_N1= []\ncluster_3_N1=[]\nfor key in stock_cluster:\n    if stock_cluster[key]==0:\n        cluster_0_N1.append(key)    \n    elif stock_cluster[key]==1:\n        cluster_1_N1.append(key)\n    elif stock_cluster[key]==2:\n        cluster_2_N1.append(key)\n    else:\n        cluster_3_N1.append(key)\nprint('Stocks belonging to cluster 0:',','.join(set(cluster_0_N1)))\nprint('\\nStocks belonging to cluster 1:',','.join(set(cluster_1_N1)))\nprint('\\nStocks belonging to cluster 2:',','.join(set(cluster_2_N1)))\nprint('\\nStocks belonging to cluster 3:',','.join(set(cluster_3_N1)))","12deecf8":"def plot_price_curve(cluster,n_stocks):\n    plt.rcParams['figure.figsize']=(50,20)\n    count = 1\n    for stock in cluster[:n_stocks]:\n        df = bar_S_windowed[bar_S_windowed['symbol']==stock]\n        features = ['mean open_price','mean high_price','mean low_price','mean close_price',\n                    'mean average_price','mean VWAP','mean volume','mean accumulated_volume']\n        for feature in features:\n            plt.title('For stock '+stock)\n            plt.subplot(n_stocks,8,count)\n            sns.lineplot(data = df,x = df.index,y = feature)\n            count+=1  ","d2211bb1":"print('For cluster 0 of N1')\nplot_price_curve(cluster_0_N1,4)","5c8694ae":"print('For cluster 1 of N1')\nplot_price_curve(cluster_1_N1,4)","5727bccd":"print('For cluster 2 of N1')\nplot_price_curve(cluster_2_N1,4)","a003530d":"print('For cluster 3 of N1')\nplot_price_curve(cluster_3_N1,4)","785eea15":"def stationary_test(timeseries):\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    return dftest[2]\n\ndef ARIMA_prediction_plot(timeseries,split):\n    train_ts = timeseries[:split]\n    test_ts = timeseries[split:]\n    p_val = stationary_test(timeseries)\n    model = ARIMA(train_ts, order=(p_val,1,2))  \n    results_ARIMA = model.fit(disp=-1)\n    #Future Forecasting\n    history = list(train_ts)\n    predictions = []\n    test_data = list(test_ts)\n    for i in range(len(test_data)):\n        model = ARIMA(history, order=(p_val,1,2))\n        model_fit = model.fit(disp=-1)\n        output = model_fit.forecast()\n        yhat = output[0]\n        predictions.append(float(yhat))\n        history.append(float(yhat))\n    plt.rcParams['figure.figsize'] = (20,10)\n    plt.plot(timeseries)\n    plt.plot(test_ts.index,predictions,color='green')\n    plt.axvline(train_ts.index[-1],color='orange',dashes=(5,2,1,2))\n    plt.xlabel('Average price')\n    plt.ylabel('time')\n    plt.legend(['actual values','predicted values for test data'])","285c06ad":"bar_S_window_time.index = bar_S_window_time['time']\nbar_S_window_time = bar_S_window_time.sort_index()","95af44e8":"stock = cluster_0_N1[0]\nprint('For N1 cluster 0 stock: ',stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nARIMA_prediction_plot(timeseries,int(0.90*len(timeseries)))","aeb64826":"stock = cluster_1_N1[0]\nprint('For N1 cluster 1 stock: ',stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nARIMA_prediction_plot(timeseries,int(0.90*len(timeseries)))","1b9ad79c":"stock = cluster_2_N1[0]\nprint('For N1 cluster 2 stock: ',stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nARIMA_prediction_plot(timeseries,int(0.90*len(timeseries)))","a16a09b9":"stock = cluster_3_N1[0]\nprint('For N1 cluster 3 stock: ',stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nARIMA_prediction_plot(timeseries,int(0.90*len(timeseries)))","dbdf7e4c":"cluster_list = [cluster_0_N1, cluster_1_N1, cluster_2_N1, cluster_3_N1]\nplt.rcParams['figure.figsize']=(20,5)\nfor index,cluster in enumerate(cluster_list):\n    text = 0\n    stock = cluster[0]\n    if cluster==cluster_0_N1:\n        text='For N1 cluster 0 stock: '+stock\n    elif cluster==cluster_1_N1:\n        text='For N1 cluster 1 stock: '+stock\n    elif cluster==cluster_2_N1:\n        text='For N1 cluster 2 stock: '+stock\n    else:\n        text='For N1 cluster 3 stock: '+stock\n    timeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\n    model = arch_model(timeseries, mean='ARX', vol='GARCH', p=9)\n    model_fit = model.fit(disp=-1)\n    y = model_fit.forecast(horizon=30)\n    plt.subplot(1,4,index+1)\n    plt.title(text)\n    plt.ylabel('variance')\n    plt.xlabel('epochs')\n    plt.plot(y.variance.values[-1],color='red')","030c9514":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = [],[]\n    for i in range(len(sequence)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        if out_end_ix > len(sequence):\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","af4d3719":"def R2_score(y_true, y_pred):\n    from tensorflow.keras import backend as K\n    SS_res = K.sum(K.square(y_true-y_pred))\n    SS_tot = K.sum(K.square(y_true-K.mean(y_true)))\n    return (1-SS_res\/(SS_tot+K.epsilon()))","59a0f84a":"def LSTM_prediction_plot(timeseries):\n    train_ts = timeseries[:int(0.90*len(timeseries))]\n    test_ts = timeseries[int(0.90*len(timeseries)):]\n    X_train, Y_train = split_sequence(train_ts,3,1)\n    Y_train = np.squeeze(Y_train)\n    X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n    # defining LSTM model\n    model = Sequential()\n    model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(3, 1)))\n    model.add(LSTM(100, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse', metrics=[R2_score])\n    model.summary()\n    training = model.fit(X_train,Y_train,epochs=10,verbose=0)\n    history = list(train_ts)\n    predictions = []\n    for i in range(len(test_ts)):\n        pred = np.squeeze(model.predict(np.array(history[-3:]).reshape(1,3,1)))\n        history.append(float(pred))\n        predictions.append(float(pred))\n    plt.plot(timeseries)\n    plt.plot(test_ts.index,predictions,color='green')\n    plt.axvline(train_ts.index[-1],color='orange',dashes=(5,2,1,2))\n    plt.xlabel('time')\n    plt.ylabel('average price')\n    plt.rcParams['figure.figsize'] = (25,10)","5595ccd8":"stock = cluster_0_N1[0]\nprint('For N1 cluster 0 stock: '+stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nLSTM_prediction_plot(timeseries)","eaf9aee2":"stock = cluster_1_N1[0]\nprint('For N1 cluster 1 stock: '+stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nLSTM_prediction_plot(timeseries)","2604e08c":"stock = cluster_2_N1[0]\nprint('For N1 cluster 2 stock: '+stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nLSTM_prediction_plot(timeseries)","4f1e66ef":"stock = cluster_3_N1[0]\nprint('For N1 cluster 3 stock: '+stock)\ntimeseries = bar_S_window_time[bar_S_window_time['symbol']==stock]['average_price']\nLSTM_prediction_plot(timeseries)","5015f113":"quote = quote.sort_values(by='time')\nquote['date'] = pd.to_datetime(quote['time']).apply(lambda x: x.strftime('%Y-%m-%d'))","74eeb467":"stock_list = list(set(quote['ticker']))\ndf_list = []\nfor i in stock_list:\n    df = quote[quote['ticker']==i]\n    df['bid_price_change'] = df['bid_price'].diff().values\n    df['bid_price_returns'] = df['bid_price_change']\/df['bid_price']\n    df['ask_price_returns'] = df['ask_price'].diff().values\/df['ask_price'].values\n    average_bid_price = df['bid_price'].median()\n    df['bid_price_volatility'] = (((df['bid_price'] - average_bid_price)**2)\/len(df))**0.5 \n    df_list.append(df)\nquote = pd.concat(df_list)","f1f31d04":"df_list = []\nwindow = 30\nn_minutes = 10\ndate_list = list(set(quote['date']))\nfor i in stock_list:\n    new_quote_S = quote[quote['ticker']==i]\n    for j in date_list: \n        try:\n            lower_limit = list(new_quote_S[new_quote_S['date']==j].sort_values(by='time')['time'])[0]\n            lower_limit = pd.to_datetime(lower_limit)\n            upper_limit = lower_limit + timedelta(minutes=window*n_minutes)\n            df = new_quote_S[(new_quote_S['date']==j) & (new_quote_S['time']>=str(lower_limit)) & \n                             (new_quote_S['time']<str(upper_limit))]\n            #Filling null values in records with 0 as default\n            df['bid_price_change'] = df['bid_price_change'].fillna(0)\n            df['bid_price_returns'] = df['bid_price_returns'].fillna(0)\n            df['ask_price_returns'] = df['ask_price_returns'].fillna(0)\n            df = df.sort_values(by='time')\n            df['cumulative bid_price'] = np.cumsum(df['bid_price'])\n            df_list.append(df)\n        except Exception as e:\n            pass\nquote_S_windowed = pd.concat(df_list)\nquote_S_windowed = quote_S_windowed.sort_index()\nquote_S_windowed.index = quote_S_windowed['date']\nquote_S_windowed.drop('date',axis=1,inplace=True)\nquote_S_windowed.head()","29be80a5":"bar_quote_data = quote_S_windowed.merge(bar_S,left_on=['ticker','time'],right_on=['symbol','time'],how='inner')","9332504a":"plt.title('cumulative bid price vs volume bubble chart based on ticker-wise bid size')\nsns.scatterplot(data=bar_quote_data,x='volume',y='cumulative bid_price',hue='ticker',size='bid_size')","372e304b":"def plot_elbow(features):\n    cm = plt.get_cmap('rainbow')\n    date_list = list(set(quote_S_windowed.index))\n    colors = cm(np.linspace(0, 1, len(date_list)))\n    count=0\n    plt.rcParams['figure.figsize'] = (10,10)\n    for index in date_list: \n        quote_S_data = quote_S_windowed.loc[index,features]\n        cluster_list = range(1,21)\n        wss_list = []\n        for i in cluster_list:\n            km = KMeans(n_clusters=i)\n            km.fit(quote_S_data)\n            wss_list.append(km.inertia_)\n        plt.plot(cluster_list,wss_list,marker='o',color=colors[count])\n        plt.xlabel('No. of clusters')\n        plt.ylabel('Within sum of squares (WSS)')\n        plt.title('Elbow Curve')\n        plt.legend(date_list)\n        count+=1\n        for i, txt in enumerate(cluster_list):\n            plt.annotate(txt, (cluster_list[i], wss_list[i]))","73228fe7":"returns_features = ['bid_price_returns','ask_price_returns']\nplot_elbow(returns_features)","6f59925f":"km = KMeans(n_clusters=2)\nquote_S_returns_data = quote_S_windowed.loc[:,returns_features]\nkm.fit(quote_S_returns_data)\nlabel = km.predict(quote_S_returns_data)","ab5c87c2":"stock_cluster = dict(zip(quote_S_windowed.loc[:,'ticker'],label))\ncluster_0_N2=[]\ncluster_1_N2=[]\nfor key in stock_cluster:\n    if stock_cluster[key]==0:\n        cluster_0_N2.append(key)    \n    else:\n        cluster_1_N2.append(key)\nprint('Stocks belonging to cluster 0:',','.join(set(cluster_0_N2)))\nprint('\\nStocks belonging to cluster 1:',','.join(set(cluster_1_N2)))","e4150d24":"bid_size_feature = ['bid_size']\nplot_elbow(bid_size_feature)","07235711":"km = KMeans(n_clusters=3)\nquote_S_size_data = quote_S_windowed.loc[:,bid_size_feature]\nkm.fit(quote_S_size_data)\nlabel = km.predict(quote_S_size_data)","12aa3173":"stock_cluster = dict(zip(quote_S_windowed.loc[:,'ticker'],label))\ncluster_0_N3=[]\ncluster_1_N3=[]\ncluster_2_N3=[]\nfor key in stock_cluster:\n    if stock_cluster[key]==0:\n        cluster_0_N3.append(key)    \n    elif stock_cluster[key]==1:\n        cluster_1_N3.append(key)\n    else:\n        cluster_2_N3.append(key)\ncluster_0_N3 = list(set(cluster_0_N3))\ncluster_1_N3 = list(set(cluster_1_N3))\ncluster_2_N3 = list(set(cluster_2_N3))\nprint('Stocks belonging to cluster 0:',','.join(cluster_0_N3))\nprint('\\nStocks belonging to cluster 1:',','.join(cluster_1_N3))\nprint('\\nStocks belonging to cluster 2:',','.join(cluster_2_N3))","acb168fb":"def plot_bid_curve(cluster,n_stocks):\n    count = 1\n    cluster = list(set(cluster))\n    plt.rcParams['figure.figsize'] = (20,20)\n    for stock in cluster[:n_stocks]: \n        df = quote_S_windowed[['time','bid_price_change','ticker','bid_size','bid_price_volatility','cumulative bid_price']]\n        df = df[df['ticker']==stock].sort_values(by='time')\n        plt.subplot(n_stocks,4,count)\n        plt.plot(list(df['bid_price_change']))\n        plt.xlabel('time')\n        plt.ylabel('bid price change')\n        plt.title('For stock '+stock)\n        count+=1\n        plt.subplot(n_stocks,4,count)\n        plt.plot(list(df['bid_size']))\n        plt.xlabel('time')\n        plt.ylabel('bid size')\n        plt.title('For stock '+stock)\n        count+=1\n        plt.subplot(n_stocks,4,count)\n        plt.plot(list(df['bid_price_volatility']))\n        plt.xlabel('time')\n        plt.ylabel('bid price volatility')\n        plt.title('For stock '+stock)\n        count+=1\n        plt.subplot(n_stocks,4,count)\n        plt.plot(list(df['cumulative bid_price']))\n        plt.xlabel('time')\n        plt.ylabel('cumulative bid price')\n        plt.title('For stock '+stock)\n        count+=1","94394ca9":"print('Cluster 0 bid plot:')\nplot_bid_curve(cluster_0_N3,4)","0c33901b":"print('Cluster 1 of N3 bid plot:')\nplot_bid_curve(cluster_1_N3,4)","61b22f26":"print('Cluster 2 of N3 bid plot:')\nplot_bid_curve(cluster_2_N3,4)","55ea9510":"### Optimize N1 using GARCH","f9a6bcde":"### Cumulative bid price vs Volume","581e400f":"### Clustering based on returns\nHere, we are creating N2 cluster based on returns which are of two types: bidprice returns and ask returns, taken into consideration while grouping. We are also observing variation of cumulative bid price with respect to the stock volume.","c11b5370":"So, the elbow point of optimum no.of clusters appears to be 3 for majority of the dates. So, N3 value, which is based on bid size is 3.","3bb36ca0":"### Windowing Method\nAs mentioned in the task, it was mentioned to perform analysis for 30 windows of 10 mins of each day leading to 5 hrs of stock movement capturing in our analysis. ","f99de8ab":"### Optimize N1 using LSTM","0465e8aa":"As stated reasons above, we are running the LSTM model for one stock of each cluster N1. ","38e1f64f":"### Viewing bar-S and quote-S dataset","bb9f676b":"### Optimize N1 by ARIMA Model accuracy ","72ffd209":"Since running ARIMA model is computationally expensive and also displaying predictions on graph for each stock of cluster N1 is cumbersome, so running for only one stock from each clusters of N1.","c97f71d6":"### Listing out stocks based on N3","d5d4ef84":"We are checking change in variance for one stock from each of the cluster of N1. The operation can be repeated for any number of stocks in each cluster. Although, it will become cumbersome to display variance of all the stocks in each cluster N1 in this notebook. That's why it's been avoided.","213b19ff":"### Installing Libraries","eecdc521":"So, the elbow point of optimum no.of clusters appears to be 2 for majority of the dates. So, N2 value, which is based on bid price return and ask price returns is 2.","a452aed6":"Elbow method states that the optimum no. of cluster required to minimize sum of intra-cluster distances of clusters is located at the Elbow of the inertia vs no. of clusters curve also known as Elbow curve. For, majority of dates, the elbow point appears to be 4 as no. of clusters. So, no.of clusters N1 is 4.  ","41bfaf35":"### Displaying Movement of Average day prices of 4 stocks in each cluster of N1","e420ee5b":"### Listing out stocks in each cluster of N1","d4793fa9":"### Listing out stocks based on clusters in N2\n","71954df8":"### Importing Libraries","90e6465e":"Merging quote-S and bar-S data for our analysis","3460c6e2":"### Clustering based on bid size\nHere, we are clustering stocks based on bid size and quote data. We are also calculating bid price volatility and bid price change and exploring its trends over time in this cluster.","fa1eea7d":"### Clustering based on price paramters\nThe basis for clustering is using open, close, high, low, average and VWAP prices to group the stocks into different clusters. We are calculating the mean of these prices in a 5-hour window of the day and then performing the clustering operation."}}