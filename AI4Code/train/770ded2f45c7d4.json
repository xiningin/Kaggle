{"cell_type":{"b1b95f33":"code","3d10ee11":"code","fe010752":"code","140c97a9":"code","67ddbfb0":"code","f8d53d06":"code","cc58f940":"code","cb3b37cd":"code","95701678":"code","cea05e24":"code","e73f1cc9":"code","d4fbffe3":"code","c01dda6e":"code","1dba2a56":"code","1dc5e798":"code","fca38855":"code","7b8fd1a5":"code","57815da2":"code","3fecc949":"code","b94097b6":"code","2aa17b96":"code","d7343b06":"code","876d6c79":"code","9b0cc05e":"code","d86be6a1":"code","379b3b41":"markdown","ba657ea4":"markdown","fa9a9833":"markdown","63a1dc85":"markdown","ab9857c3":"markdown","9e4450f1":"markdown","ab82ca21":"markdown","794a1f0d":"markdown"},"source":{"b1b95f33":"import tensorflow as tf\nimport numpy as np\nimport imageio\nimport glob\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow.keras as keras\nimport os \nimport PIL\nimport time\nimport pandas as pd\n\nfrom keras.layers import Dense, BatchNormalization, LeakyReLU, Reshape, Conv2DTranspose, Conv2D, Dropout, Flatten\nfrom keras import Sequential\nfrom IPython import display\n\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d10ee11":"(train_images, train_labels), (_, _) = tf.keras.datasets.fashion_mnist.load_data()","fe010752":"train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\ntrain_images = (train_images - 127.5)\/ 127.5\n\nBUFFER_SIZE = 60000\nBATCH_SIZE = 256\n\nX_train = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","140c97a9":"def build_generator():\n  model = Sequential()\n  model.add(Dense(7*7*256, use_bias = False, input_shape=(100,)))\n  model.add(BatchNormalization())\n  model.add(LeakyReLU())\n\n  model.add(Reshape((7, 7, 256)))\n  assert model.output_shape == (None, 7, 7, 256)\n\n  model.add(Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False))\n  assert model.output_shape == (None, 7, 7, 128)\n  model.add(BatchNormalization())\n  model.add(LeakyReLU())\n\n  model.add(Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n  assert model.output_shape == (None, 14, 14, 64)\n  model.add(BatchNormalization())\n  model.add(LeakyReLU())\n\n  model.add(Conv2DTranspose(1, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh' ))\n  assert model.output_shape ==(None, 28, 28, 1)\n\n  return model","67ddbfb0":"gen = build_generator()\n\nnoise = tf.random.normal([1,100])\ngenerated_image = gen(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')","f8d53d06":"def build_discriminator():\n  model = Sequential()\n  model.add(Conv2D(64, (5, 5), strides=(2,2), padding='same', input_shape=[28,28,1]))\n\n  model.add(LeakyReLU())\n  model.add(Dropout(0.3))\n\n  model.add(Conv2D(128, (5,5), strides=(2,2), padding='same'))\n  model.add(LeakyReLU())\n  model.add(Dropout(0.3))\n\n  model.add(Flatten())\n  model.add(Dense(1))\n\n  return model","cc58f940":"disc = build_discriminator()\ndecision = disc(generated_image)\n\nprint(decision)","cb3b37cd":"bce = keras.losses.BinaryCrossentropy(from_logits=True)","95701678":"def discriminator_loss(real_output, fake_output):\n  real_loss = bce(tf.ones_like(real_output), real_output)\n  fake_loss = bce(tf.zeros_like(fake_output), fake_output)\n  total_loss = real_loss + fake_loss\n\n  return total_loss","cea05e24":"def generator_loss(fake_output):\n  return bce(tf.ones_like(fake_output), fake_output)","e73f1cc9":"gen_optimizer = keras.optimizers.Adam(1e-4)\ndis_optimizer = keras.optimizers.Adam(1e-4)","d4fbffe3":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer = gen_optimizer,\n                                 discriminator_optimizer=dis_optimizer,\n                                 generator=gen,\n                                 discriminator=disc)","c01dda6e":"epochs = 250\n\nnoise_dim = 100\n\nto_gen = 16\n\nseed = tf.random.normal([to_gen, noise_dim])","1dba2a56":"@tf.function\ndef train_step(images):\n  noise = tf.random.normal([BATCH_SIZE, noise_dim])\n\n  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n    generated_images = gen(noise, training=True)\n\n    real_output = disc(images, training=True)\n    fake_output = disc(generated_images, training=True)\n\n    gen_loss = generator_loss(fake_output)\n    dis_loss = discriminator_loss(real_output, fake_output)\n\n    gradients_of_generator = gen_tape.gradient(gen_loss, gen.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(dis_loss, disc.trainable_variables)\n\n    gen_optimizer.apply_gradients(zip(gradients_of_generator, gen.trainable_variables))\n    dis_optimizer.apply_gradients(zip(gradients_of_discriminator, disc.trainable_variables))","1dc5e798":"def train(dataset, epochs):\n  for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch in dataset:\n      train_step(image_batch)\n\n    display.clear_output(wait=True)\n    generate_and_save_images(gen, epoch +1, seed)\n\n    if(epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n  display.clear_output(wait=True)\n  generate_and_save_images(gen, epochs, seed)","fca38855":"def generate_and_save_images(model, epoch, test_input):\n  \n  predictions = model(test_input, training=False)\n\n  fig = plt.figure(figsize=(4,4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","7b8fd1a5":"train(X_train, epochs)","57815da2":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","3fecc949":"def display_image(epoch_no):\n  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))","b94097b6":"display_image(epochs)","2aa17b96":"anim_file = 'dcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  last = -1\n  for i,filename in enumerate(filenames):\n    frame = 2*(i**0.5)\n    if round(frame) > round(last):\n      last = frame\n    else:\n      continue\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n\nimport IPython\nif IPython.version_info > (6,2,0,''):\n  display.Image(filename=anim_file)","d7343b06":"from IPython.display import Image \nImage(open('dcgan.gif','rb').read())","876d6c79":"from PIL import Image\n\nframes = []\nimgs = glob.glob(\"*.png\")\nimgs.sort()\nfor i in imgs:\n    new_frame = Image.open(i)\n    frames.append(new_frame)","9b0cc05e":"frames[0].save('training.gif', format='GIF',\n               append_images=frames[1:],\n               save_all=True,\n               duration=100, loop=0)","d86be6a1":"from IPython.display import Image \nImage(open('training.gif','rb').read())","379b3b41":"# Setting up the Optimizer","ba657ea4":"**Importing the necessary Libraries and Packages**","fa9a9833":"**Setting the Hyper-parameters**","63a1dc85":"# Defining the training function","ab9857c3":"Processing the data to fit the model","9e4450f1":"# Defining the Losses","ab82ca21":"# Building the Model","794a1f0d":"# Introduction"}}