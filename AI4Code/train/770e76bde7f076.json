{"cell_type":{"58fa168a":"code","cb75467e":"code","bf4ca535":"code","d169aa7b":"code","6da3af7d":"code","bff6a534":"code","657912e0":"code","7e90e0f3":"code","1a9f4846":"code","a67991da":"code","883bf151":"code","d7e74cf1":"code","8e288689":"code","04db4249":"code","c20ceb26":"code","22149e1b":"code","7391e8fc":"code","eab2777c":"code","d019e780":"code","6d2a4297":"markdown","55ff5d81":"markdown","a740ad55":"markdown","6175f9f7":"markdown","d8b0e2b3":"markdown","476c369a":"markdown","6ebfc395":"markdown","eb5d32b9":"markdown","e743c516":"markdown","d3f56722":"markdown","d834b0ae":"markdown","f7824a19":"markdown","55d5252f":"markdown","3c9ce0df":"markdown","ed86461a":"markdown","4188e7b1":"markdown"},"source":{"58fa168a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom textwrap import wrap\nimport networkx as nx\n\nfrom itertools import combinations\nfrom igraph import *\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb75467e":"# Read the data and filter the roles:\n\ndata_df = pd.read_csv(\"\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv\")\ndata_df = data_df.drop_duplicates()\n\n_data_df = data_df[(data_df.Q5 == \"Data Scientist\") | \n                   (data_df.Q5 == \"Data Analyst\") | \n                   (data_df.Q5 == \"Machine Learning Engineer\") | \n                   (data_df.Q5 == \"Data Engineer\") | \n                   (data_df.Q5 == \"DBA\/Database Engineer\")|\n                   (data_df.Q5 == \"Statistician\")].reset_index(drop=True).iloc[:,1:]","bf4ca535":"_data_df[[\"Q1\", \"Q5\"]].groupby([\"Q5\"]).count().plot.barh(color=\"lightblue\", figsize = (10, 6))\nplt.show()","d169aa7b":"selected_cols = [col.startswith(\"Q5\") | col.startswith(\"Q23\") for col in _data_df.columns]\n\n# Select inly some observations (for visualization purposes)\nppl_df = _data_df.iloc[:1000,selected_cols]\n\n# Generate vertices combinations (edges)\nprint(\"Generating edges (node combinations)...\")\ndf_nodes = pd.DataFrame(data = list(combinations(ppl_df.index.to_list(),2)),\n                       columns = [\"Source\", \"Target\"])\n\n\n# Function to calculate the edge weights between every two nodes (vertices)\ndef calc_weight(df, x):\n    numerator = sum([ a == b for a, b in zip(df.loc[x.Source].values[1:], df.loc[x.Target].values[1:])])\n    denominator = sum([pd.notna(a) | (pd.notna(b)) for a, b in zip(df.loc[x.Source].values[1:], df.loc[x.Target].values[1:])])\n\n    try:\n        weight = numerator\/denominator\n    except ZeroDivisionError:\n        weight = 0.0\n        \n    return weight\n\n# Calculate the edge weights\nweights = df_nodes.apply(lambda x: calc_weight(ppl_df, x), axis=1)\ndf_nodes[\"weights\"] = round(weights, 1)*100\n\n# igraph allows loading a graph from tuples\ntuples = [(int(row[0]), int(row[1])) for row in df_nodes.values]\n\n# Map every role to a color to be represented in the graph\njob_cols = {\n    'Data Analyst':\"pink\",\n    'Data Engineer':\"lightgreen\",\n    'Data Scientist':\"lightblue\",\n    'Machine Learning Engineer':\"yellow\",\n    'DBA\/Database Engineer': \"purple\",\n    'Statistician':\"gray\"}\n\njob_col_lst = list(map(job_cols.get, list(ppl_df.Q5)))\n\n# Generate the graph network from tuples\ng = Graph(tuples)","6da3af7d":"# Add the edge and node properties: edge weights, node colors and job corresponding to each node\ng.es[\"weight\"]=(df_nodes[\"weights\"])\n\n# Delete the edges of 0 weight\ng.es.select(weight = 0).delete()\ng.es.select(weight = 5).delete()\ng.es.select(weight = 10).delete()\ng.es.select(weight = 15).delete()\ng.es.select(weight = 20).delete()\ng.es.select(weight = 25).delete()\ng.es.select(weight = 30).delete()\ng.es.select(weight = 35).delete()\ng.es.select(weight = 40).delete()\ng.es.select(weight = 45).delete()\n# g.es.select(weight = 50).delete()\n# g.es.select(weight = 55).delete()\n# g.es.select(weight = 60).delete()\n# g.es.select(weight = 65).delete()\n# g.es.select(weight = 70).delete()\n# g.es.select(weight = 75).delete()\n# g.es.select(weight = 80).delete()\n# g.es.select(weight = 85).delete()\n# g.es.select(weight = 90).delete()\n# g.es.select(weight = 95).delete()\n# g.es.select(weight = 100).delete()\n\n\n# Delete the unconnected nodes\ng.vs.select(_degree=0).delete()\n\ng.vs[\"job\"] = ppl_df.Q5.values\ng.vs[\"job_cols\"] = job_col_lst","bff6a534":"layout = g.layout_fruchterman_reingold()\n#layout = g.layout_lgl()\n#layout = g.layout_kamada_kawai()\n\nplot(g, layout=layout,\n     bbox = (600, 300), \n     margin = 20, \n     vertex_color = g.vs['job_cols'], \n     vertex_size = 5,\n     edge_color = \"lightgray\",\n     edge_width = ([i\/50 for i in g.es['weight']]))","657912e0":"community = g.community_multilevel()\nprint(\"Number of identified communities: {}\".format(max(community.membership)+1))\n#community = g.community_edge_betweenness().as_clustering()\ncolor_list = [\"pink\", 'lightblue', \"lightgreen\",\"purple\", \"red\", \"blue\",\"yellow\"]\n\nplot(community, layout=layout, vertex_size=5, bbox = (600, 300),\n     vertex_color=[color_list[x] for x in community.membership])","7e90e0f3":"# # calculate dendrogram\n# dendogram = g.community_fastgreedy()\n# # convert it into a flat clustering\n# clusters = dendrogram.as_clustering()\n# # get the membership vector\n# membership = clusters.membership\n\n# plot(dendogram)","1a9f4846":"selected_cols = [col.startswith(\"Q5\") | col.startswith(\"Q23\")  for col in _data_df.columns]\n_lang = _data_df.iloc[:,selected_cols]\n\n# _lang.columns = [\"Q5\",\"Analyze and understand data to influence product or business decisions\", \n# \"Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data\",\n# \"Build prototypes to explore applying machine learning to new areas\",\n# \"Build and\/or run a machine learning service that operationally improves my product or workflows\",\n# \"Experimentation and iteration to improve existing ML models\",\n# \"Do research that advances the state of the art of machine learning\",\n# \"None of these activities are an important part of my role at work\",\n# \"Other\"]\n\n_lang.columns = [\"\\n\".join(wrap(i, 40)) for i in _lang.columns]\n\n# [\"Q5\",\"Python\",\"R\", \"SQL\", \"C\", \"C++\",\"Java\", \"Javascript\", \"Julia\", \"Swift\", \"Bash\", \"MATLAB\", \"None\", \"Other\"]\n\n_lang[_lang.iloc[:,1:].notna() ==  True] = 1\n_lang[_lang.iloc[:,1:].notna() ==  False] = 0","a67991da":"# Rank the activities\nrank_activities = _lang.groupby([\"Q5\"]).sum().rank(ascending=False,axis=1).reset_index()\nrank_activities[\"color\"] = [0,1,2,3,4,5]\n\nfig = px.parallel_coordinates(rank_activities,color = rank_activities.color, #['pink','light green','light blue', 'yellow', 'purple', 'gray']\n                              labels = {'color': \"job\", \n                                        'Q23_Part_1': 'Analize and understand data', \n                                        'Q23_Part_2': 'Build data infrastructure', \n                                        'Q23_Part_3': 'Build prototypes to explore ML', \n                                        'Q23_Part_4': 'Build\/run ML services',\n                                        'Q23_Part_5': 'Experimentation to improve exist. ML', \n                                        'Q23_Part_6': 'Reaserch on ML advances', \n                                        'Q23_Part_7': 'None', \n                                        'Q23_OTHER': \"Other\"},\n                              dimensions =['color', 'Q23_Part_1', 'Q23_Part_2', 'Q23_Part_3', 'Q23_Part_4',\n       'Q23_Part_5', 'Q23_Part_6', 'Q23_Part_7', 'Q23_OTHER'])\n\nfig.show()","883bf151":"f = lambda x: x\/sum(x)\n\nselected_cols_ml = [col.startswith(\"Q5\") | col.startswith(\"Q15\")  for col in _data_df.columns]\n_ml = _data_df.iloc[:,selected_cols_ml]\n_ml[\"I\"] = [1] * len(_ml)\n\nfig, ax = plt.subplots(ncols = 1, nrows = 1, figsize = (15, 8))\n\n\nz = _ml.groupby(['Q15', 'Q5']).count().unstack(\"Q5\")\n\nidx = ['I do not use machine learning methods','Under 1 year', '1-2 years', '2-3 years',\n       '3-4 years', '4-5 years', '5-10 years','10-20 years', '20 or more years' ]\n\nz = z.loc[idx,:]\n\nz.apply(f).T.plot.barh( stacked=True, ax = ax)\n\nax.set_title(\"The number of years of general ML experience by role (% of the role's total)\")\nplt.show()","d7e74cf1":"selected_cols_Q17 = [col.startswith(\"Q5\") | col.startswith(\"Q17\")  for col in _data_df.columns]\n\nlang_mix = _data_df.iloc[:,selected_cols_Q17]\n\nlang_mix.columns = [\"Q5\",\"Linear or Logistic Regression\",\n                      \"Decision Trees or Random Forests\",\n                   \"Gradient Boosting Machines (xgboost, lightgbm, etc)\",\n                    \"Bayesian Approaches\",\n                    \"Evolutionary Approaches\",\n                    \"Dense Neural Networks (MLPs, etc)\",\n                    \"Convolutional Neural Networks\",\n                    \"Generative Adversarial Networks\",\n                    \"Recurrent Neural Networks\", \n                    \"Transformer Networks (BERT, gpt-3, etc)\",\n                    \"None\",\n                    \"Other\"]\n\nlang_mix[lang_mix.iloc[:,1:].notna() ==  True] = 1\nlang_mix[lang_mix.iloc[:,1:].notna() ==  False] = 0","8e288689":"fig, ax = plt.subplots(figsize=(12,8))\n\nsns.heatmap(lang_mix.groupby([\"Q5\"]).apply(lambda x: x.sum()\/len(x)), \n           cmap=\"YlGnBu\")\n\nplt.show()","04db4249":"# Extract columns starting with \"Q7\": \"What programming languages do you use?\" and rename the columns clearly.\n\nselected_cols = [col.startswith(\"Q5\") | col.startswith(\"Q7\")  for col in _data_df.columns]\n_lang = _data_df.iloc[:,selected_cols]\n_lang.columns = [\"Q5\",\"Python\",\"R\", \"SQL\", \"C\", \"C++\",\"Java\", \"Javascript\", \"Julia\", \"Swift\", \"Bash\", \"MATLAB\", \"None\", \"Other\"]\n\n_lang[_lang.iloc[:,1:].notna() ==  True] = 1\n_lang[_lang.iloc[:,1:].notna() ==  False] = 0","c20ceb26":"fig, ax = plt.subplots(3,2,num=1, figsize = (15,25))\n[axi.set_axis_off() for axi in ax.ravel()]\n\nr = 0\nc = 0\n\nfor job in list(set(_lang.Q5)):\n\n    #plt.figure()\n    _job_df = _lang[_lang.Q5 == job].iloc[:,1:]\n    nr_ppl = len(_job_df)\n    \n    # Generate the co-occurance matrix\n    _job_adj = _job_df.T.dot(_job_df)\n    \n    exclusive_lang = np.diag(_job_adj)\/nr_ppl\n    \n    # Normalize to the total\n    total = sum(sum(_job_adj.values))\n    diag = sum(np.diag(_job_adj))\n    _job_adj = _job_adj\/((total-diag)\/2 + diag)\n    \n    _job_adj = pd.DataFrame((_job_adj.values.tolist()))\n    _job_adj.columns = _job_df.columns\n    _job_adj.index = _job_df.columns\n\n    # Generate the graph \n\n    G = nx.from_pandas_adjacency(_job_adj)\n\n    #nodelist = G.nodes()\n    n_size = [float(ech)*2000 for ech in exclusive_lang]\n\n    w = nx.get_edge_attributes(G, 'weight')\n    edge_w = [ech*100 for ech in list(w.values())]\n    nodelist=G.nodes()\n\n    pos = nx.shell_layout(G)\n    \n    \n    #ax[c,r].scatter(r,c)\n                    \n    nx.draw_networkx_nodes(G,pos,\n                           nodelist=G.nodes(),\n                           node_size=n_size,\n                           node_color='lightgray',\n                           alpha=0.7,\n                           ax = ax[c,r])\n    nx.draw_networkx_edges(G,pos,\n                           edgelist = w.keys(),\n                           width=edge_w,\n                           edge_color='lightblue',\n                           alpha=0.6,\n                           ax = ax[c, r])\n    \n    nx.draw_networkx_labels(G, pos=pos,\n                        labels=dict(zip(nodelist,nodelist)),\n                        font_color='black',\n                        font_size= 10,\n                        ax = ax[c,r])\n    \n    ax[c,r].set_title(job, fontsize=12)\n    \n    c = c + 1\n    if (c)%3==0:\n        c = 0\n        r = r + 1\n","22149e1b":"lang_mix = _data_df.iloc[:,selected_cols]\n\ncombo_lang = []\n\nfor idx, row in lang_mix.iloc[:,1:].iterrows():\n    combo_lang.append(\";\".join(list(row[row.notnull()])))\n    \nlang_mix[\"lg_mix\"] = combo_lang\nlang_mix[\"I\"] = [1] * len(lang_mix)\nlang_mix = lang_mix[[\"Q5\", \"lg_mix\", \"I\"]]\n\nlang_count = lang_mix.groupby([ 'Q5', 'lg_mix']).count()['I'].reset_index().sort_values(by=[\"Q5\",\"I\"], \n                                                                                        ascending=[True, False]).reset_index(drop=True)\n\nfig, ax = plt.subplots(3, 2,num=1, figsize = (15,25))\n#ax.ravel()[-1].set_axis_off()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=None)\n\n\nr = 0\nc = 0\n\nfor job in list(set(_lang.Q5)):\n    _job_df = lang_count[lang_count.Q5 == job].reset_index(drop=True)\n    nr_ppl = sum(_job_df[\"I\"])\n    \n    _job_df[\"I\"] = _job_df[\"I\"]\/nr_ppl\n    \n    #Select first 10\n    _10 = _job_df[0:12].sort_values(by=\"I\")\n    \n    ax[c,r].barh(_10[\"lg_mix\"], _10[\"I\"], color = \"lightblue\")\n    \n    ax[c,r].set_title(job, fontsize=12)\n    c = c + 1\n    if (c)%3==0:\n        c = 0\n        r = r + 1\n","7391e8fc":"# How many years in programming?\n\nselected_cols = [col.startswith(\"Q5\") | col.startswith(\"Q6\")  for col in _data_df.columns]\n_prg = _data_df.iloc[:,selected_cols]\n_prg[\"I\"] = [1] * len(_prg)\n\nf = lambda x: x\/sum(x)\n\n\n\nfig, ax = plt.subplots(ncols = 1, nrows = 1, figsize = (15, 8))\n\nz = _prg.groupby(['Q6', 'Q5']).count().unstack(\"Q5\")\n\nidx = ['I have never written code', '< 1 years', '1-2 years','3-5 years', '5-10 years', '10-20 years', '20+ years']\n\nz = z.loc[idx,:]\n\nz.apply(f).T.plot.barh( stacked=True, ax = ax)\n\n\nax.set_title(\"The number of years of general programming experience by role (% of the role's total)\")\n\nplt.show()\n\n\nz = _ml.groupby(['Q15', 'Q5']).count().unstack(\"Q5\")\n\n\n","eab2777c":"# Q14 Visualization libraries used on a regular basis:\n\nselected_cols_Q14 = [col.startswith(\"Q5\") | col.startswith(\"Q14\")  for col in _data_df.columns]\n\nviz_mix = _data_df.iloc[:,selected_cols_Q14]\n\ncombo_viz = []\n\nfor idx, row in viz_mix.iloc[:,1:].iterrows():\n    combo_viz.append(\";\".join(list(row[row.notnull()])))\n    \nviz_mix[\"viz_mix\"] = combo_viz\nviz_mix[\"I\"] = [1] * len(viz_mix)\nviz_mix = viz_mix[[\"Q5\", \"viz_mix\", \"I\"]]\n\n#lang_mix.ide_mix[lang_mix.ide_mix == \"\"] = \"None\"\nviz_mix = viz_mix[viz_mix.viz_mix != \"\"]\nviz_count = viz_mix.groupby([ 'Q5', 'viz_mix']).count()['I'].reset_index().sort_values(by=[\"Q5\",\"I\"], \n                                                                                        ascending=[True, False]).reset_index(drop=True)\n\n\n\n","d019e780":"fig, ax = plt.subplots(3,2,num=1, figsize = (15,25))\n#ax.ravel()[-1].set_axis_off()\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\n\n\nr = 0\nc = 0\n\nfor job in list(set(_lang.Q5)):\n    _job_df = viz_count[viz_count.Q5 == job].reset_index(drop=True)\n    nr_ppl = sum(_job_df[\"I\"])\n    \n    _job_df[\"I\"] = _job_df[\"I\"]\/nr_ppl\n    \n    #Select first 10\n    _10 = _job_df[0:10].sort_values(by=\"I\")\n    labels = [\"\\n\".join(wrap(i, 30)) for i in _10[\"viz_mix\"]]\n    \n    ax[c,r].barh(labels, _10[\"I\"], color = \"lightblue\")\n    \n    ax[c,r].set_title(job, fontsize=12)\n    c = c + 1\n    if (c)%3==0:\n        c = 0\n        r = r + 1","6d2a4297":"### Interpretation:\nThe role with the least machine learning experience is that of **data analyst**, with almost 60% of practitioners not using ML methods, or using them for less than a year. This is also the role with the lowest number of highly experience ML users, under 5% having more than 5 years of general machine learning experience.\n\n10 to 20% of **data scientists, machine learning engineers and statisticians** are highly experienced, with over 5 years of ML experience. The highest percentage of data professionals with over 20 years of experience are statisticians. Data scientists' and machine learning engineers' populations are characterized by a low number of peers not using ML methods (under 5%), and a high share of novices (between 40 and 50% of the total).\n\nIt is interesting to observe a similar percent of professionals with 2-3 years of experience across all roles, as well as that for some jobs (data scientist, statistician) the shares with higher experience are almost equal, as if there would be a \"role retention\" across time.","55ff5d81":"# Community detection analysis\n\nThe survey includes one question of interest for this study (Q23), where each person listed the activities that make a big part of their role. This is a multiple choice question, so in order to draw conclusions, we create a graph of people. Each node in the graph represents one person and a link between two persons was calculated as:\n\n![image.png](attachment:image.png) , where A and B are nodes in the graph.\n\n\n\nThe values range between 0 and 1, where 0 mean no similarity and 1 means perfectly similar activities.\n\nFor visualization purposes I only selected 1000 nodes (to minimize the runtime).\n\n#### Colors in the graph:\n- <span style=\"background-color: pink; color: white;\"><b>Data Analyst<\/b><\/span>\n- <span style=\"background-color: lightgreen; color: white;\"><b>Data Engineer<\/b><\/span>\n- <span style=\"background-color: lightblue; color: white;\"><b>Data Scientist<\/b><\/span>\n- <span style=\"background-color: yellow; color: white;\"><b>Machine Learning Engineer<\/b><\/span>\n- <span style=\"background-color: purple; color: white;\"><b>DBA\/Database Engineer<\/b><\/span>'\n- <span style=\"background-color: gray; color: white;\"><b>Statistician<\/b><\/span>\n\n**In the first graph** we see how people with different roles are linked together, based on the type of activities they consider most important in their role. We fix a threshold of at least 0.5 similarity score, for a link to exist. \n\nBased on who they link with, we are performing community detection (**second graph**), which is a more complex clustering, which besides the person's features, also takes into consideration it's connections. ","a740ad55":"# Programming languages used on a regular basis (Q7)\n\nThere are some programming languages we expect a data science professional to use: R is known for it's diversity of algorithms, Python has some useful libraries, with MATLAB we are used since the university years. But what is even more interesting, is what combinations of languages do they use, given their job title.\n\nI chose **graph networks** in order to visualize the programming languages used by these professionals. This way, we can go beyond \"single value\" histograms and observe co-occurences. \nA graph is built if nodes (or vertices) and edges (links between the vertices). In this case, each node represents a programming language. The width of an edge represents how often are two programming languages used by the same person.\n\nThe goal is to observe any similar patterns between different jobs, in terms of programming skills. Given the data unballance, we normalize all values, so reliable conclusions can be drawn.\n","6175f9f7":"# Data jobs. Are they truly different?\n\n**Data scientis, data analysit, AI specialist, ML engineer, big data consultant, statistician, data enthusiast. Who is who?**\n\n![yuh43yxmtpe31.jpg](attachment:yuh43yxmtpe31.jpg)\n\n\nIn my team we are four people and we all do very similar work: two data scientists, an AI specialist and a machine learning engineer. I have a masters in Statistics, so I was considering myself a statistician. My first job was of data analyst, then I became a big data consultant, and then my job title changed without any of my tasks or skills dramatically following, to AI specialist. I would consider myself more as a \"jack of all trades\", but that isn't a professional role naming.\n\nWhat I would like to understand with this analysis is: what skills and job responsibilities make the difference between the data sicence roles. **In theory we do have definitions of what each role should do, but do these apply in practice?** Are people in data science, really acting within the standard responsibilities, or are they going beyond them?","d8b0e2b3":"### Interpretation:\n\nThe most important activity for **DAB\/Database engineers** (code:0) is to **build and\/or run the data infrastructure** that the business uses for storing, analyzing, and operationalizing data. On the second position is understanding and analyzing the data.\n\nFor **Data analysts, Data scientists, Data engineers and Statisticians** the number one priority is analyzing and understanding the data. Data analysts, data engineers and statisticians are more involved in building and running the data infrastructure that the bussiness uses, they explore new areas where ML could be used by building prototypes and experiment with existing ML techniques. Data scientists are not building data infrastructures so much, but they consider an important part of their job to build and\/or run machine learning services that operationally improves a product or workflows.\n\n**ML engineers** are focusing on building prototypes to explore applying machine learning to new areas, experimenting and iterating to improve existing ML models and building and\/or running machine learning services. They don't contribute to building the data infrastructure, but they spend much time understanding the data that goes into their models.\n\n\nWhen it comes to the most solid technical background, needed in order to advance the research in ML, statisticians seem to contribute the most.\n\nTwo of the activities mentioned above rank highly among **all data science roles** we are looking at, and they are related to machine learning:\n\n- Build prototypes to explore applying machine learning to new areas\n- Experimentation and iteration to improve existing ML models\n\nLet's see how much experience in using machine learning algorithms do people in each role have:","476c369a":"### Interpretation:\nAs expected, everybody can do a bit of machine learning. Linear regression, logistic regression, decision trees and random forests are a must. \nStatisticians, as they have a solid mathematical background and know their way around probabilities, also have the knowledge to use Bayes approaches.\nMachine learning engineers are the group which uses neural networks the most, with convolutional neural networks in the top of their preferrences. ML engineers cover the largest spectrum of algorithms, followed by data scientists.","6ebfc395":"In order to explore the second hypothesis, we inspect the use of visualization libraries.\n\n# Visualization libraries mostly used (Q14)\n\nWhen it comes to vizualization libraries used by the data science professionals, it ofthen happens that only one library is not sufficient, so we find combinations of two or even more, depending on the programming language used.\n\n40% of database engineers do not use any visualization tools. Those who use such tools, choose libraries provided in Python, such as seaborn and matplotlib.\n\nStatisticians, who mainly use R, choose the vizualization tools available in R: ggplot, ggplot2 and shiny. \n\nSomething that cought my eye, was the high percentage of data analysts who stated they don't use any visualization tool. I was expecting this to be the job with most data inspection...\n\nData engineers, data scientists and ML engineers use similar data visualization tools.","eb5d32b9":"**Community Detection** is one of the fundamental problems in network analysis, where the goal is to find groups of nodes that are, in some sense, more similar to each other than to the other nodes. Our community detection algorithm identifies several groups of people, each group represented by one color in the network plot below, with high similarity within the groups and high dissimilarity between the groups. \n\nThe question arising is: **Are the groups the same as the job roles?**\nThe answer is NO. We keep the same position of the original graph and see how, the nodes in the <span style=\"background-color: red; color: white;\"><b> red comunity <\/b><\/span> correspond to data analysts, data scientists, ..., indicating  that it is likely that in practice, people with different roles, actually perform tasks similar to other roles. Therefore, in practice, **we don't have strict boundries between roles in terms of activities.**","e743c516":"## The role frequency in the survey\n\nThe data related job titles, which are six in total, are represented in the bar chart.\n\nMost of the respondents are data scientists, followed by data analysts and ML engineers. Due to the unballance in the data, the numbers reported will be normalized.","d3f56722":"### **How come, the roles in data science, overlap so much?** \n\nOne hypothesis might be **the use of a common programming language**. Another hypothesis could be **the need to understand the data**, in order to draw the correct, realistic conclusions.\nLet's start with the first one:","d834b0ae":"For this purpose, I choose only the data related job titles, which are six in total. I also added a short description of what each role could entail, as per (https:\/\/www.northeastern.edu\/graduate\/blog\/data-science-careers-shaping-our-future\/).\n\n### Data Scientist \nTypical Job Requirements: Find, clean, and organize data for companies. Data scientists will need to be able to analyze large amounts of complex raw and processed information to find patterns that will benefit an organization and help drive strategic business decisions.\n\n### Data Analyst\nTypical Job Requirements: Transform and manipulate large data sets to suit the desired analysis for companies. Data analysts also aid in the decision-making process by preparing reports for organizational leaders.\n\n### Machine Learning Engineer\nTypical Job Requirements: Machine learning engineers create data funnels and deliver software solutions. They typically need strong statistics and programming skills, as well as a knowledge of software engineering. In addition to designing and building machine learning systems, they are also responsible for running tests and experiments to monitor the performance and functionality of such systems.\n\n### Data Engineer\nTypical Job Requirements: Perform batch processing or real-time processing on gathered and stored data. Data engineers are also responsible for building and maintaining data pipelines which create a robust and interconnected data ecosystem within an organization, making information accessible for data scientists.\n\n### DBA\/Database Engineer\nTypical Job Requirements: Ensure data solutions are built for performance and design analytics applications for multiple platforms. In addition to creating new database systems, they often find ways to improve the performance and functionality of existing systems, as well as working to provide access to database administrators and analysts.\n\n### Statistician\nTypical Job Requirements: Statisticians work to collect, analyze, and interpret data in order to identify trends and relationships which can be used to inform organizational decision-making. Additionally, the daily responsibilities of statisticians often include design data collection processes, communicating findings to stakeholders, and advising organizational strategy.","f7824a19":"\n\nThis gives rise to aother question: **How advanced are the machine learning algorithms used by each profession?**\n\n# The ML algorithms used on a regular basis (Q17)\n\nLinear and logistic regression constitute the foundation, the first algorithms we learn as data science people. Linear regression is applied when the response variable is continuous and logistic regression is used for binary classification.\n\nDecision trees are a more complex, rule based algorithm, mostly preferred in situation when we need transparency. The results are easy to interpret due to its tree-like structure. The disadvantage is their randomness. Random forest is an ensamble method, which aggregates the results of multiple trees. \n\nGradient boosting is also linked to decision trees, specifically an ensamble algorithm trying to transform multiple weak learners into a strong one.\n\nBayesian approaches apply probabilities to statistical problems. Bayesian statistics reqire solid mathematical background. If you've heard of hypothesis testing, Markov Chains and Monte Carlo, they are all Bayesian approaches.\n\nNeural networks come in many shapes and forms and their usage increased in the last years. From convolutional neural networks for image classification, to GANs for synthetic data generation, to U-nets for segmentation or YOLO for detection, everybody seems to be using them.\n\nEvolutionary approaches are inspired from biology, but not many of the respondents use them.","55d5252f":"### Interpretation:\n\nThe programming language graphs of Data Analysts and Data Scientists look alike: different combinations between Python, SQL and R, as well as high usage of these languages as standalone. \n\nStatisticians use R the most, sometimes together with SQL or SQL and Python.\n\nDBA\/Database Engineers and Data Engineers use SQL and Python together, but also together with Bash.\n\nML Engineers seem to use Python the most, as standalone. When they combine it with other programming languages they choose C, C++, Java, so we might be tempted to believe they have a more programming-oriented background.\n\nNow, let's look at the top 12 programming language combinations and see if our conclusions stay valid.","3c9ce0df":"### Interpretation:\n\nSo, if there is a difference between these jobs in terms of visualizations, it is not related to the tools, but to the type of plots interesting to each profession. Data engineers, who are monitoring pipelines, could use the same library as the ML engineers monitoring an algorithm's performance metrics, or the data analyst performing some exploratory analysis of data.\n\n# Final Conclusions\n\n\n- Analyzing and understanding the data is in **top 3 important activities** for all roles in data science, from DAB\/Database engineer to the data analyst.\n- All data science jobs require some **basic understanding of ML algorithms**, as well as the ability to put them in practice, in different business contexts.\n- Machine Learning Engineers have strong programming skills, and are able to use a diverse range of algortithms, from liniar regression to neural networks.\n- Statisticians have a thorow understanding of the math behind the algorithms, so they can use the complex ones, based on Bayesian statistics.\n- Even if, in theory there are clear definitions of the data science roles, in practice, people tend to explore more than their own piece of cake. Kudos to them! :)","ed86461a":"Let's have a closer look at how much general programming experience do data science people have.\n\nOnly ~ 40% of data analysts have more than 3 years of experience in programming. On the opposite side we have DBA\/Database engineers and Data engineers, with more than half of their peers programming for more than 5 years.\n\nData scientists, ML engineers and Statisticians have a similar distribution of coding experience.","4188e7b1":"Let's further explore the actitivies with respect to the roles.\n\n# The activities that make an important part of the role\n\nIn order to look for differences between each role's activities, we use a parrallel coordinates plot. Each color\/ job code represents a role, as following:\n- 0 := DAB\/ Database engineer\n- 1 := Data analyst\n- 2 := Data engineer\n- 3 := Data scientist\n- 4 := ML engineer\n- 5 := Statistician\n\nOn all other axis, we rank each activity according to the number of times a person considers it relevant for his\/her job. The highest rank is 1 and the lowest is 8."}}