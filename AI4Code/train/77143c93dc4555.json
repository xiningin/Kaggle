{"cell_type":{"e9dc27e2":"code","adb7c80e":"code","c0e36dbe":"code","8f5b0167":"code","ccb4f15b":"code","917f7720":"code","22a4c0e1":"code","5537f14f":"code","dabd6086":"code","8802f02c":"code","c1c0c0e7":"code","f6f41aa4":"code","19bea3e1":"code","15144214":"code","bbcbeaf7":"code","c13652e3":"code","1617c5fb":"code","a77832b5":"code","7f72c869":"code","f5f53498":"code","2b01e872":"code","e0e20bef":"code","4a36ee03":"code","f2a2c210":"code","71bfdb90":"code","d0f6d8d4":"code","8c9ec47e":"code","56a5e921":"code","dfd71f7c":"code","092d1ed5":"code","159f9600":"code","1778ecd6":"code","9c841a69":"code","b60c1814":"code","f0b56a6b":"code","738a43df":"code","6fbbe514":"code","e3a86a21":"code","24632123":"code","e1bda67f":"code","29694673":"code","43eedee1":"code","dbcee47d":"code","4f2994ff":"code","b9fc8c98":"code","8c389316":"code","21104b28":"code","a49349e6":"code","57af2a70":"code","68aad344":"code","98a2fba9":"code","08b04bf4":"code","a07ed994":"code","b212e9ed":"code","990711fb":"code","321c9ca3":"code","94a349da":"markdown","4d89873e":"markdown","eb83800e":"markdown","b9021f4c":"markdown","01b5775e":"markdown","62cceb76":"markdown","1fba8293":"markdown","8300178a":"markdown","c1441e59":"markdown","b2dd02e6":"markdown","fe5ba1b3":"markdown","3575a69c":"markdown","c5834cf4":"markdown","2e512de2":"markdown","fbbf4107":"markdown","b8c98f90":"markdown","eea7f10a":"markdown","4b15380d":"markdown","295f8fc1":"markdown","4b2fc6a1":"markdown","4c0e39e8":"markdown","5466ce4f":"markdown","9159d7a6":"markdown","055a1ae9":"markdown","d58cdeb6":"markdown","3bf65381":"markdown","5e8c439b":"markdown","900cc16c":"markdown","48d787b1":"markdown","6b57d9c2":"markdown","2ff732bc":"markdown","d977ccc4":"markdown","6d8a048a":"markdown","970c81ac":"markdown","3825cc8d":"markdown","8be72ea8":"markdown","e6f298bb":"markdown","38144633":"markdown","2db19d90":"markdown"},"source":{"e9dc27e2":"# for data manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# pandas options\npd.set_option('display.max_columns', 50)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('mode.use_inf_as_na', True)\npd.options.mode.chained_assignment = None\n\n# for date manipulation\nfrom datetime import datetime\n\n# for visualization: matplotlib\nfrom matplotlib import pyplot as plt\nfrom IPython.core.pylabtools import figsize\n%matplotlib inline\n# to display visuals in the notebook\n\n# for visualization: seaborn\nimport seaborn as sns\nsns.set_context(font_scale=2)\n\n# for data preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import combinations\nfrom sklearn.model_selection import KFold\n\n# for building the model and calculate RMSE\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# to cleanup memory usage\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","adb7c80e":"## Function to reduce the DF size and reduce test dataframe size\ndef reduce_memory_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))","c0e36dbe":"# load training data created in the second notebook into dataframes\nX = pd.read_csv(\"\/kaggle\/input\/save-the-energy-for-the-future-2-fe-lightgbm\/X.csv\")\ny = pd.read_csv(\"\/kaggle\/input\/save-the-energy-for-the-future-2-fe-lightgbm\/y.csv\", header=None)\n\nreduce_memory_usage(X)\nreduce_memory_usage(y)","8f5b0167":"# rename target as log_meter_reading\ny.rename(columns = {0: \"log_meter_reading\"}, \n         inplace=True)","ccb4f15b":"# create categorical features \ncategorical_features = ['building_id', 'site_id', 'meter',\n                        'primary_use', 'wind_compass_direction',\n                        'day_of_week', 'hour','is_weekend', 'season']\n\n# initial parameters of light gbm algorithm\ninitial_params = {\"objective\": \"regression\",\n                  \"boosting\": \"gbdt\",\n                  \"num_leaves\": 60,\n                  \"learning_rate\": 0.05,\n                  \"feature_fraction\": 0.85,\n                  \"reg_lambda\": 2,\n                  \"metric\": {'rmse'}\n}","917f7720":"# cretae kfold object and empty model and evaluation lists\nkf = KFold(n_splits=4, shuffle=False, random_state=42)\n\n# save 4 model as a list\nmodels = []\n\n# dynamically split X and y with the k-fold split indexes\nfor train_index,valid_index in kf.split(X):\n    X_train_kf = X.loc[train_index]\n    y_train_kf = y.loc[train_index]\n    \n    X_valid_kf = X.loc[valid_index]\n    y_valid_kf = y.loc[valid_index]\n    \n    d_train = lgb.Dataset(X_train_kf, \n                          label=y_train_kf,\n                          categorical_feature=categorical_features, \n                          free_raw_data=False)\n    \n    d_valid = lgb.Dataset(X_valid_kf, \n                          label=y_valid_kf,\n                          categorical_feature=categorical_features, \n                          free_raw_data=False)\n    \n    model = lgb.train(initial_params, \n                      train_set=d_train, \n                      num_boost_round=1000, \n                      valid_sets=[d_train, d_valid],\n                      verbose_eval=250, \n                      early_stopping_rounds=500)\n    \n    models.append(model)\n    \n    del X_train_kf, y_train_kf, X_valid_kf, y_valid_kf, d_train, d_valid\n    gc.collect()","22a4c0e1":"X.columns","5537f14f":"del X\ndel y\ngc.collect()","dabd6086":"# load building  data modify\nbuilding = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv\")\n# drop floor_count\nbuilding.drop(columns=[\"floor_count\"], inplace=True)\n\n# load weather_test modify\nweather_test = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/weather_test.csv\")\nweather_test[\"timestamp\"] = pd.to_datetime(weather_test[\"timestamp\"],\n                                            format='%Y-%m-%d %H:%M:%S')\n\n# load test and modify\ntest = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/test.csv\")\ntest[\"timestamp\"] = pd.to_datetime(test[\"timestamp\"],\n                                   format='%Y-%m-%d %H:%M:%S')\n\nreduce_memory_usage(building)\nreduce_memory_usage(weather_test)\nreduce_memory_usage(test)","8802f02c":"# add building age column\ncurrent_year = datetime.now().year\nbuilding['building_age'] = current_year - building['year_built']\nbuilding.drop(columns=['year_built'], inplace=True)\n\n# since NA values only present in building age fillna can be used\nbuilding.fillna(round(building.building_age.mean(),0),\n                inplace=True)","c1c0c0e7":"# create label encoder object and transform the column\nle = LabelEncoder()\nle_primary_use = le.fit_transform(building.primary_use)\n\n# add label encoded column to dataframe\nbuilding['primary_use'] = le_primary_use\n\ndel le, le_primary_use\ngc.collect()","f6f41aa4":"# check if any NA values left\nbuilding.isna().sum()","19bea3e1":"def convert_season(month):\n    if (month <= 2) | (month == 12):\n        return 0\n    # as winter\n    elif month <= 5:\n        return 1\n    # as spring\n    elif month <= 8:\n        return 2\n    # as summer\n    elif month <= 11:\n        return 3\n    # as fall","15144214":"# add month, day of week, day of month, hour, season\nweather_test['month'] = weather_test['timestamp'].dt.month.astype(np.int8)\nweather_test['day_of_week'] = weather_test['timestamp'].dt.dayofweek.astype(np.int8)\nweather_test['day_of_month']= weather_test['timestamp'].dt.day.astype(np.int8)\nweather_test['hour'] = weather_test['timestamp'].dt.hour\nweather_test['season'] = weather_test.month.apply(convert_season)\n\n# add is weekend column\nweather_test['is_weekend'] = weather_test.day_of_week.apply(lambda x: 1 if x>=5 else 0)","bbcbeaf7":"# reset index for fast update\nweather_test = weather_test.set_index(\n    ['site_id','day_of_month','month'])","c13652e3":"# create dataframe of daily means per site id\nair_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['air_temperature'].mean(),\n                                      columns=[\"air_temperature\"])\n# create dataframe of air_temperatures to fill\ntemporary_df = pd.DataFrame({'air_temperature' : weather_test.air_temperature})\n\n# update NA air_temperature values\ntemporary_df.update(air_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"air_temperature\"] = temporary_df[\"air_temperature\"]\n\ndel temporary_df, air_temperature_filler\ngc.collect()","1617c5fb":"# create dataframe of daily means per site id\ncloud_coverage_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['cloud_coverage'].mean(),\n                                     columns = ['cloud_coverage'])\ncloud_coverage_filler.fillna(round(cloud_coverage_filler.cloud_coverage.mean(),0), \n                             inplace=True)\n\n# create dataframe of cloud_coverages to fill\ntemporary_df = pd.DataFrame({'cloud_coverage' : weather_test.cloud_coverage})\n\n# update NA cloud_coverage values\ntemporary_df.update(cloud_coverage_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"cloud_coverage\"] = temporary_df[\"cloud_coverage\"]\n\ndel temporary_df, cloud_coverage_filler\ngc.collect()","a77832b5":"# create dataframe of daily means per site id\ndew_temperature_filler = pd.DataFrame(weather_test\n                                      .groupby(['site_id','day_of_month','month'])\n                                      ['dew_temperature'].mean(),\n                                      columns=[\"dew_temperature\"])\n# create dataframe of dew_temperatures to fill\ntemporary_df = pd.DataFrame({'dew_temperature' : weather_test.dew_temperature})\n\n# update NA dew_temperature values\ntemporary_df.update(dew_temperature_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"dew_temperature\"] = temporary_df[\"dew_temperature\"]\n\ndel temporary_df, dew_temperature_filler\ngc.collect()","7f72c869":"# create dataframe of daily means per site id\nprecip_depth_filler = pd.DataFrame(weather_test\n                                   .groupby(['site_id','day_of_month','month'])\n                                   ['precip_depth_1_hr'].mean(),\n                                   columns=['precip_depth_1_hr'])\nprecip_depth_filler.fillna(round(precip_depth_filler['precip_depth_1_hr'].mean(),0)\n                           , inplace=True)\n\n# create dataframe of precip_depth_1_hr to fill\ntemporary_df = pd.DataFrame({'precip_depth_1_hr' : weather_test.precip_depth_1_hr})\n\n# update NA precip_depth_1_hr values\ntemporary_df.update(precip_depth_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"precip_depth_1_hr\"] = temporary_df[\"precip_depth_1_hr\"]\n\ndel precip_depth_filler, temporary_df\ngc.collect()","f5f53498":"# create dataframe of daily means per site id\nsea_level_filler = pd.DataFrame(weather_test\n                                .groupby(['site_id','day_of_month','month'])\n                                ['sea_level_pressure'].mean(),\n                                columns=['sea_level_pressure'])\nmean_sea_level_pressure = round(\n    sea_level_filler\n    ['sea_level_pressure']\n    .astype(float)\n    .mean(),2)\n\nsea_level_filler.fillna(mean_sea_level_pressure, inplace=True)\n\n# create dataframe of sea_level_pressure to fill\ntemporary_df = pd.DataFrame({'sea_level_pressure' : weather_test.sea_level_pressure})\n\n# update NA sea_level_pressure values\ntemporary_df.update(sea_level_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"sea_level_pressure\"] = temporary_df[\"sea_level_pressure\"]\n\ndel sea_level_filler, temporary_df\ngc.collect()","2b01e872":"# create dataframe of daily means per site id\nwind_direction_filler = pd.DataFrame(weather_test\n                                     .groupby(['site_id','day_of_month','month'])\n                                     ['wind_direction'].mean(),\n                                     columns=['wind_direction'])\n# create dataframe of wind_direction to fill\ntemporary_df = pd.DataFrame({'wind_direction' : weather_test.wind_direction})\n\n# update NA wind_direction values\ntemporary_df.update(wind_direction_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_direction\"] = temporary_df[\"wind_direction\"]\n\ndel temporary_df, wind_direction_filler\ngc.collect()","e0e20bef":"# create dataframe of daily means per site id\nwind_speed_filler = pd.DataFrame(weather_test\n                                 .groupby(['site_id','day_of_month','month'])\n                                 ['wind_speed'].mean(),\n                                 columns=['wind_speed'])\n# create dataframe of wind_speed to fill\ntemporary_df = pd.DataFrame({'wind_speed' : weather_test.wind_speed})\n\n# update NA wind_speed values\ntemporary_df.update(wind_speed_filler, overwrite=False)\n\n# update in the weather train dataset\nweather_test[\"wind_speed\"] = temporary_df[\"wind_speed\"]\n\ndel temporary_df, wind_speed_filler\ngc.collect()","4a36ee03":"# check if NA values are left\nweather_test.isna().sum()","f2a2c210":"weather_test = weather_test.reset_index()","71bfdb90":"def convert_direction(degrees):\n    if degrees <= 90:\n        return 0\n    # as norteast direction\n    elif degrees <= 180:\n        return 1\n    # as southeast direction\n    elif degrees <= 270:\n        return 2\n    # as southwest direction\n    elif degrees <= 360:\n        return 3\n    # as northwest direction\nweather_test['wind_compass_direction'] = weather_test.wind_direction.apply(convert_direction)\nweather_test.drop(columns=['wind_direction'], inplace=True)","d0f6d8d4":"# create weather variables combinations\nweather_test['meansea_level_pressure_wind_speed'] = (weather_test['sea_level_pressure'] +\n                                                     weather_test['wind_speed']) \/ 2\nweather_test['meancloud_coverage_sea_level_pressure'] = (weather_test['sea_level_pressure'] + \n                                                         weather_test['cloud_coverage']) \/ 2\nweather_test['meancloud_coverage_wind_speed '] = (weather_test['cloud_coverage'] + \n                                                  weather_test['wind_speed']) \/ 2\nweather_test['meanprecip_depth_1_hr_sea_level_pressure'] = (weather_test['precip_depth_1_hr'] + \n                                                            weather_test['sea_level_pressure']) \/ 2\nweather_test['meanair_temperature_sea_level_pressure'] = (weather_test['air_temperature'] + \n                                                          weather_test['sea_level_pressure']) \/ 2","8c9ec47e":"# merge dataframes on test dataframe\ntest = test.merge(building, on = \"building_id\", how = \"left\")\ntest = test.merge(weather_test, on = [\"site_id\", \"timestamp\"], how=\"left\")\n\n# delete the other ones to save space from the memory\ndel weather_test\ndel building\ngc.collect()","56a5e921":"test.columns","dfd71f7c":"test.drop(columns = [\"row_id\", \n                     \"timestamp\"], inplace=True)","092d1ed5":"#feature_set = ['building_age', 'le_primary_use', 'cloud_coverage',\n#               'is_weekend','wind_speed', 'day_of_week',\n#               'wind_compass_direction', 'sea_level_pressure', 'air_temperature',\n#               'day_of_month', 'dew_temperature', 'hour', \n#               'month', 'meter', 'building_id', \n#               'site_id', 'floor_count', 'square_feet', 'year']","159f9600":"print(\"Number of unique columns in the test dataset:\", test.shape[1])","1778ecd6":"test.isna().sum()","9c841a69":"# split test set into two for faster imputations\nX_test_2017 = test[:20848800]\nX_test_2018 = test[20848800:]\n\ndel test\ngc.collect()","b60c1814":"X_test_2017 = X_test_2017.fillna(method='ffill', axis=1)\nreduce_memory_usage(X_test_2017)\ngc.collect()","f0b56a6b":"X_test_2018 = X_test_2018.fillna(method='ffill', axis=1)\nreduce_memory_usage(X_test_2018)\ngc.collect()","738a43df":"print('2017 Test Data Shape:', X_test_2017.shape)","6fbbe514":"print('2018 Test Data Shape:', X_test_2018.shape)","e3a86a21":"X_test_2017.memory_usage()","24632123":"X_test_2017.dtypes","e1bda67f":"# features that datatypes to be converted\nint_features = ['building_age', 'primary_use', \n                'is_weekend',  'wind_compass_direction']\n\nfor feature in int_features:\n    X_test_2017[feature] = X_test_2017[feature].astype('int8')\n    X_test_2018[feature] = X_test_2018[feature].astype('int8')","29694673":"X_test_2017.memory_usage()","43eedee1":"gc.collect()","dbcee47d":"predictions_2017 = []\n\nfor model in models:\n    if  predictions_2017 == []:\n        predictions_2017 = (np\n                            .expm1(model\n                                   .predict(X_test_2017, \n                                            num_iteration=model.best_iteration)) \/ len(models))\n    else:\n        predictions_2017 += (np\n                             .expm1(model\n                                    .predict(X_test_2017,\n                                             num_iteration=model.best_iteration)) \/ len(models))","4f2994ff":"del X_test_2017\ngc.collect()","b9fc8c98":"predictions_2018 = []\n\nfor model in models:\n    if  predictions_2018 == []:\n        predictions_2018 = (np\n                            .expm1(model\n                                   .predict(X_test_2018, \n                                            num_iteration=model.best_iteration)) \/ len(models))\n    else:\n        predictions_2018 += (np\n                             .expm1(model\n                                    .predict(X_test_2018, \n                                             num_iteration=model.best_iteration)) \/ len(models))","8c389316":"del X_test_2018\ngc.collect()","21104b28":"for model in models:\n    lgb.plot_importance(model)\n    plt.show()","a49349e6":"# to fetch row_ids\nsample_submission = pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv\")\nrow_ids = sample_submission.row_id\n\ndel sample_submission\ngc.collect()","57af2a70":"# make sure of the shape of predictions\npredictions_2017.shape","68aad344":"predictions_2018.shape","98a2fba9":"# split row_id's with the indexes of 2017 and 2018 predictions\nrow_ids_2017 = row_ids[:predictions_2017.shape[0]]\nrow_ids_2018 = row_ids[predictions_2018.shape[0]:]","08b04bf4":"submission_2017 = pd.DataFrame({\"row_id\": row_ids_2017, \n                                \"meter_reading\": np.clip(predictions_2017, 0, a_max=None)})\n\nsubmission_2018 = pd.DataFrame({\"row_id\": row_ids_2018, \n                                \"meter_reading\": np.clip(predictions_2018, 0, a_max=None)})","a07ed994":"submission = pd.concat([submission_2017,\n                        submission_2018])\n\ndel submission_2017, submission_2018\ngc.collect()","b212e9ed":"submission","990711fb":"submission.to_csv(\"submission.csv\", index=False)","321c9ca3":"del models\ngc.collect()","94a349da":"**Reset Index for Update**","4d89873e":"I tried to generate the predictions with the as-is X_test however, I ran into the memory errors:\n> Unable to allocate array with shape (41498571, 18) and data type float64)\n\nSo I am going to convert some float columns to int data types to occupy less space in the memory.","eb83800e":"### <a id = '6-3-2'> 6.3.2. Weather features transformation <\/a>\n<a href = '#top'> Back to top <\/a>","b9021f4c":"**Dew Temperature**","01b5775e":"# <a id = 'top'> Table of Contents <\/a>\n  - <a href = '#1'> Quick Recap from Previous Notebook <\/a>\n  - <a href = '#6'> 6. Evaluate Model with Test Data <\/a>\n    - <a href = '#6-1'> 6.1. Load training data for re-creating model <\/a>\n    - <a href = '#6-2'> 6.2. Build model <\/a>\n    - <a href = '#6-3'> 6.3. Feature alignment between training and test set & Imputation <\/a>\n      - <a href = '#6-3-1'> 6.3.1. Building features transformation <\/a>\n      - <a href = '#6-3-2'> 6.3.2. Weather features transformation <\/a>\n      - <a href = '#6-3-3'> 6.3.3. Create one test dataframe <\/a>\n      - <a href = '#6-3-4'> 6.3.4. Split test data for 2017 and 2018 <\/a>\n      - <a href = '#6-3-5'> 6.3.5. Impute missing values <\/a>\n    - <a href = '#6-4'> 6.4. Make sure test data shape and check for missing values <\/a>\n    - <a href = '#6-5'> 6.5. Generate predictions <\/a>\n  - <a href = '#7'> 7. Interpret Model Results <\/a>\n  - <a href = '#8'> 8. Submissions & Summary & Conclusions <\/a>","62cceb76":"Thank you for sticking with me until the very end! \ud83d\ude0d\n\nIn this notebook, we covered:\n\n6. Evaluate model with test data\n7. Interpret model results\n8. Submissions & summary & conclusions\n\nBefore summarizing the project I will submit predicitons. As I predicted 2017 and 2018 results seperately, I am going to merge them now into one dataframe.","1fba8293":"I will perform same steps performed in the feature engineering to align test set with the training set. With the cross validated model I will generate the predictions and evaluate them.\n\nFinally, I will end the project with an overall summary.","8300178a":"Let's have a look at the columns of X and align accordingly","c1441e59":"# <a id = '6'> 6. Evaluate Model with Test Data <\/a>\n<a href = '#top'> Back to top <\/a>","b2dd02e6":"In the previous notebook, we focused on feature engineering, building model and improving the evaluation metric (RMSE) further with cross validation and hyperparameter optimization.\n\nWith feature engineering, we extracted and transformed features from the existing columns, added features by combining weather variables. Afterwards, with a univariate feature selection method, looking at the pearson coefficients of the features to the log1p of the meter_reading, we determined on final 24 features.\n\nFor model selection, linear regression, k nearest neighbor regressor and light gradient boosting models are built. Light gbm delivered the best results. After applying the cross-validation results RMSE improved from 1.21 to 1.1, hyperparameter search did not help much in improving the results. \n\n**Here is a visual from how training and validation error improved in from one of the cross-validation folds of Light GBM:**\n![image.png](attachment:image.png)","fe5ba1b3":"**Precip Depth 1 Hour**","3575a69c":"### <a id = '6-3-5'> 6.3.5. Impute missing values <\/a>\n<a href = '#top'> Back to top <\/a>","c5834cf4":"**Air Temperature**","2e512de2":"We've come to an end in this long and comprehensive project, the highlights from the 3 notebooks and the machine learning workflow are: \n\n* Exploratory data analysis\n* Feature engineering & selection\n* Building an efficient model with model selection, tuning and cross-validation\n\nIn the [first notebook](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-1-detailed-eda), first I understand analyzed what affects most a building's energy consumption in electricity, chilled water, steam and hot water. The area, purpose and age are the most important factors for a building to determine its energy consumption. The air temperature, humidity, sea level pressure and how cloudy the sky is at the current time are the most important weather factors for building's energy consumption.\n\nIn the [second notebook](om\/cereniyim\/save-the-energy-for-the-future-2-fe-lightgbm\/notebook), feature engineering showed its importance: with the newly generated features as part of this project had also high impact in determining the meter_reading values.\n\nI moved forward with the light gbm model which seems as efficient solution when dealing with 20 or 40 million rows after exploring some other notebooks in this competition. Based on the 30 features selected and 1 year time-series data available (nearly 20 million rows) I predicted the next 2 years electricity, chilled water, hot water and steam consumption of each building (nearly 40 million rows). Even though I did not climb high in the competition ladder I learned a lot on the way which is more important for me.\n\nDuring this project, I find a chance to explore new libraries such as pandas profiling, hyperopt and light gbm and learned to deal with large dataset as well as the RAM. Most importantly, I applied the machine learning workflow to a supervised regression problem for the first time, introduced by Will Koehrsen [in this article](https:\/\/towardsdatascience.com\/a-complete-machine-learning-walk-through-in-python-part-one-c62152f39420).\n\nThank you everyone for reading, upvoting and commenting on my kernels. I hope that I contributed to make energy-efficient buildings with this project and inspire other people to join this cause with me. ","fbbf4107":"### <a id = '6-3-4'> 6.3.4. Split test data for 2017 and 2018 <\/a>\n<a href = '#top'> Back to top <\/a>","b8c98f90":"With the memory usage concerns, I will split test dataset for 2017 and 2018 based on `year` column. If we ignore year column test dataset has 24 columns, aligning with the number of columns X has. Now let's check if there are any NA columns still exist after the merge:","eea7f10a":"This notebook aims to predict a building's energy consumption over 2017 and 2018 using the data from 2016 in 4 different consumpiton categories (electricity, chilled water, steam, hot water) using ASHRAE data, which is our problem statement as well.\n\nThis is a supervised machine learning model, meaning based on the columns available in the datasets and data from 2016, we are going to train the model to predict an energy consumption of a building in each category. Since, consumption values are labeled as meter_reading and they are continuous, we are going to apply regression techniques to generate predictions on meter_reading.\n\nIt is a highly debated and popular competition in Kaggle currently, however my main motivation is to contribute to make energy-efficient buildings by estimating its energy consumption. It seemed like a good start to save our energy for future!\n\nThere will be 3 notebooks covering the complete machine learning building pipeline. \n* [Notebook 1](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-1-detailed-eda) covered understanding, formating the data and detailed EDA. \n* [Notebook 2](https:\/\/www.kaggle.com\/cereniyim\/save-the-energy-for-the-future-2-fe-lightgbm) consists of the feature engineering, building the model and tuning its parameters.\n\nThis third and last notebook will focus on parts 6, 7 and 8 focusing on generating predictions, evaluating them and a summary about the whole project.\n\n1) Understand, Cleand and Format Data\n\n2) Exploratory Data Analysis\n\n3) Feature Engineering & Selection\n\n4) Compare Several Machine Learning Models\n\n5) Perform Hyperparameter Tuning and Cross Validation\n\n**6) Evaluate Model with Test Data**\n\n**7) Interpret Model Results**\n\n**8) Summary & Conclusions**\n\nMachine Learning application and building is not a linear and one time process. Steps above enable me to follow a structured way for an end-to-end machine project flow and preparation for the each step ahead. All in all, steps might be modified or revisited according to findings. You can use the table of contents to navigate to each section. \ud83d\udc47\n\nEnjoy reading !","4b15380d":"**Add combination of weather variables**","295f8fc1":"## <a id = '6-2'> 6.2. Build model <\/a>\n<a href = '#top'> Back to top <\/a>","4b2fc6a1":"# <a id = '8'> 8. Submissions & Summary & Conclusions  <\/a>\n<a href = '#top'> Back to top <\/a>","4c0e39e8":"In the feature engineering part, I selected features based on its pearson coefficients to the log_meter_reading values.  Since final results are generated, I will revisit this picture and observe features with the highest contribution to the model predictions. `plot_importances`  attribute will be used for this, visualizing each of the four model's feature importance individually.","5466ce4f":"![image](https:\/\/www.technotification.com\/wp-content\/uploads\/2018\/09\/Renewable-Energy-Ideas-1200x600.jpg)\nImage source: [technotification](https:\/\/www.technotification.com\/2018\/09\/amazing-renewable-energy-ideas.html)","9159d7a6":"## <a id = '6-1'> 6.1. Load training data for re-creating model <\/a>\n<a href = '#top'> Back to top <\/a>","055a1ae9":"**Reset indexes to transfrom weather dataframe to original form**","d58cdeb6":"## <a id = '6-5'> 6.5. Generate predictions <\/a>\n<a href = '#top'> Back to top <\/a>","3bf65381":"### <a id = '6-3-1'> 6.3.1. Building features transformation <\/a>\n<a href = '#top'> Back to top <\/a>","5e8c439b":"### <a id = '6-3-3'> 6.3.3. Create one test dataframe <\/a>\n<a href = '#top'> Back to top <\/a>","900cc16c":"**Convert Wind Direction**","48d787b1":"# <a id = '1'> Quick Recap from Previous Notebook <\/a>\n<a href = '#top'> Back to top <\/a>","6b57d9c2":"If you remember, we took the np.log1p (natural logarithm of meter reading+1) of the meter reading values and trained our model with those values. So, our model learned to predict np.log1p values. \n\nTo align the predictions back to the meter reading values, I am going to use the inverse function of log1p, np.expm1 and generate predictions for 2017 and 2018 seperately due to high memory usage.\n\nnp.expm1 function calculates the  exp(x) - 1 for all elements in the array.\n\nLooking at the training and validation errors of this notebook and previous notebook I expect to see RMSE between 1.2 and 2.5.","2ff732bc":"## <a id = '6-4'> 6.4. Make sure test data shape and check for missing values <\/a>\n<a href = '#top'> Back to top <\/a>","d977ccc4":"**Sea Level Pressure**","6d8a048a":"# <a id = '7'> 7. Interpret Model Results <\/a>\n<a href = '#top'> Back to top <\/a>","970c81ac":"![image](https:\/\/i.pinimg.com\/736x\/7c\/6e\/1b\/7c6e1b412a365376894828a348955372.jpg)\nImage source: [pinterest](https:\/\/www.pinterest.com\/pin\/576179346069675606\/) ","3825cc8d":"**In all of the 4 models built, top 15 features for determining the meter reading values:**\n* building_id\n* meter\n* month\n* day of month\n* square_feet\n* season\n* air temperature\n* dew temperature\n* building age\n* site id\n* primary use\n* day of week\n* mean dew tempereature and sea level pressure\n* mean air temperature precip depth 1 hr\n* day of week","8be72ea8":"## <a id = '6-3'> 6.3. Feature alignment between training and test set & Imputation <\/a>\n<a href = '#top'> Back to top <\/a>","e6f298bb":"**Cloud Coverage**","38144633":"**Wind Speed**","2db19d90":"**Wind Direction**"}}