{"cell_type":{"07100393":"code","037f9c70":"code","5c7c7d35":"code","db0f5213":"code","b6b44cea":"code","43903e2c":"code","81291e9a":"code","edbdea45":"code","741347ea":"code","e7d4d942":"code","1a5c040f":"code","73753dbb":"code","841683eb":"code","5f8e6a22":"markdown","2bfc233f":"markdown","ccd9c8d4":"markdown","412020b5":"markdown","fe3be62d":"markdown","cdf3ef69":"markdown","6051185d":"markdown","aa2554ae":"markdown","af499155":"markdown","cac39d9b":"markdown"},"source":{"07100393":"from tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nfrom functools import partial\ntqdm = partial(tqdm, position=0, leave=True)","037f9c70":"%%capture\n# Download transformers  and install required packages.\n!git clone https:\/\/github.com\/huggingface\/transformers\n%cd transformers\n!pip install .\n!pip install -r .\/examples\/_tests_requirements.txt \n%cd ..","5c7c7d35":"train_df = pd.read_csv('\/kaggle\/input\/scl2021data\/cleaned_train.csv').fillna('')\ntrain_df.head()","db0f5213":"test_df = pd.read_csv('\/kaggle\/input\/scl2021data\/cleaned_test.csv').fillna('')\ntest_df.head()","b6b44cea":"# train dev split - 90:10\nimport numpy as np\n#train, dev, test =  np.split(train_df.sample(frac=1, random_state=42), \n#                       [int(.6*len(train_df)), int(.8*len(train_df))])\ntrain, dev =  np.split(train_df.sample(frac=1, random_state=42), \n                       [int(.9*len(train_df))])","43903e2c":"dev.to_csv('dev.csv', index=False)","81291e9a":"# Process raw data by assigning entity POI or ST or O to each word\ndef preprocess_train_data(raw_df, output_name):\n    with open(output_name, 'a') as text_file:\n        for index, row in tqdm(raw_df.iterrows()):\n            address = row['raw_address'].replace(\",\", \"\").split()\n            if row['POI'] == '':\n                poi = ''\n            else:\n                poi = row['POI'].split()\n            if row['street'] == '':\n                st = ''\n            else:\n                street = row['street'].split()\n            for address_word in address:\n                if any(address_word in p for p in poi):\n                    text_file.write(address_word + ' POI \\n')  \n                elif any(address_word in s for s in street):\n                    text_file.write(address_word + ' ST \\n') \n                else:\n                    text_file.write(address_word + ' O \\n')  \n\npreprocess_train_data(train, 'train_temp.txt')\npreprocess_train_data(dev, 'dev_temp.txt')","edbdea45":"train_temp_txt = pd.read_table('train_temp.txt')\nprint(train_temp_txt)","741347ea":"# Process raw data by assigning entity POI or ST or O to each word\ndef preprocess_test_data(raw_df, output_name):\n    start_counter = 1\n    end_counter = 0\n    raw_df['start'] = start_counter\n    raw_df['end'] = end_counter\n    with open(output_name, 'a') as text_file:\n        for index, row in tqdm(raw_df.iterrows()):\n            raw_df.loc[index, 'start'] = start_counter\n            address = row['raw_address'].replace(\",\", \"\").split()\n            for address_word in address:\n                text_file.write(address_word + '\\n')\n                start_counter += 1\n                end_counter += 1\n            raw_df.loc[index, 'end'] = end_counter\n    return raw_df\n\ntest_processed_df = preprocess_test_data(test_df, 'test_temp.txt')\ntest_processed_df.head()\n","e7d4d942":"%%time\n# Set parameters\nMAX_LENGTH = 128 #@param {type: \"integer\"}\nMODEL = \"indobenchmark\/indobert-lite-base-p1\" #@param [\"chriskhanhtran\/spanberta\", \"bert-base-multilingual-cased\", \"indobenchmark\/indobert-lite-base-p1\", \"indobenchmark\/indobert-large-p1\"]\nPATH = \"\/kaggle\/input\/scl2021-src\/\"\n!python3 $PATH\/preprocess.py train_temp.txt $MODEL $MAX_LENGTH > train.txt\n!python3 $PATH\/preprocess.py dev_temp.txt $MODEL $MAX_LENGTH > dev.txt\n!python3 $PATH\/preprocess.py test_temp.txt $MODEL $MAX_LENGTH > test.txt\n# Generate labels.txt\n!cat train.txt dev.txt | cut -d \" \" -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\nlabels_txt = pd.read_table('labels.txt')\nprint(labels_txt)\ntrain_txt = pd.read_table('train.txt')\nprint(train_txt)","1a5c040f":"%%capture\n# install required packages\n!pip install seqeval\n!pip install datasets","73753dbb":"%%time\n# Set training parameters\nMAX_LENGTH = 128 #@param {type: \"integer\"}\nMODEL = \"indobenchmark\/indobert-large-p1\" #@param [\"chriskhanhtran\/spanberta\", \"bert-base-multilingual-cased\", \"indobenchmark\/indobert-lite-base-p1\", \"indobenchmark\/indobert-large-p1\"]\nOUTPUT_DIR = \"\/kaggle\/working\/indobert-ner\" #@param [\"spanberta-ner\", \"bert-base-ml-ner\", \"indobert-ner\", \"drive\/MyDrive\/Shopee\"]\nBATCH_SIZE = 16 #@param {type: \"integer\"}\nNUM_EPOCHS = 3 #@param {type: \"integer\"}\nSAVE_STEPS = 2000 #@param {type: \"integer\"}\nLOGGING_STEPS = 1000 #@param {type: \"integer\"}\nSEED = 42 #@param {type: \"integer\"}\n\n!python3 $PATH\/run_ner.py \\\n  --data_dir .\/ \\\n  --model_type bert \\\n  --labels .\/labels.txt \\\n  --model_name_or_path $MODEL \\\n  --output_dir $OUTPUT_DIR \\\n  --max_seq_length  $MAX_LENGTH \\\n  --num_train_epochs $NUM_EPOCHS \\\n  --per_gpu_train_batch_size $BATCH_SIZE \\\n  --save_steps $SAVE_STEPS \\\n  --logging_steps $LOGGING_STEPS \\\n  --seed $SEED \\\n  --do_train \\\n  --do_eval \\\n  --do_predict \\\n  --overwrite_output_dir\n","841683eb":"test_predictions = pd.read_table('indobert-ner\/test_predictions.txt')\nprint(test_predictions)","5f8e6a22":"test_processed_df['POI\/street']=''\nprintcounter = 0\nfor i in tqdm(range(0, len(test_processed_df))):\n    row_start = test_processed_df.loc[i, 'start']\n    row_end = test_processed_df.loc[i, 'end']\n    test_processed_df.loc[i, 'POI\/street'] = extract_word('indobert-ner\/test_predictions.txt', row_start, row_end)\n    # add checkpoints\n    if (printcounter == 1000):\n        test_processed_df[['id','POI\/street']].to_csv('submit.csv', index=False)\n        printcounter = 0\n        printcounter += 1\ntest_processed_df.head()","2bfc233f":"# Preprocess data","ccd9c8d4":"def extract_word(file, row_start, row_end):\n    '''\n    format model prediction output in submission format\n    '''\n    poi = ''\n    st = ''\n    f=open(file)\n    lines = f.readlines()\n    i = row_start\n    while i >= row_start and i <= row_end:\n        if len(lines[i]) > 1 :\n            word = lines[i].split()[0]  \n            tag = lines[i].split()[1]\n        else:\n            tag = 'O'\n        if tag == 'POI':\n            poi = poi + ' ' + word\n        elif tag == 'ST':\n            st = st + ' ' + word\n        i += 1\n    return poi.strip() + '\/' + st.strip()","412020b5":"test_predictions = pd.read_table('indobert-ner\/test_predictions.txt')\nprint(test_predictions)","fe3be62d":"# Data","cdf3ef69":"Add data from:\nhttps:\/\/www.kaggle.com\/numerator\/scl2021data","6051185d":"# Process output to submission format","aa2554ae":"test_predictions = pd.read_table('indobert-ner\/test_predictions.txt')\nprint(test_predictions)","af499155":"# Fine-tuning Model","cac39d9b":"# # Run run_ner_indobert.ipynb\nCredits:\nCleaned train and test data from:\nhttps:\/\/www.kaggle.com\/zeyalt\/scl-2021-data-science-part-1-data-cleaning\/comments?select=cleaned_test.csv"}}