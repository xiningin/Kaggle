{"cell_type":{"ecfb5dad":"code","7498e554":"code","43885790":"code","a4682b96":"code","2782759a":"code","bd3e3667":"markdown","0df100d9":"markdown"},"source":{"ecfb5dad":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler \nfrom sklearn.compose import ColumnTransformer,make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom lightgbm import LGBMClassifier\nfrom category_encoders import OneHotEncoder\nfrom sklearn.model_selection import cross_val_predict\nfrom warnings import filterwarnings\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfilterwarnings('ignore')","7498e554":"train = pd.read_csv(\"..\/input\/train_2.csv\") \ntest = pd.read_csv(\"..\/input\/test_2.csv\") \ntrain_input = train.drop(['id','target','B_15'],axis = 1)\ntrain_labels = train['target']\napp_train = pd.get_dummies(train_input)\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp_mean.fit(app_train)\ntrain_imputed = imp_mean.transform(app_train)\nscaler = StandardScaler()\nscaler.fit(train_imputed)\ntrain_imputed = scaler.transform(train_imputed)\nfeatures = list(app_train.columns)\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)\nrandom_forest.fit(train_imputed,train_labels)\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance':feature_importance_values})","43885790":"def plot_feature_importances(df):\n    #Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    #Normalise the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    \n    #Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10,6))\n    ax = plt.subplot()\n    \n    #Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))),\n           df['importance_normalized'].head(15),\n           align = 'center', edgecolor = 'k')\n    #Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    #Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importance')\n    plt.show()\n    \n    return df\nfeature_importances_sorted = plot_feature_importances(feature_importances)","a4682b96":"train = pd.read_csv(\"..\/input\/train_2.csv\")\ntrain=train[['target','B_15','B_10','B_3','B_12','B_8','B_7','B_4','D_121','D_26','D_17','B_11','D_56','D_138','D_1','D_40','D_166',\n            'C_10','D_102','D_132','D_99','C_14','C_3','C_2','D_13','D_34','D_66','D_2','D_142','D_143','D_21','D_156','D_158','D_37','B_9',\n            'D_14','C_12','D_28','D_6','D_29','D_54','D_117','C_5','D_86','D_107','D_30']]\ntest = pd.read_csv(\"..\/input\/test_2.csv\")\ntest=test[['B_15','B_10','B_3','B_12','B_8','B_7','B_4','D_121','D_26','D_17','B_11','D_56','D_138','D_1','D_40','D_166',\n            'C_10','D_102','D_132','D_99','C_14','C_3','C_2','D_13','D_34','D_66','D_2','D_142','D_143','D_21','D_156','D_158','D_37','B_9',\n            'D_14','C_12','D_28','D_6','D_29','D_54','D_117','C_5','D_86','D_107','D_30']]\ntarget_column = \"target\"\nid_column = \"id\"\ncategorical_cols = [c for c in test.columns if test[c].dtype in [np.object]]\nnumerical_cols = [c for c in test.columns if test[c].dtype in [np.float, np.int] and c not in [target_column, id_column]]\npreprocess = make_column_transformer(\n    (numerical_cols, make_pipeline(SimpleImputer(), StandardScaler())),\n    (categorical_cols, OneHotEncoder()))\nclassifier = make_pipeline(preprocess,LGBMClassifier(n_jobs=-1,eta=0.01,max_depth=4))\n\noof_pred = cross_val_predict(classifier, \n                             train, \n                             train[target_column], \n                             cv=5,\n                             method=\"predict_proba\")\n                  \nprint(\"LGBMClassifier Cross validation AUC {:.4f}\".format(roc_auc_score(train[target_column], oof_pred[:,1])))","2782759a":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nclassifier.fit(train, train[target_column])\ntest_preds = classifier.predict_proba(test)[:,1]\nsub[target_column] = test_preds\nsub.to_csv(\"submissionWith42Predictors.csv\", index=False)","bd3e3667":"**Credit**: Many functions were adapted from https:\/\/www.kaggle.com\/paweljankiewicz\/lightgbm-with-sklearn-pipelines.\n\nMade by https:\/\/www.kaggle.com\/hassamraja, https:\/\/www.kaggle.com\/therealrainier, and https:\/\/www.kaggle.com\/paultimothymooney during the 4\/11\/2019 KaggleDaysSF Hackathon.","0df100d9":"KaggleDaysSF Hackathon 17th Place Solution\n\nLGBMClassifier with SimpleImputer, StandardScaler, OneHotEncoder, and 42 hand-picked features."}}