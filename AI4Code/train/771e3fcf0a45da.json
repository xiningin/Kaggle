{"cell_type":{"1b9a270e":"code","e41e63f2":"code","78e597a3":"code","48177114":"code","e1a23cf5":"code","08862e67":"code","15d9c9a6":"code","6e07c1b5":"code","a90422c1":"code","29b42683":"code","9ef5456f":"code","54c1db6b":"code","becd82fa":"code","23ba6cc3":"code","63f88f74":"code","cfb5fdfb":"code","6f1bcb96":"code","526d7ee6":"code","514c27b0":"code","4013ea94":"markdown","dbacd915":"markdown","6008f98d":"markdown","f51d615b":"markdown","e5f83d07":"markdown","87b41157":"markdown","5788e3fd":"markdown","585d9dba":"markdown","3850b4bd":"markdown","53993aea":"markdown","dfe66dcc":"markdown","b2dd4131":"markdown","fb58811f":"markdown","91ae069e":"markdown","ac747a0f":"markdown","9c232491":"markdown"},"source":{"1b9a270e":"!nvidia-smi","e41e63f2":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport gc\npd.set_option('display.max_columns', None)\nnp.seterr(divide='ignore', invalid='ignore')\ngc.enable()\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n# NLP\nfrom transformers import AutoTokenizer, AutoModel\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')","78e597a3":"data_dir = '..\/input\/jrstc-train-folds'\ntrain_file_path = os.path.join(data_dir, 'validation_data_5_folds.csv')\nprint(f'Train file: {train_file_path}')","48177114":"train_df = pd.read_csv(train_file_path)","e1a23cf5":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","08862e67":"tqdm.pandas()\ntrain_df['less_toxic'] = train_df['less_toxic'].progress_apply(text_cleaning)\ntrain_df['more_toxic'] = train_df['more_toxic'].progress_apply(text_cleaning)","15d9c9a6":"train_df.sample(10)","6e07c1b5":"train_df.groupby(['kfold']).size()","a90422c1":"params = {\n    'device': device,\n    'debug': False,\n    'checkpoint': 'roberta-base',\n    'output_logits': 768,\n    'max_len': 256,\n    'num_folds': train_df['kfold'].nunique(),\n    'batch_size': 16,\n    'dropout': 0.2,\n    'num_workers': 2,\n    'epochs': 3,\n    'lr': 2e-5,\n    'margin': 0.7,\n    'scheduler_name': 'OneCycleLR',\n    'max_lr': 5e-5,                 # OneCycleLR\n    'pct_start': 0.1,               # OneCycleLR\n    'anneal_strategy': 'cos',       # OneCycleLR\n    'div_factor': 1e3,              # OneCycleLR\n    'final_div_factor': 1e3,        # OneCycleLR\n    'no_decay': True\n}","29b42683":"if params['debug']:\n    train_df = train_df.sample(frac=0.01)\n    print('Reduced training Data Size for Debugging purposes')","9ef5456f":"class BERTDataset:\n    def __init__(self, more_toxic, less_toxic, max_len=params['max_len'], checkpoint=params['checkpoint']):\n        self.more_toxic = more_toxic\n        self.less_toxic = less_toxic\n        self.max_len = max_len\n        self.checkpoint = checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.num_examples = len(self.more_toxic)\n\n    def __len__(self):\n        return self.num_examples\n\n    def __getitem__(self, idx):\n        more_toxic = str(self.more_toxic[idx])\n        less_toxic = str(self.less_toxic[idx])\n\n        tokenized_more_toxic = self.tokenizer(\n            more_toxic,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        tokenized_less_toxic = self.tokenizer(\n            less_toxic,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        ids_more_toxic = tokenized_more_toxic['input_ids']\n        mask_more_toxic = tokenized_more_toxic['attention_mask']\n        token_type_ids_more_toxic = tokenized_more_toxic['token_type_ids']\n\n        ids_less_toxic = tokenized_less_toxic['input_ids']\n        mask_less_toxic = tokenized_less_toxic['attention_mask']\n        token_type_ids_less_toxic = tokenized_less_toxic['token_type_ids']\n\n        return {'ids_more_toxic': torch.tensor(ids_more_toxic, dtype=torch.long),\n                'mask_more_toxic': torch.tensor(mask_more_toxic, dtype=torch.long),\n                'token_type_ids_more_toxic': torch.tensor(token_type_ids_more_toxic, dtype=torch.long),\n                'ids_less_toxic': torch.tensor(ids_less_toxic, dtype=torch.long),\n                'mask_less_toxic': torch.tensor(mask_less_toxic, dtype=torch.long),\n                'token_type_ids_less_toxic': torch.tensor(token_type_ids_less_toxic, dtype=torch.long),\n                'target': torch.tensor(1, dtype=torch.float)}","54c1db6b":"def get_scheduler(optimizer, scheduler_params=params):\n    if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n        scheduler = CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=scheduler_params['T_0'],\n            eta_min=scheduler_params['min_lr'],\n            last_epoch=-1\n        )\n    elif scheduler_params['scheduler_name'] == 'OneCycleLR':\n        scheduler = OneCycleLR(\n            optimizer,\n            max_lr=scheduler_params['max_lr'],\n            steps_per_epoch=int(df_train.shape[0] \/ params['batch_size']) + 1,\n            epochs=scheduler_params['epochs'],\n            pct_start=scheduler_params['pct_start'],\n            anneal_strategy=scheduler_params['anneal_strategy'],\n            div_factor=scheduler_params['div_factor'],\n            final_div_factor=scheduler_params['final_div_factor'],\n        )\n    return scheduler","becd82fa":"class MetricMonitor:\n    def __init__(self, float_precision=4):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )","23ba6cc3":"class ToxicityModel(nn.Module):\n    def __init__(self, checkpoint=params['checkpoint'], params=params):\n        super(ToxicityModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(params['output_logits'])\n        self.dropout = nn.Dropout(params['dropout'])\n        self.dense = nn.Sequential(\n            nn.Linear(params['output_logits'], 128),\n            nn.SiLU(),\n            nn.Dropout(params['dropout']),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds","63f88f74":"def train_fn(train_loader, model, criterion, optimizer, epoch, params, scheduler=None):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(train_loader)\n    \n    for i, batch in enumerate(stream, start=1):\n        ids_more_toxic = batch['ids_more_toxic'].to(device)\n        mask_more_toxic = batch['mask_more_toxic'].to(device)\n        token_type_ids_more_toxic = batch['token_type_ids_more_toxic'].to(device)\n        ids_less_toxic = batch['ids_less_toxic'].to(device)\n        mask_less_toxic = batch['mask_less_toxic'].to(device)\n        token_type_ids_less_toxic = batch['token_type_ids_less_toxic'].to(device)\n        target = batch['target'].to(device)\n\n        logits_more_toxic = model(ids_more_toxic, token_type_ids_more_toxic, mask_more_toxic)\n        logits_less_toxic = model(ids_less_toxic, token_type_ids_less_toxic, mask_less_toxic)\n        loss = criterion(logits_more_toxic, logits_less_toxic, target)\n        metric_monitor.update('Loss', loss.item())\n        loss.backward()\n        optimizer.step()\n            \n        if scheduler is not None:\n            scheduler.step()\n        \n        optimizer.zero_grad()\n        stream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")","cfb5fdfb":"def validate_fn(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(val_loader)\n    all_loss = []\n    with torch.no_grad():\n        for i, batch in enumerate(stream, start=1):\n            ids_more_toxic = batch['ids_more_toxic'].to(device)\n            mask_more_toxic = batch['mask_more_toxic'].to(device)\n            token_type_ids_more_toxic = batch['token_type_ids_more_toxic'].to(device)\n            ids_less_toxic = batch['ids_less_toxic'].to(device)\n            mask_less_toxic = batch['mask_less_toxic'].to(device)\n            token_type_ids_less_toxic = batch['token_type_ids_less_toxic'].to(device)\n            target = batch['target'].to(device)\n\n            logits_more_toxic = model(ids_more_toxic, token_type_ids_more_toxic, mask_more_toxic)\n            logits_less_toxic = model(ids_less_toxic, token_type_ids_less_toxic, mask_less_toxic)\n            loss = criterion(logits_more_toxic, logits_less_toxic, target)\n            all_loss.append(loss.item())\n            metric_monitor.update('Loss', loss.item())\n            stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor}\")\n            \n    return np.mean(all_loss)","6f1bcb96":"best_models_of_each_fold = []","526d7ee6":"gc.collect()\nfor fold in range(params['num_folds']):\n    print(f'******************** Training Fold: {fold+1} ********************')\n    current_fold = fold\n    df_train = train_df[train_df['kfold'] != current_fold].copy()\n    df_valid = train_df[train_df['kfold'] == current_fold].copy()\n\n    train_dataset = BERTDataset(\n        df_train.more_toxic.values,\n        df_train.less_toxic.values\n    )\n    valid_dataset = BERTDataset(\n        df_valid.more_toxic.values,\n        df_valid.less_toxic.values\n    )\n\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=params['batch_size'], shuffle=True,\n        num_workers=params['num_workers'], pin_memory=True\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset, batch_size=params['batch_size']*2, shuffle=False,\n        num_workers=params['num_workers'], pin_memory=True\n    )\n    \n    model = ToxicityModel()\n    model = model.to(params['device'])\n    criterion = nn.MarginRankingLoss(margin=params['margin'])\n    if params['no_decay']:\n        param_optimizer = list(model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=params['lr'])\n    else:\n        optimizer = optim.AdamW(model.parameters(), lr=params['lr'])\n    scheduler = get_scheduler(optimizer)\n\n    # Training and Validation Loop\n    best_loss = np.inf\n    best_epoch = 0\n    best_model_name = None\n    for epoch in range(1, params['epochs'] + 1):\n        train_fn(train_dataloader, model, criterion, optimizer, epoch, params, scheduler)\n        valid_loss = validate_fn(valid_dataloader, model, criterion, epoch, params)\n        if valid_loss <= best_loss:\n            best_loss = valid_loss\n            best_epoch = epoch\n            if best_model_name is not None:\n                os.remove(best_model_name)\n            torch.save(model.state_dict(), f\"{params['checkpoint']}_{epoch}_epoch_f{fold+1}.pth\")\n            best_model_name = f\"{params['checkpoint']}_{epoch}_epoch_f{fold+1}.pth\"\n\n    # Print summary of this fold\n    print('')\n    print(f'The best LOSS: {best_loss} for fold {fold+1} was achieved on epoch: {best_epoch}.')\n    print(f'The Best saved model is: {best_model_name}')\n    best_models_of_each_fold.append(best_model_name)\n    del df_train, df_valid, train_dataset, valid_dataset, train_dataloader, valid_dataloader, model\n    _ = gc.collect()\n    torch.cuda.empty_cache()","514c27b0":"for i, name in enumerate(best_models_of_each_fold):\n    print(f'Best model of fold {i+1}: {name}')","4013ea94":"# CFG","dbacd915":"# About This Notebook:-\n<ul style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li>This notebook tried to demonstrate the use of Transfer learning using the Huggingface and Pytorch library.<\/li>\n<li>We use a vanilla <b>roberta-base<\/b> transformer model for extracting language embeddings and pass them through a dense head to find the rankings.<\/li>\n<li>We use <a href='https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.MarginRankingLoss.html'><b>MarginRankingLoss<\/b><\/a> as our loss function.<\/li>\n<li>This notebook only covers the training part. Inference can be found in the notebook link below.<\/li>\n<\/ul>\n\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>Inference Notebook:- <a href='https:\/\/www.kaggle.com\/manabendrarout\/jrstc-pytorch-roberta-ranking-baseline-infer'><b>https:\/\/www.kaggle.com\/manabendrarout\/jrstc-pytorch-roberta-ranking-baseline-infer<\/b><\/a><\/p>","6008f98d":"# Scheduler","f51d615b":"# Imports","e5f83d07":"# Training And Validation Loops","87b41157":"# Get GPU Info","5788e3fd":"## 2. Validate Function","585d9dba":"# NLP Model","3850b4bd":"# Metrics","53993aea":"# Problem Statement\n<ul style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>\n<li>Build a model that produces scores that rank each pair of comments the same way as the professional raters in the training dataset.<\/li>\n<\/ul>\n\n<h2 style='font-family: Segoe UI; font-weight: 400;'>Why this competition?<\/h2>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>As evident from the problem statement, this competition presents an unique challenge for a greater purpose. Online bullying has become a epidemic with the boom in connectivity.<br>Hopefully the solutions contribute towards controlling this behaviour so that the internet remains a safe place for everyone.<\/p>\n\n<h2 style='font-family: Segoe UI; font-weight: 400;'>Expected Outcome<\/h2>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>In this competition we will be ranking comments in order of severity of toxicity.<br>We are given a list of comments, and each comment should be scored according to their relative toxicity. Comments with a higher degree of toxicity should receive a higher numerical value compared to comments with a lower degree of toxicity.<\/p>\n\n<h2 style='font-family: Segoe UI; font-weight: 400;'>Data Description<\/h2>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>There is no training data for this competition. We can refer to previous Jigsaw competitions for data that might be useful to train models.<br>However, we are provided a set of paired toxicity rankings(as per expert raters) that can be used to validate models.<\/p>\n\n<h2 style='font-family: Segoe UI; font-weight: 400;'>Grading Metric<\/h2>\n<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>Submissions are evaluated on <b>Average Agreement<\/b> with Annotators.<br>\nFor the ground truth, annotators were shown two comments and asked to identify which of the two was more toxic. Pairs of comments can be, and often are, rated by more than one annotator, and may have been ordered differently by different annotators.<\/p>\n\n<p style='background:MediumSeaGreen; border:0; color: white; text-align: center; font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 24px'>If you found this notebook useful or use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share such public kernels.<br>Thanks! \ud83d\ude0a<\/p>","dfe66dcc":"# Text Cleaning","b2dd4131":"# Reading File","fb58811f":"# Run","91ae069e":"## 1. Train Function","ac747a0f":"<p style='font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 15px'>This is a simple starter kernel on implementation of Transfer Learning using Pytorch for this problem. Huggingface transformers library has many SOTA NLP models which you can try out using the guidelines in this notebook.<br>I hope you have learnt something from this notebook. I have created this notebook as a baseline model, which you can easily fork and paly-around with to get much better results. I might update parts of it down the line when I get more GPU hours and some interesting ideas.<\/p>\n\n<p style='background:MediumSeaGreen; border:0; color: white; text-align: center; font-family: Segoe UI; font-size: 1.5em; font-weight: 400; font-size: 24px'>If you found this notebook useful or use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share such public kernels.<br>Thanks and Happy Kaggling! \ud83d\ude0a<\/p>","9c232491":"# Dataset"}}