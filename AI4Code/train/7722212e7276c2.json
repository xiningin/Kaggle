{"cell_type":{"3659cc57":"code","4b26bdb9":"code","503c9829":"code","0a9f08f2":"code","b14506ca":"code","09b66974":"code","5072f813":"code","00405793":"code","0d97f094":"code","63463800":"code","40238c86":"code","1bdccb2d":"code","491e5f68":"code","eb990938":"code","4b6f74a7":"markdown","84fcbf95":"markdown","21f3b164":"markdown","5c62f882":"markdown"},"source":{"3659cc57":"#Importing library\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\nfrom keras.utils import *\nfrom keras.initializers import *\nimport tensorflow as tf\nimport time, random","4b26bdb9":"#Hyperparameters\nbatch_size = 64\nlatent_dim = 256\nnum_samples = 10000","503c9829":"#Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_chars = set()\ntarget_chars = set()\n\nwith open('..\/input\/frenchenglish-bilingual-pairs\/fra.txt', 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text = line.split('\\t')\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    for char in input_text:\n        if char not in input_chars:\n            input_chars.add(char)\n    for char in target_text:\n        if char not in target_chars:\n            target_chars.add(char)\n\ninput_chars = sorted(list(input_chars))\ntarget_chars = sorted(list(target_chars))\nnum_encoder_tokens = len(input_chars)\nnum_decoder_tokens = len(target_chars)\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\n#Print size\nprint('Number of samples:', len(input_texts))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)","0a9f08f2":"#Define data for encoder and decoder\ninput_token_id = dict([(char, i) for i, char in enumerate(input_chars)])\ntarget_token_id = dict([(char, i) for i, char in enumerate(target_chars)])\n\nencoder_in_data = np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype='float32')\n\ndecoder_in_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n\ndecoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_in_data[i, t, input_token_id[char]] = 1.\n    for t, char in enumerate(target_text):\n        decoder_in_data[i, t, target_token_id[char]] = 1.\n        if t > 0:\n            decoder_target_data[i, t - 1, target_token_id[char]] = 1.\n","b14506ca":"#Define and process the input sequence\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n#We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n#Using `encoder_states` set up the decoder as initial state.\ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","09b66974":"#Final model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","5072f813":"model.summary()","00405793":"#Model data Shape\nprint(\"encoder_in_data shape:\",encoder_in_data.shape)\nprint(\"decoder_in_data shape:\",decoder_in_data.shape)\nprint(\"decoder_target_data shape:\",decoder_target_data.shape)","0d97f094":"#Visuaize the model\nplot_model(model,show_shapes=True)","63463800":"from keras.optimizers import Adam","40238c86":"#Compiling and training the model\nmodel.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n\nmodel.fit([encoder_in_data, decoder_in_data], decoder_target_data, batch_size = batch_size, epochs=50, validation_split=0.2)","1bdccb2d":"#Define sampling models\nencoder_model = Model(encoder_inputs, encoder_states)\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)","491e5f68":"reverse_input_char_index = dict((i, char) for char, i in input_token_id.items())\nreverse_target_char_index = dict((i, char) for char, i in target_token_id.items())\n\n#Define Decode Sequence\ndef decode_sequence(input_seq):\n    #Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    #Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    #Get the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_id['\\t']] = 1.\n\n    #Sampling loop for a batch of sequences\n    #(to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        #Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        #Exit condition: either hit max length\n        #or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        #Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        #Update states\n        states_value = [h, c]\n\n    return decoded_sentence","eb990938":"for seq_index in range(10):\n    input_seq = encoder_in_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)","4b6f74a7":"# Sequence-to-Sequence Modeling using LSTM for Language Translation","84fcbf95":"Here,we will implement deep learning in the Sequence-to-Sequence (Seq2Seq) modelling for language translation. This approach will be applied to convert the short English sentences into the corresponding French sentences. The LSTM encoder and decoder are used to process the sequence to sequence modelling in this task. ","21f3b164":"The below part of codes will define the decode sequence for the text that we will pass to the model as the input sequence.","5c62f882":"Finally, we will check our model to decode the input sequence into the target sequence, i.e., translate the English sentences into the French sentences."}}