{"cell_type":{"a0245784":"code","3a0bbdca":"code","cf08c079":"code","be8ab654":"code","60df7ebd":"code","ac83fc26":"code","abf8e544":"code","9636a908":"markdown","fc699a1f":"markdown","d059f1c9":"markdown","402047db":"markdown","0501364c":"markdown","0b4181e3":"markdown"},"source":{"a0245784":"# necessary imports\nimport tensorflow as tf\nfrom tensorboard.plugins.hparams import api as hp\nfrom sklearn.model_selection import ParameterGrid\nimport os\n%load_ext tensorboard","3a0bbdca":"fashion_mnist = tf.keras.datasets.fashion_mnist\n\n(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0","cf08c079":"def logger_and_tuner(build_model, fit_fn, hparams, logdir, chkptdir, additional_cbs=None):\n    \"\"\"\n    Args:\n        build_model: function that takes hparams as arg and returns a keras model,\n        fit_fn: function that takes model and callbacks as arg and trains the model,\n        hparams: dictionary of hyperparameters,\n        logdir: directory for logging,\n        chkptdir: directory for saving checkpoints,\n        additional_cbs: list of additional callbacks to be added\n    \"\"\"\n    hp_list = list(ParameterGrid(hparams)) # makes a list of dictionary containing possible combinations\n    \n    session_num = 1 \n    print('TOTAL COMBINATIONS: %d' % len(hp_list))\n    if not os.path.isdir(chkptdir):\n        os.makedirs(chkptdir)\n\n    for params in hp_list:\n        model = build_model(params) # build model with desired hparams\n        run_name = \"run-%d\" % session_num\n        print('--- Starting trial: %s' % run_name)\n        print(params)\n\n        # these are directory name; change to your needs\n        MNAME = \":\".join([model.name, run_name])\n        tb_logdir = logdir + '\/' + MNAME\n        checkpoint_filepath =  chkptdir + '\/' + MNAME + \"_weights-improvement-{epoch:03d}-{val_accuracy:.4f}.hdf5\"\n        \n        # saves model according to condition; change if necessary\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n          verbose=1,\n          filepath=checkpoint_filepath,\n          save_weights_only=True,\n          monitor='val_accuracy',\n          mode='max',\n          save_best_only=True)\n\n        # if you decide to remove checkpoint callbacks, remove it from here as well\n        callback_list = [\n            tf.keras.callbacks.TensorBoard(tb_logdir),\n            hp.KerasCallback(tb_logdir, params),\n            model_checkpoint_callback\n        ]\n        # handles extra callbacks\n        if additional_cbs is not None and type(additional_cbs) == list:\n          callback_list.extend(additional_cbs)\n\n        # fit the model\n        fit_fn(model, callback_list)\n        session_num += 1","be8ab654":"# define hparam space\nHPARAMS = [{\n    'num_layers': [1],\n    'num_units_1': [32],\n    'dropout': [0.1, 0.2],\n    'optimizer': ['adam', 'sgd']\n},\n{\n    'num_layers': [2],\n    'num_units_1': [32],\n    'num_units_2': [16, 32],\n    'dropout': [0.1, 0.2],\n    'optimizer': ['adam', 'sgd']\n}]\n\n# build your model with hparams\ndef build_model(hparams):\n  model = tf.keras.models.Sequential(name='super')\n  model.add(tf.keras.layers.Flatten())\n  for num in range(1, hparams['num_layers'] + 1):\n    model.add(tf.keras.layers.Dense(hparams['num_units_' + str(num)], activation='relu'))\n  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\n  model.compile(\n      optimizer=hparams['optimizer'],\n      loss='sparse_categorical_crossentropy',\n      metrics=['accuracy'],\n  )\n  return model\n\n# write the fit function; you can add training settings to it if you are using data generators\ndef fit_function(model, callbacks):\n  model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=EPOCHS, callbacks=callbacks)","60df7ebd":"def scheduler(epoch):\n  return 0.0001 * tf.math.exp(0.01 * (10 - epoch))\n\ncallback = tf.keras.callbacks.LearningRateScheduler(scheduler)","ac83fc26":"EPOCHS = 5\nlogger_and_tuner(build_model, fit_function, hparams=HPARAMS, logdir='logs', chkptdir='weights', additional_cbs=[callback])","abf8e544":"# %tensorboard --logdir logs","9636a908":"## User Code\n\nContains three parts:\n1. Hyperparameter space definition\n2. Build model function\n3. Fit model function","fc699a1f":"# Keras Logger and hyperparameter tuner\n\nLogging and tuning can sometimes be a little complicated when working with keras, tensorflow and tensorboard. This notebook contains a drag and drop tuner that takes care of the basic Keras logging and hp tuning with Tensorboard. You only need to write a small amount of code to get it running on your machine.\n\nCarry on below where I have explained it with the help of an example.","d059f1c9":"## Load the example dataset","402047db":"## Visualize using Tensorboard\n\nYou can explore the results and then choose the best weights as well. Happy training!","0501364c":"## The logger and tuner\n\nPlease note: you can change code according to your needs. Go crazy!","0b4181e3":"## Adding another callback and calling the tuner"}}