{"cell_type":{"a947a58d":"code","44ac0462":"code","a2ee013f":"code","f97ba881":"code","568c5b57":"code","8b3656dc":"code","94c9e51d":"code","aa9cf2b3":"code","5255bc24":"code","67921b8e":"code","0f5410e2":"code","00cd645d":"code","c727541b":"code","da197f34":"code","82934ccd":"code","b04459a1":"code","e1252aa5":"code","291ee783":"code","50ebcd47":"code","c202d93c":"code","ec006bfd":"code","ebca4e4c":"code","dc4acf9b":"code","bfe6653c":"code","f55cef4c":"code","e63ea3eb":"code","31affad8":"code","da9ff1dc":"code","0fbd1458":"markdown","309c1bf9":"markdown","21c8f66a":"markdown","a072665c":"markdown","0c6ff9f0":"markdown"},"source":{"a947a58d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nfrom sklearn.preprocessing import scale \nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import tree\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom scipy.stats import shapiro\nfrom sklearn import preprocessing\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44ac0462":"train = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv\")","a2ee013f":"train.drop(\"id\", axis = 1, inplace = True)","f97ba881":"test.drop(\"id\", axis = 1, inplace = True)","568c5b57":"train","8b3656dc":"def Missing_Values(data):\n    variable_name=[]\n    total_value=[]\n    total_missing_value=[]\n    missing_value_rate=[]\n    unique_value_list=[]\n    total_unique_value=[]\n    data_type=[]\n    for col in data.columns:\n        variable_name.append(col)\n        data_type.append(data[col].dtype)\n        total_value.append(data[col].shape[0])\n        total_missing_value.append(data[col].isnull().sum())\n        missing_value_rate.append(round(data[col].isnull().sum()\/data[col].shape[0],3))\n        unique_value_list.append(data[col].unique())\n        total_unique_value.append(len(data[col].unique()))\n    missing_data=pd.DataFrame({\"Variable\":variable_name,\"Total_Value\":total_value,\\\n                             \"Total_Missing_Value\":total_missing_value,\"Missing_Value_Rate\":missing_value_rate,\n                             \"Data_Type\":data_type,\"Unique_Value\":unique_value_list,\\\n                               \"Total_Unique_Value\":total_unique_value})\n    return missing_data.sort_values(\"Missing_Value_Rate\",ascending=False)","94c9e51d":"Missing_Values(train)","aa9cf2b3":"train[\"Gender\"] = train[\"Gender\"].replace(\"Male\", 0)\ntrain[\"Gender\"] = train[\"Gender\"].replace(\"Female\", 1)\n\n\n\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].replace(\"No\", 0)\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].replace(\"Yes\", 1)\n\n\n\n","5255bc24":"# Male = 0\n# Female = 1\n\n# Vehicle_Damage Yes = 1\n# Vehicle_Damage No = 0\ntrain","67921b8e":"train.corr()","0f5410e2":"train.shape","00cd645d":"train_dummies = pd.get_dummies(train)","c727541b":"train_dummies","da197f34":"train_dummies.groupby(\"Response\").count()","82934ccd":"ratio = 46710 \/ 334399\nratio","b04459a1":"train_dummies.rename(columns={\"Vehicle_Age_1-2 Year\": \"Vehicle_Age_1_2 Year\", \n                              \"Vehicle_Age_< 1 Year\": \"Vehicle_Age_0_1_Year\", \"Vehicle_Age_> 2 Years\":\"Vehicle_Age_2_inf\"},inplace = True)","e1252aa5":"X = train_dummies.drop(\"Response\", axis = 1)\ny = train_dummies[\"Response\"]","291ee783":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","50ebcd47":"def plot_confusion_matrix(y_real, y_pred):\n    cm = confusion_matrix(y_real, y_pred)\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')","c202d93c":"X_train","ec006bfd":"# 1 : Customer is interested, 0 : Customer is not interested\nXGB = XGBClassifier(scale_pos_weight=1)\nXGB_model = XGB.fit(X_train, y_train)\ny_pred = XGB_model.predict(X_test)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","ebca4e4c":"y_probs = XGB_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.5, 1, 0)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","dc4acf9b":"Importance=pd.DataFrame({\"Importance\":XGB_model.feature_importances_*100},\n                       index=X_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\nplt.xlabel(\"Feature Importance\")\n\n\u0131mportance_sort_values = Importance.sort_values(\"Importance\", ascending = False)\n#\u0131mportance_sort_values[\u0131mportance_sort_values['Importance']>1]\n\u0131mportance_sort_values","bfe6653c":"XGB = XGBClassifier(scale_pos_weight=3, learning_rate = 0.1, n_estimators = 100,  max_depth = 5,  min_child_weight= 7, colsample_bytree = 0.5)\nXGB_model = XGB.fit(X_train, y_train)\ny_pred = XGB_model.predict(X_test)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","f55cef4c":"y_probs = XGB_model.predict_proba(X_test)[:,1] # This will give you positive class prediction probabilities  \ny_pred = np.where(y_probs > 0.5, 1, 0)\n\nxgb_roc_auc_score = roc_auc_score(y_test, y_pred)\nac = accuracy_score(y_test, y_pred)\nclass_reportt = classification_report(y_test, y_pred)\n\nprint(\"AC: {}\\nROC AUC: {}\\nCLASS REPORT:\\n {}\".format(ac,xgb_roc_auc_score,class_reportt))","e63ea3eb":"Importance=pd.DataFrame({\"Importance\":XGB_model.feature_importances_*100},\n                       index=X_train.columns)\nImportance.sort_values(by=\"Importance\",\n                      axis=0,ascending=True).plot(kind=\"barh\",color=\"r\")\nplt.xlabel(\"Feature Importance\")\n\n\u0131mportance_sort_values = Importance.sort_values(\"Importance\", ascending = False)\n#\u0131mportance_sort_values[\u0131mportance_sort_values['Importance']>1]\n\u0131mportance_sort_values","31affad8":"def plot_roc_curve(fpr, tpr,model_name):\n    plt.figure(figsize=(5,5))\n    plt.title(model_name)\n    plt.plot(fpr,tpr, label = roc_auc_score) \n    plt.plot([0,1],ls='--')\n    plt.plot([0,0],[1,0],c='.5')\n    plt.plot([1,1],c='.5')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","da9ff1dc":"fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\nplot_roc_curve(fpr, tpr, \"XGB  ROC CURVE\")","0fbd1458":"As a result  roc curve is 0.76 \n\nf1 score is 0.47 for 1. I think it could be better.\n\n\nThis is my first kernel. What do you think about importance and Roc Curve(did not satisfy me) waiting for your comment.","309c1bf9":"The important thing is to catch customers who are not interested so estimating 0s is more important for us.\n","21c8f66a":"I fit the model with the best parameters:","a072665c":"# XGB","0c6ff9f0":"\nLet's check the Importance of values "}}