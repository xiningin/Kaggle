{"cell_type":{"500264a5":"code","6d94c9b5":"code","7b8f4171":"code","42076f43":"code","4527dbef":"code","66ace796":"code","b716cbb9":"code","5ee79cf3":"code","733f6f9b":"code","61033d8a":"code","d3991d85":"code","071d6645":"code","92a69b6d":"code","ab687a5b":"code","8866812b":"code","272ead17":"code","f5a919b1":"code","bfd9e5de":"code","f00e12aa":"code","f147e86f":"code","1fdce1fe":"code","002a4989":"code","c8c25df5":"code","a7c2c8fe":"code","af62510d":"code","6e943a9f":"code","5fc56577":"code","513eb461":"code","92533706":"code","34e727d9":"markdown","eb61ce74":"markdown","a51cc017":"markdown","33d0a7f4":"markdown","764b3d7b":"markdown","51eac934":"markdown","94c02f6c":"markdown","89a56961":"markdown","576118dd":"markdown","8c523ec5":"markdown","cecc0506":"markdown","9a8286b7":"markdown"},"source":{"500264a5":"!pip install kaggle-environments\n\nimport os\nimport time\nimport numpy as np\nimport time\nimport random\nfrom collections import defaultdict\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model,models,layers\n\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.halite.helpers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\npd.set_option('display.max_columns', None)\nnp.set_printoptions(precision=2)","6d94c9b5":"BOARD_SIZE = 21\nb2 = BOARD_SIZE\/\/2\nN_ACTIONS = 5\nN_AGENTS = 4\nFEATURE_SIZE = 1+3*N_AGENTS\nINIT_ENV = \"1yard1ship\"\n\n# you can experiment with these\nDISCOUNT = 0.90\nHNORM = 1000\nIDLE_COST = 2\nN_RTG = 6   # reward-to-go steps\n\nSAVE_DIR = \"\/checkpoints\/\"\n\nship_action_encode = {0:ShipAction.NORTH,\n                 1:ShipAction.SOUTH,\n                 2:ShipAction.WEST,\n                 3:ShipAction.EAST,\n                 4:None,\n                 5:ShipAction.CONVERT} # convert is not used in this notebook\n\nshipyard_action_encode = {0:None,\n                          1:ShipyardAction.SPAWN}","7b8f4171":"def initializeEnv(option=None,ff=0):\n    \"\"\"\n    creates a new environment\n    \n    option: init option, (e.g. 1yard1ship, converts first ship to shipyard and spawn a ship)\n    \n    ff: fast forward, waits ff steps before doing initialization moves\n    \"\"\"\n    env = make(\"halite\", configuration={\"size\": BOARD_SIZE,\"episodeSteps\":400})\n    env.reset(N_AGENTS)    \n\n    for i in range(ff): # fast forward\n      _ = env.step(None)\n\n    if option == \"1yard1ship\":   \n\n      board = Board(env.state[0].observation, env.configuration) \n      for player in board.players.values():     \n        player.ships[0].next_action = ShipAction.CONVERT\n      state = env.step([player.next_actions for player in board.players.values()])[0]\n      \n      board = Board(env.state[0].observation, env.configuration) \n      for player in board.players.values():     \n        player.shipyards[0].next_action = ShipyardAction.SPAWN\n      state = env.step([player.next_actions for player in board.players.values()])[0]\n\n    return env","42076f43":"def getObs(board,entity):\n    \"\"\"returns observation of an entity (ship or shipyard) of a player\n    (BOARD_SIZE, BOARD_SIZE, FEATURE_SIZE)\n    \n    currently features:\n    \n    -halite on cells\n    \n    for each player (starting from the player the entity belong to)\n        -player ships\n        -players ships halite in cargo\n        -player shipyards\n    \n    halite values are divided by HNORM constant\n    \"\"\"\n    x,y = coordTransform(entity.position)\n\n    b2 = BOARD_SIZE\/\/2\n    layers =[]\n\n    halites = np.reshape(board.observation[\"halite\"],(BOARD_SIZE,BOARD_SIZE))\n    halites = np.roll(halites,(b2-x,b2-y),axis=(0,1)) \n    layers.append(halites\/HNORM)\n\n    for player in [board.current_player]+list(board.opponents):\n\n      array_layer = np.zeros((BOARD_SIZE,BOARD_SIZE))\n      array_layer2 = np.zeros((BOARD_SIZE,BOARD_SIZE))\n      for ship in player.ships:\n          i,j = coordTransform(ship.position)\n          array_layer[i,j] = 1\n          array_layer2[i,j] = ship.halite\/HNORM\n      array_layer = np.roll(array_layer,(b2-x,b2-y),axis=(0,1)) \n      array_layer2 = np.roll(array_layer2,(b2-x,b2-y),axis=(0,1))\n      layers.append(array_layer)   \n      layers.append(array_layer2)   \n\n      array_layer = np.zeros((BOARD_SIZE,BOARD_SIZE))\n      for shipyard in player.shipyards:\n          i,j = coordTransform(shipyard.position)\n          array_layer[i,j] = 1\n      array_layer = np.roll(array_layer,(b2-x,b2-y),axis=(0,1)) \n      layers.append(array_layer)    \n    \n    layers = tf.convert_to_tensor(layers,dtype=tf.float32)\n    layers = tf.transpose(layers, [1, 2, 0])\n    return layers\n\ndef coordTransform(coords):\n  \"\"\" change coordinates returned by (entity).position method to numpy coordinates\n  \"\"\"\n  x,y = coords\n  x_new = BOARD_SIZE-1-y\n  y_new = x\n  return x_new,y_new  ","4527dbef":"# exploration policies\ndef Boltzmann(model,X,T):\n  if T == None:\n    T = 0\n  probs = tf.math.exp(probs\/T)\/tf.reduce_sum(tf.math.exp(probs\/T))\n  action = np.random.choice(N_ACTIONS, p=np.squeeze(probs))\n  return action\n\ndef EGreedy(model,X,epsilon):\n  if epsilon==None:\n    epsilon = 0\n  if np.random.rand()<epsilon:\n    action = np.random.randint(model.output.shape[1])\n  else:\n    probs = model(X)\n    action = tf.argmax(probs,axis=1)\n  return action\n\nclass DoubleDQN:\n  def __init__(self,model,target_update_method=\"periodic\",\n                          tau=0.999,\n                          exploration=\"egreedy\"):\n    self.Qnet = model\n    self.target_update_method = target_update_method\n    self.tau = tau\n    if exploration==\"egreedy\":\n      self.explorationPolicy = EGreedy\n    if exploration==\"boltzmann\":\n      self.explorationPolicy = Boltzmann\n\n  def compile(self,optimizer):\n\n    def masked_error(args):\n            y_true, y_pred, mask = args\n            loss = huber_loss(y_true*mask, y_pred*mask)\n            return loss\n\n    self.Qnet.compile(loss=\"mse\",optimizer=\"Adam\") # this is not important\n    self.targetNet = models.clone_model(self.Qnet)\n\n\n    # these couple of lines adapted from https:\/\/github.com\/keras-rl\/keras-rl\/blob\/master\/rl\/agents\/dqn.py\n    # Qnet predicts Q values for each action. Target Q values come from rewards of selected actions. \n    # So we need to mask losses for actions that are not selected.\n    # We add a mask layer on top of Q net\n    y_pred = self.Qnet.output\n    y_true = layers.Input(name='y_true', shape=(self.Qnet.output.shape[1],))\n    mask_layer = layers.Input(name='mask', shape=(self.Qnet.output.shape[1],))\n    loss_out = layers.Lambda(masked_error, output_shape=(1,), name='loss')([y_true, y_pred, mask_layer])\n    ins = [self.Qnet.input] if type(self.Qnet.input) is not list else self.Qnet.input\n    self.Qnet_trainable = Model(inputs=ins + [y_true, mask_layer], outputs=[loss_out])\n    masked_loss = lambda y_true, y_pred: y_pred\n    self.Qnet_trainable.compile(optimizer=optimizer, loss=[masked_loss])\n  \n\n  def copy_to_target(self):\n    self.targetNet = models.clone_model(self.Qnet)\n    for t, e in zip(self.targetNet.trainable_variables, self.Qnet.trainable_variables):\n              t.assign(e)\n\n  def load_weights(self, filepath):\n    self.Qnet.load_weights(filepath)\n    self.copy_to_target()\n\n  def save_weights(self, filepath, overwrite=True):\n    self.Qnet.save_weights(filepath, overwrite=overwrite)\n\n  def __call__(self,X,expl_param=None,model=\"Q\"):\n    if model==\"T\":\n      q_values = self.targetNet(X)\n      return q_values\n    if model==\"Q\":\n      actions = self.explorationPolicy(self.Qnet,X,expl_param)\n      return actions\n\n  def train_on_batch(self,X,targets,mask):\n    loss = self.Qnet_trainable.train_on_batch([X,targets,mask],[targets])\n    return loss\n\n  def update_target(self):\n    if self.target_update_method==\"polyak\":\n      for t, e in zip(self.targetNet.trainable_variables, self.Qnet.trainable_variables):\n        t.assign(t * self.tau + e * (1 - self.tau))\n\n    if self.target_update_method==\"periodic\":\n      self.copy_to_target()\n      \n              ","66ace796":"# neural network\n\nhuber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.NONE)\n\ngrid_input = layers.Input(shape=(BOARD_SIZE,BOARD_SIZE,FEATURE_SIZE)) #for features extracted from map\n\nconv1 = layers.Conv2D(16, (1, 1), activation=\"tanh\",\n                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.001, maxval=0.001))(grid_input)\n\nconv2 = layers.Conv2D(16, (3, 3), activation=\"tanh\",\n                     kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.001, maxval=0.001))(conv1)\n\nflat = layers.Flatten()(conv2)\n\n\nflat_input = layers.Input(shape=(1,)) # input for undeserved halite in cargo. Future global\/scalar values will append here\nmerged = layers.Concatenate(axis=1)([flat, flat_input])\n\nd1 = layers.Dense(32,activation=\"tanh\",\n                  kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.001, maxval=0.001))(merged)\n\nd2 = layers.Dense(N_ACTIONS,activation=\"tanh\",\n                  kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.001, maxval=0.001))(d1)\n\nqnet = models.Model(inputs=[grid_input, flat_input], outputs=d2)","b716cbb9":"dqn = DoubleDQN(qnet)\ndqn.compile(optimizer=\"Adam\")","5ee79cf3":"class data_collector:\n    def __init__(self,player_id,expl_param=1):\n        self.player_id = player_id\n        self.spawn_probability = 0.5\n        self.expl_param = expl_param\n        self.discount_factor = DISCOUNT\n\n        self.transitions = {}  \n        self.state0_ship_halite = {}\n        self.state0_ships_loc = {}\n\n        # to be reset after end of episodes\n        self.prev_und_gains = defaultdict(lambda: 0)\n        self.paths = defaultdict(list)\n\n    def process_0(self,observation,configuration):\n        # State 0\n        self.transitions = {}  \n        self.state0_ship_halite = {}  \n        self.state0_ships_loc = {}\n\n        board = Board(observation, configuration)\n        me = board.players[self.player_id]\n        for shipyard in me.shipyards:\n          \n          syard_obs = getObs(board,shipyard)\n          loaded_ship_nearby = ((syard_obs[:,:,1][b2-1,b2] + syard_obs[:,:,1][b2+1,b2] +\n                                syard_obs[:,:,1][b2,b2-1] + syard_obs[:,:,1][b2,b2+1]) > (10\/HNORM))\n                                # nearby: 1 manhattan distance\n          if not loaded_ship_nearby:\n            if np.random.rand()<self.spawn_probability:\n              shipyard.next_action = shipyard_action_encode[1]\n\n        for ship in me.ships:\n            ID = ship.id\n            self.transitions[ID] = []\n\n            self.state0_ships_loc[ID] = coordTransform(ship.position)\n            self.state0_ship_halite[ID] = ship.halite\n            ship_obs = getObs(board,ship)           # obs 0\n            ship_und_gain = self.prev_und_gains[ID]\n            self.transitions[ID].append(ship_obs)\n            self.transitions[ID].append(ship_und_gain)\n\n            ship_obs = tf.expand_dims(ship_obs, 0) # batch dimension \n            ship_und_gain = tf.expand_dims(ship_und_gain, 0)\n\n            action = dqn([ship_obs,ship_und_gain],self.expl_param)\n            ship.next_action = ship_action_encode[int(action)]\n            self.transitions[ID].append(int(action))\n        return me.next_actions\n\n    def process_1(self,observation,configuration):\n        # State 1\n        board = Board(observation, configuration)\n        me = board.players[self.player_id]  \n        state1_ships_loc = {ship.id: coordTransform(ship.position) for ship in me.ships}\n\n        ships_destroyed = np.setdiff1d(list(self.state0_ships_loc),list(state1_ships_loc))\n        team_killers = []\n        undeserved_gain = {}\n    \n        for ID in ships_destroyed:\n          ship_next_obs = tf.identity(self.transitions[ID][0])\n          ship_next_obs = ship_next_obs.numpy()\n          ship_next_obs[:,:,1][b2,b2]=0\n          ship_next_obs = tf.convert_to_tensor(ship_next_obs)\n          self.transitions[ID].append(ship_next_obs)\n\n          # calculate where the victim is destroyed \n          x,y = self.state0_ships_loc[ID] \n          last_action = (int(self.transitions[ID][2]) == np.arange(5)) * np.array([-1,1,-1,1,0]) # \n          crimeX,crimeY = x+np.sum(last_action[:2]),y+np.sum(last_action[2:5])\n\n          team_killerFound = False\n        \n          for IDsus in state1_ships_loc:                            # looking for guilty ship\n            # ( It would be much easier to do these checks with built-in methods.- cell.ships etc.)\n            if IDsus not in self.state0_ships_loc:                    # skip ships that spawned in state1\n              continue\n            if state1_ships_loc[IDsus] == (crimeX,crimeY):          # ally at crime location\n              if (self.transitions[IDsus][2]==4):                      #  ally was standing. No punishment    \n                undeserved_gain[IDsus] =  self.state0_ship_halite[ID] #  halite taken from victim         \n              else:\n                team_killers.append(IDsus)\n                team_killerFound = True\n                undeserved_gain[IDsus] =  self.state0_ship_halite[ID]\n            \n          if team_killerFound:\n            self.transitions[ID].append(self.prev_und_gains[ID])\n            reward = 0                      # no punishment\n            self.transitions[ID].append(reward)  \n            gamma = 0\n            self.transitions[ID].append(gamma)      \n\n          else: # killed by enemy | victim crashed suspect | new ship spawned on the location \n            self.transitions[ID].append(self.prev_und_gains[ID])\n            reward = (-500\/HNORM)           # punishing the victim\n            self.transitions[ID].append(reward)       \n            gamma = 0                       # this consequence only depend on last action (maybe not?)\n            self.transitions[ID].append(gamma)\n\n        # alive ships\n        for ID in list(state1_ships_loc):\n          if ID not in self.state0_ships_loc:       # skip ships that spawned in state1\n            state1_ships_loc.pop(ID)\n            continue\n          ship = [ship for ship in me.ships if ID==ship.id][0]\n          ship_next_obs = getObs(board,ship)  # obs 1\n          self.transitions[ID].append(ship_next_obs) \n\n          ship_gain = (ship.halite - self.state0_ship_halite[ID])\n          returned = int(ship_next_obs[:,:,3][b2,b2]) # if arrived on a friendly shipyard                  \n          idle = (self.transitions[ID][2] == 4) and (ship_gain==0)               # if stayed on a cell without halite\n          if ID not in undeserved_gain:\n            undeserved_gain[ID] = 0\n          self.prev_und_gains[ID] += undeserved_gain[ID]\n          self.transitions[ID].append(self.prev_und_gains[ID])\n          \n          reward = (ship_gain\n                    +2*returned*self.state0_ship_halite[ID]  # if returned, ship_gain + 2*(ships prev halite) = ships prev halite  \n                    -returned*(self.prev_und_gains[ID])                 \n                    -idle*IDLE_COST\n                    -(ID in team_killers)*500\n                    -undeserved_gain[ID])\/HNORM        # ship_gain and undeserved_gain will cancel out\n          \n          \n          self.transitions[ID].append(reward) \n          \n          if returned:\n            self.prev_und_gains[ID] = 0\n\n          gamma = 0 if ID in team_killers else self.discount_factor\n          self.transitions[ID].append(gamma)\n\n        for ID in self.transitions:\n          self.paths[ID].append(self.transitions[ID])    \n    \n","733f6f9b":"def reward_to_go(rewards, gammas, N = N_RTG):\n    \"\"\"\n    Future rewards affect current reward for N steps.\n    N = None: All future rewards \n    \"\"\"\n    rlen = len(rewards)\n    res = []\n    if N == None:\n      for i in range(rlen-1):\n          res.append(rewards[i] + np.sum(rewards[i+1:]*(gammas[i+1:]**np.arange(1,rlen-i,dtype=np.float32))))\n      res.append(rewards[-1])\n    else:\n      for i in range(rlen-1):\n          res.append(rewards[i] + np.sum(rewards[i+1:i+N]*(gammas[i+1:i+N]**np.arange(1,min(N,rlen-i),dtype=np.float32))))\n      res.append(rewards[-1])\n    return res","61033d8a":"def simulate(expl_param=0,initOption = INIT_ENV,steps=400,ff=0,rtg=False,spawn_probability=0.5):\n  \"\"\"  creates an env., runs an episode, returns rewards of each ship\n  \"\"\"\n  env = initializeEnv(option=initOption)\n  step = env.state[0][\"observation\"][\"step\"]\n  board = Board(env.state[0].observation, env.configuration)\n  agents = [data_collector(ID,expl_param) for ID in list(board.players)]\n\n  rewards = defaultdict(lambda: np.full([steps], np.nan))\n  gammas = defaultdict(list)\n  belongTo = defaultdict(set)\n  while not env.done:     \n      actions = []\n      for agent in agents:\n        actions.append(agent.process_0(env.state[0].observation, env.configuration))\n        \n      env.step(actions)\n    \n      for agent in agents:\n        agent.process_1(env.state[0].observation, env.configuration)\n        for ID in agent.transitions:\n          rewards[ID][step] = agent.transitions[ID][5]\n          gammas[ID].append(agent.transitions[ID][6])\n          belongTo[agent.player_id].add(ID)\n      step += 1\n  if rtg:\n    for ID in rewards:\n      rewards[ID][~np.isnan(rewards[ID])] = reward_to_go(rewards[ID][~np.isnan(rewards[ID])],gammas[ID],N=N_RTG)\n  env.render(mode=\"ipython\",width=800, height=600)\n  return rewards,belongTo","d3991d85":"def generateSamples(batch_size,expl_param):  \n  samples = []\n  env = initializeEnv(option=\"1yard1ship\")\n  board = Board(env.state[0].observation, env.configuration)\n  agents = [data_collector(ID,expl_param) for ID in list(board.players)]\n\n  while len(samples)<batch_size:\n    while not env.done:\n      actions = []\n      board = Board(env.state[0].observation, env.configuration) \n        \n      for agent in agents: # sample actions for each player\n        actions.append(agent.process_0(env.state[0].observation, env.configuration))\n\n      env.step(actions)    # update environment\n      \n      for agent in agents: # calculate rewards\n        agent.process_1(env.state[0].observation, env.configuration)\n\n    for agent in agents:    \n      # recalculating rewards: R + (gamma*future_R)\n      for p in agent.paths:\n        rewards = [t[5] for t in agent.paths[p]]\n        gammas = [t[6] for t in agent.paths[p]]\n        rewards = reward_to_go(rewards,gammas,N=N_RTG)\n        for i,t in enumerate(agent.paths[p]):\n          t[5] = rewards[i]\n          samples.append(t)    \n      agent.paths = defaultdict(list)\n      agent.prev_und_gains = defaultdict(lambda: 0)\n\n    env = initializeEnv(option=\"1yard1ship\")\n  return samples","071d6645":"max_size_memory = 300000 # depends on your memory limitation. \ninit_size_memory = 20000","92a69b6d":"memory = generateSamples(batch_size=init_size_memory,expl_param=1) # create random dataset","ab687a5b":"len(memory)","8866812b":"def mirrorAction(a,axis):\n  # n,s,w,e = 0,1,2,3\n  if a > 3:          # 4: \"None\"\n    new_a = a\n  else:\n    if axis==\"h\": # w<>e\n      new_a = 2*(a==3) + 3*(a==2) + 1*(a==1)\n\n    if axis==\"v\": # n<>s\n      new_a = 2*(a==2) + 3*(a==3) + (not a)\n  \n  return new_a\n\ndef augmentData(transitions):\n  \"\"\" \n  mirroring transitions to increase data\n  transition: (s,a,s',r,g)\n  (s,a,s')  will be mirrored\n  \"\"\"\n\n  length = len(transitions)\n  for i in range(length):\n    \n    t = transitions[i].copy()\n\n    t[0] = tf.reverse(t[0],(1,)) # horizontal (west,east change)\n    t[3] = tf.reverse(t[3],(1,))\n    t[2] = mirrorAction(t[2],\"h\")\n    transitions.append(t)\n\n    t = t.copy()\n    t[0] = tf.reverse(t[0],(0,)) # vertical (north,south change)\n    t[3] = tf.reverse(t[3],(0,))\n    t[2] = mirrorAction(t[2],\"v\")\n    transitions.append(t)\n\n    t = t.copy()\n    t[0] = tf.reverse(t[0],(1,)) # horizontal again \n    t[3] = tf.reverse(t[3],(1,))\n    t[2] = mirrorAction(t[2],\"h\")\n    transitions.append(t)","272ead17":"a_sample = memory[42]","f5a919b1":"# Check if layers (ships, halite, shipyards) moved correctly after action.\nprint(\"halites at state 0\")\nprint(a_sample[0][5:16,5:16,0])       # printing 5:15 because 21x21 is too big for output cell \nprint(\"Action: %s\"%ship_action_encode[a_sample[2]])\nprint(\"halites at state 1\")\nprint(a_sample[3][5:16,5:16,0])\nprint(\"Reward: %f\"%a_sample[5])","bfd9e5de":"a_sample = [a_sample]\naugmentData(a_sample)","f00e12aa":"len(a_sample) # 3 new samples created from 1","f147e86f":"# Checking the one that should also flip the action. e.g. Action NORTH should become SOUTH\nprint(a_sample[2][0][5:16,5:16,0]) # halites at state 0\nprint(\"Action: %s\"%ship_action_encode[a_sample[2][2]])\nprint(a_sample[2][3][5:16,5:16,0]) # halites at state 1\nprint(\"Reward: %f\"%a_sample[1][5])\n","1fdce1fe":"print(len(memory))\naugmentData(memory)\nprint(len(memory))","002a4989":"# arbitrary values\nn_iter = 10000\nbatch_size = 1000\nadd_to_memory_every = 100\nupdate_target_every = 200\nsave_model_every  = 400\n\nitr_last = 0\nlosses = []","c8c25df5":"start = time.time()\n# epsilon = 0.9 \nfor itr in range(itr_last,n_iter,1):\n  epsilon = 1-(0.3*itr\/n_iter)                          # decreasing epsilon gradually\n\n  samples = random.sample(memory, batch_size)\n  obs_batch, scalar_batch, actions_batch, next_obs_batch,next_scalar_batch,rewards_batch, gammas_batch = map(tf.stack,zip(*samples))\n\n  q_values_target = dqn([next_obs_batch,next_scalar_batch],model=\"T\") # next Q values from target network\n\n  mask = np.zeros(shape=q_values_target.shape)    \n  for i in range(mask.shape[0]):\n    mask[i][actions_batch[i]]=1 \n\n  rewards_batch = tf.cast(tf.expand_dims(rewards_batch,1),dtype=tf.float32) # fixing dimension\n  targets_batch =  rewards_batch + DISCOUNT * q_values_target\n  loss = dqn.train_on_batch([obs_batch,scalar_batch],targets_batch,mask)\n\n  # generating new samples\n  if itr%add_to_memory_every == 0:\n    new_samples = generateSamples(batch_size=batch_size,expl_param=epsilon)\n    augmentData(new_samples)\n    memory.extend(new_samples)\n    while len(memory) > max_size_memory:\n          memory.pop(0)\n\n  # updating target network\n  if dqn.target_update_method == \"periodic\":         \n    if itr%update_target_every==0:\n      dqn.update_target()\n  if dqn.target_update_method == \"polyak\":\n    dqn.update_target()   \n\n  losses.append(loss)\n  itr_last = itr\n\n  if itr%1000==0:\n    print(\"---- Iteration %d ----\"%itr)\n    print(\"Epsilon: %f\"%epsilon)\n    print(\"Loss: %f\"%loss)\n    print(time.time()-start)\n#   if itr%save_model_every == 0:\n#     print(\"Saving model\")\n#     dqn.save_weights(SAVE_DIR + 'dqn')\n    ","a7c2c8fe":"rewards, belongTo = simulate(expl_param=0.1)  # epsilon 0.1","af62510d":"# checking rewards of ships each step. \ndf = pd.DataFrame(rewards).transpose()\ndf = df.replace(np.nan, '', regex=True)","6e943a9f":"df.loc[belongTo[0]] # rewards of ships belong to 0: First player","5fc56577":"rewards, belongTo = simulate(expl_param=0) ","513eb461":"df = pd.DataFrame(rewards).transpose()\ndf = df.replace(np.nan, '', regex=True)","92533706":"df.loc[belongTo[1]] # Second player","34e727d9":"# Imports","eb61ce74":"If looks correct, apply it on all samples","a51cc017":"**Map Observations**    \n2-D arrays are formed for each feature on board, and stacked to be 3-D.\nE.g. features = {halite on cells, ally ships}, shape of observation: (21,21,2)\n\nObserver (ship or shipyard) is always at the center. If board size is 21, location of the observer is (10,10)\n","33d0a7f4":"  \ndata_collector samples actions from dqn and creates transitions. method \"process_0\" takes the former state, samples action, and stores information. \"process_1\" takes the latter state and information about previous state, calculates rewards and returns transitions.  \n\n# Reward\nShips reward for each step is sum of following:  \n  \nGain: Current halite - Previous halite in cargo  \nReturn: 2 times Halite in cargo, if returned to a shipyard  \nIdle: (-) constant, if ship did not move or collect  \nTeam killer: (-) 500 if ship destroyed an ally ship. (Detail in version 3)  \nUndeserved Gain: (-) Amount of halite taken from destroyed ally ship in previous step  \nUndeserved Return: (-) Amount of total halite taken from ally ships, if returned to a shipyard  \n  \n! some behaviours (e.g. destroying an enemy shipyard) is not rewarded.  \n\n**Reward-to-go**  \nNormally,  \nReward(t) = Reward(t) + [discount_factor^n * Reward(t+n) for n = 1,2,3...]\n\nA difference in our implementation is discount factor differs according to event. \nReturning halite to a shipyard requires many steps, so discount factor of this reward is high. Whereas crashing to an ally ship only depend on last action\/s.\n\nSo,   \nReward(t) = Reward(t) + [discount_factor(reward_type(t+n))^n * Reward(t+n) for n = 1,2,3...]","764b3d7b":"# Globals","51eac934":"Checking a sample","94c02f6c":"# Training","89a56961":"Notice the weight initialization of the layers. We want to keep the magnitudes of Q value predictions small relative to rewards, at the beginning. Since the model is not trained yet, those predictions are meaningless and should not dominate loss values.","576118dd":"# Increasing dataset size\nWe can create 4 transitions from 1 by rotating map observations and actions, vertically and horizontally.","8c523ec5":"# Double DQN  \nPaper: https:\/\/www.aaai.org\/ocs\/index.php\/AAAI\/AAAI16\/paper\/download\/12389\/11847","cecc0506":"**Dataset**  \n\nDuring training, we want to sample actions from new policy, to improve it further. But it is quite expensive to do it every iteration. We will create a dataset with random actions and add new samples every 'x' iteration. We will remove oldest samples when dataset reaches a maximum size we define.\n","9a8286b7":"In a previous version of this notebook (3) , we trained Double Q Network only for ships on a smaller (11x11), single player game. Here, we train on full size (21x21) with 4 players, all players are controlled with DQN. We need to change the implementation to collect samples from all players, hence this version.    \n  \nAt the moment, only ships are considered. Ships cannot convert to shipyards. So we initialize every player by making a shipyard.        \n  \nIn (11x11) single player version, we see that ships can avoid friendly ships, collect and return halite. Here, DQN is trained with same parameters (number of   iterations, neural network) except batch size is reduced (from 1600 to 512) to make it faster. Ships do not perform as good as in single player version, since game got more complicated and training is shorter (in amount, not time). But there is sign of learning.      \n\nThings left to do:     \n     \n* Decide how to include shipyards. (train separate network or one single network for both? If single, how to handle impossible actions? How to reward a ship after it is converted to a shipyard?)    \n    \n* Tuning reward function, neural net parameters, training parameters...    \n    \n* Train a lot    \n    "}}