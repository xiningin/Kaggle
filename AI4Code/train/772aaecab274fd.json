{"cell_type":{"f5b8e8a6":"code","f4f1fb21":"code","43379439":"code","3ea3459b":"code","48ebd3b5":"code","48cdbecd":"code","91401513":"code","f2a3ffec":"code","40b84d26":"code","affaa926":"code","293ce56b":"code","4f188723":"code","efb38e24":"code","4616bd23":"code","7b04e1d9":"code","fb100973":"code","1ea0322d":"code","3959525d":"code","85f403ba":"code","c1cba8cc":"code","4579f997":"code","8c28600a":"code","12f56d27":"code","c37294c0":"code","30a96819":"code","e98c8037":"code","8bb5b68b":"code","1f4fb7f8":"code","0919c355":"code","e3bcf5c1":"code","732998bb":"code","795e9995":"code","d524500a":"code","a77ed1cb":"code","fa079260":"code","ef4be7ec":"code","6761288b":"code","e99cf7e6":"code","a40540aa":"code","b6ed5e60":"code","f3330992":"code","8da6bd6e":"code","a9d018da":"code","61f879db":"code","eb336c14":"code","c3edc74e":"code","a6db22f2":"code","9fecdfd9":"code","ea87bfff":"code","2c97a544":"code","e8042fd2":"code","e4b308ab":"code","9718c215":"code","223635ab":"code","85b0f8d6":"code","b93e3783":"code","587af1da":"code","c34f987e":"code","86dc9bd5":"code","d5de1908":"code","38d18730":"code","f93e3687":"code","ecd892a1":"code","10bf31a8":"code","8e3ffdec":"code","e434cbb5":"code","71bd4124":"code","8820fa3a":"code","9c1769dc":"code","b173ad0a":"code","82daf36f":"code","c6245cfa":"code","b4e7dcf0":"code","8bda5fa6":"code","1c493600":"code","7b546c08":"code","2cc737fe":"code","9d81c2e8":"code","ebd76b5b":"code","2ce4820d":"code","ac25b3e9":"code","68489b78":"code","e78f0403":"code","d5388724":"code","c4a8d406":"code","45a43baf":"code","78c05816":"code","065bf14e":"code","4316e436":"code","73518c63":"code","8cb568b4":"code","4fc42d68":"code","2dfb78dd":"code","afbedcce":"code","876676d8":"code","bb2de1fa":"code","4cb3dc3b":"code","3826378c":"code","4c10437f":"code","b06fc6ab":"code","c9080f30":"code","d4e6db61":"code","d26fbd4b":"markdown","e8fe4823":"markdown","228cc416":"markdown","ea203327":"markdown","94e40264":"markdown","639d6c67":"markdown","40b3e9a5":"markdown","99689318":"markdown","0f83aa35":"markdown","b2f3a631":"markdown","ef09fac2":"markdown","c1534311":"markdown","23b725b1":"markdown","2fcb9f49":"markdown","3440aa2d":"markdown","0a4da972":"markdown","acd0f239":"markdown","a0402ffb":"markdown","25b8197a":"markdown","ef69564e":"markdown","07554a4a":"markdown","a04accea":"markdown","37773ef0":"markdown","00684939":"markdown","c0aeda0c":"markdown","65602645":"markdown","264020ff":"markdown","57187868":"markdown","b967aa0e":"markdown","87cff0e8":"markdown","e6f810b9":"markdown","9c8faf2f":"markdown","47915251":"markdown","9799d389":"markdown","38e03fdb":"markdown","5c228e80":"markdown","3f2ee0db":"markdown","ba964c04":"markdown","edbab29f":"markdown","5b4788ba":"markdown","408407ba":"markdown","dfac1115":"markdown","272d6875":"markdown","f9d0b4c4":"markdown","48fcc6fc":"markdown","3ef986a1":"markdown","e43b1f5a":"markdown","03b4aa7d":"markdown","543131af":"markdown","b0e54855":"markdown"},"source":{"f5b8e8a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)h\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4f1fb21":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config Completer.use_jedi = False\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.monospace'] = 'Ubunto Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (16,10)\nimport random\nrandom.seed(21)\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)","43379439":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\ndf","3ea3459b":"df.shape","48ebd3b5":"df.nunique()","48cdbecd":"df.info()","91401513":"## Checking the NULL values in the dataset\ndf.isna().sum()","f2a3ffec":"## Statistics of the dataset\ndf.describe()","40b84d26":"## Correlation plot\ncorr = df.corr()\nplt.figure(figsize=(10,10))\nmask = np.zeros_like(corr,dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,mask=mask,annot=True)\nplt.show()","affaa926":"## Pairplot\nsns.pairplot(df,diag_kind='kde',hue='output')","293ce56b":"df['output'].value_counts()","4f188723":"## Functions for creating barplot and for adding labels on the respective bars\ndef addlabels(x,y,df):\n    for i in range(len(x)):\n        plt.text(i,y[i]+5,str(np.round((y[i]\/df.shape[0])*100,1)) + \"%\")\ndef barplot(df,feature):\n    plt.figure(figsize=(8,4))\n    y = df[feature].value_counts().sort_index(ascending=True).values\n    x = list(df[feature].value_counts().sort_index(ascending=True).index)\n    addlabels(x,y,df)\n    sns.countplot(df[feature])","efb38e24":"barplot(df,'output')","4616bd23":"df['age']","7b04e1d9":"plt.figure(figsize=(10,6))\nsns.boxplot(x = \"output\",y = \"age\",data = df)","fb100973":"## Total unique values with their frequencies\ndf['age'].value_counts()","1ea0322d":"age = df['age'].value_counts().sort_index(ascending=True)","3959525d":"sns.countplot(x = df['age'])","85f403ba":"## Countplot of the feature \"age\" with respect to output.\nsns.countplot(x = df['age'],hue=df['output'])","c1cba8cc":"## Age distribution of can Heart attack\nplt.figure(figsize = (15,5))\nplt.plot(age)\nplt.xlabel(\"Age of the person\")\nplt.ylabel(\"Frequency count of ages\")\nplt.title(\"Age distribution of Heart attack dataset\")\nplt.show()","4579f997":"age_0 = df[df['output'] == 0]['age'].value_counts().sort_index(ascending=True)\nage_1 = df[df['output'] == 1]['age'].value_counts().sort_index(ascending=True)","8c28600a":"plt.figure(figsize = (15,5))\nplt.plot(age_0,c='c',label=\"didn't get heart attack\")\nplt.plot(age_1,c='r',label='get heart attack')\nplt.xlabel(\"Age of the person\")\nplt.ylabel(\"Frequency count of ages\")\nplt.title(\"Age distribution of Heart attack dataset\")\nplt.legend()\nplt.show()","12f56d27":"df['sex']","c37294c0":"df['sex'].value_counts()","30a96819":"barplot(df,'sex')","e98c8037":"sns.catplot(x='output',y='age',hue='sex',data=df,kind='bar')","8bb5b68b":"df['cp']","1f4fb7f8":"df['cp'].value_counts()","0919c355":"barplot(df,'cp')","e3bcf5c1":"sns.catplot(x='cp',y='age',hue='output',data=df,kind='bar')","732998bb":"df['trtbps']","795e9995":"df['trtbps'].value_counts()","d524500a":"sns.distplot(df['trtbps'])","a77ed1cb":"sns.lmplot(x='age',y='trtbps',hue='output',data=df)","fa079260":"sns.jointplot(x='age',y='trtbps',kind='reg',data=df)","ef4be7ec":"df['chol']","6761288b":"sns.distplot(df['chol'])","e99cf7e6":"sns.scatterplot(x='age',y='chol',hue='output',data=df)","a40540aa":"sns.jointplot(x='age',y='chol',kind='reg',data=df)","b6ed5e60":"df['fbs']","f3330992":"barplot(df,'fbs')","8da6bd6e":"barplot(df,'restecg')","a9d018da":"df['restecg'].value_counts()","61f879db":"df['restecg'].replace([2],[1],inplace=True)","eb336c14":"barplot(df,'restecg')","c3edc74e":"sns.distplot(df['thalachh'])","a6db22f2":"sns.scatterplot(x='thalachh',y='age',hue='output',data=df)","9fecdfd9":"sns.lmplot(y='age',x='thalachh',hue='output',data=df)","ea87bfff":"sns.lmplot(y='chol',x='thalachh',hue='output',data=df)","2c97a544":"sns.lmplot(y='trtbps',x='thalachh',hue='output',data=df)","e8042fd2":"df['exng'].value_counts()","e4b308ab":"barplot(df,'exng')","9718c215":"sns.distplot(df['oldpeak'])","223635ab":"sns.scatterplot(y='oldpeak',x='age',hue='output',data=df)","85b0f8d6":"sns.lmplot(y='age',x='oldpeak',hue='output',data=df)","b93e3783":"sns.lmplot(y='trtbps',x='oldpeak',hue='output',data=df)","587af1da":"sns.lmplot(y='chol',x='oldpeak',hue='output',data=df)","c34f987e":"sns.lmplot(y='thalachh',x='oldpeak',hue='output',data=df)","86dc9bd5":"barplot(df,'slp')","d5de1908":"barplot(df,'caa')","38d18730":"df['caa'].value_counts()","f93e3687":"df['caa'].replace([4],[0],inplace=True)","ecd892a1":"barplot(df,'caa')","10bf31a8":"barplot(df,'thall')","8e3ffdec":"df['thall'].value_counts()","e434cbb5":"df['thall'].replace([0],[2],inplace=True)","71bd4124":"barplot(df,'thall')","8820fa3a":"df.info()","9c1769dc":"X = df.drop(['output'],axis=1)\ny = df['output']\nprint(X.shape,y.shape)","b173ad0a":"## Splitting the dataset into train and test set\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,y,train_size=0.75,random_state=42)\nx_train = pd.DataFrame(x_train,columns = df.drop(['output'],axis=1).columns)\nx_train.index = range(x_train.shape[0])\nx_test = pd.DataFrame(x_test,columns = df.drop(['output'],axis=1).columns)\nx_test.index = range(x_test.shape[0])\nprint(x_train.shape,y_train.shape)\nprint(x_test.shape,y_test.shape)","82daf36f":"num_features = ['age','trtbps','chol','thalachh','oldpeak']\ncat_features = ['sex','cp','fbs','restecg','exng','slp','caa','thall']","c6245cfa":"from sklearn.preprocessing import MinMaxScaler\nscaler =  MinMaxScaler()\nscaler.fit(x_train[num_features])\nx_train_scaled = pd.DataFrame(scaler.transform(x_train[num_features]),columns=num_features)\nx_test_scaled = pd.DataFrame(scaler.transform(x_test[num_features]),columns=num_features)\n","b4e7dcf0":"x_train_scaled","8bda5fa6":"x_test_scaled","1c493600":"x_train_scaled.shape","7b546c08":"x_train_scaled[cat_features] = x_train[cat_features]\nx_test_scaled[cat_features] = x_test[cat_features]","2cc737fe":"x_train_scaled","9d81c2e8":"x_test_scaled","ebd76b5b":"x_train_scaled = x_train_scaled.values\nx_test_scaled = x_test_scaled.values","2ce4820d":"from sklearn.feature_selection import SelectPercentile\nselect = SelectPercentile(percentile=60)\nselect.fit(x_train_scaled, y_train)\n# transform training set\nx_train_selected = select.transform(x_train_scaled)\nprint(\"x_train.shape: {}\".format(x_train.shape))\nprint(\"x_train_selected.shape: {}\".format(x_train_selected.shape))","ac25b3e9":"mask = select.get_support()\nprint(mask)\nplt.matshow(mask.reshape(1,-1),cmap='gray_r')\nplt.xlabel('Sample Index')","68489b78":"## Preparing the train and test set with the selected features only.\nx_train_scaled = select.transform(x_train_scaled)\nx_test_scaled = select.transform(x_test_scaled)","e78f0403":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x_train_scaled,y_train)\nprint(knn.score(x_train_scaled,y_train))\nprint(knn.score(x_test_scaled,y_test))","d5388724":"from sklearn.neighbors import KNeighborsClassifier\ntraining_accuracy=[]\ntesting_accuracy=[]\nneighbors_settings=range(1,11)\nfor n_neighbors in neighbors_settings:\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors,n_jobs=-1)\n    clf.fit(x_train_scaled,y_train)\n    training_accuracy.append(clf.score(x_train_scaled,y_train))\n    testing_accuracy.append(clf.score(x_test_scaled,y_test))\n    print(\"For k = {:.0f} : Training accuracy {:.3f} and Testing accuracy {:.3f}\".format(n_neighbors,clf.score(x_train_scaled,y_train),clf.score(x_test_scaled,y_test)))\nplt.plot(neighbors_settings,training_accuracy,label='Training Accuracy')\nplt.plot(neighbors_settings,testing_accuracy,label='Testing Accuracy')\nplt.xlabel(\"n_neighbors\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n","c4a8d406":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=10)\nlogreg.fit(x_train_scaled,y_train)\nprint(\"Trainig Score:\",logreg.score(x_train_scaled,y_train))\nprint(\"Testing Score:\",logreg.score(x_test_scaled,y_test))","45a43baf":"from sklearn.svm import LinearSVC\nlinearsvm = LinearSVC().fit(x_train_scaled,y_train)\nprint(\"Coefficient :\",linearsvm.coef_)\nprint(\"Intercept :\",linearsvm.intercept_)\nprint(\"Trainig Score:\",linearsvm.score(x_train_scaled,y_train))\nprint(\"Testing Score:\",linearsvm.score(x_test_scaled,y_test))","78c05816":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=4)\ntree.fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(tree.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(tree.score(x_test_scaled,y_test)))","065bf14e":"## Performing cross validation for hyperparamter tunning of DecisionTreeClassifier model.\nfrom sklearn.model_selection import GridSearchCV\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split' : [2,3,4,5],\n    'min_samples_leaf' :[2,3,4,5]\n}\ntree_cv = GridSearchCV(estimator= DecisionTreeClassifier(),\n                             param_grid = param_test,scoring='accuracy',\n                             n_jobs=-1,cv=5)\ntree_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(tree_cv.best_params_, tree_cv.best_score_)\nprint(\"Training Accuracy {:.3f}\".format(tree_cv.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(tree_cv.score(x_test_scaled,y_test)))","4316e436":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier().fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(clf.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(clf.score(x_test_scaled,y_test)))","73518c63":"## Performing cross validation for hyperparamter tunning of MLPClassifier model.\nparam_test = {\n    'activation' : ['logistic', 'tanh', 'relu'],\n    'alpha' : [0.01,0.1,0.5,1],\n    'learning_rate' : ['adaptive']\n    \n}\nmlp_cv = GridSearchCV(estimator= MLPClassifier(),\n                             param_grid = param_test,scoring='accuracy',\n                             n_jobs=-1,cv=5)\nmlp_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(mlp_cv.best_params_, mlp_cv.best_score_)\nprint(\"Training Accuracy {:.3f}\".format(mlp_cv.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(mlp_cv.score(x_test_scaled,y_test)))","8cb568b4":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=500,max_depth=4).fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(rf.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(rf.score(x_test_scaled,y_test)))","4fc42d68":"## Performing cross validation for hyperparamter tunning of Random Forest Classifier model.\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split' : [2,3,4,5],\n    'min_samples_leaf' :[2,3,4,5]\n}\nrf_cv = GridSearchCV(estimator= RandomForestClassifier(n_estimators=500),\n                             param_grid = param_test,scoring='accuracy',\n                             n_jobs=-1,cv=5)\nrf_cv.fit(x_train_scaled,y_train)\n## This best score is the mean of five cross validation folds with the best hyperparamters. \nprint(rf_cv.best_params_, rf_cv.best_score_)\nprint(\"Training Accuracy {:.3f}\".format(rf_cv.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(rf_cv.score(x_test_scaled,y_test)))","2dfb78dd":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier()\ngbrt.fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(gbrt.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(gbrt.score(x_test_scaled,y_test)))","afbedcce":"## Performing cross validation for hyperparamter tunning of GradientBoostingClassifier model.\nparam_test = {\n    'max_depth' : [2,3,4,5,6,7],\n    'min_samples_split': np.arange(2, 12, 3),\n    'min_samples_leaf': np.arange(1, 10, 3)\n}\ngbrt_cv = GridSearchCV(estimator= GradientBoostingClassifier(n_estimators=1000),\n                             param_grid = param_test,scoring='accuracy',\n                             n_jobs=-1,cv=5)\ngbrt_cv.fit(x_train_scaled,y_train)\nprint(gbrt_cv.best_params_, gbrt_cv.best_score_)\nprint(\"Training Accuracy {:.3f}\".format(gbrt_cv.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(gbrt_cv.score(x_test_scaled,y_test)))","876676d8":"from xgboost import XGBClassifier\nxgb = XGBClassifier(max_depth=1)\nxgb.fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(xgb.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(xgb.score(x_test_scaled,y_test)))","bb2de1fa":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=1000,learning_rate=0.05)\nada.fit(x_train_scaled,y_train)\nprint(\"Training Accuracy {:.3f}\".format(ada.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(ada.score(x_test_scaled,y_test)))","4cb3dc3b":"import lightgbm\nfrom lightgbm import LGBMClassifier\nlgbm = LGBMClassifier(max_depth=1,learning_rate=0.1,reg_alpha=0.05,reg_lambda=0.01)\nlgbm.fit(x_train_scaled, y_train)\nprint(\"Training Accuracy {:.3f}\".format(lgbm.score(x_train_scaled,y_train)))\nprint(\"Testing Accuracy {:.3f}\".format(lgbm.score(x_test_scaled,y_test)))","3826378c":"import keras\nimport tensorflow as tf","4c10437f":"def make_model():\n    model = keras.Sequential()\n    model.add(tf.keras.layers.Dense(128,activation='tanh',input_shape=(x_train_scaled.shape[1],)))\n    model.add(tf.keras.layers.Dense(64,activation='tanh'))\n    #model.add(tf.keras.layers.Dense(32,activation='tanh'))\n    model.add(tf.keras.layers.Dense(16,activation='tanh'))\n    model.add(tf.keras.layers.Dense(4,activation='tanh'))\n    model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='Adam',metrics=['accuracy'])\n    return model\n    ","b06fc6ab":"model = make_model()\nmodel.summary()","c9080f30":"model.fit(x_train_scaled,y_train,validation_data=(x_test_scaled,y_test),epochs=20,batch_size=32,\n          callbacks=[\n              tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=5),\n              tf.keras.callbacks.ModelCheckpoint('model_{val_accuracy:.3f}.h5',\n                                                save_best_only=True,save_weights_only=False,mode='auto',\n                                                monitor='val_accuracy')\n              \n    \n])","d4e6db61":"print(\"The score of KNN with k = 9: {:.3f}\".format(testing_accuracy[8]))\nprint(\"The score of Logistic Regression : {:.3f}\".format(logreg.score(x_test_scaled,y_test)))\nprint(\"The score of Linear SVC : {:.3f}\".format(linearsvm.score(x_test_scaled,y_test)))\nprint(\"The score of Decision Tree Classifier : {:.3f}\".format(tree_cv.score(x_test_scaled,y_test)))\nprint(\"The score of MLP Classifier : {:.3f}\".format(mlp_cv.score(x_test_scaled,y_test)))\nprint(\"The score of Random Forest Classifier : {:.3f}\".format(rf_cv.score(x_test_scaled,y_test)))\nprint(\"The score of Gradient Boosting Classifier : {:.3f}\".format(gbrt_cv.score(x_test_scaled,y_test)))\nprint(\"The score of XGB Classifier : {:.3f}\".format(xgb.score(x_test_scaled,y_test)))\nprint(\"The score of AdaBoost Classifier : {:.3f}\".format(ada.score(x_test_scaled,y_test)))\nprint(\"The score of LighGBM Classifier : {:.3f}\".format(lgbm.score(x_test_scaled,y_test)))\nprint(\"The score of ANN Classifier : \",(model.evaluate(x_test_scaled,y_test)))","d26fbd4b":"### Feature \"Sex\"","e8fe4823":"### Random Forest Classifier","228cc416":"### Linear SVC","ea203327":"### Barplot of Feature \"Output\"","94e40264":"### Final Scores of every model","639d6c67":"### MLP Classifier","40b3e9a5":"### Feature \"fbs\"","99689318":"Positively skewed.","0f83aa35":"From the above plot we can conclude that the output is moderately correlated with features like 'cp','thalachh','exng','oldpeak','slp','caa','thall'. Further in this kernel, we are going to analysis every feature with respect to output and other features.","b2f3a631":"### Feature \"chol\"","ef09fac2":"Very few instances having \"restecg\" value as 2 are present. They maybe because of error and therefore we are going to merge them into the majority category of value 1.","c1534311":"Fairly Normal distributed feature with some positive skewness.","23b725b1":"If we carefully look from the above output, then we'll get to know that all features in this dataset are numerical features with some discrete valued features and some continuous valued features.\n\nFeatures with discrete values are age, sex, cp, fbs, restecg, exng, slp, caa, thall, output and rest are numerical features with values that can be assumed as continuous.","2fcb9f49":"### Decision Tree Classifier","3440aa2d":"### XGB Classifier","0a4da972":"Hence, this dataset contains more instances of one gender than other.","acd0f239":"From the above plot, the optimal value of k is 9 with training accuracy 0.868 and testing accuracy 0.895","a0402ffb":"From the above plot, most of the persons in this dataset are having age in between 41 - 67 years.","25b8197a":"From here onwards, we are going to perform both univariate and multivariate analysis on every feature of the dataset in order to have insights above the data.","ef69564e":"# Task 1 - Importing Libraries and Dataset","07554a4a":"Since dataset is small and the outliers are very less in the feature \"age\" and not much extreme, so we need not to make changes.","a04accea":"### Logistic Regression","37773ef0":"Feature \"cp\" is distributed over 4 categories.","00684939":"### K-Nearest Neighbor","c0aeda0c":"### Feature \"caa\"","65602645":"Normal distributed to some extent with negative skewness.","264020ff":"### Feature \"thall\"","57187868":"### Feature \"oldpeak\"","b967aa0e":"### Automate Feature Selection ","87cff0e8":"### Dense layer based ANN Classifier","e6f810b9":"### Feature \"age\"","9c8faf2f":"### Feature \"trtbps\"","47915251":"There are 138 negative instances and 165 positive instances. Thus, this dataset is not imbalanced otherwise we have to make arrangements for handling the imbalanced dataset.","9799d389":"# Task 3 - Data preparation for model evaluation","38e03fdb":"Looks like persons having age 41,44,51,52,54 years, have more chances of heart attack than others.","5c228e80":"### Gradient Boosting Classifier","3f2ee0db":"Checking the number of unique values for each feature of dataset :","ba964c04":"### Feature \"Cp\"","edbab29f":"The above plot shows which features have been selected (in black).","5b4788ba":"Can be assumed as normaly distributed with positive skewness.","408407ba":"## End Notes\n\nMost of the model performed very well and similar as this dataset is not much complicated and most of the features are category based. The best testing accuracy in my case is with KNN and MLP Classifier.\n\nI would suggest more hypertunning and feature engineering can bring better results also.\n\nThis is the best intution I have. If anyone is having better approaches or better analysis, please let me know. Any kind of queries, improvements or feedbacks are most welcome.\n\nIf you like my work, please show your appreciation by upvoting the notebook. Thank you...............!!!!!!!","dfac1115":"### Feature \"thalachh\"","272d6875":"### LightGBM","f9d0b4c4":"### Feature \"exng\"","48fcc6fc":"# Task 2 - Exploratory Data Analysis (EDA) ","3ef986a1":"### Applying scaling to numerical features only.","e43b1f5a":"### Feature \"restecg\"","03b4aa7d":"# Task 4 - Model evaluation","543131af":"### AdaBoost Classifier","b0e54855":"### Feature \"slp\""}}