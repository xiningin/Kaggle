{"cell_type":{"c0354825":"code","d94e6b14":"code","1ddffc7a":"code","f1bf7c26":"code","8780bbc8":"code","d5270e4e":"code","eedc729d":"code","7921a23a":"code","1c66f98a":"code","fa13cae5":"code","54e96d84":"code","8ea5d6e8":"code","e44b9a64":"code","3e4bcee9":"code","c476f3ea":"code","13dad1e3":"code","6ae35815":"code","90be4d6c":"code","0de8d77b":"code","6723f8a3":"code","c78ae957":"code","4c489a46":"code","aaa6df00":"code","53040289":"code","67c7a3ee":"code","fc7add64":"code","fb3c0cd0":"code","810b2614":"markdown","455a5e93":"markdown","b33db6cf":"markdown","3372e66b":"markdown","a7e06373":"markdown","fae23d6d":"markdown","1d22d955":"markdown","35a2392d":"markdown","e8a5a4cc":"markdown","4c496f9a":"markdown","d06a112b":"markdown","ad75073d":"markdown","12197635":"markdown","eefb99a9":"markdown","169e6d7e":"markdown","10aa99f3":"markdown","0bf04f12":"markdown","65136c5d":"markdown","3b92d2ac":"markdown","ffbcc048":"markdown"},"source":{"c0354825":"import pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport tensorflow as tf \nimport re \nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\nimport seaborn as sns \nplt.style.use('ggplot')","d94e6b14":"fake_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","1ddffc7a":"fake_df.isnull().sum()","f1bf7c26":"real_df.isnull().sum()","8780bbc8":"fake_df.subject.unique()","d5270e4e":"real_df.subject.unique()","eedc729d":"fake_df.drop(['date', 'subject'], axis=1, inplace=True)\nreal_df.drop(['date', 'subject'], axis=1, inplace=True)","7921a23a":"fake_df['class'] = 0 \nreal_df['class'] = 1","1c66f98a":"plt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df), color='orange')\nplt.bar('Real News', len(real_df), color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('# of News Articles', size=15)\n\n\ntotal_len = len(fake_df) + len(real_df)\nplt.figure(figsize=(10, 5))\nplt.bar('Fake News', len(fake_df) \/ total_len, color='orange')\nplt.bar('Real News', len(real_df) \/ total_len, color='green')\nplt.title('Distribution of Fake News and Real News', size=15)\nplt.xlabel('News Type', size=15)\nplt.ylabel('Proportion of News Articles', size=15)","fa13cae5":"print('Difference in news articles:',len(fake_df)-len(real_df))","54e96d84":"news_df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\nnews_df","8ea5d6e8":"news_df['text'] = news_df['title'] + news_df['text']\nnews_df.drop('title', axis=1, inplace=True)","e44b9a64":"features = news_df['text']\ntargets = news_df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.20, random_state=18)","3e4bcee9":"def normalize(data):\n    normalized = []\n    for i in data:\n        i = i.lower()\n        # get rid of urls\n        i = re.sub('https?:\/\/\\S+|www\\.\\S+', '', i)\n        # get rid of non words and extra spaces\n        i = re.sub('\\\\W', ' ', i)\n        i = re.sub('\\n', '', i)\n        i = re.sub(' +', ' ', i)\n        i = re.sub('^ ', '', i)\n        i = re.sub(' $', '', i)\n        normalized.append(i)\n    return normalized\n\nX_train = normalize(X_train)\nX_test = normalize(X_test)","c476f3ea":"max_vocab = 10000\ntokenizer = Tokenizer(num_words=max_vocab)\ntokenizer.fit_on_texts(X_train)","13dad1e3":"# tokenize the text into vectors \nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","6ae35815":"X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)","90be4d6c":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(max_vocab, 128),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.summary()","0de8d77b":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])","6723f8a3":"history_dict = history.history\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\nepochs = history.epoch\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Loss', size=20)\nplt.legend(prop={'size': 20})\nplt.show()\n\nplt.figure(figsize=(12,9))\nplt.plot(epochs, acc, 'g', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy', size=20)\nplt.xlabel('Epochs', size=20)\nplt.ylabel('Accuracy', size=20)\nplt.legend(prop={'size': 20})\nplt.ylim((0.5,1))\nplt.show()","c78ae957":"model.evaluate(X_test, y_test)","4c489a46":"pred = model.predict(X_test)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","aaa6df00":"print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test))\nprint('Precision on testing set:', precision_score(binary_predictions, y_test))\nprint('Recall on testing set:', recall_score(binary_predictions, y_test))","53040289":"matrix = confusion_matrix(binary_predictions, y_test, normalize='all')\nplt.figure(figsize=(16, 10))\nax= plt.subplot()\nsns.heatmap(matrix, annot=True, ax = ax)\n\n# labels, title and ticks\nax.set_xlabel('Predicted Labels', size=20)\nax.set_ylabel('True Labels', size=20)\nax.set_title('Confusion Matrix', size=20) \nax.xaxis.set_ticklabels([0,1], size=15)\nax.yaxis.set_ticklabels([0,1], size=15)","67c7a3ee":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape) # shape: (vocab_size, embedding_dim)","fc7add64":"word_index = list(tokenizer.word_index.keys())\nword_index = word_index[:max_vocab-1]","fb3c0cd0":"import io\n\nout_v = io.open('fakenews_vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('fakenews_meta.tsv', 'w', encoding='utf-8')\n\nfor num, word in enumerate(word_index):\n  vec = weights[num+1] # skip 0, it's padding.\n  out_m.write(word + \"\\n\")\n  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\nout_v.close()\nout_m.close()","810b2614":"Apply padding so we have the same length for each article ","455a5e93":"Normalizing our data: lower case, get rid of extra spaces, and url links. ","b33db6cf":"We are going to use early stop, which stops when the validation loss no longer improve.","3372e66b":"Confusion matrix ","a7e06373":"Combining the title with the text, it is much easier to process this way. ","fae23d6d":"Visualize our training over time ","1d22d955":"0 for fake news, and 1 for real news","35a2392d":"Read the data ","e8a5a4cc":"Check out the distribution of fake news compare to real news","4c496f9a":"Convert text to vectors, our classifier only takes numerical data. ","d06a112b":"Split into training and testing ","ad75073d":"Checking for unique values for subject. We want both data frames to have a similar distribution.","12197635":"Write to file so we can use tensorflow's embedding projector to visualize what our network learned. This is only based on the fake news dataset. ","eefb99a9":"Drop the date from the dataset, I don't think there is a strong correlation between date and validity of the news. As we see above, subjects are not distributed evenly. We do not want that to influence the accuracy of our classifier. Therefore, we need to drop that as well. ","169e6d7e":"Evaluate the testing set ","10aa99f3":"Building the RNN.","0bf04f12":"# Fake News Detection Using RNN \n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/2d\/Tensorflow_logo.svg\/115px-Tensorflow_logo.svg.png\"> \n\nThis notebook aims to classify fake news from real news using a recurrent neural network. To simplify the text preprocessing procedure, we will be using the built in functions from tensorflow instead of more established libraries like NLTK. \n\n**Here are the results: **\n\n* Accuracy on testing set: 0.9904231625835189\n* Precision on testing set: 0.9879573876794813\n* Recall on testing set: 0.9920930232558139\n\n*Free free to provide me with feedbacks.  \n\n**New update: confusion matrix is now expressed in terms of percentage rather than frequency.**\n\n\nImage Source: https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/2d\/Tensorflow_logo.svg\/115px-Tensorflow_logo.svg.png\nCode is on my Github: https:\/\/github.com\/therealcyberlord\n","65136c5d":"Embedding Projector: http:\/\/projector.tensorflow.org\/\n<br>Picture credits: https:\/\/www.tensorflow.org\/tensorboard\/images\/embedding_projector.png?raw=1\n\n![](https:\/\/www.tensorflow.org\/tensorboard\/images\/embedding_projector.png?raw=1)","3b92d2ac":"Checking for null values ","ffbcc048":"Saves the weights for visualiation"}}