{"cell_type":{"4c0ac9b1":"code","9ac7befc":"code","c127961c":"code","4a7999fd":"code","12de69d0":"code","189e545a":"code","dc57669e":"code","0c7f77d3":"code","dff68b2c":"code","4c9ee4f5":"code","941e6c4e":"code","46db51b3":"code","610f16d5":"code","7102c8be":"code","1b4e8053":"code","38dee8bd":"code","7c04898b":"code","e0547296":"code","290c4062":"code","ef9e48fd":"code","b187a3e3":"code","43b1acab":"code","a3f7900c":"code","0dea8d72":"code","d8009d68":"code","6861cffa":"code","40b4ac0f":"code","acf61616":"code","cfd75496":"code","969b5c5e":"code","73fd844a":"code","3605b1c5":"code","d1a16abb":"code","396f91f7":"code","613965f8":"code","65b65e48":"code","72c53465":"code","3b855524":"code","f6722204":"code","897a58e0":"code","9bfe8a3d":"code","50128644":"code","2946e6ef":"code","af0258fe":"code","21521f11":"code","008d9102":"code","1debc4ba":"code","6db68737":"code","d390995a":"code","02af4ae5":"code","d1e52b06":"code","9b8a4d9a":"code","9fd3e52c":"code","69c0f765":"code","6c0afc7a":"code","fe099d02":"markdown","c8e53d43":"markdown","7e0924cf":"markdown","f20987e4":"markdown","e4343b3f":"markdown","0939b734":"markdown","b0915306":"markdown","8c108356":"markdown","c8292bf9":"markdown","5144a276":"markdown","fd915d79":"markdown","440ab7a9":"markdown","7f2de585":"markdown","d142fa2e":"markdown","ec559edd":"markdown","7452cfc4":"markdown","29469d69":"markdown","c4201c25":"markdown","5c163bb2":"markdown","f1bc4730":"markdown","7442ef21":"markdown","2df3e8e0":"markdown","cf92ad15":"markdown","6944d4ce":"markdown","3317910a":"markdown","ec252eb9":"markdown","7f2115c2":"markdown","778c3bef":"markdown","0d1574d7":"markdown","18832ef8":"markdown","1816ae75":"markdown"},"source":{"4c0ac9b1":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nimport sklearn.ensemble  as ens\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom scipy.special import inv_boxcox\nfrom sklearn.compose import TransformedTargetRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom mlxtend.regressor import StackingCVRegressor\nsns.set()\n%matplotlib inline\nimport time\n\nvalidPercent = 0.1      \n\ndropLowImportantFeatures = False # drop low import feature (value ~ zero)\n\ndoesHyperparameter = False # test to find the best parameters\n\nremoveLowCorr2Price = False     # remove features if they have low correlation with Sale Price (<0.2) --> not good, don't apply\nlowCorrValue = 0.2\n\nremoveHighCorr = True     #remove 1 of 2 features if they have high correlation each other(>0.8) --> ok and apply\nhighCorrValue = 0.8\n\nremoveMissingValue = True #remmove feature if they have too many missing values (>80% values = NA)\nmissingValuePercent = 0.8\n\ndropOutlier = False      #drop outlier --> not good for this data, don't apply\ncoefSigma = 3 \ndfTest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndfTrainAll = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n                    \nprint(\"Train data: \"+str(dfTrainAll.shape))\nprint(\"Test data: \"+str(dfTest.shape))\nprint(set(dfTrainAll.columns.values) - set(dfTest.columns.values))\nprint(\"---*********************----\")\nprint(dfTrainAll.dtypes.value_counts())\ndfTrainAll.head()\n#print(dfTest.columns)\n","9ac7befc":"#split training data to training data and validate data\nfrom sklearn.model_selection import train_test_split\nif (validPercent>0):\n    dfTrain, dfValid  = train_test_split(dfTrainAll, test_size = validPercent, random_state = 100)\nelse:\n    dfTrain = dfTrainAll\n    dfValid = dfTrainAll.copy(deep=True)","c127961c":"print(\"Train data: \"+str(dfTrain.shape))\nprint(\"Valid data: \"+str(dfValid.shape))\nprint(\"Test data: \"+str(dfTest.shape))","4a7999fd":"lstFeature = pd.DataFrame([\n  ['LotArea', 'Lot size in square feet'],\n  ['TotalBsmtSF', 'Total square feet of basement area'],\n  ['GarageCars', 'Size of garage in car capacity'],\n  ['MSZoning', 'Identifies the general zoning classification of the sale'],\n  ['Street', 'Type of road access to property'],\n  ['LotFrontage', 'Linear feet of street connected to property'],\n  ['Neighborhood', 'Physical locations within Ames city limits'],\n  ['HouseStyle', 'Style of dwelling'],\n  ['YearBuilt', 'Original construction date'],\n],\n  columns=['Column names','Description'\n  ])\nprint(lstFeature)","12de69d0":"def plotFeature(df,lstFeature):\n    #plt.style.use('dark_background')\n    fig, axes = plt.subplots(nrows= 3,ncols = 3, figsize=(24,17))\n    for index,row in lstFeature.iterrows():\n        j = index % 3\n        i = index \/\/ 3\n        axes[i,j].scatter(df[row['Column names']], df['SalePrice'], marker = 'x', color='red')\n        axes[i,j].set_title(row['Description'])\n        axes[i,j].set_xlabel(row['Column names'])\n        axes[i,j].set_ylabel('Sale price')\n    return True\nplotFeature(dfTrain,lstFeature)   ","189e545a":"plt.figure(figsize=(24,8))\nplt.scatter(dfTrain['Neighborhood'], dfTrain['SalePrice'],color='red', alpha=1)\nplt.show()","dc57669e":"corrMatrix = dfTrain.corr() # for numerical feature values only\nf, ax = plt.subplots(figsize=(30, 12))\nsns.heatmap(corrMatrix, vmax=0.9, vmin=0.05, annot=True);","0c7f77d3":"#find the high correlation feature to each other\nfor index,row in corrMatrix.iterrows():  #iterate through all dataframe rows\n    sTemp = corrMatrix.loc[(corrMatrix[index]>highCorrValue) & (corrMatrix[index]<1),index] #filter all correlation values > 0.7 & < 1\n    if (sTemp.shape[0]>0):\n        print('~~~~~')\n        for index1,value1 in sTemp.iteritems(): #interate through all series values\n            print(\" - \"+index + \" --- \" + index1+\" : \"+str(value1))","dff68b2c":"#find the low correlation feature to SalePrice\nindex = 'SalePrice'\nsTemp = corrMatrix.loc[(corrMatrix[index]<lowCorrValue) & (corrMatrix[index]>-lowCorrValue),index]\nif (sTemp.shape[0]>0):\n    lstLowCorr = [index1 for index1,value1 in sTemp.iteritems()]\n    lstLowCorr.remove('Id')\n    for index1,value1 in sTemp.iteritems(): #interate through all series values\n        print(\" - \"+index + \" --- \" + index1+\" : \"+str(value1))","4c9ee4f5":"#sort by correlation value with house price (column 38)\ncorrMatrix['SalePrice'].sort_values(ascending = False).head(31)","941e6c4e":"#update the list of \"important\" feature and take a look at them\nlstFeature = pd.DataFrame([\n  ['OverallQual', 'Rates the overall material and finish of the house'],\n  ['GarageArea', 'Size of garage in square feet'],\n  ['TotalBsmtSF', 'Total rooms above grade (does not include bathrooms)'],\n  ['FullBath', 'Full bathrooms above grade'],\n  ['GrLivArea', 'Above grade (ground) living area square feet'],\n  ['YearBuilt', 'Original construction date'],\n  ['LotFrontage', 'Linear feet of street connected to property'],\n  ['YearRemodAdd', 'Remodel date (same as construction date if no remodeling or additions)'],\n],columns=['Column names','Description'])\n#then re-plot to see values of some new \"important\" features\nplotFeature(dfTrain,lstFeature)","46db51b3":"fig, ax = plt.subplots(nrows= 1,ncols = 1, figsize=(10,5))\nax.hist(dfTrain['SalePrice'], bins=30)","610f16d5":"fig, axes = plt.subplots(nrows= 3,ncols = 3, figsize=(24,17))\nfor index,row in lstFeature.iterrows():\n    j = index % 3\n    i = index \/\/ 3\n    axes[i,j].hist(dfTrain[row['Column names']],bins=20)\n    axes[i,j].set_title(row['Description'])","7102c8be":"colTypes = dfTrain.dtypes\nuniqueCount = dfTrain.nunique()\nuniqueCountObject = uniqueCount[colTypes[colTypes == 'object'].index].sort_values(ascending = False)\nprint(uniqueCountObject.head(20))\n#print(uniqueCount[colTypes[colTypes == 'int'].index].sort_values(ascending = False))","1b4e8053":"print(dfTrain.Neighborhood.value_counts().head(20))\n#print(dfTrain.Exterior2nd.value_counts())\n#print(dfTrain.Exterior1st.value_counts())\n#print(dfTrain.SaleType.value_counts())\n#print(dfTrain.Condition1.value_counts())","38dee8bd":"#Show all unique values of all categorical features --> use this informaiotn for aggregateion \nfor colName in uniqueCountObject.index:\n    print(colName +\" --- \" +str(dfTest[colName].unique()))","7c04898b":"#too many NAN valuse should be removed\ndef findTopMissingValueFeature(dfTrain,dfTest,percentNA,NAOnly):\n    if ('SalePrice' in dfTrain.columns):\n        dfTotal = pd.concat([dfTrain.drop('SalePrice', axis=1), dfTest],axis=0) #drop remove column from dataframe, concat --> stack 2 dataframe\n    else:\n        dfTotal = pd.concat([dfTrain, dfTest],axis=0) #drop remove column from dataframe, concat --> stack 2 dataframe\n    if (NAOnly):\n        null_cols = dfTotal.isnull().sum()\n    else:\n        null_cols = dfTotal.isnull().sum() +(dfTotal==0).sum() #caculate all zero value\n    print(len(null_cols[null_cols > 0])) #number of columns having NAN value\n    print(null_cols[null_cols > 0].sort_values(ascending = False).head(20)) #sort list list by number of NAN value, DESC\n    return (null_cols[null_cols > (percentNA*dfTotal.shape[0])].sort_values(ascending = False).index)\n\nlstFeatureMissingValue = findTopMissingValueFeature(dfTrain,dfTest,missingValuePercent,True)\nprint(lstFeatureMissingValue)","e0547296":"dfTrain.describe().T.head(20)","290c4062":"descTrain = dfTrain.describe().T\n#check for 3 sigma: mean - 3*std -- mean + 3*std\nprint(\"Features having values > mean + 3*std\")\nprint(descTrain[descTrain['max'] > descTrain['mean'] + descTrain['std'] * 3].index)\nprint(\"Features having values < mean - 3*std\")\nprint(descTrain[descTrain['min'] < descTrain['mean'] - descTrain['std'] * 3].index)","ef9e48fd":"dfTrainFinal = dfTrain.copy(deep=True)\ndfValidFinal = dfValid.copy(deep=True)\ndfTestIDs = dfTest['Id']\ndfValidIDs = dfValid['Id']\ndfTestFinal =dfTest.copy(deep=True)\ndfTrainFinal.drop(columns=['Id'], axis=1, inplace=True)\ndfValidFinal.drop(columns=['Id'], axis=1, inplace=True)\ndfTestFinal.drop(columns=['Id'], axis=1, inplace=True)\n#remove High Correlation features\n#coding optimization: move to 3.1 Feature Engineering\/Aggregations\n\n#remove feature having too many NAN (> 80%)\nif (removeMissingValue):\n    dfTrainFinal.drop(columns=lstFeatureMissingValue, axis=1, inplace=True)\n    dfValidFinal.drop(columns=lstFeatureMissingValue, axis=1, inplace=True)\n    dfTestFinal.drop(columns=lstFeatureMissingValue, axis=1, inplace=True)\n    \nprint(dfTrainFinal.shape)\nprint(dfValidFinal.shape)\nprint(dfTestFinal.shape)\n","b187a3e3":"def dropColOutliers(df, colName, coef):\n    mean = df[col].mean()\n    std = df[col].std()\n    df.drop(df[df[col] > (mean + coef * std)].index, inplace=True)\n    df.drop(df[df[col] < (mean - coef * std)].index, inplace=True)\n    return df","43b1acab":"print(dfTrainFinal.shape)\nprint(dfValidFinal.shape)\nprint(dfTestFinal.shape)\n\n#drop outlier for some features but RSMLE increase --> don't apply (data set is small)\nif (dropOutlier):\n    for col in dfTrainFinal.columns:\n        if (dfTrainFinal.dtypes[col] != 'object'):\n            dfTrainFinal = dropColOutliers(dfTrainFinal,col,coefSigma)\n\n#dfTrainFinal = dfTrainFinal.drop(dfTrainFinal[(dfTrainFinal['GrLivArea']>4000) & (dfTrainFinal['SalePrice']<300000)].index)\n#dfTrainFinal = dfTrainFinal.drop(dfTrainFinal[(dfTrainFinal['GrLivArea']>4000) & (dfTrainFinal['SalePrice']>700000)].index)\n\nprint(dfTrainFinal.shape)\nprint(dfTestFinal.shape)\n","a3f7900c":"plotFeature(dfTrainFinal,lstFeature)","0dea8d72":"lstNA = ['PoolQC','MiscFeature','Alley','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2','MSZoning']\nlstZero =['LotFrontage','GarageYrBlt','GarageArea','GarageCars','TotalBsmtSF','MasVnrArea','BsmtHalfBath','BsmtFullBath','BsmtUnfSF','BsmtFinSF1','BsmtFinSF2']\nlstMode =['KitchenQual','Exterior1st','Exterior2nd']\nlstMean =[]\nlstCols =['MasVnrType','Electrical','Functional','Utilities','SaleType']\nlstColsText =['None','Mix','Typ','AllPub','Oth']\nlstAllMode = dfTrainFinal.mode()","d8009d68":"def fillNAValues(df):\n    for col in lstNA:\n        if col in df.columns:\n            df[col].fillna('NA', inplace=True)\n    for col in lstZero:\n        if col in df.columns:\n            df[col].fillna(0, inplace=True)\n    for col in lstMode:\n        if col in df.columns:\n            df[col].fillna(lstAllMode[col][0], inplace=True)    \n    for i in range(len(lstCols)):\n        if col in df.columns:\n            df[lstCols[i]].fillna(lstColsText[i], inplace=True) ","6861cffa":"#fill NA values\nfillNAValues(dfTrainFinal)\nfillNAValues(dfValidFinal)\nfillNAValues(dfTestFinal)","40b4ac0f":"print(dfTrainFinal.isnull().any().any())\nprint(dfValidFinal.isnull().any().any())\nprint(dfTestFinal.isnull().any().any())","acf61616":"dfTarget = dfTrainFinal['SalePrice']\ndfTargetValid = dfValidFinal['SalePrice']\ndfTrainFinal.drop('SalePrice', axis=1, inplace=True)\ndfValidFinal.drop('SalePrice', axis=1, inplace=True)\n\nprint(dfTrainFinal.isnull().any().any())\nprint(dfValidFinal.isnull().any().any())\nprint(dfTestFinal.isnull().any().any())","cfd75496":"def aggregation(df):\n    #map value for categorical features\n    df['Neighborhood'] = df['Neighborhood'].map({'MeadowV': 0,'IDOTRR': 0,'BrDale': 0,'BrkSide': 0,'Edwards': 0,'OldTown': 0,'Sawyer': 0,'Blueste': 0,\n        'SWISU': 0,'NPkVill': 0,'NAmes': 0,'Mitchel': 0,'SawyerW': 0,'NWAmes': 0,'Gilbert': 0,'Blmngtn': 0,'CollgCr': 0,'Crawfor': 1,'ClearCr': 1,\n        'Somerst': 1,'Veenker': 1,'Timber': 1,'StoneBr': 2,'NridgHt': 2,'NoRidge': 2})\n  \n    df['MSZoning'] = df['MSZoning'].map({'NA': 0, 'C (all)': 0,'FV': 1,'I': 0,'RH': 1,'RL': 1,'RP': 1,'RM': 1})\n      \n    df['HouseStyle_stories'] = df['HouseStyle'].map({'1Story': 1,'1.5Fin': 1.5,'1.5Unf': 1.5,'2Story': 2,'2.5Fin': 2.5,'2.5Unf': 2.5,'SFoyer': 1.5,'SLvl': 1.5})\n\n    df['HouseStyle_fin'] = df['HouseStyle'].map({'1Story': 1,'1.5Fin': 1,'1.5Unf': 0,'2Story': 1,'2.5Fin': 1,'2.5Unf': 0,'SFoyer': 1,'SLvl': 1})\n    \n    #combine feature\n    df['TotFullBath'] = (df['BsmtFullBath'] + df['FullBath']*0.8)+(df['BsmtHalfBath'] + df['HalfBath']*0.8)*0.2\n    df.drop(columns=['BsmtFullBath','FullBath','BsmtHalfBath','HalfBath'], axis=1, inplace=True)\n    df['TotalBsmtSF'] = df['TotalBsmtSF'] + df['1stFlrSF']+ df['2ndFlrSF']\n    \n    df['Total_porch_sf'] = df['OpenPorchSF'] + df['3SsnPorch'] +  df['EnclosedPorch'] + df['ScreenPorch'] + df['WoodDeckSF']\n    df = df.drop(['OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch','WoodDeckSF'], axis=1)\n    \n    #df['YearBuilt'] = df['YearBuilt'] + df['GarageYrBlt']*0.3\n    #df['GrLivArea'] = df['GrLivArea'] + df['TotRmsAbvGrd']*0.5\n    #df['GarageArea'] = df['GarageArea'] + df['GarageCars']*100\n    \n    #remove High Correlation features\n    if (removeHighCorr):\n     #YearBuilt --- GarageYrBlt : 0.825228078796012\n     #TotalBsmtSF --- 1stFlrSF : 0.821105896760658\n     #GrLivArea --- TotRmsAbvGrd : 0.819525144369214\n     #TotRmsAbvGrd --- GrLivArea : 0.819525144369214\n     #GarageArea --- GarageCars : 0.8870237927614472\n        df.drop(columns=['GarageYrBlt', '1stFlrSF', 'TotRmsAbvGrd', 'GarageCars','2ndFlrSF'], axis=1, inplace=True)","969b5c5e":"dfTestFinal['MSZoning'].unique()","73fd844a":"aggregation(dfTrainFinal)\naggregation(dfTestFinal)\naggregation(dfValidFinal)\nprint(dfTrainFinal.isnull().any().any())\nprint(dfValidFinal.isnull().any().any())\nprint(dfTestFinal.isnull().any().any())","3605b1c5":"null_cols = dfTestFinal.isnull().sum() \nprint(len(null_cols[null_cols > 0])) #number of columns having NAN value\nnull_cols[null_cols > 0].sort_values(ascending = False)","d1a16abb":"dfTrainFinal['Neighborhood'].unique()","396f91f7":"# try to remove features having low correlation with SalePrice but RSMLE increase --> don't apply\nif (removeLowCorr2Price):\n    dfTrainFinal.drop(columns=lstLowCorr, axis=1, inplace=True)\n    dfValidFinal.drop(columns=lstLowCorr, axis=1, inplace=True)\n    dfTestFinal.drop(columns=lstLowCorr, axis=1, inplace=True)","613965f8":"\nprint(dfTrainFinal.shape)\nprint(dfTestFinal.shape)\ncol_types = dfTrainFinal.dtypes\ndfTrainFinal = pd.get_dummies(dfTrainFinal, columns=col_types[col_types == 'object'].index.values, drop_first=True)\ndfValidFinal = pd.get_dummies(dfValidFinal, columns=col_types[col_types == 'object'].index.values, drop_first=True)\ndfTestFinal = pd.get_dummies(dfTestFinal, columns=col_types[col_types == 'object'].index.values, drop_first=True)\nprint(dfTrainFinal.shape)\nprint(dfTestFinal.shape)","65b65e48":"def adapt_columns(train_columns, df):\n    # Add missing columns\n    for column in train_columns:\n        if (column not in df.columns):\n            df[column] = 0\n\n    # Delete columns that don't exist in train\n    for column in df.columns:\n        if (column not in train_columns):\n            df.drop(column, axis=1, inplace=True)\n    return df","72c53465":"adapt_columns(dfTrainFinal.columns, dfTestFinal)\nadapt_columns(dfTrainFinal.columns, dfValidFinal)\nprint(dfTrainFinal.shape)\nprint(dfValidFinal.shape)\nprint(dfTestFinal.shape)","3b855524":"print(dfTrainFinal.dtypes.value_counts())\nprint(dfTrainFinal.isnull().any().any())\nprint(dfTestFinal.isnull().any().any())","f6722204":"def normalization(df):\n    array_val = df.values \n    min_max_scaler = preprocessing.MinMaxScaler()\n    array_norm = min_max_scaler.fit_transform(array_val)\n    return pd.DataFrame(data=array_norm, columns=df.columns.values)","897a58e0":"dfTrainFinal = normalization(dfTrainFinal)\ndfValidFinal = normalization(dfValidFinal)\ndfTestFinal = normalization(dfTestFinal)\nprint(dfTrainFinal.isnull().any().any())\nprint(dfValidFinal.isnull().any().any())\nprint(dfTestFinal.isnull().any().any())","9bfe8a3d":"xgb_params = {'n_estimators': 3000,'learning_rate': 0.01,'max_depth': 3, 'min_child_weight': 0,'gamma': 0.0,'colsample_bytree': 0.7,\"colsample_bylevel\": 0.5,'subsample': 0.7,'reg_alpha': 0.00006,'reg_lambda': 0.00006}","50128644":"max_depth = range(1,15,2)\nmin_child_weight = range(1,10,2)\ndfa1 = pd.DataFrame.from_records([(x,y) for x in max_depth for y in min_child_weight], columns = ['max_depth', 'min_child_weight'])\n\nsplits = 2\nkf = KFold(n_splits=splits, shuffle=True, random_state=142)\ndef model_eval(max_depth, min_child_weight):\n    mae = 0\n    rmsle = 0\n    xgb_params_temp = xgb_params.copy()\n    xgb_params_temp['n_estimators'] = 500\n    xgb_params_temp['learning_rate'] = 0.1\n    xgb_params_temp['max_depth'] = max_depth\n    xgb_params_temp['min_child_weight'] = min_child_weight\n    model = xgb.XGBRegressor(params=xgb_params_temp)\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]          \n        model.fit(X_train_k, y_train_k)\n        y_pred_k = model.predict(X_test_k)\n        np.round(y_pred_k)\n        #mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n    return (rmsle\/splits)\n\nif (doesHyperparameter):\n    dfa1['rmsle'] = dfa1.apply(lambda x: model_eval(max_depth = int(x[0]), min_child_weight = x[1]), axis = 1).values\n    #fig, ax = plt.subplots()\n    #dfa1.groupby(by = 'max_depth').plot(x= 'min_child_weight', y = 'rmsle', ax = ax,  kind = 'line', figsize=(4,10) )\n    #ax.legend(max_depth)\n    print(dfa1.sort_values(by = 'rmsle', ascending=False).head(20))","2946e6ef":"reg_alpha = reg_lambda = [0, 0.001, 0.01, 0.1, 0.2, 0.4, 0.8, 1]\ndfa2 = pd.DataFrame.from_records([(x,y) for x in reg_alpha for y in reg_lambda], columns = ['reg_alpha', 'reg_lambda'])\n\nsplits = 2\nkf = KFold(n_splits=splits, shuffle=True, random_state=42)\n\ndef model_eval2(reg_alpha, reg_lambda):\n    mae = 0\n    rmsle = 0\n    xgb_params_temp = xgb_params.copy()\n    xgb_params_temp['n_estimators'] = 500\n    xgb_params_temp['learning_rate'] = 0.1\n    xgb_params_temp['reg_alpha'] = reg_alpha\n    xgb_params_temp['reg_lambda'] = reg_lambda\n    model = xgb.XGBRegressor(params=xgb_params_temp)\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]          \n        model.fit(X_train_k, y_train_k)\n        y_pred_k = model.predict(X_test_k)\n        np.round(y_pred_k)\n        #mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n    return (rmsle\/splits)\n    \nif (doesHyperparameter):\n    dfa2['rmsle'] = dfa2.apply(lambda x: model_eval2(reg_alpha = x[0],reg_lambda = x[1]), axis = 1).values\n    print(dfa2.sort_values(by = 'rmsle', ascending=False).head(20))","af0258fe":"n_estimators = [200, 500, 800, 1000, 1500, 2000, 4000]\nlearning_rate = [0.1, 0.05, 0.01, 0.005, 0.001]\ndfa5 = pd.DataFrame.from_records([(x,y) for x in n_estimators for y in learning_rate], columns = ['n_estimators', 'learning_rate'])\n\nsplits = 2\nkf = KFold(n_splits=splits, shuffle=True, random_state=412)\n\ndef model_eval5(n_estimators, learning_rate ):\n    mae = 0\n    rmsle = 0\n    xgb_params_temp = xgb_params.copy()\n    xgb_params_temp['n_estimators'] = n_estimators\n    xgb_params_temp['learning_rate'] = learning_rate\n    model = xgb.XGBRegressor(params=xgb_params_temp)\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]          \n        model.fit(X_train_k, y_train_k)\n        y_pred_k = model.predict(X_test_k)\n        np.round(y_pred_k)\n        #mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n    return (rmsle\/splits)\n    \nif (doesHyperparameter):\n    dfa5['rmsle'] = dfa5.apply(lambda x: model_eval5(n_estimators = x[0],learning_rate = x[1]), axis = 1).values\n    print(dfa5.sort_values(by = 'rmsle', ascending=False).head(20))","21521f11":"X_train, X_test, y_train, y_test = train_test_split(dfTrainFinal, dfTarget, test_size=0.33, random_state=7)","008d9102":"from sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = xgb.XGBRegressor(params=xgb_params)\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\ny_pred = model.predict(X_test)\nrmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\nprint(\"RMSLE: %.5f\" % (rmsle))\n","1debc4ba":"thresholds = np.sort(model.feature_importances_)\nthresholds = thresholds[thresholds>0]\nprint(thresholds)","6db68737":"for thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = xgb.XGBRegressor(params=xgb_params)\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n    print(\"Thresh=%.5f, n=%d, RMSLE: %.5f\" % (thresh, select_X_train.shape[1], rmsle))","d390995a":"# plot feature importance\nfig, ax = plt.subplots(figsize=(16, 14))\nxgb.plot_importance(model,importance_type='weight',max_num_features=60,ax=ax).set_yticklabels(dfTrainFinal.columns)\nplt.show()\nprint(model.feature_importances_)","02af4ae5":"mae = 0\nrmsle = 0\nsplits = 5\n\nkf = KFold(n_splits=splits, shuffle=True, random_state=42)\nmodel = xgb.XGBRegressor(params=xgb_params)    \n\nfor train_index, test_index in kf.split(dfTrainFinal):\n    X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n    y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]      \n    \n    #model_k = xgb.XGBRegressor(params=xgb_params)    \n    model.fit(X_train_k, y_train_k)\n    y_pred_k = model.predict(X_test_k)\n    \n    np.round(y_pred_k)\n    mae = mae + median_absolute_error(y_test_k, y_pred_k)\n    rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n\nprint('MAE: ' + '{:.2f}'.format(mae\/splits)) \nprint('Train data RMSLE: ' + '{:.4f}'.format(rmsle\/splits)) \n\n\nif (validPercent>0):\n    y_predValid = model.predict(dfValidFinal.values)\n    mae = median_absolute_error(dfTargetValid.values, y_predValid)\n    rmsle = np.sqrt(mean_squared_log_error(dfTargetValid.values, y_predValid))\n    print('MAE: ' + '{:.2f}'.format(mae))\n    print('Validate data RMSLE: ' + '{:.4f}'.format(rmsle))","d1e52b06":"if (False):\n    mae = 0\n    rmsle = 0\n    splits = 10\n\n    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n    modelLGBM = lgb.LGBMRegressor(objective='regression', \n                                           num_leaves=4,\n                                           learning_rate=0.01, \n                                           n_estimators=3000,\n                                           max_bin=200, \n                                           bagging_fraction=0.75,\n                                           bagging_freq=5, \n                                           bagging_seed=7,\n                                           feature_fraction=0.2,\n                                           feature_fraction_seed=7,\n                                           verbose=-1,\n                                           )\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]      \n\n        #print(modelLGBM)\n        ts = time.time()\n        modelLGBM.fit(X_train_k, y_train_k)\n        #print(time.time()-ts)\n        y_pred_k = modelLGBM.predict(X_test_k)\n\n        np.round(y_pred_k)\n        mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n        print('Train data RMSLE: ' + '{:.4f}'.format(rmsle)) \n\n\n    print('MAE: ' + '{:.2f}'.format(mae\/splits)) \n    print('Train data RMSLE: ' + '{:.4f}'.format(rmsle\/splits)) \n","9b8a4d9a":"if (False):\n    mae = 0\n    rmsle = 0\n    splits = 10\n\n    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n    modelGBR = ens.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]      \n\n        #print(modelLGBM)\n        ts = time.time()\n        modelGBR.fit(X_train_k, y_train_k)\n        #print(time.time()-ts)\n        y_pred_k = modelGBR.predict(X_test_k)\n\n        np.round(y_pred_k)\n        mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n        print('Train data RMSLE: ' + '{:.4f}'.format(rmsle)) \n\n\n    print('MAE: ' + '{:.2f}'.format(mae\/splits)) \n    print('Train data RMSLE: ' + '{:.4f}'.format(rmsle\/splits)) \n","9fd3e52c":"if (False):\n    mae = 0\n    rmsle = 0\n    splits = 10\n\n    kf = KFold(n_splits=splits, shuffle=True, random_state=42)\n    modelStackCV = StackingCVRegressor(regressors=(model, modelLGBM,modelGBR),\n                                    meta_regressor=model,\n                                    use_features_in_secondary=True)\n    for train_index, test_index in kf.split(dfTrainFinal):\n        X_train_k, X_test_k = dfTrainFinal.values[train_index], dfTrainFinal.values[test_index]\n        y_train_k, y_test_k = dfTarget.values[train_index], dfTarget.values[test_index]      \n\n        #print(modelLGBM)\n        ts = time.time()\n        modelStackCV.fit(X_train_k, y_train_k)\n        #print(time.time()-ts)\n        y_pred_k = modelStackCV.predict(X_test_k)\n\n        np.round(y_pred_k)\n        mae = mae + median_absolute_error(y_test_k, y_pred_k)\n        rmsle = rmsle + np.sqrt(mean_squared_log_error(y_test_k, y_pred_k))\n        print('Train data RMSLE: ' + '{:.4f}'.format(rmsle)) \n\n\n    print('MAE: ' + '{:.2f}'.format(mae\/splits)) \n    print('Train data RMSLE: ' + '{:.4f}'.format(rmsle\/splits)) \n","69c0f765":"model = xgb.XGBRegressor(params=xgb_params)\nmodel.fit(dfTrainFinal.values, dfTarget.values)\ny_pred = model.predict(dfTestFinal.values)\nif (dropLowImportantFeatures):\n    selection = SelectFromModel(model, threshold= 0.0001, prefit=True)\n    #show all droped feature\n    print(dfTrainFinal.columns[selection.get_support()== False])\n    select_X_train = selection.transform(dfTrainFinal)\n    selection_model = xgb.XGBRegressor(params=xgb_params)\n    selection_model.fit(select_X_train, dfTarget)\n    select_X_test = selection.transform(dfTestFinal)\n    y_pred = selection_model.predict(select_X_test)\n    \nif (False):\n    modelLGBM.fit(dfTrainFinal.values, dfTarget.values)\n    modelGBR.fit(dfTrainFinal.values, dfTarget.values)\n    #modelStackCV.fit(dfTrainFinal.values, dfTarget.values)\n    y_predLGBM = modelLGBM.predict(dfTestFinal.values)\n    y_predGBR = modelGBR.predict(dfTestFinal.values)\n    #y_predStack = modelStackCV.predict(dfTestFinal.values)\n    #y_pred = y_pred*0.5 + y_predLGBM*0.5 + y_predGBR*0.0\n    y_pred = y_predGBR\n\ndf_pred = pd.DataFrame(data=y_pred, columns=['SalePrice'])\ndf_pred = pd.concat([dfTestIDs, df_pred['SalePrice']], axis=1)\ndf_pred.SalePrice = df_pred.SalePrice.round(0)\ndf_pred.to_csv('submission.csv', sep=',', encoding='utf-8', index=False)","6c0afc7a":"dfTest['SalePrice']=df_pred['SalePrice']\ndef plotFeatureTest(df,lstFeature):\n    #plt.style.use('dark_background')\n    fig, axes = plt.subplots(nrows= 3,ncols = 3, figsize=(24,17))\n    for index,row in lstFeature.iterrows():\n        j = index % 3\n        i = index \/\/ 3\n        axes[i,j].scatter(df[row['Column names']], df['SalePrice'], marker = 'x', color='red')\n        axes[i,j].set_title(row['Description'])\n        axes[i,j].set_xlabel(row['Column names'])\n        axes[i,j].set_ylabel('Sale price')\n    return True\nplotFeatureTest(dfTest,lstFeature)  ","fe099d02":"\n# 4. Hyperparameters optimization","c8e53d43":"## 1.4 Distribution ","7e0924cf":"## 1.5 Categorical features","f20987e4":"## 6.1. LGBM","e4343b3f":"# 6. Other model: test performance\njust do some test with other model","0939b734":"# 3. Feature Engineering","b0915306":"## 2.1 Drop not important features: correlated features, missing values","8c108356":"## 2.2 Drop outliers","c8292bf9":"## 6.3. StackingCV","5144a276":"# VEF MACHINE LEARNING 2019 - House Prices: Advanced Regression Techniques\nwith a lot of reference from Kaggle and Google :)","fd915d79":"# 1. Explore dataset","440ab7a9":"## 3.3 Normalization","7f2de585":"## 5.2 Cross Validation","d142fa2e":"The LotArea just show a little information --> so this feature not much important","ec559edd":"## 1.2 Guess and Review some \"important\" features\n* LotArea: Lot size in square feet\n* TotalBsmtSF: Total square feet of basement area\n* GarageCars: Size of garage in car capacity\n* MSZoning: Identifies the general zoning classification of the sale\n* Street: Type of road access to property\n* LotFrontage: Linear feet of street connected to property\n* Neighborhood: Physical locations within Ames city limits\n* HouseStyle: Style of dwelling\n* YearBuilt: Original construction date\n","7452cfc4":"## 6.2. Gradient Boosting","29469d69":"## 1.6 NAN values","c4201c25":"\n# 5. XGBoost Regressor\n","5c163bb2":"## 4.2 reg_alpha, reg_lamda","f1bc4730":"## 3.2 One-hot-encoding\n","7442ef21":"## 4.3 n_estimators and learning_rate[](http:\/\/)","2df3e8e0":"## 3.1 Aggregations","cf92ad15":"## 1.1 Load library, dataset","6944d4ce":"**Many features don't effect to model (feature_importances_ values = 0) --> we can drop them**\n* Already test in 2 cases: drop or keep not important features --> RMSLE on training\/validation\/test data are same :)","3317910a":"# 2. Clean dataset","ec252eb9":"## 2.3 Fill NA","7f2115c2":"## 1.3 Correlation","778c3bef":"## 2.4 Separate the target feature from the training dataset","0d1574d7":"## 4.1 Max_depth and min_child_weight","18832ef8":"# 7. Train the final model and make predictions on the test dataset\n","1816ae75":"## 5.1 Feature important"}}