{"cell_type":{"eecd39a4":"code","34e1faf7":"code","b3728a4d":"code","9a3dbf8e":"code","28c9807a":"code","38fe27e0":"code","2cb846ba":"code","28046d15":"code","1f50ded1":"code","af359001":"code","7caa423f":"code","1b9d06ea":"code","06451ce5":"code","45127626":"code","216e5d7c":"code","e32495b7":"code","ae6906f9":"code","c5278b82":"code","681d7586":"code","3ca9af13":"code","5c0a15c4":"code","e58ac959":"code","75be77ca":"code","e13eab9e":"code","9cbc842f":"code","9f0452b7":"markdown","55d6b8c1":"markdown","b4e16b80":"markdown","c16326e2":"markdown","b1cfe133":"markdown","f752fc5b":"markdown","3bb393b9":"markdown"},"source":{"eecd39a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pandas_profiling\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34e1faf7":"df = pd.read_csv('..\/input\/updated\/proj.csv')\n\ndf","b3728a4d":"#Numpy arrays for Features and Target Label\n\nX = df.drop('class',axis=1).values\ny = df['class'].values","9a3dbf8e":"from sklearn.model_selection import train_test_split\n\n#Stratified Train-Test Data Split\n\nX_train,X_test,y_train,y_test = train_test_split(X,                 #Features\n                                                 y,                 #Labels\n                                                 test_size=0.2,     #80-20 Train-Test Split\n                                                 random_state=42, \n                                                 stratify=y         #To perform Stratified Data Split\n                                                )","28c9807a":"from sklearn import svm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import multilabel_confusion_matrix\n\ncross_accuracy = []\ntrain_accuracy = []\ntest_accuracy  = []\n\n#To search for Best Parameters for the Classifier\nkernel = ['linear', 'rbf', 'poly']\n\nC = [int(x) for x in range(1,10)]\nrandom_grid = {\n    'kernel': kernel,\n    'C' : C\n}\n\nsvc = svm.SVC()\n\nsvc_random = RandomizedSearchCV(estimator = svc, \n                               param_distributions = random_grid, \n                               n_iter = 27, \n                               cv = 10, \n                               random_state=42, \n                               n_jobs = -1)\n\nscores = cross_val_score(svc_random, X_train, y_train, cv=10)\n\nsvc_random.fit(X_train, y_train)\nprint(\"Best Parameters for SVM: \", svc_random.best_params_, \"\\n\")\n\nprint(\"Cross Validation Score: \")\nprint(\"Accuracy : \", scores.mean() * 100, \"% with a Standard Deviation of \", scores.std() * 100, \"\\n\")\ncross_accuracy.append(scores.mean() *100)\n\ny_pred = svc_random.predict(X_train)\nprint('Training Score: ')\nprint(\"Accuracy :\", accuracy_score(y_train,y_pred) * 100 , \"\\n\")\ntrain_accuracy.append(accuracy_score(y_train,y_pred) * 100)\n\nsvc_random.fit(X_train, y_train)\ny_pred = svc_random.predict(X_test)\nprint('Testing Score: ')\nprint(\"Accuracy : \", accuracy_score(y_test,y_pred) * 100, \"\\n\")\ntest_accuracy.append(accuracy_score(y_test,y_pred) * 100)\nprint(precision_score(y_test, y_pred, average='macro'))\nprint(recall_score(y_test, y_pred, average='macro'))\nprint(f1_score(y_test, y_pred, average='macro'))\nprint(multilabel_confusion_matrix(y_test, y_pred))","38fe27e0":"from sklearn.naive_bayes import GaussianNB, MultinomialNB, CategoricalNB\n\n\nmodel = [MultinomialNB(), GaussianNB(), CategoricalNB()]\nmodel_name = [\"Multinomial Naive Bayes\", \"Gaussian Naive Bayes\", \"Categorical Naive Bayes\"]\n\nfor clf, name in zip(model, model_name):\n    print(\"###  \", name, \"  ###\\n\")\n    \n    scores = cross_val_score(clf, X_train, y_train, cv=10)\n    print(\"Cross Validation Score: \")\n    print(\"Accuracy : \", scores.mean() * 100, \"% with a Standard Deviation of \", scores.std() * 100, \"\\n\")\n    cross_accuracy.append(scores.mean() *100)\n    \n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_train)\n    print('Training Score: ')\n    print(\"Accuracy :\", accuracy_score(y_train,y_pred) * 100 , \"\\n\")\n    train_accuracy.append(accuracy_score(y_train,y_pred) * 100)\n\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print('Testing Score: ')\n    print(\"Accuracy :\", accuracy_score(y_test,y_pred) * 100 , \"\\n\")\n    test_accuracy.append(accuracy_score(y_test,y_pred) * 100)\n    print(precision_score(y_test, y_pred, average='macro'))\n    print(recall_score(y_test, y_pred, average='macro'))\n    print(f1_score(y_test, y_pred, average='macro'))","2cb846ba":"from sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\n\nscores = cross_val_score(clf, X_train, y_train, cv=10)\nprint(\"Cross Validation Score: \")\nprint(\"Accuracy : \", scores.mean() * 100, \"% with a Standard Deviation of \", scores.std() * 100, \"\\n\")\ncross_accuracy.append(scores.mean() *100)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_train)\nprint('Training Score: ')\nprint(\"Accuracy :\", accuracy_score(y_train,y_pred) * 100 , \"\\n\")\ntrain_accuracy.append(accuracy_score(y_train,y_pred) * 100)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint('Testing Score: ')\nprint(\"Accuracy :\", accuracy_score(y_test,y_pred) * 100 , \"\\n\")\ntest_accuracy.append(accuracy_score(y_test,y_pred) * 100)    \nprint(precision_score(y_test, y_pred, average='macro'))\nprint(recall_score(y_test, y_pred, average='macro'))\nprint(f1_score(y_test, y_pred, average='macro'))","28046d15":"from sklearn.ensemble import RandomForestClassifier\n\n#To search for Best Parameters for the Classifier\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in range(1,15)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in range(1,10)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nforest_model = RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = forest_model, \n                               param_distributions = random_grid, \n                               n_iter = 25, \n                               cv = 12, \n                               random_state=42, \n                               n_jobs = -1)\n\n# Fit the random search model\n\nrf_random.fit(X_train, y_train)\nprint(\"Best Parameters for Random Forest: \", rf_random.best_params_, \"\\n\")\n\nprint(\"Cross Validation Score: \")\nprint(\"Accuracy : \", scores.mean() * 100, \"% with a Standard Deviation of \", scores.std() * 100, \"\\n\")\ncross_accuracy.append(scores.mean() *100)\n\ny_pred = rf_random.predict(X_train)\nprint('Training Score: ')\nprint(\"Accuracy :\", accuracy_score(y_train,y_pred) * 100 , \"\\n\")\ntrain_accuracy.append(accuracy_score(y_train,y_pred) * 100)\n\nrf_random.fit(X_train, y_train)\ny_pred = rf_random.predict(X_test)\nprint('Testing Score: ')\nprint(\"Accuracy : \", accuracy_score(y_test,y_pred) * 100, \"\\n\")\ntest_accuracy.append(accuracy_score(y_test,y_pred) * 100)\nprint(precision_score(y_test, y_pred, average='macro'))\nprint(recall_score(y_test, y_pred, average='macro'))\nprint(f1_score(y_test, y_pred, average='macro'))","1f50ded1":"from sklearn.ensemble import GradientBoostingClassifier\n\n#To search for Best Parameters for the Classifier\nn_estimators = [int(x) for x in range(1,15)]\n\nrandom_grid = {\n    'n_estimators': n_estimators\n}\n\nclf = GradientBoostingClassifier()\n\nclf_random = RandomizedSearchCV(estimator = clf, \n                               param_distributions = random_grid, \n                               n_iter = 14, \n                               cv = 10, \n                               random_state=42, \n                               n_jobs = -1)\n\nscores = cross_val_score(clf_random, X_train, y_train, cv=10)\n\nclf_random.fit(X_train, y_train)\nprint(\"Best Parameters for Gradient Boosting Classifier: \", svc_random.best_params_, \"\\n\")\n\nprint(\"Cross Validation Score: \")\nprint(\"Accuracy : \", scores.mean() * 100, \"% with a Standard Deviation of \", scores.std() * 100, \"\\n\")\ncross_accuracy.append(scores.mean() *100)\n\ny_pred = clf_random.predict(X_train)\nprint('Training Score: ')\nprint(\"Accuracy :\", accuracy_score(y_train,y_pred) * 100 , \"\\n\")\ntrain_accuracy.append(accuracy_score(y_train,y_pred) * 100)\n\nclf_random.fit(X_train, y_train)\ny_pred = clf_random.predict(X_test)\nprint('Testing Score: ')\nprint(\"Accuracy : \", accuracy_score(y_test,y_pred) * 100, \"\\n\")\ntest_accuracy.append(accuracy_score(y_test,y_pred) * 100)\nprint(precision_score(y_test, y_pred, average='macro'))\nprint(recall_score(y_test, y_pred, average='macro'))\nprint(f1_score(y_test, y_pred, average='macro'))","af359001":"#import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Setup arrays to store training and test accuracies\nneighbors = np.arange(1,9)\ncross_acc = np.empty(len(neighbors))\ntrain_acc =np.empty(len(neighbors))\ntest_acc = np.empty(len(neighbors))\n\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    scores = cross_val_score(knn, X_train,y_train, cv=10)\n    \n    #Fit the model\n    cross_acc[i] = scores.mean() * 100\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_acc[i] = knn.score(X_train, y_train) * 100\n    \n    #Compute accuracy on the test set\n    test_acc[i] = knn.score(X_test, y_test) * 100\n    print(precision_score(y_test, y_pred, average='macro'))\n    print(recall_score(y_test, y_pred, average='macro'))\n    print(f1_score(y_test, y_pred, average='macro'))\n\nmaxi = 0\nj = 0\nfor i in range(8):\n    if train_acc[i] > maxi:\n        maxi = train_acc[i]\n        j = i\n        \ncross_accuracy.append(cross_acc[j])\ntrain_accuracy.append(train_acc[i])\ntest_accuracy.append(test_acc[i])\nprint(cross_accuracy)\nprint(train_accuracy)\nprint(test_accuracy)","7caa423f":"#Generate plot\nplt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_acc, label='Testing Accuracy')\nplt.plot(neighbors, train_acc, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","1b9d06ea":"plotdata = pd.DataFrame({\n    \"Cross Validation Accuracy\":cross_accuracy,\n    \"Training Accuracy\": train_accuracy,\n    \"Testing Accuracy\": test_accuracy\n    }, \n    index=[\"SVM\", \"MNB\", \"GNB\", \"CNB\", \"DT\", \"RF\", \"GB\", \"KNN\"]\n)\nplotdata.plot(kind=\"bar\", ylim= 75)\nplt.title(\"ML ALGORITHM COMPARISON\")\nplt.xlabel(\"ML ALGORITHM\")\nplt.ylabel(\"PERCENT\")","06451ce5":"import pickle\n\nfilename = 'finalized_model.sav'\npickle.dump(rf_random, open(filename, 'wb'))","45127626":"df = pd.read_csv('..\/input\/final-donor\/myFile0 (5).csv')\n\n#print(df.at[10, 'firstname'])\n#Getting Number of rows and cols\nrow, col = df.shape \nprint(row,col)\n\ndf","216e5d7c":"#Getting Names of all columns into a list\ncol_names = list(df.columns)\n\n#Getting Datatype of each column into a Pandas series\ndata_type = df.dtypes\nprint(data_type)\nprint(\"[STATUS] Found Data Type\")\n\n\n#int, float, string, date\n#Percentage of Null values in each column\nnull_per = df.isna().sum() \/ row * 100\nprint(null_per)\nprint(\"[STATUS] Calculated Percentage of NULL values\")","e32495b7":"#Percentage of Unique values in each column\n#Approximate if a column is categorical or not\nunique_per = []\ncategorical = []\nfor c in col_names:\n    b = df.pivot_table(index=[c], aggfunc='size')\n    unique =0\n    for i in b.array:\n        if i == 1:\n            unique+=1\n    col_unique_per = unique \/ row * 100\n    if col_unique_per <= 2:\n        categorical.append(1)\n    else:\n        categorical.append(0)\n    unique_per.append(col_unique_per)\nprint(unique_per)\nprint(\"[STATUS] Calculated Percentage of Unique values\")\nprint(categorical)\nprint(\"[STATUS] Found Categorical data\")","ae6906f9":"#Default value && Present or not\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nencdf = df\nfor i in col_names:\n    if encdf[i].dtype == 'object':\n        encdf[i] = le.fit_transform(df[i].astype('str'))","c5278b82":"#Find Correlation Matrix for numerical data\ncorrMatrix = encdf.corr()\nprint (corrMatrix)\n#print(corrMatrix['id']['firstname'])\ncorr = [0 for i in range(col)]\nfor i in range(col):\n    for j in range(i-1):\n        if corrMatrix[col_names[i]][col_names[j]] > 0.75 or corrMatrix[col_names[i]][col_names[j]] < -0.75:\n            corr[i] = 1\nprint(corr)\nprint(\"[STATUS] Calculated Correlation Matrix\")","681d7586":"import seaborn as sn\nimport matplotlib.pyplot as plt\n\nsn.heatmap(corrMatrix, annot=True)\nplt.show","3ca9af13":"#Pattern Matching to find if a column is important\nsensitive = []\npatterns = [\"id\", \"aadhaar\", \"ssn\", \"name\", \"phone\", \"address\", \"mail\",\"location\"]\nfor c in col_names:\n    f = 0\n    c = c.lower()\n    for pattern in patterns:\n        if pattern in c:\n            sensitive.append(1)\n            f = 1\n            break\n    if f == 0:\n        sensitive.append(0)\nprint(sensitive)\nprint(\"[STATUS] Found Pattern based Sensitive data\")","5c0a15c4":"#Combining all data analysis results\nX_pred = np.zeros([col, 6], dtype=int)\n\nfor i in range(col):\n    for j in range(6):\n        if j == 0:\n            if data_type[i] == 'int':\n                X_pred[i][0] = 1\n            elif data_type[i] == 'float':\n                X_pred[i][0] = 2\n            elif data_type[i] == 'object':\n                X_pred[i][0] = 3\n            else:\n                X_pred[i][0] = 4\n        elif j == 1:\n            X_pred[i][1] = int(null_per[i] * 100)\n        elif j == 2:\n            X_pred[i][2] = unique_per[i]\n        elif j == 3:\n            X_pred[i][3] = categorical[i]\n        elif j == 4:\n            X_pred[i][4] = corr[i]\n        elif j == 5:\n            X_pred[i][5] = sensitive[i]\n\nprint(X_pred)\nprint(\"[STATUS] Data Analyzed Successfully\")","e58ac959":"#Predicting the sensitivity\nimport pickle \n\nfilename = '..\/input\/finalmod\/finalized_model (2).sav'\n\nloaded_model = pickle.load(open(filename, 'rb'))\n\nr_pred = loaded_model.predict(X_pred)\nprint(r_pred)","75be77ca":"!pip install pycryptodome\n!pip install pycryptodomex","e13eab9e":"from Crypto.Protocol.KDF import PBKDF2\nsalt = b'245' # Salt you generated\npassword = 'password123' # Password provided by the user, can use input() to get this\n\nkey = PBKDF2(password, salt, dkLen=32) # Your key that you can encrypt with\nprint(key)","9cbc842f":"from base64 import b64encode, b64decode\nimport hashlib\nfrom Cryptodome.Cipher import AES\nimport os\nfrom Cryptodome.Random import get_random_bytes\n\nprint(AES.block_size)\ndef encrypt(plain_text, password):\n    # generate a random salt\n    salt = get_random_bytes(AES.block_size)\n    salt = b\"fsgsfgssbbhhfs\"\n    print(salt)\n\n    # use the Scrypt KDF to get a private key from the password\n    private_key = hashlib.scrypt(\n        password.encode(), salt=salt, n=2**14, r=8, p=1, dklen=32)\n    \n    print(private_key)\n    # create cipher config\n    cipher_config = AES.new(private_key, AES.MODE_GCM)\n\n    # return a dictionary with the encrypted text\n    cipher_text, tag = cipher_config.encrypt_and_digest(bytes(plain_text, 'utf-8'))\n    return {\n        'cipher_text': b64encode(cipher_text).decode('utf-8'),\n        'salt': b64encode(salt).decode('utf-8'),\n        'nonce': b64encode(cipher_config.nonce).decode('utf-8'),\n        'tag': b64encode(tag).decode('utf-8')\n    }\n\n\ndef decrypt(enc_dict, password):\n    # decode the dictionary entries from base64\n    salt = b64decode(enc_dict['salt'])\n    cipher_text = b64decode(enc_dict['cipher_text'])\n    nonce = b64decode(enc_dict['nonce'])\n    tag = b64decode(enc_dict['tag'])\n    \n\n    # generate the private key from the password and salt\n    private_key = hashlib.scrypt(\n        password.encode(), salt=salt, n=2**14, r=8, p=1, dklen=32)\n\n    # create the cipher config\n    cipher = AES.new(private_key, AES.MODE_GCM, nonce=nonce)\n\n    # decrypt the cipher text\n    decrypted = cipher.decrypt_and_verify(cipher_text, tag)\n\n    return decrypted\n\n\ndef main():\n    password = \"kawin\"\n\n    # First let us encrypt secret message\n    encrypted = encrypt(\"The secretest message here\", password)\n    print(encrypted)\n\n    # Let us decrypt using our original password\n    decrypted = decrypt(encrypted, password)\n    print(bytes.decode(decrypted))\n\nmain()","9f0452b7":"# **DataSet Description:**\n\n1) **Data Type** - *1* : Int, *2* : Float, *3* : String, *4* : DateTime\n\n2) **Null Percent** - Percentage of NULL values in a column\n\n3) **Unique Percent** - Percentage of Unique values in a column\n\n4) **Categorical** - *0* : No, *1* : Yes\n\n5) **Correlation** - *0* : No, *1* : Yes, correlated with another column in the data frame\n\n6) **Sensitivity** - Pattern based Sensitivity analysed using RegEx - *0* : No, *1* : Yes\n\n7) **Class** - Target label for sensitivity\n","55d6b8c1":"# Data Analyzer","b4e16b80":"# Machine Learning to find Data Sensitivity","c16326e2":"# General Technique\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_workflow.png\" alt=\"drawing\" style=\"width:500px;\"\/>","b1cfe133":"**Reference:**\n1) https:\/\/ieeexplore.ieee.org\/document\/6868432 - 2014 - KNN (hardcoded) - 2 Class - RSA\n\n2) https:\/\/ieeexplore.ieee.org\/document\/8821135 - 2018 - Survey on 1)\n\n3) https:\/\/ieeexplore.ieee.org\/document\/8885114 - 3 Class - Frequency & Pattern - Distributed \n\n4) https:\/\/ieeexplore.ieee.org\/document\/7877709 - 2016 - AES- OnCloud - Password Authentication\n\n5) https:\/\/ieeexplore.ieee.org\/document\/8372705 - 2018 - AES Application - Delay\n\n6) https:\/\/www.semanticscholar.org\/paper\/Secure-Framework-Enhancing-AES-Algorithm-in-Cloud-Awan-Shiraz\/422d15a24e736a47f22e4b560a3bd92142b1476b - 2020 - Fast - Trusted Third Parties \n","f752fc5b":"# Cross Validation\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" alt=\"drawing\" style=\"width:500px;\"\/>","3bb393b9":"# Encryptor - AES"}}