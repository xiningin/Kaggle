{"cell_type":{"6520d78c":"code","3aac4d80":"code","c85a4b22":"code","65b6f4a4":"code","552a7f0e":"code","3dd09bfa":"code","0a6a6630":"code","b69cff7d":"code","d2d9dd7c":"code","3d18399b":"code","e2f935c4":"code","533d78a9":"code","6088fc73":"code","c97991fb":"code","d21e5e58":"code","75e1a29e":"code","a80638e9":"code","7a7b7a71":"code","c1ca582b":"code","51413a40":"code","7f8d94e4":"code","38bf8eda":"code","76ee1d8f":"code","3d4c4a67":"code","fb84628c":"code","b0ebf984":"code","cb1a09d1":"code","0f613964":"markdown","9bc229bb":"markdown","1250a22e":"markdown"},"source":{"6520d78c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  as ws\nws.filterwarnings(\"ignore\")","3aac4d80":"df = pd.read_csv(\"\/kaggle\/input\/diabetes-data-set\/diabetes-dataset.csv\")","c85a4b22":"df.head()","65b6f4a4":"sns.set()\nsns.countplot(df[\"Outcome\"])\nplt.show()","552a7f0e":"df.isna().sum()","3dd09bfa":"from sklearn.preprocessing import StandardScaler","0a6a6630":"# Split the data \nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns = \"Outcome\")\ny = df[\"Outcome\"]\nX_train , X_test, y_train, y_test = train_test_split (X, y, test_size = 0.2, random_state = 42, stratify = y)\n","b69cff7d":"# Scaling the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","d2d9dd7c":"# Fitting the model to the training data\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost","3d18399b":"from sklearn.model_selection import KFold, cross_val_score\nfor model in [ \n    DummyClassifier,\n    LogisticRegression,\n    DecisionTreeClassifier,\n    KNeighborsClassifier,\n    GaussianNB,\n    SVC,\n    RandomForestClassifier,\n    xgboost.XGBClassifier,\n]:\n    cls = model()\n    kf = KFold(n_splits = 5, random_state = 45)\n    score = cross_val_score(cls, X_train_scaled, y_train, cv = kf, scoring=\"roc_auc\")\n    print(\n        f\"{model.__name__:22}  AUC: \"\n        \n        f\"\\t {score.mean():.3f} STD: {score.std():.2f}\"\n        \n    )","e2f935c4":"# Withour any certain hyper param tuning the Random_Forest Model is best\n# leta get model working\n\nrfe = RandomForestClassifier(n_estimators=1000, random_state = 42)","533d78a9":"rfe.fit(X_train_scaled, y_train)","6088fc73":"# Evaluating the Random Forest Model\nprint (\"Accuacy on test set is \", round(rfe.score(X_test_scaled, y_test) * 100, 2), \"%\")","c97991fb":"from sklearn.metrics import precision_score\nprint(\"precision Score is \", round (precision_score(y_test, rfe.predict(X_test_scaled)) * 100 , 2))","d21e5e58":"print(\"Feature importance is \\n\" )\nfor i,j in zip(X_train.columns.to_list(), rfe.feature_importances_.tolist()):\n    if(i == \"DiabetesPedigreeFunction\"):\n        print (i , \"\\t \\t \", j)\n    elif(i==\"Age\" or i ==\"BMI\"):\n        print (i , \"\\t \\t \\t\\t\\t\", j)\n    else :   \n        print (i , \"\\t \\t \\t \\t\", j)","75e1a29e":"# As we seleted the model we can try the hyperparam tuning\nfrom sklearn.model_selection import GridSearchCV\n\nnew_rfe = RandomForestClassifier()\n\nparams = {\n     \"max_features\": [0.4, \"auto\"],\n     \"n_estimators\": [15, 200, 500, 1000],\n     \"min_samples_leaf\": [1, 0.1],\n     \"random_state\": [42],\n}\n\ncvs = GridSearchCV(new_rfe, params, n_jobs = -1).fit(X_train_scaled, y_train)","a80638e9":"print(cvs.best_score_)","7a7b7a71":"print(cvs.best_estimator_)","c1ca582b":"print(cvs.best_params_)","51413a40":"# fitting the model with best params\nrfe_final = RandomForestClassifier(\n**{'max_features': 0.4, 'min_samples_leaf': 1, 'n_estimators': 1000, 'random_state': 42}\n)","7f8d94e4":"rfe_final.fit(X_train_scaled, y_train)","38bf8eda":"y_pred = rfe_final.predict(X_test_scaled)","76ee1d8f":"print(precision_score(y_test, y_pred))","3d4c4a67":"from sklearn.metrics import confusion_matrix, classification_report","fb84628c":"sns.set()\nsns.heatmap(confusion_matrix(y_test, y_pred), annot =True)\nplt.show()","b0ebf984":"print(classification_report(y_test, y_pred))","cb1a09d1":"# print Roc_auc_score\nfrom sklearn.metrics import roc_auc_score\nprint(roc_auc_score(y_test, y_pred))","0f613964":"Please upvote it !! It wll motivate me to create content more like this","9bc229bb":"# This is Cool :) ","1250a22e":"Above graph clearly shows the class imbalance "}}