{"cell_type":{"62fe5774":"code","961d336b":"code","475a2e82":"code","4b46dc69":"code","ca70ac29":"code","8d65846c":"code","6a6bc4e3":"code","f845e0cb":"code","42aa43ed":"code","ff3a3511":"code","0b77af18":"code","d425ac89":"code","5e16ef6a":"code","a181bbbd":"code","d86bd9a0":"code","b86c896a":"code","af154bc6":"code","9566d143":"code","fea59ffb":"code","c2549cb6":"code","6cdae8c4":"code","72ea1698":"code","3ffeb7d9":"code","1a803d1d":"code","b9487675":"code","2ba86305":"markdown","332a28f0":"markdown","a05d69a7":"markdown","8f15f79c":"markdown","b772e4ff":"markdown","d87dbaab":"markdown","6b4429a6":"markdown","4fcad296":"markdown","df492e0b":"markdown","369dad89":"markdown","092e549e":"markdown","2b9a46f4":"markdown","3cf6ccfd":"markdown","1eed9d76":"markdown","94dfbeff":"markdown","50c7167a":"markdown","af04dd9c":"markdown","8bdee86e":"markdown","653ba743":"markdown","bc2b9724":"markdown","69cc6310":"markdown"},"source":{"62fe5774":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","961d336b":"#train_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\n#test_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","475a2e82":"# Helper functions\n# 1. For calculating % na values in  columns\ndef percent_na(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_df = pd.DataFrame({'column_name': percent_missing.index,\n                                 'percent_missing': percent_missing.values})\n    return missing_value_df\n# 2. For plotting grouped histograms \ndef sephist(col):\n    yes = train_transaction[train_transaction['isFraud'] == 1][col]\n    no = train_transaction[train_transaction['isFraud'] == 0][col]\n    return yes, no","4b46dc69":"# Helper function for column value details\n\ndef column_value_freq(df,sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','num_values_99'])\n    for col in sel_col:\n        col_value = df[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            num_col_99 = len(colpercount.loc[colpercount['per_count'] > (1- cum_per),])\n        else:\n            num_col_99 = len(colpercount.loc[colpercount['cum_per_count']< cum_per,] )\n        dfpercount=dfpercount.append({'col_name': col,'num_values_99': num_col_99},ignore_index = True)\n    dfpercount['unique_values'] = df[sel_col].nunique().values\n    dfpercount['unique_value_to_num_values_99_ratio'] = 100 * (dfpercount.num_values_99\/dfpercount.unique_values)\n    dfpercount['percent_missing'] = percent_na(df[sel_col])['percent_missing'].round(3).values\n    return dfpercount\n\ndef column_value_details(df,sel_col,cum_per):\n    dfpercount = pd.DataFrame(columns=['col_name','values_'+str(round(cum_per,2)),'values_'+str(round(1-cum_per,2))])\n    for col in sel_col:\n        col_value = df[col].value_counts(normalize=True)\n        colpercount = pd.DataFrame({'value' : col_value.index,'per_count' : col_value.values})\n        colpercount['cum_per_count'] = colpercount['per_count'].cumsum()\n        if len(colpercount.loc[colpercount['cum_per_count'] < cum_per,] ) < 2:\n            values_freq = colpercount.loc[colpercount['per_count'] > (1- cum_per),'value'].tolist()\n        else:\n            values_freq = colpercount.loc[colpercount['cum_per_count']< cum_per,'value'].tolist() \n        values_less_freq =  [item for item in colpercount['value'] if item not in values_freq]\n        dfpercount=dfpercount.append({'col_name': col,'values_'+str(round(cum_per,2)) : values_freq ,'values_'+str(round(1-cum_per,2)): values_less_freq},ignore_index = True)\n    return dfpercount","ca70ac29":"def col_unique(df,cols):\n    dat=df[cols].nunique()\n    sns.set(rc={'figure.figsize':(8,4)})\n    plot=sns.barplot(x=dat.index,y=dat.values)\n    for p in plot.patches:\n        plot.annotate(\"%d\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n               ha='center', va='top', fontsize=12, color='black', xytext=(0, 20),\n                 textcoords='offset points')\n    plot=plot.set(xlabel='Column ',ylabel= 'Number of unique values')","8d65846c":"Ccols= train_transaction.columns[train_transaction.columns.str.startswith('C')]\ntrain_transaction[Ccols].describe()","6a6bc4e3":"col_freq = column_value_freq(train_transaction,Ccols,0.965)\nsns.set(rc={'figure.figsize':(12,8)})\nplot=col_freq.plot(x='col_name',y='percent_missing',color='r')\nplot.set(ylabel='Percentage of missing values')\nax1=plot.twinx()\n#Dcol_freq['percent_missing'].plot(secondary_y=True, color='k', marker='o')\n#Dcol_freq['unique_value_to_num_values_99_ratio'].plot(secondary_y=True, color='r', marker='o')\nplot1=col_freq.plot(x='col_name',y=['unique_values','num_values_99'],ax=ax1,kind='bar')\nfor p in plot1.patches[1:]:\n    h = p.get_height()\n    x = p.get_x()+p.get_width()\/2.\n    if h != 0:\n        plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\nplot1.set(ylabel='Count')\nplot= plot.set(title='Data Details  in each C columns of train_transaction')\n","f845e0cb":"col_freq = column_value_freq(test_transaction,Ccols,0.965)\nsns.set(rc={'figure.figsize':(12,8)})\nplot=col_freq.plot(x='col_name',y='percent_missing',color='r')\nplot.set(ylabel='Percentage of missing values')\nax1=plot.twinx()\n#Dcol_freq['percent_missing'].plot(secondary_y=True, color='k', marker='o')\n#Dcol_freq['unique_value_to_num_values_99_ratio'].plot(secondary_y=True, color='r', marker='o')\nplot1=col_freq.plot(x='col_name',y=['unique_values','num_values_99'],ax=ax1,kind='bar')\nfor p in plot1.patches[1:]:\n    h = p.get_height()\n    x = p.get_x()+p.get_width()\/2.\n    if h != 0:\n        plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\nplot1.set(ylabel='Count')\nplot= plot.set(title='Data Details  in each C columns of test_transaction')","42aa43ed":"# cards=['card1','card2','card3','card4','card5','card6']\n# by = cards+['addr1']+Ccols.tolist()\n# group1=train_transaction.groupby(by,as_index=False)['TransactionID'].count()\n# group1.sort_values(by='TransactionID',ascending=False).head(30)","ff3a3511":"# pd.options.display.max_columns = None\n# Dcols =train_transaction.columns[train_transaction.columns.str.startswith('D')]\n# select=train_transaction.columns[1:55]\n# group1_details=pd.merge(group1,train_transaction[select],on=by,how='right')\n# #group1_details.sort_values(by=['TransactionID','TransactionDT'],ascending=False)\n# #group1_details[(group1_details.card1==16075) & (group1_details.TransactionID==60)]\n# group1_details[(group1_details[['D1','D2','D3']].notnull().all(1)) & (group1_details.TransactionID>30 )].head(5)\n# group1_details[(group1_details.card1==1342) & (group1_details.TransactionID==39)]","0b77af18":"pd.options.display.max_columns = None\nby=['card1','card2','card3','card4','card5','card6','addr1','C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11']\ncards_addr1_Ccolsgroup_count=train_transaction.groupby(by,as_index=False)['TransactionID'].count()\ncards_addr1_Ccolsgroup_count.rename(columns={\"TransactionID\": \"Count\"},inplace=True)\ncards_addr1_Ccolsgroup_count.sort_values(by='Count',ascending=False).head(30)","d425ac89":"print('Total number of groups: ',len(cards_addr1_Ccolsgroup_count))\nprint('Average number of transaction per group: ',len(train_transaction)\/len(cards_addr1_Ccolsgroup_count))","5e16ef6a":"cards_addr1_Ccolsgroup_count[cards_addr1_Ccolsgroup_count.card1==9885].sort_values(by='Count',ascending=False).head(30)","a181bbbd":"select=train_transaction.columns[0:55]\ncards_addr1_Ccolsgroup_details=pd.merge(cards_addr1_Ccolsgroup_count,train_transaction[select],on=by,how='left')\n#cards_addr1_Ccolsgroup_details.sort_values(by=['TransactionID','TransactionDT'],ascending=True)","d86bd9a0":"pd.options.display.max_rows = None\ncards_addr1_Ccolsgroup_details[(cards_addr1_Ccolsgroup_details.card1==9885) & (cards_addr1_Ccolsgroup_details.Count==64) ]","b86c896a":"train_transaction[(train_transaction.card1==9885) & (train_transaction.D2==181.0) ]","af154bc6":"Dcols= train_transaction.columns[train_transaction.columns.str.startswith('D')]","9566d143":"col_freq = column_value_freq(train_transaction,Dcols,0.965)\nsns.set(rc={'figure.figsize':(12,8)})\nplot=col_freq.plot(x='col_name',y='percent_missing',color='r')\nplot.set(ylabel='Percentage of missing values')\nax1=plot.twinx()\n#Dcol_freq['percent_missing'].plot(secondary_y=True, color='k', marker='o')\n#Dcol_freq['unique_value_to_num_values_99_ratio'].plot(secondary_y=True, color='r', marker='o')\nplot1=col_freq.plot(x='col_name',y=['unique_values','num_values_99'],ax=ax1,kind='bar')\nfor p in plot1.patches[1:]:\n    h = p.get_height()\n    x = p.get_x()+p.get_width()\/2.\n    if h != 0:\n        plot1.annotate(\"%g\" % p.get_height(), xy=(x,h), xytext=(0,4), rotation=90, \n                   textcoords=\"offset points\", ha=\"center\", va=\"bottom\")\nplot1.set(ylabel='Count')\nplot= plot.set(title='Data Details  in each D columns')","fea59ffb":"np.warnings.filterwarnings('ignore')\nsns.set(rc={'figure.figsize':(14,16)})\nfor num, alpha in enumerate(Dcols):\n    plt.subplot(5,3,num+1)\n    yes = train_transaction[(train_transaction['isFraud'] == 1)][alpha]\n    no = train_transaction[(train_transaction['isFraud'] == 0) ][alpha]\n    plt.hist(yes[yes>0], alpha=0.75, label='Fraud', color='r')\n    plt.hist(no[no>0], alpha=0.25, label='Not Fraud', color='g')\n    plt.legend(loc='upper right')\n    plt.title('Histogram of values  in column ' + str(alpha) )\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","c2549cb6":"cards_addr1_Ccolsgroup_details[(cards_addr1_Ccolsgroup_details[['D6','D7']].notnull().all(1)) & (cards_addr1_Ccolsgroup_details.Count >10)].head(30)","6cdae8c4":"pd.options.display.max_colwidth =300\nCcol_details=column_value_details(train_transaction,Ccols,0.965)\nnum_values_96 =[]\nfor i in range(len(Ccol_details)):\n    num_values_96.append(len(Ccol_details['values_0.96'][i]))\nCcol_details['num_values_96'] = num_values_96\nCcol_details","72ea1698":"C_cat = Ccol_details[Ccol_details['num_values_96'] <= 15].reset_index()\n","3ffeb7d9":"sns.set(rc={'figure.figsize':(14,16)})\nx=1\nfor num, alpha in enumerate(C_cat.col_name):\n    plt.subplot(len(C_cat),2,x)\n    sns.countplot(data=train_transaction[train_transaction[alpha].isin (C_cat['values_0.96'][num])],y=alpha,hue='isFraud')\n    plt.legend(loc='lower right',title='is Fraud')\n    plt.title('Count of unique values which make 96.5% of data in column ' + str(alpha) )\n    plt.subplot(len(C_cat),2,x+1)\n    yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (C_cat['values_0.04'][num]))][alpha]\n    no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (C_cat['values_0.04'][num]))][alpha]\n    plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n    plt.hist(no, alpha=0.25, label='Not Fraud', color='g')\n    plt.legend(loc='upper right')\n    plt.title('Histogram of values which make 3.5% of data in column ' + str(alpha) )\n    x= x+2\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","1a803d1d":"C_num = Ccol_details[Ccol_details['num_values_96'] > 15].reset_index()","b9487675":"sns.set(rc={'figure.figsize':(14,16)})\nx=1\nfor num, alpha in enumerate(C_num.col_name):\n    plt.subplot(len(C_num),2,x)\n    yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (C_num['values_0.96'][num]))][alpha]\n    no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (C_num['values_0.96'][num]))][alpha]\n    plt.hist(yes, alpha=0.75, label='yes', color='r')\n    plt.hist(no, alpha=0.25, label='no', color='b')\n    plt.legend(loc='upper right')\n    plt.title('Histogram of values which make 96.5% of data in column ' + str(alpha) )\n    plt.subplot(len(C_num),2,x+1)\n    yes = train_transaction[(train_transaction['isFraud'] == 1) & (train_transaction[alpha].isin (C_num['values_0.04'][num]))][alpha]\n    no = train_transaction[(train_transaction['isFraud'] == 0) & (train_transaction[alpha].isin (C_num['values_0.04'][num]))][alpha]\n    plt.hist(yes, alpha=0.75, label='Fraud', color='r')\n    plt.hist(no, alpha=0.25, label='Not Fraud', color='b')\n    plt.title('Histogram of values which make 3.5% of data in column ' + str(alpha) )\n    plt.legend(loc='upper right')\n    x= x+2\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","2ba86305":"There seems to be a few cases where the D3 value is noth edifference in days between succesive row. On case is where the d1 value is 236 and D2 value is 185 . D3 is 4 for this row . But the previous row D1& D2s value are 209 and 158. Hence instead of 4 the D3 value should have been 27 . So it looks like a row of values with D1=232 and D2=181 has missed out","332a28f0":"It looks a combination of Card =features card1-card 6 and C columns will help us to identify the unique payment cards (cards with unique 15 or 16 digit card numbers).\n\nAfter exploring various combiantion of features a combination  of ['card1' ,'card2','card3','card4','card5','card6', 'addr1','C1','C2' ,'C3', 'C4','C5','C6','C7','C8','C9','C10','C11']  shows some interesting patterns","a05d69a7":"**Unique Card Identifier**","8f15f79c":"From the above there is a fair uniform distribution of values in Dcolumns and these are truly numerical columns. histogram of data other than 0 and nulls are shown in the plot below.","b772e4ff":"###### For Columns C3,C4 ,C7 ,C8 ,C10 & C12 15 or less values make 96.5% of the column values . These are like categorical values in a sense. \n\nThe graph below shows a count plot of these categorical values which account for 96.5% values on the left and a histogram of remaining 3.5%  numeric values on the right.","d87dbaab":"From the above histogram D6,D7,D8,D12,D13,D14 seems to be number of days from some card event date .In any case thes columns have close to 90% null values. D9 is the day fraction of D8. This is confirmed by sample of data below.","6b4429a6":"The above row is the missing one and since it had null values for card2-card 6 it was not included in our grouping.\n\nThis throws up the possibility of imputing missing values in card2- card6 based on the feature groupings we identified as payment card identifier.\n","4fcad296":"Let's also look at test_transaction data set to verify whether the distribution of values are similiar","df492e0b":"**Exploring D Columns further**","369dad89":"The plot below shows the data distribution in D columns ","092e549e":"The graph below shows number of unique values in each of the C Columns as blue bars.Orange bar shows the number of  unique values in 96.5% of the data in each of the columns. The difference between the two bars is a measure of how distributed the data is across the range of unique values in the column. \n\nRed line is the percentage of missing values in  the columns.","2b9a46f4":"**Interesting to  note that none of the C columns have missing values**.\n\nAcross the range of values in each of the column  few values make up 96.5% of data in each of the columns compared to total unique values.\n","3cf6ccfd":"**Exploring C Columns further**","1eed9d76":"**Curiously almost 30% of values that fall in the 1% of data in these columns except C3 are part of Fraud transactions**","94dfbeff":"Taking as sample details of transactions with card1 =9885","50c7167a":"The table below shows for each column the values that make 96.5% of data in each column(values_0.99) and values that make the remaining 1% data in (values_0.01)","af04dd9c":"There seem to be no major differnce in proportion of fraud transactions between the two types. However it's interesting to note that the majority of values in these columns are in a narrow range of 0-20  in most of the cases except C13, even though maximum value in these most of columns exceed 3000.","8bdee86e":"From the above data for card  with  card1 = 9885 and number of transactions during the 6 month period = 64 it can easily be seen that D3 column is the difference in number of days for  succesive transaction values of D1 and D2.\n\nD5 values are almost same as D3 . But where D4 is null D5 is also null which means D5 is the difference in days of successive D4 values\n\nD1,D2,D4,D11,D15 are days from some card events as their values increase with time. D4 ,D11 and D15 appears to be the same value but D11 has some nulls. D10 values don't follow the time series and need further analysis.\n\nThe increase in the values of D1,D2 ,D4 ,D11 and D15 corresponds with increase in days of the TransactionDT values.\n\nThe difference between the value of D1 between the first and last transaction in this group is 174 days corresponding t a 6 month period.\n\nfrom the above this set of values appear to be  transactions of a specific card during the 6 month period.\n\n**Hence it's safe to assume that the combination of the features 'card1','card2','card3','card4','card5','card6', 'addr1', 'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10'and 'C11' can  uniquely identify the credit card.**","653ba743":"###### Let's now examine the remaining columns which are numeric in nature.\n\nThe graph below shows a histogram of  96.5% of column values on the left and a histogram of remaining 3.5%  values on the right.\n\n\n\n","bc2b9724":"Of the 394 features\/columns in the train_transaction data 15 columns begin in C .\nThe officaila explanation of these columns is.\n\n*C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.*\n\nAll C columns are of the numeric data type and summary is as below","69cc6310":"Interestingly C13 has more than 90% missing values and C14 has 20% missing values.  These features are potentially candidates to be dropped while building models."}}