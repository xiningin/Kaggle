{"cell_type":{"aba5b840":"code","18f1637f":"code","3d2c4406":"code","31039397":"code","c236d64e":"code","716b3680":"code","3e9884e1":"code","d1d234f7":"code","93408095":"code","a54806d5":"code","c3fe04bb":"code","56a3d3df":"code","3b6c50ea":"code","b7418bdd":"code","6c0731a4":"code","0fecef2d":"code","e88de521":"code","6ac12390":"code","2b63b699":"code","e4279a54":"code","0f09f544":"code","3b784f3b":"code","f1f9350d":"code","fb362c89":"code","425c49a8":"code","33f56284":"code","8c292d9d":"code","0b166ddb":"code","a1921335":"code","23c91eec":"code","b6a440a9":"code","53cec69a":"code","29eebde6":"code","590ba8d3":"code","d01d6179":"code","ac2a6513":"code","ce315b95":"code","93e3b666":"code","95c9562f":"code","e4f6c291":"code","7303b78a":"code","4a482117":"code","5e488f1c":"code","70439959":"code","bf39dbda":"code","aa7f0ada":"code","7ba372da":"code","ee3d6682":"code","add77784":"code","70446770":"code","74d38277":"code","199f9661":"code","8db37813":"markdown","2c702844":"markdown","3dde00f9":"markdown","b58ff6da":"markdown","8701cbd1":"markdown","3bd392b1":"markdown","16c416d8":"markdown","4971ac27":"markdown","1afc3a38":"markdown","7f95aee4":"markdown","e66f2186":"markdown","50b938c1":"markdown","400e31a0":"markdown","198ae286":"markdown","62e20ce9":"markdown","3e3b32b8":"markdown","d219243c":"markdown","6578a7f3":"markdown","eb69c04d":"markdown","0df138e6":"markdown","ef35b4a0":"markdown","976b67e9":"markdown","e1ab0b05":"markdown","2a06c2d6":"markdown","f60ce034":"markdown","dd837813":"markdown","827c5f7b":"markdown","9589d5e4":"markdown","d49cd552":"markdown","7d59415f":"markdown","f7b43955":"markdown","eddd28d8":"markdown","3f6552dc":"markdown","44d7096e":"markdown","42387cb9":"markdown","9cdbb12d":"markdown","f8b06be2":"markdown","8a6d81cd":"markdown","fdea3a1c":"markdown","899584db":"markdown","9206d570":"markdown","c43145b1":"markdown"},"source":{"aba5b840":"!pip install reverse-geocode","18f1637f":"import folium\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport plotly.express as px\nimport reverse_geocode\nimport seaborn as sns\nimport shutil\n\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import MaxPooling2D, Conv2D, Dense, Flatten, Dropout, GlobalAveragePooling2D\nfrom keras.models import Sequential, load_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\n\nfrom pathlib import Path\nfrom PIL.ExifTags import TAGS, GPSTAGS\nfrom PIL import Image\n\nfrom skimage.feature import hog\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize\nfrom skimage import exposure\n\nfrom sklearn.model_selection import train_test_split","3d2c4406":"# if using gpu - confirm\nimport tensorflow as tf\ntf.test.gpu_device_name()","31039397":"np.random.seed(1)\ntf.random.set_seed(1)","c236d64e":"def extract_exif(filename):\n    \"\"\" Extract img EXIF data \"\"\"\n    image = Image.open(filename)\n    image.verify()\n    return image._getexif()\n\n\ndef extract_exif_labelled(exif_data):\n    \"\"\" Extract EXIF data with formatted labels \"\"\"\n    labelled_data = {}\n    for (key, val) in exif_data.items():\n        labelled_data[TAGS.get(key)] = val\n    return labelled_data\n\n\ndef extract_geotags(exif_data):\n    \"\"\" Obtain better formatted geotag data \"\"\"\n    if not exif_data:\n        raise ValueError(\"EXIF metadata not found.\")\n    geotags = {}\n    for (idx, geotag) in TAGS.items():\n        if geotag == 'GPSInfo':\n            if idx not in exif_data:\n                raise ValueError(\"No EXIF geotagging found\")\n            for (key, val) in GPSTAGS.items():\n                if key in exif_data[idx]:\n                    geotags[val] = exif_data[idx][key]\n    return geotags\n\n\ndef lat_long_alt_from_geotag(geotags):\n    \"\"\" Obtain decimal lat, long and altitude from geotags \"\"\"\n    lat = dms_to_decimal(geotags['GPSLatitudeRef'], geotags['GPSLatitude'])\n    long = dms_to_decimal(geotags['GPSLongitudeRef'], geotags['GPSLongitude'])\n    \n    # obtain altitude data and process, if it exists\n    altitude = None\n    try:\n        alt = geotags['GPSAltitude']\n        altitude = alt[0] \/ alt[1]\n        \n        # multiple by -1 if below sea level\n        if geotags['GPSAltitudeRef'] == 1: \n            altitude *= -1\n    except KeyError:\n        altitude = 0\n  \n    return lat, long, altitude\n\n\ndef dms_to_decimal(lat_long_ref, deg_min_sec):\n    \"\"\" Convert degrees, minutes, seconds tuples into decimal\n        lat and lon values. Given to 5 decimal places - more \n        than sufficient for commercial GPS \"\"\"\n    \n    degrees = deg_min_sec[0][0] \/ deg_min_sec[0][1]\n    minutes = deg_min_sec[1][0] \/ deg_min_sec[1][1] \/ 60.0\n    seconds = deg_min_sec[2][0] \/ deg_min_sec[2][1] \/ 3600.0\n    \n    if lat_long_ref in ['S', 'W']:\n        degrees = -degrees\n        minutes = -minutes\n        seconds = -seconds\n        \n    return round(degrees + minutes + seconds, 5)","716b3680":"example_filename = '\/kaggle\/input\/geolocated-imagery-dataset-scotland\/300-399\/322.jpg'\nexif = extract_exif(example_filename)\nlabeled = extract_exif_labelled(exif)\n\nfor key, val in labeled.items():\n    print(f\"{key} : {val}\")","3e9884e1":"geo_data = extract_geotags(exif)\n\nfor key, val in geo_data.items():\n    print(f\"{key} : {val}\")","d1d234f7":"coords = lat_long_alt_from_geotag(geo_data)\nprint(coords)","93408095":"img_name, img_path = [], []\nlatitudes, longitudes, altitudes = [], [], []\nimg_width, img_height = [], []\nmakes, models = [], []\ntime_dates = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    \n    for filename in filenames:\n        \n        if filename.endswith('.jpg'):\n        \n            file_path = os.path.join(dirname, filename)\n        \n            exif_data = extract_exif(file_path)\n            exif_labels = extract_exif_labelled(exif_data)\n            geo_data = extract_geotags(exif_data)\n            lat, long, alt = lat_long_alt_from_geotag(geo_data)\n        \n            img_name.append(filename)\n            img_path.append(file_path)\n            latitudes.append(lat)\n            longitudes.append(long)\n            altitudes.append(alt)\n            img_width.append(exif_labels.get('ImageWidth', 0))\n            img_height.append(exif_labels.get('ImageLength', 0))\n            makes.append(exif_labels.get('Make', 'Unknown'))\n            models.append(exif_labels.get('Model', 'Unknown'))\n            time_dates.append(exif_labels.get('DateTime', 0))","a54806d5":"# reverse geocode our coordinates for the city\ncoord_pairs = [(lat,long) for lat, long in zip(latitudes, longitudes)]\ncities = [x['city'] for x in reverse_geocode.search(coord_pairs)]","c3fe04bb":"metadata_df = pd.DataFrame({ 'filename' : img_name, 'filepath' : img_path, \n                             'img_width' : img_width, 'img_height' : img_height, \n                             'make' :makes, 'model' : models,\n                             'latitude' : latitudes, 'longitude' : longitudes, \n                             'altitude' : altitudes, 'time_date' : time_dates,\n                             'city' : cities})\n\nmetadata_df.head()","56a3d3df":"metadata_df['img_height'].value_counts()","3b6c50ea":"plt.figure(figsize=(10,5))\nmetadata_df['altitude'].plot()\nplt.ylabel(\"Altitude (m)\")\nplt.xlabel(\"Image number\")\nplt.show()","b7418bdd":"date_index_df = metadata_df.copy()\ndate_index_df['Date'] = pd.to_datetime(date_index_df['time_date'], format='%Y:%m:%d %H:%M:%S')\ndate_index_df.sort_values(by=['Date'], inplace=True, ascending=True)\ndate_index_df.reset_index(inplace=True, drop=True)\n\nplt.figure(figsize=(10,5))\ndate_index_df['altitude'].plot()\nplt.ylabel(\"Altitude (m)\")\nplt.xlabel(\"Image number\")\nplt.show()","6c0731a4":"plt.figure(figsize=(12,6))\nvalues = metadata_df['city'].value_counts()\nsns.barplot(x = values.index.values, y = values.values)\nplt.xticks(rotation=90)\nplt.show()","0fecef2d":"# obtain towns \/ cities with top ten image counts\nvalues = metadata_df['city'].value_counts()[:10]\n\nplt.figure(figsize=(10,5))\nsns.barplot(x = values.index.values, y = values.values)\nplt.xticks(rotation=90)\nplt.show()\n\ntop_towns = list(values.index.values)\ntop_towns","e88de521":"fig = plt.figure(figsize=(12, 6))\n\nfor i, example in enumerate(top_towns):\n    \n    ax = fig.add_subplot(2, 5, i+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    class_imgs = metadata_df[metadata_df['city'] == example]\n    example_img_path = class_imgs.iloc[0]['filepath']\n    \n    example_img = imread(example_img_path)\n    \n    ax.imshow(example_img)\n    \n    ax.set_xlabel(example)","6ac12390":"def plot_data_coords(row):\n    folium.Circle(location=[row.latitude, row.longitude],\n                  color='crimson',\n                  tooltip = \"<h5 style='text-align:center;font-weight: bold'>Img Name : \"+row.filename+\"<\/h5>\"+\n                            \"<hr style='margin:10px;'>\"+\n                            \"<ul style='color: #444;list-style-type:circle;align-item:left;\"+\n                            \"padding-left:20px;padding-right:20px'>\"+\n                            \"<li>Town : \"+str(row.city)+\"<\/li>\"+\n                            \"<li>Lat : \"+str(row.latitude)+\"<\/li>\"+\n                            \"<li>Long : \"+str(row.longitude)+\"<\/li>\"+\n                            \"<li>Altitude : \"+str(row.altitude)+\"<\/li>\"+\n                            \"<li>Time date : \"+str(row.time_date)+\"<\/li><\/ul>\",\n                  radius=20, weight=6).add_to(m)\n\n    \nm = folium.Map(location=[metadata_df['latitude'].mean(), \n                         metadata_df['longitude'].mean()], \n               tiles='OpenStreetMap',\n               min_zoom=7, max_zoom=12, zoom_start=7.5)\n\n\n# iterate through all rows and plot coords\nmetadata_df.apply(plot_data_coords, axis = 1)\n\nm","2b63b699":"colors = ['red', 'blue', 'gray', 'darkred', 'black', 'orange', 'beige', 'green', \n          'purple', 'lightgreen', 'darkblue', 'lightblue', 'darkgreen', 'darkpurple',\n          'lightred', 'cadetblue', 'lightgray', 'pink']\n\n# dict comp to form unique color for each town\ntown_colors = { town : color for town, color in zip(top_towns, colors[:len(top_towns)]) }\n\n# select only data containing our selected towns \/ cities\ntop_towns_df = metadata_df[metadata_df['city'].isin(top_towns)]","e4279a54":"def plot_top_towns(row):\n    \n    marker_colour = town_colors[row['city']]\n    \n    folium.Circle(location=[row.latitude, row.longitude],\n                  color=marker_colour,\n                  tooltip = \"<h5 style='text-align:center;font-weight: bold'>Img Name : \"+row.filename+\"<\/h5>\"+\n                            \"<hr style='margin:10px;'>\"+\n                            \"<ul style='color: #444;list-style-type:circle;align-item:left;\"+\n                            \"padding-left:20px;padding-right:20px'>\"+\n                            \"<li>Town : \"+str(row.city)+\"<\/li>\"+\n                            \"<li>Lat : \"+str(row.latitude)+\"<\/li>\"+\n                            \"<li>Long : \"+str(row.longitude)+\"<\/li>\"+\n                            \"<li>Altitude : \"+str(row.altitude)+\"<\/li>\"+\n                            \"<li>Time date : \"+str(row.time_date)+\"<\/li><\/ul>\",\n                  radius=20, weight=6).add_to(m)\n    \nm = folium.Map(location=[top_towns_df['latitude'].mean(), \n                         top_towns_df['longitude'].mean()], \n               tiles='OpenStreetMap',\n               min_zoom=7, max_zoom=12, zoom_start=7.5)\n\n# iterate through all rows and plot coords\ntop_towns_df.apply(plot_top_towns, axis = 1)\n\nm","0f09f544":"# obtain our data classes (output labels) using top towns from the data\nclasses = [town.lower() for town in top_towns]\n\n# create our new directories - pathlib Path to avoid preexisting errors\nbase_dir = os.path.join(os.getcwd(), 'Base_Data')\nPath(base_dir).mkdir(parents=True, exist_ok=True)\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'validation')\ntest_dir = os.path.join(base_dir, 'test')\n\n# form each of the dirs above\nfor directory in [train_dir, validation_dir, test_dir]:\n    Path(directory).mkdir(parents=True, exist_ok=True)\n    \n# create sub-directories for each town class for train, val and test dirs\nfor town_class in classes:\n    # create sub-directories within training directory\n    current_train_dir = os.path.join(train_dir, town_class)\n    Path(current_train_dir).mkdir(parents=True, exist_ok=True)\n    \n    # repeat for validation dir\n    current_val_dir = os.path.join(validation_dir, town_class)\n    Path(current_val_dir).mkdir(parents=True, exist_ok=True)\n    \n    # repeat for test dir\n    current_test_dir = os.path.join(test_dir, town_class)\n    Path(current_test_dir).mkdir(parents=True, exist_ok=True)","3b784f3b":"# create training, validation and test splits for all images using the file paths\nX_path = top_towns_df['filepath'].values\ny = top_towns_df['city'].values\n\n# first split - training + validation split combined, and seperate 10% test split.\nX_train_val_paths, X_test_paths, y_train, y_test = train_test_split(X_path, y, \n                                                                    test_size=0.1, \n                                                                    random_state=1, \n                                                                    stratify=y)\n\n# second split - 75% training and 25% validation data\nX_train_paths, X_val_paths, y_train, y_val = train_test_split(X_train_val_paths, \n                                                              y_train, \n                                                              test_size=0.25, \n                                                              random_state=1, \n                                                              stratify=y_train)","f1f9350d":"# get counts of class labels within each split\ntrg_towns, trg_counts =  np.unique(y_train, return_counts=True)\nval_towns, val_counts =  np.unique(y_val, return_counts=True)\ntest_towns, test_counts =  np.unique(y_test, return_counts=True)\n\n# plot number of classes within each data split for confirmation\nfig = plt.figure(figsize=(12, 4))\nsplit_types = ['Training', 'Validation', 'Test']\n\nfor i, data_split in enumerate([trg_counts, val_counts, test_counts]):\n    \n    ax = fig.add_subplot(1, 3, i+1)\n    sns.barplot(x = trg_towns, y = data_split)\n    plt.xticks(rotation=90)\n    plt.title(split_types[i])\n\nplt.show()","fb362c89":"!ls \/kaggle\/working\/Base_Data\/train\/","425c49a8":"%%time\n\n# copy training data\nfor i, img_location in enumerate(X_train_paths):\n    class_label = y_train[i].lower()\n    img_name = f\"train_{i}.jpg\"\n    src_loc = img_location\n    dest_loc = os.path.join(train_dir, class_label, img_name)\n    \n    # resize img and then move\n    img = Image.open(src_loc)\n    img_new = img.resize((504,378), Image.ANTIALIAS)\n    img_new.save(dest_loc, 'JPEG', quality=90)\n    \n    # move img without resizing using shutil\n    #_ = shutil.copyfile(src_loc, dest_loc)\n\n# copy validation data\nfor i, img_location in enumerate(X_val_paths):\n    class_label = y_val[i].lower()\n    img_name = f\"validation_{i}.jpg\"\n    src_loc = img_location\n    dest_loc = os.path.join(validation_dir, class_label, img_name)\n    \n    # resize img and then move\n    img = Image.open(src_loc)\n    img_new = img.resize((504,378), Image.ANTIALIAS)\n    img_new.save(dest_loc, 'JPEG', quality=90)\n    \n    #_ = shutil.copyfile(src_loc, dest_loc)\n    \n# copy test data\nfor i, img_location in enumerate(X_test_paths):\n    class_label = y_test[i].lower()\n    img_name = f\"test_{i}.jpg\"\n    src_loc = img_location\n    dest_loc = os.path.join(test_dir, class_label, img_name)\n    \n    # resize img and then move\n    img = Image.open(src_loc)\n    img_new = img.resize((504,378), Image.ANTIALIAS)\n    img_new.save(dest_loc, 'JPEG', quality=90)\n    \n    #_ = shutil.copyfile(src_loc, dest_loc)","33f56284":"img_height, img_width = 299, 299\nbatch_size = 10\n\n# training data augmentation - rotate, shear, zoom and flip\ntrain_datagen = ImageDataGenerator(\n    rotation_range = 30,\n    rescale = 1.0 \/ 255.0,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    vertical_flip=True)\n\n# no augmentation for test data - only rescale\ntest_datagen = ImageDataGenerator(rescale = 1. \/ 255.0)\n\n# generate batches of augmented data from training data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# generate val data from val dir\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nnb_train_samples = len(train_generator.classes)\nnb_validation_samples = len(validation_generator.classes)\n\n# create pandas dataframes for our train data\ntraining_data = pd.DataFrame(train_generator.classes, columns=['classes'])\ntesting_data = pd.DataFrame(validation_generator.classes, columns=['classes'])","8c292d9d":"def create_CNN():\n    \"\"\" Basic CNN with 4 Conv layers, each followed by a max pooling \"\"\"\n    cnn_model = Sequential()\n    \n    # four Conv layers with max pooling\n    cnn_model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(299, 299, 3)))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    cnn_model.add(Conv2D(128, (3, 3), activation='relu'))\n    cnn_model.add(MaxPooling2D(2, 2))\n    \n    # flatten output and feed to dense layer, via dropout layer\n    cnn_model.add(Flatten())\n    cnn_model.add(Dropout(0.5))\n    cnn_model.add(Dense(512, activation='relu'))\n    \n    # add output layer - softmax with 10 outputs\n    cnn_model.add(Dense(10, activation='softmax'))\n    \n    cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return cnn_model","0b166ddb":"CNN_model = create_CNN()\nCNN_model.summary()","a1921335":"#history = CNN_model.fit_generator(train_generator, epochs=30, \n#                                  validation_data=validation_generator, shuffle=True)","23c91eec":"# save model as a HDF5 file with weights + architecture\n#CNN_model.save('Basic_CNN_model_1.h5')\n\n# save the history of training to a datafile for later retrieval\n#with open('train_history_basic_CNN_model_1.pickle', 'wb') as pickle_file:\n#        pickle.dump(history.history, pickle_file)","b6a440a9":"# if already trained - import history file and training weights\nCNN_model = load_model('\/kaggle\/input\/basic-cnn-model\/Basic_CNN_model_1.h5')\n\n# get history of trained model\nwith open('\/kaggle\/input\/basic-cnn-model\/train_history_basic_CNN_model_1.pickle', 'rb') as handle:\n    history = pickle.load(handle)","53cec69a":"hist_dict_1 = history\n\ntrg_loss = hist_dict_1['loss']\nval_loss = hist_dict_1['val_loss']\n\ntrg_acc = hist_dict_1['accuracy']\nval_acc = hist_dict_1['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(12,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.show()","29eebde6":"test_generator = test_datagen.flow_from_directory(test_dir, target_size=(299, 299), \n                                                  batch_size=4, class_mode='categorical')\n\ntest_loss, test_accuracy = CNN_model.evaluate_generator(test_generator)\nprint(f\"Test accuracy: {test_accuracy}\")","590ba8d3":"from keras.applications import xception","d01d6179":"# create our pretrained convolutonal base from xception\nconv_base = xception.Xception(weights='imagenet', include_top=False)","ac2a6513":"conv_base.summary()","ce315b95":"for layer in conv_base.layers:\n  layer.trainable = False","93e3b666":"tl_xception = Sequential()\n\n# add pre-trained xception base\ntl_xception.add(conv_base)\n\n# flatten and add dense layer, with dropout\ntl_xception.add(GlobalAveragePooling2D())\ntl_xception.add(Dropout(0.5))\ntl_xception.add(Dense(256, activation='relu'))\n\n# output softmax, with 10 classes\ntl_xception.add(Dense(10, activation='softmax'))\n\ntl_xception.compile(loss='categorical_crossentropy', \n                    optimizer='adam', \n                    metrics=['accuracy'])","95c9562f":"# set up a check point for our model - save only the best val performance\nsave_path =\"tl_xception_1_best_weights.hdf5\"\n\ntrg_checkpoint = ModelCheckpoint(save_path, monitor='val_accuracy', \n                                 verbose=1, save_best_only=True, mode='max')\n\ntrg_callbacks = [trg_checkpoint]","e4f6c291":"# batch steps before an epoch is considered complete (trg_size \/ batch_size):\nsteps_per_epoch = np.ceil(nb_train_samples\/batch_size)\n\n# validation batch steps (val_size \/ batch_size):\nval_steps_per_epoch = np.ceil(nb_validation_samples\/batch_size)","7303b78a":"#history = tl_xception.fit(train_generator, epochs=25, \n#                          steps_per_epoch=steps_per_epoch, \n#                          validation_data=validation_generator, \n#                          validation_steps=val_steps_per_epoch,\n#                          callbacks=trg_callbacks,\n#                          shuffle=True)","4a482117":"# save model as a HDF5 file with weights + architecture\n#tl_xception.save('tl_xception_1.h5')\n\n# save the history of training to a datafile for later retrieval\n#with open('tl_xception_history_1.pickle', \n#          'wb') as pickle_file:\n#        pickle.dump(history.history, pickle_file)\n\nloaded_model = False","5e488f1c":"# if already trained - import history file and training weights\ntl_xception = load_model('\/kaggle\/input\/inception-transfer-learning-model\/tl_xception_1_model.hdf5')\n\n# get history of trained model\nwith open('\/kaggle\/input\/inception-transfer-learning-model\/tl_xception_history_1.pickle', 'rb') as handle:\n    history = pickle.load(handle)\n    \nloaded_model = True","70439959":"# if loaded model set history accordingly\nif loaded_model:\n    hist_dict_2 = history\nelse:\n    hist_dict_2 = history.history\n\ntrg_loss = hist_dict_2['loss']\nval_loss = hist_dict_2['val_loss']\n\ntrg_acc = hist_dict_2['accuracy']\nval_acc = hist_dict_2['val_accuracy']\n\nepochs = range(1, len(trg_acc) + 1)\n\n# plot losses and accuracies for training and validation \nfig = plt.figure(figsize=(14,6))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss, marker='o', label='Training Loss')\nplt.plot(epochs, val_loss, marker='x', label='Validation Loss')\nplt.title(\"Training \/ Validation Loss\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\n\nax = fig.add_subplot(1, 2, 2)\nplt.plot(epochs, trg_acc, marker='o', label='Training Accuracy')\nplt.plot(epochs, val_acc, marker='^', label='Validation Accuracy')\nplt.title(\"Training \/ Validation Accuracy\")\nax.set_ylabel(\"Accuracy\")\nax.set_xlabel(\"Epochs\")\nplt.legend(loc='best')\nplt.show()","bf39dbda":"test_generator = test_datagen.flow_from_directory(test_dir, \n                                                  target_size=(299, 299), \n                                                  batch_size=5, \n                                                  class_mode='categorical')\n\ntest_loss, test_accuracy = tl_xception.evaluate(test_generator, steps=10)\nprint(f\"Test accuracy: {test_accuracy}\")","aa7f0ada":"# get class labels dict containing index of each class for decoding predictions\nclass_labels = train_generator.class_indices\n\n# obtain a reverse dict to convert index into class labels\nreverse_class_index = {i : class_label for class_label, i in class_labels.items()}","7ba372da":"def process_and_predict_img(image_path, model):\n    \"\"\" Utility function for making predictions for an image. \"\"\"\n    img_path = image_path\n    img = image.load_img(img_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = test_datagen.standardize(x)\n    predictions = model.predict(x)\n    return img, predictions","ee3d6682":"def top_n_predictions(predict_probs, top_n_labels=3):\n    \"\"\" Obtain top n prediction indices for array of predictions \"\"\"\n    top_indices = np.argpartition(predict_probs[0], -top_n_labels)[-top_n_labels:]\n    \n    # negate prediction array to sort in descending order\n    sorted_top = top_indices[np.argsort(-predict_probs[0][top_indices])]\n    \n    # dict comp to create dict of labels and probs\n    labels = {\"label_\" + str(i + 1) : (reverse_class_index[index].capitalize(), \n                                       predict_probs[0][index]) for i, index in enumerate(sorted_top)}\n    return labels","add77784":"img, prediction = process_and_predict_img(test_dir + '\/caol\/test_26.jpg', \n                                          model=tl_xception)\ntop_labels = top_n_predictions(prediction, \n                               top_n_labels=3)\nplt.imshow(img)\nplt.title(f\"Location: {top_labels['label_1'][0]}\")\nplt.show()\n\nprint(\"Top predictions:\")\nfor label in top_labels:\n    print(\"- {0}: {1:.2f}%\".format(top_labels[label][0], top_labels[label][1] * 100))","70446770":"example_test_i = np.random.permutation(len(y_test))[:12]\nexample_test_img = X_test_paths[example_test_i]\nexample_test_y = y_test[example_test_i]\n\n# create fig to display 12 different predictions\nfig = plt.figure(figsize=(15,9))\nimg_num = 0\n\nfor i in range(12):\n    ax = fig.add_subplot(3, 4, img_num + 1)\n    \n    img_path = example_test_img[img_num]\n    \n    # make prediction on image - select desired model (e.g. CNN_basic, or tl_xception)\n    img, predictions = process_and_predict_img(img_path, model=tl_xception)\n    top_labels = top_n_predictions(predictions, top_n_labels=3)\n    \n    prediction_string = \"\"\n    for label in top_labels:\n        prediction_string += f\"- {top_labels[label][0]}: {top_labels[label][1]*100:.2f}% \\n\"\n    \n    ax.imshow(img)\n    \n    #title = reverse_class_index[np.argmax(predictions,axis=-1)[0]].capitalize()\n    ax.set_title(f\"Town: {example_test_y[img_num]}\")\n    ax.set_xlabel(f\"Predictions: \\n{prediction_string}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    img_num += 1\n\nplt.tight_layout()\nplt.show()","74d38277":"# create fig to display 12 different predictions\nfig = plt.figure(figsize=(15,9))\nimg_num = 0\n\nfor i in range(12):\n    ax = fig.add_subplot(3, 4, img_num + 1)\n    \n    img_path = example_test_img[img_num]\n    \n    # make prediction on image - select desired model (e.g. CNN_basic, or tl_xception)\n    img, predictions = process_and_predict_img(img_path, model=CNN_model)\n    top_labels = top_n_predictions(predictions, top_n_labels=3)\n    \n    prediction_string = \"\"\n    for label in top_labels:\n        prediction_string += f\"- {top_labels[label][0]}: {top_labels[label][1]*100:.2f}% \\n\"\n    \n    ax.imshow(img)\n    \n    #title = reverse_class_index[np.argmax(predictions,axis=-1)[0]].capitalize()\n    ax.set_title(f\"Town: {example_test_y[img_num]}\")\n    ax.set_xlabel(f\"Predictions: \\n{prediction_string}\")\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    img_num += 1\n\nplt.tight_layout()\nplt.show()","199f9661":"try:\n    shutil.rmtree(base_dir)\nexcept OSError as e:\n    print(\"Error: %s : %s\" % (base_dir, e.strerror))","8db37813":"### Processing data into a form suitable for deep learning\n\nSince all the images were taken in Scotland, it is not feasible to classify images by Country, since they are all the same. However, a potential option is to train a model to classify by County, City or Town. Another option could be to classify location based on location proximity, which could be achieved using a clustering algorithm to assign labels.\n\nFor this work, we'll classify images based on the town the image was taken in. This will be achieved by selected only the top n towns analysed above. Before we can do this, we need to further process our data. This will involve the following steps: \n\n1. Seperate our data according to the classification we want - in this case, we'll seperate the data by the town the image was taken in. This should be simple using the existing meta-data dataframe we created earlier.\n\n2. Creation of a new set of directories: training, validation and test. Each of these directories will require n sub-directories for each of the n class labels (towns).\n\n3. Selection of splits for training, validation and testing from the original dataset, and movement of these into the applicable new directories created in the previous step.\n\n4. Pre-processing our data with the following: conversion from JPEG into RGB pixel grids formatted as floating-point tensors, rescaling and standardisation of our image pixel values so that they lie between 0 and 1.\n\nThe formatting of our data needs to be organised into a specific directory format so that we can apply useful tools such as data augmentation and transformations to our data prior to feeding into a machine learning model.","2c702844":"#### Visualise an example image from each of the top ten towns \/ cities","3dde00f9":"Sort dataset by date and visualise change in altitude as each image was taken:","b58ff6da":"###### Prediction on a single example from the test set","8701cbd1":"#### Quick visualisation of dataset features for interest\n\nAltitude of the images taken in the dataset:","3bd392b1":"#### Determine city for each coordinate","16c416d8":"We've only imported the xception network without its top fully-connected layers, since we want to design our own final layers suitable for our geolocation problem. \n\nWe need to ensure that we have frozen all the existing layers of the Xception model, which is highly trained on the pre-existing imagenet data.","4971ac27":"If already trained and we have an existing weight and history file, run the following:","1afc3a38":"---\n## 2. Formation of a Deep Convolutional Neural Network\n\nWe need to take our dataset of jpeg images and preprocess them accordingly prior to use in our deep learning model. In summary, we need to perform the following:\n\n- Read in each of our images as a jpeg file\n- Decode each image into floating-point tensor form, with RGB grids of pixels for each image\n- Standardise our images through rescaling of the pixel values.\n\nThese functions are performed automatically using the data generators we created previously. For our first convolutional neural network, we'll form a custom smaller sized ConvNet.","7f95aee4":"### Extract EXIF metadata from all images and form a dataset","e66f2186":"### Extract EXIF metadata including GPS locations and other details from all images","50b938c1":"This more complex model actually performs worse than the initial ConvNet produced. Its likely that with some further work, research and tinkering of the transfer learning model the performance could be made much better. \n\nIn addition, we could perform fine-tuning of the actual Convolutional Layers within the Xception (or other chosen base convolutional layer network). This would allow our model to better generalise to the types of images we are using within the geolocation dataset.\n\nAll things considered, we are always going to struggle to effectively classify with this dataset due to the limited number of training samples. Through collection of many more geolocated images with a wider range of examples and locations, I'm sure we could produce a much more impressive model.","400e31a0":"###### Plot number of classes within each data split","198ae286":"Lets set up checkpointing to save the best performance found on the val dataset:","62e20ce9":"Set random seed for the work used throughout this notebook:","3e3b32b8":"We're very limited with test cases for some of our classes (i.e. Aberlour and Sandbank). We're also not very well balanced, with some classes having many more images for training \/ testing compared to others. Despite these issues, this distribution of data for training, validation and testing will suffice for now.","d219243c":"### Annotate image locations on a map of Scotland using Folium","6578a7f3":"###### Resize and move images from original directory into associated training, validation and test directories","eb69c04d":"---\n## 1. Dataset exploration and pre-processing","0df138e6":"#### Choose an image as an example, and display metadata","ef35b4a0":"###### Form splits of our data using the file paths obtained for each image in the meta-data dataframe previously.","976b67e9":"For interest, we can also get back the top *n* predictions to see how certain (or uncertain) or model was in making a given prediction:","e1ab0b05":"Caol is located near Fort William and as such is very picturesque - no wonder the most images throughout the dataset were taken here!\n\nWith the large number of towns\/cities with insignificant numbers of images when compared to the top ten (greater than 20 images), it is worth only selecting the top n cities with a similar number of images. This avoids biasing the training of our dataset.","2a06c2d6":"#### Form dataframe for all our imagery metadata","f60ce034":"---\n## 3. Improving our model using transfer learning\n\nFor this we'll obtain a highly trained convolutional base using an existing xception network. We'll take this and train additional layers on top tailored to our image geolocation problem, which should hopefully provide a boost to the performance obtained from the simple ConvNet produced ealier.","dd837813":"###### Plot top ten town \/ city locations, with a different colour for each on the map:","827c5f7b":"---\n\n## Finally - remove dataset from Kaggle output directory to prevent thousands of images being output (uncomment code if you'd like to keep \/ download these)","9589d5e4":"Visualise the number of images taken in each town \/ city:","d49cd552":"###### Evaluate on the test dataset","7d59415f":"###### Evaluate on the test dataset","f7b43955":"---\n## 4. Visualising predictions from the test data with our model(s)\n\nTo decode one-hot encoded predictions back into class labels, we need to create a lookup dictionary from the decoded class labels, as follows:","eddd28d8":"###### Plot all points, regardless of location:","3f6552dc":"In the following section, we'll plot the locations of all images on a map of Scotland to give us an appreciation of the variation of geographical images we have within the dataset.","44d7096e":"###### **Only run the following once on a GPU - after this just import the training weights and history file to avoid having to re-train.**","42387cb9":"###### **Only run the following code once - simply import the training weights and history file to save an hour or training again**","9cdbb12d":"###### Data preprocessing and augmentation of our images using Keras ImageDataGenerator","f8b06be2":"###### Prediction on a range of random examples from the test set with the Xception Transfer Learning Model:","8a6d81cd":"# Exploration of Scotland image location data \n\n\nWithin this notebook we start by performing some basic extraction of EXIF formatted data from all the images within the dataset. From this, various image meta-data features are explored, which help to plot the location of all images taken on a map of Scotland. Subsequently, a dataset of images and their associated town or city is created, which then allows the formation of a model to make predictions of the town in which an image was taken.\n\nTwo different Deep Convolutional Neural Network models are formed to perform these predictions. Each model predict the town in which an image was taken based entirely on the visual features of a given image. \n\nThe first model is a Simple convolutional neural network architecture, whilst the second is more complex and is formed through transfer learning and feature extraction from a pre-trained Xception imagenet model.\n\nThe intention of this work was to evaluate the feasibility of using a deep neural convolutional neural network to classify the town \/ city of an image based entirely on its visual features. The task was shown to be extremely difficult, which is suspected to be due to a limitation in the number of images to perform this task effectively. However, given enough data, this task could be much more feasible with higher performing models that can generalise to more locations.","fdea3a1c":"When considering we have 10 different classes, this is not bad for a first attempt at classifying locations based only on visual features of the images! Especially when considering how ambiguous and vague some of the images are - this is an extremely difficult task even for humans that know the locations. \n\nClearly, there is still a lot of scope for improvement of our model as it stands. One potential option could be to perform transfer learning and fine-tuning of an existing high-performing CNN, such as Xception, Inception, VGG19 or ResNet50. This will be formed in the next section.","899584db":"###### Lets predict on the same examples, but using the original Basic CNN model:","9206d570":"Lets focus on just the top ten towns \/ cities that have a reasonable number of samples (20+):","c43145b1":"Create our xception transfer learning model by adding a small dense network trained on top of the base conv network:"}}