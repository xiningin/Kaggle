{"cell_type":{"6ed6d79a":"code","8c93e419":"code","8da77b85":"code","850ed5e3":"code","dd1c43a1":"code","061c6199":"code","6d8e074b":"code","7985673a":"code","24f12b1f":"code","6ae34b36":"code","ea78980f":"code","49f9cf4f":"code","330d8fd6":"code","1b141233":"code","63f188cc":"code","f0fc8b33":"code","2a1b4839":"code","60b6c303":"code","a4d4b6ce":"code","c961e775":"code","354f1ffe":"code","2a919edb":"code","f93a7fe9":"markdown","e1940c70":"markdown","46f1a0a8":"markdown","e68af3f4":"markdown","3d3075e6":"markdown","3e85f8dc":"markdown","b60f95b3":"markdown","ecffa3fa":"markdown","0fa3baa8":"markdown","fc20fe4d":"markdown","e9fcae89":"markdown","401aa6ef":"markdown","342297c0":"markdown","f1669045":"markdown","ae673c2d":"markdown","eeba3813":"markdown","37d90e3d":"markdown","d461301d":"markdown","b1f9faba":"markdown","6c77e259":"markdown","3d616161":"markdown","35295ac2":"markdown","ecd3096e":"markdown"},"source":{"6ed6d79a":"#importing libs\nimport numpy as np\nimport pandas as pd\npd.set_option('MAX_COLUMNS', None)\n\n#Data viz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Modelling and Testing\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, SGDRegressor\n\n#Evaluating the models\nfrom sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\n\n\n#Paths\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8c93e419":"data = pd.read_csv('\/kaggle\/input\/car-price-prediction\/CarPrice_Assignment.csv')\ndata.set_index('car_ID', inplace=True)\ndata.drop('CarName', inplace=True, axis=1)\ndata.head()","8da77b85":"#Describe the data\ndata.describe()","850ed5e3":"#My own describe\npd.DataFrame({'missing':data.isna().mean(),\n             'unicos':data.nunique(),\n             'tipos': data.dtypes})","dd1c43a1":"# Changing the type of variable 'symboling' because it was guessed as int and its in fact a ordinal categorical variable.\ndata['symboling'] = data['symboling'].astype('object') ","061c6199":"#Spliting the data\ntarget = data[['price']]\ncategorical_vars = data.select_dtypes('object')\nnumerical_vars = data.select_dtypes(['int64','float64'])\nnumerical_vars.drop('price', inplace=True, axis=1)","6d8e074b":"#Target Analysis\n#Skew and Kurt\nskew = target.skew()\nkurt = target.kurt()\nprint(f'Skewness: {skew}')\nprint(f'Kurtosis: {kurt}')\n\n#Distribucion\nf,ax = plt.subplots(figsize=(12,6))\nsns.distplot(target)\n\n#qqplot\nnorm = sm.qqplot(target, line='s')\n\n#Normality Shapiro test\nstats, p = shapiro(target)\nprint(f'Statistics p-value: {p}')\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Target looks Gaussian (fail to reject H0)')\nelse:\n    print('Target does not look Gaussian (reject H0)')","7985673a":"#transforming the target\n\ntarget_norm = np.log(data['price'])\n\n#Skew and Kurt\nskew = target_norm.skew()\nkurt = target_norm.kurt()\nprint(f'Skewness: {skew}')\nprint(f'Kurtosis: {kurt}')\n\n#Distribucion\nf,ax = plt.subplots(figsize=(12,6))\nsns.distplot(target_norm)\n\n#qqplot\nnorm = sm.qqplot(target_norm, line='s')\n\n#Normality Shapiro test\nstats, p = shapiro(target_norm)\nprint(f'Statistics p-value: {p}')\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Target looks Gaussian (fail to reject H0)')\nelse:\n    print('Target does not look Gaussian (reject H0)')","24f12b1f":"# Numerical variables univariate analysis\ndef univariate_analysis(df):\n    \"\"\"\n    Function to perform univariate analysis.\n    \n    df: DataFrame\n    \"\"\"\n    for col in df.columns.to_list():\n        plt.figure(figsize=(8,5))\n        plt.title(f'{col}\\n distribuition',fontsize=16)\n        sns.distplot(df[col])\n        plt.xlabel(col,fontsize=14)\n        plt.show()\n        \n        #Normality Shapiro test\n        stats, p = shapiro(df[col])\n        print(f'Statistics p-value: {p}')\n        # interpret\n        alpha = 0.05\n        if p > alpha:\n            print('Target looks Gaussian (fail to reject H0)')\n        else:\n            print('Target does not look Gaussian (reject H0)')\n\nunivariate_analysis(numerical_vars)","6ae34b36":"#Checking for outliers\ndef outliers_analysis(df):\n    \"\"\"\n    Function to check for outliers visually through a boxplot\n    \n    df: DataFrame\n    \"\"\"\n    for col in df.columns.to_list():\n        plt.figure(figsize=(8,5))\n        plt.title(f'{col}\\n',fontsize=16)\n        sns.boxplot(x=col, data=df)\n        plt.xlabel(col,fontsize=14)\n        plt.ylabel('Target',fontsize=14)\n        plt.show()\n\noutliers_analysis(numerical_vars)","ea78980f":"# Variables vs Target\ndef variables_vs_target(df,target):\n    \"\"\"\n    Function to compare variabels with the target with a scatterplot.\n    \n    df: DataFrame\n    target: Variabel Target\n    \"\"\"\n    for col in df.columns.to_list():\n        plt.figure(figsize=(8,5))\n        plt.title(f'{col} vs. \\nTarget',fontsize=16)\n        plt.scatter(x=df[col],y=target,color='blue',edgecolor='k')\n        plt.xlabel(col,fontsize=14)\n        plt.ylabel('Target',fontsize=14)\n        plt.show()\n\nvariables_vs_target(numerical_vars,target=target)","49f9cf4f":"categorical_boxplots = categorical_vars.copy()\ncategorical_boxplots['Target'] = target\n\n\ndef categorical_analysis(df):\n    \"\"\"\n    Function to analyze the target variable vs categorical ones.\n    \n    df: DataFrame\n    \"\"\"\n    for col in df.columns.to_list():\n        plt.figure(figsize=(8,5))\n        plt.title(f'{col}\\n distribuition',fontsize=16)\n        sns.boxplot(x=col, y='Target', data=df)\n        plt.xlabel(col,fontsize=14)\n        plt.ylabel('Target',fontsize=14)\n        plt.show()\n\ncategorical_analysis(categorical_boxplots)","330d8fd6":"#Getting 'symboling' out of the mix, once its an ordinal categorical\nlabels = categorical_vars.columns.to_list()[1:]\ncategorical_vars = categorical_vars[labels]\n\n#Spliting the data\nX_treino, X_teste, y_treino, y_teste = train_test_split(data, target_norm, test_size=.10, shuffle=True, random_state=3)\n\n\n#Categorical Pipeline\ncategorical_pipeline = Pipeline([('ohe', OneHotEncoder())])\n\n#Numerical Pipeline\nnumerical_pipeline = Pipeline([('scaler', RobustScaler())])\n\n#Pipeline\npipeline = ColumnTransformer([('cat',categorical_pipeline, categorical_vars.columns.to_list()),\n                              ('num',numerical_pipeline, numerical_vars.columns.to_list())])","1b141233":"reg_pipe = Pipeline([('ct',pipeline),\n                     ('reg',LinearRegression())])\n\n#Fitting\nreg_pipe.fit(X_treino,y_treino)\n\n#Predicting\nreg_pred = reg_pipe.predict(X_teste)\n\n#Evaluating\nprint(f'The R2: {r2_score(np.expm1(y_teste),np.expm1(reg_pred))}')\nprint(f'The RMSE: {np.sqrt(mean_squared_error(np.expm1(y_teste),np.expm1(reg_pred)))}')\nprint('\\n')\nprint(f'Intercept: {reg_pipe.named_steps.reg.intercept_}')\nprint(f'Coefs: {reg_pipe.named_steps.reg.coef_}')","63f188cc":"lasso_pipe = Pipeline([('ct',pipeline),\n                         ('reg',LassoCV(alphas = (0.001, 0.1,1.0,5.0,10.0,50.0,100), cv=5))])\n\n#Fitting\nlasso_pipe.fit(X_treino,y_treino)\n\n#Predicting\nlasso_pred = lasso_pipe.predict(X_teste)\n\n#Evaluating\nprint(f'The R2: {r2_score(np.expm1(y_teste),np.expm1(lasso_pred))}')\nprint(f'The RMSE: {np.sqrt(mean_squared_error(np.expm1(y_teste),np.expm1(lasso_pred)))}')\nprint('\\n')\nprint(f'Best alpha: {lasso_pipe.named_steps.reg.alpha_}')\nprint(f'Non-zero coefs:{len(lasso_pipe.named_steps.reg.coef_!=0)} from {X_treino.shape[0]} variables')\nprint(f'Intercept: {lasso_pipe.named_steps.reg.intercept_}')\nprint(f'Coefs: {lasso_pipe.named_steps.reg.coef_}')","f0fc8b33":"ridge_pipe = Pipeline([('ct',pipeline),\n                         ('reg',RidgeCV(alphas=(0.1,1.0,5.0,10.0,50.0,100), cv=5))])\n\n\n#Fitting\nridge_pipe.fit(X_treino,y_treino)\n\n#Predicting\nridge_pred = ridge_pipe.predict(X_teste)\n\n#Evaluating\nprint(f'The R2: {r2_score(np.expm1(y_teste),np.expm1(ridge_pred))}')\nprint(f'The RMSE: {np.sqrt(mean_squared_error(np.expm1(y_teste),np.expm1(ridge_pred)))}')\nprint('\\n')\nprint(f'Best alpha: {ridge_pipe.named_steps.reg.alpha_}')\nprint(f'Intercept: {ridge_pipe.named_steps.reg.intercept_}')\nprint(f'Coefs: {ridge_pipe.named_steps.reg.coef_}')","2a1b4839":"#Getting 'symboling' out of the mix, once its an ordinal categorical\nlabels = categorical_vars.columns.to_list()[1:]\ncategorical_vars_stats = categorical_vars[labels]\n\n#One-Hot-Encoding\ncategorical_vars_ohe = pd.get_dummies(categorical_vars_stats)\n\n#Numerical transformation\nskewed_vars= numerical_vars.apply(lambda x: x.skew())\nskewed_labels = skewed_vars[skewed_vars > 0.75].index\nnumerical_vars[skewed_labels] = np.log1p(numerical_vars[skewed_labels])\n\n#Putting together\ndataset = categorical_vars_ohe.merge(numerical_vars, left_index=True, right_index=True)\n\n#Spliting the data\nX_treino, X_teste, y_treino, y_teste = train_test_split(dataset, target_norm, test_size=.10, shuffle=True, random_state=4)","60b6c303":"#OLS\nmodel = sm.OLS(y_treino,X_treino)\nresults = model.fit()\nresults.summary()","a4d4b6ce":"#Independece assumption\n\nvariables = X_treino[['wheelbase', 'carlength', 'carwidth', 'carheight',\n       'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio',\n       'horsepower', 'peakrpm', 'citympg', 'highwaympg']].columns.to_list()\n\nfor col in variables:\n    plt.figure(figsize=(8,5))\n    plt.title('Independece Assumption',fontsize=16)\n    plt.scatter(x=X_treino[col],y=results.resid,color='blue',edgecolor='k')\n    plt.hlines(y=0,xmin = min(X_treino[col]) , xmax = max(X_treino[col]),color='red',linestyle='--',lw=3)\n    plt.xlabel(col,fontsize=14)\n    plt.ylabel('Residuals',fontsize=14)\n    plt.show()","c961e775":"# Homoscedasticity plot\nplt.figure(figsize=(8,5))\nplt.title('Fitted vs Residuals',fontsize=16)\nplt.scatter(x=results.fittedvalues,y=results.resid,color='blue',edgecolor='k')\nplt.hlines(y=0,xmin = 8.5 , xmax = max(results.fittedvalues),color='red',linestyle='--',lw=3)\nplt.xlabel('Fitted Values',fontsize=14)\nplt.ylabel('Residuals',fontsize=14)\nplt.show()","354f1ffe":"#Distribucion\nf,ax = plt.subplots(figsize=(12,6))\nsns.distplot(results.resid)\n\n#qqplot\nnorm = sm.qqplot(results.resid, line='s')","2a919edb":"#Normality Shapiro test\nstats, p = shapiro(results.resid)\nprint(f'Statistics p-value: {p}')\n# interpret\nalpha = 0.05\nif p > alpha:\n    print('Resids looks Gaussian (fail to reject H0)')\nelse:\n    print('Resids does not look Gaussian (reject H0)')","f93a7fe9":"# 1.1 Bivariate Analysis","e1940c70":"### Categorical variables\n\nThrough the plots below, we can see some interesting patterns such as:\n\n- Cars with diesel and turbo trends to be more expensive.\n- Rwd drivewheel trends to be more expensive than the others.\n- Engine located at rear trends to be more expensive.\n\nBetween other patterns.","46f1a0a8":"From the results above we can see that our r2 is about 0.98 meaning that our model are able to explain 98% of the price(response variable) variance.\n\nAlso, the RMSE is just about 1100 which is a good range given the range of our target. The Min price is 5118 and the Max price 45400.","e68af3f4":"# Homoscedasticity\n\nThe assumption of a data set to have constant variance is called homoscedasticity. And it\u2019s opposite, where the variance is a function of explanatory variables X is called heteroscedasticity.\n\nBelow we can see the response values (as per the model) vs the residuals, they are randomly distributed attending the Homoscedasticity assumption.\n\nWhy the Homoscedasticity assumption is important?\n\nBecause, if the residual errors are not identically distributed, we cannot use tests of significance or perform confidence interval checking. These tests assume that the residuals are independent and equally distributed.","3d3075e6":"# 0.1 Colect the Data","3e85f8dc":"### Numerical variables.\nWe can see by plots below that most of the numerical variables hold a linear relationship with our target variable.","b60f95b3":"# Normality of Resids\n\nAs we can see below the residuals of our model pass in the normality test since the distribution is bell shaped and the p-value of shapiro test is higher than 0.05\n\nWhat\u2019s normality test telling us is that most of the prediction errors from your model are zero or close to zero and large errors are much less frequent than the small errors.","ecffa3fa":"# 3.1 - Statsmodel - Ordinary Least Squared","0fa3baa8":"# 2.0 Preprocessing to modeling with scikitlearn","fc20fe4d":"# 0.0 Problem Description\n### Problem Statement\n\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\nWhich variables are significant in predicting the price of a car How well those variables describe the price of a car Based on various market surveys, the consulting firm has gathered a large data set of different types of cars across the America market.\n\n### Business Goal\n\nWe are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.\n","e9fcae89":"Our dataset is not outliers heavy but there are some outliers so we will use RobustScale() to transform our numerical features at Preprocessing stage.","401aa6ef":"The lasso model keeps the good performance of the regular linear regression, but due to how its regularization term works some coefficients are penalized to zero performing a kind of feature selection. In our model, we can see that just 51 variables were relevant to it.","342297c0":"The target distribution still not gaussian, once it fails to the Shapiro test, but it's way closer to a Gaussian distribution than before.","f1669045":"There is no missing values. Horray!","ae673c2d":"# 2.1 - SciKit - Multiple Linear Regression\n**Multiple Linear Regression:**\nMultiple regression is an extension of simple regression we use it when we want to predict a continuous variable based on more than one predictor(simple regression).\n\nThe formula of multiple Regression is\n\n$$ y = b1*variable1 + b2*variable2 ... bn*variablen + a $$\n  \n   Where:\n\n    b = Angular coefficient of each variable.\n    a = Intercept of the population.\n   \n   \nWe can evaluate the performance of our model, basically, by calculating the distance of the prediction to the observed value in a validation dataset.\n\nBelow the approaches that we will use to evaluate de models here.\n\n\n\nWhy R2 and RMSE  to evaluate the models?\n\n**R-squared:**\n\nThe R-squared tells us the amount of variance our model is capturing from the response variable.\n\nIt goes by the formula below:\n\n$$ R2 = 1 - \\frac {\\sum (y-\u0177)\u00b2} {\\sum (y-ymean)\u00b2} $$\n\n**RMSE:**\n\nThe RMSE is the Root Mean Squared Error it measures the average of the squared difference between the predictions and the observed value.\nIt works like a standard deviation of our predictions, the smaller its value the better, it's also sensitive to outliers and it comes in the same unit values of the response variable.\n\nIt goes by the formula below:\n\n$$ RMSE =\\sqrt \\frac {\\sum (Predicted-Actual)\u00b2} {N} $$","eeba3813":"# 1.0 Univariate Analysis\nWe can see that our target variable is right-skewed let's fix it performing a log transformation.","37d90e3d":"We can see that all of the numerical variables don't pass the Shapiro test so we will perform some transformation on them in the preprocessing stage.","d461301d":"# 2.2 - SciKit - Lasso\nLasso regression is a method of feature elimination and shrinkage to linear models. The goal here is to obtain a subset of features that minimizes the prediction errors. It reduces the complexity of the model preventing overfitting.","b1f9faba":"# Independence assumption\n\nThis assumption checks  if the residuals are distributed uniformly randomly around the zero x-axes and do not form specific clusters.\n\nWe can see that for all our Xs variables, the residuals are distributed uniformly except the variable compression ratio that formed two clusters.","6c77e259":"#### Shapiro Test","3d616161":"# 2.3 - SciKit - Ridge\nRidge is a regularization method in which the main goal is to smooth the variables that are related to each other(multicollinearity) by penalizing ist coeficients through the term lambda. So, ridge regression shrinks the coefficients and it helps to reduce the model complexity and multicollinearity.\n\nWhen lambda = 0, we have a regular linear regression cost function, that is the lower lambda is more closer to a regular linear regression.\n\nRidge Cost Function:\n\n$$\\text{Cost} = \\sum_{i=1}^N\\Big\\{ y_i - \\sum_{j=0}^M \\beta_j x_{ij}\\Big\\}^2 + \\lambda \\sum_{j=0}^M w_j^2$$","35295ac2":"The ridge model performed a little worst than the other two. \n\nIf I have to choose here a model to go into production, I would go if Lasso because of its capability of feature selection.","ecd3096e":"# 3.0 Preprocessing to modeling with Statsmodel\n\nNow let's perform Linear Regression, Lasso regression, and Ridge regression with Statsmodel.\nIt's a great library to work with in case you need a more statistical interpretation of the model."}}