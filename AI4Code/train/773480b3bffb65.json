{"cell_type":{"d5e99940":"code","f649a689":"code","c7cdaf6d":"code","9ff27621":"code","458d06e2":"code","87e49014":"code","09f70184":"code","3bc58f25":"code","f618139e":"code","ba47bde0":"code","87e56f94":"code","95c5ed34":"code","b2e6b079":"code","4f235134":"code","dcd647d3":"code","04b47bab":"code","a4a1f8ca":"code","932d537a":"code","2202e790":"code","12472b6a":"code","b5a03119":"code","40228cd3":"code","28baf92c":"code","cbfbfdf0":"code","0e1204ef":"code","83058576":"code","4e993b38":"code","87a4cb53":"code","2ebcaddb":"code","5c6024ce":"code","74782ec5":"code","9154df98":"code","e2db9c93":"code","7078e286":"code","7413cb8f":"code","655292f5":"code","8908f4e2":"code","e0884aa4":"code","41c3461d":"code","ead0f7cd":"code","bb921d2e":"code","944ee4d9":"code","a47c6459":"code","169c417c":"code","d8cd577d":"code","9be70c74":"code","27412918":"code","315e8d62":"code","68180557":"markdown","5faa3137":"markdown","a3855b8e":"markdown","0e493a50":"markdown","1da4f3f7":"markdown","6c77b1ec":"markdown","a6a0658a":"markdown","3b9855ec":"markdown","32f7d39a":"markdown","32d76451":"markdown","c56adefa":"markdown"},"source":{"d5e99940":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom textblob import TextBlob\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nimport math\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest, f_regression,f_classif,chi2\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.tree import DecisionTreeClassifier\nimport string\nfrom sklearn.impute import SimpleImputer, KNNImputer\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom sklearn.compose import ColumnTransformer\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler,PowerTransformer,QuantileTransformer,Normalizer,RobustScaler,StandardScaler,MaxAbsScaler\nfrom keras.preprocessing.text import Tokenizer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f649a689":"\ndata=pd.read_csv(\"..\/input\/iba-ml1-final-project\/train.csv\")\n\nidcon=data['Id']\ndata=data.drop(['Id'], axis=1)\ndata2=pd.read_csv(\"..\/input\/iba-ml1-final-project\/test.csv\")\ndata2=data2.drop(['Id'], axis=1)","c7cdaf6d":"data.head(10)","9ff27621":"data.tail(10)","458d06e2":"data2.isna().sum()","87e49014":"data.isna().sum()","09f70184":"data['Review_Title']=data['Review_Title'].replace(np.nan, ' ', regex=True)\ndata['Review']=data['Review'].replace(np.nan, ' ', regex=True)\ndata2['Review_Title']=data2['Review_Title'].replace(np.nan, ' ', regex=True)\ndata2['Review']=data2['Review'].replace(np.nan, ' ', regex=True)","3bc58f25":"data.isna().sum()","f618139e":"\" \".join(data[\"Review\"][:20].tolist())","ba47bde0":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndata['Review'] = data['Review'].str.replace('[{}]'.format(string.punctuation), '',regex=True)\n\ndata['Review'] = data['Review'].str.lower()\n\ndata['Review'] = data['Review'].str.replace('[^\\w\\s]','',regex=True)\n\ndata['Review'] =data['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n\ndata['Review'] = data['Review'].str.replace('\\d+', '',regex=True)\n\n# lemmatizer = WordNetLemmatizer()\n\n# data['Review'] = [lemmatizer.lemmatize(row) for row in data['Review']]","87e56f94":"!pip install contractions\nimport contractions\ndata[\"Review\"]=data[\"Review\"].apply(lambda x: contractions.fix(x))\ndata[\"Review_Title\"]=data[\"Review_Title\"].apply(lambda x: contractions.fix(x))","95c5ed34":"\" \".join(data[\"Review\"][:20].tolist())","b2e6b079":"def get_average_word_len(x):\n    words=x.split()\n    word_len=0\n    if len(x.split())==0:\n        return 0\n        \n    for word in words:\n        word_len=word_len+len(word)\n       \n    return word_len\/len(words)\n","4f235134":"data['polarity']= data[\"Review\"].apply(lambda x: (TextBlob(x).sentiment.polarity) )\ndata['rewiew_lenght']=data[\"Review\"].apply(lambda x:len(x))\ndata['word_count']=data[\"Review\"].apply(lambda x:len(x.split()))\ndata['avr_word_len']=data[\"Review\"].apply(lambda x: get_average_word_len(x))","dcd647d3":"data2['polarity']= data2[\"Review\"].apply(lambda x: (TextBlob(x).sentiment.polarity) )\ndata2['rewiew_lenght']=data2[\"Review\"].apply(lambda x:len(x))\ndata2['word_count']=data2[\"Review\"].apply(lambda x:len(x.split()))\ndata2['avr_word_len']=data2[\"Review\"].apply(lambda x: get_average_word_len(x))","04b47bab":"data.head(10)","a4a1f8ca":"X=data.drop(['Rating', 'Recommended'], axis=1)\ny=data.drop(['Age', 'Review_Title','Review','Pos_Feedback_Cnt','Division','Department','Product_Category','polarity','rewiew_lenght','word_count','avr_word_len'], axis=1)\n","932d537a":"data.describe()","2202e790":"X_train, X_test, y_train, y_test = train_test_split(X, y ,shuffle=True,stratify=y,test_size=0.33,random_state=43)\n","12472b6a":"\nnumeric_preprocessing1 = Pipeline(steps=[\n        ('imputation', KNNImputer(n_neighbors=9)),\n        ('scaling',  RobustScaler())\n    ])\nnumeric_preprocessing2 = Pipeline(steps=[\n        ('impute', SimpleImputer(strategy='most_frequent')),\n        ('one_hot_encoding', OneHotEncoder(handle_unknown='ignore'))\n    \n   \n    ])\npreprocessing = ColumnTransformer(transformers=[\n     \n        ('numeric', numeric_preprocessing1, [\n 'Age',\n 'Pos_Feedback_Cnt','polarity','rewiew_lenght','word_count','avr_word_len']),\n    \n            ('cat', numeric_preprocessing2, ['Division','Department','Product_Category'])\n    ])\n\nmodel_pipeline4 = Pipeline(steps=[\n        ('preprocessing', preprocessing)\n    ])","b5a03119":"\nL=model_pipeline4.fit_transform(X_train)\nL=pd.DataFrame.sparse.from_spmatrix(L)\nL2=model_pipeline4.transform(X_test)\nL2=pd.DataFrame.sparse.from_spmatrix(L2)\n\n\n","40228cd3":"\n\ny_train2 = np.zeros((y_train.shape[0], 5))\ny_train2[np.arange(y_train.shape[0]), y_train['Rating']-1] = 1\ny_test2 = np.zeros((y_test.shape[0], 5))\ny_test2[np.arange(y_test.shape[0]), y_test['Rating']-1] = 1\ny_train2 = pd.DataFrame(y_train2,columns=['1','2','3','4','5'],dtype='int64')\ny_test2 = pd.DataFrame(y_test2,columns=['1','2','3','4','5'],dtype='int64')\ny_train=y_train.drop(['Rating'], axis=1)\ny_test=y_test.drop(['Rating'], axis=1)\n\n","28baf92c":"L=model_pipeline4.fit_transform(X_train)\nL=pd.DataFrame.sparse.from_spmatrix(L)\nL2=model_pipeline4.transform(X_test)\nL2=pd.DataFrame.sparse.from_spmatrix(L2)","cbfbfdf0":"initializer = tf.keras.initializers.GlorotNormal()\nimage_input1 = keras.Input(shape=(None,L.shape[1]))\n\nx5 = layers.Dense(L.shape[1], activation=layers.LeakyReLU(),kernel_initializer=initializer)(image_input1)\nx5 = layers.Dense(L.shape[1]*2, activation=layers.LeakyReLU(),kernel_initializer=initializer)(x5)\nx5 = layers.Dense(L.shape[1], activation=layers.LeakyReLU(),kernel_initializer=initializer)(x5)\nscore_output = layers.Dense(1,activation='sigmoid', name=\"score_output\",kernel_initializer=initializer)(x5)\nclass_output = layers.Dense(5,activation='softmax', name=\"class_output\",kernel_initializer=initializer)(x5)\n\nmodel = keras.Model(\n    inputs=[image_input1], outputs=[score_output,class_output]\n)","0e1204ef":"\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.00016, beta_1=0.8, beta_2=0.888\n),\n    loss=['binary_crossentropy','categorical_crossentropy'],\n    metrics=['accuracy'])","83058576":"early_stopping = EarlyStopping(patience=3,restore_best_weights=True,monitor='val_class_output_accuracy')\nmy_callbacks = [\n    early_stopping\n]\n\nhistory = model.fit([L], [y_train,y_train2], epochs=20, validation_data=([L2], [y_test,y_test2]),callbacks=[my_callbacks])","4e993b38":"L8=model_pipeline4.transform(data.drop(['Rating','Recommended'], axis=1))\nm=model.predict(L8)\nz=m[0]\nr=m[1]\n\nr2=np.zeros((r.shape[0],1),dtype='int64')\nfor i in range(r.shape[0]):\n    r2[i]=r[i].argmax()+1\nz=z.round()\nz=z.astype('int64')\ndf = pd.DataFrame(r2,columns=['Rating'] ,dtype='int64')\n\n# index=idcon\n\ndf['Recommended'] = z","87a4cb53":"o=0\nf=0\nfor i in range(len(data.Review_Title)):\n\n    if data.Review_Title[i] ==\" \":\n        f=f+1\n        \n        if (df.Rating[i]+df.Recommended[i])==6:\n            o=o+1\nprint(f\"{f} is a Total number of NAN Rewiew_title values in training data\")\nprint(f\"{o} is a number of NAN Rewiew_title values that are predicted to have Rating:5 and Recommended:1\")\n\n   ","2ebcaddb":"L8=model_pipeline4.transform(data2)\nm=model.predict(L8)\nz=m[0]\nr=m[1]\n\nr2=np.zeros((r.shape[0],1),dtype='int64')\nfor i in range(r.shape[0]):\n    r2[i]=r[i].argmax()+1\nz=z.round()\nz=z.astype('int64')\ndf = pd.DataFrame(r2,columns=['Rating'] ,dtype='int64')\n\n# index=idcon\n\ndf['Recommended'] = z","5c6024ce":"o=0\nf=0\nfor i in range(len(data2.Review_Title)):\n\n    if data2.Review_Title[i] ==\" \":\n        f=f+1\n        if (df.Rating[i]+df.Recommended[i])==6:\n            o=o+1\nprint(f\"{f} is a Total number of NAN Rewiew_title values in test data\")\nprint(f\"{o} is a number of NAN Rewiew_title values that are predicted to have Rating:5 and Recommended:1\")","74782ec5":"data['Review_Title']=data['Review_Title'].replace(' ', 'Gorgeous', regex=True)\ndata['Review']=data['Review'].replace(' ', 'Love this top! the quality is magnificent', regex=True)\ndata2['Review_Title']=data2['Review_Title'].replace(' ', 'Gorgeous', regex=True)\ndata2['Review']=data2['Review'].replace(' ', 'Love this top! the quality is magnificent', regex=True)","9154df98":"data","e2db9c93":"data.hist(data.columns,grid=False, figsize=(20,15))","7078e286":"fig, axes = plt.subplots(nrows=3, ncols=2,figsize=(10,10))\n\nX_train.boxplot(column=['Age'],grid=False, ax=axes[0,0])\nX_train.boxplot(column=['Pos_Feedback_Cnt'],grid=False, ax=axes[0,1]) \nX_train.boxplot(column=['polarity'],grid=False, ax=axes[1,0])\nX_train.boxplot(column=['rewiew_lenght'],grid=False, ax=axes[1,1]) \nX_train.boxplot(column=['word_count'],grid=False,ax=axes[2,0])\nX_train.boxplot(column=['avr_word_len'],grid=False, ax=axes[2,1]) \nfig.tight_layout()\nplt.show()","7413cb8f":"plt.figure(figsize = (15,15))\nsns.heatmap(data.corr(), annot = True, vmax = 1, vmin = -1, square = True)","655292f5":"sns.pairplot(data)","8908f4e2":"sns.catplot(x='Division',y='polarity',height=8,data=data)","e0884aa4":"sns.catplot(x='Department',y='polarity',height=8,data=data)","41c3461d":"sns.catplot(x='Product_Category',y='polarity',height = 10,data=data)\nplt.xticks(rotation=90,fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('Product_Category',fontsize=30)\nplt.ylabel('Polarity',fontsize=30)\nplt.show","ead0f7cd":"sns.catplot(x='Division',y='rewiew_lenght',height=8,kind='box',data=data)\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('Product_Category',fontsize=30)\nplt.ylabel('Polarity',fontsize=20)\nplt.show","bb921d2e":"sns.catplot(x='Department',y='rewiew_lenght',height=8,kind='box',data=data)\nplt.xticks(rotation=90,fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('Department',fontsize=30)\nplt.ylabel('rewiew_lenght',fontsize=20)\nplt.show","944ee4d9":"sns.catplot(x='Product_Category',y='rewiew_lenght',height=8,kind='box',data=data)\nplt.xticks(rotation=90,fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('Department',fontsize=30)\nplt.ylabel('rewiew_lenght',fontsize=20)\nplt.show","a47c6459":"data.groupby('Division').mean()","169c417c":"data.groupby('Department').mean()","d8cd577d":"data.groupby('Product_Category').mean()","9be70c74":"import nltk","27412918":"corpus = data[\"Review\"][:1000]\nlst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=\" \"))\nfig, ax = plt.subplots(nrows=1, ncols=2)\nfig.suptitle(\"Most frequent words in rewiews\", fontsize=45)\n    \n## unigrams\ndic_words_freq = nltk.FreqDist(lst_tokens)\ndtf_uni = pd.DataFrame(dic_words_freq.most_common(), \n                       columns=[\"Word\",\"Freq\"])\ndtf_uni.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Unigrams\", ax=ax[0],figsize=(15,8), fontsize=17,\n                  legend=False).grid(axis='x')\nax[0].set(ylabel=None)\n    \n## bigrams\ndic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))\ndtf_bi = pd.DataFrame(dic_words_freq.most_common(), \n                      columns=[\"Word\",\"Freq\"])\ndtf_bi[\"Word\"] = dtf_bi[\"Word\"].apply(lambda x: \" \".join(\n                   string for string in x) )\ndtf_bi.set_index(\"Word\").iloc[:10,:].sort_values(by=\"Freq\").plot(\n                  kind=\"barh\", title=\"Bigrams\", ax=ax[1],figsize=(15,8),fontsize=17,\n                  legend=False).grid(axis='x')\nax[1].set(ylabel=None)\nfig.tight_layout()\nplt.show()","315e8d62":"blob=TextBlob(str(data['Review']))\npos_df=pd.DataFrame(blob.tags,columns=['words','pos'])\npos_df=pos_df['pos'].value_counts()\n\npos_df.plot.bar(figsize=(15,8))\nplt.xticks(rotation=45,fontsize=20)\nplt.yticks(fontsize=20)\nplt.xlabel('Part of speech',fontsize=20)\nplt.ylabel('Avverage occurrence',fontsize=20)","68180557":"## ","5faa3137":"## After looking up at all samples I understood that I have to handle somehow contraction forms(I'm,You're,I'll), I tried various text preprocessing techniques but most of them did not have much impact on my results so I removed them and used basic ones.","a3855b8e":"## While examining all Nan values I figured out that there is a huge amount of missing samples in rewiew and rewiew title column, mostly in rewiew title column. Then I decided to write a custom method to impute that NAN values by examining other columns.","0e493a50":"## General Distribution among distribution among Data","1da4f3f7":"## Pos tagging","6c77b1ec":"### I will temporarily impute this columns by empty strings","a6a0658a":"### So we can simple impute all empty rewiew values with rewies which will is corresponding to rating:5 and reccomeded:1 ","3b9855ec":"### while looking up at predicted distribution I found that most of the missing data are having best possible evaluation","32f7d39a":"### I will construct a pipeline for scaling and imputing data and I will use it to find what is the distirbution among NAN rewiews and rewiew titles","32d76451":"### Doing same operations on test data","c56adefa":"### Doing some feature engeneering by adding colums for average word length,polarity ,number of wors in text or whole length of text."}}