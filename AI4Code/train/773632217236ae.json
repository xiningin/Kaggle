{"cell_type":{"89bb534d":"code","bafac457":"code","326706be":"code","29d123c3":"code","074c4396":"code","a844b740":"code","96d489b6":"code","63f85216":"code","72fe3e2e":"code","b5c069a5":"code","b7d76e7c":"code","755e28f3":"code","81a24389":"code","e919cf88":"markdown","211f9284":"markdown","6c1a996a":"markdown","68fa64ab":"markdown","9ade9d71":"markdown","7986ba69":"markdown","a5a84f80":"markdown","ebc0255d":"markdown","ff45943f":"markdown","2964e899":"markdown"},"source":{"89bb534d":"# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Importing data\ndata = pd.read_csv('..\/input\/data.csv')\ndel data['Unnamed: 32']","bafac457":"X = data.iloc[:, 2:].values\ny = data.iloc[:, 1].values\n\n# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_X_1 = LabelEncoder()\ny = labelencoder_X_1.fit_transform(y)\n\n# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","326706be":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","29d123c3":"# Initialising the ANN\nclassifier = Sequential()","074c4396":"# Adding the input layer and the first hidden layer\nclassifier.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(p=0.1))","a844b740":"# Adding the second hidden layer\nclassifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n# Adding dropout to prevent overfitting\nclassifier.add(Dropout(p=0.1))","96d489b6":"# Adding the output layer\nclassifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))","63f85216":"# Compiling the ANN\nclassifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","72fe3e2e":"# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size=100, nb_epoch=150)\n# Long scroll ahead but worth\n# The batch size and number of epochs have been set using trial and error. Still looking for more efficient ways. Open to suggestions. ","b5c069a5":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)","b7d76e7c":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","755e28f3":"print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])\/57)*100))","81a24389":"sns.heatmap(cm,annot=True)\nplt.savefig('h.png')","e919cf88":"# Aim\nThis is a small yet useful kernel for providing an introduction to **Artificial Neural Networks** for people who want to begin their journey into the field of **deep learning**. For this, I have used Keras which is a high-level Neural Networks API built on top of low level neural networks APIs like Tensorflow and Theano. As it is high-level, many things are already taken care of therefore it is easy to work with and a great tool to start with. [Here's the documentation for keras](https:\/\/keras.io\/)\n\n# What is Deep learning?\nDeep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks. Deep learning is a machine learning technique that teaches computers to do what comes naturally to humans: learn by example. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost. It is the key to voice control in consumer devices like phones, tablets, TVs, and hands-free speakers. Deep learning is getting lots of attention lately and for good reason. It\u2019s achieving results that were not possible before.\n\n\n# What are artificial neural networks?\nAn artificial neuron network (ANN) is a computational model based on the structure and functions of biological neural networks. Information that flows through the network affects the structure of the ANN because a neural network changes - or learns, in a sense - based on that input and output. ANNs are considered nonlinear statistical data modeling tools where the complex relationships between inputs and outputs are modeled or patterns are found. ANN is also known as a neural network.\n","211f9284":"Thanks for reading this. May this help you on your \"deep\" journey into machine learning.","6c1a996a":"output_dim is 1 as we want only 1 output from the final layer.\n\nSigmoid function is used when dealing with classfication problems with 2 types of results.(Submax function is used for 3 or more classification results)\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*Xu7B5y9gp0iL5ooBj7LtWw.png\">","68fa64ab":"Batch size defines number of samples that going to be propagated through the network.\n\nAn Epoch is a complete pass through all the training data.","9ade9d71":"<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*ZX05x1xYgaVoa4Vn2kKS9g.png\">","7986ba69":"**Now that we have prepared data, we will import Keras and its packages.**","a5a84f80":"A single neuron is known as a perceptron. It consists of a layer of inputs(corresponds to columns of a dataframe). Each input has a weight which controls the magnitude of an input.\nThe summation of the products of these input values and weights is fed to the activation function. Activation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable. \n\nThey introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack.  Specifically in A-NN we do the sum of products of inputs(X) and their corresponding Weights(W) and apply a Activation function f(x) to it to get the output of that layer and feed it as an input to the next layer. [Refer to this article for more info.](https:\/\/towardsdatascience.com\/activation-functions-and-its-types-which-is-better-a9a5310cc8f)\n<img src=\"https:\/\/cdnpythonmachinelearning.azureedge.net\/wp-content\/uploads\/2017\/09\/Single-Perceptron.png\">\n**Concept of backpropagation** - Backpropagation, short for \"backward propagation of errors,\" is an algorithm for supervised learning of artificial neural networks using gradient descent. Given an artificial neural network and an error function, the method calculates the gradient of the error function with respect to the neural network's weights. It is a generalization of the delta rule for perceptrons to multilayer feedforward neural networks.\n<img src=\"https:\/\/www.researchgate.net\/profile\/Hassan_Al-Haj_Ibrahim\/publication\/235338024\/figure\/fig6\/AS:299794191929349@1448487913220\/Flow-chart-for-the-back-propagation-BP-learning-algorithm.png\">\n**Gradient Descent** - To explain Gradient Descent I\u2019ll use the classic mountaineering example. Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake? The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake. [Refer to this article for more information.](https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/introduction-to-gradient-descent-algorithm-along-its-variants\/)","ebc0255d":"input_dim - number of columns of the dataset \n\noutput_dim - number of outputs to be fed to the next layer, if any\n\nactivation - activation function which is ReLU in this case\n\ninit - the way in which weights should be provided to an ANN\n \nThe **ReLU** function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this. Source : [StackExchange](https:\/\/stats.stackexchange.com\/questions\/226923\/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)","ff45943f":"Optimizer is chosen as adam for gradient descent.\n\nBinary_crossentropy is the loss function used. \n\nCross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [More about this](http:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/loss_functions.html)","2964e899":"About Breast Cancer Wisconsin (Diagnostic) Data Set\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 \/ area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant"}}