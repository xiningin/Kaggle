{"cell_type":{"03f79f3e":"code","4c90c38d":"code","68ca1a8a":"code","b1b6c233":"code","550054d7":"code","ad88e2e3":"code","a5e11526":"code","d067b9a8":"code","aa6e4b74":"code","d5f940d0":"code","80812a9f":"code","f548a72a":"code","7fa43dfc":"code","b3e13842":"code","324e40ce":"code","fe7ff977":"markdown","fef9be54":"markdown","a8dba13e":"markdown","90e3b9a7":"markdown","957a8c48":"markdown"},"source":{"03f79f3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sns\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split as tts\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c90c38d":"data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndata.head()\n\ndata.drop('Id', axis=1, inplace=True)\ntest_Id = data_test.Id\ndata_test.drop('Id', axis=1, inplace=True)\n\ndata.head()","68ca1a8a":"''' 1: Find NaN values '''\nNaN_values = data.isna().sum()\/1460 #There are 1460 total rows so this gives the percentage of values that are missing\nNaN_values = NaN_values.loc[NaN_values>0]\nprint(\"NaN_values\")\nprint(NaN_values)\nprint('')\n\ntry: #include this to be able to run this code block multiple times without getting errors\n    data.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=\"columns\", inplace = True)\n    data_test.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=\"columns\", inplace = True)\nexcept: \n    pass\n\n''' 2: Heatmap'''\ncorr = data.corr()\nprint(\"Heatmap\")\nprint(corr.SalePrice.sort_values(ascending=False))\nplt.subplots(figsize=(14,8))\nprint(\"Correlations\")\nprint(sns.heatmap(corr))","b1b6c233":"'''3: Visualizations'''\n#Make sure there are no missing values in our premier features\nfeatures = ['OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\n\nprint(\"Missing values\")\nfor feature in features:\n    print(data[feature].isna().sum())\n\n# OverallQuality vs SalePrice\nplt.figure(figsize=(14,6))\nsns.scatterplot(data=data, x = 'OverallQual', y = 'SalePrice')\n# I want to see the relationship between GarageCars and GarageArea as I believe I can squash them into one\nsns.lmplot(data=data, x=\"GarageArea\", y=\"SalePrice\", hue=\"GarageCars\")\nplt.figure(figsize=(15,15))\nsns.swarmplot(x=data.GarageCars, y=data.SalePrice)\n#features.remove('GarageCars')\n# Relationship between Basement and First floor area on Price\nplt.figure(figsize=(14,6))\nsns.scatterplot(data=data, x = 'TotalBsmtSF', y = 'SalePrice', label = 'Basement Area')\nsns.scatterplot(data=data, x = '1stFlrSF', y = 'SalePrice', label = 'First Floor Area')\n# Full Bath vs SalePrice\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=data.FullBath, y=data.SalePrice)\n# Total Rooms above Grade\nplt.figure(figsize=(15,10))\nsns.swarmplot(x=data.TotRmsAbvGrd, y=data.SalePrice)\n# Year Built\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=data.YearBuilt, y=data.SalePrice)\n# Remodeled Year\nplt.figure(figsize=(15,10))\nsns.scatterplot(x=data.YearRemodAdd, y=data.SalePrice)","550054d7":"# Removing outliers\ndata.drop(data[(data['GrLivArea']>3000) & (data['SalePrice']<240000)].index, inplace=True)\ndata.drop(data[(data['GrLivArea']>3000) & (data['SalePrice']>700000)].index, inplace=True)\ndata.drop(data[(data['OverallQual']==10) & (data['SalePrice']<200000)].index, inplace=True)\ndata.drop(data[(data['TotalBsmtSF']>4000) & (data['1stFlrSF']>4000)].index, inplace=True)\ndata.drop(data[(data['TotRmsAbvGrd']==14)].index, inplace=True)\ndata.drop(data[(data['YearBuilt']<1900) & (data['SalePrice']>400000)].index, inplace=True)\n\n\n# Feature Distributions\nplt.figure()\nsns.distplot(data['SalePrice'] , fit=norm)\n#data['SalePrice'] = np.log1p(data['SalePrice'])\nplt.figure()\nsns.distplot(data['SalePrice'] , fit=norm)\n\nfig = plt.figure(figsize=(10,10))\nfig.subplots_adjust(hspace=4, wspace=1)\nfor feature in features:\n    ax = fig.add_subplot(5, 2, features.index(feature)+1)\n    sns.distplot(data[feature] , fit=norm)\n    plt.xlabel(feature)\n\n# I don't think I will normalize the below even though they are slightly skewed because they all are...\n# ...columns with area where the meaning in differences\/distances can be altered if squashed.\n\n# data['GrLivArea'] = np.log1p(data['GrLivArea'])\n# data['TotalBsmtSF'] = np.log1p(data['TotalBsmtSF'])\n# data['1stFlrSF'] = np.log1p(data['1stFlrSF'])\n\ndata[features]\n\n# Encoding Categorical Variables\n# No categorical ones in my selected features so this step can be skipped","ad88e2e3":"test_data = data_test[features]\ntr_features = ['SalePrice'] + features\nmodel_data = data[tr_features]\nmodel_data\nX_train, X_test, y_train, y_test = tts(model_data.iloc[:, 1:], model_data.iloc[:, 0:1], test_size=0.20)","a5e11526":"from sklearn.linear_model import Ridge, Lasso, ElasticNet\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV # This is to decide the best parameters for each model\nfrom sklearn.ensemble import StackingRegressor # I might stack some of the high performing regressors later","d067b9a8":"ridgeReg = Ridge(alpha=0.0005).fit(X_train, y_train)\nlassoReg = Lasso(alpha = 0.0005).fit(X_train, y_train)\n\neNetReg = GridSearchCV(ElasticNet(), {'l1_ratio': [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0], 'alpha': [0.0005]}, scoring='neg_mean_squared_error')\neNetReg.fit(X_train, y_train)\n# pd.DataFrame(eNetReg.cv_results_)['mean_test_score'] #turns out changing the l1 ratio didn't do much\neNetReg = ElasticNet(alpha=0.0005).fit(X_train, y_train)\n\nrfReg = GridSearchCV(RandomForestRegressor(), {'n_estimators': [300], 'min_samples_leaf': [1,2,3,4,5], 'max_depth': [200, 250, 300, 350, 400]})\nrfReg = RandomForestRegressor(n_estimators = 300, min_samples_leaf = 3, max_depth = 200) # Seem to be best from above\nrfReg.fit(X_train, y_train)\n\ngbReg = GridSearchCV(GradientBoostingRegressor(), {'loss': ['ls', 'lad', 'huber', 'quartile'], 'n_estimators': [100, 200, 300, 400], 'min_samples_leaf': [3, 5, 10, 20]})\ngbReg = GradientBoostingRegressor(loss= 'lad', n_estimators=300, min_samples_leaf=5)\ngbReg.fit(X_train, y_train)\n\nhgbReg = GridSearchCV(HistGradientBoostingRegressor(), {'scoring': ['neg_mean_squared_error'], 'min_samples_leaf': [20]})\nhgbReg = HistGradientBoostingRegressor()\nhgbReg.fit(X_train, y_train)\n\n#xgbReg = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817, n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571, subsample=0.5213, silent=1, random_state =7, nthread = -1)\n#xgbReg.fit(X_train, y_train)\n\n#lgbReg = lgb.LGBMRegressor(objective='regression',num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin = 55, bagging_fraction = 0.8, bagging_freq = 5, feature_fraction = 0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n#lgbReg.fit(X_train, y_train)","aa6e4b74":"print('Ridge')\nprint(ridgeReg.score(X_test, y_test))\nprint('')\nprint('Lasso')\nprint(lassoReg.score(X_test, y_test))\nprint('')\nprint('Elastic Net')\nprint(eNetReg.score(X_test, y_test))\nprint('')\nprint('Random Forest')\nprint(rfReg.score(X_test, y_test))\nprint('')\nprint('GB')\nprint(gbReg.score(X_test, y_test))\nprint('')\nprint('HGB')\nprint(hgbReg.score(X_test, y_test))\nprint('')\n# print('xgb')\n# print(xgbReg.score(X_test, y_test))\n# print('')\n# print('lgb')\n# print(lgbReg.score(X_test, y_test))","d5f940d0":"estimators = [('rf', rfReg), ('gb', gbReg), ('hgb', hgbReg)]\nstacking = StackingRegressor(estimators = estimators)\nstacking.fit(X_train, y_train)","80812a9f":"stacking.score(X_test, y_test)","f548a72a":"stacking.fit(model_data.iloc[:, 1:], model_data.iloc[:, 0:1])","7fa43dfc":"test_data\nprint(features)\n\ntest_data['GarageArea'].fillna(test_data['GarageArea'].median(), inplace=True)\ntest_data['TotalBsmtSF'].fillna(test_data['TotalBsmtSF'].median(), inplace=True)\ntest_data['GarageCars'].fillna(test_data['GarageCars'].median(), inplace=True)\n\nfor feature in features:\n    print(test_data[feature].isna().sum())","b3e13842":"prediction_stacking = stacking.predict(test_data)\n#prediction_stacking = np.expm1(prediction_stacking) # to inverse log the sales price\n\nprediction_stacking = pd.DataFrame({'Id': test_Id, 'SalePrice': prediction_stacking})\nprint(prediction_stacking)\n\nprediction_stacking.to_csv('prediction_stacking.csv', index = False)\n\n","324e40ce":"output = pd.read_csv('.\/prediction_stacking.csv')\noutput","fe7ff977":"## Model Creation\n\nSince this dataset is all about regression, the models I choose will all be tuned to this form of learning.\n\n**Models:** Ridge, Lasso, ElasticNet, Random Forest, Gradient Boosting and Histogram based Gradient Boosting.\n\nThese were all selected based on some research about regression and the Sklearn website's descriptions.","fef9be54":"## Preprocessing\n\nNow that we have the main features we'll be looking at, let's ensure that they're ripe for consumption by our models.\n\nIncludes:\n- Removing outliers\n- Normalizing feature distributions\n- Encoding categorical variables","a8dba13e":"### Insights and Actions\n1. It is very clear that Alley, PoolQC, Fence, and MiscFeature have way too many missing values for us to fill in and still have sensible data. Therefore, I will remove them (and Id) from the dataset. \n    * FireplaceQu also has almost 50% missing values but I'll wait to see how strong of a relation it has to sale price before removing it.\n\n2. From the heatmap, we see most clearly that the biggest correlated features to SalePrice are OverallQual, GrLivArea, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF, and FullBath. \n\n3. It looks like there are no missing values so we can go ahead and visualize the relationships.\n\nRelationships: \n- GarageCars and GarageArea are closely related as expected so I will keep GarageArea and remove GarageCars (**This turned out to be a bad idea and keeping both increased accuracy**)\n- Basement Area and 1st Floor Area have a very similar relationship to Sale Price but I think I'll keep them for now just to give the model more data points but consider removing if accuracy isn't high due to overfitting or such\n","90e3b9a7":"# Outline\n1. EDA and Visualization\n2. Preprocessing and cleaning up the data\n3. Model Creation\n4. Testing Models\n5. Generating Submission File\n\nI chose to do EDA first for this dataset to determine the most important features so I'm not filling in values for unnecessary columns.","957a8c48":"## EDA\n\nI'll first take a look at the number of NaN values in each column to easily determine what features are rendered useless due to poor data volume.\n\nThen, I'll create a heatmap and see what columns map best to the price of the house.\n\nGiven these important features, I'll create some visualizations to see how they relate to e"}}