{"cell_type":{"6a82d566":"code","873f840a":"code","8fdb2a69":"code","c10d1ee6":"code","f48853f2":"code","4d5c1d7f":"code","2014b43b":"code","ec20d687":"code","aa509f9e":"code","f4f12541":"code","99a8acd7":"code","94974f48":"code","b04b7158":"code","77072091":"code","e0992c31":"code","0411a26b":"code","73c4c34a":"code","68fd354f":"code","6780948f":"code","8affd745":"code","f158f4ee":"code","28dbbf87":"code","9b711734":"code","7b47d88d":"code","54d96b02":"code","65373623":"code","dfd6589a":"markdown","e5df3f78":"markdown","8113571d":"markdown","a709d5fb":"markdown","6bf018c9":"markdown","7cb5061d":"markdown","6194c1be":"markdown","b2ced042":"markdown","9168c4da":"markdown","67177c6a":"markdown","06fa1a5c":"markdown","6086dfe9":"markdown","a858472e":"markdown","8709bd99":"markdown"},"source":{"6a82d566":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","873f840a":"!pip install nb_black -q","8fdb2a69":"%load_ext nb_black","c10d1ee6":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport math\n\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","f48853f2":"train.info()","4d5c1d7f":"def plot_hists(df, labels):\n    row = 1\n    col = 1\n    num_graphs = len(labels)\n    rows = math.ceil(num_graphs \/ 3)\n    fig = make_subplots(rows=rows, cols=3, subplot_titles=labels)\n\n    index = []\n    for row in range(1, rows + 1):\n        for col in range(1, 4):\n            index.append({\"row\": row, \"col\": col})\n\n    graphs = []\n    pos_g = 0\n    for label in labels:\n        local_data = df[label].value_counts()\n        x = list(local_data.index)\n        y = list(local_data)\n        fig.add_trace(\n            go.Bar(x=x, y=y, text=y, textposition=\"auto\",),\n            row=index[pos_g][\"row\"],\n            col=index[pos_g][\"col\"],\n        )\n        pos_g = pos_g + 1\n\n    fig.show()","2014b43b":"with sns.axes_style(\"white\"):\n    table = train.corr()\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(18, 7))\n    sns.heatmap(\n        table,\n        cmap=\"Reds\",\n        mask=mask,\n        vmax=0.3,\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 15},\n    )","ec20d687":"plot_hists(train, [\"SibSp\", \"Survived\", \"Pclass\", \"Parch\", \"Embarked\", \"Sex\"])","aa509f9e":"def serie_categorie(value, serie):\n    if value <= serie.quantile(0.2):\n        return 1\n    elif value <= serie.quantile(0.4):\n        return 2\n    elif value <= serie.quantile(0.6):\n        return 3\n    elif value <= serie.quantile(0.8):\n        return 4\n    else:\n        return 5","f4f12541":"from sklearn.preprocessing import OneHotEncoder\nfrom string import punctuation\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\n\npunctuation = [p for p in punctuation]\nstopwords = nltk.corpus.stopwords.words(\"english\")\nstopwords = stopwords + punctuation + [\"...\"] + [\"!!\"]\ntoken_punct = nltk.WordPunctTokenizer()\nstemmer = nltk.RSLPStemmer()\n\n\n\ndef remove_punct(my_str):\n    no_punct = \"\"\n    for char in my_str:\n        if char not in punctuation:\n            no_punct = no_punct + char\n    return no_punct\n\n\ndef tokenizer_column(serie):\n    clear_col = list()\n    for row in serie:\n        new_line = list()\n        line = token_punct.tokenize(remove_punct(row.lower()))\n        for word in line:\n            if word not in stopwords:  # stopwords\n                new_line.append(stemmer.stem(word))\n        clear_col.append(\" \".join(new_line))\n    return clear_col\n\n\ndef formating_df(df):\n    aux = df.drop([\"PassengerId\", \"Ticket\"], axis=1)\n    # Fare\n    aux.Fare.fillna(0, inplace=True)\n    aux[\"Fare\"] = aux[\"Fare\"].astype(int)\n    aux[\"Fare\"] = [serie_categorie(val, aux[\"Fare\"]) for val in aux[\"Fare\"]]\n\n    # SibsSp, Parch and Cabin\n    bin_fill = lambda a: 1 if a > 0 else 0\n    aux.Cabin.fillna(0, inplace=True)\n    cabin_fill = lambda a: 1 if a != 0 else 0  # 1 means cabin e 2 means not cabin\n    aux.Cabin = [cabin_fill(line) for line in aux.Cabin]\n    aux[\"SibSp\"] = [bin_fill(line) for line in aux.SibSp]\n    aux[\"Parch\"] = [bin_fill(line) for line in aux.Parch]\n\n    # Sex\n    to_change = {\"male\": 0, \"female\": 1}\n    aux.Sex = aux.Sex.map(to_change)\n    to_change = {\"S\": 1, \"C\": 2, \"Q\": 3}\n    aux.Embarked = aux.Embarked.map(to_change)\n\n    # Age\n    aux.fillna(0, inplace=True)\n    aux[\"Age\"] = [serie_categorie(val, aux[\"Age\"]) for val in aux[\"Age\"]]\n\n    # New features don't divide cause 2\/2=3\/3=1 then you'll lose information\n    aux[\"Age_Fare\"] = aux[\"Fare\"] * aux[\"Age\"]\n    aux[\"Fare_Pclass\"] = aux[\"Fare\"] * aux[\"Pclass\"]\n    aux[\"Age_Pclass\"] = aux[\"Pclass\"] * aux[\"Age\"]\n    \n    aux.Name = aux.Name.fillna(\"no name\")\n    aux.Name = tokenizer_column(aux.Name)\n    return aux\n\n","99a8acd7":"train_fmt = formating_df(train)\ntest_fmt = formating_df(test)","94974f48":"plot_hists(train_fmt, train_fmt.columns)","b04b7158":"with sns.axes_style(\"white\"):\n    table = train_fmt.corr()\n    mask = np.zeros_like(table)\n    mask[np.triu_indices_from(mask)] = True\n    plt.figure(figsize=(18, 7))\n    sns.heatmap(\n        table,\n        cmap=\"Reds\",\n        mask=mask,\n        vmax=0.3,\n        linewidths=0.5,\n        annot=True,\n        annot_kws={\"size\": 15},\n    )","77072091":"import plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2, subplot_titles=(\"Age\", \"Fare\"))\n\nfig.add_trace(go.Box(y=train_fmt.Age, name=\"Age\"), row=1, col=1)\nfig.add_trace(go.Box(y=train_fmt.Fare, name=\"Fare\"), row=1, col=2)\n\nfig.update_layout(height=600, width=975, title_text=\"Age and Fare Boxplots\")\nfig.show()","e0992c31":"print(\"Train shape:\", train_fmt.shape)\nprint(\"Test shape:\", test_fmt.shape)","0411a26b":"# Formating all the datas\ny_train = train_fmt.Survived\nenc = OneHotEncoder()\n\nvectorizer = TfidfVectorizer(\n    max_features=100,\n    min_df=10,\n    ngram_range=(1, 3),\n    analyzer=\"word\",\n    stop_words=\"english\",\n)\nvectorizer.fit(train_fmt[\"Name\"])\n\nx_train = np.concatenate(\n    (\n        vectorizer.transform(train_fmt[\"Name\"]).toarray(),\n        enc.fit_transform(\n            train_fmt.drop([\"Name\", \"Survived\"], axis=1).values\n        ).toarray(),\n    ),\n    axis=1,\n)\n\n\nx_test = np.concatenate(\n    (\n        vectorizer.transform(test_fmt[\"Name\"]).toarray(),\n        enc.transform(test_fmt.drop([\"Name\"], axis=1).values).toarray(),\n    ),\n    axis=1,\n)","73c4c34a":"x_train.shape","68fd354f":"print(\"Train shape:\", x_train.shape)\nprint(\"Test shape:\", x_test.shape)","6780948f":"import time\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\n\nmodels = [\n    (\"SVC\", SVC()),  # I change my mind, this model is taking too much time\n    (\"SGDClassifier\", SGDClassifier()),\n    (\"LGBMClassifier\", lgb.LGBMClassifier()),\n    (\"MLPClassifier\", MLPClassifier()),\n    (\n        \"DecisionTreeClassifier\",\n        DecisionTreeClassifier(criterion=\"entropy\", class_weight=\"balanced\"),\n    ),\n    (\"NearestCentroid\", NearestCentroid()),\n    (\"KNeighborsClassifier\", KNeighborsClassifier()),\n    (\"DummyClassifier\", DummyClassifier(strategy=\"most_frequent\")),\n    (\"LogisticRegression\", LogisticRegression()),\n    (\"RidgeClassifier\", RidgeClassifier()),\n    (\"RandomForestClassifier\", RandomForestClassifier(n_estimators=20)),\n]\n\n\ndef train_test_validation(model, name, X, Y):\n    print(f\"Starting {name}.\")  # Debug\n    ini = time.time()  # Start clock\n    scores = cross_val_score(model, X, Y, cv=4)  # Cross-validation\n    fim = time.time()  # Finish clock\n    print(f\"Finish {name}.\")  # Debug\n    return (\n        name,\n        scores.mean(),\n        scores.std() \/ scores.mean(),\n        scores.std(),\n        scores.max(),\n        scores.min(),\n        fim - ini,\n    )","8affd745":"%%time\nresults = [ train_test_validation(model[1], model[0], x_train, y_train) for model in models ] # Testing for all models\nresults = pd.DataFrame(results, columns=['Classifier', 'Mean', 'CV','Std','Max', 'Min', 'TimeSpend (s)']) # Making a data frame\nresults.sort_values('Mean', ascending=False)","f158f4ee":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\n\n\ndef train_test_model(name, model, X, Y):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, Y, test_size=0.2, random_state=42\n    )\n    model.fit(X_train, y_train)\n    predict = model.predict(X_test)\n    accuracy = accuracy_score(y_test, predict) * 100\n    return (name, accuracy)\n\n\nresults_test = [\n    train_test_model(model[0], model[1], x_train, y_train) for model in models\n]  # Testing for all models\nresults_test = pd.DataFrame(\n    results_test, columns=[\"Model\", \"Accuracy\"]\n)  # Making a data frame\nresults_test.sort_values(\"Accuracy\", ascending=False)","28dbbf87":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n\nrandom_grid ={'num_leaves': sp_randint(2, 100), \n             'min_child_samples': sp_randint(10, 1000), \n             'min_child_weight': [1e-9,1e-8,1e-7,1e-6,1e-5,1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.2, scale=0.8),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n\n\nlgb_model = lgb.LGBMClassifier()\nlgb_random = RandomizedSearchCV(\n    estimator=lgb_model,\n    param_distributions=random_grid,\n    n_iter=3000,\n    cv=4,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1,\n)\n\nlgb_random.fit(x_train, y_train)\nbp = dict(lgb_random.best_params_)\nbp","9b711734":"lgb_tunned = lgb.LGBMClassifier(\n    num_leaves=bp[\"num_leaves\"],\n    min_child_samples=bp[\"min_child_samples\"],\n    min_child_weight=bp[\"min_child_weight\"],\n    subsample=bp[\"subsample\"],\n    colsample_bytree=bp[\"colsample_bytree\"],\n    reg_alpha=bp[\"reg_alpha\"],\n    reg_lambda=bp[\"reg_lambda\"],\n)","7b47d88d":"import plotly.figure_factory as ff\n\ncolumns = [\"Classifier\", \"Mean\", \"CV\", \"Std\", \"Max\", \"Min\", \"TimeSpend (s)\"]\ncv_score = train_test_validation(lgb_tunned, \"LGB Tunned\", x_train, y_train)\nfig = ff.create_table([columns, cv_score])\nfig.show()","54d96b02":"from sklearn.metrics import confusion_matrix\n\nX_train_tunned, X_test_tunned, y_train_tunned, y_test_tunned = train_test_split(\n    x_train, y_train, test_size=0.2, random_state=42\n)\n\nlgb_tunned.fit(X_train_tunned, y_train_tunned)\n\nm_c = confusion_matrix(y_test_tunned, lgb_tunned.predict(X_test_tunned))\nplt.figure(figsize=(5, 4))\nsns.heatmap(m_c, annot=True, cmap=\"Reds\", fmt=\"d\").set(xlabel=\"Predict\", ylabel=\"Real\")","65373623":"lgb_tunned.fit(x_train, y_train)\ngender_submission[\"Survived\"] = lgb_tunned.predict(x_test)\ngender_submission.to_csv(\"LGBC\" + \"_submission.csv\", index=False)","dfd6589a":"### A little analysis of the data not formatted.","e5df3f78":"## Train with 'train_test_split'","8113571d":"This kernel is to do a predicting model which determine who could live or die in the titanic accident. I'm trying to make a model with three options of machine learning: Linear, Not Linear, and Tree. Here you will see a little data analysis and my results. For more information about the Kaggle competition click [here](https:\/\/www.kaggle.com\/c\/titanic).","a709d5fb":"### A little analysis of the formatted data.","6bf018c9":"### LGBMClassifier Model","7cb5061d":"Taken from the [competition overview](https:\/\/www.kaggle.com\/c\/titanic\/data)\n![image.png](attachment:image.png)","6194c1be":"# Importing libs and datasets","b2ced042":"## Training with Cross Validation","9168c4da":"# Submission","67177c6a":"# Overview","06fa1a5c":"#### Matrix","6086dfe9":"The results of this last model are our best and will pick to submission.","a858472e":"#### CV","8709bd99":"# Machine Learning"}}