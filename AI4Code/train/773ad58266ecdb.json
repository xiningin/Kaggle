{"cell_type":{"6a25a1a4":"code","a9c1d6ac":"code","89160aec":"code","6c386cb6":"code","44760be6":"code","2a449e42":"code","8bc991b8":"code","a03b5a63":"code","05af74ea":"code","777e11bf":"code","63d7512f":"code","da95cca3":"code","2fbe0407":"code","cfe55d8a":"code","f467d33a":"code","6cd33408":"code","65af8f6a":"code","9e1794b1":"code","5fdfddf0":"code","69c0306b":"code","e5858acd":"code","d1f27117":"code","d66fceaa":"code","38d8371a":"code","32d02da5":"code","c0269079":"code","6490e8a2":"code","e7ddd161":"code","d3a2b985":"code","2f8edefa":"code","eef15dac":"code","b7c014f7":"code","3770a8a9":"code","ec5853c7":"code","19b8d2c6":"code","e8309c35":"code","ac148a23":"code","cc0e2e7d":"code","b0c65b2f":"code","1c716d06":"code","ced6e1b0":"code","ea27e959":"code","354588f2":"markdown","6dd4506c":"markdown","4278f743":"markdown","4f148f91":"markdown","333e2e45":"markdown","cf7c6f5d":"markdown","84a66ebf":"markdown","694670ff":"markdown","295536ab":"markdown","8194b2f4":"markdown","2f214e84":"markdown","7e061935":"markdown","2274dd1d":"markdown","f1415b86":"markdown","6fbc3d17":"markdown","df0b97a7":"markdown","c0758929":"markdown","f92b2ce2":"markdown","eb78172d":"markdown","fe26421e":"markdown","8507efa3":"markdown","37ce611b":"markdown","6978750c":"markdown","9e33f23c":"markdown","6ddc11ff":"markdown","8f51847d":"markdown","77862c76":"markdown","a43986aa":"markdown","1c798e34":"markdown","5c39fc53":"markdown","db2c952a":"markdown","93626b49":"markdown","4dd8e07f":"markdown","270c70f2":"markdown","e788fd7a":"markdown","334bbcc1":"markdown","d7478ef1":"markdown","19215e16":"markdown","a08fb405":"markdown","be00b670":"markdown","30ff6d9b":"markdown"},"source":{"6a25a1a4":"import tensorflow as tf\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nprint(tf.__version__)\nprint(\"ALL MODULES IMPORTED SUCCESSFULLY\")","a9c1d6ac":"(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n\nX_train = X_train.astype('float32')\/255.\nX_test = X_test.astype('float32') \/255.\n\nprint()\nprint(f\"Shape of Train Data: {X_train.shape}\")\nprint(f\"Shape of Test Data: {X_test.shape}\")","89160aec":"fashionMNIST_classes = {0:\"T-shirt\/top\", 1:\"Trouser\",\n                        2:\"Pullover\", 3:\"Dress\",\n                        4:\"Coat\", 5:\"Sandal\",\n                        6:\"Shirt\", 7:\"Sneaker\", \n                        8:\"Bag\", 9:\"Ankle boot\"}","6c386cb6":"n = 10\nplt.figure(figsize = (20, 4))\nfor i in range(n):\n    plt.subplot(1, n, i+1)\n    idx = np.random.randint(i, len(X_train))\n    plt.imshow(X_train[idx])\n    plt.title(fashionMNIST_classes[y_train[idx]])\n    plt.gray()\n    plt.xticks([]); plt.yticks([])\nplt.show()","44760be6":"EMBEDDING_DIM = 64\n\nclass MNIST_AutoEncoder(Model):\n    def __init__(self, embedding_dim):\n        super(MNIST_AutoEncoder, self).__init__()\n        self.embedding_dim = embedding_dim\n\n        # Building our Encoder\n        self.encoder = tf.keras.Sequential([\n            layers.Input(shape=(28, 28)), \n            layers.Flatten(),\n            layers.Dense(self.embedding_dim, activation = \"relu\"),\n        ])\n\n        # Building our Decoder\n        self.decoder = tf.keras.Sequential([\n            layers.Input(shape=(self.embedding_dim)),\n            layers.Dense(784, activation=\"sigmoid\"),      \n            layers.Reshape((28, 28))        \n        ])\n\n    def call(self, X):\n        encoded = self.encoder(X)\n        decoded = self.decoder(encoded)\n        return decoded","2a449e42":"vanillaAutoencoder = MNIST_AutoEncoder(EMBEDDING_DIM)\nvanillaAutoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())","8bc991b8":"vanillaAutoencoder.encoder.summary()","a03b5a63":"vanillaAutoencoder.decoder.summary()","05af74ea":"mnist_training_history = vanillaAutoencoder.fit(x = X_train, y = X_train,\n                                       epochs = 10, \n                                       batch_size = 16,\n                                       shuffle = True,\n                                       validation_data = (X_test, X_test))","777e11bf":"# Storing our training and validation losses\ntrain_loss = mnist_training_history.history[\"loss\"]\nval_loss = mnist_training_history.history[\"val_loss\"]\nepochs = [d for d in range(1,11)]\n\n# Code for plotting train and val loss\nplt.figure(figsize=(18, 10))\nplt.plot(epochs, train_loss, '-r', label=\"Training\")\nplt.plot(epochs, val_loss, '--b', label=\"Validation\")\nplt.title(\"LOSS Curve for Fashion MNIST\")\nplt.grid(True)\nplt.xlabel('Epochs'); plt.ylabel(\"LOSS\")\nplt.legend()\nplt.xticks([d for d in range(1, 11)])\nplt.show();","63d7512f":"# collecting the encoded and decoded versions of our test data\nencoded_imgs = vanillaAutoencoder.encoder(X_test).numpy()\ndecoded_imgs = vanillaAutoencoder.decoder(encoded_imgs).numpy()\n\n# Plotting our original and reconstructed images\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n  # To display the original images\n  ax = plt.subplot(2, n, i + 1)\n  idx = np.random.randint(i, len(X_test))\n  plt.imshow(X_test[idx])\n  plt.title(\"original\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\n\n  # To display reconstructed images\n  ax = plt.subplot(2, n, i + 1 + n)\n  plt.imshow(decoded_imgs[idx])\n  plt.title(\"reconstructed\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\nplt.show()","da95cca3":"VAL_LOC = \"..\/input\/only-faces\/Only_faces\/Validation\"\nTRAIN_LOC = \"..\/input\/only-faces\/Only_faces\/Training\"","2fbe0407":"def build_data(img_folder_path, normalize = True):\n    labels = []\n    images = []\n    for class_ in os.listdir(img_folder_path):\n        img_paths = f\"{img_folder_path}\/{class_}\"\n        for img in os.listdir(img_paths):\n            image = cv2.imread(os.path.join(img_paths, img))\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = image.astype('float32')\n            if normalize:\n                image = image\/255.\n            image = cv2.resize(image, (128, 128))\n\n            images.append(image)\n            labels.append(class_)\n    images = np.array(images)\n    return images, labels","cfe55d8a":"train_imgs, train_labels = build_data(TRAIN_LOC)\nval_imgs, val_labels = build_data(VAL_LOC)","f467d33a":"print(f\"Shape of TRAIN DATA : {train_imgs.shape}\")\nprint(f\"Shape of VAL DATA : {val_imgs.shape}\")","6cd33408":"n = 5\nplt.figure(figsize = (20, 17))\nfor i in range(n):\n    plt.subplot(1, n, i+1)\n    idx = np.random.randint(i, len(train_imgs))\n    plt.imshow(train_imgs[idx])\n    plt.title(train_labels[idx])\n    plt.gray()\n    plt.xticks([]); plt.yticks([])\nplt.show()","65af8f6a":"class Face_Autoencoder:\n        \n    def __init__(self,\n                 input_dim,\n                 encoder_conv_filters,\n                 encoder_conv_kernel_size,\n                 encoder_conv_strides,\n                 decoder_conv_t_filters,\n                 decoder_conv_t_kernel_size,\n                 decoder_conv_t_strides,\n                 z_dim,\n                 use_batch_norm = False,\n                 use_dropout= False\n                ):\n\n        self.input_dim = input_dim\n        self.encoder_conv_filters = encoder_conv_filters\n        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n        self.encoder_conv_strides = encoder_conv_strides\n\n        self.decoder_conv_t_filters = decoder_conv_t_filters\n        self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n        self.decoder_conv_t_strides = decoder_conv_t_strides\n        self.z_dim = z_dim\n\n        self.use_batch_norm = use_batch_norm\n        self.use_dropout = use_dropout\n\n        self.n_layers_encoder = len(encoder_conv_filters)\n        self.n_layers_decoder = len(decoder_conv_t_filters)\n\n    def build(self):\n        # Encoder\n        encoder_input = layers.Input(shape=self.input_dim, name='encoder_input')\n\n        x = encoder_input\n\n        for i in range(self.n_layers_encoder):\n           \n            conv_layer = layers.Conv2D(\n                filters = self.encoder_conv_filters[i],\n                kernel_size = self.encoder_conv_kernel_size[i],\n                strides = self.encoder_conv_strides[i],\n                padding = 'same',\n                name = 'encoder_conv_' + str(i)\n                )\n\n            x = conv_layer(x)\n\n            if self.use_batch_norm:\n                x = layers.BatchNormalization()(x)\n           \n            x = layers.Activation(\"relu\")(x)\n\n            if self.use_dropout:\n                x = layers.Dropout(rate = 0.25)(x)\n\n           \n        shape_before_flattening = K.int_shape(x)[1:]\n\n        x = layers.Flatten()(x)\n        embedding = layers.Dense(128, name=\"Embedding\")(x)\n\n        # DECODER\n        x = layers.Dense(np.prod(shape_before_flattening))(embedding)\n        x = layers.Reshape(shape_before_flattening)(x)\n\n        for i in range(self.n_layers_decoder):           \n            conv_t_layer = layers.Conv2DTranspose(\n                filters = self.decoder_conv_t_filters[i],\n                kernel_size = self.decoder_conv_t_kernel_size[i],\n                strides = self.decoder_conv_t_strides[i],\n                padding = 'same',\n                name = 'decoder_conv_t_' + str(i)\n                )\n\n            x = conv_t_layer(x)\n           \n            if i < self.n_layers_decoder - 1:\n                if self.use_batch_norm:\n                    x = layers.BatchNormalization()(x)\n                x = layers.Activation('relu')(x)\n                if self.use_dropout:\n                    x = layers.Dropout(rate = 0.25)(x)\n            else:\n              x = layers.Activation('sigmoid')(x)           \n           \n\n        decoder_output = x\n\n        ae = Model(inputs=encoder_input, outputs=decoder_output, name=\"Face_Autoencoder\")\n        print(ae.summary())\n        return ae\n\n    \n    def __str__():\n        return \"FACE AUTOENCODER\"","9e1794b1":"IMG_SHAPE = (128, 128, 3)\nfae = Face_Autoencoder(input_dim = IMG_SHAPE,\n                    encoder_conv_filters=[32,64,64,64],\n                    encoder_conv_kernel_size=[3,3,3,3],\n                    encoder_conv_strides=[2,2,2,2],\n                    decoder_conv_t_filters=[64,64,32,3],\n                    decoder_conv_t_kernel_size=[3,3,3,3],\n                    decoder_conv_t_strides=[2,2,2,2],\n                    z_dim=128,\n                    use_batch_norm = False,\n                    use_dropout = False)","5fdfddf0":"face_ae = fae.build()","69c0306b":"plot_model(face_ae, show_shapes = True, show_layer_names=True)","e5858acd":"# Training Hyperparamters\nLR = 1e-3\nBS = 16\nEPOCHS = 20\n\n# Defining Callbacks\nclass ShowLearning(Callback):\n    def on_epoch_end(self, epoch, logs = {}):       \n\n        original_imgs = [\n                val_imgs[val_labels.index('Subject1') + 17],\n                val_imgs[val_labels.index('Subject2')],\n                val_imgs[val_labels.index('Subject3')],\n                val_imgs[val_labels.index('Subject4') + 2],\n                train_imgs[train_labels.index('Subject5') + 7],\n                val_imgs[val_labels.index('Subject6') + 4]                 \n                ]\n        decoded_imgs = []\n\n        for img in original_imgs:\n            decoded_imgs.append(face_ae.predict(np.array([img]))[0])\n\n        n = len(original_imgs)\n        plt.figure(figsize=(20, 7))\n        for i in range(1, n + 1):\n            # Display original\n            ax = plt.subplot(2, n, i)\n            plt.imshow(original_imgs[i-1]);\n            plt.gray()\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)\n            plt.title(\"Original\")    \n\n            # Display reconstruction\n            ax = plt.subplot(2, n, i + n)\n            plt.imshow(decoded_imgs[i-1]);\n            plt.gray()\n            ax.get_xaxis().set_visible(False)\n            ax.get_yaxis().set_visible(False)\n            plt.title(\"Reconstructed\")\n\n        plt.show();\n\n\nshow_learning = ShowLearning()\n\n\ncallbacks = [show_learning]\n\n# Model Compilation\nface_ae.compile(loss = 'mse', optimizer = \"adam\")","d1f27117":"face_training_history = face_ae.fit(train_imgs, train_imgs,                                   \n                            epochs = EPOCHS,\n                            verbose = 1,\n                            validation_data = (val_imgs, val_imgs),\n                            batch_size = BS,\n                            shuffle = True,\n                            callbacks = callbacks)","d66fceaa":"# Storing our training and validation losses\ntrain_loss = face_training_history.history[\"loss\"]\nval_loss = face_training_history.history[\"val_loss\"]\nepochs = [d for d in range(1,EPOCHS + 1)]\n\n# Code for plotting train and val loss\nplt.figure(figsize=(18, 10))\nplt.plot(epochs, train_loss, '-r', label=\"Training\")\nplt.plot(epochs, val_loss, '--b', label=\"Validation\")\nplt.title(\"LOSS Curve for Face reconstruction\")\nplt.grid(True)\nplt.xlabel('Epochs'); plt.ylabel(\"LOSS\")\nplt.legend()\nplt.xticks(epochs)\nplt.show();","38d8371a":"decoded_imgs = face_ae.predict(val_imgs)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n  # display original\n  idx = np.random.randint(i, len(val_imgs))\n  ax = plt.subplot(2, n, i + 1)\n  plt.imshow(val_imgs[idx])\n  plt.title(\"original\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\n\n  # display reconstruction\n  ax = plt.subplot(2, n, i + 1 + n)\n  plt.imshow(decoded_imgs[idx])\n  plt.title(\"reconstructed\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\nplt.show()","32d02da5":"noise_factor = 0.3\ntrain_noisy_imgs = train_imgs + noise_factor*tf.random.normal(shape=train_imgs.shape)\nval_noisy_imgs = val_imgs + noise_factor*tf.random.normal(shape=val_imgs.shape)\n\ntrain_noisy_imgs = tf.clip_by_value(train_noisy_imgs, clip_value_min = 0., clip_value_max = 1.)\nval_noisy_imgs = tf.clip_by_value(val_noisy_imgs, clip_value_min = 0., clip_value_max = 1.)","c0269079":"n = 7\nplt.figure(figsize=(20, 6))\nfor i in range(1, n + 1):\n    # Display original\n    ax = plt.subplot(2, n, i)\n    idx = np.random.randint(len(train_imgs))\n    plt.imshow(train_imgs[idx]);\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.title(\"Original\")    \n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(train_noisy_imgs[idx]);\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    plt.title(\"Original + Noise\")\n\nplt.show();","6490e8a2":"IMG_SHAPE = (128, 128, 3)\nfae = Face_Autoencoder(input_dim = IMG_SHAPE,\n                    encoder_conv_filters=[32,64,64,64],\n                    encoder_conv_kernel_size=[3,3,3,3],\n                    encoder_conv_strides=[2,2,2,2],\n                    decoder_conv_t_filters=[64,64,32,3],\n                    decoder_conv_t_kernel_size=[3,3,3,3],\n                    decoder_conv_t_strides=[2,2,2,2],\n                    z_dim=128,\n                    use_batch_norm = False,\n                    use_dropout = False)\n\ndenoisy_fae = fae.build()","e7ddd161":"# Training Hyperparamters\nLR = 1e-3\nBS = 16\nEPOCHS = 10\n\n# Model Compilation\ndenoisy_fae.compile(loss = 'mse', optimizer = \"adam\")","d3a2b985":"denoisy_training_history = denoisy_fae.fit(train_noisy_imgs, train_imgs,\n                            epochs = EPOCHS,\n                            batch_size = BS,\n                            shuffle = True,\n                            validation_data = (val_noisy_imgs, val_imgs))","2f8edefa":"# Storing our training and validation losses\ntrain_loss = denoisy_training_history.history[\"loss\"]\nval_loss = denoisy_training_history.history[\"val_loss\"]\nepochs = [d for d in range(1,EPOCHS + 1)]\n\n# Code for plotting train and val loss\nplt.figure(figsize=(18, 10))\nplt.plot(epochs, train_loss, '-r', label=\"Training\")\nplt.plot(epochs, val_loss, '--b', label=\"Validation\")\nplt.title(\"LOSS Curve for Face reconstruction\")\nplt.grid(True)\nplt.xlabel('Epochs'); plt.ylabel(\"LOSS\")\nplt.legend()\nplt.xticks(epochs)\nplt.show();","eef15dac":"decoded_imgs = denoisy_fae.predict(val_noisy_imgs)\n\nn = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n\n    # display original + noise\n    ax = plt.subplot(2, n, i + 1)\n    plt.title(\"original + noise\")\n    idx = np.random.randint(i, len(val_imgs))\n    plt.imshow(tf.squeeze(val_noisy_imgs[idx]))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    bx = plt.subplot(2, n, i + n + 1)\n    plt.title(\"reconstructed\")\n    plt.imshow(tf.squeeze(decoded_imgs[idx]))\n    plt.gray()\n    bx.get_xaxis().set_visible(False)\n    bx.get_yaxis().set_visible(False)\nplt.show()","b7c014f7":"for i, layer in enumerate(face_ae.layers):\n    print(i, layer.name)","3770a8a9":"encoder = Model(face_ae.input, face_ae.layers[10].output, name=\"Encoder\")\nencoder.summary()","ec5853c7":"sub1_encs = []\nsub2_encs = []\nsub3_encs = []\n\ndef preprocess_img(image):\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = image.astype('float32')\n    image = image\/255.\n    image = cv2.resize(image, (128, 128))\n    return image\n\n\nfor sub_img in os.listdir(\"..\/input\/only-faces\/Only_faces\/Training\/Subject1\")[:10]:\n    image = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Training\/Subject1\", sub_img))\n    image = preprocess_img(image)\n    enc_img = encoder.predict(np.array([image]))\n    sub1_encs.append(enc_img)\n\nfor sub_img in os.listdir(\"..\/input\/only-faces\/Only_faces\/Training\/Subject2\")[:10]:\n    image = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Training\/Subject2\", sub_img))\n    image = preprocess_img(image)\n    enc_img = encoder.predict(np.array([image]))\n    sub2_encs.append(enc_img)\n\nfor sub_img in os.listdir(\"..\/input\/only-faces\/Only_faces\/Training\/Subject3\")[:10]:\n    image = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Training\/Subject3\", sub_img))\n    image = preprocess_img(image)\n    enc_img = encoder.predict(np.array([image]))\n    sub3_encs.append(enc_img)\n\nsub1_encs = np.array(sub1_encs)\nsub2_encs = np.array(sub2_encs)\nsub3_encs = np.array(sub3_encs)","19b8d2c6":"# Creating base encodings for our subjects\n\nsub1_enc = np.mean(sub1_encs, axis = 0)\nsub2_enc = np.mean(sub2_encs, axis = 0)\nsub3_enc = np.mean(sub3_encs, axis = 0)","e8309c35":"rand_idx = np.random.randint(len(val_imgs))\n\n\nrand_sub1_img = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject1\",\n                                        os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject1\")[np.random.randint(len(os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject1\")))]))\n\nrand_sub2_img = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject2\",\n                                        os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject2\")[np.random.randint(len(os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject2\")))]))\n\nrand_sub3_img = cv2.imread(os.path.join(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject3\",\n                                        os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject3\")[np.random.randint(len(os.listdir(\"..\/input\/only-faces\/Only_faces\/Validation\/Subject3\")))]))\n\nrand_sub1_img = preprocess_img(rand_sub1_img)\nrand_sub2_img = preprocess_img(rand_sub2_img)\nrand_sub3_img = preprocess_img(rand_sub3_img)\n\nrand_sub1_enc = np.mean(encoder.predict(np.array([rand_sub1_img])), axis = 0)\nrand_sub2_enc = np.mean(encoder.predict(np.array([rand_sub2_img])), axis = 0)\nrand_sub3_enc = np.mean(encoder.predict(np.array([rand_sub3_img])), axis = 0)","ac148a23":"from scipy.spatial.distance import euclidean\n\ndef euclidean_dist(base_encs, unknown_enc):\n    distances = []\n    for base_enc in base_encs:\n        distances.append(euclidean(base_enc, unknown_enc))\n    distances  = np.array(distances)\n    return distances","cc0e2e7d":"\"\"\"\nbase-encs : DB containing image encodings of subjects\nrand_encs : Random subjects\nrand_encs : Encodings of random subjects\n\n\"\"\"\nbase_encs = [sub1_enc, sub2_enc, sub3_enc]\nrand_imgs = [rand_sub1_img, rand_sub2_img, rand_sub3_img]\nrand_encs = [rand_sub1_enc, rand_sub2_enc, rand_sub3_enc]\n\ndistances = []\nfor rand_enc in rand_encs:\n    distances.append(euclidean_dist(base_encs, rand_enc))\nsubjects = [\"SUBJECT1\", \"SUBJECT2\", \"SUBJECT3\"]\ndistances","b0c65b2f":"# analogue to DataBase contaning employess images\n\nbase_imgs = [train_imgs[train_labels.index('Subject1')],\n            train_imgs[train_labels.index('Subject2')],\n            train_imgs[train_labels.index('Subject3')]]","1c716d06":"plt.figure(figsize=(20, 17))\n\nplt.subplot(1, 4, 1)\nplt.title('UNKOWN')\nplt.imshow(rand_imgs[0])\nplt.xlabel(f\"Identified as : {subjects[np.argmin(distances[0])]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 2)\nplt.title('SUBJECT1')\nplt.imshow(base_imgs[0])\nplt.xlabel(f\"Distance : {distances[0][0]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 3)\nplt.title('SUBJECT2')\nplt.imshow(base_imgs[1])\nplt.xlabel(f\"Distance : {distances[0][1]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 4)\nplt.title('SUBJECT3')\nplt.imshow(base_imgs[2])\nplt.xlabel(f\"Distance : {distances[0][2]}\")\nplt.xticks([]); plt.yticks([])\n\n\n\nplt.show();","ced6e1b0":"plt.figure(figsize=(20, 17))\n\nplt.subplot(1, 4, 1)\nplt.title('UNKOWN')\nplt.imshow(rand_imgs[1])\nplt.xlabel(f\"Identified as : {subjects[np.argmin(distances[1])]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 2)\nplt.title('SUBJECT1')\nplt.imshow(base_imgs[0])\nplt.xlabel(f\"Distance : {distances[1][0]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 3)\nplt.title('SUBJECT2')\nplt.imshow(base_imgs[1])\nplt.xlabel(f\"Distance : {distances[1][1]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 4)\nplt.title('SUBJECT3')\nplt.imshow(base_imgs[2])\nplt.xlabel(f\"Distance : {distances[1][2]}\")\nplt.xticks([]); plt.yticks([])\n\n\n\nplt.show();","ea27e959":"plt.figure(figsize=(20, 17))\n\nplt.subplot(1, 4, 1)\nplt.title('UNKOWN')\nplt.imshow(rand_imgs[2])\nplt.xlabel(f\"Identified as : {subjects[np.argmin(distances[2])]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 2)\nplt.title('SUBJECT1')\nplt.imshow(base_imgs[0])\nplt.xlabel(f\"Distance : {distances[2][0]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 3)\nplt.title('SUBJECT2')\nplt.imshow(base_imgs[1])\nplt.xlabel(f\"Distance : {distances[2][1]}\")\nplt.xticks([]); plt.yticks([])\n\nplt.subplot(1, 4, 4)\nplt.title('SUBJECT3')\nplt.imshow(base_imgs[2])\nplt.xlabel(f\"Distance : {distances[2][2]}\")\nplt.xticks([]); plt.yticks([])\n\n\n\nplt.show();","354588f2":"<h4><em><strong>Face Verification is<\/strong> 1 : 1 MATCHING<\/em><\/h4>\n<h5><em>where as<\/em><\/h5>\n    <h4><em><strong>Face Identification is<\/strong> 1 : N MATCHING<\/em><\/h4>","6dd4506c":"<div align=\"center\">\n<img src=\"https:\/\/static.packt-cdn.com\/products\/9781789612011\/graphics\/06965e3d-46b3-418b-a4c6-a39df696159c.png\"\/>\n<\/div>","4278f743":"<h3><b>Types of Autoencoders : <\/b><\/h3>\n<p>\n<ol>\n<li>Vanilla Autoencoder<\/li>\n<li>Sparse Autoencoder<\/li>\n<li>Convolutional Autoencoder<\/li>\n<li>Denoising Autoencoder<\/li>\n<li>Variational Autoencoder<\/li>\n<\/ol>\netc...\n<\/p>\n","4f148f91":"<p><b>Function for loading train and validation data<\/b><\/p>","333e2e45":"#### **Training our Autoencoder**","cf7c6f5d":"#### **Building our Autoencoder**","84a66ebf":"# Points to Remember !!!\n<img src=\"https:\/\/cdn1.vectorstock.com\/i\/thumb-large\/95\/50\/pointing-to-forehead-emoticon-vector-27559550.jpg\"\/>\n\n\n<ul>\n<li>Data specific compression<\/li>\n<li>Unsupervised<\/li>\n<li>Lossy in nature<\/li>\n<\/ul>\n","694670ff":"#### **Loading Data**\n<ul>\n<p><strong>DESC: <\/strong><\/p>\n<li>Self-made dataset by ML4e members.<\/li>\n<li>It has ~2k images for training and ~1k for validation.<\/li>\n<li>Images are of RGB format<\/li>\n<li># of classes : 6<\/li>\n<li>Images has variations like\n<ol>\n<li>Different facial experssions<\/li>\n<li>Lighting conditions<\/li>\n<li>Face alignments<\/li>\n<\/ol>\n\n<\/li>\n\n<ul>","295536ab":"<h4><b>What is Face Recognition<\/b><\/h4>\n<p>A facial recognition system is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image.<\/p>","8194b2f4":"We will scipy's euclidean distance function to calculate the closeness between two images","2f214e84":"### **Objective#2 : Face reconstruction via autoencoder**","7e061935":"### **Objective#1 : Training an AUTOENCODER over Fashion MNIST Dataset**","2274dd1d":"# Introduction to Autoencoders ","f1415b86":"<ul>\n<li><b><i>Autoencoders<\/i><\/b> are a type of neural network that attempts to mimic its input as closely as possible to its output.<\/li>\n<li>It has aims to take an input, transform it into a reduced representation called <b>Embedding or Latent Space representation<\/b>. Then, this embedding is transformed back into the original input.<\/li>\n<\/ul>","6fbc3d17":"<p>This notebook is aimed at Machine Learning and Deep Learning beginners who are interested in getting a brief understanding of the underlying concepts behind <strong>autoencoders<\/strong><p>","df0b97a7":"# Objectives\n\n<ol>\n<li>To demonstrate how we can train an AUTOENCODER over <b>Fashion MNIST Dataset<\/b><\/li>\n<li>To demostrate how we can use an AUTOENCODER for <strong>Face reconstruction<\/strong><\/li>\n<li>To demonstrate how we can perform <b>Image Denoising<\/b> via an AUTOENCODER<\/li>\n<li>To demonstrate how we can build a <b>Face Recognition<\/b> using an AUTOENCODER<\/li>\n<\/ol>\n\n\n\n","c0758929":"<h4><b>Callbacks : <\/b> A callback is a powerful tool to customize the behavior of your model during training, evaluation, or inference.<\/h4>\n<h4>Tensorflow has some built-in callbacks which we can access from tf.keras.callback module<\/h4>\n<h4>Examples ...<\/h4>\n<ul>\n<li>Wants to visualize your training process : <strong>Tensorboard<\/strong><\/li>\n<li>Wants to schedule your LR while training: <strong>LearningRateSchedular<\/strong><\/li>\n<li>Wants to save best weights in the training process : <strong>ModelCheckpoint<\/strong><\/li>\n<li>Wants if stop the training before the completion of all epochs : <strong>EarlyStopping<\/strong><\/li>\n<\/ul>\n<p>Tensorflow also has provided a Class <b>Callback<\/b> which helps us to create some custom callbacks<\/p>","f92b2ce2":"## Importing Required Modules","eb78172d":"#### **Results**","fe26421e":"<p>Here, I am considering 3 classes from my training set<\/p>","8507efa3":"<h3><b>Architecture of Autoencoders<\/b><h3>\n<p>An Autoencoder consists of three main components : <\/p>\n\n* Encoder\n* Embedding\n* Decoder\n\n<b><em>NOTE: The decoder architecture is the mirror image of an encoder<\/em><\/b>","37ce611b":"## What is an **AUTOENCODER** ?","6978750c":"<div align=\"center\">\n<img src=\"https:\/\/miro.medium.com\/max\/1784\/1*CRP98rEvxpcdo6gJI3tXWA.png\"\/>\n<\/div>","9e33f23c":"#### **Loading Dataset**\n<ul>\n<li><p><strong>DESC: <\/strong>Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.<\/p><\/li>\n\n<li><p><strong>Why we are dividing the arrays by 255 ??<\/strong>\n<ol>\n<li>To prevent overshooting of activation values.<\/li>\n<li>To have a homogenous distribution of weights and biases.<\/li>\n<\/ol><\/p><\/li>\n<ul>","6ddc11ff":"Lets check for our 2nd random image","8f51847d":"## Building our Encoder","77862c76":"#### **Sample Images**","a43986aa":"#### **Sample Images**","1c798e34":"#### **Building our Autoencoder**","5c39fc53":"<div align=\"center\">\n<img src=\"https:\/\/www.learnopencv.com\/wp-content\/uploads\/2019\/04\/face_verification_vs_recognition.jpg\"\/>\n<\/div>","db2c952a":"Lets check for our 3rd random image","93626b49":"Lets check for our 1st random image","4dd8e07f":"### **Objective#3 : Image Denoising via Autoencoder**","270c70f2":"<div align=\"center\"><img src=\"https:\/\/miro.medium.com\/max\/724\/1*qKiQ1noZdw8k05-YRIl6hw.jpeg\" width=\"800px\" height=\"400px\"\/><\/div>","e788fd7a":"So, we saw that the randomly choosen images were correctly identified","334bbcc1":"<p align=\"left\">\n<img src=\"https:\/\/miro.medium.com\/max\/3110\/0*uq2_ZipB9TqI9G_k\"\/>\n<\/p>","d7478ef1":"### **Objective#4 : Face identification via Autoencoder**","19215e16":"<p>We are using <strong>Model<\/Strong> class from tf.keras.model submodule to build our Encoder. It requires the input layer and the output layer. We can also pass a name to our model by using the <em><b>name<\/b><\/em> parameter.<\/p>","a08fb405":"#### **Results**","be00b670":"#### **Results**","30ff6d9b":"<p>We are iterating over the layers, so as to get the index of our embedding layer<\/p>\n"}}