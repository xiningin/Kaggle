{"cell_type":{"9d3289bd":"code","e62f996c":"code","414d11ce":"code","8bb42b3b":"code","0d6ebec0":"code","7d048f42":"code","3ce51a00":"code","fdda5d46":"code","e62060d2":"code","63c7fb35":"code","07c78cf8":"code","a95537d0":"code","5cf36c07":"code","8749d5aa":"code","45c8730e":"code","14cce946":"code","fc33389e":"code","8bc3730c":"code","4fa01cc6":"code","4f86b304":"code","e1e0c7c1":"code","9d85093f":"code","26a54bf2":"markdown","ddda414d":"markdown","4bffbef9":"markdown","fcf7b98e":"markdown","f683f40b":"markdown","a25bca27":"markdown","5e1731b7":"markdown","f63dee7c":"markdown","13ae406b":"markdown","776de87c":"markdown","1205fda2":"markdown","eb78dd2d":"markdown","2bd0e4c8":"markdown"},"source":{"9d3289bd":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM\n!apt-get install -y -qq libboost-all-dev","e62f996c":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","414d11ce":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","8bb42b3b":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","0d6ebec0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.\n\n# sklearn, LGBM and CatBoost\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nimport catboost as ctb\n\n# Visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\")","7d048f42":"# Loading datasets\ndf_train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\n\nprint(\"Train shape: \" + str(df_train.shape))\nprint(\"Test shape: \" + str(df_test.shape))\n\n# Splitting the target variable and the features\nX_train = df_train.loc[:,'var_0':]\ny_train = df_train.loc[:,'target']\n\nprint(X_train.shape)\nprint(y_train.shape)","3ce51a00":"synthetic_samples_indexes = pd.read_csv('..\/input\/synthetic-samples-indexes\/synthetic_samples_indexes.csv')\n\ndf_test_real = df_test.copy()\ndf_test_real = df_test_real[~df_test_real.index.isin(list(synthetic_samples_indexes['synthetic_samples_indexes']))]\nX_test = df_test_real.loc[:,'var_0':]\nX_test.shape","fdda5d46":"def get_count(df):\n    '''\n    Function that adds frequency columns for each variable (excluding 'ID_code', 'target').\n    New columns names will be like 'var_X_count'.\n    '''\n    for var in [i for i in df.columns if i not in ['ID_code','target']]:\n        df[var+'_count'] = df.groupby(var)[var].transform('count')\n    return df","e62060d2":"# Using both train and (real) test datasets to creates frequencies columns\n\nX_tot = pd.concat([X_train, X_test])\nprint(X_tot.shape)\n\nstart = time.time()\nX_tot = get_count(X_tot)\nend = time.time()\nprint('Frequency encoding took %.2f seconds\\nShape: ' %(end - start))\nprint(X_tot.shape)\n\nX_train_count = X_tot.iloc[0:200000]\nX_test_count = X_tot.iloc[200000:]","63c7fb35":"# 0.8 train, 0.2 dev\nX_train,X_valid,y_train,y_valid = train_test_split(X_train_count, y_train, test_size=0.2, random_state=42, stratify=y_train)\n\nprint('X_train shape: {}\\n'.format(X_train.shape))\nprint('y_train shape: {}\\n'.format(y_train.shape))\nprint('X_valid shape: {}\\n'.format(X_valid.shape))\nprint('y_valid shape: {}'.format(y_valid.shape))\n\n# List of all the features\nfeatures = [c for c in X_train.columns if c not in ['ID_code', 'target']]","07c78cf8":"# Data Augmentation\n\ndef augment(x,y,t=2, model = 'lgbm'):\n    '''\n    Data Augmentation tx if y = 1 , (t\/2)x if y = 0. Default t=6.\n    Model could be 'lgbm' (Default) or 'catboost'\n    '''\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(int(x1.shape[1]\/2)):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n            x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n        xs.append(x1)\n\n    if model == 'lgbm':\n        for i in range(t\/\/2):\n            mask = y==0\n            x1 = x[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(int(x1.shape[1]\/2)):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n            xn.append(x1)\n    elif model == 'catboost':\n        for i in range(t\/\/3):\n            mask = y==0\n            x1 = x[mask].copy()\n            ids = np.arange(x1.shape[0])\n            for c in range(int(x1.shape[1]\/2)):\n                np.random.shuffle(ids)\n                x1[:,c] = x1[ids][:,c]\n                x1[:,c+200] = x1[ids][:,c+200] # The new features must go with their original one!\n            xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","a95537d0":"start = time.time()\n# Augmentation Only for training set!\nX_tr_lgbm, y_tr_lgbm = augment(X_train.values, y_train.values, t=2, model = 'lgbm')\n# X_tr_lgbm, y_tr_lgbm = augment(X_train.values, y_train.values, t=6, model = 'lgbm')\n# With t=6 we can see a larger gap between gpu and cpu lgbm!\n\nprint('X_tr_lgbm Augmented shape: {}'.format(X_tr_lgbm.shape))\nend = time.time()\nprint('t=2 Augmentation took %.2f seconds' %(end - start))\n\nstart2 = time.time()\nX_tr_cat, y_tr_cat = augment(X_train.values, y_train.values, t=15, model = 'catboost')\nprint('X_tr_cat Augmented shape: {}'.format(X_tr_cat.shape))\nend2 = time.time()\nprint('t=15 Augmentation took %.2f seconds' %(end2 - start2))\n\nX_tr_lgbm = pd.DataFrame(data=X_tr_lgbm,columns=X_train.columns)\ny_tr_lgbm = pd.DataFrame(data=y_tr_lgbm)\ny_tr_lgbm.columns = ['target']\n\nX_tr_cat = pd.DataFrame(data=X_tr_cat,columns=X_train.columns)\ny_tr_cat = pd.DataFrame(data=y_tr_cat)\ny_tr_cat.columns = ['target']","5cf36c07":"performance = {}","8749d5aa":"# CPU parameters\n\nlgb_params = {\n        'bagging_fraction': 0.77,\n        'bagging_freq': 2,\n        'lambda_l1': 0.7,\n        'lambda_l2': 2,\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'min_data_in_leaf': 22,\n        'min_gain_to_split': 0.07,\n        'min_sum_hessian_in_leaf': 19,\n        'num_leaves': 20,\n        'feature_fraction': 1,\n        'save_binary': True,\n        'seed': 42,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'drop_seed': 42,\n        'data_random_seed': 42,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': 'false',\n        'num_threads': 6\n}","45c8730e":"trn_data = lgb.Dataset(X_tr_lgbm, label=y_tr_lgbm) # Augmentation\nval_data = lgb.Dataset(X_valid, label=y_valid)\n\nstart = time.time()\n# Training\nclf = lgb.train(lgb_params, trn_data, 100000, valid_sets = [val_data], verbose_eval=-1, early_stopping_rounds = 3000)\nend = time.time()\n\nlgb_cpu = end - start\nprint('>> CPU: It took %.2f seconds' %(lgb_cpu))\n\nval_pred = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['lgb_cpu'] = lgb_cpu","14cce946":"# GPU parameters\n\nlgb_params = {\n        'bagging_fraction': 0.77,\n        'bagging_freq': 2,\n        'lambda_l1': 0.7,\n        'lambda_l2': 2,\n        'learning_rate': 0.01,\n        'max_depth': 3,\n        'min_data_in_leaf': 22,\n        'min_gain_to_split': 0.07,\n        'min_sum_hessian_in_leaf': 19,\n        'num_leaves': 20,\n        'feature_fraction': 1,\n        'save_binary': True,\n        'seed': 42,\n        'feature_fraction_seed': 42,\n        'bagging_seed': 42,\n        'drop_seed': 42,\n        'data_random_seed': 42,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbosity': -1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': 'false',\n        'num_threads': 6,\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0\n}","fc33389e":"trn_data = lgb.Dataset(X_tr_lgbm, label=y_tr_lgbm) # Augmentation\nval_data = lgb.Dataset(X_valid, label=y_valid)\n\nstart = time.time()\n# Training\nclf = lgb.train(lgb_params, trn_data, 100000, valid_sets = [val_data], verbose_eval=-1, early_stopping_rounds = 3000)\nend = time.time()\n\nlgb_gpu = end - start\nprint('>> GPU: It took %.2f seconds' %(lgb_gpu))\n\nval_pred = clf.predict(X_valid[features], num_iteration=clf.best_iteration)\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['lgb_gpu'] = lgb_gpu","8bc3730c":" cat_params = {\n        'max_depth' : 7,\n        'learning_rate' : 0.04,\n        'colsample_bylevel' : 1.0,\n        'objective' : \"Logloss\",\n        'eval_metric' : 'AUC',\n        'task_type' : \"GPU\",\n        'random_seed': 42,\n        'iterations': 100000,\n        'use_best_model': True\n }","4fa01cc6":"start = time.time()\nclf = ctb.CatBoostClassifier(**cat_params)\nclf.fit(X=X_tr_cat, y=y_tr_cat, eval_set=[(X_valid, y_valid)], verbose=5000, early_stopping_rounds = 3000)\nend = time.time()\n\ncat_gpu = end - start\nprint('>> GPU: It took %.2f seconds' %(cat_gpu))\n\nval_pred = clf.predict_proba(X_valid)[:,1]\nprint(\">> CV score: {:<8.5f}\".format(roc_auc_score(y_valid, val_pred)))\n\nperformance['cat_gpu'] = cat_gpu","4f86b304":"def print_result(first_time, first_name, second_time, second_name, title):\n    results = (first_time\/60)-(second_time\/60) # converting to minutes\n    print('>> {} result:'.format(title))\n    if results < 0:\n        print(str(first_name)+' spent '+str(int(abs(results)))+' minutes less than '+str(second_name))\n        print('It was %.2f times faster!'% (second_time\/first_time))\n    elif results > 0:\n        print(str(second_name)+' spent '+str(int(abs(results)))+' minutes less than '+str(first_name))\n        print('It was %.2f times faster!'% (first_time\/second_time))\n    else:\n        print('CPU and GPU spent the exact same time: %.2f minutes'% first_time)","e1e0c7c1":"print_result(lgb_cpu,\"LGBM CPU\",lgb_gpu,\"LGBM GPU\", 'LGBM CPU vs GPU')\nprint_result(cat_gpu,\"Cat GPU\",lgb_cpu,\"LGBM CPU\",'CatBoost GPU vs LGBM CPU')\nprint_result(cat_gpu,\"Cat GPU\",lgb_gpu,\"LGBM GPU\",'CatBoost GPU vs LGBM GPU')","9d85093f":"# Displaying results - Horizontal Barplot\n\nperformance_df = pd.DataFrame.from_dict(performance, orient='index')\nperformance_df.columns = ['time']\nperformance_df.reset_index(inplace=True)\nperformance_df['time'] = round(performance_df['time']\/60, 2)\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(12, 5))\n\n# Plot the total crashes\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"time\", y='index', data=performance_df, color=\"b\", tick_label=True)\nf.suptitle('Model Training Time', fontsize=16)\nax.set_xlabel('Minutes', fontsize=14)\nax.set_ylabel('Model', fontsize=14)\nplt.show()","26a54bf2":"# Results\n---","ddda414d":"**Great, we are ready to go!**","4bffbef9":"### MODEL: CatBoost with GPU","fcf7b98e":"Sorting only *True Test data*... Yes, you already know the [kernel](https:\/\/www.kaggle.com\/yag320\/list-of-fake-samples-and-public-private-lb-split) ;)\n\nOne of the true winners: [YaG320](https:\/\/www.kaggle.com\/yag320)!","f683f40b":"### Additional comments\n\n[](http:\/\/)\nWe can see the advantage of LGBM trained using GPU on a large enough dataset.\n\nAnyway, we can observe the real GPU power looking at CatBoost (last model trained more than 1 mln rows much faster than others with less rows!!). Its performances are incredibly fast and even more accurate!\n\n[Here](https:\/\/github.com\/FedericoRaimondi\/me\/blob\/master\/Santander_Customer_Transaction_Prediction\/PredictiveAnalysis_ModelTuning\/PredictiveAnalysis_ModelTuning_GPU.ipynb) you can find the 200 models (2 feat each, LGBM) comparision and see how things change in that situation!\n\nA SPECIAL THANKS to Chris Deotte for your tips and your contribution!\n\n### Hope you liked this kernel... if you did, please upvote!","a25bca27":"### 2nd MODEL: LGBM with GPU","5e1731b7":"### 1st MODEL: LGBM with CPU","f63dee7c":"### Frequency Encoding\n---","13ae406b":"# CatBoost\n---","776de87c":"# LBGM Comparison\n---","1205fda2":"---\n### _Let's start! :)_\n\nFirst, in the notebook settings set **Internet connected**, and **GPU on**.\n\n#### Enabling LGBM GPU\nThen execute the 4 following cells. You can find the explanation in this [kernel](https:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\/)![](http:\/\/)","eb78dd2d":"### Augmentation\n---\n\nThis data augmentation function is described [here](https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment) by [Jiwei Liu](https:\/\/www.kaggle.com\/jiweiliu).\n\nI only modified the fact that the FE features are considered togheter with their own original features.\n\nFor instance: `var_0` and `var_0_count` are augmented togheter.","2bd0e4c8":"# CPU vs GPU with GBM - 400 features + augmentation\n---\n\nThe aim of this notebook is to compare two GBM models trained using CPU and with the [Kaggle's free GPU](https:\/\/www.kaggle.com\/dansbecker\/running-kaggle-kernels-with-a-gpu). \n\nI have already compared the [200 models (2 features each)](https:\/\/github.com\/FedericoRaimondi\/me\/blob\/master\/Santander_Customer_Transaction_Prediction\/PredictiveAnalysis_ModelTuning\/PredictiveAnalysis_ModelTuning_GPU.ipynb) version and CPU performed better for LGBM because of the not large enough training datasets.\n\nAfter [this discussion](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/89004#521415) with [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte), I decided to compare also a 400 features + augmentation model to see if GPU wins against CPU.\n\n**EDIT**: Chris correctly suggested to try also other kind of GBM (not only LGBM as I did in the previous version of the kernel) because, for instance with CatBoost the power of GPU is way more significative!"}}