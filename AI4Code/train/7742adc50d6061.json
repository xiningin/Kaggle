{"cell_type":{"757320f6":"code","10be4a13":"code","047e1eea":"code","24b0e214":"code","cd237f21":"code","f4f23ff6":"code","c792909b":"code","61320468":"code","96e80d8c":"code","698fe695":"code","4fbfc69c":"code","c46075d2":"code","277be06a":"code","40c43882":"code","aef121bb":"code","f3610d02":"code","04419b93":"code","4e69d914":"code","d8bd69f4":"code","c63c0862":"code","444d5a7f":"code","474d33fb":"code","c6b0ece6":"code","85df9dc3":"code","373efd5e":"code","6068b929":"code","01cbf728":"code","a5a76b5c":"code","b25c884c":"code","8e8b7a59":"code","88804eaa":"code","4ad64e5a":"code","5baf40b5":"code","da599fc2":"code","ec7ef326":"code","546989fc":"code","45340824":"code","0d9f9e3d":"code","aaa42990":"code","b8b41a4f":"code","e9de1379":"code","5ecd026a":"code","be0a1d61":"code","ac58fdc1":"code","18915a14":"code","acb39e0e":"code","afb82bc0":"code","9193efbb":"code","3b5e0060":"markdown","f9d843ca":"markdown","3412bb1e":"markdown","6130e332":"markdown","ac62cb32":"markdown","f6a8d6d7":"markdown","628f2de6":"markdown","3f62e7d6":"markdown","a3ee847e":"markdown","0c361e99":"markdown","5dfe68d7":"markdown","0163c8bd":"markdown","5b4efd43":"markdown","25d01077":"markdown","6667699e":"markdown","840868d5":"markdown","446039ed":"markdown","5f1dfd5d":"markdown","3a757e8f":"markdown","1e523c8b":"markdown","a923b683":"markdown","9942b68e":"markdown","5fc29b27":"markdown","25ecf071":"markdown","bd79481f":"markdown","8d3c6e2a":"markdown","adc99051":"markdown","7444ee1d":"markdown","7058dbbd":"markdown"},"source":{"757320f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff # import figure factory\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# close warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","10be4a13":"df = pd.read_csv(\"..\/input\/column_2C_weka.csv\")","047e1eea":"# to see features and target variable\ndf.head()","24b0e214":"# Display the content of data\ndf.info()","cd237f21":"# shape gives number of rows and columns in a tuple\ndf.shape","f4f23ff6":"df.describe()","c792909b":"# Display positive and negative correlation between columns\ndf.corr()","61320468":"#sorts all correlations with ascending sort.\ndf.corr().unstack().sort_values().drop_duplicates()","96e80d8c":"#correlation map\nf, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True, linewidth=\".5\", cmap=\"RdPu\", fmt=\".2f\", ax = ax)\nplt.title(\"Correlation Map\",fontsize=20)\nplt.show()","698fe695":"sns.pairplot(data=df,hue=\"class\",palette=\"Set1\")\nplt.suptitle(\"Pair Plot of Data\",fontsize=20)\nplt.show()   # pairplot without standard deviaton fields of data","4fbfc69c":"color_list = [\"red\" if each==\"Abnormal\" else \"cyan\" for each in df.loc[:,\"class\"]]\npd.plotting.scatter_matrix(df.loc[:, df.columns != \"class\"],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal=\"hist\",\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = \"*\",\n                                       edgecolor= \"black\")\nplt.show()","c46075d2":"df_abnormal = df[df[\"class\"]==\"Abnormal\"]\npd.plotting.scatter_matrix(df_abnormal.loc[:, df_abnormal.columns != \"class\"],\n                                       c=\"red\",\n                                       figsize= [15,15],\n                                       diagonal=\"hist\",\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = \"*\",\n                                       edgecolor= \"black\")\nplt.show()","277be06a":"df_normal = df[df['class']=='Normal']\npd.plotting.scatter_matrix(df_normal.loc[:, df_normal.columns != \"class\"],\n                                       c=\"cyan\",\n                                       figsize= [15,15],\n                                       diagonal=\"hist\",\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = \"*\",\n                                       edgecolor= \"black\")\nplt.show()","40c43882":"# prepare data\ndata1 = len(df[\"class\"][df[\"class\"] == \"Abnormal\"])\ndata2 = len(df[\"class\"][df[\"class\"] == \"Normal\"])\n\ndata = [go.Bar(\n            x=[\"Abnormal\",\"Normal\"],\n            y=[data1,data2],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","aef121bb":"df[\"class\"] = [0 if each == \"Abnormal\" else 1 for each in df[\"class\"]]\n\ny = df[\"class\"].values\nx_data = df.drop([\"class\"], axis=1)","f3610d02":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","04419b93":"x.head()","4e69d914":"x.isnull().sum() #Indicates values not defined in our data","d8bd69f4":"x.isnull().sum().sum()  #Indicates sum of values in our data","c63c0862":"print(x.shape)\nprint(y.shape)","444d5a7f":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3,random_state=1)\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","474d33fb":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)\n\n#Print Train Accuracy\nlr_train_accuracy = lr_model.score(x_train,y_train)\nprint(\"lr_train_accuracy = \",lr_model.score(x_train,y_train))\n#Print Test Accuracy\nlr_test_accuracy = lr_model.score(x_test,y_test)\nprint(\"lr_test_accuracy = \",lr_model.score(x_test,y_test))","c6b0ece6":"data = [go.Bar(\n            x=[\"lr_train_accuracy\",\"lr_test_accuracy\"],\n            y=[lr_train_accuracy,lr_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","85df9dc3":"y_pred = lr_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_lr, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = ('y_pred')\nplt.ylabel = ('y_true')\nplt.show()","373efd5e":"tp ,fp ,fn ,tn= cm_lr.ravel()\nprint(\"lr_RECALL = \",tp\/(tp+fn))\nprint(\"lr_PRECISION = \",(tp\/(tp+fp)))","6068b929":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors=3)\nknn_model.fit(x_train,y_train)\n\n#Print Train Accuracy\nknn_train_accuracy = knn_model.score(x_train,y_train)\nprint(\"knn_train_accuracy = \",knn_model.score(x_train,y_train))\n#Print Test Accuracy\nknn_test_accuracy = knn_model.score(x_test,y_test)\nprint(\"knn_test_accuracy = \",knn_model.score(x_test,y_test))","01cbf728":"# Model complexity\nneighboors = np.arange(1,30)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neighboors):\n    # k from 1 to 30(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # fit with knn\n    knn.fit(x_train, y_train)\n    train_accuracy.append(knn.score(x_train, y_train))           # train accuracy\n    test_accuracy.append(knn.score(x_test, y_test))              # test accuracy\n\n# import graph objects as \"go\"\nimport plotly.graph_objs as go\n\n# Creating trace1\ntrace1 = go.Scatter(\n                    x = neighboors,\n                    y = train_accuracy,\n                    mode = \"lines\",\n                    name = \"train_accuracy\",\n                    marker = dict(color = 'rgba(160, 112, 2, 0.8)'),\n                    text= \"train_accuracy\")\n# Creating trace2\ntrace2 = go.Scatter(\n                    x = neighboors,\n                    y = test_accuracy,\n                    mode = \"lines+markers\",\n                    name = \"test_accuracy\",\n                    marker = dict(color = 'rgba(80, 26, 80, 0.8)'),\n                    text= \"test_accuracy\")\ndata = [trace1, trace2]\nlayout = dict(title = 'K Value vs Accuracy',\n              xaxis= dict(title= 'Number of Neighboors',ticklen= 10,zeroline= True)\n             )\nfig = dict(data = data, layout = layout)\niplot(fig)\n\nknn_train_accuracy = np.max(train_accuracy)\nknn_test_accuracy = np.max(test_accuracy)\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy), 1+test_accuracy.index(np.max(test_accuracy))))","a5a76b5c":"data = [go.Bar(\n            x=[\"knn_train_accuracy\",\"knn_test_accuracy\"],\n            y=[knn_train_accuracy,knn_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","b25c884c":"y_pred = knn_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_knn, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","8e8b7a59":"tp ,fp ,fn ,tn= cm_knn.ravel()\nprint(\"knn_RECALL = \",tp\/(tp+fn))\nprint(\"knn_PRECISION = \",(tp\/(tp+fp)))","88804eaa":"from sklearn.svm import SVC\n\nsvm_model = SVC(random_state=1)\nsvm_model.fit(x_train,y_train)\n\n#Print Train Accuracy\nsvm_train_accuracy = svm_model.score(x_train,y_train)\nprint(\"svm_train_accuracy = \",svm_model.score(x_train,y_train))\n#Print Test Accuracy\nsvm_test_accuracy = svm_model.score(x_test,y_test)\nprint(\"svmr_test_accuracy = \",svm_model.score(x_test,y_test))","4ad64e5a":"data = [go.Bar(\n            x=[\"svm_train_accuracy\",\"svm_test_accuracy\"],\n            y=[svm_train_accuracy,svm_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","5baf40b5":"y_pred = svm_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_svm = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_svm, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","da599fc2":"tp ,fp ,fn ,tn= cm_svm.ravel()\nprint(\"svm_RECALL = \",tp\/(tp+fn))\nprint(\"svm_PRECISION = \",(tp\/(tp+fp)))","ec7ef326":"from sklearn.naive_bayes import GaussianNB\n\nnb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\n\n#Print Train Accuracy\nnb_train_accuracy = nb_model.score(x_train,y_train)\nprint(\"nb_train_accuracy = \",nb_model.score(x_train,y_train))\n#Print Test Accuracy\nnb_test_accuracy = nb_model.score(x_test,y_test)\nprint(\"nb_test_accuracy = \",nb_model.score(x_test,y_test))","546989fc":"data = [go.Bar(\n            x=[\"nb_train_accuracy\",\"nb_test_accuracy\"],\n            y=[nb_train_accuracy,nb_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","45340824":"y_pred = nb_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_nb, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","0d9f9e3d":"tp ,fp ,fn ,tn= cm_nb.ravel()\nprint(\"nb_RECALL = \",tp\/(tp+fn))\nprint(\"nb_PRECISION = \",(tp\/(tp+fp)))","aaa42990":"from sklearn.tree import DecisionTreeClassifier\n#if you remove random_state=1, you can see how accuracy is changing\n#Accuracy changing depends on splits\ndt_model = DecisionTreeClassifier(random_state=1)\ndt_model.fit(x_train,y_train)\n\n#Print Train Accuracy\ndt_train_accuracy = dt_model.score(x_train,y_train)\nprint(\"dt_train_accuracy = \",dt_model.score(x_train,y_train))\n#Print Test Accuracy\ndt_test_accuracy = dt_model.score(x_test,y_test)\nprint(\"dt_test_accuracy = \",dt_model.score(x_test,y_test))","b8b41a4f":"data = [go.Bar(\n            x=[\"dt_train_accuracy\",\"dt_test_accuracy\"],\n            y=[dt_train_accuracy,dt_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","e9de1379":"y_pred = dt_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_dt = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_dt, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","5ecd026a":"tp ,fp ,fn ,tn= cm_dt.ravel()\nprint(\"dt_RECALL = \",tp\/(tp+fn))\nprint(\"dt_PRECISION = \",(tp\/(tp+fp)))","be0a1d61":"from sklearn.ensemble import RandomForestClassifier\n\n#n_estimators = 100 => Indicates how many trees we have\nrf_model = RandomForestClassifier(n_estimators=100, random_state=1)\nrf_model.fit(x_train,y_train)\n\n#Print Train Accuracy\nrf_train_accuracy = rf_model.score(x_train,y_train)\nprint(\"rf_train_accuracy = \",rf_model.score(x_train,y_train))\n#Print Test Accuracy\nrf_test_accuracy = rf_model.score(x_test,y_test)\nprint(\"rf_test_accuracy = \",rf_model.score(x_test,y_test))","ac58fdc1":"data = [go.Bar(\n            x=[\"rf_train_accuracy\",\"rf_test_accuracy\"],\n            y=[rf_train_accuracy,rf_test_accuracy],\n            marker=dict(color='rgb(158,202,225)',\n            line=dict(color='rgba(254, 69, 62, 1)',\n            width=1.5),\n        ),\n    opacity=0.6\n    )]\n\niplot(data, filename='text-hover-bar')","18915a14":"y_pred = rf_model.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_rf = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(8,8))\nsns.heatmap(cm_rf, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='RdPu', ax = ax)\nplt.xlabel = (\"y_pred\")\nplt.ylabel = (\"y_true\")\nplt.show()","acb39e0e":"tp ,fp ,fn ,tn= cm_rf.ravel()\nprint(\"rf_RECALL = \",tp\/(tp+fn))\nprint(\"rf_PRECISION = \",(tp\/(tp+fp)))","afb82bc0":"plt.figure(figsize=(20,10))\nplt.suptitle(\"Confusion Matrixes of Classification Models\",fontsize=30)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Classification\")\nsns.heatmap(cm_lr,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Classification\")\nsns.heatmap(cm_knn,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,3)\nplt.title(\"K Nearest Neighbors(KNN) Classification\")\nsns.heatmap(cm_svm,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Classification\")\nsns.heatmap(cm_nb,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,5)\nplt.title(\"Random Forest Classification\")\nsns.heatmap(cm_dt,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.subplot(2,3,6)\nplt.title(\"Support Vector Machine(SVM) Classification\")\nsns.heatmap(cm_rf,annot=True,cmap='YlGnBu',fmt=\".0f\",cbar=False)\n\nplt.show()","9193efbb":"# create trace1 \ntrace1 = go.Bar(\n         x = np.array(\"Logistic Regression\"),\n         y = np.array(lr_test_accuracy),\n         name = \"Logistic Regression\",\n         marker = dict(color ='rgba(255, 77, 77, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n# create trace2 \ntrace2 = go.Bar(\n         x = np.array(\"KNN\"),\n         y = np.array(knn_test_accuracy),\n         name = \"KNN\",\n         marker = dict(color ='rgba(9, 220, 125, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n# create trace3 \ntrace3 = go.Bar(\n         x = np.array(\"SVM\"),\n         y = np.array(svm_test_accuracy),\n         name = \"SVM\",\n         marker = dict(color ='rgba(36, 44, 188, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n# create trace4 \ntrace4 = go.Bar(\n         x = np.array(\"Naive Bayes\"),\n         y = np.array(nb_test_accuracy),\n         name = \"Naive Bayes\",\n         marker = dict(color ='rgba(209, 0, 224, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n# create trace5 \ntrace5 = go.Bar(\n         x = np.array(\"Decision Tree\"),\n         y = np.array(dt_test_accuracy),\n         name = \"Decision Tree\",\n         marker = dict(color ='rgba(0, 224, 209, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n# create trace6 \ntrace6 = go.Bar(\n         x = np.array(\"Random Forest\"),\n         y = np.array(rf_test_accuracy),\n         name = \"Random Forest\",\n         marker = dict(color ='rgba(255, 255, 61, 1)',\n         line=dict(color='rgb(0,0,0)',width=1.5))\n                    )\n\ndata = [trace1,trace2,trace3,trace4,trace5,trace6]\nlayout = go.Layout(barmode = \"group\",title=\"Machine Learning Classification Models Comparison\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","3b5e0060":"<a id=\"17\"><\/a> <br>\n# Evaluation Classification Models","f9d843ca":"<a id=\"5\"><\/a> <br>\n### Scatter Matrix","3412bb1e":"<a id=\"16\"><\/a> <br>\n### Random Forest Classification","6130e332":"<a id=\"18\"><\/a> <br>\n### Confusion Matrixes Comparison","ac62cb32":"<a id=\"13\"><\/a> <br>\n### Support Vector Machine(SVM) Classification","f6a8d6d7":"**pd.plotting.scatter_matrix:**\n* green: *normal* and red: *abnormal*\n* c:  color\n* figsize: figure size\n* diagonal: histohram of each features\n* alpha: opacity\n* s: size of marker\n* marker: marker type ","628f2de6":"<a id=\"3\"><\/a> <br>\n#### Correlation Map","3f62e7d6":"<br> Now the question is why we choose K = 3 or what value we need to choose K. The answer is in model complexity\n\n<br> **Model complexity:**\n* K has general name. It is called a hyperparameter. For now just know K is hyperparameter and we need to choose it that gives best performace. \n* Literature says if k is small, model is complex model can lead to overfit. It means that model memorizes the train sets and cannot predict test set with good accuracy.\n* If k is big, model that is less complex model can lead to underfit. \n* At below, I range K value from 1 to 30(exclude) and find accuracy for each K value. As you can see in plot, when K is 1 it memorize train sets and cannot give good accuracy on test set (overfit). Also if K is 20 or 22, model is lead to underfit. Again accuracy is not enough. However look at when K is 20 or 22(best performance), accuracy has highest value almost 82%. ","a3ee847e":"<a id=\"14\"><\/a> <br>\n### Naive Bayes Classification","0c361e99":"<a id=\"20\"><\/a> <br>\n# Conclusion\n**If you like it, Please upvote my kernel.**<br>\n**If you have any question, I will happy to hear it**","5dfe68d7":"<a id=\"10\"><\/a> <br>\n### Logistic Regression Classification","0163c8bd":"<a id=\"7\"><\/a> <br>\n### Normalization Data","5b4efd43":"**As you can see,**\nthe best machine learning algorithm for our data is Random Forest Classification algorithm with 86%.","25d01077":"<a id=\"2\"><\/a> <br>\n# EDA Visualization","6667699e":"<a id=\"23\"><\/a> <br>\n### Scatter Matrix-Abnormal Class","840868d5":"<a id=\"21\"><\/a> <br>\n### Pair Plot","446039ed":"<a id=\"11\"><\/a> <br>\n## K-Nearest Neighbors (KNN) Classification","5f1dfd5d":"<a id=\"15\"><\/a> <br>\n### Decision Tree Classification","3a757e8f":"<a id=\"9\"><\/a> <br>\n# Machine Learning Classification Models","1e523c8b":"<a id=\"1\"><\/a> <br>\n# Exploratory Data Analysis (EDA)","a923b683":"<a id=\"6\"><\/a> <br>\n# Data PreProcessing ","9942b68e":"* Normalization Formula = (x - min(x))\/(max(x)-min(x))","5fc29b27":"<a id=\"24\"><\/a> <br>\n### Scatter Matrix-Normal Class","25ecf071":"### Import Data","bd79481f":"<a id=\"19\"><\/a> <br>\n### Bar Charts Comparison","8d3c6e2a":"# INTRODUCTION\n\n**In this kernel,**\nWe will apply the machine learning algorithms on our data.Then we will find which machine learning algorithm is the best for our data at the end of tutorial.\n\n\n**Content:**\n1. [Exploratory Data Analysis (EDA)](#1)\n2. [EDA Visualization](#2)\n   2. [Correlation Map](#3)\n   2. [Pair Plot](#4)\n   2. [Scatter Matrix](#5)\n   2. [Scatter Matrix-Abnormal Class](#23)\n   2. [Scatter Matrix-Normal Class](#24)\n3. [Data PreProcessing](#6)\n   3. [Normalization Data](#7)\n   3. [Train-Test Split Data](#8)\n4. [Machine Learning Classification Models](#9)\n      4. [Logistic Regression Classification](#10)\n      4. [K-Nearest Neighbors (KNN) Classification](#11)\n         4. [Model Complexity](#12)\n      4. [Support Vector Machine(SVM) Classification](#13)\n      4. [Naive Bayes Classification](#14)\n      4. [Decision Tree Classification](#15)\n      4. [Random Forest Classification](#16)\n5. [Evaluation Classification Models](#17)\n   5. [Confusion Matrixes Comparison](#18)\n   5. [Bar Charts Comparison](#19)\n6. [Conclusion](#20)   ","adc99051":"<a id=\"12\"><\/a> <br>\n### Model Complexity","7444ee1d":"As you can see:\n\n* length: 310 (range index)\n* Features are float\n* Target variables are object that is like string","7058dbbd":"<a id=\"8\"><\/a> <br>\n### Train-Test Split Data"}}