{"cell_type":{"eb06a32c":"code","2fb886c2":"code","00a755e8":"code","7d2fb85b":"code","4186cb1c":"code","f2c4b48d":"code","2683ee54":"code","de69a988":"code","32002a48":"code","eca9c534":"markdown","1fb3473d":"markdown","4796ebac":"markdown","f90e697c":"markdown"},"source":{"eb06a32c":"!pip install tensorflow_decision_forests\n!pip install wurlitzer\nimport tensorflow_decision_forests as tfdf\nfrom wurlitzer import sys_pipes\nimport os\nimport matplotlib.pyplot as plt\nimport math ","2fb886c2":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.models import Model\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\n\nfrom keras.models import Sequential\nimport keras \nfrom keras.layers import Dense, Dropout,Concatenate, Activation,Input\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import model_from_json\nfrom keras import backend as K","00a755e8":"\ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","7d2fb85b":"loss=tf.keras.losses.SparseCategoricalCrossentropy()\nmetrics=tf.keras.metrics.SparseCategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0.0000001, patience=2, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=0, verbose=0,\n    mode='min', min_delta=0.0000001, cooldown=0, min_lr=10e-7)\n","4186cb1c":"# data for final prediction :\ntest1_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_df.iloc[:,1:])\n\n# features for preprocessing :\nfeatures = [col for col in train_df.iloc[:,1:].columns]\ntarget = pd.get_dummies(train_df.iloc[:,-1])","f2c4b48d":"\nN_FOLDS = 3\nSEED = 2021\n\noof_NN = np.zeros((train_df.shape[0],4))\npred_NN = np.zeros((test_df.shape[0],4))\noof_GBT_NN = np.zeros((train_df.shape[0],4))\npred_GBT_NN = np.zeros((test_df.shape[0],4))\noof_GBT = np.zeros((train_df.shape[0],4))\npred_GBT = np.zeros((test_df.shape[0],4))\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train_df.iloc[:,1:],train_df.iloc[:,-1])):\n    \n    print(f\"\\n======== FOLD {fold} TRAINING  =========\\n\")\n       \n    x_tr = train_df.iloc[:,1:].iloc[tr_idx]\n    y_tr = train_df.iloc[:,-1].iloc[tr_idx]\n    x_ts = train_df.iloc[:,1:].iloc[ts_idx]\n    y_ts = train_df.iloc[:,-1].iloc[ts_idx]\n\n    train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(x_tr, label='target')\n    test_ds = tfdf.keras.pd_dataframe_to_tf_dataset(x_ts, label='target')\n    \n    #---------- NN model  preprocessing ----------\n    Normalization = tf.keras.layers.experimental.preprocessing.Normalization\n    nn_raw_inputs = []\n    nn_processed_inputs = []\n    for col in features[:-1] :\n\n        inp = tf.keras.Input(shape=(1,),name = col)\n        nn_raw_inputs.append(inp)\n        \n        values = train_df[col].values\n        input_normalizer = Normalization()\n        input_normalizer.adapt(values)\n        normalized_input = input_normalizer(inp)\n        nn_processed_inputs.append(normalized_input)\n    \n    #---------- NN model definition ----------------\n    \n    y = tf.keras.layers.Concatenate()(nn_processed_inputs)\n    y = tf.keras.layers.Dense(128, activation='relu')(y)\n    y = tf.keras.layers.Dropout(0.3)(y)\n    GBT_layer = tf.keras.layers.Dense(128, activation='relu')(y)\n    y = tf.keras.layers.Dropout(0.3)(GBT_layer)\n    y = tf.keras.layers.Dense(16, activation='sigmoid')(y)\n    output = tf.keras.layers.Dense(4,activation = 'softmax')(y)\n    \n    nn_model = tf.keras.models.Model(nn_raw_inputs, output)\n   \n    \n    #------- Model preparation for GradientBoostedTrees ----\n    \n    nn_model_without_head = tf.keras.models.Model(inputs=nn_model.inputs,\n                                            outputs=GBT_layer)\n    \n    #******** Gradient Boosted Trees with NN *******************  \n    # with \"preprocessing=nn_model_without_head\" you link the 2 models.\n    \n    model_Gradient_with_NN = tfdf.keras.GradientBoostedTreesModel(\n    preprocessing=nn_model_without_head) # Prepocessing with NN model \n    \n    #---------- Gradient Boosted Trees without NN-----------\n    \n    model_Gradient_without_NN = tfdf.keras.GradientBoostedTreesModel()\n    \n    #---------- NN Training --------------------------------\n    print(\"\\nTraining of NN model starting\\n\")\n    nn_model.compile(tf.keras.optimizers.Adam(learning_rate=0.001),\n                                            loss=loss ,\n                                            metrics=metrics)\n    nn_model.fit(train_ds,\n            validation_data=test_ds,\n            epochs=20,\n            verbose=0,\n            callbacks=[es,plateau])\n    \n    #-------- Gradient Boosted Trees with NN Training ------\n    print(\"\\nTraining of GBT with NN model starting\\n\")\n    model_Gradient_with_NN.compile(metrics=metrics)\n    model_Gradient_with_NN.fit(train_ds)\n    \n    # Training display\n    print()\n    logs = model_Gradient_with_NN.make_inspector().training_logs()\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Accuracy (out-of-bag)\")\n    plt.title(\"GBT with NN training\")\n    plt.subplot(1, 2, 2)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Logloss (out-of-bag)\")\n    plt.title(\"GBT with NN training\")\n    plt.show()\n    \n    #------- Gradient Boosted Trees without NN Training -----\n    print(\"\\nTraining of GBT without NN model starting\\n\")\n    model_Gradient_without_NN.compile(metrics=metrics)\n    model_Gradient_without_NN.fit(train_ds)\n    \n    # Training display\n    print()\n    logs = model_Gradient_without_NN.make_inspector().training_logs()\n    plt.figure(figsize=(12, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Accuracy (out-of-bag)\")\n    plt.title(\"GBT alone training\")\n    plt.subplot(1, 2, 2)\n    plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n    plt.xlabel(\"Number of trees\")\n    plt.ylabel(\"Logloss (out-of-bag)\")\n    plt.title(\"GBT alone training\")\n    plt.show()\n    \n    #------------------- NN Prediction -------------------\n    \n    oof_NN[ts_idx] = nn_model.predict(test_ds)\n    pred_NN += nn_model.predict(test1_ds) \/ N_FOLDS\n    score_NN = log_loss(y_ts, oof_NN[ts_idx])\n    print(f\"Score of NN alone {score_NN}\\n\")\n    \n    #---Gradient Boosted Trees with NN Prediction --------\n    \n    oof_GBT_NN[ts_idx] = model_Gradient_with_NN.predict(test_ds)\n    pred_GBT_NN += model_Gradient_with_NN.predict(test1_ds) \/ N_FOLDS\n    score_GBT_NN = log_loss(y_ts, oof_GBT_NN[ts_idx])\n    print(f\"Score of GBT with NN {score_GBT_NN}\\n\")\n    \n    #---Gradient Boosted Trees without NN Prediction ------\n    \n    oof_GBT[ts_idx] = model_Gradient_without_NN.predict(test_ds)\n    pred_GBT+= model_Gradient_without_NN.predict(test1_ds) \/ N_FOLDS\n    score_GBT = log_loss(y_ts, oof_GBT[ts_idx])\n    print(f\"Score of GBT alone {score_GBT}\\n\")\n    \nscore_NN = log_loss(train_df.iloc[:,-1], oof_NN)\nscore_GBT_NN = log_loss(train_df.iloc[:,-1], oof_GBT_NN)\nscore_GBT = log_loss(train_df.iloc[:,-1], oof_GBT)\nprint(\"\\n========== FINAL SCORES =================\\n\")\nprint(f\"Score total of NN alone {score_NN}\\n\")\nprint(f\"Score total of GBT with NN  {score_GBT_NN}\\n\")\nprint(f\"Score total of GBT alone {score_GBT}\\n\")","2683ee54":"\"\"\"\n**To get all the parameters description :\n\n?tfdf.keras.GradientBoostedTreesModel\n\n**To get verbose = 1 for GBT :\n\nwith sys_pipes():\n    model_GBT.fit(x=train_ds)\n\n\n\n**If you don't use Kfold or train_test_split\n\ndef split_dataset(dataset, test_ratio=0.30):\n    test_indices = np.random.rand(len(dataset)) < test_ratio\n    return dataset[~test_indices], dataset[test_indices]\n\ntrain_ds_pd, test_ds_pd = split_dataset(train_df.iloc[:,1:])\nprint(\"{} examples in training, {} examples for testing.\".format(\n    len(train_ds_pd), len(test_ds_pd)))\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label='target')\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label='target')\n\n\n\"\"\"\n","de69a988":"#submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","32002a48":"\"\"\"\nsubmission_df = pd.DataFrame(pred_NN)\nsubmission_df.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\nsubmission_df['id'] = submission['id']\nsubmission_df = submission_df[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\nsubmission_df.to_csv(\"submission_Keras_31.csv\", index=False)\n\"\"\"","eca9c534":"<h3>It leads quickly to overfitting with the standard parameters and don't give better result for NN...but maybe with a fine tuning it can...I let you play with it :-)","1fb3473d":"<h2> Training parameters","4796ebac":"\nThe goal of the notebook is to study how we can merge NN Model and TF Gradient Boosted Trees.\nUnfortunately, TF Gradient Boosted trees (TF GBT) can not be directly trained in parallel with functional API.\nTF labs told me that TF GBT was trained in only 1 epoch and could only be trained with NN in 2 consecutive steps...\nSo the only way to proceed is to get one of the layers of a NN Model already trained and includes it into \nthe argument (input) of TF GBT as \"preprocessing features\". Then we can train the TF GBT with the NN layers already trained. \nYou will find in the notebook a training in 3 kfolds for the NN and TF GBT alone and the (NN model + TF GBT) results.\n\n![image.png](attachment:3c40096c-a5dc-45a6-9840-327793b610d1.png)","f90e697c":"<h1> Training with 3 Folds of all models"}}