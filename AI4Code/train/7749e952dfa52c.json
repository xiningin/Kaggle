{"cell_type":{"d668066c":"code","bbb3c39c":"code","2d97ce76":"code","a574b809":"code","64907de5":"code","5e237317":"code","116839c3":"code","2bba45b5":"code","eee9f5a4":"code","aa3e8be0":"code","75bb7405":"code","b5d71480":"code","3f28a8aa":"code","10b91af6":"code","de229ecf":"code","2d4d89c7":"code","62d42206":"code","ec03fcaa":"code","65dfa4ff":"code","1928ba0d":"code","cd0a3a95":"code","e1f5f785":"code","cddda313":"code","767fee6c":"code","276b361a":"code","212a771c":"code","22ca6b38":"code","1ded9cd5":"code","95a7ded9":"code","d21a3166":"code","176b74a6":"code","1df62df8":"code","08771c16":"code","04c0ff73":"code","e6ea7c8c":"code","9252b25d":"code","742e5cb0":"code","f98afb7f":"code","34a7e2b3":"code","c86e54d4":"code","08206b20":"code","5e1b26c4":"code","2ce365d8":"code","e7de3048":"code","81afd007":"code","ae9b6bca":"code","c24b6fb2":"code","97f088c6":"code","33c7e604":"code","7ef9fab9":"code","b60c6da8":"code","c21e745b":"code","7c52005b":"code","42a97b92":"code","464703f0":"code","99ab4505":"code","cdec524d":"code","c6bc01a8":"code","9cb64943":"code","d67646e4":"code","e769b169":"code","939f7d67":"code","3f97b5d1":"code","ed11d366":"code","8c8ae25c":"code","7f63cefa":"code","327c8bed":"markdown","d48ecaa7":"markdown"},"source":{"d668066c":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 10,10\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV ,RepeatedStratifiedKFold, cross_val_score , StratifiedKFold ,GroupKFold,TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score,accuracy_score ,confusion_matrix\nfrom sklearn.preprocessing import PolynomialFeatures\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import ClusterCentroids , NearMiss","bbb3c39c":"train_data = pd.read_csv('..\/input\/customersegmentation\/CustomerSegmentation\/Train.csv')\ntest_data = pd.read_csv('..\/input\/customersegmentation\/CustomerSegmentation\/Test.csv')","2d97ce76":"print(train_data.shape)\ntrain_data.head()","a574b809":"print(test_data.shape)\ntest_data.head()","64907de5":"def nullColumns(train_data):\n    list_of_nullcolumns =[]\n    for column in train_data.columns:\n        total= train_data[column].isna().sum()\n        try:\n            if total !=0:\n                print('Total Na values is {0} for column {1}' .format(total, column))\n                list_of_nullcolumns.append(column)\n        except:\n            print(column,\"-----\",total)\n    print('\\n')\n    return list_of_nullcolumns\n\n\ndef percentMissingFeature(data):\n    data_na = (data.isnull().sum() \/ len(data)) * 100\n    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n    return data_na\n\n\ndef plotMissingFeature(data_na):\n    f, ax = plt.subplots(figsize=(15, 12))\n    plt.xticks(rotation='90')\n    if(data_na.empty ==False):\n        sns.barplot(x=data_na.index, y=data_na)\n        plt.xlabel('Features', fontsize=15)\n        plt.ylabel('Percent of missing values', fontsize=15)\n        plt.title('Percent missing data by feature', fontsize=15)","5e237317":"print('train data')\nprint(nullColumns(train_data))\nprint(percentMissingFeature(train_data))\nprint('\\n')\nprint('test_data')\nprint(nullColumns(test_data))\nprint(percentMissingFeature(test_data))","116839c3":"segments = train_data.loc[:,\"Segmentation\"].value_counts()\nplt.xlabel(\"Segment\")\nplt.ylabel('Count')\nsns.barplot(segments.index , segments.values).set_title('Segments')","2bba45b5":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 2, figsize=(10, 10))\n#sns.despine(left=True)\n\nsegments_male = train_data.loc[train_data.Gender == 'Male',\"Segmentation\"].value_counts()\nsns.barplot(segments_male.index , segments_male.values,  color=\"b\", ax=axes[0]).set_title('Male')\n\nsegments_female = train_data.loc[train_data.Gender == 'Female',\"Segmentation\"].value_counts()\nsns.barplot(segments_female.index , segments_female,   color=\"g\", ax=axes[1]).set_title('Female')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","eee9f5a4":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2, 2, figsize=(10, 10))\n#sns.despine(left=True)\n\nsns.distplot(train_data[train_data.Segmentation =='A'][\"Work_Experience\"],  color=\"b\", ax=axes[0, 0]).set_title('Segment : A')\n\nsns.distplot(train_data[train_data.Segmentation =='B'][\"Work_Experience\"],   color=\"r\", ax=axes[0, 1]).set_title('Segment : B')\n\nsns.distplot(train_data[train_data.Segmentation =='C'][\"Work_Experience\"],  color=\"g\", ax=axes[1, 0]).set_title('Segment : C')\n\nsns.distplot(train_data[train_data.Segmentation =='D'][\"Work_Experience\"], color=\"m\", ax=axes[1, 1]).set_title('Segment : D')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","aa3e8be0":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2, 2, figsize=(10, 10))\n#sns.despine(left=True)\n\nsns.barplot(train_data[train_data.Segmentation =='A'][\"Spending_Score\"].value_counts().index,train_data[train_data.Segmentation =='A'][\"Spending_Score\"].value_counts(),  color=\"b\", ax=axes[0, 0]).set_title('Segment : A')\n\nsns.barplot(train_data[train_data.Segmentation =='B'][\"Spending_Score\"].value_counts().index,train_data[train_data.Segmentation =='B'][\"Spending_Score\"].value_counts(),   color=\"r\", ax=axes[0, 1]).set_title('Segment : B')\n\nsns.barplot(train_data[train_data.Segmentation =='C'][\"Spending_Score\"].value_counts().index,train_data[train_data.Segmentation =='C'][\"Spending_Score\"].value_counts(),  color=\"g\", ax=axes[1, 0]).set_title('Segment : C')\n\nsns.barplot(train_data[train_data.Segmentation =='D'][\"Spending_Score\"].value_counts().index,train_data[train_data.Segmentation =='D'][\"Spending_Score\"].value_counts(), color=\"m\", ax=axes[1, 1]).set_title('Segment : D')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","75bb7405":"train_data.groupby('Age')['Segmentation'].describe()","b5d71480":"train_data.groupby(['Segmentation'])[\"Age\"].describe()","3f28a8aa":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2, 2, figsize=(15, 15))\n#sns.despine(left=True)\n\nsns.distplot(train_data[train_data.Segmentation =='A'][\"Age\"],  color=\"b\", ax=axes[0, 0]).set_title('Segment : A')\n\nsns.distplot(train_data[train_data.Segmentation =='B'][\"Age\"],   color=\"r\", ax=axes[0, 1]).set_title('Segment : B')\n\nsns.distplot(train_data[train_data.Segmentation =='C'][\"Age\"],  color=\"g\", ax=axes[1, 0]).set_title('Segment : C')\n\nsns.distplot(train_data[train_data.Segmentation =='D'][\"Age\"], color=\"m\", ax=axes[1, 1]).set_title('Segment : D')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","10b91af6":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(2, 2, figsize=(20, 20))\n#sns.despine(left=True)\n\nsns.barplot(train_data[train_data.Segmentation =='A'][\"Profession\"].value_counts().index,train_data[train_data.Segmentation =='A'][\"Profession\"].value_counts(),  color=\"b\", ax=axes[0, 0]).set_title('Segment : A')\n\nsns.barplot(train_data[train_data.Segmentation =='B'][\"Profession\"].value_counts().index,train_data[train_data.Segmentation =='B'][\"Profession\"].value_counts(),   color=\"r\", ax=axes[0, 1]).set_title('Segment : B')\n\nsns.barplot(train_data[train_data.Segmentation =='C'][\"Profession\"].value_counts().index,train_data[train_data.Segmentation =='C'][\"Profession\"].value_counts(),  color=\"g\", ax=axes[1, 0]).set_title('Segment : C')\n\nsns.barplot(train_data[train_data.Segmentation =='D'][\"Profession\"].value_counts().index,train_data[train_data.Segmentation =='D'][\"Profession\"].value_counts(), color=\"m\", ax=axes[1, 1]).set_title('Segment : D')\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","de229ecf":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\nf, axes = plt.subplots(1, 2, figsize=(20, 20))\n#sns.despine(left=True)\n\nsns.boxplot(train_data['Segmentation'],train_data['Work_Experience'],  color=\"b\", ax=axes[0])\n\nsns.boxplot(train_data['Segmentation'],train_data['Family_Size'],  color=\"r\", ax=axes[1])\n\n\nplt.setp(axes, yticks=[])\nplt.tight_layout()","2d4d89c7":"train_data[\"Age\"].value_counts()","62d42206":"train_data['Var_1'].value_counts()","ec03fcaa":"train_data['Profession'].value_counts()","65dfa4ff":"train_data['Family_Size'].value_counts()","1928ba0d":"train_data['Graduated'].value_counts()","cd0a3a95":"train_data['Work_Experience'].value_counts()","e1f5f785":"train_data['Spending_Score'].value_counts()","cddda313":"train_ids = train_data[\"ID\"]\ntest_ids = test_data[\"ID\"]\n#set(train_ids).intersection(set(test_ids))","767fee6c":"print(len(train_data['ID'].unique()))\nprint(len(test_data['ID'].unique()))\nprint(\"Duplicate Ids: \",len(set(train_ids).intersection(set(test_ids))))","276b361a":"def create_submission_file(model_list,test):\n    df = test.copy()\n    preds = np.zeros(shape= (len(df),4))\n    submission = pd.read_csv('..\/input\/customersegmentation\/CustomerSegmentation\/sample_submission.csv')\n\n    submission = pd.merge(submission,train_data , on = 'ID', how= 'left')[['ID','Segmentation_y']].rename(columns = {'ID':'ID','Segmentation_y':'Segmentation'})\n    \n    for model in model_list:\n        preds = preds + (model.predict_proba(df.iloc[:,1:]))\n    \n    df[\"Segmentation\"] = np.argmax(preds,axis =1)\n    df[\"Segmentation\"] = df[\"Segmentation\"].map({0:'A',1:'B',2:'C',3:'D'})\n    \n    preds_dict = df[['ID','Segmentation']].set_index('ID').to_dict()\n    \n    for key, val in preds_dict['Segmentation'].items():\n        submission.loc[(submission.ID == key),'Segmentation'] = val\n    \n    print(submission)\n    \n    submission.to_csv('submission.csv', index = False, header = True)","212a771c":"sub = pd.read_csv('..\/input\/customersegmentation\/CustomerSegmentation\/sample_submission.csv')['ID']\nsub = pd.merge(sub,train_data , on = 'ID', how= 'left')\ntemp_train_data1 = sub[sub.Segmentation.isna() == 0]\ntemp_train_data1.head()","22ca6b38":"temp_test_data = test_data[test_data.ID.isin(sub[sub.Segmentation.isna() == 1]['ID'])]","1ded9cd5":"#temp_train_data.to_csv('temp.csv')","95a7ded9":"#temp_train_data = train_data[(train_data.Age  != 89)]","d21a3166":"combined_data = pd.concat([train_data,temp_test_data],axis = 0)","176b74a6":"#'Ever_Married', 'Graduated', 'Profession', 'Work_Experience', 'Family_Size', 'Var_1'\ncombined_data['Ever_Married'] = combined_data['Ever_Married'].fillna('unk')\ncombined_data['Graduated'] = combined_data['Graduated'].fillna('unk')\ncombined_data['Profession'] = combined_data['Profession'].fillna('unk')\ncombined_data['Work_Experience'] = combined_data[\"Work_Experience\"].fillna(-1)\ncombined_data['Family_Size'] = combined_data[\"Family_Size\"].fillna(-1)\ncombined_data['Var_1'] = combined_data['Var_1'].fillna('unk')","1df62df8":"#combined_data['Age_Category'] = 'all'\n#combined_data.loc[(combined_data.Age <= 25) & (combined_data.Age >= 18),['Age_Category']] = '18_25'\n#combined_data.loc[(combined_data.Age <= 35) & (combined_data.Age >= 26),['Age_Category']] = '26_35'\n#combined_data.loc[(combined_data.Age <= 45) & (combined_data.Age >= 36),['Age_Category']] = '36_45'\n#combined_data.loc[(combined_data.Age <= 55) & (combined_data.Age >= 46),['Age_Category']] = '46_55'\n#combined_data.loc[combined_data.Age >= 56,['Age_Category']]= '>55'\n#combined_data = combined_data.drop(columns = ['Age'])","08771c16":"combined_data = combined_data.sort_values(['ID'])\ncombined_data = combined_data.reset_index(drop =  True)\ncombined_data_cat = combined_data.copy()","04c0ff73":"le = LabelEncoder()\n#combined_data['f1'] = le.fit_transform(combined_data['Age'].astype(str) +\"_\" + combined_data['Profession'])","e6ea7c8c":"combined_data['Gender'] = combined_data['Gender'].map({'Male':1,'Female':2})\ncombined_data['Ever_Married'] = combined_data['Ever_Married'].map({'No':1,'Yes':2,'unk':3})\ncombined_data['Graduated'] = combined_data['Graduated'].map({'No':1,'Yes':2,'unk':3})\ncombined_data['Profession'] = combined_data['Profession'].map({'Artist':1,'Healthcare':2,'Entertainment':3,'Engineer':4,'Doctor':5,'Lawyer':6,'Executive':7,'Marketing':8,'Homemaker':9,'unk':10})\ncombined_data['Spending_Score'] = combined_data['Spending_Score'].map({'Low':1,'Average':2,'High':3})\ncombined_data[\"Var_1\"] = combined_data['Var_1'].map({'Cat_1':1,'Cat_2':2,'Cat_3':3,'Cat_4':4,'Cat_5':5,'Cat_6':6,'Cat_7':7,'unk':8})\ncombined_data['Segmentation'] = combined_data['Segmentation'].map({'A':0,'B':1,'C':2,'D':3})","9252b25d":"#combined_data[\"Group\"] = 0\n#combined_data.loc[0,\"Group\"] = 1\n#combined_data[\"Group_First\"] = 0\n#combined_data.loc[0,\"Group_First\"] = 1\n#combined_data['Group_Last'] = 0\n#combined_data[\"Group_Oldest\"] = 0\n#combined_data['Group_Youngest'] = 0","742e5cb0":"#for idx, row in combined_data.iterrows():\n#    if idx !=0:\n#        if combined_data.loc[idx-1 , 'Profession'] == combined_data.loc[idx , 'Profession']:\n#            combined_data.loc[idx , 'Group'] = combined_data.loc[idx-1 , 'Group']\n#            combined_data.loc[idx ,\"Group_First\"] = 0\n#        else:\n#            combined_data.loc[idx , 'Group'] = combined_data.loc[idx-1 , 'Group'] + 1\n#            combined_data.loc[idx ,\"Group_First\"] = 1\n#            combined_data.loc[idx-1 ,\"Group_Last\"] = 1\n    \n    ","f98afb7f":"#combined_data[\"Group_Oldest\"] = combined_data.groupby('Group')['Age'].transform('max')\n#combined_data['Group_Youngest'] = combined_data.groupby('Group')['Age'].transform('min')\n#combined_data['Group_Age_Mean'] = combined_data.groupby('Group')['Age'].transform('mean')\n#combined_data['Group_Age_STD'] = combined_data.groupby('Group')['Age'].transform('std').fillna(-1)\n#combined_data.head(50)","34a7e2b3":"combined_data['magic1'] = combined_data['ID']%7\ncombined_data['magic2'] = combined_data['ID']%30\ncombined_data['magic3'] = combined_data['ID']%365\ncombined_data['magic4'] = combined_data['ID']\/\/7\ncombined_data['magic5'] = combined_data['ID']\/\/365\ncombined_data['magic6'] = combined_data['ID']\/\/90\ncombined_data['magic7'] = combined_data['ID']%90\ncombined_data['magic8'] = (combined_data['ID'].values - 458982)\ncombined_data['magic9'] = (combined_data['ID'].values - 458982)\/\/7\ncombined_data['magic10'] = (combined_data['ID'].values - 458982)\/\/30\ncombined_data['magic11'] = np.sin((combined_data['ID']-combined_data['ID'].mean())*2*3.14\/combined_data['ID'].std())\ncombined_data['magic12'] = np.cos((combined_data['ID']-combined_data['ID'].mean())*2*3.14\/combined_data['ID'].std())\n#combined_data['magic13'] = \n#combined_data['magic14'] = combined_data.groupby(['Profession','Spending_Score'])['ID'].transform('std')\n#combined_data['magic14'] = combined_data.groupby('Family_Size')['ID'].apply(lambda x : (x - np.sin((x - x.mean())\/x.std())))","c86e54d4":"#combined_data['magic1'] = combined_data['Age']%7\n#combined_data['magic2'] = combined_data['Age']%30\n#combined_data['magic3'] = combined_data['Age']%365","08206b20":"#combined_data['magic13'] = combined_data['ID']\/\/combined_data['Age']\n#combined_data['magic14'] = combined_data['ID']%combined_data['Age']\n#combined_data['magic6'] = combined_data['Age']\/\/90\n#combined_data['magic7'] = combined_data['Age']%90","5e1b26c4":"#combined_data['Work_Ex_Per_Spending_Score'] = combined_data['Work_Experience']\/combined_data['Spending_Score']\n#combined_data['Age_Per_Spending_Score'] = combined_data['Age']\/combined_data['Spending_Score']\n#combined_data['Family_Size_per_Score'] = combined_data['Family_Size']\/combined_data['Spending_Score']\n#combined_data['Avg_Family_Size_Spending'] = combined_data.groupby('Family_Size')['Spending_Score'].transform('mean')\n#combined_data['diff'] = combined_data['Age'] - combined_data['Work_Experience']","2ce365d8":"#combined_data['Age_Category'] = 0\n#combined_data.loc[(combined_data.Age <= 25) & (combined_data.Age >= 18),['Age_Category']] = 1\n#combined_data.loc[(combined_data.Age <= 35) & (combined_data.Age >= 26),['Age_Category']] = 2\n#combined_data.loc[(combined_data.Age <= 45) & (combined_data.Age >= 36),['Age_Category']] = 3\n#combined_data.loc[(combined_data.Age <= 55) & (combined_data.Age >= 46),['Age_Category']] = 4\n#combined_data.loc[combined_data.Age >= 56,['Age_Category']]= 5\n#combined_data = combined_data.drop(columns = ['Age'])","e7de3048":"#combined_data['Family_Type'] = 0\n#combined_data.loc[(combined_data.Family_Size <= 2) ,['Family_Type']] = 1\n#combined_data.loc[(combined_data.Family_Size <= 5) & (combined_data.Family_Size >= 3),['Family_Type']] = 2\n#combined_data.loc[(combined_data.Family_Size >= 6),['Family_Type']] = 3\n#combined_data = combined_data.drop(columns = ['Family_Size'])","81afd007":"#combined_data['Work_Ex_Type'] = 0\n#combined_data.loc[(combined_data.Work_Experience == 1)  ,['Work_Ex_Type']] = 1\n#combined_data.loc[(combined_data.Work_Experience >= 2) & (combined_data.Work_Experience <= 5),['Work_Ex_Type']] = 2\n#combined_data.loc[(combined_data.Work_Experience >= 6) & (combined_data.Work_Experience <= 10),['Work_Ex_Type']]  = 3\n#combined_data.loc[(combined_data.Work_Experience >= 11) & (combined_data.Work_Experience <= 15),['Work_Ex_Type']] =4\n#combined_data = combined_data.drop(columns = ['Work_Experience'])","ae9b6bca":"#combined_data['Age_lag'] = combined_data['Age'].rolling(window = 5).mean().fillna(-1)\n#combined_data['Age_lag1'] = combined_data.sort_values(['ID']).groupby(['Age'])['ID'].apply(lambda x : x.rolling(window = 5,min_periods =1).mean()).fillna(-1)\n#combined_data['Age_lag1'] = combined_data.sort_values(['ID']).groupby(['Work_Experience'])['ID'].apply(lambda x : x.rolling(window = 5,min_periods =1).mean()).fillna(-1)\n#combined_data['Age_lag2'] = combined_data.sort_values(['ID']).groupby(['Age'])['Segmentation'].apply(lambda x : x.shift(2)).fillna(-1)\n#combined_data['Age_lag3'] = combined_data.sort_values(['ID']).groupby(['Age'])['Segmentation'].apply(lambda x : x.shift(3)).fillna(-1)\n#combined_data['Age_lag4'] = combined_data.sort_values(['ID']).groupby(['Age'])['Segmentation'].apply(lambda x : x.shift(4)).fillna(-1)\n#combined_data['Age_lag5'] = combined_data.sort_values(['ID']).groupby(['Age'])['Segmentation'].apply(lambda x : x.shift(5)).fillna(-1)\n#combined_data['work_roll5'] = combined_data.sort_values(['ID']).groupby(['Work_Experience'])['Segmentation'].apply(lambda x : x.rolling(window = 5,min_periods =1).mean()).fillna(-1)","c24b6fb2":"combined_data.head()","97f088c6":"features = [col for col in combined_data.columns if col !='ID' and col != 'Segmentation' and col != 'Group'  ]","33c7e604":"train_features = combined_data.loc[(combined_data.Segmentation.isna() == 0 ),features]\ntarget = combined_data.loc[(combined_data.Segmentation.isna() == 0 ),'Segmentation'].astype(int)\n\nX_test = combined_data.loc[(combined_data.ID.isin(set(test_ids) - set(train_ids))),['ID'] + features]","7ef9fab9":"train_features.columns","b60c6da8":"train_features.head()","c21e745b":"rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = []\nX_train_cv,y_train_cv = train_features.copy(), target.copy()\nfor i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n    print('Fold',i)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clf = lgb.LGBMClassifier(\n            n_estimators=500,\n            max_depth=12,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            objective = 'multiclass'\n        )        \n    \n    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                verbose=100,eval_metric='multi_logloss',\n                early_stopping_rounds=50)\n    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n    scores.append(acc)\n    print ('LGB Val CV=',acc)\n    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n    print('#'*100)\n\n\nprint('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","7c52005b":"rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\nscores = []\nX_train_cv,y_train_cv = train_features.copy(), target.copy()\nfor i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n    print('Fold',i)\n    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n    clf = xgb.XGBClassifier(\n            n_estimators=500,\n            max_depth=6,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            objective = 'multi:softprob'\n        )        \n    \n    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                verbose=100,eval_metric='merror',\n                early_stopping_rounds=50)\n    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n    scores.append(acc)\n    print ('XGB Val CV=',acc)\n    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n    print('#'*100)\n\n\nprint('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","42a97b92":"#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n#scores = []\n#X_train_cv,y_train_cv = train_features.copy(), target.copy()\n#for i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n#    print('Fold',i)\n#    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#    clf = CatBoostClassifier(\n#    iterations=500,\n#    learning_rate=0.1,\n#    random_strength=0.1,\n#    depth=8,\n#    loss_function='MultiClass',\n#    eval_metric='Accuracy',\n#    leaf_estimation_method='Newton'\n#)    \n    \n#    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT],\n#                eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n#               early_stopping_rounds=50,verbose = 0)\n#    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n#    scores.append(acc)\n#    print ('CatBoost Val CV=',acc)\n#    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n#    print('#'*100)\n\n\n#print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","464703f0":"#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n#scores = []\n#X_train_cv,y_train_cv = train_features.copy(), target.copy()\n#for i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n#    print('Fold',i)\n#    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#    clf = RandomForestClassifier(n_estimators=500 ,\n#                             max_depth=6, min_samples_split=2, \n#                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n#                             n_jobs=-1, random_state=123, verbose=0) \n#    \n##    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT])\n##    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n#    scores.append(acc)\n #   print ('RFC Val CV=',acc)\n#    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n#    print('#'*100)\n\n\n#print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","99ab4505":"def feature_importance(model, X_train):\n\n    print(model.feature_importances_)\n    names = X_train.columns.values\n    ticks = [i for i in range(len(names))]\n    plt.bar(ticks, model.feature_importances_)\n    plt.xticks(ticks, names,rotation =90)\n    plt.show()","cdec524d":"#X_train, X_val, y_train, y_val = train_test_split(train_features,target , test_size=0.2, random_state=1)\ntrees = 15\nseeds = [32,432,45,76,93,67]","c6bc01a8":"#model_cat = [0] *trees*len(seeds)\n#for i in range(trees*len(seeds)):\n\n#        print(\"Tree {0:d}\".format(i+1))\n#        j = 0\n#        X_train, X_val, y_train, y_val = train_test_split(train_features,target , test_size=0.2)\n#        if i%30 == 0:\n#            random_state = seeds[j]\n#            j +=1\n            \n#        model_cat[i] = CatBoostClassifier(\n#                            iterations=500,\n#                            learning_rate=0.1,\n#                            random_strength=0.1,\n#                            depth=6,\n###                            loss_function='MultiClass',\n##                            eval_metric='Accuracy',\n#                            leaf_estimation_method='Newton',\n#                            random_seed = random_state\n#                        )    \n#    \n#        model_cat[i].fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT],\n###                        eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n##                       early_stopping_rounds=50,verbose = 0)\n#        \n#        \n#        print(\"Multi Log Loss {0:.5f}\".format(model_cat[i].get_best_score()['validation']['MultiClass']))","9cb64943":"model_lgb = [0] *trees*len(seeds)\navg_loss = []\nfor i in range(trees*len(seeds)):\n\n        print(\"Tree {0:d}\".format(i+1))\n        j = 0\n        X_train, X_val, y_train, y_val = train_test_split(train_features,target , test_size=0.2)\n        if i%30 == 0:\n            random_state = seeds[j]\n            j +=1\n        model_lgb[i] = lgb.LGBMClassifier(boosting_type='gbdt',\n                               n_estimators= 500,\n                               max_depth=8,\n                               learning_rate=0.1,\n                               subsample=0.8,\n                               colsample_bytree=0.4,\n                               objective = 'multiclass',\n                               random_state = random_state\n                              )\n        \n        model_lgb[i].fit(X_train, y_train,\n              eval_set=[(X_train, y_train),(X_val, y_val)],\n              eval_metric=['multi_logloss'],\n              early_stopping_rounds = 100,\n              verbose=0)\n        avg_loss.append(model_lgb[i].best_score_['valid_1']['multi_logloss'])\n        print(\"Multi Log Loss {0:.5f}\".format(model_lgb[i].best_score_['valid_1']['multi_logloss']))\n        \nprint('#'*100)\nprint(\"Multi Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))","d67646e4":"#print(model_lgb.best_score_['valid_1'])\nfeature_importance(model_lgb[0],train_features)","e769b169":"avg_loss = []\nmodel_xgb = [0] *trees *len(seeds)\nfor i in range(trees *len(seeds)):\n        print(\"Tree {0:d}\".format(i+1))\n        X_train, X_val, y_train, y_val = train_test_split(train_features,target , test_size=0.2)\n        j = 0\n        if i%30 == 0:\n            random_state = seeds[j]\n            j +=1\n            \n        model_xgb[i] = xgb.XGBClassifier(\n            n_estimators=500,\n            max_depth=6,\n            learning_rate=0.1,\n            subsample=0.8,\n            colsample_bytree=0.4,\n            objective = 'multi:softprob',\n            random_state = random_state\n        )    \n        \n        model_xgb[i].fit(X_train, y_train,\n              eval_set=[(X_train, y_train),(X_val, y_val)],\n              eval_metric=['merror'],\n              early_stopping_rounds = 50,\n              verbose=0)\n        #print(model_xgb[j].best_score)\n        avg_loss.append(model_xgb[i].best_score)\n        print(\"Multi Log Loss {0:.5f}\".format(model_xgb[i].best_score))\n        \nprint('#'*100)\nprint(\"Multi Log Loss Stats {0:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))","939f7d67":"create_submission_file(model_lgb,X_test)","3f97b5d1":"#model_rfc = [0] *len(seeds)\n#for i,seed in enumerate(seeds):\n#        print(\"Tree {0:d}\".format(i+1))\n        \n#        model_rfc[i] = RandomForestClassifier(n_estimators=500 ,\n#                             max_depth=6, min_samples_split=2, \n#                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n#                             n_jobs=-1, random_state=seed, verbose=0) \n       \n        \n#        model_rfc[i].fit(train_features.copy(), target.copy())\n#        #print(model_xgb[j].best_score)\n","ed11d366":"#create_submission_file(model_lgb+model_xgb+model_rfc,X_test)","8c8ae25c":"#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n#scores = []\n#X_train_cv,y_train_cv = train_features.copy(), target.copy()\n#for i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n#    print('Fold',i)\n#    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#    clf = LogisticRegression(max_iter = 500)\n    \n#    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT])\n#    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n#    scores.append(acc)\n#    print ('LogisticRegression Val CV=',acc)\n#    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n#    print('#'*100)\n\n\n#print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","7f63cefa":"#rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n#scores = []\n#X_train_cv,y_train_cv = train_features.copy(), target.copy()\n#for i, (idxT, idxV) in enumerate(rskf.split(X_train_cv, y_train_cv)):\n#    print('Fold',i)\n#    print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n#    clf = SVC(max_iter = -1,probability=True)\n#    \n#    h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT])\n#    acc = accuracy_score(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1))\n#    scores.append(acc)\n#    print ('SVC Val CV=',acc)\n#    print(confusion_matrix(y_train_cv.iloc[idxV],np.argmax(clf.predict_proba(X_train_cv.iloc[idxV]),axis =1)))\n#    print('#'*100)\n\n\n#print('%.3f (%.3f)' % (np.array(scores).mean(), np.array(scores).std()))","327c8bed":"# Logistic Regression CV\nCV accuracy--->poor","d48ecaa7":"# SVC CV\nCV accuracy--->poor"}}