{"cell_type":{"ef6a2139":"code","e4e04214":"code","19c12506":"code","579cfd98":"code","b8cbe406":"code","d3b5ef7f":"code","596e1832":"code","02eba058":"code","5e70eef1":"code","1d220102":"code","b238a116":"code","fd361999":"code","5d2a6ea8":"code","63ca3315":"code","ca068775":"code","eef186c5":"code","358c6c0c":"code","376c628c":"code","60b63c03":"code","6be98ea1":"code","a3cc727e":"code","704d5b63":"code","e49d6b34":"markdown","c82918a6":"markdown","d654b385":"markdown","75ea5ce9":"markdown","d310b4a6":"markdown","498cbdec":"markdown","d0e1b34b":"markdown","061344d7":"markdown","57006783":"markdown"},"source":{"ef6a2139":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom matplotlib import pyplot as plt\nimport cv2\nfrom PIL import Image\nimport random\nfrom imgaug import augmenters as iaa\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model,Model\nfrom keras.layers import Activation,Dropout,Flatten,Dense,Input,BatchNormalization,Conv2D\nfrom keras.applications.inception_resnet_v2 import preprocess_input\nfrom keras.applications import InceptionResNetV2\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import LambdaCallback\nfrom keras.callbacks import Callback\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport tensorflow as tf\nimport keras\n","e4e04214":"df = pd.read_csv(\"..\/input\/train.csv\")\nprint(\"Total number of unique ids:\",df.Id.count())\nprint(\"Total number of images:\", df.Id.count()*4)","19c12506":"df.head(2)","579cfd98":"labels = {\n    0:  \"Nucleoplasm\",  \n    1:  \"Nuclear membrane\",   \n    2:  \"Nucleoli\",   \n    3:  \"Nucleoli fibrillar center\",   \n    4:  \"Nuclear speckles\",\n    5:  \"Nuclear bodies\",   \n    6:  \"Endoplasmic reticulum\",   \n    7:  \"Golgi apparatus\",   \n    8:  \"Peroxisomes\",   \n    9:  \"Endosomes\",   \n    10:  \"Lysosomes\",   \n    11:  \"Intermediate filaments\",   \n    12:  \"Actin filaments\",   \n    13:  \"Focal adhesion sites\",   \n    14:  \"Microtubules\",   \n    15:  \"Microtubule ends\",   \n    16:  \"Cytokinetic bridge\",   \n    17:  \"Mitotic spindle\",   \n    18:  \"Microtubule organizing center\",   \n    19:  \"Centrosome\",   \n    20:  \"Lipid droplets\",   \n    21:  \"Plasma membrane\",   \n    22:  \"Cell junctions\",   \n    23:  \"Mitochondria\",   \n    24:  \"Aggresome\",   \n    25:  \"Cytosol\",   \n    26:  \"Cytoplasmic bodies\",   \n    27:  \"Rods & rings\"\n}","b8cbe406":"for key in labels.keys():\n    df[labels[key]] = 0\n    \ndef filltargets(row):\n    tar = row.Target.split(\" \")\n    for i in tar:\n        col = labels[int(i)]\n        row[str(col)]=1\n    return row\n","d3b5ef7f":"df = df.apply(filltargets, axis=1)\ndf = df.drop([\"Target\"],axis = 1)","596e1832":"df.head(2)","02eba058":"freq_df = df.drop([\"Id\"],axis=1).sum(axis=0).sort_values(ascending = True)\nplt.figure(figsize=(15,15))\nsns.barplot(y=freq_df.index.values, x=freq_df.values*100\/len(df), order=freq_df.index,palette=\"Blues\")\n","5e70eef1":"def correlated_distribution(label):\n    df_ = df[df[label]==1]\n    freq_df = df_.drop([\"Id\",label],axis=1).sum(axis=0).sort_values(ascending = True)\n    plt.figure(figsize=(5,5))\n    plt.xlabel(\"Distribution for \"+label)\n    sns.barplot(y=freq_df.index.values, x=freq_df.values*100\/len(df_), order=freq_df.index,palette=\"Blues\")\n\ncorrelated_distribution(\"Endosomes\")","1d220102":"correlated_distribution(\"Rods & rings\")","b238a116":"correlated_distribution(\"Peroxisomes\")","fd361999":"correlated_distribution(\"Cytosol\")","5d2a6ea8":"correlated_distribution(\"Mitochondria\")","63ca3315":"freq_df = pd.DataFrame()\nfreq_df[\"number_of_targets\"] = df.drop([\"Id\"],axis=1).sum(axis=1).sort_values(ascending = True)\ncount_perc = np.round(100 * freq_df[\"number_of_targets\"].value_counts() \/ freq_df.shape[0], 2)\nplt.figure(figsize=(20,5))\nsns.barplot(x=count_perc.index.values, y=count_perc.values, palette=\"Blues\")\nplt.xlabel(\"Number of targets per image\")\nplt.ylabel(\"% of data\")","ca068775":"shape = (299,299)\n\ndef load_image(id,path=\"..\/input\/train\/\"):\n    global shape\n    R = np.array(Image.open(path+id+'_red.png'))\n    G = np.array(Image.open(path+id+'_green.png'))\n    B = np.array(Image.open(path+id+'_blue.png'))\n    Y = np.array(Image.open(path+id+'_yellow.png'))\n\n    image = np.stack((\n        R\/2+Y\/2,\n        G\/2+Y\/2, \n        B),-1)\n\n    image = cv2.resize(image, (shape[0], shape[1]))\n    image = np.divide(image, 255)\n    return image  \n\nplt.figure(figsize=(15,15))\nfor n in range(4):\n    idx = np.random.randint(0,20000,1)\n    im_name = df.Id[idx[0]]\n    im = load_image(im_name)\n    plt.subplot(1,4,n+1)\n    plt.imshow(im)\nplt.show()\n","eef186c5":"path_to_train = '\/kaggle\/input\/train\/'\ndata = pd.read_csv('\/kaggle\/input\/train.csv')\n\ntrain_dataset_info = []\nfor name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n    train_dataset_info.append({\n        'path':os.path.join(path_to_train, name),\n        'labels':np.array([int(label) for label in labels])})\ntrain_dataset_info = np.array(train_dataset_info)\n\nfrom sklearn.model_selection import train_test_split\ntrain_ids, test_ids, train_targets, test_target = train_test_split(\n    data['Id'], data['Target'], test_size=0.2, random_state=42)","358c6c0c":"class data_generator:\n    \n    def create_train(dataset_info, batch_size, shape, augument=True):\n        assert shape[2] == 3\n        while True:\n            random_indexes = np.random.choice(len(dataset_info), batch_size)\n            batch_images = np.empty((batch_size, shape[0], shape[1], shape[2]))\n            batch_labels = np.zeros((batch_size, 28))\n            for i, idx in enumerate(random_indexes):\n                image = data_generator.load_image(\n                    dataset_info[idx]['path'], shape)   \n                if augument:\n                    image = data_generator.augment(image)\n                batch_images[i] = image\n                batch_labels[i][dataset_info[idx]['labels']] = 1\n            yield batch_images, batch_labels\n    \n    def load_image(path, shape):\n        R = np.array(Image.open(path+'_red.png'))\n        G = np.array(Image.open(path+'_green.png'))\n        B = np.array(Image.open(path+'_blue.png'))\n        Y = np.array(Image.open(path+'_yellow.png'))\n\n        image = np.stack((\n            R\/2+Y\/2,\n            G\/2+Y\/2, \n            B),-1)\n\n        image = cv2.resize(image, (shape[0], shape[1]))\n        image = np.divide(image, 255)\n        return image        \n    \n    def augment(image):\n        augment_img = iaa.Sequential([\n            iaa.OneOf([\n                iaa.Affine(rotate=0),\n                iaa.Affine(rotate=90),\n                iaa.Affine(rotate=180),\n                iaa.Affine(rotate=270),\n                iaa.Fliplr(0.5),\n                iaa.Flipud(0.5),\n            ])], random_order=True)\n        \n        image_aug = augment_img.augment_image(image)\n        return image_aug","376c628c":"train_datagen = data_generator.create_train(\n    train_dataset_info, 5, (299,299,3), augument=True)\n\nimages, labels = next(train_datagen)\n\nfig, ax = plt.subplots(1,5,figsize=(25,5))\nfor i in range(5):\n    ax[i].imshow(images[i])\nprint('min: {0}, max: {1}'.format(images.min(), images.max()))","60b63c03":"def create_model(input_shape, n_out):\n    model = Sequential()\n    model.add(InceptionResNetV2(include_top=False,input_shape= input_shape, pooling='avg', weights=\"imagenet\"))\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(n_out, activation='softmax'))\n    return model","6be98ea1":"def f1(y_true, y_pred):\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\ndef show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('f1')\n    ax[1].plot(history.epoch, history.history[\"f1\"], label=\"Train f1\")\n    ax[1].plot(history.epoch, history.history[\"val_f1\"], label=\"Validation f1\")\n    ax[2].set_title('acc')\n    ax[2].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[2].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()","a3cc727e":"from keras.optimizers import SGD\nmodel = create_model(\n    input_shape=(299,299,3), \n    n_out=28)\n\ncheckpointer = ModelCheckpoint(\n    '\/kaggle\/working\/InceptionResNetV2.model',\n    verbose=2, save_best_only=True)\n\nBATCH_SIZE = 10\nINPUT_SHAPE = (299,299,3)\n\ntrain_generator = data_generator.create_train(\n    train_dataset_info[train_ids.index], BATCH_SIZE, INPUT_SHAPE, augument=False)\nvalidation_generator = data_generator.create_train(\n    train_dataset_info[test_ids.index], 256, INPUT_SHAPE, augument=False)\n\nmodel.layers[0].trainable = True\n\nmodel.compile(\n    loss='binary_crossentropy',  \n    optimizer=SGD(lr = 0.0001, momentum=0.9, nesterov=True),\n    metrics=['acc', f1])\n\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=500,\n    validation_data=next(validation_generator),\n    epochs=15, \n    verbose=1,\n    callbacks=[checkpointer])\nshow_history(history)","704d5b63":"from tqdm import tqdm\nsubmit = pd.read_csv('..\/input\/sample_submission.csv')\npredicted = []\nfor name in tqdm(submit['Id']):\n    path = os.path.join('..\/input\/test\/', name)\n    image = data_generator.load_image(path, INPUT_SHAPE)\n    score_predict = model.predict(image[np.newaxis])[0]\n    label_predict = np.arange(28)[score_predict>=0.2]\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    predicted.append(str_predict_label)\n    \nsubmit['Predicted'] = predicted\nsubmit.to_csv('submission.csv', index=False)","e49d6b34":"## Frequency of labels in the data","c82918a6":"Let's train a classifier for baseline. I'm choosing InceptionResnet50 but another interesting candidate is NasNet.","d654b385":"Insights: \n\n* Nucleoplasm is the most common occurance (>40%) followed by Cytosol (>27%)\n* Plasma Membrane, Nucleoli, Mitochondria(Power-house of the cell) and other relatively bigger objects are present in a quantity that may not surprise any one who knows the basic cell structure.\n* The most interesting things in the distribution  of less frequent objects such as Rods and Rings, Microtubules Ends, Lysosomes etc as compared to those of Nucleoplasm. Hence a clear classs imbalance that can lead to harmful bias in model. This also gives us the idea about the kind of metric that we might endup using. From my very limited knowledge I think it should be F1.\n","75ea5ce9":"# Multilabel Analysis","d310b4a6":"## The grouping of labels\n\nLet's see the grouping of the least frequent variables first.","498cbdec":" Some coding in this kernel is inspired(read blatantly stolen) from [this really great work](https:\/\/www.kaggle.com\/allunia\/protein-atlas-exploration-and-baseline) Also, suggestions are a welcome so far they are not about my grammer and spelling(which I admit is borderline horrible and there's no spell check in this environment)\n\n\n# LET's EDA\n\nLet's see the total available images in the dataset","d0e1b34b":"Hmm... Interesting!\n\n\nThe key insight here is that the more frequent labels occure in more spread-out manner, i.e, they seems to be partnering up with way more variables while the least frequent variables are loyal to a small number of labels.\n\nWork in progress...","061344d7":"Let's see the grouping of most frequent labels","57006783":"Okay so majority is either single or double labeled. "}}