{"cell_type":{"8c7e6bb6":"code","3faeab64":"code","e2f74389":"code","74f29227":"code","d730f5d0":"code","6d7a9b46":"code","0f6949a0":"code","51df77c5":"code","1ee4af7f":"code","8ca0b827":"code","11e2e662":"code","cc13e782":"code","7ea12257":"code","668e752f":"code","c10a4a5b":"code","bba106bd":"code","2b146628":"code","cc3133f2":"code","7ec3d7b2":"code","d74553d6":"code","92ed58e1":"code","e16f6352":"code","587c57c4":"code","c3dbb84c":"code","7438f45b":"code","540571ca":"code","3463b347":"code","49eb16a5":"code","448074b5":"code","d10e14ba":"code","c73f7171":"code","e96b73c0":"code","d5368cac":"code","4818dafb":"code","c4a8395e":"code","1eb63731":"code","1b8e7603":"code","e03b5f68":"code","ba1dea8e":"code","c6887a96":"code","77799094":"code","19a4d103":"code","d01666a9":"code","5e19ef63":"code","b650e146":"code","23653692":"code","7edfae17":"code","8d0a1e7e":"code","b61ab93d":"code","5091d6e4":"code","ee3e06a3":"code","c497ee7d":"code","a04b9fe7":"code","cfe14f87":"code","17fd1577":"code","ae8aaa84":"code","91547c9b":"code","5b818359":"code","c8e6aed6":"markdown","f2a78e77":"markdown","a3bb1584":"markdown","12d23186":"markdown","ea2c6c3f":"markdown","dc847c3c":"markdown","a7b1982b":"markdown","f7a0ba39":"markdown","47ced079":"markdown","b8e276fb":"markdown","cee9dfe0":"markdown","a25647a8":"markdown","d3b8a5a3":"markdown","0b9fbbb3":"markdown","4d1c77d9":"markdown","bdc525f5":"markdown","a3392eeb":"markdown","6178160d":"markdown","ceaa9aba":"markdown","97684c65":"markdown","01910f51":"markdown","f059490d":"markdown","f673458d":"markdown","5831e997":"markdown","a4cc3d5e":"markdown","6ee127e6":"markdown","0f102772":"markdown","fa8ba1c0":"markdown","697492fb":"markdown","c7e654f3":"markdown","ed0e45a5":"markdown","d71d2ece":"markdown","23732218":"markdown","bfa1cd56":"markdown","d311ca52":"markdown","c7fa3ab5":"markdown","938bec09":"markdown","21d4f586":"markdown","6e8f084e":"markdown","2e0ac102":"markdown","d18deddd":"markdown","eb71c036":"markdown","a96a395d":"markdown","d0c4756c":"markdown","00f3ce7d":"markdown","2e2165a2":"markdown"},"source":{"8c7e6bb6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","3faeab64":"df_features = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip', sep=',')\ndf_stores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv', sep=',')\n\ndf_features_stores = df_features.merge(df_stores, how='inner', on='Store')\ndf_features_stores.head()","e2f74389":"df_train = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip', sep=',')\ntrain = df_train.merge(df_features_stores, how='inner', on=['Store','Date','IsHoliday'])\ntrain.head()","74f29227":"df_test = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip', sep=',')\ntest = df_test.merge(df_features_stores, how='inner', on=['Store','Date','IsHoliday'])\ntest.head()","d730f5d0":"print(\"Primeiro registro treino: \", train['Date'].min())\nprint(\"\u00daltimo registro treino:\", train['Date'].max())\n\nprint(\"Primeiro registro teste: \", test['Date'].min())\nprint(\"\u00daltimo registro teste:\", test['Date'].max())","6d7a9b46":"train['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\n\ntrain['Week'] = train['Date'].dt.isocalendar().week\ntest['Week'] = test['Date'].dt.isocalendar().week\n\ntrain['Year'] = train['Date'].dt.isocalendar().year\ntest['Year'] = test['Date'].dt.isocalendar().year","0f6949a0":"train.describe()","51df77c5":"test.describe()","1ee4af7f":"train.dtypes","8ca0b827":"train['Type'].unique()","11e2e662":"train['Date'] = pd.to_datetime(train['Date'])\ntrain['Type'] = train['Type'] .apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\ntrain['IsHoliday'] = train['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n\ncols = train.columns.drop(['Date'])\ntrain[cols] = train[cols].apply(pd.to_numeric, errors='coerce')","cc13e782":"test['Date'] = pd.to_datetime(test['Date'])\ntest['Type'] = test['Type'].apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\ntest['IsHoliday'] = test['IsHoliday'].apply(lambda x: 1 if x == True else 0)\n\ncols = test.columns.drop(['Date'])\ntest[cols] = test[cols].apply(pd.to_numeric, errors='coerce')","7ea12257":"holiday_train = train[['Date','Week','Year','IsHoliday']]\nholiday_train = holiday_train.loc[holiday_train['IsHoliday']==True].drop_duplicates()\n\nholiday_test = test[['Date','Week','Year','IsHoliday']]\nholiday_test = holiday_test.loc[holiday_test['IsHoliday']==True].drop_duplicates()\n\nholidays = pd.concat([holiday_train, holiday_test])\nholidays","668e752f":"def holiday_type(x):\n    if   (x['IsHoliday']== 1) & (x['Week']==6):\n       return 1 #SuperBowl\n    elif (x['IsHoliday']== 1) & (x['Week']==36):\n       return 2 #LaborDay\n    elif (x['IsHoliday']== 1) & (x['Week']==47):\n       return 3 #Thanksgiving\n    elif (x['IsHoliday']== 1) & (x['Week']==52):\n       return 4 #Christmas\n    else:\n       return 0","c10a4a5b":"train['IsHoliday'] = train.apply(holiday_type, axis=1)\ntrain['IsHoliday'].unique()","bba106bd":"test['IsHoliday'] = test.apply(holiday_type, axis=1)\ntest['IsHoliday'].unique()","2b146628":"train = train.replace('None', np.nan)\ntrain = train.replace('NaN', np.nan)\ntrain = train.replace('NaT', np.nan)\ntrain = train.replace('', np.nan)\ntrain_nulls = (train.isnull().sum(axis = 0)\/len(train))*100\ntrain_nulls","cc3133f2":"plt.figure(figsize=(15, 10))\n\nheatmap = sns.heatmap(train.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\nheatmap.set_title('Correlation Matrix - Train', fontdict={'fontsize':12}, pad=12);","7ec3d7b2":"test = test.replace('None', np.nan)\ntest = test.replace('NaN', np.nan)\ntest = test.replace('NaT', np.nan)\ntest = test.replace('', np.nan)\ntest_nulls = (test.isnull().sum(axis = 0)\/len(test))*100\ntest_nulls","d74553d6":"plt.figure(figsize=(15, 10))\n\nheatmap = sns.heatmap(test.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\nheatmap.set_title('Correlation Matrix - Test', fontdict={'fontsize':12}, pad=12);","92ed58e1":"train = train.fillna(0)\ntest = test.fillna(0)\n\ntrain.isnull().sum()","e16f6352":"weekly_sales = train.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2010 = train.loc[train['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2011 = train.loc[train['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})\nweekly_sales2012 = train.loc[train['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean', 'median']})","587c57c4":"weekly_sales.plot(figsize=(20,5))","c3dbb84c":"plt.figure(figsize=(20, 7))\n\nsns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values)\nsns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values)\nsns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values)\n\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'])\nplt.show()","7438f45b":"stores = train.groupby(['Store']).agg({'Weekly_Sales': ['mean']})\n\nplt.figure(figsize=(20, 7))\nplt.bar(stores.index,stores['Weekly_Sales']['mean'])\nplt.xticks(np.arange(1, 46, step=1))\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()","540571ca":"stores_sales2010 = train.loc[train['Year']==2010].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\nstores_sales2011 = train.loc[train['Year']==2011].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\nstores_sales2012 = train.loc[train['Year']==2012].groupby(['Store']).agg({'Weekly_Sales': ['mean', 'median']})\n\nplt.figure(figsize=(20, 7))\nsns.lineplot(stores_sales2010['Weekly_Sales']['mean'].index, stores_sales2010['Weekly_Sales']['mean'].values)\nsns.lineplot(stores_sales2011['Weekly_Sales']['mean'].index, stores_sales2011['Weekly_Sales']['mean'].values)\nsns.lineplot(stores_sales2012['Weekly_Sales']['mean'].index, stores_sales2012['Weekly_Sales']['mean'].values)\n\nplt.xticks(np.arange(1, 46, step=1))\nplt.legend(['2010', '2011', '2012'])\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()\n\n","3463b347":"departament = train.groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n\nplt.figure(figsize=(20, 7))\nplt.bar(departament.index,departament['Weekly_Sales']['mean'])\nplt.xticks(np.arange(1, 100, step=2))\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Departament', fontsize=16)\nplt.show()\n\n","49eb16a5":"departament_sales2010 = train.loc[train['Year']==2010].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\ndepartament_sales2011 = train.loc[train['Year']==2011].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\ndepartament_sales2012 = train.loc[train['Year']==2012].groupby(['Dept']).agg({'Weekly_Sales': ['mean', 'median']})\n\nplt.figure(figsize=(20, 7))\nsns.lineplot(departament_sales2010['Weekly_Sales']['mean'].index, departament_sales2010['Weekly_Sales']['mean'].values)\nsns.lineplot(departament_sales2011['Weekly_Sales']['mean'].index, departament_sales2011['Weekly_Sales']['mean'].values)\nsns.lineplot(departament_sales2012['Weekly_Sales']['mean'].index, departament_sales2012['Weekly_Sales']['mean'].values)\n\nplt.xticks(np.arange(1, 100, step=2))\nplt.legend(['2010', '2011', '2012'])\n\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Departament', fontsize=16)\nplt.show()\n","448074b5":"size = train.groupby(['Size']).agg({'Weekly_Sales': ['mean']})\n\nplt.figure(figsize=(20, 7))\nplt.plot(size)\n#plt.xticks(np.arange(1, 100, step=2))\n#plt.show()\n\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Size', fontsize=16)","d10e14ba":"size_sales2010 = train.loc[train['Year']==2010].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\nsize_sales2011 = train.loc[train['Year']==2011].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\nsize_sales2012 = train.loc[train['Year']==2012].groupby(['Size']).agg({'Weekly_Sales': ['mean', 'median']})\n\nplt.figure(figsize=(20, 7))\nsns.lineplot(size_sales2010['Weekly_Sales']['mean'].index, size_sales2010['Weekly_Sales']['mean'].values)\nsns.lineplot(size_sales2011['Weekly_Sales']['mean'].index, size_sales2011['Weekly_Sales']['mean'].values)\nsns.lineplot(size_sales2012['Weekly_Sales']['mean'].index, size_sales2012['Weekly_Sales']['mean'].values)\n\nplt.legend(['2010', '2011', '2012'])\nplt.ylabel('Week Sales', fontsize=16)\nplt.xlabel('Size', fontsize=16)\nplt.show()\n\n","c73f7171":"plt.figure(figsize=(10, 7))\nsns.boxplot(x='Type', y='Weekly_Sales', data=train,showfliers = False)","e96b73c0":"plt.figure(figsize=(15, 7))\nsns.boxplot(x='Type', y='Weekly_Sales', data=train,showfliers = True)","d5368cac":"sample_weight = train['IsHoliday'].apply(lambda x: 1 if x==0 else 5)\nsample_weight_frame = pd.DataFrame(sample_weight, index=train.index)","4818dafb":"from sklearn.metrics import make_scorer\n\ndef WMAE(y_test, y_pred):\n        y_pred_df = pd.DataFrame(y_pred,index=y_test.index)\n        \n        weights_5 = sample_weight_frame.loc[(y_test.index)].loc[sample_weight_frame.IsHoliday==5].index\n        weights_1 = sample_weight_frame.loc[(y_test.index)].loc[sample_weight_frame.IsHoliday==1].index\n        \n        sum_5 = np.sum(5*(abs(y_test.loc[weights_5].values-y_pred_df.loc[weights_5].values)))\n        sum_1 = np.sum(abs(y_test.loc[weights_1].values-y_pred_df.loc[weights_1].values))           \n        \n        return np.round((sum_5+sum_1)\/(5*len(weights_5)+len(weights_1)),2)\n \nmy_score = make_scorer(WMAE,greater_is_better=False)","c4a8395e":"train_all = train.drop(['Date'],axis=1)\ntrain_all","1eb63731":"y_train_all = train_all.loc[:, ['Weekly_Sales']]\nx_train_all = train_all.drop(['Weekly_Sales'], axis=1)","1b8e7603":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_train_all, y_train_all, test_size=0.2, random_state=0)\n\nprint(x_train.shape)\nprint(x_test.shape)","e03b5f68":"#RandomForest, ExtraTrees, XGB\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\n\nclf = RandomForestRegressor(random_state=0)\npca = PCA()\n\npipe = Pipeline(steps=[('clf', clf)])\n\nparam_grid = [ {\n                'clf':[RandomForestRegressor()],\n                'clf__n_estimators': [50,100,150],\n                'clf__max_depth': [10,20,30]\n                },\n               \n                {\n                'clf': [ExtraTreesRegressor()],\n                'clf__n_estimators': [50,100,150],\n                'clf__max_depth': [10,20,30]\n                },\n               \n                {\n                'clf': [XGBRegressor()],  \n                'clf__learning_rate':[0.1,0.05],\n                'clf__min_samples_split':[5,7,9],\n                'clf__max_depth':[10,20,30]\n                }\n              ]\n\nrscv_all_tree = RandomizedSearchCV(pipe, param_grid, cv = 3, scoring = my_score, n_jobs=-1)\nmodel_all_tree = rscv_all_tree.fit(x_train, y_train)","ba1dea8e":"rscv_all_tree.best_estimator_","c6887a96":"y_pred = rscv_all_tree.best_estimator_.predict(x_test)\nprint('WMAE:', WMAE(y_test, y_pred))","77799094":"plt.figure(figsize=(15, 10))\n\nheatmap = sns.heatmap(train.corr(), vmin=-1, vmax=1, annot=True,cmap=\"Blues\",annot_kws={\"fontsize\":10})\nheatmap.set_title('Matriz de Correla\u00e7\u00e3o', fontdict={'fontsize':12}, pad=12);","19a4d103":"train_relevant = train.drop(['Date','Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment'],axis=1)\ntrain_relevant","d01666a9":"y_relevant = train_relevant.loc[:, ['Weekly_Sales']]\nx_relevant = train_relevant.drop(['Weekly_Sales'], axis=1)","5e19ef63":"from sklearn.model_selection import train_test_split\n\nx_train_relevant, x_test_relevant, y_train_relevant, y_test_relevant = train_test_split(x_relevant, y_relevant, test_size=0.2, random_state=0)\n\nprint(x_train_relevant.shape)\nprint(x_test_relevant.shape)","b650e146":"#RandomForest, ExtraTrees, XGB\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np\n\nclf = RandomForestRegressor(random_state=0)\npca = PCA()\n\npipe = Pipeline(steps=[('clf', clf)])\n\nparam_grid = [ {\n                'clf':[RandomForestRegressor()],\n                'clf__n_estimators': [50,100,150],\n                'clf__max_depth': [10,20,30]\n                },\n               \n                {\n                'clf': [ExtraTreesRegressor()],\n                'clf__n_estimators': [50,100,150],\n                'clf__max_depth': [10,20,30]\n                },\n               \n                {\n                'clf': [XGBRegressor()],  \n                'clf__learning_rate':[0.1,0.05],\n                'clf__min_samples_split':[5,7,9],\n                'clf__max_depth':[10,20,30]\n                }\n              ]\n\nrscv_relevant_tree = RandomizedSearchCV(pipe, param_grid, cv = 3, scoring = my_score, n_jobs=-1)\nmodel_relevant_tree = rscv_relevant_tree.fit(x_train_relevant, y_train_relevant)","23653692":"rscv_relevant_tree.best_estimator_","7edfae17":"y_pred= rscv_relevant_tree.best_estimator_.predict(x_test_relevant)\nprint('WMAE:', WMAE(y_test_relevant, y_pred))","8d0a1e7e":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nclf = RandomForestRegressor(random_state=0)\n\npipe = Pipeline(steps=[('clf', clf)])\n\nparam_grid_rf = [ {\n                'clf':[RandomForestRegressor()],\n                'clf__n_estimators': [140,150,160],\n                'clf__max_depth': [25,30,35],\n                'clf__max_features': ['auto',5,6]\n                }\n              ]\n\ngscv_rf1 = GridSearchCV(pipe, param_grid_rf, cv = 3, scoring = my_score, n_jobs=-1)\nmodel_rf1 = gscv_rf1.fit(x_train_relevant, y_train_relevant)","b61ab93d":"gscv_rf1.best_estimator_","5091d6e4":"y_pred_rf = gscv_rf1.best_estimator_.predict(x_test_relevant)\nprint('WMAE:', WMAE(y_test_relevant, y_pred_rf))","ee3e06a3":"plt.rcParams[\"figure.figsize\"] = (5,3)\n\nimportances = gscv_rf1.best_estimator_._final_estimator.feature_importances_\n\nattributes = list(x_train_relevant.columns)\nindices = np.argsort(importances)\nattributes_rank = []\nfor i in indices:\n    attributes_rank.append(attributes[i])\nplt.title('Feature Importances')\nplt.tight_layout()\nplt.barh(range(len(indices)), importances[indices], color='gray', align='center')\nplt.yticks(range(len(indices)), attributes_rank, fontsize=5)\nplt.xlabel('Relative Importance',fontsize=5)\nplt.xticks(color='k', size=15)\nplt.yticks(color='k', size=15)\nplt.xlim([0.0, 0.25])\nplt.show()","c497ee7d":"date = test['Date']\ntest = test.drop(['Date'], axis=1)","a04b9fe7":"test_relevant = test.drop(['Temperature','Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI', 'Unemployment'],axis=1)\ntest_relevant = test_relevant.sort_values(['Store', 'Dept'], ascending=[True, True])\ny_pred_rf = gscv_rf1.best_estimator_.predict(test_relevant)","cfe14f87":"test_relevant['Date'] = date\ntest_relevant = test_relevant.sort_values(['Store', 'Dept'], ascending=[True, True])\ntest_relevant['Weekly_Sales'] = y_pred_rf\ntest_relevant","17fd1577":"test = test_relevant\n\nweekly_sales_train = train.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean']}).reset_index()\nweekly_sales_test = test.groupby(['Year','Week']).agg({'Weekly_Sales': ['mean']}).reset_index()\n\nindices = weekly_sales_train.shape[0] + weekly_sales_test['Weekly_Sales'].index \nplt.figure(figsize=(20, 7))\nsns.lineplot(weekly_sales_train['Weekly_Sales'].index,weekly_sales_train['Weekly_Sales']['mean'], color='gray')\nsns.lineplot(indices,weekly_sales_test['Weekly_Sales']['mean'],color = 'red')\n","ae8aaa84":"plt.figure(figsize=(20, 7))\n\nweekly_sales2010 = train.loc[train['Year']==2010].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2011 = train.loc[train['Year']==2011].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2012 = train.loc[train['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2012_test = test.loc[test['Year']==2012].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\nweekly_sales2013_test = test.loc[test['Year']==2013].groupby(['Week']).agg({'Weekly_Sales': ['mean']})\n\nsns.lineplot(weekly_sales2010['Weekly_Sales']['mean'].index, weekly_sales2010['Weekly_Sales']['mean'].values, color='gray')\nsns.lineplot(weekly_sales2011['Weekly_Sales']['mean'].index, weekly_sales2011['Weekly_Sales']['mean'].values, color='gray')\nsns.lineplot(weekly_sales2012['Weekly_Sales']['mean'].index, weekly_sales2012['Weekly_Sales']['mean'].values, color='gray')\nsns.lineplot(weekly_sales2012_test['Weekly_Sales']['mean'].index, weekly_sales2012_test['Weekly_Sales']['mean'].values, color='red')\nsns.lineplot(weekly_sales2013_test['Weekly_Sales']['mean'].index, weekly_sales2013_test['Weekly_Sales']['mean'].values, color='red')\n\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012','2012 test', '2013 test'])\nplt.show()","91547c9b":"sampleSubmission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip', sep=',')","5b818359":"sampleSubmission['Weekly_Sales'] = y_pred_rf\nsampleSubmission.to_csv('submission.csv',index=False)\nsampleSubmission","c8e6aed6":"Since the records are weekly, the \"date\" variable was converted to week of the year and year, as two new variables.","f2a78e77":"\nAnalyzing the type of variables, it is observed that all of them are in numeric format, except for the Date, Type and IsHoliday variables. The \"Date\" variable will not be used for training the model, using only Week and Year. \"IsHoliday\" was transformed to numeric binary and \"Type\" to ordinal numeric format. These transformation was applied to train and test datasets. ","a3bb1584":"#### **5.2. TRAINING WITH MAIN FEATURES**","12d23186":"In order to identify not only if it is holiday, but also which holiday it is, and try to improve the sales volume prediction for these dates, the IsHoliday binary variable was transformed to:\n\n*   0 - if it is not a holiday\n*   1 - if the holiday is SuperBowl\n*   2 - if the holiday is LaborDay\n*   3 - if the holiday is Thanksgiving\n*   4 - if the holiday is Christmas","ea2c6c3f":"The data was also grouped by week but separately for each year, in order to identify patterns between the weeks of different years. As a result, a similar pattern can be seen over the years, with a significant increase in sales in weeks 51 and 47 (Christmas and Thanksgiving). The Superbowl (week 6) and LaborDay holidays (week 36) have little impact on increased sales volume.","dc847c3c":"#### **5.3. HYPERPARAMETERS TUNING**\n\n\n","a7b1982b":"In an attempt to obtain even better results in the prediction, models were also trained only with the features of greatest impact in the \"Weekly Sales\", based on the correlation matrix.\n\nTherefore, the features with the highest correlation (\"Size\", \"Type\" and \"Dept\") were used to train these models, in addition to \"IsHoliday\", needed to calculate the evaluation metric and the features \"Store\", \"Week\" and \"Year\", essential for identifying the record and future prediction.\n\nThe results show that the models using only the most relevant features performs better than the models using all variables.","f7a0ba39":"Analyzing the correlation matrix of the training database, it can be noticed a weak correlation between tWeekly_Sales and the other variables. The variables \"size\", followed by the \"type\" and \"dept\" variables, appear to have the greatest impact on Weekly Sales.","47ced079":"According to the challenge instructions, the holiday dates are expected to have a greater weight in the model training, since in general they represent a greater volume of sales.\n\nThe code below shows all dates that represent holidays, both in the train and test dataset. It is observed that the holidays are in the same weeks (6, 36, 47 and 52) for the years 2010, 2011, 2012 and 2013. From the data provided by the challenge, it is possible to identify what these holidays are.\n\n*   Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13 --> WEEK 6\n*   Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13 --> WEEK 36\n*   Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13 --> WEEK 47\n*   Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13 --> WEEK 52\n\nIt is noticed that there are no sales records on the laborday holiday at the test dataset, since the holiday is in September and the test runs until July.\n\n","b8e276fb":"#### **SIZE x WEEK SALES**","cee9dfe0":"### **4. EVALUATION FUNCTION**","a25647a8":"### **5. MODEL TRAINING**\n\n","d3b8a5a3":"#### **STORES X WEEK SALES**","0b9fbbb3":"The challenge evaluation is based on Weighted Mean Absolute Error (WMAE), with a weight of 5 for Holiday Weeks and 1 otherwise. A function was created to evaluate the model considering these criteria. ","4d1c77d9":"### **2. FEATURES TYPES**","bdc525f5":"### **6. SUBMISSION PREDICTIONS**","a3392eeb":"The 'type' feature also seems to have a certain relationship with Weekly Sales. Type 'A' (transformed to '3') has a higher sales median than types 'B' and 'C', in addition to a greater dispersion of sales values around this median. Type 'C' (transformed to '1') tends to have lower weekly sales.","6178160d":"Despite this differentiation around median, the three types have many outlier records. ","ceaa9aba":"Weekly sales by department are even more irregular, with departments with average sales ranging from 0 to more than 70000.","97684c65":"#### **5.1. TRAINING WITH ALL FEATURES**","01910f51":"Checking the first and last records of the training and test datasets. The train dataset contains 421570 weekly sales records detailed by stores and departments from 02-05-2010 to 10-26-2012. The test base starts one week later and runs until 07-26-2013.","f059490d":"#### **2.1 TRANSFORMATION**","f673458d":"### **1. DATA LOADING**","5831e997":"\n### **3. EXPLORATORY ANALYSIS**","a4cc3d5e":"#### **AVG OF SALES X WEEK X YEAR**","6ee127e6":"The \"Dept\" and \"Size\" seem to be the most important features of the model training. Although \"IsHoliday\" is used to weight the evaluation metric, a large majority of records are holiday-free, and therefore the small proportion of holiday sales records was not as deterministic for forecasting weekly sales.","0f102772":"Finally, we use the training model with the lowest WMAE score to predict the test dataset values. ","fa8ba1c0":"#### **TYPE x WEEK SALES**","697492fb":"In this challenge, the following files were available:\n\n*   stores.csv\n*   features.csv\n*   train.csv\n*   test.csv\n*   sampleSubmission.csv\n\nThe \"store\" and \"features\" files have been combined and joined to the \"train\" and \"test\" datasets. The sampleSubmission file is the template to be used for submission to Kaggle and will be used at the end. The submission dataset is equivalent to the test dataset but only with the triple (store, department and date) to be predicted.\n\n","c7e654f3":"#### **NULLS AND CORRELATIONS**\n","ed0e45a5":"### **7. SUBMISSION**","d71d2ece":"By analyzing a summary of the training and test dataset, there is a certain similarity in the patterns of both datasets, without much discrepancy in the values.","23732218":"#### **2.2. HOLIDAYS**","bfa1cd56":"The training dataset has a percentage of 64% to 74% of null records for the MarkDown variables. However, before removing them, we will analyze their correlation with the other variables in order to check their impact on weekly sales. ","d311ca52":"Despite this discrepancy in weekly sales by store, this behavior seems to remain stable over the years. Some stores showed a decrease in sales over the years, such as stores 14, 27, 35 and 36.","c7fa3ab5":"\nAnalyzing the average weekly sales per store, there is a strong variation in sales volume between stores, ranging from 5000 up to 30000.","938bec09":"Despite this discrepancy in weekly sales by departament, this behavior seems to remain stable over the years. Some departaments showed a decrease in sales over the years, such as departaments 18, 65 and 73.","21d4f586":"After predction, we prepare the file with the results to submit it to Kaggle evaluation and check the final score.","6e8f084e":"The training models were initially fitted with all the features of the train dataset. In order to select the best regression algorithm for this model, Random Search was applied to some of the main regression algorithms.The  RandomForestRegressor algorithm obtained the best result.\n\n","2e0ac102":"Weekly sales data were grouped by week and year in order to identify the average and median sales per week over the years.\n\nIn general, the average values are well above the median, which indicates a high dispersion and variation in sales by stores and departments in a week.\n\nDespite this, there is a certain pattern over the years, with high seasonality at the end of the year.","d18deddd":"Since the model that obtained the best result was the Random Forest algorithm trained with the most relevant features, we will tune some hyperparameters to try to obtain even better results.","eb71c036":"By plotting the weekly sales average of the training base and the predictions for the test dataset, it is possible to conclude that the forecasts appear to be consistent, able to get data pattern and sazonal component.","a96a395d":"Unlike the train dataset, the test has a much lower percentage of null in the MarkDown variables, and about 33% of null records in the CPI and Unemployment variables. \n\nThe correlation matrix shows that the \"Markdown\" variables have a certain correlation with the IsHoliday variable, which can help in predicting. Therefore, these variables will not be eliminated at first.\n\nNull records have been replaced by zero in both datasets.","d0c4756c":"Grouping weekly sales by store size, the chart below seems to indicate a certain trend towards higher sales for larger stores.\n\nHowever, this relationship is far from being proportionally linear, with several cases contradicting this trend.","00f3ce7d":"#### **DEPARTAMENT x WEEK SALES**","2e2165a2":"The pattern of weekly sales by store size seems stable over the years, despite some cases of increase or decrease in sales of stores of the same size from 2010 to 2012."}}