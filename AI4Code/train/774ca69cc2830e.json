{"cell_type":{"3a741745":"code","c38b70b6":"code","8f4aabac":"code","5d02ed14":"code","5ce37e38":"code","fa88128e":"code","1eb4b836":"code","5d1e3084":"code","fb7bd206":"code","6f80514b":"code","af1253e2":"code","cb837c6f":"code","33fe9451":"code","6f0f6d0c":"code","b1d60184":"code","77830d84":"code","acf16172":"code","e635ae25":"code","7fe676fe":"code","b2f5193c":"code","0937add0":"code","b979929c":"code","572e05e1":"code","7b76a6cb":"code","97dfc8ce":"code","a59746a6":"code","7c50b2f8":"code","bc7acb20":"code","c96d03c8":"code","6c7375e8":"code","974afed6":"code","ddea015f":"code","abc9a3dc":"code","e03d5e2a":"code","c7715143":"code","e7cbc951":"code","cb00f7fc":"code","c66d2086":"code","78a3be9d":"code","d7216967":"code","3e573c95":"code","afee1952":"code","0b786ffb":"code","3fe10d05":"code","85f11ada":"markdown","0702fa34":"markdown","e642238e":"markdown","edb8cb51":"markdown","badc6af4":"markdown","80c3288b":"markdown","4f36c2d7":"markdown","7b4ef868":"markdown","753e5cdb":"markdown","cb7d5bc7":"markdown","d565e8e6":"markdown","14cf6b2a":"markdown","613aad12":"markdown","04fd72f0":"markdown","19c6309c":"markdown","0c778218":"markdown","2c38a275":"markdown","b819fa1f":"markdown","1711e81c":"markdown","713fbdd1":"markdown","e24fe96b":"markdown","a3acc82e":"markdown","2233d9ed":"markdown","543dd7be":"markdown","395047b5":"markdown","30d1f939":"markdown","2c185b6b":"markdown","6f9d76dd":"markdown","0959a39e":"markdown"},"source":{"3a741745":"import pandas as pd\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain.head()","c38b70b6":"test = pd.read_csv('..\/input\/test.csv')\ntest.head()","8f4aabac":"train.info()","5d02ed14":"test.info()","5ce37e38":"train.isnull().sum()","fa88128e":"test.isnull().sum()","1eb4b836":"print('Age : %d  percent missing values in the training set' % (train['Age'].isna().sum()*100\/len(train)))\nprint('Cabin : %d percent missing values in the training set' % (train['Cabin'].isna().sum()*100\/len(train)))","5d1e3084":"train.Ticket.value_counts()[:10]","fb7bd206":"train.Cabin.value_counts()[:10]","6f80514b":"train.Embarked.value_counts()[:10]","af1253e2":"train.Pclass.value_counts()","cb837c6f":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nsurvivors = train[train['Survived']==1]\nnot_survivors = train[train['Survived']==0]\nsurvived_class = survivors['Pclass'].value_counts().sort_index()\ndied_class = not_survivors['Pclass'].value_counts().sort_index()\nclasses = survived_class.index\n\nfig,ax = plt.subplots(figsize=(12,8))\nbar_width = 0.35\nbar0 = ax.bar(classes,died_class,bar_width,label='Died')\nbar1 = ax.bar(classes+bar_width,survived_class,bar_width,label='Survived')\nax.set_title('Survivals by passenger class')\nax.set_xlabel('Passenger Class')\nax.set_xticklabels(classes)\nax.set_xticks(classes + bar_width \/ 2)\nax.set_ylabel('Count')\nax.legend()\nplt.show()","33fe9451":"survived = survivors['Sex'].value_counts().sort_index()\ndied = not_survivors['Sex'].value_counts().sort_index()\n\nfig,ax = plt.subplots(figsize=(12,8))\nbar_width = 0.35\nindex = pd.Index([1,2])\nbar0 = ax.bar(index,died,bar_width,label='Died')\nbar1 = ax.bar(index+bar_width,survived,bar_width,label='Survived')\nax.set_title('Survival by sex')\nax.set_xlabel('Sex')\nax.set_xticklabels(survived.index)\nax.set_xticks(index + bar_width \/ 2)\nax.set_ylabel('Count')\nax.legend()","6f0f6d0c":"import seaborn as sns\nax = plt.figure(figsize=(12,8))\nax = sns.swarmplot(x='Survived',y='Age',data=train)\nax.set_title('Suvival by Age')\nax.set_xticklabels(['Died','Survived'])\nax.set_xlabel('')","b1d60184":"survived_class = survivors['Embarked'].value_counts().sort_index()\ndied_class = not_survivors['Embarked'].value_counts().sort_index()\nindex = pd.Index([1,2,3])\n\nfig,ax = plt.subplots(figsize=(12,8))\nbar_width = 0.35\nbar0 = ax.bar(index,died_class,bar_width,label='Died')\nbar1 = ax.bar(index+bar_width,survived_class,bar_width,label='Survived')\nax.set_title('Survivals by port of embarkation')\nax.set_xlabel('Port of embarkation')\nax.set_xticklabels(survived_class.index)\nax.set_xticks(index + bar_width \/ 2)\nax.set_ylabel('Count')\nax.legend()\nplt.show()","77830d84":"s_sib = survivors['SibSp'].value_counts().sort_index()\nd_sib = not_survivors['SibSp'].value_counts().sort_index()\ns_sib = s_sib.reindex(d_sib.index).fillna(0.0)\nfig,ax = plt.subplots(figsize=(12,8))\nindex = s_sib.index\nbar_width = 0.25\nbar0 = ax.bar(index,d_sib,bar_width,label='Died')\nbar1 = ax.bar(index+bar_width,s_sib,bar_width,label='Survived')\nax.set_title('Survival by number of sibling\/spouse on board')\nax.set_xlabel('Number of sibling\/spouse')\nax.set_xticks(index + bar_width \/ 2)\nax.set_xticklabels(s_sib.index)\nax.set_ylabel('Count')\nax.legend()","acf16172":"sibsp = train['SibSp'].value_counts()\nsibsp_survival = survivors['SibSp'].value_counts()\nsurvival_pct_by_sibling = sibsp_survival \/ sibsp * 100\n\nplt.bar(survival_pct_by_sibling.index,survival_pct_by_sibling)\nplt.title('Survival % by number of sibling\/spouse on board')\nplt.xlabel('Number of sibling\/spouse on board')\nplt.ylabel('Survival %')","e635ae25":"s_parch = survivors['Parch'].value_counts().sort_index()\nd_parch = not_survivors['Parch'].value_counts().sort_index()\ns_parch = s_parch.reindex(d_parch.index).fillna(0.0)\n\nfig,ax = plt.subplots(figsize=(12,8))\nindex = s_parch.index\nbar_width = 0.25\nbar0 = ax.bar(index,d_parch,bar_width,label='Died')\nbar1 = ax.bar(index+bar_width,s_parch,bar_width,label='Survived')\nax.set_title('Survival by number of parents\/children on board')\nax.set_xlabel('Number of parents\/children')\nax.set_xticks(index + bar_width \/ 2)\nax.set_xticklabels(s_parch.index)\nax.set_ylabel('Count')\nax.legend()","7fe676fe":"train['par_ch'] = np.where(train['Parch']==0,0,1)\n\nsurvivors = train.loc[train['Survived']==1]\nnot_survivors = train.loc[train['Survived']==0]\nd_parch = not_survivors['par_ch'].value_counts().sort_index()\ns_parch = survivors['par_ch'].value_counts().sort_index()\n\nfig, ax = plt.subplots(figsize=(12,8))\nindex = pd.Index([1,2])\nbar_width = 0.4\n\nbar0 = ax.bar(index,d_parch,bar_width,label='Died')\nbar1 = ax.bar(index+bar_width,s_parch,bar_width,label='Survived')\nax.set_title('Survival by having parents\/children onboard')\nax.set_xlabel('')\nax.set_xticks(index + bar_width \/ 2)\nax.set_xticklabels(['No parents\/children','One or more'])\nax.legend()","b2f5193c":"ax = plt.figure(figsize=(12,8))\nax = sns.boxplot(x = 'Survived', y = 'Fare', data = train)\nax.set_title('Survival by fare')\nax.set_xticklabels(['Died','Survived'])\nax.set_xlabel('')","0937add0":"plt.figure(figsize=(12,8))\nax = sns.boxplot(x = 'Pclass', y = 'Fare', data = train)\nax.set_title('Fare paid for each Pclass')","b979929c":"cols = ['Survived','Pclass','Sex','Age','SibSp','Parch','Fare']\ncorr = train[cols].corr()\ncorr","572e05e1":"plt.figure(figsize=(12,8))\nplt.title('Age of passengers')\nplt.xlabel('Age')\nplt.ylabel('Count')\ntrain['Age'].hist(bins=30)","7b76a6cb":"age_mean = train['Age'].mean()\nage_median = train['Age'].median()\nprint('Mean age : ', age_mean, ' Median age : ',age_median)","97dfc8ce":"mean_imp_age = train['Age'].fillna(age_mean)\nmedian_imp_age = train['Age'].fillna(age_median)\n\nplt.figure(figsize=(12,8))\nax1 = plt.subplot(1,2,1)\nax1 = mean_imp_age.hist(bins=30)\nplt.title('Mean imputation')\nplt.xlabel('Age')\n\nax2 = plt.subplot(1,2,2)\nplt.title('Median imputation')\nplt.xlabel('Age')\nax2 = median_imp_age.hist(bins=30)","a59746a6":"male_children = train.loc[train['Name'].str.contains('Master')]\nmale_children['Age'].describe()","7c50b2f8":"train.loc[train['Sex']=='male'].drop(male_children.index).sort_values(by='Age')[:5]","bc7acb20":"male_children[male_children['Age'].isnull()]","c96d03c8":"mrs = train['Name'].str.contains('Mrs.')\nmiss = train['Name'].str.contains('Miss.')\nparch = train['Parch']>0\nno_parch = train['Parch']==0\n\nfig,axes = plt.subplots(figsize=(12,7))\n\nax = sns.distplot(train[mrs]['Age'].dropna(),axlabel='Age',label='\"Mrs\"',kde=False,bins=20)\nax.set_title('Comparison of ages with the titles \"Mrs\" and \"Miss\"')\nax = sns.distplot(train[miss]['Age'].dropna(),axlabel='',label=\"Miss\",kde=False,bins=20)\nax.set_yticks([0,5,10,15,20])\nplt.legend()\n","6c7375e8":"fig,axes = plt.subplots(figsize=(12,7))\n\nax1 = sns.distplot(train[miss & parch]['Age'].dropna(),axlabel='Age',label='\"Miss with parch\"',kde=False,bins=20)\nax1 = sns.distplot(train[miss & no_parch]['Age'].dropna(),axlabel='',label='\"Miss without parch\"',kde=False,bins=20)\nax1.set_title('Comparison of ages of \"Miss\" with and without \"parch onboard')\nplt.legend()","974afed6":"from sklearn.base import TransformerMixin\n\nclass Age_Imputer(TransformerMixin):\n    def __init__(self):\n        \"\"\"\n        Imputes ages of passengers in the Titanic, values to be imputed will be dependant \n        on passenger titles and the presence of parents or children on board\n        \"\"\"\n        pass\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        def value_imp(passengers):\n            \"\"\"\n            Imputes an age, based on a weighted random choice derived from the non\n            null entries in the subsets of the dataset.\n            \"\"\"\n            passengers=passengers.copy()\n            # Create 3 year age bins\n            bins = np.arange(0,passengers['Age'].max()+3,step=3)\n            # Assign each passenger an age bin\n            passengers['age_bins'] = pd.cut(passengers['Age'],bins=bins,labels=bins[:-1]+1.5)\n            # Count totals of age bins\n            count = passengers.groupby('age_bins')['age_bins'].count()\n            # Assign each age bin a weight\n            weights = count\/len(passengers['Age'].dropna())\n            null = passengers['Age'].isna()\n            # For each missing value, give the passenger an age from the age bins available\n            passengers.loc[passengers['Age'].isna(),'Age']=np.random.RandomState(seed=42).choice(weights.index,\n                           p=weights.values,size=len(passengers[null]))\n            return passengers\n        master = X.loc[X['Name'].str.contains('Master')]\n        mrs = X.loc[X['Name'].str.contains('Mrs')]\n        miss = X.loc[X['Name'].str.contains('Miss')]\n        no_parch = X.loc[X['Parch']==0]\n        parch = X.loc[X['Parch']!=0]\n        miss_no_parch = miss.drop([x for x in miss.index if x in parch.index])\n        miss_parch = miss.drop([x for x in miss.index if x in no_parch.index])\n        remaining_mr = X.loc[X['Name'].str.contains('Mr. ')]\n        # Imputing 'Mrs' first, as in cases where passengers have the titles\n        # 'Miss' and 'Mrs', they are married so will be in the older category\n        name_cats = [master,mrs,miss_no_parch,miss_parch,remaining_mr]\n        for name in name_cats:\n            X.loc[name.index] = value_imp(name)\n        return X","ddea015f":"def value_imp(passengers):\n            \"\"\"\n            Imputes an age, based on a weighted random choice derived from the non\n            null entries in the subsets of the dataset.\n            \"\"\"\n            passengers = passengers.copy()\n            bins = np.arange(0,passengers['Age'].max()+3,step=3)\n            passengers['age_bins'] = pd.cut(passengers['Age'],bins=bins,labels=bins[:-1]+1.5)\n            count = passengers.groupby('age_bins')['age_bins'].count()\n            weights = count\/len(passengers['Age'].dropna())\n            null = passengers['Age'].isna()\n            passengers.loc[passengers['Age'].isna(),'Age']=np.random.RandomState(seed=42).choice(weights.index,\n                           p=weights.values,size=len(passengers[null]))\n            return passengers","abc9a3dc":"train2 = train.copy()\nmrs = train2.loc[train2['Name'].str.contains('Mrs.')]\ntrain2.loc[mrs.index] = value_imp(mrs)","e03d5e2a":"fig,axes = plt.subplots(figsize = (12,7))\n\nax = sns.distplot(train2.loc[mrs.index]['Age'],label='Custom Imputation',kde=False,bins=20)\nax = sns.distplot(train.loc[mrs.index]['Age'].fillna(value=mrs['Age'].mean()),\n                  label='Mean Imputation',kde=False,bins=20\n                 )\nax.set_title('Comparison of custom and mean imputation')\nplt.legend()","c7715143":"train3 = train.copy()\nimp = Age_Imputer()\nimp.fit_transform(train3)\nfig,axes = plt.subplots(figsize = (12,7))\n\nax = sns.distplot(train.loc[mrs.index]['Age'].dropna(),label='No imputation',kde=False,bins=20)\nax = sns.distplot(train3.loc[mrs.index]['Age'],label='Full custom imputation',kde=False,bins=20)\nax.set_title('Comparison of full custom imputation and no imputation')\nplt.legend()","e7cbc951":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder(TransformerMixin):\n    def __init__(self,columns = None):\n        self.columns = columns \n\n    def fit(self,X,y=None):\n        return self\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n    \nclass DataFrameSelector(TransformerMixin):\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        return X[self.attribute_names].values\n\nclass ValueImputer(TransformerMixin):\n    \"\"\"\n    Imputes a fixed value\n    \"\"\"\n    def __init__(self,attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X[self.attribute_names] = X[self.attribute_names].fillna('S')\n        return X[self.attribute_names]\n\nnumerical_atts = ['Age','SibSp','Parch','Fare']\ncat_atts = ['Sex','Pclass','Embarked']\n\nnum_pipeline = Pipeline([\n    ('imputer', Age_Imputer()),\n    ('selector', DataFrameSelector(numerical_atts)),\n    ('imp', Imputer(strategy='mean')),\n    ('scaler', StandardScaler()),\n])\n\ncat_pipeline = Pipeline([\n    ('cat_imputer', ValueImputer(cat_atts)),\n    ('encoder', MultiColumnLabelEncoder(columns=cat_atts)),\n    ('selector', DataFrameSelector(cat_atts)),\n])\n\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])\n\ntrain_data_prepared = full_pipeline.fit_transform(train)\ntrain_labels = train['Survived']\n\nfeature_list = numerical_atts + cat_atts","cb00f7fc":"train_data_prepared.shape","c66d2086":"feature_list","78a3be9d":"from sklearn.linear_model import RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import cross_val_score\n\nclassifiers = [RidgeClassifier(),KNeighborsClassifier(),\n              SGDClassifier(\n                  max_iter=1000),DecisionTreeClassifier(),RandomForestClassifier(),\n              MLPClassifier(max_iter=1000)]\nnames = ['Ridge','KNN','SGD','Decision Tree','Random Forest','MLP']\n\nfor name, classifier in zip(names,classifiers):\n    classifier.fit(train_data_prepared,train_labels)\n    print('Scores for ',name,' : ',cross_val_score(classifier,train_data_prepared,train_labels,cv=3).mean())\n    \n","d7216967":"from sklearn.model_selection import RandomizedSearchCV\n\nmlp = RandomizedSearchCV(MLPClassifier(),cv=3,n_iter=20,param_distributions=(\n    {'hidden_layer_sizes':[(100,),(200,),(500,),(1000,)],\n    'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n    'solver' : ['lbfgs','sgd','adam'],\n    'alpha' : np.linspace(0,0.001),\n    'max_iter' : [200,500,1000,2000],\n    }))\nmlp.fit(train_data_prepared,train_labels)\n","3e573c95":"print('Best score : {}'.format(mlp.best_score_))\nmlp.best_params_\n","afee1952":"test_prepared = full_pipeline.fit_transform(test)\nbest_mlp = mlp.best_estimator_\npredictions = best_mlp.predict(test_prepared)\npredictions[:20]","0b786ffb":"submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':predictions})\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","3fe10d05":"forest = RandomForestClassifier()\nforest.fit(train_data_prepared,train_labels)\nsorted(zip(forest.feature_importances_,feature_list),reverse=True)","85f11ada":"This is clearly an unsatisfactory method that will skew the data.\n### Custom imputation methods\nThere are again many custom methods that could be applied to fill in the missing values. Two of which are outlined below.\n\nStratified imputation : Imputing values into age brackets in the same ratio of the values present in the age brackets in the training set.\nName-based impuation : Using information available in the names of passengers, try and predict more accurately which age bracket the passenger falls in to.\n\nUsing a combination of the two methods above, we will look at the titles of passengers and whether they have parents or children ('Parch') on board.","0702fa34":"These four 'Master's with missing values for age were also travelling with at least one 'Parch', presumably a parent in these cases. We can therefore be confident of these four passengers being 12 or under. \n\nA similar inference can be made for females with the title 'Mrs', missing age values here will indicate these passengers not being children. Those females with the title 'Miss' are far more likely to have parents than children when having 0 in the 'Parch' column. We can confirm this hypothesis with a simple visulatisaton.","e642238e":"The best parameters returned by the search. These can now be used to make a prediction on the test set, the pipeline now making the job of transforming the test data a simple one.","edb8cb51":"This result scored 0.77033, nearly 80% of passengers correctly predicted.\n## Next steps\nTo try and improve the model further there are several different avenues to explore.\n#### Feature engineering\nAdapting the available features or creating entirely new ones out of the current data.","badc6af4":"While by no means concrete, males with the title 'Master' will be children, males with the title 'Mr' are 11 or over. Females with the title 'Mrs' can be assumed to be 14 or older and females with the title 'Miss' will be on average much younger if travelling with at least one 'Parch'.\n\nThis will give us a more accurate way of imputing the ages than just assigning an age on a simple stratified basis.","80c3288b":"None of the columns have high collinearity, while not being independent variables they may all be important in the final model.","4f36c2d7":"## Data Cleaning\n### Missing values\nEmbarked has two missing values, these can be filled with the most frequent value 'S' as we have no other information to go on.\nFare has one missing value in the test set which can be imputed with the median.\n\nThe age column has a lot of null entries, as we do not wish to discard the column we can pick from a number of methods to impute these missing values.\n\n1. Impute with 0.0 : Not applicable in this dataset\n2. Impute with the mean or median : A common method for values that are known to not be 0, however it risks skewing the dataset.\n3. Custom imputation : Design a different method for imputing the missing values.\n\nVisualising the distribution of ages will give us an insight into which method may be appropriate","7b4ef868":"### Data visualisation","753e5cdb":"The train and test set have identical features, the target column only being present in the training data.","cb7d5bc7":"By filtering for males and removing those with the title 'Master' we can see here that the youngest remaining male is 11 years old.","d565e8e6":"With a max value of 12, males whose name contains 'Master' are all children.","14cf6b2a":"Being female gave you a considerably better chance of survivng.","613aad12":"Importing a selection of models, fitting to the train data and predicting the training labels, using the average score of a 3 fold cross validation to try and avoid overfitting to the training data.","04fd72f0":"Ticket and Cabin are questionable features, ticket seems to be a generic alpha numeric value and cabin has over 70% of its values missing, without knowing information about the location of cabins I will discard both of these features.","19c6309c":"Passengers embarking at port 'C' have the best survival rate of the training data.","0c778218":"### Initial modelling\nWe will first run a selection of classifiers on the training data with their default values, then choosing the most promising to pursue further with hyperparameter tuning.\n#### Pipeline\nA pipeline is not an essential piece of a project, however it allows easy access to add or remove a feature or tweak a hyperparameter and quickly be able to reproduce results. It will also allow us to implement GridSearchCV and RandomizedSearchCV to automatically test out many different hyperparameters, imputation methods or features. It would also allow quick transformation of any additional training data added to the dataset.\n\nGiven the different scales of numeric values, we will use a standard scaler on all numeric columns. We will encode all categorical labels ","2c38a275":"### Mean and median imputation","b819fa1f":"To submit the predictions to the kaggle leaderboard a csv must be created.\n","1711e81c":"Given information like this about feature importance we can choose to adapt the age column into categories or make a new feature that is a combination of exisiting ones such as Age\/Fare. We could also onehotencode all categorical features to remove any chance of the model inferring relationships between the numbers currently assigned.\n#### Hyperparameter tuning\/model selection\nFurther fine tuning the existing model or trying different ones, new features may lead to improved performance of different models.\n#### Error evaluation\nGiven access to the answers we could categorise the errors that the model made, did it give too many false positives for young women for example. Using this information both the model and input features can be adapted to improve the models accuracy.","713fbdd1":"Having a relative or spouse on board greatly improved your chances of survival","e24fe96b":"### Hyperparameter tuning\nThe MLP classifier has performed the best on the training data so we will focus on that with some hyperparameter tuning.\n\nRandomizedSearchCV lets you input a selection of hyperparameters, select a number of searches to make and will randomly select the models hyperparameters. This is a very good option if you are not sure where to start looking for hyperparameter values as it will cover a wide selection of values. Performing many iterations of fitting and predicting this way is however very computationally expensive with large datasets.","a3acc82e":"## Initial Exploratory Data Analysis\n","2233d9ed":"Children were more likely to survive and the over 70s were very unlikely to make it. Young men look to have the worst survival rate.","543dd7be":"As an example, I will impute the 'Mrs' values and compare mean imputation and my custom imputation.","395047b5":"Survival rates are significantly worse for those in third class, with first class having the best chances.","30d1f939":"# Titanic Dataset\nA classification task, predict whether or not passengers in the test set survived. I started learning to code about 12 months ago in my spare time and thanks to some friends have become interested in data science, this is my first data science project","2c185b6b":"This method gives both a stratified imputation and by utilising some simple logic of titles of the era it has allowed us to more accurately predict the ages of those passengers with missing values.","6f9d76dd":"Higher paying passengers were more likely to survive, easily inferred from the passenger class information earlier.","0959a39e":"891 entries in the training set, with 418 in the test set."}}