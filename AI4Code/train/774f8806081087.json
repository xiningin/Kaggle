{"cell_type":{"99fbbfa4":"code","08d6dd69":"code","97323b64":"code","4d52b89b":"code","fd836749":"code","104efacf":"code","57906b43":"code","fb5ee0aa":"code","d7c8aa05":"code","b44e2ca6":"code","170b52a0":"code","f12655af":"code","41d1cce7":"code","2bbef361":"code","fca44a79":"code","416d02e0":"code","5aca4677":"code","5654483a":"code","99167db5":"code","efb1b100":"code","e61f7571":"code","565152ef":"code","8ae77c7b":"code","a10d0209":"code","7b1aefe9":"code","80edd0f0":"code","f91ce232":"code","27eae9d9":"code","0464b5fa":"code","d2ef0fe3":"code","fa575718":"code","1d0f0c73":"code","1e1918d2":"code","40e0b40a":"code","1099b251":"code","be04c15e":"code","bf91b86c":"code","1697686c":"code","54a6a124":"code","f4a80e7d":"code","f766abc4":"code","539616ed":"code","5979de9a":"code","8c31c1e3":"code","c588d007":"code","38aa371e":"code","9c4a5f5f":"code","0f8bff96":"code","083b8efc":"code","a62c7bbd":"code","3644f985":"code","f9d0c7fa":"code","5771eaa5":"code","0b43cfff":"code","b3522008":"code","a914dd47":"code","139bd976":"code","3dd4c69b":"code","771d32f6":"code","9fa49b85":"code","acbaf47b":"code","12f6a924":"code","7cc388d1":"code","cf378534":"code","f3db54fc":"code","0467ffa1":"code","6de49330":"code","79074185":"code","7f089dfb":"code","73114260":"code","faa91b63":"code","db275a1c":"code","76d9b095":"code","b455dfb7":"code","fc3fe93f":"code","6444a24d":"code","69608f79":"code","1ffbd0d9":"code","bd90e5c6":"code","0c42e86a":"code","e407f56b":"code","c5f82b11":"code","f526f67d":"code","f05622e0":"code","559dd37f":"code","7daed783":"code","b7249163":"code","79adae91":"code","2a6a1973":"code","bf4b4bf0":"code","76e34afc":"code","30f2eac5":"code","1648a37b":"code","b15d1fb0":"code","d596db9c":"code","4f27ec87":"code","f3c8a9ab":"code","0a741b40":"code","af846d4b":"code","3d744039":"code","11afeb88":"code","37c8a3df":"code","cf348702":"code","65e0f681":"code","81614a4c":"code","0112fb54":"code","989a1dba":"code","03fba424":"code","17cde047":"code","dee8deae":"code","12bb7754":"code","91bb7b6c":"code","29e581f3":"code","bcfa56c2":"code","60376aa4":"code","604d7472":"code","26a224b7":"code","6095b9b5":"code","f9287503":"code","985fac13":"code","0f495fc4":"code","801e657a":"code","e776cf8e":"code","708c167b":"code","d114a88a":"code","71c3018a":"code","c15f98d6":"code","6515d2df":"code","21c65705":"code","9c479edc":"code","633a847e":"code","4f15dc33":"code","406351d2":"code","6a5c57b5":"code","4cc02e71":"code","7f5936c5":"code","52e65895":"code","70e25559":"markdown","944a1473":"markdown","0d21ca9d":"markdown","2690dc87":"markdown","fdc586bc":"markdown","752d9429":"markdown","bf27d3f5":"markdown","e4e5dcac":"markdown","e09bc694":"markdown","48a8467a":"markdown","fb3ffe3e":"markdown","938b8752":"markdown","6d666278":"markdown","7634fecb":"markdown","02a16b26":"markdown","8a03fed7":"markdown","6f6adcbe":"markdown","9595cdab":"markdown","fd62a45a":"markdown","8fedc164":"markdown","23fb1d22":"markdown","16a7533c":"markdown","12ce77c6":"markdown","ede48da0":"markdown","2e2762ef":"markdown","08b8cf4f":"markdown","728c4684":"markdown","c37bf1f7":"markdown","3c370593":"markdown","103061e3":"markdown","d48d84c2":"markdown","c9771b85":"markdown"},"source":{"99fbbfa4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08d6dd69":"# imports \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import (train_test_split, StratifiedKFold, \n                                     cross_val_score, cross_validate, \n                                     cross_val_predict, GridSearchCV, RandomizedSearchCV)\nfrom sklearn import (preprocessing as pp,\n                    feature_selection as fs)\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier)\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import (roc_auc_score, auc, roc_curve, \n                            confusion_matrix, f1_score, log_loss, \n                            classification_report)\n\nimport missingno as msno \n\nfrom scipy.stats import chi2_contingency\n\nfrom statsmodels.stats import weightstats as stests\n\nfrom scipy import stats\n\nfrom xgboost import XGBClassifier\n","97323b64":"df = pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')","4d52b89b":"df_train, df_test = train_test_split(df, test_size=0.20, random_state=42, stratify=df['TARGET'])","fd836749":"set(df_train.dtypes)","104efacf":"#separating columns by type\n\nbinary_features = [col for col in df_train.drop(columns=['TARGET', 'SK_ID_CURR']).columns.values if df_train[col].nunique()<=2]\n\nnum_features = df_train.drop(columns=['TARGET', 'SK_ID_CURR']).select_dtypes(include=['int64', 'float64']).columns \nnum_features = list(set(num_features) - set(binary_features))\n\ncat_features = df_train.drop(columns=['TARGET', 'SK_ID_CURR']).select_dtypes(include='O').columns\ncat_features = list(set(cat_features) - set(binary_features))\n","57906b43":"title = str(df_train['TARGET'].value_counts(normalize=True))\ndf_train['TARGET'].value_counts(normalize=True).plot.bar(title=title)","fb5ee0aa":"#plotting missing numerical features values\nplt.figure(figsize=(10,40))\nsns.barplot(x=df_train.count()[:],y=df_train.count().index)\nplt.xlabel('Non-Null Values Count')\nplt.ylabel('Numerical Features')","d7c8aa05":"# showing percentage of missing data per feature\nmissing_p_feat= {feat: round(df_train[feat].isnull().mean(), 3) for feat in df_train.columns.values if df_train[feat].isnull().sum()>0}\n\n# frequency of columns per missing percentage\nplt.hist( np.array(list(missing_p_feat.values())), bins=70)","b44e2ca6":"sorted([(value,key) for (key,value) in missing_p_feat.items()])","170b52a0":"msno.dendrogram(df_train)","f12655af":"housig_features_w_missing = [ \n        'LANDAREA_AVG',\n        'LANDAREA_MEDI',\n        'LANDAREA_MODE',\n        'BASEMENTAREA_AVG',\n        'BASEMENTAREA_MEDI',\n        'BASEMENTAREA_MODE',\n        'YEARS_BEGINEXPLUATATION_AVG',\n        'YEARS_BEGINEXPLUATATION_MEDI',\n        'YEARS_BEGINEXPLUATATION_MODE',\n        'TOTALAREA_MODE',\n        'EMERGENCYSTATE_MODE',\n        'LIVINGAREA_AVG',\n        'LIVINGAREA_MEDI',\n        'LIVINGAREA_MODE',\n        'FLOORSMAX_AVG',\n        'FLOORSMAX_MEDI',\n        'FLOORSMAX_MODE',\n        'ENTRANCES_AVG',\n        'ENTRANCES_MEDI',\n        'ENTRANCES_MODE',\n        'WALLSMATERIAL_MODE',\n        'HOUSETYPE_MODE',\n        'APARTMENTS_AVG',\n        'APARTMENTS_MEDI',\n        'APARTMENTS_MODE',    \n        'ELEVATORS_AVG',\n        'ELEVATORS_MEDI',\n        'ELEVATORS_MODE',\n        'NONLIVINGAPARTMENTS_AVG',\n        'NONLIVINGAPARTMENTS_MEDI',\n        'NONLIVINGAPARTMENTS_MODE',\n        'LIVINGAPARTMENTS_AVG',\n        'YEARS_BUILD_AVG',\n        'YEARS_BUILD_MEDI',\n        'YEARS_BUILD_MODE',\n        'FLOORSMIN_AVG',\n        'FLOORSMIN_MEDI',\n        'FLOORSMIN_MODE',\n        'NONLIVINGAREA_AVG',\n        'NONLIVINGAREA_MEDI',\n        'NONLIVINGAREA_MODE',\n        'LIVINGAPARTMENTS_MEDI',\n        'LIVINGAPARTMENTS_MODE',\n        'FONDKAPREMONT_MODE',\n        'COMMONAREA_AVG',\n        'COMMONAREA_MEDI',\n        'COMMONAREA_MODE']","41d1cce7":"all_housing_missing = df_train[housig_features_w_missing].isnull().all(axis=1)\nratio_on_housing_miss = df_train['TARGET'][all_housing_missing].mean()\nratio_on_housing_avail = df_train['TARGET'][~all_housing_missing].mean()\n\nprint(f'default rate on missing housing data aplicants: {ratio_on_housing_miss:.2%}')\nprint(f'default rate on available housing data aplicants: {ratio_on_housing_avail:.2%}')","2bbef361":"cont = pd.crosstab( all_housing_missing, df_train['TARGET'])\n\ntest = chi2_contingency(cont, lambda_=\"log-likelihood\")\np_val = test[1]\nis_stats_diff = p_val<0.03\n\nprint(f'is there statistical significance: {is_stats_diff}')","fca44a79":"num_housing = list(set(num_features).intersection(set(housig_features_w_missing)))\ndf_train[num_housing].hist(figsize=(25,18), bins=50)","416d02e0":"xtreme_inputer = SimpleImputer(fill_value=9999, strategy=\"constant\")\n\ndf_train[num_housing] = xtreme_inputer.fit_transform(df_train[num_housing])\ndf_test[num_housing] = xtreme_inputer.transform(df_test[num_housing])","5aca4677":"# replacing missing values in categorical features\ncat_housing = list(set(cat_features).intersection(set(housig_features_w_missing)))\ncat_housing","5654483a":"fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\nfor i, col in enumerate(cat_housing):\n    df_train[col].hist(ax=axes[i])\nplt.show()","99167db5":"df_train[cat_housing] = df_train[cat_housing].fillna(\"MISSING\")\ndf_test[cat_housing] = df_test[cat_housing].fillna(\"MISSING\")","efb1b100":"fig, axes = plt.subplots(nrows=3, figsize=(10, 8))\nfor i, col in enumerate(cat_housing):\n    df_train[col].hist(ax=axes[i])\nplt.show()","e61f7571":"#Listing the rest of the feature with missing values\n\nsorted([(value,key) for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing])","565152ef":"to_drop_missing = sorted([key for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing and value < 0.01])\nto_drop_missing","8ae77c7b":"# dropping values where missing <1%\n\ndf_train = df_train.dropna(axis=0, how='any', subset=to_drop_missing)\ndf_test = df_test.dropna(axis=0, how='any', subset=to_drop_missing)","a10d0209":"rest_of_missing = sorted([key for (key,value) in missing_p_feat.items() if key not in housig_features_w_missing and value>0.01])\nrest_of_missing","7b1aefe9":"df_train['OCCUPATION_TYPE'].value_counts().plot.bar()","80edd0f0":"df_train['OCCUPATION_TYPE'] = df_train['OCCUPATION_TYPE'].fillna(\"MISSING\")\ndf_test['OCCUPATION_TYPE'] = df_test['OCCUPATION_TYPE'].fillna(\"MISSING\")","f91ce232":"df_train['OCCUPATION_TYPE'].value_counts().plot.bar()","27eae9d9":"df_train[rest_of_missing[:6]].hist(figsize=(8,8))","0464b5fa":"missing_req_credit = df_train[rest_of_missing[:6]].isnull().any(axis=1)\nratio_on_missing_req_credit = df_train['TARGET'][missing_req_credit].mean()\nratio_on_avail_req_credit = df_train['TARGET'][~missing_req_credit].mean()\n\nprint(f'default rate on missing credit requests: {ratio_on_missing_req_credit:.2%}')\nprint(f'default rate on available credit requests: {ratio_on_avail_req_credit:.2%}')\n\ncont = pd.crosstab(missing_req_credit, df_train['TARGET'])\n\ntest = chi2_contingency(cont, lambda_=\"log-likelihood\")\np_val = test[1]\nis_stats_diff = p_val<0.03\n\nprint(f'is there statistical significance: {is_stats_diff}')\nprint(f'p: {p_val}')","d2ef0fe3":"req_credit = [\n    'AMT_REQ_CREDIT_BUREAU_DAY',\n    'AMT_REQ_CREDIT_BUREAU_HOUR',\n    'AMT_REQ_CREDIT_BUREAU_MON',\n    'AMT_REQ_CREDIT_BUREAU_QRT',\n    'AMT_REQ_CREDIT_BUREAU_WEEK',\n    'AMT_REQ_CREDIT_BUREAU_YEAR'\n]\n\ndf_train[req_credit] = xtreme_inputer.fit_transform(df_train[req_credit])\ndf_test[req_credit] = xtreme_inputer.transform(df_test[req_credit])","fa575718":"for col in ['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']:\n    missing = df_train[col].isnull()\n    missing_rate = df_train['TARGET'][missing].mean()\n    not_missing_rate = df_train['TARGET'][~missing].mean()\n    \n    print(col)\n    print(f'default rate on missing: {missing_rate:.2%}')\n    print(f'default rate on available: {not_missing_rate:.2%}')\n\n    cont = pd.crosstab(missing, df_train['TARGET'])\n\n    test = chi2_contingency(cont, lambda_=\"log-likelihood\")\n    p_val = test[1]\n    is_stats_diff = p_val<0.03\n\n    print(f'Is there statistical significance: {is_stats_diff}')\n    print(f'p val: {p_val}')\n    print('_'*20, '\\n')\n    ","1d0f0c73":"df_train[['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']].hist(figsize=(8,8))","1e1918d2":"cols = ['EXT_SOURCE_1','EXT_SOURCE_3','OWN_CAR_AGE']\n\ndf_train[cols] = xtreme_inputer.fit_transform(df_train[cols])\ndf_test[cols] = xtreme_inputer.transform(df_test[cols])","40e0b40a":"#checking if there's any remaining features with missing values\nremaining_missing = df_train.columns[np.where(df_train.isnull().sum()!=0)]\nremaining_missing","1099b251":"df_train['EMERGENCYSTATE_MODE'].hist(figsize=(5, 5))","be04c15e":"df_train = df_train.dropna(axis=0, how='any', subset=remaining_missing)\ndf_test = df_test.dropna(axis=0, how='any', subset=remaining_missing)","bf91b86c":"df_train[cat_features].head(5)","1697686c":"sns.set(style=\"whitegrid\")\n\ndef show_plots(feature, figsize=None):\n    if not figsize:\n        figsize = (20, 5)\n    fig, ax = plt.subplots(1, 2, figsize=figsize)\n    fig.suptitle(feature)\n    \n    order = df_train[[feature,'TARGET']].groupby(feature)['TARGET'].mean().sort_values().keys()\n\n    ct = sns.countplot(data=df_train, y=feature, ax=ax[0], order=order)\n    ct.set_title(\"COUNT\")\n    ax[0].set_xlabel('')\n    ax[0].set_ylabel('')\n    \n\n    dfr = sns.barplot(y=feature, x=\"TARGET\", data=df_train, ax=ax[1], order=order)\n    dfr.set_title(\"DEFAULT %\")\n    dfr.set(yticklabels=list())\n    ax[1].set_xlabel('')\n    ax[1].set_ylabel('')","54a6a124":"\ndf_train = df_train.drop(df_train.loc[~df_train['CODE_GENDER'].isin(['F','M'])].index)\ndf_test = df_test.drop(df_test.loc[~df_test['CODE_GENDER'].isin(['F','M'])].index)","f4a80e7d":"show_plots(\"CODE_GENDER\", figsize = (15,2))","f766abc4":"df_train[\"CODE_GENDER\"] = df_train[\"CODE_GENDER\"].replace({'F':0, 'M':1})\ndf_test[\"CODE_GENDER\"] = df_test[\"CODE_GENDER\"].replace({'F':0, 'M':1})","539616ed":"show_plots(\"WALLSMATERIAL_MODE\", figsize = (15,3))","5979de9a":"vc = dict(df_train[\"WALLSMATERIAL_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.05]\n\ndf_train[\"WALLSMATERIAL_MODE\"] = df_train[\"WALLSMATERIAL_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"WALLSMATERIAL_MODE\"] = df_test[\"WALLSMATERIAL_MODE\"].replace(rare_lb,'RARE')","8c31c1e3":"show_plots(\"WALLSMATERIAL_MODE\", figsize = (15,3))","c588d007":"#One hot encode WALLSMATERIAL_MODE\n\ndf_train = pd.concat([df_train.drop('WALLSMATERIAL_MODE', axis=1), \n                       pd.get_dummies(df_train['WALLSMATERIAL_MODE'], prefix='WALLSMATERIAL_MODE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('WALLSMATERIAL_MODE', axis=1), \n                       pd.get_dummies(df_test['WALLSMATERIAL_MODE'], prefix='WALLSMATERIAL_MODE')], \n                          axis=1)","38aa371e":"show_plots(\"HOUSETYPE_MODE\", figsize=(15, 2))","9c4a5f5f":"vc = dict(df_train[\"HOUSETYPE_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.5]\n\ndf_train[\"HOUSETYPE_MODE\"] = df_train[\"HOUSETYPE_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"HOUSETYPE_MODE\"] = df_test[\"HOUSETYPE_MODE\"].replace(rare_lb,'RARE')","0f8bff96":"show_plots(\"HOUSETYPE_MODE\", figsize=(15, 2))","083b8efc":"#One hot encode HOUSETYPE_MODE\n\ndf_train = pd.concat([df_train.drop('HOUSETYPE_MODE', axis=1), \n                       pd.get_dummies(df_train['HOUSETYPE_MODE'], prefix='HOUSETYPE_MODE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('HOUSETYPE_MODE', axis=1), \n                       pd.get_dummies(df_test['HOUSETYPE_MODE'], prefix='HOUSETYPE_MODE')], \n                          axis=1)","a62c7bbd":"show_plots(\"NAME_INCOME_TYPE\", figsize=(10, 3))","3644f985":"df_train['NAME_INCOME_TYPE'].value_counts()","f9d0c7fa":"df_train[\"NAME_INCOME_TYPE\"] = np.where(df_train[\"NAME_INCOME_TYPE\"]=='Working', 1, 0)\ndf_test[\"NAME_INCOME_TYPE\"] = np.where(df_test[\"NAME_INCOME_TYPE\"]=='Working', 1, 0)","5771eaa5":"show_plots(\"ORGANIZATION_TYPE\", figsize=(20, 15))","0b43cfff":"df_train = df_train.drop(\"ORGANIZATION_TYPE\", axis=1)\ndf_test = df_test.drop(\"ORGANIZATION_TYPE\", axis=1)","b3522008":"show_plots(\"NAME_FAMILY_STATUS\", figsize=(15, 2))","a914dd47":"set(df_train[\"NAME_FAMILY_STATUS\"])^set(df_test[\"NAME_FAMILY_STATUS\"])","139bd976":"# one hot encode NAME_FAMILY_STATUS\n\ndf_train = pd.concat([df_train.drop('NAME_FAMILY_STATUS', axis=1), \n                       pd.get_dummies(df_train['NAME_FAMILY_STATUS'], prefix='NAME_FAMILY_STATUS')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('NAME_FAMILY_STATUS', axis=1), \n                       pd.get_dummies(df_test['NAME_FAMILY_STATUS'], prefix='NAME_FAMILY_STATUS')], \n                          axis=1)","3dd4c69b":"show_plots(\"FONDKAPREMONT_MODE\", figsize=(15, 2))","771d32f6":"vc = dict(df_train[\"FONDKAPREMONT_MODE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.1]\n\ndf_train[\"FONDKAPREMONT_MODE\"] = df_train[\"FONDKAPREMONT_MODE\"].replace(rare_lb,'RARE')\ndf_test[\"FONDKAPREMONT_MODE\"] = df_test[\"FONDKAPREMONT_MODE\"].replace(rare_lb,'RARE')","9fa49b85":"df_train = df_train.drop(\"FONDKAPREMONT_MODE\", axis=1)\ndf_test = df_test.drop(\"FONDKAPREMONT_MODE\", axis=1)","acbaf47b":"show_plots(\"OCCUPATION_TYPE\", figsize=(15, 5))","12f6a924":"df_train['OCCUPATION_TYPE'].value_counts(1)","7cc388d1":"df_train['OCCUPATION_TYPE'] = np.where(df_train['OCCUPATION_TYPE']=='MISSING',0,1)\ndf_test['OCCUPATION_TYPE'] = np.where(df_test['OCCUPATION_TYPE']=='MISSING',0,1)","cf378534":"show_plots(\"WEEKDAY_APPR_PROCESS_START\", figsize=(15, 3))","f3db54fc":"# one hot encode WEEKDAY_APPR_PROCESS_START\n\ndf_train = pd.concat([df_train.drop('WEEKDAY_APPR_PROCESS_START', axis=1), \n                       pd.get_dummies(df_train['WEEKDAY_APPR_PROCESS_START'], prefix='WEEKDAY_APPR_PROCESS_START')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('WEEKDAY_APPR_PROCESS_START', axis=1), \n                       pd.get_dummies(df_test['WEEKDAY_APPR_PROCESS_START'], prefix='WEEKDAY_APPR_PROCESS_START')], \n                          axis=1)","0467ffa1":"show_plots(\"NAME_TYPE_SUITE\", figsize=(15,3))","6de49330":"df_train[\"NAME_TYPE_SUITE\"] = np.where(df_train[\"NAME_TYPE_SUITE\"]=='Unaccompanied', 0, 1) \ndf_test[\"NAME_TYPE_SUITE\"] = np.where(df_test[\"NAME_TYPE_SUITE\"]=='Unaccompanied', 0, 1) ","79074185":"show_plots(\"NAME_HOUSING_TYPE\", figsize=(15,3))","7f089dfb":"set(df_train['NAME_HOUSING_TYPE'])^set(df_train['NAME_HOUSING_TYPE'])","73114260":"vc = dict(df_train[\"NAME_HOUSING_TYPE\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.04]\n\ndf_train = df_train[~df_train[\"NAME_HOUSING_TYPE\"].isin(rare_lb)]\ndf_test = df_test[~df_test[\"NAME_HOUSING_TYPE\"].isin(rare_lb)]\n","faa91b63":"show_plots(\"NAME_HOUSING_TYPE\", figsize=(15,3))","db275a1c":"# one hot encode NAME_HOUSING_TYPE\n\ndf_train = pd.concat([df_train.drop('NAME_HOUSING_TYPE', axis=1), \n                       pd.get_dummies(df_train['NAME_HOUSING_TYPE'], prefix='NAME_HOUSING_TYPE')], \n                          axis=1)\n\ndf_test = pd.concat([df_test.drop('NAME_HOUSING_TYPE', axis=1), \n                       pd.get_dummies(df_test['NAME_HOUSING_TYPE'], prefix='NAME_HOUSING_TYPE')], \n                          axis=1)","76d9b095":"show_plots(\"NAME_EDUCATION_TYPE\", figsize=(15,3))","b455dfb7":"df_train['NAME_EDUCATION_TYPE'].value_counts(1)","fc3fe93f":"set(df_train['NAME_EDUCATION_TYPE'])^set(df_train['NAME_EDUCATION_TYPE'])","6444a24d":"df_train['NAME_EDUCATION_TYPE'].unique()","69608f79":"mapping = {\n'Higher education': 2,\n'Secondary \/ secondary special' : 1,\n'Incomplete higher' : 1,\n'Lower secondary' : 1,\n'Academic degree': 2\n}\n\ndf_train[\"NAME_EDUCATION_TYPE\"] = df_train[\"NAME_EDUCATION_TYPE\"].map(mapping)\ndf_test[\"NAME_EDUCATION_TYPE\"] = df_test[\"NAME_EDUCATION_TYPE\"].map(mapping)\n","1ffbd0d9":"df_train[['NAME_EDUCATION_TYPE','TARGET']].groupby('NAME_EDUCATION_TYPE')['TARGET'].value_counts(1)","bd90e5c6":"cols = list(set(cat_features).intersection(set(df_train.columns.values)))\ncols","0c42e86a":"df_train[binary_features].head()","e407f56b":"for feature in binary_features:\n    show_plots(feature, figsize=(15,2))","c5f82b11":"df_train[\"NAME_CONTRACT_TYPE\"] = df_train[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans':1, 'Revolving loans':0})\ndf_test[\"NAME_CONTRACT_TYPE\"] = df_test[\"NAME_CONTRACT_TYPE\"].replace({'Cash loans':1, 'Revolving loans':0}) \n\ndf_train[\"FLAG_OWN_CAR\"] = df_train[\"FLAG_OWN_CAR\"].replace({'Y':1, 'N':0})\ndf_test[\"FLAG_OWN_CAR\"] = df_test[\"FLAG_OWN_CAR\"].replace({'Y':1, 'N':0})\n\ndf_train[\"FLAG_OWN_REALTY\"] = df_train[\"FLAG_OWN_REALTY\"].replace({'Y':1, 'N':0})\ndf_test[\"FLAG_OWN_REALTY\"] = df_test[\"FLAG_OWN_REALTY\"].replace({'Y':1, 'N':0})\n\ndf_train[\"EMERGENCYSTATE_MODE\"] = df_train[\"EMERGENCYSTATE_MODE\"].replace({'Yes':1, 'No':0})\ndf_test[\"EMERGENCYSTATE_MODE\"] = df_test[\"EMERGENCYSTATE_MODE\"].replace({'Yes':1, 'No':0})","f526f67d":"var = fs.VarianceThreshold(0.02)\nvar.fit(df_train)\n\nnon_informative = df_train.columns[~var.get_support()]\nnon_informative","f05622e0":"df_train[non_informative].hist(bins=30, figsize=(20,20))","559dd37f":"non_informative = list(filter(lambda x: x!='REGION_POPULATION_RELATIVE', non_informative))\n\ndf_train = df_train.drop(non_informative, 1)\ndf_test = df_test.drop(non_informative, 1)","7daed783":"correlation = df_train.corr().abs()\nsns.clustermap(correlation, cmap='coolwarm', \n               vmin=0, vmax=0.8, center=0, \n               square=True, linewidths=.5, \n               figsize=(50,50), yticklabels=1)","b7249163":"corr_mat = correlation.unstack() \ncorr_mat = corr_mat.sort_values(ascending=False)\ncorr_mat = corr_mat[(corr_mat >= 0.8) & (corr_mat < 1)]\ncorr_mat = pd.DataFrame(corr_mat).reset_index()\ncorr_mat.columns = ['f1', 'f2', 'correlation']\ncorr_mat.head()","79adae91":"grouped_feature = set()\ncorrelated_groups = list()\n\nfor feature in corr_mat.f1.unique():\n    if feature not in grouped_feature:\n        correlated_block = corr_mat[corr_mat.f1==feature]\n        grouped_feature = grouped_feature | set(correlated_block.f2) | set(feature)\n        correlated_groups.append(correlated_block)\n        \nprint(f'correlated groups count: {len(correlated_groups)}')","2a6a1973":"for i, group in enumerate(correlated_groups):\n    print(f'\\tgroup {i+1}:\\n{group}\\n\\n')","bf4b4bf0":"importance_groups = list()\n\nfor group in correlated_groups:\n    group_features = list(set(group.f1) | set(group.f2))\n    \n    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4, verbose=0)\n    \n    f_import = rf.fit(df_train[group_features], \n                       df_train['TARGET']).feature_importances_\n    imp_g = pd.DataFrame({'feature': group_features,\n                            'importance': f_import}).sort_values(by='importance', ascending=False)\n    importance_groups.append(imp_g)\n    ","76e34afc":"print(f'total importance groups: {len(importance_groups)}\\n\\n')\nfor i, g in enumerate(importance_groups):\n    print(f'img_g: {i}\\n {g}\\n\\n')","30f2eac5":"not_imp_features = set()\n\nfor g in importance_groups:\n    not_imp_features |= set(g.feature[1:])","1648a37b":"df_train = df_train.drop(columns=list(not_imp_features)).reset_index()\ndf_test = df_test.drop(columns=list(not_imp_features)).reset_index()","b15d1fb0":"updated_numericals = list(set(num_features).intersection(set(df_train.columns.values)))","d596db9c":"df_train[updated_numericals].hist(figsize=(20,20), bins=50)","4f27ec87":"updated_numericals.remove('CNT_CHILDREN')\nupdated_numericals.remove('REGION_RATING_CLIENT_W_CITY')","f3c8a9ab":"ss = pp.StandardScaler()\nss.fit(df_train[updated_numericals])\n\ndf_train[updated_numericals] = ss.transform(df_train[updated_numericals])\ndf_test[updated_numericals] = ss.transform(df_test[updated_numericals])","0a741b40":"show_plots(\"CNT_CHILDREN\", figsize=(15,3))","af846d4b":"vc = dict(df_train[\"CNT_CHILDREN\"].value_counts(normalize=True))\nrare_lb = [k for k,v in vc.items() if v<=0.05]\n\ndf_train = df_train[~df_train[\"CNT_CHILDREN\"].isin(rare_lb)]\ndf_test = df_test[~df_test[\"CNT_CHILDREN\"].isin(rare_lb)]\n","3d744039":"show_plots(\"CNT_CHILDREN\", figsize=(15,3))","11afeb88":"df_train[['CNT_CHILDREN', 'TARGET']].groupby('CNT_CHILDREN')['TARGET'].mean()","37c8a3df":"oh = pp.OneHotEncoder(handle_unknown='ignore')\noh.fit(df_train[['CNT_CHILDREN']])\n\ndf_train = pd.concat([df_train.drop('CNT_CHILDREN', axis=1).reset_index(drop=True), \n                       pd.DataFrame( oh.transform(df_train['CNT_CHILDREN'].values.reshape(-1,1)).toarray(), \n                                        columns=oh.get_feature_names(['CNT_CHILDREN']) \n                                    )  \n                      ], \n                    axis=1).drop('index', axis=1)\n\ndf_test = pd.concat([df_test.drop('CNT_CHILDREN', axis=1).reset_index(drop=True), \n                       pd.DataFrame( oh.transform(df_test['CNT_CHILDREN'].values.reshape(-1,1)).toarray(), \n                                        columns=oh.get_feature_names(['CNT_CHILDREN']) \n                                    )  \n                      ], \n                    axis=1).drop('index', axis=1)","cf348702":"show_plots(\"REGION_RATING_CLIENT_W_CITY\", figsize=(15,3))","65e0f681":"df_train['REGION_RATING_CLIENT_W_CITY'].value_counts(1)","81614a4c":"df_train[['REGION_RATING_CLIENT_W_CITY', 'TARGET']].groupby('REGION_RATING_CLIENT_W_CITY')['TARGET'].mean()","0112fb54":"df_train.head()","989a1dba":"df_sample = df_train.sample(frac=0.1,replace=False, random_state=1)\n\nX_f = df_sample[[col for col in df_sample.columns if col not in ['TARGET', 'SK_ID_CURR']]]\ny_f = df_sample['TARGET']\n\nX_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_f, y_f, test_size=0.3, random_state=0) ","03fba424":"sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1',solver='liblinear'))\nsel_.fit(X_f_train, y_f_train)","17cde047":"not_select_feat = X_f_train.columns[~(sel_.get_support())]\nnot_select_feat","dee8deae":"df_train = df_train.drop('FLAG_WORK_PHONE', axis=1)\ndf_test = df_test.drop('FLAG_WORK_PHONE', axis=1)","12bb7754":"AUC_values = list()\n\nfor feature in X_f.columns:\n    clf = DecisionTreeClassifier()\n    clf.fit(X_f_train[[feature]], y_f_train)\n    pred = clf.predict_proba(X_f_test[[feature]])\n    score = roc_auc_score(y_f_test,pred[:,1])\n    AUC_values.append(score)","91bb7b6c":"AUC_df = pd.Series(AUC_values)\nAUC_df.index = X_f_train.columns\nAUC_df = AUC_df.sort_values(ascending=False)","29e581f3":"sns.distplot(AUC_values)","bcfa56c2":"fig, ax = plt.subplots(figsize=(10,20))\nsns.barplot(y=AUC_df.index, x=AUC_df)","60376aa4":"informative_features = list(AUC_df[AUC_df>0.5].index)","604d7472":"rf_model = RandomForestClassifier()\nrf_model.fit(X_f, y_f)\n\nfv = dict(zip(X_f.columns,rf_model.feature_importances_))\nfv_dict = {k: v for k, v in sorted(fv.items(), key=lambda item: item[1], reverse=True)}","26a224b7":"fig, ax = plt.subplots(figsize=(10,20))\nsns.barplot(y=list(fv_dict.keys()), x=list(fv_dict.values()))","6095b9b5":"important_features = [k for k,v in fv_dict.items() if v>0.01]","f9287503":"kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nmetrics = ['neg_log_loss', 'accuracy', 'f1', 'precision', 'roc_auc']","985fac13":"informative_and_important_features = list(set(informative_features).union(set(important_features)))\nall_features = [col for col in df_train.columns if col not in ['SK_ID_CURR','TARGET']]","0f495fc4":"results = dict()\n\nfor set_name, feature_set in dict(zip(['', 'important_features', 'informative_features', 'informative_and_important_features'], \n                                      [all_features, important_features, informative_features, informative_and_important_features])).items():\n    \n    for model_name, model in dict(zip(['XGBClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier'], \n                               [XGBClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier])).items():\n\n        validator = cross_validate(model(), df_train[feature_set], df_train['TARGET'], \n                                   cv=kfold, scoring=metrics, return_train_score=True,\n                                   n_jobs=-1, verbose=10)\n        results[f'{model_name}_{set_name}'] = validator\n        print(f'{model_name}_{set_name}')","801e657a":"d = pd.DataFrame(results.values())\nd.index = results.keys()\nd = d.applymap(np.mean)\nd = d.sort_values(by='test_roc_auc',ascending=False)\nd['diff'] = (d['train_roc_auc'] - d['test_roc_auc']) *100\nd\n","e776cf8e":"parameters = { \n                'n_estimators' : range(50, 150, 50),\n                'max_depth' : range(4, 5, 1),\n                'min_samples_split': range(200,400,200),\n                \"reg_alpha\": [1.5, 2, 2.5],\n                \"reg_lambda\": [3.5, 4, 4.5]\n             }\n\n\ngrid_search = GridSearchCV(\n            XGBClassifier(tree_method='gpu_hist'),\n            parameters,\n            scoring=['roc_auc', 'neg_log_loss'],\n            refit='roc_auc',\n            n_jobs=-1, \n            cv = kfold, \n            verbose=10,\n            return_train_score = True \n) \n\ngrid_search.fit(df_train[all_features], df_train['TARGET'])\n","708c167b":"print(grid_search.best_score_)\nprint(grid_search.best_params_)","d114a88a":"train = grid_search.cv_results_['mean_train_roc_auc']\ntest = grid_search.cv_results_['mean_test_roc_auc']\nparams = grid_search.cv_results_['params']\n\nsns.lineplot(y=test, x=list(range(0,len(test),1)))\nsns.lineplot(y=train, x=list(range(0,len(train),1)))","71c3018a":"for train, test,  param in zip(train, test, params):\n    print(\"test: %f train: %f with: %r\" % (test, train, param))","c15f98d6":"print(\"mean_train_roc_auc -----------------{}\".format( np.mean(grid_search.cv_results_['mean_train_roc_auc']) ))\nprint(\"mean_test_roc_auc ------------------{}\".format( np.mean(grid_search.cv_results_['mean_test_roc_auc']) ))\n\nprint(\"mean_train_neg_log_loss ------------{}\".format( np.mean(grid_search.cv_results_['mean_train_neg_log_loss'])))\nprint(\"mean_test_neg_log_loss -------------{}\".format( np.mean(grid_search.cv_results_['mean_test_neg_log_loss'])))","6515d2df":"sns.distplot(grid_search.cv_results_['mean_test_roc_auc'])\nsns.distplot(grid_search.cv_results_['mean_train_roc_auc'])","21c65705":"best_model = XGBClassifier(**grid_search.best_params_)\n\nbest_model.fit(df_train[all_features], df_train['TARGET'])\n\ny_predict = best_model.predict_proba( df_test[all_features] )","9c479edc":"df_results = pd.DataFrame({'true':df_test['TARGET'],\n                            'predict': y_predict[:,1]})","633a847e":"roc_auc_score(df_results['true'], df_results['predict'])","4f15dc33":"sns.distplot( df_results.predict[df_results.true==1] )\nsns.distplot( df_results.predict[df_results.true==0] )","406351d2":"print( confusion_matrix(df_results.true, \n                        np.where(df_results.predict>0.5, 1, 0)\n                       ) \n     )","6a5c57b5":"df_results.true.value_counts()","4cc02e71":"print( classification_report( df_results.true, \n                        np.where(df_results.predict>0.5, 1, 0)\n                       ) \n     )","7f5936c5":"print( log_loss( df_results.true, df_results.predict) )","52e65895":"df_test['TARGET'].value_counts(1)","70e25559":"# Feature importancy","944a1473":"# Numerical features","0d21ca9d":"# Informative features","2690dc87":"The results above do indicate a difference. The question is whether this difference relevant to default or is due to random chance. To answer this question I am using the chi_squared statistical hypothesis test. If the p values is less than 0.03, then it means that missing housing information is in fact informative in determining default.","fdc586bc":"# Identify feature types","752d9429":"Missingness in the features above features do show meaning in predicting defualt. Therefore I am going to replace missing values with an extreme in order to single these cases out.","bf27d3f5":"There's an interesting pattern I've noticed from the list above. That is, most of the missing features seem to be household related. They could be purposely missing due to aplicants' living situtions (ex living with parents). Let's further investigate with a nullity correlation dendogram.","e4e5dcac":"ORGANIZATION_TYPE has very high cardinality. Therefore I will simply drop this feature.","e09bc694":"From the above we can see that there're lots of features with more than 20% of missing data. These are best to be removed, but first, let's investigate if these features are Missing Completly at Random (MCAR).","48a8467a":"For features with less than 1% missing values, it is safe to simply drop missing values","fb3ffe3e":"# Feature Correlation","938b8752":"Given the high percentage of missing values in FONDKAPREMONT_MODE and also the fact that I am not sure what it means and I cannot find much information on the topic, I am going to simply remove this feature.","6d666278":"# Hyperparameter tuning","7634fecb":"# Feature engineering","02a16b26":"# Trying out multiple models","8a03fed7":"OCCUPATION_TYPE has high cardinality and a high percentage of missing values, therefore I will drop this feature.","6f6adcbe":"Most of the variables in WALLSMATERIAL_MODE are rare labels, which leads to model overfitment especially in tree based algorithms where variables with lots of labels dominate the ones with fewer labels. Therefore I merge all rare labels into one group titled \"Rare\".","9595cdab":"Looking at the Dumb Logloss curve from below, the model's logloss slightly falls under the curve.\nTherefore, this model is informative.\n\n![](https:\/\/i.stack.imgur.com\/54KwE.png)","fd62a45a":"Data Skeweness","8fedc164":"## Categorical Features","23fb1d22":"#### Numerical features","16a7533c":"# Selecting features using Lasso Regularization","12ce77c6":"OCCUPATION_TYPE does not have an \"Unemployed\" option it could be that the missing values indicate unemployment. I am labeling all missing values with \"missing\" to signal this observation.","ede48da0":"The above demonstrates that missing housing data does have an effect over the outcome to be predicted (default).\nSince I am planing on using a desission tree based model, I will be replacing numerical housing values with an extreme value(9999). This way, these data points will be better sepparated in their own group, adding meaning to the model.","2e2762ef":"Let's see if there's any difference between aplicants that have missing housing information and the ones that do not.","08b8cf4f":"The test above confirms that AMT_REQ_CREDIT_BUREAU does influence outcome of default. To represent this in the model, I replace the missing values with an extreme value.","728c4684":"Checking if there's statistical significance when the remaining columns are missing, EXT_SOURCE_1, EXT_SOURCE_3 and OWN_CAR_AGE.","c37bf1f7":"AMT_REQ_CREDIT_BUREAU series show the Number of enquiries to Credit Bureau about the client at a certain amount of time unit before application. Missing values in this case can have a predictive power over target. I am using the chi-square test to find out if thsi is the case.","3c370593":"# Binary Features","103061e3":"We observe that lots of the features are constant, therefore I am going to remove them as they do not add any information to the model.","d48d84c2":"## Missing data","c9771b85":"# Data Analysis"}}