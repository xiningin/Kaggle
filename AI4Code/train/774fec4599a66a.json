{"cell_type":{"b7f35ecf":"code","51fe1134":"code","cb48e0b3":"code","48e0a989":"code","b00ec21f":"code","1d5d1650":"code","9e5e6a8e":"code","ef1b0b58":"code","f71fd83c":"code","d103a880":"code","04e90a4b":"code","322ac166":"code","a5654f6f":"code","578be3ce":"code","3db564f0":"code","24dd917e":"code","4bb52d8a":"code","0fd649a2":"code","c0f48971":"code","2414e0dc":"code","7a268fe3":"code","f6fbb214":"code","a13d2bc3":"code","ec76eee8":"code","1bcbee69":"code","cae6d980":"code","9372ed31":"code","281a9368":"code","68ca014a":"code","6957de1d":"code","aed0fa14":"code","fb1c475b":"code","3fa82b6e":"code","ea18e027":"code","1628472a":"code","6ecc2544":"code","a492ed0e":"markdown","8a9108ac":"markdown","817c69d5":"markdown","90bed1f6":"markdown","17737f17":"markdown","2bd5cce5":"markdown","9b9e8830":"markdown","e17693bb":"markdown","63001a78":"markdown","f0e2bea3":"markdown","e5153723":"markdown","2081b065":"markdown","2036515b":"markdown","36ecefd6":"markdown","c5192475":"markdown","1bf97b22":"markdown","35a63abc":"markdown","b2d3c177":"markdown","da7f539b":"markdown","87b65e5b":"markdown","013556f2":"markdown","cb690706":"markdown","44b526cb":"markdown","6639dc29":"markdown","bd48dd9b":"markdown","0181b6d4":"markdown","0997bb7d":"markdown","92a3a33d":"markdown","4b2cc094":"markdown","27cb89f3":"markdown","244b4ca8":"markdown","50331e27":"markdown","a223e093":"markdown","f768a450":"markdown","aab1d18f":"markdown","843dbda1":"markdown","b8609e75":"markdown"},"source":{"b7f35ecf":"# to supress some warnings (they are as important as they are annoying)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# data structure and analysis\nimport pandas as pd\nimport numpy as np\n\n# scikit-learn realated imports\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\n# we are going to use it to verify the data skewness\nfrom scipy.stats import skew\n\n# to analyse the data graphically \nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nplt.style.use('ggplot')\nimport seaborn as sns\n\n# regressors\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR","51fe1134":"train_data = pd.read_csv('..\/input\/train.csv', index_col='Id')\ntest_data = pd.read_csv('..\/input\/test.csv', index_col='Id')\nprint(f'Train set shape: {train_data.shape}\\nTest set shape: {test_data.shape}')","cb48e0b3":"train_data.head(10)","48e0a989":"train_data = train_data.drop((train_data[train_data['GrLivArea']>4000]).index)\nprint(f'Train data shape after droping outliers: {train_data.shape}')","b00ec21f":"y = train_data['SalePrice']\nX = train_data.drop(['SalePrice'], axis=1)\n\n# it's good to be sure that we don't have any missing target.\nprint('Presence of missing target: {0}'.format(y.isnull().any()))","1d5d1650":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2)\nprint(f'Train test shape: {X_train_full.shape}\\nValidation test shape:{X_valid_full.shape}')\n\n# unite train and test sets on helper dataframe\nfull_data = pd.concat([X_train_full, X_valid_full, test_data])\n\n# we are also taking the indexes of the train, validation and test sets, in case we need it latter\ntrain_index = X_train_full.index\nvalid_index = X_valid_full.index\ntest_index = test_data.index\n\n# get the name of columns with missing values\nmissing = [col for col in full_data.columns if full_data[col].isnull().any()]\nprint(f'This are the {len(missing)} features presenting missing values:\\n{missing}')","9e5e6a8e":"print(f'Different types: {X_train_full.dtypes.unique()}')\ncategoricals = X_train_full.select_dtypes(include='object').columns\nnumericals = X_train_full.select_dtypes(exclude='object').columns\nprint(f'There are {len(categoricals)} categorical features and {len(numericals)} numerical ones.')","ef1b0b58":"cat_missing = list(set(categoricals).intersection(set(missing)))\nprint(f'Categorical missing data:\\n{cat_missing}')","f71fd83c":"temp = pd.concat([X_train_full[cat_missing],test_data[cat_missing]])\nprint(f'Number of missing values for each feature:\\n{temp.isnull().sum().sort_values()}')\n\n# Observe that we are not using temp here since it's contaminated with test set info.\nX_train_full[categoricals].describe()","d103a880":"# step one\ndef impute_categoricals(data):\n    columns = [\n        'GarageCond', 'GarageType', 'GarageQual', 'GarageFinish',\n        'Fence', 'PoolQC', 'Alley', 'FireplaceQu', 'MiscFeature',\n        'BsmtFinType1', 'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'BsmtExposure'\n    ]\n    data[columns] = data[columns].fillna(\"Absent\")","04e90a4b":"# step two\ndef impute_categoricals(data):\n    columns = [\n        'GarageCond', 'GarageType', 'GarageQual', 'GarageFinish',\n        'Fence', 'PoolQC', 'Alley', 'FireplaceQu', 'MiscFeature',\n        'BsmtFinType1', 'BsmtCond', 'BsmtFinType2', 'BsmtQual', 'BsmtExposure'\n    ]\n    data[columns] = data[columns].fillna(\"Absent\")\n    \n    data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","322ac166":"# step three\ndef impute_categoricals(data):\n    columns = [\n        'GarageCond', 'GarageType', 'GarageQual', 'GarageFinish',\n        'Fence', 'PoolQC', 'Alley', 'FireplaceQu', 'MiscFeature',\n        'BsmtFinType1', 'BsmtCond', 'BsmtFinType2', 'BsmtQual','BsmtExposure'\n    ]\n    \n    data[columns] = data[columns].fillna(\"Absent\")\n    \n    data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    data[['Exterior1st', 'Exterior2nd']] = data[['Exterior1st', 'Exterior2nd']].fillna(\"VinylSd\")","a5654f6f":"X_train_full.MasVnrArea[X_train_full.MasVnrType.isnull()]","578be3ce":"# step four\ndef impute_categoricals(data):\n    columns = [\n        'GarageCond', 'GarageType', 'GarageQual', 'GarageFinish',\n        'Fence', 'PoolQC', 'Alley', 'FireplaceQu', 'MiscFeature',\n        'BsmtFinType1', 'BsmtCond', 'BsmtFinType2', 'BsmtQual','BsmtExposure'\n    ]\n    data[columns] = data[columns].fillna(\"Absent\")\n    \n    data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    data[['Exterior1st', 'Exterior2nd']] = data[['Exterior1st', 'Exterior2nd']].fillna('VinylSd')\n    \n    data['MasVnrType'] = data['MasVnrType'].fillna('None')","3db564f0":"# step five\ndef impute_categoricals(data):\n    data = data.copy()\n    \n    columns_absent = [\n        'GarageCond', 'GarageType', 'GarageQual', 'GarageFinish',\n        'Fence', 'PoolQC', 'Alley', 'FireplaceQu', 'MiscFeature',\n        'BsmtFinType1', 'BsmtCond', 'BsmtFinType2', 'BsmtQual','BsmtExposure'\n    ]\n    data[columns_absent] = data[columns_absent].fillna(\"Absent\")\n    \n    data['MSZoning'] = data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    data[['Exterior1st', 'Exterior2nd']] = data[['Exterior1st', 'Exterior2nd']].fillna('VinylSd')\n    \n    data['MasVnrType'] = data['MasVnrType'].fillna('None')\n    \n    columns_mf = ['Utilities', 'KitchenQual', 'Electrical', 'SaleType', 'Functional']\n    \n    for col in columns_mf:\n        data[col] = data[col].fillna(data[col].mode()[0])\n        \n    return data","24dd917e":"temp = full_data.copy()\nprint(f'Number of categorical columns with missing data: {temp[categoricals].isnull().any().sum()}')\ntemp = impute_categoricals(temp)\nprint(f'Number of categorical columns with missing data after imputation: {temp[categoricals].isnull().any().sum()}')","4bb52d8a":"num_missing = list(set(numericals).intersection(set(missing)))\nprint(f'Numerical missing data:\\n{num_missing}\\n')\n\ntemp = full_data[num_missing].copy()\nprint(f'Number of missing values for each feature:\\n{temp.isnull().sum().sort_values()}')\n\n# Observe that we are not using temp here since it's contaminated with test set info.\nX_train_full[numericals].describe()","0fd649a2":"correlation = pd.concat([y_train,X_train_full], axis=1).corr()\nplt.figure(figsize = (40,20))\nsns.heatmap(data=correlation, vmax=.8, square=True, annot=True)","c0f48971":"fig = sns.regplot(x='1stFlrSF', y='LotFrontage', data=X_train_full)\n\n# get two line points\nx0 = fig.get_lines()[0].get_data()[0][0]\ny0 = fig.get_lines()[0].get_data()[1][0]\nx1 = fig.get_lines()[0].get_data()[0][1]\ny1 = fig.get_lines()[0].get_data()[1][1]\n\n# calculate the line formula\nm = (y1-y0)\/(x1-x0)\n# lotfrontage = m*(1stFlrSf - x1) + y1","2414e0dc":"plt.figure(figsize = (40,20))\nsns.scatterplot(x='Neighborhood', y='LotFrontage', hue='1stFlrSF', data=X_train_full)","7a268fe3":"# step one\ndef impute_numericals(data):\n    data = data.copy()\n    \n    temp = data.groupby('Neighborhood').apply(\n        lambda x: x['LotFrontage'].fillna(m*(x['1stFlrSF'] - x1) + y1)).reset_index()\n    temp = temp.set_index(temp.Id)\n    data['LotFrontage'] = temp.drop(['Neighborhood', 'Id'], axis=1)\n    \n    return data","f6fbb214":"# step two\ndef impute_numericals(data):\n    data = data.copy()\n    \n    temp = data.groupby('Neighborhood').apply(\n        lambda x: x['LotFrontage'].fillna(m*(x['1stFlrSF'] - x1) + y1)).reset_index()\n    temp = temp.set_index(temp.Id)\n    data['LotFrontage'] = temp.drop(['Neighborhood', 'Id'], axis=1)\n    \n    data['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['YearBuilt'])\n    \n    return data","a13d2bc3":"# step three\ndef impute_numericals(data):\n    data = data.copy()\n    \n    temp = data.groupby('Neighborhood').apply(\n        lambda x: x['LotFrontage'].fillna(m*(x['1stFlrSF'] - x1) + y1)).reset_index()\n    temp = temp.set_index(temp.Id)\n    data['LotFrontage'] = temp.drop(['Neighborhood', 'Id'], axis=1)\n    \n    data['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['YearBuilt'])\n        \n    data['MasVnrArea'] = data['MasVnrArea'].fillna(0)\n    \n    return data","ec76eee8":"# step four\ndef impute_numericals(data):\n    data = data.copy()\n    \n    temp = data.groupby('Neighborhood').apply(\n        lambda x: x['LotFrontage'].fillna(m*(x['1stFlrSF'] - x1) + y1)).reset_index()\n    temp = temp.set_index(temp.Id)\n    data['LotFrontage'] = temp.drop(['Neighborhood', 'Id'], axis=1)\n    \n    data['GarageYrBlt'] = data['GarageYrBlt'].fillna(data['YearBuilt'])\n        \n    columns_zero = ['BsmtFullBath', 'BsmtHalfBath', 'GarageCars', 'BsmtFinSF2',\n                    'BsmtUnfSF', 'BsmtFinSF1', 'GarageArea', 'TotalBsmtSF', 'MasVnrArea']\n    data[columns_zero] = data[columns_zero].fillna(0)\n    \n    return data","1bcbee69":"print(f'Number of numerical columns with missing data: {full_data[numericals].isnull().any().sum()}')\ntemp = impute_numericals(full_data)\n\nprint(f'Number of numerical columns with missing data after imputation: {temp[numericals].isnull().any().sum()}')","cae6d980":"def fix_numericals(data):\n    data = data.copy()\n    \n    temp = pd.concat([X_train_full,X_valid_full,test_data]).copy()\n    \n    values_mssubclass = []\n    for i in temp.MSSubClass.unique():\n        values_mssubclass.append(\"SC\"+str(i))\n    \n    values_yearbuilt = []\n    for i in temp.YearBuilt.unique():\n        values_yearbuilt.append(\"Y\"+str(i))\n    \n    values_yearremodadd = []\n    for i in temp.YearRemodAdd.unique():\n        values_yearremodadd.append(\"Y\"+str(i))\n    \n    values_mssubclass = dict(zip(temp.MSSubClass.unique(), values_mssubclass))\n    values_yearbuilt = dict(zip(temp.YearBuilt.unique(), values_yearbuilt))\n    values_yearremodadd = dict(zip(temp.YearRemodAdd.unique(), values_yearremodadd))\n    \n    data = data.replace(\n        {\n            'MSSubClass' : values_mssubclass,\n            'MoSold' : {1 : 'Jan', 2 : 'Feb', 3 : 'Mar', 4 : 'Apr', 5 : 'May', 6 : 'Jun',\n                        7 : 'Jul', 8 : 'Aug', 9 : 'Sep', 10 : 'Oct', 11 : 'Nov', 12 : 'Dec'},\n            'YrSold': {2006: 'Y2006', 2007: 'Y2007', 2008: 'Y2008', 2009: 'Y2009', 2010:'Y2010'},\n            'YearBuilt': values_yearbuilt,\n            'YearRemodAdd': values_yearremodadd,\n        }\n    )\n\n    return data","9372ed31":"temp = pd.concat([X_train_full[categoricals], y_train], axis=1)\n\ng = sns.PairGrid(temp, y_vars=\"SalePrice\",\n                 x_vars=categoricals,\n                 height=5, aspect=.5)\n\n# Draw a seaborn pointplot onto each Axes\ng.map(sns.pointplot, scale=1.3, errwidth=4, color=\"xkcd:plum\")","281a9368":"le = ['LandContour',  'MSZoning',  'Alley',\n      'MasVnrType',  'ExterQual',  'ExterCond',\n      'BsmtQual',  'BsmtCond',  'BsmtExposure',\n      'BsmtFinType1',  'BsmtFinType2', 'HeatingQC',\n      'CentralAir',  'KitchenQual',  'FireplaceQu',\n      'GarageFinish',  'GarageQual',  'GarageCond',\n      'PavedDrive',  'PoolQC',  'MiscFeature', 'Utilities']\n\nohe = ['LotShape', 'LotConfig', 'LandSlope', 'Neighborhood',\n       'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n       'Foundation', 'Heating', 'Electrical', 'Functional',\n       'GarageType', 'GarageCond', 'Fence', 'SaleType',\n       'SaleCondition', 'Street']","68ca014a":"def correct_skew(data):\n    data = data.copy()\n    skewed_features = data[data.select_dtypes(exclude='object').columns].apply(lambda x: skew(x))\n    skewed_index = skewed_features[abs(skewed_features) > 0.5].index\n    data[skewed_index] = np.log1p(data[skewed_index])\n    \n    return data","6957de1d":"def pre_process():\n    data = full_data.copy()\n    \n    # filling the missing values\n    data = impute_numericals(data)\n    data = impute_categoricals(data)\n    \n    # label encoding\n    encoder = LabelEncoder()\n    for col in le:\n        data[col] = encoder.fit_transform(data[col].values)\n    \n    # one-hot encoding\n    temp = pd.get_dummies(data[ohe], drop_first=True)\n    data = data.drop(ohe, axis=1)\n    data = pd.concat([data, temp], axis=1)\n    \n    # normalizing the variables distribution\n    data = correct_skew(data)\n    \n    # remember the indexes we created a long time ago?\n    new_train = data.loc[train_index]\n    new_valid = data.loc[valid_index]\n    new_test = data.loc[test_index]\n    \n    # now we are going to scale the sets\n    scaler = StandardScaler()\n    new_train[numericals] = scaler.fit_transform(new_train[numericals])\n    new_valid[numericals] = scaler.transform(new_valid[numericals])\n    new_test[numericals] = scaler.transform(new_test[numericals])\n        \n    # let's also correct the skewness on our target to ensure errors will be normally distributed    \n    new_y_train = correct_skew(pd.DataFrame(y_train))\n    new_y_valid = correct_skew(pd.DataFrame(y_valid))\n    \n    return (new_train, new_valid, new_test, new_y_train, new_y_valid)","aed0fa14":"new_train, new_valid, new_test, new_y_train, new_y_valid = pre_process()","fb1c475b":"# We won't use this, it would be used to search for a good tune for each model.\n# A good module to do so is the `mlens` (http:\/\/ml-ensemble.com\/info\/index.html) which I recomend.\n#\n# gbr_params = {'n_estimators': [3000, 4000, 5000],\n#              'learning_rate': [0.04, 0.05, 0.06],\n#              'max_depth':[4, 5, 6],\n#              'max_features': ['sqrt'],\n#              'min_samples_leaf': [14, 15, 16],\n#              'min_samples_split': [8, 10, 12],\n#              'loss': ['huber'],\n#              'random_state': [42]\n#             }\n#\n# svr_params = {'C': [18, 20, 22],\n#              'epsilon': [0.006, 0.008, 0.01],\n#              'gamma': [0.0002, 0.0003, 0.0004]\n#             }\n#                                       \n# lgbm_params = {'objective': ['regression'],\n#               'num_leaves': [3, 4, 5],\n#               'learning_rate': [0.01, 0.02, 0.03],\n#               'n_estimators: [5000],\n#               'max_bin': [150, 200, 250],\n#               'bagging_fraction': [0.6, 0.75, 0.9],\n#               'bagging_freq': [4, 5, 6],\n#               'bagging_seed': [6, 7, 8],\n#               'feature_fraction': [0.1, 0.2, 0.3],\n#               'feature_fraction_seed': [6, 7, 8],\n#               'verbose' = [-1]\n#              }\n#\n# xgbt_params = {'learning_rate': [0.01, 0.02, 0.03],\n#               'n_estimators': [3000, 4000, 5000],\n#               'max_depth': [3, 4, 5],\n#               'min_child_weight': [0, 1, 2, 3],\n#               'gamma': [0],\n#               'subsample': [0.6, 0.7, 0.8],\n#               'colsample_bytree': [0.6, 0.7, 0.8],\n#               'objective': ['reg:squarederror'],\n#               'nthread': [-1],\n#               'scale_pos_weight': [1],\n#               'seed': [42],\n#               'reg_alpha': [0.00005, 0.00006, 0.00007],\n#              }","3fa82b6e":"svr = make_pipeline(RobustScaler(), SVR(C= 24, epsilon= 0.008, gamma=0.0003))\n\nlgb = LGBMRegressor(\n    objective='regression', num_leaves=4, learning_rate=0.01,\n    n_estimators=5542, max_bin=250, bagging_fraction=0.75,\n    bagging_freq=5, bagging_seed=7, feature_fraction=0.3,\n    feature_fraction_seed=7, verbose=-1)\n\ngbr = GradientBoostingRegressor(n_estimators=5542, learning_rate=0.01,\n                                max_depth=5, max_features='sqrt',\n                                min_samples_leaf=15, min_samples_split=10,\n                                loss='huber', random_state =42)\n\nxgb = XGBRegressor(learning_rate=0.01, n_estimators=5542,\n                       max_depth=4, min_child_weight=4,\n                       gamma=0.0003, subsample=0.6,\n                       colsample_bytree=0.5,\n                       objective='reg:squarederror', nthread=-1,\n                       scale_pos_weight=2, seed=42,\n                       reg_alpha=0.00008)","ea18e027":"def fit_models(X, y):\n    svr_model = svr.fit(X, y)\n    gbr_model = gbr.fit(X, y)\n    xgb_model = xgb.fit(X, y)\n    lgb_model = lgb.fit(X, y)\n\n    models = [svr_model, lgb_model, gbr_model, xgb_model]\n    return models\n\ndef evaluate_models(models, X, y=None):\n    predictions = []\n    for model in models:\n        pred = model.predict(X)\n        predictions.append(pred)\n        if y is not None:\n            print(f':MAE SCORE:{mean_absolute_error(np.expm1(pred), np.expm1(y))}')\n    return predictions\n\ndef combine_results(predictions):\n    final_prediction = (0.3 * predictions[0]) + (0.3 * predictions[1])\\\n                     + (0.3 * predictions[2]) + (0.1 * predictions[3])\n    return final_prediction","1628472a":"models = fit_models(new_train, new_y_train)\npredictions = combine_results(evaluate_models(models, new_valid, new_y_valid))\nprint(f'MAE SCORE: {mean_absolute_error(np.expm1(predictions), np.expm1(new_y_valid))}')","6ecc2544":"X_final = pd.concat([new_train, new_valid])\ny_final = pd.concat([new_y_train, new_y_valid])\nmodels = fit_models(X_final, y_final)\nprediction = combine_results(evaluate_models(models, new_test))\nprediction_final = np.expm1(prediction)\nsubmission = pd.DataFrame({'Id':new_test.index,'SalePrice':prediction_final})\nsubmission.to_csv('submission.csv', index=False)","a492ed0e":"As I suspected! I think the `NaN` in both features means there is no masonry in the house. So we will imput `None` for this feature here; and when treating the numericals, we'll impute zero for `MasVnrArea`.","8a9108ac":"---\n\n## Final Considerations\n\nI believe there are still several optimizations to be made here. To enumarete some, we have:\n- some categories only show two levels, and the samples are almost all in the same one! This features aren't much useful and could be droped;\n- we could analyse the relationship between the `SalePrice` and some variables that high correlate with it. By plotting this relationships we could have an idea if adding polynomials could help;\n- we could use crossvalidation to get better estimations for our model;\n- model stacking can probably get us better results.\n\nI hope you like this solution, and please let me know if I can clarify any part of it.\n\nMy best regards, and let's keep learning from data.","817c69d5":"- `GarageYrBlt`: year garage was built.\n\nWell, we have seen in the correlation heatmap that this variable is highly correlated to `YearBuilt`, and I think it's ok to guess that the garage was built in the same year as the house. So let's impute the same value of the `YearBuilt` for it!","90bed1f6":"We are going to work with four algorithms, that are quite popular on regression problems, they are: `Light Gradient Boosting`, `Gradient Boosting`, `Extreme Gradient Boosting` and `Epsilon-Support Vector Regression`.\n\nI particularly chose this four because I'm used to them, and they perform well on the data I work everyday.\n\n![](http:\/\/)The normal approach I would take is to use some tool to gridsearch the best parameters for each model. But this requires time and machinery (depending on the granularity, it can take several days or weeks). So what I'm going to do is to manually adjust the parameters, starting from some already tuned models I have.","17737f17":"Hmm... not so good.\n\nLet's try this: le'ts correlate the 1sfFlrSF and the Neighborhood with LotFrontage. Maybe, in a given Neighborhood, for some size of houses, there is a pattern in the lot frontage!","2bd5cce5":"The columns `BsmtFullBath`, `BsmtHalfBath`, `FullBath`, `HalfBath`, `Bedroom`, `Kitchen`, `TotRmsAbvGrd`, `Fireplaces` and `GarageCars` represent categories; but as they are already in a way that ressembles a label encoding (they are in fact ordinal), we won't mess with them.\n\nLet's pass through the categoricals to decide which ones are ordinal (and we will label encode) and which ones are not (and will be one-hot encoded).","9b9e8830":"By analysing the graphic above and the `problem description`, I devided the categorical features in two groups.\n\nIn the first one are the features \n(`LotShape`, `LotConfig`, `LandSlope`, `Neighborhood`, `Condition1`, `Condition2`, `BldgType`, `HouseStyle`, `RoofStyle`, `RoofMatl`, `Exterior1st`, `Exterior2nd`, `Foundation`, `Heating`, `Electrical`, `Functional`, `GarageType`, `GarageCond`, `Fence`, `SaleType`, `SaleCondition`, `Street`, `Utilities`)\nthat I believe don't have an order (they represent values that cannot be compared in \"better or worse\").\n\nIn the second are the features (`LandContour`, `MSZoning`, `Alley`, `MasVnrType`, `ExterQual`, `ExterCond`, `BsmtQual`, `BsmtCond`, `BsmtExposure`, `BsmtFinType1`, `BsmtFinType2`, `HeatingQC`, `CentralAir`, `KitchenQual`, `FireplaceQu`, `GarageFinish`, `GarageQual`, `GarageCond`, `PavedDrive`, `PoolQC`, `MiscFeature`) that we can compare, that are ordinals.\n\nThe `author's tip` told us the dataset was expected to have\n> 23 nominal, 23 ordinal, 14 discrete, and 20 continuous.\n\nSo I think we are not far from it!\n\nLet's save this groups now. We will leave the actual encoding for the final step, to simplify the process.","e17693bb":"## Treating missing values\n\nOne of the most important things when creating a model is to avoid data leakage. \n\nThe strategy we are going to use is: list all features (from the train and test sets) with missing values, but only use information from train set to treat them.\n\nWe will split the train set so we have some data to validate our model. This set has 1456 entries; an 80-20 split of the data sounds reasonable.\n\nWe are also going to create a set with the full data to facilitate our work. Notice that this set will not be used to infer things (like mean, mode and so on). It's only going to be necessary when treating missing data and encoding.","63001a78":"... and load the data!","f0e2bea3":"We are now going to take a first look at the data.","e5153723":"- `MasVnrType`: masonry veneer type.\n\nThis feature presents a `None` category, meaning the house doesn't have masonry.\n\nLet's see how the `MasVnrArea` feature is for each of this missing values.","2081b065":"Ok, let's prepare our data.","2036515b":"Let's separate the numerical columns and the categorical columns, to create a more \"readable\" and \"error safe\" code.","36ecefd6":"Again, we are going the build the imputation function step by step.\n\nLet's also take a look on how the numericals are correlated to each other.\n","c5192475":"Observation: dropping the outliers from our train set may have a negative impact on our predictions if the test set also present outliers. A better solution would be making our models robust to outliers. We may revisit this essue latter, if our predictions go south.\n\nLet's separate the target from the features.","1bf97b22":"Let's check how our model is behaving on the train data.","35a63abc":"There are two things we should worry about: the first one is the distribution of our data, and the second one is the scale of our data. Both have impact in most algorithms implemented in scikit-learn.\n\nWe are going to adpt a trivial approach to the first issue: we will use the logarithm transform in the features presenting a absolute skew value higher than 0.5.","b2d3c177":"Let's keep noted that some columns (`GarageYrBlt` and `YearBuilt`, `TotalBsmtSF` and `1stFlrSF`, `GrLivArea` and `TotRmsAvbGrd`, `GarageArea` and `GarageCars`) have shown strong correlation index (greater than 0.8).\n\nEven though I would normally drop the columns with correlation above 80%, to correct colinearity, I won't do that here yet.\n\n- `LotFrontage`: linear feet of street connected to property.\n\nWe can see that the variable most correlated to `LotFrontage` is `1stFlrSF`. Let's plot them together.","da7f539b":"After some runnings, I think I'm ready to settle with the following configuration:","87b65e5b":"- `Exterior1st`: exterior covering on house;\n- `Exterior2nd`: Exterior covering on house (if more than one material).\n    \nIn both columns we have only one missing value, so it's safe (and easy) to impute the most common value for both.\n\n_Another approach would imput 'Absent' for `Exterior2nd`, since it seems `NaN` indicates that no other material was used, but it would create a new class unnecessarily._","013556f2":"Let's see how many mising values we have for each column, and see the description for these features.","cb690706":"We are going to build the imputation function step by step.\n\n- `GarageCond`, `GarageType`, `GarageQual`, `GarageFinish`, `Fence`, `PoolQC`, `Alley`, `FireplaceQu`, `MiscFeature`, `BsmtFinType1`, `BsmtCond`, `BsmtFinType2`, `BsmtQual`, `BsmtExposure`:\nfor all these features, according to the `problem description`, `NaN` means the absense of that specific item in the house. We are gointo to impute them with `Absent`.\n","44b526cb":"Let's first analyse the categorical columns with missing data.","6639dc29":"- `MSZoning`: identifies the general zoning classification of the sale.\n\nNormally in cities a zone has the same type of buildings. So I think we can impute the `MSZoning` with the most frequent value of city zoning in the same building class.","bd48dd9b":"For `Utilities`, `KitchenQual`, `Electrical`, `SaleType`, `Functional` we have only one or two missing values, and not enough correlated information to draw from. So we will impute the most frequent value for all of them.","0181b6d4":"Great! Not let's check the numerical features.\n\nFirst, let's see which ones have missing values.","0997bb7d":"It's nice to have some dummy functions to help us in the process. So let's code them.","92a3a33d":"Finally, let's train the model with the full train set and do our final predictions.","4b2cc094":"We can already make some observations:\n- as expected, we have missing values;\n- some features are in very different scale from our target (take LotFrontage, for instance);\n- there are some numerical columns that make more sense as categorical (like GarageYrBlt).\n\nFrom the `author's tip` we know that some outliers are present. The author advises:\n> I would recommend removing any houses with more than 4000 square feet from the data set [...]\n\nSo let's do it.","27cb89f3":"We will let the scaling for the final step, since it has to be carefully done so we don't contaminate our train set.\n\n---\n\n## Wrapping the model\n\nFirst, let's create a function to wrap all the steps we did so far! It will give us the ready to go sets (training, validation and test sets, and also the target set).","244b4ca8":"And it concludes the imputation for the numerical data! Let's check if everything is fine.","50331e27":"The other features, namely `BsmtFinSF2`, `BsmtUnfSF`, `BsmtFinSF1`, `GarageArea`, `TotalBsmtSF`, `GarageCars`, `BsmtFullBath` and `BsmtHalfBath`, have only one or two missing entries. Besides, they all refer to the garage or to the basement, where missing values means the absense of such. Therefore, we will impute them with zero.","a223e093":"- `MasVnrArea`: masonry veneer area in square feet.\n\nWe already talked about this one. I believe `NaN` means there is no masonry in the house. So let's impute it with zero.","f768a450":"## Code Challenge \/ Data Science\n\nIn this notebook, I'm going to create a solution for the problem \"House Prices: Advanced Regression Techniques\" proposed as a code challenge in the hiring process at Z\u00e9 Delivery.\n\nAiming to achieve what's been asked here:\n> We expect you to create a notebook and explain how you approach the problem presented, what decisions you have made and the rationale on each. Also, you should explain how you processed the data, which algorithms you used and what strategy applied on feature engineering, and everything else you should think is important for us to understand the solution.\n\nI'll try to be very specific about my thinking and decision making process, but I encorage you to ask any questions that may rise while reading this solution, I'll be glad to answer (and probably learn with it)!\n\nSo, without further due, let's code!\n\n------\n\n### Inicial Steps: what is the data about?\n\nBefore even importing the data, I carefully read the file `data_description.txt` provided (that I'll reference as `problem description`), and also tryed to find some information about this dataset on the internet, where I found this [information provided by the author](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) (that I'm referencing as `author's tips`).\n\n\n## Prepare the grounds\n\nThe first thing I'm doing is importing the modules we are going to use...\n","aab1d18f":"## Transforming features\n\nSome features need some transformations to better capture what they are representing.\n\n- `MSSubClass`, `MoSold`, `YrSold`, `YearBuilt` and `YearRemodAdd` are now numeric, but they don't represent a value, they are more like a category. We are going to fix this.","843dbda1":"It's better! See that light colours tend to be lower in the graphic (meaning smaller houses), and in some neighborhood the dark colours are prevalent (meaning bigger houses). At the same time, the lot frontage agroups in clusters of colors for each neighborhood. I believe this is a good guess! \n\nWe are going to group the rows by neighborhood and use the regression line we calculated a few cels before to estimate the `LotFrontage`. Let's code.","b8609e75":"Now let's check if we aren't missing any categorical."}}