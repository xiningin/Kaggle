{"cell_type":{"e8288aa9":"code","b091a263":"code","3c49910a":"code","c9f842c5":"code","bf240d0d":"code","d7f2b8c2":"code","67a80ec0":"code","7132b91d":"code","2631b5b1":"code","ad2ca2d1":"code","39428924":"code","96b27a94":"code","66d1de5a":"code","b1f70710":"code","8be4439c":"code","c50a582f":"code","30648585":"code","9abab4b1":"code","dd371588":"code","bbd28a47":"code","6e159fc4":"code","81574775":"code","371d19f0":"code","bfd7e1fc":"code","ed60e043":"code","da8d4c1f":"code","c0f5a213":"code","d332cbd2":"code","a08c65ba":"code","db19b9b3":"code","76758bd7":"code","b37f1427":"code","36394b38":"code","056d24ca":"code","084beae5":"markdown","a54a5c46":"markdown","d324c45f":"markdown","7dcebef0":"markdown","d9391276":"markdown","189756f0":"markdown","39dc04cf":"markdown","77ae3d93":"markdown","33ef7594":"markdown","96ede55c":"markdown","8f5ab408":"markdown","9177f708":"markdown","5e9e609a":"markdown","052b54fa":"markdown","2742d139":"markdown","6e89626c":"markdown","bf8d7db1":"markdown","dd0ed32d":"markdown","71026d12":"markdown","11df7edf":"markdown","62a5070d":"markdown","f352c144":"markdown"},"source":{"e8288aa9":"!pip install -U dabl","b091a263":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport dabl\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom mlens.ensemble import SuperLearner\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix , classification_report\nimport optuna\nfrom optuna.samplers import TPESampler\nimport warnings\n\nwarnings.filterwarnings('ignore')","3c49910a":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","c9f842c5":"df.describe()","bf240d0d":"df = df.dropna()","d7f2b8c2":"sns.heatmap(df.corr(), cmap='viridis_r')","67a80ec0":"sns.set_theme(style=\"darkgrid\")\nsns.displot(df['age'], kde=True)\nplt.xlabel(\"Age (in years)\")\nplt.title(f\"Distribution of Ages\")\nplt.show()","7132b91d":"sns.distplot(df[df['stroke'] == 0][\"age\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"age\"], label='Stroke')\nplt.title('No Stroke\/Stroke by Age')\nplt.legend()\nplt.show()","2631b5b1":"sns.displot(df['avg_glucose_level'], kde=True)\nplt.xlabel(\"Average Glucose Level\")\nplt.title(f\"Distribution of Average Glucose Level\")\nplt.show()","ad2ca2d1":"sns.distplot(df[df['stroke'] == 0][\"avg_glucose_level\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"avg_glucose_level\"], label='Stroke')\nplt.title('No Stroke\/Stroke by Avg Glucose Level')\nplt.legend()\nplt.show()","39428924":"sns.displot(df['bmi'], kde=True)\nplt.xlabel(\"Body Mass Index\")\nplt.title(f\"Distribution of Body Mass Index\")\nplt.show()","96b27a94":"sns.distplot(df[df['stroke'] == 0][\"bmi\"], label='No Stroke')\nsns.distplot(df[df['stroke'] == 1][\"bmi\"], label='Stroke')\nplt.title('No Stroke\/Stroke by BMI')\nplt.legend()\nplt.show()","66d1de5a":"dabl.plot(df, target_col='stroke')","b1f70710":"x = df.iloc[:, 1:-1].values\ny = df.iloc[:, -1].values","8be4439c":"ct = ColumnTransformer(transformers= [('encoder', OneHotEncoder(), [0,5,9])], remainder= 'passthrough')\nx = np.array(ct.fit_transform(x))","c50a582f":"le = LabelEncoder()\nx[:, 15] = le.fit_transform(x[:, 15])\nx[:, 16] = le.fit_transform(x[:, 16])","30648585":"x, y = SMOTE().fit_resample(x,y)","9abab4b1":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.33, random_state= 0)","dd371588":"print(\"Size x_train: \", x_train.shape)\nprint(\"Size y_train: \", y_train.shape)\nprint(\"Size x_test: \", x_test.shape)\nprint(\"Size y_test: \", y_test.shape)","bbd28a47":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","6e159fc4":"class Optimizer:\n    def __init__(self, metric, trials=30):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=42)\n        \n    def objective(self, trial):\n        model = create_model(trial)\n        model.fit(x_train, y_train)\n        preds = model.predict(x_test)\n        if self.metric == 'acc':\n            return accuracy_score(y_test, preds)\n        else:\n            return f1_score(y_test, preds)\n            \n    def optimize(self):\n        study = optuna.create_study(direction=\"maximize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        return study.best_params","81574775":"rf = RandomForestClassifier(random_state=42)\nrf.fit(x_train, y_train)\npreds = rf.predict(x_test)\n\nprint(\"Random Forest accuracy: \", accuracy_score(y_test, preds))\nprint(\"Random Forest f1-score: \", f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n    model = RandomForestClassifier(\n        min_samples_leaf=min_samples_leaf, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nrf_f1_params = optimizer.optimize()\nrf_f1_params['random_state'] = 42\nrf_f1 = RandomForestClassifier(\n    **rf_f1_params\n)\nrf_f1.fit(x_train, y_train)\npreds = rf_f1.predict(x_test)\n\nprint('Optimized on F1 score')\nprint('Optimized Random Forest: ', accuracy_score(y_test, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nrf_acc_params = optimizer.optimize()\nrf_acc_params['random_state'] = 42\nrf_acc = RandomForestClassifier(\n    **rf_acc_params\n)\nrf_acc.fit(x_train, y_train)\npreds = rf_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized Random Forest: ', accuracy_score(y_test, preds))\nprint('Optimized Random Forest f1-score: ', f1_score(y_test, preds))","371d19f0":"xgb = XGBClassifier(\n    random_state=42\n)\nxgb.fit(x_train, y_train)\npreds = xgb.predict(x_test)\n\nprint('XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('XGBoost f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 1)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    subsample = trial.suggest_uniform('subsample', 0.0001, 1.0)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        subsample=subsample,\n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nxgb_f1_params = optimizer.optimize()\nxgb_f1_params['random_state'] = 42\nxgb_f1 = XGBClassifier(\n    **xgb_f1_params\n)\nxgb_f1.fit(x_train, y_train)\npreds = xgb_f1.predict(x_test)\n\nprint('Optimized on F1 score')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nxgb_acc_params = optimizer.optimize()\nxgb_acc_params['random_state'] = 42\nxgb_acc = XGBClassifier(\n    **xgb_acc_params\n)\nxgb_acc.fit(x_train, y_train)\npreds = xgb_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized XGBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized XGBoost f1-score: ', f1_score(y_test, preds))","bfd7e1fc":"lr = LogisticRegression(\n    random_state=666\n)\nlr.fit(x_train, y_train)\npreds = lr.predict(x_test)\n\nprint('Logistic Regression: ', accuracy_score(y_test, preds))\nprint('Logistic Regression f1-score: ', f1_score(y_test, preds))","ed60e043":"dt = DecisionTreeClassifier(\n    random_state=666\n)\ndt.fit(x_train, y_train)\npreds = dt.predict(x_test)\n\nprint('Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Decision Tree f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 6)\n    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n    min_weight_fraction_leaf = trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5)\n    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n    model = DecisionTreeClassifier(\n        min_samples_split=min_samples_split, \n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_depth=max_depth, \n        min_samples_leaf=min_samples_leaf, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\ndt_f1_params = optimizer.optimize()\ndt_f1_params['random_state'] = 666\ndt_f1 = DecisionTreeClassifier(\n    **dt_f1_params\n)\ndt_f1.fit(x_train, y_train)\npreds = dt_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\ndt_acc_params = optimizer.optimize()\ndt_acc_params['random_state'] = 666\ndt_acc = DecisionTreeClassifier(\n    **dt_acc_params\n)\ndt_acc.fit(x_train, y_train)\npreds = dt_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized Decision Tree accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized Decision Tree f1-score: ', f1_score(y_test, preds))","da8d4c1f":"knn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\npreds = knn.predict(x_test)\n\nprint('KNN accuracy: ', accuracy_score(y_test, preds))\nprint('KNN f1-score: ', f1_score(y_test, preds))\n\nsampler = TPESampler(seed=0)\ndef create_model(trial):\n    n_neighbors = trial.suggest_int(\"n_neighbors\", 2, 25)\n    model = KNeighborsClassifier(n_neighbors=n_neighbors)\n    return model\n\noptimizer = Optimizer('f1')\nknn_f1_params = optimizer.optimize()\nknn_f1 = KNeighborsClassifier(\n    **knn_f1_params\n)\nknn_f1.fit(x_train, y_train)\npreds = knn_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized KNN accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nknn_acc_params = optimizer.optimize()\nknn_acc = KNeighborsClassifier(\n    **knn_acc_params\n)\nknn_acc.fit(x_train, y_train)\npreds = knn_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized KNN accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized KNN f1-score: ', f1_score(y_test, preds))","c0f5a213":"abc = AdaBoostClassifier(\n    random_state=666\n)\nabc.fit(x_train, y_train)\npreds = abc.predict(x_test)\n\nprint('AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('AdaBoost f1-score: ', f1_score(y_test, preds))\n\ndef create_model(trial):\n    n_estimators = trial.suggest_int(\"n_estimators\", 2, 150)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0005, 1.0)\n    model = AdaBoostClassifier(\n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        random_state=666\n    )\n    return model\n\noptimizer = Optimizer('f1')\nabc_f1_params = optimizer.optimize()\nabc_f1_params['random_state'] = 666\nabc_f1 = AdaBoostClassifier(\n    **abc_f1_params\n)\nabc_f1.fit(x_train, y_train)\npreds = abc_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_test, preds))\n\noptimizer = Optimizer('acc')\nabc_acc_params = optimizer.optimize()\nabc_acc_params['random_state'] = 666\nabc_acc = AdaBoostClassifier(**abc_acc_params)\nabc_acc.fit(x_train, y_train)\npreds = abc_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint('Optimized AdaBoost accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized AdaBoost f1-score: ', f1_score(y_test, preds))","d332cbd2":"svc = SVC(random_state=666)\nsvc.fit(x_train, y_train)\npreds = svc.predict(x_test)\n\nprint(\"SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"SupportVectorClassifier f1-score: \", f1_score(y_test, preds))\n\ndef create_model(trial):\n    kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n    degree = trial.suggest_int('degree', 2, 5)\n    gamma = trial.suggest_categorical('gamma', ['auto', 'scale'])\n    model = SVC(\n        kernel=kernel,\n        degree=degree,\n        gamma=gamma,\n        random_state=0\n    )\n    return model\n\noptimizer = Optimizer('f1')\nsvc_f1_params = optimizer.optimize()\nsvc_f1_params['random_state'] = 666\nsvc_f1 = SVC(**svc_f1_params)\nsvc_f1.fit(x_train, y_train)\npreds = svc_f1.predict(x_test)\n\nprint('Optimized on F1-score')\nprint(\"Optimized SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"Optimized SupportVectorClassifier f1-score: \", f1_score(y_test, preds))\n\noptimizer = Optimizer('accuracy')\nsvc_acc_params = optimizer.optimize()\nsvc_acc_params['random_state'] = 666\nsvc_acc = SVC(**svc_acc_params)\nsvc_acc.fit(x_train, y_train)\npreds = svc_acc.predict(x_test)\n\nprint('Optimized on accuracy')\nprint(\"Optimized SupportVectorClassifier accuracy: \", accuracy_score(y_test, preds))\nprint(\"Optimized SupportVectorClassifier f1-score: \", f1_score(y_test, preds))","a08c65ba":"model = SuperLearner(folds=5, random_state=42)\nmodel.add([svc, abc, xgb, rf, dt, knn])\nmodel.add_meta(LogisticRegression())\nmodel.fit(x_train, y_train)\npreds = model.predict(x_test)\nprint('SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('SuperLearner f1-score: ', f1_score(y_test, preds))","db19b9b3":"mdict = {\n    'RF': RandomForestClassifier(random_state=666),\n    'XGB': XGBClassifier(random_state=666),\n    'LR': LogisticRegression(random_state=666),\n    'DT': DecisionTreeClassifier(random_state=666),\n    'KNN': KNeighborsClassifier(),\n    'ABC': AdaBoostClassifier(random_state=666),\n    'SVC': SVC(random_state=666),\n    'OARF': RandomForestClassifier(**rf_acc_params),\n    'OFRF': RandomForestClassifier(**rf_f1_params),\n    'OAXGB': XGBClassifier(**xgb_acc_params),\n    'OFXGB': XGBClassifier(**xgb_f1_params),\n    'OADT': DecisionTreeClassifier(**dt_acc_params),\n    'OFDT': DecisionTreeClassifier(**dt_f1_params),\n    'OAKNN': KNeighborsClassifier(**knn_acc_params),\n    'OFKNN': KNeighborsClassifier(**knn_f1_params),\n    'OAABC': AdaBoostClassifier(**abc_acc_params),\n    'OFABC': AdaBoostClassifier(**abc_f1_params),\n    'OASVC': SVC(**svc_acc_params),\n    'OFSVC': SVC(**svc_f1_params)\n}","76758bd7":"def create_model(trial):\n    model_names = list()\n    models_list = [\n        'RF', 'XGB', 'DT', 'LR', 'KNN', 'ABC', 'SVC', 'OARF', 'OFRF', 'OAXGB', 'OFXGB',\n        'OADT', 'OFDT', 'OAKNN', 'OFKNN', 'OAABC', 'OFABC', 'OASVC', 'OFSVC'\n    ]\n    \n    head_list = [\n        'RF', 'XGB', 'DT', \n        'KNN', 'LR', 'ABC', \n        'SVC'\n    ]\n    n_models = trial.suggest_int(\"n_models\", 2, 6)\n    for i in range(n_models):\n        model_item = trial.suggest_categorical('model_{}'.format(i), models_list)\n        if model_item not in model_names:\n            model_names.append(model_item)\n    \n    folds = trial.suggest_int(\"folds\", 2, 6)\n    \n    model = SuperLearner(\n        folds=folds, \n        random_state=666\n    )\n    \n    models = [\n        mdict[item] for item in model_names\n    ]\n    model.add(models)\n    head = trial.suggest_categorical('head', head_list)\n    model.add_meta(\n        mdict[head]\n    )\n        \n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(x_train, y_train)\n    preds = model.predict(x_test)\n    score = accuracy_score(y_test, preds)\n    return score\n\nstudy = optuna.create_study(\n    direction=\"maximize\", \n    sampler=sampler\n)\nstudy.optimize(\n    objective, \n    n_trials=50\n)","b37f1427":"params = study.best_params\n\nhead = params['head']\nfolds = params['folds']\ndel params['head'], params['n_models'], params['folds']\nresult = list()\nfor key, value in params.items():\n    if value not in result:\n        result.append(value)\n        \nprint(result)","36394b38":"model = SuperLearner(\n    folds=folds, \n    random_state=666\n)\n\nmodels = [\n    mdict[item] for item in result\n]\nmodel.add(models)\nmodel.add_meta(mdict[head])\n\nmodel.fit(x_train, y_train)\n\npreds = model.predict(x_test)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_test, preds))","056d24ca":"print(confusion_matrix(y_test, preds))\nprint(classification_report(y_test, preds))","084beae5":"### Distribution Plots","a54a5c46":"### Ensemble","d324c45f":"## Data Preprocessing","7dcebef0":"### General Plots","d9391276":"### Feature Scaling","189756f0":"### Split dataset","39dc04cf":"### Logistic Regression","77ae3d93":"### K-Nearest Neighbors","33ef7594":"### Random Forest","96ede55c":"### Super Learner","8f5ab408":"### SMOTE","9177f708":"### AdaBoost","5e9e609a":"### Decision Tree","052b54fa":"## Model Selection","2742d139":"## EDA","6e89626c":"### Encoding","bf8d7db1":"## Final Model","dd0ed32d":"### Heat Map Correlation","71026d12":"### Support Vector Machine","11df7edf":"### XGBoost","62a5070d":"## Imports","f352c144":"## Load dataset"}}