{"cell_type":{"81161556":"code","88a11e62":"code","a1e9b966":"code","24468f04":"code","1b5a6060":"code","81407e20":"code","516977ae":"code","c0aa8c85":"code","5549a72c":"code","3f02df3d":"code","cdc5254f":"code","34895671":"code","4a2b2df5":"code","ad8ad85e":"code","5a0218cb":"code","3f403600":"code","f8ed6d02":"code","0259794d":"code","90c84b0c":"code","4fd2d2f4":"code","9e66d88c":"code","1d3e2ffe":"code","d31f3d22":"code","7b46576f":"code","998e23b1":"code","b7393f10":"code","72413af7":"code","08ad06d5":"code","c35e986f":"code","41b2b50f":"code","8af9365e":"code","b526f049":"code","04c7d587":"code","30bd7a6d":"code","41c486e3":"code","ff398027":"code","c8edcb3b":"code","c2357dcb":"code","bb2d1d31":"code","45553c77":"code","da722dfe":"markdown","8f629a4d":"markdown","06ca5500":"markdown","3e5a4e25":"markdown","6e3e9a7a":"markdown","040d507b":"markdown","b379ebe3":"markdown","1a87d25e":"markdown","4b54e49c":"markdown","31265e2b":"markdown","11976372":"markdown","0181431d":"markdown","cc071c9e":"markdown","84b3ad7a":"markdown","67302597":"markdown","04424f78":"markdown","616c7591":"markdown","322ff24c":"markdown","4bad34b4":"markdown","435e88b7":"markdown","db50edb2":"markdown","eb095f37":"markdown","f12bbe9f":"markdown","a708a6de":"markdown","aa9b001a":"markdown","be22b5b9":"markdown","f2afef24":"markdown","da68cabc":"markdown","c40fc407":"markdown","9f85d0a8":"markdown","c97828bc":"markdown"},"source":{"81161556":"import numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nfrom sklearn.linear_model import LinearRegression,Ridge,ElasticNet,Lasso\nfrom sklearn.preprocessing import MinMaxScaler,OneHotEncoder,PolynomialFeatures,StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold,cross_val_score,cross_val_predict,cross_validate,RepeatedStratifiedKFold,RepeatedKFold\nfrom sklearn.metrics import accuracy_score,confusion_matrix,mean_squared_error,r2_score,plot_confusion_matrix\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeRegressor,plot_tree,export_text,DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,VotingRegressor,StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n\nfrom xgboost import XGBRegressor,plot_tree\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport opendatasets as od\n\nimport json\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\n\n%matplotlib inline","88a11e62":"with open('..\/kaggle.json') as f:\n    kaggle = json.load(f)\nkaggle['key']","a1e9b966":"#od.download('https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques')","24468f04":"os.listdir('house-prices-advanced-regression-techniques')","1b5a6060":"file_name = 'train.csv'","81407e20":"path = 'house-prices-advanced-regression-techniques\/' + file_name","516977ae":"housing = pd.read_csv(path)","c0aa8c85":"housing.head(5)","5549a72c":"housing.info()","3f02df3d":"null_cols = ['Alley','FireplaceQu','MiscFeature','PoolQC','Fence']\nhousing = housing.drop(columns=null_cols)\nhousing.head(2)","cdc5254f":"corr = housing.corr()['SalePrice'].sort_values(ascending=False)\ncorr","34895671":"plt.figure(figsize=(10,20))\nsns.barplot(x=corr,y=corr.index)","4a2b2df5":"px.scatter(data_frame=housing,x='OverallQual',y='SalePrice')","ad8ad85e":"cols = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']\nfig,axs = plt.subplots(nrows=2,ncols=5,figsize=(20, 10))\naxs = axs.flatten()\nindex = 0\nfor i in cols:\n    sns.boxplot(data=housing,x=i,ax=axs[index])\n    index+=1","5a0218cb":"housing = pd.read_csv(path)\nnull_cols = ['Alley','FireplaceQu','MiscFeature','PoolQC','Fence']\nhousing = housing.drop(columns=null_cols)\nhousing.head(2)\nhousing = housing[(housing['GrLivArea'] < 2700) & (housing['GarageArea'] < 950) & (housing['TotalBsmtSF'] < 2000) & (housing['1stFlrSF'] < 2200)]\n\ninputs_cols = housing.columns.tolist()\ninputs_cols.remove('SalePrice')\ninputs_cols.remove('Id')\ntargets_cols = 'SalePrice'\ninputs_df = housing[inputs_cols]\ntargets_df = housing[targets_cols]\n\nnumeric_cols = inputs_df.select_dtypes(include=np.number).columns.tolist()[1:-1]\nprint(numeric_cols)\n\nprint('\\n')\n\ncategorical_cols = inputs_df.select_dtypes(include='object').columns.tolist()\nprint(categorical_cols)","3f403600":"imputer = SimpleImputer()\nimputer.fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = imputer.transform(inputs_df[numeric_cols])\ninputs_df.head(2)","f8ed6d02":"scaler = StandardScaler()\nscaler.fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])\ninputs_df.head(5)","0259794d":"encoder = OneHotEncoder(sparse=False,handle_unknown='ignore')\nencoder.fit(inputs_df[categorical_cols])\nencoded_cols = list(encoder.get_feature_names_out())\ninputs_df[encoded_cols] = encoder.transform(inputs_df[categorical_cols])\ninputs_df.head()","90c84b0c":"def rmse(x,y):\n    return np.sqrt(np.mean(np.square(x-y)))","4fd2d2f4":"#X = inputs_df[numeric_cols+encoded_cols]\nX = inputs_df[step_cols]\nY = targets_df\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nlinear = LinearRegression()\nlinear.fit(train_inputs,train_targets)\ntrain_preds = linear.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = linear.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\nlinear = LinearRegression()\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(linear, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","9e66d88c":"#X = inputs_df[numeric_cols+encoded_cols]\nX = inputs_df[step_cols]\nY = targets_df\npoly = PolynomialFeatures(degree=2).fit(X)\nX1 = poly.transform(X)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X1,Y,random_state=42)\npoly_model = LinearRegression().fit(train_inputs,train_targets)\ntrain_preds = poly_model.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = poly_model.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\npoly = PolynomialFeatures(degree=2).fit(X)\nX1 = poly.transform(X)\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(poly_model, X1, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","1d3e2ffe":"#X = inputs_df[numeric_cols+encoded_cols]\nX = inputs_df[step_cols]\nY = targets_df\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nridge = Ridge()\nridge.fit(train_inputs,train_targets)\ntrain_preds = ridge.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = ridge.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\nridge = Ridge()\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(ridge, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","d31f3d22":"#X = inputs_df[numeric_cols+encoded_cols]\nX = inputs_df[step_cols]\nY = targets_df\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nlasso = Lasso()\nlasso.fit(train_inputs,train_targets)\ntrain_preds = lasso.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = lasso.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\nlasso = Lasso()\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(lasso, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","7b46576f":"#X = inputs_df[numeric_cols+encoded_cols]\nX = inputs_df[step_cols]\nY = targets_df\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nelastic = ElasticNet()\nelastic.fit(train_inputs,train_targets)\ntrain_preds = elastic.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = elastic.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\nelastic = ElasticNet()\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(elastic, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","998e23b1":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nX = inputs_df[numeric_cols+encoded_cols]\n  \n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n\n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n  \nprint(vif_data)","b7393f10":"import statsmodels.api as sm\n\nX = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\n\ndef get_stats():\n    results = sm.OLS(Y, X).fit()\n    print(results.summary())\n\nget_stats()","72413af7":"import warnings\nwarnings.filterwarnings('ignore')","08ad06d5":"def stepwise_selection(X, y, \n                           initial_list=[], \n                           threshold_in=0.01, \n                           threshold_out = 0.05, \n                           verbose=True):\n        \"\"\" Perform a forward-backward feature selection \n        based on p-value from statsmodels.api.OLS\n        Arguments:\n            X - pandas.DataFrame with candidate features\n            y - list-like with the target\n            initial_list - list of features to start with (column names of X)\n            threshold_in - include a feature if its p-value < threshold_in\n            threshold_out - exclude a feature if its p-value > threshold_out\n            verbose - whether to print the sequence of inclusions and exclusions\n        Returns: list of selected features \n        Always set threshold_in < threshold_out to avoid infinite looping.\n        See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n        \"\"\"\n        included = list(initial_list)\n        while True:\n            changed=False\n            # forward step\n            excluded = list(set(X.columns)-set(included))\n            new_pval = pd.Series(index=excluded)\n            for new_column in excluded:\n                model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n                new_pval[new_column] = model.pvalues[new_column]\n            best_pval = new_pval.min()\n            if best_pval < threshold_in:\n                best_feature = new_pval.idxmin()\n                included.append(best_feature)\n                changed=True\n                if verbose:\n                    print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n            # backward step\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n            # use all coefs except intercept\n            pvalues = model.pvalues.iloc[1:]\n            worst_pval = pvalues.max() # null if pvalues is empty\n            if worst_pval > threshold_out:\n                changed=True\n                worst_feature = pvalues.idxmax()\n                included.remove(worst_feature)\n                if verbose:\n                    print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n            if not changed:\n                break\n        return included\n\nresult = stepwise_selection(X, Y)\n\nprint('resulting features:')\nprint(result)","c35e986f":"step_cols = ['OverallQual', 'GrLivArea', 'BsmtFinSF1', 'YearBuilt', 'BsmtQual_Ex', 'BldgType_1Fam', 'TotalBsmtSF', 'OverallCond', 'ExterQual_TA', 'LotArea', 'KitchenQual_Ex', 'Neighborhood_Crawfor', 'GarageCars', 'BsmtExposure_Gd', 'Neighborhood_NridgHt', 'SaleType_New', 'Neighborhood_NoRidge', 'Neighborhood_StoneBr', 'SaleCondition_Normal', 'Neighborhood_Somerst', 'Functional_Typ', 'Exterior1st_BrkFace', 'WoodDeckSF', 'Condition1_Norm', 'Neighborhood_BrkSide', 'BsmtFinType2_nan', 'BsmtQual_nan', 'BsmtFinType1_nan', 'BsmtCond_nan', 'BsmtFullBath', 'Fireplaces', 'YearRemodAdd', 'Neighborhood_Mitchel', 'LotConfig_CulDSac', 'LandSlope_Sev', 'HouseStyle_2.5Fin', 'BsmtFinType1_GLQ', 'Condition1_RRAe', 'Functional_Sev', 'PoolArea', 'ExterQual_Ex', 'BedroomAbvGr', 'MSZoning_C (all)', 'RoofMatl_Membran', 'Heating_OthW', 'GarageType_nan', 'GarageArea', 'GarageCond_nan', 'GarageFinish_nan', 'GarageQual_nan', 'HeatingQC_Ex', 'ScreenPorch']","41b2b50f":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nlinear = LinearRegression()\nselector = RFE(linear,step=1)\nselector = selector.fit(X, Y)\nX = selector.transform(X)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nlinear.fit(train_inputs,train_targets)\ntrain_preds = linear.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = linear.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\nlinear = LinearRegression()\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(linear, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","8af9365e":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nridge = Ridge()\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nridge.fit(train_inputs,train_targets)\ntrain_preds = ridge.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = ridge.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(ridge, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","b526f049":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nlasso = Lasso(selection='random')\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nlasso.fit(train_inputs,train_targets)\ntrain_preds = lasso.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = lasso.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(lasso, X, Y, cv=skf)\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","04c7d587":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nelastic = ElasticNet()\nselector = RFE(elastic,step=1)\nselector = selector.fit(X, Y)\nX = selector.transform(X)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nelastic.fit(train_inputs,train_targets)\ntrain_preds = elastic.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = elastic.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#Cross Validation\nkf = KFold(n_splits=10)\nskf = StratifiedKFold(n_splits=10)\nscores = cross_val_score(elastic, X, Y, cv=skf,scoring='r2')\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","30bd7a6d":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nrf = RandomForestRegressor(n_estimators=200,n_jobs=-1,random_state=42,max_depth=7,max_features=50)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nrf.fit(train_inputs,train_targets)\ntrain_preds = rf.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = rf.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#rf = RandomForestRegressor(n_jobs=-1,random_state=42)\n#Cross Validation\nkf = KFold(n_splits=10)\nscores = cross_val_score(rf, X, Y, cv=kf,scoring='r2')\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","41c486e3":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nvoting = VotingRegressor(estimators=[('ridge',ridge),('lasso',lasso),('elastic_net',elastic),('random_forest',rf)],n_jobs=-1)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nvoting.fit(train_inputs,train_targets)\ntrain_preds = voting.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = voting.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#rf = RandomForestRegressor(n_jobs=-1,random_state=42)\n#Cross Validation\nkf = KFold(n_splits=10)\nscores = cross_val_score(voting, X, Y, cv=kf,scoring='r2')\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","ff398027":"X = inputs_df[numeric_cols+encoded_cols]\nY = targets_df\nxgb = XGBRegressor(n_estimators=240,n_jobs=-1,random_state=42,max_depth=6)\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\nxgb.fit(train_inputs,train_targets)\ntrain_preds = xgb.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = xgb.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))\n\n#rf = RandomForestRegressor(n_jobs=-1,random_state=42)\n#Cross Validation\nkf = KFold(n_splits=10)\nscores = cross_val_score(xgb, X, Y, cv=kf,scoring='r2')\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))","c8edcb3b":"def get_stacking():\n    # define the base models\n    level0 = list()\n    level0.append(('Voting', VotingRegressor(estimators=[('ridge',ridge),('lasso',lasso),('elastic_net',elastic),('random_forest',rf)],n_jobs=-1)))\n    level0.append(('Rdige', Ridge()))\n    level0.append(('Lasso', Lasso(selection='random')))\n    level0.append(('XGBoost', XGBRegressor(n_estimators=240,n_jobs=-1,random_state=42,max_depth=6)))\n    # define meta learner model\n    level1 = LinearRegression()\n    # define the stacking ensemble\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)\n    return model\n\n# get a list of models to evaluate\ndef get_models():\n    models = dict()\n    models['Voting'] = VotingRegressor(estimators=[('ridge',ridge),('lasso',lasso),('elastic_net',elastic),('random_forest',rf)],n_jobs=-1)\n    models['Rdige'] = Ridge()\n    models['Lasso'] = Lasso(selection='random')\n    models['XGBoost'] = XGBRegressor(n_estimators=240,n_jobs=-1,random_state=42,max_depth=6)\n    models['stacking'] = get_stacking()\n    return models\n\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n    scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores\n\n# define dataset\nX, y = X.copy(), Y.copy()\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n\tscores = evaluate_model(model, X, y)\n\tresults.append(scores)\n\tnames.append(name)\n\tprint('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","c2357dcb":"%%time\n# define dataset\ntrain_inputs,val_inputs,train_targets,val_targets = train_test_split(X,Y,test_size=0.2,random_state=42)\n# define the base models\nlevel0 = list()\nlevel0.append(('Voting', VotingRegressor(estimators=[('ridge',ridge),('lasso',lasso),('elastic_net',elastic),('random_forest',rf)],n_jobs=-1)))\nlevel0.append(('Rdige', Ridge()))\nlevel0.append(('Lasso', Lasso(selection='random')))\nlevel0.append(('XGBoost', XGBRegressor(n_estimators=240,n_jobs=-1,random_state=42,max_depth=6)))\n# define meta learner model\nlevel1 = LinearRegression()\n# define the stacking ensemble\nstacking = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)\n# fit the model on all available data\nstacking.fit(train_inputs, train_targets)\n# make a prediction for one example\ntrain_preds = stacking.predict(train_inputs)\nprint('RMSE on train set :',rmse(train_targets,train_preds))\nval_preds = stacking.predict(val_inputs)\nprint('RMSE on validation set :',rmse(val_targets,val_preds))\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(val_targets, val_preds))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(val_targets, val_preds))","bb2d1d31":"submit = pd.read_csv('.\/house-prices-advanced-regression-techniques\/test.csv')\nsubmit[numeric_cols] = imputer.transform(submit[numeric_cols])\nsubmit[numeric_cols] = scaler.transform(submit[numeric_cols])\nsubmit[encoded_cols] = encoder.transform(submit[categorical_cols])\ntest_preds = stacking.predict(submit[numeric_cols+encoded_cols])","45553c77":"submission = pd.DataFrame({'Id':submit['Id'],'SalePrice':test_preds})\nsubmission.to_csv('submission.csv',index=False)","da722dfe":"#### Predicting the logarithmic of `SalePrice`","8f629a4d":"#### Ridge Regression","06ca5500":"#### RandomForest","3e5a4e25":"#### XGB Regressor","6e3e9a7a":"#### ElasticNet Regression","040d507b":"### Imputing","b379ebe3":"submission = pd.DataFrame({'Id':submit['Id'],'SalePrice':test_preds})\nsubmission.to_csv('submission.csv',index=False)","1a87d25e":"#### Linear - With Recursive Feature Elimination","4b54e49c":"### Stacking Regressor","31265e2b":"#### Ridge","11976372":"* SalePrice follow kind of normal distribution, so we use StandardScaler","0181431d":"### Downloading and importing dataset","cc071c9e":"#### Linear Regression","84b3ad7a":"### Feature Scaling","67302597":"normalizer = MinMaxScaler()\nnormalizer.fit(inputs_df[numeric_cols])\ninputs_df[numeric_cols] = normalizer.transform(inputs_df[numeric_cols])\ninputs_df.head(5)","04424f78":"#### Forward Backward Feature Selection ","616c7591":"#### Polynomial Regression","322ff24c":"### Modelling","4bad34b4":"submit = pd.read_csv('.\/house-prices-advanced-regression-techniques\/test.csv')\nsubmit[numeric_cols] = imputer.transform(submit[numeric_cols])\nsubmit[numeric_cols] = scaler.transform(submit[numeric_cols])\nsubmit[encoded_cols] = encoder.transform(submit[categorical_cols])\ntest_preds = rf.predict(submit[numeric_cols+encoded_cols])\ntest_preds = np.exp(test_preds)","435e88b7":"#### ElasticNet","db50edb2":"#### Voting Regressor","eb095f37":"#### Stepwise Regression","f12bbe9f":"#### Lasso","a708a6de":"### Kaggle Submission","aa9b001a":"### Encoding","be22b5b9":"#### Normal Submission","f2afef24":"### Exploratory Data Analysis ","da68cabc":"#### Lasso Regression","c40fc407":"### Run this first","9f85d0a8":"### Removing Outliers","c97828bc":"#### Log Submission"}}