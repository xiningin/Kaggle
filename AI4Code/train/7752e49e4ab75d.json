{"cell_type":{"a6089bb5":"code","5db5b74e":"code","5c220049":"code","a00f7962":"code","1a7d0c87":"code","ddeec8c4":"code","74489c40":"code","82e461c3":"code","2e981edb":"code","f00501e4":"code","ccd5ae25":"code","3095d9e5":"code","5b32499e":"code","cd8eba8a":"code","d63bf325":"code","1dc68ea6":"code","50b28caf":"code","4d0baf5b":"code","c449fea4":"code","13619a48":"code","4f94b574":"code","cbc626bc":"code","0a8be247":"code","465a76f8":"code","c349c244":"code","cb324c39":"code","54985774":"code","70be7bfb":"code","b1ff673e":"code","6ca864eb":"code","5e318bdb":"code","00a92c40":"code","5762f66d":"code","87bd9511":"code","df0e7d31":"code","402b79f8":"code","8a8ef14d":"code","f9f7a8e7":"code","4196a3a9":"code","73cb0c67":"code","ade12931":"code","90b7f7e3":"code","3939455a":"code","52e913ec":"code","9e02bd78":"code","10c11aa3":"code","3ae12af9":"code","a3e62599":"code","59709b74":"code","4f1ec6ac":"code","ffe215bc":"code","15c977b3":"code","82350c42":"markdown","2bc6e748":"markdown","78925c24":"markdown","e536200e":"markdown","558099f1":"markdown","b484bbf3":"markdown","c417f8e1":"markdown","4f175bd1":"markdown","711ba0a1":"markdown","937d798a":"markdown","a285465f":"markdown","b213decd":"markdown","1a113818":"markdown","ec745c04":"markdown","0cd64269":"markdown","4dce27ec":"markdown","73c68811":"markdown","97623e66":"markdown","c1f243cc":"markdown","c86a657e":"markdown"},"source":{"a6089bb5":"import numpy as np \nimport pandas as pd \nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\n%matplotlib notebook\n\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom lime.lime_text import LimeTextExplainer\nfrom tqdm import tqdm\nimport string\nimport random\nimport operator\nimport seaborn as sns\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport warnings\nimport nltk\n\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5db5b74e":"#list of data that we have in the workspace\n\nprint(os.listdir(\"..\/input\"))","5c220049":"# countries that use English as an official language\nbritish_youtube = pd.read_csv(\"..\/input\/GBvideos.csv\")\ncanadian_youtube = pd.read_csv(\"..\/input\/CAvideos.csv\")\nus_youtube = pd.read_csv(\"..\/input\/USvideos.csv\")\n","a00f7962":"canadian_youtube.head()","1a7d0c87":"british_youtube.head()","ddeec8c4":"us_youtube.head()","74489c40":"#combine tables\nthree_countries=pd.concat([canadian_youtube, british_youtube,us_youtube])\nthree_countries.shape\n","82e461c3":"#Check duplicate. It is always good to check whether there are some duplicates in dataset!\nthree_countries.video_id.value_counts()[:10]","2e981edb":"#remove duplicate\nthree_countries= three_countries.drop_duplicates(['video_id'], keep='first')\n","f00501e4":"three_countries.video_id.value_counts()[:10]","ccd5ae25":"#need to be decoded \nthree_countries.category_id.head()","3095d9e5":"import json\n\ndef category_name(path):\n    with open(path) as json_file:  \n        data = json.load(json_file)\n    category_info_list=[]\n    for row in data['items']:\n        id_info=row['id']\n        category_name=row['snippet']['title']\n        categoty_info=(id_info ,category_name)\n        category_info_list.append(categoty_info)\n    return(dict(category_info_list))\n        \n    ","5b32499e":"category_name(\"..\/input\/CA_category_id.json\")","cd8eba8a":"category_list=category_name(\"..\/input\/CA_category_id.json\")\ncategory_names=[]\nfor i in three_countries.category_id:\n    category_name=category_list.get(str(i))\n    category_names.append(category_name)\n\nthree_countries['category_names']=category_names","d63bf325":"#now, we have category name :)\nthree_countries['category_names'].head()","1dc68ea6":"three_countries.info()","50b28caf":"#give you a report of distribution of data\/correlation\nimport pandas_profiling as pp\n\npp.ProfileReport(three_countries[['views','likes','dislikes','comment_count']])","4d0baf5b":"\nQ1 = three_countries.views.quantile(0.25)\nQ3 = three_countries.views.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\n\npopular_videos=three_countries.loc[three_countries.views > (Q3 + 1.5 * IQR)]\n\nthree_countries['popular']=0\nthree_countries.loc[three_countries.views > (Q3 + 1.5 * IQR),'popular']=1\n\nthree_countries['popular'].value_counts()","c449fea4":"#make a variable that tells ratio of like and dislike\nthree_countries['like_percentage']=(three_countries['likes']\/(three_countries['likes']+three_countries['dislikes'])*100)\n#date column as datatime datatype\nthree_countries[\"publish_time\"] = pd.to_datetime(three_countries[\"publish_time\"])","13619a48":"#top 20 channels by the mean of views\n\nfig = plt.figure(figsize=(15, 10))\nthree_countries.groupby('channel_title').mean().sort_values(by=['views'], ascending=False)[:20].views.sort_values(ascending=True).plot(kind='barh',colormap='winter',fontsize=20)\n","4f94b574":"#among popular videos, here are top 10 like percentage video. All of them are k-pop stars MV lol \n\nthree_countries.loc[three_countries.popular==1].sort_values(by=['like_percentage'], ascending=False)[:10]","cbc626bc":"#among popular videos, here are top 10 dislike percentage video. \n\nthree_countries.loc[three_countries.popular==1].sort_values(by=['like_percentage'], ascending=True)[:10]","0a8be247":"#simple example \n\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(u\"An apple is not a banana\")\nfor token in doc:\n    print(token.text)","465a76f8":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nparser = English()\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\n\ntqdm.pandas()\n\nnormal = three_countries[\"title\"][three_countries[\"popular\"] == 0].progress_apply(spacy_tokenizer)\npopular = three_countries[\"title\"][three_countries[\"popular\"] == 1].progress_apply(spacy_tokenizer)","c349c244":"#tokenize words by popularity \n\ndef word_generator(text):\n    word = list(text.split())\n    return word\ndef bigram_generator(text):\n    bgram = list(nltk.bigrams(text.split()))\n    bgram = [' '.join((a, b)) for (a, b) in bgram]\n    return bgram\ndef trigram_generator(text):\n    tgram = list(nltk.trigrams(text.split()))\n    tgram = [' '.join((a, b, c)) for (a, b, c) in tgram]\n    return tgram\n\n\nnormal_words = normal.progress_apply(word_generator)\npopular_words = popular.progress_apply(word_generator)\nnormal_bigrams = normal.progress_apply(bigram_generator)\npopular_bigrams = popular.progress_apply(bigram_generator)\nnormal_trigrams = normal.progress_apply(trigram_generator)\npopular_trigrams = popular.progress_apply(trigram_generator)","cb324c39":"#function that makes a pretty word frequency plot\n\ndef word_plot(words,my_color):\n    slist =[]\n    for x in words:\n        slist.extend(x)\n    fig = plt.figure(figsize=(15, 10))\n    pd.Series(slist).value_counts()[:20].sort_values(ascending=True).plot(kind='barh',fontsize=20, color=my_color)\n    plt.show()\n","54985774":"word_plot(popular_words,'blue')\n","70be7bfb":"word_plot(popular_bigrams,'orange')\n","b1ff673e":"word_plot(popular_trigrams,'red')","6ca864eb":"txt1 = ['I like banana', 'An apple is not a banana', 'banana banana oh banana']\ntf = TfidfVectorizer(smooth_idf=False, sublinear_tf=False, norm=None, analyzer='word')\ntxt_fitted = tf.fit(txt1)\ntxt_transformed = txt_fitted.transform(txt1)\nprint (\"The text: \", txt1)","5e318bdb":"tf.vocabulary_","00a92c40":"idf = tf.idf_\nprint(dict(zip(txt_fitted.get_feature_names(), idf)))\nprint(\"\\nThe token 'banana' appears 5 times but it is also in all documents, so its idf is the lowest\")","5762f66d":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(three_countries.title)\nword_features = word_vectorizer.transform(three_countries.title)\n\nclassifier_popular = LogisticRegression(C=0.1, solver='sag')\nclassifier_popular.fit(word_features ,three_countries.popular)\n","87bd9511":"names=['normal','popular']","df0e7d31":"c_tf = make_pipeline( word_vectorizer,classifier_popular)\nexplainer_tf = LimeTextExplainer(class_names=names)\n\nexp = explainer_tf.explain_instance(three_countries.title.iloc[10], c_tf.predict_proba, num_features=4, top_labels=1)\nexp.show_in_notebook(text=three_countries.title.iloc[10])\n","402b79f8":"exp = explainer_tf.explain_instance(three_countries.title.iloc[4], c_tf.predict_proba, num_features=5, top_labels=1)\nexp.show_in_notebook(text=three_countries.title.iloc[4])","8a8ef14d":"exp = explainer_tf.explain_instance(three_countries.title.iloc[10035], c_tf.predict_proba, num_features=5, top_labels=1)\nexp.show_in_notebook(text=three_countries.title.iloc[10035])","f9f7a8e7":"import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.tools as tls\n\n\nlabels = list(three_countries.category_names.value_counts().index.values)\nvalues = list(three_countries.category_names.value_counts().values)\n\ntrace = go.Pie(labels=labels, values=values)\n\niplot([trace], filename='basic_pie_chart')","4196a3a9":"three_countries.groupby('category_names')['views'].describe()","73cb0c67":"entertainment_title= three_countries[\"title\"][(three_countries['category_names'] == 'Entertainment')] \nnews_politics_title= three_countries[\"title\"][(three_countries['category_names'] == 'News & Politics')] \npeople_title= three_countries[\"title\"][(three_countries['category_names'] == 'People & Blogs')] \nmusic_title= three_countries[\"title\"][(three_countries['category_names'] == 'Music')] \nsports_title= three_countries[\"title\"][(three_countries['category_names'] == 'Sports')] \ncomedy_title= three_countries[\"title\"][(three_countries['category_names'] == 'Comedy')] ","ade12931":"vectorizer_entertainment_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nentertainment_title_vectorized = vectorizer_entertainment_title.fit_transform(entertainment_title)\nlda_popular_entertainment_title_vectorized = LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\nentertainment_title_vectorized_lda = lda_popular_entertainment_title_vectorized.fit_transform(entertainment_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_popular_entertainment_title_vectorized,entertainment_title_vectorized, vectorizer_entertainment_title, mds='tsne')\ndash","90b7f7e3":"vectorizer_news_politics_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nnews_politics_title_vectorized = vectorizer_news_politics_title.fit_transform(news_politics_title)\nlda_news_politics_title_vectorized= LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\nnews_politics_title_vectorized_lda = lda_news_politics_title_vectorized.fit_transform(news_politics_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_news_politics_title_vectorized,news_politics_title_vectorized, vectorizer_news_politics_title , mds='tsne')\ndash","3939455a":"vectorizer_people_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\npeople_title_vectorized = vectorizer_people_title.fit_transform(people_title)\nlda_people_title_vectorized= LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\npeople_title_vectorized_lda = lda_people_title_vectorized.fit_transform(people_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_people_title_vectorized,people_title_vectorized, vectorizer_people_title , mds='tsne')\ndash","52e913ec":"vectorizer_music_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nmusic_title_vectorized = vectorizer_music_title.fit_transform(music_title)\nlda_music_title_vectorized= LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\nmusic_title_vectorized_lda = lda_music_title_vectorized.fit_transform(music_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_music_title_vectorized,music_title_vectorized, vectorizer_music_title , mds='tsne')\ndash","9e02bd78":"vectorizer_sports_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\nsports_title_vectorized = vectorizer_sports_title.fit_transform(sports_title)\nlda_sports_title_vectorized= LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\nsports_title_vectorized_lda = lda_sports_title_vectorized.fit_transform(sports_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_sports_title_vectorized,sports_title_vectorized, vectorizer_sports_title , mds='tsne')\ndash","10c11aa3":"vectorizer_comedy_title = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ncomedy_title_vectorized = vectorizer_comedy_title.fit_transform(comedy_title)\nlda_comedy_title_vectorized= LatentDirichletAllocation(n_components=7, max_iter=5, learning_method='online',verbose=True)\ncomedy_title_vectorized_lda = lda_comedy_title_vectorized.fit_transform(comedy_title_vectorized )\n\npyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda_comedy_title_vectorized,comedy_title_vectorized, vectorizer_comedy_title , mds='tsne')\ndash","3ae12af9":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(popular)\ninp_sequences[:10]","a3e62599":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","59709b74":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","4f1ec6ac":"model.fit(predictors, label, epochs=50, verbose=5)\n","ffe215bc":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","15c977b3":"print (generate_text(\"Drake\", 5, model, max_sequence_len))\nprint (generate_text(\"united states\", 5, model, max_sequence_len))\nprint (generate_text(\"Bangtan\", 4, model, max_sequence_len))\nprint (generate_text(\"Fergie\", 4, model, max_sequence_len))\nprint (generate_text(\"korea\", 4, model, max_sequence_len))\nprint (generate_text(\"Minnesota\", 4, model, max_sequence_len))","82350c42":"# **Closer look on Titles of Popular Videos**","2bc6e748":"#### For example, the word \"banana\" appears all documents , so its idf is the lowest","78925c24":"<br><br>","e536200e":"<br><br>","558099f1":"![](http:\/\/www.shivambansal.com\/blog\/text-lstm\/2.png)\n\nUnlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a \u2018memory state\u2019 of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n\nThe memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n\nLSTMs have an additional state called \u2018cell state\u2019 through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code. I have added total three layers in the model.\n\nInput Layer : Takes the sequence of words as input\nLSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\nDropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\nOutput Layer : Computes the probability of the best possible next word as output\nWe will run this model for total 50 epoochs but it can be experimented further.\n\n[source](http:\/\/www.shivambansal.com\/blog\/text-lstm\/2.png)","b484bbf3":"<center>**No.1 dislike percentage video among popular videos in the dataset..No offense Fergie fans out there \ud83d\ude43**<\/center> <br>\n<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/V5cOvyDpWfM\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>\n\n\n\n","c417f8e1":"<br> <br>","4f175bd1":"# **EDA\/Data Cleaning**","711ba0a1":"## Latent Dirichlet Allocation (LDA) by Category","937d798a":"<br> <br>","a285465f":"First of all, let's take a look at data :)","b213decd":"**Among popular video, here are top 10 like percentage video. All of them are k-pop stars MV lol** \n\n \n<table><tr>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/PMEkmiQP5bg\/default.jpg\" alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/v9ea5VDQfXg\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/c5_LROaHGtw\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/3-FXW0CW_8o\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/-7tSTUR7FG0\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/J41qe-TM1DY\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/nQySbNGu4g0\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/Q48VduIflPk\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/floMqK_yHf8\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/VM-g_bkFdzo\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<\/tr><\/table>\n\n","1a113818":"[spaCy](https:\/\/spacy.io\/usage\/spacy-101) is a free, open-source library for advanced Natural Language Processing (NLP) in Python. \n\n> \"Tokens\" are usually individual words (at least in languages like English) and \"tokenization\" is taking a text or set of text and breaking it up into individual its words. These tokens are then used as the input for other types of analysis or tasks, like parsing (automatically tagging the syntactic relationship between words). We need to tokenize word so that we can use it for our title generating model\/other cool analysis ([source](https:\/\/www.kaggle.com\/rtatman\/tokenization-tutorial))","ec745c04":"**Among popular video, here are top 10 dislike percentage video** \n\n\n<table><tr>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/V5cOvyDpWfM\/default.jpg\" alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/qu-biRtYEcU\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/Aqx41JrNTSw\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/C-rumHvmqCA\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/ivYp5NMaUY4\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/xZZyckBhCmY\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/zhUmo88gzwg\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/CMA2iF6RuXk\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/wJJqGh2HLM8\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<td> <img src=\"https:\/\/i.ytimg.com\/vi\/eT9eWtb7C4c\/default.jpg\"  alt=\"Drawing\" style=\"width: 700px;\"\/> <\/td>\n<\/tr><\/table>\n","0cd64269":"<center>**NO.1 like percentage video among popular videos in the dataset! Congrat to Bangtan :)) A decent song to listen while coding \ud83d\udc69\u200d\ud83d\udcbb**<\/center> <br>\n<center><iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/PMEkmiQP5bg\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>\n\n","4dce27ec":"# Generating titles by lstm\n\n> Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. ([source](https:\/\/medium.com\/phrasee\/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b))\n<br> <br>\nLanguage modelling requires a sequence input data, as given a sequence (of words\/tokens) the aim is the predict next word.  \n\n","73c68811":"### TfidfVectorizer\n\nTf-idf analyzes the impact of tokens (words) throughout the whole documents. For example, the more times a word appears in a document (each title), the more weight it will have. However, the more documents (titles) the word appears in, it is 'penalized' and the weight is diminished because it is empirically less informative than features that occur in a small fraction of the training corpus ([source](https:\/\/www.kaggle.com\/adamschroeder\/countvectorizer-tfidfvectorizer-predict-comments))\n\n* tf(t)= the term frequency is the number of times the term appears in the document\n* idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'","97623e66":"<br> <br>","c1f243cc":"\n<table>\n<tr><td>0 <\/td><td> 1 <\/td> <td>2 <\/td><td>3 <\/td><td>4 <\/td> <td>5 <\/td><\/tr>\n<tr><td>An <\/td> <td>  apple<\/td><td>  is<\/td><td>not <\/td> <td>  a<\/td><td> banana<\/td><\/tr>\n<\/table>","c86a657e":"# **Finding Outlier Youtube Video**\n\nThe dataset already includes trending Youtube video. However, I am interested in taking a look at popular ones among trending videos. I will use number of view as the variable to define the popularity of video. In the analysis, a popular video means the number of view for the video is more than 1.5 interquartile ranges (IQRs) above the third quartile. "}}