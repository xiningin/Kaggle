{"cell_type":{"10403ff3":"code","64cea1fa":"code","e53ff51b":"code","d4c912f1":"code","6639a4d3":"code","b138ead4":"code","ca534f8a":"code","a9e9f037":"code","7e30c7e0":"code","b5eccb5e":"code","eb5d193d":"code","8127ad65":"code","de830378":"code","ca32f010":"code","e4cc6aef":"code","318fa90f":"code","9b994a94":"code","f999eaf0":"code","0a7a42cf":"code","c4c73c23":"code","5116297d":"code","5d639570":"code","4fd17767":"code","b2d3d2ea":"code","b20572e4":"code","8af0ca7d":"markdown","2a266251":"markdown","d4141cde":"markdown","6fdcb57e":"markdown","b6ee1e45":"markdown","aae353a2":"markdown","c44acda7":"markdown","164a9a6a":"markdown","b536be8d":"markdown","ad028c07":"markdown","33031e92":"markdown"},"source":{"10403ff3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datatable as dt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\npd.options.display.max_rows = 999\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64cea1fa":"%%time\n\ntrain_df = dt.fread('..\/input\/jane-street-market-prediction\/train.csv').to_pandas()\nfeature_tag = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv', index_col=0)\nprint(train_df.shape)\nprint(train_df.columns)\n","e53ff51b":"# Helper functions\ndef display_query(query, show = True):\n    query_raw = feature_tag.query(query)\n    query_compact = query_raw.loc[:, query_raw.any()]\n    if show:\n        display((query_compact*1).style.background_gradient(cmap='Oranges', vmin=0, vmax=1))\n        \n    return query_compact\n\n\ndef check_unique(df, sub_cols):\n    return df.drop_duplicates().equals(df.drop_duplicates(subset=sub_cols))","d4c912f1":"# How many tags for each feature?\ntag_counts = feature_tag.sum(axis=1)\nfig = px.bar(tag_counts, title = 'Tag counts')\nfig.show()","6639a4d3":"# How many times each tag occurs in features:\ntag_counts = feature_tag.sum(axis=0)\nfig = px.bar(tag_counts, title = 'Feature counts')\nfig.show()","b138ead4":"# Overall visualization:\ndisplay((feature_tag*1).style.background_gradient(cmap='Oranges', vmin=0, vmax=1))","ca534f8a":"# You can play around with different query\n# display_query('tag_6') \n# display_query('tag_6 & tag_9')\n# display_query('tag_20 | tag_28')\nfeatures = display_query('(tag_0 | tag_1 | tag_2 | tag_3 | tag_4)&(tag_23)')\n","a9e9f037":"train_df['feature_0'].value_counts()","7e30c7e0":"# Mean resp, ratio of pos vs neg resp for each side:\ndf = pd.DataFrame()\ndf['Mean resp'] = train_df.groupby('feature_0')['resp'].mean()\ndf['Pos resp ratio'] = train_df.groupby('feature_0')['resp'].apply(lambda s: sum(s>0)\/len(s))\n\ndf","b5eccb5e":"# Visualze the distribution of both side:\nrow_index = train_df['feature_0']>0\n\nfig, ax = plt.subplots()\nax.hist(train_df.loc[row_index, 'resp'], label = 'Buy Order', bins=100, alpha = 0.3, density=True)\nax.hist(train_df.loc[~row_index, 'resp'], label = 'Sell Order', bins=100, alpha = 0.3, density=True)\nax.legend()\nplt.show()","eb5d193d":"features = display_query('tag_22')","8127ad65":"# Look at a random sample date:\ndate = 42\nfeature = 'feature_64'\n\nsample_df = train_df.query(f'date == {date}')\nprint(f'Range in date {date}: {min(sample_df[feature]):.4f} - {max(sample_df[feature]):.4f}')\nsample_df[feature].plot()","de830378":"# Range of feature 64 in all dates:\nf_range_df = pd.DataFrame()\nf_range_df['MAX'] = train_df.groupby('date')[feature].max()\nf_range_df['MIN'] = train_df.groupby('date')[feature].min()\nf_range_df = f_range_df.reset_index()\n\npx.line(f_range_df, x='date', y=['MAX', 'MIN'])","ca32f010":"# Spot outlier dates from graphs:\noutlier_dates = [2, 14, 87, 294] # 2 & 294 is abnormally short\n\nprint(f'Average trades in a date: {train_df[\"date\"].value_counts().mean():.2f}')\nprint(train_df.loc[train_df.date.isin(outlier_dates), 'date'].value_counts())","e4cc6aef":"# Consistency check of clock feature:\nreverse_clock = train_df[feature] < train_df[feature].shift(1)\nnew_date = train_df['date'] > train_df['date'].shift(1)\n\nall(reverse_clock == new_date)","318fa90f":"# Finding the lunch gap:\nsub_df = train_df[['date',feature]].copy()\nsub_df[f'{feature}_pre'] = sub_df[feature].shift(1)\ngap_df = sub_df.loc[(sub_df[feature].diff() > 0.5) & (sub_df[feature]>0) & (sub_df[feature]<4), :]\nlunch_start = gap_df[feature+\"_pre\"].mean()\nlunch_end = gap_df[feature].mean()\n\nprint(f'The lunch gap is from {lunch_start:.4f} to {lunch_end:.4f}')\npx.line(gap_df, x='date', y = [feature, feature+'_pre'])\n# any(gap_df.date.duplicated())","9b994a94":"# Visualization of number of trades during a trading date:\nfig, ax = plt.subplots()\nfor i in range(10, 18):\n    sample_ser = train_df.loc[train_df.date==i, feature]\n    ax.scatter(x=sample_ser, y = list(range(len(sample_ser))), label = f'Date {i}', alpha=0.2, s=1)\n\nax.legend(markerscale = 10)\nplt.show()","f999eaf0":"basic_query = '(tag_0|tag_1|tag_2|tag_3|tag_4)'\nadd_query = '&(tag_6|tag_23)'\nfeatures = display_query(basic_query+add_query)\n","0a7a42cf":"date = 12\nx_col = 'feature_64'\nsample_df = train_df.query(f'date == {date}')\ndf = pd.DataFrame()\ndf['time'] = sample_df[x_col]\nlunch_end = 1.3769\n\ncols = []\nfor i in range(5):\n    y_col = features.index[features[f'tag_{i}']]\n    count_col = f'NA_counts_tag_{i}'\n    cols.append(count_col)\n    df[count_col] = sample_df[y_col].isnull().sum(axis=1)\n    \n    missing_time = df.loc[df[count_col]>0, 'time']\n    if len(missing_time)>0:\n        window = max(missing_time) - lunch_end\n        print(f'Estimated window len of tag_{i}: {window:.4f}')\n    else:\n        print(f'Fail at date {date}')\n\npx.line(df, x='time', y=cols)","c4c73c23":"def estimate_window_len(date, tag):\n    x_col = 'feature_64'\n    sample_df = train_df.query(f'date == {date}')\n    features = display_query(f'(tag_23|tag_6) & {tag}', show=False)\n    y_cols = features.index\n    \n    df = pd.DataFrame()\n    df['time'] = sample_df[x_col]\n    df['NA_counts'] = sample_df[y_cols].isnull().sum(axis=1)\n    missing_time = df.loc[df['NA_counts']>0, 'time']\n    if len(missing_time)>0:\n        window = max(missing_time) - lunch_end\n        if window > 0:\n            return window\n        else:\n            return 0\n    else:\n        return np.nan\n\n# estimate_window_len(12, 'tag_2')\n\nestimate_df = pd.DataFrame()\noutlier_dates = [2, 14, 87, 294]\nfor i in range(0, 5):\n    for date in range(50):\n        if date in outlier_dates:\n            continue\n        estimate_df.loc[date, f'tag_{i}'] = estimate_window_len(date, f'tag_{i}')\n\nprint('Estimate windows: (first 50 dates)')\nprint(estimate_df.median())\n\n\nestimate_df = pd.DataFrame()\noutlier_dates = [2, 14, 87, 294]\nfor i in range(0, 5):\n    for date in range(450, 500):\n        if date in outlier_dates:\n            continue\n        estimate_df.loc[date, f'tag_{i}'] = estimate_window_len(date, f'tag_{i}')\n\nprint('Estimate windows: (last 50 dates)')\nprint(estimate_df.median())\n        ","5116297d":"date = 12\nid_cols = ['feature_41', 'feature_42', 'feature_43']\n\nsample_df = train_df.query(f'date == {date}').copy()\nsample_df['stock_id'] = sample_df['feature_41'].astype(str) +\"_\"+sample_df['feature_42'].astype(str) +\"_\"+ sample_df['feature_43'].astype(str)\nsample_df[id_cols+['stock_id']]\n\n","5d639570":"sample_df['stock_id'].value_counts()","4fd17767":"features = display_query('tag_5')\nfeature_name = features.index.values","b2d3d2ea":"# Relation of tag 5 features support the identification:\nfor i in range(features.shape[0]\/\/2):\n    col_x = feature_name[2*i]\n    col_y = feature_name[2*i+1]\n    fig = px.scatter(sample_df, x=col_x, y=col_y, color = 'stock_id')\n    fig.show()","b20572e4":"outlier_dates = [2, 14, 87, 294]\n\ndf = pd.DataFrame()\ndf['date'] = list(range(500))\ndf['trade'] = train_df['date'].value_counts()\ndf['stock'] = train_df.groupby('date').apply(lambda df: len(df[id_cols].value_counts()) )\ndf = df.loc[~df.date.isin(outlier_dates), :]\ndf['ratio'] = df['trade']\/df['stock']\n\ndf.set_index('date').plot(subplots=True)\n\nprint(f'Trades: {df.trade.mean():.2f} with std ({df.trade.std():.2f})')\nprint(f'Stocks: {df.stock.mean():.2f} with std ({df.stock.std():.2f})')\nprint(f'Ratio: {df.ratio.mean():.4f} with std {df.ratio.std():.4f}')","8af0ca7d":"**TL; DR**\n\n* Feature 0: the side of the trade\n* Feature 64: the time in a date\n* Tag 0-4: time window, (length see below, based on feature 64)\n* Feature 41-43: identify a stock in a date\n\n|tag|window|\n|:----|:--------|\n|tag_0|    0.000000|  \n|tag_1|    0.006602|\n|tag_2|    0.020753|\n|tag_3|    0.058880|\n|tag_4|    0.236351|\n","2a266251":"Over long term, the return a holding is slightly positive skew.  \n\nTherefore, the guess here is that \"-1\" indidates a **sell** order on the market, which means we are **buying** if the trade is executed.   \nSimilarly, the \"1\" indicates a **buy** order and we take short position if executed.","d4141cde":"<a id='content'><\/a>\n# Table of Contents\n\n\n* [Explore & Visualization](#section-1)\n* [Feature 0](#section-2)\n<!--     - [Subsection 1](#subsection-one) -->\n<!--     - [Subsection 2](#anything-you-like) -->\n* [Feature 64 & Tag 22](#section-3)\n* [Tag 0-4](#section-4)\n* [Feature 41-43 & Tag 14](#section-5)\n\n* [TODO](#section-100)\n","6fdcb57e":"<a id = \"section-5\"><\/a>\n# Feature 41-43 & Tag 14: Stock in Date\n\nUsing feature 41-45 (or just only feature 45) provide the same identity.\n\n[Back to content](#content)","b6ee1e45":"<a id = \"section-100\"><\/a>\n# TODO:\n\n* use time and stock identification to have better understand of other features and tags.\n* better NA filling methods than mean\/median or naive backward\/forward filling\n* feature engineer is possible after better understanding meanings of tags.\n\n[Back to content](#content)","aae353a2":"<a id = \"section-4\"><\/a>\n# Tag 0-4: Time windows\n\n[Back to content](#content)","c44acda7":"<a id=\"section-1\"><\/a>\n# Explore & Visualization\n[Back to content](#content)","164a9a6a":"<a id=section-2><\/a>\n# Feature 0: Side of trade\n\nA binary feature with value 1 and -1, with roughly same number of rows.\n\nEducated guess: side of the trade, i.e. buy\/sell the stock\n\n[Back to content](#content)","b536be8d":"![Decoding](http:\/\/www.daskeyboard.com\/blog\/decode-our-das-keyboard-holiday-message-and-win\/decryptthemessage-2\/)","ad028c07":"Inspired by the excellent notebooks [\"De-anonymization: Time Aggregation Tags\"](https:\/\/www.kaggle.com\/gregorycalvez\/de-anonymization-time-aggregation-tags\/notebook#De-anonymization:-Time-Aggregation-Tags) and [\"De-anonymization: Price, Quantity, Stocks\"](https:\/\/www.kaggle.com\/gregorycalvez\/de-anonymization-price-quantity-stocks). (I am wondering how I can @author of notebook, sorry about that.) Here a notebook sharing my insights about the features and the meaning of tags. Comments and ideas are welcome!","33031e92":"<a id=section-3><\/a>\n# Feature_64 & Tag 22: Intraday time\n\n[Back to content](#content)"}}