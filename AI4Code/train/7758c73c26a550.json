{"cell_type":{"08084627":"code","3da8bf90":"code","68bf80f2":"code","f070b3a5":"code","b27de72d":"code","7d8751e4":"code","693b980b":"code","285ac1b0":"code","b7183f57":"code","2f807f34":"code","ce74ab6b":"code","61938dd4":"code","fd53b065":"code","ea19c7b6":"code","064bf2f0":"code","810132cd":"code","62cea9be":"code","71ca6414":"code","48382d77":"code","7e260e22":"code","b3469552":"code","af97d85f":"code","04b15b55":"code","089fe009":"code","3eeddeb0":"markdown","820f1510":"markdown"},"source":{"08084627":"!pip install rudalle==0.4.0 > \/dev\/null","3da8bf90":"from rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip\nfrom rudalle.image_prompts import ImagePrompts\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip\nfrom rudalle.utils import seed_everything","68bf80f2":"!nvidia-smi","f070b3a5":"device = 'cuda'\ndalle = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)","b27de72d":"# realesrgan = get_realesrgan('x4', device=device)\ntokenizer = get_tokenizer()\nvae = get_vae()  # .to(device)\n# ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')\n# ruclip = ruclip.to(device)","7d8751e4":"from collections import defaultdict\nimport requests\nfrom PIL import Image\nimport torch\n\nmannequin_woman_url = 'https:\/\/manekens.ru\/storage\/app\/uploads\/public\/34\/5d25eabf21090114407732.jpg'\nmannequin_man_url = 'https:\/\/ncm-torg.ru\/wp-content\/uploads\/2019\/08\/CGE-11.jpg'\nophanim_url = 'https:\/\/cdn.discordapp.com\/attachments\/730484623028519072\/916667249736183828\/unknown.png'\ncat_url = 'https:\/\/media.discordapp.net\/attachments\/730484623028519072\/916667866839912478\/Untitled3_20211122123019.png'\n\ndownload = lambda x: Image.open(requests.get(x, stream=True).raw).resize((256, 256))\n\nmannequin_woman = download(mannequin_woman_url)\nmannequin_man = download(mannequin_man_url)\nophanim = download(ophanim_url)\ncat = download(cat_url)","693b980b":"from tqdm.auto import tqdm\nimport os\n\n\ninput_dir = \"\/kaggle\/input\/flickrfaceshq-dataset-nvidia-resized-256px\/resized\"\nimgs = []\ntrain_imgs = 500\nfor image in tqdm(os.listdir(input_dir)[:train_imgs]):\n    imgs.append(Image.open(os.path.join(input_dir, image)))\nshow(imgs[:100], 10)","285ac1b0":"vae = vae.to(device)\nvae.model.device = device","b7183f57":"show([ophanim, cat], 2)\nshow([mannequin_woman, mannequin_man], 2)","2f807f34":"import torch\nfrom tqdm.auto import tqdm\nfrom copy import copy\nimport more_itertools\n\n\ndef encode(image_prompts, bs=3):\n    image_prompts = copy(image_prompts)\n    with torch.no_grad():\n        for i, e in enumerate(tqdm(list(more_itertools.chunked(image_prompts, bs)))):\n            *_, (*_, prompt) = vae.model.encode(torch.cat(list(ImagePrompts._preprocess_img(vae.model, x) for x in e), dim=0))\n            image_prompts[i*bs:i*bs+bs] = list(prompt.reshape((len(e), -1)))\n    return image_prompts","ce74ab6b":"faces = encode(imgs[:400], bs=20)","61938dd4":"# uncomment for more JIT\n# for layer in dalle.module.transformer.layers:\n#     layer.mlp.activation = \"gelu\"","fd53b065":"from functools import reduce\nimport torch.nn.functional as F\nfrom rudalle.dalle.utils import exists, is_empty\nfrom einops import rearrange\n\n\n# idk why but this is necessary\nclass Layer(torch.nn.Module):\n    def __init__(self, x, f, *args, **kwargs):\n        super(Layer, self).__init__()\n        self.x = x\n        self.f = f\n        self.args = args\n        self.kwargs = kwargs\n    \n    def forward(self, x):\n        return self.f(self.x(x, *self.args, **self.kwargs))\n\n\ndef forward(\n        self,\n        e,\n        layernorms,\n        input_ids,\n        attention_mask,\n        return_loss=False,\n        has_cache=False,\n        use_cache=False,\n        gradient_checkpointing=False\n):\n    text = input_ids[:, :self.text_seq_length]\n    text_range = torch.arange(self.text_seq_length)\n    text_range += (self.vocab_size - self.text_seq_length)\n    text_range = text_range.to(self.device)\n    text = torch.where(text == 0, text_range, text)\n    # some hardcode :)\n    text = F.pad(text, (1, 0), value=2)\n    text_embeddings = self.text_embeddings(text) + \\\n        self.text_pos_embeddings(torch.arange(text.shape[1], device=self.device))\n    text_embeddings = torch.cat((text_embeddings[:, :e.shape[0]] + e, text_embeddings[:, e.shape[0]:]), dim=1)\n#     text_embeddings += e\n\n    image_input_ids = input_ids[:, self.text_seq_length:]\n\n    if exists(image_input_ids) and not is_empty(image_input_ids):\n        image_embeddings = self.image_embeddings(image_input_ids) + \\\n            self.get_image_pos_embeddings(image_input_ids, past_length=0)\n        embeddings = torch.cat((text_embeddings, image_embeddings), dim=1)\n    else:\n        embeddings = text_embeddings\n    # some hardcode :)\n    if embeddings.shape[1] > self.total_seq_length:\n        embeddings = embeddings[:, :-1]\n\n    alpha = 0.1\n    embeddings = embeddings * alpha + embeddings.detach() * (1-alpha)\n\n    attention_mask = attention_mask[:, :, :embeddings.shape[1], :embeddings.shape[1]]\n    t = self.transformer\n    layers = []\n    if not layernorms:\n        norm_every = 0\n    else:\n        norm_every = len(t.layers) \/\/ len(layernorms)\n    for i in range(len(t.layers)):\n        layers.append(Layer(t.layers[i],\n                            lambda x: \n                                x[0] * layernorms[i \/\/ norm_every][0] + \n                                layernorms[i \/\/ norm_every][1] if norm_every and i % norm_every == 0 else x[0],\n                            torch.mul(attention_mask, t._get_layer_mask(i)[:attention_mask.size(2), :attention_mask.size(3),]),\n                            use_cache=False, has_cache=False))\n    if gradient_checkpointing:  # don't use this under any circumstances\n        # actually please do\n        # i just spent 3 hours debugging this \n        embeddings = torch.utils.checkpoint.checkpoint_sequential(layers, 6, embeddings)\n        transformer_output = embeddings\n        present_has_cache = False\n    else:\n        hidden_states = embeddings\n        for i in range(len(t.layers)):\n            mask = torch.mul(attention_mask, t._get_layer_mask(i)[:attention_mask.size(2), :attention_mask.size(3)])\n            hidden_states, present_has_cache = t.layers[i](hidden_states, mask, has_cache=has_cache, use_cache=use_cache)\n        transformer_output = hidden_states\n    transformer_output = self.transformer.final_layernorm(transformer_output)\n\n    logits = self.to_logits(transformer_output)\n    if return_loss is False:\n        return logits, present_has_cache\n\n    labels = torch.cat((text[:, 1:], image_input_ids), dim=1).contiguous().long()\n    logits = rearrange(logits, 'b n c -> b c n')\n\n    text_logits = logits[:, :self.vocab_size, :self.text_seq_length].contiguous().float()\n    image_logits = logits[:, self.vocab_size:, self.text_seq_length:].contiguous().float()\n    \n    loss_text = F.cross_entropy(\n        text_logits,\n        labels[:, :self.text_seq_length])\n    loss_img = F.cross_entropy(\n        image_logits,\n        labels[:, self.text_seq_length:])\n\n    loss = (loss_text + self.loss_img_weight * loss_img) \/ (self.loss_img_weight + 1)\n    return loss, {'text': loss_text.data.detach().float(), 'image': loss_img}","ea19c7b6":"import random\nimport os\nfrom glob import glob\nfrom os.path import join\n\nimport cv2\nimport torch\nimport torchvision\nimport transformers\nimport more_itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output, display\nfrom ipywidgets import Output  # https:\/\/github.com\/tqdm\/tqdm\/issues\/818\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nfrom copy import copy\nfrom rudalle import utils\n\n\ndef param(x):\n    return torch.nn.Parameter(x.detach().clone().float(), requires_grad=True)\n\n\ndef find_prompt(dalle, vae, text=\"\", image_prompts=[], init_emb=None, bs=8, grad_acc=4, seed=None,\n                epochs=1, lr=1e-2, gradient_checkpointing=False, crop=32, lns=0):\n    # TODO docstring\n    if seed is not None:\n        utils.seed_everything(seed)\n    print(\"encoding...\")\n\n    try:\n        image_prompts = encode(image_prompts)\n    except AttributeError:\n        pass\n    \n    vocab_size = dalle.get_param('vocab_size')\n    text_seq_length = dalle.get_param('text_seq_length')\n    image_seq_length = dalle.get_param('image_seq_length')\n    total_seq_length = dalle.get_param('total_seq_length')\n    device = dalle.get_param('device')\n    \n    text_ids = tokenizer.encode_text(text, text_seq_length)\n\n    dalle.zero_grad()\n    dalle.requires_grad_(False)\n    dalle.eval()\n    \n    emb = dalle.module.text_pos_embeddings.weight\n    if init_emb is not None:\n        e = init_emb\n    else:\n        e = torch.randn_like(emb)\n        e = e[:crop]\n    e = param(e)\n    \n    layernorms = []  # layernorms\n    for i in range(lns):\n        layernorms.append(param(torch.cat((\n            torch.zeros_like(emb[:1]),\n            torch.ones_like(emb[:1]))\n        )))\n\n    optim = torch.optim.Adam(layernorms + [e], lr=lr)\n#     scaler = torch.cuda.amp.GradScaler()\n    print(\"training...\")\n    i = 0\n    tl = 0\n    tls = []\n    out_ipy = Output()\n    display(out_ipy)\n    try:\n        image_prompts *= epochs\n        random.shuffle(image_prompts)\n        bar = tqdm(list(more_itertools.chunked(image_prompts, bs)))\n        for chunk in bar:\n    #         print(e)\n            optim.zero_grad()\n        \n            with torch.cuda.amp.autocast():\n                chunk_bs = len(chunk)\n                attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))\n                out = torch.zeros((chunk_bs, text_seq_length))\n                out += torch.stack((text_ids,) * chunk_bs, dim=0)\n                input_ids = torch.cat((out.to(device), torch.stack(chunk, dim=0)), dim=1).long().to(device)\n                _, loss = forward(dalle.module, e.half(), [l.half() for l in layernorms], input_ids, attention_mask.half(),\n                                  return_loss=True, use_cache=False, gradient_checkpointing=gradient_checkpointing)\n            loss = loss[\"image\"]\n            loss.backward()\n            tl += loss.item()\n    #             scaler.scale(loss).backward()\n    #             scaler.step(optim)\n    #         scaler.update()\n            if i % grad_acc == grad_acc - 1:\n                tl \/= grad_acc\n                bar.set_description(f\"loss: {tl}\")\n                tls.append(tl)\n                optim.step()\n                optim.zero_grad()\n                tl = 0\n            if i % (grad_acc ** 2) == grad_acc ** 2 - 1:\n                with out_ipy:\n                    clear_output(wait=True)\n                    plt.plot(tls)\n                    plt.show()\n            i += 1\n    #             dalle.zero_grad()\n    except KeyboardInterrupt:\n        pass\n    return e, layernorms","064bf2f0":"seed_everything(32)\n# fine-tuning with faces dataset\ne_faces, n_faces = find_prompt(dalle, vae, image_prompts=faces, bs=6, epochs=1,\n                               crop=128, lns=4, lr=5e-2, grad_acc=1, gradient_checkpointing=True)","810132cd":"from functools import partial\nfrom math import sqrt\n# from inspect import getsource\n# print(getsource(vae.decode))\ndef decode(self, img_seq):\n    b, n = img_seq.shape\n    one_hot_indices = torch.nn.functional.one_hot(img_seq.to(self.model.device), num_classes=self.num_tokens).float()\n    z = (one_hot_indices @ self.model.quantize.embed.weight)\n    z = rearrange(z, 'b (h w) c -> b c h w', h=int(sqrt(n)))\n    img = self.model.decode(z)\n    img = (img.clamp(-1., 1.) + 1) * 0.5\n    return img\nvae.decode = partial(decode, vae)","62cea9be":"from functools import partial\ndef generate_prompt(model, tune, txt=\"\", top_k=1024, top_p=0.98, images_num=3):\n    e, n = tune\n#     e = torch.zeros_like(e)\n    model.module.forward = lambda *args, **kwargs: [t.float() if isinstance(t, torch.Tensor) else t\n                                                    for t in forward(dalle.module, e.half(), [l.half() for l in n],\n                                                                     *[x.half() if x.dtype == torch.float32 else x for x in args],\n                                                                     **kwargs)]\n    seed_everything(42)\n    return generate_images(\n        txt,\n        tokenizer,\n        model,\n        vae,\n        top_k=top_k,\n        images_num=images_num,\n#         image_prompts=ImagePrompts(mannequin_woman, top=8),\n        top_p=top_p,\n        use_cache=True\n    )[0]\n    # top_images, _ = cherry_pick_by_clip(pil_images, text, ruclip, ruclip_processor, device=device, count=6)\n    # all_images[name] += super_resolution(top_images, realesrgan)","71ca6414":"show(generate_prompt(dalle, (e_faces, n_faces)), 3)","48382d77":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n!nvidia-smi","7e260e22":"tuned_cat = find_prompt(dalle, vae, text=\"\u0441\u0435\u0440\u044b\u0439 \u043a\u043e\u0442\", image_prompts=[cat.resize((256, 256))], bs=4, epochs=100, crop=32, lr=5e-2, grad_acc=1, gradient_checkpointing=True)","b3469552":"show(generate_prompt(dalle, tuned_cat), 3)","af97d85f":"import gc\ngc.collect()\ntorch.cuda.empty_cache()\n!nvidia-smi","04b15b55":"e_woman = find_prompt(dalle, vae, image_prompts=[mannequin_woman.resize((256, 256))], bs=4, epochs=80, crop=32, lr=5e-2, grad_acc=1, gradient_checkpointing=True)","089fe009":"show(generate_prompt(dalle, e_woman), 3)","3eeddeb0":"## image prompts to dress manequins by ruDALLE","820f1510":"# ruDALLE tuning\n\nby @nev#4905"}}