{"cell_type":{"c5e43847":"code","143434a0":"code","2a91ff4e":"code","a463847a":"code","1658ed79":"code","540012bb":"code","9c80d328":"code","760f7fd8":"code","d60dde80":"code","a1a9cf63":"code","0ae62a20":"code","56fc8303":"code","1d252f9b":"code","83851175":"code","069cbde3":"code","39f1dabd":"code","7b9eda7b":"code","1fd69f52":"markdown","de8755e2":"markdown","331296d0":"markdown","7abc19b0":"markdown","9739da06":"markdown","67fa318f":"markdown","ce693d53":"markdown","654d621c":"markdown","09a02b6a":"markdown","4f472bd8":"markdown","47267907":"markdown"},"source":{"c5e43847":"!pip install transformers","143434a0":"# Standard imports\nimport os\nfrom pprint import pprint\nimport json\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom tqdm import trange\nfrom colorama import Fore, Back, Style\n\n# For plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# For Evaluation and model selection \nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\n# For model building\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# For Transformer\nimport transformers\nfrom transformers import AutoTokenizer, BertModel\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","2a91ff4e":"cfg = {}\ncfg['train_csv'] = \"..\/input\/commonlitreadabilityprize\/train.csv\"\ncfg['test_csv'] = \"..\/input\/commonlitreadabilityprize\/test.csv\"\ncfg['sample_sub'] = \"..\/input\/commonlitreadabilityprize\/sample_submission.csv\"\ncfg['epochs'] = 20\ncfg['max-len'] = 256\ncfg['train_bs'] = 8\ncfg['val_bs'] = 16\ncfg['active-model'] = 'bert-base-uncased'\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nscaler = torch.cuda.amp.GradScaler()\n\npprint(cfg)\nprint(DEVICE)\nprint(scaler)","a463847a":"data_df = pd.read_csv(cfg['train_csv'])\ndata_df","1658ed79":"data_df['excerpt'].values[0]","540012bb":"txt_len = []\nfor d in data_df.excerpt.values:\n  txt_len.append(len(d.split(\" \")))\n\nfig = px.histogram(x = txt_len)\nfig.update_layout(\n    {\n    'title' : 'Distribution of text length',\n    'title_x' : 0.5,\n    'xaxis_title' : 'Length of text'\n\n    }\n)\n\nfig.update_xaxes(title_font_family=\"Arial\")\nfig.show()","9c80d328":"# Creating a data loader class\n\nclass BERTDataset(Dataset):\n    def __init__(self, txt, target):\n        self.txt = txt\n        self.target = target\n        self.tokenizer = AutoTokenizer.from_pretrained(cfg['active-model'])\n        self.max_len = cfg['max-len']\n    \n    def __len__(self):\n        return len(self.txt)\n    \n    def __getitem__(self, idx):        \n        # I have oberverd that some of the sentences have new line character\n        txt = str(self.txt[idx]).replace(\"\\n\", \"\")\n        \n        # Inputs from hugging face tokenizer\n        inputs = self.tokenizer.encode_plus(\n            txt, \n            add_special_tokens = True,\n            max_length = self.max_len,\n            truncation = True,\n            padding = 'max_length',\n        )\n        \n        \n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"] \n        \n        return {\n            'ids' : torch.tensor(ids, dtype = torch.long),\n            'mask' : torch.tensor(mask, dtype = torch.long),\n            'token_type_ids' : torch.tensor(token_type_ids, dtype = torch.long),\n            'target' : torch.tensor(self.target[idx], dtype = torch.float)\n        }","760f7fd8":"class BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        \n        self.bert = BertModel.from_pretrained(cfg['active-model'])\n        self.drop = nn.Dropout(0.2)\n        self.relu = nn.ReLU()\n        self.linear = nn.Linear(768, 10)\n        self.out = nn.Linear(10, 1)\n    \n    def forward(self, ids, mask):\n        outs= self.bert(\n            ids, \n            attention_mask = mask,            \n        )\n        \n        mean_pool = torch.mean(outs[0], 1)\n        # max_pool = torch.max(outs[0], 1)\n        # cat_ = torch.cat((mean_pool, max_pool), 1)\n                \n        # x = self.linear(cat_)\n        x = self.linear(mean_pool)\n        x = self.relu(x)\n        x = self.drop(x)\n        output = self.out(x)\n        \n        return output ","d60dde80":"# Creating our loss function\ndef loss_fn(outputs, targets):\n    outputs = outputs.squeeze(-1)\n    mse = nn.MSELoss()\n    rmse = torch.sqrt(mse(outputs, targets))\n    return rmse","a1a9cf63":"def create_folds(data, target=\"target\", num_splits = 5): \n    data[\"kfold\"] = -1 \n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Applying Sturg's rule to calculate the no. of bins for target\n    num_bins = int(1 + np.log2(len(data))) \n\n    data.loc[:, \"bins\"] = pd.cut(data[target], bins=num_bins, labels=False) \n    \n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)): \n        data.loc[v_, 'kfold'] = f\n        \n    data = data.drop([\"bins\"], axis = 1)         \n    return data \n\ncv_data_df = create_folds(data_df, target = 'target', num_splits = 5)\ncv_data_df.kfold.value_counts()","0ae62a20":"def start_train(model, dataLoader, optimizer, scheduler):\n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allPreds = []; allTargets = []\n\n    for d in dataLoader:\n        losses = []\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            ids = d['ids'].to(DEVICE, non_blocking = True)\n            mask = d['mask'].to(DEVICE, non_blocking = True)\n            target = d['target'].to(DEVICE, non_blocking = True)\n\n            preds = model(ids, mask)\n            cur_loss = loss_fn(preds, target)\n\n            losses.append(cur_loss.item())\n            allPreds.append(preds.squeeze(-1).detach().cpu().numpy())\n            allTargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(cur_loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        scheduler.step()\n\n    allPreds = np.concatenate(allPreds)\n    allTargets = np.concatenate(allTargets)\n\n    loss = np.mean(losses)\n\n    return allPreds, loss\n\ndef validation(model, dataLoader):\n    model.eval()\n\n    allPreds = []; allTargets = []\n\n    for d in dataLoader:\n        losses = []\n\n        with torch.no_grad():\n            ids = d['ids'].to(DEVICE, non_blocking = True)\n            mask = d['mask'].to(DEVICE, non_blocking = True)\n            target = d['target'].to(DEVICE, non_blocking = True)\n\n            preds = model(ids, mask)\n            cur_loss = loss_fn(preds, target)\n\n            losses.append(cur_loss.item())\n            allPreds.append(preds.squeeze(-1).detach().cpu().numpy())\n            allTargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n\n    allPreds = np.concatenate(allPreds)\n    allTargets = np.concatenate(allTargets)\n\n    loss = np.mean(losses)\n\n    return allPreds, loss\n        ","56fc8303":"# Creating the training process\ndef run():\n    \n    # torch.cuda.empty_cache()\n    for fold in range(5):\n        print(Fore.GREEN)\n        print(\"_ \"*20, \"\\n\")\n        print(f\"{' '*11}Current Fold : {fold}\")\n        print(\"_ \"*20, \"\\n\")\n\n        train_df = cv_data_df.loc[cv_data_df.kfold != fold].reset_index(drop = True)\n        val_df = cv_data_df.loc[cv_data_df.kfold == fold].reset_index(drop = True)\n\n        # Building train and val dataloader\n        train_dataset = BERTDataset(\n            txt = train_df.excerpt.values,\n            target = train_df.target.values,\n        )\n\n        val_dataset = BERTDataset(\n            txt = val_df.excerpt.values,\n            target = val_df.target.values,\n        )\n\n        train_data_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size = cfg['train_bs'],\n            num_workers = 4,\n            shuffle = True,\n            pin_memory = True\n        )\n\n        val_data_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            batch_size = cfg['val_bs'],\n            num_workers = 4,\n            shuffle = True,\n            pin_memory = True\n        )\n\n\n        # Calling model\n        model = BERTBaseUncased()\n        model.to(DEVICE)\n        LR = 2e-5\n        optimizer = AdamW(model.parameters(),\n                         LR,\n                         betas = (0.9, 0.999),\n                         weight_decay = 1e-2)\n\n\n        train_steps = len(train_df)\/cfg['train_bs']*cfg['epochs']\n        num_steps = int(train_steps*0.1)\n\n        scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                   num_steps,\n                                                   train_steps)\n\n        train_losses = []; val_losses = []; best = 1e8\n\n#         for epoch in trange(cfg['epochs'], bar_format=\"{l_bar}%s{bar:50}%s{r_bar}\" % (Fore.CYAN, Fore.RESET), position = 0, leave = True):\n        for epoch in range(cfg['epochs']):  \n            print()\n            print(Fore.YELLOW,\"#\"*40)\n            print(Fore.YELLOW,\"#\"*10,\" EPOCH #\",epoch+1,\" \",\"#\"*15)\n            print(Fore.YELLOW,\"#\"*40)\n            print()\n            \n            trainPreds, trainloss = start_train(model,\n                                          train_data_loader,\n                                          optimizer,\n                                          scheduler                                         \n                                         )\n            valPreds, valloss = validation(model,\n                                          val_data_loader                                  \n                                         )\n\n            oldbest = best\n            best = min(best, valloss)\n\n            print(f\" TrainLoss : {trainloss}\")\n            print(f\" ValLoss : {valloss}\")\n            print(f\" Best : {best}\")\n\n            if oldbest > best:\n                model_state = {\n                    'state_dict' : model.state_dict(),\n                    'optimizer_dict' : optimizer.state_dict(),\n                    'bestScore' : best\n                }\n                \n                # Makes a folder to save our Model weights\n                if not os.path.exists(\".\/BestBERTBaseUncasedModels\"):\n                    os.mkdir(\".\/BestBERTBaseUncasedModels\")\n                torch.save(model_state, f\".\/BestBERTBaseUncasedModels\/model_{fold}.pth\")\n  \n    ","1d252f9b":"run()","83851175":"!pip install kaggle","069cbde3":"!cp ..\/input\/kaggle-token\/kaggle_token.json .\/\n!mv .\/kaggle_token.json .\/kaggle.json\n\n!ls -l ..\/..\/root\n!cp .\/kaggle.json ..\/..\/root\/\n!ls ..\/..\/root\n\n\n!mkdir ..\/..\/root\/.kaggle\n!mv ..\/..\/root\/kaggle.json ..\/..\/root\/.kaggle\/kaggle.json\n\n!chmod 600 \/root\/.kaggle\/kaggle.json\n!kaggle datasets init -p .\/BestBERTBaseUncasedModels","39f1dabd":"!cat .\/BestBERTBaseUncasedModels\/dataset-metadata.json\nwith open(\".\/BestBERTBaseUncasedModels\/dataset-metadata.json\", 'r+') as file_:\n    meta_data = json.load(file_)\n    meta_data['title'] = 'BestBERTBaseUncasedModelsCLRP'\n    meta_data['id'] = 'hotsonhonet\/BestBERTBaseUncasedModelsCLRP'\n    file_.seek(0)        \n    json.dump(meta_data, file_, indent=4)\n    file_.truncate()\n    \nprint(meta_data['title'], meta_data['id'])\nprint(\"\\nAfter modification...\\n\")\n!cat .\/BestBERTBaseUncasedModels\/dataset-metadata.json","7b9eda7b":"!kaggle datasets create -p .\/BestBERTBaseUncasedModels","1fd69f52":"# Loss function : RMSE","de8755e2":"# Importing Modules","331296d0":"# Training and evaluation functions","7abc19b0":"# Saving Model weight files","9739da06":"## Distribution of Text length","67fa318f":"From the chart we can see that most of the text's length are in the range of 170-180. So, we should choose our model accordingly","ce693d53":"# Loading train and test Data","654d621c":"# CONFIG","09a02b6a":"# Training","4f472bd8":"# Stratifying our dataset for train and validation\n\n[By Abhishek Thakur](http:\/\/https:\/\/www.kaggle.com\/abhishek)\n\n![image.png](attachment:2eb27814-bb5c-496c-bfb0-09a431de6353.png)         ","47267907":"# Building Model"}}