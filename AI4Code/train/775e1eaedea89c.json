{"cell_type":{"9ae204f3":"code","8658fdcf":"code","7a2767b8":"code","94fde439":"code","e089d41b":"code","3dd4360d":"code","71c3c6e9":"code","66287830":"code","74f2a3db":"code","6edf0444":"code","d015ea6d":"code","0f99bb61":"code","deb4ad64":"code","c21d9476":"code","4c443c47":"code","82b0441e":"code","614f384a":"code","c8972db0":"code","3a06c8fb":"code","94c903a9":"code","cddc4db8":"code","457a0f1d":"code","4fc8c8ae":"code","86b9aada":"code","627e7e52":"code","707b4972":"code","00d78d6c":"code","5e63cf6e":"code","dadc2c8b":"code","18ba274a":"code","30a7c590":"code","1d44f567":"code","31914a90":"code","b057331a":"code","37cfbed7":"code","2c78a905":"code","d5f0d03c":"code","e1f7a7e2":"code","e1f815a7":"code","79fbc122":"code","dd88280d":"code","ba60ba1a":"markdown","e00aaba8":"markdown","4ccac22f":"markdown","a89221f1":"markdown","881c28c4":"markdown","6d55c15b":"markdown","75866e1c":"markdown","6be2aa1a":"markdown","efcda909":"markdown","d21cb0da":"markdown"},"source":{"9ae204f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport networkx as nx\nimport os\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import TruncatedSVD, NMF,LatentDirichletAllocation\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,accuracy_score,log_loss\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport seaborn as sns\nfrom tqdm import tqdm as tqdm_base\nfrom sklearn.model_selection import train_test_split\n#from gensim.models.ldamodel import LdaModel\nfrom sklearn import preprocessing\n#from gensim.corpora import Dictionary\nimport umap\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8658fdcf":"BASE_PATH = \"\/kaggle\/input\/nlp-getting-started\/\"","7a2767b8":"train =pd.read_csv(BASE_PATH + \"train.csv\")\ntrain.head()","94fde439":"test =pd.read_csv(BASE_PATH + \"test.csv\")\ntest.head()","e089d41b":"train.dtypes","3dd4360d":"train['keyword'].value_counts(dropna=False, normalize=True).head()","71c3c6e9":"train['location'].value_counts(dropna=False, normalize=True).head()","66287830":"plt.hist(train['target'], label='train');\nplt.legend();\nplt.title('Distribution of target labels');","74f2a3db":"#train['location'].fillna('NULL', inplace=True)\n#train['keyword'].fillna('NULL', inplace=True)","6edf0444":"temp = train.dropna()\ntemp = temp.reset_index()","d015ea6d":"temp.shape","0f99bb61":"g = nx.DiGraph()\nfor x in range(500):\n    g.add_edge(temp['location'][x],temp['keyword'][x], weight=x,capacity=5,length = 100) \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","deb4ad64":"g = nx.DiGraph()\nfor x in range(500,1000):\n    g.add_edge(train['location'][x], train['keyword'][x]) \n    \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","c21d9476":"g = nx.DiGraph()\nfor x in range(1000,1500):\n    g.add_edge(train['location'][x], train['keyword'][x]) \n    \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","4c443c47":"g = nx.DiGraph()\nfor x in range(1500,2000):\n    g.add_edge(train['location'][x], train['keyword'][x]) \n    \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","82b0441e":"g = nx.DiGraph()\nfor x in range(2000,2500):\n    g.add_edge(train['location'][x], train['keyword'][x]) \n    \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","614f384a":"g = nx.DiGraph()\nfor x in range(2500,3000):\n    g.add_edge(train['location'][x], train['keyword'][x]) \n    \npos=nx.spring_layout(g,k=0.15,)\nplt.figure(figsize =(20, 20)) \nnx.draw_networkx(g,pos,alpha=0.8,node_color='red',node_size=25,font_size=9, with_label = True) ","c8972db0":"# Lets try out PorterStemmer first\nstemmer_ = PorterStemmer()\nprint(\"The stemmed form of running is: {}\".format(stemmer_.stem(\"running\")))\nprint(\"The stemmed form of runs is: {}\".format(stemmer_.stem(\"runs\")))\nprint(\"The stemmed form of run is: {}\".format(stemmer_.stem(\"run\")))","3a06c8fb":"# And then Lemmatization\nlemm = WordNetLemmatizer()\nprint(\"I  case of Lemmatization, running is: {}\".format(lemm.lemmatize(\"running\")))\nprint(\"I  case of Lemmatization, runs is: {}\".format(lemm.lemmatize(\"runs\")))\nprint(\"I  case of Lemmatization, is: {}\".format(lemm.lemmatize(\"run\")))","94c903a9":"def pre_Process_data(documents):\n    '''\n    For preprocessing we have regularized, transformed each upper case into lower case, tokenized,\n    Normalized and remove stopwords. For normalization, we have used PorterStemmer. Porter stemmer transforms \n    a sentence from this \"love loving loved\" to this \"love love love\"\n    \n    '''\n    STOPWORDS = set(stopwords.words('english'))\n    stemmer = PorterStemmer()\n    #lemm = WordNetLemmatizer()\n    Tokenized_Doc=[]\n    print(\"Pre-Processing the Data.........\\n\")\n    for data in documents:\n        review = re.sub('[^a-zA-Z]', ' ', data)\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n        review = url.sub(r'',review)\n        html=re.compile(r'<.*?>')\n        review = html.sub(r'',review)\n        emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n        review = emoji_pattern.sub(r'',review)\n        gen_docs = [w.lower() for w in word_tokenize(review)] \n        tokens = [stemmer.stem(token) for token in gen_docs if not token in STOPWORDS]\n        #tokens = [lemm.lemmatize(token) for token in gen_docs if not token in STOPWORDS]\n        final_=' '.join(tokens)\n        Tokenized_Doc.append(final_)\n    return Tokenized_Doc","cddc4db8":"def Vectorization(processed_data):\n    '''\n    Vectorization is an important step in Natural Language Processing. We have\n    Used Tf_Idf vectorization in this script. The n_gram range for vectorization \n    lies between 2 and 3, that means minimum and maximum number of words in \n    the sequence that would be vectorized is two and three respectively. There\n    are other different types of vectorization algorithms also, which could be added to this \n    function as required.\n    \n    '''\n    vectorizer = TfidfVectorizer(stop_words='english', \n                                    #max_features= 200000, # keep top 200000 terms \n                                    min_df = 3, ngram_range=(2,3),\n                                    smooth_idf=True)\n    X = vectorizer.fit_transform(processed_data)\n    print(\"\\n Shape of the document-term matrix\")\n    print(X.shape) # check shape of the document-term matrix\n    return X, vectorizer","457a0f1d":"def topic_modeling(model,X):\n    '''\n    We have used three types of decomposition algorithm for unsupervised learning, anyone could \n    be selected with the help of the \"model\" parameter. Three of them are TruncatedSVD ,Latent\n    Dirichlet Allocation and Matrix Factorization. This function is useful for comparing\n    different model performances, by switching between different algorithms with the help of \n    the \"model\" parameter and also more algorithms could be easily added to this function.\n    \n    '''\n    components = 900\n    if model=='svd':\n        print(\"\\nTrying out Truncated SVD......\")\n        model_ = TruncatedSVD(n_components=components,n_iter=20)\n        model_.fit(X)\n    if model=='MF':\n        print(\"\\nTrying out Matrix Factorization......\")\n        model_ = NMF(n_components=components, random_state=1,solver='mu',\n                      beta_loss='kullback-leibler', alpha=.1,max_iter=20,\n                      l1_ratio=.5).fit(X)\n        model_.fit(X)\n    if model=='LDA':\n        print(\"\\nTrying out Latent Dirichlet Allocation......\")\n        #Tokenized_Doc=[doc.split() for doc in processed_data]\n        #dictionary = Dictionary(Tokenized_Doc)\n        #corpus = [dictionary.doc2bow(tokens) for tokens in Tokenized_Doc]\n        #model_ = LdaModel(corpus, num_topics=components, id2word = dictionary)\n        model_ = LatentDirichletAllocation(n_components=components,n_jobs=-1,\n                                           max_iter=20,\n                                           random_state=42,verbose=0\n                                          )\n        model_.fit(X)\n    if model=='k-means':\n        print(\"\\nTrying out K-Means clustering......\")\n        true_k = 2\n        model_ = KMeans(n_clusters=components, init='k-means++',max_iter=20, n_init=1)\n        model_.fit(X)\n        \n    X = model_.transform(X)\n    \n    scl = preprocessing.StandardScaler()\n    scl.fit(X)\n    x_scl = scl.transform(X)\n\n    return x_scl","4fc8c8ae":"def Visualize_clusters(X_topics, title):\n    '''\n    This function is used to visualize the clusters generated by our \n    model through unsupervised learning. We have used UMAP for better \n    visualization of clusters.\n    \n    '''\n    #embedding = umap.UMAP(n_neighbors=30,\n    #                        min_dist=0.0,\n    #                        n_components=2,).fit_transform(X_topics)#20\n    embedding = TSNE(n_components=2, \n                     verbose=1, random_state=0, angle=0.75).fit_transform(X_topics)\n    \n    plt.figure(figsize=(20,20))\n    plt.gca().set_aspect('equal', 'datalim')\n    #plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n    plt.title(title,fontsize=16)\n    plt.scatter(embedding[:, 0], embedding[:, 1], \n    c = train['target'], cmap='Spectral', alpha=0.7,\n    s = 20, # size\n    )\n    plt.show()","86b9aada":"processed_data = pre_Process_data(train['text'])","627e7e52":"X, vectorizer = Vectorization(processed_data)","707b4972":"X_transform_1 = topic_modeling('svd',X)","00d78d6c":"Visualize_clusters(X_transform_1, \"Clustering for Truncated SVD\")","5e63cf6e":"#X, vectorizer = Vectorization(processed_data)\nX_transform_2 = topic_modeling('LDA',X)\nVisualize_clusters(X_transform_2, \"Clustering for LDA\")","dadc2c8b":"X_transform_3 = topic_modeling('MF',X)\nVisualize_clusters(X_transform_3, \"Clustering for MF\")","18ba274a":"X_transform_4 = topic_modeling('k-means',X)\nVisualize_clusters(X_transform_4, \"Clustering for K-Means\")","30a7c590":"y = train['target']\n#xtrain, xvalid, ytrain, yvalid = train_test_split(X_transform_3, y, \n#                                                  stratify=y, \n#                                                  random_state=42, \n#                                                  shuffle=True)","1d44f567":"def evaluate_performance(model):\n    kf = StratifiedKFold(n_splits=3,shuffle=True,random_state=42)\n    i=1\n    model_list=[]\n    for train_index,test_index in kf.split(X_transform_3, y):\n        print('{} of KFold {}'.format(i,kf.n_splits))\n        xtrain,xvalid = X_transform_3[train_index],X_transform_3[test_index]\n        ytrain,yvalid = y[train_index],y[test_index]\n        model.fit(xtrain, ytrain)\n        predictions = model.predict(xvalid)\n        print(\"Accuracy score: \"+str(accuracy_score(predictions,yvalid)))\n        print(\"ROC score: \"+str(roc_auc_score(predictions,yvalid)))\n        print(\"Log Loss: \"+ str(log_loss(predictions,yvalid)))\n        i=i+1\n        model_list.append(model)\n    return model_list","31914a90":"clf = lgb.LGBMClassifier(max_depth=12,\n                             learning_rate=0.5,\n                             n_estimators = 1000,\n                             subsample=0.25,\n                           )\nmodel_1 = evaluate_performance(clf)","b057331a":"clf = xgb.XGBClassifier(max_depth=12, n_estimators=600, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.5)\nmodel_2 = evaluate_performance(clf)","37cfbed7":"clf = LogisticRegression()\nmodel_3 = evaluate_performance(clf)","2c78a905":"clf = KNeighborsClassifier(n_neighbors=2)\nmodel_4 = evaluate_performance(clf)","d5f0d03c":"clf = ExtraTreesClassifier(n_estimators=600,max_depth=12)\nmodel_5 = evaluate_performance(clf)","e1f7a7e2":"clf = CatBoostClassifier(max_depth=12, n_estimators=600, learning_rate=0.5,verbose=0)\nmodel_6 = evaluate_performance(clf)","e1f815a7":"processed_data = pre_Process_data(test['text'])\nX, vectorizer = Vectorization(processed_data)\nX_transform_1 = topic_modeling('MF',X)","79fbc122":"prob=model_5[2].predict(X_transform_1)","dd88280d":"my_submission = pd.DataFrame({'id': test['id'], 'target': prob})\nmy_submission.to_csv('SubmissionVictor.csv', index=False)\nmy_submission.head()","ba60ba1a":"There are two aspects to show their differences:\n\n    A stemmer will return the stem of a word, which needn't be identical to the morphological root of the word. It usually sufficient that related words map to the same stem,even if the stem is not in itself a valid root, while in lemmatisation, it will return the dictionary form of a word, which must be a valid word.\n\n    In lemmatisation, the part of speech of a word should be first determined and the normalisation rules will be different for different part of speech, while the stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech.\n    \nSource: [https:\/\/textminingonline.com\/dive-into-nltk-part-iv-stemming-and-lemmatization](https:\/\/textminingonline.com\/dive-into-nltk-part-iv-stemming-and-lemmatization)\n","e00aaba8":"All the algorithms used below in the function \"topic_modeling\" are all unsupervised learning techniques. You could know more about them if you google them, than if I explain you in this kernel.<br>\nOr I could refer you to one of my kernels, which would also help you to understand them.<br>\nLink to my kernel - https:\/\/www.kaggle.com\/basu369victor\/recommender-system-using-un-supervised-learning","4ccac22f":"## The END\nIf you have come till here, then I hope You have enjoyed it.<br>\nI have learned from Kagglers sharing through open-source. That's why I would also love to share my knowledge.<br>\nFeel free to say it, if you could improve this kernel any further, I would definitely listen to them.<br>\n### Till then, do upvote if U think this kernel was informative and if you enjoyed it.","a89221f1":"In the function below I have commented out the Lemmatizer, and worked only with PorterStemmer.<br>\nThe data-cleaning part is inspired from the kernel - [Basic EDA,Cleaning and GloVe](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\/data)","881c28c4":"## The problem statement\nIn one word, it is simply  a binary classification problem. You are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","6d55c15b":"**Stemming** is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.<br>\n**Lemmatization**, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.<br>\nFor example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.<br>\n\nSource: [https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python](https:\/\/www.datacamp.com\/community\/tutorials\/stemming-lemmatization-python)","75866e1c":"## I gained knowledge from them\nI have no certified course in NLP, but I have gained knowledge from kernels created by the Kagglers some of them are masters while some of them are grandmasters. Especially for this kernel, I am very much thankful to [Abishek Thakur](https:\/\/www.kaggle.com\/abhishek), [Parul Pandey](https:\/\/www.kaggle.com\/parulpandey), [Anisotropic](https:\/\/www.kaggle.com\/arthurtok).<br>\nLink to their kernels-\n* https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle\n* https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n* https:\/\/www.kaggle.com\/parulpandey\/visualizing-kannada-mnist-with-t-sne\n\nYou must visit these kernels which I have mentioned, I hope they would be very much helpful to you in your way of learning NLP.","6be2aa1a":"## The Last Kernel\nTime: 6:00 PM<br>\nDate: 31\/12\/2019<br>\nBy the time you would read this kernel, it would be the end of a year, an end of a decade. I am currently moving towards learning advanced techniques, like using berts, transformer models, concepts like look-ahead and attention networks. Although all of them involves deep learning.<br>\nBut before moving towards deep learning I would like to contribute a little to the community, what I have learned in NLP with Machine Learning.<br>\nI hope this kernel would also be helpful to those who want basics to learn and also those who want to move forward from basics.","efcda909":"## Representing the relation between location and key-Words through directed graph","d21cb0da":"## Lemmatizer and PorterStemmer\nBoth of them are Normalization techniques, but the acutual difference is the way they normalize the word-tokens."}}