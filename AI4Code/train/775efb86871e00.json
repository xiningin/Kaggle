{"cell_type":{"d29d2976":"code","5aa7683f":"code","8cbfb9dd":"code","49619f65":"code","2ee2f43d":"code","59e7e603":"code","947c6ba5":"code","ac9c1e69":"code","cd798171":"code","4ac1a61b":"code","02316384":"code","4eac2d7a":"markdown","5d90a515":"markdown","a6525b5e":"markdown","e2ef37e0":"markdown","86377087":"markdown","4ac0d79e":"markdown","edfacae9":"markdown","ee82ece3":"markdown","6a1f7a78":"markdown","e93d40a0":"markdown"},"source":{"d29d2976":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5aa7683f":"import matplotlib.pyplot as plt\nimport re\nfrom string import punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\nfrom PIL import Image","8cbfb9dd":"# 1: to make it lowercase\ndef to_lower(text):\n    return text.lower()\n\n# 2: make the contractions dictionary\ncontractions_dict = {     \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I had\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"iit will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they had\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\"gonna\": \"going to\",\n\"wanna\": \"want to\"\n}\n\ndef expand_contractions(text, contractions_dict):\n    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n                                      flags=re.IGNORECASE | re.DOTALL)\n\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contractions_dict.get(match) \\\n            if contractions_dict.get(match) \\\n            else contractions_dict.get(match.lower())\n        expanded_contraction = expanded_contraction\n        return expanded_contraction\n\n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\ndef main_contraction(text):\n    text = expand_contractions(text, contractions_dict)\n    return text\n\n# 3: to remove number\ndef remove_numbers(text):\n    output = ''.join(c for c in text if not c.isdigit())\n    return output\n\n# 4: remove punctuation\ndef remove_punct(text):\n    return ''.join(c for c in text if c not in punctuation)\n\n# 5: remove whitespace\ndef to_strip(text):\n    return \" \".join(text.split())\n\n# 5\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef remove_stopwords(sentence):\n    stop_words = stopwords.words('english')\n    return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n\n# 6: \nsnowball_stemmer = SnowballStemmer('english')\n\ndef stem(text):\n    \"\"\"\n    :param word_tokens:\n    :return: list of words\n    \"\"\"\n    stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n    return \" \".join(stemmed_word)\n\n# 7\nnltk.download('wordnet')\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef lemmatize(text):\n    lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n    return \" \".join(lemmatized_word)","49619f65":"df = pd.read_csv('..\/input\/eminem-lyrics-from-all-albums\/eminem_lyrics\/ALL_eminem.txt', delimiter='\\t', header=None)","2ee2f43d":"df","59e7e603":"df['text_clean1'] = df[0].apply(to_lower)\ndf['text_clean2'] = df['text_clean1'].apply(main_contraction)\ndf['text_clean3'] = df['text_clean2'].apply(remove_numbers)\ndf['text_clean4'] = df['text_clean3'].apply(remove_punct)\ndf['text_clean5'] = df['text_clean4'].apply(to_strip)\ndf['text_clean6'] = df['text_clean5'].apply(remove_stopwords)\ndf['text_lemma'] = df['text_clean6'].apply(lemmatize)","947c6ba5":"df","ac9c1e69":"def word_frequency(check):\n    check = check.str.extractall('([a-zA_Z]+)')\n    check.columns = ['check']\n    b = check.reset_index(drop=True)\n    check = b['check'].value_counts()\n\n    word_frequency = {'word':check.index,'freq':check.values}\n    word_frequency = pd.DataFrame(word_frequency)\n    word_frequency.index = word_frequency['word']\n    word_frequency.drop('word', axis = 1, inplace = True)\n    word_frequency.sort_values('freq',ascending=False,inplace=True)\n    \n    return word_frequency    ","cd798171":"df_word = word_frequency(df['text_lemma'])","4ac1a61b":"df_word.head(20).plot(kind='barh', figsize=(12,8))\nplt.title('Most Used Words', size=15)\nplt.show()","02316384":"mask = np.array(Image.open('..\/input\/eminem-image\/pngaaa.com-2669010.png'))\n\nword_cloud = WordCloud(\n    background_color=\"white\", \n    width=1000, \n    height=4000, \n    max_words=500,\n    mask=mask\n).generate_from_frequencies(df_word['freq'])\n\nplt.figure(figsize=(20, 10))\n# Display image\nplt.imshow(word_cloud) \n# No axis details\nplt.axis(\"off\")","4eac2d7a":"We make another columns to see how the text data changed","5d90a515":"# Library","a6525b5e":"# Functions","e2ef37e0":"Basically, we cleaned the data to remove several things:\n* Make it lowercase\n* Contractions\n* Remove number\n* Remove punctuation\n* Remove whitespace\n* Remove stopwords\n* Text lemmatization","86377087":"Functions for text preprocesing","4ac0d79e":"# Word Cloud","edfacae9":"# Text Preprocessing","ee82ece3":"# Word Frequency","6a1f7a78":"# Data","e93d40a0":"We want to load all of the emimem's songs in the file ALL_eminem.txt."}}