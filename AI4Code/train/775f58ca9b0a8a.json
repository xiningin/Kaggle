{"cell_type":{"d6ceda24":"code","fc22b284":"code","3f223d9b":"code","9543e0ba":"code","1dd6ff67":"code","786cf03f":"code","0b116670":"code","96b3630e":"code","936e5f2f":"code","062db8c3":"code","88daaf07":"code","e18d8440":"code","701fd720":"code","88252958":"code","640d4aad":"code","74a9f933":"code","02612f6c":"code","de19f826":"code","97e1890e":"code","0f13912c":"code","b482e327":"code","b4b33f19":"code","1a07c73d":"code","21aa285b":"code","d8a5971f":"code","ce5f39ef":"code","2dd101fd":"code","3e34da08":"code","3c0b3471":"code","ed3215cf":"code","bef52615":"code","961ae1d8":"code","77e5a21a":"code","b5f0e71f":"code","d84e283b":"code","fefc6697":"code","65720112":"code","9ced58b9":"code","6bee0f8a":"code","3851327b":"code","44bdaef3":"code","cd52691b":"code","70ab0b78":"code","19829e4e":"code","2efcb726":"code","100b8351":"code","583f0151":"code","d0483c96":"code","232e288b":"code","d6c42b04":"code","a2dce960":"code","cecc95a0":"code","c925a127":"code","52070cbd":"code","8e095b24":"code","11d48d27":"code","2b4dcd8d":"code","3aa9ae7c":"code","11597635":"code","78a1e524":"code","3f6caabc":"code","715ff0c5":"code","50d66355":"code","d8f7d320":"code","9d63a561":"code","7b918633":"code","f17274df":"code","e89b95f6":"code","5ef0f28d":"code","46ddc83f":"code","49559e7d":"code","53bce478":"code","eef50ef3":"code","5ea0e2ca":"code","3daf2b7f":"code","f85a237b":"code","5e6e5f4c":"code","9b73141f":"code","2cd246e6":"code","9f949c75":"code","0b8295ce":"code","cd8c90cf":"code","77b2efde":"code","37b4e815":"code","5a14422c":"code","c572ca27":"code","a7defdfb":"code","2cc4b628":"code","ede37ac9":"code","74526cc0":"code","2eba3a42":"code","775a355d":"code","b8f4a025":"code","c2ec6fa5":"code","4bd95ddc":"code","2dd694ed":"code","526e9d8e":"code","6755bebf":"code","147a5124":"code","47182bf5":"code","4a6236f5":"code","a0a84891":"code","82e1d61d":"code","ae5b6f1b":"code","d68d4bfb":"code","a753239a":"code","f4ecc244":"code","82965c8b":"code","0951891a":"code","14170378":"code","88b3b6ac":"code","03a4910a":"code","35cbcb74":"code","05a5c11b":"code","86a0aaa1":"code","57601f8e":"code","13d8d59d":"code","15fabd5d":"code","e5560a57":"code","0dfe1bc6":"code","a77a42c1":"code","e44c1224":"code","5480ee09":"code","cb14bcd6":"code","62fb399e":"code","7be3b377":"code","5022f3ba":"code","202fb5c7":"code","0ac10e64":"code","58bac33a":"code","d8506c0a":"code","36669dfb":"code","c83983a9":"code","86a02d5d":"code","e34a645e":"code","6dde0042":"code","a250184a":"code","4b678882":"code","f561e8e8":"code","1039a749":"code","d17b218c":"code","266591ce":"code","e8099aad":"code","942c3071":"code","2a299910":"code","41b86eef":"code","72febee1":"code","c248c49b":"code","8eadfac1":"code","3a4b591f":"code","90e767c6":"code","6b9cb4d8":"code","c0c79a0d":"code","7f5cfa88":"code","c0a93de2":"code","2935dc65":"code","8108b6fa":"code","810dc2dc":"code","4dde932f":"code","3040db1b":"code","34cdeb4f":"code","94be7392":"code","613684f8":"code","94e0f4a3":"code","e67ef48a":"code","969b5690":"code","c390ea16":"code","52413dff":"code","8be7cd3a":"code","659cc742":"code","2e939b15":"code","961b8a9e":"code","c3f755bc":"code","0e707b0c":"code","ed1b6ea6":"code","ebe9cc43":"code","65ddf805":"code","3873f411":"code","c35d7678":"code","db215361":"code","2a7a86b8":"code","15856597":"code","95e2f92c":"code","e4e3d96f":"code","445963b2":"code","bf97955f":"code","d5f854e8":"code","73ecec5d":"code","4d7d6d71":"code","3d0f7495":"code","14b38869":"code","20a46557":"code","32f0863a":"code","9cd307df":"code","c455fc19":"code","3651dd33":"code","3829a4ee":"code","262e1a23":"code","e57a65b3":"code","b64e7831":"code","4718e149":"code","28f85dbf":"code","fc08ec3f":"code","231cf277":"code","9a2cd4d7":"code","54b0fe33":"code","d27b59cd":"code","9897e405":"code","99d6966f":"code","4fadea80":"code","2487fe3d":"markdown","f426cc5d":"markdown","6e29de9b":"markdown","14b12397":"markdown","8eff64c3":"markdown","39a6548c":"markdown","3975b84e":"markdown","e898edb1":"markdown","a986ed7f":"markdown","4d06cce0":"markdown","c35e8573":"markdown","7cf14e11":"markdown","24ba7aea":"markdown","4189b65f":"markdown","f0340100":"markdown","57ef71c1":"markdown","075060ec":"markdown","54993959":"markdown","dd4437e6":"markdown","2623e304":"markdown","c7fdb7f7":"markdown","8a3ad3e4":"markdown","6bcb11aa":"markdown","f7168311":"markdown","3ab44e4f":"markdown","0ebb050c":"markdown","c22c9c8d":"markdown","232dcae1":"markdown","2e98d1e4":"markdown","ac750fc7":"markdown","8aa8ddef":"markdown","bdce101c":"markdown","61480631":"markdown","15249288":"markdown","2f70dda3":"markdown","423ce302":"markdown","221328c3":"markdown","6d317208":"markdown","ba986016":"markdown","1d941f67":"markdown","340b06f9":"markdown","a7bf9678":"markdown","86c39250":"markdown","1de687f4":"markdown","7026ee42":"markdown","dfc9d0b3":"markdown","a264bd7c":"markdown","55631c74":"markdown","464a6aa6":"markdown","994fa3ac":"markdown","3599a3e3":"markdown","af13e013":"markdown","11ab03bc":"markdown","2e3f2185":"markdown","ee68581f":"markdown","bbb7a6b5":"markdown","0d4a3954":"markdown","675091e1":"markdown","c8c2f700":"markdown","3b93a71e":"markdown","413c93b1":"markdown","a7404290":"markdown","5a6e3e74":"markdown","31f62614":"markdown","65076d87":"markdown","98fa0406":"markdown","70daa899":"markdown","a9ce41a8":"markdown","1e1c5bf6":"markdown","ff6ea99a":"markdown","6ae8c23f":"markdown","2f36b0aa":"markdown","28525ef4":"markdown","4cb18f21":"markdown","7c8eea79":"markdown"},"source":{"d6ceda24":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\npd.set_option(\"display.max_columns\", 300)\npd.set_option(\"display.max_rows\", 300)\n\nimport warnings\nwarnings.filterwarnings('ignore')","fc22b284":"# Read data\ndf = pd.read_csv(\"\/kaggle\/input\/telecom\/telecom_churn_data.csv\")","3f223d9b":"df.head(5)","9543e0ba":"df.info()","1dd6ff67":"# Find the percentage of missing values in dataframe\nmissing_df = pd.DataFrame({\n    \"Columns\": df.columns[df.isnull().sum()>0],\n    \"Values\": df[df.columns[df.isnull().sum()>0]].isnull().sum()\/len(df)*100\n})\nmissing_df = missing_df.reset_index(drop=True)","786cf03f":"missing_df[missing_df[\"Values\"]>10]","0b116670":"# Function to label the count on top of each bar in graph\ndef label_values(ax, spacing=5):\n    total = 0\n    for rect in ax.patches:\n        total += rect.get_height()\n\n    for rect in ax.patches:\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        space = spacing\n        \n        va = 'bottom'\n        \n        if y_value < 0:\n            space *= -1\n            va = 'top'\n        label = \"{:.2f}, {:.2f}\".format(y_value, y_value\/total*100)\n        ax.annotate(\n            label,                      \n            (x_value, y_value),         \n            xytext=(0, space),          \n            textcoords=\"offset points\", \n            ha='center',                \n            va=va)    ","96b3630e":"# Fill null value of total number of recharge data by zero\ndf['total_rech_data_6'] = df['total_rech_data_6'].fillna(0.0)\ndf['total_rech_data_7'] = df['total_rech_data_7'].fillna(0.0)\ndf['total_rech_data_8'] = df['total_rech_data_8'].fillna(0.0)\n\n# Fill null value of average recharge amount data by zero\ndf['av_rech_amt_data_6'] = df['av_rech_amt_data_6'].fillna(0.0)\ndf['av_rech_amt_data_7'] = df['av_rech_amt_data_7'].fillna(0.0)\ndf['av_rech_amt_data_8'] = df['av_rech_amt_data_8'].fillna(0.0)\n\n# Fill null values of max_recharge_amount data by zero\ndf['max_rech_data_6'] = df['max_rech_data_6'].fillna(0.0)\ndf['max_rech_data_7'] = df['max_rech_data_7'].fillna(0.0)\ndf['max_rech_data_8'] = df['max_rech_data_8'].fillna(0.0)\n\n\n# Fill null values of count recharge 3g data by zero\ndf['count_rech_3g_6'] = df['count_rech_3g_6'].fillna(0.0)\ndf['count_rech_3g_7'] = df['count_rech_3g_7'].fillna(0.0)\ndf['count_rech_3g_8'] = df['count_rech_3g_8'].fillna(0.0)","936e5f2f":"# Create new column total recharge amount : total_rech_amt_data for calculating High Value customer process\ndf['total_rech_data_amt_6'] = df['av_rech_amt_data_6'] * df['total_rech_data_6']\ndf['total_rech_data_amt_7'] = df['av_rech_amt_data_7'] * df['total_rech_data_7']\ndf['total_rech_data_amt_8'] = df['av_rech_amt_data_8'] * df['total_rech_data_8']\n\n# Total recharge amount data\ndf['total_recharge_amt_call_data_6'] = (df['av_rech_amt_data_6'] * df['total_rech_data_6']) + df['total_rech_amt_6']\ndf['total_recharge_amt_call_data_7'] = (df['av_rech_amt_data_7'] * df['total_rech_data_7']) + df['total_rech_amt_7']\ndf['total_recharge_amt_call_data_8'] = (df['av_rech_amt_data_8'] * df['total_rech_data_8']) + df['total_rech_amt_8']","062db8c3":"# Calculate the total average recharge amount for june and july i.e. for months 6 and 7 (the good phase)\ndf['total_avg_rech_amnt_67'] = (df['total_recharge_amt_call_data_6'] + df['total_recharge_amt_call_data_7'])\/2","88daaf07":"# Look at 70 percentile of the calculated average amount\npercentile_70_6_7 = np.percentile(df['total_avg_rech_amnt_67'], 70.0)\nprint(\"70 percentile is : \", percentile_70_6_7)","e18d8440":"# Retain only those customers who have recharged more than or equal to 70 percentile of recharge amount\ndf_hvc = df[df['total_avg_rech_amnt_67'] >= percentile_70_6_7].reset_index(drop=True)\nprint(\"Shape: \", df_hvc.shape)","701fd720":"df_hvc.head()","88252958":"df_hvc[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].isnull().sum()","640d4aad":"# Create column \"churn\" whose values would be either 1 (churn) or 0 (non-churn)\ndf_hvc['churn'] = np.where(df_hvc[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']].sum(axis=1) == 0, 1, 0)\ndf_hvc.head()","74a9f933":"ax = sns.countplot(x = df_hvc['churn'])\nlabel_values(ax, spacing=-15)\nplt.show()","02612f6c":"# In the dataframe column list we can see we have 4 columns whose name is wrongly written,\n# aug_vbc_3g, jul_vbc_3g, jun_vbc_3g, sep_vbc_3g\n# Rename these columns\n\ndf_hvc = df_hvc.rename(columns={'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g': 'vbc_3g_7', 'aug_vbc_3g': 'vbc_3g_8', 'sep_vbc_3g': 'vbc_3g_9'})","de19f826":"churn_phase_columns =  df_hvc.columns[df_hvc.columns.str.contains('_9')]\nlen(churn_phase_columns)","97e1890e":"df_hvc = df_hvc.drop(churn_phase_columns, axis = 1)","0f13912c":"df_hvc.shape","b482e327":"# Find variables with no variance i.e. only one value in the variable\n\nvariance_df_hvc = pd.DataFrame({\n    \"Columns\": df_hvc.columns[df_hvc.nunique()==1],\n    \"Missing Percentage\": df_hvc[df_hvc.columns[df_hvc.nunique()==1]].isnull().sum()\/len(df_hvc)*100,\n    \"Values\": df_hvc[df_hvc.columns[df_hvc.nunique()==1]].nunique()\n})\nvariance_df_hvc = variance_df_hvc.reset_index(drop=True)\nvariance_df_hvc","b4b33f19":"# These columns have no variance and number of missing values are also less\ndf_hvc = df_hvc.drop(list(variance_df_hvc[\"Columns\"]), axis = 1)\ndf_hvc.shape","1a07c73d":"df_hvc.dtypes","21aa285b":"df_hvc.head()","d8a5971f":"# Find number of values in remaining columns\n\nunique_df_hvc = pd.DataFrame({\n    \"Columns\": df_hvc.columns,\n    \"Values\": df_hvc.nunique()\n})\nunique_df_hvc = variance_df_hvc.reset_index(drop=True)\nunique_df_hvc","ce5f39ef":"# We can drop mobile number as it is a unique or indentity columns\n\ndf_hvc = df_hvc.drop(['mobile_number'], axis = 1)","2dd101fd":"int_float_columns = df_hvc[list(df_hvc.columns[df_hvc.dtypes == int]) + list(df_hvc.columns[df_hvc.dtypes == float])].nunique()\nint_float_columns","3e34da08":"# Target variable\nvar_target = ['churn']\n\n# Based on business understanding, we will say columns with less than or equal to 15 unique values has potential to be categorical columns\n\nvar_categorical = list(set(list(int_float_columns[int_float_columns<=15].keys())) - set(var_target))\n\n# Other features can be numerical variables\n\nvar_numerical = list(set(list(int_float_columns.keys())) - set(list(var_categorical))) \n","3c0b3471":"len(list(var_numerical + var_categorical + var_target))","ed3215cf":"# Countplot for each categorical variable\n# https:\/\/matplotlib.org\/2.0.2\/examples\/statistics\/countplot_demo.html\n\ndef plot_countplots(col_prefix):\n    plt.figure(figsize=(20,16))\n    plt.subplot(2,3,1)\n    ax = sns.countplot(y=df_hvc[col_prefix+\"_6\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.subplot(2,3,2)\n    ax = sns.countplot(y=df_hvc[col_prefix+\"_7\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.subplot(2,3,3)\n    ax = sns.countplot(y=df_hvc[col_prefix+\"_8\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.show()","bef52615":"# Identify month dependent and independent columns\ndef month_dependent_col_identify(x):\n    if ('_6' in x) or ('_7' in x) or ('_8' in x):\n        return x\n\nmonth_dependent_columns = list(map(month_dependent_col_identify, var_numerical))\nmonth_dependent_columns = list(filter(None, month_dependent_columns))\n\nmonth_independent_columns = list(set(var_numerical) - set(month_dependent_columns))","961ae1d8":"month_independent_columns","77e5a21a":"# Plot for variables which are not dependent on months\n\nfor column in month_independent_columns:\n    plt.subplots(figsize=(16, len(var_numerical)*6))\n    plt.subplot(len(var_numerical) + 1, 3, 1)\n    sns.boxplot(x = df_hvc[column])\n    plt.subplot(len(var_numerical) + 1, 3, 2)\n    sns.distplot(x = df_hvc[column])\n    plt.subplot(len(var_numerical) + 1, 3, 3)\n    sns.boxplot(x = df_hvc[\"churn\"], y = df_hvc[column])\n    plt.show()","b5f0e71f":"# Countplot for each categorical variable\n# https:\/\/matplotlib.org\/2.0.2\/examples\/statistics\/countplot_demo.html\n\ndef plot_countplots(col_prefix):\n    plt.figure(figsize=(20,16))\n    plt.subplot(2,3,1)\n    ax = sns.countplot(x=df_hvc[col_prefix+\"_6\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.subplot(2,3,2)\n    ax = sns.countplot(x=df_hvc[col_prefix+\"_7\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.subplot(2,3,3)\n    ax = sns.countplot(x=df_hvc[col_prefix+\"_8\"], hue=df_hvc[\"churn\"])\n    label_values(ax)\n    plt.show()","d84e283b":"# Box plot for  6th, 7th and 8th month\n# https:\/\/matplotlib.org\/2.0.2\/examples\/statistics\/boxplot_demo.html\n# showfliers = False to remove the representation of outliers & showmeans to represent means in boxplot\n\ndef plot_boxplots(col_prefix):\n    plt.figure(figsize=(20,16))\n    plt.subplot(2,3,1)\n    sns.boxplot(y=df_hvc[col_prefix+\"_6\"],x=df_hvc[\"churn\"],showfliers=False, showmeans=True)\n    plt.subplot(2,3,2)\n    sns.boxplot(y=df_hvc[col_prefix+\"_7\"],x=df_hvc[\"churn\"],showfliers=False, showmeans=True)\n    plt.subplot(2,3,3)\n    sns.boxplot(y=df_hvc[col_prefix+\"_8\"],x=df_hvc[\"churn\"],showfliers=False, showmeans=True)\n    plt.show()","fefc6697":"# Total recharge Amount\nplot_boxplots('total_rech_amt')","65720112":"# Ploting for total recharge amount for data:\nplot_boxplots('total_rech_data')","9ced58b9":"# Ploting for number of times recharge:\nplot_boxplots('total_rech_num')","6bee0f8a":"# Ploting for total recharge amount for data and calls:\nplot_boxplots('total_recharge_amt_call_data')","3851327b":"# Maximum recharge amount \nplot_boxplots('max_rech_amt')","44bdaef3":"# Maximum recharge data\nplot_boxplots('max_rech_data')","cd52691b":"rech_columns = list(df_hvc.columns[df_hvc.columns.str.contains('rech')])","70ab0b78":"plt.figure(figsize=(16, 16))\nsns.heatmap(df_hvc[rech_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","19829e4e":"# So we will drop count_rech_2g columns and total_rech_data columns due to its high correlations\ndf_hvc = df_hvc.drop(['count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8', 'total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8'], axis = 1)","2efcb726":"df_hvc.shape","100b8351":"# Local incoming calls minutes usage\nplot_boxplots('loc_ic_mou')","583f0151":"# Local outgoing calls minutes usage\nplot_boxplots('loc_og_mou')","d0483c96":"# Outside the calling circles incoming usage\n\nplot_boxplots('std_ic_mou')","232e288b":"# Outside the calling circles outgoing usage\n\nplot_boxplots('std_og_mou')","d6c42b04":"ic_columns = list(df_hvc.columns[df_hvc.columns.str.contains('.*_ic_.*mou_*')])","a2dce960":"plt.figure(figsize=(16, 16))\nsns.heatmap(df_hvc[ic_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","cecc95a0":"# So we will drop local_ic_mou columns\ndf_hvc = df_hvc.drop(['loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8','std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8'], axis = 1)","c925a127":"df_hvc.shape","52070cbd":"og_columns = list(df_hvc.columns[df_hvc.columns.str.contains('.*_og_.*mou_*')])","8e095b24":"plt.figure(figsize=(16, 16))\nsns.heatmap(df_hvc[og_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","11d48d27":"df_hvc['total_og_mou_6']","2b4dcd8d":"df_hvc[['isd_og_mou_6', 'isd_og_mou_7', 'isd_og_mou_8', 'std_og_mou_6', 'std_og_mou_7', 'std_og_mou_8']].isnull().sum()*100\/len(df_hvc)","3aa9ae7c":"# So we will drop any two of them as they are highly coorelated with each other and has maximum missing values\ndf_hvc = df_hvc.drop(['isd_og_mou_6', 'isd_og_mou_8'], axis = 1)","11597635":"df_hvc.shape","78a1e524":"# ARPU (Average revenue per user)\nplot_boxplots('arpu')","3f6caabc":"arpu_columns = list(df_hvc.columns[df_hvc.columns.str.contains('arpu')])","715ff0c5":"plt.figure(figsize=(8, 8))\nsns.heatmap(df_hvc[arpu_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","50d66355":"df_hvc[['arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8', 'arpu_3g_6','arpu_3g_7', 'arpu_3g_8']].isnull().sum()*100\/len(df_hvc)","d8f7d320":"# So we will drop any two of them as they are highly coorelated with each other\ndf_hvc = df_hvc.drop(['arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8'], axis = 1)","9d63a561":"df_hvc.shape","7b918633":"# ONNET (All kind of calls within the same operator network)\nplot_boxplots(\"onnet_mou\")","f17274df":"onnet_columns = list(df_hvc.columns[df_hvc.columns.str.contains('onnet')])","e89b95f6":"sns.heatmap(df_hvc[onnet_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","5ef0f28d":"# OFFNET (All kind of calls outside the operator T network)\nplot_boxplots('offnet_mou')","46ddc83f":"offnet_columns = list(df_hvc.columns[df_hvc.columns.str.contains('offnet')])","49559e7d":"\nsns.heatmap(df_hvc[offnet_columns].corr(), \n            annot=True, fmt=\".2f\", linewidths=0.01, linecolor='black', cmap='Spectral')\nplt.show()","53bce478":"# Volumne of 2g data used in mb\nplot_boxplots('vol_2g_mb')","eef50ef3":"# Average revenue used 2g\nplot_boxplots('arpu_2g')","5ea0e2ca":"# Service schemes with validity equivalent to a month\nplot_countplots('monthly_2g')","3daf2b7f":"# Volumne of 2g data used in mb\nplot_boxplots('vol_3g_mb')","f85a237b":"# Service schemes with validity equivalent to a month\nplot_countplots('monthly_3g')","5e6e5f4c":"object_columns = list(df_hvc.columns[df_hvc.dtypes == 'object'])","9b73141f":"# Convert the date fields to datetime object\n\nfor col in object_columns:\n    df_hvc[col] = pd.to_datetime(df_hvc[col])","2cd246e6":"# Find the percentage of missing values in dataframe\nmissing_df_hvc = pd.DataFrame({\n    \"Columns\": df_hvc.columns[df_hvc.isnull().sum()>0],\n    \"Values\": df_hvc[df_hvc.columns[df_hvc.isnull().sum()>0]].isnull().sum()\/len(df_hvc)*100\n})\nmissing_df_hvc = missing_df_hvc.reset_index(drop=True)","9f949c75":"missing_df_hvc.sort_values(by='Values')","0b8295ce":"df_hvc[['arpu_6', 'arpu_2g_6', 'total_recharge_amt_call_data_6', 'total_rech_data_amt_6', 'total_rech_amt_6']].head(15)","cd8c90cf":"df_hvc['arpu_2g_6'] = df_hvc['arpu_2g_6'].fillna(0.0)\ndf_hvc['arpu_2g_7'] = df_hvc['arpu_2g_7'].fillna(0.0)\ndf_hvc['arpu_2g_8'] = df_hvc['arpu_2g_8'].fillna(0.0)","77b2efde":"# Fill null values of fb_user data with 2 as it means that the user data is unknown (using separate value method)\n\ndf_hvc['fb_user_6'] = df_hvc['fb_user_6'].fillna(2.0)\ndf_hvc['fb_user_7'] = df_hvc['fb_user_7'].fillna(2.0)\ndf_hvc['fb_user_8'] = df_hvc['fb_user_8'].fillna(2.0)\n\n# Fill null values of night_pk_user data with 0 as it means that the user data is unknown because that user probably does not use at night(assumption)\n\ndf_hvc['night_pck_user_6'] = df_hvc['night_pck_user_6'].fillna(2.0)\ndf_hvc['night_pck_user_7'] = df_hvc['night_pck_user_7'].fillna(2.0)\ndf_hvc['night_pck_user_8'] = df_hvc['night_pck_user_8'].fillna(2.0)","37b4e815":"columns_to_impute = list(set(missing_df_hvc[\"Columns\"]) - set(object_columns))\n\nfor col in columns_to_impute:\n    if col in var_categorical:\n        df_hvc[col] = df_hvc[col].fillna(df_hvc[col].mode().values[0])\n    else:\n        df_hvc[col] = df_hvc[col].fillna(df_hvc[col].median())","5a14422c":"object_columns","c572ca27":"df_hvc[object_columns].head(20)","a7defdfb":"def date_imputer(date1, date2):\n    if pd.isnull(date1) and pd.isnull(date2):\n        return 0\n    elif pd.isnull(date1):\n        return -1\n    elif pd.isnull(date2):\n        return -2\n    else:\n        return int((date1 - date2).days)","2cc4b628":"date_imputer(df_hvc['date_of_last_rech_data_7'][2], df_hvc['date_of_last_rech_data_6'][2])","ede37ac9":"df_hvc[\"interval_of_last_rech_67\"] = list(map(date_imputer, df_hvc['date_of_last_rech_7'], df_hvc['date_of_last_rech_6']))\ndf_hvc[\"interval_of_last_rech_78\"] = list(map(date_imputer, df_hvc['date_of_last_rech_8'], df_hvc['date_of_last_rech_7']))\ndf_hvc[\"interval_of_last_rech_data_67\"] = list(map(date_imputer, df_hvc['date_of_last_rech_data_7'], df_hvc['date_of_last_rech_data_6']))\ndf_hvc[\"interval_of_last_rech_data_78\"] = list(map(date_imputer, df_hvc['date_of_last_rech_data_8'], df_hvc['date_of_last_rech_data_7']))","74526cc0":"# Now drop the date columns\ndf_hvc = df_hvc.drop(['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', \n                      'date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8'], axis = 1)","2eba3a42":"df_hvc.head()","775a355d":"# Find the percentage of missing values in dataframe\nmissing_df_hvc = pd.DataFrame({\n    \"Columns\": df_hvc.columns[df_hvc.isnull().sum()>0],\n    \"Values\": df_hvc[df_hvc.columns[df_hvc.isnull().sum()>0]].isnull().sum()\/len(df_hvc)*100\n})\nmissing_df_hvc = missing_df_hvc.reset_index(drop=True)\nmissing_df_hvc","b8f4a025":"cat_list = set(['monthly_2g', 'night_pck_user', 'fb_user', 'monthly_3g'])","c2ec6fa5":"# Create a new colulmn, which would be average  of 6th & 7th months except the categorical columns\n# Derive list of columns belonging to 6th and 7th months\ncol_list = set(df_hvc.filter(regex='_6$').columns.str[:-2])\n\nset_of_numerical_columns = col_list - cat_list\nnumerical_col = []\n# Take average\nfor idx, col in enumerate(set_of_numerical_columns):\n#     print(col)\n    avg_col_name = col+\"_av67\"\n    numerical_col.append(avg_col_name)\n    col_6 = col+\"_6\"\n    col_7 = col+\"_7\"\n    numerical_col.append(col+\"_8\")\n    df_hvc[avg_col_name] = (df_hvc[col_6]  + df_hvc[col_7])\/ 2","4bd95ddc":"# We will drop _6 and _7 colums columns\nprint (\"dimension of the updated dataset after creating dervied features:\",df_hvc.shape)\ncol_to_drop = df_hvc.filter(regex='_6|_7').columns\ndf_hvc = df_hvc.drop(list(set(col_to_drop)-set(var_categorical)), axis=1)\n\nprint(\"dimension of the dataset after dropping un-necessary columns:\",df_hvc.shape)","2dd694ed":"# Correlation of churn with other columns\nplt.figure(figsize=(20,10))\ndf_hvc.corr()['churn'].sort_values(ascending = False).plot(kind='bar')","526e9d8e":"df_hvc.describe()","6755bebf":"def convert_values_to_log(all_values):\n    transformed = []\n    for value in all_values:\n        if value <= 0:\n            transformed.append(float(0))\n        elif float(value) == float(1.0):\n            transformed.append(float(1.5))\n        else:\n            transformed.append(float(np.log(value)))\n    return transformed","147a5124":"df_hvc[numerical_col] = df_hvc[numerical_col].apply(convert_values_to_log)","47182bf5":"df_hvc.describe()","4a6236f5":"df_hvc.shape","a0a84891":"y = df_hvc['churn']\nX = df_hvc.drop('churn', axis = 1)","82e1d61d":"# Split in train & Test with stratification\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,  y, test_size=0.3, train_size=0.7, stratify=y, random_state=100)","ae5b6f1b":"X_train.shape, X_test.shape","d68d4bfb":"# For categorical columns we will use one hot multiclass encoding\ndef top_labels_train(X_train, col1, max_val):\n    col1 = [col1+'_6', col1+'_7', col1+'_8']\n#     print(col1)\n    top_all = []\n    for col in col1:\n        top = list(X_train[col].value_counts().sort_values(ascending=False).head(max_val).index)\n        print(\"Column: \" + col + \"\\nTop Labels: \", top)\n        for categories in top:\n            X_train[col+ \"_\" +str(categories)]=np.where(X_train[col]==categories,1,0)\n#             print(col + \"_\" + str(categories))\n        top_all.append(top)\n    return top_all\n\ndef top_labels_test(X_test, col, max_val, top):\n#     print(\"Top Labels: \", top)\n    for categories in top:\n        X_test[col+ \"_\" +str(categories)]=np.where(X_test[col]==categories,1,0)\n#         print(col + \"_\" + str(categories))","a753239a":"df_hvc['monthly_2g_6'].value_counts()","f4ecc244":"df_hvc['monthly_2g_7'].value_counts()","82965c8b":"df_hvc['monthly_2g_8'].value_counts()","0951891a":"top_monthly_2g = top_labels_train(X_train, 'monthly_2g', 3)\ntop_labels_monthly_2g_6, top_labels_monthly_2g_7, top_labels_monthly_2g_8 = top_monthly_2g[0], top_monthly_2g[1], top_monthly_2g[2]\nX_train = X_train.drop(['monthly_2g_6', 'monthly_2g_7', 'monthly_2g_8'], axis = 1)","14170378":"top_labels_test(X_test, 'monthly_2g_6', 3, top_labels_monthly_2g_6)\ntop_labels_test(X_test, 'monthly_2g_7', 3, top_labels_monthly_2g_7)\ntop_labels_test(X_test, 'monthly_2g_8', 3, top_labels_monthly_2g_8)\nX_test = X_test.drop(['monthly_2g_6', 'monthly_2g_7', 'monthly_2g_8'], axis = 1)","88b3b6ac":"df_hvc['monthly_3g_6'].value_counts()","03a4910a":"df_hvc['monthly_3g_7'].value_counts()","35cbcb74":"df_hvc['monthly_3g_8'].value_counts()","05a5c11b":"top_monthly_3g = top_labels_train(X_train, 'monthly_3g', 3)\ntop_labels_monthly_3g_6, top_labels_monthly_3g_7, top_labels_monthly_3g_8 = top_monthly_3g[0], top_monthly_3g[1], top_monthly_3g[2]\nX_train = X_train.drop(['monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8'], axis = 1)","86a0aaa1":"top_labels_test(X_test, 'monthly_3g_6', 3, top_labels_monthly_3g_6)\ntop_labels_test(X_test, 'monthly_3g_7', 3, top_labels_monthly_3g_7)\ntop_labels_test(X_test, 'monthly_3g_8', 3, top_labels_monthly_3g_8)\nX_test = X_test.drop(['monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8'], axis = 1)","57601f8e":"df_hvc['night_pck_user_6'].value_counts()","13d8d59d":"df_hvc['night_pck_user_7'].value_counts()","15fabd5d":"df_hvc['night_pck_user_8'].value_counts()","e5560a57":"X_train = pd.get_dummies(X_train, columns=[\"night_pck_user_6\"], drop_first=True, prefix=[\"night_pck_user_6\"])\nX_test = pd.get_dummies(X_test, columns=[\"night_pck_user_6\"], drop_first=True, prefix=[\"night_pck_user_6\"])\nX_train = pd.get_dummies(X_train, columns=[\"night_pck_user_7\"], drop_first=True, prefix=[\"night_pck_user_7\"])\nX_test = pd.get_dummies(X_test, columns=[\"night_pck_user_7\"], drop_first=True, prefix=[\"night_pck_user_7\"])\nX_train = pd.get_dummies(X_train, columns=[\"night_pck_user_8\"], drop_first=True, prefix=[\"night_pck_user_8\"])\nX_test = pd.get_dummies(X_test, columns=[\"night_pck_user_8\"], drop_first=True, prefix=[\"night_pck_user_8\"])","0dfe1bc6":"df_hvc['fb_user_6'].value_counts()","a77a42c1":"df_hvc['fb_user_7'].value_counts()","e44c1224":"df_hvc['fb_user_8'].value_counts()","5480ee09":"X_train = pd.get_dummies(X_train, columns=[\"fb_user_6\"], drop_first=True, prefix=[\"fb_user_6\"])\nX_test = pd.get_dummies(X_test, columns=[\"fb_user_6\"], drop_first=True, prefix=[\"fb_user_6\"])\nX_train = pd.get_dummies(X_train, columns=[\"fb_user_7\"], drop_first=True, prefix=[\"fb_user_7\"])\nX_test = pd.get_dummies(X_test, columns=[\"fb_user_7\"], drop_first=True, prefix=[\"fb_user_7\"])\nX_train = pd.get_dummies(X_train, columns=[\"fb_user_8\"], drop_first=True, prefix=[\"fb_user_8\"])\nX_test = pd.get_dummies(X_test, columns=[\"fb_user_8\"], drop_first=True, prefix=[\"fb_user_8\"])","cb14bcd6":"X_train.shape, X_test.shape","62fb399e":"from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nscaler = StandardScaler()\nX_train[:] = scaler.fit_transform(X_train)\nX_test[:] = scaler.transform(X_test)","7be3b377":"# Draw ROC curve from training and test data probability\ndef draw_roc( train_actual, train_probs, test_actual, test_probs ):\n    train_fpr, train_tpr, train_thresholds = metrics.roc_curve( train_actual, train_probs,\n                                              drop_intermediate = False )\n    test_fpr, test_tpr, test_thresholds = metrics.roc_curve( test_actual, test_probs,\n                                              drop_intermediate = False )\n    train_auc_score = metrics.roc_auc_score( train_actual, train_probs )\n    test_auc_score = metrics.roc_auc_score( test_actual, test_probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( train_fpr, train_tpr, label='ROC curve train (area = %0.2f)' % train_auc_score )\n    plt.plot( test_fpr, test_tpr, label='ROC curve test (area = %0.2f)' % test_auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n","5022f3ba":"# Train model using hyperparmeter tuning\ndef training_model_hyperparameter(model, scoring, params_grid, X_train, y_train):\n    # using stratified k fold to handle imbalance data\n    folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state=100)\n\n    grid = GridSearchCV(estimator = model, scoring=scoring, param_grid = params_grid, cv = folds, \n                           verbose=0, return_train_score=True, n_jobs=-1)\n    grid.fit(X_train, y_train)\n    return grid","202fb5c7":"# Predict values and propability of training and testing data\ndef prediction_model(model, X_train, y_train, X_test, y_test):\n    y_train_pred = model.predict(X_train)\n    y_train_pred_prob = model.predict_proba(X_train)[:, 1]\n    y_test_pred = model.predict(X_test)\n    y_test_pred_prob = model.predict_proba(X_test)[:, 1]\n    return y_train_pred, y_train_pred_prob, y_test_pred, y_test_pred_prob","0ac10e64":"# Draw scree plot of pca\ndef get_scree_plot(pca):\n    fig = plt.figure(figsize = (8,6))\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('number of components')\n    plt.ylabel('Cummulative explained variance ratio')\n    plt.show()\n    return pca","58bac33a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold","d8506c0a":"# Create the param grid for logistic regression\nweights = np.linspace(0.05,.95, 10)\n#Creating a dictionary grid for grid search\nparam_grid = {}\nlr_param_grid = {\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}\n# Hyperparameter tuning on class weights in logistic regression","36669dfb":"lr_model = LogisticRegression(solver='liblinear', random_state=100)","c83983a9":"# Train model using grid search cv and stratified k fold method\nlr_grid = training_model_hyperparameter(lr_model, 'roc_auc', lr_param_grid, X_train, y_train)","86a02d5d":"#Ploting the score for different values of weight\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,8))\nweigh_data = pd.DataFrame({'score': lr_grid.cv_results_['mean_test_score'], 'weight': (1- weights)})\nsns.lineplot(weigh_data['weight'], weigh_data['score'])\nplt.xlabel('Weight for class 1')\nplt.ylabel('AUC ROC')\nplt.xticks([round(i\/10,1) for i in range(0,11,1)])\nplt.title('Scoring for different class weights', fontsize=24)","e34a645e":"lr_grid.best_params_","6dde0042":"y_train_pred_lr, y_train_pred_lr_prob, y_test_pred_lr, y_test_pred_lr_prob = \\\n                                    prediction_model(lr_grid, X_train, y_train, X_test, y_test)","a250184a":"draw_roc(y_train, y_train_pred_lr_prob, y_test, y_test_pred_lr_prob)","4b678882":"print(metrics.classification_report(y_train, y_train_pred_lr))","f561e8e8":"print(metrics.classification_report(y_test, y_test_pred_lr))","1039a749":"from sklearn.feature_selection import RFE","d17b218c":"logreg_rfe = LogisticRegression(solver='liblinear', random_state = 100, class_weight={0:0.13, 1:0.87})","266591ce":"rfe50 = RFE(logreg_rfe, n_features_to_select = 50)\nrfe50 = rfe50.fit(X_train, y_train)","e8099aad":"col = X_train.columns[rfe50.support_]","942c3071":"# Creating X_train_rfe dataframe with RFE selected variables\n\nX_train_rfe50 = X_train[col]\nX_test_rfe50 = X_test[col]","2a299910":"logreg_rfe50 = LogisticRegression(solver='liblinear', random_state = 100, class_weight={0:0.15, 1:0.85})\nlogreg_rfe50 = logreg_rfe50.fit(X_train_rfe50, y_train)","41b86eef":"y_train_pred_rfe50, y_train_pred_rfe50_prob, y_test_pred_rfe50, y_test_pred_rfe50_prob = prediction_model(logreg_rfe50, X_train_rfe50, y_train, X_test_rfe50, y_test)","72febee1":"draw_roc(y_train, y_train_pred_rfe50_prob, y_test, y_test_pred_rfe50_prob)","c248c49b":"importances = logreg_rfe50.coef_[0]\nweights = pd.Series(importances,\n                 index=X_train_rfe50.columns.values)\nweights.sort_values()[-15:].plot(kind = 'barh')","8eadfac1":"print(metrics.classification_report(y_train, y_train_pred_rfe50))","3a4b591f":"print(metrics.classification_report(y_test, y_test_pred_rfe50))","90e767c6":"# Create the param grid for random forest\nweights = np.linspace(0.01,.99,5)\n\n# Keeping max depth low as number of features are high and max features is also in range of 10 to 20.\n# To not overfit our model using these parameters\nparam_grid_rf = [{\n               'max_depth': [3, 5, 7],\n               'max_features': [10, 15, 20],\n               'class_weight': [{0:x, 1:1.0-x} for x in weights]}]","6b9cb4d8":"rf_model = RandomForestClassifier(random_state=100)","c0c79a0d":"rf_grid = training_model_hyperparameter(rf_model, 'roc_auc', param_grid_rf, X_train, y_train)","7f5cfa88":"rf_grid.best_params_","c0a93de2":"y_train_pred_rf, y_train_pred_rf_prob, y_test_pred_rf, y_test_pred_rf_prob = \\\n                                    prediction_model(rf_grid, X_train, y_train, X_test, y_test)","2935dc65":"draw_roc(y_train, y_train_pred_rf_prob, y_test, y_test_pred_rf_prob)","8108b6fa":"print(metrics.classification_report(y_train, y_train_pred_rf))","810dc2dc":"print(metrics.classification_report(y_test, y_test_pred_rf))","4dde932f":"# Because of high number of features we will choose n_estimators for hyperparameter tuning\nparam_grid_ada = [{\n               'n_estimators': [50, 100, 200],\n               }]","3040db1b":"# Build AdaBoostClassifier\nada_model = AdaBoostClassifier()","34cdeb4f":"# Train model using grid search cv and stratified k fold method\nada_grid = training_model_hyperparameter(ada_model, 'roc_auc', param_grid_ada, X_train, y_train)","94be7392":"ada_grid.best_params_","613684f8":"y_train_pred_ada, y_train_pred_ada_prob, y_test_pred_ada, y_test_pred_ada_prob = \\\n                                    prediction_model(ada_grid, X_train, y_train, X_test, y_test)","94e0f4a3":"draw_roc(y_train, y_train_pred_ada_prob, y_test, y_test_pred_ada_prob)","e67ef48a":"print(metrics.classification_report(y_train, y_train_pred_ada))","969b5690":"print(metrics.classification_report(y_test, y_test_pred_ada))","c390ea16":"# Create the param grid for random forest\nweights = np.linspace(0.01,.99,5)\n\n# Class weight as hyperparameter tuning parameter\nparam_grid_svc = [{\n               'class_weight': [{0:x, 1:1.0-x} for x in weights]}]","52413dff":"svc_model = SVC(probability=True, random_state=100)","8be7cd3a":"# Train model using grid search cv and stratified k fold method\nsvc_grid = training_model_hyperparameter(svc_model, 'roc_auc', param_grid_svc, X_train, y_train)","659cc742":"svc_grid.best_params_","2e939b15":"y_train_pred_svc, y_train_pred_svc_prob, y_test_pred_svc, y_test_pred_svc_prob = \\\n                                    prediction_model(svc_grid, X_train, y_train, X_test, y_test)","961b8a9e":"draw_roc(y_train, y_train_pred_svc_prob, y_test, y_test_pred_svc_prob)","c3f755bc":"print(metrics.classification_report(y_train, y_train_pred_svc))","0e707b0c":"print(metrics.classification_report(y_test, y_test_pred_svc))","ed1b6ea6":"from sklearn.decomposition import IncrementalPCA, PCA","ebe9cc43":"pca = PCA(random_state=101)\n\n# Apply PCA on train data\npca.fit(X_train)","65ddf805":"get_scree_plot(pca)","3873f411":"pca = IncrementalPCA(n_components = 43)\n\n# Apply PCA on train data\npca.fit(X_train)","c35d7678":"X_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)","db215361":"# Create the param grid for logistic regression\n\n# Setting the range for class weights\nweights = np.linspace(0.01,.99,10)\n\n#Creating a dictionary grid for grid search\nparam_grid = {}\nlr_param_grid = {\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}","2a7a86b8":"# Build Logistic regression model\nlr_model = LogisticRegression(solver='liblinear', random_state=100)","15856597":"# Train model using grid search cv and stratified k fold method\nlr_grid = training_model_hyperparameter(lr_model, 'roc_auc', lr_param_grid, X_train_pca, y_train)","95e2f92c":"#Ploting the score for different values of weight\nsns.set_style('whitegrid')\nplt.figure(figsize=(12,8))\nweigh_data = pd.DataFrame({'score': lr_grid.cv_results_['mean_test_score'], 'weight': (1- weights)})\nsns.lineplot(weigh_data['weight'], weigh_data['score'])\nplt.xlabel('Weight for class 1')\nplt.ylabel('AUC ROC')\nplt.xticks([round(i\/10,1) for i in range(0,11,1)])\nplt.title('Scoring for different class weights', fontsize=24)","e4e3d96f":"# Print best parameters\nlr_grid.best_params_","445963b2":"# Predict the training and testing results \ny_train_pred_lr, y_train_pred_lr_prob, y_test_pred_lr, y_test_pred_lr_prob = \\\n                                    prediction_model(lr_grid, X_train_pca, y_train, X_test_pca, y_test)","bf97955f":"# Draw ROC\ndraw_roc(y_train, y_train_pred_lr_prob, y_test, y_test_pred_lr_prob)","d5f854e8":"print(metrics.classification_report(y_train, y_train_pred_lr))","73ecec5d":"print(metrics.classification_report(y_test, y_test_pred_lr))","4d7d6d71":"# Create the param grid for random forest\nweights = np.linspace(0.01,.99,5)\n\n# Keeping max depth low as number of features are high and max features is also in range of 10 to 20.\n# To not overfit our model using these parameters\nparam_grid_rf = [{\n               'max_depth': [3, 5, 7],\n               'max_features': [7, 10, 15],\n               'class_weight': [{0:x, 1:1.0-x} for x in weights]}]","3d0f7495":"rf_model = RandomForestClassifier(random_state=100)","14b38869":"# Train model using grid search cv and stratified k fold method\nrf_grid = training_model_hyperparameter(rf_model, 'roc_auc', param_grid_rf, X_train_pca, y_train)","20a46557":"rf_grid.best_params_","32f0863a":"y_train_pred_rf, y_train_pred_rf_prob, y_test_pred_rf, y_test_pred_rf_prob = \\\n                                    prediction_model(rf_grid, X_train_pca, y_train, X_test_pca, y_test)","9cd307df":"draw_roc(y_train, y_train_pred_rf_prob, y_test, y_test_pred_rf_prob)","c455fc19":"print(metrics.classification_report(y_train, y_train_pred_rf))","3651dd33":"print(metrics.classification_report(y_test, y_test_pred_rf))","3829a4ee":"# Create the param grid for random forest\nweights = np.linspace(0.01,.99,5)\n\nparam_grid_svc = [{\n               'class_weight': [{0:x, 1:1.0-x} for x in weights]}]","262e1a23":"svc_model = SVC(probability=True, random_state=100)","e57a65b3":"# Train model using grid search cv and stratified k fold method\nsvc_grid = training_model_hyperparameter(svc_model, 'roc_auc', param_grid_svc, X_train_pca, y_train)","b64e7831":"svc_grid.best_params_","4718e149":"y_train_pred_svc, y_train_pred_svc_prob, y_test_pred_svc, y_test_pred_svc_prob = \\\n                                    prediction_model(svc_grid, X_train_pca, y_train, X_test_pca, y_test)","28f85dbf":"draw_roc(y_train, y_train_pred_svc_prob, y_test, y_test_pred_svc_prob)","fc08ec3f":"print(metrics.classification_report(y_train, y_train_pred_svc))","231cf277":"print(metrics.classification_report(y_test, y_test_pred_svc))","9a2cd4d7":"lr_model = LogisticRegression(solver='liblinear', class_weight={0:0.13, 1:0.87})\nlr_model.fit(X_train, y_train)","54b0fe33":"importances = lr_model.coef_[0]\nweights = pd.Series(importances,\n                 index=X_train.columns.values)\nweights.sort_values()[-15:].plot(kind = 'barh')","d27b59cd":"importances = logreg_rfe50.coef_[0]\nweights = pd.Series(importances,\n                 index=X_train_rfe50.columns.values)\nweights.sort_values()[-15:].plot(kind = 'barh')","9897e405":"rf_model = RandomForestClassifier(random_state=100, class_weight = {0: 0.255, 1: 0.745}, max_depth = 7, max_features = 20)","99d6966f":"rf_model.fit(X_train, y_train)","4fadea80":"importances = rf_model.feature_importances_\nweights = pd.Series(importances,\n                 index=X_train.columns.values)\nweights.sort_values()[-15:].plot(kind = 'barh')","2487fe3d":"\nIn terms of offnet there is a decrease in the values but its decrease is not significant as compared to other variables.","f426cc5d":"We will fill null values with 0 as it means that no recharge is done by the customer.","6e29de9b":"### iv. AdaBoost Classifier","14b12397":"For monthly 2g data we will take top 3 features which means only 0,1,2 will have one hot encoding and other values will be zero.","8eff64c3":"# Data Preprocessing","39a6548c":"We will look into the feature importance of random forest classifier as it has highest roc_auc score out of all the models.","3975b84e":"### iii. Random Forest Classifier","e898edb1":"Average revenue per user decrease for the churn customer in 8th month. ","a986ed7f":"## Business Problem Overview","4d06cce0":"\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase.","c35e8573":"# Telecom Churn - Case Study","7cf14e11":"After tagging churners, **remove all the attributes corresponding to the churn phase** (all attributes having \u2018 _9\u2019, etc. in their names).","24ba7aea":"### 3. Tag churners and remove attributes of the churn phase","4189b65f":"## Business Objective","f0340100":"By looking at the heatmap we can see high correlation between: <br>\n  \n    1. isd_og_mou_7 v\/s isd_og_mou_6 (0.93) <br> \n    2. isd_og_mou_8 v\/s isd_og_mou_6 (0.95) <br> \n    3. isd_og_mou_8 v\/s isd_og_mou_7 (0.95) <br>\n    4. total_og_mou_6 v\/s std_og_mou_6 (0.83) <br>\n    4. total_og_mou_7 v\/s std_og_mou_7 (0.85) <br>\n    4. total_og_mou_8 v\/s std_og_mou_8 (0.85) <br>\n    \nThis explains that isd outgoing calls minutes usage was almost same in all three months and total outgoing revenue is similar to std outgoing minutes usage.","57ef71c1":"# Train and test split","075060ec":"By looking at the heatmap we can see high correlation between: <br>\n  \n    1. count_rech_2g_6 v\/s total_rech_data_6 (0.89) <br> \n    2. count_rech_2g_7 v\/s total_rech_data_7 (0.89) <br> \n    3. count_rech_2g_8 v\/s total_rech_data_8 (0.89) <br> \n    4. total_recharge_amt_call_data_6 v\/s total_rech_data_6 (0.93) <br>\n    5. total_recharge_amt_call_data_7 v\/s total_rech_data_7 (0.93) <br>\n    6. total_recharge_amt_call_data_8 v\/s total_rech_data_8 (0.94) <br>\n    \n    \nThis explains that most data recharge done was for 2g data and most recharges are done by customers were of data.","54993959":"To impute the null values of date columns we will derive a recharge age\/interval columns here. It will help us handle users who did not take any data plans.\n<br>\nImputation method for age\/interval columns:<br\/>\n  \n    1. If both dates i.e. '_6' column date and '_7' column date is null then impute it will 0\n    2. If '_6' month date is null then impute with -1.\n    3. If '_7' month date is null then impute with -2.\n    4. Otherwise, take the difference of days.","dd4437e6":"Similar to volumne of 3g data consumption decreases in 8th month but the average revenue and schemes were similar.","2623e304":"### ii. Random Forest Classifier","c7fdb7f7":"For evaluation of the models we will use ROC AUC score. Due to imbalanced dataset and 1 being more important for us.","8a3ad3e4":"Total recharge amount data significantly decrease for the churn customer in 8th month. So data recharge amount is important features based on the boxplots above.","6bcb11aa":"### Categorical Columns","f7168311":"### No variance columns","3ab44e4f":"STD outgoing calls minutes usage significantly decrease for the churn customer in 8th month. Outgoing calls values are signifcantly higher as coming to incoming when it comes to STD calls.","0ebb050c":"The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.","c22c9c8d":"Values are not signicantly changed in max recharge data.","232dcae1":"We can create new feature as total_rech_amt_data using total_rech_data and av_rech_amt_data to capture amount utilized by customer for data.","2e98d1e4":"#### Now we need to impute the date values","ac750fc7":"# Exploratory Data Analysis","8aa8ddef":"Local outgoing calls minutes usage significantly decrease for the churn customer in 8th month. It depreciated more as compared to incoming calls.","bdce101c":"### i. Logistic Regression","61480631":"### Columns with dtypes as object","15249288":"In terms of onnet there is a decrease in the values but its decrease is not significant as compared to other variables.","2f70dda3":"#### By looking at this data, we will impute arpu_2g null values with zero because that means that user is not using data.","423ce302":"### v. Support Vector Machine","221328c3":"### Numerical Columns","6d317208":"**Feature importance without RFE:**","ba986016":"### Feature Engineering","1d941f67":"### 2. Filter High-Value Customers (HVC)","340b06f9":"STD incoming calls minutes usage significantly decrease for the churn customer in 8th month. It depreciated more as compared to incoming calls.","a7bf9678":"## 1. Models without PCA","86c39250":"**Feature importance with RFE**","1de687f4":"Local incoming calls minutes usage significantly decrease for the churn customer in 8th month.","7026ee42":"Total recharge amount for data and calls significantly decrease for the churn customer in 8th month.","dfc9d0b3":"### iii. Support Vector Machine","a264bd7c":"We will take the class weights which we got in our logistic regression model using hyperparameter tuning","55631c74":"# Feature Importance","464a6aa6":"Number of times recharge significantly decrease for the churn customer in 8th month.","994fa3ac":"# Scaling the features","3599a3e3":"Volumne of 2g data consumption decreases in 8th month but the average revenue and schemes were similar.","af13e013":"With PCA we got around same AUC score as before but we have only 43 features(components) now to deal with. <br>\nSVM performed better with PCA in comparision to other models with PCA in terms of AUC ROC.","11ab03bc":"By looking at the heatmap we can see high correlation between: <br>\n  \n    1. total_ic_mou_6 v\/s loc_ic_mou_6 (0.90) <br> \n    2. total_ic_mou_7 v\/s loc_ic_mou_7 (0.89) <br> \n    3. total_ic_mou_8 v\/s loc_ic_mou_8 (0.89) <br> \n    4. std_ic_mou_6 v\/s std_ic_t2f_mou_6 (0.81) <br> \n    5. std_ic_mou_7 v\/s std_ic_t2f_mou_7 (0.82) <br> \n    6. std_ic_mou_8 v\/s std_ic_t2f_mou_8 (0.85) <br>\n    \nThis explains that most data incoming minutes usage was for local calls.","2e3f2185":"#### Feature importance of random forest classifier","ee68581f":"We should not care much about data leakage in this case as missing values are very less.","bbb7a6b5":"In random forest we don't need to do feature selection as it robust to the higher number of dimension as it takes a set of max features in a model.","0d4a3954":"#### Now remaining data has less than 4% missing values for we will use median or mode approach on those columns (numerical and categorical respectively)","675091e1":"### **Explainable Models:** <br>","c8c2f700":"By looking at the heatmap we can see high correlation between: <br>\n  \n    1. arpu_2g_6 v\/s arpu_3g_6 (0.93) <br> \n    2. arpu_2g_7 v\/s arpu_3g_7 (0.93) <br> \n    3. arpu_2g_8 v\/s arpu_3g_8 (0.93) <br> \n    \nThis explains that 2g and 3g average revenue is almost same.","3b93a71e":"## 2. Models with PCA","413c93b1":"Total recharge amount significantly decrease for the churn customer in 8th month.","a7404290":"**AOU (Age on network - number of days the customer is using the operator T network):** <br>\nWe can most people who churn has smaller age as compared to non-churn customers.","5a6e3e74":"### Missing value treatment","31f62614":"### ii. Logistic Regression with feature selection","65076d87":"### i. Logistic Regression","98fa0406":"# Handle Outliers","70daa899":"Maximum recharge amount for data and calls significantly decrease for the churn customer in successive months.","a9ce41a8":"We will use these models to find the feature importance.","1e1c5bf6":"For columns which has many categories but has few values in them, we will use one hot encoding for multiclass variables concept.\n\nBased on the winning solution of KDD 2009 Cup i.e. we are going to limit the number of categories in the those variables to 10 most frequent labels.","ff6ea99a":"In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, **customer retention** has now become even more important than customer acquisition.\n<br>\n<br>\nTo reduce customer churn, telecom companies need to predict which customers are at high risk of churn.","6ae8c23f":"For nigh_pck_user we will use one hot encoding.","2f36b0aa":"### 1. Derive new features","28525ef4":"# Model Building","4cb18f21":"### Columns with dtypes as int and float","7c8eea79":"For monthly 3g data we will take top 5 features which means only 0,1,2,3,4 will have one hot encoding and other values will be zero."}}