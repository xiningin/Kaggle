{"cell_type":{"2ed4df54":"code","909192dc":"code","8c8411ec":"code","3da841c3":"code","7ed6045a":"code","4a595a75":"code","4a09e4c0":"code","2129aaf7":"code","be76c7fb":"code","e779d259":"code","c9c1f1a4":"code","b62c23ab":"code","ecff8e3d":"code","7621539f":"code","e2f32e53":"code","09405e38":"code","a880a328":"code","4b2119a6":"code","8602467d":"code","f809a0f2":"code","c28ab589":"code","fb0f419a":"code","e83c5914":"code","90d01b07":"code","de63c4ef":"code","24dcc86e":"code","6963dfcb":"code","29dbf4d3":"code","ee0ccbdc":"code","0f9b698a":"code","573dd44f":"code","101e8e83":"code","1d059303":"markdown","8d749b24":"markdown","56f10962":"markdown"},"source":{"2ed4df54":"DATA_DIR = '\/kaggle\/input'\n\n# Directory to save logs and trained model\nROOT_DIR = '\/kaggle\/working'\n\nimport os\nimport sys\nimport itertools\nimport math\nimport logging\nimport json\nimport re\nimport random\nfrom collections import OrderedDict\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.lines as lines\nfrom matplotlib.patches import Polygon\n\n!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))\nfrom mrcnn import utils\nfrom mrcnn import visualize\nfrom mrcnn.visualize import display_images\nimport mrcnn.model as modellib\nfrom mrcnn.model import log\nfrom mrcnn.config import Config\n\nimport pandas as pd  \nimport glob\n\n%matplotlib inline","909192dc":"\nimport cv2\nimport pydicom\nimport glob","8c8411ec":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):\n    \"\"\"Configuration for training pneumonia detection on the RSNA pneumonia dataset.\n    Overrides values in the base Config class.\n    \"\"\"\n    \n    # Give the configuration a recognizable name  \n    NAME = 'pneumonia'\n    \n    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n    # GPU because the images are small. Batch size is 8 (GPUs * images\/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 8 \n    \n    BACKBONE = 'resnet50'\n    \n    NUM_CLASSES = 2  # background + 1 pneumonia classes\n    \n    # Use small images for faster training. Set the limits of the small side\n    # the large side, and that determines the image shape.\n    IMAGE_MIN_DIM = 384\n    IMAGE_MAX_DIM = 384\n    \n    RPN_ANCHOR_SCALES = (32, 64)\n    \n    TRAIN_ROIS_PER_IMAGE = 16\n    \n    MAX_GT_INSTANCES = 3\n    \n    DETECTION_MAX_INSTANCES = 3\n    DETECTION_MIN_CONFIDENCE = 0.9\n    DETECTION_NMS_THRESHOLD = 0.1\n    \n    RPN_TRAIN_ANCHORS_PER_IMAGE = 16\n    STEPS_PER_EPOCH = 100 \n    TOP_DOWN_PYRAMID_SIZE = 32\n    STEPS_PER_EPOCH = 100\n    \n    \nconfig = DetectorConfig()","3da841c3":"train_dicom_dir = os.path.join(DATA_DIR, 'stage_1_train_images')\ntest_dicom_dir = os.path.join(DATA_DIR, 'stage_1_test_images')","7ed6045a":"def get_dicom_fps(dicom_dir):\n    dicom_fps = glob.glob(dicom_dir+'\/'+'*.dcm')\n    return list(set(dicom_fps))\n\ndef parse_dataset(dicom_dir, anns): \n    image_fps = get_dicom_fps(dicom_dir)\n    image_annotations = {fp: [] for fp in image_fps}\n    for index, row in anns.iterrows(): \n        fp = os.path.join(dicom_dir, row['patientId']+'.dcm')\n        image_annotations[fp].append(row)\n    return image_fps, image_annotations \n\nclass DetectorDataset(utils.Dataset):\n    \"\"\"Dataset class for training pneumonia detection on the RSNA pneumonia dataset.\n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('pneumonia', 1, 'Lung Opacity')\n   \n        # add images \n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations[fp]\n            self.add_image('pneumonia', image_id=i, path=fp, \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        ds = pydicom.read_file(fp)\n        image = ds.pixel_array\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                if a['Target'] == 1:\n                    x = int(a['x'])\n                    y = int(a['y'])\n                    w = int(a['width'])\n                    h = int(a['height'])\n                    mask_instance = mask[:, :, i].copy()\n                    cv2.rectangle(mask_instance, (x, y), (x+w, y+h), 255, -1)\n                    mask[:, :, i] = mask_instance\n                    class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","4a595a75":"anns = pd.read_csv(os.path.join(DATA_DIR, 'stage_1_train_labels.csv'))\nimage_fps, image_annotations = parse_dataset(train_dicom_dir, anns=anns)","4a09e4c0":"# Original DICOM image size: 1024 x 1024\nORIG_SIZE = 1024\n\n######################################################################\n# Modify this line to use more or fewer images for training\/validation. \n# To use all images, do: image_fps_list = list(image_fps)\nimage_fps_list = list(image_fps[:5000]) \n#####################################################################\n\n# split dataset into training vs. validation dataset \n# split ratio is set to 0.9 vs. 0.1 (train vs. validation, respectively)\nsorted(image_fps_list)\nrandom.seed(42)\nrandom.shuffle(image_fps_list)\n\nvalidation_split = 0.1\nsplit_index = int((1 - validation_split) * len(image_fps_list))\n\nimage_fps_train = image_fps_list[:split_index]\nimage_fps_val = image_fps_list[split_index:]\n\nprint(len(image_fps_train), len(image_fps_val))","2129aaf7":"dataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","be76c7fb":"dataset = dataset_train","e779d259":"masked = [i for i in dataset.image_ids if np.sum(dataset.load_mask(i)[0]) > 0]","c9c1f1a4":"len(dataset.image_ids), len(masked)","b62c23ab":"np.hstack([np.random.choice(dataset.image_ids, 4), np.random.choice(masked, 4)])","ecff8e3d":"# Load random image and mask.\nimage_id = random.choice(masked)\n# 1572\nimage = dataset.load_image(image_id)\nmask, class_ids = dataset.load_mask(image_id)\n# Compute Bounding box\nbbox = utils.extract_bboxes(mask)\n\n# Display image and additional stats\nprint(\"image_id \", image_id, dataset.image_reference(image_id))\nlog(\"image\", image)\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"bbox\", bbox)\n# Display image and instances\nvisualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)","7621539f":"# Load random image and mask.\ndef show_image_with_resized(image_id):\n\n    # image_id = 1572\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n    # Display image and instances\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)\n\n    original_shape = image.shape\n    # Resize\n    image, window, scale, padding, _ = utils.resize_image(\n        image, \n        min_dim=config.IMAGE_MIN_DIM, \n        max_dim=config.IMAGE_MAX_DIM,\n        mode=config.IMAGE_RESIZE_MODE)\n    mask = utils.resize_mask(mask, scale, padding)\n    # Compute Bounding box\n    bbox = utils.extract_bboxes(mask)\n\n    # Display image and additional stats\n    print(\"image_id: \", image_id, dataset.image_reference(image_id))\n\n    # Display image and instances\n    visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names, show_mask=False)\n    \nshow_image_with_resized(random.choice(dataset.image_ids))\nshow_image_with_resized(random.choice(masked))","e2f32e53":"from https:\/\/www.kaggle.com\/thomasjpfan\/q-a-with-only-pictures it seems that ratio should start at 0.3\n    ","09405e38":"# are there any images with bbox but not diagnosed?\nnp.sum((anns['width'] > 0) & (anns['Target'] == 0))","a880a328":"# are there any images without bbox but diagnosed?\nnp.sum(~(anns['width'] > 0) & (anns['Target'] > 0))","4b2119a6":"predicted_anns = anns[anns.Target == 1]","8602467d":"import seaborn as sns\nsns.distplot(predicted_anns['width'])\nsns.distplot(predicted_anns['height'])","f809a0f2":"sns.distplot((predicted_anns['width'] \/ predicted_anns['height']))","c28ab589":"# lets pick this configuration:\n\n#Ratios of anchors at each cell (width\/height)\nconfig.RPN_ANCHOR_RATIOS = [0.3,0.5,1,2]\nconfig.RPN_ANCHOR_SCALES = np.array([96, 192, 256, 384, 512]) * (config.IMAGE_MIN_DIM \/ 1024)\nconfig.RPN_ANCHOR_SCALES","fb0f419a":"# Generate Anchors\nbackbone_shapes = modellib.compute_backbone_shapes(config, config.IMAGE_SHAPE)\nanchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES, \n                                          config.RPN_ANCHOR_RATIOS,\n                                          backbone_shapes,\n                                          config.BACKBONE_STRIDES, \n                                          config.RPN_ANCHOR_STRIDE)\n\n# Print summary of anchors\nnum_levels = len(backbone_shapes)\nanchors_per_cell = len(config.RPN_ANCHOR_RATIOS)\nprint(\"Count: \", anchors.shape[0])\nprint(\"Scales: \", config.RPN_ANCHOR_SCALES)\nprint(\"ratios: \", config.RPN_ANCHOR_RATIOS)\nprint(\"Anchors per Cell: \", anchors_per_cell)\nprint(\"Levels: \", num_levels)\nanchors_per_level = []\nfor l in range(num_levels):\n    num_cells = backbone_shapes[l][0] * backbone_shapes[l][1]\n    anchors_per_level.append(anchors_per_cell * num_cells \/\/ config.RPN_ANCHOR_STRIDE**2)\n    print(\"Anchors in Level {}: {}\".format(l, anchors_per_level[l]))","e83c5914":"## Visualize anchors of one cell at the center of the feature map of a specific level\n\n# Load and draw random image\nimage_id = np.random.choice(dataset.image_ids, 1)[0]\nimage, image_meta, _, _, _ = modellib.load_image_gt(dataset, config, image_id)\nfig, ax = plt.subplots(1, figsize=(10, 10))\nax.imshow(image)\nlevels = len(backbone_shapes)\n\nfor level in range(levels):\n    colors = visualize.random_colors(levels)\n    # Compute the index of the anchors at the center of the image\n    level_start = sum(anchors_per_level[:level]) # sum of anchors of previous levels\n    level_anchors = anchors[level_start:level_start+anchors_per_level[level]]\n    print(\"Level {}. Anchors: {:6}  Feature map Shape: {}\".format(level, level_anchors.shape[0], \n                                                                  backbone_shapes[level]))\n    center_cell = backbone_shapes[level] \/\/ 2\n    center_cell_index = (center_cell[0] * backbone_shapes[level][1] + center_cell[1])\n    level_center = center_cell_index * anchors_per_cell \n    center_anchor = anchors_per_cell * (\n        (center_cell[0] * backbone_shapes[level][1] \/ config.RPN_ANCHOR_STRIDE**2) \\\n        + center_cell[1] \/ config.RPN_ANCHOR_STRIDE)\n    level_center = int(center_anchor)\n\n    # Draw anchors. Brightness show the order in the array, dark to bright.\n    for i, rect in enumerate(level_anchors[level_center:level_center+anchors_per_cell]):\n        y1, x1, y2, x2 = rect\n        p = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, facecolor='none',\n                              edgecolor=(i+1)*np.array(colors[level]) \/ anchors_per_cell)\n        ax.add_patch(p)","90d01b07":"# let's display some image with regions:\n\n","de63c4ef":"# Create data generator\nrandom_rois = 2000\ng = modellib.data_generator(\n    dataset, config, shuffle=True, random_rois=random_rois, \n    batch_size=4,\n    detection_targets=True)\n\n# Uncomment to run the generator through a lot of images\n# to catch rare errors\n# for i in range(1000):\n#     print(i)\n#     _, _ = next(g)\n\n# Get Next Image\nif random_rois:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_class_ids, gt_boxes, gt_masks, rpn_rois, rois], \\\n    [mrcnn_class_ids, mrcnn_bbox, mrcnn_mask] = next(g)\n    \n    log(\"rois\", rois)\n    log(\"mrcnn_class_ids\", mrcnn_class_ids)\n    log(\"mrcnn_bbox\", mrcnn_bbox)\n    log(\"mrcnn_mask\", mrcnn_mask)\nelse:\n    [normalized_images, image_meta, rpn_match, rpn_bbox, gt_boxes, gt_masks], _ = next(g)\n    \nlog(\"gt_class_ids\", gt_class_ids)\nlog(\"gt_boxes\", gt_boxes)\nlog(\"gt_masks\", gt_masks)\nlog(\"rpn_match\", rpn_match, )\nlog(\"rpn_bbox\", rpn_bbox)\nimage_id = modellib.parse_image_meta(image_meta)[\"image_id\"][0]\nprint(\"image_id: \", image_id, dataset.image_reference(image_id))\n\n# Remove the last dim in mrcnn_class_ids. It's only added\n# to satisfy Keras restriction on target shape.\nmrcnn_class_ids = mrcnn_class_ids[:,:,0]","24dcc86e":"b = 0\n\n# Restore original image (reverse normalization)\nsample_image = modellib.unmold_image(normalized_images[b], config)\n\n# Compute anchor shifts.\nindices = np.where(rpn_match[b] == 1)[0]\nrefined_anchors = utils.apply_box_deltas(anchors[indices], rpn_bbox[b, :len(indices)] * config.RPN_BBOX_STD_DEV)\nlog(\"anchors\", anchors)\nlog(\"refined_anchors\", refined_anchors)\n\n# Get list of positive anchors\npositive_anchor_ids = np.where(rpn_match[b] == 1)[0]\nprint(\"Positive anchors: {}\".format(len(positive_anchor_ids)))\nnegative_anchor_ids = np.where(rpn_match[b] == -1)[0]\nprint(\"Negative anchors: {}\".format(len(negative_anchor_ids)))\nneutral_anchor_ids = np.where(rpn_match[b] == 0)[0]\nprint(\"Neutral anchors: {}\".format(len(neutral_anchor_ids)))\n\n# ROI breakdown by class\nfor c, n in zip(dataset.class_names, np.bincount(mrcnn_class_ids[b].flatten())):\n    if n:\n        print(\"{:23}: {}\".format(c[:20], n))\n\n# Show positive anchors\nvisualize.draw_boxes(sample_image, boxes=anchors[positive_anchor_ids], \n                     refined_boxes=refined_anchors)","6963dfcb":"visualize.draw_boxes(sample_image, boxes=anchors[negative_anchor_ids])","29dbf4d3":"visualize.draw_boxes(sample_image, boxes=anchors[np.random.choice(neutral_anchor_ids, 100)])","ee0ccbdc":"if random_rois:\n    # Class aware bboxes\n    bbox_specific = mrcnn_bbox[b, np.arange(mrcnn_bbox.shape[1]), mrcnn_class_ids[b], :]\n\n    # Refined ROIs\n    refined_rois = utils.apply_box_deltas(rois[b].astype(np.float32), bbox_specific[:,:4] * config.BBOX_STD_DEV)\n\n    # Class aware masks\n    mask_specific = mrcnn_mask[b, np.arange(mrcnn_mask.shape[1]), :, :, mrcnn_class_ids[b]]\n\n    visualize.draw_rois(sample_image, rois[b], refined_rois, mask_specific, mrcnn_class_ids[b], dataset.class_names)\n    \n    # Any repeated ROIs?\n    rows = np.ascontiguousarray(rois[b]).view(np.dtype((np.void, rois.dtype.itemsize * rois.shape[-1])))\n    _, idx = np.unique(rows, return_index=True)\n    print(\"Unique ROIs: {} out of {}\".format(len(idx), rois.shape[1]))","0f9b698a":"if random_rois:\n    # Dispalay ROIs and corresponding masks and bounding boxes\n    ids = random.sample(range(rois.shape[1]), 8)\n\n    images = []\n    titles = []\n    for i in ids:\n        image = visualize.draw_box(sample_image.copy(), rois[b,i,:4].astype(np.int32), [255, 0, 0])\n        image = visualize.draw_box(image, refined_rois[i].astype(np.int64), [0, 255, 0])\n        images.append(image)\n        titles.append(\"ROI {}\".format(i))\n        images.append(mask_specific[i] * 255)\n        titles.append(dataset.class_names[mrcnn_class_ids[b,i]][:20])\n\n    display_images(images, titles, cols=4, cmap=\"Blues\", interpolation=\"none\")","573dd44f":"# Check ratio of positive ROIs in a set of images.\nif random_rois:\n    limit = 10\n    temp_g = modellib.data_generator(\n        dataset, config, shuffle=True, random_rois=10000, \n        batch_size=1, detection_targets=True)\n    total = 0\n    for i in range(limit):\n        _, [ids, _, _] = next(temp_g)\n        positive_rois = np.sum(ids[0] > 0)\n        total += positive_rois\n        print(\"{:5} {:5.2f}\".format(positive_rois, positive_rois\/ids.shape[1]))\n    print(\"Average percent: {:.2f}\".format(total\/(limit*ids.shape[1])))","101e8e83":"! rm -rf \/kaggle\/working\/*","1d059303":"# RPN explortion from mask_rcnn model:\nBased on:\n1. official exploration: https:\/\/github.com\/matterport\/Mask_RCNN\/blob\/master\/samples\/coco\/inspect_data.ipynb\n2. starter notebook for mask-rcnn: https:\/\/www.kaggle.com\/drt2290078\/mask-rcnn-sample-starter-code\n\nWith that we can look at what is the expected resolution and what should be some hyperparams regarding region proposal network\n    ","8d749b24":"# what should candidate region look like...\n\n## First let's look at the prior...","56f10962":"# Choosing the minimal resolution\n\nI want to see how do the xrays look after resizing, and whether it is theoretically possibe to localize opacity for a resized image.\n\nI found out that 256x256 is too small at least for some images while 384 might be enough. I choose this as a minimal acceptable resolution for convolutional models."}}