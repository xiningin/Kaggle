{"cell_type":{"fe14dd72":"code","1bf1fa52":"code","430b15f6":"code","0bf1d8ad":"code","1e2dd766":"code","ba159278":"code","5fcdfa25":"code","2cc5fa6c":"code","a629fa36":"code","2ee271e4":"code","3bde0070":"code","4a6cf08c":"code","8ce0d75c":"code","8fec4ea6":"code","21ba3e85":"code","7094d4dd":"markdown"},"source":{"fe14dd72":"%matplotlib inline\nimport pandas as pd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\npd.set_option('display.max_columns', 99)\npd.set_option('display.max_rows', 99)\nimport os\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\nimport datetime as dt\nimport xgboost as xgb\nfrom sklearn import preprocessing\nfrom scipy.stats import gmean","1bf1fa52":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\n\nimport plotly.express as px\nimport plotly.graph_objects as go","430b15f6":"\nDATA_PATH = '..\/input\/covid19-metadata'\nDATEFORMAT = '%Y-%m-%d'\n\n\ndef get_comp_data(COMP):\n    train = pd.read_csv(f'{COMP}\/train.csv')\n    test = pd.read_csv(f'{COMP}\/test.csv')\n    submission = pd.read_csv(f'{COMP}\/submission.csv')\n    print(train.shape, test.shape, submission.shape)\n    train['Country_Region'] = train['Country_Region'].str.replace(',', '')\n    test['Country_Region'] = test['Country_Region'].str.replace(',', '')\n\n    train['Location'] = train['Country_Region'] + '-' + train['Province_State'].fillna('')\n\n    test['Location'] = test['Country_Region'] + '-' + test['Province_State'].fillna('')\n\n    train['LogConfirmed'] = to_log(train.ConfirmedCases)\n    train['LogFatalities'] = to_log(train.Fatalities)\n    train = train.drop(columns=['Province_State'])\n    test = test.drop(columns=['Province_State'])\n\n    country_codes = pd.read_csv(f'{DATA_PATH}\/country_codes.csv', keep_default_na=False)\n    train = train.merge(country_codes, on='Country_Region', how='left')\n    test = test.merge(country_codes, on='Country_Region', how='left')\n\n    train['DateTime'] = pd.to_datetime(train['Date'])\n    test['DateTime'] = pd.to_datetime(test['Date'])\n\n    train = train.sort_values(by='Date')\n    test = test.sort_values(by='Date')\n\n    train = train.fillna('#N\/A')\n    test = test.fillna('#N\/A')\n\n    return train, test, submission\n\n\ndef process_each_location(df):\n    dfs = []\n    for loc, df in df.groupby('Location'):\n        df = df.sort_values(by='Date')\n        df['Fatalities'] = df['Fatalities'].cummax()\n        df['ConfirmedCases'] = df['ConfirmedCases'].cummax()\n        df['LogFatalities'] = df['LogFatalities'].cummax()\n        df['LogConfirmed'] = df['LogConfirmed'].cummax()\n        df['LogConfirmedNextDay'] = df['LogConfirmed'].shift(-1)\n        df['ConfirmedNextDay'] = df['ConfirmedCases'].shift(-1)\n        df['DateNextDay'] = df['Date'].shift(-1)\n        df['LogFatalitiesNextDay'] = df['LogFatalities'].shift(-1)\n        df['FatalitiesNextDay'] = df['Fatalities'].shift(-1)\n        df['LogConfirmedDelta'] = df['LogConfirmedNextDay'] - df['LogConfirmed']\n        df['ConfirmedDelta'] = df['ConfirmedNextDay'] - df['ConfirmedCases']\n        df['LogFatalitiesDelta'] = df['LogFatalitiesNextDay'] - df['LogFatalities']\n        df['FatalitiesDelta'] = df['FatalitiesNextDay'] - df['Fatalities']\n        dfs.append(df)\n    return pd.concat(dfs)\n\n\ndef add_days(d, k):\n    return dt.datetime.strptime(d, DATEFORMAT) + dt.timedelta(days=k)\n\n\ndef to_log(x):\n    return np.log(x + 1)\n\n\ndef to_exp(x):\n    return np.exp(x) - 1\n\n\ndef create_features(train_set):\n    dfs = []\n    for loc, df in train_set.groupby('Location'):\n        df = df.sort_values(by='Date').copy()\n        df['f_lc_7d'] = df['LogConfirmed'].shift(7)\n        df['f_lf_7d'] = df['LogFatalities'].shift(7)\n        df['f_lc_3d'] = df['LogConfirmed'].shift(3)\n        df['f_lf_3d'] = df['LogFatalities'].shift(3)\n        df['f_lc_1d'] = df['LogConfirmed'].shift(1)\n        df['f_lf_1d'] = df['LogFatalities'].shift(1)\n        df['f_lc_0d'] = df['LogConfirmed']\n        df['f_lf_0d'] = df['LogFatalities']\n        df['f_fc_rate'] = np.clip((to_exp(df['LogFatalities']) + 1) \/ (to_exp(df['LogConfirmed']) + 1), 0, 0.15)\n        dfs.append(df)\n    dfs = pd.concat(dfs)\n    dfs['d_lc_7d'] = dfs['f_lc_0d'] - dfs['f_lc_7d']\n    dfs['d_lf_7d'] = dfs['f_lf_0d'] - dfs['f_lf_7d']\n    dfs['d_lc_3d'] = dfs['f_lc_3d'] - dfs['f_lc_3d']\n    dfs['d_lf_3d'] = dfs['f_lf_3d'] - dfs['f_lf_3d']\n    dfs['d_lc_1d'] = dfs['f_lc_0d'] - dfs['f_lc_1d']\n    dfs['d_lf_1d'] = dfs['f_lf_0d'] - dfs['f_lf_1d']\n    return dfs\n","0bf1d8ad":"\nCOMP = '..\/input\/covid19-global-forecasting-week-4'\nstart = dt.datetime.now()\ntrain, test, submission = get_comp_data(COMP)\ntrain.shape, test.shape, submission.shape\ntrain.head(2)\ntest.head(2)\n\nTRAIN_START = train.Date.min()\nTEST_START = test.Date.min()\nTRAIN_END = train.Date.max()\nTEST_END = test.Date.max()\nprint(TRAIN_START, TRAIN_END, TEST_START, TEST_END)","1e2dd766":"train_clean = process_each_location(train)\n\nprint('train cleaned', train_clean.shape)\n\ntrain_clean = train_clean[[\n    'Location', 'Date', 'continent',\n    'LogConfirmed', 'LogFatalities',\n    'LogConfirmedDelta', 'LogFatalitiesDelta'\n]]\n\ncontinent_encoder = preprocessing.LabelEncoder()\ntrain_clean['f_continent'] = continent_encoder.fit_transform(train_clean.continent)\n\ntrain_features = create_features(train_clean)\ntrain_features.head()","ba159278":"def predict(min_child_weight, eta, colsample_bytree, max_depth, subsample,\n           NROUND, PRECISION, DECAY, WEIGHT_NORM, MIN_DATE):\n    train, test, submission = get_comp_data(COMP)\n    train_clean = process_each_location(train)\n    print('train cleaned', train_clean.shape)\n\n    train_clean = train_clean[[\n        'Location', 'Date', 'continent',\n        'LogConfirmed', 'LogFatalities',\n        'LogConfirmedDelta', 'LogFatalitiesDelta'\n    ]]\n\n    continent_encoder = preprocessing.LabelEncoder()\n    train_clean['f_continent'] = continent_encoder.fit_transform(train_clean.continent)\n\n    train_features = create_features(train_clean)\n\n    VALID_CUTOFF = TRAIN_END\n\n    features = [\n        'f_lc_7d', 'f_lf_7d', 'f_lc_3d', 'f_lf_3d', 'f_continent',\n        'f_lc_1d', 'f_lf_1d', 'f_lc_0d', 'f_lf_0d', 'f_fc_rate'\n    ]\n    diff_features = ['d_lc_7d', 'd_lf_7d', 'd_lc_3d', 'd_lf_3d', 'd_lc_1d', 'd_lf_1d']\n    features = features + diff_features\n    print(f'{len(features)} features: ')\n    print(features)\n    \n    fix = {\n    'lambda': 1., 'nthread': 3, 'booster': 'gbtree',\n    'silent': 1, 'eval_metric': 'rmse',\n    'objective': 'reg:squarederror'}\n    config = dict(\n        min_child_weight=min_child_weight,\n        eta=eta, colsample_bytree=colsample_bytree,\n        max_depth=max_depth, subsample=subsample)\n    config.update(fix)\n\n    Xtr = train_features[(train_features.Date >= MIN_DATE) & (train_features.Date < VALID_CUTOFF)].copy()\n    Xtr['days'] = -(pd.to_datetime(train_features.Date) - dt.datetime.strptime(VALID_CUTOFF, DATEFORMAT)).dt.days\n    \n    print(Xtr.shape)\n    print(config)\n    print(PRECISION, NROUND, DECAY, WEIGHT_NORM)\n\n    def weighting(days):\n        return 1. \/ days ** WEIGHT_NORM\n\n    dtrain_lc = xgb.DMatrix(Xtr[features].round(PRECISION), label=Xtr.LogConfirmedDelta, weight=weighting(Xtr.days))\n    dtrain_lf = xgb.DMatrix(Xtr[features].round(PRECISION), label=Xtr.LogFatalitiesDelta, weight=weighting(Xtr.days))\n\n    model_lc = xgb.train(config, dtrain_lc, NROUND, evals=[(dtrain_lc, 'train-lc')], verbose_eval=100)\n    model_lf = xgb.train(config, dtrain_lf, NROUND, evals=[(dtrain_lf, 'train-lf')], verbose_eval=100)\n\n    # Predict\n\n    predictions = Xtr.copy()\n    predictions = train_features[(train_features.Date >= MIN_DATE) & (train_features.Date <= VALID_CUTOFF)].copy()\n    predictions.LogConfirmedDelta = np.nan\n    predictions.LogFatalitiesDelta = np.nan\n\n    for i, d in enumerate(pd.date_range(VALID_CUTOFF, add_days(TEST_END, 1))):\n        last_day = str(d).split(' ')[0]\n        next_day = dt.datetime.strptime(last_day, DATEFORMAT) + dt.timedelta(days=1)\n        next_day = next_day.strftime(DATEFORMAT)\n\n        p_next_day = predictions[predictions.Date == last_day].copy()\n        p_next_day.Date = next_day\n        p_next_day['plc'] = model_lc.predict(xgb.DMatrix(p_next_day[features].round(PRECISION)))\n        p_next_day['plf'] = model_lf.predict(xgb.DMatrix(p_next_day[features].round(PRECISION)))\n\n        p_next_day.LogConfirmed = p_next_day.LogConfirmed + np.clip(p_next_day['plc'], 0, None) * DECAY ** i\n        p_next_day.LogFatalities = p_next_day.LogFatalities + np.clip(p_next_day['plf'], 0, None) * DECAY ** i\n\n        predictions = pd.concat([predictions, p_next_day], sort=True)\n        predictions = create_features(predictions)\n\n    predictions['PC'] = to_exp(predictions.LogConfirmed)\n    predictions['PF'] = to_exp(predictions.LogFatalities)\n    return predictions","5fcdfa25":"decay = 0.99\nprediction_1 = predict(min_child_weight=5, eta=0.01, colsample_bytree=0.8, max_depth=5, subsample=0.9,\n           NROUND=800, PRECISION=2, DECAY=decay, WEIGHT_NORM=0.25, MIN_DATE='2020-03-22')\nprediction_2 = predict(min_child_weight=7, eta=0.01, colsample_bytree=0.7, max_depth=6, subsample=0.8,\n           NROUND=1000, PRECISION=2, DECAY=decay, WEIGHT_NORM=0.23, MIN_DATE='2020-03-15')\nprediction_3 = predict(min_child_weight=3, eta=0.01, colsample_bytree=0.6, max_depth=7, subsample=0.7,\n           NROUND=1200, PRECISION=2, DECAY=decay, WEIGHT_NORM=0.2, MIN_DATE='2020-03-08')\nprediction_4 = predict(min_child_weight=3, eta=0.011, colsample_bytree=0.75, max_depth=10, subsample=0.6,\n           NROUND=1200, PRECISION=3, DECAY=decay, WEIGHT_NORM=0.15, MIN_DATE='2020-03-15')\nprediction_5 = predict(min_child_weight=10, eta=0.008, colsample_bytree=0.75, max_depth=10, subsample=0.6,\n           NROUND=1500, PRECISION=3, DECAY=decay, WEIGHT_NORM=0.2, MIN_DATE='2020-03-22')\nprediction_6 = predict(min_child_weight=20, eta=0.01, colsample_bytree=0.7, max_depth=5, subsample=0.85,\n           NROUND=1000, PRECISION=3, DECAY=decay, WEIGHT_NORM=0.3, MIN_DATE='2020-03-22')","2cc5fa6c":"cols = ['Location', 'Date', 'PC', 'PF']\np12 = pd.merge(prediction_1[cols], prediction_2[cols], on=['Location', 'Date'], suffixes=['_1', '_2'])\np34 = pd.merge(prediction_3[cols], prediction_4[cols], on=['Location', 'Date'], suffixes=['_3', '_4'])\np56 = pd.merge(prediction_5[cols], prediction_6[cols], on=['Location', 'Date'], suffixes=['_5', '_6'])\n\npreds = pd.merge(p12, p34, on=['Location', 'Date'])\npreds = pd.merge(preds, p56, on=['Location', 'Date'])\npreds.head()\n\nc = preds.loc[preds.Date >= '2020-04-15'].corr()\nc\nfig = px.imshow(c)\nfig.show()","a629fa36":"pcs = ['PC_1', 'PC_2', 'PC_3', 'PC_4', 'PC_5', 'PC_6']\npfs = ['PF_1', 'PF_2', 'PF_3', 'PF_4', 'PF_5', 'PF_6']\npreds['PC'] = to_exp(to_log(preds[pcs]).mean(axis=1))\npreds['PF'] = to_exp(to_log(preds[pfs]).mean(axis=1))\npreds.tail()","2ee271e4":"top_locations = preds[preds.Date == TRAIN_END].sort_values(by='PF', ascending=False).Location.values[:25]\nfig3 = px.line(preds[preds.Location.isin(top_locations)],\n               x='Date', y='PC', color='Location')\n_ = fig3.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Predicted Cumulative Confirmed Cases by Location [Updated: {TRAIN_END}]'\n)\nfig3.show()","3bde0070":"top_locations = preds[preds.Date == TRAIN_END].sort_values(by='PF', ascending=False).Location.values[:25]\nfig3 = px.line(preds[preds.Location.isin(top_locations)],\n               x='Date', y='PF', color='Location')\n_ = fig3.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Predicted Cumulative Deaths by Location [Updated: {TRAIN_END}]'\n)\nfig3.show()","4a6cf08c":"total = preds.groupby('Date')[['PC', 'PF'] + pcs + pfs].sum().reset_index()\ntotal.tail()\nfig2 = px.line(pd.melt(total, id_vars=['Date']), x='Date', y='value', color='variable')\n_ = fig2.update_layout(\n    yaxis_type=\"log\",\n    title_text=f'COVID-19 Cumulative Prediction Total [Updated: {TRAIN_END}]'\n)\nfig2.show()","8ce0d75c":"my_submission = test.copy()\nmy_submission = my_submission.merge(preds)\nmy_submission['ConfirmedCases'] = my_submission['PC']\nmy_submission['Fatalities'] = my_submission['PF']\nmy_submission.shape\nmy_submission.head()","8fec4ea6":"my_submission[[\n    'ForecastId', 'ConfirmedCases', 'Fatalities'\n]].to_csv('submission.csv', index=False)\n\nmy_submission.groupby('Date')[['ConfirmedCases', 'Fatalities']].sum().reset_index().tail()","21ba3e85":"end = dt.datetime.now()\nprint('Finished', end, (end - start).seconds, 's')","7094d4dd":"## Create submission"}}