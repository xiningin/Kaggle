{"cell_type":{"ad6f8cb0":"code","fb63eefb":"code","52b56726":"code","edef51fc":"code","eee5912f":"code","d0dfc56b":"code","62dd8486":"code","52dd8728":"code","aab7bf1e":"code","2ff8418e":"code","d3acf3d1":"code","d4065495":"code","f856f07b":"code","fee92006":"code","de64c12e":"code","230df75b":"code","adafe604":"code","ff451eb5":"code","ef9ee8e2":"code","8cde6307":"code","b87ec51e":"code","0993a4b3":"code","c88f614e":"code","912346e9":"code","9a26d139":"code","ac1586dd":"code","35390700":"code","7785fed7":"code","a7d6a817":"markdown","1b2e28d4":"markdown","d269ee89":"markdown","559e4c16":"markdown","1548402d":"markdown","7faa7627":"markdown","d342662f":"markdown","b85963f4":"markdown","45243058":"markdown","68c14b58":"markdown","87145f84":"markdown","2b8a2c13":"markdown","0d2c4551":"markdown","d0de913b":"markdown"},"source":{"ad6f8cb0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split # to split input data to train\/dev\nimport matplotlib.pyplot as plt # draw data\nimport tensorflow as tf\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb63eefb":"\nX = pd.read_csv(filepath_or_buffer=\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nY = X['label']\nX = X.drop(columns='label')\ntrain_x, dev_x, train_y, dev_y = train_test_split(X, Y, test_size=0.05, random_state=13)\nprint(\"Train_x:\", train_x.shape, \", dev_x:\", dev_x.shape, \"\\nTrain_y:\", train_y.shape, \", dev_y:\", dev_y.shape)","52b56726":"# Dataframe to numpy.ndarray\ntrain_x = train_x.to_numpy(dtype='float32', copy=True)\ndev_x = dev_x.to_numpy(dtype='float32', copy=True)\ntrain_y = train_y.to_numpy(dtype='int32', copy=True)\ndev_y = dev_y.to_numpy(dtype='int32', copy=True)\n\n# Normalization\ntrain_x = train_x \/ 255.\ndev_x = dev_x \/ 255.","edef51fc":"train_x = train_x.reshape((len(train_x),28,28))\n# Je\u017celi w jednym parametrze u\u017cyjemy -1, automatycznie wyliczy jej rozmiar.\ndev_x = dev_x.reshape(-1,28,28)\n# Ostatni\u0105 warstw\u0119 dodano po prezentacji danych","eee5912f":"#Plot hist\nplt.yscale('symlog')\nplt.hist(train_y, label=\"train\")\nplt.hist(dev_y, label=\"dev\")\nplt.legend(loc='lower center')\nplt.show()","d0dfc56b":"print(train_x.shape)\nprint(train_y.shape)\nprint(train_y[5])","62dd8486":"offset = 420 # If you want to choose pictures from a middle\nplt.figure(figsize=(15,10))\nfor i in range(15):\n    plt.subplot(3,5,i+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_x[i+offset], cmap=plt.cm.binary)\n    plt.xlabel(train_y[i+offset])\nplt.show()","52dd8728":"train_x = train_x.reshape(-1,28,28,1)\ndev_x = dev_x.reshape(-1,28,28,1)","aab7bf1e":"linear_model = tf.keras.Sequential()\n\nlinear_model.add(tf.keras.layers.Flatten(input_shape=(28,28,1)))\nlinear_model.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nlinear_model.summary()","2ff8418e":"linear_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n                    metrics=['accuracy'], loss_weights=None)","d3acf3d1":"linear_history = linear_model.fit(x=train_x,\n                    y=train_y,\n                    epochs=20,\n                    verbose=1,\n                    validation_data=(dev_x, dev_y))","d4065495":"plt.plot(linear_history.history['accuracy'], label='accuracy')\nplt.plot(linear_history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.xticks(np.arange(0, 20, step=2))\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')","f856f07b":"test_loss, test_acc = linear_model.evaluate(dev_x,  dev_y, verbose=2)","fee92006":"lenet_model = tf.keras.Sequential()\n# Conv6\nlenet_model.add(tf.keras.layers.Conv2D(6, \n                                 kernel_size=(5,5), \n                                 strides=(1, 1), \n                                 padding='same',\n                                 activation='relu',\n                                 input_shape=(28,28,1)))\n# Avgpool\nlenet_model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), \n                                           strides=2, \n                                           padding='valid'))\n# Conv16\nlenet_model.add(tf.keras.layers.Conv2D(16, \n                                 kernel_size=(5,5), \n                                 strides=(1, 1), \n                                 padding='valid',\n                                 activation='relu'))\n\n# Avgpool\nlenet_model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), \n                                           strides=2, \n                                           padding='valid'))\n# Flatten\nlenet_model.add(tf.keras.layers.Flatten())\n\n# FC120\nlenet_model.add(tf.keras.layers.Dense(120, activation='relu'))\n# FC84\nlenet_model.add(tf.keras.layers.Dense(84, activation='relu'))\n# Softmax\nlenet_model.add(tf.keras.layers.Dense(10, activation='softmax'))","de64c12e":"# Podsumowanie przygotowanego modelu, wy\u015bwietla ilo\u015b\u0107 parametr\u00f3w.\nlenet_model.summary()","230df75b":"lenet_model.compile(optimizer='adam', \n                    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n                    metrics=['accuracy'], loss_weights=None)","adafe604":"history = lenet_model.fit(x=train_x,\n                    y=train_y,\n                    epochs=20,\n                    verbose=1,\n                    validation_data=(dev_x, dev_y))","ff451eb5":"plt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.xticks(np.arange(0, 20, step=2))\nplt.ylabel('Accuracy')\nplt.ylim([0.5, 1])\nplt.legend(loc='lower right')","ef9ee8e2":"le_bias = tf.keras.Sequential()\n# Conv6\nle_bias.add(tf.keras.layers.Conv2D(6, \n                                 kernel_size=(5,5), \n                                 strides=(1, 1), \n                                 padding='same',\n                                 activation='relu',\n                                 input_shape=(28,28,1)))\nle_bias.add(tf.keras.layers.BatchNormalization())\n# Avgpool\nle_bias.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), \n                                           strides=2, \n                                           padding='valid'))\n# Conv16\nle_bias.add(tf.keras.layers.Conv2D(16, \n                                 kernel_size=(5,5), \n                                 strides=(1, 1), \n                                 padding='valid',\n                                 activation='relu'))\nle_bias.add(tf.keras.layers.BatchNormalization())\n# Avgpool\nle_bias.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), \n                                           strides=2, \n                                           padding='valid'))\n# Flatten\nle_bias.add(tf.keras.layers.Flatten())\nle_bias.add(tf.keras.layers.BatchNormalization())\n# FC120\nle_bias.add(tf.keras.layers.Dense(120, activation='relu'))\nle_bias.add(tf.keras.layers.Dropout(0.25))\nle_bias.add(tf.keras.layers.BatchNormalization())\n# FC120\nle_bias.add(tf.keras.layers.Dense(120, activation='relu'))\nle_bias.add(tf.keras.layers.Dropout(0.25))\nle_bias.add(tf.keras.layers.BatchNormalization())\n# FC84\nle_bias.add(tf.keras.layers.Dense(84, activation='relu'))\nle_bias.add(tf.keras.layers.BatchNormalization())\n# Softmax\nle_bias.add(tf.keras.layers.Dense(10, activation='softmax'))","8cde6307":"optimizer = tf.keras.optimizers.RMSprop(\n    learning_rate=0.001, rho=0.9, momentum=0.1, epsilon=1e-07, centered=False,\n    name='RMSprop')\nle_bias.compile(optimizer=optimizer, \n                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'], loss_weights=None)","b87ec51e":"le_bias.summary()","0993a4b3":"history_bias = le_bias.fit(x=train_x,\n                    y=train_y,\n                    epochs=40,\n                    batch_size=128,\n                    verbose=1,\n                    validation_data=(dev_x, dev_y))","c88f614e":"plt.plot(rms_history.history['accuracy'][-20:], label='accuracy', color='green')\nplt.plot(rms_history.history['val_accuracy'][-20:], label = 'val_accuracy', color='lightgreen')\nplt.plot(history_bias.history['accuracy'][-20:], label='b_accuracy', color='navy')\nplt.plot(history_bias.history['val_accuracy'][-20:], label = 'b_val_accuracy', color='lightblue')\nplt.xlabel('Epoch')\nplt.xticks(np.arange(0, 20, step=2))\nplt.ylabel('Accuracy')\nplt.yscale('logit')\nplt.legend(loc='lower right')","912346e9":"# import copy\n# rms_history = copy.copy(history_bias)","9a26d139":"test_x = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\ntest_x = test_x.to_numpy(copy=False)\ntest_x = test_x.reshape((-1,28,28,1)) \/ 255.0\nprint(\"Test_x:\", test_x.shape)","ac1586dd":"predictions = le_bias.predict(test_x).argmax(axis=-1)\nprint(\"Three random predictions:\", predictions[3:6])","35390700":"offset = 1\nplt.figure(figsize=(15,10))\nfor i in range(15):\n    plt.subplot(3,5,i+1)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(test_x[i+offset].reshape((28,28)), cmap=plt.cm.binary)\n    plt.xlabel(predictions[i+offset])\nplt.show()","7785fed7":"output = pd.DataFrame({'ImageId': np.arange(1, len(predictions)+1), 'Label': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a7d6a817":"## Przygotowanie modelu","1b2e28d4":"## Prezentacja danych\nZ wykorzystaniem biblioteki Mathplotlib, wy\u015bwietlenie kilku przyk\u0142adowych cyfr z etykietami.","d269ee89":"# LeNet - improve 1 ()\nB\u0119d\u0119 testowa\u0142 r\u00f3\u017cne pomys\u0142y w miejscu, a wnioski zamieszcz\u0119 w li\u015bcie poni\u017cej. W podsumowaniu umieszcz\u0119 tylko ten, kt\u00f3ry w istotny spos\u00f3b poprawi\u0142 problem z bias.\n\n* Rev point -                                              99.0% | 96.6%\n* Opt. alg - just RMSprop default -                        99.0% | 98.8%\n* Longer (40 epochs) training with Adam -                  99.0% | 98.8%\n* Longer (40 epochs) training with RMS  -                  99.4% | 99.0%\n* RMS 40 - l_rate=0.001, rho=0.9, mom=0.1, eps=1e-07 -     99.6% | 99.0% \n* Add Dense 120                                            99.1% | 98.8% X\n* Add Dense 120 with BatchNorm                             99.6% | 99.0% same as ~2\n* Dropout  0.2                                             99.3% | 99.0%\n* Dropout  0.5                                             99.3% | 99.0%\n* BatchNorm, Dropout .25                                   99.5% | 99.14%","559e4c16":"W podsumowaniu widzimy ilo\u015b\u0107 parametr\u00f3w do trenowania, tak jak zak\u0142adali\u015bmy jest ich oko\u0142o 60tys. Sk\u0142\u0105d te liczby? Np. 48120 parametr\u00f3w w przypadku dense_9, to suma Weights n[l] x n[l-1] = 400x120 oraz Bias n[l] = 120.","1548402d":"## Analiza wyniku\nW celu analizy wyniku i jako podstawa do dalszego rozwoju modelu, nale\u017cy por\u00f3wna\u0107 dok\u0142adno\u015b\u0107 trzech wynik\u00f3w: Human Level, Training i Dev Performence.\n\n\"Epoch 20\/20\n1247\/1247 [==============================] - 15s 12ms\/step - loss: 1.4709 - accuracy: **0.9901** - val_loss: 1.4751 - val_accuracy: **0.9857**\"\n* Human:    100% - sprawdzi\u0142em oko\u0142o 150 zdj\u0119\u0107, czy jestem wstanie odgand\u0105\u0107 co to za liczba, rozpozna\u0142em 100% ;)\n* Training: 99%\n* Dev:      98.6%\n\nR\u00f3\u017cnica pomi\u0119dzy warto\u015bci\u0105 Human a Training wynosi 1% (Avoidable Bias), a pomi\u0119dzy Training a Dev 0.4% (Variance)\nW pierwszej kolejno\u015bci zajmiemy si\u0119 Avoidable Bias, recepta: Bigger Model, Optimalize Algorythm.","7faa7627":"# Podsumowanie\nOszacowanie zdj\u0119\u0107 z pliku test. I zapisanie wyniku i ocena.","d342662f":"## Compile","b85963f4":"# CNN digits recognizer in Polish\n*I've briefly looked through the community solutions for this task and I will give neither better nor prettier solution, but I could sum up my basic knowlege for my compatriots - Poland friendly, in my native language. So... If you are english speeker person I'm sure you find some brilliant commitions here on Keggle ;)*\n\nCelem tego projektu jest wytrenowanie modelu, kt\u00f3ry rozpoznaje liczby pisane 0-9 na zdj\u0119ciach o rozmiarze 28x28x1.\nW projekcie wykorzystano Convolutional Neural Network, czyli w pe\u0142ni po\u0142\u0105czon\u0105 (Fully Connected) sie\u0107 neuron\u00f3w, poprzedzon\u0105 zestawem filtr\u00f3w.\nPierwsza cz\u0119\u015b\u0107 pozwala zmniejszy\u0107 ilo\u015b\u0107 danych wej\u015bciowych w wydajny spos\u00f3b, a co za tym idzie ilo\u015bci parametr\u00f3w i czasu treningu.\n\n1. [Przygotowanie danych](#przygotowanie-danych)\n2. [Regresja liniowa](#regresja-liniowa-(linear-regression))\n3. [LeNet 5](#LeNet-5)","45243058":"# Regresja liniowa (linear regression)\nPierwszy, najbardziej podstawowy model to regresja liniowa. Wykonano t\u0119 pr\u00f3b\u0119 w celu ustalenia punktu odniesienia.","68c14b58":"# LeNet-5\nBadacze zaproponowali na przestrzeni lat kilka r\u00f3\u017cnych rozwi\u0105za\u0144 dla CNN. Pierwsz\u0105 szeroko rozpowszechnion\u0105 architektur\u0105 by\u0142a LeNet-5 i opracowano j\u0105 dla obraz\u00f3w 32x32 pixele odcieni szaro\u015bci. Dlatego w pierwszej kolejno\u015bci si\u0119gniemy po to rozwi\u0105zanie.\nArchitektura LeNet-5 wygl\u0105da nast\u0119puj\u0105co:\n* Conv6 (6 docelowo warstw) f=5x5(frame=ramka) s=1(stride=krok) p=None(padding)\/Ja u\u017cyj\u0119 p=\"same\", \u017ceby po filtrze posiada\u0107 wci\u0105\u017c 28x28, tak jak w dokumentacji.\n* Avgpool f=2 s=2\n* Conv16 f=5x5 s=1 p=None\n* Avgpool f=2x2 s=2\n* FC 120\n* FC 84\n* Softmax\n\nZalet\u0105 jest niewielka ilo\u015b\u0107 parametr\u00f3w ~60tys.\nInnymi popularnymi architekturami s\u0105:\n* **AlexNet:** 60mln params, input 227x227x3,\n* **VGG-16:** 138mln params, input 224x224x3,\n* **GoogleNet:** 4mln params, input 224x224x3, ciekawe rozwi\u0105zanie z modu\u0142ami FC-softmax nie tylko jako ostatnimi warstwami (side branches) oraz filtrami wyst\u0119puj\u0105cymi r\u00f3wnolegle.","87145f84":"# Commit\nW ko\u0144cu mo\u017cna wys\u0142a\u0107 szacunki do oceny.","2b8a2c13":"## Dystrybucja\nCzy po losowym podziale na train i dev, posiadaj\u0105 mniej wi\u0119cej taki sam rozk\u0142ad danych wej\u015bciowych. Nale\u017cy unikn\u0105\u0107 przypadku, w kt\u00f3rym w jednej z grup, zabraknie jakiej\u015b klasy lub b\u0119dzie jej znacznie mniej.","0d2c4551":"# Przygotowanie danych\nPierwszym etapem procesu jest pobranie danych z pliku i odpowiednie ich zorganizowanie.\nOtrzymujemy dwa pliki: test i train. Plik train podzielimy na dwa zbiory: train i dev, a test b\u0119dzie naszym testem.\nZgodnie z teori\u0105 taki podzia\u0142 danych wykorzystujemy w nast\u0119puj\u0105cy spos\u00f3b: \n* Train - trenowania naszych modeli\n* Dev - do test\u00f3w developerskich, s\u0142u\u017c\u0105 do wielokrotnego sprawdzania i ulepszania modelu\n* Test - powinien by\u0107 traktowany jako test gotowego modelu i pomiar jego sprawno\u015bci\n\n**Dane wej\u015bciowe:** \nPlik train.csv z 42000 zdj\u0119ciami, ka\u017cde: label 0-9 + 784 warto\u015bciami pixeli (shape=[42000x785])\n\n**Cel:** Przygotowanie danych w czterech tablicach:\n* train_x - [0.95*n,28,28,1]\n* train_y - [0.95*n,10]\n* dev_x  - [0.05*n,28,28,1]\n* dev_y  - [0.05*n,10]\n, gdzie n - liczba rekord\u00f3w w pliku train, 28 - rozmiar obrazk\u00f3w, 1 - ilo\u015b\u0107 kolor\u00f3w tu skala szaro\u015bci, 10 - ilo\u015b\u0107 klas\n\nTest zostanie pobrany i zwalidowany w rozdziale ostatnim, \u017ceby na czas treningu nie obci\u0105\u017ca\u0107 pami\u0119ci.\n\nPobrano struktur\u0119 folder\u00f3w, w celu p\u00f3\u017aniejszego wyboru odpowiedniego pliku.","d0de913b":"## Normalizacja i zmiana struktury danych\nZmieniono pd.dataframe na np.array\nKa\u017cdy piksel oznaczony jest skal\u0105 szaro\u015bci od 0 do 255. Dane znormalizowano do warto\u015bci 0-1."}}