{"cell_type":{"b41a3d4c":"code","d0cc1152":"code","a3b6bf90":"code","b62f8660":"code","a0a9704b":"code","c398e0b5":"code","c2b5207f":"code","d5034b67":"code","7b02e965":"code","4e52ebbf":"code","072bca75":"code","4ab2eda2":"code","63239d12":"code","a1b03b03":"code","520eb68d":"code","99570b33":"markdown","21dbe4ef":"markdown","7249a462":"markdown","7d6990bd":"markdown","c7ccd090":"markdown","d4f0816f":"markdown","d6042f24":"markdown","9b4f54c4":"markdown","90573d82":"markdown","1165f5ed":"markdown","650a2fab":"markdown","837e054b":"markdown","57e4ba43":"markdown","75d86d02":"markdown","31183eb7":"markdown","ac26392d":"markdown","c481e52c":"markdown","b80bbed0":"markdown","a51360ce":"markdown","2c86fb84":"markdown","3050c89a":"markdown","fba4c70c":"markdown","47ca7274":"markdown","5e85863d":"markdown","cb7c7316":"markdown","29772b58":"markdown","9b067f62":"markdown","4c50c12e":"markdown"},"source":{"b41a3d4c":"%matplotlib notebook\n# enable animations\nfrom shutil import copyfile\ncopyfile(src=\"..\/input\/q-funcs\/qlearning_functions.py\",\n         dst=\"..\/working\/qlearning_functions.py\")\nfrom qlearning_functions import *","d0cc1152":"my_board = new_board()\n\nshow(my_board)","a3b6bf90":"my_board = new_board()\nprint(my_board)","b62f8660":"show(my_board,\n     helpers=True)","a0a9704b":"test_boards = [\"         \",\n               \"X O XO OX\",\n               \"OXOXXOXOX\" ]\n\nfor board in test_boards:\n    show(board)\n    print('evaluator says:', evaluate(board))","c398e0b5":"my_board = 'X  OOO XX'\nflip_board(my_board)","c2b5207f":"e_init = 0.7     # how much to explore at the start (1 => all exploration)\ne_terminal = 0   # how much to exploit at the end   (0 => all exploitation)\n\nsimulate_e_greedy(e_init, e_terminal)","d5034b67":"get_move(new_board(), epsilon=1)","7b02e965":"simulate_game( epsilon_x = 1 ,\n               epsilon_o = 1 ,\n               slow_down = 3 ) # increase this to make the game slower","4e52ebbf":"qlearn_flow()","072bca75":"steps, winner = simulate_game(verb=True)\n\nbackpropagate(steps, winner, verb=True, wait_seconds=3) # <- increase wait_seconds to slow down","4ab2eda2":"# run as-is,\u00a0or set your own hyperparameters:\nvisualize_learning(lrate=.1,          # how quickly new values replace old values\n                   discount=.9,       # how important are future rewards \n                   init_e=.8,         # maximum epsilon value\n                   batches=5,         # number of times to shrink epsilon\n                   sims_per_batch=50, # games per batch, per game type (3)\n                   update_freq=5,     # how often to redraw plots (lower=>slower)\n                   boards=['         ', # observe X's first move\n                           '    X    ']) # observe O's response ","63239d12":"full_training()","a1b03b03":"print(f'The agent has encountered {round(100*len(q_table)\/4520, 2)}% of all possible board states')","520eb68d":"versus_agent()","99570b33":"#### 1.3 Evaluate","21dbe4ef":"\n## 2. Making a move\n\nHow does a computer-driven agent \"decide\" where to move? In our case, the answer will lie in the **Q Table**: a dictionary for storing thousands of possible board states along with the rewards of the possible moves (actions) from each state. By taking the action with the highest Q value at every step, a fully-trained agent develops general strategies for winning instead of memorizing specific paths to victory.\n\n\n#### 2.1 Agent Perception\n\nWe want an agent who can play both as X (going first) and as O (going second), so we need to train for both. Instead of maintaining a separate Q table for each player, by flipping the tiles for half the process we let the agent see itself as X all the time. We'll use the `flip_board` function any time we reference the Q Table from O's perspective. Run the following for an example:","7249a462":"## 1. Set up the environment\n\nWe may think of tic-tac-toe in terms of rows and columns, but for our agent it's easier to refer to the board as a list of position indexes from 1-9. The `new_board` function thus generates a list object, which is how the agent will \"see\" a blank board, with spaces representing empty cells:","7d6990bd":"#### 3.2 Updating Q-Values","c7ccd090":"## Testing\n\nThis final function is for you to play against the agent. Now epsilon is set to zero, so all of the agent's moves are determined by the Q Table. After sufficient training, give it your best shot\u2014playing as `X` or `O`, can you outsmart the Q Table?","d4f0816f":"The `show` function prints the board in a more familiar format. It accepts two arguments: `board` and a setting called `helpers` to show optional position indexes for humans to select a move. You'll use the `show` function a lot, because it lets us see how exactly the agent is developing. Run `show` on the board you created above with `helpers=True` to include labels for empty positions:","d6042f24":"### Q-Learning Overview\n\nQ-Learning is a technique for teaching an agent how to respond to different states of an environment by simulating actions and evaluating their outcomes. Over the course of many games, **backpropagation** assigns penalties and rewards to particular moves based on how the game ended. Our goal is for the agent to discover the best possible action from any given state.","9b4f54c4":"#### 1.2 Display","90573d82":"Run `full_training` once more and you should see this number hit 100. This means no matter how many games we play, the agent will recognize every board state from at least one previous encounter.","1165f5ed":"## 3. Simulations and Backpropagation\n\n#### 3.1 Match simulator\n\nTo populate the Q Table with more accurate estimates of the quality of different moves, we first need to simulate an entire game. This function will drive our training sessions. The `simulate_game` function starts by calling `new_board` to make a fresh environment, then switches back and forth between players (always starting with `X`) calling `get_move` at every step to update the board until reaching a win or a tie.\n\nWe can set a unique **\u03b5** value for each player's epsilon-greedy `get_move` decision using the `epsilon_x` and `epsilon_o` arguments. As the agent collects information over time, the trainer will shrink these values to encourage more reliance on the Q Table for move decisions.","650a2fab":"The size of the reward or penalty assigned to each move in backpropagation depends on a few factors. For example, the later a move is made in a game, the larger the affect it has on the Q Table. Moves leading to a tie are also penalized, but less than those leading to a loss. You can find the entire Q value\u2013update formula in the [technical version](#tech_) of this notebook, but for now just know that **previous learning affects how the agent acquires new knowledge**.","837e054b":"As you can see, shrinking the epsilon value over time allows more exploration early on while increasing the agent's confidence to make more informed decisions near the end. The agent uses a function called `get_move` to apply this formula:","57e4ba43":"If needed, you can bypass the 15-minute time limit on Kaggle's public notebook platform by copying the URL of this page and visiting it again from a new [incognito window](https:\/\/www.computerworld.com\/article\/3356840\/how-to-go-incognito-in-chrome-firefox-safari-and-edge.html). If you encounter an error, refresh the page. Just remember to run the inital code block where we imported `qlearning_functions`.","75d86d02":"Nothing too special yet: playing as X, the agent learns to capitalize on its status as the first player fairly quickly. As O, the agent still struggles to avoid defeat, even playing against a random opponent. In the third game type, the agent assumes both roles, so the proportion of ties should rise. This initial simulation lets us evaluate how the agent responds to different **hyperparameters** controlling the learning process\u2014listed above as arguments passed to the `visualize_learning` function. Feel free to play with these values and re-run the simulation.\n\nThe Q Tables to the right depict moves from two select states. Observe the evolving Q values for actions from `'         '` \u2014 a blank board. An empty board is the only state guaranteed to appear in every game, so it recieves an update every time we `backpropagate`. A heatmap for `'    X    '` is also included to visualize O's first move if X starts on position 5.\n\nDepending on which opening move X learns to favor, you can alter the `boards` argument in `visualize_learning` to watch O develop its response. For instance, if the X agent gives position 1 the highest Q value, change the second board to `'X         '`. Keep in mind that the further into a game you go (with more tiles on the board), the more obscure the state will be and the less often its Q values will recieve an update.","31183eb7":"# Build your own machine-learning agent\n\n\n##  What is Machine-Learning?\n\n\nWith an increased focus on analytics technologies across countless industries in recent years, and with phrases like \"data is the new oil\" abound, the topics of big data and machine-learning are increasingly difficult to avoid in conversations with anybody from a close friend to a passing stranger. As the widespread applications of analytics technologies by companies and governments increasingly affect our daily lives, it is up to the global population of digital users to understand, appreciate, and ultimately regulate their power.\n\nWhile any specialized aspect of data science would require decades of studying to master, there are some fundumantal precepts in data science that non-coders should try to grasp. This tutorial is designed to demonstrate how a simple machine-learning algorithm works by allowing you to build one yourself\u2014without any experience in coding or mathematics required. If you are a practicing data scientist and want to build your own agent, now is a good time to switch to the technical version of this notebook where you can write all the functions for yourself.\n\n\n## Q-Learning Tic-Tic-Toe \n\n\nToday you'll learn the concepts underlying a Q-Learning algorithm by creating and training an advanced tic-tac-toe agent. In case this is your first time in Jupyter, to run a chunk of code, select it and type `shift-enter`. You know the code has run succesfully when the `[ ]` at the top-left of the block fills with a number. Import the prewritten functions for this project by typing `shift-enter` with the following block selected:","ac26392d":"Next we need a function to evaluate the board. It should always return one of a few possible outcomes: a `Win`, a `Tie`, or `Continue` (empty spaces remaining). You can customize any of the following test boards, just make sure it's exactly 9 characters long.","c481e52c":"## 4. Training\n\nTime to bring it all together. With the code above, you backpropagated through all the steps in a game. Below, we simulate hundreds of games, focusing throughout on a few select board states. The `visualize_learning` function calls `simulate_game` repeatedly and runs `backpropagate` over all the `steps`, continuously updating the Q Table for every game. Here you will see how Q values develop over time within the action space of a given state.\n\n#### 4.1 Getting our feet wet\n\nWe run three types of games: one for each player to compete against a random opponent, and a third where both players try to win. In addition to the winning rates of each player over time, the dashed lines plot the shrinking epsilon level for the player(s) being trained. To stop the training and the animation, click the power button at the top of the plot.","b80bbed0":"#### 3.3. Backpropagation\n\nNow that we can simulate a game, we will feed the `steps` of a game into `backpropagate` with `verb=True` to watch the algorithm go back through the moves of each player, updating the Q Table in every state.","a51360ce":"In the examples above, you also saw how `show` assigns a color to each player \u2014 this is for our sake, not the computer's. If empty spaces remain and neither player has won, `evaluate` returns `Continue` to trigger the next move.  With the environment established, now it's time to build an agent.","2c86fb84":"Math tells us that are [4,520](https:\/\/math.stackexchange.com\/questions\/3276315\/number-of-uncompleted-tic-tac-toe-games) total possible incomplete states in tic-tac-toe, i.e. states we our agent to learn about. With over 45,000 games under its belt, the length of the Q Table tells us how many of these states the has agent seen:","3050c89a":"#### 4.2 More Practice\n\nFor the full round of simulations, run the following code. We save the plotting for the end to speed up the process:","fba4c70c":"For every action taken in a game, backpropagation adjusts the corresponding Q values depending on whether the player ultimately won, lost, or tied. Launch the following diagram for an overview of the Q-Learning process:","47ca7274":"The `simulate_game` function also builds a history object called `steps` (seen above) with the entire sequence of moves in a game. Each move is associated with the decision space from which it was chosen\u2014in other words, the state of the board just before the action. Using this brief \"memory\", the algorithm makes small adjustments after every game to reflect rewards and penalties for winning and losing strategies.","5e85863d":"#### 2.2 Exploration-Exploitation Trade-Off\n\nOne challenge in reinforcement learning comes in finding a balance between **exploration** and **exploitation**. Avery Parkinson illustrates with this analogy:\n\n> Let\u2019s say that you and your friends are trying to decide where to eat. In the past, you\u2019ve always gone to a Mexican restaurant around the corner, and you\u2019ve all really enjoyed it. However, this time, one of your friends mentions that a new Lebanese place has opened up down the street, and it\u2019s supposed to be really good. None of you guys can come to a consensus \u2014 should you go to the Mexican restaurant which you know to be really good, or should you try the Lebanese place which has the potential to be better or worse? ([source](https:\/\/medium.com\/analytics-vidhya\/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870))\n\nWe pursue this balance over many simulated decisions in Q-Learning using an **epsilon-greedy algorithm**. With epsilon (**\u03b5**) set to 1, the agent always moves completely randomly (exploring), while decreasing **\u03b5** to zero triggers more moves based on existing knowledge (exploiting).\n\n#### 2.3 The \u03b5-Greedy Move Function\n\n\nBy setting epsilon to shrink over time, we let the agent rely more heavily on the Q Table towards the end of a simulation. This effect is demonstrated below in 100 iterations. How does the \u03b5-greedy algorithm help the agent change strategies over time?","cb7c7316":"Every time the agent sees a new board state, it will add it to the Q Table and initialize the rewards for all the moves at a default initial value. We use a relatively high initial Q of `0.3` for a strategy called **optimistic learning**, encouraging the agent to try out unknown moves. ","29772b58":"At this point our Q values are all set to the initial guess of `0.3`, so `get_move` has no real prior information to work with. Setting epsilon to 1 ensures that all moves will be made at random for now.","9b067f62":"Each step in backpropagation incorporates a key variable called `max_future_q`, representing the reward value of the best possible move from future states given the current action. Accounting for future rewards to rate the quality of a given action drives the agent to optimize long-term rewards.","4c50c12e":"Now you can call any of the commands from `qlearning_functions` inside this notebook. For example, you can create a fresh board by running the `new_board` function. To display the board, pass the result of `new_board` as an argument to the `show` function:"}}