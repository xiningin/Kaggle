{"cell_type":{"b8619f75":"code","07b48f67":"code","e5ff3a62":"code","a042c231":"code","777b2bfb":"code","a385e2a3":"code","b3e20520":"code","efd4af07":"code","44fd4ba7":"code","90fa69c2":"code","37063c51":"code","66873179":"code","ed8c3e8a":"code","5dcbad84":"code","bba381b8":"code","eaf74b3d":"code","300d0c1b":"code","8ca4ba0e":"code","6ee9f5d2":"code","576bb076":"code","1fa114a5":"code","0add433d":"code","1f7cfa1b":"code","7f43e2f1":"markdown","11f7b5c5":"markdown","20e8df02":"markdown","6b1efc06":"markdown"},"source":{"b8619f75":"import os\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport albumentations as A\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport nnabla as nn\nimport nnabla.functions as F\nimport nnabla.parametric_functions as PF\nimport nnabla.solvers as S\nfrom nnabla.ext_utils import get_extension_context\nfrom nnabla.utils.data_source import DataSource\nfrom nnabla.utils.data_iterator import data_iterator\n\n\n%matplotlib inline","07b48f67":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['NNABLA_CUDNN_DETERMINISTIC'] = '1'\n    random.seed(seed)\n    np.random.seed(seed)\n    \n\ndef enable_cudnn_extension(device_id=0, type_config='float'):\n    assert type_config in ['float', 'half', 'mixed_half']\n    cxt = get_extension_context('cudnn', device_id=device_id, type_config=type_config)\n    nn.set_default_context(cxt)\n    return cxt","e5ff3a62":"DIRPATH = '..\/input\/ailab-ml-training-0\/'\nTRAIN_IMAGE_DIR = 'train_images\/train_images\/'\nTEST_IMAGE_DIR = 'test_images\/test_images\/'\n\nID = 'fname'\nTARGET = 'label'\n\nVALID_SIZE = 0.2\nEPOCHS = 5\nBATCH_SIZE = 64\nLR = 1e-3\nWEIGHT_DECAY = 1e-5\n\nSEED = 42\nseed_everything(SEED)\n\nDEVICE_ID = 0\nTYPE_CONFIG = 'float'\n\nenable_cudnn_extension(device_id=DEVICE_ID, type_config=TYPE_CONFIG)","a042c231":"os.listdir(DIRPATH)","777b2bfb":"train_df = pd.read_csv(os.path.join(DIRPATH, 'train.csv'))","a385e2a3":"train_df.head()","b3e20520":"sample_index = [0, 10, 100]\n\nfig, ax = plt.subplots(1, len(sample_index))\nfig.set_size_inches(4 * len(sample_index), 4)\n\nfor i, idx in enumerate(sample_index):\n    fname, label = train_df.loc[idx, [ID, TARGET]]\n    img = cv2.imread(os.path.join(DIRPATH, TRAIN_IMAGE_DIR, fname))\n    ax[i].imshow(img)\n    ax[i].set_title(f'{fname} - label: {label}')\n\nplt.show()","efd4af07":"class MNISTDataSource(DataSource):\n    def __init__(\n        self,\n        fname_list,\n        label_list,\n        image_dir,\n        transform=None,\n        shuffle=False,\n        rng=None,\n    ):\n        super().__init__(shuffle=shuffle, rng=rng)\n        \n        self.fname_list = fname_list\n        self.label_list = label_list\n        self.image_dir = image_dir\n        self.transform = transform\n        \n        self._size = len(fname_list)\n        self._variables = ('x', 'y')\n        self.rng = rng if rng is not None else np.random.RandomState(313)\n        self.reset()\n    \n    def reset(self):\n        if self._shuffle:\n            self._indexes = self.rng.permutation(self._size)\n        else:\n            self._indexes = np.arange(self._size)\n        super(MNISTDataSource, self).reset()\n    \n    def _get_data(self, position):\n        idx = self._indexes[position]\n        \n        fname = self.fname_list[idx]\n        label = self.label_list[idx]\n        \n        image = cv2.imread(os.path.join(self.image_dir, fname))\n        if self.transform is not None:\n            image = self.transform(image=image)['image']\n        \n        return image, label","44fd4ba7":"def simple_classifier(x, test=False, scope='simple_classifier'):\n    with nn.parameter_scope(scope):\n        # (N, 3, 28, 28) --> (N, 32, 14, 14)\n        x = PF.convolution(x, 32, (3, 3), stride=(1, 1), pad=(1, 1), name='conv1', with_bias=True)\n        x = F.relu(x)\n        x = F.max_pooling(x, (2, 2))\n        # (N, 32, 14, 14) --> (N, 64, 7, 7)\n        x = PF.convolution(x, 64, (3, 3), stride=(1, 1), pad=(1, 1), name='conv2', with_bias=True)\n        x = F.relu(x)\n        x = F.max_pooling(x, (2, 2))\n        # (N, 64, 7, 7) --> (N, 128, 7, 7)\n        x = PF.convolution(x, 128, (3, 3), stride=(1, 1), pad=(1, 1), name='conv3', with_bias=True)\n        x = F.relu(x)\n        # (N, 128 * 7 * 7) --> (N, 10)\n        x = F.reshape(x, (x.shape[0], -1))\n        x = PF.affine(x, 10, name='affine1', with_bias=True)\n    return x","90fa69c2":"fname_list = train_df[ID].to_list()\nlabel_list = train_df[TARGET].to_list()\n\ntrain_fname_list, valid_fname_list, train_label_list, valid_label_list = train_test_split(\n    fname_list, label_list, test_size=VALID_SIZE, random_state=SEED, shuffle=True\n)","37063c51":"len(fname_list), len(train_fname_list), len(valid_fname_list)","66873179":"image_dir = os.path.join(DIRPATH, TRAIN_IMAGE_DIR)\n\ntransform = A.Compose([\n    A.Rotate(limit=10, interpolation=1, p=1.0),\n])\n\ntrain_data_source = MNISTDataSource(\n    train_fname_list, train_label_list, image_dir,\n    transform=transform, shuffle=True, rng=None,\n)\nvalid_data_source = MNISTDataSource(\n    valid_fname_list, valid_label_list, image_dir, \n    transform=transform, shuffle=False, rng=None\n)\n\ntrain_data_iterator = data_iterator(\n    train_data_source, BATCH_SIZE, rng=None, with_memory_cache=False, with_file_cache=False,\n)\nvalid_data_iterator = data_iterator(\n    valid_data_source, BATCH_SIZE, rng=None, with_memory_cache=False, with_file_cache=False,\n)","ed8c3e8a":"x, y = train_data_iterator.next()\nimage = nn.Variable(x.shape)\nlabel = nn.Variable((y.shape[0], 1))\nlabel_hat = simple_classifier(image, test=False, scope='simple_classifier')\nlabel_hat.persistent = True\nloss = F.mean(F.softmax_cross_entropy(label_hat, label, axis=1))\n\nx, y = valid_data_iterator.next()\nval_image = nn.Variable(x.shape)\nval_label = nn.Variable((y.shape[0], 1))\nval_label_hat = simple_classifier(val_image, test=True, scope='simple_classifier')\nval_label_hat.persistent = True\nval_loss = F.mean(F.softmax_cross_entropy(val_label_hat, val_label, axis=1))\n\nsolver = S.Adam(alpha=LR)\nwith nn.parameter_scope('simple_classifier'):\n    solver.set_parameters(nn.get_parameters())","5dcbad84":"for epoch in range(EPOCHS):\n    \n    # training\n    \n    train_loss_list = []\n    train_accuracy_list = []\n    \n    while epoch == train_data_iterator.epoch:\n        x, y = train_data_iterator.next()\n        image.d = x\n        label.d = y.reshape(y.shape[0], 1)\n        solver.zero_grad()\n        loss.forward()\n        loss.backward(clear_buffer=True)\n        solver.weight_decay(WEIGHT_DECAY)\n        solver.update()\n        \n        train_loss_list.append(loss.d)\n        accuracy = accuracy_score(np.argmax(label_hat.d, axis=1), y)\n        train_accuracy_list.append(accuracy)\n    \n    # validation\n    \n    valid_loss_list = []\n    valid_accuracy_list = []\n    \n    while epoch == valid_data_iterator.epoch:\n        x, y = valid_data_iterator.next()\n        val_image.d = x\n        val_label.d = y.reshape(y.shape[0], 1)\n        val_loss.forward(clear_buffer=True)\n        \n        valid_loss_list.append(val_loss.d)\n        accuracy = accuracy_score(np.argmax(val_label_hat.d, axis=1), y)\n        valid_accuracy_list.append(accuracy)\n    \n    # verbose\n    \n    print('epoch: {}\/{} - loss: {:.5f} - accuracy: {:.3f} - val_loss: {:.5f} - val_accuracy: {:.3f}'.format(\n        epoch,\n        EPOCHS, \n        np.mean(train_loss_list),\n        np.mean(train_accuracy_list),\n        np.mean(valid_loss_list),\n        np.mean(valid_accuracy_list)\n    ))","bba381b8":"submission_df = pd.read_csv(os.path.join(DIRPATH, 'sample_submission.csv'))","eaf74b3d":"submission_df.head()","300d0c1b":"fname_list = submission_df[ID].to_list()\nlabel_list = submission_df[TARGET].to_list()\n\nimage_dir = os.path.join(DIRPATH, TEST_IMAGE_DIR)\n\ntransform = None\n\ntest_data_source = MNISTDataSource(\n    fname_list, label_list, image_dir, \n    transform=transform, shuffle=False, rng=None\n)\n\ntest_data_iterator = data_iterator(\n    test_data_source, 1, rng=None, with_memory_cache=False, with_file_cache=False,\n)","8ca4ba0e":"predictions = []\nwhile 1 > test_data_iterator.epoch:\n    x, y = test_data_iterator.next()\n    val_image.d = x\n    val_label.d = y.reshape(y.shape[0], 1)\n    val_label_hat.forward(clear_buffer=True)\n    pred = np.argmax(val_label_hat.d, axis=1)\n    predictions.append(pred[0])","6ee9f5d2":"submission_df[TARGET] = predictions","576bb076":"sample_index = [0, 10, 100]\n\nfig, ax = plt.subplots(1, len(sample_index))\nfig.set_size_inches(4 * len(sample_index), 4)\n\nfor i, idx in enumerate(sample_index):\n    fname, label = submission_df.loc[idx, [ID, TARGET]]\n    img = cv2.imread(os.path.join(DIRPATH, TEST_IMAGE_DIR, fname))\n    ax[i].imshow(img)\n    ax[i].set_title(f'{fname} - label: {label}')\n\nplt.show()","1fa114a5":"submission_df.head()","0add433d":"submission_df.to_csv('submission.csv', index=False)","1f7cfa1b":"from IPython.display import FileLink\nFileLink('submission.csv')","7f43e2f1":"# Loading","11f7b5c5":"# Training","20e8df02":"# Define Dataset & Model","6b1efc06":"# Prediction & Submission"}}