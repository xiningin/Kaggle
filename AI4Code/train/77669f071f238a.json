{"cell_type":{"b0a70782":"code","67133ce3":"code","2f9bdf7c":"code","1830a15c":"code","3fbecb43":"code","7d128c1d":"code","464c28b9":"code","f7a4c1ce":"code","49fc8b00":"code","2bc96f23":"code","c81bd269":"code","ba839d73":"code","0392f61c":"code","88a1faa5":"code","0dd5f2d8":"code","93d40fac":"code","f9ff9815":"code","d8c8a828":"code","9c660f5c":"code","0fe575d0":"code","4e4bf685":"code","b723fd53":"code","9a49ac66":"code","fd687772":"code","53955cf1":"code","193fbcef":"code","ada968d4":"code","46039e0a":"code","26bf593f":"code","e8976126":"code","cd0c5bb1":"code","914b990b":"code","d93aab6f":"code","000ba677":"code","47a2b555":"code","653c61c2":"code","903df0e5":"code","4f582c0a":"code","49831cf3":"code","d665fc4d":"code","d6f825f4":"code","71dd3a96":"code","338046c3":"code","b2b998bd":"code","8d8f5547":"code","da240c60":"code","2b561430":"code","1d68c9db":"code","766a3556":"code","7723d1c4":"code","194b504d":"code","66207a73":"code","04cd77a7":"code","581bdffc":"code","3134f2c4":"code","5b655faa":"code","fbd3266f":"code","e962df4d":"code","10a2451b":"code","b61ef3b5":"code","7b4f081d":"code","df80b91f":"code","63579362":"code","0fcab6f6":"code","588d070c":"code","de1696d2":"code","912c1190":"code","5bda6095":"code","4c3a9f69":"code","77caca3a":"code","442b0c70":"code","7fb74353":"code","532bfd9c":"code","e66f89cc":"code","31bce722":"code","b8c0ba7b":"code","a4b195f8":"code","1959a733":"code","04e7edd9":"code","55628035":"code","0ed4744a":"code","3b1be8da":"code","d5314310":"code","3bfc5bd2":"code","3157899d":"code","92040a6a":"code","57130904":"code","a309041d":"code","ba8053cc":"code","59b182d4":"code","920daa9a":"code","0073d269":"code","51d57417":"code","2d246f71":"code","c606d002":"code","c127b042":"code","b1893f18":"code","6f1f7fc0":"code","22fd2f84":"code","ddbc0afe":"code","e763bb70":"code","3b09e16f":"code","b8dc7376":"code","498c9daa":"code","b44f109e":"code","72798ea0":"code","e722e4ec":"code","3ee6805b":"code","42d6db63":"code","830a1333":"code","daeb3429":"code","dee9de2a":"code","44eeae08":"code","58898fa9":"code","e839c436":"code","59cc833b":"code","26fbe3fb":"code","0592966e":"code","d9e30941":"code","c679e1ec":"code","40ddffc6":"code","e346db5a":"code","5477e8c8":"code","149a6fb2":"code","f68d01e9":"markdown","a986d4cf":"markdown","8f268d72":"markdown","eaab28b9":"markdown","d0ba06b7":"markdown","89dd156e":"markdown","65f5508b":"markdown","2dda1c01":"markdown","7bd1b530":"markdown","c94155ac":"markdown","6ad7376b":"markdown","b3db264b":"markdown","83ab5688":"markdown","eb22db6d":"markdown","88839eaa":"markdown","45300973":"markdown","8319ab2e":"markdown","6a300c09":"markdown","6679803a":"markdown","22f345f2":"markdown","e276ce39":"markdown","8632ccb1":"markdown","dcc2d7d7":"markdown","b6686c67":"markdown","96b8d4e0":"markdown","db9f7cb0":"markdown","5fcd82aa":"markdown","b6592179":"markdown","cef1de12":"markdown","8817e32b":"markdown","fffffd8b":"markdown","a3c7dc05":"markdown","b5baed28":"markdown","3ffa71f4":"markdown","4c37438b":"markdown","87bbfc1c":"markdown","a3e830a4":"markdown","480e14fd":"markdown","9e3f9775":"markdown","dad005cd":"markdown","b754a80a":"markdown","67f3cd11":"markdown","b92cfa8a":"markdown","84b6008b":"markdown","7f9d1c0e":"markdown","c89cf902":"markdown","a6c02d04":"markdown","4af31296":"markdown","152afaaa":"markdown","86f11d23":"markdown","240d7873":"markdown","1fe2ba14":"markdown","bae62ad1":"markdown","77e542e4":"markdown","a2465ae5":"markdown","372a8a91":"markdown","de468bd7":"markdown","3ca06ac6":"markdown","7b1ea7b9":"markdown","ed3451d2":"markdown","58c4b08c":"markdown","f10d83a5":"markdown","24ab47b9":"markdown","5f901126":"markdown","753fe663":"markdown","0e37e6b0":"markdown","b103a136":"markdown","78337fa5":"markdown","bd830c6a":"markdown","71501642":"markdown","03054150":"markdown","ad02e2af":"markdown","8e18a34b":"markdown","2ff2c1d4":"markdown","8cd0172c":"markdown","dba98352":"markdown","ad71497e":"markdown","73d26c69":"markdown","62d64841":"markdown","1d647d8c":"markdown","f066a947":"markdown","d43e09ae":"markdown","08892cdc":"markdown","f0fc181b":"markdown","beb5a30a":"markdown","4dbb1e03":"markdown","63339fc8":"markdown","830b1e23":"markdown","b3e72200":"markdown","91ed9d5b":"markdown","03d8d017":"markdown","f77fd541":"markdown","848ead44":"markdown","d686bb30":"markdown","fc3b15e9":"markdown","7d6cd294":"markdown","9a09b748":"markdown","9f72e97b":"markdown","407ac179":"markdown","b354a9a6":"markdown","108cec94":"markdown","1c28908f":"markdown","b363b68c":"markdown","3bca9def":"markdown","bfee4dfe":"markdown","4f0abb3a":"markdown","8cf97b8c":"markdown"},"source":{"b0a70782":"# Ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Data processing and analysis\nimport numpy as np\nimport pandas as pd\nimport math \nimport re\n\n\n# Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n\n# Configure visualisations\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\n\n\n# Classification algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\n\n\n# Data preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer, scale, LabelEncoder, OneHotEncoder\n\n\n# Modeling helper functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n\n# Classification metrices\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report, precision_score,recall_score,f1_score \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","67133ce3":"# Load train and Test set\n\n%time\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nIDtest = test['PassengerId']","2f9bdf7c":"print('The shape of the training set : {} '.format(train.shape))","1830a15c":"import pandas_profiling as pp","3fbecb43":"pp.ProfileReport(train)","7d128c1d":"train.head()","464c28b9":"test.head()","f7a4c1ce":"train.info()","49fc8b00":"var1 = [col for col in train.columns if train[col].isnull().sum() != 0]\n\nprint(train[var1].isnull().sum())","2bc96f23":"train.describe()","c81bd269":"# find categorical variables\n\ncategorical = [var for var in train.columns if train[var].dtype =='O']\n\nprint('There are {} categorical variables in training set.\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","ba839d73":"# find numerical variables\n\nnumerical = [var for var in train.columns if train[var].dtype !='O']\n\nprint('There are {} numerical variables in training set.\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","0392f61c":"print('The shape of the test set : {} '.format(test.shape))","88a1faa5":"pp.ProfileReport(test)","0dd5f2d8":"test.head()","93d40fac":"test.info()","f9ff9815":"var2 = [col for col in test.columns if test[col].isnull().sum() != 0]\n\nprint(test[var2].isnull().sum())","d8c8a828":"test.describe()","9c660f5c":"# find categorical variables\n\ncategorical = [var for var in test.columns if test[var].dtype =='O']\n\nprint('There are {} categorical variables in test set.\\n'.format(len(categorical)))\n\nprint('The categorical variables are :', categorical)","0fe575d0":"# find numerical variables\n\nnumerical = [var for var in test.columns if test[var].dtype !='O']\n\nprint('There are {} numerical variables in test set.\\n'.format(len(numerical)))\n\nprint('The numerical variables are :', numerical)","4e4bf685":"# view missing values in training set\nmsno.matrix(train, figsize = (30,10))","b723fd53":"train['Survived'].value_counts()","9a49ac66":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Survived'], data = train, palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","fd687772":"train.groupby('Survived')['Sex'].value_counts()","53955cf1":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Survived'], data = train, hue='Sex', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people who survived', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","193fbcef":"females = train[train['Sex'] == 'female']\nfemales.head()","ada968d4":"females['Survived'].value_counts()\/len(females)","46039e0a":"males = train[train['Sex'] == 'male']\nmales.head()","26bf593f":"males['Survived'].value_counts()\/len(males)","e8976126":"# create the first of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 1)   # (rows, columns, panel number)\nlabels1 = females['Survived'].value_counts().index\nsize1 = females['Survived'].value_counts()\ncolors1=['cyan','pink']\nplt.pie(size1, labels = labels1, colors = colors1, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of females who survived', fontsize = 20)\nplt.legend(['1:Survived', '0:Not Survived'], loc=0)\nplt.show()\n\n# create the second of two pie-charts and set current axis\nplt.figure(figsize=(8,6))\nplt.subplot(1, 2, 2)   # (rows, columns, panel number)\nlabels2 = males['Survived'].value_counts().index\nsize2 = males['Survived'].value_counts()\ncolors2=['pink','cyan']\nplt.pie(size2, labels = labels2, colors = colors2, shadow = True, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage of males who survived', fontsize = 20)\nplt.legend(['0:Not Survived','1:Survived'])\nplt.show()\n","cd0c5bb1":"train['Sex'].value_counts()","914b990b":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Sex'], data=train, palette = 'bone')\ngraph.set_title('Distribution of sex among passengers', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","d93aab6f":"train['Sex'].value_counts()\/len(train)","000ba677":"plt.figure(figsize=(8,6))\nlabels = train['Sex'].value_counts().index\nsize = train['Sex'].value_counts()\ncolors=['cyan','pink']\nplt.pie(size, labels = labels, shadow = True, colors=colors, autopct='%1.1f%%',startangle = 90)\nplt.title('Percentage distribution of sex among passengers', fontsize = 20)\nplt.legend()\nplt.show()","47a2b555":"train.groupby('Pclass')['Sex'].value_counts()","653c61c2":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Pclass'], data=train, palette = 'bone')\ngraph.set_title('Number of people in different classes', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","903df0e5":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Pclass'], data=train, hue='Survived', palette = 'bone')\ngraph.set_title('Distribution of people segregated by survival', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","4f582c0a":"# percentage of survivors per class\nsns.factorplot('Pclass', 'Survived', data = train)","49831cf3":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Embarked'], data=train, palette = 'bone')\ngraph.set_title('Number of people across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","d665fc4d":"fig, ax = plt.subplots(figsize=(8,6))\ngraph = sns.countplot(ax=ax,x=train['Embarked'], data=train, hue='Survived', palette = 'bone')\ngraph.set_title('Number of people across different embarkment', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","d6f825f4":"x = train['Age']\nplt.figure(figsize=(8,6))\nplt.hist(x, bins=25, color='g')\nplt.xlabel('Age')\nplt.ylabel('Number of passengers')\nplt.title('Age distribution of passengers', fontsize = 20)\nplt.show()","71dd3a96":"train.drop(['Ticket', 'PassengerId'], axis = 1, inplace = True)\ntest.drop(['Ticket','PassengerId'], axis = 1, inplace = True)","338046c3":"# function to extract title from Name feature\ndef passenger_title(passenger):\n    line = passenger\n    if re.search('Mrs', line):\n        return 'Mrs'\n    elif re.search('Mr', line):\n        return 'Mr'\n    elif re.search('Miss', line):\n        return 'Miss'\n    elif re.search('Master', line):\n        return 'Master'\n    else:\n        return 'Other'","b2b998bd":"# extract title  \ntrain['Title'] = train['Name'].apply(passenger_title)\ntest['Title'] = test['Name'].apply(passenger_title)","8d8f5547":"# fill missing age, with median from title segregation: funtion\ndef fill_age(passenger):\n    \n    # determine age by group \n    temp = train.groupby(train.Title).median()\n    \n    age, title = passenger\n    \n    if age == age:\n        return age\n    else:\n        if title == 'Mr':\n            return temp.Age['Mr']\n        elif title == 'Miss':\n            return temp.Age['Miss']\n        elif title == ['Mrs']:\n            return temp.Age['Mrs']\n        elif title == 'Master':\n            return temp.Age['Master']\n        else:\n            return temp.Age['Other']","da240c60":"# fill age according to title\ntrain['Age'] = train[['Age', 'Title']].apply(fill_age, axis = 1)\ntest['Age'] = test[['Age', 'Title']].apply(fill_age, axis = 1)","2b561430":"# Remove column Name, it is not useful for predictions and we extracted the title already\ntrain.drop('Name', axis = 1, inplace = True)\ntest.drop('Name', axis = 1, inplace = True)","1d68c9db":"# Remove column Title, it is not useful for predictions and we imputed the age already\ntrain.drop('Title', axis = 1, inplace = True)\ntest.drop('Title', axis = 1, inplace = True)","766a3556":"def isNaN(num):\n    return num != num # checks if cell is NaN","7723d1c4":"# get the first letter of cabin \ndef first_letter_of_cabin(cabin):\n    if not isNaN(cabin):\n        return cabin[0]\n    else:\n        return 'Unknown'","194b504d":"train['Deck'] = train['Cabin'].apply(first_letter_of_cabin)\ntest['Deck'] = test['Cabin'].apply(first_letter_of_cabin)","66207a73":"# drop old variable Cabin\ntrain.drop('Cabin', axis = 1, inplace = True)\ntest.drop('Cabin', axis = 1, inplace = True)","04cd77a7":"train[\"Embarked\"].fillna(\"S\", inplace = True)\ntest['Embarked'].fillna(\"S\", inplace = True)","581bdffc":"train.isnull().sum()","3134f2c4":"test.isnull().sum()","5b655faa":"#we can replace missing value in fare by taking median of all fares of those passengers \n#who share 3rd Passenger class and Embarked from 'S' \ntest['Fare'].fillna(test['Fare'].median(), inplace = True)\n","fbd3266f":"test.isnull().sum()","e962df4d":"# draw boxplots to visualize outliers\n\nplt.figure(figsize=(15,10))\n\n\nplt.subplot(1, 2, 1)\nfig = train.boxplot(column='Age')\nfig.set_title('')\nfig.set_ylabel('Age')\n\n\nplt.subplot(1, 2, 2)\nfig = train.boxplot(column='Fare')\nfig.set_title('')\nfig.set_ylabel('Fare')\n","10a2451b":"# find outliers in Age variable\n\nIQR = train.Age.quantile(0.75) - train.Age.quantile(0.25)\nLower_fence = train.Age.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Age.quantile(0.75) + (IQR * 3)\nprint('Age outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=max(0, Lower_fence), upperboundary=Upper_fence))","b61ef3b5":"# find outliers in Fare variable\n\nIQR = train.Fare.quantile(0.75) - train.Fare.quantile(0.25)\nLower_fence = train.Fare.quantile(0.25) - (IQR * 3)\nUpper_fence = train.Fare.quantile(0.75) + (IQR * 3)\nprint('Fare outliers are values < {lowerboundary} or > {upperboundary}'.format(lowerboundary=max(0, Lower_fence), upperboundary=Upper_fence))","7b4f081d":"def max_value(df, variable, top):\n    return np.where(df[variable]>top, top, df[variable])\n\nfor df in [train, test]:\n    df['Age'] = max_value(df, 'Age', 81.0)\n    df['Fare'] = max_value(df, 'Fare', 100.2688)\n    ","df80b91f":"train.Age.max(), test.Age.max()","63579362":"train.Fare.max(), test.Fare.max()","0fcab6f6":"# label minors as child, and remaining people as female or male\ndef male_female_child(passenger):\n    # take the age and sex\n    age, sex = passenger\n    \n    # compare age, return child if under 16, otherwise leave sex\n    if age < 16:\n        return 'child'\n    else:\n        return sex","588d070c":"# new columns called person specifying if the person was female, male or child\ntrain['Person'] = train[['Age', 'Sex']].apply(male_female_child, axis = 1)\ntest['Person'] = test[['Age', 'Sex']].apply(male_female_child, axis = 1)\n","de1696d2":"# Number of male, female and children on board\ntrain['Person'].value_counts()","912c1190":"# age segregated by class\nfig = sns.FacetGrid(train, hue = 'Person', aspect = 4)\nfig.map(sns.kdeplot, 'Age', shade = True)\nfig.add_legend()","5bda6095":"# age segregated by class\nfig = sns.FacetGrid(train, hue = 'Pclass', aspect = 4)\nfig.map(sns.kdeplot, 'Age', shade = True)\nfig.add_legend()","4c3a9f69":"sns.factorplot('Pclass', 'Survived', hue = 'Person', data = train)","77caca3a":"def travel_alone(df):\n    df['Alone'] = df.Parch + df.SibSp\n    df['Alone'].loc[df['Alone'] > 0] = 'With Family'\n    df['Alone'].loc[df['Alone'] == 0] = 'Alone'\n    \n    return df","442b0c70":"train = travel_alone(train)\ntest = travel_alone(test)","7fb74353":"# check how many passengers are travelling with family and alone\ntrain['Alone'].value_counts()","532bfd9c":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Alone'], data = train, palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people travelling alone or with family', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","e66f89cc":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Alone'], data = train, hue = 'Survived', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people travelling alone or with family', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","31bce722":"# percentage of survivors depending on traveling alone or with family\nsns.factorplot('Alone', 'Survived', hue = 'Person', data = train)","b8c0ba7b":"train.head()","a4b195f8":"fig, ax = plt.subplots(figsize=(6,6))\ngraph = sns.countplot(ax=ax,x=train['Deck'], data = train[train.Deck != 'Unknown'], hue = 'Survived', palette = 'PuBuGn_d')\ngraph.set_title('Distribution of people on each deck', fontsize = 12)\ngraph.set_xticklabels(graph.get_xticklabels(),rotation=30)\nfor p in graph.patches:\n    height = p.get_height()\n    graph.text(p.get_x()+p.get_width()\/2., height + 0.1,height ,ha=\"center\")","1959a733":"train.corr()['Survived']","04e7edd9":"corr=train.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\nsns.heatmap(corr, vmax=.8, linewidths=0.01, square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features')\nplt.show()","55628035":"train.head()","0ed4744a":"train.drop('Sex', axis=1, inplace=True)\ntest.drop('Sex', axis=1, inplace=True)","3b1be8da":"train['Alone'] = pd.get_dummies(train['Alone'])\ntest['Alone'] = pd.get_dummies(test['Alone'])\n","d5314310":"labelenc=LabelEncoder()\n\ncategorical=['Embarked','Deck','Person']\nfor col in categorical:\n    train[col]=labelenc.fit_transform(train[col])\n    test[col]=labelenc.fit_transform(test[col])\n\ntrain.head()","3bfc5bd2":"test.head()","3157899d":"train_cols = train.columns\ntest_cols = test.columns","92040a6a":"scaler = StandardScaler()\ntrain[['Age', 'Fare']] = scaler.fit_transform(train[['Age', 'Fare']])\ntest[['Age', 'Fare']] = scaler.transform(test[['Age', 'Fare']])","57130904":"# Declare feature vector and target variable\nX = train.drop(labels = ['Survived'],axis = 1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","a309041d":"names = [\"Logistic Regression\", \"Nearest Neighbors\", \"Naive Bayes\", \"Linear SVM\", \"RBF SVM\", \n         \"Gaussian Process\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"Gradient Boosting\", \n         \"LDA\", \"QDA\", \"Neural Net\", \"LightGBM\", \"XGBoost\" ]    \n\n","ba8053cc":"classifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(5),\n    GaussianNB(),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(kernel = \"rbf\", gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    MLPClassifier(alpha=1, max_iter=1000),\n    lgb.LGBMClassifier(),    \n    xgb.XGBClassifier()\n   ]","59b182d4":"\naccuracy_scores = []\n\n# iterate over classifiers and predict accuracy\nfor name, clf in zip(names, classifiers):\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    score = round(score, 4)\n    accuracy_scores.append(score)\n    print(name ,' : ' , score)","920daa9a":"classifiers_performance = pd.DataFrame({\"Classifiers\": names, \"Accuracy Scores\": accuracy_scores})\nclassifiers_performance","0073d269":"classifiers_performance.sort_values(by = 'Accuracy Scores' , ascending = False)[['Classifiers', 'Accuracy Scores']]","51d57417":"fig, ax = plt.subplots(figsize=(8,6))\nx = classifiers_performance['Accuracy Scores']\ny = classifiers_performance['Classifiers']\nax.barh(y, x, align='center', color='green')\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Accuracy Scores')\nax.set_ylabel('Classifiers', rotation=0)\nax.set_title('Classifier Accuracy Scores')\nplt.show()","2d246f71":"# instantiate the classifier with n_estimators = 100\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n\n# fit the classifier to the training set\nclf.fit(X_train, y_train)","c606d002":"# view the feature scores\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nfeature_scores","c127b042":"feature_scores.values","b1893f18":"feature_scores.index","6f1f7fc0":"# Creating a seaborn bar plot to visualize feature scores\nf, ax = plt.subplots(figsize=(8,6))\nax = sns.barplot(x=feature_scores.values, y=feature_scores.index, palette='spring')\nax.set_title(\"Visualize feature scores of the features\")\nax.set_yticklabels(feature_scores.index)\nax.set_xlabel(\"Feature importance score\")\nax.set_ylabel(\"Features\")\nplt.show()","22fd2f84":"# drop the least important feature from X_train, X_test and test set for further analysis\nX1_train = X_train.drop(['Alone'], axis=1)\nX1_test = X_test.drop(['Alone'], axis=1)\ntest = test.drop(['Alone'], axis=1)","ddbc0afe":"accuracy_scores1 = []\n\n# iterate over classifiers and predict accuracy\nfor name, clf in zip(names, classifiers):\n    clf.fit(X1_train, y_train)\n    score = clf.score(X1_test, y_test)\n    score = round(score, 4)\n    accuracy_scores1.append(score)\n    print(name ,' : ' , score)","e763bb70":"classifiers_performance1 = pd.DataFrame({\"Classifiers\": names, \"Accuracy Scores\": accuracy_scores, \n                                         \"Accuracy Scores1\": accuracy_scores1})\nclassifiers_performance1","3b09e16f":"# instantiate the XGBoost classifier\ngpc_clf = GaussianProcessClassifier(1.0 * RBF(1.0))\n\n\n# fit the classifier to the modified training set\ngpc_clf.fit(X1_train, y_train)","b8dc7376":"# predict on the test set\ny1_pred = gpc_clf.predict(X1_test)\n","498c9daa":"# print the accuracy\nprint('Gaussian Process Classifier model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y1_pred)))","b44f109e":"# print confusion-matrix\n\ncm = confusion_matrix(y_test, y1_pred)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","72798ea0":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","e722e4ec":"print(classification_report(y_test, y1_pred))","3ee6805b":"TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","42d6db63":"# print classification accuracy\n\nclassification_accuracy = (TP + TN) \/ float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))","830a1333":"# print classification error\n\nclassification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))","daeb3429":"# print precision score\n\nprecision = TP \/ float(TP + FP)\n\nprint('Precision : {0:0.4f}'.format(precision))","dee9de2a":"recall = TP \/ float(TP + FN)\n\nprint('Recall or Sensitivity : {0:0.4f}'.format(recall))","44eeae08":"true_positive_rate = TP \/ float(TP + FN)\n\nprint('True Positive Rate : {0:0.4f}'.format(true_positive_rate))","58898fa9":"false_positive_rate = FP \/ float(FP + TN)\n\nprint('False Positive Rate : {0:0.4f}'.format(false_positive_rate))","e839c436":"specificity = TN \/ (TN + FP)\n\nprint('Specificity : {0:0.4f}'.format(specificity))","59cc833b":"# iterate over classifiers and calculate cross-validation score\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X1_train, y_train, cv = 10, scoring='accuracy')\n    print(name , ':{:.4f}'.format(scores.mean()))\n   ","26fbe3fb":"abc_params = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\n\ndtc_clf = DecisionTreeClassifier(random_state = 0, max_features = \"auto\", class_weight = \"balanced\", max_depth = None)\n\nabc_clf = AdaBoostClassifier(base_estimator = dtc_clf)\n\n\nabc_grid_search = GridSearchCV(estimator = abc_clf,  \n                               param_grid = abc_params,\n                               scoring = 'accuracy',\n                               cv = 5,\n                               verbose=0)\n\n\nabc_grid_search.fit(X1_train, y_train)\n","0592966e":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('AdaBoost GridSearch CV best score : {:.4f}\\n\\n'.format(abc_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('AdaBoost Parameters that give the best results :','\\n\\n', (abc_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\nabc_best = abc_grid_search.best_estimator_\nprint('\\n\\nXGBoost Estimator that was chosen by the search :','\\n\\n', (abc_best))","d9e30941":"lgb_clf = lgb.LGBMClassifier()\n\n\nlgb_params={'learning_rate': [0.005],\n    'num_leaves': [6,8,12,16],\n    'objective' : ['binary'],\n    'colsample_bytree' : [0.5, 0.6],\n    'subsample' : [0.65,0.66],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n\n\nlgb_grid_search = GridSearchCV(estimator = lgb_clf,  \n                               param_grid = lgb_params,\n                               scoring = 'accuracy',\n                               cv = 5,\n                               verbose=0)\n\n\nlgb_grid_search.fit(X1_train, y_train)\n","c679e1ec":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('LightGBM GridSearch CV best score : {:.4f}\\n\\n'.format(lgb_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('LightGBM Parameters that give the best results :','\\n\\n', (lgb_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\nlgb_best = lgb_grid_search.best_estimator_\nprint('\\n\\nLightGBM Estimator that was chosen by the search :','\\n\\n', (lgb_best))","40ddffc6":"gbc_clf = GradientBoostingClassifier()\n\ngbc_params = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngbc_grid_search = GridSearchCV(estimator = gbc_clf, \n                               param_grid = gbc_params, \n                               scoring = \"accuracy\", \n                               cv = 5,\n                               verbose = 0)\n\ngbc_grid_search.fit(X1_train,y_train)\n","e346db5a":"# examine the best model\n\n# best score achieved during the GridSearchCV\nprint('Gradient Boosting GridSearch CV best score : {:.4f}\\n\\n'.format(gbc_grid_search.best_score_))\n\n# print parameters that give the best results\nprint('Gradient Boosting Parameters that give the best results :','\\n\\n', (gbc_grid_search.best_params_))\n\n# print estimator that was chosen by the GridSearch\ngbc_best = gbc_grid_search.best_estimator_\nprint('\\n\\nGradient Boosting Estimator that was chosen by the search :','\\n\\n', (gbc_best))","5477e8c8":"votingC = VotingClassifier(estimators=[('abc', abc_best), ('lgb',lgb_best), ('gbc',gbc_best)], voting='soft')\n\nvotingC = votingC.fit(X1_train, y_train)","149a6fb2":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nsubmission = pd.concat([IDtest,test_Survived],axis=1)\n\n\nsubmission.to_csv(\"titanic_submission.csv\", index=False)","f68d01e9":"- Here `0 stands for not survived` and `1 stands for survived`.\n\n- So, 549 people survived and 342 people did not survive.\n\n- Let's visualize it by plotting.","a986d4cf":"## **12.9 f1-score** <a class=\"anchor\" id=\"12.9\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- **f1-score** is the weighted harmonic mean of precision and recall. \n- The best possible f1-score would be 1.0 and the worst would be 0.0. \n- f1-score is the harmonic mean of precision and recall. \n- So, f1-score is always lower than accuracy measures as they embed precision and recall into their computation. \n- The weighted average of f1-score should be used to compare classifier models, not global accuracy.","8f268d72":"## **10.1 Feature Importance with Random Forest model** <a class=\"anchor\" id=\"10.1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- Until now, I have used all the features given in the model. Now, I will select only the important features, build the model using these features and see its effect on accuracy.\n\n- First, I will create the Random Forest model as follows:-","eaab28b9":"## **3.1 Explore the training set** <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Notebook Contents](#0.1)","d0ba06b7":"# **6. Feature Engineering** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- In this section, we will make additional columns for future analysis.","89dd156e":"## **10.2 Visualize feature scores** <a class=\"anchor\" id=\"10.2\"><\/a>\n\n[Notebook Contents](#0.1)","65f5508b":"# **10. Feature Selection** <a class=\"anchor\" id=\"10\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- In this section, we will see how to improve model performance by feature selection.\n\n- We will visualize feature importance with random forest classifier and drop the least important feature, rebuild the model and check effect on accuracy.  \n \n- For a comprehensive overview on feature selection techniques, please see the kernel - \n\n   - [A Reference Guide to Feature Selection Methods](https:\/\/www.kaggle.com\/prashant111\/a-reference-guide-to-feature-selection-methods)\n","2dda1c01":"## **5.5 Outlier Detection** <a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- The `Age` and `Fare` variable contain putliers. Now, let's check for outliers in `Age` and `Fare`.\n\n- Let's draw boxplots to visualise outliers in the above variables.\n","7bd1b530":"# **16. Submission** <a class=\"anchor\" id=\"16\"><\/a>\n\n[Notebook Contents](#0.1)\n","c94155ac":"## **4.5 Embarked** <a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Notebook Contents](#0.1)","6ad7376b":"# **8. Feature Scaling** <a class=\"anchor\" id=\"8\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- We need to do Feature Scaling first before proceeding with modeling.","b3db264b":"- We can see that the most important feature is `Person` and least important feature is `Alone`.","83ab5688":"- As expected females have higher probability of survival (value 1) 74.20% than males 18.89%.","eb22db6d":"- We can see that the peak over 0 age for classes 2 and 3, coincides with the classes that had children. Class 1 did not have a lot of children, unsurprisingly. Note also, that older people were high class.","88839eaa":"## **3.2 Explore the test set** <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Notebook Contents](#0.1)","45300973":"## **5.2 Imputation of missing values in Age** <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- We will make additional column with the title of the person (Mr, Mrs, Miss, etc).\n\n- Then, we impute the missing values in age with the median age for each title.\n\n- Let's first make a function to extract title from `Name` feature.","8319ab2e":"- `0` indicates that person is travelling with family and `1` indicates that he is travelling alone.","6a300c09":"The confusion matrix shows 156 + 93 = 249 correct predictions and 19 + 27 = 46 incorrect predictions.\n\nIn this case, we have\n\n- True Positives (Actual Positive:1 and Predict Positive:1) - 156\n- True Negatives (Actual Negative:0 and Predict Negative:0) - 93\n- False Positives (Actual Negative:0 but Predict Positive:1) - 19 (Type I error)\n- False Negatives (Actual Positive:1 but Predict Negative:0) - 27 (Type II error)","6679803a":"### **Types of Variables**\n\n\n- Now, we will classify the variables into categorical and numerical variables.","22f345f2":"### **View shape of test set**","e276ce39":"### **Types of Variables**\n\n\n- Now, we will classify the variables into categorical and numerical variables.","8632ccb1":"## **12.1 Classification Report** <a class=\"anchor\" id=\"12.1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- **Classification Report** is another way to evaluate the classification model performance. \n- It displays the **precision**, **recall**, **f1** and **support** scores for the model.\n- We can print a classification report as follows:-","dcc2d7d7":"- We can see that almost half number of people who are travelling with family survived whereas large number of people travelling alone did not survive.\n\n- So, travelling alone or with family plays a major role in deciding the survival probability.","b6686c67":"## **12.10 Support** <a class=\"anchor\" id=\"12.10\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- Support is the actual number of occurrences of the class in our dataset.","96b8d4e0":"So, now we will come to the end of this notebook.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","db9f7cb0":"## **4.6 Age** <a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Notebook Contents](#0.1)","5fcd82aa":"<a class=\"anchor\" id=\"0\"><\/a>\n# **Titanic Survival : A Detailed Classification Pipeline** \n\n\n\n## **Introduction**\n\n\nPrashant Banerjee\n\n\nApril 2020\n\n\nMachine Learning model development is a complicated process. It is a difficult task for beginners. Most beginners get lost in the process. So, in this notebook, I will try to provide a framework for model development to solve a binary classification problem. It will teach you how to approach a problem, how to think like a data scientist and what to code.\n\nI have used the famous titanic dataset for the illustration purposes. I will try to provide clear explanations and clean code so that the reader can solve any classification problem.\n\n\nThe reader is welcome to use, comment, experiment and fork this notebook for their personnal use.\n\n\nSo, let's dive in.","b6592179":"# **12. Classification Metrices** <a class=\"anchor\" id=\"12\"><\/a>\n\n[Notebook Contents](#0.1)","cef1de12":"## **12.4 Precision** <a class=\"anchor\" id=\"12.4\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- **Precision** can be defined as the percentage of correctly predicted positive outcomes out of all the predicted positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true and false positives (TP + FP).\n\n- So, Precision identifies the proportion of correctly predicted positive outcome. It is more concerned with the positive class than the negative class.\n\n- Mathematically, precision can be defined as the ratio of TP to (TP + FP).","8817e32b":"- Here `0 stands for not survived` and `1 stands for survived`.\n\n- So, we can see that `Pclass` plays a major role in survival.\n\n- Majority of people survived in `Pclass 1` while a large number of people do not survive in `Pclass 3`.","fffffd8b":"# **15. Ensemble Modeling** <a class=\"anchor\" id=\"15\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- I decided to choose a voting classifier to combine the predictions coming from the above 3 classifiers.\n","a3c7dc05":"- It can be seen that cross-validation does not result in improved performance.","b5baed28":"### **Observations about dataset**","3ffa71f4":"# **9. Modelling** <a class=\"anchor\" id=\"9\"><\/a>\n\n[Notebook Contents](#0.1)","4c37438b":"- Let's check the percentage of survival for males and females separately.","87bbfc1c":"# **1. The Problem Statement** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- The first step in a machine learning model development is to define the problem statement. It is a necessary step as it will help us to stay focused and move in the right direction.\n\n- So, in this case the problem statement is to predict how many people survive the titanic shipwreck disaster.\n","a3e830a4":"## **12.8 Specificity (True Negative Rate)** <a class=\"anchor\" id=\"12.8\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- **Specificity** is also called **True Negative Rate**.","480e14fd":"- Drop the `Sex` variable.","9e3f9775":"- We can see that males have lower probability of survival than females and children, regardless of the class they were in. \n- As for women and children, being in class 3 meant that their chances of survival were lower.","dad005cd":"## **12.2 Classification Accuracy** <a class=\"anchor\" id=\"12.2\"><\/a>\n\n[Notebook Contents](#0.1)\n","b754a80a":"### **View statistical properties of training set**","67f3cd11":"# **13. Cross Validation** <a class=\"anchor\" id=\"13\"><\/a>\n\n[Notebook Contents](#0.1)\n","b92cfa8a":"- Let's check that the above variables are capped at their maximum values.","84b6008b":"### **View concise summary of test set**","7f9d1c0e":"### **Print variables containing missing values**","c89cf902":"### **Preview test set**","a6c02d04":"# **18. Credits** <a class=\"anchor\" id=\"18\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- This notebook is based on couple of excellent notebooks on titanic dataset. These are -\n\n  - [Titanic top 4 % ensemble modeling](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n  - [Titanic Survival Prediction End to End ML Pipeline](https:\/\/www.kaggle.com\/poonaml\/titanic-survival-prediction-end-to-end-ml-pipeline)\n  \n\n- We have adapted several lines of code from above notebooks.\n","4af31296":"## **4.1 Missing values** <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Notebook Contents](#0.1)","152afaaa":"### **View shape of training set**","86f11d23":"# **4. Data Visualization** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Notebook Contents](#0.1)","240d7873":"### **View statistical properties of test set**","1fe2ba14":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Notebook Contents**\n\n- [Part 1 - The Problem Statement](#1)\n- [Part 2 - Basic Set Up](#2)\n    - [2.1  Import libraries](#2.1)\n    - [2.2 Read dataset](#2.2)\n- [Part 3 - Data Exploration](#3)\n    - [3.1\tExplore the training set](#3.1)\n    - [3.2\tExplore the test set](#3.2)\n- [Part 4 - Data Visualization](#4)\n    - [4.1\tMissing values](#4.1)\n    - [4.2\tSurvived](#4.2)\n    - [4.3\tSex](#4.3)\n    - [4.4\tPclass](#4.4)\n    - [4.5\tEmbarked](#4.5)\n    - [4.6\tAge](#4.6)\n- [Part 5 - Data Preprocessing](#5)\n    - [5.1\tRemove redundant features](#5.1)\n    - [5.2\tImputation of missing values in Age](#5.2)\n    - [5.3\tImputation of missing values in Cabin](#5.3)\n    - [5.4\tImputation of missing values in Embarked](#5.4)\n    - [5.5\tOutlier detection](#5.5)\n- [Part 6 - Feature Engineering](#6)\n    - [6.1\tCategorize passengers as male, female or child](#6.1)\n    - [6.2\tMake additional variable : travel alone](#6.2)\n    - [6.3\tCorrelation of features with target](#6.3)\n- [Part 7 - Categorical Variable Encoding](#7)\n- [Part 8 - Feature Scaling](#8)\n- [Part 9 - Modeling](#9)\n    - [9.1\tPredict accuracy with different algorithms](#9.1)\n    - [9.2\tPlot the classifier accuracy scores](#9.2)\n- [Part 10 - Feature Selection](#10)\n    - [10.1\tFeature Importance with Random Forest Model](#10.1)\n    - [10.2\tVisualize feature scores](#10.2)\n    - [10.3\tDrop least important feature](#10.3)\n- [Part 11 - Confusion Matrix](#11)\n- [Part 12 - Classification Metrices](#12)\n    - [12.1\tClassification Report](#12.1)\n    - [12.2\tClassification Accuracy](#12.2)\n    - [12.3\tClassification Error](#12.3)\n    - [12.4\tPrecision](#12.4)\n    - [12.5\tRecall](#12.5)\n    - [12.6\tTrue Positive Rate](#12.6)\n    - [12.7\tFalse Positive Rate](#12.7)\n    - [12.8\tSpecificity or True Negative Rate](#12.8)\n    - [12.9\tF1 score](#12.9)\n   -  [12.10\tSupport](#12.10)\n- [Part 13 - Cross-Validation](#13)\n- [Part 14 - Hyperparameter Optimization using Grid Search CV](#14)\n- [Part 15 - Ensemble Modeling](#15)\n- [Part 16 - Submission](#16)\n- [Part 17 - Conclusion](#17)\n- [Part 18 - Credits](#18)\n","bae62ad1":"### **View profile report of training set**","77e542e4":"- We can see that majority of passengers are aged between 20 and 40.","a2465ae5":"## **12.6 True Positive Rate** <a class=\"anchor\" id=\"12.6\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- **True Positive Rate** is synonymous with **Recall**.","372a8a91":"### **AdaBoost Classifier Parameters tuning**","de468bd7":"## **5.1 Remove redundant features** <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- The `Ticket` and `PassengerId` are redundant features. So, we will remove them from the dataset.","3ca06ac6":"- Now females have higher probability of survival than males.\n- Let' check it","7b1ea7b9":"- We can see that `Gaussian Process` has the maximum accuracy of 0.8441.\n\n- We will use the `Gaussian Process Classifier` to plot the confusion-matrix.\n","ed3451d2":"### **View profile report of test set**","58c4b08c":"## **4.4 Pclass** <a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Notebook Contents](#0.1)","f10d83a5":"## **12.3 Classification Error** <a class=\"anchor\" id=\"12.3\"><\/a>\n\n[Notebook Contents](#0.1)","24ab47b9":"## **10.3 Drop least important feature** <a class=\"anchor\" id=\"10.3\"><\/a>\n\n[Notebook Contents](#0.1)","5f901126":"- So, we are right that `Age`, `Cabin` and `Embarked` contain missing values.","753fe663":"## **9.2 Plot the classifier accuracy scores** <a class=\"anchor\" id=\"9.2\"><\/a>\n\n[Notebook Contents](#0.1)\n","0e37e6b0":"- Since, `Age` and `Fare` do not have values less than 0. So, we assume their minimum values to be 0.\n\n- I will use top-coding approach to cap maximum values and remove outliers from the above variables.\n\n","b103a136":"- Now, I will drop the least important feature `Alone` from the model, rebuild the model and check its effect on accuracy.","78337fa5":"## **5.3 Imputation of missing values in Cabin** <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- To extract missing values in Cabin, we extract Deck from Cabin and add 'Unknown' where NA.\n","bd830c6a":"- Now, let's take a look at train and test set.","71501642":"# **14. Hyperparameter Optimization using GridSearch CV** <a class=\"anchor\" id=\"14\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- I choose the top 3 classifiers with maximum accuracy for ensemble modeling.\n\n- They are `AdaBoost`, `LightGBM` and `Gradient Boosting`.\n\n- So, we will tune the hyperparameters of these models before proceeding.\n","03054150":"## **4.3 Sex** <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Notebook Contents](#0.1)","ad02e2af":"- There are 4 variables that need to be categorical encoded. \n\n- They are `Embarked`,`Deck`,`Person` and `Alone`","8e18a34b":"## **2.2 Read Dataset** <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Notebook Contents](#0.1)\n\n","2ff2c1d4":"- We will again visit this data visualization section in Feature Engineering section.","8cd0172c":"Now, I will use the feature importance variable to see feature importance scores.","dba98352":"- The accuracy score of top performing algorithms in descending order is given below -","ad71497e":"### **Print variables containing missing values**","73d26c69":"**I hope you find this notebook useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n\n","62d64841":"### **Preview training set**","1d647d8c":"# **2. Basic Set Up** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Notebook Contents](#0.1)","f066a947":"# **17. Conclusion** <a class=\"anchor\" id=\"17\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- In this notebook, we have build a classification model on the famous titanic dataset.\n\n- We have used a voting ensemble classifier for making predictions.","d43e09ae":"### **Let's again check for missing values**","08892cdc":"- From the above pie-charts, we can deduce that females probability of survival is 74.2% (cyan color) while males probability of survival is 18.9% (cyan color).","f0fc181b":"[Go to Top](#0)","beb5a30a":"# **5. Data Preprocessing** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Notebook Contents](#0.1)","4dbb1e03":"- It seems that several of the variables - `Age`, `Cabin` and `Embarked` contain missing values. Let's check it.","63339fc8":"## **6.1 Categorize passengers as male, female or child** <a class=\"anchor\" id=\"6.1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- Children have much larger probability of survival than men or women. So, we will categorize the passengers as men, women or child.","830b1e23":"## **6.3 Correlation of features with target** <a class=\"anchor\" id=\"6.3\"><\/a>\n\n[Notebook Contents](#0.1)","b3e72200":"## **6.2 Make additional variable : travel alone** <a class=\"anchor\" id=\"6.2\"><\/a>\n\n[Notebook Contents](#0.1)\n","91ed9d5b":"## **4.2 Survived ** <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Notebook Contents](#0.1)","03d8d017":"- We can see that `Survived` is negatively correlated with `Pclass`,`Age`,`SibSp`,`Embarked`,`Deck`,`Person`,`Alone` and positively correlated with `Parch` and `Fare`.\n\n- We can also plot a heatmap to visualize the relationship between features.","f77fd541":"## **2.1 Import Libraries** <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Notebook Contents](#0.1)\n","848ead44":"### **View concise summary of training set**","d686bb30":"- We can see that port of embarkment plays a major role in survival probability.","fc3b15e9":"### **LightGBM Parameters tuning**","7d6cd294":"- The people who are on deck `C` and `B` have larger probability of survival.","9a09b748":"- So, we are right that `Age`, `Cabin` and `Embarked` contain missing values.","9f72e97b":"# **11. Confusion matrix** <a class=\"anchor\" id=\"11\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- A **confusion matrix** is a tool for summarizing the performance of a classification algorithm. A confusion matrix will give us a clear picture of classification model performance and the types of errors produced by the model. It gives us a summary of correct and incorrect predictions broken down by each category. The summary is represented in a tabular form.\n\n- Four types of outcomes are possible while evaluating a classification model performance. These four outcomes are described below:-\n\n- **True Positives (TP)** \u2013 True Positives occur when we predict an observation belongs to a certain class and the observation actually belongs to that class.\n\n- **True Negatives (TN)** \u2013 True Negatives occur when we predict an observation does not belong to a certain class and the observation actually does not belong to that class.\n\n- **False Positives (FP)** \u2013 False Positives occur when we predict an observation belongs to a certain class but the observation actually does not belong to that class. This type of error is called **Type I error**.\n\n- **False Negatives (FN)** \u2013 False Negatives occur when we predict an observation does not belong to a certain class but the observation actually belongs to that class. This is a very serious error and it is called **Type II error**.\n\n- These four outcomes are summarized in a confusion matrix.\n\n- We will use the `Gaussian Process Classifier` to plot the confusion-matrix.\n","407ac179":"# **7. Categorical Variable Encoding** <a class=\"anchor\" id=\"7\"><\/a>\n\n[Notebook Contents](#0.1)","b354a9a6":"## **5.4 Imputation of missing values in Embarked** <a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- We impute Embarked with the most frequent port (S).","108cec94":"### **Gradient Boost Parameters tuning**","1c28908f":"- So, 537 people are travelling alone and 354 people are travelling with family.","b363b68c":"## **12.7 False Positive Rate** <a class=\"anchor\" id=\"12.7\"><\/a>\n\n[Notebook Contents](#0.1)","3bca9def":"## **12.5 Recall** <a class=\"anchor\" id=\"12.5\"><\/a>\n\n[Notebook Contents](#0.1)\n\n- **Recall** can be defined as the percentage of correctly predicted positive outcomes out of all the actual positive outcomes. It can be given as the ratio of true positives (TP) to the sum of true positives and false negatives (TP + FN). \n\n- **Recall** is also called **Sensitivity**.\n\n- Recall identifies the proportion of correctly predicted actual positives.\n\n- Mathematically, Recall can be given as the ratio of TP to (TP + FN).","bfee4dfe":"## **9.1 Predict accuracy with different algorithms** <a class=\"anchor\" id=\"9.1\"><\/a>\n\n[Notebook Contents](#0.1)\n\n\n- I predict accuracy with 15 popular classifiers and evaluate their performance.","4f0abb3a":"The above plot indicates the percentage of survivors per class.","8cf97b8c":"# **3. Data Exploration** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Notebook Contents](#0.1)\n"}}