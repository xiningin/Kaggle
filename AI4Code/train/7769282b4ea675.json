{"cell_type":{"8b10b658":"code","809d2b73":"code","a7a6f5bd":"code","8955aa7e":"code","8b1a23d3":"code","9e69a935":"code","67f18003":"code","1ddad0c9":"code","0401b13d":"code","ba1a8967":"code","51cae49f":"code","945e38ba":"code","506e87e2":"code","be2b8fc7":"code","b9af94d4":"code","bbca0d7e":"markdown","63218394":"markdown","1bf29033":"markdown"},"source":{"8b10b658":"pip install larq","809d2b73":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport larq as lq ","a7a6f5bd":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","8955aa7e":"x_train = (train.iloc[:,1:].values).astype('float32') # all pixel values\ny_train = train.iloc[:,0].values.astype('int32') # only labels i.e targets digits","8b1a23d3":"# you can on hot encode or us it as it is, only change the loss to sparse_categorical_crossentropy\nfrom keras.utils.np_utils import to_categorical\ny_train = to_categorical(y_train)","9e69a935":"# don't forget test set\ntest = test.values.astype('float32')","67f18003":"train_images, test_images, train_labels, test_labels = train_test_split(\n                    x_train, y_train, \n                    test_size=0.33, random_state=42)","1ddad0c9":"print(train_images.shape)\nprint(test_images.shape)","0401b13d":"train_images = train_images.reshape((28140, 28, 28, 1))  \ntest_images = test_images.reshape((13860, 28, 28, 1)) \n\ntest = test.reshape((28000, 28, 28, 1))","ba1a8967":"# Normalize pixel values to be between -1 and 1\ntrain_images, test_images = train_images \/ 127.5 - 1, test_images \/ 127.5 - 1","51cae49f":"kwargs = dict(input_quantizer=\"ste_sign\", kernel_quantizer=\"ste_sign\", kernel_constraint=\"weight_clip\")\n\nmodel = tf.keras.models.Sequential()\n\nmodel.add(lq.layers.QuantConv2D(32, (3, 3), kernel_quantizer=\"ste_sign\", \n                                kernel_constraint=\"weight_clip\", use_bias=False, \n                                input_shape=(28, 28, 1)))\nmodel.add(tf.keras.layers.MaxPooling2D((2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization(scale=False))\n\nmodel.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\nmodel.add(tf.keras.layers.MaxPooling2D((2, 2)))\nmodel.add(tf.keras.layers.BatchNormalization(scale=False))\n\nmodel.add(lq.layers.QuantConv2D(64, (3, 3), use_bias=False, **kwargs))\nmodel.add(tf.keras.layers.BatchNormalization(scale=False))\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(lq.layers.QuantDense(64, use_bias=False, **kwargs))\nmodel.add(tf.keras.layers.BatchNormalization(scale=False))\nmodel.add(lq.layers.QuantDense(10, use_bias=False, **kwargs))\nmodel.add(tf.keras.layers.BatchNormalization(scale=False))\nmodel.add(tf.keras.layers.Activation(\"softmax\"))","945e38ba":"# always nice to have a look at this lovely model\nlq.models.summary(model)","506e87e2":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nsparse_categorical_crossentropy is the loss function for integer labels\n\nh = model.fit(train_images, train_labels, batch_size=64, epochs=6)\n\ntest_loss, test_acc = model.evaluate(test_images, test_labels)","be2b8fc7":"print(f\"Test accuracy {test_acc * 100:.2f} %\")\nprint(f\"Test loss {test_loss}\")","b9af94d4":"pd.DataFrame(h.history).plot()\n# and as an ending, I'll let you plot the accuracy and loss changes per epoch ;) \n","bbca0d7e":"Test accuracy 97.80 %\nTest loss 0.405355900223293","63218394":"Epoch 1\/6\n28140\/28140 [==============================] - 41s 1ms\/step - loss: 0.7582 - acc: 0.8733\nEpoch 2\/6\n28140\/28140 [==============================] - 39s 1ms\/step - loss: 0.5240 - acc: 0.9502\nEpoch 3\/6\n28140\/28140 [==============================] - 40s 1ms\/step - loss: 0.4837 - acc: 0.9603\nEpoch 4\/6\n28140\/28140 [==============================] - 36s 1ms\/step - loss: 0.4613 - acc: 0.9644\nEpoch 5\/6\n28140\/28140 [==============================] - 37s 1ms\/step - loss: 0.4494 - acc: 0.9682\nEpoch 6\/6\n28140\/28140 [==============================] - 36s 1ms\/step - loss: 0.4420 - acc: 0.9709\n13860\/13860 [==============================] - 5s 343us\/step","1bf29033":"Hello, \n\nI thought to try something new, as in a new library that some lovely people I know put together. You can find more info [here](http:\/\/medium.com\/dataseries\/localized-computation-3e1edad63000).\n\nNow let's have a quick ride:"}}