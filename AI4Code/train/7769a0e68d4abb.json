{"cell_type":{"787d5420":"code","865e0355":"code","fef9c01c":"code","58566085":"code","8f8e2da2":"code","f485e628":"code","d3fb4c94":"code","7efdec86":"code","68379c3e":"code","717969df":"code","4e55f598":"code","20f3aee2":"code","49f78d4c":"code","b80d948a":"code","87586cb1":"code","d4805fb1":"code","72d5f625":"code","e5e1254b":"code","3df0b5a2":"code","8181cd2b":"code","d6406d4f":"code","5195e327":"code","480c19e6":"code","2b256e3e":"code","c4ece250":"code","64022a17":"code","83a9c4d4":"code","fe3172f6":"code","7188c6f2":"code","8c79e4bc":"code","d12ac577":"code","bfe76cbe":"code","e7bfb962":"code","b61adbf4":"markdown","4ee799f3":"markdown","e724c657":"markdown","16114581":"markdown","9f511680":"markdown","751b6b56":"markdown","184c8d39":"markdown","de368892":"markdown","8cbf74ba":"markdown","54924033":"markdown","f57d64cd":"markdown","3090927d":"markdown","dbe9fa22":"markdown","08142efb":"markdown","1e26ec32":"markdown","9d8f452c":"markdown","066e96a4":"markdown","6b61342f":"markdown","1cc06c75":"markdown","1c6e7e2c":"markdown"},"source":{"787d5420":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","865e0355":"# Packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot","fef9c01c":"data = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndata.head()","58566085":"# Lets find what column have a null value\ndata.info()","8f8e2da2":"# now lets removed a row that bmi have a null value\ndata = data.dropna(subset = ['bmi'])\ndata.info()","f485e628":"Attri = data[['age', 'avg_glucose_level', 'bmi', 'stroke']]\nAttri.head()","d3fb4c94":"g = sns.pairplot(Attri, diag_kind=\"kde\")\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","7efdec86":"plt.figure(figsize=(7,4))\ncor = Attri.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","68379c3e":"cat = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']","717969df":"for f in cat:\n    data[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","4e55f598":"data.drop(data.loc[data['gender']== 'Other'].index, inplace=True)\ndata['gender'].value_counts()","20f3aee2":"data.info()","49f78d4c":"#cat is the column name on the above for categorical feature\ndf = data\ndata_feature = pd.get_dummies(df, columns=cat)\ndata_feature.head()","b80d948a":"#lets drop the work_type_Never_worked because based on the chart on the above for work type never work is the least count \n#lets drop also the id number because it is only a counter number of patients on the data\ndata_feature.pop('work_type_Never_worked')\ndata_feature.pop('id')\nFeature = data_feature\nFeature.head()","87586cb1":"Feature.columns","d4805fb1":"X = Feature[['age', 'avg_glucose_level', 'bmi', 'gender_Female',\n       'gender_Male', 'hypertension_0', 'hypertension_1',\n       'heart_disease_0', 'heart_disease_1', 'ever_married_No',\n       'ever_married_Yes', 'work_type_Govt_job', 'work_type_Private',\n       'work_type_Self-employed', 'work_type_children', 'Residence_type_Rural',\n       'Residence_type_Urban', 'smoking_status_Unknown',\n       'smoking_status_formerly smoked', 'smoking_status_never smoked',\n       'smoking_status_smokes']]\nX.shape","72d5f625":"y = Feature[['stroke']]\ny.head()","e5e1254b":"X = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]","3df0b5a2":"stan_data = pd.DataFrame(X)\nfig, ax = pyplot.subplots(figsize= (10,6))\nsns.kdeplot(ax = ax, data=X , legend = False)","8181cd2b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","d6406d4f":"from sklearn.metrics import classification_report, confusion_matrix\nimport itertools\nfrom sklearn.metrics import f1_score\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.figure(figsize=(5,4))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5195e327":"RF = RandomForestClassifier( max_depth= 10, random_state=0, n_estimators=10)\nRFF = RF.fit(X_train, y_train)\n#Get the Score of Random Forest Classifier both Train and Test\nRFM_Train = RF.score(X_train, y_train)\nRFM_Test = RF.score(X_test, y_test)\nprint('Random Forest Train Score: ' + str(RFM_Train))\nprint('Random Forest Test Score: ' + str(RFM_Test))\n#Predict value of RFM\nRFM_pred = RF.predict(X_test)\nprint(RFM_pred[0:5])","480c19e6":"f1_score(y_test, RFM_pred, average='weighted') \n# Compute confusion matrix  \nRFM_matrix = confusion_matrix(y_test, RFM_pred)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, RFM_pred))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(RFM_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='Random Forest Confusion matrix')","2b256e3e":"### Logistic Regression CLassifier\nfrom sklearn.linear_model import LogisticRegression\nLRC = LogisticRegression(random_state=0)\nLRCF = LRC.fit(X_train, y_train)\n#Get the Score of Logistic Regression Classifier both Train and Test\nLRC_Train = LRC.score(X_train, y_train)\nLRC_Test = LRC.score(X_test, y_test)\nprint('Logistic Regression Classifier Train Score: ' + str(LRC_Train))\nprint('Logistic Regression Classifier Test Score: ' + str(LRC_Test))\n#Predict value of LRC\nLRC_pred = LRC.predict(X_test)\nprint(LRC_pred[0:5])","c4ece250":"f1_score(y_test, LRC_pred, average='weighted') \n# Compute confusion matrix  \nLRC_matrix = confusion_matrix(y_test, LRC_pred)\nnp.set_printoptions(precision=2)\nprint (classification_report(y_test, LRC_pred))\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(LRC_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='Logistic Regression Confusion matrix')","64022a17":"from sklearn import svm\nSVM = svm.SVC()\nSVMF = SVM.fit(X_train, y_train)\n#Get the Score of Support Vector Machine both Train and Test\nSVM_Train = SVM.score(X_train, y_train)\nSVM_Test = SVM.score(X_test, y_test)\nprint('Support Vector Machine Train Score: ' + str(SVM_Train))\nprint('Support Vector Machine Test Score: ' + str(SVM_Test))\n#Predict value of SVM\nSVM_pred = SVM.predict(X_test)\nprint(SVM_pred[0:5])","83a9c4d4":"f1_score(y_test, SVM_pred, average='weighted') \n# Compute confusion matrix  \nSVM_matrix = confusion_matrix(y_test, SVM_pred)\nnp.set_printoptions(precision=2)\nprint (classification_report(y_test, SVM_pred))\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(SVM_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='SVM Confusion matrix')","fe3172f6":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=10)\nKNNF = neigh.fit(X_train, y_train)\n#Get the Score of KNN Classifier both Train and Test\nKNN_Train = neigh.score(X_train, y_train)\nKNN_Test = neigh.score(X_test, y_test)\nprint('KNN Classifier Train Score: ' + str(KNN_Train))\nprint('KNN Classifier Machine Test Score: ' + str(KNN_Test))\n#Predict value of KNN\nKNN_pred = neigh.predict(X_test)\nprint(KNN_pred[0:5])","7188c6f2":"f1_score(y_test, KNN_pred, average='weighted') \n# Compute confusion matrix  \nKNN_matrix = confusion_matrix(y_test, KNN_pred)\nnp.set_printoptions(precision=2)\nprint (classification_report(y_test, KNN_pred))\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(KNN_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='K-Neighbor Confusion matrix')","8c79e4bc":"from sklearn.naive_bayes import GaussianNB\nNBC = GaussianNB()\nNBCF = NBC.fit(X_train, y_train)\n#Get the Score of Naive Bayes Classifier both Train and Test\nNBC_Train = NBC.score(X_train, y_train)\nNBC_Test = NBC.score(X_test, y_test)\nprint('Naive Bayes Classifier Train Score: ' + str(NBC_Train))\nprint('Naive Bayes Classifier Test Score: ' + str(NBC_Test))\n#Predict value of Naive Bayes Classifier\nNBC_pred = NBC.predict(X_test)\nprint(NBC_pred[0:5])","d12ac577":"f1_score(y_test, NBC_pred, average='weighted') \n# Compute confusion matrix  \nNBC_matrix = confusion_matrix(y_test, NBC_pred)\nnp.set_printoptions(precision=2)\nprint (classification_report(y_test, NBC_pred))\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(NBC_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='Naive Bayes Confusion matrix')","bfe76cbe":"from sklearn.ensemble import GradientBoostingClassifier\nGBC = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nGBCF = GBC.fit(X_train, y_train)\n#Get the Score of Gradient Boosting Classifier both Train and Test\nGBC_Train = GBC.score(X_train, y_train)\nGBC_Test = GBC.score(X_test, y_test)\nprint('Gradient Boosting Classifier Train Score: ' + str(GBC_Train))\nprint('Gradient Boosting Classifier Test Score: ' + str(GBC_Test))\n#Predict value of Gradient Boosting Classifier\nGBC_pred = GBC.predict(X_test)\nprint(GBC_pred[0:5])","e7bfb962":"f1_score(y_test, GBC_pred, average='weighted') \n# Compute confusion matrix  \nGBC_matrix = confusion_matrix(y_test, GBC_pred)\nnp.set_printoptions(precision=2)\n\nprint (classification_report(y_test, GBC_pred))\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(GBC_matrix, classes=['No Stroke','Have Stroke'],normalize= False,  title='Gradient Boosting Classifier Confusion matrix')","b61adbf4":"### Support Vector Machine (SVM)","4ee799f3":"## MODELS","e724c657":"### Feature Selection","16114581":"#### Categorical Features","9f511680":"### Data Cleaning","751b6b56":"## Engineering Features","184c8d39":"### KNN Classifier","de368892":"### Numerical Features","8cbf74ba":"### Creating test and train data coming from the dataset","54924033":"#### One Hot Encoding for Categorical Features","f57d64cd":"### Conclusion:\nBased on the 6 models, all precision for Not having stroke is more than 95% but for precision for predicting the  patient to have stroke, the is highest Random Forest Model which is 43% and the 2nd to the highest is only 17% which is gradient boosting classifier.\n    \nTherefore the most suitable to Model among the 6 model is Random Forest Classifier with a: <br>\n    Having Stroke: 43% Precision<br>\n    Not Having Stroke: 96% Precision<br>\n    F1-Score: 98%<br>\n    Accuracy: 96%<br>\n    ","3090927d":"### Drop NaN value on the avg_glucose _level","dbe9fa22":"### Gradient Boosting CLassifier","08142efb":"### Confusion Matrix Function","1e26ec32":"### Drop the Other Value on the gender column","9d8f452c":"### Naive Bayes Classifier ","066e96a4":"#### As you can see the column bmi is the only column have a null value which is have a total of 5110 - 4909 = 201 null values.","6b61342f":"### Normalization\/Standardization","1cc06c75":"#### Based on the Numerical Features Chart:\n    1) Age is the most correlated to the stroke.\n    2) At the age of greater than 60 years old he have a high risk to have stroke.\n    3) Between 25 to 35 bmi have a high risk to have stroke.\n    4) Between 50 to 100mg\/l of average glucose on blood have a high risk to have stroke.","1c6e7e2c":" ### Random Forest Model"}}