{"cell_type":{"3f119eaa":"code","d92f3468":"code","6ed3354d":"code","14dfdabd":"code","c5d7f9da":"code","6a6ff293":"code","8de28825":"code","7e638c61":"code","f1bdbb0a":"code","272b158e":"code","a955e49d":"code","aaf674ca":"code","2aa84ccb":"code","51c834f0":"code","7570c910":"code","337530ff":"code","32d5b901":"code","084aebda":"code","77d0e4a8":"code","12839c1f":"code","80040681":"code","ec96d98a":"code","b79acecc":"code","ad0b2605":"markdown","0bfb9c21":"markdown","3f805fb0":"markdown","87d1ace5":"markdown","02ee3e68":"markdown","675ae436":"markdown","74c2453e":"markdown","2b369ab6":"markdown","068f8177":"markdown","b88b40ee":"markdown","4a4ff98b":"markdown","710811f4":"markdown","80722ce0":"markdown","9c62dd99":"markdown"},"source":{"3f119eaa":"import numpy as np \nimport tensorflow as tf\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.examples.tutorials.mnist import input_data","d92f3468":"train_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\" , nrows=5000)\n#test_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\", nrows=10000)","6ed3354d":"import struct\ndef read_idx(filename):\n    with open(filename, 'rb') as f:\n        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)","14dfdabd":"test_data = read_idx(\"..\/input\/mytestdata\/Test_data.idx3-ubyte\")[:1000]\ntest_labels = read_idx(\"..\/input\/mytestdata\/Test_labels.idx1-ubyte\")[:1000]","c5d7f9da":"train_data.head()","6a6ff293":"labels = train_data[\"label\"].values\nlabels.shape","8de28825":"train_data.drop(\"label\",axis=1, inplace=True)\nplt.figure(figsize=(20,6))\nsns.countplot(labels)","7e638c61":"one_hot_labels = pd.get_dummies(labels)\none_hot_test_labels = pd.get_dummies(test_labels)\nprint(\"The shape of training labels :{} \\n The shape of test labels {}\".format(one_hot_labels.shape, one_hot_test_labels.shape))","f1bdbb0a":"image_data = train_data.values\nimages = np.array([image_data[i].reshape(28,28) for i in range(image_data.shape[0])])\ntest_images = test_data.shape\ntest_data = np.array([test_data[i].reshape(-1) for i in range(test_data.shape[0])])","272b158e":"print(\"shape of image data {} \\n shape of images {} \\n shape of test data{} \\n shape of test images {}\"\n      .format(image_data.shape , images.shape , test_data.shape , test_images))","a955e49d":"plt.imshow(images[2])","aaf674ca":"X = tf.placeholder(tf.float32 , shape= [None, 784], name=\"Images\")\ny = tf.placeholder(tf.float32,  shape= [None , 10], name=\"Labels\")","2aa84ccb":"def initialisation_weight(shape):\n    init_weight = tf.truncated_normal(shape , stddev=0.1)\n    return tf.Variable(init_weight)","51c834f0":"def initialisation_bias(shape):\n    init_bias = tf.constant(0.1 , shape=shape)\n    return tf.Variable(init_bias)","7570c910":"def conv2d(x,W):\n    return tf.nn.conv2d(x,W, strides=[1,1,1,1], padding=\"SAME\")","337530ff":"def maxpool(x):\n    return tf.nn.max_pool(x , ksize = [1,2,2,1] , strides=[1,2,2,1] , padding=\"SAME\")","32d5b901":"def convolutional_layer(input_x , shape):\n    W= initialisation_weight(shape)\n    b= initialisation_bias([shape[3]])\n    \n    return tf.nn.relu(conv2d(input_x , W) + b)","084aebda":"def normal_layer(input_layer , size):\n    input_size = int(input_layer.get_shape()[1])\n    W = initialisation_weight([input_size , size])\n    b= initialisation_bias([size])\n    \n    return tf.matmul(input_layer , W) +b","77d0e4a8":"x_image = tf.reshape(X,[-1,28,28,1])","12839c1f":"convo_1 = convolutional_layer(x_image,shape=[6,6,1,32])\nconvo_1_pooling = maxpool(convo_1)\nconvo_2 = convolutional_layer(convo_1_pooling,shape=[6,6,32,64])\nconvo_2_pooling = maxpool(convo_2)\nconvo_2_flat = tf.reshape(convo_2_pooling,[-1,7*7*64])\nfull_layer_one = tf.nn.relu(normal_layer(convo_2_flat,256))\nhold_prob = tf.placeholder(tf.float32)\nfull_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)","80040681":"y_pred = normal_layer(full_one_dropout,10)","ec96d98a":"cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=y_pred))\noptimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cross_entropy)","b79acecc":"init = tf.global_variables_initializer()\nsteps = 20\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(steps):\n        \n        sess.run(optimizer,feed_dict={X:image_data,y:one_hot_labels,hold_prob:0.5})\n        \n        # PRINT OUT A MESSAGE EVERY 100 STEPS\n        if i%2 == 0:\n            \n            print('Currently on step {}'.format(i))\n            print('Accuracy is:')\n            # Test the Train Model\n            matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y,1))\n            acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n\n            #print(sess.run(acc,feed_dict={X:test_images,y:mnist.test.labels,hold_prob:1.0}))\n            #print('\\n')\n            print(sess.run(acc , feed_dict={X:test_data, y:one_hot_test_labels , hold_prob:1}))","ad0b2605":"### Visualizing  the data","0bfb9c21":"# CNN using MNIST dataset","3f805fb0":"### Loss fucntion and Optimizer","87d1ace5":"### defining convolution2d","02ee3e68":"### Create the Model","675ae436":"### Loading data","74c2453e":"### Defining CNN Model","2b369ab6":"### Loading all the dependency","068f8177":"### defining max pooling layer","b88b40ee":"#### Thanks a lot. **@tylerneylon**\nI got the following function to read \"idx\" files from [**here**](https:\/\/gist.github.com\/tylerneylon\/ce60e8a06e7506ac45788443f7269e40))","4a4ff98b":"### Bias initialisation","710811f4":"### defining place holders","80722ce0":"### weight initialisation","9c62dd99":"### Output layer ( prediction layer )"}}