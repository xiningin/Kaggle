{"cell_type":{"6ed4a5f2":"code","0ec964af":"code","dce06d4b":"code","eaec4b7e":"code","02909a77":"code","687dff91":"code","654809b4":"code","32faa53f":"code","e6e9138c":"code","2c957aa6":"code","96670a06":"code","c70bbddd":"code","0ab3f4e3":"code","fd95b1f4":"code","673e2a09":"code","5097fb64":"code","a35c17ff":"code","59dcb903":"code","a8b48f77":"code","4ece5887":"code","da980b68":"code","056ffeb1":"code","33e6ae7c":"code","a49d8105":"code","7aaa87e9":"code","043f863c":"code","c85cc672":"code","5adb69fd":"code","cfd7811c":"code","994e8ee3":"code","029274fd":"code","bbeb867a":"code","aebe73d6":"code","d1d2759c":"code","821886ae":"code","7bb6b82e":"code","deaa344f":"markdown","eb812eeb":"markdown","56944ebd":"markdown","25499a9e":"markdown","89fa838d":"markdown","9c5b696b":"markdown","330c7ccf":"markdown","24d113a1":"markdown","7f2a7dbd":"markdown","66636403":"markdown","4c9da106":"markdown","9f8e121f":"markdown","b2ad98e1":"markdown","37e3b72f":"markdown","42c23140":"markdown","60cbd175":"markdown","5c6d275d":"markdown","41fe9cc1":"markdown","cbfde35b":"markdown","71ec862e":"markdown","7be179aa":"markdown","a3cacde6":"markdown","6fedfec0":"markdown","3f0e8cab":"markdown","8c35d41d":"markdown","d1668317":"markdown","2612964c":"markdown","bcc518f1":"markdown","d5eaba96":"markdown","65c50b9c":"markdown","727d0cda":"markdown","2eb99c97":"markdown","d02ff9b4":"markdown","134128ed":"markdown","e6a65b25":"markdown","c07d2e51":"markdown","fd97ef91":"markdown","9df889c7":"markdown","79a0b504":"markdown"},"source":{"6ed4a5f2":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Handle table-like data and matrices\nimport numpy as np\nimport pandas as pd\n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n\n# Modelling Helpers\n# from sklearn.preprocessing import Imputer , Normalizer , scale\n# from sklearn.cross_validation import train_test_split , StratifiedKFold\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\npylab.rcParams[ 'figure.figsize' ] = 8 , 6\n","0ec964af":"def plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n        ax.set_xticklabels( [] , visible=False )\n        ax.set_yticklabels( [] , visible=False )\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = titanic.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels\n\ndef plot_variable_importance( X , y ):\n    tree = DecisionTreeClassifier( random_state = 99 )\n    tree.fit( X , y )\n    plot_model_var_imp( tree , X , y )\n    \ndef plot_model_var_imp( model , X , y ):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print (model.score( X , y ))\n    ","dce06d4b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","eaec4b7e":"# get titanic & test csv files as a DataFrame\ntrain = pd.read_csv(\"..\/input\/\/titanic\/train.csv\")\ntest    = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nfull = train.append( test , ignore_index = True )\ntitanic = full[ :891 ]\n\ndel train , test\n\nprint ('Datasets:' , 'full:' , full.shape , 'titanic:' , titanic.shape)","02909a77":"# Run the code to see the variables, then read the variable description below to understand them.\ntitanic.head()","687dff91":"titanic.describe()","654809b4":"plot_correlation_map( titanic )","32faa53f":"# Plot distributions of Age of passangers who survived or did not survive\nplot_distribution( titanic , var = 'Age' , target = 'Survived' , row = 'Sex' )","e6e9138c":"# Excersise 1\n# Plot distributions of Fare of passangers who survived or did not survive\n","2c957aa6":"# Plot survival rate by Embarked\nplot_categories( titanic , cat = 'Embarked' , target = 'Survived' )","96670a06":"# Excersise 2\n# Plot survival rate by Sex\n","c70bbddd":"# Excersise 3\n# Plot survival rate by Pclass\n","0ab3f4e3":"# Excersise 4\n# Plot survival rate by SibSp\n","fd95b1f4":"# Excersise 5\n# Plot survival rate by Parch\n","673e2a09":"# Transform Sex into binary values 0 and 1\nsex = pd.Series( np.where( full.Sex == 'male' , 1 , 0 ) , name = 'Sex' )","5097fb64":"# Create a new variable for every unique value of Embarked\nembarked = pd.get_dummies( full.Embarked , prefix='Embarked' )\nembarked.head()","a35c17ff":"# Create a new variable for every unique value of Embarked\npclass = pd.get_dummies( full.Pclass , prefix='Pclass' )\npclass.head()","59dcb903":"# Create dataset\nimputed = pd.DataFrame()\n\n# Fill missing values of Age with the average of Age (mean)\nimputed[ 'Age' ] = full.Age.fillna( full.Age.mean() )\n\n# Fill missing values of Fare with the average of Fare (mean)\nimputed[ 'Fare' ] = full.Fare.fillna( full.Fare.mean() )\n\nimputed.head()","a8b48f77":"title = pd.DataFrame()\n# we extract the title from each name\ntitle[ 'Title' ] = full[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n\n# a map of more aggregated titles\nTitle_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n\n                    }\n\n# we map each title\ntitle[ 'Title' ] = title.Title.map( Title_Dictionary )\ntitle = pd.get_dummies( title.Title )\n#title = pd.concat( [ title , titles_dummies ] , axis = 1 )\n\ntitle.head()","4ece5887":"cabin = pd.DataFrame()\n\n# replacing missing cabins with U (for Uknown)\ncabin[ 'Cabin' ] = full.Cabin.fillna( 'U' )\n\n# mapping each Cabin value with the cabin letter\ncabin[ 'Cabin' ] = cabin[ 'Cabin' ].map( lambda c : c[0] )\n\n# dummy encoding ...\ncabin = pd.get_dummies( cabin['Cabin'] , prefix = 'Cabin' )\n\ncabin.head()","da980b68":"# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\ndef cleanTicket( ticket ):\n    ticket = ticket.replace( '.' , '' )\n    ticket = ticket.replace( '\/' , '' )\n    ticket = ticket.split()\n    ticket = map( lambda t : t.strip() , ticket )\n    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n    if len( ticket ) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\nticket = pd.DataFrame()\n\n# Extracting dummy variables from tickets:\nticket[ 'Ticket' ] = full[ 'Ticket' ].map( cleanTicket )\nticket = pd.get_dummies( ticket[ 'Ticket' ] , prefix = 'Ticket' )\n\nticket.shape\nticket.head()","056ffeb1":"family = pd.DataFrame()\n\n# introducing a new feature : the size of families (including the passenger)\nfamily[ 'FamilySize' ] = full[ 'Parch' ] + full[ 'SibSp' ] + 1\n\n# introducing other features based on the family size\nfamily[ 'Family_Single' ] = family[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\nfamily[ 'Family_Small' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\nfamily[ 'Family_Large' ]  = family[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n\nfamily.head()","33e6ae7c":"# Select which features\/variables to include in the dataset from the list below:\n# imputed , embarked , pclass , sex , family , cabin , ticket\n\nfull_X = pd.concat( [ imputed , embarked , cabin , sex ] , axis=1 )\nfull_X.head()","a49d8105":"# Create all datasets that are necessary to train, validate and test models\ntrain_valid_X = full_X[ 0:891 ]\ntrain_valid_y = titanic.Survived\ntest_X = full_X[ 891: ]\ntrain_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )\n\nprint (full_X.shape , train_X.shape , valid_X.shape , train_y.shape , valid_y.shape , test_X.shape)","7aaa87e9":"plot_variable_importance(train_X, train_y)","043f863c":"model = RandomForestClassifier(n_estimators=100)","c85cc672":"model = SVC()","5adb69fd":"model = GradientBoostingClassifier()","cfd7811c":"model = KNeighborsClassifier(n_neighbors = 3)","994e8ee3":"model = GaussianNB()","029274fd":"model = LogisticRegression()","bbeb867a":"model.fit( train_X , train_y )","aebe73d6":"# Score the model\nprint (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))","d1d2759c":"#plot_model_var_imp(model, train_X, train_y)","821886ae":"rfecv = RFECV( estimator = model , step = 1 , cv = StratifiedKFold( train_y , 2 ) , scoring = 'accuracy' )\nrfecv.fit( train_X , train_y )\n\n#print (rfecv.score( train_X , train_y ) , rfecv.score( valid_X , valid_y ))\n#print( \"Optimal number of features : %d\" % rfecv.n_features_ )\n\n# Plot number of features VS. cross-validation scores\n#plt.figure()\n#plt.xlabel( \"Number of features selected\" )\n#plt.ylabel( \"Cross validation score (nb of correct classifications)\" )\n#plt.plot( range( 1 , len( rfecv.grid_scores_ ) + 1 ) , rfecv.grid_scores_ )\n#plt.show()","7bb6b82e":"test_Y = model.predict( test_X )\npassenger_id = full[891:].PassengerId\ntest = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\ntest.shape\ntest.head()\ntest.to_csv( 'titanic_pred.csv' , index = False )","deaa344f":"### 4.1.4 K-nearest neighbors\nTry a k-nearest neighbors model by running the cell below. ","eb812eeb":"### 2.4.3 Let's further explore the relationship between the features and survival of passengers \nWe start by looking at the relationship between age and survival.\n\n*Select the cell below and run it by pressing the play button.*","56944ebd":"### 2.4.4 Excersise 2 - 5: Investigating categorical variables\nEven more coding practice! Try to plot the survival rate of Sex, Pclass, SibSp and Parch below. \n\n*Hint: use the code from the previous cell as a starting point.*\n\nAfter considering these graphs, which variables do you expect to be good predictors of survival? ","25499a9e":"# 5. Evaluation\nNow we are going to evaluate model performance and the feature importance.\n\n## 5.1 Model performance\nWe can evaluate the accuracy of the model by using the validation set where we know the actual outcome. This data set have not been used for training the model, so it's completely new to the model. \n\nWe then compare this accuracy score with the accuracy when using the model on the training data. If the difference between these are significant this is an indication of overfitting. We try to avoid this because it means the model will not generalize well to new data and is expected to perform poorly.\n\n*Select the cell below and run it by pressing the play button.*","89fa838d":"# 1. Business Understanding\n\n## 1.1 Objective\nPredict survival on the Titanic\n\n## 1.2 Description\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n**Before going further, what do you think is the most important reasons passangers survived the Titanic sinking?**\n\n[Description from Kaggle](https:\/\/www.kaggle.com\/c\/titanic)","9c5b696b":"**VARIABLE DESCRIPTIONS:**\n\nWe've got a sense of our variables, their class type, and the first few observations of each. We know we're working with 1309 observations of 12 variables. To make things a bit more explicit since a couple of the variable names aren't 100% illuminating, here's what we've got to deal with:\n\n\n**Variable Description**\n\n - Survived: Survived (1) or died (0)\n - Pclass: Passenger's class\n - Name: Passenger's name\n - Sex: Passenger's sex\n - Age: Passenger's age\n - SibSp: Number of siblings\/spouses aboard\n - Parch: Number of parents\/children aboard\n - Ticket: Ticket number\n - Fare: Fare\n - Cabin: Cabin\n - Embarked: Port of embarkation\n\n[More information on the Kaggle site](https:\/\/www.kaggle.com\/c\/titanic\/data)","330c7ccf":"### 2.4.2 A heat map of correlation may give us a understanding of which variables are important\n*Select the cell below and run it by pressing the play button.*","24d113a1":"# 6. Deployment\n\nDeployment in this context means publishing the resulting prediction from the model to the Kaggle leaderboard. To do this do the following:\n\n 1. select the cell below and run it by pressing the play button.\n 2. Press the `Publish` button in top right corner.\n 3. Select `Output` on the notebook menubar\n 4. Select the result dataset and press `Submit to Competition` button","7f2a7dbd":"### 4.1.5 Gaussian Naive Bayes\nTry a Gaussian Naive Bayes model by running the cell below. ","66636403":"### 2.4.4 Embarked\nWe can also look at categorical variables like Embarked and their relationship with survival.\n\n- C = Cherbourg  \n- Q = Queenstown\n- S = Southampton","4c9da106":"### 5.2.1 Automagic\nIt's also possible to automatically select the optimal number of features and visualize this. This is uncommented and can be tried in the competition part of the tutorial.\n\n*Select the cell below and run it by pressing the play button.*","9f8e121f":"### 3.3.4 Create family size and category for family size\nThe two variables *Parch* and *SibSp* are used to create the famiy size variable\n\n*Select the cell below and run it by pressing the play button.*","b2ad98e1":"# 4. Modeling\nWe will now select a model we would like to try then use the training dataset to train this model and thereby check the performance of the model using the test set. \n\n## 4.1 Model Selection\nThen there are several options to choose from when it comes to models. A good starting point is logisic regression. \n\n**Select ONLY the model you would like to try below and run the corresponding cell by pressing the play button.**","37e3b72f":"## 5.3 Competition time!\nIt's now time for you to get your hands even dirtier and go at it all by yourself in a `challenge`! \n\n1. Try to the other models in step 4.1 and compare their result\n    * Do this by uncommenting the code and running the cell you want to try\n2. Try adding new features in step 3.4.1\n    * Do this by adding them in to the function in the feature section.\n\n\n**The winner is the one to get the highest scoring model for the validation set**","42c23140":"## 3.2 Fill missing values in variables\nMost machine learning alghorims require all variables to have values in order to use it for training the model. The simplest method is to fill missing values with the average of the variable across all observations in the training set.\n\n*Select the cells below and run it by pressing the play button.*","60cbd175":"## 2.2 Setup helper Functions\nThere is no need to understand this code. Just run it to simplify the code later in the tutorial.\n\n*Simply run the cell below by selecting it and pressing the play button.*","5c6d275d":"### 3.4.1 Variable selection\nSelect which features\/variables to inculde in the dataset from the list below:\n\n - imputed \n - embarked\n - pclass\n - sex\n - family\n - cabin\n - ticket\n\n*Include the variables you would like to use in the function below seperated by comma, then run the cell*","41fe9cc1":"### 2.4.1 Next have a look at some key information about the variables\nAn numeric variable is one with values of integers or real numbers while a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, such as blood type.\n\nNotice especially what type of variable each is, how many observations there are and some of the variable values.\n\nAn interesting observation could for example be the minimum age 0.42, do you know why this is?\n\n*Select the cell below and run it by pressing the play button.*","cbfde35b":"### 3.4.3 Feature importance\nSelecting the optimal features in the model is important. \nWe will now try to evaluate what the most important variables are for the model to make the prediction.\n\n*Select the cell below and run it by pressing the play button.*","71ec862e":"## 2.4 Statistical summaries and visualisations\n\nTo understand the data we are now going to consider some key facts about various variables including their relationship with the target variable, i.e. survival.\n\nWe start by looking at a few lines of the data\n\n*Select the cell below and run it by pressing the play button.*","7be179aa":"### 2.4.3 Excersise 1: Investigating numeric variables\nIt's time to get your hands dirty and do some coding! Try to plot the distributions of Fare of passangers who survived or did not survive. Then consider if this could be a good predictive variable.\n\n*Hint: use the code from the previous cell as a starting point.*","a3cacde6":"### 4.1.6 Logistic Regression\nTry a Logistic Regression model by running the cell below. ","6fedfec0":"Consider the graphs above. Differences between survival for different values is what will be used to separate the target variable (survival in this case) in the model. If the two lines had been about the same, then it would not have been a good variable for our predictive model. \n\nConsider some key questions such as; what age does males\/females have a higher or lower probability of survival? ","3f0e8cab":"# 3. Data Preparation","8c35d41d":"# **Surviving the Titanic**\n\n\n*[Based on the Titanic competition on Kaggle](https:\/\/www.kaggle.com\/c\/titanic)*\n\n*by Helge Bjorland & Stian Eide*\n\n*January 2017*\n\n---\n\n## Content\n\n\n 1. Business Understanding (5 min)\n     * Objective\n     * Description\n 2. Data Understanding (15 min)\n    * Import Libraries\n    * Load data\n    * Statistical summaries and visualisations\n    * Excersises\n 3. Data Preparation (5 min)\n    * Missing values imputation\n    * Feature Engineering\n 4. Modeling (5 min)\n     * Build the model\n 5. Evaluation (25 min)\n     * Model performance\n     * Feature importance\n     * Who gets the best performing model?\n 6. Deployment  (5 min)\n     * Submit result to Kaggle leaderboard     \n\n[*Adopted from Cross Industry Standard Process for Data Mining (CRISP-DM)*](http:\/\/www.sv-europe.com\/crisp-dm-methodology\/)\n\n![CripsDM](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/220px-CRISP-DM_Process_Diagram.png \"Process diagram showing the relationship between the different phases of CRISP-DM\")","d1668317":"### 4.1.2 Support Vector Machines\nTry a Support Vector Machines model by running the cell below. ","2612964c":"## 3.3 Feature Engineering &ndash; Creating new variables\nCredit: http:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html","bcc518f1":"## 4.2 Train the selected model\nWhen you have selected a dataset with the features you want and a model you would like to try it is now time to train the model. After all our preparation model training is simply done with the one line below.\n\n*Select the cell below and run it by pressing the play button.*","d5eaba96":"### 3.3.2 Extract Cabin category information from the Cabin number\n\n*Select the cell below and run it by pressing the play button.*","65c50b9c":"## 3.4 Assemble final datasets for modelling\n\nSplit dataset by rows into test and train in order to have a holdout set to do model evaluation on. The dataset is also split by columns in a matrix (X) containing the input data and a vector (y) containing the target (or labels).","727d0cda":"## 5.2 Feature importance - selecting the optimal features in the model\nWe will now try to evaluate what the most important variables are for the model to make the prediction. The function below will only work for decision trees, so if that's the model you chose you can uncomment the code below (remove # in the beginning)  and see the feature importance.\n\n*Select the cell below and run it by pressing the play button.*","2eb99c97":"### 4.1.3 Gradient Boosting Classifier\nTry a Gradient Boosting Classifier model by running the cell below. ","d02ff9b4":"## 3.1 Categorical variables need to be transformed to numeric variables\nThe variables *Embarked*, *Pclass* and *Sex* are treated as categorical variables. Some of our model algorithms can only handle numeric values and so we need to create a new variable (dummy variable) for every unique value of the categorical variables.\n\nThis variable will have a value 1 if the row has a particular value and a value 0 if not. *Sex* is a dichotomy (old school gender theory) and will be encoded as one binary variable (0 or 1).\n\n*Select the cells below and run it by pressing the play button.*","134128ed":"### 3.3.3 Extract ticket class from ticket number\n\n*Select the cell below and run it by pressing the play button.*","e6a65b25":"### 4.1.1 Random Forests Model\nTry a random forest model by running the cell below. ","c07d2e51":"### 3.4.2 Create datasets\nBelow we will seperate the data into training and test datasets.\n\n*Select the cell below and run it by pressing the play button.*","fd97ef91":"## 2.3 Load data\nNow that our packages are loaded, let's read in and take a peek at the data.\n\n*Select the cell below and run it by pressing the play button.*","9df889c7":"### 3.3.1 Extract titles from passenger names\nTitles reflect social status and may predict survival probability\n\n*Select the cell below and run it by pressing the play button.*","79a0b504":"# 2. Data Understanding\n\n## 2.1 Import Libraries\nFirst of some preparation. We need to import python libraries containing the necessary functionality we will need. \n\n*Simply run the cell below by selecting it and pressing the play button.*"}}