{"cell_type":{"d6f04767":"code","d368fdd7":"code","ec2ede6c":"code","e20f3cbb":"code","3278a818":"code","83d08a74":"code","1cf5e6c3":"code","aa6ac2ac":"code","fcfa30e6":"code","64068d08":"code","b1955755":"code","c627d989":"code","deb03d55":"code","82f5cbf9":"code","15571bee":"code","0591a7c1":"code","ea1c661a":"code","46d4a341":"code","79485c99":"code","1eecfad1":"code","cea94a2b":"code","43058d6f":"code","14dda070":"code","cc6e0149":"code","9a4a3661":"code","eb6fd995":"code","140eba64":"code","1525c614":"code","369e5ad5":"markdown","279ed430":"markdown","70831098":"markdown","eb0d3dc9":"markdown","b939e9cb":"markdown","89eb7c9d":"markdown","4d4a21aa":"markdown","70cb633d":"markdown","3cadbaa5":"markdown","a5fd4842":"markdown","69e2c42c":"markdown","770bd89c":"markdown","472c9c94":"markdown","cf0511d7":"markdown","7630ee92":"markdown","e39e368f":"markdown","a5c31c68":"markdown","b44ac54a":"markdown","0eb370f8":"markdown","6f642574":"markdown","be87b3f0":"markdown","19949640":"markdown","c7e87ca4":"markdown","1b056b51":"markdown","f8ac9176":"markdown","66f5f0e0":"markdown","115e1880":"markdown"},"source":{"d6f04767":"!pip install git+https:\/\/github.com\/rwightman\/pytorch-image-models\n!pip install --upgrade wandb","d368fdd7":"import os\nimport gc\nimport cv2\nimport copy\nimport time\nimport random\nfrom PIL import Image\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","ec2ede6c":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","e20f3cbb":"ROOT_DIR = \"..\/input\/petfinder-pawpularity-score\"\nTRAIN_DIR = \"..\/input\/petfinder-pawpularity-score\/train\"\nTEST_DIR = \"..\/input\/petfinder-pawpularity-score\/test\"","3278a818":"CONFIG = dict(\n    seed = 42,\n    backbone = 'swin_base_patch4_window7_224',\n    embedder = 'tf_efficientnet_b4_ns',\n    train_batch_size = 16,\n    valid_batch_size = 32,\n    img_size = 448,\n    epochs = 5,\n    learning_rate = 1e-4,\n    scheduler = 'CosineAnnealingLR',\n    min_lr = 1e-6,\n    T_max = 100,\n#     T_0 = 25,\n#     warmup_epochs = 0,\n    weight_decay = 1e-6,\n    n_accumulate = 1,\n    n_fold = 5,\n    num_classes = 1,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n    competition = 'PetFinder',\n    _wandb_kernel = 'deb'\n)","83d08a74":"def set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","1cf5e6c3":"def get_train_file_path(id):\n    return f\"{TRAIN_DIR}\/{id}.jpg\"","aa6ac2ac":"df = pd.read_csv(f\"{ROOT_DIR}\/train.csv\")\ndf['file_path'] = df['Id'].apply(get_train_file_path)","fcfa30e6":"feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]","64068d08":"def create_folds(df, n_s=5, n_grp=None):\n    df['kfold'] = -1\n    \n    if n_grp is None:\n        skf = KFold(n_splits=n_s, random_state=CONFIG['seed'])\n        target = df['Pawpularity']\n    else:\n        skf = StratifiedKFold(n_splits=n_s, shuffle=True, random_state=CONFIG['seed'])\n        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False)\n        target = df.grp\n    \n    for fold_no, (t, v) in enumerate(skf.split(target, target)):\n        df.loc[v, 'kfold'] = fold_no\n\n    df = df.drop('grp', axis=1)\n    \n    return df","b1955755":"df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14)\ndf.head()","c627d989":"class PawpularityDataset(Dataset):\n    def __init__(self, root_dir, df, transforms=None):\n        self.root_dir = root_dir\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.targets = df['Pawpularity'].values\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        target = self.targets[index]\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n            \n        return img, target","deb03d55":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                max_pixel_value=255.0, \n                p=1.0\n            ),\n        ToTensorV2()], p=1.)\n}","82f5cbf9":"class HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n    def __init__(self, backbone, img_size=224, patch_size=1, feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = (img_size, img_size)\n        patch_size = (patch_size, patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # NOTE Most reliable way of determining output dims is to run forward pass\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n                if isinstance(o, (list, tuple)):\n                    o = o[-1]  # last feature if backbone outputs list\/tuple of features\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = (feature_size, feature_size)\n            if hasattr(self.backbone, 'feature_info'):\n                feature_dim = self.backbone.feature_info.channels()[-1]\n            else:\n                feature_dim = self.backbone.num_features\n        assert feature_size[0] % patch_size[0] == 0 and feature_size[1] % patch_size[1] == 0\n        self.grid_size = (feature_size[0] \/\/ patch_size[0], feature_size[1] \/\/ patch_size[1])\n        self.num_patches = self.grid_size[0] * self.grid_size[1]\n        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        if isinstance(x, (list, tuple)):\n            x = x[-1]  # last feature if backbone outputs list\/tuple of features\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x","15571bee":"class PawpularityModel(nn.Module):\n    def __init__(self, backbone, embedder, pretrained=True):\n        super(PawpularityModel, self).__init__()\n        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n        self.embedder = timm.create_model(embedder, features_only=True, out_indices=[2], pretrained=pretrained)\n        self.backbone.patch_embed = HybridEmbed(self.embedder, img_size=CONFIG['img_size'], embed_dim=128)\n        self.n_features = self.backbone.head.in_features\n        self.backbone.reset_classifier(0)\n        self.fc = nn.Linear(self.n_features, CONFIG['num_classes'])\n\n    def forward(self, images):\n        features = self.backbone(images)              # features = (bs, embedding_size)\n        output = self.fc(features)                    # outputs  = (bs, num_classes)\n        return output\n    \nmodel = PawpularityModel(CONFIG['backbone'], CONFIG['embedder'])\nmodel.to(CONFIG['device']);","0591a7c1":"# test\nimg = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\nmodel(img)","ea1c661a":"def criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))","46d4a341":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, targets) in bar:         \n        images = images.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(images)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG['n_accumulate']\n            \n        scaler.scale(loss).backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss","79485c99":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    TARGETS = []\n    PREDS = []\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, (images, targets) in bar:        \n        images = images.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    TARGETS = np.concatenate(TARGETS)\n    PREDS = np.concatenate(PREDS)\n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    \n    return epoch_loss, val_rmse","1eecfad1":"def run_training(model, optimizer, scheduler, device, num_epochs):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, \n                                                         device=CONFIG['device'], \n                                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        history['Valid RMSE'].append(val_epoch_rmse)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        wandb.log({\"Valid RMSE\": val_epoch_rmse})\n        \n        print(f'Valid RMSE: {val_epoch_rmse}')\n        \n        # deep copy the model\n        if val_epoch_rmse <= best_epoch_rmse:\n            print(f\"{c_}Validation Loss Improved ({best_epoch_rmse} ---> {val_epoch_rmse})\")\n            best_epoch_rmse = val_epoch_rmse\n            run.summary[\"Best RMSE\"] = best_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            wandb.save(PATH)\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history","cea94a2b":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","43058d6f":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","14dda070":"train_loader, valid_loader = prepare_loaders(fold=0)","cc6e0149":"optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\nscheduler = fetch_scheduler(optimizer)","9a4a3661":"run = wandb.init(project='Pawpularity', \n                 config=CONFIG,\n                 job_type='Train',\n                 anonymous='must')","eb6fd995":"model, history = run_training(model, optimizer, scheduler,\n                              device=CONFIG['device'],\n                              num_epochs=CONFIG['epochs'])","140eba64":"run.finish()","1525c614":"# Code taken from https:\/\/www.kaggle.com\/ayuraj\/interactive-eda-using-w-b-tables\n\n# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1000, height=720)\niF","369e5ad5":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Required Libraries \ud83d\udcda<\/h1>","279ed430":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from <a href=\"https:\/\/www.kaggle.com\/tolgadincer\/continuous-target-stratification?rvi=1&scriptVersionId=52551118&cellId=6\">this notebook<\/a><\/span>","70831098":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Read the Data \ud83d\udcd6<\/h1>","eb0d3dc9":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Folds<\/h1>","b939e9cb":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Set Seed for Reproducibility<\/h1>","89eb7c9d":"<br>\n<h2 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">PetFinder.my - Pawpularity Training<\/h2>\n<br>","4d4a21aa":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Define Optimizer and Scheduler<\/span>","70cb633d":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\"><a href=\"https:\/\/wandb.ai\/dchanda\/Pawpularity\/runs\/39ro4emr\">View the Complete Dashboard Here \u2b95<\/a><\/span>","3cadbaa5":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Dataset Class<\/h1>","a5fd4842":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Function<\/h1>","69e2c42c":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run Training<\/h1>","770bd89c":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Validation Function<\/h1>","472c9c94":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Start Training<\/span>","cf0511d7":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Create Dataloaders<\/span>","7630ee92":"<img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation<\/strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.<\/span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">\u23f3 Lots of components = Lots of places to go wrong = Lots of time spent debugging<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\">kernel<\/a><\/strong>.<\/span>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","e39e368f":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Notice that after the 1st epoch the model doesn't learn much. Any suggestions to improve training are welcome<\/span>","a5c31c68":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Training Configuration \u2699\ufe0f<\/h1>","b44ac54a":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">This kernel tries to apply the recent winning solution of Google Landmark Recognition and Retrieval by <a href=\"https:\/\/www.kaggle.com\/christofhenkel\">@dieter<\/a> to the current Pawpularity Challenge<\/span><br><span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">Winning Solution can be found <a href=\"https:\/\/www.kaggle.com\/c\/landmark-recognition-2021\/discussion\/277098\">here<\/a><\/span><br><br>\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">This has also been recently shared by <a href=\"https:\/\/www.kaggle.com\/cdeotte\">@cdeotte<\/a> in the following <a href=\"https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/277917\">discussion<\/a><\/span><br><br>\n\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">I tried to reproduce the model referring to the following tweet by <a href=\"https:\/\/www.kaggle.com\/christofhenkel\">@dieter<\/a><\/span><br>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\" style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">exactly. I used your excellent timm library for it<br>backbone = timm.create_model(swin_base_patch4_window7_224)<br>embedder = timm.create_model(tf_efficientnet_b5,out_indices=[2])<br>backbone.patch_embed = HybridEmbed(embedder)<\/p>&mdash; Dieter (@kagglingdieter) <a href=\"https:\/\/twitter.com\/kagglingdieter\/status\/1444521143499689997?ref_src=twsrc%5Etfw\">October 3, 2021<\/a><\/blockquote> <script async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"><\/script><br>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">I haven't got great results yet, the following things can be tried further to improve performance:<\/span><br>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">Better learning rate scheduling<\/span><br>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">First training transformer and CNN separately and then training them jointly as in the winning solution<\/span><br>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">Trying classification instead of regression<\/span><br>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">Try a SVR head as suggested <a href=\"https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/276724\">here<\/a><\/span><br>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.3em; font-weight: 300;\">There is a lot of room for improvement, I hope this helps...<\/span><br>","0eb370f8":"<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.5em; font-weight: 300;\">Code taken from Ross Wightman's timm library <a href=\"https:\/\/github.com\/rwightman\/pytorch-image-models\/blob\/cd34913278f8511ba53492ed186b4e08f890add6\/timm\/models\/vision_transformer_hybrid.py#L100\">here<\/a><\/span>","6f642574":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Augmentations<\/h1>","be87b3f0":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Visualizations<\/h1>","19949640":"![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Petfinder\/PetFinder%20-%20Logo.png)","c7e87ca4":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Introduction<\/h1>","1b056b51":"![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","f8ac9176":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Create Model<\/h1>","66f5f0e0":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Install Required Libraries<\/h1>","115e1880":"# <h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Loss Function<\/h1>"}}