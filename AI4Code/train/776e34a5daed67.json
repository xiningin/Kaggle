{"cell_type":{"a4cf5bd3":"code","e6011b6b":"code","af22ad82":"code","3ebc79f0":"code","ffc090f5":"code","6ea21ca9":"code","896a9357":"code","9d003957":"code","b9badf69":"code","207f76b8":"code","1012aa5e":"code","c77d5b01":"code","13b0a7fd":"code","694ef6de":"code","0399a751":"code","5a8ce564":"code","337014dc":"code","e1fe8bc9":"code","6153baa2":"code","fc7199e3":"code","b3597ae7":"markdown","f5284dc6":"markdown","ca1c14a5":"markdown","5b401752":"markdown","6755206b":"markdown","87db5e2c":"markdown","d772669f":"markdown","3d5eae04":"markdown","c8975859":"markdown","9cff383b":"markdown","06810486":"markdown","051e279c":"markdown","71e70a92":"markdown","c6f5a521":"markdown","bc359d99":"markdown"},"source":{"a4cf5bd3":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.naive_bayes import GaussianNB\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n","e6011b6b":"diabetes_dataset = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes_dataset.head()","af22ad82":"diabetes_dataset.describe()","3ebc79f0":"diabetes_dataset.shape","ffc090f5":"plt.figure(figsize=(8,5))\nsns.countplot(x=diabetes_dataset['Outcome'])\nplt.title('Outcome vs count',fontsize=20)\nplt.xlabel('Outcome',fontsize=15)\nplt.ylabel('Count',fontsize=15);","6ea21ca9":"diabetes_dataset[\"Outcome\"].value_counts()","896a9357":"diabetes_dataset.groupby('Outcome').mean()","9d003957":"X = diabetes_dataset.iloc[:,:-1].values\ny = diabetes_dataset.iloc[:,-1].values","b9badf69":"X","207f76b8":"y","1012aa5e":"scalar = StandardScaler()\nX = scalar.fit_transform(X)\nprint(X)","c77d5b01":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, stratify = y)","13b0a7fd":"X_train.shape\ny_train.shape","694ef6de":"classifier = svm.SVC(C=0.5, kernel='linear')\n#classifier = LogisticRegression()\n#classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski' , p=2)\n#classifier = svm.SVC(kernel = 'rbf')\n#classifier = GaussianNB()\n#classifier = DecisionTreeClassifier(criterion='entropy')\n#classifier = RandomForestClassifier(n_estimators=10,criterion = 'entropy')\nclassifier.fit(X_train,y_train)","0399a751":"y_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_pred,y_test)\naccuracy = accuracy_score(y_pred, y_test)\nprint(cm)\ncf_matrix = cm\nprint(\"Accuracy of the model is:\", accuracy)","5a8ce564":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,X = X_train,y= y_train , cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f}\".format(accuracies.std()*100))","337014dc":"#.  visualizing the confusion matrix!\nsns.heatmap(cf_matrix\/np.sum(cf_matrix), annot=True, \n            fmt='.2%', cmap='Reds')","e1fe8bc9":"#Visualzing. with labels!\nlabels = ['True Neg','False Pos','False Neg','True Pos']\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds')","6153baa2":"from sklearn.model_selection import GridSearchCV\nparameters = [{'C':[0.25,0.5,0.75,1], 'kernel' : ['linear']},\n              {'C':[0.25,0.5,0.75,1], 'kernel' : ['rbf'], 'gamma' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}]\ngrid_search = GridSearchCV(estimator=classifier,\n                          param_grid=parameters,\n                          scoring='accuracy',\n                          cv=10)\ngrid_search.fit(X_train,y_train)\nprint(\"Best Accuracy: {:.2f} %\".format(grid_search.best_score_*100))\nprint(\"Best Parameters: \", grid_search.best_params_)","fc7199e3":"input_data = np.array([1,85,66,29,0,26.6,0.351,31])\ninput_data = input_data.reshape(1,-1)\nstanderdized_input_data = scalar.transform(input_data)\nprediction = classifier.predict(standerdized_input_data)\nif(prediction[0] == 0):\n  print(\"Person is not diabetic\")\nelse:\n  print(\"The Person is diabetic\")","b3597ae7":"Making a predictive system","f5284dc6":"Accuracy comparision of all classifier methods\n\n1.   SVM - using svm we get a accuracy of 78% and as we can see the best hyperparameters to use is c=0.5 and linear kernel\n2.   Logistic Regression - accuracy of 78%\n3.   KNN - accuracy of 74%\n4.   Kernel SVM - accuracy of 77%\n5.   Naive Bayes - accuracy of 74%\n6.   Decision Tree - accuracy of 71%\n7.   Random Forest - accuracy of 78% though random forest has good accuracy it performs too good in train set and is over fitted\n\n","ca1c14a5":"### This project is basically an overview of how we can train a machine learning model on the diabeties data set and predict with the conditions given whether the person is diabetic or not.\n\n\n\n\n","5b401752":"Let's Start!","6755206b":"About the libraries\n\n1.   numpy - for numpy arrays, useful for processing and scientific computing \n2.   pandas - helpful for creating dataframes and storing data\n3.   StandardScalar - in this project we're using support vector machine classification and this class cannot process the data given to it unless the data is standardized. \n4.   svm - the suport vector machine class in the sklearn package\n5.   accuracy_score - to check the accuracy of our model\n6.   train_test_split - to split the data into training and test set\n\n\n","87db5e2c":"Let's visualize the number of\n\n* 0 --> Not diabetic\n* 1 --> Diabetic","d772669f":"Using K-Fold cross validation to check if we didn't get lucky on the test set!","3d5eae04":"Data Collecting and Analysis\n\n\n*   dataset - PIMA Indians diabetes dataset (Kaggle) \n    this data set contains data about females \n\n","c8975859":"Data Standardization\n","9cff383b":"Model Evaluation\nAccuracy Score","06810486":"Training the model","051e279c":"Now we have to check if there is a better hyperparameter we can tune to improve over all accuracy.\nWe use **Grid Search** here","71e70a92":"Train Test Split","c6f5a521":"## SVM\n### We use svm classification for this project\nSupport vector machine used to classify a data point based on a maximum margin hyperplane created by the supporting points known as support vectors (vectors - as in higher dimension it is tough to call them points, supporting - since these points are enough than all the other data points together as this marks the boundary to. help classify the point) which seperate the data points.","bc359d99":"Importing the libraries"}}