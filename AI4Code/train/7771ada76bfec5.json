{"cell_type":{"1f84f68a":"code","c8acfd4f":"code","2a4fb246":"code","dfd55cbd":"code","033d91d9":"code","e9629284":"code","f5c34310":"code","ae9904e6":"code","61ce1eb2":"code","dea7a291":"code","e0f27508":"code","28563baa":"code","06c71dea":"code","927de62c":"code","08ede8cc":"code","2986ec43":"code","738188c7":"code","ed3958da":"code","dcf17a6e":"code","ba6feaf3":"code","68155918":"markdown"},"source":{"1f84f68a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8acfd4f":"train = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ntrain.head()","2a4fb246":"train.info()","dfd55cbd":"train.describe()","033d91d9":"#REPLACE ALL ZERO WITH 'NAN'\ntrain_copy = train.copy(deep = True)\ntrain_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = train_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n## showing the count of Nans\nprint(train_copy.isnull().sum())\n","e9629284":"train_copy.hist(figsize=(20,20))","f5c34310":"#REPLACE 'NAN' WITH THE DISTRIBUTION VALUES \ntrain_copy['Glucose'].fillna(train_copy['Glucose'].mean(), inplace = True)\ntrain_copy['BloodPressure'].fillna(train_copy['BloodPressure'].mean(), inplace = True)\ntrain_copy['SkinThickness'].fillna(train_copy['SkinThickness'].median(), inplace = True)\ntrain_copy['Insulin'].fillna(train_copy['Insulin'].median(), inplace = True)\ntrain_copy['BMI'].fillna(train_copy['BMI'].median(), inplace = True)","ae9904e6":"#PLOT AFTER 'NAN' REMOVAL\ntrain_copy.hist(figsize=(20,20))","61ce1eb2":"train_copy.shape","dea7a291":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","e0f27508":"#HEATMAP FOR TRAIN DATA\nplt.figure(figsize=(12,10)) \np=sns.heatmap(train_copy.corr(), annot=True,cmap ='RdYlGn')","28563baa":"#SCALING THE DATA\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(train_copy.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])","06c71dea":"X.head()","927de62c":"y = train_copy.Outcome","08ede8cc":"#BUILDING LOGISTIC REGRESSION MODEL\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = logreg.predict(X_test)\n\n# Compute and print the confusion matrix and classification report\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","2986ec43":"#ROC CURVE\nfrom sklearn.metrics import roc_curve\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","738188c7":"# Import necessary modules\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# Compute predicted probabilities: y_pred_prob\ny_pred_prob = logreg.predict_proba(X_test)[:,1]\n\n# Compute and print AUC score\nprint(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n\n# Compute cross-validated AUC scores: cv_auc\ncv_auc = cross_val_score(logreg, X, y, cv=5,\nscoring='roc_auc')\n\n# Print list of AUC scores\nprint(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))\n","ed3958da":"#HYPER PARAMETER TUNING WITH GridSearchCV\n# Import necessary modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Setup the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space}\n\n# Instantiate a logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the data\nlogreg_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \nprint(\"Best score is {}\".format(logreg_cv.best_score_))","dcf17a6e":"#HYPER PARAMETER TUNING WITH RandomizedSearchCV\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X, y)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","ba6feaf3":"#Hold-out set in practice I: Classification\n# Import necessary modules\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create the hyperparameter grid\nc_space = np.logspace(-5, 8, 15)\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2']}\n\n# Instantiate the logistic regression classifier: logreg\nlogreg = LogisticRegression()\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# Instantiate the GridSearchCV object: logreg_cv\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=5)\n\n# Fit it to the training data\nlogreg_cv.fit(X_train, y_train)\n\n# Print the optimal parameters and best score\nprint(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\n","68155918":"1. Tuned Logistic Regression Parameter: {'C': 0.05179474679231213, 'penalty': 'l2'}\n2. Tuned Logistic Regression Accuracy: 0.7695652173913043"}}