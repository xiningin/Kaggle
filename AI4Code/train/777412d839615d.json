{"cell_type":{"247db918":"code","f45c87b8":"code","50753f31":"code","ae4201d5":"code","6326e866":"code","af8a53da":"markdown"},"source":{"247db918":"!pip install -U pip setuptools wheel\n!pip install -U spacy","f45c87b8":"#globals \ntrain_texts, valid, oof = None, None, None","50753f31":"import os, pandas as pd, numpy as np\nimport spacy\nimport re\n\nfrom nltk.corpus import brown\nfrom nltk import sent_tokenize, word_tokenize, pos_tag\nfrom nltk import pos_tag_sents\n\n#pretty handy\n#https:\/\/stackoverflow.com\/questions\/38783027\/jupyter-notebook-display-two-pandas-tables-side-by-side\ndef display_side_by_side(*args):\n    tds = \"text-align:left; border-left: 1px solid #000;border-right: 1px solid #000;\"\n    html_str=''\n    wd = int(100\/len(args))\n    for df,title in args:\n        html_str+=f'<td style=\"{tds}min-width: {wd}%;width: {wd}%;\">'\n        html_str+=f'<b>{title}<\/b><br>{df}'\n        html_str+='<\/td>'\n    #print(html_str)\n    display_html(\"<table style='border: 1px solid black;'>\"+html_str+\"<\/table>\",raw=True)\n    \n    \n\n#stolen from https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533 \ndef init(validcsv='..\/input\/tfbasic\/valid.csv', oofcsv='..\/input\/tfbasic\/oof.csv', feedbackdir=\"..\/input\/feedback-prize-2021\/train\"):\n    global train_texts, valid, oof\n    train_names, train_texts = [], []\n    print(\"Loading train texts..\", end = \" \")\n    for f in list(os.listdir(feedbackdir)):\n        train_names.append(f.replace('.txt', ''))\n        train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\n    train_texts = pd.DataFrame({'id': train_names, 'text': train_texts})    \n    print(\".. done.\")\n    print(\"Loading valid and oof..\", end = \" \")\n    train_texts.head()\n    #import dataset with valid (subset of train, holdout for CV) and oof (predictions for valid set)\n    valid = pd.read_csv('..\/input\/tfbasic\/valid.csv')\n    oof = pd.read_csv('..\/input\/tfbasic\/oof.csv')\n    print(\".. done.\")\n    oof['oof_discourse_id'] = range(len(oof)) #i find this more clear than index\n    oof[\"pred_len\"] = oof[\"predictionstring\"].apply(lambda x: len(x.split()))\n    valid[\"pred_len\"] = valid[\"predictionstring\"].apply(lambda x: len(x.split()))\n    oof[\"start\"] = oof[\"predictionstring\"].apply(lambda x: int(x.split()[0]))\n    oof[\"end\"] = oof[\"predictionstring\"].apply(lambda x: int(x.split()[-1]))\n    valid[\"start\"] = valid[\"predictionstring\"].apply(lambda x: int(x.split()[0]))\n    valid[\"end\"] = valid[\"predictionstring\"].apply(lambda x: int(x.split()[-1]))\n    valid[\"pred_len\"] = valid[\"predictionstring\"].apply(lambda x: len(x.split()))\n    print(f\"Train Texts Shape {np.shape(train_texts)}  OOF Shape {np.shape(oof)} Valid Shape {np.shape(valid)} \")\n    return oof, valid, train_texts\n\n                        \nimport matplotlib.pyplot as plt\nimport matplotlib\n\nimport io\nimport base64\n\ndef fig_to_html(fig):\n    img = io.BytesIO()\n    fig.savefig(img, format='png',\n                bbox_inches='tight')\n    img.seek(0)\n    plt.close(fig)\n    base = base64.b64encode(img.getvalue())\n    encoded = fig_to_base64(fig)\n    my_html = '<img src=\"data:image\/png;base64, {}\">'.format(encoded.decode('utf-8'))\n    return my_html\n\n\ndef show_lengths(oof, valid):\n    def get_html(train, dtn):\n        #print(\"get_html:\", train.keys())\n        df = train.groupby(dtn).mean()[['pred_len', 'start', 'end']]\n        #display(df)\n        df.insert(0, \"count\", train.groupby(dtn).count()['id'])\n        df.insert(0, \"perc\", df['count']\/len(train))\n        #print(df.keys())\n        df.loc['Column_TotalOrMean'] = df.sum(numeric_only=True, axis=0)\n        df.loc['Column_TotalOrMean']['pred_len'] = df['pred_len'].mean()\n        df.loc['Column_TotalOrMean']['start'] = df['start'].mean()\n        df.loc['Column_TotalOrMean']['end'] = df['end'].mean()\n        return df.to_html()\n    display_side_by_side((get_html(oof, \"class\"), \"oof class perc, counts and avg lengths \"), (get_html(valid, \"discourse_type\"), \"valid type perc, counts and avg lengths\"))\n\n#just used for visualization, should be mostly accurate.  let me know if you see an error or improvement.\n#tricky going from pi -> discourse start\/end because of regex limitations\ndef ps_to_pos(txt, ts, ps):  \n    tl = 0\n    for i in range(ps):\n        tl = tl + len(ts[i])\n    tl = tl + ps - 4\n    if tl < 0:\n        tl = 0\n    words_re = re.compile(re.escape(ts[ps]))\n    sub = txt[tl:]\n    #print(piv, tl, txt, sub, words_re)\n    #print('----')\n    span = words_re.search(sub).span()\n    return span[0]+tl, span[1]+tl\n\n\ndef get_span(txt, piv):\n    pi = [int(x) for x in piv.split()]\n    ts = txt.split()\n    ss = ps_to_pos(txt, ts, pi[0])    \n    se = ps_to_pos(txt, ts, pi[-1])    \n    return (ss[0],se[1])\n\ndef gen_spacy(idv, oof_filterlist = None, valid_filterlist = None):\n    global valid, oof\n    print(idv)\n    txt = train_texts.query(f\"id == '{idv}'\").iloc[0]['text']\n    t = valid.query(f\"id == '{idv}'\")    \n    ents = []\n    for i,r in t.iterrows():\n        if valid_filterlist is not None and not r['discourse_id'] in valid_filterlist:\n            continue\n        s = int(r['discourse_start'])\n        e = int(r['discourse_end'])\n        l = r['discourse_type']\n        ents.append({\"start\": s, \"end\": e, \"label\": l})\n    tret = { \"text\": txt,\"ents\": ents,\"title\": None}\n    o = oof.query(f\"id == '{idv}'\")\n    ts = txt.split()\n    ents = []\n    for i,r in o.iterrows():  \n        if oof_filterlist is not None and not r['oof_discourse_id'] in oof_filterlist:\n            continue\n        s,e = get_span(txt, r['predictionstring'])\n        l = r['class']\n        ents.append({\"start\": s, \"end\": e, \"label\": l})\n    oret = { \"text\": txt,\"ents\": ents,\"title\": None}\n    return tret, oret\n\n\nfrom IPython.display import display_html\nfrom itertools import chain,cycle\n\n    \ndef get_id_from_dis(lst, comp, idc = 'pred_id'):\n    #print(lst[0:10])\n    df = pd.DataFrame(np.column_stack([lst]), columns = [idc])\n    mrg = pd.merge(comp, df, on=idc)['id'].value_counts().reset_index()\n    #print(mrg[0:10])\n    mrg.columns = ['id', 'value_count']\n    return mrg\n\n# CODE FROM : Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n#from https:\/\/www.kaggle.com\/cdeotte\/tensorflow-longformer-ner-cv-0-633, with mods\ndef score_feedback_comp(pred_df, gt_df, output = False):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_id', 'discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df['discourse_id']\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    if output:\n        print(\"joined\")\n        display(joined)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()[['gt_id','pred_id']].values\n    if output:\n        print(\"tp_pred_ids\")\n        display(tp_pred_ids)\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids[:,1:]]\n    if output:\n        print(\"fp_pred_ids\")\n        display(fp_pred_ids)\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n    if output:\n        print(\"unmatched_gt_ids\")\n        display(unmatched_gt_ids)\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP \/ (TP + 0.5*(FP+FN))\n    print(my_f1_score)\n    score = (my_f1_score, \n             tp_pred_ids, get_id_from_dis(tp_pred_ids[:,1:], pred_df), \n             fp_pred_ids, get_id_from_dis(fp_pred_ids, pred_df), \n             unmatched_gt_ids, get_id_from_dis(unmatched_gt_ids, gt_df,'discourse_id')\n            )\n\n    #print(score[IF1])\n\n    display_side_by_side((score[TPID].describe().to_html(), \"True Positive ID Counts\"), \n                         (score[FPID].describe().to_html(), \"False Positive ID Counts\"),  \n                         (score[UNID].describe().to_html(), \"Unmatched ID Counts\"))\n    return score\n\nimport random\ndef color_generator(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(number_of_colors)]\n    return color\n\n#cols = color_generator(10)\n#_ = [print(i,cols[i]) for i in range(10)]\n\ndef display_txt(idvs,oof_filterlist = None, valid_filterlist = None ):\n    for idv in idvs:\n        spans = gen_spacy(idv, oof_filterlist, valid_filterlist)\n        #print(spans[1], spans[0])\n        options = {\"ents\": ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n           'Counterclaim', 'Rebuttal'],\n               \"colors\": {\"Lead\": \"#A7D8FE\", 'Position': \"#D2B4DE\", 'Evidence': \"#D7BDE2\", 'Claim': \"#F5B041\", \n                          'Concluding Statement': \"#E88049\",'Counterclaim': \"#7FB3D5\", 'Rebuttal': \"#82E0AA\"}}\n        r1 = spacy.displacy.render(spans[1], options = options, manual=True, jupyter = False, style=\"ent\")\n        r2 = spacy.displacy.render(spans[0],options = options, manual=True, jupyter = False, style=\"ent\")\n        display_side_by_side((r1,\"oof\"), (r2, \"valid\"))\n\n#used to index into score\nIF1=0\nTPDI=1 #tp (discourse_id, oof_discourse_id)\nTPID=2 #tp (text id, value count) sorted by value counts descending\nFPDI=3 #fp (oof_discourse_id)\nFPID=4 #fp (text id, value count) sorted by value counts descending\nUNDI=5 #unmatched (discourse_id)\nUNID=6 #unmatched (text id, value count) sorted by value counts descending\n","ae4201d5":"#valid.csv is small (10%?) subset of train.csv that wasn't used for training and held out for cross validation\n#oof.csv is \"Out Of Fold\" predictions from the model on valid -> id,class, predictionstring\n#the goal of this notebook is to understanding differences between valid and oof to see how the model is succeeding and failing\n#average \n\noof, valid, train_texts = init(validcsv='..\/input\/tfbasic\/valid.csv', oofcsv='..\/input\/tfbasic\/oof.csv', feedbackdir=\"..\/input\/feedback-prize-2021\/train\")\n\n#I don't include CV score here, assuming this is already done elsewhere\nscore = score_feedback_comp(oof, valid, output = False)\n\n","6326e866":"#print(oof.keys(), valid.keys())\n\nshow_lengths(oof, valid)\n\nprint(\"Top 3 False Positives\")\n#FP\nidvs = score[FPID][0:3]['id']\ndisplay_txt(idvs, oof_filterlist = score[FPDI])\nprint(\"\\n\\n\\n\\n\\n\\nTop 3 Unmatched\")\n#unmatched\nidvs = score[UNID][0:10]['id']\ndisplay_txt(idvs, valid_filterlist = score[UNDI])\nprint(\"\\n\\n\\n\\n\\n\\nTop 3 True Positive\")\n#unmatched\nidvs = score[TPID][0:3]['id']\ndisplay_txt(idvs, oof_filterlist = score[TPDI])","af8a53da":"See threads here - \nhttps:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/299169, regarding some observations\nhttps:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/296879, initial impetus, including linked papers.\n\nThanks to @cdeotte for a lot of his contributions as well. "}}