{"cell_type":{"b5a293e2":"code","0ff0130a":"code","5a4ff44e":"code","6a1c0abb":"code","d1bb4798":"code","16c8bc5b":"code","9884fa63":"code","41195c8b":"code","0d7cc8fb":"code","97e8df74":"code","32818d67":"code","2f450c30":"code","3ec40a39":"code","eca955eb":"code","5a7a77aa":"code","9d55cd17":"code","122c9d6c":"code","51386d3e":"code","3545da2a":"code","0bec0100":"code","2e8c5caf":"code","3698e669":"code","754f39d9":"code","573db048":"code","e50a2312":"code","e992b59f":"code","994055b7":"code","3e0ffc68":"code","fa837f04":"code","cfcc7119":"code","42b70208":"code","2f4f22fb":"code","14a13f86":"code","cd0ba856":"code","c7db10c4":"code","605fead3":"code","43a61c70":"code","7341d46b":"code","80b5dea4":"code","eb9b26d0":"code","e0e71118":"code","04c7ee26":"code","0d3bfc71":"code","a937b0b0":"code","5262c270":"code","fbead94b":"code","0843ce7a":"code","b81493b4":"code","27120a55":"code","af02ce61":"code","e9721e2a":"code","65d188db":"code","88829043":"code","f9e635c9":"code","94a588e8":"code","a4fbcfc3":"code","874d4220":"code","ef5e50f5":"code","cd1a92dd":"code","67f5c435":"code","de7a40d4":"code","4c713f78":"code","68a7ac4a":"code","36c6c4b3":"code","4712d80e":"code","a76abbc8":"code","6cc1ca1d":"code","599d23d8":"code","338b8d2c":"markdown","47bac90b":"markdown","6709d3ee":"markdown","b46acf7e":"markdown","37732ca5":"markdown","11d26747":"markdown","c9dbcfe5":"markdown","3b5872e8":"markdown","4455f1d5":"markdown","8582cba3":"markdown","6fb670bf":"markdown","9b7e373c":"markdown","17b0b710":"markdown","80d81ff7":"markdown","26171e59":"markdown","0e2cf6b3":"markdown","131c9e50":"markdown","69beb87f":"markdown","777d4f39":"markdown","a6b82d2a":"markdown","a25368f5":"markdown","c8eae2ba":"markdown","6da19c7e":"markdown","c2236d1b":"markdown"},"source":{"b5a293e2":"!pip install --upgrade pip\n!pip install -U scikit-learn","0ff0130a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined b\n\n# the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a4ff44e":"import sklearn\nprint(sklearn.__version__)","6a1c0abb":"#Scraper Imports\nimport requests\nfrom bs4 import BeautifulSoup\nimport re, sys","d1bb4798":"#Pre-Processing Imports\nfrom datetime import datetime, timedelta\nfrom dateutil.relativedelta import relativedelta\nimport matplotlib.pyplot as plt\n\n!pip install contractions\nimport contractions","16c8bc5b":"#Feature Extraction Imports\n\n#Sentiment\nfrom textblob import TextBlob\n\n#POS Tagging\nimport nltk\nnltk.download(\"popular\")\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nfrom collections import Counter\nstop_words = set(stopwords.words('english')) \nimport re\n\n#Text Feature Generation\nimport string","9884fa63":"#Model Training and Evaluation Imports\nfrom time import time\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport pickle\nimport joblib \nfrom sklearn.tree import DecisionTreeClassifier\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold","41195c8b":"#Creating Soup object from URL\ndef getsoup(url):\n    response = requests.get(url, headers={'User-Agent':'Mozilla\/5.0'})\n    Status_Code = response.status_code\n    print(url)\n    print(Status_Code)\n    \n    if Status_Code == 200:\n      soup = BeautifulSoup(response.content, features=\"lxml\")\n    else:\n      soup = getsoup(url)\n    return soup \n\n#Get Last Page number\ndef getLastPageNumber(soup, site):\n    pageNumber = []\n    if site == 'flipkart':\n        review_number = int(soup.find(\"span\", \"_2_R_DZ\").text.strip().replace(',', '').split()[-2])\n        if review_number <=10:\n            lastPage = 1\n        else:\n            link = soup.find(attrs={\"class\": \"_2MImiq _1Qnn1K\"})\n            pageNumber = link.find('span').text.strip().replace(',', '').split()\n            lastPage1 = pageNumber[len(pageNumber)-1]\n            lastPage = int(lastPage1)\n    elif site == 'amazon':\n        review_number = int(soup.find(\"div\", {\"data-hook\": \"cr-filter-info-review-rating-count\"}).text.strip().replace(',', '').split()[-3])\n        if review_number <=10:\n            lastPage = 1\n        else:\n            lastPage = review_number \/\/ 10\n    if lastPage > 500:\n        lastPage = 2\n    return lastPage\n\n# Function to create a list of URLs for all the review pages for a product\n\ndef geturllist(url, lastPage):\n    urllistPages = []\n    url = url[:-1]\n    for i in range(1,lastPage+1):\n      urllistPages.append (url + str(i))\n    return urllistPages","0d7cc8fb":"#Function to extract all the required elements from a product review page\ndef getReviews(soup, site, url):\n    if site == 'flipkart':\n        #Extracting the Titles\n        title_sec = soup.find_all(\"p\",'_2-N8zT')\n        title = []\n        for s in title_sec:\n            title.append(s.text)\n\n        #Extracting the Author names\n        author_sec = soup.find_all(\"p\",\"_2sc7ZR _2V5EHH\")\n        author = []\n        for r in author_sec:\n            author.append(r.text)\n\n        #Extracting the Text\n        Review_text_sec = soup.find_all(\"div\",'t-ZTKy')\n        text = []\n        for t in Review_text_sec:\n            text.append(t.text)\n            \n        #Extracting the Star rating  \n        Rating = soup.find_all(\"div\", {\"class\": [\"_3LWZlK _1BLPMq\", \"_3LWZlK _32lA32 _1BLPMq\", \"_3LWZlK _1rdVr6 _1BLPMq\"]})    \n        rate = []\n        for d in Rating:\n            rate.append(d.text)\n\n        #Extracting the Date\n        Date_sec = soup.find_all(lambda tag: tag.name == 'p' and tag.get('class') == ['_2sc7ZR'])    \n        date = []\n        for d in Date_sec:\n          date.append(d.text)\n\n        #Extracting the Helpful rating\n        help_sec = soup.find_all(lambda tag: tag.name == 'div' and tag.get('class') == ['_1LmwT9'])    \n        help1 = []\n        for d in help_sec:\n          help1.append(d.text)\n    \n    elif site == 'amazon':\n        n_ = 0\n        #Extracting the Titles\n        title_sec = soup.find_all(attrs={\"data-hook\": \"review-title\", \"class\": \"a-size-base a-link-normal review-title a-color-base review-title-content a-text-bold\"})\n        title = []\n        for s in title_sec:\n            title.append(s.text.replace('\\n', ''))\n        n_ = len(title)\n\n        #Extracting the Author names\n        author_sec = soup.find_all(attrs = {\"class\": \"a-profile-name\"})\n        author = []\n        for r in author_sec:\n            author.append(r.text)\n        while(1):\n            if len(author) > n_:\n                author.pop(0)\n            else:\n                break\n\n        #Extracting the Text\n        Review_text_sec = soup.find_all(attrs={\"data-hook\": \"review-body\", \"class\": \"a-size-base review-text review-text-content\"})\n        text = []\n        for t in Review_text_sec:\n            text.append(t.text.replace('\\n', ''))\n\n        #Extracting the Star rating  \n        Rating = soup.find_all(attrs={\"data-hook\": \"review-star-rating\"})    \n        rate = []\n        for d in Rating:\n            rate.append(d.text)\n\n        #Extracting the Date \n        Date_sec = soup.find_all(attrs={\"data-hook\": \"review-date\"})    \n        date = []\n        for d in Date_sec:\n          date.append(d.text) \n\n        #Extracting the Helpful rating \n        help_sec = soup.find_all(attrs={\"data-hook\": \"helpful-vote-statement\"})    \n        help1 = []\n        for d in help_sec:\n             help1.append(d.text.replace('\\n          ', '')) \n        while(1):\n            if len(help1) < n_:\n                help1.append(0)\n            else:\n                break\n        \n    #Adding the url of the first page of the review for reference  \n    url1 = []\n    url1 = [url] * len(date)\n\n    #Creating a DataFrame\n    collate = {'Date': date, 'URL': url1, 'Review_Title': title, 'Author': author, 'Rating': rate, 'Review_text': text, 'Review_helpful': help1}          \n    collate_df = pd.DataFrame.from_dict(collate)\n    return collate_df","97e8df74":"def scraper(url):\n    df2 = []      \n    soup = getsoup(url)\n    site = url.split('.')[1]\n    if site == 'flipkart':\n        url = url + '&page=1'\n    elif site == 'amazon':\n        url = url + '&pageNumber=1'\n    product = url.split('\/')[3]\n    lastPage = getLastPageNumber(soup, site)\n    urllistPages = geturllist(url, lastPage)\n    x = 1\n    for url in urllistPages:\n        soup = getsoup(url)\n        df1 = getReviews(soup, site, url)         \n        if x == 1:\n            df3 = []\n            df3 = df1\n        else:                        \n            df2 = df3\n            result = df2.append(df1, ignore_index=True)\n            df3 = result\n        x += 1\n    df3.to_csv(product + \".csv\")","32818d67":"#d1 = pd.read_csv('..\/input\/reviews\/boat-rockerz-510-super-extra-bass-bluetooth-headset.csv', index_col=0)\nd1 = pd.read_csv('..\/input\/reviews\/Rockerz-510-Wireless-Bluetooth-Headphones.csv', index_col=0)","2f450c30":"d1.head()\n#d1.shape","3ec40a39":"d2.head()\nd2.shape","eca955eb":"print('Dataset1:')\nprint(data.isnull().sum())\nprint('\\nDataset2:')\n#print(d2.isnull().sum())","5a7a77aa":"np.isinf(data).values.sum()\n","9d55cd17":"data.replace([np.inf, -np.inf], np.nan,inplace=True)","122c9d6c":"data.dropna(inplace=True)","51386d3e":"d1['Review_text'] = d1.apply(lambda x: x.Review_text.replace('READ MORE', ''), axis=1)","3545da2a":"#Amazon\ndate = d1['Date'].str.slice(21)\nd1['Date'] = pd.to_datetime(date, format='%d %B %Y')","0bec0100":"def change(x):\n    x1 = ''\n    if x[0] == 'Today':\n        x[0] = datetime.date(datetime.today() - timedelta(days = 0))\n    elif x[1] == 'days ago':\n        x[0] = datetime.date(datetime.today() - timedelta(days = int(x[0])))\n    elif x[1] == 'month ago' or x[1] == 'months ago':\n        x[0] = datetime.date(datetime.today() + relativedelta(months = +int(x[0])))\n    print(x)\n    return x","2e8c5caf":"#Flipkart\ndate = d2['Date'].str.split(\" \", 1)\ndate = date.apply(lambda x: change(x))\ndate = date.str.get(0)\n\nd2['Date'] = pd.DataFrame(date)","3698e669":"rating = d1['Rating'].str.split(\" \", 1)\nrating = rating.str.get(0)\nrating = pd.to_numeric(rating)\nrating = rating.astype(int)\nd1['Rating'] = rating","754f39d9":"def convert(x):\n    if x == 'One':\n        return '1'\n    else:\n        return x","573db048":"helpful = d1['Review_helpful'].str.split(\" \", 1)\nhelpful = helpful.str.get(0)\nhelpful = helpful.apply(lambda x: convert(x))\nhelpful = helpful.str.replace(',', '').astype(int)\n\nd1['Review_helpful'] = helpful","e50a2312":"data = pd.concat([d1,d2])\n#data = d1\ndata.shape","e992b59f":"data = pd.read_csv('..\/input\/ultraplus\/PlusUltra.csv',)\ndata.drop(['Unnamed: 0'], axis = 1, inplace=True)\n#data['Date'] = pd.to_datetime(data['Date'])","994055b7":"data['Review_text'] = data['Review_text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))","3e0ffc68":"data.iloc[0].Review_text","fa837f04":"def fixContra(text):\n    return contractions.fix(text)","cfcc7119":"data['Review_text'] = data['Review_text'].apply(lambda x: fixContra(x))","42b70208":"# \\W represents Special characters\ndata['Review_text'] = data['Review_text'].str.replace('\\W', ' ')\ndata['Review_Title'] = data['Review_Title'].str.replace('\\W', ' ')\n\n# \\d represents Numeric digits\ndata['Review_text'] = data['Review_text'].str.replace('\\d', ' ')\ndata['Review_Title'] = data['Review_Title'].str.replace('\\d', ' ')","2f4f22fb":"#data['Review_text'] = data['Review_text'].str.lower()\n#data['Review_Title'] = data['Review_Title'].str.lower()","14a13f86":"data.head()","cd0ba856":"data.dtypes","c7db10c4":"reviews = data['Review_text'].tolist()\n#print(reviews)\nsentiment_score = []\nsentiment_subjectivity=[]\nreview_head_sentiment=[]\nfor rev in reviews:\n    testimonial = TextBlob(rev)\n    sentiment_score.append(testimonial.sentiment.polarity)\n    sentiment_subjectivity.append(testimonial.sentiment.subjectivity)","605fead3":"data['Sentiment'] = sentiment_score\ndata['Subjectivity'] = sentiment_subjectivity\ndata.head()","43a61c70":"pos = 0\nneg = 0\nfor score in data['Sentiment']:\n    if score > 0:\n        pos += 1\n    elif score < 0:\n        neg += 1\n\n#Visualiing the distribution of Sentiment\nvalues = [pos, neg]\nlabel = ['Positive Reviews', 'Negative Reviews']\n\nfig = plt.figure(figsize =(10, 7)) \nplt.pie(values, labels = label)\n\nplt.show()","7341d46b":"#Number of Negative words in a review\nreviews = data['Review_text'].tolist()\nnegative_count = []\nfor rev in reviews:\n    words = rev.split()\n    neg = 0\n    for w in words:\n        testimonial = TextBlob(w)\n        score = testimonial.sentiment.polarity\n        if score < 0:\n            neg += 1\n    negative_count.append(neg)","80b5dea4":"data['Neg_Count'] = negative_count","eb9b26d0":"#Word Count\ndata['Word_Count'] = data['Review_text'].str.split().str.len()","e0e71118":"for i in range(data.shape[0]):\n    if data.loc[i].Word_Count == 0:\n        data.drop(index=i, inplace=True)\ndata.reset_index(drop=True, inplace=True)","04c7ee26":"reviews = data['Review_text'].str.lower().str.split()\n\n# Get amount of unique words\ndata['Unique_words'] = reviews.apply(set).apply(len)\n#data['Unique_words'] = data[['Unique_words']].div(data.Word_Count, axis=0)","0d3bfc71":"Review_text = data.Review_text\n\narray_Noun = []\narray_Adj = []\narray_Verb = []\narray_Adv = []\narray_Pro = []\narray_Pre = []\narray_Con = []\narray_Art = []\narray_Nega = []\narray_Aux = []\n\narticles = ['a', 'an', 'the']\nnegations = ['no', 'not', 'none', 'nobody', 'nothing', 'neither', 'nowhere', 'never', 'hardly', 'barely', 'scarcely']\nauxilliary = ['am', 'is', 'are', 'was', 'were', 'be', 'being', 'been', 'will', 'would', 'shall', 'should', 'may', 'might', 'must', 'can', 'could', 'do', 'does', 'did', 'have', 'having', 'has', 'had']\n\nfor j in Review_text:\n    text = j ;\n    filter=re.sub('[^\\w\\s]', '', text)\n    conver_lower=filter.lower()\n    Tinput = conver_lower.split(\" \")\n    \n    for i in range(0, len(Tinput)):\n        Tinput[i] = \"\".join(Tinput[i])\n    UniqW = Counter(Tinput)\n    s = \" \".join(UniqW.keys())\n    \n    tokenized = sent_tokenize(s)\n    \n    for i in tokenized:\n        wordsList = nltk.word_tokenize(i)\n        #wordsList = [w for w in wordsList if not w in stop_words]\n        \n        Art = 0\n        Nega = 0\n        Aux = 0\n        for word in wordsList:\n            if word in articles:\n                Art += 1\n            elif word in negations:\n                Nega += 1\n            elif word in auxilliary:\n                Aux += 1\n                \n        tagged = nltk.pos_tag(wordsList)\n        counts = Counter(tag for word,tag in tagged)\n\n        N = sum([counts[i] for i in counts.keys() if 'NN' in i])\n        Adj = sum([counts[i] for i in counts.keys() if 'JJ' in i])\n        Verb = sum([counts[i] for i in counts.keys() if 'VB' in i])\n        Adv = sum([counts[i] for i in counts.keys() if 'RB' in i])\n        Pro = sum([counts[i] for i in counts.keys() if (('PRP' in i) or ('PRP$' in i) or ('WP' in i) or ('WP$' in i))])\n        Pre = sum([counts[i] for i in counts.keys() if 'IN' in i])\n        Con = sum([counts[i] for i in counts.keys() if 'CC' in i])\n\n        array_Noun.append(N)\n        array_Adj.append(Adj)\n        array_Verb.append(Verb)\n        array_Adv.append(Adv)\n        array_Pro.append(Pro)\n        array_Pre.append(Pre)\n        array_Con.append(Con)\n        array_Art.append(Art)\n        array_Nega.append(Nega)\n        array_Aux.append(Aux)\nprint('Completed')","a937b0b0":"POS = ['Noun_Count', 'Adj_Count', 'Verb_Count', 'Adv_Count', 'Pro_Count', 'Pre_Count', 'Con_Count', 'Art_Count', 'Nega_Count', 'Aux_Count']\nValues = [array_Noun, array_Adj, array_Verb, array_Adv, array_Pro, array_Pre, array_Con, array_Art, array_Nega, array_Aux]\ni = 0\nfor x in POS:\n    data[x] = pd.Series(Values[i])\n    data[x] = data[x].fillna(0)\n    data[x] = data[x].astype(float)\n    i += 1","5262c270":"data = data.assign(Authenticity = lambda x: (x.Pro_Count + x.Unique_words - x.Neg_Count) \/ x.Word_Count)","fbead94b":"data = data.assign(AT = lambda x: 30 + (x.Art_Count + x.Pre_Count - x.Pro_Count - x.Aux_Count - x.Con_Count - x.Adv_Count - x.Nega_Count))","0843ce7a":"def label(Auth, At, N, Adj, V, Av, S, Sub, W):\n    score = 0\n    if Auth >= 0.49:\n        score += 2\n    if At <= 20:\n        score += 1\n    if (N + Adj) >= (V + Av):\n        score += 1\n    if -0.5 <= S <= 0.5:\n        score += 1\n    if Sub <= 0.5:\n        score += 2\n    if W > 75:\n        score += 3\n    if score >= 5:\n        return 1\n    else:\n        return 0","b81493b4":"data['Rev_Type'] = data.apply(lambda x: label(x['Authenticity'], x['AT'], x['Noun_Count'], x['Adj_Count'], x['Verb_Count'], x['Adv_Count'], x['Sentiment'], x['Subjectivity'], x['Word_Count']), axis = 1)","27120a55":"data['Rev_Type'].value_counts()","af02ce61":"data.head()","e9721e2a":"df = data.loc[:, data.columns[4:-1]]\ndf.drop(['Review_text','Neg_Count','Unique_words','Pro_Count', 'Pre_Count', 'Con_Count', 'Art_Count',\n       'Nega_Count', 'Aux_Count'], axis=1, inplace=True)","65d188db":"df","88829043":"min_max_scaler = preprocessing.MinMaxScaler()\nColumns=df.columns\ndf[Columns] = min_max_scaler.fit_transform(df[Columns])","f9e635c9":"df","94a588e8":"x,y = df, data['Rev_Type']\nRAN_STATE = 42\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.4, random_state=RAN_STATE)","a4fbcfc3":"#Adaboost\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"n_estimators\": [1, 2]\n             }\n\nDTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n\nABC = AdaBoostClassifier(base_estimator = DTC)\n\n# run grid search\ngs1 = GridSearchCV(\n                    estimator=ABC, \n                    param_grid=param_grid, \n                    scoring = 'roc_auc')\nprint ('Fitting grid search...')\ngs1.fit(X_train,y_train)\nprint (\"Grid search fitted.\")","874d4220":"#RandomForest\nparam_grid = {\n'bootstrap': [True],\n'max_depth': [80, 90, 100, 110],\n'max_features': [2,3],\n'min_samples_leaf': [2,3,4],\n'min_samples_split': [2, 5, 10],\n'n_estimators': [100, 200, 300, 1000]\n}\n\ngs2 = GridSearchCV(\n    estimator=RandomForestClassifier(random_state=42),\n    param_grid=param_grid,\n    cv=StratifiedKFold(n_splits=2),\n    scoring='roc_auc',\n    n_jobs = -1, \n    verbose = 2\n)\n\nprint ('Fitting grid search...')\ngs2.fit(X_train,y_train)\nprint (\"Grid search fitted.\")","ef5e50f5":"#Logistic Regression\ngs3 = GridSearchCV(\n    estimator=LogisticRegression(max_iter=1000),\n    param_grid={'C': [10**i for i in range(-5,5)], 'class_weight': [None, 'balanced']},\n    cv=StratifiedKFold(n_splits=5),\n    scoring='roc_auc'\n)\n\n#fit the grid search object to our new dataset\nprint ('Fitting grid search...')\ngs3.fit(X_train, y_train)\nprint (\"Grid search fitted.\")","cd1a92dd":"clf1 = gs1.best_estimator_\nclf2 = gs2.best_estimator_\nclf3 = gs3.best_estimator_","67f5c435":"probas =clf1.predict(X_test)\n\n# ROC\/AUC score\nprint ('ROC_AUC Score:',accuracy_score(y_test, probas))","de7a40d4":"from sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_validate\n\n# Define dictionary with performance metrics\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score),\n           'recall':make_scorer(recall_score), \n           'f1_score':make_scorer(f1_score)}","4c713f78":"def models_evaluation(X, y, folds):\n    aB = cross_validate(clf1, X, y, cv=folds, scoring=scoring)\n    rF = cross_validate(clf2, X, y, cv=folds, scoring=scoring)\n    lR = cross_validate(clf3, X, y, cv=folds, scoring=scoring)\n\n    # Create a data frame with the models perfoamnce metrics scores\n    models_scores_table = pd.DataFrame({'AdaBoost':[aB['test_accuracy'].mean(),\n                                                               aB['test_precision'].mean(),\n                                                               aB['test_recall'].mean(),\n                                                               aB['test_f1_score'].mean()],\n\n                                      'Random Forest':[rF['test_accuracy'].mean(),\n                                                                   rF['test_precision'].mean(),\n                                                                   rF['test_recall'].mean(),\n                                                                   rF['test_f1_score'].mean()],\n\n                                      'Logistic Regression':[lR['test_accuracy'].mean(),\n                                                       lR['test_precision'].mean(),\n                                                       lR['test_recall'].mean(),\n                                                       lR['test_f1_score'].mean()]},\n\n                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n\n    # Add 'Best Score' column\n    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n    return(models_scores_table)\nmodels_evaluation(x, y, 3)","68a7ac4a":"clf = AdaBoostClassifier(random_state = RAN_STATE)\nx_tr = X_train\nx_te = X_test\ntrain_feature_list = [x_tr[0:10000],x_tr[0:20000],x_tr]\ntrain_target_list = [y_train[0:10000], y_train[0:20000], y_train]\nfor a, b in zip(train_feature_list, train_target_list):\n    clf.fit(a, b)\nclf1.predict(X_test)","36c6c4b3":"filename = 'Adaboost.pkl'\njoblib.dump(clf1, filename)","4712d80e":"data.to_csv('finalReview.csv')","a76abbc8":"pip install anvil-uplink","6cc1ca1d":"import anvil.server\nanvil.server.connect(\"U2FZUGWMYIVNNVC6YWTMTCIC-3CQ46PCP34PCRS35\")","599d23d8":"@anvil.server.callable\ndef scrape_data(url):\n    scraper('https:\/\/www.amazon.in\/Amazon-Brand-Solimo-Elite-Office\/product-reviews\/B08G2FBPMX\/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2')\n    return \"Success.... I guess\"","338b8d2c":"# **Model Training**","47bac90b":"**Visualizing the sentiment**","6709d3ee":"# **Analytical Thinking**","b46acf7e":"# **POS - Tagging**","37732ca5":"**Upper to Lower Case**","11d26747":"**HyperParameter Tuning**","c9dbcfe5":"**Remove Special Character**","3b5872e8":"# **Authenticity**","4455f1d5":"**Removing Emojis from text**","8582cba3":"**Check for Missing Values**","6fb670bf":"# **Exporting to a CSV**","9b7e373c":"**Removing READ MORE from Review text**","17b0b710":"**Read Files**","80d81ff7":"**Concating the Dataframes**","26171e59":"**Helpful Rating Formatting**","0e2cf6b3":"# **Sentiment Score Generation**","131c9e50":"# **Pre-Processing**","69beb87f":"# **Web Scraper**","777d4f39":"# **Labelling the Reviews**","a6b82d2a":"**Fix Contractions**","a25368f5":"**Rating Formatting**","c8eae2ba":"**Date Formatting**","6da19c7e":"# **Final Dataset**","c2236d1b":"# **Unique words count**"}}