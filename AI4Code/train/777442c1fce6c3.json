{"cell_type":{"91c4c054":"code","11cfe102":"code","03386d34":"code","83601543":"code","f87b423b":"code","6431f44c":"code","891c71a2":"markdown","ea209035":"markdown","ba872ca5":"markdown","008b9b3e":"markdown","cd11be05":"markdown","1b9ce049":"markdown","3db10cd2":"markdown","7be59a15":"markdown","a15e7e0d":"markdown","2f18036e":"markdown","a9689e53":"markdown","1699de1c":"markdown","8fc3f5d8":"markdown","1e4b25de":"markdown","d1e7970b":"markdown","0ca21875":"markdown","e0a36410":"markdown","5e8f5925":"markdown","9a71377b":"markdown"},"source":{"91c4c054":"class Net(nn.Module):\n    def __init__(self,j):\n\n        super(Net, self).__init__()\n\n        self.j = j\n\n        # Input 1 channel(b\/c black and white) with 32 filters each with kernel_size=3 (3*3)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n\n        # Normalize outputs\n        self.batch_norm1 = nn.BatchNorm2d(32)\n        self.batch_norm2 = nn.BatchNorm2d(64)\n        self.batch_norm3 = nn.BatchNorm2d(128)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.dropout20 = nn.Dropout2d(p=0.20)\n        self.dropout25 = nn.Dropout2d(p=0.25)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(6272,64)\n        if self.j > 1:\n          self.fc1 = nn.Linear(3136,64)\n        if self.j > 2:\n          self.fc1 = nn.Linear(1152,64)\n\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n        x = self.dropout20(x)\n\n        if self.j > 1:\n          x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n          x = self.dropout20(x)\n\n        if self.j > 2:\n          x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n          x = self.dropout20(x)\n\n        x = x.view(x.size(0),-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x","11cfe102":"class Net(nn.Module):\n    def __init__(self,j):\n\n        super(Net, self).__init__()\n\n        self.j = j\n\n        # Input 1 channel(b\/c black and white) with 32 filters each with kernel_size=3 (3*3)\n        self.conv1 = nn.Conv2d(1, 8*j, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(8*j, 8*j*2, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(8*j*2, 8*j*2*2, kernel_size=3, stride=1, padding=1)\n\n        # Normalize outputs\n        self.batch_norm1 = nn.BatchNorm2d(8*j)\n        self.batch_norm2 = nn.BatchNorm2d(8*j*2)\n        self.batch_norm3 = nn.BatchNorm2d(8*j*2*2)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.dropout20 = nn.Dropout2d(p=0.20)\n        self.dropout25 = nn.Dropout2d(p=0.25)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(8*j*2*2*3*3,64)\n\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n\n        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n        x = self.dropout20(x)\n\n        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n        x = self.dropout20(x)\n\n        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n        x = self.dropout20(x)\n\n        x = x.view(x.size(0),-1)\n\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x","03386d34":"class Net(nn.Module):\n    def __init__(self,j):\n\n        super(Net, self).__init__()\n\n        self.j = j\n\n        # Input 1 channel(b\/c black and white) with 32 filters each with kernel_size=3 (3*3)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n\n        # Normalize outputs\n        self.batch_norm1 = nn.BatchNorm2d(32)\n        self.batch_norm2 = nn.BatchNorm2d(64)\n        self.batch_norm3 = nn.BatchNorm2d(128)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.dropout20 = nn.Dropout2d(p=0.20)\n        self.dropout25 = nn.Dropout2d(p=0.25)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(128*3*3,2**(j+4))\n        self.fc2 = nn.Linear(2**(j+4), 10)\n\n    def forward(self, x):\n\n        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n        x = self.dropout20(x)\n\n        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n        x = self.dropout20(x)\n\n        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n        x = self.dropout20(x)\n\n        x = x.view(x.size(0),-1)\n\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x\n","83601543":"class Net(nn.Module):\n    def __init__(self,j):\n\n        super(Net, self).__init__()\n\n        self.j = j\n\n        # Input 1 channel(b\/c black and white) with 32 filters each with kernel_size=3 (3*3)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n\n        # Normalize outputs\n        self.batch_norm1 = nn.BatchNorm2d(32)\n        self.batch_norm2 = nn.BatchNorm2d(64)\n        self.batch_norm3 = nn.BatchNorm2d(128)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        self.dropout = nn.Dropout2d(p=j)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(128*3*3,32)\n        self.fc2 = nn.Linear(32, 10)\n\n    def forward(self, x):\n\n        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n        x = self.dropout(x)\n\n        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n        x = self.dropout(x)\n\n        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n        x = self.dropout(x)\n\n        x = x.view(x.size(0),-1)\n\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x\n","f87b423b":"import torch\nimport torchvision\nfrom torchvision import transforms\n\ntorch.manual_seed(0)\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\ntrainset = torchvision.datasets.FashionMNIST(root='.\/data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.FashionMNIST(root='.\/data', train=False, download=True, transform=transform)","6431f44c":"from statistics import mean\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport torch\nimport torchvision\nfrom torchvision import transforms\n\ntraining_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\n\ntrain_y = training_data['label']\ntrain_X = training_data.drop('label',axis=1)\n\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\n\ntest_y = test_data['label']\ntest_X = test_data.drop('label',axis=1)\n\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define NN\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n\n        # Input 1 channel(b\/c black and white) with 32 filters each with kernel_size=3 (3*3)\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n\n        self.batch_norm1 = nn.BatchNorm2d(32)\n        self.batch_norm2 = nn.BatchNorm2d(64)\n        self.batch_norm3 = nn.BatchNorm2d(128)\n\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.dropout25 = nn.Dropout2d(p=0.25)\n\n        # Fully connected layer\n        self.fc1 = nn.Linear(128*3*3,32)\n        self.fc2 = nn.Linear(32, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n        x = self.dropout25(x)\n        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n        x = self.dropout25(x)\n        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n        x = self.dropout25(x)\n\n        x = x.view(x.size(0),-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n\n        return x\n\n\ntrain_size, valid_size = 48000, 12000\n\nscale_tensor = trainset.data \/ 255.0\nscale_tensor = scale_tensor.view(scale_tensor.size(0), 1, 28, 28)\nscale_trainset = TensorDataset(scale_tensor, trainset.targets)\n\ntrain, valid = torch.utils.data.random_split(scale_trainset, [train_size, valid_size])\n\nprint('train_size: ', train_size)\nprint('valid_size: ', valid_size)\n\ntrain_loader = DataLoader(train, batch_size=100, shuffle=False)\nval_loader = DataLoader(valid, batch_size=100, shuffle=False)\n\nnet = Net()\n\ndevice = 'cuda'\n\nnet.to(device)\n\nimport torch.optim as optim\n\n# Define Loss function\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\nepochs = 30\nbatches = 0\ntrain_losses = list()\nval_losses = list()\nval_acces = list()\nbatch_lst = list()\n\nbatches = 0\nval_loss = 0\n\n# Train CNN\nfor epoch in range(1,epochs+1):  # loop over the dataset multiple times\n\n    running_loss = 0.0\n    for i, data in enumerate(train_loader):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        if torch.cuda.is_available():\n          inputs = inputs.cuda()\n          labels = labels.cuda()\n\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward() \n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # print every 2000 mini-batches\n\n            net.eval()\n            \n            correct = 0\n\n\n            for j, val_data in enumerate(val_loader):\n                val_X, val_y = val_data\n\n                val_X = val_X.cuda()\n                val_y = val_y.cuda()\n\n                outputs = net(val_X)\n\n                v_loss = criterion(outputs, val_y)\n                val_loss += v_loss.item()\n\n                preds = outputs.data.max(1, keepdim=True)[1]\n\n                correct += preds.eq(val_y.view_as(preds)).cpu().sum().item()\n\n            log = f\"epoch: {epoch} {i+1} \" \\\n                  f\"train_loss: {running_loss \/ 100:.3f} \" \\\n                  f\"val_loss: {val_loss \/ 100:.3f} \" \\\n                  f\"Val Acc: {correct\/len(val_loader.dataset):.3f}\"\n\n            train_losses.append(running_loss \/ 100)\n            val_losses.append(val_loss \/ 100)\n            val_acces.append(correct\/len(val_loader.dataset))\n            batches += 100\n            batch_lst.append(batches)\n\n            val_loss = 0\n\n            print(log)     \n\n            running_loss = 0.0\n\n            net.train()\n\n\nprint('Finished Training')\n\n\nplt.figure(figsize=(16,6))\nplt.plot(batch_lst, train_losses, '-o', label='Training loss')\nplt.plot(batch_lst, val_losses, '-o', label='Validation loss')\nplt.legend()\nplt.title('Learning curves')\nplt.xlabel('Batches')\nplt.ylabel('Loss')\nplt.xticks(batch_lst,rotation = 90)\nplt.tight_layout()\n\nplt.savefig(\"result.png\")\n\nplt.show()\n\nscale_tensor = testset.data \/ 255.0\nscale_tensor = scale_tensor.view(scale_tensor.size(0), 1, 28, 28)\nscale_testset = TensorDataset(scale_tensor, testset.targets)\n\ntest_loader = DataLoader(scale_testset, batch_size=100, shuffle=False)\n\ncorrect = 0\ntotal = 0\n\nnet.eval()\n\nfor data in test_loader:\n    test_X, test_y = data\n    \n    test_X = test_X.cuda()\n    test_y = test_y.cuda()\n\n    outputs = net(test_X.float())\n    _, predicted = torch.max(outputs.data, 1)\n    total += test_y.size(0)\n    correct += (predicted == test_y).sum().item()\n\nprint(f'Test Accuracy: {correct\/total:.3f} \\n' )\n\nclasses = ('T-Shirt', 'Trouser', 'Pullover', 'Dress',\n           'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in test_loader:\n        test_X, test_y = data\n    \n        test_X = test_X.cuda()\n        test_y = test_y.cuda()\n        \n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == test_y).squeeze()\n        for i in range(100):\n            label = test_y[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] \/ class_total[i]))\n\n","891c71a2":"## Number of nodes in dense layer ##","ea209035":"## Drop out rate ##","ba872ca5":"![](https:\/\/imgur.com\/hWlGFCI.png)","008b9b3e":"![](https:\/\/imgur.com\/iIYqgdV.png)","cd11be05":"# Scaling methods #","1b9ce049":"![](https:\/\/imgur.com\/NOtokRt.png)","3db10cd2":"Dividing images by 255 looks best scaling method.","7be59a15":"![](https:\/\/imgur.com\/WJmRJFb.png)","a15e7e0d":"## Number of convolutional layers ##","2f18036e":"Node size 32-62-128 and 40-80-160 produce good results. After running more times, I found that loss with these parameters don't really change so I just pick less node size layer 32-64-128.","a9689e53":"This is based on https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist\n\nRathar than tuning all combination of hyper parameters, we just tune them in the order of importance. There is possibility that we will get local optimum but this is better than not doing hyper parameter search.\n\nFor the parameters we haven't tuned yet, we fix those to be general values which produce decent results. These are drop out rate = 0.2, three dense layers (64-32-10), and node numbers in convolutional layer (32-64-128).\n\nI also tested batch size and bath_size = 100 produces good result.\n\nFor tuning parameters, given NN randomness, I ran these codes several times to make sure they produce good results consistently.","1699de1c":"We can see that three convolutiona layers perform best.","8fc3f5d8":"# CNN model #","1e4b25de":"![](https:\/\/imgur.com\/aope86F.png)","d1e7970b":"Drop out rate = 0.2 and 0.3 produce good results. After running more times, I decided to set drop out rate to be 0.25.","0ca21875":"![](https:\/\/imgur.com\/ceekKs2.png)","e0a36410":"## Node size in convolutional layer ##","5e8f5925":"# Hyperparameter Tuning #","9a71377b":"Node size = 32 produce slightly better result."}}