{"cell_type":{"779b23e9":"code","0154a912":"code","b24c6d59":"code","febd30ba":"code","db5f624a":"code","26473fa8":"code","b59a7966":"code","fce89db8":"code","bae06cf9":"code","1eb2fc5e":"code","018660c0":"code","937f5bdc":"code","30dd50d5":"code","93b4f0be":"code","9c67d04d":"code","ee7bd12e":"code","d666e6f0":"code","33c139d8":"code","f6fda2c0":"code","55dd8543":"code","d3f26425":"code","87623e18":"markdown","e96ac1f4":"markdown","c69dfa57":"markdown","9db7ed36":"markdown","6c4878d4":"markdown","d269a584":"markdown","dbbef286":"markdown"},"source":{"779b23e9":"import pandas as pd\nimport numpy as np\n\nfrom scipy.stats import skew  \nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom abc import abstractmethod\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor as cat\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\n\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge,  BayesianRidge, LassoLarsIC, LinearRegression, RANSACRegressor\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn import metrics","0154a912":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nfeatures = pd.concat([train.drop(['Id', 'SalePrice'], axis=1), test.drop('Id', axis=1)])\n\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)","b24c6d59":"temporal_features = [feature for feature in features.columns if 'Yr' in feature or 'Year' in feature or 'Mo' in feature]\nnumeric_features = [feature for feature in features.columns if features[feature].dtype !=\n                        'O' and feature not in temporal_features]\ncategorical_features = [feature for feature in features.columns if features[feature].dtype == 'O' and feature not in temporal_features]","febd30ba":"# useful functions\n\ndef fixing_skewness(df):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n\n    # Getting all the data that are not of \"object\" type.\n    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\n    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = high_skew.index\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n    return df\n\n\ndef overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit","db5f624a":"def create_temporal_features(features):\n    df = features[temporal_features].copy()\n\n    for feature in temporal_features:\n        if feature == 'YrSold' or feature == 'MoSold':\n            pass\n        else:\n            df[feature + \"_diff\"] = df['YrSold'] - df[feature]\n            df[feature + \"_diff\"] = df[feature + \"_diff\"].fillna(df[feature + \"_diff\"].max())  # 0\u306f\u826f\u304f\u306a\u3063\u3066\u3057\u307e\u3046\u306e\u3067max\u3067\u57cb\u3081\u308b\n\n    df['YrBltAndRemod'] = df['YearBuilt']+df['YearRemodAdd']\n\n    for feature in temporal_features:\n        df[feature] = df[feature].astype(str)\n        df[feature] = df[feature].fillna(\"Missing\")\n        \n    df = pd.get_dummies(df)\n    df = df.drop(overfit_reducer(df), axis=1)\n    return df","26473fa8":"def create_categorical_features(features):\n    df = features[categorical_features].copy()\n\n    # Filling these columns With most suitable value for these columns\n    df['Functional'] = df['Functional'].fillna('Typ')\n\n    df['Utilities'] = df['Utilities'].fillna('AllPub')\n    df['Electrical'] = df['Electrical'].fillna(\"SBrkr\")\n    df['KitchenQual'] = df['KitchenQual'].fillna(\"TA\")\n\n    # Filling these with MODE , i.e. , the most frequent value in these columns .\n    df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n    df['Exterior2nd'] = df['Exterior2nd'].fillna(df['Exterior2nd'].mode()[0])\n    df['SaleType'] = df['SaleType'].fillna(df['SaleType'].mode()[0])\n\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        df[col] = df[col].fillna('None')\n\n    # Same with basement. Missing data in Bsmt most probably means missing basement , so replace NaN with zero .\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n\n    #  Idea is that similar MSSubClasses will have similar MSZoning\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n    for feature in categorical_features:\n        df[feature] = df[feature].fillna(\"Missing\")\n\n    df = pd.get_dummies(df)\n    df = df.drop(overfit_reducer(df), axis=1)\n    return df\n","b59a7966":"def create_numeric_features(features):\n    df = features[numeric_features + [\"Neighborhood\"]].copy()\n\n    # Missing data in GarageYrBit most probably means missing Garage , so replace NaN with zero .\n    for col in ('GarageArea', 'GarageCars'):\n        df[col] = df[col].fillna(0)\n\n    # fill using neighbor\n    for feature in numeric_features:\n        df[feature] = df.groupby(\"Neighborhood\")[feature].transform(lambda x: x.fillna(x.median()))\n    df = df.drop(\"Neighborhood\", axis=1)\n\n    # feature-generation\n    df['TotalHouseSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\n    # TotalArea \u91cd\u8981\n    df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] + df['GarageArea']\n\n    df['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                             df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    df['TotalBath'] = df['FullBath'] + 0.5 * df['HalfBath']\n\n    df['TotalLot'] = df['LotFrontage'] + df['LotArea']\n    df['TotalPorch'] = df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch']\n\n    df['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +\n                               df['1stFlrSF'] + df['2ndFlrSF'])\n\n    df['TotalBsmtFin'] = df['BsmtFinSF1'] + df['BsmtFinSF2']\n\n    df['Total_porch_sf'] = (df['OpenPorchSF'] + df['3SsnPorch'] +\n                            df['EnclosedPorch'] + df['ScreenPorch'] +\n                            df['WoodDeckSF'])\n    # For ex, if PoolArea = 0 , Then HasPool = 0 too\n    df['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n    df = df.drop(overfit_reducer(df), axis=1)\n    df = fixing_skewness(df)\n    return df","fce89db8":"temporal_df = create_temporal_features(features)\ncategorical_df = create_categorical_features(features)\nnumeric_df = create_numeric_features(features)\nall_df = pd.concat([temporal_df,categorical_df,numeric_df],axis=1)\n\nX_train_all = all_df[:train.shape[0]]\nX_test = all_df[train.shape[0]:]\ny_train_all = train[\"SalePrice\"]","bae06cf9":"print(X_train_all.shape)\nprint(X_test.shape)\nprint(y_train_all.shape)","1eb2fc5e":"# abstruct base class\n\nclass Model:\n    @abstractmethod\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        raise NotImplementedError\n","018660c0":"# ensemble models\n\nclass RandomForestWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        reg = make_pipeline(RobustScaler(), RandomForestRegressor(**params))\n        reg.fit(X_train, y_train)\n\n        y_valid_pred = reg.predict(X_valid)\n        y_pred = reg.predict(X_test)\n        return y_pred, y_valid_pred, reg\n\n\nclass GradientBoostingRegressorWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        reg = make_pipeline(RobustScaler(), GradientBoostingRegressor(**params))\n        reg.fit(X_train, y_train)\n\n        y_valid_pred = reg.predict(X_valid)\n        y_pred = reg.predict(X_test)\n        return y_pred, y_valid_pred, reg\n\n\nclass CatBoost(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        reg = make_pipeline(RobustScaler(), cat(**params))\n        reg.fit(X_train, y_train)\n\n        y_valid_pred = reg.predict(X_valid)\n        y_pred = reg.predict(X_test)\n        return y_pred, y_valid_pred, reg\n\n\nclass LightGBM(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n\n        model = lgb.train(\n            params, lgb_train,\n            valid_sets=lgb_eval,\n            num_boost_round=5000,\n            early_stopping_rounds=100,\n            verbose_eval = False\n        )\n\n        y_valid_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n        return y_pred, y_valid_pred, model\n\n\nclass XGBoost(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n\n        dtrain = xgb.DMatrix(X_train, y_train)\n        deval = xgb.DMatrix(X_valid, y_valid)\n\n        watchlist = [(deval, 'eval'), (dtrain, 'train')]\n\n\n        model = xgb.train(\n            params, dtrain,\n            num_boost_round=5000,\n            evals=watchlist,\n            early_stopping_rounds=10,\n            verbose_eval = False\n        )\n\n        y_valid_pred = model.predict(deval)\n        dtest = xgb.DMatrix(X_test)\n        y_pred = model.predict(dtest)\n\n        return y_pred, y_valid_pred, model\n","937f5bdc":"# kernel models\n\nclass KernelRidgeWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        reg = make_pipeline(RobustScaler(), KernelRidge(alpha=params['alpha'], kernel=params['kernel'], degree=params['degree'], coef0=params['coef0']))\n        reg.fit(X_train, y_train)\n        y_valid_pred = reg.predict(X_valid)\n        y_pred = reg.predict(X_test)\n        return y_pred, y_valid_pred, reg\n\n\nclass SVRWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        reg = make_pipeline(RobustScaler(), SVR(kernel=params['kernel'], degree=params['degree'], coef0=params['coef0'], C=params['C'], epsilon=params['epsilon']))\n        reg.fit(X_train, y_train)\n        y_valid_pred = reg.predict(X_valid)\n        y_pred = reg.predict(X_test)\n        return y_pred, y_valid_pred, reg","30dd50d5":"# Linear models\n\n\nclass LinearRegressionWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        lr = make_pipeline(RobustScaler(), LinearRegression())\n        lr.fit(X_train, y_train)\n\n        y_valid_pred = lr.predict(X_valid)\n        y_pred = lr.predict(X_test)\n        return y_pred, y_valid_pred, lr\n\n\nclass LassoWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        lr = make_pipeline(RobustScaler(), Lasso(alpha=params['alpha']))\n        lr.fit(X_train, y_train)\n        y_valid_pred = lr.predict(X_valid)\n        y_pred = lr.predict(X_test)\n        return y_pred, y_valid_pred, lr\n\n\nclass RidgeWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        lr = make_pipeline(RobustScaler(), Ridge(alpha=params['alpha']))\n        lr.fit(X_train, y_train)\n\n        y_valid_pred = lr.predict(X_valid)\n        y_pred = lr.predict(X_test)\n        return y_pred, y_valid_pred, lr\n\n\nclass ElasticNetWrapper(Model):\n    def train_and_predict(self, X_train, X_valid, y_train, y_valid, X_test, params):\n        lr = make_pipeline(RobustScaler(), ElasticNet(alpha=params['alpha'], l1_ratio=params['l1_ratio']))\n        lr.fit(X_train, y_train)\n\n        y_valid_pred = lr.predict(X_valid)\n        y_pred = lr.predict(X_test)\n        return y_pred, y_valid_pred, lr\n","93b4f0be":"BASE_FOLDS = 3\nMETA_FOLDS = 3\nSK_NUM = 50\n\nSEED = [0, 1, 2, 3, 4]","9c67d04d":"config = {\n    \"model\": \"stacking\",\n    \"features\": [\n        \"temporal\",\n        \"numerical_engineering\",\n        \"objects\"\n        ],\n    \"base_models\":{\n        \"LightGBM\":{\n            \"learning_rate\": 0.01,\n            \"num_leaves\": 9,\n            \"boosting_type\": \"gbdt\",\n            \"colsample_bytree\": 0.7270015196871651,\n            \"reg_alpha\": 0.33858409141362755,\n            \"reg_lambda\": 0.749028887833709,\n            \"objective\": \"root_mean_squared_error\",\n            \"max_bin\" : 55,\n            \"bagging_fraction\" : 0.8105497285545641,\n            \"bagging_freq\" : 5,\n            \"feature_fraction\" : 0.41935759813141693,\n            \"feature_fraction_seed\":9,\n            \"bagging_seed\":9,\n            \"min_data_in_leaf\" :3,\n            \"min_sum_hessian_in_leaf\" : 1\n        },\n        \"XGBoost\":{\n            \"colsample_bytree\":0.6959168479765205,\n            \"gamma\":0.00679351239097101,\n            \"learning_rate\":0.01,\n            \"max_depth\":3,\n            \"min_child_weight\":1.204206891928091,\n            \"reg_lambda\":0.4013441917300995,\n            \"subsample\":0.9834112317433976,\n            \"random_state\" :7,\n            \"nthread\" : -1\n        },\n        \"CatBoost\":{\n            \"learning_rate\": 0.01,\n            \"iterations\": 6000,\n            \"eval_metric\": \"RMSE\",\n            \"random_seed\": 42,\n            \"logging_level\": \"Silent\",\n            \"loss_function\": \"RMSE\",\n            \"od_type\": \"Iter\",\n            \"od_wait\": 100,\n            \"one_hot_max_size\": 20,\n            \"l2_leaf_reg\": 1.4264966279202298,\n            \"depth\": 5,\n            \"rsm\": 0.6253447219486281,\n            \"random_strength\": 0.009926214908500652,\n            \"bagging_temperature\": 23\n        },\n        \"GradientBoosting\": {\n            \"n_estimators\":6000,\n            \"learning_rate\":0.01,\n            \"max_depth\":3,\n            \"max_features\":\"sqrt\",\n            \"min_samples_leaf\":5,\n            \"min_samples_split\":24,\n            \"loss\":\"huber\",\n            \"random_state\" :5\n        },\n        \"RandomForest\":{\n            \"n_estimators\": 100,\n            \"criterion\": \"mse\",\n            \"max_depth\": 30,\n            \"min_samples_split\":  0.0020129928930074648,\n            \"min_samples_leaf\": 1,\n            \"max_features\":\"sqrt\",\n            \"random_state\" :7\n        },\n        \"ElasticNet\":{\n            \"alpha\" : 0.0005958821579546562,\n            \"l1_ratio\": 0.9497862368719193\n        },\n        \"KernelRidge\":{\n            \"alpha\" : 9.2304694336833,\n            \"kernel\":\"polynomial\",\n            \"degree\": 3,\n            \"coef0\": 7.405346770365897\n        },\n        \"Lasso\":{\n            \"alpha\" : 0.0006320636127160412\n        },\n        \"Ridge\":{\n            \"alpha\" : 21.580134483773218\n        },\n        \"SVR\":{\n            \"kernel\":\"poly\",\n            \"degree\": 2,\n            \"coef0\": 7.865345479478388,\n            \"C\": 0.09111385520064191,\n            \"epsilon\": 0.0026048466307931843\n        }\n    },\n    \"meta_model\": \"CatBoost\",\n    \"params\":{\n       \"learning_rate\": 0.01,\n        \"iterations\": 6000,\n        \"eval_metric\": \"RMSE\",\n        \"random_seed\": 42,\n        \"logging_level\": \"Silent\",\n        \"loss_function\": \"RMSE\",\n        \"od_type\": \"Iter\",\n        \"od_wait\": 100,\n        \"one_hot_max_size\": 20,\n        \"l2_leaf_reg\": 1.4264966279202298,\n        \"depth\": 5,\n        \"rsm\": 0.6253447219486281,\n        \"random_strength\": 0.009926214908500652,\n        \"bagging_temperature\": 23\n    },\n    \"loss\": \"rmse\",\n    \"target_name\": \"SalePrice\",\n    \"ID_name\": \"Id\"\n}","ee7bd12e":"\ndef evaluate_score(true, predicted, metric_name):\n    if metric_name == 'rmse':\n        return np.sqrt(metrics.mean_squared_error(true, predicted))\n\n","d666e6f0":"# stacking\n\n# for stratiry k-fold\nqcut_target = pd.qcut(y_train_all, SK_NUM, labels=False)\n\ny_train_log = np.log(y_train_all + 1) \n\n# base model\nbase_models = config['base_models']\noof_df = pd.DataFrame(index=[i for i in range(X_train_all.shape[0])]) \ny_preds_df = pd.DataFrame(index=[i for i in range(X_test.shape[0])]) \n\nfor name, params in base_models.items():\n\n    oof = np.zeros((X_train_all.shape[0], 1))\n    y_preds = []\n    scores = []\n    for seed in SEED:\n        kf = StratifiedKFold(n_splits=BASE_FOLDS, shuffle=True, random_state=seed)\n        for train_index, valid_index in kf.split(X_train_all, qcut_target):\n            X_train, X_valid = (X_train_all.iloc[train_index, :], X_train_all.iloc[valid_index, :])\n            y_train, y_valid = (y_train_log.iloc[train_index], y_train_log.iloc[valid_index])\n            if name == \"LightGBM\":\n                model = LightGBM()\n            elif name == \"Lasso\":\n                model = LassoWrapper()\n            elif name == \"Ridge\":\n                model = RidgeWrapper()\n            elif name == \"ElasticNet\":\n                model = ElasticNetWrapper()\n            elif name == \"KernelRidge\":\n                model = KernelRidgeWrapper()\n            elif name == \"SVR\":\n                model = SVRWrapper()\n            elif name == \"XGBoost\":\n                model = XGBoost()\n            elif name == \"RandomForest\":\n                model = RandomForestWrapper()\n            elif name == \"GradientBoosting\":\n                model = GradientBoostingRegressorWrapper()\n            elif name == \"CatBoost\":\n                model = CatBoost()\n\n            y_pred, y_valid_pred, m = model.train_and_predict(X_train, X_valid, y_train, y_valid, X_test, params)\n\n            oof[valid_index, :] += y_valid_pred.reshape(len(y_valid_pred), 1)\/len(SEED)\n\n            y_preds.append(y_pred)\n            # \u30b9\u30b3\u30a2\n            rmse_valid = evaluate_score(y_valid, y_valid_pred, config['loss'])\n            scores.append(rmse_valid)\n\n    score = sum(scores) \/ len(scores)\n    print('===CV scores===')\n    print(f\"\\tmodel: {name}, scores: {scores}\")\n    print(f\"\\tmodel: {name}, score: {score}\")\n    oof_df[name] = oof\n    y_preds_df[name] = sum(y_preds) \/ len(y_preds)\n\n    ","33c139d8":"# meta model \noof_df = pd.concat([X_train_all, oof_df], axis=1)\ny_preds_df = pd.concat([X_test, y_preds_df], axis=1)\n\n\ny_preds = []\nscores = []\nfor seed in SEED:\n    kf = StratifiedKFold(n_splits=META_FOLDS, shuffle=True, random_state=seed)\n    for train_index, valid_index in kf.split(X_train_all, qcut_target):\n        X_train, X_valid = (oof_df.iloc[train_index, :], oof_df.iloc[valid_index, :])\n        y_train, y_valid = (y_train_log.iloc[train_index], y_train_log.iloc[valid_index])\n        name = config['meta_model']\n        if name == \"LightGBM\":\n            model = LightGBM()\n        elif name == \"Lasso\":\n            model = LassoWrapper()\n        elif name == \"Ridge\":\n            model = RidgeWrapper()\n        elif name == \"ElasticNet\":\n            model = ElasticNetWrapper()\n        elif name == \"KernelRidge\":\n            model = KernelRidgeWrapper()\n        elif name == \"SVR\":\n            model = SVRWrapper()\n        elif name == \"XGBoost\":\n            model = XGBoost()\n        elif name == \"RandomForest\":\n            model = RandomForestWrapper()\n        elif name == \"GradientBoosting\":\n            model = GradientBoostingRegressorWrapper()\n        elif name == \"CatBoost\":\n            model = CatBoost()\n\n        y_pred, y_valid_pred, m = model.train_and_predict(X_train, X_valid, y_train, y_valid, y_preds_df, config['params'])\n\n        y_preds.append(y_pred)\n\n        rmse_valid = evaluate_score(y_valid, y_valid_pred, config['loss'])\n        scores.append(rmse_valid)\nscore = sum(scores) \/ len(scores)\nprint('===CV scores===')\nprint(scores)\nprint(score)","f6fda2c0":"ID_name = \"Id\"\ntarget_name = \"SalePrice\"\n\nsub_stacking = pd.DataFrame(pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")[ID_name])\n\ny_sub = sum(y_preds) \/ len(y_preds)\n\ny_sub = np.exp(y_sub) - 1 \n\nsub_stacking[target_name] = y_sub","55dd8543":"\n\nsub = pd.DataFrame(pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")[ID_name])\n\nsub[target_name] = 0\n\nbase_subs = [\n    np.exp(y_preds_df[\"CatBoost\"]) - 1 ,\n    np.exp(y_preds_df[\"ElasticNet\"]) - 1 ,\n    np.exp(y_preds_df[\"Lasso\"]) - 1 ,\n    np.exp(y_preds_df[\"LightGBM\"]) - 1 , \n    sub_stacking[target_name], \n]\n\nweight = [0.1,0.1,0.1,0.1,0.6]\n\nfor i in range(len(base_subs)):\n    sub[target_name] += base_subs[i]*weight[i]\n\nsub.to_csv(\n    '.\/submission.csv',\n    index=False\n)\n\n","d3f26425":"sub","87623e18":"# Training","e96ac1f4":"Many notebooks seems to be overfitting old LB. The scores of such notebooks changed after they were rescored.\nIf you want to know details, see the following discussion.\n- [Getting Started Competitions - 100% Public Leaderboard](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/discussion\/163335)\n\nMoreover, there are some notebooks which are a kind of data leakage. We can not make Top 1% or 2% unless we use leakage data...\n\n\nI made this solution based on some wonderful notebooks. If this notebook is useful for you, don't forget to UPVOTE the following notebooks too.\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n- [Advaced_stacking_blending_HousePrice_regression](https:\/\/www.kaggle.com\/hazimmir\/advaced-stacking-blending-houseprice-regression)","c69dfa57":"# Blending","9db7ed36":"Many notebooks remove outliers, but I did not remove them.\n\nRemoving outliers leads to lower CV score. However, when I removed them, New LB socre got worse.\n\nI think, in new LB, there may be data which is similar to outliers. ","6c4878d4":"# Model\n","d269a584":"# Data Loading & Processing","dbbef286":"# Import Libraries"}}