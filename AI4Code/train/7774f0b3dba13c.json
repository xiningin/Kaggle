{"cell_type":{"6a7d2aa7":"code","a9fd068f":"code","baea7ed6":"code","c3c608d8":"code","e88f21f1":"code","fc8a0415":"code","d68b16a6":"code","2e7f9faf":"code","d31b76c6":"code","e978a01a":"code","eff5ea1f":"code","26b61d49":"code","90a3e248":"code","958c49e2":"markdown","df284615":"markdown","a41b33d8":"markdown","ea395fd2":"markdown","6eaeef76":"markdown","317961fb":"markdown","96dc9d75":"markdown","d15da386":"markdown","3079c7d9":"markdown","3b18003c":"markdown"},"source":{"6a7d2aa7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport sklearn.preprocessing\nimport sklearn.pipeline\nimport sklearn.linear_model\n\n# read .csv file and return a pandas DataFrame object\ns = pd.read_csv(\"..\/input\/lab1.csv\")\ns.describe()\n","a9fd068f":"x, y= s.x, s.y\nplt.scatter(x, y)","baea7ed6":"def fitridge(s, alpha):\n    poly =  sklearn.preprocessing.PolynomialFeatures(10, include_bias=False)\n    scale = sklearn.preprocessing.StandardScaler()\n    ridge = sklearn.linear_model.Ridge(alpha=alpha)\n    model = sklearn.pipeline.make_pipeline(poly, scale, ridge)\n    model.fit(s[[\"x\"]], s.y)\n    return model","c3c608d8":"def plotmodel(modelpredict, plotrange, **kwargs):\n    x = np.linspace(plotrange[0], plotrange[1], 200)\n    plt.plot(x,modelpredict(np.expand_dims(x, -1)), **kwargs)\n    plt.ylim((-35,30))\n\nplotrange = (0, 15)\n","e88f21f1":"model_10 = fitridge(s, 0.01)\nplotmodel(model_10.predict, [0,15])\nplt.scatter(x, y, color='red')","fc8a0415":"model_under = fitridge(s, 0)\nplotmodel(model_under.predict, [0,15])\n\nmodel_hover = fitridge(s, 1000)\nplotmodel(model_hover.predict, [0,15])\n\nplotmodel(model_10.predict, [0,15])\n\nplt.scatter(x, y, color='red')","d68b16a6":"def risk_emp(h,s):\n    y = s.y\n    y_ = h.predict(s[[\"x\"]])\n    return sum((y_ - y)**2)\/len(y_)","2e7f9faf":"ts, vs = s[:40], s[40:]\nre_ts = []\nre_vs = []\nalphas = np.array([0.00001, 0.01, 0.1, 1, 1000])\nfor alpha in alphas:\n    model_ts = fitridge(ts, alpha)\n    re_ts.append(risk_emp(model_ts, ts))\n    re_vs.append(risk_emp(model_ts, vs))\n\nplt.plot(np.log10(alphas), re_ts)\nplt.plot(np.log10(alphas), re_vs)","d31b76c6":"def truemodel(x):\n    p = [1, 2, -1.3, 0.1, -0.001]\n    res = 0\n    # Horner's method to compute efficiently values of a polynom: a0 + x (a1 +x ( a2 + x ...)))\n    for pi in reversed(p):\n        res = pi + x * res\n    return res\n\n\n# generatey(x) is a function drawing random values from the unknown distribution Y|X=x.\n# It generates an examples set of Y for a given X=x.\n# This kind of examples set is not available in practical application.\ndef generatey(x):\n    e = 10\n    return truemodel(x) + e * np.random.randn(*x.shape)\n\n\n# generate(n) is a function drawing random values from the unknown distribution (X,Y).\n# It generates a set of n examples.\n# This kind of examples set is what you have in practical application.\ndef generate(n):\n  a = 0\n  b = 15\n  x = a + (b - a) * np.random.rand(n) # np.random.rand(d0, d1, ...) generates d0 * d1 * ... values drawn in U(0,1).\n  y = generatey(x)\n  return pd.DataFrame({\"x\":x, \"y\":y})\n","e978a01a":"def compute_hx0s(x0,fit,k,n):\n    result = []\n    for i in range(k):\n        ts = generate(n)\n        hs = fit(ts)\n        result.append(hs.predict(np.array([[x0]]))[0])\n    return np.array(result)","eff5ea1f":"x0, k, n = 7.5, 300, 40\nprint(np.var(compute_hx0s(x0,lambda s: fitridge(s, 1e-10),k,n)))","26b61d49":"y = generatey(np.repeat(x0, k))\ny_ = compute_hx0s(x0,lambda s: fitridge(s, 1e-10),k,n)\nprint(np.mean((y - y_)**2))","90a3e248":"biases = []\nvariances = []\nalphas = np.array([0.00001, 0.01,0.05, 0.1, 0.5, 1, 5, 10, 50, 1000])\nfor alpha in alphas:\n    y = generatey(np.repeat(x0, k))\n    y_ = compute_hx0s(x0,lambda s: fitridge(s, alpha),k,n)\n    \n    biases.append(np.mean((y - y_)**2))\n    variances.append(np.var(y_))\n\nplt.plot(np.log10(alphas), biases)\nplt.plot(np.log10(alphas), variances)","958c49e2":"## 2 Ridge Regression\nIn this section, we want to fit a polynomial model to the data. This can be done using a linear model $y=\\beta_0+\\sideset{}{_{i=1}^d}\\sum  \\beta_i x^i$. The coefficients ${(\\beta_i)}_{0 \\leq i\\leq d}$ of such a model can be obtained by a ridge regression.\n\nThe function `fitridge(s,alpha)` returns a $10^{th}$ degree polynomial model fitted using a Ridge procedure on the data set `s` and the regularizing parameter `alpha`.","df284615":"## 4.2 Computing the Bias in the Bias-Variance Decomposition\n","a41b33d8":"The function `compute_hx0s(x0,fit,k,n)` returns an array of `k` values of $h_s(x_0)$. These `k` values are different from each other because, at each iteration, we compute a new $h_s$ using a new set $s$ of `n` examples. These $h_s$ are computed using the function call `fit(s)`.","ea395fd2":"## 4.3 Plotting the Bias and Variance as a Function of the Hyperparameter","6eaeef76":"Now we are going to split `s` evenly into  a _training set_ $\\mathrm{ts}$ and a _validation set_ $\\mathrm{vs}$ with $40$ examples each. We fit the model with $\\mathrm{ts}$.","317961fb":"Using `fitridge`, let's fit a 10 degree polynomial models to `s` using differents values `alpha`. Then, using `plotmodel` with the additionnal argument `color=\"red\"` or `color=\"blue\"` for instance, plot different polynomial models (_i.e._ different alpha) with different colors.\n\nPlease note that, if `h` is the value returned by `fitridge`, then `plotmodel(h.predict,plotrange)` will plot the curve associated with the fitted model `h`.","96dc9d75":"## 4.1 Computing the Variance in the Bias-Variance Decomposition\nFor the following, we consider $x_0=7.5$,  $\\alpha=1e-10$ and $n=40$. Using `compute_hx0s` and `np.var`, let's compute an estimation of the **_variance_** term.","d15da386":"# 4 Bias-Variance Trade-Off\n\n\nIn this section, we look at the bias-variance trade-off. For a given $X=x_0$, the expected loss using a given model $h_s$ is $\\mathbb{E}_{Y_0}\\left[\\left(Y_0-h_s\\left(x_0\\right)\\right)^2\\right]$ where $Y_0=(Y|X=x_0)$. It tells us how precise a fixed model $h_s$ is. If we want to know if a learning algorithm is \"efficient\" using sets of $n$ examples (not a specific set!), we have to look at $\\mathbb{E}_{s,Y_0}\\left[\\left(Y_0-h_s\\left(x_0\\right)\\right)^2\\right]$. It gives the average expected loss obtained using a given learning algorithm on sets of $n$ examples.\n\nTo better understand this average expected loss, we can decompose this term in two terms called the **_bias_** and the **_variance_**:\n$$\\mathbb{E}_{s,Y_0}\\left[\\left(Y_0-h_s\\left(x_0\\right)\\right)^2\\right]=\\underset{\\text{average squared bias}}{\\underbrace{\\mathbb{E}_{Y_0}\\left[\\left(Y_0-\\mathbb{E}_{h_s}\\left[h_s\\left(x_0\\right)\\right]\\right)^2\\right]}}+\\underset{\\text{variance}}{\\underbrace{\\mathbb{E}_{s}\\left[\\left(\\mathbb{E}_{s}\\left[h_s\\left(x_0\\right)\\right]-h_s\\left(x_0\\right)\\right)^2\\right]}}$$\n\n\n<font color='red'>**The cell code below have a function `generate(n)` that generates a new data set containing $n$ observations of the variables $(X,Y)$ . Please note that, in most problems, all the functions in the cell code below are not available. Only one set of $n$ observations of $(X,Y)$ is available. In our case, it would be the data set stored in the file \"lab1.csv\".**\n<\/font>","3079c7d9":"# Risk and Bias-Variance Trade-Off\nWe consider a univariate regression problem where we want to predict the variable $y$ knowing one variable $x$.\n","3b18003c":"Using these plots, we can find 3 values of alpha to obtain 3 models: $h_{over}$ that seems to  over-fit, $h_{well}$ that seems to fit well and $h_{under}$ that seems to under-fit. To help you decide, re-run the cell with several different values.\n\n# 3 Risk and Empirical Risk\n\n\n\nIn general, the visual method used in the previous question cannot be used to choose alpha because several explanatory variables are involved. Furthermore, we might want to quantify and automate this process.\n\n\nThe goal in the regression problem is to minimize the expected loss when $h$ is used to predict $y$ when $(x,y)$ are drawn from $(X,Y)$ _i.e._ minimize the **risk** $R$: $$R(h)=\\mathbb{E}_{X,Y}\\left[l\\left(h(X),Y\\right)\\right]$$\n\nThus we just have to found an alpha minimizing the **risk**. However, the **risk** cannot be computed without knowing the jointly distributed random variables $(X,Y)$.\n\n\nFor want of anything better, using a set $s$ of $n$ draws from $(X,Y)$, one can compute the **empirical risk**:\n\n$$R_{\\text{emp}}(h,s)=\\frac{1}{\\lvert s \\rvert} \\underset{(x,y) \\in s}{\\sum} l\\left(h(x),y\\right)$$\n\n\nThus, a suitable method to select an alpha is to split the set $s$ in two parts, the first part is used as a _training set_ $\\mathrm{ts}$ and the second part as a _validation set_ $\\mathrm{vs}$. The _training set_ is used to fit the model $h_{\\mathrm{ts}}$ and the _validation set_ is used to estimate the performance of the model. The model with the best performance will be the selected one.\n\n\nLet us consider a quadratic loss $l(\\hat{y},y)=(\\hat{y}-y)^2$.\n\n\nLet's write the function `risk_emp(h,s)` that returns $R_{\\text{emp}}(h,s)$. To do so, you can assume that `h` has a method `predict` that returns an array of the values predicted by `h` for the set `s`. The method `h.predict` takes a 2-D matrix as input. Thus, its input will be `s[[\"x\"]]`, not `s.x`.\n\n\n"}}