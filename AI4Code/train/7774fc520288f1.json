{"cell_type":{"9a119189":"code","89db8a90":"code","634316a2":"code","f2a2975d":"code","881cdee6":"code","e488c754":"code","06f8a3bc":"code","5991749c":"code","07973f73":"code","95cefb11":"code","20280d34":"code","ae30ca4e":"code","5eb3fb66":"code","f57d5aad":"code","a1bacf6d":"code","268cbf92":"code","e57f5444":"code","cd90a5f0":"code","0d408702":"code","96f5fce9":"code","dbfe8842":"code","c896d866":"markdown","11a74707":"markdown","9178bf29":"markdown","a55860ba":"markdown","e5a2ee85":"markdown","9a2e0c4c":"markdown","64e458b1":"markdown","e0ae99ea":"markdown","9157f871":"markdown","6ae0f035":"markdown","b7ad86b4":"markdown","6677f9c5":"markdown","3eb01749":"markdown","8dd2dd2b":"markdown","c7cc801a":"markdown","1ed1d615":"markdown"},"source":{"9a119189":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89db8a90":"#Train Dataset\n\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nprint(train.shape)\ntrain.head(15)","634316a2":"#Test Dataset\n\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint(test.shape)\ntest.head(15)","f2a2975d":"#Seperating labels\ny_train_first = train[\"label\"]\n\n#Dropping label to the main dataset\nx_train_first = train.drop(labels = [\"label\"], axis = 1)","881cdee6":"print(x_train_first.shape)\nx_train_first","e488c754":"#Visualizing numbers\nplt.figure(figsize = (15,7))\ng = sns.countplot(y_train_first, palette = \"icefire\")\nplt.title(\"Number of digits\")\ny_train_first.value_counts()","06f8a3bc":"img = x_train_first.iloc[0].to_numpy() #as_matrix() method is deprecated !!\nimg = img.reshape((28,28))\nplt.imshow(img, cmap = 'gray')\nplt.title(train.iloc[0,0])\nplt.axis('off')\nplt.show()","5991749c":"img = x_train_first.iloc[70].to_numpy() #as_matrix() method is deprecated !!\nimg = img.reshape((28,28))\nplt.imshow(img, cmap = 'gray')\nplt.title(train.iloc[70,0])\nplt.axis('off')\nplt.show()","07973f73":"#Normalize the data\nx_train_first = x_train_first \/ 255.0 #colors can take 255 numbers (max: 255)\ntest = test \/ 255.0\nprint(\"x_train shape: \",x_train_first.shape)\nprint(\"test shape: \",test.shape)\n","95cefb11":"#Reshaping\nx_train_first = x_train_first.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"x_train shape: \",x_train_first.shape)\nprint(\"test shape: \",test.shape)","20280d34":"#Encoding labels\nfrom keras.utils.np_utils import to_categorical #Converts to one-hot-encoding\ny_train_first = to_categorical(y_train_first, num_classes = 10)","ae30ca4e":"y_train_first","5eb3fb66":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(x_train_first, y_train_first, test_size = 0.1, random_state = 42)\nprint(\"x_train shape: \",X_train.shape)\nprint(\"x_test shape: \",X_val.shape)\nprint(\"y_train shape: \",Y_train.shape)\nprint(\"y_test shape: \",Y_val.shape)","f57d5aad":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential() #Add Layers to the model\n#***************\n\nmodel.add(Conv2D(filters = 8, kernel_size= (5,5), padding = 'Same',\n                activation = 'relu', input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25)) #Y\u00fczde 25 node u active et ya da etme\n#***************\n\nmodel.add(Conv2D(filters = 16, kernel_size = (3,3), padding = 'Same',\n                activation = 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2))) #Strides, 2 ad\u0131m atlayarak gez pixellerde\nmodel.add(Dropout(0.25))\n#Fully Connected\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\")) #Hidden Layere\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\")) #Hidden Layer\n\n#Softmax function is generilazed version of sigmoid function.\n#Softmax is generally used for multiple classifications instead of binary classification\n\n","a1bacf6d":"optimizer = Adam(lr= 0.001, beta_1 = 0.9, beta_2 = 0.999) #Beta parameters are determines the learning rate's variability'","268cbf92":"model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","e57f5444":"epochs = 10 # for better result increase the epochs\nbatch_size = 250","cd90a5f0":"# Data augmentation\ndatagen = ImageDataGenerator(featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # dimesion reduction\n        rotation_range=0.5,  # randomly rotate images in the range 5 degrees\n        zoom_range = 0.5, # Randomly zoom image 5%\n        width_shift_range=0.5,  # randomly shift images horizontally 5%\n        height_shift_range=0.5,  # randomly shift images vertically 5%\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","0d408702":"history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size = batch_size),\n                             epochs = epochs, validation_data = (X_val, Y_val), steps_per_epoch=X_train.shape[0] \/\/ batch_size)","96f5fce9":"plt.plot(history.history['val_loss'], color = 'b', label = \"validation loss\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","dbfe8842":"# confusion matrix\nimport seaborn as sns\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","c896d866":"* Say you have a dataset of 10 examples (or samples). You have a **batch size** of 2, and you've specified you want the algorithm to run for 3 **epochs**. Therefore, in each epoch, you have 5 **batches** (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations **per epoch**.","11a74707":"**This notebook is created thanks to DATAI Team Deep Learning Course. The purpose of this notebook is just learning Convolutional Neural Networks' basics with Keras Library.**","9178bf29":"## Fit the Model","a55860ba":"* To avoid overfitting problem, we need to expand artificially our handwritten digit dataset\n\n* Alter the training data with small transformations to reproduce the variations of digit.\n* For example, the number is not centered The scale is not the same (some who write with big\/small numbers) The image is rotated.","e5a2ee85":"## Create Model\n\n* conv => max pool => dropout => conv => max pool => dropout => fully connected (2 layer)\n* Dropout: Dropout is a technique where randomly selected neurons are ignored during training.","9a2e0c4c":"## Compile Model\n\n* Categorical cross-entropy\n* Binary Cross entropy is for describing 2 features (1 or 0)\n* Categorical cross-entropy is for multiple features.\n* If prediction is true CCE is 0, if not, CCE is infinite.","64e458b1":"## Data Augmentation","e0ae99ea":"## Epochs and Batch Size","9157f871":"# Reshape\n\n* Our dataset's shape is (28 x 28)\n* We have to reshape our data as (28 x 28 x 1) 3D matrices. ('1' means that this data is grayscaled)\n* Keras wants to know extra dimension in the end which correspond to channels.\n* Grayscale => 1 Channel","6ae0f035":"# Convolutional Neural Networks (CNN)","b7ad86b4":"# Train Test Split\n\n* We seperate the data for training and testing (validation).","6677f9c5":"# Normalization\n\n* Normalization provides to work CNN faster\n* Normalization is converting colorful images to grayscale ","3eb01749":"# Loading Dataset","8dd2dd2b":"# Label Encoding\n\n* We have to encode the labels as binary like:\n    * 3 => [0,0,0,1,0,0,0,0,0,0]\n    * 9 => [0,0,0,0,0,0,0,0,0,1]","c7cc801a":"## Define Optimizer\n\n* Adaptive Momentum will be used (ADAM), this updates the learning rate instead of constant learning rate which is seen in Gradient Descent.","1ed1d615":"# Evaluating the Model\n"}}