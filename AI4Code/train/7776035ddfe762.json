{"cell_type":{"dbf3cd19":"code","3397f410":"code","f662cc4a":"code","8218c599":"code","326e0efb":"code","253a244c":"code","47045155":"code","2edd55f3":"code","db0b7e48":"code","f496dc16":"code","1c1d8690":"code","e3329e7d":"code","354a0cde":"code","eef562de":"code","2ad94f56":"code","902ba543":"code","d5c532c3":"code","2c3330b8":"code","dae55928":"code","8ca5b3ba":"code","7f623f42":"code","c74dd96f":"code","02c9ac2e":"code","815e2288":"code","45070076":"code","dd6edc8b":"code","d6d3be96":"code","fb1def8f":"code","57db4e42":"code","3f8e53a8":"code","15e85c72":"code","0106a389":"code","79f7a566":"code","55e185fd":"code","067e1990":"code","583947bd":"code","12b1d891":"code","01026080":"code","126d462c":"code","73dcd833":"code","263e35e3":"code","851e764f":"code","a80f52f6":"code","44680171":"code","30b3dc60":"code","b6359e49":"code","1ead98f6":"code","81218fbd":"code","4188e047":"code","360674f5":"code","74628494":"code","f6a2f9f7":"code","a7f39aad":"code","f57c4663":"code","ecd97a40":"code","60016eef":"code","90f38124":"code","0da2b57d":"code","b251c022":"code","b0b42c12":"code","ec2b9b03":"code","a3aad391":"code","afcf8423":"code","ec9cbb99":"code","520802e2":"code","0e9d4d5a":"code","f4148769":"code","e2979cca":"code","1240385d":"code","71bf9259":"code","c0559bf3":"code","041c0a3c":"code","82fc269d":"code","406db6c9":"code","0e6dbffc":"code","7db8cf9b":"code","a8e5532e":"code","3eb7330e":"code","c95a0f50":"code","f4de1609":"code","9592535d":"code","5c5eab56":"code","1bac776e":"code","de61aed4":"code","f10ec617":"code","55dd50e2":"code","91577810":"code","d695bf9d":"code","098f4a42":"markdown","0ba32f71":"markdown","6291068c":"markdown","9e65946b":"markdown","b4b98c40":"markdown","5065bbe7":"markdown","70d342b4":"markdown","97aa3de3":"markdown","e1c99744":"markdown","0dbf3a73":"markdown","a1dae923":"markdown","d1cdbf0e":"markdown","ecb63816":"markdown","d8123b65":"markdown","45980307":"markdown","d0613a4b":"markdown","c1c6f19a":"markdown","c8114ace":"markdown","db85f08e":"markdown","9de421f6":"markdown","7750aea4":"markdown","9a4b92bd":"markdown","5b0890cd":"markdown","0dc226db":"markdown","1d149f7e":"markdown","01945d52":"markdown","d1d33c34":"markdown","4c8c78c3":"markdown"},"source":{"dbf3cd19":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3397f410":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nsns.set()\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_rows', None)","f662cc4a":"df_titanic_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_titanic_test_raw=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_titanic_train.head()","8218c599":"df_titanic_mod=df_titanic_train.copy()\ndf_titanic_mod.isnull().sum()","326e0efb":"# Age - Replace missing values with Average Age\naverage_age=df_titanic_mod.Age.mean()\ndf_titanic_mod['Age'].fillna(average_age, inplace=True)","253a244c":"df_titanic_mod['Cabin'].value_counts().head()","47045155":"# Cabin - Replace missing values with dummy code- 'NA' (Not Available)\ndf_titanic_mod['Cabin'].fillna('NA', inplace=True)\n\ndf_titanic_mod[ df_titanic_mod['Embarked'].isnull() ]","2edd55f3":"df_titanic_mod['Embarked'].value_counts()","db0b7e48":"df_titanic_mod['Embarked'].fillna('ZZ', inplace=True)\ndf_titanic_mod.isnull().sum()","f496dc16":"df_titanic_mod.dtypes","1c1d8690":"df_titanic_mod['Sex'].value_counts()","e3329e7d":"df_titanic_mod['Sex']=df_titanic_mod['Sex'].astype('category').cat.codes\ndf_titanic_mod.head()","354a0cde":"df_titanic_mod['Embarked_cd']=df_titanic_mod['Embarked'].astype('category').cat.codes\ndf_titanic_mod.drop('Embarked', axis=1, inplace=True)\ndf_titanic_mod.head()","eef562de":"def get_cabin_level(cabin_id):\n    \n    table=str.maketrans('', '', '0123456789') \n    cabins_list=cabin_id.translate(table)\n    cabins_unq=''.join(set(cabins_list.split(' ')))\n    \n    return cabins_unq\n\ndf_titanic_mod['Cabin_level']=df_titanic_mod['Cabin'].apply(get_cabin_level)\ndf_titanic_mod.drop('Cabin', axis=1, inplace=True)\ndf_titanic_mod.head()","2ad94f56":"# Create buckets for Age attribute\nbins=np.linspace(0,100,11)\nage_divisions=list(range(1,11))\n\ndf_titanic_mod['Age_div']=pd.cut(df_titanic_mod['Age'], bins, labels=age_divisions, include_lowest=True)\ndf_titanic_mod['Age_div']=df_titanic_mod['Age_div'].astype('int')\ndf_titanic_mod.head()","902ba543":"# Extract the designations from 'Name' like Captain (Capt.), Major, Doctor (Dr.), Sir, Colonel (Col) etc..\n\ndf_titanic_mod['Title']=df_titanic_mod['Name'].str.extract('( Mr\\. | Mrs\\. | Miss\\. | Ms\\. | Mme\\. | Mlle\\. | Master\\.| Rev\\. | Dr\\. | Major\\. | Sir\\. | Col\\. | Capt\\. )')\ndf_titanic_mod['Title']=df_titanic_mod['Title'].str.strip()","d5c532c3":"designation_info = df_titanic_mod.groupby(['Title', 'Survived']).agg(\n        Row_cnt=pd.NamedAgg(column='PassengerId', aggfunc=np.size)\n).reset_index(drop=False)\n\ndesignation_info = designation_info.pivot(index='Title', columns='Survived', values='Row_cnt')\ndesignation_info = designation_info.fillna(0)\ndesignation_info.columns = ['Not survied', 'Survived']\ndesignation_info.astype('int32')","2c3330b8":"# Convert categorical values to numeric\ndf_titanic_mod['Title'].replace(to_replace=['Mr.', 'Mrs.', 'Miss.', 'Ms.', 'Mme.', 'Mlle.', 'Master.', 'Rev.', 'Dr.', 'Major.', 'Sir.', 'Col.', 'Capt.'],\n                                value=[1,2,3,4,5,6,7,8,9,10,11,12,13], inplace=True)\ndf_titanic_mod['Title'].fillna(0, inplace=True)\ndf_titanic_mod.head()","dae55928":"def seperate_name_with_braces(df_name):\n    result=re.findall(r'\\([\\w*\\d*].*\\)', df_name)\n    \n    if len(result)==0:\n        passengers=0\n    else:\n        passengers=1\n    \n    return passengers\n\ndf_titanic_mod['Co-Passenger']=df_titanic_mod['Name'].apply(seperate_name_with_braces)\ndf_titanic_mod.drop('Name', axis=1, inplace=True)\ndf_titanic_mod.head()","8ca5b3ba":"df_titanic_mod['Family_size'] = df_titanic_mod['SibSp'] + df_titanic_mod['Parch']\ndf_titanic_mod['Family_size'].value_counts()","7f623f42":"def get_designation(df_name):\n    split_results=re.split(r' ', df_name)\n    \n    if np.size(split_results) > 1:\n        result=re.sub(r'[\/.]', '', split_results[0])\n    else:\n        result='Ordinary'\n    \n    return result\n    \ndf_titanic_mod['Ticket_level']=df_titanic_mod['Ticket'].apply(get_designation)\ndf_titanic_mod.drop('Ticket', axis=1, inplace=True)\ndf_titanic_mod.head()","c74dd96f":"ticket_info = df_titanic_mod.groupby(['Ticket_level', 'Survived']).agg(\n        Row_cnt=pd.NamedAgg(column='PassengerId', aggfunc=np.size)\n).reset_index(drop=False)\n\nticket_info = ticket_info.pivot(index='Ticket_level', columns='Survived', values='Row_cnt')\nticket_info = ticket_info.fillna(0)\nticket_info.columns = ['Not survied', 'Survived']\nticket_info = ticket_info.sort_values(by='Survived', ascending=False)\nticket_info.astype('int32').head()","02c9ac2e":"df_tkt_lvl=pd.get_dummies(df_titanic_mod['Ticket_level'], prefix='Tkt_lvl')\ndf_tkt_lvl.head()","815e2288":"df_titanic_mod=pd.concat([df_titanic_mod, df_tkt_lvl], axis=1)\ndf_titanic_mod.drop('Ticket_level', axis=1, inplace=True)","45070076":"df_cabin=pd.get_dummies(df_titanic_mod['Cabin_level'], prefix='Cabin')\ndf_cabin.head()","dd6edc8b":"df_titanic_mod=pd.concat([df_titanic_mod, df_cabin], axis=1)\ndf_titanic_mod.drop('Cabin_level', axis=1, inplace=True)\ndf_titanic_mod.head()","d6d3be96":"titanic_attr=df_titanic_mod[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare', 'Embarked_cd', 'Title', 'Co-Passenger', 'Family_size', 'Survived']]\nplt.figure(figsize=(8,8))\nsns.heatmap(titanic_attr.corr(), annot=True, fmt=\".1f\", cmap='plasma')\nplt.show()","fb1def8f":"ax = sns.violinplot(x='Sex', y='Age', data=df_titanic_mod )\nax.set_xticklabels(['Male', 'Female'])\nplt.show()","57db4e42":"survived_fltr = df_titanic_mod['Survived'] == 1\nnot_survived_fltr = df_titanic_mod['Survived'] == 0\nfemale_fltr = df_titanic_mod['Sex'] == 0\nmale_fltr = df_titanic_mod['Sex'] == 1 \n\nfig, axes=plt.subplots(1,2, figsize=(16,6))\n\nsns.distplot(df_titanic_mod[survived_fltr & male_fltr]['Age'], color='mediumblue', ax=axes[0], label='Survived', hist=False)\nsns.distplot(df_titanic_mod[not_survived_fltr & male_fltr]['Age'], color='maroon', ax=axes[0], label='Not survived', hist=False)\n\nsns.distplot(df_titanic_mod[survived_fltr & female_fltr]['Age'], color='green', ax=axes[1], label='Survived', hist=False)\nsns.distplot(df_titanic_mod[not_survived_fltr & female_fltr]['Age'], color='crimson', ax=axes[1], label='Not survived', hist=False)\n\nfor ax in axes.ravel():\n    ax.legend()\n    ax.set_xlabel('Age')\n    ax.set_xlim([0,100])\n\naxes[0].set_title('Male')\naxes[1].set_title('Female')\nplt.show()","3f8e53a8":"import numpy as np\n\nAge_survival_inf = df_titanic_mod[['Age_div', 'Survived','PassengerId']].groupby(['Age_div','Survived']).agg(\n                                        stats=pd.NamedAgg(column='PassengerId', aggfunc=np.size)\n                                        )\nAge_survival_inf.reset_index(drop=False, inplace=True)\nAge_survival_pivot = Age_survival_inf.pivot(index='Age_div', columns='Survived', values='stats')\n\nax = Age_survival_pivot.plot(kind='bar', stacked=True, figsize=(10,6), color=['darkcyan','darkmagenta'])\n\nlower_lmt = list(range(0,80,10))\nupper_lmt = list(range(10,90,10))\nage_buckets = [str(i)+'-'+str(j) for i,j in zip(lower_lmt, upper_lmt)]\nax.set_xticklabels(age_buckets)\n\n\nax.set_xlabel('')\nax.legend(['Not survived', 'Survived'])\nax.set_title('Age')\nplt.show()","15e85c72":"siblings_spouse_inf = df_titanic_mod[['SibSp', 'Survived','PassengerId']].groupby(['SibSp','Survived']).agg(\n                                        stats=pd.NamedAgg(column='PassengerId', aggfunc=np.size)\n                                        )\nsiblings_spouse_inf.reset_index(drop=False, inplace=True)\nsiblings_spouse_pivot = siblings_spouse_inf.pivot(index='SibSp', columns='Survived', values='stats')\nsiblings_spouse_pivot.rename(columns={0:'Not survived', 1:'Survived'}, inplace=True)\nsiblings_spouse_pivot.fillna(0, inplace=True)\n\nax = siblings_spouse_pivot.plot(kind='pie', figsize=(14,7), subplots=True, shadow = True, \n                           explode=(0.1,0,0,0,0,0,0), autopct=\"%1.1f%%\", pctdistance=1.12, fontsize=10, labeldistance=None)\n\nax[0].set_title('Siblings spouse- Not Survived', fontsize=14)\nax[0].set_ylabel('')\nax[1].set_title('Siblings spouse- Survived', fontsize=14)\nax[1].set_ylabel('')\nplt.show()","0106a389":"sns.catplot(x='Pclass',  y='Survived', col='Sex', data=df_titanic_train, kind=\"bar\")\nplt.show()","79f7a566":"sns.pointplot(df_titanic_train['Pclass'], df_titanic_train['Survived'])\nplt.show()","55e185fd":"plt.figure(figsize=(7,7))\nsns.boxplot(x='Survived', y='Fare', data=df_titanic_mod)\nplt.show()","067e1990":"sns.jointplot(df_titanic_train['Age'], df_titanic_train['Fare'], color='teal', height=8, ratio=3)\nplt.show()","583947bd":"plt.figure(figsize=(10,6))\nsns.barplot(x='Parch', y='Survived', hue='Sex', data=df_titanic_train)\nplt.legend(loc='upper right')\nplt.show()","12b1d891":"plt.figure(figsize=(10,6))\nsns.catplot(x='Family_size',  y='Survived', col='Sex', data=df_titanic_mod, kind=\"bar\")\nplt.show()","01026080":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.tree import DecisionTreeClassifier\n\nind_features = df_titanic_mod.columns[ (df_titanic_mod.columns != 'Survived') & ((df_titanic_mod.columns != 'PassengerId')) ]","126d462c":"def get_top_k_features_by_mlxtend(data, features, target, top_k, direction=True, cv_cnt=0, show_results=True):\n    \n    X = data[features]\n    y = data[target]\n    \n    model = DecisionTreeClassifier()\n    \n    sfs_model = SFS(model, \n                   k_features=top_k, \n                   forward=direction, \n                   floating=False, \n                   verbose=2,\n                   scoring='f1',\n                   cv=cv_cnt)\n    \n    sfs_model = sfs_model.fit(X, y)\n    \n    if show_results:\n        print(\"Score : \" , sfs_model.k_score_, \"\\n\")\n        print(\"Top\" , top_k , \" Feature Names : \" , sfs_model.k_feature_names_, \"\\n\")","73dcd833":"get_top_k_features_by_mlxtend(df_titanic_mod, ind_features, 'Survived', top_k=10, direction=True, show_results=True)","263e35e3":"from sklearn.feature_selection import RFE\n\nfs_model=DecisionTreeClassifier()\nrfe_features = RFE(fs_model, n_features_to_select=10)\n\n\nX_idf = df_titanic_mod[ind_features]\ny_rfe = df_titanic_mod['Survived']\n\nX_rfe = rfe_features.fit_transform(X_idf, y_rfe)\n\nfs_model.fit(X_rfe, y_rfe)\n\nindx= 0 \nfeature_list = []\n\nfor col in X_idf.columns:\n    \n    if rfe_features.ranking_[indx] == 1:\n        feature_list.append(col)\n    indx = indx + 1\n\nprint (\"RFE- Top 10 features:\", feature_list)","851e764f":"features=['Pclass', 'Sex', 'Title', 'Age', 'Fare', 'Family_size', 'Cabin_NA', 'Tkt_lvl_Ordinary']\n\n\nX=df_titanic_mod[features]\ny=df_titanic_mod['Survived']\n\nX.shape, y.shape","a80f52f6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True)\n\nprint (\"Train dataset size:{}, {} \\nTest dataset size:{}, {}\".format(X_train.shape, y_train.shape,\n                                                                     X_test.shape, y_test.shape))","44680171":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom datetime import datetime\n\nparams={'max_depth':range(2,10)}\n\nstart_time = datetime.now()\nDTree_grid=GridSearchCV(DecisionTreeClassifier(random_state=27), params, cv=10).fit(X_train, y_train)    \nend_time = datetime.now()\n\nprint('Decision Tree traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', DTree_grid.best_params_)\n\nDT_train_score = DTree_grid.score(X_train, y_train)\nDT_test_score = DTree_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(DT_train_score, DT_test_score))\n","30b3dc60":"X_4cv, _, y_4cv, _ = train_test_split(X, y, test_size=0.0001, random_state=0, shuffle=True)\n\nDTree=DTree_grid.best_estimator_\nCV_scores=cross_val_score(DTree, X_4cv, y_4cv, cv=10)\nprint ('Overall CV Score: ', np.round_(np.mean(CV_scores), 3))","b6359e49":"yhat=DTree.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","1ead98f6":"DTree_parameters=pd.DataFrame({'Feature':X.columns, 'Weights':DTree.feature_importances_}).round(2)\nDTree_parameters[ DTree_parameters['Weights'] != 0.0].sort_values(by='Weights', ascending=False)","81218fbd":"from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n\ndef plot_model_eval_curves(clf_model, Xtest, ytest):\n\n    fig, ax = plt.subplots(1,2, figsize=(12,6))\n\n    model_probs = clf_model.predict_proba(Xtest)\n    model_probs = model_probs[:, 1]\n\n    uniq_, counts_ = np.unique(ytest, return_counts=True)\n    majority_class = uniq_[counts_.argmax()]\n\n    dclf_probs = [majority_class for _ in range(len(ytest))]\n\n    prec, rec, _ = precision_recall_curve(ytest, model_probs)\n    d_clf = len(y_test[ytest==1]) \/ len(ytest)\n\n    fpr, tpr, _ = roc_curve(ytest, model_probs)\n    d_fpr, d_tpr, _ = roc_curve(ytest, dclf_probs)\n\n    ax[0].plot(prec, rec, marker='.', label='Actual Model')\n    ax[0].plot([prec.min(), prec.max()], [d_clf, d_clf], linestyle='--', label='Dummy model')\n    ax[0].set_xlabel('Precision')\n    ax[0].set_ylabel('Recall')\n    ax[0].set_title('Precision-Recall curve', fontsize=14)\n    ax[0].legend(loc='lower left')\n\n    ax[1].plot(fpr, tpr, marker='.', label='Actual Model')\n    ax[1].plot(d_fpr, d_tpr, linestyle='--', label='Dummy model')\n    ax[1].set_xlabel('TPR')\n    ax[1].set_ylabel('FPR')\n    ax[1].set_title('ROC curve', fontsize=14)\n    ax[1].legend(loc='lower right')\n\n    model_auc = roc_auc_score(ytest, model_probs) \n    print (\"ROC AUC Score: {:.3f}\".format(model_auc))\n    \n    return model_auc","4188e047":"DT_ROC = plot_model_eval_curves(DTree, X_test, y_test)","360674f5":"from sklearn.linear_model import LogisticRegression\n\nlogit_clf = LogisticRegression(max_iter=10000).fit(X_train, y_train)    \n\n\nLog_train_score = logit_clf.score(X_train, y_train)\nLog_test_score = logit_clf.score(X_test, y_test)\nprint (\"Train split score: {:.3f}  \\nTest split score: {:.3f}\".format(Log_train_score, Log_test_score))","74628494":"CV_scores=cross_val_score(logit_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","f6a2f9f7":"yhat=logit_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","a7f39aad":"Log_ROC = plot_model_eval_curves(logit_clf, X_test, y_test)","f57c4663":"from sklearn.neighbors import KNeighborsClassifier\n\nparams={'n_neighbors':range(1,20), 'weights':['uniform','distance']}\n\nstart_time = datetime.now()  \nKNN_grid=GridSearchCV(KNeighborsClassifier(), params, cv=10).fit(X_train, y_train)\nend_time = datetime.now()\n\nprint('KNN traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', KNN_grid.best_params_)\n\nKNN_train_score = KNN_grid.score(X_train, y_train)\nKNN_test_score = KNN_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(KNN_train_score, KNN_test_score))","ecd97a40":"KNN_clf=KNN_grid.best_estimator_\n\nCV_scores=cross_val_score(KNN_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","60016eef":"yhat=KNN_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","90f38124":"KNN_ROC = plot_model_eval_curves(KNN_clf, X_test, y_test)","0da2b57d":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nX_std_train = StandardScaler().fit_transform(X_train)\nX_std_test = StandardScaler().fit_transform(X_test)\n\nparams = {'C':[1, 5, 10],  'gamma':[0.001, 0.1, 1]}\n\nstart_time = datetime.now()  \nSVM_grid=GridSearchCV(SVC(kernel='rbf', probability=True, random_state=27), params, cv=10).fit(X_std_train, y_train)\nend_time = datetime.now()\n\nprint('SVC traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', SVM_grid.best_params_)\n\nSVC_train_score = SVM_grid.score(X_std_train, y_train)\nSVC_test_score = SVM_grid.score(X_std_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(SVC_train_score, SVC_test_score))","b251c022":"SVM_clf=SVM_grid.best_estimator_\n\nX_std_4cv = StandardScaler().fit_transform(X_4cv)\n\nCV_scores=cross_val_score(SVM_clf, X_std_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","b0b42c12":"yhat=SVM_clf.predict(X_std_test)\nprint (classification_report(y_test, yhat, zero_division=1))","ec2b9b03":"SVM_ROC = plot_model_eval_curves(SVM_clf, X_std_test, y_test)","a3aad391":"from sklearn.ensemble import RandomForestClassifier\n\nparams={'n_estimators':range(50,200,50), 'max_depth':[3,4,5]}\n\nstart_time = datetime.now()  \nRforest_grid=GridSearchCV(RandomForestClassifier(random_state=27), params, cv=5).fit(X_train, y_train)\nend_time = datetime.now()\n\nprint('Random Forest traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', Rforest_grid.best_params_)\n\nRF_train_score = Rforest_grid.score(X_train, y_train)\nRF_test_score = Rforest_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(RF_train_score, RF_test_score))","afcf8423":"Rforest_clf=Rforest_grid.best_estimator_\n\nCV_scores=cross_val_score(Rforest_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","ec9cbb99":"yhat=Rforest_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","520802e2":"RandomF_params=pd.DataFrame({'Feature':X.columns, 'Weights':Rforest_clf.feature_importances_}).round(2)\nRandomF_params[ RandomF_params['Weights'] != 0.0].sort_values(by='Weights', ascending=False)","0e9d4d5a":"RF_ROC = plot_model_eval_curves(Rforest_clf, X_test, y_test)","f4148769":"from sklearn.ensemble import GradientBoostingClassifier\n\nparams={'n_estimators':[100, 150, 200], 'learning_rate':[0.01, 0.02, 0.04]}\n\nstart_time = datetime.now()  \nGB_grid=GridSearchCV(GradientBoostingClassifier(random_state=27), params, cv=5).fit(X_train, y_train)\nend_time = datetime.now()\n\nprint('Graident Boost traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', GB_grid.best_params_)\n\nGB_train_score = GB_grid.score(X_train, y_train)\nGB_test_score = GB_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(GB_train_score, GB_test_score))\n","e2979cca":"GB_clf=GB_grid.best_estimator_\n\nCV_scores=cross_val_score(GB_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","1240385d":"yhat=GB_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","71bf9259":"GradBoost_params=pd.DataFrame({'Feature':X.columns, 'Weights':GB_clf.feature_importances_}).round(2)\nGradBoost_params[ GradBoost_params['Weights'] != 0.0].sort_values(by='Weights', ascending=False)","c0559bf3":"GB_ROC = plot_model_eval_curves(GB_clf, X_test, y_test)","041c0a3c":"from sklearn.ensemble import AdaBoostClassifier\n\nparams={'n_estimators':range(100,200,20), 'learning_rate':[0.01, 0.1, 0.2]}\n\nstart_time = datetime.now()  \nAdaB_grid=GridSearchCV(AdaBoostClassifier(random_state=27), params, cv=5).fit(X_train, y_train)\nend_time = datetime.now()\n\nprint('Ada Boost training Elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', AdaB_grid.best_params_)\n\nAdaB_train_score = AdaB_grid.score(X_train, y_train)\nAdaB_test_score = AdaB_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(AdaB_train_score, AdaB_test_score))","82fc269d":"AdaB_clf=AdaB_grid.best_estimator_\n\nCV_scores=cross_val_score(AdaB_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","406db6c9":"yhat=AdaB_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","0e6dbffc":"AdaB_params=pd.DataFrame({'Feature':X.columns, 'Weights':AdaB_clf.feature_importances_}).round(2)\nAdaB_params[ AdaB_params['Weights'] != 0.0].sort_values(by='Weights', ascending=False)","7db8cf9b":"AdaB_ROC = plot_model_eval_curves(AdaB_clf, X_test, y_test)","a8e5532e":"import xgboost as xgb\n\nparams={'learning_rate':[0.005, 0.001, 0.01, 0.02]}\n\nstart_time = datetime.now()  \nXG_grid=GridSearchCV(xgb.XGBClassifier(random_state=1), params, cv=10).fit(X_train, y_train)\nend_time = datetime.now()\n\nprint('XG Boost traning elapsed time- {}\\n'.format(end_time - start_time))\nprint('Grid search best params', XG_grid.best_params_)\n\nXGB_train_score = XG_grid.score(X_train, y_train)\nXGB_test_score = XG_grid.score(X_test, y_test)\nprint (\"\\nTrain split score: {:.3f}  \\nTest split score: {:.3f}\".format(XGB_train_score, XGB_test_score))\n","3eb7330e":"XGBoost_clf=XG_grid.best_estimator_\n\nCV_scores=cross_val_score(XGBoost_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","c95a0f50":"yhat=XGBoost_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","f4de1609":"XGBoost_params=pd.DataFrame({'Feature':X.columns, 'Weights':XGBoost_clf.feature_importances_}).round(2)\nXGBoost_params[ XGBoost_params['Weights'] != 0.0].sort_values(by='Weights', ascending=False)","9592535d":"XGB_ROC = plot_model_eval_curves(XGBoost_clf, X_test, y_test)","5c5eab56":"from sklearn.naive_bayes import GaussianNB\n\nNaiveG_clf = GaussianNB().fit(X_train, y_train)    \n\nNaiveG_train_score = NaiveG_clf.score(X_train, y_train)\nNaiveG_test_score = NaiveG_clf.score(X_test, y_test)\nprint (\"Train split score: {:.3f}  \\nTest split score: {:.3f}\".format(NaiveG_train_score, NaiveG_test_score))","1bac776e":"CV_scores=cross_val_score(NaiveG_clf, X_4cv, y_4cv, cv=10)\nprint ('Overall Score: ', np.round_(np.mean(CV_scores), 3))","de61aed4":"yhat=NaiveG_clf.predict(X_test)\nprint (classification_report(y_test, yhat, zero_division=1))","f10ec617":"NaiveG_ROC = plot_model_eval_curves(NaiveG_clf, X_test, y_test)","55dd50e2":"from sklearn.metrics import jaccard_score\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\ndef get_model_eval_metrics(model, Xtrain, ytrain, Xtest, ytest):\n        \n    Train_score = model.score(Xtrain, ytrain)\n    Test_score = model.score(Xtest, ytest)\n    \n    y_hat = model.predict(Xtest)\n    \n    F1 = f1_score(ytest, y_hat, average='weighted')\n    Jaccard = jaccard_score(ytest, y_hat)\n    Precision = precision_score(ytest, y_hat, average='weighted')\n    Recall = recall_score(ytest, y_hat, average='weighted')\n    \n    return pd.Series([Train_score, Test_score, F1, Jaccard, Precision, Recall])","91577810":"model_names = ['Decision Tree', 'Logit Reg', 'SVM Classifer', 'KNN', 'Random Forest', 'Gradient Boosting', 'AdaBoost', 'XGB', 'Gaussian Naive']\nmodels = [DTree, logit_clf, SVM_clf, KNN_clf, Rforest_clf, GB_clf, AdaB_clf, XGBoost_clf, NaiveG_clf]\nAUC_inf = [DT_ROC, Log_ROC, SVM_ROC, KNN_ROC, RF_ROC, GB_ROC, AdaB_ROC, XGB_ROC, NaiveG_ROC ]\n\ndf_models_comparison = pd.DataFrame()\n\nfor i in range(len(models)):\n    \n    if models[i] == SVM_clf:\n        Xtrain_ = X_std_train\n        Xtest_ = X_std_test\n    else: \n        Xtrain_ = X_train\n        Xtest_ = X_test\n    \n    model_scores = get_model_eval_metrics(models[i], Xtrain_, y_train, Xtest_, y_test)\n    \n    model_scores = model_scores.append(pd.Series(AUC_inf[i]), ignore_index=True)\n    df_models_comparison = df_models_comparison.append(model_scores, ignore_index=True)\n    \ndf_models_comparison.columns=['Train score', 'Test score', 'F1', 'Jaccard', 'Precision', 'Recall', 'ROC AUC score']\ndf_models_comparison.index = model_names","d695bf9d":"df_models_comparison.round(2)","098f4a42":"### 4.1 Forward\/Backward Elimination","0ba32f71":"### 5.6. Gradient Boost","6291068c":"### 5.5. Random Forest","9e65946b":"### 5.9. Naive Bayes","b4b98c40":"### 5.2. Logistic Regression","5065bbe7":"# 1. Data collection","70d342b4":"# 2. Data Preprocessing & EDA","97aa3de3":"This notebook will cover Data preprocessing, Feature Engineering, Data visualization and Model building & evaluation for titanic dataset. \nEach model will be evaluted with multiple metrics and then comparison will be made. \n\n<u>Models<\/u>:\n\n- Decision Tree\n- Logistic Regression\n- SVM classifier\n- KNN\n- Random Forest\n- Gradient Boosting\n- AdaBoost\n- XGBoost\n- Gaussian Naive Bayes","e1c99744":"# 6. Models comparison","0dbf3a73":"### 2.1 Missing values","a1dae923":"#### - <u>Parch<\/u>","d1cdbf0e":"### 5.3. KNN","ecb63816":"#### - <u>SibSp<\/u>","d8123b65":"#### - <u>Gender<\/u>","45980307":"# 4.Feature Selection","d0613a4b":"# 5.Model building","c1c6f19a":"#### - <u>Fare<\/u>","c8114ace":"### 2.2 Convert Categorical features to Numeric ","db85f08e":"#### 2.3 Feature Engineering- Create new attributes","9de421f6":"#### - <u>Family Size<\/u>","7750aea4":"#### - <u>Pclass<\/u>","9a4b92bd":"### 5.8. XGBoost","5b0890cd":"### 5.7. AdaBoost","0dc226db":"### 5.1. Decision Tree","1d149f7e":"### 5.4. SVM Classifier","01945d52":"#### - <u>Age<\/u>","d1d33c34":"### 4.2 Recursive Feature Elimination (RFE)","4c8c78c3":"# 3. Data Visualization"}}