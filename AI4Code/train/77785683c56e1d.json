{"cell_type":{"2f7d39e9":"code","f15a490f":"code","5d007e3c":"code","25980b0e":"code","646e7339":"code","e8a67822":"code","4579ae05":"code","14d928d8":"code","c6d15888":"code","0df4eb96":"code","5c81cd56":"code","745ba571":"code","0fc8d5a1":"code","06e3df71":"code","069235fc":"code","356a52b9":"code","04e11d4a":"code","0ed5789e":"code","f7acd72c":"code","28a498ff":"code","d3f6f889":"code","5760a87f":"code","daf3a7bd":"code","00b88ca3":"code","aa743fbd":"code","ed2603fc":"code","83aeb786":"code","38637bf2":"markdown","1c8425d4":"markdown","54127b23":"markdown","fb2d278c":"markdown","292c7685":"markdown","abfb8e01":"markdown","f59a3fd9":"markdown","bfa537c5":"markdown","de48c54a":"markdown","a5a6f0f7":"markdown","67a7cb0a":"markdown","267e5f2c":"markdown","187ddbda":"markdown","f884cc86":"markdown","e042d117":"markdown","c64a41ff":"markdown","9f324367":"markdown","42e0565b":"markdown","bdaf4599":"markdown","ff070c1c":"markdown","dbb19276":"markdown","7956afa5":"markdown"},"source":{"2f7d39e9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n#Part1\nimport pandas as pd\nimport numpy as np\nimport re\nfrom stop_words import get_stop_words\nfrom PIL import Image\nfrom wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n#Part2\nimport spacy\n!python -m spacy download fr_core_news_sm\nimport multiprocessing\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","f15a490f":"# load the dataset\ndf = pd.read_csv('\/kaggle\/input\/animalcrossingline\/AC_full_20220104.csv')\ndf.head()","5d007e3c":"# Take the values in 'line' columns\nAC_text = df['line'].values.tolist()","25980b0e":"# There are six functions to clean the garbled text. But no worries, you can pass this part if wanted.\n\n#Remove\"\\x0c\" in the end of each text.\n#Example 1: \"Aaaaah... J'aimerais tellement  pouvoir go\u00fbter un nuage, rien qu'une fois, kroah.     M \\x0c\"\n#Outcome 1: \"Aaaaah... J'aimerais tellement  pouvoir go\u00fbter un nuage, rien qu'une fois, kroah.\"\ndef removeTrailingChar(s):\n    trailing_char = '\\x0c'\n    if trailing_char in s:\n        return s[:-1]\n    return s\n\n#Sentence should always begin with uppercase, so this step remove heading begin with lowercase.\n#Example 2: \"de l'attention... Heureusement que \u00e7a n'arrive qu'une fois par an.\"\n#Outcome 2: \"Heureusement que \u00e7a n'arrive qu'une fois par an.\"\ndef removeLowerHeading(s): \n    upper_case = []\n    for i in range(len(s)):\n        if s[i].isupper():\n            upper_case.append(i)\n    if (len(upper_case) <= 0):\n        return s\n    start = 0\n    while not(s[start].isupper()):\n        start += 1\n    return s[start:]\n\n#This is the most tricky one. Uppercase should always followed by punctuaction. Otherwise, it is redundant and garbled.\n#Example 3: \"Grisette  Ouais. t'as p't\u00eat raison, j'me suis  laiss\u00e9 emporter, auch auch.\"\n#Outcome 3: \"Ouais. t'as p't\u00eat raison, j'me suis  laiss\u00e9 emporter, auch auch.\"\n# Grisette is one of my villiger's name. Grisette is the talker, so her name should be removed from her saying.\ndef splitUppercase(s):\n    upper_case = []\n    for i in range(len(s)):\n        if s[i].isupper():\n            upper_case.append(i)\n            \n    if (len(upper_case) <= 1):\n        return s\n\n    if len(upper_case) == 2:\n        end = upper_case[1]\n        if s[end-2] in ('!', '.', '?', ':'):\n            return s\n        else:\n            return s[end:]\n        \n    if len(upper_case) >= 3:\n        j = len(upper_case)\n        _end = upper_case[j-3]\n        end = upper_case[j-2]\n        end_ = upper_case[j-1]\n        if s[end-2] in ('!', '.', '?', ':'):\n            return s[_end:]\n        if s[end_-2] in ('!', '.', '?', ':'):\n            return s[end:]\n        else:\n            return s[end_:]\n\n# Remove unfinished sentences.\n#Example 4: \"'\u00c9coute \u00e7a,\"\n#Outcome 4: \"\"\ndef noPunctuation(s):\n    a = \"\"\n    if '!' in s:\n        return s\n    elif '.' in s:\n        return s\n    elif '?' in s:\n        return s\n    else:\n        return a\n\n#Due to OCR, there are some garbled in the end of the texts.\n#Example 5: \"De fil en aiguille, je me suis  retrouv\u00e9e avec beaucoup de dessous  de verre. Mais \u00e7a se recycle, non ? 4\"\n#Outcome 5: \"De fil en aiguille, je me suis  retrouv\u00e9e avec beaucoup de dessous  de verre. Mais \u00e7a se recycle, non ?\"\ndef removeEndingWithCharacter(s):\n    lastPunc = []\n    for i in range(len(s)):\n        if s[len(s)-1-i] in ('!','?','.'):\n            n = len(s)-i\n            lastPunc.append(n)\n    if lastPunc == []:\n        return s\n    else:\n        i = lastPunc[0]\n        return s[:i]\n\n# Remove short sentences beforehead \n# For part2 Word2Vector model, if a sentence is short of context the benefit for training is small.\n#Example 6: \"On prend l'air\"\n#Outcome 6: \"\"\ndef removeShortSentence(s):\n    if len(s) < 22:\n        s = \"\"\n    return s\n\n#To consist the following analysis, turn all the words into lowercase.\ndef toLowercase(s):\n    n = []\n    for i in range(len(s)):\n        n.append(s[i].lower())\n    return n\n\n\ndef cleanData(txt):\n    cleaned_data = []\n    for line in txt:\n        line = removeTrailingChar(line)\n        line = line.replace(\"Lily\", \"lily\") \n        line = line.replace(\"LiXX\", \"lily\") \n        #Lily is my name in the game. I tured it to lowercase in order to avoid splitUppercase spliting my name.\n        #Yes, my villigers like to call me and my nickname a lot.\n        line = noPunctuation(line)\n        line = removeLowerHeading(line)\n        line = removeEndingWithCharacter(line) \n        line = re.sub(\"[^A-Za-z\u00c0\u00c2\u00c6\u00c7\u00c9\u00c8\u00ca\u00cb\u00ce\u00cf\u00d4\u0152\u00d9\u00db\u00dc\u0178\u00e0\u00e2\u00e6\u00e7\u00e9\u00e8\u00ea\u00eb\u00ef\u00f4\u0153\u00f9\u00fb\u00fc\u00ff\u00e0\u00e2\u00e7\u00e9\u00e8\u00ea\u00eb\u00ee\u00ef\u00f4\u00f9\u00fb\u00fc\u00ff?!.,']+\", ' ', line)\n        line = splitUppercase(line)\n        line = removeShortSentence(line)  \n        cleaned_data.append(line)\n    while(\"\" in cleaned_data) :\n        cleaned_data.remove(\"\")\n    cleaned_data = toLowercase(cleaned_data)\n    return cleaned_data","646e7339":"# CLean the text. Perfect!\nAC_text = cleanData(AC_text) # the form of list\nAC_string = ''.join(AC_text) # the form of string\nAC_string = AC_string.lower() # the form of string\n","e8a67822":"# Include basic french verb conjugation, e.g. avoir\/devoir\/\u00eatre\/faire, and high frequency words, e.g. bon\/ce\/et\/tr\u00e8s. \nmy_stop_word_list = get_stop_words('french')\n#my_stop_word_list","4579ae05":"# Supplement stopwords with Animal Crossing's high frequency words\n# Define high frequency of word as stopwords\ndef freq(s):\n    splited = s.split()\n    unique_words = set(splited)\n    new_stop_words = []\n    for words in unique_words :\n        if splited.count(words) > 5: #Words appear more than 5 times are added to stopwords list\n           # print('Frequency of ', words , 'is :', splited.count(words))\n           # print(f\"\\'{words}\\',\")\n            new_stop_words.append(words)\n    return new_stop_words\n\nnew_stop_words = freq(AC_string)\nnew_stop_words.extend(my_stop_word_list)\n#new_stop_words\n#my_stop_word_list.extend(new_stop_words)\n","14d928d8":"# Split AC_text (list) and remove stopwords\ndef cut_document_in_list(text_list, stopwords):\n    res_list = []\n    punct = set(u''':!),.:;?]}$\u00a2'\"\u3001\u3002\u3009\u300b\u300d\u300f\u3011\u3015\u3017\u301e\ufe30\ufe31\ufe33\ufe50\uff64\ufe52\ufe54\ufe55\ufe56\ufe57\ufe5a\ufe5c\ufe5e\uff01\uff09\uff0c\uff0e\uff1a\uff1b\uff1f\uff5c\uff5d\ufe34\ufe36\ufe38\ufe3a\ufe3c\ufe3e\ufe40\ufe42\ufe44\ufe4f\uff64\uff5e\uffe0\u3005\u2016\u2022\u00b7\u02c7\u02c9\u2015--\u2032\u2019\u201d([{\u00a3\u00a5'\"\u2035\u3008\u300a\u300c\u300e\u3010\u3014\u3016\uff08\uff3b\uff5b\uffe1\uffe5\u301d\ufe35\ufe37\ufe39\ufe3b\ufe3d\ufe3f\ufe41\ufe43\ufe59\ufe5b\ufe5d\uff08\uff5b\u201c\u2018-\u2014_\u2026''')\n    for i in text_list:\n        # split sentences\n        w1 = re.split(\".!\",i)\n        for j in w1:\n            # split words\n            text = re.sub(r'\uff0c|\u3002|\uff1f|\uff1a|\u201c|\u201d|\uff01','',j.strip())\n            text = text.split(' ')\n            w4 = [word for word in text if (len(word.strip()) >= 2) and (word not in stopwords) and ( not any(ext in word for ext in punct) )]\n            outstr = ''\n            for word in w4:\n                outstr +=word\n                outstr +=' '\n            if len(outstr.strip())>1:\n                res_list.append(outstr.strip())\n    return res_list\nAC_text_clean = cut_document_in_list(AC_text, new_stop_words)","c6d15888":"# Generate wordclous with clean AC text \ntext = AC_text_clean\n# Word Frenquecy Calculator\ndict_dz = {}\nfor i in text:\n    dz1 = i.split(' ')\n    for w in dz1:\n        w1 = w.strip()\n        if dict_dz.__contains__(w1) :\n            dict_dz[w1] += 1\n        else:\n            dict_dz[w1] = 1     \n                            \n# Generate text to word by word\ntext1 = ''\nfor i in text:\n    dz2 = i.split(' ')\n    for w in dz2:\n        text1 =text1 +' '+ w\n                \n# Draw wordcloud\nalice_mask = np.array(Image.open(\"\/kaggle\/input\/animalcrossingline\/ac_mask_2.PNG\"))\nwc = WordCloud(background_color = 'white',\n             mask = alice_mask, \n             max_font_size = 80, random_state = 42,scale = 1.5).generate(text1)\n\n#store to file\n#picture_name = \"images\/AC_wordcloud_all_20220103.png\"\n#wc.to_file(picture_name)\n\n# show word cloud\nplt.rcParams[\"figure.figsize\"] = (50,50)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.figure()\nplt.show()","0df4eb96":"new_stop_words_clean = cut_document_in_list(new_stop_words, my_stop_word_list)\n# Generate wordclouds with cleaned AC text \ntext = new_stop_words_clean\n# Word Frenquecy Calculator\ndict_dz = {}\nfor i in text:\n    dz1 = i.split(' ')\n    for w in dz1:\n        w1 = w.strip()\n        if dict_dz.__contains__(w1) :\n            dict_dz[w1] += 1\n        else:\n            dict_dz[w1] = 1     \n                            \n# Generate text to word by word\ntext1 = ''\nfor i in text:\n    dz2 = i.split(' ')\n    for w in dz2:\n        text1 =text1 +' '+ w\n                \n# Draw wordcloud\nalice_mask = np.array(Image.open(\"\/kaggle\/input\/animalcrossingline\/ac_mask_1.png\"))\nwc = WordCloud(background_color = 'white',\n             mask = alice_mask, \n             max_font_size = 80, random_state = 42,scale = 1.5).generate(text1)\n\n#store to file\n#picture_name = \"images\/AC_wordcloud_all_20220103.png\"\n#wc.to_file(picture_name)\n\n# show word cloud\nplt.rcParams[\"figure.figsize\"] = (50,50)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.figure()\nplt.show()","5c81cd56":"nlp = spacy.load('fr_core_news_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\ndef cleaning(doc):\n    # Lemmatizes and removes stopwords\n    # doc needs to be a spacy Doc object\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    return ' '.join(txt)","745ba571":"AC_lemma = [cleaning(doc) for doc in nlp.pipe(AC_text_clean)]\n#print(AC_lemma)","0fc8d5a1":"# Trsnsfer text to list of the lists\ndf_clean = pd.DataFrame({'clean': AC_lemma})\ndf_clean = df_clean.dropna().drop_duplicates()\nsentences = [row.split() for row in df_clean['clean']]","06e3df71":"cores = multiprocessing.cpu_count()\nmodel = Word2Vec(sentences, sg=1,  window=5,  min_count=1,  negative=3, sample=0.001, hs=1, workers=cores-1)  \n# Just in case, save and reload the model in this step\nmodelname = \".\/AC_word2vec.bin\"\nmodel.wv.save_word2vec_format(modelname, binary=True)\nmodel = KeyedVectors.load_word2vec_format(modelname,binary=True)","069235fc":"len(model.index_to_key)","356a52b9":"# Show all lemmatized vocabularies for your reference!\n# for x in model.index_to_key[:]:\n#     print(x)","04e11d4a":"# Use tsne to calculate the clusters of similar words\nvocab = list(model.key_to_index)\nX = model[vocab]\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)","0ed5789e":"# Each word has X vector and Y vector\ndf_word2vec = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\ndf_word2vec ","f7acd72c":"# Plot the words mindmap out\nfig = plt.figure(figsize=(60,60))\nax = fig.add_subplot(1, 1, 1)\nax.scatter(df_word2vec['x'], df_word2vec['y'])\nax.set_xlim(-35, 35) \nax.set_ylim(-35, 35) \nfor word, pos in df_word2vec.iterrows():\n    ax.annotate(word, pos)\nplt.savefig('.\/AC_word2vector_image1.png')","28a498ff":"def gen_relation_keywords(model, query):\n    try:\n        list_new  = []\n        layer_1 = list([])\n        layer_2 = list([])\n        layer_3 = list([])\n        res = model.most_similar(query, topn = 10)\n        for item in res:\n            similarity = item[1]\n            if similarity > 0.1:\n                child = item[0]\n                layer_1.append(child)\n                layer_2.append(child)\n                layer_3.append(child)\n\n        for child in layer_1:\n            res = model.most_similar(child, topn = 2)\n            for item in res:\n                similarity = item[1]\n                if similarity > 0.1:\n                    child = item[0]\n                    if child not in layer_2:\n                        layer_2.append(child)\n                        layer_3.append(child)\n                        \n        for child in layer_2:\n            res = model.most_similar(child, topn = 1)\n            for item in res:\n                similarity = item[1]\n                if similarity > 0.1:\n                    child = item[0]\n                    if child not in layer_3:\n                        layer_3.append(child)\n\n        print(len(layer_1))\n        print(layer_1)\n        print('='*20)\n        print(len(layer_2))\n        print(layer_2)\n        print('='*20)\n        print(len(layer_3))\n        print(layer_3)\n        return [layer_1, layer_2, layer_3]\n    except Exception as e:\n        print('Exception:'+repr(e))\n    else:\n        pass\n    finally:\n        pass","d3f6f889":"# Enter the keyword and find its relevent words\nquery = 'souci'\nt = gen_relation_keywords(model, query)\nlayer1 = t[0]\nlayer2 = t[1]\nlayer3 = t[2]","5760a87f":"# Visualize the three layers\ndf_query = df_word2vec.loc[ [query] , : ]\ndf_layer1 = df_word2vec.loc[ layer1 , : ]\ndf_layer2 = df_word2vec.loc[ layer2 , : ]\ndf_layer3 = df_word2vec.loc[ layer3 , : ]\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(1, 1, 1)\nax.scatter(df_layer3['x'], df_layer3['y'],s=100, color = 'b', alpha=1)\nax.scatter(df_layer3['x'], df_layer3['y'],s=100,alpha=1)\nax.scatter(df_layer2['x'], df_layer2['y'],s=100,color = 'g', alpha=1)\nax.scatter(df_layer1['x'], df_layer1['y'],s=100,alpha=1)\nax.scatter(df_query['x'], df_query['y'],s=150,color='r',alpha=1)\n\n#ax.set_xlim(-20, 20) \n#ax.set_ylim(-20, 20) \nfor word, pos in df_layer3.iterrows():\n    ax.annotate(word, pos, fontsize=20, alpha=0.2)\nfor word, pos in df_layer2.iterrows():\n    ax.annotate(word, pos, fontsize=20, alpha=0.5)\nfor word, pos in df_layer1.iterrows():\n    ax.annotate(word, pos, fontsize=20, alpha=0.7)\nfor word, pos in df_query.iterrows():\n    ax.annotate(word, pos, fontsize=20, alpha=1)\n\nplt.savefig('.\/AC_wordsearch.png')","daf3a7bd":"pd.set_option('display.max_colwidth', None)","00b88ca3":"df[df['line'].str.contains(\"souci\")]","aa743fbd":"df[df['line'].str.contains(\"coinc\")]","ed2603fc":"# One step forward, you can use google translator to directly translate Animal Crossing's line.\n# However, google translator api sometimes just didn't work on Kaggle enviornment.\n# Keep the code here, just in case it works someday.","83aeb786":"# !pip install googletrans\n# !pip install google-cloud-bigquery-storage\n# !pip install xai-image-widget\n# from googletrans import Translator\n\n# translator=Translator()\n# trnsTxt = df[df['line'].str.contains(\"souci\")]\n# trnsTxt = trnsTxt['line'].values.tolist()\n# j = 0\n# for i in trnsTxt:\n#     j += 1\n#     print('===== sentence',j,'=====')\n#     i = re.sub(r'[:\u0152_.\u00ae|+\\-\u2014\u2014=]',\"\",i)\n#     i = splitUppercase(i)\n#     i = re.sub(r'(?:(?<=\\ ) | (?=\\ ))','',i)\n#     translation = translator.translate(i, src=\"fr\",dest='en')\n#     translation2 = translator.translate(i, src=\"fr\",dest='zh-tw') \n#     print(f\"FR\\n{translation.origin}\\nEN\\n{translation.text}\\nCN\\n{translation2.text}\")","38637bf2":"<a id=\"E\"><\/a>\n# PART2 Word2Vector - Visualization of the word embedding space\nFor an efficient language learner, you not only peek at the popular vocabularies, but are curious which group of words are more related to each other, so that you're able to create your own mindmap and memorize them together.\n\nFor example, 'bread' is often related to 'butter', 'morning', 'eat', and etc. ","1c8425d4":"<a id=\"e5\"><\/a>\n## Apply Google translate API\nOne step forward, apply google translator to directly translate Animal Crossing's line can save your time for \"copy and paste\" while you're learning your favorite language.\nHowever, google translator api sometimes just didn't work on Kaggle enviornment. Keep the code here, just in case it works someday.","54127b23":"<a id=\"d2\"><\/a>\n## Use stopwords to clean context","fb2d278c":"It works! And it looks tidy.\nCan you pick up a few french words that you think they do related to each other?","292c7685":"### Enjoy your learning and playing in the Animal Crossing! \nUpvote if you like my project!","abfb8e01":"<a id=\"B\"><\/a>\n# Import Libaries","f59a3fd9":"The villigers are very polite! They often said 'Merci' (Thank you)!","bfa537c5":"<a id=\"d1\"><\/a>\n## Create my stopwords list\nIf the words repeat show more than 5 times, I define them as stopwords.","de48c54a":"<a id=\"e2\"><\/a>\n## Visualize word embedding","a5a6f0f7":"### Let's go ahead with Lily and her villigers!\n![villigers.jpg](attachment:373d4b42-7e38-44ce-90a7-a9251f261f88.jpg)","67a7cb0a":"<a id=\"e1\"><\/a>\n## Train Word2Vec model","267e5f2c":"<a id=\"D\"><\/a>\n# PART1 Wordcolud - Visualization of the word frequency\nAs an exciting language learner, I cannot wait to see what vocabularies I should be familiar with in the first place. Instead of reading the sentences line by line, taking the notes, and looking up the dictionaries, Wordcloud is going to help me find the popular vocabularies.","187ddbda":"<a id=\"e4\"><\/a>\n## Search the vocabulary in Animal Crossing lines\nFor the vocabulary I'm interested in, I navigate to the dataframe to understand the villiger's original saying.","f884cc86":"<a id=\"d3\"><\/a>\n## Create a general wordcloud","e042d117":"<a id=\"d4\"><\/a>\n##  Create a stopwords wordcloud\nVoil\u00e0! A wordcloud for Animal Crossing's high-frequencies words.","c64a41ff":"<a id=\"C\"><\/a>\n# Preprocess Aninmal Crossing Dataset\nAs mentioned before, the lines are generated by sreenshot and OCR so there are some garbled texts.\nIn the preprocess I write functions to clean the text as much as possible.","9f324367":"![2021103021463400_c.jpg](attachment:6e483b3c-7d4f-41ea-8520-03db10df5571.jpg)","42e0565b":"Well done. Wonder why 'souci'(worry) is most related with 'coincer'(stick)? \nIn the Animal Crossing world, the villigers talks quite jumply, from one topic suddenly to anoter. That's exactly how the game always brings you surprise!","bdaf4599":"Core = Red  \nLayer1 = Orange  \nLayer2 = Green  \nLayer3 = Blue","ff070c1c":"<a id=\"e3\"><\/a>\n## Recursively search Word2Vec words\nCheck out what vocabularies are surrounded by 'souci' (worry) in French!","dbb19276":"<a id=\"A\"><\/a>\n# Introduction\nThis is an in-game analysis of Animal Crossing New Horizons. I apply multiple Natural Language Processing(NLP) methods to analyze the villiger's conversations.  <br \/>\nThere are four sectoins in the notebook:  <br \/>\n#### Part1 Wordcolud - Visualization of the word frequency  <br \/>\n#### Part2 Word2vec - Visualization of the word embedding space  <br \/>\n#### Part3 Who said what - TF-IDF + SVM baseline Classification  <br \/>\n#### Part4 Can Villigers automatically talk - Use NLP with Rnns Tensorflow to Generate new lines  <br \/>\n\nGoing through Part1 and Part2, I want to validate if NLP can assist the palyers to easily acquired the vocabularies, especially for poeple who is learning secondary language through playing game. Taking me myself for example, I'm interested in learning french, so I want to quickly grasp new words in the dialogues.\n\nAs for Part3 and Part4, I want to study the narrative pattern of Animal Crossing. After all, this is a great game with delicated design between characters and their personalities. It will be great if Machine Learning can recognize the categories of how the eight types of personalities are expressed through the conversations. Lastly, I also wonder if based on the pattern, can the text generated new line like how the playwright create the lines.\n\n### Data Collection\nTo collect the dataset, I apply Optical Character Recognition (OCR) to reproduce the in-game screenshots on Switch to texts. \n\n### Table of Contents\n* [Introduction](#A)\n* [Import Libaries](#B)\n* [Preprocess Dataset](#C)\n* [Part1 Wordcolud - Visualization of the word frequency](#D)\n    - [Create my stopwords list](#d1)\n    - [Use stopwords to clean context](#d2)\n    - [Create a general wordcloud](#d3)\n    - [Create a stopwords wordcloud](#d4)\n* [Part2 Word2vec - Visualization of the word embedding space](#E)\n    - [Train Word2Vec model](#e1)\n    - [Visualize word embedding](#e2)\n    - [Recursively search Word2Vec words](#e3)\n    - [Search the vocabulary in Animal Crossing lines](#e4)\n    - [Apply Google translate API](#e5)\n\n\n\n\n### Progress (updated on 2022\/01\/16)\nPart 1 and Part 2 are updated.  <br \/>\nPart 3 and Part 4 are not yet updated.","7956afa5":"Excellent! I copy and paste the translation to understand what's my villiger's souci (worry).  \n##### ============\n### Souci\nB\u00ea\u00eatty: When I want to forget my worries, I immerse myself in a nice hot bath.  \nKolala: Everyone is on top form on the island, that's not the problem. I'm on the move, that's all. What do you think about it ?  \nRonchon: I like to lay outside, and take the time to look at the sky while my worries fly away.\n\n### Coiner\nFabien: It's happened to me before, you know! I got stuck in the toilet for days and days!  \n##### ============\nIt seems that the outcome are quite good becuase the words are likely to be used in similar daily scenarios. Goal acheived! I can learn french much faster with the help of NLP!"}}