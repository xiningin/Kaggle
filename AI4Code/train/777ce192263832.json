{"cell_type":{"bad54b4c":"code","168debb1":"code","c9ff9d04":"code","4c3e205b":"code","b0938ddd":"code","8f1e9f32":"code","dfb9462b":"code","4bb38c84":"code","8d41d3dc":"code","77e7888b":"code","102937ee":"code","56cfe386":"code","810744b3":"code","b66fbf81":"code","49716226":"code","4b33bb55":"code","02c391a2":"code","876f6d2b":"code","d8e52762":"code","54964f78":"code","e526f138":"code","eb959249":"code","0b8fc904":"code","ae55c554":"code","972be853":"code","e2649613":"code","4cbc92ce":"code","94c78138":"markdown","32186574":"markdown","5788254f":"markdown","862cff6b":"markdown","755c272a":"markdown","2af4c0ad":"markdown","622f8fc7":"markdown","26bc1598":"markdown","9d80acde":"markdown","708d0131":"markdown"},"source":{"bad54b4c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nimport datetime\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","168debb1":"df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()\ndf.head()","c9ff9d04":"df.shape","4c3e205b":"df.info()","b0938ddd":"pd.DataFrame(data=[[col,len(df[col].unique())] for col in df.columns],columns=['Feature','Unique Items']).style.background_gradient()","8f1e9f32":"df['YearsSinceBuilt'] = datetime.datetime.now().year - df.YearBuilt\ndf.drop('YearBuilt',axis=1,inplace=True)\ndf.head()","dfb9462b":"df.columns","4bb38c84":"cat_features = ['MSSubClass', 'MSZoning','Street','LotShape', ]","8d41d3dc":"lblEncoders = {}\nfor feature in cat_features:\n    lblEncoders[feature] = LabelEncoder()\n    df[feature] = lblEncoders[feature].fit_transform(df[feature])","77e7888b":"catTensor = np.stack([df[col] for col in cat_features],1)\ncatTensor = torch.tensor(catTensor,dtype=torch.int64)\ncatTensor","102937ee":"cont_features = [col for col in df.columns if col not in cat_features and col!= 'SalePrice']\ncont_features ","56cfe386":"contTensor = np.stack([df[col] for col in cont_features],axis=1)\ncontTensor = torch.tensor(contTensor,dtype=torch.float)\ncontTensor","810744b3":"y = torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\ny","b66fbf81":"cat_dims = [len(df[col].unique()) for col in cat_features]\nembedding_dims = [(dim,min(50,(dim+1)\/\/2)) for dim in cat_dims]\nembedding_dims","49716226":"embed_repr = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\nembed_repr","4b33bb55":"embed_values = []\nfor i,e in enumerate(embed_repr):\n    embed_values.append(e(catTensor[:,i]))","02c391a2":"embed_values = torch.cat(embed_values,1)\nembed_values","876f6d2b":"embed_values = nn.Dropout(.4)(embed_values)\nembed_values","d8e52762":"class Model(nn.Module):\n    def __init__(self, embedding_dim, n_cont, out_sz, layers, drop=0.5):\n        super().__init__()\n        self.embed_repr = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dims])\n        self.embed_dropout = nn.Dropout(drop)\n        self.bn_cont = nn.BatchNorm1d(n_cont)\n        \n        layerlist = []\n        n_emb = sum((val[1] for val in embedding_dim))\n        n_in = n_cont + n_emb\n        \n        for layer in layers:\n            layerlist.append(nn.Linear(n_in,layer))\n            layerlist.append(nn.ReLU(inplace=True))\n            layerlist.append(nn.BatchNorm1d(layer))\n            layerlist.append(nn.Dropout(drop))\n            n_in = layer\n        layerlist.append(nn.Linear(layers[-1],out_sz))\n        \n        self.layers = nn.Sequential(*layerlist)\n        \n    def forward(self, cat,cont):\n        embeddings = []\n        for i,e in enumerate(self.embed_repr):\n            embeddings.append(e(cat[:,i]))\n        x = torch.cat(embeddings,1)\n        x = self.embed_dropout(x)\n        x_cont = self.bn_cont(cont)\n        x = torch.cat([x,x_cont],1)\n        x = self.layers(x)\n        return x","54964f78":"torch.manual_seed(100)\nmodel = Model(embedding_dims, len(cont_features), 1, [100,50], drop = .4)","e526f138":"model","eb959249":"loss_function = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr =0.01)","0b8fc904":"batch_size = len(df)\ntest_size = int(batch_size*.15)\ntrain_cat = catTensor[:batch_size-test_size]\ntest_cat = catTensor[batch_size-test_size:batch_size]\ntrain_cont = contTensor[:batch_size-test_size]\ntest_cont = contTensor[batch_size-test_size:batch_size]\ny_train = y[:batch_size-test_size]\ny_test = y[batch_size-test_size:batch_size]","ae55c554":"len(test_cat)","972be853":"epochs = 5000\nlosses = []\nfor i in range(epochs):\n    i += 1\n    y_pred = model.forward(train_cat,train_cont)\n    loss = torch.sqrt(loss_function(y_pred,y_train))\n    losses.append(loss)\n    if i%50 == 0:\n        print(f\"Epoch {i} : {loss}\")\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()","e2649613":"plt.plot(range(epochs), losses)\nplt.ylabel('RMSE Loss')\nplt.xlabel('epoch')","4cbc92ce":"with torch.no_grad():\n    y_pred=model(test_cat,test_cont)\n    loss=torch.sqrt(loss_function(y_pred,y_test))\nprint('RMSE: {}'.format(loss))","94c78138":"# Data Preprocessing","32186574":"##### Adding Regularization to Embeddings using Dropout","5788254f":"# Model Training","862cff6b":"##### Dependant Feature","755c272a":"### Unique Value Count","2af4c0ad":"## Handling Categorical Features","622f8fc7":"# Building Neural Network Architecture","26bc1598":"# Categorical Embeddings \nAt most 50 categorical features to be implemented, thus we are following `min(50 , (dimension+1)\/\/2)` Rule","9d80acde":"## Handling Continuos Features","708d0131":"# House Prices: Advanced Regression Techniques\n\nWith the following notebook I have implemented a Neural Network that creates Embeddings out of Categorical Features proven to improve performance."}}