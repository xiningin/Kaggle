{"cell_type":{"84ceab92":"code","cd1fe348":"code","12258cd2":"code","31543225":"code","53d4eaa1":"code","93681c19":"code","a0203ab7":"code","37e66afa":"code","bb44389d":"code","bc1e8c29":"code","27d77bde":"code","b8bba830":"code","80ce75d7":"code","3324b4f1":"code","c26caff8":"code","4e661355":"code","9fe05565":"code","27825c56":"code","8c0a3948":"code","4e8fc21c":"code","0e47c128":"code","10396ce5":"code","b01acfad":"code","3a37a607":"code","a8d3d8f8":"code","6628a525":"code","f9f95289":"code","3d6c835d":"code","ff97acef":"code","b6c6f5ce":"code","4f584624":"code","9b5c141c":"code","a5e00c7a":"markdown","bc7e166d":"markdown","5a809a9b":"markdown","77673292":"markdown","c54909d6":"markdown","8dc337aa":"markdown","7294e3d5":"markdown","28bdc725":"markdown","abd579bc":"markdown","49cc956d":"markdown","b0d8ea91":"markdown","acd7bd53":"markdown","1e368fd2":"markdown","cbe2aabc":"markdown","e0340a87":"markdown","5d6aed8d":"markdown","dcf0bf52":"markdown","2a4262d6":"markdown"},"source":{"84ceab92":"# Some mandatory Libraries\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\n\n#EDA Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n# Classifier Libraries\nfrom sklearn import linear_model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve\n\n# Other Libraries\nfrom tqdm import tqdm\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","cd1fe348":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12258cd2":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","31543225":"print(df.columns)","53d4eaa1":"print('Our total data point: ', df.shape)\n\n# Total data points are 284807","93681c19":"df.describe()","a0203ab7":"df.isnull().sum()\n\n# No null values present in data frame.","37e66afa":"# The classes are heavily skewed we need to solve this issue later.\nprint('No. of frauds in persentage: ', round(df['Class'].value_counts()[0]\/len(df) * 100,2))\nprint('No. of no frauds in persentage: ', round(df['Class'].value_counts()[1]\/len(df) * 100,2))","bb44389d":"ax = sns.countplot(x='Class', data=df, facecolor=(0, 0, 0, 0), linewidth=5, edgecolor=sns.color_palette(\"dark\", 3))","bc1e8c29":"corr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nwith sns.axes_style(\"white\"):\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(15,10))\n    ax = sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","27d77bde":"# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x='Class', y='V2', data=df, ax=axes[0])\naxes[0].set_title('V2 vs Class')\n\nsns.boxplot(x='Class', y='V4', data=df, ax=axes[1])\naxes[1].set_title('V4 vs Class')\n\nsns.boxplot(x='Class', y='V11', data=df, ax=axes[2])\naxes[2].set_title('V11 vs Class')\n\nsns.boxplot(x='Class', y='V19', data=df, ax=axes[3])\naxes[3].set_title('V19 vs Class')\n\nplt.show()","b8bba830":"# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n\nf, axes = plt.subplots(ncols=4, figsize=(20,4))\n\nsns.boxplot(x='Class', y='V10', data=df, ax=axes[0])\naxes[0].set_title('V10 vs Class')\n\nsns.boxplot(x='Class', y='V12', data=df, ax=axes[1])\naxes[1].set_title('V12 vs Class')\n\nsns.boxplot(x='Class', y='V14', data=df,  ax=axes[2])\naxes[2].set_title('V14 vs Class')\n\nsns.boxplot(x='Class', y='V17', data=df, ax=axes[3])\naxes[3].set_title('V17 vs Class')\n\nplt.show()","80ce75d7":"#cols = ['V10','V11','V12','V14']\n\n#for col in cols:\n    #Q1 = df[col].quantile(0.25)\n    #Q3 = df[col].quantile(0.75)\n    #IQR = Q3 - Q1\n    \n    # lower and upper limits\n    #low_lim = Q1 - 1.5 * IQR \n    #up_lim = Q3 + 1.5 * IQR\n    \n    #true_ind = ((df[col] > low_lim) & (df[col] < up_lim))\n    #false_ind = ~true_ind\n\n    #df[col][false_ind] = np.median(df[col])","3324b4f1":"print('Our current dataset shape based on class is', df[df['Class'] == 0].shape,' and ',df[df['Class'] == 1].shape)","c26caff8":"# Class count\ncount_class_0, count_class_1 = df.Class.value_counts()\n\n# Divide by class\ndf_0 = df[df['Class'] == 0]\ndf_1 = df[df['Class'] == 1]\n\n# Create new dataset\ndf_1_over_sampled = df_1.sample(count_class_0, replace=True)\ndf_final = pd.concat([df_0, df_1_over_sampled], axis=0)\n\nprint('New dataset contain: ', df_final[df_final['Class'] == 0].shape, df_final[df_final['Class'] == 1].shape)","4e661355":"print('Distribution of the Classes in the subsample dataset')\nprint(df_final['Class'].value_counts()\/len(df_final))","9fe05565":"# New_df is from the random undersample data (fewer instances)\n\nX = df_final.drop('Class', axis=1)\nY = df_final['Class']","27825c56":"#X_reduced_tsne = TSNE(n_components=2, random_state=1).fit_transform(X)\n#X_reduced_pca = PCA(n_components=2, random_state=1).fit_transform(X)","8c0a3948":"#f, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,6))\n#f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n#blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n#red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n## t-SNE scatter plot\n#ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n#ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n#ax1.set_title('t-SNE', fontsize=14)\n#ax1.grid(True)\n#ax1.legend(handles=[blue_patch, red_patch])\n\n## PCA scatter plot\n#ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n#ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n#ax2.set_title('PCA', fontsize=14)\n#ax2.grid(True)\n#ax2.legend(handles=[blue_patch, red_patch])\n#plt.show()","4e8fc21c":"# Before StandardScaler:\n\nf, axes = plt.subplots(ncols=2, figsize=(10,4))\n\nsns.distplot(df_final['Amount'], kde=False, color=\"g\", ax=axes[0])\n\nsns.distplot(df_final['Time'], kde=False, color=\"g\", ax=axes[1])\n\nplt.tight_layout()","0e47c128":"# Create the Scaler object\nscaler = StandardScaler()\n\n# Fit and Transform Amount\nscaler.fit(df_final['Amount'].values.reshape(-1, 1))\ndf_final['std_Amount'] = scaler.transform(df_final['Amount'].values.reshape(-1, 1))\n\n# Fit and Transform Time\nscaler.fit(df_final['Time'].values.reshape(-1, 1))\ndf_final['std_Time'] = scaler.transform(df_final['Time'].values.reshape(-1, 1))\n\n# Delete old features\ndf_final.drop(['Time','Amount'], axis=1, inplace=True)","10396ce5":"# After StandardScaler:\n\nf, axes = plt.subplots(ncols=2, figsize=(10,4))\n\nsns.distplot(df_final['std_Amount'], kde=False, color=\"g\", ax=axes[0])\n\nsns.distplot(df_final['std_Time'], kde=False, color=\"g\", ax=axes[1])\n\nplt.tight_layout()","b01acfad":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=22)","3a37a607":"# Define classifiers with default parameters.\n\nclassifiers = {\n    \"Logisitic Regression\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(),\n    \"Linear Regression\":linear_model.LinearRegression(),\n    \"Gaussian NB\": GaussianNB()\n}","a8d3d8f8":"for name, classifier in classifiers.items():\n    classifier.fit(X_train, y_train) \n    training_score = cross_val_score(classifier, X_train, y_train, cv=3)\n    print('Classifiers: ',name, 'has a training score of', round(training_score.mean(),2) * 100)","6628a525":"# LogisticRegression\nparams = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngs = GridSearchCV(LogisticRegression(), params, cv = 3, n_jobs=-1)\ngs_results = gs.fit(X_train, y_train)\n\nlog_reg = gs.best_estimator_ # store best estimators for future analysis\n\nprint('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","f9f95289":"# KNN\nparams = {'n_neighbors':list(range(1, 50, 2)), 'weights':['uniform', 'distance']}\n\ngs = GridSearchCV(KNeighborsClassifier(), params, cv = 3, n_jobs=-1)\ngs_results = gs.fit(X_train, y_train)\n\nknears_neighbors = gs.best_estimator_ # store best estimators for future analysis\n\nprint('Best Accuracy: ', gs_results.best_score_)\nprint('Best Parametrs: ', gs_results.best_params_)","3d6c835d":"log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=3)\nprint('LR CV Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=3)\nprint('KNN CV Score: ', round(knears_score.mean() * 100, 2).astype(str) + '%')","ff97acef":"model = LogisticRegression(penalty = 'l2', C = 0.1)\nmodel.fit(X_train,y_train)\npridict = model.predict(X_test)\nprint('Accuracy: ', accuracy_score(y_test, pridict))","b6c6f5ce":"cm = confusion_matrix(y_test, pridict)\nlabels = ['No Fraud', 'Fraud']\nprint(pd.DataFrame(cm, index=labels, columns=labels))","4f584624":"lr_pred_prob = model.predict_proba(X_test)[:,1]\nfpr,tpr,thrsld = roc_curve(y_test,lr_pred_prob)\nprint('AUC score:',roc_auc_score(y_test,lr_pred_prob))","9b5c141c":"# Ploting ROC Curve \n\nplt.figure(figsize=(7,5))\nplt.plot([0,1],[0,1])\nplt.plot(fpr,tpr,':', color='red')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.grid()","a5e00c7a":"### [4.4] Confusion Matrix:","bc7e166d":"## [03] Data Preprocessing:","5a809a9b":"### [4.2] Model Training:","77673292":"We can see here in these features have some extream outliers.\n\n### [3.2] Dimensionality Reduction:","c54909d6":"### [3.1] Random Oversampleing:","8dc337aa":"As we know KNN impact allot by outliers so let's go with the LR and check our test accuracy.\n\n### Accuracy on Test Data:","7294e3d5":"### [4.5] ROC Curve:","28bdc725":"### [3.1] Remove Outlers:","abd579bc":"### Some insights from above metrice:\n\n* Now let's analyse each features (except Amount & Time) by devideing them w.r.t possitive and negative corelation. \n* These features can be use for reduce the dimentions. \n* Here we can use box plot to measure outliers and distributions at a time.","49cc956d":"### [4.1] Train Test Split:\n\n__Note:__ Accuracy score might not work as expected as we have some extream outliers and immabalced nature of data.","b0d8ea91":"Here we can see we have some extream outliers in V10, V11, V12, V14. We will remove those when we preprocess our data.","acd7bd53":"## [04] Classifiers:","1e368fd2":"## [01] Read Data:","cbe2aabc":"## [02] EDA:\n\nHere let's find some insights about our data.","e0340a87":"**Note:** \n\nNotice how imbalanced is our original dataset! Most of the transactions are non-fraud. If we use this dataframe as the base for our models and analysis we might get miss intiution.","5d6aed8d":"# Problem Statement:\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.","dcf0bf52":"KNN's test accuracy is quite good in compare to other. Let's try to improve LR model using hyper parameter tuning.\n\n### [4.3] Optomise Parameters:","2a4262d6":"### [3.3] StandardScaler:"}}