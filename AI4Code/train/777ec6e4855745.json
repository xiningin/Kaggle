{"cell_type":{"5f6eb800":"code","3b6d30a7":"code","178032d9":"code","1bce6c23":"code","b799d294":"code","8d9686f5":"code","ac98dd01":"code","7802ea80":"code","fef57909":"code","4473581b":"code","10340fbd":"code","78aa8f5d":"code","b593832f":"code","3d3b97cb":"code","f1e74200":"code","c35b54aa":"code","783fb454":"code","215ad9c4":"code","7befd98e":"code","a3c7da49":"code","08485787":"code","95f798c4":"code","f3d94f65":"code","39958b2d":"code","790e543e":"code","3b52a506":"code","b1081473":"code","0f2b53db":"code","0c56e53a":"code","aa99d30c":"code","5bb1f9c3":"code","f841d6af":"code","4b4a8021":"code","80732bd3":"code","440c83cc":"code","c8f33aba":"code","4d0637dd":"code","bf9b16ec":"code","fd20142a":"code","ce0c8cb0":"code","08517478":"code","f575f962":"code","86a25f3d":"code","fa7cc94b":"code","e5791372":"markdown","9ade856c":"markdown","bd3983e5":"markdown","e34a98bb":"markdown","77ef7f14":"markdown","37f98a92":"markdown","c7c23fa2":"markdown","1412ba7d":"markdown","09841b3e":"markdown","2f743804":"markdown","1089f884":"markdown","4fe570f1":"markdown","c8d27a6f":"markdown","106b2b4d":"markdown","1273fdf8":"markdown","b36f6835":"markdown","d01772ab":"markdown","2b6c1712":"markdown","d249d50e":"markdown","b9a13765":"markdown","f0e68016":"markdown","dec331a4":"markdown","aa833c55":"markdown","13ab555a":"markdown","ec0bad27":"markdown","f5f2d7b4":"markdown","2689f952":"markdown","61f5967b":"markdown","31e62e2a":"markdown","c4b4300c":"markdown","8a845c9b":"markdown","d0dfb046":"markdown","0024cd4c":"markdown","86320b40":"markdown","48420cff":"markdown","e1bc366a":"markdown","126ced4b":"markdown","a9fe1421":"markdown","20f12cbb":"markdown","ee005962":"markdown","dfed76e4":"markdown","f3fb11ce":"markdown","d89bcfc3":"markdown","a9b1e47d":"markdown","c6a2ed04":"markdown","bb5c479a":"markdown","2702e3ea":"markdown"},"source":{"5f6eb800":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline","3b6d30a7":"# Importing the dataset\ndf_Train = pd.read_csv(r\"..\/input\/titanic\/train.csv\")\ndf_Test = pd.read_csv(r\"..\/input\/titanic\/test.csv\")\n\n# Dataset Information\ndf_Train.info()","178032d9":"# First DataFrame rows\ndf_Train.head(10)","1bce6c23":"# Describing The Data\ndf_Train.describe()","b799d294":"# Missing Values Training Set\ndf_Train.isnull().sum()","8d9686f5":"def autolabel(patches,ax,mode):\n    if mode == 'percentage':\n        \"\"\"Display Percentage\"\"\"\n        for j in range(len(patches)):\n            rects = patches[j]\n            height = rects.get_height()\n            percentage = '{:.1f}%'.format(rects.get_height())       \n            ax.annotate(percentage,\n                        xy=(rects.get_x() + rects.get_width() \/ 2, height),\n                        xytext=(0, 0.5),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')            \n    elif mode == 'count':\n        \"\"\"Display Count\"\"\"\n        for j in range(len(patches)):\n            rects = patches[j]\n            height = rects.get_height().astype('int')   \n            height = height if height >= 0 else -1 # To avoid error\n            ax.annotate(height,\n                        xy=(rects.get_x() + rects.get_width() \/ 2, height),\n                        xytext=(0, 0.5),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')         \n               \ndef autoplot(X,hue,data,colors,labels):\n    fig, ax = plt.subplots(1,2,figsize=(15, 10))\n    \n    plt.subplot(1,2,1)\n    ax[0] = sns.barplot(x=X.value_counts().index,\n                        y=(X.value_counts()\/len(X))*100,\n                        data=data,palette='Blues_d')    \n    ax[0].set_xlabel(X.name,fontsize=13)\n    ax[0].set_ylabel(\"Percentage\",fontsize=13)\n    autolabel(ax[0].patches,ax[0],'percentage')\n    \n    plt.subplot(1,2,2)\n    ax[1] = sns.countplot(x=X,hue=hue,data=df_Train,palette=colors)\n    ax[1].set_ylabel(\"Number of Occurrences\",fontsize=13)\n    ax[1].set_xlabel(X.name,fontsize=13)\n    ax[1].legend(title=hue.name,labels=labels,fontsize=12,title_fontsize=12,loc='upper right')\n    autolabel(ax[1].patches,ax[1],'count')        ","ac98dd01":"# Constants that we will use later       \ncolors1 =['#C03028','#78C850']#Survived: No\/Yes\nlabels1 = ['No','Yes']\ncolors2 = ['#6890F0','#F85888']#Sex: Male\/Female\nlabels2 = ['Male','Female']\ncolors3 = ['#78C850','#34495e','#e74c3c']#Pclass: 1\/2\/3\nlabels3 = [1,2,3]","7802ea80":"# Survived\nSurvived = pd.crosstab(df_Train['Survived'],df_Train['Survived']).sum()\nfig, ax = plt.subplots(figsize=(5, 5))\nax.pie(Survived, labels=Survived.index, autopct='%1.1f%%',colors=colors1)\nplt.legend(title='Survived',labels=['0: No','1: Yes'],fontsize=10,title_fontsize=10)","fef57909":"# Sex - Survived\nautoplot(df_Train['Sex'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Sex'], df_Train['Survived']).apply(lambda r: r\/r.sum(),axis=1)","4473581b":"# Embarked\n# Taking care the 2 missing values of the feature Embarked by filling the most common value which the port of S: Southampton \ndf_Train['Embarked'].value_counts()\ndf_Train['Embarked'].isnull().sum()\ndf_Train['Embarked'].fillna('S',inplace=True)","10340fbd":"# Embarked - Survived\nautoplot(df_Train['Embarked'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Embarked'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","78aa8f5d":"# Pclass - Survived\nautoplot(df_Train['Pclass'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Pclass'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","b593832f":"# Embarked - Pclass\nautoplot(df_Train['Embarked'],df_Train['Pclass'],df_Train,colors3,labels3)","3d3b97cb":"# SibSp - Survived\nautoplot(df_Train['SibSp'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['SibSp'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","f1e74200":"# Parch\nautoplot(df_Train['Parch'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Parch'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","c35b54aa":"# Fare\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['Fare'], ax=ax1)\nsns.boxplot(df_Train['Fare'], ax=ax2)\nprint('Mean Fare = %0.2f\\nMedian Fare = %0.2f' % (df_Train['Fare'].mean(),df_Train['Fare'].median()))","783fb454":"# Fare - Survived\nax = sns.FacetGrid(df_Train, hue='Survived',palette=colors1,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"Fare\",shade= True)\nax.fig.legend(title='Survived',labels=['No','Yes'],fontsize=12,title_fontsize=12)","215ad9c4":"# Fare - Pclass\nax = sns.FacetGrid(df_Train, hue='Pclass',palette=colors3,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"Fare\",shade= True)\nax.fig.legend()\n\nfig, ax = plt.subplots()\nax = sns.boxplot(x='Pclass', y='Fare', data=df_Train)\n\nF_1 = df_Train['Fare'][df_Train['Pclass'] == 1].mean()\nF_2 = df_Train['Fare'][df_Train['Pclass'] == 2].mean()\nF_3 = df_Train['Fare'][df_Train['Pclass'] == 3].mean()\nprint('Mean Fare Pclass 1: %0.1f \\nMean Fare Pclass 2: %0.1f \\nMean Fare Pclass 3: %0.1f' % (F_1,F_2,F_3))","7befd98e":"# Fare binned\nFare_binned = pd.Series.copy(df_Train['Fare'])\nFare_binned = pd.cut(Fare_binned,8)\nlabels = ['0-64','64-128','128-192','192-256','256-320','320-384','384-448','448-512']\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.barplot(x=Fare_binned.value_counts().index,y=Fare_binned.value_counts(),data=df_Train)\nax.set_xlabel('Fare',fontsize=13)\nax.set_ylabel(\"Number of Occurrences\",fontsize=13)\nax.set_xticklabels(labels)\nautolabel(ax.patches,ax,'count')","a3c7da49":"# Age\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['Age'], ax=ax1)\nsns.boxplot(df_Train['Age'], ax=ax2)\nprint('Mean Age = %0.2f\\nMedian Age = %0.2f' % (df_Train['Age'].mean(),df_Train['Age'].median()))\nprint('Age Skew: %0.2f' % (df_Train['Age'].skew()))","08485787":"# Age binned\nAge_binned = pd.Series.copy(df_Train['Age'])\nAge_binned = pd.cut(Age_binned,8)\nlabels = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80']\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.barplot(x=Age_binned.value_counts().index,y=Age_binned.value_counts(),data=df_Train)\nax.set_xlabel('Age',fontsize=13)\nax.set_ylabel(\"Number of Occurrences\",fontsize=13)\nax.set_xticklabels(labels)\nautolabel(ax.patches,ax,'count')","95f798c4":"# Age - Survived\nax = sns.FacetGrid(df_Train, hue='Survived',palette=colors1,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"Age\",shade=True)\nax.fig.legend(title='Survived',labels=['No','Yes'],fontsize=12,title_fontsize=12)","f3d94f65":"# Age - Survived Boxplot\nfig, ax = plt.subplots()\nax = sns.boxplot(x='Survived', y=df_Train['Age'], data=df_Train)\nQ1 = df_Train['Age'].quantile(0.25)\nQ3 = df_Train['Age'].quantile(0.75)\nIQR = Q3 - Q1\n\nth1 = Q1 - 1.5 * IQR\nth2 = Q3 + 1.5 * IQR\nfliers = df_Train['Age'][(df_Train['Age'] < th1) | (df_Train['Age'] > th2)]\n\nA_0 = df_Train['Age'][df_Train['Survived'] == 0].mean()\nA_1 = df_Train['Age'][df_Train['Survived'] == 1].mean()\nprint('Mean Age Not Survived: %0.1f \\nMean Age Survived: %0.1f' % (A_0,A_1))","39958b2d":"# Age - Sex \nfig, ax = plt.subplots()\nax = sns.boxplot(x='Sex', y=df_Train['Age'], data=df_Train)\n\nF = df_Train['Age'][df_Train['Sex'] == 'female'].mean()\nM = df_Train['Age'][df_Train['Sex'] == 'male'].mean()\nprint('Mean Age Men: %0.1f \\nMean Age Women: %0.1f' % (M,F))","790e543e":"# Age - Sex - Survived\nax = sns.FacetGrid(df_Train,col='Sex', hue='Survived',palette=colors1,height=5)\nax = ax.map(sns.kdeplot, \"Age\",shade= True)\n#ax = ax.map(sns.distplot, \"Age\")\nax.fig.legend(title='Survived',labels=['No','Yes'],fontsize=12,title_fontsize=12)","3b52a506":"# Age - Pclass Boxplot\nfig, ax = plt.subplots()\nax = sns.boxplot(x='Pclass', y='Age',hue='Survived', data=df_Train,palette=colors1)\n\nfor i in range(df_Train['Pclass'].nunique()):\n    print('Mean Age Pclass %i: %0.1f' %\n       (i+1,df_Train['Age'][(df_Train['Pclass'] == i+1)].mean()))","b1081473":"# Family Feature: Combine the features of SibSp and Parch\ndf_Train[\"Family\"] = df_Train[\"SibSp\"] + df_Train[\"Parch\"]\ndf_Test[\"Family\"] = df_Test[\"SibSp\"] + df_Test[\"Parch\"]\n\n# Family size above 3 will be signed the number 4\ndf_Train[\"Family\"][(df_Train[\"Family\"] > 3)] = 4\ndf_Test[\"Family\"][(df_Test[\"Family\"] > 3)] = 4\n\n# Family\nautoplot(df_Train['Family'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Family'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","0f2b53db":"# Age - Family - Survived Boxplot\nfig, ax = plt.subplots()\nax = sns.boxplot(x='Family', y='Age',hue='Survived', data=df_Train,palette=colors1)\n\nfor i in range(df_Train['Family'].nunique()):\n    print('Mean Age Family %i: %0.1f' %\n       (i,df_Train['Age'][(df_Train['Family'] == i)].mean()))","0c56e53a":"# Cabin feature has 1014 missing values (both Train and Test) and takes alphanumeric values e.g.\"C23 C25 C27\"\n# We create the Deck feature for both Train Set and Test Set by extracting the first letter of the Cabin feature\n# and filling missing information with the letter U: Unknown.\n# Deck Feature\ndf_Train['Deck'] = df_Train['Cabin'].str[:1]\ndf_Test['Deck'] = df_Test['Cabin'].str[:1]\n\ndf_Train['Deck'].isna().sum()\ndf_Test['Deck'].isna().sum()\ndf_Train['Deck'] = df_Train['Deck'].fillna('U')\ndf_Test['Deck'] = df_Test['Deck'].fillna('U')\n\n# Deck\nautoplot(df_Train['Deck'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Deck'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","aa99d30c":"# Name Feature includes the first name, last name and title information of each passenger.\n# We create the Title feature which has meaning for our model by extracting the title info for each passenger\n# Title takes 17 unique values, we keep the 4 most frequent ones and name all the others with the Title: \"Other\"\ndf_Train['Title'] = df_Train['Name'].str.split(', ').str[1].str.split('.').str[0]\ndf_Test['Title'] = df_Test['Name'].str.split(', ').str[1].str.split('.').str[0]\n\ndf_Train['Title'].value_counts()\n\ndf_Train[\"Title\"] = df_Train[\"Title\"].replace(['Dr', 'Rev','Col','Major', 'Col','Jonkheer','Dona','Don',\n                                               'the Countess', 'Lady', 'Capt', 'Sir','Ms','Mlle','Mme'], 'Other')\ndf_Test[\"Title\"] = df_Test[\"Title\"].replace(['Dr', 'Rev','Col','Major', 'Col','Jonkheer','Dona','Don',\n                                               'the Countess', 'Lady', 'Capt', 'Sir','Ms','Mlle','Mme'], 'Other')\n\n# Title\nautoplot(df_Train['Title'],df_Train['Survived'],df_Train,colors1,labels1)\npd.crosstab(df_Train['Title'], df_Train['Survived']).apply(lambda r: r\/r.sum(), axis=1)","5bb1f9c3":"# Drop no needed columns\ndf_Train = df_Train.drop(columns=['PassengerId','Name','SibSp','Parch','Ticket','Cabin'])\n\n# Dataset split to Categorical (Nominal,Ordinal,Binary) and Numeric Vars\ndf_Cat_Bin = df_Train[['Sex']].iloc[:]\ndf_Cat_Nom = df_Train[['Embarked','Title','Deck','Family']].iloc[:]\ndf_Cat_Ord = df_Train[['Pclass']].iloc[:]\ndf_Num = df_Train[['Age','Fare']].iloc[:]\n\n# Categorical Output\ny = df_Train['Survived'].iloc[:]","f841d6af":"# LABEL ENCODING - ONE HOT ENCODING CATEGORICAL FEATURES\n\n# Categorical Binary Features Encoding\ndf_Cat_Bin_Ld = df_Cat_Bin.replace({'male': 0, 'female': 1})\n\n# Categorical Nominal Features Encoding\ndf_Cat_Nom_OHEd = pd.get_dummies(df_Cat_Nom.astype('str'))\n\n# Categorical Ordinal Features Encoding\ndf_Cat_Ord_OHEd = pd.get_dummies(df_Cat_Ord.astype('str'))\n\n# All Categorical Features\ndf_Cat = pd.concat([df_Cat_Bin_Ld,df_Cat_Nom_OHEd,df_Cat_Ord_OHEd],axis=1)","4b4a8021":"# ALL the Selected IVs\nX = pd.concat([df_Num,df_Cat],axis=1)\ncolumns=X.columns\nX.head(10)","80732bd3":"# Correlation Matrix\ncorr = X.corr()\nplt.figure(figsize = (15,15))\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,cmap = \"coolwarm\",annot=True,annot_kws = {'size': 6})\nplt.title(\"Correlation\")\nplt.show()","440c83cc":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c8f33aba":"# Age Imputing: Taking care of missing values\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=3)\nX_train = pd.DataFrame(data = imputer.fit_transform(X_train),\n                                 columns = X_train.columns,\n                                 index = X_train.index)\nX_test = pd.DataFrame(data = imputer.fit_transform(X_test),\n                                 columns = X_test.columns,\n                                 index = X_test.index)","4d0637dd":"# Fix Fare skewness\nprint('Fare Train Skew: %0.2f \\nFare Test Skew: %0.2f' %\n       (X_train['Fare'].skew(),X_test['Fare'].skew()))\n\n# Using log\nX_train['Fare'] = np.log(X_train['Fare'] + 1)\nX_test['Fare'] = np.log(X_test['Fare'] + 1)\n\nprint('Fare Train Fixed Skew: %0.2f \\nFare Test fixed Skew: %0.2f' %\n       (X_train['Fare'].skew(),X_test['Fare'].skew()))","bf9b16ec":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","fd20142a":"# Choosing Classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier","ce0c8cb0":"# Used GridSearchCV for parameter tuning\nCF = [None]*9\nNames = ['Logistic Regression','SVM linear','SVM rbf','Naive Bayes','kNN','Decision Tree','Random Forest','Gradient Boosting','Ada Boost']\nCF[0] = LogisticRegression(penalty='l2',solver='newton-cg')\nCF[1] = SVC(kernel = 'linear', random_state = 0,probability=True)\nCF[2] = SVC(kernel = 'rbf', random_state = 0,probability=True)\nCF[3] = GaussianNB()\nCF[4] = KNeighborsClassifier(n_neighbors=15,leaf_size=10,metric='manhattan')\nCF[5] = DecisionTreeClassifier(max_depth = 3, min_samples_split = 4,min_samples_leaf=2,random_state = 0)\nCF[6] = RandomForestClassifier(n_estimators=300,max_depth=5,min_samples_split=7,min_samples_leaf=2,random_state = 0)\nCF[7] = GradientBoostingClassifier(learning_rate=0.05,min_samples_leaf=2,random_state = 0)\nCF[8] = AdaBoostClassifier(random_state = 0)","08517478":"# Classification Metrics\nClassifiers = ['Logistic Regression','SVM linear','SVM rbf','Naive Bayes','k-NN','Decision Tree','Random Forest','Gradient Boosting','Ada Boost']\nCols = ['Accuracy','Recall','Precision','f1 score','AUC ROC score']\nScores = pd.DataFrame(index=Classifiers,columns=Cols).astype('float')\nfor i in range(len(CF)):\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)\n    c_probs = classifier.predict_proba(X_test)\n    c_probs = c_probs[:, 1]\n    \n    y_pred = classifier.predict(X_test)\n    \n    from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_auc_score\n    Scores.Accuracy[i] = accuracy_score(y_test,y_pred)\n    Scores.Recall[i] = recall_score(y_test,y_pred)\n    Scores.Precision[i] = precision_score(y_test,y_pred)\n    Scores['f1 score'][i] = f1_score(y_test,y_pred)\n    Scores['AUC ROC score'][i] = roc_auc_score(y_test,c_probs)\n    \nprint(Scores)","f575f962":"# Feature Importance plots\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    classifier = CF[i+5]\n    classifier.fit(X_train, y_train)     \n\n    FImportances = pd.DataFrame(data=classifier.feature_importances_,index=columns,columns=['Importance']).sort_values(by=['Importance'])\n    plt.barh(range(FImportances.shape[0]),FImportances['Importance'],color = '#78C850')\n    plt.yticks(range(FImportances.shape[0]), FImportances.index)\n    plt.title('Feature Importances: %s' % (Names[i+5]))","86a25f3d":"# ROC - Curves for models\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)    \nfor i in range(len(CF)):\n    plt.subplot(3, 3, i+1)\n    #i=4\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)  \n     \n    # Predict probabilities\n    r_probs = [0 for _ in range(len(y_test))]\n    c_probs = classifier.predict_proba(X_test)\n\n    # Keep probabilities for the positive outcome only\n    c_probs = c_probs[:, 1]\n\n    # Calculate AUROC\n    from sklearn.metrics import roc_curve, roc_auc_score, auc\n    r_auc = roc_auc_score(y_test, r_probs)\n    c_auc = roc_auc_score(y_test, c_probs)\n    #print('Random (chance) Prediction: AUROC = %.3f' % (r_auc))\n    #print('%s: AUROC = %.3f' % (Names[i],c_auc))\n\n    # Calculate ROC curve\n    r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n    c_fpr, c_tpr, _ = roc_curve(y_test, c_probs)\n    plt.plot(r_fpr, r_tpr, linestyle='--',c='r', label='Random Prediction (AUROC = %0.3f)' % r_auc)\n    plt.plot(c_fpr, c_tpr, marker='.',c='b', label='%s (AUROC = %0.3f)' % (Names[i],c_auc))\n\n    plt.title('ROC Plot')\n    plt.xlabel('False Positive Rate - 1 - Specificity')\n    plt.ylabel('True Positive Rate - Sensitivity')\n    plt.legend(fontsize='small')","fa7cc94b":"# Cap Curve\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)    \nfor i in range(len(CF)):\n    plt.subplot(3, 3, i+1)\n    \n    total = len(y_test)\n    class_1_count = np.sum(y_test)\n    class_0_count = total - class_1_count\n\n    plt.plot([0, total], [0, class_1_count], c = 'r', linestyle = '--', label = 'Random Model')\n\n    plt.plot([0, class_1_count, total], \n             [0, class_1_count, class_1_count], \n             c = 'grey', linewidth = 2, label = 'Perfect Model')\n\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)  \n    c_probs = classifier.predict_proba(X_test)\n\n    # Keep probabilities for the positive outcome only\n    c_probs = c_probs[:, 1]\n\n    model_y = [y for _, y in sorted(zip(c_probs, y_test), reverse = True)]\n    y_values = np.append([0], np.cumsum(model_y))\n    x_values = np.arange(0, total + 1)\n\n    from sklearn.metrics import auc\n    # Area under Random Model\n    a = auc([0, total], [0, class_1_count])\n\n    # Area between Perfect and Random Model\n    aP = auc([0, class_1_count, total], [0, class_1_count, class_1_count]) - a\n\n    # Area between Trained and Random Model\n    aR = auc(x_values, y_values) - a\n\n    AR = aR \/ aP\n\n    plt.plot(x_values, y_values, c = 'g', label = '%s (AR = %0.3f)' % (Names[i],AR), linewidth = 4)\n\n    # Plot information\n    plt.xlabel('Total observations')\n    plt.ylabel('Class 1 observations')\n    plt.title('Cumulative Accuracy Profile')\n    plt.legend(fontsize='small')","e5791372":"Feature Importances are different for each of the 4 classifiers but they all share some common features with significant importances like Title_Mr, Fare, Pclass_3 and Age.","9ade856c":"A plot of the Age binned with numbers. Most of the passengers fall between tha age 10 and 40 years old.","bd3983e5":"- The age distribution is a bit Right-Skewed (Positive Skewness) with some outliers and mean age: 29.7, median age: 28.\n- We can see a second smaller peak (bell) which corresponds to the many children onboard.","e34a98bb":"The plot shows the fare distribution binned. For most people the fare was between 0 and 64.","77ef7f14":"- Features\n- Categorical: Binary: Survived, Sex\n             Nominal: Cabin, Embarked, Name(Title)\n             Ordinal: Pclass\n                       \n- Numerical: Discrete: SibSp, Parch\n           Continuous: Age, Fare","37f98a92":"- 68.2% had no siblings or spouses aboard the Titanic with a survival percentage of 34.5%.\n- 23.5% had 1 sibling or spouse aboard with a significant higher survival percentage of 53.5%","c7c23fa2":"We know from the previous plots that the higher the Pclass the better chance to survive but higher Pclass also means higher fare which can be explained from the plots above. Higher fare could also mean having a family member onboard which again gives you a better chance to survive.","1412ba7d":"We can see that Age and Pclass are related since the higher the Pclass the higher the mean Age. Also the Age is smaller for the people that Survived in each Pclass probably because there were children among them. ","09841b3e":"Title is related with Sex and we can see that 58% had title Mr as long as a low chance to survive since only 15.6% survived.\nWomen on the other hand and Masters (usually for young boys) has much better percentages of survival. Mrs: 79.2%, Miss: 69.7%, Master: 57.5% ","2f743804":"A noticable second peak in the Age distribution of the Survived indicates that children had a priority since many of them survived.","1089f884":"There are 12 columns in the dataset with the below dtypes: \nfloat64(2): Age, Fare                     \nint64(5): PassengerId, Survived, Pclass, SibSp, Parch                          \nobject(5): Name, Sex, Ticket, Cabin, Embarked                            ","4fe570f1":"The mean age is almost similar for men and women. Sex might not be related with Age.","c8d27a6f":"60.3% of the passengers were alone onboard and only 30% of them survived. On the other hand it seems that having 1, 2, or 3 family members gives you a better chance to survive. For large families (4 or more members) the chances are low again but maybe there were a some large families that didn't manage to be saved and account for these numbers.","106b2b4d":"## 2. Data Profiling","1273fdf8":"From the plot we can see that the fare distribution for the survived people is wider which might mean that people with higher fares had better chance to survive.","b36f6835":"### Categorial Features","d01772ab":"#### Cumulative Accuracy Profile (CAP) Curve\nThe CAP Curve tries to analyse how to effectively identify all data points of a given class using minimum number of tries.","2b6c1712":"We first used GridSearchCV to tune some of hyperparameters.","d249d50e":"### Categorical Features","b9a13765":"72.5% of the people boarded from the port of Southampton which also accounts for the most deaths.\n- Southampton: 33.9% of the people boarded from Southampton survived\n- Cherbourg: 55.3% of the people boarded from Cherbourg survived\n- Queenstown: 38.9% of the people boarded from Queentown survived","f0e68016":"Family and Age seem to be related since the Age drops as the family size gets bigger. This can be explained since as the family size gets larger there are more kids among them and the mean Age gets smaller.","dec331a4":"Decision Tree has the highest AUC with 0.874 and then Random Forest and Gradient Boosting follow with 0.808 and 0.805 respectively. Logistic Regression is very close with AUC = 0.796.","aa833c55":"61.6% of the people died and 38.4% of the people survived the sinking of the RMS Titanic","13ab555a":"## 4. Feature Engineering ( Visualization )","ec0bad27":"Best Score after submission: 0.79425 using the Random Forest Classifier --> position 3852\/24648 top 15%","f5f2d7b4":"From the male distributions we can see that many young boys survived from the second peak.","2689f952":"- 'Age': 177 missing values\n- 'Cabin': 687 missing values\n- 'Embarked': 2 missing values","61f5967b":"The higher the AUC, the better the model is at distinguishing between survived passengers and not survived passengers.\nRandom Forest Classifier and Gradient Boosting have the highest AUC with 0.904 and 0.902 respectively. Logistic Regression is very close with AUC = 0.898.","31e62e2a":"- We can see that the Fare feature has many outliers and its distribution is Right-Skewed (Positive Skewness).\n- The mean fare is 32.2 and the median fare is 14.45.","c4b4300c":"## 1. Introduction","8a845c9b":"- 55.1% of the people had ticket Class 3 while 24.2% had ticket Class 1 and 20.7%  Class 2.\n- Ticket Class 3 accounts for the most deaths.\n- Pclass 1: 62.9% Survived\n- Pclass 2: 47.3% Survived\n- Pclass 3: 24.2% Survived\n- The higher the class the better chance of survival","d0dfb046":"- Age: Min = 0, Max = 80\n- Fare: Min = 0, Max = 512.33","0024cd4c":"## 6. Evaluation","86320b40":"Most of the Cabin values are missing, actually for 77.1% of the passengers. Only 29.9% of the passengers assigned Unknown deck survived. Passenger of other decks have better chance of survival.","48420cff":"As an conclusion our choice would fall between the Random Forest and Gradient Boosting.","e1bc366a":"## 3. Feature analysis (Visualization)","126ced4b":"The right plot shows that most of the people that boarded from the Cherbourg port where in Pclass 1 which has the highest survival percentage. So that can be related to the high perctentage of survival that Cherbourg port has.","a9fe1421":"In this notebook we examine the Titanic dataset and then we build a model that can predict if a passenger survived the sinking or not. We start with finding feature types, missing values and we continue with feature analysis and visualization of the data. Feature engineering is implemented to create new attributes, encoding and imputation of the missing values. At last we test several classifiers and we evaluate them with the help of the ROC and CAP curves.\n\n#### History\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters.\n\n#### Data Dictionary\n- Survived: 0: No, 1: Yes\n- Pclass (Ticket Class): 1: 1st, 2: 2nd, 3: 3rd\n- Sex: Male, Female\n- Age (in Years)\n- SibSp (# of siblings \/ spouses aboard the Titanic)\n- ParCh (# of siblings \/ spouses aboard the Titanic)\n- Ticket (Ticket Number)\t\n- Fare (Passenger Fare)\t\n- Cabin (Cabin Number)\t\n- Embarked (Port of Embarkation):\tC: Cherbourg, Q: Queenstown, S: Southampton    \n\n#### Structure\n1. Introduction\n2. Data Profiling\n3. Feature Analysis (Visualization)\n4. Feature Engineering (Visualization)\n5. Feature Engineering (Encoding)\n6. Evaluation - Selection\n\n#### Goal\nOur goal is to predict if a passenger Survived '1' or Not Survived '0' the sinking of the Titanic.\n\n#### P.S. \nFeel free to comment if you have any question, something to note or suggest about this notebook. It will only make us better!  ","20f12cbb":"The mean age is almost similar for those that survived and those who didn't.","ee005962":"- 64.8% of the people were men and 35.2% were women.\n- It seems that Sex is an important feature since 74.2% of women survived in comparison with men where only 18.9% survived.","dfed76e4":"### Numerical Features","f3fb11ce":"- 76.1% had no parents or children aboard the Titanic with a survival percentage of 34.3%.\n- 13.2% had 1 parent or child aboard with a significant higher survival percentage of 55% and\n- 9.0% had 2 parents or children aboard with a survival percentage of 50%.\n- It seems that having a family member onboard would give you a better chance of survival.","d89bcfc3":"We used the functions above to auto plot some features with annotations (percentages, counts)","a9b1e47d":"The correlation between Sex and Title features is obvious as well as Pclass with Deck and Fare.","c6a2ed04":"We ended with 28 features after the encoding, still we need to impute the missing values in the Age feature and fix the skewness in the Fare feature. This will be done after we split our Train (Titanic) dataset into train and test. We do this in order to avoid information passing to from the training set to the testing set. \nTo sum up, we build and evaluate our model based on the Train data provided by splitting them again in train and test sets and  then we use the developed model on the whole Train data provided and predict for the Test data.","bb5c479a":"## 5. Feature Engineering ( Encoding )","2702e3ea":"#### Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic Curve, better known as the ROC Curve, is an excellent method for measuring the performance of a Classification model. It tells how much model is capable of distinguishing between classes. The True Positive Rate (TPR) is plot against False Positive Rate (FPR) for the probabilities of the classifier predictions. Then, the area under the plot is calculated."}}