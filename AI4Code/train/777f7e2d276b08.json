{"cell_type":{"eaa4a8da":"code","371025e4":"code","4b468a20":"code","2f3e79d7":"code","5cd503c0":"code","d3afad57":"code","8990ed7b":"code","7d958352":"code","3385cde6":"code","f295c2da":"code","b635244f":"code","1f176018":"code","00894b60":"code","3bcbcaed":"code","687e8ded":"code","c2d174ed":"code","d3d0a387":"markdown","b2a6d577":"markdown","ca65bd15":"markdown","162a7d3e":"markdown","d9aacb64":"markdown","f91c96e3":"markdown","179dfd6e":"markdown","8bd6fdf6":"markdown","e68421cd":"markdown","1e5abf46":"markdown","0efdd22a":"markdown","e0f41855":"markdown","73bd825a":"markdown","6c8ce5b3":"markdown","98f31e42":"markdown","a97491cd":"markdown"},"source":{"eaa4a8da":"# numpy and pandas for data manipulation\nimport numpy as np\nimport pandas as pd \n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nimport re\nimport pickle\n\n# Text to vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# splitting the dataset\nfrom sklearn.model_selection import train_test_split\n\n# nltk lib for sentiment analysis\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","371025e4":"data = pd.read_csv('\/kaggle\/input\/amazon-music-reviews\/Musical_instruments_reviews.csv')","4b468a20":"# duplicating the data\ntest_data = data.copy()","2f3e79d7":"# data \ntest_data.head(5)","5cd503c0":"test_data.drop([\"reviewerID\", \"asin\", \"reviewerName\", \"helpful\", \"unixReviewTime\", \"reviewTime\",], axis = 1, inplace = True)","d3afad57":"test_data.head(5)","8990ed7b":"# combaining both text data\ncombined = test_data['reviewText'] + test_data['summary']\n","7d958352":"def decontracted(phrase):\n    # replace '\n    phrase = phrase.replace('\u2019', '\\'')\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef line_breaks(phrase_1):\n    phrase_1 = phrase_1.replace('\\\\r', ' ')\n    phrase_1 = phrase_1.replace('\\\\\"', ' ')\n    phrase_1 = phrase_1.replace('\\n', ' ')\n    return phrase_1\n\ndef remove_special_character(phase_2):\n    phase_2 = re.sub('[^A-Za-z0-9]+', ' ', phase_2)\n    return(phase_2)\n\ndef remove_continues_char(s) :\n    senta = \"\"\n    for i in s.split():\n        #print(i)\n        if len(i) <= 15:\n            \n            senta += i\n            senta += ' '\n        else:\n            pass\n    return(senta)\n\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]\n\ndef stop_words(phrase):\n    after = ' '.join(e for e in phrase.split() if e.lower() not in stopwords)\n    return(after)","3385cde6":"preprocessed_titles = []\n# tqdm is for printing the status bar\nfor review in tqdm(combined.values):\n    review_1 = str(review)\n    review_1 = decontracted(review_1)\n    review_1 = line_breaks(review_1)\n    review_1 = remove_special_character(review_1)\n    review_1 = remove_continues_char(review_1)\n    review_1 = stop_words(review_1)\n    preprocessed_titles.append(review_1.lower().strip())","f295c2da":"test_data['combined'] = preprocessed_titles","b635244f":"vectorizer = CountVectorizer(min_df=10,ngram_range=(1,4))\nvectorizer.fit(test_data['combined'].values) \n\n# we use the fitted CountVectorizer to convert the text to vector\nbow = vectorizer.transform(test_data['combined'].values)\n\nprint(\"After vectorizations\")\nprint(bow.shape)","1f176018":"vectorizer = TfidfVectorizer(min_df=10,ngram_range=(1,4))\nvectorizer.fit(test_data['combined'].values)\n\ntfidf = vectorizer.transform(test_data['combined'].values)\n\n\nprint(\"After vectorizations\")\nprint(tfidf.shape)\n","00894b60":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''","3bcbcaed":"# import nltk\n\nnltk.download('vader_lexicon')\nsid = SentimentIntensityAnalyzer()\n\nsida_X1_neg = []\nsida_X1_pos = []\nsida_X1_neu = []\nsida_X1_compound = []\nfor sentence1 in (test_data['combined'].values): \n    ss = sid.polarity_scores(sentence1)\n    sentiment_neg=ss['neg']\n    sentiment_pos=ss['pos']\n    sentiment_neu=ss['neu']\n    sentiment_compound=ss['compound'] \n    sida_X1_neg.append(sentiment_neg)\n    sida_X1_pos.append(sentiment_pos)\n    sida_X1_neu.append(sentiment_neu)\n    sida_X1_compound.append(sentiment_compound)\n","687e8ded":"negative = np.array(sida_X1_neg).reshape(-1,1)\npositive = np.array(sida_X1_pos).reshape(-1,1)\nneutral = np.array(sida_X1_neu).reshape(-1,1)\ncompound = np.array(sida_X1_compound).reshape(-1,1)","c2d174ed":"print(negative.shape)\nprint(positive.shape)\nprint(neutral.shape)\nprint(compound.shape)","d3d0a387":"## Sentiment Analysis","b2a6d577":"![avg%20w2v.jpg](attachment:avg%20w2v.jpg)","ca65bd15":"# Cleaning Text Data","162a7d3e":"![pic.jpg](attachment:pic.jpg)","d9aacb64":"### I have posted a screenshot of jupyter notebook, because I cant upload an glove vector size of 128 mb to this notebook.","f91c96e3":"# Average Word to Vector","179dfd6e":"# Reading the data","8bd6fdf6":"# Vectorizing text data","e68421cd":"## Bag of Words","1e5abf46":"## TFIDF weighted WORD TO VECTOR ","0efdd22a":"### I have posted a screenshot of jupyter notebook, because I cant upload an glove vector size of 128 mb to this notebook.","e0f41855":"# Featues extended:\n## 1. Bag of Words = 7985\n## 2. TFIDF = 7985\n## 3. Average Word to Vector = 300\n## 3. TFIDF weighted Word to Vector = 300\n## 4. Sentiment Analysis = 4","73bd825a":"## I have written an brief notebook about [Text Cleaning](https:\/\/www.kaggle.com\/prasad789\/text-pre-processing-cleaning)","6c8ce5b3":"# Importing Libraries","98f31e42":"# Droping unwanted columns","a97491cd":"## TFIDF Vectorizer"}}