{"cell_type":{"1f63aa5d":"code","d0717434":"code","7a5a5a79":"code","ea9e1bc2":"code","a42ff6bd":"code","76d98397":"code","ca1d8cbe":"code","70c8f5e7":"code","d664ba03":"code","9b825ee0":"code","fa391770":"code","06c521a0":"code","028aef80":"code","633040b2":"code","108bd5f8":"code","5fa399a1":"code","d420958b":"markdown","73d20777":"markdown","418dfcc4":"markdown","65441ff5":"markdown","4250732d":"markdown","204a1bcb":"markdown","20115ba6":"markdown","9d136a31":"markdown","84fe1c36":"markdown","45cea8ad":"markdown","aba866d8":"markdown","10367487":"markdown","6e38ae59":"markdown","2bfe5986":"markdown","439501e3":"markdown","a39ae048":"markdown"},"source":{"1f63aa5d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d0717434":"data = pd.read_csv('\/kaggle\/input\/coronavirusdataset\/PatientInfo.csv')","7a5a5a79":"! pip install lifelines","ea9e1bc2":"from datetime import datetime\nfrom lifelines import CoxPHFitter\nfrom lifelines.utils.sklearn_adapter import sklearn_adapter\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport numpy as np\nimport re\nimport math\nsns.set(style=\"ticks\", context=\"talk\")\nplt.style.use(\"dark_background\")\npd.set_option('mode.chained_assignment', None)\n","a42ff6bd":"df = data\ndf['start_date'] = df.confirmed_date\ndf['start_date'] = pd.to_datetime(df['start_date'])\ndf['deceased_date'] = pd.to_datetime(df['deceased_date'])\ndf['released_date'] = pd.to_datetime(df['released_date'])\n# The assumption here is that if the patient is not deceased or recovered, he is censored, meaning we do not know the outcome yet.\ndf['survived_date'] = df.deceased_date.fillna(df.released_date).fillna(datetime.today())\n\ndf = df.drop(df[(df.state == 'deceased') & (df.deceased_date.isna())].index)\ndf = df.drop(df[(df.state == 'released') & (df.released_date.isna())].index)\n\ndf['died'] = df.deceased_date.notnull().astype(int)\ndf['recovered'] = df.released_date.notnull().astype(int)\ndf['survived'] = (df.survived_date - df.start_date).dt.days\n\ndf['is_male'] = (df.sex == 'male').astype(int)\ndf['age_temp_a'] = df.age.apply(lambda x: int(re.findall(\"\\d+\", str(x))[0]) \/ 10 if pd.notnull(x) else None)\ndf['age_temp_b'] = df.birth_year.apply(lambda x: np.floor((2020 - x) \/ 10) if x != None else None)\ndf['age_decade'] = df.age_temp_a.fillna(df.age_temp_b)\ndf = df.drop(df[df.age_decade.isna()].index)\ndf = df[['patient_id', 'age_decade', 'is_male', 'died', 'recovered', 'survived']]\ndf = df.drop(df[df.isnull().any(axis = 1)].index)\ndf = df.drop(df[df.patient_id.duplicated()].index)\ndf.head()","76d98397":"train, test = train_test_split(df, test_size = 0.3)\nX_train_death = train[['died', 'age_decade', 'is_male']]\ny_train_death = train['survived']\nX_test_death = test[['died', 'age_decade', 'is_male']]\ny_test_death = test['survived']\n\nX_train_recovery = train[['recovered', 'age_decade', 'is_male']]\ny_train_recovery = train['survived']\nX_test_recovery = test[['recovered', 'age_decade', 'is_male']]\ny_test_recovery = test['survived']\nprint('train size : {}, test_size : {}'.format(train.shape[0], test.shape[0]))","ca1d8cbe":"def cox_ph_fitter(X_train, y_train, X_test, y_test, event):\n    CoxRegression = sklearn_adapter(CoxPHFitter, event_col = event)\n    cph = CoxRegression()\n    cph.fit(X_train, y_train)\n    cph.lifelines_model.print_summary()\n    print('---')\n    print('Test Score = {}'.format(cph.score(X_test, y_test)))\n    return cph.lifelines_model","70c8f5e7":"cph_death = cox_ph_fitter(X_train_death, y_train_death, X_test_death, y_test_death,  'died')","d664ba03":"cph_recovery = cox_ph_fitter(X_train_recovery, y_train_recovery, X_test_recovery, y_test_recovery, 'recovered')","9b825ee0":"base_case_death = pd.Series({'age_decade' : 0, 'is_male' : 0})\ncummulative_baseline_hazard_death = cph_death.predict_cumulative_hazard(base_case_death)\nbaseline_hazard_death = cummulative_baseline_hazard_death.diff().fillna(cummulative_baseline_hazard_death)\n\nbase_case_recovery = pd.Series({'age_decade' : 0, 'is_male' : 0})\ncummulative_baseline_hazard_recovery = cph_recovery.predict_cumulative_hazard(base_case_recovery)\nbaseline_hazard_recovery = cummulative_baseline_hazard_recovery.diff().fillna(cummulative_baseline_hazard_recovery)\n\nbaseline_hazards = pd.merge(baseline_hazard_death[0:], baseline_hazard_recovery[0:], left_index=True, right_index=True)\nbaseline_hazards = baseline_hazards.rename(columns = {'0_x' : 'baseline_hazard_death', '0_y' : 'baseline_hazard_recovery'})\nbaseline_hazards.head()","fa391770":"test['key'] = 1\nbaseline_hazards['key'] = 1\n\n#cross join of patient in the test set with the baseline hazards\ntest_lines = pd.merge(test, baseline_hazards[:30], on = 'key')\n\n#simple row number, indicating the number of days since confirmed.\ntest_lines['days'] = test_lines.groupby('patient_id').cumcount()\n\n#according to the definition of the hazard function in cox regression, we calculate the hazard for each patient individually\n#in effect this is the conditional probability of death and recovery for each patient, given the patient has survived so far\ntest_lines['p_death_given_s'] = test_lines.baseline_hazard_death * np.exp(np.dot(test_lines[['age_decade', 'is_male']], cph_death.params_))\ntest_lines['p_recovery_given_s'] = test_lines.baseline_hazard_recovery * np.exp(np.dot(test_lines[['age_decade', 'is_male']], cph_recovery.params_))\n\ntest_lines['survive_death'] = 1-test_lines.p_death_given_s\ntest_lines['survive_recovery'] = 1-test_lines.p_recovery_given_s\n\n#probability of survival defined as if the patient did not die and did not recover so far \ntest_lines['p_survive'] = test_lines.groupby('patient_id').survive_death.cumprod() * test_lines.groupby('patient_id').survive_recovery.cumprod()\n\n#unconditional probability of death and recovery. \ntest_lines['p_death'] = test_lines.p_death_given_s * test_lines.groupby('patient_id').p_survive.shift().fillna(1)\ntest_lines['p_recovery'] = test_lines.p_recovery_given_s * test_lines.groupby('patient_id').p_survive.shift().fillna(1)\n\n#observed deaths and recoveries\ntest_lines['recovered'] = ((test_lines.survived == test_lines.days) & (test_lines.recovered == 1)).astype(int)\ntest_lines['died'] = ((test_lines.survived == test_lines.days) & (test_lines.died == 1)).astype(int)","06c521a0":"agg = test_lines.groupby('days').agg({\n    'died' : np.sum\n    ,'recovered' : np.sum\n    ,'p_death' : np.sum\n    ,'p_recovery' : np.sum\n    ,'patient_id' : pd.Series.nunique\n}).reset_index()\nagg['death_rate'] = agg.died.cumsum() \/ agg.patient_id\nagg['death_rate_pred'] = agg.p_death.cumsum() \/ agg.patient_id\nagg['recovery_rate'] = agg.recovered.cumsum() \/ agg.patient_id\nagg['recovery_rate_pred'] = agg.p_recovery.cumsum() \/ agg.patient_id","028aef80":"sns.lineplot(agg.days, agg.death_rate_pred, label = 'predictions')\nsns.lineplot(agg.days, agg.death_rate, label = 'actuals')","633040b2":"sns.lineplot(agg.days, agg.recovery_rate_pred, label = 'predictions')\nsns.lineplot(agg.days, agg.recovery_rate, label = 'actuals')","108bd5f8":"def cox_ph_fitter(X_train, y_train, X_test, y_test, event):\n    CoxRegression = sklearn_adapter(CoxPHFitter, event_col = event)\n    cph = CoxRegression()\n    cph.fit(X_train, y_train)\n    #cph.lifelines_model.print_summary()\n    #print('---')\n    #print('Test Score = {}'.format(cph.score(X_test, y_test)))\n    return cph.lifelines_model\n\naggregations = pd.DataFrame(columns = ['iteration','days', 'death_rate', 'death_rate_pred', 'recovery_rate', 'recovery_rate_pred'])\n\nfor iteration in range (1,21):\n    train, test = train_test_split(df, test_size = 0.3)\n    X_train_death = train[['died', 'age_decade', 'is_male']]\n    y_train_death = train['survived']\n    X_test_death = test[['died', 'age_decade', 'is_male']]\n    y_test_death = test['survived']\n\n    X_train_recovery = train[['recovered', 'age_decade', 'is_male']]\n    y_train_recovery = train['survived']\n    X_test_recovery = test[['recovered', 'age_decade', 'is_male']]\n    y_test_recovery = test['survived']\n\n    cph_death = cox_ph_fitter(X_train_death, y_train_death, X_test_death, y_test_death,  'died')\n    cph_recovery = cox_ph_fitter(X_train_recovery, y_train_recovery, X_test_recovery, y_test_recovery, 'recovered')\n\n    base_case_death = pd.Series({'age_decade' : 0, 'is_male' : 0})\n    cummulative_baseline_hazard_death = cph_death.predict_cumulative_hazard(base_case_death)\n    baseline_hazard_death = cummulative_baseline_hazard_death.diff().fillna(cummulative_baseline_hazard_death)\n\n    base_case_recovery = pd.Series({'age_decade' : 0, 'is_male' : 0})\n    cummulative_baseline_hazard_recovery = cph_recovery.predict_cumulative_hazard(base_case_recovery)\n    baseline_hazard_recovery = cummulative_baseline_hazard_recovery.diff().fillna(cummulative_baseline_hazard_recovery)\n\n    baseline_hazards = pd.merge(baseline_hazard_death[0:], baseline_hazard_recovery[0:], left_index=True, right_index=True)\n    baseline_hazards = baseline_hazards.rename(columns = {'0_x' : 'baseline_hazard_death', '0_y' : 'baseline_hazard_recovery'})\n\n    test['key'] = 1\n    baseline_hazards['key'] = 1\n\n    #cross join of patient in the test set with the baseline hazards\n    test_lines = pd.merge(test, baseline_hazards[:30], on = 'key')\n\n    #simple row number, indicating the number of days since confirmed.\n    test_lines['days'] = test_lines.groupby('patient_id').cumcount()\n\n    #according to the definition of the hazard function in cox regression, we calculate the hazard for each patient individually\n    #in effect this is the conditional probability of death and recovery for each patient, given the patient has survived so far\n    test_lines['p_death_given_s'] = test_lines.baseline_hazard_death * np.exp(np.dot(test_lines[['age_decade', 'is_male']], cph_death.params_))\n    test_lines['p_recovery_given_s'] = test_lines.baseline_hazard_recovery * np.exp(np.dot(test_lines[['age_decade', 'is_male']], cph_recovery.params_))\n\n    test_lines['survive_death'] = 1-test_lines.p_death_given_s\n    test_lines['survive_recovery'] = 1-test_lines.p_recovery_given_s\n\n    #probability of survival defined as if the patient did not die and did not recover so far \n    test_lines['p_survive'] = test_lines.groupby('patient_id').survive_death.cumprod() * test_lines.groupby('patient_id').survive_recovery.cumprod()\n\n    #unconditional probability of death and recovery. \n    test_lines['p_death'] = test_lines.p_death_given_s * test_lines.groupby('patient_id').p_survive.shift().fillna(1)\n    test_lines['p_recovery'] = test_lines.p_recovery_given_s * test_lines.groupby('patient_id').p_survive.shift().fillna(1)\n\n    #observed deaths and recoveries\n    test_lines['recovered'] = ((test_lines.survived == test_lines.days) & (test_lines.recovered == 1)).astype(int)\n    test_lines['died'] = ((test_lines.survived == test_lines.days) & (test_lines.died == 1)).astype(int)\n\n    agg = test_lines.groupby('days').agg({\n        'died' : np.sum\n        ,'recovered' : np.sum\n        ,'p_death' : np.sum\n        ,'p_recovery' : np.sum\n        ,'patient_id' : pd.Series.nunique\n    }).reset_index()\n    agg['death_rate'] = agg.died.cumsum() \/ agg.patient_id\n    agg['death_rate_pred'] = agg.p_death.cumsum() \/ agg.patient_id\n    agg['recovery_rate'] = agg.recovered.cumsum() \/ agg.patient_id\n    agg['recovery_rate_pred'] = agg.p_recovery.cumsum() \/ agg.patient_id\n    \n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n    sns.lineplot(agg.days, agg.death_rate_pred, label = 'predictions', ax = ax1)\n    sns.lineplot(agg.days, agg.death_rate, label = 'actuals', ax = ax1)\n\n    sns.lineplot(agg.days, agg.recovery_rate_pred, label = 'predictions', ax = ax2)\n    sns.lineplot(agg.days, agg.recovery_rate, label = 'actuals', ax = ax2)\n    fig.suptitle('Iteration : {}'.format(iteration) )\n    \n    agg['iteration'] = iteration\n    agg_to_append = agg[['iteration', 'days', 'death_rate', 'death_rate_pred', 'recovery_rate', 'recovery_rate_pred']]\n    aggregations = aggregations.append(agg_to_append, ignore_index = True)\n    ","5fa399a1":"aggregations['squared_death_diff'] = (aggregations.death_rate - aggregations.death_rate_pred) ** 2\naggregations['squared_recovery_diff'] = (aggregations.recovery_rate - aggregations.recovery_rate_pred) ** 2\nerrors = aggregations.groupby('days').agg({\n    'squared_death_diff' : np.sum\n    ,'squared_recovery_diff' : np.sum\n    ,'iteration' : np.size\n}).reset_index()\nerrors['RMSD_death_rate'] = np.sqrt(errors.squared_death_diff \/ errors.iteration)\nerrors['RMSD_recovery_rate'] = np.sqrt(errors.squared_recovery_diff \/ errors.iteration)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\nsns.lineplot(errors.days, errors.RMSD_death_rate, ax = ax1)\nsns.lineplot(errors.days, errors.RMSD_recovery_rate, ax = ax2)","d420958b":"Let's install the lifelines library","73d20777":"Let's speak about the summary of the model after fitting is completed. Our concordance index is ~0.9, meaning that the feautres selected have good ranking power. If we revisit the Hazard function in Cox Regression, we will see that both coefficients of our futures are positive. In effect, this means that if we increase the age decade we increase the hazard of dying. Similarly men have a higher risk of dying than women.","418dfcc4":"# Building the dataset","65441ff5":"As you can see, the model seems to predict pretty well. Maybe the specific random fold of the data into train and test is such, that favors prediction. What will follow is a \"manual\" cross validation, where we will iterate with different folds and average the erros.\nBut before going into that let's observe the produced chart. According to the model, if 100 new patients enter the hospital, by 10 days ~1.5% of the patients will be deceased and by 30 days ~2%","4250732d":"Because we will manually predict the hazard of each patient and because lifelines outputs the baseline hazard of a patient with average features ( see [this issue](https:\/\/github.com\/CamDavidsonPilon\/lifelines\/issues\/543) raised) we will costruct the baseline hazards ourseves, by encoding a base case where all X's are set to 0","204a1bcb":"# Purpose:\nI wanted take this opportunity and apply one of the most famous survival analysis methods, called Cox Regression on this dataset. I am sure many of you are familiar with this one.\nWe are dealing with right censored data, (left censored as well, but we could make the assumption that time 0 is the confirmed date), so survival analysis seemed to me really applicable in this case. We will build and evaluate the predictive power of our model, as well as extract some key results\n\nI would like to thank all involved into gathering this dataset. Additionally the author of the Lifelines python package, for implementing all sorts of Survival Analysis algorithms. I am not a data scientist myself, but I am actively using these survival techniques in the consumer lending industry.\n","20115ba6":"For this exercise, we will work with Patient Level data.","9d136a31":"# Thank you\nOnce more, I would like to thank all involved into gathering this dataset. The descipline required for this effort really amazes me and I could not feel anything less than gratitude. You really enabled lots of researchers to proceed in complex pieces of analysis. I wish all of you to keep on \"surviving\" and even better in our context \"recovering\"!\n\nIlias Katsabalos\n\nStrategic Analyst","84fe1c36":"Let's aggregate and visualize the results","45cea8ad":"# Hazard and Baseline Hazard\n\nIn Survival Analysis, Hazard is a fucntion of time \n\n$h(t) =  \\lim_{\\delta t \\rightarrow 0 } \\; \\frac{Pr( t \\le T \\le t + \\delta t  |  T > t)}{\\delta t}$\n\nand in effect it models the probability of experiencing an event *T* given that the subject has survived more than *t*. In Cox Regression particularly, the hazard of a subject is described by the following function\n\n$\\underbrace{h(t | x)}_{\\text{hazard}} = \\overbrace{b_0(t)}^{\\text{baseline hazard}} \\underbrace{\\exp \\overbrace{\\left(\\sum_{i=1}^n b_i (x_i - \\overline{x_i})\\right)}^{\\text{log-partial hazard}}}_ {\\text{partial hazard}}$\n\nwhere $b_0(t)$ is the baseline hazard and which is scaled up or down based on some regressors X\n\nIn this exercise I am using the hazard concept as the conditional probability of experiencing an event (death or recovery) given the patient's survival until a specific day. The features I am using for predicting the hazard are the age decade of the patient and the gender.\n\nBut since we are operating in a competing risks setting, we will have to fit 2 different models. The first one outputs the baseline hazard for the death event, while the second one will output the baseline hazard of the recovery event. An important distinction that we need to make is that we censor competing events at the time of the occurence. Let's say for example that we are modeling the event of death. A patient who recovered 14 days after he was diagnosed, will be censored at 14 days of survival (the \"survived\" column in the dataset will have the value 14). The opposite will happen for a patient who died, when we will be modeling the event of recovery.\nThis approach is the \"cause specific\" hazard approach, in contrast with \"subdistribution\" hazard approach, where in the latter, subjects that experience a competing risk remain in the risk set. [An interesting read](https:\/\/statisticalhorizons.com\/for-causal-analysis-of-competing-risks) for someone who wants to become more familiar with the topic","aba866d8":"Our Root Mean Square Deviation when predicting death rates at 30 days is 0.6%. We need to be cautious about this number, since it depends on the magnitude of the number of deceased patients. It is not with certainty that if the death rate was higher, the RMSD of the death rate would still be 0.6%. Similarly our RMSD for the recovery rate is 10% at 30 days. For sure, the further we look into the future, the less accurate our results become.","10367487":"Above we modeled the recovery event. Obvously, the younger you are, the higher the \"hazard\" of being recovered. Similarly, women have higher chances of being recovered than men.","6e38ae59":"# Survival Analysis - Cox Regression\nI would not like to bore you with the details of survival analysis. These concepts are much better explained in the original [lifelines](https:\/\/lifelines.readthedocs.io\/en\/latest\/) documentation. The only thing that I want to describe is how I handle the events:\n\n* died : If died = 1 the patient is deceased\n* recovered : If recovered = 1 the patient has recovered\n* survived: Number of days that the patient has \"survived\". This is not to be confused with the recovery of a patient. It rather means how many days a patient survived while being observed. Survival in this case means not death and not recovery. Consequently, if the patient is not deceased or has he not recovered, the patient is censored, meaning we do not know the outcome yet.\n\nAt this point I would like to state that we are working within a \"competing risks\" setting. In effect, recovery from the virus is a competing risk of death and vice versa. Let's move on and split our dataset into train and test sets","2bfe5986":"Similarly, the model predicts 40% recovery rate by the 30th after being diagnosed.","439501e3":"Let's compute our errors and visualize them","a39ae048":"# Cross Validation\nSince there is not a big amount of data so far, the split into train and test could make the results fluctuate a lot. For that reason I proceeded in coding a \"manual\" cross validation and I will take average error of each iteration as a final metric for the model"}}