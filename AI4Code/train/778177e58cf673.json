{"cell_type":{"1b0c56a3":"code","1655bfbb":"code","7ba30f1b":"code","7c786a96":"code","04563884":"code","bb8d0c5c":"code","51203459":"code","391eb545":"code","0eb21eaf":"code","d5c137f1":"code","483b765f":"code","1a184b61":"code","610584e4":"code","c79c9272":"code","77f4ef3a":"markdown","c0adaa89":"markdown","0ac13bbb":"markdown","87634d5d":"markdown","197cca29":"markdown","f3ec69c4":"markdown","807eefa5":"markdown","d7409214":"markdown","b857b705":"markdown","2f905ce5":"markdown","c2c6354d":"markdown","5b59567f":"markdown","452539ea":"markdown","5f24dcfa":"markdown","0eaf3442":"markdown","b54dc05d":"markdown","beec73e5":"markdown","f9cd7fcf":"markdown","562068f3":"markdown","8a65378c":"markdown","3af3e343":"markdown"},"source":{"1b0c56a3":"import os, sys, math\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE","1655bfbb":"# Detect hardware\ntry:\n  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n  tpu = None\n#If TPU not found try with GPUs\n  gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy for hardware\nif tpu:\n  tf.config.experimental_connect_to_cluster(tpu)\n  tf.tpu.experimental.initialize_tpu_system(tpu)\n  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n  print('Running on TPU ', tpu.master())  \nelif len(gpus) > 0:\n  strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n  print('Running on ', len(gpus), ' GPU(s) ')\nelse:\n  strategy = tf.distribute.get_strategy()\n  print('Running on CPU')\n\n# How many accelerators do we have ?\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","7ba30f1b":"GCS_PATTERN = 'gs:\/\/flowers-public\/tfrecords-jpeg-192x192-2\/*.tfrec'\nIMAGE_SIZE = [192, 192]\n\nif tpu:\n  BATCH_SIZE = 16*strategy.num_replicas_in_sync  # A TPU has 8 cores so this will be 128\nelse:\n  BATCH_SIZE = 32  # On GPU, a higher batch size does not help and sometimes does not fit on the GPU (OOM)\n\nVALIDATION_SPLIT = 0.20\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\n\n# splitting data files between training and validation\nfilenames = tf.io.gfile.glob(GCS_PATTERN)\nsplit = int(len(filenames) * VALIDATION_SPLIT)\ntraining_filenames = filenames[split:]\nvalidation_filenames = filenames[:split]\nprint(\"Pattern matches {} data files. Splitting dataset into {} training files and {} validation files\".format(len(filenames), len(training_filenames), len(validation_filenames)))\nvalidation_steps = int(3670 \/\/ len(filenames) * len(validation_filenames)) \/\/ BATCH_SIZE\nsteps_per_epoch = int(3670 \/\/ len(filenames) * len(training_filenames)) \/\/ BATCH_SIZE\nprint(\"With a batch size of {}, there will be {} batches per training epoch and {} batch(es) per validation run.\".format(BATCH_SIZE, steps_per_epoch, validation_steps))","7c786a96":"def dataset_to_numpy_util(dataset, N):\n  dataset = dataset.batch(N)\n  \n  # In eager mode, iterate in the Datset directly.\n  for images, labels in dataset:\n    numpy_images = images.numpy()\n    numpy_labels = labels.numpy()\n    break;\n\n  return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n  label = np.argmax(label, axis=-1)  # one-hot to class number\n  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n  correct = (label == correct_label)\n  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n                              CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=16, color='red' if red else 'black')\n    return subplot+1\n  \ndef display_chunk_images_from_dataset(dataset):\n  subplot=331\n  plt.figure(figsize=(15,15))\n  images, labels = dataset_to_numpy_util(dataset, 9)\n  for i, image in enumerate(images):\n    title = CLASSES[np.argmax(labels[i], axis=-1)]\n    subplot = display_one_flower(image, title, subplot)\n    if i >= 8:\n      break;\n              \n  plt.tight_layout()\n  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n  plt.show()\n  \ndef display_chunk_images_with_predictions(images, predictions, labels):\n  subplot=331\n  plt.figure(figsize=(15,15))\n  for i, image in enumerate(images):\n    title, correct = title_from_label_and_target(predictions[i], labels[i])\n    subplot = display_one_flower(image, title, subplot, not correct)\n    if i >= 8:\n      break;\n              \n  plt.tight_layout()\n  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n  plt.show()\n  \ndef display_training_curves(training, validation, title, subplot):\n  if subplot%10==1: # set up the subplots on the first call\n    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n    plt.tight_layout()\n  ax = plt.subplot(subplot)\n  ax.set_facecolor('#F8F8F8')\n  ax.plot(training)\n  ax.plot(validation)\n  ax.set_title('model '+ title)\n  ax.set_ylabel(title)\n  ax.set_xlabel('epoch')\n  ax.legend(['train', 'valid.'])","04563884":"def read_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size will be needed for TPU\n    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n    one_hot_class = tf.reshape(one_hot_class, [5])\n    return image, one_hot_class\n\ndef load_dataset(filenames):\n  # read from TFRecords. For optimal performance, read from multiple\n  # TFRecord files at once and set the option experimental_deterministic = False\n  # to allow order-altering optimizations.\n\n  option_no_order = tf.data.Options()\n  option_no_order.experimental_deterministic = False\n\n  dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n  dataset = dataset.with_options(option_no_order)\n  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n  return dataset","bb8d0c5c":"display_chunk_images_from_dataset(load_dataset(training_filenames))","51203459":"def data_augment(image, one_hot_class):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_saturation(image, 0, 2)\n    return image, one_hot_class","391eb545":"def get_batched_dataset(filenames, train=False):\n  dataset = load_dataset(filenames)\n  dataset = dataset.cache() # This dataset fits in RAM\n  if train:\n    # Best practices for Keras:\n    # Training dataset: repeat then batch\n    # Evaluation dataset: do not repeat\n    dataset = dataset.repeat()\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.shuffle(2000)\n  dataset = dataset.batch(BATCH_SIZE)\n  dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n  # should shuffle too but this dataset was well shuffled on disk already\n  return dataset\n  # source: Dataset performance guide: https:\/\/www.tensorflow.org\/guide\/performance\/datasets\n\n# instantiate the datasets\ntraining_dataset = get_batched_dataset(training_filenames, train=True)\nvalidation_dataset = get_batched_dataset(validation_filenames, train=False)\n\nsome_flowers, some_labels = dataset_to_numpy_util(load_dataset(validation_filenames), 160)","0eb21eaf":"with strategy.scope(): # this line is all that is needed to run on TPU (or multi-GPU, ...)\n\n  bnmomemtum=0.9\n  def fire(x, squeeze, expand):\n    y  = tf.keras.layers.Conv2D(filters=squeeze, kernel_size=1, activation='relu', padding='same')(x)\n    y = tf.keras.layers.BatchNormalization(momentum=bnmomemtum)(y)\n    y1 = tf.keras.layers.Conv2D(filters=expand\/\/2, kernel_size=1, activation='relu', padding='same')(y)\n    y1 = tf.keras.layers.BatchNormalization(momentum=bnmomemtum)(y1)\n    y3 = tf.keras.layers.Conv2D(filters=expand\/\/2, kernel_size=3, activation='relu', padding='same')(y)\n    y3 = tf.keras.layers.BatchNormalization(momentum=bnmomemtum)(y3)\n    return tf.keras.layers.concatenate([y1, y3])\n\n  def fire_module(squeeze, expand):\n    return lambda x: fire(x, squeeze, expand)\n\n  x = tf.keras.layers.Input(shape=[*IMAGE_SIZE, 3]) # input is 192x192 pixels RGB\n\n  y = tf.keras.layers.Conv2D(kernel_size=3, filters=32, padding='same', use_bias=True, activation='relu')(x)\n  y = tf.keras.layers.BatchNormalization(momentum=bnmomemtum)(y)\n  y = fire_module(24, 48)(y)\n  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n  y = fire_module(48, 96)(y)\n  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n  y = fire_module(64, 128)(y)\n  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n  y = fire_module(48, 96)(y)\n  y = tf.keras.layers.MaxPooling2D(pool_size=2)(y)\n  y = fire_module(24, 48)(y)\n  y = tf.keras.layers.GlobalAveragePooling2D()(y)\n  y = tf.keras.layers.Dense(5, activation='softmax')(y)\n\n  model = tf.keras.Model(x, y)\n\n  model.compile(\n    optimizer='adam',\n    loss= 'categorical_crossentropy',\n    metrics=['accuracy'])\n\n  model.summary()","d5c137f1":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=False, show_layer_names=True,\n    rankdir='TB', expand_nested=False, dpi=96\n)","483b765f":"EPOCHS = 35\n\nhistory = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,\n                    validation_data=validation_dataset)","1a184b61":"display_training_curves(history.history['accuracy'], history.history['val_accuracy'], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'], history.history['val_loss'], 'loss', 212)","610584e4":"# randomize the input so that you can execute multiple times to change results\npermutation = np.random.permutation(160)\nsome_flowers, some_labels = (some_flowers[permutation], some_labels[permutation])\n\npredictions = model.predict(some_flowers, batch_size=16)\nevaluations = model.evaluate(some_flowers, some_labels, batch_size=16)\n  \nprint('[val_loss, val_acc]', evaluations)","c79c9272":"display_chunk_images_with_predictions(some_flowers, predictions, some_labels)","77f4ef3a":"## Predictions","c0adaa89":"## Writting few Helper Function...","0ac13bbb":"## TPU detection","87634d5d":"## Buckle Up... And start diving into it....\n\n## Imports","197cca29":"# END NOTES...\nThis notebook is About the SqueezeNet.And I try my level best to explain each and every thing as I learn about the SqueezeNet as simple way as possible. There are lot more in SqueezeNet like it's different structure, uses, size and accuracy.\n\n## <font color='blue'>I hope you find this kernel useful and enjoyable, If so Please upvote it. And Your comments and feedback are most welcome. ;-)<\/font>\n","f3ec69c4":"## About this Notebook...\n\nNowadays, the technology is at its peak. People trying to get the best of its, for this they trying to change, modify and invent new technologies which is better of the present.Like Self driving cars and IoT is going to be household talks in the next few years to come. Therefore, everything is controlled remotely, say for example, in self-driving cars we will need our system to constantly communicate with the servers. So therefore, if we have a model which has a small size then we can easily deploy it in cloud. So that\u2019s why we needed a architecture which is less in size and also achieve the same level of accuracy that other architecture achieves.\n\n### That's squeezeNet is come....\n\nI try my level best to give all things as simple as possible.\n### <font color='red'>If you find this kernel useful, Please Upvote it.<font>","807eefa5":"## Little Description About TPUs...\n![tpu.JPG](attachment:tpu.JPG)\nTPUs are hardware accelerators specialized in deep learning tasks. In this Kernal,we will see how to use them with Keras and Tensorflow 2. Cloud TPUs are available in a base configuration with 8 cores and also in larger configurations called \"TPU pods\" of up to 2048 cores(Written in artile about TPUs in Google). The extra hardware can be used to accelerate training by increasing the training batch size.\n\n### Why TPUs ?\nModern GPUs are organized around programmable \"cores\", a very flexible architecture that allows them to handle a variety of tasks such as 3D rendering, deep learning, physical simulations, etc.. TPUs on the other hand pair a classic vector processor with a dedicated matrix multiply unit and excel at any task where large matrix multiplications dominate, such as neural networks.\n\n## The hardware...\n* **MXU and VPU:-** A TPU v2 core is made of a Matrix Multiply Unit (MXU) which runs matrix multiplications and a Vector Processing Unit (VPU) for all other tasks such as activations, softmax, etc. The VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16-32 bit floating point format.\n![hardware.JPG](attachment:hardware.JPG)\n## The software...\n* **Under the hood: XLA:-** Tensorflow programs define computation graphs. The TPU does not directly run Python code, it runs the computation graph defined by your Tensorflow program. Under the hood, a compiler called XLA (accelerated Linear Algebra compiler) transforms the Tensorflow graph of computation nodes into TPU machine code. This compiler also performs many advanced optimizations on your code and your memory layout. The compilation happens automatically as work is sent to the TPU. You do not have to include XLA in your build chain explicitly.\n![xla%20tpu.JPG](attachment:xla%20tpu.JPG)\n","d7409214":"## Architectural Design Strategies...\nBefore moving to the Code let first understand the Architectural Design\n### Strategy 1: Replace 3\u00d73 filters with 1\u00d71 filters\nTypically a 3X3 filter may capture the spatial information of pixels close to each other while the 1X1 filter zeros in on a pixel and captures the features amongst its channels. Given a budget of a certain number of convolution filters, we can choose to make the majority of these filters 1\u00d71, since a 1\u00d71 filter has 9 times fewer parameters than a 3\u00d73 filter.\n### Strategy 2: Decrease the number of input channels to 3\u00d73 filters\nWe decrease the number of input channels to 3x3 filters using squeeze layers. The author of the paper has used a term called the **fire module**. \n   * **FireModule:-** In which there is a squeeze layer and an expand layer. In the squeeze layer we are only using 1X1 filters while in expand layer we are using a combination of 3X3 filters and 1X1 filters. The author is trying to limit the number of inputs to the 3X3 filters so as to reduce the number of parameters in the layer. \n![fire.png](attachment:fire.png)\nConsider a convolution layer that is comprised entirely of 3\u00d73 filters. The total quantity of parameters in this layer is:(number of input channels) \u00d7 (number of filters) \u00d7 (3\u00d73).\n\n### Strategy 3: Downsample late in the network so that convolution layers have large activation maps.\nAs per paper,due to delayed downsampling large activation maps can lead to higher classification accuracy.\n### Summary:-\n* Strategies 1 and 2 are about to decreasing the quantity of parameters in a CNN while attempting to preserve accuracy.\n* Strategy 3 is about maximizing accuracy on a limited budget of parameters.\n\n## Let see the Final Architecture look like....\n![archi.JPG](attachment:archi.JPG)\n\n## Details of SqueezeNet Architecture\n![parameter.JPG](attachment:parameter.JPG)\n* SqueezeNet (Left): begins with a standalone convolution layer (conv1), followed by 8 Fire modules (fire2\u20139), ending with a final conv layer (conv10).\n* The number of filters per fire module is gradually increased from the beginning to the end of the network.\n* Max-pooling with a stride of 2 is performed after layers conv1, fire4, fire8, and conv10.\n* SqueezeNet with simple bypass (Middle) and SqueezeNet with complex bypass (Right): The use of bypass is inspired by ResNet.","b857b705":"## Checkout the dataset...","2f905ce5":"## View the predicted Results...","c2c6354d":"## References...\n* Paper on SqueezeNet by DeepScale & UC Berkeley and Stanford University.[Click here](https:\/\/arxiv.org\/pdf\/1602.07360.pdf)\n* Resources on SqueezeNet.[Click here](https:\/\/en.wikipedia.org\/wiki\/SqueezeNet)\n* Article on SqueezeNet on Medium.[Click here](https:\/\/medium.com\/@smallfishbigsea\/notes-of-squeezenet-4137d51feef4)\n\n## My other works on Kaggle...\n* NeuroImaging Challange.[click here](https:\/\/www.kaggle.com\/saife245\/neuroimaging-in-depth-understanding-eda-model)\n* Image Augmentation Techniques.[Click here](https:\/\/www.kaggle.com\/saife245\/cutmix-vs-mixup-vs-gridmask-vs-cutout)\n* Reticlute:Run Python on R.[click here](https:\/\/www.kaggle.com\/saife245\/reticulate-a-journey-of-python-in-r)\n* 100+ visulization on R. [click here](https:\/\/www.kaggle.com\/saife245\/r-challenge-of-100-visualization-in-r)\n","5b59567f":"## Data augmentation\nYou will get a couple more percentage points by augmenting the data with easy transformations like left-right flips of saturation changes:\n\n![aug.JPG](attachment:aug.JPG)","452539ea":"## About Data Type...\n* Data is in [.tfrec] format which is tensorflow record Format.\n* **TensorFlow Record:-** A TFRecord file stores your data as a sequence of binary strings.\n* **Importances...**\n    * Binary data takes up less space on disk, takes less time to copy and can be read much more efficiently from disk. This is especially true if your data is stored on spinning disks, due to the much lower read\/write performance in comparison with SSDs.\n    *  Especially for datasets that are too large to be stored fully in memory this is an advantage as only the data that is required at the time (e.g. a batch) is loaded from disk and then processed.\n    * Another major advantage of TFRecords is that it is possible to store sequence data \u2014 for instance, a time series or word encodings \u2014 in a way that allows for very efficient and (from a coding perspective) convenient import of this type of data.\n* you need to specify the structure of your data before you write it to the file. Tensorflow provides two components for this purpose: \n    * tf.train.Example and \n    * tf.train.SequenceExample.\n* You have to store each sample of your data in one of these structures, then serialize it and use a tf.python_io.TFRecordWriter to write it to disk.","5f24dcfa":"## training and validation datasets","0eaf3442":"## Read images and labels from TFRecords","b54dc05d":"## Training","beec73e5":"## Model","f9cd7fcf":"## SqueezeNet to model compression approaches...\n![model_perform.png](attachment:model_perform.png)\n* With SqueezeNet, we achieve a 50\u00d7 reduction in model size compared to AlexNet.\n* while we matchup or, exceeding the top-1 and top-5 accuracy of AlexNet.\n* Model size reduction is much higher in Squeezenet than any other method of models.\n\n## Different Hyperparameter Values for SqueezeNet...\n![param.png](attachment:param.png)\n* Squeeze ratio (SR) (Left): the ratio between the number of filters in squeeze layers and the number of filters in expand layers.\n* Increasing SR ratio beyond 0.125 can further increase ImageNet top-5 accuracy from 80.3% (i.e. AlexNet-level) with a 4.8MB model to 86.0% with a 19MB model. Accuracy plateaus at 86.0% with SR=0.75 (a 19MB model), and setting SR=1.0 further increases model size without improving accuracy.\n* Percentage of 3\u00d73 Filters (Right): Top-5 accuracy plateaus at 85.6% using 50% 3\u00d73 filters, and further increasing the percentage of 3\u00d73 filters leads to a larger model size but provides no improvement in accuracy on ImageNet.","562068f3":"## Configuration","8a65378c":"## What is the idea behind squeeze net?\nIn designing SqueezeNet, the authors' goal was to create a smaller neural network with fewer parameters that can more easily fit into computer memory and can more easily be transmitted over a computer network.\n\n## History Chunk....\nSqueezeNet was originally released on February 22, 2016.This original version of SqueezeNet was implemented on top of the Caffe deep learning software framework. Shortly thereafter, the open-source research community ported SqueezeNet to a number of other deep learning frameworks. On February 26, 2016, Eddie Bell released a port of SqueezeNet for the Chainer deep learning framework.On March 2, 2016, Guo Haria released a port of SqueezeNet for the Apache MXNet framework. On June 3, 2016, Tammy Yang released a port of SqueezeNet for the Keras framework.In 2017, companies including Baidu, Xilinx, Imagination Technologies, and Synopsys demonstrated SqueezeNet running on low-power processing platforms such as smartphones, FPGAs, and custom processors.\n\nAs of 2018, SqueezeNet ships \"natively\" as part of the source code of a number of deep learning frameworks such as PyTorch, Apache MXNet, and Apple CoreML.In addition, 3rd party developers have created implementations of SqueezeNet that are compatible with frameworks such as TensorFlow.\n\n## Why we need it?\nAs we already discuss, if we have a model which has a small size then we can easily deploy it in cloud. So that\u2019s why we needed a architecture which is less in size and also achieve the same level of accuracy that other architecture achieves. With equivalent accuracy, smaller CNN architectures offer at least three advantages:- \n1. Smaller Convolutional Neural Networks (CNNs) require less communication across servers during distributed training.\n2. Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car.\n3. Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory.\n\n## SqueezeNet Relationship to Deep Compression...\nModel compression (e.g. quantization and pruning of model parameters) can be applied to a deep neural network after it has been trained.In the SqueezeNet paper, the authors demonstrated that a model compression technique called Deep Compression can be applied to SqueezeNet to further reduce the size of the parameter file from 5MB to 500KB.Deep Compression has also been applied to other DNNs such as AlexNet and VGG.","3af3e343":"## Family of SqueezeNet...\n![fam.JPG](attachment:fam.JPG)\n* Interestingly, the simple bypass enabled a higher accuracy accuracy improvement than complex bypass.\n* Adding the simple bypass connections yielded an increase of 2.9 percentage-points in top-1 accuracy and 2.2 percentage-points in top-5 accuracy without increasing model size.\n\n### Some other approach...\n![fa.JPG](attachment:fa.JPG)"}}