{"cell_type":{"82c3467f":"code","98ebc18b":"code","a44b165e":"code","eba578d4":"code","b9939be5":"code","41967f21":"code","2e37af44":"code","3777cac6":"code","3a0374b2":"code","6e19d9f1":"code","49979497":"code","159ef661":"code","93b7482f":"code","4d83457e":"code","0e0ccd79":"code","df04b714":"code","a411b5c1":"code","1bcf8f93":"code","c8fbc1b1":"code","fe0d3d5c":"code","cf898bca":"code","47a43598":"code","0450ac7c":"code","c64e04dd":"code","fdab8781":"code","32b45582":"code","a8b6523f":"code","17a746e3":"code","12006b89":"code","1b4b75fc":"code","af100c33":"code","29ae12af":"code","22b4381b":"markdown","03f54547":"markdown","ea02fc07":"markdown","60660c9e":"markdown","a18f2b58":"markdown","5e17cb47":"markdown","cbecc0c4":"markdown","422bd625":"markdown","2ace72ca":"markdown","3f9502bd":"markdown","c5e0caf2":"markdown","176ea64e":"markdown","8444215f":"markdown","d644632e":"markdown","dd720611":"markdown","f386bc88":"markdown","a4e02eba":"markdown","254a687d":"markdown","905e963a":"markdown"},"source":{"82c3467f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\n\nfrom tqdm import tqdm\n","98ebc18b":"train = pd.read_csv(\"..\/input\/scl-2021-ds\/train.csv\")\ntrain.head()","a44b165e":"test = pd.read_csv(\"..\/input\/scl-2021-ds\/test.csv\")\ntest.head()","eba578d4":"splitstring = train[\"POI\/street\"].str.partition('\/')\ntrain[\"POI\"] = splitstring[0]\ntrain[\"street\"] = splitstring[2]\n\ntrain.head()","b9939be5":"from textblob import TextBlob\n\ntrain[\"tokenize\"] = train[\"raw_address\"].map(lambda x: TextBlob(x).words)\ntrain[\"tokenizedPOI\"] = train[\"POI\"].map(lambda x: TextBlob(x).words)\ntrain[\"tokenizedStreet\"] = train[\"street\"].map(lambda x: TextBlob(x).words)\n\ntest[\"tokenize\"] = test[\"raw_address\"].map(lambda x: TextBlob(x).words)\n","41967f21":"train.head()","2e37af44":"test.head()","3777cac6":"x = pd.concat([train.tokenize, test.tokenize], axis=0)\nx.head()","3a0374b2":"import gzip\nimport gensim \nimport logging\nimport warnings\n\n#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nwarnings.filterwarnings('ignore')\n\n#train the model\nmodel = gensim.models.FastText(\n    x,\n    size=10, \n    window=2, \n    min_count=1, \n    workers=10, \n    sg = 0, \n    iter=100\n  )","6e19d9f1":"longest = 0\n\nfor _, row in train.iterrows():\n    if longest < len(row.tokenize): longest = len(row.tokenize)\n\nprint(\"train max token: \", longest)\n\nlongest = 0\n\nfor _, row in test.iterrows():\n    if longest < len(row.tokenize): longest = len(row.tokenize)\n\nprint(\"test max token: \", longest)","49979497":"TRAIN_DATA = []\nTRAIN_LABEL = []\n\nfor _, row in train.iterrows():\n    temp = []\n    templabel = []\n\n    count = 0\n    for word in row[\"tokenize\"]:\n        temp.append(np.array(model[word]))\n        count = count + 1\n        if(word in row['tokenizedPOI']):\n            templabel.append(np.array([1,0,0,0])) # POI (if else prioritize POI, either i am too lazy or the the time constraining me hard)\n        elif(word in row['tokenizedStreet']):\n            templabel.append(np.array([0,1,0,0])) # street\n        else:\n            templabel.append(np.array([0,0,1,0])) # not POI or Street\n\n    if(count != 32):\n        for i in range(32-count):\n            temp.append(np.zeros(10))\n            templabel.append(np.zeros(4))\n\n    TRAIN_DATA.append(np.array(temp))\n    TRAIN_LABEL.append(np.array(templabel))","159ef661":"TRAIN_DATA = np.asarray(TRAIN_DATA)\nTRAIN_DATA.shape","93b7482f":"TRAIN_LABEL = np.asarray(TRAIN_LABEL)\nTRAIN_LABEL.shape","4d83457e":"TEST_DATA = []\n\nfor _, row in test.iterrows():\n    temp = []\n    count = 0\n    for word in row[\"tokenize\"]:\n        temp.append(np.array(model[word]))\n        count = count + 1\n\n    if(count != 32):\n        for i in range(32-count):\n            temp.append(np.zeros(10))\n\n    TEST_DATA.append(np.array(temp))\n","0e0ccd79":"TEST_DATA = np.asarray(TEST_DATA)\nTEST_DATA.shape","df04b714":"np.save(\".\/shopee_train_data\",TRAIN_DATA )\nnp.save(\".\/shopee_train_label\",TRAIN_LABEL )\nnp.save(\".\/shopee_test_data\",TEST_DATA )\n","a411b5c1":"TRAIN_DATA = np.load(\"..\/input\/shopee-address-extraction\/shopee_train_data.npy\")\nTRAIN_LABEL = np.load(\"..\/input\/shopee-address-extraction\/shopee_train_label.npy\")\nTEST_DATA = np.load(\"..\/input\/shopee-address-extraction\/shopee_test_data.npy\")","1bcf8f93":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(TRAIN_DATA, TRAIN_LABEL, test_size=0.2)","c8fbc1b1":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input\n\ninput_layer = Input(\n    shape= (32, 10),\n    name='input'\n)\n\ndropout_layer = Dropout(0.1)(input_layer)\n\nbilstm_layer = Bidirectional(LSTM(units=100, return_sequences=True))(dropout_layer)\n\noutput_layer = TimeDistributed(Dense(4, activation=\"softmax\"))(bilstm_layer)\n\nmodel = Model(inputs=[input_layer], outputs=[output_layer])\n\nmodel.compile(\n  optimizer='adam',\n  loss='categorical_crossentropy',\n  sample_weight_mode='temporal',\n  metrics=['accuracy']\n)","fe0d3d5c":"model.summary()","cf898bca":"history = model.fit(\n  x = X_train, \n  y = y_train, \n  validation_data = (X_val, y_val),\n  epochs = 25,\n  initial_epoch = 0\n)","47a43598":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.ylim(ymin=0)\nplt.ylim(ymax=1)\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","0450ac7c":"model.save(\n    filepath = '.\/shopee.h5' \n)","c64e04dd":"from tensorflow.keras.models import load_model\n\nmodel = load_model('..\/input\/shopee-address-extraction\/shopee.h5')","fdab8781":"import re","32b45582":"pred = model.predict(TRAIN_DATA)\ntrain[\"prediction\"] = \"\"","a8b6523f":"for index, row in train.iterrows():\n    street = \"\"\n    POI = \"\"\n    temp = str(np.argmax(pred[index], axis=1)[:len(row.tokenize)]).replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n        \n    startIndexStreet = temp.index(max(re.findall(r'[1]*', temp), key=len))\n    endIndexStreet = startIndexStreet + len(max(re.findall(r'[1]*', temp), key=len))\n    startIndexPOI = temp.index(max(re.findall(r'[0]*', temp), key=len))\n    endIndexPOI = startIndexPOI + len(max(re.findall(r'[0]*', temp), key=len))\n\n    if(startIndexStreet != endIndexStreet): \n        endStreet = row[\"raw_address\"].find(row[\"tokenize\"][endIndexStreet-1]) + len(row[\"tokenize\"][endIndexStreet-1])\n        street = str(row[\"raw_address\"][startIndexStreet:endStreet])\n\n    if(startIndexPOI != endIndexPOI): \n        endPOI = row[\"raw_address\"].find(row[\"tokenize\"][endIndexPOI-1]) + len(row[\"tokenize\"][endIndexPOI-1])\n        POI = str(row[\"raw_address\"][startIndexPOI:endPOI])\n        \n    train.at[index, \"prediction\"] = str(POI+\"\/\"+street).strip()\n  \n    ","17a746e3":"train[[\"POI\/street\", \"prediction\"]].head(10)","12006b89":"predtest = model.predict(TEST_DATA)\ntest[\"POI\/street\"] = \"\"\n","1b4b75fc":"for index, row in test.iterrows():\n    street = \"\"\n    POI = \"\"\n    temp = str(np.argmax(pred[index], axis=1)[:len(row.tokenize)]).replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n        \n    startIndexStreet = temp.index(max(re.findall(r'[1]*', temp), key=len))\n    endIndexStreet = startIndexStreet + len(max(re.findall(r'[1]*', temp), key=len))\n    startIndexPOI = temp.index(max(re.findall(r'[0]*', temp), key=len))\n    endIndexPOI = startIndexPOI + len(max(re.findall(r'[0]*', temp), key=len))\n\n    if(startIndexStreet != endIndexStreet): \n        endStreet = row[\"raw_address\"].find(row[\"tokenize\"][endIndexStreet-1]) + len(row[\"tokenize\"][endIndexStreet-1])\n        street = str(row[\"raw_address\"][startIndexStreet:endStreet])\n\n    if(startIndexPOI != endIndexPOI): \n        endPOI = row[\"raw_address\"].find(row[\"tokenize\"][endIndexPOI-1]) + len(row[\"tokenize\"][endIndexPOI-1])\n        POI = str(row[\"raw_address\"][startIndexPOI:endPOI])\n        \n    test.at[index, \"POI\/street\"] = str(POI+\"\/\"+street).strip()\n      ","af100c33":"test.head()","29ae12af":"test[[\"id\",\"POI\/street\"]].to_csv(\"submission.csv\", index = False)","22b4381b":"## Generate Submission","03f54547":"## Tokenize 'raw_address', 'POI', and 'street' column ","ea02fc07":"# 2. Sequence Labelling with Bi-LSTM","60660c9e":"## Check longest length of 'raw_address' for padding purposes","a18f2b58":"# This was an attempt to extract addresses with Bi-LSTM.\n\nThe score wasn't great, I believe there shall be a modification on the evaluation.\n\nIn this notebook we will only consider the longest consecutive label of \"POI\" and \"street\". \n\nFor example, in [POI POI POI NOT POI NOT NOT], the 3 first consecutive words is considered as the \"POI\", although there is a possibility the \"NOT\" in the fourth position is also \"POI\", thus, making the POI the 5 first consecutive word (possibility: handling in softmax result?)","5e17cb47":"## Split the 'POI\/street' column into 2 distinct column","cbecc0c4":"## Save and Load Data","422bd625":"## Vector Embedding using Fasttext","2ace72ca":"## Evaluation Metric: Categorisation Accuracy\n![](https:\/\/drive.google.com\/uc?id=14S26ivnzGWKjKhGXefsu-_Ab3QDDwUT9)\n\nThere are addresses with missing POI or street elements. For such cases, you should leave it empty for that specific element of that address. The formula for the overall metric is defined as the average of accuracy score for each address as following:\n\n![](https:\/\/drive.google.com\/uc?id=109GvXu2PybfYflQzVJZeYQrrbB6vrkmC)","3f9502bd":"## Read the data","c5e0caf2":"## Save and Load Model","176ea64e":"# 1. Details of Competition","8444215f":"## Split Data into Train and Validation","d644632e":"## Evaluate Model on Train Data","dd720611":"## Generate LSTM-friendly Data","f386bc88":"## Task\nIn this competition, you\u2019ll work on addresses collected by us to build a model to correctly extract Point of Interest (POI) Names and Street Names from unformatted Indonesia addresses.\n\nParticipants are expected to build their own model for this competition, submissions by teams which directly call any third party APIs on the test set will not be taken into consideration.","a4e02eba":"## Background\nAt Shopee, we strive to ensure our customers' highest satisfaction for their shopping and delivery experience - fast and accurate delivery of goods. This can be better achieved if we have key address elements for each user address which allows us to accurately geocode it to obtain geographic coordinates to ship the parcel to our customers. These key address elements include Point of Interest (POI) Names and Street Names. However, most addresses that Shopee receives are unstructured and in free text format, not following a certain pattern. Thus it is important for us to develop a model to precisely extract the key address elements from it.\n","254a687d":"## Import Library","905e963a":"## Bi-LSTM "}}