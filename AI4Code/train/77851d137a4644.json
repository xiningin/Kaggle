{"cell_type":{"9ca26ad4":"code","febcfa60":"code","378e333f":"code","1ee2f6db":"code","473a7a5d":"code","46dbe134":"code","a7ef3097":"code","f241a4ab":"code","36be897a":"code","28ca8487":"code","78532421":"code","02cff50e":"code","07f7a6b9":"code","29de6bef":"markdown","4cfbce34":"markdown","705cf800":"markdown","e6ca674e":"markdown","ae605f25":"markdown","dac28de4":"markdown","041a903a":"markdown","0c812b64":"markdown","167a4803":"markdown","d507b1e0":"markdown","a3e8d98b":"markdown"},"source":{"9ca26ad4":"from scipy.signal import butter, detrend, filtfilt\nfrom lini_read_bcidat import lini_read_bcidat\nimport numpy as np\n\n# default params\ndp = {'car': 1, 'ica': 0, 'local_tend': 1, 'norm': 1, 'filtro': []}\ndw = [-100., 800.]\ndch = np.arange(10)\n\n\ndef obten_p300_bci2000(f, channels=dch, window=dw, paco=dp, decimation=False, decimation_samples=4):\n\n    data = lini_read_bcidat(f)\n    raw_signal, states_matrix, labels, paco['fs'] = data\n\n    states = {}\n\n    for i, label in enumerate(labels):\n        states[label] = states_matrix[:, i]\n\n    vi = int(np.round(np.abs(window[0]) * paco['fs'] \/ 1000))\n    vf = int(np.round(window[1] * paco['fs'] \/ 1000))\n\n    t = np.linspace(window[0], window[1], vf + vi)\n    if not paco['filtro']:\n        low = 2 \/ paco['fs']\n        high = 24 \/ paco['fs']\n\n        b, a = butter(4, [low, high], btype='bandpass')\n        paco['filtro'].append({'b': b, 'a': a})\n\n    signal = raw_signal[:, channels].copy()\n    n_muestras, n_canales = signal.shape\n    \n    if paco['car'] == 1:\n        signal = detrend(signal.T, type='constant', bp=0)\n        signal = signal.T\n        signal = detrend(signal, type='constant', bp=0)\n        \n    if len(paco['filtro']) != 0:\n        for item in paco['filtro']:\n            signal = filtfilt(item['b'], item['a'], signal.T).T\n\n    ind = (np.array(np.where(np.diff(states['StimulusCode']) > 0)) + 1).reshape(-1)\n\n    letras = (np.where(np.diff(states['PhaseInSequence']) > 0))[0]\n    letras = np.concatenate([letras[0::2], [letras[-1]]])\n    \n    data = np.zeros([ind.shape[0], int(vf + vi), n_canales])\n    etiqueta = -np.ones((ind.shape[0], 2))\n    \n    for i, index in enumerate(ind):\n        rowcol = states['StimulusCode'][index]\n\n        if index - vi < 0:\n            raise ValueError('tiempo inicial muy largo')\n            \n        data[i, :, :] = signal[(index - vi):(index + vf), :]\n\n        if states['StimulusType'][index] == 1:\n            etiqueta[i, :] = [1, rowcol]\n        else:\n            etiqueta[i, :] = [-1, rowcol]\n            \n    if decimation:\n        idx = np.arange(0, data.shape[1], decimation_samples)\n        data = data[:, idx, :]\n        t = t[idx]\n\n    return (raw_signal, signal, data, etiqueta, ind, states, t, letras)\n\n\nchannel_names = [\n    'Fz', 'C3', 'Cz', 'C4',\n    'P3', 'Pz', 'P4', 'PO7',\n    'PO8', 'Oz'\n]","febcfa60":"import os\n\ndef load_train(train_path):\n    data = None\n    \n    for filename in os.listdir(train_path):\n        full = os.path.join(train_path, filename)\n        \n        if '.dat' in full:            \n            rt, st, da, et, it, sta, tt, lt = obten_p300_bci2000(full, decimation=True)\n            \n            if data is None:\n                data = da\n                t = tt\n                etiquetas = et\n            else:\n                data = np.vstack([data, da])\n                etiquetas = np.vstack([etiquetas, et])\n                \n    return data, etiquetas, t\n\n\ndef load_test(test_path):\n    \n    files = []\n    for filename in os.listdir(test_path):\n        full = os.path.join(test_path, filename)\n        \n        if '.dat' in filename:\n            files.append(full)\n    \n    return files","378e333f":"def avg_train_idx(signal, split_samples=5, shuffle=True):\n    # Returns the indices of the signal split into subarrays\n    \n    samples = signal.shape[0]\n    \n    if samples % split_samples > 0:\n        samples = samples - (samples % split_samples)\n    \n    idx = np.arange(samples)\n    \n    if shuffle:\n        np.random.shuffle(idx)\n\n    idx = np.split(idx, samples \/ split_samples)\n    \n    return idx\n\n\ndef avg_split_signal(signal, labels, split_samples=5, shuffle=True):\n    erp_idx = np.where(labels == 1)[0]\n    nerp_idx = np.where(labels == 0)[0]\n\n    erp = signal[erp_idx, :, :]\n    nerp = signal[nerp_idx, :, :]\n\n    erp_shuffle_idx = avg_train_idx(erp, split_samples=split_samples, shuffle=shuffle)\n    nerp_shuffle_idx = avg_train_idx(nerp, split_samples=split_samples, shuffle=shuffle)\n\n    erp = erp[erp_shuffle_idx].mean(axis=1)\n    nerp = nerp[nerp_shuffle_idx].mean(axis=1)\n\n    new_labels = np.hstack([np.ones(erp.shape[0]), np.zeros(nerp.shape[0])])\n    new_signal = np.vstack([erp, nerp])\n\n    idx = np.arange(new_signal.shape[0])\n    \n    if shuffle:\n        np.random.shuffle(idx)\n\n    new_signal = new_signal[idx]\n    new_labels = new_labels[idx]\n    \n    return new_signal, new_labels\n\n\ndef make_labels_sweep(input_labels):\n    splits = np.split(input_labels, int(input_labels.shape[0] \/ 12))\n    labels = []\n    \n    for split in splits:\n        labels.append(1 * (split[split[:, 1].argsort()][:, 0] == 1))\n        \n    return labels","1ee2f6db":"class BaseClassifier:\n    def speller_predictions(self, data):\n        preds = self.predict(data)\n        \n        row_idx = np.argmax(preds[:6, 1])\n        col_idx = np.argmax(preds[6:, 1])\n\n        vector = np.zeros(preds.shape[0], dtype='uint8')\n\n        vector[row_idx] = 1\n        vector[col_idx + 6] = 1\n\n        return row_idx, col_idx, vector","473a7a5d":"def plot_confusion_matrix(cm, ax=None, cbar=True):\n    # Code inspired in https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/fd237278e\/sklearn\/metrics\/_plot\/confusion_matrix.py\n    \n    labels = ['Background', 'ERP']\n    \n    im = ax.imshow(cm)\n    cmap_min, cmap_max = im.cmap(0), im.cmap(256)\n    \n    if cbar:\n        ax.figure.colorbar(im)\n    \n    ax.set_xticks(np.arange(len(labels)))\n    ax.set_yticks(np.arange(len(labels)))\n    \n    ax.set_ylim((len(labels) - 0.5, -0.5))\n    \n    ax.set_xticklabels(labels)\n    ax.set_yticklabels(labels)\n    \n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')\n    \n    thresh = (cm.max() - cm.min()) \/ 2\n    \n    for i in range(len(labels)):\n        for j in range(len(labels)):\n            color = cmap_max if cm[i, j] < thresh else cmap_min\n            text = ax.text(j, i, cm[i, j], ha='center', va='center', color=color)","46dbe134":"import sklearn.svm\nimport scipy\n\n\nclass SVMBaseline(BaseClassifier):\n    def __init__(self, channels):\n        # channels contains the channels that will be used to train the classifier\n        # note: this channels are zero-indexed.\n        \n        clfs = []\n        \n        for channel in channels:\n            clf = sklearn.svm.SVC(kernel='rbf', gamma='scale', class_weight='balanced')\n            clfs.append(('Classifier for channel {}'.format(channel + 1), channel, clf))\n            \n        self.clfs = clfs\n\n    def fit(self, signal, labels, avg=True, verbose=False):\n        # avg is for averaging the signal before training\n        \n        labels = (labels[:, 0] + 1) \/\/ 2\n        \n        if avg:\n            signal, labels = avg_split_signal(signal, labels)\n        \n        for label, channel, clf in self.clfs:   \n            if verbose:\n                print('[+] Training classifier for channel {}'.format(channel + 1))\n\n            clf.fit(signal[:, :, channel], labels)\n            \n    def predict(self, signal, hard=False):\n        predictions = np.zeros((signal.shape[0], 2))\n        \n        for label, channel, clf in self.clfs:\n            prediction = clf.decision_function(signal[:, :, channel])\n            \n            idx = np.where(prediction < clf.intercept_)\n            predictions[idx, 0] = predictions[idx, 0] + np.abs(prediction[idx])\n            \n            idx = np.where(prediction >= clf.intercept_)\n            predictions[idx, 1] = predictions[idx, 1] + np.abs(prediction[idx])\n        \n        return scipy.special.softmax(predictions, axis=1)","a7ef3097":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass OCLNN(BaseClassifier, torch.nn.Module):\n    def __init__(self, n_channels, n_samples, channel_order=None):\n        super(OCLNN, self).__init__()\n        \n        if n_channels == 0:\n            if channel_order is None:\n                raise ValueError('if n_channels equals 0, channel_order must be specified.')\n                \n            n_channels = len(channel_order)\n        \n        div = n_samples \/\/ 15\n        \n        self.conv1 = nn.Conv1d(n_channels, 16, div, stride=div)\n        self.dropout  = nn.Dropout(p=0.25)\n        self.linear = nn.Linear(self.out_shape(self.conv1, n_samples), 2)\n        \n        if torch.cuda.is_available():\n            device = torch.device('cuda')\n        else:\n            device = torch.device('cpu')\n        \n        self.to(device)\n        self.device = device\n        self.channel_order = channel_order\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.linear(x)\n        \n        return x\n    \n    def fit(self, signal, labels, epochs=1000, avg=True, verbose=True):\n        # Assuming the current signal shape is (batch, data, channels)\n        # Assuming labels come directly from obten_p300...\n        self.train()\n        \n        signal = np.swapaxes(signal, 1, 2)\n        labels = (labels[:, 0] + 1) \/\/ 2\n        \n        if self.channel_order is not None:\n            signal = signal[:, self.channel_order, :]\n        \n        criteron = torch.nn.CrossEntropyLoss().to(self.device)\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01, momentum=0.9, weight_decay=0.0005)\n        \n        if avg:\n            signal, labels = avg_split_signal(signal, labels, split_samples=5, shuffle=True)\n\n        torch_signal = torch.from_numpy(signal).float().to(self.device)\n        torch_labels = torch.from_numpy(labels).long().to(self.device)\n\n        for t in range(epochs):\n            y_pred = self(torch_signal).float()\n            loss = criteron(y_pred.float(), torch_labels)\n\n            if t % 100 == 99 and verbose:\n                print('[+] Loss at epoch {}: {:.6f}'.format(t, loss.item()))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n    def predict(self, signal):\n        # Assuming signal is in the form: (batch, samples, channels)\n        self.eval()\n        \n        signal = np.swapaxes(signal, 1, 2)\n        \n        if self.channel_order is not None:\n            signal = signal[:, self.channel_order, :]\n        \n        signal = torch.from_numpy(signal).float().to(self.device)\n        predictions = F.softmax(self(signal), dim=1)\n        \n        return predictions.detach().cpu().numpy()\n        \n    def out_shape(self, conv, len_in):        \n        num = len_in + 2 * conv.padding[0] - conv.dilation[0] * (conv.kernel_size[0] - 1) - 1\n        den = conv.stride[0]\n        \n        return ((np.floor(num \/ den) + 1) * conv.out_channels).astype('uint64')","f241a4ab":"class Benchmark:\n    def __init__(self, test_path):\n        _, _, signal, labels, _, _, _, _ = obten_p300_bci2000(test_path, decimation=True)\n            \n        type_labels = (labels[:, 0] + 1) \/\/ 2\n        \n        self.type_labels = type_labels\n        self.test_path = test_path\n        self.labels = labels\n        self.signal = signal\n        \n        matrix = [\n            ['a', 'b', 'c', 'd', 'e', 'f'],\n            ['g', 'h', 'i', 'j', 'k', 'l'],\n            ['m', 'n', 'o', 'p', 'q', 'r'],\n            ['s', 't', 'u', 'v', 'w', 'x'],\n            ['y', 'z', '_', '1', '2', '3'],\n            ['4', '5', '6', '7', '8', '9'],\n        ]\n        \n        self.matrix = matrix\n        \n    def confusion_matrix(self, classifier):\n        # Returns the confusion matrices:\n        #     1. Individual prediction\n        #     2. Averaged prediction\n        \n        test_signal = self.signal\n        test_labels = self.type_labels\n        \n        preds = []\n        \n        # Signal averaging\n        test_erp = None\n        test_nerp = None\n        \n        for feature, label in zip(test_signal, test_labels):\n            feature = feature[np.newaxis, :, :]\n            \n            if label == 0:\n                if test_nerp is None:\n                    test_nerp = feature\n                else:\n                    test_nerp = np.vstack([test_nerp, feature])\n                    \n                latest = test_nerp\n                \n            elif label == 1:\n                if test_erp is None:\n                    test_erp = feature\n                else:\n                    test_erp = np.vstack([test_erp, feature])\n                    \n                latest = test_erp\n                \n            pred = classifier.predict(latest.mean(axis=0)[np.newaxis])[0]\n            pred = 1 * (pred[0] < pred[1])\n            \n            preds.append(pred)\n            \n        ind_pred = classifier.predict(test_signal)\n        ind_pred = 1 * (ind_pred[:, 0] < ind_pred[:, 1])\n        \n        cm1 = sklearn.metrics.confusion_matrix(test_labels, ind_pred)\n        cm2 = sklearn.metrics.confusion_matrix(test_labels, preds)\n        \n        return cm1, cm2\n    \n    def speller_accuracy(self, classifier):\n        signal_test = self.signal\n        etiquetas_test = self.labels\n        matrix = self.matrix\n        \n        sweeps = np.array_split(etiquetas_test[:, 1], int(etiquetas_test.shape[0] \/ 12))\n\n        r_sum = np.zeros((12, signal_test.shape[1], signal_test.shape[2]))\n        letters = []\n\n        targets = make_labels_sweep(etiquetas_test)\n        epoch_preds = np.zeros(15)\n        epoch_matrix = []\n        row_matrix = []\n\n        bn = 0\n        i = 0\n        for target, sweep in zip(targets, sweeps):\n            sweep_data = np.zeros((12, signal_test.shape[1], signal_test.shape[2]))\n\n            for row in sweep:\n                if row > 0:\n                    row = int(row - 1)\n                    sweep_data[row, :, :] = signal_test[i, :, :]\n\n                    i += 1\n\n            r_sum = r_sum + sweep_data\n\n            if bn % 14 == 0 and bn > 0:\n                row_idx, col_idx, vec = classifier.speller_predictions(r_sum \/ 15)\n\n                if np.array_equal(vec, target):\n                    epoch_preds[bn] += 1\n\n                l = matrix[row_idx][col_idx]\n\n                row_matrix.append(l)\n                epoch_matrix.append(row_matrix)\n                letters.append(l)\n\n                bn = 0\n                row_matrix = []\n                r_sum = np.zeros((12, signal_test.shape[1], signal_test.shape[2]))\n\n            else:\n                row_idx, col_idx, vec = classifier.speller_predictions(r_sum \/ (bn + 1))\n\n                if np.array_equal(vec, target):\n                    epoch_preds[bn] += 1\n\n                l = matrix[row_idx][col_idx]\n                row_matrix.append(l)\n\n                bn += 1\n\n        return epoch_preds, epoch_matrix, ''.join(letters)","36be897a":"import matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.backends.backend_pdf import PdfPages","28ca8487":"subject = 'LAG'\n\nbase_path = '..\/input\/akimpech\/P300db\/'\nkaggle_output = f'\/kaggle\/working\/{subject}'\n\ntrain_path = f'{base_path}{subject}\/{subject}001\/'\n\nsignal, etiquetas, t = load_train(train_path)\n        \nif not os.path.exists(kaggle_output):\n    os.mkdir(kaggle_output)","78532421":"fig = plt.figure(figsize=(15, 25))\nouter_grid = fig.add_gridspec(5, 1, hspace=0.5)\n\nplot_offset = 0.032\n\nerp = signal[np.where(etiquetas[:, 0] == 1)[0], :, :]\nnerp = signal[np.where(etiquetas[:, 0] == -1)[0], :, :]\n\ninner_grid = outer_grid[0:2].subgridspec(2, 5, wspace=0.5, hspace=0.5)\n\nplt.figtext(0.5, 0.92, 'Report for Subject {}'.format(subject), ha='center', fontsize='xx-large', weight='bold')\nplt.figtext(0.5, 0.9, 'Channel Averages', ha='center', fontsize='large', weight='bold')\n\nfor channel in range(10):\n    ax = fig.add_subplot(inner_grid[channel])\n\n    ax.plot(t, erp[:, :, channel].mean(axis=0), label='ERP')\n    ax.plot(t, nerp[:, :, channel].mean(axis=0), label='Background')\n    \n    ax.set(\n        title=channel_names[channel],\n        ylabel='A.U.',\n        xlabel='Time [ms]',\n    )\n    \n    ax.legend(loc='lower left')\n    ax.grid()\n    fig.add_subplot(ax)\n\n    \nocl_avg = OCLNN(signal.shape[2], signal.shape[1])\nocl_nrm = OCLNN(signal.shape[2], signal.shape[1])\n\nocl_avg.fit(signal, etiquetas, verbose=False)\nocl_nrm.fit(signal, etiquetas, avg=False, verbose=False)\n\nsvm_avg = SVMBaseline([1, 2, 3, 5, 7])\nsvm_nrm = SVMBaseline([1, 2, 3, 5, 7])\n\nsvm_avg.fit(signal, etiquetas)\nsvm_nrm.fit(signal, etiquetas, avg=False)\n\nbench = Benchmark(f'{base_path}{subject}\/{subject}002\/{subject}S002R01.dat')\n\nocl_cm = bench.confusion_matrix(ocl_nrm)\nsvm_cm = bench.confusion_matrix(svm_nrm)\n\ninner_grid = outer_grid[2].subgridspec(1, 4, wspace=0.7)\n\nax1 = fig.add_subplot(inner_grid[0])\nax1.set_title('CNN\\nIndividual')\nplot_confusion_matrix(ocl_cm[0], ax=ax1, cbar=False)\n\nax2 = fig.add_subplot(inner_grid[1])\nax2.set_title('CNN\\nAveraging')\nplot_confusion_matrix(ocl_cm[1], ax=ax2, cbar=False)\n\nax3 = fig.add_subplot(inner_grid[2])\nax3.set_title('SVM\\nIndividual')\nplot_confusion_matrix(svm_cm[0], ax=ax3, cbar=False)\n\nax4 = fig.add_subplot(inner_grid[3])\nax4.set_title('SVM\\nAveraging')\nplot_confusion_matrix(svm_cm[1], ax=ax4, cbar=False)\n\nbbox = ax4.get_position().bounds\nplt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Classifiers Trained Without Averaging Aggregated Data', ha='center', fontsize='large', weight='bold')\n\nfig.add_subplot(ax1)\nfig.add_subplot(ax2)\nfig.add_subplot(ax3)\nfig.add_subplot(ax4)\n\nocl_cm = bench.confusion_matrix(ocl_avg)\nsvm_cm = bench.confusion_matrix(svm_avg)\n\ninner_grid = outer_grid[3].subgridspec(1, 4, wspace=0.7)\n\nax1 = fig.add_subplot(inner_grid[0])\nax1.set_title('CNN\\nIndividual')\nplot_confusion_matrix(ocl_cm[0], ax=ax1, cbar=False)\n\nax2 = fig.add_subplot(inner_grid[1])\nax2.set_title('CNN\\nAveraging')\nplot_confusion_matrix(ocl_cm[1], ax=ax2, cbar=False)\n\nax3 = fig.add_subplot(inner_grid[2])\nax3.set_title('SVM\\nIndividual')\nplot_confusion_matrix(svm_cm[0], ax=ax3, cbar=False)\n\nax4 = fig.add_subplot(inner_grid[3])\nax4.set_title('SVM\\nAveraging')\nplot_confusion_matrix(svm_cm[1], ax=ax4, cbar=False)\n\nbbox = ax4.get_position().bounds\nplt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Classifiers Trained Averaging Aggregated Data', ha='center', fontsize='large', weight='bold')\n\nfig.add_subplot(ax1)\nfig.add_subplot(ax2)\nfig.add_subplot(ax3)\nfig.add_subplot(ax4)\n\n# Speller\n\nocl_nrm_speller = bench.speller_accuracy(ocl_nrm)\nsvm_nrm_speller = bench.speller_accuracy(svm_nrm)\n\nocl_avg_speller = bench.speller_accuracy(ocl_avg)\nsvm_avg_speller = bench.speller_accuracy(svm_avg)\n\nepochs = np.arange(ocl_nrm_speller[0].shape[0]) + 1\nword_size = len(ocl_nrm_speller[2])\n\ninner_grid = outer_grid[4].subgridspec(1, 2)\n\nax = fig.add_subplot(inner_grid[0])\n\nax.plot(epochs, ocl_nrm_speller[0] \/ word_size, label='OCLNN')\nax.plot(epochs, svm_nrm_speller[0] \/ word_size, label='SVM')\n\nax.set(\n    title='Trained Without Averaging Data',\n    xlabel='# Epoch',\n    ylabel='Accuracy',\n    ylim=[0, 1.1],\n    xlim=[1, 15],\n)\n\nax.grid()\nax.legend(loc='lower right')\n\nfig.add_subplot(ax)\n\nax = fig.add_subplot(inner_grid[1])\n\nax.plot(epochs, ocl_avg_speller[0] \/ word_size, label='OCLNN')\nax.plot(epochs, svm_avg_speller[0] \/ word_size, label='SVM')\n\nax.set(\n    title='Trained Averaging Data',\n    xlabel='# Epoch',\n    ylabel='Accuracy',\n    ylim=[0, 1.1],\n    xlim=[1, 15],\n)\n\nax.grid()\nax.legend(loc='lower right')\n\nbbox = ax.get_position().bounds\nplt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Speller Accuracy for Second Session', ha='center', fontsize='large', weight='bold')\n\nfig.add_subplot(ax)\n\nplt.show()","02cff50e":"for subject in os.listdir(base_path):\n    if os.path.isdir(os.path.join(base_path, subject)):\n        print('[+] Creating results for subject {}'.format(subject))\n        \n        kaggle_output = f'\/kaggle\/working\/{subject}'\n        pdf_output = f'\/kaggle\/working\/{subject}\/report.pdf'\n        \n        train_path = f'{base_path}{subject}\/{subject}001\/'\n\n        signal, etiquetas, t = load_train(train_path)\n        \n        if not os.path.exists(kaggle_output):\n            os.mkdir(kaggle_output)\n        \n        fig = plt.figure(figsize=(15, 25))\n        outer_grid = fig.add_gridspec(5, 1, hspace=0.5)\n\n        plot_offset = 0.032\n\n        erp = signal[np.where(etiquetas[:, 0] == 1)[0], :, :]\n        nerp = signal[np.where(etiquetas[:, 0] == -1)[0], :, :]\n\n        inner_grid = outer_grid[0:2].subgridspec(2, 5, wspace=0.5, hspace=0.5)\n\n        plt.figtext(0.5, 0.92, 'Report for subject {}'.format(subject), ha='center', fontsize='xx-large', weight='bold')\n        plt.figtext(0.5, 0.9, 'Channel Averages', ha='center', fontsize='large', weight='bold')\n\n        for channel in range(10):\n            ax = fig.add_subplot(inner_grid[channel])\n\n            ax.plot(t, erp[:, :, channel].mean(axis=0), label='ERP')\n            ax.plot(t, nerp[:, :, channel].mean(axis=0), label='Background')\n\n            ax.set(\n                title=channel_names[channel],\n                ylabel='A.U.',\n                xlabel='Time [ms]',\n            )\n\n            ax.legend(loc='lower left')\n            ax.grid()\n            fig.add_subplot(ax)\n\n\n        ocl_avg = OCLNN(signal.shape[2], signal.shape[1])\n        ocl_nrm = OCLNN(signal.shape[2], signal.shape[1])\n\n        ocl_avg.fit(signal, etiquetas, verbose=False)\n        ocl_nrm.fit(signal, etiquetas, avg=False, verbose=False)\n\n        svm_avg = SVMBaseline([1, 2, 3, 5, 7])\n        svm_nrm = SVMBaseline([1, 2, 3, 5, 7])\n\n        svm_avg.fit(signal, etiquetas)\n        svm_nrm.fit(signal, etiquetas, avg=False)\n        \n        # Try to find the file associated with the second session.\n        base_subject = f'{base_path}{subject}\/{subject}002\/'\n        \n        fname = list(filter(lambda x: '.dat' in x, os.listdir(base_subject)))[0]\n        fname = os.path.join(base_subject, fname)\n        \n        print(fname)\n        bench = Benchmark(fname)\n\n        ocl_cm = bench.confusion_matrix(ocl_nrm)\n        svm_cm = bench.confusion_matrix(svm_nrm)\n\n        inner_grid = outer_grid[2].subgridspec(1, 4, wspace=0.7)\n\n        ax1 = fig.add_subplot(inner_grid[0])\n        ax1.set_title('CNN\\nIndividual')\n        plot_confusion_matrix(ocl_cm[0], ax=ax1, cbar=False)\n\n        ax2 = fig.add_subplot(inner_grid[1])\n        ax2.set_title('CNN\\nAveraging')\n        plot_confusion_matrix(ocl_cm[1], ax=ax2, cbar=False)\n\n        ax3 = fig.add_subplot(inner_grid[2])\n        ax3.set_title('SVM\\nIndividual')\n        plot_confusion_matrix(svm_cm[0], ax=ax3, cbar=False)\n\n        ax4 = fig.add_subplot(inner_grid[3])\n        ax4.set_title('SVM\\nAveraging')\n        plot_confusion_matrix(svm_cm[1], ax=ax4, cbar=False)\n\n        bbox = ax4.get_position().bounds\n        plt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Classifiers Trained Without Averaging Aggregated Data', ha='center', fontsize='large', weight='bold')\n\n        fig.add_subplot(ax1)\n        fig.add_subplot(ax2)\n        fig.add_subplot(ax3)\n        fig.add_subplot(ax4)\n\n        ocl_cm = bench.confusion_matrix(ocl_avg)\n        svm_cm = bench.confusion_matrix(svm_avg)\n\n        inner_grid = outer_grid[3].subgridspec(1, 4, wspace=0.7)\n\n        ax1 = fig.add_subplot(inner_grid[0])\n        ax1.set_title('CNN\\nIndividual')\n        plot_confusion_matrix(ocl_cm[0], ax=ax1, cbar=False)\n\n        ax2 = fig.add_subplot(inner_grid[1])\n        ax2.set_title('CNN\\nAveraging')\n        plot_confusion_matrix(ocl_cm[1], ax=ax2, cbar=False)\n\n        ax3 = fig.add_subplot(inner_grid[2])\n        ax3.set_title('SVM\\nIndividual')\n        plot_confusion_matrix(svm_cm[0], ax=ax3, cbar=False)\n\n        ax4 = fig.add_subplot(inner_grid[3])\n        ax4.set_title('SVM\\nAveraging')\n        plot_confusion_matrix(svm_cm[1], ax=ax4, cbar=False)\n\n        bbox = ax4.get_position().bounds\n        plt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Classifiers Trained Averaging Aggregated Data', ha='center', fontsize='large', weight='bold')\n\n        fig.add_subplot(ax1)\n        fig.add_subplot(ax2)\n        fig.add_subplot(ax3)\n        fig.add_subplot(ax4)\n\n        # Speller\n\n        ocl_nrm_speller = bench.speller_accuracy(ocl_nrm)\n        svm_nrm_speller = bench.speller_accuracy(svm_nrm)\n\n        ocl_avg_speller = bench.speller_accuracy(ocl_avg)\n        svm_avg_speller = bench.speller_accuracy(svm_avg)\n\n        epochs = np.arange(ocl_nrm_speller[0].shape[0]) + 1\n        word_size = len(ocl_nrm_speller[2])\n\n        inner_grid = outer_grid[4].subgridspec(1, 2)\n\n        ax = fig.add_subplot(inner_grid[0])\n\n        ax.plot(epochs, ocl_nrm_speller[0] \/ word_size, label='OCLNN')\n        ax.plot(epochs, svm_nrm_speller[0] \/ word_size, label='SVM')\n\n        ax.set(\n            title='Trained Without Averaging Data',\n            xlabel='# Epoch',\n            ylabel='Accuracy',\n            ylim=[0, 1.1],\n            xlim=[1, 15],\n        )\n\n        ax.grid()\n        ax.legend(loc='lower right')\n\n        fig.add_subplot(ax)\n\n        ax = fig.add_subplot(inner_grid[1])\n\n        ax.plot(epochs, ocl_avg_speller[0] \/ word_size, label='OCLNN')\n        ax.plot(epochs, svm_avg_speller[0] \/ word_size, label='SVM')\n\n        ax.set(\n            title='Trained Averaging Data',\n            xlabel='# Epoch',\n            ylabel='Accuracy',\n            ylim=[0, 1.1],\n            xlim=[1, 15],\n        )\n\n        ax.grid()\n        ax.legend(loc='lower right')\n\n        bbox = ax.get_position().bounds\n        plt.figtext(0.5, bbox[1] + bbox[3] + plot_offset, 'Speller Accuracy for Second Session', ha='center', fontsize='large', weight='bold')\n\n        fig.add_subplot(ax)\n        \n        with PdfPages(pdf_output) as pdf:\n            pdf.savefig(fig)\n            \n        plt.close()","07f7a6b9":"!zip -r output.zip \/kaggle\/working\/","29de6bef":"# OCLNN Definition","4cfbce34":"# SVM Definition","705cf800":"The following code provides base functions for the speller simulation trials.","e6ca674e":"# Data Loading\n\n`obten_p300_bci2000` is used to process the data from the files, filtering and removing the trend.","ae605f25":"# Single Report Generation","dac28de4":"# Full Report Generation","041a903a":"# Classifier benchmark\n\nReturns values used to plot the performance of the classifier to test, such as the confusion matrices and the overall performance over the speller. It assumes the classifiers are already trained. The data to test over is given by the user.","0c812b64":"# Experimental functions\n\nThe following functions are to split the signal and average it into equal size-groups to aid the classifiers learning the P300 average representation. Although, in some cases training the classifiers with the averages is not as effective as training them with the individual signals.","167a4803":"`load_train` is used to load all the files from a single folder.\n\n`load_test` returns a list of files that `lini_read_bcidat` is able to interpret.","d507b1e0":"# Graphing functions","a3e8d98b":"# Introduction\n\nThis notebook is used to generate the results for each of the subjects located under the akimpech folder.\n\nThe classifiers are trained using the data from the first session (every subfolder ending with S001) and tested again the data from the third folder folder (every subfolder ending with S003)."}}