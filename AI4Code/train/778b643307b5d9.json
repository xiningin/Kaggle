{"cell_type":{"497e59ab":"code","0287c78b":"code","e4cdb7ff":"code","c5e5556e":"code","5f1f2a36":"code","d4bc85fd":"code","671d7616":"code","4286747c":"code","ebb2d55f":"code","1b085559":"code","09f559ae":"code","3ad8bdf2":"code","840c1f7e":"code","002d1dd7":"code","082f6b1d":"code","7ac293f1":"code","56aa108a":"code","80cb3e02":"code","3c05e897":"code","62806846":"code","4ae13be1":"code","60db0317":"code","bbf80144":"code","f2cf2e7a":"code","98df882f":"code","d122d34a":"code","2e7bf6e7":"code","25dc27f7":"code","ff598c50":"code","6ca9c1ca":"code","b301771c":"code","716a922f":"code","6ed0eda4":"code","9fb5822f":"code","53a0f8c2":"code","f0660c64":"code","0256a442":"code","4f77724a":"code","628c1850":"code","8bd73d9f":"code","a9e7157d":"code","96dbd8a1":"code","45175439":"code","6c48b20e":"code","74d67189":"code","373716e6":"code","4f570386":"code","4f939ac7":"code","46845f35":"code","68030681":"code","c906c9d9":"code","befc5d8e":"code","f50d51e6":"code","1aaef43f":"code","5407c851":"code","c31a54b8":"code","2b8e7c44":"code","274177b8":"code","1d04ed30":"code","9802e145":"code","9e0d2c58":"code","64e4c16b":"code","49509f48":"code","271a0420":"code","e9b33899":"code","2afcdb9d":"code","c2388a3b":"code","2b0841e5":"code","1fff183e":"code","3e8c7440":"markdown","a0ef1d56":"markdown","b084791c":"markdown","5f38e1e2":"markdown","3e03d660":"markdown","fab320ad":"markdown","1f1fdd54":"markdown","bfebd5cc":"markdown","840dd982":"markdown","aece4cd2":"markdown","01aca76a":"markdown","d2d90a07":"markdown","17d685f6":"markdown","5b913d9a":"markdown","7bb1090f":"markdown","d314abf3":"markdown","25bfdb7c":"markdown","887d2634":"markdown","088e0162":"markdown","c6e71cdc":"markdown","ae326cab":"markdown","335e9dd8":"markdown"},"source":{"497e59ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport glob\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport seaborn as sns\nimport xgboost as xg\nimport tables as tb\nfrom tqdm import tqdm\nfrom itertools import cycle, islice\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.datasets import make_blobs\nfrom IPython import display\nfrom sklearn.neighbors import BallTree, KDTree, DistanceMetric\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import Normalizer\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.metrics import log_loss\nfrom keras.layers.core import Dense, Activation\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nimport utils\n\n%matplotlib inline\n%pylab inline\n\n# Make the images larger\nplt.rcParams['figure.figsize'] = (16, 9)\nfigsize = (10,10)\npoint_size=150\npoint_border=0.8\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","0287c78b":"training = pd.read_csv('..\/input\/particle-identification\/training.csv')\ntest = pd.read_csv('..\/input\/particle-identification\/test.csv')","e4cdb7ff":"training.shape","c5e5556e":"test.shape","5f1f2a36":"training.head()","d4bc85fd":"test.head()","671d7616":"def my_percentile(arr, w, q):\n    left = 0.\n    right = (w).sum()\n    sort_inds = np.argsort(arr, axis=0)\n    if left\/right >= q\/100.:\n        return arr[0]\n    for i in sort_inds:\n        left += w[i]\n        if left\/right >= q\/100.:\n            return arr[i]\n\ndef plot(prediction, spectator, cut, percentile=True, weights=None, n_bins=100,\n              color='b', marker='o', ms=4, label=\"MVA\", fmt='o', markeredgecolor='b', markeredgewidth=2, ecolor='b'):\n    if weights is None:\n        weights = np.ones(len(prediction))\n\n    if percentile:\n        if weights is None:\n            cut = np.percentile(prediction, 100-cut)\n        else:\n            cut = my_percentile(prediction, weights, 100-cut)\n    \n    edges = np.linspace(spectator.min(), spectator.max(), n_bins)\n    \n    xx = []\n    yy = []\n    xx_err = []\n    yy_err = []\n    \n    for i_edge in range(len(edges)-1):\n\n        left = edges[i_edge]\n        right = edges[i_edge + 1]\n        \n        N_tot_bin = weights[((spectator >= left) * (spectator < right))].sum()\n        N_cut_bin = weights[((spectator >= left) * (spectator < right) * (prediction >= cut))].sum()\n        \n        if N_tot_bin != 0:\n            \n            x = 0.5 * (right + left)\n            y = 1. * N_cut_bin \/ N_tot_bin\n            \n            if y > 1.:\n                y = 1.\n            if y < 0:\n                y = 0\n            \n            xx.append(x)\n            yy.append(y)\n            \n            x_err = 0.5 * (right - left)\n            y_err = np.sqrt(y*(1-y)\/N_tot_bin)\n            \n            xx_err.append(x_err)\n            yy_err.append(y_err)\n        \n        else:\n            pass\n\n    plt.errorbar(xx, yy, yerr=yy_err, xerr=xx_err, fmt=fmt, color=color, marker=marker, ms=ms, label=label, markeredgecolor=markeredgecolor, markeredgewidth=markeredgewidth, ecolor=ecolor)\n    \n    return cut","4286747c":"training['Label'].value_counts()","ebb2d55f":"# Get numeric labels for each of the string labels, to make them compatible with our model\nlabel_class_correspondence = {'Electron': 0, 'Ghost': 1, 'Kaon': 2, 'Muon': 3, 'Pion': 4, 'Proton': 5}\nclass_label_correspondence = {0: 'Electron', 1: 'Ghost', 2: 'Kaon', 3: 'Muon', 4: 'Pion', 5: 'Proton'}\n\ndef get_class_ids(labels):\n    return np.array([label_class_correspondence[alabel] for alabel in labels])","1b085559":"training['Class'] = get_class_ids(training.Label.values)","09f559ae":"features = list(set(training.columns) - {'Label', 'Class'})","3ad8bdf2":"train, valid = train_test_split(training, random_state=42, train_size=0.90, test_size=0.10)\nprint(train.shape[0], valid.shape[0])","840c1f7e":"train.head()","002d1dd7":"clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.01, random_state=42,\n                             base_estimator=DecisionTreeClassifier(max_depth=6, min_samples_leaf=30, random_state=42))\nclf.fit(train[features].values, train.Class.values)","082f6b1d":"proba_clf = clf.predict_proba(valid[features].values)\nlog_loss(valid.Class.values, proba_clf)","7ac293f1":"def roc_curves(predictions, labels):\n    plt.figure(figsize=(9, 6))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        y_pred = predictions[:, lab]\n        fpr, tpr, _ = roc_curve(y_true, y_pred)\n        auc = roc_auc_score(y_true, y_pred)\n        plt.plot(tpr, 1-fpr, linewidth=3, label=class_label_correspondence[lab] + ', AUC = ' + str(np.round(auc, 4)))\n        plt.xlabel('Signal efficiency (TPR)', size=15)\n        plt.ylabel(\"Background rejection (1 - FPR)\", size=15)\n        plt.xticks(size=15)\n        plt.yticks(size=15)\n        plt.xlim(0., 1)\n        plt.ylim(0., 1)\n        plt.legend(loc='lower left', fontsize=15)\n        plt.title('One particle vs rest ROC curves', loc='right', size=15)\n        plt.grid(b=1)","56aa108a":"roc_curves(proba_clf, valid.Class.values)","80cb3e02":"def efficiency(predictions, labels, spectator, eff=60, n_bins=20, xlabel='Spectator'):\n    plt.figure(figsize=(5.5*2, 3.5*3))\n    u_labels = np.unique(labels)\n    for lab in u_labels:\n        y_true = labels == lab\n        pred = predictions[y_true, lab]\n        spec = spectator[y_true]\n        plt.subplot(3, 2, lab+1)\n        plot(pred, spec, cut=eff, percentile=True, weights=None, n_bins=n_bins, color='1', marker='o', \n                  ms=7, label=class_label_correspondence[lab], fmt='o')\n        \n        plt.plot([spec.min(), spec.max()], [eff \/ 100., eff \/ 100.], label='Global signal efficiecny', color='r', linewidth=3)\n        plt.legend(loc='best', fontsize=12)\n        plt.xticks(size=12)\n        plt.yticks(size=12)\n        plt.ylabel('Signal efficiency (TPR)', size=12)\n        plt.xlabel(xlabel,size=12)\n        plt.ylim(0, 1)\n        plt.xlim(spec.min(), spec.max())\n        plt.grid(b=1)\n    plt.tight_layout()\n        \n\ndef efficiency_on_p(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 200 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] \/ 10**3, eff, n_bins, 'Momentum, GeV\/c')","3c05e897":"efficiency_on_p(proba_clf, valid.Class.values, valid.TrackP.values, 60, 50)\nplt.show()","62806846":"def efficiency_on_pt(predictions, labels, spectator, eff=60, n_bins=20):\n    sel = spectator < 10 * 10**3\n    efficiency(predictions[sel], labels[sel], spectator[sel] \/ 10**3, eff, n_bins, 'Transverse momentum, GeV\/c')","4ae13be1":"efficiency_on_pt(proba_clf, valid.Class.values, valid.TrackPt.values, 60, 50)\nplt.show()","60db0317":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(100, input_dim=input_dim))\n    model.add(Activation('tanh'))\n\n    model.add(Dense(6))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n    return model","bbf80144":"nn = nn_model(len(features))\nnn.fit(train[features].values, np_utils.to_categorical(train.Class.values), verbose=1, nb_epoch=5, batch_size=256)","f2cf2e7a":"proba_nn = nn.predict_proba(valid[features].values)\nlog_loss(valid.Class.values, proba_nn)","98df882f":"proba_nn","d122d34a":"roc_curves(proba_nn, valid.Class.values)","2e7bf6e7":"roc_curves(proba_clf, valid.Class.values)","25dc27f7":"# release unreferenced memory to ensure we don't run out of memory\nimport gc\ngc.collect()","ff598c50":"del train, test, valid, training","6ca9c1ca":"train = pd.read_hdf('..\/input\/dark-matter-from-opera-experiments\/open30.h5') # pick just a single brick\ntest = pd.read_hdf('..\/input\/dark-matter-from-opera-experiments\/test.h5')","b301771c":"train.shape","716a922f":"test = test.reset_index(drop=True)\ntest.shape","6ed0eda4":"train.head()","9fb5822f":"train.columns","53a0f8c2":"train.signal.value_counts()","f0660c64":"test.head()","0256a442":"train_signal = train.copy()\ntrain_signal.head()","4f77724a":"train_signal = train_signal[train['signal']==1]\ntrain_signal.signal.value_counts()","628c1850":"train_signal.head()","8bd73d9f":"CMAP = sns.diverging_palette(220, 20, s=99, as_cmap=True, n=2500)\n\ndef plot3D(X, target, elev=0, azim=0, title=None, sub=111):\n    x = X[:, 0]\n    y = X[:, 1]\n    z = X[:, 2]\n    \n    fig = plt.figure(figsize=(12, 8))\n    ax = Axes3D(fig)\n    mappab = ax.scatter(x, y, z, c=target, cmap=CMAP)\n\n    if title is not None:\n        ax.set_title(title)\n    ax.set_xlabel('Component 1')\n    ax.set_ylabel('Component 2')\n    ax.set_zlabel('Component 3')\n\n    ax.view_init(elev=elev, azim=azim)\n    fig.colorbar(mappable=mappab, label='Target variable')\n    plt.show()\n    \nfeat_XY = ['TX', 'TY', 'X', 'Y']\nfirst = train.loc[train.data_ind == 21, :]\nplot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=20, azim=60)","a9e7157d":"plot3D(first.loc[first.signal==1, ['Z', 'X', 'Y']].values,\n       first.loc[first.signal==1].signal.values, elev=45, azim=0)","96dbd8a1":"axis = 'X'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","45175439":"axis = 'Y'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","6c48b20e":"axis = 'Z'\n\nfig = plt.figure(figsize = [20, 10])\nfig.add_subplot(221)\nplt.hist(first.loc[first.signal == 1, axis], bins=500, histtype='step')\nfig.add_subplot(222)\nplt.hist(first.loc[first.signal == 0, axis], bins=500, histtype='step')\nfig.add_subplot(223)\nvalues_X = plt.hist(first.loc[:, axis], bins=500, histtype='step')\nplt.show()","74d67189":"def add_neighbours(df, k, metric='minkowski'):\n    res = []\n    \n    for data_ind in tqdm(np.unique(df.data_ind)):\n        ind = df.loc[df.data_ind == data_ind].copy()\n        ind[['TX', 'TY']] *= 1293\n        values = np.unique(ind.Z)\n        \n        for j in range(1, len(values)):\n            z, z_next = (ind.loc[ind.Z == values[j-1]].copy(),\n                         ind.loc[ind.Z == values[j]].copy())\n            \n            b_tree = BallTree(z_next[feat_XY], metric=metric)\n            d, i = b_tree.query(z[feat_XY], k=min(k, len(z_next)))\n            \n            for m in range(i.shape[1]):\n                data = z_next.iloc[i[:, m]]\n                z_copy = z.copy()\n                for col in feat_XY + ['Z']:\n                    z_copy[col + '_pair'] = data[col].values\n                res.append(z_copy)\n            \n        res.append(z_next)\n        \n    res = pd.concat(res)\n    for col in feat_XY + ['Z']:\n        res['d' + col] = res[col].values - res[col + '_pair'].values\n    return res\n\ndef balance_train(df, k):\n    data = add_neighbours(df, k=k)\n    noise = data.event_id == -999\n    signal, not_signal = data.loc[np.logical_not(noise)], data.loc[noise]\n    noise_part = not_signal.sample(len(signal))\n    return pd.concat([signal, noise_part]).reset_index(drop=True)\ntrain = []\nfor file in glob.glob('..\/input\/dark-matter-from-opera-experiments\/open*.h5')[:5]: # just 5 bricks\n    train.append(balance_train(pd.read_hdf(file), k=3))\ntrain = pd.concat(train)","373716e6":"y_train = train.signal\nX_train = train.drop(['event_id', 'signal', 'data_ind'], axis=1)","4f570386":"transformer = Normalizer()\nX_train_norm = transformer.fit_transform(X_train.fillna(0))","4f939ac7":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\ntrain_index, val_index = next(sss.split(X_train_norm, y_train))","46845f35":"def nn_model(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(128))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(64))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy', optimizer=Adam())\n    return model","68030681":"callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto'),\n            ModelCheckpoint('{val_loss:.4f}.hdf5', monitor='val_loss', verbose=2, save_best_only=True, mode='auto')]","c906c9d9":"nn = nn_model(X_train_norm.shape[1])\nnn.fit(X_train_norm, y_train, validation_split=0.2, epochs=20, verbose=2, batch_size=256, shuffle=True, callbacks=callbacks)","befc5d8e":"prepared_test = add_neighbours(test, k=3)\nX_test = prepared_test.drop(['data_ind'], axis=1)","f50d51e6":"X_test_norm = transformer.transform(X_test.fillna(0))\nX_test = transformer.transform(X_test.fillna(0))","1aaef43f":"X_test_norm[:5]","5407c851":"probas = nn.predict_proba(X_test_norm)","c31a54b8":"probas = np.squeeze(probas)","2b8e7c44":"df = pd.DataFrame({'id': prepared_test.index, 'signal': probas}).groupby('id')\nagg = df.aggregate(('mean')).loc[:, ['signal']]","274177b8":"agg.head()","1d04ed30":"train_signal.fillna(0, inplace=True)","9802e145":"'''\neps=0.000001\nmin_samples=2\ndbscan = DBSCAN(eps=eps, min_samples=min_samples)\nclustering_labels = dbscan.fit_predict(train_signal)\n'''","9e0d2c58":"kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(train_signal)\n                for k in range(1, 10)]\ninertias = [model.inertia_ for model in kmeans_per_k]\nk = [inertias.index(i) for i in inertias]","64e4c16b":"plt.plot(k, inertias, linewidth=2.0)\nline, = plt.plot(k, inertias, 'o')\nplt.xlabel(\"$k$\", fontsize=14)\nplt.ylabel(\"Inertia\", fontsize=14)","49509f48":"kmeans = KMeans(n_clusters=6, random_state=42).fit(train_signal)\nclustering_labels = kmeans.labels_","271a0420":"train_signal.shape","e9b33899":"clustering_labels.shape","2afcdb9d":"clusters = train_signal\nclusters['cluster'] = clustering_labels","c2388a3b":"X_sample = train_signal.sample(frac=0.1, random_state=42)","2b0841e5":"X_sample.head()","1fff183e":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_sample.X, X_sample.Y, X_sample.Z, c=X_sample.cluster)","3e8c7440":"Create a dataframe of signals for clustering later on","a0ef1d56":"Just to compare performances, here are the ROC curves for our AdaBoostClassifier model.","b084791c":"## Train an AdaBoost model ","5f38e1e2":"**Find the optimal number of clusters**","3e03d660":"# Searching for dark matter","fab320ad":"Since we're trying to predict the label (or class), we remove them from our list of features.","1f1fdd54":"# Particle Identification","bfebd5cc":"## Using a Neural Net to separate signal from background","840dd982":"The inertia's rate of decline flattens around k=6 clusters. So we'll train a KMeans with 6 clusters.","aece4cd2":"## Explore the dataset","01aca76a":"## If you like this kernel, please give it an upvote. Thank you! :)","d2d90a07":"Training a second, neural network model and comparing performance via ROC curves","17d685f6":"# Introduction\nThis kernel accompanies the [blog post](https:\/\/lavanya.ai\/searching-for-dark-matter\/) I wrote about searching for traces of dark matter in data produced by CERN.\n\nI\u2019ve recently been having a lot of fun playing with the Large Hadron Collider datasets, and I thought I\u2019ll share some of the things I\u2019ve learnt along the way. The 2 main things I\u2019ll be exploring in this post are:\n\n- **Particle identification**: training a classifier to detect electrons, protons, muons, kaons and pions\n- **Searching for dark matter**: training a classifier to distinguish between background noise and the signal, and then applying clustering algorithms to find potential traces of dark matter in this signal\n\nYou can get more background by reading the blog post here: https:\/\/lavanya.ai\/searching-for-dark-matter\/\n\nPS: I would love to hear what you think about the post and the kernel, and any suggestions you have for how I can improve it! This kernel is a WIP. I'll add to it over the next few weeks as I explore the dataset further. Thank you Coursera for presenting some of the ideas that inspired this kernel!\n\n## If you like this kernel, please give it an upvote. Thank you! :)","5b913d9a":"As we can see, AdaBoostClassifier performs slightly better than the neural net across the board for all particles.","7bb1090f":"## Explore dataset and find neighboring base tracks","d314abf3":"Split the training data into training and validation sets.","25bfdb7c":"## Using KMeans to find signal clusters","887d2634":"Plot showers in the brick","088e0162":"## Train a Neural Network","c6e71cdc":"From the dataset description, we know that the features describe particle responses in the detector systems, and represent the following:\n\n- ID - id value for tracks (presents only in the test file for the submitting purposes)\n- Label - string valued observable denoting particle types. Can take values \"Electron\", \"Muon\", \"Kaon\", \"Proton\", \"Pion\" and \"Ghost\". This column is absent in the test file.\n- FlagSpd - flag (0 or 1), if reconstructed track passes through Spd\n- FlagPrs - flag (0 or 1), if reconstructed track passes through Prs\n- FlagBrem - flag (0 or 1), if reconstructed track passes through Brem\n- FlagEcal - flag (0 or 1), if reconstructed track passes through Ecal\n- FlagHcal - flag (0 or 1), if reconstructed track passes through Hcal\n- FlagRICH1 - flag (0 or 1), if reconstructed track passes through the first RICH detector\n- FlagRICH2 - flag (0 or 1), if reconstructed track passes through the second RICH detector\n- FlagMuon - flag (0 or 1), if reconstructed track passes through muon stations (Muon)\n- SpdE - energy deposit associated to the track in the Spd\n- PrsE - energy deposit associated to the track in the Prs\n- EcalE - energy deposit associated to the track in the Hcal\n- HcalE - energy deposit associated to the track in the Hcal\n- PrsDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Prs\n- BremDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Brem\n- TrackP - particle momentum\n- TrackPt - particle transverse momentum\n- TrackNDoFSubdetector1  - number of degrees of freedom for track fit using hits in the tracking sub-detector1\n- TrackQualitySubdetector1 - chi2 quality of the track fit using hits in the tracking sub-detector1\n- TrackNDoFSubdetector2 - number of degrees of freedom for track fit using hits in the tracking sub-detector2\n- TrackQualitySubdetector2 - chi2 quality of the track fit using hits in the  tracking sub-detector2\n- TrackNDoF - number of degrees of freedom for track fit using hits in all tracking sub-detectors\n- TrackQualityPerNDoF - chi2 quality of the track fit per degree of freedom\n- TrackDistanceToZ - distance between track and z-axis (beam axis)\n- Calo2dFitQuality - quality of the 2d fit of the clusters in the calorimeter \n- Calo3dFitQuality - quality of the 3d fit in the calorimeter with assumption that particle was electron\n- EcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Ecal\n- EcalDLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from Ecal\n- EcalShowerLongitudinalParameter - longitudinal parameter of Ecal shower\n- HcalDLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from Hcal\n- HcalDLLbeMuon - delta log-likelihood for a particle candidate to be using information from Hcal\n- RICHpFlagElectron - flag (0 or 1) if momentum is greater than threshold for electrons to produce Cherenkov light\n- RICHpFlagProton - flag (0 or 1) if momentum is greater than threshold for protons to produce Cherenkov light\n- RICHpFlagPion - flag (0 or 1) if momentum is greater than threshold for pions to produce Cherenkov light\n- RICHpFlagKaon - flag (0 or 1) if momentum is greater than threshold for kaons to produce Cherenkov light\n- RICHpFlagMuon - flag (0 or 1) if momentum is greater than threshold for muons to produce Cherenkov light\n- RICH_DLLbeBCK  - delta log-likelihood for a particle candidate to be background using information from RICH\n- RICH_DLLbeKaon - delta log-likelihood for a particle candidate to be kaon using information from RICH\n- RICH_DLLbeElectron - delta log-likelihood for a particle candidate to be electron using information from RICH\n- RICH_DLLbeMuon - delta log-likelihood for a particle candidate to be muon using information from RICH\n- RICH_DLLbeProton - delta log-likelihood for a particle candidate to be proton using information from RICH\n- MuonFlag - muon flag (is this track muon) which is determined from muon stations\n- MuonLooseFlag muon flag (is this track muon) which is determined from muon stations using looser criteria\n- MuonLLbeBCK - log-likelihood for a particle candidate to be not muon using information from muon stations\n- MuonLLbeMuon - log-likelihood for a particle candidate to be muon using information from muon stations\n- DLLelectron - delta log-likelihood for a particle candidate to be electron using information from all subdetectors\n- DLLmuon - delta log-likelihood for a particle candidate to be muon using information from all subdetectors\n- DLLkaon - delta log-likelihood for a particle candidate to be kaon using information from all subdetectors\n- DLLproton - delta log-likelihood for a particle candidate to be proton using information from all subdetectors\n- GhostProbability - probability for a particle candidate to be ghost track. This variable is an output of classification model used in the tracking algorithm.\n\nSpd stands for Scintillating Pad Detector, Prs - Preshower, Ecal - electromagnetic calorimeter, Hcal - hadronic calorimeter, Brem denotes traces of the particles that were deflected by detector.","ae326cab":"Closer look at base track distribution along the axes","335e9dd8":"Group base tracks from neighboring plates (see blog post for intuition behind why we do this)"}}