{"cell_type":{"7789ff9b":"code","2d50091d":"code","88dfd468":"code","dbaed08a":"code","d485b8d8":"code","b5f67ec2":"code","cd582773":"code","0a6fefc9":"code","4ced0dda":"code","c2634c8b":"code","eda3442e":"code","b763736c":"code","c66ca602":"code","e7665ea6":"code","5d7c3816":"code","0ab98915":"code","51b6df8f":"code","d4df9f98":"code","a37b0657":"code","daefb376":"code","22c1c05b":"code","99524f73":"code","f8dbe4e0":"code","b099982e":"code","ae0dc08d":"code","fc00c22e":"code","869e9a69":"code","996a593d":"code","f1ed149a":"code","c17db193":"code","ad4d6283":"code","cdbc6573":"code","ccfa3807":"code","4be2dd0b":"code","4545785a":"code","e31e7150":"code","03a45ec9":"code","f2f4e919":"code","f77e4dcb":"code","6a63319c":"code","1776fa2c":"code","b48fe178":"code","4691d58a":"code","8d15221b":"code","cfb8e139":"code","8e5dafbd":"code","161c64f8":"code","0cd1e816":"code","2d9441d6":"code","e5159a17":"code","78d2dfcd":"code","9deb819e":"code","5317ed0e":"code","f7d1fcca":"markdown","ddbbe351":"markdown","b4663c4b":"markdown","49e4d163":"markdown","b31b99c5":"markdown","809f4bc9":"markdown","ef1ef474":"markdown","e6ee5e32":"markdown","1aaa1595":"markdown","27e0ad0a":"markdown","900b972a":"markdown","689e0430":"markdown","5981bcc8":"markdown","3d93becb":"markdown","b65e9d64":"markdown","3d80b286":"markdown","9caa6ba7":"markdown","ba53aa0d":"markdown","4dc835ef":"markdown","5869a37c":"markdown","20cff557":"markdown","163696c1":"markdown","9214d3d5":"markdown","ebefe264":"markdown","54f6e1aa":"markdown","8e570959":"markdown"},"source":{"7789ff9b":"import pandas as pd \nimport numpy as np \nfrom IPython.display import display\n\nimport matplotlib.pyplot as plt \nimport re\nimport string\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import words\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n\nfrom collections import Counter\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nimport plotly.express as px\n\nsns.set(style=\"darkgrid\")","2d50091d":"df=pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')\ndf.head(5)","88dfd468":"df.shape","dbaed08a":"needed_columns=['user_name','date','text']\ndf=df[needed_columns]\ndf.head()","d485b8d8":"df.user_name=df.user_name.astype('category')\ndf.user_name=df.user_name.cat.codes # assign a unique numerical code to each category\ndf.date=pd.to_datetime(df.date).dt.date","b5f67ec2":"df.head(5)","cd582773":"texts=df.text\ntexts","0a6fefc9":"remove_url=lambda x:re.sub(r'http\\S+','',str(x))\ntexts_lr=texts.apply(remove_url)\ntexts_lr","4ced0dda":"to_lower=lambda x: x.lower()\ntexts_lr_lc=texts_lr.apply(to_lower)\ntexts_lr_lc","c2634c8b":"remove_puncs= lambda x:x.translate(str.maketrans('','',string.punctuation))\ntexts_lr_lc_np=texts_lr_lc.apply(remove_puncs)\ntexts_lr_lc_np","eda3442e":"more_words=['say','going','like','U','u','#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19','coronavirus', 'covid19']\nstop_words=set(stopwords.words('english')) #nltk package\nstop_words.update(more_words)\n\nremove_words=lambda x: ' '.join([word for word in x.split() if word not in stop_words]) #.join is from package string\ntexts_lr_lc_np_ns=r=texts_lr_lc_np.apply(remove_words)\ntexts_lr_lc_np_ns","b763736c":"words_list=[word for line in texts_lr_lc_np_ns for word in line.split()]\nwords_list[:5]","c66ca602":"word_counts=Counter(words_list).most_common(50)\nword_df=pd.DataFrame(word_counts)\nword_df.columns=['word','frq']\ndisplay(word_df.head(5))\n# px=import plotly.express\npx.bar(word_df,x='word',y='frq',title='Most common words')","e7665ea6":"display(df.head(5))\ndf.text=texts_lr_lc_np_ns\ndisplay(df.head(5))","5d7c3816":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\ndisplay(df)","0ab98915":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n","51b6df8f":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\ndisplay(df)","d4df9f98":"sid=SentimentIntensityAnalyzer()\nps=lambda x:sid.polarity_scores(x)\nsentiment_scores=df.text.apply(ps)\nsentiment_scores","a37b0657":"sentiment_df=pd.DataFrame(data=list(sentiment_scores))\ndisplay(sentiment_df)","daefb376":"labelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\nsentiment_df['label']=sentiment_df.compound.apply(labelize)\ndisplay(sentiment_df.head(10))","22c1c05b":"display(df.head(5))\ndata=df.join(sentiment_df.label)\ndisplay(data.head(5))","99524f73":"counts_df=data.label.value_counts().reset_index()\ndisplay(counts_df)","f8dbe4e0":"plt.figure(figsize=(8,5)) \nsns.barplot(x='index',y='label',data=counts_df)","b099982e":"tweets_df=data[['label','text']]\ntweets_df['length']=tweets_df['text'].apply(len)\ntweets_df","ae0dc08d":"tweets_df.describe()","fc00c22e":"# hist plot for the length of tweets\ntweets_df['length'].plot(bins=100,kind='hist')","869e9a69":"positive=tweets_df[tweets_df.label=='positive']\nnegative=tweets_df[tweets_df.label=='negative']","996a593d":"positive.head()","f1ed149a":"sentence_as_one_string = \" \".join(tweets_df.text)","c17db193":"from wordcloud import WordCloud\n\nplt.figure(figsize=(30,30))\nplt.imshow(WordCloud().generate(sentence_as_one_string))","ad4d6283":"# negative wordcloud\nnegative_list=negative.text.tolist()\nnegative_as_one_string = \" \".join(negative_list)\n\nplt.figure(figsize=(30,30))\nplt.imshow(WordCloud().generate(negative_as_one_string))\n","cdbc6573":"from sklearn.feature_extraction.text import CountVectorizer","ccfa3807":"vectorizer= CountVectorizer()\n\nsample_data=tweets_df.text[0:3]\nX=vectorizer.fit_transform(sample_data)","4be2dd0b":"print(vectorizer.get_feature_names())\nprint(X.toarray())","4545785a":"def message_cleaning(message):\n    punc_removed=[char for char in message if char not in string.punctuation]\n    punc_removed_join=''.join(punc_removed)\n    punc_removed_join_clean=[word for word in punc_removed_join.split() if word.lower() not in stopwords.words('english')]\n    return punc_removed_join_clean","e31e7150":"tweets_df_clean=tweets_df.text.apply(message_cleaning)","03a45ec9":"print(tweets_df_clean[15])","f2f4e919":"vectorizer=CountVectorizer(analyzer=message_cleaning)\ntweets_countvectorizer=CountVectorizer(analyzer=message_cleaning,dtype='uint8').fit_transform(tweets_df.text).toarray()","f77e4dcb":"tweets_countvectorizer.shape","6a63319c":"X=tweets_countvectorizer\ny=tweets_df.label","1776fa2c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)","b48fe178":"from sklearn.naive_bayes import MultinomialNB\nNB_classifier=MultinomialNB()\nNB_classifier.fit(X_train,y_train)","4691d58a":"from sklearn.metrics import classification_report,confusion_matrix","8d15221b":"labels = ['negative', 'neutral','positive']\ny_predict_test=NB_classifier.predict(X_test)\ncm=confusion_matrix(y_test,y_predict_test,labels)\nsns.heatmap(cm,annot=True)\nprint(cm)","cfb8e139":"import collections\ncollections.Counter(y_test)","8e5dafbd":"print(classification_report(y_test,y_predict_test))","161c64f8":"data_agg=data[['user_name','date','label']]\ndisplay(data_agg.head(5))","0cd1e816":"data_agg=data_agg.groupby(['date','label'])\ndisplay(data_agg.head(5))","2d9441d6":"data_agg=data_agg.count()\ndisplay(data_agg.head(5))","e5159a17":"data_agg=data_agg.reset_index()\ndisplay(data_agg.head(5))","78d2dfcd":"data_agg.columns=['date','label','counts']\ndisplay(data_agg.head())","9deb819e":"px.line(data_agg,x='date',y='counts',color='label',\n       title='Daily Tweet Sentimental Analysis')","5317ed0e":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\ndisplay(df)","f7d1fcca":"# NB Performance","ddbbe351":"### Removing punctuations","b4663c4b":"### Importing the Modules","49e4d163":"### Converting all tweets to lowercase","b31b99c5":"### Picking out the tweet texts","809f4bc9":"### Importing the Dataset","ef1ef474":"### group number of counts by\n<li>date\n<li>positive,neutral,negative","e6ee5e32":"### Labeling the scores based on the compound polarity value","1aaa1595":"# Pipeline to remove punc,wtop-words and tokenization and count vectorizer","27e0ad0a":"change the type of some columns","900b972a":"let's check the shape of the dataframe","689e0430":"### put the Cleaned text in main dataframe","5981bcc8":"# Twitter sentiment analysis using nltk package and naive bayes model \n[related to Covid-19 (2020-07-24 to 2020-08-30)]\n------------------","3d93becb":"### let's create a big list of words out of all the tweets ","b65e9d64":"### Removing stopwords","3d80b286":"### addtional clean","9caa6ba7":"### actually the 'user_name' is the count of users, so need to change the column name","ba53aa0d":"# Build a Naive Bayes Classifier","4dc835ef":"# Sentiment Analysis ","5869a37c":"# Count Vectorization","20cff557":"let's select the needed columns for our project","163696c1":"### Removing URLs from tweets","9214d3d5":"# Naive Bayes ","ebefe264":"### let's join two dataframes","54f6e1aa":"Getting the polarity scores for each tweet","8e570959":"### Plotting the sentiment score counts"}}