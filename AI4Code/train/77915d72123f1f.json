{"cell_type":{"dcc80743":"code","4c8f1fcb":"code","47331a16":"code","84cbb7f3":"code","0ced0819":"code","3149dacb":"code","e139beb6":"code","c3315237":"code","3dcc86f0":"code","3056363d":"code","b52f132c":"code","6ddd2023":"code","4624d6f4":"code","c1a25723":"code","df957057":"code","5dc16d47":"markdown"},"source":{"dcc80743":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport gc\nimport tqdm\n\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, lr_scheduler\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (AutoModel, AutoTokenizer, \n                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)","4c8f1fcb":"# Set Configs\/Constants\n\nclass config:\n    \n    SEED = 42\n    MAX_LEN = 256\n    TRAIN_BATCH_SIZE = 128\n    VAL_BATCH_SIZE = 64\n    ROBERTA_MODEL_PATH = '..\/input\/roberta-base'\n    EPOCHS = 3\n    LR = 1e-5\n    TEXT_COLUMN = 'excerpt'","47331a16":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nrandom_state = set_seed(config.SEED)","84cbb7f3":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","0ced0819":"def create_kfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df\n\ndef create_Stratkfolds(df,target_col, seed):\n\n    df[\"kfold\"] = -1\n\n    df = df.sample(frac=1).reset_index(drop=True)\n\n    ### This was taken from https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(df))))\n    \n    # bin targets\n    df.loc[:, \"bins\"] = pd.cut(\n        df[target_col], bins=num_bins, labels=False\n    )\n\n    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=df, y = df.bins.values)):\n        print(len(train_idx), len(val_idx))\n        df.loc[val_idx, 'kfold'] = fold\n\n    return df","3149dacb":"# Reading Data\ntrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\ntarget = train['target'].to_numpy()","e139beb6":"class ReadabiltyDataset(nn.Module):\n    def __init__(self, data, tokenizer):\n        self.sentences = data[config.TEXT_COLUMN].to_numpy()\n        self.tokenizer = tokenizer\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n\n        encode = self.tokenizer(self.sentences[idx],\n            return_tensors='pt',\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True\n        )\n\n        return encode","c3315237":"# Thanks to this kernel to help me load the embeddings : https:\/\/www.kaggle.com\/lars123\/neural-tangent-kernel-2\ndef get_embeddings(df,path,plot_losses=True, verbose=True):\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n\n    ds = ReadabiltyDataset(df,tokenizer)\n    dl = DataLoader(ds,\n                  batch_size = config.TRAIN_BATCH_SIZE,\n                  shuffle=False,\n                  num_workers = 4,\n                  pin_memory=True,\n                  drop_last=False)\n        \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm.tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n            outputs = model(**inputs)\n            outputs = outputs[0][:,0].detach().cpu().numpy()\n            embeddings.extend(outputs)\n    return np.array(embeddings)","3dcc86f0":"train_embeddings1 =  get_embeddings(train,'..\/input\/modelf1')\ntest_embeddings1 = get_embeddings(test,'..\/input\/modelf1')\n\ntrain_embeddings2 =  get_embeddings(train,'..\/input\/modelf2')\ntest_embeddings2 = get_embeddings(test,'..\/input\/modelf2')\n\ntrain_embeddings3 =  get_embeddings(train,'..\/input\/modelf3')\ntest_embeddings3 = get_embeddings(test,'..\/input\/modelf3')\n\ntrain_embeddings4 =  get_embeddings(train,'..\/input\/modelf4')\ntest_embeddings4 = get_embeddings(test,'..\/input\/modelf4')\n\ntrain_embeddings5 =  get_embeddings(train,'..\/input\/modelf5')\ntest_embeddings5 = get_embeddings(test,'..\/input\/modelf5')","3056363d":"def runLGB_reg(train_X, train_y, test_X, test_y=None, test_X2=None, dep=8, seed=0, data_leaf=50, rounds=20000):\n    params = {}\n    params[\"objective\"] = \"regression\"\n    params['metric'] = 'rmse'\n    params[\"max_depth\"] = dep\n    params[\"num_leaves\"] = 30\n    params[\"min_data_in_leaf\"] = data_leaf\n    #     params[\"min_sum_hessian_in_leaf\"] = 50\n    params[\"learning_rate\"] = 0.01\n    params[\"bagging_fraction\"] = 0.8\n    params[\"feature_fraction\"] = 0.2\n    params[\"feature_fraction_seed\"] = seed\n    params[\"bagging_freq\"] = 1\n    params[\"bagging_seed\"] = seed\n    params[\"lambda_l2\"] = 3\n    params[\"lambda_l1\"] = 3\n    params[\"verbosity\"] = -1\n#     params[\"sample_weight\"] = sample_weight\n    num_rounds = rounds\n\n    plst = list(params.items())\n    lgtrain = lgb.Dataset(train_X, label=train_y)\n\n    if test_y is not None:\n        lgtest = lgb.Dataset(test_X, label=test_y)\n        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=200, verbose_eval=500)\n\n    #         model = lgb.LGBMRegressor()\n    else:\n        lgtest = lgb.DMatrix(test_X)\n        model = lgb.train(params, lgtrain, num_rounds)\n\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    #imps = model.feature_importance()\n    #names = model.feature_name()\n    #for fi, fn in enumerate(names):\n    #    print(fn, imps[fi])\n\n    loss = 0\n    if test_y is not None:\n        loss = np.sqrt(metrics.mean_squared_error(test_y, pred_test_y))\n        print(loss)\n        return model, loss, pred_test_y, pred_test_y2\n    else:\n        return model, loss, pred_test_y, pred_test_y2","b52f132c":"def run_lgb(X_train, y_train, test_X):\n    print(\"Building model..\")\n    cv_scores = []\n    pred_test_full = 0\n    pred_train = np.zeros(X_train.shape[0])\n    n_splits = 5\n    kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=7988)\n    # gkf = model_selection.GroupKFold(n_splits=n_splits)\n    model_name = \"lgb\"\n    for dev_index, val_index in kf.split(X_train, y_train):\n        dev_X, val_X = X_train[dev_index,:], X_train[val_index,:]\n        dev_y, val_y = y_train[dev_index], y_train[val_index]\n\n        pred_val = 0\n        pred_test = 0\n        n_models = 0.\n\n        model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, val_y, test_X, dep=6, data_leaf=200, seed=2019)\n        pred_val += pred_v\n        pred_test += pred_t\n        n_models += 1\n\n        model, loss, pred_v, pred_t = runLGB_reg(dev_X, dev_y, val_X, val_y, test_X,  dep=7, data_leaf=180, seed=9873)\n        pred_val += pred_v\n        pred_test += pred_t\n        n_models += 1\n\n    #     model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, dep=7, data_leaf=200, seed=4568)\n    #     pred_val += pred_v\n    #     pred_test += pred_t\n    #     n_models += 1\n\n\n        pred_val \/= n_models\n        pred_test \/= n_models\n\n        loss = np.sqrt(metrics.mean_squared_error(val_y, pred_val))\n\n        pred_train[val_index] = pred_val\n        pred_test_full += pred_test \/ n_splits\n        cv_scores.append(loss)\n        print(cv_scores)\n    #     break\n    print(np.mean(cv_scores))\n    return pred_test","6ddd2023":"preds_1 = run_lgb(train_embeddings1,target,test_embeddings1)\npreds_2 = run_lgb(train_embeddings2,target,test_embeddings2)\npreds_3 = run_lgb(train_embeddings3,target,test_embeddings3)\npreds_4 = run_lgb(train_embeddings4,target,test_embeddings4)\npreds_5 = run_lgb(train_embeddings5,target,test_embeddings5)","4624d6f4":"final_preds = (preds_1 + preds_2 + preds_3 + preds_4 + preds_5)\/5","c1a25723":"submission_df = pd.DataFrame({'id': test.id, 'target': final_preds})\nsubmission_df","df957057":"submission_df.to_csv('submission.csv', index = False)","5dc16d47":"#### Thanks to Abhishek Thakur for the AutoNLP trained Language Models. In this kernel, we will be using these models and taking out the last layer embedding which would go as features for the machine learning model."}}