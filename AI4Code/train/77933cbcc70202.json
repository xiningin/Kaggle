{"cell_type":{"9abf199a":"code","45132ebe":"code","4c22be8b":"code","d5e4d85d":"code","dcb58137":"code","17265269":"code","b3a6403a":"code","c7018cf8":"code","86e6338f":"code","da21a29b":"code","24d6854f":"code","0895c8d4":"code","7dac2794":"code","4c842170":"code","05a64e50":"code","429bb053":"code","808c9b37":"code","4a5a66e7":"code","8951792f":"code","a02df9e2":"code","a2b4a30a":"code","0744468a":"code","e632512c":"code","a8b89055":"markdown","89aa96ec":"markdown","fcb205b4":"markdown","9204e5fe":"markdown","30a78fbe":"markdown","76f13a95":"markdown","c0e0b9b3":"markdown","f98f4873":"markdown"},"source":{"9abf199a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","45132ebe":"import random\nfrom xgboost import XGBClassifier\nfrom scipy import stats\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold,KFold\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd \nfrom xgboost import plot_importance,plot_tree,to_graphviz\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","4c22be8b":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest  = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","d5e4d85d":"train.head()","dcb58137":"test.head()","17265269":"train.info()","b3a6403a":"test.info()","c7018cf8":"#Check if there'is null values\ntrain.isnull().sum()","86e6338f":"#Check if there'is null values\ntest.isnull().sum()","da21a29b":"train.describe()","24d6854f":"cols=['f'+str(i) for i in range(1,119)]","0895c8d4":"# Numerical features distribution: part1\ni = 1\nplt.figure()\nfig, ax = plt.subplots(15, 4,figsize=(20, 22))\nfor feature in cols[:60]:\n    plt.subplot(15, 4,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","7dac2794":"# Numerical features distribution: part2\ni = 1\nplt.figure()\nfig, ax = plt.subplots(15, 4,figsize=(20, 22))\nfor feature in cols[60:]:\n    plt.subplot(15, 4,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","4c842170":"#Features correlation\ncorr = train[cols+['claim']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","05a64e50":"train.claim.value_counts()","429bb053":"# Target distibution\nsns.catplot(x=\"claim\", kind=\"count\", palette=\"ch:.25\", data=train)","808c9b37":"params = {'objective':'binary:logistic',\n'tree_method':'gpu_hist',\n'random_state': 48,\n'n_estimators': 30000,\n'lambda': 0.18427562536318878,\n'alpha': 0.027053317312588015,\n'colsample_bytree': 0.1,\n'subsample': 0.8,\n'learning_rate': 0.01,\n'max_depth': 7,\n'min_child_weight': 14,\n'use_label_encoder':False}","4a5a66e7":"preds = np.zeros(test.shape[0])\nkf = StratifiedKFold(n_splits=10,random_state=48,shuffle=True)\nauc=[]  # list contains auc for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[cols],train['claim']):\n    X_tr,X_val=train[cols].iloc[trn_idx],train[cols].iloc[test_idx]\n    y_tr,y_val=train['claim'].iloc[trn_idx],train['claim'].iloc[test_idx]\n    model = XGBClassifier(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],eval_metric = 'auc',early_stopping_rounds=100,verbose=False)\n    preds += model.predict_proba(test[cols])[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    print(f\"fold: {n+1}, auc: {auc[n]}\")\n    n+=1 ","8951792f":"np.mean(auc)  ","a02df9e2":"# let's plot most 40 important features\nfig, ax = plt.subplots(figsize=(10,10))\nplot_importance(model, max_num_features=40, height=0.5, ax=ax,importance_type='weight')\nplt.show()","a2b4a30a":"fig, ax = plt.subplots(figsize=(44, 44))\nplot_tree(model, num_trees=99,rankdir='LR', ax=ax)\nplt.show()","0744468a":"sub['claim']=preds\nsub.to_csv('submission.csv', index=False)","e632512c":"sub","a8b89055":"#### The [XGBoost Python API](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.plotting) provides a function for plotting decision trees within a trained XGBoost model.\nLet's plot the 100th boosted tree","89aa96ec":"## Let's import some libraries\n","fcb205b4":"### Target distribution","9204e5fe":"# Let's do some Exploratory Data Analysis (EDA)","30a78fbe":"# I hope that you find this kernel usefull\ud83c\udfc4","76f13a95":"# let's try XGBoost","c0e0b9b3":"# Making a Submission","f98f4873":"# Hi kagglers \ud83d\ude4b\ud83c\udffb\u200d\u2642\ufe0f and Welcome to this competition!"}}