{"cell_type":{"638dd580":"code","222102d1":"code","1959a413":"code","4ee0dd53":"code","7bacbb63":"code","9c9353f3":"code","e580d690":"code","b45f2e34":"code","32467b7b":"code","fe8ca6dc":"code","648c8ab0":"code","8dd943e3":"code","2cc69e81":"code","92061a46":"code","07f04800":"code","d2b5df57":"code","a21dee12":"code","fe02d61e":"code","b5fbe1e8":"code","a9d3fafd":"code","8861cde9":"code","b7e979c8":"code","2cb586f6":"code","a6f04a72":"code","06a2acf9":"code","7976cbfc":"code","e6a32a89":"code","c5367c15":"markdown","258d4268":"markdown","05ec5ab9":"markdown","ca96949e":"markdown","02de865f":"markdown","77038d20":"markdown","f0aae00e":"markdown","6fd357d1":"markdown","40b0f0bf":"markdown","f09400d0":"markdown","56ef2597":"markdown","9fd7a7f8":"markdown","714ac358":"markdown","5f3f66f8":"markdown","bae36a6a":"markdown","6486adcb":"markdown","d208c912":"markdown","989c2ef4":"markdown","c1f337ed":"markdown","5578f1b5":"markdown","6104edda":"markdown","fd7e7cdb":"markdown","5e748166":"markdown","1e9fc191":"markdown","6327e5c8":"markdown","74451060":"markdown","038caf39":"markdown","3082bd8b":"markdown","02008df1":"markdown","4f6555d4":"markdown"},"source":{"638dd580":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')","222102d1":"!pip install -q wandb --upgrade\n!pip install -q adamp","1959a413":"import warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Python\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport glob\npd.set_option('display.max_columns', None)\n\n# Visualizations\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Image Augmentations\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\n# Utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\n# Pytorch for Deep Learning\nimport torch\nimport torchvision\nimport timm\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom adamp import AdamP\n\n# Weights and Biases Tool\nimport wandb","4ee0dd53":"params = {\n    'seed': 42,\n    'model': 'eca_nfnet_l0',\n    'size' : 224,\n    'inp_channels': 1,\n    'device': 'cuda',\n    'lr': 1e-4,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 4,\n    'epochs': 3,\n    'out_features': 1,\n    'name': 'CosineAnnealingLR',\n    'T_max': 10,\n    'min_lr': 1e-6,\n    'nfolds': 5,\n}","7bacbb63":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(params['seed'])","9c9353f3":"train_dir = ('..\/input\/seti-breakthrough-listen\/train')\ntest_dir = ('..\/input\/seti-breakthrough-listen\/test')\ntrain_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ntest_df = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')","e580d690":"def return_filpath(name, folder=train_dir):\n    path = os.path.join(folder, name[0], f'{name}.npy')\n    return path","b45f2e34":"train_df['image_path'] = train_df['id'].apply(lambda x: return_filpath(x))\ntest_df['image_path'] = test_df['id'].apply(lambda x: return_filpath(x, folder=test_dir))\ntrain_df.head()","32467b7b":"dist = train_df.target.map({0:'Target 0', 1:'Target 1'})\ndist = dist.value_counts()\nfig = px.pie(dist,\n             values='target',\n             names=dist.index,\n             hole=.4,title=\"Imbalanced Dataset\")\nfig.update_traces(textinfo='percent+label', pull=0.05)\nfig.show()","fe8ca6dc":"def get_train_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Rotate(limit=180, p=0.7),\n            albumentations.RandomBrightness(limit=0.6, p=0.5),\n            albumentations.Cutout(\n                num_holes=10, max_h_size=12, max_w_size=12,\n                fill_value=0, always_apply=False, p=0.5\n            ),\n            albumentations.ShiftScaleRotate(\n                shift_limit=0.25, scale_limit=0.1, rotate_limit=0\n            ),\n            ToTensorV2(p=1.0),\n        ]\n    )\n\ndef get_valid_transforms():\n    return albumentations.Compose(\n        [\n            albumentations.Resize(params['size'],params['size']),\n            ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_test_transforms():\n        return albumentations.Compose(\n            [\n                albumentations.Resize(params['size'],params['size']),\n                ToTensorV2(p=1.0)\n            ]\n        )","648c8ab0":"class SETIDataset(Dataset):\n    def __init__(self, images_filepaths, targets, transform=None):\n        self.images_filepaths = images_filepaths\n        self.targets = targets\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images_filepaths)\n\n    def __getitem__(self, idx):\n        image_filepath = self.images_filepaths[idx]\n        image = np.load(image_filepath).astype(np.float32)\n        image = np.vstack(image).transpose((1, 0))\n            \n        if self.transform is not None:\n            image = self.transform(image=image)[\"image\"]\n        else:\n            image = image[np.newaxis,:,:]\n            image = torch.from_numpy(image).float()\n        \n        label = torch.tensor(self.targets[idx]).float()\n        return image, label","8dd943e3":"def mixup(x, y, alpha=1.0, use_cuda=True):\n\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)","2cc69e81":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n    \ndef use_roc_score(output, target):\n    try:\n        y_pred = torch.sigmoid(output).cpu()\n        y_pred = y_pred.detach().numpy()\n        target = target.cpu()\n\n        return roc_auc_score(target, y_pred)\n    except:\n        return 0.5","92061a46":"def get_sampler(train_data):\n    class_counts = train_data['target'].value_counts().to_list()\n    num_samples = sum(class_counts)\n    labels = train_data['target'].to_list()\n\n    class_weights = [num_samples\/class_counts[i] for i in range(len(class_counts))]\n    weights = [class_weights[labels[i]] for i in range(int(num_samples))]\n    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))","07f04800":"def get_loader(train_data, valid_data, sampler):\n    training_set = SETIDataset(\n        images_filepaths=train_data['image_path'].values,\n        targets=train_data['target'].values,\n        transform=get_train_transforms()\n            )\n\n    validation_set = SETIDataset(\n        images_filepaths=valid_data['image_path'].values,\n        targets=valid_data['target'].values,\n        transform=get_valid_transforms()\n            )\n\n    train_loader = DataLoader(\n        training_set,\n        batch_size=params['batch_size'],\n        shuffle=True,\n        num_workers=params['num_workers'],\n        sampler = sampler,\n        pin_memory=True\n            )\n\n    valid_loader = DataLoader(\n        validation_set,\n        batch_size=params['batch_size'],\n        shuffle=False,\n        num_workers=params['num_workers'],\n        pin_memory=True\n            )\n    \n    return train_loader, valid_loader","d2b5df57":"class EcaNFNet(nn.Module):\n    def __init__(self, model_name=params['model'], out_features=params['out_features'],\n                 inp_channels=params['inp_channels'], pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained,\n                                       in_chans=inp_channels)\n        n_features = self.model.head.fc.in_features\n        self.model.head.fc = nn.Linear(n_features, out_features, bias=True)    \n        \n    \n    def forward(self, x):\n        x = self.model(x)\n        return x","a21dee12":"class SAM(torch.optim.Optimizer):\n    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n\n        defaults = dict(rho=rho, **kwargs)\n        super(SAM, self).__init__(params, defaults)\n\n        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n        self.param_groups = self.base_optimizer.param_groups\n\n    @torch.no_grad()\n    def first_step(self, zero_grad=False):\n        grad_norm = self._grad_norm()\n        for group in self.param_groups:\n            scale = group[\"rho\"] \/ (grad_norm + 1e-12)\n\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                e_w = p.grad * scale.to(p)\n                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n                self.state[p][\"e_w\"] = e_w\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def second_step(self, zero_grad=False):\n        for group in self.param_groups:\n            for p in group[\"params\"]:\n                if p.grad is None: continue\n                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n\n        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n\n        if zero_grad: self.zero_grad()\n\n    @torch.no_grad()\n    def step(self, closure=None):\n        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n\n        self.first_step(zero_grad=True)\n        closure()\n        self.second_step()\n\n    def _grad_norm(self):\n        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n        norm = torch.norm(\n                    torch.stack([\n                        p.grad.norm(p=2).to(shared_device)\n                        for group in self.param_groups for p in group[\"params\"]\n                        if p.grad is not None\n                    ]),\n                    p=2\n               )\n        return norm","fe02d61e":"model = EcaNFNet()\nmodel = model.to(params['device'])\ncriterion = nn.BCEWithLogitsLoss().to(params['device'])\nbase_optimizer = AdamP\noptimizer = SAM(model.parameters(), base_optimizer, lr=params['lr'], weight_decay=params['weight_decay'])\n\nscheduler = CosineAnnealingLR(optimizer,\n                              T_max=params['T_max'],\n                              eta_min=params['min_lr'],\n                              last_epoch=-1)\n\n","b5fbe1e8":"def train(train_loader, model, criterion, optimizer, epoch, params, scheduler):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(enumerate(train_loader), total=len(train_loader))\n       \n    for i, (images, target) in stream:\n\n        images = images.to(params['device'])\n        target = target.to(params['device']).float().view(-1, 1)\n        images, targets_a, targets_b, lam = mixup(images, target.view(-1, 1))\n    \n        output = model(images)\n        loss = mixup_criterion(criterion, output, targets_a, targets_b, lam)\n        \n            \n        loss.backward(retain_graph = True)\n        optimizer.first_step(zero_grad=True)\n        \n        mixup_criterion(criterion, model(images), targets_a, targets_b, lam).backward()\n        optimizer.second_step(zero_grad=True)\n        \n        \n        \n        roc_score = use_roc_score(output, target)\n        metric_monitor.update('Loss', loss.item())\n        metric_monitor.update('ROC', roc_score)\n        wandb.log({\"Train Epoch\":epoch,\"Train loss\": loss.item(), \"Train ROC\":roc_score})\n        \n\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(\n                epoch=epoch,\n                metric_monitor=metric_monitor)\n        )\n    \n    scheduler.step()","a9d3fafd":"def validate(val_loader, model, criterion, epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(enumerate(val_loader), total=len(val_loader))\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for i, (images, target) in stream:\n            images = images.to(params['device'], non_blocking=True)\n            target = target.to(params['device'], non_blocking=True).float().view(-1, 1)\n            output = model(images)\n            loss = criterion(output, target)\n            roc_score = use_roc_score(output, target)\n            metric_monitor.update('Loss', loss.item())\n            metric_monitor.update('ROC', roc_score)\n            wandb.log({\"Valid Epoch\": epoch, \"Valid loss\": loss.item(), \"Valid ROC\":roc_score})\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(\n                    epoch=epoch,\n                    metric_monitor=metric_monitor)\n            )\n            \n            targets = target.detach().cpu().numpy().tolist()\n            outputs = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n    return final_outputs, final_targets","8861cde9":"best_roc = -np.inf\nbest_epoch = -np.inf\nbest_model_name = None\n\nkfold = StratifiedKFold(n_splits=params['nfolds'], shuffle=True, random_state=params['seed'])\n\nfor fold, (trn_idx, val_idx) in enumerate(kfold.split(train_df, train_df['target'])):\n    \n    run = wandb.init(project='Seti-ECA-NFNet-Mixup', \n             config=params, \n             group = 'ECA-NFNet-New-Data',\n             job_type='train',\n             name = f'Fold{fold}')\n    \n    print(f\"{'='*40} Fold: {fold} {'='*40}\")\n\n    train_data = train_df.loc[trn_idx]\n    valid_data = train_df.loc[val_idx]\n    \n    sampler = get_sampler(train_data)\n    \n    train_loader, valid_loader = get_loader(train_data, valid_data, sampler)\n\n    for epoch in range(1, params['epochs'] + 1):\n\n        train(train_loader, model, criterion, optimizer, epoch, params, scheduler)\n        predictions, valid_targets = validate(valid_loader, model, criterion, epoch, params)\n        roc_auc = round(roc_auc_score(valid_targets, predictions), 3)\n        torch.save(model.state_dict(),f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\")\n\n        if roc_auc > best_roc:\n            best_roc = roc_auc\n            best_epoch = epoch\n            best_model_name = f\"{params['model']}_{epoch}_epoch_{roc_auc}_roc_auc.pth\"\n            \n        \n            \n    print(f\"Best ROC-AUC in fold: {fold} was: {best_roc:.4f}\")\n    print(f\"Final ROC-AUC in fold: {fold} was: {roc_auc:.4f}\")","b7e979c8":"model = EcaNFNet()\nmodel.load_state_dict(torch.load(best_model_name))\nmodel = model.to(params['device'])","2cb586f6":"model.eval()\npredicted_labels = None\n\ntest_dataset = SETIDataset(\n    images_filepaths = test_df['image_path'].values,\n    targets = test_df['target'].values,\n    transform = get_test_transforms()\n)\ntest_loader = DataLoader(\n    test_dataset, batch_size=params['batch_size'],\n    shuffle=False, num_workers=params['num_workers'],\n    pin_memory=True\n)\n\ntemp_preds = None\nwith torch.no_grad():\n    for (images, target) in tqdm(test_loader):\n        images = images.to(params['device'], non_blocking=True)\n        output = model(images)\n        predictions = torch.sigmoid(output).cpu().numpy()\n        if temp_preds is None:\n            temp_preds = predictions\n        else:\n            temp_preds = np.vstack((temp_preds, predictions))\n\nif predicted_labels is None:\n    predicted_labels = temp_preds\nelse:\n    predicted_labels += temp_preds","a6f04a72":"torch.save(model.state_dict(), f\"{params['model']}_{best_epoch}epochs_weights.pth\")","06a2acf9":"sub_df = pd.DataFrame()\nsub_df['id'] = test_df['id']\nsub_df['target'] = predicted_labels","7976cbfc":"sub_df.head()","e6a32a89":"sub_df.to_csv('submission.csv', index=False)","c5367c15":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>","258d4268":"<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders \/ augmentations, and reference training \/ validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.<br><br>\nUsing timm we will create the ECA NFNet for our problem statement. We will be using the ECA NFNET l0 which is a slimmed down from the original F0 variant. <\/p>","05ec5ab9":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Sharpness Aware Minimization (SAM) Optimizer<\/p>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">In today's heavily overparameterized models, the value of the training loss provides few guarantees on model generalization ability. Indeed, optimizing only the training loss value, as is commonly done, can easily lead to suboptimal model quality. <br><br>Sharpness-Aware Minimization (SAM), seeks parameters that lie in neighborhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. The empirical results show that SAM improves model generalization across a variety of benchmark datasets (e.g., CIFAR-10, CIFAR-100, ImageNet, finetuning tasks) and models, yielding novel state-of-the-art performance for several. Additionally, SAM natively provides robustness to label noise on par with that provided by state-of-the-art procedures that specifically target learning with noisy labels.<\/p>\n\n![](https:\/\/raw.githubusercontent.com\/davda54\/sam\/main\/img\/loss_landscape.png)","ca96949e":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GPU Utilization<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/EmpBUbP.png\" width=\"1500\" alt=\"GPU\" \/><\/center>","02de865f":"<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">The dataset is very imbalanced and we will see later how we use a sampler to handle it. <\/p>","77038d20":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","f0aae00e":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spacing: 1px; background-color: #f6f5f5; color :#6666ff; border-radius: 200px 200px; text-align:center\">Efficient Channel Attention for  Normalizer Free Networks<\/h1>\n\n\n![](https:\/\/blog.paperspace.com\/content\/images\/size\/w1600\/2020\/09\/eca_module.jpg)\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">We have seen in recent years that channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most of the existing methods dedicated to developing more sophisticated attention modules for achieving better\nperformance are inevitably increasing model complexity. <br><br>To overcome the performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting\nthe channel attention module in SENet (Squeeze and Excitation), the paper empirically shows avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, a local crosschannel interaction strategy without dimensionality reduction is proposed, which can be efficiently implemented via 1D convolution. Furthermore, a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction has been developed <br><br>ECA-Net's architecture is extremely similar to that of SE-Net as shown in the above figure. ECA-Net takes an input tensor which is the output of a convolutional layer and is 4-dimensional of the shape (B,C,H,W) where B represents the batch size, C represents the number of channels or total number of feature maps in that tensor and finally, H and W, represent the spatial dimensions of each feature map, namely, the height and width. The output of ECA block is also a 4-D tensor of the same shape. ECA-block is also made up of 3 modules which include:<br><br>\n1. Global Feature Descriptor<br>\n2. Adaptive Neighborhood Interaction<br>\n3. Broadcasted Scaling<\/p>","6fd357d1":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Submission File<\/p>","40b0f0bf":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Test Loop<\/p>\n","f09400d0":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Dataset<\/p>","56ef2597":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Cross Validation Results<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">We are able to achieve a Validation ROC score of .7670!<br><br> Weights & Biases provides us with such easy to use interface and tools to keep a track of our Evaluation metrics like training and validation loss and Roc along with other resources like Gpu usage<br><br> Let's take a look at some of our K-Fold CV training and GPU Utilization graphs.<\/p>","9fd7a7f8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Minimal EDA <br> Code Credit <a href = 'https:\/\/www.kaggle.com\/datafan07\/pytorch-lightning-single-fold-training-lb-0-97'>Ertu\u011frul Demir <\/a><\/p>\n","714ac358":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Image Augmentation<\/p>","5f3f66f8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Big Shoutout to <a href = 'https:\/\/www.kaggle.com\/nakshatrasingh'>nakshatrasingh<\/a> for pointing out the missing Mixup implementation<\/p>","bae36a6a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">W&B Initialization for K-FOLD CV<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">K-Fold CV gives a model with less bias compared to other methods. In K-Fold CV, we have a paprameter \u2018k\u2019. This parameter decides how many folds the dataset is going to be divided. Every fold gets chance to appears in the training set (k-1) times, which in turn ensures that every observation in the dataset appears in the dataset, thus enabling the model to learn the underlying data distribution better.<br><br>Another approach is to shuffle the dataset just once prior to splitting the dataset into k folds, and then split, such that the ratio of the observations in each class remains the same in each fold. Also the test set does not overlap between consecutive iterations. This approach is called Stratified K-Fold CV. This approach is useful for imbalanced datasets.<\/p>\n","6486adcb":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:25px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\">Normalizer Free Networks with ECA <br>\n Sam Optimizer with AdamP <br>\n Mixup Augmentation <br>\n Weighted Random Sampler <br>\n K-Fold Cross Validation <br>\n Weights and Biases for Experiment Tracking","d208c912":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Class for Monitoring Loss and ROC<\/p>","989c2ef4":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train and Validation Loops<\/p>","c1f337ed":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\"><a href = 'https:\/\/wandb.ai\/tanishqgautam\/Seti-ECA-NFNet?workspace=user-tanishqgautam'>Check out the Weights and Biases Dashboard here $\\rightarrow$ <\/a><\/p>","5578f1b5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Train and Test<\/p>","6104edda":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Loss Function, Optimizer and Scheduler<\/p>","fd7e7cdb":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">TIMM Pytorch Models<\/p>","5e748166":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Weighted Random Sampler<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Samples elements from [0 ,.., len(weights)-1] with given probabilities (weights).<\/p>","1e9fc191":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">KFold Metrics Visualization<\/p><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/xBqIjjp.png\" width=\"1500\" alt=\"metrics\" \/><\/center>","6327e5c8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Configurations\/Parameters<\/p>","74451060":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">MixUp Augmentation<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Large deep neural networks are powerful, but exhibit undesirable behaviors such\nas memorization and sensitivity to adversarial examples. MixUp is a data augmentation technique to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior\nin-between training examples. Experiments show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.<\/p> <br><br>\n\n![](https:\/\/miro.medium.com\/max\/1838\/0*CdJ256L9RTDGGLrS.png)","038caf39":"# <center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","3082bd8b":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">ECA NFNet<\/p>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples.<br><br> Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations.<br><br> Normalizer-Free ResNets use an adaptive gradient clipping technique which overcomes these instabilities. The smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and the largest models attain a new state-of-the-art top-1 accuracy of 86.5%. <br><br>In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2%. <\/p>\n\n![](https:\/\/miro.medium.com\/max\/910\/1*CjpipU_oChc899f_Esjpyg.png)\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">ECA NFNet model variant is slimmed down from the original F0 variant in the paper for improved runtime characteristics (throughput, memory use) in PyTorch, on a GPU accelerator. It utilizes Efficient Channel Attention (ECA) instead of Squeeze-Excitation. It also features SiLU activations instead of the usual GELU.<br><br>\nLike other models in the NF family, this model contains no normalization layers (batch, group, etc). The models make use of Weight Standardized convolutions with additional scaling values in lieu of normalization layers.<\/p>","02008df1":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train and Valid Loader<\/p>\n","4f6555d4":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>"}}