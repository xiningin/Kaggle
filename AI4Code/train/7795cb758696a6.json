{"cell_type":{"b6ac9fc4":"code","5bafb17f":"code","c94d63d4":"code","d1e65b87":"code","5db4d7cb":"code","e754d9ec":"code","d6249715":"code","93e5a82a":"code","93c89fca":"code","77682c4e":"code","6210e702":"code","6ff986ff":"code","3a7c21f9":"code","fd50eca5":"code","efa3ea50":"code","90a3342e":"code","f6dba95b":"code","91375f4d":"code","1322c0cd":"code","e06bc66d":"code","0dc743f4":"code","49e2a96a":"code","a758b506":"code","34ac1f0d":"code","a775ebd9":"code","51c04ffa":"code","6dfb8dd1":"code","61f6c3b3":"code","fb835bbf":"code","103195d6":"code","4b6aebbe":"code","fd306a38":"code","229dfffa":"code","ef576660":"code","85a95420":"code","7a4fee09":"code","eecc9c17":"code","8bd32e62":"code","95013598":"code","c86dd9f3":"code","e1182f5f":"code","e902856d":"code","af15d9a4":"code","9976da40":"markdown","bd8d03ad":"markdown","8dcddc53":"markdown","439c03df":"markdown","74e7b0e6":"markdown","2fed1fdb":"markdown","f087f998":"markdown","31e925e3":"markdown","ea5e1f0c":"markdown","922ad87b":"markdown","b1c2c0ca":"markdown","8a8d192b":"markdown","0301d18b":"markdown","e1863b7c":"markdown","c1356bd6":"markdown","4a2b42a0":"markdown","d0d2e2ff":"markdown","e9f28ea5":"markdown","f443c09d":"markdown","72918e64":"markdown","caa371ad":"markdown","28bf22e4":"markdown","895c082a":"markdown","8738fae3":"markdown","e657958e":"markdown","ead6eb2d":"markdown","451a0a18":"markdown","1b00864a":"markdown","2f21d90c":"markdown","4447acb2":"markdown","33d9134b":"markdown","9308be0d":"markdown","4a64e478":"markdown","a0951258":"markdown"},"source":{"b6ac9fc4":"#importing necessary libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\n\ndata=pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nprint(\"This dataset has {0} rows and {1} columns\".format(data.shape[0],data.shape[1]))\ndata.head(4)","5bafb17f":"data.dtypes","c94d63d4":"data['TotalCharges']=pd.to_numeric(data['TotalCharges'], errors='coerce')\ncols=['MonthlyCharges', 'TotalCharges','tenure']\ndata[cols].describe()","d1e65b87":"fig=plt.figure(figsize=(6,5))\np= sns.countplot(x='Churn', data=data)\nax=plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2.,height+2, '{:.2f}%'.format(100*(height\/data.shape[0])),fontsize=14,ha='center',va='bottom')\nsns.set(font_scale=1.5)\nax.set_xlabel(\"Labels for Customer Churn\")\nax.set_ylabel(\"Numbers of records\")\nplt.title(\"Data distriblution\")\n;","5db4d7cb":"data.isnull().sum()","e754d9ec":"meanTotalCharge = data.TotalCharges.mean()\ndata['TotalCharges']=data['TotalCharges'].fillna(meanTotalCharge)","d6249715":"data.groupby('Churn').mean()","93e5a82a":"f,axes = plt.subplots(ncols=3, figsize=(17,6))\nsns.distplot(data.tenure,kde=True,ax=axes[0], color='darkorange').set_title(\"Customer tenure\")\naxes[0].set_ylabel('No of Customers')\n\nsns.distplot(data.MonthlyCharges,kde=True,ax=axes[1],color='maroon').set_title('Monthly Charges')\naxes[1].set_ylabel('No of Customers')\n\nsns.distplot(data.TotalCharges,kde=True,ax=axes[2]).set_title('Total Charges')\naxes[2].set_ylabel('No of Customers')\n;","93c89fca":"plt.figure(figsize=(8,4))\np=sns.countplot(x=\"gender\", hue=\"Churn\", data=data)\nax=plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height+2, '{:.2f}%'.format(100*(height\/data.shape[0])),fontsize=12,ha='center',va='bottom')\nsns.set(font_scale=1.5)\nplt.title('Churn Distribution by gender', fontweight=\"bold\")\n;","77682c4e":"plt.figure(figsize=(10,5))\np=sns.countplot(x='Contract',hue='Churn',data=data)\nax=plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height+2,'{:.2f}%'.format(100*(height\/data.shape[0])),fontsize=14,ha='center',va='bottom')\nsns.set(font_scale=1.5)\nplt.title('Churn Distribution by Contract', fontweight='bold')\n;","6210e702":"plt.figure(figsize=(15,4))\np=sns.countplot(x='PaymentMethod',hue='Churn', data=data)\n\nplt.title ('Churn by payment method', fontweight='bold')\n;","6ff986ff":"plt.figure(figsize=(15,4))\nax = sns.kdeplot(data.loc[(data['Churn']=='No'),'MonthlyCharges'],shade=True,label='No Churn')\nax = sns.kdeplot(data.loc[(data['Churn']=='Yes'),'MonthlyCharges'],shade=True,label='Churn')\nax.set(xlabel='Customer Montly Charges',ylabel='Frequency')\nplt.title('Customer Monthly Charges - Churn vs No Churn', fontweight='bold');\n\n\n# ","3a7c21f9":"plt.figure(figsize=(15,4))\nax=sns.kdeplot(data.loc[(data['Churn']=='No'),'TotalCharges'], shade=True, label='No Churn')\nax=sns.kdeplot(data.loc[(data['Churn']=='Yes'),'TotalCharges'], shade=True, label='Churn')\nax.set(xlabel='Customer Total Charges',ylabel='Frequency')\nplt.title('Customer Total Charges - Churn vs No Churn', fontweight='bold');","fd50eca5":"plt.figure(figsize=(8,4))\np=sns.countplot(x='SeniorCitizen',hue='Churn', data=data)\n\nplt.title ('Churn by SeniorCitizen', fontweight='bold')\n;","efa3ea50":"data.head()","90a3342e":"def percent_outlier(data):\n    Q1 = np.percentile(data,25)\n    Q3 = np.percentile(data,75)\n    IQR = Q3-Q1\n    lower_bound = Q1-(IQR*1.5)\n    upper_bound = Q3+(IQR*1.5)\n    return (lower_bound,upper_bound)","f6dba95b":"ax = sns.boxplot(y='tenure',x='Churn',data=data)\nax.set_title('Tenure box plot by Churn')\n;","91375f4d":"lowerbound,upperbound=percent_outlier(data.tenure)\ntenureout=[x for x in data.tenure if (x<lowerbound) or (x>upperbound)]\ntenureout\nprint (\"All tenure value less than {0} and more than {1} are considered outliers\".format(lowerbound,upperbound))\nprint(\"The min tenure is \",min(data.tenure))\nprint(\"The max tenure is \",max(data.tenure))","1322c0cd":"lowerbound,upperbound=percent_outlier(data.MonthlyCharges)\ntenureout=[x for x in data.tenure if (x<lowerbound) or (x>upperbound)]\ntenureout\nprint (\"All monthly charges less than {0} and more than {1} are considered outliers\".format(lowerbound,upperbound))\nprint(\"The min monthly charges is \",min(data.MonthlyCharges))\nprint(\"The max monthly charges is \",max(data.MonthlyCharges))","e06bc66d":"lowerbound,upperbound=percent_outlier(data.TotalCharges)\ntenureout=[x for x in data.tenure if (x<lowerbound) or (x>upperbound)]\ntenureout\nprint (\"All Total Charges  less than {0} and more than {1} are considered outliers\".format(lowerbound,upperbound))\nprint(\"The min Total Charges is \",min(data.TotalCharges))\nprint(\"The max Total Chargess is \",max(data.TotalCharges))","0dc743f4":"# Converting string boolean to numeric boolean\ndata['PhoneService']=data['PhoneService'].map({'Yes':1,'No':0})\ndata['PaperlessBilling'] =data['PaperlessBilling'].map({'Yes':1,'No':0})\ndata['Churn'] =data['Churn'].map({'Yes':1,'No':0})\ndata['Partner'] = data['Partner'].map({'Yes':1,'No':0})\ndata['Dependents']=data['Dependents'].map({'Yes':1,'No':0})","49e2a96a":"# One hot encoding for categorical features\ncontract = pd.get_dummies(data['Contract'], prefix='Contract',drop_first=True)\n# combining to the original dataframe\ndata = pd.concat([data,contract],axis=1)\n\npayement = pd.get_dummies(data['PaymentMethod'],prefix='PaymentMethod',drop_first=True)\ndata=pd.concat([data,payement],axis=1)\n\ngender=pd.get_dummies(data['gender'],prefix='gender',drop_first=True)\ndata=pd.concat([data,gender],axis=1)\n\nTelLines  = pd.get_dummies(data['MultipleLines'],prefix='MultiLines',drop_first=True)\ndata = pd.concat([data,TelLines], axis=1)\n\n\n# dummy for'InternetService'\ninternet = pd.get_dummies(data['InternetService'],prefix='InternetService',drop_first=True)\ndata = pd.concat([data,internet],axis=1)\n\n\n# dummy for 'OnlineSecurity'.\nsecurity = pd.get_dummies(data['OnlineSecurity'],prefix='OnlineSecurity')\nsecurity1= security.drop(['OnlineSecurity_No internet service'],axis=1)\ndata = pd.concat([data,security1],axis=1)\n\n# dummy for 'OnlineBackup'.\nbackup =pd.get_dummies(data['OnlineBackup'],prefix='OnlineBackup')\nbackup1 =backup.drop(['OnlineBackup_No internet service'],axis=1)\ndata = pd.concat([data,backup1],axis=1)\n\n# dummy for 'DeviceProtection'. \ndevice =pd.get_dummies(data['DeviceProtection'],prefix='DeviceProtection')\ndevice1 = device.drop(['DeviceProtection_No internet service'],axis=1)\ndata = pd.concat([data,device1],axis=1)\n\n# dummy for 'TechSupport'. \nsupport =pd.get_dummies(data['TechSupport'],prefix='TechSupport')\nsupport1 = support.drop(['TechSupport_No internet service'],axis=1)\ndata = pd.concat([data,support1],axis=1)\n\n# dummy for 'StreamingTV'.\nTV =pd.get_dummies(data['StreamingTV'],prefix='StreamingTV')\nTV1 = TV.drop(['StreamingTV_No internet service'],axis=1)\ndata = pd.concat([data,TV1],axis=1)\n\n# dummy for 'StreamingMovies'. \nmovies =pd.get_dummies(data['StreamingMovies'],prefix='StreamingMovies')\nmovies1 = movies.drop(['StreamingMovies_No internet service'],axis=1)\ndata = pd.concat([data,movies1],axis=1)\n\n\n# Dropping the original variables\ndata = data.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], axis=1)\n\nprint(\"After creating dumming variables, the new dataset has {0} rows and {1} columns\".format(data.shape[0],data.shape[1]))","a758b506":"cols=['tenure','MonthlyCharges','TotalCharges']\nnum_data = data.loc[:,cols]\nnorm_data = (num_data-num_data.mean())\/num_data.std()\ndata.drop(cols,axis=1, inplace=True)\ndata=pd.concat([data,norm_data],axis=1)\ndata.head()","34ac1f0d":"X = data.drop(['Churn','customerID'], axis=1)\ny = data.Churn\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=100, stratify=y)","a775ebd9":"# K-Nearest Neighbor (KNN)\nknn = KNeighborsClassifier(n_neighbors=5,weights='uniform',algorithm='auto',leaf_size=30,p=2,metric='minkowski',metric_params=None)\n\n# Logistic regression\nlogreg = LogisticRegression(penalty='l1',dual=False,tol=0.0001,C=1.0,fit_intercept=True,intercept_scaling=1,class_weight=None,\n                           random_state=None,solver='liblinear',max_iter=100,multi_class='ovr',verbose=1)\n\n#adaBoost\nada = AdaBoostClassifier(base_estimator=None,n_estimators=200,learning_rate=1.0)\n#Gradient Boosting\ngboosting =GradientBoostingClassifier(loss='deviance',learning_rate=0.1,n_estimators=200,subsample=1.0,min_samples_split=2,\n                                     min_samples_leaf=1,min_weight_fraction_leaf=0.0,max_depth=3,init=None,random_state=None,\n                                      max_features=None,verbose=0    )\n#RandomForest Classifier\nrf = RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,min_samples_split=2,min_samples_leaf=1,\n                           min_weight_fraction_leaf=0.0,max_features='auto',max_leaf_nodes=None,bootstrap=True,oob_score=False,\n                           n_jobs=1,random_state=None,verbose=0)\n","51c04ffa":"#Creating Dictionnary of models to train\n\nmodels ={}\nmodels['K-NN']=knn\nmodels['Logistic Regression']=logreg\nmodels['adaBoost']=ada\nmodels['gradient Boosting']=gboosting\nmodels['random Forest']=rf","6dfb8dd1":"#Creating a function to train all our algorithms\n\ndef trainfunc(X_train,y_train,models):\n    for label,model in models.items():\n        print(\"Training our {0} model\".format(label))\n        model.fit(X_train,y_train)\n;","61f6c3b3":"#Training all base models\ntrainfunc(X_train=X_train,y_train=y_train,models=models)","fb835bbf":"result=pd.DataFrame(columns=['Basecore','BaseAUC-ROC'],index=models.keys())\n\ndef testingfunc(X_test,y_test,models,lscore='Basecore',auc='BaseAUC-ROC'):\n    for label,model in models.items():\n        print(\"Testing our {0} model\".format(label))\n        score=str(round(model.score(X_test,y_test),2))\n        labels=model.predict_proba(np.array(X_test.values))[:,1]\n        roc=str(round(roc_auc_score(y_test,labels,average='macro',sample_weight=None),2))\n        result.loc[label,lscore]=score\n        result.loc[label,auc]=roc\n    return result","103195d6":"#Testing base model\ntestingfunc(X_test=X_test,y_test=y_test,models=models,lscore='Basecore',auc='BaseAUC-ROC')","4b6aebbe":"mylabels = X_train.columns\nforest = RandomForestClassifier(n_estimators=500,random_state=1)\nforest.fit(X_train,y_train)\nimportances=forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" %(f+1,30,mylabels[indices[f]],importances[indices[f]]))","fd306a38":"plt.figure(figsize=(15,6))\nplt.title(\"Feature Importances\")\nplt.bar(range(X_train.shape[1]), importances[indices], color=\"red\", align=\"center\")\nplt.xticks(range(X_train.shape[1]),mylabels, rotation=75, fontsize=12)\nplt.xlim([-1,X_train.shape[1]])\n;","229dfffa":"from sklearn.feature_selection import SelectFromModel\nsfm = SelectFromModel(forest, threshold=0.0125, prefit=True)\nselected = sfm.transform(X_train)\nprint(\"{0} best features were seleted\".format(selected.shape[1]) )\nselected_columns=[]\nfor feat in range(selected.shape[1]):\n    selected_columns.append(mylabels[indices[feat]])\n    print(\"%2d) %-*s %f\"%(feat+1,30,mylabels[indices[feat]],importances[indices[feat]]))","ef576660":"#selected_columns\nX_train_selected=X_train[selected_columns]\nX_test_selected=X_test[selected_columns]\n\n#training with best features\ntrainfunc(X_train=X_train_selected,y_train=y_train,models=models)\n#Testing with best features\nresult[\"Dimrecscore\"]=\"\"\nresult[\"DimrecAUC-ROC\"]=\"\"\ntestingfunc(X_test=X_test_selected,y_test=y_test,models=models,lscore='Dimrecscore',auc='DimrecAUC-ROC')","85a95420":"from sklearn.model_selection import cross_val_score\n\ndef cvbuild(cvmodel, scr,X_train,y_train,cv=10):\n    \n    for label,model in cvmodel.items():\n        cvscore = cross_val_score(model,X_train,y_train,cv=cv,scoring=scr)\n        result.loc[label,'cvAUC-ROC']=str(round(cvscore.mean(),2))\n        result.loc[label,'cvscore_std']=str(round(cvscore.std(),4))\n    return result\n","7a4fee09":"cvd=cvbuild(cvmodel=models,X_train=X_train_selected,y_train=y_train,scr='roc_auc')\ncvd\n","eecc9c17":"from sklearn.model_selection import RandomizedSearchCV\n# param tuning for ada boost\nadaParams ={'n_estimators':[10,50,200,420]}\ngridSearchada = RandomizedSearchCV(estimator=ada,param_distributions=adaParams,n_iter=4, scoring='roc_auc',\n                                  cv=5).fit(X_train_selected,y_train);","8bd32e62":"# best params\ngridSearchada.best_estimator_,gridSearchada.best_score_","95013598":"#param tuning for gradient boosting\nfrom scipy.stats import randint\ngbParams = {'loss' :['deviance','exponential'],'n_estimators' : randint(10,500), 'max_depth':randint(1,10)}\ngridSearchGB= RandomizedSearchCV(estimator=gboosting,param_distributions=gbParams,n_iter=10,\n                                scoring='roc_auc',cv=5).fit(X_train_selected,y_train)","c86dd9f3":"gridSearchGB.best_params_,gridSearchGB.best_score_","e1182f5f":"#Training the best approach\nbestada = gridSearchada.best_estimator_.fit(X_train_selected,y_train)\nbestgboosting=gridSearchGB.best_estimator_.fit(X_train_selected,y_train)\n\n# Getting score for the best approach\n# ada\nada_labels = bestada.predict_proba(np.array(X_test_selected))[:,1]\nbestadaroc=round(roc_auc_score(y_test,ada_labels, average='macro',sample_weight=None),2)\n# gradient boosting\ngb_labels = bestgboosting.predict_proba(np.array(X_test_selected))[:,1]\nbestgbroc=round(roc_auc_score(y_test,gb_labels, average='macro',sample_weight=None),2)\n\nresult.loc['adaBoost','bestAUC-ROC']=bestadaroc\nresult.loc['gradient Boosting','bestAUC-ROC']=bestgbroc\n\nresult","e902856d":"from sklearn.metrics import classification_report\nada_pred = bestada.predict(X_test_selected)\nprint(classification_report(y_test,ada_pred))","af15d9a4":"gb_pred=bestgboosting.predict(X_test_selected)\nprint(classification_report(y_test,gb_pred))","9976da40":"### Training the baseline model\nUsing the algorithms above, we will train our data using all features.","bd8d03ad":"## Model Building\nIn this section, we are going to build our first model.  We are going to choose find different machine algorithms to train our base model using all features, then select the one that perform well to tune in order to have better accuracy.\n### Selecting machine learning algorithms\nThis is a classification problem, we want to predict whether or not a customer will churn. Here are the classifications that we will explore:\n* K-Nearest Neighbor (KNN)\n* Logistic Regression\n* AdaBoost\n* GradientBoosting\n* RandomForest\n\n\n#### Splitting the data in training set and test set\nWe are going to keep 70% of data for training and 30% for testing. Based on our analysis above, we saw that  about 73% of customers did not churn and about 27%, which is somewhat unbalanced. We are going to add the argument **stratify=y** to make sure that both training and test datasets have the same class proportions as the original dataset.","8dcddc53":"### Performance evaluation\n","439c03df":"# Telecom customer churn prediction\n   ## Introduction\nCustomer churn or customer turnover refers to when a customer ceases services with a company. Churn prediction is a subset of problem that can be extend to many area such as employees in a company, customer churn from a mobile subscription etc. \nWe are going to use the Telecom data to predict churn. After loading the the data, we will explore attributes and different relationships between them before building our model.\n \n ## Loading Data","74e7b0e6":"After cross validation, we see an improvement in Knn and random forest score. Logistic regression, adaboost score and gradient boosting all have the same score as in the dimension reduction. However, we see that adaBoost and gradient boosting have with equal score lower standard deviation, therefore, they seems to be more consistent for our model compare to other algorithms.\n\n#### Hyperparameter tuning\nGiven that implemeting parameter tuning can be very time consuming, we will tune only two of our best models (adaBoost and Gradient Boosting)","2fed1fdb":"Above is the text summary of the precision, recall and F1 score.\n\n* Precision is the ability of the classifier to not label a sample that is negative as positve\n* Recall is the ability of the classifier to fin positive samples\n* F1 score is the weigthed mean of the precision and recall.\n\ncomparatively, we see that gradient boosting has a better accuracy and recall compare to adaBoost.","f087f998":"Customers who are charged less that 40 a month seems to churn less. As the monthly rate increase, they churn more. Customers who churn the most pay between 70-100 a month.\n\nCustomers who pay by Electronic check seems to churm more than customers who pay by mailed check, bank transfer or credit card. Mailed check, bank transfer or credit card customers seems to churn in about the same rate.\n\n###  Churn by total charges","31e925e3":"The accuracy after dimensionality reduction.The new columns are **Dimrecscore,DimrecAUC-ROC**.As we can see, our accuracy did not change much after we dropped some columns.","ea5e1f0c":"#### Retaining our model after features selection\nWe are going to now retraining our model after features selection. The goal of this exercise is to make sure our accuracy does not decrease drop drastically after we removed some columns.","922ad87b":"Most customers are month to month customers, they churn more than customers who subscribe for one year or two years contrats.\n\n###  Churn by Payment method","b1c2c0ca":"Based on the method used here to detect outliers, all values seems to be in the normal range. Therefore, our dataset does not have outliers.\n\n## Feature engineering\nIn this section, we will find the feature that are more predictive for our model. Before proceed to our features engineering, we are going to map all the string boolean to numeric boolean yes=1 and No=0","8a8d192b":"## Detecting outliers\nAn outlier is a value that lies at an abnormally high distance from other values in the dataset. It can be much smaller or much larger. Basically, it doe not show the same pattern as other values. We will be using interquartile range(IQR) to detect outliers. The interquartile range is te range between the first quartile(Q1) and the third quartile (Q3). With this approach, any value which is more than 1.5 IQR+Q3 or less than Q1 - 1.5 IQR is considered as outlier. We will check the outlier in price.\n","0301d18b":"We are going to draw the boxplot for the tenure column and get the outlier list.","e1863b7c":"###  Inspecting Churn by gender","c1356bd6":"For other categorical features, we will do a one-hot encoding to transform them to binary. For each variable that has n features, we will create n-1 features.Basically one-hot encoding creates a dummy feature for each unique value in the nominal feature and assign 1 if it has a value and 0 otherwise.","4a2b42a0":"### Baseline Model evaluation\nWe are going to see how our algorithms will perform on the testing set. We will use as evaluation metrics the mean accuracy score and the ROC-AUC score.","d0d2e2ff":"From the observations above, it looks like:\n* The **tenure** seems to be bimodal. The first most represent who haven't been in the company for a long time, the second some faithfull customers who have been with the company for a very long time.\n* Looking at **Monthlycharges**, It looks like newer customer are charged more than those who stay longer with the company. Most customers seems to pay between 70-90.\n* **Totalcharges** is a right skewed distribution, there are a lot of customers with lower total charges, but fewer with very large balance.","e9f28ea5":"There are more male customer than female customers. But box sexes seems to churn with the same percentage.","f443c09d":"\nWe see that about 73% of customers did not churn and about 27% churned.The data seems to be somewhat imbalance.  \n\n### 2. Finding the missing values","72918e64":"#### Cross validation\nWe are going to implement our cross validation now using scikit-learn cross-validation score module. We will be choosing k=5.","caa371ad":"Now let's set a threshold to take the most important features. we will set our thresold to 0.05.","28bf22e4":"###  Churn by contract type","895c082a":"As we can see, customers who churn seems on average to stay less in the company and have a monthly greater charges  compare to those who do not churn. Their total charges is lower than customers that do not churn.","8738fae3":"We are going to convert Total charges that is a numeric from object data type to float.","e657958e":"Customers who pay by Electronic check seems to churm more than customers who pay by mailed check, bank transfer or credit card. Mailed check, bank transfer or credit card customers seems to churn in about the same rate.\n\n###  Churn by Montly rate","ead6eb2d":"## Understanding the dataset\nThe dataset has 21 attributes and below is the definition:\n* customerID: Customer ID, unique identifier for each customer\n* gender    : Whether the customer is a male or a female\n* SeniorCitizen: Whether the customer is a senior citizen or not (1, 0)\n* Partner : Whether the customer has a partner or not (Yes, No)\n* Dependents: Whether the customer has dependents or not (Yes, No)\n* tenure : Number of months the customer has stayed with the company\n* PhoneService : Whether the customer has a phone service or not (Yes, No)\n* MultipleLines : Whether the customer has multiple lines or not (Yes, No, No phone service)\n* InternetService : Customer\u2019s internet service provider (DSL, Fiber optic, No)\n* OnlineSecurity : Whether the customer has online security or not (Yes, No, No internet service)\n* OnlineBackup : Whether the customer has online backup or not (Yes, No, No internet service)\n* DeviceProtection: Whether the customer has device protection or not (Yes, No, No internet service)\n* TechSupport : Whether the customer has tech support or not (Yes, No, No internet service)\n* StreamingTV : Whether the customer has streaming TV or not (Yes, No, No internet service)\n* StreamingMovies : Whether the customer has streaming movies or not (Yes, No, No internet service)\n* Contract : The contract term of the customer (Month-to-month, One year, Two year)\n* PaperlessBilling: Whether the customer has paperless billing or not (Yes, No)\n* PaymentMethod : The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n* MonthlyCharges: The amount charged to the customer monthly\n* TotalCharges : The total amount charged to the customer\n* Churn: Whether the customer churned or not (Yes or No\n\n## Exploratory data analyis\nIn this section, we will first  do an exploratory data analysis by exploring most attributes and check their contribution or how they are related to customers churn. We will follow the steps below:\n* 1. Listing statistical properties\n* 2. Finding the missing values\n* 3. Correlation\n* 4. Detecting Outliers\n\n### 1. Listing statistical properties\nBefore running our statistic, we will take a look at the data type.","451a0a18":"Non senior citizens churn more that senior citizens","1b00864a":"###  Inspecting the mean attributes of customers who churn ","2f21d90c":"The table above has the mean accuracy score and the Auc-roc score, which is more significant than the former.In fact, the mean accuracy score considers only one threshold value, while ROC-AUC score takes in consideration all possible treshold value and return the score.\nWe see that Logistic regression, adaBoost and gradient boosting gives us the best ROC-AUC. Their scores are very competitive as well. From now on, we are going to work with **logistic regression** and **gradient boosting** to see how we can improve our prediction.\n\n### Model optimization\nIn this section, we are going to try to improve the accuracy of our model. We will first focus on two techniques:\n* Features selection\n* Cross validation\n* Hyperparameter tuning\n\n**Features selection** consist of choosing the set of features that is most important to the model and that reduces overfitting.\n**Cross validation** is a technique that consist of dividing the data in multiple folds(k), and at each iteration, using one k-1 fold for training and one fold for validation. This will help to avoid **overfitting**(our model does not generalize properly on unseen data) and help us choosing the best model. The general term is k-fold cross validation which k in the number of fold the training data is split into.\n\n**Hyperparameter tuning** consist of feeding our model with a range of paramters and consider the one that allow the model generate better accuracy.\n\n#### Features selection\nWe will be assessing feature importance with random forests. We can measure the features importance as the averaged impurity decrease computed from all the decision trees in the forest, without making any assumptions about whether our data is linearly separable or not.\n","4447acb2":"## Feature Standardisation\nWe are going to bring all our continues features to the same magnitude by standisize them. ","33d9134b":"Customers who have a total balance less than 1500 seems to churn more than customers with higher balance.\n\n###  Churn by Monthy charges and tenure","9308be0d":"As we can see, each customer get charge a minimum of 64.76, the average total charges per customer is about 2283 and the average of month a customer stay with the company is about about 32 months.","4a64e478":"As we see above, the column TotalCharges has 11 missing values. We are going to replace the value missing by the mean.         \n","a0951258":"Above are the best parameters found for adaBoost and gradient Boosting.\n#### Implementing and testing the better model.\nIn this section, we are going to re-implement our two models using the best parameters found.\n"}}