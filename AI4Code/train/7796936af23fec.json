{"cell_type":{"27cdb976":"code","4ea1b0c3":"code","c8e8ef80":"code","8ad46014":"code","4f23ad22":"code","6045cfcb":"code","3f948882":"code","695c1466":"code","1710424b":"code","f26d809b":"code","e104854e":"code","1112b537":"markdown","478de2f7":"markdown","b35f68a5":"markdown","6d8a9333":"markdown"},"source":{"27cdb976":"import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms","4ea1b0c3":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","c8e8ef80":"# convert data to torch.FloatTensor\ntransform = transforms.ToTensor()\n\n# load the training and test datasets\ntrain_data = datasets.CIFAR10(root='data', train=True,\n                                   download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='data', train=False,\n                                  download=True, transform=transform)","8ad46014":"# Create training and test dataloaders\n\nnum_workers = 0\n# how many samples per batch to load\nbatch_size = 20\n\n# prepare data loaders\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)","4f23ad22":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# helper function to un-normalize and display an image\ndef imshow(img):\n    img = img \/ 2 + 0.5  # unnormalize\n    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n    \n# specify the image classes\nclasses = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n           'dog', 'frog', 'horse', 'ship', 'truck']","6045cfcb":"# obtain one batch of training images\ndataiter = iter(train_loader)\nimages, labels = dataiter.next()\nimages = images.numpy() # convert images to numpy for display\n\n# plot the images in the batch, along with the corresponding labels\nfig = plt.figure(figsize=(25, 4))\n# display 20 images\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","3f948882":"rgb_img = np.squeeze(images[3])\nchannels = ['red channel', 'green channel', 'blue channel']\n\nfig = plt.figure(figsize = (36, 36)) \nfor idx in np.arange(rgb_img.shape[0]):\n    ax = fig.add_subplot(1, 3, idx + 1)\n    img = rgb_img[idx]\n    ax.imshow(img, cmap='gray')\n    ax.set_title(channels[idx])\n    width, height = img.shape\n    thresh = img.max()\/2.5\n    for x in range(width):\n        for y in range(height):\n            val = round(img[x][y], 2) if img[x][y] !=0 else 0\n            ax.annotate(str(val), xy=(y,x),\n                    horizontalalignment='center',\n                    verticalalignment='center', size=8,\n                    color='white' if img[x][y]<thresh else 'black')","695c1466":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# define the NN architecture\nclass ConvAutoencoder(nn.Module):\n    def __init__(self):\n        super(ConvAutoencoder, self).__init__()\n        ## encoder layers ##\n        # conv layer (depth from 3 --> 16), 3x3 kernels\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n        # conv layer (depth from 16 --> 4), 3x3 kernels\n        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n        self.pool = nn.MaxPool2d(2, 2)\n        \n        ## decoder layers ##\n        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n        self.t_conv2 = nn.ConvTranspose2d(16, 3, 2, stride=2)\n\n    def forward(self, x):\n        ## encode ##\n        # add hidden layers with relu activation function\n        # and maxpooling after\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        # add second hidden layer\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)  # compressed representation\n        \n        ## decode ##\n        # add transpose conv layers, with relu activation function\n        x = F.relu(self.t_conv1(x))\n        # output layer (with sigmoid for scaling from 0 to 1)\n        x = F.sigmoid(self.t_conv2(x))\n                \n        return x\n\n# initialize the NN\nmodel = ConvAutoencoder()\nprint(model)","1710424b":"# specify loss function\ncriterion = nn.BCELoss()\n\n# specify loss function\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","f26d809b":"# number of epochs to train the model\nn_epochs = 100\n\nfor epoch in range(1, n_epochs+1):\n    # monitor training loss\n    train_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    for data in train_loader:\n        # _ stands in for labels, here\n        # no need to flatten images\n        images, _ = data\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        outputs = model(images)\n        # calculate the loss\n        loss = criterion(outputs, images)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*images.size(0)\n            \n    # print avg training statistics \n    train_loss = train_loss\/len(train_loader)\n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, \n        train_loss\n        ))","e104854e":"# obtain one batch of test images\ndataiter = iter(test_loader)\nimages, labels = dataiter.next()\n\n# get sample outputs\noutput = model(images)\n# prep images for display\nimages = images.numpy()\n\n\n# output is resized into a batch of iages\noutput = output.view(batch_size, 3, 32, 32)\n# use detach when it's an output that requires_grad\noutput = output.detach().numpy()\n\n# # plot the first ten input images and then reconstructed images\n# fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\n\n# # input images on top row, reconstructions on bottom\n# for images, row in zip([images, output], axes):\n#     for img, ax in zip(images, row):\n#         ax.imshow(np.squeeze(img))\n#         ax.get_xaxis().set_visible(False)\n#         ax.get_yaxis().set_visible(False)\n\n# plot the first ten input images and then reconstructed images\nfig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(output[idx])\n    ax.set_title(classes[labels[idx]])\n    \n# plot the first ten input images and then reconstructed images\nfig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(24,4))\nfor idx in np.arange(20):\n    ax = fig.add_subplot(2, 20\/2, idx+1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(classes[labels[idx]])","1112b537":"---\n## Training\n\nHere I'll write a bit of code to train the network. I'm not too interested in validation here, so I'll just monitor the training loss and the test loss afterwards. \n\nWe are not concerned with labels in this case, just images, which we can get from the `train_loader`. Because we're comparing pixel values in input and output images, it will be best to use a loss that is meant for a regression task. Regression is all about comparing quantities rather than probabilistic values. So, in this case, I'll use `BCELoss`. And compare output images and input images as follows:\n```\nloss = criterion(outputs, images)\n```\n\nOtherwise, this is pretty straightfoward training with PyTorch. Since this is a convlutional autoencoder, our images _do not_ need to be flattened before being passed in an input to our model.","478de2f7":"### Visualize the data","b35f68a5":"## Convolutional  Autoencoder\n\n#### Encoder\nThe encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers. \n\n#### Decoder\n\nThe decoder though might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 32x32x3 image out from the decoder so we need to work our way back up from the compressed representation.","6d8a9333":"# Convolutional Autoencoder\n\nSticking with the CIFAR10 dataset, let's improve our autoencoder's performance using convolutional layers. We'll build a convolutional autoencoder to compress the MNIST dataset. \n\n>The encoder portion will be made of convolutional and pooling layers and the decoder will be made of **transpose convolutional layers** that learn to \"upsample\" a compressed representation.\n\n### Compressed Representation\n\nA compressed representation can be great for saving and sharing any kind of data in a way that is more efficient than storing raw data. In practice, the compressed representation often holds key information about an input image and we can use it for denoising images or oher kinds of reconstruction and transformation!\n\nLet's get started by importing our libraries and getting the dataset."}}