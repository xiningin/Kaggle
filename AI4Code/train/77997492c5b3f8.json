{"cell_type":{"51746ffc":"code","a9e93f4c":"code","5bf60b2c":"code","e28bde31":"code","e211452a":"code","ac02487a":"code","8ab05810":"code","42d53471":"code","00890fb8":"code","077a4653":"code","fd4acd15":"code","aa98549a":"code","e394f141":"code","b642b5c7":"code","e2268318":"code","0fa29a09":"code","17f93eab":"code","f3068e75":"code","4be3648f":"code","3850feff":"code","388cf627":"code","45d3cccc":"code","aa82fdbc":"code","fb717e9e":"code","46382409":"code","5704aff9":"code","6bdbe3a7":"code","106efc11":"code","94b0aa02":"code","83b71d82":"code","0756dac6":"code","a599ba34":"code","b29f7526":"code","8d56a637":"code","4fd5df95":"code","af239da6":"code","31d7e6c0":"code","63e66f00":"code","0520f701":"code","67254aed":"code","d8573c17":"code","31234000":"code","15296597":"code","1b8ea438":"code","e13d3f92":"code","5b429dae":"code","fc026082":"code","02668557":"code","8c61cea2":"code","7367297d":"code","2adcf4be":"code","58d6a008":"code","909104e9":"code","6a6ed4e4":"code","42ee8baf":"code","87256099":"code","6170a177":"code","1d239d3b":"code","538da81d":"code","23ccb11f":"code","39c78a8b":"code","a8b251b1":"code","8bf2c32d":"code","25a4614a":"code","91534f04":"code","1e764313":"markdown","2c3e63c9":"markdown","6230857f":"markdown","909f78d0":"markdown","f18901bc":"markdown","df6fd444":"markdown"},"source":{"51746ffc":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.nonparametric.kde import KDEUnivariate\nfrom statsmodels.nonparametric import smoothers_lowess\nfrom pandas import Series, DataFrame\nfrom patsy import dmatrices\n#from KaggleAux import predict as ka # see github.com\/agconti\/kaggleaux for more details\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# sklearn imports\nfrom sklearn.preprocessing import StandardScaler        \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n\n# Keras imports \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\n\n","a9e93f4c":"# loading data and getting shape\ndf_treino = pd.read_csv(\"..\/input\/titanic\/train.csv\") \ndf_teste = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndf_gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nprint(df_treino.shape, df_teste.shape, df_gender_submission.shape)\n","5bf60b2c":"# Verifying the NaN values\nprint(f'Treino \\n{df_treino.isnull().sum()}')\nprint('\\n')\nprint(f'Treino \\n{df_teste.isnull().sum()}')","e28bde31":"# Dropping 'Cabin' because have very NaN values\ndf_teste = df_teste.drop(['Cabin'], axis = 1)\ndf_treino = df_treino.drop(['Cabin'], axis = 1)\n","e211452a":"# Fitting missing values. Using mean of Ages\ndf_treino['Age'].fillna(df_treino['Age'].mean(), inplace = True)\ndf_teste['Age'].fillna(df_teste['Age'].mean(), inplace = True)\n","ac02487a":"# Fill only one NaN 'Fare'. Using mean too.\ndf_teste['Fare'].fillna(df_teste['Fare'].mean(), inplace = True)\n","8ab05810":"# Searching for outliers\nsns.boxplot('Survived', 'Fare', data = df_treino)","42d53471":"# Cutting the outliers\ndf_treino = df_treino[df_treino['Fare']<400]\nsns.boxplot('Survived', 'Fare', data = df_treino)","00890fb8":"# Creating new columns for Family_size and Family group\n\ndf_treino['Family_size'] = df_treino['SibSp'] + df_treino['Parch'] + 1\ndf_teste['Family_size'] = df_teste['SibSp'] + df_teste['Parch'] + 1\n\ndef family_group(size):\n    a = ''\n    if size <= 1:\n        a = 'loner'\n    elif size < 4:\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ndf_treino['Family_group'] = df_treino['Family_size'].map(family_group)\ndf_teste['Family_group'] = df_teste['Family_size'].map(family_group)","077a4653":"# Ploting \"Family_group\"\nsns.barplot( df_treino['Family_group'].value_counts().index, df_treino['Family_group'].value_counts().values)","fd4acd15":"# Cast Sex and Family_group values to categorical\ndf_treino['Sex_code'] = pd.Categorical(df_treino.Sex).codes\ndf_teste['Sex_code'] = pd.Categorical(df_teste.Sex).codes\ndf_treino['Family_group_code'] = pd.Categorical(df_treino['Family_group']).codes\ndf_teste['Family_group_code'] = pd.Categorical(df_teste['Family_group']).codes","aa98549a":"# Mapping 'Fare' in groups and creating 'Fare_group_code'\ndef fare_group(fare):\n    a = ''\n    if fare <=4:\n        a = 'very_Low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    else:\n        a = 'very_high'\n    return a \n\ndf_treino['Fare_group'] = df_treino['Fare'].map(fare_group)\ndf_teste['Fare_group'] = df_teste['Fare'].map(fare_group)\ndf_treino['Fare_group_code'] = pd.Categorical(df_treino['Fare_group']).codes\ndf_teste['Fare_group_code'] = pd.Categorical(df_teste['Fare_group']).codes","e394f141":"# Trying to get correlations between 'Survived' and Name's length\n## It is crazy way !!!!\ndf_treino['len_names'] = df_treino['Name'].apply(len)\ndf_teste['len_names'] = df_teste['Name'].apply(len)","b642b5c7":"## Plotting len names\nsns.distplot(df_treino['len_names'])","e2268318":"## Mapping the Age in groups and codding\ndef age_group(age):\n    a = ''\n    if age <= 5:\n        a = 'infant'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'young_adult'\n    elif age <= 65:\n        a = 'adult'\n    else:\n        a = 'senior'\n    return a \n\ndf_treino['Age_group'] = df_treino['Age'].map(age_group)\ndf_teste['Age_group'] = df_teste['Age'].map(age_group)\ndf_treino['Age_group_code'] = pd.Categorical(df_treino['Age_group']).codes\ndf_teste['Age_group_code'] = pd.Categorical(df_teste['Age_group']).codes","0fa29a09":"# Dropping missing values in 'Embarked'\ndf_treino.dropna(inplace = True)\ndf_treino.info()\n\n# creating a categorical variable with 'Embarked' values\ndf_treino['Embarked_code'] = pd.Categorical(df_treino['Embarked']).codes\ndf_teste['Embarked_code'] = pd.Categorical(df_teste['Embarked']).codes\n\n","17f93eab":"# Specifies the parameters of graphs\nfig = plt.figure(figsize=(12, 8))\n\n# Lets plot differents graphs \nax1 = plt.subplot2grid((2, 3), (0, 0))\n# plots the bar graph\ndf_treino.Survived.value_counts().plot(kind = 'bar', alpha = 0.8)\n# setting the range of 'x' values\nax1.set_xlim(-1, 2)\nplt.title(\"Distribution of Survival, (1 = Survived)\")\n\n# another graph here\nax2 = plt.subplot2grid((2, 3), (0, 1))\nplt.scatter(df_treino.Survived, df_treino.Age, alpha = 0.8)\nplt.xlabel = \"Age\"\nplt.grid(axis='y')\nplt.title(\"Survival by Age,  (1 = Survived)\")\n\n# another graph here\nax3 = plt.subplot2grid((2, 3), (0, 2))\ndf_treino.Pclass.value_counts().plot(kind = 'barh', alpha = 0.8 )\nplt.title('Class Distribution')\nax3.set_ylim(-1, len(df_treino.Pclass.value_counts()))\nplt.grid()\n\n# another graph here\nax2 = plt.subplot2grid((2, 3), (1, 0), colspan=2)\ndf_treino[df_treino.Pclass == 1]['Age'].plot(kind = 'kde')\ndf_treino[df_treino.Pclass == 2]['Age'].plot(kind = 'kde')\ndf_treino[df_treino.Pclass == 3]['Age'].plot(kind = 'kde')\nplt.title(\"Age Distribution within classes\")\nplt.legend(['1 Class', '2 Class', '3 Class'], loc  ='best')\n\n# another graph here\nax2 = plt.subplot2grid((2, 3), (1, 2))\ndf_treino.Embarked.value_counts().plot(kind = 'bar', alpha = 0.8)\nax2.set_xlim(-1, 3)\nplt.title(\"Passengers per boarding location\")\n","f3068e75":"fig = plt.figure(figsize=(18, 6))\ndf_male = df_treino[df_treino.Sex_code == 1]['Survived'].value_counts().sort_index()\ndf_female = df_treino[df_treino.Sex_code == 0]['Survived'].value_counts().sort_index()\n\nax1 = fig.add_subplot(121)\ndf_male.plot(kind = 'barh', label = 'male', alpha = 0.55)\ndf_female.plot(kind = 'barh', label = 'female', color = 'red', alpha = 0.55)\nplt.title(\"Who Survived? with respect to Gender, (raw value counts) \"); plt.legend(loc='best')\nax1.set_ylim(-1, 2)\nplt.legend()\n\nax1 = fig.add_subplot(122)\n(df_male\/float(df_male.sum())).plot(kind='barh',label='Male', alpha=0.55)  \n(df_female\/float(df_female.sum())).plot(kind='barh', color='#FA2379',label='Female', alpha=0.55)\nplt.title(\"Who Survived proportionally? with respect to Gender\"); plt.legend(loc='best')","4be3648f":"# Let's see the correlation among variables\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_treino.corr(), annot=True)","3850feff":"# values without scaler\nX = df_treino.loc[:, ['Pclass', 'Sex_code', 'Age',  'Family_group_code', 'Fare_group_code', 'len_names', 'Embarked_code', 'Age_group_code']].values\ny = df_treino.loc[:, 'Survived'].values\nX_teste = df_teste.loc[:, ['Pclass', 'Sex_code', 'Age', 'Family_group_code', 'Fare_group_code', 'len_names', 'Embarked_code', 'Age_group_code']].values\n\n# train test without scaler\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.25)\nprint(len(X_train), len(X_valid), len(y_train), len(y_valid))\n\n\n## with scaler\nscaler = StandardScaler()\nX_scaler = scaler.fit_transform(X)\nX_teste_scaler = scaler.fit_transform(X_teste)\ny_scaler = y\n\n# train test with scaler\nX_train_scaler, X_valid_scaler, y_train_scaler, y_valid_scaler = train_test_split(X_scaler, y, test_size = 0.25)\nprint(len(X_train_scaler), len(X_valid_scaler), len(y_train_scaler), len(y_valid_scaler))\n","388cf627":"# Random Forest with Kfold without scaler\n\nresultado_random = []\nfold = KFold(n_splits=5, shuffle=True)\nfor train_index, valid_index in fold.split(X):\n    print(\"TRAIN:\", len(train_index), \"TEST:\", len(valid_index))\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    modelo_random = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n    modelo_random.fit(X_train, y_train)\n    p = modelo_random.predict(X_valid)\n    resultado_random.append(accuracy_score(y_valid, p))\nprint(f'Mean Random results without scaler {np.mean(resultado_random)}')\nprint(f'All results random {resultado_random}')\n\n\n","45d3cccc":"# Random Forest with Kfold and scaler\nresultado_random = []\nfold = KFold(n_splits=5, shuffle=True)\nfor train_index, valid_index in fold.split(X):\n    print(\"TRAIN:\", len(train_index), \"TEST:\", len(valid_index))\n    X_train, X_valid = X_scaler[train_index], X_scaler[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    modelo_random_scaler = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n    modelo_random_scaler.fit(X_train, y_train)\n    p = modelo_random_scaler.predict(X_valid)\n    resultado_random.append(accuracy_score(y_valid, p))\nprint(f'Mean random results with scaler {np.mean(resultado_random)}')\nprint(f'All results random {resultado_random}')\n","aa82fdbc":"# Using GridSearchCV with and without scaler\n\nvariables = {'X': X, 'X_scaler': X_scaler}\nresults_random = {}\nmodelos = ['modelo_random', 'modelo_random_scaler']\nfor i, j in variables.items():\n    parameters = {'n_estimators': [100, 500, 1000, 2000]}\n    metrics = ['accuracy']\n    for i in modelos:\n        modelo = RandomForestClassifier(n_jobs=-1)\n        grid_random = GridSearchCV(modelo, parameters, scoring = metrics, refit=False, cv = 10)\n        results_random[i] = grid_random.fit(j, y)\n    print(results_random)\n\n","fb717e9e":"results_random['modelo_random'].cv_results_","46382409":"results_random['modelo_random_scaler'].cv_results_","5704aff9":"\n# Neural Network now\n# with kfold method with and without scaler\n# Changing the activation param\n\nresultado_geral = []\nresultado_rede_neural = []\n\nactivation = ['identity', 'logistic', 'tanh', 'relu']\nvariables = {'X': X, 'X_scaler': X_scaler}\nresults_neural = {}\nmodelos = ['modelo_random', 'modelo_random_scaler']\nfor i, j in variables.items():\n    for act in activation:\n        fold = KFold(n_splits=4, shuffle=True)\n        for train_index, valid_index in fold.split(X):\n            #print(\"TRAIN:\", train_index, \"TEST:\", valid_index)\n            X_train, X_valid = j[train_index], j[valid_index]\n            y_train, y_valid = y[train_index], y[valid_index]\n            modelo_rede_neural = MLPClassifier(activation = act, max_iter = 5000)\n            modelo_rede_neural.fit(X_train, y_train)\n            p = modelo_rede_neural.predict(X_valid)\n            results_neural[i] = accuracy_score(y_valid, p)\nprint(results_neural)\n        ","6bdbe3a7":"# Using GridSearchCV with and without scaler (neural network)\n# Changing the activation and solver params\n\nvariables = {'X': X, 'X_scaler': X_scaler}\nresults_grid_neural = {}\nmodelos = ['modelo_neural', 'modelo_neural_scaler']\nparameters = {'activation': ['identity', 'logistic', 'tanh', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam']}\nmetrics = ['accuracy']\nfor i, j in variables.items():\n    for mod in modelos:\n        modelo = MLPClassifier(max_iter=5000)\n        grid = GridSearchCV(modelo, parameters, scoring = metrics, refit=False, cv = 5)\n        results_grid_neural[mod] = grid.fit(j, y)\n\nprint(results_grid_neural)\n","106efc11":"results_grid_neural['modelo_neural'].cv_results_","94b0aa02":"\nresults_grid_neural['modelo_neural_scaler'].cv_results_","83b71d82":"# Creating model to sbmit in Kaggle Competition - neural first\nmodelo_rede_neural = MLPClassifier(activation='logistic', solver='adam', max_iter = 5000)\nmodelo_rede_neural.fit(X, y)\np_neural = modelo_rede_neural.predict(X_teste)","0756dac6":"# This is the format required\nrede_neural_submission = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_neural})","a599ba34":"# Save to csv file\nrede_neural_submission.to_csv('rede_neural_submission.csv', index = False)\nrede_neural_submission.head()","b29f7526":"# Creating model to sbmit in Kaggle Competition - now random\nmodelo_random = RandomForestClassifier(n_estimators=500, n_jobs=-1)\nmodelo_random.fit(X, y)\np_random = modelo_random.predict(X_teste)\n","8d56a637":"random_submission = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_random})\n","4fd5df95":"random_submission.to_csv('random_submission.csv', index = False)\nrandom_submission.head()","af239da6":"random_submission['Survived'].sum(), rede_neural_submission['Survived'].sum()","31d7e6c0":"# Classifier comparision\n# wide aproach\nclassifiers = [\n    KNeighborsClassifier(3),\n    svm.SVC(probability=True),\n    DecisionTreeClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n    \n\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog= pd.DataFrame(columns=log_cols)","63e66f00":"SSplit=StratifiedShuffleSplit(test_size=0.3,random_state=7)\nacc_dict = {}\n\nfor train_index,test_index in SSplit.split(X,y, ):\n    X_train,X_test=X[train_index],X[test_index]\n    y_train,y_test=y[train_index],y[test_index]\n    \n    for clf in classifiers:\n        name = clf.__class__.__name__\n          \n        clf.fit(X_train,y_train)\n        predict=clf.predict(X_test)\n        acc=accuracy_score(y_test,predict)\n        if name in acc_dict:\n            acc_dict[name]+=acc\n        else:\n            acc_dict[name]=acc","0520f701":"log['Classifier']=acc_dict.keys()\nlog['Accuracy']=acc_dict.values()\n#log.set_index([[0,1,2,3,4,5,6,7,8,9]])\n%matplotlib inline\nsns.set_color_codes(\"muted\")\nax=plt.subplots(figsize=(10,8))\nax=sns.barplot(y='Classifier',x='Accuracy',data=log,color='b')\nax.set_xlabel('Accuracy',fontsize=20)\nplt.ylabel('Classifier',fontsize=20)\nplt.grid(color='r', linestyle='-', linewidth=0.5)\nplt.title('Classifier Accuracy',fontsize=20)","67254aed":"modelo_XGB = XGBClassifier(n_estimators=500)\nmodelo_XGB.fit(X, y)\np_XGB = modelo_XGB.predict(X_teste)\nXGB_submission = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_XGB})\nXGB_submission.to_csv('XGB_submission.csv', index = False)","d8573c17":"modelo_gradient = GradientBoostingClassifier()\nmodelo_gradient.fit(X, y)\np_gradient = modelo_gradient.predict(X_teste)\ngradient_submission = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_gradient})\ngradient_submission.to_csv('gradient_submission.csv', index = False)","31234000":"XGB_submission['Survived'].sum(), gradient_submission['Survived'].sum()","15296597":"# working with keras\n\nmodelo_keras = Sequential()\nmodelo_keras.add(Dense(units = 5, activation = 'relu', input_dim = 8))\nmodelo_keras.add(Dense(units = 5, activation = 'relu'))\nmodelo_keras.add(Dense(units = 1, activation = 'sigmoid'))\nmodelo_keras.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","1b8ea438":"# Calculation precision in trainnig dataframe\nmodelo_keras.fit(X_train, y_train, batch_size=10, epochs = 100)\np_keras = modelo_keras.predict(X_valid)\np_keras = (p_keras > 0.75)\nprint(p_keras.sum())\nprecisao = accuracy_score(y_valid, p_keras)","e13d3f92":"modelo_keras.fit(X, y, batch_size=10, epochs = 100)\np_keras_real = modelo_keras.predict(X_teste_scaler)\np_keras_real = [1 if i > 0.75 else 0 for i in p_keras_real]","5b429dae":"keras_submission = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_keras_real})\nkeras_submission.to_csv('keras_submission_simple.csv', index = False)","fc026082":"# Keras with all parameters\n\nmodelo_keras = Sequential()\nmodelo_keras.add(Dense(units = 5, activation = 'relu', input_dim = 8))\nmodelo_keras.add(Dense(units = 5, activation = 'relu'))\nmodelo_keras.add(Dense(units = 1, activation = 'sigmoid'))\n\noptimizers = ['sgd', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nlosses = ['huber_loss',  'binary_crossentropy', 'kullback_leibler_divergence', 'poisson', 'cosine_proximity']\n\nresultados = {}\nfor opt in optimizers:\n    for loss in losses:\n        modelo_keras.compile(optimizer = opt, loss = loss, metrics = ['accuracy'])\n        modelo_keras.fit(X_train, y_train, batch_size=10, epochs = 100)\n        p_best_keras = modelo_keras.predict(X_valid)\n        p_best_keras = (p_best_keras > 0.55)\n        precisao_cross = accuracy_score(y_valid, p_best_keras)\n        resultados[opt, loss] = precisao_cross\nprint(resultados)\n\n\n","02668557":"resultados","8c61cea2":"# applying keras without scaler\n\nmodelo_teste_valores = {}\nfor i in np.arange(0.4, 0.9, 0.05):\n    modelo_keras.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    modelo_keras.fit(X_train, y_train, batch_size=10, epochs = 100)\n    p_best_keras = modelo_keras.predict(X_valid)\n    p_best_keras = (p_best_keras > i)\n    previsao = accuracy_score(y_valid, p_best_keras)\n    modelo_teste_valores[i] = previsao\nprint(modelo_teste_valores)\nplt.plot(modelo_teste_valores.keys(), modelo_teste_valores.values())","7367297d":"modelo_keras.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodelo_keras.fit(X, y, batch_size=10, epochs = 100)\np_best_keras = modelo_keras.predict(X_teste)\np_best_keras = (p_best_keras > 0.55)","2adcf4be":"p_best_keras = [1 if i == True else 0 for i in p_best_keras]\n","58d6a008":"np.sum(p_best_keras)","909104e9":"keras_submission_best = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived': p_best_keras})","6a6ed4e4":"keras_submission_best.to_csv('keras_best.csv', index = False)","42ee8baf":"### Working with pipeline\nsteps = [('scaler', StandardScaler()), ('GBC', GradientBoostingClassifier())]\npipeline = Pipeline(steps)","87256099":"parameters = {'GBC__max_depth':[1, 2, 3, 4], 'GBC__learning_rate': [0.01, 0.001, 0.0001], 'GBC__n_estimators': [100, 200, 300] }","6170a177":"grid = GridSearchCV(pipeline, parameters, cv = 5)","1d239d3b":"grid.fit(X_train, y_train)","538da81d":"grid.score(X_train, y_train)","23ccb11f":"grid.best_params_","39c78a8b":"best = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=300)","a8b251b1":"best.fit(X, y)","8bf2c32d":"grid_predict = best.predict(X_teste)","25a4614a":"grid_predict_series = pd.DataFrame({'PassengerId': df_teste['PassengerId'], 'Survived':grid_predict})","91534f04":"grid_predict_series.to_csv('grid_predict_GBC_best.csv', index = False)","1e764313":"**Getting predictions**","2c3e63c9":"##Result: 0.73684","6230857f":"**Lets break the graph down by Gender**","909f78d0":"Starting Analisys","f18901bc":"Best Combination is  {'activation': 'relu', 'solver': 'adam'}","df6fd444":"**Visualization Approach**"}}