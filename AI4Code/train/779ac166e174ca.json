{"cell_type":{"f9d05868":"code","1dbb2b85":"code","6651d58b":"code","e35d1970":"code","e54e87a5":"code","fa6f6bb7":"code","77578dc9":"code","dd864380":"code","9ecbd9c0":"code","41bebeca":"code","e30a4da0":"code","fe165955":"code","fb46c86b":"code","76be5ee8":"code","ce50cb5b":"code","7aa9bb49":"code","8736b107":"code","25065a34":"code","ec7ada54":"code","b9a12b1a":"code","23d66346":"code","ed0a3330":"code","63bfcfa9":"code","f78ccb5f":"code","caea26e2":"code","bc6202ee":"code","756cf322":"code","9eb340b0":"code","14a99f56":"code","1ed653d2":"code","f4b39a78":"code","341017e7":"code","aa12ae2e":"code","c1e0edcf":"code","f6e491af":"code","d3f2d952":"code","a6988fbe":"code","2647d819":"code","e40faaec":"code","2bdd6bcb":"code","4c3daa4e":"code","a14e347a":"code","b09c5011":"code","f10ae14c":"code","a7fcb366":"code","61e3c643":"markdown","4b2eed28":"markdown","e5b070a9":"markdown","47719804":"markdown","ad32c2fd":"markdown","6aba6cbd":"markdown","1e76f078":"markdown","a180a0d4":"markdown","dab6a892":"markdown","60090b3b":"markdown","12642ec5":"markdown","0c385c8c":"markdown","d3f83aa3":"markdown","3c3e1bab":"markdown","f04c1ca8":"markdown","c887bfc9":"markdown","992193f7":"markdown","0b676888":"markdown","6ffbbef7":"markdown","c6eb6986":"markdown","8a0f0e0d":"markdown","4d71f4e9":"markdown","a450da22":"markdown","5517f968":"markdown","a93f1942":"markdown","a2ae8e37":"markdown","3c5474e8":"markdown","a5a87ad0":"markdown","31f1caa2":"markdown","e0d48dc7":"markdown","af7c8535":"markdown","2765615e":"markdown","2ac204f9":"markdown","832a702d":"markdown","938823db":"markdown"},"source":{"f9d05868":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1dbb2b85":"fake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake['flag'] = 0\nfake","6651d58b":"true = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\ntrue['flag'] = 1\ntrue","e35d1970":"df = pd.DataFrame()\ndf = true.append(fake)","e54e87a5":"df.info()","fa6f6bb7":"df = df.drop_duplicates()\ndf = df.reset_index(drop=True)","77578dc9":"# Correcting some data\ndf['date'] = df['date'].replace(['19-Feb-18'],'February 19, 2018')\ndf['date'] = df['date'].replace(['18-Feb-18'],'February 18, 2018')\ndf['date'] = df['date'].replace(['17-Feb-18'],'February 17, 2018')\ndf['date'] = df['date'].replace(['16-Feb-18'],'February 16, 2018')\ndf['date'] = df['date'].replace(['15-Feb-18'],'February 15, 2018')\ndf['date'] = df['date'].replace(['14-Feb-18'],'February 14, 2018')\ndf['date'] = df['date'].replace(['13-Feb-18'],'February 13, 2018')\n\n\ndf['date'] = df['date'].str.replace('Dec ', 'December ')\ndf['date'] = df['date'].str.replace('Nov ', 'November ')\ndf['date'] = df['date'].str.replace('Oct ', 'October ')\ndf['date'] = df['date'].str.replace('Sep ', 'September ')\ndf['date'] = df['date'].str.replace('Aug ', 'August ')\ndf['date'] = df['date'].str.replace('Jul ', 'July ')\ndf['date'] = df['date'].str.replace('Jun ', 'June ')\ndf['date'] = df['date'].str.replace('Apr ', 'April ')\ndf['date'] = df['date'].str.replace('Mar ', 'March ')\ndf['date'] = df['date'].str.replace('Feb ', 'February ')\ndf['date'] = df['date'].str.replace('Jan ', 'January ')","dd864380":"df['date'] = df['date'].str.replace(' ', '')","9ecbd9c0":"for i, val in enumerate(df['date']):\n    df['date'].iloc[i] = pd.to_datetime(df['date'].iloc[i], format='%B%d,%Y', errors='coerce') # by setting the parameter to \"coerce\", we will set unappropriate values to NaT (null)","41bebeca":"df['date'] = df['date'].astype('datetime64[ns]')","e30a4da0":"df.info()","fe165955":"import datetime as dt\ndf['year'] = pd.to_datetime(df['date']).dt.to_period('Y')\ndf['month'] = pd.to_datetime(df['date']).dt.to_period('M')\n\ndf['month'] = df['month'].astype(str)","fb46c86b":"sub = df[['month', 'flag']]\nsub = sub.dropna()\nsub = sub.groupby(['month'])['flag'].sum()","76be5ee8":"sub = sub.drop('NaT')","ce50cb5b":"import matplotlib.pyplot as plt\n\nplt.suptitle('Dynamics of fake news')\nplt.xticks(rotation=90)\nplt.ylabel('Number of fake news')\nplt.xlabel('Month-Year')\nplt.plot(sub.index, sub.values, linewidth=2, color='green')","7aa9bb49":"sub2 = df[['subject', 'flag']]\nsub2 = sub2.dropna()\nsub2 = sub2.groupby(['subject'])['flag'].sum()","8736b107":"plt.suptitle('Fake news among different categories')\nplt.xticks(rotation=90)\nplt.ylabel('Number of fake news')\nplt.xlabel('Category')\n\nplt.bar(sub2.index, height=sub2.values, color='green')\n#ax1.plot(x, y)\n#ax2.plot(x, -y)","25065a34":"nlp = df","ec7ada54":"#nlp['title'] = nlp['title'] + ' ' + nlp['subject']","b9a12b1a":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ncorpus = nlp[nlp['flag'] == 1]['title'].iloc[0:500] # We will take a slice of fake news, to see what vocabulary there looks like\ntfidf1 = TfidfVectorizer()\nvecs = tfidf1.fit_transform(corpus)\n\nfeature_names = tfidf1.get_feature_names()\ndense = vecs.todense()\nlist_words = dense.tolist()\ndf_words = pd.DataFrame(list_words, columns=feature_names)","23d66346":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\ndf_words.T.sum(axis=1)\nCloud = WordCloud(background_color=\"white\", max_words=100).generate_from_frequencies(df_words.T.sum(axis=1))","ed0a3330":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,5))\nplt.imshow(Cloud, interpolation='bilinear')","63bfcfa9":"import nltk\nnltk.download('punkt')\nfrom nltk import word_tokenize\n\nnlp['title'] = nlp['title'].apply(lambda x: word_tokenize(str(x)))","f78ccb5f":"from nltk.stem import SnowballStemmer\n\nsnowball = SnowballStemmer(language='english')\nnlp['title'] = nlp['title'].apply(lambda x: [snowball.stem(y) for y in x])","caea26e2":"nlp['title'] = nlp['title'].apply(lambda x: ' '.join(x))","bc6202ee":"from nltk.corpus import stopwords \n\nnltk.download('words')\nnltk.download('stopwords')\nstopwords = stopwords.words('english')","756cf322":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\nX_text = tfidf.fit_transform(nlp['title'])","9eb340b0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_text, nlp['flag'], test_size=0.33, random_state=1)","14a99f56":"scores = {}","1ed653d2":"from sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nclf = LinearSVC(max_iter=100, C=1.0)\nclf.fit(X_train, y_train)\n\ny_pred_SVM = clf.predict(X_test)\nprint(cross_val_score(clf, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_SVM, y_test))\n\nscores['LinearSVC'] = accuracy_score(y_pred_SVM, y_test)\n","f4b39a78":"from sklearn.naive_bayes import MultinomialNB\n\nclf2 = MultinomialNB()\nclf2.fit(X_train, y_train)\n\ny_pred_MNB = clf2.predict(X_test)\nprint(cross_val_score(clf2, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_MNB, y_test))\n\nscores['MultinomialNB'] = accuracy_score(y_pred_MNB, y_test)","341017e7":"from xgboost import XGBClassifier\n\nclf3 = XGBClassifier(eval_metric='rmse', use_label_encoder=False)\nclf3.fit(X_train, y_train)\n\ny_pred_XGB = clf3.predict(X_test)\nprint(cross_val_score(clf3, X_text, nlp['flag'], cv=3))\nprint(accuracy_score(y_pred_XGB, y_test))\n\nscores['XGB'] = accuracy_score(y_pred_XGB, y_test)","aa12ae2e":"!pip install pycaret","c1e0edcf":"from pycaret.nlp import *\n\ncaret_nlp = setup(data=nlp, target='title', session_id=1)","f6e491af":"lda = create_model('lda')","d3f2d952":"lda_data = assign_model(lda)","a6988fbe":"lda_data","2647d819":"from catboost import CatBoostClassifier","e40faaec":"input_cat = lda_data.drop(['text','date','Perc_Dominant_Topic','flag','year'], axis=1)\ninput_cat['month'] = input_cat['month'].astype(str)\ntarget_cat = lda_data['flag']","2bdd6bcb":"from sklearn.model_selection import train_test_split\nX_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(input_cat, target_cat, test_size=0.33, random_state=1)","4c3daa4e":"clf4 = CatBoostClassifier(iterations=1000, \n                          cat_features=['title','subject','Dominant_Topic','month']\n                         )","a14e347a":"clf4.fit(X_train_cat, y_train_cat, early_stopping_rounds=10)","b09c5011":"scores['CatBoost'] = clf4.score(X_test_cat, y_test_cat)","f10ae14c":"scores","a7fcb366":"plt.bar(scores.keys(), scores.values())","61e3c643":"**Setting up the model which will implement all traditional NLP-preprocessing operation (tokenizing, lemmatizing etc.**\n\n**The PyCaret is almost fully automatic!**","4b2eed28":"## 4.1 Linear SVC","e5b070a9":"**We can see that the date format is not the one we need. I will apply the appropriate date format for future purposes.**","47719804":"# 4. Model building","ad32c2fd":"**First, I will tokenize words to pass it on to the SnowballStemmer method, which will take out lemmas from words.**","6aba6cbd":"**Next we will try to elicit insights from non-text features to get to know if they will help us boost the Text Classifier.**","1e76f078":"## 4.2 Naive Bayes","a180a0d4":"**Here's the outcome dataset:**","dab6a892":"## 4.3 XGBoost","60090b3b":"## Subject distribution","12642ec5":"**I will use several approaches to solve the classification task, such as:**\n\n1) Traditional (which are known as efficient for text classification):\n\n    1.1) SVM\n    1.2) Naive Bayes\n    1.3) XGBoost\n    \n2) Not-very-traditional (Experimental): PyCaret NLP toolkit (I will apply unsupervised model to generate features which I will in turn pass on to the supervised model)","0c385c8c":"<h3 align=\"center\"><font size=\"15\"><b>Fake News detection<\/b><\/font><\/h3> \n\n<img src=\"https:\/\/www.txstate.edu\/cache78a0c25d34508c9d84822109499dee61\/imagehandler\/scaler\/gato-docs.its.txstate.edu\/jcr:21b3e33f-31c9-4273-aeb0-5b5886f8bcc4\/fake-fact.jpg?mode=fit&width=1600\" height=200 width=400>\n\n<br><\/br>\n\n**Task type:** Classification\n\n**Models used:** LinearSVC, MultinomialNB, XGBoost, PyCaret, CatBoost\n\n**Tools used:** NLP preprocessing tools, semi-supervised learning technique, new feature engineering, Word Cloud","d3f83aa3":"## 3.1 Word Cloud visualization","3c3e1bab":"**Let's check the datatypes.**","f04c1ca8":"# 1. Loading data","c887bfc9":"**And finally TfidfVectorizing. You can also take CountVectorizer, but I prefer Tfidf as it has masses of advantages.**","992193f7":"**Take the standard english bag of stopwords from nltk.**","0b676888":"**LDA stands for Latent Dirichlet Allocation and is widely used in unsupervised learning tasks.**\n\n*Read more:* https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation","6ffbbef7":"**We'll utilize the 'Topic' features generated by PyCaret.**","c6eb6986":"**I will add the 'subject' feature to the title field as it might have an influence on the outcome of classification.**","8a0f0e0d":"## 4.4 PyCaret + CatBoost","4d71f4e9":"**What a spike in the dynamics of fake news in late 2017!**","a450da22":"# 3. Text preparation","5517f968":"## Fake news dynamics","a93f1942":"## 3.2 Tfidf-vectorizing","a2ae8e37":"**Removing the duplicates and preventing problems with indexing.**","3c5474e8":"**Okay, this model performs a little worse, but still very good.**","a5a87ad0":"**This looks suspiciously good, but lets try another algorithm.**","31f1caa2":"**Here I am going to take one example and try visualize tfidf as a wordcloud.**","e0d48dc7":"**As we have discovered, such features as**\n* subject\n* date\n\n**might be also crucial for the algorithm to decide whether the piece of news is fake or real. We will try to include them in the model.**","af7c8535":"# 5. Conclusion\n\n**We have trained & tested 4 models for NLP task (implementing the traditional NLP preprocessing strategies). They all perform very good, however this is most likely due to the high correlation of the target other categorical features (such as 'subject'). If we did not add it to analysis, the result could have been totally different.**\n\n**We also used a combination of supervised & unsupervised learning, which can be an interesting method to use.**\n\n**Also, for text classification tasks I recommend using BERT models and DNN.**\n\n*For more information on this and code snippets, read here:* https:\/\/medium.com\/engineering-zemoso\/text-classification-bert-vs-dnn-b226497c9de7\n\n<font color='blue'><b>Thank you for your attention!<\/b><br><\/br><br><\/br>\nYour comments and discussion contributions are always welcome.<\/font>","2765615e":"**An important step in every NLP-task is to get the roots of words in order not to distract the model by 'different' words.**","2ac204f9":"# 2. EDA + Data cleaning","832a702d":"**PyCaret\u2019s Natural Language Processing module is an unsupervised machine learning module that can be used for analyzing text data by creating topic models that can find hidden semantic structures  within documents. PyCaret\u2019s NLP module comes with a wide range of text pre-processing techniques. It has over 5 ready-to-use algorithms and several plots to analyze the performance of trained models and text corpus.**\n\n*Read more:* https:\/\/pycaret.org\/nlp\/","938823db":"**Indeed, looks definitely like fake news :)**\n\n**And we can also see out 'subject' feature in the foreground as it has been added manually in every title. Therefore, out vectorizer considers it as an important & frequent word.**"}}