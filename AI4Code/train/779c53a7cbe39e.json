{"cell_type":{"c10d9867":"code","892d799f":"code","98cbfd1d":"code","b3bb8c65":"code","d347c90b":"code","29fa4eba":"code","f4dc2ca1":"code","2714efbb":"code","cf662559":"code","7c51b567":"code","321659c3":"code","1675aba6":"code","8b29ccba":"code","16778bfc":"code","34f4489c":"code","dd5eeeea":"code","cc6265b5":"code","844cd93d":"code","3980df62":"code","2e97964a":"code","e6cb65e1":"code","0fbdf9b5":"code","9725ed86":"code","01cfbd34":"code","6f899b95":"code","4a82943e":"code","ce4a2484":"code","87e48df9":"code","3a623cbd":"code","9bbcb075":"code","60d3a26c":"code","1da638a8":"code","4b56d9f5":"code","816d7ed4":"code","8fbeeb3c":"code","2351411f":"code","e8c9ccf3":"code","b06dab15":"code","dac7665c":"code","a4266db9":"code","8afbf6e7":"code","6935d2ae":"code","b4e53465":"code","b833b18c":"code","94faa00d":"code","8b159e8a":"code","5f5d11fb":"code","c2e341a8":"code","be45dbca":"code","95e1d477":"code","89295336":"markdown","49ee004b":"markdown","c32e7351":"markdown","2146c147":"markdown","a376fa6e":"markdown","ef354d1f":"markdown","620e418d":"markdown"},"source":{"c10d9867":"import gc\ngc.collect()\n\nimport pandas as pd\nimport numpy as np\n # Manually Annotated dataset from the above mentioned paper\ndf0 = pd.read_csv('..\/input\/dataset-for-french\/profilesmanualannotation.csv') ","892d799f":"import collections\ndf1 = df0[['UserId', 'party']] #Trimming down the first dataset\nfr = pd.read_csv('..\/input\/annotatedfriends\/manualannotationFriends.csv', names=['id', 'friend']) #Dataset of Friends\nfr.count","98cbfd1d":"import matplotlib.pyplot as plt\nGroupedData = fr[['friend', 'id']].groupby(['friend']).count().sort_values(['id'], ascending=False)\nchunkSize = int(GroupedData.shape[0]\/1000)\n","b3bb8c65":"counter = 0\naverageCounte = 0\nSum = 0\ntop = pd.DataFrame(columns=['Percentage', 'Number of Followers'])\nfor i in GroupedData['id']:\n    counter = counter + 1\n    Sum = Sum + i\n    if (counter >= chunkSize):\n        counter = 0\n        averageCounte = averageCounte + 0.1\n        average = Sum\/chunkSize\n        top = top.append({'Percentage':averageCounte, 'Number of Followers':average}, ignore_index=True)\n        Sum = 0\n        if average < 10:\n            break\n   ","d347c90b":"ax = top.plot(x='Percentage', y='Number of Followers', figsize = (10, 10))\nax.axhline(y=70, linewidth=0.5, color='r')\nax.set_xlabel('Percentage of Friends')","29fa4eba":"GroupedData.reset_index(inplace = True)\nprint(GroupedData.shape)\nGroupedData = GroupedData.nlargest(int(GroupedData['friend'].shape[0]*0.0001), 'id', keep='first')\nGroupedData.shape","f4dc2ca1":"print(GroupedData)","2714efbb":"fr = fr[fr['friend'].isin(GroupedData['friend'])]","cf662559":"#Graph = nx.from_pandas_edgelist(fr, source = 'id', target = 'friend', edge_attr=None,  create_using=nx.DiGraph())\n#print(Graph.size())\n#nx.draw(Graph, node_size = 5)","7c51b567":"#fr = fr.head(smallerTest)\nDicList = []\nfor group, frame in fr.groupby('id'):\n    ak = frame['friend'].tolist()\n    dictOf = dict.fromkeys(ak , 1)\n    DicList.append(dictOf)","321659c3":"from sklearn.feature_extraction import DictVectorizer\ndictvectorizer = DictVectorizer(sparse = True)\nfeatures = dictvectorizer.fit_transform(DicList)\nfeatures.todense().shape","1675aba6":"df1['party'].fillna(0, inplace = True)","8b29ccba":"parties = {'fi': 1,'ps': 1,'em': 2,'lr': 2,'fn': 2,'fi\/ps': 4,'fi\/em': 4, 'fi\/lr': 4,'fi\/fn': 5, 'ps\/em': 6,\n'ps\/lr': 3, 'ps\/fn': 4, 'em\/lr': 4,'em\/fn': 6, 'lr\/fn': 6}\n#print(df1['party'])\n\ndf1['party'] = df1['party'].map(parties)\n\n#print(labels)\nprint(features.shape)","16778bfc":"#dataFrame = pd.SparseDataFrame(features, columns = dictvectorizer.get_feature_names(), \n#                               index = fr['id'].unique())\ndataFrame = pd.DataFrame.sparse.from_spmatrix(features, columns = dictvectorizer.get_feature_names(), \n                               index = fr['id'].unique())","34f4489c":"df1 = df1.set_index('UserId')\ndf1.index","dd5eeeea":"dataFrame.index.names = ['UserId']","cc6265b5":"#dataFrame.reset_index()\n#df1","844cd93d":"#df1['UserId'] = df1['UserId'].astype(int)\n#dataFrame.reset_index()\n#dataFrame\n#print(dataFrame.index.type())\n\n#df1.set_index('UserId')\n#print(df1.index.type())\ndataFrame = dataFrame.join(df1, how='inner')\n","3980df62":"print(dataFrame['party'])","2e97964a":"\ndataFrame = dataFrame[(dataFrame['party']==1.0) | (dataFrame['party']==2.0)] \n                      #|(dataFrame['party']== 4.0) | (dataFrame['party']==5.0)]\n#dataFrame","e6cb65e1":"dataFrame['party'][dataFrame['party'] == 2].count()","0fbdf9b5":"from sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\ndataFrame.fillna(0, inplace = True)\nfeatureSelector = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=0)\nfeatureSelector.fit(dataFrame.iloc[:, :-1], dataFrame['party'])","9725ed86":"model = SelectFromModel(featureSelector, prefit=True, max_features= 500)\nX_new = model.transform(dataFrame.iloc[:, :-1].to_dense())\nprint(X_new.shape)\nfeatureNames = []\nfor i in model.get_support(indices = True):\n    featureNames.append(dataFrame.columns[i])\ndataFrame2 = pd.DataFrame.from_dict(X_new)","01cfbd34":"#print(featureNames)\nimportancesOfAllFeatures = (sorted(zip(map(lambda x: round(x, 4), featureSelector.feature_importances_), dataFrame.columns), \n             reverse=True))\nprint(importancesOfAllFeatures[:10])","6f899b95":"dataFrame2.index = dataFrame.index\ndataFrame['party'] = dataFrame['party'].astype(float)\ndataFrame2['party'] = dataFrame['party']\ndataFrame2","4a82943e":"#dataFrame['party'] = labels.head(dataFrame.shape[0]).values #This is the problem\nfrom sklearn.model_selection import train_test_split\ndataFrame2.fillna(0, inplace = True)\ntrain, test = train_test_split(dataFrame2, test_size=0.1, shuffle = True)","ce4a2484":"test.shape","87e48df9":"from sklearn.neural_network import MLPClassifier\n#clf =  MLPClassifier(solver='lbfgs', alpha=50, hidden_layer_sizes=(100), random_state=1)\n#from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier( solver='lbfgs', alpha=1e-5, random_state=1, max_iter=500)\nclf.fit(train.iloc[:, :-1].to_dense(), train['party'].to_dense())\n#clf.fit(train.iloc[:, :-1].to_dense(), train['party'].to_dense())\n","3a623cbd":"print(clf.score(test.iloc[:, :-1], test['party'].to_dense()))\nprint(clf.score(train.iloc[:, :-1].to_dense(), train['party'].to_dense()))","9bbcb075":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import expon\nimport scipy.stats\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n#\"solver\": ['lbfgs', 'sgd', 'adam'],\nparam_dist = { \"solver\": ['lbfgs', 'sgd', 'adam'],\n               \"hidden_layer_sizes\": [(100), (100, 100), (50)],\n              \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n              \"alpha\": scipy.stats.expon(scale=.1),}\n\nn_iter_search = 20\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, cv=5, iid=False)\nrandom_search.fit(train.iloc[:, :-1].to_dense(), train['party'].to_dense())","60d3a26c":"# Utility function to report best scores\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","1da638a8":"report(random_search.cv_results_)","4b56d9f5":"from sklearn.ensemble import RandomForestClassifier\nclf2 = RandomForestClassifier(n_estimators=100, max_depth=50, random_state=0)\nclf2.fit(train.iloc[:, :-1].to_dense(), train['party'].to_dense())","816d7ed4":"print(clf2.score(test.iloc[:, :-1], test['party'].to_dense() ))\nprint(clf2.score(train.iloc[:, :-1].to_dense(), train['party'].to_dense()))","8fbeeb3c":"\n#import sys\n#np.set_printoptions(threshold=sys.maxsize)\n#print(clf2.feature_importances_)","2351411f":"test","e8c9ccf3":"train.iloc[:, :-1]","b06dab15":"from sklearn import linear_model\nclf3 = linear_model.Lasso(alpha=.00001, max_iter=1500)\n#clf3.fit(train.iloc[:, :-1], train['party'])\n#print(clf3.score(train.iloc[:, :-1].to_dense(), train['party'].to_dense()))\n","dac7665c":"#clf3.score(test.iloc[:, :-1], test['party'])","a4266db9":"def get_score(model, X_train, Y_train, X_test, Y_test):\n        model.fit(X_train, Y_train)\n        return model.score(X_test, Y_test)\n    ","8afbf6e7":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nskf = StratifiedKFold(n_splits=4)\nfor i in [clf, clf2, clf3]:\n    scores = cross_val_score(i, dataFrame.iloc[:, :-1].to_dense(), dataFrame['party'].to_dense(), cv=5)\n    print(i)\n    print(np.mean(scores))","6935d2ae":"print(train[train['party'] == None].size)\n#from sklearn import linear_model\n#clf3 = linear_model.Lasso(alpha=1)\n#clf3.fit(train.iloc[:, :-1], train['party'].fillna(0))","b4e53465":"from sklearn.metrics import confusion_matrix\ny_pred = []\n#print(np.transpose(test.iloc[i, :-1].reset_index().values.shape))\n\nfor i in range(0, test[0].count()):\n #'model' is the name of classifier from keras   \n    y_pred.append(model.predict(np.transpose(test.iloc[i, :-1].reset_index().values)))\n","b833b18c":"predicted_y = []\nfor i in y_pred:\n    predicted_y.append(i[1])\nprint(predicted_y)\ncm = confusion_matrix(test['party'].to_dense(), predicted_y)","94faa00d":"from sklearn.utils.multiclass import unique_labels\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","8b159e8a":"\n#print(len(predicted_y))\nrealValues = []\ntest['party'].fillna(0, inplace = True)\nclass_names = [0, 1,2,3,4,5,6,7,8,9,10,11,12,13,15,16]\nclass_names = np.asarray(class_names)\n#class_names = test['party'].unique()\n#class_names = np.append(class_names , [0])\nfor i in test['party']:\n    realValues.append(int(i))\n\n#print(test.shape)\n#print(test['party'])\n#print(test['party'].tolist())\nplot_confusion_matrix(realValues, predicted_y, classes=class_names,\n                      title='Confusion matrix, without normalization')","5f5d11fb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential()\nmodel.add(Dense(100, activation='relu', input_dim=35220)) #input shape of 50\nmodel.add(Dense(64, activation='relu')) #input shape of 50\nmodel.add(Dense(28, activation='softmax'))\n\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(train.iloc[:, :-1], train['party'], epochs=8)","c2e341a8":"#model.summary()\nmodel.evaluate(test.iloc[:, :-1], test['party'])","be45dbca":"from sklearn.dummy import DummyClassifier\nfor strategy in ['stratified', 'most_frequent', 'prior', 'uniform']:\n    dummy = DummyClassifier(strategy=strategy)\n    dummy.fit(train.iloc[:, :-1].to_dense(), train['party'].to_dense())\n    print(dummy.score(test.iloc[:, :-1].to_dense(), test['party'].to_dense()))","95e1d477":"fr['id'].nunique()\nfr['friend'].count()\n\n#fr['']","89295336":"As seen from the above diagram, only top 5 percent of the friends have more than than 10 followers among the manually annotated dataset that we named df0. It would therefore make sense to only take top 5 percent of the friends profiles in terms of their frequency and consider them to be features in political alligiance as taking more would unnecessarily increase the size of the features.","49ee004b":"As seen above we have multiple featured manually annotated data using multiple annotaters. The field that we are most interested in is the 'UserId' and the 'Party'. We will therefore ignore the other columns and focus on these two. In the party affiliations, the annotators have used the following codes for annotation.\n\n* 'FI' for La France Insoumise [Unbowed France], led by Jean-Luc Melenchon. \u00b4\n\n* 'PS' for Le Parti Socialiste [Socialist Party], led by Beno\u02c6\u0131t Hamon. \n\n* 'EM' for En Marche ! [Forward! or Working!], led by Emmanuel Macron.\n\n* 'LR' for Les Republicains \u00b4 [Republicans], led by Franc\u00b8ois Fillon.\n\n* 'FN' for Le Front National [National Front], led by Marine Le Pen\n\nAs visible from the data there are also some columns in the 'Party' column which are using dual affiliation. These rows represent the users who have dual affiliations or switch between the candidates. For this model i will also keep the users with 'Nan' in party column as it represents unclear affiliation which can be a possibly predicted value. Lets also explore the second dataset of 'Friends' created from getting twitter Friends of the profiles that are present in the above-mentioned manually annotated dataset.","c32e7351":"****Cross Validation****\n\nHere we will do k-fold cross validation and model evaluation to see which model is performing better ","2146c147":"**Keras Model**\n","a376fa6e":"The aim of this notebook is to create a machine learning model that will predict the political affiliation of French twitter users. As training data we will use the manualy annotated twitter profiles from following source '[#\u00c9lys\u00e9e2017fr: The 2017 French Presidential Campaign on Twitte](https:\/\/www.aaai.org\/ocs\/index.php\/ICWSM\/ICWSM18\/paper\/view\/17821)'. As for features, we will only consider the friends of each of the 22853 profiles that are present in the paper. Lets start by looking at the data-sets:","ef354d1f":"**Feature Selection**\n\nI will be using random forest model to select the most relevant features from the data-set","620e418d":" As there are more then 2.5 million profiles in this table and creating a feature list with it will be not possible. We therefore need to identify the most important profiles among there 2.5 million profiles and only keep them for our analysis. It would make sense to use in-degree from the manually annotated profiles to select these profiles. Plotting the degree might help in finding out how many of these friend of manually annotated profiles should be taken as features. "}}