{"cell_type":{"3c0da63c":"code","ee0f0896":"code","d87f2c20":"code","c7b81b4e":"code","bc7b9750":"code","7ee1e624":"code","46d0b3eb":"markdown","4e7a9a8e":"markdown","67b6e3cc":"markdown","d5a185c9":"markdown","368fae75":"markdown","bc9f7146":"markdown","9cf12c26":"markdown"},"source":{"3c0da63c":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.utils import to_categorical\n\nimport pandas as pd\nimport numpy as np","ee0f0896":"#  DEFINE CONSTANTS\n\nINPUT_SHAPE = 784\nNUM_CATEGORIES = 10\n\nLABEL_DICT = {\n 0: \"T-shirt\/top\",\n 1: \"Trouser\",\n 2: \"Pullover\",\n 3: \"Dress\",\n 4: \"Coat\",\n 5: \"Sandal\",\n 6: \"Shirt\",\n 7: \"Sneaker\",\n 8: \"Bag\",\n 9: \"Ankle boot\"\n}\n\n# LOAD THE RAW DATA\ntrain_raw = pd.read_csv('..\/input\/fashion-mnist_train.csv').values\ntest_raw = pd.read_csv('..\/input\/fashion-mnist_test.csv').values","d87f2c20":"# split into X and Y, after one-hot encoding\ntrain_x, train_y = (train_raw[:,1:], to_categorical(train_raw[:,0], num_classes = NUM_CATEGORIES))\ntest_x, test_y = (test_raw[:,1:], to_categorical(test_raw[:,0], num_classes = NUM_CATEGORIES))\n\n# normalize the x data\ntrain_x = train_x \/ 255\ntest_x = test_x \/ 255","c7b81b4e":"# BUILD THE MODEL\nmodel = Sequential()\n\nmodel.add(Dense(512, input_dim = INPUT_SHAPE))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(NUM_CATEGORIES))\nmodel.add(Activation('softmax'))\n\n# compile it - categorical crossentropy is for multiple choice classification\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","bc7b9750":"# train the model!\nmodel.fit(train_x,\n          train_y,\n          epochs = 8,\n          batch_size = 32,\n          validation_data = (test_x, test_y))","7ee1e624":"# how'd the model do?\nmodel.evaluate(train_x, train_y)","46d0b3eb":"Finally, the training. We tell it to use our `train_x` and `train_y` as our training data, `test_x` and `test_y` to validate, use 32 samples per training pass, and run through the whole dataset 8 times.","4e7a9a8e":"First up: importing modules. This model just feeds forwards, so we can use a `Sequential` class. As for the layers themselves, we're only using `Dense` and `Activation`. Nothing fancy.","67b6e3cc":"Next some constants. `INPUT_SHAPE` is 784 (28 x 28 - flattened form of the image), and `NUM_CATEGORIES` is 10. All fairly self explanatory.  At the bottom, we use `pd.read_csv` to pull in our data, and we grab the `values` property, which is a numpy array version of the `DataFrame` we just read in.","d5a185c9":"Next, we split the import data into training and testing data (as well as X and Y). Any \"x\" variable is an input, while \"y\" is the expected output. We set train and test x to everything but the first column of data in our input data (hence the slice), and use Keras' `to_categorical` to one-hot encode the output label to a vector of length `NUM_CATEGORIES` (10). We then normalize the X data. We change the range from 0 - 255 to 0 - 1 by dividing by 255","368fae75":"Nice! The first parameter is loss, while the second parameter is accuracy. 90%. Yay!\n\n**Resources:**\n[Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist),\n[Keras](https:\/\/keras.io)","bc9f7146":"Now for the fun part - defining our model! In this case it's a simple four layer network - an input shape of `INPUT_SHAPE` (784), three 512 neuron layers, and an output layer with `NUM_CATEGORIES` neurons (10). We use categorical crossentroy as our loss, as we've got a multi-class classification problem. For an activation function, we use ReLU all the way, except for the output layer, which uses softmax.","9cf12c26":"**Fashion MNIST - A Multilayer Perceptron**\n\nHello Internet! In this kernel, we'll make a simple neural network that gets ~90% accuracy on the Fashion MNIST dataset (a ten class, 28x28 image classification problem)."}}