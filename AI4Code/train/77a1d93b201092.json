{"cell_type":{"9156996b":"code","f270024a":"code","23eba156":"code","021f9e12":"code","c22a9115":"code","627c717a":"code","89379a22":"code","5efaf8c7":"code","da1dd274":"code","df5821b0":"code","c1654f02":"code","5bec944b":"code","12a5a2f0":"code","65981f9b":"code","f35bcbcb":"code","9bf536a2":"code","101c892c":"code","8f05c0d1":"code","c8007aaa":"code","2e519656":"markdown","9ab60262":"markdown","a55e9e67":"markdown","43074f86":"markdown"},"source":{"9156996b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\n\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nimport category_encoders as ce\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f270024a":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'","23eba156":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","021f9e12":"train['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)","c22a9115":"dprint('Clean features...')\ncols = ['dependency']\nfor c in tqdm(cols):\n    x = df_all[c].values\n    strs = []\n    for i, v in enumerate(x):\n        try:\n            val = float(v)\n        except:\n            strs.append(v)\n            val = np.nan\n        x[i] = val\n    strs = np.unique(strs)\n\n    for s in strs:\n        df_all[c + '_' + s] = df_all[c].apply(lambda x: 1 if x == s else 0)\n\n    df_all[c] = x\n    df_all[c] = df_all[c].astype(float)\ndprint(\"Done.\")","627c717a":"dprint(\"Extracting features...\")\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)\ndprint(\"Done.\")         ","89379a22":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])","5efaf8c7":"dprint(\"Encoding Data....\")\nencode_data(train)\nencode_data(test)\ndprint(\"Done...\")","da1dd274":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df","df5821b0":"dprint(\"Do_feature Engineering....\")\ntrain = do_features(train)\ntest = do_features(test)\ndprint(\"Done....\")","c1654f02":"dprint(\"Fill Na value....\")\ntrain = train.fillna(0)\ntest = test.fillna(0)\ndprint(\"Done....\")","5bec944b":"train.shape,test.shape","12a5a2f0":"cols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train[target_name].values","65981f9b":"X.shape,test.shape,y.shape","f35bcbcb":"import lightgbm as lgb\ngc.collect()","9bf536a2":"%%time\nsvm_model = SVC(kernel='rbf', gamma=0.8, C=12)\nfit = svm_model.fit(X, y)\npred = fit.predict(test)\nsub =  pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"Target\"] = pred\nsub.to_csv(\"submission_svm.csv\" ,index=False)","101c892c":"%%time\nxgb_model = xgb.XGBClassifier(learning_rate= 0.1, n_estimators= 1000, max_depth= 5, min_child_weight= 1, gamma= 0, \n                              subsample= 0.9, colsample_bytree= 0.8, objective= \"multi:softmax\", scale_pos_weight= 1, \n                              eval_metric= \"merror\", silent= 1, verbose= False, num_class= 5, seed= 27)\nfit = xgb_model.fit(X, y)\npred = fit.predict(test)\nsub =  pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"Target\"] = pred\nsub.to_csv(\"submission_xgb.csv\" ,index=False)","8f05c0d1":"%%time\nlgbm_model = lgb.LGBMClassifier(\n        n_estimators=2000,\n        learning_rate=0.1,\n        num_leaves=123,\n        colsample_bytree=.8,\n        subsample=.7,\n        max_depth=15,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        scale_pos_weight=5,\n    )\nfit = lgbm_model.fit(X, y)\npred = fit.predict(test)\nsub =  pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"Target\"] = pred\nsub.to_csv(\"submission_lgbm.csv\" ,index=False)","c8007aaa":"%%time\nrf_model = RandomForestClassifier(\n    n_jobs=4,\n    class_weight='balanced',\n    n_estimators=325,\n    max_depth=5\n)\nfit = lgbm_model.fit(X, y)\npred = fit.predict(test)\nsub =  pd.read_csv(\"..\/input\/sample_submission.csv\")\nsub[\"Target\"] = pred\nsub.to_csv(\"submission_rf.csv\" ,index=False)","2e519656":"## 3.Light GBM","9ab60262":"## **2.XGBOOST**","a55e9e67":"## 4. RandomForest","43074f86":"## **1. SVM**"}}