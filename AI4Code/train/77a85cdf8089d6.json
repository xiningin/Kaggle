{"cell_type":{"f195b692":"code","1c3f98f2":"code","a0bb7389":"code","d2f8fd33":"code","df386ac4":"code","3aef029a":"code","433017ab":"code","4dcae163":"code","5d04e581":"code","0cf498cd":"code","4ce17853":"code","3222ac76":"code","adf7ee5c":"code","f4984618":"code","4f77b7bc":"code","4c7f5152":"code","e23930cc":"code","53b63f8c":"code","a6ff5f98":"code","3344a1bd":"code","35ccfa73":"code","235d9920":"code","eb419c37":"code","46c1d0a0":"code","b76c1659":"code","1193b78a":"code","b321357c":"code","ed601b12":"code","b80a6b36":"markdown","3188a4c0":"markdown","ec9272e2":"markdown","bcd24210":"markdown","b4537a7c":"markdown","c1a58efc":"markdown","62b8e0d5":"markdown","4a53f333":"markdown","0bba0d93":"markdown","300ae119":"markdown","0bc60c8d":"markdown","2c4f4c28":"markdown"},"source":{"f195b692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1c3f98f2":"import warnings\nimport seaborn as sns\nimport matplotlib.pylab as plt\nimport PIL\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.applications import Xception\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras import layers, models, optimizers","a0bb7389":"warnings.filterwarnings('ignore')\nK.image_data_format()","d2f8fd33":"IMAGE_SIZE = 299\nBATCH_SIZE = 32\nEPOCHS = 100\nk_folds=5","df386ac4":"DATA_PATH = '..\/input'","3aef029a":"TRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')","433017ab":"df_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))","4dcae163":"df_train.head()","5d04e581":"plt.figure(figsize=(15,6))\nsns.countplot('class', data=df_train)\nplt.show()","0cf498cd":"df_train['class'].value_counts()","4ce17853":"df_train['class'].value_counts().mean()","3222ac76":"df_train['class'].value_counts().describe()","adf7ee5c":"def crop_boxing_img(img_name, margin=8, size=(IMAGE_SIZE,IMAGE_SIZE)):\n    if img_name.split('_')[0] == 'train':\n        PATH = TRAIN_IMG_PATH\n        data = df_train\n    else:\n        PATH = TEST_IMG_PATH\n        data = df_test\n\n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, ['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1, y1, x2, y2)).resize(size)","f4984618":"TRAIN_CROPPED_PATH = '..\/cropped_train'\nTEST_CROPPED_PATH = '..\/cropped_test'","4f77b7bc":"if (os.path.isdir(TRAIN_CROPPED_PATH) == False):\n    os.mkdir(TRAIN_CROPPED_PATH)\n\nif (os.path.isdir(TEST_CROPPED_PATH) == False):\n    os.mkdir(TEST_CROPPED_PATH)\n\nfor i, row in df_train.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TRAIN_CROPPED_PATH, row['img_file']))\n\nfor i, row in df_test.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TEST_CROPPED_PATH, row['img_file']))","4c7f5152":"df_train['class'] = df_train['class'].astype('str')\ndf_train = df_train[['img_file', 'class']]\ndf_test = df_test[['img_file']]","e23930cc":"model_path = '.\/'","53b63f8c":"def recall_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\ndef precision_m(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","a6ff5f98":"def get_callback(model_name, patient):\n    ES = EarlyStopping(\n        monitor='val_f1_m', \n        patience=patient, \n        mode='max', \n        verbose=1)\n    RR = ReduceLROnPlateau(\n        monitor = 'val_f1_m', \n        factor = 0.5, \n        patience = patient \/ 2, \n        min_lr=0.000001, \n        verbose=1, \n        mode='max')\n    MC = ModelCheckpoint(\n        filepath=model_name, \n        monitor='val_f1_m', \n        verbose=1, \n        save_best_only=True, \n        mode='max')\n\n    return [ES, RR, MC]","3344a1bd":"def get_model(model_name, iamge_size):\n    base_model = model_name(weights='imagenet', input_shape=(iamge_size,iamge_size,3), include_top=False)\n    #base_model.trainable = False\n    model = models.Sequential()\n    model.add(base_model)\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.25))\n    model.add(layers.Dense(1024, activation='relu'))\n    model.add(layers.Dropout(0.25))\n \n    model.add(layers.Dense(196, activation='softmax'))\n    model.summary()\n\n    optimizer = optimizers.RMSprop(lr=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', f1_m])\n\n    return model","35ccfa73":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.2,\n    shear_range=0.2,\n    #brightness_range=(1, 1.2),\n    fill_mode='nearest'\n    )\n\nvalid_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True  # divide each input by its std\n    )\ntest_datagen = ImageDataGenerator(\n    rescale=1.\/255\n    #featurewise_center= True,  # set input mean to 0 over the dataset\n    #samplewise_center=True,  # set each sample mean to 0\n    #featurewise_std_normalization= True,  # divide inputs by std of the dataset\n    #samplewise_std_normalization=True,  # divide each input by its std\n    )","235d9920":"skf = StratifiedKFold(n_splits=k_folds, random_state=2019)\n#skf = KFold(n_splits=k_folds, random_state=2019)","eb419c37":"j = 1\nmodel_xception_names = []\nfor (train_index, valid_index) in skf.split(\n    df_train['img_file'], \n    df_train['class']):\n\n    traindf = df_train.iloc[train_index, :].reset_index()\n    validdf = df_train.iloc[valid_index, :].reset_index()\n\n    print(\"=========================================\")\n    print(\"====== K Fold Validation step => %d\/%d =======\" % (j,k_folds))\n    print(\"=========================================\")\n    \n    train_generator = train_datagen.flow_from_dataframe(\n        dataframe=traindf,\n        directory=TRAIN_CROPPED_PATH,\n        x_col='img_file',\n        y_col='class',\n        target_size= (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        seed=2019,\n        shuffle=True\n        )\n\n    valid_generator = valid_datagen.flow_from_dataframe(\n        dataframe=validdf,\n        directory=TRAIN_CROPPED_PATH,\n        x_col='img_file',\n        y_col='class',\n        target_size= (IMAGE_SIZE, IMAGE_SIZE),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=BATCH_SIZE,\n        seed=2019,\n        shuffle=True\n        )\n\n    model_name = model_path + str(j) + '_xception.hdf5'\n    model_xception_names.append(model_name)\n    \n    model_xception = get_model(Xception, IMAGE_SIZE)\n    \n    try:\n        model_xception.load_weights(model_name)\n    except:\n        pass\n        \n    history = model_xception.fit_generator(\n        train_generator,\n        steps_per_epoch=len(traindf.index) \/ BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_data=valid_generator,\n        validation_steps=len(validdf.index) \/ BATCH_SIZE,\n        verbose=1,\n        shuffle=False,\n        callbacks = get_callback(model_name, 6)\n        )\n        \n    j+=1","46c1d0a0":"test_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_CROPPED_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (IMAGE_SIZE, IMAGE_SIZE),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)","b76c1659":"xception_prediction = []\nfor i, name in enumerate(model_xception_names):\n    model_xception = get_model(Xception, IMAGE_SIZE)\n    model_xception.load_weights(name)\n    test_generator.reset()\n    pred = model_xception.predict_generator(\n        generator=test_generator,\n        steps = len(df_test)\/BATCH_SIZE,\n        verbose=1\n    )\n    xception_prediction.append(pred)\n\ny_pred_xception = np.mean(xception_prediction, axis=0)","1193b78a":"preds_class_indices=np.argmax(y_pred_xception, axis=1)","b321357c":"labels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nfinal_pred = [labels[k] for k in preds_class_indices]","ed601b12":"submission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nsubmission[\"class\"] = final_pred\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","b80a6b36":"* EarlyStoping\uc740 \uc778\uc790\uc778 Patient\uac00 10\uc73c\ub85c \ub4e4\uc5b4\uc628\ub2e4\uba74, Val_loss\uac00 10\ud68c \uc774\uc0c1 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 Stop.\n* ReduceLROnPlateau\ub294 Patient\/2 \ud68c\ub9cc\ud07c Val_loss\uac00 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc73c\uba74 learing late\ub97c \ubc18\uc73c\ub85c \uc904\uc774\ub294 \uc6a9\ub3c4.\n* ModelCheckPoint\ub294 Val_loss\uac00 \uac00\uc7a5 \uc88b\uc740(\uac00\uc7a5\uc791\uc740) \uacbd\uc6b0\uc5d0 \ud574\ub2f9 \ud6c8\ub828 Weight \ud30c\uc77c\ub85c \uc800\uc7a5.\n\nPC\uc790\uc6d0\uc774 \ub418\uc2dc\ub294 \ubd84\ub4e4 \uc5ec\uae30\uc5d0 Tensorboard\ub3c4 \ucd94\uac00\ud558\uc2dc\uba74 \uc2e4\uc2dc\uac04 \ud6c8\ub828 \uadf8\ub798\ud504\ub97c \ud655\uc778\ud574 \ubcf4\uc2e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","3188a4c0":"5\uac1c\uc758 Fold\uc5d0\uc11c \ud6c8\ub828\ud55c \uacb0\uacfc\ub4e4(Weight)\ub97c \ubd88\ub7ec\ub4e4\ub824\uc11c 5\ubc88 \uc608\uce21\ud558\uace0 \ud3c9\uade0.\n\n\uc704\uc5d0\uc11c\ub3c4 \ub9d0\uc500\ub4dc\ub838\uc9c0\ub9cc \uc790\uc6d0\uc774 \ub418\uc2dc\ub294 \ubd84\ub4e4\uc740 \uacb0\uacfc \ub2e4\ub978 \uc0ac\uc804\ud6c8\ub828 \ubaa8\ub378\uc744 \ucd94\uac00\ud558\uc5ec \uac19\uc774 \ub3cc\ub9ac\uc2dc\uace0,\n\nA\ubaa8\ub378 5Fold \uc608\uce21\ud3c9\uade0 + B\ubaa8\ub378 5Fold \ud3c9\uade0..... \uc774\ub7f0\uc2dd\uc73c\ub85c \ud558\uc2dc\uba74 \uc870\uae08\uc774\ub77c\ub3c4\uc810\uc218\uac00 \uc88b\uc544\uc9c0\uc9c0 \uc54a\uc744\uae4c \ud569\ub2c8\ub2e4.\n","ec9272e2":"\uac01 \ucc28\uc885\ubcc4 \ud3c9\uade0 \ubd84\ud3ec\uac1c\uc218\ub294 51\uac1c\uc784.","bcd24210":"\uc0ac\uc804 \ud6c8\ub828\ubaa8\ub378\uc778 Xception\uc5d0 \uce35\uc744 \ub354\ud574\uc11c \ud6c8\ub828 \uc2dc\ud0a4\ub294 \uad6c\uc870\ub85c \ubaa8\ub378\uc744 \uad6c\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4.\nDropout\uc740 \uacfc\uc801\ud569\uc744 \ucd5c\ub300\ud55c \uc904\uc5ec\ubcfc \uc6a9\ub3c4\ub85c \ucd94\uac00\ud588\uc2b5\ub2c8\ub2e4. (0.5)\uba74 \ubc18\uc744 \ub0a0\ub824 \ubc84\ub9b0\ub2e4\ub294 \uc758\ubbf8\uc785\ub2c8\ub2e4.\n\ud0c0\ub178\uc2a4\uc758 \uc190\uac00\ub77d \ud295\uae30\uae30","b4537a7c":"\uc81c\uac00 \uc62c\ub9ac\ub294 \ucf54\ub4dc\ub294 \ub9c8\uc74c\uac83 \uc774\uc6a9\ud558\uc154\ub3c4 \ub429\ub2c8\ub2e4.\n\uc62c\ub77c\uc628 Weight\ub4e4 \uac00\uc838\ub2e4 \uc4f0\uc154\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4. \n\n\ub2e4\ub9cc \uc870\uae08\uc774\ub77c\ub3c4 \ub3c4\uc6c0\uc774 \ub418\uc168\ub2e4\uba74,\uc774\uc6a9\ud558\uc2e4\ub54c \ucd9c\ucc98 \ubc1d\ud600\uc8fc\uc2dc\uace0, \ud22c\ud45c \ud55c\ubc88 \ud574\uc8fc\uc138\uc694. \uc791\uc131\ud558\uc2e0 Kernel \uacf5\uac1c, \ub610\ub294 Discussion\uc5d0\uc11c \uac19\uc774 \uacf5\uc720\ud558\uace0 \ub354 \ub098\uc740 \ubc29\ubc95\uc774 \uc788\ub294\uc9c0 \uac19\uc774 \uace0\ubbfc\ud588\uc73c\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4. \n\n\uc81c\uac00 \ud55c \uacb0\uacfc\ub97c \uacf5\uc720 \ub4dc\ub9ac\ub294 \uc774\uc720\ub294 \uc804 \uc774\ub860\uc5d0 \ub300\ud574\uc11c \uc57d\ud569\ub2c8\ub2e4. \ud574\uc11c \uc81c \uacf5\uac1c\ub41c \ucee4\ub110\ub85c \ub2e4\ub978 \ubd84\ub4e4\uc758 \ub3c4\uc6c0\uc744 \ubc1b\uace0 \uc2f6\uc5b4\uc11c\uc785\ub2c8\ub2e4.\n\nkeras \uc0ac\uc6a9\ubc95, \ud568\uc218 \uc0ac\uc6a9\ubc95\uc5d0 \uc775\uc219\ud558\uc2dc\uc9c0 \uc54a\uc73c\uc2e0 \ubd84\ub4e4\uc5d0\uac8c \uc870\uae08\uc774\ub77c\ub3c4 \ub3c4\uc6c0\uc774 \ub418\uc168\uc73c\uba74 \ud558\ub294 \ub9c8\uc74c\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n(\uadf8\ub807\ub2e4\uace0 \uc81c\uac00 \uc62c\ub9b0 Kernel\uc774 \uc544\uc8fc \ub098\uc774\uc2a4\ud55c \ubb38\ubc95\uc774\ub77c\ub294 \ub73b \uc544\ub2d9\ub2c8\ub2e4.)\n\n\ub2e8\uc21c \ubcf5\uc0ac\uc5d0 Leader\ubcf4\ub4dc \ub4f1\ub85d\ub9cc \ud558\uc2dc\ub294 \ubd84\ub4e4\uc740 \uc5c6\uc73c\uc2e4 \uac70\ub77c \ubbff\uc2b5\ub2c8\ub2e4.\n\n\ud558\uae30 Kernel \ub0b4\uc6a9\uc744 \uc694\uc57d\ud558\uba74\n1. Xception \uc0ac\uc804 \ud6c8\ub828 \ubaa8\ub378\uc744 \uc0ac\uc6a9\n2. StratifiedKFold\ub85c 5 Fold\ub85c \ub098\ub220\uc11c \ud6c8\ub828 \ubc0f \uac80\uc99d \uc218\ud589\n3. \uac01 Fold \ub9c8\ub2e4 val_loss\uac00 \uac00\uc791 \uc791\uac8c \ub098\uc628 \uac83\uc5d0 \ub300\ud574\uc11c Weight \ud30c\uc77c\ub85c \uc800\uc7a5\n4. \ud6c8\ub828\uc5d0\uc11c \ub098\uc628 5\uac1c\uc758 Weight\ub85c \uc608\uce21 \uc9c4\ud589\n5. 5\uac1c\uc758 \uc608\uce21\uacb0\uacfc \ud3c9\uade0(\uc559\uc0c1\ube14)\n\n\ucd94\uac00\ub85c StratifiedKFold\uc640 Keras\uc758 ImageDataGenerator()\ub97c \uac19\uc774 \uc6b4\uc601\ud588\uc2b5\ub2c8\ub2e4.","c1a58efc":"crop_boxing\uc740 \n\n [Daehun Gwak\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/daehungwak\/keras-how-to-use-pretrained-model)\n\n [\ud5c8\ud0dc\uba85\ub2d8 \ucee4\ub110](https:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping)\n \n \uc5d0\uc11c \uac00\uc838\uc654\uc2b5\ub2c8\ub2e4.","62b8e0d5":"5\uac1c\uc758 Fold\ub97c \ud6c8\ub828\ud558\uba74\uc11c \uac01 \ud3f4\ub4dc\ubcc4 Val loss\uac00 \uac00\uc7a5 \uc791\uc740 model weight\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n\n\ud6c8\ub828\uc774 \ub05d\ub098\uba74 5\uac1c\uc758 Weight\ud30c\uc77c\uc774 \uc0dd\uc131\ub429\ub2c8\ub2e4.\n\n\uc790\uc6d0\uc774 \ub418\uc2dc\uba74 \uc5ec\uae30\uc5d0 xception \uc774\uc678\uc5d0 \ub2e4\ub978 \uc0ac\uc804 \ud6c8\ub828 \ubaa8\ub378\uc744 \ud558\ub098 \ub354 \ucd94\uac00\ud558\uc2dc\uace0, \ub098\uc911\uc5d0 \uadf8 \ubaa8\ub378\uae4c\uc9c0 \ud1b5\ud569\ud558\uc5ec \uc559\uc0c1\ube14 \ud558\uc154\ub3c4 \ub420 \ub4ef \ud569\ub2c8\ub2e4.","4a53f333":"![image.png](attachment:image.png)\n\uc5ec\uae30\uc11c \ub2e4\ub8e8\ub294 \uc790\ub3d9\ucc28 \uc774\ubbf8\uc9c0 \ubd84\ub958\uc758 \uacbd\uc6b0\n\uadf8\ub9bc\uc5d0\uc11c \n\n1.Class\uc218\uac00 196\uac1c\ub85c,\n\n2.Fold\ub97c 5\uac1c\ub85c,\n\n3.Testing Set\uc774\ub77c\ub294 \uba85\uce6d \ub300\uc2e0 Valid Set\ub85c \uc774\ud574\ud558\uc2dc\uba74 \ub429\ub2c8\ub2e4.\n\n\uac01 \ud074\ub798\uc2a4\ubcc4 \ud6c8\ub828\ub370\uc774\ud130\uc758 \ubd84\ud3ec\uac00 \ub2e4\ub974\ub2c8 StratifiedKFold\ub97c \uc0ac\uc6a9\ud558\uac8c \ub418\uba74 \uac01 \ud074\ub798\uc2a4 \ubd84\ud3ec\uc5d0 \ub9de\ucdb0\uc11c\n\ud3f4\ub4dc\ub97c \ub098\ub204\uac8c \ub429\ub2c8\ub2e4. \n(\uc55e\ucabd\uc5d0\uc11c \uc0b4\ud3b4\ubcf8 \uacb0\uacfc 119\ubc88 Class\uc758 \uacbd\uc6b0 84\uac1c\uc758 \uc0d8\ud50c\uc774 \uc874\uc7ac\ud588\uace0, 136\ubc88\uc758 \uacbd\uc6b0 30\uac1c\uc758 \uc0d8\ud50c\uc774 \uc874\uc7ac.\n\uc989 \uac01 \ud074\ub798\uc2a4\ubcc4 \uc0d8\ud50c \uc218\uc758 \ube44\uc728\uc744 \uc720\uc9c0\ud558\uba74\uc11c Fold\ub97c \ub098\ub204\uae30 \uc704\ud568.)\n","0bba0d93":"119\ubc88 \uc720\ud615\uc774 84\uac1c\ub85c \uc81c\uc77c \ub9ce\uace0,\n\n136\ubc88 \uc720\ud615\uc774 30\uac1c\ub85c \uac00\uc7a5 \uc801\uc74c.","300ae119":"\uc704\uc758 \ub0b4\uc6a9\uc744 \uc694\uc57d\ud558\uc5ec \ucd9c\ub825\ud574 \ubcf4\uba74,\n\ucd5c\ub300\uac12 84, \ucd5c\uc18c\uac12 30, \ud3c9\uade051\ub85c \ud3b8\ucc28\uac00 \uc880 \uc788\ub2e4\uace0 \ubcfc \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\n\uadf8\ub798\uc11c KFold\ub300\uc2e0 StratifiedKFold\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.","0bc60c8d":"Cropping\ud55c \uc774\ubbf8\uc9c0 \uc800\uc7a5\ud558\uae30 \uc704\ud55c \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131\ubc0f Cropping","2c4f4c28":"* IMAGE_SIZE\ub294 \uc0ac\uc6a9\ud558\ub294 \uc0ac\uc804\ud6c8\ub828 \ubaa8\ub378\ubcc4 \ucd94\ucc9c\ud558\ub294 \uc774\ubbf8\uc9c0 \uc0ac\uc774\uc988\ub97c \uc8fc\uae30 \uc704\ud574\uc11c\uc785\ub2c8\ub2e4.\n\nXception\uc758 \uacbd\uc6b0 299 * 299\uc784.\n\n![image.png](attachment:image.png)\n\n* BATCH_SIZE \ub97c 4\ub85c \ud55c \uc774\uc720\ub294 \uc81c PC\uc5d0\uc11c \uc774\ubcf4\ub2e4 \ud06c\uac8c \uc8fc\uba74 \uc790\uc6d0 \ubd80\uc871\uc73c\ub85c \ubaa8\ub378 \ud6c8\ub828 \uc9c4\ud589\uc774 \ubd88\uac00\ud568. \uc790\uc6d0 \ub418\uc2dc\ub294 \ubd84\ub4e4\uc73c Batch \uc0ac\uc774\uc988 \ub298\ub824 \uc7a1\uc73c\uc2dc\uba74 \ud6c8\ub828\uc2dc\uac04 \ub2e8\ucd95\uc774 \uac00\ub2a5\ud569\ub2c8\ub2e4.\n\n* EPOCHS \ub294 100\ub85c \uc8fc\uc5c8\uc9c0\ub9cc EarlyStoping \ud568\uc218\uc640 \uac19\uc774 \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 \uc2e4\uc81c\ub85c \ub300\ub7b5 30Epoch \uc774\ud558\uc5d0\uc11c Fold\ubcc4 \ud6c8\ub828\uc774 \uc911\ub2e8 \ub429\ub2c8\ub2e4.\n\n* Folds\ub294 5\uac1c\ub85c \ud55c \uc774\uc720\ub294 \uc774\ub807\uac8c \ud558\uc5ec \ub9e4 Fold\ub2f9 \uac80\uc99d \ub370\uc774\ud130 20% \ub9de\ucd94\ub824\ub294 \uc758\ub3c4 \uc785\ub2c8\ub2e4."}}