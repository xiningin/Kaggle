{"cell_type":{"f9938ac4":"code","f6eac41e":"code","eaf898cd":"code","9d27ed46":"code","cf6f5879":"code","f68c0f6b":"code","b4998547":"code","77f0898c":"code","6511ac1d":"code","c75ce308":"code","a453168e":"code","ffdb932b":"code","c8d0110e":"code","0d9b1ab3":"markdown","fb5fd135":"markdown","e60fcf29":"markdown","b3d6dcb8":"markdown","45cf33fd":"markdown","5439e6a8":"markdown","35f3d11e":"markdown","eb8752ac":"markdown"},"source":{"f9938ac4":"import numpy as np # to handle matrix and data operation\nimport pandas as pd # to read csv and handle dataframe\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nfrom torch.autograd import Variable\n\nfrom sklearn.model_selection import train_test_split","f6eac41e":"df = pd.read_csv('..\/input\/train.csv')\nprint(df.shape)","eaf898cd":"y = df['label'].values\nX = df.drop(['label'],1).values\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)","9d27ed46":"print(y_test.shape)","cf6f5879":"BATCH_SIZE = 32\n\ntorch_X_train = torch.from_numpy(X_train).type(torch.LongTensor)\ntorch_y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\ntorch_X_test = torch.from_numpy(X_test).type(torch.LongTensor)\ntorch_y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\ntest = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n","f68c0f6b":"class MLP(nn.Module):\n    def __init__(self):\n        super(MLP, self).__init__()\n        self.linear1 = nn.Linear(784,250)\n        self.linear2 = nn.Linear(250,100)\n        self.linear3 = nn.Linear(100,10)\n    \n    def forward(self,X):\n        X = F.relu(self.linear1(X))\n        X = F.relu(self.linear2(X))\n        X = self.linear3(X)\n        return F.log_softmax(X, dim=1)\n \nmlp = MLP()\nprint(mlp)\n","b4998547":"def fit(model, train_loader):\n    optimizer = torch.optim.Adam(model.parameters())#,lr=0.001, betas=(0.9,0.999))\n    error = nn.CrossEntropyLoss()\n    EPOCHS = 5\n    model.train()\n    for epoch in range(EPOCHS):\n        correct = 0\n        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n            var_X_batch = Variable(X_batch).float()\n            var_y_batch = Variable(y_batch)\n            optimizer.zero_grad()\n            output = model(var_X_batch)\n            loss = error(output, var_y_batch)\n            loss.backward()\n            optimizer.step()\n\n            # Total correct predictions\n            predicted = torch.max(output.data, 1)[1] \n            correct += (predicted == var_y_batch).sum()\n            #print(correct)\n            if batch_idx % 50 == 0:\n                print('Epoch : {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Accuracy:{:.3f}%'.format(\n                    epoch, batch_idx*len(X_batch), len(train_loader.dataset), 100.*batch_idx \/ len(train_loader), loss.data[0], float(correct*100) \/ float(BATCH_SIZE*(batch_idx+1))))\n                ","77f0898c":"fit(mlp, train_loader)","6511ac1d":"def evaluate(model):\n#model = mlp\n    correct = 0 \n    for test_imgs, test_labels in test_loader:\n        #print(test_imgs.shape)\n        test_imgs = Variable(test_imgs).float()\n        output = model(test_imgs)\n        predicted = torch.max(output,1)[1]\n        correct += (predicted == test_labels).sum()\n    print(\"Test accuracy:{:.3f}% \".format( float(correct) \/ (len(test_loader)*BATCH_SIZE)))\nevaluate(mlp)","c75ce308":"torch_X_train = torch_X_train.view(-1, 1,28,28).float()\ntorch_X_test = torch_X_test.view(-1,1,28,28).float()\nprint(torch_X_train.shape)\nprint(torch_X_test.shape)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(torch_X_train,torch_y_train)\ntest = torch.utils.data.TensorDataset(torch_X_test,torch_y_test)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = BATCH_SIZE, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = BATCH_SIZE, shuffle = False)\n","a453168e":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n        self.conv3 = nn.Conv2d(32,64, kernel_size=5)\n        self.fc1 = nn.Linear(3*3*64, 256)\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        #x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = F.relu(F.max_pool2d(self.conv3(x),2))\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.view(-1,3*3*64 )\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n \ncnn = CNN()\nprint(cnn)\n\nit = iter(train_loader)\nX_batch, y_batch = next(it)\nprint(cnn.forward(X_batch).shape)","ffdb932b":"fit(cnn,train_loader)","c8d0110e":"evaluate(cnn)","0d9b1ab3":"# History\n\nContrary to what most people think, Neural Networks is quite an old concept. It was first introduced in 1957 under the name ***perceptron***. Peceptron is a 1-layer feed forward neural network. However the infrastructure and the algorthm around it was not good enough to allow large scale training. Later on in 1986, ***Multi Layer Perceptron (MLP)*** was introduced with the backpropagation algorithm in order to train a network with more than 1 layer. Thanks to this algorithm we are not able to train non-linear model which can learn high level abstract features. Then ***Convolutional Neural Network (CNN)*** has been introduced in order to learn better features and with the possibility to reduce the number of parameters to be trained. And now, here we are, in the ***Deep Learning era*** ","fb5fd135":"# Introduction\n\nThis notebook aims at discovering Convolutional Neural Network. We will see the theory behind it, and an implementation in Pytorch for hand-digits classification on MNIST dataset. ","e60fcf29":"## Explanation\n\nTo better understand convolutional neural network I recommend the great section on it here : http:\/\/cs231n.github.io\/convolutional-networks\/\n\n**Convolutional operation** : First let's clarify briefly how we can perform the convolutional operation on an image. For that we need to define a **kernel** which is a small matrix of size 5 \\* 5 for example. To perform the convolution operation, we just need to slide the kernel along the image horizontally and vertically and do the dot product of the kernel and the small portion of the image.\n\n**Pooling** : the convolutional operation give an output of the same size of the input image. To reduce the size of the image and thus reduce the number of paramers in the model we perform a Pooling operation. The pooling operation need a window size.. By sliding the window along the image, we compute the mean or the max of the portion of the image inside the window in case of MeanPooling or MaxPooling.\n\n**Stride** is the number of pixels to pass at a time when sliding the convolutional kernel.  \n\n**Padding** to preserve exactly the size of the input image, it is useful to add a zero padding on the border of the image. \n\n\n**To remember** : What makes a CNN so interesting for images is that it is invariant by translation and for each convolutional layer we only need to store the kernels. Thus we can stack a lot of layers to learn deep features without having too much parameters that would make a model untrainnable. ","b3d6dcb8":"<center><h2>Convolutional Neural Network<\/h2><\/center>","45cf33fd":"## Data loader\n\nSince a CNN needs a image shape as input let's reshape our flatten images to real image","5439e6a8":"## MLP Evaluation","35f3d11e":"We have 784\\*(250+1) + 250\\*(100+1) + 100\\*(10+1) = 222 360 parameters to train","eb8752ac":"    # Multi-Layer Perceptron\n\nThe first thing to ask is : why do we needed Convolutional Neural Network in the first place... Well, let's see what happen when we train a Multi-Layer Perceptron to recognize hand-written digits. In Machine Learning we have our own \"Hello World\" which is the MNIST dataset. Let's see what this dataset is about and how a multi-layer perceptron will perform.   "}}