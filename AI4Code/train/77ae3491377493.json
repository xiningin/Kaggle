{"cell_type":{"5f36226c":"code","8c7d62d6":"code","47f8f8ff":"code","9ef03551":"code","ee4d1e77":"code","13f02bbb":"code","147512a7":"code","75f8deef":"code","6d39300f":"code","b1c0d208":"code","9d58bc0a":"code","3774a5e0":"code","0672c8b8":"code","ed8b3cd8":"code","425fdcec":"code","27751c77":"code","41d5b7f6":"code","a87818bd":"code","67478e92":"code","f6a22b73":"code","6c1b5c8a":"code","2311100d":"code","2fad57eb":"code","f6a0f646":"code","73f00724":"code","bdadeb05":"code","49787e34":"code","c8f31124":"markdown","26aa8ffa":"markdown","5eebc5b8":"markdown","a7927926":"markdown","34fe594c":"markdown","8bdc3730":"markdown","99ed288d":"markdown","4f87b173":"markdown","eca10c4d":"markdown","482b375b":"markdown","1c6a11af":"markdown","7075cf8a":"markdown","1ca02680":"markdown","cf046b87":"markdown","e40c6cad":"markdown","b282c385":"markdown","73f1bdea":"markdown","9626df12":"markdown","e757dc7c":"markdown","3d0e021b":"markdown","bd729e97":"markdown","b0714fd5":"markdown"},"source":{"5f36226c":"import os\nprint(os.listdir(\"..\/input\/\"))","8c7d62d6":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score, roc_auc_score\nfrom sklearn.pipeline import make_pipeline\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","47f8f8ff":"train  = pd.read_csv(\"..\/input\/train_E6oV3lV.csv\")\ntest = pd.read_csv(\"..\/input\/test_tweets_anuFYb8.csv\")\ntrain.sample(2)","9ef03551":"train.shape, test.shape","ee4d1e77":"df = train.append(test, ignore_index = True)\ndf.shape","13f02bbb":"train['cleaned_tweet'] = train.tweet.apply(lambda x: ' '.join([word for word in x.split() if not word.startswith('@')]))\ntest['cleaned_tweet'] = test.tweet.apply(lambda x: ' '.join([word for word in x.split() if not word.startswith('@')]))","147512a7":"#Select all words from normal tweet\nnormal_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 0]])\n#Collect all hashtags\npos_htag = [htag for htag in normal_words.split() if htag.startswith('#')]\n#Remove hashtag symbol (#)\npos_htag = [pos_htag[i][1:] for i in range(len(pos_htag))]\n#Count frequency of each word\npos_htag_freqcount = nltk.FreqDist(pos_htag)\npos_htag_df = pd.DataFrame({'Hashtag' : list(pos_htag_freqcount.keys()),\n                            'Count' : list(pos_htag_freqcount.values())})","75f8deef":"#Select top 20 most frequent hashtags and plot them   \nmost_frequent = pos_htag_df.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=most_frequent, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","6d39300f":"#Repeat same steps for negative tweets\nnegative_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 1]])\nneg_htag = [htag for htag in negative_words.split() if htag.startswith('#')]\nneg_htag = [neg_htag[i][1:] for i in range(len(neg_htag))]\nneg_htag_freqcount = nltk.FreqDist(neg_htag)\nneg_htag_df = pd.DataFrame({'Hashtag' : list(neg_htag_freqcount.keys()),\n                            'Count' : list(neg_htag_freqcount.values())})","b1c0d208":"most_frequent = neg_htag_df.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=most_frequent, x= \"Hashtag\", y = \"Count\")\nplt.show()","9d58bc0a":"normal_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 0]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(normal_words)\nprint('Normal words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","3774a5e0":"negative_words = ' '.join([word for word in train['cleaned_tweet'][train['label'] == 1]])\nwordcloud = WordCloud(width = 800, height = 500, max_font_size = 110).generate(negative_words)\nprint('Negative words')\nplt.figure(figsize= (12,8))\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.show()","0672c8b8":"train.sample(2)","ed8b3cd8":"X_train, X_val, y_train, y_val = train_test_split(train['cleaned_tweet'], train['label'], random_state = 0)\nX_train.shape, X_val.shape","425fdcec":"vect = CountVectorizer().fit(X_train)\nvect","27751c77":"print('Total features =', len(vect.get_feature_names()))\nprint(vect.get_feature_names()[::5000])","41d5b7f6":"X_train_vectorized = vect.transform(X_train)\nX_train_vectorized","a87818bd":"model = MultinomialNB()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1 :', f1_score(y_val, pred))","67478e92":"model = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1 :', f1_score(y_val, pred))","f6a22b73":"# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer().fit(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","6c1b5c8a":"vect = CountVectorizer(min_df = 2, ngram_range = (1,2)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","2311100d":"pipe = make_pipeline(CountVectorizer(), LogisticRegression())\nparam_grid = {\"logisticregression__C\": [0.01, 0.1, 1, 10, 50, 100],\n              \"countvectorizer__min_df\": [1,2,3],\n              \"countvectorizer__ngram_range\": [(1,1), (1,2), (1,3)]}\ngrid = GridSearchCV(pipe, param_grid, cv = 5, scoring = 'f1', n_jobs = -1)\ngrid.fit(X_train, y_train)\nprint('Best parameters:', grid.best_params_)","2fad57eb":"vect = CountVectorizer(min_df = 1, ngram_range = (1,1)).fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nprint('Total Features =', len(vect.get_feature_names()))\n\nmodel = LogisticRegression(C = 10)\nmodel.fit(X_train_vectorized, y_train)\npred = model.predict(vect.transform(X_val))\nprint('F1: ', f1_score(y_val, pred))","f6a0f646":"print('Fraction of racist\/sexist tweet in train data :', train.label.sum()\/len(train))\nprint('Fraction of racist\/sexist tweet predicted by model :', pred.sum()\/len(pred))","73f00724":"pred_prob = model.predict_proba(vect.transform(X_val))\npred = np.where(pred_prob[:,1] > 0.35, 1, 0)\nprint('Fraction of racist\/sexist tweet predicted by model :', sum(pred)\/len(pred))\nprint('F1: ', f1_score(y_val, pred))","bdadeb05":"feature_names = np.array(vect.get_feature_names())\nsorted_coef_index = model.coef_[0].argsort()\nprint('Smallest_coefs :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest_coefs :\\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","49787e34":"X_test = test['tweet']\ntest_pred = model.predict_proba(vect.transform(X_test))\npredictions = np.where(test_pred[:,1] > 0.35, 1, 0)\nresults = pd.DataFrame(data = {'id' : test.id, 'label' : predictions})\n#results.to_csv('results.csv', index = False)","c8f31124":"The aim of this kernel is to detect hate speech (racist\/sexist) in tweets.","26aa8ffa":"## Applying Bag-of-Words","5eebc5b8":"Using hyperparameter tuning and probability method, we were able to improve our model score by 5%.\n\nLets look at largest and smallest coefficients that our model used.","a7927926":"### Hyper parameter tuning","34fe594c":"From both plots, we can conclude that hashtags are very important for sentiment analysis and should not be ignored.","8bdc3730":"The smallest coefficients are indicating to normal tweets to the model whereas the largest coeeficients are indicative for racist\/sexist tweets.","99ed288d":"### Logistic Regression","4f87b173":"Words used like love, friend, happy are used in normal tweets whereas racist\/sexist can be found in words like trump, black, politics etc.","eca10c4d":"### Naive Bayes","482b375b":"## Finding common words in both classes of tweets using Visualization","1c6a11af":"### Racist\/Sexist Tweets","7075cf8a":"Remove twitter handlers i.e., @user","1ca02680":"### min_df & n-grams","cf046b87":"OOPS!!! For this dataset, our default parameters were the best except fot our model (C = 10)","e40c6cad":"tf-idf not performed well for this data.","b282c385":"Rescale data using CountVectorizer\n### CountVectorizer","73f1bdea":"Logistic Regression performed well then Naive Bayes for the default parameters. Thus, we will be using only Logistic Regression ahead.\n\nLets now rescale the data using tf-idf","9626df12":"Fraction is very less. Lets change the default predict probability.","e757dc7c":"### Tfidf","3d0e021b":"## Bag-of-Words with more than one word (n-grams)","bd729e97":"### Hashtags","b0714fd5":"### Normal Tweets"}}