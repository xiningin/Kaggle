{"cell_type":{"723618c6":"code","22f7ad51":"code","4d1922a5":"code","b32c30bf":"code","96d161c7":"code","ba4da5bb":"code","ff62d183":"code","9ca4e12b":"code","271356cb":"code","96d3a5c8":"code","d484777e":"code","5c1dde69":"code","79da4a4b":"code","ee9a730a":"code","ed0900cb":"code","08e9ffbe":"code","2a75743d":"code","afe1a169":"code","44ff471f":"code","4dd3c97c":"code","4c22dc2e":"code","777d7ff8":"code","5fb697e3":"code","4b201942":"code","15c31ccd":"code","9330fc83":"code","ed0a2a86":"code","a9606e66":"code","9a6463e2":"code","1ac5fd99":"markdown","629db2d9":"markdown","2b47f911":"markdown","e88e0d7c":"markdown","550a1864":"markdown","86ad95d6":"markdown","9ad186f5":"markdown","10c50d13":"markdown","f5b8c0aa":"markdown"},"source":{"723618c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","22f7ad51":"import pandas as pd\n#loading data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","4d1922a5":"train_df.info()\ntest_df.info()","b32c30bf":"#Dealing with the data to be filled with zero\ncols_to_fill_zero = ['Alley','FireplaceQu','GarageType','GarageFinish',\n                     'GarageQual','GarageCond','PoolQC' ,'Fence','MiscFeature',\n                    'GarageFinish','GarageQual','GarageQual', 'GarageCond', 'PoolQC',\n                     'Fence','MiscFeature','MasVnrArea','BsmtQual' ,'BsmtCond','BsmtExposure',\n                     'BsmtFinType1','BsmtFinSF2' ,'TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n                     'GarageArea','GarageCars']\ntrain_df[cols_to_fill_zero] =  train_df[cols_to_fill_zero].fillna(0, inplace = True)\ntest_df[cols_to_fill_zero] = train_df[cols_to_fill_zero].fillna(0, inplace = True)","96d161c7":"#fill missing data with mean\/ median\ncols_to_fill_mean = ['LotFrontage','GarageYrBlt','BsmtUnfSF']\n\ntrain_df['LotFrontage'] = train_df['LotFrontage'].fillna(train_df['LotFrontage'].dropna().mean())\ntest_df['LotFrontage'] = test_df['LotFrontage'].fillna(train_df['LotFrontage'].dropna().mean())\n\ntrain_df['GarageYrBlt'] = train_df['GarageYrBlt'].fillna(train_df['GarageYrBlt'].dropna().median())\ntest_df['GarageYrBlt'] = test_df['GarageYrBlt'].fillna(train_df['GarageYrBlt'].dropna().median())\n\n\ntrain_df['BsmtUnfSF'] = train_df['BsmtUnfSF'].fillna(train_df['BsmtUnfSF'].dropna().mean())\ntest_df['BsmtUnfSF'] = test_df['BsmtUnfSF'].fillna(train_df['BsmtUnfSF'].dropna().mean())\n\n","ba4da5bb":"train_df.isnull().sum()","ff62d183":"#filling categorical data with most_frequent occurings\ncategorical_missing_cols = ['MasVnrType', 'BsmtFinType2', 'Electrical','MSZoning','Utilities',\n                            'Exterior1st','Exterior2nd','MasVnrType','BsmtFinSF1','BsmtFinType2',\n                            'KitchenQual','Functional','SaleType']\n\n\nfor col in categorical_missing_cols:\n    most_frequent = train_df[col].dropna().value_counts().idxmax()\n    train_df[col] = train_df[col].fillna(most_frequent)\n    test_df[col] = train_df[col].fillna(most_frequent)\n\n\n","9ca4e12b":"#dummy code to print the null values in training data\ntrain_cols_with_null = []\n\nfor col in train_df.columns:\n    if train_df[col].isnull().sum()>0:\n        train_cols_with_null.append(col)\n\ntrain_cols_with_null","271356cb":"#dummy code to print the columns with null values\ntest_cols_with_null = []\n\nfor col in test_df.columns:\n    if test_df[col].isnull().sum()>0:\n        test_cols_with_null.append(col)\n\ntest_cols_with_null","96d3a5c8":"#setting the dependent and independent variables\ntrain_X =train_df.drop(['SalePrice'],axis=1)\ntest_X = test_df\ny = train_df[['SalePrice']]\n","d484777e":"#storing object cols in a list\nobject_cols = train_X.select_dtypes(include=['object']).columns\nlen(object_cols)","5c1dde69":"#cols to one hot encode\nnominal_vars= ['MSZoning', 'Street', 'Alley','Utilities','LotConfig'\n                     ,'Condition1', 'Condition2', 'BldgType', 'HouseStyle'\n                     , 'RoofStyle', 'RoofMatl', 'MasVnrType', 'Foundation'\n                     ,  'Heating','CentralAir', 'Electrical','Functional'\n                     , 'PavedDrive','SaleType','SaleCondition']\n\nlen(nominal_vars)","79da4a4b":"#columns to label encode\nordinal_vars = []\nfor col in object_cols:\n    \n    if col not in nominal_vars:\n        ordinal_vars.append(col)\n        \nlen(ordinal_vars)","ee9a730a":"#label encoding to all the categorical columns\n#to-do: OneHotEncode nominal variables\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoded_train_X = train_X.copy()\nlabel_encoded_test_X = test_X.copy()\n\nfor col in ordinal_vars:\n    label_encoded_train_X[col] = pd.DataFrame(label_encoder.fit_transform(train_X[col]))\n    label_encoded_test_X[col] = pd.DataFrame(label_encoder.transform(test_X[col]))","ed0900cb":"#replacing the object columns of data with label encoded data\ntrain_X[ordinal_vars] = label_encoded_train_X[ordinal_vars]\ntest_X[ordinal_vars] = label_encoded_train_X[ordinal_vars]","08e9ffbe":"train_X.shape\ny.shape\ntest_X.shape","2a75743d":"#one hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder()\n\nOH_encoded_train_X = pd.DataFrame(OH_encoder\n                                  .fit_transform(train_X[nominal_vars])\n                                  .toarray())\nOH_encoded_test_X = pd.DataFrame(OH_encoder\n                                 .transform(test_X[nominal_vars])\n                                 .toarray())","afe1a169":"#drop nominal variables, replace them with OH_encoded variables\ntrain_X = train_X.drop(nominal_vars,axis = 1)\nfinal_train_X = pd.concat([train_X,OH_encoded_train_X],axis = 1)\n\ntest_X = test_X.drop(nominal_vars,axis = 1)\nfinal_test_X = pd.concat([test_X,OH_encoded_test_X],axis = 1)","44ff471f":"train_X.isnull().sum()\ntest_X.info()","4dd3c97c":"#splitting the training and validation data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(final_train_X, y,\n                                                    train_size = 0.8,\n                                                    test_size = 0.2, \n                                                    random_state = 0)","4c22dc2e":"y_train.shape","777d7ff8":"from sklearn.tree import DecisionTreeRegressor\n\ndt_model = DecisionTreeRegressor(random_state=0,max_leaf_nodes= 50)\ndt_model.fit(X_train,y_train)\ndt_predictions = dt_model.predict(X_valid)","5fb697e3":"#mean absolute error of the dt model\nfrom sklearn.metrics import mean_absolute_error\ndt_mae_score = mean_absolute_error(dt_predictions,y_valid)\nprint(dt_mae_score)","4b201942":"#function to fine tune RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef rf_score(n_estimators):\n    rf_model = RandomForestRegressor(random_state=1\n                                     ,n_estimators=n_estimators\n                                     ,max_leaf_nodes=430)\n    rf_model.fit(X_train,y_train.values.ravel())\n    rf_predictions = rf_model.predict(X_valid)\n    rf_mae_score = mean_absolute_error(rf_predictions,y_valid)\n    \n    return rf_mae_score\n\n\n#calling the function iteratively for different values\n\nn_estimators_value = [260,270,280,290,300,310,320,330,340]\n\n# max_leaf_nodes_value = [360,370,380,390,400,410,420,430,440,450]\n\nrf_score_list = []\nfor value in n_estimators_value:\n    rf_score_list.append(rf_score(value))\n\nprint(rf_score_list)","15c31ccd":"#randomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nrf_model = RandomForestRegressor(random_state=1,n_estimators=300,max_leaf_nodes=380)\nrf_model.fit(X_train,y_train.values.ravel())\nrf_predictions = rf_model.predict(X_valid)","9330fc83":"#mae score of random forest model\nfrom sklearn.metrics import mean_absolute_error\nrf_mae_score = mean_absolute_error(rf_predictions,y_valid)\nprint(rf_mae_score)","ed0a2a86":"#XGboost\nfrom xgboost import XGBRegressor\nxgb_model = XGBRegressor(n_estimators = 5000\n                         ,learning_rate = 0.1\n                         ,n_jobs = 4)\nxgb_model.fit(X_train, y_train.values.ravel()\n              ,early_stopping_rounds = 5\n              ,eval_set = [(X_valid, y_valid)]\n              ,verbose= False)\n\nxgb_predictions = xgb_model.predict(X_valid)","a9606e66":"#mae_score of xgboost model\nfrom sklearn.metrics import mean_absolute_error\nxgb_mae_score = mean_absolute_error(xgb_predictions,y_valid)\nprint(xgb_mae_score)","9a6463e2":"#submission\nxgb_model.fit(final_train_X,y.values.ravel())\nxgb_predictions = xgb_model.predict(final_test_X)\nids = test_df['Id']\noutput = pd.DataFrame({'Id': ids,\n                       'SalePrice': xgb_predictions})\noutput.to_csv('hr_submission_xgb_1.csv', index=False)","1ac5fd99":"Columns to be one hot encoded:\n* ['MSZoning', 'Street', 'Alley','Utilities','LotConfig','Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'MasVnrType', 'Foundation',  'Heating','CentralAir', 'Electrical','Functional', 'PavedDrive','SaleType','SaleCondition']\n\n\n","629db2d9":"Step 1: Loading Data","2b47f911":"*Perfect. Now we are ready to test out models for predictions.*","e88e0d7c":"**Machine Learning for beginners, by beginner (Regression problem)**\n* This code is self explanatory and comments are written wherever necessary to understand code.","550a1864":"**Strategy to be used to fill missing data**\n* 'LotFrontage': missing data to be filled with mean\/ median\n* Utilities(in test data): Categorical most frequent\n* Alley  : filled with zero          \n* 'MasVnrType':Categorical most frequent\n* 'MasVnrArea': mean or median\n* 'Electrical': categorical\n* FireplaceQu : zero\n* GarageType  : zero\n* GarageYrBlt : median or most frequent\n* GarageFinish,GarageQual,GarageQual, GarageCond, PoolQC, Fence,MiscFeature: zero \n* GarageQual   : zero\n* GarageCond   : zero  \n* PoolQC       : zero\n* Fence        : zero\n* MiscFeature  : zero\n**test data**\n* MSZoning     : Categorical most frequent\n* Exterior1st  : categorical   \n* Exterior2nd  : categorical \n* MasVnrType   : categorical\n* MasVnrArea   : mean \/ meadian \n* BsmtQual     : zero   \n* BsmtCond     : zero\n* BsmtExposure : zero\n* BsmtFinType1 : zero\n* BsmtFinSF1   :zero\n* BsmtFinType2 :zero\n* BsmtFinSF2   : zero\n* BsmtUnfSF    : mean or median  \n* TotalBsmtSF  : zero\n* BsmtFullBath : zero\n* BsmtHalfBath : zero\n* KitchenQual  : categorical\n* Functional   : categorical  \n* GarageArea   : zero     \n* GarageCars   : zero","86ad95d6":"Alright, let's try submitting the output","9ad186f5":"Note the columns \"Neighbourhood', \"Exterior1st\", \"Exterior2nd\" are nominal variables but are label encoded since they have high number of unique values which would highly increase the column size (threshold usually preferred is 15 unique values)","10c50d13":"**Missing Data**\n* There are ordinal variables in the missing data where \"Nan\" is equivalent to zero.\n* Categorical Data : to be filled with most frequent.\n* Numerical Data: to be filled with mean\/median.\n","f5b8c0aa":"Dealt with missing data.\nIt's time to deal with **categorical** data.\n\nWe have some ordinal and nominal data\n\nWe will one hot encode nominal data nd level encode ordinal data."}}