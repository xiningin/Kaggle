{"cell_type":{"85e61f0a":"code","c810a675":"code","4af65957":"code","d7f8ded5":"code","fcb663cd":"code","5dc1390f":"code","afb379ca":"code","bc9feb23":"code","d19ce139":"code","fe4b82ad":"code","88abb19f":"code","0aebbb08":"code","f5fdf4e7":"code","cc3613ad":"code","60815e84":"code","f9eaf40e":"code","26db6631":"code","ea1e4699":"code","d6b35950":"code","0efa1e59":"code","8c13fff9":"code","27ba52e7":"code","d632973a":"code","f92e8d8b":"code","e3f56c5e":"code","8f9723bc":"code","c6b65a36":"markdown","c14b8f4c":"markdown","396b3870":"markdown","1ebd2a30":"markdown","4382f1d2":"markdown","26ee313f":"markdown","c1610d41":"markdown","dbb1a372":"markdown","6acab39c":"markdown","1a309d80":"markdown","710b639f":"markdown","0416c305":"markdown","5d8e88b6":"markdown","dcf175b0":"markdown","29810d30":"markdown","3782b0ae":"markdown","fecac353":"markdown","a3a0a07b":"markdown","d4d00ad9":"markdown"},"source":{"85e61f0a":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c810a675":"train = pd.read_csv('\/kaggle\/input\/hacklive-3-guided-hackathon-nlp\/Train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hacklive-3-guided-hackathon-nlp\/Test.csv')\ntags = pd.read_csv('\/kaggle\/input\/hacklive-3-guided-hackathon-nlp\/Tags.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/hacklive-3-guided-hackathon-nlp\/SampleSubmission.csv')","4af65957":"train.head(3)","d7f8ded5":"TARGET_COLS = list(tags[\"Tags\"])\nTOPIC_COLS = train.iloc[:,2:6].columns","fcb663cd":"import re\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom functools import lru_cache","5dc1390f":"stemmer = SnowballStemmer(\"english\")","afb379ca":"nltk.download('stopwords')","bc9feb23":"#load processed data\ntrain = pd.read_csv('..\/input\/my-data-hacklive\/train.csv')\ntest = pd.read_csv('..\/input\/my-data-hacklive\/test.csv')","d19ce139":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model import LogisticRegression\nimport random\nrandom.seed(500)                            #setting a random seed","fe4b82ad":"# train data is split into train and validation sets\nX_train,X_val,Y_train,Y_val = train_test_split(train.iloc[:,1:6],train.loc[:,TARGET_COLS],test_size=0.2,random_state = 101)","88abb19f":"# Utility function to obtain the best thresholds for predicted probability\ndef get_best_thresholds(true, preds):\n    thresholds = [i\/100 for i in range(100)]\n    best_thresholds = []\n    for idx in range(25):\n        f1_scores = [f1_score(true[:, idx], (preds[:, idx] > thresh) * 1) for thresh in thresholds]\n        best_thresh = thresholds[np.argmax(f1_scores)]\n        best_thresholds.append(best_thresh)\n    return best_thresholds","0aebbb08":"tfidf = TfidfVectorizer(max_features=10000)\ntfidf.fit(list(train[\"ABSTRACT\"])+list(test[\"ABSTRACT\"]))\ntrain_data = tfidf.transform(X_train[\"ABSTRACT\"])\nval_data = tfidf.transform(X_val[\"ABSTRACT\"])\ntest_data = tfidf.transform(test[\"ABSTRACT\"])\n\ntrain_data = csr_matrix(np.concatenate((train_data.toarray(),X_train.loc[:,TOPIC_COLS]),axis=1).astype('float32'))\nval_data = csr_matrix(np.concatenate((val_data.toarray(),X_val.loc[:,TOPIC_COLS]),axis=1).astype('float32'))\ntest_data = csr_matrix(np.concatenate((test_data.toarray(),test.loc[:,TOPIC_COLS]),axis=1).astype('float32'))","f5fdf4e7":"tfidf_bigram = TfidfVectorizer(max_features = 10000,ngram_range=(2,2))\ntfidf_bigram.fit(list(train[\"ABSTRACT\"])+list(test[\"ABSTRACT\"]))\ntrain_data_bigram = tfidf_bigram.transform(X_train[\"ABSTRACT\"])\nval_data_bigram = tfidf_bigram.transform(X_val[\"ABSTRACT\"])\ntest_data_bigram = tfidf_bigram.transform(test[\"ABSTRACT\"])\n\ntrain_data_bigram = csr_matrix(np.concatenate((train_data_bigram.toarray(),X_train.loc[:,TOPIC_COLS]),axis=1).astype('float32'))\nval_data_bigram = csr_matrix(np.concatenate((val_data_bigram.toarray(),X_val.loc[:,TOPIC_COLS]),axis=1).astype('float32'))\ntest_data_bigram = csr_matrix(np.concatenate((test_data_bigram.toarray(),test.loc[:,TOPIC_COLS]),axis=1).astype('float32'))","cc3613ad":"del train,X_train,X_val,test","60815e84":"clf_lgb = OneVsRestClassifier(LGBMClassifier(\n                    n_estimators=100,\n                    objective='binary',\n                    verbose=1,\n                    max_depth=18 ,\n                    subsample=0.5, \n                    colsample_bytree=0.8,\n                    reg_alpha=0,\n                    reg_lambda=1\n                    ))\n                         \nclf_lgb.fit(train_data,Y_train)","f9eaf40e":"val_preds = clf_lgb.predict_proba(val_data)\nval_preds_lgb = np.copy(val_preds)                        # copy for use in ensembling\nbest_thresholds = get_best_thresholds(Y_val.values,val_preds)\n\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(Y_val.values,val_preds,average='micro')","26db6631":"test_preds = clf_lgb.predict_proba(test_data)\nlgb_test_preds = np.copy(test_preds)                    # copy for use in ensembling\n\nfor i, thresh in enumerate(best_thresholds):\n    test_preds[:, i] = (test_preds[:, i] > thresh) * 1\nsub = pd.DataFrame(test_preds,columns=list(TARGET_COLS)).astype(int)\nsample_submission.iloc[:,1:] = sub.values\n\nsample_submission.to_csv('.\/submission.csv',index=False)\nfrom IPython.display import FileLinks\nFileLinks('.\/')","ea1e4699":"clf_lgb_bigram = OneVsRestClassifier(LGBMClassifier(\n                    n_estimators=100,\n                    objective='binary',\n                    verbose=1,\n                    max_depth=18 ,\n                    subsample=0.5, \n                    colsample_bytree=0.8,\n                    reg_alpha=0,\n                    reg_lambda=1\n                    ))\n                         \nclf_lgb_bigram.fit(train_data_bigram,Y_train)","d6b35950":"val_preds = clf_lgb_bigram.predict_proba(val_data_bigram)\nval_preds_lgb_bigram = np.copy(val_preds)                        # copy for use in ensembling\nbest_thresholds = get_best_thresholds(Y_val.values,val_preds)\n\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(Y_val.values,val_preds,average='micro')","0efa1e59":"test_preds = clf_lgb_bigram.predict_proba(test_data_bigram)\nlgb_test_preds_bigram = np.copy(test_preds)                    # copy for use in ensembling\n\nfor i, thresh in enumerate(best_thresholds):\n    test_preds[:, i] = (test_preds[:, i] > thresh) * 1\nsub = pd.DataFrame(test_preds,columns=list(TARGET_COLS)).astype(int)\nsample_submission.iloc[:,1:] = sub.values\n\nsample_submission.to_csv('.\/submission.csv',index=False)\nfrom IPython.display import FileLinks\nFileLinks('.\/')","8c13fff9":"clf_logreg = OneVsRestClassifier(LogisticRegression(C=5,n_jobs=-1))\nclf_logreg.fit(train_data,Y_train)\n\nval_preds = clf_logreg.predict_proba(val_data)\nval_preds_logreg = np.copy(val_preds)\n\nbest_thresholds = get_best_thresholds(Y_val.values,val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(Y_val.values,val_preds,average='micro')","27ba52e7":"test_preds = clf_logreg.predict_proba(test_data)\nlogreg_test_preds =np.copy(test_preds)\n\nfor i, thresh in enumerate(best_thresholds):\n    test_preds[:, i] = (test_preds[:, i] > thresh) * 1\nsub = pd.DataFrame(test_preds,columns=list(TARGET_COLS)).astype(int)\nsample_submission.iloc[:,1:] = sub.values\n\nsample_submission.to_csv('.\/submission.csv',index=False)\nfrom IPython.display import FileLinks\nFileLinks('.\/')","d632973a":"clf_logreg_bigram = OneVsRestClassifier(LogisticRegression(C=5,n_jobs=-1))\nclf_logreg_bigram.fit(train_data_bigram,Y_train)\n\nval_preds = clf_logreg_bigram.predict_proba(val_data_bigram)\nval_preds_logreg_bigram = np.copy(val_preds)\n\nbest_thresholds = get_best_thresholds(Y_val.values,val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(Y_val.values,val_preds,average='micro')","f92e8d8b":"test_preds = clf_logreg_bigram.predict_proba(test_data_bigram)\nlogreg_test_preds_bigram =np.copy(test_preds)\n\nfor i, thresh in enumerate(best_thresholds):\n    test_preds[:, i] = (test_preds[:, i] > thresh) * 1\nsub = pd.DataFrame(test_preds,columns=list(TARGET_COLS)).astype(int)\nsample_submission.iloc[:,1:] = sub.values\n\nsample_submission.to_csv('.\/submission.csv',index=False)\nfrom IPython.display import FileLinks\nFileLinks('.\/')","e3f56c5e":"val_preds = (val_preds_lgb+val_preds_lgb_bigram+val_preds_logreg+val_preds_logreg_bigram)\/4\nbest_thresholds = get_best_thresholds(Y_val.values,val_preds)\nfor i, thresh in enumerate(best_thresholds):\n    val_preds[:, i] = (val_preds[:, i] > thresh) * 1\n\nf1_score(Y_val.values,val_preds,average='micro')","8f9723bc":"test_preds = (lgb_test_preds+lgb_test_preds_bigram+logreg_test_preds+logreg_test_preds_bigram)\/4\nfor i, thresh in enumerate(best_thresholds):\n    test_preds[:, i] = (test_preds[:, i] > thresh) * 1\nsub = pd.DataFrame(test_preds,columns=list(TARGET_COLS)).astype(int)\nsample_submission.iloc[:,1:] = sub.values\n\nsample_submission.to_csv('.\/submission.csv',index=False)\nfrom IPython.display import FileLinks\nFileLinks('.\/')","c6b65a36":"![](https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/cover_copy-thumbnail-1200x1200-90.jpg)","c14b8f4c":"# <a id='model'>Model Building<\/a>\nThe models used here are the LightGbm and logistic regression models. The parmeters for these models were selected using `RandomizedSearchCV`. Individual models were built for unigram and bigram data. Therefore we have a total of four models. The predicted probabilities for these models were averaged and using the `get_best_thresholds` function defined earlier, the optimum probability thresholds were obtained. ","396b3870":"### unigram model","1ebd2a30":"### Tfidf vectorization of the text data and then concatenate with the topic columns","4382f1d2":"### average results","26ee313f":"### bigram model","c1610d41":"# Welcome\n# Table of contents\n<ol style=\"background-color: #c2d6d6;\">\n    <li> <a  href=\"#about\"> About the Competition<\/a><\/li>\n    <li> <a  href=\"#desc\"> Brief description of Approach<\/a><\/li>\n    <li> <a  href=\"#data\"> About the Data <\/a> <\/li>\n    <li> <a  href=\"#text\"> Text Preprocessing <\/a> <\/li>\n    <li> <a  href=\"#prep\"> Data Preparation <\/a> <\/li>\n    <li> <a  href=\"#model\"> Model Building <\/a> <\/li>\n    <li> <a  href=\"#conc\"> Conclusion <\/a> <\/li>\n<\/ol>\n    ","dbb1a372":"# Logistic Regression","6acab39c":"# <a id=\"data\"> About the Data<\/a>\nAs seen below the data conisists the abstracts of certain articles pertaining to the domains of Computer Science, Mathematics, Physics and Statistics. The articles have certain tags associated with them. Given the abstract and the domain for a article, our job is to predict the tags associated with it. <a style=\"color: orange;\">Note: One article may have multiple tags<\/a>.<br><br>\nIn the train data, the ABSTRACT column contains the abstract of the article. The binary columns of Computer Science, Mathematics, Physics and Statistics convey information regarding the domain and the next 25 columns are the tags associated with(1 implying True)","1a309d80":"# <a id=\"desc\"> Brief Description of the Approach<\/a>\n\nThis notebook describes my approach in detail. The summary for the same is as follows:\n* After cleaning the text data by removing special characters and stemming the words the data is divided into train and test set\n* The approach is based on building unigram and bigram models and then averaging the results from these models.\n* The particular algorithms being used in this approach are Logistic Regression and LightGBM.\n* Separate unigram and bigram models were built for both these algorithms and the predicted probabilites were averaged out.\n* The probability threshold was selected individually for each columns based on the predictions on the validation set.","710b639f":"# <a id=\"text\"> Text Preprocessing<\/a>\n\nText preprocessing refers to cleaning the text given in the `ABSTRACT` column. For cleaning the text the following steps have been performed.\n* Removing all special characters and retaining only the Alpha-Numeric characters.\n* Removing the single lettered words.\n* Stemming the words using SnowballStemmer and then repeating the previous step.","0416c305":"### Unigram model","5d8e88b6":"# <a id=\"about\">About The Competition<\/a>\n\nThis notebooks tries to present an approach to the competition of [Guided NLP hackathon](\"https:\/\/datahack.analyticsvidhya.com\/contest\/hacklive-3-guided-hackathon-text-classification\/#About\") hosted on Analytics Vidhya. The problem statement is as follows:<br>\n<pre>\nGiven the abstracts for a set of research articles, predict the tags for each article included in the test set. \nNote that a research article can possibly have multiple tags. The research article abstracts are sourced from the following 4 topics: \n\n1. Computer Science\n\n2. Mathematics\n\n3. Physics\n\n4. Statistics\n\nList of possible tags are as follows:\n\n[Tags, Analysis of PDEs, Applications, Artificial Intelligence,Astrophysics of Galaxies, Computation and Language, Computer Vision and Pattern Recognition, Cosmology and Nongalactic Astrophysics, Data Structures and Algorithms, Differential Geometry, Earth and Planetary Astrophysics, Fluid Dynamics,Information Theory, Instrumentation and Methods for Astrophysics, Machine Learning, Materials Science, Methodology, Number Theory, Optimization and Control, Representation Theory, Robotics, Social and Information Networks, Statistics Theory, Strongly Correlated Electrons, Superconductivity, Systems and Control]<\/pre>\n","dcf175b0":"# LGBM ","29810d30":"### Bigram model","3782b0ae":"# <a id=\"prep\"> Data Preparation<\/a>\n\nSince we aim to build bigram and unigram models, the tfidf vectorizer from sklearn was used with the `n_gram_range` parameter. The obtained tfidf matrix was then concatenated with the respective topic columns ( reffering to Computer Science, Maths etc.)","fecac353":"Since the above cell takes some time to run, I stored the processed data onto a csv file and for subsequent operations the data was loaded from this file.","a3a0a07b":"# cell was run only once and data was stored into csv file\n\n<pre>\nstem = lru_cache(maxsize=10000)(stemmer.stem)\ndef clean_text(col):\n        col = re.sub('[^a-zA-Z0-9]',' ',col)\n        col = col.lower()                                                                     # to lowercase\n        col = col.split(' ')                                                                  # split into words\n        col = [word for word in col if len(col)>1]\n        col = [stem(word) for word in col if not word in stopwords.words('english')]          # remove stopwords and stem\n        col = [word for word in col if len(col)>1]                                            #remove single character\n        return ' '.join(col)\n\ntrain[\"ABSTRACT\"] = train[\"ABSTRACT\"].apply(clean_text)\n\ntest[\"ABSTRACT\"] = test[\"ABSTRACT\"].apply(clean_text)\n<\/pre>","d4d00ad9":"# <a id='conc'> Conclusion<\/a>\nThe approach mentioned in this notebook is a very simple one. There is huge scope for improvement for this approach since it was formulated in somewhat of a hurry. The objective of this kernel however is to demonstrate the use of non neural net based algorithms  in NLP to obtain a good score. Thank you for reading this far."}}