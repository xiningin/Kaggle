{"cell_type":{"5509c01a":"code","ad38ed4f":"code","bf95ea73":"code","4e6b0f73":"code","b92ac155":"code","9ae7dd2b":"code","819aa470":"code","46502eb1":"code","7485740c":"code","a3951df8":"code","66dad444":"code","f29bf5dd":"code","1ddb8726":"markdown","f020ab95":"markdown","1831ee26":"markdown","a7f859f8":"markdown"},"source":{"5509c01a":"!pip install ..\/input\/timm034\/timm-0.3.4-py3-none-any.whl","ad38ed4f":"#!python ..\/input\/ttach-master\/setup.py install","bf95ea73":"import time\nimport os\nimport timm\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n#from efficientnet_pytorch import model as enet\nimport albumentations\nfrom albumentations.pytorch import ToTensorV2\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n#import ttach as tta","4e6b0f73":"image_folder = '..\/input\/cassava-leaf-disease-classification\/test_images\/'\n\nenet_type = 'cspresnext50'\nimage_size = 512\nbatch_size = 4\nnum_workers = 4\nout_dim = 5\n\ndevice = torch.device('cuda')","b92ac155":"ls ..\/input\/cdl-cspresnext50-512\/ ","9ae7dd2b":"model_pths = [\n    '..\/input\/cdl-cspresnext50-512\/light_best_model_fold0.pth',\n    '..\/input\/cdl-cspresnext50-512\/light_best_model_fold1.pth',\n    '..\/input\/cdl-cspresnext50-512\/light_best_model_fold2.pth',\n    '..\/input\/cdl-cspresnext50-512\/light_best_model_fold3.pth',\n    '..\/input\/cdl-cspresnext50-512\/light_best_model_fold4.pth',\n            ]","819aa470":"class net(nn.Module):\n    def __init__(self, model_name=enet_type, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        \n        n_features = self.model.head.fc.in_features\n        self.model.head.fc = nn.Linear(n_features, 5)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output","46502eb1":"class LEAFDataset(Dataset):\n    def __init__(self, folder, transforms=None):\n\n        self.file_names = os.listdir(folder)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.file_names)\n\n    def __getitem__(self, index):\n        image_id = self.file_names[index]\n        \n        image_file = os.path.join(image_folder, image_id)\n        image = cv2.imread(image_file)\n        image = image[:, :, ::-1]\n\n        if self.transforms is not None:\n            image = self.transforms(image=image)['image']\n\n        return image, image_id","7485740c":"transform = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize(),\n    ToTensorV2()\n])","a3951df8":"# ====================================================\n# inference\n# ====================================================\n\nres = []\nfor model_pth in model_pths:\n    model = net(enet_type)\n    model.load_state_dict(torch.load(model_pth))\n    model.eval()\n    model.to(device)\n    \n    test_dataset = LEAFDataset(image_folder, transforms=transform)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers)\n\n    single_model_probs = []\n    image_ids_list = []\n    with torch.no_grad():\n        for idx, (images, image_ids) in enumerate(tqdm(test_loader)):\n            outputs = model(images.to(device))\n            pred = outputs.detach().softmax(1).cpu().numpy()\n            single_model_probs.append(pred)\n            image_ids_list.append(image_ids)\n#             if idx == 5:\n#                 break\n        \n    single_model_probs = np.concatenate(single_model_probs)\n    image_ids_list = np.concatenate(image_ids_list)\n    res.append(single_model_probs)\n    \n    del model\n    torch.cuda.empty_cache()\n    \nres = sum(res) \/ len(model_pths)\nprobs = res.argmax(1)","66dad444":"sub = pd.DataFrame({'image_id': image_ids_list, 'label': probs});sub.head()","f29bf5dd":"sub.to_csv('submission.csv', index=False)","1ddb8726":"# Model","f020ab95":"# Config","1831ee26":"# Augmentations","a7f859f8":"# Dataset"}}