{"cell_type":{"f57a81ce":"code","9901b0f1":"code","fd3b0eb9":"code","8caf5e54":"code","97a0cc6e":"code","eb4f183a":"code","74d4b93e":"code","b4b05aef":"code","5d5e0936":"code","5b558eba":"code","0d38a06a":"code","d36d7344":"code","9bc1a4d2":"code","970f68c5":"code","bc5f7e6c":"code","e6c88b02":"code","adb08b5c":"code","b8eb5bc9":"code","f31cde84":"code","0c40dcdb":"markdown","6750766b":"markdown","4463e723":"markdown","a48c679f":"markdown","05074da9":"markdown","1c4a697c":"markdown","d0b9cfd1":"markdown","79ec68c5":"markdown","0ed195bc":"markdown","61c1083d":"markdown","7ecaf218":"markdown","a28c6887":"markdown","2c85ec54":"markdown","800ece6d":"markdown","865ee754":"markdown","6b967781":"markdown","cbcc1646":"markdown","eaf58eb8":"markdown"},"source":{"f57a81ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9901b0f1":"dataset_df = pd.read_csv('..\/input\/Churn_Modelling.csv')\ndataset_df.head()","fd3b0eb9":"dataset_df.drop(['RowNumber', 'CustomerId'], axis=1).describe()","8caf5e54":"dataset_df.corr()","97a0cc6e":"dataset_df['Exited'].sum()","eb4f183a":"print(\"Surname:\")\nprint(dataset_df['Surname'].nunique())\nprint(dataset_df['Surname'].unique())\nprint()\nprint(\"Geography:\")\nprint(dataset_df['Geography'].nunique())\nprint(dataset_df['Geography'].unique())\nprint()\nprint(\"Gender:\")\nprint(dataset_df['Gender'].nunique())\nprint(dataset_df['Gender'].unique())","74d4b93e":"dataset_df = dataset_df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\ndataset_df.head()","b4b05aef":"dataset_df = pd.get_dummies(dataset_df)\ndataset_df.head()","5d5e0936":"features = np.array(dataset_df.drop('Exited', axis=1))\nfeature_names = dataset_df.drop('Exited', axis=1).columns.tolist()\nlabels = np.array(dataset_df['Exited'])\nfeatures, feature_names, labels","5b558eba":"features = (features - features.mean(axis=0, keepdims=True)) \/ features.std(axis=0, keepdims=True)\nfeatures","0d38a06a":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\nfrom collections import OrderedDict","d36d7344":"class ChurnModellingDataset(Dataset):\n    def __init__(self):\n        global features, labels\n        assert features.shape[0] == labels.shape[0], \"The lengths of features and labels do not match\"\n\n        self.feature_names = feature_names\n        self.features = torch.tensor(features, dtype=torch.float)\n        self.labels = torch.tensor(labels)\n        \n    def __len__(self):\n        return self.features.shape[0]\n    \n    def __getitem__(self, idx):\n        return (self.features[idx], self.labels[idx])\n    \n    def get_feature_names(self):\n        return self.feature_names\n\ndataset = ChurnModellingDataset()\nlen(dataset), dataset[0], len(dataset.get_feature_names())","9bc1a4d2":"test_size = 0.1\nshuffle = True\n\n# Taking the same proportion of examples from both classes\ntrue_idx = dataset[:][1].eq(1).nonzero().view(-1).long()\nfalse_idx = dataset[:][1].eq(0).nonzero().view(-1).long()\n\nt_new_idx = true_idx[torch.randperm(len(true_idx))] if shuffle else true_idx\nf_new_idx = false_idx[torch.randperm(len(false_idx))] if shuffle else false_idx\n\n# trying out matching the number of examples for each class\nf_new_idx = f_new_idx[:(len(t_new_idx) \/\/ 2) * 2]\n\nt_test_len = int(round(len(t_new_idx) * test_size))\nf_test_len = int(round(len(f_new_idx) * test_size))\ntrain_idx = torch.cat((t_new_idx[:len(t_new_idx) - t_test_len], f_new_idx[:len(f_new_idx) - f_test_len]))\nvalidation_idx = torch.cat((t_new_idx[len(t_new_idx) - t_test_len:], f_new_idx[len(f_new_idx) - f_test_len:]))\n\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalidation_sampler = SubsetRandomSampler(validation_idx)\n\ntrainloader = DataLoader(dataset, batch_size=len(train_sampler), sampler=train_sampler)\nvalidationloader = DataLoader(dataset, batch_size=len(validation_sampler), sampler=validation_sampler)\nlen(trainloader), len(train_sampler), len(validationloader), len(validation_sampler), next(iter(trainloader))","970f68c5":"class NAC(nn.Module):\n    \"\"\"Neural Accumulator from Google Deepmind\"\"\"\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        \n        self.in_features = in_features\n        self.out_features = out_features\n        \n        self.sigmoid_weight = nn.Parameter(init.xavier_normal_(torch.empty(in_features, out_features)))\n        self.tanh_weight = nn.Parameter(init.xavier_normal_(torch.empty(in_features, out_features)))\n        \n    def forward(self, x):\n        W = self.sigmoid_weight.sigmoid() * self.tanh_weight.tanh()\n        a = torch.mm(x, W)\n        return a\n    \n    def extra_repr(self):\n        return f\"in_features={self.in_features}, out_features={self.out_features}\"\n\nclass NALU(nn.Module):\n    \"\"\"Neural Arithmetic Logic Unit from Google Deepmind\"\"\"\n    def __init__(self, in_features, out_features, epsilon=1e-8):\n        super().__init__()\n        \n        self.in_features = in_features\n        self.out_features = out_features\n        self.epsilon = epsilon\n\n        self.gate_weight = nn.Parameter(init.xavier_normal_(torch.empty(in_features, 1)))\n        self.accumulator = NAC(in_features, out_features)\n        self.multiplier = NAC(in_features, out_features)\n        \n    def forward(self, x):\n        a = self.accumulator(x)\n        m = torch.exp(self.multiplier(torch.log(x.abs() + self.epsilon)))\n        g = torch.mm(x, self.gate_weight)\n        \n        y = g * a + (1 - g) * m\n        return y\n    \n    def extra_repr(self):\n        return f\"in_features={self.in_features}, out_features={self.out_features}, epsilon={self.epsilon}\"\n\nclass ChurnClassifier(nn.Module):\n    def __init__(self, in_features, out_features, hidden_sizes=[], dropout=0):\n        super().__init__()\n\n        # Constructing layers\n        previous_size = in_features\n        for layer_num, hidden_size in enumerate(hidden_sizes, 1):\n            self.add_module(f\"hidden_layer{layer_num}\",\n                nn.Sequential(\n                    OrderedDict(\n                        [\n                            ('fc', nn.Linear(previous_size, hidden_size)),\n                            ('relu', nn.ReLU()),\n                            ('norm', nn.LayerNorm(hidden_size)),\n                            ('dropout', nn.Dropout(p=dropout))\n                        ]\n                    )\n                )\n            )\n            previous_size = hidden_size\n\n        self.add_module(\"output_layer\",\n            nn.Sequential(\n                OrderedDict(\n                    [\n                        ('fc', nn.Linear(previous_size, out_features))\n                    ]\n                )\n            )\n        )\n        \n        self.initialize_weights()\n        \n    def forward(self, x):\n        # Process the input sequentially through the layers of the model\n        for layer in self.children():\n            x = layer(x)\n        \n        return x\n    \n    def num_params(self, trainable_only=False):\n        # Total number of parameters\n        if trainable_only:\n            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n        else:\n            return sum(param.numel() for param in self.parameters())\n        \n    def initialize_weights(self):\n        # Initialize the weights and biases\n        for param in self.parameters():\n            if len(param.shape) > 1:\n                init.xavier_normal_(param.data)\n            else:\n                param.data.zero_()\n                \nclass ChurnClassifierWithNALU(nn.Module):\n    def __init__(self, in_features, out_features, nalu_sizes=[], hidden_sizes=[], dropout=0, epsilon=1e-8, input_normalize=True):\n        super().__init__()\n        \n        # Layers\n        if input_normalize:\n            self.add_module('norm0', nn.LayerNorm(in_features))\n\n        previous_size = in_features\n\n        if len(nalu_sizes) != 0:\n            nalu_layers = OrderedDict()\n            for layer_num, nalu_size in enumerate(nalu_sizes, 1):\n                nalu_layers[f\"nalu_layer{layer_num}\"] = nn.Sequential(OrderedDict([\n                    ('nalu', NALU(previous_size, nalu_size, epsilon=epsilon)),\n                    ('norm', nn.LayerNorm(nalu_size)),\n                    ('dropout', nn.Dropout(p=dropout))\n                ]))\n                previous_size = nalu_size\n            self.add_module('nalu_layers', nn.Sequential(nalu_layers))\n        \n        self.add_module('classifier', ChurnClassifier(in_features=previous_size, out_features=out_features, hidden_sizes=hidden_sizes, dropout=dropout))\n        \n    def forward(self, x):\n        # Process the input sequentially through the layers of the model\n        for layer in self.children():\n            x = layer(x)\n        \n        return x\n    \n    def num_params(self, trainable_only=False):\n        # Total number of parameters\n        if trainable_only:\n            return sum(param.numel() for param in self.parameters() if param.requires_grad)\n        else:\n            return sum(param.numel() for param in self.parameters())\n        \n    def initialize_weights(self):\n        # Initialize the weights and biases\n        for param in self.parameters():\n            if len(param.shape) > 1:\n                init.xavier_normal_(param.data)\n            else:\n                param.data.zero_()","bc5f7e6c":"model = ChurnClassifierWithNALU(in_features=13, out_features=2, nalu_sizes=[], hidden_sizes=[500, 500, 500, 500], dropout=0.2)\nprint(model)\nprint(\"Total number of trainable parameters :\", model.num_params(True))","e6c88b02":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1.0, weight_decay=0.01)","adb08b5c":"epoch = 0\ntrain_logs = {'losses': [], 'accuracies': [], 'recalls': [], 'precisions': [], 'f1_scores': []}\nvalidation_logs = {'losses': [], 'accuracies': [], 'recalls': [], 'precisions': [], 'f1_scores': []}","b8eb5bc9":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\ncriterion.to(device)\nmodel.train()\n\nepochs = 1000\nprint_every = 50  # epochs\n# threshold = 0.5  # prediction value greater than this is interpreted as a prediction of 1\n\nfor e in range(epochs):\n    running_train_loss = 0\n    running_train_accuracy = 0\n    running_train_recall = 0\n    running_train_precision = 0\n    running_train_f1 = 0\n    for features, labels in trainloader:\n        features = features.to(device)\n        labels = labels.to(device)\n        \n        output = model(features)\n        loss = criterion(output, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        current_loss = loss.item()\n        current_accuracy = torch.mean((output.argmax(dim=1) == labels).type(torch.FloatTensor)).item()\n        current_recall = torch.mean((output.argmax(dim=1) == labels)[labels == 1].type(torch.FloatTensor)).item()\n        current_precision = torch.mean((output.argmax(dim=1) == labels)[output.argmax(dim=1) == 1].type(torch.FloatTensor)).item() if (output.argmax(dim=1) == 1).sum() != 0 else 0\n        current_f1 = 2 * current_recall * current_precision \/ (current_recall + current_precision) if current_recall > 0 and current_precision > 0 else 0\n\n        running_train_loss += current_loss * features.shape[0]\n        running_train_accuracy += current_accuracy * features.shape[0]\n        running_train_recall += current_recall * features.shape[0]\n        running_train_precision += current_precision * features.shape[0]\n        running_train_f1 += current_f1 * features.shape[0]\n    running_train_loss \/= len(trainloader.sampler)\n    running_train_accuracy \/= len(trainloader.sampler)\n    running_train_recall \/= len(trainloader.sampler)\n    running_train_precision \/= len(trainloader.sampler)\n    running_train_f1 \/= len(trainloader.sampler)\n    \n    model.eval()\n    running_validation_loss = 0\n    running_validation_accuracy = 0\n    running_validation_recall = 0\n    running_validation_precision = 0\n    running_validation_f1 = 0\n    with torch.no_grad():\n        for features, labels in validationloader:\n            features = features.to(device)\n            labels = labels.to(device)\n\n            output = model(features)\n            loss = criterion(output, labels)\n\n            current_loss = loss.item()\n            current_accuracy = torch.mean((output.argmax(dim=1) == labels).type(torch.FloatTensor)).item()\n            current_recall = torch.mean((output.argmax(dim=1) == labels)[labels == 1].type(torch.FloatTensor)).item()\n            current_precision = torch.mean((output.argmax(dim=1) == labels)[output.argmax(dim=1) == 1].type(torch.FloatTensor)).item() if (output.argmax(dim=1) == 1).sum() != 0 else 0\n            current_f1 = 2 * current_recall * current_precision \/ (current_recall + current_precision) if current_recall > 0 and current_precision > 0 else 0\n\n            running_validation_loss += current_loss * features.shape[0]\n            running_validation_accuracy += current_accuracy * features.shape[0]\n            running_validation_recall += current_recall * features.shape[0]\n            running_validation_precision += current_precision * features.shape[0]\n            running_validation_f1 += current_f1 * features.shape[0]\n    running_validation_loss \/= len(testloader.sampler)\n    running_validation_accuracy \/= len(testloader.sampler)\n    running_validation_recall \/= len(testloader.sampler)\n    running_validation_precision \/= len(testloader.sampler)\n    running_validation_f1 \/= len(testloader.sampler)\n    model.train()\n    \n    train_logs['losses'].append(running_train_loss)\n    train_logs['accuracies'].append(running_train_accuracy)\n    train_logs['recalls'].append(running_train_recall)\n    train_logs['precisions'].append(running_train_precision)\n    train_logs['f1_scores'].append(running_train_precision)\n    validation_logs['losses'].append(running_validation_loss)\n    validation_logs['accuracies'].append(running_validation_accuracy)\n    validation_logs['recalls'].append(running_validation_recall)\n    validation_logs['precisions'].append(running_validation_precision)\n    validation_logs['f1_scores'].append(running_validation_precision)\n    \n    if e == 0 or (e + 1) % print_every == 0:\n        print(f\"Epoch : {epoch}\")\n        print(f\"Training Loss : {running_train_loss}\")\n        print(f\"Training Accuracy : {round(running_train_accuracy * 100, 2)}%\")\n        print(f\"Training Recall : {round(running_train_recall * 100, 2)}%\")\n        print(f\"Training Precision : {round(running_train_precision * 100, 2)}%\")\n        print(f\"Training F1 Score : {round(running_train_f1, 4)}\")\n        print(f\"Validation Loss : {running_validation_loss}\")\n        print(f\"Validation Accuracy : {round(running_validation_accuracy * 100, 2)}%\")\n        print(f\"Validation Recall : {round(running_validation_recall * 100, 2)}%\")\n        print(f\"Validation Precision : {round(running_validation_precision * 100, 2)}%\")\n        print(f\"Validation F1 Score : {round(running_validation_f1, 4)}\")\n        print()\n    \n    epoch += 1","f31cde84":"fig, axes = plt.subplots(2, 2, figsize=(24, 24))\n\naxes[0, 0].set_title('Losses')\naxes[0, 0].set_xlabel('epoch')\naxes[0, 0].set_ylabel(\"loss\")\naxes[0, 0].grid()\naxes[0, 0].plot(range(len(train_logs['losses'])), train_logs['losses'], color='blue')\naxes[0, 0].plot(range(len(train_logs['losses'])), validation_logs['losses'], color='orange')\naxes[0, 0].set_xlim((0, None))\naxes[0, 0].legend(['Training', 'Test'])\n\naxes[0, 1].set_title('Accuracies')\naxes[0, 1].set_xlabel('epoch')\naxes[0, 1].set_ylabel(\"accuracy\")\naxes[0, 1].grid()\naxes[0, 1].plot(range(len(train_logs['accuracies'])), train_logs['accuracies'], color='blue')\naxes[0, 1].plot(range(len(train_logs['accuracies'])), validation_logs['accuracies'], color='orange')\naxes[0, 1].set_xlim((0, None))\naxes[0, 1].set_ylim((0, 1))\naxes[0, 1].legend(['Training', 'Test'])\n\naxes[1, 0].set_title('Recalls')\naxes[1, 0].set_xlabel('epoch')\naxes[1, 0].set_ylabel(\"recall\")\naxes[1, 0].grid()\naxes[1, 0].plot(range(len(train_logs['recalls'])), train_logs['recalls'], color='blue')\naxes[1, 0].plot(range(len(train_logs['recalls'])), validation_logs['recalls'], color='orange')\naxes[1, 0].set_xlim((0, None))\naxes[1, 0].set_ylim((0, 1))\naxes[1, 0].legend(['Training', 'Test'])\n\naxes[1, 1].set_title('Precisions')\naxes[1, 1].set_xlabel('epoch')\naxes[1, 1].set_ylabel(\"precision\")\naxes[1, 1].grid()\naxes[1, 1].plot(range(len(train_logs['precisions'])), train_logs['precisions'], color='blue')\naxes[1, 1].plot(range(len(train_logs['precisions'])), validation_logs['precisions'], color='orange')\naxes[1, 1].set_xlim((0, None))\naxes[1, 1].set_ylim((0, 1))\naxes[1, 1].legend(['Training', 'Test'])\n\nfig.show();","0c40dcdb":"Labels? (Total number of positive cases)","6750766b":"Create a custom dataset.","4463e723":"Starting with reading the dataset file and checking the first few rows","a48c679f":"Since the range of values are inconsistent over different features, it should be normalized.","05074da9":"# Reading and checking the dataset","1c4a697c":"Number and names of categorical entries","d0b9cfd1":"# Visualizing the training progresses","79ec68c5":"RowNumber, CustomerId, and Surname features seem irrelevant for predicting the labels. Let's drop it.","0ed195bc":"Now, the categorical values (Geography, Gender) should be converted into dummy variables.","61c1083d":"## Now, it's time for training!","7ecaf218":"Almost there! Creating a model here!|","a28c6887":"With all data converted to numerical values, it's time to put them into a matrix!","2c85ec54":"# Moving on to PyTorch!","800ece6d":"# Preparing the data","865ee754":"Create 2 dataloaders: a training set loader and a validation set loader.","6b967781":"Import necessary modules","cbcc1646":"Perhaps some statistics?","eaf58eb8":"Correlation?"}}