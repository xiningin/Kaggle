{"cell_type":{"d10ad273":"code","cfe3db80":"code","18cd87c7":"code","409b39de":"code","bdc4a343":"code","92b10cbe":"code","d8f49cd4":"code","fdaee41b":"code","f7df3f36":"code","43f888b3":"code","40a2bab9":"code","dd1ade10":"code","ea4baf06":"code","b0e4836f":"code","30e2a398":"code","5271ebf1":"code","fc678014":"markdown","a3846b7f":"markdown","c1b6b728":"markdown","96aee569":"markdown"},"source":{"d10ad273":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cfe3db80":"train = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","18cd87c7":"train['review'][0]","409b39de":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n","bdc4a343":"from nltk.stem.porter import PorterStemmer\nporter = PorterStemmer()\n\ndef stemmer(text):\n    return [porter.stem(word) for word in text.split()]","92b10cbe":"stemmer(train['review'][0])","d8f49cd4":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(strip_accents = None, lowercase = False, tokenizer = stemmer, use_idf = True, norm ='l2',smooth_idf = True )","fdaee41b":"Y = train.sentiment.values\nX = tfidf.fit_transform(train.review)","f7df3f36":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.5, shuffle=False)","43f888b3":"import pickle\nfrom sklearn.linear_model import LogisticRegressionCV\n\nlogit = LogisticRegressionCV(cv=5, scoring='accuracy', max_iter=100)\nlogit.fit(X_train,y_train)","40a2bab9":"filename = 'lr.sav'\npickle.dump(logit, open(filename, 'wb'))","dd1ade10":"logit = pickle.load(open(filename, 'rb'))\n# get_accuracy_metrics(y_test, logit.predict(X_test))","ea4baf06":"logit.score(X_test,y_test)","b0e4836f":"y_pred = logit.predict(X_test)","30e2a398":"def get_accuracy_metrics(y_test, y_hat):\n\n  #Generating accuracy score\n  print(\"\\nAccuracy attained : {}\\n\".format(accuracy_score(y_hat,y_test)))\n\n  #Getting the classification matrix\n  print(\"The classification report is :\\n\\n{}\".format(classification_report(y_test, y_hat)))\n\n  #Confusion matrix\n  # print(\"Confusion matrix generated\"confusion_matrix(y_test, y_hat))\n  print(\"\\n\")\n  sns.heatmap(confusion_matrix(y_test, y_hat),annot=True,fmt='g', square=True)\n\ndef plot_accuracies(y_hat,model=''):\n  x,accs=[],[]\n  max_x,max_acc = 0,0\n\n  #iterating through various values of threshold possible ie 0-100\n  for i in range(0,105,5):\n    \n    x.append(i)\n    z=[]\n    \n    #checking if the probability is greater than threshold for each y_hat predicted\n    for row in y_hat*100:\n      if max(row)>i:\n        z.append(np.argmax(row))\n      else :\n        z.append(np.argmin(row))\n    \n    #Generating a list for accuracy scores\n    accs.append(accuracy_score(z,y_test))\n    \n    if accuracy_score(z,y_test) >= max_acc : \n      max_acc = accuracy_score(z,y_test)\n      max_x = i\n    else : \n      continue\n\n  #Plotting function\n  plt.figure(figsize=(10,5))\n  plt.plot(x,accs)\n  plt.axvline(x=max_x,label='Maximum accuracy at x = {}, value is {}'.format(max_x, max_acc), color='red')\n  plt.title(\"Accuracies of various thresholds of {}\".format(model))\n  plt.xlabel(\"Value of threshold\")\n  plt.ylabel(\"Accuracy\")\n  plt.legend()\n\n  return accs,x","5271ebf1":"import numpy    as np\nimport pandas   as pd\nimport seaborn  as sns\n\nimport pickle\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing    import StandardScaler\nfrom sklearn.preprocessing    import RobustScaler\nfrom sklearn.preprocessing    import MinMaxScaler\nfrom sklearn.model_selection  import train_test_split\nfrom sklearn.metrics          import accuracy_score\nfrom sklearn.metrics          import classification_report\nfrom sklearn.metrics          import confusion_matrix\n\nfrom sklearn.decomposition  import PCA\nget_accuracy_metrics(y_test, y_pred)","fc678014":"### 5. Classification using Logistic Regression","a3846b7f":"### 3. Stemming","c1b6b728":"### 2. Data Preperation ","96aee569":"### 4. Vectorizing of document"}}