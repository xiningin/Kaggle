{"cell_type":{"153166f9":"code","a7e79d70":"code","826f185f":"code","cbaeed01":"code","d520bbdc":"code","f91186d6":"code","fcb22d58":"code","ab2bef0c":"code","c0ebd033":"markdown","14e81d7f":"markdown","8bd658c4":"markdown","98145571":"markdown","b7b93e65":"markdown","5ab24133":"markdown","955df1c1":"markdown","3f97413a":"markdown","c9b04a8b":"markdown","4cfde9e1":"markdown","e1fb4b40":"markdown","62bd8fae":"markdown","8a629258":"markdown","7ec77b2d":"markdown","94e14ddd":"markdown"},"source":{"153166f9":"import numpy as np\nfrom matplotlib import pyplot as plt\nfrom scipy import stats\n\nspace = np.linspace(0, 1, 1000)\nfig, ax = plt.subplots(figsize = (20,10))\nfor a, b in [(0,0), (2, 8), (10, 40), (20, 80), (40, 160)]:\n    ax.plot(space, stats.beta.pdf(space, a+1, b+1),label=f'a={a}, b={b}')\nax.legend(prop={'size': 20})","a7e79d70":"fig, ax = plt.subplots(figsize = (20,10))\nfor a, b, proba in [(2, 9, '20%'), (8, 7, '50%'), (15, 3, '80%')]:\n    ax.plot(space, stats.beta.pdf(space, a+1, b+1),label=proba)\nax.legend(prop={'size': 20})","826f185f":"g, ax = plt.subplots(figsize = (20,10))\nfor a, b, proba in [(3, 11, '20%'), (12, 11, '50%'), (45, 12, '80%')]:\n    ax.plot(space, stats.beta.pdf(space, a+1, b+1),label=proba)\nax.legend(prop={'size': 20})","cbaeed01":"!pip install kaggle-environments --upgrade","d520bbdc":"%%writefile submission.py\n\nimport json\nimport numpy as np\nimport pandas as pd\n\nbandit_state = None\ntotal_reward = 0\nlast_step = None\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    global history, history_bandit\n\n    step = 1.0 #you can regulate exploration \/ exploitation balacne using this param\n    decay_rate = 0.97 # how much do we decay the win count after each call\n    \n    global bandit_state,total_reward,last_step\n        \n    if observation.step == 0:\n        # initial bandit state\n        bandit_state = [[1,1] for i in range(configuration[\"banditCount\"])]\n    else:       \n        # updating bandit_state using the result of the previous step\n        last_reward = observation[\"reward\"] - total_reward\n        total_reward = observation[\"reward\"]\n        \n        # we need to understand who we are Player 1 or 2\n        player = int(last_step == observation.lastActions[1])\n        \n        if last_reward > 0:\n            bandit_state[observation.lastActions[player]][0] += last_reward * step\n        else:\n            bandit_state[observation.lastActions[player]][1] += step\n        \n        bandit_state[observation.lastActions[0]][0] = (bandit_state[observation.lastActions[0]][0] - 1) * decay_rate + 1\n        bandit_state[observation.lastActions[1]][0] = (bandit_state[observation.lastActions[1]][0] - 1) * decay_rate + 1\n\n#     generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in range(configuration[\"banditCount\"]):\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    last_step = best_agent\n    return best_agent","f91186d6":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","fcb22d58":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.reset()\nenv.run([\"random_agent.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","ab2bef0c":"env.reset()\nenv.run([\"submission.py\", \"submission.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","c0ebd033":"After 40-50 rounds, we can see that distributions are already very different, and now when we sample p_i, most probably the highest number will have a bandit with the highest win rate. So, we will call the best bandit more and more often and maximize the total reward. After 100 steps, we will see something like that:","14e81d7f":"# Competition specific changes","8bd658c4":"This notebook shows how to use multi-armed bandit.\n\nMulti-armed bandit is a widely used RL-algorithm because it is very balanced in terms of exploitation\/exploration.\n\nAlgorithm logic:\n\n- At each step for each bandit generate a random number from B(a+1, b+1). B - beta-distribution, a - total reward from this bandit, b - number of this bandits's historical losses.\n- Select the bandit with the largest generated number and use it to generate the next step.\n- Repeat","98145571":"## Beta-distribution","b7b93e65":"Initially all bandits have a = 0, b = 0. Beta distribution for each of them is uniform, and we just select the random bandit. After a few steps, the distributions will be different - for high win probability bandits, the win rate will be higher. Let's look at some artificial example: ","5ab24133":"# Intro","955df1c1":"# Multi-armed bandits logic (high level description)","3f97413a":"This is how the multi-armed bandit algorithm works with exploration\/exploitation balance. Each bandit call contributes to the exploration part as we update a or b and become more confident. But at the same time, the more promising bandit is, the more often we call it. After 1000 rounds, we will call \"80%\" bandit much more times than two others and have a relatively high total reward.","c9b04a8b":"Let's look at the simplified version of this competition: assume that we have only 3 bandits and don't have any decay and competitor agent - so we play with 3 bandits by ourselves.\n\nLet's also assume that these bandits have win probabilities (that are unknown for our agent) of 20%, 50%, and 80%.","4cfde9e1":"We have 2 parameters, `a` and `b` - the number of wins and losses of each bandit. At each step, we sample some number p_i for each bandit from the corresponding beta distribution and select the bandit with the largest number.","e1fb4b40":"# Submission","62bd8fae":"# Selecting bandit on each step","8a629258":"I have described the general logic above, but this competition has two additional factors we need to consider:\n1. Decay\n2. Competitor\n\nAfter every call, the probability of reward from the particular bandit decays by 3%. To adjust it, I decrease the `a` param of the bandit each time any agent calls it. In the current version, I don't use any other adjustments.","7ec77b2d":"As we can see, initially, the distribution is uniform, but when we increase a and b, the distribution density concentrates around the current win rate. This distribution reflects our win probability expectations, given the current number of wins and losses using a particular bandit. The more data we have, the more confident our estimation becomes.","94e14ddd":"\nWe use the Beta distribution to model the probability of win of each bandit B(a+1, b+1). Beta distribution has 2 parameters, `a` and `b.` We can interpret them as the number of wins and number of losses for each bandit. The mode value of the distribution in this case is a\/(a+b) - the average win rate, mean = 1\/(1 + (b+1)\/(a+1)) - close to win rate if a and b are big enough.\n\nAlternatively, we can use B(a,b) and initialize everything by calling each bandit once - but the results will be very close.\n\nLet's look at how the B(a+1, b+1) probability density function looks like:"}}