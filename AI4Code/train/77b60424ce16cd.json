{"cell_type":{"90e7dfe7":"code","1d4bad8e":"code","0d537c05":"code","9898c0d7":"code","7f412349":"code","2f864909":"code","b49fade6":"code","81883cc6":"markdown","412c18ca":"markdown","78ea4824":"markdown","c562850f":"markdown","27144264":"markdown"},"source":{"90e7dfe7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndf=pd.read_csv('\/kaggle\/input\/150-famous-movie-catchphrases-with-context\/Catchphrase.csv')","1d4bad8e":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\ntext = df.Catchphrase.sum()\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(text)\nplt.figure(figsize=(40, 30))\nplt.imshow(wordcloud) \nplt.axis(\"off\")","0d537c05":"from nltk import FreqDist\nimport nltk\n\nwords=[]\nfiltered=[]\nfor i in df.Catchphrase:\n    words.append(nltk.tokenize.word_tokenize(i))\n    \nfor i in words:\n    for a in i:\n        if len(a)>1:\n            filtered.append(a)\n            \nwords_2=nltk.tokenize.word_tokenize(text)\nfdist = FreqDist(words_2)\n\nlength=pd.DataFrame([len(i) for i in words])\nlength.hist(bins=20, figsize=(20,10))\nplt.grid(b=None)\nplt.title('Length of catch phrases',family='serif', size=40)\nplt.yticks(family='serif', size=40)\nplt.xticks(family='serif', size=40)\nplt.show()","9898c0d7":"fdf=pd.DataFrame(fdist, index=['freq']).transpose()\nfdf.sort_values(by='freq',ascending=False,inplace=True)\nfdf=fdf.drop(axis=0,labels=['.',',','!','...'])\nfdf['word']=fdf.index\nfdf[0:50].plot(x='word',y='freq',kind='bar',figsize=(45,10))\nplt.grid(b=None)\nplt.xticks(rotation=45,family='serif', size=40)\nplt.title('Most frequent words ',family='serif', size=40)\nplt.yticks(family='serif', size=40)\nplt.show()","7f412349":"words_df=pd.DataFrame(words)\nslices=[]\nfor i in range(words_df.shape[1]-1):\n    slices.append(pd.DataFrame(words_df.iloc[:,range(i,2+i)]))\nfor i in slices:\n    i.columns=[1,2]\nlisti=pd.concat(slices)\nlisti=listi.dropna().reset_index().drop(axis=1,labels='index')","2f864909":"!pip install networkx","b49fade6":"import matplotlib.pyplot as plt\nimport networkx as nx\n\nG = nx.Graph()\nfor i in range(listi.shape[0]):\n    G.add_edge(listi.iloc[i,0], listi.iloc[i,1])\nplt.figure(figsize=(30, 30))    \npos = nx.spring_layout(G)\nnx.draw(G, pos, font_size=16, with_labels=False)\nfor p in pos:  \n    pos[p][1] += 0.02\nnx.draw_networkx_labels(G, pos)\nplt.show()","81883cc6":"<p style = \"font-family:palatino linotype,serif;font-size:25px;\">\n    ","412c18ca":"<p style = \"font-family:palatino linotype,serif;font-size:25px;\">\n    Please, leave comments on how to make this network better and thanks a lot for reading!","78ea4824":"<p style = \"font-family:palatino linotype,serif;font-size:25px;\">\n    Let us make a Wordcloud and some plots","c562850f":"<p style = \"font-family:palatino linotype,serif;font-size:25px;\">\n    Intro\n   <p style = \"font-family:palatino linotype,serif;font-size:20px;\">\n    This notebook contains wordcloud of 150 catch phrases from movies as well as network of most frequent words with others so one can make catchy phrase by themself\ud83d\ude09","27144264":"<p style = \"font-family:palatino linotype,serif;font-size:25px;\">\n    So, let us make network of words. \n    \n<p style = \"font-family:palatino linotype,serif;font-size:20px;\">\n    All phrases were tokenized and fetch to a df. Then the df sliced by 2 columns and these slices were concatenated, thus we saved connections of nearby words in phrases. After, I made edges of a network from each pair of words and visualise it with networkx package.\n    "}}