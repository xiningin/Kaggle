{"cell_type":{"50ab1dc9":"code","1e955de0":"code","3ab85431":"code","fdd29814":"code","7c51ddcc":"code","2566c45f":"code","74aa7edf":"code","c583b755":"code","9dc80261":"code","5a4c412c":"code","86bf6916":"code","773703b7":"code","606869ee":"code","0c885cdc":"code","bcd2a311":"code","3c464f28":"code","1ad90a41":"code","f5b4f13e":"code","a0f88817":"code","a42bbce8":"code","0df44cf5":"code","bcbe6510":"code","330eff84":"code","29148fef":"code","59ff443d":"code","f3978bb3":"code","e15d27b2":"code","62140a85":"code","01326b46":"code","6ddb7f8f":"code","5236eabc":"code","10f40835":"code","12ddc0a5":"code","31dd3ec1":"code","1c448a5b":"code","64b009f5":"code","d4ede669":"code","48c193ce":"code","425bc33d":"code","a3a4f1dd":"code","fe4fa350":"code","b35eb4ac":"code","491737eb":"code","7a14a389":"code","d06a2164":"code","4ed59104":"code","9e4d68e6":"code","c04a1029":"code","04a16902":"code","ac62ae4c":"code","9c2529ac":"code","ca3a2f7b":"code","f50d952c":"code","3db2c873":"code","e6fd69ca":"code","1ee1ec87":"code","0d0d84a8":"code","d0b73ba4":"markdown","a3152236":"markdown","0079ebd9":"markdown","421edf1e":"markdown","9e4a4707":"markdown","45afd65e":"markdown","298efb00":"markdown","7646c5bb":"markdown","24971628":"markdown","d8da160f":"markdown","da1b73c1":"markdown","354ef3d0":"markdown","546ac9c8":"markdown","3a7efe41":"markdown","4fd655e0":"markdown","1dcfde0d":"markdown","90a913f1":"markdown","7118ccb7":"markdown","ebf0411c":"markdown"},"source":{"50ab1dc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/loan-predication'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","1e955de0":"train = pd.read_csv('\/kaggle\/input\/loan-predication\/train_u6lujuX_CVtuZ9i (1).csv')\ntest = pd.read_csv('..\/input\/av-test\/test.csv')","3ab85431":"train.shape","fdd29814":"test.head()","7c51ddcc":"train.isnull().sum()","2566c45f":"train.info()","74aa7edf":"import seaborn as sns\nsns.set(rc={'figure.figsize':(10,8)})\nsns.countplot('Loan_Status',hue='Gender',data=train\n             )","c583b755":"train['self'] = 'train'\ntest['self'] = 'test'\nraw_data = pd.concat([train,test])","9dc80261":"raw_data['Gender'] = raw_data['Gender'].fillna('Male')","5a4c412c":"sns.countplot('Loan_Status',hue='Married',data=raw_data)","86bf6916":"raw_data['Married'] = raw_data['Married'].fillna('Yes')","773703b7":"raw_data['Dependents'] = raw_data['Dependents'].fillna(-99)\nfor i in range(len(raw_data)):\n    if(raw_data['CoapplicantIncome'].iloc[i]!=0 and raw_data['Dependents'].iloc[i]==-99 ):\n        raw_data['Dependents'].iloc[i]= '1'\n    elif(raw_data['CoapplicantIncome'].iloc[i]==0 and raw_data['Dependents'].iloc[i]==-99 ):\n        raw_data['Dependents'].iloc[i]=='0'\n    else:\n        continue\n        \n        \n","606869ee":"raw_data.isnull().sum()","0c885cdc":"raw_data['more_than_one_dependents'] = raw_data['Dependents'].apply(lambda x : 1 if x!='0' else 0)","bcd2a311":"raw_data['no_dependent'] = raw_data['Dependents'].apply(lambda x : 1 if x=='0' else 0)","3c464f28":"raw_data.head()","1ad90a41":"sns.countplot('Loan_Status',hue='Self_Employed',data=raw_data)","f5b4f13e":"raw_data['Self_Employed'] = raw_data['Self_Employed'].fillna('No')","a0f88817":"sns.distplot(raw_data['LoanAmount'])","a42bbce8":"sns.scatterplot(raw_data['LoanAmount'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","0df44cf5":"mean=raw_data[raw_data['LoanAmount']<=200]['LoanAmount'].mean()\nraw_data['LoanAmount']=raw_data['LoanAmount'].fillna(mean)\n","bcbe6510":"sns.scatterplot(raw_data['Loan_Amount_Term'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","330eff84":"raw_data['Loan_Amount_Term'].fillna(raw_data['Loan_Amount_Term'].value_counts().idxmax(), inplace=True)\n","29148fef":"raw_data['Credit_History'].unique()","59ff443d":"sns.countplot('Loan_Status',hue='Credit_History',data=raw_data)","f3978bb3":"sns.scatterplot(raw_data['Credit_History'],y=np.arange(0,981),hue=raw_data['Loan_Status'])","e15d27b2":"raw_data['Credit_History'].fillna(raw_data['Credit_History'].value_counts().idxmax(), inplace=True)\n\n","62140a85":"raw_data.isnull().sum()","01326b46":"raw_data.head()\n","6ddb7f8f":"raw_data['total_income'] = raw_data['ApplicantIncome'] + raw_data['CoapplicantIncome']\nraw_data['EMI'] = (raw_data['LoanAmount']*0.09*(1.09**raw_data['Loan_Amount_Term']))\/(1.09**(raw_data['Loan_Amount_Term']-1))\n","5236eabc":"raw_data['Loan_Amount_Term'].value_counts()\nraw_data['common_loan_term'] = raw_data['Loan_Amount_Term'].apply(lambda x : 1 if x==360.0 else 0)\nraw_data['uncommon_loan_term'] = raw_data['Loan_Amount_Term'].apply(lambda x: 1 if x!=360.0 else 0)\n","10f40835":"import matplotlib.pyplot as plt\nplt.figure(figsize=(12,12))\nsns.heatmap(raw_data.corr(),annot=True,cbar=False)","12ddc0a5":"raw_data = raw_data.drop(['Loan_ID','ApplicantIncome','CoapplicantIncome','Dependents','Loan_Amount_Term'],1)","31dd3ec1":"train = raw_data[raw_data['self']=='train']\ntrain = train.drop(['self'],1)\ntest = raw_data[raw_data['self']=='test']\ntest = test.drop(['self','Loan_Status'],1)","1c448a5b":"train.head()","64b009f5":"test.head()","d4ede669":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\nfrom sklearn.metrics import roc_auc_score","48c193ce":"print(train['Gender'].unique())\nprint(train['Married'].unique())\nprint(train['Education'].unique())\nprint(train['Self_Employed'].unique())\nprint(train['Property_Area'].unique())","425bc33d":"train['Gender'] = lb.fit_transform(train['Gender'])\ntrain['Married'] = lb.fit_transform(train['Married'])\ntrain['Education'] = lb.fit_transform(train['Education'])\ntrain['Self_Employed'] = lb.fit_transform(train['Self_Employed'])\ntrain['Property_Area'] = lb.fit_transform(train['Property_Area'])\ntrain['Loan_Status'] = lb.fit_transform(train['Loan_Status'])\n\n\ntest['Gender'] = lb.fit_transform(test['Gender'])\ntest['Married'] = lb.fit_transform(test['Married'])\ntest['Education'] = lb.fit_transform(test['Education'])\ntest['Self_Employed'] = lb.fit_transform(test['Self_Employed'])\ntest['Property_Area'] = lb.fit_transform(test['Property_Area'])\n\n","a3a4f1dd":"print(train['Gender'].unique())\nprint(train['Married'].unique())\nprint(train['Education'].unique())\nprint(train['Self_Employed'].unique())\nprint(train['Property_Area'].unique())\nprint(train['Loan_Status'].unique())\n","fe4fa350":"from sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import train_test_split\nrb = RobustScaler()","b35eb4ac":"test.head()","491737eb":"col = ['LoanAmount','total_income','EMI']\nfor i in col:\n    train[[i]] = rb.fit_transform(train[[i]])\n    test[[i]] = rb.transform(test[[i]])","7a14a389":"X = train.drop(['Loan_Status'],axis=1)\ny = train['Loan_Status']\n\n","d06a2164":"X.describe()","4ed59104":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)","9e4d68e6":"def training(model,X,y):\n    from sklearn.model_selection import train_test_split\n    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size =0.25,random_state=42,stratify=y)\n    model.fit(X_train,y_train)\n    preds_train = model.predict(X_train)\n    preds_test = model.predict(X_test)\n    print(\"ROC score for train:\",roc_auc_score(y_train,preds_train))\n    print(\"ROC score for test:\",roc_auc_score(y_test,preds_test))\n    print('\\n')\n    print(\"Confusion Matrix for train :\\n\",confusion_matrix(y_train,preds_train))\n    print(\"Confusion Matrix for test :\\n\",confusion_matrix(y_test,preds_test))","c04a1029":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,confusion_matrix\nlgt = LogisticRegression()\ntraining(lgt,X,y)","04a16902":"lgt_preds = lgt.predict(test)\nsub = pd.read_csv('..\/input\/sample-sub\/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = lgt_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('lgt_prediction_2.csv',index=False)","ac62ae4c":"from sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier\n\noof_pred               = np.zeros((len(train),))\ny_pred_final           = np.zeros((len(test),))\nnum_models             = 3\n\nn_splits               = 20\nerror                  = []\n\nkf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=294)\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X,y)):    \n    \n    wghts                     = [0]*num_models\n    test_roc_score            = []\n    \n    \n    X_train, y_train = X.iloc[train_idx,:], y.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n\n    model1 = LGBMClassifier(boosting_type='gbdt',n_estimators=500,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.5,random_state=294,n_jobs=-1)\n    model1.fit(X_train,y_train)\n    testpred1 = model1.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred1))\n    print(\"Test ROC AUC for model 1: %.4f\"%(roc_auc_score(y_val, testpred1)))\n    \n    model2 = LGBMClassifier(boosting_type='gbdt',n_estimators=300,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.3,reg_alpha=2,random_state=294,n_jobs=-1)\n    model2.fit(X_train,y_train)\n    testpred2 = model2.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred2))\n    print(\"Test ROC AUC for model 2: %.4f\"%(roc_auc_score(y_val, testpred2)))\n    \n    model3 = LGBMClassifier(boosting_type='gbdt',n_estimators=400,depth=-1,learning_rate=0.03,scale_pos_weight=7,objective='binary',metric='auc',\n                 colsample_bytree=0.4,reg_alpha=2,random_state=294,n_jobs=-1)\n    model3.fit(X_train,y_train)\n    testpred3 = model3.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred3))\n    print(\"Test ROC AUC for model 3: %.4f\"%(roc_auc_score(y_val, testpred3)))\n    \n    wghts              = np.exp(-1000*np.array(test_roc_score\/sum(test_roc_score)))\n    wghts              = wghts\/sum(wghts)\n    \n    val_pred           = wghts[0]*testpred1+wghts[1]*testpred2 +wghts[2]*testpred3\n    print('validation roc_auc_score fold-',i+1,': ',roc_auc_score(y_val, val_pred))\n    \n    oof_pred[val_idx]  = val_pred\n    y_pred_final += (wghts[0]*model1.predict_proba(test)[:,1]+wghts[1]*model2.predict_proba(test)[:,1]+wghts[2]*model3.predict_proba(test)[:,1])\/(n_splits)\n    \n    print('\\n')\n    \nprint('OOF ROC_AUC_Score:- ',(roc_auc_score(y,oof_pred)))","9c2529ac":"from catboost import CatBoostClassifier\noof_pred               = np.zeros((len(train),))\ny_pred_final           = np.zeros((len(test),))\nnum_models             = 3\n\nn_splits               = 20\nerror                  = []\n\nkf=StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=294)\n    \nfor i,(train_idx,val_idx) in enumerate(kf.split(X,y)):    \n    \n    wghts                     = [0]*num_models\n    test_roc_score            = []\n    \n    \n    X_train, y_train = X.iloc[train_idx,:], y.iloc[train_idx]\n\n    X_val, y_val = X.iloc[val_idx, :], y.iloc[val_idx]\n    \n\n    print('\\nFold: {}\\n'.format(i+1))\n\n    model1 = CatBoostClassifier(learning_rate = 0.03,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model1.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=30,verbose=100)\n    testpred1 = model1.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred1))\n    print(\"Test ROC AUC for model 1: %.4f\"%(roc_auc_score(y_val, testpred1)))\n    \n    model2 = CatBoostClassifier(learning_rate = 0.04,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model2.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=40,verbose=100)\n    testpred2 = model2.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred2))\n    print(\"Test ROC AUC for model 2: %.4f\"%(roc_auc_score(y_val, testpred2)))\n    \n    model3 = CatBoostClassifier(learning_rate = 0.05,random_state=42,scale_pos_weight=7, custom_metric=['AUC'])\n    model3.fit(X_train,y_train,eval_set=(X_val, y_val),early_stopping_rounds=20,verbose=100)\n    testpred3 = model3.predict_proba(X_val)[:,1]\n    test_roc_score.append(roc_auc_score(y_val, testpred3))\n    print(\"Test ROC AUC for model 3: %.4f\"%(roc_auc_score(y_val, testpred3)))\n    \n    wghts              = np.exp(-1000*np.array(test_roc_score\/sum(test_roc_score)))\n    wghts              = wghts\/sum(wghts)\n    \n    val_pred           = wghts[0]*testpred1+wghts[1]*testpred2 +wghts[2]*testpred3\n    print('validation roc_auc_score fold-',i+1,': ',roc_auc_score(y_val, val_pred))\n    \n    oof_pred[val_idx]  = val_pred\n    y_pred_final += (wghts[0]*model1.predict_proba(test)[:,1]+wghts[1]*model2.predict_proba(test)[:,1]+wghts[2]*model3.predict_proba(test)[:,1])\/(n_splits)\n    \n    print('\\n')\n    \nprint('OOF ROC_AUC_Score:- ',(roc_auc_score(y,oof_pred)))","ca3a2f7b":"cb_preds = cb.predict(test)\nsub = pd.read_csv('..\/input\/sample-sub\/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = cb_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('cb_prediction_2.csv',index=False)","f50d952c":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(base_estimator=lgt)\nada.fit(X_train,y_train)\nada.score(X_test,y_test)","3db2c873":"from sklearn import svm\nfrom sklearn.metrics import accuracy_score\nclf = svm.SVC(kernel='linear')\nclf.fit(X_train,y_train)\npreds = clf.predict(X_test)\nprint(accuracy_score(y_test,preds))","e6fd69ca":"import lightgbm as lgb\nmodel = lgb.LGBMClassifier(objective='mse', seed=8798, num_threads=1)\nmodel.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], verbose=10)\nlgb.plot_metric(model)\n","1ee1ec87":"model.score(X_test,y_test)","0d0d84a8":"model_preds = model.predict(test)\nsub = pd.read_csv('..\/input\/sample-sub\/sample_submission_49d68Cx.csv')\nsub['Loan_Status'] = model_preds\nsub['Loan_Status'] = sub['Loan_Status'].replace({1:'Y',0:'N'})\nsub.to_csv('model_lgb_prediction.csv',index=False)","d0b73ba4":"We see outliers as data seems to right skewed , so we will not fill values by mean of this data, we gonna take the mean without containing outliers.","a3152236":"# Part 2\nWe are first going to one hot encode every Categorcial data then we are going to deploy it in our model","0079ebd9":"Let us check if the category doesnt contain any duplicates such as yes, YES ...","421edf1e":"# Feel free to comment any doubt , if you got more better result or thought of any other feature engineering you can comment, as sharing make us stronger.\n** Thanks if you liked my work dont forget to upvote. **","9e4a4707":"**Consider this as just starting model, what more could be done:<br>**\n\n1) Hyperparamater tunings of best model.<br>\n2) Adding features using best feature engineering.<br>\n3) I haven't tried neural network but you can use it.<br>\n4) More over we can achieve atleast accuracy of 0.82-0.85, using proper hyperparameter tuning, and <br>\n   using additional features.","45afd65e":"LightGBM","298efb00":"Since most of the dependents are 0 we are gonna fill with it with 0","7646c5bb":"*Most probably after looking through the graph we are going to fill with mode of the data i.e 'Male'*","24971628":"CATBOOST","d8da160f":"Using SVM","da1b73c1":"*Again we are going to use mode*","354ef3d0":"Its very important to know the type of Data to decide which Machine learning algorthim you have to use.There are three types of Data:\n\n* Categorical Data\n* Numerical Data\n* Ordinal Data\n\nIn these notebook we will perfom following task:\n* First we will look through data and explore\n* We will see if data contains any missing value and try to fix it.\n* Insights of data are always usefull hence we will explore through seaborn , matplotlib libraries.\n* We will deploy through Logistic Regression Model.","546ac9c8":"Lets see our result","3a7efe41":"For the missing value we are going to use mode.","4fd655e0":"**First we deal with the missing values** ","1dcfde0d":"**THERE IS AN OVERFITTING**<br>\nWe need to tune parameters to prevent the overfitting.","90a913f1":"**Hence from Logistic regression we get an accuracy of about 85%**<br>\nJust as starting our Logisitic Regression performed well.","7118ccb7":"We can infer from this that those who were married were most probably to get the loan.","ebf0411c":"# Part 3\nDeploying it in the model"}}