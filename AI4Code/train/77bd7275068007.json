{"cell_type":{"cb0eef1f":"code","0a19c3df":"code","d79a4806":"code","3a791745":"code","911a0e75":"code","2de7ce14":"code","30b9fdfd":"code","a80ba47d":"code","0827051c":"markdown","2f15f0c4":"markdown","159e404a":"markdown","a1ade63d":"markdown","ebfb54e8":"markdown","c0e4b9df":"markdown"},"source":{"cb0eef1f":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer","0a19c3df":"%config InlineBackend.figure_format = \"svg\"","d79a4806":"x0, y0 = np.meshgrid(np.linspace(0.0,200.0,num=30), np.linspace(-100,100,num=20))\n# Convert to feature matrix\nX = np.concatenate((np.asanyarray(x0).reshape(x0.size,1),\n                    np.asanyarray(y0).reshape(y0.size,1)),axis=1);\ny = 10.0+3.0*X[:,0]-2.0*X[:,1]+0.1*(X[:,0]**2)+0.4*(X[:,1]**2);\nnp.random.seed(123);\ny[0:20] = y[0:20] + np.random.rand(20)*1e+4; # insert outlier\nnp.random.shuffle(y)\ny = y.reshape(y.size,1)","3a791745":"def plotscaled(vec,i,ncol=2,nrow=6,title=\"\"):\n    plt.subplot(nrow,ncol,i);i+=1;\n    plt.hist(vec,24)\n    plt.legend([title])\n    plt.subplot(nrow,ncol,i);\n    plt.plot(vec,\"k.\");\n    y_mean,y_med,y_std,y_min,y_max = np.mean(vec),np.median(vec),np.std(vec),np.min(vec),np.max(vec);\n    plt.plot([0,vec.size],[y_mean,y_mean],\"r-\")\n    plt.plot([0,vec.size],[y_med,y_med],\"g--\")\n    plt.plot([0,vec.size],[y_mean-y_std*2,y_mean-y_std*2],\"b-\")\n    plt.plot([0,vec.size],[y_mean+y_std*2,y_mean+y_std*2],\"b-\")\n    if i == 2:\n        plt.legend([\"data\",\"mean\",\"median\",\"2*$\\sigma$\"],ncol=4,fontsize=9);\n    return i+1","911a0e75":"plt.figure(figsize=(9.5,9));\ni = 1;\nfor m in [StandardScaler(),RobustScaler(),MinMaxScaler(),\n          PowerTransformer(),QuantileTransformer(output_distribution=\"normal\")]:\n    y_scaled = m.fit_transform(y);\n    i = plotscaled(y_scaled,i,title=str(m).split(\"(\")[0])","2de7ce14":"from scipy.stats import skewnorm","30b9fdfd":"def plotskew(s):\n    y = skewnorm.rvs(s, size=1000);\n    plt.figure(figsize=(8,5));\n    plt.subplot(2,2,1);\n    plt.hist(y,20);\n    if s > 0:\n        plt.title(\"Positive skew\");\n    else:\n        plt.title(\"Negative skew\")\n    plt.subplot(2,2,2);\n    return y","a80ba47d":"np.random.seed(1)\ny = plotskew(4)\nplt.hist(np.log(y-np.min(y)+1),20);\nplt.title(\"log-transform: log(const+y)\");\ny = plotskew(-4)\nplt.hist((y-np.min(y))**2,20);\nplt.title(\"square power-transform: (const+y)^2\");","0827051c":"Function to plot scaling result","2f15f0c4":"## Compare transormation algorithms<a name=\"code\"\/>","159e404a":"#### Generate data\n* Create data set with non-zero mean, variance, including outliers","a1ade63d":"# Data Transformation\nI have prepared this Jupyter notebook to give a brief **introduction** to data transformations. See <a href=#references>Referenes<\/a> for sources and further details. The first part of the notebook describes theory while the second part shows a synthetic example in <a href=#code>Python<\/a>.    \n\n## Theory\nData transformation is the application of a deterministic mathematical function to each point in a data set<sup>1<\/sup>. This function should be invertible and continuous. Main **aims of the transformation** are:\n* Achieve numerical stability, e.g., in regression\n* Achieve linear separability in otherwise non-linear data set\n* Reduce variance and thus dimensions, e.g., PCA\n* To obtain certain, ofter normal, distribution\n* Comparability, e.g., unit conversion\n* Comprehensive visualization\n\n> If possible, always visualize data prior and after the transformation\n\nThere are different types of transformation. These can be divided (in simplified way) into:\n### Linear transformation\n* Linear function with offset and multiplication (scale) that can be applied successively. \n* Can be presented as coordinate transformation composed of a shift of the origin and axis scaling (e.g., via projection)\n* **Pros**\n    * It does not break-up linear relations between dependent and independent variable\n    * Simple and mostly computationally inexpensive\n    * User is on a safe side when applying linear transformation\n* **Cons**\n    * Will not help to solve non-linear separability<sup>5<\/sup> or visualization problems\n* **Examples**<sup>3<\/sup>\n    * [MinMaxScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.MinMaxScaler.html): scale to specific renge\n    * [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html): subtract mean and set to unit standard deviation\n    * [Normalizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.Normalizer.html): normalize the data to have unit norm, independently of the distribution of the samples (norm can be maximum values, abs. sum (L1) square sum (L2))\n        * Refers to a per sample transformation instead of a per feature transformation\n    * [RobustScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html): similar to StandardScaler but using only 1st-3rd quartile to compute median and deviation that are then used for whole data set \n        * Is linear, does not remove outliers\n    * [PCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html): is ortogonal = linear tranformation of the coordinate system providing all components are preserved\n        * Requires uses of [StandardScaler](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) = zero mean and 1 variance\n\n### Non-linear transformation\n* Can have arbitrary form, offset, multiplication, and other variable scale \n* **Pros**\n    * Transformed data can be used to solve non-linear problems\n    * Useful for modeling issues related to heteroscedasticity (non-constant variance)<sup>6<\/sup>\n    * Can be applied to meet criteria for certain distribution and thus to apply common rules\n        * For example 2\\*sigma rule for normal distribution \n* **Cons**\n    *  breaks up linear relations\n        * For example of square or log transformation: the difference of 0.1 between 1.0 and 1.1 is after transformation equal to difference between 2.0 and 2.1\n    * Following blindly the aim to meet some distribution after transformation does not change the fact that the original data set will still not follow such distribution\n        * Furthermore, such assumptions may lead to false and literally dangerous conclusions<sup>2,8<\/sup>\n* **Examples**<sup>3<\/sup>\n    * [QuantileTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.QuantileTransformer.html#sklearn.preprocessing.QuantileTransformer): transforms the data to follow a uniform or a normal distribution. \n        * Can handle multi-nomial distributions<sup>7<\/sup>\n        * This transformation tends to spread out the most frequent values\n        * It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme\n        * Because it is a non-parametric method, it is harder to interpret than the parametric ones (power)<sup>7<\/sup>\n    * [PowerTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PowerTransformer.html): a parametric, monotonic transformations that is applied to make data more Gaussian-like. \n        * Scikit-learn supports [Box-Cox](https:\/\/en.wikipedia.org\/wiki\/Power_transform#Box\u2013Cox_transformation)<sup>6<\/sup> (non-negative values only) and the [Yeo-Johnson](https:\/\/en.wikipedia.org\/wiki\/Power_transform#Yeo\u2013Johnson_transformation)<sup>6<\/sup> transformation\n        * The power transform finds the optimal scaling factor to stabilize variance and mimimize skewness through maximum likelihood estimation<sup>3<\/sup>\n    * [KernelPCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA): non-linear dimensionality reduction through the use of kernels\n* Scikit-learn [comparison](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_map_data_to_normal.html):  \n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_map_data_to_normal_001.png\"\/>\n\n\n#### References<a name=\"references\"\/>\n* <sup>1<\/sup>[Wikipedia Data Transfromation](https:\/\/en.wikipedia.org\/wiki\/Data_transformation_%28statistics%29)\n* <sup>2<\/sup>[The abnormal need for normal distributions](https:\/\/www.qualitydigest.com\/inside\/quality-insider-column\/do-you-have-leptokurtophobia.html) and [Log-transformation and its implications for data analysis](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4120293\/)\n* <sup>3<\/sup>[Scikit-Learn scaling comparison](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html)\n* <sup>4<\/sup>[Scikit-Learn data preprocessing steps](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html)\n* <sup>5<\/sup>[Scikit-Learn: Advantages of non-linear tranfomation](https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_transformed_target.html#sphx-glr-auto-examples-compose-plot-transformed-target-py)\n* <sup>6<\/sup>[Wikipedia Power Tranformation](https:\/\/en.wikipedia.org\/wiki\/Power_transform#Example)\n* <sup>7<\/sup>[Scikit-Learn: Map data to normal distribution](scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_map_data_to_normal.html)\n* <sup>8<\/sup>[Rank-Based Inverse Normal Transformations are Increasingly Used, But are They Merited?](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2921808\/)\n\n<br><\/br>\n\n> _Michal Mikolaj, 28.02.2019_","ebfb54e8":"#### User-defined non-linear transformation\n* In most cases, either positive or negative skewness should be corrected\n* Left-skewed (negative) distribution\/data can be often \"normalized\" using log transformation\n* Righ-skewed (positive) distribution\/data can be sometimes \"normalized\" using square\/cube root transformation ","c0e4b9df":"Import libraries"}}