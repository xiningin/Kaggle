{"cell_type":{"b0f3cb7e":"code","a8ef7930":"code","1fe074a5":"code","a7a7ac93":"code","deb0ba3b":"code","9313e82f":"code","860917fb":"code","a8f00c20":"code","3873b087":"code","faa4d136":"code","72cc304b":"code","3a812a8f":"code","34c21d25":"code","fa3d7c71":"code","58f80ab6":"code","a5c8057c":"code","78be3e28":"code","1696e790":"code","84e82f45":"code","cad1c5a9":"code","bf250beb":"code","c670a7bf":"code","1b3b0c17":"code","1344211a":"code","5ec33716":"code","c8761dcd":"code","216e2a47":"code","0d491520":"code","6a08c74b":"code","f042f3c1":"code","d0306a1f":"code","c80cecb5":"code","b035f49e":"code","6050be07":"code","0c9b902c":"code","9873ec9b":"code","cc87b32b":"markdown","21c98aea":"markdown","e2fccae7":"markdown","c49ebac0":"markdown","200fdd65":"markdown","5a89e109":"markdown","9160d0a3":"markdown","b3c2b36d":"markdown","b5266f53":"markdown","0f9d9f22":"markdown","cca6f623":"markdown","89e62ac3":"markdown","9ea193ec":"markdown","a7fb47a5":"markdown"},"source":{"b0f3cb7e":"#import some necessary librairies\n# https:\/\/www.kaggle.com\/edumagalhaes\/quality-prediction-in-a-mining-process\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","a8ef7930":"#Now let's import and put the train and test datasets in  pandas dataframe\npath1 = '..\/input\/quality-prediction-in-a-mining-process\/'\npath2 = '..\/input\/defect_per_model\/'\ntrain = pd.read_csv(path1+'MiningProcess_Flotation_Plant_Database.csv',decimal=\",\",parse_dates=[\"date\"],infer_datetime_format=True)","1fe074a5":"train.describe()","a7a7ac93":"print (\"Size of train data : {}\" .format(train.shape))","deb0ba3b":"pd.set_option('display.max_columns', None)\ntrain.head(100)","9313e82f":"train.info()\n# d = train.date.value_counts()\n# d.index\n# i = 0\n# for ind in d.index:\n#     if '2017-03-13' in ind:\n#         print(ind)\n#         i+=1\n# print('total unique timestamp',i)\n# 2017-03-13 01:00:00","860917fb":"#corelation\ncorrmat = train.corr()\ncorrmat","a8f00c20":"plt.figure(figsize=(10,10))\ng = sns.heatmap(train.corr(),annot=True,cmap=\"RdYlGn\")","3873b087":"train['Flotation Column 01 Level'].hist()","faa4d136":"train['% Iron Feed'].hist()\n# from scipy.stats import kstest\n# for item in train.columns.drop('date'):\n#     print(kstest(train[item],'norm'))\n    ","72cc304b":"train['Flotation Column 01 Air Flow'].hist()","3a812a8f":"# train.skew(axis=0).sort_values(ascending=False)","34c21d25":"fig, ax = plt.subplots()\nax.scatter(x = train['% Iron Feed'], y = train['% Silica Concentrate'])\nplt.ylabel('% Silica Concentrate', fontsize=13)\nplt.xlabel('% Iron Feed', fontsize=13)\nplt.show()","fa3d7c71":"def check_skewness(col):\n    sns.distplot(train[col] , fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(train[col], plot=plt)\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(train[col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    \ncheck_skewness('% Silica Concentrate')","58f80ab6":"check_skewness('% Iron Feed')","a5c8057c":"check_skewness('Flotation Column 01 Level')","78be3e28":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"% Silica Concentrate\"] = np.log1p(train[\"% Silica Concentrate\"])\n\ncheck_skewness('% Silica Concentrate')","1696e790":"# Check the skew of all numerical features\nskewed_feats = train.drop('date',axis=1).apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","84e82f45":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    train[feat] = boxcox1p(train[feat], lam)","cad1c5a9":"train.head()","bf250beb":"train.describe()","c670a7bf":"y = train['% Silica Concentrate']\nX = train.iloc[:,1:-2]\nX.head()","1b3b0c17":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=3)\n# gc.collect()  \nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","1344211a":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","5ec33716":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","c8761dcd":"#http:\/\/sklearn.lzjqsdd.com\/modules\/linear_model.html#ridge-regression\n#Ridge not sucessful\n'''KRR = KernelRidge(alpha=0.5,kernel='polynomial,  degree=2, coef0=2.5)#\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))'''","216e2a47":"'''lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))'''\n# Lasso score: 0.1477 (0.0003)","0d491520":"'''ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))'''\n# ElasticNet score: 0.1477 (0.0003)","6a08c74b":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\n'''GBoost = GradientBoostingRegressor(verbose=1, random_state =5)#n_estimators=3000, learning_rate=0.05,\n#                                    max_depth=4, max_features='sqrt',\n#                                    min_samples_leaf=15, min_samples_split=10, \n#                                    loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))'''\n# Gradient Boosting score: 0.1290 (0.0003)","f042f3c1":"from joblib import dump, load\n\n'''GBoostMd = GBoost.fit(X_train.values,y_train)\ndump(GBoost, 'GBoost_fitted.joblib')'''","d0306a1f":"import lightgbm as lgb\n'''myLGB = lgb.LGBMRegressor(objective='regression')\nmyLGB.fit(X_train, y_train)\nscore = rmsle_cv(myLGB)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))'''\n# myLGB.booster_.save_model('lgbmodel.txt')\n# Lasso score: 0.1033 (0.0003)","c80cecb5":"from xgboost import XGBRegressor\n'''myXGB = XGBRegressor(n_estimators=1000, learning_rate=0.05,verbose=True)\n\n# myXGB.fit(X_train, X_train, early_stopping_rounds=50, verbose=True)#eval_set=[(X_test, y_test)],\nscore = rmsle_cv(myXGB)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#0.11\nmyXGB.fit(X_train, y_train)'''\n# myXGB.save_model('myXGB_save.joblib')","b035f49e":"#following 3 not good\n# LassoMd = lasso.fit(train.values,y_train)\n# ENetMd = ENet.fit(train.values,y_train)\n# KRRMd = KRR.fit(train.values,y_train)\nmyLGB = lgb.Booster(model_file=path2+'lgbmodel.txt')\nmyXGB = xgb.Booster({'nthread':-1}) # init model\nmyXGB.load_model(path2+'myXGB_save.joblib')      # load data\n\nGBoost = load(path2+'GBoost_fitted.joblib')","6050be07":"\nSilica_pred = np.expm1(myLGB.predict(X_test))*0.6+np.expm1(myXGB.predict(xgb.DMatrix(X_test)))*0.3+np.expm1(GBoost.predict(X_test.values))*0.1","0c9b902c":"                                                                                                       \n# from sklearn.metrics import mean_squared_error\nprint (\"The RMSE of the model ensemble is %0.3f\" %(mean_squared_error(y_test, Silica_pred)))\n# The RMSE of the model ensemble is 0.214","9873ec9b":"print (\"The RMSE of the lgb model  is %0.3f\" %(mean_squared_error(y_test, np.expm1(myLGB.predict(X_test)))))\n# The RMSE of the lgb model  is 0.215","cc87b32b":"## Lets apply Modelling\n\n1. Importing Libraries\n\n2. We will use models\n - Lasso\n - Ridge\n - ElasticNet\n - Gradient Boosting\n \n3. Find the Cross Validation Score.\n\n ","21c98aea":"## Mean of all model's prediction.\nnp.expm1 ( ) is used to calculate exp(x) - 1 for all elements in the array. ","e2fccae7":"**This distribution is positively skewed.** Notice that the black curve is more deviated towards the right. If you encounter that your predictive (response) variable is skewed, it is **recommended to fix the skewness** to make good decisions by the model.\n\n## Okay, So how do I fix the skewness?\nThe best way to fix it is to perform a **log transform** of the same data, with the intent to reduce the skewness.","c49ebac0":"After taking logarithm of the same data the curve seems to be normally distributed, although not perfectly normal, this is sufficient to fix the issues from a skewed dataset as we saw before.\n\n**Important : If you log transform the response variable, it is required to also log transform feature variables that are skewed.**","200fdd65":"Here used some LGB, XGB, GBM regression to predict the %Silica Concentrate, after ensemble,t he rmse is 0.214.\nLGB has the potential for optimization.\n","5a89e109":"With lgb optimzation, the rmse can be improved a little.","9160d0a3":"# Loading and Inspecting data\nThe first column shows time and date range (from march of 2017 until september of 2017). Some columns were sampled every 20 second. Others were sampled on a hourly base.\n\nThe second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. Target is to predict the last column, which is the % of silica in the iron ore concentrate.\n","b3c2b36d":"## Gradient Boosting Regression\nRefer [here](https:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d)","b5266f53":"## Ridge Regression\n- It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n- It reduces the model complexity by coefficient shrinkage.\n- It uses L2 regularization technique.","0f9d9f22":"## Box Cox Transformation of (highly) skewed features\n\nWhen you are dealing with real-world data, you are going to deal with features that are heavily skewed. Transformation technique is useful to **stabilize variance**, make the data **more normal distribution-like**, improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is **estimating lambda**. This value will depend on the existing data, and should be considered when performing cross validation on out of sample datasets.","cca6f623":"## Cross Validation\nIt's simple way to calculate error for evaluation. \n\n**KFold( )** splits the train\/test data into k consecutive folds, we also have made shuffle attrib to True.\n\n**cross_val_score ( )** evaluate a score by cross-validation.","89e62ac3":"# Modelling\nSince in this dataset we have a large set of features. So to make our model avoid Overfitting and noisy we will use Regularization.\nThese model have Regularization parameter.\n\nRegularization will reduce the magnitude of the coefficients.","9ea193ec":"## Elastic Net Regression\n\nElastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters.","a7fb47a5":"## Lasso Regression\nLASSO (Least Absolute Shrinkage Selector Operator), is quite similar to ridge.\n\nIn case of lasso, even at smaller alpha\u2019s, our coefficients are reducing to absolute zeroes.\n Therefore, lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge.\n \n- Lasso uses L1 regularization technique.\n- Lasso is generally used when we have more number of features, because it automatically does feature selection.\n"}}