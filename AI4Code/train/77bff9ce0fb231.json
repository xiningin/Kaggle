{"cell_type":{"460a896d":"code","8fe8b13c":"code","b2564061":"code","1f074c10":"code","0f146840":"code","71a816d2":"code","69bf3270":"code","fe12b7e0":"code","852ee295":"code","58a5d103":"code","134ca289":"code","b5887664":"code","6e0e8272":"code","9460babd":"code","330de522":"code","fc2c0e4e":"code","82a532dc":"code","a686cac9":"code","bb7c193e":"code","3fa17220":"code","e1f0ccee":"code","750b533a":"code","411d7f51":"code","fdc3f493":"code","24bc1979":"code","2a9e430a":"code","41d7c4fb":"code","e35d6f76":"code","9a589b1e":"code","afe9a4fa":"code","fad13c7a":"code","a8e48d47":"code","9046dd49":"code","4995fc81":"code","3efa6af6":"code","6455d7cb":"code","8aab300e":"code","70e3b210":"code","8466c06b":"code","b0b06a73":"code","5920f68d":"code","8d7e13e8":"code","f3103c0c":"code","14ee430f":"code","ddcc08cf":"code","37103845":"code","7a61f1dc":"code","d2be5146":"code","3d4d4f2c":"code","0efee1ed":"code","79616072":"code","45eea4da":"code","d3fefcab":"code","4592d8de":"code","9ade7181":"code","436369d1":"code","b0ba4da8":"code","bb3f41c5":"code","c79078c3":"code","a7b28543":"code","467352c7":"code","0b34299d":"code","8518a21c":"code","ede44ee8":"code","8b212adf":"code","752bfa00":"code","c558003a":"code","0f4ebcdc":"code","5f879b19":"code","4881a45d":"markdown","5cdb9dc9":"markdown"},"source":{"460a896d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8fe8b13c":"stroke= pd.read_csv('\/kaggle\/input\/healthcare-dataset-stroke-data\/train_2v.csv')\nstroke.head()","b2564061":"#checking the shape of our data\nstroke.shape","1f074c10":"stroke.columns","0f146840":"stroke= stroke.drop('id', axis=1)","71a816d2":"#checking the null values\nstroke.isnull().sum()","69bf3270":"stroke[stroke==0].count()","fe12b7e0":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(stroke['stroke'])","852ee295":"#filling null values with the mean\nstroke['bmi'].fillna(stroke['bmi'].mean(), inplace= True)","58a5d103":"#filling null values with mode\nstroke['smoking_status'].fillna(stroke['smoking_status'].mode()[0], inplace=True)","134ca289":"#checking the data\nstroke.isnull().sum()","b5887664":"stroke.describe()","6e0e8272":"stroke.info()","9460babd":"sns.distplot(stroke['avg_glucose_level'], bins=20)","330de522":"sns.distplot(stroke['bmi'], bins=20)","fc2c0e4e":"sns.distplot(stroke['age'], bins=20)","82a532dc":"#chances of stroke incraeses with incraese in age\nstroke.loc[stroke['stroke'] == 0,\n                 'age'].hist(label='No Stroke')\nstroke.loc[stroke['stroke'] == 1,\n                 'age'].hist(label='Heart Stroke')\nplt.xlabel('Age')\nplt.ylabel('Heart Stroke')\nplt.legend()","a686cac9":"#chances of stroke more with bmi 20-40\n\nstroke.loc[stroke['stroke'] == 0,\n                 'bmi'].hist(label='No Stroke')\nstroke.loc[stroke['stroke'] == 1,\n                 'bmi'].hist(label='Heart Stroke')\nplt.xlabel('BMI')\nplt.ylabel('Heart Stroke')\nplt.legend()","bb7c193e":"#chances of stroke high with glucose levels in range of 70-100\n\nstroke.loc[stroke['stroke'] == 0,\n                 'avg_glucose_level'].hist(label='No Stroke')\nstroke.loc[stroke['stroke'] == 1,\n                 'avg_glucose_level'].hist(label='Heart Stroke')\nplt.xlabel('Glucose Level')\nplt.ylabel('Heart Stroke')\nplt.legend()","3fa17220":"#married females have more chances of heart stroke than married males\npd.pivot_table(stroke, index= 'stroke', columns='gender', values='ever_married', aggfunc= 'count')","e1f0ccee":"#females with hypertension has more chance of heart stroke than males having hypertension problem\npd.pivot_table(stroke, index= 'stroke', columns='gender', values='hypertension', aggfunc= 'count')","750b533a":"#females with heart disease has more chances of stroke\npd.pivot_table(stroke, index= 'stroke', columns='gender', values='heart_disease', aggfunc= 'count')","411d7f51":"#people having private jobs and has a habit of smoking has more chance of heart stroke \npd.pivot_table(stroke, index= 'stroke', columns='work_type', values='smoking_status', aggfunc= 'count')","fdc3f493":"#as age incraeses gender does not play any role in heart stroke\nsns.scatterplot(x= 'stroke', y='age', hue='gender', sizes= (15,200), data=stroke)\nplt.xticks(rotation=90)","24bc1979":"#can't say that marriage plays a role in heart stroke as people generally marry after the age of 25years\nsns.relplot(x= 'stroke', y='age', hue= 'ever_married', sizes= (15,200), data=stroke)\nplt.xticks(rotation=90)","2a9e430a":"#with age glucose level increases which increases the chances of stroke\nplt.figure(figsize=(28,20))\nsns.relplot(x= 'avg_glucose_level', y='age', hue= 'stroke', sizes= (15,200), data=stroke)\nplt.xticks(rotation=90)","41d7c4fb":"stroke['stroke'].value_counts()","e35d6f76":"#performing label encoding for the dataset\nfrom sklearn import preprocessing \n\nencoder = preprocessing.LabelEncoder()\n\nfor i in stroke.columns:\n    if isinstance(stroke[i][0], str):\n            stroke[i] = encoder.fit_transform(stroke[i])","9a589b1e":"#standardizing the dataset with Standard Scaler\nfrom sklearn.preprocessing import StandardScaler \n  \nscalar = StandardScaler() \n  \nscalar.fit(stroke) \nscaled_data = scalar.transform(stroke)","afe9a4fa":"#checing the data for first 10 values\nstroke.head(10)","fad13c7a":"#dropping the output label\nX= stroke.drop('stroke', axis=1)\nX.shape","a8e48d47":"y= stroke['stroke']\ny.shape","9046dd49":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3, random_state = 1000)","4995fc81":"log= LogisticRegression()","3efa6af6":"log.fit(X_train,y_train)","6455d7cb":"log.score(X_train, y_train)","8aab300e":"stroke['stroke'].value_counts()","70e3b210":"#to retain the original data, we craeted a copy of the dataset\nstroke_copy= stroke.copy()","8466c06b":"stroke_copy.head()","b0b06a73":"#creating a list of data values which is more in number\n#to make a balance data\nli = list(stroke_copy[stroke_copy.stroke == 0].sample(n=41800).index)","5920f68d":"#dropping the values\nstroke_copy = stroke_copy.drop(stroke_copy.index[li])\n\nstroke_copy['stroke'].value_counts()","8d7e13e8":"X_drop= stroke_copy.drop('stroke', axis=1)\nX_drop.shape","f3103c0c":"y_drop= stroke_copy.stroke\ny_drop.shape","14ee430f":"X_droptr,X_dropts,y_droptr,y_dropts = train_test_split(X_drop,y_drop,test_size=.3, random_state = 1000)","ddcc08cf":"#creating a Logistic Model for the new data\nlog.fit(X_droptr, y_droptr)","37103845":"#the accuracy has dropped\nlog.score(X_droptr, y_droptr)","7a61f1dc":"#predicting the output with Logistic\ny_underlog= log.predict(X_dropts)","d2be5146":"from sklearn.metrics import accuracy_score, f1_score, classification_report, recall_score, confusion_matrix\nprint('The accuracy score of the model is:', accuracy_score(y_dropts,y_underlog)*100)\nprint('The F1 score of the model is:', f1_score(y_dropts, y_underlog)*100)\nprint('The recall score of the model is:', recall_score(y_dropts, y_underlog)*100)\nprint('The confusion matrix of the model is:', confusion_matrix(y_dropts, y_underlog))\nprint('The classification report of logistic model is:', classification_report(y_dropts, y_underlog))","3d4d4f2c":"from sklearn import tree\nmodel = tree.DecisionTreeClassifier()","0efee1ed":"model.fit(X_droptr, y_droptr)","79616072":"#tuning the model using criterion and max_depth only\n\nfrom sklearn.model_selection import GridSearchCV\nparam = {\n    'criterion': ['entropy', 'gini'],\n    'max_depth' :[2,3,4,5]\n}\ngrid_svc = GridSearchCV(model, param_grid=param, scoring='accuracy', cv=10)","45eea4da":"grid_svc.fit(X_droptr, y_droptr)","d3fefcab":"grid_svc.best_params_","4592d8de":"model= tree.DecisionTreeClassifier(criterion= 'gini', max_depth= 3)\nmodel.fit(X_droptr, y_droptr)","9ade7181":"model.score(X_droptr, y_droptr)","436369d1":"y_undersDT= model.predict(X_dropts)","b0ba4da8":"print('The accuracy score of the model is:', accuracy_score(y_dropts,y_undersDT)*100)\nprint('The F1 score of the model is:', f1_score(y_dropts, y_undersDT)*100)\nprint('The recall score of the model is:', recall_score(y_dropts, y_undersDT)*100)\nprint('The confusion matrix of the model is:', confusion_matrix(y_dropts, y_undersDT))\nprint('The classification report of base model is:', classification_report(y_dropts, y_undersDT))","bb3f41c5":"from sklearn import ensemble\nrf= ensemble.RandomForestClassifier()","c79078c3":"rf.fit(X_droptr, y_droptr)","a7b28543":"#tuning the model using criterion, n_estimators, bootstrap and max_depth\nparam = {\n    'criterion': ['entropy', 'gini'],\n    'n_estimators': [10,20,30,40,50],\n    'bootstrap': ['True', 'False'],\n    'max_depth': [2,3,4,5]\n}\ngrid_svc = GridSearchCV(rf, param_grid=param, scoring='accuracy', cv=10)","467352c7":"grid_svc.fit(X_droptr, y_droptr)","0b34299d":"grid_svc.best_params_","8518a21c":"rf= ensemble.RandomForestClassifier(bootstrap= 'True', criterion= 'entropy', max_depth= 4, n_estimators=50)","ede44ee8":"rf.fit(X_droptr, y_droptr)","8b212adf":"#checking the accuracy score of the Random Forest Model\nrf.score(X_droptr, y_droptr)","752bfa00":"#predicting the values through Random Forest\ny_predRF= rf.predict(X_dropts)","c558003a":"print('The accuracy score of the model is:', accuracy_score(y_dropts,y_predRF)*100)\nprint('The F1 score of the model is:', f1_score(y_dropts, y_predRF)*100)\nprint('The recall score of the model is:', recall_score(y_dropts, y_predRF)*100)\nprint('The confusion matrix of the model is:', confusion_matrix(y_dropts, y_predRF))\nprint('The classification report of base model is:', classification_report(y_dropts, y_predRF))","0f4ebcdc":"cm_log= confusion_matrix(y_dropts, y_underlog)\ncm_DT= confusion_matrix(y_dropts, y_undersDT)\ncm_RF= confusion_matrix(y_dropts, y_predRF)","5f879b19":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes After Undersampling\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_log,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"Decision Tree Confusion Matrix\")\nsns.heatmap(cm_DT,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_RF,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","4881a45d":"#this clearly shows that the model is overfit or is only considering the values which is high in number\n#we need to balance the data which could be done in 2 ways\n#either we can undersample the data by dropping the values or oversample using SMOTE\n#three models i.e. Logistic Regression, Decision Tree and Random Forest will be created after undersampling \n#confusion matrix for each models will be displayed at the end\n","5cdb9dc9":"#Preparing the data for the model\n#first creating a base model"}}