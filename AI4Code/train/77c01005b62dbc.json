{"cell_type":{"790e6d0e":"code","64f5d069":"code","3f9aacf3":"code","bb5f7427":"code","4c14aba1":"code","633821e2":"code","da588e66":"code","08892ad7":"code","2249f707":"code","10a56027":"code","0b49d213":"markdown","954d93ca":"markdown","9b7de668":"markdown","ad76c50f":"markdown","5c2989d6":"markdown","b9f717ab":"markdown","1d3d7736":"markdown","ec133aed":"markdown","ff9e6b88":"markdown"},"source":{"790e6d0e":"import sys\n!cp ..\/input\/rapids\/rapids.0.12.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","64f5d069":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom scipy.stats import mode\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom cuml.neighbors import KNeighborsClassifier, NearestNeighbors\nimport cuml; cuml.__version__","3f9aacf3":"train = pd.read_csv('\/kaggle\/input\/data-without-drift\/train_clean.csv')\ntrain['group'] = -1\nx = [(0,500000),(1000000,1500000),(1500000,2000000),(2500000,3000000),(2000000,2500000)]\nfor k in range(5): train.iloc[x[k][0]:x[k][1],3] = k\n    \nres = 1000\nplt.figure(figsize=(20,5))\nplt.plot(train.time[::res],train.signal[::res])\nplt.plot(train.time,train.group,color='black')\nplt.title('Clean Train Data. Blue line is signal. Black line is group number.')\nplt.xlabel('time'); plt.ylabel('signal')\nplt.show()","bb5f7427":"test = pd.read_csv('\/kaggle\/input\/data-without-drift\/test_clean.csv')\ntest['group'] = -1\nx = [[(0,100000),(300000,400000),(800000,900000),(1000000,2000000)],[(400000,500000)], \n     [(100000,200000),(900000,1000000)],[(200000,300000),(600000,700000)],[(500000,600000),(700000,800000)]]\nfor k in range(5):\n    for j in range(len(x[k])): test.iloc[x[k][j][0]:x[k][j][1],2] = k\n        \nres = 400\nplt.figure(figsize=(20,5))\nplt.plot(test.time[::res],test.signal[::res])\nplt.plot(test.time,test.group,color='black')\nplt.title('Clean Test Data. Blue line is signal. Black line is group number.')\nplt.xlabel('time'); plt.ylabel('signal')\nplt.show()","4c14aba1":"step = 0.2\npt = [[],[],[],[],[]]\ncuts = [[],[],[],[],[]]\nfor g in range(5):\n    mn = train.loc[train.group==g].signal.min()\n    mx = train.loc[train.group==g].signal.max()\n    old = 0\n    for x in np.arange(mn,mx+step,step):\n        sg = train.loc[(train.group==g)&(train.signal>x-step\/2)&(train.signal<x+step\/2)].open_channels.values\n        if len(sg)>100:\n            m = mode(sg)[0][0]\n            pt[g].append((x,m))\n            if m!=old: cuts[g].append(x-step\/2)\n            old = m\n    pt[g] = np.vstack(pt[g])\n    \nmodels = ['1 channel low prob','1 channel high prob','3 channel','5 channel','10 channel']\nplt.figure(figsize=(15,8))\nfor g in range(5):\n    plt.plot(pt[g][:,0],pt[g][:,1],'-o',label='Group %i (%s model)'%(g,models[g]))\nplt.legend()\nplt.title('Traing Data Open Channels versus Clean Signal Value',size=16)\nplt.xlabel('Clean Signal Value',size=16)\nplt.ylabel('Open Channels',size=16)\nplt.show()","633821e2":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfor g in range(5):\n    if g==0: res = 100\n    else: res = 10\n\n    plt.figure(figsize=(16,8))\n    plt.subplot(1,2,1)\n    for k in range(0,11):\n        idx = np.array( train.loc[(train.open_channels==k) & (train.group==g)].index )\n        if len(idx)==0: continue\n        plt.scatter(train.signal[idx-1],train.signal[idx],s=0.01,label='%i open channels'%k)\n    plt.xlabel('Previous Signal Value',size=14)\n    plt.ylabel('Signal Value',size=14)\n    lgnd = plt.legend(numpoints=1, fontsize=10)\n    for k in range( len(lgnd.legendHandles) ):\n        lgnd.legendHandles[k]._sizes = [30]\n    \n    data = test.loc[test.group==g]\n    #plt.scatter(data.signal[:-1][::res],data.signal[1:][::res],s=0.1,color='black')\n    xx = plt.xlim(); yy = plt.ylim()\n    for k in range(len(cuts[g])):\n        if (g!=4)|(k!=0): plt.plot([xx[0],xx[1]],[cuts[g][k],cuts[g][k]],':',color='black')\n    plt.title('Train Data in group %i'%g,size=16)\n    \n    plt.subplot(1,2,2)\n    plt.scatter(data.signal[:-1][::res],data.signal[1:][::res],s=0.1,color='black')\n    plt.xlim(xx); plt.ylim(yy)\n    for k in range(len(cuts[g])):\n        if (g!=4)|(k!=0): plt.plot([xx[0],xx[1]],[cuts[g][k],cuts[g][k]],':',color='black')\n        if (g==4)&(k!=0): plt.text(xx[0]+1,cuts[g][k],'%i open channels'%(k+2),size=12)\n        elif g!=4: plt.text(xx[0]+1,cuts[g][k],'%i open channels'%(k+1),size=14)\n    plt.xlabel('Previous Signal Value',size=14)\n    plt.ylabel('Signal Value',size=14)\n    plt.title('Unknown Test Data in group %i'%g,size=16)\n\n    plt.show()","da588e66":"def wiggle(df, row, plt, xx=None, yy=None):\n    plt.plot([-3,-2,-1,0,1,2,3],df.loc[df.index[row-3:row+4],'signal'],'-')\n    sizes = np.array([1,2,3,12,3,2,1])*50\n    colors = ['red','red','red','green','blue','blue','blue']\n    for k in range(7):\n        plt.scatter(k-3,df.loc[df.index[row+k-3],'signal'],s=sizes[k],color=colors[k])\n    if xx!=None: plt.xlim(xx)\n    if yy!=None: plt.ylim(yy)\n    return plt.xlim(),plt.ylim()\n\nrow=2; col=4;\nnp.random.seed(42)\nplt.figure(figsize=(4*col,4*row))\nfor k in range(row*col):\n    plt.subplot(row,col,k+1)\n    r = np.random.randint(2e6)\n    wiggle(test,r,plt)\n    if k%col==0: plt.ylabel('signal')\n    g = test.loc[r,'group']\n    plt.title('Test row %i group %i'%(r,g))\nplt.tight_layout(pad=3.0)\nplt.show()","08892ad7":"%%time\n\nKNN = 100\nbatch = 1000\n\ntest_pred = np.zeros((test.shape[0]),dtype=np.int8)\nfor g in [0,1,2,3,4]:\n    print('Infering group %i'%g)\n    \n    # TRAIN DATA\n    data = train.loc[train.group==g]\n    X_train = np.zeros((len(data)-6,7))\n    X_train[:,0] = 0.25*data.signal[:-6]\n    X_train[:,1] = 0.5*data.signal[1:-5]\n    X_train[:,2] = 1.0*data.signal[2:-4]\n    X_train[:,3] = 4.0*data.signal[3:-3]\n    X_train[:,4] = 1.0*data.signal[4:-2]\n    X_train[:,5] = 0.5*data.signal[5:-1]\n    X_train[:,6] = 0.25*data.signal[6:]\n    y_train = data.open_channels[3:].values\n\n    # TEST DATA\n    data = test.loc[test.group==g]\n    X_test = np.zeros((len(data)-6,7))\n    X_test[:,0] = 0.25*data.signal[:-6]\n    X_test[:,1] = 0.5*data.signal[1:-5]\n    X_test[:,2] = 1.0*data.signal[2:-4]\n    X_test[:,3] = 4.0*data.signal[3:-3]\n    X_test[:,4] = 1.0*data.signal[4:-2]\n    X_test[:,5] = 0.5*data.signal[5:-1]\n    X_test[:,6] = 0.25*data.signal[6:]\n\n    # HERE IS THE CORRECT WAY TO USE CUML KNN \n    #model = KNeighborsClassifier(n_neighbors=KNN)\n    #model.fit(X_train,y_train)\n    #y_hat = model.predict(X_test)\n    #test_pred[test.group==g][1:-1] = y_hat\n    #continue\n    \n    # WE DO THIS BECAUSE CUML v0.12.0 HAS A BUG\n    model = NearestNeighbors(n_neighbors=KNN)\n    model.fit(X_train)\n    distances, indices = model.kneighbors(X_test)\n\n    # FIND PREDICTIONS OURSELVES WITH STATS.MODE\n    ct = indices.shape[0]\n    pred = np.zeros((ct+6),dtype=np.int8)\n    it = ct\/\/batch + int(ct%batch!=0)\n    #print('Processing %i batches:'%(it))\n    for k in range(it):\n        a = batch*k; b = batch*(k+1); b = min(ct,b)\n        pred[a+3:b+3] = mode( y_train[ indices[a:b].astype(int) ], axis=1)[0][:,0]\n        #print(k,', ',end='')\n    #print()\n    test_pred[test.group==g] = pred","2249f707":"data1 = test.loc[test.group==4].iloc[3:]\ndata1.reset_index(inplace=True)\n\ndata2 = train.loc[train.group==4].iloc[3:]\ndata2.reset_index(inplace=True)\n\nfor j in range(5):\n    r = np.random.randint(data1.shape[0])\n    distances, indices = model.kneighbors(X_test[r:r+1,])\n\n    row=2; \n    plt.figure(figsize=(16,row*4))\n    for k in range(row*4):\n        if k in [1,2,3]: continue\n        plt.subplot(row,4,k+1)\n        if k==0: \n            xx,yy = wiggle(data1,r,plt)\n            g = data1.loc[r,'group']\n            rw = data1.loc[r,'index']\n            plt.title('UNKNOWN Test row %i group %i'%(rw,g))\n        else:\n            r=indices[0,k-4].astype('int')\n            wiggle(data2,r,plt,xx,yy)\n            g = data2.loc[r,'group']\n            rw = data2.loc[r,'index']\n            t = data2.loc[r,'open_channels']\n            plt.title('LABEL = %i. Train row %i group %i'%(t,rw,g))\n        if k%4==0: plt.ylabel('signal')\n    plt.tight_layout(pad=3.0)\n    plt.show()","10a56027":"sub = pd.read_csv('..\/input\/liverpool-ion-switching\/sample_submission.csv')\nsub.open_channels = test_pred\nsub.to_csv('submission.csv',index=False,float_format='%.4f')\n\nres=200\nplt.figure(figsize=(20,5))\nplt.plot(sub.time[::res],sub.open_channels[::res])\nplt.show()","0b49d213":"# Visualize kNN\nBelow are some random rows from test data group 4 and the 4 closest kNN training rows. We then use the most common known training label to predict the unknown test label.","954d93ca":"# Seven Feature Model\nNow if we wish to predict a row of the test data, we will use the three rows before and three rows after. In total we will use 7 features. We will use simple kNN compared with the train data. We can visualize a row of test data as a \"wiggle\". The green dot is the unknown test row and it's signal value can be found on the y axis. The red dots are the 3 rows before and the blue dots are the 3 rows after. Given a unknown test data \"wiggle\", our job is to find the most similar looking training data \"wiggle\".","9b7de668":"# Load Clean Competition Data\nWe will use the Kaggle dataset [here][1] containing this competition's train and test data with drift removed. Drift is explained [here][2]. And visualized [here][3]. This competition's data is believed to be `data = computer generated + real life noise + synthetic drift`. Drift was found in training data batches 2, 7, 8, 9, 10. And test batches 1, 2, 3. [Markus][5] demonstrated in his great notebook [here][4] that drift can be cleanly removed with 4th order approximation functions. \n\nClean data allows to produce better EDA and allows us to use models like kNN.\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/data-without-drift\n[2]: https:\/\/www.kaggle.com\/c\/liverpool-ion-switching\/discussion\/133874\n[3]: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930\n[4]: https:\/\/www.kaggle.com\/friedchips\/clean-removal-of-data-drift\n[5]: https:\/\/www.kaggle.com\/friedchips","ad76c50f":"# Install RAPIDS 0.12.0","5c2989d6":"# One Feature Model\nWith clean data, we can now make a one feature model. A full explanation of the one feature model is [here][1]. Summary of one feature model is below. If you have an unknown test row, then first you determine which group that row belongs to. Next you find the signal from that row on the x axis below and find the corresponding open channels prediction on the y axis below. This model has CV 0.926 and LB 0.929.\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/one-feature-model-0-930","b9f717ab":"# RAPIDS cuML GPU kNN\nGiven an unknown test \"wiggle\" we need to search all 500,000 \"wiggles\" from the train data's similar group. That's a lot of kNN searching. If we use CPU, this will takes days! With GPU we can predict all the 2,000,000 (two million) unknown test data in 30 seconds. We need to compare each unknown test row with 500,000 (five hundred thousand) train rows. So that is 1,000,000,000,000 (one trillion) comparisons!\n\nNormally, we would use three RAPIDS cuML function calls. Normally we call `model = KNeighborsClassifier(n_neighbors=KNN)` then `model.fit(X_train,y_train)` and finally `y_hat = model.predict(X_test)`. However RAPIDS is under development and the current version 0.12.0 of RAPIDS throws some errors when we do that. So instead we will call `model = NearestNeighbors(n_neighbors=KNN)` then `model.fit(X_train)` and finally `distances, indices = model.kneighbors(X_test)`. This returns all the neighbors successfully. With the neighbors, we then use `scipy.stats.mode` to find the prediction. This extra work takes a total of 2 minutes whereas it should be 30 seconds with the usual function calls.","1d3d7736":"# Simple kNN scores LB 0.938 in 30 seconds!\nIn this notebook, we build a simple model using [Nvidia RAPIDS][1] cuML's blazingly fast kNN algorithm. The high CV and LB of this model confirms the simple nature of Kaggle's Ion Switching competition data. We will be using this competition's cleaned dataset posted [here][2]. So this notebook also shows that the clean dataset is infact very clean. Feel free to use the clean dataset in your future notebooks and don't forget to upvote the clean dataset :-) The clean dataset uses Markus' great cleaning method [here][3].\n\n[1]: https:\/\/rapids.ai\/\n[2]: https:\/\/www.kaggle.com\/cdeotte\/data-without-drift\n[3]: https:\/\/www.kaggle.com\/friedchips\/clean-removal-of-data-drift","ec133aed":"# Export the Test Predictions","ff9e6b88":"# Two Feature Model\nWe can improve the one feature model by adding a second feature. For every unknown test row, we will also use the preceeding row's signal value. This model has CV 0.9295 and LB 0.932. Below are plots of the two feature model. When given an unknown test row. Just find the most similar training row that has the same signal and preceeding row signal.\n\nSpecifically, here is how to use the plots below to predict an unknown test row. First determine what group the test row is in and find the corresponding group plots below. Next match the signal to the y axis and the preceeding row's signal to the x axis. Whatever color is beneath your point is the number of open channels you will predict."}}