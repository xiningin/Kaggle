{"cell_type":{"03915977":"code","f2273ff3":"code","a7b7f746":"code","d412e677":"code","8f84317d":"code","e58c9771":"code","f6ed3bca":"code","7a5c3bff":"code","cfc727b9":"code","8699f126":"code","bf9d816f":"code","e0c6c966":"code","a04e8868":"code","5471205a":"code","6a1b2a76":"code","ddbc0c09":"code","bb4e896e":"code","da1e2c68":"code","fc3f068a":"code","3afd2126":"markdown","311e3c71":"markdown","11149ad0":"markdown","dfec83aa":"markdown","0e5c5ebc":"markdown","eb8c84dd":"markdown","29c270af":"markdown","b4e36116":"markdown","5d6b9428":"markdown"},"source":{"03915977":"######################################################################################\n# Handle warnings\n######################################################################################\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\n\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\nwarnings.simplefilter(action='ignore', category=FutureWarning) # Scipy warnings\n\n######################################################################################\n# Standard imports\n######################################################################################\nimport shutil\nimport os\nimport math\nfrom copy import copy\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.models import load_model\nfrom keras.layers import average\nfrom keras.layers import Input\nfrom keras.initializers import glorot_uniform\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')","f2273ff3":"# Load data from kernel eda-and-preprocessing-of-the-datasets\ntrain_path = \"..\/input\/eda-and-preprocessing-of-the-datasets\/preprocessed_train.csv\"\ntest_path = \"..\/input\/eda-and-preprocessing-of-the-datasets\/preprocessed_test.csv\"\n\n# Load the dataset with Pandas\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)","a7b7f746":"print(train.shape)\ntrain.head()","d412e677":"print(test.shape)\ntest.head()","8f84317d":"# Number of  samples\nn_train = train.shape[0]\nn_test = test.shape[0]\n\n# Extract the GT labels and IDs from the data\ntrain_y = train['SalePrice']\ntrain_ids = train['Id']\ntest_ids = test['Id']\n\ntrain = train.drop('SalePrice', axis=1)\ntrain = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)","e58c9771":"# Plot the results of the training\ndef plot_history(history):\n    fig = plt.figure(figsize=(15,8))\n    ax = plt.subplot(211)\n    \n    plt.xlabel('Epoch')\n    plt.ylabel('MALE, MSLE [1000$]')\n    \n    # Losses\n    ax.set_yscale('log')\n    ax.plot(history.epoch, history.history['loss'], label='Train LOSS')\n    ax.plot(history.epoch, history.history['val_loss'], label='Val LOSS')\n    ax.plot(history.epoch, history.history['mean_absolute_error'], label='Train MALE')\n    ax.plot(history.epoch, history.history['mean_squared_error'], label='Train MSLE')\n    ax.plot(history.epoch, history.history['val_mean_squared_error'], label ='Val MSLE')\n    ax.plot(history.epoch, history.history['val_mean_absolute_error'], label='Val MALE')\n    plt.legend()\n    \n    # Plot the learning_rate\n    if 'lr' in history.history:\n        ax = plt.subplot(212)\n        plt.ylabel('Learning rate')\n        ax.plot(history.epoch, history.history['lr'], label='learning_rate')\n        plt.legend()\n    plt.show()\n    plt.close(fig)","f6ed3bca":"def reset_weights(model):\n    session = K.get_session()\n    for layer in model.layers: \n        if hasattr(layer, 'kernel_initializer'):\n            layer.kernel.initializer.run(session=session)","7a5c3bff":"def get_callbacks(bm_path):\n    checkpointer = keras.callbacks.ModelCheckpoint(\n        filepath=bm_path,\n        monitor='val_loss', \n        save_best_only=True, \n        save_weights_only=False, \n        mode='auto', \n        period=1\n    )\n\n    # Scheduling the learning rate\n    lr_patience = 10\n    lr_scheduler = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.95,\n        patience=lr_patience,\n        verbose=0,\n        mode='auto',\n        min_delta=0.0001,\n        cooldown=25,  # Min. number of epochs until next lr drop\n        min_lr=0\n    )\n    \n    return lr_scheduler, checkpointer","cfc727b9":"def get_model(input_shape, num_neurons):\n    model = keras.Sequential([\n        # Input layer\n        keras.layers.Dense(num_neurons, activation='linear', input_shape=input_shape),\n        keras.layers.Dropout(0.25),\n        keras.layers.LeakyReLU(alpha=0.1),\n        \n        # Output layer\n        keras.layers.Dense(1)\n    ])\n    return model","8699f126":"X = train.values\nY = train_y.values\n\nBATCH_SIZE = 2\nNUM_FOLDS = 5\nEPOCHS = 250\nHOLDOUT_DICT = {'fold{}'.format(fc) : {} for fc in range(NUM_FOLDS)}\n\nif os.path.exists('out'):\n    shutil.rmtree('out')\nos.makedirs('out')\n\nBASE_MODELS = {'m0' : 16, 'm1' : 32, 'm2' : 64, 'm3' : 128}","bf9d816f":"best_models = {mod_name : [] for mod_name in BASE_MODELS.keys()}\nr2_stacked = []\nmse_stacked = []\nmae_stacked = []\nrmsle_stacked = []\n\nkfold = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n\nfold_counter = 0\nfor train_idxs, test_idxs in kfold.split(X, Y):\n    \n    HOLDOUT_DICT['fold{}'.format(fold_counter)]['gt'] = Y[test_idxs]\n    \n    for model_name, model_config in BASE_MODELS.items(): \n        \n        # Integrate model config\n        model = get_model((train.shape[1],), num_neurons=model_config)\n        \n        # Be sure the weights are randomized\n        reset_weights(model)\n\n        optimizer = 'adam'\n        model.compile(loss='mse', optimizer=optimizer, metrics=['mse','mae'])\n        \n        bm_path = \"out\/fold{}{}_config{}.h5\".format(fold_counter, model_name, model_config)\n\n        # Get callbacks for the training process\n        lr_scheduler, checkpointer = get_callbacks(bm_path)\n\n        history = model.fit(\n            x=X[train_idxs],\n            y=Y[train_idxs],\n            validation_data=(X[test_idxs], Y[test_idxs]),\n            batch_size=BATCH_SIZE,\n            epochs=EPOCHS,\n            callbacks=[\n                checkpointer, \n                #lr_scheduler\n            ],\n            verbose=0,\n            shuffle=True\n        )\n\n        model.load_weights(bm_path)\n        predict_val = model.predict(X[test_idxs]).flatten()\n        \n        HOLDOUT_DICT['fold{}'.format(fold_counter)]['{}'.format(model_name)] = predict_val.copy()\n        \n        r2 = r2_score(predict_val, Y[test_idxs])\n        mse = mean_squared_error(predict_val, Y[test_idxs])\n        mae = mean_absolute_error(predict_val, Y[test_idxs])\n        rmsle = math.sqrt(mse)\n\n        best_models[model_name].append(bm_path)\n        mse_stacked.append(mse)\n        mae_stacked.append(mae)\n        rmsle_stacked.append(rmsle)\n        r2_stacked.append(r2)\n\n        plot_history(history)\n        print(\"{}\/{} folds model {}: MSE: {} || MAE: {} || RMSLE: {} || R2: {}\".format(fold_counter + 1, NUM_FOLDS, model_name, mse, mae, rmsle, r2))\n\n        K.clear_session()\n    fold_counter += 1\n\nprint(\"AVG_RMSLE: {} || AVG_R2: {}\".format(np.mean(rmsle_stacked), np.mean(r2_stacked)))","e0c6c966":"df_list = []\nfor fold, fold_data in HOLDOUT_DICT.items():\n    df_list.append(pd.DataFrame(fold_data))\n\nholdouts_df = pd.concat(df_list)\nprint(holdouts_df.head(3))\nprint(holdouts_df.shape)","a04e8868":"from sklearn.linear_model import Lasso","5471205a":"meta_model = Lasso(alpha=0.0005, random_state=1)\nmeta_model.fit(X=holdouts_df[list(BASE_MODELS.keys())], y=holdouts_df['gt'])\nrmsle = math.sqrt(mean_squared_error(meta_model.predict(holdouts_df[list(BASE_MODELS.keys())]), holdouts_df['gt']))\nprint(rmsle)","6a1b2a76":"# Stack the best model of each fold together\ndef ensemble_models(models, model_input):\n    # collect outputs of models in a list\n    yModels = [model(model_input) for model in models] \n    \n    # averaging outputs\n    yAvg = average(yModels) \n    \n    # build model from same input and avg output\n    modelEns = Model(inputs=model_input, outputs=yAvg, name='ensembleModel')  \n    return modelEns","ddbc0c09":"ensembles = {}\n\nfor model_name, bm_paths in best_models.items():\n    temp_ens_models = []\n    for model_path in bm_paths:\n        model_temp = load_model(model_path)\n        model_temp.name = model_path\n        temp_ens_models.append(model_temp)\n    ens_input = keras.Input(shape=temp_ens_models[0].input_shape[1:])\n    ensembles[model_name] = ensemble_models(temp_ens_models, ens_input)\n    \n    ensembles[model_name].save('out\/ensemble_base_model_{}.h5'.format(model_name))\n    print(ensembles[model_name].summary())","bb4e896e":"def metamodel_predict(ensemble_models_dict, meta_model, data):\n    res = {}\n    for model_name, model in ensemble_models_dict.items():\n        res[model_name] = model.predict(data).flatten()\n    \n    res_df = pd.DataFrame(res)\n    meta_res = meta_model.predict(res_df.values)\n    res_df['meta'] = meta_res\n    \n    return res_df, meta_res","da1e2c68":"res_df, meta_res = metamodel_predict(ensembles, meta_model, X)\nfor col in res_df.columns:\n    pred = res_df[col]\n    print(\"Model {}: RMSLE: {} || R2: {}\".format(col, math.sqrt(mean_squared_error(pred, Y)), r2_score(pred, Y)))","fc3f068a":"res_submission_df, meta_res_submission = metamodel_predict(ensembles, meta_model, test)\nsub = pd.DataFrame()\nsub['Id'] = test_ids\nsub['SalePrice'] = np.expm1(meta_res_submission)\nsub.to_csv(os.path.join('submission_sm.csv') ,index=False)\n\nsub.head()","3afd2126":"## Prepare the training data and model parameter","311e3c71":"# Start the modelling process","11149ad0":"# Ensemble the models to one big model","dfec83aa":"## Load the datasets","0e5c5ebc":"## Start the training process","eb8c84dd":"# Stacked Neural Networks\nIdea of this kernel is to adapt the idea of ensemble multiple model behind one final meta model. It offers not the best results, but it shows the basic workflow and architecture.\n\nThe data preprocessing is imorted from this separate kernel: [Preprocessing](https:\/\/www.kaggle.com\/blaskowitz100\/eda-and-preprocessing-of-the-datasets)","29c270af":"### Predict the complete train dataset","b4e36116":"# Make the prediction for the submission","5d6b9428":"## Train a meta model"}}