{"cell_type":{"7ddc8f24":"code","b3909548":"code","7ed3ba44":"code","5d855455":"code","aa20b561":"code","9565b2ff":"code","dcc060c9":"code","639171a9":"code","42f25df2":"code","8b9b9ef5":"code","274fbff4":"code","9b4a9a3c":"code","e8ca8c54":"code","d4686663":"code","718344aa":"code","c077ea29":"markdown","aca16f79":"markdown","73f25714":"markdown","f8593d6e":"markdown","f5e1ed33":"markdown","9c788846":"markdown","acd8fcb6":"markdown","c0299de8":"markdown","1fee1f0c":"markdown","ec155ef6":"markdown"},"source":{"7ddc8f24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n# Any results you write to the current directory are saved as output.","b3909548":"train_data = pd.read_csv('..\/input\/train.csv', delimiter=',')\ntrain_data.head()","7ed3ba44":"X = train_data[['Id', 'ciphertext']]\ny = train_data['target']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n#I use these samples for testing the steps without waiting a long time\nsample_size = 1000\nX_sample = X_train.iloc[0:sample_size,0:2] #rows, columns\ny_sample = y_train.iloc[0:sample_size] #rows, columns","5d855455":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords \nimport nltk\n\ndef Tokenizer(str_input):\n    str_input = str_input.lower()\n    words = word_tokenize(str_input)\n    #remove stopwords\n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    #stem the words\n    porter_stemmer=nltk.PorterStemmer()\n    words = [porter_stemmer.stem(word) for word in words]\n    return words","aa20b561":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom xgboost import XGBClassifier\n\ntext_clf = Pipeline([\n    ('tfidf', TfidfVectorizer(tokenizer=Tokenizer, max_df=0.3, min_df=0.001, max_features=100000)),\n    #('svd',   TruncatedSVD(algorithm='randomized', n_components=500)),\n    ('clf',   XGBClassifier(objective='multi:softmax', n_estimators=500, num_class=20, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8, eval_metric='merror')),\n])","9565b2ff":"from sklearn.model_selection import GridSearchCV\n\nparameters = {\n    #'tfidf__max_df': (0.25, 0.5, 0.75),\n    #'tfidf__min_df': (0.001, 0.0025, 0.005),\n    #'tfidf__max_features': (50000, 100000, 150000),\n    #'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    #'tfidf__use_idf': (True, False),\n    # 'tfidf__norm': ('l1', 'l2'),\n    #'svd__n_components': (250, 500, 750),\n    #'clf__n_estimators': (250, 500, 750),\n    'clf__max_depth': (4, 6, 8),\n    'clf__min_child_weight': (1, 5, 10),\n    #'clf__alpha': (0.00001, 0.000001),\n    #'clf__penalty': ('l2', 'elasticnet'),\n    #'clf__max_iter': (10, 50, 80),\n}\n\n#gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n#gs_clf.fit(X_sample.message, y_sample)\n\n#print(\"Best score: %0.3f\" % gs_clf.best_score_)\n#print(\"Best parameters set:\")\n#best_parameters = gs_clf.best_estimator_.get_params()\n#for param_name in sorted(parameters.keys()):\n#    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","dcc060c9":"text_clf.fit(X_train.ciphertext, y_train)\npredictions = text_clf.predict(X_test.ciphertext)\nprint(\"The training predictions are ready\")","639171a9":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.tokenize import word_tokenize\n\nvectorizer = TfidfVectorizer(tokenizer=Tokenizer, min_df=0.001, max_df=0.3)\nX_tfidf = vectorizer.fit_transform(X.ciphertext)\n#vectorizer.get_feature_names()\nX_tfidf.shape","42f25df2":"from sklearn.decomposition import TruncatedSVD\n\nsvd_transformer = TruncatedSVD(algorithm='randomized', n_components=300)\nX_svd = svd_transformer.fit_transform(X_tfidf)\nX_svd.shape","8b9b9ef5":"from xgboost import XGBClassifier\n\nxgb_classifier = XGBClassifier(max_depth=3, n_estimators=1000, learning_rate=0.075, colsample_bytree=0.7, subsample=0.8)\nxgb_classifier.fit(X_svd, y)\nprint(\"The model is ready.\")","274fbff4":"X_test = pd.read_csv('..\/input\/classifying-20-newsgroups-test\/test.csv', delimiter=',')\nX_test.head()","9b4a9a3c":"X_test_tfidf = vectorizer.transform(X_test.ciphertext)\nX_test_svd = svd_transformer.transform(X_test_tfidf)\n\nxgb_predictions = xgb_classifier.predict(X_test_svd)\npredictions = xgb_predictions\npredictions[0:10]","e8ca8c54":"from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n\nprint(\"Accuracy:\", accuracy_score(y_test, predictions))\nprint(\"Precision:\", precision_score(y_test, predictions, average='weighted'))\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))","d4686663":"output = X_test.copy()\noutput.insert(2, 'target', predictions)\noutput.to_csv('submission.csv', sep=',', columns=['id', 'topic'], index=False)\nprint(os.listdir(\"..\/working\"))\noutput.iloc[1000:5010, :]","718344aa":"results = pd.read_csv('submission.csv', delimiter=',')\nresults.iloc[5000:5010, :]","c077ea29":"XGBoost won't accept the sparse matrix that comes from TFIDFVectorizer.  We will use the TruncatedSVD transformer to change the matrix into one that XGBoost can work with.  This is way complicated stuff.","aca16f79":"I'm switching to a pipeline.  It makes building multiple models with seperate data sets easier.  Also allows for Grid Search to work on hyper parameters.","73f25714":"Below are some metrics to measure interative tweaks to the model.","f8593d6e":"Ready to package up the predictions and create the file to be submitted for scoring.","f5e1ed33":"This is a tokenizer that will be used when transforming the message to a Bag of Words.","9c788846":"### Thanks to this kernel:\n**https:\/\/www.kaggle.com\/collinsjosh\/xgboost-classifier**","acd8fcb6":"This last block just gives a peek into the submission file to sanity check it.","c0299de8":"I'm partitioning the training data into train and test sets just so I have something to test against without having to go to the online submission form of the competition to get a result.  I wish the whole result set was provided!","1fee1f0c":"Import the data that we will use to learn from.","ec155ef6":"Now we can build the input vectors for the classifier with the TFIDFVectorizer."}}