{"cell_type":{"9386495a":"code","533708ee":"code","c53fe496":"code","13129008":"code","7fd5c881":"code","34bafbda":"code","15e4b159":"code","8a153462":"code","845a2c97":"code","a367378a":"code","2f390118":"code","6691f561":"code","411584f8":"code","271ef361":"code","784e70ef":"code","0f0b732e":"code","11daf79b":"code","2ceebfe2":"code","843882c9":"code","e2df660b":"code","07a354fd":"code","201cf2a0":"code","0cafd61b":"code","980ae9d1":"code","724f14fd":"code","2afa3df9":"code","395b2c37":"code","b761e0a3":"code","a86701bf":"code","39f652ec":"code","9434e1ed":"code","fae78593":"code","a9b0eb38":"code","13beada4":"code","bbd199c9":"markdown","356126d7":"markdown","6bd2826b":"markdown","0c389287":"markdown","19c05428":"markdown","b4a7acbe":"markdown","7fe24d03":"markdown","a35f48a3":"markdown","01b5146d":"markdown","d38ccb88":"markdown","902692e5":"markdown","4db76130":"markdown","160e1685":"markdown","86b25c7c":"markdown","dbe85dd8":"markdown","20e31ca5":"markdown","7ea4e165":"markdown","30864b3a":"markdown","b732173c":"markdown"},"source":{"9386495a":"%reset -f\n\n# Import modules\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.neural_network import MLPClassifier\nimport sys\nfrom utils import *\n\nmpl.rcParams.update({'font.size': 13})\nmpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=wong_colors.values())\n\nprint('Python version', sys.version.split()[0])","533708ee":"# Import training and test sets\nfolder = '\/kaggle\/input\/titanic\/'\n\ntrain = pd.read_csv(folder + 'train.csv')\ntest = pd.read_csv(folder + 'test.csv')\n\nprint('Training set shape:', train.shape)\nprint('Test set shape:', test.shape)\n\n# Convert columns names to lowercase\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","c53fe496":"train.head()","13129008":"pd.concat([train, test]).describe()","7fd5c881":"df_info(train)","34bafbda":"df_info(test)","15e4b159":"train.loc[train['embarked'].isnull(), 'embarked'] = train['embarked'].mode()[0]","8a153462":"test.loc[test['fare'].isnull(), 'fare'] = test['fare'].mean()","845a2c97":"def plot_discrete_feature(train, feature):\n\n    survived = train.groupby(feature)['survived'].mean() * 100\n    x = range(len(survived))\n\n    _, axs = plt.subplots(1, 2, figsize=(14,5))\n\n    plt.sca(axs[0])\n    plt.bar(x, train.groupby(feature)['survived'].size())\n    plt.ylabel('N. of passengers')\n\n    plt.sca(axs[1])\n    plt.bar(x, survived)\n    plt.bar(x, 100 - survived, bottom=survived)\n    plt.ylabel('Survival percentage')\n    plt.legend(['Survived', 'Not survived'], loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n\n    \n    for ax in axs:\n        set_spines_vis(ax=ax)      \n        ax.set_xticks(x)\n        ax.set_xticklabels(survived.index)\n        ax.set_xlabel(feature)","a367378a":"def plot_continuous_feature(train, feature, binwidth):\n\n    plt.figure(figsize=(10,5))\n    sns.histplot(train, x=feature, hue='survived', binwidth=binwidth, legend=False)\n    plt.legend(['Survived', 'Not survived'], frameon=False)\n    set_spines_vis()","2f390118":"plot_discrete_feature(train, 'pclass')","6691f561":"plot_continuous_feature(train, 'fare', 5)","411584f8":"plot_discrete_feature(train, 'sex')","271ef361":"plot_continuous_feature(train, 'age', 1)","784e70ef":"plot_discrete_feature(train, 'sibsp')","0f0b732e":"plot_discrete_feature(train, 'parch')","11daf79b":"plot_discrete_feature(train, 'embarked')","2ceebfe2":"def get_dummies(train, test, columns):\n\n    # Concatenate training and test sets\n    df = pd.concat([train[columns], test[columns]])\n\n    # Convert categorical variables into dummy variables\n    df = pd.get_dummies(df)\n\n    X_train = df.iloc[:train.shape[0]]\n    X_test = df.iloc[train.shape[0]:]\n\n    return X_train, X_test","843882c9":"train['pclass'] = train['pclass'].astype('category')\ntest['pclass'] = test['pclass'].astype('category')\n\nfeatures = ['pclass', 'sex', 'sibsp', 'parch', 'embarked']\nX_train, X_test = get_dummies(train, test, features)\n\ny_train = train['survived']","e2df660b":"create_params = {'max_iter':1000, 'random_state':0, 'hidden_layer_sizes': 5}\nclf = MLPClassifier(**create_params)\n\nsklearn_fit_eval(clf, {'X':X_train, 'y':y_train}, cv=5);","07a354fd":"def save_submission(y, passenger, filename):\n    submission = pd.DataFrame({'PassengerId':passenger,'Survived':y})\n    submission.to_csv(filename, index=False)\n\nsave_submission(clf.predict(X_test), test['passengerid'], 'submission_5.csv')","201cf2a0":"train['name'].sample(10)","0cafd61b":"titles = train['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitles.unique()","980ae9d1":"titles_map = {'Mr': 'Mr', 'Master': 'Master'}\ntitles_map.update(dict.fromkeys(['Mrs', 'Ms', 'Mme', 'Ms'], 'Mrs'))\ntitles_map.update(dict.fromkeys(['Miss', 'Mlle'], 'Miss'))\ntitles_map.update(dict.fromkeys(['Capt', 'Col', 'Major', 'Dr', 'Rev'], 'Officer'))\ntitles_map.update(dict.fromkeys(['Jonkheer', 'Don', 'Sir', 'Countess', 'Dona', 'Lady'], 'Royalty'))\n\ndef extract_title(df):\n    df['title'] = df['name'].str.extract(' ([A-Za-z]+)\\.', expand=False).map(titles_map)\n    \nextract_title(train)\nextract_title(test)","724f14fd":"train.groupby('title')['survived'].mean().plot.bar(rot=45, ylabel='Proportion of survivors')\nset_spines_vis()","2afa3df9":"def extract_family(df):\n    surname = df['name'].str.extract('(.+?), ', expand=False)\n    df['family'] = surname + '_' + df['ticket'].str[:-1]\n\nextract_family(train)\nextract_family(test)\n\nfamilies_intersection = set(train['family']) & set(test['family'])\nprint('Number of families contained in both sets:', len(families_intersection))\n\ntrain.loc[~train['family'].isin(families_intersection), 'family'] = np.nan\ntest.loc[~test['family'].isin(families_intersection), 'family'] = np.nan","395b2c37":"features = ['pclass', 'sex', 'sibsp', 'parch', 'embarked', 'title', 'family']\nX_train, X_test = get_dummies(train, test, features)\n\nclf = MLPClassifier(**create_params)\n\nsklearn_fit_eval(clf, {'X':X_train, 'y':y_train}, cv=5);","b761e0a3":"save_submission(clf.predict(X_test), test['passengerid'], 'submission_6.csv')","a86701bf":"# Tune the initial learning rate\nvals = list(np.logspace(-4, -2, 4))\ncreate_params = {'max_iter':1000, 'random_state':0, 'learning_rate_init':vals}\nfit_params = {'X':X_train, 'y':y_train}\n\nsklearn_plot_losses(MLPClassifier, create_params, fit_params, ['{:.5f}'.format(val) for val in vals])\nplt.title('Initial learning rate');","39f652ec":"create_params['learning_rate_init'] = 0.002\n\n# Tune the batch size\ncreate_params['batch_size'] = [128, 256, 512]\n\nsklearn_plot_losses(MLPClassifier, create_params, fit_params)\nplt.title('Batch size');","9434e1ed":"create_params['batch_size'] = 256\ncreate_params['alpha'] = list(np.logspace(-3, 1, 5))\nfit_eval_params = {'fit_params': fit_params, 'cv': 5, 'verbose': False}\n\nplot_models_metrics(MLPClassifier, create_params, sklearn_fit_eval, fit_eval_params, axes_params={'xscale': 'log'})","fae78593":"create_params['alpha'] = 1\n\n# Tune the hidden layer sizes\ncreate_params['hidden_layer_sizes'] = list(range(2, 11))\nfit_eval_params = {'fit_params': fit_params, 'cv': 5, 'verbose': False}\n\nplot_models_metrics(MLPClassifier, create_params, sklearn_fit_eval, fit_eval_params)","a9b0eb38":"create_params['hidden_layer_sizes'] = 6\n\nclf = MLPClassifier(**create_params)\n\nsklearn_fit_eval(clf, {'X':X_train, 'y':y_train}, cv=5);","13beada4":"save_submission(clf.predict(X_test), test['passengerid'], 'submission_7.csv')","bbd199c9":"The columns of this DataFrame are:\n\n* `passengerid` - Unique identifier of each passenger (used for submissions).\n* `survived` - Whether the passenger survived or not (0=No, 1=Yes).\n* `pclass` - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd).\n* `name` - Name of the passenger.\n* `sex` - The passenger's sex.\n* `age` - The passenger's age in years.\n* `sibsp` - The number of siblings or spouses the passenger had aboard the Titanic.\n* `parch` - The number of parents or children the passenger had aboard the Titanic.\n* `ticket` - The passenger's ticket number.\n* `fare` - The fare the passenger paid.\n* `cabin` - The passenger's cabin number.\n* `embarked` - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton).\n\nBoth sets have 11 features. The extra column of the training set is the target variable `survived`.\n\n# 3. Exploratory data analysis\n\nNow let\u2019s look at the columns\u2019 data types and the number of missing values.","356126d7":"Let's calculate our model's accuracy with the additional features `title` and `family`.","6bd2826b":"Let's calculate our model's accuracy with the optimised hyperparameters and predict which passengers of the test set survived.","0c389287":"We will now use the utility function **plot_models_metrics** to compare the training and validation accuracies and training times.","19c05428":"The test set predictions were submitted to Kaggle, and an accuracy of 77.99% was obtained.\n\n# 5. Hyperparameters tuning\n\nWe can also try different combinations of hyperparameters to improve performance. Although several hyperparameters for each model were tunned, only those that led to a considerable improvement are shown. The utility function **sklearn_plot_losses** is used to compare the loss curves.\n","b4a7acbe":"We can observe that:\n\n* Passengers with upper-class tickets had better chances of survival than the rest;\n* Females survived in much higher proportion than males;\n* People younger than five had more chances of surviving, while the rest did not;\n* Passengers with no family members or with a lot of members have fewer chances of surviving;\n* Passengers with lower fares had worse chances of survival;\n* The port where the passenger embarked seems to influence the chances of survival.\n\nThe fare seems to capture the same information as the ticket class, and hence it will not be used. The columns `pclass`, `sex` and `embarked` are categorical features. Although the column `pclass` is numeric, there is not a numeric relationship between the different values. For instance, class 3 is not the triple of class 1. Most machine learning algorithms cannot understand text labels, and so we have to convert our values into numbers. The pandas function [get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) turns each categorical feature into a series of zeros and ones.","7fe24d03":"# Titanic: Neural network optimization\n\n# Table of contents\n\n1. [Introduction](#1.-Introduction)\n2. [Import data](#2.-Import-data)\n3. [Exploratory data analysis](#3.-Exploratory-data-analysis)\n4. [Feature engineering](#4.-Feature-engineering)\n5. [Hyperparameters tuning](#5.-Hyperparameters-tuning)\n6. [Conclusions and further developments](#6.-Conclusions-and-further-developments)\n\n# 1. Introduction\n\nThe objective of the Kaggle competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) is to develop machine learning models that predict which passengers survived the shipwreck. This notebook focus on feature engineering and hyperparameter tuning. If you have further ideas to improve this notebook or anything you don\u2019t understand, please leave a comment.\n\n# 2. Import data","a35f48a3":"The title of each name might be useful information. We will create a new column with this.","01b5146d":"We have too many unique values, which might lead to overfitting. We will group these titles into the categories `Mr`, `Mrs`, `Master`, `Miss`, `Officer` and `Royalty`.","d38ccb88":"The columns `age` and `cabin` have too many missing values and will not be used. Now let's plot our data.","902692e5":"We can see that:\n\n* Only 38.4% of the passengers survived;\n* The age ranges from 0.17 to 80;\n* Some columns have missing values (missing values of column `survived` are from the test set).","4db76130":"The test set predictions were submitted to Kaggle, and an accuracy of 80.86% was obtained.\n\n# 6. Conclusions and further developments\n\nThis notebook uses neural networks to predict which passengers survived the Titanic shipwreck. It explores how feature engineering and hyperparameter optimization influences the accuracy of our model. The best model is 80.86% accurate. To further improve our model, we can:\n\n* Read more about the titanic and this Kaggle competition to get ideas for new features.\n* Use different models such as support vector machines or gradient boosted trees.\n\nAlso, there are several other excellent notebooks related to this competition. Thanks for reading my notebook and any comments and suggestions are very welcome.","160e1685":"We got a validation accuracy of 79.35%. Now let\u2019s calculate the predictions for the test set.","86b25c7c":"The columns' data types seem to be correct, but the following columns have missing values:\n\n* `age` and `cabin` of both datasets\n* `embarked` of the training set\n* `fare` of the test set\n\nWe will fill the two missing values of the column embarked with the mode.","dbe85dd8":"The validation accuracy increased from 79.35% to 82.71%. Now we will predict which passengers of the test set survived.","20e31ca5":"The survival chances of a passenger might be correlated with those of the rest of the family. To extract the families, we assume that if passengers share the same surname and have a similar ticket, they belong to the same family. Tickets are considered to be identical if their last digit is the same.","7ea4e165":"We will use the Scikit-learn [MLPClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html) class to predict if the passengers survived or not and the utility function **sklearn_fit_eval** to evaluate and validate our model.","30864b3a":"The test set predictions were submitted to Kaggle, and an accuracy of 77.03% was obtained.\n\n# 4. Feature engineering\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem and enable more accurate predictive models. We will start with the column `name`.","b732173c":"Regarding the missing fare value, we will assign the mean value."}}