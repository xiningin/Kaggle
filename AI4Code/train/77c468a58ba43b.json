{"cell_type":{"53611018":"code","68896a26":"code","9c43bc62":"code","a22fb159":"code","d3491756":"code","6412d7cd":"code","a1ee75d2":"code","71ccc78d":"code","35318f60":"code","dbd7b669":"code","555b800d":"code","0bc6eb9e":"code","5662c067":"code","c8502c76":"code","b05680d3":"code","b2a6dbd4":"code","537c24dc":"code","a25c315b":"code","0c7c2aab":"code","159fa900":"code","7f96d5b8":"code","3d99ff1a":"code","a5eb12c2":"code","9e6f1c41":"code","81ee8dd2":"code","7288a138":"code","69618286":"code","95968ecd":"code","7b946d91":"code","4747c205":"code","3877f577":"code","f5c9afe1":"code","d245a5d6":"code","043014b7":"code","7c64690f":"code","0be20c0c":"code","f77f4a39":"code","c2ceb1a9":"code","7a40127b":"code","a35ae742":"code","70ade154":"code","5d2c0fc6":"code","5131a983":"code","148c4c6f":"code","4523a46b":"code","2eebe5dc":"code","64101dbb":"code","0ed284da":"code","c483597f":"code","9d9a6bed":"code","8618161a":"code","64a90dda":"code","dc6bd315":"code","744cf950":"code","1832b2cf":"code","a0eb9c5d":"code","266b8774":"code","4ad46968":"code","8dcf1b16":"code","242f3413":"code","828bac5d":"code","16ef16b0":"code","41fce5d2":"markdown","888e9678":"markdown","1eb2c231":"markdown","7a7a61a6":"markdown","2c1510c2":"markdown","f74f9dd7":"markdown","f907407b":"markdown","1c40ba01":"markdown","51dfec24":"markdown","39b42212":"markdown","24e1a789":"markdown","b9d374b3":"markdown","5443c82f":"markdown","d723b364":"markdown","da045c4c":"markdown","c688abcf":"markdown","55bf8d41":"markdown","54643281":"markdown"},"source":{"53611018":"from wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n%matplotlib inline\nimport seaborn as sns\n\nimport numpy as np # linear algebra\nimport pandas as pd #data processing\n\nimport os\nimport re\nimport nltk\n\nimport pandas as pd\nfrom collections import Counter\nimport re\nimport numpy as np\nfrom sklearn.utils import shuffle\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import f1_score, accuracy_score , recall_score , precision_score\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","68896a26":"train=pd.read_csv('..\/input\/fake-news-dataset-2\/all_data_fake_news.csv',nrows=5000)","9c43bc62":"train.head()","a22fb159":"print(train.shape)","d3491756":"print(train.isnull().sum())\nprint('************')\n","6412d7cd":"train=train.fillna(' ')\ntrain['total']=train['Source']+' '+train['Headline']+train['Body']","a1ee75d2":"real_words = ''\nfake_words = ''\nstopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in train[train['Label']==1].total: \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    real_words += \" \".join(tokens)+\" \"\n\nfor val in train[train['Label']==0].total: \n      \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    fake_words += \" \".join(tokens)+\" \"","71ccc78d":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(fake_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","35318f60":"#Remove punctuations from the String  \ns = \"!<\/> hello please$$ <\/>^s!!!u%%bs&&%$cri@@@be^^^&&!& <\/>*to@# the&&\\ cha@@@n##%^^&nel!@# %%$\"","dbd7b669":"s = re.sub(r'[^\\w\\s]','',s)","555b800d":"#if any substring doesn't hold words or space","0bc6eb9e":"print(s)","5662c067":"#Downloading nltk data\nnltk.download('punkt')","c8502c76":"nltk.word_tokenize(\"Hello how are you\")","b05680d3":"from nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\nprint(stop_words)","b2a6dbd4":"print(stop_words)","537c24dc":"stop_words.append('helo')","a25c315b":"sentence = \"Covid-19 pandemic has impacted many countries and what it did to economy is very stressful\"","0c7c2aab":"words = nltk.word_tokenize(sentence)\nwords = [w for w in words if w not in stop_words]","159fa900":"words","7f96d5b8":"from nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\n\ninput_str=\"been had done languages cities mice\"","3d99ff1a":"#Tokenize the sentence\ninput_str=nltk.word_tokenize(input_str)\n\n#Lemmatize each word\nfor word in input_str:\n    print(lemmatizer.lemmatize(word))","a5eb12c2":"lemmatizer=WordNetLemmatizer()\nfor index,row in train.iterrows():\n    filter_sentence = ''\n    \n    sentence = row['total']\n    sentence = re.sub(r'[^\\w\\s]','',sentence) #cleaning\n    \n    words = nltk.word_tokenize(sentence) #tokenization\n    \n    words = [w for w in words if not w in stop_words]  #stopwords removal\n    \n    for word in words:\n        filter_sentence = filter_sentence + ' ' + str(lemmatizer.lemmatize(word)).lower()\n        \n    train.loc[index,'total'] = filter_sentence\n","9e6f1c41":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","81ee8dd2":"X_train = train['total']\nY_train = train['Label']","7288a138":"corpus = [\n    'This is the first document.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","69618286":"print(X.toarray())","95968ecd":"class cross_validation(object):\n    '''This class provides cross validation of any data set why incrementally increasing number \n       of samples in the training and test set and performing KFold splits at every iteration. \n       During cross validation the metrics accuracy, recall, precision, and f1-score are recored. \n       The results of the cross validation are display on four learning curves. '''\n    \n    def __init__(self, model, X_data, Y_data, X_test=None, Y_test=None, \n                 n_splits=3, init_chunk_size = 1000000, chunk_spacings = 100000, average = \"binary\"):\n\n        self.X, self.Y =  shuffle(X_data, Y_data, random_state=1234)\n        \n        \n        self.model = model\n        self.n_splits = n_splits\n        self.chunk_size = init_chunk_size\n        self.chunk_spacings = chunk_spacings        \n        \n        self.X_train = []\n        self.X_test = []\n        self.Y_train = []\n        self.Y_test = []\n        self.X_holdout = []\n        self.Y_holdout = []\n        \n        self.f1_train = []\n        self.f1_test = []\n        self.acc_train = []\n        self.acc_test = []\n        self.pre_train = []\n        self.pre_test = []\n        self.rec_train = []\n        self.rec_test = []\n        \n        self.f1_mean_train = []\n        self.f1_mean_test = []\n        self.acc_mean_train = []\n        self.acc_mean_test = []\n        self.pre_mean_train = []\n        self.pre_mean_test = []\n        self.rec_mean_train = []\n        self.rec_mean_test = []\n        \n        self.training_size = []\n        self.averageType = average\n    \n    def make_chunks(self):\n        '''Partitions data into chunks for incremental cross validation'''\n        \n        # get total number of points\n        self.N_total = self.X.shape[0]\n        # partition data into chunks for learning\n        self.chunks = list(np.arange(self.chunk_size, self.N_total, self.chunk_spacings ))\n        self.remainder = self.X.shape[0] - self.chunks[-1]\n        self.chunks.append( self.chunks[-1] + self.remainder )\n\n\n\n    def train_for_learning_curve(self):\n        '''KFold cross validates model and records metric scores for learning curves. \n           Metrics scored are f1-score, precision, recall, and accuracy'''\n\n        # partiton data into chunks \n        self.make_chunks()\n        # for each iteration, allow the model to use 10 more samples in the training set \n        self.skf = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=1234)\n        # iterate through the first n samples\n        for n_points in self.chunks: \n            \n        \n            # split the first n samples in k folds \n            for train_index, test_index in self.skf.split(self.X[:n_points], self.Y[:n_points]):\n                self.train_index, self.test_index = train_index, test_index                \n                self.X_train = self.X[self.train_index]\n                self.X_test = self.X[self.test_index]\n                self.Y_train = self.Y[self.train_index]\n                self.Y_test = self.Y[self.test_index]\n                \n                self.model.fit(self.X_train, self.Y_train)\n                self.y_pred_train = self.model.predict(self.X_train)\n                self.y_pred_test = self.model.predict(self.X_test)\n                self.log_metric_scores_()   \n                \n            self.log_metric_score_means_()\n            self.training_size.append(n_points)\n        \n    def validate_for_holdout_set(self, X_holdout, Y_holdout):\n        \n        \n        self.X_test = X_holdout\n        self.Y_test = Y_holdout\n        \n        # partiton data into chunks \n        self.make_chunks()\n        \n        for n_points in self.chunks:\n            \n            self.X_train = self.X[:n_points]\n            self.Y_train = self.Y[:n_points]\n\n            self.model.fit(self.X_train, self.Y_train)\n            self.y_pred_train = self.model.predict(self.X_train)\n            self.y_pred_test = self.model.predict(self.X_test)\n            self.log_metric_scores_()   \n\n            self.log_metric_score_means_()\n            self.training_size.append(n_points)\n            \n            \n    \n                            \n    def log_metric_score_means_(self):\n        '''Recrods the mean of the four metrics recording during training'''\n        self.f1_mean_train.append(np.sum(self.f1_train)\/len(self.f1_train))\n        self.f1_mean_test.append(np.sum(self.f1_test)\/len(self.f1_test))\n        \n        self.acc_mean_train.append(np.sum(self.acc_train)\/len(self.acc_train))\n        self.acc_mean_test.append(np.sum(self.acc_test)\/len(self.acc_test))\n        \n        self.pre_mean_train.append(np.sum(self.pre_train)\/len(self.pre_train))\n        self.pre_mean_test.append(np.sum(self.pre_test)\/len(self.pre_test))\n        \n        self.rec_mean_train.append(np.sum(self.rec_train)\/len(self.rec_train))\n        self.rec_mean_test.append(np.sum(self.rec_test)\/len(self.rec_test))\n        \n        self.reinitialize_metric_lists_()\n            \n            \n    def reinitialize_metric_lists_(self):\n        '''Reinitializes metrics lists for training'''\n        self.f1_train = []\n        self.f1_test = []\n        self.acc_train = []\n        self.acc_test = []\n        self.pre_train = []\n        self.pre_test = []\n        self.rec_train = []\n        self.rec_test = []\n\n            \n    def log_metric_scores_(self):\n        '''Records the metric scores during each training iteration'''\n        self.f1_train.append(f1_score(self.Y_train, self.y_pred_train, average=self.averageType))\n        self.acc_train.append(accuracy_score( self.Y_train, self.y_pred_train) )\n\n        self.pre_train.append(precision_score(self.Y_train, self.y_pred_train, average=self.averageType))\n        self.rec_train.append(recall_score( self.Y_train, self.y_pred_train, average=self.averageType) )\n\n        self.f1_test.append(f1_score(self.Y_test, self.y_pred_test, average=self.averageType))\n        self.acc_test.append(accuracy_score(self.Y_test, self.y_pred_test))\n\n        self.pre_test.append(precision_score(self.Y_test, self.y_pred_test, average=self.averageType))\n        self.rec_test.append(recall_score(self.Y_test, self.y_pred_test,average=self.averageType))\n            \n\n    def plot_learning_curve(self):\n        '''Plots f1 and accuracy learning curves for a given model and data set'''\n        \n        fig = plt.figure(figsize = (17,12))\n        # plot f1 score learning curve\n        fig.add_subplot(221)   # left\n        plt.title(\"F1-Score vs. Number of Training Samples\")\n        plt.plot(self.training_size, self.f1_mean_train, label=\"Train\")\n        plt.plot(self.training_size, self.f1_mean_test, label=\"Test\");\n        plt.xlabel(\"Number of Training Samples\")\n        plt.ylabel(\"F1-Score\")\n        plt.legend(loc=4);\n        \n        # plot accuracy learning curve\n        fig.add_subplot(222)   # right \n        plt.title(\"Accuracy vs. Number of Training Samples\")\n        plt.plot(self.training_size, self.acc_mean_train, label=\"Train\")\n        plt.plot(self.training_size, self.acc_mean_test, label=\"Test\");\n        plt.xlabel(\"Number of Training Samples\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend(loc=4);\n        \n        # plot precision learning curve\n        fig.add_subplot(223)   # left\n        plt.title(\"Precision Score vs. Number of Training Samples\")\n        plt.plot(self.training_size, self.pre_mean_train, label=\"Train\")\n        plt.plot(self.training_size, self.pre_mean_test, label=\"Test\");\n        plt.xlabel(\"Number of Training Samples\")\n        plt.ylabel(\"Precision\")\n        plt.ylim(min(self.pre_mean_test), max(self.pre_mean_train) + 0.05)\n        plt.legend(loc=4);\n        \n        # plot accuracy learning curve\n        fig.add_subplot(224)   # right \n        plt.title(\"Recall vs. Number of Training Samples\")\n        plt.plot(self.training_size, self.rec_mean_train, label=\"Train\")\n        plt.plot(self.training_size, self.rec_mean_test, label=\"Test\");\n        plt.xlabel(\"Number of Training Samples\")\n        plt.ylabel(\"Recall\")\n        plt.legend(loc=4);","7b946d91":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n# from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom time import time\nfrom collections import Counter, defaultdict\nimport nltk\nfrom pprint import pprint\nimport copy\nimport seaborn as sb\nfrom IPython.core.display import HTML\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split ,StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc ,confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import scale, MinMaxScaler, normalize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.utils import shuffle\nfrom collections import Counter\n\nimport os\nimport time\nfrom sklearn.datasets import load_files\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","4747c205":"X_total_text = train.total.values\ny = train.Label.values\nngram_range = [(1,1) ]               #,(1,2),(1,3)]\nmax_df = [0.65,0.75,0.85,0.90]\nmin_df = [0.001,0.01]\npenal = ['l1','l2']\nc_values =    [1.0]                     #[100000,10000,1000,100, 10, 1.0, 0.1, 0.01]\nfor penalt in penal:\n    for gram in ngram_range:\n        for mx_df in max_df:\n            for mn_df in min_df:\n                for c_val in c_values:\n                    print (\"For the parameters of \\nmax_df=\",mx_df,\"min_df=\",mn_df,\"\\nngram_range=\",gram,\"penalty as=\",penalt,\"c value as=\",c_val)\n                    tfidf = TfidfVectorizer(stop_words='english',ngram_range=gram,max_df=mx_df,min_df=mn_df)\n                    X_total_tfidf = tfidf.fit_transform(X_total_text)\n                    X_total_train_tfidf, X_total_test_tfidf, y_total_train, y_total_test = train_test_split(X_total_tfidf,y, test_size = 0.2, random_state=1234)\n                    if (penalt == \"l1\"):\n                        lr = LogisticRegression(C = c_val, penalty=penalt, solver='liblinear')\n                    else:\n                        lr = LogisticRegression(penalty=penalt,n_jobs=3)\n                    lr.fit(X_total_train_tfidf, y_total_train)\n                    y_pred = lr.predict(X_total_test_tfidf)\n                    print (\"Logistig Regression F1 and Accuracy Scores : \\n\")\n                    print ( \"F1 score {:.4}%\".format( f1_score(y_total_test, y_pred, average='macro')*100 ) )\n                    print ( \"Accuracy score {:.4}%\\n\\n\".format(accuracy_score(y_total_test, y_pred)*100) )","3877f577":"X_total_text = train.total.values\ny = train.Label.values\ntfidf = TfidfVectorizer(stop_words='english',ngram_range=(1,3),max_df= 0.65, min_df= 0.001)\nX_total_tfidf = tfidf.fit_transform (X_total_text)","f5c9afe1":"\nX_total_tfidf_train, X_total_tfidf_test, y_total_train, y_total_test = train_test_split(X_total_tfidf,y, test_size = 0.2, random_state=1234)\n","d245a5d6":"#Using total article --\nlr_total = LogisticRegression(C=1, penalty='l1', solver='liblinear')","043014b7":"# train model\nlr_total.fit(X_total_tfidf_train, y_total_train)","7c64690f":"# get predictions for article section\ny_total_pred = lr_total.predict(X_total_tfidf_test)","0be20c0c":"# print metrics \nprint(\"Logistig Regression F1 and Accuracy Scores : \\n\")\nprint ( \"F1 score {:.4}%\".format( f1_score(y_total_test, y_total_pred, average='macro')*100 ) )\nprint ( \"Accuracy score {:.4}%\".format(accuracy_score(y_total_test, y_total_pred)*100) )","f77f4a39":"xtrain,xtest,ytrain,ytest = train_test_split(X_total_tfidf,y)\n\ncv = cross_validation(lr_total, xtrain, ytrain , n_splits=5,init_chunk_size = 1000, chunk_spacings = 100, average = \"binary\")\ncv.validate_for_holdout_set(xtest, ytest)\ncv.plot_learning_curve()","c2ceb1a9":"xgb_total = XGBClassifier()","7a40127b":"xgb_total.fit(X_total_tfidf_train, y_total_train)\ny_xgb_total_pred = xgb_total.predict(X_total_tfidf_test)","a35ae742":"X_total_tfidf_test","70ade154":"# print metrics\nprint (\"XGBoost F1 and Accuracy Scores : \\n\")\nprint ( \"F1 score {:.4}%\".format( f1_score(y_total_test, y_xgb_total_pred, average='macro')*100 ) )\nprint ( \"Accuracy score {:.4}%\".format(accuracy_score(y_total_test, y_xgb_total_pred)*100) )","5d2c0fc6":"xtrain,xtest,ytrain,ytest = train_test_split(X_total_tfidf,y)\n\ncv = cross_validation(xgb_total, xtrain, ytrain , n_splits=5,init_chunk_size = 1000, chunk_spacings = 100, average = \"binary\")\ncv.validate_for_holdout_set(xtest, ytest)\n","5131a983":"cv.plot_learning_curve()","148c4c6f":"xgb_total","4523a46b":"## Grid Search on xg boost","2eebe5dc":"# import warnings\n# warnings.filterwarnings('ignore')\n# import numpy as np\n# import pandas as pd\n# from datetime import datetime\n# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import StratifiedKFold\n# from xgboost import XGBClassifier\n\n\n# '''params = {\n    \n#         'max_depth': [2,3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n#         'min_child_weight': [0.0001, 0.5, 0.001,1,2,3,4,5,6,7,8,9,10],\n#         'gamma': [0.0,40.0,0.005,0.5, 1, 1.5, 2, 5],\n#         'learning_rate': [0.0005,0.01,0.1,0.3],\n#         'subsample': [i\/10.0 for i in range(0,10)],\n#         'colsample_bylevel': np.round(np.arange(0.1,1.0,0.01)),\n#         'colsample_bytree': [i\/10.0 for i in range(0,10)],\n#                                       }'''\n# params = {\n#         'eta':[0.02,0.1,0.3,0.8],\n#         'reg_alpha':[1e-5, 1e-2,0, 0.1, 1, 100],\n#         'max_depth': [ 4, 5, 6, 7, 8, 9],\n#         'min_child_weight': [1,3,6,9],\n#         'gamma': [0.0,0.005,0.5, 1, 1.5, 2],\n#         'subsample': [0.3,0.6,1],\n#         'colsample_bylevel': [0.3,0.5, 0.7],\n#         'colsample_bytree': [0.3,0.5,0.7],\n#                                       }\n\n# def timer(start_time=None):\n#     if not start_time:\n#         start_time = datetime.now()\n#         return start_time\n#     elif start_time:\n#         thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n#         tmin, tsec = divmod(temp_sec, 60)\n#         print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","64101dbb":"# xgb = XGBClassifier( n_estimators=600, objective='binary:logistic',\n#                     silent=True, nthread=3, scale_pos_weight=1)","0ed284da":"# folds = 5\n# param_comb = 15\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='accuracy', n_jobs=4, cv=skf.split(X_total_tfidf_train, y_total_train), verbose=3, random_state=1001 )\n\n# # Here we go\n# start_time = timer(None) # timing starts from this point for \"start_time\" variable\n# random_search.fit(X_total_tfidf_train, y_total_train)\n# timer(start_time) # timing ends here for \"start_time\" variable","c483597f":"# print('\\n All results:')\n# print(random_search.cv_results_)\n# print('\\n Best estimator:')\n# print(random_search.best_estimator_)\n# print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n# print(random_search.best_score_ * 2 - 1)\n# print('\\n Best hyperparameters:')\n# print(random_search.best_params_)\n# results = pd.DataFrame(random_search.cv_results_)\n# results.to_csv('xgb-random-grid-search-results-01.csv', index=False)","9d9a6bed":"# y_test = random_search.predict_proba(X_total_tfidf_test)\n# results_df = pd.DataFrame(data={'target':y_test[:,1]})\n# results_df.to_csv('submission-random-grid-search-xgb-porto-01.csv', index=False)","8618161a":"# y_xgb_total_pred_random_search = random_search.predict(X_total_tfidf_test)","64a90dda":"# # print metrics\n# print (\"XGBoost F1 and Accuracy Scores : \\n\")\n# print ( \"F1 score {:.4}%\".format( f1_score(y_total_test, y_xgb_total_pred_random_search, average='macro')*100 ) )\n# print ( \"Accuracy score {:.4}%\".format(accuracy_score(y_total_test, y_xgb_total_pred_random_search)*100) )","dc6bd315":"from sklearn.pipeline import Pipeline\nimport joblib\nfrom sklearn import linear_model","744cf950":"pipelinexgbtuned = Pipeline([\n    ('tfidf',TfidfVectorizer(stop_words='english',ngram_range=(1,3),max_df= 0.65, min_df= 0.001)),\n    ('xgbt', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.7,\n              colsample_bynode=1, colsample_bytree=0.3, eta=0.1, gamma=0.0,\n              gpu_id=-1, importance_type='gain', interaction_constraints='',\n              learning_rate=0.100000001, max_delta_step=0, max_depth=7,\n              min_child_weight=3, monotone_constraints='()',\n              n_estimators=600, n_jobs=3, nthread=3, num_parallel_tree=1,\n              objective='binary:logistic', random_state=0, reg_alpha=0.01,\n              reg_lambda=1, scale_pos_weight=1, silent=True, subsample=0.6,\n              tree_method='exact', validate_parameters=1, verbosity=None)),\n])","1832b2cf":"pipelinexgbtuned.fit(X_train, Y_train)","a0eb9c5d":"pipelinexgbtuned.predict([\"flynn hillary clinton big woman campus breitbart daniel j flynnever get feeling life circle roundabout rather head straight line toward intended destination hillary clinton remains big woman campus leafy liberal wellesley massachusetts everywhere else vote likely inauguration dress remainder day way miss havisham forever wore wedding dress speaking great expectations hillary rodham overflowed 48 year ago first addressed wellesley graduating class the president college informed gathered 1969 student needed debate far i could ascertain spokesman kind like democratic primary 2016 minus term unknown even seven sisters school i glad miss adams made clear i speaking today u 400 u miss rodham told classmate after appointing edger bergen charlie mccarthys mortimer snerds attendance bespectacled granny glass awarding matronly wisdom least john lennon wisdom took issue previous speaker despite becoming first win election seat u s senate since reconstruction edward brooke came criticism calling empathy goal protestors criticized tactic though clinton senior thesis saul alinsky lamented black power demagogue elitist arrogance repressive intolerance within new left similar word coming republican necessitated brief rebuttal trust rodham ironically observed 1969 one word i asked class rehearsal wanted say everyone came said talk trust talk lack trust u way feel others talk trust bust what say what say feeling permeates generation perhaps even understood distrusted the trust bust certainly busted clintons 2016 plan she certainly even understand people distrusted after whitewater travelgate vast conspiracy benghazi missing email clinton found distrusted voice friday there load compromising road broadening political horizon and distrust american people trump edged 48 percent 38 percent question immediately prior novembers election stood major reason closing horizon clinton described vanquisher supporter embracing lie con alternative fact assault truth reason she failed explain american people chose lie truth as history major among today know well people power invent fact attack question mark beginning end free society offered that hyperbole like many people emerge 1960s hillary clinton embarked upon long strange trip from high school goldwater girl wellesley college republican president democratic politician clinton drank time place gave degree more significantly went idealist cynic comparison two wellesley commencement address show way back lamented long leader viewed politics art possible challenge practice politics art making appears impossible possible now big woman campus odd woman white house wonder current station even possible why arent i 50 point ahead asked september in may asks isnt president the woman famously dubbed congenital liar bill safire concludes lie mind getting stood election day like finding jilted bride wedding day inspires dangerous delusion\"])","266b8774":"\nX_total_train, X_total_test, y_total_train, y_total_test = train_test_split(X_train,Y_train, test_size = 0.2, random_state=1234)\n\npipelinexgbtuned.fit(X_total_train, y_total_train)\ny_xgbtuned_total_pred = pipelinexgbtuned.predict(X_total_test)\n","4ad46968":"# print metrics\nprint (\"F1 and Accuracy Scores : \\n\")\nprint ( \"F1 score {:.4}%\".format( f1_score(y_total_test, y_xgbtuned_total_pred, average='macro')*100 ) )\nprint ( \"Accuracy score {:.4}%\".format(accuracy_score(y_total_test, y_xgbtuned_total_pred)*100) )","8dcf1b16":"# #saving the pipeline\n# filename = 'pipelinexgbt.sav'\n# joblib.dump(pipeline1, filename)\n# filename = '.\/pipelinexgbt.sav'","242f3413":"# loaded_model = joblib.load(filename)\n# result = loaded_model.predict([\"flynn hillary clinton big woman campus breitbart daniel j flynnever get feeling life circle roundabout rather head straight line toward intended destination hillary clinton remains big woman campus leafy liberal wellesley massachusetts everywhere else vote likely inauguration dress remainder day way miss havisham forever wore wedding dress speaking great expectations hillary rodham overflowed 48 year ago first addressed wellesley graduating class the president college informed gathered 1969 student needed debate far i could ascertain spokesman kind like democratic primary 2016 minus term unknown even seven sisters school i glad miss adams made clear i speaking today u 400 u miss rodham told classmate after appointing edger bergen charlie mccarthys mortimer snerds attendance bespectacled granny glass awarding matronly wisdom least john lennon wisdom took issue previous speaker despite becoming first win election seat u s senate since reconstruction edward brooke came criticism calling empathy goal protestors criticized tactic though clinton senior thesis saul alinsky lamented black power demagogue elitist arrogance repressive intolerance within new left similar word coming republican necessitated brief rebuttal trust rodham ironically observed 1969 one word i asked class rehearsal wanted say everyone came said talk trust talk lack trust u way feel others talk trust bust what say what say feeling permeates generation perhaps even understood distrusted the trust bust certainly busted clintons 2016 plan she certainly even understand people distrusted after whitewater travelgate vast conspiracy benghazi missing email clinton found distrusted voice friday there load compromising road broadening political horizon and distrust american people trump edged 48 percent 38 percent question immediately prior novembers election stood major reason closing horizon clinton described vanquisher supporter embracing lie con alternative fact assault truth reason she failed explain american people chose lie truth as history major among today know well people power invent fact attack question mark beginning end free society offered that hyperbole like many people emerge 1960s hillary clinton embarked upon long strange trip from high school goldwater girl wellesley college republican president democratic politician clinton drank time place gave degree more significantly went idealist cynic comparison two wellesley commencement address show way back lamented long leader viewed politics art possible challenge practice politics art making appears impossible possible now big woman campus odd woman white house wonder current station even possible why arent i 50 point ahead asked september in may asks isnt president the woman famously dubbed congenital liar bill safire concludes lie mind getting stood election day like finding jilted bride wedding day inspires dangerous delusion\"])\nresult = pipelinexgbtuned.predict([\"flynn hillary clinton big woman campus breitbart daniel j flynnever get feeling life circle roundabout rather head straight line toward intended destination hillary clinton remains big woman campus leafy liberal wellesley massachusetts everywhere else vote likely inauguration dress remainder day way miss havisham forever wore wedding dress speaking great expectations hillary rodham overflowed 48 year ago first addressed wellesley graduating class the president college informed gathered 1969 student needed debate far i could ascertain spokesman kind like democratic primary 2016 minus term unknown even seven sisters school i glad miss adams made clear i speaking today u 400 u miss rodham told classmate after appointing edger bergen charlie mccarthys mortimer snerds attendance bespectacled granny glass awarding matronly wisdom least john lennon wisdom took issue previous speaker despite becoming first win election seat u s senate since reconstruction edward brooke came criticism calling empathy goal protestors criticized tactic though clinton senior thesis saul alinsky lamented black power demagogue elitist arrogance repressive intolerance within new left similar word coming republican necessitated brief rebuttal trust rodham ironically observed 1969 one word i asked class rehearsal wanted say everyone came said talk trust talk lack trust u way feel others talk trust bust what say what say feeling permeates generation perhaps even understood distrusted the trust bust certainly busted clintons 2016 plan she certainly even understand people distrusted after whitewater travelgate vast conspiracy benghazi missing email clinton found distrusted voice friday there load compromising road broadening political horizon and distrust american people trump edged 48 percent 38 percent question immediately prior novembers election stood major reason closing horizon clinton described vanquisher supporter embracing lie con alternative fact assault truth reason she failed explain american people chose lie truth as history major among today know well people power invent fact attack question mark beginning end free society offered that hyperbole like many people emerge 1960s hillary clinton embarked upon long strange trip from high school goldwater girl wellesley college republican president democratic politician clinton drank time place gave degree more significantly went idealist cynic comparison two wellesley commencement address show way back lamented long leader viewed politics art possible challenge practice politics art making appears impossible possible now big woman campus odd woman white house wonder current station even possible why arent i 50 point ahead asked september in may asks isnt president the woman famously dubbed congenital liar bill safire concludes lie mind getting stood election day like finding jilted bride wedding day inspires dangerous delusion\"])\n\nprint(result) ","828bac5d":"abc = pipelinexgbtuned.predict_proba([\"flynn hillary clinton big woman campus breitbart daniel j flynnever get feeling life circle roundabout rather head straight line toward intended destination hillary clinton remains big woman campus leafy liberal wellesley massachusetts everywhere else vote likely inauguration dress remainder day way miss havisham forever wore wedding dress speaking great expectations hillary rodham overflowed 48 year ago first addressed wellesley graduating class the president college informed gathered 1969 student needed debate far i could ascertain spokesman kind like democratic primary 2016 minus term unknown even seven sisters school i glad miss adams made clear i speaking today u 400 u miss rodham told classmate after appointing edger bergen charlie mccarthys mortimer snerds attendance bespectacled granny glass awarding matronly wisdom least john lennon wisdom took issue previous speaker despite becoming first win election seat u s senate since reconstruction edward brooke came criticism calling empathy goal protestors criticized tactic though clinton senior thesis saul alinsky lamented black power demagogue elitist arrogance repressive intolerance within new left similar word coming republican necessitated brief rebuttal trust rodham ironically observed 1969 one word i asked class rehearsal wanted say everyone came said talk trust talk lack trust u way feel others talk trust bust what say what say feeling permeates generation perhaps even understood distrusted the trust bust certainly busted clintons 2016 plan she certainly even understand people distrusted after whitewater travelgate vast conspiracy benghazi missing email clinton found distrusted voice friday there load compromising road broadening political horizon and distrust american people trump edged 48 percent 38 percent question immediately prior novembers election stood major reason closing horizon clinton described vanquisher supporter embracing lie con alternative fact assault truth reason she failed explain american people chose lie truth as history major among today know well people power invent fact attack question mark beginning end free society offered that hyperbole like many people emerge 1960s hillary clinton embarked upon long strange trip from high school goldwater girl wellesley college republican president democratic politician clinton drank time place gave degree more significantly went idealist cynic comparison two wellesley commencement address show way back lamented long leader viewed politics art possible challenge practice politics art making appears impossible possible now big woman campus odd woman white house wonder current station even possible why arent i 50 point ahead asked september in may asks isnt president the woman famously dubbed congenital liar bill safire concludes lie mind getting stood election day like finding jilted bride wedding day inspires dangerous delusion\"])","16ef16b0":"print(round(abc[0][0] * 100, 2))","41fce5d2":"# 2. Tokenization\nTokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.","888e9678":"# 4. Lemmatization\nLemmatisation in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form","1eb2c231":"# 3. StopWords\nStop words are words which are filtered out before or after processing of natural language data (text).","7a7a61a6":"## Pipeline","2c1510c2":"# Grid Search","f74f9dd7":"# Creating Wordcloud Visuals\n\nWhat is Wordcloud:\nA word cloud is a novelty visual representation of text data, typically used to depict keyword metadata on websites, or to visualize free form text. Tags are usually single words, and the importance of each tag is shown with font size or color.","f907407b":"# 1. Regex","1c40ba01":"# Let's Apply","51dfec24":"# Applying NLP Techniques","39b42212":"TFIDFTransformer - With Tfidftransformer you will compute word counts using CountVectorizer and then compute the IDF values and only then compute the Tf-idf scores.\n\nCountVectorizer - It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\n\n\nTFIDFVectorizer - TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents.\n","24e1a789":"## Learning Curve and CV","b9d374b3":"# Cleaning and preprocessing ","5443c82f":"# XGBoost Classifier\n##### Using all text from the dataset","d723b364":"# Logistic Regression - Applying Grid Search enhancements","da045c4c":"# Bag-of-words \/ CountVectorizer","c688abcf":"# Prediction","55bf8d41":"Learning Curve for total logistic regression","54643281":"# Fake News Prediction "}}