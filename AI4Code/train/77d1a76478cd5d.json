{"cell_type":{"0177e91f":"code","0798c38c":"code","768a1c4b":"code","387bd154":"code","f937624f":"code","071844d1":"code","0f8dae64":"code","f834ac5a":"code","89c69254":"code","f2df0c43":"code","55539234":"code","e28d48b1":"code","d2975499":"code","8530ab9c":"code","16e94824":"code","ac4c4f6b":"code","c36f554f":"code","00452267":"code","48a24e45":"code","f2787295":"code","42f81f84":"code","41ffe6df":"code","d5e34a01":"code","8e2fb5f7":"code","463e33f7":"code","f9a2c58c":"markdown","a6d54b7a":"markdown","db351210":"markdown","2c5d8c06":"markdown","a774a218":"markdown","6e0f0151":"markdown","cf77d536":"markdown","ecff5c86":"markdown","80a558df":"markdown","9ff8f2f2":"markdown","5f3994fe":"markdown","6925f1ca":"markdown","a715c745":"markdown","b1bd4ad7":"markdown","9ab28265":"markdown","c048c5c4":"markdown","f993a6ee":"markdown","a604a533":"markdown","7a18dd6d":"markdown","ed83ad89":"markdown","1fbfa192":"markdown","d2e8ffd1":"markdown","698cfa9e":"markdown","9af61bc7":"markdown","b536eaea":"markdown","b030c149":"markdown","8d0dff7b":"markdown","efe3e24c":"markdown","a32ccd7c":"markdown","aa1794a4":"markdown","9b93decf":"markdown","0f6b32f6":"markdown"},"source":{"0177e91f":"import pandas as pd\npd.set_option('display.max_colwidth', -1)\npath = '..\/input\/'\nsar_acc = pd.read_json(path+ 'Sarcasm_Headlines_Dataset.json',lines=True)\nimport re\nsar_acc['source'] = sar_acc['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\nsar_acc.head()","0798c38c":"import numpy as np\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nsar_acc_tar = sar_acc['is_sarcastic'].value_counts()\nlabels = ['Acclaim', 'Sarcastic']\nsizes = (np.array((sar_acc_tar \/ sar_acc_tar.sum())*100))\ncolors = ['#58D68D', '#9B59B6']\n\ntrace = go.Pie(labels=labels, values=sizes, opacity = 0.8, hoverinfo='label+percent',\n               marker=dict(colors=colors, line=dict(color='#FFFFFF', width=2)))\nlayout = go.Layout(\n    title='Sarcastic Vs Acclaim'\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"Sa_Ac\")","768a1c4b":"all_words = sar_acc['headline'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Viridis',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Frequent Occuring word (unclean) in Headlines'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\niplot(fig, filename='basic-bar')","387bd154":"sar_det = sar_acc[sar_acc.is_sarcastic==1]\nsar_det.reset_index(drop=True, inplace=True)\nacc_det = sar_acc[sar_acc.is_sarcastic==0]\nacc_det.reset_index(drop=True, inplace=True)\n\n# Tokenizing the Headlines of Sarcasm\nsar_news = []\nfor rows in range(0, sar_det.shape[0]):\n    head_txt = sar_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    sar_news.append(head_txt)\n\n#Converting into single list for Sarcasm\nimport itertools\nsar_list = list(itertools.chain(*sar_news))\n\n# Tokenizing the Headlines of Acclaim\nacc_news = []\nfor rows in range(0, acc_det.shape[0]):\n    head_txt = acc_det.headline[rows]\n    head_txt = head_txt.split(\" \")\n    acc_news.append(head_txt)\n    \n#Converting into single list for Acclaim\nacc_list = list(itertools.chain(*acc_news))","f937624f":"import nltk\n\nstopwords = nltk.corpus.stopwords.words('english')\nsar_list_restp = [word for word in sar_list if word.lower() not in stopwords]\nacc_list_restp = [word for word in acc_list if word.lower() not in stopwords]\n\nprint(\"Length of original Sarcasm list: {0} words\\n\"\n      \"Length of Sarcasm list after stopwords removal: {1} words\"\n      .format(len(sar_list), len(sar_list_restp)))\n\nprint(\"==\"*46)\n\nprint(\"Length of original Acclaim list: {0} words\\n\"\n      \"Length of Acclaim list after stopwords removal: {1} words\"\n      .format(len(acc_list), len(acc_list_restp)))","071844d1":"#Data cleaning for getting top 30\nfrom collections import Counter\nsar_cnt = Counter(sar_list_restp)\nacc_cnt = Counter(acc_list_restp)\n\n#Dictonary to Dataframe\nsar_cnt_df = pd.DataFrame(list(sar_cnt.items()), columns = ['Words', 'Freq'])\nsar_cnt_df = sar_cnt_df.sort_values(by=['Freq'], ascending=False)\nacc_cnt_df = pd.DataFrame(list(acc_cnt.items()), columns = ['Words', 'Freq'])\nacc_cnt_df = acc_cnt_df.sort_values(by=['Freq'], ascending=False)\n\n#Top 30\nsar_cnt_df_30 = sar_cnt_df.head(30)\nacc_cnt_df_30 = acc_cnt_df.head(30)","0f8dae64":"#Plotting the top 30 Sarcasm Vs Acclaim\nfrom plotly import tools\nsar_tr  = go.Bar(\n    x=sar_cnt_df_30['Freq'],\n    y=sar_cnt_df_30['Words'],\n    name='Sarcasm',\n    marker=dict(\n        color='rgba(155, 89, 182, 0.6)',\n        line=dict(\n            color='rgba(155, 89, 182, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nacc_tr  = go.Bar(\n    x=acc_cnt_df_30['Freq'],\n    y=acc_cnt_df_30['Words'],\n    name='Acclaim',\n    marker=dict(\n        color='rgba(88, 214, 141, 0.6)',\n        line=dict(\n            color='rgba(88, 214, 141, 1.0)',\n            width=.3,\n        )\n    ),\n    orientation='h',\n    opacity=0.6\n)\n\nfig = tools.make_subplots(rows=2, cols=1, subplot_titles=('Top 30 Most occuring words in Sarcasm Headlines',\n                                                          'Top 30 Most occuring words in Acclaim Headlines'))\n\nfig.append_trace(sar_tr, 1, 1)\nfig.append_trace(acc_tr, 2, 1)\n\n\nfig['layout'].update(height=1200, width=800)\n\niplot(fig, filename='sar_vs_acc')","f834ac5a":"stemmer = nltk.stem.SnowballStemmer(\"english\", ignore_stopwords=True)\n\nprint(\"The stemmed form of learning is: {}\".format(stemmer.stem(\"learning\")))\nprint(\"The stemmed form of learns is: {}\".format(stemmer.stem(\"learns\")))\nprint(\"The stemmed form of learn is: {}\".format(stemmer.stem(\"learn\")))\nprint(\"==\"*46)\nprint(\"The stemmed form of leaves is: {}\".format(stemmer.stem(\"leaves\")))\nprint(\"==\"*46)","89c69254":"from nltk.stem import WordNetLemmatizer\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","f2df0c43":"#Sarcasm headline after Lemmatization\nsar_wost_lem = []\nfor batch in sar_news:\n    sar_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    sar_list_lemm =  [lemm.lemmatize(word) for word in sar_list_restp]\n    sar_wost_lem.append(sar_list_lemm)\n\n#Acclaim headline after Lemmatization\nacc_wost_lem = []\nfor batch in acc_news:\n    acc_list_restp = [word for word in batch if word.lower() not in stopwords]\n    lemm = WordNetLemmatizer()\n    acc_list_lemm =  [lemm.lemmatize(word) for word in acc_list_restp]\n    acc_wost_lem.append(sar_list_lemm)","55539234":"from sklearn.feature_extraction.text import CountVectorizer\nvec = []\nfor block in sar_wost_lem:\n    vectorizer = CountVectorizer(min_df=0)\n    sentence_transform = vectorizer.fit_transform(block)\n    vec.append(sentence_transform)\n    \nprint(\"The features are:\\n {}\".format(vectorizer.get_feature_names()))\nprint(\"\\nThe vectorized array looks like:\\n {}\".format(sentence_transform.toarray()))","e28d48b1":"# Converting all sarcasm keywords to single list after lemmatization\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nsar_list_wd = list(itertools.chain(*sar_wost_lem))\nfrom wordcloud import WordCloud\nsar_cloud = WordCloud(background_color='black', width=20000,height=10000).\\\n                generate(\" \".join(sar_list_wd))\nplt.imshow(sar_cloud)\nplt.axis('off')\nplt.show()","d2975499":"acc_list_wd = list(itertools.chain(*acc_wost_lem))\nacc_cloud = WordCloud(background_color='black', width=20000,height=10000).\\\n                generate(\" \".join(acc_list_wd))\nplt.imshow(acc_cloud)\nplt.axis('off')\nplt.show()","8530ab9c":"sar_wost_lem_df = pd.DataFrame({'sarcasm':sar_wost_lem})\nacc_wost_lem_df = pd.DataFrame({'acclaim':acc_wost_lem})\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    ngrams = zip(*[text[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n#Plotting the Bigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of Sarcasm Headlines\", \n                                          \"Frequent bigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","16e94824":"#Plotting the Trigram plot\nfrom collections import defaultdict\nfreq_dict = defaultdict(int)\nfor sent in sar_wost_lem_df[\"sarcasm\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nsar_2 = horizontal_bar_chart(fd_sorted.head(50), '#9B59B6')\n\n\nfreq_dict = defaultdict(int)\nfor sent in acc_wost_lem_df[\"acclaim\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\nacc_2 = horizontal_bar_chart(fd_sorted.head(50), '#58D68D')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent Trigrams of Sarcasm Headlines\", \n                                          \"Frequent Trigrams of Acclaim Headlines\"])\nfig.append_trace(sar_2, 1, 1)\nfig.append_trace(acc_2, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Plots Sarcasm Vs Acclaim after removing Stopwords\")\niplot(fig, filename='word-plots')","ac4c4f6b":"from wordcloud import STOPWORDS\nimport string\n## Number of words in the text ##\nsar_acc[\"num_words\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\nsar_acc[\"num_unique_words\"] = sar_acc[\"headline\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\nsar_acc[\"num_chars\"] = sar_acc[\"headline\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\nsar_acc[\"num_stopwords\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\nsar_acc[\"num_punctuations\"] =sar_acc['headline'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_upper\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\nsar_acc[\"num_words_title\"] = sar_acc[\"headline\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\nsar_acc[\"mean_word_len\"] = sar_acc[\"headline\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","c36f554f":"## Truncate some extreme values for better visuals ##\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nsar_acc['num_words'].loc[sar_acc['num_words']>60] = 60 #truncation for better visuals\nsar_acc['num_punctuations'].loc[sar_acc['num_punctuations']>10] = 10 #truncation for better visuals\nsar_acc['num_chars'].loc[sar_acc['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='is_sarcastic', y='num_words', data=sar_acc, ax=axes[0])\naxes[0].set_xlabel('is_sarcastic', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_chars', data=sar_acc, ax=axes[1])\naxes[1].set_xlabel('is_sarcastic', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='is_sarcastic', y='num_punctuations', data=sar_acc, ax=axes[2])\naxes[2].set_xlabel('is_sarcastic', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()\n","00452267":"#Getting X and Y ready\nfrom sklearn.preprocessing import LabelEncoder\nX = sar_acc.headline\nY = sar_acc.is_sarcastic\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","48a24e45":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)","f2787295":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nmax_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","42f81f84":"from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.2)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","41ffe6df":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","d5e34a01":"from keras.callbacks import EarlyStopping\nmodel.fit(sequences_matrix,Y_train,batch_size=100,epochs=5,\n          validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","8e2fb5f7":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","463e33f7":"accr = model.evaluate(test_sequences_matrix,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","f9a2c58c":"**Play with Tri-grams** \n\n![](https:\/\/media.giphy.com\/media\/2XskdWAtr8Dz0OFRVAY\/giphy.gif)\n","a6d54b7a":"**4. Defining the RNN structure for the model**","db351210":"**Play with Bi-grams**  \n\n![](https:\/\/media.giphy.com\/media\/xTiN0h0Kh5gH7yQYUw\/source.gif)\n\nA bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.\n\n**Source:**  \nhttps:\/\/en.wikipedia.org\/wiki\/Bigram","2c5d8c06":"**6. Fitting the model on training data**","a774a218":"**WordCloud of Sarcasm Headlines after Lemmatisation**  \n\n![](https:\/\/media.giphy.com\/media\/lXPmuMllourZml0Ws\/200w_d.gif)","6e0f0151":"**Top 30 Occuring words after removing Stopwords from Headlines - Sarcasm Vs Acclaim**","cf77d536":"***Sarcasm in Play***  \nSarcasm is \"a sharp, bitter, or cutting expression or remark; a bitter gibe or taunt\". Sarcasm may employ ambivalence, although sarcasm is not necessarily ironic. Most noticeable in spoken word, sarcasm is mainly distinguished by the inflection with which it is spoken and is largely context-dependent.  \n\n**Source:**  \nhttps:\/\/en.wikipedia.org\/wiki\/Sarcasm  \n\n**Objective**  \nThe objective of the notebook is to do Exploratory Data Analysis (EDA) and make prediction based on headlines as whether the headline is Sarcastic or not. ","ecff5c86":"**Loading and Viewing the Sample Dataset**","80a558df":"**Lemmatisation**  \n\n![](https:\/\/media.giphy.com\/media\/Tl25hw4bD37AQ\/200w_d.gif)\n\nLemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. Unlike a stemmer, lemmatizing the dataset aims to reduce words based on an actual dictionary or vocabulary (the Lemma) and therefore will not chop off words into stemmed forms that do not carry any lexical meaning. \n\n**Source:**  \nhttps:\/\/en.wikipedia.org\/wiki\/Lemmatisation","9ff8f2f2":"![](https:\/\/media.giphy.com\/media\/QMkPpxPDYY0fu\/giphy.gif)","5f3994fe":"**Distribution of Sarcasm Vs Acclaim Headlines**  \n\n![](https:\/\/media.giphy.com\/media\/cg5QIQdI50Jy\/source.gif)","6925f1ca":"**Frequent Occuring word (unclean) in Headlines**","a715c745":"Now let us see how these meta features are distributed between both Sarcasm and Acclaim headlines.","b1bd4ad7":"**3. Processing the data for the model**  \n* Tokenize the data and convert the text to sequences.\n* Add padding to ensure that all the sequences have the same shape.\n* There are many ways of taking the max_len and here an arbitrary length of 150 is chosen","9ab28265":"From the above plot its clearly evident that the headlines need to be cleaned as the top 50 most occuring words are joing words and indirect words which does not provide any meaning.","c048c5c4":"**Bag of Words**  \n\n![](https:\/\/media.giphy.com\/media\/LPDH1s2GCFOk8\/200w_d.gif)\n\nThe bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n\nThe bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier\n\n**Source:**  \nhttps:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model","f993a6ee":"**Example of SnowballStemmer**","a604a533":"**Stemming**  \n\n![](https:\/\/media.giphy.com\/media\/3otPoSaQ8DMQeIJZ60\/200w_d.gif)\n\nIn linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form - generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.  \n\n**Source:**  \nhttps:\/\/en.wikipedia.org\/wiki\/Stemming  \n\nNLTK provides three different forms of steamming namely Porter stemming algorithm, the lancaster stemmer and the Snowball stemmer. Here, for our analysis we will be using Snowball stemmer.\n","7a18dd6d":"**5. Compiling the model**","ed83ad89":"**Time to Play with Keras**  \n**1. Data preparation for Model building**  \nFor any model, first the data needs to be prepared in a suitable format. Here, for our keras model we need to create the input and output vectors and then preprocess the lables.","1fbfa192":"**7. Time to test the model**","d2e8ffd1":"**Getting Hands on NLP**  \nNow, We will get our hands dirty into the data using various NLP techniques.  \n\n**Tokenization**  \n\n![](https:\/\/media.giphy.com\/media\/26ueZPuike2EXBhba\/200w_d.gif)\n\nTokenization is the process of breaking up the given text into units called tokens. The tokens may be words or number or punctuation mark. Tokenization does this task by locating word boundaries. Ending point of a word and beginning of the next word is called word boundaries. Tokenization is also known as word segmentation.","698cfa9e":"**Meta Features**  \n\nThanks to Sudalairajkumar for the awesome kernel which taught me about the meta features.\n\nLink for the kernel:\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc","9af61bc7":"In the above output, I have showed as how the bag of words looks like which is achieved through vectorized operation.","b536eaea":"**Summary statistics of the Dataset**","b030c149":"**Removing Stopwords**  \n\n![](https:\/\/media.giphy.com\/media\/tp93WxQmW3o5y\/200w_d.gif)\n\nA stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n\nWe would not want these words taking up space in our database, or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words.   \n\nWe will remove the stopwords provided by NLTK package from our headlines.\n\n**Source:**  \nhttps:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/","8d0dff7b":"**WordCloud of Acclaim Headlines after Lemmatisation**","efe3e24c":"Here is the caveat in using stemming. In the above example for the word 'leaves', it just stemms the word. As the name 'stemming' suggest, it at times simply stems the word which will become meaningless. So, to overcome this issue we have lemmatization.","a32ccd7c":"![](https:\/\/i.imgur.com\/AzPl9.jpg)","aa1794a4":"**2. Split into Training and Test data**","9b93decf":"**8. Model Accuracy**","0f6b32f6":"**Note:**  \nKindly upvote the kernel if you find it useful. Suggestions are always welome. Let me know your thoughts in the comment if any."}}