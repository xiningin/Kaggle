{"cell_type":{"d5edec47":"code","973c2a76":"code","1686119e":"code","750b0a2f":"code","42b88e24":"code","cf10b5f8":"code","0863ff9a":"code","006761e7":"code","e9fc9fc6":"code","76eee1d8":"code","efcb515f":"code","43dd5652":"code","7701e006":"code","4406b7b8":"code","ddb87937":"code","f3010be3":"code","2eb7ae7b":"code","869515c4":"code","9e85fa49":"code","ece8ec8c":"code","c753eb2e":"code","57b1dcdb":"code","bc2045d2":"code","c4a2271d":"code","d5bda02e":"code","dd4638c6":"code","0543282b":"code","a1a2df5b":"code","980701c8":"code","bf5dfd9b":"markdown","62063e91":"markdown","aabc4567":"markdown","183126bd":"markdown","aefc609b":"markdown","9b4768ba":"markdown"},"source":{"d5edec47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport string\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","973c2a76":"# Load train data\nusecols = ['id', 'review_title', 'review_text', 'product_name']\nraw_train_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', index_col='id', usecols=usecols + ['rating'])\n\n# Load test data\nraw_test_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/test.csv', index_col='id', usecols=usecols)","1686119e":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\nraw_test_data['review'] = raw_test_data['review_title'] + ' ' + raw_test_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","750b0a2f":"corpus = ' '.join([*raw_train_data['review']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","42b88e24":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","cf10b5f8":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","0863ff9a":"long_words = [word for word in vocab if len(word) > 20]\n\n# Show some long words\npprint(long_words[:10], compact=True)","006761e7":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:110], compact=True)","e9fc9fc6":"# Compile regular expressions\nremove_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nremove_email = re.compile(r'\\S+@\\S+')\nremove_duplicate_word = re.compile(r'\\b(\\w+?)\\1+')\nremove_duplicate_char = re.compile(r'([^rs])(?=\\1+)|(r)(?=r{2,})|(s)(?=s{2,})')\nremove_number = re.compile(r'\\d+')\nremove_extra_space = re.compile(r'\\s+')\n\n# Load punctuation\npunctuation = [*string.punctuation]\npunctuation.extend(['\u00ba', '\u00aa'])\n\ndef preprocess_text(text: str) -> str:\n\n    # Convert to lowercase\n    text = text.lower() \n    \n    # Apply regular expressions\n    text = remove_url.sub('', text)    # remove urls\n    text = remove_email.sub('', text)  # remove emails\n    text = remove_duplicate_word.sub(r'\\1', text) # remove duplicate words\n    text = remove_duplicate_char.sub('', text)    # remove duplicate chars; except \"rr\" and \"ss\" digraphs\n    text = remove_number.sub(' ', text)  # remove numbers\n\n    ## Expand abbreviatons\n    text = re.sub(r'\\b(n|\u00f1)([a\u00e3\u00e2]o)?\\b', ' n\u00e3o ', text)\n    text = re.sub(r'\\bt[a\u00e1]\\b', ' est\u00e1 ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1])?)\\b', ' para ', text)\n    text = re.sub(r'\\bq\\b', ' que ', text)\n    text = re.sub(r'\\bpq[s]?\\b', ' porque ', text)\n    text = re.sub(r'\\btb[m|n]?\\b', ' tamb\u00e9m ', text)\n    text = re.sub(r'\\vc[s]?\\b', ' voc\u00ea ', text)\n    text = re.sub(r'\\bmt[ao]?s?\\b', ' muito ', text)\n    text = re.sub(r'\\b(p(r[oa\u00e1]s?)?)\\b', ' para ', text)\n    text = re.sub(r'\\bhj\\b', ' hoje ', text)\n    text = re.sub(r'\\bobs\\b', ' observa\u00e7\u00e3o ', text)\n    text = re.sub(r'\\beh\\b', ' \u00e9 ', text)\n\n    # Preprocess long words\n    text = re.sub(r'(\u00f3timo)[v]?\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(ok)\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(natural)', r' \\1 ', text)\n    text = re.sub(r'(((bla)(ba)?)+\\2)', r' \\2 ', text)\n    text = re.sub(r'(altura|largura)x', r' \\1 ', text)\n    text = re.sub(r'(compra|embalagem)x', r' \\1 ', text) \n    text = re.sub(r'(ruim|regular|bom|[o\u00f3]timo|excelente)', r' \\1 ', text)\n\n    # Remove trailing spaces\n    text = remove_extra_space.sub(' ', text)\n\n    # Instatiate a TweetTokenizer object\n    tokenizer = TweetTokenizer(preserve_case=False, # lowercasing\n                               reduce_len=True, \n                               strip_handles=True)  # remove mentions\n                               \n    # Tokenize the text    \n    tokens = tokenizer.tokenize(text)\n\n    # Remove punctuation\n    tokens = [token for token in tokens if token not in punctuation]\n    \n    # Remove non-alphabetic and longer than twenty chars words\n    tokens = [token for token in tokens if token.isalpha() and len(token) <= 20]\n\n    return ' '.join(tokens)","76eee1d8":"%%time\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocess_text)\nraw_test_data['review_clean'] = raw_test_data['review'].apply(preprocess_text)","efcb515f":"corpus = ' '.join([*raw_train_data['review_clean']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","43dd5652":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","7701e006":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","4406b7b8":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:105], compact=True)","ddb87937":"%%time\n\nX = raw_train_data['review_clean']\ny = raw_train_data['rating']\n\nvectorizer = TfidfVectorizer(strip_accents='unicode', # remove accents                              \n                             token_pattern=r'\\w{2,}',\n                             ngram_range=(1, 3),\n                             min_df=3,\n                             use_idf=1, \n                             smooth_idf=1, \n                             sublinear_tf=1)\n\nmodels = [\n    ('Naive Bayes', MultinomialNB()),\n    ('Logistic Regression', LogisticRegression(class_weight='balanced', random_state=0, n_jobs=-1)),\n    ('SVM', LinearSVC(class_weight='balanced', random_state=0)),\n    ('KNN', KNeighborsClassifier(n_jobs=-1)),\n    ('Decision Tree', DecisionTreeClassifier(random_state=0, class_weight='balanced')),    \n    ('Random Forest', RandomForestClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('Extra Trees', ExtraTreesClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('XGBoost', XGBClassifier(random_state=0, n_jobs=-1)), \n    ('LightGBM', LGBMClassifier(objective='multiclass',class_weight='balanced', random_state=0, n_jobs=-1))\n]\n\n# Perform cross-validation\nfor name, model in tqdm(models, leave=None):\n\n    pipeline = Pipeline([('vectorizer', vectorizer),\n                         (name, model)])\n    \n    scores = cross_val_score(pipeline, X, y, scoring='accuracy')\n    \n    acc = np.mean(scores)\n    std = np.std(scores)\n    \n    print(f'Accuracy {acc:.2%} +\/- {std:.2%} = {(acc - std):.2%} -- {name}.')","f3010be3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","2eb7ae7b":"vectorizer = TfidfVectorizer(strip_accents='unicode', # remove accents                              \n                             token_pattern=r'\\w{2,}',\n                             ngram_range=(1, 3),\n                             min_df=3,\n                             use_idf=1, \n                             smooth_idf=1, \n                             sublinear_tf=1)\n\nvectorizer.fit(list(raw_train_data['review_clean']) + list(raw_test_data['review_clean']))","869515c4":"X = vectorizer.transform(raw_train_data['review_clean'])\nX_test = vectorizer.transform(raw_test_data['review_clean'])\n\ny = raw_train_data['rating']\n\nX_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2)","9e85fa49":"lr = LogisticRegression(max_iter=1000)\nlr = lr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_dev)\naccuracy_score(y_dev, y_pred)","ece8ec8c":"y_test_logistic = lr.predict(X_test)","c753eb2e":"clf = RandomForestClassifier()\nclf = clf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_dev)\naccuracy_score(y_dev, y_pred)","57b1dcdb":"y_test_rforest = clf.predict(X_test)","bc2045d2":"lsvc = LinearSVC(max_iter=1000)\nlsvc = lsvc.fit(X_train, y_train)\n\ny_pred = lsvc.predict(X_dev)\naccuracy_score(y_dev, y_pred)","c4a2271d":"y_test_lsvc = lsvc.predict(X_test)","d5bda02e":"xg = XGBClassifier()\nxg = xg.fit(X_train, y_train)\n\ny_pred = xg.predict(X_dev)\naccuracy_score(y_dev, y_pred)","dd4638c6":"y_test_xg = xg.predict(X_test)","0543282b":"submission = pd.read_csv('\/kaggle\/input\/i2a2-nlp-2021-sentiment-analysis\/sample_submission.csv')","a1a2df5b":"pred_all=pd.DataFrame({'LR':y_test_logistic,'LinearSCV':y_test_lsvc, 'RF':y_test_rforest, 'XG': y_test_xg})\npred_mode=pred_all.agg('mode',axis=1)[0].values\npred_all.head()","980701c8":"pred_mode=[int(i) for i in pred_mode]\nsubmission.rating=pred_mode\nsubmission.to_csv('ensemble_submission.csv',index=False)\nsubmission.head()","bf5dfd9b":"# LinearSVC","62063e91":"# Ensemble","aabc4567":"# LogisticRegression","183126bd":"# XGBClassifier","aefc609b":"# RandomForestClassifier","9b4768ba":"# After data preprocessing"}}