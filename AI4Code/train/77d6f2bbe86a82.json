{"cell_type":{"1c8a8fbc":"code","0342ef21":"code","d429dfdf":"code","6d722935":"code","dc0eecdf":"code","afacbd9f":"code","7959f1b2":"code","f9d14c40":"code","ae378ac6":"code","53e475ab":"code","024b308a":"code","0010be94":"code","fd92d681":"code","45ce3a7b":"code","2f428696":"code","48b7a105":"code","053647cc":"code","f82a098d":"code","94a53d02":"code","bdd6fd62":"code","0aa25c8f":"code","da67f83e":"code","88c0560f":"code","c1de4875":"code","45a10c1d":"code","9d6408ec":"code","8deeac24":"code","9e974d05":"code","0d0ddcbd":"code","21a8be11":"code","426b25a4":"code","88ea307d":"code","d89fb6f1":"code","e63b0365":"code","e74b4c7e":"code","55c07579":"code","1a11aad8":"code","15840846":"code","91573bca":"code","40a40c0e":"code","1771d195":"code","01c26929":"markdown","410e997b":"markdown","c83d17b2":"markdown","819579a9":"markdown","f25831dc":"markdown","08641add":"markdown","08da93fa":"markdown","35c3724a":"markdown","7c86095f":"markdown","7e26c299":"markdown","95489200":"markdown","badc683f":"markdown","cec2ec18":"markdown","1042adfd":"markdown","f82176d2":"markdown","7ad8cefe":"markdown","4068ff96":"markdown","80716cea":"markdown","9de175a5":"markdown","52d4ee42":"markdown","ddf87fcb":"markdown","0e23105b":"markdown","0f9a0211":"markdown","c20227a6":"markdown","08ed5c9b":"markdown","54ddc406":"markdown","6fa98194":"markdown","9464a922":"markdown","3e0745f2":"markdown","a437d7fb":"markdown"},"source":{"1c8a8fbc":"from IPython.display import Image\nimport os\nImage('..\/input\/youtubemarketing\/youtube-marketing.jpg')","0342ef21":"# import necessary libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm\nimport collections\nimport pickle\nimport gc","d429dfdf":"# Read all files\njson_files=[]\ncsv_files=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        fname=os.path.join(dirname, filename)\n        if fname.endswith('csv'):\n            csv_files.append(fname)\n        elif fname.endswith('json'):\n            json_files.append(fname)\n            \n            \n# Reorder CSV files\ncountry_codes=list(map(lambda string:''.join(list(filter(lambda word:word.isupper(),string))),csv_files))\ncountry_codes,order=zip(*sorted(list(zip(country_codes,range(len(country_codes)))), key=lambda val:val[0]))\ncsv_files=[csv_files[ind] for ind in order]\n\n# Reorder json files\ncountry_codes=list(map(lambda string:''.join(list(filter(lambda word:word.isupper(),string))),json_files))\ncountry_codes,order=zip(*sorted(list(zip(country_codes,range(len(country_codes)))), key=lambda val:val[0]))\njson_files=[json_files[ind] for ind in order]\n\n\ndef initialize_country_dataframe(dataframe,json_fname,country_code):\n    '''First, remove duplicate rows from the dataframe, second, map category_id column to actual categories, thrid,\n    new column in the dataframe called country_code'''\n    \n    df=dataframe.copy()\n    df.drop_duplicates(inplace=True)\n    \n    with open(json_fname,'r') as f:\n        json_data=json.loads(f.read())\n\n    mapping_dict=dict([(int(dictionary['id']),dictionary['snippet']['title']) for dictionary in json_data['items']])\n\n    df['category']=df['category_id'].replace(mapping_dict)\n    del df['category_id']\n\n    df['country_code']=country_code\n    \n    return df\n\n# Initialize country-by-country dataframe using above written function\ndataframes=[]\nfor ind,code in enumerate(country_codes):\n    try:\n        df=pd.read_csv(csv_files[ind])\n    except:\n        df=pd.read_csv(csv_files[ind],engine='python')\n                \n    df=initialize_country_dataframe(df,json_files[ind],code)\n    print(code,df.shape)\n    dataframes.append(df)\n    \n    \n# Concatenate individual dataframe to form single main dataframe\ndataframe=pd.concat(dataframes)\nprint(dataframe.shape)\n\n\n# Remove videos with unknown video id\ndrop_index=dataframe[dataframe.video_id.isin(['#NAME?','#VALUE!'])].index\ndataframe.drop(drop_index, axis=0, inplace=True)","6d722935":"'''# Create feature num_days that indicates the number of days the videos are in trend\nvideo_ids=dataframe.video_id.unique().tolist()\nnum_days=[]\nid_days={}\nfor vid in tqdm(video_ids):\n    days=len(dataframe[dataframe.video_id==vid].trending_date.unique())\n    id_days[vid]=days\n    num_days.append(days)\n    \n\n# Create feature num_countries that indicates the number of countries in the videos trended\nvideo_ids=dataframe.video_id.unique().tolist()\nnum_countries=[]\nid_countries={}\nfor vid in tqdm(video_ids):\n    days=len(dataframe[dataframe.video_id==vid].country_code.unique())\n    id_countries[vid]=days\n    num_countries.append(days)'''","dc0eecdf":"# Reading pre-calculated dictionaries\nwith open('\/kaggle\/input\/id-dayspickle\/id_days.pickle','rb') as f:\n    id_days=pickle.load(f)\n\nwith open('\/kaggle\/input\/id-countriespickle\/id_countries.pickle','rb') as f:\n    id_countries=pickle.load(f)\n\nnum_days=id_days.values()\nnum_countries=id_countries.values()\n\n# Adding feature num_days into the dataframe\ndef n_days_replace(vid):\n    return id_days[vid]\n\ndataframe['num_days']=dataframe.video_id.apply(func=n_days_replace)\n\n# Adding feature num_countries into the dataframe\ndef n_countries_replace(vid):\n    return id_countries[vid]\n\ndataframe['num_countries']=dataframe.video_id.apply(func=n_countries_replace)","afacbd9f":"# Create feature days_lapse that indicates the number of days before videos are in trend\ndef unique_video_id(keep='last'):\n    '''Removes duplicate videos to keep single record according to trending_date and keep argument.'''\n    df=dataframe.copy()\n    \n    df.sort_values(by=['video_id','trending_date'],axis=0,inplace=True)\n    df.drop_duplicates(subset='video_id',keep='last',inplace=True)\n    \n    return df\n\ndf=unique_video_id(keep='first')\n\ndef publish_date(string):\n    return string.split('T')[0]\n\ndf['publish_date']=pd.to_datetime(df.publish_time.apply(func=lambda val:publish_date(val)),format='%Y-%m-%d')\ndf['trending_date']=pd.to_datetime(df.trending_date,format='%y.%d.%m')\ndf['days_lapse']=df['trending_date']-df['publish_date']\n\ndf.days_lapse=df.days_lapse.apply(func=lambda val:val.days).values\nid_days_lapse=dict(zip(df.video_id.values,df.days_lapse.values))\n\ndef n_days_lapse_replace(vid):\n    return id_days_lapse[vid]\n\ndataframe['days_lapse']=dataframe.video_id.apply(func=n_days_lapse_replace)\n\n# Create feature trend_month that indicates month the videos are in trend\ndef trend_month(string):\n    return int(string.split('.')[2])\n\ndataframe['trend_month']=dataframe.trending_date.apply(func=lambda val:trend_month(val))\n\n# Create feature publish_month that indicates the months that the videos are published in\ndef publish_month(string):\n    return int(string.split('T')[0].split('-')[1])\ndataframe['publish_month']=dataframe.publish_time.apply(func=lambda val:publish_month(val))\n\n# Create feature publish_hour that indicates the hours that the videos are published in\ndef publish_hour(string):\n    return int(string.split('T')[1].split(':')[0])\n\ndataframe['publish_hour']=dataframe.publish_time.apply(func=lambda val:publish_hour(val))","7959f1b2":"trending_days=collections.Counter(num_days)\ndays,freq=zip(*sorted(trending_days.items(),key=lambda val:val[0]))\n\nfig,[ax1,ax2]=plt.subplots(nrows=2,ncols=1,figsize=(14,10))\n\ncmap = plt.get_cmap('GnBu')\ncolors=[cmap(i) for i in np.linspace(0, 1, len(days))]\nax1.bar(range(len(days)),np.log(freq),color=colors)\nax1.set_xticks(range(len(days)))\nax1.set_xticklabels(days)    \n\nlabels=[str(val) for val in freq]\nfor ind,val in enumerate(np.log(freq)):\n    ax1.text(ind,val+0.1,labels[ind],ha='center')\n\nax1.set_xticks(range(len(days)))\nax1.set_xticklabels(days)\n\nax1.set_ylabel('Log frequency')\n\ncum_arr=np.cumsum(freq)\nmax_val=np.max(cum_arr)\nmin_val=np.min(cum_arr)\n\nax2.plot((cum_arr-min_val)\/(max_val-min_val))\nax2.set_xticks(range(len(days)))\nax2.set_xticklabels(days)\nax2.set_ylabel('Cumulative proportion of number of videos')\nax2.set_xlabel('For number of days videos are in trend');","f9d14c40":"trending_countries=collections.Counter(num_countries)\nnc,freq=zip(*sorted(trending_countries.items(),key=lambda val:val[0]))\n\nfig,ax=plt.subplots(figsize=(14,6))\n\ncmap = plt.get_cmap('PuBu')\ncolors=[cmap(i) for i in np.linspace(0, 1, len(nc))]\nax.bar(range(len(nc)),np.log(freq),color=colors)\nax.set_xticks(range(len(nc)))\nax.set_xticklabels(nc)    \n\nlabels=[str(val) for val in freq]\nfor ind,val in enumerate(np.log(freq)):\n    ax.text(ind,val+0.1,labels[ind],ha='center')\n\nax.set_ylabel('Log frequency')\nax.set_xlabel('For number of countries videos are in trend')\nax.set_title('Discrete log frequency plot for the number of countries videos are in trend');","ae378ac6":"df=unique_video_id(keep='first')\n\nmonths,counts=zip(*sorted(df.publish_month.value_counts().to_dict().items(), key=lambda val:val[0]))\n\nfig,ax=plt.subplots(figsize=(15,5))\n\ncmap = plt.get_cmap('Set3')\ncolors=[cmap(i) for i in range(len(months))]\n\nax.bar(months,counts,color=colors)\nax.set_xticks(range(1,len(months)+1))\nax.set_xticklabels(months)\nax.set_xlabel('Months')\nax.set_ylabel('Number of videos published')\nfor ind,val in enumerate(counts):\n    ax.text(months[ind],val+500,val,ha='center');","53e475ab":"yr_months=list(map(lambda val:'-'.join(val.split('T')[0].split('-')[:-1]),df.publish_time.unique()))\n\nyr_months,counts=zip(*sorted(collections.Counter(yr_months).items(),key=lambda val:val[0]))\n\nfig,ax=plt.subplots(figsize=(20,6))\n\ncmap = plt.get_cmap('Reds')\ncolors=[cmap(i) for i in np.linspace(0, 1, len(counts))]\n\nax.scatter(range(len(yr_months)),counts,color=colors,)\nax.set_xticks(range(len(yr_months)))\nax.set_xticklabels(yr_months,rotation=90)\nax.set_xlabel('Publsihed-months in the years')\nax.set_ylabel('Number of videos published')\nax.set_title('Frequency plot of number of videos published in the months of the years');","024b308a":"df=unique_video_id(keep='first')\n\nhours,counts=zip(*sorted(df.publish_hour.value_counts().to_dict().items(),key=lambda val:val[0]))\n\nfig,ax=plt.subplots(figsize=(10,6))\n\ncmap = plt.get_cmap('twilight')\ncolors=[cmap(i) for i in np.linspace(0, 1, len(hours))]\n\nax.bar(hours,counts,color=colors)\nax.set_xticks(range(len(hours)))\nax.set_xticklabels(hours)\nax.set_xlabel('Hour of a day')\nax.set_ylabel('Number of videos published');","0010be94":"df=unique_video_id(keep='first')\n\ndays_lapse=df['days_lapse']\ndays_lapse_count=days_lapse.value_counts().to_dict()\ndays,count=zip(*sorted(list(filter(lambda val:val[1]>1,days_lapse_count.items())),key=lambda val:val[0]))\n\nfig,[ax1,ax2]=plt.subplots(figsize=(19,13),nrows=2,ncols=1)\n\ncmap = plt.get_cmap('autumn')\ncolors=[cmap(i) for i in np.linspace(0, 1, len(days))]\n\nax1.bar(range(len(days)),np.log(count),width=0.6,color=colors)\nax1.set_xticks(range(len(days)))\nax1.set_xticklabels(days,rotation=45)\nax1.set_ylabel('log of frequency count')\nax1.set_xlabel('Number of days pass before videos are trending')\nax1.set_title('Discrete frequency plot for number of days before videos are trending')\n\n\ncum_arr=np.cumsum(count)\nmax_val=np.max(cum_arr)\nmin_val=np.min(cum_arr)\nax2.plot((cum_arr-min_val)\/(max_val-min_val))\nax2.set_xticks(range(len(days)))\nax2.set_xticklabels(days,rotation=45)\nax2.set_ylabel('Cumulative proportion of number of videos')\nax2.set_xlabel('Number of days pass before videos are trending');","fd92d681":"df=unique_video_id(keep='first')\nprint('Total number of unique videos:',df.shape[0])\nprint('Total number of unique videos that take less than 31 days to be in trend:',df[df.days_lapse<31].shape[0])\nfig,ax=plt.subplots(figsize=(20,6))\ndf[df.days_lapse<31].boxplot(column='days_lapse',by='publish_hour',rot=90,ax=ax)\nax.set_xlabel('Hours')\nax.set_ylabel('days lapse')\nax.set_title('')\nfig.suptitle('Box plot for videos that took less than 31 days to be in trend');","45ce3a7b":"df=dataframe.copy()\ndf.sort_values(by=['video_id','trending_date'],inplace=True)\ndf.drop_duplicates(subset='video_id',keep='last',inplace=True)\n\nfig=plt.figure(figsize=(20,15))\nfig.tight_layout()\n\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\nax3=fig.add_subplot(212)\n\ndf.boxplot(column='views',by='num_countries',ax=ax1)\nax1.set_xlabel('For number of countries videos trend')\nax1.set_ylabel('Final number of views')\nax1.set_title('')\n\ndf.boxplot(column='likes',by='num_countries',ax=ax2)\nax2.set_xlabel('For number of countries videos trend')\nax2.set_ylabel('Final number of likes')\nax2.set_title('')\n\n\ndf.boxplot(column='num_days',by='num_countries',ax=ax3)\nax3.set_xlabel('For number of countries videos trend')\nax3.set_ylabel('For number of days videos trend')\nax3.set_title('')\n\nfig.suptitle('Boxplot: grouped by the number of countries');","2f428696":"df_disabled=unique_video_id()\ndf_disabled=df_disabled[df_disabled.comments_disabled==True]\ndf_disabled.sort_values(by=['video_id','comments_disabled'],inplace=True)\ndf_disabled.drop_duplicates(subset='video_id',keep='last',inplace=True)\ndisabled_dict=df_disabled.country_code.value_counts().to_dict()\n\ndf_enabled=unique_video_id()\ndf_enabled=df_enabled[df_enabled.comments_disabled==False]\ndf_enabled.sort_values(by=['video_id','comments_disabled'],inplace=True)\ndf_enabled.drop_duplicates(subset='video_id',keep='first',inplace=True)\nenabled_dict=df_enabled.country_code.value_counts().to_dict()\n\ndis_ena_prop={}\nfor country in disabled_dict.keys():\n    dis_ena_prop[country]=disabled_dict[country]\/enabled_dict[country]\n\nfig,ax=plt.subplots(figsize=(13,6))\n\ncmap = plt.get_cmap('Set3')\ncolors=[cmap(i) for i in range(len(days))]\n\ncountries=dis_ena_prop.keys()\nvalues=list(dis_ena_prop.values())\nax.bar(countries,np.log(np.array(list(values))+1),color=colors)\nax.set_ylabel('log(1+values) transformed values')\nax.set_xlabel('Country codes')\nax.set_title('Percentage of unique trending videos that have comments section disabled over each country')\n\nfor ind,val in enumerate(np.log(np.array(list(values))+1)):\n    ax.text(ind,val+0.001,str(round(values[ind]*100,1))+'%',ha='center')","48b7a105":"df=unique_video_id()\n\n# 1. Get number of views for each categories\nviews_per_category=df.groupby(by=['category'],as_index=False).views.sum()\n\n# 2. Get number of views and category names\nviews_in_million=[int(views\/1000000) for views in views_per_category.sort_values(by='views').views.values]\ncat_val=views_per_category.sort_values(by='views').category.values\n\n# 3. Normalize number of views for data visualization\nrelative_vals=views_per_category.sort_values(by='views').views.values\nmax_val=np.max(relative_vals)\nmin_val=np.min(relative_vals)\ndiff=max_val-min_val\nms_val=(relative_vals-min_val)\/diff\n\n# 4. Create axes for plotting\nfig,ax=plt.subplots(figsize=(20,3))\n# 4.1 Add one more axis\nbx=ax.twiny()\nx=range(len(cat_val))\ny=[5]*len(cat_val)\n\n# 5. Plot one category at a time using matplotlib scatter plot\nfor ind,cat in enumerate(cat_val):\n    ax.scatter(x[ind],y[ind],s=ms_val[ind]*10000,cmap='Blues',alpha=0.5,edgecolors=\"grey\", linewidth=2)\nax.set_xticks(range(len(cat_val)))\nax.set_xticks(range(len(cat_val)))\nax.set_yticklabels([])\nax.set_xticklabels(cat_val,rotation=90)\nax.set_xlabel('Video categories',fontsize=16)\n\n# 6. Write number of views in millions on the x-axis above the plot\nbx.set_xticks(range(len(views_in_million)))\nbx.set_xticklabels(views_in_million,rotation=90)\nbx.set_xlabel('Number of views in millions',fontsize=16);","053647cc":"# 1. Replace category 29 by string 'Other'\ndataframe.category.replace({29:'Other'},inplace=True)\n\n# 2. Count number of occurances of video category for each category\ncountry_by_category=dataframe.groupby(by='country_code')['category'].value_counts()\n\n# 3. Write function that will plot a pie-chart\ndef pie_chart(country_code,axis):\n    '''Plots a pie_chart for a country_by_category series for a given country code on given axis'''\n    cmap = plt.get_cmap('Spectral')\n    colors=[cmap(i) for i in np.linspace(0, 1, len(country_by_category[country_code].index))]\n    axis.pie(country_by_category[country_code].values,labels=country_by_category[country_code].index,autopct='%.2f',colors=colors,shadow=True)\n    axis.set_title(country_code,fontsize=14);\n\n# 4. Plot individual pie-chart for each country\nfig,ax=plt.subplots(nrows=5,ncols=2,figsize=(16,20))\nfor c_i in range(0,len(country_codes),2):\n    col=0\n    ind=c_i\/\/2\n    pie_chart(country_codes[c_i],ax[ind][col])\n    pie_chart(country_codes[c_i+1],ax[ind][col+1])\n\n# 5. Plot pie-chart for all countries together\nfig,ax=plt.subplots(figsize=(4,4))\ncmap = plt.get_cmap('Spectral')\nall_countries_prop=dataframe.category.value_counts()\ncolors=[cmap(i) for i in np.linspace(0, 1, len(all_countries_prop.index))]\nax.pie(all_countries_prop.values,labels=all_countries_prop.index,autopct='%.2f',colors=colors,shadow=True)\nax.set_title('All Countries');","f82a098d":"# 1. Create temporary dataframe\ndf=dataframe[dataframe.comments_disabled==False].copy()\n\n# 2. Keep single record of each video_id the last day video is in trend\ndf.sort_values(by=['video_id','trending_date'],inplace=True)\ndf.drop_duplicates(subset='video_id',keep='last',inplace=True)\n\n# 3. Add new column on the proportion\ndf['odds_of_likes']=df['likes']\/(df['views'])\n\n# 4. Create axis for distribution plot\nfig1,ax1=plt.subplots(figsize=(20,20))\nfig1.tight_layout()\n\n# 5. Plot histogram\n\ndf.hist(column='odds_of_likes',by='category',ax=ax1)\nax1.set_title('Distribution plot of the proportion likes per views grouped by the video categories')\n\n# 6. Create axis for box-plot\nfig2,ax2=plt.subplots(figsize=(15,7))\nfig2.tight_layout()\n\n# 7. Create box-plot\ndf.boxplot(column='odds_of_likes',by='category',rot=90,ax=ax2)","94a53d02":"# 1. Plot correation matrix and pairs of countries with higher than 60% correlation\ndef get_corr_pairs(category, threshold=0.6):\n    '''Get list of pairs of country codes those have greater than or equal to 60% pearson correlation'''\n    series={}\n    for country in country_codes:\n        series[country]=dataframe[(dataframe.category==category) & (dataframe.country_code==country)].groupby(by='trending_date')['views'].sum().values\n    \n    key_list=series.keys()\n    drop_list=[]\n    \n    for key in key_list:\n        if not len(series[key])==205:\n            drop_list.append(key)\n            \n    for key in drop_list:\n        del series[key]\n    \n    df=pd.DataFrame(series)\n    \n    #fig,ax=plt.subplots(figsize=(8,8))\n    corr=df.corr()\n    #mask = np.triu(np.ones_like(corr, dtype=np.bool))\n    #sns.heatmap(corr,mask=mask,annot=True,ax=ax)\n    #fig.suptitle(category);\n    \n    corr=corr.abs()\n    s = corr.unstack()\n    so = s.sort_values(kind=\"quicksort\")\n\n    df=so.to_frame().reset_index().rename(columns={0:'corr_val'})\n    corr_pairs=df[(df.corr_val>=threshold) & (df.corr_val!=1.0)][['level_0','level_1']].values\n    \n    list_of_sets=[set((pair[0],pair[1])) for pair in corr_pairs]\n\n    corr_pairs=[]\n    for pair in list_of_sets:\n        if pair not in corr_pairs:\n            corr_pairs.append(pair)\n    corr_pairs=[list(pair) for pair in corr_pairs]\n    return corr_pairs\n\n# 2. Plot sum of viewers over the series for the country pairs\ndef plot_patterns(corr_pairs, category, figure_size=(15,20)):\n    n_rows=len(corr_pairs)\n    fig,ax=plt.subplots(nrows=n_rows, ncols=1,figsize=figure_size)\n    fig.tight_layout(pad=5)\n    fig.suptitle(category,fontsize=17)\n    if n_rows>1:\n        for ind,pair in enumerate(corr_pairs):\n            country_1=pair[0]\n            country_2=pair[1]\n            dataframe[(dataframe.category==category) & (dataframe.country_code==country_1)].groupby(by='trending_date')['views'].sum().plot(ax=ax[ind],label=country_1)\n            dataframe[(dataframe.category==category) & (dataframe.country_code==country_2)].groupby(by='trending_date')['views'].sum().plot(ax=ax[ind],label=country_2)\n            ax[ind].legend()\n            ax[ind].set_title(' '.join(pair))\n    else:\n        for ind,pair in enumerate(corr_pairs):\n            country_1=pair[0]\n            country_2=pair[1]\n            dataframe[(dataframe.category==category) & (dataframe.country_code==country_2)].groupby(by='trending_date')['views'].sum().plot(ax=ax,label=country_1)\n            dataframe[(dataframe.category==category) & (dataframe.country_code==country_1)].groupby(by='trending_date')['views'].sum().plot(ax=ax,label=country_2)\n            ax.legend()\n            ax.set_title(' '.join(pair))\n\n# 3. Plot series for each video category at a time\nfor category in dataframe.category.unique():\n    if category in ['Music', 'Comedy', 'Entertainment', 'News & Politics','People & Blogs', 'Howto & Style', 'Film & Animation', 'Sports']:\n        corr_pairs=get_corr_pairs(category)\n        if len(corr_pairs)>0:\n            if len(corr_pairs)<5:\n                figure_size=(20,8)\n            else:\n                figure_size=(15,20)\n            plot_patterns(corr_pairs,category,figure_size)\n        else:\n            print(category,'does not have series having correlation atleast 0.6')","bdd6fd62":"# 1. Clean tags data\n\n# 1.1 Write function to decontract tags data\nimport re\ndef decontraction(phrase):\n    '''Decontracts given strings'''\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\n# 1.2 Write function to clean data\ndef clean_data(string):\n    '''Removes all unwanted charetchters from a string'''\n    return ' '.join(decontraction(string.lower().replace('\"','').replace('\/','|')).split('|'))\n\n# 1.3 Use pandas apply function to make use of clean_data funcion for every row of the dataset to clean data\ntag_list=dataframe.tags.apply(func=clean_data)\n\n# 2. Plot word cloud\n\n# 2.1 Wirte function to create wordcloud\nfrom wordcloud import WordCloud\ndef plot_wordcloud(df,country,axis):\n    '''Plots wordcloud of tags columns for a given country code on the given axis'''\n    tag_list=df[df.country_code==country].tags.apply(func=clean_data)\n    text=' '.join(tag_list)\n    wordcloud=WordCloud().generate(text)\n    \n    axis.imshow(wordcloud)\n    axis.set_title(country)\n\n# 2.2 Plot word cloud using above written function\nfig,ax=plt.subplots(nrows=5,ncols=2,figsize=(20,20))\ndf=unique_video_id(keep='first')\nfor c_i in range(0,len(country_codes),2):\n    ind=c_i\/\/2\n    col=0\n    plot_wordcloud(df,country_codes[c_i],ax[ind][col])\n    plot_wordcloud(df,country_codes[c_i+1],ax[ind][col+1])","0aa25c8f":"# Convert trending date column into datetime object\ndataframe.trending_date=pd.to_datetime(dataframe.trending_date,format='%y.%d.%m')\n\n# Sort dataframe according to the trending date\ndataframe.sort_values(by='trending_date',axis=0,inplace=True)\n\n# Create train and test data frames based on trending date where we want predict number of likes of\n# the future trending videos\ndf_train=dataframe[dataframe.trending_date<'2018-05-01'].copy()\ndf_test=dataframe[dataframe.trending_date>='2018-05-01'].copy()\n\ndel df\ngc.collect()\n\n# Seperate predictors from the target feature\n\ny_train=df_train.likes.values\ny_test=df_test.likes.values\n\ndel df_train['likes'], df_test['likes']\ngc.collect()\n\nX_train=df_train.copy()\nX_test=df_test.copy()\n\ndel df_train,df_test\ngc.collect()\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","da67f83e":"fig,ax=plt.subplots()\nax.hist(y_train,bins=500)\nax.set_xlabel('number of likes')\nax.set_title('Histogram of y_train');","88c0560f":"# Log transformation of the train-target feature\nfig,ax=plt.subplots()\nax.hist(np.log(y_train+1))\nax.set_xlabel('transformed number of likes')\nax.set_title('Histogram of transformed y_train');","c1de4875":"X_train['views']=np.log(X_train['views']+1)\nX_train['comment_count']=np.log(X_train['comment_count']+1)\n\nX_test['views']=np.log(X_test['views']+1)\nX_test['comment_count']=np.log(X_test['comment_count']+1)\n\ntrain=X_train.copy()\ntrain['likes']=np.log(y_train+1)\n\ntest=X_test.copy()","45a10c1d":"train.plot.scatter(x='views',y='likes');","9d6408ec":"train['views_cat']=pd.cut(train.views.values,bins=[0.0,7.5,10.0,12.5,15.0,17.5,20.0],labels='a b c d e f'.split(),right=True)\n\ntest['views_cat']=pd.cut(test.views.values,bins=[0.0,7.5,10.0,12.5,15.0,17.5,20.0],labels='a b c d e f'.split(),right=True)","8deeac24":"train.boxplot(column='likes',by='views_cat');","9e974d05":"train.plot.scatter(x='comment_count',y='likes');","0d0ddcbd":"train['comment_count_cat']=pd.cut(train.comment_count.values,bins=[-0.5,1,2,4,6,8,10,12,14,20],labels='a b c d e f g h i'.split(),right=True)\n\ntest['comment_count_cat']=pd.cut(test.comment_count.values,bins=[-0.5,1,2,4,6,8,10,12,14,20],labels='a b c d e f g h i'.split(),right=True)\n\ntrain.boxplot(column='likes',by='comment_count_cat');","21a8be11":"fig,ax=plt.subplots(figsize=(10,4))\ntrain.boxplot(column='likes',by='comments_disabled',ax=ax)\nax.set_ylabel('log(likes+1)');","426b25a4":"fig,ax=plt.subplots(figsize=(10,4))\ntrain.boxplot(column='likes',by='video_error_or_removed',ax=ax)\nax.set_ylabel('log(likes+1)');","88ea307d":"fig,ax=plt.subplots(figsize=(10,5))\ntrain.boxplot(column='likes',by='category',ax=ax, rot=90)\nax.set_xlabel('category')\nax.set_ylabel('log(likes+1)');","d89fb6f1":"fig,ax=plt.subplots(figsize=(10,5))\ntrain.boxplot(column='likes',by='country_code',ax=ax)\nax.set_xlabel('country_code')\nax.set_ylabel('log(likes+1)');","e63b0365":"fig,ax=plt.subplots(figsize=(10,5))\ntrain.boxplot(column='likes',by='num_countries',ax=ax)\nax.set_xlabel('num_countries')\nax.set_ylabel('log(likes+1)');","e74b4c7e":"fig,ax=plt.subplots(figsize=(10,5))\ntrain.boxplot(column='likes',by='num_days',ax=ax)\nax.set_xticks(range(1,len(train.num_days.unique())+1))\nax.set_xticklabels(sorted(train.num_days.unique()),rotation=90)\nax.set_xlabel('num_days')\nax.set_ylabel('log(likes+1)');","55c07579":"train.num_days[(train.num_days>=8) & (train.num_days<15)]=8\ntrain.num_days[(train.num_days>=15) & (train.num_days<30)]=15\ntrain.num_days[(train.num_days>=30) & (train.num_days<40)]=30\n\ntest.num_days[(test.num_days>=8) & (test.num_days<15)]=8\ntest.num_days[(test.num_days>=15) & (test.num_days<30)]=15\ntest.num_days[(test.num_days>=30) & (test.num_days<40)]=30\n\ntrain.num_days.replace(dict(zip(list(range(1,8))+[8,15,30],'a b c d e f g h i j'.split())),inplace=True)\n\ntest.num_days.replace(dict(zip(list(range(1,8))+[8,15,30],'a b c d e f g h i j'.split())),inplace=True)\n\nfig,ax=plt.subplots(figsize=(15,6))\ntrain.boxplot(column='likes',by='num_days',ax=ax)\n#ax.set_xticks(range(1,len(list(range(8))+[8,15,30])+1))\n#ax.set_xticklabels(list(range(8))+[8,15,30],rotation=90)\nax.set_xlabel('num_days')\nax.set_ylabel('log(likes+1)');","1a11aad8":"train.num_countries.replace(dict(zip(list(range(1,11)),'a b c d e f g h i j'.split())),inplace=True)\ntrain.comments_disabled.replace({False:'false',True:'true'},inplace=True)\ntrain.ratings_disabled.replace({False:'false',True:'true'},inplace=True)\ntrain.video_error_or_removed.replace({False:'false',True:'true'},inplace=True)\n\ntest.num_countries.replace(dict(zip(list(range(1,11)),'a b c d e f g h i j'.split())),inplace=True)\ntest.comments_disabled.replace({False:'false',True:'true'},inplace=True)\ntest.ratings_disabled.replace({False:'false',True:'true'},inplace=True)\ntest.video_error_or_removed.replace({False:'false',True:'true'},inplace=True)","15840846":"# Create train and test data frames for prediction\nX_train=train[['comments_disabled','ratings_disabled','video_error_or_removed','category','country_code','num_countries','num_days','days_lapse','views_cat','comment_count_cat']]\nX_test=test[['comments_disabled','ratings_disabled','video_error_or_removed','category','country_code','num_countries','num_days','days_lapse','views_cat','comment_count_cat']]","91573bca":"train_rows=X_train.shape[0]\ndata=pd.concat([X_train,X_test])\n\ndata=pd.get_dummies(data)\n\nX_train=data[:train_rows].copy()\nX_test=data[train_rows:].copy()\n\ndel data\ngc.collect()\n\nX_train.shape,X_test.shape","40a40c0e":"# Baseline linear regression model\nfrom sklearn.linear_model import LinearRegression\n\nlr=LinearRegression()\nlr.fit(X_train,np.log(y_train+1))\nlr.score(X_train,np.log(y_train+1))","1771d195":"from sklearn import metrics\n\ny_pred=lr.predict(X_test)\ny_pred=np.exp(y_pred)-1\nmetrics.mean_absolute_error(y_test,y_pred)","01c26929":"## 16. Word cloud of video tags","410e997b":"### 2. comment count","c83d17b2":"## C) Creating train and test data set according to trending date feature","819579a9":"### 6. country code","f25831dc":"## 2. Frequency of the number of countries that videos trend in","08641add":"#### Note: Encoding as follows:<br>\na=1,b=2,c=3,d=4,e=5,f=6,g=7,h=[8,14],i=[15,29],j=[30,40]<br>\n## E) Feature Encoding","08da93fa":"## G) Predicting likes next two months of trending videos","35c3724a":"In the histogram plots above, we can see that even though these are trending videos very less number of viewers actually hit these videos likes. In the box-plot, we see for all video categories 3rd quartile falls below 20% hit likes to viewers ratio.<br>\n## 15. Which countries have similar behaviors in terms of views for the different video categories","7c86095f":"In the first two box-plots, we see being in trend in more number of countries help the videos to gain more views and likes, which is obvious. But, the third box-plot gives an evidence that the videos that trend in more number of countries are tend to be in trend for longer period of days than the videos that are in trend in lesser number of countries. The box plot shows that the median or 2nd quartile, that represents the median number of days videos are in trend, increases from left plot to right plot.<br>\n## 9. Percentage of videos by the countries that have comments section disabled","7e26c299":"## B) Exploratory Data Analysis<br>\n## 1. Frequency of number of days videos are in trend","95489200":"This plot matches our conclusion from the plot number 4, where we concluded that the time context is important factor in the trending videos. In the above frequency plot, most trending videos take less than a week before coming into trend. As days pass after publishing the videos, we see lesser and lesser videos becoming trending.<br>\n## 7. Resolving the misconception of the effect of the hours videos are published","badc683f":"Predicting likes for the futher videos is not a naive task. To build baseline model, we need understand what are the inherent relationships between features of the available YouTube dataset. So, we will start with understanding the entire dataset and building features along with it. Next, we will explore which of the features help for likes prediction. The final step is to fit simple linear regression model to pave the way for more complex and advance machine learning model prediction.<br>\n\n## A) Data Preparation<br>\n### a) Constructing single data frame by combining data from all of the countries","cec2ec18":"### b) Feature engineering<br>\n#### Below cell takes around 5-6 hours to run. Hence, I am commenting it and using the locally precalculated dictionaries instead to save time. But, feel free to comment it out to try yourself.","1042adfd":"Frome the plot above we can say that more videos are published during afternoon time slot than the other times of the day.<br>\n## 6. How long usually it takes for the videos to become trending?","f82176d2":"### 3. Comments disabled","7ad8cefe":"## 3. Does number of videos published in a month vary over the months?","4068ff96":"Commonly people watch youtube as a leisure activity. Hence, it does make sense that the most frequent trending video categories across all the countries are related to the leisure activities such as Entertenment, People & Blogs, Music, etc.<br>\n## 14. Distribution and box-plot for the proportion of likes per views for each category","80716cea":"## F) Baseline linear regression model train","9de175a5":"The histogram above gives away that number of likes have very high range and very unevenly distributed frequencies. In addition, distribution of likes is right skewed. For linear regression, we would like to have target feature normally distributed or even close to norally distributed. So, we will perform log-transformation on the likes distribution.","52d4ee42":"Note that, in addition to the likes, we also log-transformed views and comment_count.<br>\n## D) Analysis of dependence of the targer feature on the independent features<br>\n### 1.Views","ddf87fcb":"In the above plot, months are numerically encoded in sequence starting January to December, i.e, 1 represent January and 12 represent December.<br>\nBy observing the bar chart, the count of trending videos published in July, August, September, October are significantly less than the rest of months in a year.<br>\nBut, it is worth noticing that published dates span in many differnt year, showed in plot below, whereas the trending dates span from November 2017 to June 2018.<br>\n## 4. What are the frequencies of the number of videos published by months from the all different years?","0e23105b":"## 10. Number of views by the video categories","0f9a0211":"### 7. num countries","c20227a6":"## 11. Proportion of video categories trending in the countries over the entire period of given time","08ed5c9b":"### 8. Num days","54ddc406":"# Baseline regression model for predicting YouTube likes for the next 2 months of trending videos","6fa98194":"We observed in plot 5 that after-noon hours are most popular time-frame for publishing trending videos. But, in the above box-plot diagram above, we see no significant difference in the videos becoming trending (quickly) based on the hours in which they are published.<br>\n## 8. Does the trending in more number of countries get videos more number views and likes? Also does it help to stay in trend for more number of days?","9464a922":"### 5. Category","3e0745f2":"In this plot, we have more trending videos published in the years 2017 and 2018, which indicates that the most of the trending videos from the data set have time context. Eventhough some of the trending videos at a time might belong to earlier times, but most of the videos are made or published recently in the time of reference.<br>\n## 5. Do the trending videos from the data set are published in specific time slot of 24 hours day more than the other times?","a437d7fb":"### 4. Video error or removed"}}