{"cell_type":{"96895776":"code","97f82e1e":"code","ba5d7322":"code","d8c2b0d1":"code","b9ce835f":"code","00559cb4":"code","3c3894a4":"code","2a96e3aa":"code","8b35787c":"code","a5537464":"code","4323c339":"code","9f84160f":"code","6068e861":"code","a79d99bb":"code","2e704e1c":"code","908dda06":"code","ea20a775":"code","66fa1a1c":"code","137bb8cc":"code","8c241c97":"markdown","3d6a292b":"markdown","2788a852":"markdown","353f9a6b":"markdown","052c1629":"markdown","6d0d98e1":"markdown","6be93eaa":"markdown","28995b18":"markdown","314aca08":"markdown","30df5e58":"markdown","7157ae07":"markdown","d59bb009":"markdown","a1eaf2bd":"markdown","ed091c4b":"markdown","d3237a75":"markdown","a60290c5":"markdown","61f6d37f":"markdown"},"source":{"96895776":"# Python\nimport time\nimport os\nimport numpy as np\n# Manejo de data y gr\u00e1ficas (Pandas, Matplotlib)\nimport pandas as pd\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nfrom matplotlib import cbook as cbook\n# Framework (Keras)\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Add, GaussianNoise, Dropout\nfrom tensorflow.keras.models import Model\nfrom sklearn.metrics import roc_auc_score\nfrom tensorflow.keras.layers import Wrapper\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import regularizers\nfrom sklearn.preprocessing import StandardScaler\n# Random seed\nnp.random.seed(1)\n# Config pandas\npd.set_option('display.max_columns', None)","97f82e1e":"test_csv = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_csv = pd.read_csv('..\/input\/titanic\/train.csv')","ba5d7322":"print(' \u2b9e Train shape: ', train_csv.shape)\nprint(' \u2b9e Observaciones en train: ', train_csv.shape[0])\nprint(' \u2b9e Test shape: ', test_csv.shape)\nprint(' \u2b9e Observaciones en test: ', test_csv.shape[0])","d8c2b0d1":"train_csv.describe()","b9ce835f":"train_dfY = train_csv.Survived\ntrain_csv = train_csv.drop(['Survived'], axis=1)\n\ntrain_test = train_csv.append(test_csv)\ntrain_test.reset_index(inplace=True)\ntrain_test.drop(['index'], inplace=True, axis=1)\n\ntrain_test.head()","00559cb4":"train_test.describe()","3c3894a4":"train_test['Age'] = train_test['Age'].fillna(train_test['Age'].median())\ntrain_test.describe()","2a96e3aa":"train_test['Title'] = train_test['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain_test['Title'] = train_test['Title'].replace(['Mlle', 'Ms'], 'Miss')\ntrain_test['Title'] = train_test['Title'].replace('Mme', 'Mrs')\ntrain_test['Title'] = train_test['Title'].replace(['Lady', 'Sir', 'Countess', 'Don', 'Dona', 'Jonkheer'], 'Royal')\n# Llenamos los 'Embarked' con el valor m\u00e1s com\u00fan\nembarked_most_common = train_test['Embarked'].describe().values[2]\ntrain_test['Embarked'] = train_test['Embarked'].fillna(embarked_most_common)\n# Cambiamos valores de las variables que son strings a num\u00e9ricos\nsex_dictionary = { \"female\": 0, \"male\": 1 }\nembarked_dictionary = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntitle_dictionary = { \"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Royal\": 5 }\ncabins_dictionary = { 'N': 0, 'C': 1, 'E': 2, 'G': 3, 'D': 4, 'A': 5, 'B': 6, 'F': 8, 'T': 9 }\ntrain_test['Sex'] = train_test['Sex'].map(sex_dictionary)\ntrain_test['Embarked'] = train_test['Embarked'].map(embarked_dictionary)\ntrain_test['Title'] = train_test['Title'].map(title_dictionary)\ntrain_test['Title'] = train_test['Title'].fillna(0)\ntrain_test['Title'] = train_test['Title'].astype(int)\ntrain_test['Cabin'] = train_test['Cabin'].fillna('N\/A')\ntrain_test['Cabin'] = train_test['Cabin'].apply(lambda x: x[0])\ntrain_test['Cabin'] = train_test['Cabin'].map(cabins_dictionary)\ntrain_test['Age'] = train_test['Age'].astype(int)\ntrain_test['Fare'] = train_test['Fare'].fillna(train_test['Fare'].mean())\ntrain_test['Fare'] = train_test['Fare'].astype(int)\ntrain_test['KinfolkNumber'] = train_test['Parch'] + train_test['SibSp'] + 1\n    \ntrain_test.head()   ","8b35787c":"# Se copian los ids de los pasajeros que est\u00e1n en el test set\npredictions = test_csv[['PassengerId']].copy()\n# Se obtienen los nuevos datasets eliminando informaci\u00f3n que no se usar\u00e1\ntrain_test = train_test.drop(columns=['PassengerId', 'Name', 'Ticket'], axis=1)\n\ntrain_test.head()  ","a5537464":"# One Hot Encoding con 'Pclass'\npclass_encoding = pd.get_dummies(train_test['Pclass'], prefix='Pclass')  \ntrain_test = pd.concat([train_test, pclass_encoding], axis=1)\n# One Hot Encoding con 'Cabin'\ncabin_encoding = pd.get_dummies(train_test['Cabin'], prefix='Cabin')  \ntrain_test = pd.concat([train_test, cabin_encoding], axis=1)\n# One Hot Encoding con 'Embarked'\nembarked_encoding = pd.get_dummies(train_test['Embarked'], prefix='Embarked')  \ntrain_test = pd.concat([train_test, embarked_encoding], axis=1)\n# One Hot Encoding con 'Title'\ntitle_encoding = pd.get_dummies(train_test['Title'], prefix='Title')  \ntrain_test = pd.concat([train_test, title_encoding], axis=1)\n\n# Se eliminan los features que ya se hicieron en One Hot Encoding\ntrain_test.drop(['Pclass', 'Title', 'Cabin', 'Embarked'], axis=1, inplace=True)\n\nprint(' \u2b9e Shape de train-test', train_test.shape)\ntrain_test.head()","4323c339":"Y_train = train_dfY\nX_train = train_test.iloc[:891]\nX_test = train_test.iloc[891:]\n\nprint(' \u2b9e Shape de train Y(output): ', Y_train.shape)\nprint(' \u2b9e Shape de train X(input): ', X_train.shape)\nprint(' \u2b9e Shape de test X(input): ', X_test.shape)\nprint(' \u2b9e Cantidad de observaciones en train: ', X_train.shape[0])\nprint(' \u2b9e Cantidad de observaciones en test: ', X_test.shape[0])\nprint('\\n')\nprint(' \u2b9e Cantidad de features: ', X_train.shape[1])","9f84160f":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6068e861":"print()\nX_train, X_dev, Y_train, Y_dev = train_test_split(X_train, Y_train, test_size=0.2, stratify=Y_train)\nprint(' \u2b9e Shape de train X(input): ', X_train.shape)\nprint(' \u2b9e Shape de dev X(input): ', X_dev.shape)","a79d99bb":"precisiones_globales=[]\ndef graf_model(train_history):\n    f = plt.figure(figsize=(15,10))\n    ax = f.add_subplot(121)\n    ax2 = f.add_subplot(122)\n    # summarize history for accuracy\n    ax.plot(train_history.history['binary_accuracy'])\n    ax.plot(train_history.history['val_binary_accuracy'])\n    ax.set_title('model accuracy')\n    ax.set_ylabel('accuracy')\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'test'], loc='upper left')\n    # summarize history for loss\n    ax2.plot(train_history.history['loss'])\n    ax2.plot(train_history.history['val_loss'])\n    ax2.set_title('model loss')\n    ax2.set_ylabel('loss')\n    ax2.set_xlabel('epoch')\n    ax2.legend(['train', 'test'], loc='upper left')\n    plt.show()\ndef precision(model, registrar=False):\n    Y_pred = model.predict(X_train)\n    train_auc = roc_auc_score(Y_train, Y_pred)\n    Y_pred = model.predict(X_dev)\n    val_auc = roc_auc_score(Y_dev, Y_pred)\n    print('Train AUC: ', train_auc)\n    print('Vali AUC: ', val_auc)\n    if registrar:\n        precisiones_globales.append([train_auc,val_auc])","2e704e1c":"# def model(): \n#     X = Input(shape=(27,))\n#     a=Dense(12, activation = \"relu\", kernel_regularizer = None, kernel_initializer='glorot_normal', bias_initializer='zeros')(X)\n#     a=Dense(12, activation = \"relu\", kernel_regularizer = None, kernel_initializer='glorot_normal', bias_initializer='zeros')(a)\n#     a=Dense(10, activation = \"relu\", kernel_regularizer = None, kernel_initializer='glorot_normal', bias_initializer='zeros')(a)\n#     a=Dense(10, activation = \"relu\", kernel_regularizer = None, kernel_initializer='glorot_normal', bias_initializer='zeros')(a)\n#     y_hat=Dense(1, activation = \"sigmoid\")(a) \n#     model = Model(inputs = X, outputs = y_hat)\n#     model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['binary_accuracy'])\n#     return model\n\n# model_FQS = model()\n# # print(model_FQS.summary())","908dda06":"# train_history_WR = model_FQS.fit(X_train, Y_train, batch_size=32, epochs=100, validation_data=(X_dev, Y_dev), verbose=0)\n# # Se imprimen gr\u00e1ficas\n# graf_model(train_history_WR)\n# precision(model_FQS)","ea20a775":"def modelReg(): \n    X = Input(shape=(27,))\n    a=Dropout(0.01)(X)\n    a=Dense(20, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01), bias_initializer='zeros')(a)\n    a=Dropout(0.5)(a)\n    a=Dense(20, activation = \"relu\", kernel_regularizer = None, bias_initializer='zeros')(a)\n    a=Dropout(0.6)(a)\n    a=Dense(10, activation = \"relu\", kernel_regularizer = None, bias_initializer='zeros')(a)\n    a=Dropout(0)(a)\n    y_hat=Dense(1, activation = \"sigmoid\")(a) \n    model = Model(inputs = X, outputs = y_hat)\n    model.compile(loss = 'binary_crossentropy', optimizer = 'Adam', metrics = ['binary_accuracy'])\n    return model\n\nmodel_FQS_R = modelReg()\n# print(model_FQS.summary())","66fa1a1c":"train_history = model_FQS_R.fit(X_train, Y_train, batch_size=32, epochs=200, validation_data=(X_dev, Y_dev), verbose=0)\n# Se imprimen gr\u00e1ficas\ngraf_model(train_history)\nprecision(model_FQS_R)","137bb8cc":"Y_test = model_FQS_R.predict(X_test)\nfor i in range(len(Y_test)):\n    if Y_test[i] < 0.5: \n        Y_test[i] = 0\n    else:\n        Y_test[i] = 1        \npredictions['Survived'] = Y_test.astype(int)\npredictions.to_csv('submission.csv', index=False)\n\npred = pd.read_csv('submission.csv')\npred.head()","8c241c97":"> ### Se vuelve a separar los dataset en train y test","3d6a292b":"> ### Modelo ***con regularizaci\u00f3n***","2788a852":"# **Se cargan y verifican los datos de entrenamiento y de test**","353f9a6b":"> ### Se eliminan las variables que no se van a usar de los dataset. Los ids del test set se almacenar\u00e1n en otra variable (*predictions*)  en la cual ser\u00e1n alojadas las predicciones de la competici\u00f3n.\n\n> El training set se quedar\u00e1 con nx=5 -> **'Pclass', 'Sex', 'Age', 'Fare', 'Embarked'**","052c1629":"> ### Se rellenan las edades que faltan (pasajeros que no tienen asignada una edad) con la mediana de las edades de todos los pasajeros","6d0d98e1":"> ### ***Predicci\u00f3n***","6be93eaa":"# **Preprocesamiento de datos**","28995b18":"# **Se cargan todas las librerias que se usar\u00e1n en el modelo**","314aca08":"> ### Se hace one hot encoding para 'Pclass', 'Cabin', 'Embarked' y 'Title'","30df5e58":"> ### Modelo ***sin regularizaci\u00f3n***","7157ae07":"# **Entrenamiento del modelo**","d59bb009":"> ### Se agrega el 'title' como nueva variable de entrada\n\n> Como hay t\u00edtulos que se repiten muy pocas veces, se proceder\u00e1 a agrupar esos que no se repiten tantas veces en los que m\u00e1s se repiten:\n\n> * 'Rev', 'Dr', 'Capt', 'Col', 'Major' **-> 'Mr'**\n> * 'Mlle', 'Ms' **-> 'Miss'**\n> * 'Mme' **-> 'Mrs'**\n> * 'Lady', 'Sir', 'Countess', 'Don', 'Dona', 'Jonkheer' **-> 'Royal'**\n\n> Se cambian los valores strings a num\u00e9ricos poder pasarlos por el modelo y se llenan otros datos vacios en el dataset\n\n> Se agrega una nueva variable de entrada 'KinfolkNumber' para definir el n\u00famero de parientes en el barco (incluye el pasajero)","a1eaf2bd":"> ### Se combinan el training set y el test set para preprocesar la data de los dos al mismo tiempo. Posteriormente se volver\u00e1n a separar","ed091c4b":"> ### Se normalizan los inputs X","d3237a75":"> Se verifican shapes y cantidades de observaciones de los dataset","a60290c5":"> ### Funci\u00f3n auxiliar","61f6d37f":"> ### Se separa una porci\u00f3n del training set del dev set (20%)"}}