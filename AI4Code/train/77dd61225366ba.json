{"cell_type":{"b1f3f313":"code","44516c62":"code","01f0da3c":"code","bd599f38":"code","24ff5f6b":"code","865dca25":"code","ac1494a1":"code","543c470d":"code","e0b2fc90":"code","1d717d23":"code","008a64df":"code","fe5f27a0":"code","3188717e":"code","839eb427":"code","cdbbdd19":"code","7be65504":"code","81d1c205":"markdown","1a5949ae":"markdown","fb902478":"markdown","9ceb7037":"markdown","bea17bc7":"markdown","e7381cb8":"markdown","1fbaebc6":"markdown","45a102d9":"markdown","2b0b8980":"markdown"},"source":{"b1f3f313":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV","44516c62":"TRAIN_PATH = \"..\/input\/titanic\/train.csv\"\nTARGET = \"Survived\"\nSELECT_TWO_COL = ['Sex_female', 'Sex_male']","01f0da3c":"train = pd.read_csv(TRAIN_PATH)","bd599f38":"#1. delete unnecessary columns\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch']\ntrain = train.drop(drop_elements, axis = 1)\n\n#2.find null data and fill new data \ndef checkNull_fillData(df):\n    for col in df.columns:\n        if len(df.loc[df[col].isnull() == True]) != 0:\n            if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n                df.loc[df[col].isnull() == True,col] = df[col].mean()\n            else:\n                df.loc[df[col].isnull() == True,col] = df[col].mode()[0]\n                \ncheckNull_fillData(train)\n\n#3.one hot encoding \nstr_list = [] \nnum_list = []\nfor colname, colvalue in train.iteritems():\n    if type(colvalue[1]) == str:\n        str_list.append(colname)\n    else:\n        num_list.append(colname)\n        \ntrain = pd.get_dummies(train, columns=str_list)","24ff5f6b":"y = train[TARGET]\nX = train.drop([TARGET],axis=1)","865dca25":"train.columns","ac1494a1":"X_2d = X[SELECT_TWO_COL]\ny_2d = y","543c470d":"scaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_2d = scaler.fit_transform(X_2d)","e0b2fc90":"C_range = np.logspace(-2, 10, 13)\ngamma_range = np.logspace(-9, 3, 13)\nparam_grid = dict(gamma=gamma_range, C=C_range)\ncv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\ngrid.fit(X_2d, y_2d)","1d717d23":"grid.best_params_","008a64df":"grid.best_score_","fe5f27a0":"grid.cv_results_.keys()","3188717e":"grid.cv_results_[\"param_C\"].data","839eb427":"grid.cv_results_[\"param_gamma\"].data","cdbbdd19":"C_2d_range = [1e-2, 1, 1e2]\ngamma_2d_range = [1e-1, 1, 1e1]\nclassifiers = []\nfor C in C_2d_range:\n    for gamma in gamma_2d_range:\n        clf = SVC(C=C, gamma=gamma)\n        clf.fit(X_2d, y_2d)\n        classifiers.append((C, gamma, clf))","7be65504":"plt.figure(figsize=(8, 6))\nxx, yy = np.meshgrid(np.linspace(-3, 3, 200), np.linspace(-3, 3, 200))\nfor (k, (C, gamma, clf)) in enumerate(classifiers):\n    # evaluate decision function in a grid\n    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    # visualize decision function for these parameters\n    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n    plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)), size=\"medium\")\n\n    # visualize parameter's effect on decision function\n    plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y_2d, cmap=plt.cm.RdBu_r, edgecolors=\"k\")\n    plt.xticks(())\n    plt.yticks(())\n    plt.axis(\"tight\")","81d1c205":"# select two column","1a5949ae":"# load data","fb902478":"# data scaling","9ceb7037":"# decision_function","bea17bc7":"# gamma parameter \n\ndefines how far the influence of a single training example reaches, \n\n### **with low values meaning \u2018far\u2019** \n\nand high values meaning \u2018close\u2019.\n\n# C parameter\nFor larger values of C, a smaller margin will be accepted \n\nif the decision function is better at classifying all training points correctly. \n\n### **A lower C will encourage a larger margin**, \n\ntherefore a simpler decision function, at the cost of training accuracy","e7381cb8":"# preprocess","1fbaebc6":"# global variable","45a102d9":"# split data(input data and target data)","2b0b8980":"# get best param ( C and gamma)"}}