{"cell_type":{"bf2fe160":"code","eb79e8e5":"code","05539843":"code","40960463":"code","bd8415f8":"code","65fb83c6":"code","5e9d09d5":"code","8335b166":"code","b7b860c0":"code","6bee49e1":"code","86403090":"code","b670b00b":"code","5073fde6":"code","54e462f9":"code","e9f43b37":"code","4f2501d1":"code","3b08146c":"code","7d00159e":"code","0af98b20":"code","f28bfb20":"code","fbd5aed2":"code","20a37cef":"code","6504a374":"code","1ce2bbb2":"code","202cf5c0":"code","10d471e6":"code","13d1b408":"code","aece7fc5":"code","033140d0":"code","e1862e4d":"code","dd4169c6":"code","4db8195f":"code","d285e491":"code","761705b8":"code","17e97bbc":"code","2f48261b":"code","b5742246":"code","d7405ac2":"code","3c64a590":"code","3a8421b6":"code","81dfdd2b":"code","ebc2bbc9":"code","7c418ffa":"code","a9530b25":"code","51d560c7":"code","6ebfd721":"code","f4ce0fa1":"code","82a73222":"code","ef2fb7cf":"code","03c502b4":"code","8fde00eb":"code","46def53e":"code","1ec48e0e":"code","09c7aaf4":"code","5c28ead3":"code","3cef16e4":"code","759f6b14":"code","e15d26ca":"code","0065e2b4":"code","92b63d30":"code","f6880f4c":"code","e20a8cea":"code","ecbbeabf":"code","b7e5fb3d":"code","df7719cc":"code","44f4c7d1":"code","0f6e3311":"code","b7fe0666":"code","db432e73":"code","db1e112b":"code","43f9b631":"code","9437ce55":"code","c2cde61c":"code","99fa9132":"code","3056491c":"code","ba98ea96":"code","e926ea8a":"code","e8fb81d3":"code","e292aaf5":"code","7fcb4e04":"code","4023f297":"code","6e11c7e6":"code","5e794ea0":"code","079b8639":"code","200236ea":"code","e009e8d5":"code","fc723ca3":"code","9121d266":"code","6a4c6155":"code","e5c66ae4":"code","152261ac":"markdown","0dea79ea":"markdown","4bd85138":"markdown","4c387759":"markdown","7bda2da1":"markdown","40a7d4d5":"markdown","bbfd1964":"markdown","28444944":"markdown","ca5a6ebe":"markdown","0c870045":"markdown","131a6510":"markdown","ca0823ef":"markdown","9d677386":"markdown","f183977c":"markdown","deff92ef":"markdown","54aa242b":"markdown","0ca1b4c4":"markdown","098c61d9":"markdown","b21e7df3":"markdown","b6efb370":"markdown","9a9ba15a":"markdown","12be0cb4":"markdown","ea615810":"markdown","9d9cd0e2":"markdown","a9d4d44c":"markdown","0487decf":"markdown","54fe5df2":"markdown","3c8d4fc6":"markdown","16a2bd0a":"markdown","2cf40354":"markdown","babb6975":"markdown","81b2ea84":"markdown","a9c020c4":"markdown","2960e643":"markdown","35afc3ff":"markdown","6a714de0":"markdown","c3012947":"markdown","36020387":"markdown","9fb12e53":"markdown","7eeea4d3":"markdown","ab37995b":"markdown","737f2283":"markdown","ad7b7c79":"markdown","649eb936":"markdown","8f23459e":"markdown","3f68ae57":"markdown","3e370b8a":"markdown","96a2ec3d":"markdown","05f11aef":"markdown"},"source":{"bf2fe160":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb79e8e5":"from datetime import datetime\nfrom dateutil.parser import parse\nfrom dateutil.relativedelta import relativedelta\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","05539843":"data_init = pd.read_csv(\"\/kaggle\/input\/nba2k20-player-dataset\/nba2k20-full.csv\")\ndata_init.head()","40960463":"#Checking the size of the dataset \ndata_init.shape","bd8415f8":"# Data Types\ndata_init.dtypes","65fb83c6":"data = data_init.copy()\ndata.head()","5e9d09d5":"data[\"draft_round\"].value_counts() # We have to replace the \"Undrafted\" to 0","8335b166":"plt.figure(figsize = (20,8))\nsns.countplot(data = data.sort_values(by = \"draft_peak\", ascending = True) , x = \"draft_peak\")\nplt.xticks(rotation = 90)\n# We have to replace the \"Undrafted\" to 0\nplt.show()","b7b860c0":"data[\"college\"].value_counts()","6bee49e1":"plt.figure(figsize = (25,10))\ndata[\"college\"].value_counts(ascending = False).plot(kind = 'bar')\nplt.show()","86403090":"plt.figure(figsize = (15,8))\nplt.title(\"Country Distribution\", fontsize = 13)\ndata[\"country\"].value_counts(ascending = False).plot(kind = 'bar')\nplt.show()","b670b00b":"data[\"draft_round\"].value_counts()","5073fde6":"def weight_to_kg(col):\n    start = col.find(\"\/\")+1\n    end = col.find(\"kg\")\n    return float(col[start:end])\n\ndef height_to_mts(col):\n    start = col.find(\"\/\")+1\n    end = len(col)\n    return float(col[start:end])\n","54e462f9":"# Transformer\ncols_to_drop = [\"full_name\", \"jersey\", \"height\", \"weight\", \"draft_peak\", \"salary\", \"b_day\", \"draft_year\"]\nclass Attribs_transformer(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bmi = True):\n        self.add_bmi = add_bmi\n    def fit(self, X, y = None):\n        return self\n    def transform(self, X, y = None):\n        X[\"weight_in_kg\"] = X[\"weight\"].transform(lambda x: weight_to_kg(x))\n        X[\"height_in_mts\"] = X[\"height\"].transform(lambda x: height_to_mts(x))\n        X[\"salary_amount\"] = X[\"salary\"].transform(lambda x: float(x[1:]))\n        X[\"draft_round\"] = X[\"draft_round\"].transform([lambda x: 0 if x == \"Undrafted\" else x]).astype(int)\n        X[\"draft_pick\"] = X[\"draft_peak\"].transform([lambda x: 0 if x == \"Undrafted\" else x]).astype(int)\n        X[\"age\"] = X[\"b_day\"].transform(lambda x: relativedelta(datetime.today(),parse(x)).years)\n        X[\"years_from_draft_year\"] = X[\"draft_year\"].transform(lambda x: datetime.today().year - x)\n        if self.add_bmi:\n            X[\"bmi\"] = X[\"weight_in_kg\"] \/ X[\"height_in_mts\"]**2\n        #X.drop(cols_to_drop, axis = 1, inplace = True)\n        return X","e9f43b37":"columns_transformer = Attribs_transformer()\ndata_transformed = columns_transformer.transform(data)\ndata_transformed.head()","4f2501d1":"#Checking for Missing Values\ndata_transformed.isna().sum().plot(kind = 'barh')\nplt.show()","3b08146c":"df = data_transformed.copy() ","7d00159e":"df_sal_by_age = df[\"salary_amount\"].groupby(pd.cut(df[\"age\"], bins = 7)).median().reset_index()\nplt.figure(figsize = (10,6))\nsns.barplot(data = df_sal_by_age, x = \"age\", y = \"salary_amount\", palette = 'plasma_r')\nplt.xticks(rotation  = 90)\nplt.xlabel(\"Age bracket\", fontsize = 13)\nplt.ylabel(\"Salary\", fontsize = 13)\nplt.title(\"Age vs Salary\", fontsize = 14)\nplt.plot()","0af98b20":"df_sal_by_college = df[[\"salary_amount\", \"college\"]].groupby(\"college\").agg({\"salary_amount\": [\"median\", \"size\"]}).reset_index()\ndf_sal_by_college.columns = [\"college\", \"mean_salary\", \"number_players\"]\nplt.figure(figsize = (30,20))\nax1 = plt.subplot(2,1,1)\nsns.barplot(data = df_sal_by_college.sort_values(by = \"mean_salary\", ascending = False), x = \"college\", y = \"mean_salary\", palette = 'plasma_r')\nplt.xticks(rotation  = 90)\nplt.xlabel(\"College\", fontsize = 14)\nplt.ylabel(\"Salary\", fontsize = 14)\nplt.title(\"Salary and Number of players of different colleges\", fontsize = 16)\nax1.spines[['top','right']].set_visible(False)\nplt.plot()\nax2 = ax1.twinx()\nsns.lineplot(data = df_sal_by_college.sort_values(by = \"mean_salary\", ascending = False), x = \"college\", y = \"number_players\", palette = 'plasma_r', color = 'black', marker = \"o\")\nplt.xticks(rotation  = 90)\nplt.xlabel(\"College\", fontsize = 14)\nplt.ylabel(\"Number of Players\", fontsize = 14)\nax2.spines['top'].set_visible(False)\nplt.plot()","f28bfb20":"df[[\"salary_amount\", \"team\", \"country\"]].groupby([\"team\", \"country\"]).agg({\"salary_amount\": [\"median\", \"size\"]}).reset_index()","fbd5aed2":"df_sal_by_team = df[[\"salary_amount\", \"team\"]].groupby(\"team\").agg({\"salary_amount\": [\"median\", \"size\"]}).reset_index()\ndf_sal_by_team.columns = [\"team\", \"median_salary\", \"number_players\"]\nplt.figure(figsize = (30,20))\nax1 = plt.subplot(2,1,1)\nsns.barplot(data = df_sal_by_team.sort_values(by = \"median_salary\", ascending = False), x = \"team\", y = \"median_salary\", palette = 'plasma_r')\nplt.xticks(rotation  = 90)\nplt.xlabel(\"Team\", fontsize = 14)\nplt.ylabel(\"Salary\", fontsize = 14)\nplt.title(\"Salary and Number of players of different teams\", fontsize = 16)\nax1.spines[['top','right']].set_visible(False)\nplt.plot()\nax2 = ax1.twinx()\nsns.lineplot(data = df_sal_by_team.sort_values(by = \"median_salary\", ascending = False), x = \"team\", y = \"number_players\", palette = 'plasma_r', color = 'black', marker = \"o\")\nplt.xticks(rotation  = 90)\nplt.xlabel(\"Team\", fontsize = 14)\nplt.ylabel(\"Number of Players\", fontsize = 14)\nax2.spines['top'].set_visible(False)\nplt.plot()","20a37cef":"df_sal_by_country = df[[\"salary_amount\", \"country\"]].groupby(\"country\").median().reset_index()\nplt.figure(figsize = (20,8))\n#ax1 = plt.subplot(2,1,1)\nsns.barplot(data = df_sal_by_country.sort_values(by = \"salary_amount\", ascending = False), x = \"country\", y = \"salary_amount\", palette = 'plasma_r')\nplt.xticks(rotation  = 90)\nplt.xlabel(\"Country\", fontsize = 14)\nplt.ylabel(\"Salary\", fontsize = 14)\nplt.title(\"Salary by different Countries\", fontsize = 16)\nplt.show()","6504a374":"plt.figure(figsize = (8,6))\nsns.regplot(data = df, x = \"rating\", y = \"salary_amount\")\nplt.title(\"Regression Plot between Rating and Salary\", fontsize = 14)\nplt.show()","1ce2bbb2":"plt.figure(figsize = (8,6))\nax = sns.regplot(x = df[\"draft_pick\"], y = df[\"salary_amount\"])\nplt.title(\"Draft Picks vs Salary\")\nplt.show()","202cf5c0":"plt.figure(figsize = (8,6))\nax = sns.regplot(data = df, x = \"height_in_mts\", y = \"salary_amount\")\nplt.title(\"Regression Plot between height and Salary\", fontsize = 14)\nplt.show()","10d471e6":"plt.figure(figsize = (8,6))\nsns.regplot(data = df, x = \"bmi\", y = \"salary_amount\")\nplt.title(\"Regression Plot between bmi and Salary\", fontsize = 14)\nplt.show()","13d1b408":"sns.pairplot(df)\nplt.show()","aece7fc5":"# Checking the Correlation coefficient\nplt.figure(figsize = (10,8))\nsns.heatmap(df.corr(), annot = True, cmap = 'plasma_r')\nplt.show()","033140d0":"cat_columns = [\"team\", \"position\", \"country\", \"college\"]","e1862e4d":"# Let us check the Correlation between the categories and the Salary Amount\n# Team\ndf_cat = df[cat_columns].copy()\ndf_cat[\"salary_amount\"] = df[\"salary_amount\"].copy()\ndf_cat.head()\n#OHE \ndf_cat = pd.get_dummies(df_cat)\ndf_cat.head()","dd4169c6":"plt.figure(figsize = (30,8))\nax = df_cat.corr()[\"salary_amount\"].plot(kind = 'bar')\nplt.title(\"Correlation Coeffcients of Categorical Variable and Salary Amount\", fontsize = 14)\nplt.axhline(0.3, c = 'r', linestyle = '--')\nplt.axhline(-0.3, c = 'r', linestyle = '--')\nplt.show()","4db8195f":"data_init.head()\n# data_init.columns","d285e491":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(data_init, test_size = 0.2, random_state = 1)\ndf_train.shape, df_test.shape","761705b8":"df_train.isna().sum()[df_train.isna().sum()>0]","17e97bbc":"## Importing Useful Libraries\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","2f48261b":"df_train_transformed = Attribs_transformer().transform(df_train)\ndf_train_transformed","b5742246":"df_train_transformed = df_train_transformed.drop(cols_to_drop, axis = 1)\ndf_train_transformed","d7405ac2":"num_cols = [\"rating\", \"draft_round\",\"weight_in_kg\", \"height_in_mts\", \"draft_pick\", \"age\", \"years_from_draft_year\"]\ncat_cols = [\"team\", \"position\", \"country\"]","3c64a590":"df_train_transformed.fillna(\"Unknown\", axis = 1, inplace = True)","3a8421b6":"X_train_df = df_train_transformed.drop(\"salary_amount\", axis = 1)\ny_train_df = df_train_transformed[\"salary_amount\"].copy()\n","81dfdd2b":"X_train_df.head()","ebc2bbc9":"num_pipeline = Pipeline([ \n                          #('transformer', Attribs_transformer()),\n                          ('imputer', SimpleImputer(strategy = 'mean')),\n                          ('scaler', StandardScaler())])\n\n# [\"rating\",\"team\", ]\n# transformed = transformer_pipeline.fit_transform(df_train)\n# transformed","7c418ffa":"ohe = OneHotEncoder(sparse = False)\nohe.fit(df_train_transformed[cat_cols])","a9530b25":"total_cols_train = [*num_cols,*list(ohe.get_feature_names(cat_cols))]\n","51d560c7":"X_train_df.head()","6ebfd721":"full_pipeline = ColumnTransformer([\n                        ('num',num_pipeline, num_cols),\n                        ('cat',ohe, cat_cols)\n                        \n], remainder = 'drop')\n\ndf_train_final  = full_pipeline.fit_transform(X_train_df)\ndf_train_final.shape","f4ce0fa1":"df_train_final_df = pd.DataFrame(df_train_final, columns = total_cols_train) \ndf_train_final_df.head()","82a73222":"## Fitting a Regression on the training dataset\nX_train = df_train_final\ny_train = y_train_df","ef2fb7cf":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\n#X_train = df_train_final_df[[\"rating\", \"draft_pick\", \"years_from_draft_year\"]].values\n#X_train = df_train_final_df[[\"rating\", \"age\",\"draft_pick\", \"years_from_draft_year\"]].values\n\nX_train = df_train_final_df.values\ny_train = y_train_df.values\n\nX_train.shape, y_train.shape","03c502b4":"lr.fit(X_train, y_train)\ny_train_pred = lr.predict(X_train)\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nprint(\"R Squared : {}\".format(r2_score(y_train, y_train_pred)))","8fde00eb":"import statsmodels.api as sm\nX_train = sm.add_constant(X_train, prepend = False)\nX_train\n\nmodel = sm.OLS(y_train, X_train, hasconst = True)\nresults = model.fit()\n#print(results.summary())\nprint(results.summary(xname = list(df_train_final_df.columns)+['constant']))","46def53e":"# removing \"country\"columns\n\ntraincols = [i for i in list(df_train_final_df.columns) if i.find(\"country\")==-1]\nX_train = df_train_final_df[traincols].values\nX_train = sm.add_constant(X_train, prepend = False)\n\nmodel = sm.OLS(y_train, X_train, hasconst = True)\nresults = model.fit()\n#print(results.summary())\nprint(results.summary(xname = traincols + ['constant']))","1ec48e0e":"# removing country and team columns\n\ntraincols = [i for i in list(df_train_final_df.columns) if i.find(\"country\")==-1 and i.find(\"team\")==-1 and i.find(\"position\")==-1 ]\nX_train = df_train_final_df[traincols].values\nX_train = sm.add_constant(X_train, prepend = False)\n\nmodel = sm.OLS(y_train, X_train, hasconst = True)\nresults = model.fit()\n#print(results.summary())\nprint(results.summary(xname = traincols + ['constant']))","09c7aaf4":"# Considering only the coolumns with p-values < 0.05\n\ntraincols = [\"age\", \"rating\", \\\n             \"position_F\", \"position_G\",\"position_C\", \\\n             \"team_Charlotte Hornets\",\"team_Houston Rockets\",\"team_Los Angeles Lakers\", \"team_Milwaukee Bucks\",\"team_Miami Heat\",\"team_Golden State Warriors\",\\\n             \"country_Finland\", \"country_France\",\"country_New Zealand\", \"country_Puerto Rico\", \"country_The Bahamas\"]\nX_train = df_train_final_df[traincols].values\nX_train = sm.add_constant(X_train, prepend = False)\n\nmodel = sm.OLS(y_train, X_train, hasconst = True)\nresults = model.fit()\n#print(results.summary())\nprint(results.summary(xname = traincols + ['constant']))","5c28ead3":"residuals = y_train - y_train_pred\nresiduals.shape\nprint(\"Mean of Residuals :{}\".format(residuals.mean()))","3cef16e4":"# Plotting the residuals\nax = sns.displot(residuals)\nplt.show()","759f6b14":"plt.figure(figsize = (8,6))\nax = plt.scatter(residuals, y_train_pred)\nplt.xlabel(\"Predicted Values\")\nplt.ylabel(\"Residual\")\nplt.axhline(residuals.mean(), c = 'r', linestyle = '--')\nplt.title(\"Homoscedasticity check - Residual vs Predited Values\", fontsize = 14)\nplt.show()","e15d26ca":"## Checking for homoskedasticity staistically\nfrom statsmodels.stats.diagnostic import het_goldfeldquandt\ncheck1 = het_goldfeldquandt(residuals, X_train)\ncheck1","0065e2b4":"sns.lineplot(residuals, y_train_pred)\nplt.show()","92b63d30":"# y_train_df","f6880f4c":"# df_test","e20a8cea":"# df_test_final_df.head()","ecbbeabf":"## Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X_train, y_train)\n\ny_train_pred = lr.predict(X_train)\n\nfrom sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_train, y_train_pred)\nmae = mean_absolute_error(y_train, y_train_pred)\n\nscore = np.sqrt(mse)\nprint(f' Training MSE score :',round(score,1))\nprint(f' Training MAE score :',round(mae,1))","b7e5fb3d":"# Fit Regression using Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor\ndct = DecisionTreeRegressor()\ndct.fit(X_train, y_train)\ny_train_pred_dct = dct.predict(X_train)\nprint(f'MSE of Decision Tree Regressor : ', np.sqrt(mean_squared_error(y_train, y_train_pred_dct)))","df7719cc":"\"\"\"We will keep using this code to create the X_train and y_train for new models\"\"\"\nX_train = df_train_final_df.values\ny_train = y_train_df.values\nX_train.shape, y_train.shape","44f4c7d1":"# Let us try to break the training data into Training + Validation to see how good the Decision Tree performs from Variance standpoint\n\nX_train.shape, y_train.shape\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\nX_train.shape, X_val.shape\ndct.fit(X_train, y_train)\ny_pred_val = dct.predict(X_val)\n\nmse = mean_squared_error(y_val, y_pred_val)\nmae = mean_absolute_error(y_val, y_pred_val)\n\nscore = np.sqrt(mse)\nprint(f' Validation MSE score :',round(score,1))\nprint(f' Validation MAE score :',round(mae,1))","0f6e3311":"# Feature Importances\nsorted(zip(dct.feature_importances_,df_train_final_df.columns), reverse = True)","b7fe0666":"## This is clearly Overfitting, but we can try to tune its hyperparamters to see if it turns out to be a not-so-strict model\nfeatures = ['rating', 'years_from_draft_year', \"weight_in_kg\",\"age\", \"height_in_mts\", \"draft_pick\", 'position_F','team_Milwaukee Bucks', 'team_Sacramento Kings' ]\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\nX_train.shape, y_train.shape\n\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\nX_train.shape, X_val.shape\n\n# Lets us fit a Decision Tree model to the a little less res restrictive model\n#dct = DecisionTreeRegressor(max_leaf_nodes =10,min_samples_split = 50, max_depth = 5 )\ndct = DecisionTreeRegressor(random_state = 10)\ndct.fit(X_train, y_train)\ny_pred_val_dct = dct.predict(X_val)\nmse = mean_squared_error(y_val, y_pred_val_dct)\nmae = mean_absolute_error(y_val, y_pred_val_dct)\n\nscore = np.sqrt(mse)\nprint(f' Validation MSE score for Decision Tree with Selected Features:',round(score,1))\nprint(f' Validation MAE score for Decision Tree with Selected Features:',round(mae,1))","db432e73":"\"\"\" Training and Validation for Polynomial Regression\"\"\"\n\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_features = PolynomialFeatures(degree = 2, include_bias = False)\n\nX_train_poly = poly_features.fit_transform(df_train_final_df[features])\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_poly, y_train, test_size = 0.2, random_state = 10)\n\nlr.fit(X_train, y_train)\n\ny_poly_train_pred = lr.predict(X_train)\n\nmse = mean_squared_error(y_train, y_poly_train_pred)\nmae = mean_absolute_error(y_train, y_poly_train_pred)\n\nscore = np.sqrt(mse)\nprint(f' Training MSE score :',round(score,1))\nprint(f' Training MAE score :',round(mae,1))\n\n\ny_poly_val_pred = lr.predict(X_val)\n\nmse = mean_squared_error(y_val, y_poly_val_pred)\nmae = mean_absolute_error(y_val, y_poly_val_pred)\n\nscore = np.sqrt(mse)\nprint(f' Validation MSE score :',round(score,1))\nprint(f' Validation MAE score :',round(mae,1))","db1e112b":"sns.displot(y_poly_val_pred - y_val, bins = 10)\nplt.title(\"Validation vs Actual Values\", fontsize = 14)\nplt.xlabel(\"Error in Salary\")\nplt.show()","43f9b631":"\n\"\"\" Training and Validation for Decision Tree Regression\"\"\"\n\nfrom sklearn.tree import DecisionTreeRegressor  \n\ndct= DecisionTreeRegressor()\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\ndct.fit(X_train, y_train)\n\ny_dct_train_pred = dct.predict(X_train)\n\nmse = mean_squared_error(y_train, y_dct_train_pred)\nmae = mean_absolute_error(y_train, y_dct_train_pred)\n\nscore = np.sqrt(mse)\nprint(f' Training MSE score :',round(score,1))\nprint(f' Training MAE score :',round(mae,1))\n\n\ny_dct_val_pred = dct.predict(X_val)\n\nmse = mean_squared_error(y_val, y_dct_val_pred)\nmae = mean_absolute_error(y_val, y_dct_val_pred)\n\nscore = np.sqrt(mse)\nprint(f' Validation MSE score :',round(score,1))\nprint(f' Validation MAE score :',round(mae,1))","9437ce55":"sns.displot(y_dct_val_pred - y_val, bins = 10)\nplt.title(\"Validation vs Actual Values\", fontsize = 14)\nplt.xlabel(\"Error in Salary\")\nplt.show()","c2cde61c":"## Using Bagging Strategy with Linear Regression\nfrom sklearn.ensemble import BaggingRegressor\n\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_train_linreg = lin_reg.predict(X_train)\ny_pred_val_linreg = lin_reg.predict(X_val)\n\nprint(f'Training MSE with Linear Regression: ', np.sqrt(mean_squared_error(y_pred_train_linreg, y_train)))\nprint(f'Validation MSE with Linear Regression: ', np.sqrt(mean_squared_error(y_pred_val_linreg, y_val)))\nprint(f'Training MAE with Linear Regression : ', mean_absolute_error(y_pred_train_linreg, y_train))\nprint(f'Validation MAE with Linear Regression : ', mean_absolute_error(y_pred_val_linreg, y_val))\n\nbag = BaggingRegressor(LinearRegression(), n_estimators=500, max_samples = 0.9, bootstrap = True, n_jobs = -1)\n      \nbag.fit(X_train, y_train)\ny_train_bag_pred = bag.predict(X_train)\ny_val_bag_pred = bag.predict(X_val)\n\nprint(\"\\n************************************************************\\n\")\nprint(f'Training MSE with Bagging: ', np.sqrt(mean_squared_error(y_train_bag_pred, y_train)))\nprint(f'Validation MSE with Bagging: ', np.sqrt(mean_squared_error(y_val_bag_pred, y_val)))     \nprint(f'Training MAE with Bagging: ',mean_absolute_error(y_pred_train_linreg, y_train))\nprint(f'Validation MAE with Linear Regression : ', mean_absolute_error(y_pred_val_linreg, y_val))\n\n","99fa9132":"sns.displot(y_val_bag_pred - y_val, bins = 10)\nplt.title(\"Validation vs Actual Values\", fontsize = 14)\nplt.xlabel(\"Error in Salary\")\nplt.show()","3056491c":"## Using Bagging Strategy with Decision Tree Regression - Same as Random Forest Regressor\nfrom sklearn.ensemble import BaggingRegressor\n\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\ndtree = DecisionTreeRegressor(random_state = 10)\ndtree.fit(X_train, y_train)\n\ny_pred_train_dtree = dtree.predict(X_train)\ny_pred_val_dtree = dtree.predict(X_val)\n\nprint(f'Training MSE with Decision Tree Regression: ', np.sqrt(mean_squared_error(y_pred_train_dtree, y_train)))\nprint(f'Validation MSE with Decision Tree  Regression: ', np.sqrt(mean_squared_error(y_pred_val_dtree, y_val)))\nprint(f'Training MAE with Decision Tree : ', mean_absolute_error(y_pred_train_dtree, y_train))\nprint(f'Validation MAE with Decision Tree: ', mean_absolute_error(y_pred_val_dtree, y_val))\n\nbag = BaggingRegressor(DecisionTreeRegressor(random_state = 10), n_estimators=500, max_samples = 0.9, bootstrap = True, n_jobs = -1)\n      \nbag.fit(X_train, y_train)\ny_train_bag_pred = bag.predict(X_train)\ny_val_bag_pred = bag.predict(X_val)\n      \nprint(\"\\n************************************************************\\n\")\nprint(f'Training MSE with Bagging: ', np.sqrt(mean_squared_error(y_train_bag_pred, y_train)))\nprint(f'Validation MSE with Bagging: ', np.sqrt(mean_squared_error(y_val_bag_pred, y_val))) \nprint(f'Training MAE with Bagging: ', mean_absolute_error(y_train_bag_pred, y_train))\nprint(f'Validation MAE with Bagging: ', mean_absolute_error(y_val_bag_pred, y_val))","ba98ea96":"sns.displot(y_val_bag_pred - y_val, bins = 10)\nplt.title(\"Validation vs Actual Values\", fontsize = 14)\nplt.xlabel(\"Error in Salary\")\nplt.show()","e926ea8a":"\n\"\"\" Training and Validation for Random Forest Regressor\"\"\"\n\nfrom sklearn.ensemble import RandomForestRegressor  \n\nfrst= RandomForestRegressor(random_state = 10)\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\nfrst.fit(X_train, y_train)\n\ny_frst_train_pred = frst.predict(X_train)\n\nmse = mean_squared_error(y_train, y_frst_train_pred)\nmae = mean_absolute_error(y_train, y_frst_train_pred)\n\ny_frst_val_pred = frst.predict(X_val)\n\nmse = mean_squared_error(y_val, y_frst_val_pred)\nmae = mean_absolute_error(y_val, y_frst_val_pred)\n\nprint(f'Training MSE with Random Forest Regression: ', np.sqrt(mean_squared_error(y_train, y_frst_train_pred)))\nprint(f'Validation MSE with Random Forest  Regression: ', np.sqrt(mean_squared_error(y_val, y_frst_val_pred)))\nprint(f'Training MAE with Random Forest  Regression : ', mean_absolute_error(y_train, y_frst_train_pred))\nprint(f'Validation MAE with Random Forest  Regression : ', mean_absolute_error(y_val, y_frst_val_pred))","e8fb81d3":"sns.displot(y_frst_val_pred - y_val, bins = 10)\nplt.title(\"Validation vs Actual Values\", fontsize = 14)\nplt.xlabel(\"Error in Salary\")\nplt.show()","e292aaf5":"\"\"\"Predictions vs Actual for Random Forest\"\"\"\nplt.figure(figsize = (8,6))\nplt.scatter(y =y_val, x = np.linspace(0,len(y_val), len(y_val)), c = 'b', label = \"Actual from Validation\")\nplt.scatter(y =y_frst_val_pred, x = np.linspace(0,len(y_val), len(y_val)), c = 'r', label = \"Predicted from Validation\" )\nplt.grid(True)\nplt.axhline(np.mean(y_val), c = 'b', linestyle = \"--\")\nplt.axhline(np.mean(y_frst_val_pred), c = 'r', linestyle = \"--\")\nplt.xlabel(\"Index of observation\", fontsize = 13)\nplt.ylabel(\"Prediction \/ Actual\", fontsize = 13)\nplt.legend()\nplt.title(\"Prediction vs Actual in Validation Set\", fontsize = 14)\nplt.show()\n","7fcb4e04":"## GradientBoostingRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth = 1, learning_rate = 0.1, n_estimators = 150, random_state = 10)\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\ngbrt.fit(X_train, y_train)\n\ny_gbrt_train_pred = gbrt.predict(X_train)\n\ny_gbrt_val_pred = gbrt.predict(X_val)\n\nprint(f'Training MSE with GB Regression: ', np.sqrt(mean_squared_error(y_train, y_gbrt_train_pred)))\nprint(f'Validation MSE with GB  Regression: ', np.sqrt(mean_squared_error(y_val, y_gbrt_val_pred)))\nprint(f'Training MAE with GB Regression : ', mean_absolute_error(y_train, y_gbrt_train_pred))\nprint(f'Validation MAE with GB  Regression : ', mean_absolute_error(y_val, y_gbrt_val_pred))\n","4023f297":"plt.figure(figsize = (8,6))\nplt.scatter(y =y_val, x = np.linspace(0,len(y_val), len(y_val)), c = 'b', label = \"Actual from Validation\")\nplt.scatter(y =y_gbrt_val_pred, x = np.linspace(0,len(y_val), len(y_val)), c = 'r', label = \"Predicted from Validation\" )\nplt.grid(True)\nplt.axhline(np.mean(y_val), c = 'b', linestyle = \"--\")\nplt.axhline(np.mean(y_gbrt_val_pred), c = 'r', linestyle = \"--\")\nplt.xlabel(\"Index of observation\", fontsize = 13)\nplt.ylabel(\"Prediction \/ Actual\", fontsize = 13)\nplt.legend()\nplt.title(\"Prediction vs Actual in Validation Set\", fontsize = 14)\nplt.show()","6e11c7e6":"from sklearn.model_selection import GridSearchCV, cross_val_score\n","5e794ea0":"param_grid = [{ \"max_depth\" : [1,2,4,8],\n               \"n_estimators\" : [10,100,150,500,1000],\n               \"learning_rate\" : [0.01, 0.1, 1.0]}\n            ]\n\ngrid_search = GridSearchCV(GradientBoostingRegressor(random_state = 10), param_grid, cv = 5, scoring = \"neg_mean_squared_error\", n_jobs = -1, verbose = 5)\n\nX_train = df_train_final_df[features].values\ny_train = y_train_df.values\n\n# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 10)\n\ngrid_search.fit(X_train, y_train)","079b8639":"grid_search.best_estimator_, np.sqrt(-grid_search.best_score_)","200236ea":"cv_res = grid_search.cv_results_\n# for (params, scores) in zip(cv_res[\"params\"],cv_res[\"mean_test_score\"]):\n#     print(\"Params : \" , params, \" Scores : \", np.sqrt(-scores))","e009e8d5":"grid_res_df = pd.DataFrame(columns = [\"Params\", \"Score\"])\ngrid_res_df[\"Params\"] = [str(i) for i in cv_res[\"params\"]]\ngrid_res_df[\"Score\"] = [np.round(np.sqrt(-i),2) for i in cv_res[\"mean_test_score\"]]\n\nplt.figure(figsize = (20,15))\nsns.barplot(data = grid_res_df.sort_values(by = \"Score\", ascending = True), x = \"Score\", y = \"Params\")\nplt.title(\"Param-wise Score\", fontsize = 14)\nplt.show()","fc723ca3":"best_model = grid_search.best_estimator_\n\nbest_model.fit(X_train, y_train)\n","9121d266":"# Preparation of X_test and Y_test\ndf_test_transformed = Attribs_transformer().transform(df_test)\ndf_test_transformed.drop(cols_to_drop, axis = 1, inplace = True)\ndf_test_transformed.fillna(\"Unknown\", axis = 1, inplace = True)\nohe.fit(df_test_transformed[cat_cols])\ntotal_test_cols = [*num_cols,*list(ohe.get_feature_names(cat_cols))]\n\nX_test_df = df_test_transformed.drop(\"salary_amount\", axis = 1)\ny_test_df = df_test_transformed[\"salary_amount\"].copy()\n\ndf_test_final  = full_pipeline.fit_transform(X_test_df)\ndf_test_final_df = pd.DataFrame(df_test_final, columns = total_test_cols) \n\n\nmissing_cols = set(df_train_final_df.columns) - set(df_test_final_df)\nfor c in missing_cols:\n    df_test_final_df[c] = 0\ndf_test_final_df = df_test_final_df[total_cols_train]\n#df_test_final_df.head()\n\n\nX_test = df_test_final_df[features].values\ny_test = y_test_df.values","6a4c6155":"y_test_pred = best_model.predict(X_test)\n\nprint(\"Score in  the test set is : \", np.sqrt(mean_squared_error(y_test,  y_test_pred)))\nprint(\"R2 Score of the Test Set is : \", r2_score(y_test,  y_test_pred))","e5c66ae4":"from scipy import stats\nconfidence = 0.95\nsquared_errors = (y_test - y_test_pred)**2 \nnp.sqrt(stats.t.interval(confidence, len(squared_errors)-1, loc = squared_errors.mean(), scale = stats.sem(squared_errors)))","152261ac":"The Miami Heat team offers maximum salary to its players. ","0dea79ea":"The fields \"college\" and \"team\" has missing values. We willl later impute this using KNNImputer","4bd85138":"### 5. Autocorrelation in the residuals\n","4c387759":"### Do younger players earn more?","7bda2da1":"The baseline Decision Tree is highly overfitting the data. THe Mean error on the training data is 0 but the error on unseen data(X_val) is $7M.","40a7d4d5":"**Observations**\n* The Salary is linearly related with Rating. Higher the rating, better the salary.\n* The Salary is linearly related with Years from Draft Year. Salary increases as the experience of player increases.\n* There is also a negative relationship between Salary and Draft Pick. The graph looks lika a loarithmic curve. The Salary is higher for players with early picks. ","bbfd1964":"## Hypothesis:\n* Do younger players earn more?\n* Which college has a good earning probability?\n* Players of which country earn the most?\n* Does rating decide the Salary?\n* How does the Draft Pick is associated with Salary?\n* Do tall players earn more salary?\n* What is the height to weight ratio of players is a good indicator of salary?\n\nIn all the above hypothesis, we will check for the median salary, to avoid effects of outliers","28444944":"The Error terms are all over the place, so rejecting this for baseline consideration","ca5a6ebe":"## Data Exploration","0c870045":"## Checking for Assumptions of Linear Regression\n0. Checking with SM\n1. Linearity - Visual\n2. Mean of Residuals - \n3. Check for Homoscedasticity - Durbin Watson test\n4. Check for Normality of error terms\/residuals\n5. No autocorrelation of residuals - \n6. No perfect multicollinearity - Condition Number\n \n### Linearity\n* From the pair plot, we observed that the there is only linear relationship with Rating and the years from Draft Year\n\nFor the other assumptions, we have to fit the regression model first.\n","131a6510":"## Model Creation\n### Before entering the model creation phase, let us create a text dataset from the original dataset(the first DF when we collected the data). We create a test dataset and forget it for now untill we have developed a model to test on the test dataset. The data preparation pipeline will be fitted on the test dataset befroe the prediction","ca0823ef":"## Data Collection","9d677386":"It is very evident that none of the categorical features are correlated with the target value i.e. Salary Amount. Any correlation coefficients > 0.3 or < -0.3 could be considered","f183977c":"### Which college has a good earning probability?","deff92ef":"Since for most fields the does not vary linearly with the target variable i.e. the salary_amount, no wonder a simple linear regression will fit poorly.\nSince it is underfitting the data, we will have to user a more complex model","54aa242b":"From the look of it, we can make out below observations about the data:\n\n* There are fields in the dataset that are not relevant for predicting the salary of a player, like - Full Name, Jersey.\n* College - This field contains missing values that may not necessairily mean that the player is an undergrad. It may also mean a true missing information. However, we can use this column for hypothesis testing to see if the salary depends on the college \n* From the Date of birth, we can figure out the age of a player and use that for hypotheis testing. But the date format is MM\/DD\/YY format\n* The height and weights are present in both metric systems. We will have to use any one metric system and convert this into numerical colums intead of a string\n* Salary needs to be converted to Numeric\n* Draft round and Draft Pick are also strings. We must change them to Integer\n\nWe can skip the EDA for now and we will first do the data cleanup and transformation\n\n\nLet's get our hands dirty\n","0ca1b4c4":"### Players of which country earn the most?","098c61d9":"## Hyperparameter Tuning ","b21e7df3":"## Predicting in Test Set","b6efb370":"The Random Forest Regressor is consistent, meaning, the MSE in the validation is less than other models. There is scope to improve the model by hyper parameter tuning using cross validation","9a9ba15a":"Since the slope of the Regression line looks almost straight line parallel to X-axis, the Salary does not depend on the Height of the player.(Must be the Skill then. I wish if there was a metric present in the dataset for the skill)","12be0cb4":"The errors are spread out. So, the model may not be a good candidate for the baseline model","ea615810":"### Do tall players earn more salary?","9d9cd0e2":"We are getting a good Adjusted R2 score with the set of fields with controllable values for the assumptions like, Durbin Watson test score, COnd No. score","a9d4d44c":"### Does rating decide the Salary?","0487decf":"Players between 35 to 38 years have a higher probability of earning more than the others, probably beacause of the experience of the game.","54fe5df2":"1. Fill Missing Values\n2. OHE\n3. Scaling the values\n","3c8d4fc6":"### The Baseline Regressor","16a2bd0a":"## Importing Libraries","2cf40354":"Doesn't look like so. The slope of the regression line is parallel to BMI axis","babb6975":"## Data Cleaning and Transformation","81b2ea84":"### 3. HomoScedasticity\n\nTo check for homoscedasticity, we plot the residuals for each of the predicted values. The data is homoscedastic if the model has a constant variance along the regression line.","a9c020c4":"### 2. Mean of Residuals","2960e643":"### Considering the assumptions, the Linear Regression would not be the appropriate model. We need to work with non-linear models for the regression","35afc3ff":"## Which team offers better salaries?","6a714de0":"Dominican Republic,  Montenegro  and NewZealand players earn more salary than the others. ","c3012947":"### Does the BMI of a player explain the salary?","36020387":"**The salary is highly corelated with the ratings and age. Also, the years from Draft year is correlated with the Age.**","9fb12e53":"### Draft Pick Vs Salary","7eeea4d3":"Since the p-value is >0.05, we can reject the nul hypothesis. Hence, there exists hetero skdeascticity in the data.","ab37995b":"Looks like there is a relationship between thw Salary and the Draft Pick. Salary is higher for players with early picks. For the players chosen in the first 5 picks, the Salary is higher generally.\n\nThe Player who get picked on the later Picks draw less salary","737f2283":"## Data Preprocessing","ad7b7c79":"Overfitting!! - The Decision Tree Regressor has the tendecy to overfit the data most often sue to the affiliation towards the orthogonal bpundaries","649eb936":"**We will just mark the missing values as unknown**","8f23459e":"## Inference\n***Given the data and its volume, the chosen model is a Gradient Boosting Regressor that has a generalization error between 3095934.12 to 6496788 with 95% confidence,\nThe training and prediction were done in very less volumne of data***","3f68ae57":"## Generalization Error ","3e370b8a":"Even though Kentucky and Duke has almost 25 players each, the top earner is from Davidson college and with the 2nd highest salary from Arizona State.\nThere is only one player from each college in top 8 salaries.\n\nThe college does not contribute to salary, even though there are so many players.","96a2ec3d":"It is very evident from the regression plot that the Salary does depend on the Ratings. Higher the ratings, higer will be the salary","05f11aef":"### Checking the variation of the Salary with the numerical columns using a pair plot"}}