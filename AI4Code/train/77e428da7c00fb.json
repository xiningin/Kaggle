{"cell_type":{"5bbab5f6":"code","841ad8f7":"code","2c935db2":"code","f40892e3":"code","c8d2ed06":"code","9f669a06":"code","5904e6e5":"code","9c19a813":"code","4dd1a0b4":"code","53cf3ee2":"code","718200d0":"code","09a8dea0":"code","473d9c18":"markdown","d54f7c60":"markdown","9f5fc9a3":"markdown","92e27aa3":"markdown","b9b72d5b":"markdown","3431532a":"markdown","ea26093f":"markdown"},"source":{"5bbab5f6":"TPU = False\nDEBUG = False\nNAME= 'exp01-baseline'\n\nif TPU:\n    !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","841ad8f7":"import sys\nsys.path.append('..\/input\/iterativestratification\/iterative_stratification-0.1.6-py3-none-any.whl')\n\n!pip install pytorch_lightning\n!pip install git+https:\/\/github.com\/ildoonet\/pytorch-gradual-warmup-lr.git\n!pip install torchcontrib","2c935db2":"import pandas as pd\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import Adam\nimport torch.optim.lr_scheduler as lr_scheduler\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.loggers import WandbLogger\nfrom sklearn.compose import ColumnTransformer\n\nfrom warmup_scheduler import GradualWarmupScheduler\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nimport random\nimport wandb\nimport torchcontrib\nfrom torchcontrib.optim import SWA\nfrom tqdm.notebook import tqdm\nimport warnings\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nif TPU:\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pll\n    import torch_xla.distributed.xla_multiprocessing as xmp\n\nwarnings.simplefilter(\"ignore\", UserWarning)","f40892e3":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","c8d2ed06":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain['cp_time'] = train['cp_time'].astype('object')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntest['cp_time'] = test['cp_time'].astype('object')\ntargets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\n\ntarget_cols = [c for c in targets.columns if c not in ['sig_id']]","9f669a06":"cfg = dict(\n    seed = 42,\n    epochs = 1 if DEBUG else 10,\n    folds = 5,\n    num_features = train.shape[1]-4,\n    num_targets = targets.shape[1] - 1,\n    cat_feats_ohe = 7,\n    hidden_size = 512, \n    dropout = 0.3,\n    bs = 128,\n    init_lr = 1e-4,\n    lr_min = 1e-6,\n    warmup_epoch = 1,\n    warmup_multiplier = 10,\n    cosine_epoch = 49\n)\n\nseed_everything(cfg['seed'])","5904e6e5":"train = train.merge(targets, on = 'sig_id')\nFold = MultilabelStratifiedKFold(n_splits=cfg['folds'], random_state=cfg['seed'])\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train[target_cols])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\n\n\nprint(train.shape)","9c19a813":"class MoADataset(Dataset):\n    def __init__(self, df, mode = 'train'):\n        \n        self.cat_feats = df[:, :cfg['cat_feats_ohe']]\n        self.num_feats = df[:, cfg['cat_feats_ohe']:cfg['cat_feats_ohe']+cfg['num_features']]\n        self.mode = mode\n        \n        if self.mode=='train':\n            self.targets = df[:, -cfg['num_targets']:]\n        \n    def __len__(self):\n        \n        return len(self.num_feats)\n    \n    def __getitem__(self, idx):\n        \n        cat_feats = torch.tensor(self.cat_feats[idx], dtype = torch.long)\n        num_feats = torch.tensor(self.num_feats[idx], dtype = torch.float)\n        \n        if self.mode:\n            targets = torch.tensor(self.targets[idx], dtype = torch.float)\n        \n        if self.mode=='train':\n            return cat_feats, num_feats, targets\n        else:\n            return cat_feats, num_feats\n        ","4dd1a0b4":"class MoADataModule(pl.LightningDataModule):\n    def __init__(self, train, test, fold):\n        super(MoADataModule, self).__init__()\n        \n        self.data = train.iloc[:, 1:]\n        self.test = test.iloc[:, 1:]\n        self.fold = fold\n        \n    def setup(self, stage):\n        \n        if stage=='fit' or stage is None:\n            \n            train_X = self.data[self.data['fold']!=self.fold].reset_index(drop=True)\n            val_X = self.data[self.data['fold']==self.fold].reset_index(drop=True)\n            \n            train_X = train_X.drop('fold', 1)\n            val_X = val_X.drop('fold', 1)\n            \n            scaler = MinMaxScaler()\n            ohe = OneHotEncoder()\n            \n\n            ct = ColumnTransformer([('onehot', ohe, [0,1,2]), \n                                    ('minmax', scaler, [i+3 for i in range(cfg['num_features'])])])\n            \n            train_X = ct.fit_transform(train_X)\n            val_X = ct.transform(val_X)\n\n            \n            self.traindataset = MoADataset(train_X)\n            self.valdataset = MoADataset(val_X)\n            \n        if stage=='test':\n            \n            test = preprocessor.transform(self.test)\n            \n            self.testdataset = MoADataset(test, mode='test')\n            \n    def train_dataloader(self):\n        \n        return DataLoader(self.traindataset, batch_size = cfg['bs'], shuffle = True, num_workers =  4, pin_memory = True)\n    \n    def val_dataloader(self):\n        \n        return DataLoader(self.valdataset, batch_size = cfg['bs'], shuffle = False, num_workers =  4, pin_memory = True)\n    \n    def test_dataloader(self):\n        \n        return DataLoader(self.testdataset, batch_size = cfg['bs'], shuffle = False, num_workers =  4, pin_memory = True)\n            \n            ","53cf3ee2":"# Fix Warmup Bug\nclass GradualWarmupSchedulerV2(GradualWarmupScheduler):\n    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n    def get_lr(self):\n        if self.last_epoch > self.total_epoch:\n            if self.after_scheduler:\n                if not self.finished:\n                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n                    self.finished = True\n                return self.after_scheduler.get_lr()\n            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n        if self.multiplier == 1.0:\n            return [base_lr * (float(self.last_epoch) \/ self.total_epoch) for base_lr in self.base_lrs]\n        else:\n            return [base_lr * ((self.multiplier - 1.) * self.last_epoch \/ self.total_epoch + 1.) for base_lr in self.base_lrs]","718200d0":"class MoAModule(pl.LightningModule):\n    def __init__(self):\n        super(MoAModule, self).__init__()\n        \n        self.mlp = nn.Sequential(\n                          nn.Linear(cfg['num_features'], cfg['hidden_size']),\n                          nn.BatchNorm1d(cfg['hidden_size']),\n                          nn.Dropout(cfg['dropout']),\n                          nn.ReLU(),\n                          nn.Linear(cfg['hidden_size'], cfg['hidden_size']),\n                          nn.BatchNorm1d(cfg['hidden_size']),\n                          nn.Dropout(cfg['dropout']),\n                          nn.ReLU(),\n                          nn.Linear(cfg['hidden_size'], cfg['num_targets'])\n                          )\n    \n    def shared_step(self, batch, batch_nb):\n        \n        cat_feats, num_feats, targets = batch\n        logits = self.mlp(num_feats)\n        loss = self.loss_func(logits, targets)\n        \n        return loss\n    \n    def training_step(self, batch, batch_nb):\n        \n        loss = self.shared_step(batch, batch_nb)\n        result = pl.TrainResult(minimize = loss)\n        result.log('train loss', loss, on_epoch = True, prog_bar = True, logger = True)\n        \n        return result\n    \n    def validation_step(self, batch, batch_nb):\n        \n        loss = self.shared_step(batch, batch_nb)\n        result = pl.EvalResult(checkpoint_on = loss, early_stop_on = loss)\n        result.log('val loss', loss, on_epoch = True, prog_bar = True, logger = True)\n        \n        return result\n    \n    def test_step(self, batch, batch_nb):\n        \n        cat_feats, num_feats, targets = batch\n        logits = self.mlp(num_feats)\n        preds = logits.sigmoid()\n        \n        return preds\n    \n    def loss_func(self, pred, target):\n        return nn.BCEWithLogitsLoss()(pred,target)\n    \n    def configure_optimizers(self):\n        \n        optimizer = Adam(self.mlp.parameters(), lr = cfg['init_lr'])\n#         optimizer = SWA(base_opt, swa_start = cfg['swa_start'], swa_freq = cfg['swa_freq'], swa_lr = cfg['lr_min'])\n        \n        cosine = lr_scheduler.CosineAnnealingLR(optimizer, T_max = cfg['cosine_epoch'], eta_min = cfg['lr_min'])\n        scheduler = GradualWarmupSchedulerV2(optimizer, total_epoch = cfg['warmup_epoch'], multiplier = cfg['warmup_multiplier'], after_scheduler = cosine)\n        \n        return [optimizer], [scheduler]\n        ","09a8dea0":"wandblogger = WandbLogger(name = f'{NAME}', project = 'MoA')\nwandblogger.log_hyperparams(cfg)\n\nfor i in range(cfg['folds']):\n    data = MoADataModule(train, test, fold = i)\n    data.setup(stage='fit')\n    model = MoAModule()\n\n\n    if not TPU:\n        if DEBUG:\n            trainer = Trainer(gpus = 1, logger = wandblogger,\n                 limit_train_batches=5,\n                 limit_val_batches = 5,\n                    max_epochs = cfg['epochs'], default_root_dir = '\/kaggle\/working')\n        else:\n            trainer = Trainer(gpus = 1, logger = wandblogger,max_epochs = cfg['epochs'], default_root_dir = '\/kaggle\/working')\n    else:\n        if DEBUG:\n            trainer = Trainer(tpu_cores = 1, logger = wandblogger,\n                 limit_train_batches=5,\n                 limit_val_batches = 5,\n                    max_epochs = cfg['epochs'], default_root_dir = '\/kaggle\/working')\n        else:\n            trainer = Trainer(tpu_cores = 8, logger = wandblogger, default_root_dir = '\/kaggle\/working')\n            \n\n    wandblogger.watch(model, log = 'all')\n\n    trainer.fit(model, data)\n\n    trainer.save_checkpoint(f'{NAME}_fold{i}.pt')\n    wandb.save(f'{NAME}_fold{i}.pt')","473d9c18":"# LOAD AND STRATIFY","d54f7c60":"# Dataset","9f5fc9a3":"# LightningDataModule","92e27aa3":"# Trainer","b9b72d5b":"# LightningModule","3431532a":"# IMPORT","ea26093f":"# Pytorch Lightning\n\n\nIn this kernel, I use Pytorch Lightning for the whole pipeline. Pytorch Lightning is just reorganized pytorch code and on top of that, you get some extra benefits like TPU training, Distributed Data Parallel training, and many more with minimal change in code. The best part is, you dont need to learn completely new framework to use Lightning. You can look at their docusmentation for more info. https:\/\/github.com\/PyTorchLightning\/pytorch-lightning\n\n\nThe kernel has 6 sections:\n\n1. Import\n2. Load and Multilabel Stratify\n3. Create Dataset class\n4. Create LightningDataModule \n    * Select the fold to be used for validation\n    * Use sklearn ColumnTransformer for OneHot Encoding of categorical variables\n    * Use sklearn ColumnTransformer for MinMaxScaling of numerical variables\n    * Create instance of Dataset class for training and validation\n    * Define methods for train, val and test dataloaders\n5. Create LightningModule\n    * Define model\n    * Define Shared Step for training and validation\n    * Log results to wandb\n6. Trainer\n    * Use GPU or TPU for training\n    * Use Wandb\/Neptune\/Tensorboard for logging\n    * Set Epochs\n    * It can be used for debugging and many more things. Pls check the documentation\n\n\n# Wandb\n\nIn machine learning, it is very important to not only carry out many experiments, but to log and compare them systematically. Manual maintenance and creation of loss, metric,etc. plots is cumbersome. That is why, I use wandb. Wandb lets you log values on the fly. You can compare various experiments easily, write reports and much more. Check it out if you are interested. https:\/\/www.wandb.com\/ \n\n\nUse the cfg dictionary to change variable values and experiment further. All the best!\n\n# TPU \n\nThere is some bug in the TPU version. I am eager to see someone make it work. Set TPU=True to try training with TPU "}}