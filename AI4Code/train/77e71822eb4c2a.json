{"cell_type":{"d0cb770a":"code","e0b75606":"code","27e5bf62":"code","ede6f8a0":"code","f7c81f17":"code","e0bae652":"code","aef7953c":"code","b5a790e2":"code","b2d130cd":"code","332760d5":"code","1b81ecea":"code","7c120cbd":"code","a4b6c8d4":"code","e00755bf":"code","138ef591":"code","60aafb00":"code","19a6cf74":"code","ee8b48cb":"code","5abe29fb":"code","b84e429f":"code","39806909":"code","e0455ef2":"code","d228de6b":"code","5bc3866d":"code","8d75347a":"code","4ae769ee":"code","e870469a":"code","5381d2a9":"code","61d946fa":"code","bbda358a":"code","24bbe4a0":"code","0ffa6be0":"code","23df1171":"code","13910451":"code","634a1a3e":"code","af99c351":"code","d2773c0c":"code","68f26725":"code","d84462fe":"code","56a7404d":"code","98b1a4dd":"code","1f3ea444":"code","d4f38e90":"code","c2c3f103":"code","027277f2":"code","060c3cae":"code","fb0fbfdc":"code","16a95891":"code","6d890f6b":"code","365e2ae1":"code","fc4287c4":"code","ecf9f17f":"code","df219dc2":"code","d13b7e83":"code","f3cb9b7f":"code","ddc7cdb1":"code","54969929":"code","c5a1b94a":"code","34fb87df":"code","b18e5337":"code","c3d29c74":"code","e3e53b58":"code","94f376ef":"code","4c596fe5":"code","15c0f896":"code","1cf02dee":"code","9ce66d37":"code","246ec01a":"code","9b5104c9":"code","c831719e":"code","81de5924":"code","549394cb":"code","6a54e1f0":"code","933a9da4":"code","23ca0a15":"code","8c7a2545":"code","c7d03fe9":"code","e042e398":"code","b8a8132e":"code","d28fc6c5":"code","c056fd34":"code","4528fe9b":"code","dc81a9c8":"code","b5fe91ac":"code","c94cdde3":"code","2a77b24f":"code","e1e5dbed":"code","b2e1b7c7":"code","7af6c704":"code","6e18ef7e":"code","660f0a1e":"code","84e74d1a":"code","ef90fca4":"code","53dcfa75":"code","04c4f015":"code","1fc51fdd":"code","a52ce125":"code","9bee1ae7":"code","6d8ced53":"code","3e614e98":"code","12657a46":"code","9649bd4a":"code","d92e9ae7":"code","4563ba9f":"code","fd1f81ac":"markdown","ece5983a":"markdown","5416438e":"markdown","b320aed1":"markdown","5a21125d":"markdown","cdef6352":"markdown","6054b543":"markdown","7c11bddd":"markdown","3168b41c":"markdown","a00aae28":"markdown","789ed93e":"markdown","3cf2b81d":"markdown","715e68c3":"markdown","dee94132":"markdown","abd57f9e":"markdown","fbe13161":"markdown","e86f9ce2":"markdown","156da986":"markdown","cbc319bc":"markdown","7d608056":"markdown","abdb86c4":"markdown","839a5133":"markdown","4d2a3859":"markdown","a9c05826":"markdown","41203ba1":"markdown","7cd6f544":"markdown","fe28cad5":"markdown","16dd76bb":"markdown","9676ba9a":"markdown","64f655e5":"markdown","b3440ff4":"markdown","f6dd1b18":"markdown","d2694883":"markdown","5e0b9f5a":"markdown","dc2686c6":"markdown","d94c2d63":"markdown","310d2348":"markdown","79ddf854":"markdown","eb47bac0":"markdown","cea60fd6":"markdown","c8e8c12b":"markdown","bfa44d2c":"markdown","02f77470":"markdown","20556ec5":"markdown","76eb49b0":"markdown","2ebc4bf9":"markdown","55cb541f":"markdown","ed8c46f2":"markdown","418843ca":"markdown","a5abd69b":"markdown","482a4512":"markdown","69ed6c71":"markdown","1629667c":"markdown","f84d80c3":"markdown","3be3af60":"markdown","1be1984d":"markdown","e722e7ae":"markdown","c98b41f8":"markdown","20e42b3d":"markdown","885440bf":"markdown","e76b1c00":"markdown","1437b1a3":"markdown","611d7a18":"markdown","9626bdc3":"markdown","aebe3827":"markdown","a2cfa9c5":"markdown","9287aeb5":"markdown","a627af61":"markdown","faa283df":"markdown","c3e60c03":"markdown","673b1c77":"markdown","c05de7fb":"markdown","ce4fb7c6":"markdown","9af903c7":"markdown","8796ec73":"markdown","8286410d":"markdown","f7b05fd9":"markdown"},"source":{"d0cb770a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV, learning_curve, train_test_split\nfrom sklearn.linear_model import LinearRegression, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom joblib import dump, load\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning","e0b75606":"# define explicit data types\ndtypes = ({\n    'ID': int,\n    'MSSubClass': 'category',\n    'MSZoning': 'category',\n    'LotFrontage': float,\n    'LotArea': float,\n    'Street': 'category',\n    'Alley': 'category',\n    'LotShape': 'category',\n    'LandContour': 'category',\n    'Utilities': 'category',\n    'LotConfig': 'category',\n    'LandSlope': 'category',\n    'Neighborhood': 'category',\n    'Condition1': 'category',\n    'Condition2': 'category',\n    'BldgType': 'category',\n    'HouseStyle': 'category',\n    'OverallQual': int,\n    'OverallCond': int,\n    'YearBuilt': 'category',\n    'YearRemodAdd': 'category',\n    'RoofStyle': 'category',\n    'RoofMatl': 'category',\n    'Exterior1st': 'category',\n    'Exterior2nd': 'category',\n    'MasVnrType': 'category',\n    'MasVnrArea': float,\n    'ExterQual': 'category',\n    'ExterCond': 'category',\n    'Foundation': 'category',\n    'BsmtQual': 'category',\n    'BsmtCond': 'category',\n    'BsmtExposure': 'category',\n    'BsmtFinType1':'category',\n    'BsmtFinSF1': float,\n    'BsmtFinType2': 'category',\n    'BsmtFinSF2': float,\n    'BsmtUnfSF': float,\n    'TotalBsmtSF': float,\n    'Heating': 'category',\n    'HeatingQC': 'category',\n    'CentralAir': 'category',\n    'Electrical': 'category',\n    '1stFlrSF': float,\n    '2ndFlrSF': float,\n    'LowQualFinSF': float,\n    'GrLivArea': float,\n    'BsmtFullBath': float,\n    'BsmtHalfBath': float,\n    'FullBath': float,\n    'HalfBath': float,\n    'BedroomAbvGr': float,\n    'KitchenAbvGr': float,\n    'KitchenQual': 'category',\n    'TotRmsAbvGrd': float,\n    'Functional': 'category',\n    'Fireplaces': float,\n    'FireplaceQu': 'category',\n    'GarageType': 'category',\n    'GarageYrBlt': 'category',\n    'GarageFinish': 'category',\n    'GarageCars': float,\n    'GarageArea': float,\n    'GarageQual': 'category',\n    'GarageCond': 'category',\n    'PavedDrive': 'category',\n    'WoodDeckSF': float,\n    'OpenPorchSF': float,\n    'EnclosedPorch': float,\n    '3SsnPorch': float,\n    'ScreenPorch': float,\n    'PoolArea': float,\n    'PoolQC': 'category',\n    'Fence': 'category',\n    'MiscFeature': 'category',\n    'MiscVal': float,\n    'MoSold': 'category',\n    'YrSold': 'category',\n    'SaleType': 'category',\n    'SaleCondition': 'category',\n    'SalesPrice': float\n})","27e5bf62":"# create paths to training and test data\ntrain_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\n# load data into training and test sets\ndata = pd.read_csv(train_path, dtype=dtypes)\ndata_test = pd.read_csv(test_path, dtype=dtypes)","ede6f8a0":"# inspect top of dataset\ndata.head()","f7c81f17":"# inspect bottom of data set\ndata.tail()","e0bae652":"# inspect summary of dataset, making note of missing values\ndata.info()","aef7953c":"# analyze summary statistics on numeric features\ndata.describe().transpose()","b5a790e2":"# create copy of data for exploratory analysis\ndata_explore = data.copy()","b2d130cd":"def missing_data(df):\n    '''Calculate and create a dataframe that summarizes the missing data in a dataset\n    \n    Parameters\n    -----------\n    df : pandas.DataFrame\n        The dataset that you want to determine missing data for\n        \n    Returns\n    -------\n    missing : pandas.DataFrame\n        Pandas DataFrame that summarizes the missing data.\n    '''\n    \n    # get missing data and create pandas DataFrame\n    missing = df.isnull().sum().sort_values(ascending=False)\n    missing = missing[missing > 0]\n    missing = pd.DataFrame(missing, columns=['total_missing'])\n    \n    # calculate % of missing data for each category and add column to DataFrame\n    missing['missing_pct'] = np.round(missing['total_missing'] \/ len(df), 3)\n    \n    return missing","332760d5":"# get missing data for Ames dataset\nmissing_data(data_explore)","1b81ecea":"# plots histograms of all the numerical features in the dataset\ndata_explore.hist(figsize=(15,15))\nplt.tight_layout()\nplt.show()","7c120cbd":"# summary statistics on Sale Price\nnp.round(data_explore['SalePrice'].describe().transpose())","a4b6c8d4":"# plot histogram of sale price including mean\/std dev from summary stats above\nplt.figure(figsize=(10, 6), dpi=100)\nplt.hist(x=data_explore['SalePrice'], bins=20)\nplt.text(200000, 370, r'$\\mu=180,000$')\nplt.text(200000, 350, r'$\\sigma=79,500$')\nplt.title('Histogram of Sale Price', fontsize=16)\nplt.xlabel('Sale Price', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.show()","e00755bf":"# pull up potential outliers in Sale Price\ndata_explore[data_explore['SalePrice'] > 600000]","138ef591":"# plot histogram of sales price, excluding the 4 homes above\nplt.figure(figsize=(10, 6), dpi=100)\ndata_explore[data_explore['SalePrice'] < 600000]['SalePrice'].hist()\nplt.title('Histogram of Sale Price < $600,000', fontsize=16)\nplt.xlabel('Sale Price', fontsize=14)\nplt.ylabel('Count', fontsize=14)\nplt.grid(False)\nplt.show()","60aafb00":"# summary statistics on log transformed Sale Price\nnp.log(data_explore[data_explore['SalePrice'] < 600000]['SalePrice']).describe()","19a6cf74":"# perform log transformation on Sales Price and plot resulting distribution\nplt.figure(figsize=(10, 6), dpi=100)\nnp.log(data_explore[data_explore['SalePrice'] < 600000]['SalePrice']).hist()\nplt.text(12.3, 375, r'$\\mu=e^{12}$')\nplt.text(12.3, 350, r'$\\sigma=e^{0.39}$')\nplt.title('Histogram of Sale Price < $600,000 (Natural Log)', fontsize=16)\nplt.xlabel('Ln(Sale Price)', fontsize=14)\nplt.grid(False)\nplt.ylabel('Count', fontsize=14)\nplt.show()","ee8b48cb":"# \"common sense\" features that I've picked out\nnum_var = ['GrLivArea', 'LotArea', 'TotalBsmtSF', '1stFlrSF']\n\n# create subplots to plot scatter plots in\nfig, axs = plt.subplots(nrows=2, ncols=2, sharey=True, figsize=(10, 7), dpi=100)\n\n# loop through features and plot each\nfor ax, num_var in zip(axs.flat, num_var):\n    ax.plot(data_explore[num_var], data_explore['SalePrice'], \".b\", alpha=0.3)\n    ax.set_xlabel(str(num_var))\n    ax.set_title(str(num_var) + ' vs. SalePrice')\n\n# plot y axis labels\nfor ax in range(0, 2):\n    axs[ax, 0].set_ylabel('SalePrice')\n\n# chart title\nplt.suptitle(\"Analyzing Key Numerical Features Relationships with Sales Price\", fontsize=18)\nplt.tight_layout()","5abe29fb":"# same plot for Lo Area but zoomed in to lot sizes < 60000 sq ft\nplt.figure(figsize=(10, 6), dpi=100)\nplt.plot(data_explore[data_explore['LotArea'] < 60000]['LotArea'], \n         data_explore[data_explore['LotArea'] < 60000]['SalePrice'], \n         \".b\", alpha=0.3)\nplt.title('Lot Size (<60,000 sq ft) vs. Sale Price', fontsize=16)\nplt.xlabel('Lot Size', fontsize=14)\nplt.ylabel('Sale Price', fontsize=14)\nplt.show()","b84e429f":"# plot sales price vs overall quality\nplt.figure(figsize=(10, 6), dpi=100)\nsns.boxplot(x='OverallQual', y='SalePrice', data=data_explore, palette='pastel')\nplt.title('Distribution of Sale Price by Overall Quality', fontsize=16)\nplt.xlabel('Overall Quality', fontsize=14)\nplt.ylabel('Sale Price', fontsize=14)\nplt.show()","39806909":"# plot sales price vs overall condition\nplt.figure(figsize=(10, 6), dpi=100)\nsns.boxplot(x='OverallCond', y='SalePrice', data=data_explore, palette='pastel')\nplt.title('Distribution of Sale Price by Overall Condition', fontsize=16)\nplt.xlabel('Overall Condition', fontsize=14)\nplt.ylabel('Sale Price', fontsize=14)\nplt.show()","e0455ef2":"# plot correlation heatmap\nplt.figure(figsize=(12,7), dpi=100)\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(data=data_explore.corr(),\n            mask=np.triu(np.ones_like(data_explore.corr(), dtype=bool)),\n            square=True,\n            vmax=1,\n            center=0,\n            cmap=cmap,\n            linewidths=.7,\n            cbar_kws={\"shrink\": .5})\nplt.show()","d228de6b":"# plot correlation matrix for Sale Price\ncorr_matrix = data_explore.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","5bc3866d":"# consists of all features related to indoor square footage\ndata_explore['**IndoorSqFt'] = data_explore['TotalBsmtSF'] + data_explore['1stFlrSF'] + data_explore['2ndFlrSF'] + data_explore['GarageArea']\n\n# consists of all features related to outdoor square footagee\ndata_explore['**OutdoorSqFt'] = data_explore['WoodDeckSF'] + data_explore['OpenPorchSF'] + data_explore['EnclosedPorch'] + data_explore['3SsnPorch'] + data_explore['ScreenPorch']\n\n# consists of all features realted to both full and half size bathrooms\ndata_explore['**TotalBaths'] = data_explore['FullBath'] + data_explore['BsmtFullBath'] + data_explore['HalfBath'] + data_explore['BsmtHalfBath']\n\n# consists of all features related to bedrooms and bathrooms\ndata_explore['**RmsAndBathAbvGrd'] = data_explore['TotRmsAbvGrd'] + data_explore['FullBath'] + data_explore['BsmtFullBath'] + data_explore['HalfBath'] + data_explore['BsmtHalfBath']\n\n# consists of all features related basement and garage square footage\ndata_explore['**BsmtAndGarage'] = data_explore['GarageArea'] + data_explore['TotalBsmtSF']","8d75347a":"# rerun the correlation matrix with the newly aggregated features\ncorr_matrix = data_explore.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","4ae769ee":"# take a copy of the dataset for preparation purposes\ndata_prep = data.copy()","e870469a":"# get summary of missing data again\nmissing_data(data_prep)","5381d2a9":"# get value counts of Fence feature\ndata_prep['Fence'].value_counts()","61d946fa":"# get total number of houses with 0 fireplaces where fireplace quality is NaN\ndata_prep[data_prep['Fireplaces'] == 0]['FireplaceQu'].isna().sum()","bbda358a":"# get total number of houses with > 0 fireplaces where fireplace quality is NaN (opposite of the above)\ndata_prep[data_prep['Fireplaces'] != 0]['FireplaceQu'].isna().sum()","24bbe4a0":"# summary statistics on lot frontage\ndata_prep['LotFrontage'].describe()","0ffa6be0":"# histogram of lot frontage\nplt.figure(figsize=(7, 4), dpi=100)\ndata_prep['LotFrontage'].hist()\nplt.title(\"Lot Frontage Histogram\")\nplt.grid(False)\nplt.show()","23df1171":"# comapre garage missing features with garage area\ngarage_feat = ['GarageArea', 'GarageYrBlt', 'GarageCond', 'GarageType', 'GarageFinish', 'GarageQual']\ndata_prep[data_prep['GarageType'].isna()][garage_feat].sort_values('GarageArea', ascending=False)","13910451":"# compare basement missing features with basement square footage\nbsmt_feat = ['BsmtFinType2', 'BsmtExposure', 'BsmtQual', 'BsmtCond', 'BsmtFinType1','TotalBsmtSF']\ndata_prep[data_prep['BsmtFinType2'].isna()][bsmt_feat]","634a1a3e":"# summary statistics for masonry veneer area\ndata_prep['MasVnrArea'].median()","af99c351":"# determine which training instances are missing the data related to Masonry\ndata_prep[data_prep['MasVnrArea'].isna()][['MasVnrType', 'MasVnrArea']]","d2773c0c":"# get value counts of electrical feature\ndata_prep['Electrical'].value_counts()","68f26725":"# plot histogram of sales price again to show skewed distribution\nplt.figure(figsize=(7, 4), dpi=100)\nplt.title(\"Sales Price Histogram\")\ndata_prep['SalePrice'].hist()\nplt.grid(False)\nplt.show()","d84462fe":"# pull up potential outliers\ndata_prep[data_prep['SalePrice'] > 600000]","56a7404d":"# features with potential outliers\noutliers = ['LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n            'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', \n            'PoolArea', 'MiscVal']\n\n# plot histograms of these features\ndata_prep[outliers].hist(figsize=(15,10))\nplt.show()","98b1a4dd":"# get potential lot frontage outliers\ndata_prep[data_prep['LotFrontage'] > 200]['LotFrontage']","1f3ea444":"# get potential masonry veneer area outliers\ndata_prep[data_prep['MasVnrArea'] > 1000]","d4f38e90":"# get potential basement finished sqft 1 area outliers\ndata_prep[data_prep['BsmtFinSF1'] > 2500]['BsmtFinSF1']","c2c3f103":"# get potential basement finished sqft 2 area outliers\ndata_prep[data_prep['BsmtFinSF2'] > 1100]['BsmtFinSF2']","027277f2":"# create class that will create custom features in pipeline\nclass CustomFeatures(BaseEstimator, TransformerMixin):\n    \"\"\"Represents the custom features added to the Titanic dataset\"\"\"\n    \n    def __init__(self, add_outdoorsqft=False, add_totalbaths=False):\n        self.add_outdoorsqft = add_outdoorsqft\n        self.add_totalbaths = add_totalbaths\n    \n    def fit(self, X, y=None):\n        return self    # nothing else to do\n    \n    def transform(self, X, y=None):\n        # create copy of dataset to avoid changes to original\n        X = X.copy()\n        \n        # indoor sq footage feature\n        X['IndoorSqFt'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF'] + X['GarageArea']\n        \n        # total rooms feature\n        X['RmsAndBathAbvGrd'] = X['TotRmsAbvGrd'] + X['FullBath'] + X['BsmtFullBath'] + X['HalfBath'] + X['BsmtHalfBath']\n\n        # basement and garage sq footage feature\n        X['BsmtAndGarage'] = X['GarageArea'] + X['TotalBsmtSF']\n        \n        # outdoor sq footage feature\n        if self.add_outdoorsqft:\n            X['OutdoorSqFt'] = X['WoodDeckSF'] + X['OpenPorchSF'] + X['EnclosedPorch'] + X['3SsnPorch'] + X['ScreenPorch']\n        else:\n            pass\n        \n        # total baths feature\n        if self.add_totalbaths:\n            X['TotalBaths'] = X['FullBath'] + X['BsmtFullBath'] + X['HalfBath'] + X['BsmtHalfBath']\n        else:\n            pass\n        \n        return X","060c3cae":"# take a fresh copy of the data\ndata_prep = data.copy()\n\n# our training instances we will be dropping\nto_drop = [691, 898, 1169, 1182, 332, 934, 1298]","fb0fbfdc":"# training instances to be removed\ndata_prep.iloc[to_drop]","16a95891":"# drop the outliers from data\ndata_prep = data_prep.drop(labels=to_drop, axis=0)","6d890f6b":"# histogram of log sales price to demonstrate normality of distribtion\nplt.figure(figsize=(7, 4), dpi=100)\nplt.title(\"Log Sales Price Histogram\")\nnp.log(data_prep['SalePrice']).hist()\nplt.grid(False)\nplt.show()","365e2ae1":"# apply transformation to target label\ndata_prep['SalePriceLn'] = np.log(data_prep['SalePrice'])","fc4287c4":"# compare a few pre-transformed and transformed labels\ndata_prep[['SalePrice', 'SalePriceLn']].head()","ecf9f17f":"# create dataset containing only our target labels\ndata_labels = data_prep['SalePriceLn'].copy()\n\n# create dataset containing only our data features\ndata_features = data_prep.drop(['SalePrice', 'SalePriceLn'], axis=1)","df219dc2":"# create summary of numerical features\nnum_features = data_features.select_dtypes(include=[np.number])\n\n# remove features that will not be used for our ML model\nnum_features = num_features.drop(['Id', 'EnclosedPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'], axis=1)","d13b7e83":"# create a list of numerical features\nnum_features_list = list(num_features)","f3cb9b7f":"# print out list of numerical features and ensure it only contains features that I concluded to keep in dataset\nnum_features_list","ddc7cdb1":"# create list of ordinal features that will be used for our model\nord_features = ['OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                'BsmtFinType1','BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', \n                'GarageQual','GarageCond']","54969929":"# create list of categorical features, excluding Electrical\ncat_features1 = ['MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n                'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n                'YearBuilt', 'YearRemodAdd', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n                'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'GarageType', 'GarageYrBlt', \n                'GarageFinish', 'PavedDrive']","c5a1b94a":"# create list of categorical features, only including Electrical\ncat_features2 = ['Electrical']","34fb87df":"# numerical features pipeline\nnum_pipeline = Pipeline([\n    ('custom_features', CustomFeatures()),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])","b18e5337":"# categorical features pipeline 1\ncat_pipeline1 = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='none')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# categorical features pipeline 2\ncat_pipeline2 = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n])","c3d29c74":"# ordinal features pipeline\nord_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='none')),\n    ('encoder', OrdinalEncoder(handle_unknown='ignore'))\n])","e3e53b58":"# full pipeline, applies all previous pipelines and drops columns with features that won't be used in modelling \nfull_pipeline = ColumnTransformer(\n   [('num', num_pipeline, num_features_list),\n    ('cat1', cat_pipeline1, cat_features1),\n    ('cat2', cat_pipeline2, cat_features2),\n    ('ord', ord_pipeline, ord_features)],\n    remainder='drop')","94f376ef":"# apply pipeline to data\ndata_prepared = full_pipeline.fit_transform(data_features)","4c596fe5":"data_prepared.shape","15c0f896":"# helper function to score models\ndef calc_cv_scores(model, features=data_prepared, labels=data_labels, metric='neg_mean_squared_error', folds=5):\n    scores = cross_val_score(model, X=features, y=labels, scoring=metric, cv=folds)\n    scores = np.round(np.sqrt(-scores), 3)\n    return scores","1cf02dee":"# create base model and get predictions\nbase_model = np.ones(shape=(len(data_labels), 1))\nbase_pred =  base_model * data_labels.mean()","9ce66d37":"# score base model\nnp.round(np.sqrt(mean_squared_error(data_labels, base_pred)), 2)","246ec01a":"# the steps for each model below are\n# 1) create instance of model\n# 2) fit to train set\n# 3) call helper scoring function and store these in a variable\n\n# linear regression\nlin_reg = LinearRegression()\nlin_reg.fit(data_prepared, data_labels)\nlin_reg_scores = calc_cv_scores(lin_reg)\n\n# elastic net regression\nen_reg = ElasticNet()\nen_reg.fit(data_prepared, data_labels)\nen_reg_scores = calc_cv_scores(en_reg)\n\n# svm regression w\/ linear kernel\nsvm_lin_reg = SVR(kernel='linear')\nsvm_lin_reg = svm_lin_reg.fit(data_prepared, data_labels)\nsvm_lin_reg_scores = calc_cv_scores(svm_lin_reg)\n\n# svm regression w\/ rbf kernel\nsvm_rbf_reg = SVR(kernel='rbf')\nsvm_rbf_reg = svm_rbf_reg.fit(data_prepared, data_labels)\nsvm_rbf_reg_scores = calc_cv_scores(svm_rbf_reg)\n\n# decision tree regression\ndt_reg = DecisionTreeRegressor()\ndt_reg = dt_reg.fit(data_prepared, data_labels)\ndt_reg_scores = calc_cv_scores(dt_reg)\n\n# random forest regression\nrf_reg = RandomForestRegressor()\nrf_reg = rf_reg.fit(data_prepared, data_labels)\nrf_reg_scores = calc_cv_scores(rf_reg)\n\n# gradient boosting regression\ngrad_boost_reg = GradientBoostingRegressor()\ngrad_boost_reg = grad_boost_reg.fit(data_prepared, data_labels)\ngrad_boost_reg_scores = calc_cv_scores(grad_boost_reg)\n\n# xgb regression\nxgb_reg = XGBRegressor()\nxgb_reg = xgb_reg.fit(data_prepared, data_labels)\nxgb_reg_scores = calc_cv_scores(xgb_reg)","9b5104c9":"# list of models\nmodels = [lin_reg, en_reg, svm_lin_reg, svm_rbf_reg, dt_reg, rf_reg, grad_boost_reg, xgb_reg]\n\n# list of model scores\nmodel_scores = [lin_reg_scores, en_reg_scores, svm_lin_reg_scores, svm_rbf_reg_scores,\n                dt_reg_scores, rf_reg_scores, grad_boost_reg_scores, xgb_reg_scores]\n\n# lists to hold  means, std deviations, model names, and fold number\nmeans = []\nstd_devs = []\nmodel_names = []\nfolds = []\n\n# fill mean and std deviation list\nfor score in model_scores:\n    means.append(np.mean(score))\n    std_devs.append(np.std(score))\n\n# fill model names list\nfor model in models:\n    model_names.append(type(model).__name__)\n\n# fill fold number list\nfor fold in range(1, 6):\n    folds.append(\"Fold \" + str(fold))","c831719e":"# update SVR model names to include kernel type\nmodel_names[2] = 'SVR - Linear K'\nmodel_names[3] = 'SVR - RBF K'","81de5924":"# create pandas DataFrame with the above lists\nresults = pd.DataFrame(model_scores, index=model_names, columns=folds)\nresults['Mean Score'] = means\nresults['Std Dev'] = std_devs","549394cb":"# display results DataFrame sorted on on mean score\nresults.sort_values('Mean Score')","6a54e1f0":"# plot of model score means\nplt.figure(figsize=(5, 5), dpi=100)\nplt.title(\"Score Summary - Quick and Dirty Models\")\nsorted_scores = results['Mean Score'].sort_values(ascending=False)\nplt.bar(x=sorted_scores.index, height=sorted_scores)\nplt.xticks(rotation=90);","933a9da4":"# parameters for SVM RandomizedSearchCV\nsvm_rbf_reg_params = {\n    'C': stats.loguniform(0.01, 15),\n    'gamma': ['scale', 'auto'],\n    'epsilon': stats.loguniform(0.1, 5)\n}\n\n# create new SVM model instance\nsvm_rbf_reg = SVR(kernel='rbf')\n\n# set up SVM RandomizedSearchCV\nsvm_rbf_search = RandomizedSearchCV(estimator=svm_rbf_reg,\n                                    param_distributions=svm_rbf_reg_params,\n                                    n_iter=200,\n                                    cv=5,\n                                    scoring='neg_mean_squared_error',\n                                    verbose=1,\n                                    n_jobs=-1)\n\n# fit SVM RandomizedSearchCV\nsvm_rbf_search.fit(data_prepared, data_labels)","23ca0a15":"# get the best params from the RandomizedSearchCV\nsvm_rbf_search.best_params_","8c7a2545":"# parameters for SVM GridSearchCV\nsvm_rbf_reg_param_grid = {\n    'C': [1, 2, 2.25, 2.50, 2.75, 3],\n    'epsilon': [0.05, 0.1, 0.15, 0.2, 0.3]\n}\n\n# create SVM model instance\nsvm_rbf_reg = SVR(kernel='rbf', gamma='auto')\n\n# set up SVM GridSearchCV\nsvm_rbf_fine_search = GridSearchCV(estimator=svm_rbf_reg,\n                                   param_grid=svm_rbf_reg_param_grid,\n                                   cv=5,\n                                   scoring='neg_mean_squared_error',\n                                   verbose=1,\n                                   n_jobs=-1)\n\n# fit SVM GridSearchCV\nsvm_rbf_fine_search.fit(data_prepared, data_labels)","c7d03fe9":"# get the best params from the GridSearchCV\nsvm_rbf_fine_search.best_params_","e042e398":"# parameters for gb RandomizedSearchCV\ngrad_boost_reg_params = {\n    'loss': ['ls', 'lad'],\n    'learning_rate': stats.loguniform(0.1, 1),\n    'n_estimators':  stats.randint(100, 1500),\n    'subsample': stats.loguniform(0.1, 1),\n    'max_features': ['auto', 'log2']\n}\n\n# create new gb model instance\ngrad_boost_reg = GradientBoostingRegressor(random_state=42)\n\n# set up gb RandomizedSearchCV\ngrad_boost_search = RandomizedSearchCV(estimator=grad_boost_reg,\n                                       param_distributions=grad_boost_reg_params,\n                                       n_iter=200,\n                                       cv=5,\n                                       scoring='neg_mean_squared_error',\n                                       verbose=1,\n                                       n_jobs=-1)\n\n# fit gb RandomizedSearchCV\ngrad_boost_search.fit(data_prepared, data_labels)","b8a8132e":"# get the best params from the RandomizedSearchCV\ngrad_boost_search.best_params_","d28fc6c5":"# parameters for gb GridSearchCV\ngrad_boost_reg_param_grid = {\n    'learning_rate': [0.05, 0.10, 0.15, 0.20],\n    'n_estimators':  [750, 780, 800, 900],\n    'subsample': [0.4, 0.5, 0.55, 0.6, 0.7]\n}\n\n# create gb model instance\ngrad_boost_reg = GradientBoostingRegressor(loss='lad', max_features='auto', random_state=42)\n\n# set up gb GridSearchCV\ngrad_boost_fine_search = GridSearchCV(estimator=grad_boost_reg,\n                                      param_grid=grad_boost_reg_param_grid,\n                                      cv=5,\n                                      scoring='neg_mean_squared_error',\n                                      verbose=1,\n                                      n_jobs=-1)\n\n# fit gb GridSearchCV\ngrad_boost_fine_search.fit(data_prepared, data_labels)","c056fd34":"# get the best params from the GridSearchCV\ngrad_boost_fine_search.best_params_","4528fe9b":"# parameters for second gb GridSearchCV\ngrad_boost_reg_param_grid2 = {\n    'n_estimators':  [900, 925, 950, 975, 1000],\n}\n\n# create gb model instance\ngrad_boost_reg = GradientBoostingRegressor(loss='lad', max_features='auto', \n                                           learning_rate=0.1, subsample=0.5,\n                                           random_state=42)\n\n# set up second GridSearchCV\ngrad_boost_fine_search2 = GridSearchCV(estimator=grad_boost_reg,\n                                       param_grid=grad_boost_reg_param_grid2,\n                                       cv=5,\n                                       scoring='neg_mean_squared_error',\n                                       verbose=1,\n                                       n_jobs=-1)\n\n# fit gb GridSearchCV\ngrad_boost_fine_search2.fit(data_prepared, data_labels)","dc81a9c8":"# get the best params from the second GridSearchCV\ngrad_boost_fine_search2.best_params_","b5fe91ac":"# parameters for rf RandomizedSearchCV\nrf_reg_params = {\n    'n_estimators': stats.randint(100, 1500),\n    'max_features': ['auto', 'log2'],\n    'bootstrap': [True, False]\n}\n\n# create new rf model instance\nrf_reg = RandomForestRegressor(random_state=42)\n\n# set up rf RandomizedSearchCV\nrf_search = RandomizedSearchCV(estimator=rf_reg,\n                               param_distributions=rf_reg_params,\n                               n_iter=200,\n                               cv=5,\n                               scoring='neg_mean_squared_error',\n                               verbose=1,\n                               n_jobs=-1)\n\n# fit rf RandomizedSearchCV\nrf_search.fit(data_prepared, data_labels)","c94cdde3":"# get the best params from the RandomizedSearchCV\nrf_search.best_params_","2a77b24f":"# parameters for gb GridSearchCV\nrf_reg_param_grid = {\n    'n_estimators':  [700, 720, 740, 760, 800],\n}\n\n# create gb model instance\nrf_reg = RandomForestRegressor(random_state=42, max_features='auto', bootstrap=True)\n\n# set up gb GridSearchCV\nrf_fine_search = GridSearchCV(estimator=rf_reg,\n                              param_grid=rf_reg_param_grid,\n                              cv=5,\n                              scoring='neg_mean_squared_error',\n                              verbose=1,\n                              n_jobs=-1)\n\n# fit gb GridSearchCV\nrf_fine_search.fit(data_prepared, data_labels)","e1e5dbed":"# get the best params from the GridSearchCV\nrf_fine_search.best_params_","b2e1b7c7":"# parameters for en RandomizedSearchCV\nen_reg_params = {\n    'alpha': stats.loguniform(0.01, 10),\n    'l1_ratio': stats.loguniform(0.01, 0.99),\n}\n\n# create new en model instance\nen_reg = ElasticNet(random_state=42)\n\n# set up en RandomizedSearchCV\nen_search = RandomizedSearchCV(estimator=en_reg,\n                               param_distributions=en_reg_params,\n                               n_iter=200,\n                               cv=5,\n                               scoring='neg_mean_squared_error',\n                               verbose=1,\n                               n_jobs=-1)\n\n# fit en RandomizedSearchCV\nen_search.fit(data_prepared, data_labels)","7af6c704":"# get the best params from the RandomizedSearchCV\nen_search.best_params_","6e18ef7e":"# create variable for each final model\nfinal_svm = svm_rbf_fine_search.best_estimator_\nfinal_grad_boost = grad_boost_fine_search2.best_estimator_\nfinal_rf = rf_fine_search.best_estimator_\nfinal_en = en_search.best_estimator_","660f0a1e":"# save final models\ndump(final_svm, 'final_svm.joblib')\ndump(final_grad_boost, 'final_grad_boost.joblib')\ndump(final_rf, 'final_rf.joblib')\ndump(final_en, 'final_en.joblib')","84e74d1a":"# load models\nfinal_svm = load('..\/input\/ames-models\/final_svm.joblib')\nfinal_grad_boost = load('..\/input\/ames-models\/final_grad_boost.joblib')\nfinal_rf = load('..\/input\/ames-models\/final_rf.joblib')\nfinal_en = load('..\/input\/ames-models\/final_en.joblib')","ef90fca4":"# get predictions for each model\nsvm_predictions = final_svm.predict(data_prepared)\ngrad_boost_predictions = final_grad_boost.predict(data_prepared)\nrf_predictions = final_rf.predict(data_prepared)\nen_predictions = final_en.predict(data_prepared)","53dcfa75":"# list of final models\nfinal_models = [final_svm, final_grad_boost, final_rf, final_en]\n\n# list of final models predictions\nfinal_predictions = [svm_predictions, grad_boost_predictions, rf_predictions, en_predictions]\n\n# lists to store rlmse and model names\nrlmse = []\nfinal_model_names = []\n\n# fill rlmse list\nfor predictions in final_predictions:\n    rlmse.append(mean_squared_error(y_true=data_labels, y_pred=predictions, squared=False))\n\n# fill model name list\nfor model in final_models:\n    final_model_names.append(type(model).__name__)\n\n# create DataFrame with model scores and sort based on score\nfinal_scores = pd.DataFrame(rlmse, index=final_model_names, columns=['RLMSE'])\nfinal_scores.sort_values('RLMSE')","04c4f015":"def plot_learning_curves(model, X, y, title, ax=None, splits=50):\n    '''Calculate and create a dataframe that summarizes the missing data in a dataset\n    \n    Parameters\n    -----------\n    model : sklearn.estimator\n        The model that you want to plot the learning curve for\n        \n    X : np.array\n        The training set\n    \n    y: np.array\n        The training labels\n    \n    title: String\n        The title of the learning curve\n    \n    ax: matplotlib.axes\n        The ax that you would like to plot the learning curve too\n        \n    splits: int\n        The number of splits the data will be split into for plotting purposes.\n        E.g. If a dataset has 100 instances, and split is 10, the learning curve\n        will be plotted for 10, 20, 30, ..., 100 instances.  Default value is 50.\n        \n    '''\n    \n    # set the matplot lib axes\n    ax=ax\n    \n    # split the training data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # lists to store train and validation errors\n    train_errors, val_errors = [], []\n    \n    # create list of training instances to iterate through for plotting\n    train_sizes = np.linspace(0.001, 1.0, splits)\n    instances = [int(round(len(X) * i, 0)) for i in train_sizes]\n\n    # train models based on batches of dataset and append losses to lists\n    for i in instances:\n        model.fit(X_train[:i], y_train[:i])\n        y_train_predict = model.predict(X_train[:i])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:i], y_train_predict, squared=False))\n        val_errors.append(mean_squared_error(y_val, y_val_predict, squared=False))\n    \n    # plot the learning curves\n    ax.plot(instances, train_errors, \"r-+\", linewidth=2, label=\"train\")\n    ax.plot(instances, val_errors, \"b-\", linewidth=3, label=\"val\")\n    ax.legend(loc='best')\n    ax.set_title(title)","1fc51fdd":"# load models into a fresh variable so that original models are not affected\nsvm_lc = load('..\/input\/ames-models\/final_svm.joblib')\ngb_lc = load('..\/input\/ames-models\/final_grad_boost.joblib')\nrf_lc = load('..\/input\/ames-models\/final_rf.joblib')\nen_lc = load('..\/input\/ames-models\/final_en.joblib')","a52ce125":"# create dense array to plot learning curves\n# note: i did this since my function i created below didn't work with a sparse array\n# i wouldn't recommend doing this with large datasets (can take up all of memory)\n# if anyone has tips on how to improve my fucntion that would be great!!\ndense_data = data_prepared.todense()","9bee1ae7":"# filter out warnings that will occur when plotting learning curves but are not important\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n# plot learning curves\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharey=True, figsize=(15, 10))\nplot_learning_curves(svm_lc, dense_data, data_labels, ax=ax1, title=\"SVM Learning Curve\")\nplot_learning_curves(en_lc, dense_data, data_labels, ax=ax2, title=\"Elastic Net Learning Curve\")\nplot_learning_curves(rf_lc, dense_data, data_labels, ax=ax3, title=\"Random Forest Learning Curve\")\nplot_learning_curves(gb_lc, dense_data, data_labels, ax=ax4, title=\"Gradient Boosting Learning Curve\")\nax1.set_ylim(0, 0.4)\nax3.set_ylim(0, 0.4)\nplt.suptitle(\"Model Learning Curves\", fontsize=18)\nplt.tight_layout()","6d8ced53":"# model names in String format\nnames = ['final_svm', 'final_grad_boost', 'final_rf', 'final_en']\n\n# create voting regressor\nvoting_reg = VotingRegressor(estimators=list(zip(names, final_models)))\n\n# fit voting regressor to data\nvoting_reg.fit(data_prepared, data_labels)","3e614e98":"# predict with voting model\nvoting_predictions =voting_reg.predict(data_prepared)\n\n# calculated RLMSE on voting model predictions\nmean_squared_error(y_true=data_labels, y_pred=voting_predictions, squared=False)","12657a46":"# appending voting model and scores to final model lists\nfinal_models.append(voting_reg)\nfinal_predictions.append(voting_predictions)","9649bd4a":"# create copy of test data\nX_test = data_test\n\n# run test data through pipeline\nX_test_prepared = full_pipeline.transform(X_test)","d92e9ae7":"# lists for test predictions and transformed test predictions\ntest_predictions = []\ntransformed_test_predictions = []\n\n# predict for each model and append to predictions list above\nfor model in final_models:\n    test_predictions.append(model.predict(X_test_prepared))\n    \n# re-transform each models predictions and append to transformed predictions list above\nfor predictions in test_predictions:\n    transformed_test_predictions.append(pd.DataFrame(np.exp(predictions)))    ","4563ba9f":"# create .csv file for each model\nfor model_num, predictions in enumerate(transformed_test_predictions):\n    submission = pd.DataFrame()\n    submission['Id'] = data_test['Id']\n    submission['SalePrice'] = predictions\n    submission.to_csv(str(model_num) + 'submission', index=False)\n    \n# the .csv files will be named as follows:\n# 0submission.csv - this is the SVR Model\n# 1submission.csv - this is the GB Model\n# 2submission.csv - this is the RF Model\n# 3submission.csv - this is the EN Model\n# 4submission.csv - this is the Voting Regressor Model","fd1f81ac":"### 3.2.1 Quick and Dirty Models: Training","ece5983a":"# 1.0 Exploratory Data Analysis\n- In this section I perform some exploratory data analysis, data exploration, and data visualization\n- Note that I take a copy of the dataset to perform this analysis on, to ensure there are no accidental changes to the original dataset.","5416438e":"### 3.3.1 Support Vector Machine Regressor with RBF Kernel\n- In this section I'll tune a SVM Regressor further\n- I'll tune the following hyper parameters:\n    - **C**: This is essentially a regualization parameter for an SVM.  In sk-learn, the strength of the regularization is inversely proportional to C, and the value passed must be positive.\n    - **gamma**: This is the kernel coefficient for an rbf kernel and essentially also regularizes the model.  A higher gamma reduces regularization and vice versa.  For purposes of this model, I'll just choose either 'scale' or 'auto' rather than tuning an integer value (see sk-learn documentation for further explanation)\n    - **epsilon**: For SVM regression, the model is trying to fit as many instances \"on the street\" (or hyper street to be more accurate) as possibel.  This means that it is trying to fit as many training instances between its margins and avoid margin violations (instances that are not on the hyper street).  Epsilon control the \"width\" of the street.\n- You'll see that based on the RandomizedSearchCV 'auto' works best for gamma.  Therefore for the finer grid search, I'll set gamma to this, and then tune the C and epsilon values further","b320aed1":"## 3.7 Model Learning Curves\n- In this section I'll plot each of the models learning curves in order to assess if any of the models are over or under fitting\n- First I'll define a function to plot the learning curves\n    - I created this function myself with inspiration from Hands on ML by Aurelien Geron\n    - If anyone reviewing this workbook has any tips on how it can be improved, please feel free to reach out!","5a21125d":"### 2.2.2 Features with Potential Outliers\n- In section 1, I identified that the following numerical features had skewed distributions\n- First, I've replotted the histograms to show this, and then I analyze what to do with each feature seperately\n- Note, I haven't analyzed every single feature, as some of these are so skewed in the sense that all the values are almost zero\n- See the next section for how I'll deal with those","cdef6352":"## 0.3 Overall Goal\nThe overall goal of this project is to train a supervised learning model that is able to predict the price of a home in Ames, Iowa which is a regression problem.  The metric that is used for this kaggle competition is RLMSE - or root log mean squared error.  This is a slight variation on regular RMSE, and assesses models on the difference between the logaritm of the predicted value and the logarithm of the observed sales price.  This essentially means that errors in predicting expensive houses and cheap houses affect the final score equally","6054b543":"## 0.5 Data Import and Initial Inspection\nIn this section I read in the dataset, and run some common pandas commands to perform a high level inspection of it","7c11bddd":"### 1.3.2 Transformed Target Label\n- Here, I'll try to transform the Sales Price distribtion into a normal distribution by applying a log transformation\n- More specifically, I'll take the natural logarithm of the Sales Price for each training instance, and plot the resulting distribution\n- You'll see that this transforms our target label nicely into a distribtion that is pretty normal","3168b41c":"As I expected, it looks like as the overall quality of a home goes up, the sales price also does for the most part","a00aae28":"# 2.0 Data Preparation\n- In this section I prepare the data so that we can start testing out some ML models\n- Note, I'll take a copy of the original data again so that no changes to the original dataset are made by accident\n- At a high level I performed the following:\n    - Assess how to deal with missing data\n    - Assess how to deal with outliers\n    - Discuss any potential data leakage\n    - Create a custom transformer for aggregated features\n    - Build an sk-learn pipeline to transform the dataset","789ed93e":"# 3.0 Model Selection and Model Tuning\n- In this section I perform the following:\n    - Create a Base Model, which is the score to beat - this model is one that always predicts the mean sales price from the training set\n    - Model selection with \"quick and dirty models\"\n    - Model tuning based on promising models above\n    - Save the final models with joblib\n- Below is a helper function I created to make scoring the models a bit faster","3cf2b81d":"It looks like the Lot Area scatter plot was impacted by a few outliers - below I've zoomed in on the plot for lot areas between 0 to 60,000 sq feet.","715e68c3":"## 2.1 Analysis of Missing Data\n- Here I'll look at missing data for relevant categories and decide how to deal with each category respectively","dee94132":"### 3.2.2 Quick and Dirty Model Results\n- Now I'll assess how these quick and dirty models did","abd57f9e":"## 2.3 Missing Data and Outlier Conclusions\n- Here's a summary of how I'll handle each feature that I identified that has either missing data and\/or outliers","fbe13161":"### 2.1.7 Missing Data: Masonry Veneer Features\n- Masonry veneer area and type both have 8 missing values (which is a very little amount of missing data)\n- It looks like its the same training instances that are missing both types of data\n- Based on the analysis below for veneer area, I'll simply replace the missing values with 0 (which is actually also the median)\n- In terms of masonry veneer type, I'll replace the missing values with \"none\"","e86f9ce2":"### 1.4.1 Scatterplots of Common Sense Features vs. Sales price","156da986":"- It looks like IndoorSqFt, BsmtAndGarage, and RmsAndBAthAbvGrd have promising correlations with Sales Price.\n- For learning purposes, I'll build all of these aggregated features into my ML pipeline later, and also include hyperparameters in the pipeline to turn the features with lower correlations \"on\" and \"off\" which can help in model tuning.","cbc319bc":"### 2.2.6 Basement Finished Sq Ft 2: Potential Outliers\n- It looks like BsmtFinSF2 may have some outliers\n- However, looking more closely at the distribution of BsmtFinSF2, most of the values in this feature are 0\n- Therefore I'll just leave this feature as is","7d608056":"## 3.2 Quick and Dirty Models\n- Next, I'll train what I like to call \"quick and dirty\" models\n- These are models that are purely trained based off of sk-learn's base settings with no modifications\n- Once these are trained, I'll performed cross validation scoring to assess them, and then select the most promising ones for further tuning\n- The models I've decided to include are:\n    - Linear Regression\n    - ElasticNet Regression\n    - SVM Regression (Linear kernel)\n    - SVM Regression (RBF kernel)\n    - Decision Tree Regression\n    - Random Forest Regression\n    - Gradient Boosting Regression\n    - XGB Regression","abdb86c4":"## 3.5 Load Models\n- Now I'll load the models back up","839a5133":"## 1.1 Missing Data Quick Look\n- Here, I use a helper function that I've created in the past to print a dataframe that contains features where data is missing\n- The dataframe contains the total missing values, as well as the % of missing data for each category\n- Further, in this section I only take a quick look at what the missing values are, so that I have a sense of this before data visualization\n- In section 2 of this workbook, I actually decide how to deal with the missing values","4d2a3859":"### 1.4.2 Sales Price vs. Overall Quality\n- Here I'll take a look at the relationship between Overall Quality and Sales Price\n- My intuition would tell me that these should be positively correleated","a9c05826":"- It looks like 'LotFrontage', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', 'ScreenPorch', 'PoolArea', 'MiscVal' are quite skewed.\n- I'll analyze this further in section 2 where I deal with outliers","41203ba1":"### 2.1.1 Missing Data: PoolQC, MiscFeature, Alley\n- These features have > 90% of data missing, I'll just drop them as this would be nearly impossible to fill accurately","7cd6f544":"## 2.2 Outlier analysis\n- In this section, I'll analyze potential outliers in our target label and within the numerical features","fe28cad5":"| Feature       | Missing Data | Significant Outliers | Missing Data Conclusion                       | Outlier Conclusion                    |\n|---------------|--------------|----------------------|-----------------------------------------------|---------------------------------------|\n| PoolQC        | Yes          | No                   | Drop feature, too much missing data           | NA                                    |\n| MiscFeature   | Yes          | No                   | Drop feature, too much missing data           | NA                                    |\n| Alley         | Yes          | No                   | Drop feature, too much missing data           | NA                                    |\n| Fence         | Yes          | No                   | Drop feature, too much missing data           | NA                                    |\n| FireplaceQu   | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| LotFrontage   | Yes          | Yes                  | Replace missing values with median            | Drop training instance #1298 and #934 |\n| GarageYrBlt   | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| GarageCond    | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| GarageType    | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| GarageFinish  | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| GarageQual    | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| BsmtFinType2  | Yes          | No                   | Remove instance 332, replace rest with \"None\" | NA                                    |\n| BsmtExposure  | Yes          | No                   | Remove instance 332, replace rest with \"None\" | NA                                    |\n| BsmtQual      | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| BsmtCond      | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| BsmtFinType1  | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| MasVnrArea    | Yes          | Yes                  | Replace with 0                                | Leave as is                           |\n| MasVnrType    | Yes          | No                   | Replace with \"None\"                           | NA                                    |\n| Electrical    | Yes          | No                   | Replace with most common                      | NA                                    |\n| BsmtFinSF1    | No           | Yes                  | NA                                            | Drop training instance #1298          |\n| BsmtFinSF2    | No           | Yes                  | NA                                            | Leave as is                           |\n| WoodDeckSF    | No           | Yes                  | NA                                            | Leave as is                           |\n| OpenPorchSF   | No           | Yes                  | NA                                            | Leave as is                           |\n| EnclosedPorch | No           | Yes                  | NA                                            | Drop feature, majority nil values     |\n| ScreenPorch   | No           | Yes                  | NA                                            | Drop feature, majority nil values     |\n| PoolArea      | No           | Yes                  | NA                                            | Drop feature, majority nil values     |\n| MiscVal       | No           | Yes                  | NA                                            | Drop feature, majority nil values     |\n","16dd76bb":"## 1.4 Analysis of Sales Price vs. Some \"Common Sense\" Features\n- Here, I've picked out a few numerical features that I would think would have a strong correlation with sales price based on my common sense\n- To pick these out I tried to imagine myself as a home buyer, or as a realator, and then to decide what features I'd pay more for in a house, or what features would help improve a selling price\n- Specifically, the features I've looked at are Living Area, Lot Area, Basement SF, and 1st Floor SF","9676ba9a":"- It looks like we have a range of correlations with Sale Price\n- A couple features are strongly positively correlated, names overall quality and living area\n- There are then a bunch of features in the range of 0.5 to 0.65 positive correlation\n- A small subset of them almost have 0 correlation, and there theres also a few with some level of weak negative correlation as well","64f655e5":"# Ames Regression Project ","b3440ff4":"## 1.5 Correlation Heat Map and Correlation Matrix\n- In this section I'll plot a correlation heat map and correlation matrix in order to analyze how the numerical features are correlated with Sales Price","f6dd1b18":"## 2.9 Data Preparation Final Notes\n- And that's it!  The data is now ready to build ML models\n- Our final dataset has 492 features and 1453 training instances after the transformations and pipeline was applied","d2694883":"### 3.3.2 Gradient Boosting Regressor\n- In this section I'll tune a Gradient Boosting Regressor further\n- I'll tune the following hyper parameters:\n    - **loss**: The loss function that is optimized.  I'll choose between 'ls' which is least squares regression, or 'lad' which is least absolute deviation.\n    - **learning_rate**: This affects how much each tree in the gradient booster contributes to the ensembled model.  A lower value means that more trees are required to fit the training set and vice versa.  Lower values normally generalize better to new data.\n    - **n_estimators**: The total number of trees\/boosting stages to perform\n    - **subsample**: The fraction of training instances that is used to train each tree.  For example, if its set to 0.50, 50% of the training instances are used.  A lower value increases bias and reduces variance.\n    - **max_features**: The number of features to look at when determining a split in the tree.  I'll choose between sklearns options 'auto' and 'log' (see sk-learn documentation for further details)\n- You'll see that based on the RandomizedSearchCV 'lad' loss, and 'auto' max_features work best.  Therefore for the finer grid search, I'll set these parameters to these options, and then tune the the remaining hyper parameters further.","5e0b9f5a":"## 3.1 Base Model: Always Predict Mean\n- First, I'll create a Base Model that always predicts the mean value from the train set\n- This will be the model will provide the score to beat for the more formal ML models","dc2686c6":"- It looks like Sale Price has a long tail to the right, with a few houses that sold for excess of 600,000\n- Below I'll pull up these homes to see if there are any common features that jump out","d94c2d63":"## 3.8 Model Ensembling\n- In this section I'll ensemble the models I've tuned to create a voting regressor.\n- Often, when you aggregate multiple models with similar scores\/accuracy, the predictions that you get from an ensembled modeled will actually be better than the individual models\n- This tends to work best if the models themselves are inherently different (e.g. different algorithms)","310d2348":"# Table of Contents\n- [0.0 Introduction, Library Imports, Data Import and Initial Inspection](#0.0-Introduction,-Library-Imports,-Data-Import-and-Initial-Inspection)\n   - [0.1 Project Summary](#0.1-Project-Summary)\n   - [0.2 Introduction to Dataset](#0.2-Introduction-to-Dataset)\n   - [0.3 Overall Goal](#0.3-Overall-Goal)\n   - [0.4 Library Imports](#0.4-Library-Imports)\n   - [0.5 Data Import and Initial Inspection](#0.5-Data-Import-and-Initial-Inspection)\n- [1.0 Exploratory Data Analysis](#1.0-Exploratory-Data-Analysis)\n   - [1.1 Missing Data Quick Look](#1.1-Missing-Data-Quick-Look)\n   - [1.2 Numerical Data Distributions](#1.2-Numerical-Data-Distributions)\n   - [1.3 Analysis Of Sales Price](#1.3-Analysis-Of-Sales-Price)\n       - [1.3.1 Distribution of Sales Price](#1.3.1-Distribution-of-Sales-Price)\n       - [1.3.2 Transformed Target Label](#1.3.2-Transformed-Target-Label)\n   - [1.4 Analysis of Sales Price vs. Some \"\"Common Sense\"\" Features](#1.4-Analysis-of-Sales-Price-vs.-Some-\"\"Common-Sense\"\"-Features)\n       - [1.4.1 Scatterplots of Common Sense Features vs. Sales price](#1.4.1-Scatterplots-of-Common-Sense-Features-vs.-Sales-price)\n       - [1.4.2 Sales Price vs. Overall Quality](#1.4.2-Sales-Price-vs.-Overall-Quality)\n       - [1.4.3 Sales Price vs. Overall Condition](#1.4.3-Sales-Price-vs.-Overall-Condition)\n   - [1.5 Correlation Heat Map and Correlation Matrix](#1.5-Correlation-Heat-Map-and-Correlation-Matrix)\n   - [1.6 Experimenting with Attribute Combinations](#1.6-Experimenting-with-Attribute-Combinations)\n- [2.0 Data Preparation](#2.0-Data-Preparation)\n   - [2.1 Analysis of Missing Data](#2.1-Analysis-of-Missing-Data)\n       - [2.1.1 Missing Data: PoolQC, MiscFeature, Alley](#2.1.1-Missing-Data:-PoolQC,-MiscFeature,-Alley)\n       - [2.1.2 Missing Data: Fence](#2.1.2-Missing-Data:-Fence)\n       - [2.1.3 Missing Data: Fireplace Quality](#2.1.3-Missing-Data:-Fireplace-Quality)\n       - [2.1.4 Missing Data: Lot Frontage](#2.1.4-Missing-Data:-Lot-Frontage)\n       - [2.1.5 Missing Data: Garage Features](#2.1.5-Missing-Data:-Garage-Features)\n       - [2.1.6 Missing Data: Basement Features](#2.1.6-Missing-Data:-Basement-Features)\n       - [2.1.7 Missing Data: Masonry Veneer Features](#2.1.7-Missing-Data:-Masonry-Veneer-Features)\n       - [2.1.8 Missing Data: Electrical](#2.1.8-Missing-Data:-Electrical)\n   - [2.2 Outlier analysis](#2.2-Outlier-analysis)\n       - [2.2.1 Sales Price Outliers](#2.2.1-Sales-Price-Outliers)\n       - [2.2.2 Features with Potential Outliers](#2.2.2-Features-with-Potential-Outliers)\n       - [2.2.3 Lot Frontage: Potential Outliers](#2.2.3-Lot-Frontage:-Potential-Outliers)\n       - [2.2.4 Masonry Veneer Area: Potential Outliers](#2.2.4-Masonry-Veneer-Area:-Potential-Outliers)\n       - [2.2.5 Basement Finished Sq Ft 1: Potential Outliers](#2.2.5-Basement-Finished-Sq-Ft-1:-Potential-Outliers)\n       - [2.2.6 Basement Finished Sq Ft 2: Potential Outliers](#2.2.6-Basement-Finished-Sq-Ft-2:-Potential-Outliers)\n   - [2.3 Missing Data and Outlier Conclusions](#2.3-Missing-Data-and-Outlier-Conclusions)\n   - [2.4 Potential Data Leakage](#2.4-Potential-Data-Leakage)\n   - [2.5 Create Transformer for Custom Features](#2.5-Create-Transformer-for-Custom-Features)\n   - [2.6 Remove Outliers from Data](#2.6-Remove-Outliers-from-Data)\n   - [2.7 Transformation of Target Label](#2.7-Transformation-of-Target-Label)\n   - [2.8 Create data pipeline](#2.8-Create-data-pipeline)\n   - [2.9 Data Preparation Final Notes](#2.9-Data-Preparation-Final-Notes)\n- [3.0 Model Selection and Model Tuning](#3.0-Model-Selection-and-Model-Tuning)\n   - [3.1 Base Model: Always Predict Mean](#3.1-Base-Model:-Always-Predict-Mean)\n   - [3.2 Quick and Dirty Models](#3.2-Quick-and-Dirty-Models)\n       - [3.2.1 Quick and Dirty Models: Training](#3.2.1-Quick-and-Dirty-Models:-Training)\n       - [3.2.2 Quick and Dirty Model Results](#3.2.2-Quick-and-Dirty-Model-Results)\n       - [3.2.3 Quick and Dirty Model Conclusions](#3.2.3-Quick-and-Dirty-Model-Conclusions)\n   - [3.3 Model Tuning](#3.3-Model-Tuning)\n       - [3.3.1 Support Vector Machine Regressor with RBF Kernel](#3.3.1-Support-Vector-Machine-Regressor-with-RBF-Kernel)\n       - [3.3.2 Gradient Boosting Regressor](#3.3.2-Gradient-Boosting-Regressor)\n       - [3.3.3 Random Forest Regressor](#3.3.3-Random-Forest-Regressor)\n       - [3.3.4 ElasticNet](#3.3.4-ElasticNet)\n   - [3.4 Save Models](#3.4-Save-Models)\n   - [3.5 Load Models](#3.5-Load-Models)\n   - [3.6 Score Tuned Models](#3.6-Score-Tuned-Models)\n   - [3.7 Model Learning Curves](#3.7-Model-Learning-Curves)\n   - [3.8 Model Ensembling](#3.8-Model-Ensembling)\n- [4.0 Final Predictions](#4.0-Final-Predictions)\n","79ddf854":"### 1.4.3 Sales Price vs. Overall Condition\n- Here I'll perform a similar analysis to the above, except we'll compare Sales Price to overall condition rather than overall quality","eb47bac0":"### 2.1.2 Missing Data: Fence\n- The fence feature has 81% of data missing\n- Also, based on the categories that it contains, it would be difficult to fill this missing data accurately\n- I'll drop this feature as well","cea60fd6":"### 2.2.4 Masonry Veneer Area: Potential Outliers\n- It looks like masonry veneer area could have some outliers with > 1000 sq ft\n- However, based on the below theres 7 training instances in this category, so I'll just keep it as is","c8e8c12b":"### 2.1.3 Missing Data: Fireplace Quality\n- It looks like fireplace quality has 690 missing values\n- Based on a comparison to the feature Fireplaces (number of fireplaces), it looks like all 690 of these missing values are houses where there are 0 fireplaces\n- Therefore I'll fill these mising data with \"none\"","bfa44d2c":"### 3.3.3 Random Forest Regressor\n- In this section I'll tune a Random Forest Regressor further\n- I'll tune the following hyper parameters:\n    - **n_estimators**: The total number of trees\/boosting stages to perform\n    - **max_features**: The number of features to look at when determining a split in the tree.  I'll choose between sklearns options 'auto' and 'log' (see sk-learn documentation for further details)\n    - **bootstrap**: Whether or not to perform bootstrapping when forming each tree.\n- You'll see that based on the RandomizedSearchCV 'auto' max_features, and 'True' bootstrap work best.  Therefore for the finer grid search, I'll set these parameters to these options, and then tune n_estimators further.","02f77470":"## 1.3 Analysis Of Sales Price\n- Here I'll perfrom some quick analysis over Sales Price, which is the target label of our dataset","20556ec5":"# 4.0 Final Predictions\n- Finally, I'll use the models I've created to make predictions on the test set\n- I'll use each model to get a set of predictions, and submit each to kaggle to see which model does the best\n- Here, you'll see the power of the sk-learn pipeline, we can do everything we did to the train set to the test set in one line of code\n- One important thing to note is that I'll need to re-transform the predictions back to normal $ amounts since I applied a log transfomration to the sales price in creating the models","76eb49b0":"Based on these scatter plots it looks like most of the features have varying strengths of positive correlations with Sales Price, as I expected","2ebc4bf9":"### 2.1.6 Missing Data: Basement Features\n- Similarily, it looks like there are a bunch of basement features with missing data\n- I've compared these features with missing data to the feature that includes the total square footage of the basement\n- It looks like for almost all of them, the total basement square footage is 0\n- For these features, I'll replace the NaN's with \"none\"\n- For training instance 332, I'll just delete this one from our dataset","55cb541f":"### 1.3.1 Distribution of Sales Price","ed8c46f2":"- It looks like PoolQC, MiscFeature, Alley, and Fence all have < 80% of their values missing.\n- FireplaceQu has ~50% missing, and then the remaining categories with missing data have between 0.001% to 20% of data missing.","418843ca":"## 0.1 Project Summary\nThis is my second Kaggle competition and is a regression task.  Specifically, the project involves building a supervised machine learning model to predict the price of a home in Ames Iowa, based on a dataset compiled by Dean De Cock.  I have included a summary of the dataset in section 0.1 of this notebook.  \n\nThis was quite a fun project as there are 80 explanatory features to work with in the dataset, so a significant chunk of the work is feature engineering, which I enjoy.  I've also placed an emphasis on preprocessing data with sk-learn pipelines rather than pandas, as this is much more efficient when it comes time to pre-process the test set.  \n\nAt a high level, I ended up tuning SVM, Random Forest, Gradient Boosting, and Elastic Net Regression models.  I also created an Ensemble Model from these four models.  The model that performed the best on the test set was the SVM Regressor (rbf kernel), with a final score of 0.12274 (it beat the Ensemble model which is interesting!).  One thing to note - as Kaggle already splits the dataset into a train set and a test set, I did not further split the test set again into a test set and validation set.  I chose not to do this since the dataset only has 1,460 training instances.  Rather, I use cross validation when building my ML models.\n\nI hope you enjoy my notebook!\n\nAll work is my own.","a5abd69b":"## 0.2 Introduction to Dataset\nThe dataset that I work with in this notebook is the \"Ames Housing dataset\", created by Dean De Cock.  It contains data on 1,460 homes in Ames, Iowa.  Specifically, there are 80 explanatory features that describe each of these residential homes.  A summary of these features , including their types, is included below:\n\n\n| Feature       | Description                                                            | Type                   |\n|---------------|------------------------------------------------------------------------|------------------------|\n| MSSubClass    | Identifies the type of dwelling involved in the sale                   | Categorical            |\n| MSZoning      | Identifies the general zoning classification of the sale               | Categorical            |\n| LotFrontage   | Linear feet of street connected to property                            | Numerical (continuous) |\n| LotArea       | Lot size in square feet                                                | Numerical (continuous) |\n| Street        | Type of road access to property                                        | Categorical            |\n| Alley         | Type of alley access to property                                       | Categorical            |\n| LotShape      | General shape of property                                              | Categorical            |\n| LandContour   | Flatness of the property                                               | Categorical            |\n| Utilities     | Type of utilities available                                            | Categorical            |\n| LotConfig     | Lot configuration                                                      | Categorical            |\n| LandSlope     | Slope of property                                                      | Categorical            |\n| Neighborhood  | Physical locations within Ames city limits                             | Categorical            |\n| Condition1    | Proximity to various conditions                                        | Categorical            |\n| Condition2    | Proximity to various conditions (if more than one is present)          | Categorical            |\n| BldgType      | Type of dwelling                                                       | Categorical            |\n| HouseStyle    | Style of dwelling                                                      | Categorical            |\n| OverallQual   | Rates the overall material and finish of the house                     | Ordinal                |\n| OverallCond   | Rates the overall condition of the house                               | Ordinal                |\n| YearBuilt     | Original construction date                                             | Categorical            |\n| YearRemodAdd  | Remodel date (same as construction date if no remodeling or additions) | Categorical            |\n| RoofStyle     | Type of roof                                                           | Categorical            |\n| RoofMatl      | Roof material                                                          | Categorical            |\n| Exterior1st   | Exterior covering on house                                             | Categorical            |\n| Exterior2nd   | Exterior covering on house (if more than one material)                 | Categorical            |\n| MasVnrType    | Masonry veneer type                                                    | Categorical            |\n| MasVnrArea    | Masonry veneer area in square feet                                     | Numerical (continuous) |\n| ExterQual     | Evaluates the quality of the material on the exterior                  | Ordinal                |\n| ExterCond     | Evaluates the present condition of the material on the exterior        | Ordinal                |\n| Foundation    | Type of foundation                                                     | Categorical            |\n| BsmtQual      | Evaluates the height of the basement                                   | Categorical\/Ordinal?   |\n| BsmtCond      | Evaluates the general condition of the basement                        | Categorical\/Ordinal?   |\n| BsmtExposure  | Refers to walkout or garden level walls                                | Categorical\/Ordinal?   |\n| BsmtFinType1  | Rating of basement finished area                                       | Categorical\/Ordinal?   |\n| BsmtFinSF1    | Type 1 finished square feet                                            | Numerical (continuous) |\n| BsmtFinType2  | Rating of basement finished area (if multiple types)                   | Categorical\/Ordinal?   |\n| BsmtFinSF2    | Type 2 finished square feet                                            | Numerical (continuous) |\n| BsmtUnfSF     | Unfinished square feet of basement area                                | Numerical (continuous) |\n| TotalBsmtSF   | Total square feet of basement area                                     | Numerical (continuous) |\n| Heating       | Type of heating                                                        | Categorical            |\n| HeatingQC     | Heating quality and condition                                          | Ordinal                |\n| CentralAir    | Central air conditioning                                               | Categorical            |\n| Electrical    | Electrical system                                                      | Categorical            |\n| 1stFlrSF      | First Floor square feet                                                | Numerical (continuous) |\n| 2ndFlrSF      | Second floor square feet                                               | Numerical (continuous) |\n| LowQualFinSF  | Low quality finished square feet (all floors)                          | Numerical (continuous) |\n| GrLivArea     | Above grade (ground) living area square feet                           | Numerical (continuous) |\n| BsmtFullBath  | Basement full bathrooms                                                | Numerical (discrete)   |\n| BsmtHalfBath  | Basement half bathrooms                                                | Numerical (discrete)   |\n| FullBath      | Full bathrooms above grade                                             | Numerical (discrete)   |\n| HalfBath      | Half baths above grade                                                 | Numerical (discrete)   |\n| Bedroom       | Bedrooms above grade (does NOT include basement bedrooms)              | Numerical (discrete)   |\n| Kitchen       | Kitchens above grade                                                   | Numerical (discrete)   |\n| KitchenQual   | Kitchen quality                                                        | Ordinal                |\n| TotRmsAbvGrd  | Total rooms above grade (does not include bathrooms)                   | Numerical (discrete)   |\n| Functional    | Home functionality (Assume typical unless deductions are warranted)    | Ordinal                |\n| Fireplaces    | Number of fireplaces                                                   | Numerical (discrete)   |\n| FireplaceQu   | Fireplace quality                                                      | Categorical\/Ordinal?   |\n| GarageType    | Garage location                                                        | Categorical            |\n| GarageYrBlt   | Year garage was built                                                  | Categorical            |\n| GarageFinish  | Interior finish of the garage                                          | Categorical            |\n| GarageCars    | Size of garage in car capacity                                         | Numerical (discrete)   |\n| GarageArea    | Size of garage in square feet                                          | Numerical (continuous) |\n| GarageQual    | Garage quality                                                         | Categorical\/Ordinal?   |\n| GarageCond    | Garage condition                                                       | Categorical\/Ordinal?   |\n| PavedDrive    | Paved driveway                                                         | Categorical            |\n| WoodDeckSF    | Wood deck area in square feet                                          | Numerical (continuous) |\n| OpenPorchSF   | Open porch area in square feet                                         | Numerical (continuous) |\n| EnclosedPorch | Enclosed porch area in square feet                                     | Numerical (continuous) |\n| 3SsnPorch     | Three season porch area in square feet                                 | Numerical (continuous) |\n| ScreenPorch   | Screen porch area in square feet                                       | Numerical (continuous) |\n| PoolArea      | Pool area in square feet                                               | Numerical (continuous) |\n| PoolQC        | Pool quality                                                           | Categorical\/Ordinal?   |\n| Fence         | Fence quality                                                          | Categorical\/Ordinal?   |\n| MiscFeature   | Miscellaneous feature not covered in other categories                  | Categorical\/Ordinal?   |\n| MiscVal       | Value of miscellaneous feature                                         | Numerical (continuous) |\n| MoSold        | Month Sold (MM)                                                        | Categorical            |\n| YrSold        | Year Sold (YYYY)                                                       | Categorical            |\n| SaleType      | Type of sale                                                           | Categorical            |\n| SaleCondition | Condition of sale                                                      | Categorical            |","482a4512":"# Thank you!\n- If you've reviewd my notebook, thank you very much!\n- Please feel free to let me know if you have any comments\/suggestions ","69ed6c71":"As there are only 4 homes that have a much higher sales price that the majority of the homes, I'll remove them from our dataset","1629667c":"## 2.4 Potential Data Leakage\n- Another thing to note is that the dataset contains the features MoSold (month sold), YrSold (year sold), SaleType, SaleCondition for each house\n- If we leave these in, this would cause data leakage, we are creating a model to determine the future price of a house, which, before it's sold, we would not know the month or year of sale, we would also not know the type of sale or condition of sale\n- Therefore I will remove these features well","f84d80c3":"- Interesting, this is not what I expected, it looks like overall condition and sales price don't have as strong relationship of a relationship as overall quality\n- Category 5 seems to also have quite a skewed distribution","3be3af60":"## 2.7 Transformation of Target Label\n- In this section I'll formally transform our target label, Sales Price\n- In section 1 and 2 I discussed how Sales Price was quite skewed\n- I've removed the outliers in section 2.6, so here as that is left to do is apply the log transformation","1be1984d":"## 2.5 Create Transformer for Custom Features\n- Now I'll create a transformer to add the custom features (aggregated features) that I made earlier in section 1\n- Note, in creating this class I'll inherit BaseEstimator and TransformerMixin from sklearn's base classes.  This gives CustomFeatures access to get_params and set_params from BaseEstimator, and fit_transform from TransformerMixin\n- I'll also include an option to turn on the outdoorsqft and totalbaths features (these features weren't very correlated with sales price)","e722e7ae":"## 1.6 Experimenting with Attribute Combinations\n- Here I'll experiment with some attribute combinations, it order to see if these end up being strongly correlated with Sales Price\n- I have prefixed all of these with two stars ** so that we can easily identify them in the correlation matrix that I'll rerun after creating them\n- Note that I'm creating these within the data_explore copy dataset, if an aggregated feature turns out to be promising, I'll build it into the ML pipeline later\n- Specifically, I've created the following aggregated features:\n    - Total indoor square footage\n    - Total outdoor square footage\n    - Total bathrooms (full and half)\n    - Total rooms above ground (bedrooms and all types of bathrooms)\n    - Total basement and garage square footage","c98b41f8":"### 3.2.3 Quick and Dirty Model Conclusions\n- Based on the above, it looks like the top three models are:\n    - SVR with radial basis kernel\n    - Gradient boosting regressor\n    - Random forest regressor\n- The worst model was elastic net regression\n- Based on this, I'm going to tune the top 3, and also ElasticNet, just too see if I can figure out why it performed so badly, and to see how much I can improve it","20e42b3d":"### 2.2.5 Basement Finished Sq Ft 1: Potential Outliers\n- It looks like BsmtFinSF1 may have some significant outliers\n- Based on the below, training instance 1298 is a significant outlier with a BsmtFinSF1 area of 5644 sq foot (that's a huge basement lol)\n- This training instance is actually the same one with outliers in lot frontage, so it'll be dropped as well","885440bf":"### 2.2.1 Sales Price Outliers\n- In Section 1, I identified that Sales Price was quite skewed - here I dig into this a bit more","e76b1c00":"## 2.6 Remove Outliers from Data\n- In this section I'll remove the training instances noted in the summary of missing data and outlier conclusions from section 2.3\n- I'll also remove the outliers in the target label Sales Price\n- Specifically these are training instances 691, 898, 1169, 1182, 332, 934, 1298\n- I'll take a copy of the dataset first just to ensure we have a untouched dataset to begin with","1437b1a3":"## 3.3 Model Tuning\n- Now I'll tune the models mentioned above, and try to improve their scores\n- For each model, I am going to:\n    - First, perform a RandomizedSearchCV, training 1,000 models in order to determine a rough estimate for the ideal parameters\n    - Second, perform a finer GridSearchCV based on the results of the RandomizedSearchCV, to hone in on the best parameters","611d7a18":"It looks like n_estimators has potential to be tuned further, let's run another quick grid search below:","9626bdc3":"Based on the above learning curves it looks like the SVM and ElasticNet models are fitting the data nicely.  While I was hoping for an RMLSE of 0.10, I think the above is quite decent as well.\n\nIn terms of the random forest and GB models - it looks like these models are overfitting the training set quite a bit.  In a real life scenario, you could attempt to correct this by adding more data.  Another way to address overfitting would be to regularize the models further.  However - I'm going to ensemble them next, and I'm happy with the way the scores are for the purposes of this project - therefore I won't tune them any further.  (some ways that they could have been tuned would be, reducing the number of trees, increasing the number of samples required to split at a leaf etc.)","aebe3827":"### 2.1.8 Missing Data: Electrical\n- Electrical is missing one value\n- I'll replace this with the most common value that comes up in the data","a2cfa9c5":"## 3.6 Score Tuned Models\n- Here I'll use the tuned models to predict on the training set and look at the RLMSE","9287aeb5":"# 0.0 Introduction, Library Imports, Data Import and Initial Inspection","a627af61":"### 2.2.3 Lot Frontage: Potential Outliers\n- It looks like lot frontage may have a couple significant outliers where the frontage is greater than 200 sq ft\n- Based on the below, there are two features with a lot frontage of 313 sq ft, training instances 934 and 1298\n- I'll make a call to drop these two from the data set","faa283df":"## 2.8 Create data pipeline\n- In this section I'll build a pipeline to prepare the data using sklearn\n- Doing the preprocessing in sklearn is beneficial compared to doing it in something like pandas, as oncce the pipeline for preprocessing is built, you can apply it to the test set (or new data), rather than having to re-write the pandas processing code for each dataset\n- This pipeline will also either automatically drop features that I decided to drop earlier, or I'll do it when making the lists of feature labels\n- The pipeline will be split into the following steps:\n    - **Numerical pipeline**: Applied to numerical features.  This pipeline adds the custom numerical features discussed earlier in this notebook, fills the missing numerical data in LotFrontage with the median value, and then scales all the numerical features\n    - **Categorical pipeline 1**: Applied to all categorical featuers except Electrical.  This pipeline imputes \"none\" into the categorical categories with missing data, and then one hot encodes the features.\n    - **Categircal pipeline 2**: Applied to categorical feature Electrical.  Similar to the above, except replaces the missing values in Electrical with the most common occurence, and then one hot encodes\n    - **Ordinal pipeline**: Applied to all ordinal featuers.  This pipeline imputes \"none\" into the ordinal categories with missing data, and then ordinal encodes the features.","c3e60c03":"### 2.1.5 Missing Data: Garage Features\n- It looks like we have a pattern where there are several garage related features with missing data\n- To analyze these, I've created a dataframe that contains the 81 missing values for each feature and compared then to garage area (sq footage of garage) and garage type\n- It looks like all of these features, the garage area is 0, and the garage type is NaN, meaning there is no garage in the house\n- Therefore, I'll replace these NaNs in these categories with \"none\"","673b1c77":"## 3.4 Save Models\n- Now I'll save the models, which is an important step in case I want to work with them again and not have to re-run the above cells\n- This will save a lot of time (especially for the gb and rf models, which took 4-5 hours to train on my computer)","c05de7fb":"- Below, I'll plot a histogram of Sales Price, but I'll exclude the homes with a price in excess of $600,000\n- You'll see that the distribution is still quite skewed, and not a normal distribution which is what we would want","ce4fb7c6":"### 2.1.4 Missing Data: Lot Frontage\n- Lot frontage is missing 259 values, or ~18% of data\n- Based on the below, if we remove the outlier (which is analyzed in the next section), the data is a bit skewed, but not overly skewed.\n- I'll fill the missing values with the median.","9af903c7":"## 0.4 Library Imports\nBelow you can find the imports of the libraries that I used throughout this notebook.","8796ec73":"## 1.2 Numerical Data Distributions\n- In this section I'll take a quick look at the distributions of the numerical features to see if there are any features that appear to have skewed distributions, and potentially contain any outliers","8286410d":"### 3.3.4 ElasticNet\n- In this section I'll tune an Elastic Net model further\n- I'll tune the following hyper parameters:\n    - **alpha**:  Determines the amount of regulariazation to perform.  If = 0, the this is just normal linear regression (least squares)\n    - **l1 ratio**: Determines the mixing parameter of lasso and ridge regressor (the type of penalties, either l1 or l2).  If = 0 then the penalty is l2, if = 1 then the penalty is l1\n - You'll see that based on the RandomizedSearchCV, alpha almost equal to 0 is what works best.  This means that normal linear regression peformed better than regularized regression, which must mean the data is quite linearly seperable.\n - Based on this I won't tune the model further with GridSearchCV","f7b05fd9":"- Based on the above, I will perform the log transformation on Sales Price before building my ML models"}}