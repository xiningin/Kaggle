{"cell_type":{"b07408b1":"code","33705dc8":"code","4da61d19":"code","78c5f1de":"code","08824cda":"code","36ad38fc":"code","a863df5b":"code","21003fd0":"code","5c297cd5":"code","109372b0":"code","8dcb070a":"code","2a809b37":"code","a6a2eeff":"code","e0d89ad3":"markdown","75d1632f":"markdown","6b37ba78":"markdown","038f71ce":"markdown","b5e366c3":"markdown","013b64b9":"markdown","9a0b4e98":"markdown","6fc06fe0":"markdown"},"source":{"b07408b1":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))","33705dc8":"random_state = 42\n\nn_samples = 1000\nn_features = 10\nn_classes = 2\n\nnoise_moon = 0.3\nnoise_circle = 0.3\nnoise_class = 0.3\n\nX,y = make_classification(n_samples= n_samples,\n                    n_features= n_features,\n                    n_classes = n_classes,\n                    n_repeated = 0,\n                    n_redundant= 0,\n                    n_informative = n_features-1,\n                    random_state = random_state,\n                    n_clusters_per_class = 1,\n                    flip_y = noise_class)","4da61d19":"data = pd.DataFrame(X)\ndata[\"target\"] = y\nplt.figure(figsize = (8,6))\nsns.scatterplot(x = data.iloc[:,0], y = data.iloc[:,1], hue = \"target\", data = data)\nplt.show()","78c5f1de":"data_classification = (X,y)","08824cda":"moon = make_moons(n_samples = n_samples, noise = noise_moon, random_state = random_state)\ndata = pd.DataFrame(moon[0])\ndata[\"target\"] = moon[1]\nplt.figure(figsize = (8,6))\nsns.scatterplot(x = data.iloc[:,0], y = data.iloc[:,1], hue = \"target\", data = data)\nplt.show()","36ad38fc":"circle = make_circles(n_samples = n_samples,factor = 0.1, noise = noise_circle, random_state = random_state)\ndata = pd.DataFrame(circle[0])\ndata[\"target\"] = circle[1]\nplt.figure(figsize = (8,6))\nsns.scatterplot(x = data.iloc[:,0], y = data.iloc[:,1], hue = \"target\", data = data)\nplt.show()","a863df5b":"datasets = [moon, circle]","21003fd0":"n_estimators = 10\n\nsvc = SVC()\nknn = KNeighborsClassifier(n_neighbors=15)\ndt = DecisionTreeClassifier(random_state = random_state, max_depth= 2)","5c297cd5":"rf = RandomForestClassifier(n_estimators= n_estimators, random_state= random_state)","109372b0":"ada = AdaBoostClassifier(base_estimator = dt, n_estimators=n_estimators, random_state = random_state)","8dcb070a":"v1 = VotingClassifier(estimators=[(\"svc\", svc),(\"knn\", knn),(\"dt\", dt),(\"rf\", rf),(\"ada\", ada)])","2a809b37":"names = [\"SVC\", \"KNN\", \"Decision Tree\",\"Random Forest\",\"AdaBoost\",\"V1\"]\nclassifiers = [svc, knn, dt, rf, ada, v1]","a6a2eeff":"h = 0.2\ni = 1\nfigure = plt.figure(figsize = (18,6))\n\nfor ds_cnt, ds in enumerate(datasets):\n    \n    X,y = ds\n    X = RobustScaler().fit_transform(X)\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.4, random_state = random_state)\n    \n    x_min, x_max = X[:,0].min() -0.5, X[:,0].max() + 0.5\n    y_min, y_max = X[:,1].min() -0.5, X[:,1].max() + 0.5\n    #g\u00f6rselle\u015ftirme i\u00e7in\n    xx,yy = np.meshgrid(np.arange(x_min, x_max, h),\n                       np.arange(y_min, y_max, h))\n    \n    cm = plt.cm.RdBu\n    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n    \n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    \n    if ds_cnt == 0:\n        ax.set_title(\"Input Data\")\n        \n    ax.scatter(X_train[:,0], X_train[:,1], c = y_train, cmap = cm_bright, edgecolors = 'k')\n    #Plot the testing points\n    ax.scatter(X_test[:,0], X_test[:,1], c = y_test, cmap = cm_bright, alpha = 0.6, marker = '^', edgecolors = 'k')\n    \n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n    \n    print(\"Dataset # {}\". format(ds_cnt))\n    \n    for name, clf in zip(names, classifiers):\n        \n        ax= plt.subplot(len(datasets), len(classifiers) + 1, i)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print(\"{}: test set score: {}\". format(name, score))\n        score_train = clf.score(X_train, y_train)\n        print(\"{}: train set score. {}\".format(name, score_train))\n        print()\n    \n    \n        if hasattr(clf, \"decision_function\"):\n            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n        else:\n            Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n        \n        #Put the result into a color plot\n    \n        Z = Z.reshape(xx.shape)\n        ax.contourf(xx, yy, Z, cmap = cm, alpha = 0.8)\n    \n        #Plot the training points\n        ax.scatter(X_train[:,0], X_train[:,1], c = y_train, cmap = cm_bright,\n              edgecolors = 'k')\n    \n        #Plot the testing points\n    \n        ax.scatter(X_test[:,0], X_test[:,1], c = y_test, cmap = cm_bright,marker = '^',\n              edgecolors = 'white', alpha = 0.6)\n    \n        ax.set_xticks(())\n        ax.set_yticks(())\n    \n        if ds_cnt == 0:\n            ax.set_title(name)\n        score = score*100\n        ax.text(xx.max() - 0.3, yy.min() + 0.3, ('%.1f' % score),\n               size = 15, horizontalalignment = 'right')\n    \n        i += 1\n    \n    print(\"----------------------------------------\")\nplt.tight_layout()\nplt.show()\n\n\ndef make_classify(dc, clf, name):\n    \n    x,y = dc\n    x = RobustScaler().fit_transform(x)\n    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 4, random_state = random_state)\n    \n    for name, clf in zip(names, classifiers):\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        print(\"{}: test set score: {}\" .format(name,score))\n        score_train = clf.score(X_train, y_train)\n        print(\"{}: train set score: {}\".format(name, score_train))\n        print()\n        \nprint(\"Dataset #2\")\nmake_classify(data_classification, classifiers, names)\n","e0d89ad3":"<a id = \"5\"><\/a>\n# Adaptive Boosting Classifier\n\nAn AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.","75d1632f":"<a id = \"4\"><\/a>\n# Random Forest Classifier (Bagging)\n\nBAGGing, or Bootstrap AGGregating. BAGGing gets its name because it combines Bootstrapping and Aggregation to form one ensemble model. Given a sample of data, multiple bootstrapped subsamples are pulled. A Decision Tree is formed on each of the bootstrapped subsamples. After each subsample Decision Tree has been formed, an algorithm is used to aggregate over the Decision Trees to form the most efficient predictor.\n\nRandom Forest Models can be thought of as BAGGing, with a slight tweak. When deciding where to split and how to make decisions, BAGGed Decision Trees have the full disposal of features to choose from. Therefore, although the bootstrapped samples may be slightly different, the data is largely going to break off at the same features throughout each model. In contrary, Random Forest models decide where to split based on a random selection of features. Rather than splitting at similar features at each node throughout, Random Forest models implement a level of differentiation because each tree will split based on different features. This level of differentiation provides a greater ensemble to aggregate over, ergo producing a more accurate predictor.","6b37ba78":"<a id = \"1\"><\/a>\n# Libraries and Utilities","038f71ce":"<a id = \"7\"><\/a>\n# Models","b5e366c3":"<a id = \"2\"><\/a>\n# Creating Datasets","013b64b9":"# Ensemble Learning\n\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n\n\n1. [Libraries and Utilities](#1)\n2. [Creating Datasets](#2)\n3. [Basic Classifiers: SVM, KNN, DT](#3)\n4. [Random Forest Classifier (Bagging)](#4)\n5. [Adaptive Boosting Classifier](#5)\n6. [Voting Classifier](#6)\n7. [Models](#7)","9a0b4e98":"<a id = \"3\"><\/a>\n# Basic Classifiers: SVM, KNN, DT","6fc06fe0":"<a id = \"6\"><\/a>\n# Voting Classifier\nA Voting Classifier is a machine learning model that trains on an ensemble of numerous models and predicts an output (class) based on their highest probability of chosen class as the output.\nIt simply aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based on the highest majority of voting. The idea is instead of creating separate dedicated models and finding the accuracy for each them, we create a single model which trains by these models and predicts output based on their combined majority of voting for each output class.\n\nVoting Classifier supports two types of votings.\n\n1. **Hard Voting:** In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction.\n2. **Soft Voting:** In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier."}}