{"cell_type":{"a0fc298c":"code","0744945c":"code","9fb5bf84":"code","6ea8e85a":"code","774d5fb7":"code","5dbd401b":"code","4e7e8ddc":"code","fbf517cd":"code","fc03a7de":"code","11e42bb8":"code","08964963":"code","546c362f":"code","5b04f655":"code","8f93be3e":"code","9c6e863a":"code","d2d4c2b1":"code","9165712c":"code","1941efd3":"code","93257583":"code","99117e9c":"code","fc4c5d8c":"code","b943da43":"code","c80b8196":"code","044bbe4e":"code","c60ece33":"code","db7edae2":"code","963d7692":"code","acc940a6":"code","017e18c0":"code","fb0b54bb":"markdown","4ef76f7f":"markdown","01c03660":"markdown","5da4279c":"markdown","233aba09":"markdown","7792e023":"markdown","8d98b0fe":"markdown","1e4d012a":"markdown","647b8a84":"markdown","587e6545":"markdown","caae6fb5":"markdown","225fcdb9":"markdown","4abeb147":"markdown","84a4b2fb":"markdown"},"source":{"a0fc298c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection,linear_model, metrics\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0744945c":"cr_data = pd.read_csv(\"\/kaggle\/input\/credit-risk-dataset\/credit_risk_dataset.csv\")\nshape = cr_data.shape\nprint(\"There are {} rows and {} features.\".format(shape[0], shape[1]))\nprint(cr_data.dtypes)\ncr_data","9fb5bf84":"# we will shorten the last 2 feature names and address the null values\ncr_data = cr_data.rename(columns = {\"cb_person_default_on_file\":\"default_hist\", \"cb_person_cred_hist_length\": \"cr_hist_len\"})\ncr_data.isnull().sum()","6ea8e85a":"# percentage of null values from loan int rate col\ncr_data.loan_int_rate.isnull().sum() \/ cr_data.shape[0]","774d5fb7":"plt.hist(cr_data['person_emp_length'])\nplt.xlabel(\"Employment Length\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Freq vs Employment Length\")\nplt.show()\n\nplt.hist(cr_data['loan_int_rate'])\nplt.xlabel(\"Interest Rate\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Freq vs Interest Rate\")","5dbd401b":"emp_len_null = cr_data[cr_data['person_emp_length'].isnull()].index\nint_rate_null = cr_data[cr_data['loan_int_rate'].isnull()].index\n\ncr_data['person_emp_length'].fillna((cr_data['person_emp_length'].median()), inplace=True)\ncr_data['loan_int_rate'].fillna((cr_data['loan_int_rate'].median()), inplace = True)","4e7e8ddc":"# check distribution of age and interest rate\n\n\ncolors = [\"blue\",\"red\"]\nplt.scatter(cr_data['person_age'], cr_data['loan_int_rate'],\n            c = cr_data['loan_status'],\n            cmap = mpl.colors.ListedColormap(colors), alpha=0.5)\nplt.xlabel(\"Person Age\")\nplt.ylabel(\"Loan Interest Rate\")\nplt.title(\"Interest Rate vs Age\")\n","fbf517cd":"# Clean 1\ncr_clean1 = cr_data[cr_data['person_age']<=100]\n\ncr_data[cr_data['person_age']>100]","fc03a7de":"pd.crosstab(cr_clean1['default_hist'], cr_clean1['loan_grade'])","11e42bb8":"# note 0 is non default and 1 is default\ndefault_hist_status_tab = pd.crosstab(cr_clean1['default_hist'], cr_clean1['loan_status'])\ndefault_hist_status_tab","08964963":"total1 = default_hist_status_tab.iloc[0].sum()\ndefaulted1 = default_hist_status_tab.iloc[0,1]\n\ntotal2 = default_hist_status_tab.iloc[1].sum()\ndefaulted2 = default_hist_status_tab.iloc[1,1]\n\nfirst_default = round(defaulted1 \/ total1 * 100, 2)\nsecond_default = round(defaulted2 \/ total2 * 100, 2)\n\nprint(\"Despite the measures taken, {}% of clients defaulted for the first time.\".format(first_default))\nprint(\"And {}% of clients who had previously defaulted, defaulted again.\".format(second_default))","546c362f":"pd.crosstab(cr_clean1['default_hist'], cr_clean1['loan_intent'], \n            values = cr_clean1['loan_int_rate'], aggfunc = 'median')","5b04f655":"cr_clean1","8f93be3e":"# one hot encoding categorical variables\nnum_col = cr_clean1.select_dtypes(exclude = 'object')\nchar_col = cr_clean1.select_dtypes(include = 'object')\n\nencoded_char_col = pd.get_dummies(char_col)\n\ncr_clean2 = pd.concat([num_col, encoded_char_col], axis=1)\ncr_clean2","9c6e863a":"# Split Train and Test Sets\nY = cr_clean2['loan_status']\nX = cr_clean2.drop('loan_status',axis=1)\n \n\n\nx_train, x_test, y_train, y_test = model_selection.train_test_split(X, Y, random_state=2020, test_size=.30)\n\n#Start of Classification Logistics Regression\n\nlog_clf = linear_model.LogisticRegression()\n\nlog_clf.fit(x_train, np.ravel(y_train))","d2d4c2b1":"col_effect = pd.DataFrame()\ncol_effect['col_names'] = X.columns\ncol_effect['col_coef'] = log_clf.coef_[0]\ncol_effect","9165712c":"int_val = float(log_clf.intercept_)\nprint('The overall probablity of non default is {:.3%}'.format(int_val))\n","1941efd3":"# first column is the logistic regression value\n# second column is the predicted probability of default == 1\npredict_log = pd.DataFrame(log_clf.predict_proba(x_test)[:,1], columns=['prob_default'])\n\npred_df = pd.concat([y_test.reset_index(drop=True), predict_log],axis=1)\npred_df","93257583":"# check the accuracy\ninitial_accuracy = round(log_clf.score(x_test,  y_test),2)\nprint(\"The initial accuracy is {}\".format(initial_accuracy))","99117e9c":"thresh = np.linspace(0,1,21)\nthresh","fc4c5d8c":"metrics.recall_score(pred_df.iloc[:,0],y_test, labels = [0,1])","b943da43":"def find_opt_thresh(predict,thr =thresh, y_true = y_test):\n    data = predict\n    \n    def_recalls = []\n    nondef_recalls = []\n    accs =[]\n\n    \n    for threshold in thr:\n        # predicted values for each threshold\n        data['loan_status'] = data['prob_default'].apply(lambda x: 1 if x > threshold else 0 )\n        \n        accs.append(metrics.accuracy_score(y_true, data['loan_status']))\n        \n        stats = metrics.precision_recall_fscore_support(y_true, data['loan_status'])\n        \n        def_recalls.append(stats[1][1])\n        nondef_recalls.append(stats[1][0])\n        \n        \n    return accs, def_recalls, nondef_recalls\n\naccs, def_recalls, nondef_recalls= find_opt_thresh(pred_df)","c80b8196":"plt.plot(thresh,def_recalls)\nplt.plot(thresh,nondef_recalls)\nplt.plot(thresh,accs)\nplt.xlabel(\"Probability Threshold\")\nplt.xticks(thresh, rotation = 'vertical')\nplt.legend([\"Default Recall\",\"Non-default Recall\",\"Model Accuracy\"])\n#plt.axvline(x=0.45, color='pink')\nplt.show()\n","044bbe4e":"max_accuracy_index = accs.index(max(accs))\n\nprint('The maximum accuracy is {:.0%}.'.format(accs[max_accuracy_index]))\nprint('Therefore we should have a threshold of {:.0%}.'.format(thresh[max_accuracy_index]))","c60ece33":"cr_clean2","db7edae2":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=[0, 1])\ndata_rescaled = scaler.fit_transform(cr_clean2)\n\n#Fitting the PCA algorithm with our Data\npca = PCA().fit(data_rescaled)\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Hotel Booking Dataset Explained Variance')\nplt.show()","963d7692":"# normalize data\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\npie = cr_clean2.drop('loan_status',axis=1)\n\ndata_scaled = pd.DataFrame(preprocessing.scale(pie),columns = pie.columns) \n\n# PCA\npca = PCA(n_components=14)\npca_val = pca.fit_transform(data_scaled)\npca_dataset = pd.DataFrame(pca_val)","acc940a6":"x_train, x_test, y_train, y_test = model_selection.train_test_split(pca_dataset, Y, random_state=2020, test_size=.32)\n\n#Start of Classification Logistics Regression\n\nlog_clf = linear_model.LogisticRegression()\n\nlog_clf.fit(x_train, np.ravel(y_train))\n\n# first column is the logistic regression value\n# second column is the predicted probability of default == 1\npca_predict_log = pd.DataFrame(log_clf.predict_proba(x_test)[:,1], columns=['prob_default'])\n\npca_pred_df = pd.concat([y_test.reset_index(drop=True), predict_log],axis=1)\npca_pred_df\n\npca_accuracy = round(log_clf.score(x_test,  y_test),2)\npca_accuracy\n","017e18c0":"round(default_hist_status_tab.iloc[:,1].sum() \/ pca_dataset.shape[0],2)\n","fb0b54bb":"Both features are not normally distributed. Therefore we will fill the NaNs with the median values for both the loan interest rate and employment length features.","4ef76f7f":"Now let's consider if there are outliers.","01c03660":"There are individuals who are above 120 years of age with loans and are unlikely to apply for new loans in the future. Therefore we will remove individuals who exceed 100 years of age. \n\nThere is no outlier for loan interest rates.","5da4279c":"This tells  to deaultfor every one unit of increase in each column, the person is more likely when the coefficient is more positive and less likely when the coefficient is more negative.","233aba09":"In the figure above, I have identified 14 components would be optiminal number to have the most simplied model with the most amount of information.","7792e023":"We can use he previous the intercept and coefficient values of calculate the probability of default ( P = 1 ) and non default ( P = 0 ).\n\nFirst we need the sum of the intercept and coefficients x column value. For example, int_coef_sum = intercept + [col_coef] X [col_values].\n\nThen we can calculate the probabilities of default and non default witht eh logistic regression formula.\n\nprob_default = 1\/ (1 + np.exp(-int_coef_sum))\n\nprob_nondefault = 1 - prob_default ","8d98b0fe":"The entries shown above have been removed and created a cleaned dataset saved as `cr_clean1`.","1e4d012a":"# *Summary*\n### In this notebook, the credit risk data was cleaned, explored for better understanding of the current credit risk situation, and modelled the data to accurately predict the probability of default of a loan. This can be used to automate approving and declining loan applcations more accurately.\n\n### An 86% accuracy level was achieved in predicting the loan defaults on 32,576 loans and 12 benchmarks. With this model, the default rate would decrease by 8%, resulting in minimized risk for both the lender and applicant.\n   \n","647b8a84":"We want to whether there is a more suitable threshold to improve our accuracy.","587e6545":"There is no surpise here as we see the lender focuses on issuing higher grade loans to clients with better credit history and less loans to those with worse credit history.","caae6fb5":"Those who had not previously defaulted has a median loan interest rate 4% less than those who have defaulted. Issing a loan to client who may default has negative outcomes not only for the lender but also long term negative consequences for the client. We will use machine learning algorithms to improve credit risk modelling to reduce risk for both the lender and client.","225fcdb9":"# Further optimize the accuracy level with PCA","4abeb147":"We have improved the accuracy of our model from ****81% to 86%**** by leveraging ****Principle Component Analysis*** and ****hyperparameter tuning****.\n\nThe current process for credit assessment had a default rate of 22% as shown below. The new credit risk assessment algorithm which we had develop with principal component analysis and logistic regression had a reduced default rate from ****22% to 14%****  (1-0.86). \n\nThe ****5% increase in accuracy resulted in the an 8% reduction of defaulted loans****, minimizing the lender's risk and improving their confidence to lend credit.","84a4b2fb":"There are 2 features which has null values. Since the  relative to our sample size, we will investigate their distributions and decide how to fill the NaNs."}}