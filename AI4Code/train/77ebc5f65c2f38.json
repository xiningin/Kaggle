{"cell_type":{"1a78fea5":"code","b50c5f44":"code","6f9df14b":"code","2f389706":"code","0cced86c":"code","87d6b3d9":"code","7a2477b7":"code","27a5482c":"code","36efe0b1":"code","8151e1bc":"code","b6acd2e9":"code","3f430da2":"code","0bf34f39":"code","d99804e1":"code","a556eacf":"code","31fddc23":"code","77b184e6":"code","8af1e204":"code","fd6f829b":"code","033e2969":"code","cf2907e2":"code","b82d4320":"code","c71093d7":"code","979cfee0":"code","72f844a7":"code","3c1dcc8b":"code","d749189d":"code","c8e81f21":"code","9ac678f4":"code","3ec01940":"code","2f8ce688":"code","2bab9859":"code","e7f83f3a":"code","953a58a8":"code","c40f0bd9":"code","788e3c7e":"code","6812e3ef":"code","de22d253":"code","fa94f076":"code","ef247254":"code","184c3946":"code","989b5afe":"code","822539f2":"code","ffabbeb8":"code","8154b3e9":"code","5e969d82":"code","627b7a00":"code","ab4b393b":"code","657a17b0":"code","2bf7bc25":"code","97f6dacb":"code","509cfe9a":"code","9acc0563":"code","bcdc1c0c":"code","37d0c86d":"code","493ac13c":"code","190db3fa":"code","0d265ff7":"code","58ba3426":"code","dae4a48b":"code","7343ac86":"code","12444a27":"code","812e2450":"code","027c3dd5":"code","15a29758":"code","a22fd4c1":"code","6facf719":"code","6bcfa976":"code","3b9c4be8":"code","bb0c085f":"code","affc6112":"code","2b3c231a":"code","94f45e5a":"code","f315741f":"code","c3b1e24a":"code","ba929486":"code","58630c83":"code","febcd3ad":"code","fcbe0310":"code","39b6022c":"code","7a71a63b":"code","3c932585":"code","904b6a03":"code","4e660a52":"code","89c1ece8":"code","533fa2ea":"code","c9e43a6a":"code","951abe74":"code","0b7fdf51":"code","b7707eb1":"code","d4afbbf0":"code","e4585a52":"code","940c2cb9":"markdown","3ac3acc1":"markdown","36316a5b":"markdown","0fa1fec0":"markdown","2a5e3efd":"markdown","49206fb6":"markdown","8cbdd1c1":"markdown","c015b73b":"markdown","cbd9b74d":"markdown","c63f96ec":"markdown","21fbfac3":"markdown","650dc07b":"markdown","9897697f":"markdown","eec60552":"markdown","1f75139c":"markdown","bb01e929":"markdown","11a75a9f":"markdown","43da3685":"markdown","7606f2fe":"markdown","3d6ec00b":"markdown","e6969394":"markdown","72d7d822":"markdown","54bc75b3":"markdown","039c572e":"markdown","ca859937":"markdown","c2423978":"markdown","6e8eccb1":"markdown"},"source":{"1a78fea5":"# Import basic libraries. Other libraries will be added as and when required\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\npd.set_option(\"display.max_columns\", None)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b50c5f44":"df = pd.read_csv(\"\/kaggle\/input\/hotel-booking-demand\/hotel_bookings.csv\")\ndf.head()","6f9df14b":"df.drop([\"arrival_date_year\"], axis = 1, inplace = True)","2f389706":"df.drop([\"arrival_date_week_number\"], axis = 1, inplace = True)","0cced86c":"df[\"stays_in_nights\"] = df[\"stays_in_weekend_nights\"] + df[\"stays_in_weekend_nights\"]","87d6b3d9":"df.isnull().sum()","7a2477b7":"df.children.fillna(0, inplace = True)","27a5482c":"df.country.fillna(\"Unknown\", inplace = True)","36efe0b1":"df.agent.fillna(0, inplace = True)","8151e1bc":"df.company.fillna(0, inplace = True)","b6acd2e9":"df.isnull().sum()","3f430da2":"df.drop(\"babies\", axis = 1, inplace = True)","0bf34f39":"df[\"paying_guests\"] = df[\"adults\"] ","d99804e1":"df.drop([\"adults\", \"children\"], axis = 1, inplace = True)","a556eacf":"df.meal.unique()","31fddc23":"df.meal.value_counts()","77b184e6":"df.meal.replace(to_replace = dict(Undefined = \"SC\"), inplace = True)","8af1e204":"df.market_segment.value_counts()","fd6f829b":"df.drop(df[df[\"market_segment\"] == \"Undefined\"].index, inplace = True)","033e2969":"df.distribution_channel.value_counts()","cf2907e2":"df.drop(df[df[\"distribution_channel\"] == \"Undefined\"].index, inplace = True)","b82d4320":"df.adr.describe()","c71093d7":"df.drop(df[df[\"adr\"] == -6.38].index, inplace = True)","979cfee0":"df.drop(df[df[\"adr\"] == 5400].index, inplace = True)","72f844a7":"df.adr.describe()","3c1dcc8b":"df.reservation_status.value_counts()","d749189d":"df.is_canceled.value_counts()","c8e81f21":"df.drop(\"reservation_status\", axis = 1, inplace = True)","9ac678f4":"df.drop(\"reservation_status_date\", axis = 1, inplace = True)","3ec01940":"df[\"price\"] = df[\"adr\"] * df[\"paying_guests\"]","2f8ce688":"df.drop([\"adr\"], axis = 1, inplace = True)","2bab9859":"df.info()","e7f83f3a":"df.describe()","953a58a8":"corr = df.corr()\nsns.heatmap(corr,\n           xticklabels = corr.columns,\n           yticklabels = corr.columns)","c40f0bd9":"# Convert categorical values to numeric using label encoder\nfrom sklearn import preprocessing\nfrom collections import defaultdict\nd = defaultdict(preprocessing.LabelEncoder)\n\n# Encoding the categorical variable\nfit = df.select_dtypes(include=['object']).fillna('NA').apply(lambda x: d[x.name].fit_transform(x))\n\n#Convert the categorical columns based on encoding\nfor i in list(d.keys()):\n    df[i] = d[i].transform(df[i].fillna('NA'))","788e3c7e":"features = df[df.columns.difference(['is_canceled'])]\nlabels = df['is_canceled']","6812e3ef":"import pandas as pd\nimport numpy as np\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport traceback\nimport string\n\nmax_bin = 20\nforce_bin = 3\n\n# define a binning function\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)\n\ndef data_vars(df1, target):\n    \n    stack = traceback.extract_stack()\n    filename, lineno, function_name, code = stack[-2]\n    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n    \n    x = df1.dtypes.index\n    count = -1\n    \n    for i in x:\n        if i.upper() not in (final.upper()):\n            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n                conv = mono_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i\n                count = count + 1\n            else:\n                conv = char_bin(target, df1[i])\n                conv[\"VAR_NAME\"] = i            \n                count = count + 1\n                \n            if count == 0:\n                iv_df = conv\n            else:\n                iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv)","de22d253":"final_iv, IV = data_vars(df[df.columns.difference([\"is_canceled\"])],df.is_canceled)","fa94f076":"final_iv","ef247254":"IV = IV.rename(columns={'VAR_NAME':'index'})\nIV.sort_values(['IV'],ascending=0)","184c3946":"transform_vars_list = df.columns.difference(['is_canceled'])\ntransform_prefix = 'new_' # leave this value blank if you need replace the original column values","989b5afe":"transform_vars_list","822539f2":"for var in transform_vars_list:\n    small_df = final_iv[final_iv['VAR_NAME'] == var]\n    transform_dict = dict(zip(small_df.MAX_VALUE,small_df.WOE))\n    replace_cmd = ''\n    replace_cmd1 = ''\n    for i in sorted(transform_dict.items()):\n        replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n        replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n    replace_cmd = replace_cmd + '0'\n    replace_cmd1 = replace_cmd1 + '0'\n    if replace_cmd != '0':\n        try:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd))\n        except:\n            df[transform_prefix + var] = df[var].apply(lambda x: eval(replace_cmd1))","ffabbeb8":"df.head()","8154b3e9":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\n\nclf.fit(features,labels)\n\npreds = clf.predict(features)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(preds,labels)\nprint(accuracy)","5e969d82":"from pandas import DataFrame\nVI = DataFrame(clf.feature_importances_, columns = [\"RF\"], index=features.columns)","627b7a00":"VI = VI.reset_index()\nVI.sort_values(['RF'],ascending=0)","ab4b393b":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select = 20)\nfit = rfe.fit(features, labels)\n","657a17b0":"from pandas import DataFrame\nSelected = DataFrame(rfe.support_, columns = [\"RFE\"], index=features.columns)\nSelected = Selected.reset_index()","2bf7bc25":"Selected[Selected[\"RFE\"] == True]","97f6dacb":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier()\nmodel.fit(features, labels)\n\nprint(model.feature_importances_)","509cfe9a":"from pandas import DataFrame\nFI = DataFrame(model.feature_importances_, columns = [\"Extratrees\"], index=features.columns)","9acc0563":"FI = FI.reset_index()","bcdc1c0c":"FI.sort_values([\"Extratrees\"], ascending = False)","37d0c86d":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nmodel = SelectKBest(score_func=chi2, k=5)\nfit = model.fit(features.abs(), labels)","493ac13c":"from pandas import DataFrame\npd.options.display.float_format = '{:.2f}'.format\nchi_sq = DataFrame(fit.scores_, columns = [\"Chi_Square\"], index=features.columns)\n","190db3fa":"chi_sq = chi_sq.reset_index()","0d265ff7":"chi_sq.sort_values('Chi_Square',ascending=0)","58ba3426":"from sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(features, labels)\nmodel = SelectFromModel(lsvc,prefit=True)","dae4a48b":"from pandas import DataFrame\nl1 = DataFrame(model.get_support(), columns = [\"L1\"], index=features.columns)\nl1 = l1.reset_index()","7343ac86":"l1[l1['L1'] == True]","12444a27":"from functools import reduce\ndfs = [IV, VI, Selected, FI, chi_sq, l1]\nfinal_results = reduce(lambda left,right: pd.merge(left,right,on='index'), dfs)","812e2450":"columns = ['IV', 'RF', 'Extratrees', 'Chi_Square']\n\nscore_table = pd.DataFrame({},[])\nscore_table['index'] = final_results['index']\n\nfor i in columns:\n    score_table[i] = final_results['index'].isin(list(final_results.nlargest(5,i)['index'])).astype(int)\n    \nscore_table['RFE'] = final_results['RFE'].astype(int)\nscore_table['L1'] = final_results['L1'].astype(int)","027c3dd5":"score_table['final_score'] = score_table.sum(axis=1)","15a29758":"score_table.sort_values('final_score',ascending=0)","a22fd4c1":"x = df.lead_time\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot lead_time vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","6facf719":"x = df.deposit_type\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot deposit_type vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","6bcfa976":"x = df.country\ny = df.is_canceled\narea = np.pi*3\n\n# Plot\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.title('Scatter plot country vs cancellation')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","3b9c4be8":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n","bb0c085f":"def calculate_vif(features):\n    vif = pd.DataFrame()\n    vif[\"Features\"] = features.columns\n    vif[\"VIF\"] = [variance_inflation_factor(features.values, i) for i in range(features.shape[1])]    \n    return(vif)","affc6112":"features = features[list(score_table[score_table['final_score'] >= 2]['index'])]","2b3c231a":"vif = calculate_vif(features)\nwhile vif['VIF'][vif['VIF'] > 10].any():\n    remove = vif.sort_values('VIF',ascending=0)['Features'][:1]\n    features.drop(remove,axis=1,inplace=True)\n    vif = calculate_vif(features)","94f45e5a":"list(vif['Features'])","f315741f":"final_vars = list(vif['Features']) + [\"is_canceled\"]","c3b1e24a":"df1 = df[final_vars].fillna(0)","ba929486":"df1.describe()","58630c83":"bar_color = '#058caa'\nnum_color = '#ed8549'\n\nfinal_iv,_ = data_vars(df1,df1['is_canceled'])\nfinal_iv = final_iv[(final_iv.VAR_NAME != 'is_canceled')]\ngrouped = final_iv.groupby(['VAR_NAME'])\nfor key, group in grouped:\n    ax = group.plot('MIN_VALUE','EVENT_RATE',kind='bar',color=bar_color,linewidth=1.0,edgecolor=['black'])\n    ax.set_title(str(key) + \" vs \" + str('is_canceled'))\n    ax.set_xlabel(key)\n    ax.set_ylabel(str('is_canceled') + \" %\")\n    rects = ax.patches\n    for rect in rects:\n        height = rect.get_height()\n        ax.text(rect.get_x()+rect.get_width()\/2., 1.01*height, str(round(height*100,1)) + '%', \n                ha='center', va='bottom', color=num_color, fontweight='bold')","febcd3ad":"x = df1.iloc[:, :-1]\ny = df1.iloc[:, -1]","fcbe0310":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)","39b6022c":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nx_train = sc_X.fit_transform(x_train)\nx_test = sc_X.transform(x_test)\n'''sc_y = StandardScaler()\ny_train = sc_y.fit_transform(y_train)''' #since already its categorical dep variable\n","7a71a63b":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression() #classifier is the object of logistic reg class\nclassifier.fit(x_train, y_train)\n","3c932585":"pred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)","904b6a03":"from sklearn.metrics import confusion_matrix \ncm = confusion_matrix(y_test, pred_test)                                           \ncm","4e660a52":"pd.crosstab(y_train,pd.Series(pred_train),rownames=['ACTUAL'],colnames=['PRED'])","89c1ece8":"from sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\n\nprint(accuracy_train,accuracy_test)","533fa2ea":"from sklearn.naive_bayes import GaussianNB \nclassifier = GaussianNB()\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\nprint(accuracy_train,accuracy_test)","c9e43a6a":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier()\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)\n\nprint(accuracy_train,accuracy_test)","951abe74":"plt.scatter(y_test, pred_test, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"pred_test\")\nplt.show()","0b7fdf51":"cm = confusion_matrix(y_test, pred_test)                                           \ncm","b7707eb1":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 500, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(3, 10, num = 1)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\nrf_random.fit(x_train, y_train)\n\nprint(rf_random.best_params_)","d4afbbf0":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(**rf_random.best_params_)\n\nclassifier.fit(x_train,y_train)\n\npred_train = classifier.predict(x_train)\npred_test = classifier.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_train = accuracy_score(pred_train,y_train)\naccuracy_test = accuracy_score(pred_test,y_test)","e4585a52":"print(accuracy_train, accuracy_test)","940c2cb9":"Terrible score!! drop the idea of naaaivee bayes immediately.","3ac3acc1":"Read and inspect the dataset.","36316a5b":"**L1 for Feature Selection**","0fa1fec0":"# Feature Selection\n**Weight of Evidence and Information Value**\n(reference from Sundar Balkrishnan's github repository)","2a5e3efd":"good score but we can check for better and worse","49206fb6":"**Combine all**","8cbdd1c1":"**Extra Trees Classifier for feature selection**","c015b73b":"final table for importances of various features is above","cbd9b74d":"**On going through this dataset we can do the following feature engineering**\n* Drop arrival_date_year\n* Drop arrival_date_week_number\n* Make a new column for total stays in nights\n* Drop Null Values\n* It is obvious that babies and children are just guests they won't pay or cancel the booking so we can either make a column for total guests or simply drop them all keeping adults in a column named paying_guests\n* identify the Undefined categorical values if they are meaningless then drop them or replace them with some relevant attribute via google search on terminoligies on hotel data\n","c63f96ec":"Better than Logistic Regression","21fbfac3":"**Random Forest Classifier**","650dc07b":"**Removing Out-liars**\nadr stands for average daily rate. Its descriptive stats says that it has a minimum value in negative which is possibly an error or at least is unjustified we can drop it and on making distplot it is found that the maximum value is also an outlier so delete it as well. \nForm a new column price multiplying adr with paying guests","9897697f":"It is intriguing that reservation_status_values and is_canceled has the same data we can drop either of the columns","eec60552":"**Random Forest Classifier for feature selection**","1f75139c":"**Make new dataframe with relevant fetures to end the curse of dimensionality**","bb01e929":"**Chi2 Test**","11a75a9f":"# Tuning of model","43da3685":"**Final scores look fine. With this we can expect good prediction model**","7606f2fe":"# Model Building","3d6ec00b":"**Heatmap for Correlation**","e6969394":"# **This Notebook deals with Data cleaning, Feature Engineering, EDA, Feature selection, model selection, model tuning and prediction**","72d7d822":"wuhooo!! good score..\nlet's check the confusion matrix","54bc75b3":"**Multicolinearity check**","039c572e":"**Recursive Feature Elimination for feature selection**","ca859937":"**One Hot encode the categorical records**","c2423978":"**Logistic Regression**","6e8eccb1":"Not great distinction of true positives and true negatives"}}