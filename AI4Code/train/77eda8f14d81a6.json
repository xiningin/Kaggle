{"cell_type":{"2d6a27f3":"code","c51b3eed":"code","8071223d":"code","c510877e":"code","f458d0da":"code","bc131d1a":"code","48b552ba":"code","162e3030":"code","3ef4e0fd":"code","e74662eb":"code","41985912":"code","46ddaab3":"code","686668b0":"code","bf06ff6c":"code","ffd243dc":"code","a607ad76":"code","2e9a66fc":"code","5b4ebcbe":"code","7e209820":"code","30984fb2":"code","ef3edacf":"markdown","979d9470":"markdown","29189e26":"markdown","e7a35359":"markdown","51e28966":"markdown","8f300bb7":"markdown","06885b92":"markdown","31fd3416":"markdown","f8712b00":"markdown","f9aafb52":"markdown","95331336":"markdown","70588994":"markdown","efb1436d":"markdown","79834dd2":"markdown","4a2d7fba":"markdown"},"source":{"2d6a27f3":"#!pip install Torch --upgrade","c51b3eed":"\n\n#import custom utils\nfrom shutil import copyfile\ncopyfile(src = \"..\/input\/lrp-scripts\/utils.py\", dst = \"..\/working\/utils.py\")\nfrom utils import *","8071223d":"import numpy\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n#img = numpy.array(cv2.imread('..\/input\/lrp-pics\/castle.jpg'))[...,::-1]\/255.0\n#img_s=mpimg.imread('..\/input\/lrp-pics\/castle.jpg')\n\nfile = \"..\/input\/melanoma\/DermMel\/train_sep\/Melanoma\/AUG_0_1072.jpeg\"\nimg = numpy.array(cv2.imread(file))[...,::-1]\/255.0\nimg_s=mpimg.imread(file)\n\n\nimgplot = plt.imshow(img_s)\nplt.show()","c510877e":"import torch\n\nmean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1)\nstd  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1)\n\nX = (torch.FloatTensor(img[numpy.newaxis].transpose([0,3,1,2])*1) - mean) \/ std","f458d0da":"import torchvision\n\nmodel = torchvision.models.vgg16(pretrained=True); model.eval()\nlayers = list(model._modules['features']) + toconv(list(model._modules['classifier']))\nL = len(layers)","bc131d1a":"from torchvision import datasets, models, transforms\nimport os\n\ndata_dir = '..\/input\/melanoma\/DermMel'\nTRAIN = 'train_sep'\nVAL = 'valid'\nTEST = 'test'\nsubset_indices = list(range(100))\n\n# VGG-16 Takes 224x224 images as input, so we resize all of them\ndata_transforms = {\n    TRAIN: transforms.Compose([\n        # Data augmentation is a good practice for the train set\n        # Here, we randomly crop the image to 224x224 and\n        # randomly flip it horizontally. \n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n    ]),\n    VAL: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ]),\n    TEST: transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n    ])\n}\n\nimage_datasets = {\n    x: datasets.ImageFolder(\n        os.path.join(data_dir, x), \n        transform=data_transforms[x]\n    )\n    for x in [TRAIN, VAL, TEST]\n}\n\ndataloaders = {\n    x: torch.utils.data.DataLoader(\n        image_datasets[x], batch_size=8,\n        num_workers=4, sampler=torch.utils.data.SubsetRandomSampler(subset_indices)\n    )\n    for x in [TRAIN, VAL, TEST]\n}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in [TRAIN, VAL, TEST]}\n\nfor x in [TRAIN, VAL, TEST]:\n    print(\"Loaded {} images under {}\".format(dataset_sizes[x], x))\n    \nprint(\"Classes: \")\nclass_names = image_datasets[TRAIN].classes\nprint(image_datasets[TRAIN].classes)","48b552ba":"print(model.classifier[6].out_features)\n\n# Freeze training for all layers\nfor param in model.features.parameters():\n    param.require_grad = False\n    \n# Newly created modules have require_grad=True by default\nnum_features = model.classifier[6].in_features\nfeatures = list(model.classifier.children())[:-1] # Remove last layer\n\nnum_classes = 2\nfeatures.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\n\n#features.extend([nn.Linear(num_features, len(class_names))]) # Add our layer with 4 outputs\nmodel.classifier = nn.Sequential(*features) # Replace the model classifier\nprint(model)","162e3030":"import torch.optim as optim\nfrom torch.optim import lr_scheduler\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","3ef4e0fd":"def train_model(vgg, criterion, optimizer, scheduler, num_epochs=10):\n    since = time.time()\n    best_model_wts = copy.deepcopy(vgg.state_dict())\n    best_acc = 0.0\n    \n    avg_loss = 0\n    avg_acc = 0\n    avg_loss_val = 0\n    avg_acc_val = 0\n    \n    train_batches = len(dataloaders[TRAIN])\n    val_batches = len(dataloaders[VAL])\n    \n    for epoch in range(num_epochs):\n        print(\"Epoch {}\/{}\".format(epoch, num_epochs))\n        print('-' * 10)\n        \n        loss_train = 0\n        loss_val = 0\n        acc_train = 0\n        acc_val = 0\n        \n        vgg.train(True)\n        \n        for i, data in enumerate(dataloaders[TRAIN]):\n            if i % 100 == 0:\n                print(\"\\rTraining batch {}\/{}\".format(i, train_batches \/ 2), end='', flush=True)\n                \n            # Use half training dataset\n            if i >= train_batches \/ 2:\n                break\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n            else:\n                inputs, labels = Variable(inputs), Variable(labels)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            loss_train += loss.data#[0]\n            acc_train += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        print()\n        # * 2 as we only used half of the dataset\n        avg_loss = loss_train * 2 \/ dataset_sizes[TRAIN]\n        avg_acc = acc_train * 2 \/ dataset_sizes[TRAIN]\n        \n        vgg.train(False)\n        vgg.eval()\n            \n        for i, data in enumerate(dataloaders[VAL]):\n            if i % 100 == 0:\n                print(\"\\rValidation batch {}\/{}\".format(i, val_batches), end='', flush=True)\n                \n            inputs, labels = data\n            \n            if use_gpu:\n                inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n            else:\n                inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n            \n            optimizer.zero_grad()\n            \n            outputs = vgg(inputs)\n            \n            _, preds = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            \n            loss_val += loss.data#[0]\n            acc_val += torch.sum(preds == labels.data)\n            \n            del inputs, labels, outputs, preds\n            torch.cuda.empty_cache()\n        \n        avg_loss_val = loss_val \/ dataset_sizes[VAL]\n        avg_acc_val = acc_val \/ dataset_sizes[VAL]\n        \n        print()\n        print(\"Epoch {} result: \".format(epoch))\n        print(\"Avg loss (train): {:.4f}\".format(avg_loss))\n        print(\"Avg acc (train): {:.4f}\".format(avg_acc))\n        print(\"Avg loss (val): {:.4f}\".format(avg_loss_val))\n        print(\"Avg acc (val): {:.4f}\".format(avg_acc_val))\n        print('-' * 10)\n        print()\n        \n        if avg_acc_val > best_acc:\n            best_acc = avg_acc_val\n            best_model_wts = copy.deepcopy(vgg.state_dict())\n        \n    elapsed_time = time.time() - since\n    print()\n    print(\"Training completed in {:.0f}m {:.0f}s\".format(elapsed_time \/\/ 60, elapsed_time % 60))\n    print(\"Best acc: {:.4f}\".format(best_acc))\n    \n    vgg.load_state_dict(best_model_wts)\n    return vgg","e74662eb":"import time\nfrom torch.autograd import Variable\nuse_gpu = torch.cuda.is_available() \n\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=2)\ntorch.save(model.state_dict(), 'VGG16_v2-OCT_Retina_half_dataset.pt')","41985912":"layers = list(model._modules['features']) + toconv(list(model._modules['classifier']))\nL = len(layers)\n\n\nmean = torch.Tensor([0.485, 0.456, 0.406]).reshape(1,-1,1,1)\nstd  = torch.Tensor([0.229, 0.224, 0.225]).reshape(1,-1,1,1)\n\nX = (torch.FloatTensor(img[numpy.newaxis].transpose([0,3,1,2])*1) - mean) \/ std","46ddaab3":"A = [X]+[None]*L\nfor l in range(L): A[l+1] = layers[l].forward(A[l])","686668b0":"scores = numpy.array(A[-1].data.view(-1))\nind = numpy.argsort(-scores)\nfor i in ind[:10]:\n    print('%20s (%3d): %6.3f'%(imgclasses[i][:20],i,scores[i]))","bf06ff6c":"T = torch.FloatTensor((1.0*(numpy.arange(2)==1).reshape([1,2,1,1])))\nR = [None]*L + [(A[-1]*T).data]","ffd243dc":"# Whenever we meet a max-pooling layer, convert it into an average pooling layer.\n\nfor l in range(1,L)[::-1]:\n    \n    A[l] = (A[l].data).requires_grad_(True)\n\n    if isinstance(layers[l],torch.nn.MaxPool2d): layers[l] = torch.nn.AvgPool2d(2)\n\n    if isinstance(layers[l],torch.nn.Conv2d) or isinstance(layers[l],torch.nn.AvgPool2d):\n\n        if l <= 16:       rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9\n        if 17 <= l <= 30: rho = lambda p: p;                       incr = lambda z: z+1e-9+0.25*((z**2).mean()**.5).data\n        if l >= 31:       rho = lambda p: p;                       incr = lambda z: z+1e-9\n\n        z = incr(newlayer(layers[l],rho).forward(A[l]))  # step 1\n        s = (R[l+1]\/z).data                                    # step 2\n        (z*s).sum().backward(); c = A[l].grad                  # step 3\n        R[l] = (A[l]*c).data                                   # step 4\n        \n    else:\n        \n        R[l] = R[l+1]","a607ad76":"for i,l in enumerate([31,21,11,1]):\n    heatmap(numpy.array(R[l][0]).sum(axis=0),0.5*i+1.5,0.5*i+1.5)","2e9a66fc":"A[0] = (A[0].data).requires_grad_(True)\n\nlb = (A[0].data*0+(0-mean)\/std).requires_grad_(True)\nhb = (A[0].data*0+(1-mean)\/std).requires_grad_(True)\n\nz = layers[0].forward(A[0]) + 1e-9                                     # step 1 (a)\nz -= newlayer(layers[0],lambda p: p.clamp(min=0)).forward(lb)    # step 1 (b)\nz -= newlayer(layers[0],lambda p: p.clamp(max=0)).forward(hb)    # step 1 (c)\ns = (R[1]\/z).data                                                      # step 2\n(z*s).sum().backward(); c,cp,cm = A[0].grad,lb.grad,hb.grad            # step 3\nR[0] = (A[0]*c+lb*cp+hb*cm).data                                       # step 4","5b4ebcbe":"heatmap(numpy.array(R[0][0]).sum(axis=0),3.5,3.5)\n","7e209820":"def eval_model(vgg, criterion):\n    since = time.time()\n    avg_loss = 0\n    avg_acc = 0\n    loss_test = 0\n    acc_test = 0\n    \n    test_batches = len(dataloaders[TEST])\n    print(\"Evaluating model\")\n    print('-' * 10)\n    \n    for i, data in enumerate(dataloaders[TEST]):\n        if i % 100 == 0:\n            print(\"\\rTest batch {}\/{}\".format(i, test_batches), end='', flush=True)\n\n        vgg.train(False)\n        vgg.eval()\n        inputs, labels = data\n\n        if use_gpu:\n            inputs, labels = Variable(inputs.cuda(), volatile=True), Variable(labels.cuda(), volatile=True)\n        else:\n            inputs, labels = Variable(inputs, volatile=True), Variable(labels, volatile=True)\n\n        outputs = vgg(inputs)\n\n        _, preds = torch.max(outputs.data, 1)\n        loss = criterion(outputs, labels)\n\n        loss_test += loss.data\n        acc_test += torch.sum(preds == labels.data)\n\n        del inputs, labels, outputs, preds\n        torch.cuda.empty_cache()\n        \n    avg_loss = loss_test \/ dataset_sizes[TEST]\n    avg_acc = acc_test \/ dataset_sizes[TEST]\n    \n    elapsed_time = time.time() - since\n    print()\n    print(\"Evaluation completed in {:.0f}m {:.0f}s\".format(elapsed_time \/\/ 60, elapsed_time % 60))\n    print(\"Avg loss (test): {:.4f}\".format(avg_loss))\n    print(\"Avg acc (test): {:.4f}\".format(avg_acc))\n    print('-' * 10)","30984fb2":"print(\"Test before training\")\neval_model(model, criterion)","ef3edacf":" Then we need ot apply the pixel-specific **z-rule** for the last layer. This rule can again be implemented in terms of forward passes and gradient computations. ","979d9470":" You can see that the heatmap highlights the outline of the castle as evidence for the corresponding class. Some elements such as the traffic sign or the roof on the left are seen as having a negative effect on the neuron \"castle\" and are consequently highlighted in blue.","29189e26":"The image is given as input to the VGG-16 network. ","e7a35359":"This notebook contains an implementation of LRP on the VGG-16 network trained on ImageNet and applicable to general image classification. \nTutorial\/code from : http:\/\/heatmapping.org\/tutorial\/","51e28966":"### Retrain\n\nBelow the VGG16 Model is retrained ","8f300bb7":"### Convolutional layers\nObserving that convolutions are special types of linear layers, a similar four-step procedure is applied. Steps 2 and 4 are simple element-wise computations. Step 1 can be implemented as a forward computation in the layer and where we apply the increment function afterwards.  Step 3 can instead be computed as a gradient in the space of input activations: $c_j = \\big[\\nabla~\\big({\\textstyle \\sum_k}~z_k(\\boldsymbol{a}) \\cdot s_k\\big)\\big]_j$\n\n### Pooling layers\n\ntreat max-pooling layers as average pooling layers in the backward pass, because average pooling is also a special linear layer. That way the same propagation rules as for the convolutional layers become applicable. ","06885b92":"## Classification\nThe input can be propagated in the network and the activations at each layer are collected in *A*","31fd3416":"The VGG-16 network is loaded and its top-level dense layers are converted into equivalent 1x1 convolutions. The dense or fully connected layer that is normally used as final classification-layer needs to be in this shape for the following steps.","f8712b00":"Activations in the top layer are the scores the neural network predicts for each class.The highest scores are computed below:","f9aafb52":"## Model evaluation","95331336":"# Layer-Wise Relevance Propagation on VGG16","70588994":"load a RGB image and convert it to a numpy-array with 0-1 normalization applied.","efb1436d":"## Transfer Learning","79834dd2":"##  Explaining the prediction with LRP\n\nThis code iterates from the top layer to the first layer and applies propagation rules at each layer. Top-layer activations are first multiplied by the mask to retain only the predicted evidence for the class \"castle\". This evidence can then be propagated backward in the network by applying propagation rules at each layer. ","4a2d7fba":"## Visualization of the heatmap\n\nAs each layer is composed of a collection of two-dimensional feature maps, relevance scores at each layer can be visualized as a two-dimensional map. The two-dimensional maps are shown for a selection of VGG-16 layers (31,21,11,1)."}}