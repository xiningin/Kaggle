{"cell_type":{"ba0dda78":"code","b6d42a29":"code","71aeccf1":"code","3490bc58":"code","83343560":"code","9a7dfba0":"code","d0a5fdbe":"code","1330399a":"code","7fa4af7e":"code","aa23c009":"code","487cfba4":"code","68584da0":"code","1e0dd0fb":"code","80252e14":"code","deeeb737":"code","8ae4c85e":"code","d6cb10a8":"code","0a2140b1":"code","f6a946f0":"code","64741384":"code","55e301cc":"code","a9f23962":"code","6f8ce95e":"code","46f45174":"code","156f44a3":"code","3530c142":"code","b728a35c":"code","af579fa2":"markdown","b3c466a7":"markdown","d9e1389a":"markdown","c1dad7dc":"markdown","3b9db360":"markdown","1941bd60":"markdown","9d790cbf":"markdown","7a1d1647":"markdown","bc3b3151":"markdown","ba592bb6":"markdown","a514827b":"markdown","6e7dce44":"markdown","7cb89fc8":"markdown","1803888b":"markdown","077d6f51":"markdown","b3755644":"markdown","f41ec7c1":"markdown","8a4cdf66":"markdown","db95471f":"markdown","08d1bfc7":"markdown","01e4b42e":"markdown","6ef8913c":"markdown","7757a3d5":"markdown","fabe5dcc":"markdown","139ae452":"markdown","cb7e96f1":"markdown","d0503365":"markdown","3a46e852":"markdown","5d62ab2b":"markdown","7af1841e":"markdown","b270422d":"markdown"},"source":{"ba0dda78":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.svm import SVC # For SVM model\nfrom sklearn.tree import DecisionTreeClassifier # For Decission Tree Classifier\nfrom sklearn import tree\nfrom ipywidgets import interact,interactive\nfrom sklearn.ensemble import RandomForestClassifier # For random Forest Classifier\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression # For Logistic Regression\nfrom xgboost import XGBClassifier,plot_importance # For XGBoost Classifier\nfrom catboost import CatBoostClassifier # For Cat Boost Classifier\nfrom imblearn.over_sampling import SMOTE\nfrom keras.models import Sequential # For Neural Network Sequential Model\nfrom keras.layers import Dense, Activation,Layer,Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold","b6d42a29":"df = pd.read_csv(\"..\/input\/df-cleancsv\/df_Clean.csv\").iloc[:,1:]\ndf.head()","71aeccf1":"[f\"{i} is {df[i].dtype}\" for i in df.columns]","3490bc58":"df_cat = pd.DataFrame.copy(df)\ncolumns = ['Product_Age','Call_details','Claim_Value']\nfor i in df_cat.columns:\n    if i not in columns:\n        df_cat[i] = df_cat[i].astype('category')\n[f\"{i} is {df_cat[i].dtype}\" for i in df_cat.columns]","83343560":"df_Label = pd.DataFrame.copy(df_cat)\nfor i in df_Label.columns:\n    if df_Label[i].dtype.name =='category':\n        enco = preprocessing.LabelEncoder()\n        enco.fit(list(set(df_Label[i])))\n        df_Label[i] = enco.transform(df_Label[i])","9a7dfba0":"df_Label.info()","d0a5fdbe":"_df_norm = pd.DataFrame.copy(df_cat)\ncolum = []\nfor i in _df_norm.columns:\n    if _df_norm[i].dtype.name == 'category':\n        colum.append(i)\ndf_dummy = pd.get_dummies(_df_norm, columns =colum[:-1])\nmin_max_scaler = preprocessing.MinMaxScaler()\nScaled = min_max_scaler.fit_transform(df_dummy[['Product_Age','Call_details','Claim_Value']] )","1330399a":"col = list(df_dummy.columns)\n_norm_dummy = np.concatenate((Scaled,df_dummy.values[:,3:99]),axis=1)\ndf_norm_dummy = pd.DataFrame(_norm_dummy,columns=col)\ndf_norm_dummy","7fa4af7e":"sm_x_lab = pd.DataFrame.copy(df_Label)\nsm_y_lab = sm_x_lab.pop('Fraud')\nsm = SMOTE(random_state =101)\nX_lab_balance, Y_lab_balance = sm.fit_sample(sm_x_lab, sm_y_lab)\nX_lab_balance.shape, Y_lab_balance.shape","aa23c009":"df_majority = df_cat[df_cat.Fraud==0]\ndf_minority = df_cat[df_cat.Fraud==1]\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=323,    # to match majority class\n                                 random_state=101) # reproducible results\ndf_cat_balance = pd.concat([df_majority, df_minority_upsampled])\ndf_cat_balance.Fraud.value_counts()\nX_cat_balance = pd.DataFrame.copy(df_cat_balance)\nY_cat_balance = X_cat_balance.pop('Fraud')\nX_cat_balance.shape, Y_cat_balance.shape","487cfba4":"sm_x_nd = pd.DataFrame.copy(df_norm_dummy)\nsm_y_nd = sm_x_nd.pop('Fraud')\nsm = SMOTE(random_state =101)\nX_nd_balance, Y_nd_balance = sm.fit_sample(sm_x_nd, sm_y_lab)\nX_nd_balance.shape, Y_nd_balance.shape","68584da0":"# Balanced\nX_train_cat_bal,X_test_cat_bal,Y_train_cat_bal,Y_test_cat_bal = train_test_split(X_cat_balance, Y_cat_balance,test_size=0.3,random_state=101)\nX_train_cat_bal.shape,X_test_cat_bal.shape,Y_train_cat_bal.shape,Y_test_cat_bal.shape","1e0dd0fb":"# Original\nX_c = pd.DataFrame.copy(df_cat)\nY_c = X_c.pop('Fraud')\nX_train_cat,X_test_cat,Y_train_cat,Y_test_cat = train_test_split(X_c,Y_c,test_size=0.3,random_state=101)\nX_train_cat.shape,X_test_cat.shape,Y_train_cat.shape,Y_test_cat.shape","80252e14":"# Balanced\nX_train_lab_bal,X_test_lab_bal,Y_train_lab_bal,Y_test_lab_bal = train_test_split(X_lab_balance, Y_lab_balance,test_size=0.3,random_state=101)\nX_train_lab_bal.shape,X_test_lab_bal.shape,Y_train_lab_bal.shape,Y_test_lab_bal.shape","deeeb737":"# Original\nX_l = pd.DataFrame.copy(df_Label)\nY_l = X_l.pop('Fraud')\nX_train_lab,X_test_lab,Y_train_lab,Y_test_lab = train_test_split(X_l,Y_l,test_size=0.3,random_state=101)\nX_train_lab.shape,X_test_lab.shape,Y_train_lab.shape,Y_test_lab.shape","8ae4c85e":"# Balanced\nX_train_nd_bal,X_test_nd_bal,Y_train_nd_bal,Y_test_nd_bal = train_test_split(X_nd_balance, Y_nd_balance,test_size=0.3,random_state=101)\nX_train_nd_bal.shape,X_test_nd_bal.shape,Y_train_nd_bal.shape,Y_test_nd_bal.shape","d6cb10a8":"# Original\nX_d = pd.DataFrame.copy(df_norm_dummy)\nY_d = X_d.pop('Fraud')\nX_train_nd,X_test_nd,Y_train_nd,Y_test_nd = train_test_split(X_d,Y_d,test_size=0.3,random_state=101)\nX_train_nd.shape,X_test_nd.shape,Y_train_nd.shape,Y_test_nd.shape","0a2140b1":"def treebuild(cri='entropy',mxd=10,minsl=2,rs=28,spl='best'):\n    Warrenty_Tree = DecisionTreeClassifier(criterion=cri,max_depth=mxd,min_samples_leaf=minsl,random_state=rs,splitter=spl)\n    Warrenty_Tree.fit(X_train_lab_bal,Y_train_lab_bal)\n    pred_bal = Warrenty_Tree.predict(X_test_lab_bal)\n    pred_ = Warrenty_Tree.predict(X_test_lab)\n    prr_bal = Warrenty_Tree.predict(X_train_lab_bal)\n    prr_ = Warrenty_Tree.predict(X_train_lab)\n    print(\"Test Accuracy original Data\",np.mean(Y_test_lab==pred_))\n    print(\"Train Accuracy Original Data\",np.mean(Y_train_lab==prr_))\n    print (\"Test Accuracy Balanced Data\",np.mean(Y_test_lab_bal==pred_bal))\n#     print(classification_report(Y_test_lab_bal,pred_bal))\n    print(\"Train Accuracy Balanced Data\",np.mean(Y_train_lab_bal==prr_bal))\n    print(classification_report(Y_test_lab,pred_))\ninteract(treebuild,cri=['entropy','gini'],mxd=[i for i in range(1,20)],minsl=[i for i in range(1,10)],rs=[i for i in  range(30)],spl=['best','random'])","f6a946f0":"def forests(n_est=20,cri='entropy',mxd=9,mslf=3,mf='auto',rs=9):\n    forest = RandomForestClassifier(n_estimators=n_est,criterion=cri,max_depth=mxd,min_samples_leaf=mslf,max_features=mf,random_state=rs)\n    forest.fit(X_train_lab_bal,Y_train_lab_bal)\n    pred_bal = forest.predict(X_test_lab_bal)\n    pred_ = forest.predict(X_test_lab)\n    prr_bal = forest.predict(X_train_lab_bal)\n    prr_ = forest.predict(X_train_lab)\n    print(\"Test Accuracy original Data\",np.mean(Y_test_lab==pred_))\n    print(classification_report(Y_test_lab,pred_))\n    print (\"Test Accuracy Balanced Data\",np.mean(Y_test_lab_bal==pred_bal))\n    print(classification_report(Y_test_lab_bal,pred_bal))\n    print(\"Train Accuracy Original Data\",np.mean(Y_train_lab==prr_))\n    print(\"Train Accuracy Balanced Data\",np.mean(Y_train_lab_bal==prr_bal))\ninteract(forests,n_est=[i for i in range(10,100)],cri=['gini','entropy'],mxd=[i for i in range(1,10)],mslf=[i for i in range(1,10)],mf=['auto','sqrt','log2'],rs=[i for i in range(30)])","64741384":"xgm = XGBClassifier(max_depth=5,learning_rate=0.2,n_estimators=200)\nxgm.fit(X_train_lab_bal,Y_train_lab_bal)\npred_bal = xgm.predict(X_test_lab_bal)\npred_ = xgm.predict(X_test_lab.values)\nprr_bal = xgm.predict(X_train_lab_bal)\nprr_ = xgm.predict(X_train_lab.values)\n\nprint(\"Train Accuracy Original Data\",np.mean(Y_train_lab==prr_))\nprint(\"Test Accuracy original Data\",np.mean(Y_test_lab==pred_))\nprint(classification_report(Y_test_lab,pred_))\nprint (\"Test Accuracy Balanced Data\",np.mean(Y_test_lab_bal==pred_bal))\n# print(classification_report(Y_test_lab_bal,pred_bal))\n\nprint(\"Train Accuracy Balanced Data\",np.mean(Y_train_lab_bal==prr_bal))","55e301cc":"modelcat = CatBoostClassifier(learning_rate=0.1,depth=3,n_estimators=400,cat_features=[0,1,2,3,4,5,6,7,8,9,10,11,12,14,16,18])\nmodelcat.fit(X_train_cat_bal,Y_train_cat_bal)\npred_bal = modelcat.predict(X_test_cat_bal)\npred_ = modelcat.predict(X_test_cat)\nprr_bal = modelcat.predict(X_train_cat_bal)\nprr_ = modelcat.predict(X_train_cat)","a9f23962":"print(\"Test Accuracy original Data\",np.mean(Y_test_cat==pred_))\nprint(classification_report(Y_test_cat,pred_))\nprint (\"Test Accuracy Balanced Data\",np.mean(Y_test_cat_bal==pred_bal))\n# print(classification_report(Y_test_cat_bal,pred_bal))\nprint(\"Train Accuracy Original Data\",np.mean(Y_train_cat==prr_))\nprint(\"Train Accuracy Balanced Data\",np.mean(Y_train_cat_bal==prr_bal))","6f8ce95e":"Proba = pd.DataFrame(modelcat.predict_proba(X_test_cat))\nProba.columns = ['Fraud_no','Fraud_yes']\nProba['Actual'] = list(Y_test_cat)\nProba['Predicted'] = list(pred_)\nProba.loc[(Proba.Actual!=Proba.Predicted)]","46f45174":"methods = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\ndef Logitt(method):\n    lr1 = LogisticRegression(solver=method) \n    lr1.fit(X_train_nd_bal,Y_train_nd_bal)\n    pred_bal = lr1.predict(X_test_nd_bal)\n    pred_ = lr1.predict(X_test_nd)\n    prr_bal = lr1.predict(X_train_nd_bal)\n    prr_ = lr1.predict(X_train_nd)\n    print(\"Test Accuracy original Data\",np.mean(Y_test_nd==pred_))\n    print(classification_report(list(Y_test_nd),pred_))\n    print (\"Test Accuracy Balanced Data\",np.mean(list(Y_test_nd_bal)==pred_bal))\n#     print(classification_report(list(Y_test_nd_bal),pred_bal))\n    print(\"Train Accuracy Original Data\",np.mean(list(Y_train_nd)==prr_))\n    print(\"Train Accuracy Balanced Data\",np.mean(list(Y_train_nd_bal)==prr_bal))\ninteract(Logitt,method = methods)","156f44a3":"classifier = Sequential()\nclassifier.add(Dense(activation=\"relu\", input_dim=97, units=10, kernel_initializer=\"he_uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=10, kernel_initializer=\"he_uniform\"))\nclassifier.add(Dense(activation=\"relu\", units=10, kernel_initializer=\"he_uniform\"))\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"glorot_uniform\"))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.fit(X_train_nd_bal, Y_train_nd_bal, batch_size = 10, nb_epoch = 100)\npred_bal = classifier.predict(X_test_nd_bal);pred_bal=(pred_bal > 0.5)\npred_ = classifier.predict(X_test_nd);pred_=(pred_ > 0.5)\nprr_bal = classifier.predict(X_train_nd_bal);prr_bal=(prr_bal > 0.5)\nprr_ = classifier.predict(X_train_nd);prr_=(prr_ > 0.5)","3530c142":"print(\"Test Accuracy original Data\",np.mean(list(Y_test_nd)==pred_))\nprint(classification_report(list(Y_test_nd),pred_))\nprint (\"Test Accuracy Balanced Data\",np.mean(list(Y_test_nd_bal)==pred_bal))\n# print(classification_report(list(Y_test_nd_bal),pred_bal))\nprint(\"Train Accuracy Original Data\",np.mean(list(Y_train_nd)==prr_))\nprint(\"Train Accuracy Balanced Data\",np.mean(list(Y_train_nd_bal)==prr_bal))","b728a35c":"svclassifier = SVC(kernel = 'linear')\nsvclassifier.fit(X_train_nd_bal, Y_train_nd_bal)\npred_bal = svclassifier.predict(X_test_nd_bal)\npred_ = svclassifier.predict(X_test_nd)\nprr_bal = svclassifier.predict(X_train_nd_bal)\nprr_ = svclassifier.predict(X_train_nd)\nprint(\"Test Accuracy original Data\",np.mean(Y_test_nd==pred_))\nprint(classification_report(list(Y_test_nd),pred_))\nprint (\"Test Accuracy Balanced Data\",np.mean(list(Y_test_nd_bal)==pred_bal))\n# print(classification_report(list(Y_test_nd_bal),pred_bal))\nprint(\"Train Accuracy Original Data\",np.mean(list(Y_train_nd)==prr_))\nprint(\"Train Accuracy Balanced Data\",np.mean(list(Y_train_nd_bal)==prr_bal))","af579fa2":"## For Labeled Data","b3c466a7":"## Balancing to Lable Data (X_lab_balance,Y_lab_balance)","d9e1389a":"## Simple logistic Regression","c1dad7dc":"## Using the XGB model","3b9db360":"## Balancing The Normalised Dummy data","1941bd60":"## Label Encoded data (df_Lable)","9d790cbf":"## For Normalised Dummy Data","7a1d1647":"## Random Forest","bc3b3151":"cri=['entropy','gini']\nmxd=[i for i in range(1,20)]\nminsl=[i for i in range(1,10)]\nrs=[i for i in  range(30)]\nspl=['best','random']\nWarrenty_Tree = DecisionTreeClassifier()\nparm_grid = dict(criterion=cri,max_depth=mxd,min_samples_leaf=minsl,random_state=rs,splitter=spl)\nkfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=101)\ngridsearch = GridSearchCV(Warrenty_Tree,parm_grid,scoring=\"balanced_accuracy\", n_jobs=4, cv=kfold)\ngrid_result = gridsearch.fit(X_lab_balance,Y_lab_balance);\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","ba592bb6":"parm_grid = dict(learning_rate=learning_rate,depth=max_depth,n_estimators=n_estimators)\nmodelcat = CatBoostClassifier(cat_features=[0,1,2,3,4,5,6,7,8,9,10,11,12,14,16,18])\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\ngridsearch = GridSearchCV(modelcat,parm_grid,scoring=\"balanced_accuracy\", n_jobs=4, cv=kfold)\ngrid_result = gridsearch.fit(X,Y)","a514827b":"___So we are getting more than 90% Accuracy from this model (20,entropy,9,3,auto,9) combination","6e7dce44":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","7cb89fc8":"<hr>    \n\n# Model Fitting And Evaluation\n\n<hr>","1803888b":"___entropy,8,2,21,random","077d6f51":"## Using Cat Boost","b3755644":"## Neural Network","f41ec7c1":"# Balancing the data Before Modeling","8a4cdf66":"## Normalised Dummy Data (df_norm_dummy)","db95471f":"## A Simple Decission Tree (Using Labeled Data)","08d1bfc7":"## Categorical Data (df_cat)","01e4b42e":"# Data Preprocessing Before Modelling","6ef8913c":"### Hypertuning the decission tree","7757a3d5":"## For Categorical Data","fabe5dcc":"## SVM","139ae452":"## Balancing the Categorical Data (X_cat_balance, Y_cat_balance)","cb7e96f1":"Best: 0.532262 using {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 400}","d0503365":"n_est=[50,100,200]\ncri=['gini','entropy']\nmxd=[i for i in range(1,10)]\nmslf=[i for i in range(1,10)]\nmf=['auto']\nrs=[i for i in range(30)]\nforest = RandomForestClassifier()\nparm_grid = dict(n_estimators=n_est,criterion=cri,max_depth=mxd,min_samples_leaf=mslf,max_features=mf,random_state=rs)\nkfold = StratifiedKFold(n_splits=4, shuffle=True, random_state=101)\ngridsearch = GridSearchCV(forest,parm_grid,scoring=\"balanced_accuracy\", n_jobs=4, cv=kfold)\ngrid_result = gridsearch.fit(X_lab_balance,Y_lab_balance);\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","3a46e852":"# Train Test Split","5d62ab2b":"__Best: 0.902477 using {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'random_state': 28, 'splitter': 'best'}","7af1841e":"Best: 0.938080 using {'criterion': 'gini', 'max_depth': 7, 'max_features': 'auto', 'min_samples_leaf': 4, 'n_estimators': 50, 'random_state': 13}","b270422d":"# Importing Required Packages and Modules"}}