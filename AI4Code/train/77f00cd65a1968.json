{"cell_type":{"4156f5f2":"code","a3f68bd2":"code","3ee2be5a":"code","e9705044":"code","7e122659":"code","d0a84641":"code","759ff75a":"code","22e85d2b":"code","95ee0a5d":"code","a7bbfc37":"code","acf47366":"code","3926fe21":"code","e2797ced":"code","882c061f":"code","c82f0ab5":"code","13c4e18e":"code","dc79f5ed":"code","164338e8":"code","dccc44a2":"code","56f493e4":"code","46078dd4":"code","4fce5212":"code","c287db0c":"code","e9615cd4":"code","6d89842d":"code","059044e7":"code","a83c9964":"code","337729ba":"code","41c3006f":"code","71721028":"code","bb44a41b":"code","bbb92aaa":"code","0f8cc713":"code","46c989ca":"code","1208a8dd":"code","07fe810f":"code","2d8c1f75":"code","7a669dc9":"code","e73ed89e":"code","23d98aa5":"code","1a8655ff":"code","c9da3f5e":"code","1f188f57":"code","c2158eb8":"code","ee38fc02":"code","a33f9910":"code","e53bfcf6":"code","10244f9e":"code","07d41b0c":"code","7d9fbfd7":"code","b4f3a997":"code","2efbb4f3":"code","dea7ef52":"code","3449c743":"code","24fee7b9":"markdown","cb173821":"markdown","a65ae66c":"markdown","b271cca6":"markdown","e5efea6a":"markdown","5ad7b46d":"markdown","61b7e14d":"markdown","b668bc80":"markdown","9bfde347":"markdown","d63cc9ae":"markdown","b2dabcd6":"markdown","9949a36a":"markdown","04dc97ed":"markdown","e4744e4c":"markdown","6aecda88":"markdown","27a0867f":"markdown","c6734fd8":"markdown","b428df08":"markdown","21153d80":"markdown","35640d45":"markdown","bf01edbf":"markdown","6dcff61d":"markdown","ba6c9618":"markdown","fd814efc":"markdown","e64a5d45":"markdown","2d1fac6e":"markdown","8fc375be":"markdown","7f9be18a":"markdown","b20d1ce0":"markdown","30608ae5":"markdown","be933ead":"markdown","00efd352":"markdown","c39de91a":"markdown","23e93c83":"markdown","157bc8a1":"markdown","6c5737eb":"markdown","e9431073":"markdown","3dd0b1a6":"markdown","e9ff82bf":"markdown","d3b63bff":"markdown","e3e8d52e":"markdown","86ea8ae6":"markdown","e186251e":"markdown","d1bd6ff5":"markdown","fa1a7c77":"markdown","69a4e4e9":"markdown","115bb557":"markdown","4d5ce431":"markdown","8fbea0e1":"markdown","7e9fe639":"markdown","6158b454":"markdown","4b55d7fa":"markdown","fabfc63b":"markdown","c4fcf75c":"markdown","2dad9644":"markdown","1e9a07e9":"markdown","bc5f3adf":"markdown","4e55cca8":"markdown","9c65b015":"markdown","cb54076f":"markdown","3c6624ee":"markdown","5888fe86":"markdown","f1dac61f":"markdown"},"source":{"4156f5f2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import accuracy_score, auc, confusion_matrix, roc_curve\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom datetime import datetime\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM\nfrom numpy.random import seed","a3f68bd2":"def supervised_lag(data, max_lag):\n    df = data.copy() # Avoids overwriting of the orginal df\n    c = pd.DataFrame() # Empty dataframe\n    for name in df.columns:\n        for i in range(max_lag,0,-1): # Count backwards\n            c[name+str(-i)]=df[name].shift(i) # Lag\n        c[name] = df[name]\n    \n    c = c.dropna().reset_index(drop=True)\n    # Reshape : number of observations, variables, lags +1 to include t\n    \n    return c","3ee2be5a":"def ts_train_test(data, predictors, target, time):\n    \"\"\"\n    predictors: list with the predictors' names\n    targer: target variable name (string)\n    time: length of test (int)\n    \"\"\"\n    train_X = data[predictors][:-time]\n    train_y = data[target][:-time]\n    test_X = data[predictors][-time:]\n    test_y = data[target][-time:]\n    \n    return train_X, train_y, test_X, test_y","e9705044":"def ts_lag(data, max_lag):\n    df = data.copy() # Avoids overwriting of the orginal df\n    c = pd.DataFrame() # Empty dataframe\n    for name in df.columns:\n        for i in range(max_lag,0,-1): # Count backwards\n            c[name+str(-i)]=df[name].shift(i) # Lag\n        c[name] = df[name]\n    \n    c = c.dropna().reset_index(drop=True)\n    # Reshape : number of observations, variables, lags +1 to include t\n    c = c.values.reshape(len(c),len(data.columns),max_lag+1) \n    \n    # Above code reshape in horizontal, we need each column = variable -> Transpose\n    l = [] \n    for i in range(0,len(c)):\n        l.append(c[i].transpose())\n    \n    return np.array(l)","7e122659":"def ts_lag_test(train_original, test, max_lag):\n    train = train_original.copy() # To avoid inplace errors.\n    test = test.copy()\n    c = pd.DataFrame() # Empty dataset to append the new lag variables in order.\n    for name in test.columns: # Name of the variables.\n        for i in range(max_lag,0,-1): # We want a sorted sequence t-2,t-1,t So we first compute de max lag and then go decreasing. 0 is not included.\n            c[name+str(-i)]=test[name].shift(i)\n            c[name+str(-i)][0:i] = train[name][len(train)-i:len(train)] # Replace the NA value with the last observation of the complete train.\n        c[name] = test[name]\n        \n        \n    c = c.dropna().reset_index(drop=True)\n    # Reshape : number of observations, variables, lags +1 to include t\n    c = c.values.reshape(len(c),len(train_original.columns),max_lag+1) \n    \n    # Above code reshape in horizontal, we need each column = variable -> Transpose\n    l = [] \n    for i in range(0,len(c)):\n        l.append(c[i].transpose())\n    \n    return np.array(l)","d0a84641":"def make_LSTM(train_X,train_y,test_X,test_y,units,epochs,batch_size,seed):\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(seed)\n    model = Sequential()\n    model.add(LSTM(units,\n                      input_shape=(train_X.shape[1],train_X.shape[2]),kernel_initializer=\"he_uniform\"\n                  ))\n    model.add(Dense(1,activation=\"sigmoid\")) # Must be sigmoid for 0 - 1. \n    model.compile(loss=\"BinaryCrossentropy\", optimizer=\"rmsprop\", metrics=[\"AUC\"])\n    history=model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size,\n                         validation_data=(test_X,test_y), verbose=0, shuffle = False)\n \n    return model, history","759ff75a":"def evaluate_nn(train_X_R, test_X_R, test_y, seed=[303,305,3,1], units=16,epochs=50,batch_size=64):\n    loss_list,auc_list=list(),list()\n    for i in seed:\n        model, _=make_LSTM(train_X_R, train_y, test_X_R, test_y, units, epochs, batch_size, seed=i)\n        loss, auc = model.evaluate(test_X_R,test_y,verbose=1)\n        loss_list.append(loss)\n        auc_list.append(auc)\n    return np.mean(loss_list),np.mean(auc_list)\n\ndef plot_loss(history):\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n    plt.title(\"LOSS\")\n    plt.show()\n    \ndef plot_auc(history):\n    plt.plot(history.history['auc'], label='train')\n    plt.plot(history.history['val_auc'], label='test')\n    plt.legend()\n    plt.title(\"AUC\")\n    plt.show()\n    \ndef plot_roc(y_real, y_pred):\n    false_positive_rate, recall, thresholds = roc_curve(y_real, y_pred)\n    roc_auc = auc(false_positive_rate, recall)\n    print(f'- AUC: {roc_auc}')\n    print(f'- AVG Recall: {np.mean(recall)}')\n    plt.plot(false_positive_rate, recall, 'b')\n    plt.plot([0, 1], [0, 1], 'r--')\n    plt.title('ROC AUC = %0.2f' % roc_auc)","22e85d2b":"def tune_nn(units, epochs, batch_size):\n    \"\"\"\n    # LIST format []\n    \"\"\"\n    dic={\n        \"units\":[],\n        \"epochs\":[],\n        \"batch_size\":[],\n        \"AUC\":[]\n    }\n    for u in range(len(units)):\n        for i in range(len(epochs)):\n            for j in range(len(batch_size)):\n                print(units[u],epochs[i],batch_size[j])\n                _,avg_auc=evaluate_nn(train_X_R, test_X_R, test_y, seed=[303,305], units=units[u],epochs=epochs[i],batch_size=batch_size[j])\n                dic[\"units\"].append(units[u])\n                dic[\"epochs\"].append(epochs[i])\n                dic[\"batch_size\"].append(batch_size[j])\n                dic[\"AUC\"].append(avg_auc)\n    return pd.DataFrame(dic)","95ee0a5d":"def filter_startswith(columns_vector, pattern):\n    mask = []\n    for i in columns_vector.values: mask.append(i.startswith(pattern))\n    return columns_vector.values[mask]\n\ndef Find_Optimal_Cutoff(target, predicted):\n    fpr, tpr, threshold = roc_curve(target, predicted)\n    i = np.arange(len(tpr)) \n    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n\n    return list(roc_t['threshold'])","a7bbfc37":"ozone = pd.read_csv(\n    \"https:\/\/raw.githubusercontent.com\/r4msi\/LSTM_Ozone\/master\/ozone.csv\",\n    parse_dates=[\"Date\"],\n    index_col=\"Date\",\n    dtype=float,\n    na_values=\"?\"\n)","acf47366":"# ozone_report = sv.analyze(ozone, target_feat=\"target\")\n# ozone_report.show_html('ozone.html')","3926fe21":"print(f\" -Observations: {ozone.shape[0]}\\n -Variables: {ozone.shape[1]}\")\nozone.head()","e2797ced":"ozone.describe()","882c061f":"print(f\"{np.min(ozone.index)}\\n{np.max(ozone.index)}\")","c82f0ab5":"np.array(ozone.isnull().sum())","13c4e18e":"fig=sns.countplot(ozone.target)\nfig=plt.title(\"Ozone Days\")","dc79f5ed":"pd.crosstab(ozone.target, \"count\")","164338e8":"fig=plt.figure(figsize=(15,6))\nfig=plt.subplot()\nfig = ozone.target[:365].plot()\nfig = ozone.target[365:730].plot()\nfig = ozone.target[730:1095].plot()\nfig = ozone.target[1095:1460].plot()\nfig = ozone.target[1460:1825].plot()\nfig = ozone.target[1825:2190].plot()\nfig=ozone.target[-365:].plot()\nfig=plt.title(\"Ozone\")","dccc44a2":"series = [\"WSR_AV\",\"T_AV\",\"T85\",\"RH50\",\"U70\",\"V50\",\"HT50\",\"KI\",\"TT\",\"SLP\",\"Precp\",\"target\"] # Just a few...\nfor i in series:\n    fig=plt.figure(figsize=(10,5))\n    fig=ozone[i].plot(c=np.random.rand(3,))\n    fig=plt.title(i)\n    fig=None","56f493e4":"fig=plt.figure(figsize=(14,5))\nfig=plt.subplot(1,2,1)\nfig=ozone.T_AV.interpolate().plot(c=\"blue\",title=\"T_AV\")\nfig=plt.subplot(1,2,2)\nfig=ozone.WSR_AV.interpolate().plot(title=\"WSR_AV\")","46078dd4":"WSR_columns = filter_startswith(ozone.columns, \"WSR\")\nlen(WSR_columns)","4fce5212":"ozone.iloc[:,26:]=ozone.drop(WSR_columns,axis=1).interpolate(method=\"linear\",limit_direction='forward')\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nozone[ozone.columns]=imputer.fit_transform(ozone)\nozone.isnull().sum()","c287db0c":"fig=plt.figure(figsize=(14,5))\nfig=plt.subplot(1,2,1)\nfig=ozone.T_AV.interpolate().plot(c=\"blue\",title=\"T_AV\")\nfig=plt.subplot(1,2,2)\nfig=ozone.WSR_AV.interpolate().plot(title=\"WSR_AV\")","e9615cd4":"T_columns = filter_startswith(ozone.columns, \"T\")\nT_columns = np.append(T_columns,\"target\")\n\ncorr = ozone[T_columns].corr()\nfig = plt.figure(figsize=(20,10))\nfig = sns.heatmap(round(corr,2), cmap=\"coolwarm\",fmt='.2f',annot=True)\nfig = plt.title(\"Correlation Temperature\")","6d89842d":"ozone[\"T14_16\"]=(ozone.T14+ozone.T15+ozone.T16)\/3\nozone[[\"target\",\"T14_16\", \"T14\",\"T15\", \"T16\", \"T_PK\"]].corr(\"kendall\")","059044e7":"WSR_columns = filter_startswith(ozone.columns, \"WSR\")\nWSR_columns = np.append(WSR_columns,\"target\")\n\ncorr = ozone[WSR_columns].corr()\nfig = plt.figure(figsize=(20,10))\nfig = sns.heatmap(round(corr,3), cmap=\"coolwarm\",fmt='.2f')\nfig = plt.title(\"Correlation WSR\")","a83c9964":"ozone[\"WSR10_13\"]=(ozone.WSR10+ozone.WSR11+ozone.WSR12)\/3","337729ba":"features = [\"WSR_PK\",\"T_PK\",\"T85\",\"T70\",\"RH70\",\"RH50\",\"U70\",\"U50\",\"V50\",\"V70\",\"HT70\",\"HT50\",\"KI\",\"TT\",\"SLP\",\"SLP_\",\"Precp\",\"target\"]\ncorr = ozone[features].corr()\nfig = plt.figure(figsize=(11,7))\nfig = sns.heatmap(round(corr,2), cmap=\"coolwarm\",fmt='.2f', annot=True)\nfig = plt.title(\"Correlation Importance Variables\")","41c3006f":"scaler=MinMaxScaler()\nx=scaler.fit_transform(ozone.drop(\"target\",axis=1))\ntsne_fitting = TSNE(n_components=2, perplexity=25, n_iter=250, learning_rate=100, metric=\"braycurtis\")\nX_embedded = tsne_fitting.fit_transform(x)\nplot_data = pd.DataFrame([])\nplot_data['tsne-one'] = X_embedded[:,0]\nplot_data['tsne-two'] = X_embedded[:,1]\nplot_data['y'] = ozone.target.values\nplt.figure(figsize=(16,10))\nplt.title('t-SNE vs Target')\nfig=sns.scatterplot(\n    x=\"tsne-one\", y=\"tsne-two\",\n    hue=\"y\",\n    data=plot_data,\n)","71721028":"predictors = [\"WSR_PK\",\"T_PK\",\"T14_16\",\"WSR10_13\",\"RH70\",\"RH50\",\"U70\",\"U50\",\"V50\",\"V70\",\"HT70\",\"HT50\",\"KI\",\"TT\",\"SLP\",\"SLP_\",\"Precp\"]\nX = supervised_lag(ozone[predictors], 3)\ny = ozone.target.values[3:]\nX.head(2)","bb44a41b":"impRF = RandomForestClassifier(n_estimators=500, criterion=\"gini\", random_state=123)\nimpRF.fit(X,y)\n\nimp = {}\nfor i in range(len(X.columns)):\n    imp[X.columns[i]] = [impRF.feature_importances_[i]]\npd.DataFrame.from_dict(imp, orient=\"index\", columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False).head(25).style.background_gradient()","bbb92aaa":"ozone[\"month\"]=pd.DataFrame(ozone.index).Date.apply(lambda x: x.strftime(\"%b\")).values\ndummy_month=pd.get_dummies(ozone.month)\nozone=pd.concat([ozone.drop(\"month\",axis=1),dummy_month],axis=1)","0f8cc713":"final_predictors = [\"WSR10_13\",\"T14_16\", \"U70\", \"RH50\",\"V70\", \"TT\", \"KI\"]\nfinal_predictors = np.append(final_predictors, dummy_month.columns)\nfinal_predictors","46c989ca":"train_X, train_y, test_X, test_y = ts_train_test(ozone, final_predictors, \"target\", 640)","1208a8dd":"train_y.tail(1)","07fe810f":"test_X.head(1)","2d8c1f75":"pd.crosstab(train_y,\"count\",normalize=True)","7a669dc9":"pd.crosstab(test_y,\"count\",normalize=True)","e73ed89e":"scaler = MinMaxScaler().fit(train_X)\ntrain_X[final_predictors] = scaler.transform(train_X)\ntest_X[final_predictors] = scaler.transform(test_X)","23d98aa5":"train_X.columns","1a8655ff":"train_X_R = ts_lag(train_X, 2)\nprint(train_X_R.shape)\ntrain_y=train_y[2:]\ntest_X_R = ts_lag_test(train_X,test_X,2)\ntest_X_R.shape","c9da3f5e":"model, history, = make_LSTM(train_X_R, train_y, test_X_R, test_y, units=16, epochs=50, batch_size=64,seed=303)\nevaluate_nn(train_X_R, test_X_R, test_y)\nmodel.evaluate(test_X_R,test_y)","1f188f57":"plot_loss(history)\nplot_auc(history)\nplot_roc(test_y, model.predict(test_X_R))","c2158eb8":"units = [8,16,32] # Try [8,16,32,64]\nepochs=[256] # Try [50,64,128,256]\nbatch_size=[128] # Try [32,64,128]\ntune_nn(units=units, epochs=epochs, batch_size=batch_size)","ee38fc02":"model, history, = make_LSTM(train_X_R, train_y, test_X_R, test_y, units=32, epochs=256, batch_size=128,seed=303)\nmodel.evaluate(test_X_R,test_y)","a33f9910":"plot_loss(history)\nplot_auc(history)\nplot_roc(test_y, model.predict(test_X_R))","e53bfcf6":"y_prob=model.predict(test_X_R)\nFind_Optimal_Cutoff(test_y,y_prob)","10244f9e":"y_pred=np.where(y_prob>0.1977,1,0) # Optimum cut","07d41b0c":"visual=pd.DataFrame()\nvisual[\"target\"]=ozone.target[-640:]\nvisual[\"pred\"]=y_pred\nfig=visual.plot(alpha=.8)\nfig=plt.figure()\nfig=visual.target.plot()","7d9fbfd7":"confusion_matrix(test_y,y_pred,labels=(1,0))","b4f3a997":"print(f\"Acc: {(24+531)\/(24+531+81+4)}, Recall:{24\/(24+4)}, Especificidad: {531\/(531+81)}\")","2efbb4f3":"def make_Stacked_LSTM(train_X,train_y,test_X,test_y,units,epochs,batch_size,seed):\n    tf.keras.backend.clear_session()\n    tf.random.set_seed(seed)\n    model = Sequential()\n    model.add(LSTM(units, return_sequences=True,\n                      input_shape=(train_X.shape[1],train_X.shape[2]),kernel_initializer=\"he_uniform\"\n                  ))\n    model.add(LSTM(units,kernel_initializer=\"he_uniform\"))\n    model.add(Dense(1,activation=\"sigmoid\"))\n    model.compile(loss=\"BinaryCrossentropy\", optimizer=\"rmsprop\", metrics=[\"AUC\"])\n    history=model.fit(train_X, train_y, epochs=epochs, batch_size=batch_size,\n                         validation_data=(test_X,test_y), verbose=0, shuffle = False)\n \n    return model, history","dea7ef52":"model, history, = make_Stacked_LSTM(train_X_R, train_y, test_X_R, test_y, units=32, epochs=115, batch_size=128,seed=303)\nmodel.evaluate(test_X_R,test_y)","3449c743":"plot_loss(history)\nplot_auc(history)\nplot_roc(test_y, model.predict(test_X_R))","24fee7b9":"A very strong imbalance of the response variable is observed. Accuracy won't be a good model performance metric, AUC or F-score are recommended. Ozone days represent the 6.7%.","cb173821":"For WSR it was preferred the mean.","a65ae66c":"# 9 Train\/Test\n\nWe are going to predict the last 1 and 9 months. Why not 2 years? Because from January to March, target is always 0 and the target variable may be similarly distributed in both train & test.\n![](input\/ozone-level-detection-data-set\/dist_test.png)\n![](https:\/\/github.com\/r4msi\/LSTM_Ozone\/blob\/master\/dist_test.png)","b271cca6":"Having so many variables can make information redundant and increase the cost of computing. That is, the temperature at 00 in the morning, may be very similar to that of 1, therefore, one could make its average and remove a variable. \n\n**When the target variable is binary, the pearson correlation is not the best method, but it still works.**\n\n- Note how temperatures at central times of the day are associated with a higher correlation with the target variable.\n- That's why T_pk is also very correlated. There's not point to use all those variables having T_pk.\n- Maybe the mean between T14,T15,T16.","e5efea6a":"# 7 Feature Selection","5ad7b46d":"# 13. Stacked LSTM","61b7e14d":"### 10.2 Lag the data & Reshape\n\nN\u00ba obs, 3 time steps (sequence) (t-2,t-1,t) & 17 variables.","b668bc80":"# 11 Model & Evaluation","9bfde347":"# 1. Workflow through Fuctions\n\nOwn functions to make the process esasier. You can skip this part.","d63cc9ae":"### 3.2 Missing Values","b2dabcd6":"- There is not much different betweek Test & Train. Usually test is above train in Loss... Maybe it is a signal of **underfiting.**\n- AUC starts below in test and then is above train.\n- Maybe around 22 epochs are enough. However, it does not seem to overfit.\n- The AUC is high so the model is not predicting randomly.","9949a36a":"### 3.3 Target Distribution","04dc97ed":"### 1.6 Evaluation ","e4744e4c":"To keep it fast:","6aecda88":"*******************************************************","27a0867f":"1. Introduce class_weight for imbalance data.\n2. Use a sophisticated imputer algorithm.\n3. More feature engineering: Include weeks...\n4. Try CNN+LSTM models.","c6734fd8":"# 6. Dimensional Reduction\n\nT-SNE. It does not seem to be useful.","b428df08":"- `Batch`: It the set of observations that are used in each backpropagation iteration. At the end of each iteration, the parameters will be updated and goes the next batch.\n- `epoch`: It is the number of times that the backpropagation algorithm goes through the whole train set.","21153d80":"NNs are stochastics models so the result might vary because of the initial wights randomness. That is why it is advisable to test several NN and check if the results are steady. \nIn addition, 3 quite simple functions have been created to evaluate the model graphically.","35640d45":"### 1.2 Train\/Test \nIt is going to be used to split the data.","bf01edbf":"References: Deep Learning for\nTime Series Forecasting\nPredict the Future with MLPs,\nCNNs and LSTMs in Python\nJason Brownlee","6dcff61d":"How are the predictions the for the last year? \n\n- They seem to be good, even the ones that fail, are close to being high ozone days. It has defenitely learn6 that the ozone is low in winter.","ba6c9618":"# 8 Feature Engineering","fd814efc":"# 4. Imputation","e64a5d45":"Overall, it's a proper model. Despite the target is imbalanced, it has a nice recall 0.86, therefore, its ability to recognize true positives is high","2d1fac6e":"While not a bad model, an attempt will be made to improve the underfiting situation. \nMultiple hidden LSTM layers can be stacked. The problem is that the output of a LSTM will be two-dimensional, nontheless, it can be tackle by setting the return sequences=True.\n\n- Introducing activation function in LSTM layer did not improve the model.","8fc375be":"Time variables are going to be created in order to capture the posible seasonality.","7f9be18a":"For time series, interpolation is a fast and good way of imputation:\n   - Having values for t-1 = 10 & t-3 = 20, while t-2 is missing, the replacement will be the mean between t-1 and t-3; t-2=15.\nHowever it's not clear for temperature & WSR.\n- In the plots, it seems that for WSR it not going to be very accurate.","b20d1ce0":"### 1.7 Tune hyperparameters\n\nNeural networks have quite hyperparameters to tune. The number of units, batches, epochs, the optimizer, dropout rate... Here it is simply left out and only testing, units, batches and epochs. This function goes through all possible combinations and evaluates the model on different seeds.","30608ae5":"*The aim of this practice is to solve a time series classification problem. The target variable is whether or not the ozone level was high for a particular day at Houston, Texas.* The predictor variables are the following:\n\n- `WSR` variables: Wind Speep (There is a variable for every hour of the day).\n- `WSR_PK`: Maximun Wind Speed for that day.\n- `WSR_AV`: Average Wind Speed for that day.\n- `T` variables: Temperature (There is a variable for every hour of the day).\n- `T_PK`: Peak temperature for that day.\n- `T_AV`: Average temperature for that day.\n- Upwind ozone background level, Precursor emissions related factor, Total Solar Radiation for the day. (U, HT, TH, V...).\n- `SLP`: Sea level pressure.\n- `SLP_`: SLP change for that day.\n- `KI`: K-Index.\n- `TT`: T (totals).\n","be933ead":"### 1.4 Lag variables in Test\n\nOne problem with creating lag variables is that observations are lost as the lag increases. The same transformations should be done in train and test, but while rows can be lost in train, they should not be lost in test. The thing is that when you lag a variable in test, the previous value is still in train. **A simpler alternative is to first lag the entire dataset and then split it.**","00efd352":"# 14 To improve:","c39de91a":"# 2. Load Data:\n- NaN values were saved as \"?\"\n- \"Date\" was the time variable so was converted to datetime64 and set as the index.","23e93c83":"\n\n* The missing values from WSR & Temperature correspond to the second half of 2002. For the remaining variables are missing at Random (So interpolation would be a good method, except for WSR and Temperature).","157bc8a1":"Graphical representation by year:","6c5737eb":"- T_PK & T85 have a positive correlation of 0.88.\n- KI & TT of 0.86.\n- There are not strong linear relationship with the target variable. ","e9431073":"Final:","3dd0b1a6":"Remaining Variables:","e9ff82bf":"# 12 Grid Search","d3b63bff":"### 1.3 Create lag variables for Neural Networks (CNN, LSTM...)\n\nOne futher transformation is needed to fit RNN or CNN because data must be in a three-dimensional structure.\nSimilar to \"supervised_lag\" function but it also resaphes the 2d dataframe into 3d tensor. For Time series the dimensions are:\n- N\u00ba of observations.\n- Time Steps to make the sequence.\n- N\u00ba of variables.","e3e8d52e":"The EDA process can be atomated with the Sweetviz library. However a \"manual\" EDA will be also done.","86ea8ae6":"# 5 Linear Correlation","e186251e":"365*2 = 730 days, -3 months (90 days) = 640 days.","d1bd6ff5":"### 1.1 Create Lag Variables for ML algorithms\n\nWhen working with multivariate time series, data must be transformed in order to use \"the past\" to predict the future. This fuction takes the variables of a dataframe and creates lags of that variables: t-1,t-2 ... t-x, in a neat way, so that it can be used inmediately to fit ML models or even a Multilayer Perceptron NN.\nIt is going to be used for feature selection with a Random Forest.","fa1a7c77":"A logical way to select variables would be to create all possible combinations of models, from more to less variables and get the best one. One way to approach this is to use a RandomForest that creates X tree models in which the variables are randomly selected to build the trees and not all of them come in at once. Also, not all the observations are used to build the trees, so they can be evaluated with what is known as OutOfBag, which are the observations that did not enter the model so they are honest.\nRF can be applied to variable selection because it obtains the variables that most reduce the gini index. The impurity is understood when CARTs create heterogeneous groups, that is, that in their final nodes there are probabilities of 0.5 and 0.5 for the two classes, since it does not discriminate them. On the other hand, if a node distributes with 0.9 and 0.1, is a homogeneous group and that is what is interesting to classify. \nThe number of variables entering the trees by default is the square root of the number of variables and 500 trees will be perform. ","69a4e4e9":"### 10.1 Scale Data","115bb557":"# 10 Data Preparation for NN","4d5ce431":"### 3.4 Predictors","8fbea0e1":"# 3. EDA","7e9fe639":"- **Effectively complicating the model, the loss of train and test is approaching.**\n- AUC is 0.92 whereas the simpler model was 0.88\n- Maybe 200 epochs would have been better.","6158b454":"Additional helpers:","4b55d7fa":"# 12 Try the best model","fabfc63b":"There seems to be more variety with WSR.","c4fcf75c":"It seems that every single column excepting Date & target contains NaN. 300 missing values represent around a 12% for that variables. ","2dad9644":"### 1.5 The Model\n\nIn this case the use of RNN is required. LSTMs work especially well for time series. To begin with, a \"vanilla\" LSTM is going to be created with a single hidden layer of LSTM units and an output layer used to make predictions. As it is classification, the activation function for the output must be \"Sigmoid\", because it will always get result between 0 and 1.\n\nIt's a binary problem so the loss will be Binary Cross Entropy and based on empirical testing, rmsprop worked better than adam.\nThe good think about LSTM is that, unlike, CNN (that reads across the entire imput vector), LSTM reads one time step of the sequence at a time and builds up an internal state representation that can be used as a learned context for making prediction.\n(Later on stacked LSTM will be created).","1e9a07e9":"The dataset contains so many variables because of Wind Speed and Tempeture: *There are records for each hour of the day.* Nevertheless, its average variables are also in the data so there is not need to plot all series. Similar to U50-U70, T85-T70 etc.","bc5f3adf":"### 3.1 General stats","4e55cca8":"The data describes almost 7 years of ground ozone concentration.","9c65b015":"Libraries:","cb54076f":"A little bit better.","3c6624ee":" # RNN: LSTM for Ozone Level Detection\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Ozone+Level+Detection","5888fe86":"It is important to train the scaler with the minimum and maximum of train, and transfer them to the test. If not, data leak could occur, that is, give information of the test data (its min and max) to the model and it could not be validated in an honest way.","f1dac61f":"The dataset is composed by 2534 observations & 74 variables taking into accout the \"Date\"."}}