{"cell_type":{"994c2696":"code","d9f60cc2":"code","5dd4c02c":"code","ce430a59":"code","67a8a60c":"code","46965267":"code","a45c3377":"code","cf2350d4":"code","7cb706eb":"code","54038a0f":"code","859e2a5c":"code","bbf99536":"code","77f30fbb":"code","a082245d":"code","5b0287b4":"code","5640a8b6":"code","2d3d7573":"code","8950c536":"code","f350c4b1":"markdown","af8a2fc7":"markdown","fde4b5a0":"markdown","631a8f0d":"markdown","902a8a42":"markdown","f47d3839":"markdown","bcc343e4":"markdown","9fcd0e3f":"markdown","6be211b1":"markdown","acacb846":"markdown","555e529f":"markdown","045b926f":"markdown"},"source":{"994c2696":"import numpy as np \nimport pandas as pd \nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n\n\n\nprint(os.listdir(\"..\/input\"))","d9f60cc2":"# The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and \n# statistical natural language processing for English written in the Python programming language.\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nfrom bs4 import BeautifulSoup\nimport re\n\n#TQDM is a progress bar library with good support for nested loops and Jupyter\/IPython notebooks.\nfrom tqdm import tqdm\n","5dd4c02c":"from keras.utils import to_categorical\nimport random\nfrom tensorflow import set_random_seed\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\n\n#set random seed for the session and also for tensorflow that runs in background for keras\nset_random_seed(123)\nrandom.seed(123)\n","ce430a59":"\ntrain= pd.read_csv(\"..\/input\/train.tsv\", sep=\"\\t\")\ntest = pd.read_csv(\"..\/input\/test.tsv\", sep=\"\\t\")\n\ntrain.head()","67a8a60c":"train.shape","46965267":"test.head()","a45c3377":"test.shape","cf2350d4":"\ndef clean_sentences(df):\n    reviews = []\n\n    for sent in tqdm(df['Phrase']):\n        \n        #remove html content\n        review_text = BeautifulSoup(sent).get_text()\n        \n        #remove non-alphabetic characters\n        review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    \n        #tokenize the sentences\n        words = word_tokenize(review_text.lower())\n    \n        #lemmatize each word to its lemma\n        lemma_words = [lemmatizer.lemmatize(i) for i in words]\n    \n        reviews.append(lemma_words)\n\n    return(reviews)\n\n","7cb706eb":"#cleaned reviews for both train and test set retrieved\ntrain_sentences = clean_sentences(train)\ntest_sentences = clean_sentences(test)\nprint(len(train_sentences))\nprint(len(test_sentences))","54038a0f":"target=train.Sentiment.values\ny_target=to_categorical(target)\nnum_classes=y_target.shape[1]","859e2a5c":"X_train,X_val,y_train,y_val=train_test_split(train_sentences,y_target,test_size=0.2,stratify=y_target)","bbf99536":"#It is needed for initializing tokenizer of keras and subsequent padding\n\nunique_words = set()\nlen_max = 0\n\nfor sent in tqdm(X_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \n#length of the list of unique_words gives the no of unique words\nprint(len(list(unique_words)))\nprint(len_max)","77f30fbb":"tokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(X_train))\n\n#texts_to_sequences(texts)\n\n    # Arguments- texts: list of texts to turn to sequences.\n    #Return: list of sequences (one per text input).\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\nX_test = tokenizer.texts_to_sequences(test_sentences)\n\n#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\n\nX_train = sequence.pad_sequences(X_train, maxlen=len_max)\nX_val = sequence.pad_sequences(X_val, maxlen=len_max)\nX_test = sequence.pad_sequences(X_test, maxlen=len_max)\n\nprint(X_train.shape,X_val.shape,X_test.shape)","a082245d":"early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]\n\n","5b0287b4":"#Model using Keras LSTM\n\n#Multilayer Perceptron (MLP) for multi-class softmax classification:\n#Let\u2019s build what\u2019s probably the most popular type of model in NLP at the moment: Long Short Term Memory network. \n#This architecture is specially designed to work on sequence data.\n#It fits perfectly for many NLP tasks like tagging and text classification.\n#It treats the text as a sequence rather than a bag of words or as ngrams.\n\n#Here\u2019s a possible model definition:\n\nmodel=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(LSTM(128,dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\nmodel.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\nmodel.add(Dense(100,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes,activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\nmodel.summary()","5640a8b6":"#This is done for learning purpose only. One can play around with different hyper parameters combinations\n#and try increase the accuracy even more. For example, a different learning rate, an extra dense layer \n# before output layer, etc. Cross validation could be used to evaluate the model and grid search \n# further to find unique combination of parameters that give maximum accuracy. This model has a validation\n#accuracy of around 66.5%\nhistory=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6, batch_size=256, verbose=1, callbacks=callback)","2d3d7573":"import matplotlib.pyplot as plt\n\n# Create count of the number of epochs\nepoch_count = range(1, len(history.history['loss']) + 1)\n\n# Visualize learning curve. Here learning curve is not ideal. It should be much smoother as it decreases.\n#As mentioned before, altering different hyper parameters especially learning rate can have a positive impact\n#on accuracy and learning curve.\nplt.plot(epoch_count, history.history['loss'], 'r--')\nplt.plot(epoch_count, history.history['val_loss'], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","8950c536":"#make the predictions with trained model and submit the predictions.\ny_pred=model.predict_classes(X_test)\n\nsub_file = pd.read_csv('..\/input\/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=y_pred\nsub_file.to_csv('Submission.csv',index=False)","f350c4b1":"## Early stopping to prevent overfitting\nEarly stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset. In this tutorial, you will discover the Keras API for adding early stopping to overfit deep learning neural network models.","af8a2fc7":"## Loading important Libraries","fde4b5a0":"## Load Dataset","631a8f0d":"## split into train and validation sets.","902a8a42":"### What is Keras\nKeras is a deep learning framework that actually under the hood uses other deep learning frameworks in order to expose a beautiful, simple to use and fun to work with, high-level API. Keras can use either of these backends:\n\n\n    Tensorflow \u2013 Google\u2019s deeplearning library\n    Theano \u2013 may not be further developed\n    CNTK \u2013 Microsoft\u2019s deeplearning library\n    MXNet \u2013 deeplearning library from Apache.org (currently under development)\n\nKeras uses these frameworks to deliver powerful computation while exposing a beautiful and intuitive (that kinda looks like scikit-learn) API.\n\nHere\u2019s what Keras brings to the table:\n\n    The integration with the various backends is seamless\n    Run training on either CPU\/GPU\n    Comes in two flavours: sequential or functional. Just to ways of thinking about building models. The resulting models are perfectly equivalent. We\u2019re going to use the sequential one.\n    Fast prototyping \u2013 With all these good abstractions in place, you can just focus more on the problem and hyperparameter tunning.\n    \nLet\u2019s now start using Keras to develop various types of models for Natural Language Processing. Here\u2019s what we\u2019ll be building:\n\n    (Dense) Deep Neural Network \u2013 The NN classic model \u2013 uses the BOW model\n    Convolutional Network \u2013 build a network using 1D Conv Layers \u2013 uses word vectors\n    Recurrent Networks \u2013 LSTM Network \u2013 Long Short-Term Memory \u2013 uses word vectors\n    Transfer learning for NLP \u2013 Learn how to load spaCy\u2019s vectors or GloVe vectors \u2013 uses word vectors","f47d3839":"## Submission","bcc343e4":"# Function for cleaning the reviews, tokenize and lemmatize them.\n\nThis function will take each phrase iteratively and it will \n    \n        remove html content\n        remove non-alphabetic characters\n        tokenize the sentences\n        lemmatize each word to its lemma\nand then return the result in the list named reviews","9fcd0e3f":"## Collect the dependent values and convert to one-hot encoded output using to_categorical","6be211b1":"## Geting the no of unique words and max length of a review available in the list of cleaned reviews.","acacb846":"## Actual tokenizer of keras and convert to sequences","555e529f":"  # About Dataset\nThe dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train\/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short\/common words) are only included once in the data.\n\ntrain.tsv contains the phrases and their associated sentiment labels. We have additionally provided a SentenceId so that you can track which phrases belong to a single sentence.\n\ntest.tsv contains just phrases. You must assign a sentiment label to each phrase.\n\nThe sentiment labels are:\n\n0 - negative\n\n1 - somewhat negative\n\n2 - neutral\n\n3 - somewhat positive\n\n4 - positive","045b926f":"## fit the model"}}