{"cell_type":{"0cbfce59":"code","d75ecff6":"code","250ba983":"code","b374da69":"code","295b6966":"code","9a27a042":"code","95ce1d59":"code","0f5f878b":"code","8991ad72":"code","4daeaa47":"code","363e26aa":"code","feef0b6e":"code","9da403b2":"code","a402d6da":"code","2d05471f":"code","5d66406c":"code","d926612d":"code","aef01ff2":"code","6f567d0c":"code","d9a10e7f":"code","6fb188ca":"code","8757bc5e":"code","775a1928":"code","b49f063f":"code","3eb61507":"code","2b6d2bcd":"code","cded0bfa":"code","0db1d4f2":"code","cf893623":"code","1348a9fa":"code","f399206d":"code","61c4d9e0":"markdown","0d52ac05":"markdown","082509a2":"markdown","4e57a404":"markdown","c2d4a5bb":"markdown","f7f8c346":"markdown","4e4091aa":"markdown","65df1cf4":"markdown","2b534be0":"markdown","55e17ad4":"markdown","0631ea8e":"markdown","59ff6c50":"markdown","7f9b7b76":"markdown","d162dee8":"markdown","e223520c":"markdown","0a77fce9":"markdown","7c4ad519":"markdown","5fff82f9":"markdown","7f79b913":"markdown","9dce2e81":"markdown","2bcdd515":"markdown","54816a72":"markdown","362e46de":"markdown","585cb6ee":"markdown","5221a7df":"markdown","5d8eb8a5":"markdown","175bf981":"markdown","1f9c47d7":"markdown","e2c591d8":"markdown","20e67860":"markdown","fdbdd93f":"markdown","15dcda97":"markdown","05776c33":"markdown","a1e71ae4":"markdown","6d2eb67b":"markdown"},"source":{"0cbfce59":"!pip install bayesian-optimization","d75ecff6":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import rankdata\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nimport warnings\n\npd.set_option('display.max_columns', 200)","250ba983":"train_df = pd.read_csv('..\/input\/train.csv')\n\ntest_df = pd.read_csv('..\/input\/test.csv')","b374da69":"train_df.head()","295b6966":"test_df.head()","9a27a042":"target = 'target'\npredictors = train_df.columns.values.tolist()[2:]","95ce1d59":"train_df.target.value_counts()","0f5f878b":"bayesian_tr_index, bayesian_val_index  = list(StratifiedKFold(n_splits=2, shuffle=True, random_state=1).split(train_df, train_df.target.values))[0]","8991ad72":"def LGB_bayesian(\n    num_leaves,  # int\n    min_data_in_leaf,  # int\n    learning_rate,\n    min_sum_hessian_in_leaf,    # int  \n    feature_fraction,\n    lambda_l1,\n    lambda_l2,\n    min_gain_to_split,\n    max_depth):\n    \n    # LightGBM expects next three parameters need to be integer. So we make them integer\n    num_leaves = int(num_leaves)\n    min_data_in_leaf = int(min_data_in_leaf)\n    max_depth = int(max_depth)\n\n    assert type(num_leaves) == int\n    assert type(min_data_in_leaf) == int\n    assert type(max_depth) == int\n\n    param = {\n        'num_leaves': num_leaves,\n        'max_bin': 63,\n        'min_data_in_leaf': min_data_in_leaf,\n        'learning_rate': learning_rate,\n        'min_sum_hessian_in_leaf': min_sum_hessian_in_leaf,\n        'bagging_fraction': 1.0,\n        'bagging_freq': 5,\n        'feature_fraction': feature_fraction,\n        'lambda_l1': lambda_l1,\n        'lambda_l2': lambda_l2,\n        'min_gain_to_split': min_gain_to_split,\n        'max_depth': max_depth,\n        'save_binary': True, \n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,   \n\n    }    \n    \n    \n    xg_train = lgb.Dataset(train_df.iloc[bayesian_tr_index][predictors].values,\n                           label=train_df.iloc[bayesian_tr_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[bayesian_val_index][predictors].values,\n                           label=train_df.iloc[bayesian_val_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    num_round = 5000\n    clf = lgb.train(param, xg_train, num_round, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    \n    predictions = clf.predict(train_df.iloc[bayesian_val_index][predictors].values, num_iteration=clf.best_iteration)   \n    \n    score = metrics.roc_auc_score(train_df.iloc[bayesian_val_index][target].values, predictions)\n    \n    return score","4daeaa47":"# Bounded region of parameter space\nbounds_LGB = {\n    'num_leaves': (5, 20), \n    'min_data_in_leaf': (5, 20),  \n    'learning_rate': (0.01, 0.3),\n    'min_sum_hessian_in_leaf': (0.00001, 0.01),    \n    'feature_fraction': (0.05, 0.5),\n    'lambda_l1': (0, 5.0), \n    'lambda_l2': (0, 5.0), \n    'min_gain_to_split': (0, 1.0),\n    'max_depth':(3,15),\n}","363e26aa":"from bayes_opt import BayesianOptimization","feef0b6e":"LGB_BO = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=13)","9da403b2":"print(LGB_BO.space.keys)","a402d6da":"init_points = 5\nn_iter = 5","2d05471f":"print('-' * 130)\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings('ignore')\n    LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)","5d66406c":"LGB_BO.max['target']","d926612d":"LGB_BO.max['params']","aef01ff2":"# parameters from version 2 of\n#https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction?scriptVersionId=10522231\n\nLGB_BO.probe(\n    params={'feature_fraction': 0.1403, \n            'lambda_l1': 4.218, \n            'lambda_l2': 1.734, \n            'learning_rate': 0.07, \n            'max_depth': 14, \n            'min_data_in_leaf': 17, \n            'min_gain_to_split': 0.1501, \n            'min_sum_hessian_in_leaf': 0.000446, \n            'num_leaves': 6},\n    lazy=True, # \n)","6f567d0c":"LGB_BO.maximize(init_points=0, n_iter=0) # remember no init_points or n_iter","d9a10e7f":"for i, res in enumerate(LGB_BO.res):\n    print(\"Iteration {}: \\n\\t{}\".format(i, res))","6fb188ca":"LGB_BO.max['target']","8757bc5e":"LGB_BO.max['params']","775a1928":"param_lgb = {\n        'num_leaves': int(LGB_BO.max['params']['num_leaves']), # remember to int here\n        'max_bin': 63,\n        'min_data_in_leaf': int(LGB_BO.max['params']['min_data_in_leaf']), # remember to int here\n        'learning_rate': LGB_BO.max['params']['learning_rate'],\n        'min_sum_hessian_in_leaf': LGB_BO.max['params']['min_sum_hessian_in_leaf'],\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'feature_fraction': LGB_BO.max['params']['feature_fraction'],\n        'lambda_l1': LGB_BO.max['params']['lambda_l1'],\n        'lambda_l2': LGB_BO.max['params']['lambda_l2'],\n        'min_gain_to_split': LGB_BO.max['params']['min_gain_to_split'],\n        'max_depth': int(LGB_BO.max['params']['max_depth']), # remember to int here\n        'save_binary': True,\n        'seed': 1337,\n        'feature_fraction_seed': 1337,\n        'bagging_seed': 1337,\n        'drop_seed': 1337,\n        'data_random_seed': 1337,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n        'boost_from_average': False,\n    }","b49f063f":"nfold = 5","3eb61507":"gc.collect()","2b6d2bcd":"skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=2019)","cded0bfa":"oof = np.zeros(len(train_df))\npredictions = np.zeros((len(test_df),nfold))\n\ni = 1\nfor train_index, valid_index in skf.split(train_df, train_df.target.values):\n    print(\"\\nfold {}\".format(i))\n    xg_train = lgb.Dataset(train_df.iloc[train_index][predictors].values,\n                           label=train_df.iloc[train_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )\n    xg_valid = lgb.Dataset(train_df.iloc[valid_index][predictors].values,\n                           label=train_df.iloc[valid_index][target].values,\n                           feature_name=predictors,\n                           free_raw_data = False\n                           )   \n\n    \n    clf = lgb.train(param_lgb, xg_train, 5000, valid_sets = [xg_valid], verbose_eval=250, early_stopping_rounds = 50)\n    oof[valid_index] = clf.predict(train_df.iloc[valid_index][predictors].values, num_iteration=clf.best_iteration) \n    \n    predictions[:,i-1] += clf.predict(test_df[predictors], num_iteration=clf.best_iteration)\n    i = i + 1\n\nprint(\"\\n\\nCV AUC: {:<0.2f}\".format(metrics.roc_auc_score(train_df.target.values, oof)))","0db1d4f2":"predictions","cf893623":"print(\"Rank averaging on\", nfold, \"fold predictions\")\nrank_predictions = np.zeros((predictions.shape[0],1))\nfor i in range(nfold):\n    rank_predictions[:, 0] = np.add(rank_predictions[:, 0], rankdata(predictions[:, i].reshape(-1,1))\/rank_predictions.shape[0]) \n\nrank_predictions \/= nfold","1348a9fa":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = rank_predictions\nsub_df[:10]","f399206d":"sub_df.to_csv(\"Customer_Transaction_rank_predictions.csv\", index=False)","61c4d9e0":"Now, let's the the key space (parameters) we are going to optimize:","0d52ac05":"## Notebook  Content\n0. [Installing Bayesian global optimization library](#0) <br>    \n1. [Loading the data](#1)\n2. [Black box function to be optimized (LightGBM)](#2)\n3. [Training LightGBM model](#3)\n4. [Rank averaging](#4)\n5. [Submission](#5)","082509a2":"So we got 0.90 AUC in 5 fold cross validation. And 5 fold prediction look like:","4e57a404":"# Bayesian global optimization with gaussian processes for finding (sub-)optimal parameters of LightGBM\n\nAs many of fellow kaggler asking how did I get LightGBM parameters for the kernel [Customer Transaction Prediction](https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction) I published. So, I decided to publish a kernel to optimize parameters. \n\n\n\nIn this kernel I use Bayesian global optimization with gaussian processes for finding optimal parameters. This optimization attempts to find the maximum value of an black box function in as few iterations as possible. In our case the black box function will be a function that I will write to optimize (maximize) the evaluation function (AUC) so that parameters get maximize AUC in training and validation, and expect to do good in the private. The final prediction will be **rank average on 5 fold cross validation predictions**.\n\nContinue to the end of this kernel and **upvote it if you find it is interesting**.\n\n![image.jpg](https:\/\/i.imgur.com\/XKS1oqU.jpg)\n\nImage taken from : https:\/\/github.com\/fmfn\/BayesianOptimization","c2d4a5bb":"<a id=\"2\"><\/a> <br>\n## 2. Black box function to be optimized (LightGBM)","f7f8c346":"Now I need to give bounds for these parameters, so that Bayesian optimization only search inside the bounds.","4e4091aa":"Let's build a model together use therse parameters ;)","65df1cf4":"These `bayesian_tr_index` and `bayesian_val_index` indexes will be used for the bayesian optimization as training and validation index of training dataset.","2b534be0":"If you are still reading, bare with me. I will not take much of your time. :D We are almost done. Let's do a rank averaging on 5 fold predictions.","55e17ad4":"In this kernel I will be using **50% Stratified rows** as holdout rows for the validation-set to get optimal parameters. Later I will use 5 fold cross validation in the final model fit.","0631ea8e":"As the optimization is done, let's see what is the maximum value we have got.","59ff6c50":"We have got a better validation score in the probe! As previously I ran `LGB_BO` only for 10 runs. In practice I increase it to arround 100.","7f9b7b76":"As you see, I assined `LGB_BO`'s optimal parameters to the `param_lgb` dictionary and they will be used to train a model with 5 fold.","d162dee8":"Let's put all of them in BayesianOptimization object","e223520c":"The problem is unbalanced! ","0a77fce9":"<a id=\"4\"><\/a> <br>\n## 4. Rank averaging","7c4ad519":"Number of Kfolds:","5fff82f9":"Wait, I want to show one more cool option from BayesianOptimization library. You can probe the `LGB_bayesian` function, if you have an idea of the optimal parameters or it you get **parameters from other kernel** like mine [mine](https:\/\/www.kaggle.com\/fayzur\/customer-transaction-prediction). I will copy and paste parameters from my other kernel here. You can probe as folowing:","7f79b913":"<a id=\"1\"><\/a> <br>\n## 1. Loading the data","9dce2e81":"<a id=\"0\"><\/a> <br>\n## 0. Installing Bayesian global optimization library\n\nLet's install the latest release from pip","2bcdd515":"Test dataset:","54816a72":"OK, by default these will be explored lazily (lazy=True), meaning these points will be evaluated only the next time you call maximize. Let's do a maximize call of `LGB_BO` object.","362e46de":"As data is loaded, let's create the black box function for LightGBM to find parameters.","585cb6ee":"Let's submit prediction to Kaggle.","5221a7df":"Finally, the list of all parameters probed and their corresponding target values is available via the property LGB_BO.res.","5d8eb8a5":"The above `LGB_bayesian` function will act as black box function for Bayesian optimization. I already defined the the trainng and validation dataset for LightGBM inside the `LGB_bayesian` function. \n\nThe `LGB_bayesian` function takes values for `num_leaves`, `min_data_in_leaf`, `learning_rate`, `min_sum_hessian_in_leaf`, `feature_fraction`, `lambda_l1`, `lambda_l2`, `min_gain_to_split`, `max_depth` from Bayesian optimization framework. Keep in mind that `num_leaves`, `min_data_in_leaf`, and `max_depth` should be integer for LightGBM. But Bayesian Optimization sends continous vales to function. So I force them to be integer. I am only going to find optimal parameter values of them. The reader may increase or decrease number of parameters to optimize.","175bf981":"Do not forget to upvote :) Also fork and modify for your own use. ;)","1f9c47d7":"<a id=\"3\"><\/a> <br>\n## 3. Training LightGBM model","e2c591d8":"Now we can use these parameters to our final model!","20e67860":"Distribution of target variable","fdbdd93f":"I have created the BayesianOptimization object (`LGB_BO`), it will not work until I call maximize. Before calling it, I want to explain two parameters of BayesianOptimization object (`LGB_BO`) which we can pass to maximize:\n- `init_points`: How many initial random runs of **random** exploration we want to perform. In our case `LGB_bayesian` will be called `n_iter` times.\n- `n_iter`: How many runs of bayesian optimization we want to perform after number of `init_points` runs. ","15dcda97":"Now, it's time to call the function from Bayesian optimization framework to maximize. I allow `LGB_BO` object to run for 5 `init_points` (exploration) and 5 `n_iter` (exploitation).","05776c33":"We are given anonymized dataset containing 200 numeric feature variables from var_0 to var_199. Let's have a look train dataset:","a1e71ae4":"The validation AUC for parameters is 0.89 ! Let's see parameters is responsible for this score :)","6d2eb67b":"<a id=\"5\"><\/a> <br>\n## 5. Submission"}}