{"cell_type":{"e9b6c2b0":"code","821e0845":"code","e0f9515c":"code","9be79bdc":"code","8b9c0917":"code","492124d5":"code","317a9969":"code","20228425":"code","3bd1972e":"code","16f61cab":"code","4befb513":"code","6211ef04":"code","cefa8c80":"code","ca8cbe3a":"code","31a99ebf":"code","eee39acf":"code","2b4a574a":"code","2ef32931":"code","7461cc8e":"code","c3985689":"code","25014b63":"code","744f1ff0":"code","3cf5f432":"code","81220f38":"code","6acce54e":"code","f9601410":"code","ff4a5b93":"code","960d0dca":"code","fc0fb6c1":"code","e45c8208":"code","cb1ef6ae":"code","b03963a9":"code","f8fe9d65":"code","49fa3f09":"code","969c0b7c":"code","84a24767":"code","9612bd5c":"code","d3f343cc":"markdown","3bbda0d5":"markdown","59e2b211":"markdown","fd889a4c":"markdown","573e9190":"markdown","eff77c2e":"markdown","9f6fc96c":"markdown"},"source":{"e9b6c2b0":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","821e0845":"from fastai.conv_learner import *","e0f9515c":"PATH = '..\/input\/planet-understanding-the-amazon-from-space\/'","9be79bdc":"ls {PATH}","8b9c0917":"from fastai.plots import *","492124d5":"def get_1st(path, pattern): return glob(f'{path}\/*{pattern}.*')[0]","317a9969":"dc_path = \"..\/input\/dogs-vs-cats-redux-kernels-edition\/train\"\nlist_paths = [get_1st(f\"{dc_path}\", \"cat\"), get_1st(f\"{dc_path}\", \"dog\")]\nplots_from_files(list_paths, titles=[\"cat\", \"dog\"], maintitle=\"Single-label classification\")","20228425":"list_paths = [f\"{PATH}train-jpg\/train_0.jpg\", f\"{PATH}train-jpg\/train_1.jpg\"]\ntitles=[\"haze primary\", \"agriculture clear primary water\"]\nplots_from_files(list_paths, titles=titles, maintitle=\"Multi-label classification\")","3bd1972e":"# planet.py\n\nfrom fastai.imports import *\nfrom fastai.transforms import *\nfrom fastai.dataset import *\nfrom sklearn.metrics import fbeta_score\nimport warnings\n\ndef f2(preds, targs, start=0.17, end=0.24, step=0.01):\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        return max([fbeta_score(targs, (preds>th), 2, average='samples')\n                    for th in np.arange(start,end,step)])","16f61cab":"metrics=[f2]\nf_model = resnet34","4befb513":"label_csv = f'{PATH}train_v2.csv'\nn = len(list(open(label_csv)))-1\nval_idxs = get_cv_idxs(n)","6211ef04":"def get_data(sz):\n    tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_top_down, max_zoom=1.05)\n    return ImageClassifierData.from_csv(PATH, 'train-jpg', label_csv, tfms=tfms,\n                    suffix='.jpg', val_idxs=val_idxs, test_name='test-jpg-v2')","cefa8c80":"data = get_data(256)","ca8cbe3a":"x,y = next(iter(data.val_dl))","31a99ebf":"y","eee39acf":"list(zip(data.classes, y[0]))","2b4a574a":"plt.imshow(data.val_ds.denorm(to_np(x))[0]*1.4);","2ef32931":"sz=64","7461cc8e":"data = get_data(sz)","c3985689":"data = data.resize(int(sz*1.3), '\/tmp')","25014b63":"TMP_PATH = \"\/tmp\/tmp\"\nMODEL_PATH = \"\/tmp\/model\/\"","744f1ff0":"learn = ConvLearner.pretrained(f_model, data, metrics=metrics, tmp_name=TMP_PATH, models_name=MODEL_PATH)","3cf5f432":"lrf=learn.lr_find()\nlearn.sched.plot()","81220f38":"lr = 0.2","6acce54e":"learn.fit(lr, 3, cycle_len=1, cycle_mult=2)","f9601410":"lrs = np.array([lr\/9,lr\/3,lr])","ff4a5b93":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)","960d0dca":"learn.save(f'{sz}')","fc0fb6c1":"learn.sched.plot_loss()","e45c8208":"sz=128","cb1ef6ae":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","b03963a9":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","f8fe9d65":"sz=256","49fa3f09":"learn.set_data(get_data(sz))\nlearn.freeze()\nlearn.fit(lr, 3, cycle_len=1, cycle_mult=2)","969c0b7c":"learn.unfreeze()\nlearn.fit(lrs, 3, cycle_len=1, cycle_mult=2)\nlearn.save(f'{sz}')","84a24767":"multi_preds, y = learn.TTA()\npreds = np.mean(multi_preds, 0)","9612bd5c":"f2(preds,y)","d3f343cc":"We use a different set of data augmentations for this dataset - we also allow vertical flips, since we don't expect vertical orientation of satellite images to change our classifications.","3bbda0d5":"### End","59e2b211":"## Multi-label versus single-label classification","fd889a4c":"In single-label classification each sample belongs to one class. In the previous example, each image is either a *dog* or a *cat*.","573e9190":"In multi-label classification each sample can belong to one or more clases. In the previous example, the first images belongs to two clases: *haze* and *primary*. The second image belongs to four clases: *agriculture*, *clear*, *primary* and  *water*.","eff77c2e":"## Multi-label models for Planet dataset","9f6fc96c":"## Multi-label classification"}}