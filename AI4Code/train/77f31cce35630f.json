{"cell_type":{"344cd5bb":"code","63353cde":"code","a57a9541":"code","8843fb6e":"code","d8c04295":"code","2ba7f73c":"code","f4fb15d5":"code","175cfe27":"code","414acf52":"code","ecdd2b24":"code","99d5123e":"code","8ea5f0e0":"code","c9567588":"code","bc899370":"code","5e0e845e":"code","37e54243":"code","e80c2eb7":"code","47878ec1":"code","4534a00c":"code","d48a8362":"code","9480ef1c":"code","85b0fa7e":"code","b8ce9cf1":"code","c3faa416":"code","5ad0beb2":"code","535256d8":"code","aa678563":"code","1080df06":"code","e95a7d39":"code","ea529660":"code","99739c5c":"code","18eb80c8":"code","6c35f3fe":"code","5ad1cdb3":"code","94493d99":"code","ebc6a644":"code","063b2eb6":"markdown","45da4b9e":"markdown","8879a68d":"markdown","69535aa2":"markdown","ad9cb03a":"markdown"},"source":{"344cd5bb":"# necessary imports\nimport pandas as pd\nimport numpy as np","63353cde":"# Reading files\nt_claim = pd.read_csv(\"..\/input\/Train_ClaimDetails.csv\")\nt_policy_demo = pd.read_csv(\"..\/input\/Train_Policy_Demographics.csv\")\nt_train = pd.read_csv(\"..\/input\/Train.csv\")","a57a9541":"# merging multiple dataframes with ClaimID\ndfs = [t_claim, t_policy_demo, t_train]\nfrom functools import reduce\ndf_final2 = reduce(lambda left,right: pd.merge(left,right,on='ClaimID'), dfs)\nClaimID = df_final2['ClaimID']","8843fb6e":"# Removing redundent variables\n# ClaimID, PolicyID\ndf_final = df_final2.drop(['ClaimID', 'PolicyID'], axis = 1)","d8c04295":"# converting date variables to date objects\ndf_final = df_final.astype({'Injury_Date':'datetime64', 'Date_reported': 'datetime64'})","2ba7f73c":"# taking time difference\ntime_diff = df_final['Date_reported'] - df_final['Injury_Date']\ndf_final['time_diff'] = time_diff.astype('str').str.split(\" \").str[0]","f4fb15d5":"# dropping date variables after taking day differences\ndf_final.drop(['Injury_Date', 'Date_reported'], axis = 1, inplace = True)","175cfe27":"# droping age_injured because the given format and the numbers does fit together\ndf_final.drop(['Age_Injured'], axis = 1, inplace = True)","414acf52":"# since the needed target is different from what's there, cleaning the target variables \ndf_final.loc[:, 'ClaimSize'] = df_final.loc[:, 'ClaimSize'].astype('category')\ndf_final.drop(df_final.loc[(df_final['ClaimSize'] == '3') | (df_final['ClaimSize'] == '2') |\\\n               (df_final['ClaimSize'] == '1')].index, axis = 0, inplace = True)","ecdd2b24":"# Strategy is to fill NaN's with zero where there is one element in a row and remaining are NaN's with the nested dict.\ndf_sparse = df_final.replace({'Work_related_injury_status': {'N':0, 'Y':1},\\\n                  'Amputation': {'B': 1, np.nan: 0},\\\n                  'Death' : {'A' : 1, np.nan: 0},\\\n                  'Burns_heat' : {'C' :1, np.nan: 0},\\\n                  'Burns_chemical' : {'D' :1, np.nan: 0},\\\n                  'SystemicPoisoning_toxic' : {'E' :1, np.nan: 0},\\\n                  'SystemicPoisoning_other' : {'F' :1, np.nan: 0},\\\n                  'Eye_injury_blindness' : {'G' :1, np.nan: 0},\\\n                  'RespiratoryCondition' : {'H' :1, np.nan: 0},\\\n                  'NervousCondition' : {'I' :1, np.nan: 0},\\\n                  'HearingLoss': {'J' :1, np.nan: 0},\\\n                  'CirculatoryCondition' : {'K':1, np.nan:0},\\\n                  'MultipleInjuries' : {'L':1, np.nan:0},\\\n                  'BackInjury' : {'M':1, np.nan:0},\\\n                  'SkinDisorder': {'N':1, np.nan:0},\\\n                  'BrainDamage' : {'O':1, np.nan:0},\\\n                  'Scarring' : {'P':1, np.nan:0},\\\n                  'SpinalCordInjuries' : {'Q':1, np.nan:0},\\\n                  'OtherInjuries' : {'R':1, np.nan:0},\\\n                  'OffRoadVehicle' : {'A':1, np.nan:0},\\\n                  'AirTransportation' : {'B':1, np.nan:0},\\\n                  'Railway' : {'C' :1, np.nan: 0},\\\n                  'OtherMotorVehicle' : {'D' :1, np.nan: 0},\\\n                  'SurgicalCare' : {'E' :1, np.nan: 0},\\\n                  'Falls' : {'F' :1, np.nan: 0},\\\n                  'Drowning' : {'G' :1, np.nan: 0},\\\n                  'UseOfDefectiveProduct' : {'H' :1, np.nan: 0},\\\n                  'Fire' : {'I' :1, np.nan: 0},\\\n                  'Firearm': {'J' :1, np.nan: 0},\\\n                  'Pollution_ToxicExposure' : {'K':1, np.nan:0},\\\n                  'Explosions' : {'L':1, np.nan:0},\\\n                  'UseOfAgrlMachinery' : {'M':1, np.nan:0},\\\n                  'Oil_gasExtraction': {'N':1, np.nan:0},\\\n                  'OtherModeOfInjury' : {'O':1, np.nan:0},\\\n                  \n                  'MedicalInsurance' : {'Y':1, np.nan:0},\\\n                  'DisabilityInsurance' : {'Y':1, np.nan:0},\\\n                  'SocialSecurityBenefits' : {'Y':1, np.nan:0},\\\n                  'Medicare_Medicaid' : {'Y':1, np.nan:0},\\\n                  'OtherCollateralSources' : {'Y':1, np.nan:0}})\n                  \n                  \n                 ","99d5123e":"# extracting the target\ndf_sparse_target = df_sparse['ClaimSize']","8ea5f0e0":"# dropping it since we already extracted target and needed to send the df to dummy function\ndf_sparse.drop(['ClaimSize'], axis = 1, inplace = True)","c9567588":"# some type conversions, since get_dummies do dummies with categorical and object dtypes \ndf_sparse.loc[:, 'time_diff'] = df_sparse.loc[:, 'time_diff'].astype('int')","bc899370":"# this has NA's and I don't find a reasonable argument to impute with\ndf_sparse.drop(['Work_related_injury_status'], axis = 1, inplace = True)","5e0e845e":"# some variables to categorical and dummifying\ncat = ['PolicyType', 'PolicyForm', 'BusinessClass', 'Non_economicloss', \\\n       'Exemplarydamages', 'WhetherPrimaFacie_JointandSeveralLiability', \\\n      'WorkersCompAvailability', 'CollateralSourcesAvailability', \\\n      'Anyothercontributors', 'AnyMultipleInterestedparties',\\\n      'Employment_status']\n\ndf_sparse.loc[:, cat] = df_sparse.loc[:, cat].astype('category')\ndf_model = pd.get_dummies(df_sparse)","37e54243":"# necessary imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e80c2eb7":"# Does some one who took more time to claim, claim higher ?\nplt.figure(figsize = (15, 7))\nsns.boxplot(df_sparse_target, df_sparse.time_diff)\nplt.title(\"time and claim amount\")\nplt.show()","47878ec1":"# mode of transport\ndf_model.iloc[:, 18:22].sum().plot(kind = 'bar')\nplt.ylabel(\"count\")\nplt.show()","4534a00c":"# How the injury has occured\nplt.figure(figsize = (15, 5))\ndf_model.iloc[:, 22:33].sum().plot(kind = 'bar')\nplt.ylabel(\"count\")\nplt.title(\"Injury Occured mostly of\")\nplt.show()","d48a8362":"# type of Injury\nplt.figure(figsize = (15, 5))\ndf_model.iloc[:, :18].sum().plot(kind = 'bar')\nplt.ylabel(\"count\")\nplt.title(\"type of Injury,\")\nplt.show()","9480ef1c":"# doing the train and validation split\nfrom sklearn.model_selection import train_test_split","85b0fa7e":"X_train, x_test, Y_train, y_test = train_test_split(df_model, df_sparse_target)","b8ce9cf1":"# Scaling the remaining numerical variables, since it has some big numbers on different scales\nnum = ['PolicyLimitPerInjury', 'PrimaFacie_percentagefault_uninsured', \\\n       'PrimaFacie_percentagefault_otherinsured', 'PrimaFacie_percentagefault_insured',\\\n      'PrimaFacie_percentagefault_injured', 'CombinedSingleLimit', 'PerOccurrence_PolicyLimit', 'Perperson_Policylimit']\n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(X_train.loc[:, num])\nX_train.loc[:, num] = scale.transform(X_train.loc[:, num])\nx_test.loc[:, num] = scale.transform(x_test.loc[:, num])","c3faa416":"# Implementing logistic Regression, LogReg taking the threshold >0.5\nfrom sklearn.linear_model import LogisticRegression\nLReg = LogisticRegression()\n\n# fitting the regression\nLReg.fit(X_train, Y_train)\n\n# predicting on train and validation\npred_lreg_train = LReg.predict(X_train)\npred_lreg_test = LReg.predict(x_test)\n\n# checking the metric of interest\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy on train\", \" \", accuracy_score(Y_train, pred_lreg_train))\nprint(\"Accuracy on validation\", \" \", accuracy_score(y_test, pred_lreg_test))\n\n# I don't see much difference in accuracy So, I see there is no bias-variance problem","5ad0beb2":"# Building RandomForest \nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\n\n# fitting the train data\nclf.fit(X_train, Y_train)\n\n# predicting on train and validation\nclf_pred_train = clf.predict(X_train)\nclf_pred_test = clf.predict(x_test)\n\n# checking the metric of interest\nprint(\"Accuracy on train\", \" \", accuracy_score(Y_train, clf_pred_train))\nprint(\"Accuracy on validation\", \" \", accuracy_score(y_test, clf_pred_test))\n\n# Since there is a huge difference in trian and validation split we go with GridSearch for right hyper-parameters","535256d8":"from sklearn.model_selection import GridSearchCV\n# taking interested hyper parameters\nparam = { \"n_estimators\" : [10, 40, 70, 90], \\\n         \"max_depth\" : [7, 9, 12], \\\n         \"min_samples_leaf\" : [2, 3, 4]}\n \nrfc_cv_grid = GridSearchCV(estimator = clf, param_grid = param, cv = 10)\n# Fitting data\nrfc_cv_grid.fit(X = X_train, y = Y_train)\n\n# taking the best score parameters\nprint(rfc_cv_grid.best_score_,rfc_cv_grid.best_params_)\n\n# predicting with best parameters\npred_lreg_train = rfc_cv_grid.predict(X_train)\npred_lreg_test = rfc_cv_grid.predict(x_test)\n\n# accuracy scores\nprint(accuracy_score(Y_train, pred_lreg_train))\nprint(accuracy_score(y_test, pred_lreg_test))\n\n# With this there is no Bias-variance problem","aa678563":"# Building the model with complete train data with best hyper parameters of RandomForest extracted from GridSearchCV, \n# This is to train model on more data for better prediction on test\n\n# Scaling the train.csv\nscale = StandardScaler()\nscale.fit(df_model.loc[:, num])\ndf_model.loc[:, num] = scale.transform(df_model.loc[:, num])","1080df06":"# Fitting the model\nclf.fit(df_model, df_sparse_target)\n\n# the prediction score should be completely ignored \nclf_total = clf.predict(df_model)\nprint(accuracy_score(df_sparse_target, clf_total))\n","e95a7d39":"# Readin data\nt1 = pd.read_csv(\"..\/input\/Test_Policy_Demographics.csv\")\nt2 = pd.read_csv(\"..\/input\/Test.csv\")\nt3 = pd.read_csv(\"..\/input\/Test_ClaimDetails.csv\")\ndfs_test = [t1, t2, t3]\ndf_final_test2 = reduce(lambda left,right: pd.merge(left,right,on='ClaimID'), dfs_test)\nclaimID = df_final_test2['ClaimID']\ndf_final_test = df_final_test2.drop(['ClaimID', 'PolicyID'], axis = 1)","ea529660":"df_final_test = df_final_test.astype({'Injury_Date':'datetime64', 'Date_reported': 'datetime64'})\ntime_diff = df_final_test['Date_reported'] - df_final_test['Injury_Date']\ndf_final_test['time_diff'] = time_diff.astype('str').str.split(\" \").str[0]\ndf_final_test.drop(['Injury_Date', 'Date_reported'], axis = 1, inplace = True)\ndf_final_test.drop(['Age_Injured'], axis = 1, inplace = True)","99739c5c":"# Strategy is to fill NA's with zero \ndf_sparse_test = df_final_test.replace({'Work_related_injury_status': {'N':0, 'Y':1},\\\n                  'Amputation': {'B': 1, np.nan: 0},\\\n                  'Death' : {'A' : 1, np.nan: 0},\\\n                  'Burns_heat' : {'C' :1, np.nan: 0},\\\n                  'Burns_chemical' : {'D' :1, np.nan: 0},\\\n                  'SystemicPoisoning_toxic' : {'E' :1, np.nan: 0},\\\n                  'SystemicPoisoning_other' : {'F' :1, np.nan: 0},\\\n                  'Eye_injury_blindness' : {'G' :1, np.nan: 0},\\\n                  'RespiratoryCondition' : {'H' :1, np.nan: 0},\\\n                  'NervousCondition' : {'I' :1, np.nan: 0},\\\n                  'HearingLoss': {'J' :1, np.nan: 0},\\\n                  'CirculatoryCondition' : {'K':1, np.nan:0},\\\n                  'MultipleInjuries' : {'L':1, np.nan:0},\\\n                  'BackInjury' : {'M':1, np.nan:0},\\\n                  'SkinDisorder': {'N':1, np.nan:0},\\\n                  'BrainDamage' : {'O':1, np.nan:0},\\\n                  'Scarring' : {'P':1, np.nan:0},\\\n                  'SpinalCordInjuries' : {'Q':1, np.nan:0},\\\n                  'OtherInjuries' : {'R':1, np.nan:0},\\\n                  'OffRoadVehicle' : {'A':1, np.nan:0},\\\n                  'AirTransportation' : {'B':1, np.nan:0},\\\n                  'Railway' : {'C' :1, np.nan: 0},\\\n                  'OtherMotorVehicle' : {'D' :1, np.nan: 0},\\\n                  'SurgicalCare' : {'E' :1, np.nan: 0},\\\n                  'Falls' : {'F' :1, np.nan: 0},\\\n                  'Drowning' : {'G' :1, np.nan: 0},\\\n                  'UseOfDefectiveProduct' : {'H' :1, np.nan: 0},\\\n                  'Fire' : {'I' :1, np.nan: 0},\\\n                  'Firearm': {'J' :1, np.nan: 0},\\\n                  'Pollution_ToxicExposure' : {'K':1, np.nan:0},\\\n                  'Explosions' : {'L':1, np.nan:0},\\\n                  'UseOfAgrlMachinery' : {'M':1, np.nan:0},\\\n                  'Oil_gasExtraction': {'N':1, np.nan:0},\\\n                  'OtherModeOfInjury' : {'O':1, np.nan:0},\\\n                  \n                  'MedicalInsurance' : {'Y':1, np.nan:0},\\\n                  'DisabilityInsurance' : {'Y':1, np.nan:0},\\\n                  'SocialSecurityBenefits' : {'Y':1, np.nan:0},\\\n                  'Medicare_Medicaid' : {'Y':1, np.nan:0},\\\n                  'OtherCollateralSources' : {'Y':1, np.nan:0}})\n                  ","18eb80c8":"cat = ['PolicyType', 'PolicyForm', 'BusinessClass', 'Non_economicloss', \\\n       'Exemplarydamages', 'WhetherPrimaFacie_JointandSeveralLiability', \\\n      'WorkersCompAvailability', 'CollateralSourcesAvailability', \\\n      'Anyothercontributors', 'AnyMultipleInterestedparties',\\\n      'Employment_status']\n\ndf_sparse_test.loc[:, 'time_diff'] = df_sparse_test.loc[:, 'time_diff'].astype('int')\ndf_sparse_test.drop(['Work_related_injury_status'], axis = 1, inplace = True)\ndf_sparse_test.loc[:, cat] = df_sparse.loc[:, cat].astype('category')\ndf_model_test = pd.get_dummies(df_sparse_test)\ndf_model_test.shape","6c35f3fe":"num = ['PolicyLimitPerInjury', 'PrimaFacie_percentagefault_uninsured', \\\n       'PrimaFacie_percentagefault_otherinsured', 'PrimaFacie_percentagefault_insured',\\\n      'PrimaFacie_percentagefault_injured', 'CombinedSingleLimit', 'PerOccurrence_PolicyLimit', 'Perperson_Policylimit']\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ndf_model_test.loc[:, num] = scale.fit_transform(df_model_test.loc[:, num])","5ad1cdb3":"# Doing necessary predictions\npred = clf.predict(df_model_test)\nsub = pd.DataFrame({'ClaimSize':pred}, index = claimID)","94493d99":"import base64\nimport html\nfrom IPython.display import HTML","ebc6a644":"# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"prediction.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(sub)","063b2eb6":"From above the insurance company can reduce the insurance coverage on things where the count is higher in their new draft","45da4b9e":"# Model Building","8879a68d":"# Some Exploratory Plotting","69535aa2":"#Going with the Random Forest because of higher accuracy than Logistic Regr.\n# Getting Prediction on Test data","ad9cb03a":"# Pre-Processing"}}