{"cell_type":{"6424ccfd":"code","9cd694de":"code","29909f23":"code","53b5af4c":"code","1b58a0cc":"code","aff49505":"markdown","d7f75c8b":"markdown","afacc4f7":"markdown","ed0a312a":"markdown"},"source":{"6424ccfd":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\ntrain_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_path = '..\/input\/home-data-for-ml-course\/test.csv'\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\n\n\ndef transform(x, onehoter=None):\n    columns = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'TotalBsmtSF', 'GrLivArea', 'TotRmsAbvGrd', 'BedroomAbvGr', 'FullBath'\n           , 'BsmtFinSF1', 'Fireplaces', 'GarageArea', 'GarageCars', 'PoolArea', 'MiscVal', 'YrSold'\n           , 'Neighborhood', 'SaleType', 'SaleCondition'\n           , 'CentralAir', 'YearsRemodBuilt'\n          ]\n    \n    x['CentralAir'] = x['CentralAir'].map({'Y':1, 'N':0})\n    x['YearsRemodBuilt'] = x['YearRemodAdd'] - x['YearBuilt']\n    \n    x = x.fillna(0)\n    x = x[columns]\n    cat_col = (x.dtypes == 'object')\n    columns_categorical = list(cat_col[cat_col].index)\n    \n    if onehoter==None:\n        onehoter = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        cat_x = pd.DataFrame(onehoter.fit_transform(x[columns_categorical]), index=x.index) # One-hot encoding removed index; put it back\n    else:\n        cat_x = pd.DataFrame(onehoter.transform(x[columns_categorical]), index=x.index)\n    num_x = x.drop(columns_categorical, axis=1)\n    x = pd.concat([num_x, cat_x], axis=1)\n        \n    return x, onehoter\n\ny = train.SalePrice\nX, onehoter = transform(train)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state = 0)\n    \nmodel = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmodel.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_valid, y_valid)], verbose=False)\n\npredict = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, predict)\nprint(mae)\n\ntest_ids = test['Id']\ntest, _ = transform(test, onehoter)\n\npredict_test = model.predict(test)\nprint(predict_test)\nsubmission = pd.DataFrame({'Id': test_ids, 'SalePrice': predict_test})\nsubmission.to_csv('submission.csv', index = False)","9cd694de":"### Main statistic\n#train.head()\n#train.describe()\n#train.columns\n#print(X_train.shape)\n\n###filled_X['Alley'].fillna(value='NA', inplace=True)\n\n### N\/A. Number of missing values in each column of training data\n#missing_val_count_by_column = (train.isnull().sum())\n#print(missing_val_count_by_column[missing_val_count_by_column > 0])\n#train[columns].dropna(axis=0)\n\n###Correlation\n#corr_mat = train.corr()\n#corr_mat['SalePrice'].sort_values(ascending=False)\n#plt.figure(figsize=(15,15))\n#corr_cols = corr_mat.nlargest(15, 'SalePrice')['SalePrice']\n#sns.heatmap(train[corr_cols.index].corr(), annot=True, square=True)\n\n###Change values by dictionary\n#BsmtQual = {'Ex': 110, 'Gd': 90, 'TA': 80, 'Fa': 70, 'Po': 35, 'NA': 0}\n#train = train.replace({'BsmtQual': BsmtQual})\n\n#add_columns = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n#    add_columns_dict = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0}\n#    for col in add_columns:\n#        x[col] = x[col].map(add_columns_dict)   \n#    for col in add_columns:\n#        columns.append(col)\n\n### Select categorical columns with relatively low cardinality and numerical columns\n#low_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and X_train[cname].dtype == \"object\"]\n#numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\n### Keep selected columns only\n#my_cols = low_cardinality_cols + numerical_cols\n#X_train_low = X_train[my_cols].copy()\n#X_valid_low = X_valid[my_cols].copy()\n\n###Outliers\n#outliers = train.loc[(train.SalePrice < 200000) & (train.GrLivArea > 4000)]\n#train.drop(outliers.index, inplace=True)","29909f23":"### 1) Drop columns with Missing Values\n#cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n#reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n#reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\n### 2)  Imputation\n### Use SimpleImputer to replace missing values with the mean value along each column.\n#from sklearn.impute import SimpleImputer\n#my_imputer = SimpleImputer()\n#imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n#imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n### Imputation removed column names; put them back\n#imputed_X_train.columns = X_train.columns\n#imputed_X_valid.columns = X_valid.columns\n\n### 3) An Extension to Imputation\n### Make copy to avoid changing original data (when imputing)\n#X_train_plus = X_train.copy()\n#X_valid_plus = X_valid.copy()\n### Make new columns indicating what will be imputed\n#for col in cols_with_missing:\n#    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n#    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n### Imputation\n#my_imputer = SimpleImputer()\n#imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n#imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n### Imputation removed column names; put them back\n#imputed_X_train_plus.columns = X_train_plus.columns\n#imputed_X_valid_plus.columns = X_valid_plus.columns","53b5af4c":"### 1) Drop Catehorical Variables\n#drop_X_train = X_train.select_dtypes(exclude=['object'])\n#drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\n### 2) Label Encoding\n#from sklearn.preprocessing import LabelEncoder\n### Make copy to avoid changing original data \n#label_X_train = X_train.copy()\n#label_X_valid = X_valid.copy()\n### Get list of categorical variables\n#s = (X_train.dtypes == 'object')\n#object_cols = list(s[s].index)\n### Apply label encoder to each column with categorical data\n#label_encoder = LabelEncoder()\n#for col in object_cols:\n#    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n#    label_X_valid[col] = label_encoder.transform(X_valid[col])\n\n### 3) One-Hot Encoding\n### Use the OneHotEncoder class from scikit-learn to get one-hot encodings. \n### Parameters that can be used to customize its behavior:\n### - handle_unknown='ignore' to avoid errors when the validation data contains classes that aren't represented in the training data\n### - Sparse=False ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n#from sklearn.preprocessing import OneHotEncoder\n### Apply one-hot encoder to each column with categorical data\n#OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n#OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n#OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n### One-hot encoding removed index; put it back\n#OH_cols_train.index = X_train.index\n#OH_cols_valid.index = X_valid.index\n### Remove categorical columns (will replace with one-hot encoding)\n#num_X_train = X_train.drop(object_cols, axis=1)\n#num_X_valid = X_valid.drop(object_cols, axis=1)\n### Add one-hot encoded columns to numerical features\n#OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n#OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)","1b58a0cc":"#from sklearn.tree import DecisionTreeRegressor\n#from sklearn.ensemble import RandomForestRegressor\n###DecisionTree\n#def decision_tree(max_leaf_nodes, train_X, val_X, train_y, val_y):\n#    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n#    model.fit(train_X, train_y)\n#    predict_val = model.predict(val_X)\n#    mae = mean_absolute_error(val_y, predict_val)\n#    return model, mae\n\n#model, mae = decision_tree(5, train_X, val_X, train_y, val_y)\n#for max_leaf_nodes in [130, 140, 150, 160]:\n#    decision_model, decision_mae = decision_tree(max_leaf_nodes, train_X, val_X, train_y, val_y)\n#    if decision_mae < mae:\n#        mae = decision_mae\n#        model = decision_model\n#        print(\"Decision Tree \\t Max leaf nodes: %d  \\t Mean Absolute Error:  %d\" %(max_leaf_nodes, decision_mae))\n \n###RandomForest\n#forest_model = RandomForestRegressor(random_state=1)\n#forest_model.fit(train_X, train_y)\n#forest_predict = forest_model.predict(val_X)\n#forest_mae = mean_absolute_error(val_y, forest_predict)\n#if forest_mae < decision_mae:\n#    model = forest_model\n#    print(\"Random Forest \\t Mean Absolute Error:  %d\" %(forest_mae))\n\n### XGB\n#model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n#model.fit(OH_X_train, y_train, \n#             early_stopping_rounds=5, \n#             eval_set=[(OH_X_valid, y_valid)], \n#             verbose=False)\n#predict = model.predict(OH_X_valid)\n#mae = mean_absolute_error(y_valid, predict)","aff49505":"# 3. Categorical Variables\n* 1) Drop Categorical Variables\n* 2) Label Encoding. Assigns each unique value to a different integer\n* 3) One-Hot Encoding","d7f75c8b":"# 2. Missing Values\n\n* 1) A Simple Option: Drop Columns with Missing Values\n* 2) A Better Option: Imputation. Imputation fills in the missing values with some number. \n* 3) An Extension To Imputation. Impute the missing values and, additionally, for each column with missing entries in the original dataset, we add a new column that shows the location of the imputed entries.","afacc4f7":"# 1. Data Exploration","ed0a312a":"# 4. Models choose"}}