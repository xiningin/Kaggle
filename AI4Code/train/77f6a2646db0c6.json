{"cell_type":{"94a880f1":"code","a71885b4":"code","d22d432f":"code","c06001ff":"code","c62182d6":"code","7c625201":"code","b202725b":"code","0a0353cc":"code","ecd0dce0":"code","28f2ea79":"code","f36fef93":"code","0a333fa8":"code","d7e60de9":"code","0056ad56":"code","c60b807e":"code","1411fa45":"code","4e972188":"code","8b5eb9d3":"code","fa6edae0":"code","734d6be4":"code","7ff019a5":"code","a3bc7f74":"code","d42465fa":"code","13ae28aa":"code","fff5af81":"code","6289e8ae":"code","49f324bc":"code","342fb1a4":"code","5c0041b7":"code","e9d23b79":"code","cc7a17dd":"code","0a8e1d25":"code","b4f0f54d":"code","85344041":"code","e0d3e3b6":"code","6a3667b2":"code","beda3679":"code","8510154a":"code","0efcbd97":"code","decd890d":"code","391b22b3":"code","b9ef60d2":"code","051c1394":"code","f86e0f81":"code","f337e075":"code","06002afd":"code","fc54e9a0":"code","6563ddd6":"code","9ab297e2":"code","412537db":"code","917e25fd":"code","956d742d":"code","4b362c2c":"code","a83ccbcf":"code","13c9be56":"code","18a14d07":"code","125831d6":"code","5115498e":"code","9f68ff86":"code","e947560e":"code","068e114d":"code","da930c3b":"code","bd6a9412":"code","bb255be8":"code","043f38ff":"code","2032818c":"code","b5691270":"code","2734d3ae":"code","f4523d1f":"code","e2710c20":"code","6e0d388d":"code","c27c1506":"markdown","607cd959":"markdown","0a126bff":"markdown","e90030e8":"markdown","7b407b3d":"markdown","39328b5d":"markdown","19105257":"markdown","9c69ce31":"markdown","c80d829f":"markdown","03881c13":"markdown","50c1ae3c":"markdown","2ebda26e":"markdown","58c4f2a9":"markdown","c6ec2a59":"markdown","8cb1c1b6":"markdown","c92f3059":"markdown","b17bcd95":"markdown","9038444e":"markdown","ed28dcbe":"markdown","0504cfde":"markdown","ce5af395":"markdown","64fc25f7":"markdown","a1fa13f3":"markdown","ba8c4445":"markdown","1965a717":"markdown","d32a2136":"markdown","e725a8fe":"markdown","bb14a704":"markdown","bca3c71c":"markdown","779da4ff":"markdown","6e771554":"markdown","ddd11d34":"markdown","2b6d3ea2":"markdown","6c714a0b":"markdown","37eb81cf":"markdown","0ec6ddae":"markdown","76b7a513":"markdown","b6be75e0":"markdown"},"source":{"94a880f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport matplotlib.dates as mdates\nimport seaborn as sns\nfrom sklearn.model_selection import GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom matplotlib.pyplot import figure\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import linear_model\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.feature_selection import SelectFromModel","a71885b4":"gold_df = pd.read_csv('\/kaggle\/input\/gold-price-prediction-dataset\/FINAL_USO.csv', na_values = ['N\/A'], index_col = 'Date', parse_dates = True, infer_datetime_format = True)\ngold_df.head()","d22d432f":"gold_df.shape","c06001ff":"gold_df.describe()","c62182d6":"gold_df.isnull().sum()","7c625201":"gld_adj_close = gold_df['Adj Close']\nspy_adj_close = gold_df['SP_Ajclose']\ndj_adj_close = gold_df['DJ_Ajclose']\n\ngold_df_p = pd.DataFrame({'GLD': gld_adj_close, 'SPY': spy_adj_close, 'DJ': dj_adj_close})\ngold_df_ax = gold_df_p.plot(title = 'Effect of Index Prices on Gold Rates', figsize = (15,10))\ngold_df_ax.set_ylabel('Price')\ngold_df_ax.legend(loc = 'upper left')\nplt.show()","b202725b":"def calc_daily_returns(df): # calculate and return daily returns values\n    daily_returns = (df \/ df.shift(1)) - 1\n    daily_returns[0] = 0\n    return daily_returns\n","0a0353cc":"gld_adj_close = gold_df['Adj Close']\nspy_adj_close = gold_df['SP_Ajclose']\ndj_adj_close = gold_df['DJ_Ajclose']\neg_adj_close = gold_df['EG_Ajclose']\nuso_adj_close = gold_df['USO_Adj Close']\ngdx_adj_close = gold_df['GDX_Adj Close']\neu_price = gold_df['EU_Price']\nof_price = gold_df['OF_Price']\nos_price = gold_df['OS_Price']\nsf_price = gold_df['SF_Price']\nusb_price = gold_df['USB_Price']\nplt_price = gold_df['PLT_Price']\npld_price = gold_df['PLD_Price']\nrho_price = gold_df['RHO_PRICE']\nusdi_price = gold_df['USDI_Price']\n\ngld_daily_returns = calc_daily_returns(gld_adj_close)\nspy_daily_returns = calc_daily_returns(spy_adj_close)\ndj_adj_returns = calc_daily_returns(dj_adj_close)\neg_adj_returns = calc_daily_returns(eg_adj_close)\nuso_adj_returns = calc_daily_returns(uso_adj_close)\ngdx_adj_returns = calc_daily_returns(gdx_adj_close)\neu_returns = calc_daily_returns(eu_price)\nof_returns = calc_daily_returns(of_price)\nos_returns = calc_daily_returns(os_price)\nsf_returns = calc_daily_returns(sf_price)\nusb_returns = calc_daily_returns(usb_price)\nplt_returns = calc_daily_returns(plt_price)\npld_returns = calc_daily_returns(pld_price)\nrho_returns = calc_daily_returns(rho_price)\nusdi_returns = calc_daily_returns(usdi_price)\n\ngold_df_d = pd.DataFrame({'GLD': gld_daily_returns, 'SPY': spy_daily_returns, 'DJ': dj_adj_returns, 'EG': eg_adj_returns, 'USO': uso_adj_returns, 'GDX': gdx_adj_returns, 'EU': eu_returns, 'OF': of_returns, 'OS': os_returns, 'SF': sf_returns, 'USB': usb_returns, 'PLT': plt_returns, 'PLD': pld_returns, 'RHO': rho_returns, 'USDI': usdi_returns})\ngold_daily_ax = gold_df_d[-100:].plot(title = 'Last 100 records of daily return of all the features:', figsize = (15,10))\ngold_daily_ax.set_ylabel('Daily Return')\ngold_daily_ax.legend(loc = 'lower left')\n\nplt.show()","ecd0dce0":"gold_df_s = pd.DataFrame({'GLD': gld_daily_returns, 'SPY': spy_daily_returns, 'DJ': dj_adj_returns})\ngold_daily_ax = gold_df_s[-100:].plot(title = 'Last 100 records of daily return of Stock Indices:', figsize = (15,10))\ngold_daily_ax.set_ylabel('Daily Return')\ngold_daily_ax.legend(loc = 'lower left')\n\nplt.show()","28f2ea79":"gold_df_d.plot(kind = 'scatter', x = 'SPY', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'DJ', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'EG', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'USO', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'GDX', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'EU', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'OF', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'OS', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'SF', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'USB', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'PLT', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'PLD', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'RHO', y = 'GLD')\ngold_df_d.plot(kind = 'scatter', x = 'USDI', y = 'GLD')","f36fef93":"gold_mean = gold_df_d['GLD'].mean() #calculating mean gold price\ngold_std = gold_df_d['GLD'].std() # calculating standard deviation\ngold_kurt = gold_df_d['GLD'].kurtosis() # calculating kurtosis value\n\nprint('Mean = ', gold_mean)\nprint('Standard deviation = ', gold_std)\nprint('Kurtosis = ', gold_kurt)\n\ngold_df_d['GLD'].hist(bins = 20) # plotting histogram\n\nplt.axvline(gold_mean, color = 'yellow', linestyle = '-.', linewidth = 2)\nplt.axvline(gold_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.axvline(-gold_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.title('Mean, Standard Deviation and Kurtosis of Gold Prices')\nplt.figure(figsize = (15,8))\n\nplt.show()","0a333fa8":"gold_sp_mean = gold_df_d['SPY'].mean() # computing mean of gold stock\ngold_sp_std = gold_df_d['SPY'].std() # computing standard deviation of gold stock\ngold_sp_kurt = gold_df_d['SPY'].kurtosis() # computing kurtosis of gold stock\n\nprint('Mean = ', gold_sp_mean)\nprint('Standard deviation = ', gold_sp_std)\nprint('Kurtosis = ', gold_sp_kurt)\n\ngold_df_d['SPY'].hist(bins = 20) # plotting histogram\n\nplt.axvline(gold_sp_mean, color = 'yellow', linestyle = '-.', linewidth = 2)\nplt.axvline(gold_sp_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.axvline(-gold_sp_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.title('Mean, Standard Deviation and Kurtosis of SPY Prices')\nplt.figure(figsize = (15,8))\n\nplt.show()","d7e60de9":"gold_dj_mean = gold_df_d['DJ'].mean() # computing mean of gold stock\ngold_dj_std = gold_df_d['DJ'].std() # computing standard deviation of gold stock\ngold_dj_kurt = gold_df_d['DJ'].kurtosis() # computing kurtosis of gold stock\n\nprint('Mean = ', gold_dj_mean)\nprint('Standard deviation = ', gold_dj_std)\nprint('Kurtosis = ', gold_dj_kurt)\n\ngold_df_d['DJ'].hist(bins = 20) # plotting histogram\n\nplt.axvline(gold_dj_mean, color = 'yellow', linestyle = '-.', linewidth = 2)\nplt.axvline(gold_dj_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.axvline(-gold_dj_std, color = 'red', linestyle = '--', linewidth = 2)\nplt.title('Mean, Standard Deviation and Kurtosis of Dow Jones Prices')\nplt.figure(figsize = (15,8))\n\nplt.show()","0056ad56":"plt.figure(figsize = (20,15))\nsns.heatmap(gold_df.corr(), annot = True)","c60b807e":"X = gold_df.drop(['Adj Close'], axis = 1)\nX = X.drop(['Close'], axis = 1)","1411fa45":"X.corrwith(gold_df['Adj Close']).plot.bar(title = 'Correlation with Adj Close', rot = 90, grid = True, figsize = (20,15), fontsize = 15)\n","4e972188":"correlation_matrix = gold_df.corr()\ncoeff = correlation_matrix['Adj Close'].sort_values(ascending = False)","8b5eb9d3":"posi_corr = coeff[coeff > 0]\nposi_corr","fa6edae0":"nega_corr = coeff[coeff < 0]\nnega_corr","734d6be4":"def compute_macd(df, nslow = 26, nfast = 12):\n    emaslow = df.ewm(span = nslow, min_periods = nslow, adjust = True, ignore_na = False).mean()\n    emafast = df.ewm(span = nfast, min_periods = nfast, adjust = True, ignore_na = False).mean()\n    diff = emafast - emaslow\n    macd = diff.ewm(span = 9, min_periods = 9, adjust = True, ignore_na = False).mean()\n    return diff, macd\n\ndef compute_rsi(df, periods = 14):\n    dif = df.diff()\n    up, down = dif.copy(), dif.copy()\n    up[up < 0] = 0\n    down[down > 0] = 0\n    rsi_up = up.ewm(com = periods, adjust = False).mean()\n    rsi_down = down.ewm(com = periods, adjust = False).mean().abs()\n    rsi = 100 - 100 \/ (1 + rsi_up \/ rsi_down)\n    return rsi\n\ndef compute_sma(df, periods = 15):\n    sma = df.rolling(window = periods, min_periods = periods, center = False).mean()\n    return sma\n\ndef compute_bands(df, periods = 15):\n    std = df.rolling(window = periods, min_periods = periods, center = False).std()\n    sma = compute_sma(df)\n    upper_band = sma + (2 * std)\n    lower_band = sma - (2 * std)\n    return upper_band, lower_band\n\ndef compute_std_dev(df, periods = 5):\n    std_dev = df.rolling(periods).std()\n    return std_dev\n\n","7ff019a5":"figure, axes = plt.subplots(nrows = 1, ncols = 4, figsize = (15,5))\n\nsma_gld = compute_sma(gld_adj_close) # computing the simple moving average for GLD\n\ngld_adj_close[:365].plot(title = \"Simple Moving Average for GLD\", label = 'GLD', ax = axes[0])\nsma_gld[:365].plot(label = 'SMA', ax = axes[0])\n\nupper_band, lower_band = compute_bands(gld_adj_close) # calculating Bollinger Bands for GLD\n\nupper_band[:365].plot(label = 'Upper Band', ax = axes[0])\nlower_band[:365].plot(label = 'Lower Band', ax = axes[0])\n\ndif, macd = compute_macd(gld_adj_close) # Calculating MACD for GLD\n\ndif[:365].plot(title = 'DIF and MACD', label = 'DIF', ax = axes[1])\nmacd[:365].plot(label = 'MACD', ax = axes[1])\n\nrsi = compute_rsi(gld_adj_close) # Calculating RSI for GLD\nrsi[:365].plot(title = 'RSI', label = 'RSI', ax = axes[2])\n\nstd_dev = compute_std_dev(gld_adj_close) # Calculating Standard Deviation for GLD\nstd_dev[:365].plot(title = 'Standard Deviation', label = 'Std Dev', ax = axes[3])\n\nopen_close = gold_df.Open - gold_df.Close\nhigh_low = gold_df.High - gold_df.Low\n\naxes[0].set_ylabel('Price')\naxes[1].set_ylabel('Price')\naxes[2].set_ylabel('Price')\naxes[3].set_ylabel('Price')\n\naxes[0].legend(loc = 'lower left')\naxes[1].legend(loc = 'lower left')\naxes[2].legend(loc = 'lower left')\naxes[3].legend(loc = 'lower left')\n","a3bc7f74":"figure, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (15,5))\n\nopen_close = gold_df.Open - gold_df.Close\nopen_close[:365].plot(title = 'Open - Close', label = 'Open_Close', ax = axes[0])\n\nhigh_low = gold_df.High - gold_df.Low\nhigh_low[:365].plot(title = 'High - Low', label = \"High_Low\", ax = axes[1])\n\naxes[0].set_ylabel('Price')\naxes[1].set_ylabel('Price')\n\naxes[0].legend(loc = 'lower left')\naxes[1].legend(loc = 'lower left')","d42465fa":"test_df = gold_df\ntest_df['SMA'] = sma_gld\ntest_df['Upper Band'] = upper_band\ntest_df['Lower Band'] = lower_band\ntest_df['DIF'] = dif\ntest_df['MACD'] = macd\ntest_df['RSI'] = rsi\ntest_df['Std Dev'] = std_dev\ntest_df['Open_Close'] = open_close\ntest_df['High_Low'] = high_low\n\ntest_df = test_df[33:] # Dropping first 33 rows as it has null values because technical indicators are introduced\n\ntarget_adj_close = pd.DataFrame(test_df['Adj Close']) # Target Column\n\ndisplay(test_df.head())\n","13ae28aa":"test_df.columns","fff5af81":"feature_cols = ['Open', 'High', 'Low', 'Volume', 'SP_open',\n       'SP_high', 'SP_low', 'SP_Ajclose', 'SP_volume', 'DJ_open',\n       'DJ_high', 'DJ_low', 'DJ_Ajclose', 'DJ_volume', 'EG_open',\n       'EG_high', 'EG_low', 'EG_Ajclose', 'EG_volume', 'EU_Price',\n       'EU_open', 'EU_high', 'EU_low', 'EU_Trend', 'OF_Price', 'OF_Open',\n       'OF_High', 'OF_Low', 'OF_Volume', 'OF_Trend', 'OS_Price', 'OS_Open',\n       'OS_High', 'OS_Low', 'OS_Trend', 'SF_Price', 'SF_Open', 'SF_High',\n       'SF_Low', 'SF_Volume', 'SF_Trend', 'USB_Price', 'USB_Open', 'USB_High',\n       'USB_Low', 'USB_Trend', 'PLT_Price', 'PLT_Open', 'PLT_High', 'PLT_Low',\n       'PLT_Trend', 'PLD_Price', 'PLD_Open', 'PLD_High', 'PLD_Low',\n       'PLD_Trend', 'RHO_PRICE', 'USDI_Price', 'USDI_Open', 'USDI_High',\n       'USDI_Low', 'USDI_Volume', 'USDI_Trend', 'GDX_Open', 'GDX_High',\n       'GDX_Low', 'GDX_Close', 'GDX_Adj Close', 'GDX_Volume', 'USO_Open',\n       'USO_High', 'USO_Low', 'USO_Close', 'USO_Adj Close', 'USO_Volume',\n       'SMA', 'Upper Band', 'Lower Band', 'DIF', 'MACD', 'RSI', 'Std Dev',\n       'Open_Close', 'High_Low']","6289e8ae":"feat_minmax_trans_data = MinMaxScaler().fit_transform(test_df[feature_cols])\nfeat_minmax_transform = pd.DataFrame(columns = feature_cols, data = feat_minmax_trans_data, index = test_df.index)\nfeat_minmax_transform.head()","49f324bc":"display(feat_minmax_transform.head())\nprint('Shape of features: ', feat_minmax_transform.shape)\nprint('Shape of target: ', target_adj_close.shape)\n\ntarget_adj_close = target_adj_close.shift(-1) # to predict the target on the (n+1)th day\nvalidation_yvalue = target_adj_close[-85:-1]\ntarget_adj_close = target_adj_close[:-85]\n\nvalidation_xvalue = feat_minmax_transform[-85:-1] # taking last 85 rows to be as validation set\nfeat_minmax_transform = feat_minmax_transform[:-85]\ndisplay(validation_xvalue.tail())\ndisplay(validation_yvalue.tail())\n\nprint('\\n AFTER PROCESS \\n')\nprint('Shape of features: ', feat_minmax_transform.shape)\nprint('Shape of target: ', target_adj_close.shape)\ndisplay(target_adj_close.tail())","342fb1a4":"for train_index, test_index in TimeSeriesSplit(n_splits = 10).split(feat_minmax_transform):\n    x_train, x_test = feat_minmax_transform[:len(train_index)], feat_minmax_transform[len(train_index):(len(train_index) + len(test_index))]\n    y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index):(len(train_index) + len(test_index))].values.ravel()","5c0041b7":"x_train.shape","e9d23b79":"x_test.shape","cc7a17dd":"y_train.shape","0a8e1d25":"y_test.shape","b4f0f54d":"def result_validation(model, model_name):\n    predicted = model.predict(validation_xvalue)\n    rmse_score = np.sqrt(mean_squared_error(validation_yvalue, predicted))\n    print('Root Mean Squared Error: ', rmse_score)\n    \n    R2_score = r2_score(validation_yvalue, predicted)\n    print('R2 Score: ', R2_score)\n    \n    plt.plot(validation_yvalue.index, predicted, color = 'purple', label = 'Predicted')\n    plt.plot(validation_yvalue.index, validation_yvalue, color = 'green', label = 'Actual')\n    plt.ylabel('Price')\n    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%d-%m-%Y'))\n    plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n    plt.title(model_name + 'Predicted V\/s Actual')\n    plt.legend(loc = 'upper right')\n    plt.show()\n    ","85344041":"benchmark_dt = DecisionTreeRegressor(random_state = 0).fit(x_train, y_train)\nresult_validation(benchmark_dt, 'Decision Tree Regression ')","e0d3e3b6":"soln_models = {}\nsvr_linear_kern = SVR(kernel = 'linear') # Support Vector Regressor with linear kernel\nlinear_svr_clf_feature = svr_linear_kern.fit(x_train, y_train)\nresult_validation(linear_svr_clf_feature, 'Linear SVR All Features ')","6a3667b2":"linear_svr_param = {'C': [0.5, 1.0, 10, 50], 'epsilon': [0, 0.1, 0.5, 0.7, 0.9], } # Hyperparameter Tuning\nlin_svr_grid_search_feat = GridSearchCV(estimator = linear_svr_clf_feature, param_grid = linear_svr_param, cv = TimeSeriesSplit(n_splits = 10))\nlin_svr_grid_search_feat = lin_svr_grid_search_feat.fit(x_train, y_train)\nresult_validation(lin_svr_grid_search_feat, 'Linear SVR GS All Features ')","beda3679":"soln_models['Linear SVR All Features '] = lin_svr_grid_search_feat","8510154a":"rand_for_clf_feat = RandomForestRegressor(n_estimators = 50, random_state = 0).fit(x_train, y_train) # Random Forest as solution model\nresult_validation(rand_for_clf_feat, 'Random Forest with All Features ')","0efcbd97":"rand_for_param = {'n_estimators': [10, 15, 20, 50, 100], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [2, 3, 5, 7, 10],}\nrand_for_grid_search_feat = GridSearchCV(estimator = rand_for_clf_feat, param_grid = rand_for_param, cv = TimeSeriesSplit(n_splits = 10))\nrand_for_grid_search_feat.fit(x_train, y_train)","decd890d":"print(rand_for_grid_search_feat.best_params_)\nresult_validation(rand_for_grid_search_feat, \"Random Forest GridSearch \")","391b22b3":"soln_models['Random Forest with All Features '] = rand_for_clf_feat","b9ef60d2":"lasso_clf = LassoCV(n_alphas = 1000, max_iter = 4000, random_state = 0) # using Lasso Regression and Ridge Regression\nridge_clf = RidgeCV(gcv_mode = 'auto')\n\nlasso_clf_feat = lasso_clf.fit(x_train, y_train)\nresult_validation(lasso_clf_feat, 'Lasso CV ')\nsoln_models['Lasso CV with All Features'] = lasso_clf_feat\n\nridge_clf_feat = ridge_clf.fit(x_train, y_train)\nresult_validation(ridge_clf_feat, 'Ridge CV ')\nsoln_models['Ridge CV with All Features '] = ridge_clf_feat","051c1394":"bayesian_feat = linear_model.BayesianRidge().fit(x_train, y_train) # using Bayesian Ridge\nresult_validation(bayesian_feat, 'Bayesian Ridge ')\nsoln_models['Bayesian Ridge with all Features'] = bayesian_feat","f86e0f81":"# Gradient Boosting Regressor\n\ngrad_boost_feat = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1, max_depth = 3, random_state = 0, loss = 'ls').fit(x_train, y_train)\nresult_validation(grad_boost_feat, 'Gradient Boosting Regressor ')\nsoln_models['Gradient Boosting with all Features'] = grad_boost_feat","f337e075":"# Stochastic Gradient Descent\n\nstoc_grad_desc_feat = SGDRegressor(max_iter = 1000, tol = 1e-3, loss = 'squared_epsilon_insensitive', penalty = 'l1', alpha = 0.1).fit(x_train, y_train)\nresult_validation(stoc_grad_desc_feat, 'Stochastic Gradient Descent ')\nsoln_models['SGD with all Features'] = stoc_grad_desc_feat","06002afd":"RMSEs = {}\ndef evaluate_model(models):\n    fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (15,15))\n    \n    # Plotting the benchmark model\n    bench_pred = benchmark_dt.predict(validation_xvalue)\n    bench_rmse = np.sqrt(mean_squared_error(validation_yvalue, bench_pred))\n    RMSEs['Benchmark score'] = bench_rmse\n    \n    axes[0,0].plot(validation_yvalue.index, bench_pred, color = 'purple', label = 'Predicted')\n    axes[0,0].plot(validation_yvalue.index, validation_yvalue, color = 'orange', label = 'Actual')\n    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%m - %Y'))\n    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n    axes[0,0].set_ylabel('Price')\n    axes[0,0].set_title('Benchmark Predicted Models RMSE Score- '+ '{0:.2f}'.format(bench_rmse))\n    axes[0,0].legend(loc = 'upper right')\n    \n    #Plot Block\n    ax_x = 0\n    ax_y = 1\n    \n    # Plotting the Solution Model\n    for name, model in models.items():\n        predicted = model.predict(validation_xvalue)\n        soln_rmse = np.sqrt(mean_squared_error(validation_yvalue, predicted))\n        \n        axes[ax_x, ax_y].plot(validation_yvalue.index, predicted, color = 'purple', label = 'Predicted')\n        axes[ax_x, ax_y].plot(validation_yvalue.index, validation_yvalue, color = 'orange', label = 'Actual')\n        axes[ax_x, ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%m - %Y'))\n        axes[ax_x, ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n        axes[ax_x, ax_y].set_ylabel('Price')\n        axes[ax_x, ax_y].set_title(name + ' RMSE Score - '+ '{0:.2f}'.format(soln_rmse))\n        axes[ax_x, ax_y].legend(loc = 'upper right')\n        RMSEs[name] = soln_rmse\n        \n        if ax_x <= 2:\n            if ax_y < 2:\n                ax_y = ax_y + 1\n            else:\n                ax_x = ax_x + 1\n                ax_y = 0\n    \n    plt.show()\n\nevaluate_model(soln_models)","fc54e9a0":"model_names = []\nmodel_values = []\n\nfor model_name, model_value in RMSEs.items():\n    model_names.append(model_name)\n    model_values.append(model_value)\n\nmodel_values = np.array(model_values)\nmodel_names = np.array(model_names)\n\nindices = np.argsort(model_values)\ncolumns = model_names[indices[:8]]\nvalues = model_values[indices[:8]]\n\nfig = plt.figure(figsize = (16,9))\nplt.bar(np.arange(8), values, width = 0.5, color = '#470670')\nplt.xticks(np.arange(8), columns, rotation = 90)\nplt.xlabel('Model Name')\nplt.ylabel('RMSE')\nplt.title('Comparing RMSE Scores for Different Models')\nplt.show()","6563ddd6":"# Using Lasso CV Regressor as it has lowest RMSE Score\n\nsfm = SelectFromModel(lasso_clf_feat).fit(feat_minmax_transform, target_adj_close.values.ravel())\ndisplay(feat_minmax_transform.head())\nsupport = sfm.get_support()\nZip = zip(feat_minmax_transform, support)\nprint(*Zip)","9ab297e2":"# Selecting the Features for Model Building\nfeat_selected = feat_minmax_transform[['Open', 'High', 'Low', 'OF_Trend', 'OS_Trend', 'SF_Volume', 'SF_Trend', 'USB_High', 'USB_Trend', 'PLT_Trend', 'PLD_Trend', 'USDI_Open', 'USDI_Trend', 'GDX_Close', 'SMA', 'Upper Band', 'RSI', 'Open_Close']]\nfeat_selected_validation_xvalue = validation_xvalue[['Open', 'High', 'Low', 'OF_Trend', 'OS_Trend', 'SF_Volume', 'SF_Trend', 'USB_High', 'USB_Trend', 'PLT_Trend', 'PLD_Trend', 'USDI_Open', 'USDI_Trend', 'GDX_Close', 'SMA', 'Upper Band', 'RSI', 'Open_Close']]\ndisplay(feat_selected.head())\ndisplay(feat_selected_validation_xvalue.head())","412537db":"for train_index, test_index in TimeSeriesSplit(n_splits = 10).split(feat_selected):\n    x_train, x_test = feat_selected[:len(train_index)], feat_selected[len(train_index):(len(train_index) + len(test_index))]\n    y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index):(len(train_index) + len(test_index))].values.ravel()","917e25fd":"def feat_selected_validate_result(model, model_name):\n    predicted = model.predict(feat_selected_validation_xvalue)\n    \n    RMSE = np.sqrt(mean_squared_error(validation_yvalue, predicted))\n    R2_score = r2_score(validation_yvalue, predicted)\n    \n    print(model_name + '\\n')\n    print('RMSE: ', RMSE)\n    print('R2 Score: ', R2_score)\n    print('________________')\n\nprint('BENCHMARK:')\nbenchmark_dt_feat_sel = DecisionTreeRegressor(random_state = 0).fit(x_train, y_train)\nfeat_selected_validate_result(benchmark_dt_feat_sel, 'Decision Tree')\n\nfeat_selected_soln_models = {}\n\nprint('SOLUTION MODELS:')\n\n# Random Forest\nrand_for_clf_feat_sel = RandomForestRegressor(random_state = 0, max_depth = 3, max_features = 'auto', n_estimators = 10)\nrand_for_param = {'n_estimators': [10, 50, 100], 'max_features': ['auto', 'sqrt', 'log2'], 'max_depth': [3, 5, 7],}\ngrid_search_rand_for_feat_sel = GridSearchCV(estimator = rand_for_clf_feat_sel, param_grid = rand_for_param, cv = TimeSeriesSplit(n_splits = 10), )\ngrid_search_rand_for_feat_sel.fit(x_train, y_train)\nfeat_selected_validate_result(grid_search_rand_for_feat_sel, 'Feature Selected Random Forest GridSearch')\nfeat_selected_soln_models['Feature Selected Random Forest'] = grid_search_rand_for_feat_sel\n\n# Linear Support Vector Regression\nlinear_svr_feat_sel = SVR(C = 50.0, epsilon = 0, kernel = 'linear')\nlinear_svr_clf_feat_sel = linear_svr_feat_sel.fit(x_train, y_train)\nfeat_selected_validate_result(linear_svr_clf_feat_sel, 'Feature Selected Linear SVR')\nfeat_selected_soln_models['Feature Selected Linear SVR'] = linear_svr_clf_feat_sel\n\n# Lasso CV Regression\nlasso_feat_sel = LassoCV(n_alphas = 1000, max_iter = 4000, random_state = 0)\nlasso_clf_feat_sel = lasso_feat_sel.fit(x_train, y_train)\nfeat_selected_validate_result(lasso_clf_feat_sel, 'Feature Selected Lasso CV')\nfeat_selected_soln_models['Feature Selected Lasso CV'] = lasso_clf_feat_sel\n\n# Ridge CV Regression\nridge_feat_sel = RidgeCV(gcv_mode = 'auto')\nridge_clf_feat_sel = ridge_feat_sel.fit(x_train, y_train)\nfeat_selected_validate_result(ridge_clf_feat_sel, 'Feature Selected Ridge CV')\nfeat_selected_soln_models['Feature Selected Ridge CV'] = ridge_clf_feat_sel\n\n# Bayesian Ridge\nbayesian_feat_sel = linear_model.BayesianRidge().fit(x_train, y_train)\nfeat_selected_validate_result(bayesian_feat_sel, 'Feature Selected Bayesian Ridge')\nfeat_selected_soln_models['Feature Selected Bayesian Ridge'] = bayesian_feat_sel\n\n# Gradient Boosting\ngrad_boost_feat_sel = GradientBoostingRegressor(n_estimators = 100, learning_rate = 0.1, max_depth = 3, random_state = 0, loss = 'ls').fit(x_train, y_train)\nfeat_selected_validate_result(grad_boost_feat_sel, 'Feature Selected Gradient Boosting')\nfeat_selected_soln_models['Feature Selected Gradient Boosting'] = grad_boost_feat_sel\n\n# Stochastic Gradient Descent\nsgd_feat_sel = SGDRegressor(max_iter = 1000, tol = 1e-3, loss = 'squared_epsilon_insensitive', penalty = 'l1', alpha = 0.1).fit(x_train, y_train)\nfeat_selected_validate_result(sgd_feat_sel, 'Feature Selected SGD')\nfeat_selected_soln_models['Feature Selected SGD'] = sgd_feat_sel","956d742d":"feat_sel_RMSEs = {}\ndef feat_sel_evaluate_model(models):\n    fig, axes = plt.subplots(nrows = 3, ncols = 3, figsize = (15,15))\n    \n    # Plotting the benchmark model\n    bench_dt_pred = benchmark_dt_feat_sel.predict(feat_selected_validation_xvalue)\n    bench_rmse = np.sqrt(mean_squared_error(validation_yvalue, bench_dt_pred))\n    feat_sel_RMSEs['Benchmark score'] = bench_rmse\n    \n    axes[0,0].plot(validation_yvalue.index, bench_dt_pred, color = 'purple', label = 'Predicted')\n    axes[0,0].plot(validation_yvalue.index, validation_yvalue, color = 'orange', label = 'Actual')\n    axes[0,0].xaxis.set_major_formatter(mdates.DateFormatter('%m - %Y'))\n    axes[0,0].xaxis.set_major_locator(mdates.MonthLocator())\n    axes[0,0].set_ylabel('Price')\n    axes[0,0].set_title('Benchmark Predicted Models RMSE Score- '+ '{0:.2f}'.format(bench_rmse))\n    axes[0,0].legend(loc = 'upper right')\n    \n    #Plot Block\n    ax_x = 0\n    ax_y = 1\n    \n    # Plotting the Solution Model\n    for name, model in models.items():\n        predicted = model.predict(feat_selected_validation_xvalue)\n        soln_rmse = np.sqrt(mean_squared_error(validation_yvalue, predicted))\n        \n        axes[ax_x, ax_y].plot(validation_yvalue.index, predicted, color = 'purple', label = 'Predicted')\n        axes[ax_x, ax_y].plot(validation_yvalue.index, validation_yvalue, color = 'orange', label = 'Actual')\n        axes[ax_x, ax_y].xaxis.set_major_formatter(mdates.DateFormatter('%m - %Y'))\n        axes[ax_x, ax_y].xaxis.set_major_locator(mdates.MonthLocator())\n        axes[ax_x, ax_y].set_ylabel('Price')\n        axes[ax_x, ax_y].set_title(name + ' RMSE Score - '+ '{0:.2f}'.format(soln_rmse))\n        axes[ax_x, ax_y].legend(loc = 'upper right')\n        feat_sel_RMSEs[name] = soln_rmse\n        \n        if ax_x <= 2:\n            if ax_y < 2:\n                ax_y = ax_y + 1\n            else:\n                ax_x = ax_x + 1\n                ax_y = 0\n    \n    plt.show()\n\nfeat_sel_evaluate_model(feat_selected_soln_models)","4b362c2c":"feat_sel_model_names = []\nfeat_sel_model_values = []\n\nfor model_name, model_value in feat_sel_RMSEs.items():\n    feat_sel_model_names.append(model_name)\n    feat_sel_model_values.append(model_value)\n\nfeat_sel_model_values = np.array(feat_sel_model_values)\nfeat_sel_model_names = np.array(feat_sel_model_names)\n\nfeat_sel_indices = np.argsort(feat_sel_model_values)\nfeat_sel_columns = feat_sel_model_names[feat_sel_indices[:8]]\nfeat_sel_values = feat_sel_model_values[feat_sel_indices[:8]]\nall_feat_values = model_values[feat_sel_indices[:8]]\n\nfig = plt.figure(figsize = (16,9))\nplt.bar(np.arange(8) - 0.2, all_feat_values, width = 0.3, color = '#470670', label = 'All Features')\nplt.bar(np.arange(8), feat_sel_values, width = 0.3, color = '#34ebc0', label = 'Features selected')\nplt.xticks(np.arange(8), feat_sel_columns, rotation = 90)\nplt.xlabel('Model Name')\nplt.ylabel('RMSE')\nplt.title('Comparing RMSE Scores after Feature Selection')\nplt.legend(loc = 'upper center')\nplt.show()","a83ccbcf":"ensem_soln_models = [lasso_clf_feat, ridge_clf_feat, bayesian_feat]\n\nclass EnsembleSolution:\n    models = []\n    def __init__(self, models):\n        self.models = models\n    def fit(self, x, y):\n        for i in self.models:\n            i.fit(x, y)\n    def predict(self, x):\n        result = 0\n        for i in self.models:\n            result = result + i.predict(x)\n        result = result \/ len(self.models)\n        return result\n    \n    ","13c9be56":"print('Ensemble Solution Model with All Features')\nensem_model = EnsembleSolution(ensem_soln_models)\nresult_validation(ensem_model, 'Ensemble Solution ')","18a14d07":"feat_sel_ensem_soln_models = [linear_svr_feat_sel, lasso_clf_feat_sel, sgd_feat_sel]\n\nprint('Ensemble Solution Model with Features Selected')\nfeat_sel_ensem_model = EnsembleSolution(feat_sel_ensem_soln_models)\nfeat_selected_validate_result(feat_sel_ensem_model, 'Ensemble Solution with Feature Selection')","125831d6":"def train_multiple_times(model, times):\n    tot_rmse = 0\n    tot_r2 = 0\n    for i in range(times):\n        regression = model\n        for train_index, test_index in TimeSeriesSplit(n_splits = i+2).split(feat_minmax_transform):\n            x_train, x_test = feat_minmax_transform[:len(train_index)], feat_minmax_transform[len(train_index):(len(train_index) + len(test_index))]\n            y_train, y_test = target_adj_close[:len(train_index)].values.ravel(), target_adj_close[len(train_index):(len(train_index) + len(test_index))].values.ravel()\n            regression.fit(x_train, y_train)\n        predicted = regression.predict(validation_xvalue)\n        rmse, r2 = show_result(validation_yvalue, predicted, [0, len(validation_yvalue)])\n        tot_rmse = tot_rmse + rmse\n        tot_r2 = tot_r2 + r2\n    return tot_rmse \/ times, tot_r2 \/ times\n\ndef show_result(actual, predicted, index):\n    rmse_score = np.sqrt(mean_squared_error(actual, predicted))\n    print('From {} to {}'.format(index[0], index[-1]))\n    print('RMSE Score: ', rmse_score)\n    R2_score = r2_score(actual, predicted)\n    print('R2 Score: ', R2_score)\n    print('_______________')\n    return rmse_score, R2_score","5115498e":"print('Benchmark:')\nmulti_times_benchmark_rmse, multi_times_benchmark_r2 = train_multiple_times(benchmark_dt, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_benchmark_rmse, multi_times_benchmark_r2))","9f68ff86":"print('Lasso CV')\nmulti_times_lasso_rmse, multi_times_lasso_r2 = train_multiple_times(lasso_clf_feat, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_lasso_rmse, multi_times_lasso_r2))","e947560e":"print('Ridge CV')\nmulti_times_ridge_rmse, multi_times_ridge_r2 = train_multiple_times(ridge_clf_feat, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_ridge_rmse, multi_times_ridge_r2))","068e114d":"print('Bayesian Ridge')\nmulti_times_bayesian_rmse, multi_times_bayesian_r2 = train_multiple_times(bayesian_feat, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_bayesian_rmse, multi_times_bayesian_r2))","da930c3b":"print('Linear SVR')\nmulti_times_lsvr_rmse, multi_times_lsvr_r2 = train_multiple_times(linear_svr_clf_feature, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_lsvr_rmse, multi_times_lsvr_r2))","bd6a9412":"print('Ensemble')\nmulti_times_ensemble_rmse, multi_times_ensemble_r2 = train_multiple_times(ensem_model, 8)\nprint('RMSE: {} \/\/ R2: {} \\n'.format(multi_times_ensemble_rmse, multi_times_ensemble_r2))","bb255be8":"def cross_validation(model, ts_split):\n    clf = model\n    tot_rmse = 0\n    tot_r2 = 0\n    count = 0\n    for train_index, test_index in TimeSeriesSplit(n_splits = 10).split(validation_xvalue):\n        x_test1, x_test2 = validation_xvalue[:len(train_index)], validation_xvalue[len(train_index):(len(train_index) + len(test_index))]\n        y_test1, y_test2 = validation_yvalue[:len(train_index)].values.ravel(), validation_yvalue[len(train_index):(len(train_index) + len(test_index))].values.ravel()\n        predicted_test1 = clf.predict(x_test1)\n        temp1_rmse, temp1_r2 = show_result(y_test1, predicted_test1, train_index)\n        predicted_test2 = clf.predict(x_test2)\n        temp2_rmse, temp2_r2 = show_result(y_test2, predicted_test2, test_index)\n            \n        tot_rmse = tot_rmse + temp1_rmse + temp2_rmse\n        tot_r2 = tot_r2 + temp1_r2 + temp2_r2\n        count = count + 2\n    return tot_rmse \/ count, tot_r2 \/ count","043f38ff":"test_benchmark_rmse, test_benchmark_r2 = cross_validation(benchmark_dt, TimeSeriesSplit(n_splits = 10))","2032818c":"test_lasso_rmse, test_lasso_r2 = cross_validation(lasso_clf_feat, TimeSeriesSplit(n_splits = 10))","b5691270":"test_ridge_rmse, test_ridge_r2 = cross_validation(ridge_clf_feat, TimeSeriesSplit(n_splits = 10))","2734d3ae":"test_bayesian_rmse, test_bayesian_r2 = cross_validation(bayesian_feat, TimeSeriesSplit(n_splits = 10))","f4523d1f":"test_lsvr_rmse, test_lsvr_r2 = cross_validation(linear_svr_clf_feature, TimeSeriesSplit(n_splits = 10))","e2710c20":"test_ensemble_rmse, test_ensemble_r2 = cross_validation(ensem_model, TimeSeriesSplit(n_splits = 10))","6e0d388d":"print('Benchmark RMSE: {}, Benchmark R2 Score: {}'.format(test_benchmark_rmse, test_benchmark_r2))\nprint('Lasso CV RMSE: {}, Lasso CV R2 Score: {}'.format(test_lasso_rmse, test_lasso_r2))\nprint('Ridge CV RMSE: {}, Ridge CV R2 Score: {}'.format(test_ridge_rmse, test_ridge_r2))\nprint('Bayesian Ridge RMSE: {}, Bayesian Ridge R2 Score: {}'.format(test_bayesian_rmse, test_bayesian_r2))\nprint('Linear SVR RMSE: {}, Linear SVR R2 Score: {}'.format(test_lsvr_rmse, test_lsvr_r2))\nprint('Ensemble Model RMSE: {}, Ensemble Model R2 Score: {}'.format(test_ensemble_rmse, test_ensemble_r2))","c27c1506":"# Model Building","607cd959":"# Calculation of Daily Returns of all Features","0a126bff":"# Scatterplot","e90030e8":"### Calculating mean, standard deviation and kurtosis for Gold Rates","7b407b3d":"### 1. Benchmark Model:","39328b5d":"It is observed that the model improves with Features Selected in Lasso CV, Bayesian Ridge, Ridge CV, and Random Forest models. Among models with All Features, Lasso CV has the lowest RMSE and works best, and among models with Features Selected, Linear SVR has the lowest RMSE and works best. Lasso CV works best overall because it has an RMSE score of 0.68 with All Features and 0.7 with Features Selected.   ","19105257":"# Importing Libraries","9c69ce31":"Therefore, the Random Forest with default parameters has better RMS Error and R2 Score values than the tuned one. So this can be taken as the 2nd solution model.","c80d829f":"## Cross Validation","03881c13":"## Evaluating the Feature Selected Model","50c1ae3c":"## Validation of Features Selected: Benchmark and Solution Model","2ebda26e":"## Comparison of RMSE of Benchmark and Solution Models","58c4f2a9":"Here, Adjusted Close is the target variable.","c6ec2a59":"### Calculating mean, standard deviation and kurtosis of S&P 500 Index Daily Return","8cb1c1b6":"## Training the Model Multiple Times","c92f3059":"Thus, The Ensemble Solution Model with All Features shows the best result with RMSE = 0.672 and R2 score = 0.896.","b17bcd95":"# Comparison of RMSE Scores of Feature Selected Models and All Features Model","9038444e":"# Checking for Missing Values","ed28dcbe":"### Plotting Technical Indicators:","0504cfde":"Using gridsearch on Support Vector Regressor gives lowest RMS Error and highest R2 value. So this can be taken as the first solution model.","ce5af395":"### Plotting Correlation Matrix:","64fc25f7":"# Normalizing the Data","a1fa13f3":"# Using Statistical Measures:\n\nHere, mean, standard deviation, and kurtosis will be used as statistical measures.","ba8c4445":"# Final Solution\n\nHere, we'll take the top 3 performing models (All Features: Lasso CV, Ridge CV, Bayesian Ridge and Features Selected: Linear SVR, Lasso CV, Stochastic Gradient Descent) and will compare All Features ensemble models with Feature Selected emsemble models.","1965a717":"# Effect of Index Prices on Gold Rates","d32a2136":"### Positively Correlated Variables","e725a8fe":"# Correlation Analysis:","bb14a704":"# Computing Daily Returns of Stock Indices","bca3c71c":"# Technical Indicators:\nThe following technical indicators will be used as features to help in gold price prediction:\n1. Moving average convergence-divergence (MACD)\n2. Relative strength index (RSI)\n3. Simple moving average (SMA)\n4. Upper Band\n5. Lower Band\n6. DIFF\n7. Open-Close\n8. High-Low","779da4ff":"## Feature Selection","6e771554":"## Evaluation of Models\n### (Based on RMSE and R2 scores)","ddd11d34":"# Train Test Split","2b6d3ea2":"### Negatively Correlated Variables:","6c714a0b":"### 2. Solution Model:","37eb81cf":"# Introduction\n\nIn this project, different machine learning algorithms will be used for forecasting the gold rates. Also, an attempt will be made to identify the attributes that highly influence the gold prices.","0ec6ddae":"### Computing mean, standard deviation and kurtosis of DJ Index daily return:","76b7a513":"So, there are no null values.","b6be75e0":"## Train Test Split"}}