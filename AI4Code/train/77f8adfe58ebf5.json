{"cell_type":{"3b83217e":"code","50e009e6":"code","83d937b3":"code","9474d300":"code","740e1bea":"code","e5dc8946":"code","24e379b4":"code","648fe210":"code","050d5a69":"code","1f21f9e7":"code","054cfc51":"code","85b5f4f7":"code","48e7d950":"code","839fbcb3":"code","5db56077":"code","49d9ac38":"code","fe6fbc3c":"markdown","fa12eb76":"markdown","0345312f":"markdown","c29b787a":"markdown","84d6ba54":"markdown","26c3cd74":"markdown","ca6f6ac0":"markdown","7069d0c4":"markdown","f238bc3e":"markdown","7bdd0861":"markdown","ecd1663c":"markdown","99814766":"markdown","4294133a":"markdown"},"source":{"3b83217e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50e009e6":"from sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, LeakyReLU\nfrom keras.optimizers import RMSprop","83d937b3":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntrain.head()\ny = train.label\nx = train.drop(columns= 'label')\nx = x\/255\nx.head()\n\ny = to_categorical(y, num_classes = 10)\nprint(y.shape)","9474d300":"X_train,X_val,y_train,y_val = train_test_split(x,y,test_size=0.2, random_state=100)\nprint(X_train.shape ,X_val.shape, y_train.shape, y_val.shape)\nX_train= X_train.values.reshape( -1 , 28 , 28 , 1)\nX_val= X_val.values.reshape( -1 , 28 , 28 , 1)\nprint(X_train.shape ,X_val.shape)\n","740e1bea":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='tanh', input_shape = (28,28,1)))\n\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation=LeakyReLU(alpha=0.3)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation=LeakyReLU(alpha=0.2)) )\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))","e5dc8946":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"binary_crossentropy\", metrics=[\"accuracy\"])\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\nepochs = 25 \nbatch_size = 80","24e379b4":"model.summary()","648fe210":"# Fit the model \nprint(X_train.shape ,X_val.shape, y_train.shape, y_val.shape)\n\n\nhistory = model.fit(x= X_train,y= y_train, batch_size=batch_size,\n                              epochs = epochs, validation_data = (X_val, y_val),\n                              verbose = 2, steps_per_epoch=(X_train.shape[0] \/\/ batch_size)\n                            , callbacks=[learning_rate_reduction])","050d5a69":"test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nprint(test.shape)\n\nxtest = test\/255\nxtest= xtest.values.reshape( -1 , 28 , 28 , 1)\nprint(xtest.shape)\nypre= model.predict(xtest)\nypre","1f21f9e7":"sample = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsample1 = sample.drop(columns= 'ImageId')\nsample1[:5]","054cfc51":"y_predicted = [np.argmax(i) for i in ypre]\ny_predicted[:5]","85b5f4f7":"print(len(sample1),'<-- sample prediction-->  ' ,  len(y_predicted))","48e7d950":"y_predicted = pd.DataFrame(y_predicted)\nnewY = pd.concat([sample.drop(columns='Label'), y_predicted], axis = 1)\n","839fbcb3":"newY.columns=['ImageId', 'Label']\nnewY","5db56077":"model.evaluate(xtest,ypre)","49d9ac38":"newY.to_csv('my_submission.csv', index= False)\n","fe6fbc3c":"# display some answers ","fa12eb76":"# importing the libraries ","0345312f":"# Evaluate the Modle ","c29b787a":"after reading the data we are going to normalize it by divid each pixel with 255 ","84d6ba54":"# saving the Model","26c3cd74":"# model fitting","ca6f6ac0":"# model \n>* first we are going to create CNN with 32 filters,  kernel 5x5 , and relu activation funciton  followed by Max pooling layer \n>* then create the second CNN layer with 64 filters,  kernel 3x3 , and relu activation funciton  followed by Max pooling layer \n>* then using flat the data into 1 dimension using flatten funciton, with 1024 NN,  relu activation function, and 50% dropout \n>* followed by second NN with 256 neuron with leaky Relu activation function, 50% dropout \n>* lastly the output NN with 10 neurons that calculated using softmax activation function. ","7069d0c4":" <img src= \"https:\/\/www.researchgate.net\/profile\/Woongje-Sung\/publication\/322313274\/figure\/fig1\/AS:653009751134212@1532701065757\/LeNet-59-CNN-for-handwritten-digit-recognition-task.png\" alt =\"nuroal network \"  align=\"center\" style='width: 500px;' >\n","f238bc3e":"<h1 style=\"color:black\" align=\"center\">Handwritten digits classification using neural network<\/h1>\n\n\nIn this notebook we will classify handwritten digits using a simple neural network which has only input and output layers. We will than add a hidden layer and see how the performance of the model improves","7bdd0861":"# CNN  digital recognition with tensorflow","ecd1663c":"here we are going to use sklearn models to fit the data into a NN model that have **64 nuroal** in the frist hidden layer, **32 nuroal **in the second hidden layer, and** 10 output** layers ","99814766":"###  **Please upvote if you like my approach or if you learned something from this notebook. Your support gives me motivation to create interesting stuff. Thank you.** ","4294133a":"# model predicting"}}