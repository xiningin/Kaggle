{"cell_type":{"7b53af42":"code","21b07447":"code","462f4183":"code","0952bfd1":"code","5cd6b366":"code","59d8ce01":"code","e2f4b7e3":"code","90fd348c":"code","fed94493":"code","91ae220a":"code","ab1bbdfb":"code","caf4a04a":"code","c59770d6":"code","6dca8827":"code","c99854e7":"code","ed6605b0":"code","2e07fdc3":"code","21d76f7a":"code","cdc61fae":"code","6fa77805":"code","6222cd71":"code","e603db9d":"code","a07071d7":"code","36f415d4":"code","356d69ba":"code","6897bc9b":"code","4635bfe9":"code","50169751":"code","6637961d":"code","e3826778":"code","8885184a":"code","c93aec06":"code","17dbf79f":"code","82b7521a":"code","1e7727c9":"code","fc58bf03":"code","d5de974c":"code","a0241b20":"code","1f34557b":"code","8d042eb7":"code","8e0dd680":"code","36d9ec76":"code","d3839c4c":"code","96c8808b":"code","8d9c4f53":"code","4930ec17":"code","517ea366":"code","96698b14":"code","f3293c4b":"code","ed6034d9":"code","d6711dea":"code","fca2a27c":"code","580d6ac9":"code","f2cec49f":"code","66318c61":"code","94d45287":"code","e2248d84":"code","012ed65a":"code","cec704ff":"code","ff385898":"code","e10d3038":"markdown","ae2a1a1a":"markdown","49f417e4":"markdown","889ca86f":"markdown","0a73c5a9":"markdown","e8fa20e4":"markdown","cb0968d4":"markdown","ee6c3564":"markdown","37a34ae0":"markdown","a7f3635e":"markdown","fa91fd14":"markdown","4b09fd93":"markdown","d2317ba9":"markdown","a5393f4a":"markdown","fe752c07":"markdown","310d6652":"markdown","e49c5d4d":"markdown","be221bba":"markdown","30bb5c4d":"markdown","be6659d5":"markdown","cc398dbe":"markdown","8d263931":"markdown","c73f36ea":"markdown","a006e54f":"markdown","ac1cba4a":"markdown","9f3c7fb7":"markdown","e012b210":"markdown","cf0bc254":"markdown","902a1795":"markdown","ff6cc38f":"markdown","00a9b969":"markdown","faa74b35":"markdown","7f971adb":"markdown","cc5143ce":"markdown","3b56e710":"markdown","537565aa":"markdown","df903ae6":"markdown","09ec4477":"markdown","8a856bf1":"markdown","e58eea6b":"markdown","7190a05a":"markdown","36362c8f":"markdown","28790db0":"markdown","de00d81a":"markdown","f42eda89":"markdown","9e473cda":"markdown","311ad749":"markdown","e2f3792c":"markdown","4d21a2f3":"markdown","1abf9c65":"markdown","c3ef49d4":"markdown","d32b7ebb":"markdown","5ce4b4af":"markdown","ddd7ff6e":"markdown","e2513bee":"markdown","af9d7251":"markdown","6882c171":"markdown","ddb8ee12":"markdown","fe9b44cd":"markdown"},"source":{"7b53af42":"# Flattens stacked grouped columns\ndef flatten(dataframe):\n    dataframe.columns = [' '.join(col).strip() for col in dataframe.columns.values]\n    return dataframe\n\n# Object to extend the functionality of the ML models\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        self.__name__ = clf.__name__\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def cv_score(self,x_train,y_train):\n        array = cross_val_score(self.clf,x_train,y_train, cv = 5,scoring = 'accuracy')\n        return array.mean()\n    \n    def feature_importances(self,x,y):\n        return self.clf.fit(x,y).feature_importances_","21b07447":"# Importing datatable modules\nimport numpy as np\nimport pandas as pd\n\n# Importing Graphing Modules\nimport plotly_express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\n\n# Importing ML models\/metrics\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n# Importing other useful libraries\nimport os\nfrom collections import defaultdict\n\n# Printing list of files in input folder\nprint(os.listdir(\"..\/input\"))\n\n# Initializing plotly offline\ninit_notebook_mode(connected=True)","462f4183":"# Loading datasets \ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nfull_dataset = [train,test]","0952bfd1":"# Printing dataset information\nprint(\"Training Dataset Information:\")\nprint(train.info())\nprint(\"\\nTest Dataset Information:\")\nprint(test.info())","5cd6b366":"# Characterizing null values\nprint(\"Training Null Values (%):\")\nprint(train.isnull().sum()*100\/train.shape[0])\nprint(\"\\n\")\nprint(\"Test Null Values (%):\")\nprint(test.isnull().sum()*100\/test.shape[0])","59d8ce01":"# Grouping by survival rate and standard deviation and plotting results\npclass_grouped = train[['Pclass','Survived']].groupby('Pclass', as_index=False).agg({'Survived':['mean','std']})\npclass_grouped = flatten(pclass_grouped)\nfig = px.bar(pclass_grouped,x = \"Pclass\", y = \"Survived mean\", color = \"Pclass\", error_y = \"Survived std\")\nfig.update_traces(error_y_color = \"black\")","e2f4b7e3":"# Looking at survival by gender\nsex_grouped = train[['Sex','Survived']].groupby('Sex',as_index=False).agg({'Survived':['mean','std']})\nsex_grouped = flatten(sex_grouped)\nfig = px.bar(sex_grouped,x = \"Sex\",y = \"Survived mean\", error_y = \"Survived std\", color = \"Sex\")\nfig.update_traces(error_y_color = \"black\")","90fd348c":"# Age purely versus survival\npx.histogram(train,x = \"Age\", opacity = 0.7, color = \"Survived\")","fed94493":"# Looking at age distribution and Pclass relationship\npx.histogram(train, x = \"Age\", y = \"Name\", color = \"Survived\", facet_row = \"Pclass\", labels = dict(Name = \"People\"), opacity = 0.7)","91ae220a":"# Plotting distribution of fare on a log graph\npx.histogram(train, x = \"Fare\", log_y = True, color = \"Survived\", opacity = 0.7)","ab1bbdfb":"# Plotting distribution of fares by class\npx.histogram(train, x = \"Fare\", log_y = True, facet_col = \"Pclass\", color = \"Pclass\")","caf4a04a":"# Plotting embarkation point by survival rate\nemb_grouped = train[['Embarked','Survived']].groupby('Embarked',as_index=False).agg({'Survived':['mean','std']})\nemb_grouped = flatten(emb_grouped)\nfig = px.bar(emb_grouped,x = \"Embarked\",y = \"Survived mean\", error_y = \"Survived std\", color = \"Embarked\")\nfig.update_traces(error_y_color = \"black\")","c59770d6":"# Checking correlation between features to understand relative trends\/comparisons before feature engineering\nz = train.corr()\ntrace = go.Heatmap(\n    z = z,\n    x = z.columns,\n    y = z.columns\n)\niplot([trace])","6dca8827":"# Combining SibSp and Parch in one column since they're highly correlated\nfor dataset in full_dataset:\n    dataset['Family_Size'] = dataset['SibSp'] + dataset['Parch'] + 1","c99854e7":"# Grouping by family size and plotting\ngrouped_family_size = train[['Family_Size','Survived','Name']].groupby(['Family_Size','Survived']).count().reset_index()\ngrouped_family_size.columns = [\"Family_Size\",\"Survived\",\"Name\"]\npx.bar(grouped_family_size, x = \"Family_Size\", y = \"Name\", color = \"Survived\", barmode = \"group\", labels = dict(Name = \"Count\"))","ed6605b0":"# Cutting data\nfamily_bins = [0, 1, 4, 20]\nfamily_labels = [1, 2, 3]\nfor dataset in full_dataset:\n    dataset['Family_Cat'] = pd.cut(dataset['Family_Size'], bins = family_bins, labels = family_labels, include_lowest = True)","2e07fdc3":"# Re-visualizing the grouping\ngrouped_family_size = train[['Family_Cat','Survived','Name']].groupby(['Family_Cat','Survived']).count().reset_index()\ngrouped_family_size.columns = [\"Family_Cat\",\"Survived\",\"Name\"]\npx.bar(grouped_family_size, x = \"Family_Cat\", y = \"Name\", color = \"Survived\", barmode = \"group\", labels = dict(Name = \"Count\"))","21d76f7a":"# Visualizing a few name variables\ntrain['Name'].head(10)","cdc61fae":"# Extracting name information and storing it in the 'Title' column\nfor dataset in full_dataset:\n    dataset['Title'] = dataset.Name.str.extract(r\"([A-Za-z]+)\\.\", expand = False)","6fa77805":"# Printing all unique titles\nset(train['Title'].unique()) | set(test['Title'].unique())","6222cd71":"# Replacing values with the following mappings\ntitle_map = {'Rare': [ 'Capt', 'Col','Countess','Dr','Jonkheer','Lady','Major','Rev','Sir'],\n             'Mr': ['Mr','Don'],\n             'Mrs':['Mme','Mrs','Dona'],\n             'Miss':['Ms', 'Miss','Mlle'],\n             'Master':['Master']}\n\nfor dataset in full_dataset:\n    for key, value in title_map.items():\n        dataset['Title'] = dataset['Title'].replace(value,key)","e603db9d":"# Printing new set of unique values to make sure that we didn't miss anything\nunique_titles = list(set(train['Title'].unique()) | set(test['Title'].unique()))\nprint(\"Unique Titles:\",unique_titles)","a07071d7":"# Categorizing the titles into buckets and printing the mapping\ntitle_mapping = dict(zip(unique_titles,list(range(1,6))))\nprint(\"Title Mapping:\",title_mapping)\nfor dataset in full_dataset:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","36f415d4":"# Visualizing survival and death rates by title\ngrouped_title = train[['Title','Survived']].groupby('Title',as_index=False).agg({\"Survived\":['mean','std']})\ngrouped_title = flatten(grouped_title).sort_values(by=\"Survived mean\", ascending = False)\nfig = px.bar(grouped_title, x = \"Title\", y = \"Survived mean\", error_y = \"Survived std\", color = \"Title\")\nfig.update_traces(error_y_color = \"black\")","356d69ba":"# Filling in null cabin values with U\nfor dataset in full_dataset:\n    dataset['Cabin'].fillna(\"U\",inplace=True)","6897bc9b":"# Extracting only deck letter from cabin\nfor dataset in full_dataset:\n    dataset['Cabin_Deck'] = dataset['Cabin'].apply(lambda x: x.strip()[0])","4635bfe9":"# Grouping decks by class \ngrouped_cabinclass = train.groupby(['Cabin_Deck','Pclass']).agg({'Name':'nunique'})\ngrouped_cabindeck = train.groupby(['Cabin_Deck']).agg({'Name':'nunique'})\n\n# Normalizing the grouping\ngrouped_cabin = (grouped_cabinclass\/grouped_cabindeck).reset_index()\nfig2 = px.bar(grouped_cabin, x = \"Cabin_Deck\",y = \"Name\", color = \"Pclass\", labels = dict(Name = \"% of Total\"), title = \"Normalized Distribution of Classes By Deck\")\niplot(fig2)","50169751":"# Grouping by cabin and plotting bar graph\ngrouped_cabindeck = train[['Cabin_Deck','Survived','Fare']].groupby('Cabin_Deck',as_index=False).agg({'Survived':['mean','std'],\"Fare\":['mean','std']})\ngrouped_cabindeck = flatten(grouped_cabindeck)\nfig = px.bar(grouped_cabindeck, x = \"Cabin_Deck\", y = \"Survived mean\", error_y = \"Survived std\", color = \"Cabin_Deck\")\nfig.update_traces(error_y_color = \"black\")","6637961d":"# Generating map for cabin deck\npossible_decks = list(set(test['Cabin_Deck'].unique()) | set(train['Cabin_Deck'].unique()))\ncabin_mappings = dict(zip(possible_decks,list(range(1,len(possible_decks)+1))))\nprint(cabin_mappings)","e3826778":"# Mapping cabin decks\nfor dataset in full_dataset:\n    dataset['Cabin_Deck'] = dataset['Cabin_Deck'].map(cabin_mappings)","8885184a":"# Converting sex to binary mapping\nfor dataset in full_dataset:\n    dataset['Gender'] = dataset['Sex'].map({'male':1,'female':2})","c93aec06":"# Filling missing values for age with random integers within 1 standard deviation of the mean for combined train\/test datasets\nconcatenated = pd.concat([train.drop('Survived', axis = 1),test])\nage_avg = concatenated['Age'].mean()\nage_std  = concatenated['Age'].std()\n\nfor dataset in full_dataset:\n    age_null_count = dataset['Age'].isnull().sum()\n\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n\n    # Setting NaN \n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = age_null_random_list\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)","17dbf79f":"# Creating bins to cut ages into. These bins were determined by splitting the age data into 10 quantiles of approximately equal \n# number of people. The work was done in a separate notebook \n# Please comment if you'd like to see how this was done\n\nage_bins = [0,16,19,22,25,28,31,35,40,47,100]\nages = [\"0-16\",\"16-19\",\"19-22\",\"22-25\",\"25-28\",\"28-31\",\"31-35\",\"35-40\",'40-47',\"47+\"]\nage_labels = list(range(1,11))\nprint(\"Age Mappings:\",dict(zip(ages,age_labels)) )","82b7521a":"# Cutting the data by the bins\nfor dataset in full_dataset:\n    dataset['Age_Cat'] = pd.cut(dataset['Age'], bins = age_bins, include_lowest = True, labels = age_labels)","1e7727c9":"# Plotting the cut data\ngrouped_agecats = train.groupby('Age_Cat', as_index=False).agg({'Survived':['mean','std']})\ngrouped_agecats = flatten(grouped_agecats)\nfig = px.bar(grouped_agecats,x = \"Age_Cat\", y = \"Survived mean\", error_y = \"Survived std\", range_y = [0, 1], color = \"Age_Cat\")\nfig.update_traces(error_y_color = \"black\")","fc58bf03":"# Filling missing values for train set\ntrain[train['Embarked'].isnull()]","d5de974c":"# Filling in missing values accordingly\ntrain.loc[[61,829],\"Embarked\"] = \"S\"","a0241b20":"# Converting Embarked to numerical\nembarked_mapping = {\"S\":1,\"C\":2,\"Q\":3}\nprint(\"Embarked Mapping:\",embarked_mapping)\nfor dataset in full_dataset:\n    dataset['Embarked_Cat'] = dataset['Embarked'].map(embarked_mapping)","1f34557b":"# Filling missing values for test set with median\ntest['Fare'].fillna(test['Fare'].median(),inplace=True)","8d042eb7":"# Creating bins to cut fares into. These bins were generated by analyzing quantiles of data in a separate jupyter notebook (similar \n# process as Section 6.5)\nfare_bins = [0, 7.8, 10.5, 21.7, 39.7, 550]\nfares = [\"0-7.8\",\"7.8-10.5\",\"10.5-21.7\",\"21.7-39.7\",\"39.7+\"]\nfare_labels = list(range(1,6))\nprint(\"Fare Mappings:\",dict(zip(fares,fare_labels)))","8e0dd680":"# Cutting the dataset according to the bins above\nfor dataset in full_dataset:\n    dataset['Fare_Cats'] = pd.cut(dataset['Fare'], bins = fare_bins, labels = fare_labels, include_lowest = True)","36d9ec76":"grouped_fares = train[['Fare_Cats', \"Survived\"]].groupby(\"Fare_Cats\",as_index=False).mean()\npx.bar(grouped_fares,x = \"Fare_Cats\",y=\"Survived\", color = \"Fare_Cats\")","d3839c4c":"# Finalized training and test models\nfeatures = ['Pclass','Title','Gender','Age_Cat','Family_Cat','Fare_Cats','Cabin_Deck','Embarked_Cat']\nX_train = train[features].values\nY_train = np.array(train[['Survived']]).ravel()\nX_test = test[features].values\nprint(\"X_train\")\nprint(X_train[0:10])\nprint(\"\\nX_test\")\nprint(X_test[0:10])","96c8808b":"# Random seed\nSEED = 0\n\n# Declaring parameters for each of the models\n# Extra Trees Classifier\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# SVC\nsvc_params = {\n    'kernel' : 'rbf',\n    'C' : 1,\n    'gamma': 'auto'\n}\n\n# RandomForestClassifier\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n    'warm_start': True, \n    'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# XGB\nxgb_params = {\n    \"learning_rate\": 0.02,\n    \"n_estimators\": 2000,\n    \"max_depth\": 4,\n    \"min_child_weight\": 2,\n    \"gamma\":1,                        \n    \"subsample\":0.8,\n    \"colsample_bytree\":0.8,\n    \"objective\": 'binary:logistic',\n    \"nthread\": -1,\n    \"scale_pos_weight\": 1\n}\n\n# Gradient Boosting\ngb_params = {\n    'n_estimators': 500,\n    'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}","8d9c4f53":"# Initializing models using the SklearnHelper function defined in Section 2\net = SklearnHelper(clf = ExtraTreesClassifier, seed = SEED, params = et_params)\nsvc = SklearnHelper(clf = SVC, seed = SEED, params = svc_params)\nrf = SklearnHelper(clf = RandomForestClassifier, seed = SEED, params = rf_params)\nxgb = SklearnHelper(clf = XGBClassifier, seed = SEED, params = xgb_params)\ngb = SklearnHelper(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)\nada = SklearnHelper(clf = AdaBoostClassifier, seed = SEED, params = ada_params)\nmodels = [ada,et,gb, rf,svc,xgb]","4930ec17":"# Looping through scores and getting dataframe of cross validation scores\nscore_dict = defaultdict(list)\nfor model in models:\n    score_dict['Model'].append(model.__name__)\n    score_dict['CV_Score'].append(model.cv_score(X_train,Y_train))\nscores = pd.DataFrame(score_dict).sort_values(by = \"CV_Score\", ascending = False)\nprint(scores)","517ea366":"# Plotting cross validation scores as a function of model\npx.bar(scores, y = \"Model\", x = \"CV_Score\", color = \"CV_Score\", orientation = \"h\")","96698b14":"# Capturing feature importances in Plotly figures and in a dictionary\nfeature_imps = defaultdict(list)\nfigs = []\nfor i, model in enumerate(models):\n    if model.__name__ == \"SVC\":\n        continue\n        \n    trace = go.Scatter(\n        y = model.feature_importances(X_train,Y_train),\n        x = features,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 25,\n            color = model.feature_importances(X_train,Y_train),\n            colorscale='Portland',\n            showscale=True\n            )\n    )\n    layout = go.Layout(\n            autosize= True,\n            title= model.__name__,\n            hovermode= 'closest',\n            yaxis=dict(\n                title= 'Feature Importance',\n                ticklen= 5,\n                gridwidth= 2\n            ),\n            showlegend= False\n            )\n    figs.append(dict(data = [trace], layout = layout))\n    \n    feature_imps[i].append(model.__name__)\n    feature_imps[i].extend(model.feature_importances(X_train,Y_train))\nfeature_imps = pd.DataFrame.from_dict(feature_imps, orient = \"index\", columns = ['Model_Name'] + features)","f3293c4b":"# Plotting feature importances by classification model\nfor fig in figs:\n    iplot(fig)","ed6034d9":"# Extract mean feature importance by model and plot\nmean_imp = pd.concat([feature_imps.mean(axis = 0),feature_imps.std(axis = 0)], axis = 1).reset_index()\nmean_imp.columns = [\"Feature\",\"Mean_Importance\",\"Std\"]\nfig = px.bar(mean_imp.sort_values(by=\"Mean_Importance\"), x = \"Feature\", y = \"Mean_Importance\", color = \"Mean_Importance\", error_y = \"Std\")\nfig.update_traces(error_y_color = \"black\")","d6711dea":"# Setting the parameter grid for each model\n# Extra Trees\net_grid = {\n    'n_estimators': [250, 500, 1000],\n    'max_depth' : [2,4,8],\n    'min_samples_leaf': [2,4,8]\n}\n\n# RF\nrf_grid = {\n    'n_estimators' : [250, 500, 1000],\n    'max_depth' : [2,4,6],\n    'min_samples_leaf': [2,4,8]\n}\n\n\n# XGB\nxgb_param_grid = {\n    \"learning_rate\": [0.01, 0.1, 1],\n    \"n_estimators\": [1000, 2000, 4000],\n    \"max_depth\": [2,3, 4]\n}","fca2a27c":"# Setting up Grid Search Models to tune\net_gs = GridSearchCV(\n    estimator = ExtraTreesClassifier(),\n    param_grid = et_grid,\n    cv = 5,\n    scoring = 'accuracy'\n)\n\nrf_gs = GridSearchCV(\n    estimator = RandomForestClassifier(),\n    param_grid = rf_grid,\n    cv = 5,\n    scoring = 'accuracy'\n)\n\nxgb_gs = GridSearchCV(\n    estimator = XGBClassifier(    \n        min_child_weight = 2,\n        gamma = 1,                        \n        subsample = 0.8,\n        colsample_bytree = 0.8,\n        objective = 'binary:logistic',\n        nthread = -1,\n        scale_pos_weight = 1),\n    param_grid = xgb_param_grid,\n    cv = 5,\n    scoring = 'accuracy'\n)","580d6ac9":"# Determining best parameters for each model\nbest_params = {}\nbest_params['Extra Trees'] = et_gs.fit(X_train,Y_train).best_params_","f2cec49f":"best_params['RF'] = rf_gs.fit(X_train,Y_train).best_params_","66318c61":"best_params['XGB'] = xgb_gs.fit(X_train,Y_train).best_params_","94d45287":"best_params","e2248d84":"# Creating the ensemble model with hard voting\nensemble = VotingClassifier(\n    estimators = [('ET',ExtraTreesClassifier(max_depth = 8, min_samples_leaf = 4, n_estimators = 1000)),\n                  ('RF',RandomForestClassifier(max_depth = 4, min_samples_leaf = 2, n_estimators = 500)), \n                  ('XGB',XGBClassifier(min_child_weight = 2,gamma = 1,  subsample = 0.8, colsample_bytree = 0.8, objective = 'binary:logistic', nthread = -1,\\\n                                       scale_pos_weight = 1, learning_rate = 0.01, max_depth = 4, n_estimators = 4000))],\n    voting = 'hard'\n)","012ed65a":"# Fitting the ensemble to the training data\nensemble_fit = ensemble.fit(X_train,Y_train)","cec704ff":"# Preparing submission\nsubmission = pd.concat([test['PassengerId'],pd.Series(ensemble_fit.predict(X_test))], axis = 1,)\nsubmission.columns = ['PassengerId','Survived']\nsubmission.head()","ff385898":"submission.to_csv(\"submission.csv\",index=False)","e10d3038":"### Observations:\n- People who embarked on the ship at Cherbourg had a higher chance of survival compared to those embarking from Queenstown or Southampton\n- Unsure why this trend exists. Should follow up by analyzing the distribution of classes and genders between cities","ae2a1a1a":"# 8. Submission File","49f417e4":"In this section, I select by engineered features and create test and training datasets.","889ca86f":"## 5.2 Sex","0a73c5a9":"# 1. Introduction\nThis notebook aims to build a binary classification model that predicts survival for the Titanic dataset. The primary purpose of this kernel is to improve my knowledge of data visualization techniques, feature engineering, and classification algorithms. A description of the dataset can be found [here](https:\/\/www.kaggle.com\/c\/titanic).\n\nI hope that my work in this notebook can help other beginners in data science develop their own skills in the aformentioned areas. So, I've included comments and my observations as much as possible within the notebook. \n\nThat being said, my approach is by no means perfect - hence, any recommendations and suggestions for improvement is always recommended. ","e8fa20e4":"# 5. Data Exploration","cb0968d4":"This section explores the basic structure, features, and limitations of our dataset.","ee6c3564":"## 6.2 Name","37a34ae0":"Based on the correlation between siblings and parents, we will combine them into one feature.","a7f3635e":"## 5.5 Embarkation Point","fa91fd14":"In this section, I define general parameters for the following models:\n- Extra Trees\n- Support Vector Machine\n- Random Forest\n- XG Boost\n- Gradient Boost\n- ADA Boost\n\nI then fit each model to the training data and quantify the best performing models by comparing cross validation scores on the training set.","4b09fd93":"We will categorize the 'Family_Size' feature to represent the observations above:\n- 1: Travelling alone\n- 2: Families of 2-4 people\n- 3: Families of 5 and above","d2317ba9":"In this section, I characterize the important features for each of the classification models (minus SVM). I look at this on a per-model and average basis.","a5393f4a":"## 7.5 Parameter Tuning Using GridSearch","fe752c07":"## 6.1 SibSp & Parch","310d6652":"There's only two people whose embarkation point is unknown. \n\nBoth these people boarded in Southampton.\n\n**Reference:**\n- https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html","e49c5d4d":"### Observations:\n- The youngest people have a higher chance of survival than older people\n- People within younger age brackets (teens to early 20s) have a lower chance of survival\n- In general, age doesn't seem to be a very huge factor in survival. This supports our earlier observations","be221bba":"### Observations:\n- Mrs and Miss (2 and 3) have the best chance of surviving because of female priority in evacuation\n- Males (Master (1), Mr (4)) have lower chance of survival \n- Rares weren't prioritized as much over women but they still have higher survival likliehood than Mr","30bb5c4d":"### Observations:\n> **Note:** There's a lot of \"U\" (missing) data - Refer to section 4\n\n- First class people were predominantly assigned to A, B, C, D, T, E. Other decks have mostly second and third class passengers. This supports the reference shared earlier\n- Most unassigned passengers were third class\n- People unassigned cabins have a much lower chance of survival than those assigned cabins (probably because they were mostly third class passengers)\n- Those assigned cabins have ~50%+ chance of survival\n- Due to the large amount of missing data, I am very skeptical of imputing \"U\" using the trends observed above. I think that I will treat each cabin type (including the missing category) separately. Please share your feedback on this approach if you have any","be6659d5":"In this section I will establish my final test and training datasets, test a variety of sophisticated classification algorithms, tune the best performing algorithms, and combine them in an ensemble ML model.","cc398dbe":"### Observations:\n- Lower class passengers seem to have had a lower chance of survival on average. This is probably because lower class passengers were given decks at lower levels in the ship or rooms with poor evacuation routes.","8d263931":"### Observations:\n- People who paid higher fares tend to be more likely to survive; the pink bars are more prominent and are usually larger as you move toward higher fares\n- The reasons most likely have to do with Pclass. Higher classed people were more likely to pay higher fares that could give them better rooms with better evacuation routes, etc","c73f36ea":"## 7.1 Final Datasets","a006e54f":"## 7.2 Setting Parameters & Fitting Models","ac1cba4a":"### Observations:\n- There's a large proportion of missing age and cabin data. We will need to figure out a robust method to impute these values\n- Fares and embarked have very few missing values; we could possibly replace them with medians\/modes","9f3c7fb7":"### Observations:\n- Title and Gender are the most important features for our models. This suggests that the most important reason for survival was down to being a female (because title and gender are very correlated)\n- PClass and Cabin deck (they too, are correlated) have similar importances and suggest that your social class was the second most important factor in survival; this was probably because position will play a larger role during emergency evacuations\n- Family, Age, Fare and Embarkation point are much less important","e012b210":"### Observations:\n> **Note:** I'm unsure why the Pclasses are in an awkward order...\n\n- There's a higher proportion of younger people (particularly those between the ages of 20 and 30) in the lower classes (Pclass 2 and 3)\n- Pclass 1 has the 'flattest' distribution of people\n- There's a higher survival chance the higher your socioeconomic status (similar observation as above). Pclass 1 has pink bars that are higher in more age brackets compared to Pclass 2 or 3\n- Extremely young children seem to have a high likliehood of survival (refer to graph of Pclass 2)\n- It **seems** that age is not as important as class. We will explore this concept in later sections","cf0bc254":"### Notes Before Feature Engineering:\n- I will ignore Ticket Number and Passenger ID for the time-being because they are unique values and probably do not have any bearing on survival (not that I know of, at least)","902a1795":"## 6.6 Embarkation Point","ff6cc38f":"In this section, we will tune each of the 3 aforementioned models (XGB, ADAB and SVC) using GridSearch.","00a9b969":"### Observations:\nThese name variables contain Mr., Mrs., etc. Considering the previous relationship we observed with gender, it may be useful to extract this information. These titles might also give an indication of whether the person is single or married.\n\nLet's extract it.","faa74b35":"# Table of Contents\n1. Introduction\n2. Useful Functions\n3. Loading Modules & Datasets\n4. Dataset Fundamentals\n5. Data Exploration\n>5.1. Pclass<br\/>\n>5.2. Sex<br\/>\n>5.3. Age<br\/>\n>5.4 Fare<br\/>\n>5.5 Embarkation Point<br\/>\n>5.6 Heatmap<br\/>\n6. Feature Engineering\n>6.1. SibSp & Parch<br\/>\n>6.2. Name<br\/>\n>6.3. Cabin<br\/>\n>6.4 Sex<br\/>\n>6.5 Age<br\/>\n>6.6 Embarkation Point<br\/>\n>6.7 Fare<br\/>\n7. Machine Learning\n>7.1 Final Datasets<br\/>\n>7.2 Setting Parameters & Fitting Models<br\/>\n>7.3 Feature Importances<br\/>\n>7.4 Correlation Between Models<br\/>\n>7.5 Parameter Tuning Using GridSearch<br\/>\n>7.6 Ensemble Model<br\/>\n8. Submission File","7f971adb":"This section will extend the data exploration section. We will also clump together similar features and\/or categorize discrete features.","cc5143ce":"# 6. Feature Engineering","3b56e710":"This section loads all necessary modules and raw datasets.","537565aa":"# 7. Machine Learning","df903ae6":"## 6.4 Sex","09ec4477":"## 6.5 Age","8a856bf1":"This section includes some functions that help me further on in the notebook.","e58eea6b":"# 2. Useful Functions","7190a05a":"Cabin has a lot of null values. However, cabins can allow us to infer a lot about a person's position on the ship and their proximity to evacuation routes or the collision. \n\nFrom the references below, I couldn't find information to link the cabin number to location. However, I could extract cabin deck. For the time-being, I will fill null values with a U until I figure out a better solution.\n\nBased on the references, it seems like most of the first class cabins\/amenities were in decks A-D and other sections were for lower class passengers. Hence, it'll be interesting to see the relationship between passenger class and deck\n\n**References**:\n- Cutout of the Titanic: https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/84\/Titanic_cutaway_diagram.png <br\/>\n- More Information: https:\/\/www.dummies.com\/education\/history\/titanic-facts-the-layout-of-the-ship\/","36362c8f":"In this section, we combine the 3 aforementioned classifiers into one ensemble model","28790db0":"Here we explore a non-numerical feature that was not considered in the Data Exploration section.","de00d81a":"# 3. Loading Modules & Datasets","f42eda89":"## 5.1 Pclass","9e473cda":"Since there's only 1 missing fare value, I'll simply fill it with the median.","311ad749":"### Observations:\n- Higher fares have higher chances of survival. This ties in with the Pclass observations made earlier","e2f3792c":"## 5.6 Heatmap","4d21a2f3":"## 6.3 Cabin","1abf9c65":"## 7.6 Ensemble Model","c3ef49d4":"### Observations:\n- Females had a much higher chance of survival on average. This is probaly due to the fact that evacuation procedures prioritized women.","d32b7ebb":"## 5.3 Age","5ce4b4af":"## 5.4 Fare","ddd7ff6e":"# 4. Dataset Fundamentals","e2513bee":"## 6.7 Fare","af9d7251":"### Observations:\n- If travelling alone, there's a higher chance of the person not surviving than if he\/she travels in families of 2 to 4\n- If travelling in families of 2 to 4, theres a higher chance of the person surviving - probably because these families consist of both children and parents who get priority in evacuation procedures\n- Larger than 4, there's a very high chance of dying; probably because locating all people in your family during an evacuation is difficult","6882c171":"## 7.3 Feature Importances","ddb8ee12":"### Observations:\n- Of the features, Sex, Passenger Class, and Fare have small to medium correlations with survival and will most likely be the most important features. This follows well from the observations earlier\n- In general, most of the features seem uncorrelated with each other which suggests that each feature will probably play an important role\n    - Siblings and parents are quite correlated; probably due to the fact that families travelled together\n    - Age, Parents\/Children, and Siblings\/Spouses are also somewhat correlated; probably because the younger you are, the more likely you are to travel with siblings\/parents. Likewise, the older you are, the more likely you are to have children\/family members that you travel with    \n    - Age and Pclass are quite correlated; this supports the plots in Section 5.3\n    - Fare and Pclass are very correlated; this supports the plots in Section 5.4\n","fe9b44cd":"In this section, we will explore a few numerical\/discrete features of the data (Pclass, Sex, Age, Fare & Embarkation Point) through graphical representations using [Plotly Express\/Plotly](https:\/\/plot.ly\/python\/)."}}