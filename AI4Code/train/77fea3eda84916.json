{"cell_type":{"ff0bba60":"code","7b5e3123":"code","09336f78":"code","dd6a5171":"code","dd9257da":"code","5ed27280":"code","8071cf07":"code","29358c3b":"code","ed983df1":"code","ed374759":"code","d1c4e606":"code","202b21fa":"code","c75ddca6":"markdown","a5faf18c":"markdown","5d9cc60e":"markdown","750df9f9":"markdown","4ca035a1":"markdown","19714035":"markdown","bb206a1a":"markdown"},"source":{"ff0bba60":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","7b5e3123":"def feature_engineering(df):\n    df['Aspect'][df['Aspect'] < 0] += 360 \n    df['Aspect'][df['Aspect'] > 359] -= 360\n    \n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    \n    features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    df[\"hillshade_mean\"] = df[features_Hillshade].mean(axis = 1)\n    df['hillshade_amp'] = df[features_Hillshade].max(axis = 1) - df[features_Hillshade].min(axis = 1)\n    \n    df[\"ecldn_dist_hydrlgy\"] = (df[\"Horizontal_Distance_To_Hydrology\"] ** 2 + df[\"Vertical_Distance_To_Hydrology\"] ** 2) ** 0.5\n    df[\"mnhttn_dist_hydrlgy\"] = np.abs(df[\"Horizontal_Distance_To_Hydrology\"]) + np.abs(df[\"Vertical_Distance_To_Hydrology\"])\n    df['binned_elevation'] = [np.floor(v\/50.0) for v in df['Elevation']]\n    df['highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    \n    soil_features = [x for x in df.columns if x.startswith(\"Soil_Type\")]\n    df[\"soil_type_count\"] = df[soil_features].sum(axis=1)\n    \n    wilderness_features = [x for x in df.columns if x.startswith(\"Wilderness_Area\")]\n    df[\"wilderness_area_count\"] = df[wilderness_features].sum(axis = 1)\n    \n    df['soil_Type12_32'] = df['Soil_Type32'] + df['Soil_Type12']\n    df['soil_Type23_22_32_33'] = df['Soil_Type23'] + df['Soil_Type22'] + df['Soil_Type32'] + df['Soil_Type33']\n    \n    df['Horizontal_Distance_To_Roadways'][df['Horizontal_Distance_To_Roadways'] < 0] = 0\n    df['horizontal_Distance_To_Roadways_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Roadways']]\n    df['Horizontal_Distance_To_Fire_Points'][df['Horizontal_Distance_To_Fire_Points'] < 0] = 0\n    df['horizontal_Distance_To_Fire_Points_Log'] = [np.log(v+1) for v in df['Horizontal_Distance_To_Fire_Points']]\n    return df","09336f78":"%%time\n# import train & test data\ndf_train = dt.fread('..\/input\/tabular-playground-series-dec-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-dec-2021\/test.csv').to_pandas()\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n\n# drop underrepresented class\ndf_train = df_train[df_train['Cover_Type'] != 5]\n\n# apply feature-engineering\n# thanks to https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n# thanks to https:\/\/www.kaggle.com\/teckmengwong\/dcnv2-softmaxclassification#Feature-Engineering\ndf_train = feature_engineering(df_train)\ndf_test = feature_engineering(df_test)\n\n# split dataframes for later modeling\nX = df_train.drop(columns=['Id','Cover_Type','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\ny = df_train['Cover_Type'].copy()\n\nX_test = df_test.drop(columns=['Id','Soil_Type7','Soil_Type15','Soil_Type1']).copy()\n\n# create label-encoded one-hot-vector for softmax, mutliclass classification\nle = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\n\ndel df_train, df_test\ngc.collect()\n\nprint(X.shape, y.shape, target.shape, X_test.shape)","dd6a5171":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits\/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm \/ cm.sum(),\n        'recall' : cm \/ cm.sum(axis=1),\n        'precision': cm \/ cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","dd9257da":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_acc\", \n    patience=10, \n    verbose=True, \n    mode=\"max\", \n    restore_best_weights=True\n)","5ed27280":"# create custom dense-block\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, dropout_rate=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units,\n            kernel_initializer=\"lecun_normal\"\n        )\n        self.selu = layers.Activation(keras.activations.selu)\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.selu(x)\n        x = self.dropout(x)\n        return x\n    \n# create dense & cross model\nclass CrossNet(keras.Model):\n    def __init__(self, hidden_layers, dropout_rate=0):\n        super().__init__()\n        self.dense_layers = [\n            DenseBlock(units)\n            for units in hidden_layers\n        ]\n        self.dense = layers.Dense(units=X.shape[-1])\n        self.concat = layers.Concatenate()\n        self.batchn = layers.BatchNormalization()\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        \n    def call(self, inputs):\n        dense, cross = inputs, inputs\n        for dense_layer in self.dense_layers:\n            #dense\n            dense = dense_layer(dense)\n            #cross\n            cross_current = self.dense(cross)\n            cross = inputs * cross_current + cross\n        cross = self.batchn(cross)\n        merged = self.concat([dense, cross])\n        return self.softmax(merged)","8071cf07":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","29358c3b":"seed = 2021\nset_seed(seed)\n\ncv = StratifiedKFold(n_splits=20, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n\n    scl = RobustScaler()\n    X_train = scl.fit_transform(X_train)\n    X_valid = scl.transform(X_valid)\n    \n    with tf_strategy.scope():\n        model = CrossNet(\n            hidden_layers=[312, 256, 192, 128, 128],\n            dropout_rate=0.1\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.CategoricalCrossentropy(),\n            metrics=['acc']\n        )\n\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_nn[fold] = history.history\n\n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n\n    prediction = model.predict(scl.transform(X_test), batch_size=4096)\n    predictions.append(prediction)\n\n    del model, prediction\n    gc.collect()\n    K.clear_session()\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n    print('_'*65)\n    \noverall_score = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\nprint('_'*65)\nprint(f\"Overall Mean Validation Loss: {np.mean(overall_score)}\")","ed983df1":"plot_eval_results(scores_nn, cv.n_splits)","ed374759":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","d1c4e606":"#create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))\n\nsample_submission['Cover_Type'] = final_predictions\nsample_submission.to_csv('.\/baseline_nn.csv', index=False)\n\nsns.countplot(final_predictions)\nsns.despine()","202b21fa":"sample_submission.head()","c75ddca6":"# Model Setup","a5faf18c":"<img src=\"https:\/\/i.ibb.co\/PWvpT9F\/header.png\" alt=\"header\" border=\"0\" width=800 height=300>","5d9cc60e":"# Training","750df9f9":"# Evaluation & Submission","4ca035a1":"# Introduction","19714035":"# Import & Prepare Data","bb206a1a":"<div style=\"font-size:110%;line-height:155%\">\n<p>Hi,<\/p>\n<p>while I'm still enjoying my deep-learning adventure, reading and learning a ton of stuff - I decided it's time to implement different kinds of networks with this month competition. With this notebook I'm trying a <b>Deep & Cross<\/b> architecture. The idea here is to create polynomial features of increasing degrees with each layer while training a fully-connected neural net in parallel. Both networks will be concatenated in the end to make the final prediction.\n    \n<blockquote><img src=\"https:\/\/i.ibb.co\/37CQmF9\/DNN-CROSS-DEEP.png\" width=\"35%\" alt=\"DNN-CROSS-DEEP\" border=\"0\"><\/blockquote>\n    \n<p>Feel free to take a look at my other notebooks, covering some different ideas and architectures:\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-simple-nn-baseline-keras\">Simple NN Baseline<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-deep-wide-nn-keras\">Deep & Wide NN <\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-bn-autoencoder-nn-keras\">Bottleneck Autoencoder<\/a><\/li>\n    <li><a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-12-denoising-autoencoder-nn-keras\">Deepstack Denoising Autoencoder<\/a><\/li>\n<\/p>\n    \n<em>Thank you very much for taking some time to read my notebook. Please leave an upvote if you find any of this information useful.<\/em>\n<\/div>"}}