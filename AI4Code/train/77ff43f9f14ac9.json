{"cell_type":{"f45803fb":"code","439aa800":"code","9879bbb0":"code","70772d75":"code","63107ebe":"code","1a638f56":"code","1b087b28":"code","c75ea8df":"code","974e3688":"code","fddf4e11":"code","1b9b185a":"code","ab2cb333":"code","89f1d5f3":"code","67c34621":"code","35744451":"code","05ab8e99":"code","9999f5e6":"code","9c88eb7c":"code","88abebc6":"code","9235344c":"code","bb028acd":"code","ddc1d667":"code","f3d29449":"code","0822443b":"code","a006771d":"code","45bb6ff0":"code","ba3c400b":"code","e3337156":"code","78a233cf":"code","4a03092d":"markdown","08ff30f3":"markdown","a776d191":"markdown","7f9c49b3":"markdown","2284e724":"markdown","200893fe":"markdown","afdde29f":"markdown","5bd19105":"markdown","f8eb3c5e":"markdown","c784768d":"markdown","81f5771c":"markdown","cd3bf063":"markdown","71ae6d52":"markdown","6811c0d9":"markdown","1c546130":"markdown","d5e67b11":"markdown","c2c0ba66":"markdown","3804cd76":"markdown","df7bc80f":"markdown","49645d94":"markdown"},"source":{"f45803fb":"import numpy as np \nimport pandas as pd \n\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\") #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\n\nimport warnings\nwarnings.simplefilter(action='ignore')","439aa800":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# preview train data\ntrain_df.head()","9879bbb0":"print(\"Numero de personas en el barco\",train_df.shape[0])\ntrain_df.isnull().sum()","70772d75":"train_data = train_df.copy()\ntrain_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntrain_data[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)","63107ebe":"# comprobamos que no queda ning\u00fan Nulo\ntrain_data.isnull().sum()","1a638f56":"## Create categorical variable for TravelWithMale and TravelWithFemale\ntrain_data['TravelWithParch']= np.where(train_data[\"Parch\"] > 0, 1, 0)\ntrain_data['TravelWithSibSp']= np.where(train_data[\"SibSp\"] > 0, 1, 0)\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)","1b087b28":"train_data['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n#Cambiamos los nombres mal escritos por rare\ntrain_data[\"Title\"].replace([\"Dr\",\"Rev\",\"Col\",\"Mlle\",\"Major\",\"Sir\",\"Mme\",\"Jonkheer\",\"Lady\",\"Don\",\"Ms\",\"Countess\",\"Capt\"], \"Rare\", inplace =True)\n#Creamos un ordinal encoding para esta categoria\ntrain_data[\"Title\"] = train_data[\"Title\"].map({\"Mr\": 1, \"Mrs\" : 2 , \"Miss\" : 3, \"Master\" : 4, \"Rare\" : 5})","c75ea8df":"#create categorical variables and drop some variables\ntraining=pd.get_dummies(train_data, columns=[\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n","974e3688":"final_train = training","fddf4e11":"print(\"Datos \u00fanicos en Pclass: {}\" .format(train_data.Pclass.unique()))\nprint(\"Tipo de datos de Pclass: {}\" .format(train_data.Pclass.dtype))","1b9b185a":"final_train.head()","ab2cb333":"test_data = test_df.copy()\ntest_data[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train_df[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntest_data['TravelWithParch']= np.where(test_data[\"Parch\"] > 0, 1, 0)\ntest_data['TravelWithSibSp']= np.where(test_data[\"SibSp\"] > 0, 1, 0)\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)\ntest_data['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data[\"Title\"].replace([\"Dr\",\"Rev\",\"Col\",\"Mlle\",\"Major\",\"Sir\",\"Mme\",\"Jonkheer\",\"Lady\",\"Don\",\"Ms\",\"Countess\",\"Capt\"], \"Rare\", inplace =True)\ntest_data[\"Title\"] = test_data[\"Title\"].map({\"Mr\": 1, \"Mrs\" : 2 , \"Miss\" : 3, \"Master\" : 4, \"Rare\" : 5})\n\ntesting=pd.get_dummies(test_data, columns=[\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","89f1d5f3":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","67c34621":"plt.figure(figsize=(28,8))\navg_survival_byage = final_train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\ng = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\nplt.show()","35744451":"final_train['IsMinor']=np.where(final_train['Age']<=16, 1, 0)\nfinal_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)\n\nfinal_train['IsRetired']=np.where(final_train['Age']>=65, 1, 0)\nfinal_test['IsRetired']=np.where(final_test['Age']>=65, 1, 0)","05ab8e99":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Fare\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-20,200)\nplt.show()","9999f5e6":"sns.barplot('Pclass', 'Survived', data=train_df, color=\"darkturquoise\")\nplt.show()","9c88eb7c":"sns.barplot('Embarked', 'Survived', data=train_df, color=\"teal\")\nplt.show()","88abebc6":"sns.barplot('TravelWithParch', 'Survived', data=final_train, color=\"mediumturquoise\")\nplt.show()\nsns.barplot('TravelWithSibSp', 'Survived', data=final_train, color=\"pink\")\nplt.show()","9235344c":"sns.barplot('Sex', 'Survived', data=train_df, color=\"aquamarine\")\nplt.show()","bb028acd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\ncols = ['Pclass','Title', 'Age', 'Fare', 'TravelWithParch','TravelWithSibSp', 'Embarked_C', 'Embarked_Q', 'Embarked_S','Sex_male', 'IsMinor', \"IsRetired\"] \nX = final_train[cols]\ny = final_train['Survived']\n# Build a logreg and compute the feature importances\nmodel = LogisticRegression(solver='liblinear')\n# create the RFE model and select 10 attributes\nrfe = RFE(model, 8)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))","ddc1d667":"from sklearn.feature_selection import RFECV\n# Create the RFE object and compute a cross-validated score.\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nrfecv = RFECV(estimator=LogisticRegression(solver='liblinear'), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","f3d29449":"Selected_features = ['Pclass', 'Title', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Sex_male', 'IsMinor', 'IsRetired']\nX = final_train[Selected_features]\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","0822443b":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\n# create X (features) and y (response)\nX = final_train[Selected_features]\ny = final_train['Survived']\n\n# use train\/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","a006771d":"model = keras.Sequential([\n    layers.Dense(units=256,input_shape= [8], activation = \"relu\"),\n    layers.Dropout(rate=0.3),\n    layers.Dense(units=256, activation = \"relu\"),\n    layers.Dropout(rate=0.3),\n    layers.Dense(units=512, activation = \"relu\"),\n    layers.Dropout(rate=0.3),\n    layers.Dense(units=1, activation=\"sigmoid\"),\n])\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"binary_accuracy\"]\n)\n\nearly_stopping = EarlyStopping(\n    min_delta = 0.001,\n    patience = 20,\n    restore_best_weights = True,\n)\n\nhistory = model.fit(X_train, y_train, validation_data=(X_test, y_test), callbacks = [early_stopping], batch_size=256, epochs=200, verbose=2)\n","45bb6ff0":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot(title=\"Accuracy\")","ba3c400b":"final_test['Survived'] = np.where(model.predict(final_test[Selected_features]) >= 0.5, 1, 0)\nfinal_test['PassengerId'] = test_df['PassengerId']\nsubmission = final_test[['PassengerId','Survived']]\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.tail()","e3337156":"# import xgboost as xgb\n# gbm = xgb.XGBClassifier(\n#     #learning_rate = 0.02,\n#  n_estimators= 2000,\n#  max_depth= 4,\n#  min_child_weight= 2,\n#  #gamma=1,\n#  gamma=0.9,                        \n#  subsample=0.8,\n#  colsample_bytree=0.8,\n#  objective= 'binary:logistic',\n#  nthread= -1,\n#  scale_pos_weight=1).fit(X_train, y_train)\n# xgb_predictions = gbm.predict(X_test)\n# print('Accuracy:',accuracy_score(y_test, xgb_predictions))","78a233cf":"# from sklearn.metrics import accuracy_score\n\n# clasifier = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n# clasifier.fit(X_train, y_train)\n# y_pred = clasifier.predict(X_test)\n\n# print(y_pred)\n# print('Accuracy:',accuracy_score(y_test, y_pred))\n\n# final_test['Survived'] = clasifier.predict(final_test[Selected_features])\n# final_test['PassengerId'] = test_df['PassengerId']\n# submission = final_test[['PassengerId','Survived']]\n\n# submission.to_csv(\"submission.csv\", index=False)\n\n# submission.tail()","4a03092d":"<a id=\"t4.1.2.\"><\/a>\n### 4.1.2. Feature ranking with recursive feature elimination and cross-validation\n\nRFECV performs RFE in a cross-validation loop to find the optimal number or the best number of features. Hereafter a recursive feature elimination applied on logistic regression with automatic tuning of the number of features selected with cross-validation.","08ff30f3":"Se puede observar que la mayor\u00eda de gente que murio y sobrevivi\u00f3 se encuentra en la misma zona. Esto puede deberse a que imputamos los nulls con la mediana por lo que es normal que se forme esta especie de campana que no aporta gran informaci\u00f3n. Pero lo que si es interesante es que se observa un pico de supervivientes en los menores de edad ya que tendr\u00edan prioridad a la hora de subir a los barcos. Por eso **introduciremos dos nuevas variables categ\u00f3ricas, Es menor de 16 y es mayor de 65**, ya que nos ayudar\u00e1 a diferenciar estos grupos.","a776d191":"<a id=\"t3.2.\"><\/a>\n## 3.2. Exploration of Fare","7f9c49b3":"**XGB**","2284e724":"<a id=\"t1.\"><\/a>\n# 1. Import Data & Python Packages","200893fe":"Se aprecia una gran diferencia por lo que ser mujer aumentaba enormemente tus probabilidades de sobrevivir.\n<a id=\"t4.\"><\/a>\n# 4. Logistic Regression and Results\n<a id=\"t4.1.\"><\/a>\n## 4.1. Feature selection\n\n<a id=\"t4.1.1.\"><\/a>\n### 4.1.1. Recursive feature elimination\n\nUn estimador externo asigna pesos a las caracter\u00edsticas, recursive feature elimination (RFE) selecciona recursivamente cada vez sets m\u00e1s peque\u00f1os de caracter\u00edsticas. El estimador es entrenado y va eliminando las caracter\u00edsticas menos importantes hasta que el numero de caracter\u00edsticas deseado es alcanzado.\n\nReferences: <br>\nhttp:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html <br>","afdde29f":"<a id=\"t2.\"><\/a>\n# 2. Data Quality & Missing Value Assessment\n<a id=\"t2.1.\"><\/a>\n## 2.1. Missing Values\nUna vez los datos han sido cargados, comenzaremos con su tratamiento. En primer lugar observando el numero de valores ","5bd19105":"En este caso a diferencia del ejemplo vemos que el n\u00famero optipo de caracter\u00edsticas fueron 10.","f8eb3c5e":"Observamos que existen 3 columnas con datos faltantes.\n- **Edad**: En primer lugar la columna de edad contiene 177 datos faltantes, esta columna nos aporta informaci\u00f3n \u00fatil para\n        nuestra predicci\u00f3n por lo que intentaremos realizar alguna t\u00e9cnica para imputar los datos. En este caso\n        sustituiremos los valores faltantes con la mediana, ya que la media nos dejar\u00eda decimales en la edad.\n- **Cabina**: Vemos que la mayoria de los datos de la cabina faltan, por lo que no ser\u00eda preciso intentar rellenar estos datos\n          de alguna manera. Adem\u00e1s esta columna no nos da una gran cantidad de informaci\u00f3n sobre el pasajero, por lo que\n          simplemente no la usaremos para nuestra predicci\u00f3n.\n- **Puerto embarque**: Solamente faltan dos datos en esta columna por lo que los imputaremos con el puerto en el que m\u00e1s\n                   pasajeros embarcaron al ser el m\u00e1s probable.","c784768d":"<a id=\"t2.1.\"><\/a>\n## 2.1. Categorical Values\nA diferencia del cuaderno de guia el cual combina SibSp y Parch, en una columna de viaja acompa\u00f1ado. Yo mantendr\u00e9 un poco mas de informaci\u00f3n y crear\u00e9 dos columnas, viajaConSibSp, viajaConParch.\nCambiaremos el puerto de embarque utilizando la t\u00e9cnica de OneHotEncoding y de la misma manera el sexo. Adem\u00e1s eliminaremos las columnas del nombre, ticket y Id.","81f5771c":"<a id=\"t3.\"><\/a>\n# 3. Exploratory Data Analysis\nAhora realizaremos un analisis EDA de los datos.\n<a id=\"t3.1.\"><\/a>\n## 3.1. Exploration of Age","cd3bf063":"**Introducci\u00f3n**\n\nEste cuaderno esta inspirado en el trabajo de https:\/\/www.kaggle.com\/mnassrib\/titanic-logistic-regression-with-python , el objetivo es, observando el trabajo realizado en dicho notebook poder realizar un trabajo similar pero introduciendo mis cambios con el objetivo de mejorarlo.","71ae6d52":"<a id=\"t3.5.\"><\/a>\n## 3.5. Exploration of Traveling with Female and Male","6811c0d9":"Vemos que ha diferencia de lo esperado viajar en primera clase fue m\u00e1s seguro que en tercera.\n<a id=\"t3.4.\"><\/a>\n## 3.4. Exploration of Embarked Port","1c546130":"****RandomForestRegressor****","d5e67b11":"<a id=\"t4.2.\"><\/a>\n## 4.2. Review of model evaluation procedures\n<a id=\"t4.2.1.\"><\/a>\n### 4.2.1. Model evaluation based on simple train\/test split using `train_test_split()` function","c2c0ba66":"A diferencia del modelo de guia el cual realiza un OneHotEncoding a las clases, en mi caso las dejar\u00e9 sin modificar ya que como observamos a continuaci\u00f3n son de tipo int, por lo que no es necesario, adem\u00e1s creo conveniente dejarlo ya que realmente, si existe m\u00e1s diferencia entre un pasajero de primera clase y uno de tercera, por lo que al estar ordenados as\u00ed esta diferencia se mantiene.","3804cd76":"<a id=\"t3.3.\"><\/a>\n## 3.3. Exploration of Passenger Class","df7bc80f":"Realizaremos los mismos cambios en el conjunto de test","49645d94":"Observamos que viajar acompa\u00f1ado tiene una mayor tasa de supervivencia y que viajar con hombres tiene una peque\u00f1a mayor tasa de supervivencia ya alrededor del 51% sobrevivieron mientras que los que viajaban acompa\u00f1ados de muejeres sobrevivieron alrededor del 46%\n<a id=\"t3.6.\"><\/a>\n## 3.6. Exploration of Gender Variable"}}