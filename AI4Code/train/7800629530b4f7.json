{"cell_type":{"2015535f":"code","ad553705":"code","58497dfe":"code","0ce0167d":"code","f1756024":"code","1e5b7130":"code","57eaf635":"code","45e71b7e":"code","86f8475b":"code","e44b45ab":"code","6f174085":"code","fc84ade4":"code","3e14de84":"code","b91a66f3":"code","a34dcc4e":"code","6ed07c23":"code","11b2b954":"code","e9328245":"code","a93f39ff":"code","973e966d":"code","4ed37367":"code","fc81d441":"code","e4d94b2f":"code","ae175927":"code","6bf2642f":"code","d8c9c8f8":"markdown","386f1cd2":"markdown","62a6aa19":"markdown","0e6a3ff5":"markdown","1ac8c352":"markdown","c8158383":"markdown","2ba48d88":"markdown","e939d52c":"markdown","11f08663":"markdown","3f428d8a":"markdown","c645e747":"markdown","6221252c":"markdown","dffaefae":"markdown","0de36f9e":"markdown","c8edc2cd":"markdown","a9f02c9a":"markdown"},"source":{"2015535f":"!pip install -U git+https:\/\/github.com\/qubvel\/segmentation_models.pytorch","ad553705":"import os\nimport json\nimport nibabel as nib\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport torchvision\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as A # \u0411\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0434\u0430\u043d\u043d\u044b\u0445\nimport segmentation_models_pytorch as smp","58497dfe":"device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c gpu\n\ndevice = torch.device(device)\nprint(device)","0ce0167d":"# from google.colab import drive # \u041c\u0430\u0443\u043d\u0442\u0438\u043c \u0434\u0438\u0441\u043a \n# drive.mount('\/content\/drive')","f1756024":"# core_path = \".\/drive\/MyDrive\/Deep Learning\/CovidKaggleTask\/\" # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u0443\u0442\u0438 \u0434\u043e \u0444\u0430\u0439\u043b\u043e\u0432\n# path = core_path + \"data\/data\/\"","1e5b7130":"# \u0414\u043b\u044f \u043a\u0430\u0433\u0433\u043b\u0430\ncore_path = \"..\/input\/tgcovid\/\"\npath = core_path + \"data\/data\/\"","57eaf635":"from os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(path + \"images\") if isfile(join(path + \"images\", f))]\nonlyfiles[:5] # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u043f\u0438\u0441\u043e\u043a \u0432\u0441\u0435\u0445 \u0444\u0430\u0439\u043b\u043e\u0432","45e71b7e":"class CovidDataset(Dataset):\n    def __init__(self, X_data, without_covid_max=9999999):\n        # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0441\u043a\u0430\u043d\u044b \u043a\u0442\n        path_images = os.path.join(path, 'images')\n        path_labels = os.path.join(path, 'labels')\n        \n        # \u041f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c json \u0441 \u0438\u043d\u0444\u043e\u0439 \u043f\u043e \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0435\n        with open(core_path + 'training_data.json', 'r') as f:\n            dict_training = json.load(f)\n\n        self.X = [] \n        self.Y = []\n        without_covid = 0\n        for entry in tqdm(dict_training):\n            image = nib.load(os.path.join(path_images, entry['image'][:-3])) # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u0439 \u043a\u0442-\u0441\u043a\u0430\u043d \u043f\u043e \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044e \u0438\u0437 json\n            label = nib.load(os.path.join(path_labels, entry['label'][:-3])) # \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043b\u0435\u0439\u0431\u043b\u044b\/\u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0434\u043b\u044f \u043a\u0442-\u0441\u043a\u0430\u043d\u0430\n            image = torch.tensor(image.get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1) # \u041c\u0435\u043d\u044f\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c \u0441 [43, 512, 512]\n            label = torch.tensor(label.get_fdata(), dtype=torch.uint8).transpose(1, 2).transpose(0, 1) # \u043d\u0430 [512, 512, 43] \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a\n            \n            \n            if entry['image'][:-3] in X_data: # \u0415\u0441\u043b\u0438 \u044d\u0442\u043e\u0442 \u043a\u0442-\u0441\u043a\u0430\u043d \u0432 \u0442\u0440\u0435\u0439\u043d\u0435 - \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0435\u0433\u043e \u0442\u0443\u0434\u0430\n                for i in range(len(image)): # \u041f\u0440\u043e\u0431\u0435\u0433\u0430\u0435\u043c\u0441\u044f \u043f\u043e \u0432\u0441\u0435\u043c \u0441\u043b\u043e\u044f\u043c \u0432 \u043d\u0443\u0436\u043d\u043e\u043c \u043a\u0442-\u0441\u043a\u0430\u043d\u0435 image\n                    if label[i].sum() != 0:\n                        self.X.append(image[i]) # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n                        self.Y.append(label[i])\n                    else:\n                        if without_covid >= without_covid_max:\n                            continue\n                        else:\n                            without_covid += 1\n                            self.X.append(image[i]) # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438\n                            self.Y.append(label[i])\n    \n    \n    def __len__(self):\n        return len(self.X)\n    \n    \n    def __getitem__(self, idx):\n        # \u0414\u0435\u043b\u0430\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u0443\u044e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e\n        # \u041c\u0435\u0442\u043e\u0434 \u0434\u0435\u043b\u0430\u0435\u0442 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u043a\u0430\u043a \u0434\u043b\u044f image - \u043d\u0430\u0448\u0435\u0433\u043e \u0441\u043a\u0430\u043d\u0430 \u0441\u043b\u043e\u044f, \u0442\u0430\u043a \u0438 \u0434\u043b\u044f \u0435\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0438\n        # \u0414\u043b\u044f \u043d\u0430\u0447\u0430\u043b\u0430 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u043d\u0430 \u0443\u0433\u043e\u043b...\n        degrees = [-35, -30, -25, -20, -15, -10, -5, 0, 5, 10, 15, 20, 25, 30, 35]\n        X = self.X[idx]\n        y = self.Y[idx]\n        X = X.type(torch.float)\n        y = y.type(torch.float)\n        X = (torch.Tensor(np.array([X.numpy()]) \/ 255))\n        y = (torch.Tensor(np.array([y.numpy()])))\n        value = random.random()\n        if random.random() > 0.5:\n            value = random.random()\n            if value > 0.5:\n                X = torchvision.transforms.functional.vflip(X)\n                y = torchvision.transforms.functional.vflip(y)\n            else:\n                X = torchvision.transforms.functional.hflip(X)\n                y = torchvision.transforms.functional.hflip(y)\n        value = random.random()\n        if value >= 0.1:\n            degree = random.choice(degrees)\n            X = torchvision.transforms.functional.rotate(X, degree)\n            y = torchvision.transforms.functional.rotate(y, degree)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(X)\n            y = torchvision.transforms.RandomPerspective(distortion_scale=0.15, p=0.5, interpolation=2, fill=0)(y)\n        else:\n            pass\n        value = random.random()\n        if value > 0.5:\n            X = torchvision.transforms.GaussianBlur(1)(X)\n            y = torchvision.transforms.GaussianBlur(1)(y)\n        else:\n            pass\n        \n        # \u0412\u0430\u0436\u043d\u043e! \u041d\u0435\u043b\u044c\u0437\u044f \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 (512, 512), \u0442\u0430\u043a \u043a\u0430\u043a \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0441\u0432\u0451\u0440\u0442\u043a\u0430 \u043f\u043e \u043c\u043d\u043e\u0433\u0438\u043c \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u044f\u043c\n        # \u041d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0432 \u0444\u043e\u0440\u043c\u0430\u0442\u0435 [\u043f\u0430\u043b\u0438\u0442\u0440\u0430, \u0448\u0438\u0440\u0438\u043d\u0430, \u0432\u044b\u0441\u043e\u0442\u0430] - [1, 512, 512]\n        return torch.Tensor(X), torch.Tensor(y) \n                                            ","86f8475b":"batch_size = 8\n\n# \u041f\u0435\u0440\u0435\u043c\u0435\u0448\u0430\u0435\u043c \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0444\u0430\u0439\u043b\u043e\u0432 \u0432 \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u043c \u043f\u043e\u0440\u044f\u0434\u043a\u0435 (\u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0442\u0440\u044d\u0439\u043d\u0430 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438)\nnp.random.shuffle(onlyfiles) \ntrain_dataset = CovidDataset(onlyfiles[:33], 50)\nvalid_dataset = CovidDataset(onlyfiles[33:], 50)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)","e44b45ab":"import torch.nn as nn\n\nclass Unet(nn.Module): # \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438 Unet\n    def block_down(self, in_features, out_features):\n        return nn.Sequential(*[nn.Conv2d(in_features, out_features, (3, 3), padding=1),\n                              nn.ReLU(),\n                              nn.BatchNorm2d(out_features)])\n    \n    def block_up(self, in_features, out_features):\n        return nn.Sequential(*[nn.Conv2d(in_features, out_features, (3, 3), padding=1),\n                              nn.ReLU(),\n                              nn.BatchNorm2d(out_features)])\n    \n    \n    def __init__(self):\n        super(Unet, self).__init__()\n        self.block_up11 = self.block_down(1, 32)\n        self.block_up12 = self.block_down(32, 32)\n        self.max_pooling11 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up21 = self.block_down(32, 64)\n        self.block_up22 = self.block_down(64, 64)\n        self.max_pooling22 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up31 = self.block_down(64, 128)\n        self.block_up32 = self.block_down(128, 128)\n        self.max_pooling33 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up41 = self.block_down(128, 256)\n        self.block_up42 = self.block_down(256, 256)\n        self.max_pooling44 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        \n        self.block_up51 = self.block_down(256, 512)\n        self.block_up52 = self.block_down(512, 512)\n        \n        self.block_up61 = nn.Upsample(scale_factor=2)\n        self.block_up62 = self.block_up(512, 256)\n        self.block_up63 = self.block_up(512, 256)\n        self.block_up64 = self.block_up(256, 256)\n        \n        self.block_up71 = nn.Upsample(scale_factor=2)\n        self.block_up72 = self.block_up(256, 128)\n        self.block_up73 = self.block_up(256, 128)\n        self.block_up74 = self.block_up(128, 128)\n        \n        self.block_up81 = nn.Upsample(scale_factor=2)\n        self.block_up82 = self.block_up(128, 64)\n        self.block_up83 = self.block_up(128, 64)\n        self.block_up84 = self.block_up(64, 64)\n        \n        self.block_up91 = nn.Upsample(scale_factor=2)\n        self.block_up92 = self.block_up(64, 32)\n        self.block_up93 = self.block_up(64, 32)\n        self.block_up94 = self.block_up(32, 32)\n        \n        self.block_up100 = self.block_up(32, 1) \n        \n    \n    def forward(self, x):\n        out = self.block_up11(x)\n        out = self.block_up12(out)\n        \n        save1 = out.clone()\n        \n        out = self.max_pooling11(out)\n        \n        out = self.block_up21(out)\n        out = self.block_up22(out)\n        \n        save2 = out.clone()\n        \n        out = self.max_pooling22(out)\n        \n        out = self.block_up31(out)\n        out = self.block_up32(out)\n        \n        save3 = out.clone()\n        \n        out = self.max_pooling33(out)\n        \n        out = self.block_up41(out)\n        out = self.block_up42(out)\n        \n        save4 = out.clone()\n        \n        out = self.max_pooling44(out)\n        \n        out = self.block_up51(out)\n        out = self.block_up52(out)\n        \n        \n        out = self.block_up61(out)\n        out = self.block_up62(out)\n        out = self.block_up63(torch.cat((out, save4), 1))\n        out = self.block_up64(out)\n\n        out = self.block_up71(out)\n        out = self.block_up72(out)\n        out = self.block_up73(torch.cat((out, save3), 1))\n        out = self.block_up74(out)\n\n        out = self.block_up81(out)\n        out = self.block_up82(out)\n        out = self.block_up83(torch.cat((out, save2), 1))\n        out = self.block_up84(out)\n\n        out = self.block_up91(out)\n        out = self.block_up92(out)\n        out = self.block_up93(torch.cat((out, save1), 1))\n        out = self.block_up94(out)\n\n        out = self.block_up100(out)\n        out = nn.Sigmoid()(out)\n        \n        return out","6f174085":"import torch\nimport torch.nn.functional as F\n\ndef dice_loss(inputs: torch.Tensor, targets: torch.Tensor):\n    inp = inputs.contiguous().view(-1)\n    tar = targets.contiguous().view(-1)\n    noise = random.randint(1, 1000) \/ 10000000000\n    \n    return 1 - ((2 * (inp * tar).sum() + noise) \/ ((inp).sum() + (tar).sum() + noise))","fc84ade4":"class TverskyLoss(nn.Module):\n    def __init__(self, alpha=0.7):\n        super(TverskyLoss, self).__init__()\n        self.alpha = alpha\n\n    def forward(self, inputs, targets, smooth=1):\n        y_pred = inputs\n        y_true = targets\n        y_true_pos = y_true.view(-1)\n        y_pred_pos = y_pred.view(-1)\n        true_pos = torch.sum(y_true_pos * y_pred_pos)\n        false_neg = torch.sum(y_true_pos * (1 - y_pred_pos))\n        false_pos = torch.sum((1 - y_true_pos) * y_pred_pos)\n        return 1 - (true_pos + smooth) \/ (true_pos + self.alpha * false_neg + (1 - self.alpha) * false_pos + smooth)","3e14de84":"def sigmoid_focal_loss(inputs, targets, alpha=0.25, gamma=2, reduction=\"none\"):\n    p = torch.sigmoid(inputs)\n    ce_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n    p_t = p * targets + (1 - p) * (1 - targets)\n    loss = ce_loss * ((1 - p_t) ** gamma)\n\n    if alpha >= 0:\n        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n        loss = alpha_t * loss\n\n    if reduction == \"mean\":\n        loss = loss.mean()\n    elif reduction == \"sum\":\n        loss = loss.sum()\n\n    return loss","b91a66f3":"import segmentation_models_pytorch as smp\n\nuse_previous_versions = False\nprevious_i = 0\npath_to_model = \"output\/kaggle\/working\/\"\n\n# \u041f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c, \u0432\u0434\u0440\u0443\u0433 \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u043f\u043e\u0434\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u0443\u0436\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nif use_previous_versions:\n    models_variation = []\n    for put, papki, files in os.walk(\".\"):\n        for el in files:\n            if \"lungs_ct_model\" in el:\n                models_variation.append(el)\n                \n    # \u041d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0444\u0430\u0439\u043b\u0430 - lungs_ct_model_1.h5\n    if len(models_variation) != 0:\n        models_variation = sorted(models_variation, key=lambda x: - int(x.split(\"_\")[-1].split(\".\")[0]))\n        model = torch.load(models_variation[-1])\n        previous_i = int(models_variation[0].split(\"_\")[-1].split(\".\")[0])\n        print(\"\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u0430 \u043f\u0440\u043e\u0448\u043b\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c: {}\".format(str(previous_i)))\n    else:\n        model = smp.UnetPlusPlus(encoder_name='resnet18', in_channels=1, classes=1, activation=\"tanh\")\n        print(\"\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d \u043d\u0435\u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 Unet++\")\nelse:\n    model = smp.UnetPlusPlus(encoder_name='resnet18', in_channels=1, classes=1, activation=\"sigmoid\")\n    print(\"\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d \u043d\u0435\u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0439 Unet++\")\n\n\ndevice = torch.device('cuda:0')\nmodel = model.to(device)","a34dcc4e":"def recall(output_batch, correct_batch, threshold=0.99):    \n    output_numpy = output_batch.detach().numpy()\n    correct_numpy = correct_batch.detach().numpy()\n    \n    amount_of_correct = np.count_nonzero(np.where(output_numpy > threshold, output_numpy, 0) + correct_numpy == 2)\n    amount_all = np.count_nonzero(correct_numpy == 1)\n    try:\n        return amount_of_correct \/ amount_all\n    except:\n        return np.nan\n\ndef precision(output_batch, correct_batch, threshold=0.99):   \n    output_numpy = output_batch.detach().numpy()\n    correct_numpy = correct_batch.detach().numpy()\n    \n    amount_of_correct = np.count_nonzero(np.where(output_numpy > threshold, output_numpy, 0) + correct_numpy == 2)\n    amount_all = np.count_nonzero(output_numpy == 1)\n    try:\n        return amount_of_correct \/ amount_all\n    except:\n        return np.nan\n    \ndef f1_score(precision, recall):\n    return 2 * (precision * recall) \/ (precision + recall)","6ed07c23":"num_epoch = 125\nlr = 0.0005\n\ndice_loss_criterion = dice_loss\nfocal_loss_criterion = sigmoid_focal_loss\ntverskoy_loss = TverskyLoss(alpha=0.7)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lambda x: 0.9825)","11b2b954":"losses = []\n\nfor epoch in tqdm(range(num_epoch)):\n    epoch_losses = []\n    \n    # \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\n    for X, Y in train_loader:\n        X = X.to(device)\n        Y = Y.to(device)\n\n        optimizer.zero_grad()\n        output = model(X)\n        \n        loss = tverskoy_loss(output, Y)\n        loss.backward()\n        clip_grad_norm_(model.parameters(), 99999)\n        \n        optimizer.step()\n\n        del X\n        del Y\n        torch.cuda.empty_cache()\n        epoch_losses.append(loss.item())\n        \n    # \u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043b\u043e\u0441\u0441\n    common_loss = sum(epoch_losses) \/ len(epoch_losses)\n    losses.append(common_loss)\n    \n    # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438:\n    valid_precision = []\n    valid_recall = []\n    for X, Y in valid_loader:\n        X = X.to(device)\n        rec = recall(model(X).cpu(), Y)\n        prec = precision(model(X).cpu(), Y)\n        if prec is not np.nan:\n            valid_precision.append(prec)\n        if rec is not np.nan:\n            valid_recall.append(rec)\n        del X\n        del Y\n    \n    # \u0412\u044b\u0432\u043e\u0434\u0438\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e\n    print(\"--\" * 15)\n    print(\"Epoch: {}\".format(str(epoch)))\n    print(\"Loss:\\t\\t {:7.5f}\".format(common_loss))\n    print(\"Learning rate: {:9.8f}\".format(float(optimizer.state_dict()[\"param_groups\"][0][\"lr\"])))\n    \n    try:\n        prec = sum(valid_precision) \/ len(valid_precision)\n        print(\"Precision:\\t {:7.3%}\".format(prec))\n    except:\n        print(\"Precision:\\t No info\")\n        \n    try:\n        rec = sum(valid_recall) \/ len(valid_recall)\n        print(\"Recall:\\t\\t {:7.3%}\".format(rec))\n    except:\n        print(\"Recall:\\t\\t No info\")\n        \n    try:\n        print(\"F1-score: \\t {:7.3f}\".format(f1_score(prec, rec)))\n    except:\n        print(\"F1-score: No info\".format(f1_score(prec, rec)))\n    \n    # \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0434\u0435\u043b\u0430\u0435\u043c \u0448\u0430\u0433 scheduler\n    torch.save(model, \"lungs_ct_model_\" + str(epoch + previous_i) + \".h5\")\n    scheduler.step()","e9328245":"from sklearn.metrics import roc_curve\n\ny_real = []\ny_pred = []\nfor X, Y in valid_loader:\n    X = X.to(device)\n    y_real += Y.view(Y.shape[0] * Y.shape[1] * Y.shape[2] * Y.shape[3]).int().cpu().detach().tolist()\n    y_pred += model(X).view(Y.shape[0] * Y.shape[1] * Y.shape[2] * Y.shape[3]).cpu().detach().tolist()","a93f39ff":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 12))\nfpr, tpr, thresholds = roc_curve(np.array(y_real), np.array(y_pred))\nplt.plot(fpr, tpr, 'b', linewidth=3)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0, 0], [0, 1], 'k')\nplt.plot([1, 1], [0, 1], 'k')\nplt.plot([0, 1], [0, 0], 'k')\nplt.plot([0, 1], [1, 1], 'k')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.xlim((0, 1))\nplt.ylim((0, 1))\nplt.axis('equal')\nplt.title('ROC curve')\nplt.show()","973e966d":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_real, y_pred)","4ed37367":"#Visualize some of the slices\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef blend(image, mask): # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0441 \u043d\u0430\u043b\u043e\u0436\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 \u043d\u0435\u0451 \u043c\u0430\u0441\u043a\u043e\u0439 - label, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043a\u043e\u0432\u0438\u0434\u043d\u043e\u0439 \u0448\u0442\u0443\u043a\u043e\u0439\n    print(image)\n    image = image.astype(np.float32)\n    min_in = image.min()\n    max_in = image.max()\n    image = (image - min_in) \/ (max_in - min_in + 1e-8) * 255\n    image = np.dstack((image, image, image)).astype(np.uint8)\n    zeros = np.zeros_like(mask)\n    mask = np.dstack((zeros, zeros, mask * 255)).astype(np.uint8)\n    return Image.blend(\n        Image.fromarray(image),\n        Image.fromarray(mask),\n        alpha=.3\n    )\n\nslices_num = (100, )\nslices = []\nfor idx in slices_num:\n    k = valid_dataset[idx]\n    slices.append(blend(\n        k[0][0].numpy(),\n        k[1][0].numpy()\n    ))\n    prediction = model.forward(k[0].view(1, 1, 512, 512).to(device)).cpu().detach().transpose(0, 1).transpose(1, 2).transpose(2, 3)[0]\n    prediction[prediction >= 0.99] = 1\n    prediction[prediction < 0.99] = 0\n    slices.append(\n        torch.cat([prediction, prediction, prediction], 2)\n    )\n\nfigure = plt.figure(figsize=(18, 18))\nfor i, image in enumerate(slices):\n    ax = figure.add_subplot(1, len(slices), i + 1)\n    ax.imshow(slices[i])","fc81d441":"\"\"\"\nLoad testing data into images and labels lists\n\nimages list consists of CT scans -  numpy arrays of shape (512, 512, n_slices)\n\"\"\"\nwith open(core_path + 'testing_data.json', 'r') as f:\n    dict_testing = json.load(f)\n\nimages_testing = []\nlabel_testing = []\nfor entry in tqdm(dict_testing):\n    image = nib.load(os.path.join(path + \"images\/\", entry['image'][:-3]))\n    images_testing.append(image.get_fdata())","e4d94b2f":"def rle_encoding(x):\n    dots = np.where(x.T.flatten() >= 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b > prev + 1):\n            run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return [str(item) for item in run_lengths]","ae175927":"thresh = 0.99\nn_id = len(images_testing)\n\nfor i in range(n_id):\n    n_imgs = images_testing[i][1].shape[-1]\n    name = images_testing[i][0]\n    \n    for j in range(n_imgs):\n        inp = images_testing[i][:, :, j]\n\n        inp = torch.from_numpy(np.array(inp))\n        inp = inp.to(device)\n        inp = inp.type(torch.float)\n        inp = inp.view(-1, 1, 512, 512)\n\n        res = model(inp)\n        res = res.cpu()\n        outp = res.detach().numpy()[0][0]\n\n        outp \/= np.max(outp)\n        outp[outp >= thresh] = 1\n        outp[outp < thresh] = 0\n\n        label_testing.append(outp)","6bf2642f":"import csv\nwith open(f'{core_path}testing_data.json', 'r') as f:\n            dict_testing = json.load(f)\n\nwith open(f'submission.csv', \"wt\") as sb:\n    submission_writer = csv.writer(sb, delimiter=',')\n    submission_writer.writerow([\"Id\", \"Predicted\"])\n    for k_i, patient_i in tqdm(zip(dict_testing, label_testing)):\n        submission_writer.writerow([\n                f\"{k_i['image'][:-7]}\",\n                \" \".join(rle_encoding(patient_i))\n            ])","d8c9c8f8":"#### \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438\n\n\u0421\u043d\u0430\u0447\u0430\u043b\u0430 \u043f\u043e\u0434\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c **Unet++**, \u0435\u0441\u043b\u0438 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0434\u0440\u0443\u0433\u0438\u0445 **\u0443\u0436\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445** \u043c\u043e\u0434\u0435\u043b\u0435\u0439","386f1cd2":"\u041c\u044b \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043b\u0438 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c **Unet**. \u041e\u043d\u0430 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u0430 \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u0430, \u043e\u0434\u043d\u0430\u043a\u043e \u0432 \u0441\u043b\u043e\u0436\u043d\u044b\u0445 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u0435\u0451 \u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u0445\u0432\u0430\u0442\u0430\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043a \u043d\u0435\u0439 \u043d\u0430\u0432\u0435\u0448\u0438\u0432\u0430\u044e\u0442 \u0445\u043e\u0440\u043e\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c \u0434\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 (\u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440, ResNet).\n\n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u0441\u043b\u043e\u0436\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u0431\u0443\u0434\u0435\u0442 **Unet++**","62a6aa19":"### 3. \u041e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","0e6a3ff5":"\u0420\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u0440\u0438 \u043b\u043e\u0441\u0441\u0430 \u0438 \u0432\u044b\u0431\u0435\u0440\u0435\u043c, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u043e\u043b\u0435\u0435 **\u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0439**:\n- Dice Loss\n- TverskyLoss\n- Focal loss","1ac8c352":"\u0423\u0436\u0435 \u0438\u0437\u0440\u044f\u0434\u043d\u043e \u043e\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u0432 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043b\u043e\u0441\u0441\u044b, \u043b\u0443\u0447\u0448\u0438\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0439 \u043c\u043e\u043c\u0435\u043d\u0442 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f **TverskyLoss**\n\n\u0415\u0433\u043e \u0438 \u0431\u0443\u0434\u0435\u043c \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0438\u0442\u043e\u0433\u043e\u0432\u043e\u0433\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f","c8158383":"## \u0421\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n\n\u041f\u0435\u0440\u0435\u0434 \u043d\u0430\u043c\u0438 \u0441\u0442\u043e\u0438\u0442 \u0437\u0430\u0434\u0430\u0447\u0430 **\u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438** \u043a\u0442-\u0441\u043d\u0438\u043c\u043a\u043e\u0432 \u043b\u0435\u0433\u043a\u0438\u0445 \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043e\u0431\u043b\u0430\u0441\u0442\u0435\u0439 \u043f\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043e\u0442 Covid-19. \n\n\u0418\u0441\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435: **json** \u0441 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f\u043c\u0438 \u0444\u0430\u0439\u043b\u043e\u0432,\n**images** - \u0441\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0435, **labels** - \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0430: 0 - \u044d\u0442\u043e\u0442 \u043f\u0438\u043a\u0441\u0435\u043b\u044c **\u041d\u0415 \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f** \u043a \u043f\u043e\u0432\u0440\u0435\u0436\u0434\u0435\u043d\u043d\u043e\u043c\u0443 \u043b\u0451\u0433\u043a\u043e\u043c\u0443, 1 - \u043f\u0438\u043a\u0441\u0435\u043b\u044c **\u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f** \u043a \u043f\u043e\u0432\u0440\u0435\u0436\u0434\u0451\u043d\u043d\u043e\u043c\u0443 \u043b\u0451\u0433\u043a\u043e\u043c\u0443\n\n\u041a\u0430\u0436\u0434\u044b\u0439 \u0441\u043a\u0430\u043d - numpy arrays of shape (512, 512, n_slices)\n\n","2ba48d88":"## 0. \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0438\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","e939d52c":"### 5. Inference\n\n\u041d\u0438\u0436\u0435 \u043a\u043e\u0434 \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u043e\u0442\u043f\u0440\u0430\u0432\u043a\u0438 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043d\u0430 **Kaggle**","11f08663":"\u0421\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430 \u043f\u0440\u043e\u0435\u043a\u0442\u0430:\n1. core_path - \u043e\u0431\u0449\u0438\u0439 \u043f\u0443\u0442\u044c \u0434\u043e \u043f\u0440\u043e\u0435\u043a\u0442\u0430. \u0412 \u043a\u043e\u043b\u043b\u0430\u0431\u0435 \u044d\u0442\u043e \".\/drive\/MyDrive\/Deep Learning\/CovidKaggleTask\/\"\n\n2. path = core_path + \"data\/data\/\" - \u041f\u0443\u0442\u044c \u0434\u043e \u0441\u0430\u043c\u0438\u0445 \u0444\u0430\u0439\u043b\u043e\u0432. \u0412 \u044d\u0442\u043e\u0439 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438 \u0435\u0441\u0442\u044c \u0441\u0430\u043c json \u0444\u0430\u0439\u043b, \u043f\u0430\u043f\u043a\u0438 images \u0438 labels, \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0435\u0441\u0442\u044c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u043a\u0442-\u0441\u043a\u0430\u043d\u0430\u0445\n\n3. core_path\/models - \u043f\u0443\u0442\u044c \u0434\u043e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f. \u0412 \u0441\u0438\u043b\u0443 \u0431\u043e\u043b\u044c\u0448\u0438\u0445 \u043f\u043e \u043e\u0431\u044a\u0451\u043c\u0443 \u0434\u0430\u043d\u043d\u044b\u0445, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0440\u0430\u0437 \u043c\u043e\u0434\u0435\u043b\u044c. \u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0432 \u044d\u0442\u043e\u0439 \u043f\u0430\u043f\u043a\u0435 \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u044e\u0442\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0439 \u044d\u043f\u043e\u0445\u0438. lungs_ct_model_1.h5 - \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u044b\u043b\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0430 \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0430 \u043e\u0434\u043d\u043e\u0439 \u044d\u043f\u043e\u0445\u0435","3f428d8a":"\u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043a\u043b\u0430\u0441\u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043f\u0440\u043e\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043c\u0435\u0442\u043e\u0434 __init__(self, \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u0441\u043a\u0430\u043d\u043e\u0432 \u0434\u043b\u044f \u0442\u0440\u0435\u0439\u043d\u0430, \u0441\u043f\u0438\u0441\u043e\u043a \u0438\u0437 \u0441\u043a\u0430\u043d\u043e\u0432 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438, \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438), \u043c\u0435\u0442\u043e\u0434 \u0434\u043b\u0438\u043d\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430 \u0438\u0437 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","c645e747":"## 2. \u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c\u0441\u044f \u0441\u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\n\n\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435, \u0431\u044b\u043b\u0430 \u0432\u044b\u0431\u0440\u0430\u043d\u0430 \u0441\u0435\u0442\u044c Unet (https:\/\/pytorch.org\/hub\/mateuszbuda_brain-segmentation-pytorch_unet\/)\n\n\u041e\u043d\u0430 \u043e\u0442\u043b\u0438\u0447\u043d\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0434\u043b\u044f \u0430\u043d\u0430\u043b\u0438\u0437\u0430 \u043c\u0435\u0434\u0438\u0446\u0438\u043d\u0441\u043a\u0438\u0445 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439, \u0442\u0430\u043a \u043a\u0430\u043a \u0432 \u043d\u0435\u0439 \u0435\u0441\u0442\u044c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u043e skip-connection \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0437\u0430\u0442\u0443\u0445\u0430\u043d\u0438\u044f \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 (vanishing gradient), \u0430 \u0442\u0430\u043a\u0436\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438 \u0434\u043b\u044f \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0445 \u0441\u043b\u043e\u0451\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0442\u0432\u0435\u0447\u0430\u044e\u0442 \u0437\u0430 \u0432\u044b\u0431\u043e\u0440 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 (\u0442. \u0435. decoder)","6221252c":"### 4. \u0412\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f","dffaefae":"#### \u041c\u0435\u0442\u0440\u0438\u043a\u0438\n\n\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c\u0441\u044f \u0441 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\u0438:\n\n\u041d\u0430\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u043e \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u0432\u0438\u0434, \u0430 \u0437\u043d\u0430\u0447\u0438\u0442, \u043d\u0430\u043c \u043f\u043e\u0434\u043e\u0439\u0434\u0451\u0442 \u043f\u043e\u043b\u043d\u043e\u0442\u0430 **recall** (\u0435\u0441\u043b\u0438 \u043c\u044b \u0431\u0443\u0434\u0435\u043c \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043a\u043e\u0432\u0438\u0434, \u0433\u0434\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0441\u0442\u044c \u043f\u043e\u0434\u043e\u0437\u0440\u0435\u043d\u0438\u044f - \u0431\u0443\u0434\u0435\u0442 \u0445\u043e\u0440\u043e\u0448\u043e)\n\n\u041e\u0434\u043d\u0430\u043a\u043e \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0451 \u043f\u043e\u0434\u0440\u044f\u0434 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c, \u0430 \u0437\u043d\u0430\u0447\u0438\u0442, \u043d\u0443\u0436\u043d\u043e \u0438 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 **precision**\n\n\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, **f-\u043c\u0435\u0440\u0430** \u043f\u0440\u0435\u043a\u0440\u0430\u0441\u043d\u043e \u043f\u043e\u0434\u043e\u0439\u0434\u0451\u0442","0de36f9e":"\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c\u0441\u044f \u0441 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0435\u0439 \u0434\u0430\u043d\u043d\u044b\u0445. \u041f\u044b\u0442\u0430\u0435\u043c\u0441\u044f \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u043c\u0438 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438:\n- **\u041f\u043e\u0432\u043e\u0440\u043e\u0442\u044b \u043d\u0430 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u0440\u0430\u0434\u0443\u0441\u043e\u0432** (\u043f\u043e\u0432\u043e\u0440\u043e\u0442 \u043d\u0438\u043a\u0430\u043a \u043d\u0435 \u043c\u0435\u043d\u044f\u0435\u0442 \u043b\u043e\u0433\u0438\u043a\u0443 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\n- **\u0412\u0435\u0440\u0442\u0438\u043a\u0430\u043b\u044c\u043d\u043e\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435**\n- **\u0413\u043e\u0440\u0438\u0437\u043e\u043d\u0442\u0430\u043b\u044c\u043d\u043e\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435**\n- **\u041f\u0435\u0440\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u0430** (\u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0438\u043d\u043e\u0439 \u0443\u0433\u043e\u043b \u0441\u043d\u044f\u0442\u0438\u044f \u043a\u0442 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442 \u043d\u0430\u0431\u043e\u0440 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445)\n- **\u0411\u043b\u044e\u0440** (\u0448\u0443\u043c \u043f\u043e\u043c\u043e\u0436\u0435\u0442 \u043d\u0435 \u043f\u0435\u0440\u0435\u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f \u043c\u043e\u0434\u0435\u043b\u0438)","c8edc2cd":"## 1. \u0420\u0430\u0431\u043e\u0442\u0430 \u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u043c","a9f02c9a":"\u0422\u0430\u043a\u0438\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c, \u043c\u044b \u043e\u0431\u0443\u0447\u0438\u043b\u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u043e\u043d\u0430 \u0434\u043e\u0441\u0442\u0438\u0433\u043b\u0430 \u043f\u0440\u0438\u0435\u043c\u043b\u0435\u043c\u043e\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430"}}