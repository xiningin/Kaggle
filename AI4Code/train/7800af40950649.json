{"cell_type":{"0498c203":"code","addb3a68":"code","e6dcb782":"code","e1773aed":"code","bcaa6b0b":"code","f612af62":"code","0c07f9bd":"code","f33bdbc1":"code","a0a69d71":"code","204621ab":"code","d1d5befe":"code","90707f2c":"code","5a14af37":"code","59b1225c":"markdown","559cc6c2":"markdown","a36692c4":"markdown","010376ae":"markdown"},"source":{"0498c203":"# Set environment variables\nimport os\nimport time\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nVERSION = 1\nINPUT_PATH = f\"\/kaggle\/input\/m5-forecasting-accuracy-sales-basic-features\"\nBASE_PATH = f\"\/kaggle\/working\/m5-forecasting-accuracy-ver{VERSION}\"","addb3a68":"# Turn off warnings\n\nwarnings.filterwarnings(\"ignore\")","e6dcb782":"# Change directory\n\nos.chdir(INPUT_PATH)\nprint(f\"Change to directory: {os.getcwd()}\")","e1773aed":"# Memory usage function\n\ndef format_memory_usage(total_bytes):\n    unit_list = [\"\", \"Ki\", \"Mi\", \"Gi\"]\n    for unit in unit_list:\n        if total_bytes < 1024:\n            return f\"{total_bytes:.2f}{unit}B\"\n        total_bytes \/= 1024\n    return f\"{total_bytes:.2f}{unit}B\"","bcaa6b0b":"# Set global variables\n\ndays_to_predict = 28\nrolling_days = [60, 90, 180, 365]","f612af62":"# Load dataset from our previous work\n\ndf_rolling_features = pd.read_pickle(\"m5-forecasting-accuracy-ver1\/sales_basic_features.pkl\")\ndf_rolling_features.head(10)","0c07f9bd":"# Get necessary columns only\n\ndf_rolling_features = df_rolling_features[[\"id\", \"d\", \"sales\"]]\ndf_rolling_features.head(10)","f33bdbc1":"# Create features\n# Generate rolling lag features and control the memory usage\n\ndf_rolling_grouped = df_rolling_features.groupby([\"id\"])[\"sales\"]\n\nfor day in rolling_days:\n\n    start_time = time.time()\n    print(f\"Rolling {str(day)} Start.\")\n\n    df_rolling_features[f\"rolling_{str(day)}_max\"] = df_rolling_grouped.transform(lambda x: x.shift(days_to_predict).rolling(day).max()).astype(np.float16)\n    df_rolling_features[f\"rolling_{str(day)}_min\"] = df_rolling_grouped.transform(lambda x: x.shift(days_to_predict).rolling(day).min()).astype(np.float16)\n    df_rolling_features[f\"rolling_{str(day)}_median\"] = df_rolling_grouped.transform(lambda x: x.shift(days_to_predict).rolling(day).median()).astype(np.float16)\n    df_rolling_features[f\"rolling_{str(day)}_mean\"] = df_rolling_grouped.transform(lambda x: x.shift(days_to_predict).rolling(day).mean()).astype(np.float16)\n    df_rolling_features[f\"rolling_{str(day)}_std\"] = df_rolling_grouped.transform(lambda x: x.shift(days_to_predict).rolling(day).std()).astype(np.float16)\n\n    end_time = time.time()\n    print(f\"Calculation time: {round(end_time - start_time)} seconds\")","a0a69d71":"# Check dataset\n\ndf_rolling_features.head(120)","204621ab":"# Check data type\n\ndf_rolling_features.info()","d1d5befe":"# Check current memory usage\n\nmemory_usage_string = format_memory_usage(df_rolling_features.memory_usage().sum())\nprint(f\"Current memory usage: {memory_usage_string}\")","90707f2c":"# Change to output path\n\ntry:\n    os.chdir(BASE_PATH)\n    print(f\"Change to directory: {os.getcwd()}\")\nexcept:\n    os.mkdir(BASE_PATH)\n    os.chdir(BASE_PATH)\n    print(f\"Create and change to directory: {os.getcwd()}\")","5a14af37":"# Save pickle file\n\ndf_rolling_features.to_pickle(\"sales_rolling_features.pkl\")","59b1225c":"- For each longer period, we calculate some descriptive statistics of its distribution to capture the characteristics.\n- For example, for sales information in past 60 days, instead of shifting \"sales\" column 60 times,\n- we can create features like mean or median by a moving window with 1 step each time,\n- which stores the crucial information efficiently, and condense 60 columns into one.\n- And remember that 28 days shift is to ensure that every prediction row contains those features.","559cc6c2":"# Feature Engineering - Sales - Rolling Lag Features\n- From day 1 to 28, we have created lag features to contain raw information.\n- Because of memory constraint, it is not possible to keep creating those features until day 1941,\n- and even if we have enough spaces, it is still not the most efficient way to utilize the memory.","a36692c4":"# Note\n- Some more ways to make features better:\n- Now, for sales within past 28 days, we collect all raw data as features in previous notebook,\n- but we don't know if raw data will perform better than descriptive statistics.\n- Also, keep shifting 28 days is making the training data have a lot of NaN, especially in longer periods.\n- So, 1) Shifting within 28 days, which will increase the effective training data, but let features in prediction row fewer.\n- 2) Calculating descriptive statistics in shorter periods, such as 7, 14, 21, 30.\n- 3) We have also canceled the calculation of skewness and kurtosis due to memory limit, and they are worth trying actually.","010376ae":"# About this notebook\n- To continue the creation of lag features, we definitely want to have more information from the past sales,\n- but it is just not possible and not efficiently to create features by shifting \"sales\" column until day 1941,\n- so we must have a better way to get the information we want with reasonable calculation and memory usage.\n- Basic features: https:\/\/www.kaggle.com\/kaiweihuang\/m5-forecasting-accuracy-sales-basic-features\n- Lag features: https:\/\/www.kaggle.com\/kaiweihuang\/m5-forecasting-accuracy-sales-lag-features"}}