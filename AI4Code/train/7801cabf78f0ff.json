{"cell_type":{"f2940507":"code","0aa26ce7":"code","0276cc04":"code","0f2cfab7":"code","b83c1006":"code","a488dffc":"code","1252c614":"code","f416537d":"code","280e963c":"code","613c253a":"code","5e02a3dd":"code","5be8c86f":"code","5729851d":"code","b4f5294a":"code","c7faad0a":"code","df9c40b5":"markdown","8dc45ad2":"markdown","1a8f48cf":"markdown","c4a99a5e":"markdown","290e8d9d":"markdown","74edcdcb":"markdown"},"source":{"f2940507":"import numpy as np \nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0aa26ce7":"!pip install py7zr","0276cc04":"import py7zr\nfrom subprocess import check_output\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        archive = py7zr.SevenZipFile(os.path.join(dirname, filename), mode='r')\n        archive.extractall(path=\"\/kaggle\/working\")\n        archive.close()\n\nprint(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","0f2cfab7":"#train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/working\/test.csv\")\nstores = pd.read_csv(\"..\/working\/stores.csv\")\nitems = pd.read_csv(\"..\/working\/items.csv\")\ntrans = pd.read_csv(\"..\/working\/transactions.csv\")\noil = pd.read_csv(\"..\/working\/oil.csv\")\nholiday = pd.read_csv(\"..\/working\/holidays_events.csv\")\nprint(\"done\")","b83c1006":"#check memory use for the two biggest files - train and test\n#mem_train = train.memory_usage(index=True).sum()\nmem_test=test.memory_usage(index=True).sum()\n#print(\"train dataset uses \",mem_train\/ 1024**2,\" MB\")\nprint(\"test dataset uses \",mem_test\/ 1024**2,\" MB\")\n\ntest.head()","a488dffc":"# optimize test.csv\n# First check the contents of train.csv\nprint(test.max())\nprint(test.min())\n#check datatypes\nprint(test.dtypes)","1252c614":"#There are only 54 stores\ntest['store_nbr'] = test['store_nbr'].astype(np.uint8)\n\n# The ID column is a continuous number from 1 to 128867502 in train and 128867503 to 125497040 in test\ntest['id'] = test['id'].astype(np.uint32)\n\n# item number is unsigned \ntest['item_nbr'] = test['item_nbr'].astype(np.uint32)\n\n#Converting the date column to date format\ntest['date']=pd.to_datetime(test['date'],format=\"%Y-%m-%d\")\n\n#check memory\nprint(test.memory_usage(index=True))\nnew_mem_test=test.memory_usage(index=True).sum()\nprint(\"test dataset uses \",new_mem_test\/ 1024**2,\" MB after changes\")\nprint(\"memory saved =\",(mem_test-new_mem_test)\/ 1024**2,\" MB\")","f416537d":"print(test.memory_usage())\n\n#check range of float 16\nmin_value = np.finfo(np.float16).min\nmax_value = np.finfo(np.float16).max\nprint(\"range of float16 is\",min_value,max_value)","280e963c":"dtype_dict={\"id\":np.uint32,\n            \"store_nbr\":np.uint8,\n            \"item_nbr\":np.uint32,\n            \"unit_sales\":np.float32\n           }\n\ntrain_part1 = pd.read_csv(\"..\/working\/train.csv\",dtype=dtype_dict,usecols=[0,2,3,4])\nprint(train_part1.dtypes)","613c253a":"train_part2=pd.read_csv(\"..\/working\/train.csv\",dtype=dtype_dict,usecols=[1,5],parse_dates=[0])\ntrain_part2['Year'] = pd.DatetimeIndex(train_part2['date']).year\ntrain_part2['Month'] = pd.DatetimeIndex(train_part2['date']).month\ntrain_part2['Day'] =pd.DatetimeIndex(train_part2['date']).day.astype(np.uint8)\ndel(train_part2['date'])\ntrain_part2['Day']=train_part2['Day'].astype(np.uint8)\ntrain_part2['Month']=train_part2['Month'].astype(np.uint8)\ntrain_part2['Year']=train_part2['Year'].astype(np.uint16)\n\n#impute the missing values to be -1\ntrain_part2[\"onpromotion\"].fillna(0, inplace=True)\ntrain_part2[\"onpromotion\"]=train_part2[\"onpromotion\"].astype(np.int8)\nprint(train_part2.head())\nprint(train_part2.dtypes)","5e02a3dd":"# joining part one and two\n# For people familiar with R , the equivalent of cbind in pandas is the following command\ntrain = pd.concat([train_part1.reset_index(drop=True), train_part2], axis=1)\n#drop temp files\ndel(train_part1)\ndel(train_part2)\n#Further Id is just an indicator column, hence not required for analysis\nid=train['id']\ndel(train['id'])\n# check memory\nprint(train.memory_usage())\n#The extracted train.csv file is approx 5 GB\nmem_train=5*1024**3\nnew_mem_train=train.memory_usage().sum()\nprint(\"Train dataset uses \",new_mem_train\/ 1024**2,\" MB after changes\")\nprint(\"memory saved is approx\",(mem_train-new_mem_train)\/ 1024**2,\" MB\")","5be8c86f":"sale_day_store_level=train.groupby(['Year','Month','Day','store_nbr'])['unit_sales'].sum()\nsale_day_item_level=train.groupby(['Year','Month','Day','item_nbr'])['unit_sales'].sum()","5729851d":"def aggregate_level1(df):\n    #day-store level\n    sale_day_store_level=df.groupby(['Year','Month','Day','store_nbr'],as_index=False)['unit_sales'].agg(['sum','count'])\n    #drop index and rename\n    sale_day_store_level=sale_day_store_level.reset_index().rename(columns={'sum':'store_sales','count':'item_variety'})\n    \n    #day-item level  \n    sale_day_item_level=df.groupby(['Year','Month','Day','item_nbr'],as_index=False)['unit_sales'].agg(['sum','count'])\n    sale_day_item_level=sale_day_item_level.reset_index().rename(columns={'sum':'item_sales','count':'store_spread'})\n    \n    #store item level   \n    sale_store_item_level=df.groupby(['Year','store_nbr','item_nbr'],as_index=False)['unit_sales'].agg(['sum','count'])\n    sale_store_item_level=sale_store_item_level.reset_index().rename(columns={'sum':'item_sales','count':'entries'})\n\n    return sale_day_store_level,sale_day_item_level,sale_store_item_level","b4f5294a":"import time\nstart_time = time.time()\nsale_day_store_level,sale_day_item_level,sale_store_item_level=aggregate_level1(train)\n\nend_time=time.time()\ntime_taken=end_time-start_time\nprint(\"This block took \",time_taken,\"seconds\")","c7faad0a":"sale_day_store_level.to_csv(\"sale_day_store_level.csv\")\nsale_day_item_level.to_csv(\"sale_day_item_level.csv\")\nsale_store_item_level.to_csv(\"sale_store_item_level.csv\")","df9c40b5":"# Around 50% save in memory utilization","8dc45ad2":"Since train.csv has 125 mil records, it is best to consider performing some data engineering before starting any analysis.","1a8f48cf":"1.6GB is a managable size","c4a99a5e":"\n Store-day level sale -- This variable indicates the sale of a particular store over time\n\n Store-day level count -- This variable gives an indication of the variaty\/spread of the items sold\n\n Item-day level sale -- Sale of an item over time\n \n Item-day level count -- This gives an indication of the popularity of the item across the supermarket chain.\n","290e8d9d":"# Now lets look into the dataset","74edcdcb":"\n\n## Further to make EDA easier, rolling up the sales to different levels\n\n - Day-Store level\n - Day-Item level\n - Store level\n - Item level\n - Day level\n\n"}}