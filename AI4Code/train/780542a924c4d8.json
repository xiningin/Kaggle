{"cell_type":{"49b31010":"code","974e23de":"code","feb54f0e":"code","bfdb934d":"code","9f5da0b0":"code","77c7c851":"code","085bdf76":"code","c85534b9":"code","3874e07b":"code","b38b1c07":"code","3361e220":"code","c9aa6548":"code","5d6003a0":"code","2463d22e":"code","7a49e0b7":"code","43ea1fb6":"code","60cd2c27":"code","9e8b63f8":"code","ed3d60bb":"code","f6e0bbca":"markdown","8bcc3248":"markdown","71018ea6":"markdown","5c7c9955":"markdown","a0a200cf":"markdown"},"source":{"49b31010":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, PoissonRegressor\nfrom lightgbm.sklearn import LGBMRegressor\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","974e23de":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv', parse_dates=['date'])\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['target'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\n\ndf = df.drop(columns=[\n    'total_revenue', \n    'revenue_share_percent'\n])","feb54f0e":"df['dayofweek'] = df['date'].dt.dayofweek","bfdb934d":"df.head()","9f5da0b0":"VAL_DATE = '2019-06-20'\nTEST_DATE = '2019-06-22'\n\ntest_mask = df['date'] >= TEST_DATE\ntrain_mask = df['date'] < VAL_DATE\nval_mask = (df['date'] >= VAL_DATE) & (df['date'] < TEST_DATE)\n\n\nX, y = df.drop(columns=['target', 'date']), df['target']\n\nX_train, y_train = X[train_mask], y[train_mask]\nX_val, y_val = X[val_mask], y[val_mask]\nX_test, y_test = X[test_mask], y[test_mask]\n\n\ntest_max = y_test.quantile(0.95)\ntest_mask_filter = (y_test <= test_max) & (y_test >= 0)\nX_test, y_test = X_test[test_mask_filter], y_test[test_mask_filter]\n\ntrain_max = y_train.quantile(0.95)\ntrain_mask_filter = (y_train <= train_max) & (y_train >= 0)\nX_train, y_train = X_train[train_mask_filter], y_train[train_mask_filter]\n\nval_max = y_val.quantile(0.95)\nval_mask_filter = (y_val <= val_max) & (y_val >= 0)\nX_val, y_val = X_val[val_mask_filter], y_val[val_mask_filter]\n\nX_train_val = pd.concat([X_train, X_val])\ny_train_val = pd.concat([y_train, y_val])","77c7c851":"plt.hist(y_train, bins=50)","085bdf76":"numeric_features = [\n    'measurable_impressions', \n    'total_impressions',\n    'viewable_impressions',\n    \n]\n\ncategorical_features = [\n    'site_id', \n    'ad_type_id', \n    'geo_id',\n    'device_category_id',\n    'advertiser_id',\n    'os_id',\n    'monetization_channel_id',\n    'ad_unit_id',\n    'order_id',\n    'line_item_type_id',\n    'integration_type_id',\n    'dayofweek',\n]\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', numeric_transformer, numeric_features),\n    ('cat', categorical_transformer, categorical_features)\n])\n\nlr = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression()),\n])\n\nlr.fit(X_train, y_train)\nmean_squared_error(lr.predict(X_test), y_test)","c85534b9":"X_train_norm = preprocessor.fit_transform(X_train)\nX_val_norm = preprocessor.transform(X_val)","3874e07b":"class TabularDataset(Dataset):\n    \n    def __init__(self, X, y):\n        self.X = torch.from_numpy(X.todense()).float()\n        self.y = torch.from_numpy(y.values).float().unsqueeze(-1)\n        \n    def __getitem__(self, index):\n        return self.X[index], self.y[index]\n        \n    def __len__ (self):\n        return len(self.y)\n    \ntrain_dataset = TabularDataset(X_train_norm, y_train)\nval_dataset = TabularDataset(X_val_norm, y_val)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_dataloader = DataLoader(val_dataset)","b38b1c07":"class Net(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.fc4 = nn.Linear(hidden_dim, 1)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n\n        x = self.fc4(x)\n        x = F.relu(x) # \u043e\u0441\u0442\u0430\u0432\u0438\u043c \u0438 \u0437\u0434\u0435\u0441\u044c relu, \u0438\u0431\u043e \u0442\u0430\u0440\u0433\u0435\u0442 >= 0\n        return x","3361e220":"input_dim = X_train_norm.shape[1]\nmodel = Net(input_dim, 512)\nopt = optim.Adam(model.parameters(), lr=3e-4)","c9aa6548":"for epoch in range(10):\n    \n    epoch_train_loss = 0\n    epoch_val_loss = 0\n    \n    model.train()\n    for X_train_batch, y_train_batch in train_dataloader:\n        y_pred = model(X_train_batch)\n        loss = F.mse_loss(y_pred, y_train_batch)\n        \n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        epoch_train_loss += loss.item()    \n    \n    with torch.no_grad():\n        model.eval()\n        for X_val_batch, y_val_batch in val_dataloader:\n            y_pred = model(X_val_batch)\n            loss = F.mse_loss(y_pred, y_val_batch)\n            epoch_val_loss += loss.item()\n            \n        \n    epoch_train_loss = epoch_train_loss \/ len(train_dataloader)\n    epoch_val_loss = epoch_val_loss \/ len(val_dataloader)\n    print(f'train: {epoch_train_loss:.2f}, val: {epoch_val_loss:.2f}')\n        ","5d6003a0":"X_test_tensor = torch.from_numpy(preprocessor.transform(X_test).todense()).float()\nwith torch.no_grad():\n    preds = model(X_test_tensor).squeeze(-1).numpy()\n    \nmean_squared_error(y_test, preds)","2463d22e":"(y_train == 0).sum() \/ len(y_train)","7a49e0b7":"params = {\n    'learning_rate': 0.1, \n    'n_estimators': 250, \n    'reg_lambda': 0.1, \n    'num_leaves': 63,\n    'objective': 'tweedie', # \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0442\u0430\u0440\u0433\u0435\u0442\u0430 \u0441\u043a\u043e\u0448\u0435\u043d\u043d\u043e\u0435 \u0438 \u043c\u043d\u043e\u0433\u043e \u043d\u0443\u043b\u0435\u0439\n    'tweedie_variance_power': 1.5,\n}","43ea1fb6":"lgbm = LGBMRegressor(**params)\nlgbm.fit(\n    X_train, \n    y_train, \n    eval_set=(X_val, y_val), \n    eval_metric=['mse'], \n    verbose=20, \n    categorical_feature=categorical_features\n)","60cd2c27":"mean_squared_error(lgbm.predict(X_test), y_test)","9e8b63f8":"lgbm_all_train = LGBMRegressor(**params)\nlgbm_all_train.fit(X_train_val, y_train_val, categorical_feature=categorical_features)","ed3d60bb":"mean_squared_error(lgbm_all_train.predict(X_test), y_test)","f6e0bbca":"## 2. DNN (not the best choice for tabular data... but why not)","8bcc3248":"## 1. LinearRegression (baseline)","71018ea6":"\u041e\u0431\u0443\u0447\u0438\u043c \u0441 \u0442\u0435\u043c\u0438 \u0436\u0435 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438, \u043d\u043e \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0438 \u0434\u043b\u044f \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438","5c7c9955":"# Best MSE: 2603.4","a0a200cf":"## 3. LightGBM"}}