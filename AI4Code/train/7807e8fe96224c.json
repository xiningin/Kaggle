{"cell_type":{"afcf280d":"code","7a21020b":"code","5ae15cf8":"code","cf0ef19d":"code","f95bc232":"code","85b605be":"code","188e0c88":"code","908747a7":"code","c8d9401c":"code","26d7e293":"code","f0fd89cc":"code","3780ba52":"code","68684105":"code","94c83943":"code","ee564a0f":"code","4aabb6d1":"code","8412f145":"code","e46df1aa":"code","bd802d64":"code","ad2dd69c":"code","6ead313e":"code","e10d7720":"code","f06ddecc":"code","80aad1e6":"code","076969e9":"code","8197918c":"code","a90ed042":"code","745d4578":"code","96f6a5a4":"code","84f6e1c7":"code","f5fd7475":"code","8c1f73e0":"code","2ef64496":"code","377cc378":"code","d6e37423":"code","33f9598d":"code","f16c7a00":"code","d52da88e":"code","efdcb8cd":"markdown","f44e35a8":"markdown","b6ef18bf":"markdown","e64fbae4":"markdown","5eef513a":"markdown","8134bcff":"markdown","62b83e50":"markdown","20405f4d":"markdown","673aac1b":"markdown","50cd18f7":"markdown","7ce66aec":"markdown","95867aec":"markdown","b8801dd0":"markdown","b9feacee":"markdown","f965292b":"markdown","25210fc8":"markdown"},"source":{"afcf280d":"# Necessary imports\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7a21020b":"data_dir = ('..\/input\/brain-tumor-mri-dataset\/Training')\ncategories = ['glioma', 'meningioma', 'notumor', 'pituitary']\nfor i in categories:\n    path = os.path.join(data_dir, i)\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img)) ","5ae15cf8":"plt.imshow(img_array);","cf0ef19d":"# The image shape.\nimg_array.shape","f95bc232":"plt.figure(figsize=(20, 16))\n\nimages_path = ['\/glioma\/Tr-glTr_0000.jpg', '\/meningioma\/Tr-meTr_0000.jpg', '\/notumor\/Tr-noTr_0000.jpg', '\/pituitary\/Tr-piTr_0000.jpg']\n\nfor i in range(4):\n    ax = plt.subplot(2, 2, i + 1)\n    img = cv2.imread(data_dir + images_path[i])\n    img = cv2.resize(img, (250, 250))\n    plt.imshow(img)\n    plt.title(categories[i])","85b605be":"# 96.95%\n\nmodel1 = Sequential()\n\n# Convolutional layer 1\nmodel1.add(Conv2D(32,(3,3), input_shape=(64, 64, 1), activation='relu'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 2\nmodel1.add(Conv2D(32,(3,3), activation='relu'))\nmodel1.add(BatchNormalization())\nmodel1.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel1.add(Flatten())\n\n# Neural network\n\nmodel1.add(Dense(units= 252, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(units=252, activation='relu'))\nmodel1.add(Dropout(0.2))\nmodel1.add(Dense(units=4, activation='softmax'))\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, clipvalue=0.5)\nmodel1.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])\n\n\n\n# using the ImageDataGenerator to prepare the images (Resize, nomalize, etc)\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False) \n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n# Creating the train and test data.\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n","188e0c88":"# Creating callbacks for the model.\n# If the model dosen't continue to improve (loss), the trainning will stop.\n\n# Stop training if loss doesn't keep decreasing.\nmodel1_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel1_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\n\n# Automatically saves the best weights of the model, based on best val_accuracy\nmodel1_mcp = ModelCheckpoint(filepath = 'model1_weights.h5', monitor = 'val_categorical_accuracy', \n                      save_best_only = True, verbose = 1)\n\n# Fiting the model.\nhistory1 = model1.fit(train, steps_per_epoch=5712\/\/32, epochs=100, validation_data=test, validation_steps= 1311\/\/32,\n                     callbacks=[model1_es, model1_rlr, model1_mcp])","908747a7":"model1.evaluate(test)","c8d9401c":"model1.summary()","26d7e293":"\nmodel2 = Sequential()\n\n# Convolutional layer 1\nmodel2.add(Conv2D(64,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\n\n#Convolutional layer 2\nmodel2.add(Conv2D(128,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 3\nmodel2.add(Conv2D(256,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 4\nmodel2.add(Conv2D(512,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel2.add(Flatten())\n\n# Full connect layers\n\nmodel2.add(Dense(units= 512, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(units=512, activation='relu'))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(units=4, activation='softmax'))\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, clipvalue=0.5)\nmodel2.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])\n\n","f0fd89cc":"# using the ImageDataGenerator\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                   featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\n\n\n","3780ba52":"# Creating callbacks for the model.\n# If the model dosen't continue to improve (loss), the trainning will stop.\n\n# Stop training if loss doesn't keep decreasing.\nmodel2_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel2_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\n\n# Automatically saves the best weights of the model, based on best val_accuracy\nmodel2_mcp = ModelCheckpoint(filepath = 'model2_weights.h5', monitor = 'val_categorical_accuracy', \n                      save_best_only = True, verbose = 1)\n\n\n# Fiting the model.\nhistory2 = model2.fit(train, steps_per_epoch=5712\/\/32, epochs=100, validation_data=test, validation_steps= 1311\/\/32,\n                      callbacks=[model2_es, model2_rlr, model2_mcp])","68684105":"model2.evaluate(test)","94c83943":"model2.summary()","ee564a0f":"model3 = Sequential()\n\n# Convolutional layer 1\nmodel3.add(Conv2D(64,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(MaxPooling2D(pool_size=(2,2)))\n\n#Convolutional layer 2\nmodel3.add(Conv2D(64,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 3\nmodel3.add(Conv2D(64,(7,7), input_shape=(64, 64, 1), padding='same', activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel3.add(Flatten())\n\n# Full connect layers\n\nmodel3.add(Dense(units= 512, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(units=512, activation='relu'))\nmodel3.add(Dropout(0.2))\nmodel3.add(Dense(units=4, activation='softmax'))\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.0001, clipvalue=0.5)\nmodel3.compile(optimizer=optimizer, loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])\n","4aabb6d1":"# using the ImageDataGenerator\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                   featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(64,64),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')","8412f145":"# Creating callbacks for the model.\n# If the model dosen't continue to improve (loss), the trainning will stop.\n\n# Stop training if loss doesn't keep decreasing.\nmodel3_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel3_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\n\n# Automatically saves the best weights of the model, based on best val_accuracy\nmodel3_mcp = ModelCheckpoint(filepath = 'model3_weights.h5', monitor = 'val_categorical_accuracy', \n                      save_best_only = True, verbose = 1)\n\n# Fitting the model\nhistory3 = model3.fit(train, steps_per_epoch=5712\/\/32, epochs=100, validation_data=test, validation_steps= 1311\/\/32,\n                     callbacks=[model3_es, model3_rlr, model3_mcp])","e46df1aa":"model3.evaluate(test)","bd802d64":"# Code for plotting\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(3, 2, figsize=(20,17))\n\naxs[0, 0].plot(history1.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history1.history[\"val_loss\"],c = \"orange\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss on Model_1 32 Filtters - 3x3 kernel - 2 Conv Layers')\n\naxs[0, 1].plot(history1.history[\"categorical_accuracy\"],c = \"purple\")\naxs[0, 1].plot(history1.history[\"val_categorical_accuracy\"],c = \"orange\")\naxs[0, 1].set_ylim([0.8,1.1])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Accuracy on Model_1 32 Filtters - 3x3 kernel - 2 Conv Layers')\n\naxs[1, 0].plot(history2.history[\"loss\"],c = \"purple\")\naxs[1, 0].plot(history2.history[\"val_loss\"],c = \"orange\")\naxs[1, 0].set_ylim([-0.1,2])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Loss on Model_2 64 Filtters - 7x7 kernel - 4 Conv Layers')\n\naxs[1, 1].plot(history2.history[\"categorical_accuracy\"],c = \"purple\")\naxs[1, 1].plot(history2.history[\"val_categorical_accuracy\"],c = \"orange\")\naxs[1, 1].set_ylim([0.8,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy on Model_2 64 Filtters - 7x7 kernel - 4 Conv Layers')\n\naxs[2, 0].plot(history3.history[\"loss\"],c = \"purple\")\naxs[2, 0].plot(history3.history[\"val_loss\"],c = \"orange\")\naxs[2, 0].set_ylim([-0.1,2])\naxs[2, 0].legend([\"train\", \"test\"])\naxs[2, 0].set_title('Loss on Model_3 64 Filtters - 7x7 kernel - 3 Conv Layers')\n\naxs[2, 1].plot(history3.history[\"categorical_accuracy\"],c = \"purple\")\naxs[2, 1].plot(history3.history[\"val_categorical_accuracy\"],c = \"orange\")\naxs[2, 1].set_ylim([0.8,1.1])\naxs[2, 1].legend([\"train\", \"test\"])\naxs[2, 1].set_title('Accuracy on Model_3 64 Filtters - 7x7 kernel - 3 Conv Layers')","ad2dd69c":"model3.summary()","6ead313e":"model3.evaluate(test)","e10d7720":"model4 = Sequential()\n\n# Convolutional layer 1\nmodel4.add(Conv2D(64,(7,7), input_shape=(200, 200, 1), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\n#Convolutional layer 2\nmodel4.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 3\nmodel4.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 4\nmodel4.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\n # Convolutional layer 5\nmodel4.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 6\nmodel4.add(Conv2D(512,(7,7), padding='same', activation='relu'))\nmodel4.add(BatchNormalization())\nmodel4.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel4.add(Flatten())\n\n# Full connect layers\n\nmodel4.add(Dense(units= 1024, activation='relu'))\nmodel4.add(Dropout(0.3))\nmodel4.add(Dense(units=1024, activation='relu'))\nmodel4.add(Dropout(0.3))\nmodel4.add(Dense(units=4, activation='softmax'))\n\n\n\nmodel4.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])","f06ddecc":"# using the ImageDataGenerator\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(200, 200),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(200, 200),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')","80aad1e6":"# Creating callbacks for the model.\n# If the model dosen't continue to improve (loss), the trainning will stop.\n\n# Stop training if loss doesn't keep decreasing.\nmodel4_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel4_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\n\n\n# Automatically saves the best weights of the model, based on best val_accuracy\nmodel4_mcp = ModelCheckpoint(filepath = 'model4_weights.h5', monitor = 'val_accuracy_score', \n                      save_best_only = True, verbose = 1)\n\nhistory4 = model4.fit(train, steps_per_epoch=5712\/\/32, epochs=100, validation_data=test, validation_steps= 1311\/\/32,\n                     callbacks=[model4_es, model4_rlr, model4_mcp])","076969e9":"model4.summary()","8197918c":"model4.evaluate(test)","a90ed042":"model5 = Sequential()\n\n# Convolutional layer 1\nmodel5.add(Conv2D(64,(7,7), input_shape=(200, 200, 1), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\n#Convolutional layer 2\nmodel5.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 3\nmodel5.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 4\nmodel5.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\n # Convolutional layer 5\nmodel5.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 6\nmodel5.add(Conv2D(512,(7,7), padding='same', activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel5.add(Flatten())\n\n# Full connect layers\n\nmodel5.add(Dense(units= 1024, activation='relu'))\nmodel5.add(Dropout(0.25))\nmodel5.add(Dense(units=512, activation='relu'))\nmodel5.add(Dropout(0.25))\nmodel5.add(Dense(units=4, activation='softmax'))\n\n\n\nmodel5.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])","745d4578":"# using the ImageDataGenerator\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                   featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(200, 200),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(200, 200),\n                                              batch_size=32, class_mode= \"categorical\", color_mode='grayscale')","96f6a5a4":"model5_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel5_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\nmodel5_mcp = ModelCheckpoint(filepath = 'model5_weights.h5', monitor = 'val_categorical_accuracy', \n                      save_best_only = True, verbose = 1)\n\nhistory5 = model5.fit(train, steps_per_epoch=5712\/\/32, epochs=100, validation_data=test, validation_steps= 1311\/\/32,\n                     callbacks=[model5_es, model5_rlr, model5_mcp])","84f6e1c7":"model5.summary()","f5fd7475":"model5.evaluate(test)","8c1f73e0":"model6 = Sequential()\n\n# Convolutional layer 1\nmodel6.add(Conv2D(64,(7,7), input_shape=(200, 200, 1), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\n#Convolutional layer 2\nmodel6.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 3\nmodel6.add(Conv2D(128,(7,7), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 4\nmodel6.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\n # Convolutional layer 5\nmodel6.add(Conv2D(256,(7,7), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\n# Convolutional layer 6\nmodel6.add(Conv2D(512,(7,7), padding='same', activation='relu'))\nmodel6.add(BatchNormalization())\nmodel6.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel6.add(Flatten())\n\n# Full connect layers\n\nmodel6.add(Dense(units= 1024, activation='relu'))\nmodel6.add(Dropout(0.25))\nmodel6.add(Dense(units=1024, activation='relu'))\nmodel6.add(Dropout(0.25))\nmodel6.add(Dense(units=512, activation='relu'))\nmodel6.add(Dropout(0.2))\nmodel6.add(Dense(units=4, activation='softmax'))\n\n\n\nmodel6.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy',\n                   metrics= ['categorical_accuracy'])","2ef64496":"# using the ImageDataGenerator\n\ngenerator_train = ImageDataGenerator(rescale=1.\/255,\n                                    featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\ngenerator_test = ImageDataGenerator(rescale=1.\/255,\n                                   featurewise_center=False,\n                                    samplewise_center=False,\n                                    featurewise_std_normalization=False,\n                                    samplewise_std_normalization=False,\n                                    zca_whitening=False,\n                                    rotation_range=0,\n                                    zoom_range = 0,\n                                    width_shift_range=0,\n                                    height_shift_range=0,\n                                    horizontal_flip=True,\n                                    vertical_flip=False)\n\n\ntrain = generator_train.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Training', target_size=(200, 200,),\n                                              batch_size=64, class_mode= \"categorical\", color_mode='grayscale')\n\ntest = generator_test.flow_from_directory('..\/input\/brain-tumor-mri-dataset\/Testing', target_size=(200, 200,),\n                                              batch_size=64, class_mode= \"categorical\", color_mode='grayscale')","377cc378":"model6_es = EarlyStopping(monitor = 'loss', min_delta = 1e-11, patience = 12, verbose = 1)\nmodel6_rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 6, verbose = 1)\nmodel6_mcp = ModelCheckpoint(filepath = 'model6_weights.h5', monitor = 'val_categorical_accuracy', \n                      save_best_only = True, verbose = 1)\n\n\nhistory6 = model6.fit(train, steps_per_epoch=5712\/\/64, epochs=100, validation_data=test, validation_steps= 1311\/\/64,\n                     callbacks=[model6_es, model6_rlr, model6_mcp])","d6e37423":"model6.summary()","33f9598d":"model6.evaluate(test)","f16c7a00":"# Code for plotting\nimport matplotlib.pyplot as plt\nfig, axs = plt.subplots(3, 2, figsize=(20,17))\n\naxs[0, 0].plot(history4.history[\"loss\"],c = \"blue\")\naxs[0, 0].plot(history4.history[\"val_loss\"],c = \"green\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss on Model_4 64 Filtters - 7x7 kernel - 6 Conv Layers and 2 FC, Batch 32')\n\naxs[0, 1].plot(history4.history[\"categorical_accuracy\"],c = \"blue\")\naxs[0, 1].plot(history4.history[\"val_categorical_accuracy\"],c = \"green\")\naxs[0, 1].set_ylim([0.8,1.1])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Accuracy on Model_4 64 Filtters - 7x7 kernel - 6 Conv Layers and 2 FC, Batch 32')\n\naxs[1, 0].plot(history5.history[\"loss\"],c = \"blue\")\naxs[1, 0].plot(history5.history[\"val_loss\"],c = \"green\")\naxs[1, 0].set_ylim([-0.1,2])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Loss on Model_5 64 Filtters - 7x7 kernel - 6 Conv Layers and 2 FC less units, Batch 32')\n\naxs[1, 1].plot(history5.history[\"categorical_accuracy\"],c = \"blue\")\naxs[1, 1].plot(history5.history[\"val_categorical_accuracy\"],c = \"green\")\naxs[1, 1].set_ylim([0.8,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy on Model_5 64 Filtters - 7x7 kernel - 6 Conv Layers and 2 FC less unit, Batch 32')\n\naxs[2, 0].plot(history6.history[\"loss\"],c = \"blue\")\naxs[2, 0].plot(history6.history[\"val_loss\"],c = \"green\")\naxs[2, 0].set_ylim([-0.1,2])\naxs[2, 0].legend([\"train\", \"test\"])\naxs[2, 0].set_title('Loss on Model_6 64 Filtters - 7x7 kernel - 6 Conv Layers, 3 FC and Batch 64')\n\naxs[2, 1].plot(history6.history[\"categorical_accuracy\"],c = \"blue\")\naxs[2, 1].plot(history6.history[\"val_categorical_accuracy\"],c = \"green\")\naxs[2, 1].set_ylim([0.8,1.1])\naxs[2, 1].legend([\"train\", \"test\"])\naxs[2, 1].set_title('Accuracy on Model_6 64 Filtters - 7x7 kernel - 6 Conv Layers, 3 FC and Batch 64')","d52da88e":"# Saving the model\nmodel5_json = model5.to_json()\nwith open('classifier_brain_tumor.json', 'w') as json_file:\n    json_file.write(model5_json)\n    \n# Saving the best weights was already save on the training process through callback.\n    \n    \n    ","efdcb8cd":"# CNN - Model 1","f44e35a8":"# Conclusion:\nModel 5 pleased me a lot.\nNot too complex and not too simple either.\nWe avoid underfitting and overfitting with a smooth process.\n\nI will save model 5 for now and I will continue to testing and working on this data.","b6ef18bf":"## Model 6","e64fbae4":"# Modelling\n","5eef513a":"# Evaluating models 1, 2 and 3\n","8134bcff":"## Model 5","62b83e50":"## Model 4","20405f4d":"#### Ploting a image of each brain tumor type","673aac1b":"# Let's visualize the data.","50cd18f7":"# Making changes on new models\nI will change the number of Conv layers, FC layers, optmizer and input_shape.","7ce66aec":"# Brain Tumor Classifier\n\n#### What is a brain tumor?\nA brain tumor is a collection, or mass, of abnormal cells in your brain. Your skull, which encloses your brain, is very rigid. Any growth inside such a restricted space can cause problems. Brain tumors can be cancerous (malignant) or noncancerous (benign). When benign or malignant tumors grow, they can cause the pressure inside your skull to increase. This can cause brain damage, and it can be life-threatening.\n\n#### The importance of the subject\nEarly detection and classification of brain tumors is an important research domain in the field of medical imaging and accordingly helps in selecting the most convenient treatment method to save patients life therefore\n\n#### Methods\nThe application of deep learning approaches in context to improve health diagnosis is providing impactful solutions. According to the World Health Organization (WHO), proper brain tumor diagnosis involves detection, brain tumor location identification, and classification of the tumor on the basis of malignancy, grade, and type. This experimental work in the diagnosis of brain tumors using Magnetic Resonance Imaging (MRI) involves detecting the tumor, classifying the tumor in terms of grade, type, and identification of tumor location. This method has experimented in terms of utilizing one model for classifying brain MRI on different classification tasks rather than an individual model for each classification task. The Convolutional Neural Network (CNN) based multi-task classification is equipped for the classification and detection of tumors. The identification of brain tumor location is also done using a CNN-based model by segmenting the brain tumor.\n\n\n### The Objective:\nWe are gonna use Convolution Neural Network to predict the type of a brain tumor base on x rays pictures.\n\nThe dataset with the data is here: \nhttps:\/\/www.kaggle.com\/masoudnickparvar\/brain-tumor-mri-dataset\n\n","95867aec":"# Model 2","b8801dd0":"### Training model 2","b9feacee":"# Model 3","f965292b":"## Evaluating models 4, 5 and 6","25210fc8":"### Trainig model 1"}}