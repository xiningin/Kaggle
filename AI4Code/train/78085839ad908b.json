{"cell_type":{"6310b8cc":"code","3cb9758d":"code","4320a620":"code","b1b9bd7f":"code","10224983":"code","95f27a7d":"code","319453ca":"code","5f704bdf":"code","1eee4ca5":"code","4b764429":"code","0761f518":"code","ccb709c1":"code","aa764598":"code","7266bf04":"code","44d0e6fc":"code","221cc837":"code","465faac7":"code","747c718a":"code","7d535e0f":"code","e4f22dc3":"code","4595e85e":"code","d55995ba":"code","7b353727":"code","0e3b7bec":"code","3ce17a7d":"code","4f2c6585":"code","41c77bed":"code","11933c81":"code","be616d5e":"code","34e99a31":"code","e924f28f":"code","36400c03":"code","2a131e11":"code","6c5e20fb":"code","749b88fc":"code","f00eab43":"code","529a682f":"code","4086db2b":"code","886663fe":"code","47053ba7":"code","4447bcb6":"code","e10c4f10":"code","816dc98b":"code","16e94247":"code","1f0018cb":"code","44ccf02f":"code","ae6b6be6":"code","7ae58ecb":"code","9e695574":"code","df21c4af":"code","b1bf3625":"code","80a8b730":"code","01dd53d8":"code","c3708c15":"code","49eaee5e":"code","7abbeeda":"code","4cba9e78":"code","1c499257":"code","e9c11f02":"code","dd934b82":"code","4260fec6":"code","d86500be":"code","f4d99cab":"code","6aed55a6":"code","30cff843":"code","34d41bd0":"code","d5a781c9":"code","b921d8e4":"code","2dc09869":"code","0ec6b691":"code","80dced2d":"code","4028b0de":"code","78fc1754":"code","b61b2ca9":"code","1ca67e3a":"code","8c4c06f5":"code","81a736b3":"code","3e9addd3":"code","3af75604":"code","1a7e815b":"code","5103592c":"code","7787c67c":"code","4a0b1d8d":"code","0b9b417b":"code","6c40a249":"code","d45e1eb6":"code","274cbe01":"code","97bf29e5":"code","3bbd01c4":"code","12306f60":"code","9ef6a87b":"code","a8a3d85a":"code","616f0c12":"markdown","fffadaa0":"markdown","a9648678":"markdown","94cb27e4":"markdown","ab2747f6":"markdown","de4decb7":"markdown","258d1980":"markdown","f6d8ff98":"markdown","c3e11768":"markdown","cfd64a97":"markdown","666f43ba":"markdown","45b73306":"markdown","4483e2c5":"markdown"},"source":{"6310b8cc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as plt","3cb9758d":"!wget https:\/\/raw.githubusercontent.com\/alexeygrigorev\/datasets\/master\/AB_NYC_2019.csv","4320a620":"df=pd.read_csv('AB_NYC_2019.csv')","b1b9bd7f":"df.head()","10224983":"df=df[['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\ndf","95f27a7d":"#df.isnull().sum() #10052","319453ca":"#df.describe()","5f704bdf":"'''\ntarget=df.price\ndf=df.drop('price', axis=1)\ndf\n\nsns.histplot(target, bins=10)\n\nnp.log1p([1,10,1000,10000000])\ntarget=np.log1p(target)\ntarget\n\nplt.rcParams['figure.figsize']=[12,10]\nsns.histplot(df.price, bins=10)\n'''","1eee4ca5":"n=len(df)\nn_val=int(0.2*n)\nn_test=int(0.2*n)\nn_train=int(n-(n_val+n_test))\nn_val, n_test, n_train","4b764429":"df_train=df.iloc[:n_train]\ndf_val=df.iloc[n_train:n_val+n_train]\ndf_test=df.iloc[n_val+n_train:]\ndf_test","0761f518":"idx=np.arange(n)\nnp.random.seed(9) #42\nnp.random.shuffle(idx)\nidx","ccb709c1":"df_train=df.iloc[idx[:n_train]]\ndf_val=df.iloc[idx[n_train:n_val+n_train]]\ndf_test=df.iloc[idx[n_val+n_train:]]\n","aa764598":"np.log10([1,10,1000,10000000])\ny_train=np.log1p(df_train.price.values)\ny_val=np.log1p(df_val.price.values)\ny_test=np.log1p(df_test.price.values)\nsns.histplot(y_train, bins=10)","7266bf04":"del df_train['price']\ndel df_val['price']\ndel df_test['price']","44d0e6fc":"#df_train.isna().sum()\n#X_train=df_train.fillna(0).values # .values convert dataframe to numpy array\n#X_train","221cc837":"df_train.iloc[10]","465faac7":"\nxi=[354,1,4] # example values of one feature from our dataset\nw0=5.17 # random bias\nw =[.01, .04, .002]\n","747c718a":"\ndef linear_regression(xi):\n    n_loops=len(xi)\n    pred=w0 #initial weight for bias\n    \n    for j in range(n_loops):\n        pred+=w[j]*xi[j]\n    return pred\n","7d535e0f":"print(linear_regression(xi))\n# Note that it produces lower value due to we are going to apply log on y values","e4f22dc3":"# To get the exact values, we have to reverse the log values by exponential\nnp.expm1(8.758) # compare this predicted price with 'y' value","4595e85e":"np.log1p(6360.3760945639915)","d55995ba":"def dot_mat(xi,w):\n    \"\"\"Dot Product of xi vector\"\"\"\n    n_runs = len(xi)\n\n    result = 0.0\n\n    for run in range(n_runs):\n        result += xi[run] * w[run]\n    return result\n","7b353727":"[w0] + w # w0 = 5.17","0e3b7bec":"[1] + xi # xi0 =1","3ce17a7d":"def lr_form(xi):\n    w_new = [w0] + w \n    xi = [1] + xi\n    return dot_mat(xi,w_new)\n\nprint(lr_form(xi))\n","4f2c6585":"w0 = 5.17\nw_new = [w0] + w\nw_new\n","41c77bed":"df_train.iloc[2]","11933c81":"# three sample features\nx1  = [1, 0, 2, 7]\nx2  = [1, 73, 21, 0]\nx10 = [1, 354, 1, 4]\n\nX = [x1, x2, x10]\nX = np.array(X)\nX\n","be616d5e":"def lr_multiple_features(xi,w):\n    return xi.dot(w)\n\nprint(lr_multiple_features(xi=X,w=w_new))\n","34e99a31":"# Multiple features\nX = [\n    [0, 2, 7],\n    [73, 21, 0],\n    [354, 1, 4],\n    [0, 4, 10],\n    [156, 4, 6],\n    [347, 1, 11],\n    [175, 3, 18],\n    [5, 3, 7],\n    [71, 1, 392],\n]\nX = np.array(X)\nX\n    \n    ","e924f28f":"np.set_printoptions(suppress=True) # roundoff value instead of showng values 1e+07 to normal numbe","36400c03":"# creating bias for X, keeping bias would help us to understand baseline price of the car\nones = np.ones(X.shape[0])\nones\n","2a131e11":"# append ones values in X column wise for considering ones as bias\nX = np.column_stack([ones,X])\nX\n","6c5e20fb":"# y values\ny = [10000, 20000, 15000, 20050, 10000, 20000, 15000, 25000, 12000]\ny\n","749b88fc":"XTX = X.T.dot(X)\nXTX\n","f00eab43":"XTX_inv = np.linalg.inv(XTX)\nXTX_inv\n","529a682f":"X.T\n","4086db2b":"# putting together everything to calculate W\nw_full = XTX_inv.dot(X.T).dot(y)\nw_full\n","886663fe":"# Separating w0 and other weights (w)\nw0 = w_full[0]\nw = w_full[1:]\nw0, w\n","47053ba7":"X","4447bcb6":"X = [\n    [0, 2, 7],\n    [73, 21, 0],\n    [354, 1, 4],\n    [0, 4, 10],\n    [156, 4, 6],\n    [347, 1, 11],\n    [175, 3, 18],\n    [5, 3, 7],\n    [71, 1, 392],\n]\nX = np.array(X)\nX","e10c4f10":"y","816dc98b":"# putting everything inside function - DRY (Dont Repeat Yourself)\ndef linear_regression(X,y):\n    \"\"\"Calculate linear regression\"\"\"\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones,X])\n\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n\n    return w_full[0], w_full[1:]\n    \n","16e94247":"print(linear_regression(X,y)) #(16507.159909645296, array([ -2.47970322, 164.39468258, -11.34438897]))\n","1f0018cb":"df_train.columns\n","44ccf02f":"'''\n# selecting few features to create baseline model\nbase = ['engine_hp', 'engine_cylinders', 'highway_mpg',\n        'city_mpg', 'popularity']\n\n# before creating baseline model, check for missing values\ndf_train[base].isnull().sum().sort_values(ascending=False)\n\n'''","ae6b6be6":"df_train.describe()","7ae58ecb":"\n#mean=df_train.reviews_per_month.mean()\n#mean","9e695574":"X_train=df_train.fillna(0).values # .values convert dataframe to numpy array\nX_train","df21c4af":"w0,w = linear_regression(X_train, y_train)\nw0","b1bf3625":"w","80a8b730":"# Calculating predictions based on base and weights\ny_pred = w0 + X_train.dot(w)\ny_pred\n","01dd53d8":"\n# Plot and compare our results\nsns.histplot(y_pred, alpha = 0.5, color='red', bins=50) # alpha for transparency\nsns.histplot(y_train, alpha = 0.5, color='pink', bins=50);","c3708c15":"def rmse(y,y_pred):\n    \"\"\"Calculate RMSE value of a model\"\"\"\n    sq_error = (y - y_pred)**2\n    mean_sq_error = sq_error.mean()\n    return np.sqrt(mean_sq_error)\n","49eaee5e":"'''\n0.6428860484860429 - fillna0 \/ 0.6429347474014219 - fillna-mean\n\n\nseed 42 - 0.6428860484860429 - std 0.2588534886758579\nseed 0 - 0.6428860484860429 - std 0.2598704302266462\nseed1 - 0.6435218881530491 - std 0.25628413449176873\nseed5 - 0.6518143927773479 - std 0.26173783919049237\nseed9 - 0.6455498824478455 - std 0.258989161883956\n'''\nrmse(y_train,y_pred)","7abbeeda":"np.std(y_pred)","4cba9e78":"# from earlier selected features\ndf_train\n","1c499257":"def prepare_X(df_train):\n    df_num = df_train # selecting features from df\n    df_num = df_num.fillna(0) # filling missing values with 0\n    X = df_num.values # converting dataframe to NumPy array\n    return X\n","e9c11f02":"X","dd934b82":"# From earlier written linear regression\ndef linear_regression(X,y):\n    \"\"\"Calculate linear regression\"\"\"\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones,X])\n\n    XTX = X.T.dot(X)\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n\n    return w_full[0], w_full[1:]\n    \n","4260fec6":"X_train = prepare_X(df_train) # preparing X_train dataset after cleaning missing values\nw0, w = linear_regression(X_train,y_train) # produces weights of base and parameters\n\nX_val = prepare_X(df_val) # Preparing validation dataset to compare with train dataset\ny_pred = w0 + X_val.dot(w) # applying known values of w0, w from train dataset\n\n# Comparing the evaluated validation prediction with existing validation target values\nrmse(y_val,y_pred) #- 0.6430337788500516 - fillna0 \/ 0.6427545031700502 - fillna-mean - mean use training only\n\n#seed 0 - 0.6549779960887462 - std 0.26565297219533074\n#seed1 - 0.6462523685757932 - std 0.2502224032678212\n#seed5 - 0.6305809996967069 - std 0.26316812056978356\n#seed9 - 0.6437565168390085 - std 0.25952132585423937\n#seed42 - 0.6430337788500516 - std 0.25645745391866454","d86500be":"np.std(y_pred)","f4d99cab":"X = [\n    [0, 2, 7],\n    [73, 21, 0],\n    [354, 1, 4],\n    [0, 4, 10],\n    [156, 4, 6],\n    [347, 1, 11],\n    [175, 3, 18],\n    [5, 3, 7],\n    [71, 1, 392],\n]\nX = np.array(X)\nX","6aed55a6":"\n# Applying the regularization parameter in our linear regression function\ndef linear_regression_reg(X, y, r=0.001): #r=0.001\n    ones = np.ones(X.shape[0]) # Creating bias term for dataset\n    X = np.column_stack([ones, X]) # Adding column wise, bias with dataset\n\n    XTX = X.T.dot(X) # Matrix Multiplication\n    XTX = XTX + r * np.eye(XTX.shape[0]) # Adding regularization parameter at the diagonals\n\n    XTX_inv = np.linalg.inv(XTX) # Inverse of XTX\n    w_full = XTX_inv.dot(X.T).dot(y) # Normal equation to find the coefficients of bias and weights\n    \n    return w_full[0], w_full[1:] # Bias term, Weights - w1,..wn","30cff843":"X_train = prepare_X(df_train)\nw0, w = linear_regression_reg(X_train, y_train, r=0.001)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred) #0.6557528427439105 - fillna0 \/ 0.6556412987917956-fillnamean\n\n'''\nfillna0\nr=0.001 - 0.6557528427439105\n'''","34d41bd0":"for r_value in [0.001]:#[0.0,0.01,0.001,0.0001,0.00001,10]: # Manually given r_values\n    # Generate training results for each r_value to find out the optimum r-value\n    X_train = prepare_X(df_train)\n    w0, w = linear_regression_reg(X_train, y_train, r=r_value)\n\n    X_val = prepare_X(df_val)\n    y_pred = w0 + X_val.dot(w)\n    score = rmse(y_val, y_pred)\n\n    print(r_value, w0, score)\n\n    '''\n    fillna 0\n    0.0 -419.9126587294167 0.6430337788500516\n0.01 -191.783840532472 0.6557528427439105\n0.001 -375.2736527030572 0.6437669735263444\n0.0001 -414.97649243207684 0.6430723153526027\n1e-05 -419.4137637716231 0.6430371927849962\n10 -0.35127675916757617 0.6828430212097046\n1 -3.499216837729211 0.6823116950157312\n    fillna mean\n0.0 -423.53930827995964 0.6427545031700502\n0.01 -193.56560982606868 0.6556412987917956\n0.001 -378.5627566894573 0.6434861758114766\n0.0001 -418.5663770294138 0.6427920886358791\n1e-05 -423.0367038276912 0.6427578122635195\n10 -0.3547322129293042 0.6832176687749313\n0.001 -381.1884469082559 0.643982431038086\n    \n    '''","d5a781c9":"X_val\n","b921d8e4":"# Combine df_train and df_val as single train dataset\ndf_full_train = pd.concat([df_train,df_val])\ndf_full_train.head()\n","2dc09869":"# Seem the index is shuffled, lets reset the index\ndf_full_train = df_full_train.reset_index(drop=True) # drop = True will drop existing index\ndf_full_train.head()\n","0ec6b691":"'''\n# From earlier written function to do feature engg \ndef prepare_X(df):\n    # copying just to make sure adding\/modifying new features shouldn't affect original data\n    df = df.copy()\n    features = df_train.copy() # Creating Copy of base features\n\n    df['age'] = 2017 - df['year'] # creating new feature in copied dataset\n    features.append('age') # adding 'age' feature with existing features in base list\n\n    for doors in [2,3,4]:\n        # Creating new columns for each door value \n        # Convert them into binary value wherever condition meets (One_Hot Encoder)\n        df['num_doors_%s' %doors] = (df.number_of_doors == doors).astype(int)\n        features.append('num_doors_%s' %doors) # Adding new features to existing feature list\n\n    for name, values in categorical.items():\n        # name - name of the column\n        # values - top most value in each column and looped to create as new feature\n        for value in values:\n            df['%s_%s' % (name, value)] = (df[name] == value).astype(int)\n            features.append('%s_%s' % (name, value))\n\n    df_num = df_train #[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n\n    return X\n'''\n\ndef prepare_X(df_train):\n    df_num = df_train # selecting features from df\n    df_num = df_num.fillna(0) # filling missing values with 0\n    X = df_num.values # converting dataframe to NumPy array\n    return X","80dced2d":"# prepare and clean full dataset\nX_full_train = prepare_X(df_full_train)\nX_full_train\n","4028b0de":"# Combine y values of train and validation dataset together\ny_full_train = np.concatenate([y_train,y_val])\ny_full_train\n","78fc1754":"# train the x_full_train and y_full_train dataset to find the coefficients\nw0, w = linear_regression_reg(X_full_train, y_full_train, r=0.001)\n","b61b2ca9":"# Apply w0, w to find the prediction values\nX_test = prepare_X(df_test) # feature engg on features\ny_pred = w0 + X_test.dot(w) # prediction on X_test values\nscore = rmse(y_test, y_pred) # comparing existing y values with predicted values\nscore\n","1ca67e3a":"X_test","8c4c06f5":"df_test.iloc[5]\n","81a736b3":"car = df_test.iloc[5].to_dict()\ncar\n","3e9addd3":"# To make this test data to give results, \n# we have to modify the features as like in train data by prepare_X function\n# To do that, we have convert that into dataframe\n# remember prepare_X function accepts dataframe only\n\ndf_test_car = pd.DataFrame([car])\ndf_test_car\n","3af75604":"# Now apply on prepare_X function to create features in age, no.doors and for categorical variables\nX_test_car = prepare_X(df_test_car)\nX_test_car\n","1a7e815b":"# Lets check our model prediction on our test car details\ny_pred = w0 + X_test_car.dot(w) # We already know w0, w\ny_pred = y_pred[0]\ny_pred\n","5103592c":"# Our model predicted in logarithmic values, convert them to check values in actual MSRP\nnp.expm1(y_pred)\n","7787c67c":"# Lets check with our actual y value\nnp.expm1(y_test[5])\n","4a0b1d8d":"df.columns = df.columns.str.lower().str.replace(\" \",\"_\")\ndf.columns","0b9b417b":"df.dtypes\n","6c40a249":"df_strings=list(df.dtypes[df.dtypes=='object'].index)\ndf_strings","d45e1eb6":"for column in df_strings:\n    df[column]=df[column].str.lower().str.replace(\" \",\"_\")","274cbe01":"df.select_dtypes(\"object\").head()","97bf29e5":"for column in df.columns:\n    print(column) \n    print('_'*10)\n    print(df[column].unique()[:5])\n    print(df[column].nunique())","3bbd01c4":"df.describe(include='object')","12306f60":"plt.rcParams['figure.figsize']=[12,10]\nsns.histplot(df.msrp, bins=10)\n","9ef6a87b":"sns.histplot(df.msrp[df.msrp<100000], bins=50)","a8a3d85a":"df.msrp.describe()","616f0c12":"**Example - With Multiple features**","fffadaa0":"# Testing Out the Model\nLets apply the model on an unseen data and check its performance","a9648678":"**RMSE**","94cb27e4":"# **Putting together everything**","ab2747f6":"> **Car price baseline model**","de4decb7":"# Regularization\n\nThe higher value of RMSE maybe due to some values in features are identical to other feature values","258d1980":"# Using the Model\n\n\n    Apply everything\n    Combine train + validation as train dataset\n    Compare results with test dataset\n","f6d8ff98":"By Comparing, we came to know that we\u2019re around $ 5K lower than actual which is actually good model as we did only few feature engg and trained with few variables.","c3e11768":"# **Validating the Model on Validation Dataset**","cfd64a97":"> Based on earlier data preparation steps below, we are going to create function\n\n    base = [\u2018engine_hp\u2019, \u2018engine_cylinders\u2019, \u2018highway_mpg\u2019,\u2019city_mpg\u2019, \u2018popularity\u2019]\n    df_train[base].isnull().sum().sort_values(ascending=False)\n    X_train = df_train[base].fillna(0).values\n    w0, w = linear_regression(X_train,y_train)\n    y_pred = w0 + X_train.dot(w)\n","666f43ba":"****VECTOR FORM","45b73306":"**Training Linear Regression**","4483e2c5":"# Tuning the Model"}}