{"cell_type":{"da177ded":"code","8bbdd8ba":"code","d1b84f79":"code","bdafeee0":"code","ac3fed84":"code","4a8bfaf6":"code","54dd8cd3":"code","62c726a7":"code","37a457b9":"code","659ac430":"code","553d880f":"code","907e8e27":"code","f800d742":"code","1cb30780":"code","d8f97677":"code","97742a01":"code","ca7979a4":"code","de6ad9aa":"code","6b5ef4a3":"code","186c9c2a":"code","e111df5f":"code","7f2e0906":"code","1c4aa280":"code","6a49f4e6":"code","903962d7":"code","e7851404":"code","1336d9a8":"code","ce13476d":"code","47dd484c":"markdown","10a29ce1":"markdown","9c55c065":"markdown","e3c135d7":"markdown","67acfad5":"markdown","ef73a064":"markdown","d855c8e8":"markdown","117fb39b":"markdown","aa93131c":"markdown","a3bfd850":"markdown","ee94f8a1":"markdown","933e6702":"markdown","3f5176dc":"markdown","5a04ad11":"markdown","1542359b":"markdown","1e52c37b":"markdown","6ec45b9c":"markdown","3f35b58c":"markdown","60478704":"markdown","bc9328db":"markdown","036d5657":"markdown","670519a7":"markdown","b9667fff":"markdown","1eb67832":"markdown","db7e7706":"markdown","ce15aaf8":"markdown","48728cc8":"markdown","473b4d3f":"markdown","18f87114":"markdown","c23a1a28":"markdown"},"source":{"da177ded":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm # progress bar\nfrom glob import glob # reading multiple files at once\nimport cv2 # opencv\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numba\nimport skimage\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","8bbdd8ba":"from IPython.display import Image\nImage(\"\/kaggle\/input\/probav-report-images\/lr_to_hr.jpg\")\n","d1b84f79":"from IPython.display import Image\nImage(\"\/kaggle\/input\/probav-report-images\/Proba-v process.png\")","bdafeee0":"# [============================================================================]\n\n\ndef baseline_upscale(path):\n\t\"\"\"\n\tReimplementation of the image enhancement operation performed by the\n\tbaseline code (`generate_sample_submission.py`) provided in:\n\thttps:\/\/kelvins.esa.int\/proba-v-super-resolution\/submission-rules\/\n\t\n\t\t\"takes all low resolution images that have the maximum amount of clear\n\t\tpixels, computes a bicubic upscaling by a factor of 3 and averages their\n\t\tpixel intensities.\"\n\t\n\tThis function takes as argument the `path` to a single scene, and returns\n\tthe matrix with the scene's enhanced image.\n\t\"\"\"\n\tclearance = {}\n\tfor (l, c) in lowres_image_iterator(path, img_as_float=True):\n\t\tclearance.setdefault(c.sum(), []).append(l)\n\t\n\t# take all the images that have the same maximum clearance\n\timgs = max(clearance.items(), key=lambda i: i[0])[1]\n\t\n\tsr = np.mean([\n\t\tbicubic_upscaling(i)\n\t\tfor i in imgs\n\t\t], axis=0)\n\t\n\treturn sr\n\t\n\n\ndef central_tendency(images, agg_with='median',\n\t                 only_clear=False, fill_obscured=False,\n\t                 img_as_float=True):\n\t\"\"\"\n\tAggregate the given `images` through a statistical central tendency measure,\n\tchosen by setting `agg_with` to either 'mean', 'median' or 'mode'.\n\t\n\tExpects `images` to be a list of `(image, status map)` tuples.\n\tShould `images` be a string, it's interpreted as the path to a scene's\n\tfiles. The code will then aggregate that scene's low resolution images\n\t(LR*.png), while taking also into account their status maps (QM*.png).\n\t\n\tWill optionally aggregate only images' clear pixels (if `only_clear=True`)\n\tby using the information in images' corresponding status maps.\n\t\n\tIn some scenes, some pixels are obscured in all of the low-resolution\n\timages. Aggregation with mean\/median will return np.nan for those pixels,\n\tand aggregation with mode will return 0.0.\n\tIf called with `fill_obscured=True` those pixels will be filled with the\n\t`agg_with` aggregate of the values at all those obscured pixels. Setting\n\t`fill_obscured` to one of 'mean', 'median' or 'mode' will indicate that is\n\tthe measure that should be used to aggregate obscured pixels.\n\t\"\"\"\n\tagg_opts = {\n\t\t'mean'   : lambda i: np.nanmean(i, axis=0),\n\t\t'median' : lambda i: np.nanmedian(i, axis=0),\n\t\t'mode'   : lambda i: scipy.stats.mode(i, axis=0, nan_policy='omit').mode[0],\n\t\t}\n\tagg = agg_opts[agg_with]\n\t\n\timgs = []\n\tobsc = []\n\t\n\tif isinstance(images, str):\n\t\timages = lowres_image_iterator(images, img_as_float or only_clear)\n\telif only_clear:\n\t\t# Images were given by the caller, rather than loaded here.\n\t\t# Because `only_clear=True`, we generate copies of all lr images, so the\n\t\t# function will have no unintended side-effects on the caller's side.\n\t\timages = [(l.copy(), c) for (l,c) in images]\n\t\n\tfor (l, c) in images:\n\t\t\n\t\tif only_clear:\n\t\t\t\n\t\t\t# keep track of the values at obscured pixels\n\t\t\tif fill_obscured != False:\n\t\t\t\to = l.copy()\n\t\t\t\to[c] = np.nan\n\t\t\t\tobsc.append(o)\n\t\t\t\n\t\t\t# replace values at obscured pixels with NaNs\n\t\t\tl[~c] = np.nan\n\t\t\n\t\timgs.append(l)\n\t\n\t# aggregate the images\n\twith np.warnings.catch_warnings():   ## https:\/\/stackoverflow.com\/a\/29348184\n\t\t# suppress the warnings that originate when `only_clear=True`\n\t\t# but some pixels are never clear in any of the images\n\t\tnp.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n\t\tnp.warnings.filterwarnings('ignore', r'Mean of empty slice')\n\t\t\n\t\tagg_img = agg(imgs)\n\t\t\n\t\tif only_clear and fill_obscured != False:\n\t\t\tif isinstance(fill_obscured, str):\n\t\t\t\tagg = agg_opts[fill_obscured]\n\t\t\tsome_clear = np.isnan(obsc).any(axis=0)\n\t\t\tobsc = agg(obsc)\n\t\t\tobsc[some_clear] = 0.0\n\t\t\tnp.nan_to_num(agg_img, copy=False)\n\t\t\tagg_img += obsc\n\t\n\treturn agg_img\n\n\nfrom collections import Counter, defaultdict\nfrom scipy.stats import percentileofscore\n\n# from .aggregate import central_tendency\n# from .score import score_image_fast, hr_crops\n# from .io import highres_image, scene_id\n\n\n\n# [============================================================================]\n\n\ndef describe(data, **kwargs):\n\t\"\"\"\n\tGenerates descriptive statistics that summarize the central tendency,\n\tdispersion and shape of a dataset's distribution, excluding NaN values.\n\t\n\tBasic wrapper to pandas' `describe()`, to which extra arguments are\n\tredirected. See:\n\thttps:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.describe.html\n\t\"\"\"\n\tdf = pd.DataFrame(data).describe(**kwargs)\n\tdf.columns = ['']\n\treturn df\n\t\n\n\ndef cdf_plot(x, **kwargs):\n\t\"\"\"\n\tEmpirical Cumulative Distribution Function.\n\thttps:\/\/en.wikipedia.org\/wiki\/Empirical_distribution_function\n\t\"\"\"\n\tn = np.arange(1, len(x) + 1) \/ np.float(len(x))\n\tplt.step(np.sort(x), n, **kwargs)\n\tplt.title('Empirical Cumulative Distribution Function')\n\tplt.xlabel('X')\n\tplt.ylabel('Cumulative probability')\n\t\n\n\ndef score_summary(*score_sets, labels=None, **kwargs):\n\t\"\"\"\n\tGenerate a table of summary statistics and an empirical cumulative\n\tdistribution plot for one or more result sets.\n\t\n\tExamples\n\t--------\n\t# Statistics for a single set of results, no labels:\n\t>>> (mean_score, img_scores) = score(sr_imgs, per_image=True)\n\t>>> score_summary(img_scores)\n\t\n\t# Comparison of different named sets of results:\n\t>>> score_summary(test1_scores, test2_scores, labels=['test 1', 'test 2'])\n\t\n\tLabels, if given, must be sent though the `labels` keyword argument.\n\t\"\"\"\n\tlabeled = labels is not None\n\tif not labeled:\n\t\tlabels = [''] * len(score_sets)\n\tif isinstance(labels, str):\n\t\tlabels = [labels]\n\t\n\tassert not labeled or len(score_sets) == len(labels)\n\t\n\twith sns.axes_style('darkgrid'):\n\t\twith sns.plotting_context('notebook', font_scale=1.025):\n\t\t\t\n\t\t\tfor sc, lb in zip(score_sets, labels):\n\t\t\t\tcdf_plot(sc, label=lb, **kwargs)\n\t\t\tif labeled:\n\t\t\t\tplt.legend()\n\t\t\tplt.xlabel('cPSNR')\n\t\n\tdf = pd.DataFrame(list(zip(*score_sets)), columns=labels).describe()\n\tdf.index.name = 'cPSNR'\n\treturn df\n\t\n\n\n# [============================================================================]\n\n\ndef create_panel(ncols=2, nrows=1):\n\t\"\"\"\n\tInitialize a panel with `ncols` columns and `nrows` rows.\n\tConfigures the axes for image display: no ticks, no frame and equal aspect.\n\t\n\tUsage example:\n\t>>> fig, axs = create_panel(2, 2)\n\t>>> axs.flat[3].imshow(image)\n\t\"\"\"\n\tax_cfg = dict(xticks=[], yticks=[], aspect='equal', frame_on=False)\n\t# additional options at:\n\t# https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.figure.Figure.html#matplotlib.figure.Figure.add_subplot\n\t\n\tfig, axs = plt.subplots(nrows, ncols,\n\t                        figsize=(5 * ncols, 5 * nrows),\n\t                        subplot_kw=ax_cfg)\n\tfig.tight_layout()\n\t\n\treturn fig, axs\n\t\n\n\ndef compare_images(a, b, d=None):\n\t\"\"\"\n\tCompare side-by-side the images `a` and `b`.\n\tShows on a third panel the squared difference between both, while\n\tcompensating for the bias in brightness.\n\t\"\"\"\n\tfig, axs = create_panel(ncols=3, nrows=1)\n\t\n\tif d is None:\n\t\td = a - b\n\t\td -= np.mean(d)\n\t\td *= d\n\t\n\tfor ax, img, args in zip(axs.flat,\n\t                         [a, b, d],\n\t                         [{}, {}, dict(cmap=plt.cm.gray_r, vmin=0)]):\n\t\t\n\t\tax.imshow(img, **args)\n\t\n\treturn fig, axs\n\t\n\n\ndef compare_to_hr(sr, scene, only_clear=True):\n\t\"\"\"\n\tCompare side-by-side a super-resolved image `sr` of a given `scene` and its\n\tground-truth (`hr`, the scene's high-resolution image).\n\t\n\tShows on a third panel the squared difference between both, while\n\tcompensating for the bias in brightness (`b`). Conceals by default\n\t(`only_clear=True`) the pixels that are obscured in the `hr` image and don't\n\ttherefore interfere in the determination of `sr`'s cPSNR.\n\t\n\tApplies the same registration process that is employed by the competition's\n\tscoring function, which means the displayed images have a total of 6 pixels\n\tcropped along the edges in each dimension.\n\t\n\tThe scene's cPSNR value shown on the left panel relates to the (rounded)\n\t`mean` value on the right panel (if `only_clear=True`) as:\n\t>>> baseline_cPSNR.loc[scene_id(scene)] \/ (-10. * np.log10(mean))\n\t\"\"\"\n\t(hr, sm) = highres_image(scene)\n\t\n\tsr_score = score_image_fast(sr, scene, (hr, sm))\n\t\n\t# image registration\n\tsr = sr[3 : -3, 3 : -3]\n\tmin_cmse = None\n\tfor (_hr, _sm) in hr_crops(hr, sm):\n\t\td = _hr - sr\n\t\tif only_clear:\n\t\t\td[~_sm] = np.nan\n\t\td -= np.nanmean(d)\n\t\td *= d\n\t\tm = np.nanmean(d)\n\t\tif min_cmse is None or m < min_cmse[0]:\n\t\t\tmin_cmse = (m, d, _hr)\n\t(m, d, hr) = min_cmse\n\t\n\tfig, axs = compare_images(sr, hr, d)\n\t\n\taxs[0].set_title('super-resolved image, cPSNR: %.4f' % sr_score)\n\taxs[1].set_title('high-resolution image (ground-truth)')\n\taxs[2].set_title('(hr - sr - b)^2\\n' + \\\n\t\t'mean: %.2e, std: %.2e, max: %.2e' % (m, np.nanstd(d), np.nanmax(d)),\n\t\tfontdict=dict(verticalalignment='center'))\n\t\n\t# display the scene's id to the left of the image\n\taxs[0].text(-.01, 0.5, scene_id(scene, incl_channel=True),\n\t\thorizontalalignment='right',\n\t\tverticalalignment='center',\n\t\trotation='vertical',\n\t\ttransform=axs[0].transAxes)\n\t\n\treturn fig, axs\n\t\n\n\n# [============================================================================]\n\n\ndef compare_aggregates(img_path,\n\t                   type_opts=('mean', 'median', 'mode'),\n\t                   clear_opts=(False, True),\n\t                   fill_opts=(False, True)):\n\t\"\"\"\n\tPlot side-by-side the mean\/median\/mode aggregations of all images in a given\n\tscene. Also compares aggregation with and without occluded pixels.\n\t\"\"\"\n\tfigs = []\n\t\n\tfor only_clear in clear_opts:\n\t\tfor fill_obscured in (fill_opts if only_clear else [False]):\n\t\t\t\n\t\t\tfig = plt.figure(figsize=(5 * len(type_opts), 6))\n\t\t\tfigs.append(fig)\n\t\t\t\n\t\t\tfor i, agg_type in enumerate(type_opts):\n\t\t\t\t\n\t\t\t\targs = (agg_type, only_clear, fill_obscured)\n\t\t\t\timg = central_tendency(img_path, *args)\n\t\t\t\t\n\t\t\t\tax = fig.add_subplot(1, len(type_opts), i + 1)\n\t\t\t\tax.imshow(img)\n\t\t\t\tax.set_title(agg_type)\n\t\t\t\tax.axis('off')\n\t\t\t\n\t\t\tt = 'only_clear=%s' % str(only_clear)\n\t\t\tif only_clear:\n\t\t\t\tt += ', fill_obscured=%s' % str(fill_obscured)\n\t\t\tfig.suptitle(t)\n\t\t\tfig.tight_layout()\n\t\n\treturn figs\n\n\n# [============================================================================]\n\n\ndef highres_image(path, img_as_float=True):\n\t\"\"\"\n\tLoad a scene's high resolution image and its corresponding status map.\n\t\n\tReturns a `(hr, sm)` tuple, where:\n\t* `hr`: matrix with the loaded high-resolution image (values as np.uint16 or\n\t        np.float64 depending on `img_as_float`),\n\t* `sm`: the image's corresponding \"clear pixel?\" boolean mask.\n\t\n\tScenes' image files are described at:\n\thttps:\/\/kelvins.esa.int\/proba-v-super-resolution\/data\/\n\t\"\"\"\n\tpath = path if path[-1] in {'\/', '\\\\'} else (path + '\/')\n\thr = skimage.io.imread(path + 'HR.png')\n\tsm = skimage.io.imread(path + 'SM.png')\n\tif img_as_float:\n\t\thr = skimage.img_as_float64(hr)\n\treturn (hr, sm)\n\t\n\n\ndef lowres_image_iterator(path, img_as_float=True):\n\t\"\"\"\n\tIterator over all of a scene's low-resolution images (LR*.png) and their\n\tcorresponding status maps (QM*.png).\n\t\n\tReturns at each iteration a `(l, c)` tuple, where:\n\t* `l`: matrix with the loaded low-resolution image (values as np.uint16 or\n\t       np.float64 depending on `img_as_float`),\n\t* `c`: the image's corresponding \"clear pixel?\" boolean mask.\n\t\n\tScenes' image files are described at:\n\thttps:\/\/kelvins.esa.int\/proba-v-super-resolution\/data\/\n\t\"\"\"\n\tpath = path if path[-1] in {'\/', '\\\\'} else (path + '\/')\n\tfor f in glob(path + 'LR*.png'):\n\t\tq = f.replace('LR', 'QM')\n\t\tl = skimage.io.imread(f)\n\t\tc = skimage.io.imread(q)\n\t\tif img_as_float:\n\t\t\tl = skimage.img_as_float64(l)\n\t\tyield (l, c)\n\t\n\n\n# [============================================================================]\n\n\ndef check_img_as_float(img, validate=True):\n\t\"\"\"\n\tEnsure `img` is a matrix of values in floating point format in [0.0, 1.0].\n\tReturns `img` if it already obeys those requirements, otherwise converts it.\n\t\"\"\"\n\tif not issubclass(img.dtype.type, np.floating):\n\t\timg = skimage.img_as_float64(img)\n\t# https:\/\/scikit-image.org\/docs\/dev\/api\/skimage.html#img-as-float64\n\t\n\tif validate:\n\t\t# safeguard against unwanted conversions to values outside the\n\t\t# [0.0, 1.0] range (would happen if `img` had signed values).\n\t\tassert img.min() >= 0.0 and img.max() <= 1.0\n\t\n\treturn img\n\t\n\n\n# [============================================================================]\n\n\ndef all_scenes_paths(base_path):\n\t\"\"\"\n\tGenerate a list of the paths to all scenes available under `base_path`.\n\t\"\"\"\n\tbase_path = base_path if base_path[-1] in {'\/', '\\\\'} else (base_path + '\/')\n\treturn [\n\t\tbase_path + c + s\n\t\tfor c in ['RED\/', 'NIR\/']\n\t\tfor s in sorted(os.listdir(base_path + c))\n\t\t]\n\t\n\n\ndef scene_id(scene_path, incl_channel=False):\n\t\"\"\"\n\tExtract from a scene's path its unique identifier.\n\t\n\tExamples\n\t--------\n\t>>> scene_id('probav\/train\/RED\/imgset0559\/')\n\t'imgset0559'\n\t>>> scene_id('probav\/train\/RED\/imgset0559', incl_channel=True)\n\t'RED\/imgset0559'\n\t\"\"\"\n\tsep = os.path.normpath(scene_path).split(os.sep)\n\tif incl_channel:\n\t\treturn '\/'.join(sep[-2:])\n\telse:\n\t\treturn sep[-1]\n\t\n\n\n# [============================================================================]\n\n\ndef prepare_submission(images, scenes, subm_fname='submission.zip'):\n\t\"\"\"\n\tPrepare a set of images for submission.\n\t\n\tGiven a list of `images` (as matrices of shape (384, 384)), and the paths\n\tto the `scenes` to which they correspond, write a zip file containing all\n\timages as .png files, named after their scene's identification\n\t(example: imgset1160.png).\n\t\"\"\"\n\tassert len(images) == 290, '%d images provided, 290 expected.' % len(images)\n\tassert len(images) == len(scenes), \"Mismatch in number of images and scenes.\"\n\tassert subm_fname[-4:] == '.zip'\n\t\n\t# specific warnings we wish to ignore\n\twarns = [\n\t\t'tmp.png is a low contrast image',\n\t\t'Possible precision loss when converting from float64 to uint16']\n\t\n\twith np.warnings.catch_warnings():\n\t\tfor w in warns:\n\t\t\tnp.warnings.filterwarnings('ignore', w)\n\t\t\n\t\tprint('Preparing submission. Writing to \"%s\".' % subm_fname)\n\t\t\n\t\twith ZipFile(subm_fname, mode='w') as zf:\n\t\t\t\n\t\t\tfor img, scene in zip(tqdm(images), scenes):\n\t\t\t\tassert img.shape == (384, 384), \\\n\t\t\t\t\t'Wrong dimensions in image for scene %s.' % scene\n\t\t\t\t\n\t\t\t\tskimage.io.imsave('tmp.png', img)\n\t\t\t\tzf.write('tmp.png', arcname=scene_id(scene) + '.png')\n\t\t\n\t\tos.remove('tmp.png')\n        \n# Baseline cPSNR values for the dataset's images. Used for normalizing scores.\n# (provided by the competition's organizers)\nbaseline_cPSNR = pd.read_csv('\/kaggle\/input\/probav-superresolution\/probav_data\/norm.csv',\n\tnames = ['scene', 'cPSNR'],\n\tindex_col = 'scene',\n\tsep = ' ')\n\t\n\n\n# [============================================================================]\n\n\ndef score_images(imgs, scenes_paths, *args):\n\t\"\"\"\n\tMeasure the overall (mean) score across multiple super-resolved images.\n\t\n\tTakes as input a sequence of images (`imgs`), a sequence with the paths to\n\tthe corresponding scenes (`scenes_paths`), and optionally a sequence of\n\t(hr, sm) tuples with the pre-loaded high-resolution images of those scenes.\n\t\"\"\"\n\treturn np.mean([\n#\t\tscore_image(*i)\n\t\tscore_image_fast(*i)\n\t\tfor i in zip(tqdm(imgs), scenes_paths, *args)\n\t\t])\n\t\n\n\ndef score_image(sr, scene_path, hr_sm=None):\n\t\"\"\"\n\tCalculate the individual score (cPSNR, clear Peak Signal to Noise Ratio) for\n\t`sr`, a super-resolved image from the scene at `scene_path`.\n\t\n\tParameters\n\t----------\n\tsr : matrix of shape 384x384\n\t\tsuper-resolved image.\n\tscene_path : str\n\t\tpath where the scene's corresponding high-resolution image can be found.\n\thr_sm : tuple, optional\n\t\tthe scene's high resolution image and its status map. Loaded if `None`.\n\t\"\"\"\n\thr, sm = highres_image(scene_path) if hr_sm is None else hr_sm\n\t\n\t# \"We assume that the pixel-intensities are represented\n\t# as real numbers \u2208 [0,1] for any given image.\"\n\tsr = check_img_as_float(sr)\n\thr = check_img_as_float(hr, validate=False)\n\t\n\t# \"Let N(HR) be the baseline cPSNR of image HR as found in the file norm.csv.\"\n\tN = baseline_cPSNR.loc[scene_id(scene_path)][0]\n\t\n\t# \"To compensate for pixel-shifts, the submitted images are\n\t# cropped by a 3 pixel border, resulting in a 378x378 format.\"\n\tsr_crop = sr[3 : -3, 3 : -3]\n\t\n\tcrop_scores = []\n\t\n\tfor (hr_crop, sm_crop) in hr_crops(hr, sm):\n\t\t# values at the cropped versions of each image that\n\t\t# fall in clear pixels of the cropped `hr` image\n\t\t_hr = hr_crop[sm_crop]\n\t\t_sr = sr_crop[sm_crop]\n\t\t\n\t\t# \"we first compute the bias in brightness b\"\n\t\tpixel_diff = _hr - _sr\n\t\tb = np.mean(pixel_diff)\n\t\t\n\t\t# \"Next, we compute the corrected clear mean-square\n\t\t# error cMSE of SR w.r.t. HR_{u,v}\"\n\t\tpixel_diff -= b\n\t\tcMSE = np.mean(pixel_diff * pixel_diff)\n\t\t\n\t\t# \"which results in a clear Peak Signal to Noise Ratio of\"\n\t\tcPSNR = -10. * np.log10(cMSE)\n\t\t\n\t\t# normalized cPSNR\n\t\tcrop_scores.append(N \/ cPSNR)\n#\t\tcrop_scores.append(cMSE)\n\t\n\t# \"The individual score for image SR is\"\n\tsr_score = min(crop_scores)\n#\tsr_score = N \/ (-10. * np.log10(min(crop_scores)))\n\t\n\treturn sr_score\n\t\n\n\n# [===================================]\n\n\ndef hr_crops(hr, sm):\n\t\"\"\"\n\t\"We denote the cropped 378x378 images as follows: for all u,v \u2208 {0,\u2026,6},\n\tHR_{u,v} is the subimage of HR with its upper left corner at coordinates\n\t(u,v) and its lower right corner at (378+u, 378+v).\"\n\t-- https:\/\/kelvins.esa.int\/proba-v-super-resolution\/scoring\/\n\t\"\"\"\n\tnum_cropped = 6\n\tmax_u, max_v = np.array(hr.shape) - num_cropped\n\t\n\tfor u in range(num_cropped + 1):\n\t\tfor v in range(num_cropped + 1):\n\t\t\tyield hr[u : max_u + u, v : max_v + v], \\\n\t\t\t\t  sm[u : max_u + u, v : max_v + v]\n\t\n\n\n# [============================================================================]\n\n\ndef score_image_fast(sr, scene_path, hr_sm=None):\n\t\"\"\"\n\tCalculate the individual score (cPSNR, clear Peak Signal to Noise Ratio) for\n\t`sr`, a super-resolved image from the scene at `scene_path`.\n\t\n\tParameters\n\t----------\n\tsr : matrix of shape 384x384\n\t\tsuper-resolved image.\n\tscene_path : str\n\t\tpath where the scene's corresponding high-resolution image can be found.\n\thr_sm : tuple, optional\n\t\tthe scene's high resolution image and its status map. Loaded if `None`.\n\t\"\"\"\n\thr, sm = highres_image(scene_path) if hr_sm is None else hr_sm\n\t\n\t# \"We assume that the pixel-intensities are represented\n\t# as real numbers \u2208 [0,1] for any given image.\"\n\tsr = check_img_as_float(sr)\n\thr = check_img_as_float(hr, validate=False)\n\t\n\t# \"Let N(HR) be the baseline cPSNR of image HR as found in the file norm.csv.\"\n\tN = baseline_cPSNR.loc[scene_id(scene_path)][0]\n\t\n\treturn score_against_hr(sr, hr, sm, N)\n\t\n\n\n@numba.jit('f8(f8[:,:], f8[:,:], b1[:,:], f8)', nopython=True, parallel=True)\ndef score_against_hr(sr, hr, sm, N):\n\t\"\"\"\n\tNumba-compiled version of the scoring function.\n\t\"\"\"\n\tnum_cropped = 6\n\tmax_u, max_v = np.array(hr.shape) - num_cropped\n\t\n\t# \"To compensate for pixel-shifts, the submitted images are\n\t# cropped by a 3 pixel border, resulting in a 378x378 format.\"\n\tc = num_cropped \/\/ 2\n\tsr_crop = sr[c : -c, c : -c].ravel()\n\t\n\t# create a copy of `hr` with NaNs at obscured pixels\n\t# (`flatten` used to bypass numba's indexing limitations)\n\thr_ = hr.flatten()\n\thr_[(~sm).ravel()] = np.nan\n\thr = hr_.reshape(hr.shape)\n\t\n#\tcrop_scores = []\n\tcMSEs = np.zeros((num_cropped + 1, num_cropped + 1), np.float64)\n\t\n\tfor u in numba.prange(num_cropped + 1):\n\t\tfor v in numba.prange(num_cropped + 1):\n\t\t\t\n\t\t\t# \"We denote the cropped 378x378 images as follows: for all u,v \u2208\n\t\t\t# {0,\u2026,6}, HR_{u,v} is the subimage of HR with its upper left corner\n\t\t\t# at coordinates (u,v) and its lower right corner at (378+u, 378+v)\"\n\t\t\thr_crop = hr[u : max_u + u, v : max_v + v].ravel()\n\t\t\t\n\t\t\t# \"we first compute the bias in brightness b\"\n\t\t\tpixel_diff = hr_crop - sr_crop\n\t\t\tb = np.nanmean(pixel_diff)\n\t\t\t\n\t\t\t# \"Next, we compute the corrected clear mean-square\n\t\t\t# error cMSE of SR w.r.t. HR_{u,v}\"\n\t\t\tpixel_diff -= b\n\t\t\tpixel_diff *= pixel_diff\n\t\t\tcMSE = np.nanmean(pixel_diff)\n\t\t\t\n\t\t\t# \"which results in a clear Peak Signal to Noise Ratio of\"\n#\t\t\tcPSNR = -10. * np.log10(cMSE)\n\t\t\t\n\t\t\t# normalized cPSNR\n#\t\t\tcrop_scores.append(N \/ cPSNR)\n\t\t\t\n\t\t\tcMSEs[u, v] = cMSE\n\t\n\t# \"The individual score for image SR is\"\n#\tsr_score = min(crop_scores)\n\tsr_score = N \/ (-10. * np.log10(cMSEs.min()))\n\t\n\treturn sr_score\n\t\n\n\n# [============================================================================]\n\n\nclass scorer(object):\n\t\n\tdef __init__(self, scene_paths, preload_hr=True):\n\t\t\"\"\"\n\t\tWrapper to `score_image()` that simplifies the scoring of multiple\n\t\tsuper-resolved images.\n\t\t\n\t\tThe scenes over which the scorer will operate should be given in\n\t\t`scene_paths`. This is either a sequence of paths to a subset of scenes\n\t\tor a string with a single path. In this case, it is interpreted as the\n\t\tbase path to the full dataset, and `all_scenes_paths()` will be used to\n\t\tlocate all the scenes it contains.\n\t\t\n\t\tScene paths are stored in the object's `.paths` variable.\n\t\tWhen scoring, only the super-resolved images need to be provided.\n\t\tThey are assumed to be in the same order as the scenes in `.paths`.\n\t\t\n\t\tIf the object is instantiated with `preload_hr=True` (the default),\n\t\tall scene's high-resolution images and their status maps will be\n\t\tpreloaded. When scoring they will be sent to `score_image()`, thus\n\t\tsaving computation time in repeated scoring, at the expense of memory.\n\t\t\"\"\"\n\t\tif isinstance(scene_paths, str):\n\t\t\tself.paths = all_scenes_paths(scene_paths)\n\t\telse:\n\t\t\tself.paths = scene_paths\n\t\t\n\t\tself.hr_sm = [] if not preload_hr else [\n\t\t\thighres_image(scn_path, img_as_float=True)\n\t\t\tfor scn_path in tqdm(self.paths, desc='Preloading hi-res images')]\n\t\t\n\t\tself.scores = []\n\t\t\n\t\n\tdef __call__(self, sr_imgs, per_image=False, progbar=True, desc=''):\n\t\t\"\"\"\n\t\tScore all the given super-resolved images (`sr_imgs`), which correspond\n\t\tto the scenes at the matching positions of the object's `.paths`.\n\t\t\n\t\tReturns the overall score (mean normalized cPSNR).\n\t\t\n\t\tAn additional value is returned if `per_image=True`: a list with each\n\t\timage's individual cPSNR score. In either case, this list remains\n\t\tavailable in the object's `.scores` variable until the next call.\n\t\t\"\"\"\n\t\tscenes_paths = tqdm(self.paths, desc=desc) if progbar else self.paths\n\t\thr_sm = [] if self.hr_sm == [] else [self.hr_sm]\n\t\t\n\t\tself.scores = [\n#\t\t\tscore_image(*i)\n\t\t\tscore_image_fast(*i)\n\t\t\tfor i in zip(sr_imgs, scenes_paths, *hr_sm)]\n\t\t\n\t\tassert len(self.scores) == len(self.paths)\n\t\t\n\t\tscore = np.mean(self.scores)\n\t\t\n\t\tif per_image:\n\t\t\treturn score, self.scores\n\t\telse:\n\t\t\treturn score\n        \nimport skimage\n\n\n\n# [============================================================================]\n\n\ndef bicubic_upscaling(img):\n\t\"\"\"\n\tCompute a bicubic upscaling by a factor of 3.\n\t\"\"\"\n\tr = skimage.transform.rescale(img, scale=3, order=3, mode='edge',\n\t                              anti_aliasing=False, multichannel=False)\n\t# NOTE: Don't change these options. They're required by `baseline_upscale`.\n\t# http:\/\/scikit-image.org\/docs\/dev\/api\/skimage.transform.html#skimage.transform.rescale\n\t# http:\/\/scikit-image.org\/docs\/dev\/api\/skimage.transform.html#skimage.transform.warp\n\treturn r\n\n\n\n\ndef scenes_paths(folder):\n    '''\n    This function returns the paths of all the files present in the given folder\n\n    Parameters:\n    ----------------------------------------------------------------------------\n    folder : String\n            folder path to all the scenes in the dataset\n    '''\n    if folder[-1:] == '\/':\n        folder = folder[:-1]\n    sub_folders = ['\/RED\/', '\/NIR\/'] #All scenes are divided in two folders namely RED and NIR\n    paths = []\n    for sub in sub_folders:\n        path = folder + sub\n        for scene in (os.listdir(path)):\n            paths.append(path + scene)\n    return paths\n\n\ndef get_lr_images(scene_path):\n    '''\n    This function returns an iterator for the low resolution images and their respective status masks for a given scene\n\n    Parameters:\n    ----------------------------------------------------------------------------\n    scene_path : String\n            path to scene to get get the low resolution images for\n    '''\n    lr_images = []\n    qm_images = []\n    for image_file_path in glob(scene_path + '\/LR*.png'):\n        mask_file_path = image_file_path.replace('LR', 'QM')\n        lr_images.append(cv2.imread(image_file_path, 0))\n        qm_images.append(cv2.imread(mask_file_path, 0))\n    lr_images = lr_images[:9]\n    qm_images = qm_images[:9]\n    return(lr_images, qm_images)\n\n\n\n\ndef get_hr_image(scene_path):\n    '''\n    This function returns the high resolution image and its respective status mask foe the given scene\n\n    Parameters:\n    ----------------------------------------------------------------------------\n    scene_path : String\n            path to scene to get high resolution image for\n    '''\n    hr_image = cv2.imread(scene_path + '\/HR.png', 0)\n    sm_image = cv2.imread(scene_path + '\/SM.png', 0)\n    return (hr_image, sm_image)","ac3fed84":"# save the train images directory, which is all we will be using for our model as we don't have high resolution images \n# for the images present in test dataset\n\ntrain_path = '\/kaggle\/input\/probav-superresolution\/probav_data\/train'\n\n# Extract all the scenes paths and make a list\ntrain = all_scenes_paths(train_path)\n\nbaseName = '\/kaggle\/input\/probav-superresolution\/probav_data\/'","4a8bfaf6":"# Get the high resolution image and its mask for the scene '0348'\nhr, sm = highres_image(baseName + 'train\/RED\/imgset0348')\n\n\n#### Plot the high resolution image along with the mask of the same scne hr image '0348'\n\n# HR image\nfig = plt.figure(figsize=(11,11))\nax1 = fig.add_subplot(121)\nax1.imshow(hr)\nax1.set_title('High-Resolution Image')\nax1.axis('off')\n\n#HR image mask\nax2 = fig.add_subplot(122)\nax2.imshow(sm)\nax2.axis('off')\nax2.set_title('Image Clearance Mask')\nplt.tight_layout()","54dd8cd3":"#### Plot the low resolution images present fot the given scene ex. scene '0348'\n\nfig = plt.figure(figsize=(12, 6))\n\ni = 0\n\n# Extract the low resolution images and their masks, and display only the low-resolution images\nfor (lr, sm) in lowres_image_iterator(baseName + 'train\/RED\/imgset0348'):\n    \n    i += 1\n    ax = fig.add_subplot(2, 4, i)\n    ax.axis('off')\n    \n    ax.imshow(lr, vmin=0, vmax=0.2)\n    \n    if i == 8: break\n\n# Eight images are plotted\nfig.tight_layout(pad=0)","62c726a7":"from IPython.display import Image\nImage(\"\/kaggle\/input\/probav-report-images\/model_structure.png\")\n","37a457b9":"from IPython.display import Image\nImage(\"\/kaggle\/input\/probav-report-images\/data.png\")","659ac430":"# We define an iterator for getting images from the directory\nfrom itertools import cycle\ndef get_scene_images():\n    '''\n    This is a generator function which loads all the low resolution images, low_resolution images mask, high resolution\n    image,and high-resolution image mask for a specific scene and then it creates a single image by combining all the low\n    resolution images using the median value for each pixel in all the low-resolution images.\n    \n    Returns: low-resolution \n    '''\n    all_scene_paths = scenes_paths('\/kaggle\/input\/probav-superresolution\/probav_data\/train')\n    for scene in all_scene_paths:\n        lr_images, qm_images = get_lr_images(scene)\n        lr_images = central_tendency(scene, agg_with='median', only_clear=False)\n        hr_image, sm_image = highres_image(scene)\n        yield(lr_images, qm_images, hr_image, sm_image)\n","553d880f":"# create an object of the generator get_scene_images()\nscene_images = get_scene_images()\n\n# create a cycle iterator so that it never gets exhausted\nscene_images = cycle(scene_images)\n\ndef get_next_batch():\n    \"\"\"\n    This method returns a new batch of 16 low-resolution and high-resolution images\n    \"\"\"\n    lr_images = []\n    hr_images = []  \n    for i in range(16):\n        # Get the images from the generator for the next scene\n        lr_image_stack,qm_images,hr_image,sm_image = next(scene_images)\n        lr_images.append(lr_image_stack)\n        hr_images.append(hr_image)\n    lr_images = np.asarray(lr_images)\n    hr_images = np.asarray(hr_images)\n    return (lr_images, hr_images)\n\n","907e8e27":"# Perform all the imports required for our implementation\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid","f800d742":"#Check if GPU is available in the system or not\ntorch.cuda.is_available()\n\n# set device to GPU if available otherwise set device to cpu only\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","1cb30780":"class SuperResolution(nn.Module):\n    \"\"\"\n    This is our network model class which defines the structure of our model and the forward function for training and testing \n    the data.\n    It has 7 Convolution layers and 1 Deconvolution layer\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.conv_1 = nn.Conv2d(in_channels = 1, out_channels = 128, kernel_size = 5, stride = 1, padding = 2)\n        self.conv_2 = nn.Conv2d(in_channels = 128, out_channels = 32, kernel_size = 1, stride = 1, padding = 0)\n        self.conv_3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv_5 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv_6 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv_7 = nn.Conv2d(in_channels = 32, out_channels = 128, kernel_size = 1, stride = 1, padding = 0)\n        self.de_conv_1 = nn.ConvTranspose2d(in_channels=128, out_channels=1, kernel_size=9, stride=3, padding=3, output_padding=0)\n        #self.de_conv_2 = nn.ConvTranspose2d(in_channels = 32, out_channels = 16, kernel_size = 3, stride=1, padding=2)\n\n    def forward(self, img):\n        out = F.relu(self.conv_1(img))\n        out = F.relu(self.conv_2(out))\n        out = F.relu(self.conv_3(out))\n        out = F.relu(self.conv_4(out))\n        out = F.relu(self.conv_5(out))\n        out = F.relu(self.conv_6(out))\n        out = F.relu(self.conv_7(out))\n        out = self.de_conv_1(out)\n        #out = F.relu(self.de_conv_2(out))\n        return out","d8f97677":"def get_optimizer(model, learning_rate):\n    '''\n    This method returns an optimizer (Adam) for a given model and given learning rate\n    '''\n    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n    return (optimizer)","97742a01":"def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    '''This method saves a checktoint of the model state if a new best is achieved'''\n    if is_best:\n        print (\"=> Saving a new best\")\n        torch.save(state, filename)  # save checkpoint\n    else:\n        pass","ca7979a4":"import time\n\ndef train(super_model, learning_rate, n_epochs, batch_size):\n    \"\"\"\n    This function is defined for pre-processing the data and training the model\n    \"\"\"\n    print (\"===============MODEL SPECIFICATIONS===============\")\n    print(\"Learning rate: {0}\".format(learning_rate))\n    print(\"Epochs: {0}\".format(n_epochs))\n    print(\"Batch size: {0}\".format(batch_size))\n   # print(summary(super_model,(9,128,128))) # Print the summary of all layers and parameters in the model\n    \n    optimizer = get_optimizer(super_model, learning_rate)\n    scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[200,300], gamma=0.1)\n    is_best = False\n    training_start_time = time.time()\n    criterion = nn.MSELoss()\n    best_loss = 999999\n    for e in range(n_epochs):\n        print(\"[INFO]===================\")\n        print(\"Epoch: {0}\".format(e+1))\n        for i in tqdm(range(72)):\n            start_time = time.time()\n            lr_images, hr_images = get_next_batch()\n            lr_images = lr_images.reshape(16, 1,128,128)\n            hr_images = hr_images.reshape(16,1,384,384)\n            optimizer.zero_grad()\n            lr_images = torch.from_numpy(lr_images).float()\n            lr_images = lr_images.to(device)\n            output = super_model.forward(lr_images)\n            output = output.view(16,1,384,384)\n            hr_images = torch.from_numpy(hr_images).float()\n            hr_images = hr_images.to(device)\n            max_val = torch.max(output).item()\n            loss = criterion(output, hr_images)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        print(\"Current Loss: {0}\".format(loss))\n        if (loss < best_loss):\n#             print(\"We got a better loss value\")\n            is_best = True\n            best_loss = loss\n        \n        save_checkpoint({\n        'epoch': n_epochs + 1,\n        'state_dict': super_model.state_dict(),\n        'optimizer' : optimizer.state_dict(),}, is_best)\n        is_best = False\n    return best_loss","de6ad9aa":"# Create an object of the SuperResolution class we have defined (the neural net model class)\nsuper_model = SuperResolution()\nsuper_model.to(device)","6b5ef4a3":"# Call the train function on super_model object with learning_rate = 0.001, number_of_epochs = 200, batch_size = 16),\n# the train function return the final loss of the model\nmodel_loss = train(super_model,0.001, 250, 16)\n ","186c9c2a":"\ndef mse(img_1, img_2):\n    \"\"\"\n    This method calculates Mean Squared Error between two given images.\n    \"\"\"\n    diff = img_1 - img_2\n    return (np.sum(diff**2)\/(len(img_1)**2))\n\ndef get_bicubic_loss():\n    \"\"\"\n    This function calculates the MSE for bicubic interpolation of images for comparing this baseline with our model\n    \"\"\"\n    all_scenes = scenes_paths('\/kaggle\/input\/probav-superresolution\/probav_data\/train')\n    bicubic_loss = 0\n    for scene in all_scenes:\n        # Calculate the median of all low-resolution images to combine it into a single image\n        lr_images_t = central_tendency(scene, agg_with='median', only_clear=False)\n        lr_images_t = bicubic_upscaling(lr_images_t)\n        hr_img, sm_img = highres_image(scene)\n        # Find the maximum value in the mask and divide the whole array by that value for normalisation\n        sm_img = sm_img\/np.max(sm_img)\n        # Multiply both output image and target image by the mask so as to ignore those pixels for calculating MSE\n        hr_img = hr_img*sm_img\n        lr_images_t = lr_images_t*sm_img\n        bicubic_loss += mse(lr_images_t, hr_img)\n    # Average the MSE loss for all the scenes\n    bicubic_loss \/= len(all_scenes)\n    \n    return bicubic_loss","e111df5f":"# Call the get_bicubic_loss to calculate the bicubic interpolation MSE loss\nbicubic_loss = get_bicubic_loss()","7f2e0906":"import pandas as pd \n# intialise data of lists with loss values. \ndata = {'Method':['MSE Bicubic', 'MSE Super-Resolution Network'], 'MSE_loss':[bicubic_loss, model_loss.item()]}\n  \n# Create DataFrame \ndf = pd.DataFrame(data) \n\nplt.figure()\nsns.barplot(df['Method'], df['MSE_loss'], alpha=0.8)\nplt.title('Mean Squared Error Comparison')\nplt.ylabel('MSE ERROR', fontsize=12)\nplt.xlabel('Method for Super-Resolution', fontsize=12)\nplt.show()\n","1c4aa280":"def get_model_cpsnr():\n    all_test_scene_paths = scenes_paths('\/kaggle\/input\/probav-superresolution\/probav_data\/train')\n    total_cPSNR = 0\n    for scene in all_test_scene_paths:\n        lr_images = central_tendency(scene, agg_with='median', only_clear=False)\n        lr_images = lr_images.reshape(1, 1,128,128)\n        lr_images = torch.from_numpy(lr_images).float()\n        lr_images = lr_images.to(device)\n        test_output = super_model.forward(lr_images)\n        test_output = test_output.cpu()\n        test_output = test_output.detach().numpy()\n        test_output = test_output.reshape(384,384)\n        total_cPSNR += score_image(test_output, scene)\n    total_cPSNR \/= len(all_test_scene_paths)\n    return (total_cPSNR)\n\n#Call the get_cpsnr function to calculate the model cpsnr score\ncPSNR_score = get_model_cpsnr()\nprint('The cPSNR score of our model is: {0}'.format(cPSNR_score))","6a49f4e6":"# We define a scene images iterator\ntest_scene_images = get_scene_images()\n\n# We set it to cycle, so that its never exhausted and never returns an error to StopIteration\ntest_scene_images = cycle(test_scene_images)\n\ndef get_next_test_batch():\n    \"\"\"\n    This method gets the next batch of images from the dataset\n     \"\"\"\n    lr_images = []\n    hr_images = []  \n    for i in range(16):\n        # Get the images from the generator for the next scene\n        lr_image_stack,qm_images,hr_image,sm_image = next(test_scene_images)\n        lr_images.append(lr_image_stack)\n        hr_images.append(hr_image)\n    lr_images = np.asarray(lr_images)\n    hr_images = np.asarray(hr_images)\n    return (lr_images, hr_images)\n\n# Get the next batch of 16 images\nlr_test_images, hr_test_images = get_next_test_batch()\n\n# Reshape the image batch to feed it as input to the neuural network\nlr_test_images = lr_test_images.reshape(16, 1,128,128)\nhr_test_images = hr_test_images.reshape(16,1,384,384)\n\n# Convert the image batches into tensors\nlr_test_images = torch.from_numpy(lr_test_images).float()\nhr_test_images = torch.from_numpy(hr_test_images).float()\n\n# Transfer the data to CUDA device\nlr_test_images = lr_test_images.to(device)\nhr_test_images = hr_test_images.to(device)\n\n# Perform the forward pass of the model\ntest_output = super_model.forward(lr_test_images)\n","903962d7":"\"\"\"\nTranfer all data from GPU to CPU, then detach the variables from the graph so that we can perform operations on the data,\nannd finally rehape the data as required.\n\"\"\"\ntest_output = test_output.cpu()\ntest_output = test_output.detach().numpy()\ntest_output = test_output.reshape(16,384,384)\nhr_test_images = hr_test_images.cpu()\nhr_test_images = hr_test_images.detach().numpy()\nhr_test_images = hr_test_images.reshape(16,384,384)","e7851404":"###############################################################################################################\n# PLOT SOME OF THE EXAMPLES FROM THE DATASET, THE SUPER-RESOLVED IMAGE AND HIGH-RESOLUTION IMAGE SIDE BY SIDE.#\n###############################################################################################################\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(11,11))\nax1 = fig.add_subplot(121)\nax1.imshow(test_output[4])\nax1.axis('off')\nax1.set_title('Super-Resolution Image')\n\nax2 = fig.add_subplot(122)\nax2.imshow(hr_test_images[4])\nax2.axis('off')\nax2.set_title('High-Resolution Image')\nplt.tight_layout()\n\n","1336d9a8":"fig = plt.figure(figsize=(11,11))\nax1 = fig.add_subplot(121)\nax1.imshow(test_output[9])\nax1.set_title('Super-Resolution Image')\n\nax2 = fig.add_subplot(122)\nax2.imshow(hr_test_images[9])\nax2.axis('off')\nax2.set_title('High-Resolution Image')\nplt.tight_layout()","ce13476d":"# HR image\nfig = plt.figure(figsize=(11,11))\nax1 = fig.add_subplot(121)\nax1.imshow(test_output[1])\nax1.axis('off')\nax1.set_title('Super-Resolution Image')\n\n#HR image mask\nax2 = fig.add_subplot(122)\nax2.imshow(hr_test_images[1])\nax2.axis('off')\nax2.set_title('High-Resolution Image')\nplt.tight_layout()","47dd484c":"***NOTE:***  I also tried a **different method**, shown on the left side in the image below for **super-resolution that is purely multi-image,** but **unfortunately I was not able to get expected results** due to time contraints and system constraints which made me stop doing other experiments.  \nThe method is like this:  \n\n* Instead of using a median image as input, I tried to use multiple low-resolution images(9 because atleast 9 images in each scene are  available) by **stacking them on top of each other as channels**, and then feeding it to the model.  \n* The model then perform multiple convolutions on the image stack and learn features from all the channels(images), which eventually removes all the artifacts and faults in the image.\n* Then we combine all the channels into a single channel image.\n* Finally, we perform a **deconvolution** on the single channel image we obtained from the previous layer, to scale the image by the erquired factor.","10a29ce1":"# Contents\n\n### [1. Introduction](Introduction)\n* [Spot-Vegetation](Spot-V)  \n* [Proba-V](Proba-V)  \n* [Super Resolution](Super Resolution)\n\n### [2. Problem Statement](Problem)  \n\n### [3. Data](Data)  \n* [Exploratory Data Analysis](EDA)  \n\n### [4. Analysis](Analysis)  \n### [5. Implementation](Implementation)  \n\n### [6. FSRCNN Implementation](FSRCNN)\n### [7. Results](Results)\n### [8. Conclusion](Conclusion)","9c55c065":"<a id=\"Data\"><\/a>","e3c135d7":"### 1.3 Super Resolution\n\nSuper resolution is the process of upscaling and or improving the details within an image. Often a low resolution image is taken as an input and the same image is upscaled to a higher resolution, which is the output. The details in the high resolution output are filled in where the details are essentially unknown.\n\nSuper resolution has wide scope in so many different fields including satellite imagery, microscopes, and telescopes. There is so much of old low-resolution data available that be used by performing super resolution and finding new insights from the data, which will save cost. Therefore, this task of super-resolution can have great impact on a lot of industries and hence this needs to be studied and more researched upon. \n\nThe below image is taken from kelvins proba-v challenge [website.](https:\/\/kelvins.esa.int\/proba-v-super-resolution\/home\/)","67acfad5":"<a id=\"MultiISR\"><\/a>","ef73a064":"### 1.2 PROBA-V\n**PROBA-V** (Project for On-Board Autonomy \u2013 Vegetation) is an ESA (European Space\nAgency) mission developed within the framework of the Agency\u2019s General Support\nTechnology Programme (GSTP) devoted to the **observation of the Earth\u2019s vegetation**.The Proba-V mission provides multispectral images to study the evolution of the vegetation cover on a daily and global basis. The 'V' stands for Vegetation. It is a miniaturised satellite tasked with a full-scale mission: to map land cover and vegetation growth across the entire planet every two days. It was initiated by the Space and Aeronautics department of the BELgian Science Policy Office launched into space from ELA-1( *Ensemble de Lancement Ariane* 1 ) at Guiana Space Centre on board Vega flight VV02, on 7 May 2013. It is built by QinetiQ Space N.V. and operated by European Space Agency and uses a PROBA platform. It was launched to support **applications such as land use, worldwide vegetation classification, crop monitoring, famine prediction, food security, disaster monitoring and biosphere studies**.\n\n\n\nThe main PROBA-V payload is the Vegetation sensor: a multispectral pushbroom spectrometer with 4 spectral bands and with a very large swath of 2285 km to guarantee daily coverage above 35\u00b0 latitude.\n\nThe payload consists of 3 identical cameras, each with a very compact Three Mirror Anastigmat (TMA) telescope. Each TMA, having a FOV of 34\u00b0, contains **4 spectral bands: 3 bands in the visible range (Blue, Red and NIR) and one band in the SWIR spectral range**. The swath Total FOV is 103\u00b0.\n\n","d855c8e8":"Below, we show some of the predicted images alongside with the original images.","117fb39b":"![](http:\/\/)The image below shows **two different methods** I thought I could follow to get our task done. In my implementation, I have used the method on **right** i.e. I have combined all the low resolution in a single image by taking median of each pixel in the image, which gave us a cloud artifacts free image.\n","aa93131c":"<a id='EDA'><\/a>","a3bfd850":"# Data\n\n#### Origin of the data\nWe collected satellite data from the PROBA-V mission of the European Space Agency from 74 hand-selected regions around the globe at different points in time. The data is composed of radiometrically and geometrically corrected Top-Of-Atmosphere (TOA) reflectances for the RED and NIR spectral bands at 300m and 100m resolution in Plate Carr\u00e9e projection. The 300m resolution data is delivered as 128x128 grey-scale pixel images, the 100m resolution data as 384x384 grey-scale pixel images. The bit-depth of the images is 14, but they are saved in a 16-bit .png-format (which makes them look relatively dark if opened in typical image viewers).\n\nEach image comes with a quality map, indicating which pixels in the image are concealed (i.e. clouds, cloud shadows, ice, water, missing, etc) and which should be considered clear. For an image to be included in the dataset, at least 75% of its pixels have to be clear for 100m resolution images, and 60% for 300m resolution images. Each data-point consists of exactly one 100m resolution image and several 300m resolution images from the same scene. In total, the dataset contains 1450 scenes, which are split into 1160 scenes for training and 290 scenes for testing. On average, each scene comes with 19 different low resolution images and always with at least 9. We expect you to submit a 384x384 image for each of the 290 test-scenes, for which we will not provide a high resolution image.\n\n#### Changes in landscape\nThe images have been recorded within a time window of 30 days. As such, the scenes are not always permanent but can change marginally.\n\n#### Image registration\nThe images are delivered as recorded by PROBA-V and have not been modified to align with each other. While the mean geolocation accuracy of PROBA-V is about 61m, (sub)-pixelshifts in the content occur and are - in a sense - necessary to perform super-resolution. Thus, the dataset itself is not corrected for these shifts and none of the images are registered to each other. The scoring will take pixel-shifts of the high resolution image into account, as registration cannot be computed by the participant for the test-set.\n\nSource: [https:\/\/kelvins.esa.int\/proba-v-super-resolution\/data\/](https:\/\/kelvins.esa.int\/proba-v-super-resolution\/data\/)","ee94f8a1":"**Since our models' cPSNR score is less than 1, we can say for sure that this model performs good at creating super-resolution images using multiple low-resolution images, and it also beats the baseline method.**","933e6702":"<a id=\"Introduction\" ><\/a>","3f5176dc":"Next, we call the train() function to start training the model with our passed parameters as shown below.  \n\nThe train function returns the best loss value it acquired while training and saves the model to local disk","5a04ad11":"# Results\n\nWe have our model loss (MSE) in the 'model_loss' variable and we have the bicubic interpolation MSE loss stored in 'bicubic_loss' variable- Bicubic interpolation is the baseline method for super-resolution defined by the challenge posters.\nWe plot a graph below, of both the losses to compare their values.","1542359b":"<a id=\"Problem\" ><\/a>","1e52c37b":"Now, we create an object of our model class 'SuperResolution' and transfer the models parameters to the CUDA device","6ec45b9c":"## Utility Code","3f35b58c":"\n# Introduction\n\n","60478704":"<a id=\"Results\"><\/a>","bc9328db":"Next, we define some of the methods and calculate the MSE for bicubic upsampling method.","036d5657":"# FSRCNN Implementation","670519a7":"# Conclusion\n* The model we trained above gave us better results than the baseline bicubic interpolation method. Although, we combined multiple images by taking their median and forming a single image and then creating a super resolution image, it is not truly a **multi image super-resolution** algorithm, which the challenge demanded.  \n\n* I tried to create a model for multi-image super-resolution as I have explained in my analysis above [[1]](MultiISR), but unfortunately due to time and system capability contraints, I was to not able to make it succesfully work, although I believe that that the method will work given some more in depth study and experimenting.  \n\n* Also, I think GAN's are good for this kind of tasks and if we go in that direction and do some kind to gradual increase in the resolution of the low-resolution image to high-resolution image, we might be able to get even better results than state-of-the-art techniques, as GANs are very good at creating synthetic data, and that is the requirement of super-resolution as we need to fill in those pixel in super-resolution images which were not actually present.","b9667fff":"# Implementation","1eb67832":"But the challenge website tells us that they have defined another scoring technique named cPSNR(clear Peak-Signal Noise Ratio) which we have use for scoring our model.  \nThey have provided the baseline bicubic interpolation cPSNR score in a file named 'norm.csv', using which we have to normalize our cPSNR score to get the final cPSNR score.\n\nIf our final cPSNR value is less than 1, then we can say for sure that our model is better than baseline bicubic interpolation method. So below we score our model with cPSNR score.","db7e7706":"We see from the above plot that the MSE of our 'model' is less than the 'bicubic interpolation' MSE, and as we know that MSE is the value of average of difference between each pixel value between two images (the smaller, the better), therefore we can say that our 'model' performs better than the bicubic interpolation method-which is ouur baseline method, in estimating the target image as its MSE is lower than bicubic interpolation MSE, which means the difference between the pixels of super-resolution image predicted by the model and the and original high-resolution image is very less.\n\n**Hence we can say that our model performs better, and beats the baseline method, which is the aim of this challenge.**","ce15aaf8":"# Analysis\n\nFirst, I needed some domain knowledge of how super-resolution works. Although I knew about what super-resolution is and have thought about how low resoltuion images can be converted to high resolution images, I needed some technical details on how super-resolution can be performed and does it match with my previous hypothesis that I have maintained in my head, and therefore first I watched some videos on super-resolution to understand the technical details better and I also read some blogs on the topic.  \n* [Deep Learning for Image Super-Resolution](https:\/\/www.youtube.com\/watch?v=ppsqbOL073U&t=61s)\n* [Enhance! Super Resolution From Google | Two Minute Papers #124](https:\/\/www.youtube.com\/watch?v=WovbLx8C0yA)\n* [An Introduction to Super Resolution using Deep Learning](https:\/\/medium.com\/beyondminds\/an-introduction-to-super-resolution-using-deep-learning-f60aff9a499d)\n* [Deconvolution](youtube.com\/watch?v=Xk7myx9_OmU)  \n* [Deconvolutional Microscopy](https:\/\/www.youtube.com\/watch?v=MJ2FOFqr7hw&t=12s)\n\nThere are mainly two different types of methods of super-resolution that can be performed, they are:\n1. Single Image Super-Resolution\n1. Multi Image Super-Resolution\n\n\nAfter my analysis to understand the problem and techniques to solve it, I went through some of the latest research papers in the domain.  \n\nFSRCNN, SRGAN, CSFM, and the winning model of the Kelvin PROBA-V Challenge **DeepSUM**.  \n\n* DeepSUM is large and difficult to implement in the given timeframe, and hence I didn't dig deep into it.\n\n\nThese two below models are the one I studied in deep because they covered two different aspects and methods of doing super-resolution.\n\n| Model | Technique | Date Released |\n| --- | --- | --- |\n| FSRCNN | Fast Super-Resolution Convolutional Neural Network | 2016 |  \n| SRGAN | Super-Resolution Generative Adversarial Network | 2017 |  \n\n\n  \n  These two above methods are single image super resolution methods, which we modify a little bit to work for completing our task. We will be constructing a model based on FSRCNN, with some minor changes.  \n\n#### FSRCNN\n\nFast Super-Resolution CNN is an advanced method to speed up the SRCNN super-resolution method. **In case of SRCNN** the input images need to be upscaled by the required factor before feeding it to the neural network and then consecutive convolutions are done to enhance the image quality to match the target high-resolution image. This upscaling method and then enhancing the quality takes high amount of time. To address this time problem, FSRCNN came into existance; FSRCNN doesn't upscale  the input image initally and input the low-resolution image to the network as it is, then multiple convolutions are done on the image: **Feature Exraction, Shrinking, Mapping, Expanding** and finally a **Deconvolution** is performed on the image (Transpose Convolution actually) to upscale the image by the given factor.\n","48728cc8":"### 1.1 SPOT-VEGETATION\nThe overall objectives of the \"VEGETATION\" systems is to provide accurate measurements of basic characteristics of vegetation canopies on an operational basis,\n* Either for scientific studies involving both regional and global scales experiments over long time periods (for example development of models of the biosphere dynamics interacting with climate models), \n* Or for systems designed to monitor important vegetation resources, like crops, pastures and forests.","473b4d3f":"<a id=\"Conclusion\"><\/a>\n","18f87114":"# Problem Statement\n\nPROBA-V payload sensors allow for an almost global coverage (90%) per day, providing 300m resolution images. PROBA-V also provides 100m \"high resolution\" images, but at a lower frequency, of roughly every 5 days (dependent on the location).\n\nThe goal of this challenge is to construct such high-resolution images by fusion of the more frequent 300m images. This process, which is known as Multi-image Super-resolution has already been applied to satellite before: some of these satellites, such as SPOT-VGT or ZY-3 TLC, have payloads that are able to take multiple images of the same area during a single satellite pass, which creates a most favorable condition for the use of super resolution algorithms as images are taken simultaneously. Thus, the super resolution can already be included in the final product of these satellites during post-processing on the ground. However, this is not the case for PROBA-V or other satellites, which could benefit from a post-acquisition enhancement. In those cases, multiple images from the same patch are still available, but originate from successive revisits over longer periods of time.\n\nThus, PROBA-Vs products represent a convenient way to explore super-resolution in a relevant setting. The images provided for this challenge are not artificially degraded, but are real images recorded from the very same scene, just at different resolutions and different times. Any improvements on this data-set might be transferable to larger collections of remote sensing data without the need to deploy more expensive sensors or satellites, as resolution enhancement can happen post-acquisition.  \n  \n  Source: [Kelvins-PROBA-V Challenge](https:\/\/kelvins.esa.int\/proba-v-super-resolution\/problem\/).  \n  \nThe image shown below depicts the task that we need to perform.","c23a1a28":"### Exploratory  Data Analysis\n\n\nThere are atmost 19 low-resolution images and atleast 9 low-resolution images available for every given scene in the dataset, along with their **clearance mask** which tells us, which pixels in the image are obscured due to some artifact or some glitch. For every scene, we are also given a high-Resolution image(our target image 'HR.png') and a clearance map for the high-resolution image.\n\nBelow we display some examples of all low-resolution, high-resolution images, and their clearance masks."}}