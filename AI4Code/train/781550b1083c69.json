{"cell_type":{"4bc86bd8":"code","0fa852c0":"code","8276fb59":"code","65a69044":"code","88abae2c":"code","491eb7d0":"code","4ff82008":"code","5d999628":"code","e11d6cde":"code","4b639590":"code","48736679":"code","d8f0f58d":"code","8be46fa4":"code","dc6bb409":"code","991ccc8a":"code","e59aed33":"code","6aa9919f":"code","821a0885":"code","237c4163":"code","fe1a0e84":"code","18781a4a":"code","1735a6dc":"code","5773ad3a":"code","f64d5447":"code","0607600a":"code","1d7c333d":"code","0c3ddd3f":"code","d6236dc0":"code","2243d72e":"code","b81462b5":"code","6119b175":"code","70f0fffc":"code","b4dadaba":"code","cc237188":"code","55fdafa7":"code","579d53e2":"code","43dc0bb3":"code","14737ee1":"code","308162f7":"code","18583ea1":"code","fb5ebaa3":"code","55e48966":"code","6443813b":"code","e31f4600":"code","332f510c":"code","7b5c63cc":"code","5bd99724":"code","94779837":"code","bbac7a7a":"code","ac483306":"code","4d07d03a":"code","57a70040":"code","b3cacac5":"code","c8f5ed10":"code","71e16971":"code","f74f7694":"markdown","d9c20c0e":"markdown","2d947932":"markdown","0392f36b":"markdown","0deadd8b":"markdown","e97ca3e6":"markdown","1792861e":"markdown","3957be2c":"markdown","089fd415":"markdown","fcad42b9":"markdown","64093f1e":"markdown","b9a1c54f":"markdown","2615fca5":"markdown","848170f3":"markdown","a1ee8db5":"markdown","426ebbed":"markdown","6ea4ed9c":"markdown","0b30615d":"markdown","97f9d616":"markdown","80428272":"markdown","9c8b3036":"markdown","cd4600a8":"markdown","47138cf0":"markdown","c93eb358":"markdown","85c89c3b":"markdown","168cc832":"markdown","391781b0":"markdown","5ff21356":"markdown","ff4cfe7c":"markdown","b28d3744":"markdown","c589b50f":"markdown","c0104635":"markdown","24b7c522":"markdown","4034b034":"markdown","57cd22dc":"markdown","f939f88d":"markdown","48fc4122":"markdown","49b87fea":"markdown","9e51cb55":"markdown","039336e9":"markdown"},"source":{"4bc86bd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0fa852c0":"from IPython.display import Image,display","8276fb59":"# Import numpy and pandas\nimport numpy as np\nimport pandas as pd\n# Import data visualization libraries\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","65a69044":"import matplotlib.style\nimport matplotlib as mpl\nmpl.style.use('classic')\nsns.set_style('whitegrid')","88abae2c":"# Read the data from the csv file\niris = pd.read_csv(\"..\/input\/iris\/Iris.csv\")","491eb7d0":"# Understand the datatypes in iris dataset\niris.info()","4ff82008":"# Check the missing values.\niris.isnull().sum()","5d999628":"# Quick look to data\niris.head()","e11d6cde":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nplt.hist(x='SepalLengthCm',data=iris,bins=15)\nplt.title('Distribution of Sepal Length(cm)')\nplt.subplot(2,2,2)\nplt.hist(x='SepalWidthCm',data=iris,bins=15)\nplt.title('Distribution of Sepal Width(cm)')\nplt.subplot(2,2,3)\nplt.hist(x='PetalLengthCm',data=iris,bins=15)\nplt.title('Distribution of Petal Length(cm)')\nplt.subplot(2,2,4)\nplt.hist(x='PetalWidthCm',data=iris,bins=15)\nplt.title('Distribution of Petal Width(cm)')","4b639590":"# Countplot of Species\nsns.countplot(iris['Species'],palette='Blues_r')\nplt.title('Distribution of Species')","48736679":"sns.jointplot(x='SepalLengthCm',y='SepalWidthCm',data=iris)","d8f0f58d":"sns.jointplot(x='PetalLengthCm',y='PetalWidthCm',data=iris)","8be46fa4":"plt.figure(figsize=(12,10))\nplt.subplot(2,2,1)\nsns.boxplot(x='Species',y='SepalLengthCm',data=iris)\nplt.subplot(2,2,2)\nsns.boxplot(x='Species',y='SepalWidthCm',data=iris)\nplt.subplot(2,2,3)\nsns.boxplot(x='Species',y='PetalLengthCm',data=iris)\nplt.subplot(2,2,4)\nsns.boxplot(x='Species',y='PetalWidthCm',data=iris)","dc6bb409":"sns.lmplot(x='PetalLengthCm',y='PetalWidthCm',data=iris,hue='Species')","991ccc8a":"sns.lmplot(x='SepalLengthCm',y='SepalWidthCm',data=iris,hue='Species')","e59aed33":"sns.pairplot(data=iris,hue='Species',diag_kind='kde',kind='scatter')","6aa9919f":"sns.heatmap(data=iris.corr(),cmap='viridis',annot=True)","821a0885":"X=iris[['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']] #features","237c4163":"y=iris['Species'] #target variable","fe1a0e84":"from sklearn.model_selection import train_test_split","18781a4a":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","1735a6dc":"from sklearn.linear_model import LogisticRegression #lodistic regression algorithm\nfrom sklearn.neighbors import KNeighborsClassifier #Kneighbors algorithm\nfrom sklearn.svm import SVC #support vector machine algorithm\nfrom sklearn.tree import DecisionTreeClassifier #Decision tree algorithm\nfrom sklearn.ensemble import RandomForestClassifier #Random forest algorithm\nfrom sklearn import metrics #cheking the model accuracy","5773ad3a":"print(X_train.shape)\nprint(X_test.shape)","f64d5447":"X_train.head()","0607600a":"y_train.head()","1d7c333d":"LR=LogisticRegression() # create a logistic regression model ","0c3ddd3f":"LR.fit(X_train,y_train) # fit our model using training data","d6236dc0":"predictions=LR.predict(X_test)  #now we pass the testing data to the trained algorithm","2243d72e":"print('The accuracy of the Logistic Regression is',metrics.accuracy_score(predictions,y_test))","b81462b5":"from sklearn.metrics import classification_report","6119b175":"print(classification_report(y_test,predictions))","70f0fffc":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","b4dadaba":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","cc237188":"KNN=KNeighborsClassifier(n_neighbors=3) # Number of neighbors=3","55fdafa7":"KNN.fit(X_train,y_train)","579d53e2":"predictions=KNN.predict(X_test)","43dc0bb3":"print('The accuracy of the KNN is:',metrics.accuracy_score(predictions,y_test))","14737ee1":"from sklearn.metrics import classification_report","308162f7":"print(classification_report(y_test,predictions))","18583ea1":"SVM=SVC()","fb5ebaa3":"SVM.fit(X_train,y_train)","55e48966":"predictions=SVM.predict(X_test)","6443813b":"print('The accuracy of the SVM is:',metrics.accuracy_score(predictions,y_test))","e31f4600":"tree=DecisionTreeClassifier()","332f510c":"tree.fit(X_train,y_train)","7b5c63cc":"predictions=tree.predict(X_test)","5bd99724":"print('The accuracy of the Decision Tree is:',metrics.accuracy_score(predictions,y_test))","94779837":"from sklearn.metrics import classification_report","bbac7a7a":"print(classification_report(y_test,predictions))","ac483306":"forest=RandomForestClassifier()","4d07d03a":"forest.fit(X_train,y_train)","57a70040":"predictions=forest.predict(X_test)","b3cacac5":"print('The accuracy of the Random Forest is:',metrics.accuracy_score(predictions,y_test))","c8f5ed10":"from sklearn.metrics import classification_report","71e16971":"print(classification_report(y_test,predictions))","f74f7694":"We can easily differentiate setosa based on Sepal but for versicolor and virginica its difficult because the data is scattred.","d9c20c0e":"#### Before Starting:\nIf you liked this kernel please don't forget to upvote and comment the project, this will keep me motivated to other kernels in the future. I hope you enjoy my exploration into this dataset. Let's begin!","2d947932":"\n\nAgain based on petal we can easily classify setosa and for versicolor and virginica also we can classify but there is a thin line which should be taken care of","0392f36b":"### 4.2 Bivariate Analysis","0deadd8b":"From the plot, we can see that the species setosa is separataed from the other two across all feature combinations","e97ca3e6":"## 2. Objectives","1792861e":"## 6. Conclusions","3957be2c":"#### Overall, the measurement that seems to be most unique among the flowers are their petal lengths. When constructing the machine learning models, there should be high accuracy when predicting the Setosa flowers. However, there may be mistakes when predicting the Versicolor and Virginica Flowers","089fd415":"### 1. Logistic Regression\n","fcad42b9":"We can see higher correlations between : Sepal Length and Petal Length, Sepal Length and Petal Width, Petal Length and Petal width.\n\nPoor Correlations between : Sepal Length and width, Sepal Width and Sepal Width","64093f1e":"\n\nTop two boxplots show how sepal measurements changes with species. The Virginica flowers tend to have larger sepal lengths and the Setosa flowers tend to have larger sepal widths when compared to the other flowers. On the other hand, Versicolor and Virginica tend to have similar measurements when compared to each other.\nThe remaining boxplots show how petal measurements changes with species. The Virginica flowers tend to have larger petal lengths and widths when compared to other flowers. The graphs show that the Setosa species have extremely low petal lengths when compared to the other species.","b9a1c54f":"### 4.3 Multivariate Analysis","2615fca5":"### 5.4 Checking the accuracy of models","848170f3":"## Leave any comment or feedback\n\nThanks!","a1ee8db5":"### Heat Map","426ebbed":"### 5. Random Forest","6ea4ed9c":"### 5.1 Split the data into training and testing\n","0b30615d":"The Iris dataset was used in R.A. Fisher's classic 1936 paper. This dataset consist with 6 variables with 150 observations.\nThe variables in this dataset are:\n\n1.Id - Continuous\n\n2.SepalLengthCm - Continuous\n\n3.SepalWidthCm - Continuous\n\n4.PetalLengthCm - Continuous\n\n5.PetalWidthCm - Continuous\n\n6.Species - Categorical\n\nThere are 3 types of species called 'Iris-setosa', 'Iris-virginica', and 'Iris-versicolor' with 50 sample each as well as some properties about each flower.","97f9d616":"When it comes to the evaluation of your classifier, there are several different ways you can measure its performance.\n\n1. Classification Accuracy\n\n2. Classification Report\n\n3. Confusion Matrix\n\n4. Area Under ROC Curve (AUC)\n\n5. Logarithmic Loss\n\nClassification Accuracy is the simplest out of all the methods of evaluating the accuracy, and the most commonly used. Classification accuracy is simply the number of correct predictions divided by all predictions or a ratio of correct predictions to total predictions.\nWhile it can give you a quick idea of how your classifier is performing, it is best used when the number of observations\/examples in each class is roughly equivalent. Because this doesn't happen very often, you're probably better off using another metric.\n\nThe classification report is a Scikit-Learn built in metric created especially for classification problems. Using the classification report can give you a quick intuition of how your model is performing. Recall pits the number of examples your model labeled as Class A (some given class) against the total number of examples of Class A, and this is represented in the report.\nThe report also returns prediction and f1-score. Precision is the percentage of examples your model labeled as Class A which actually belonged to Class A (true positives against false positives), and f1-score is an average of precision and recall.","80428272":"Irrespective to the species, there is a strong relationship between petal length and petal width. When the petal length increases petal width also tend to be increase.","9c8b3036":"Sepal Length of most of the Species vary between 4.5cm and 7cm and Sepal Width of most of the Species vary between 2.5cm and 3.5cm","cd4600a8":"\n1. Explore the dataset and find basic relationships of different features (Exploratory Data Anlysis)\n2. Predict the given sepal and petal dimensions follows to which type of species.(Machine Learning Algorithms)","47138cf0":"There are no missing values available in iris dataset","c93eb358":"## 3. Basic Description of Data","85c89c3b":"![image.png](attachment:image.png)","168cc832":"#### The optimal models were selected using the largest value for accuracy. Testing the predictions made based on the KNN and SVM Models yielded results that were 100% accurate. If there was a different seed set, then the models may have produced less accurate results. Although these results are satisfying, it is useful to compare this models with another one.\u00b6","391781b0":"Irrespective to the species, there is no any considerable relationship between the sepal length and width.","5ff21356":"### Pair Plot","ff4cfe7c":"### 3. Suport Vector Machine\n","b28d3744":"## 5. Machine Learning","c589b50f":"### 2. K-Nearest Neighbours","c0104635":"### 4.1 Univariate Analysis","24b7c522":"In the end, both the SVM and the KNN Model seem to be good models for predicting the species of an iris based on its sepal width, sepal length, petal length and petal width. The exploratory analysis shows several differences between the species. These difference may have been helpful in building an accurate model. Unfortunately some explainability is lost with these models, but the models being highly accurate makes up for this loss. It would be interesting to see how this model performs on a dataset containing different measurements for these variables.","4034b034":"## 1. Introduction","57cd22dc":"### 4. Decision Tree","f939f88d":"### 5.3 Importing the all necessary packages to use the various classification algorithms","48fc4122":"From the output we can see that the mean error is zero when the value of the K is 1,3,5,6,9,10. We can play around with the value of K to see how it impacts the accuracy of the predictions. I used K=3 here.","49b87fea":"## 4. Exploratory Data Analysis[](http:\/\/)","9e51cb55":"### 5.2 Train test split","039336e9":"\nThis graph shows the distribution of three iris species. It is clear that all three iris species are equally distributed."}}