{"cell_type":{"60d48ada":"code","c417d0fa":"code","78415675":"code","32c56112":"code","a3796aa0":"code","9264eff1":"code","b4b3afca":"code","8d9cf272":"code","27acadf2":"code","03ab79d8":"code","793cf9ea":"code","adde10f3":"code","bfaa59ee":"code","cf51c51f":"code","262793ea":"code","54b954e5":"code","80c0e933":"code","8d69f6c5":"code","4c24c53b":"code","7aeecc06":"code","9ad5cbf0":"code","6b930185":"code","67a04ce8":"code","8680ba6e":"code","b6662b63":"code","c208cd43":"code","387fc2e7":"code","c952e454":"code","b9a5b4e1":"code","c0a75d08":"code","6a865b52":"code","a60ac6c8":"code","83a3896d":"code","cc049cdb":"code","e03e2122":"code","08d53bf8":"code","0e9c7e00":"code","043500a0":"code","9bff37f1":"code","053a6f24":"code","488ba253":"code","5627d37c":"code","064478d1":"markdown","761e3326":"markdown","8a13e22b":"markdown","e08b658f":"markdown","667c732f":"markdown","eb8f955b":"markdown","6fa145f8":"markdown","357fdfed":"markdown","5536f6cd":"markdown","a58d8837":"markdown","ae5bef9a":"markdown","565c7b5b":"markdown","d50e4015":"markdown","14ad6e85":"markdown"},"source":{"60d48ada":"import pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain.head()","c417d0fa":"print(set(train[\"wheezy-copper-turtle-magic\"]))","78415675":"train_1 = train.loc[train[\"wheezy-copper-turtle-magic\"]==1,].copy()\ntest_1 = test.loc[test[\"wheezy-copper-turtle-magic\"]==1,].copy()\ntrain_1.shape","32c56112":"test_1.shape","a3796aa0":"train_1.head()","9264eff1":"Y = train_1[\"target\"]\nX = train_1.drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX.shape","b4b3afca":"import numpy as np\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nmodel = RandomForestClassifier(n_estimators=10)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","8d9cf272":"import seaborn as sns\n\nsns.boxplot(x=scores_rf)","27acadf2":"model = RandomForestClassifier(n_estimators=100)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","03ab79d8":"model = RandomForestClassifier(n_estimators=200)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","793cf9ea":"model = RandomForestClassifier(n_estimators=400)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","adde10f3":"model = RandomForestClassifier(n_estimators=600)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","bfaa59ee":"from sklearn import svm\n\nmodel = svm.SVC()\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","cf51c51f":"model","262793ea":"model = svm.SVC(kernel='linear')\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","54b954e5":"model = svm.SVC(kernel='poly')\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","80c0e933":"model = svm.SVC(kernel='poly', degree=2)\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","8d69f6c5":"model = svm.SVC(kernel='poly', degree=4)\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","4c24c53b":"model = svm.SVC(kernel='poly', degree=5)\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","7aeecc06":"model = svm.SVC(kernel='poly', degree=4)\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\n\nsns.boxplot(x=scores_svm)","9ad5cbf0":"columns = train.columns[1:-1]\n\nfirst_name = [i.split(\"-\")[0] for i in columns]\nprint(set(first_name))\nprint(len(first_name))\nprint(len(set(first_name)))\nfor first in first_name:\n    filter_col = [col for col in train_1 if col.startswith(first)]\n    test_1.loc[:, first+\"-mean\"] = test_1.loc[:, filter_col].mean(axis=1)\n    train_1.loc[:, first+\"-mean\"] = train_1.loc[:, filter_col].mean(axis=1)\n    test_1.loc[:, first+\"-std\"] = test_1.loc[:, filter_col].std(axis=1)\n    train_1.loc[:, first+\"-std\"] = train_1.loc[:, filter_col].std(axis=1)","6b930185":"second_name = [i.split(\"-\")[1] for i in columns]\nprint(set(second_name))\nprint(len(second_name))\nprint(len(set(second_name)))\nfor second in second_name:\n    filter_col = [col for col in columns if second==col.split(\"-\")[1]]\n    test_1[second+\"-mean\"] = test_1.loc[:, filter_col].mean(axis=1)\n    train_1[second+\"-mean\"] = train_1.loc[:, filter_col].mean(axis=1)\n    test_1[second+\"-std\"] = test_1.loc[:, filter_col].std(axis=1)\n    train_1[second+\"-std\"] = train_1.loc[:, filter_col].std(axis=1)","67a04ce8":"third_name = [i.split(\"-\")[2] for i in columns]\nprint(set(third_name))\nprint(len(third_name))\nprint(len(set(third_name)))\nfor third in third_name:\n    filter_col = [col for col in columns if third==col.split(\"-\")[1]]\n    test_1[third+\"-mean\"] = test_1.loc[:, filter_col].mean(axis=1)\n    train_1[third+\"-mean\"] = train_1.loc[:, filter_col].mean(axis=1)\n    test_1[third+\"-std\"] = test_1.loc[:, filter_col].std(axis=1)\n    train_1[third+\"-std\"] = train_1.loc[:, filter_col].std(axis=1)","8680ba6e":"fourth_name = [i.split(\"-\")[3] for i in columns]\nprint(set(fourth_name))\nprint(len(fourth_name))\nprint(len(set(fourth_name)))\nfor fourth in fourth_name:\n    filter_col = [col for col in columns if fourth==col.split(\"-\")[1]]\n    test_1[fourth+\"-mean\"] = test_1.loc[:, filter_col].mean(axis=1)\n    train_1[fourth+\"-mean\"] = train_1.loc[:, filter_col].mean(axis=1)\n    test_1[fourth+\"-std\"] = test_1.loc[:, filter_col].std(axis=1)\n    train_1[fourth+\"-std\"] = train_1.loc[:, filter_col].std(axis=1)","b6662b63":"for col in train_1.columns:\n    if (train_1[col].isnull().sum()>0):\n        train_1.drop([col], axis=1, inplace=True)\n        test_1.drop([col], axis=1, inplace=True)\n\ntrain_1.shape","c208cd43":"test_1.shape","387fc2e7":"Y = train_1[\"target\"]\nX = train_1.drop([\"id\", \"target\", \"wheezy-copper-turtle-magic\"], axis=1)\nX.shape\n\nmodel = svm.SVC(kernel='poly', degree=4)\nscores_svm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","c952e454":"model = RandomForestClassifier(n_estimators=400)\nscores_rf = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","b9a5b4e1":"model = RandomForestClassifier(n_estimators=400)\nmodel.fit(X,Y)\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))","c0a75d08":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier(njobs=-1)\nscores_xgb = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_xgb), \"+\/-\", np.std(scores_xgb))","6a865b52":"model","a60ac6c8":"model = XGBClassifier(learning_rate=0.01, n_estimators=1000)\nscores_xgb = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_xgb), \"+\/-\", np.std(scores_xgb))","83a3896d":"from lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier(njobs=-1)\nscores_gbm = cross_val_score(model, X, Y, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_gbm), \"+\/-\", np.std(scores_gbm))","cc049cdb":"model","e03e2122":"X2 = test_1.drop([\"id\", \"wheezy-copper-turtle-magic\"], axis=1)\nX2.shape","08d53bf8":"X.shape","0e9c7e00":"adv = pd.concat((X, X2), axis=0)\nadv.head()","043500a0":"adv.shape","9bff37f1":"label = [\"0\"]*510+[\"1\"]*250\n\nmodel = svm.SVC(kernel='poly', degree=4)\nscores_svm = cross_val_score(model, adv, label, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(scores_svm)\nprint(np.mean(scores_svm), \"+\/-\", np.std(scores_svm))","053a6f24":"model = RandomForestClassifier(n_estimators=400)\nscores_rf = cross_val_score(model, adv, label, cv=10, n_jobs=-1, scoring='roc_auc')\nprint(np.mean(scores_rf), \"+\/-\", np.std(scores_rf))","488ba253":"model = RandomForestClassifier(n_estimators=400).fit(adv, label)\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfor f in range(adv.shape[1]):\n    print(\"%d. feature %s (%f)\" % (f + 1, adv.columns[indices[f]], importances[indices[f]]))","5627d37c":"sns.distplot(adv.loc[:, \"crappy-carmine-eagle-entropy\"][0:510])\nsns.distplot(adv.loc[:, \"crappy-carmine-eagle-entropy\"][510:760])","064478d1":"## Adversarial validation\n\nCan I [train a classifier to distinguish between train\/test](http:\/\/fastml.com\/adversarial-validation-part-one\/)? If that is the case, the relevant features are thos that are \"different\" between train and test. A value of the AUC of 0.5 would lead to the conclusion of no significant difference between train\/test, a value close to 1 as train and test sets radically different.","761e3326":"# The strategy\n\nFollowing many different source ([Chris Deotte's post](https:\/\/www.kaggle.com\/c\/instant-gratification\/discussion\/92930#latest-534847), [Chris Deotte's kernel](https:\/\/www.kaggle.com\/cdeotte\/support-vector-machine-0-925)), a distinct model is built for each distinct value of variable $\\mathrm{wheezy-copper-turtle-magic}$. Let's start with model 0.","8a13e22b":"## Adding Features","e08b658f":"## Model 0","667c732f":"# Traing Lightgbm","eb8f955b":"Some keywords appear just once, so they can be dropped:","6fa145f8":"## Retrained a Random Forest model","357fdfed":"## Retrained a SVM model","5536f6cd":"## Train Xgboost","a58d8837":"The number of observations is comparable to the number of columns: this require some classifiers like Random Forest or SVM. Preparing data:","ae5bef9a":"Trying with SVM:","565c7b5b":"# Import data","d50e4015":"Thus the most promising approach is to use a polynomial of degree 4 as kernel.","14ad6e85":"The difference does not seems to be significant."}}