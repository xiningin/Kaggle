{"cell_type":{"9c307722":"code","ad128b1e":"code","b0203607":"code","d35fc8a9":"code","a1961605":"code","eaf97245":"code","684d7ef0":"code","d68cd59e":"code","6a11b4a2":"code","cdd085e6":"code","31f86df7":"code","91cb2934":"code","50762bb2":"code","f40bb9bf":"code","f6d490fe":"code","e05aafe9":"code","9ebbd827":"code","79db06d7":"markdown"},"source":{"9c307722":"import numpy as np  \nimport pandas as pd \nimport re           \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords   \nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")\n\nprint(tf.__version__)","ad128b1e":"# import attention.py\n\nimport tensorflow as tf\nimport os\nfrom tensorflow.python.keras.layers import Layer\nfrom tensorflow.python.keras import backend as K\n\nclass AttentionLayer(Layer):\n    \"\"\"\n    This class implements Bahdanau attention (https:\/\/arxiv.org\/pdf\/1409.0473.pdf).\n    There are three sets of weights introduced W_a, U_a, and V_a\n     \"\"\"\n\n    def __init__(self, **kwargs):\n        super(AttentionLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert isinstance(input_shape, list)\n        # Create a trainable weight variable for this layer.\n\n        self.W_a = self.add_weight(name='W_a',\n                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.U_a = self.add_weight(name='U_a',\n                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n                                   initializer='uniform',\n                                   trainable=True)\n        self.V_a = self.add_weight(name='V_a',\n                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n                                   initializer='uniform',\n                                   trainable=True)\n\n        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n\n    def call(self, inputs, verbose=False):\n        \"\"\"\n        inputs: [encoder_output_sequence, decoder_output_sequence]\n        \"\"\"\n        assert type(inputs) == list\n        encoder_out_seq, decoder_out_seq = inputs\n        if verbose:\n            print('encoder_out_seq>', encoder_out_seq.shape)\n            print('decoder_out_seq>', decoder_out_seq.shape)\n\n        def energy_step(inputs, states):\n            \"\"\" Step function for computing energy for a single decoder state\n            inputs: (batchsize * 1 * de_in_dim)\n            states: (batchsize * 1 * de_latent_dim)\n            \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            \"\"\" Some parameters required for shaping tensors\"\"\"\n            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n            de_hidden = inputs.shape[-1]\n\n            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n            # <= batch size * en_seq_len * latent_dim\n            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n\n            \"\"\" Computing hj.Ua \"\"\"\n            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n            if verbose:\n                print('Ua.h>', U_a_dot_h.shape)\n\n            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n            # <= batch_size*en_seq_len, latent_dim\n            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n            if verbose:\n                print('Ws+Uh>', Ws_plus_Uh.shape)\n\n            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n            # <= batch_size, en_seq_len\n            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n            # <= batch_size, en_seq_len\n            e_i = K.softmax(e_i)\n\n            if verbose:\n                print('ei>', e_i.shape)\n\n            return e_i, [e_i]\n\n        def context_step(inputs, states):\n            \"\"\" Step function for computing ci using ei \"\"\"\n\n            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n\n            # <= batch_size, hidden_size\n            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n            if verbose:\n                print('ci>', c_i.shape)\n            return c_i, [c_i]\n\n        fake_state_c = K.sum(encoder_out_seq, axis=1)\n        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n\n        \"\"\" Computing energy outputs \"\"\"\n        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n        last_out, e_outputs, _ = K.rnn(\n            energy_step, decoder_out_seq, [fake_state_e],\n        )\n\n        \"\"\" Computing context vectors \"\"\"\n        last_out, c_outputs, _ = K.rnn(\n            context_step, e_outputs, [fake_state_c],\n        )\n\n        return c_outputs, e_outputs\n\n    def compute_output_shape(self, input_shape):\n        \"\"\" Outputs produced by the layer \"\"\"\n        return [\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n        ]","b0203607":"df = pd.read_csv(\"..\/input\/scl-2021-ds\/train.csv\")\ntest = pd.read_csv(\"..\/input\/scl-2021-ds\/test.csv\")\n\nprint(len(df))\nprint(len(test))","d35fc8a9":"# Feature Engineering\n\ndf['POI'] = df['POI\/street'].str.extract(r'(.*)\/', expand=True)\ndf['street'] = df['POI\/street'].str.extract(r'\/(.*)', expand=True)\ndf.sample(10)","a1961605":"# filter out all POI = null\ndf = df[df['POI'] != '']\n\n\ndf['POI'] = df['POI'].apply(lambda x : '_START_ '+ x + ' _END_')\ndf['street'] = df['street'].apply(lambda x : '_START_ '+ x + ' _END_')\n\ndf.sample(5)","eaf97245":"# Train test split\n\nmsk = np.random.rand(len(df)) < 0.9\n\ntrain = df[msk]\nval = df[~msk]\n\nx_tr = train['raw_address']\ny1_tr = train['POI']\ny2_tr = train['street']\n\nx_val = val['raw_address']\ny1_val = val['POI']\ny2_val = val['street']\n\nprint(len(train), len(val))","684d7ef0":"max_len_text=20 \nmax_len_summary=5\n\n#prepare a tokenizer for reviews on training data\nx_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_tr))\n\n#convert text sequences into integer sequences\nx_tr    =   x_tokenizer.texts_to_sequences(x_tr) \nx_val   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \nx_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n\nx_voc_size   =  len(x_tokenizer.word_index) +1","d68cd59e":"#preparing a tokenizer for summary on training data \ny1_tokenizer = Tokenizer()\ny1_tokenizer.fit_on_texts(list(y1_tr))\n\n#convert summary sequences into integer sequences\ny1_tr    =   y1_tokenizer.texts_to_sequences(y1_tr) \ny1_val   =   y1_tokenizer.texts_to_sequences(y1_val) \n\n#padding zero upto maximum length\ny1_tr    =   pad_sequences(y1_tr, maxlen=max_len_summary, padding='post')\ny1_val   =   pad_sequences(y1_val, maxlen=max_len_summary, padding='post')\n\ny1_voc_size  =   len(y1_tokenizer.word_index) +1","6a11b4a2":"from keras import backend as K \nK.clear_session() \nlatent_dim = 100 #500 \n\n# Encoder \nencoder_inputs = Input(shape=(max_len_text,)) \nenc_emb = Embedding(x_voc_size, latent_dim,trainable=True)(encoder_inputs) \n\n#LSTM 1 \nencoder_lstm1 = LSTM(latent_dim,return_sequences=True, return_state=True) \nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n\n#LSTM 2 \nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n\n#LSTM 3 \nencoder_lstm3 = LSTM(latent_dim, return_state=True, return_sequences=True) \nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n\n# Set up the decoder. \ndecoder_inputs = Input(shape=(None,)) \ndec_emb_layer = Embedding(y1_voc_size, latent_dim,trainable=True) \ndec_emb = dec_emb_layer(decoder_inputs) \n\n#LSTM using encoder_states as initial state\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \ndecoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n\n#Attention Layer\n\nattn_layer = AttentionLayer(name='attention_layer') \nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n\n# Concat attention output and decoder LSTM output \ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n#Dense layer\ndecoder_dense = TimeDistributed(Dense(y1_voc_size, activation='softmax')) \ndecoder_outputs = decoder_dense(decoder_concat_input) \n\n# Define the model\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs) \nmodel.summary()","cdd085e6":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')","31f86df7":"es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)","91cb2934":"epoch = 10\n\nhistory=model.fit(x=[x_tr, y1_tr[:,:-1]], \n                  y=y1_tr.reshape(y1_tr.shape[0], y1_tr.shape[1], 1)[:,1:], \n                  epochs=epoch,\n                  callbacks=[es],\n                  batch_size=512, \n                  validation_data=([x_val,y1_val[:,:-1]], y1_val.reshape(y1_val.shape[0],y1_val.shape[1], 1)[:,1:]))\n","50762bb2":"reverse_target_word_index=y1_tokenizer.index_word \nreverse_source_word_index=x_tokenizer.index_word \ntarget_word_index=y1_tokenizer.word_index\n\nreverse_target_word_index[0] = ''","f40bb9bf":"# encoder inference\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# decoder inference\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_len_text,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs)\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","f6d490fe":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['start']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='end'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'end'  or len(decoded_sentence.split()) >= (max_len_summary-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","e05aafe9":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","9ebbd827":"for i in range(50):\n    print(\"Review:\",seq2text(x_val[i]))\n    print(\"Original POI:\",seq2summary(y1_val[i]))\n    print(\"Predicted POI:\", decode_sequence(x_val[i].reshape(1,max_len_text)))\n    print(\"\\n\")","79db06d7":"For sharing and learning. \n\nSeq2Seq. The intuition here is that since we need to map the short form of a word to its long form. Instead of mapping them manually, maybe the neural network could learn the mapping and generate the correct POI and street names. \n\nDidn't get to make seq2seq work. \n\nReference:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/06\/comprehensive-guide-text-summarization-using-deep-learning-python\/"}}