{"cell_type":{"573fbb10":"code","6967caca":"code","e1ae01b5":"code","8a193709":"code","70617358":"code","83ae81fa":"code","6b50ea92":"code","8d80dbc9":"code","6b8e75c9":"markdown","c8280f98":"markdown","9d220025":"markdown","93126769":"markdown","3370425e":"markdown","0e2b03e5":"markdown","87384440":"markdown"},"source":{"573fbb10":"!pip install xlrd","6967caca":"!pip install AutoViz","e1ae01b5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime as dt\nfrom typing import Tuple, List, Dict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline\n\n\n# read data\nin_kaggle = True\n\ndef get_data_file_path(is_in_kaggle: bool) -> Tuple[str, str, str]:\n    train_path = ''\n    test_path = ''\n    sample_submission_path = ''\n\n    if is_in_kaggle:\n        # running in Kaggle, inside the competition\n        train_path = '..\/input\/tabular-playground-series-feb-2021\/train.csv'\n        test_path = '..\/input\/tabular-playground-series-feb-2021\/test.csv'\n        sample_submission_path = '..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv'\n    else:\n        # running locally\n        train_path = 'data\/train.csv'\n        test_path = 'data\/test.csv'\n        sample_submission_path = 'data\/sample_submission.csv'\n\n    return train_path, test_path, sample_submission_path\n\n","8a193709":"# main flow\nstart_time = dt.datetime.now()\nprint(\"Started at \", start_time)","70617358":"%%time\n# get the training set and labels\ntrain_set_path, test_set_path, sample_subm_path = get_data_file_path(in_kaggle)\n\ndf_train = pd.read_csv(train_set_path)\ndf_test = pd.read_csv(test_set_path)\n\nsubm = pd.read_csv(sample_subm_path)","83ae81fa":"df_train.info()","6b50ea92":"\nfrom autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='target', \n    dfte=df_train, \n    header=0, \n    verbose=2, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)\n","8d80dbc9":"print('We are done. That is all, folks!')\nfinish_time = dt.datetime.now()\nprint(\"Finished at \", finish_time)\nelapsed = finish_time - start_time\nprint(\"Elapsed time: \", elapsed)","6b8e75c9":"# Basic Data Overview","c8280f98":"# Initial Preparations\n\nWe are going to start with the essential pre-requisites as follows\n\n- installing *AutoViz* into this notebook\n- importing the standard Python packages we need to use down the road","9d220025":"## Express Analysis of Training Set","93126769":"# Introduction\n\nThis notebook is intended to extract useful insights for the datasets of \u2018Tabular Playground Series - Feb 2021\u2019 competition in Kaggle. For this competition, it is required to tackle the Regression problem to predict a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous.\n\nWe are going to perform the complete and comprehensive EDA as follows\n-\tAutomate the generic aspects of EDA with AutoViz, one of the leading freeware Rapid EDA tools in Pythonic Data Science world\n-\tDeep into the problem-specific advanced analytical questions\/discoveries with the custom manual EDA routines programmed on top of standard capabilities of Plotly and Matplotlib\n","3370425e":"# Roadmap For Additional EDA Visualizations\n\nThe good insights we quickly got from the express EDA Analysis with *AutoViz* above were very helpful per se. However, they did not address all and every analytical issues we would like to address, when tackling the fundumantal question of what the impact of features on the *target* are.\n\nNow we are going to undertake the additional manual EDA discoveries to review\n\n- pair associations between the selective cat variables\n- multi-variative associations between selective cat variables, factored by the impact of such association on the conditional distributions of the *target* and numeric features on the training set\n\nWhile doing it, we will be paying the most attention to the cat features highlighted in the express EDA analysis above. These are\n\n- *cat2, cat5, cat6, cat7, and cat9* that have a good association with *target* on the training set\n- *cat8* that has good association with every feature variable both in the training and test sets","0e2b03e5":"## Express Analysis Insights\n\nAs we can see, the simple express EDA analysis yielded a lot of useful insights out of the box, in less then 20 minutes of the data crunching. Below are the key finding from the charts generated by *AutoViz* on a generic basis.\n\n### Feature-to-Target Relations\n\nWe find that the training set data manifests the following relations between the *target* and feature variables\n\n- It seems like the training set observations with *target* < 3.5 or\/and cont5 < 0.1 could be clearly attributed to as outliers\n- Target variable is a little skewed to the right\n- There is no any numeric feature that is highly correlated with *target*\n- *target* distribution by the labels of the respective cat variables demonstrated that there is a relatively huge association of the *target* with *cat2, cat5, cat6, cat7, cat9*\n- The best association between the *target* values and the cat labels is demonstrated by cat7\n- In turn, there is a weaker association of the target variable with the rest of categorical features\n\n\n### Numeric Feature Findings\n\nIt is demonstrated that\n\n- There is a clear separation of the observations in the training and test sets into well-contained and well separable clusters by the values of *cont1* (6-8 clusters observed, subject to further clustering experiments)\n- Distribution of the continual variables is identic on both the training and testing sets (the details for each variables are provided below)\n- *cont3, cont4, cont5, cont6*, and *cont12* are highly skewed to the left \n- *cont8, cont9, cont10, cont11*, and *cont13* have a polynomial distribution (binomial distribution, presumably)\n- *cont1* and *cont11* are skewed to the right\n- *cont0, cont2* have almost normal distribution\n- as per the review of the respective violin plots on the training set, it could be possible to use the extreme tail values of *cont0, cont5, cont6*, and *cont12* for the outlier removal when training the ML models on the training set\n- there are several quite highly correlated numeric feature pairs detected on the training and test sets (with the Pierson\u2019s correlation coefficient >= 0.6): *cat5-cat8, cat5-cat9*, and *cat5-cat12* (among them, *cat5* has the highest absolute correlation with the target variable on the training set)\n\n\n### Categorical Feature Findings\n\nIt has been detected that\n\n- *cat0* is a two-label categorical variable, and it is unbalance by the label value distribution on the training set (\u2018A\u2019 drastically predominates \u2018B\u2019)\n- *cat1* is a two-label categorical variable, and it is unbalanced a little (\u2018A\u2019 vs. \u2018B\u2019)\n- *cat2* is a two-label categorical variable, and it is unbalance by the label value distribution on the training set (\u2018A\u2019 drastically predominates \u2018B\u2019)\n- *cat3* is a four-label categorical variable, and two of its labels (\u2018C\u2019, \u2018A\u2019) predominate the rest of the labels (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat4* is a four-label categorical variable, and one of its labels (\u2018A\u2019) predominates others (such labels can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat5* is a four-label categorical variable, and two of its labels (\u2018B\u2019, \u2018D\u2019) predominate the rest of the labels (the latter ones can be binned into a single category label \u2018Other\u2019, cont reduce the dimensionality of the respective feature space)\n- *cat6* is an eight-label categorical variable, and one of its labels (\u2018E\u2019) predominates others (such labels can be binned into a single category label \u2018Other\u2019, to reduce the\n- *cat8* is a 7-label categorical variable, and 4 of its categories (\u2018C\u2019, \u2018E\u2019, \u2018G\u2019, and \u2018A\u2019) predominate the rest of the categories on the training set (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n- *cat9* is a 15-label categorical variable, and 3 of its labels (\u2018F\u2019, \u2018I\u2019, and \u2018L\u2019) predominate others (the latter ones can be binned into a single category label \u2018Other\u2019, to reduce the dimensionality of the respective feature space)\n\n### Categorical-to-Numerical Feature Associations\n\nThere are quite strong associations found between the following categorical and numerical features on the training set\n\n- cont1 by cat3\n- cont5 by cat3\n- cont6 by cat3\n- cont9 by cat3\n- cont10 by cat3\n- cont11 by cat3\n- cont12 by cat3\n- cont0 by cat4\n- cont5 by cat4\n- cont6 by cat4\n- cont8 by cat4\n- cont9 by cat4\n- cont10 by cat4\n- cont11 by cat4\n- cont12 by cat4\n- cont13 by cat4\n- all continual variables by cat5\n- all continual variables by cat6\n- all continual variables by cat7\n- all continual variables by cat8\n- cont0 by cat9\n- cont1 by cat9\n- cont2 by cat9\n- cont5 by cat9\n- cont6 by cat9\n- cont8 by cat9\n- cont9 by cat9\n- cont10 by cat9\n- cont11 by cat9\n- cont12 by cat9\n- cont13 by cat9\n- cont1 by cat1\n\nThe above-mentioned continual-to-categorical feature associations are also confirmed on the test set","87384440":"# Express EDA Analysis \n\nWe are going to invoke *AutoViz*, one of the prominent freeware Pythonic Rapid EDA tools, to quickly draw the basic insights about the data"}}