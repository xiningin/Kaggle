{"cell_type":{"8fb85a0e":"code","7d404ae9":"code","a84e30c8":"code","01405d5f":"code","f30ddb09":"code","fbbc9937":"code","48d41073":"code","0569f1b6":"code","d747d7a3":"code","3b28848c":"code","13a42c95":"code","afdb30ca":"code","cbc61c38":"code","4f5b2786":"code","ca381fa5":"code","ead846bb":"code","999b2aec":"code","22b9eca7":"code","0a3efbae":"code","93ce1def":"code","d775d30a":"code","e4dbf3fe":"code","9d356e72":"code","b856b949":"code","2ef8309e":"code","2cf23b54":"code","6fe2e118":"code","5cfcfbf1":"code","4f9c0a9c":"code","d4b56f49":"code","7dbca132":"code","9cfdd7aa":"code","db7b57f9":"code","e84de767":"code","6f1785ff":"code","594f8768":"code","a2670609":"code","5d374a32":"code","f813f40c":"code","08422bf8":"code","dd8cc8d5":"markdown","28e4f392":"markdown","6c21759b":"markdown","b939b5d7":"markdown","ba1c6364":"markdown","0591c2ac":"markdown","87433438":"markdown","eb1906f2":"markdown","f687b175":"markdown","fba06697":"markdown","2d41a976":"markdown","95d995ac":"markdown","bd8dccf8":"markdown","27c99449":"markdown"},"source":{"8fb85a0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7d404ae9":"data=pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv')","a84e30c8":"data.info()","01405d5f":"# filtering only required column review.text\namazon_reviews=data[['reviews.text']]\n\nprint(amazon_reviews.info())","f30ddb09":"amazon_reviews.sample(10)","fbbc9937":"amazon_reviews.sample(10).values","48d41073":"# Doing some data cleaning\nreviews_text=amazon_reviews['reviews.text'].values.tolist()","0569f1b6":"reviews_text[0]","d747d7a3":"# trying to remove all characters except alphabet letters and space\nimport re\nreviews_text=[re.sub(r'[^A-Za-z\\s]','',text) for text in reviews_text]   # extra punctuations,numbers are removed","3b28848c":"from pprint import pprint\npprint(len(reviews_text[0]))\n\nprint(reviews_text[0:3])","13a42c95":"import gensim\nfrom gensim.utils import simple_preprocess\ndef sent_to_words(texts):\n    for text in texts:\n        yield(simple_preprocess(str(text),deacc=True))  # deacc=True removes the punctuation marks\n        \ndata_words=list(sent_to_words(reviews_text))","afdb30ca":"print(data_words[:3])","cbc61c38":"from nltk.corpus import stopwords\n\nstop_words=stopwords.words('english')\nprint(stop_words)\n# remove stopwords\ndef remove_stopwords(texts):\n    return [[word for word in text if word not in stop_words] for text in texts]\n\ndata_words_nostops=remove_stopwords(data_words)\n    ","4f5b2786":"print(data_words_nostops[:3])","ca381fa5":"import gensim\n# Applying Bigrams and trigrams \nbigram=gensim.models.Phrases(data_words_nostops,min_count=5,threshold=100)\ntrigram=gensim.models.Phrases(bigram[data_words_nostops],threshold=100)\n\nbigram_mod=gensim.models.phrases.Phraser(bigram)\n\ndef make_bigram(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndata_words_nostops_bigrams=make_bigram(data_words_nostops)","ead846bb":"print(data_words_nostops_bigrams[:4])","999b2aec":"#lemmatize\nimport spacy\nnlp=spacy.load('en',disable=['parser','ner'])\n\ndef lemmatize(texts,allowed_postags=['NOUN','ADJ','VERB','ADV']):\n    texts_out=[]\n    for sent in texts:\n        doc=nlp(\" \".join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n        \n    return texts_out","22b9eca7":"data_lemmatized=lemmatize(data_words_nostops_bigrams,allowed_postags=['NOUN','ADJ','VERB','ADV'] )","0a3efbae":"print(data_lemmatized[:4])","93ce1def":"import gensim.corpora as corpora\nid2word=corpora.Dictionary(data_lemmatized)\ntexts=data_lemmatized  # list of list of tokens\ncorpus=[id2word.doc2bow(text) for text in texts]\nprint(corpus[:1])   #first document","d775d30a":"id2word[0]   # dictionary of int,str","e4dbf3fe":"# token word and token id\nprint([[(id2word[id],freq) for (id,freq) in cp]for cp in corpus[:2]])","9d356e72":"# building LDA Model\nlda_model=gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=9,random_state=100,update_every=1,\n                                         chunksize=100,passes=10,alpha='auto',per_word_topics=True)\n\n\npprint(lda_model.print_topics())\n\ndoc_lda=lda_model[corpus]\n","b856b949":"print(list(doc_lda)[0])","2ef8309e":"print(lda_model[corpus[0]])\nprint(\"*****************\")\nprint(\"The Topics Distribution for first doc: \")\nprint(lda_model[corpus[0]][0])","2cf23b54":"#LDA Model Perfromance check\nprint(\"Perplexity: \",lda_model.log_perplexity(corpus))\n\n#compute coherence score\nfrom gensim.models import CoherenceModel\ncoherence_model_lda=CoherenceModel(model=lda_model,texts=data_lemmatized,dictionary=id2word,coherence='c_v')\n\ncoherence_lda=coherence_model_lda.get_coherence()\nprint('Coherence Score: ',coherence_lda)","6fe2e118":"import pyLDAvis.gensim\nimport pyLDAvis\n\nimport warnings\nwarnings.filterwarnings('ignore',category=FutureWarning)\n\npyLDAvis.enable_notebook()\nvis=pyLDAvis.gensim.prepare(lda_model,corpus,id2word)\nvis","5cfcfbf1":"def compute_coherence_values(dictionary,corpus,texts,start,limit,step):\n    coherence_vals=[]\n    model_list=[]\n    \n    for num_topics in range(start,limit,step):\n        # building LDA Model\n        model=gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=dictionary,\n                                              num_topics=num_topics,random_state=100,\n                                              chunksize=100,passes=10,alpha='auto',per_word_topics=True)\n\n        model_list.append(model)\n        \n        coherencemodel=CoherenceModel(model=model,texts=texts,dictionary=dictionary,coherence='c_v')\n        \n        coherence_vals.append(coherencemodel.get_coherence())\n    return model_list,coherence_vals\n\nmodel_list,coherence_vals=compute_coherence_values(dictionary=id2word,\n                                                   corpus=corpus,texts=data_lemmatized,\n                                                   start=2,limit=20,step=4)","4f9c0a9c":"import matplotlib.pyplot as plt\n\n# visualize the optimal LDA Model\nlimit=20\nstart=2\nstep=4\nx=range(start,limit,step)\n\nplt.plot(x,coherence_vals)\nplt.xlabel('Num_topics')\nplt.ylabel('Coherence score')\nplt.legend(('coh'),loc='best')\nplt.show()","d4b56f49":"for m, cv in zip(x,coherence_vals):\n    print(\"num topics: \",m,'has coherence value of :',round(cv,4))","7dbca132":"optimal_model=model_list[0]  # number of topics is 2\nmodel_topics=optimal_model.show_topics(formatted=False)\n\npprint(optimal_model.print_topics(num_words=10))","9cfdd7aa":"model_topics=optimal_model.show_topics(formatted=False)  \nprint(model_topics)","db7b57f9":"model_topics1=optimal_model.show_topics(formatted=True)\nprint(model_topics1)","e84de767":"pyLDAvis.enable_notebook()\nvis1=pyLDAvis.gensim.prepare(optimal_model,corpus,id2word)\nvis1","6f1785ff":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row=row[0]\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant highest weighted topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n","594f8768":"df_sent_topic_keywords=format_topics_sentences(ldamodel=optimal_model,corpus=corpus,texts=data_lemmatized)\ndf_dominant_topic=df_sent_topic_keywords.reset_index()\ndf_dominant_topic.columns=['DocumentNo','Dominant_Topic','Perc_Contribution','Topic_Keywords','texts']\n\ndf_dominant_topic.sample(10)","a2670609":"print(df_dominant_topic.groupby('Dominant_Topic').count())","5d374a32":"# showing best relevant document under each topic\ntopic_sentences_df =pd.DataFrame()\ndf_topic_sents_grped=df_dominant_topic.groupby('Dominant_Topic')\n\nfor i,grp in df_topic_sents_grped:\n    topic_sentences_df=pd.concat([topic_sentences_df,grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], axis=0)\n    \n    \n#reset index\ntopic_sentences_df.reset_index(drop=True,inplace=True)\n\n#Format\ntopic_sentences_df.columns=['Document No','Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\ntopic_sentences_df.head()","f813f40c":"# Number of Documents for Each Topic\ntopic_counts = df_sent_topic_keywords['Dominant_Topic'].value_counts()\nprint(topic_counts)\n# Percentage of Documents for Each Topic\ntopic_contribution = round(topic_counts\/topic_counts.sum(), 4)\nprint(topic_contribution)\n","08422bf8":"\n\n# Concatenate Column wise\ndf_dominant_topics = pd.concat([ topic_counts, topic_contribution], axis=1,)\n\n\n# Show\ndf_dominant_topics.reset_index(inplace=True)\n\n\n# Change Column names\ndf_dominant_topics.columns = ['Topic id', 'Num_Documents', 'Perc_Documents']\n\ndf_dominant_topics","dd8cc8d5":"Let's look at topics in plot","28e4f392":"Referenced source :\nhttps:\/\/www.machinelearningplus.com\/nlp\/topic-modeling-gensim-python\/","6c21759b":"We will find dominant topic for each document","b939b5d7":"We will now see the most relevant document for each topic.\n\n\nReading just the topic keywords may not be enough\n\nto make sense of what a topic is about. \n\nSo,to help with understanding the topic, one can find the documents a given topic has \n\ncontributed to the most and infer the meaningful topic by reading that document.","ba1c6364":"Surprisingly number of topics as 2 gives optimum coherence score.\n\nwe will the number of topics as 2 for out final model","0591c2ac":"A good topic model will have fairly big, non-overlapping bubbles \n\nscattered throughout the chart\n\ninstead of being clustered in one quadrant.","87433438":"show_topics() paramter formatted =False gives list of \ntuples of word and it's weight.","eb1906f2":"Analysis of pyLDAvis Topics plot:\n\n\n1.Each bubble represents a topic\n\n2.larger bubble corresponds to more prevalent topic in the \n\ncorpus(most talked about)\n\n3. Good topics are those which are well separated in the quardants.\n\nUpon Observing the above plot, topics 1,2,3,5 and 4 are well segregated \n\nand are better topics as compared with collapsing topics 8,7,6","f687b175":"let's list the topics distribution for the first document.\n\nWe need to consider only the first element of below output \n\nfor inferring the topic ditribution.","fba06697":"Now we will find optimum number of topics for better coherence score","2d41a976":"Building Base LDA model","95d995ac":"Displays the word and it's count in each document.\n\nIt's more human friendly to understand by looking \n\nat words and it's frequency, than word id.","bd8dccf8":"Coherence score for the base LDA Model came out very less .\n\nWe need to find optimum number of topics to get better coherence score","27c99449":"Finally, we can check for number of documents distributed across the topics"}}