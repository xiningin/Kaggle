{"cell_type":{"e72911dd":"code","3a4ee70d":"code","0c4f81ed":"code","e1882d93":"code","69987a42":"code","3ead21f4":"code","2980e1a5":"code","8302d4b5":"code","d2fbe07d":"code","3ae387d3":"code","fe9ac57b":"code","7e9882f9":"code","cdd94c01":"code","ff8392b3":"code","2946f6d2":"code","fdeb10e3":"code","ebd3fc65":"code","f4877a12":"code","45d5995d":"code","831df84d":"code","79dceb59":"code","9ff81888":"code","85041942":"code","2b2d36e8":"code","06f8793d":"code","69b27a91":"code","bc4a40ee":"code","e2f569b4":"code","46f0ecda":"code","244a90ed":"code","0f534aba":"code","264224ed":"code","5ac7727d":"code","ee75fcfc":"code","0fede3d8":"code","202e4dc4":"code","561d9631":"code","5f8868e0":"code","da527803":"code","c8c13961":"code","b48db646":"code","e29f4d4a":"code","6a3eba2f":"code","221b29c5":"code","fd1b85a7":"code","e21babef":"code","540e7590":"code","47ad01d4":"code","8a89286a":"code","62902a46":"code","b6661ed5":"code","b87e0c8a":"code","c0f66ae6":"code","aad7feb6":"code","4ccfa193":"code","4106b831":"code","388de7ee":"code","ab7da835":"code","ae2d1a13":"code","9f4b9eaf":"code","3405d563":"code","76bd4f51":"code","2f1a5203":"code","61ef99f0":"code","cb4d2801":"code","18b9862b":"code","ab111de0":"code","3610122c":"code","4a63f65c":"code","4381df0f":"code","749da974":"code","8d597038":"markdown","f421143d":"markdown","c5effa50":"markdown","29296b1e":"markdown","26425ac3":"markdown","b9e7f519":"markdown","a9d75925":"markdown","40462f69":"markdown","22d3e7e8":"markdown","68f54397":"markdown","2c8cd7f4":"markdown","b9fc22a4":"markdown","46adbacc":"markdown","bfeea156":"markdown","07d3efab":"markdown","ddab3c3f":"markdown","bca05782":"markdown","49f105eb":"markdown","97fdffcc":"markdown","0b782e20":"markdown","098f0b21":"markdown","2cc5948c":"markdown","0e37e30b":"markdown","ef68df21":"markdown","871607cd":"markdown","a8684216":"markdown","dd4077e0":"markdown","9667d94a":"markdown","194a180a":"markdown","aa6fe796":"markdown","e7a979a2":"markdown","098bb2c5":"markdown","7b9c965f":"markdown","09bac572":"markdown","83ca82d0":"markdown","b0f3c066":"markdown"},"source":{"e72911dd":"#Importing libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd #visualization\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\nchurn_data=pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\nchurn_data.info()","3a4ee70d":"churn_data.TotalCharges = pd.to_numeric(churn_data.TotalCharges, errors='coerce')\nchurn_data.TotalCharges = churn_data.TotalCharges.fillna(method='ffill')","0c4f81ed":"churn_data.TotalCharges = pd.to_numeric(churn_data.TotalCharges, errors='coerce')\nchurn_data.isnull().sum()","e1882d93":"churn_data.info()","69987a42":"churn_data.nunique() #Number of unique values for categorical variables","3ead21f4":"print(churn_data[\"Churn\"].value_counts()\/len(churn_data)*100)","2980e1a5":"#Separating catagorical and numerical columns\nId_col     = ['customerID']\ntarget_col = [\"Churn\"]\ncat_cols   = churn_data.nunique()[churn_data.nunique() < 6].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col] #categorical predictor variables\nnum_cols   = [x for x in churn_data.columns if x not in cat_cols + target_col + Id_col] #numerical predictor variables","8302d4b5":"#Separating churn and non churn customers\nchurn = churn_data[churn_data[\"Churn\"] == \"Yes\"]\nnot_churn = churn_data[churn_data[\"Churn\"] == \"No\"]","d2fbe07d":"\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go#visualization\n\n\ndef plot_pie(column) :\n    trace1 = go.Pie(values  = churn[column].value_counts().values.tolist(),\n                    labels  = churn[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"Churn Customers\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")),\n                    hole    = .6\n                   )\n    trace2 = go.Pie(values  = not_churn[column].value_counts().values.tolist(),\n                    labels  = not_churn[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    domain  = dict(x = [.52,1]),\n                    hole    = .6,\n                    name    = \"Non churn customers\" \n                   )\n    layout = go.Layout(dict(title = column + \" distribution in customer attrition \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            annotations = [dict(text = \"churn customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .15, y = .5),\n                                           dict(text = \"Non churn customers\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .88,y = .5\n                                               )\n                                          ]\n                           )\n                      )\n    data = [trace1,trace2]\n    fig  = go.Figure(data = data,layout = layout)\n    py.iplot(fig)","3ae387d3":"#for all categorical columns plot pie\nfor i in cat_cols :\n    plot_pie(i)","fe9ac57b":"sns.heatmap(pd.crosstab(churn_data.Dependents, churn_data.Partner, normalize='all', margins=True), annot=True, cmap='ocean')","7e9882f9":"sns.heatmap(pd.crosstab(churn_data.Dependents, churn_data.SeniorCitizen, normalize='all', margins=True), annot=True, cmap='ocean')\n","cdd94c01":"sns.heatmap(pd.crosstab(churn_data.PhoneService, churn_data.MultipleLines, normalize='all', margins=True), annot=True, cmap='ocean')\n","ff8392b3":"sns.heatmap(pd.crosstab(churn_data.InternetService,churn_data.PaymentMethod, normalize='all', margins=True), annot=True, cmap='ocean')\n","2946f6d2":"sns.heatmap(pd.crosstab(churn_data.InternetService,churn_data.PaperlessBilling, normalize='all', margins=True), annot=True, cmap='ocean')\n","fdeb10e3":"sns.heatmap(pd.crosstab(churn_data.SeniorCitizen,churn_data.PaymentMethod, normalize='all', margins=True), annot=True, cmap='ocean')\n","ebd3fc65":"#function  for histogram for customer attrition types\ndef plot_histogram(column) :\n    trace1 = go.Histogram(x = churn[column],\n                          histnorm = \"percent\",\n                          name = \"Churn Customers\",\n                          marker = dict(line = dict(width = .5,\n                                                    color = \"black\"\n                                                    )\n                                        ),   \n                         opacity = .6 \n                         ) \n    \n    trace2 = go.Histogram(x = not_churn[column],\n                          histnorm = \"percent\",\n                          name = \"Non churn customers\",\n                          marker = dict(line = dict(width = .5,\n                                              color = \"black\"\n                                             )\n                                 ),\n                          opacity = .6\n                         )         \n    \n    data = [trace1,trace2]\n    layout = go.Layout(dict(title =column + \" distribution in customer attrition \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(300,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = column,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                           ))\n    fig  = go.Figure(data=data,layout=layout)\n    py.iplot(fig)","f4877a12":"#for all categorical columns plot histogram    \nfor i in num_cols :\n    plot_histogram(i)\n","45d5995d":"import plotly.express as px\n\ndef plotly_scatterplot(xc, yc, colour, template, trendline=None):\n    fig1 = px.scatter(churn_data, x=xc, y=yc,\n                color=colour, render_mode='svg', template=template,\n                hover_name=\"customerID\",\n                marginal_x=None,\n                marginal_y=None, trendline=trendline)\n    return fig1","831df84d":"plotly_scatterplot(xc='MonthlyCharges', yc='TotalCharges', colour='Churn', template='plotly_dark',trendline='ols')","79dceb59":"plotly_scatterplot(xc='MonthlyCharges', yc='TotalCharges', colour='Contract', template='plotly')","9ff81888":"from sklearn.cluster import KMeans \nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\nscaler = MinMaxScaler()\nchurn_data[['MonthlyCharges','tenure']] = scaler.fit_transform(churn_data[['MonthlyCharges','tenure']])\n\n\ndef elbow_plot(data=churn_data[['MonthlyCharges','tenure']]):\n    score = []\n    for cluster in range(1,11):\n        kmeans = KMeans(n_clusters = cluster, init=\"k-means++\", random_state=10)\n        kmeans.fit(data)\n        score.append(kmeans.inertia_)\n    \n    plt.plot(range(1,11), score)\n    plt.title('The Elbow Method')\n    plt.xlabel('no of clusters')\n    plt.ylabel('wcss')\n    plt.grid()\n    return plt.show()\n\nelbow_plot()","85041942":"#Apply kmeans clustering to the entire dataset\nkmeans = KMeans(n_clusters = 4, random_state = 1000).fit(churn_data[['MonthlyCharges','tenure']])\nchurn_data['cluster'] = kmeans.labels_\nchurn_data[['MonthlyCharges','tenure']] = scaler.inverse_transform(churn_data[['MonthlyCharges','tenure']])\n\n\n#Plot a plotly interactive scatter plot\nfig = px.scatter(churn_data, x='MonthlyCharges', y='tenure',\n                color='cluster', render_mode='svg', template='plotly',\n                hover_data=['SeniorCitizen','Dependents','Contract','InternetService',\n                            'PaperlessBilling','PaymentMethod'],\n                hover_name=\"customerID\",\n                marginal_x=\"violin\",\n                marginal_y=\"violin\")\n\nfig.update_layout(title='Clusters of churned users by monthly charges and tenure',\n                  paper_bgcolor='LightBlue')\n                      \nfig.show()","2b2d36e8":"cluster_charges = pd.pivot_table(churn_data, index=['cluster'], columns=['SeniorCitizen'],\n                     values=['MonthlyCharges','tenure'], margins=True, aggfunc='mean')\n\nsns.heatmap(cluster_charges, annot=True, cmap='ocean')","06f8793d":"churn_data['cluster'].value_counts() #count distribution of clusters","69b27a91":"sns.countplot('cluster',hue='Churn',data=churn_data, orient='h')","bc4a40ee":"sns.countplot('Churn', hue='MultipleLines',data=churn_data, orient='h')","e2f569b4":"sns.countplot('cluster',hue='SeniorCitizen',data=churn_data, orient='h')","46f0ecda":"sns.countplot('cluster',hue='Contract',data=churn_data, orient='h')","244a90ed":"def plot_boxplot(column):\n    fig = px.box(churn_data, x=column, y=\"MonthlyCharges\", color=\"Churn\",points=\"outliers\", \n                 hover_name=\"customerID\",template='plotly')\n    fig.update_traces(quartilemethod=\"inclusive\")\n    fig.update_layout(title='Monthly Charges against {} Segregated By Churn and Non-Churn Customers'.format(column))\n    return fig.show()","0f534aba":"for cols in ['MultipleLines','OnlineSecurity','StreamingTV','PaperlessBilling','PaymentMethod','SeniorCitizen']:\n    plot_boxplot(cols)","264224ed":"# A person is a family man if he has a spouse and dependents. \n#These groups of people chalk higher monthly bill and may churn\nchurn_data['Family_Person'] = np.where((churn_data['Dependents']=='Yes') & (churn_data['Partner']=='Yes'),1,0)\n\n\n# Protection is defined by availability of security, backup and customer technical support\nchurn_data['Protection'] = np.where((churn_data['TechSupport'] == 'Yes') |\\\n                                    (churn_data['OnlineSecurity'] == 'Yes') |\\\n                                    (churn_data['OnlineBackup'] == 'Yes') |\\\n                                    (churn_data['DeviceProtection'] == 'Yes'),1,0)\n\n# Total services - total counts of phone, internet, streaming and protection related services\nchurn_data['TotalServices'] = (churn_data[['PhoneService', 'InternetService', 'OnlineSecurity',\n                                       'OnlineBackup', 'DeviceProtection', 'TechSupport',\n                                       'StreamingTV', 'StreamingMovies']]== 'Yes').sum(axis=1)\n\n# Presence of internet determines churn probability as well as numerous facilities availability\nchurn_data['Has_Internet']=churn_data['InternetService'].replace(['Fiber optic','DSL','No'], [1,1,0])\n\n# Presence of either streaming movies or TV show facilities determine if a customer is using streaming services\nchurn_data['Streaming'] = np.where((churn_data['StreamingTV']=='Yes') | (churn_data['StreamingMovies']=='Yes'),1,0)\n\n# Manual check payment does not utilises advanced modern electronic technology\nchurn_data['Tech_Payment'] = np.where((churn_data['PaymentMethod']!='Mailed check'),1,0)\n\n# A person is a techie if he utilises high-tech payment methods or enjoys streaming services\nchurn_data['Techie'] = np.where((churn_data['Streaming']==1) | (churn_data['Tech_Payment']==1),1,0)\n\n# Premium services defined by fiber optic usage and multiple phone lines\nchurn_data['Premium_Services'] = np.where((churn_data['MultipleLines']=='Yes') & (churn_data['InternetService']=='Fiber optic'),1,0)","5ac7727d":"y=churn_data['Churn']\n\n#Get rid of columns you don't wish to feed to machine learning models\ndrop_list=[\"customerID\",\"Churn\",\"gender\",\"cluster\",\"Partner\",\"Dependents\",\"StreamingTV\",\"StreamingMovies\",\"TechSupport\",\n           \"OnlineSecurity\",\"OnlineBackup\",\"DeviceProtection\",\"TotalCharges\",\"MultipleLines\",\"PhoneService\",\"Contract\",\n          \"InternetService\",\"PaymentMethod\"]\nx=churn_data.drop(drop_list, axis=1)","ee75fcfc":"def label_encoder(dataframe, col_name): # Function for label encoding categorical variables\n    from sklearn.preprocessing import LabelEncoder\n    le = LabelEncoder()\n    le.fit(dataframe[col_name].unique())\n    dataframe[col_name] = le.transform(dataframe[col_name])\n    \nfor i in list(x.columns[x.dtypes =='object']):\n    label_encoder(x, i)","0fede3d8":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=8,stratify=y,train_size=0.75)\nprint(x_train.shape, x_test.shape, y_train.shape, y_test.shape)","202e4dc4":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(bootstrap=True, oob_score=True, n_estimators=30, random_state=8,\n                            max_depth=8, class_weight={'No': 1, 'Yes': 1.6},n_jobs=-1,max_features=0.3,min_samples_leaf=3,\n                           min_samples_split=3)\nrf.fit(x_train, y_train)","561d9631":"print(\"Training set score: {:.3f}\".format(rf.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(rf.score(x_test, y_test)))\nprint(\"Out of bound score: {:.3f}\".format(rf.oob_score_))","5f8868e0":"rf_predictions = rf.predict(x_test)","da527803":"from sklearn.metrics import confusion_matrix\n\nax = sns.heatmap(confusion_matrix(y_test, rf_predictions), annot=True, fmt='d')\nax.set(xlabel='Random Forest Prediction', ylabel='Truth')","c8c13961":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,rf_predictions))","b48db646":"from sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam = dict(max_depth = np.arange(6,15,1), max_features = np.arange(0.1,0.9,0.1))\n\nrf_cv = RandomizedSearchCV(rf, param, cv=5,n_jobs=-1)\n# Fit it to the data\nrf_cv.fit(x_train, y_train)","e29f4d4a":"# Print the tuned parameters and score\nprint(\"Tuned Random Forest Parameters: {}\".format(rf_cv.best_params_))\nprint(\"Best score is {}\".format(rf_cv.best_score_))","6a3eba2f":"rf2 = RandomForestClassifier(bootstrap=True, oob_score=True, n_estimators=30, random_state=8,\n                            max_depth=6, class_weight={'No': 1, 'Yes': 1.6},n_jobs=-1,max_features=0.1,min_samples_leaf=5,\n                           min_samples_split=5)\nrf2.fit(x_train, y_train)","221b29c5":"rf2_predictions = rf2.predict(x_test)","fd1b85a7":"from sklearn.metrics import confusion_matrix\n\nax = sns.heatmap(confusion_matrix(y_test, rf2_predictions), annot=True, fmt='d')\nax.set(xlabel='Random Forest Prediction', ylabel='Truth')","e21babef":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test,rf2_predictions))","540e7590":"def plot_feature_importances (model, kind, title, color, dataframe):\n    importances = pd.Series(data=model.feature_importances_, index= dataframe.columns)\n    # Sort importances\n    importances_sorted = importances.sort_values()\n    # Draw a horizontal barplot of importances_sorted\n    importances_sorted.plot(kind=kind, color=color)\n    plt.title(title)\n    plt.grid()\n    return(plt.show())\n\nfig,ax=plt.subplots(figsize=(10,10))\nplot_feature_importances (rf2, 'barh', 'Churn Random Forest Classifier Importances', 'green', x_train)","47ad01d4":"#DT visualizatin method 1\n\nfrom sklearn.tree import export_graphviz\n\ndotfile = open(\"dt2.dot\", 'w')\n\nexport_graphviz(rf[0], out_file=dotfile,feature_names = x.columns,class_names=['0','1'])\ndotfile.close()\n# Copying the contents of the created file ('dt2.dot' ) to a graphviz rendering agent at http:\/\/webgraphviz.com\/\n# check out https:\/\/www.kdnuggets.com\/2017\/05\/simplifying-decision-tree-interpretation-decision-rules-python.html","8a89286a":"#DT visualizatin method 2\n# need to install Graphviz first https:\/\/graphviz.gitlab.io\/_pages\/Download\/Download_windows.html\nfrom sklearn.tree import export_graphviz\nimport os\n\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Users\/anirban\/Desktop\/customer-churn\/codes'\n\nexport_graphviz(rf[0], out_file='tree.dot', feature_names=x.columns,class_names=['0','1'])\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","62902a46":"from xgboost import XGBClassifier\n\nxgb=XGBClassifier(objective='reg:squarederror', n_estimators=12, max_depth=4,\n                  learning_rate=0.5, seed=100,reg_lambda=0, reg_alpha=0.2, colsample_bynode=1,colsample_bytree=1,\n                  scale_pos_weight=1.6)\nxgb.fit(x_train, y_train)","b6661ed5":"print(\"Training set score: {:.3f}\".format(xgb.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(xgb.score(x_test, y_test)))","b87e0c8a":"xgb_predictions = xgb.predict(x_test)","c0f66ae6":"ax = sns.heatmap(confusion_matrix(y_test, xgb_predictions), annot=True, fmt='d')\nax.set(xlabel='XGBoost Prediction', ylabel='Truth')","aad7feb6":"print(classification_report(y_test, xgb_predictions))","4ccfa193":"fig,ax=plt.subplots(figsize=(10,10))\nplot_feature_importances (xgb, 'barh', 'XGBoost Importances', 'maroon', x_train)","4106b831":"from sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    train_scores.append(knn.score(x_train,y_train))\n    test_scores.append(knn.score(x_test,y_test))","388de7ee":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))\n\n## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))\n","ab7da835":"plt.figure(figsize=(12,5))\nplt.plot(range(1,15),train_scores,marker='*',label='Train Score')\nplt.plot(range(1,15),test_scores,marker='o',label='Test Score')\nplt.grid()","ae2d1a13":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(11)\nknn.fit(x_train,y_train)\nknn.score(x_test,y_test)","9f4b9eaf":"#import confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(x_test)\nsns.heatmap(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=False), annot=True, fmt='d')\nprint(classification_report(y_test,y_pred))","3405d563":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn2 = KNeighborsClassifier()\nknn2_cv= GridSearchCV(knn,param_grid,cv=5)\nknn2_cv.fit(x_train,y_train)\n\nprint(\"Best Score:\" + str(knn2_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn2_cv.best_params_))","76bd4f51":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(C=0.01,class_weight={'No': 1, 'Yes': 1.8},penalty='l2').fit(x_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(x_test, y_test)))","2f1a5203":"y_pred = logreg.predict(x_test)\nprint(sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\"))  \nprint(classification_report(y_test, y_pred)) ","61ef99f0":"#Plot ROC charts for the MLP classifiers to compare performance\nfrom sklearn.metrics import plot_roc_curve\n\n# Instantiate the classfiers and make a list\nclassifiers = [('Random Forest Classifier', rf), ('XGBoost Classifier', xgb), ('KNN Classifier', knn),\n               ('Logistic Regression', logreg)]\n\nfor name, cls in classifiers:\n    cls_pred = cls.predict_proba(x_test)\n    cls_classifier_disp = plot_roc_curve(cls, x_test, y_test)\n    cls_classifier_disp.figure_.suptitle(\"{} ROC curve\".format(name))\n    plt.plot([0,1],[0,1],'k--', label='no skill')\n    plt.legend()\n    plt.grid()\n    plt.show()","cb4d2801":"from sklearn import model_selection\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nseed=8\nfor name, model in classifiers:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: mean-%f, std-(%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nplt.ylabel(\"Accuracy Scores\", fontsize=15)\nax.set_xticklabels(names)\nax.grid()\nplt.xticks(rotation=90)\nplt.show()","18b9862b":"from sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    axes[0].set_title(\"Learning curve of the model: {}\".format(title))\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model: {}\".format(title))\n    \n    return plt","ab111de0":"plot_learning_curve(rf, 'Random Forest Classification', x, y, axes=None, ylim=(0.7, 1.01),cv=5, n_jobs=-1)\nplot_learning_curve(xgb, 'XGBoost Classification', x, y, axes=None, ylim=(0.7, 1.01),cv=5, n_jobs=-1)\nplot_learning_curve(knn, 'KNN Classification', x, y, axes=None, ylim=(0.7, 1.01),cv=5, n_jobs=-1)\nplot_learning_curve(logreg, 'Logistic Regression', x, y, axes=None, ylim=(0.7, 1.01),cv=5, n_jobs=-1)\nplt.show()","3610122c":"import shap\n\n# load JS visualization code to notebook\nshap.initjs()\n\n#Relative importance of each feature in determining target class probability\nexplainerRF = shap.TreeExplainer(rf2)\nshap_values = explainerRF.shap_values(x_test)\nshap.summary_plot(shap_values, x_test)","4a63f65c":"# visualize the training set predictions similarity with respect to predictor variables\nshap.force_plot(explainerRF.expected_value[0], shap_values[0], x)","4381df0f":"import scikitplot as skplt\n\n# Deriving Class probabilities\npredicted_probabilities = rf2.predict_proba(x_test)\n# Creating the plot\nskplt.metrics.plot_cumulative_gain(y_test, predicted_probabilities)\nskplt.metrics.plot_lift_curve(y_test, predicted_probabilities)","749da974":"churn_data['Churn_Predictions'] = rf2.predict(x)\n\nax = sns.heatmap(confusion_matrix(churn_data['Churn_Predictions'], churn_data['Churn']), annot=True, fmt='d')\nax.set(xlabel='Random Forest Predictions', ylabel='Truth')\n\nprint(classification_report(churn_data['Churn_Predictions'] , churn_data['Churn']))","8d597038":"K-means clustering can be used to partition the dataset based on tenure and monthly charges, the significant numeric variables. <br>\nPurpose is to group instances of similar traits together.<br>\nThe K in K-Means denotes the number of clusters. <br>\nThis algorithm initialises cluster centroids that randomly converges to a solution after some point in time <br>\nis bound to converge to a solution after some iterations.<br>\n","f421143d":"__Inferences from pie chars:__<br>\n1) Gender is not a good indicator of churn <br>\n2) Customers that doesn't have partners are more likely to churn <br>\n3) Customers without dependents are also more likely to churn <br>\n4) Customers who are on month-to-month contract are likely to abandon company services <br>\n5) Customers who have internet available, opt for paperless billing and automatic payment services are more\n   likely to churn. These groups of customers tend   to be tech savvy, read widely and be updated on latest market \n   trends and rates.<br>\n6) Customers who enjoy premium stream services are likely to leave, if they are lured by competitors offering\n   similar services whose prices are competitive and offer better quality.<br>\n7) Customers also tend to leave because of lack of technical support and online security as they're unlikely to find success in    a company's products.<br>\n8) Presence of phone service, especially multiple lines drive churn.","c5effa50":"**Takeaway:**\n\nAcquiring a customer is far more costly than keeping a customer. Any company that wants to retain its customers should find some value in analysing and lowering down the churn rate. Even emerging markets, which witnessed high growth in the past, are now looking to consolidate their customer base and differentiate themselves from their peers to reduce churn rates.\nTelecom players use a variety of different metrics to determine when customers are about to leave. It is profitable for companies to explore the reasons why customers are leaving, and then target at risk customers with enticing offers. There are several different tactics companies use to maintain their customer bases\n","29296b1e":"People generally prefer manual transfer, probably due to safety reasons as well as lower cost, compared to automatic transfers.<br>\nThis is regardless of age.","26425ac3":"Clusters with descending order of churning probability: 2, 1, 0, 3 <br><br>\n\nCluster 3 is defined by high tenure and low, monthly charges, __ideal for retaining customers__.<br>\nCustomers in category 2 (low tenure and high monthly charges), have highest probability of churning.<br>\nCluster 0 customers have high tenure but high monthly charge. This shows monthly charge also an important predictor.","b9e7f519":"So far, most of the models are finetuned well such that they do not overfit, as seen by the convergence between training and validation scores. However, random forest is the only model that has fastest computation time w.r.t increase numbers of samples.","a9d75925":"## 5. Feature Engineering","40462f69":"### 2.2 Customer Attrition based on categorical influencers","22d3e7e8":"### 6.4 Logistic Regression","68f54397":"Inferences from histogram diagrams: <br>\n1) 39% of the churn customers have a tenure of about 5 months. <br>\n2) Churn customers have monthly charges peaked at around $75 per month.<br>\n3) Approximately 55% of churn customers have a cumulative total charge of 900 dollars ","2c8cd7f4":"Customers of over two year contracts are found in clusters with high tenure.\nCustomers of month-to-month contract are found in clusters with low tenure and they cause churn signficantly.","b9fc22a4":"Senior citizens have a higher probability of having dependents.<br>\nPeople without partners generally do not have dependents.","46adbacc":"People who opt for paperless billing tend to utilise internet service. <br>\nThose with internet service have a clearcut preference for automatic transfers, especially Fiber optic subscribers.\nThose without internet services tend to use mailed check mostly.","bfeea156":"There are 11 missing values under 'TotalCharges' column.","07d3efab":"## 7. Comparing Model Performances\n\nAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC score, better the model predictive power in predicting 0s as 0s and 1s as 1s. ","ddab3c3f":"Overall, clusters are well segregated as seen above.\n\nCluster 0: High tenure, high monthly charge\nCluster 1: Low tenure, low monthly charge\nCluster 2: Low tenure, high monthly charge\nCluster 3: High tenure, low monthly charge,","bca05782":"## <font color=blue> Project: Churn Fortune Teller <\/font>\n\n\n## <font color=blue>Objectives: Exploratory Data Analysis <\/font>\nProduce Visualisations to understand importance of all predictor variables, as well as their\nunderlying data distribution.<br>\nThat will enable us to determine the meanings and importance of various predictor variables in how they influence\nprediction of churn customers.<br>\n\nk-means clustering carried out to profile customers by their tenure and monthly charge.\n\nLastly, predictive models trained on dataset to determine which customers will churn or not.\n\n## <font color=blue>Overview: <\/font>\n\n1) Importing Data & Examing Data Types<br><br>\n2) Data Visualisation\n   - 2.1 Categorical Influencers<br>\n   - 2.2 Numerical Influencers <br>\n   \n3) Cluster Analysis Based Tenure and Monthly Charges<br>\n4) Boxplots Of Monthly Charges Against Categorical Predictors<br>\n5) Feature Engineering<br>\n6) Predictive Modelling (XGBoost Classifier, Random Forest Classifier, KNN Classifier, Logistic Regression)<br>\n7) Comparing Model performances<br>\n8) Model Interpretation","49f105eb":"Let's investigate monthly charge","97fdffcc":"\u2022\t__Inertia__ is the sum of squared error for each cluster. Therefore, the smaller the inertia the denser the cluster (closer together all the points are) <br>\n\u2022\tTip for choosing optimal number of clusters is looking at rate of decrease in inertia for addition of a cluster <br>\n\u2022\t__Optimal number__ of clusters is __4__ since inertia does not decrease noticeably after additional clusters are added\n","0b782e20":"### 2) Exploratory Data Analysis","098f0b21":"### 2.2 Customer Churn Analysis based on numeric influencers","2cc5948c":"### 3. Cluster Analysis Based On Monthly Charges and Tenure","0e37e30b":"It is understood from the two scatterplots that:\n\n1) Clients with __lower tenure__ are more likely to churn \n\n2) Clients with __higher MonthlyCharges__ are also more likely to churn\n\n3) Tenure and MonthlyCharges are **very significant** features in determining churn outcome\n","ef68df21":"## 1) Importing Data & Examing Data Types","871607cd":"Those with phoneservices have equal probability of having mutliple phone lines.<br>\n<br>Multiple lines is not actually a strong predictor.","a8684216":"## 6. Predictive Modelling\n### 6.1 Random Forest","dd4077e0":"### 8. Model Interpretation\n\nThe relative importance score for each features generated is based on gini. It shows importance of each score in determining churn and non-churn categories.\n\nCommittment status of a customer (contract he\/she signed up for), how long that person has been using telecom company service and monthly charges chalked up matters most regardless of demographics of the customer. This is for predicting both churn andd non-churn customers.","9667d94a":"Overall, clusters are well segregated as seen above.<br>\n\nCluster 0: High tenure, high monthly charge <br>\nCluster 1: Low tenure, low monthly charge <br>\nCluster 2: Low tenure, high monthly charge <br>\nCluster 3: High tenure, low monthly charge,  <br>\n\n\nThe pivot table below shows mean monthly charges and tenure of senior citizens in a cluster.<br>\nThe figures in the table can be verified by hovering the cursor over the interactive graph above.","194a180a":"## 4. Boxplots Measuring Monthly Cost Against Amenities and Securities","aa6fe796":"Senior citizens fall under clusters 0 and 2, clusters with high monthly charge. This means high monthly charge is problematic for senior citizens.","e7a979a2":"### 6.2 Extreme Gradient Boosting Classifier","098bb2c5":"### 6.3 K-Nearest Neighbour Classifier","7b9c965f":"__Lift and cumulative charts__:<br><br>\n1) __Lift__ is a measure of the effectiveness of a predictive model calculated as the ratio between the results obtained with and without the predictive model.<br>\n2) __Cumulative gains__ and lift charts are visual aids for measuring model performance.<br>\n3) Both charts consist of a curve and a baseline.\n\nIf we observe carefully, we can reach out to over 80% of churn customers with this random forest model if marketing budget targets 50% of its customers according to __cumulative gain curve__. With a random pick in absence of this model, we will be reaching out to 50% of churn customers.<br><br>\n\n__Lift__ is calculated as the ratio of Cumulative Gains from classification and random models. If the average incidence of targets is 20%, so the lift is 2.5. Thus, the model allows addressing two-and-half times more targets for this group, compared with addressing without the model (randomly).","09bac572":"__References for plots:__ <br>\nUdemy course on plotly & dash: https:\/\/github.com\/Pierian-Data\/Plotly-Dashboards-with-Dash<br>\nPlotly website basic tutorials: https:\/\/plotly.com\/python\/line-and-scatter\/#line-plot-with-plotly-express<br>\nPlotly colors: https:\/\/plotly.com\/python\/discrete-color\/<br>","83ca82d0":"__Boxplot Inferences:__<br>\n1) Senior citizens tend to have higher cost monthly, even for those in churn groups. They are likely to churn but bring great benefits in revenue.<br>\n2) Manual payment through checks are less costlier. Mailed check payment has largest range.<br>\n3) Presence of internet increases monthly cost significantly. Addition of streaming cost poses greater costs.<br>\n4) Having phoneline increases cost. Multiple phonelines raises monthly cost.<br>\n5) High increases in costs due to internet and phone related services increases probability of churn.<br>","b0f3c066":"For comparison, a multi-prediction force plot is shown here. It is a combination of many individual force plots that are rotated 90 degrees and stacked horizontally.<br>\n\nThe force plot shows that approximately three-quarters of the predictions follow the prediction path dominated by \ntenure, monthly charges and if the committment status, for both churn and non-churn customers.\n\nFeel free to try out how various features interact with each other segregated by churn status flag."}}