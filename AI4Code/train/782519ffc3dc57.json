{"cell_type":{"a4eed678":"code","5bbace3d":"code","faf56a2e":"code","4bac23af":"code","07a67727":"code","774459c4":"code","e3abf66c":"code","57150ae0":"code","16922128":"code","67215557":"code","6c5b10f4":"code","94defe48":"code","14402164":"code","00f56d85":"code","0384a27e":"code","57047595":"code","2d9c03f9":"code","30b937ba":"code","87eec58d":"code","fafd2966":"code","cd2328ed":"code","6635ce10":"code","d63533e5":"code","a07ce9b1":"markdown","177e289e":"markdown","24e54e6b":"markdown","b1bdcccf":"markdown","22081831":"markdown","2330aee3":"markdown","2d2cb207":"markdown","8afa9b44":"markdown","1e5cbb58":"markdown","9c18c030":"markdown","f792cbeb":"markdown","1d4590f2":"markdown","67675576":"markdown","d05ce8c8":"markdown","1c36db20":"markdown","afceb8ae":"markdown","f9c43708":"markdown","3a5a17f1":"markdown","4f0c4c70":"markdown","14a8dc13":"markdown","6cbbda67":"markdown","ea3a478c":"markdown","e6fc5354":"markdown","b7528523":"markdown","7420d41d":"markdown","11cfab6d":"markdown","10f7b77c":"markdown","2d4baf49":"markdown","c0052e15":"markdown","663cb27c":"markdown","cbfdc47f":"markdown","203a19a2":"markdown","7dfb8db4":"markdown"},"source":{"a4eed678":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/focus.csv')\nprint('Raw Data')\ndata","5bbace3d":"print(data['transmission'].unique())\nprint(data['fuelType'].unique())","faf56a2e":"transmission = pd.get_dummies(data['transmission'], drop_first=True)\nfuelType = pd.get_dummies(data['fuelType'], drop_first=True)","4bac23af":"data = data.drop(columns=['model', 'transmission', 'fuelType'])\ndata = data.join(transmission)\ndata = data.join(fuelType)\ndata = data.rename(columns={'engineSize': 'engine_size', 'Manual': 'manual', 'Semi-Auto': 'semi_auto', 'Petrol': 'petrol'})\nprint('Organized Data')\nprint(data.head())","07a67727":"from sklearn.model_selection import train_test_split\n\nX = data[['year', 'mileage', 'engine_size', 'manual', 'semi_auto', 'petrol']]\ny = data['price']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\nX_train = X_train.to_numpy()\nX_test = X_test.to_numpy()\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()\n\nprint('X_train\\n' + str(X_train[:4,:]) + '\\n')\nprint('y_train\\n' + str(y_train[:4]) + '\\n')\nprint('X_test\\n' + str(X_test[:4,:]) + '\\n')\nprint('y_test\\n' + str(y_test[:4]) + '\\n')","774459c4":"X_train_mileage = X_train[:,1]\nX_test_mileage = X_test[:,1]\n\nprint('X_train_mileage: ' + str(X_train_mileage))\nprint('X_test_mileage: ' + str(X_test_mileage))\n\nfrom bokeh.plotting import figure, show\nfrom bokeh.io import output_notebook\noutput_notebook()\n\ntrain_plot = figure(title='Training Data', x_axis_label='Mileage', y_axis_label='Price')\ntrain_plot.circle(x=X_train_mileage, y=y_train, color='blue')\nshow(train_plot)","e3abf66c":"from sklearn.preprocessing import PolynomialFeatures\n\ncubic = PolynomialFeatures(degree=3)\nX_train_mileage_cubic = cubic.fit_transform(X_train_mileage[:,None])\nX_test_mileage_cubic = cubic.fit_transform(X_test_mileage[:,None])\n\nprint('X_train_mileage_cubic \\n' + str(X_train_mileage_cubic[:4,:]))","57150ae0":"from sklearn import linear_model\n\ncubic_model = linear_model.LinearRegression()\ncubic_model.fit(X_train_mileage_cubic, y_train)\n\ntrain_predictions_cubic = cubic_model.predict(X_train_mileage_cubic)\ntest_predictions_cubic = cubic_model.predict(X_test_mileage_cubic)","16922128":"from bokeh.layouts import row\n\ntrain_plot_cubic = figure(title='Train Data', x_axis_label='Mileage', y_axis_label='Price')\ntest_plot_cubic = figure(title='Test Data', x_axis_label='Mileage', y_axis_label='Price')\n\ntrain_plot_cubic.circle(x=X_train_mileage, y=y_train, color='blue', legend_label='Actual')\ntrain_plot_cubic.circle(x=X_train_mileage, y=train_predictions_cubic, color='red', legend_label='Predicted')\n\ntest_plot_cubic.circle(x=X_test_mileage, y=y_test, color='blue', legend_label='Actual')\ntest_plot_cubic.circle(x=X_test_mileage, y=test_predictions_cubic, color='red', legend_label='Predicted')\n\nshow(row(train_plot_cubic, test_plot_cubic))\n\nprint('Train Data R-squared: ' + str(cubic_model.score(X_train_mileage_cubic, y_train)))\nprint('Test Data R-squared: ' + str(cubic_model.score(X_test_mileage_cubic, y_test)))","67215557":"X_train_mileage_rl = X_train_mileage[:,None]\nX_test_mileage_rl = X_test_mileage[:,None]\n\n\ndef addInvLogFeatures(numeric):\n    log_feats = numeric.copy()\n    valid = (log_feats != 1) & (log_feats > 0)\n    log_feats[valid] = np.log(log_feats[valid]) \/ np.log(10)\n    log_feats[log_feats <= 0] = 1e-10\n    inv_log_feats = 1 \/ log_feats\n    return np.hstack([numeric, inv_log_feats, numeric * inv_log_feats])\n\n\nprint(X_train_mileage[1])\nX_train_mileage_rl = addInvLogFeatures(X_train_mileage_rl)\nX_test_mileage_rl = addInvLogFeatures(X_test_mileage_rl)\nprint(X_train_mileage_rl[1])","6c5b10f4":"rl_model = linear_model.LinearRegression()\nrl_model.fit(X_train_mileage_rl, y_train)\n\ntrain_predictions_rl = rl_model.predict(X_train_mileage_rl)\ntest_predictions_rl = rl_model.predict(X_test_mileage_rl)","94defe48":"train_plot_rl = figure(title='Train Data', x_axis_label='Mileage', y_axis_label='Price')\ntest_plot_rl = figure(title='Test Data', x_axis_label='Mileage', y_axis_label='Price')\n\ntrain_plot_rl.circle(x=X_train_mileage, y=y_train, color='blue', legend_label='Actual')\ntrain_plot_rl.circle(x=X_train_mileage, y=train_predictions_rl, color='red', legend_label='Predicted')\n\ntest_plot_rl.circle(x=X_test_mileage, y=y_test, color='blue', legend_label='Actual')\ntest_plot_rl.circle(x=X_test_mileage, y=test_predictions_rl, color='red', legend_label='Predicted')\n\nshow(row(train_plot_rl, test_plot_rl))\n\nprint('Train Data R-squared: ' + str(rl_model.score(X_train_mileage_rl, y_train)))\nprint('Test Data R-squared: ' + str(rl_model.score(X_test_mileage_rl, y_test)))","14402164":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nX_train_numeric = X_train[:,:3]\nX_test_numeric = X_test[:,:3]\n\nscaler = StandardScaler()\nX_train_numeric_scaled = scaler.fit_transform(X_train_numeric)\nX_test_numeric_scaled = scaler.fit_transform(X_test_numeric)\n\npca = PCA(n_components=1)\nX_train_pca = pca.fit_transform(X_train_numeric_scaled)\n\ntrain_plot_pca = figure(title='Principal Component Analysis', x_axis_label='Principal Component',\n                       y_axis_label='Price')\ntrain_plot_pca.circle(x=X_train_pca[:,0], y=y_train, color='blue')\nshow(train_plot_pca)","00f56d85":"X_train_cubic = cubic.fit_transform(X_train_numeric)\nX_test_cubic = cubic.fit_transform(X_test_numeric)\n\nprint(\"X_train_numeric \\n\" + str(X_train_numeric[1,:]) + \"\\n\")\nprint(\"X_train_cubic \\n\" + str(X_train_cubic[1,:]))","0384a27e":"import numpy as np\n\nX_train_cubic_scaled = scaler.fit_transform(X_train_cubic)\nprint(X_train_cubic_scaled[:2,:])\nX_test_cubic_scaled = scaler.fit_transform(X_test_cubic)\n\n# Nominal data is a type of categorical data, where the order does not matter\nX_train_nominal = X_train[:,3:]\nX_test_nominal = X_test[:,3:]\n\nX_train_cubic_full = np.hstack([X_train_cubic_scaled, X_train_nominal])\nprint()\nprint(X_train_cubic_full[:2,:])\nX_test_cubic_full = np.hstack([X_test_cubic_scaled, X_test_nominal])","57047595":"# Training\ncubic_model = linear_model.RidgeCV()\ncubic_model.fit(X_train_cubic_full, y_train)\n\n# Predictions\ntrain_predictions_cubic = cubic_model.predict(X_train_cubic_full)\ntest_predictions_cubic = cubic_model.predict(X_test_cubic_full)","2d9c03f9":"print('Train Data R-squared: ' + str(cubic_model.score(X_train_cubic_full, y_train)))\nprint('Test Data R-squared: ' + str(cubic_model.score(X_test_cubic_full, y_test)))","30b937ba":"df = pd.DataFrame({\"Predicted\": test_predictions_cubic, \"Actual\": y_test})\ndf['% Difference'] = (abs(df['Predicted']-df['Actual'])\/df['Actual'])*100\n\nprint(\"Percentage Difference between Predicted and Actual Values (Cubic Model)\")\nprint(df.head())\nprint(\"\\nMean % Difference between Predicted and Actual Values: \" + str(df['% Difference'].mean()) +\"%\")\ndf[df['Predicted'] < 0]","87eec58d":"X_train_rl = addInvLogFeatures(X_train_numeric)\nX_test_rl = addInvLogFeatures(X_test_numeric)\nprint(X_train_numeric[1])\nprint(X_train_rl[1])","fafd2966":"X_train_rl_scaled = scaler.fit_transform(X_train_rl)\nX_test_rl_scaled = scaler.fit_transform(X_test_rl)\n\nprint(X_train_rl[1])\nprint(X_train_rl_scaled[1])\n\nX_train_rl_full = np.hstack([X_train_rl_scaled, X_train_nominal])\nX_test_rl_full = np.hstack([X_test_rl_scaled, X_test_nominal])\n\nprint('X_train_rl_full\\n' + str(X_train_rl_full[:4,:]))","cd2328ed":"# Training\nrl_model = linear_model.RidgeCV()\nrl_model.fit(X_train_rl_full, y_train)\n\n# Predictions\ntrain_predictions_rl = rl_model.predict(X_train_rl_full)\ntest_predictions_rl = rl_model.predict(X_test_rl_full)","6635ce10":"print('Train Data R-squared: ' + str(rl_model.score(X_train_rl_full, y_train)))\nprint('Test Data R-squared: ' + str(rl_model.score(X_test_rl_full, y_test)))","d63533e5":"df = pd.DataFrame({\"Predicted\": test_predictions_rl, \"Actual\": y_test})\ndf['% Difference'] = (abs(df['Predicted']-df['Actual'])\/df['Actual'])*100\n\nprint(\"Percentage Difference between Predicted and Actual Values (Cubic Model)\")\nprint(df.head())\nprint(\"\\nMean % Difference between Predicted and Actual Values: \" + str(df['% Difference'].mean()) +\"%\")\ndf[df['Predicted'] < 0]","a07ce9b1":"### Convert Categorical Data to Dummy Values\n\nBefore running regression, the *categorical (nominal) data* must first be converted to *dummy values*. Dummy values indicate the absence or presence of a feature with a `0` or `1` value. For nominal data with $k$ possible values, we create $k-1$ dummy variables. The `drop_first=True` parameter drops the first of the $k$ possible values, leading to $k-1$ features. The two nominal features that must be converted are `transmission` and `fuelType`.","177e289e":"Once again, linear regression is run to fit the model and generate predictions for the train and test set.","24e54e6b":"## Model 1: Simple Polynomial Regression\n\nThe first model will use just one numeric variable: `mileage`, to predict the output variable: `price`.","b1bdcccf":"Finally, the cubic model is fit using `RidgeCV` - an advanced form of linear regression that *regularizes* data to prevent overfitting.","22081831":"There are 6 input features: `year`, `transmission`, `mileage`, `fuelType`, and `engineSize`. The `model` column can be dropped because it is constant across each row. The `price` column is the output ($y$) variable.","2330aee3":"And again, predicted results are plotted against the train and test data and assigned an $R^2$ value (measure of how close the data fit the model).","2d2cb207":"### Reciprocal Logarithmic Model\n\nAnother approach is a model similar in shape to $\\frac{1}{\\log(x)}$, with all the features.","8afa9b44":"Instead of plotting the predictions, the following code compares the predicted price to the actual price, and calculates, on average, how far off the prediction was (for the test data).","1e5cbb58":"Then, linear regression is run to fit the model and generate predictions for the train and test set.","9c18c030":"### Cubic Model\nThe first model is a cubic one. That means it takes the form $y = ax^3 + bx^2 + cx + d$ for constants $a, b, c, d$. This time however, every variable is used as input, and not just `mileage`.","f792cbeb":"Again, the reciprocal $\\log$ model is fit using `RidgeCV` - a *regularized* version of regression.","1d4590f2":"The above plot suggests that both cubic and reciprocal logarithmic functions might be good fits for the data.","67675576":"Before running regression to fit the model, the features are scaled to make the orders of magnitude roughly the same. They are then combined with the ordinal features from before.","d05ce8c8":"Instead of plotting the predictions, the following code compares the predicted price to the actual price, and calculates, on average, how far off the prediction was (for the test data).","1c36db20":"First, each numeric feature is transformed into polynomial features of degree 3.","afceb8ae":"### Cubic Model\n\nThe first model is a cubic one. That means it takes the form $y = ax^3 + bx^2 + cx + d$ for constants $a, b, c, d$, where $x$ is the `mileage` variable and $y$ is the predicted price.","f9c43708":"The data is either cubic or close to the function $\\frac{1}{\\log(x)}$. Each curve is then plotted to determine the optimal model.","3a5a17f1":"Closeness of fit is assessed using $R^2$ once again.","4f0c4c70":"### Import and Read Data\n\nThe first step is to import necessary libraries and read the raw data into a Pandas DataFrame.","14a8dc13":"### Split into Train\/Test Data\n\nThe data must be split into a *training set* to build the model and a *test set* to evaluate its effectiveness. Here, 75% of the data is used to train and 25% is used to test.","6cbbda67":"\nFirst, the single feature $x$, is transformed into multiple featuers $x^0, x^1, x^2, x^3$.","ea3a478c":"### Reciprocal Logarithmic Model\n\nThe second approach is a model similar in shape to $\\frac{1}{\\log(x)}$.","e6fc5354":"## Preprocessing","b7528523":"### Organize the DataFrame\n\nThe following step joins dummy variables, drops unnecessary features, and rearranges features in the DataFrame.","7420d41d":"First, the numeric features are transformed to match the reciprocal $\\log$ function $\\frac{1}{\\log(x)}$.","11cfab6d":"## Principle Component Analysis (PCA)\n\n*Principal Component Analysis* flattens multiple numeric features into fewer features (known as components) that preserve the most important information from the original data. This allows for easy visualization of the data to get an intuition of which function would be best applied.","10f7b77c":"First, the data is transformed to match the reciprocal $\\log$ function $\\frac{1}{\\log(x)}$.","2d4baf49":"## Model 2: Multiple Polynomial Regression\n\nThe final model will use all numeric variables: `year`, `mileage`, `engine_size`, and dummy variables `manual`, `semi_auto`, `petrol`, to predict the output variable: `price`.","c0052e15":"### Visualize the Shape of the Data\n\nIn order to develop an intuition around which function will fit the data best, it is helpful to visualize the relationship between $x$ (`mileage`) and $y$ (`price`) in the training data.","663cb27c":"Once again, closeness of fit is measured with $R^2$.","cbfdc47f":"# \ud83d\ude98 Used Car Pricing Model\n\n**Goal:** The goal of this project is to estimate the resale price of a used Ford Focus using regression.","203a19a2":"Before running regression to fit the model, the features are scaled to make the orders of magnitude roughly the same. They are then combined with the ordinal features from before.","7dfb8db4":"Finally, predicted results are plotted against the train and test data and assigned an $R^2$ value (measure of how close the data fit the model)."}}