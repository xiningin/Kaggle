{"cell_type":{"ab2f0dfa":"code","6909f16b":"code","d74415c0":"code","495bdf6b":"code","8e976a7c":"code","4b4648d6":"code","1f584cd4":"code","583894f2":"code","5da7a5a6":"code","94f745dc":"code","cb0c65e8":"code","80845f3b":"code","5393153f":"code","c729f423":"code","e82b4582":"code","526d311b":"code","87f1e005":"code","1e8b084a":"code","39d4d1d7":"code","5f8ced00":"code","9aab2db3":"code","719506e0":"code","dee70881":"code","ad565d51":"code","2d0dd126":"code","a92198d5":"code","9ee5cbd4":"code","2671893c":"code","511afae7":"code","592a230f":"code","3b1e618b":"code","724f0f74":"code","44a7c240":"code","1cf244cb":"code","b6b4a87a":"code","e890d035":"code","336a940e":"code","80444860":"code","716f8bf9":"code","875a95ef":"code","3f709abd":"code","c68f407f":"code","96c93879":"markdown","db458ff4":"markdown","6c5b447c":"markdown","dcc00537":"markdown","3e842719":"markdown","ee7c89e3":"markdown","40826fa3":"markdown","894e1812":"markdown","b7cd50e2":"markdown","532035d3":"markdown","07819ec3":"markdown","c57d1aac":"markdown","ad56128a":"markdown","5878ebdb":"markdown","cc0dbba4":"markdown","932c58de":"markdown","e07aa06d":"markdown","c57613d5":"markdown"},"source":{"ab2f0dfa":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\n# Statistics\nimport scipy.stats as stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# machine learning tools\nimport h2o\n# from h2o.estimators import H2ORandomForestEstimator\n# from h2o.estimators import H2OGradientBoostingEstimator\nfrom h2o.estimators import H2OXGBoostEstimator","6909f16b":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","d74415c0":"df_train.shape","495bdf6b":"df_train.info()","8e976a7c":"# basic stats\ndf_train.target.describe(percentiles=[0.1,0.25,0.5,0.75,0.9])","4b4648d6":"# histogram of target\ndf_train.target.plot(kind='hist', bins=50)\nplt.title('Target - Histogram')\nplt.grid()\nplt.show()","1f584cd4":"# boxplot of target => looking for outliers\ndf_train.target.plot(kind='box')\nplt.title('Target - Boxplot')\nplt.grid()\nplt.show()","583894f2":"df_zero = df_train[df_train.target==0]\ndf_zero","5da7a5a6":"# let's remove this one observation\ndf_train = df_train[df_train.target>0]\ndf_train.target.describe()","94f745dc":"features_num = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5',\n                'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12',\n                'cont13']","cb0c65e8":"# plot distribution of numerical features\nfor f in features_num:\n    plt.figure(figsize=(8,4))\n    df_train[f].plot(kind='hist', bins=100)\n    plt.title(f)\n    plt.grid()\n    plt.show()","80845f3b":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')","5393153f":"fig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","c729f423":"fig = plt.figure(figsize = (12,9))\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","e82b4582":"# example of scatter plot - we pick the feature pair having highest (Pearson) correlation\nsns.jointplot(data=df_train, x='cont5', y='cont12',\n              joint_kws = {'alpha': 0.1})\nplt.show()","526d311b":"# different visualization => hexbin\nsns.jointplot(data=df_train, x='cont5', y='cont12', kind='hex')\nplt.show()","87f1e005":"# yet another visualization type => kde(2D)\nt1 = time.time()\n# sns.jointplot(data=df_train.sample(10000), x='cont5', y='cont12', kind='kde') # subsample => much faster rendering\nsns.jointplot(data=df_train, x='cont5', y='cont12', kind='kde')\nplt.show()\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","1e8b084a":"features_cat = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', \n                'cat7', 'cat8', 'cat9']","39d4d1d7":"# plot distribution of categorical features\nfor f in features_cat:\n    plt.figure(figsize=(8,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","5f8ced00":"# scatter plot of target vs each feature\nfor f in features_num:\n    c = df_train[f].corr(df_train.target, method='pearson')\n    c = np.round(c,4)\n    plt.figure(figsize=(7,7))\n    plt.scatter(df_train[f], df_train.target, alpha=0.01)\n    plt.title('Target vs ' + f + ' \/ corr = ' + str(c))\n    plt.xlabel(f)\n    plt.ylabel('Target')\n    plt.grid()\n    plt.show()","9aab2db3":"# different visualization type\nfor f in features_num:\n    sns.jointplot(data=df_train, x=f, y='target',\n                  kind='hex', height=7)\n    plt.show()","719506e0":"for f in features_num:\n    new_var = f + '_bin'\n    df_train[new_var] = pd.cut(df_train[f], bins=10, include_lowest=True)\n    plt.figure(figsize=(7,7))\n    # sns.boxplot(data=df_train, x=new_var, y='target')\n    sns.violinplot(data=df_train, x=new_var, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","dee70881":"for f in features_cat:\n    plt.figure(figsize=(10,5))\n    # sns.boxplot(data=df_train, x=f, y='target')\n    sns.violinplot(data=df_train, x=f, y='target')\n    plt.xticks(rotation=90)\n    plt.grid()\n    plt.show()","ad565d51":"# mean of target as trivial prediction\nm0 = df_train.target.mean()\nprint('Mean of target:', m0)\n\n# metrics on training data\nfoo = df_train.target - m0 # difference target vs. trivial mean prediction\nfoo = (foo*foo).mean() # mean squared error\nprint('RMSE(train) - Trivial Benchmark: ', np.round(np.sqrt(foo),6))","2d0dd126":"# select predictors\npredictors = features_num + features_cat\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","a92198d5":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","9ee5cbd4":"# upload training and test data in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","2671893c":"# define XGB model\nfit_1 = H2OXGBoostEstimator(tree_method='hist',\n                            ntrees=400,\n                            score_tree_interval=10,\n                            nfolds=10,\n                            learn_rate=0.05,\n                            col_sample_rate=0.5,\n                            max_depth=7,\n                            subsample=1,\n                            gamma=0.0,\n                            min_rows=1,\n                            categorical_encoding='auto',\n                            reg_lambda=0.0,\n                            reg_alpha=0.0,                            \n                            seed=999,\n                            booster='gbtree',\n                            backend='GPU')","511afae7":"# fit models\nt1 = time.time()\nfit_1.train(x=predictors,\n            y='target',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","592a230f":"# show cross validation metrics\nfit_1.cross_validation_metrics_summary()","3b1e618b":"# show scoring history - training vs cross validations\nfor i in range(10):\n    cv_model_temp = fit_1.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [RMSE]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_rmse, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_rmse, \n                c='darkorange', label='validation')\n    plt.ylim(0.7,1)\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.legend()\n    plt.grid()\n    plt.show()","724f0f74":"# show model parameters\nfit_1.params","44a7c240":"# variable importance using shap values => see direction as well as severity of feature impact\nt1 = time.time()\nfit_1.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","1cf244cb":"# predict on training data\npred_train = fit_1.predict(train_hex)\ny_train_pred = pred_train.as_data_frame().predict.values # predictions\n\n# and add prediction to original data frame\ndf_train['prediction'] = y_train_pred","b6b4a87a":"# plot predictions vs actual\np=sns.jointplot(data=df_train, x='target', y='prediction',\n              joint_kws={'alpha' : 0.1})\np.fig.suptitle('Prediction vs Actual - Training Data')\nplt.xlabel('Actual')\nplt.ylabel('Prediction')\nplt.show()","e890d035":"# correlations\nprint('Correlations - Training Data')\nprint('Correlation Pearson:', stats.pearsonr(df_train.target, y_train_pred))\nprint('Correlation Spearman:', stats.spearmanr(df_train.target, y_train_pred))","336a940e":"# metrics on training data\nprint('MAE (train): ', np.round(mean_absolute_error(df_train.target, y_train_pred),6))\nprint('RMSE(train): ', np.round(np.sqrt(mean_squared_error(df_train.target, y_train_pred)),6))","80444860":"# predict on test data\npred_test = fit_1.predict(test_hex)\ny_test_pred = pred_test.as_data_frame().predict.values # predictions\n\n# and add prediction to original data frame\ndf_test['prediction'] = y_test_pred","716f8bf9":"# plot distribution of predictions (on test set)\nplt.hist(y_test_pred, bins=100)\nplt.title('Predictions on Test Set')\nplt.grid()\nplt.show()","875a95ef":"plt.hist(y_train_pred, bins=100)\nplt.title('Predictions on Training Data')\nplt.grid()\nplt.show()","3f709abd":"# prepare submission\ndf_sub.target = y_test_pred\ndf_sub.head(10)","c68f407f":"# save to file for submission\ndf_sub.to_csv('submission.csv', index=False)","96c93879":"### A very simple benchmark for comparison:","db458ff4":"### Now let's fit a \"real\" model:","6c5b447c":"## Numerical Features","dcc00537":"<a id='5'><\/a>\n# Build Model","3e842719":"### Alternative Visualization using Binning of Features","ee7c89e3":"<a id='1'><\/a>\n# Target","40826fa3":"# Table of Contents\n* [Target](#1)\n* [Numerical Features](#2)\n* [Categorical Features](#3)\n* [Target vs Features](#4)\n* [Build Model](#5)\n* [Predict on Test Set + Submission](#6)","894e1812":"#### Compare with training data:","b7cd50e2":"#### Check the zero value:","532035d3":"<a id='6'><\/a>\n# Predict on Test Set + Submission","07819ec3":"#### Documentation H2O XGB: http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-py\/docs\/modeling.html#h2oxgboostestimator","c57d1aac":"<a id='3'><\/a>\n# Categorical Features","ad56128a":"#### Nice, no missing values at all!","5878ebdb":"### Scatter Plot Target vs Features","cc0dbba4":"<a id='2'><\/a>\n# Numerical features","932c58de":"<a id='4'><\/a>\n# Target vs Features","e07aa06d":"## Correlations","c57613d5":"## Categorical Features"}}