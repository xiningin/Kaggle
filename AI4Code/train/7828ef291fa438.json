{"cell_type":{"830392c5":"code","cf9f9de5":"code","1aae3cd2":"code","7fbddd5e":"code","d3afcf36":"code","4a6c93c4":"code","b47c4194":"code","95645043":"code","4735034f":"code","54964e84":"code","0298da9c":"code","608a2be1":"code","a634cbb4":"code","de44d21f":"code","c0b1c46b":"code","1803473a":"code","1dc137a9":"markdown","80ea7fac":"markdown","5115ef15":"markdown","5d11b9b4":"markdown","4d44619f":"markdown","fff1fb77":"markdown","c7674640":"markdown"},"source":{"830392c5":"# First we install the fast part in C. http:\/\/www.fftw.org\/download.html\n!wget http:\/\/www.fftw.org\/fftw-3.3.8.tar.gz","cf9f9de5":"# Extract the contents. Do not use verbose or else you get spammed with lots of files it unpacked.\n!tar xzf fftw-3.3.8.tar.gz","1aae3cd2":"# We follow the installation instructions from here:\n# http:\/\/www.fftw.org\/fftw3_doc\/Installation-on-Unix.html#Installation-on-Unix\n!.\/fftw-3.3.8\/configure --silent","7fbddd5e":"# Instead of running !make, we do this to silence all output.\n# The output is really long and can make the web page crash because of all the text needed to display.\n!make >\/dev\/null || make","d3afcf36":"!make install --silent","4a6c93c4":"# Now we just need to download the t-SNE part.\n!git clone https:\/\/github.com\/KlugerLab\/FIt-SNE.git","b47c4194":"# Compile the program\n!g++ -std=c++11 -O3 FIt-SNE\/src\/sptree.cpp FIt-SNE\/src\/tsne.cpp FIt-SNE\/src\/nbodyfft.cpp -I fftw-3.3.8\/api\/ -L .libs\/ -o FIt-SNE\/bin\/fast_tsne -pthread -lfftw3 -lm","95645043":"import sys; sys.path.append('.\/FIt-SNE\/') # Append it so we can import fast_tsne from that directory","4735034f":"import pandas as pd\nimport numpy as np\nfrom fast_tsne import fast_tsne\nimport matplotlib.pyplot as plt\n%matplotlib inline","54964e84":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","0298da9c":"y = train['label'].values\ntrain = train[test.columns].values\ntest = test[test.columns].values","608a2be1":"train_test = np.vstack([train, test])\ntrain_test.shape","a634cbb4":"%%time\n# We have to have 'random' initialization instead of default 'pca', because RAPIDS also uses random initialization.\n# This is so we can have a fair comparison.\ntrain_test_2D = fast_tsne(train_test, map_dims=2, initialization='random', seed=1337)","de44d21f":"%%time\n# We have to have 'random' initialization instead of default 'pca', because RAPIDS also uses random initialization.\n# This is so we can have a fair comparison.\ntrain_2D = fast_tsne(train, map_dims=2, initialization='random', seed=1337)","c0b1c46b":"plt.scatter(train_2D[:,0], train_2D[:,1], c = y, s = 0.5)","1803473a":"train_2D = train_test_2D[:train.shape[0]]\ntest_2D = train_test_2D[train.shape[0]:]\n\nnp.save('train_2D', train_2D)\nnp.save('test_2D', test_2D)","1dc137a9":"The takeaway from this notebook is that RAPIDS makes t-SNE very fast, and in fact it is best in class. Recall Bojan's notebook only took 22.7 seconds to complete. But it required a GPU to recreate. There is another implementation called CannyLab (https:\/\/github.com\/CannyLab\/tsne-cuda) that apparently is about half as fast as RAPIDS; but again, it's written in CUDA and requires a GPU.\n\nThis is probably the fastest implementation that you can get with a CPU. But any claims that RAPIDS t-SNE is 2000x faster or 600x faster as some Medium articles and Reddit posts claim, is only true if you compare it to Sklearn which is coded to use a single core. But you shouldn't use the Sklearn implementation because it's incredibly slow.\n\nSo now you know how long it takes. I make no claims that these comparisons remain if you scale up the number of data, or change the data type from ints to floats.","80ea7fac":"## FFT-accelerated Interpolation-based t-SNE (FIt-SNE)","5115ef15":"Misleading t-SNE speedup claims:\n\n1. https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/e0j9cb\/p_2000x_faster_rapids_tsne_3_hours_down_to_5\/\n2. https:\/\/medium.com\/rapids-ai\/tsne-with-gpus-hours-to-seconds-9d9c17c941db\n3. https:\/\/www.youtube.com\/watch?v=_4OehmMYr44\n4. https:\/\/towardsdatascience.com\/600x-t-sne-speedup-with-rapids-5b4cf1f62059","5d11b9b4":"From Bojan's notebook, RAPIDS took 22.7 seconds to finish the t-SNE. This one took about 4 minutes 15 seconds.","4d44619f":"But anyway, people should honestly stop using t-SNE because UMAP is much more superior. UMAP easily allows you to have more than 2 dimensions, while t-SNE will slow to a crawl if you want to output lots of dimensions. Furthermore, UMAP attempts to preserve global structure while t-SNE does not preserve global structure. This is because t-SNE optimizes Kullback-Leibler (KL) divergence, while UMAP optimizes cross entropy. The problem with KL divergence is that large distances in high dimensions are not guaranteed to be large distances in low dimensions in sense of the KL penalty. But, for UMAP's cross entropy, you would receive a big penalty, which causes the distances to be large in low dimensions.\n\nFurthermore, UMAP runs faster than t-SNE, even using just a CPU. And the visual representation is often better because it preserves global structure. I will make a kernel showing how much better it is.\n\nI highly recommend reading these articles by Nikolay Oskolkov if you are curious:\n1. https:\/\/towardsdatascience.com\/why-umap-is-superior-over-tsne-faa039c28e99\n2. https:\/\/towardsdatascience.com\/tsne-vs-umap-global-structure-4d8045acba17","fff1fb77":"Various colors corrrespond to various digits. We see that same digits are fairly well clustered together, which is probably one of the main reasons why this problem is relatively easy for ML models to do. ","c7674640":"t-Stochastic Neighborhood Embedding (t-SNE) is a highly successful method for dimensionality reduction and visualization of high dimensional datasets. A popular implementation of t-SNE uses the Barnes-Hut algorithm to approximate the gradient at each iteration of gradient descent. We accelerated this implementation as follows:\n\n- Computation of the N-body Simulation: Instead of approximating the N-body simulation using Barnes-Hut, we interpolate onto an equispaced grid and use FFT to perform the convolution, dramatically reducing the time to compute the gradient at each iteration of gradient descent. See the this post for some intuition on how it works.\n- Computation of Input Similarities: Instead of computing nearest neighbors using vantage-point trees, we approximate nearest neighbors using the Annoy library. The neighbor lookups are multithreaded to take advantage of machines with multiple cores. Using \"near\" neighbors as opposed to strictly \"nearest\" neighbors is faster, but also has a smoothing effect, which can be useful for embedding some datasets (see Linderman et al. (2017)). If subtle detail is required (e.g. in identifying small clusters), then use vantage-point trees (which is also multithreaded in this implementation).\n\nThe purpose of this kernel is to showcase the speedup that one gets with t-SNE algorithm between the SKLEARN version, RAPIDS version, and the FIt-SNE version. The Sklearn version can be found [here](https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne). The RAPIDS version can be found [here](https:\/\/www.kaggle.com\/tunguz\/mnist-2d-t-sne-with-rapids).\n\nThe FIt-SNE version of t-SNE is written in C. It has a Python wrapper which I am showing you in this notebook. It runs on your CPU. It's probably the fastest implementation of t-SNE that you can get on a CPU.\n\nThis notebook shows that FIt-SNE is slower than RAPIDS. However, it does not require a GPU and is also significantly faster than the Sklearn version."}}