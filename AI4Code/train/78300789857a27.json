{"cell_type":{"ba360202":"code","f3e66512":"code","64d7681b":"code","b0916bbf":"code","681abf36":"markdown","f1a4656b":"markdown","a53faf71":"markdown","75577d71":"markdown","26ff04e5":"markdown","5ecf1335":"markdown"},"source":{"ba360202":"from IPython.display import Image","f3e66512":"Image(\"..\/input\/rapidresponsedatascienceforcovid19-images\/creatingaugmentedspace.png\")","64d7681b":"Image(\"..\/input\/rapidresponsedatascienceforcovid19-images\/usingaugmentedspace.png\")","b0916bbf":"Image(\"..\/input\/rapidresponsedatascienceforcovid19-images\/user1ui.png\")","681abf36":"# Rapid Response Data Science for COVID19\n\nThis notebook describes the results of a seven day effort to assist subject matter experts address a problem\nrelated to COVID-19. In the course of this effort, we analyzed the 29K documents provided as part of\nthe White House\u2019s call to action. This involved applying a variety of natural language processing\ntechniques and compression-based analytics in combination with visualization techniques and\nassessment with subject matter experts to pursue answers to a speciffc question.\n\nIn the [associated paper](https:\/\/github.com\/sandialabs\/galen-view\/blob\/master\/SAND2020-4510-RapidResponseDataScienceForCOVID-19.pdf), we describe the algorithms, the software, the study performed, and availability of the software developed during the effort.\n\nThis work can be cited via:\n\n```\n@techreport{bauer2020rapid,\n  title={Rapid Response Data Science for COVID19},\n  author={Alisa Bandlow, Travis Bauer, Patricia Crossno, Rudy Garcia, Lisa Gribble, Patricia Hernandez, Shawn Martin, Jonathan McClain, and Laura Patrizi},\n  year={2020},\n  institution={Sandia National Lab.(SNL-NM), Albuquerque, NM (United States)}\n}\n```\n\n","f1a4656b":"This is a screenshot of the interface that the SME's used.","a53faf71":"# Phase 2: Using an Augmented Space\n\nThis project was only a week long and was not a tool building exercise.  A key\noutcome of this project was to capture SME feedback regarding the\nusability of our developed methodology, specifically whether it allowed them to\nfind relevant information faster.  In Phase 2, we gave the SMEs\nthe ability to interactively explore the document set.\nThe advent of python, jupyter notebooks, and the holoviews family of libraries make it easier than\nin the past to build interactive interfaces for SMEs. In just 55 lines of code\n(including blank lines for code readability), it was possible to define a user\ninterface that would run in a Jupyter notebook and provide a user with the\nability to zoom and pan through the 2D space.","75577d71":"# Introduction\n\nThis notebook summarizes the results of a one-week effort to augment subject matter experts with natural\nlanguage processing and compression-based analytics to make them more efficient at analyzing almost\n30 thousand documents. This work will enable a user to navigate a space with respect to the document\nsemantics. It is difficult for an analyst to communicate complex multifaceted research needs to a search\nengine. For example, through this study we realized that a query as simple as \u201cstability of coronavirus\u201d\nassumes a significant amount of background knowledge. A manual query into a search engine, based on\nour work with subject matter experts (SMEs), might include something like:\n\n> (stability OR inactivation OR disinfection OR survival OR degradation OR \u201chalf life\u201d)\nAND (coronavirus OR \u201cSARS 2\u201d OR \u201cSARS-CoV2\u201d OR \u201cSARS-COV-2\u201d) AND\n(sputum OR \u201cthroat swab\u201d OR aerosol OR droplet OR phlegm OR mucus OR mucosa\nOR feces, OR stool OR urine)\n\nEven a query this sophisticated would return many documents that are not relevant to the problem.\nOur subject matter experts discovered in the course of exploring the documentation that \u201cstability\u201d\noften referred to the state of patients and not the virus. It is more natural to present a system with\nexamples of useful text and for the system to find related documents. Also, when displaying results, it\nmay be easier to give the user a \u201cbig picture\u201d overview of where their interests lie in a corpus rather than\nan ordered list.\n\nSpecically, we developed a novel combination of compression-based analytics with dimensionality\nreduction and visualization techniques to assess their application to COVID-19 related problems. This\nresulted in a method to identify key documents for analysts to rapidly arrive at useful data for addressing\nCOVID-19 research questions. The application of these unsupervised algorithms organized the\ninformation space to assist SMEs. We also assessed a variety of other technologies available at Sandia\nNational Laboratories (SNL) to determine their applicability to this problem in future work.\n\nSandia is uniquely positioned to quickly bring together staff with expertise in biosecurity, public health,\ndata science, and human factors. We have ongoing research in compression-based analytics and other\ninformation theoretic algorithms. These algorithms operate with minimal feature engineering.\nConsequently, it is easy to rapidly apply these algorithms in new domains. In other research, they have\nbeen applied to authorship identification, analysis of seismic activity, and partition discovery in binary\nfiles. In this new work, we adapted them to biomedical and public health questions for COVID-19.\nWe also have SMEs in biosecurity, public health, and human factors who facilitated the rapid\nadaptation of these capabilities into this problem domain.\n\nFor this work, our SMEs\u2019 research question was \u201cStability of SARS-CoV-2 in aerosol droplets and other\nmatrices.\u201d Among the algorithms explored, t-SNE demonstrated the strongest capability of separating\ndocuments into clusters for exploration. Our SMEs were able to investigate this problem quickly when\nthey were augmented by the machine learning algorithms applied in this research. They determined\nthat the scientific documents available were likely insufficient due to the lack of research available at the\ntime of the study to answer their specific question.","26ff04e5":"# Software Availability\n\nSoftware used in this project is available on github (https:\/\/github.com\/sandialabs\/galen-view) and can be installed via pip.\n\nOther pieces of software, including Slycat (https:\/\/github.com\/sandialabs\/slycat) were also brought to bear on the project.  Slycat is available publicly but was not used in the user study.","5ecf1335":"# Phase 1: Creating an Augmented Space\n\nThe data used in this project was referenced in the \"White House Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset.\"  This document set contained 29,315 documents at the time this study was conducted containing a variety of topics relevant for specific problems to the COVID-19 pandemic. Note that new versions of the dataset with more documents were released as this research was concluding.\n\nThese documents were converted into a document\/term matrix using Term Frequency\/Inverse Document Frequency (TFIDF). In order to display these results to a user, the documents were placed in a 2D space using three algorithms, Singular Value Decomposition (SVD), t-distributed Stochastic Neighbor Embedding (t-SNE), and Uniform Manifold Approximation and Projection (UMAP). Because t-SNE produced clearly defined clusters for the parameters used, and further inspection showed that these clusters were semantically related was sufficient for this study. Figure 3-3 shows the same plot, but with the individual points were made smaller.\n\nEach search session was conducted as a Skype meeting during \"work at home\" orders during the pandemic because of social distancing policies. This unique situation provided to conduct a study when the relevant parties are not co-located.  The sessions were designed by our Human Factors expert based on best practices. \n\nA product of each search study was a document containing the SMEs text snippets from Search Study 1. The next step in this study involved using these snippets to score the CORD-19 data for the next study. To create these scores, we used a compression-based analytic."}}