{"cell_type":{"85631449":"code","c318bfa9":"code","96db0776":"code","a6f881e0":"code","f1f51761":"code","79c0901b":"code","33f0378f":"code","b992b1c5":"code","2b64cf0d":"code","62ac347f":"code","804a17be":"code","21bdc52e":"code","4ab5a855":"code","15a53601":"code","fcecdc47":"code","11d3d60e":"code","90f89bf9":"code","bc122e88":"code","84fba37a":"code","d9fe2e26":"code","c730d72e":"code","d499fa8c":"code","237bf2cb":"code","c8be23f9":"code","b818d298":"code","1c0c6581":"code","3562f2fe":"code","e74317dc":"code","4f3de29d":"code","f0cce3a3":"code","a1b8e13d":"code","bfcaa4ac":"code","0d1f92c2":"code","fa1c0ea0":"code","52e1ae43":"code","1e85a774":"code","e4f66987":"code","f666cbf4":"code","e21d2e3a":"code","c8b16466":"code","ea64634d":"code","16ad0fff":"code","e63137a0":"code","2aa95c97":"code","c9b557b7":"code","7c1a8acd":"code","9dc661b6":"code","a6391b05":"code","8c2b843e":"code","19bbc725":"code","ad435c4c":"code","bc60bf44":"code","ffa95a3a":"code","61b2aaf9":"code","3a4a9578":"code","cec8dfb6":"code","7ad5dd17":"code","172ef7dd":"code","872f3947":"code","7e306a6c":"code","3f205188":"code","22c0b093":"code","a0d2e075":"code","9c83995d":"code","eb17a546":"code","c3eebf63":"code","996c3831":"code","9f622056":"code","b9592bc2":"code","f7ebc5d4":"markdown","9b330e60":"markdown","1901f98b":"markdown","b8a088ca":"markdown","f679a0cd":"markdown","7d5245ce":"markdown","be0f5ba2":"markdown","a275728f":"markdown","f2a4a819":"markdown","32824763":"markdown","8808a1b3":"markdown","452ab178":"markdown","6a90881b":"markdown","a36c9f5d":"markdown","3ff378d7":"markdown","504cffba":"markdown","771cadc0":"markdown"},"source":{"85631449":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import tree\nimport pandas_profiling\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","c318bfa9":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","96db0776":"data.head()","a6f881e0":"data.info()","f1f51761":"data.describe()","79c0901b":"data.isnull().sum()","33f0378f":"print(\"Features : {}\".format(data.columns[:-1].values))\n\nprint(\"Total number of Features : {}\".format(len(data.columns)))\n\nprint(\"Target Variable : {}\".format(data.columns[-1]))\n\nprint(\"Total No of samples present in dataset : {}\".format(len(data)))\n\nprint(\"Our Decision Tree will classify if the sample belongs to class {} or {}\".format(data.target.unique()[0],data.target.unique()[1]))","b992b1c5":"\nplt.figure(figsize=(15,8))\n\nsns.distplot(data.age,bins=10)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")\nplt.title(\"Age Distribution\")\n","2b64cf0d":"data[data.age<30]","62ac347f":"data[(data.age>29) & (data.age<=50)][data.target==0].count()\n","804a17be":"data[(data.age>29) & (data.age<=50)][data.target==1].count()","21bdc52e":"data[(data.age>50)][data.target==0].count()","4ab5a855":"data[(data.age>50)][data.target==1].count()","15a53601":"plt.figure(figsize=(15,8))\n\nsns.countplot(data.target,hue=data.sex)","fcecdc47":"data.groupby([\"sex\"])[\"target\"].count()","11d3d60e":"female = data[data.sex==0][\"age\"].describe()\nfemale","90f89bf9":"sns.boxplot(female)","bc122e88":"male = data[data.sex==1][\"age\"].describe()\nmale","84fba37a":"sns.boxplot(male)","d9fe2e26":"len(data[(data.target==1) &(data.sex==0)])","c730d72e":"len(data[(data.target==1) &(data.sex==1)])","d499fa8c":"data.groupby(\"cp\")[\"target\"].count()","237bf2cb":"#plotting data for female having chest pain and diagnosed as +ve [cp wise].\n\ndata[(data.sex==0)&(data.target==1)].groupby(\"cp\")[\"target\"].count().plot(kind=\"bar\")","c8be23f9":"#plotting data for male having chest pain and diagnosed as +ve [cp wise].\n\ndata[(data.sex==1)&(data.target==1)].groupby(\"cp\")[\"target\"].count().plot(kind=\"bar\")","b818d298":"data[(data.sex==0)&(data.target==1)].groupby(\"cp\")[\"target\"].count()","1c0c6581":"data[(data.sex==1)&(data.target==1)].groupby(\"cp\")[\"target\"].count()","3562f2fe":"data.trestbps.describe()","e74317dc":"# avg blood pressure for female diagnose +ve \n\ndata[(data.sex==0)&(data.target==1)][\"trestbps\"].mean()","4f3de29d":"# avg blood pressure for male diagnose +ve \n\ndata[(data.sex==1)&(data.target==1)][\"trestbps\"].mean()","f0cce3a3":"data[data.sex==0].groupby(\"cp\")[\"trestbps\"].mean().plot(kind=\"bar\")","a1b8e13d":"data[data.sex==1].groupby(\"cp\")[\"trestbps\"].mean().plot(kind=\"bar\")","bfcaa4ac":"data.chol.describe(percentiles=(0.3,0.4,0.5,0.6,0.7,0.75,0.85,0.9,1))","0d1f92c2":"# For example, we will consider 130 md\/dl as borderline \n\nprint(data[data.chol<130])\nprint(\"\\n\\n Gender :\",data[data.chol<130].sex.count())\n# this shows only 1 patient is having cholestrol level under 130  (male)","fa1c0ea0":"data[(data.target==1)&(data.fbs==1)].groupby(\"sex\")[\"target\"].count()","52e1ae43":"data[(data.target==1)&(data.fbs==1)].groupby(\"sex\")[\"trestbps\"].mean().plot(kind=\"bar\")","1e85a774":"data[(data.target==1)&(data.fbs==1)].groupby([\"sex\",\"cp\"])[\"trestbps\"].mean().plot(kind=\"bar\")","e4f66987":"data.thalach.describe()","f666cbf4":"data[data.target==1].groupby(\"sex\")[\"thalach\"].mean().plot(kind=\"bar\") ","e21d2e3a":"data[(data.target==1) & (data.fbs==1)].groupby(\"sex\")[\"thalach\"].mean().plot(kind=\"bar\") ","c8b16466":"data[(data.target==1) & (data.fbs==1)].groupby([\"sex\",\"cp\"])[\"thalach\"].mean()","ea64634d":"data[(data.target==1) & (data.fbs==1)].groupby([\"sex\",\"cp\"])[\"thalach\"].mean().plot(kind=\"bar\")","16ad0fff":"X = data.drop(\"target\",axis=1)\nY = data[\"target\"]\n\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42)","e63137a0":"DTREE = DecisionTreeClassifier(random_state=0)\nDTREE.fit(x_train,y_train)","2aa95c97":"plt.figure(figsize=(35,28))\n\nfeatures = x_test.columns\nclasses = [\"No Disease\",\"Diasease\"]\ntree.plot_tree(DTREE,feature_names=features,class_names=classes,filled=True,rounded=True)","c9b557b7":"pred_on_train = DTREE.predict(x_train)\npred_on_test = DTREE.predict(x_test)","7c1a8acd":"print(\"Accuracy on training set : {}\".format(accuracy_score(y_train,pred_on_train)))\nprint(\"Accuracy on testing set : {}\".format(accuracy_score(y_test,pred_on_test)))","9dc661b6":"# confusion Matrix\n\ncm_1 = confusion_matrix(y_test,pred_on_test)\ncm_2 =confusion_matrix(y_train,pred_on_train)\nprint(\"Train Confusion Matrix\")\nsns.heatmap(cm_2,annot=True,yticklabels=classes,xticklabels=classes,cmap=\"Blues\")\nplt.show()\n\nprint(\"Test Confusion Matrix\")\nsns.heatmap(cm_1,annot=True,yticklabels=classes,xticklabels=classes,cmap=\"Blues\")\nplt.show()","a6391b05":"# Lets prune the tree tp reduce the overfitting","8c2b843e":"# # Lets first pre-prune it and check the accuracy\n\n# Pre pruning is nothing but stoping the growth of decision tree on an early stage. For that we can limit the growth of trees by setting constrains. We can limit parameters like max_depth , min_samples etc.\n\n# An effective way to do is that we can grid search those parameters and choose the optimum values that gives better performace on test data.\n\n# As of now we will control these parameters\n\n#     max_depth: maximum depth of decision tree\n#     min_sample_split: The minimum number of samples required to split an internal node\n#     min_samples_leaf: The minimum number of samples required to be at a leaf node","19bbc725":"from sklearn.model_selection import GridSearchCV\n\n\nparams = {\"max_depth\":[2,4,6,8,10,12],\n         \"min_samples_split\":[1,2,3,4],\n         \"min_samples_leaf\":[1,2]}\n\npre_pruning = DecisionTreeClassifier()\ngcv = GridSearchCV(estimator=pre_pruning, param_grid=params)\n\ngcv.fit(x_train,y_train)","ad435c4c":"model = gcv.best_estimator_\nmodel.fit(x_train,y_train)","bc60bf44":"pred_on_train = model.predict(x_train)\npred_on_test = model.predict(x_test)","ffa95a3a":"print(\"Accuracy on  Train data : {}\".format(accuracy_score(y_train,pred_on_train)) )\nprint(\"Accuracy on  TEST data : {}\".format(accuracy_score(y_test,pred_on_test)) )","61b2aaf9":"cm_train = confusion_matrix(y_train,pred_on_train)\ncm_test = confusion_matrix(y_test,pred_on_test)\n\nprint(\"Train Confusion Matrix\")\nsns.heatmap(cm_train,annot=True,xticklabels=classes,yticklabels=classes,cmap=\"Blues\")\nplt.show()\n\nprint(\"Test Confusion Matrix\")\nsns.heatmap(cm_test,annot=True,xticklabels=classes,yticklabels=classes)\nplt.show()","3a4a9578":"plt.figure(figsize=(25,25))\ntree.plot_tree(model,filled=True,rounded=True, class_names=classes)","cec8dfb6":"# Now Lets use post pruning\n# Cost Complexity Pruning","7ad5dd17":"# # Decision trees can easily overfit. \n# One way to avoid it is to limit the growth of trees by setting constrains.\n# We can limit parameters like max_depth , min_samples etc. \n# But a most effective way is to use post pruning methods like cost complexity pruning. \n# This helps to improve test accuracy and get a better model.s\n\n# # Cost complexity pruning is all about finding the right parameter foar alpha.We will get the alpha values for this tree and will check the accuracy with the pruned trees.","172ef7dd":"post_pruning = DecisionTreeClassifier()\n\npath = post_pruning.cost_complexity_pruning_path(x_train,y_train)\n\nccp_alphas, impurities = path.ccp_alphas, path.impurities","872f3947":"clfs = []\n\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n    clf.fit(x_train,y_train)\n    clfs.append(clf)","7e306a6c":"#We will remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.","3f205188":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]","22c0b093":"nodes = [clf.tree_.node_count for clf in clfs]\ndepth = [clf.tree_.max_depth for clf in clfs]","a0d2e075":"plt.figure(figsize=(15,8))\n\nplt.plot(ccp_alphas, nodes,drawstyle=\"steps-post\")\nplt.plot(ccp_alphas, depth,drawstyle=\"steps-post\")\nplt.scatter(ccp_alphas, nodes)\nplt.scatter(ccp_alphas, depth)\nplt.legend()\nplt.show()","9c83995d":"plt.figure(figsize=(15,8))\n\ntrain_acc = []\ntest_acc = []\nfor c in clfs:\n    y_train_pred = c.predict(x_train)\n    y_test_pred = c.predict(x_test)\n    train_acc.append(accuracy_score(y_train_pred,y_train))\n    test_acc.append(accuracy_score(y_test_pred,y_test))\n\nplt.scatter(ccp_alphas,train_acc)\nplt.scatter(ccp_alphas,test_acc)\nplt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\nplt.plot(ccp_alphas,test_acc,label='test_accuracy',drawstyle=\"steps-post\")\nplt.legend()\nplt.title('Accuracy vs alpha')\nplt.show()","eb17a546":"# We will select 0.023","c3eebf63":"DTREE = DecisionTreeClassifier(ccp_alpha=0.02)\nDTREE.fit(x_train,y_train)\n\npred_on_train = DTREE.predict(x_train)\npred_on_test = DTREE.predict(x_test)","996c3831":"print(f'Train score {accuracy_score(y_train,pred_on_train)}')\nsns.heatmap(confusion_matrix(y_train,pred_on_train),annot=True,xticklabels=classes,yticklabels=classes,cmap=\"Blues\")\nplt.show()\nprint(f'Test score {accuracy_score(y_test,pred_on_test)}')\nsns.heatmap(confusion_matrix(y_test,pred_on_test),annot=True,xticklabels=classes,yticklabels=classes,cmap=\"Blues\")\nplt.show()","9f622056":"# Cost Comlexity Pruning is giving the better output","b9592bc2":"plt.figure(figsize=(25,25))\ntree.plot_tree(DTREE,class_names=classes,filled=True,rounded=True)","f7ebc5d4":"# Blood Pressure","9b330e60":" Lets visualize our data now","1901f98b":"# Model building","b8a088ca":"# Age ","f679a0cd":"Observations \n\n* out of +ve male and female, 6 female and 17 male have high sugar level\n* avg BP for +ve male and female is 131.16 and 136.5 resp ","7d5245ce":"Lets analyze Gender\n\n0 -> female\n1 -> male ","be0f5ba2":"# Maximum Hear Rate","a275728f":" Observations\n\n*  Age seems to be normally distributed\n*  there is only 1 case having age 29\n*  65 cases if age is >29 and <=50\n*  99 cases if age>50","f2a4a819":"Observations\n\n* 1. no. of male = 207 female =96\n* 1. out of 207 male and 96 female, 93 male and 72 female are diagnosed positive \n* 1.  risk of having disease for female ranges from age 34 to 76 and for male from age 29 to 77","32824763":"# Chest Pain ","8808a1b3":"# Blood Sugar > 120","452ab178":"# Now lets split our data","6a90881b":"Observations\n\n* avg high heart rate for male, female tested +ve is 154.02 161.90\n* avg high heart rate for male, female tested +ve and having high blood sugar level is 145.66 161.17","a36c9f5d":" Observations :\n\n*  avg BP for female and male diagnosed +ve is 128.73 and 129.74 resp\n*  for female, avg bp for cp type 0,1,2,3 is 138.58, 128.05, 127.88, 147.5 resp\n*  for male, avg bp for cp type 0,1,2,3 is 129.55,128.59,132.05,139.47 resp","3ff378d7":"As we can see, this model has low bias and high variance.\nThis is an example of Overfitting","504cffba":"# Cholestrola","771cadc0":"Observations\n\n* for  72 +ve diagnosed female, 18,16,34,4 have cp of type 0,1,2,3 respectively\n* for  93 +ve diagnosed female, 21,25,35,12 have cp of type 0,1,2,3 respectively"}}