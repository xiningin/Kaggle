{"cell_type":{"a1ae0638":"code","78f67f80":"code","a32606ab":"code","a81ac190":"code","5a36fb39":"code","ca28707f":"markdown","0da6fa92":"markdown","3ec86ea4":"markdown","174fea96":"markdown","26a013dd":"markdown","40a644f9":"markdown"},"source":{"a1ae0638":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(2019)\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=1, n_redundant=0, flip_y=0.05, random_state=125)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","78f67f80":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nnp.random.seed(2019)\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","a32606ab":"from sklearn.datasets import make_classification \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\n# generate dataset \nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=0)\n\n\nplt.scatter(X[y==0, 0], X[y==0, 1], label='target=0')\nplt.scatter(X[y==1, 0], X[y==1, 1], label='target=1')\nplt.legend()","a81ac190":"from sklearn import mixture\nfrom matplotlib.colors import LogNorm\nfrom sklearn.covariance import OAS\n\ndef get_mean_cov(X):\n    model = OAS(assume_centered=False)\n    \n    ms = []\n    ps = []\n    for xi in X:\n        model.fit(xi)\n        ms.append(model.location_)\n        ps.append(model.precision_)\n    return np.array(ms), np.array(ps)\n\nknn_clf = mixture.GaussianMixture(n_components=2, init_params='random',\n                          covariance_type='full',\n                          n_init=1, \n                          random_state=0)\n\n\nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\nx_t =X.copy()\ny_t =y.copy()\ntrain3_pos = X[y==1]\ntrain3_neg = X[y==0]\n\nprint(train3_pos.shape, train3_neg.shape)\nms, ps = get_mean_cov([train3_pos, train3_neg])\n\nclf = mixture.GaussianMixture(n_components=2, covariance_type='full', means_init=ms, precisions_init=ps,)\nclf.fit(X)\n\nx = np.linspace(-5., 5.)\ny = np.linspace(-5., 5.)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nplt.figure()\nplt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),  levels=np.logspace(0, 3, 10))\nplt.scatter(x_t[y_t==0, 0], x_t[y_t==0, 1], label='target=0', alpha=.3)\nplt.scatter(x_t[y_t==1, 0], x_t[y_t==1, 1], label='target=1', alpha=.3)\nplt.legend()","5a36fb39":"from sklearn import mixture\nfrom matplotlib.colors import LogNorm\nfrom sklearn.covariance import OAS\nX, y = make_classification(1024, 2, n_informative=2, n_clusters_per_class=2, n_redundant=0, flip_y=0.05, random_state=5)\n\nx_t =X.copy()\ny_t =y.copy()\ntrain3_pos = X[y==1]\ntrain3_neg = X[y==0]\n\ncluster_num_pos = knn_clf.fit_predict(train3_pos)\ntrain3_pos_1 = train3_pos[cluster_num_pos==0]\ntrain3_pos_2 = train3_pos[cluster_num_pos==1]\n#print(train3_pos.shape, train3_pos_1.shape, train3_pos_2.shape, train3_pos_3.shape)\n\n## FIND CLUSTERS IN CHUNKS WITH TARGET = 0\ncluster_num_neg = knn_clf.fit_predict(train3_neg)\ntrain3_neg_1 = train3_neg[cluster_num_neg==0]\ntrain3_neg_2 = train3_neg[cluster_num_neg==1]\n        \n    \nprint(train3_pos.shape, train3_neg.shape)\nms, ps = get_mean_cov([train3_pos_1, train3_pos_2, train3_neg_1, train3_neg_2])\n\nclf = mixture.GaussianMixture(n_components=4, covariance_type='full', means_init=ms, precisions_init=ps,)\nclf.fit(X)\n\nx = np.linspace(-5., 5.)\ny = np.linspace(-5., 5.)\nX, Y = np.meshgrid(x, y)\nXX = np.array([X.ravel(), Y.ravel()]).T\nZ = -clf.score_samples(XX)\nZ = Z.reshape(X.shape)\n\nplt.figure()\nplt.contour(X, Y, Z, norm=LogNorm(vmin=1.0, vmax=1000.0),  levels=np.logspace(0, 3, 10))\nplt.scatter(x_t[y_t==0, 0], x_t[y_t==0, 1], label='target=0', alpha=.3)\nplt.scatter(x_t[y_t==1, 0], x_t[y_t==1, 1], label='target=1', alpha=.3)\nplt.legend()","ca28707f":"This competition was a fun one and I learned a lot from it mostly because it had a relatively small dataset that let me design various experiments and run them in a short time.\n\nHere I am going to share how I achieved public LB score 0.97482; at this point I assume all of the competitiors who broke the 0.973 barrier found the same thing that I will describe here. If you still don't know how the dataset is generated first read my other kernel at https:\/\/www.kaggle.com\/mhviraf\/synthetic-data-for-next-instant-gratification\n\nMoreover, you can find my complete code at https:\/\/www.kaggle.com\/mhviraf\/mhviraf-s-best-submission-in-instant-gratification\n\nAfter the abovementioned kernel and Vlad's public kernel on QDA were published, everyone assumed that after removing useless columns in each `wheezy-copper-turtle-magic` the dataset must look like somehow to the following graph (of course in higher dimensions) because QDA gave us more accurate results than SVD, Logistic regression, LGBM, etc. etc. ","0da6fa92":"### The key!\nIf we rather cluster each class into 2 or 3 clusters before fitting the model and then set `n_components=4 or 6` in `GaussianMixture` we will achieve an even more accurate model such as the one shown below.","3ec86ea4":"Now if we fit a guassian model to the former dataset with 2 clusters per class, whether it's a `QDA` or `mixture.GaussianMixture` we will get the model shown below. Still doing ok, accurate enough but not the best possible model.","174fea96":"This is actually very close to what the competition dataset is. HOWEVER, there is one important and different thing. In `make_classification`, the default value of `n_clusters_per_class` is 2 not 1! and I think it is set to 3 in Instant Gratification competition dataset but for the sake of easier illustration i'm gonna stick with 2 here. So the above graph would turn into the following graph if `n_clusters_per_class=2`:","26a013dd":" Hopefully other competitiors will share their solutions soon but I think everyone with LB score > 0.974 has been using the same technique that is described here.","40a644f9":"QDA would still perform ok on this dataset, particularly if you use the exact same parameters but a different `random_state`, the dataset might be much easier for QDA to model. refer to the figure below. Still 2 clusters per class but you can imagine fiting 1 gaussian distribution to each class too."}}