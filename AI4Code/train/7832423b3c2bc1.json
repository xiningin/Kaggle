{"cell_type":{"22625438":"code","31d5e701":"code","cd80c33e":"code","9a95e9fa":"code","40730f6f":"code","d6f8cf45":"code","50c09495":"code","0d02f517":"code","3097baf7":"code","6f3728ca":"code","5d174251":"code","49ee7998":"code","e41560ff":"code","9e41722c":"code","f58f91a3":"code","6959d4fc":"code","1c3ef8ff":"code","bcdd63f4":"code","32e1aa7f":"code","840ce7ef":"code","31bcb77a":"code","698bf03b":"code","aeb365ac":"code","098579aa":"code","07d5cff3":"code","57b61542":"code","e1e99c2c":"code","b4453589":"code","39934b3c":"code","cd1a4103":"code","51624871":"code","072eb41d":"code","247b61bf":"code","28b36b0d":"code","c00c0ec3":"code","42bacb09":"code","ab885e2e":"code","2c04760d":"code","bf53291d":"code","5ad21f1f":"code","5cf73873":"code","84370d41":"code","f1cfdc11":"code","3ee70a31":"code","5c6791d5":"code","c41bf1f0":"code","2636da80":"code","25fc0141":"code","be4f17a5":"code","4989687b":"code","0e88d8a7":"code","9ebe297f":"code","82eff596":"code","3a95f3fa":"code","2dd9ad83":"code","8490ea25":"code","90f063b5":"code","02725305":"code","95db9c77":"code","9fc7e08a":"code","18830d4b":"code","1c8e2e2a":"code","f001c3ab":"code","1e466cb5":"code","3561e115":"code","27c93fa7":"code","ab0f7cce":"code","b3d7832d":"code","ea2cc08f":"code","e5dd3f71":"code","6156048d":"code","e85ed47e":"code","4fd0b337":"code","7f97cc17":"code","7d4c0644":"code","f8752d2f":"code","b54fe13c":"code","8d8e58e6":"code","b448f98c":"code","a7a59689":"code","d572de4b":"code","a384871b":"code","d6c861af":"code","78815d0c":"code","8dab8efe":"code","a3b407c8":"code","fe260e33":"code","a6d25550":"code","65e30b94":"code","0df277be":"code","d9a0fb54":"code","f4b609f6":"code","f5776196":"code","522a3fab":"code","25a4368f":"code","b8419195":"code","95e535b3":"code","b4fcb571":"code","02ebc6a8":"code","5e4a5e91":"code","67bcac85":"code","99ddf74c":"code","f8623d5f":"code","935122b8":"markdown","8386d342":"markdown","288b9b70":"markdown","e6ae0ca5":"markdown","37b1fbe2":"markdown","1573e947":"markdown","a09b7f66":"markdown","2b4b04da":"markdown","cf3a37ec":"markdown","d0229ad9":"markdown","847cb03c":"markdown","eb2a02af":"markdown","7d1e65d5":"markdown","f9f273aa":"markdown","a6b62353":"markdown"},"source":{"22625438":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31d5e701":"# Importing all required packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n\n%matplotlib inline","cd80c33e":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","9a95e9fa":"# set the maximum display columns and rows\npd.set_option('display.max_columns', 111)\npd.set_option('display.max_rows', 50)","40730f6f":"# Importing dataset\ndf = pd.read_csv(\"\/kaggle\/input\/house-price-prediction\/train.csv\")\ndf.head()","d6f8cf45":"df.tail()","50c09495":"df.info()","0d02f517":"df.shape","3097baf7":"# Column which contains null data\nround(100*(df.isnull().sum()\/len(df.index)), 2)[round(df.isnull().sum()\/len(df.index), 2).values > 0.00].sort_values(ascending=False)","6f3728ca":"# Checking numeric column data\ndf.select_dtypes(include=['float64', 'int64']).describe()","5d174251":"# Convert year column to number or calculate the age for the column YearBuilt, YearRemodAdd, GarageYrBlt, YrSold\ndf['AgeYearBuilt'] = df.YearBuilt.max() - df['YearBuilt']\ndf['AgeYearRemodAdd'] = df.YearRemodAdd.max() - df['YearRemodAdd']\ndf['AgeGarageYrBlt'] = df.GarageYrBlt.max() - df['GarageYrBlt']\ndf['AgeYrSold'] = df.YrSold.max() - df['YrSold']\n\n# drop the original column as we will use above created column\ndf.drop(['YearBuilt','YearRemodAdd','GarageYrBlt','YrSold'],axis=1,inplace=True)","49ee7998":"df[['AgeYearBuilt', 'AgeYearRemodAdd', 'AgeGarageYrBlt', 'AgeYrSold']].head()","e41560ff":"# Droping the column which have \ndf.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis=1,inplace=True)","9e41722c":"# No use of 'Id' column so droping it\ndf.drop(['Id'], axis=1, inplace=True)","f58f91a3":"# List of column still have empty data\nround(100*(df.isnull().sum()\/len(df.index)), 2)[round(df.isnull().sum()\/len(df.index), 2).values > 0.00].sort_values(ascending=False)","6959d4fc":"# viewing data based on the interval percentage\ndf.describe(percentiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99, 1])","1c3ef8ff":"# As per the above details few columns have standard value which we can use as categorical instead of numerical\ndf['MoSold'] = df['MoSold'].astype('object')\ndf['OverallQual'] = df['OverallQual'].astype('object')\ndf['OverallCond'] = df['OverallCond'].astype('object')\ndf['BsmtFullBath'] = df['BsmtFullBath'].astype('object')\ndf['BsmtHalfBath'] = df['BsmtHalfBath'].astype('object')\ndf['FullBath'] = df['FullBath'].astype('object')\ndf['HalfBath'] = df['HalfBath'].astype('object')\ndf['BedroomAbvGr'] = df['BedroomAbvGr'].astype('object')\ndf['KitchenAbvGr'] = df['KitchenAbvGr'].astype('object')\ndf['TotRmsAbvGrd'] = df['TotRmsAbvGrd'].astype('object')\ndf['Fireplaces'] = df['Fireplaces'].astype('object')\ndf['GarageCars'] = df['GarageCars'].astype('object')","bcdd63f4":"# Column which contains outliers \nout_col = [\n    'LotArea',\n    'TotalBsmtSF',\n    'PoolArea',\n    'MiscVal']","32e1aa7f":"# Boxplot method to generate the graph to Check the outliers \n\ndef draw_boxplot(cols):\n    int_range = range(len(cols))[::3]\n    col_length = len(cols)\n    for col in int_range:\n        print('----------------',cols[col:col+3],' ----------------')\n        plt.figure(figsize=(17, 5))\n        if col < col_length:  \n            plt.subplot(1,3,1)\n            sns.boxplot(x=cols[col], orient='v', data=df)\n        if col+1 < col_length:                    \n            plt.subplot(1,3,2)\n            sns.boxplot(x=cols[col+1], orient='v', data=df)\n        if col+2 < col_length:                \n            plt.subplot(1,3,3)\n            sns.boxplot(x=cols[col+2], orient='v', data=df)\n                        \n        plt.show()","840ce7ef":"# Method call to draw boxplot for the outliers\ndraw_boxplot(out_col)","31bcb77a":"# Size before removing the outliers\ndf.shape","698bf03b":"# method to remove outliers\ndef remove_outliers(x, num_cols, s_quntl=0.05, e_quntl=0.95):\n    for col in num_cols:\n        Q1 = x[col].quantile(s_quntl)\n        Q3 = x[col].quantile(e_quntl)\n        IQR = Q3-Q1\n        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] \n    return x   ","aeb365ac":"# call remove outliers method for the selected columns\ndf=remove_outliers(df, out_col)","098579aa":"# dataframe size after removing the outliers\ndf.shape","07d5cff3":"df.describe(percentiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])","57b61542":"# Method to replace the null value with the selected values\ndef filling_missing_values(col, replace_type:str, other_value=None):\n    if replace_type == 'mean':\n        df[col].fillna(df[col].mean(), inplace=True)  \n    if replace_type == 'mode':\n        df[col].fillna(df[col].mode()[0], inplace=True)\n    if replace_type == 'median':\n        df[col].fillna(df[col].median(), inplace=True)\n    if replace_type == 'other':\n        df[col].fillna(other_value, inplace=True)\n","e1e99c2c":"# list of columns which contains null value\nnull_cols = df.columns[round(df.isnull().sum()\/len(df.index), 2).values > 0.00]\nnull_cols","b4453589":"# column list which has null value\ndf[null_cols].describe(percentiles = [0.05, 0.1, 0.25, 0.5, 0.75, 0.90, 0.95, 0.99])","39934b3c":"# Categorical value updating using mode values\nfor col in null_cols:\n    if col not in ['LotFrontage', 'AgeGarageYrBlt', 'MasVnrArea']:\n        filling_missing_values(col, 'mode')\n\n# updating with mean value for the variable MasVnrArea   \nfilling_missing_values('LotFrontage', 'mean') \nfilling_missing_values('MasVnrArea', 'mean') \nfilling_missing_values('AgeGarageYrBlt', 'other', other_value=0)\n","cd1a4103":"# Check the column which still have null values\nround(100*(df.isnull().sum()\/len(df.index)), 2)[round(df.isnull().sum()\/len(df.index), 2).values > 0.00].sort_values(ascending=False)","51624871":"num_cols=df.select_dtypes(include=['int64', 'float']).columns\nnum_cols","072eb41d":"# Method to fetch column list which contains more than 90% duplicate value\ndef percentage_of_duplicate(num_cols):\n    x=list()\n    for col in (num_cols):\n        if(df[col].value_counts().max()\/df.shape[0] >= 0.90):\n            x.append(col)\n    return x","247b61bf":"# drop filtered column\nfilter_cols=percentage_of_duplicate(num_cols)\nprint(filter_cols)\ndf.drop(filter_cols, axis = 1, inplace = True)","28b36b0d":"# target variable SalePrice\nplt.figure(figsize=(15,5))\nplt.title('SalePrice')\nsns.distplot(df.SalePrice)\nplt.show()","c00c0ec3":"plt.title('SalePrice')\nsns.distplot(np.log1p(df['SalePrice']), bins=10)\nplt.show()","42bacb09":"from scipy import stats\nstats.probplot(df['SalePrice'], plot=plt)\nplt.show()","ab885e2e":"# Method to viewing all the categorical variable\ndef categorical_data(cols):\n    for col in cols:\n        print('\\n')\n        print('---------------------------------------------- ',col,' -----------------------------------------------')\n        print(df[col].astype('category').value_counts())\n        f, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12,3), dpi=90) \n        sns.countplot(data=df, x=col, order=df[col].value_counts().index, ax=ax1) \n        ax1.set_ylabel('Count') \n        ax1.set_title(f'{col}', weight=\"bold\") \n        ax1.set_xlabel(col) \n        if col == 'Neighborhood':\n            xticks(rotation = 90)\n        plt.show()\n","2c04760d":"# list of categorical columns\ncateg_var = df.select_dtypes(include=['object']).columns\n# Visualise the data\ncategorical_data(categ_var)","bf53291d":"# As per the above graph Street and utilities has lower variance so dorpping it\ndf.drop(['Street','Utilities'],axis=1,inplace=True)","5ad21f1f":"# Numerical variable analysis using pairplots\ndef numerical_data(cols):\n    int_range = range(len(cols))[::3]\n    col_length = len(cols)\n    for col in int_range:\n        print('------------------ ',cols[col:col+3],' ---------------------')\n        sns.pairplot(df, x_vars=cols[col:col+3], y_vars='SalePrice',height=3, aspect=1,kind='scatter')            \n        plt.show()\n","5cf73873":"# List of numeric columns \nnum_cols=df.select_dtypes(include=['int64', 'float']).columns\nnum_cols","84370d41":"num_cols = num_cols.drop(labels='SalePrice')\nnum_cols","f1cfdc11":"# Check the numerical values using pairplots\n\n# Target variable SalePrice and other variables\nnumerical_data(num_cols)","3ee70a31":"# correlation table to check the correlation for the variable with others\ndf.corr()","5c6791d5":"# Heatmap to check correlatoin between the variables \nfig, ax = plt.subplots() \nfig.set_size_inches(35, 30) \nsns.heatmap(df.corr(),cmap =\"YlGnBu\",linewidths = 0.1, annot = True)\ntop, bottom = ax.get_ylim()\nax.set_ylim(top+0.5, bottom-0.5)\nplt.show()","c41bf1f0":"# positive correlation with SalePrice greater than 50%\ncorr = df.corr()\ntop_feature = corr.index[abs(corr['SalePrice']>0.5)]\nfig, ax = plt.subplots() \nfig.set_size_inches(15, 10) \ntop_corr = df[top_feature].corr()\nsns.heatmap(top_corr,cmap =\"YlGnBu\",linewidths = 0.1, annot = True)\ntop, bottom = ax.get_ylim()\nax.set_ylim(top+0.5, bottom-0.5)\nplt.show()","2636da80":"# negative correlation with SalePrice less then -0.5\ncorr = df.corr()\ntop_feature = corr.index[abs(corr['SalePrice']<-0.5)]\nfig, ax = plt.subplots() \nfig.set_size_inches(10, 5) \ntop_corr = df[top_feature].corr()\nsns.heatmap(top_corr,cmap =\"YlGnBu\",linewidths = 0.1, annot = True)\ntop, bottom = ax.get_ylim()\nax.set_ylim(top+0.5, bottom-0.5)\nplt.show()","25fc0141":"# Dividing dataframe into X and Y sets for the model building\nX=df.drop(columns=['SalePrice'])\ny=np.log(df['SalePrice'])","be4f17a5":"# Viewing categorical data\ncategorical_data = X.select_dtypes(include=['object'])\ncategorical_data.head(3)","4989687b":"# Use pandas library to create the dummy variables\ndummies = pd.get_dummies(categorical_data, drop_first=True)\ndummies.head(15)","0e88d8a7":"# drop categorical data for that dummy variable has created\nX=X.drop(columns=categorical_data)\nX.head(3)","9ebe297f":"# concat dummies with the X numerical variable\nX=pd.concat([X,dummies],axis=1)\nX.head(3)","82eff596":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)","3a95f3fa":"# Train and test data shape\nprint(X_train.shape)\nprint(X_test.shape)","2dd9ad83":"from sklearn.preprocessing import StandardScaler","8490ea25":"num_col=X_train.select_dtypes(include=['int64','float64']).columns\nnum_col","90f063b5":"# Apply scaler() to all the columns except the dummy variables which we creaeted before\nscaler = StandardScaler()\nX_train[num_col] = scaler.fit_transform(X_train[num_col])\nX_test[num_col] = scaler.transform(X_test[num_col])","02725305":"X_train.head(3)","95db9c77":"# Importing RFE and LinearRegression from the sklearn\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","9fc7e08a":"# Running RFE with the output number of the variable \nlm = LinearRegression()\nlm.fit(X_train,y_train)\nrfe = RFE(lm, 25)\nrfe = rfe.fit(X_train, y_train)","18830d4b":"rfe_df = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)), columns=['Variable', 'rfe_support', 'rfe_ranking'])\nrfe_df = rfe_df.loc[rfe_df['rfe_support'] == True]\nrfe_df.reset_index(drop=True, inplace=True)\nrfe_df","1c8e2e2a":"# Selected column list \ncol = X_train.columns[rfe.support_]\ncol","f001c3ab":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()","1e466cb5":"# import Ridge and Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n# from sklearn.metrics import mean_squared_error","3561e115":"# list of alphas to tune our model\nparams = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.001, 0.002, 0.003, 0.004, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}","27c93fa7":"# Ridge \nridge = Ridge()","ab0f7cce":"# Cross Validation \nfolds = 5\nRidgeModelCV = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)           \nRidgeModelCV.fit(X_train, y_train) ","b3d7832d":"#checking the value of optimum number of parameters\nprint(RidgeModelCV.best_params_)\nprint(RidgeModelCV.best_score_)","ea2cc08f":"# Result based on the mean score\n\nRidgeModelCVResults = pd.DataFrame(RidgeModelCV.cv_results_)\nRidgeCVResults = RidgeModelCVResults[RidgeModelCVResults['param_alpha']<=200]\nRidgeCVResults[['param_alpha', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by = ['rank_test_score'])","e5dd3f71":"\nRidgeCVResults['param_alpha'] = RidgeCVResults['param_alpha'].astype('int32')\n# plotting mean for train and test score with alpha \nRidgeCVResults['param_alpha'] = RidgeCVResults['param_alpha']\nplt.figure(figsize=(16,5))\nplt.plot(RidgeCVResults['param_alpha'], RidgeCVResults['mean_train_score'])\nplt.plot(RidgeCVResults['param_alpha'], RidgeCVResults['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","6156048d":"RidgeModelCV.best_estimator_","e85ed47e":"print(int(RidgeModelCV.best_params_.get('alpha')))","4fd0b337":"alpha = int(RidgeModelCV.best_params_.get('alpha'))\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_","7f97cc17":"#lets predict the R-squared value of test and train data\ny_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","7d4c0644":"y_test_pred = ridge.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","f8752d2f":"# RMSE\nmetrics.mean_squared_error(y_test, ridge.predict(X_test))","b54fe13c":"alpha = int(RidgeModelCV.best_params_.get('alpha'))*2\nridge = Ridge(alpha=alpha)\n\nridge.fit(X_train, y_train)\nridge.coef_\n","8d8e58e6":"y_train_pred = ridge.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","b448f98c":"y_test_pred = ridge.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","a7a59689":"# RMSE\nmetrics.mean_squared_error(y_test, ridge.predict(X_test))","d572de4b":"ridge_df = pd.DataFrame({'Features':X_train.columns, 'Coefficient':ridge.coef_.round(4)})\nridge_df.reset_index(drop=True, inplace=True)\nridge_df","a384871b":"# convert in dict for other usages\nridge_coeff = dict(pd.Series(ridge.coef_.round(4), index = X_train.columns))","d6c861af":"# minimise the feature using RFE\nX_train_ridge = X_train[ridge_df.Features]\n\nlm = LinearRegression()\nlm.fit(X_train_ridge, y_train)\n\nrfe = RFE(lm, 15)            \nrfe = rfe.fit(X_train_ridge, y_train)","78815d0c":"ridge_df1 = pd.DataFrame(list(zip( X_train_ridge.columns, rfe.support_, rfe.ranking_)), columns=['Features', 'rfe_support', 'rfe_ranking'])\nridge_df1 = ridge_df1.loc[ridge_df1['rfe_support'] == True]\nridge_df1.reset_index(drop=True, inplace=True)\n\nridge_df1['Coefficient'] = ridge_df1['Features'].apply(lambda x: ridge_coeff[x])\nridge_df1 = ridge_df1.sort_values(by=['Coefficient'], ascending=False)\nridge_df1 = ridge_df1.head(10)\nridge_df1","8dab8efe":"plt.figure(figsize=(15,5))\nsns.barplot(y = 'Features', x='Coefficient', data = ridge_df1)\nplt.show()","a3b407c8":"lasso = Lasso()\n\n# list of alphas\nparams = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.001, 0.002, 0.003, 0.004, 0.005, 0.01]}\n\n# cross validation\nfolds = 5\nLassoModelCV = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)             \n\nLassoModelCV.fit(X_train, y_train) ","fe260e33":"#checking the value of optimum number of parameters\nprint(LassoModelCV.best_params_)\nprint(LassoModelCV.best_score_)","a6d25550":"# display the mean scores\n\nLassoModelCVResults = pd.DataFrame(LassoModelCV.cv_results_)\nLassoModelCVResults[['param_alpha', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by = ['rank_test_score'])","65e30b94":"LassoModelCVResults['param_alpha'] = LassoModelCVResults['param_alpha'].astype('float32')\n\n# plotting mean for train and test score with alpha \n\nplt.figure(figsize=(16,5))\nplt.plot(LassoModelCVResults['param_alpha'], LassoModelCVResults['mean_train_score'])\nplt.plot(LassoModelCVResults['param_alpha'], LassoModelCVResults['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","0df277be":"LassoModelCV.best_params_.get('alpha')","d9a0fb54":"alpha = LassoModelCV.best_params_.get('alpha')\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train) \nlasso.coef_","f4b609f6":"#lets predict the R-squared value of test and train data\ny_train_pred = lasso.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","f5776196":"y_test_pred = lasso.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","522a3fab":"# RMSE\nmetrics.mean_squared_error(y_test, lasso.predict(X_test))","25a4368f":"alpha = LassoModelCV.best_params_.get('alpha')*2\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train) \nlasso.coef_","b8419195":"y_train_pred = lasso.predict(X_train)\nprint(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))","95e535b3":"y_test_pred = lasso.predict(X_test)\nprint(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))","b4fcb571":"metrics.mean_squared_error(y_test, lasso.predict(X_test))","02ebc6a8":"lasso_df = pd.DataFrame({'Features':X_train.columns, 'Coefficient':lasso.coef_.round(4)})\nlasso_df.reset_index(drop=True, inplace=True)\nlasso_df","5e4a5e91":"lasso_coeff = dict(pd.Series(lasso.coef_.round(4), index = X_train.columns))\n","67bcac85":"# minimise the feature using RFE\nX_train_lasso = X_train[lasso_df.Features]\n\nlm = LinearRegression()\nlm.fit(X_train_lasso, y_train)\n\nrfe = RFE(lm, 15)            \nrfe = rfe.fit(X_train_lasso, y_train)","99ddf74c":"lasso_df = pd.DataFrame(list(zip( X_train_lasso.columns, rfe.support_, rfe.ranking_)), columns=['Features', 'rfe_support', 'rfe_ranking'])\nlasso_df = lasso_df.loc[lasso_df['rfe_support'] == True]\nlasso_df.reset_index(drop=True, inplace=True)\n\nlasso_df['Coefficient'] = lasso_df['Features'].apply(lambda x: lasso_coeff[x])\nlasso_df = lasso_df.sort_values(by=['Coefficient'], ascending=False)\nlasso_df = lasso_df.head(10)\nlasso_df","f8623d5f":"plt.figure(figsize=(15,5))\nsns.barplot(y = 'Features', x='Coefficient', data = lasso_df)\nplt.show()","935122b8":"**Reading and Understanding the Data**","8386d342":"### Visualising the Data","288b9b70":"### Change in the model if we choose double the value of aplha","e6ae0ca5":"### Splitting the Data into Training and Testing Sets","37b1fbe2":"### RFE\nRecursive feature elimination","1573e947":"### Dummy Variables","a09b7f66":"### Data preperation","2b4b04da":"### Lasso","cf3a37ec":"### Change in the model if we choose double the value of aplha","d0229ad9":"**Cleaning the Data**","847cb03c":"### Building model & evaluations using Ridge & Lasso Regression","eb2a02af":"### Ridge","7d1e65d5":"#### As per the above null % below are the column list which has 50 % of missing data so droping it.\n- PoolQC           99.52\n- MiscFeature      96.30\n- Alley            93.77\n- Fence            80.75\n- FireplaceQu      47.26","f9f273aa":"### Rescaling the Features \n\nusing scaling from the sklearn.","a6b62353":"#### Numerical"}}