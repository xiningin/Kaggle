{"cell_type":{"69fcd95c":"code","9f2705ca":"code","67730bc3":"code","9b4c3052":"code","0e871aec":"code","7fc42d94":"code","d3ae2d91":"code","6d894e4e":"code","3059c1f4":"code","52438d8d":"code","94d07fb7":"code","27f11696":"code","90ced847":"code","02ea9cbd":"code","98401a9a":"code","c1757b6f":"code","ca5000c1":"code","028c7ad7":"code","a87a9e66":"code","e7b7e288":"code","6c1676e7":"code","6a02e2b6":"code","ebca7d09":"code","adcc5cfc":"code","5b7dfd83":"code","682bcf82":"code","9c5229d1":"code","d4dc82ce":"code","58bb8ed6":"code","ce476c50":"code","c92dd9eb":"code","6d987cf5":"code","c483b43f":"code","a065d184":"code","7e0e4282":"code","9c85b559":"code","3a355ff0":"markdown","95b7808d":"markdown","a56bbe6c":"markdown","477447ec":"markdown","3328ac8b":"markdown","f83b9722":"markdown","0a2c3d76":"markdown","3836df61":"markdown","7c75dcb2":"markdown","f87da029":"markdown","1dfa941c":"markdown","c22bf87c":"markdown","a9374cb6":"markdown","40e539d6":"markdown","595bb7a9":"markdown"},"source":{"69fcd95c":"import matplotlib.pyplot as plt\nimport os\nimport glob\nimport pandas as pd\nimport random\nimport numpy as np\nimport cv2\nimport base64\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nfrom copy import deepcopy\nfrom torch.autograd import Variable\nfrom tqdm import tqdm\nfrom pprint import pprint\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport os\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint('Training on',DEVICE)","9f2705ca":"DATASET_PATH =\"\/kaggle\/input\/lfw-dataset\/lfw-deepfunneled\/lfw-deepfunneled\/\"\nATTRIBUTES_PATH = \"\/kaggle\/input\/lfw-attributes\/lfw_attributes.txt\"","67730bc3":"dataset = []\nfor path in glob.iglob(os.path.join(DATASET_PATH, \"**\", \"*.jpg\")):\n    person = path.split(\"\/\")[-2]\n    dataset.append({\"person\":person, \"path\": path})\n    \ndataset = pd.DataFrame(dataset)\n#too much Bush\ndataset = dataset.groupby(\"person\").filter(lambda x: len(x) < 25 )\ndataset.head(10)","9b4c3052":"dataset.groupby(\"person\").count()[:200].plot(kind='bar', figsize=(20,5))","0e871aec":"plt.figure(figsize=(20,10))\nfor i in range(20):\n    idx = random.randint(0, len(dataset))\n    img = plt.imread(dataset.path.iloc[idx])\n    plt.subplot(4, 5, i+1)\n    plt.imshow(img)\n    plt.title(dataset.person.iloc[idx])\n    plt.xticks([])\n    plt.yticks([])\nplt.tight_layout()\nplt.show()","7fc42d94":"def fetch_dataset(dx=80,dy=80, dimx=45,dimy=45):\n    \n    df_attrs = pd.read_csv(ATTRIBUTES_PATH, sep='\\t', skiprows=1,) \n    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n    \n    photo_ids = []\n    for dirpath, dirnames, filenames in os.walk(DATASET_PATH):\n        for fname in filenames:\n            if fname.endswith(\".jpg\"):\n                fpath = os.path.join(dirpath,fname)\n                photo_id = fname[:-4].replace('_',' ').split()\n                person_id = ' '.join(photo_id[:-1])\n                photo_number = int(photo_id[-1])\n                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n\n    photo_ids = pd.DataFrame(photo_ids)\n    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n\n    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n    \n    all_photos = df['photo_path'].apply(imageio.imread)\\\n                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n                                .apply(lambda img: np.array(Image.fromarray(img).resize([dimx,dimy])) )\n\n    all_photos = np.stack(all_photos.values).astype('uint8')\n    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n    \n    return all_photos,all_attrs","d3ae2d91":"data, attrs = fetch_dataset()","6d894e4e":"#45,45\nIMAGE_H = data.shape[1]\nIMAGE_W = data.shape[2]\n\nN_CHANNELS = 3","3059c1f4":"data = np.array(data \/ 255, dtype='float32')\nX_train, X_val = train_test_split(data, test_size=0.2, random_state=42)","52438d8d":"X_train = torch.FloatTensor(X_train)\nX_val = torch.FloatTensor(X_val)","94d07fb7":"dim_z=100","27f11696":"X_train.shape","90ced847":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(45*45*3,1500),\n            nn.BatchNorm1d(1500),\n            nn.ReLU(),\n            nn.Linear(1500,1000),\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            nn.Linear(1000, dim_z),\n            nn.BatchNorm1d(dim_z),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(dim_z,1000),\n            nn.BatchNorm1d(1000),\n            nn.ReLU(),\n            #nn.Linear(500,1000),\n            #nn.ReLU(),\n            nn.Linear(1000,1500),\n            nn.BatchNorm1d(1500),\n            nn.ReLU(),\n            nn.Linear(1500,45*45*3)\n        )\n      \n    def encode(self,x):\n        return self.encoder(x)\n    \n    def decode(self,z):\n        return self.decoder(z)\n        \n    def forward(self, x):\n        encoded = self.encode(x) \n        decoded = self.decode(encoded)     \n\n        \n        return encoded, decoded","02ea9cbd":"class Autoencoder_cnn(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=5, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(in_channels=16, out_channels=3, kernel_size=5, stride=2),\n            #nn.ReLU(),\n            #nn.ConvTranspose2d(in_channels=8, out_channels=8, kernel_size=3, stride=2),\n            #nn.ReLU(),\n            #nn.ConvTranspose2d(in_channels=8, out_channels=3, kernel_size=5, stride=2)\n        )\n        \n    def decode(self,z):\n        return self.decoder(z)\n        \n    def forward(self, x):\n        x = x.permute(0,3,1,2)\n        encoded = self.encoder(x)  \n        decoded = self.decode(encoded)     \n\n        \n        return encoded, decoded","98401a9a":"model_auto = Autoencoder().to(DEVICE)","c1757b6f":"def get_batch(data, batch_size=64):\n    total_len = data.shape[0]\n    for i in range(0, total_len, batch_size):\n        yield data[i:min(i+batch_size,total_len)]\n\ndef plot_gallery(images, h, w, n_row=3, n_col=6, with_title=False, titles=[]):\n    plt.figure(figsize=(1.5 * n_col, 1.7 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n        try:\n            plt.imshow(images[i].reshape((h, w, 3)), cmap=plt.cm.gray, vmin=-1, vmax=1, interpolation='nearest')\n            if with_title:\n                plt.title(titles[i])\n            plt.xticks(())\n            plt.yticks(())\n        except:\n            pass\n        \ndef fit_epoch(model, train_x, criterion, optimizer, batch_size, is_cnn=False):\n    running_loss = 0.0\n    processed_data = 0\n    \n    for inputs in get_batch(train_x,batch_size):\n        \n        if not is_cnn:\n            inputs = inputs.view(-1, 45*45*3)\n        inputs = inputs.to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        encoder, decoder = model(inputs)\n        \n        #print('decoder shape: ', decoder.shape)\n        \n        if not is_cnn:\n            outputs = decoder.view(-1, 45*45*3)\n        else:\n            outputs = decoder.permute(0,2,3,1)\n        \n        loss = criterion(outputs,inputs)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.shape[0]\n        processed_data += inputs.shape[0]\n    \n    train_loss = running_loss \/ processed_data    \n    return train_loss\n\ndef eval_epoch(model, x_val, criterion, is_cnn=False):\n    running_loss = 0.0\n    processed_data = 0\n    model.eval()\n    \n    for inputs in get_batch(x_val):\n        if not is_cnn:\n            inputs = inputs.view(-1, 45*45*3)\n        inputs = inputs.to(DEVICE)\n        \n        with torch.set_grad_enabled(False):\n            encoder, decoder = model(inputs)\n            \n            if not is_cnn:\n                outputs = decoder.view(-1, 45*45*3)\n            else:\n                outputs = decoder.permute(0,2,3,1)\n                \n            loss = criterion(outputs,inputs)\n            running_loss += loss.item() * inputs.shape[0]\n            processed_data += inputs.shape[0]\n    \n    val_loss = running_loss \/ processed_data\n    \n    #draw\n    with torch.set_grad_enabled(False):\n        pic = x_val[3]\n        \n        if not is_cnn:            \n            pic_input = pic.view(-1, 45*45*3)\n        else:\n            pic_input = torch.FloatTensor(pic.unsqueeze(0))\n            \n        pic_input = pic_input.to(DEVICE)        \n        encoder, decoder = model(pic_input)\n        \n        if not is_cnn:\n            pic_output = decoder.view(-1, 45*45*3).squeeze()\n        else:\n            pic_output = decoder.permute(0,2,3,1)\n            \n        pic_output = pic_output.to(\"cpu\")        \n        pic_input = pic_input.to(\"cpu\")\n        plot_gallery([pic_input, pic_output],45,45,1,2)\n    \n    return val_loss\n\ndef train(train_x, val_x, model, epochs=10, batch_size=32, is_cnn=False):     \n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)        \n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        for epoch in range(epochs):            \n            train_loss = fit_epoch(model,train_x,criterion,optimizer,batch_size,is_cnn)\n            val_loss = eval_epoch(model,val_x,criterion, is_cnn)\n            print(\"loss: \", train_loss)\n\n            history.append((train_loss,val_loss))\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n        \n    return history","ca5000c1":"history = train(X_train, X_val, model_auto, epochs=50, batch_size=64)","028c7ad7":"train_loss, val_loss = zip(*history)\nplt.figure(figsize=(15,10))\nplt.plot(train_loss, label='Train loss')\nplt.plot(val_loss, label='Val loss')\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.plot();","a87a9e66":"z = np.random.randn(25, dim_z)\nprint(z.shape)\n\nwith torch.no_grad():\n    inputs = torch.FloatTensor(z)    \n    inputs = inputs.to(DEVICE)\n    model_auto.eval()\n    output = model_auto.decode(inputs)\n    plot_gallery(output.data.cpu().numpy(), IMAGE_H, IMAGE_W, n_row=5, n_col=5)","e7b7e288":"attrs.head()","6c1676e7":"attrs.columns","6a02e2b6":"smile_ids = attrs['Smiling'].sort_values(ascending=False).iloc[100:125].index.values\nsmile_data = data[smile_ids]\n\nno_smile_ids = attrs['Smiling'].sort_values(ascending=True).head(25).index.values\nno_smile_data = data[no_smile_ids]\n\neyeglasses_ids = attrs['Eyeglasses'].sort_values(ascending=False).head(25).index.values\neyeglasses_data = data[eyeglasses_ids]\n\nsunglasses_ids = attrs['Sunglasses'].sort_values(ascending=False).head(25).index.values\nsunglasses_data = data[sunglasses_ids]","ebca7d09":"plot_gallery(smile_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=smile_ids)","adcc5cfc":"plot_gallery(no_smile_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=no_smile_ids)","5b7dfd83":"plot_gallery(eyeglasses_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=eyeglasses_ids)","682bcf82":"plot_gallery(sunglasses_data, IMAGE_H, IMAGE_W, n_row=5, n_col=5, with_title=True, titles=sunglasses_ids)","9c5229d1":"def to_latent(pic):\n    with torch.no_grad():\n        inputs = torch.FloatTensor(pic.reshape(-1, 45*45*3))\n        inputs = inputs.to(DEVICE)\n        model_auto.eval()\n        output = model_auto.encode(inputs)        \n        return output\n\ndef from_latent(vec):\n    with torch.no_grad():\n        inputs = vec.to(DEVICE)\n        model_auto.eval()\n        output = model_auto.decode(inputs)        \n        return output","d4dc82ce":"smile_latent = to_latent(smile_data).mean(axis=0)\nno_smile_latent = to_latent(no_smile_data).mean(axis=0)\nsunglasses_latent = to_latent(sunglasses_data).mean(axis=0)\n\nsmile_vec = smile_latent-no_smile_latent\nsunglasses_vec = sunglasses_latent - smile_latent\n\ndef make_me_smile(ids):\n    for id in ids:\n        pic = data[id:id+1]\n        latent_vec = to_latent(pic)\n        latent_vec[0] += smile_vec\n        pic_output = from_latent(latent_vec)\n        pic_output = pic_output.view(-1,45,45,3).cpu()\n        plot_gallery([pic,pic_output], IMAGE_H, IMAGE_W, n_row=1, n_col=2)\n        \ndef give_me_sunglasses(ids):\n    for id in ids:\n        pic = data[id:id+1]\n        latent_vec = to_latent(pic)\n        latent_vec[0] += sunglasses_vec\n        pic_output = from_latent(latent_vec)\n        pic_output = pic_output.view(-1,45,45,3).cpu()\n        plot_gallery([pic,pic_output], IMAGE_H, IMAGE_W, n_row=1, n_col=2)\n    ","58bb8ed6":"make_me_smile(no_smile_ids)","ce476c50":"give_me_sunglasses(smile_ids)","c92dd9eb":"dim_z = 256","6d987cf5":"class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        self.fc1 = nn.Linear(45*45*3, 1500)\n        self.fc21 = nn.Linear(1500, dim_z)\n        self.fc22 = nn.Linear(1500, dim_z)\n        self.fc3 = nn.Linear(dim_z, 1500)\n        self.fc4 = nn.Linear(1500, 45*45*3)        \n        self.relu = nn.LeakyReLU()\n\n    def encode(self, x):\n        x = self.relu(self.fc1(x))\n        return self.fc21(x), self.fc22(x)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 *logvar)\n        eps = torch.randn_like(std)\n        return eps.mul(std).add_(mu)\n    \n    def decode(self, z):\n        z = self.relu(self.fc3(z)) #1500\n        return torch.sigmoid(self.fc4(z))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        z = self.decode(z)\n        return z, mu, logvar\n    \ndef loss_vae_fn(x, recon_x, mu, logvar):    \n    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return BCE + KLD","c483b43f":"model_vae = VAE().to(DEVICE)","a065d184":"def fit_epoch_vae(model, train_x, optimizer, batch_size, is_cnn=False):\n    running_loss = 0.0\n    processed_data = 0\n    \n    for inputs in get_batch(train_x,batch_size):\n        inputs = inputs.view(-1, 45*45*3)\n        inputs = inputs.to(DEVICE)        \n        optimizer.zero_grad()\n        \n        decoded,mu,logvar, = model(inputs)\n        outputs = decoded.view(-1, 45*45*3)\n        outputs = outputs.to(DEVICE)\n        \n        loss = loss_vae_fn(inputs,outputs,mu,logvar)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * inputs.shape[0]\n        processed_data += inputs.shape[0]\n    \n    train_loss = running_loss \/ processed_data    \n    return train_loss\n\ndef eval_epoch_vae(model, x_val, batch_size):\n    running_loss = 0.0\n    processed_data = 0\n    model.eval()\n    \n    for inputs in get_batch(x_val,batch_size=batch_size):\n        inputs = inputs.view(-1, 45*45*3)\n        inputs = inputs.to(DEVICE)\n        \n        with torch.set_grad_enabled(False):\n            decoded,mu,logvar = model(inputs)\n            outputs = decoded.view(-1, 45*45*3)        \n            loss = loss_vae_fn(inputs,outputs,mu,logvar)\n            running_loss += loss.item() * inputs.shape[0]\n            processed_data += inputs.shape[0]\n    \n    val_loss = running_loss \/ processed_data\n    \n    #draw\n    with torch.set_grad_enabled(False):\n        pic = x_val[3]         \n        pic_input = pic.view(-1, 45*45*3)            \n        pic_input = pic_input.to(DEVICE)        \n        decoded,mu,logvar = model(inputs)        \n        pic_output = decoded[0].view(-1, 45*45*3).squeeze()\n        pic_output = pic_output.to(\"cpu\") \n        pic_input = pic_input.to(\"cpu\")\n        plot_gallery([pic_input, pic_output],45,45,1,2)\n    \n    return val_loss\n\ndef train_vae(train_x, val_x, model, epochs=10, batch_size=32, lr=0.001):\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)        \n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} val_loss: {val_loss:0.4f}\"\n    \n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        for epoch in range(epochs):            \n            train_loss = fit_epoch_vae(model,train_x,optimizer,batch_size)\n            val_loss = eval_epoch_vae(model,val_x,batch_size)\n            print(\"loss: \", train_loss)\n\n            history.append((train_loss,val_loss))\n\n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss, val_loss=val_loss))            \n        \n    return history","7e0e4282":"history_vae = train_vae(X_train, X_val, model_vae, epochs=50, batch_size=128, lr=0.001)","9c85b559":"train_loss, val_loss = zip(*history_vae)\nplt.figure(figsize=(15,10))\nplt.plot(train_loss, label='Train loss')\nplt.plot(val_loss, label='Val loss')\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.plot();","3a355ff0":"# A bit of theory\n\n\"Autoencoding\" is a data compression algorithm where the compression and decompression functions are 1) data-specific, 2) lossy, and 3) learned automatically from examples rather than engineered by a human. Additionally, in almost all contexts where the term \"autoencoder\" is used, the compression and decompression functions are implemented with neural networks.\n\n1) Autoencoders are data-specific, which means that they will only be able to compress data similar to what they have been trained on. This is different from, say, the MPEG-2 Audio Layer III (MP3) compression algorithm, which only holds assumptions about \"sound\" in general, but not about specific types of sounds. An autoencoder trained on pictures of faces would do a rather poor job of compressing pictures of trees, because the features it would learn would be face-specific.\n\n2) Autoencoders are lossy, which means that the decompressed outputs will be degraded compared to the original inputs (similar to MP3 or JPEG compression). This differs from lossless arithmetic compression.\n\n3) Autoencoders are learned automatically from data examples, which is a useful property: it means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input. It doesn't require any new engineering, just appropriate training data.\n\nsource: https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n","95b7808d":"# Train autoencoder","a56bbe6c":"Let's generate some samples from random vectors","477447ec":"# Sampling","3328ac8b":"# Prepare the dataset","f83b9722":"So far we have trained our encoder to reconstruct the very same image that we've transfered to latent space. That means that when we're trying to **generate** new image from the point decoder never met we're getting _the best image it can produce_, but the quelity is not good enough. \n\n> **In other words the encoded vectors may not be continuous in the latent space.**\n\nIn other hand Variational Autoencoders makes not only one encoded vector but **two**:\n- vector of means, \u03bc;\n- vector of standard deviations, \u03c3.\n\n![https:\/\/miro.medium.com\/max\/657\/1*CiVcrrPmpcB1YGMkTF7hzA.png](https:\/\/miro.medium.com\/max\/657\/1*CiVcrrPmpcB1YGMkTF7hzA.png)\n\n> picture from https:\/\/towardsdatascience.com\/intuitively-understanding-variational-autoencoders-1bfe67eb5daf\n\n","0a2c3d76":"Variational autoencoders are cool. Although models in this particular notebook are simple they let us design complex generative models of data, and fit them to large datasets. They can generate images of fictional celebrity faces and high-resolution digital artwork.\nThese models also yield state-of-the-art machine learning results in image generation and reinforcement learning. Variational autoencoders (VAEs) were defined in 2013 by Kingma et al. and Rezende et al.","3836df61":"While the concept is pretty straightforward the simple autoencoder have some disadvantages. Let's explore them and try to do better.","7c75dcb2":"# Adding smile and glasses\n\nLet's find some attributes like smiles or glasses on the photo and try to add it to the photos which don't have it. We will use the second dataset for it. It contains a bunch of such attributes. ","f87da029":"# Variational autoencoder","1dfa941c":"# Fun with Variational Autoencoders\n\nThis is a starter kernel to use **Labelled Faces in the Wild (LFW) Dataset** in order to maintain knowledge about main Autoencoder principles. PyTorch will be used for modelling.\n\n\n### **Fork it and give it an upvote.**\n\n\n\n\n![architecture](https:\/\/miro.medium.com\/max\/3636\/1*LSYNW5m3TN7xRX61BZhoZA.png)\n\nUseful links:\n\n\n* [Building Autoencoders in Keras](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html)\n* [Conditional VAE (Russian)](https:\/\/habr.com\/ru\/post\/331664\/)\n* [Tutorial on Variational Autoencoders](https:\/\/arxiv.org\/abs\/1606.05908)\n* [Introducing Variational Autoencoders (in Prose and Code)](https:\/\/blog.fastforwardlabs.com\/2016\/08\/12\/introducing-variational-autoencoders-in-prose-and.html)\n* [How Autoencoders work - Understanding the math and implementation (Notebook)](https:\/\/www.kaggle.com\/shivamb\/how-autoencoders-work-intro-and-usecases)\n* [Tutorial - What is a variational autoencoder?](https:\/\/jaan.io\/what-is-variational-autoencoder-vae-tutorial\/)\n\n","c22bf87c":"# Explore the data","a9374cb6":"# Building simple autoencoder","40e539d6":"# Conclusion","595bb7a9":"Calculating latent space vector for the selected images."}}