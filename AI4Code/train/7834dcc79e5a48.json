{"cell_type":{"757724be":"code","6bf8a402":"code","274e6d9a":"code","444ab49d":"code","6942216b":"code","484b58ba":"code","71ca06f0":"code","d5c7b46d":"code","30e798e7":"code","1a2d437e":"code","0d273f4d":"code","18a1e8b9":"code","52762124":"code","a81dc893":"code","987c2c4d":"code","44aa7a3d":"code","fa839f68":"code","3bb65458":"code","d2f63b19":"code","64be1c73":"markdown"},"source":{"757724be":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nimport random\nimport os\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","6bf8a402":"%%time\nfiles = ['..\/input\/lish-moa\/test_features.csv', \n         '..\/input\/lish-moa\/train_targets_scored.csv',\n         '..\/input\/lish-moa\/train_features.csv',\n         '..\/input\/lish-moa\/train_targets_nonscored.csv',\n         '..\/input\/lish-moa\/sample_submission.csv']\n\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test, train_target, train, train_nonscored, sub = pool.map(load_data, files)\n    \n    \n    \n","274e6d9a":"def mapping_and_filter(train, train_targets, test):\n    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n    cp_dose = {'D1': 0, 'D2': 1}\n    for df in [train, test]:\n        df['cp_type'] = df['cp_type'].map(cp_type)\n        df['cp_dose'] = df['cp_dose'].map(cp_dose)\n    train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n    train = train[train['cp_type'] == 0].reset_index(drop = True)\n    train_targets.drop(['sig_id'], inplace = True, axis = 1)\n    return train, train_targets, test\n\n# Function to scale our data\ndef scaling(train, test):\n    features = train.columns[2:]\n    scaler = RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n# Function to extract pca features\ndef fe_pca(train, test, n_components_g = 520, n_components_c = 46, SEED = 123):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n        train_ = train[features].copy()\n        test_ = test[features].copy()\n        data = pd.concat([train_, test_], axis = 0)\n        pca = PCA(n_components = n_components,  random_state = SEED)\n        data = pca.fit_transform(data)\n        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n        data = pd.DataFrame(data, columns = columns)\n        train_ = data.iloc[:train.shape[0]]\n        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n        train = pd.concat([train, train_], axis = 1)\n        test = pd.concat([test, test_], axis = 1)\n        return train, test\n    \n    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n    return train, test\n\n# Function to extract common stats features\ndef fe_stats(train, test):\n    \n    features_g = list(train.columns[4:776])\n    features_c = list(train.columns[776:876])\n    \n    for df in [train, test]:\n        df['g_sum'] = df[features_g].sum(axis = 1)\n        df['g_mean'] = df[features_g].mean(axis = 1)\n        df['g_std'] = df[features_g].std(axis = 1)\n        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n        df['g_skew'] = df[features_g].skew(axis = 1)\n        df['c_sum'] = df[features_c].sum(axis = 1)\n        df['c_mean'] = df[features_c].mean(axis = 1)\n        df['c_std'] = df[features_c].std(axis = 1)\n        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n        df['c_skew'] = df[features_c].skew(axis = 1)\n        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n        \n    return train, test\n\ndef c_squared(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_squared'] = df[feature] ** 2\n    return train, test\n\n# Function to calculate the mean log loss of the targets including clipping\ndef mean_log_loss(y_true, y_pred):\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n    metrics = []\n    for target in range(206):\n        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n    return np.mean(metrics)","444ab49d":"print(train.shape)\nprint(test.shape)","6942216b":"#train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\n#train_target = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n#test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n#sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntrain, train_target, test = mapping_and_filter(train, train_target, test)\ntrain, test = fe_stats(train, test)\ntrain, test = c_squared(train, test)\ntrain, test = fe_pca(train, test, n_components_g = 520, n_components_c = 46, SEED = 123)\ntrain, test, features = scaling(train, test)","484b58ba":"print(train.shape)\nprint(test.shape)","71ca06f0":"targets = [col for col in train_target.columns if col != 'sig_id']\nprint('Number of different labels:', len(targets))","d5c7b46d":"features = [col for col in train.columns if col != 'sig_id']\nprint('Number of features:', len(features))","30e798e7":"print(train_target.shape)\n","1a2d437e":"#for feature in ['cp_type', 'cp_dose']:\n#    le = LabelEncoder()\n#    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n#    train[feature] = le.transform(list(train[feature].astype(str).values))\n#    test[feature] = le.transform(list(test[feature].astype(str).values))","0d273f4d":"X = train[features]","18a1e8b9":"print(X.shape)","52762124":"params = {'num_leaves': 491,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.3,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 106,\n          'objective': 'binary',\n          'max_depth': -1,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'binary_logloss',\n          \"verbosity\": 0,\n          'reg_alpha': 0.4,\n          'reg_lambda': 0.6,\n          'random_state': 47\n         }","a81dc893":"accumulative_loss = 0\nskf = StratifiedKFold(n_splits=3, random_state=47, shuffle=True)\n\n# 206 different models. One for each label\nfor model, target in enumerate(targets, 1):\n    y = train_target[target]\n    start_time = time()\n    preds = np.zeros(test.shape[0])\n    oof = np.zeros(X.shape[0])\n\n    for trn_idx, test_idx in skf.split(X, y):\n        trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n        val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n        clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=0, early_stopping_rounds=25)\n        oof[test_idx] = clf.predict(X.iloc[test_idx])\n        preds += clf.predict(test[features]) \/ skf.n_splits\n\n    sub[target] = preds\n    loss = log_loss(y, oof)\n    accumulative_loss += loss\n    print('[{}] Model: {} logloss: {:.3f}'.format(str(datetime.timedelta(seconds=time() - start_time))[:7], model, loss))\n\n    del preds, oof, start_time, y, loss\n    gc.collect();","987c2c4d":"print('Overall mean loss: {:.3f}'.format(accumulative_loss \/ 206))","44aa7a3d":"def submission(test_pred):\n    sub.loc[:, train_target.columns] = test_pred\n    sub.loc[test['cp_type'] == 1, train_target.columns] = 0\n    sub.to_csv('submission.csv', index = False)\n    return sub","fa839f68":"#submission(sub)","3bb65458":"sub = submission(sub)\nsub.head()","d2f63b19":"# sub.to_csv('submission.csv', index=False)","64be1c73":"fork from https:\/\/www.kaggle.com\/ragnar123\/moa-dnn-feature-engineering\nand https:\/\/www.kaggle.com\/nroman\/moa-lightgbm-206-models"}}