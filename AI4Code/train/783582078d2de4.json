{"cell_type":{"6840e085":"code","65d4c099":"code","e2df827a":"code","c831e4cb":"code","3cb5e754":"code","ec36c7d5":"code","c9f02da3":"code","1657d494":"code","7d486d2a":"code","5088ac5e":"code","319e90f9":"code","67ac5b99":"code","223b5c39":"code","bb362768":"code","f6f9340d":"markdown","59d4ee66":"markdown","c8afe814":"markdown","6241e1f4":"markdown"},"source":{"6840e085":"import spacy\nimport pandas as pd\nimport numpy as np\nnlp = spacy.load('en_core_web_lg')\n\n# Load the GAP data\n\ndata = pd.concat([pd.read_csv('https:\/\/github.com\/google-research-datasets\/gap-coreference\/blob\/master\/gap-development.tsv?raw=true', sep='\\t'),\n                  pd.read_csv('https:\/\/github.com\/google-research-datasets\/gap-coreference\/blob\/master\/gap-validation.tsv?raw=true', sep='\\t'),\n                  pd.read_csv('https:\/\/github.com\/google-research-datasets\/gap-coreference\/blob\/master\/gap-test.tsv?raw=true', sep='\\t')\n                 ], ignore_index = True)","65d4c099":"# Two useful syntactic relations\n\ndef domain(t):\n    while not t._.subj and not t._.poss and\\\n            not (t.dep_ == 'xcomp' and t.head._.obj) and\\\n            t != t.head:\n        t = t.head\n    return t\n\ndef ccom(t):\n    return [t2 for t2 in t.head._.d]\n\n# spacy extensions:\n#   doc._.to(offset) => t at text char offset\n#   t._.c => t's children (list)                     #   t._.d => t's descendents (list)\n#   t._.subj => t's subject else False               #   t._.obj => t's object else False\n#   t._.domain => t's syntactic domain               #   t._.ccom => t's c-command domain\n\nspacy.tokens.doc.Doc.set_extension(\n    'to', method=lambda doc, offset: [t for t in doc if t.idx == offset][0], force=True)\nspacy.tokens.token.Token.set_extension(\n    'c', getter=lambda t: [c for c in t.children], force=True)\nspacy.tokens.token.Token.set_extension(\n    'd', getter=lambda t: [c for c in t.sent if t in list(c.ancestors)], force=True)\nspacy.tokens.token.Token.set_extension(\n    'subj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('nsubj')] + [False])[0], force=True)\nspacy.tokens.token.Token.set_extension(\n    'obj', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('dobj')] + [False])[0], force=True)\nspacy.tokens.token.Token.set_extension(\n    'poss', getter=lambda t: ([c for c in t._.c if c.dep_.startswith('poss')] + [False])[0], force=True)\nspacy.tokens.token.Token.set_extension(\n    'span', method=lambda t, t2: t.doc[t.i:t2.i] if t.i < t2.i else t.doc[t2.i:t.i], force=True)\nspacy.tokens.token.Token.set_extension('domain', getter=domain, force=True)\nspacy.tokens.token.Token.set_extension('ccom', getter=ccom, force=True)","e2df827a":"# Disqualification functions\n\n# Prune candidate list given a disqualifying condition (a set of tokens)\ndef applyDisq(condition, candidates, candidate_dict, debug = False):\n    badnames = sum([nameset(c, candidate_dict) for c in candidates if c in condition[0]], [])\n    badcands = [c for c in candidates if c.text in badnames]\n    if debug and len(badcands) > 0: print('Disqualified:', badcands, '<', condition[1])\n    return [c for c in candidates if c not in badcands]\n\n# Apply a list of disqualifying conditions\ndef applyDisqs(conditions, candidates, candidate_dict, debug = False):\n    for condition in conditions:\n        if len(candidates) < 1: return candidates\n        candidates = applyDisq(condition, candidates, candidate_dict, debug)\n    return candidates\n\n# Pass the list of disqualifying conditions for possessive pronouns (his, her)\ndef disqGen(t, candidates, candidate_dict, debug = False):\n    conds = [(t._.ccom,\n             \"disqualify candidates c-commanded by genpn; e.g. e.g. *Julia read his_i book about John_i's life.\"),\n             ([t2 for t2 in candidates if t in t2._.ccom and t2.head.dep_ == 'appos'],\n             \"disqualify candidates modified by an appositive with genpn; e.g. *I wanted to see John_i, his_i father.\")\n            ]\n    return applyDisqs(conds, candidates, candidate_dict, debug)\n\n# Pass the list of list of disqualifying conditions for other pronouns\ndef disqOthers(t, candidates, candidate_dict, debug = False):\n    conds = [([t2 for t2 in t._.ccom if t2.i > t.i],\n             \"disqualify candidates c-commanded by pn, unless they were preposed;\\\n             e.g. *He_i cried before John_i laughed. vs. Before John_i laughed, he_i cried.\"),\n             ([t2 for t2 in candidates if t in t2._.ccom and t2._.domain == t._.domain\n              and not (t.head.text == 'with' and t.head.head.lemma_ == 'take')],\n             \"disqualify candidates that c-command pn, unless in different domain;\\\n             e.g. Mary said that *John_i hit him_i. vs. John_i said that Mary hit him_i;\\\n             random hard-coded exception: `take with'\"),\n             ([t2 for t2 in candidates if t2._.domain.dep_ == 'xcomp' and t2._.domain.head._.obj and t2 == t2._.domain.head._.obj],\n             \"for xcomps with subjects parsed as upstairs dobj, disallow coref with that dobj;\\\n             e.g. *Mary wanted John_i to forgive him_i.\")\n            ]\n    return applyDisqs(conds, candidates, candidate_dict, debug)\n\n# Decide whether possessive or not and call appropriate function\ndef disq(t, candidates, candidate_dict, debug = False):\n    func = disqGen if t.dep_ == 'poss' else disqOthers\n    candidates = func(t, candidates, candidate_dict, debug)\n    return candidates","c831e4cb":"# Name functions\n\n# Find word of interest at provided offset; sometimes parsed words don't align with provided data, so need to look back\ndef find_head(w, wo, doc):\n    t = False; backtrack = 0\n    while not t:\n        try:\n            t = doc._.to(wo)\n        except IndexError:\n            wo -= 1; backtrack += 1\n    while t.dep_ == 'compound' and t.head.idx >= wo and t.head.idx < len(w) + wo + backtrack: t = t.head\n    return t\n\n# Returns subsequences of a name\ndef subnames(name):\n    if type(name) != str: name = candidate_dict[name]\n    parts = name.split(' ')\n    subnames_ = []\n    for i in range(len(parts)): \n        for j in range(i + 1, len(parts) + 1): \n            sub = ' '.join(parts[i:j])\n            if len(sub) > 2: subnames_.append(sub)\n    return subnames_\n\n# Returns subsequences of a name unless potentially ambiguous (if another candidate picks out same subsequence)\ndef nameset(name, candidate_dict):\n    if type(name) != str: name = candidate_dict[name]\n    subnames_ = [sn for sn in subnames(name)]\n    return [c for c in subnames_ if c not in sum([subnames(c) for c in candidate_dict.values() \n                                                  if c not in subnames_ and name not in subnames(c)], [])]\n\n# Given the original candidate dict and the final candidate list, returns new dict grouping putative candidate instances under a single key\ndef candInstances(candidates, candidate_dict):\n    candidates_by_name = {}\n    for c in sorted(candidates, key = lambda c: len(candidate_dict[c]), reverse = True):\n        name = candidate_dict[c]\n        for name2 in candidates_by_name.keys():\n            if name in nameset(name2, candidate_dict): name = name2; break\n        candidates_by_name[name] = candidates_by_name.get(name, []) + [c]\n    return candidates_by_name\n\nimport gender_guesser.detector as gender # oops\ngd = gender.Detector()\n\n# Needed to prune candidate dict-- removes non-provided candidates that don't match in most common gender with pn\ndef filterGender(candidates_by_name, a, b, pn):\n    badnames = []\n    gender = 'female' if pn in ['She', 'she', 'her', 'Her'] else 'male'\n    for name in candidates_by_name.keys():\n        if a in subnames(name) or b in subnames(name): continue\n        genderii = gd.get_gender(name.split(' ')[0])\n        if gender == 'male' and genderii == 'female': badnames += [name]; continue\n        if gender == 'female' and genderii == 'male': badnames += [name]; continue\n    for name in badnames: candidates_by_name.pop(name)\n    return candidates_by_name","3cb5e754":"# Metrics\n\nfrom urllib.parse import unquote\nimport re\n\n# Authors' metric 1: Does the Wikipedia url contain the candidate's name?\ndef urlMatch(a, b, url, candidate_dict):\n    url = re.sub('[^\\x00-\\x7F]', '*', unquote(url.split('\/')[-1])).replace('_', ' ').lower()\n    return {'a_url': (sorted([len(n.split(' ')) for n in nameset(a.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0],\n            'b_url': (sorted([len(n.split(' ')) for n in nameset(b.lower(), candidate_dict) if n in nameset(url, candidate_dict)], reverse = True) + [0])[0]}\n\n# Authors' metric 2: When pn is subject or object, does the candidate match?\ndef parallel(t1, t2):\n    if t1.dep_.startswith('nsubj'): return t2.dep_.startswith('nsubj')\n    if t1.dep_.startswith('dobj'): return t2.dep_.startswith('dobj')\n    if t1.dep_.startswith('dative'): return t2.dep_.startswith('dative')\n    return False\n\n# Depth from a node to a parent node\ndef depthTo(t1, t2):\n    depth = 0\n    while t1 != t2 and t1 != t1.head:\n        t1 = t1.head\n        depth += 1\n    return depth\n\n# Syntactic distance within a single tree\ndef nodeDist(t1, t2):\n    if t1 == t2: return 0\n    if t2 in t1._.d: return depthTo(t2, t1)\n    if t1 in t2._.d: return depthTo(t1, t2)\n    t = t1\n    while t1 not in t._.d or t2 not in t._.d and t != t.head: t = t.head\n    return depthTo(t1, t) + depthTo(t2, t)\n\n# Authors' metric 3: Syntactic distance (within or across trees)\ndef synDist(t, pn, doc, debug = False):\n    doc_sents = list(doc.sents)\n    sspan = doc_sents.index(pn.sent) - doc_sents.index(t.sent)\n    if sspan == 0: # same sentence\n        dist = nodeDist(t, pn)\n    else: # different sentence\n        dist = nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root) + nodeDist(t, doc_sents[doc_sents.index(t.sent)].root) # dist from two roots\n    if debug: \n        print('pn dist:', nodeDist(pn, doc_sents[doc_sents.index(pn.sent)].root), '; t dist:',\n              nodeDist(t, doc_sents[doc_sents.index(t.sent)].root), '; span:', sspan)\n    sspan = abs(sspan) * 1 if sspan >= 0 else abs(sspan) * 1.3 # less local if not preceding\n    return dist + sspan# * 0.7\n\n# Character distance\ndef charDist(t1, t2):\n    if t2.idx > t1.idx:\n        return t2.idx - t1.idx + len(t1.text)\n    else:\n        return (t1.idx - t2.idx + len(t2.text)) * 1.3\n\n# Theta prominence: assign a 0.1 to 1 score based on dep role of candidate -- strong feature\ndef thetaProminence(t, mult = 1, debug = False):\n    while t.dep_ == 'compound': t = t.head\n    if debug: print('t dep_:', t.dep_)\n    if t.dep_ == 'pobj': mult = 1.3 if t.head.i < t.head.head.i else 1\n    if t._.domain.dep_ == 'advcl': mult = 1.3 if t.head.i < t._.domain.head.i else 1\n    if t.dep_.startswith('nsubj'): score = 1\n    elif t.dep_.startswith('dobj'): score = 0.8\n    elif t.dep_.startswith('dative'): score = 0.6\n    elif t.dep_.startswith('pobj'): score = 0.4\n    elif t.dep_.startswith('poss'): score = 0.3\n    else: score = 0.1\n    if debug: print('mult:', mult, '; score:', score)\n    return min(1, score * mult)\n\n# Computes these metrics for each candidate, and returns, for each group of instances (A instances, B instances,\\\n# other instances), either the sum, or the highest difference from the mean\ndef score(label, candidates_by_name, a_cand, b_cand, func, minsc = None, method = 'sum'):\n    if method == 'sum':\n        scores = {name: sum([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n    elif method == 'meandiff':\n        mean = np.mean(sum([[func(t) for t in tokens] for tokens in candidates_by_name.values()], []))\n        scores = {name: mean - min([func(t) for t in tokens]) for name, tokens in candidates_by_name.items()}\n    sca = scores[a_cand] if a_cand else minsc\n    scb = scores[b_cand] if b_cand else minsc\n    screst = [v for n, v in scores.items() if n != a_cand and n != b_cand]\n    if method == 'sum':\n        screst = sum(screst) if len(screst) > 0 else minsc\n    elif method == 'meandiff':\n        screst = max(screst) if len(screst) > 0 else minsc\n    return {'a_' + label: sca, 'b_' + label: scb, 'n_' + label: screst}","ec36c7d5":"from tqdm import tqdm_notebook as tqdm\n\n# Load a rowfull of data\ndef load_row(data, i):\n    return tuple(data.iloc[i])\n\n# Row by row, populate features\ndef annotateSet(data, minsc = None, debug = False):\n    \n    annotated_data = pd.DataFrame() # init placeholder df\n    row_batch = []\n\n    for i in tqdm(range(annotated_data.shape[0], data.shape[0])):\n        id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data, i)        \n        doc = nlp(text) # parse text into doc\n        pnt, at, bt = (doc._.to(pno), find_head(a, ao, doc), find_head(b, bo, doc)) # get the tokens that correspond to offsets\n        candidate_dict = {e.root: re.sub('\\'s$', '', e.text) for e in [e for e in doc.ents if e.root.ent_type_ == 'PERSON']} # first get every PERSON ent as candidate\n        candidate_dict.update({c.root: re.sub('\\'s$', '', c.text) for c in doc.noun_chunks if c.root.pos_ == 'PROPN' and c.text in sum([subnames(n) for n in candidate_dict.values()], []) and\n                               c.root not in candidate_dict.keys()}) # get some missed ones by looking at noun chunks with PROPN roots whose text match part of a candidate but are not already in list\n        candidate_dict.update({t: w for t, w in [(at, a), (bt, b)]}) # add provided cands, overwriting in the process\n\n        candidates = disq(pnt, list(candidate_dict.keys()), candidate_dict, debug = False)\n        candidates_by_name = candInstances(candidates, candidate_dict)\n        candidates_by_name = filterGender(candidates_by_name, a, b, pn)\n        a_cand = ([name for name, tokens in candidates_by_name.items() if at in tokens] + [False])[0]\n        b_cand = ([name for name, tokens in candidates_by_name.items() if bt in tokens] + [False])[0]\n    \n        # init row dict\n        features = {'id': id, 'label': 0 if ag else 1 if bg else 2}\n        # eliminated or not\n        features.update({'a_out': 0 if a_cand else 1, 'b_out': 0 if b_cand else 1})\n        # url match or not\n        features.update(urlMatch(a, b, url, candidate_dict))\n        # c-command or not\n        features.update({'a_cc': 1 if a_cand and pnt in at._.ccom else 0, 'b_cc': 1 if b_cand and pnt in bt._.ccom else 0})\n        # parallelism score\n        features.update(score('par', candidates_by_name, a_cand, b_cand, lambda t: parallel(t, pnt), minsc = minsc))\n        # theta prominence score\n        features.update(score('th', candidates_by_name, a_cand, b_cand, thetaProminence, minsc = minsc))\n        # syntactic distance score\n        features.update(score('loc', candidates_by_name, a_cand, b_cand, lambda t: synDist(t, pnt, doc), method='meandiff', minsc = minsc))\n        # number of candidates left\n        features.update({'n_cands': len(candidates_by_name)})\n        # char dist\n        features.update(score('cloc', candidates_by_name, a_cand, b_cand, lambda t: charDist(t, pnt), method='meandiff', minsc = minsc))\n\n        if debug: print(id, '>', 'a:', 1 if ag else 0, 'b:', 1 if bg else 0, features)\n        row_batch += [features]\n\n    # add rows to placeholder df\n    if annotated_data.shape[0] != data.shape[0]: annotated_data = annotated_data.append(row_batch, ignore_index = True)\n    \n    return annotated_data","c9f02da3":"annotated_data = annotateSet(data)","1657d494":"# Readable rows\n\nfrom textwrap import TextWrapper\nwrapper = TextWrapper(width=75)\n\ndef style(w, wstyle):\n    if wstyle == None: return '{}{}{}'.format('\\033[1m', w, '\\033[0m') # bold\n    elif wstyle == True: return '{}{}{}'.format('\\033[92m', w, '\\033[0m') # green\n    elif wstyle == False: return '{}{}{}'.format('\\033[91m', w, '\\033[0m') # red\n\ndef readable_rows(data, range):\n    rows = []\n    for i in range:\n        wois = sorted([(data.iloc[i]['Pronoun'], data.iloc[i]['Pronoun-offset'], None),\\\n                   (data.iloc[i]['A'], data.iloc[i]['A-offset'], data.iloc[i]['A-coref']),\\\n                   (data.iloc[i]['B'], data.iloc[i]['B-offset'], data.iloc[i]['B-coref'])],\\\n                   key = lambda x: x[1]) # sort by offset\n        text = ''; ftext = data.iloc[i]['Text']; coffset = 0\n        for w, woffset, wstyle in wois:\n            text += ftext[coffset:woffset] + style(w, wstyle)\n            coffset += len(ftext[coffset:woffset]) + len(w)\n        text += ftext[coffset:]\n        rows += [(str(i), data.iloc[i]['ID'], 'A' if data.iloc[i]['A-coref'] else 'B' if data.iloc[i]['B-coref'] else 'N', text)]\n    return rows\n\ndef print_readable_rows(data, range):\n    for index, id, target, text in readable_rows(data, range):\n        text = '\\n\\t'.join(wrapper.wrap(text = text))\n        print('{} ({}): {}>\\n\\t{}\\n'.format(id, index, target, text))\n        \ndef print_tokens(doc):\n    for i in range(0, len(doc), 3):\n        for t in doc[i:i+3]:\n            print(\"{}[{}] >{}> {}\".format(t.text, t.pos_, t.dep_, t.head.text), end = ' | ')\n        print('')","7d486d2a":"misdisqs_a = annotated_data.loc[(annotated_data['a_out'] == 1) & (annotated_data['label'] == 0)]\nmisdisqs_b = annotated_data.loc[(annotated_data['b_out'] == 1) & (annotated_data['label'] == 1)]\n\nprint('As wrongly disqualified:', misdisqs_a.shape[0], ', Bs wrongly disqualified:', misdisqs_b.shape[0], '\\n')\n\nfor i in misdisqs_a.index[:1]: # the first\n    print_readable_rows(data, [i])\n    id, text, pn, pno, a, ao, ag, b, bo, bg, url = load_row(data,i)\n    doc = nlp(text); print_tokens(doc)\n    print('')","5088ac5e":"train = annotated_data.loc[(annotated_data['id'].str.contains('test'))] # swap later\nvalid = annotated_data.loc[(annotated_data['id'].str.contains('validation'))]\ntrain_valid = pd.concat([train, valid])\ntest = annotated_data.loc[(annotated_data['id'].str.contains('development'))] # swap later\n\nanswer_columns = ['label']\nexcl = ['id']\nexcl += ['a_out', 'b_out']\nfeature_columns = [col for col in annotated_data.columns if col not in answer_columns and col not in excl]\n\nX_train = train[feature_columns]\ny_train = np.array(train[answer_columns]).ravel().astype(int)\nX_valid = valid[feature_columns]\ny_valid = np.array(valid[answer_columns]).ravel().astype(int)\nX_train_valid = train_valid[feature_columns]\ny_train_valid = np.array(train_valid[answer_columns]).ravel().astype(int)\nX_test = test[feature_columns]\ny_test = np.array(test[answer_columns]).ravel().astype(int)\n\nrandom_state = 56\n\nX_train.corr()","319e90f9":"# Train a classifier on it\n\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.metrics import log_loss\n\nclf = XGBClassifier(random_state=random_state)\nclf.fit(X_train_valid.values, y_train_valid)\ny_pred = clf.predict(X_test.values)\ny_probs = clf.predict_proba(X_test.values)\nprint('Accuracy:', accuracy_score(y_test, y_pred),\n      '\\nRecall:', recall_score(y_test, y_pred, average=None),\n      '\\nPrecision:', precision_score(y_test, y_pred, average=None),\n      '\\nF1:', f1_score(y_test, y_pred, average=None),\n      '\\nLog-loss:', log_loss(y_test, y_probs))\n\ny_test_prime = np.array([(y_test == 0), (y_test == 1), (y_test == 2)]).transpose().astype(int)\n\n# first few predictions\nprint(pd.DataFrame({'a_pred': y_probs[:,0], 'b_pred': y_probs[:,1], 'n_pred': y_probs[:,2],\n                    'a': y_test_prime[:,0], 'b': y_test_prime[:,1], 'n': y_test_prime[:,2]}).head())\n\nfscores = pd.DataFrame({'feature': feature_columns, 'feat. importance': clf.feature_importances_})\nfscores.sort_values(by='feat. importance').plot.bar(x='feature')\nplt.title('XGBoost feature importance')\nplt.show()\n\nplt.title('Reliability of predictions')\nplt.plot([0, 1], [0, 1], linestyle='--') # diagonal for reference\nfop, mpv = calibration_curve(y_test_prime[:,0], y_probs[:,0], n_bins=10) # As\nplt.plot(mpv, fop, marker='.', label='A')\nfop, mpv = calibration_curve(y_test_prime[:,1], y_probs[:,1], n_bins=10) # Bs\nplt.plot(mpv, fop, marker='.', label='B')\nfop, mpv = calibration_curve(y_test_prime[:,2], y_probs[:,2], n_bins=10) # Ns\nplt.plot(mpv, fop, marker='.', label='Neither')\nplt.legend(loc='upper left', prop={'size': 12})\nplt.show()","67ac5b99":"# Bayesian Optimization\n\nfrom bayes_opt import BayesianOptimization\n\ndef xgb_evaluate(max_depth, n_estimators, gamma, min_child_weight, max_delta_step, subsample, colsample_bytree):\n    evalmodel = XGBClassifier(random_state=random_state, max_depth=int(round(max_depth)), n_estimators=int(n_estimators), gamma=gamma, min_child_weight=min_child_weight,\n                             max_delta_step=max_delta_step, subsample=subsample, colsample_bytree=colsample_bytree)\n    evalmodel.fit(X_train.values, y_train)\n    result = evalmodel.predict_proba(X_valid.values)\n    return -1.0 * log_loss(y_valid, result) # because result is maximized, use negative\n\noptimizer = BayesianOptimization(\n    f=xgb_evaluate,\n    pbounds={\n             'max_depth':            (3, 7),\n             'n_estimators':         (85, 100),\n             'gamma':                (0, 1),\n             'min_child_weight':     (0.2, 6),\n             'max_delta_step':       (0, 2),\n             'subsample':            (0.6, 1),\n             'colsample_bytree':     (0.3, 1)\n            },\n    verbose=2,\n    random_state=random_state\n)\n\noptimizer.probe( # make optimizer check out previous default first\n    params={\n            'max_depth':            clf.get_params()['max_depth'],\n            'n_estimators':         clf.get_params()['n_estimators'],\n            'gamma':                clf.get_params()['gamma'],\n            'min_child_weight':     clf.get_params()['min_child_weight'],\n            'max_delta_step':       clf.get_params()['max_delta_step'],\n            'subsample':            clf.get_params()['subsample'],\n            'colsample_bytree':     clf.get_params()['colsample_bytree']\n           },\n    lazy=True\n)\n\noptimizer.maximize(init_points=5, n_iter=10, acq='ei')\nprint('Best log-loss: ', -1.0 * optimizer.max['target'])","223b5c39":"# Run again with optimized params\n\ncolsample_bytree, gamma, max_delta_step, max_depth, min_child_weight, n_estimators, subsample = [v for i, v in sorted(optimizer.max['params'].items())]\nmax_depth= int(round(max_depth))\n\nclf = XGBClassifier(random_state=random_state, max_depth=int(max_depth), n_estimators=int(n_estimators), gamma=gamma, min_child_weight=min_child_weight,\n                    max_delta_step=max_delta_step, subsample=subsample, colsample_bytree=colsample_bytree)\nclf.fit(X_train_valid.values, y_train_valid)\ny_pred = clf.predict(X_test.values)\ny_probs = clf.predict_proba(X_test.values)\nprint('Accuracy:', accuracy_score(y_test, y_pred),\n      '\\nRecall:', recall_score(y_test, y_pred, average=None),\n      '\\nPrecision:', precision_score(y_test, y_pred, average=None),\n      '\\nF1:', f1_score(y_test, y_pred, average=None),\n      '\\nLog-loss:', log_loss(y_test, y_probs))\n\ny_test_prime = np.array([(y_test == 0), (y_test == 1), (y_test == 2)]).transpose().astype(int)\n\nprint(pd.DataFrame({'a_pred': y_probs[:,0], 'b_pred': y_probs[:,1], 'n_pred': y_probs[:,2],\n                    'a': y_test_prime[:,0], 'b': y_test_prime[:,1], 'n': y_test_prime[:,2]}).head())\n\nfscores = pd.DataFrame({'feature': feature_columns, 'feat. importance': clf.feature_importances_})\nfscores.sort_values(by='feat. importance').plot.bar(x='feature')\nplt.title('XGBoost feature importance')\nplt.show()\n\nplt.title('Reliability of predictions')\nplt.plot([0, 1], [0, 1], linestyle='--')\nfop, mpv = calibration_curve(y_test_prime[:,0], y_probs[:,0], n_bins=10)\nplt.plot(mpv, fop, marker='.', label='A')\nfop, mpv = calibration_curve(y_test_prime[:,1], y_probs[:,1], n_bins=10)\nplt.plot(mpv, fop, marker='.', label='B')\nfop, mpv = calibration_curve(y_test_prime[:,2], y_probs[:,2], n_bins=10)\nplt.plot(mpv, fop, marker='.', label='Neither')\nplt.legend(loc='upper left', prop={'size': 14})\nplt.show()","bb362768":"submission = pd.DataFrame([annotated_data.iloc[:2000,].id, y_probs[:,0], y_probs[:,1], y_probs[:,2]], index=['ID', 'A', 'B', 'NEITHER']).transpose()\nsubmission.to_csv('submission.csv', index=False)","f6f9340d":"## Running the annotation script and vizualizing errors","59d4ee66":"# A non-neural baseline\n\nIn this kernel I match the baseline introduced [here](https:\/\/arxiv.org\/abs\/1810.05201). Unlike in my previous kernel, I use spaCy for parsing, whose data structures are very easy to work with.\n\nBesides the parsing step, the model is completely non-neural and doesn't rely on word embeddings. As expected, it does not come close to the performance of modern deep transfer learning approaches. However, with some tweaking I was able to reach 0.57 log loss.\n\n### Model architecture\n\n1. Build a global list of candidates using an entity recognizer and make sure both provided candidates are in there.\n2. Disqualify some candidates using well-understood grammatical constraints on coreference.\n3. Divide remaining candidates into 3 groups A, B, N based on what entity they are likely an instance of.\n4. For each group, compute some features based on testing its instances on metrics like prominence, locality, etc.\n5. Feed those features into a standard ensemble classifier.\n\n### Two take-aways:\n\n1. A *major* problem is the cascading effect of misparses. Obviously this is an inherent danger of using intermediate representations like dependency parses, and not directly training on the task at hand.\n2. A zillion manual tweaks are possible, and some have elegant spaCy implementations, but the return given the effort is rather slim.\n3. The thing the model did worst on is detecting cases where neither suggested candidate was correct. I wonder if that is specific to the approach here or if everyone is finding that to some extent.\n\n","c8afe814":"This is a misparse as \\[Yang Xiong helps him\\] is not parsed as one unit. If it had been, the disqualification algorithm would have spared 'Shi Quan.'","6241e1f4":"## The ML part"}}