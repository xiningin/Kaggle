{"cell_type":{"b3db605a":"code","fe4baad0":"code","dd68d7f4":"code","d0c891e8":"code","d748913c":"code","0ec907a7":"code","da05240e":"code","595f829e":"code","34923d31":"code","cf8c0657":"code","d911d9a5":"code","86956713":"code","2d98df1a":"code","83d0abce":"code","3cbbac16":"code","b1eb0dcd":"code","8fe91690":"code","10bb7c9b":"code","87ee4f6c":"code","33b91e93":"code","7c24e7bb":"code","f0abb94b":"code","30b0d3c0":"code","41a5690e":"code","fdcbb32e":"code","72d1490a":"code","85011034":"code","e0f5bc06":"code","3374efed":"code","f00109fc":"code","207a664f":"code","fa6ce1a7":"code","f407079d":"code","f7533c12":"code","75cbc5df":"code","118f42a2":"code","d49ee499":"code","2348b1d7":"code","a67373c8":"code","25fb696d":"code","35ce3a8c":"code","7efe0bba":"code","5ca26518":"code","9ba79cff":"code","1344d14e":"code","38b53911":"code","b5a119dc":"code","c97e7342":"code","498a6406":"code","8a838cc7":"code","e7503b42":"code","1041d9de":"code","27c923e3":"code","5add6d97":"code","c27bfa32":"code","e585b887":"code","6bd8d85e":"code","295e64f2":"code","4cc8bbf4":"code","9e1b0f32":"code","ca8deca4":"code","2b790d48":"code","37d53f6f":"code","609f8547":"code","eec54e48":"code","bc696e6e":"code","3f6bc46e":"markdown","27eaab49":"markdown","22efc6b1":"markdown","fa7ccb02":"markdown","178a804f":"markdown","d2b917a6":"markdown","a34d8ea3":"markdown","644d58e8":"markdown","182a6467":"markdown","62fdd76e":"markdown","90aaa37f":"markdown","194a7823":"markdown","55e59ed6":"markdown","213d672e":"markdown","abeb4d17":"markdown","b0d668bf":"markdown","444b517f":"markdown","f165de42":"markdown","980990d2":"markdown","31c3a42a":"markdown","dfa8e69e":"markdown","cc5468da":"markdown","fa8a6651":"markdown","ae298921":"markdown","448f0f26":"markdown","c088b1ea":"markdown","1e5cd302":"markdown","7a568e19":"markdown","4d28a9ed":"markdown","a1444e67":"markdown","54929753":"markdown","407ad833":"markdown","c589f104":"markdown","bc44dd39":"markdown","8bc4b5d2":"markdown","81073c7e":"markdown","c9d86f1b":"markdown","3943eda4":"markdown","b09df456":"markdown","a612f7d8":"markdown","20112ed6":"markdown","2cda5b6c":"markdown","0c868a8d":"markdown","03cc7432":"markdown","52d28483":"markdown","73215a42":"markdown","c606fe32":"markdown","948563fd":"markdown","fb2b2f25":"markdown","f454a02a":"markdown"},"source":{"b3db605a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport warnings \nwarnings.filterwarnings(\"ignore\")\nplt.style.use(\"seaborn-whitegrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe4baad0":"\ndata_champ = pd.read_json(\"..\/input\/league-of-legends\/champion_info.json\")","dd68d7f4":"data_champ2 = pd.read_json(\"..\/input\/league-of-legends\/champion_info_2.json\")","d0c891e8":"champInfo = pd.read_json((data_champ2['data']).to_json(), orient='index') #Datayi bir sozluk olarak alip her bir anahtara karsilik bir column ekliyor","d748913c":"champInfo2 = pd.read_json((data_champ[\"data\"]).to_json(),orient = \"index\")","0ec907a7":"data_spell = pd.read_json(\"\/kaggle\/input\/league-of-legends\/summoner_spell_info.json\")","da05240e":"data_spell_info = pd.read_json((data_spell['data']).to_json(), orient='index')","595f829e":"data_game = pd.read_csv(\"\/kaggle\/input\/league-of-legends\/games.csv\")","34923d31":"data_game2 = pd.read_csv(\"\/kaggle\/input\/league-of-legends\/games.csv\")","cf8c0657":"winner = data_game2[[\"winner\"]]","d911d9a5":"winner = winner.T\nwinner.shape","86956713":"name_dict = pd.Series(champInfo.key.values,index=champInfo.id).to_dict()","2d98df1a":"for keys,values in name_dict.items():\n    print(keys,values)","83d0abce":"champInfo","3cbbac16":"champInfo2","b1eb0dcd":"data_game","8fe91690":"data_game.drop([\"gameId\"],axis = 1,inplace = True)","10bb7c9b":"data_game.drop([\"creationTime\"],axis = 1,inplace = True)","87ee4f6c":"champs = data_game[[\"t1_champ1id\",\"t1_champ2id\",\n                      \"t1_champ3id\",\"t1_champ4id\",\"t1_champ5id\",\"t2_champ1id\",\n                      \"t2_champ2id\",\"t2_champ3id\",\"t2_champ4id\",\"t2_champ5id\",\"t1_ban1\",\"t1_ban2\",\"t1_ban3\",\n                      \"t1_ban4\",\"t1_ban5\",\"t2_ban1\",\"t2_ban2\",\"t2_ban3\",\"t2_ban4\",\"t2_ban5\"]]","33b91e93":"data_game.drop([\"t1_champ1id\",\"t1_champ2id\",\n                      \"t1_champ3id\",\"t1_champ4id\",\"t1_champ5id\",\"t2_champ1id\",\n                      \"t2_champ2id\",\"t2_champ3id\",\"t2_champ4id\",\"t2_champ5id\",\"t1_ban1\",\"t1_ban2\",\"t1_ban3\",\n                      \"t1_ban4\",\"t1_ban5\",\"t2_ban1\",\"t2_ban2\",\"t2_ban3\",\"t2_ban4\",\"t2_ban5\",\"gameDuration\",\"seasonId\"],axis = 1,inplace = True)","7c24e7bb":"data_game[\"t1_total_kills\"] = data_game[\"t1_champ1_sum1\"] + data_game[\"t1_champ1_sum2\"] + data_game[\"t1_champ2_sum1\"] + data_game[\"t1_champ2_sum2\"] + data_game[\"t1_champ3_sum1\"] + data_game[\"t1_champ3_sum2\"] + data_game[\"t1_champ4_sum1\"] + data_game[\"t1_champ4_sum2\"] + data_game[\"t1_champ5_sum1\"] + data_game[\"t1_champ5_sum2\"] \ndata_game[\"t2_total_kills\"] = data_game[\"t2_champ1_sum1\"] + data_game[\"t2_champ1_sum2\"] + data_game[\"t2_champ2_sum1\"] + data_game[\"t2_champ2_sum2\"] + data_game[\"t2_champ3_sum1\"] + data_game[\"t2_champ3_sum2\"] + data_game[\"t2_champ4_sum1\"] + data_game[\"t2_champ4_sum2\"] + data_game[\"t2_champ5_sum1\"] + data_game[\"t2_champ5_sum2\"] \n                                                                                                                                   \n                                                                                                                                  \n                                                                                                                                  ","f0abb94b":"data_game[[\"t1_total_kills\",\"t2_total_kills\",\"winner\"]]","30b0d3c0":"# firstInhibitor vs winner\ndata_game[[\"firstInhibitor\",\"winner\"]].groupby([\"firstInhibitor\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","41a5690e":"# firstBaron vs winner\ndata_game[[\"firstBaron\",\"winner\"]].groupby([\"firstBaron\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","fdcbb32e":"# firstTower vs winner\ndata_game[[\"firstTower\",\"winner\"]].groupby([\"firstTower\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","72d1490a":"# firstBlood vs winner\ndata_game[[\"firstBlood\",\"winner\"]].groupby([\"firstBlood\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","85011034":"# firstDragon vs winner\ndata_game[[\"firstDragon\",\"winner\"]].groupby([\"firstDragon\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","e0f5bc06":"# firstDragon vs winner\ndata_game[[\"firstRiftHerald\",\"winner\"]].groupby([\"firstRiftHerald\"],as_index = False).mean().sort_values(by = \"winner\",ascending = False)","3374efed":"data_game[data_game[\"winner\"] == 2] #Look at the number of rows to see how many times number 2 won. Which is 25413.","f00109fc":"constraints = (data_game[\"firstInhibitor\"] == 2) & (data_game[\"firstBaron\"] == 2) & (data_game[\"firstTower\"] == 2) & (data_game[\"firstRiftHerald\"] == 2) & (data_game[\"firstBlood\"] == 2) & (data_game[\"firstDragon\"] == 2)","207a664f":"data_game[constraints & (data_game[\"winner\"] == 2)]","fa6ce1a7":"1648\/1734*100\/100","f407079d":"data_game.columns[data_game.isnull().any()]","f7533c12":"data_game.isnull().sum()","75cbc5df":"g = sns.factorplot(x = \"firstInhibitor\",y = \"winner\", data = data_game,kind = \"bar\",size = 6)\n","118f42a2":"g = sns.factorplot(x = \"firstBaron\",y = \"winner\", data = data_game,kind = \"bar\",size = 6)","d49ee499":"g = sns.factorplot(x = \"firstRiftHerald\",y = \"winner\", data = data_game,kind = \"bar\",size = 6)","2348b1d7":"sns.heatmap(data_game[[\"firstInhibitor\",\"firstBaron\",\"firstRiftHerald\",\"winner\"]].corr(),annot = True)\nplt.show()","a67373c8":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n# Predict space\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# R^2 \nprint('R^2 score: ',reg.score(x, y))\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","25fb696d":"data_game.head()","35ce3a8c":"x_data = data_game.T.drop([\"winner\"],axis = 0)\ny = data_game[\"winner\"].values","7efe0bba":"# Normalization \nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","5ca26518":"x = x.T","9ba79cff":"from sklearn.model_selection import train_test_split \nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)\n","1344d14e":"#Initialize weights and bias","38b53911":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))","b5a119dc":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print(\"Prediction: {}\".format(prediction))\n","c97e7342":"print(prediction[:20])\nprint(y_test[:20])","498a6406":"print(\"Accuracy {}\".format(knn.score(x_test,y_test)))","8a838cc7":"from sklearn.svm import SVC\n\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train)\n\nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))","e7503b42":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\nprint(\"Accuracy: \",nb.score(x_test,y_test))","1041d9de":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\nprint(\"Accuracy: {}\".format(dt.score(x_test,y_test)))","27c923e3":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nprint(\"Accuracy: {}\".format(rf.score(x_test,y_test)))","5add6d97":"y_pred = rf.predict(x_test)\ny_true = y_test\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true,y_pred)","c27bfa32":"import seaborn as sns\nf,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm,annot = True,linewidth = 0.5,linecolor = \"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"?\")\nplt.ylabel(\"?\")\nplt.show()","e585b887":"x.head()","6bd8d85e":"plt.scatter(x['firstInhibitor'],x['firstBaron'])\nplt.xlabel('firstInhibitor')\nplt.ylabel('firstBaron')\nplt.show()","295e64f2":"plt.scatter(x['t1_towerKills'],x['t2_towerKills'])\nplt.xlabel('t1_towerKills')\nplt.ylabel('t2_towerKills')\nplt.show()","4cc8bbf4":"plt.scatter(x['t1_towerKills'],x['t2_towerKills'])\nplt.xlabel('t1_towerKills')\nplt.ylabel('t2_towerKills')\nplt.show()","9e1b0f32":"x.columns","ca8deca4":"from sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(x.drop(['t1_champ1_sum1', 't1_champ1_sum2',\n       't1_champ2_sum1', 't1_champ2_sum2', 't1_champ3_sum1', 't1_champ3_sum2',\n       't1_champ4_sum1', 't1_champ4_sum2', 't1_champ5_sum1', 't1_champ5_sum2'],axis = 1))\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"?\")\nplt.ylabel(\"?\")\nplt.show()   ","2b790d48":"kmeans2 = KMeans(n_clusters = 2)\nclusters = kmeans2.fit_predict(x)\nx[\"label\"] = clusters ","37d53f6f":"x.label.head(20)","609f8547":"from scipy.cluster.hierarchy import linkage,dendrogram\nmerg = linkage(x_test.drop(['t1_champ1_sum1', 't1_champ1_sum2',\n       't1_champ2_sum1', 't1_champ2_sum2', 't1_champ3_sum1', 't1_champ3_sum2',\n       't1_champ4_sum1', 't1_champ4_sum2', 't1_champ5_sum1', 't1_champ5_sum2',\"t1_total_kills\",\"t2_total_kills\"],axis = 1),method = \"ward\")\ndendrogram(merg,leaf_rotation = 0)\nplt.xlabel(\"data points\")\nplt.ylabel(\"Distance\")","eec54e48":"from sklearn.cluster import AgglomerativeClustering\nhierarchical_cluster = AgglomerativeClustering(n_clusters = 2,affinity = \"euclidean\",linkage = \"ward\")\ncluster = hierarchical_cluster.fit_predict(x_test.drop(['t1_champ1_sum1', 't1_champ1_sum2',\n       't1_champ2_sum1', 't1_champ2_sum2', 't1_champ3_sum1', 't1_champ3_sum2',\n       't1_champ4_sum1', 't1_champ4_sum2', 't1_champ5_sum1', 't1_champ5_sum2',\"t1_total_kills\",\"t2_total_kills\"],axis = 1))\nx_test[\"label\"] = cluster","bc696e6e":"x_test.label.head(20)","3f6bc46e":"### A better way to see the accuracy is to check the score!","27eaab49":"### First tower has also big impact but less than the FirstBaron and FirstInhibitor.","22efc6b1":"### Those located above are to understand the logic behind the scenes!","fa7ccb02":"## The results are basically same as the firstInhibitor. We could argue that, if a team get the first baron, they are more likely to win the game!","178a804f":"<a id = \"8\"><\/a>\n## LETS TRY TO UNDERSTAND THE LOGIC:","d2b917a6":"<a id = \"6\"><\/a>\n# MACHINE LEARNING","a34d8ea3":"<a id = \"15\"><\/a>\n# UNSUPERVISED LEARNING","644d58e8":"#Forward and backward propagation\ndef forward_backward_pro(w,b,game_train,y_train):\n    #Forward propagation\n    z = np.dot(w.T,game_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/game_train.shape[1] #game_train.shape[1] (which is 36 in this case) for scaling \n    \n    #Backward propagation\n    derivative_weight = (np.dot(game_train,((y_head-y_train).T)))\/game_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/game_train.shape[1]\n    gradients = {\"derivative_weight\":derivative_weight,\"derivative_bias\":derivative_bias}\n    \n    return cost,gradients\n    ","182a6467":"## We can observe the direct influence of firstInhibitor on determining the winner.","62fdd76e":"<a id = \"16\"><\/a>\n## Kmeans Clustering","90aaa37f":"<a id = \"3\"><\/a> \n# Finding Corrolations\n## Basic Data Analysis\n### Corrolations: \n* firstInhibitor to winner\n* firstBaron to winner\n* firstTower to winner\n","194a7823":"<a id = \"5\"><\/a>\n# VISUALIZATION","55e59ed6":"<a id = \"9\"><\/a>\n## Logistic Regression","213d672e":"<a id = \"2\"><\/a> \n# Looking at the data","abeb4d17":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n#Lets find the sigmoid(z)\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))\n    return y_head\n","b0d668bf":"<a id = \"13\"><\/a>\n## DECISION TREE","444b517f":"### Making data_game ready for our machine learning process.","f165de42":"So, we conclude that, if all of these constraints are satisfied, the probability of winning is 95 percent!","980990d2":"<a id = \"1\"><\/a> \n# GETTING THE DATA","31c3a42a":"> Lets create our X and Y","dfa8e69e":"<a id = \"7\"><\/a>\n## Linear Regression: \n(To be continued)","cc5468da":"### With this method we got 96.1% Accuracy rate.","fa8a6651":"<a id = \"7\"><\/a>\n# SUPERVISED LEARNING","ae298921":"## Hierarchical Clustering (HC)","448f0f26":"## Lets look at lower corrolations:\n* firstBlood to winner\n* firstDragon to winner\n* firstRiftHerald to winner","c088b1ea":"<a id = \"4\"><\/a> \n# CHECKING FOR MISSING VALUES","1e5cd302":"### Not surprisingly these three have positive corrolations with determining the winner. ","7a568e19":"## Introduction:\n<font color = \"green\">\n    Content:\n    \n1. [Getting the data](#1)\n1. [Checking the data](#2)\n1. [Finding Corrolations](#3)\n1. [Checking for Missing Values](#4)    \n1. [Visualization](#5)   \n1. [Machine Learning](#6)\n    * [Supervised Learning](#7)   \n        * [Linear Regression](#7)\n        * [The logic Behind the Logistic Regression](#8)\n        * [Logistic Regression](#9)\n        * [K-Nearest Neighbors (KNN)](#10)\n        * [Support Vector Machine (SVM)](#11)\n        * [Naive Bayes](#12)\n        * [Decision Tree](#13)\n        * [Random Forest](#14)\n    * [Unsupervised Learning](#15)\n        * [Kmeans Clustering](#16)","4d28a9ed":"<a id = \"11\"><\/a>\n## SUPPORT VECTOR MACHINE (SVM)","a1444e67":"### * Lets create a confusion matrix.","54929753":"### We have found that there is no null information in our data.","407ad833":"### With this method we got 93.6% Accuracy rate.","c589f104":"#Lets update (learn) the parameters\ndef update(w,b,game_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating parameters number of iteration times\n    for i in range(number_of_iteration):\n        # do the back and forward propagation to find the cost and gradients\n        cost,gradients = forward_backward_pro(w,b,game_train,y_train)\n        cost_list.append(cost)\n        #lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(i)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\"%(i,cost))\n            \n    # we update parameters weights and bias\n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation =\"vertical\")\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list","bc44dd39":"## Lets look at them as a heatmap!","8bc4b5d2":"### With this method we got 97% Accuracy rate!","81073c7e":"### By using Logistic Regression method, we have predicted the winner and the accuracy rate for that is 96% according to LR score.","c9d86f1b":"#Lets predict\ndef predict(w,b,game_test):\n    # game_test is the input for the forward prop\n    z = sigmoid(np.dot(w,game_test) + b ) #Why w.T ?\n    Y_prediction = np.zeros((1,x_test.shape[1])) # which is 36 np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1), ??\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0), ?? \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n            \n    return Y_prediction                    ","3943eda4":"### Since the number of items in the list are too many, the code above is not working. Thats why we can look at the first 20 estimations that our model made and compare them with the actual ones. ","b09df456":"### Lets visualize it","a612f7d8":"### First inhibitor has a huge effect on defining the winner!","20112ed6":"## Even though it the rate is lower than the previous ones, we could still observe a direct relationship between the first riftherald and the chance of being winner!","2cda5b6c":"### PREPARE THE DATA","0c868a8d":"### With this method we got 95.7% Accuracy rate.","03cc7432":"### First Baron also has a huge impact on determining the winner!","52d28483":"<a id = \"14\"><\/a>\n## RANDOM FOREST","73215a42":"<a id = \"12\"><\/a>\n## NAIVE BAYES","c606fe32":"<a id = \"10\"><\/a>\n## K-NEAREST NEIGHBORS (KNN)","948563fd":"## ***So, by looking at this data, lets try to see the posibility for total when all the constraints above are satisfied*.**","fb2b2f25":"### Here we observe a 91.5% of accuracy for our KNN model.","f454a02a":"### Lets do the logistic regression\ndef logistic_regression(game_train,y_train,data_test,y_test,learning_rate,num_iterations):\n    #Initialize\n    dimension = game_train.shape[0] #Which is 36\n    w,b = initialize_weights_and_bias(dimension)\n    #Do not change the learning rate\n    parameters,gradients,cost_list = update(w,b,game_train,y_train,learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],game_test)\n    \n    # Print test errors\n    print(\"test accuracy: {}%\".format(100 - np.mean(np.abs(y_prediction_test - y_test))* 100))\n    \n  "}}