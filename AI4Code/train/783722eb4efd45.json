{"cell_type":{"919aa4b9":"code","d9087300":"code","448deb70":"code","7eb65d1b":"code","eaad55e6":"code","32c334a1":"code","1417fa4a":"code","b1c63eb8":"code","93c5292d":"code","08f86c4f":"code","4ca14300":"code","77fc6583":"code","d3199b38":"code","984124ce":"code","357ad3a0":"code","550cdd11":"code","45cae8f3":"code","2b73064c":"code","8d9fd0ff":"code","e786e653":"code","8f9f5787":"code","172ca068":"code","d5b1bdcd":"code","8af0c6a7":"code","3d2778c0":"code","fb29b347":"code","724e026d":"code","fd8b7703":"code","b8f0aee8":"markdown","2acb19ce":"markdown","34f8f05d":"markdown","3ac88d32":"markdown","b256c959":"markdown","4f5d5c5d":"markdown","4e5ef8df":"markdown","1699a5f1":"markdown","3fb88639":"markdown","167e885a":"markdown","9881a86a":"markdown","d5a08bc8":"markdown","45971581":"markdown","f6c818f7":"markdown","6677adf8":"markdown","0ce785ed":"markdown","adfe5c50":"markdown","f0325dc9":"markdown"},"source":{"919aa4b9":"import pandas as pd # To handle the data set.\nimport seaborn as sb # To display visualizations.\nimport matplotlib.pyplot as plt # To plot\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split # To split data\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn.metrics import confusion_matrix # To calculate the confusion matrix\nfrom sklearn.metrics import accuracy_score # To calculate the score\nfrom sklearn.feature_selection import SelectKBest # Univariate Feature Selection\nfrom sklearn.feature_selection import chi2 # To apply Univariate Feature Selection\nfrom sklearn.feature_selection import RFE # Recursive Feature Selection\nfrom sklearn.feature_selection import RFECV # Recursive Feature Selection with Cross Validation\nfrom sklearn.decomposition import PCA # To apply PCA\nfrom sklearn import preprocessing # To get MinMax Scaler function\n\n# To plot inline\n%matplotlib inline","d9087300":"# Loading file and dropping some columns (the justification is shown in my latest kernel \n# https:\/\/www.kaggle.com\/ferneutron\/classification-and-data-visualization\n\naustralia = pd.read_csv('..\/input\/weatherAUS.csv') \naustralia = australia.drop(['Location','Date','Evaporation','Sunshine', 'Cloud9am','Cloud3pm',\n                           'WindGustDir','WindGustSpeed','WindDir9am','WindDir3pm','WindSpeed9am',\n                           'WindSpeed3pm'], axis=1)","448deb70":"# Splitting between X and Y vector wich means the corpus and target vector respectively\nY = australia.RainTomorrow\nX = australia.drop(['RainTomorrow'], axis=1)","7eb65d1b":"# Switching 'Yes' and 'No' with a boolen value and handling NaN values, in this case replacing it with a zero\nX = X.replace({'No':0, 'Yes':1})\nX = X.fillna(0)\nY = Y.replace({'No':0, 'Yes':1})\nY = Y.fillna(0)","eaad55e6":"# Initializing the MinMaxScaler function\nmin_max_scaler = preprocessing.MinMaxScaler()","32c334a1":"# Scaling dataset keeping the columns name\nX_scaled = pd.DataFrame(min_max_scaler.fit_transform(X), columns = X.columns)\nX_scaled.head()","1417fa4a":"# Splitting  up data, seting 75% for train and 25% for test.\nx_train, x_test, y_train, y_test = train_test_split(X_scaled, Y, test_size=0.25, random_state=43)","b1c63eb8":"# Initialize SelectKBest function\nUnivariateFeatureSelection = SelectKBest(chi2, k=5).fit(x_train, y_train)","93c5292d":"# Creating a dict to visualize which features were selected with the highest score\ndiccionario = {key:value for (key, value) in zip(UnivariateFeatureSelection.scores_, x_train.columns)}\nsorted(diccionario.items())","08f86c4f":"# Using the 'UnivariateFeatureSelection' based on 'SelectKBest' function,\n# let's extract the best features from the original dataset\n\nx_train_k_best = UnivariateFeatureSelection.transform(x_train)\nx_test_k_best = UnivariateFeatureSelection.transform(x_test)","4ca14300":"print(\"Shape of original data: \", x_train.shape)\nprint(\"Shape of corpus with best features: \", x_train_k_best.shape)","77fc6583":"# Initializing and fitting data to the random forest classifier\nRandForest_K_best = RandomForestClassifier()      \nRandForest_K_best = RandForest_K_best.fit(x_train_k_best, y_train)","d3199b38":"# Making a prediction and calculting the accuracy\ny_pred = RandForest_K_best.predict(x_test_k_best)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ',accuracy)","984124ce":"# Showing performance with a confusion matrix\nconfMatrix = confusion_matrix(y_test, y_pred)\nsb.heatmap(confMatrix, annot=True, fmt=\"d\")","357ad3a0":"# Initializing Random Forest Classifier\nRandForest_RFE = RandomForestClassifier() \n# Initializing the RFE object, one of the most important arguments is the estimator, in this case is RandomForest\nrfe = RFE(estimator=RandForest_RFE, n_features_to_select=5, step=1)\n# Fit\nrfe = rfe.fit(x_train, y_train)","550cdd11":"print(\"Best features chosen by RFE: \\n\")\nfor i in x_train.columns[rfe.support_]:\n    print(i)","45cae8f3":"# Generating x_train and x_test based on the best features given by RFE\nx_train_RFE = rfe.transform(x_train)\nx_test_RFE = rfe.transform(x_test)","2b73064c":"# Fitting the Random Forest\nRandForest_RFE = RandForest_RFE.fit(x_train_RFE, y_train)","8d9fd0ff":"# Making a prediction and calculting the accuracy\ny_pred = RandForest_RFE.predict(x_test_RFE)\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: ',accuracy)","e786e653":"# Showing performance with a confusion matrix\nconfMatrix = confusion_matrix(y_test, y_pred)\nsb.heatmap(confMatrix, annot=True, fmt=\"d\")","8f9f5787":"# Initialize the Random Forest Classifier\nRandForest_RFECV = RandomForestClassifier() \n# Initialize the RFECV function setting 3-fold cross validation\nrfecv = RFECV(estimator=RandForest_RFECV, step=1, cv=3, scoring='accuracy')\n# Fit data\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Best number of features :', rfecv.n_features_)\nprint('Features :\\n')\nfor i in x_train.columns[rfecv.support_]:\n    print(i)","172ca068":"# Plotting the best features with respect to the Cross Validation Score\nplt.figure()\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Score of Selected Features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","d5b1bdcd":"# Initialize the Random Forest Classifier\nRandForest_Tree = RandomForestClassifier()  \n# Fit the random forest with the original data\nRandForest_Tree = RandForest_Tree.fit(x_train, y_train)\n# Getting the relevance between features\nrelevants = RandForest_Tree.feature_importances_","8af0c6a7":"# Apply the tree based on importance for the random forest classifier and indexing it\nstd = np.std([tree.feature_importances_ for tree in RandForest_Tree.estimators_], axis=0)\nindices = np.argsort(relevants)[::-1]","3d2778c0":"# Printting the ranking of importance\nprint(\"Feature Rank:\")\n\nfor i in range(x_train.shape[1]):\n    print(\"%d. Feature %d (%f)\" \n          % (i + 1, indices[i], relevants[indices[i]]))","fb29b347":"# Plotting the feature importances\nplt.figure(1, figsize=(9, 8))\nplt.title(\"Feature Importances\")\nplt.bar(range(x_train.shape[1]), relevants[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(x_train.shape[1]), x_train.columns[indices],rotation=90)\nplt.xlim([-1, x_train.shape[1]])\nplt.show()","724e026d":"# Initializing PCA and fitting\npca = PCA()\npca.fit(x_train)","fd8b7703":"# Plotting to visualize the best number of elements\nplt.figure(1, figsize=(9, 8))\nplt.clf()\nplt.axes([.2, .2, .7, .7])\nplt.plot(pca.explained_variance_ratio_, linewidth=2)\nplt.axis('tight')\nplt.xlabel('Number of Feautres')\nplt.ylabel('Variance Ratio')","b8f0aee8":"## 2.1 Extracting the best <i>K<\/i> values","2acb19ce":"## 3.1 Testing with Random Forest Algorithm","34f8f05d":"In this case we can visualize that the best number of features is 2.","3ac88d32":"# 1. Let's Start","b256c959":"As we can see, the last five elements have the highest score. So the best features are:\n\n<ul>\n    <li>1. RainToday<\/li>\n    <li>2. RISK_MM<\/li>\n    <li>3. Humidity3pm<\/li>\n    <li>4. Rainfall<\/li>\n    <li>5. Humidity9am<\/li>\n<\/ul>\n\nNow that we have the best features, let's extract them from the original data set and let's measure the performance \nwith the random forest algorithm.","4f5d5c5d":"## 1.3 Splitting up Data\n\nWe have scaling the values in the corpus <i>X<\/i>, now we need to separate it in train and test set.","4e5ef8df":"<a id='rfecv'><\/a>\n# 4. Recursive Feature Elimination with Cross-Validation\n\nThis method is an extention of Recursive Feature Elimination showed above. In this method we have to \nset the number of k-fold cross validation, basically takes the subset of the traing set and measure the\nperformance recurively with respect to the number of features.","1699a5f1":"## 1.2 Scaling Data\n\nWorking with values in a wide range is not convenient, we need to scale it. \nIn this case we are going to normalize it and scaling it in a 0-1 range.","3fb88639":"First we need to load all libraries we will use in this work.","167e885a":"# Feature Extraction with Different Methods","9881a86a":"As we can see, making use of dimensionality reduction we find that the best number of features are in a range of 2 - 4 \nfeatures.","d5a08bc8":"The idea of this work is to show the use of different techniques for extraction of characteristics in a given database. The methods used are shown below:\n\n<ul>\n    <li><a href='#univariate_'>Univariate Feature Selection<\/a><\/li>\n    <li><a href='#rfe_'>Recursive Feature Elimination<\/a><\/li>\n    <li><a href='#rfecv_'>Recursive Feature Elimination with Cross-Validation<\/a><\/li>\n    <li><a href='#tree_'>Tree based feature selection<\/a><\/li>\n    <li><a href='#pca_'>Feature Extraction through PCA<\/a><\/li>\n<\/ul>\n\nApplying techniques shown above, we will test the effectiveness through a Random Forest classifier.","45971581":"## 2.2 Testing with Random Forest Algorithm","f6c818f7":"## 1.1 Loading and Preparing DataSet\n\nWe need to load the dataset. In this case we are going to use the dataset provided for Kaggle Contest in: https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package.","6677adf8":"<a id='univariate_'><\/a>\n# 2. Univariate Feature Selection\n\nThis method works by selection the <i>K<\/i> beast features acording to a score. The <i>K<\/i> number of features\nis setting explicity.","0ce785ed":"<a id='tree'><\/a>\n# 5. Tree based Feature Selection\n\nThis method is to compute the relevance of each feature in the dataset.","adfe5c50":"<a id='pca'><\/a>\n# 6. Feature Extraction through PCA\n\nIn some cases it is convenient to apply dimensionality reduction to visualize the number of components\nor elements which could be the best for our model. In this case we apply PCA to discover which ones are\nthe features to obtain a acceptable performance in the model.","f0325dc9":"<a id='rfe'><\/a>\n# 3. Recursive Feature Elimination\n\nThe idea of this method is to make use of an estimator (in this case we are using random forest), and test with\ndifferent sizes of features until find the best set of features."}}