{"cell_type":{"599a5af7":"code","6a0d3993":"code","680b4fb8":"code","81c07508":"code","bcacb1cb":"code","08bd65f6":"code","5f2a9aab":"code","07592b41":"code","546ee537":"code","30852bef":"code","862ddee9":"code","3cc67492":"code","ed98858c":"code","1a4ca485":"code","8d069730":"code","6d8ac66f":"code","2119b3fc":"code","b8ddb2da":"code","f5892a29":"code","5a47fcca":"code","09aebc40":"code","b7821ebe":"code","b26ce50b":"code","14369d97":"markdown","688c17c1":"markdown","cfc0c775":"markdown","ace5317b":"markdown","ec28e425":"markdown","57b813ab":"markdown","b777317f":"markdown","635b1845":"markdown","3545d526":"markdown","bfddd2a6":"markdown","be0a8fe6":"markdown","57f0bc89":"markdown","a860c10c":"markdown","12e76449":"markdown","488beeee":"markdown","167e2322":"markdown"},"source":{"599a5af7":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n","6a0d3993":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nmy_pipeline= Pipeline([('std_scaler',StandardScaler())])","680b4fb8":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","81c07508":"dataset.head()","bcacb1cb":"dataset = dataset.drop('Unnamed: 32', axis =1)","08bd65f6":"dataset.describe()","5f2a9aab":"dataset.isnull().values.any()","07592b41":"dataset.isnull().sum()","546ee537":"dataset.shape","30852bef":"dataset['diagnosis'].value_counts()","862ddee9":"dataset.hist(bins=50,figsize=(20,15),color='violet',lw=0)","3cc67492":"X = dataset.iloc[:,2:].values\nY = dataset.iloc[:, 1:2].values\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(Y.ravel())\n\nfrom sklearn.model_selection import train_test_split\nX_train_set,X_test_set,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,random_state=42)\nprint(f\"Rows in X train set: {len(X_train_set)}\\nRows in X test set: {len(X_test_set)}\")","ed98858c":"X_train=my_pipeline.fit_transform(X_train_set)\nX_test= my_pipeline.transform(X_test_set)","1a4ca485":"plt.figure(figsize=(20, 12))\nmatrix = np.triu(dataset.corr())\nsns.heatmap(dataset.corr(), annot=True, linewidth=1, mask=matrix, cmap=\"magma\");","8d069730":"fig, ax = plt.subplots(2, 4, figsize=(18, 12))\nsns.scatterplot(x='perimeter_mean', y='radius_mean', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][0], palette='magma')\nsns.scatterplot(x='area_mean', y='radius_mean', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][1], palette='magma')\nsns.scatterplot(x='area_mean', y='perimeter_mean', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][2], palette='magma')\nsns.scatterplot(x='perimeter_worst', y='radius_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[0][3], palette='magma')\nsns.scatterplot(x='fractal_dimension_mean', y='area_mean', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][0], palette='magma')\nsns.scatterplot(x='fractal_dimension_worst', y='area_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][1], palette='magma')\nsns.scatterplot(x='smoothness_se', y='radius_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][2], palette='magma')\nsns.scatterplot(x='symmetry_se', y='radius_worst', hue=\"diagnosis\",\n                data=dataset, ax=ax[1][3], palette='magma');\n","6d8ac66f":"from sklearn.linear_model import LogisticRegression\nmodel1=LogisticRegression(random_state=0)\nmodel1.fit(X_train,Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores1=cross_val_score(model1,X_train,Y_train,scoring=\"accuracy\",cv=10)\n#rmse would be required in regression\n#rmse_scores=np.sqrt(-scores)\n#rmse_scores\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for Logistic Regression''')\nY_pred = model1.predict(X_test)\nresult = np.concatenate((Y_pred.reshape(len(Y_pred), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred = model1.predict(X_test)\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm,'\\n')\nprint('True Positives :',cm[0][0])\nprint('False Positives :',cm[0][1])\nprint('False Negatives :',cm[1][0])\nprint('True Negatives :', cm[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating Logistic Regression Model Performance''')\naccuracy = accuracy_score(Y_test, Y_pred)\nprint(accuracy,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(model1, X_train, Y_train, cv=10)\nprint(\"Accuracy for Logistic Regression: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation for Logistic Regression: {:.2f} %\".format(accuracies.std()*100),'\\n')","2119b3fc":"from sklearn.tree import DecisionTreeClassifier\nmodel2=DecisionTreeClassifier()\nmodel2.fit(X_train,Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores2=cross_val_score(model2,X_train,Y_train,scoring=\"accuracy\",cv=10)\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for Decision Tree Classifier''')\nY_pred2 = model2.predict(X_test)\nresult2 = np.concatenate((Y_pred2.reshape(len(Y_pred2), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result2,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred2 = model2.predict(X_test)\ncm2 = confusion_matrix(Y_test, Y_pred2)\nprint(cm2,'\\n')\nprint('True Positives :',cm2[0][0])\nprint('False Positives :',cm2[0][1])\nprint('False Negatives :',cm2[1][0])\nprint('True Negatives :', cm2[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred2,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating Decision Tree Classifier Model Performance''')\naccuracy2 = accuracy_score(Y_test, Y_pred2)\nprint(accuracy2,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies2 = cross_val_score(model2, X_train, Y_train, cv=10)\nprint(\"Accuracy for Decision Tree: {:.2f} %\".format(accuracies2.mean()*100))\nprint(\"Standard Deviation for Decision Tree: {:.2f} %\".format(accuracies2.std()*100),'\\n')","b8ddb2da":"from sklearn.ensemble import RandomForestClassifier\nmodel3=RandomForestClassifier()\nmodel3.fit(X_train,Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores3=cross_val_score(model3,X_train,Y_train,scoring=\"accuracy\",cv=10)\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for Random Forest Classifier''')\nY_pred3 = model3.predict(X_test)\nresult3 = np.concatenate((Y_pred3.reshape(len(Y_pred3), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result3,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred3 = model3.predict(X_test)\ncm3 = confusion_matrix(Y_test, Y_pred3)\nprint(cm3,'\\n')\nprint('True Positives :',cm3[0][0])\nprint('False Positives :',cm3[0][1])\nprint('False Negatives :',cm3[1][0])\nprint('True Negatives :', cm3[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred3,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating Random Forest Classifier Model Performance''')\naccuracy3 = accuracy_score(Y_test, Y_pred3)\nprint(accuracy3,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies3 = cross_val_score(model3, X_train, Y_train, cv=10)\nprint(\"Accuracy for Random Forest Classifier: {:.2f} %\".format(accuracies3.mean()*100))\nprint(\"Standard Deviation for Random Forrest Classifier: {:.2f} %\".format(accuracies3.std()*100),'\\n')","f5892a29":"from sklearn import svm\nmodel4 = svm.SVC(kernel='linear') # Linear Kernel\nmodel4.fit(X_train, Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores4=cross_val_score(model4,X_train,Y_train,scoring=\"accuracy\",cv=10)\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for SVM''')\nY_pred4 = model4.predict(X_test)\nresult4 = np.concatenate((Y_pred4.reshape(len(Y_pred4), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result4,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred4 = model4.predict(X_test)\ncm4 = confusion_matrix(Y_test, Y_pred4)\nprint(cm4,'\\n')\nprint('True Positives :',cm4[0][0])\nprint('False Positives :',cm4[0][1])\nprint('False Negatives :',cm4[1][0])\nprint('True Negatives :', cm4[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred4,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating SVM Performance''')\naccuracy4 = accuracy_score(Y_test, Y_pred4)\nprint(accuracy4,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies4 = cross_val_score(model4, X_train, Y_train, cv=10)\nprint(\"Accuracy for SVM: {:.2f} %\".format(accuracies4.mean()*100))\nprint(\"Standard Deviation for SVM: {:.2f} %\".format(accuracies4.std()*100),'\\n')","5a47fcca":"from sklearn.neighbors import KNeighborsClassifier\nmodel5 = KNeighborsClassifier(n_neighbors = 8)\nmodel5.fit(X_train, Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores5=cross_val_score(model5,X_train,Y_train,scoring=\"accuracy\",cv=10)\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for KNeighbors Classifier''')\nY_pred5 = model5.predict(X_test)\nresult5 = np.concatenate((Y_pred5.reshape(len(Y_pred5), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result5,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred5 = model5.predict(X_test)\ncm5 = confusion_matrix(Y_test, Y_pred5)\nprint(cm5,'\\n')\nprint('True Positives :',cm5[0][0])\nprint('False Positives :',cm5[0][1])\nprint('False Negatives :',cm5[1][0])\nprint('True Negatives :', cm5[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred5,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating K Neighbours Classifier Model Performance''')\naccuracy5 = accuracy_score(Y_test, Y_pred5)\nprint(accuracy5,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies5 = cross_val_score(model5, X_train, Y_train, cv=10)\nprint(\"Accuracy for K  Neighbours Classifier: {:.2f} %\".format(accuracies5.mean()*100))\nprint(\"Standard Deviation for K neighbours Classifiers: {:.2f} %\".format(accuracies5.std()*100),'\\n')","09aebc40":"from sklearn.naive_bayes import GaussianNB\nmodel6 = GaussianNB()\nmodel6.fit(X_train, Y_train)\n\nfrom sklearn.model_selection import cross_val_score\nscores6=cross_val_score(model6,X_train,Y_train,scoring=\"accuracy\",cv=10)\n\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nprint('''Prediciting Test Set Result for Naive Bayes''')\nY_pred6 = model6.predict(X_test)\nresult6 = np.concatenate((Y_pred6.reshape(len(Y_pred6), 1),Y_test.reshape(len(Y_test), 1)), 1)\nprint(result6,'\\n')\nprint('''Making Confusion Matrix''')\nY_pred = model6.predict(X_test)\ncm6 = confusion_matrix(Y_test, Y_pred6)\nprint(cm6,'\\n')\nprint('True Positives :',cm6[0][0])\nprint('False Positives :',cm6[0][1])\nprint('False Negatives :',cm6[1][0])\nprint('True Negatives :', cm6[0][1],'\\n')\n\nprint('''Classification Report''')\nprint(classification_report(Y_test, Y_pred6,target_names=['M', 'B'], zero_division=1))\n\nprint('''Evaluating Naive Bayes Model Performance''')\naccuracy6 = accuracy_score(Y_test, Y_pred6)\nprint(accuracy6,'\\n')\n\nprint('''Applying Cross validation''')\nfrom sklearn.model_selection import cross_val_score\naccuracies6 = cross_val_score(model6, X_train, Y_train, cv=10)\nprint(\"Accuracy for Naive Bayes: {:.2f} %\".format(accuracies6.mean()*100))\nprint(\"Standard Deviation for Naive Bayes: {:.2f} %\".format(accuracies6.std()*100),'\\n')","b7821ebe":"plt.figure(figsize=(10, 6))\nmodel_accuracies = [accuracies.mean()*100,accuracies2.mean()*100,accuracies3.mean()*100,accuracies4.mean()*100,accuracies5.mean()*100,accuracies6.mean()*100]\nmodel_names = ['LogisticRegression','Decisiontree','RandomForest','SVM', 'KNN','Naive Bayes']\nsns.barplot(x=model_accuracies,y=model_names,palette='magma');","b26ce50b":"length=len(model_names)\nfor i in range(length):\n    print(model_names[i],'Model Accuracy is:', model_accuracies[i],'%')","14369d97":"## Random Forest Classifier Model","688c17c1":" ## Logistic Regression Model","cfc0c775":"## Support Vector Machines Model","ace5317b":"**The Logistic Regression Model works best here for our dataset**","ec28e425":"## Naive Bayes Classifier Model","57b813ab":"## Exploratory Data Analysis","b777317f":"No null values have been detected in the dataset,so we can move ahead.","635b1845":"## EVALUATING EACH MODEL:","3545d526":"## Decision Tree Classifier Model","bfddd2a6":"## Plotting Features in the form of Histograms","be0a8fe6":"#**We can see that some features have very strong positive correlations and some have very strong negative correlations,hence,we plot these features.Here,we have perimeter_mean,radius_mean,area_mean,perimeter_worst,radius_worst,fractal_dimension_worst,fractal_dimension_meansmoothness_se,symmetry_se these features which give us these correlations.Now,we plot these features with respect to each other**","57f0bc89":" ## IMPORTING LIBRARIES","a860c10c":"# Finding out which model works best for our dataset","12e76449":"## CREATING PIPELINES","488beeee":"## K Neighbors Classifier Model","167e2322":"## Splitting the data into test & train set"}}