{"cell_type":{"bab3d27c":"code","6f411efa":"code","70a73e4d":"code","e85f6827":"code","8bcf6120":"code","39624958":"code","ddead7f6":"code","b89ae9c5":"code","15cbb691":"code","ae97b0e9":"code","5fae1237":"code","0723ae50":"code","ef05e5a8":"code","d4628bf6":"code","455b8e97":"code","42da4781":"code","ec7a4853":"code","02643b80":"code","a8f6d4a0":"code","45746f97":"code","32625912":"code","11c47376":"code","d114322f":"code","1e29db1f":"code","aa60e20b":"code","c8010f05":"code","36d78b56":"markdown","73fefa57":"markdown","5be53915":"markdown","968f7171":"markdown","91b6f9fc":"markdown","72368196":"markdown","9b7ce4b5":"markdown","8aeecf8f":"markdown","ef61a997":"markdown","39e713a3":"markdown","4e508c0e":"markdown","0269b463":"markdown","54d18947":"markdown","78c82db7":"markdown","441ad0dd":"markdown","f14bc92f":"markdown","7306561f":"markdown","63e131b3":"markdown"},"source":{"bab3d27c":"# For notebook plotting\n%matplotlib inline\n\n# Standard libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nimport xgboost as xgb\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\n# Seed for reproducability\nseed = 12345\nnp.random.seed(seed)\n\n# Directory\nKAGGLE_DIR = '..\/input\/'\n\nprint('\\n# Files and file sizes')\nfor file in os.listdir(KAGGLE_DIR):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + file) \/ 1000000, 2))))\n        \nprint('\\n# Files and file sizes in train: ')\nfor file in os.listdir(KAGGLE_DIR + 'train\/'):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + 'train\/' + file) \/ 1000000, 2))))\n        \nprint('\\n# Files and file sizes in test: ')\nfor file in os.listdir(KAGGLE_DIR + 'test\/'):\n    print('{}| {} MB'.format(file.ljust(30), \n                             str(round(os.path.getsize(KAGGLE_DIR + 'test\/' + file) \/ 1000000, 2))))","6f411efa":"# Read in data\ntrain_df = pd.read_csv(KAGGLE_DIR + \"train\/train.csv\")\ntest_df = pd.read_csv(KAGGLE_DIR + \"test\/test.csv\")\ntarget = train_df['AdoptionSpeed']","70a73e4d":"# Stats\nprint('Data Statistics:')\ntrain_df.describe()","e85f6827":"# Types\nprint('Info about types and missing values: ')\ntrain_df.info()","8bcf6120":"# Overview\nprint('This dataset has {} rows and {} columns\\n'.format(train_df.shape[0], train_df.shape[1]))\nprint('Example rows:')\ntrain_df.head(2)","39624958":"# Type distribution\ntrain_df['Type'].value_counts().rename({1:'Dog',\n                                        2:'Cat'}).plot(kind = 'barh',\n                                                       figsize = (15,6))\n\nplt.yticks(fontsize = 'xx-large')\nplt.title('Type Distribution', fontsize = 'xx-large')","ddead7f6":"# Gender distribution\ntrain_df['Gender'].value_counts().rename({1:'Male',\n                                          2:'Female',\n                                          3:'Mixed (Group of pets)'}).plot(kind = 'barh', \n                                                                           figsize = (15,6))\nplt.yticks(fontsize = 'xx-large')\nplt.title('Gender distribution', fontsize = 'xx-large')","b89ae9c5":"# Age distribution \ntrain_df['Age'][train_df['Age'] < 50].plot(kind = 'hist', \n                                           bins = 100, \n                                           figsize = (15,6), \n                                           title = 'Age distribution')\n\nplt.title('Age distribution', fontsize = 'xx-large')\nplt.xlabel('Age in months')","15cbb691":"# Photo amount distribution\ntrain_df['PhotoAmt'].plot(kind = 'hist', \n                          bins = 30, \n                          xticks = list(range(31)), \n                          figsize = (15,6))\n\nplt.title('PhotoAmt distribution', fontsize='xx-large')\nplt.xlabel('Photos')","ae97b0e9":"# Target variable (Adoption Speed)\nprint('The values are determined in the following way:\\n\\\n0 - Pet was adopted on the same day as it was listed.\\n\\\n1 - Pet was adopted between 1 and 7 days (1st week) after being listed.\\n\\\n2 - Pet was adopted between 8 and 30 days (1st month) after being listed.\\n\\\n3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.\\n\\\n4 - No adoption after 100 days of being listed.\\n\\\n(There are no pets in this dataset that waited between 90 and 100 days).')\n\n# Plot\ntrain_df['AdoptionSpeed'].value_counts().sort_index(ascending = False).plot(kind = 'barh', \n                                                                            figsize = (15,6))\nplt.title('Adoption Speed (Target Variable)', fontsize = 'xx-large')","5fae1237":"# Example Description (of Nibble) ^^ \nprint('Example Description (of Nibble) ^^ : ')\ntrain_df['Description'][0]","0723ae50":"# Metric used for this competition (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\ndef metric(y1,y2):\n    return cohen_kappa_score(y1, y2, weights = 'quadratic')\n\n# Make scorer for scikit-learn\nscorer = make_scorer(metric)","ef05e5a8":"# Clean up DataFrames\ntarget = train_df['AdoptionSpeed']\nclean_df = train_df.drop(columns = ['Name', 'RescuerID', 'Description', 'PetID', 'AdoptionSpeed'])\nclean_test = test_df.drop(columns = ['Name', 'RescuerID', 'Description', 'PetID'])","d4628bf6":"# Preparation for XGBoost\nx_train, x_valid, y_train, y_valid = train_test_split(clean_df, \n                                                      target, \n                                                      test_size = 0.2, \n                                                      random_state = seed)\n\nd_train = xgb.DMatrix(x_train, label = y_train)\nd_valid = xgb.DMatrix(x_valid, label = y_valid)\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]","455b8e97":"# Create base models\ngbm = GradientBoostingClassifier()\nclf = RandomForestClassifier()\nclf2 = ExtraTreesClassifier()\nclf3 = AdaBoostClassifier()\n\n# Parameters\nxgb_params = {'objective' : 'multi:softmax',\n              'eval_metric' : 'mlogloss',\n              'eta' : 0.05,\n              'max_depth' : 4,\n              'num_class' : 5,\n              'lambda' : 0.8\n}\n\n# Create parameters to use for Grid Search\ngbm_grid = {\n    'loss' : ['deviance'],\n    'learning_rate' : [.025, 0.5],\n    'max_depth': [5, 8],\n    'max_features': ['auto'],\n    'min_samples_leaf': [100],\n    'min_samples_split': [100],\n    'n_estimators': [100],\n    'subsample' : [.8],\n    'random_state' : [seed]\n}\n\nrand_forest_grid = {\n    'bootstrap': [True, False],\n    'max_depth': [50, 85],\n    'max_features': ['auto'],\n    'min_samples_leaf': [10, 15],\n    'min_samples_split': [10, 15],\n    'n_estimators': [150, 200, 215],\n    'random_state' : [seed]\n}\n\nextra_trees_grid = {\n    'bootstrap' : [True, False], \n    'criterion' : ['gini'], \n    'max_depth' : [50, 75], \n    'max_features': ['auto'], \n    'min_samples_leaf': [10, 15], \n    'min_samples_split': [10, 15],\n    'n_estimators': [150, 200, 215], \n    'random_state' : [seed]\n}\n\nadaboost_grid = {\n    'n_estimators' : [150, 200, 225],\n    'learning_rate' : [.2],\n    'algorithm' : ['SAMME.R'],\n    'random_state' : [seed]\n}\n\n# Search parameter space\ngbm_gridsearch = GridSearchCV(estimator = gbm, \n                              param_grid = gbm_grid, \n                              cv = 3, \n                              n_jobs = -1, \n                              verbose = 1, \n                              scoring = scorer)\n\nrand_forest_gridsearch = GridSearchCV(estimator = clf, \n                                      param_grid = rand_forest_grid, \n                                      cv = 3, \n                                      n_jobs = -1, \n                                      verbose = 1, \n                                      scoring = scorer)\n\nextra_trees_gridsearch = GridSearchCV(estimator = clf2, \n                                      param_grid = extra_trees_grid, \n                                      cv = 3, \n                                      n_jobs = -1, \n                                      verbose = 1, \n                                      scoring = scorer)\n\nadaboost_gridsearch = GridSearchCV(estimator = clf3, \n                                   param_grid = adaboost_grid, \n                                   cv = 3, \n                                   n_jobs = -1, \n                                   verbose = 1, \n                                   scoring = scorer)","42da4781":"# Fit XGBoost\nprint('Fitting XGBoost: ')\nbst = xgb.train(xgb_params, \n                d_train, \n                400, \n                watchlist, \n                early_stopping_rounds = 50, \n                verbose_eval = 0)","ec7a4853":"# Fit the models\nprint('Fitting GBM: ')\ngbm_gridsearch.fit(clean_df, target)\nprint('Fitting Random Forest: ')\nrand_forest_gridsearch.fit(clean_df, target)\nprint('Fitting Extra Trees: ')\nextra_trees_gridsearch.fit(clean_df, target)\nprint('Fitting AdaBoost: ')\nadaboost_gridsearch.fit(clean_df, target)","02643b80":"# What are the best parameters for each model\nprint('Best model parameters:\\n')\nprint('Gradient Boosting model:\\n{}\\n'.format(gbm_gridsearch.best_params_))\nprint('Random Forest model:\\n{}\\n'.format(rand_forest_gridsearch.best_params_))\nprint('Extra Trees model:\\n{}\\n'.format(extra_trees_gridsearch.best_params_))\nprint('Adaboost model:\\n{}\\n'.format(adaboost_gridsearch.best_params_))","a8f6d4a0":"# Score on training set\nmodels = {'XGBoost' : bst,\n          'Gradient Boosting' : gbm_gridsearch, \n          'Random Forest' : rand_forest_gridsearch, \n          'Extra Trees' : extra_trees_gridsearch, \n          'Adaboost' : adaboost_gridsearch}\n\nprint('Score on the training set. This allows us to spot overfitting, performance, etc. (Rounded to 4 decimals):\\n')\ntrain_scores = []\nfor name, model in models.items():\n    if name == 'XGBoost':\n        score = metric(bst.predict(xgb.DMatrix(clean_df)).astype(int), target)\n        print('{} score: {}'.format(str(name), round(score, 4)))\n    else:    \n        score = metric(model.predict(clean_df), target)\n        print('{} score: {}'.format(str(name), round(score, 4)))\n    train_scores.append(score)\n\nprint('\\nMean Score: {0:10.4f}'.format(np.mean(train_scores)))\n\nprint('\\nStandard Deviation of Scores: {0:10.4f}'.format(np.std(train_scores)))","45746f97":"# Cross validation\nval_GBM = list(cross_val_score(gbm_gridsearch, \n                               clean_df, \n                               target, \n                               scoring = scorer, \n                               cv = 5))\n\nval_RF = list(cross_val_score(rand_forest_gridsearch, \n                              clean_df, \n                              target, \n                              scoring = scorer, \n                              cv = 5))\n\nval_ET = list(cross_val_score(extra_trees_gridsearch, \n                              clean_df, \n                              target, \n                              scoring = scorer, \n                              cv = 5))\n\nval_ADA = list(cross_val_score(adaboost_gridsearch, \n                               clean_df, \n                               target, \n                               scoring = scorer, \n                               cv = 5))\n\n# Validation score for XGBoost\nval_XGB = metric(bst.predict(xgb.DMatrix(x_valid)).astype(int), y_valid)","32625912":"print('Cross validation scores:\\n\\n')\nprint('Validation Score XGBoost:\\n{}\\n\\n'.format(val_XGB))\n\nprint('Cross validation Gradient Boosting:\\n{},\\nMean score: {}\\nStd of scores: {}\\n\\n'.format([round(elem, 4) for elem in val_GBM],\n                                                                                               round(np.mean(val_GBM), 4),\n                                                                                               round(np.std(val_GBM), 4)))\n\nprint('Cross validation Random Forest:\\n{},\\nMean score: {}\\nStd of scores: {}\\n\\n'.format([round(elem, 4) for elem in val_RF],\n                                                                                           round(np.mean(val_RF), 4),\n                                                                                           round(np.std(val_RF), 4)))\n\nprint('Cross validation Extra Trees:\\n{},\\nMean score: {}\\nStd of scores: {}\\n\\n'.format([round(elem, 4) for elem in val_ET], \n                                                                                        round(np.mean(val_ET), 4),\n                                                                                        round(np.std(val_ET), 4)))\n\nprint('Cross validation AdaBoost:\\n{},\\nMean score: {}\\nStd of scores: {}\\n\\n'.format([round(elem, 4) for elem in val_ADA], \n                                                                                      round(np.mean(val_ADA), 4),\n                                                                                      round(np.std(val_ADA), 4)))\n\nprint('Mean Validation score: {}'.format(round(np.mean([np.mean(val_GBM),\n                                                        np.mean(val_RF), \n                                                        np.mean(val_ET), \n                                                        np.mean(val_ADA), \n                                                        val_XGB]), 4)))\n\nprint('Standard Deviation of Cross Validation scores: {}'.format(round(np.std([np.mean(val_GBM),\n                                                                               np.mean(val_RF), \n                                                                               np.mean(val_ET), \n                                                                               np.mean(val_ADA), \n                                                                               val_XGB]), 4)))\n","11c47376":"# Get predictions\npred0 = gbm_gridsearch.predict(clean_test)\npred1 = rand_forest_gridsearch.predict(clean_test)\npred2 = extra_trees_gridsearch.predict(clean_test)\npred3 = adaboost_gridsearch.predict(clean_test)\npred4 = bst.predict(xgb.DMatrix(clean_test)).astype(int)\n\n# Combine predictions\nfinal_predictions = []\n# Get average of predictions\nfor pred in zip(pred0, pred1, pred2, pred3, pred4):\n    final_predictions.append(int(round((sum(pred)) \/ len(pred), 0)))","d114322f":"# Compare predictions\nprediction_df = pd.DataFrame({'PetID' : test_df['PetID'],\n                              'Gradient Boosting' : pred0,\n                              'Random Forest' : pred1,\n                              'Extra Trees' : pred2,\n                              'Adaboost' : pred3,\n                              'XGBoost' : pred4\n})\n\nprint('Predictions for each model: ')\nprediction_df.head(10)","1e29db1f":"# Store predictions for Kaggle Submission\nsubmission_df = pd.DataFrame(data = {'PetID' : test_df['PetID'], \n                                     'AdoptionSpeed' : final_predictions})\nsubmission_df.to_csv('submission.csv', index = False)","aa60e20b":"# Check submission\nsubmission_df.head(3)","c8010f05":"# Compare distributions of training set and test set (Adoption Speed)\n\n# Plot 1\nplt.figure(figsize = (15,4))\nplt.subplot(211)\ntrain_df['AdoptionSpeed'].value_counts().sort_index(ascending = False).plot(kind = 'barh')\nplt.title('Target Variable distribution in training set', fontsize = 'large')\n\n# Plot 2\nplt.subplot(212)\nsubmission_df['AdoptionSpeed'].value_counts().sort_index(ascending = False).plot(kind = 'barh')\nplt.title('Target Variable distribution in predictions')\n\nplt.subplots_adjust(top = 2)","36d78b56":"## Tree Ensembling <a id=\"7\"><\/a>","73fefa57":"The metric used for this competition is called ''[Quadratic Weighted Kappa](https:\/\/en.wikipedia.org\/wiki\/Cohen%27s_kappa)''.\n\nWe can use [scikit-learn's 'cohen_kappa_score' function](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.cohen_kappa_score.html) almost straight out-of-the-box as long as we specify the weights to be quadratic. This is the function we will use for cross-validation.","5be53915":"## PetFinder.my Adoption Prediction","968f7171":"## Table of contents","91b6f9fc":"## Predictions <a id=\"8\"><\/a>","72368196":"## Metric <a id=\"6\"><\/a>","9b7ce4b5":"![](https:\/\/blog.groomit.me\/wp-content\/uploads\/2018\/02\/petfinder2.jpg)","8aeecf8f":"If you like this Kaggle kernel, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!","ef61a997":"## Dependencies <a id=\"2\"><\/a>","39e713a3":"## Data columns <a id=\"1\"><\/a>\n\n[Source](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/data)\n\n* PetID - Unique hash ID of pet profile\n* AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n* Type - Type of animal (1 = Dog, 2 = Cat)\n* Name - Name of pet (Empty if not named)\n* Age - Age of pet when listed, in months\n* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n* Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n* Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n* Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n* Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n* MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n* FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n* Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n* Quantity - Number of pets represented in profile\n* Fee - Adoption fee (0 = Free)\n* State - State location in Malaysia (Refer to StateLabels dictionary)\n* RescuerID - Unique hash ID of rescuer\n* VideoAmt - Total uploaded videos for this pet\n* PhotoAmt - Total uploaded photos for this pet\n* Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.\n","4e508c0e":"## Data Description <a id=\"4\"><\/a>","0269b463":"We will use predictions from a [XGBoost model](https:\/\/xgboost.readthedocs.io\/en\/latest\/), a [Gradient Boosting Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html), a [Random Forest Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html), an [Extra Trees Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html) and an [AdaBoost Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html). Later we will take the average of all models to get final predictions. [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) is used to get near-optimal parameters for almost all models.","54d18947":"## Kaggle Submission <a id=\"9\"><\/a>","78c82db7":"## Preparation <a id=\"3\"><\/a>","441ad0dd":"## Data Cleaning <a id=\"10\"><\/a>","f14bc92f":"## Visualization <a id=\"5\"><\/a>","7306561f":"This notebook features an overview of the competition, EDA (Exploratory Data Analysis) and ensembling of different models. My goal with this kernel is too keep it concise and illustrate the basic principles behind [ensembling](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning).","63e131b3":"- [Data Columns](#1)\n- [Dependencies](#2)\n- [Preparation](#3)\n- [Data Description](#4)\n- [Visualization](#5)\n- [Metric](#6)\n- [Data Cleaning](#10)\n- [Tree Ensembling](#7)\n- [Predictions](#8)\n- [Kaggle Submission](#9)"}}