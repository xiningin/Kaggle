{"cell_type":{"35ba5e5f":"code","b07a09c9":"code","919f076d":"code","b466dae2":"code","62ea94f9":"code","80858bf0":"code","dd6884f4":"code","ee338bef":"code","600e9c0b":"code","367f4f76":"code","19bb2178":"code","7a0557b3":"code","4a321b6d":"code","02b86f73":"code","58fb19ad":"code","dc450ece":"code","38bc2616":"code","c6342d94":"code","f062bc94":"code","8a146e36":"code","94f5918a":"code","54b44e06":"code","f4b6bd41":"code","5d5ee4b9":"code","4404b1bf":"code","bb4f56ef":"code","b2f77523":"code","f8d2a96f":"code","5895ee6f":"code","afc54fd0":"code","0738ac31":"code","f71455e5":"code","7bf644fb":"code","8eca8c36":"code","26513a08":"code","ced698e2":"code","4d5d4c5e":"code","afcb3433":"code","4b747f89":"code","79e429e5":"code","d699e6f7":"code","bc90a0cf":"code","89f8965b":"code","dc1fcdfe":"code","80302779":"code","de553bf7":"code","09736ba3":"code","f71471f9":"code","5b8e8e79":"code","840655d2":"code","ee404ca8":"code","2ccf0856":"code","012f250b":"code","157f6145":"code","3b4fc388":"code","9f1408fc":"code","67c7cd46":"code","bf904a7e":"code","1571166c":"code","00da6a4b":"code","a8ba5fe8":"code","97b1b2ac":"code","c97871f1":"code","e06155b5":"code","93d98869":"code","2f3741c2":"code","0bf852dc":"code","74ef810f":"code","4f80bde9":"code","ebc75a6e":"code","1c38ba22":"code","2398a55a":"code","deef8b7d":"code","98b5b927":"code","6a820e40":"code","c0d7309e":"code","1b73287d":"code","0a630152":"code","246c93b4":"code","3d5a2ce5":"code","20e4b7d4":"code","ed925687":"code","c9780a3d":"code","23903d48":"code","4e23a5f7":"code","6b1edb85":"code","8fea04d0":"code","f3bcd5e5":"code","e5f6e7b0":"code","e4b87b7e":"code","ad9f8681":"code","018f0fad":"code","02cf8a01":"code","a416ae17":"code","fa141f95":"code","60f6ea5a":"code","8e911804":"code","334e88c5":"code","3fe0c56f":"markdown","f2ca248e":"markdown","c893a2bc":"markdown","5b68524d":"markdown","d7e44606":"markdown","025ab22d":"markdown","c0117fb4":"markdown","cd6421f6":"markdown","85c2c3ca":"markdown","60efecb6":"markdown","40a97332":"markdown","93d0274c":"markdown","9918d07e":"markdown","8620d608":"markdown","a99ce4f2":"markdown","7c3c5017":"markdown"},"source":{"35ba5e5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b07a09c9":"sub_file = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsub_file.head()","919f076d":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","b466dae2":"val = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nval.head()","62ea94f9":"train.columns","80858bf0":"val.columns","dd6884f4":"train.isnull().mean()","ee338bef":"val.isnull().mean()","600e9c0b":"train.shape","367f4f76":"train.describe()","19bb2178":"val.describe()","7a0557b3":"def impute_na_numeric(train,val,var):\n    mean = train[var].mean()\n    median = train[var].median()\n    \n    train[var+\"_mean\"] = train[var].fillna(mean)\n    train[var+\"_median\"] = train[var].fillna(median)\n    \n    var_original = train[var].std()**2\n    var_mean = train[var+\"_mean\"].std()**2\n    var_median = train[var+\"_median\"].std()**2\n    \n    print(\"Original Variance: \",var_original)\n    print(\"Mean Variance: \",var_mean)\n    print(\"Median Variance: \",var_median)\n    \n    if((var_mean < var_original) | (var_median < var_original)):\n        if(var_mean < var_median):\n            train[var] = train[var+\"_mean\"]\n            val[var] = val[var].fillna(mean)\n        else:\n            train[var] = train[var+\"_median\"]\n            val[var] = val[var].fillna(median)\n    else:\n        val[var] = val[var].fillna(median)\n    train.drop([var+\"_mean\",var+\"_median\"], axis=1, inplace=True)","4a321b6d":"impute_na_numeric(train,val,\"Age\")","02b86f73":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","58fb19ad":"impute_na_numeric(train,val,\"Fare\")","dc450ece":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Fare', bins=20)","38bc2616":"train[\"Embarked\"].mode().values[0]","c6342d94":"def impute_na_non_numeric(train,val,var):\n    mode = train[var].mode().values[0]\n    train[var] = train[var].fillna(mode)\n    val[var] = val[var].fillna(mode)","f062bc94":"impute_na_non_numeric(train,val,\"Embarked\")","8a146e36":"def impute_na_max_missing(train,val,var,prefix):\n    train[prefix+\"_\"+var] = np.where(train[var].isna(),0,1)\n    train.drop([var],axis=1,inplace=True)\n    val[prefix+\"_\"+var] = np.where(val[var].isna(),0,1)\n    val.drop([var],axis=1,inplace=True)","94f5918a":"impute_na_max_missing(train,val,\"Cabin\",\"had\")","54b44e06":"train.head()","f4b6bd41":"train[\"Family_Size\"] = train[\"SibSp\"] + train[\"Parch\"]\nval[\"Family_Size\"] = val[\"SibSp\"] + val[\"Parch\"]","5d5ee4b9":"train[\"Salutation\"] = train[\"Name\"].map(lambda x: x.split(',')[1].split()[0])","4404b1bf":"train[\"Salutation\"].unique()","bb4f56ef":"val[\"Salutation\"] = val[\"Name\"].map(lambda x: x.split(',')[1].split()[0])","b2f77523":"val[\"Salutation\"].unique()","f8d2a96f":"val[val[\"Salutation\"] == \"Dona.\"]","5895ee6f":"def transform_with_target_probs(train,val,var,target):\n    var_dict = train.groupby([var])[target].mean().to_dict()\n    train[var] = train[var].map(var_dict)\n    val[var] = val[var].map(var_dict)","afc54fd0":"transform_with_target_probs(train,val,\"Pclass\",\"Survived\")","0738ac31":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","f71455e5":"transform_with_target_probs(train,val,\"Sex\",\"Survived\")","7bf644fb":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Sex', alpha=.5, bins=20)\ngrid.add_legend();","8eca8c36":"transform_with_target_probs(train,val,\"Embarked\",\"Survived\")","26513a08":"grid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Embarked', alpha=.5, bins=20)\ngrid.add_legend();","ced698e2":"train[\"Salutation\"] = train[\"Salutation\"].apply(lambda x: x.split('.')[0])\nval[\"Salutation\"] = val[\"Salutation\"].apply(lambda x: x.split('.')[0])","4d5d4c5e":"def get_salutation_map(df,var,rare):\n    sal_dict = {}\n    for sal, count in df[var].value_counts().to_dict().items():\n        count = int(count)\n        if count < 10:\n            sal_dict[sal] = rare\n        else:\n            sal_dict[sal] = sal\n    return sal_dict","afcb3433":"transform_with_target_probs(train,val,\"Salutation\",\"Survived\")","4b747f89":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0)], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1)], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","79e429e5":"# Explore Age distribution \ng = sns.distplot(train[\"Age\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"Age\"].skew()))\ng = g.legend(loc=\"best\")","d699e6f7":"train[\"Fare\"].describe()","bc90a0cf":"# Explore Fare distribution \ng = sns.distplot(train[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","89f8965b":"# Apply log to Fare to reduce skewness distribution\ntrain[\"Fare\"] = train[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","dc1fcdfe":"g = sns.factorplot(x=\"Survived\", y = \"Age\", hue = \"had_Cabin\", data = train, kind=\"violin\")","80302779":"train = pd.get_dummies(train, columns=[\"had_Cabin\"], drop_first=True)\nval = pd.get_dummies(val, columns=[\"had_Cabin\"], drop_first=True)","de553bf7":"drop_cols = ['PassengerId', 'Name', 'SibSp','Parch', 'Ticket']","09736ba3":"train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).values","f71471f9":"train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).columns","5b8e8e79":"X = train.drop(drop_cols,axis=1).drop([\"Survived\"],axis=1).values\ny = train[\"Survived\"].values","840655d2":"val[\"Salutation\"] = val[\"Salutation\"].fillna(val[\"Salutation\"].mode().values[0])\nval_test = val.drop(drop_cols,axis=1).values","ee404ca8":"val[\"Salutation\"] = val[\"Salutation\"].fillna(val[\"Salutation\"].mode().values[0])\nval_test = val.drop(drop_cols,axis=1).values","2ccf0856":"from sklearn.model_selection import train_test_split","012f250b":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=101)","157f6145":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()","3b4fc388":"mms.fit(X_train)","9f1408fc":"X_train_mms = mms.transform(X_train)","67c7cd46":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","bf904a7e":"ss.fit(X_train)","1571166c":"X_train_ss = ss.transform(X_train)","00da6a4b":"# For Age\n\nsns.jointplot(X_train[:,2], X_train_mms[:,2], kind='kde')","a8ba5fe8":"# For Age\n\nsns.jointplot(X_train[:,2], X_train_ss[:,2], kind='kde')","97b1b2ac":"# For Fare\n\nsns.jointplot(X_train[:,3], X_train_mms[:,3], kind='kde')","c97871f1":"# For Fare\n\nsns.jointplot(X_train[:,3], X_train_ss[:,3], kind='kde')","e06155b5":"X_test_ss = ss.transform(X_test)","93d98869":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,VotingClassifier","2f3741c2":"from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score","0bf852dc":"classification_models = ['LogisticRegression',\n                         'SVC',\n                         'DecisionTreeClassifier',\n                         'RandomForestClassifier',\n                         'AdaBoostClassifier']","74ef810f":"cm = []\nacc = []\nprec = []\nrec = []\nf1 = []\nmodels = []\nestimators = []","4f80bde9":"for classfication_model in classification_models:\n    \n    model = eval(classfication_model)()\n    \n    model.fit(X_train_ss,y_train)\n    y_pred = model.predict(X_test_ss)\n    \n    models.append(type(model).__name__)\n    estimators.append((type(model).__name__,model))\n    cm.append(confusion_matrix(y_test,y_pred))\n    acc.append(accuracy_score(y_test,y_pred))\n    prec.append(precision_score(y_test,y_pred))\n    rec.append(recall_score(y_test,y_pred))\n    f1.append(f1_score(y_test,y_pred))","ebc75a6e":"vc = VotingClassifier(estimators)\nvc.fit(X_train_ss,y_train)","1c38ba22":"y_pred = vc.predict(X_test_ss)\n    \nmodels.append(type(vc).__name__)\n\ncm.append(confusion_matrix(y_test,y_pred))\nacc.append(accuracy_score(y_test,y_pred))\nprec.append(precision_score(y_test,y_pred))\nrec.append(recall_score(y_test,y_pred))\nf1.append(f1_score(y_test,y_pred))","2398a55a":"model_dict = {\"Models\":models,\n             \"CM\":cm,\n             \"Accuracy\":acc,\n             \"Precision\":prec,\n             \"Recall\":rec,\n             \"f1_score\":f1}","deef8b7d":"model_df = pd.DataFrame(model_dict)\nmodel_df","98b5b927":"model_df.sort_values(by=['Accuracy','f1_score','Recall','Precision'],ascending=False,inplace=True)\nmodel_df","6a820e40":"val_test = ss.transform(val_test)","c0d7309e":"y_pred_sub = vc.predict(val_test)","1b73287d":"sub_df = pd.concat([val['PassengerId'],\n                    pd.DataFrame(y_pred_sub,columns=[\"Survived\"])],\n                   axis=1)\nsub_df.head()","0a630152":"sub_df.to_csv(\"Stacked_Ensemble_Baseline_Submission.csv\", index=False)","246c93b4":"model_param_grid = {}","3d5a2ce5":"model_param_grid['LogisticRegression'] = {'penalty' : ['l1', 'l2'],\n                                          'C' : np.logspace(0, 4, 10)}","20e4b7d4":"model_param_grid['SVC'] = [{'kernel': ['rbf'], \n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['sigmoid'],\n                            'gamma': [1e-2, 1e-3, 1e-4, 1e-5],\n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['linear'], \n                            'C': [0.001, 0.10, 0.1, 10, 25, 50, 100, 1000]},\n                           {'kernel': ['poly'], \n                            'degree' : [0, 1, 2, 3, 4, 5, 6]}\n                          ]","ed925687":"model_param_grid['DecisionTreeClassifier'] = {'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n                                              'min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11]}","c9780a3d":"model_param_grid['RandomForestClassifier'] = {'n_estimators' : [25,50,75,100],\n                                              'criterion' : [\"gini\",\"entropy\"],\n                                              'max_features': ['auto', 'sqrt', 'log2'],\n                                              'class_weight' : [\"balanced\", \"balanced_subsample\"]}","23903d48":"model_param_grid['AdaBoostClassifier'] = {'n_estimators' : [25,50,75,100],\n                                          'learning_rate' : [0.001,0.01,0.05,0.1,1,10],\n                                          'algorithm' : ['SAMME', 'SAMME.R']}","4e23a5f7":"from sklearn.model_selection import GridSearchCV\ndef tune_parameters(model_name,model,params,cv,scorer,X,y):\n    best_model = GridSearchCV(estimator = model,\n                              param_grid = params,\n                              scoring = scorer,\n                              cv = cv,\n                              n_jobs = -1).fit(X, y)\n    print(\"Tuning Results for \", model_name)\n    print(\"Best Score Achieved: \",best_model.best_score_)\n    print(\"Best Parameters Used: \",best_model.best_params_)\n    return best_model","6b1edb85":"from sklearn.metrics import make_scorer\n\n# Define scorer\ndef f1_metric(y_test, y_pred):\n    score = f1_score(y_test, y_pred)\n    return score","8fea04d0":"# Scorer function would try to maximize calculated metric\nf1_scorer = make_scorer(f1_metric,greater_is_better=True)","f3bcd5e5":"best_estimators = []","e5f6e7b0":"for m_name, m_obj in estimators:\n    best_estimators.append((m_name,tune_parameters(m_name,\n                                                   m_obj,\n                                                   model_param_grid[m_name],\n                                                   10,\n                                                   f1_scorer,\n                                                   X_train_ss,\n                                                   y_train)))","e4b87b7e":"tuned_estimators = []","ad9f8681":"tuned_lr = LogisticRegression(C=2.7825594022071245, \n                              penalty = 'l1')\ntuned_lr.fit(X_train_ss,y_train)\ntuned_estimators.append((\"LogisticRegression\",tuned_lr))","018f0fad":"tuned_svc = SVC(C = 10, gamma = 0.01, kernel = 'rbf', probability=True)\ntuned_svc.fit(X_train_ss,y_train)\ntuned_estimators.append((\"SVC\",tuned_svc))","02cf8a01":"tuned_dt = DecisionTreeClassifier(criterion = 'entropy', \n                                  max_features = 'log2', \n                                  min_samples_leaf = 5, \n                                  min_samples_split = 11)\ntuned_dt.fit(X_train_ss,y_train)\ntuned_estimators.append((\"DecisionTreeClassifier\",tuned_dt))","a416ae17":"tuned_rf = RandomForestClassifier(class_weight = 'balanced_subsample', \n                                  criterion = 'gini', \n                                  max_features = 'sqrt', \n                                  n_estimators = 100)\ntuned_rf.fit(X_train_ss,y_train)\ntuned_estimators.append((\"RandomForestClassifier\",tuned_rf))","fa141f95":"tuned_adb = AdaBoostClassifier(algorithm = 'SAMME', \n                                  learning_rate = 0.1, \n                                  n_estimators = 75)\ntuned_adb.fit(X_train_ss,y_train)\ntuned_estimators.append((\"AdaBoostClassifier\",tuned_adb))","60f6ea5a":"tuned_vc = VotingClassifier(tuned_estimators)\ntuned_vc.fit(X_train_ss,y_train)","8e911804":"y_pred_tuned_sub = tuned_vc.predict(val_test)\ntuned_sub_df = pd.concat([val['PassengerId'],\n                          pd.DataFrame(y_pred_tuned_sub,columns=[\"Survived\"])],\n                         axis=1)\ntuned_sub_df.head()","334e88c5":"tuned_sub_df.to_csv(\"Stacked_Ensemble_Tuned_Submission.csv\", index=False)","3fe0c56f":"### Achieved Kaggle Score = 0.72248","f2ca248e":"### Scale Test file data","c893a2bc":"### Treating null values for continuous data","5b68524d":"#### Standard Scaling","d7e44606":"### Treating null values for Categorical data","025ab22d":" ### Define Customer Scorer Function","c0117fb4":"### Feature Scaling\n#### Min-Max scaling","cd6421f6":"### Explore Numeric Data","85c2c3ca":"### Check Distribution after Scaling","60efecb6":"#### Extracting Salutations from the name","40a97332":"### Creating new feature extracting from existing","93d0274c":"### Function to perform Grid Search with Cross Validation","9918d07e":"#### Stacking Ensemble","8620d608":"#### Hyper Parameter Tuning","a99ce4f2":"#### Split into training & test sets","7c3c5017":"### Run iterations for all the trained baseline models"}}