{"cell_type":{"2be61b49":"code","a052f6d5":"code","49e92353":"code","2474dd77":"code","5979e8c1":"code","72d78c2a":"code","3f3e3682":"code","47401bd2":"code","b7e811fe":"code","91a9791a":"code","6a2f796c":"code","1f108d3a":"code","e3d909c8":"code","6ce7c4b9":"code","a2271899":"code","4f7a77d6":"code","61f02dc6":"code","d3ccc168":"code","b7073961":"code","f1e53f73":"code","323fead9":"code","71959b6a":"markdown","9dd3d06d":"markdown","9fcc8f26":"markdown","ac98f9b4":"markdown","8386509c":"markdown","2c716021":"markdown","184d4da7":"markdown","39c73c90":"markdown","e6af89e0":"markdown","947bbca8":"markdown","704215f1":"markdown","a6dd5882":"markdown","0e392de2":"markdown","8c4b171b":"markdown"},"source":{"2be61b49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a052f6d5":"#reading dataset\n#making our ID as an index so it doesn't mess up with our model.\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col='Id')\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col='Id')\n\n#making our traning\/testing dataset ready.\nX_train = pd.DataFrame(train)\nY_train = train['Cover_Type']\nX_train.drop(columns = ['Cover_Type'],inplace=True)\nX_test = pd.DataFrame(test)  ","49e92353":"test.head(10)","2474dd77":"#number of features, samples\ntrain.shape, test.shape","5979e8c1":"import matplotlib.pyplot as plt\n#histogram with no soils\ncols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n       'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n       'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n       'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1',\n       'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\nhist_data = test[cols]\nhist_data.hist(figsize=(15,10))\nplt.tight_layout()","72d78c2a":"test['Wilderness_Area2'].value_counts()","3f3e3682":"train.info()","47401bd2":"train['Cover_Type'].unique()","b7e811fe":"#check for all soil columns are redundant or not\nfor name in range(1,41):\n    cur_soil_name = \"Soil_Type\"+ str(name)\n    soil = pd.DataFrame(train[cur_soil_name])\n    print(soil.nunique())      ","91a9791a":"train[\"Soil_Type15\"].unique()","6a2f796c":"X_train.drop(columns = ['Soil_Type7','Soil_Type15'], inplace=True)\nX_test.drop(columns = ['Soil_Type7','Soil_Type15'], inplace=True)","1f108d3a":"X_train['Soil_Type12'].value_counts()","e3d909c8":"X_train.describe()","6ce7c4b9":"#try different models evaluate them using cross_val_score\nfrom sklearn.linear_model import LogisticRegression  \nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nscoring='accuracy'\nmodels = []\nmodels.append(('LR',  LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART',DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=10, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","a2271899":"#more advance models\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nmodels = []\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('ET', ExtraTreeClassifier()))\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=10, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","4f7a77d6":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state = 42)","61f02dc6":"from sklearn.model_selection import GridSearchCV\nrfc = RandomForestClassifier(random_state= 42,oob_score=False)\n\n# max_fea = np.arange(1,9)\nparams =  {\"n_estimators\": [150,100], \n           \"max_depth\": [15,None]\n          } \n#with max_features, min_samples_leaf it was taking too long and results merely changed,\n#so removed them.\n#max_depth above 10 and below 20 is great.\n#will try with oob_score later.\n\ngscv = GridSearchCV(estimator = rfc, param_grid = params, cv = 10, n_jobs =-1)\ngscv.fit(x_train,y_train)\n\nprint(\"GSCV Score: %.2f%% \"% gscv.score(x_val,y_val))\ngscv.best_params_ ","d3ccc168":"#final model with GridSearchCV findings \nrf = RandomForestClassifier(n_estimators = 150, random_state= 42, max_depth=None)\ncv_results = model_selection.cross_val_score(rf, X_train, Y_train, cv=10, scoring=scoring)\nprint(\"Random Forest Result: %.2f%%\" % cv_results.mean())","b7073961":"# Calculate MAE\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nrf.fit(x_train, y_train)\ny_pred = rf.predict(x_val)\nmae_score = mean_absolute_error(y_val, y_pred)\nacc_score = accuracy_score(y_val, y_pred)\nprint(\"Accuracy Score: %.2f%%\" % acc_score)\nprint(\"Mean Absolute Error: %.2f%%\" % mae_score)","f1e53f73":"#feature importance\nrf.feature_importances_","323fead9":"#submission\nrf.fit(x_train, y_train)\npred = rf.predict(X_test)\ntest_ids = X_test.index\noutput = pd.DataFrame({'Id': test_ids,\n                       'Cover_Type': pred})\noutput.to_csv('submission.csv', index=False)\noutput.head(10)","71959b6a":"Assuming this dataset is for beginners, Data Cleaning wouldn't be much of a necesssity.","9dd3d06d":"![](http:\/\/)No they are not, only Soil_Type7 and Soil_Type15 have common value 0. Not much of a use here. Lets get rid of them. ","9fcc8f26":"As for we need the y_val here to validate. As the kaggle competitions rules the y_test is hidden until the results breaking day.<br>\nSo let's split the training set and create one for our purpose.","ac98f9b4":"# Results\n**CrossValScore is 78%**.<br>\n**Accuracy Score is 88%**.<br>\n**Mean Absolute Error is 29%**.<br>\nA Good RandomForest will always make you proud.<br>","8386509c":"# Data Cleaning","2c716021":"# Increasing Performance \nRandom Forest proves a Good Model lets tune it further for good.","184d4da7":"None nulls, all ints. Alright.","39c73c90":"# Output","e6af89e0":"# Hello\nHere I will only try the models that I've worked with and practiced with. I hope you get to learn from this notebook, I will try to make changes with the flow. I think most of the defualt parameters of sklearn are suffcient, so better to invest time in Feature Enggineering than Model Tuning. \nI am learning still, so will stick to basics anyway.<br>\nHappy Kaggling.. ","947bbca8":"# Models Implementaion\nWe will use cross_val_score to validate as they are more accurate due to the number of cross validation splits tests used.","704215f1":"# Data Analysis","a6dd5882":"Next version :\n* Make use of feature importance too.\n* More Tuning.\n* More on feature Enggineering.\n","0e392de2":"The balance of Soil_Type-- data will be a slippery slope here, as the Soil_Types are One-Hot Encoder output. Makes sense so model don't overfit.","8c4b171b":"Very much of a strange isn't it."}}