{"cell_type":{"09c79f55":"code","36e3fb96":"code","472c466c":"code","ad64a0e7":"code","0ce390d0":"code","6bdcbe49":"code","1314b962":"code","bcb35814":"code","7099b122":"code","56905720":"code","b7f58b2e":"code","5556f863":"code","b1991d0f":"code","65b68ea2":"code","488a7965":"code","3731721f":"code","b6a42b54":"code","8fbaf1d1":"code","abc88ee7":"markdown"},"source":{"09c79f55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","36e3fb96":"df = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip')\ndf.head()","472c466c":"df.info()","ad64a0e7":"start_date = df.columns[1]\nlast_date = df.columns[-1]\nprint(f\"Date range: {start_date} to {last_date}\")","0ce390d0":"def plot_graph(df,n_series):\n    sample = df.sample(n_series,random_state=42)\n    page = sample[\"Page\"].to_list()\n    series_sample = sample.loc[:,start_date:last_date]\n    plt.figure(figsize=(15,10))\n    \n    \n    for i in range(series_sample.shape[0]):\n        np.log1p(pd.Series(series_sample.iloc[i]).astype(np.float64)).plot(linewidth=1.5)\n    \n    plt.title(\"Time vs Views on random website\")\n    plt.legend(page)\nplot_graph(df,6)    ","6bdcbe49":"from datetime import timedelta\n\npred_steps = 14\npred_length=timedelta(pred_steps)\n\nfirst_day = pd.to_datetime(start_date) \nlast_day = pd.to_datetime(last_date)\n\nval_pred_start = last_day - pred_length + timedelta(1)\nval_pred_end = last_day\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1)","1314b962":"enc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1)","bcb35814":"print('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)","7099b122":"date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]),\n                          data=[i for i in range(len(df.columns[1:]))])\nseries_array = df[df.columns[1:]].values\n\ndef get_time_block_series(series_array, date_to_index, start_date, end_date):\n    \n    inds = date_to_index[start_date:end_date]\n    return series_array[:,inds]\n\ndef transform_series_encode(series_array):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_mean = series_array.mean(axis=1).reshape(-1,1) \n    series_array = series_array - series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array, series_mean\n\ndef transform_series_decode(series_array, encode_series_mean):\n    \n    series_array = np.log1p(np.nan_to_num(series_array)) # filling NaN with 0\n    series_array = series_array - encode_series_mean\n    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n    \n    return series_array","56905720":"from keras.models import Model\nfrom keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate\nfrom keras.optimizers import Adam\n\n# convolutional layer parameters\nn_filters = 32 \nfilter_width = 2\ndilation_rates = [2**i for i in range(8)] \n\n# define an input history series and pass it through a stack of dilated causal convolutions. \nhistory_seq = Input(shape=(None, 1))\nx = history_seq\n\nfor dilation_rate in dilation_rates:\n    x = Conv1D(filters=n_filters,\n               kernel_size=filter_width, \n               padding='causal',\n               dilation_rate=dilation_rate)(x)\n\nx = Dense(128, activation='relu')(x)\nx = Dropout(.2)(x)\nx = Dense(1)(x)\n\n# extract the last 14 time steps as the training target\ndef slice(x, seq_length):\n    return x[:,-seq_length:,:]\n\npred_seq_train = Lambda(slice, arguments={'seq_length':14})(x)\n\nmodel = Model(history_seq, pred_seq_train)","b7f58b2e":"model.summary()","5556f863":"first_n_samples = 40000\nbatch_size = 2**11\nepochs = 10\n\n# sample of series from train_enc_start to train_enc_end  \nencoder_input_data = get_time_block_series(series_array, date_to_index, \n                                           train_enc_start, train_enc_end)[:first_n_samples]\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\n# sample of series from train_pred_start to train_pred_end \ndecoder_target_data = get_time_block_series(series_array, date_to_index, \n                                            train_pred_start, train_pred_end)[:first_n_samples]\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n\n# we append a lagged history of the target series to the input data, \n# so that we can train with teacher forcing\nlagged_target_history = decoder_target_data[:,:-1,:1]\nencoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n\nmodel.compile(Adam(), loss='mean_absolute_error')\nhistory = model.fit(encoder_input_data, decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.2)","b1991d0f":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.xlabel('Epoch')\nplt.ylabel('Mean Absolute Error Loss')\nplt.title('Loss Over Time')\nplt.legend(['Train','Valid'])","65b68ea2":"\ndef predict_sequence(input_sequence):\n\n    history_sequence = input_sequence.copy()\n    pred_sequence = np.zeros((1,pred_steps,1)) # initialize output (pred_steps time steps)  \n    \n    for i in range(pred_steps):\n        \n        # record next time step prediction (last time step of model output) \n        last_step_pred = model.predict(history_sequence)[0,-1,0]\n        pred_sequence[0,i,0] = last_step_pred\n        \n        # add the next time step prediction to the history sequence\n        history_sequence = np.concatenate([history_sequence, \n                                           last_step_pred.reshape(-1,1,1)], axis=1)\n\n    return pred_sequence","488a7965":"encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\nencoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n\ndecoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\ndecoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n","3731721f":"def predict_and_plot(encoder_input_data, decoder_target_data, sample_ind, enc_tail_len=50):\n\n    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n    pred_series = predict_sequence(encode_series)\n    \n    encode_series = encode_series.reshape(-1,1)\n    pred_series = pred_series.reshape(-1,1)   \n    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n    \n    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n    x_encode = encode_series_tail.shape[0]\n    \n    plt.figure(figsize=(10,6))   \n    \n    plt.plot(range(1,x_encode+1),encode_series_tail)\n    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n    \n    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n    plt.legend(['Encoding Series','Target Series','Predictions'])","b6a42b54":"#better than LSTM\npredict_and_plot(encoder_input_data, decoder_target_data, 100)","8fbaf1d1":"predict_and_plot(encoder_input_data, decoder_target_data, 6007)","abc88ee7":"So I am making this kernel as I am learning to make forecast with wavenet like architecture.\nI was looking for some tutorial and came across this one https:\/\/jeddy92.github.io\/JEddy92.github.io\/ts_seq2seq_conv\/ and tried to learn and imitate it."}}