{"cell_type":{"8eb4501f":"code","cd619bcb":"code","a62b77fd":"code","a3f5f980":"code","7f9dd8d4":"code","39165af7":"code","75b4ccdc":"code","d67aa089":"code","884df16b":"code","91c6279c":"code","529e1b52":"code","d705cbff":"code","0641da04":"code","de645ff1":"code","c7bd6fe7":"code","36dc9490":"code","3af2524e":"code","dc4204cd":"code","1384c628":"code","b5852ca5":"code","ea7b8241":"code","d7271de2":"code","4c17e28e":"code","bd8a8774":"code","8d369f9a":"code","f20399b7":"code","d654ffda":"code","2443876e":"code","20f0af2d":"code","72541407":"code","90add667":"code","ebe27680":"code","6fdfafb0":"code","568c62a4":"code","99df662e":"code","8cb65417":"code","9b19867a":"code","656bff11":"code","412c3f9f":"code","d8ba36dc":"code","10aefcc1":"code","c2af717a":"code","fcf5d9b2":"code","d2f03402":"markdown","a28056a1":"markdown","ef757e34":"markdown","9703a648":"markdown","0aa6f3a5":"markdown","f9a3b06e":"markdown"},"source":{"8eb4501f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cd619bcb":"# Importing the neccasry Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a62b77fd":"# Data Loading\ndf=pd.read_csv(\"\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")\ndf.head()","a3f5f980":"df.TenYearCHD.value_counts()","7f9dd8d4":"df.TenYearCHD.value_counts(normalize=True).plot(kind='bar')\nplt.show()","39165af7":"## Checking for Null Values and imputing","75b4ccdc":"df.info()","d67aa089":"df.isnull().sum()","884df16b":"# Forward and Backward fill is used to fill the null values so the distribution is not affected\ndf.fillna(method='ffill',inplace=True)\ndf.fillna(method='bfill',inplace=True)","91c6279c":"df.info()","529e1b52":"# Checking for outliers in the data set\n\ncols=[\"age\",\"cigsPerDay\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\"]\n\nfor col in cols:\n    sns.boxplot(df[col])\n    #df[col].plot(kind='box')\n    plt.show()","d705cbff":"sns.scatterplot(df[\"BMI\"],df[\"glucose\"],hue=df[\"TenYearCHD\"])","0641da04":"import statsmodels.api as sm\ny=df.TenYearCHD\nX=df.drop('TenYearCHD',axis=1)","de645ff1":"Xc=sm.add_constant(X)\nmodel=sm.Logit(y,Xc)\nresult=model.fit()\nresult.summary()","c7bd6fe7":"# check multicollinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif","36dc9490":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","3af2524e":"# VIF tabel shows tht there is no serious multicollonearity\n# backward elimination to drop varables one by one\n\ncols=list(Xc.columns)\np=[]\nwhile len(cols)>2:\n    Xc=Xc[cols]\n    model=sm.Logit(y,Xc).fit().pvalues\n    p=pd.Series(model.values[1:],index=Xc.columns[1:])\n    pmax= max(p)\n    pid=p.idxmax()\n    \n    if pmax>0.05:\n        cols.remove(pid)\n        print('column removed :',pid,pmax)\n    else:\n        break\n        \ncols","dc4204cd":"model=sm.Logit(y,Xc[cols])\nresult=model.fit()\nresult.summary()","1384c628":"# Checking the coeffecients of the features\nexp_cof=np.exp(result.params)\nexp_cof","b5852ca5":"## Age\n## 1.Positive sign of age co efficient indicate that, probability of CHD increases with age\n## 2.When age increase by 1 yr, log(odds) of CHD increase by 0.0646\n## 3.When age increase by 1 yr, odds of CHD increase by 6%(So 1.066-1)","ea7b8241":"## Male\n# 1.Positive sign of male co efficient indicate that, probability of CHD in male is high.\n# 2.log(odds) of CHD for male is higher by 0.49 compared to female\n# 3.odds(male)\/odds(female)=1.63, odds(male) is 63% higher compared to odds(female)\n","d7271de2":"## Assiging the threshold to determine the prediction from probability","4c17e28e":"prob=result.predict(Xc[cols])\nprob.name='prob'\ndf_pred=pd.DataFrame([prob,y]).T\ndf_pred['pred']=df_pred['prob'].apply(lambda x:0 if x<0.5 else 1)\ndf_pred","bd8a8774":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score","8d369f9a":"confusion_matrix(df_pred.TenYearCHD,df_pred.pred)","f20399b7":"accuracy_score(df_pred['TenYearCHD'],df_pred['pred'])","d654ffda":"print(classification_report(df_pred['TenYearCHD'],df_pred['pred']))","2443876e":"from sklearn.metrics import roc_auc_score , roc_curve","20f0af2d":"print('AUC for model:',roc_auc_score(df_pred['TenYearCHD'],df_pred['prob']))\nprint('ROC for model:',roc_curve(df_pred['TenYearCHD'],df_pred['prob']))","72541407":"fpr,tpr,threshold=roc_curve(df_pred['TenYearCHD'],df_pred['prob'])","90add667":"plt.figure(figsize=(10,5))\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.xlabel('fpr')\nplt.ylabel('tpr')","ebe27680":"threshold[0]=threshold[0]-1","6fdfafb0":"plt.figure(figsize=(10,5))\nplt.plot(fpr,tpr)\nplt.plot(fpr,fpr,'r-')\nplt.plot(fpr,threshold,'g-')\nplt.xlabel('fpr')\nplt.ylabel('tpr')","568c62a4":"# Model is not performing its best in estimation, there is a large trade of bias and variance in the  model","99df662e":"# Declare x,y and split using X_train and y_train\nX= df.drop(['TenYearCHD'],axis='columns')\ny= df.TenYearCHD\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=1)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8cb65417":"# Determine the logisticRegression to validate x_test and y_test\n# Logistic Regression is maximum likehood model-- iteration till the maximum\n# Using solver we can stimulate and converge at a faster rate\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(fit_intercept=True,solver='liblinear')","9b19867a":"lr.fit(X_train, y_train)  \n# In stats we provide y,x while machine learning we provide X,y\n# Check for any warning if its there again you need to do that","656bff11":"# Determine the prediction and Probability\ny_train_prob = lr.predict_proba(X_train)[:,1]\ny_train_pred = lr.predict(X_train)\ny_train_prob  # output- Probability for 0 and 1 i.e ( P,(1-P)) # thats the reason we slice and take the value for 1 alone","412c3f9f":"# Evaluating on the training data\n# Determine the confusion matrix ,accuracy_score,roc_curve,roc_auc_score,classification_report\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score,classification_report\nprint(\"confusion_matrix \\n\",confusion_matrix(y_train,y_train_pred))         # Evaluating the data on the trained data set-->Train and Predicted\nprint(\"accuracy_score\",accuracy_score(y_train,y_train_pred))           # Train and Predicted\nprint(\"roc_accuracy acore\",roc_auc_score(y_train,y_train_prob))                 # Train and Probability\nprint(\"classification_report \\n \",classification_report(y_train,y_train_pred))   # Train and Predicted\nfpr, tpr, thresholds =roc_curve(y_train,y_train_prob)  # Train and Probability-- Plotting\n\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, \"r-\")\nplt.show()","d8ba36dc":"# Validating the data on the test data set\ny_test_prob = lr.predict_proba(X_test)[:,1]\ny_test_pred = lr.predict(X_test)\nprint(\"confusion_matrix \\n\",confusion_matrix(y_test,y_test_pred))         # Validating the data on the trained data set-->Train and Predicted\nprint(\"accuracy_score\",accuracy_score(y_test,y_test_pred))           # Test and Predicted\nprint(\"roc_accuracy acore\",roc_auc_score(y_test,y_test_prob))                 # Test and Probability\nprint(\"classification_report \\n \",classification_report(y_train,y_train_pred))   # Test and Predicted\nfpr,tpr,thresholds = roc_curve(y_test,y_test_prob)  # Test and Probability-- Plotting\nplt.plot(fpr, tpr)\nplt.plot(fpr, fpr, \"r-\")\nplt.show()","10aefcc1":"# Model is Performing good in both Test and Training data Set","c2af717a":"# IF we want to change the thresold to estimate the predicts( threshold= 0.25)\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, roc_auc_score, precision_score\n\nX, y = make_classification(\n    n_classes=2, class_sep=1.5, weights=[0.9, 0.1],\n    n_features=20, n_samples=1000, random_state=10\n)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclf = LogisticRegression(class_weight=\"balanced\")\nclf.fit(X_train, y_train)\nTHRESHOLD = 0.25\npreds = np.where(clf.predict_proba(X_test)[:,1] > THRESHOLD, 1, 0)\n\npd.DataFrame(data=[accuracy_score(y_test, preds), recall_score(y_test, preds),\n                   precision_score(y_test, preds), roc_auc_score(y_test, preds)], \n             index=[\"accuracy\", \"recall\", \"precision\", \"roc_auc_score\"])\n","fcf5d9b2":"## Statistical Model, Machine Learning Model both performs with a normal accuracy of 85% \n## If the threshold value is been reduced to 25% rather than 50% the accuarcy value increases with the sacrifice of precision","d2f03402":"### Analysis of the target Varaible","a28056a1":"## roc analisys","ef757e34":"## VIF tabel shows there is few serious multicollonearity\n## backward elimination to drop varables one by one\n","9703a648":"## Logistic Regression - Statistical and Machine Learning Model.\n\nObjective of buidling the model is to estimate the prediction of affected by the CHD- Heart Disease.\n\n## Steps of this notebook\n\nThis notebook has the following useful features\n\n* Checking outliers,distributions\n* Data Cleaning\n* Feature Selection- Backward Elimination\n* Statistical Model with classification report,ROC analysis\n* Machine Learning LogisticRegression\n","0aa6f3a5":"## Machine Learning ","f9a3b06e":"## Performing Statistical Logit Model"}}