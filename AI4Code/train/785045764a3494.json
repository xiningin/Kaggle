{"cell_type":{"20ac9277":"code","bfcb040a":"code","b91f330d":"code","b9c92209":"code","9839d8c0":"code","fcf894c6":"code","9b0c3f8a":"code","5e1bb9bf":"code","ae7f3e48":"code","9e35a2c1":"code","8aa186df":"code","ed3f988c":"code","f3fb9ce8":"code","95326fdd":"code","c0593773":"code","07d6309e":"code","a13d91d6":"code","b0553fa2":"code","39d7c098":"code","a7730f65":"code","e6e33ce7":"code","d1e2f859":"code","dfa464ae":"code","6b1982f9":"code","3cd56feb":"code","0e113006":"code","525b6f23":"code","ed3f439f":"code","fbf512e3":"code","40ad7641":"code","19d59447":"code","f2ced12f":"code","3505d414":"code","b62bc83e":"code","e88ba5e2":"code","248e4b97":"code","5778e004":"code","f71df1f9":"code","0a0f553e":"code","7763d2d6":"code","c399f9a2":"code","4aa73513":"code","4be7e550":"code","cd2b8639":"code","396341a5":"code","4c3f06e0":"code","abd6e46b":"code","04aee832":"code","9518fa90":"code","37e23a87":"code","b7bac900":"code","78f83f7f":"markdown","9c8b4492":"markdown","a98587ce":"markdown","0228ae5f":"markdown","77b74e19":"markdown","6e9fccaf":"markdown","0b8c3691":"markdown","0f4d3405":"markdown","664075fb":"markdown","9e3d494b":"markdown","39e6f7aa":"markdown","521cbb89":"markdown"},"source":{"20ac9277":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, plot_confusion_matrix\n","bfcb040a":"!pip install openpyxl","b91f330d":"  df=pd.read_excel('..\/input\/real-estate-valuation-using-random-forest\/Real estate valuation data set.xlsx')","b9c92209":"df","9839d8c0":"df.head()","fcf894c6":"#getting the names of columns\ndf.columns","9b0c3f8a":"df.shape","5e1bb9bf":"df.skew()","ae7f3e48":" df.corr()","9e35a2c1":"#Global declartions of function names\nglobal Head\nglobal Size\nglobal Column_names\nglobal Describe\nglobal Shape\nglobal Count\nglobal Value_count\nglobal ISNULL\nglobal Tail\nglobal Ndim\nglobal Nunique\nglobal Memory_usage\nglobal Duplicated\nglobal ISNA\nglobal DTYPES\nglobal CORR\nglobal Info\nglobal operations\n        \n\n        ","8aa186df":" def Head(value=5):\n            print('\\033[1m'+'displaying the', value, 'rows'+'\\033[0m')\n            a=df.head(value)\n            return a\n            print(\"--------------------------------------------------------------------------\")\nHead()","ed3f988c":" def Tail():\n    print('\\033[1m'+\"The last five rows of the dataframe are\"+'\\033[0m')\n    co3=df.tail()\n    return(co3)\n    print(\"--------------------------------------------------------------------------\")\nTail()","f3fb9ce8":"def Describe():\n    print('\\033[1m'+\"The Description of our dataset is:\"+'\\033[0m')\n    des=df.describe()\n    return(des)\n    print(\"--------------------------------------------------------------------------\")\nDescribe()","95326fdd":"def Size():\n    print('\\033[1m'+\"The size of dataset is :\"+'\\033[0m')\n    siz=df.size\n    print(siz,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nSize()","c0593773":"def Count():\n    print('\\033[1m'+\"The count of non null values are:\"+'\\033[0m')\n    co=df.count()\n    print(co,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nCount()\n","07d6309e":"def ISNULL():\n    print('\\033[1m'+\"Detection of missing values\"+'\\033[0m')\n    co2=df.isnull().sum()\n    print(co2,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nISNULL()","a13d91d6":"def Ndim():\n    print('\\033[1m'+\"The dimensions of data set are:\"+'\\033[0m')\n    co4=df.ndim\n    print(co4,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNdim()","b0553fa2":"def Nunique():\n    print('\\033[1m'+\"Total number of unique values are:\"+'\\033[0m')\n    co5=df.nunique()\n    print(co5,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nNunique()","39d7c098":"def Memory_usage():\n    print('\\033[1m'+\"The total memory used is :\"+'\\033[0m')\n    co6=df.memory_usage()\n    print(co6,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nMemory_usage()","a7730f65":"def Duplicated():\n    print('\\033[1m'+\"Total number of duplicate rows\"+'\\033[0m')\n    co7=df.duplicated().count()\n    return(co7)\n    print(\"--------------------------------------------------------------------------\")\nDuplicated()","e6e33ce7":"def DTYPES():\n    print('\\033[1m'+\"The datatypes are :\"+'\\033[0m')\n    co9=df.dtypes\n    print(co9,'\\n')\n    print(\"--------------------------------------------------------------------------\")\nDTYPES()","d1e2f859":"def Info():\n    print('\\033[1m'+\"The info of data set is :\"+'\\033[0m')\n    co11=df.info()\n    print(\"--------------------------------------------------------------------------\")\nInfo()","dfa464ae":"def operations(df,x):\n    if df[x].dtype==\"float64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is :\\n\",df[x].mean())\n        print(\"The median is :\\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 :\\n \",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n \",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n\n            print(\"--------------------------------------------------------------------------\")\n\n\n    elif df[x].dtype==\"int64\":\n        print('\\033[1m'+'', x, 'rows'+'\\033[0m')\n        print('\\033[1m'+\"It is a quantitaive data \\n\"+'\\033[0m')\n        print(\"The mean is : \\n\",df[x].mean())\n        print(\"The median is : \\n\",df[x].median())\n        print(\"The Standard Deviation is \\n\",df[x].std())\n        q1=df[x].quantile(0.25)\n        q2=df[x].quantile(0.5)\n        q3=df[x].quantile(0.75)\n        IQR=q3-q1\n        LLP=q1-1.5*IQR\n        ULP=q3+1.5*IQR\n        print(\"The quartiles are q1 : \\n\",q1)\n        print(\"The quartiles are q2 : \\n\",q2)\n        print(\"The quartiles are q3 : \\n\",q3)\n        print(\"The Uppler limit point of the data is \\n\",ULP)\n        print(\"The lower limit point of the data is \\n\",LLP)\n        if df[x].min()>LLP and df[x].max()<ULP:\n            print(\"The outliers are not present \\n\")\n\n            print(\"--------------------------------------------------------------------------\")\n\n        else:\n\n            print(\"The outliers are present \\n\")\n            print(\"The outliers are :\")\n            print(df[df[x].values>ULP][x])\n            print(df[df[x].values<LLP][x])\n            print(\"--------------------------------------------------------------------------\")\n\n\n\n\n\n\n\n    else:\n\n        print('\\033[1m'+\"The data is Qualitative \\n\"+'\\033[0m')\n\n\n        if df[x].nunique()==1:\n            print('\\033[1m'+\"The data is singular \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()==2:\n            print('\\033[1m'+\"The data is Binary \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n        elif df[x].nunique()>2:\n            print('\\033[1m'+\"The data is Multi \\n\"+'\\033[0m')\n            print(\"The mode is :\",df[x].mode())\n            print(\"The count of mode is \\n\",df[x].value_counts())\n\n        print(\"--------------------------------------------------------------------------\")\n\nc=df.columns\nfor i in c:\n    operations(df,i)\n    print(\"\\n\")\n\n\n","6b1982f9":"def Summary():\n        print('\\033[1m'+\"The Summary of data is  \\n\"+'\\033[0m')\n        print(\"The shape of the datset is :\",df.shape)\n        print(\"The sixe o the data set is :\",df.size)\n        print(\"The dimensions of the dataset are:\",df.ndim)\n        print(\"The memory usage of the data set are\",df.memory_usage())\n        print(\"The data types of the dataset are:\",df.dtypes)\n        print(\"--------------------------------------------------------------------------\")\n\nSummary()     ","3cd56feb":" def Column_Summary():\n        print('\\033[1m'+\"The Column wise Summary of data is  \\n\"+'\\033[0m')\n        k=df.columns\n        for i in k:\n            print('\\033[1m'+'', i, 'rows'+'\\033[0m')\n            print(\"The Shape of the column \",i,\"is \",df[i].shape)\n            print(\"The Size of the column \",i,\"is \",df[i].size)\n            print(\"The Dimensions of the column \",i,\"is \",df[i].ndim)\n            print(\"The Memory used by the column \",i,\"is \",df[i].memory_usage())\n            print(\"The Data types  of the column \",i,\"is \",df[i].dtypes)\n            print(\"--------------------------------------------------------------------------\")\nColumn_Summary()","0e113006":"df","525b6f23":"df.dtypes","ed3f439f":"a=df['X1 transaction date']","fbf512e3":"df['X1 transaction date'][0]","40ad7641":"for i in range(len(a)):\n    df['X1 transaction date'][i]=int(df['X1 transaction date'][i])\n    ","19d59447":"df","f2ced12f":"for i in df.columns:\n    sns.distplot(df[i])\n    plt.show()\n    \n","3505d414":"plt.figure(figsize=(10,16))\nax = sns.heatmap(df.corr(),annot = True, cmap = 'viridis')\nplt.show()\n","b62bc83e":"! pip install autoviz","e88ba5e2":"! pip install xlrd","248e4b97":"#importing Autoviz class\nfrom autoviz.AutoViz_Class import AutoViz_Class#Instantiate the AutoViz class\nAV = AutoViz_Class()","5778e004":"df1 = AV.AutoViz('..\/input\/real-estate-valuation-using-random-forest\/Real estate valuation data set.xlsx')","f71df1f9":"plt.figure(figsize=(15,10))\nplt.scatter(x=df['X3 distance to the nearest MRT station'], y=df['Y house price of unit area'])\nplt.xlabel('The distance to the nearest MRT station (unit: meter)')\nplt.ylabel('House price of unit area')\nplt.title('Real Estate Valuation')\nplt.show()","0a0f553e":"df.columns","7763d2d6":"X = np.array(df['X3 distance to the nearest MRT station']).reshape(-1,1)\ny = np.array(df['Y house price of unit area']).reshape(-1,1)","c399f9a2":"X[1]","4aa73513":"y[1]","4be7e550":"len(X), len(y)","cd2b8639":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=99)\n\ny_train = y_train.reshape(len(y_train),)\ny_valid = y_valid.reshape(len(y_valid),)\n\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","396341a5":"from sklearn.preprocessing import StandardScaler\n# Standardisation replaces the values by their Z scores.\n\n# Initialise the Scaler \nscaler = StandardScaler() \n  \n# To scale data \nX_train_Scaled = scaler.fit_transform(X_train) \n# y_train_Scaled = scaler.fit_transform(y_train) \n\nX_valid_Scaled = scaler.transform(X_valid) \n# y_valid_Scaled = scaler.transform(y_valid) ","4c3f06e0":"from sklearn.ensemble import RandomForestRegressor","abd6e46b":"regr = RandomForestRegressor(max_depth=2, random_state=0)","04aee832":"regr.fit(X_train, y_train)","9518fa90":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import mean\nfrom numpy import std","37e23a87":"cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(regr, X_valid, y_valid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\n# report performance\nprint('MAE: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","b7bac900":"\nregr.predict([[100]])","78f83f7f":"## Cross validation","9c8b4492":"From the above visualiztions we can say that there is only realtion between Distance to nearset MRT station and the target variable House price so therefore, our feature will be distance ","a98587ce":"## Scaling the data","0228ae5f":"# Exploratory Data Analysis","77b74e19":"# Summary of EDA","6e9fccaf":"## Prediction","0b8c3691":"## Splitting Data into training and validtaion Set ","0f4d3405":"## Data Modelling","664075fb":"# Relation Plots","9e3d494b":"# Data Modelling and Feature Selection","39e6f7aa":"# Data Visualization","521cbb89":"## From the above prediction we can say that my house price of unit area is 49.13"}}