{"cell_type":{"6e4d3749":"code","da8282fa":"code","b02a1783":"code","e9a9a39a":"code","317dcf4f":"code","084962aa":"code","2f8ddfe9":"code","799b3e88":"code","225f2308":"code","ad5d598f":"code","bfbfc3e2":"code","a29dfefd":"code","15172a0f":"code","c03a49f3":"code","0e60c159":"code","e6738825":"code","3d6cba2a":"code","a0938180":"code","eeac3bce":"code","d91e9544":"code","28123430":"markdown","7204f7b6":"markdown","a53e437f":"markdown","26bfb68a":"markdown","1d7ecf37":"markdown","7e21af50":"markdown","0ab94bfa":"markdown","ebbfcb6e":"markdown","22c9c8dc":"markdown","a1b7e18c":"markdown","eaaa1ebb":"markdown","37fa4ef6":"markdown","396fdfd0":"markdown","b9e0c9eb":"markdown"},"source":{"6e4d3749":"import numpy as np \nimport pandas as pd\n\nfrom sklearn.utils import resample\n\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.graph_objects as go\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score,f1_score","da8282fa":"import warnings\nwarnings.filterwarnings(\"ignore\")","b02a1783":"df = pd.read_csv(\"\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Train.csv\")\nprint(\" Dataset size : \", df.shape)","e9a9a39a":"print(\"=\"*15,\"Total no. of missing values in each column\",\"=\"*15)\nprint(df.isna().sum())","317dcf4f":"print(\"Frequency of churns and No churns : \\n\", df[\"labels\"].value_counts())","084962aa":"fig = go.Figure([go.Bar(x=df[\"labels\"].value_counts().index, y=df[\"labels\"].value_counts().values)])\nfig['layout'].update(title={\"text\" : 'Distribution of churn labels','y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'}, xaxis_title=\"label\",yaxis_title=\"count\")\nfig.update_layout(width=500,height=500)\nfig","2f8ddfe9":"X = df.drop(\"labels\", axis=1)\ny = df[[\"labels\"]]\n\nsfs = SFS(LogisticRegression(class_weight = \"balanced\"),\n           k_features=10,\n           forward=True,\n           floating=False,\n           scoring = 'f1',\n           cv = 0)\n\nsfs.fit(X,y)\nprint(\"Top 10 features selected using Forward Propagation\",sfs.k_feature_names_ )","799b3e88":"from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nimport matplotlib.pyplot as plt\nfig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev')\nplt.title('Sequential Forward Selection (w. StdErr)')\nplt.grid()\nplt.show()","225f2308":"train,val = train_test_split(df, test_size = 0.2, random_state = 42, stratify = df['labels'])\nX_train = train[['feature_0', 'feature_2', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_11', 'feature_12']]\ny_train = train[[\"labels\"]]\nX_val = val[['feature_0', 'feature_2', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_11', 'feature_12']]\ny_val = val[[\"labels\"]]","ad5d598f":"log_clf = LogisticRegression(class_weight = \"balanced\")\nlog_clf.fit(X_train, y_train)\ntrain_pred = log_clf.predict(X_train)\nprint(\"Training data accuracy : \", accuracy_score(train_pred,y_train))\nprint(\"Training data F1-score : \", f1_score(train_pred,y_train))\n\n\nval_pred = log_clf.predict(X_val)\nprint(\"Validation data accuracy : \", accuracy_score(val_pred,y_val))\nprint(\"Validation data F1-score : \", f1_score(val_pred,y_val))","bfbfc3e2":"c = [10 ** x for x in range(-5, 2)]\nf1_score_array=[]\nfor i in c:\n    clf = LogisticRegression(C =i, class_weight = 'balanced')\n    clf.fit(X_train, y_train)\n    predict_y = clf.predict(X_val)\n    f1_score_array.append(f1_score(y_val, predict_y))\n    print('For values of alpha = ', i, \"The F1 - score is:\",f1_score(y_val, predict_y))\n    \nprint(\"\\nThe maximum value of f1_score is {} for C = {}\".format(max(f1_score_array), c[f1_score_array.index(max(f1_score_array))]))","a29dfefd":"log_clf = LogisticRegression(C = 0.001,class_weight = \"balanced\")\nlog_clf.fit(X_train, y_train)\ntrain_pred = log_clf.predict(X_train)\nprint(\"Training data accuracy : \", accuracy_score(train_pred,y_train))\nprint(\"Training data F1-score : \", f1_score(train_pred,y_train))\n\n\nval_pred = log_clf.predict(X_val)\nprint(\"Validation data accuracy : \", accuracy_score(val_pred,y_val))\nprint(\"Validation data F1-score : \", f1_score(val_pred,y_val))","15172a0f":"submission = pd.read_csv(\"\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Test.csv\")\npredictions = log_clf.predict(submission[['feature_0', 'feature_1', 'feature_3', 'feature_4', 'feature_6', 'feature_9', 'feature_10', 'feature_11']])","c03a49f3":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init(max_mem_size='16G')","0e60c159":"# Data loading\ndf = pd.read_csv(\"\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/Train.csv\")\ndf[\"labels\"] = df[\"labels\"].map({0: \"No\", 1:\"Yes\"})\nX = h2o.H2OFrame(df)\nX.describe()","e6738825":"#Splitting the data\nsplits = X.split_frame(ratios=[0.8],seed=1)\ntrain = splits[0]\nval = splits[1]","3d6cba2a":"y = \"labels\"\nx_train = train.columns\nx_train.remove(y)","a0938180":"# Fitting the model\naml = H2OAutoML(max_runtime_secs=300, seed=1,keep_cross_validation_predictions = True,balance_classes = True, max_after_balance_size = 7934)\naml.train(x = x_train, y = y, training_frame = train)","eeac3bce":"lb = aml.leaderboard\nlb","d91e9544":"# making the predictions on validation data \npred = aml.predict(val.drop(\"labels\",axis=1))\n\n#computing accuracy\nprint(\"Accuracy on validation data = \",accuracy_score(pred.as_data_frame()[\"predict\"], val[\"labels\"].as_data_frame()[\"labels\"]))\n\n#computing f1-score \nprint(\"F1- score = \",f1_score(pred.as_data_frame()[\"predict\"], val[\"labels\"].as_data_frame()[\"labels\"],pos_label = \"Yes\" ))","28123430":"### 2. Loading Data","7204f7b6":"#### 3.c Handling class imbalance\n\nThere are multiple Sampling approaches to deal with class imbalance problem. Most commonly preferred are \n1. Under(down) Sampling\n2. Over(up) Sampling\n3. SMOTE\n\nAlternatively, we can also use the algorithms(like Logistic Regression, SVM etc.) in a balanced mode by balancing the class weights.","a53e437f":"#### 4.b Splitting the data in a 80:20 ratio","26bfb68a":"### 3. Data Preprocessing\n\n#### 3.a Checking for missing values ","1d7ecf37":"### 1. Importing necessary Libraries","7e21af50":"### 5. Fitting a Logistic Regression model","0ab94bfa":"#### 3.b Checking for Class Imbalance","ebbfcb6e":"#### 5.b predictions against test data","22c9c8dc":"### Table of Contents\n\n1. [**Importing necessary Libraries**](# 1.-Importing-necessary-Libraries)   \n2. [**Loading Data**](#2.-Loading-Data)  \n3. [**Data Preprocessing**](#3.-Data-Preprocessing)  \n    3.a [**Checking for missing values**](#3.a-Checking-for-missing-values)  \n    3.b [**Checking for Class Imbalance**](#3.b-Checking-for-Class-Imbalance)  \n    3.c [**Handling class imbalance**](#3.c-Handling-class-imbalance)  \n4. [**Feature Selection**](#4.-Feature-Selection)  \n    4.a [**Forward Propagation**](#4.a-Forward-Propagation)  \n    4.b [**Splitting the data in a 80:20 ratio**](#4.b-Splitting-the-data-in-a-80:20-ratio)  \n5. [**Fitting a Logistic Regression model**](#5.-Fitting-a-Logistic-Regression-model)  \n    5.a [**Hyper-parameter tuning**](#5.a-Hyper-parameter-tuning)  \n    5.b [**predictions against test data**](#5.b-predictions-against-test-data)  \n6. [**Using AutoML**](#6.-Using-AutoML)  ","a1b7e18c":"#### 5.a Hyper-parameter tuning","eaaa1ebb":"### 4. Feature Selection\n\n#### 4.a Forward Propagation","37fa4ef6":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","396fdfd0":"Above plot indicates that the performance measure(F1-score here) becomes stationary after the use of first 8 features. So let's use these 8 features in model training.","b9e0c9eb":"### 6. Using AutoML "}}