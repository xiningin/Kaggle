{"cell_type":{"63f28693":"code","861a75cd":"code","cf74c7f6":"code","3519fe5e":"code","30a2c3b7":"code","aae88ef6":"code","cabcd149":"code","481caa85":"code","9f223186":"code","0001c0be":"code","44a93421":"code","f7165791":"code","82130357":"code","a5321047":"code","9914d401":"code","68e767fe":"code","ed054476":"code","087e0e7c":"code","5c2cf3e1":"code","bbdf70ee":"code","0fab3e66":"code","773c257a":"code","72a58cf3":"code","eb6eda3e":"code","1fe1cd9e":"code","55592aec":"code","a599e19c":"code","9d5292ed":"code","99f790e2":"code","fd6f9291":"code","e0ed1fd0":"code","388c7339":"code","ad91dc0d":"code","80d48dd7":"code","7ba9bf6b":"code","fa9a65f1":"markdown","5af02d4c":"markdown","ba207722":"markdown","0743d448":"markdown","bfa91a37":"markdown","150c3b4f":"markdown","a38d6f54":"markdown","4bcfb9fb":"markdown","6acac800":"markdown","c12c5a49":"markdown","de7feab2":"markdown","9b4db7a3":"markdown","72518c9b":"markdown"},"source":{"63f28693":"import pandas as pd\nimport numpy as np\nimport json\nimport os\nfrom multiprocessing import Pool\nfrom tqdm.notebook import tqdm\nimport gc\nimport pickle\nimport joblib\nimport cv2\nimport bz2\nfrom PIL import Image\nimport matplotlib.pyplot as plt","861a75cd":"REDUCE_MEM = True\nMODEL_FILE_DIR = '..\/input\/imaterialist2020-pretrain-models\/'\nattr_image_size = (160,160)","cf74c7f6":"to_training = not os.path.isfile(MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0])","3519fe5e":"train_df = pd.read_csv(\"..\/input\/imaterialist-fashion-2020-fgvc7\/train.csv\")","30a2c3b7":"def rle_to_mask(rle_string,height,width):\n    rows, cols = height, width\n    if rle_string == -1:\n        return np.zeros((height, width))\n    else:\n        rleNumbers = [int(numstring) for numstring in rle_string.split(' ')]\n        rlePairs = np.array(rleNumbers).reshape(-1,2)\n        img = np.zeros(rows*cols,dtype=np.uint8)\n        for index,length in rlePairs:\n            index -= 1\n            img[index:index+length] = 255\n        img = img.reshape(cols,rows)\n        img = img.T\n        return img\n\ndef mask_to_rle(mask):\n    pixels = mask.T.flatten()\n    # We need to allow for cases where there is a '1' at either end of the sequence.\n    # We do this by padding with a zero at each end when needed.\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return ' '.join(str(x) for x in rle)","aae88ef6":"max_clz = train_df.ClassId.max()","cabcd149":"max_attr = 0\nfor i in train_df.AttributesIds:\n    for a in str(i).split(','):\n        if a!='nan':\n            a = int(a)\n            if a > max_attr:\n                max_attr = a","481caa85":"clz_attr = np.zeros((max_clz+1,max_attr+1))\nclz_attrid2idx = [[] for _ in range(max_clz+1)]\nclz_attr.shape","9f223186":"for c,i in zip(train_df.ClassId,train_df.AttributesIds):\n    for a in str(i).split(','):\n        if a!='nan':\n            a = int(a)\n            clz_attr[c,a] = 1\n            if not a in clz_attrid2idx[c]:\n                clz_attrid2idx[c].append(a)","0001c0be":"clz_attr_num = clz_attr.sum(axis=1).astype(np.int64)\nclz_attr_num","44a93421":"train_df.head()","f7165791":"def ptoz(obj):\n    return bz2.compress(pickle.dumps(obj), 3) if REDUCE_MEM else obj\ndef ztop(b):\n    return pickle.loads(bz2.decompress(b)) if REDUCE_MEM else b\ndef __getitem__(imgid):\n    df = train_df[train_df.ImageId==imgid]\n    res = []\n    imag = cv2.imread(\"..\/input\/imaterialist-fashion-2020-fgvc7\/train\/\"+str(imgid)+\".jpg\")\n    for idx in range(len(df)):\n        t = df.values[idx]\n        cid = t[4]\n        mask = rle_to_mask(t[1],t[2],t[3])\n        attr = map(int,str(t[5]).split(\",\")) if str(t[5]) != 'nan' else []\n        where = np.where(mask != 0)\n        y1,y2,x1,x2 = 0,0,0,0\n        if len(where[0]) > 0 and len(where[1]) > 0:\n            y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n        if y2>y1+10 and x2>x1+10:\n            X = cv2.resize(imag[y1:y2,x1:x2], attr_image_size)\n            X = ptoz(X)\n        else:\n            X = None\n        mask = cv2.resize(mask, attr_image_size)\n        mask = ptoz(mask)\n        res.append((cid, mask, attr, X))\n    imag = cv2.resize(imag, attr_image_size)\n    imag = ptoz(imag)\n    return res, imag, imgid","82130357":"if to_training:\n    if os.path.isfile(MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0]):\n        data_cache = joblib.load(MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\n    elif REDUCE_MEM:\n        data_cache = []\n        for i in tqdm(list(set(train_df.ImageId))):\n            res, imag, imgid = __getitem__(i)\n            for cid, mask, attr, X in res:\n                data_cache.append((cid, mask, attr, imag, X, imgid))\n        joblib.dump(data_cache, MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\n    else:\n        with Pool(8) as p:\n            tmp = p.map(__getitem__, list(set(train_df.ImageId)))\n        data_cache = []\n        for res, imag, imgid in tmp:\n            for cid, mask, attr, X in res:\n                data_cache.append((cid, mask, attr, imag, X, imgid))\n        del tmp\n        joblib.dump(data_cache, MODEL_FILE_DIR+\"data_cache_%d\"%attr_image_size[0])\nelse:\n    data_cache = []","a5321047":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n@torch.jit.script\ndef mish(input):\n    return input * torch.tanh(F.softplus(input))\n\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input):\n        return mish(input)\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=1,stride=1,padding=0,dilation=1,bias=False):\n        super(SeparableConv2d,self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels,in_channels,kernel_size,stride,padding,dilation,groups=in_channels,bias=bias)\n        self.pointwise = nn.Conv2d(in_channels,out_channels,1,1,0,1,1,bias=bias)\n\n    def forward(self,x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\nclass Block(nn.Module):\n    def __init__(self,in_filters,out_filters,reps,strides=1,activation=None):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides!=1:\n            self.skip = nn.Conv2d(in_filters,out_filters,1,stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip=None\n\n        act = nn.ReLU() if activation is None else activation\n        rep=[]\n\n        rep.append(act)\n        rep.append(SeparableConv2d(in_filters,out_filters,3,stride=1,padding=1,bias=False))\n        rep.append(nn.BatchNorm2d(out_filters))\n        filters = out_filters\n\n        for i in range(reps-1):\n            rep.append(act)\n            rep.append(SeparableConv2d(filters,filters,3,stride=1,padding=1,bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3,strides,1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self,inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n        return x\n\nclass AttrXception(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(AttrXception, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 64, 3, 2, 1, bias=True)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.mish = Mish()\n\n        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1, bias=True)\n        self.bn2 = nn.BatchNorm2d(128)\n\n        self.block1 = Block(128,256,2,2)\n        self.block2 = Block(256,256,3,1)\n        self.block3 = Block(256,256,3,1)\n        self.block4 = Block(256,256,3,1)\n        self.block5 = Block(256,256,3,1)\n        self.block6 = Block(256,256,3,1)\n        self.block7 = Block(256,384,2,2)\n\n        self.conv3 = SeparableConv2d(384,512,3,stride=1,padding=0,bias=True)\n        self.fc = nn.Linear(512, num_classes)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.mish(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n\n        x = self.mish(x)\n        x = self.conv3(x)\n\n        x = self.mish(x)\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        x = x.view(x.size(0), -1)\n        result = self.fc(x)\n        \n        return torch.sigmoid(result)\n\nclass HourglassNet(nn.Module):\n    def __init__(self, depth, channel):\n        super(HourglassNet, self).__init__()\n        self.depth = depth\n        hg = []\n        for _ in range(self.depth):\n            hg.append([\n                Block(channel,channel,3,1,activation=Mish()),\n                Block(channel,channel,2,2,activation=Mish()),\n                Block(channel,channel,3,1,activation=Mish())\n            ])\n        hg[0].append(Block(channel,channel,3,1,activation=Mish()))\n        hg = [nn.ModuleList(h) for h in hg]\n        self.hg = nn.ModuleList(hg)\n\n    def _hour_glass_forward(self, n, x):\n        up1 = self.hg[n-1][0](x)\n        low1 = self.hg[n-1][1](up1)\n\n        if n > 1:\n            low2 = self._hour_glass_forward(n-1, low1)\n        else:\n            low2 = self.hg[n-1][3](low1)\n\n        low3 = self.hg[n-1][2](low2)\n        up2 = F.interpolate(low3, scale_factor=2)\n        out = up1 + up2\n        return out\n\n    def forward(self, x):\n        return self._hour_glass_forward(self.depth, x)\n\nclass XceptionHourglass(nn.Module):\n    def __init__(self, num_classes):\n        super(XceptionHourglass, self).__init__()\n        self.num_classes = num_classes\n\n        self.conv1 = nn.Conv2d(3, 128, 3, 2, 1, bias=True)\n        self.bn1 = nn.BatchNorm2d(128)\n        self.mish = Mish()\n\n        self.conv2 = nn.Conv2d(128, 256, 3, 1, 1, bias=True)\n        self.bn2 = nn.BatchNorm2d(256)\n\n        self.block1 = HourglassNet(4, 256)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.block2 = HourglassNet(4, 256)\n\n        self.sigmoid = nn.Sigmoid()\n\n        self.conv3 = nn.Conv2d(256, num_classes, 1, bias=True)\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.mish(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.mish(x)\n\n        out1 = self.block1(x)\n        x = self.bn3(out1)\n        x = self.mish(x)\n        out2 = self.block2(x)\n\n        r = self.sigmoid(out1 + out2)\n        r = F.interpolate(r, scale_factor=2)\n        \n        return self.conv3(r)","9914d401":"class AttrDataset(object):\n    def __init__(self, chaches, clzid):\n        self.clzid = clzid\n        self.chaches = [cd for cd in chaches if cd[0]==clzid]\n\n    def __getitem__(self, idx):\n        cid, mask, attr, imag, X, imgid = self.chaches[idx]\n        mask = ztop(mask)\n        imag = ztop(imag)\n        if X is None:\n            X = imag\n        else:\n            X = ztop(X)\n        y = np.zeros(clz_attr_num[self.clzid])\n        for a in attr:\n            y[clz_attrid2idx[self.clzid].index(a)] = 1\n        return X.transpose((2,0,1)).astype(np.float32), y.astype(np.float32)\n        \n    def __len__(self):\n        return len(self.chaches)\n\ndef train_attr_net(clzid, num_epochs=1):\n    data = AttrDataset(data_cache, clzid)\n    data_loader = torch.utils.data.DataLoader(\n        data, batch_size=64, shuffle=True, num_workers=1)\n\n    model = AttrXception(clz_attr_num[clzid])\n    model.cuda()\n    dp = torch.nn.DataParallel(model)\n    loss = nn.BCELoss()\n\n    params = [p for p in dp.parameters() if p.requires_grad]\n    optimizer = torch.optim.RMSprop(params, lr=2.5e-4,  momentum=0.9)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=6,\n                                                   gamma=0.9)\n    \n    prog = tqdm(list(range(num_epochs)))\n    for epoch in prog:\n        for i, (X, y) in enumerate(data_loader):\n            X = X.cuda()\n            y = y.cuda()\n            xx = dp(X)\n\n            losses = loss(xx, y)\n\n            prog.set_description(\"loss:%05f\"%losses)\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n        X, xx, y, losses = None, None, None, None\n        torch.cuda.empty_cache()\n        gc.collect()\n    return model","68e767fe":"for clzid in range(len(clz_attr_num)):\n    if clz_attr_num[clzid] > 0:\n        if not os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n            model = train_attr_net(clzid, 32)\n            torch.save(model.state_dict(), MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid))","ed054476":"model = None\ntorch.cuda.empty_cache()\ngc.collect()","087e0e7c":"data_mask = dict()\nwhile len(data_cache) > 0:\n    cid, mask, _, imag, _, imgid = data_cache.pop()\n    mask = ztop(mask)\n    if imgid not in data_mask:\n        imag = ztop(imag)\n        data_mask[imgid] = [ptoz(imag.transpose((2,0,1)).astype(np.float32)), np.zeros(attr_image_size, dtype=np.int)]\n    data_mask[imgid][1][mask!=0] = cid + 1","5c2cf3e1":"del data_cache","bbdf70ee":"for k in data_mask.keys():\n    data_mask[k][1] = ptoz(data_mask[k][1])\ngc.collect()","0fab3e66":"class MaskDataset(object):\n    def __init__(self, keys):\n        self.keys = keys\n\n    def __getitem__(self, idx):\n        k = self.keys[idx]\n        return ztop(data_mask[k][0]), ztop(data_mask[k][1])\n        \n    def __len__(self):\n        return len(self.keys)","773c257a":"def train_mask_net(num_epochs=1):\n    data = MaskDataset(list(data_mask.keys()))\n    data_loader = torch.utils.data.DataLoader(data, batch_size=8, shuffle=True, num_workers=4)\n\n    model = XceptionHourglass(max_clz+2)\n    model.cuda()\n    dp = torch.nn.DataParallel(model)\n    loss = nn.CrossEntropyLoss()\n\n    params = [p for p in dp.parameters() if p.requires_grad]\n    optimizer = torch.optim.RMSprop(params, lr=2.5e-4,  momentum=0.9)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                                   step_size=6,\n                                                   gamma=0.9)\n    for epoch in range(num_epochs):\n        total_loss = []\n        prog = tqdm(data_loader, total=len(data_loader))\n        for i, (imag, mask) in enumerate(prog):\n            X = imag.cuda()\n            y = mask.cuda()\n            xx = dp(X)\n            # to 1D-array\n            y = y.reshape((y.size(0),-1))  # batch, flatten-img\n            y = y.reshape((y.size(0) * y.size(1),))  # flatten-all\n            xx = xx.reshape((xx.size(0), xx.size(1), -1))  # batch, channel, flatten-img\n            xx = torch.transpose(xx, 2, 1)  # batch, flatten-img, channel\n            xx = xx.reshape((xx.size(0) * xx.size(1),-1))  # flatten-all, channel\n\n            losses = loss(xx, y)\n\n            prog.set_description(\"loss:%05f\"%losses)\n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            total_loss.append(losses.detach().cpu().numpy())\n\n        prog, X, xx, y, losses = None, None, None, None, None,\n        torch.cuda.empty_cache()\n        gc.collect()\n    return model","72a58cf3":"if to_training:\n    model = train_mask_net(64)\n    torch.save(model.state_dict(), MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0])","eb6eda3e":"del data_mask\ngc.collect()","1fe1cd9e":"class MaskDataset(object):\n    def __init__(self, folder):\n        self.imgids = [f.split(\".\")[0] for f in os.listdir(folder)]\n        self.folder = folder\n\n    def __getitem__(self, idx):\n        imag = cv2.imread(self.folder+self.imgids[idx]+\".jpg\")\n        imag = cv2.resize(imag, attr_image_size)\n        return imag.transpose((2,0,1)).astype(np.float32)\n        \n    def __len__(self):\n        return len(self.imgids)","55592aec":"model = XceptionHourglass(max_clz+2)\nmodel.cuda()\nmodel.load_state_dict(torch.load(MODEL_FILE_DIR+\"maskmodel_%d.model\"%attr_image_size[0]))\n\ndataset = MaskDataset(\"..\/input\/imaterialist-fashion-2020-fgvc7\/test\/\")\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=8, shuffle=False, num_workers=4)\n\npredict_imgeid = []\npredict_mask = []\npredict_rle = []\npredict_classid = []\npredict_attr = []\n\nmodel.eval()\nprog = tqdm(data_loader, total=len(data_loader))\nnum_pred = 0\nfor X in prog:\n    X = X.cuda()\n    pred = model(X).detach().cpu().numpy()\n    for i, mask in enumerate(pred):\n        imgid = dataset.imgids[num_pred]\n        num_pred += 1\n        pred_id = mask.argmax(axis=0) - 1  # -1 is background.\n        for clz in set(pred_id.reshape((-1,)).tolist()):\n            if clz >= 0:\n                maskdata = (pred_id == clz).astype(np.uint8) * 255\n                predict_imgeid.append(imgid)\n                predict_mask.append(maskdata)\n                predict_rle.append(\"\")\n                predict_classid.append(clz)\n                predict_attr.append([])\n\nprog, X, pred, dataset, data_loader = None, None, None, None, None\ntorch.cuda.empty_cache()\ngc.collect()","a599e19c":"import math\ndef _scale_image(img, long_size):\n    if img.shape[0] < img.shape[1]:\n        scale = img.shape[1] \/ long_size\n        size = (long_size, math.floor(img.shape[0] \/ scale))\n    else:\n        scale = img.shape[0] \/ long_size\n        size = (math.floor(img.shape[1] \/ scale), long_size)\n    return cv2.resize(img, size, interpolation=cv2.INTER_NEAREST)","9d5292ed":"for clzid in range(len(clz_attr_num)):\n    if clz_attr_num[clzid] > 0 and os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n        model = AttrXception(clz_attr_num[clzid])\n        model.cuda()\n        model.eval()\n        model.load_state_dict(torch.load(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)))\n        for i in range(len(predict_classid)):\n            if predict_classid[i] == clzid:\n                imag = cv2.imread(\"..\/input\/imaterialist-fashion-2020-fgvc7\/test\/\"+predict_imgeid[i]+\".jpg\")\n                imag = _scale_image(imag, 1024)\n                mask = cv2.resize(predict_mask[i], (imag.shape[1],imag.shape[0]), interpolation=cv2.INTER_NEAREST)\n                where = np.where(mask!=0)\n                y1,y2,x1,x2 = 0,0,0,0\n                if len(where[0]) > 0 and len(where[1]) > 0:\n                    y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n                    if y2>y1+80 and x2>x1+80 and np.sum(mask)\/255 > 1000:\n                        print(\"class id=\",clzid)\n                        plt.subplot(1,2,1)\n                        plt.imshow(imag)\n                        plt.subplot(1,2,2)\n                        plt.imshow(mask)\n                        plt.show()\n                        break","99f790e2":"uses_index = []\nfor clzid in tqdm(range(len(clz_attr_num))):\n    if clz_attr_num[clzid] > 0 and os.path.isfile(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n        model = AttrXception(clz_attr_num[clzid])\n        model.cuda()\n        model.eval()\n        model.load_state_dict(torch.load(MODEL_FILE_DIR+\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)))\n        for i in range(len(predict_classid)):\n            if predict_classid[i] == clzid:\n                imag = cv2.imread(\"..\/input\/imaterialist-fashion-2020-fgvc7\/test\/\"+predict_imgeid[i]+\".jpg\")\n                imag = _scale_image(imag, 1024)\n                mask = cv2.resize(predict_mask[i], (imag.shape[1],imag.shape[0]), interpolation=cv2.INTER_NEAREST)\n                #imag[mask==0] = 255\n                where = np.where(mask!=0)\n                y1,y2,x1,x2 = 0,0,0,0\n                if len(where[0]) > 0 and len(where[1]) > 0:\n                    y1,y2,x1,x2 = min(where[0]),max(where[0]),min(where[1]),max(where[1])\n                    if y2>y1+80 and x2>x1+80 and np.sum(mask)\/255 > 1000:\n                        predict_rle[i] = mask_to_rle(mask)\n                        X = cv2.resize(imag[y1:y2,x1:x2], attr_image_size).transpose((2,0,1))\n                        attr_preds = model(torch.tensor([X], dtype=torch.float32).cuda())\n                        attr_preds = attr_preds.detach().cpu().numpy()[0]\n                        for ci in range(len(attr_preds)):\n                            if attr_preds[ci] > 0.5:\n                                uses_index.append(i)\n                                predict_attr[i].append(clz_attrid2idx[predict_classid[i]][ci])","fd6f9291":"predict_attri_str = [\",\".join(list(map(str,predict_attr[i]))) for i in range(len(predict_classid))]","e0ed1fd0":"predict_imgeid = [predict_imgeid[i] for i in set(uses_index)]\npredict_mask = [predict_mask[i] for i in set(uses_index)]\npredict_rle = [predict_rle[i] for i in set(uses_index)]\npredict_classid = [predict_classid[i] for i in set(uses_index)]\npredict_attr = [predict_attr[i] for i in set(uses_index)]\npredict_attri_str = [predict_attri_str[i] for i in set(uses_index)]","388c7339":"setidlist = set(predict_imgeid)\nfor i in os.listdir(\"..\/input\/imaterialist-fashion-2020-fgvc7\/test\/\"):\n    id = i.split('.')[0]\n    if not id in setidlist:\n        predict_imgeid.append(id)\n        predict_rle.append(\"1 1\")\n        predict_classid.append(0)\n        predict_attri_str.append(\"111,137\")","ad91dc0d":"pd.DataFrame({\n    \"ImageId\":predict_imgeid,\n    \"EncodedPixels\":predict_rle,\n    \"ClassId\":predict_classid,\n    \"AttributesIds\":predict_attri_str\n}).to_csv(\"submission.csv\", index=False)","80d48dd7":"\"\"\"\nfor clzid in range(len(clz_attr_num)):\n    if os.path.isfile(\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid)):\n        os.remove(\"attrmodel_%d-%d.model\"%(attr_image_size[0],clzid))\nos.remove( \"maskmodel_%d.model\"%attr_image_size[0]))\n\"\"\"","7ba9bf6b":"!head submission.csv","fa9a65f1":"### Flag of whether or not there is an attribute ID for each class ID is a two-dimensional array","5af02d4c":"# After generating all mask images, cut out only the mask image part and put it in the saved model","ba207722":"### the trained model was saved to a file for later use","0743d448":"### If the threshold (0.5) is exceeded, it is assumed that the attribute ID is included","bfa91a37":"### Indicates the number of attribute IDs for each class ID","150c3b4f":"# Find the included attribute ID for each class ID","a38d6f54":"# Training Attribute Classification Models","4bcfb9fb":"# Import Packages","6acac800":"# Next, train the mask image","c12c5a49":"# Predict Mask Image","de7feab2":"# Read data and handle rle","9b4db7a3":"### You can see that some class IDs do not have attribute IDs associated with them","72518c9b":"# Make Submission File"}}