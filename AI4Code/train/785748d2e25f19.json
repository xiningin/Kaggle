{"cell_type":{"e5103921":"code","0182ac27":"code","d08fdf22":"code","6a2765cd":"code","975fb5b4":"code","f4467b22":"code","0355ee90":"code","f4684f71":"code","fb2487cc":"code","f9296290":"markdown","99ee1a25":"markdown"},"source":{"e5103921":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\n#\u00d6ncelikle data setini okuyoruz\niris = pd.read_csv(\"..\/input\/iris.csv\")\n#Veri setinin neye benzedi\u011fini g\u00f6rmek i\u00e7in ilk 5 sat\u0131r\u0131n\u0131 yazd\u0131r\u0131yoruz.\nprint(iris.head())\n\n#Daha sonra t\u00fcrleri string veriden say\u0131sal veriye \u00e7eviriyoruz.\niris.loc[iris['species']=='virginica','species']=2\niris.loc[iris['species']=='versicolor','species']=1\niris.loc[iris['species']=='setosa','species'] = 0\n\n#Giri\u015f ve \u00e7\u0131k\u0131\u015f kolonlar\u0131n\u0131 ay\u0131r\u0131yoruz.\nX = iris[['petal_length', 'petal_width','sepal_length','sepal_width']]\nY = iris[['species']]\n\nnum_training_instances = int(0.65 * iris.shape[0])\nnum_test_instances = int(0.85* iris.shape[0])\nindices = list(range(iris.shape[0]))\nnp.random.shuffle(indices)\nx_pd=pd.DataFrame(X)\ny_pd=pd.DataFrame(Y)\ntrain_indices = indices[:num_training_instances]\ntest_indices = indices[num_training_instances:num_test_instances]\nvalidation_indices = indices[num_test_instances:]\nx_train = x_pd.iloc[train_indices]\nx_test = x_pd.iloc[test_indices]\nx_validation = x_pd.iloc[validation_indices]\ny_train = y_pd.iloc[train_indices]\ny_test = y_pd.iloc[test_indices]\ny_validation = y_pd.iloc[validation_indices]\nx_train = x_train.T\nx_test = x_test.T\nx_validation = x_validation .T\ny_train = y_train.T\ny_test = y_test.T\ny_validation = y_validation .T\nx_train = x_train.values\nx_test = x_test.values\nx_validation = x_validation.values\ny_train = y_train.values\ny_train = y_train[0]\ny_test = y_test.values\ny_test = y_test[0]\ny_validation = y_validation.values\ny_validation = y_validation[0]\nprint(\"x_train shape: \", x_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"x_validation shape: \", x_validation.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)\nprint(\"y_validation shape \",y_validation.shape)\nprint(X.info())\nprint (x_pd.describe())","0182ac27":"#Veri setinin geneli ile ilgili bilgileri yazd\u0131r\u0131yoruz.\niris.info()","d08fdf22":"def initialize_parameters(n_x, n_h, n_y):\n    \n    np.random.seed(2) # we set up a seed so that our output matches ours although the initialization is random.\n    \n    W1 = np.random.randn(n_h, n_x) * 0.01 # 1. Katmandaki a\u011f\u0131rl\u0131k matrisi\n    b1 = np.zeros(shape=(n_h, 1))  # 1. katmandaki bias vekt\u00f6r\u00fc\n    W2 = np.random.randn(n_y, n_h) * 0.01   # 2. Katmandaki A\u011f\u0131rl\u0131k matrisi\n    b2 = np.zeros(shape=(n_y, 1))  # 2. Katmandaki bias vekt\u00f6r\u00fc\n       \n    #parametreleri dictionary format\u0131nda kaydediyoruz \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters\n\n# Katmanlardaki n\u00f6ron say\u0131lar\u0131 belirleniyor.\n\ndef layer_sizes(x_train, y_train):\n    n_x = x_train.shape[0] # giri\u015fteki de\u011fi\u015fken say\u0131s\u0131\n    n_h = 6 # gizli katmandaki n\u00f6ron say\u0131s\u0131\n    n_y = 3 # \u00e7\u0131k\u0131\u015f n\u00f6ron say\u0131s\u0131\n    return (n_x, n_h, n_y)","6a2765cd":"def forward_propagation(x_train, parameters):\n\n#parametreleri dictionaryden \u00e7ekiyoruz.     \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    \n    # Forward Propagation ile A2'yi hesapl\u0131yoruz\n    Z1 = np.dot(W1, x_train) + b1\n    A1 = np.tanh(Z1)  #tanh activation aktivasyon fonksiyonu\n    Z2 = np.dot(W2, A1) + b2\n    A2 = 1\/(1+np.exp(-Z2))  #sigmoid aktivasyon fonksiyonu\n    \n    #parametreleri dictionary format\u0131nda kaydediyoruz \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","975fb5b4":"def compute_cost(A2, y_train, parameters):\n   \n    m = y_train.shape # number of training examples\n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2), y_train) + np.multiply((1 - y_train), np.log(1 - A2))\n    cost = - np.sum(logprobs) \/ m\n    \n    return cost","f4467b22":"def backward_propagation(parameters, cache, x_train, y_train):\n# Number of training examples\n    m = x_train.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    W1 = parameters['W1']\n    W2 = parameters['W2']\n    \n        \n    # Retrieve A1 and A2 from dictionary \"cache\".\n    A1 = cache['A1']\n    A2 = cache['A2']\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    dZ2= A2 - y_train\n    dW2 = (1 \/ m) * np.dot(dZ2, A1.T)\n    db2 = (1 \/ m) * np.sum(dZ2, axis=1, keepdims=True)\n    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n    dW1 = (1 \/ m) * np.dot(dZ1, x_train.T)\n    db1 = (1 \/ m) * np.sum(dZ1, axis=1, keepdims=True)\n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","0355ee90":"def update_parameters(parameters, grads, learning_rate=1):\n# Retrieve each parameter from the dictionary \"parameters\"\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    dW1 = grads['dW1']\n    db1 = grads['db1']\n    dW2 = grads['dW2']\n    db2 = grads['db2']\n    \n    # Update rule for each parameter\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","f4684f71":"def nn_model(x_train, y_train, num_iterations=10000, print_cost=False):\n    \n    n_x,n_h,n_y=layer_sizes(x_train, y_train)\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    \n    # Loop (gradient descent\n\n    for i in range(0, num_iterations):\n         \n        # Forward propagation.\n        A2, cache = forward_propagation(x_train, parameters)\n        cost_list = []\n        cost_list2 = []\n        index = []\n        # Cost fonksiyonu\n        cost = compute_cost(A2, y_train, parameters)\n \n        # Backpropagation\n        grads = backward_propagation(parameters, cache, x_train, y_train)\n \n        # Gradient descent algoritmas\u0131 ile parametreleri g\u00fcncelliyoruz\n        parameters = update_parameters(parameters, grads)\n        plt.plot(index,cost_list2)\n        plt.xticks(index,rotation='vertical')\n        plt.xlabel(\"iteation\")\n        plt.ylabel(\"cost\")\n        plt.show() \n        \n        # Her 1000 iterasyonda cost u yazd\u0131r\u0131yoruz.\n        if print_cost and i % 1000 == 0:\n            print (\"Cost after iteration %i: %f\" % (i, cost))\n            #print (parameters)\n    return parameters,n_h","fb2487cc":"parameters = nn_model(x_train,y_train ,num_iterations=10000, print_cost=True)","f9296290":"Iris (zambak) \u00e7i\u00e7e\u011finin 3 farkl\u0131 t\u00fcr\u00fc mevcut.\n\n![irises.png](attachment:irises.png)","99ee1a25":"Bu \u00e7i\u00e7e\u011fin t\u00fcrlerinin birbirinden ayr\u0131lmas\u0131n\u0131 sa\u011flayan toplam 4 \u00f6zelli\u011fi mevcut.\n\n* Petal Length\n* Petal Width\n* Sepal Length\n* Sepal Width\n\n![iris_with_labels.jpg](attachment:iris_with_labels.jpg)"}}