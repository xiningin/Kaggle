{"cell_type":{"52117183":"code","cc23d547":"code","e63e3993":"code","5f4531ac":"code","ccbcde24":"code","1552d3f3":"code","422d01f8":"code","f3d586e3":"code","9068402b":"code","582ee858":"code","e4b4eda7":"code","082982e9":"code","fb8ec3b2":"code","96bdb9c9":"code","c959c3be":"code","dba167cc":"code","d4c9f33a":"code","dfe3ca29":"code","fe15b047":"code","5948a270":"code","31687962":"code","26103957":"code","6b1936ae":"code","8babe01e":"code","c1c0aa0e":"code","2395fe27":"code","744417c5":"code","cfb18798":"code","99569e85":"code","7a3ae445":"code","113d5cc5":"code","ac634042":"code","fae92d77":"code","19805496":"code","e67c0f4c":"code","21581809":"markdown","0c8151bb":"markdown","6c4d3303":"markdown","048b48a5":"markdown","31c58a20":"markdown","155adf7d":"markdown"},"source":{"52117183":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings","cc23d547":"warnings.filterwarnings(\"ignore\")","e63e3993":"data = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')","5f4531ac":"df = data.copy()","ccbcde24":"df.head()","1552d3f3":"df.info()","422d01f8":"# Checking if some duplicates exists\nduplicated = df.duplicated(subset=['gameId'])\nprint(duplicated.any() ==1)","f3d586e3":"# Bar plot of the outcomes to see how they are distributed\nlabels = ['Red Team Wins', 'Blue Team Wins']\nred_wins = len(df.blueWins[df['blueWins']==0])\nblue_wins = len(df.blueWins[df['blueWins']==1])\nwins = (red_wins,blue_wins)\nfig = plt.figure()\nbarPlot = plt.bar(labels,wins)\nplt.ylabel('Number of Wins')\nplt.title('Outcome Distribution')\nbarPlot[0].set_color('red')\nbarPlot[1].set_color('blue')\nplt.show()","9068402b":"# Correlation matrix with absolute vals\ncorrelation_mat = df.corr().abs()\n\n# Getting upper triangle of correlation matrix\nupper = correlation_mat.where(np.triu(np.ones(correlation_mat.shape), \n                                  k=1).astype(np.bool))\n\n\n# Getting features with correlation greater than 0.75\nto_drop = [column for column in upper.columns if any(\n        upper[column] > 0.75)]\n\n# Dropping those features \nX = df.drop(to_drop, axis=1)\n\n# Two columns are removed manually since they are some subsets of others\nX = X.drop([\"blueHeralds\"], axis = 1)\nX = X.drop([\"redHeralds\"], axis = 1)\n\n# Dropping the ID column\nX = X.drop([\"gameId\"],axis=1)\n\n# Plotting the new correlation matrix\ncorrelation_mat_X = X.corr().abs()\nplt.figure(figsize=(15,10))\nsns.heatmap(correlation_mat_X, annot = True, linewidths=.3, cmap =\"YlOrRd\")\nplt.show()","582ee858":"# Dropping the features with low correlation with the outcome column\nX = X.drop([\"blueWardsPlaced\", \"blueWardsDestroyed\",\n           \"blueTowersDestroyed\", \"blueTotalJungleMinionsKilled\",\n           \"redWardsPlaced\", \"redWardsDestroyed\",\n           \"redTowersDestroyed\", \"redTotalJungleMinionsKilled\"], axis = 1)","e4b4eda7":"# Plotting the final correlation matrix\ncorrelation_mat_X = X.corr().abs()\nplt.figure(figsize=(10,8))\nsns.heatmap(correlation_mat_X, annot = True, linewidths=.3, cmap =\"YlOrRd\")\nplt.show()","082982e9":"# Independent and dependent variables\nX = X.drop(['blueWins'], axis = 1)\nY = df[\"blueWins\"]","fb8ec3b2":"# Plotting the histograms of selected features\nX.hist(figsize=(12,12), color = 'darkblue')\nplt.show()","96bdb9c9":"# Splitting the data \nfrom sklearn.model_selection import train_test_split\ntestSize = 0.2\nx_train, x_test, y_train, y_test = train_test_split(\n        X, Y, test_size=testSize, random_state = 4)","c959c3be":"# Logistic Regression Model\nfrom sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\nlogistic.fit(x_train,y_train)\nlogistic_prob = logistic.predict_proba(x_test)","dba167cc":"# Naive Bayes Model\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nnb_predict = nb.predict(x_test)\nnb_prob = nb.predict_proba(x_test)","d4c9f33a":"# SVC Model\nfrom sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf', probability=True,\n          gamma='auto')\nsvc.fit(x_train,y_train)\nsvc_predict = svc.predict(x_test)\nsvc_prob = svc.predict_proba(x_test)","dfe3ca29":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nneighbors = int(np.sqrt(len(X)))\nknn = KNeighborsClassifier(n_neighbors= neighbors)\nknn.fit(x_train,y_train)\nknn_predict = knn.predict(x_test)\nknn_prob = knn.predict_proba(x_test)","fe15b047":"# Random Guess\nrandom = [1 for game in range(len(y_test))]","5948a270":"from sklearn.metrics import roc_curve, roc_auc_score","31687962":"logistic_prob = logistic_prob[:,-1]\nnb_prob = nb_prob[:,-1]\nsvc_prob = svc_prob[:,-1]\nknn_prob = knn_prob[:,-1]","26103957":"# ROC Curve\nlogistic_fpr, logistic_tpr, logistic_thresholds = roc_curve(y_test, logistic_prob)\nnb_fpr, nb_tpr, nb_thresholds                   = roc_curve(y_test, nb_prob)\nsvc_fpr, svc_tpr, svc_thresholds                = roc_curve(y_test, svc_prob)\nknn_fpr, knn_tpr, knn_thresholds                = roc_curve(y_test, knn_prob)","6b1936ae":"# AUC Scores\nlogistic_auc = roc_auc_score(y_test, logistic_prob)\nnb_auc = roc_auc_score(y_test, nb_prob)\nsvc_auc = roc_auc_score(y_test, svc_prob)\nknn_auc = roc_auc_score(y_test, knn_prob)\nrandom_auc = roc_auc_score(y_test, random)","8babe01e":"from tabulate import tabulate","c1c0aa0e":"auc_table = [\n['Logistic Regression   ', logistic_auc],\n['Naive Bayes', nb_auc ],\n['KNN', svc_auc ],\n['SVC',knn_auc],\n['Random Guess', random_auc]\n] \nprint (tabulate(auc_table, headers=[\"Model\", \"AUC Score\"]))","2395fe27":"# Best threshold for Logistic Regression Model\nequation_logistic = np.sqrt(logistic_tpr * (1-logistic_fpr))\nindex_logistic = np.argmax(equation_logistic)\nprint('Best Threshold for Logistic Regression Model: %f' % (logistic_thresholds[index_logistic]))","744417c5":"# Best threshold for Naive Bayes Model\nequation_nb = np.sqrt(nb_tpr * (1-nb_fpr))\nindex_nb = np.argmax(equation_nb)\nprint('Best Threshold for Naive Bayes Model: %f' % (nb_thresholds[index_nb]))","cfb18798":"# Plotting the ROC Curve for models\nplt.figure(figsize=(8,6))\nplt.plot(logistic_fpr,logistic_tpr, linestyle = '-', label = 'Logistic Regression')\nplt.plot(nb_fpr,nb_tpr,  label = 'Naive Bayes')\nplt.plot(knn_fpr,knn_tpr, label = 'KNN')\nplt.plot(svc_fpr,svc_tpr,  label = 'SVC')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.plot([0,1], [0,1], linestyle='--', label='Random Guess')\nplt.scatter(logistic_fpr[index_logistic], logistic_tpr[index_logistic], marker='D', color='k',\n            label='Best Score for Logistic Regression')\nplt.scatter(nb_fpr[index_nb], nb_tpr[index_nb], marker='o', color='k', label='Best Score for Naive Bayes')\nplt.legend(loc='lower right')\nplt.show()","99569e85":"# Logistic Regression Model predictions\nlogistic_prediction = []\nfor i in logistic_prob:\n    if i>= logistic_thresholds[index_logistic]:\n        i = 1\n        logistic_prediction.append(i)\n    else:\n        i=0\n        logistic_prediction.append(i)","7a3ae445":"# Naive Bayes Model predictions\nnb_predict = []\nfor i in logistic_prob:\n    if i>= nb_thresholds[index_nb]:\n        i = 1\n        nb_predict.append(i)\n    else:\n        i=0\n        nb_predict.append(i)","113d5cc5":"from sklearn.metrics import accuracy_score, precision_score, recall_score","ac634042":"# Accuracy, Precision, and Recall Scores of Logistic Regression Model \nlogistic_acc    = accuracy_score(logistic_prediction, y_test)\nlogistic_prec   = precision_score(logistic_prediction, y_test)\nlogistic_recall = recall_score(logistic_prediction, y_test)\n\n# Accuracy, Precision, and Recall Scores of Naive Bayes Model \nnb_acc    = accuracy_score(nb_predict, y_test)\nnb_prec   = precision_score(nb_predict, y_test)\nnb_recall = recall_score(nb_predict, y_test)\n\n# Accuracy, Precision, and Recall Scores of KNN Model\nknn_acc    = accuracy_score(knn_predict, y_test)\nknn_prec   = precision_score(knn_predict, y_test)\nknn_recall = recall_score(knn_predict, y_test)\n\n# Accuracy, Precision, and Recall Scores of SVC Model\nsvc_acc    = accuracy_score(svc_predict, y_test)\nsvc_prec   = precision_score(svc_predict, y_test)\nsvc_recall = recall_score(svc_predict, y_test)\n\n# Accuracy, Precision, and Recall Scores of Random Guess\nrandom_acc    = accuracy_score(random, y_test)\nrandom_prec   = precision_score(random, y_test)\nrandom_recall = recall_score(random, y_test)","fae92d77":"from sklearn.metrics import confusion_matrix","19805496":"# Confusion Matrix for Logistic Regression Model\nlogistic_tn, logistic_fp, logistic_fn, logistic_tp = confusion_matrix(\n        y_test, logistic_prediction).ravel()\n\n# Confusion Matrix for Naive Bayes Model\nnb_tn, nb_fp, nb_fn, nb_tp = confusion_matrix(\n        y_test, nb_predict).ravel()\n\n# Confusion Matrix for KNN Model\nknn_tn, knn_fp, knn_fn, knn_tp = confusion_matrix(\n        y_test, knn_predict).ravel()\n\n\n# Confusion Matrix for SVC Model\nsvc_tn, svc_fp, svc_fn, svc_tp = confusion_matrix(\n        y_test, svc_predict).ravel()\n\n# Confusion Matrix for Random Guess\nrandom_tn, random_fp, random_fn, random_tp = confusion_matrix(\n        y_test, random).ravel()","e67c0f4c":"table = [\n['Logistic Regression   ', logistic_acc, logistic_prec, logistic_recall, logistic_tp, logistic_fp, logistic_tn, logistic_fn],\n['Naive Bayes'           , nb_acc      , nb_prec      , nb_recall      , nb_tp      , nb_fp      , nb_tn      , nb_fn      ],\n['KNN'                   , knn_acc     , knn_prec     , knn_recall     , knn_tp     , knn_fp     , knn_tn     , knn_fn     ],\n['SVC'                   , svc_acc     , svc_prec     , svc_recall     , svc_tp     , svc_fp     , svc_tn     , svc_fn     ],\n['Random Guess'          , random_acc  , random_prec  , random_recall  , random_tp  , random_fp  , random_tn  , random_fn  ]\n        ] \nprint (tabulate(table, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\",\n                               \"TP\", \"FP\", \"TN\", \"FN\"]))","21581809":"2. Features with low correlation with the outcome column can be dropped.","0c8151bb":"### Feature Selection","6c4d3303":"The outcome column is distributed equally.","048b48a5":"### Model Selection","31c58a20":"Logistic Regression and Naive Bayes models are expected to perform better because of the higher AUC Score.","155adf7d":"1. Highly correlated features should be dropped."}}