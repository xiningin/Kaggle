{"cell_type":{"4af9711e":"code","6873e0d1":"code","3c6d0592":"code","bb89f729":"code","49456ed0":"code","a4883df5":"code","1f2630b5":"code","9301af73":"code","a7162401":"code","a839feef":"code","470a569e":"code","e90b8166":"code","fabb67fb":"code","df6ddc23":"code","5c105bd1":"code","877e2c72":"code","dfbaf03c":"code","bb349af4":"code","4a09a459":"code","05e0e80e":"code","33700d1a":"code","ec265b27":"code","6e6621b7":"code","a91b261c":"code","e4518b68":"code","0c071fc8":"code","04fbb220":"code","5fb2e4e5":"code","bb91db60":"code","43483dd3":"code","efef57bd":"code","8443b575":"code","33676eea":"code","4a3d807a":"code","ddef44ca":"code","3525caa1":"code","491efa97":"code","a83b7dd8":"code","74b7749e":"code","9fbff3c1":"code","809aa783":"code","54cc44d5":"code","1951ef87":"code","345b0769":"code","182fd3d2":"code","ec936924":"code","ba6ff799":"code","4117c6d3":"code","9c54b5e7":"code","04bc6e53":"code","cf7138bb":"code","ffa111f7":"code","70b6f90a":"code","3c855ed5":"code","52fafeaf":"code","0c413911":"code","ce766f14":"code","24ecb220":"code","147c0929":"code","ec241596":"code","d3df4512":"code","43825469":"code","54a21d31":"code","a2543d8a":"code","f98a5c74":"markdown","7dae0d64":"markdown","cbf2b2f3":"markdown","7493c9f1":"markdown","1092420f":"markdown","4f38864d":"markdown","ef4918d0":"markdown","5529ee52":"markdown","ac055381":"markdown","6fbcbd60":"markdown","66ee1cc7":"markdown","bfe5c5d1":"markdown","fe096cd5":"markdown","e2e1f38a":"markdown","b92f5eb4":"markdown","ffae40d3":"markdown","53a9007b":"markdown","a0d8474f":"markdown","8dd483c0":"markdown","776ac87e":"markdown","1a0e9b67":"markdown","4b011780":"markdown","99b73186":"markdown","b6739cca":"markdown","1432c620":"markdown","67c5deb9":"markdown","f7466d8e":"markdown","70dc9a91":"markdown","c094bde8":"markdown","a48f0a59":"markdown","ee988409":"markdown","06a67e34":"markdown","afe97e19":"markdown","100a0e00":"markdown","969d9dc2":"markdown","ad1262fe":"markdown","ff36c236":"markdown","422e3541":"markdown","d39402a1":"markdown","799bbd48":"markdown","49e1bbab":"markdown","a5d1bf0b":"markdown","359978b7":"markdown","278b1684":"markdown","ebe8848c":"markdown","6e2d7aab":"markdown","04a8f679":"markdown","9855d30f":"markdown","ee0ab3cf":"markdown","09da5090":"markdown","5aad8ff1":"markdown","f8e79d32":"markdown","4596be7b":"markdown","248114a2":"markdown","7ee3bdb6":"markdown"},"source":{"4af9711e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# import useful tools\nfrom glob import glob\nfrom PIL import Image\nimport cv2\n\n# import data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n\nfrom bokeh.plotting import figure\nfrom bokeh.io import output_notebook, show, output_file\nfrom bokeh.models import ColumnDataSource, HoverTool, Panel\nfrom bokeh.models.widgets import Tabs\n\n# import data augmentation\nimport albumentations as albu","6873e0d1":"# Setup the paths to train and test images\n# TRAIN_DIR = '..\/input\/zalo-ai\/za_traffic_2020\/traffic_train\/images\/'\n# TEST_DIR = '..\/input\/zalo-ai\/za_traffic_2020\/traffic_public_test\/images\/'\n# TRAIN_CSV_PATH = '..\/input\/train-sign-csv\/train_traffic_sign_dataset.csv'\n\nTEST_DIR = '..\/input\/global-wheat-detection\/test\/'\nTRAIN_DIR = '..\/input\/global-wheat-detection\/train\/'\nTRAIN_CSV_PATH = '..\/input\/global-wheat-detection\/train.csv'\n# Glob the directories and get the lists of train and test images\ntrain_fns = glob(TRAIN_DIR + '*')\n\ntest_fns = glob(TEST_DIR + '*')","3c6d0592":"print('Number of train images is {}'.format(len(train_fns)))\nprint('Number of test images is {}'.format(len(test_fns)))","bb89f729":"# Setup the paths to train and test images\nTRAIN_DIR = '..\/input\/zalo-ai\/za_traffic_2020\/traffic_train\/images\/'\nTEST_DIR = '..\/input\/zalo-ai\/za_traffic_2020\/traffic_public_test\/images\/'\nTRAIN_CSV_PATH = '..\/input\/train-sign-csv\/train_traffic_sign_dataset.csv'\n\n# TEST_DIR = '..\/input\/global-wheat-detection\/test\/'\n# TRAIN_DIR = '..\/input\/global-wheat-detection\/train\/'\n# TRAIN_CSV_PATH = '..\/input\/global-wheat-detection\/train.csv'\n# Glob the directories and get the lists of train and test images\ntrain_fns = glob(TRAIN_DIR + '*')\n\ntest_fns = glob(TEST_DIR + '*')","49456ed0":"print('Number of train images is {}'.format(len(train_fns)))\nprint('Number of test images is {}'.format(len(test_fns)))","a4883df5":"# Load the dataframe with the bounding boxes\ntrain = pd.read_csv(TRAIN_CSV_PATH)\n\n# Create a dataframe with all train images\nall_train_images = pd.DataFrame([fns.split('\/')[-1][:-4] for fns in train_fns])\nall_train_images.columns=['image_id']\n\n# Merge all train images with the bounding boxes dataframe\nall_train_images = all_train_images.merge(train, on='image_id', how='left')\n\n# replace nan values with zeros\nall_train_images['bbox'] = all_train_images.bbox.fillna('[0,0,0,0]')\n\n# split bbox column\nbbox_items = all_train_images.bbox.str.split(',', expand=True)\nall_train_images['bbox_xmin'] = bbox_items[0].str.strip('[ ').astype(float)\nall_train_images['bbox_ymin'] = bbox_items[1].str.strip(' ').astype(float)\nall_train_images['bbox_width'] = bbox_items[2].str.strip(' ').astype(float)\nall_train_images['bbox_height'] = bbox_items[3].str.strip(' ]').astype(float)","1f2630b5":"all_train_images.head()","9301af73":"train.head()","a7162401":"all_train_images","a839feef":"print('{} images without wheat heads.'.format(len(all_train_images) - len(train)))","470a569e":"def convert_coco_json_to_csv(filename):\n    import pandas as pd\n    import json\n    import os\n    s = json.load(open(filename, 'r'))\n    filename = os.path.split(filename)[-1][:-5]\n    print(filename)\n    out_file = filename + '_1.csv'\n    out = open(''+out_file, 'w')\n    out.write('image_id,width,height,x,y,w,h,label\\n')\n\n    all_ids = []\n    all_sizes = {}\n    for im in s['images']:\n        all_ids.append(im['id'])\n        all_sizes[im['id']] = [im['width'], im['height']]\n    all_sizes\n    all_ids_ann = []\n    for ann in s['annotations']:\n        image_id = ann['image_id']\n        all_ids_ann.append(image_id)\n        imgw = all_sizes[image_id][0]\n        imgh = all_sizes[image_id][1]\n        xmin = ann['bbox'][0]\n        ymin = ann['bbox'][1]\n        width = ann['bbox'][2]\n        height = ann['bbox'][3]\n        label = ann['category_id']\n        out.write('{},{},{},{},{},{},{},{}\\n'.format(image_id, imgw, imgh, xmin, ymin, width, height, label))\n\n    all_ids = set(all_ids)\n    all_ids_ann = set(all_ids_ann)\n    no_annotations = list(all_ids - all_ids_ann)\n    print(\"Number of images without annotations\", len(no_annotations))\n    # Output images without any annotations\n    for image_id in no_annotations:\n        out.write('{},{},{},{},{},{},{},{}\\n'.format(image_id, -1, -1, -1, -1, -1, -1, -1))\n    out.close()\n\n    # Sort file by image id\n    s1 = pd.read_csv(out_file)\n    s1.sort_values('image_id', inplace=True)\n    s1.to_csv(out_file, index=False)","e90b8166":"convert_coco_json_to_csv('..\/input\/zalo-ai\/za_traffic_2020\/traffic_train\/train_traffic_sign_dataset.json')","fabb67fb":"# Load the dataframe with the bounding boxes\nall_train_images = pd.read_csv(TRAIN_CSV_PATH)\nall_train_images.head()","df6ddc23":"\nall_train_images['x'] = all_train_images['x'].astype(float)\nall_train_images['y'] = all_train_images['y'].astype(float)\nall_train_images['w'] = all_train_images['w'].astype(float)\nall_train_images['h'] = all_train_images['h'].astype(float)","5c105bd1":"all_train_images.head()","877e2c72":"def get_all_bboxes(df, image_id):\n    image_bboxes = df[df.image_id == image_id]\n    \n    bboxes = []\n    for _,row in image_bboxes.iterrows():\n        bboxes.append((row.x, row.y, row.w, row.h))\n    #print(bboxes)\n    return bboxes\n\ndef plot_image_examples(df, rows=3, cols=3, title='Image examples'):\n    fig, axs = plt.subplots(rows, cols, figsize=(20,20))\n    for row in range(rows):\n        for col in range(cols):\n            idx = np.random.randint(len(df), size=1)[0]\n            img_id = df.iloc[idx].image_id\n            #print(type(TRAIN_DIR))\n            img_id = img_id.astype(np.int64)\n            img = Image.open(TRAIN_DIR + str(img_id) + '.png')\n            axs[row, col].imshow(img)\n            \n            bboxes = get_all_bboxes(df, img_id)\n            print(bboxes)\n            for bbox in bboxes:\n                rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n                axs[row, col].add_patch(rect)\n            \n            axs[row, col].axis('off')\n            \n    plt.suptitle(title)","dfbaf03c":"plot_image_examples(all_train_images)","bb349af4":"all_train_images.width","4a09a459":"# compute the number of bounding boxes per train image\nall_train_images['count'] = all_train_images.apply(lambda row: 1 if np.isfinite(row.width) else 0, axis=1)\ntrain_images_count = all_train_images.groupby('image_id').sum().reset_index()","05e0e80e":"all_train_images.head()","33700d1a":"# See this article on how to plot bar charts with Bokeh:\n# https:\/\/towardsdatascience.com\/interactive-histograms-with-bokeh-202b522265f3\ndef hist_hover(dataframe, column, colors=[\"#94c8d8\", \"#ea5e51\"], bins=30, title=''):\n    hist, edges = np.histogram(dataframe[column], bins = bins)\n    \n    hist_df = pd.DataFrame({column: hist,\n                             \"left\": edges[:-1],\n                             \"right\": edges[1:]})\n    hist_df[\"interval\"] = [\"%d to %d\" % (left, right) for left, \n                           right in zip(hist_df[\"left\"], hist_df[\"right\"])]\n\n    src = ColumnDataSource(hist_df)\n    plot = figure(plot_height = 400, plot_width = 600,\n          title = title,\n          x_axis_label = column,\n          y_axis_label = \"Count\")    \n    plot.quad(bottom = 0, top = column,left = \"left\", \n        right = \"right\", source = src, fill_color = colors[0], \n        line_color = \"#35838d\", fill_alpha = 0.7,\n        hover_fill_alpha = 0.7, hover_fill_color = colors[1])\n        \n    hover = HoverTool(tooltips = [('Interval', '@interval'),\n                              ('Count', str(\"@\" + column))])\n    plot.add_tools(hover)\n    \n    output_notebook()\n    show(plot)","ec265b27":"hist_hover(train_images_count, 'count', title='Number of signs per image')","6e6621b7":"less_spikes_ids = train_images_count[train_images_count['count'] < 3].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(less_spikes_ids)], title='Example images with small number of spikes')","a91b261c":"less_sign_ids = train_images_count[train_images_count['count'] < 3].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(less_sign_ids)], title='Example images with small number of signs')","e4518b68":"many_spikes_ids = train_images_count[train_images_count['count'] > 100].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(many_spikes_ids)], title='Example images with large number of spikes')","0c071fc8":"more_sign_ids = train_images_count[train_images_count['count'] > 8].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(more_sign_ids)], title='Example images with large number of signs')","04fbb220":"# compute bounding box areas\nall_train_images['bbox_area'] = all_train_images['w'] * all_train_images['h']","5fb2e4e5":"all_train_images.head()","bb91db60":"# plot a histogram of bounding box areas\nhist_hover(all_train_images, 'bbox_area', title='Area of a single bounding box')","43483dd3":"c\u00f3 v\u1ebb l\u00e0 di\u1ec7n t\u00edch box kh\u00e1 b\u00e9, ch\u1ee7 y\u1ebfu l\u00e0 c\u00e1c box b\u00e9 -> maybe c\u00e1c skill trong b\u00e0i Wheat Detection c\u00f3 th\u1ec3 d\u00f9ng \u0111\u01b0\u1ee3c","efef57bd":"all_train_images.bbox_area.max()","8443b575":"large_boxes_ids = all_train_images[all_train_images['bbox_area'] > 8000].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_boxes_ids)], title='Example images with large bbox area')","33676eea":"large_boxes_ids = all_train_images[all_train_images['bbox_area'] > 20000].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_boxes_ids)], title='Example images with large bbox area')","4a3d807a":"c\u00f3 1 v\u00e0i tr\u01b0\u1eddng h\u1ee3p bi\u1ec3n b\u1ecb cong v\u1eb9o","ddef44ca":"min_area = all_train_images[all_train_images['bbox_area'] > 0].bbox_area.min()\nprint('The smallest bounding box area is {}'.format(min_area))","3525caa1":"small_boxes_ids = all_train_images[(all_train_images['bbox_area'] < 50) & (all_train_images['bbox_area'] > 0)].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_boxes_ids)], title='Example images with large bbox area')","491efa97":"small_boxes_ids = all_train_images[(all_train_images['bbox_area'] < 50) & (all_train_images['bbox_area'] > 0)].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_boxes_ids)], title='Example images with large bbox area')","a83b7dd8":"# compute the total bounding boxes area per image\narea_per_image = all_train_images.groupby(by='image_id').sum().reset_index()\n\n# compute the percentage of the image area covered by bounding boxes\narea_per_image_percentage = area_per_image.copy()\narea_per_image_percentage['bbox_area'] = area_per_image_percentage['bbox_area'] \/ (626*1626) * 100","74b7749e":"hist_hover(area_per_image_percentage, 'bbox_area', title='Percentage of image area covered by bounding boxes')","9fbff3c1":"small_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] < 7].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_area_perc_ids)], title='Example images with small percentage of area covered by bounding boxes')","809aa783":"small_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] < 1.5].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(small_area_perc_ids)], title='Example images with small percentage of area covered by bounding boxes')","54cc44d5":"large_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] > 3].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_area_perc_ids)], title='Example images with large percentage of area covered by bounding boxes')","1951ef87":"large_area_perc_ids = area_per_image_percentage[area_per_image_percentage['bbox_area'] > 7].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(large_area_perc_ids)], title='Example images with large percentage of area covered by bounding boxes')","345b0769":"def get_image_brightness(image):\n    # convert to grayscale\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    \n    # get average brightness\n    return np.array(gray).mean()\n\ndef add_brightness(df):\n    brightness = []\n    for _, row in df.iterrows():\n        img_id = row.image_id\n        #print(img_id)\n        image = cv2.imread(TRAIN_DIR + str(img_id) + '.png')\n        brightness.append(get_image_brightness(image))\n        \n    brightness_df = pd.DataFrame(brightness)\n    brightness_df.columns = ['brightness']\n    df = pd.concat([df, brightness_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'brightness']\n    \n    return df","182fd3d2":"images_df = pd.DataFrame(all_train_images.image_id.unique())\nimages_df.columns = ['image_id']","ec936924":"# add brightness to the dataframe\nimages_df = pd.DataFrame(all_train_images.image_id.unique())\nimages_df.columns = ['image_id']\nbrightness_df = add_brightness(images_df)\n\nall_train_images = all_train_images.merge(brightness_df, on='image_id')","ba6ff799":"hist_hover(all_train_images, 'brightness', title='Images brightness distribution')","4117c6d3":"dark_ids = all_train_images[all_train_images['brightness'] < 30].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(dark_ids)], title='Darkest images')","9c54b5e7":"dark_ids = all_train_images[all_train_images['brightness'] < 40].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(dark_ids)], title='Darkest images')","04bc6e53":"bright_ids = all_train_images[all_train_images['brightness'] > 150].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(bright_ids)], title='Brightest images')","cf7138bb":"bright_ids = all_train_images[all_train_images['brightness'] > 130].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(bright_ids)], title='Brightest images')","ffa111f7":"def get_percentage_of_green_pixels(image):\n    # convert to HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (40, 40, 40) \n    hsv_higher = (70, 255, 255)\n    green_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(green_mask)) \/ 255 \/ (1024 * 1024)\n\ndef get_percentage_of_yellow_pixels(image):\n    # convert to HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n    \n    # get the green mask\n    hsv_lower = (25, 40, 40) \n    hsv_higher = (35, 255, 255)\n    yellow_mask = cv2.inRange(hsv, hsv_lower, hsv_higher)\n    \n    return float(np.sum(yellow_mask)) \/ 255 \/ (1024 * 1024)\n\ndef add_green_pixels_percentage(df):\n    green = []\n    for _, row in df.iterrows():\n        img_id = row.image_id  \n        image = cv2.imread(TRAIN_DIR + str(img_id) + '.png')\n        green.append(get_percentage_of_green_pixels(image))\n        \n    green_df = pd.DataFrame(green)\n    green_df.columns = ['green_pixels']\n    df = pd.concat([df, green_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'green_pixels']\n    \n    return df\n\ndef add_yellow_pixels_percentage(df):\n    yellow = []\n    for _, row in df.iterrows():\n        img_id = row.image_id  \n        image = cv2.imread(TRAIN_DIR + str(img_id) + '.png')\n        yellow.append(get_percentage_of_yellow_pixels(image))\n        \n    yellow_df = pd.DataFrame(yellow)\n    yellow_df.columns = ['yellow_pixels']\n    df = pd.concat([df, yellow_df], ignore_index=True, axis=1)\n    df.columns = ['image_id', 'yellow_pixels']\n    \n    return df","70b6f90a":"# add a column with the percentage of green pixels\ngreen_pixels_df = add_green_pixels_percentage(images_df)\nall_train_images = all_train_images.merge(green_pixels_df, on='image_id')","3c855ed5":"hist_hover(all_train_images, 'green_pixels', title='Percentage of green pixels distribution', colors=['#c3ea84', '#3e7a17'])","52fafeaf":"green_ids = all_train_images[all_train_images['green_pixels'] > 0.55].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(green_ids)], title='The most green images')","0c413911":"# add a column with the percentage of yellow pixels\nyellow_pixels_df = add_yellow_pixels_percentage(images_df)\nall_train_images = all_train_images.merge(yellow_pixels_df, on='image_id')","ce766f14":"hist_hover(all_train_images, 'yellow_pixels', title='Percentage of yellow pixels distribution', colors=['#fffedb', '#fffeab'])","24ecb220":"yellow_ids = all_train_images[all_train_images['yellow_pixels'] > 0.55].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(yellow_ids)], title='The most yellow images')","147c0929":"yellow_ids = all_train_images[all_train_images['yellow_pixels'] > 0.3].image_id\nplot_image_examples(all_train_images[all_train_images.image_id.isin(yellow_ids)], title='The most yellow images')","ec241596":"# setup an example augmentation pipeline\n# be sure to use bbox safe functions for data augmentation\nexample_transforms = albu.Compose([\n    albu.RandomSizedBBoxSafeCrop(512, 512, erosion_rate=0.0, interpolation=1, p=1.0),\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.OneOf([albu.RandomContrast(),\n                albu.RandomGamma(),\n                albu.RandomBrightness()], p=1.0),\n    albu.CLAHE(p=1.0)], p=1.0, bbox_params=albu.BboxParams(format='coco', label_fields=['category_id']))","d3df4512":"def apply_transforms(transforms, df, n_transforms=3):\n    idx = np.random.randint(len(df), size=1)[0]\n    \n    image_id = df.iloc[idx].image_id\n    image_id = image_id.astype(np.int64)\n    bboxes = []\n    for _, row in df[df.image_id == image_id].iterrows():\n        bboxes.append([row.x, row.y, row.w, row.h])\n        \n    image = Image.open(TRAIN_DIR + str(image_id) + '.png')\n    \n    fig, axs = plt.subplots(1, n_transforms+1, figsize=(15,7))\n    \n    # plot the original image\n    axs[0].imshow(image)\n    axs[0].set_title('original')\n    for bbox in bboxes:\n        rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n        axs[0].add_patch(rect)\n    \n    # apply transforms n_transforms times\n    for i in range(n_transforms):\n        params = {'image': np.asarray(image),\n                  'bboxes': bboxes,\n                  'category_id': [1 for j in range(len(bboxes))]}\n        augmented_boxes = transforms(**params)\n        bboxes_aug = augmented_boxes['bboxes']\n        image_aug = augmented_boxes['image']\n\n        # plot the augmented image and augmented bounding boxes\n        axs[i+1].imshow(image_aug)\n        axs[i+1].set_title('augmented_' + str(i+1))\n        for bbox in bboxes_aug:\n            rect = patches.Rectangle((bbox[0],bbox[1]),bbox[2],bbox[3],linewidth=1,edgecolor='r',facecolor='none')\n            axs[i+1].add_patch(rect)\n    plt.show()","43825469":"all_train_images.head()","54a21d31":"apply_transforms(example_transforms, all_train_images, n_transforms=3)","a2543d8a":"apply_transforms(example_transforms, all_train_images, n_transforms=3)","f98a5c74":"Let's look at some examples of images with small areas covered by bounding boxes:","7dae0d64":"Example augmentation pipeline:","cbf2b2f3":"The maximum area of bounding box:","7493c9f1":"Surprisingly, the most green images have only aoung 60% green pixels.\n\nThe majority of the images are not green at all! Most probably, they are more yellow and are the images of the plants close to harvest.","1092420f":"Let's just look at the numbers first. Those numbers will give us a hint which images might be interesting to look at.","4f38864d":"Count number of bounding boxes per image:","ef4918d0":"Let's plot some examples of large bounding boxes:","5529ee52":"`1` The number of train and test images:","ac055381":"`5` The brightness of the images:","6fbcbd60":"Compute at the number of train and test images:","66ee1cc7":"# Global Wheat Detection Competition EDA","bfe5c5d1":"## Conclusions:\n\n1. Images are taken at different zoom levels. Crop and resize data augmentations to be used for model training.\n2. Images are taken at various lighting conditions. Special filters should be used to address that.\n3. Bounding boxes are messy! \n    * Giant bounding boxes should be filtered out by area and removed before model training.\n    * Micro bounding boxes. These can stay. They won't have much effect on the IOU metric.\n    * Some spikes are not surrounded by a bounding box (missing bounding boxes).","fe096cd5":"Plot the darkest images:","e2e1f38a":"trong train set kh\u00f4ng c\u00f3 \u1ea3nh n\u00e0o kh\u00f4ng \u0111\u01b0\u1ee3c \u0111\u00e1nh nh\u00e3n ","b92f5eb4":"The most green images mostly contain the plants with very small spikes, which are just starting to appear.","ffae40d3":"These are very different from the dark ones. Some filters needed here to make the spikes more clear.\n\nI also see the missing boundaries.","53a9007b":"kh\u00f4ng c\u1ea7n so s\u00e1nh v\u00e0ng v\u1edbi xanh l\u1eafm v\u00ec \u0111\u00e2y context ph\u1ee9c t\u1ea1p kh\u00f4ng ph\u1ea3i ch\u1ec9 c\u00f3 m\u1ed7i v\u00e0ng v\u00e0 xanh nh\u01b0 GWD","a0d8474f":"## Related Papers and Links\nI found some interesting papers and articles:\n1. [Deep learning based banana plant detection and counting using high-resolution red-green-blue (RGB) images collected from unmanned aerial vehicle (UAV)](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0223906). The objective of this research is focused to automate the count of banana plants during early stage of growth, in which they are prone to lose maturity.\n2. [Fine-grained recognition of plants from images](https:\/\/plantmethods.biomedcentral.com\/articles\/10.1186\/s13007-017-0265-4). Fine-grained recognition of plants from images is a challenging computer vision task, due to the diverse appearance and complex structure of plants, high intra-class variability and small inter-class differences. We review the state-of-the-art and discuss plant recognition tasks, from identification of plants from specific plant organs to general plant recognition \u201cin the wild\u201d.\n3. [Review of deep learning algorithms for object detection](https:\/\/medium.com\/zylapp\/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852)\n4. [Fast Detection Models](https:\/\/lilianweng.github.io\/lil-log\/2018\/12\/27\/object-detection-part-4.html)\n5. [SSD Tutorial](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Object-Detection), [SSD implementation in pytorch](https:\/\/github.com\/qfgaohao\/pytorch-ssd)\n6. [EfficientDet](https:\/\/arxiv.org\/pdf\/1911.09070.pdf)\n7. [CutMix paper](https:\/\/arxiv.org\/pdf\/1905.04899.pdf)","8dd483c0":"I would like to plot images with different dominant colors. The idea is that the most green images will represent healthy plants. The most yellow images will contain plants close to maturity. The most brown images will have ground on them.","776ac87e":"`4` Area of bounding boxes per image:","1a0e9b67":"These are some very strange examples:\n* on some of the images all we see is the ground;\n* some of the images are just zoomed in a lot.","4b011780":"Similarly, let's look at very small bounding boxes:","99b73186":"Ki\u1ec3m tra l\u1ea1i 1 s\u1ed1 sample.","b6739cca":"Let's plot some examples with small number of spikes per image:","1432c620":"These actually look much better than those with small number of spikes.","67c5deb9":"`3` Area of bounding boxes:","f7466d8e":"We have only 10 test images here, other test images will be used to evaluate the prediction models during the submission.\n\nJust `3422` images for the training seems to be not much at all. Data augmentation techniques will be definetely required in this competition.","70dc9a91":"Now let's plot the images with large areas covered by bounding boxes:","c094bde8":"We can see images taken at different lighting conditions and plant maturity stages!","a48f0a59":"![image](https:\/\/raw.githubusercontent.com\/Lexie88rus\/GlobalWheatDetection\/master\/wheat_image_cropped.png)","ee988409":"If you look very close, you can probably see those tinyest bounding boxes near the corners and borders of the images. Probably, the boundries were drawn first, than the images were cut into several ones. That is why we see those strange small bounsing boxes in the corners.\n\nIt is not necessary to clean these, because they won't have much effect on the IOU metric.","06a67e34":"c\u00f3 c\u00e1c boxes qu\u00e1 nh\u1ecf < 10 pixel, maybe c\u00f3 th\u1ec3 remove ra kh\u1ecfi training process","afe97e19":"ch\u1ee7 y\u1ebfu l\u00e0 c\u00e1c box nh\u1ecf c\u00f3 1 v\u00e0i tr\u01b0\u1eddng h\u1ee3p box c\u00f2n ch\u1ed3ng l\u1ea5n v\u00e0o nhau -> c\u1ea7n ph\u1ea3i x\u00f3a ra kh\u1ecfi training process","100a0e00":"Detection of wheat spikes with computer vision opens a lot of opportunity for the farmers and breeders like:\n* controlling the growth stage of the plants on the field: the number oand the area of spikes is raising closer to the harvest date;\n* controlling the heatlth of the plants: unusually small number or small size of plants might be a signal of deceased plants;\n* spikes density characteristic and approximate yield estimation for different varieties of wheat.\n\nIn this notebook I am exploring the data and giving some thoughts on what to pay attention when making and validating the models.","969d9dc2":"`2` The number of bounding boxes (wheat spikes) per image:","ad1262fe":"Plot the brightest images:","ff36c236":"__This EDA is being updated. Your questions and comments are very welcome!__","422e3541":"`6` The most and the least green and yellow images:","d39402a1":"## References & Credits\n1. [Article on how to plot bar charts with Bokeh](https:\/\/towardsdatascience.com\/interactive-histograms-with-bokeh-202b522265f3)","799bbd48":"The distribution of individual areas of bounding boxes has a very long tail. It would be interesting to look at the images with those large bounding boxes.","49e1bbab":"Most of the images have 1-2 signs on them.","a5d1bf0b":"__What are these anomally large bounding boxes?? I think those should be removed while training!!!__","359978b7":"The most green images:","278b1684":"c\u00f3 1 v\u00e0i \u1ea3nh l\u1ea1 khi c\u00f3 nhi\u1ec1u box m\u00e0 kh\u00f4ng th\u1ea5y \u0111\u01b0\u1ee3c v\u1ebd l\u00ean \u1ea3nh -> maybe box nh\u1ecf","ebe8848c":"On some of the images, it is even hard for human to see the spikes!","6e2d7aab":"Construct dataframe with all images (images with no bboxes will have nan values in all columns except `image_id`):","04a8f679":"c\u00f3 v\u1ebb l\u00e0 di\u1ec7n t\u00edch box kh\u00e1 b\u00e9, ch\u1ee7 y\u1ebfu l\u00e0 c\u00e1c box b\u00e9 -> maybe c\u00e1c skill trong b\u00e0i Wheat Detection c\u00f3 th\u1ec3 d\u00f9ng \u0111\u01b0\u1ee3c","9855d30f":"This looks like a nice normal distribution! 20-40% of the image area is covered by the bounding boxes.\n\nThis observation can be used to validate the predictions of the resulting model. The percentage of predicted bounding boxes area should be normally distributed too.\n\nWe can also see that the maximum is actually greater than 100%. This means that the bounding boxes are overlapping.","ee0ab3cf":"Plot the images with many spikes:","09da5090":"Let's plot some image examples:","5aad8ff1":"## Thoughts on Data Augmentation\n\nData augmentation is critical in this competition, because there is a relatively small training set. Data augmentation will allow to build robust models under given circumstances.\nWhat augmentations\/filers might work:\n* flipping images horizontally and vertically, because the orientation of original images is different;\n* crop-resize, because we can see spikes at different zoom levels;\n* different filters to adjust the lighting conditions. I suggest looking at [this competition](https:\/\/www.kaggle.com\/c\/aptos2019-blindness-detection) for example.\n\nWhat to do with caution:\n* rotation might not work, because rotation messes up the bounding boxes.","f8e79d32":"Note how CLAHE emphasizes the features in darker spots. It is a must have for this competition.","4596be7b":"## General Dataset Information","248114a2":"c\u00e1c box to l\u00e0 \u1edf g\u1ea7n -> c\u00f3 th\u1ec3 kh\u00f4ng c\u1ea7n remove cho training","7ee3bdb6":"Let's look at the most yellow images:"}}