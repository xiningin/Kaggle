{"cell_type":{"2317f5c2":"code","8c034df1":"code","ec509e75":"code","b7aa22ba":"code","10b52bf1":"code","b7bee003":"code","6bd0bd0a":"code","494edf8f":"code","ac4c3cbb":"code","25248213":"code","c9855a74":"code","b00ef050":"code","65969a2b":"code","1c3735bf":"code","89b910ce":"code","06a1f47a":"code","8d2850ea":"code","1dbfd00b":"code","f584fca0":"code","baee702b":"code","b47f12f7":"code","d6050c67":"code","eb52b525":"code","1b3e7591":"code","f4fa96f2":"code","27690ae8":"code","1bf95160":"code","423c0f0c":"code","7e7dcbb6":"code","d9823f5b":"code","d519bc35":"code","91fcb718":"code","57ad3d67":"code","eba7db7a":"code","f1a1e6fc":"code","1fd1e12d":"code","d3183489":"code","e1b22f83":"code","fedadf2c":"code","99b04711":"code","f1aeb70a":"code","1e727d05":"code","621e0815":"code","36a67d12":"code","1cd1add9":"markdown","c2019ae1":"markdown","20de900a":"markdown","5ee564ba":"markdown","26a7bd14":"markdown","0991e7be":"markdown","38ec3b98":"markdown","9e107591":"markdown","10c5f173":"markdown","23ff67a7":"markdown","e56d3e01":"markdown","2decc23f":"markdown","a060ad52":"markdown","c9250049":"markdown","89991147":"markdown","dd5d0197":"markdown","2159de03":"markdown","0227e794":"markdown","fac0bab3":"markdown","830159ff":"markdown","f12473f3":"markdown","fd423932":"markdown","1ec668d1":"markdown","55a245de":"markdown","92055e88":"markdown","9abdaff7":"markdown","e8427282":"markdown","2a4e5089":"markdown","6e8d1abf":"markdown","9417f192":"markdown","e5152a9c":"markdown","a6f1017b":"markdown","c020d12a":"markdown","b656d9d3":"markdown","f3415509":"markdown","441df78f":"markdown","d15866f8":"markdown"},"source":{"2317f5c2":"import os, sys, random\nimport numpy as np\nimport pandas as pd\nimport cv2\n#\ud30c\uc774\ud1a0\uce58\uc784\ud3ec\ud2b8\nimport torch\n#\ud30c\uc774\ud1a0\uce58 \uc778\uacf5\uc2e0\uacbd\ub9dd \ubaa8\ub378\uc758 \uc7ac\ub8cc\ub4e4\uc744 \ub2f4\uace0 \uc788\ub294 \ubaa8\ub4c8 \uc784\ud3ec\ud2b8\nimport torch.nn as nn\n#\uc704\uc758 nn\ubaa8\ub4c8\uc744 \ud568\uc218\ud654 \ud55c \ubaa8\ub4c8 \uc784\ud3ec\ud2b8\nimport torch.nn.functional as F\n\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","8c034df1":"#torch.cuda.is_available():\ud604\uc7ac\uc0c1\ud0dc\uc5d0\uc11c cuda\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0\uc5ec\ubd80\n#cuda\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc73c\uba74  \"cuda:0\"\uc744 \uc544\ub2c8\uba74 \"cpu\" \ubc18\ud658\ud558\uc5ec torch.device\uc5d0 \uc124\uc815\ud55c \ud6c4 \"gpu\"\ub77c\ub294 \ubcc0\uc218\uc5d0 \uc800\uc7a5 \ud574 \ub193\uc74c\ngpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","ec509e75":"image_size = 224\nbatch_size = 64","b7aa22ba":"crops_dir = \"..\/input\/faces-155\/\"\n\nmetadata_df = pd.read_csv(\"..\/input\/deepfakefaces\/metadata.csv\")\nmetadata_df.head()","10b52bf1":"len(metadata_df)","b7bee003":"len(metadata_df[metadata_df.label == \"REAL\"]), len(metadata_df[metadata_df.label == \"FAKE\"])","6bd0bd0a":"img_path = os.path.join(crops_dir, np.random.choice(os.listdir(crops_dir)))\nplt.imshow(cv2.imread(img_path)[..., ::-1])","494edf8f":"#\uc815\uaddc\ud654\ub97c \uc704\ud574 torchvision.transform\uc5d0\uc11c \uc815\uaddc\ud654 \ubaa8\ub4c8\uc784\ud3ec\ud2b8\nfrom torchvision.transforms import Normalize\n\n#\uc5ed\uc815\uaddc\ud654 \ud074\ub798\uc2a4 \uc120\uc5b8\nclass Unnormalize:\n    \"\"\"Converts an image tensor that was previously Normalize'd\n    back to an image with pixels in the range [0, 1].\"\"\"\n    \n    #\uc0dd\uc131\uc790 \uc815\uc758(mean(\ud3c9\uade0),std(\ubd84\uc0b0)):__init__(\uc0dd\uc131\uc790):\uac1d\uccb4\uac00 \uc0dd\uc131\ub420 \ub54c \uc790\ub3d9\uc73c\ub85c \ud638\ucd9c\ub418\ub294 \uba54\uc11c\ub4dc\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        #view\ud568\uc218:\ud150\uc11c\uc758 \uc6d0\uc18c\uac1c\uc218\ub97c \uc720\uc9c0\ud558\uba74\uc11c \ubaa8\uc591\uc744 \ubc14\uafbc\ub2e4.\n        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        return torch.clamp(tensor*std + mean, 0., 1.)\n\n#\uc6b0\ub9ac\uac00 \uc0ac\uc6a9\ud558\ub294 \ubaa8\ub378\uc5d0\uc11c \uc694\uad6c\ud558\ub294 \uac01 \ucc44\ub110\uc758 \uc2dc\ud000\uc2a4\uc758 \ubd84\uc0b0\uacfc \ud3c9\uaddc\ub2c8\ub2e4\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)\nunnormalize_transform = Unnormalize(mean, std)","ac4c3cbb":"def random_hflip(img, p=0.5):\n    \"\"\"Random horizontal flip.\"\"\"\n    if random.random() < p:\n        return cv2.flip(img, 1)\n    else:\n        return img","25248213":"def load_image_and_label(filename, cls, crops_dir, image_size, augment):\n    #\ud574\ub2f9\ub418\ub294 \ud30c\uc77c\uc758 \uc774\ubbf8\uc9c0\ub97c \ud150\uc11c\uac12\uc73c\ub85c \ubcc0\ud658, \uadf8 \ub77c\ubca8 \uac12\uc744 \uac00\uc838\uc628\ub2e4.\n    #\ud574\ub2f9\uacbd\ub85c\uc5d0 \uc788\ub294 \uc774\ubbf8\uc9c0\ud30c\uc77c \uc77d\uc5b4\uc634\n    img = cv2.imread(os.path.join(crops_dir, filename))\n    #openCV\uceec\ub7ec\ub97c BGR\ub85c \uc800\uc7a5\ud558\ub294\ub370 matplotlib\ub4f1\uc5d0\uc11c\ub294 RGB\ub85c \uc800\uc7a5\ud558\ubbc0\ub85c, BGR->RGB\ub85c \ubc14\uafb8\ub294 \ud568\uc218\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    #\ud655\uc7a5\uc774 True ,\uc774\ubbf8\uc9c0\ub97c \uc218\ud3c9\uc73c\ub85c \ub4a4\uc9d1\ub294\ub2e4.\n    if augment: \n        img = random_hflip(img)\n     #\uc774\ubbf8\uc9c0\ud30c\uc77c\uc744 (224,224)\uc0ac\uc774\uc988\ub85c \uc870\uc808\ud55c\ub2e4. \n    img = cv2.resize(img, (image_size, image_size))\n    #img\ud30c\uc77c \ud30c\uc774\ud1a0\uce58 \ud150\uc11c\ub85c \ubcc0\ud658\ud558\uace0,\ucc28\uc6d0\uc744(\uc6d0\ub798\uc774\ubbf8\uc9c0\ud30c\uc77c\ucc28\uc6d0\uc778\ub371\uc2a4\ub97c \ub123\uc74c)\uc870\uc808\ud568(\uad04\ud638\uc548\ub300\ub85c ).\uc774\ubbf8\uc9c0\ub294 0~255\uae4c\uc9c0\ud53d\uc140\uac12\uc774 \uc788\uc73c\ubbc0\ub85c 255\ub85c \ub098\ub204\uc5b4 \uc815\uaddc\ud654\n    img = torch.tensor(img).permute((2, 0, 1)).float().div(255)\n    #\uc55e\uc11c \uc815\uc758 \ud55c \ud3c9\uade0\uacfc \ubd84\uc0b0\uc744 \uac00\uc9c0\uace0 \uc774\ubbf8\uc9c0 \uac12\uc744 \uc815\uaddc\ud654\ud568.\n    img = normalize_transform(img)\n#cls(\ub77c\ubca8)\uac12\uc774 \"fake\"\uba74 1\uc774\uace0, \uc544\ub2c8\uba74 0(real)\uc774\ub2e4.\n    target = 1 if cls == \"FAKE\" else 0\n    return img, target","c9855a74":"img, target = load_image_and_label(\"aabuyfvwrh.jpg\", \"FAKE\", crops_dir, 224, augment=True)\nimg.shape, target","b00ef050":"plt.imshow(unnormalize_transform(img).permute((1, 2, 0)))","65969a2b":"from torch.utils.data import Dataset\n\nclass VideoDataset(Dataset):\n    \"\"\"Face crops dataset.\n\n    Arguments:\n        crops_dir: \uc774\ubbf8\uc9c0\uc790\ub8cc\uac00 \uc788\ub294 \ud3f4\ub354,\n        df: \ub370\uc774\ud130\ud504\ub808\uc784(\uba54\ud0c0\ub370\uc774\ud130\uac00 \uc788\ub294)\n        split: \ud6c8\ub828\uc744 \ud55c\ub2e4\uba74 \ub370\uc774\ud130 \ud655\uc7a5\n        image_size: \uc0ac\uc774\uc988\uc870\uc808\ud55c\uc0ac\uc774\uc988\n        sample_size: evenly samples this many videos from the REAL\n            and FAKE subfolders (None = use all videos)\n        seed: \ub79c\ub364\uc73c\ub85c \uc120\ud0dd\ud558\ub294 \uc0d8\ud50c\ub9c1 \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\ub294 \uc22b\uc790,\n    \"\"\"\n    def __init__(self, crops_dir, df, split, image_size, sample_size=None, seed=None):\n        self.crops_dir = crops_dir\n        self.split = split\n        self.image_size = image_size\n        \n        if sample_size is not None:#\uc0d8\ud50c\ub9c1\ud560 \uac1c\uc218\uac00 \uc788\ub2e4\uba74\n            real_df = df[df[\"label\"] == \"REAL\"]\n            fake_df = df[df[\"label\"] == \"FAKE\"]\n            #sample_size\uc640, \uc9c4\uc9dc\uc640 \uac00\uc9dc\ub370\uc774\ud130\uc758 \uae38\uc774\uc911 \uac00\uc7a5 \uc791\uc740 \uac12\uc744 \ubc18\ud658\ud55c\ub2e4.\n            sample_size = np.min(np.array([sample_size, len(real_df), len(fake_df)]))\n            print(\"%s: sampling %d from %d real videos\" % (split, sample_size, len(real_df)))\n            print(\"%s: sampling %d from %d fake videos\" % (split, sample_size, len(fake_df)))\n            real_df = real_df.sample(sample_size, random_state=seed)\n            fake_df = fake_df.sample(sample_size, random_state=seed)\n            self.df = pd.concat([real_df, fake_df])\n        else:\n            self.df = df\n\n        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n\n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        filename = row[\"videoname\"][:-4] + \".jpg\"\n        cls = row[\"label\"]\n        return load_image_and_label(filename, cls, self.crops_dir, \n                                    self.image_size, self.split == \"train\")\n    def __len__(self):\n        return len(self.df)","1c3735bf":"dataset = VideoDataset(crops_dir, metadata_df, \"val\", image_size, sample_size=1000, seed=1234)","89b910ce":"plt.imshow(unnormalize_transform(dataset[0][0]).permute(1, 2, 0))","06a1f47a":"del dataset","8d2850ea":"def make_splits(crops_dir, metadata_df, frac):\n    # Make a validation split. Sample a percentage of the real videos, \n    # and also grab the corresponding fake videos.\n    real_rows = metadata_df[metadata_df[\"label\"] == \"REAL\"]\n    real_df = real_rows.sample(frac=frac, random_state=666)\n    #metadata.csv\ud30c\uc77c\uc5d0\uc11c \"original\"\uc5f4\uc5d0\uc11c real_df\uc758 \"videoname\"\uc774 \uc548\uc5d0 \uc788\uc73c\uba74 \uac00\uc9dc\ub85c \ubcf4\uace0\"fake_df\"\ubcc0\uc218\uc5d0 \uc800\uc7a5\n    fake_df = metadata_df[metadata_df[\"original\"].isin(real_df[\"videoname\"])]\n    #\uc9c4\uc9dc\uc640 \uac00\uc9dc \ud569\uce58\uae30\n    val_df = pd.concat([real_df, fake_df])\n\n    # The training split is the remaining videos.\n    train_df = metadata_df.loc[~metadata_df.index.isin(val_df.index)]\n\n    return train_df, val_df","1dbfd00b":"train_df, val_df = make_splits(crops_dir, metadata_df, frac=0.05)\n#assert(\uac00\uc815\ubb38:\uac00\ub85c\uc548\uc758 \uac12\uc774 \uc624\ub958\uac00\ub098\uba74 aseertatin Error \ubc1c\uc0dd)\n#assert()\uc758 \uc870\uac74\uc744 \ubcf4\uc99d\ud55c\ub2e4\n#train\ub370\uc774\ud130\uc758 \uae38\uc774\uc640 val\ub370\uc774\ud130\uc758 \uae38\uc774\uc758 \uc55e\uc740 \uc804\uccb4 \ub370\uc774\ud130\uc758 \uae38\uc774\uc640 \ub3d9\uc77c\ud558\ub2e4\ub294 \uac83\uc744 \ubcf4\uc99d\nassert(len(train_df) + len(val_df) == len(metadata_df))\n#train\ub370\uc774\ud130\uc758 \"videoname\"\uc548\uc5d0 val\ub370\uc774\ud130\uc758 \"videoname\"\uc774 \uc5c6\ub2e4\ub294 \uac83\uc744 \ubcf4\uc99d\ud55c\ub2e4.\nassert(len(train_df[train_df[\"videoname\"].isin(val_df[\"videoname\"])]) == 0)\n\ndel train_df, val_df","f584fca0":"#\ub370\uc774\ud130\uc14b\uc73c\ub85c\ubd80\ud130 \uc720\uc758\ubbf8\ud55c \ub370\uc774\ud130\ub97c \ubf51\uc544\uc624\ub294 \uac83\uc744 \ub370\uc774\ud130\ub85c\ub354\ub77c\uace0 \ud568,\uc774 \ubaa8\ub4c8\uc740 \uc784\ud3ec\ud2b8\ud574 batch_size\uc9c0\uc815\ud558\uba74 \ud55c \ubc88\uc5d0 batch_size\ub9cc\ud07c \ubd88\ub7ec\uc62c \uc218 \uc788\ub2e4.\nfrom torch.utils.data import DataLoader\n\ndef create_data_loaders(crops_dir, metadata_df, image_size, batch_size, num_workers):\n    #\ud2b8\ub808\uc778\ub370\uc774\ud130\uc640 \ubc1c\ub9ac\ub370\uc774\uc158 \ub370\uc774\ud130\ub97c \ub098\ub214\n    train_df, val_df = make_splits(crops_dir, metadata_df, frac=0.05)\n    \n    #\uc704\uc5d0\uc11c \uc815\uc758\ud55c VideoDataset()\ud568\uc218\ub97c \uc774\uc6a9\ud574 train_dataset\uc744 \ub9cc\ub4e6\n    train_dataset = VideoDataset(crops_dir, train_df, \"train\", image_size, sample_size=10000)\n    #torch.utils.data\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 DataLoader\ub97c \uc0ac\uc6a9\ud558\uc5ec train_dataset\uc5d0\uc11c \ud55c \ubc88\uc5d0 batch_size\ub9cc\ud07c \uc790\ub8cc\ub97c \uac00\uc838\uc628\ub2e4.\n    #DataLoader \uc18d\uc131 \uac12: num_workers:\ub370\uc774\ud130\ud504\ub85c\uc138\uc2f1\uc5d0 \ud560\ub2f9\ud558\ub294 cpu\ucf54\uc5b4\uac1c\uc218,pin_memory=True\uc774\uba74 \uba54\ubaa8\ub9ac\uc5d0 \uc0d8\ud50c\uc744 \ud560\ub2f9\ud558\uc5ec \ub370\uc774\ud130 \uc804\uc1a1\uc18d\ub3c4\ub97c \uc62c\ub9b4 \uc218 \uc788\ub2e4.\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                              num_workers=num_workers, pin_memory=True)\n\n    val_dataset = VideoDataset(crops_dir, val_df, \"val\", image_size, sample_size=500, seed=1234)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, \n                            num_workers=num_workers, pin_memory=True)\n\n    return train_loader, val_loader","baee702b":"train_loader, val_loader = create_data_loaders(crops_dir, metadata_df, image_size, \n                                               batch_size, num_workers=2)","b47f12f7":"#train_loader\uae30\uc5d0\uc11c iter\ud568\uc218\ub85c \ucc28\ub840\ub85c \uc77d\uc5b4\ub4e4\uc778\uac83\uc744 next\ud568\uc218\ub85c \ubc18\ud658\ud55c\ub2e4\nX, y = next(iter(train_loader))\nplt.imshow(unnormalize_transform(X[0]).permute(1, 2, 0))\nprint(y[0])","d6050c67":"X, y = next(iter(val_loader))\nplt.imshow(unnormalize_transform(X[0]).permute(1, 2, 0))\nprint(y[0])","eb52b525":"def evaluate(net, data_loader, device, silent=False):\n    net.train(False)\n\n    bce_loss = 0\n    total_examples = 0\n   #with tqdm(,desc=\"\") as pbar:\uc0c1\ud0dc\ubc14 \ub098\ud0c0\ub0b4\ub294 \ud568\uc218\n    with tqdm(total=len(data_loader), desc=\"Evaluation\", leave=False, disable=silent) as pbar:\n        for batch_idx, data in enumerate(data_loader):#\ub370\uc774\ud130\ub85c\ub354\uc5d0 \uc788\ub294 \uac12\uc744 \uc778\ub371\uc2a4\uc640 \ud568\uaed8 \ud3bc\uce68\n            with torch.no_grad():#\ud3c9\uac00\ud560\ub54c \uae30\uc6b8\uae30 \uc790\ub3d9\uacc4\uc0b0\uc548\ud55c\ub2e4.\n                batch_size = data[0].shape[0]\n                #\uc790\ub8cc\uc758 \ub370\uc774\ud130\uac12\uacfc \uacb0\uacfc\uac12\uc744 \ud574\ub2f9 \uba54\ubaa8\ub9ac\uc5d0 \ubcf4\ub0c4\n                x = data[0].to(device)\n                y_true = data[1].to(device).float()\n                \n#\ubaa8\ub378 net(x)\uc758 \uacb0\uacfc\uac12(\uc989,\uc608\uce21\uac12)\uc744 squeeze()\ud568\uc218 \uc0ac\uc6a9\ud558\uc5ec \ucc28\uc6d0\uc218\ub97c \ucd95\uc18c\ud55c\ub2e4.\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n               #\uc608\uce21\uac12\uacfc \uc2e4\uc81c\uac12\uc758 \uc624\ucc28\ub97c \uad6c\ud558\uc5ec \uc2a4\uce7c\ub77c(.item())\uac12\uc73c\ub85c \uad6c\ud55c\uac12\uc5d0 batch_size\uac12\uc744 \uad6c\ud55c \uac12\uc744 \ud55c \ubc30\uce58\ub9c8\ub2e4 \ub354\ud568.\n                bce_loss += F.binary_cross_entropy_with_logits(y_pred, y_true).item() * batch_size\n            #\ucd1d\uc790\ub8cc\uac1c\uc218\ub294 \ud55c\ubc30\uce58\uac00 \ub3cc\uc544\uac08\ub54c\ub9c8\ub2e4 \ub204\uc801\ud574\uc11c \ucd1d\uc790\ub8cc\uac1c\uc218\uac00 \ub428.\n            total_examples += batch_size\n            #\ud504\ub85c\uadf8\ub798\uc2a4\ubc14 \uc218\uc815\n            pbar.update()\n    #\uc624\ucc28\uc758 \ucd1d\ud569\uc744 \ucd1d \uac1c\uc218\ub85c \ub098\ub204\uba74 \uc624\ucc28\ud3c9\uade0\uc774 \ub098\uc634.\n    bce_loss \/= total_examples\n\n    if silent:\n        return bce_loss\n    else:\n        print(\"BCE: %.4f\" % (bce_loss))","1b3e7591":"def fit(epochs):\n    #\uc804\uc5ed\ubcc0\uc218 \uc120\uc5b8\n    global history, iteration, epochs_done, lr\n    #tqdm \ud568\uc218\uc640 \uc0c1\ud0dc\ubc14 \uc124\uc815\n    with tqdm(total=len(train_loader), leave=False) as pbar:\n        #\uc5d0\ud3ec\ud06c \uc218\ub9cc\ud07c for\ubb38 \ub3cc\ub9bc\n        for epoch in range(epochs):\n            #\uc0c1\ud0dc\ucc3d\uc744 \ucd08\uae30\ud654\n            pbar.reset()\n            #\uc0c1\ud0dc\ucc3d \ucd9c\ub825\uac12 \uc124\uc815\n            pbar.set_description(\"Epoch %d\" % (epochs_done + 1))\n            #\ub85c\uc2a4\uac12\uacfc \uc790\ub8cc\uac1c\uc218 \ucd08\uae30\ud654\n            bce_loss = 0\n            total_examples = 0\n            \n            #\ubaa8\ub378\uc758 \ud6c8\ub828\uc744 \ud65c\uc131\ud654\n            net.train(True)\n            #\ud6c8\ub828\ub370\uc774\ud130\ub85c\ub354\uc758 \uc790\ub8cc\ub97c \uc778\ub371\uc2a4 \uac12\uacfc \ud3bc\uce5c\ub2e4\n            for batch_idx, data in enumerate(train_loader):\n                #batch_size\ub294 \ub370\uc774\ud130\uc758 \uccab\ud589\uc758 \ubaa8\uc591\uc758 \uccab\ubc88\uc9f8\n                batch_size = data[0].shape[0]\n                #\uc790\ub8cc\uac12\uacfc \uc2e4\uc81c\uac12\uc744 \uba54\ubaa8\ub9ac\uc5d0 \ubcf4\ub0c4\n                x = data[0].to(gpu)\n                y_true = data[1].to(gpu).float()\n                #\uacbd\uc0ac\ud558\uac15\ubc95 \uae30\uc6b8\uae30(\uac00\uc911\uce58)\uac12 \ucd08\uae30\ud654\ud55c\ub2e4.(\ud559\uc2b5\uc744 \uc2dc\uc791\ud574\uc57c\ud558\ubbc0\ub85c)\n                optimizer.zero_grad()\n                #\uc608\uce21\uac12\uc744 squeeze()\ud568\uc218 \uc0ac\uc6a9\ud558\uc5ec \ucc28\uc6d0\uc744 \ucd95\uc18c\ud55c\ub2e4.\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n                #\uc624\ucc28\uac12:\uc2e4\uc81c\uac12\uacfc \uc608\uce21\uac12\uc758 \uc624\ucc28\ub97c binary_cross_entropy\ub97c \uc0ac\uc6a9\ud558\uc5ec \uad6c\ud55c\ub2e4\n                loss = F.binary_cross_entropy_with_logits(y_pred, y_true)\n                #\uc5ed\uc804\ud30c:loss\uc758 \uae30\uc6b8\uae30\uc758 \ubc18\ub300\ubc29\ud5a5\uc73c\ub85c \uc774\ub3d9\ud558\uc5ec \uae30\uc6b8\uae30 \uac1c\uc120\uc2dc\ud0a8\ub2e4.\n                loss.backward()\n                #\uc704\uc758 \uc190\uc2e4\ud568\uc218\uac00 \uc5ed\uc804\ud30c\ud558\ub294\ub3d9\ud55c \uacbd\uc0ac\ud558\uac15\ubc95\uc73c\ub85c \uae30\uc6b8\uae30 \ucd5c\uc801\ud654\uc2dc\ud0a8\ub2e4.\n                optimizer.step()\n                #batch_bce\uc5d0 loss\uc758 \uc2a4\uce7c\ub77c \uac12\uc744 \uc800\uc7a5\ud55c\ub2e4\n                batch_bce = loss.item()\n                #for\ubb38\uc774 \ud55c\ubc88 \ub3cc\ub54c\ub9c8\ub2e4 1batch\uc758 loss\ucd1d\ud569(batch_bce * batch_size)\uc744 bce_loss\uc5d0 \ub354\ud574 \ud68c\uc804\uc774 \ub05d\ub098\uba74 loss\uc758 \ucd1d\ud569\uc744 \uad6c\ud560 \uc218 \uc788\ub2e4.\n                bce_loss += batch_bce * batch_size\n                history[\"train_bce\"].append(batch_bce)\n                #\ucd1d\uc790\ub8cc\uac1c\uc218\ub294 \ud55c\ubc88\ub3cc\ub54c\ub9c8\ub2e4 batch_size\ub97c \ub354\ud574\uc11c \ucd1d \uc790\ub8cc\uac1c\uc218 \uad6c\ud568\n                total_examples += batch_size\n                #\ubc18\ubcf5\ud68c\uc218 1\uc529\uc99d\uac00\n                iteration += 1\n                #\ud504\ub85c\uc138\uc2a4\ubc14 update\ub428\n                pbar.update()\n            #1epoch\ub3cc\ub9b4\ub54c\ub9c8\ub2e4 \uc624\ucc28\uc758 \ucd1d\ud569\uc5d0 \ucd1d \ud30c\uc77c\uac1c\uc218\ub97c \ub098\ub204\uc5b4 \ud3c9\uade0\uc624\ucc28\ub97c \uad6c\ud568\n            bce_loss \/= total_examples\n            epochs_done += 1\n\n            print(\"Epoch: %3d, train BCE: %.4f\" % (epochs_done, bce_loss))\n            #validation loss\ub294 evaluate\ud568\uc218 \uc0ac\uc6a9\ud558\uc5ec \ud568\uaed8 \uad6c\ud568\n            val_bce_loss = evaluate(net, val_loader, device=gpu, silent=True)\n            history[\"val_bce\"].append(val_bce_loss)\n            \n            print(\"              val BCE: %.4f\" % (val_bce_loss))\n            \n            \n            \n\n            # TODO: can do LR annealing here\n            # TODO: can save checkpoint here\n            #\ud559\uc2b5\ub960 \uc870\uc815\n            scheduler.step()\n            #\uc5d0\ud3ec\ud06c\ub9c8\ub2e4 \ubaa8\ub378\uc800\uc7a5\ud558\uae30\n            torch.save(net.state_dict(), \"epoch:{} val_bce:{:.4f}.pth\".format(epochs_done,val_bce_loss))\n            \n\n            print(\"\")","f4fa96f2":"checkpoint = torch.load(\"..\/input\/externaldata\/pretrained-pytorch\/resnext50_32x4d-7cdf4587.pth\")","27690ae8":"import torchvision.models as models\n#torchvision.models\uc5d0 \uc788\ub294 resnet\ubaa8\ub378\uc5d0\uc11c ResNet class \uc0c1\uc18d\nclass MyResNeXt(models.resnet.ResNet):\n    \n    def __init__(self, training=True):\n        #ResNet \uc0dd\uc131\uc790 \ub04c\uc5b4\uc634.\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n\n        self.load_state_dict(checkpoint)\n\n        # Override the existing FC layer with a new one.\n        self.fc = nn.Linear(2048, 1)","1bf95160":"net = MyResNeXt().to(gpu)","423c0f0c":"del checkpoint","7e7dcbb6":"out = net(torch.zeros((10, 3, image_size, image_size)).to(gpu))\nout.shape","d9823f5b":"#\ubaa8\ub378\uacfc \ud30c\ub77c\ubbf8\ud130 \uc774\ub984\uc744 \uc785\ub825\ud558\uba74,\uc785\ub825\ud55c \ud30c\ub77c\ubbf8\ud130\uc774\uc804 \ud30c\ub77c\ubbf8\ud130\ub294 \ud559\uc2b5 \uc548\ud568(\uc804\uc774\ud559\uc2b5)\ndef freeze_until(net, param_name):\n    found_name = False\n    \n    for name, params in net.named_parameters():\n        \n        if name == param_name:\n           \n            found_name = True\n    \n        params.requires_grad = found_name","d519bc35":"[k for k,v in net.named_parameters()]","91fcb718":"freeze_until(net, \"layer4.0.conv1.weight\")","57ad3d67":"[k for k,v in net.named_parameters() if v.requires_grad]","eba7db7a":"evaluate(net, val_loader, device=gpu)","f1a1e6fc":"lr = 0.1\n\nwd=0.000\n\nhistory = { \"train_bce\": [], \"val_bce\": [] }\niteration = 0\nepochs_done = 0\n#\uacbd\uc0ac\ud558\uac15\ubc95 adam,weight_decay=L2\uaddc\uc81c\noptimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\n#\ud559\uc2b5\ub960 \uc870\uc808\uae30:stept_size=5(5\ud68c\uc804\ud560\ub54c\ub9c8\ub2e4),gamma=0.1 \ud559\uc2b5\ub960\uc5d0 0.1\uc744 \uacf1\ud574\uc900\ub2e4.\nscheduler=torch.optim.lr_scheduler.StepLR(optimizer,step_size=5,gamma=0.1)","1fd1e12d":"fit(10)","d3183489":"freeze_until(net, \"layer3.0.conv1.weight\")","e1b22f83":"fit(5)","fedadf2c":"#\uc704\uc758 \uc2a4\ucf00\uc904\ub7ec \uc124\uc815\uc73c\ub85c \ub300\uccb4\ud55c\ub2e4\n# def set_lr(optimizer, lr):\n#     for param_group in optimizer.param_groups:\n#         param_group[\"lr\"] = lr","99b04711":"#\uc704\uc758 \uc2a4\ucf00\uc904\ub7ec \uc124\uc815\uc73c\ub85c \ub300\uccb4\ud55c\ub2e4.\n# lr \/= 10\n# set_lr(optimizer, lr)","f1aeb70a":"plt.plot(history[\"train_bce\"])","1e727d05":"plt.plot(history[\"val_bce\"])","621e0815":"evaluate(net, val_loader, device=gpu, silent=True)","36a67d12":"# torch.save(net.state_dict(), \"checkpoint.pth\")","1cd1add9":"\ucd9c\ucc98(\ucc38\uc870):https:\/\/wikidocs.net\/21050(03_\uac00\uc815 \uc124\uc815\ubb38(assert))\n","c2019ae1":"## Helper code for training","20de900a":"Test the model on a small batch to see what its output shape is:","5ee564ba":"Plot training progress. It's nicer to use something like TensorBoard for this, but a simple plot also works. ;-)","26a7bd14":"## The model","0991e7be":"Simple training loop. I prefer to write those myself from scratch each time, because then you can tweak it to do whatever you like.","38ec3b98":"Before we train, let's run the model on the validation set. This should give a logloss of about 0.6931.","9e107591":"Sanity check:","10c5f173":"To use the PyTorch data loader, we need to create a `Dataset` object.\n\nBecause of the class imbalance (many more fakes than real videos), we're using a dataset that samples a given number of REAL faces and the same number of FAKE faces, so it's always 50-50.\n:'Dataset'\uac1d\uccb4\ub97c \ud65c\uc6a9\ud558\uc5ec \uc9c4\uc9dc\uc640 \uac00\uc9dc \ub370\uc774\ud130\ub97c \ub3d9\uc77c\ud55c \ube44\uc728\ub85c \ub9de\ucd94\uc790","23ff67a7":"\uc774 \ubcc0\uc218 'gpu'\ub294  \ub098\uc911\uc5d0 \ud150\uc11c\uc640 \uac00\uc911\uce58\uc5d0 \ub300\ud55c \uc5f0\uc0b0\uc744 CPU\uc640 GPU \uc911 \uc5b4\ub514\uc5d0\uc11c \uc2e4\ud589\ud560\uc9c0 \uacb0\uc815\ud560 \ub54c \uc0ac\uc6a9\ub428, \uc544\ubb34 \uac83\ub3c4 \uc548\ud558\uba74 \"CPU\"","e56d3e01":"Reference:https:\/\/www.kaggle.com\/humananalog\/binary-image-classifier-training-demo","2decc23f":"\ucc38\uc870:\uc790\ub3d9\ubbf8\ubd84(https:\/\/wikidocs.net\/60754","a060ad52":"numpy\uc758 randomchoice\uba85\ub839\uc5b4 \uc0ac\uc6a9\ud558\uc5ec crops_dir\uc5d0 \uc788\ub294 \ud30c\uc77c\uba85 \ub79c\ub364\uc73c\ub85c \uc120\ud0dd\ud558\uc5ec \ud3f4\ub354\uacbd\ub85c\uc640 \ud569\uccd0\uc11c \uacbd\ub85c\ub97c \ubcc0\uc218\ub85c \uc800\uc7a5","c9250049":"[OpenCV] BGR \uc0ac\uc9c4\uc744 RGB \uc0ac\uc9c4\uc73c\ub85c \ubcc0\ud658\ud558\uae30 (cvtColor, \ud30c\uc774\uc36c)\n\n\n\n\ud30c\uc774\uc36c\uc5d0\uc11c OpenCV\ub97c \uc0ac\uc6a9\ud574\uc11c \uc0ac\uc9c4\uc744 matplotlib \uc73c\ub85c \ud654\uba74\uc5d0 \ucd9c\ub825\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4. \uceec\ub7ec \uc0ac\uc9c4\uc744 OpenCV\uc5d0\uc11c\ub294 BGR \uc21c\uc11c\ub85c \uc800\uc7a5\ud558\ub294\ub370 matplotlib\uc5d0\uc11c\ub294 RGB \uc21c\uc11c\ub85c \uc800\uc7a5\ud569\ub2c8\ub2e4. \ub530\ub77c\uc11c BGR\uc744 RGB\ub85c \ubc14\uafb8\uc5b4 \uc8fc\uc5b4\uc57c\ub9cc \uc0ac\uc9c4\uc774 \uc81c\ub300\ub85c \ud45c\uc2dc\ub429\ub2c8\ub2e4.\n\n\n\n\ucd9c\ucc98: https:\/\/crmn.tistory.com\/49 [\ud06c\ub86c\ub9dd\uac04\uc774 \uae00 \uc4f0\ub294 \uacf5\uac04]","89991147":"\ub370\uc774\ud130\ud655\uc7a5\uc744 \uc704\ud574 \uc218\ud3c9\uc73c\ub85c \ub4a4\uc9d1\ub294\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 OpenCV2\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc ,torchvision transform \uc0ac\uc6a9\uac00\ub2a5\ud558\ub2e4.","dd5d0197":"* Split up the data into train \/ validation. There are many different ways to do this. For this kernel, we're going to just grab a percentage of the REAL faces as well as their corresponding FAKEs. This way, a real video and all the fakes that are derived from it will be either completely in the training set or completely in the validation set. (\ud574\uc11d:\ub370\uc774\ud130\ub97c \ud559\uc2b5 \/ \uac80\uc99d\uc73c\ub85c \ubd84\ud560\ud569\ub2c8\ub2e4. \uc774\ub97c \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc5d0\ub294 \uc5ec\ub7ec \uac00\uc9c0\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ucee4\ub110\uc5d0\uc11c\ub294 REAL\uba74\uc758 \ube44\uc728\uacfc \ud574\ub2f9 FAKE\ub97c \uac00\uc838\uc635\ub2c8\ub2e4. \uc774\ub7f0 \uc2dd\uc73c\ub85c \uc2e4\uc81c \ube44\ub514\uc624\uc640 \uadf8\ub85c\ubd80\ud130 \ud30c\uc0dd \ub41c \ubaa8\ub4e0 \uac00\uc9dc\ub294 \ud6c8\ub828 \uc138\ud2b8 \ub610\ub294 \uac80\uc99d \uc138\ud2b8\uc5d0 \uc644\uc804\ud788 \ub4e4\uc5b4\uac11\ub2c8\ub2e4.)\n\n* (This is still not ideal because the same person may appear in many different videos. Ideally we want a person to be either in train or in val, but not in both. But it will do for now.)\ud574\uc11d:\uac19\uc740 \uc0ac\ub78c\uc774 \uc5ec\ub7ec \ub2e4\ub978 \ube44\ub514\uc624\uc5d0 \ub098\ud0c0\ub0a0 \uc218 \uc788\uae30 \ub54c\ubb38\uc5d0 \uc5ec\uc804\ud788 \uc774\uc0c1\uc801\uc774\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4. \uc774\uc0c1\uc801\uc73c\ub85c \uc6b0\ub9ac\ub294 \uc0ac\ub78c\uc774 train \ub610\ub294 val\uc5d0 \uc788\uace0 \ub458 \ub2e4\uc5d0 \uc788\uc9c0 \uc54a\uae30\ub97c \uc6d0\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uc9c0\uae08\uc740 \uadf8\ub807\uac8c \ub420 \uac83\uc785\ub2c8\ub2e4.","2159de03":"To plot the image, we need to unnormalize it and also permute it from (3, 224, 224) to (224, 224, 3). ","0227e794":"At this point you can load the model from the previous checkpoint. If you do, also make sure to restore the optimizer state! Something like this:\n\n```python\ncheckpoint = torch.load(\"model-checkpoint.pth\")\nnet.load_state_dict(checkpoint)\n\ncheckpoint = torch.load(\"optimizer-checkpoint.pth\")\noptimizer.load_state_dict(checkpoint)\n```","fac0bab3":"Like most other torchvision models, the model we're using (ResNeXt50) requires that input images are normalized using mean and stddev. For making plots, we also define an \"unnormalize\" transform that can take a normalized image and turn it back into regular pixels.\n\ud574\uc11d:\uc9c0\uae08\uc5ec\uae30\uc11c \uc0ac\uc6a9\ud55c \ubaa8\ub378\ub3c4,\uc774\ubbf8\uc9c0 \ud30c\uc77c \uc815\uaddc\ud654\uac00 \ud544\uc694\ud558\uc9c0\ub9cc unnormalize(\uc5ed\uc815\uaddc\ud654)\ub3c4 \uc774\ubbf8\uc9c0 \ud45c\uc2dc \ubc0f \uc774\uc804 \uc774\ubbf8\uc9c0\ub85c \ub3cc\uc544\uac00\ub824\uba74 \ud544\uc694\ud574\uc11c \uc815\uc758\ud55c\ub2e4\uace0 \ud568.","830159ff":"Let's start training!","f12473f3":"\uc704 \uc5bc\uad74 \ub370\uc774\ud130 \uc911 \uc9c4\uc9dc\uc640 \uac00\uc9dc \uac1c\uc218\ub97c \ucd9c\ub825\ud558\uc790","fd423932":"\ucc38\uc870:https:\/\/jybaek.tistory.com\/799(DataLoader num_workers\uc5d0 \ub300\ud55c \uace0\ucc30)","1ec668d1":"These are the layers we will train:\ud6c8\ub828\uac00\ub2a5\ud55c \ud30c\ub77c\ubbf8\ud130 \uc774\ub984 \ucd9c\ub825","55a245de":"Evaluation function for running the model on the validation set:","92055e88":"Some helper code for loading a training image and its label:","9abdaff7":"Freeze the early layers of the model:\n**layer4\uc774\uc804 \uce35\uc740 \ud559\uc2b5\uc2dc\ud0a4\uc9c0 \uc54a\ub294\ub2e4.(\ubbf8\uc138\uc870\uc815,\uc804\uc774\ud559\uc2b5)\n**","e8427282":"\ubc30\uc5f4\uacfc \ub9ac\uc2a4\ud2b8\ub97c \ud150\uc11c \uc790\ub8cc\ud615\uc73c\ub85c \ubcc0\ud658\u00b6(\ucd9c\ucc98:https:\/\/datascienceschool.net\/view-notebook\/4f3606fd839f4320a4120a56eec1e228\/)\n* \ub9ac\uc2a4\ud2b8\ub97c \ud150\uc11c \uc790\ub8cc\ud615\uc73c\ub85c \ubc14\uafb8\ub7ec\uba74 torch.tensor() \ub610\ub294 torch.as_tensor(), torch.from_numpy() \uba85\ub839\uc744 \uc0ac\uc6a9\ud55c\ub2e4.\n* \n* torch.tensor(): \uac12 \ubcf5\uc0ac(value copy)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ud150\uc11c \uc790\ub8cc\ud615 \uc778\uc2a4\ud134\uc2a4\ub97c \uc0dd\uc131\ud55c\ub2e4.\n* torch.as_tensor(): \ub9ac\uc2a4\ud2b8\ub098 ndarray \uac1d\uccb4\ub97c \ubc1b\ub294\ub2e4. \uac12 \ucc38\uc870(refernce)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud150\uc11c \uc790\ub8cc\ud615 \ubdf0(view)\ub97c \ub9cc\ub4e0\ub2e4.\n* torch.from_numpy(): ndarray \uac1d\uccb4\ub97c \ubc1b\ub294\ub2e4. \uac12 \ucc38\uc870(refernce)\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud150\uc11c \uc790\ub8cc\ud615 \ubdf0(view)\ub97c \ub9cc\ub4e0\ub2e4.","2a4e5089":"Need to load pretrained ImageNet weights into the model.\n\nYou can get these weights from `https:\/\/download.pytorch.org\/models\/resnext50_32x4d-7cdf4587.pth`, or from [this dataset](https:\/\/www.kaggle.com\/tony92151\/pretrained-pytorch) by Kaggler [tonyguo](https:\/\/www.kaggle.com\/tony92151).","6e8d1abf":"It's always smart to test that the code actually works. The following cell should return a normalized PyTorch tensor of shape (3, 224, 224) and the target 1 (for fake).\n\nNote that this dataset has 155x155 images but our model needs at least 224x224, so we resize them.","9417f192":"Let's test that the dataset actually works...","e5152a9c":"* Use all of the above building blocks to create `DataLoader` objects. Note that we use only a portion of the full amount of training data, for speed reasons. If you have more patience, increase the `sample_size`.\n* \ud574\uc11d:\uc704\uc758 \ubaa8\ub4e0 \ube4c\ub529 \ube14\ub85d\uc744 \uc0ac\uc6a9\ud558\uc5ec 'DataLoader'\uac1d\uccb4\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \uc18d\ub3c4\uc0c1\uc758 \uc774\uc720\ub85c \uc804\uccb4 \uad50\uc721 \ub370\uc774\ud130\uc758 \uc77c\ubd80\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc778\ub0b4\uc2ec\uc774 \ub354 \uc788\uc73c\uba74`sample_size`\ub97c \ub298\ub9ac\uc2ed\uc2dc\uc624.","a6f1017b":"**All done!** You can now use this checkpoint in the [inference kernel](https:\/\/www.kaggle.com\/humananalog\/inference-demo).","c020d12a":"* And, as usual, a check that it works... The `train_loader` should give a different set of examples each time you run it (because `shuffle=True`), while the `val_loader` always returns the examples in the same order.\n* \ud574\uc11d:\uadf8\ub9ac\uace0 \ud3c9\uc18c\uc640 \uac19\uc774 \uc791\ub3d9\ud558\ub294\uc9c0 \ud655\uc778\ud558\uc2ed\uc2dc\uc624 ...`train_loader`\ub294 \uc2e4\ud589\ud560 \ub54c\ub9c8\ub2e4 \ub2e4\ub978 \uc608\uc81c \uc138\ud2b8\ub97c \uc81c\uacf5\ud574\uc57c\ud569\ub2c8\ub2e4 (`shuffle = True` \ub54c\ubb38\uc5d0)`val_loader`\ub294 \ud56d\uc0c1 \ub3d9\uc77c\ud55c \uc608\uc81c\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.","b656d9d3":"During training, we'll apply data augmentation. In this kernel we just do random horizontal flips, but you can add other image transformations here too, such as rotation, zooming, etc. It's possible to use torchvision transforms for this, or a library such as [imgaug](https:\/\/www.github.com\/aleju\/imgaug), but I rolled my own using OpenCV2.","f3415509":"## Training","441df78f":"## The dataset and data loaders","d15866f8":"batch_size\ub294 \ubaa8\ub378\uc774 \ud55c \ubc88 \uc0ac\uc6a9\ud560 \ub54c \ucc98\ub9ac\ud558\ub294 \ud30c\uc77c\uc758 \uac1c\uc218"}}