{"cell_type":{"18051684":"code","dd1d3b0e":"code","b9bb953a":"code","0bf0fb82":"code","4f3b2fd6":"code","73c3b134":"code","e9507ff2":"code","8f1215a9":"code","10780a82":"code","6528759c":"code","af6d5933":"code","18e4f96c":"code","dbb0a3ae":"markdown","0f0f0865":"markdown","580da6d4":"markdown","2e35cfd0":"markdown","a374504b":"markdown","0784e774":"markdown","fa3a1bb1":"markdown","0ced2f69":"markdown","e5dc01d8":"markdown"},"source":{"18051684":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# gensim\nimport gensim\nfrom gensim.corpora import Dictionary\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models.ldamodel import LdaModel\nfrom nltk.corpus import stopwords\n\n# plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dd1d3b0e":"train_df = pd.read_csv(\"..\/input\/train.csv\")","b9bb953a":"train_df_insincere = train_df[train_df.target==1]","0bf0fb82":"# select observations that belong to genre of insincere questions\ninsincere_questions = train_df_insincere['question_text']","4f3b2fd6":"# tokenize words and cleaning up punctuations and so\ndef sent_to_words(insincere_questions):\n    for question in insincere_questions:\n        yield(gensim.utils.simple_preprocess(str(question), deacc=True))  # deacc=True removes punctuations\n\nquestion_words = list(sent_to_words(insincere_questions))","73c3b134":"# remove stopwords\nstop_words = set(stopwords.words(\"english\"))\nquestions_without_stopwords = [[word for word in simple_preprocess(str(question)) \n                                if word not in stop_words] for question in question_words]\n","e9507ff2":"# Form Bigrams\nbigram = Phrases(questions_without_stopwords, min_count=5, threshold=100) \n# higher value of the params min_count and threshold will result in less bigrams. \n# You can playaround with these parameters to get better bigrams\n\nbigram_mod = Phraser(bigram)\nbigrams = [bigram_mod[question] for question in questions_without_stopwords]","8f1215a9":"# Create Dictionary of words - This creates id for each word\/ phrase\nid2word = Dictionary(bigrams) \nprint(\"Word at 0th id: \", id2word[0])\n\n# create corpus - Convert a list of words into the bag-of-words forma\ncorpus = [id2word.doc2bow(text) for text in bigrams]\nprint(\"First element of corpus: \", corpus[0])","10780a82":"# this is bit computation intensive and may take time\ncoherence_values = []\nmodel_list = []\nrange_num_topics = range(2,10)\nfor num_topics in range_num_topics:\n    model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics,\n                                            random_state=100, update_every=1,\n                                            chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n    model_list.append(model)\n    coherencemodel = CoherenceModel(model=model, texts=bigrams, dictionary=id2word, coherence='c_v')\n    coherence_values.append(coherencemodel.get_coherence())","6528759c":"# Print the coherence scores\nfor num_topic, coherence_value in zip(range_num_topics, coherence_values):\n    print(\"Number of Topics : \", num_topic, \" . Coherence Value: \", round(coherence_value, 3))","af6d5933":"# Plot coherence score graph\nplt.plot(range_num_topics, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(\"coherence_values\", loc='best')\nplt.show()","18e4f96c":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(model_list[1], corpus, id2word)\nvis","dbb0a3ae":"Let's visualize model with 3 number of topics. This is experimental . \n\nYou can try out different number of topics and visulaize overlapping of topic segments","0f0f0865":"Before creating topic model, Let's find the optimal number of topics. ","580da6d4":"**Notebook Objective:**\n\nObjective of the notebook is to extract topics people are discussing in insincere category","2e35cfd0":"**Text cleaning**\n\nRemove stop words\n\nRemove special characters such as punctuations","a374504b":"From the above visualization, we can visualize what is each topic is trying to convey.\n\nEach bubble represents topic and hovering mouse over each bubble highlight words. \n\nThe words are salient keywords that constitute the particular topic.\n\n\n* Topic1 talks more about Donald trump, women, americans.\n* Topic2 talks more about muslims, indians, christians. It seems to be talking more on religion \n* Topic3 talks more about quora.","0784e774":"Data preprocessing to remove unwanted words, phrases that do not account much in prediction will result \n\nin better formation of topic clusters and better coherence score.","fa3a1bb1":"Model with the highest coherence value is the best fit for the corpus. It gives intuition of what corpus looks like.\n\nBut if coherence value keeps on increasing, pick the model that gives highest coherence score before flattening out.\n\nProviding higher number of topics to model usually causes overlapping between topics i.e same keyword may become part of various topic. \n\nThus, it may not give any clear intuition of what the particular topic is trying to convey.\n\nSo, it is really important to find optimal number of topics.","0ced2f69":"With the advent of social media forums, people are becoming more vulnerable to prejudices, hatred, attacks, and insults etc.\n\nSo, it is inevitable for social media forums to identity topics intend to destroy peace and harmony among people. \n\nIn this competition quora has given opportunity to kagglers to classify questions asked on its platform as sincere or insincere.\n\n","e5dc01d8":"Bigrams are referred to as two words frequently occuring together such as Donald Trump, United states, north korea. \n\nLet's now create bigrams. \n"}}