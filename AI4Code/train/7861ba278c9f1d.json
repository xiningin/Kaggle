{"cell_type":{"29df2063":"code","aee013a1":"code","1528b51b":"code","1fc17ea1":"code","37ccc70f":"code","4706af22":"code","485fa6a5":"code","16b2da75":"code","34d45198":"code","3aad5dc7":"code","d5c01986":"code","281c3552":"markdown","ecff7991":"markdown","d03bc742":"markdown","6bf75d8a":"markdown","943980a9":"markdown","259f60fb":"markdown"},"source":{"29df2063":"BASE_MODEL = '\/kaggle\/input\/huggingface-roberta-variants\/distilroberta-base\/distilroberta-base'","aee013a1":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntrain","1528b51b":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)","1fc17ea1":"def get_data(df):\n    # These 2 lines took me days to figure out!!! \ud83d\ude16\ud83d\ude16\ud83d\ude16\n    tokenized = tokenizer(df['excerpt'].tolist(), padding=True, return_tensors=\"np\") # the \"np\" means it will return numpy arrays\n    return {feat: tokenized[feat] for feat in tokenizer.model_input_names}\n    \nX = get_data(train)\nX","37ccc70f":"y = train[[\"target\"]].values # note that this is a list of single value lists\ny","4706af22":"import matplotlib.pyplot as plt\n\nimport keras\nimport tensorflow.keras.backend as K\n\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=2,\n    verbose=1,\n    restore_best_weights=True,\n)\n\n\n# to define 'rmse' as loss instead of 'mse'\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\n\ndef plot_hist(history):\n    plt.plot(history['loss'])\n    plt.plot(history['val_loss'])\n    best_val_loss = min(history['val_loss'])\n    print(f'Best validation loss: {best_val_loss:.3f}')","485fa6a5":"from transformers import TFAutoModelForSequenceClassification\nfrom keras.optimizers import *\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1) # num_labels=1 results in a regression\n\nmodel.compile(optimizer=Adam(1e-5), loss=rmse) # small training rates are necessary!\nmodel.summary()","16b2da75":"hist = model.fit(X, y, validation_split=0.2, epochs=5, batch_size=8, callbacks=[early_stop], verbose=2)\n\nplot_hist(hist.history)","34d45198":"test = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\ntest","3aad5dc7":"X_test = get_data(test)\ny_test = model.predict(X_test)\ntest['target'] = y_test.logits[:,0]\ntest","d5c01986":"test.to_csv('submission.csv', columns=['id','target'], index=False)","281c3552":"Submission\n==========","ecff7991":"Conclusion\n==========\n\n1. ***Do not use this code to submit directly. It will uselessly re-train the model during the submission. This was done solely for demonstration purposes***\n2. This does not train on the whole data, since 20% is used for validation, so it's up to you how to include it all, whether it's K-Fold models or something else.\n3. Tweaking the model\/training parameters can lead to huge differences! Here are the most obvious ones:\n    - optimizer\n    - learning rate\n    - batch size\n    - dropout layers\n    - model config parameters\n    - ...or even simply trying another huggingface model\n    \nYou will quickly notice that the impact of these \"hyperparameters\" are huge. But they have one thing in common: you will have to fight overfitting! With < 3000 training samples, it's your enemy. If you acchieve a \"simple\" way to improve this baseline a lot, I would be glad to hear about it.","d03bc742":"Prepare data\n============","6bf75d8a":"Train\n=====","943980a9":"Helper functions\n================","259f60fb":"KISS RoBERTa\n============\n\nIf you don't know it already, KISS is an acronym for \"Keep It Simple Stupid\".\n\n*This notebook's goal is not to be competitive, but rather to quickly get started with RoBERTa training.*\n\nThis intentionally does not contain K-Fold, etc. It uses 80% of the data to train, and 20% to validate itself. It also uses \"distil-roberta\", a smaller model where you sacrifice a little bit of accuracy in exchange for faster training speed. I personally like it because this makes experimentation faster, which is the goal here. Moreover, overfitting will be your enemy, so it's not even sure the bigger model will be better anyway. "}}