{"cell_type":{"cee7abe6":"code","9a2c0224":"code","def83730":"code","acd3c821":"code","b730557a":"code","faf6e1b9":"code","59ab6161":"code","c35d8d94":"code","bdfea03d":"code","3b6d7607":"code","9093b35f":"code","b563f163":"code","0350e1c9":"code","1611ec89":"code","3140cfbf":"code","749659ea":"code","3b84820d":"code","85e83bb9":"code","0c9e338c":"code","71f145a8":"code","fbaedabd":"code","18f633f4":"code","a72a99cb":"code","5b71f5bb":"code","f28bbd91":"code","4f00d4fd":"code","99e5dce1":"code","5f195398":"code","a76ea961":"markdown","44fdac1c":"markdown","6dd1fbaf":"markdown","3c28e529":"markdown","0060fbf4":"markdown","f68ffa9c":"markdown","4e3bf4ca":"markdown","a06ed6a7":"markdown","9e9f4d31":"markdown","05c36e3f":"markdown","7fa3d4b8":"markdown","0cb6eea4":"markdown","4aa0c870":"markdown"},"source":{"cee7abe6":"!pip install python_speech_features","9a2c0224":"import librosa\nfrom python_speech_features import mfcc, logfbank\nfrom scipy.io import wavfile\n\nimport librosa.display\n# import mir_eval\n\n## For playing sound in Notebook.\nimport IPython.display as ipd \n\nfrom plotly.offline import iplot, init_notebook_mode\ninit_notebook_mode()\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n\nimport pandas as pd\nimport numpy as np","def83730":"y,sr = librosa.load(librosa.util.example_audio_file())","acd3c821":"print(type(y), type(sr))\nprint(y.shape, sr)","b730557a":"ipd.Audio(data = y, rate = sr)","faf6e1b9":"## Waveform.\n\nlibrosa.display.waveplot(y, sr);","59ab6161":"D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\nlibrosa.display.specshow(D, y_axis='linear')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Linear-frequency power spectrogram')","c35d8d94":"# n_mels = 128\n# mel = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)","bdfea03d":"# plt.figure(figsize=(15, 4));\n\n# plt.subplot(1, 3, 1);\n# librosa.display.specshow(mel, sr=sr, hop_length=hop_length, x_axis='linear');\n# plt.ylabel('Mel filter');\n# plt.colorbar();\n# plt.title('1. Our filter bank for converting from Hz to mels.');\n\n# plt.subplot(1, 3, 2);\n# mel_10 = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=10)\n# librosa.display.specshow(mel_10, sr=sr, hop_length=hop_length, x_axis='linear');\n# plt.ylabel('Mel filter');\n# plt.colorbar();\n# plt.title('2. Easier to see what is happening with only 10 mels.');\n\n# plt.subplot(1, 3, 3);\n# idxs_to_plot = [0, 9, 49, 99, 127]\n# for i in idxs_to_plot:\n#     plt.plot(mel[i]);\n# plt.legend(labels=[f'{i+1}' for i in idxs_to_plot]);\n# plt.title('3. Plotting some triangular filters separately.');\n\n# plt.tight_layout();","3b6d7607":"train = pd.read_csv('..\/input\/freesound-audio-tagging\/train.csv')\nx = pd.read_csv('..\/input\/freesound-audio-tagging\/train_post_competition.csv')\ny = pd.read_csv('..\/input\/freesound-audio-tagging\/test_post_competition.csv')\nsample_sub = pd.read_csv('..\/input\/freesound-audio-tagging\/sample_submission.csv')","9093b35f":"train.head()","b563f163":"x.head()","0350e1c9":"y.head()","1611ec89":"sample_sub.head()","3140cfbf":"## Run this and verify the graph if you want.\n# train['label'].value_counts()","749659ea":"train['label'].nunique()","3b84820d":"a,b = np.unique(train.label,return_counts = True)","85e83bb9":"trace = go.Bar(x = a, y = b)\ndata = [trace]\nlayout = {\"title\":\"Number of Labels for Each category.\",\n         \"xaxis\":{\"title\":\"Categories\",\"tickangle\":90},\n         \"yaxis\":{\"title\":\"Amount of labels\"}}\nfig = go.Figure(data = data,layout=layout)\niplot(fig)","0c9e338c":"\n## Need to combine the values in DataFrame for plotting.\nd = {'Labels': a, 'Samples':b}\nnew = pd.DataFrame(data = d)\n\n\n## Visualizing the Data. \n## px: Plotly Express.\n\nfig = px.bar(new[['Labels', 'Samples']].sort_values('Samples', ascending = True), \n             y = \"Samples\", x = \"Labels\", color = 'Labels', \n             log_y=True, template='ggplot2', title = \"Number of Labels for Each category.\")\nfig.show()","71f145a8":"fname = '..\/input\/freesound-audio-tagging\/audio_train\/' + '00044347.wav'\nipd.Audio(fname)","fbaedabd":"fname = '..\/input\/freesound-audio-tagging\/audio_train\/' + '0033e230.wav'   \nipd.Audio(fname)","18f633f4":"def plot_signals(signals):\n    fig, axes = plt.subplots(nrows = 8, ncols = 5, sharex = False, \n                            sharey = True, figsize = (20,15))\n    fig.suptitle('Time Series', size = 16)\n    i = 0\n    for x in range(8):\n        for y in range(5):\n            axes[x,y].set_title(list(signals.keys())[i])\n            axes[x,y].plot(list(signals.values())[i])\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i += 1\n            \ndef plot_fft(fft):\n    fig, axes = plt.subplots(nrows = 8, ncols = 5, sharex = False, \n                            sharey = True, figsize = (20,15))\n    fig.suptitle('Fourier Transform', size = 16)\n    i = 0\n    for x in range(8):\n        for y in range(5):\n            data = list(fft.values())[i]\n            Y,freq = data[0], data[1]\n            axes[x,y].set_title(list(fft.keys())[i])\n            axes[x,y].plot(freq, Y)\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i += 1\n            \ndef plot_fbank(fbank):\n    fig, axes = plt.subplots(nrows = 8, ncols = 5, sharex = False, \n                            sharey = True, figsize = (20,15))\n    fig.suptitle('Filter Bank Coefficients', size = 16)\n    i = 0\n    for x in range(8):\n        for y in range(5):\n            axes[x,y].set_title(list(fbank.keys())[i])\n            axes[x,y].imshow(list(fbank.values())[i],\n                            cmap = 'hot', interpolation = 'nearest')\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i += 1\n            \ndef plot_mfccs(mfccs):\n    fig, axes = plt.subplots(nrows = 8, ncols = 5, sharex = False, \n                            sharey = True, figsize = (20,15))\n    fig.suptitle('Mel Frequency Cepstrum Coefficients', size = 16)\n    i = 0\n    for x in range(8):\n        for y in range(5):\n            axes[x,y].set_title(list(mfccs.keys())[i])\n            axes[x,y].imshow(list(mfccs.values())[i],\n                            cmap = 'hot', interpolation = 'nearest')\n            axes[x,y].get_xaxis().set_visible(False)\n            axes[x,y].get_yaxis().set_visible(False)\n            i += 1","a72a99cb":"def calc_fft(y,rate):\n    n = len(y)\n    freq = np.fft.rfftfreq(n, d = 1\/rate)\n    Y = abs(np.fft.rfft(y)\/n)\n    return (Y, freq)\n\nclasses = list(np.unique(train.label))\n\n\nsignals = {}\nfft = {}\nfbank = {}\nmfccs = {}\n\nfor c in classes:\n    wav_file = train[train.label == c].iloc[0,0]\n    signal, rate = librosa.load('..\/input\/freesound-audio-tagging\/audio_train\/' + wav_file, sr = 44100)\n    signals[c] = signal\n    fft[c] = calc_fft(signal, rate)\n    \n    bank = logfbank(signal[:rate], rate, nfilt = 26, nfft = 1103).T\n    fbank[c] = bank\n    mel = mfcc(signal[:rate], rate, numcep = 13, nfilt = 26, nfft = 1103).T\n    mfccs[c] = mel\n    \n    \nplot_signals(signals)\nplt.show()","5b71f5bb":"plot_fft(fft)\nplt.show()","f28bbd91":"plot_fbank(fbank)\nplt.show()","4f00d4fd":"plot_mfccs(mfccs)\nplt.show()","99e5dce1":"def envelope(y, rate, threshold):\n    mask = []\n    y = pd.Series(y).apply(np.abs)\n    y_mean = y.rolling(window = int(rate\/10), min_periods = 1, center = True).mean()\n    for mean in y_mean:\n        if mean > threshold:\n            mask.append(True)\n        else:\n            mask.append(False)\n    return mask","5f195398":"for c in classes:\n    wav_file = train[train.label == c].iloc[0,0]\n    signal, rate = librosa.load('..\/input\/freesound-audio-tagging\/audio_train\/' + wav_file, sr = 44100)\n    mask = envelope(signal, rate, 0.0005)\n    signal = signal[mask]\n    \n    signals[c] = signal\n    fft[c] = calc_fft(signal, rate)\n    \n    bank = logfbank(signal[:rate], rate, nfilt = 26, nfft = 1103).T\n    fbank[c] = bank\n    mel = mfcc(signal[:rate], rate, numcep = 13, nfilt = 26, nfft = 1103).T\n    mfccs[c] = mel\n    \n    \nplot_signals(signals)\nplt.show()","a76ea961":"## Audio Analysis.\n\n1. **FFT**: A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). Fourier analysis    converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. \n\n5. **MFCCS** : Mel Frequency cepstral coefficients","44fdac1c":"### [Mel-frequency cepstrum](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum)\n\n- In sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency.\n\n- **Mel-frequency cepstral coefficients (MFCCs)** are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\"). \n\n- The difference between the cepstrum and the mel-frequency cepstrum is that in the MFC, **the frequency bands are equally spaced on the mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum.**\n\n#### [What is Mel Scale ?](https:\/\/en.wikipedia.org\/wiki\/Mel_scale)\n\n![mel-scale.gif](attachment:mel-scale.gif)\n\n- The mel scale, named by Stevens, Volkmann, and Newman in 1937 is a perceptual scale of pitches judged by listeners to be equal in distance from one another. \n\n- The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. \n\n- The name mel comes from the word melody to indicate that the scale is based on pitch comparisons.\n\n- See the comparison of mel scale with frequency in heartz in the image above to get a sense of the scale parameters. \n\n#### MFCCs are commonly derived as follows:\n\n1. Take the Fourier transform of (a windowed excerpt of) a signal.\n2. Map the powers of the spectrum obtained above onto the mel scale, using triangular overlapping windows.\n3. Take the logs of the powers at each of the mel frequencies.\n4. Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n5. The MFCCs are the amplitudes of the resulting spectrum.\n6.  There can be variations on this process, for example: differences in the shape or spacing of the windows used to map the scale,or addition of dynamics features such as \"delta\" and \"delta-delta\" (first- and second-order frame-to-frame difference) coefficients.\n\n![Mel-filter-banks.png](attachment:Mel-filter-banks.png)","6dd1fbaf":"### Basics of Librosa.","3c28e529":"![cover.jpg](attachment:cover.jpg)\n\n","0060fbf4":"### References:\n\n1. [Beginner's Guide to Audio Data](https:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data)\n\n2. [Deep Learning for Audio Classification](https:\/\/youtu.be\/Z7YM-HAz-IY)\n\n3. [Practical Cryptography](http:\/\/practicalcryptography.com\/miscellaneous\/machine-learning\/guide-mel-frequency-cepstral-coefficients-mfccs\/)\n\n4. [source of cover image](https:\/\/www.freepik.com\/premium-vector\/sound-wave-display-equalizer-rainbow-color-illustration-about-dynamic-audio-from-electronic-equipment_5840116.htm)\n\n5. [source of Mel filter bank image](https:\/\/www.researchgate.net\/figure\/Mel-filter-banks-basis-functions-using-20-Mel-filters-in-the-filter-bank_fig1_288632263)\n\n6. [Python_specch_features Documentation](https:\/\/python-speech-features.readthedocs.io\/en\/latest\/)\n\n7. [librosa Github](https:\/\/github.com\/librosa\/librosa)","f68ffa9c":"### Cleaning Audio Files","4e3bf4ca":"### Importing the Libraries","a06ed6a7":"**There's a lot of class imbalance.**","9e9f4d31":"## Classification of Sound.\n\n- This kernel is an introduction to working with sound data and it's classification using Convolutional Neural Network. It's just a basic kernel\nwhich will guide you to how to get started with sound data. The kernel will be constantly updated.\n\n- The main aim is to create a satisfactory model and make some decent predictions.\n\n- The Basics of Librosa isn't complete yet, so let it be please.\n\n","05c36e3f":"### Reading Audio Samples.","7fa3d4b8":"**Traditionally while creating this for production the dead noises should be cleaned out before using these files for modelling. By dead noises I mean the trail (Straight dead lines in the graph) of the graph.**","0cb6eea4":"**These here are somewhat clean and has the important part which can help in classification.**\n\n- These file should be stored and used for modelling.","4aa0c870":"## Getting into DATA."}}