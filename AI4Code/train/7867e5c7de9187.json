{"cell_type":{"79b51af0":"code","226d2869":"code","dd71b28f":"code","11e25879":"code","e85bb78f":"code","b9370e53":"code","81b66570":"code","d145b887":"code","2665ef6c":"code","927aed37":"code","44991a4a":"code","b344ac3f":"code","db7ceabd":"code","91eef9e9":"code","25823e0b":"code","7ef0a32c":"code","0badf1e7":"code","c50dc8d7":"code","64b2e70b":"code","72492f69":"code","4fef6dbf":"code","f1dd83cb":"code","fe978b2d":"code","f3191bd8":"code","4a5f3297":"code","52b22c81":"code","c658fe95":"code","e6876832":"code","8868897e":"code","1697d97e":"markdown","f711d1ea":"markdown","4f3b7515":"markdown","6af1ee97":"markdown","c88846bf":"markdown","7823e646":"markdown","630cd08c":"markdown","f6a92fb0":"markdown","4cc92910":"markdown","cdf22a3e":"markdown","1361c85e":"markdown","88509832":"markdown","7ffd1634":"markdown","eee0b251":"markdown","7f213089":"markdown","6073d82a":"markdown","f263516e":"markdown","6cf4bf8b":"markdown","f568304c":"markdown","6e03afd4":"markdown","e8de60d2":"markdown","42524131":"markdown","4b62234d":"markdown","c520183a":"markdown","1247c200":"markdown"},"source":{"79b51af0":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np","226d2869":"df = pd.read_csv('..\/input\/mammographic-mass-data-set\/Cleaned_data.csv', delimiter = \",\")\n","dd71b28f":"df.shape\n","11e25879":"df.head()\n","e85bb78f":"df_mam = df.copy()","b9370e53":"# Check for null values\nmissing_values_train = df_mam.isnull().sum()\nmissing_values_train = missing_values_train.to_frame(name='num_missing')\nmissing_values_train['perc_missing'] = (missing_values_train['num_missing']\/df_mam.shape[0])*100\nfor index, row in missing_values_train.iterrows():\n    if (row['num_missing'] > 0):\n        print (\"For \\\"%s\\\" the number of missing values are: %d (%.0f%%)\" %  (index,\n                                                                     row['num_missing'],\n                                                                    row['perc_missing']))","81b66570":"# Rename columns to align with our data dictionary.\ndf_mam = df_mam.rename(columns = {'BI-RADS': 'score', 'Age': 'age', 'Shape': 'shape', 'Margin': 'margin', 'Density':\n    'density', 'Severity': 'malignanttrue'})\ndf_mam_orig = df_mam.copy()\n\ncategorical_cols = ['shape', 'margin']\ndf_mam = pd.get_dummies(df_mam, columns = categorical_cols, drop_first=True)\ncols = list(df_mam.columns.values)\nquantitative_cols = ['score', 'shape', 'margin', 'density']\n\n# Re-order columns\ncol_names = ['malignanttrue', 'score', 'age', 'shape_2', 'shape_3', 'shape_4', 'margin_2', 'margin_3', 'margin_4',\n             'margin_5', 'density']\ndf_mam = df_mam[col_names]","d145b887":"df_mam.sample(10, random_state=0)","2665ef6c":"# Continuous density plot\nfig_missing, axes = plt.subplots(1, 1, figsize=(10, 8))\n\n# Plot frequency plot\/ histogram\n_ = sns.histplot(x=\"age\", kde=True, data=df_mam, ax=axes, bins=40);\n_ = axes.set(xlabel=\"Age\", ylabel='Density');\naxes.xaxis.label.set_size(18)\naxes.yaxis.label.set_size(18)\naxes.tick_params('y', labelsize = 14);\naxes.tick_params('x', labelsize = 14);","927aed37":"df_mam['score'].value_counts()","44991a4a":"idx = df_mam[df_mam['score'] == 55].index\ndf_mam.loc[idx, 'score'] = 5","b344ac3f":"df_mam['score'].value_counts()","db7ceabd":"df_mam = df_mam.sort_values(['age', 'score', 'density', 'malignanttrue'], ascending=[True, True, True, True])\ndf_mam = df_mam.reset_index(drop=True)","91eef9e9":"_ = plt.figure()\n\n# Plot outcome counts.a\noutcome_counts = df_mam_orig['malignanttrue'].value_counts(normalize = True)\nlegend_labels = ['Benign', 'Malignant']\n\n# change the background bar colors to be light grey\nbars = plt.bar(outcome_counts.index, outcome_counts.values, align='center', linewidth=0,\n               color='lightslategrey')\n# make one bar, the survived bar, a contrasting color\nbars[1].set_color('#1F77B4')\n\n# soften all labels by turning grey\n_ = plt.xticks(outcome_counts.index, legend_labels, fontsize=15, alpha=0.8)\n_ = plt.title('Diagnostic Outcome', fontsize=15, pad=30, alpha=0.8)\n\n# remove all the ticks (both axes), and tick labels on the Y axis\nplt.tick_params(top=False, bottom=False, left=False, right=False, labelleft=False,\n                labelbottom=True)\n\n# Remove the frame - my method\nax = plt.gca()\nax.set_frame_on(False)\n\n# Remove the frame of the chart - instructor's method\n#for spine in plt.gca().spines.values():\n#    spine.set_visible(False)\n\n# direct label each bar with Y axis values\nfor bar in bars:\n    _ = plt.gca().text(bar.get_x() + bar.get_width()\/2, bar.get_height() - 0.05,\n                       str(round((bar.get_height()*100))) + '%', ha='center', color='w',\n                       fontsize=15)\n\nplt.show()","25823e0b":"df_mam['malignanttrue'].value_counts()","7ef0a32c":"# Bar chart plot of categorical variables.\nfig, ax = plt.subplots(2, 2, figsize=(15, 13));\nfor variable, subplot in zip(quantitative_cols, ax.flatten()):\n    subplot.xaxis.label.set_size(20)\n    subplot.yaxis.label.set_size(20)\n    subplot.tick_params('y', labelsize = 20);\n    subplot.tick_params('x', labelsize = 20);\n    cp = sns.countplot(x=df_mam_orig[variable], ax=subplot, palette='deep', order =\n    df_mam_orig[variable].value_counts().index);\nplt.tight_layout()","0badf1e7":"X = df_mam.iloc[:, np.r_[1:11]]\ny = df_mam.iloc[:, 0]","c50dc8d7":"y = np.ravel(y)\n\n# Split the data into the training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","64b2e70b":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)  \nX_test = scaler.transform(X_test)","72492f69":"col_names = list(X.columns.values)\nX_train = pd.DataFrame(X_train, columns=col_names)","4fef6dbf":"X_train.head()\n","f1dd83cb":"reg = MLPClassifier(max_iter=1000, hidden_layer_sizes=(5,5), random_state=1)\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\naccuracy_score(y_pred,y_test)","fe978b2d":"validation_scores = {}\nprint(\"Nodes |Validation\")\nprint(\"      | score\")\n\nfor hidden_layer_size in [(i,j) for i in range(3,6) for j in range(3,6)]:\n\n    reg = MLPClassifier(max_iter=10000, hidden_layer_sizes=hidden_layer_size, random_state=1)\n\n    score = cross_val_score(estimator=reg, X=X_train, y=y_train, cv=2)\n    validation_scores[hidden_layer_size] = score.mean()\n    print(hidden_layer_size, \": %0.5f\" % validation_scores[hidden_layer_size])","f3191bd8":"# Check scores\nprint(\"The highest validation score is: %0.4f\" % max(validation_scores.values()))  \noptimal_hidden_layer_size = [name for name, score in validation_scores.items() \n                              if score==max(validation_scores.values())][0]\nprint(\"This corresponds to nodes\", optimal_hidden_layer_size )","4a5f3297":"clf_best = MLPClassifier(max_iter=1000, hidden_layer_sizes=optimal_hidden_layer_size, random_state=1)\nclf_best.fit(X_train, y_train)","52b22c81":"y_pred = clf_best.predict(X_test)\naccuracy_score(y_pred,y_test)","c658fe95":"log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\nrnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\nsvm_clf = SVC(gamma=\"auto\", random_state=42)\nmodel_name = ['LogisticRegression', 'RandomForestClassifier', 'SVC', 'Final']\n\nvoting_clf = VotingClassifier(\n    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n    voting='hard')\n\nvoting_clf.fit(X_train, y_train)\n\naccuracy = []\nfor clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    acc_score = accuracy_score(y_test, y_pred)\n    accuracy.append(acc_score)\n    print(clf.__class__.__name__, acc_score)","e6876832":"plt.figure(figsize = (10,5))\nsns.barplot(x = accuracy, y = model_name, palette='deep')\nplt.title(\"Voting Classifier Results\", fontsize=16)","8868897e":"X_design = X.copy()\nX_design_vec = pd.DataFrame(X_design.mean()).transpose()\n\nmargin= range(4,9)\n\nfor i in margin:\n    X_design_vec.loc[0,[\"margin_2\",\"margin_3\",\"margin_4\",\"margin_5\"]]=[0,0,0,0]\n    if i!=4:\n        X_design_vec.iloc[0,i]=1\n\n    min_resultant = min(X.loc[:,\"age\"])\n    max_resultant = max(X.loc[:,\"age\"])\n    seq = np.linspace(start=min_resultant,stop=max_resultant,num=50)\n\n    to_predict = []\n    for result in seq:\n        X_design_vec.loc[0,\"age\"] = result\n        to_predict.append(X_design_vec.copy())\n\n    to_predict = pd.concat(to_predict)\n\n    to_predict = scaler.transform(to_predict)\n    predictions = clf_best.predict_proba(to_predict)\n    plt.plot(seq,predictions[:,1])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Probability of malignancy\")\nplt.title(\"Probability of malignancy vs. Age with alternate margins\")\nplt.legend(labels=[\"circumscribed\",\"microlobulated\",\"obscured\",\"ill-defined\",\"spiculated\"],loc='best')\nplt.show()\n","1697d97e":"The curves clearly indicate an increase in probability of cancer as age increase, which makes sense. The model\ntherefore captures this signal correctly, which is a good confirmation. Interesting to see the difference in\nrelationship with the response for the different margin variables. The margins all converge to similar probabilities\nfor older ages. The spiculated margin is associated more strongly with cancer in younger patients.\nThe circumscribed and microlobulated margins have a much higher probability for older ages than any of the others,\neven though it is the lowest for lower ages. Clearly strong age related predictors.\n\nThis analysis showed some interesting results based on a very few variables. More EDA is certain to show more\ninteresting patterns in the data.\n\nOn to the next Notebook!","f711d1ea":"We can see that there is a strong signal in the data. A score of 86% for an unoptimised MLP is very good. The model\nis probably overfit, so we will do some high-level tests on number of nodes per layer. We will do a more advanced\nhyper-parameter optimisation exercise using GridSearch in a later Notebook.","4f3b7515":"We now build a baseline model followed by setting up a Voting Classifier.","6af1ee97":"Data is not sorted and does not have a meaningful index for baseline. We sort data and create new index.","c88846bf":"As expected the model was overfit. The reduced number of nodes is an indication of better fit on the validation data\nby a less complex model.","7823e646":"The dataset looks as follows now:","630cd08c":"Let us have a look at the original data:","f6a92fb0":"<div class=\"alert alert-block alert-info\">\n<b>Load data<\/b>\n<\/div>","4cc92910":"Next, fit a two-layer MLP classifier with 5 nodes on each layer.","cdf22a3e":"There is good variation in all the categorical variables. We will perform a more in depth analysis later.","1361c85e":"<div class=\"alert alert-block alert-info\">\n<b>Missing values<\/b>\n<\/div>","88509832":"The accuracy score on the optimised model slightly reduced. This is not a bad thing as the dataset is small and a\nmore robust model is preferred over a more accurate overfit model.\n\nWe will now fit a voting classifier to compare our MLP results with a few other models. Just to compare to a few\nother models.","7ffd1634":"There are no missing values! We got lucky with this dataset.\n\nWe start by renaming columns to align with our data dictionary and then we One Hot Encode categorical variables.","eee0b251":"Next we consider the categorical variables.\n\nWe first look at the distribution of the response.","7f213089":"We scale the data next.","6073d82a":"<div class=\"alert alert-block alert-info\">\n<b>Exploration of data<\/b>\n<\/div>","f263516e":"Now we analyse the distribution for patient age to get a feel for the data.","6cf4bf8b":"We observe that one value was probably incorrectly captured (as this score does not exist), we map the value of 55 to\n 5. ","f568304c":"Data seems fairly reasonable, no outliers. We will continue with analysis for now, and do a more in-depth EDA in a\nlater notebook.\n\n","6e03afd4":"We quantify the exact number of missing values in the data set:","e8de60d2":"The Support Vector Machine narrowly won over the Logistic Regression. There seems to be a fairly linear\nrelationship between features and responses. Our future EDA will probably confirm this.\n\nVoting Classifiers enable an easy way of quickly comparing various models. A very useful tool indeed!\n\nWe now draw the response curves, based on our optimised MLP results.","42524131":"<div class=\"alert alert-block alert-info\">\n<b>Build Models<\/b>\n<\/div>","4b62234d":"The scaled dataset looks as follows:","c520183a":"# I. Breast Cancer Screening ML Model - Step by Step\n### Exploration of the use of Machine Learning to improve mammographic screening.\n\nFor this analysis we use the Mammographic Mass Data Set found on the Kaggle Data Repository at the following location:\n\n[Mammographic Mass Data Set](https:\/\/www.kaggle.com\/overratedgman\/mammographic-mass-data-set)\n\nThis dataset is the result of the work performed to investigate how machine learning can be used for breast cancer\nscreening by Elter, Schulz-Wendtland, and Wittenberg (2007).\n\nMammography is widely accepted as the most effective screening tool to diagnose breast cancer at the moment. However,\n 5% to 10% of mammograms are referred for further investigation by way of biopsy due to inconclusive or abnormal\n results. It is this 5 - 10% of cases that ML is seeking to address. By using the Breast Imaging-Reporting and Data\n System (BI-RADS) and other characteristics of mammographs, the study aimed to improve mammographic screening with\n machine learning techniques in order to reduce the 5 - 10% screening referrals (Elter, Schulz-Wendtland & Wittenberg,\n 2007).\n\nThe Data Dictionary for this dataset is as follows:\n\n| Variable | Definition | Type |\n|----------|------------|-----|\n| score | BI-RADS assessment: 0\u20135\t| Ordinal |\n| age | Patient\u2019s age in years  | Continuous |\n| shape | Mass shape: round=1, oval=2, lobular=3, and irregular=4 | Categorical |\n| margin | Mass margin: circumscribed=1, microlobulated=2, obscured=3, ill-defined=4, and spiculated=5 | Categorical |\n| density | Mass density: high=1, iso=2, low=3, and fat-containing=4 | Ordinal |\n| malignant | benign=FALSE and malignant=TRUE | Boolean |\n\nIn this notebook we will do some high level analyses, implement ML pipelines for simple ML model comparison and\ninvestigate the use of Response Curve otherwise known as Partial Dependency Plots for investigation of the\nrelationship between various features and the responses. The PDPs also indicate how well the model fits the data from\n the perspective of different features, and is hence useful from two different perspectives.","1247c200":"We observe that the classes are well balanced. No need for boosting."}}