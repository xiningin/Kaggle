{"cell_type":{"6b4926ff":"code","f34c9d9f":"code","86d28e01":"code","0943763b":"code","69bb1ffc":"code","b9f5d8e5":"code","ee5abc1d":"code","9787a78c":"code","19b087aa":"code","b4510ecf":"code","6d566398":"code","42a6739f":"code","ad271f7e":"code","6ab14420":"code","35d3a631":"code","14c911eb":"code","0a69012e":"code","50c01bba":"code","05c9b9bd":"code","7bdda15f":"code","5ad8932a":"code","85bd566a":"code","642ef6be":"code","f51674df":"code","fdae0e3e":"code","ef16952c":"code","2c50fba3":"code","9a350fef":"code","72643742":"code","42276ebe":"code","c2d4db3f":"code","10200c06":"code","32b9c5c0":"code","7901521b":"markdown","01b400d7":"markdown","683557d3":"markdown","dc9a38c0":"markdown","ae9bf387":"markdown","55326d8c":"markdown","b209609b":"markdown","49d9d8f7":"markdown","9d8d1eec":"markdown","01e9ad9a":"markdown","0886597f":"markdown","7370ddad":"markdown","ef67c40f":"markdown","735108e4":"markdown","3f531070":"markdown","c33de16c":"markdown","e03a78af":"markdown","d2503184":"markdown","c6fdbeea":"markdown","2106c3f5":"markdown","a30b78ba":"markdown","f8e61935":"markdown","981a061f":"markdown","d7b5a6e3":"markdown","818df632":"markdown","681e62b8":"markdown"},"source":{"6b4926ff":"# Basic Packages\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# plotting packages\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# machine learning packages\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection, preprocessing, metrics\nfrom sklearn.preprocessing import Imputer\npd.set_option('display.max_columns',None)\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import linear_model","f34c9d9f":"dtypes = {'feature_1':'int16',\n          'feature_2':'int16',\n          'feature_3':'int16'\n         }\n\ntrain_df = pd.read_csv(\"..\/input\/train.csv\",dtype=dtypes,parse_dates=['first_active_month'])\ntest_df = pd.read_csv(\"..\/input\/test.csv\",dtype=dtypes,parse_dates=['first_active_month'])\n\nprint(f\"Training Set Shape:{train_df.shape}\")\nprint(f\"Test Set Shape:{test_df.shape}\")","86d28e01":"# Using smaller data types reduces the memory usage by ~50%\ndata_types = {'authorized_flag':'str',\n                   'card_id':'str',\n                   'city_id':'int16',\n                   'category_1':'str',\n                   'installments':'int16',\n                   'category_3':'str',\n                   'merchant_category_id':'int16',\n                   'merchant_id':'str',\n                   'purchase_amount':'float',\n                   'category_2':'str',\n                   'state_id':'int16',\n                   'subsector_id':'int16'}\n\nhist_df = pd.read_csv(\"..\/input\/historical_transactions.csv\",dtype=data_types,parse_dates=True)\nnew_trans_df = pd.read_csv('..\/input\/new_merchant_transactions.csv',dtype=data_types,parse_dates=True)","0943763b":"last_hist_date = datetime.datetime(2018,2,28)\nfor df in [new_trans_df,hist_df]:\n    print(f'Preprocessing DataFrame...')\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1,'N':0}).astype('bool')\n    df['category_1'] = df['category_1'].map({'Y':1,'N':0}).astype('bool')\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['time_since_purchase_date'] = (last_hist_date-df['purchase_date']).dt.days\n    #df.loc[:,'purchase_date'] = pd.DatetimeIndex(df['purchase_date']).astype(np.int64)*1**(-9)\n    df['installments'] = df['installments'].replace(999,-1)\n    null_cols = ['city_id','state_id','subsector_id','installments']\n    nan_cols = ['city_id','state_id','subsector_id','installments','merchant_id','category_3','category_2']\n    \n    # Identify -1 values as nans\n    for col in null_cols:\n        df[col] = df[col].replace(-1,np.nan)\n    \n    # Fill categorical nan values with mode\n    for column in nan_cols:\n        fill = df.loc[:,column].mode().values[0]\n        df[column].fillna(fill,inplace=True)","69bb1ffc":"print('Encoding Date Times...')\nfor df in [hist_df,new_trans_df]:\n    print('...')\n    df['year'] = df['purchase_date'].dt.year.astype('int16')\n    df['weekofyear'] = df['purchase_date'].dt.month.astype('int16')\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype('bool')\n    df['hour'] = df['purchase_date'].dt.hour.astype('int16')\n\n    #https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/73244\n    df['month_diff'] = (((datetime.datetime.today()-df['purchase_date']).dt.days)\/\/30).astype('int16')\n    df['month_diff'] += df['month_lag']","b9f5d8e5":"# Aggregating Historical Transactions DataFrame by card_id\n\nprint('Aggregating Historical Transactions...')\n\n# Create dictionary of column names and aggregation functions to use\nagg_func = {'authorized_flag' : ['mean'],\n            'city_id' : ['nunique'],\n            'category_1' : ['sum','mean'],\n            'installments': ['sum','min','max','var','mean'],\n            'category_3' : ['nunique'],\n            'merchant_category_id':['nunique'],\n            'merchant_id':['nunique'],\n            'purchase_amount':['sum','mean','max','min','var'],\n            'purchase_date':['max','min'],\n            'time_since_purchase_date':['min','max','mean'],\n            'category_2':['nunique'],\n            'weekend':['sum','mean'],\n            'month_lag':['min','max','mean','var'],\n            'month_diff':['mean','var']\n           }\n\n# Aggregate columns based on dictionary passed to agg function\nghist_df = hist_df.groupby(['card_id']).agg(agg_func)\n\n# Rename columns before joining train\/test set\nghist_df.columns = ['hist_'+'_'.join(col).strip() for col in ghist_df.columns.values]\nghist_df.head()","ee5abc1d":"# Aggregate Columns based on Dictionary for new_trans_df\nprint('Aggregating New Transactions DataFrame...')\n\ngnew_trans_df = new_trans_df.groupby(['card_id']).agg(agg_func)\n\n# Rename columns before joining train \/ test set\ngnew_trans_df.columns = ['new_'+'_'.join(col).strip() for col in gnew_trans_df.columns.values]\ngnew_trans_df.head()","9787a78c":"# Merge with train and test set\nprint('Merging with training set...')\ntrain = pd.merge(train_df,ghist_df,on='card_id',how='left')\ntrain = pd.merge(train,gnew_trans_df,on='card_id',how='left')\n\nprint('Merging with testing set...')\ntest = pd.merge(test_df,ghist_df,on='card_id',how='left')\ntest = pd.merge(test,gnew_trans_df,on='card_id',how='left')","19b087aa":"for df in [train,test]:\n    df['hist_purchase_date_uptonow'] = (datetime.datetime.today() - \n                                      df['hist_purchase_date_max']).dt.days\n    df['new_purchase_date_uptonow'] = (datetime.datetime.today() - \n                                      df['new_purchase_date_max']).dt.days\n    \n    dt_features = ['hist_purchase_date_max','hist_purchase_date_min',\n               'new_purchase_date_max','new_purchase_date_min']\n    \n    # Models cannot use datetime features so they are encoded here as int64s\n    for feature in dt_features:\n        df[feature] = df[feature].astype(np.int64)*1e-9","b4510ecf":"# Final Train and Test Sets\ndisplay(train.head())\ndisplay(test.head())","6d566398":"# Encoding Date times for first_active_month\nfor df in [train,test]:\n    df['first_month'] = df['first_active_month'].dt.month\n    df['first_year'] = df['first_active_month'].dt.year\n    df.drop(columns = ['first_active_month'],inplace=True)","42a6739f":"# Dealing with outliers\ntrain['outliers'] = 0\ntrain.loc[train['target'] < -30,'outliers'] = 1\ntrain['outliers'].value_counts()","ad271f7e":"target_col = train['target']\n\nfeatures = [name for name in train.columns if name not in ['target','card_id','new_authorized_flag_mean','outliers']]\n\ntarget = train['target']\ndel train['target']","6ab14420":"# Fill Nan Columns\nfiller = Imputer()\ntrain.loc[:,features] = filler.fit_transform(train[features].values)\ntest.loc[:,features] = filler.transform(test[features].values)","35d3a631":"# Set lgbm model params\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 30,\n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"boosting\": \"gbdt\",\n         \"min_child_samples\":20,\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"random_state\": 133,\n         \"nthread\":4,\n         \"verbosity\": -1}\n\n# Validating on a stratified outlier data set to give more consistent cv across the folds\n# https:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\n\nfolds = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=15)\nlgbm_oof = np.zeros(len(train))\nlgbm_pred = np.zeros(len(test))\n\nfor fold_, (train_index,valid_index) in enumerate(folds.split(train,train['outliers'].values)):\n    print(f\"fold number: {fold_ + 1}\")\n    \n    train_data = lgb.Dataset(train.iloc[train_index][features],label=target.iloc[train_index])\n    val_data = lgb.Dataset(train.iloc[valid_index][features],label=target.iloc[valid_index])\n    num_rounds = 10000\n    clf = lgb.train(param,\n                    train_data,\n                    num_rounds,\n                    valid_sets=[train_data,val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds=200)\n    lgbm_oof[valid_index] = clf.predict(train.iloc[valid_index][features],num_iteration=clf.best_iteration)\n    lgbm_pred += clf.predict(test[features],num_iteration=clf.best_iteration)\/folds.n_splits","14c911eb":"np.sqrt(mean_squared_error(lgbm_oof, target))","0a69012e":"##xgb model\nxgb_params = {\n    'booster': 'gbtree',\n    'objective': 'reg:linear',\n    'gamma': 0.1,\n    'max_depth': 6,\n    'eval_metric':'rmse',\n    'lambda': 2,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'min_child_weight': 3,\n    'silent': 1,\n    'eta': 0.1,\n    'seed': 1000,\n    'nthread': 4,\n}\n\nfolds= model_selection.KFold(n_splits=5, shuffle=True, random_state=15)\nxgb_oof = np.zeros(len(train))\nxgb_pred = np.zeros(len(test))\n\nfor fold_,(train_index,valid_index) in enumerate(folds.split(train.values, train['outliers'].values)):\n    print(\"fold: {}\/5\".format(fold_+1))\n    start = time.time()\n    train_data = xgb.DMatrix(train.iloc[train_index][features],\n                           label=target.iloc[train_index])\n    valid_data = xgb.DMatrix(train.iloc[valid_index][features],\n                           label=target.iloc[valid_index])\n    \n    xgb_evals = [(train_data, 'train'), (valid_data, 'valid')]\n    num_rounds = 2000\n    xgb_model = xgb.train(xgb_params, train_data, num_rounds, xgb_evals, early_stopping_rounds=50, verbose_eval=1000)\n    xgb_oof[valid_index] = xgb_model.predict(xgb.DMatrix(train.iloc[valid_index][features]), ntree_limit=xgb_model.best_ntree_limit+50)\n    xgb_pred += xgb_model.predict(xgb.DMatrix(test[features]), ntree_limit=xgb_model.best_ntree_limit+50) \/ folds.n_splits\n    print(f\"fold n\u00b0{fold_+1}\/5 completed after: {time.time()-start:.2f} seconds\",'\\n')","50c01bba":"print(np.sqrt(mean_squared_error(xgb_oof, target)))","05c9b9bd":"# model trainer for sklearn pipeline\ndef sk_trainer(model):\n    folds = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=15)\n    oof = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n\n    for fold_, (train_index,valid_index) in enumerate(folds.split(train,train['outliers'].values)):\n        print(f\"fold: {fold_ + 1}\/5...\")\n        start = time.time()\n        train_x = train.iloc[train_index][features]\n        train_y=target.iloc[train_index]\n        val_x = train.iloc[valid_index][features]\n        val_y = target.iloc[valid_index]\n        model.fit(train_x,train_y)\n        oof[valid_index] = model.predict(val_x)\n        predictions += model.predict(test[features])\/folds.n_splits\n        print(f\"~~~fold {fold_ +1} completed after: {time.time()-start:.2f} seconds~~~\")\n    return oof, predictions, model","7bdda15f":"rf_model = RandomForestRegressor(n_estimators=50,max_depth=10)\nrf_oof, rf_pred, rf_model = sk_trainer(rf_model)","5ad8932a":"print('CV Score:',np.sqrt(mean_squared_error(rf_oof, target)))","85bd566a":"averaged_oof = (rf_oof+lgbm_oof+xgb_oof)\/3\naveraged_pred = (rf_pred+lgbm_pred+xgb_pred)\/3\nprint('CV Score:',np.sqrt(mean_squared_error(averaged_oof, target)))","642ef6be":"x = pd.DataFrame()\nx['lgbm'] = lgbm_oof\nx['rf'] = rf_oof\nx['xgb'] = xgb_oof\n\ntest_pred = pd.DataFrame()\ntest_pred['lgbm'] = lgbm_pred\ntest_pred['rf'] = rf_pred\ntest_pred['xgb'] = xgb_pred","f51674df":"def level_2_trainer(model):\n    folds = model_selection.StratifiedKFold(n_splits=5,shuffle=True,random_state=10)\n    oof_normal = np.zeros(len(train))\n    predictions_normal = np.zeros(len(test))\n\n    for fold_, (train_index,valid_index) in enumerate(folds.split(train,train['outliers'].values)):\n        print(f\"fold number: {fold_ + 1}...\")\n        start = time.time()\n        train_x = x.iloc[train_index]\n        train_y=target.iloc[train_index]\n        val_x = x.iloc[valid_index]\n        val_y = target.iloc[valid_index]\n        model.fit(train_x,train_y)\n        oof_normal[valid_index] = model.predict(val_x)\n        predictions_normal += model.predict(test_pred)\/folds.n_splits\n        print(f\"fold{fold_ +1} completed after {time.time()-start}seconds\")\n    return oof_normal, predictions_normal, model","fdae0e3e":"bay_ridge = linear_model.BayesianRidge()\nbay_oof,bay_pred, bay_model = level_2_trainer(bay_ridge)","ef16952c":"print('CV Score:',np.sqrt(mean_squared_error(bay_oof, target)))","2c50fba3":"models = ['LGBM','RF','XGB']\nfor model,weight in zip(models,bay_model.coef_):\n    print(f\"{model} Model Weights:{weight:0.3f}\")","9a350fef":"pred_df = pd.DataFrame({\"card_id\":test[\"card_id\"].values})\npred_df['lgbm_target'] = lgbm_pred\npred_df['rf_target'] = rf_pred\npred_df['xgb_target'] = xgb_pred\npred_df['avg_target'] = averaged_pred\npred_df['bayridge_target'] = bay_pred\npred_df.head()","72643742":"sub_df = pd.DataFrame()\nsub_df['card_id'] = pred_df['card_id']\nsub_df['target'] = pred_df['lgbm_target']\n#sub_df.to_csv(\"ELOsubmission.csv\", index=False)","42276ebe":"# Test Random Forest Score\nsub_df['target'] = pred_df['rf_target']\n#sub_df.to_csv(\"ELOsubmission.csv\", index=False)","c2d4db3f":"sub_df['target'] = pred_df['xgb_target']\n#sub_df.to_csv(\"ELOsubmission.csv\", index=False)","10200c06":"sub_df['target'] = pred_df['avg_target']\n#sub_df.to_csv(\"ELOsubmission.csv\", index=False)","32b9c5c0":"sub_df['target'] = pred_df['bayridge_target']\nsub_df.to_csv(\"ELOsubmission.csv\", index=False)","7901521b":"## Aggregating Historical Transactions","01b400d7":"### LGBM CV SCORE","683557d3":"#### Random Forest Score: \n\n**CV:** 3.6988\n\n**Public LB:** 3.752\n\n\n**Difference:** +0.0542","dc9a38c0":"# Preprocessing data","ae9bf387":"## Second Level Linear Model Weights","55326d8c":"### Stacked Linear Model","b209609b":"#### Simple Average Score:\n**CV:** 3.668 \n\n**Public LB:** 3.718\n\n**Difference:** 0.05","49d9d8f7":"## Loading Historical and New Transactions","9d8d1eec":"# Adding Aggregate Features","01e9ad9a":"## Aggregating New Transactions Dataframe","0886597f":"### Stacked Bayesian Ridge CV:","7370ddad":"## Feature Engineering from Aggregate Features","ef67c40f":"#### XGBoost Score:\n\n**CV:** 3.68\n\n**Public LB:** 3.718\n\n**Difference:** +0.038","735108e4":"### Stacked Bayesian Ridge Model:","3f531070":"## Summary of Predictions (LB and CV)","c33de16c":"# LGBM Model with Outlier Stratified KFold Validation","e03a78af":"# Feature Engineering\n\nChau Huynh's \"My First Kernel\" was a starting point of this competition for me so I borrow a lot from him in this section.\nhttps:\/\/www.kaggle.com\/chauhuynh\/my-first-kernel-3-699\/\n\n\n## Encoding Date Times","d2503184":"#### LGBM Score: \n\n**CV:** 3.666\n\n**Public LB:** 3.713\n\n\n**Difference:** +0.047","c6fdbeea":"### Random Forest CV Score:","2106c3f5":"# Loading Data\n\n## Train and Test Sets:","a30b78ba":"## XGBoost Model","f8e61935":"### Averaged CV Score","981a061f":"#### Stacked Bayesian Ridge Model Score:\n\n**CV:** 3.664\n\n**Public LB:** 3.707\n\n**Difference:** 0.043","d7b5a6e3":"## Sklearn Random Forest Model","818df632":"### XGBoost CV Score:","681e62b8":"\n\n---\n<h1><center> ELO Merchant Category Recommendation <\/center><\/h1>\n\n---\n\n<h2><center> Comparing Baseline Model Performance to Stacked Models  <\/center><\/h2>\n   <h2><center> LGBM, XGBoost, RF and Stacking (LB & CV) <\/center><h2>\n\n--- \n\nThe purpose of this notebook is to provide an example of the improvements that can be made to a base LightGBM Model by stacking with other models. To summarize, the base LGBM model scores around a 3.713 on the public LB while the XGBoost and RF Model score 3.718 and 3.752 respectively. Despite these models performing worse than the LightGBM stacking them together provides a small improvement to both CV and LB reaching a result of around 3.708."}}