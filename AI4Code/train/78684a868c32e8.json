{"cell_type":{"01bb9380":"code","529cc48e":"code","0dfdb672":"code","32cb86ce":"code","74747489":"code","cd1d86cf":"code","4294be6b":"code","3bc7c76c":"code","693d92a0":"code","424bcf66":"code","410e54bb":"code","0e1e46d6":"code","f380bd84":"code","c60655e9":"code","58036b3a":"code","b5567f05":"code","8ba27f47":"code","6aa2bc3b":"code","a3f95313":"code","6f73bec6":"code","034b4746":"code","02e497d1":"code","b27a03dc":"code","99283080":"code","2460ff94":"code","c87f8975":"code","66cd9018":"markdown","f66e3845":"markdown","b7748196":"markdown","65691a8b":"markdown","70fc90b0":"markdown","219b7f91":"markdown","24cb8179":"markdown","5bf749cd":"markdown","f613043c":"markdown","666a1947":"markdown","e09ab2a0":"markdown","45f7232a":"markdown","9add57d6":"markdown"},"source":{"01bb9380":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nimport seaborn as sns","529cc48e":"from sklearn.datasets import fetch_covtype\n\ncover = fetch_covtype()","0dfdb672":"X = pd.DataFrame(cover.data, columns=[\"p_{}\".format(i+1) for i in range(cover.data.shape[1])])\ny = pd.Series(cover.target)","32cb86ce":"X = X.iloc[:200000, :]\nX","74747489":"# label start from 0\ny = y[:200000] - 1\ny","cd1d86cf":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\nscaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(y_train.shape)\nprint(y_test.shape)\nprint(X_train.shape)\nprint(X_test.shape)","4294be6b":"from sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom keras import Sequential, layers\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping","3bc7c76c":"logreg = LogisticRegression(penalty=\"l2\", max_iter=500)\n\nlogreg.fit(X_train_scaled, y_train)\npreg_logreg = logreg.predict(X_test_scaled)\n\nscore_logreg = accuracy_score(preg_logreg, y_test)\nprint(score_logreg)","693d92a0":"svc = LinearSVC()\n\nsvc.fit(X_train_scaled, y_train)\npred_svc = svc.predict(X_test_scaled)\n\nscore_svc = accuracy_score(pred_svc, y_test)\nprint(score_svc)","424bcf66":"abc = AdaBoostClassifier(n_estimators=60, learning_rate=1.0)\n\nabc.fit(X_train_scaled, y_train)\npred_abc = abc.predict(X_test_scaled)\n\nscore_abc = accuracy_score(pred_abc, y_test)\nprint(score_abc)","410e54bb":"dt = DecisionTreeClassifier(random_state=32)\n\ndt.fit(X_train, y_train)\npred_dt = dt.predict(X_test)\n\nscore_dt = accuracy_score(pred_dt, y_test)\nprint(score_dt)","0e1e46d6":"rf = RandomForestClassifier(random_state=8)\n\nrf.fit(X_train, y_train)\npred_rf = rf.predict(X_test)\n\nscore_rf = accuracy_score(pred_rf, y_test)\nprint(score_rf)","f380bd84":"et = ExtraTreesClassifier(random_state=9)\n\net.fit(X_train, y_train)\npred_et = et.predict(X_test)\n\nscore_et = accuracy_score(pred_et, y_test)\nprint(score_et)","c60655e9":"# convert to XGBoost-specific data types\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nparams_xgb = {\n    \"max_depth\": 8,\n    \"eta\": 0.1,\n    \"tree_method\": \"exact\",\n    \"objective\": \"multi:softmax\",\n    \"num_class\": 7,\n    \"eval_metric\": \"merror\"\n}\n\nprocess = {}\n\nxgboost = xgb.train(params=params_xgb,\n                    dtrain=d_train,\n                    num_boost_round=10000,\n                    early_stopping_rounds=5,\n                    verbose_eval=50,\n                    evals=[(d_train, \"train\"), (d_test, \"test\")],\n                    evals_result=process)","58036b3a":"# plot RMSEs\ntrain_error = process[\"train\"][\"merror\"]\ntest_error = process[\"test\"][\"merror\"]\nfig = plt.figure(figsize=(12, 12))\nplt.plot(train_error, label=\"train_merror\", color=\"teal\", linewidth=3)\nplt.plot(test_error, label=\"test_merror\", color=\"orange\", linewidth=3)\nplt.xlabel(\"num_rounds\", fontsize=20)\nplt.ylabel(\"RMSE\", fontsize=20)\nplt.show()","b5567f05":"# show some features that are important for prediction\nxgb.plot_importance(xgboost, max_num_features=10, color=\"teal\", height=0.6)\nplt.show()","8ba27f47":"pred_xgb = xgboost.predict(d_test, ntree_limit=xgboost.best_ntree_limit)\n\nscore_xgb = accuracy_score(pred_xgb, y_test)\nprint(score_xgb)","6aa2bc3b":"# convert to LighGBM-specific data types\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"goosting_type\": \"gbdt\",\n    \"objective\": \"multiclass\",\n    \"metric\": \"multi_logloss\",\n    \"num_class\": 7,\n    \"learning_rate\": 0.1\n}\n\nlightgbm = lgb.train(params=params_lgb,\n                     train_set=lgb_train,\n                     valid_sets=lgb_test,\n                     num_boost_round=10000,\n                     early_stopping_rounds=5,\n                     verbose_eval=50)","a3f95313":"# show some features that are important for prediction\nlgb.plot_importance(lightgbm, height=0.6, max_num_features=10, color=\"teal\", figsize=(6,8))\nplt.show()","6f73bec6":"lgb.plot_tree(lightgbm, figsize=(20,20))\nplt.show()","034b4746":"pred_lgb = lightgbm.predict(X_test)\npred_lgb = np.argmax(pred_lgb, axis=1)\n\nscore_lgb = accuracy_score(pred_lgb, y_test)\nprint(score_lgb)","02e497d1":"nn = Sequential()\n\nnn.add(layers.Dense(256, activation=\"relu\", input_shape=(54,)))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dense(256, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dense(128, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dropout(0.1))\nnn.add(layers.Dense(128, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dropout(0.1))\nnn.add(layers.Dense(64, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dropout(0.1))\nnn.add(layers.Dense(64, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dropout(0.1))\nnn.add(layers.Dense(32, activation=\"relu\"))\nnn.add(layers.BatchNormalization())\nnn.add(layers.Dense(7, activation=\"softmax\"))\n\nnn.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nnn.summary()","b27a03dc":"# apply one-hot labeling\ny_train_cat = np_utils.to_categorical(y_train)\ny_test_cat = np_utils.to_categorical(y_test)\n\n# set early stopping\ncallbacks = [EarlyStopping(monitor=\"val_accuracy\", patience=5)]\n\nhis = nn.fit(x=X_train_scaled,\n             y=y_train_cat,\n             epochs=10000,\n             batch_size=512,\n             verbose=2,    # 2 = Show the progress in \"one\" line.\n             callbacks=callbacks,\n             validation_data=(X_test_scaled, y_test_cat))","99283080":"pred_nn = nn.predict(X_test_scaled)\npred_nn = np.argmax(pred_nn, axis=1)\n\nscore_nn = accuracy_score(pred_nn, y_test)\nprint(score_nn)","2460ff94":"classifiers = pd.DataFrame([\"LogReg\",\"SVM\",\"AdaBoost\",\"DecisionTree\",\"RandomForest\",\"ExtraTrees\",\"XGBoost\",\"LighGBM\",\"DNN\"], columns=[\"classifier\"])\n\nscores = pd.DataFrame([score_logreg, score_svc, score_abc, score_dt, score_rf, score_et, score_xgb, score_lgb, score_nn], columns=[\"accuracy\"])\n\nresults = pd.concat([classifiers, scores], axis=1)","c87f8975":"plt.figure(figsize=(16, 8))\nsns.barplot(x=\"classifier\", y=\"accuracy\", data=results)\nplt.xlabel(\"classifier\", fontsize=15)\nplt.ylabel(\"accuracy\", fontsize=15)\nplt.show()","66cd9018":"## Deep Neural Network","f66e3845":"# Classification with several classifiers","b7748196":"# Results","65691a8b":"## Decision Tree","70fc90b0":"# Compare several classifiers with multi class classification\n### Classify forest cover type using 54 features.","219b7f91":"## Support Vector Machine (Classifier)\nsklearn official documentation recommends the following  \nnum_samples < 10000 sklearn.svm.SVC  \nnum_samples >= 10000 sklearn.svm.LinearSVC","24cb8179":"## Logistic Regression","5bf749cd":"## Extra Trees","f613043c":"## AdaBoost Classifier","666a1947":"## XGBoost","e09ab2a0":"## LightGBM","45f7232a":"Logistic Regression  \nSupport Vector Machine  \nAdaBoost  \nDecision Tree  \nRandom Forest  \nExtra Trees  \nXGBoost  \nLightGBM  \nDeep Neural Network  \nVoting","9add57d6":"## Random Forest"}}