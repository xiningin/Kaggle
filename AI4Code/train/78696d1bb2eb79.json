{"cell_type":{"51b4ef80":"code","5bef23c5":"code","2c86b951":"code","d3fbb2c3":"code","8c447d64":"code","7d75261e":"code","5af6c98c":"code","8b789fae":"code","9fc5941a":"code","8832b981":"code","014f7149":"code","e81a7949":"code","ea378206":"code","f6f1b2e2":"code","a889393f":"code","ef1b9502":"code","c557b427":"code","81907430":"code","442be16e":"code","4d7a4774":"code","e4e4487c":"code","564fcb4b":"code","4284220e":"code","865a9da1":"code","b0a0b216":"code","2353e01c":"code","72dbbbe0":"code","56b13120":"code","1ef487d6":"code","e45d4f21":"code","0d4af13a":"code","1e8cdfc5":"code","bf5b9ce0":"code","362f485c":"code","6837f85f":"code","e4f13560":"code","16b48b13":"code","b552a19b":"code","7d096908":"code","399c6773":"code","c5afa60a":"code","1bd4b749":"code","20b8aa63":"code","5314bce1":"code","6383b5e8":"markdown"},"source":{"51b4ef80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5bef23c5":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","2c86b951":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","d3fbb2c3":"train_df.head()","8c447d64":"train_df.describe()","7d75261e":"print(pd.isnull(train_df).sum()) ","5af6c98c":"print(pd.isnull(test_df).sum()) ","8b789fae":"train_df.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntest_df.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)","9fc5941a":"train_df1 = train_df.copy()\ntrain_df1.dropna(inplace = True)\nsns.distplot(train_df1[\"Age\"])","8832b981":"#the median will be an acceptable value to place in the NaN cells\ntrain_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace = True)\ntest_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace = True) \ntrain_df[\"Embarked\"].fillna(\"S\", inplace = True)\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace = True)\n\nprint(pd.isnull(train_df).sum())\nprint(\" \")\nprint(pd.isnull(test_df).sum()) ","014f7149":"copy = train_df.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","e81a7949":"#can ignore the testing set for now\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train_df)\nplt.title(\"Distribution of Survival based on Gender\")\nplt.show()\n\ntotal_survived_females = train_df[train_df.Sex == \"female\"][\"Survived\"].sum()\ntotal_survived_males = train_df[train_df.Sex == \"male\"][\"Survived\"].sum()\n\nprint(\"Total people survived is: \" + str((total_survived_females + total_survived_males)))\nprint(\"Proportion of Females who survived:\") \nprint(total_survived_females\/(total_survived_females + total_survived_males))\nprint(\"Proportion of Males who survived:\")\nprint(total_survived_males\/(total_survived_females + total_survived_males))\n","ea378206":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = train_df[train_df.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = train_df[train_df.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = train_df[train_df.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Total people survived is: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)","f6f1b2e2":"sns.barplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","a889393f":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train_df)\nplt.ylabel(\"Survival Rate\")\nplt.title(\"Survival Rates Based on Gender and Class\")","ef1b9502":"survived_ages = train_df[train_df.Survived == 1][\"Age\"]\nnot_survived_ages = train_df[train_df.Survived == 0][\"Age\"]\nplt.subplot(1, 2, 1)\nsns.distplot(survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Survived\")\nplt.ylabel(\"Proportion\")\nplt.subplot(1, 2, 2)\nsns.distplot(not_survived_ages, kde=False)\nplt.axis([0, 100, 0, 100])\nplt.title(\"Didn't Survive\")\nplt.show()","c557b427":"sns.stripplot(x=\"Survived\", y=\"Age\", data=train_df, jitter=True)","81907430":"sns.pairplot(train_df)","442be16e":"train_df.sample(5)","4d7a4774":"test_df.sample(5)","e4e4487c":"train_df.loc[train_df[\"Sex\"] == \"male\", \"Sex\"] = 0\ntrain_df.loc[train_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntrain_df.loc[train_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain_df.loc[train_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain_df.loc[train_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntest_df.loc[test_df[\"Sex\"] == \"male\", \"Sex\"] = 0\ntest_df.loc[test_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntest_df.loc[test_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntest_df.loc[test_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntest_df.loc[test_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","564fcb4b":"test_df.sample(10)","4284220e":"train_df[\"FamSize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\ntest_df[\"FamSize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] + 1","865a9da1":"train_df[\"IsAlone\"] = train_df.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntest_df[\"IsAlone\"] = test_df.FamSize.apply(lambda x: 1 if x == 1 else 0)","b0a0b216":"for name in train_df[\"Name\"]:\n    train_df[\"Title\"] = train_df[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \nfor name in test_df[\"Name\"]:\n    test_df[\"Title\"] = test_df[\"Name\"].str.extract(\"([A-Za-z]+)\\.\",expand=True)\n    \ntitle_replacements = {\"Mlle\": \"Other\", \"Major\": \"Other\", \"Col\": \"Other\", \"Sir\": \"Other\", \"Don\": \"Other\", \"Mme\": \"Other\",\n          \"Jonkheer\": \"Other\", \"Lady\": \"Other\", \"Capt\": \"Other\", \"Countess\": \"Other\", \"Ms\": \"Other\", \"Dona\": \"Other\", \"Rev\": \"Other\", \"Dr\": \"Other\"}\n\ntrain_df.replace({\"Title\": title_replacements}, inplace=True)\ntest_df.replace({\"Title\": title_replacements}, inplace=True)\n\ntrain_df.loc[train_df[\"Title\"] == \"Miss\", \"Title\"] = 0\ntrain_df.loc[train_df[\"Title\"] == \"Mr\", \"Title\"] = 1\ntrain_df.loc[train_df[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntrain_df.loc[train_df[\"Title\"] == \"Master\", \"Title\"] = 3\ntrain_df.loc[train_df[\"Title\"] == \"Other\", \"Title\"] = 4\n\ntest_df.loc[test_df[\"Title\"] == \"Miss\", \"Title\"] = 0\ntest_df.loc[test_df[\"Title\"] == \"Mr\", \"Title\"] = 1\ntest_df.loc[test_df[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntest_df.loc[test_df[\"Title\"] == \"Master\", \"Title\"] = 3\ntest_df.loc[test_df[\"Title\"] == \"Other\", \"Title\"] = 4","2353e01c":"train_df.sample(10)","72dbbbe0":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","56b13120":"from sklearn.metrics import make_scorer, accuracy_score ","1ef487d6":"from sklearn.model_selection import GridSearchCV","e45d4f21":"features = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Fare\", \"FamSize\", \"IsAlone\", \"Title\"]\nX_train = train_df[features] #define training features set\ny_train = train_df[\"Survived\"] #define training label set\nX_test = test_df[features] #define testing features set\n#we don't have y_test, that is what we're trying to predict with our model","0d4af13a":"from sklearn.model_selection import train_test_split #to create validation data set\n\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets","1e8cdfc5":"svc_clf = SVC() \nsvc_clf.fit(X_training, y_training)\npred_svc = svc_clf.predict(X_valid)\nacc_svc = accuracy_score(y_valid, pred_svc)\n\nprint(acc_svc)","bf5b9ce0":"linsvc_clf = LinearSVC()\nlinsvc_clf.fit(X_training, y_training)\npred_linsvc = linsvc_clf.predict(X_valid)\nacc_linsvc = accuracy_score(y_valid, pred_linsvc)\n\nprint(acc_linsvc)","362f485c":"rf_clf = RandomForestClassifier()\nrf_clf.fit(X_training, y_training)\npred_rf = rf_clf.predict(X_valid)\nacc_rf = accuracy_score(y_valid, pred_rf)\n\nprint(acc_rf)","6837f85f":"logreg_clf = LogisticRegression()\nlogreg_clf.fit(X_training, y_training)\npred_logreg = logreg_clf.predict(X_valid)\nacc_logreg = accuracy_score(y_valid, pred_logreg)\n\nprint(acc_logreg)","e4f13560":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_training, y_training)\npred_knn = knn_clf.predict(X_valid)\nacc_knn = accuracy_score(y_valid, pred_knn)\n\nprint(acc_knn)","16b48b13":"gnb_clf = GaussianNB()\ngnb_clf.fit(X_training, y_training)\npred_gnb = gnb_clf.predict(X_valid)\nacc_gnb = accuracy_score(y_valid, pred_gnb)\n\nprint(acc_gnb)","b552a19b":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_training, y_training)\npred_dt = dt_clf.predict(X_valid)\nacc_dt = accuracy_score(y_valid, pred_dt)\n\nprint(acc_dt)","7d096908":"from xgboost import XGBClassifier\nxg_clf = XGBClassifier(objective=\"binary:logistic\", n_estimators=10, seed=123)\nxg_clf.fit(X_training, y_training)\npred_xg = xg_clf.predict(X_valid)\nacc_xg = accuracy_score(y_valid, pred_xg)\n\nprint(acc_xg)","399c6773":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\", \"XGBClassifier\"],\n    \"Accuracy\": [acc_svc, acc_linsvc, acc_rf, \n              acc_logreg, acc_knn, acc_gnb, acc_dt, acc_xg]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","c5afa60a":"rf_clf = RandomForestClassifier()\n\nparameters = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n              \"max_depth\": [2, 3, 5, 10], \n              \"min_samples_split\": [2, 3, 5, 10],\n              \"min_samples_leaf\": [1, 5, 8, 10]\n             }\n\ngrid_cv = GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(X_train, y_train)\n\nprint(\"Our optimized Random Forest model is:\")\ngrid_cv.best_estimator_","1bd4b749":"rf_clf = grid_cv.best_estimator_\n\nrf_clf.fit(X_train, y_train)","20b8aa63":"submission_predictions =rf_clf.predict(X_test)","5314bce1":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nsubmission.to_csv(\"titanic.csv\", index=False)\nprint(submission.shape)","6383b5e8":"** References\n\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n\nA journey through Titanic\nGetting Started with Pandas: Kaggle's Titanic Competition\nTitanic Best Working Classifier"}}