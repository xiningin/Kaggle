{"cell_type":{"3a1a4919":"code","b67c2c09":"code","4f61410d":"code","6eba8599":"code","fe3943da":"code","b44c56fd":"code","cd3e47c2":"code","eabc8966":"code","3601459d":"code","4f534ef7":"code","58973caa":"code","3c98ebfb":"markdown","5b3b9efa":"markdown","21f469c1":"markdown","514791e1":"markdown","bf0993a1":"markdown","8ed0bf00":"markdown","f8a28ed8":"markdown","fc288f27":"markdown","f17f4c37":"markdown","6d6c62aa":"markdown","a896f286":"markdown","63c02340":"markdown","e8ab33ee":"markdown","c3267fa3":"markdown","f7a010be":"markdown","13e41978":"markdown","606ed75b":"markdown","5d9aff1f":"markdown","318f0480":"markdown","fa5f8ca5":"markdown","6fd68de7":"markdown","57bc9b10":"markdown","b115a50d":"markdown"},"source":{"3a1a4919":"!apt-get update > \/dev\/null 2>&1\n!apt-get install cmake > \/dev\/null 2>&1\n!pip install --upgrade setuptools 2>&1\n!pip install ez_setup > \/dev\/null 2>&1\n!pip install gym[atari] > \/dev\/null 2>&1\n!pip install gym pyvirtualdisplay > \/dev\/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg > \/dev\/null 2>&1","b67c2c09":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Dropout,Conv2D, Flatten,MaxPooling2D ,Activation\nfrom tensorflow.keras.models import Sequential\nimport gym\nimport numpy as np\nimport random\nfrom collections import deque\nfrom tensorflow.keras.utils import normalize as normal_values\nimport cv2\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\ngymlogger.set_level(40) #error only\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfrom IPython import display as ipythondisplay","4f61410d":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","6eba8599":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n\ndef show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","fe3943da":"import gym.envs.toy_text \nRANDOM_SEED=1\nN_EPISODES=500\n\n# random seed (reproduciblity)\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)\n\n# set the env\nenv = (wrap_env(gym.make(\"Assault-v0\"))) # env to import\nenv.seed(RANDOM_SEED)\nenv.reset() # reset to env","b44c56fd":"class DQN:\n\n  def __init__(self, env,path=None):\n    self.env=env #import env\n    self.state_shape=(1,) # the state space\n    self.action_shape=env.action_space.n # the action space\n    self.gamma=[0.99] # decay rate of past observations\n    self.alpha=1e-4 # learning rate in the policy gradient\n    self.learning_rate=0.001 # learning rate in deep learning\n    self.epsilon_initial_value=1.0 # initial value of epsilon\n    self.epsilon_current_value=1.0 # current value of epsilon\n    #self.temperature_parameter_final_value=0.0001 # final value of temperature_parameter\n    #self.temperature_parameter_initial_value=5.0 # initial value of temperature_parameter\n    #self.temperature_parameter_current_value=5.0 # current value of temperature_parameter\n    self.epsilon_final_value=0.0001 # final value of epsilon\n    #self.nma=3 # No of Top actions to take while exploring \n    self.observing_episodes=5 #No of episodes to observe before updating\n    self.batch_sizee=128\n    self.transitions= deque()\n    self.replay_memory=50000 # number of previous transitions to remember\n    if not path:\n      self.model=self._create_model() #build model\n    else:\n      self.model=load_model(path) #import model\n    \n    def remember(self,delta,state,action,next_state,reward):      #This is the function to store our experiences\n    self.transitions.append([delta,state,action,next_state,reward])\n    if len(self.transitions) > self.replay_memory:\n      self.transitions.append([delta,state,action,next_state,reward])","cd3e47c2":"def _create_model(self):\n    ''' builds the model using keras'''\n    model = Sequential()\n    model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=(250,160,4)))  \n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (4, 4),strides=(2, 2),  padding='same'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (3, 3),strides=(1, 1),  padding='same'))\n    model.add(MaxPooling2D(pool_size=(2,2)))\n    model.add(Activation('relu'))\n    model.add(Flatten())\n    model.add(Dense(512))\n    model.add(Activation('relu'))\n    model.add(Dense(7))\n    model.compile(loss='MSE',optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n    return model","eabc8966":"def get_action(self, state):\n        '''samples the next action based on the Boltzmann exploration policy'''\n\n        x=random.random()\n        if x < self.epsilon_current_value:                                    #Exlporation\n          #q_values=((self.model.predict(state)[0])**(1\/self.temperature_parameter_current_value) #This is the step where we use Boltzmann exploration policy. Uncomment this and below to use boltzmann policy\n          #top_actions=q_values.argsort()[-self.nma:][::-1]\n          #action=random.choice(top_actions)\n           action=random.choice([0,1,2,3,4,5,6])\n            \n        else:\n          q_values=(self.model.predict(state)) #Exploitation\n          max_Q = np.argmax(q_values)\n          action = max_Q\n          \n        return action","3601459d":"def update_policy(self):\n    '''\n    Updates the policy network using the NN model.\n      '''\n    transitions=random.sample(self.transitions,self.batch_size)\n    \n    inputs=np.zeros((self.batch_size, 250,160,4))\n    targets = np.zeros((self.batch_size,7)) \n\n    for i in range(0,self.batch_size):\n      delta=transitions[i][0] #wheather the state is terminal or not\n      state=transitions[i][1] # 4D stack of images\n      action=transitions[i][2] #This is action\n      next_state=transitions[i][3] #next state\n      reward=transitions[i][4] #reward at state due to action\n\n      inputs[i:i + 1] = state\n      targets[i] = (self.model.predict(state)) # predicted q values\n      Q_sa = (self.model.predict(next_state))  #predict q values for next step\n      if delta==0:\n        targets[i, action] = reward\n      else:\n        targets[i, action] = reward + np.asarray(self.gamma) * np.max(Q_sa)\n    self.model.fit(inputs, targets,epochs=20) #Training the model","4f534ef7":"def train(self, episodes):\n    '''\n          train the model\n          episodes - number of training iterations\n          ''' \n    env=self.env\n    total_rewards=np.zeros(episodes)\n\n    for episode in range(episodes):\n      # each episode is a new game env\n      state=env.reset()\n      done=False\n      state=cv2.cvtColor(state, cv2.COLOR_RGB2GRAY) #RGB to Grey Scale\n      stacked_frames = np.stack((state,state,state,state),axis=2)  # stack 4 images to create input\n      stacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2]) #1*250*160*4\n      state_=stacked_frames\n      episode_reward=0 #record episode reward\n      print(\"Episode Started\")\n      while not done:\n        # play an action and record the game state & reward per episode\n        action=self.get_action(state_) #input a stack of 4 images, get the action\n        next_state, reward, done, _=env.step(action)\n        print(\"Episode Going On.\"+\"\\n\"+\"Action taken:\"+'\\t'+\"Reward:\",action,reward)\n        next_state=cv2.cvtColor(next_state, cv2.COLOR_RGB2GRAY) #RGB to Grey Scale\n        next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1) #1x250x160x1\n        stacked_frames_1 = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3) # append the new image to input stack and remove the first one\n        next_state_=stacked_frames_1\n        if done:\n          delta=0.0\n        else:\n          delta=1.0\n\n        if action==2:\n            reward=3.0\n        elif action==3 or action==4:\n            reward=2.8\n        elif action==5 or action==6:\n            reward=1.5\n        else:\n            reward=1.0\n        \n        self.remember(delta,state_,action,next_state_,reward)\n        state_=next_state_\n        episode_reward+=reward\n      print(\"Episode_reward:{}\".format(episode_reward))\n      print(\"Episode Ended\")\n      if episode%self.observing_episodes==0 and episode!=0:\n        self.update_policy()\n        self.model.save('model_2_{}.h5'.format(episode))\n        print('Current Epsilon Value:',self.epsilon_current_value)\n        #print('Current Temperature parameter Value:',self.temperature_parameter_current_value)\n      self.epsilon_current_value=self.epsilon_current_value-(self.epsilon_initial_value-self.epsilon_final_value)\/1000\n      #self.temperature_parameter_current_value=self.temperature_parameter_current_value-(self.temperature_parameter_initial_value-self.temperature_parameter_final_value)\/1000\n\nAgent_2=DQN(env)\nAgent_2.train(episodes=2000) ","58973caa":"env = (wrap_env(gym.make(\"Assault-v0\")))\nAgent_3=DQN(env,path='model.h5')\nstate=env.reset()\ndone=False\nstate=cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\nstacked_frames = np.stack((state,state,state,state),axis=2)\nstacked_frames = stacked_frames.reshape(1,stacked_frames.shape[0],stacked_frames.shape[1],stacked_frames.shape[2])\nstate_=stacked_frames\nwhile True:\n  \n    env.render('ipython')\n    \n    #your agent goes here\n    action =Agent_3.get_action(state_)\n    next_state, reward, done, info = env.step(action)\n    print(action)\n    next_state=cv2.cvtColor(next_state, cv2.COLOR_RGB2GRAY)\n    next_state_ = next_state.reshape(1,next_state.shape[0],next_state.shape[1],1)\n    stacked_frames_1 = np.append(next_state_, stacked_frames[:, :, :, :3], axis=3) \n    state_=stacked_frames_1\n    if done:\n      break\n            \nenv.close()\nshow_video()","3c98ebfb":"# ACTION SELECTION IN DQN\nEven though DQN is an off-policy algorithm, how a DQN agent gathers experiences still matters. There are two important factors to consider.\nFirst, an agent still faces the exploration-exploitation trade-off discussed in the last notebook(Deep Reinforcement Learning- Part-3 ). An agent should rapidly explore the state-action space at the beginning of training to increase the chances of discovering good ways to act in an environment. As training progress and the agent learns more, an agent should gradually decrease the rate of exploration and spend more time exploiting what it has learned. This improves the efficiency of training as the agent focuses on better actions.\nSecond, if the state-action space is very large because it consists of continuous values or is discrete with high dimensionality1, then it will be intractable to experience all (s, a) pairs, even once. In these cases, Q-values for the unvisited (s, a) pairs may be no better than random guessing. Fortunately, function approximation with neural networks mitigates this problem because they can generalize from visited (s, a) pairs to similar states and actions. However, this does not completely solve the problem. There may still be parts of the state-action space that are far away and very different from the states and actions an agent has experienced. A neural network is unlikely to generalize well in these cases and the estimated Q-values may be inaccurate.","5b3b9efa":"**Ways to increasing the model efficiency**\n\n# Skipping Frames\nALE (The Arcade Learning Environment: An Evaluation Platform for General Agents was published in 2013, which proposes learning environments for AI. ALE has a lot of games originally designed for a classical game console, Atari 2600) is capable of rendering 60 images per second. But actually people don\u2019t take actions so much in a second. AI doesn\u2019t need to calculate Q values every frame. Skipping Frames technique is that DQN calculates Q values every 4 frames and use past 4 frames as inputs. This reduces computational cost and gathers more experiences.\n\n# Clipping Rewards\nEach game has different score scales. For example, in Pong, players can get 1 point when wining the play. Otherwise, players get -1 point. However, in SpaceInvaders, players get 10~30 points when defeating invaders. This difference would make training unstable. Thus Clipping Rewards technique clips scores, which all positive rewards are set +1 and all negative rewards are set -1.\n\n**DQN has various limitations.In the next notebook we see another methods to improve DQN**","21f469c1":"The role of the temperature parameter \u03c4 in the Boltzmann policy is analogous to that of \u220a in the \u220a-greedy policy. It encourages exploration of the state-action space. High values of \u03c4 (e.g \u03c4 = 5) encourage the probability distribution to be closer to a uniform distribution. This results in an agent acting very randomly. Low values of \u03c4 (e.g. 0.1) increase the probability of the action corresponding to the largest Q-value, and so the agent will act more greedily. \u03c4 = 1 reduces to the softmax function. Adjusting the value of \u03c4 during training balances exploration and exploitation. High values of \u03c4 at the beginning of training will encourage exploration. As \u03c4 is decreased over time, the policy will approximate the greedy policy more closely.\n\n\nThe main advantage of the Boltzmann policy when compared to the \u220a-greedy policy is that it explores the environment less randomly. Each time an agent selects an action, it samples a from the probability distribution over the actions generated by the Boltzmann policy. Instead of acting randomly with probability \u220a, the agent selects action a with probability $p_{Boltzmann}(a|s)$ so actions with higher Q-values are more likely to be chosen. Even if an agent does not select the Q-maximizing action, they are more likely to select the second than the third or fourth-best action. A Boltzmann policy also results in a smoother relationship between Q-value estimates and action probabilities compared to an \u220a-greedy policy.\n\n\u220a-greedy policies lead to more extreme behavior. If one of the Q-values is a fraction higher than the other, \u220a-greedy will assign all of the non-random probability (1 \u2013 \u220a) to that action. If in the next iteration the other Q-value was now slightly higher, \u220a-greedy would immediately switch and assign all of the non-random probability to the other action. An \u220a-greedy policy can be more unstable than a Boltzmann policy which can make it more difficult for an agent to learn.\n\nA Boltzmann policy can cause an agent to get stuck in a local minimum if the Q-function estimate is inaccurate for some parts of the state space. One way to tackle this problem with the Boltzmann policy is to use a large value for \u03c4 at the beginning of training so that the action probability distribution is more uniform. As training progresses and the agent learns more \u03c4 can be decayed, allowing the agent to exploit what it has learned. However, care needs to be taken not to decay \u03c4 too quickly otherwise a policy may get stuck in local minima.","514791e1":"# DQN ALGORITHM\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-12%20at%207.34.54%20AM.png?raw=true)","bf0993a1":"## LEARNING THE Q-FUNCTION IN DQN\nDQN, like SARSA, learns the Q-function using TD learning. Where the two algorithms differ is how $Q_{tar}(s,a)$ is constructed.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-06%20at%205.14.04%20PM.png?raw=true)\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-06%20at%205.14.14%20PM.png?raw=true)\n\nInstead of using the action a\u2032 actually taken in the next state s\u2032 to estimate $Q_{tar}(s,a)$ , DQN uses the maximum Q-value over all of the potential actions available in that state. In DQN, the Q-value that is used in the next state s\u2032 doesn\u2019t depend on the policy used to gather experiences. Since the reward r and next state s\u2032 are produced by the environment given the current state s and action a, this means that no part of the $Q_{tar}(s,a)$ estimation depends on the data gathering\npolicy. This makes DQN an off-policy algorithm because the function being learned is independent of the policy being followed to act in the environment and gather experiences. In contrast, SARSA is on-policy because it uses the action a\u2032 taken by the current policy in state s\u2032 to calculate the Q-value for the next state. It directly depends on the policy used to gather experiences.\n\n\nIt is important to note that just because DQN\u2019s objective is to learn the optimal Q-function doesn\u2019t mean that it will. There may be many reasons for this. For example, the hypothesis space represented by the neural network may not actually contain the optimal Q-function, non-convex optimization methods are imperfect and might not find a global minimum, and computation and time constraints place a limit on how long we can train an agent for. However, we can say that the upper bound on performance with DQN is optimal, compared to a potentially sub-optimal upper bound for SARSA resulting from learning the Q-function under an \u220a-greedy policy.\n\n","8ed0bf00":"# Deep Q-Network\nDQN is introduced in 2 papers, Playing Atari with Deep Reinforcement Learning on NIPS in 2013 and Human-level control through deep reinforcement learning on Nature in 2015. Deep Q-Network (DQN) is the first deep reinforcement learning method proposed by DeepMind. After the paper was published on Nature in 2015, a lot of research institutes joined this field because the deep neural network can empower RL to directly deal with high dimensional states like images, thanks to techniques used in DQN. ","f8a28ed8":"# How will Tabular Methods fail when  state-action space is very large\nLet\u2019s consider a tabular representation for $Q_@(s,a)$ .\nSuppose the state-action space is very large with millions of (s, a) pairs and at the beginning of training each cell representing a particular   is initialized to 0. During\ntraining an agent visits (s, a) pairs and the table is updated but the unvisited (s, a) pairs continue to have.\nSince the state-action space is large, many (s, a) pairs will remain unvisited and their Q-value estimates will remain at 0 even if (s, a) is desirable, with $Q^\u03c0$(s, a) >> 0. The main issue is that a tabular function representation does not learn anything about how different states and actions relate to each other.","fc288f27":"Action Selection\n\nThe get_action method guides out action choice. Initially, when training begins we use exploration policy but later we do exploitation.","f17f4c37":"This is the Part-4 of Deep Reinforcement Learning Notebook series.***In this Notebook I have introduced another Value-based Algorithm known as  Deep Q-Networks algorithm (DQN)***.\n\n\nThe Notebook series is about Deep RL algorithms so it excludes all other techniques that can be used to learn functions in reinforcement learning and also the Notebook Series is not exhaustive i.e. it contains the most widely used Deep RL algorithms only.\n\n","6d6c62aa":"# Solution to our problem \n\nIf an environment has a large state-action space it is unlikely that an agent will be able to learn good Q-value estimates for all parts of this space. It is still possible to achieve good performance in such environments, provided that an agent focuses on learning on the states and actions that a good policy is likely to visit often. When this strategy is combined with neural networks, the Q-function approximation is likely to be good in a local region surrounding the commonly visited parts of the state-action space.\n\nThe policy used by a DQN agent should, therefore, visit states and select actions that are reasonably similar the those that would be visited by acting greedily with respect to the agent\u2019s current Q-function estimate, which is the current estimate of the optimal policy.\n\nIn practice, this can be achieved by using the \u220a-greedy policy or the Boltzmann policy. We will use Boltmann's policy here.","a896f286":"Training the model\n\nThis method creates a training environment for the model. Iterating through a set number of episodes, it uses the model to sample actions and play them. When such a timestep ends, the model is using the observations to update the policy.\n\nWe can perform various pre-processing steps to improve computational efficiency.\nHere I have only done greyscaling.\n\nWe know that in a dynamic game we cannot predict action based on 1 observation(which is 1 frame of the game in this case) so we will use a stack of 4 frames to predict the output.\n\nWe have also defined the reward system ourselves. This helps model learn faster about optimal actions to take.","63c02340":"# The Boltzmann Policy\n\nThe Boltzmann policy tries to improve over random exploration(which is done in \u220a-greedy policy) by selecting actions using their relative Q-values. The Q-value maximizing action a in state s will be selected most often, but other actions with relatively high Q-values will also have a high probability of being chosen. Conversely, actions with very low Q-values will hardly ever be taken. This has the effect of focusing exploration on more promising actions off the Q-value maximizing path instead of selecting all actions with equal probability.\n\nTo produce a Boltzmann policy, we construct a probability distribution over the Q-values for all actions a in state s by applying the softmax function (Equation 4.1). The softmax function is parameterized by a temperature parameter \u03c4 \u2208 (0,\u221e), which controls how uniform or concentrated the resulting probability distribution is. High values of \u03c4 push the distribution to become more uniform, low values of \u03c4 push the distribution to become more concentrated. Actions are then sampled according to this distribution as shown in Equation\n4.2.\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-12%20at%207.08.07%20AM.png?raw=true)\n\nEquation 4.1\n\n![alt text](https:\/\/github.com\/Machine-Learning-rc\/Unimportant\/blob\/master\/Screenshot%202020-07-12%20at%207.08.13%20AM.png?raw=true)\n\nEquation 4.2","e8ab33ee":"# IMPLEMENTING DQN","c3267fa3":"This part ensures the reproducibility of the code below by using a random seed and setups the environment.","f7a010be":"Creating a Neural Network Model.","13e41978":"# EXPERIENCE REPLAY\nDQN improves on the sample efficiency of SARSA with the help of experience replay memory.\n\n**Let us first see why on-policy are sample inefficient.**\n\nWe have seen that on-policy algorithms can only use data gathered by the current policy to update the policy parameters. Each experience is used just once. This is problematic when combined with function approximation methods that learn using gradient descent, such as neural networks. Each parameter update must be small because the gradient only conveys meaningful information about a descent direction in a small area around the current parameter values. However, the optimal parameter update for some experiences may be large, for example, if there is a large difference between the Q-value a network predicts and actual Q-value. In these cases, network parameters may need to be updated many times using the experiences to make use of all of the information conveyed in them. On-policy algorithms cannot do this. Also, the experiences used to train on-policy algorithms are highly correlated. This is because the data used to compute a single parameter update is often from a single episode, where future states and rewards depend on previous states and actions. This can lead to high variance in the parameter updates.\n\n TD learning could be slow due to the trial and error mechanism for gathering data inherent in RL and the need to propagate information backward through time. Speeding up TD learning amounts to either speeding up the credit assignment process or shortening the trial and error process.\nExperience replay focuses on the latter by facilitating the re-use of experiences.\n\nAn experience replay memory stores the k most recent experiences an agent has gathered. If memory is full, the oldest experience is discarded to make space for the latest one. Each time an agent trains, one or more batches of data are sampled randomly uniformly from the experience replay memory. Each of these batches is used in turn to update the parameters of the Q- function network. k is typically quite large, between 10, 000 and 1, 000, 000, whereas the number of elements in a batch is much smaller, typically between 32 and 2048.\nThe size of the memory should be large enough to contain many episodes of experiences. Each batch will typically contain experiences from different episodes and different policies that de-correlates the experiences used to train an agent. In turn, this reduces the variance of the parameter updates, helping to\nactions. This can lead to high variance in the parameter updates.\nstabilize training. However, the memory should also be small enough so that each experience is likely to be sampled more than once before being discarded, which makes learning more efficient.\nDiscarding the oldest experiences is also important. As an agent learns, the distribution of (s, a) pairs that an agent experiences changes. Older experiences become less useful because an agent is less likely to visit the older states. With finite time and computational resources, it is preferable an agent focus on learning from the more recently gathered experiences, since these tend to be more relevant. Storing just the k most recent experiences in the memory implements this idea.\n\n","606ed75b":"With the help of below code we run our algorithm and see the success of it.With the help of below code we run our algorithm and see the success of it.(Before running this set self.epsilon_current_value=0.001 in SARSA class so that model does not choose actions randomly)","5d9aff1f":"# How is DQN different from SARSA\nLike SARSA, DQN is a value-based temporal difference (TD) algorithm that approximates the Q-function. The learned Q-function is then used by an agent to select actions. However, DQN learns a different Q-function compared to SARSA \u2014 the optimal Q- function instead of the Q-function for the current policy. This small but crucial change improves the stability and speed of learning.SARSA is an on-policy algorithm whereas DQN is an off-policy algorithm which means that DQN can learn from experiences gathered by any agent.DQN makes it possible to de-correlate and re-use experiences by sampling random batches from a large experience replay memory and allows for multiple parameter updates using the same batch.","318f0480":"Defining the DQN Class.You can see that I have commented out few things like temperature_parameter .You can uncomment them if you can want to use Boltzmann policy.In this epsilon greedy works good. So I have used that.","fa5f8ca5":"Updating the Policy\n\nThe update_policy method updates the model weights. It does it by training the model on a batch sampled from the experiences we store while training. Batch size and no of epochs can be tuned to future increase training efficiency.","6fd68de7":"# Why Neural Networks are better for  generalization \nNeural networks can extrapolate from Q-values for known (s, a) to unknown (s\u2032, a\u2032) because they learn how different states and actions are related to each other. This is very useful when (s, a) is large or has infinitely many elements because it means an agent does not have to visit all (s, a) to learn a good estimate of the Q-function. An agent only need to visit a representative subset of the state-action space.","57bc9b10":"Below code setups the environment required to run and record the game and also loads the required library.","b115a50d":"# When can Neural Networks fail\nThere are limitations to how well a network will generalize, and there are two common cases where it often fails. First, if a network receives inputs that are significantly different from the inputs it has been trained on, it is unlikely to produce good outputs. Generalization is typically much better in small neighborhoods of the input space surrounding the training data. Second, neural networks are likely to generalize poorly if the function they are approximating has sharp discontinuities. This is because neural networks implicitly assume that the input space is locally smooth. If the inputs x and x\u2032 are similar, then the corresponding outputs y and y\u2032 should also be similar."}}