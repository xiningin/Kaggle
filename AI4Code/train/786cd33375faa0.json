{"cell_type":{"be025b46":"code","5997f33d":"code","f8f2348a":"code","e5330b0d":"code","79c3e6df":"code","15b78202":"code","a7819f7b":"code","2dfa5ab7":"code","2e70a720":"code","20fe9af5":"markdown","489309db":"markdown","cd329d2d":"markdown","d4a06e04":"markdown","c8f19bdc":"markdown","765afd40":"markdown","b522f4c2":"markdown","d45452b2":"markdown"},"source":{"be025b46":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.\n# load data set\nx_l = np.load('..\/input\/sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/sign-language-digits-dataset\/Y.npy')\nimg_size = 64\nplt.subplot(1, 2, 1)\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')\n# Then lets create x_train, y_train, x_test, y_test arrays\n# Join a sequence of arrays along an row axis.\nX = np.concatenate((x_l[204:409], x_l[822:1027] ), axis=0) # from 0 to 204 is zero sign and from 205 to 410 is one sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\nX_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)\nx_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","5997f33d":"def parameter_initialize(x_train,y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0])*0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3)*0.1,\n                  \"bias2\" : np.zeros((y_train.shape[0],1))}\n    \n    print(\"Shape of weight1 : \",parameters[\"weight1\"].shape)\n    print(\"Shape of weight2 : \",parameters[\"weight2\"].shape)\n    print(\"Shape of bias1 : \",parameters[\"bias1\"].shape)\n    print(\"Shape of bias2 : \",parameters[\"bias2\"].shape)\n\n    return parameters\nparameter_initialize(x_train,y_train)","f8f2348a":"def sigmoid(z):\n    A = 1\/(1+np.exp(-z))\n    return A","e5330b0d":"def forward_propagation(x_train,parameters):\n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    results = {\"Z1\": Z1, \"A1\":A1,\"Z2\":Z2,\"A2\":A2}\n    return A2, results","79c3e6df":"def cost(A2,Y):\n    logaritmic_probability = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logaritmic_probability)\/Y.shape[1]\n    return cost","15b78202":"def backward_propagation(parameters,results,X,Y):\n    dZ2 = results[\"A2\"]-Y\n    dW2 = np.dot(dZ2,results[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2, axis = 1, keepdims = True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1-np.power(results[\"A1\"],2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1, axis=1,keepdims = True)\/X.shape[1]\n    gradients = {\"dweight1\": dW1,\n                \"dweight2\": dW2,\n                \"dbias1\": db1,\n                \"dbias2\":db2}\n    return gradients","a7819f7b":"def update_prameters(parameters,grand,learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grand[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grand[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grand[\"dweight2\"],\n                  \"bias2\" : parameters[\"bias2\"]-learning_rate*grand[\"dbias2\"]\n                 }\n    return parameters","2dfa5ab7":"def prediction(parameters, x_test):\n    A2, results = forward_propagation(x_test,parameters)\n    prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(A2.shape[1]):\n        if A2[0,i] <= 0.5:\n            prediction[0,i] = 0\n        else:\n            prediction[0,i] = 1\n    return prediction","2e70a720":"def two_layer_ANN_model(x_train, y_train, x_test, y_test, number_of_iteration):\n    cost_list = []\n    index = []\n    parameters = parameter_initialize(x_train,y_train)\n    for i in range(number_of_iteration):\n        A2, results = forward_propagation(x_train,parameters)\n        cost_result = cost(A2,y_train)\n        gradients = backward_propagation(parameters,results,x_train,y_train)\n        parameters = update_prameters(parameters, gradients)\n        \n        if i % 100 == 0:\n            cost_list.append(cost_result)\n            index.append(i)\n            print(\"Cost after iteration %i %f\" %(i,cost_result))\n    plt.plot(index,cost_list)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of \u0131teration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    y_prediction_test = prediction(parameters,x_test)\n    y_prediction_train = prediction(parameters,x_train)\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\nparameters = two_layer_ANN_model(x_train,y_train,x_test,y_test,2000)","20fe9af5":"# Forward Propagation\n* Its very similar with logistic regrassion.\n* In this part we use two different activation functions which are sigmoid and tanh.\n* Firstly, we should initialize sigmoid function and for tanh function we use numpy library.","489309db":"# Loss and Cost Function","cd329d2d":"# Backward Propagation\n* In this part, we take derivative of weights, for weight1 and weight2, bais, for bais1 and bais2, results, for Z1 and Z2 according to cost. ","d4a06e04":"# Layer Size and Weights and Bias Initialization\n* In x_train, there are 348 images x^(348)\n* To obtain variety, we should initialize weights randomly; but as small number like 0.01.\n* We should initialize weights as small number since when we use tanh activation function with big value weights, its derivative is approximetly 0 and we cannot generate optimistic result. (ex.: tanh(5) ~= 0.999 and Its derivative is ~ 0).\n* However we can initialize bias as zero.\n* I prefer to use 3 nodes hidden layer when implementing 2 layer ANN. Because of this reason: \n       * Shape of weight1 :  (3, 4096)\n       * Shape of weight2 :  (1, 3)\n       * Shape of bias1 :  (3, 1)\n       * Shape of bias2 :  (1, 1)","c8f19bdc":"# Update Parameters","765afd40":"# 2 Layer ANN Model Implementation","b522f4c2":"# Sigmoid Function","d45452b2":"# Prediction"}}