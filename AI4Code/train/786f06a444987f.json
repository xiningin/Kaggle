{"cell_type":{"d6ebb71b":"code","c9515886":"code","090ebedc":"code","2ef0fcdd":"code","293949f0":"code","a0ca287f":"code","067d7b40":"code","ce9998c7":"code","ded147c8":"code","e4012a25":"code","cc4bd90e":"code","d09f22de":"code","64b4ea41":"code","8d98430f":"code","2d70b57b":"code","d5769e6e":"code","9230122e":"code","53c2cb32":"code","0be23652":"code","ba7d96b8":"code","7da19588":"code","ee23a84c":"code","c9af41f0":"code","d6d73968":"code","cf0327ef":"code","cba4f8ed":"code","cf229546":"code","10b6b965":"code","42f66b33":"code","e40f8366":"code","61981fe8":"code","ee6e9260":"code","29c753ae":"code","30e8249a":"markdown"},"source":{"d6ebb71b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c9515886":"import datetime, pickle, copy, warnings\nfrom pandas import DataFrame, merge, concat\nimport glob\nfrom matplotlib import pyplot as plt\nplt.style.use('dark_background')","090ebedc":"df = pd.read_csv('\/kaggle\/input\/fx-min-data\/April_data_6series.csv')\ndf.sample(5)","2ef0fcdd":"df.drop(columns=['Unnamed: 0'], axis=1, inplace=True)\ndf = df.set_index('timestamp')\ndf.index = pd.to_datetime(df.index)\ndf.info()","293949f0":"X = df[:15000] # subset of data\nplt.style.use('dark_background')\ndef plot_vars(train, levels, color, leveltype):\n    \n    \"\"\"\n    Displays historical trends of variables\n    And see if it's sensible to just select levels instead of differences\n    \"\"\"\n    fig, ax = plt.subplots(1, 6, figsize=(16,3), sharex=True)\n    for col, i in dict(zip(levels, list(range(6)))).items():\n        X[col].plot(ax=ax[i], legend=True, linewidth=1.0, color=color, sharex=True)     \n    \n    fig.suptitle(f\"Historical trends of {leveltype} variables\", \n                 fontsize=12, fontweight=\"bold\")\n    \nplot_vars(X.values, levels = X.columns, color=\"red\", leveltype=\"levels\")\nplt.tight_layout()","a0ca287f":"from statsmodels.tsa.stattools import adfuller\ndef adfuller_test(series, signif=0.05, name='', verbose=False):\n    \"\"\"Perform ADFuller to test for Stationarity of given series and print report\"\"\"\n    r = adfuller(series, autolag='AIC')\n    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n    p_value = output['pvalue'] \n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Print Summary\n    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n    print(f' Significance Level    = {signif}')\n    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n\n    for key,val in r[4].items():\n        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n\n    if p_value <= signif:\n        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n        print(f\" => Series is Stationary.\")\n    else:\n        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n        print(f\" => Series is Non-Stationary.\")\n        \n# ADF Test on each column\nfor name, column in X.iteritems():\n    adfuller_test(column, name=column.name)\n    print()","067d7b40":"from statsmodels.tsa.stattools import kpss\ndef kpss_test(x, h0_type='c'):\n    indices = ['Test Statistic', 'p-value', '# of Lags']\n    kpss_test = kpss(x, regression=h0_type, nlags ='auto')\n    results = pd.Series(kpss_test[0:3], index=indices)\n    for key, value in kpss_test[3].items():\n        results[f'Critical Value ({key})'] = value\n        return results\nprint('KPSS-EURUSD:')\nprint(kpss_test(X.eurusd))\nprint('___________________')\nprint('KPSS-GBPUSD:')\nprint(kpss_test(X.gbpusd))\nprint('___________________')\nprint('KPSS-USDJPY:')\nprint(kpss_test(X.usdjpy))\nprint('___________________')\nprint('KPSS-GC:')\nprint(kpss_test(X.gc))\nprint('___________________')\nprint('KPSS-NQ:')\nprint(kpss_test(X.nq))\nprint('___________________')\nprint('KPSS-ES:')\nprint(kpss_test(X.es))","ce9998c7":"from scipy import stats\n\nstat,p = stats.normaltest(X.eurusd)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('EURUSD Data looks Gaussian (fail to reject H0)')\nelse:\n    print('EURUSD Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.gbpusd)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('GBPUSD Data looks Gaussian (fail to reject H0)')\nelse:\n    print('GBPUSD Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.usdjpy)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('USDJPY Data looks Gaussian (fail to reject H0)')\nelse:\n    print('USDJPY Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.es)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('ES Data looks Gaussian (fail to reject H0)')\nelse:\n    print('ES Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.nq)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('NQ Data looks Gaussian (fail to reject H0)')\nelse:\n    print('NQ Data do not look Gaussian (reject H0)')\nprint('______________')\nstat,p = stats.normaltest(X.gc)\nprint('Statistics=%.3f, p=%.3f' % (stat,p))\nalpha = 0.05\nif p > alpha:\n    print('GC Data looks Gaussian (fail to reject H0)')\nelse:\n    print('GC Data do not look Gaussian (reject H0)')\nprint('______________')\nprint('EURUSD: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.eurusd)))\nprint('EURUSD: Skewness of normal distribution: {}'. format(stats.skew(X.eurusd)))\nprint('************')\nprint('GBPUSD: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.gbpusd)))\nprint('GBPUSD: Skewness of normal distribution: {}'. format(stats.skew(X.gbpusd)))\nprint('************')\nprint('USDJPY: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.usdjpy)))\nprint('USDJPY: Skewness of normal distribution: {}'. format(stats.skew(X.usdjpy)))\nprint('************')\nprint('ES: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.es)))\nprint('ES: Skewness of normal distribution: {}'. format(stats.skew(df.es)))\nprint('************')\nprint('NQ: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.nq)))\nprint('NQ: Skewness of normal distribution: {}'. format(stats.skew(X.nq)))\nprint('************')\nprint('GC: Kurtosis of normal distribution: {}'. format(stats.kurtosis(X.gc)))\nprint('GC: Skewness of normal distribution: {}'. format(stats.skew(X.gc)))","ded147c8":"# visualization (EURUSD)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['eurusd'].hist(bins=50)\nplt.title('EURUSD')\nplt.subplot(1,2,2)\nstats.probplot(df['eurusd'], plot=plt);\nX['eurusd'].describe().T","e4012a25":"# visualization (GBPUSD)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['gbpusd'].hist(bins=50)\nplt.title('GBPUSD')\nplt.subplot(1,2,2)\nstats.probplot(df['gbpusd'], plot=plt);\nX['gbpusd'].describe().T","cc4bd90e":"# visualization (USDJPY)\npd.options.display.float_format = \"{:.2f}\".format\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['usdjpy'].hist(bins=50)\nplt.title('USDJPY')\nplt.subplot(1,2,2)\nstats.probplot(df['usdjpy'], plot=plt);\nX['usdjpy'].describe().T","d09f22de":"# visualization (ES)\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['es'].hist(bins=50)\nplt.title('ES')\nplt.subplot(1,2,2)\nstats.probplot(df['es'], plot=plt);\nX['es'].describe().T","64b4ea41":"# visualization (GC)\n\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nX['gc'].hist(bins=50)\nplt.title('gc')\nplt.subplot(1,2,2)\nstats.probplot(df['gc'], plot=plt);\nX['gc'].describe().T","8d98430f":"import seaborn as sns\n# Compute the correlation matrix\ncorr = X.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n# Heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot=True, fmt = '.4f', mask=mask, center=0, square=True, linewidths=.5)\nprint(\"value > 0.5 is considerred correlated, > 0.8 is highly correlated\")\nplt.show()\nprint('Correlation matrix:')\ncorr = X.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","2d70b57b":"from statsmodels.tsa.stattools import grangercausalitytests\nmax_lag = 6\ntest = 'ssr_chi2test'\ndef causation_matrix(data, variables, test='ssr_chi2test', verbose=False):\n    X = DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n    for c in X.columns:\n        for r in X.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag = max_lag, verbose = False)\n            p_values = [round(test_result[i+1][0][test][1],4) for i in range(max_lag)]\n            if verbose: print(f'Y = {r}, X = {c}, P Values = {p_values}')\n            min_p_value = np.min(p_values)\n            X.loc[r, c] = min_p_value\n    X.columns = [var + '-x axis' for var in variables]\n    X.index = [var + '-y axis' for var in variables]\n    return X\ncausation_matrix(X, variables = X.columns)","d5769e6e":"from statsmodels.tsa.vector_ar.vecm import VECM, select_order\nfrom statsmodels.tsa.vector_ar.vecm import coint_johansen\nfrom statsmodels.tsa.vector_ar.vecm import select_coint_rank\nfrom statsmodels.tsa.vector_ar.vecm import CointRankResults","9230122e":"nobs = 15\ntrain_ecm, test_ecm = X[0:-nobs], X[-nobs:]\n\n# Check size\nprint(train_ecm.shape)  \nprint(test_ecm.shape)","53c2cb32":"# VECM model fitting\nfrom statsmodels.tsa.vector_ar import vecm\n# pass \"1min\" frequency\ntrain_ecm.index = pd.DatetimeIndex(train_ecm.index).to_period('1min')\nmodel = vecm.select_order(train_ecm, maxlags=8)\nprint(model.summary())","0be23652":"# Johansen co-integration\npd.options.display.float_format = \"{:.2f}\".format\n\"\"\"definition of det_orderint:\n-1 - no deterministic terms; 0 - constant term; 1 - linear trend\"\"\"\npd.options.display.float_format = \"{:.2f}\".format\nmodel = coint_johansen(endog = train_ecm, det_order = 1, k_ar_diff = 3)\nprint('Eigen statistic:')\nprint(model.eig) \nprint()\nprint('Critical values:')\nd = DataFrame(model.cvt)\nd.rename(columns = {0:'90%', 1: '95%', 2:'99%'}, inplace=True)\nprint(d); print()\nprint('Trace statistic:')\nprint(DataFrame(model.lr1)) ","ba7d96b8":"# cointegration rank determination\nfrom statsmodels.tsa.vector_ar.vecm import select_coint_rank\nrank1 = select_coint_rank(train_ecm, det_order = 1, k_ar_diff = 3,\n                                   method = 'trace', signif=0.01)\nprint(rank1.summary())","7da19588":"rank2 = select_coint_rank(train_ecm, det_order = 1, k_ar_diff = 3, \n                              method = 'maxeig', signif=0.01)\n\nprint(rank2.summary())","ee23a84c":"# VECM fitting\n# VECM\nvecm = VECM(train_ecm, k_ar_diff=3, coint_rank = 3, deterministic='ci')\n\"\"\"estimates the VECM on the prices with 3 lags, 3 cointegrating relationship, and \na constant within the cointegration relationship\"\"\"\nvecm_fit = vecm.fit()\nprint(vecm_fit.summary())","c9af41f0":"# Residual auto-correlation\nfrom statsmodels.stats.stattools import durbin_watson\nout = durbin_watson(vecm_fit.resid)\nfor col, val in zip(train_ecm.columns, out):\n    print((col), ':', round(val, 2))","d6d73968":"# Impulse-response plot\nfrom statsmodels.tsa.vector_ar import irf\nirf = vecm_fit.irf(15)\nirf.plot(orth = False)\nplt.show()","cf0327ef":"plt.style.use('ggplot')\nirf.plot(impulse='eurusd')\nplt.show()","cba4f8ed":"plt.style.use('ggplot')\nirf.plot(impulse='usdjpy', orth = True)\nplt.show()","cf229546":"plt.style.use('ggplot')\nirf.plot(impulse='es')\nplt.show()","10b6b965":"plt.style.use('ggplot')\nirf.plot(impulse='gc')\nplt.show()","42f66b33":"plt.style.use('ggplot')\nirf.plot(impulse='nq')\nplt.show()","e40f8366":"# prediction\npd.options.display.float_format = \"{:.2f}\".format\nforecast, lower, upper = vecm_fit.predict(nobs, 0.05)\nprint(\"lower bounds of confidence intervals:\")\nprint(DataFrame(lower.round(2)))\nprint(\"\\npoint forecasts:\")\nprint(DataFrame(forecast.round(2)))\nprint(\"\\nupper bounds of confidence intervals:\")\nprint(DataFrame(upper.round(2)))","61981fe8":"pd.options.display.float_format = \"{:.2f}\".format\nforecast = DataFrame(forecast, index= test_ecm.index, columns= test_ecm.columns)\nforecast.rename(columns = {'eurusd':'eurusd_pred', 'gbpusd':'gbpusd_pred', 'usdjpy':'usdjpy_pred',\n                    'gc':'gc_pred', 'nq':'nq_pred', 'es':'es_pred'}, inplace = True)\nforecast","ee6e9260":"combine = concat([test_ecm, forecast], axis=1)\npred = combine[['eurusd', 'eurusd_pred', 'gbpusd', 'gbpusd_pred', 'usdjpy', \n                   'usdjpy_pred', 'gc', 'gc_pred', 'nq', 'nq_pred', 'es', 'es_pred']]\ndef highlight_cols(s):\n    color = 'yellow'\n    return 'background-color: %s' % color\n\npred.style.applymap(highlight_cols, subset=pd.IndexSlice[:, ['eurusd_pred', 'gbpusd_pred', 'usdjpy_pred',\n                                                               'gc_pred', 'nq_pred', 'es_pred']])","29c753ae":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# score eur_usd\nmae = mean_absolute_error(pred.eurusd, pred['eurusd_pred'])\nmse = mean_squared_error(pred.eurusd, pred.eurusd_pred)\nrmse = np.sqrt(mse)\nsum = DataFrame(index = ['Mean Absolute Error', 'Mean squared error', 'Root mean squared error'])\nsum['Accuracy metrics :    EURUSD'] = [mae, mse, rmse]\n\n# score gbp_usd\nmae = mean_absolute_error(pred.gbpusd, pred['gbpusd_pred'])\nmse = mean_squared_error(pred.gbpusd, pred.gbpusd_pred)\nrmse = np.sqrt(mse)\nsum['GBPUSD'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.usdjpy, pred['usdjpy_pred'])\nmse = mean_squared_error(pred.usdjpy, pred.usdjpy_pred)\nrmse = np.sqrt(mse)\nsum['USDJPY'] = [mae, mse, rmse]\n\n# score nq\nmae = mean_absolute_error(pred.nq, pred['nq_pred'])\nmse = mean_squared_error(pred.nq, pred.nq_pred)\nrmse = np.sqrt(mse)\nsum['NQ'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.es, pred['es_pred'])\nmse = mean_squared_error(pred.es, pred.es_pred)\nrmse = np.sqrt(mse)\nsum['ES'] = [mae, mse, rmse]\n\n# score usd_jpy\nmae = mean_absolute_error(pred.gc, pred['gc_pred'])\nmse = mean_squared_error(pred.gc, pred.gc_pred)\nrmse = np.sqrt(mse)\nsum['GC'] = [mae, mse, rmse]\nsum","30e8249a":"**Vector Error Correction Model Configuration & Analysis :: original story was published [here](https:\/\/sarit-maitra.medium.com\/vector-error-correction-model-configuration-analysis-95770699d6a5)**\n\nError correction model (ECM)is important in time-series analysis to better understand long-run dynamics. ECM can be derived from auto-regressive distributed lag model as long as there is a cointegration relationship between variables. In that context, each equation in the vector auto regressive (VAR) model is an autoregressive distributed lag model; therefore, it can be considered that the vector error correction model (VECM) is a VAR model with cointegration constraints."}}