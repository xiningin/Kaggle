{"cell_type":{"dcc5ed15":"code","f2eaa989":"code","24836c90":"code","396bc00e":"code","70421ee8":"code","318ac7db":"code","b0e79f0e":"code","53e3b5d7":"code","232baf33":"code","21d7e25d":"code","3f3564c6":"code","1786f5bd":"code","b818a262":"code","f15fcdd5":"code","4b0bb3d4":"code","f916c6df":"code","94a653c2":"code","52065d05":"code","ee1d0f21":"code","ff7e8a5c":"code","007d97a5":"code","550e4a4c":"code","fc63643d":"code","a2704d9a":"code","293cd57e":"code","8ad829ce":"code","4b99ef7d":"code","5dc2204a":"code","53a3da38":"code","d28e3b93":"code","e9f3026a":"code","74ba8018":"code","bbc71193":"code","f6ff673c":"code","5778773d":"code","c3a818f1":"code","73fef8a6":"code","41b4b1ef":"code","a6d48a8a":"code","027eea13":"code","850fa065":"code","10a2c62a":"code","9d0e4cd1":"code","267134bc":"code","bb820e2c":"code","8218a679":"code","683a357a":"code","64b635ce":"code","358df2b8":"markdown","ac9dee99":"markdown","af52964a":"markdown","e9f706dc":"markdown","f7a0010b":"markdown","d1ccaa93":"markdown","bf3d864a":"markdown","e08277ef":"markdown","cf9a8210":"markdown","2e0795ee":"markdown","ad1f0ad9":"markdown","23deb20d":"markdown","ee94dec4":"markdown","0dca8671":"markdown","b883eb4b":"markdown","c55bbabf":"markdown","33cb4209":"markdown","45822d7e":"markdown","55a92644":"markdown","dcd41954":"markdown","2a6262ee":"markdown","9b385f2c":"markdown","2e7b4772":"markdown","50862fad":"markdown","da68186d":"markdown","c7d50e9e":"markdown","cb2c552d":"markdown"},"source":{"dcc5ed15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2eaa989":"from warnings import filterwarnings\nfilterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina'\nsns.set()","24836c90":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","396bc00e":"df.columns.values","70421ee8":"df.shape","318ac7db":"#Lets start looking the difference by Normal and Fraud transactions\nprint(\"Distribuition of Normal(0) and Frauds(1): \")\nprint(df[\"Class\"].value_counts())\n\nplt.figure(figsize=(7,5))\nsns.countplot(df['Class'])\nplt.title(\"Class Count\", fontsize=18)\nplt.xlabel(\"Is fraud?\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.legend(['normal', 'fraud'])\nplt.show()","b0e79f0e":"### Normal transaction\ndf[df.Class == 0].hist(figsize = (20,20))\nplt.show()","53e3b5d7":"### Fraud transaction\ndf[df.Class == 1].hist(figsize = (20,20))\nplt.show()","232baf33":"timedelta = pd.to_timedelta(df['Time'], unit='s')\ndf['Time_min'] = (timedelta.dt.components.minutes).astype(int)\ndf['Time_hour'] = (timedelta.dt.components.hours).astype(int)","21d7e25d":"#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(df[df['Class'] == 0][\"Time_hour\"], \n             color='g')\nsns.distplot(df[df['Class'] == 1][\"Time_hour\"], \n             color='r')\nplt.title('Fraud x Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","3f3564c6":"#Exploring the distribuition by Class types throught hours and minutes\nplt.figure(figsize=(12,5))\nsns.distplot(df[df['Class'] == 0][\"Time_min\"], \n             color='g')\nsns.distplot(df[df['Class'] == 1][\"Time_min\"], \n             color='r')\nplt.title('Fraud x Normal Transactions by minutes', fontsize=17)\nplt.xlim([-1,61])\nplt.show()\n","1786f5bd":"df.shape","b818a262":"import matplotlib.gridspec as gridspec # to do the grid of plots #  gridspec work same as plt.subplots\ncolumns  = df.iloc[:,df.columns  != 'Class'].columns\nfrauds = df.Class == 1\nnormals = df.Class == 0\ngrid = gridspec.GridSpec(17, 2)\nplt.figure(figsize=(10,15*4))\n\nfor n, col in enumerate(df[columns]):\n    ax = plt.subplot(grid[n])\n    sns.distplot(df[col][frauds], bins = 50, color='g') #Will receive the \"semi-salmon\" violin\n    sns.distplot(df[col][normals], bins = 50, color='r') #Will receive the \"ocean\" color\n    ax.set_ylabel('Density')\n    ax.set_title(str(col))\n    ax.set_xlabel('')\nplt.show()","f15fcdd5":"df_selected = df.drop(['V15','V17','V24','V27','Time_hour','Time','Time_min'], axis = 1)","4b0bb3d4":"df_selected.shape","f916c6df":"from sklearn.model_selection import train_test_split\n\ndf_training, df_testing = train_test_split(df_selected,test_size = 0.2, random_state = 42,stratify = df_selected.Class)","94a653c2":"df_testing.Class.value_counts()","52065d05":"train_data,validation_data,train_lable, validation_lable = train_test_split(df_training.loc[:,df_training.columns != 'Class'],df_training.Class,test_size = 0.2, stratify  = df_training.Class,\n                                                random_state = 42) ","ee1d0f21":"##### here we are going to use min max scaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit(train_data)\ntrain_data_normalised = data_scaled.transform(train_data)\nvalidation_data_normalised = data_scaled.transform(validation_data)\n","ff7e8a5c":"test_data = df_testing.loc[:, df_testing.columns != 'Class']\ntest_lable = df_testing.Class\ntest_data_normalised = data_scaled.transform(test_data)","007d97a5":"test_data.shape","550e4a4c":"train_data_normalised.shape","fc63643d":"# lets change the labels with boolean \ntrain_lable,validation_lable, test_lable = train_lable.astype(bool), validation_lable.astype(bool), test_lable.astype(bool)\n","a2704d9a":"# now  lets seperate the normal and fraud data out of training dataset \nnormal_train_data = train_data_normalised[~train_lable] # normal transactions out of train_data_normalised \nnormal_test_data = test_data_normalised[~test_lable] # normal transactions out of test_data_normalised\nnormal_validation_data = validation_data_normalised[~validation_lable]","293cd57e":"print(len(normal_train_data))\nprint(len(normal_test_data))\nprint(len(normal_validation_data))","8ad829ce":"fraud_train_data = train_data_normalised[train_lable]\nfraud_test_data = test_data_normalised[test_lable]\nfraud_validation_data = validation_data_normalised[validation_lable]","4b99ef7d":"print(len(fraud_train_data))\nprint(len(fraud_test_data))\nprint(len(fraud_validation_data))","5dc2204a":"normal_train_data[0].shape","53a3da38":"from matplotlib.pyplot import figure\n\nfigure(figsize=(10, 6), dpi=80)\nplt.plot(np.arange(25), normal_train_data[1])\nplt.title('Normal transactions')\n\nplt.show()\n","d28e3b93":"#Lets plot one from fraud transaction\nfigure(figsize=(10, 6), dpi=80)\n\nplt.plot(np.arange(25), fraud_train_data[2])\nplt.grid()\nplt.title('Fraud transaction')\nplt.show()","e9f3026a":"import tensorflow \nfrom tensorflow.keras.layers import Dense,LSTM\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import models,layers,activations,losses,optimizers,metrics\nfrom tensorflow.keras.callbacks import EarlyStopping","74ba8018":"n_features = len(train_data.columns)\nencoder = models.Sequential(name='encoder')\nencoder.add(layer=layers.Dense(units=200, activation=activations.relu, input_shape=[n_features]))\nencoder.add(layers.Dropout(0.1))\nencoder.add(layer=layers.Dense(units=100, activation=activations.relu))\nencoder.add(layer=layers.Dense(units=5, activation=activations.relu))\n\ndecoder = models.Sequential(name='decoder')\ndecoder.add(layer=layers.Dense(units=100, activation=activations.relu, input_shape=[5]))\ndecoder.add(layer=layers.Dense(units=200, activation=activations.relu))\ndecoder.add(layers.Dropout(0.1))\ndecoder.add(layer=layers.Dense(units=n_features, activation=activations.sigmoid))\n\nautoencoder = models.Sequential([encoder, decoder])\n\nautoencoder.compile(\n\tloss=losses.MSE,\n\toptimizer=optimizers.Adam(),\n\tmetrics=[metrics.mean_squared_error])","bbc71193":"# train model\nes = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5,restore_best_weights=True)\nhistory = autoencoder.fit(x=normal_train_data, y=normal_train_data, epochs=100, verbose=1, validation_data=(normal_validation_data, normal_validation_data), callbacks=[es])\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f6ff673c":"#Now let's define a function in order to plot the original ECG and reconstructed ones and also show the error\ndef plot(data, n):\n  dec_img = autoencoder.predict(data) # this will decode or reconstruct\n  plt.plot(data[n], 'b')\n  plt.plot(dec_img[n], 'r')\n  plt.fill_between(np.arange(25), data[n], dec_img[n], color = 'lightcoral')\n  plt.legend(labels=['Input', 'Reconstruction', 'Error'])\n  plt.show()\n\nplot(normal_test_data, 4) # here n shows the index of ecg samples\nplot(fraud_test_data, 4)","5778773d":"### calculating the train error  in legit transactions\nreconstructed = autoencoder.predict(normal_train_data)\ntrain_normal_loss = losses.mae(reconstructed, normal_train_data).numpy()\nnp.mean(train_normal_loss)","c3a818f1":"plt.hist(train_normal_loss, bins = 50)\nplt.title('error normal train loss')\nplt.show()","73fef8a6":"### calculating the train error  in fraud transactions\nreconstructed = autoencoder.predict(fraud_train_data)\ntrain_fraud_loss = losses.mae(reconstructed, fraud_train_data).numpy()\nprint(np.mean(train_fraud_loss))\nplt.hist(train_fraud_loss, bins = 50)\nplt.title('error fraud train loss')\nplt.show()\n","41b4b1ef":"Fraud = df.Class.sum()\ntotal = len(df)\nfraud_percentage = (Fraud\/total)*100\nfraud_percentage\n# so lets say that 1% of transactions are fraud so lets find the threshold","a6d48a8a":"# calcualting the error in the whole train data \n\nreconstructed = autoencoder.predict(train_data_normalised)\ntrain_loss = losses.mae(reconstructed, train_data_normalised).numpy()\n","027eea13":"# we are assuming that we have 1% of fraud transaction here \ncut_off_1 = np.percentile(train_loss, 99)\ncut_off_1","850fa065":"cut_off_2 = np.mean(train_loss) + 5*np.std(train_loss)\ncut_off_2\n# 2std division covers 97% of the distribution and here i have taken 3 std division","10a2c62a":"reconstructed = autoencoder.predict(test_data_normalised)\nerrors = losses.mae(reconstructed,test_data_normalised)\nlen(errors)\npredicted = []\nfor error in errors:\n    if error > cut_off_1:\n        predicted.append(1)\n    else:\n        predicted.append(0)\n","9d0e4cd1":"np.sort(errors)[::-1]","267134bc":"predicted_fraud_cases = np.array(predicted).sum()\nactual_fraud_cases = test_lable.sum()\nprint('predicted_fruad_cases =>'.format(), predicted_fraud_cases, 'true fraud cases =>'.format(), actual_fraud_cases)","bb820e2c":"true_lable = test_lable.map({True:1,False:0})","8218a679":"### classification report \nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(true_lable, predicted))\nprint(confusion_matrix(true_lable, predicted))\n","683a357a":"from matplotlib import lines\nplot_samples = 98\nplt.figure(figsize=(12,5))\n\n# normal event\nn_features = 25\nreal_x = normal_test_data[:plot_samples]\npredicted_x = autoencoder.predict(x=real_x)\nnormal_events_mae = losses.mae(real_x, predicted_x)\nnormal_events_df = pd.DataFrame({\n\t'mae': normal_events_mae,\n\t'n': np.arange(0, plot_samples),\n\t'anomaly': np.zeros(plot_samples)})\n\n\n# abnormal event\nabnormal_x = fraud_test_data[:plot_samples]\npredicted_x = autoencoder.predict(x=abnormal_x)\nabnormal_events_mae = losses.mae(abnormal_x, predicted_x)\nabnormal_events_df = pd.DataFrame({\n\t'mae': abnormal_events_mae,\n\t'n': np.arange(0, plot_samples),\n\t'anomaly': np.ones(plot_samples)})\nabnormal_events_df\n\nmae_df = pd.concat([normal_events_df, abnormal_events_df])\nplot = sns.lineplot(x=mae_df.n, y=mae_df.mae, hue=mae_df.anomaly)\n\nline = lines.Line2D(\n\txdata=np.arange(0, plot_samples),\n\tydata=np.full(plot_samples, cut_off_1),\n\tcolor='#CC2B5E',\n\tlinewidth=1.5,\n\tlinestyle='dashed')\n\nplot.add_artist(line)\nplt.title('Threshlold: {threshold}'.format(threshold=cut_off_1))\nplt.show()","64b635ce":"plt.figure(figsize=(15,5))\n\n# plt.hist(train_normal_loss, bins = 50, label = 'normal')\nplt.hist(train_fraud_loss, bins = 50, label = 'abnormal')\nplt.axvline(cut_off_1, color = 'r', linewidth = 3, linestyle = 'dashed', label = '{:0.3f}'.format(cut_off_1))\nplt.legend(loc = 'upper right')\nplt.show()","358df2b8":"now we need to prepare our data for training the AutoEncoder model ,\nwe need to train our model on normal data and then we will test our model on abnormal data and then we set a threshold","ac9dee99":"here features those follow same distribution for both legitimate and fraud transactions are not important features \n\nso V15,V17,V24,V27,Time_hour,Time_min,Time are not important at all\n","af52964a":"#### Lets split our data into training and testing ","e9f706dc":"### Credit Card Fraud Detection Using AutoEncoder model \n#### In this notebook you will learn \n* EDA of credit card data\n* We will see Feaure Engineering \n* Feature Selection \n* Plotting \n* Creating AutoEncoder model on tesorflow usig sequential API\n* then we will see Inference Phase\n\n#### Please UPvote this notebook and help me to get my first Bronze\ud83d\ude4f","f7a0010b":"### Building our Auto-Encoder model","d1ccaa93":"#### this data is highly imbalanced so our classification task will fail \n#### so we will be using encoder-decoder technique to make a anamoly detector","bf3d864a":"#### if error is greater than this value we would say fraud transaction if lesser than cut-off means legitimate transaction","e08277ef":"#### its time to scale  our dataset so that our model can converge ","cf9a8210":"#### Final takeout:\n* we can improve this model by using LSTM encoder decoder model \n* more training data means more better results\n* this anamoly detection also can take it as classification problem but that will not work well ","2e0795ee":"#### Lets do some feature selection using distribution graph","ad1f0ad9":"#### loading the dataset","23deb20d":"#### here we can see that there is a difference in fraud and legit transaction according tho their plot","ee94dec4":"### time for prediction on our TEST data","0dca8671":"#### Calculating error(loss) for normal transaction train data ","b883eb4b":"### Time to do some feature preprocessing ","c55bbabf":"### normal transaction","33cb4209":"### Fraud transaction","45822d7e":"#### lets find the train fraud loss","55a92644":"#### here we can see the range of error in fraud transactions are so greater than the error of normal transaction\n#### so we will be using this fact to test our anamoly fraud transaction detection","dcd41954":"#### here time is given in the seconds so we need to change that in minute and hour, and then we will do some feature selection","2a6262ee":"#### lets plot our normal data ","9b385f2c":"### There are two ways to set our threshold \n* 1. set up by using percentile (suppose we have 2% of fraud transaction so our aim will be filter 2% probable data)\n* 2. set up by using distribution and try to find the outliers ","2e7b4772":"**here this dataset is made by using PCA (dimensionality reduction technique)**","50862fad":"**Here the Column Class holds 0 and 1 ( 0 means legitimate transaction and 1 means fraudulant**","da68186d":"#### now lets see the importance of the feature we just added ( minute and hour)","c7d50e9e":"### INFERENCE PHASE ! here we have only train data and we need to set threshold to catch all fraud transaction","cb2c552d":"we dont see much useful information out of Time_minute, Time_hour feature we created so we will drop minute feature "}}