{"cell_type":{"917bef1f":"code","ecc4cc7a":"code","49af4292":"code","aa1cb503":"code","c4412d3d":"code","21c30efd":"code","3f391a2d":"code","6b34eb81":"code","fafe1a12":"code","6014818a":"code","725d7d6b":"code","b90e4c25":"code","e7568153":"code","da73bba6":"code","5c1694dd":"code","4f1f8ef2":"code","6ccf1632":"code","10f85420":"code","1b1e56c3":"code","2b496446":"code","c40a54b1":"markdown","b29da8c7":"markdown","12c31ea8":"markdown","2013b731":"markdown","929b518f":"markdown"},"source":{"917bef1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecc4cc7a":"data_set = '\/kaggle\/input\/nlp-news-text\/train_set.csv'\ntest_set_a = '\/kaggle\/input\/nlp-news-text\/test_a.csv'\ntest_set_b = '\/kaggle\/input\/nlp-news-text\/test_b.csv'\n","49af4292":"import logging\nimport random\n\nimport numpy as np\nimport torch\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n\n# set seed \nseed = 666\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.manual_seed(seed)\n\n# set cuda\ngpu = 0\nuse_cuda = gpu >= 0 and torch.cuda.is_available()\nif use_cuda:\n    torch.cuda.set_device(gpu)\n    device = torch.device(\"cuda\", gpu)\nelse:\n    device = torch.device(\"cpu\")\nprint(\"Use cuda: %s, gpu id: %d\uff0cdevice:%s\"%(use_cuda, gpu,device))","aa1cb503":"# split data to 10 fold\nfold_num = 10\ndata_file = data_set\nimport pandas as pd\n\n\ndef all_data2fold(fold_num, num=10000):\n    fold_data = [] # \u4ea4\u53c9\u9a8c\u8bc1\u7684\u5143\u7d20\uff0c\u6bcf\u4e2a\u5143\u7d20\u662fdict\uff0ckey\u6709\u4e24\u4e2a\u5305\u62ec\u4e86label\u548ctext\n    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n    texts = f['text'].tolist()[:num]\n    labels = f['label'].tolist()[:num]\n\n    total = len(labels)\n    # \u6253\u4e71\u4e86text\u548clabel\u7684\u987a\u5e8f\n    index = list(range(total))\n    np.random.shuffle(index)\n\n    all_texts = []\n    all_labels = []\n    for i in index:\n        all_texts.append(texts[i])\n        all_labels.append(labels[i])\n    # label2id \u4fdd\u5b58label\u5bf9\u5e94\u7684\u884c\uff0c\u4e00\u4e2alabel\u53ef\u80fd\u6709\u5f88\u591a\u884c\uff0c\n    # \u6240\u4ee5key\u4e3alabel,value\u4e3alist\uff0c\u5b58\u50a8\u8be5\u7c7b\u5bf9\u5e94\u7684index\n    label2id = {}\n    for i in range(total):\n        label = str(all_labels[i])\n        if label not in label2id:\n            label2id[label] = [i]\n        else:\n            label2id[label].append(i)\n    # fold_num\u8fd9\u91cc\u662f10\uff0c\u8fd9\u91cc\u662f\u628a\u6bcf\u4e2alabel\u7684\u5bf9\u5e94\u7684index\u5e73\u5747\u5206\u523010\u4efd\u6570\u636e\u4e2d\n    all_index = [[] for _ in range(fold_num)]\n    for label, data in label2id.items():\n        # print(label, len(data))\n        batch_size = int(len(data) \/ fold_num)# \u6bcf\u4e2alabel\u5212\u5230\u4e00\u4efd\u6570\u636e\u7684\u5143\u7d20\u4e2a\u6570\n        other = len(data) - batch_size * fold_num # \u8fd8\u5269\u4e0b\u7684\u5143\u7d20 other < 10\n        for i in range(fold_num):\n            cur_batch_size = batch_size + 1 if i < other else batch_size # \u603b\u662f+1\u6267\u884c\u4e86other\u6b21\n            # print(cur_batch_size)\n            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n            all_index[i].extend(batch_data)# all_index\u5b58\u6bcf\u4e2afold\u5b58\u653e\u7684\u5bf9\u5e94\u7684index\n\n    batch_size = int(total \/ fold_num)# \u6240\u6709\u7684\u6570\u636e \uff08num\uff09 \/ 10\uff0c\u5373\u6bcf\u4e2afold\u5e73\u5747\u7684labels\u6570\u636e\n    other_texts = []\n    other_labels = []\n    other_num = 0\n    start = 0\n    for fold in range(fold_num):\n        # texts \u548c labels\u662f\u6bcf\u4e2afold\u4e0b\u7684text \u548c label\n        num = len(all_index[fold])\n        texts = [all_texts[i] for i in all_index[fold]]\n        labels = [all_labels[i] for i in all_index[fold]]\n\n        if num > batch_size:\n            fold_texts = texts[:batch_size]\n            other_texts.extend(texts[batch_size:])\n            fold_labels = labels[:batch_size]\n            other_labels.extend(labels[batch_size:])\n            other_num += num - batch_size\n        elif num < batch_size:\n            end = start + batch_size - num\n            fold_texts = texts + other_texts[start: end]\n            fold_labels = labels + other_labels[start: end]\n            start = end\n        else:\n            fold_texts = texts\n            fold_labels = labels\n\n        assert batch_size == len(fold_labels)\n        \n        # shuffle\n        index = list(range(batch_size))\n        np.random.shuffle(index)\n\n        shuffle_fold_texts = []\n        shuffle_fold_labels = []\n        for i in index:\n            shuffle_fold_texts.append(fold_texts[i])\n            shuffle_fold_labels.append(fold_labels[i])\n\n        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n        fold_data.append(data)\n\n    print(\"Fold lens %s\"%(str([len(data['label']) for data in fold_data])))\n\n    return fold_data\n\n\nfold_data = all_data2fold(10)","c4412d3d":"# build train, dev, test data\nfold_id = 9\n\n# dev\ndev_data = fold_data[fold_id]\n\n# train\ntrain_texts = []\ntrain_labels = []\nfor i in range(0, fold_id):\n    data = fold_data[i]\n    train_texts.extend(data['text'])\n    train_labels.extend(data['label'])\n\ntrain_data = {'label': train_labels, 'text': train_texts}\n\n# test\ntest_data_file = test_set_a\nf = pd.read_csv(test_data_file, sep='\\t', encoding='UTF-8')\ntexts = f['text'].tolist()\ntest_data = {'label': [0] * len(texts), 'text': texts}","21c30efd":"# build vocab\nfrom collections import Counter\nfrom transformers import BasicTokenizer\n\nbasic_tokenizer = BasicTokenizer()\n\n# Vocab \u7684\u4f5c\u7528\u662f\uff1a\n# 1. \u521b\u5efa \u8bcd \u548c index \u5bf9\u5e94\u7684\u5b57\u5178\uff0c\u8fd9\u91cc\u5305\u62ec 2 \u4efd\u5b57\u5178\uff0c\u5206\u522b\u662f\uff1a_id2word \u548c _id2extword\n# \u5176\u4e2d _id2word \u662f\u4ece\u65b0\u95fb\u5f97\u5230\u7684\uff0c \u628a\u8bcd\u9891\u5c0f\u4e8e 5 \u7684\u8bcd\u66ff\u6362\u4e3a\u4e86 UNK\u3002\u5bf9\u5e94\u5230\u6a21\u578b\u8f93\u5165\u7684 batch_inputs1\u3002\n# _id2extword \u662f\u4ece word2vec.txt \u4e2d\u5f97\u5230\u7684\uff0c\u6709 5976 \u4e2a\u8bcd\u3002\u5bf9\u5e94\u5230\u6a21\u578b\u8f93\u5165\u7684 batch_inputs2\u3002\n# \u540e\u9762\u4f1a\u6709\u4e24\u4e2a embedding \u5c42\uff0c\u5176\u4e2d _id2word \u5bf9\u5e94\u7684 embedding \u662f\u53ef\u5b66\u4e60\u7684\uff0c_id2extword \u5bf9\u5e94\u7684 embedding \u662f\u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u7684\uff0c\u662f\u56fa\u5b9a\u7684\n# 2.\u521b\u5efa label \u548c index \u5bf9\u5e94\u7684\u5b57\u5178\n\nclass Vocab():\n    def __init__(self, train_data):\n        self.min_count = 5\n        self.pad = 0\n        self.unk = 1\n        self._id2word = ['[PAD]', '[UNK]']\n        self._id2extword = ['[PAD]', '[UNK]']\n\n        self._id2label = []\n        self.target_names = []\n\n        self.build_vocab(train_data)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        #\u521b\u5efa\u8bcd\u548c index \u5bf9\u5e94\u7684\u5b57\u5178\n        self._word2id = reverse(self._id2word)\n        #\u521b\u5efa label \u548c index \u5bf9\u5e94\u7684\u5b57\u5178\n        self._label2id = reverse(self._id2label)\n\n        logging.info(\"Build vocab: words %d, labels %d.\" % (self.word_size, self.label_size))\n\n    #\u521b\u5efa\u8bcd\u5178\n    def build_vocab(self, data):\n        self.word_counter = Counter()\n        #\u8ba1\u7b97\u6bcf\u4e2a\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\n        for text in data['text']:\n            words = text.split()\n            for word in words:\n                self.word_counter[word] += 1\n        # \u53bb\u6389\u9891\u6b21\u5c0f\u4e8e min_count = 5 \u7684\u8bcd\uff0c\u628a\u8bcd\u5b58\u5230 _id2word\n        for word, count in self.word_counter.most_common():\n            if count >= self.min_count:\n                self._id2word.append(word)\n\n        label2name = {0: '\u79d1\u6280', 1: '\u80a1\u7968', 2: '\u4f53\u80b2', 3: '\u5a31\u4e50', 4: '\u65f6\u653f', 5: '\u793e\u4f1a', 6: '\u6559\u80b2', 7: '\u8d22\u7ecf',\n                      8: '\u5bb6\u5c45', 9: '\u6e38\u620f', 10: '\u623f\u4ea7', 11: '\u65f6\u5c1a', 12: '\u5f69\u7968', 13: '\u661f\u5ea7'}\n\n        self.label_counter = Counter(data['label'])\n\n        for label in range(len(self.label_counter)):\n            count = self.label_counter[label] # \u53d6\u51fa label \u5bf9\u5e94\u7684\u6b21\u6570\n            self._id2label.append(label) \n            self.target_names.append(label2name[label]) # \u6839\u636elabel\u6570\u5b57\u53d6\u51fa\u5bf9\u5e94\u7684\u540d\u5b57\n\n    def load_pretrained_embs(self, embfile):\n        with open(embfile, encoding='utf-8') as f:\n            lines = f.readlines()\n            items = lines[0].split()\n            # \u7b2c\u4e00\u884c\u5206\u522b\u662f\u5355\u8bcd\u6570\u91cf\u3001\u8bcd\u5411\u91cf\u7ef4\u5ea6\n            word_count, embedding_dim = int(items[0]), int(items[1])\n\n        index = len(self._id2extword)\n        embeddings = np.zeros((word_count + index, embedding_dim))\n        # \u4e0b\u9762\u7684\u4ee3\u7801\u548c word2vec.txt \u7684\u7ed3\u6784\u6709\u5173\n        for line in lines[1:]:\n            values = line.split()\n            self._id2extword.append(values[0]) # \u9996\u5148\u6dfb\u52a0\u7b2c\u4e00\u5217\u7684\u5355\u8bcd\n            vector = np.array(values[1:], dtype='float64') # \u7136\u540e\u6dfb\u52a0\u540e\u9762 100 \u5217\u7684\u8bcd\u5411\u91cf\n            embeddings[self.unk] += vector\n            embeddings[index] = vector\n            index += 1\n\n        # unk \u7684\u8bcd\u5411\u91cf\u662f\u6240\u6709\u8bcd\u7684\u5e73\u5747\n        embeddings[self.unk] = embeddings[self.unk] \/ word_count\n        # \u9664\u4ee5\u6807\u51c6\u5dee\u5e72\u561b\uff1f\n        embeddings = embeddings \/ np.std(embeddings)\n\n        reverse = lambda x: dict(zip(x, range(len(x))))\n        self._extword2id = reverse(self._id2extword)\n\n        assert len(set(self._id2extword)) == len(self._id2extword)\n\n        return embeddings\n\n    # \u6839\u636e\u5355\u8bcd\u5f97\u5230 id\n    def word2id(self, xs):\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.unk) for x in xs]\n        return self._word2id.get(xs, self.unk)\n    # \u6839\u636e\u5355\u8bcd\u5f97\u5230 ext id\n    def extword2id(self, xs):\n        if isinstance(xs, list):\n            return [self._extword2id.get(x, self.unk) for x in xs]\n        return self._extword2id.get(xs, self.unk)\n    # \u6839\u636e label \u5f97\u5230 id\n    def label2id(self, xs):\n        if isinstance(xs, list):\n            return [self._label2id.get(x, self.unk) for x in xs]\n        return self._label2id.get(xs, self.unk)\n\n    @property\n    def word_size(self):\n        return len(self._id2word)\n\n    @property\n    def extword_size(self):\n        return len(self._id2extword)\n\n    @property\n    def label_size(self):\n        return len(self._id2label)\n\n\nvocab = Vocab(train_data)","3f391a2d":"# build module\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Attention(nn.Module):\n    def __init__(self, hidden_size):\n        super(Attention, self).__init__()\n        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n        self.weight.data.normal_(mean=0.0, std=0.05)\n\n        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n        b = np.zeros(hidden_size, dtype=np.float32)\n        self.bias.data.copy_(torch.from_numpy(b))\n\n        self.query = nn.Parameter(torch.Tensor(hidden_size))\n        self.query.data.normal_(mean=0.0, std=0.05)\n\n    def forward(self, batch_hidden, batch_masks):\n        # batch_hidden: b * doc_len * hidden_size (2 * hidden_size of lstm)\n        # batch_masks:  b x doc_len\n\n        # linear\n        # key\uff1a b * doc_len * hidden\n        key = torch.matmul(batch_hidden, self.weight) + self.bias \n\n        # compute attention\n        # matmul \u4f1a\u8fdb\u884c\u5e7f\u64ad\n        #outputs: b * doc_len\n        outputs = torch.matmul(key, self.query)  \n        # 1 - batch_masks \u5c31\u662f\u53d6\u53cd\uff0c\u628a\u6ca1\u6709\u5355\u8bcd\u7684\u53e5\u5b50\u7f6e\u4e3a 0\n        # masked_fill \u7684\u4f5c\u7528\u662f \u5728 \u4e3a 1 \u7684\u5730\u65b9\u66ff\u6362\u4e3a value: float(-1e32)\n        masked_outputs = outputs.masked_fill((1 - batch_masks).bool(), float(-1e32))\n        #attn_scores\uff1ab * doc_len\n        attn_scores = F.softmax(masked_outputs, dim=1)  \n\n        # \u5bf9\u4e8e\u5168\u96f6\u5411\u91cf\uff0c-1e32\u7684\u7ed3\u679c\u4e3a 1\/len, -inf\u4e3anan, \u989d\u5916\u88650\n        masked_attn_scores = attn_scores.masked_fill((1 - batch_masks).bool(), 0.0)\n\n        # sum weighted sources\n        # masked_attn_scores.unsqueeze(1)\uff1a# b * 1 * doc_len\n        # key\uff1ab * doc_len * hidden\n        # batch_outputs\uff1ab * hidden\n        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(1), key).squeeze(1)  \n\n        return batch_outputs, attn_scores\n","6b34eb81":"# \u8bfb\u53d6\u8bad\u7ec3\u597d\u7684\u8bcd\u5411\u91cf\u6587\u4ef6\nword2vec_path = '\/kaggle\/input\/nlp-news-text\/word2vec.txt'\ndropout = 0.15\n","fafe1a12":"# \u8f93\u5165\u662f\uff1a\n# \u8f93\u51fa\u662f\uff1a\nclass WordCNNEncoder(nn.Module):\n    def __init__(self, vocab):\n        super(WordCNNEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.word_dims = 100 # \u8bcd\u5411\u91cf\u7684\u957f\u5ea6\u662f 100 \u7ef4\n        # padding_idx \u8868\u793a\u5f53\u53d6\u7b2c 0 \u4e2a\u8bcd\u65f6\uff0c\u5411\u91cf\u5168\u4e3a 0\n        # \u8fd9\u4e2a Embedding \u5c42\u662f\u53ef\u5b66\u4e60\u7684\n        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=0)\n\n        extword_embed = vocab.load_pretrained_embs(word2vec_path)\n        extword_size, word_dims = extword_embed.shape\n        logging.info(\"Load extword embed: words %d, dims %d.\" % (extword_size, word_dims))\n\n        # # \u8fd9\u4e2a Embedding \u5c42\u662f\u4e0d\u53ef\u5b66\u4e60\u7684\n        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=0)\n        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))\n        self.extword_embed.weight.requires_grad = False\n\n        input_size = self.word_dims\n\n        self.filter_sizes = [2, 3, 4]  # n-gram window\n        self.out_channel = 100\n        # 3 \u4e2a\u5377\u79ef\u5c42\uff0c\u5377\u79ef\u6838\u5927\u5c0f\u5206\u522b\u4e3a [2,100], [3,100], [4,100]\n        self.convs = nn.ModuleList([nn.Conv2d(1, self.out_channel, (filter_size, input_size), bias=True)\n                                    for filter_size in self.filter_sizes])\n\n    def forward(self, word_ids, extword_ids):\n        # word_ids: sentence_num * sentence_len\n        # extword_ids: sentence_num * sentence_len\n        # batch_masks: sentence_num * sentence_len\n        sen_num, sent_len = word_ids.shape\n        \n        # word_embed: sentence_num * sentence_len * 100\n        # \u6839\u636e index \u53d6\u51fa\u8bcd\u5411\u91cf\n        word_embed = self.word_embed(word_ids)\n        extword_embed = self.extword_embed(extword_ids)\n        batch_embed = word_embed + extword_embed\n\n        if self.training:\n            batch_embed = self.dropout(batch_embed)\n        # batch_embed: sentence_num x 1 x sentence_len x 100\n        # squeeze \u662f\u4e3a\u4e86\u6dfb\u52a0\u4e00\u4e2a channel \u7684\u7ef4\u5ea6\uff0c\u6210\u4e3a B * C * H * W\n        # \u65b9\u4fbf\u4e0b\u9762\u505a \u5377\u79ef\n        batch_embed.unsqueeze_(1)  \n\n        pooled_outputs = []\n        # \u901a\u8fc7 3 \u4e2a\u5377\u79ef\u6838\u505a 3 \u6b21\u5377\u79ef\u6838\u6c60\u5316\n        for i in range(len(self.filter_sizes)):\n            # \u901a\u8fc7\u6c60\u5316\u516c\u5f0f\u8ba1\u7b97\u6c60\u5316\u540e\u7684\u9ad8\u5ea6: o = (i-k)\/s+1\n            # \u5176\u4e2d o \u8868\u793a\u8f93\u51fa\u7684\u957f\u5ea6\n            # k \u8868\u793a\u5377\u79ef\u6838\u5927\u5c0f\n            # s \u8868\u793a\u6b65\u957f\uff0c\u8fd9\u91cc\u4e3a 1\n            filter_height = sent_len - self.filter_sizes[i] + 1\n            # conv\uff1asentence_num * out_channel * filter_height * 1\n            conv = self.convs[i](batch_embed)\n            hidden = F.relu(conv)  \n            # \u5b9a\u4e49\u6c60\u5316\u5c42\n            mp = nn.MaxPool2d((filter_height, 1))  # (filter_height, filter_width)\n            # pooled\uff1asentence_num * out_channel * 1 * 1 -> sen_num * out_channel\n            # \u4e5f\u53ef\u4ee5\u901a\u8fc7 squeeze \u6765\u5220\u9664\u65e0\u7528\u7684\u7ef4\u5ea6\n            pooled = mp(hidden).reshape(sen_num,\n                                        self.out_channel) \n            \n            pooled_outputs.append(pooled)\n        # \u62fc\u63a5 3 \u4e2a\u6c60\u5316\u540e\u7684\u5411\u91cf\n        # reps: sen_num * (3*out_channel)\n        reps = torch.cat(pooled_outputs, dim=1)  \n\n        if self.training:\n            reps = self.dropout(reps)\n\n        return reps\n","6014818a":"# build sent encoder\nsent_hidden_size = 256\nsent_num_layers = 2\n\n\nclass SentEncoder(nn.Module):\n    def __init__(self, sent_rep_size):\n        super(SentEncoder, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n\n        self.sent_lstm = nn.LSTM(\n            input_size=sent_rep_size, # \u6bcf\u4e2a\u53e5\u5b50\u7ecf\u8fc7 CNN \u540e\u5f97\u5230 300 \u7ef4\u5411\u91cf\n            hidden_size=sent_hidden_size,# \u8f93\u51fa\u7684\u7ef4\u5ea6\n            num_layers=sent_num_layers,\n            batch_first=True,\n            bidirectional=True\n        )\n\n    def forward(self, sent_reps, sent_masks):\n        # sent_reps:  b * doc_len * sent_rep_size\n        # sent_masks: b * doc_len\n        # sent_hiddens:  b * doc_len * hidden*2\n        # sent_hiddens:  batch, seq_len, num_directions * hidden_size\n        sent_hiddens, _ = self.sent_lstm(sent_reps)  \n        # \u5bf9\u5e94\u76f8\u4e58\uff0c\u7528\u5230\u5e7f\u64ad\uff0c\u662f\u4e3a\u4e86\u53ea\u4fdd\u7559\u6709\u53e5\u5b50\u7684\u4f4d\u7f6e\u7684\u6570\u503c\n        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(2)\n        \n        if self.training:\n            sent_hiddens = self.dropout(sent_hiddens)\n\n        return sent_hiddens\n","725d7d6b":"# build model\nclass Model(nn.Module):\n    def __init__(self, vocab):\n        super(Model, self).__init__()\n        self.sent_rep_size = 300 # \u7ecf\u8fc7 CNN \u540e\u5f97\u5230\u7684 300 \u7ef4\u5411\u91cf\n        self.doc_rep_size = sent_hidden_size * 2 # lstm \u6700\u540e\u8f93\u51fa\u7684\u5411\u91cf\u957f\u5ea6\n        self.all_parameters = {}\n        parameters = []\n        self.word_encoder = WordCNNEncoder(vocab)\n        \n        parameters.extend(list(filter(lambda p: p.requires_grad, self.word_encoder.parameters())))\n\n        self.sent_encoder = SentEncoder(self.sent_rep_size)\n        self.sent_attention = Attention(self.doc_rep_size)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_encoder.parameters())))\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.sent_attention.parameters())))\n        # doc_rep_size\n        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=True)\n        parameters.extend(list(filter(lambda p: p.requires_grad, self.out.parameters())))\n\n        if use_cuda:\n            self.to(device)\n\n        if len(parameters) > 0:\n            self.all_parameters[\"basic_parameters\"] = parameters\n\n        logging.info('Build model with cnn word encoder, lstm sent encoder.')\n\n        para_num = sum([np.prod(list(p.size())) for p in self.parameters()])\n        logging.info('Model param num: %.2f M.' % (para_num \/ 1e6))\n    def forward(self, batch_inputs):\n        # batch_inputs(batch_inputs1, batch_inputs2): b * doc_len * sentence_len\n        # batch_masks : b * doc_len * sentence_len\n        batch_inputs1, batch_inputs2, batch_masks = batch_inputs\n        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[0], batch_inputs1.shape[1], batch_inputs1.shape[2]\n        # batch_inputs1: sentence_num * sentence_len\n        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  \n        # batch_inputs2: sentence_num * sentence_len\n        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)\n        # batch_masks: sentence_num * sentence_len \n        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  \n        # sent_reps: sentence_num * sentence_rep_size\n        # sen_num * (3*out_channel) =  sen_num * 300\n        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2) \n        \n        \n        # sent_reps\uff1ab * doc_len * sent_rep_size\n        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  \n        # batch_masks\uff1ab * doc_len * max_sent_len\n        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  \n        # sent_masks\uff1ab * doc_len any(2) \u8868\u793a\u5728 \u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u4e0a\u5224\u65ad\n        # \u8868\u793a\u5982\u679c\u5982\u679c\u4e00\u4e2a\u53e5\u5b50\u4e2d\u6709\u8bcd true\uff0c\u90a3\u4e48\u8fd9\u4e2a\u53e5\u5b50\u5c31\u662f true\uff0c\u7528\u4e8e\u7ed9 lstm \u8fc7\u6ee4\n        sent_masks = batch_masks.bool().any(2).float()  # b x doc_len\n        # sent_hiddens: b * doc_len * num_directions * hidden_size\n        # sent_hiddens:  batch, seq_len, 2 * hidden_size\n        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  \n        \n        \n        # doc_reps: b * (2 * hidden_size)\n        # atten_scores: b * doc_len\n        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  \n        \n        # b * num_labels\n        batch_outputs = self.out(doc_reps)  \n\n        return batch_outputs\n\n\nmodel = Model(vocab)\n","b90e4c25":"# build optimizer\nlearning_rate = 2e-4\ndecay = .75\ndecay_step = 1000\n\n\nclass Optimizer:\n    def __init__(self, model_parameters):\n        self.all_params = []\n        self.optims = []\n        self.schedulers = []\n\n        for name, parameters in model_parameters.items():\n            if name.startswith(\"basic\"):\n                optim = torch.optim.Adam(parameters, lr=learning_rate)\n                self.optims.append(optim)\n\n                l = lambda step: decay ** (step \/\/ decay_step)\n                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)\n                self.schedulers.append(scheduler)\n                self.all_params.extend(parameters)\n\n            else:\n                Exception(\"no nameed parameters.\")\n\n        self.num = len(self.optims)\n\n    def step(self):\n        for optim, scheduler in zip(self.optims, self.schedulers):\n            optim.step()\n            scheduler.step()\n            optim.zero_grad()\n\n    def zero_grad(self):\n        for optim in self.optims:\n            optim.zero_grad()\n\n    def get_lr(self):\n        lrs = tuple(map(lambda x: x.get_lr()[-1], self.schedulers))\n        lr = ' %.5f' * self.num\n        res = lr % lrs\n        return res\n","e7568153":"# \n# \u4f5c\u7528\u662f\uff1a\u6839\u636e\u4e00\u7bc7\u6587\u7ae0\uff0c\u628a\u8fd9\u7bc7\u6587\u7ae0\u5206\u5272\u6210\u591a\u4e2a\u53e5\u5b50\n# text \u662f\u4e00\u4e2a\u65b0\u95fb\u7684\u6587\u7ae0\n# vocab \u662f\u8bcd\u5178\n# max_sent_len \u8868\u793a\u6bcf\u53e5\u8bdd\u7684\u957f\u5ea6\n# max_segment \u8868\u793a\u6700\u591a\u6709\u51e0\u53e5\u8bdd\n# \u6700\u540e\u8fd4\u56de\u7684 segments \u662f\u4e00\u4e2alist\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f tuple\uff1a(\u53e5\u5b50\u957f\u5ea6\uff0c\u53e5\u5b50\u672c\u8eab)\ndef sentence_split(text, vocab, max_sent_len=256, max_segment=16):\n    \n    words = text.strip().split()\n    document_len = len(words)\n    # \u5212\u5206\u53e5\u5b50\u7684\u7d22\u5f15\uff0c\u53e5\u5b50\u957f\u5ea6\u4e3a max_sent_len\n    index = list(range(0, document_len, max_sent_len))\n    index.append(document_len)\n\n    segments = []\n    for i in range(len(index) - 1):\n        # \u6839\u636e\u7d22\u5f15\u5212\u5206\u53e5\u5b50\n        segment = words[index[i]: index[i + 1]]\n        assert len(segment) > 0\n        # \u628a\u51fa\u73b0\u592a\u5c11\u7684\u8bcd\u66ff\u6362\u4e3a UNK\n        segment = [word if word in vocab._id2word else '<UNK>' for word in segment]\n        # \u6dfb\u52a0 tuple:(\u53e5\u5b50\u957f\u5ea6\uff0c\u53e5\u5b50\u672c\u8eab)\n        segments.append([len(segment), segment])\n\n    assert len(segments) > 0\n    # \u5982\u679c\u5927\u4e8e max_segment \u53e5\u8bdd\uff0c\u5219\u5c40\u6570\u51cf\u5c11\u4e00\u534a\uff0c\u8fd4\u56de\u4e00\u534a\u7684\u53e5\u5b50\n    if len(segments) > max_segment:\n        segment_ = int(max_segment \/ 2)\n        return segments[:segment_] + segments[-segment_:]\n    else:\n        # \u5426\u5219\u8fd4\u56de\u5168\u90e8\u53e5\u5b50\n        return segments\n","da73bba6":"# \u6700\u540e\u8fd4\u56de\u7684\u6570\u636e\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n# \u5176\u4e2d doc \u53c8\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\ndef get_examples(data, vocab, max_sent_len=256, max_segment=8):\n    label2id = vocab.label2id\n    examples = []\n\n    for text, label in zip(data['text'], data['label']):\n        # label\n        id = label2id(label)\n\n        # sents_words: \u662f\u4e00\u4e2alist\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5143\u7d20\u662f tuple\uff1a(\u53e5\u5b50\u957f\u5ea6\uff0c\u53e5\u5b50\u672c\u8eab)\n        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)\n        doc = []\n        for sent_len, sent_words in sents_words:\n            # \u628a word \u8f6c\u4e3a id\n            word_ids = vocab.word2id(sent_words)\n            # \u628a word \u8f6c\u4e3a ext id\n            extword_ids = vocab.extword2id(sent_words)\n            doc.append([sent_len, word_ids, extword_ids])\n        examples.append([id, len(doc), doc])\n\n    logging.info('Total %d docs.' % len(examples))\n    return examples\n","5c1694dd":"# build loader\n# data \u53c2\u6570\u5c31\u662f get_examples() \u5f97\u5230\u7684\n# data\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n# \u5176\u4e2d doc \u53c8\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\ndef batch_slice(data, batch_size):\n    batch_num = int(np.ceil(len(data) \/ float(batch_size)))\n    for i in range(batch_num):\n        # \u5982\u679c i < batch_num - 1\uff0c\u90a3\u4e48\u5927\u5c0f\u4e3a batch_size\uff0c\u5426\u5219\u5c31\u662f\u6700\u540e\u4e00\u6279\u6570\u636e\n        cur_batch_size = batch_size if i < batch_num - 1 else len(data) - batch_size * i\n        docs = [data[i * batch_size + b] for b in range(cur_batch_size)]\n\n        yield docs\n\n","4f1f8ef2":"# data \u53c2\u6570\u5c31\u662f get_examples() \u5f97\u5230\u7684\n# data\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n# \u5176\u4e2d doc \u53c8\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\ndef data_iter(data, batch_size, shuffle=True, noise=1.0):\n    \"\"\"\n    randomly permute data, then sort by source length, and partition into batches\n    ensure that the length of  sentences in each batch\n    \"\"\"\n\n    batched_data = []\n    if shuffle:\n        # \u8fd9\u91cc\u662f\u6253\u4e71\u6240\u6709\u6570\u636e\n        np.random.shuffle(data)\n        # lengths \u8868\u793a\u7684\u662f \u6bcf\u7bc7\u6587\u7ae0\u7684\u53e5\u5b50\u6570\u91cf\n        lengths = [example[1] for example in data] \n        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) for l in lengths]\n        sorted_indices = np.argsort(noisy_lengths).tolist()\n        sorted_data = [data[i] for i in sorted_indices]\n    else:\n        sorted_data = data\n    # \u628a batch \u7684\u6570\u636e\u653e\u8fdb\u4e00\u4e2a list    \n    batched_data.extend(list(batch_slice(sorted_data, batch_size)))\n\n    if shuffle:\n        # \u6253\u4e71 \u591a\u4e2a batch\n        np.random.shuffle(batched_data)\n\n    for batch in batched_data:\n        yield batch\n","6ccf1632":"# some function\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n\ndef get_score(y_ture, y_pred):\n    y_ture = np.array(y_ture)\n    y_pred = np.array(y_pred)\n    f1 = f1_score(y_ture, y_pred, average='macro') * 100\n    p = precision_score(y_ture, y_pred, average='macro') * 100\n    r = recall_score(y_ture, y_pred, average='macro') * 100\n\n    return str((reformat(p, 2), reformat(r, 2), reformat(f1, 2))), reformat(f1, 2)\n\n# \u4fdd\u7559 n \u4f4d\u5c0f\u6570\u70b9\ndef reformat(num, n):\n    return float(format(num, '0.' + str(n) + 'f'))\n","10f85420":"# build trainer\n\nimport time\nfrom sklearn.metrics import classification_report\n\nclip = 5.0\nepochs = 1\nearly_stops = 3\nlog_interval = 50\n\ntest_batch_size = 128\ntrain_batch_size = 128\n\nsave_model = '.\/cnn.bin'\nsave_test = '.\/cnn.csv'\n\nclass Trainer():\n    def __init__(self, model, vocab):\n        self.model = model\n        self.report = True\n\n        # get_examples() \u8fd4\u56de\u7684\u7ed3\u679c\u662f \u4e00\u4e2a list\n        # \u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n        # \u5176\u4e2d doc \u53c8\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\n        self.train_data = get_examples(train_data, vocab)\n        self.batch_num = int(np.ceil(len(self.train_data) \/ float(train_batch_size)))\n        self.dev_data = get_examples(dev_data, vocab)\n        self.test_data = get_examples(test_data, vocab)\n\n        # criterion\n        self.criterion = nn.CrossEntropyLoss()\n\n        # label name\n        self.target_names = vocab.target_names\n\n        # optimizer\n        self.optimizer = Optimizer(model.all_parameters)\n\n        # count\n        self.step = 0\n        self.early_stop = -1\n        self.best_train_f1, self.best_dev_f1 = 0, 0\n        self.last_epoch = epochs\n\n    def train(self):\n        logging.info('Start training...')\n        for epoch in range(1, epochs + 1):\n            train_f1 = self._train(epoch)\n\n            dev_f1 = self._eval(epoch)\n\n            if self.best_dev_f1 <= dev_f1:\n                logging.info(\n                    \"Exceed history dev = %.2f, current dev = %.2f\" % (self.best_dev_f1, dev_f1))\n                torch.save(self.model.state_dict(), save_model)\n\n                self.best_train_f1 = train_f1\n                self.best_dev_f1 = dev_f1\n                self.early_stop = 0\n            else:\n                self.early_stop += 1\n                if self.early_stop == early_stops:\n                    logging.info(\n                        \"Eearly stop in epoch %d, best train: %.2f, dev: %.2f\" % (\n                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))\n                    self.last_epoch = epoch\n                    break\n\n    def test(self):\n        self.model.load_state_dict(torch.load(save_model))\n        self._eval(self.last_epoch + 1, test=True)\n    \n    def _train(self, epoch):\n        self.optimizer.zero_grad()\n        self.model.train()\n\n        start_time = time.time()\n        epoch_start_time = time.time()\n        overall_losses = 0\n        losses = 0\n        batch_idx = 1\n        y_pred = []\n        y_true = []\n        for batch_data in data_iter(self.train_data, train_batch_size, shuffle=True):\n            torch.cuda.empty_cache()\n            # batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)\n            # \u5f62\u72b6\u90fd\u662f\uff1abatch_size * doc_len * sent_len\n            # batch_labels: batch_size\n            batch_inputs, batch_labels = self.batch2tensor(batch_data)\n            # batch_outputs\uff1ab * num_labels\n            batch_outputs = self.model(batch_inputs)\n            # criterion \u662f CrossEntropyLoss\uff0c\u771f\u5b9e\u6807\u7b7e\u7684\u5f62\u72b6\u662f\uff1aN\n            # \u9884\u6d4b\u6807\u7b7e\u7684\u5f62\u72b6\u662f\uff1a(N,C)\n            loss = self.criterion(batch_outputs, batch_labels)\n            \n            loss.backward()\n\n            loss_value = loss.detach().cpu().item()\n            losses += loss_value\n            overall_losses += loss_value\n            # \u628a\u9884\u6d4b\u503c\u8f6c\u6362\u4e3a\u4e00\u7ef4\uff0c\u65b9\u4fbf\u4e0b\u9762\u505a classification_report\uff0c\u8ba1\u7b97 f1\n            y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n            y_true.extend(batch_labels.cpu().numpy().tolist())\n            # \u68af\u5ea6\u88c1\u526a\n            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)\n            for optimizer, scheduler in zip(self.optimizer.optims, self.optimizer.schedulers):\n                optimizer.step()\n                scheduler.step()\n            self.optimizer.zero_grad()\n\n            self.step += 1\n\n            if batch_idx % log_interval == 0:\n                elapsed = time.time() - start_time\n                \n                lrs = self.optimizer.get_lr()\n                logging.info(\n                    '| epoch {:3d} | step {:3d} | batch {:3d}\/{:3d} | lr{} | loss {:.4f} | s\/batch {:.2f}'.format(\n                        epoch, self.step, batch_idx, self.batch_num, lrs,\n                        losses \/ log_interval,\n                        elapsed \/ log_interval))\n                \n                losses = 0\n                start_time = time.time()\n                \n            batch_idx += 1\n            \n        overall_losses \/= self.batch_num\n        during_time = time.time() - epoch_start_time\n\n        # reformat \u4fdd\u7559 4 \u4f4d\u6570\u5b57\n        overall_losses = reformat(overall_losses, 4)\n        score, f1 = get_score(y_true, y_pred)\n\n        logging.info(\n            '| epoch {:3d} | score {} | f1 {} | loss {:.4f} | time {:.2f}'.format(epoch, score, f1,\n                                                                                  overall_losses, during_time))\n        # \u5982\u679c\u9884\u6d4b\u548c\u771f\u5b9e\u7684\u6807\u7b7e\u90fd\u5305\u542b\u76f8\u540c\u7684\u7c7b\u522b\u6570\u76ee\uff0c\u624d\u80fd\u8c03\u7528 classification_report                                                                        \n        if set(y_true) == set(y_pred) and self.report:\n            report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n            logging.info('\\n' + report)\n\n        return f1\n\n    # \u8fd9\u91cc\u9a8c\u8bc1\u96c6\u3001\u6d4b\u8bd5\u96c6\u90fd\u4f7f\u7528\u8fd9\u4e2a\u51fd\u6570\uff0c\u901a\u8fc7 test \u6765\u533a\u5206\u4f7f\u7528\u54ea\u4e2a\u6570\u636e\u96c6\n    def _eval(self, epoch, test=False):\n        self.model.eval()\n        start_time = time.time()\n        data = self.test_data if test else self.dev_data\n        y_pred = []\n        y_true = []\n        with torch.no_grad():\n            for batch_data in data_iter(data, test_batch_size, shuffle=False):\n                torch.cuda.empty_cache()\n                            # batch_inputs: (batch_inputs1, batch_inputs2, batch_masks)\n            # \u5f62\u72b6\u90fd\u662f\uff1abatch_size * doc_len * sent_len\n            # batch_labels: batch_size                                                                  \n                batch_inputs, batch_labels = self.batch2tensor(batch_data)\n                # batch_outputs\uff1ab * num_labels                                                                  \n                batch_outputs = self.model(batch_inputs)\n                # \u628a\u9884\u6d4b\u503c\u8f6c\u6362\u4e3a\u4e00\u7ef4\uff0c\u65b9\u4fbf\u4e0b\u9762\u505a classification_report\uff0c\u8ba1\u7b97 f1                                                                  \n                y_pred.extend(torch.max(batch_outputs, dim=1)[1].cpu().numpy().tolist())\n                y_true.extend(batch_labels.cpu().numpy().tolist())\n\n            score, f1 = get_score(y_true, y_pred)\n\n            during_time = time.time() - start_time\n            \n            if test:\n                df = pd.DataFrame({'label': y_pred})\n                df.to_csv(save_test, index=False, sep=',')\n            else:\n                logging.info(\n                    '| epoch {:3d} | dev | score {} | f1 {} | time {:.2f}'.format(epoch, score, f1,\n                                                                              during_time))\n                if set(y_true) == set(y_pred) and self.report:\n                    report = classification_report(y_true, y_pred, digits=4, target_names=self.target_names)\n                    logging.info('\\n' + report)\n\n        return f1\n\n    \n    # data \u53c2\u6570\u5c31\u662f get_examples() \u5f97\u5230\u7684\uff0c\u7ecf\u8fc7\u4e86\u5206 batch\n    # batch_data\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n    # \u5176\u4e2d doc \u53c8\u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\n    def batch2tensor(self, batch_data):\n        '''\n            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]\n        '''\n        batch_size = len(batch_data)\n        doc_labels = []\n        doc_lens = []\n        doc_max_sent_len = []\n        for doc_data in batch_data:\n            # doc_data \u4ee3\u8868\u4e00\u7bc7\u65b0\u95fb\uff0c\u662f\u4e00\u4e2a tuple: (label, \u53e5\u5b50\u6570\u91cf\uff0cdoc)\n            # doc_data[0] \u662f label\n            doc_labels.append(doc_data[0])\n            # doc_data[1] \u662f \u8fd9\u7bc7\u6587\u7ae0\u7684\u53e5\u5b50\u6570\u91cf\n            doc_lens.append(doc_data[1])\n            # doc_data[2] \u662f\u4e00\u4e2a list\uff0c\u6bcf\u4e2a \u5143\u7d20\u662f\u4e00\u4e2a tuple: (\u53e5\u5b50\u957f\u5ea6\uff0cword_ids, extword_ids)\n            # \u6240\u4ee5 sent_data[0] \u8868\u793a\u6bcf\u4e2a\u53e5\u5b50\u7684\u957f\u5ea6\uff08\u5355\u8bcd\u4e2a\u6570\uff09\n            sent_lens = [sent_data[0] for sent_data in doc_data[2]]\n            # \u53d6\u51fa\u8fd9\u7bc7\u65b0\u95fb\u4e2d\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6\uff08\u5355\u8bcd\u4e2a\u6570\uff09\n            max_sent_len = max(sent_lens)\n            doc_max_sent_len.append(max_sent_len)\n        \n        # \u53d6\u51fa\u6700\u957f\u7684\u53e5\u5b50\u6570\u91cf\n        max_doc_len = max(doc_lens)\n        # \u53d6\u51fa\u8fd9\u6279 batch \u6570\u636e\u4e2d\u6700\u957f\u7684\u53e5\u5b50\u957f\u5ea6\uff08\u5355\u8bcd\u4e2a\u6570\uff09\n        max_sent_len = max(doc_max_sent_len)\n        # \u521b\u5efa \u6570\u636e\n        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)\n        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)\n        batch_labels = torch.LongTensor(doc_labels)\n\n        for b in range(batch_size):\n            for sent_idx in range(doc_lens[b]):\n                # batch_data[b][2] \u8868\u793a\u4e00\u4e2a list\uff0c\u662f\u4e00\u7bc7\u6587\u7ae0\u4e2d\u7684\u53e5\u5b50\n                sent_data = batch_data[b][2][sent_idx] #sent_data \u8868\u793a\u4e00\u4e2a\u53e5\u5b50\n                for word_idx in range(sent_data[0]): # sent_data[0] \u662f\u53e5\u5b50\u957f\u5ea6(\u5355\u8bcd\u6570\u91cf)\n                    # sent_data[1] \u8868\u793a word_ids\n                    batch_inputs1[b, sent_idx, word_idx] = sent_data[1][word_idx]\n                    # # sent_data[2] \u8868\u793a extword_ids\n                    batch_inputs2[b, sent_idx, word_idx] = sent_data[2][word_idx]\n                    # mask \u8868\u793a \u54ea\u4e2a\u4f4d\u7f6e\u662f\u6709\u8bcd\uff0c\u540e\u9762\u8ba1\u7b97 attention \u65f6\uff0c\u6ca1\u6709\u8bcd\u7684\u5730\u65b9\u4f1a\u88ab\u7f6e\u4e3a 0                                               \n                    batch_masks[b, sent_idx, word_idx] = 1\n\n        if use_cuda:\n            print(\"use gpu\")\n            batch_inputs1 = batch_inputs1.to(device)\n            batch_inputs2 = batch_inputs2.to(device)\n            batch_masks = batch_masks.to(device)\n            batch_labels = batch_labels.to(device)\n\n        return (batch_inputs1, batch_inputs2, batch_masks), batch_labels\n","1b1e56c3":"# train\ntrainer = Trainer(model, vocab)\ntrainer.train()\n","2b496446":"# test\ntrainer.test()\n","c40a54b1":"## TextCNN\nTextCNN\u5229\u7528CNN\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u6587\u672c\u7279\u5f81\u62bd\u53d6\uff0c\u4e0d\u540c\u5927\u5c0f\u7684\u5377\u79ef\u6838\u5206\u522b\u62bd\u53d6n-gram\u7279\u5f81\uff0c\u5377\u79ef\u8ba1\u7b97\u51fa\u7684\u7279\u5f81\u56fe\u7ecf\u8fc7MaxPooling\u4fdd\u7559\u6700\u5927\u7684\u7279\u5f81\u503c\uff0c\u7136\u540e\u5c06\u62fc\u63a5\u6210\u4e00\u4e2a\u5411\u91cf\u4f5c\u4e3a\u6587\u672c\u7684\u8868\u793a\u3002\n\n\u8fd9\u91cc\u6211\u4eec\u57fa\u4e8eTextCNN\u539f\u59cb\u8bba\u6587\u7684\u8bbe\u5b9a\uff0c\u5206\u522b\u91c7\u7528\u4e86100\u4e2a\u5927\u5c0f\u4e3a2,3,4\u7684\u5377\u79ef\u6838\uff0c\u6700\u540e\u5f97\u5230\u7684\u6587\u672c\u5411\u91cf\u5927\u5c0f\u4e3a100*3=300\u7ef4\u3002","b29da8c7":"\u628a\u524d 9 \u4efd\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6train_data\uff0c\u6700\u540e\u4e00\u4efd\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6dev_data\uff0c\u5e76\u8bfb\u53d6\u6d4b\u8bd5\u96c6test_data\u3002","12c31ea8":"____","2013b731":"Vocab \u7684\u4f5c\u7528\u662f\uff1a\n\n- \u521b\u5efa \u8bcd \u548c index \u5bf9\u5e94\u7684\u5b57\u5178\uff0c\u8fd9\u91cc\u5305\u62ec 2 \u4efd\u5b57\u5178\uff0c\u5206\u522b\u662f\uff1a_id2word \u548c _id2extword\u3002\n- \u5176\u4e2d _id2word \u662f\u4ece\u65b0\u95fb\u5f97\u5230\u7684\uff0c \u628a\u8bcd\u9891\u5c0f\u4e8e 5 \u7684\u8bcd\u66ff\u6362\u4e3a\u4e86 UNK\u3002\u5bf9\u5e94\u5230\u6a21\u578b\u8f93\u5165\u7684 batch_inputs1\u3002\n- _id2extword \u662f\u4ece word2vec.txt \u4e2d\u5f97\u5230\u7684\uff0c\u6709 5976 \u4e2a\u8bcd\u3002\u5bf9\u5e94\u5230\u6a21\u578b\u8f93\u5165\u7684 batch_inputs2\u3002\n- \u540e\u9762\u4f1a\u6709\u4e24\u4e2a embedding \u5c42\uff0c\u5176\u4e2d _id2word \u5bf9\u5e94\u7684 embedding \u662f\u53ef\u5b66\u4e60\u7684\uff0c_id2extword \u5bf9\u5e94\u7684 embedding \u662f\u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u7684\uff0c\u662f\u56fa\u5b9a\u7684\u3002\n- \u521b\u5efa label \u548c index \u5bf9\u5e94\u7684\u5b57\u5178\u3002\n- \u4e0a\u9762\u8fd9\u4e9b\u5b57\u5178\uff0c\u90fd\u662f\u57fa\u4e8etrain_data\u521b\u5efa\u7684\u3002\n","929b518f":"\u6570\u636e\u9996\u5148\u4f1a\u7ecf\u8fc7all_data2fold\u51fd\u6570\uff0c\u8fd9\u4e2a\u51fd\u6570\u7684\u4f5c\u7528\u662f\u628a\u539f\u59cb\u7684 DataFrame \u6570\u636e\uff0c\u8f6c\u6362\u4e3a\u4e00\u4e2alist\uff0c\u6709 10 \u4e2a\u5143\u7d20\uff0c\u8868\u793a\u4ea4\u53c9\u9a8c\u8bc1\u91cc\u7684 10 \u4efd\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f dict\uff0c\u6bcf\u4e2adict\u5305\u62ec label \u548c text\u3002\n\n\u9996\u5148\u6839\u636e label \u6765\u5212\u5206\u6570\u636e\u884c\u6240\u5728 index, \u751f\u6210 label2id\u3002\n\nlabel2id \u662f\u4e00\u4e2a dict\uff0ckey \u4e3a label\uff0cvalue \u662f\u4e00\u4e2a list\uff0c\u5b58\u50a8\u7684\u662f\u8be5\u7c7b\u5bf9\u5e94\u7684 index\u3002\n![image.png](attachment:c08d22a5-30c2-441a-9c68-376694c5331c.png)\n\u7136\u540e\u6839\u636elabel2id\uff0c\u628a\u6bcf\u4e00\u7c7b\u522b\u7684\u6570\u636e\uff0c\u5212\u5206\u5230 10 \u4efd\u6570\u636e\u4e2d\u3002\n![image-20210704115949214](https:\/\/tva1.sinaimg.cn\/large\/008i3skNgy1gs4s75uovpj317e0jgn2w.jpg)\n\u6700\u7ec8\u5f97\u5230\u7684\u6570\u636efold_data\u662f\u4e00\u4e2alist\uff0c\u6709 10 \u4e2a\u5143\u7d20\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f dict\uff0c\u5305\u62ec label \u548c text\u7684\u5217\u8868\uff1a[{labels:texts}, {labels:texts}. . .]\u3002\n\n\u6700\u540e\uff0c\u628a\u524d 9 \u4efd\u6570\u636e\u4f5c\u4e3a\u8bad\u7ec3\u96c6train_data\uff0c\u6700\u540e\u4e00\u4efd\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6dev_data\uff0c\u5e76\u8bfb\u53d6\u6d4b\u8bd5\u96c6test_data\u3002"}}