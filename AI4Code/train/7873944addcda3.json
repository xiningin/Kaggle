{"cell_type":{"8b8e50e3":"code","3a6b91b6":"code","477bcfe6":"code","7761f147":"code","0e00df56":"code","37365a34":"code","b5837c3e":"code","ae03a04b":"code","0def0995":"code","c2ed8769":"code","63fed0cd":"code","3069bbde":"code","c26a968c":"code","63b3f6ee":"code","6884b382":"code","c3dc62e9":"code","9618df08":"code","86f2df4d":"code","716970e5":"code","0f5a88d8":"code","c186eb43":"code","be7edbc6":"code","3b3a7cb9":"code","46838f74":"code","c2f12547":"code","56bd5a1e":"code","c813c68f":"code","7d9031b1":"code","c2f83f4a":"code","cc665b9e":"code","39a8a295":"code","a5d73213":"code","ee22a1ba":"code","4f9bec7d":"code","21672283":"code","753e89d6":"code","92d9703e":"code","ec26b722":"code","9e8e5ab4":"code","b097a0b9":"code","dc8850e6":"code","9db4ba29":"code","88746305":"code","34c2ebeb":"code","898ac797":"code","3ef4357a":"code","f4cde6cf":"code","8fc685b4":"code","ce56501b":"code","bca9535d":"code","21babe5e":"code","a22f90f2":"code","240b60c7":"code","98e8816d":"markdown","97e0a343":"markdown","fa99be52":"markdown","015d46d0":"markdown","273e455d":"markdown","215aced5":"markdown","e5040499":"markdown","50cddc6b":"markdown","eedd1103":"markdown","de10b938":"markdown","33aa740a":"markdown","8784e1d0":"markdown","c08a829d":"markdown","12f2349a":"markdown","8532a742":"markdown","41f8fd42":"markdown","ad9ea5cf":"markdown","15278012":"markdown","ee0e762e":"markdown","a3985948":"markdown","6f8ed9fd":"markdown","7358b7ef":"markdown","23d0d211":"markdown","cdab4b8f":"markdown","f202b6f7":"markdown","e2c7c541":"markdown","de50f16c":"markdown","cb113bee":"markdown","1073586f":"markdown","4bda0940":"markdown","bc1fabbf":"markdown","8ab2a764":"markdown","658e3814":"markdown","e0876d3d":"markdown","04329fd4":"markdown","1fea8ca9":"markdown","dce43c28":"markdown","9d922350":"markdown","d5d2aca9":"markdown"},"source":{"8b8e50e3":"#Libraries for data processing and analysis\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Library to access directory related functions\nimport os\n\n#Libraries for data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#To ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3a6b91b6":"#Checking files in the directory\nprint(os.listdir(\"..\/input\"))","477bcfe6":"data_train = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")","7761f147":"#Shape of dataset\nprint(\"Shape of train set: \",data_train.shape)\nprint(\"Shape of test set: \",data_test.shape)","0e00df56":"#Columns or features in the dataset\ndata_train.columns","37365a34":"data_train.head()","b5837c3e":"data_test.head()","ae03a04b":"data_train.describe()","0def0995":"Missing_Val = pd.concat([data_train.isnull().sum(),data_test.isnull().sum()],axis=1,keys=['Train DataSet', 'Test DataSet'])","c2ed8769":"Missing_Val[Missing_Val.sum(axis=1)>0]","63fed0cd":"#Let see which is the most frequent port of embarkation in the data.\ndata_train.groupby([\"Embarked\"]).count()[\"PassengerId\"]","3069bbde":"#Imputing Age and Embarked columns and Removing Cabin column. Will impute Age with median Age and Embarked column with S (most frequent). \ndata_train[\"Age\"] = data_train[\"Age\"].fillna(data_train[\"Age\"].median())\ndata_train[\"Embarked\"]=data_train[\"Embarked\"].fillna(\"S\")\n#Dropping Cabin column\ndata_train.drop(columns=[\"Cabin\"],axis=1,inplace=True)\ndata_train.isnull().sum()","c26a968c":"#Let's plot a bar graph of Survival by class\n\nsns.barplot(x=\"Pclass\",y=\"Survived\",data=data_train).set_title(\"Pclass vs. Survived\")\n","63b3f6ee":"#Let see if there is some relationship between people who survived and sex\n\nsns.barplot(x=\"Sex\",y=\"Survived\",data=data_train).set_title(\"Sex vs. Survived\")","6884b382":"#Let see if point of embarkation has any relation with Survival\nsns.barplot(x=\"Embarked\",y=\"Survived\",data=data_train).set_title(\"Embarked vs. Survived\")","c3dc62e9":"sns.catplot(x=\"Embarked\",y=\"Fare\",hue=\"Survived\",data=data_train,kind=\"violin\",height=5,aspect=1,legend=True,palette={0:\"r\",1:\"g\"})","9618df08":"#Let's look at a relation between, Pclass, Embarkation point, Age and Survived.\nsns.catplot(x=\"Age\", y=\"Embarked\",hue=\"Survived\", row=\"Pclass\",data=data_train,orient=\"h\", height=5, aspect=3, palette={0:\"r\",1:\"g\"},kind=\"violin\", dodge=True, cut=0, bw=.2)","86f2df4d":"#Let see if point of Sibsp has any relation with Survival\nsns.barplot(x=\"SibSp\",y=\"Survived\",data=data_train).set_title(\"Number of Siblings and Spouses vs. Survived\")","716970e5":"#Checking which age group was high in numbers\nsns.distplot(a=data_train[\"Age\"],kde=False)","0f5a88d8":"#Let's make bin of age groups and then see it's relationship with Survival\nbins = [-1,12, 17, 24, 35, 60, np.inf]\nlabels = ['Child', 'Teenager', 'Young Adult', 'Adult', 'Middle Aged', 'Senior']\ndata_train['AgeGroup'] = pd.cut(data_train[\"Age\"], bins, labels = labels)\ndata_train.head()","c186eb43":"sns.barplot(x=\"AgeGroup\",y=\"Survived\",data=data_train).set_title(\"Age vs. Survived\")","be7edbc6":"#Let's make bin of age groups and then see it's relationship with Survival\nbins = [0,50, 100, 200, np.inf]\nlabels = ['Basic Economy', 'Regular Economy', 'Luxury', 'Super Luxury']\ndata_train['Fare_Bin'] = pd.cut(data_train[\"Fare\"], bins, labels = labels)\ndata_train.head()","3b3a7cb9":"sns.barplot(x=\"Fare_Bin\",y=\"Survived\",data=data_train).set_title(\"Fare vs. Survived\")","46838f74":"data_train = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\n\n#Storing target variable in a temporary variable for later use\nResponse_Var = data_train.Survived\n\ndata_train.drop([\"Survived\"],axis=1,inplace=True)\ndata_combined = data_train.append(data_test)\ndata_combined.reset_index(inplace=True)\ndata_combined.drop([\"Ticket\",\"Cabin\"],axis=1,inplace=True)","c2f12547":"data_combined.head()","56bd5a1e":"data_combined.shape","c813c68f":"title = set()\nfor i in data_combined[\"Name\"]:\n    title.add(i.split(\",\")[1].split(\".\")[0].strip())\nprint(title)","7d9031b1":"# Let's make a dictionary to map all the titles with categories of titles\n\ntitle_dict = {\n    \"Jonkheer\" : \"Royalty\",\n    \"Dr\" : \"Officer\",\n    \"Mme\" : \"Mrs\",\n    \"Major\" : \"Officer\",\n    \"Rev\" : \"Officer\",\n    \"Mr\" : \"Mr\",\n    \"Dona\" : \"Royalty\",\n    \"Ms\" : \"Mrs\",\n    \"Mrs\" : \"Mrs\",\n    \"Mlle\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Don\" : \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"the Countess\" : \"Royalty\",\n    \"Lady\" : \"Royalty\",\n    \"Col\" : \"Officer\",\n    \"Miss\" : \"Miss\",\n    \"Capt\" : \"Officer\"\n}\n\n#Creating a new column Title\ndata_combined[\"Title\"] = data_combined[\"Name\"].map(lambda name:name.split(',')[1].split('.')[0].strip())\n\n#Mapping Title with title categories\ndata_combined[\"Title\"] = data_combined[\"Title\"].map(title_dict)\ndata_combined.head()","c2f83f4a":"#To check if there is any row where Title has not been filled correctly.\ndata_combined[\"Title\"].isnull().sum()","cc665b9e":"#Now we can drop Name column as we've extracted Title and Name column won't add any value to our model anymore.\ndata_combined.drop([\"Name\"],axis=1,inplace=True)\ndata_combined.head()","39a8a295":"#Calculating median age based on class, sex, title for training data only. Then we can use this dataframe to impute our Age column in both Train and Test data set\ntrain_grp = data_combined.iloc[:891].groupby([\"Pclass\",\"Sex\",\"Title\"])\ntrain_grp_median = train_grp.median()\ntrain_grp_median = train_grp_median.reset_index()[[\"Pclass\",\"Sex\",\"Title\",\"Age\"]]\ntrain_grp_median","a5d73213":"#Let's impute Age column in our combined data set\nfor i in range(len(data_combined[\"Age\"])):\n    if pd.isnull(data_combined[\"Age\"].iloc[i]):\n        condition = (train_grp_median[\"Sex\"]==data_combined[\"Sex\"].iloc[i]) & (train_grp_median[\"Title\"]==data_combined[\"Title\"].iloc[i]) & (train_grp_median[\"Pclass\"]==data_combined[\"Pclass\"].iloc[i])\n        data_combined[\"Age\"].iloc[i] = train_grp_median[condition][\"Age\"].iloc[0]","ee22a1ba":"data_combined[\"Fare\"].fillna(data_combined[:891][\"Fare\"].mean(), inplace=True)","4f9bec7d":"data_combined[\"Embarked\"].fillna(data_combined[:891][\"Embarked\"].mode()[0],inplace=True)","21672283":"#Let's check if there are variables with missing values\ndata_combined.isnull().sum()","753e89d6":"#Let's see which all variables we need to encode\ndata_combined.dtypes","92d9703e":"# We will use pd.get_dummies() to encode\n\nPclass_dum = pd.get_dummies(data=data_combined[\"Pclass\"],prefix=\"Pclass\",prefix_sep=\"_\")\n#Adding dummy variables into main dataset\ndata_combined = pd.concat([data_combined,Pclass_dum],axis=1)\n#Dropping the original Pclass variable since it is not required now\ndata_combined.drop(labels=\"Pclass\",axis=1,inplace=True)\ndata_combined.head()","ec26b722":"#Since Sex has only two categories so we can just convert them to 0 and 1 so no need to create dummies\ndata_combined[\"Sex\"] = data_combined[\"Sex\"].map({\"male\":1,\"female\":0})\ndata_combined.head()","9e8e5ab4":"#Creating dummies for Embarked column\nEmbarked_dum = pd.get_dummies(data=data_combined[\"Embarked\"],prefix=\"Embarked\",prefix_sep=\"_\")\ndata_combined = pd.concat([data_combined,Embarked_dum],axis=1)\ndata_combined.drop(labels=\"Embarked\",axis=1,inplace=True)\ndata_combined.head()","b097a0b9":"#Creating dummies for title column\nTitle_dum = pd.get_dummies(data=data_combined[\"Title\"],prefix=\"Title\",prefix_sep=\"_\")\ndata_combined = pd.concat([data_combined,Title_dum],axis=1)\ndata_combined.drop(labels=\"Title\",axis=1,inplace=True)\ndata_combined.head()","dc8850e6":"data_combined.reset_index()\ndata_combined.drop(labels=\"index\",axis=1,inplace=True)","9db4ba29":"data_combined.head()","88746305":"# Feature Importance\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesClassifier\ntrain = data_combined[:891]\ntest = data_combined[891:]\n","34c2ebeb":"# fit an Extra Trees model to the data\nFeature_Imp_model = ExtraTreesClassifier()","898ac797":"#We had already created a variable called Response_Var as target \nFeature_Imp_model.fit(train, Response_Var)\n# display the relative importance of each attribute\nprint(Feature_Imp_model.feature_importances_)","3ef4357a":"Importance_Df = pd.DataFrame()\nImportance_Df[\"Variables\"] = train.columns\nImportance_Df[\"Importance\"] = Feature_Imp_model.feature_importances_\nImportance_Df.sort_values(by=['Importance'],ascending=True,inplace=True)\nImportance_Df.set_index(\"Variables\", inplace=True)","f4cde6cf":"Importance_Df.plot(kind=\"bar\")","8fc685b4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","ce56501b":"predictors = train.drop([\"PassengerId\"],axis=1)\ntarget = Response_Var\nx_train,x_validation,y_train,y_validation = train_test_split(predictors,target,test_size=0.25,random_state=123)","bca9535d":"Logistic_Regression = LogisticRegression()\nK_Nearest_Neighbor = KNeighborsClassifier()\nDecision_Tree = DecisionTreeClassifier()\nSupport_Vector = SVC()\nRandom_Forest_Classifier = RandomForestClassifier()\nGradient_Boosting_Classifier = GradientBoostingClassifier()\nmodel_lst = [Logistic_Regression,K_Nearest_Neighbor,Decision_Tree,Support_Vector,Random_Forest_Classifier,Gradient_Boosting_Classifier]\n    ","21babe5e":"#Running models and validating\nfor model in model_lst:\n    model.fit(x_train,y_train)\n    y_predicted = model.predict(x_validation)\n    print(\"Accuracy of {0} model is {1}\".format(str(model.__class__).split(\".\")[3].split(\"'\")[0],accuracy_score(y_validation,y_predicted)))","a22f90f2":"PassengerId = test[\"PassengerId\"]\nprediction = Gradient_Boosting_Classifier.predict(test.drop(\"PassengerId\",axis=1))","240b60c7":"output_df = pd.DataFrame({\"PassengerId\" : PassengerId, \"Survived\" : prediction})\noutput_df.to_csv(\"Submission.csv\",index=False)","98e8816d":"## Maiden Data Science Voyage on Kaggle with Titanic","97e0a343":"## 5. Submitting Result","fa99be52":"#### Categorical Encoding\n\nNow it's time to encode categorical variables so that they can be used in modelling.","015d46d0":"* Nobody or close to negligible passengers in first and second class embarked from Queenstown. The majority of people who embarked from Queenstown were third class passengers\n* The people who embarked on Titanic from Queenstown and survived were within age group of 15-30\n* Out of all the kids aboard most of them were in either second or third class\n* We can also see that most of the kids and teens (age group 0 - 18) who perished were in third class\n* Most of the senior citizens aboard were in first class","273e455d":"Here is the Data Dictionary or definition of variables.\n\n<ul>\n    <li><b>PassengerId<\/b> : It is an unique id for each passenger on the ship<\/li>\n    <li><b>Survived<\/b> : This is the target feature or dependent variable which we have to predict. For this variable in the train set the value 1 indicates the passenger survived. This variable will not be available in the test set because there we'll predict it<\/i>\n    <li><b>Pclass<\/b> : This variable describes the class of the passengers. Values 1,2 and 3 indicate First, Second and Third class respectively<\/i>\n    <li><b>Name<\/b> : Name of the passenger<\/li>\n    <li><b>Sex<\/b> : Sex of the passenger <\/li>\n    <li><b>Age<\/b> : Age of the passenger <\/li>\n    <li><b>SibSp<\/b> : Number of siblings and spouses traveling with the passenger<\/li>\n    <li><b>Parch<\/b> : Number of parents and children traveling with the passenger<\/li>\n    <li><b>Ticket<\/b> : The ticket number<\/li>\n    <li><b>Fare<\/b> : Ticket Fare<\/li>\n    <li><b>Cabin<\/b> : Cabin number of the passenger<\/li>\n    <li><b>Embarked<\/b> : Port of Embarkation of the passenger. The three possible values are C = Cherbourg, Q = Queenstown, S = Southampton<\/li>\n<\/ul>","215aced5":"## 4. Modelling","e5040499":"Let us extract title from Name column which we will use later for our analysis. Title is nothing but the salutation or social status salutation used for a particular person e.g. Mr., Mrs., Dr. etc","50cddc6b":"Clearly the female percentage of survived is significantly high.","eedd1103":"<b>Feature Importance and Selection<\/b>","de10b938":"<b>Encoding Title<\/b>","33aa740a":"\nThis is my first submission and I know there are areas of improvement. I will keep working on it and improving it. So any constructive feedback is welcome.","8784e1d0":"Looks like except Title_Royalty and Title_Officer, all other variables have good importance value. For now we'll keep all the variables and if our model accuracy is not good then we'll remove these two variables and again run our model.","c08a829d":"The column <b>Survived<\/b> is the target feature here which we've to predict. The value 1 in this variable states the fact that passenger survived.","12f2349a":"Let's create submission file and submit for evaluation :)","8532a742":"Let's describe the numeric variables of train set.","41f8fd42":"So we've to encode Pclass, Sex, Embarked and Title","ad9ea5cf":"The results of Fare vs. Survived graph resonates with PClass Vs. Survived. So the number of people who bought expensive tickets survived outnumbered people who bought economy tickets.","15278012":"Since Cabin column seems to have lot of missing values (close to 80%) hence will drop this column for now and if needed we'll see later if imputing it and adding it back adds some value to our model. For now we will just do some exploratory analysis on train set after imputing Age and Embarked columns.","ee0e762e":"So out of all the models GradientBoostingClassifier performed well so I will use this only for prediction of test data.","a3985948":"<b>Imputing Emarbked<\/b><br\/><br\/>We will impute the Embarked column with the most frequest port i.e. mode","6f8ed9fd":"<b>Encoding Embarked<\/b>","7358b7ef":"I am going to use different techniques and see which one performs good","23d0d211":"<b>Processing Name column and extracting Title<\/b>","cdab4b8f":"Let us visualize the data.","f202b6f7":"## 1.  Importing the Libraries and data","e2c7c541":"<b>Encoding Sex<\/b>","de50f16c":"Looks like the percentage of people aboard within age group of 25-30 was high","cb113bee":"Although I have been working in data science for almost a year now but unfortunately never worked on a Kaggle plateform. This is my first Kernel on Kaggle and like everyone else I will be starting with Titanic Survival Prediction as this is the \"Hello World\" of Kaggle plateform.\n<br\/><br\/><br\/>\nI will try to solve the problem by following below steps:\n<ol>\n    <li>Importing the libraries and data<\/li>\n    <li>Exploratory data analysis<\/li>\n    <li>Data Cleaning, Feature Selection and Feature Engineering<\/li>\n    <li>Modelling and Choosing the best Model<\/li>\n    <li>Submitting our result<\/li>\n<\/ol>","1073586f":"% of Passengers who survived and embarked on their journey from Cherbourg are very high compared to other ports.","4bda0940":"<br\/><br\/><b>Sources<\/b><br\/>Here are the sources that I referred, followed and found useful for my first competition on Kaggle.<br\/><br\/>\n[How to score 0.8134 in Titanic Kaggle Challenge](https:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html)<br\/>\n[Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner)","bc1fabbf":"As you can see Age column has 177 missing values. So we'll impute those values little later. Let see what other variables have missing values.","8ab2a764":"### Some facts about Titanic and it's sinking\n\nBefore we dive into solving the problem let's take a look at some of the facts about Titanic and it's sinking. As we all know Titanic was a british ship that sank in North Atlantic Ocean in 1912 during it's maiden voyage from Southampton to New York. It hit an iceberg around 11.40pm, 14th April 1912 and sank 2 hrs 40 mins later on 15th April 1912. Here are some other facts (maybe not necessary for solving this problem but good to know) that I stumbled upon while reading about this infamous incident before working on this dataset (Source: Wikipedia).\n<ul>\n    <li>There were estimated 2224 passengers including the crew on board, of which more than 1500 died making it modern history's deadliest marine disaster<\/li>\n    <li>Titanic had total 9 decks, upper class decks were at the top. The lower decks were for lower class passengers and crew members hence the percentage of standard class and third class passengers who perished was significantly higher than first class passengers<\/li>\n    <li>63% of passengers were perished in this accident. 39% of first class, 58% of second class and 76% of third class passengers were perished<\/li>\n    <li>There were total 107 children travelling on the Titanic of which 50 were killed. Of 50 children who were killed, 1 was from the first class and 49 children were from lower class<\/li>\n    <li>Of total 685 crew members, 76% perished<\/li>\n    <li>While loading the lifeboats, \"women and children first\" protocol was followed resulting in 80% of male passengers perished. Of total female passengers, 25% were perished<\/li>\n    <li>The ship was under occupied as it could carry total 3547 people (including crew) but there were only 2223 people aboard otherwise the tragedy could have been much worse <\/li>\n    <li>The ship received 6 warnings of icebergs before the collision<\/li>\n    <li>Titanic carried only a total of 20 lifeboats, four of which were collapsible and proved hard to launch during the sinking. Lifeboats were only enough for 1,178 people - about half the number on board, and one third of her total capacity.<\/li>\n    <li>The temperature of the sea water in the area where Titanic sank was -2\u00b0C<\/li>\n    <li>The wreck of the Titanic lays at the depth of 12600 feet<\/li>\n    <li>Titanic's owner J. P. Morgan was scheduled to travel on the maiden voyage but cancelled at the last minute.       <\/li>\n    \n<\/ul>","658e3814":"## 3. Data Cleaning, Feature Selection and Feature Engineering","e0876d3d":"Children, Teenager (maybe because of \"women and children\" first protocol) and Middle Aged survived most among all the passengers.","04329fd4":"<b>Encoding Pclass<\/b>","1fea8ca9":"<b>Imputing Fare<\/b><br\/><br\/>Let's impute missing value with the mean of training set","dce43c28":"We will again load the data and impute both train and test set wherever data is missing.","9d922350":"## 2. Exploratory Data Analysis","d5d2aca9":"The Graph shows that people with more siblings or spouses were less likely to survive (Maybe because most of them were in lower class decks). However passengers who did not have any siblings or spouse were also less likely to survive which is surprising."}}