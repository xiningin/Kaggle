{"cell_type":{"74270175":"code","992af835":"code","fc32abe9":"code","dc35112a":"code","41fb0c1f":"code","a0d03dac":"code","b1a61475":"code","7b29e39e":"code","af6ab45c":"code","c31f3d24":"code","3469a9d2":"code","84fea632":"code","c234326f":"code","21b03eec":"code","22ef3609":"code","3e23ba1f":"markdown","7a2d94fe":"markdown","e996c3ad":"markdown","72d11d3f":"markdown","44610fd9":"markdown","46be39d0":"markdown","0b88e84a":"markdown","ea4eb441":"markdown"},"source":{"74270175":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","992af835":"# import necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\nfrom keras_tuner import RandomSearch","fc32abe9":"# importing dataset\nX_train = pd.read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv\")\nX_test = pd.read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv\")","dc35112a":"# train dataset\nX_train.head()","41fb0c1f":"# test dataset\nX_test.head()","a0d03dac":"# split the dataset\ny_train = X_train[\"label\"]\nX_train = X_train.drop(columns=\"label\")\n\ny_test = X_test['label']\nX_test = X_test.drop(columns=\"label\")","b1a61475":"# convert image into numpy array as type of float 32\nX_train = np.array(X_train, dtype=\"float32\")   \nX_test = np.array(X_test, dtype=\"float32\")\n\n# preprocessing image by dividing into 255.0 value\nX_train = X_train\/255.0      \nX_test = X_test\/255.0\n\n# reshape the train & test dataset\nX_train = X_train.reshape(len(X_train), 28, 28, 1)\nX_test = X_test.reshape(len(X_test), 28, 28, 1)","7b29e39e":"# create a function for Model building\ndef create_model(hp):\n  # initializing the model\n    model = Sequential(\n      # Creating 1st Conv2D layer\n      [Conv2D(filters=hp.Int(\"conv_1_filter\", min_value=16, max_value=128, step=16),\n              kernel_size=hp.Choice(\"conv_1_kernel\", values=[3,5]),\n              activation='relu',\n              kernel_initializer='he_normal',\n              input_shape=(28, 28, 1)),\n      \n      # Creating 2nd Conv2D layer\n      Conv2D(filters=hp.Int(\"conv_2_filter\", min_value=16, max_value=128, step=16),\n             kernel_initializer='he_normal',\n             kernel_size=hp.Choice(\"conv_2_kernel\", values=[3,5]),\n             activation='relu',\n             input_shape=(28, 28, 1)),\n\n      # Adding maxpooling layer to get max value within a matrix\n      MaxPooling2D(pool_size=(2,2)),\n\n      # Adding Flatten layer \n      Flatten(),\n\n      # Creating 1st hidden layer \n      Dense(units=hp.Int(\"layer_1_units\", min_value=32, max_value=512, step=32),\n            activation='relu',\n            kernel_initializer='he_normal'),\n      \n      # Creating output layer\n       Dense(units=10, activation='softmax')\n       ])\n  \n    # Finaly compile the model...\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n  \n    return model\n","af6ab45c":"# using RandomSearch from kera tuner library\ntuner = RandomSearch(create_model, objective='val_accuracy', max_trials=5, directory='log', project_name='CNN')\n# search the best model by fitting the dataset\ntuner.search(X_train, y_train, epochs=5, validation_split=0.1)","c31f3d24":"# creating a varible to store best model\nmodel = tuner.get_best_models(num_models=1)[0]   ","3469a9d2":"# summary of our model\nmodel.summary()","84fea632":"# fitting the dataset with the tuned model and validating our model using test data\nModel = model.fit(X_train, y_train, epochs=10, validation_split=0.3, validation_data=(X_test, y_test))","c234326f":"# getting history of our model in dictionary by getting keys... \nprint(Model.history.keys())","21b03eec":"# Evaluating the model accuracy\nplt.figure(figsize=(15,7))\nplt.plot(Model.history['accuracy'])\nplt.plot(Model.history['val_accuracy'])\nplt.title(\"Model Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend(['training_accuracy', 'validation_accuracy'])\nplt.show()","22ef3609":"# Evaluating the Loss\nplt.figure(figsize=(15,7))\nplt.plot(Model.history['loss'])\nplt.plot(Model.history['val_loss'])\nplt.title(\"Model loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(['training_loss', 'validation_loss'])\nplt.show()","3e23ba1f":"## **CREATING CNN MODEL AND TUNING WITH RANDOMSEARCH:**","7a2d94fe":"### **Preprocessing:**","e996c3ad":"#### **Load the Dataset:**","72d11d3f":"### **Importing Libraries:**","44610fd9":"### **Tuning CNN Model with RandomSearch:**","46be39d0":"### **Fit the Dataset with Best Model:**","0b88e84a":"### **Model Building:**","ea4eb441":"#### **Each training and test example is assigned to one of the following labels:**\n    0 T-shirt\/top\n    1 Trouser\n    2 Pullover\n    3 Dress\n    4 Coat\n    5 Sandal\n    6 Shirt\n    7 Sneaker\n    8 Bag\n    9 Ankle boot"}}