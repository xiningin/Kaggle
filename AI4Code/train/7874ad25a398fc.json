{"cell_type":{"9f2911dd":"code","117009e8":"code","5d3027ff":"code","f9616311":"code","92a67148":"code","f602cf22":"code","17a0aa2d":"code","4de922ef":"code","f9197393":"code","2186151e":"code","9f618395":"code","efb08a62":"code","ac39c692":"code","27b2d2e3":"code","4f0c8460":"code","bf276cd8":"code","2d1e8851":"code","f761df2c":"code","b2a4866a":"code","389eeba3":"code","cf3d1cfb":"code","9d124c6d":"code","2ae3b306":"code","d6549a57":"code","dcac001d":"code","8c52f380":"code","cb1fc194":"code","69f60e0e":"code","0e62cc06":"code","c45ec85a":"code","51a94222":"code","bc114059":"code","43c6122e":"code","893525a4":"code","d3c566ba":"code","4f670cb2":"code","7cb4e4b9":"code","857fcf12":"code","d6d27860":"code","abaa281b":"code","a2f11561":"code","d779615a":"code","2c857de2":"code","0987d9b6":"code","5f97fe90":"code","a78c47cf":"code","10fce597":"code","b1049fd3":"code","800dd2e3":"code","402b70e6":"code","c8fffce2":"code","483d196e":"code","d0a9e291":"code","31fb121e":"code","2d7181ec":"code","8456149e":"markdown","6294aaad":"markdown","888750fe":"markdown","38033e8d":"markdown","45bb6325":"markdown","054e26b2":"markdown","4a10e6ad":"markdown","9a4414c3":"markdown","8a0dafe8":"markdown","cdc5520f":"markdown","b7760c0a":"markdown","5db9d068":"markdown","cf62f775":"markdown","26ae51f9":"markdown","878309fc":"markdown","2a83117f":"markdown","b3672fbf":"markdown","7f3fc74f":"markdown","21844693":"markdown","50f3c55f":"markdown","25be5352":"markdown","412a1471":"markdown","eeee69f2":"markdown","66bb0453":"markdown","2f4cb04e":"markdown","2c2de31a":"markdown","41509d60":"markdown","2350af09":"markdown","99d151f2":"markdown","2ab4758a":"markdown","d3dfae35":"markdown"},"source":{"9f2911dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","117009e8":"data = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')\ntest = pd.read_csv('..\/input\/used-cars-price-prediction\/test-data.csv')","5d3027ff":"import matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f9616311":"from lightgbm import LGBMClassifier\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.datasets import make_classification\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler, Normalizer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBClassifier\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor,AdaBoostRegressor,BaggingRegressor, RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb","92a67148":"data.isnull().sum()","f602cf22":"test.isnull().sum()","17a0aa2d":"data = data.drop('New_Price', axis=1)\ndata = data.drop('Unnamed: 0', axis=1)\n\ntest = test.drop('New_Price', axis=1)\ntest = test.drop('Unnamed: 0', axis=1)","4de922ef":"data = data.dropna(how='any')\ntest = test.dropna(how='any')","f9197393":"data.shape","2186151e":"test.shape","9f618395":"listtrain = data['Name']\nlisttest = test['Name']\n  \n# prints the missing in listrain \nprint(\"Missing values in first list:\", (set(listtest).difference(listtrain))) ","efb08a62":"data['Cars'] = data['Name'].str.split(\" \").str[0] + ' ' +data['Name'].str.split(\" \").str[1]\ntest['Cars'] = test['Name'].str.split(\" \").str[0] + ' ' +test['Name'].str.split(\" \").str[1]","ac39c692":"set(test['Cars']).issubset(set(data['Cars']))","27b2d2e3":"listtrain = data['Cars']\nlisttest = test['Cars']\n  \n# prints the missing and additional elements in list1 \nprint(\"Missing values in first list:\", (set(listtest).difference(listtrain))) ","4f0c8460":"test.drop(test[test['Cars'].isin(['Toyota Land', 'Hindustan Motors', 'Fiat Abarth', 'Nissan 370Z', \n                                  'Isuzu MU', 'Bentley Flying', 'OpelCorsa 1.4Gsi'])].index, inplace = True)","bf276cd8":"test.shape","2d1e8851":"listtrain = data['Cars']\nlisttest = test['Cars']\n  \n# prints the missing and additional elements in list1 \nprint(\"Missing values in first list:\", (set(listtest).difference(listtrain))) ","f761df2c":"data.head(3)","b2a4866a":"data['Mileage'] = data['Mileage'].str.replace(' kmpl','')\ndata['Mileage'] = data['Mileage'].str.replace(' km\/kg','')\ndata['Engine'] = data['Engine'].str.replace(' CC','')\ndata['Power'] = data['Power'].str.replace('null bhp','112')\ndata['Power'] = data['Power'].str.replace(' bhp','')\n\ntest['Mileage'] = test['Mileage'].str.replace(' kmpl','')\ntest['Mileage'] = test['Mileage'].str.replace(' km\/kg','')\ntest['Engine'] = test['Engine'].str.replace(' CC','')\ntest['Power'] = test['Power'].str.replace('null bhp','112')\ntest['Power'] = test['Power'].str.replace(' bhp','')","389eeba3":"data.isnull().sum()","cf3d1cfb":"test.isnull().sum()","9d124c6d":"data.dtypes","2ae3b306":"data['Mileage'] = data['Mileage'].astype(float)\ndata['Mileage'] = data['Mileage'].astype(float)\ndata['Engine'] = data['Engine'].astype(float)\ndata['Power'] = data['Power'].astype(float)\n\ntest['Mileage'] = test['Mileage'].astype(float)\ntest['Mileage'] = test['Mileage'].astype(float)\ntest['Engine'] = test['Engine'].astype(float)\ntest['Power'] = test['Power'].astype(float)","d6549a57":"data.describe()","dcac001d":"feature = ['Cars', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', \n           'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats','Price']\ndata = pd.DataFrame(data, columns=feature)\n\nfeature1 = ['Cars', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', \n            'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats']\ntest = pd.DataFrame(test, columns=feature1)","8c52f380":"data.head(3)","cb1fc194":"sns.distplot(data['Price'])\n\nprint(\"Skewness: %f\" % data['Price'].skew())\nprint(\"Kurtosis: %f\" % data['Price'].kurt())","69f60e0e":"#applying log transformation\ndata['Price'] = np.log(data['Price'])\n#transformed histogram and normal probability plot\n#sns.distplot(data['Price']);\nsns.distplot(data['Price'], fit=None);\nfig = plt.figure()\nres = stats.probplot(data['Price'], plot=plt)","0e62cc06":"# Find most important features relative to target Price\nprint(\"Find most important features relative to Price-target\")\ncorr = data.corr()\ncorr.sort_values([\"Price\"], ascending = False, inplace = True)\nprint(corr.Price)","c45ec85a":"px.treemap(data.groupby(by='Fuel_Type').sum().reset_index(), path=['Fuel_Type'], labels='Fuel_Type', \n           values='Price', title='Price vs Fuel_type')","51a94222":"yprop = 'Price'\nxprop = 'Power'\nh= 'Fuel_Type'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","bc114059":"yprop = 'Price'\nxprop = 'Engine'\nh= 'Transmission'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")","43c6122e":"plt.figure(figsize=(15,10))\nxprop = 'Year'\nyprop = 'Price'\nsns.boxplot(data=data, x=xprop, y=yprop, hue='Transmission')\nplt.xlabel('{} range'.format(xprop), size=14)\nplt.ylabel('Number of {}'.format(yprop), size=14)\nplt.title('Boxplot of {}'.format(yprop), size=20)\nplt.show()","893525a4":"yprop = 'Price'\nxprop = 'Year'\nh= 'Owner_Type'\npx.scatter(data, x=xprop, y=yprop, color=h, marginal_y=\"violin\", marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n#fig.update_layout(xaxis_range=[0,5e5])","d3c566ba":"plt.figure(figsize=(15,10))\nxprop = 'Year'\nyprop = 'Price'\nsns.boxplot(data=data, x=xprop, y=yprop, hue='Fuel_Type')\nplt.xlabel('{} range'.format(xprop), size=14)\nplt.ylabel('Number of {}'.format(yprop), size=14)\nplt.title('Boxplot of {}'.format(yprop), size=20)\nplt.show()","4f670cb2":"fig = px.box(data, x='Fuel_Type',y='Price', color='Transmission', notched=True)\nfig.update_layout(legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.02,xanchor=\"right\",x=1))\nfig.show()","7cb4e4b9":"px.violin(data, y='Price', x='Seats', color=None, box=True, points=\"all\", hover_data=data.columns)","857fcf12":"import plotly.graph_objects as go\nfig = go.Figure(data=[go.Pie(labels=data['Fuel_Type'], values=data['Price'], hole=.3)])\nfig.update_layout(legend=dict(orientation=\"h\", yanchor=\"bottom\",y=1.02,xanchor=\"right\",x=1))\nfig.show()","d6d27860":"import copy\ndf_train=copy.deepcopy(data)\ndf_test=copy.deepcopy(test)\n\ncols=np.array(data.columns[data.dtypes != object])\nfor i in df_train.columns:\n    if i not in cols:\n        df_train[i]=df_train[i].map(str)\n        df_test[i]=df_test[i].map(str)\ndf_train.drop(columns=cols,inplace=True)\ndf_test.drop(columns=np.delete(cols,len(cols)-1),inplace=True)","abaa281b":"from sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\n\n# build dictionary function\ncols=np.array(data.columns[data.dtypes != object])\nd = defaultdict(LabelEncoder)\n\n# only for categorical columns apply dictionary by calling fit_transform \ndf_train = df_train.apply(lambda x: d[x.name].fit_transform(x))\ndf_test = df_test.apply(lambda x: d[x.name].transform(x))\ndf_train[cols] = data[cols]\ndf_test[np.delete(cols,len(cols)-1)]=test[np.delete(cols,len(cols)-1)]","a2f11561":"df_test.head(2)","d779615a":"df_train.head(2)","2c857de2":"ftrain = ['Cars', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', \n          'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats','Price']\n\ndef Definedata():\n    # define dataset\n    data2 = df_train[ftrain]\n    X = data2.drop(columns=['Price']).values\n    y0 = data2['Price'].values\n    lab_enc = preprocessing.LabelEncoder()\n    y = lab_enc.fit_transform(y0)\n    return X, y","0987d9b6":"def Models(models):\n    \n    model = models\n    X, y = Definedata()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_total = model.predict(X)\n    \n    print(\"\\t\\tError Table\")\n    print('Mean Absolute Error      : ', metrics.mean_absolute_error(y_test, y_pred))\n    print('Mean Squared  Error      : ', metrics.mean_squared_error(y_test, y_pred))\n    print('Root Mean Squared  Error : ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n    print('Accuracy on Traing set   : ', model.score(X_train,y_train))\n    print('Accuracy on Testing set  : ', model.score(X_test,y_test))\n    return y_total, y\n\ndef Featureimportances(models):\n    model = models\n    model.fit(X_train,y_train)\n    importances = model.feature_importances_\n    features = df_test.columns[:9]\n    imp = pd.DataFrame({'Features': ftest, 'Importance': importances})\n    imp['Sum Importance'] = imp['Importance'].cumsum()\n    imp = imp.sort_values(by = 'Importance')\n    return imp\n\ndef Graph_prediction(n, y_actual, y_predicted):\n    y = y_actual\n    y_total = y_predicted\n    number = n\n    aa=[x for x in range(number)]\n    plt.figure(figsize=(25,10)) \n    plt.plot(aa, y[:number], marker='.', label=\"actual\")\n    plt.plot(aa, y_total[:number], 'b', label=\"prediction\")\n    plt.xlabel('Price prediction of first {} used cars'.format(number), size=15)\n    plt.legend(fontsize=15)\n    plt.show()","5f97fe90":"style.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (12,7))\n## Plotting heatmap. # Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df_train.corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, );\nplt.title(\"Heatmap of all the Features of Train data set\", fontsize = 25);","a78c47cf":"Acc = pd.DataFrame(index=None, columns=['model','Root Mean Squared  Error','Accuracy on Traing set','Accuracy on Testing set'])","10fce597":"X, y = Definedata()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 25)\n    \nregressors = [['DecisionTreeRegressor',DecisionTreeRegressor()],\n              ['XGBRegressor', XGBRegressor()],\n              ['RandomForestRegressor', RandomForestRegressor()],\n              ['MLPRegressor',MLPRegressor()],\n              ['AdaBoostRegressor',AdaBoostRegressor()],\n              ['ExtraTreesRegressor',ExtraTreesRegressor()]]\n\nfor mod in regressors:\n    name = mod[0]\n    model = mod[1]\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    \n    RMSE = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n    ATrS =  model.score(X_train,y_train)\n    ATeS = model.score(X_test,y_test)\n    \n    Acc = Acc.append(pd.Series({'model':name, 'Root Mean Squared  Error': RMSE,'Accuracy on Traing set':ATrS,'Accuracy on Testing set':ATeS}),ignore_index=True )","b1049fd3":"Acc.sort_values(by='Accuracy on Testing set')","800dd2e3":"y_predicted, y_actual = Models(RandomForestRegressor(n_estimators=10000,min_samples_split=2,min_samples_leaf=1,max_features='sqrt',max_depth=25))\nGraph_prediction(150, y_actual, y_predicted)","402b70e6":"y_predicted, y_actual = Models(GradientBoostingRegressor(random_state=21, n_estimators=3000))\nGraph_prediction(150, y_actual, y_predicted)","c8fffce2":"y_predicted, y_actual = Models(CatBoostRegressor(iterations= 10000, learning_rate= 0.02, random_state= 60))","483d196e":"Graph_prediction(150, y_actual, y_predicted)","d0a9e291":"compare = pd.DataFrame({'Prediction': y_predicted, 'Test Data' : y_actual, 'Abs error': abs(y_actual - y_predicted), 'AAD%': abs(y_actual - y_predicted)\/y_actual*100})\ncompare.head(10)","31fb121e":"model = GradientBoostingRegressor(random_state=21, n_estimators=5000)\nfeature1 = ['Cars', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', \n            'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats']\n\nX0 = df_test[feature1]\nX, y = Definedata()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 25)\nmodel.fit(X_train,y_train)\ny_predicted = model.predict(X0)\n\nsubmission = pd.DataFrame({'Car_id':test.index,'Price':y_predicted}) \nsubmission.head(10)","2d7181ec":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'submission.csv'\n\nsubmission.to_csv(filename,index=True)\n\nprint('Saved file: ' + filename)","8456149e":"# I. INTRODUCTION\n\nThe prices of new cars in the industry is fixed by the manufacturer with some additional costs incurred by the Government in the form of taxes. So customers buying a new car can be assured of the money they invest to be worthy. But due to the increased price of new cars and the incapability of customers to buy new cars due to the lack of funds, used cars sales are on a global increase. Predicting the prices of used cars is an interesting and much-needed problem to be addressed. Customers can be widely exploited by fixing unrealistic prices for the used cars and many falls into this trap. Therefore, rises an absolute necessity of a used car price prediction system to effectively determine the worthiness of the car using a variety of features. Due to the adverse pricing of cars and the nomadic nature of people in developed countries, the cars are mostly bought on a lease basis, where there is an agreement between the buyer and seller. These cars upon completion of the agreement are resold. So reselling has become an essential part of today\u2019s world. \n\nGiven the description of used cars, the prediction of used cars is not an easy task. There are a variety of features of a car like the age of the car, its make, the origin of the car (the original country of the manufacturer), its mileage (the number of mildes it has run) and its horsepower. Due to rising fuel prices, fuel economy is also of prime importance. Other factors such as the type of fuel it uses, style, braking system, the volume of its cylinders (measured in cc), acceleration, the number of doors, safety index, size, weight, height, paint color, consumer reviews, prestigious awards won by the car manufacturer.\n\nOther options such as sound system, air conditioner, power steering, cosmic wheels, GPS navigator all may influence the\nprice as well.","6294aaad":"And Libraries for ML","888750fe":"# RELATIONSHIP OF PRICE WITH OTHER PARAMETER","38033e8d":"And then, coding the categorical parameters using LabelEncoder.","45bb6325":"# III. EXPLORATORY DATA ANALYSIS\n\nAfter preprocessing the data, it is analyzed through visual exploration to gather insights about the model that\ncan be applied to the data, understand the diversity in the data and the range of every field. We use a bar chart, box\nplot, distribution graph, etc. to explore each feature varies and its relation with other features including the target\nfeature.","054e26b2":"**A. Data preparation & Model Parameters**\n\nIn this Notebook, we do not discuss in deep about the Models' parameters, we just applied the standard or refer to previous recommendations. Let's copy the database.","4a10e6ad":"# VI. FUTURE WORKS\n\nKeeping the current model as a baseline, we intend to use some advanced techniques algorithms to predict car prices as our future work. We intend to develop a fully automatic, interactive system that contains a repository of used-cars with their prices. This enables a user to know the price of a similar car using a recommendation engine, which we would work in the future.","9a4414c3":"Therefore, we decided to create a new column under the name \"Cars\" to distinguish the car make & model. We have noticed that some authors have used the coding of vehicle names by specifying each vehicle as a separate code and conducting train and prediction, which is probably not LOGICAL.","8a0dafe8":"# VII. REFERENCES\n\n[1] Strauss, Oliver Thomas, and Morgan Scott Hansen. \"Advanced data science systems and methods useful for auction pricing optimization over network.\" U.S. Patent Application No. 15\/213,941.\n\n[2] Xinyuan Zhang , Zhiye Zhang and Changtong Qiu, \u201cModel of Predicting the Price Range of Used Car\u201d, 2017\n\n[3] W.A. Awad and S.M. ELseuofi, \u201cMachine Learning Method for SpamEmail Classification\u201d, 2011\n\n[4] Durgesh K. Srivastava, Lekha Bhambhu, \u201cData Classification Method using Support Vector Machine\u201d, 2009\n\n[5] Pudaruth, Sameerchand. \"Predicting the price of used cars using machine learning techniques.\" Int. J. Inf. Comput. Technol 4.7 (2014): 753-764.\n\n[6] Noor, Kanwal, and Sadaqat Jan. \"Vehicle Price Prediction System using Machine Learning Techniques.\" International Journal of Computer Applications 167.9 (2017).\n\n[7] Kuiper, Shonda. \"Introduction to Multiple Regression: How Much Is Your Car Worth?.\" Journal of Statistics Education 16.3 (2008).","cdc5520f":"Firstly, we imported some basic Libraries.","b7760c0a":"We can observe that the distribution of prices shows a high positive skewness to the left (skew > 1). A kurtosis value of 17 is very high, meaning that there is a profusion of outliers in the dataset.","5db9d068":"Now, let's check the Price first.","cf62f775":"Firstly, take a quick look at the correlation matrix.","26ae51f9":"Now, we create different functions to calculate deviations, important features and graphical illustration.","878309fc":"# Abstract\n \n\nApproximately 40 million used vehicles are sold each year. Effective pricing strategies can help any company to efficiently sell its products in a competitive market and making profit. In the automotive sector, pricing analytics play an essential role for both companies and individuals to assess the market price of a vehicle before putting it on sale or buying it. And, the rise of used cars sales is exponentially increasing. Car sellers sometimes take advantage of this scenario by listing unrealistic prices owing to the demand.\n\nTherefore, arises a need for a model that can assign a price for a vehicle by evaluating its features taking the prices of other cars into consideration. In this Notebook, we use supervised learning methods to predict the prices of used cars. The model has been chosen after careful exploratory data analysis to determine the impact of each feature on price. \n\nSo, we propose a methodology using Machine Learning models to predict the prices of used cars given the features. The price is estimated based on the number of features as mentioned above.\n\nNotebook outline:\n\n+ Step 1, we collect the data about used cars, identify important features that reflect the price.\n+ Step 2, we preprocess and remove entries with NA values. Discard features that are not relevant for the prediction of the price.\n+ Step 3, we apply ML models on the preprocessed dataset with features as inputs and the price as output.\n\nBy applying 9 models, the GradientBoostingRegressor gives very encouraging results with 99.1% on training set and 96.2% accuracy on prediction.","2a83117f":"# IV. MODEL DESCRIPTION\n\nTo compute the price for vehicles, this platform may compute linear regression model that defines a set of input variables. However, it does not give details as what features can be used for specific type of vehicles for such prediction. We have taken important features for predicting the price of used cars using random forest models.\n\nZhang et al. [2] use Kaggle data-set to perform price prediction of a used car. The author evaluates the performance\nof several classification methods (logistic regression, SVM, decision tree, Extra Trees, AdaBoost, random forest) to assess\nthe performance. Among all these models, random forest classifier proves to perform the best for their prediction task.\n\nThis work uses eleven (11) features ('Cars', 'Location', 'Year', 'Kilometers_Driven', 'Fuel_Type', 'Transmission', 'Owner_Type', 'Mileage', 'Engine', 'Power', 'Seats') to perform the classification task after removal of irrelevant features from the dataset which gives an accuracy of 96.2% on the test data. We also use Kaggle data-set to perform prediction of used-car prices. \n\nWork by Durgesh et al. [4] gives a good introductory paper on Support Vector Machine. The authors assess the performance of several classification techniques (K-NN, RuleBased Classifiers, etc.) by performing the comparative assessment of SVM with others. This comparative study is done using several data-sets taken from the UCI Machine Learning Repository. This assessment yields that SVM gives much better classification accuracy in comparison to others.\n\nThe Author of the paper [5] predicts the price of used cars in Mauritius by using four comparable machine learning algorithms - multiple linear regression, k-nearest neighbors, naive Bayes and decision trees algorithm. The author uses historical data collected from daily newspapers in Mauritius. The application of listed learning algorithms on this data provides comparable results with not-so-good prediction accuracy. The main difference, however, between classifying price range and spam mail, is that spam email classification task is a binary one, whereas our motive is mainly one-vs-therest.","b3672fbf":"We found that converting the value of Price to Log(Price) might be a good solution to have a more normal visualization of the distribution of the Price, however, this alternative has no major or decisive effect on the results of the train and\/ or predict procedure in the next section. Therefore, in order not to complicate matters, we decided to keep the whole processed database up to this step to analyze the parameters' correlations and conduct the modeling in the following section.","7f3fc74f":"**C. Models comparison**\n\nThe model score is the coefficient of determination R2 of the prediction. In total, we have examinated 9 models to train\/predict the used cars price.","21844693":"Wow, Diesel vehicles are the majority, NOT petrole cars.","50f3c55f":"# II. DATA SET AND PREPROCESSING","25be5352":"Now, check again, and efectively, there are only 7 cars in the test data set are missing from the train data set. Fine, we drop these items.","412a1471":"For this dataset \u01b0e found one point very interesting for the Name of the vehicle. We noticed that there weren't any rules for vehicle naming, and duting the prediction step, we reconigned that the names of the vehicles in train_data and test_data are very different and not predictable. We check the following:","eeee69f2":"Again, this confirmed that the Petrol vehicle is cheaper than the Diesel.","66bb0453":"We use dataset from Kaggle for used car price prediction. The dataset contains various features that are required to predict and classify the range of prices of used cars.","2f4cb04e":"Yeah ! \"New\" cars are more expensive than \"Old\" cars, and Automatic cars are more costly.","2c2de31a":"Now, we will convert all data of columns \"Mileage\", \"Engine\", \"Power\", \"Seats\" into float.","41509d60":"Let's drop the 'New_Price' and 'Unnamed:0' columns.","2350af09":"And, drop all NaN data.","99d151f2":"It's quite BIZZA that, the THIRD OWNER' CARS are sometime more expensive than the second-hand :)","2ab4758a":"# V. CONCLUSION\n\nThis Notebook evaluates used-car price prediction using Kaggle dataset which gives the best accuracy of 96.2% for test data and\n99.1% for train-data. Being a sophisticated model, GradientBoostingRegressor gives the BEST accuracy in comparison to all prior works using these datasets.","d3dfae35":"**B. Training and Testing**\n\nWe split our dataset into training, testing data with a 70:30 split ratio. The splitting was done by picking at random which results in a balance between the training data and testing data amongst the whole dataset. This is done to avoid overfitting and enhance generalization. Finaly, we selected 11 characters in the dataset to train the model."}}