{"cell_type":{"99f0d122":"code","9e5b6cc3":"code","30e9d61f":"code","18a0a743":"code","868cdfe6":"code","2364f277":"code","1b249bd2":"code","8c3e07a3":"code","10781df7":"code","3a9374b7":"code","5d1269b8":"code","f03bd54f":"code","49fd8cca":"code","7ffab2d9":"code","bcf07182":"code","dd94a6db":"code","44d899b4":"code","c6a48940":"code","d0a0b702":"code","830cae1b":"code","2deed2fd":"code","bea78bda":"code","9c710946":"code","d2178eab":"code","056f1068":"code","28836aa2":"code","1ee32fdd":"markdown","2315d1b6":"markdown","85029f2b":"markdown","1c324f96":"markdown","4a0d12bc":"markdown","de91be2e":"markdown","ad170054":"markdown","f5df34dd":"markdown","1ba5076d":"markdown","71bbf345":"markdown","93aaa97d":"markdown","18855a6c":"markdown","9dd6e5d9":"markdown","f520efb9":"markdown"},"source":{"99f0d122":"import pandas as pd\nimport numpy as np\nimport time\nfrom datetime import datetime\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9e5b6cc3":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\n\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"..\/input\"))","30e9d61f":"%%time\ntrain_df = load_df()\ntest_df = load_df(\"..\/input\/test.csv\")","18a0a743":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget = target.apply(lambda x: np.log1p(x))\ndel train_df['totals.transactionRevenue']","868cdfe6":"columns_to_remove = [col for col in train_df.columns if train_df[col].nunique() == 1]\nprint(\"Nb. of variables with unique value: {}\".format(len(columns_to_remove)))","2364f277":"for col in columns_to_remove:\n    if set(['not available in demo dataset']) ==  set(train_df[col].unique()): continue\n    print(col, train_df[col].dtypes, train_df[col].unique())","1b249bd2":"train_df['totals.bounces'] = train_df['totals.bounces'].fillna('0')\ntest_df['totals.bounces'] = test_df['totals.bounces'].fillna('0')\n\ntrain_df['totals.newVisits'] = train_df['totals.newVisits'].fillna('0')\ntest_df['totals.newVisits'] = test_df['totals.newVisits'].fillna('0')\n\ntrain_df['trafficSource.adwordsClickInfo.isVideoAd'] = train_df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\ntest_df['trafficSource.adwordsClickInfo.isVideoAd'] = test_df['trafficSource.adwordsClickInfo.isVideoAd'].fillna(True)\n\ntrain_df['trafficSource.isTrueDirect'] = train_df['trafficSource.isTrueDirect'].fillna(False)\ntest_df['trafficSource.isTrueDirect'] = test_df['trafficSource.isTrueDirect'].fillna(False)","8c3e07a3":"columns = [col for col in train_df.columns if train_df[col].nunique() > 1]\n#____________________________\ntrain_df = train_df[columns]\ntest_df = test_df[columns]","10781df7":"trn_len = train_df.shape[0]\nmerged_df = pd.concat([train_df, test_df])","3a9374b7":"merged_df['diff_visitId_time'] = merged_df['visitId'] - merged_df['visitStartTime']\nmerged_df['diff_visitId_time'] = (merged_df['diff_visitId_time'] != 0).astype(int)\ndel merged_df['visitId']","5d1269b8":"del merged_df['sessionId']","f03bd54f":"format_str = '%Y%m%d' \nmerged_df['formated_date'] = merged_df['date'].apply(lambda x: datetime.strptime(str(x), format_str))\nmerged_df['month'] = merged_df['formated_date'].apply(lambda x:x.month)\nmerged_df['quarter_month'] = merged_df['formated_date'].apply(lambda x:x.day\/\/8)\nmerged_df['day'] = merged_df['formated_date'].apply(lambda x:x.day)\nmerged_df['weekday'] = merged_df['formated_date'].apply(lambda x:x.weekday())\n\ndel merged_df['date']\ndel merged_df['formated_date']","49fd8cca":"merged_df['totals.hits'] = merged_df['totals.hits'].astype(int)\nmerged_df['mean_hits_per_day'] = merged_df.groupby(['day'])['totals.hits'].transform('mean')\ndel  merged_df['day']","7ffab2d9":"merged_df['formated_visitStartTime'] = merged_df['visitStartTime'].apply(\n    lambda x: time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(x)))\nmerged_df['formated_visitStartTime'] = pd.to_datetime(merged_df['formated_visitStartTime'])\nmerged_df['visit_hour'] = merged_df['formated_visitStartTime'].apply(lambda x: x.hour)\n\ndel merged_df['visitStartTime']\ndel merged_df['formated_visitStartTime']","bcf07182":"# for col in ['totals.newVisits', 'totals.pageviews', 'totals.bounces']:\n#     merged_df[col] = merged_df[col].astype(float)","dd94a6db":"# aggs = {\n#         #'date': ['min', 'max'],\n#         'totals.hits': ['sum', 'min', 'max', 'mean', 'median'],\n#         'totals.pageviews': ['sum', 'min', 'max', 'mean'],\n#         'totals.bounces': ['sum'],\n#         'totals.newVisits': ['sum']\n#     }\n# users = merged_df.groupby('fullVisitorId').agg(aggs)","44d899b4":"for col in merged_df.columns:\n    if col in ['fullVisitorId', 'month', 'quarter_month', 'weekday', 'visit_hour', 'WoY']: continue\n    if merged_df[col].dtypes == object or merged_df[col].dtypes == bool:\n        merged_df[col], indexer = pd.factorize(merged_df[col])","c6a48940":"numerics = [col for col in merged_df.columns if 'totals.' in col]\nnumerics += ['visitNumber', 'mean_hits_per_day', 'fullVisitorId']\ncategorical_feats =  [col for col in merged_df.columns if col not in numerics]","d0a0b702":"for col in categorical_feats:\n    merged_df[col] = merged_df[col].astype(int)\n#merged_df['fullVisitorId'] = merged_df['fullVisitorId'].astype(float)","830cae1b":"train_df = merged_df[:trn_len]\ntest_df = merged_df[trn_len:]","2deed2fd":"param = {'num_leaves': 300,\n         'min_data_in_leaf': 30, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.005,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 1,\n         \"verbosity\": -1}","bea78bda":"trn_cols = [col for col in train_df.columns if col not in ['fullVisitorId']]","9c710946":"folds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nstart = time.time()\nfeatures = list(train_df[trn_cols].columns)\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][trn_cols], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(train_df.iloc[val_idx][trn_cols], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n    \n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][trn_cols], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test_df[trn_cols], num_iteration=clf.best_iteration) \/ folds.n_splits","d2178eab":"print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))","056f1068":"cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:1000].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","28836aa2":"submission = test_df[['fullVisitorId']].copy()\nsubmission.loc[:, 'PredictedLogRevenue'] = np.expm1(predictions)\ngrouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test[\"PredictedLogRevenue\"] = np.log1p(grouped_test[\"PredictedLogRevenue\"])\ngrouped_test.to_csv('submit.csv',index=False)","1ee32fdd":"## Feature Engineering","2315d1b6":"First, we load the dataset and thanks to [Juli\u00e0n Peller](https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields), this is made quite simple:","85029f2b":"The train set is split with a Kfold method and the prediction on the test set are averaged:","1c324f96":"The target we want to predict, `transactionRevenue`, is contained in one of the JSON columns, ie. the `totals` column. While loading the dataset, it was renamed as `totals.transactionRevenue`. The target only contains a few non-null values and before taking its log, we fill the NAs:","4a0d12bc":"We have a look at the most import features:","de91be2e":"However, among these variables, the `nan` values could make sense, [according to the organizers](https:\/\/www.kaggle.com\/c\/google-analytics-customer-revenue-prediction\/discussion\/65691):","ad170054":"## LGBM (RF) starter\n*aknowledgment: a quick hello at [Olivier](https:\/\/www.kaggle.com\/ogrellier) to whom I borrowed many lines of code*","f5df34dd":"## LGBM\nWe adopt some ad-hoc hyperparameters, set the objective function to regression and use a **random forest** as learning method:","1ba5076d":"## Variable selection\nSome variables have a unique value:","71bbf345":"## label encoding","93aaa97d":"Many variables only contain a single class and we remove them:","18855a6c":"We perform some feature engineering on dates:","9dd6e5d9":"We create the submission file:","f520efb9":"Before performing label encoding, we merge the test and train sets to insure we have consistent labels in the two sets:"}}