{"cell_type":{"e9fcdce4":"code","62e927f2":"code","f6b30088":"code","8e64d2a9":"code","381d515c":"code","384816ea":"markdown","d2cebcfc":"markdown","026cf16b":"markdown","6c916bc5":"markdown","f35eaa7b":"markdown","9bf3d754":"markdown"},"source":{"e9fcdce4":"import captcha_dataset_py as cds\nimport captcha_model\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras.backend as K\n\n\nIMG_DIR='\/kaggle\/input\/captcha-version-2-images\/samples'\nimage_width = 200\nimage_height = 50\nbatch_size = 64\ndownsample_factor = 4\nmax_label_len = 5\n\nobj = cds.CaptchaData( IMG_DIR,image_height,image_width, batch_size,downsample_factor)\ntrain, test = obj.split_train_test()\ntrain = obj.make_batches(train)\nnum_classes  = obj.get_vocab_size()\nmodel = captcha_model.CaptchaModel(num_classes, image_width, image_height )","62e927f2":"m = captcha_model.CTCAccuracy(max_label_len)\nmodel.compile(optimizer='adam', loss = captcha_model.loss_func, metrics= m )\nhist = model.fit(train, epochs = 40)\n","f6b30088":"X = obj.get_column( test, 0 )\nX = obj.make_batches(X)\nY = obj.get_column( test, 1 )\n\ntest = obj.make_batches(test)\nresults =  model.evaluate( test )\nprint(results) \n","8e64d2a9":"\ndef decode_predictions( pred, max_label_len ):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    encoded_labels =K.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][:,0: max_label_len]\n    decoded_labels = []\n    for encoded_label in encoded_labels:\n        i = 0\n        label = []\n        for code in encoded_label:\n            if code.numpy() not in obj.label_int_to_char:\n                label.append('?')\n            else:\n                label.append(obj.label_int_to_char[code.numpy()])\n        decoded_labels.append(label)\n    return decoded_labels, encoded_labels\n\n\npred = model.predict(X)\ndecoded_labels, encoded_labels = decode_predictions(pred,max_label_len )\n","381d515c":"def display_samples( sample_images , labels):\n    _,ax = plt.subplots(2,2, figsize=(5,3))\n    for i in range(4):\n        ax[i\/\/2, i%2].imshow(np.transpose(sample_images[i],[1,0,2]) )\n        ax[i\/\/2, i%2].axis('off')\n        ax[i\/\/2, i%2].set_title(labels[i])\n    plt.show()\n\nit = X.as_numpy_iterator()\nbatch = it.next()\nsample_images = [batch[i] for i in range(4)]\ndisplay_samples(sample_images,decoded_labels[0:4])       \n     ","384816ea":"### 2. Train the model\nCustom loss and metrics are passed to the *model.fit* function. Since our custom model is extended from Keras.model class, we can use the standard Keras *fit* function to train the model.","d2cebcfc":"### 1. Read data and create custom model","026cf16b":"### 3. Evaluate the model\nOnce the training is complete, next step is to evaluate the model performance on test set. Since our custom model is extended from Keras.model class, we can use the standard Keras functions to evaluate the model.","6c916bc5":"### 5. Display the predictions along with the captcha images","f35eaa7b":"# Connectionist Temporal Classification: A Study\n\nThis notebook is a part of a project to study Connectionist Temporal Classsification or CTC. CTC is a neural network output decoding and scoring algorithm that is used in sequence to sequence deep learning models when:  \n- The length of the output sequence is less than the length of the input sequence and\n- The order of the output w.r.t. the input is the same and \n- To align the shorter output w.r.t. the input, it makes sense to repeat the same output symbol in multiple steps.\n\nCTC is primarily used in speech-recognition and OCR decoding.\n\nThis notebook explores usage of CTC algorithm for a use-case that satisfies the above criteria by creation of custom Keras model,accuracy metrics and loss function. You can visit the [project site](https:\/\/ritupande.github.io\/ctc\/) for further details.","9bf3d754":"### 4. Use the *model.predict* function to get the predictions generated by the model"}}