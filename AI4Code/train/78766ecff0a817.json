{"cell_type":{"49e476ec":"code","cd7080a1":"code","2c0f2b75":"code","7cff9e01":"code","83514c89":"code","430c2064":"code","cb7883f6":"code","a54ea48c":"code","f29ba4b7":"code","fa63f084":"code","6adbe94f":"code","a28eb79f":"code","1aabe536":"code","c2b67d18":"code","0243bff7":"code","ef443bdd":"code","c130909b":"code","08c84437":"code","0b564bc1":"code","839de0b2":"code","7cbc427a":"code","df75ff6f":"code","92fd858b":"code","70f7fda3":"code","67517f59":"code","8d407fd1":"code","031de293":"code","09404c17":"code","b6b9c9e2":"code","cf19751e":"code","bc1146a0":"code","cc39dae2":"code","02755128":"code","9ef35077":"code","31579a28":"markdown","ff39bee4":"markdown","fb58dce4":"markdown","b5983ebd":"markdown","128b576b":"markdown","c083c47b":"markdown","d64eb4a8":"markdown","2440696f":"markdown","7e38b24e":"markdown","012a8c09":"markdown","44911fda":"markdown","8b861dd4":"markdown","7a3bf87e":"markdown","23531dc2":"markdown","c927b845":"markdown","9a83e9dc":"markdown","213a6b85":"markdown","13e383b2":"markdown","5aee6a93":"markdown","2fcba595":"markdown","b06d7c54":"markdown","5a5ee28e":"markdown","f02a7c94":"markdown","855c251d":"markdown","8c68a985":"markdown","10b9db32":"markdown","0aaa6cf1":"markdown","adcff4c5":"markdown","aa1162bf":"markdown","eb4e6c56":"markdown","e0850354":"markdown","1a48a234":"markdown","bffc9742":"markdown","998a6689":"markdown","26954249":"markdown","09b5d54b":"markdown","f9a5483e":"markdown","adf25f3d":"markdown","0cf26961":"markdown","f1d404b0":"markdown","aba718c7":"markdown","bf51ff2a":"markdown","c3e425c5":"markdown","4cafe8f4":"markdown","cac71d27":"markdown","ab797594":"markdown","f9f824d9":"markdown","9e92dc0b":"markdown","c9c168ec":"markdown","4f11f380":"markdown","d344515f":"markdown","86fcbb65":"markdown","a5e234a9":"markdown","1e2846ed":"markdown","8416fb5f":"markdown","fd302d99":"markdown","bfef1d6a":"markdown","8e3b1bb9":"markdown","37b9d790":"markdown","5ad15372":"markdown","1f5364df":"markdown"},"source":{"49e476ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cd7080a1":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c0f2b75":"# Import training data - review shape and columns\nimport pandas as pd\ntrain = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nprint(train.info())","7cff9e01":"for column in train.columns[10:]:\n    print(train[column].value_counts())","83514c89":"# get the index of start and end columns for wilderness area and soil type. we will convert these individual OHE columns into categorical variables.\nprint(train.columns.get_loc('Wilderness_Area1'))\nprint(train.columns.get_loc('Wilderness_Area4'))\nprint(train.columns.get_loc('Soil_Type1'))\nprint(train.columns.get_loc('Soil_Type40'))","430c2064":"# Reverse the one hot encoding for Wilderness Area and Soil Type to create one column each for Wilderness Areas and Soil Types which contain all all possible category values\ntrain_1 = train.copy()\ntrain_1['Wild_Area'] = train_1.iloc[:,10:14].idxmax(axis=1)\ntrain_1['Soil_Type'] = train_1.iloc[:,14:54].idxmax(axis=1)\ntrain_1.head()\n# Drop the one hot encoded columns to a get a data frame only with numeric and the categorical columns\ncol_to_drop = np.arange(10,54)\ncol_to_drop\ntrain_1.drop(train_1.columns[col_to_drop], axis = 1, inplace = True)\ntrain_1.dtypes","cb7883f6":"# Look at analysis friendly dataframe which has numeric and category variables to do some EDA\ntrain_1.head()","a54ea48c":"train.iloc[:,:10].describe()","f29ba4b7":"print(train_1.Cover_Type.value_counts())","fa63f084":"print(train_1.Wild_Area.value_counts())","6adbe94f":"sns.set(style=\"whitegrid\")\nplt.figure(figsize=(5,3))\nsns.barplot(train_1.Wild_Area.value_counts().index,y=train_1.Wild_Area.value_counts())\nplt.xticks(rotation=90)","a28eb79f":"print(train_1.Soil_Type.value_counts())","1aabe536":"sns.set(style=\"whitegrid\") # To show grid lines to match numbers.\nplt.figure(figsize=(15,5))\nsns.barplot(x=train_1.Soil_Type.value_counts().index,y=train_1.Soil_Type.value_counts())\nplt.xticks(rotation=90)","c2b67d18":"plt.figure(figsize=(20,10))\nsns.countplot('Soil_Type', data = train_1, hue = 'Cover_Type', dodge=False)\nplt.xticks(rotation=90)\nplt.legend(loc='upper right', ncol = 3, frameon=False)","0243bff7":"len(train_1.Soil_Type.value_counts())","ef443bdd":"# list of non zero soil types available in training data\navail_soil = train_1.Soil_Type.value_counts().index\nprint(avail_soil)\n\n# Get all possible Soil Types available as features in the original training data as columns\nall_soil = train.columns[14:54]\nprint(all_soil)\n# Check the missing soil types from training data by comparing all_soil and avail_soil\nmiss_soil = np.setdiff1d(all_soil,avail_soil)\nprint(miss_soil)","c130909b":"# Plot histograms and kde customizing various options for hist, kde and rug. plot in a loop using range of i\nfor i in range(0,len(train_1.columns[:10])):\n    plt.figure(figsize=(15,10))\n    plt.subplot(5,2,i+1)\n    sns.distplot(train_1.iloc[:,i],rug=True,kde_kws={'color':'r','lw':3, 'label':'KDE'}, hist_kws={'color':'g', 'linewidth':5, 'histtype':'step','label':'HIST'})\n    plt.show()","08c84437":"# Plot histograms and kde customizing various options for hist, kde and rug. plot in a loop using range of i\nplt.figure(figsize=(15,10))\nfor i in range(0,len(train_1.columns[:10])):\n    plt.subplot(5,2,i+1)\n    sns.distplot(train_1.iloc[:,i],rug=True,kde_kws={'color':'r','lw':3, 'label':'KDE'}, hist_kws={'color':'g', 'linewidth':5, 'histtype':'step','label':'HIST'})\nplt.tight_layout()\n","0b564bc1":"# Plot histograms colored by cover type and overlapping with no hist bars\nfor column in train_1.columns[:10]:\n    plt.figure(figsize=(35,15))\n    g=sns.FacetGrid(train_1,hue='Cover_Type')\n    g.map(sns.distplot,column, hist=False, label='Cover_Type')\n    plt.legend(frameon=False)\n    plt.show()","839de0b2":"# Break down histograms colored by cover type and overlapping with no hist bars. Facet by Wild Area to see the impact of different wilderness area.\nfor column in train_1.columns[:10]:\n    plt.figure(figsize=(35,15))\n    g=sns.FacetGrid(train_1,hue='Cover_Type', col='Wild_Area')\n    g.map(sns.distplot,column, hist=False, label='Cover_Type')\n    for ax in g.axes.ravel():\n        ax.legend()\n    plt.show()","7cbc427a":"# Create an ordered list for Soil Type values to sort the facet plot.\nsoil_type_order = train.columns[14:54]\nsoil_type_order","df75ff6f":"# Break down histograms colored by cover type and overlapping with no hist bars. Facet by Soil Type to see the impact of different Soil Types.\nfor column in train_1.columns[:1]:\n    plt.figure(figsize=(10,10))\n    g=sns.FacetGrid(train_1,hue='Cover_Type', col='Soil_Type', col_order=soil_type_order, col_wrap=3, height=4, aspect = 1.5)\n    g.map(sns.distplot,column, hist=False, label='Cover_Type')\n    for ax in g.axes.ravel(): \n        ax.legend(frameon=False)\n        ax.set_xlabel(column)\n        plt.ylim(0,0.02)\n    plt.show()","92fd858b":"plt.figure(figsize=(15,10))\nsns.boxplot(data=train_1, palette='Set2')\nplt.xticks(rotation=90)","70f7fda3":"plt.figure(figsize=(15,10))\nfor i,column in enumerate(train_1.columns[:10]):\n    plt.subplot(2,5,i+1)\n    #g=sns.FacetGrid(train_1)\n    #g.map(sns.boxplot,y=train_1[column])\n    sns.boxplot(y=column,data = train_1)\nplt.tight_layout()","67517f59":"plt.figure(figsize=(20,10))\nfor i,column in enumerate(train_1.columns[:10]):\n    plt.subplot(5,2,i+1)\n    sns.boxplot(train_1[column], palette = 'Set2')\n        \nplt.tight_layout() # use this to show up any hidden labels when the plots are originally drawn overlapping.","8d407fd1":"plt.figure(figsize=(20,20))\nfor i,column in enumerate(train_1.columns[:10]):\n    plt.subplot(5,2,i+1)\n    sns.boxplot(x='Cover_Type', y = column, data = train_1 )\nplt.tight_layout()","031de293":"# Create a function to calculate number and % of outliers based on 1.5 times IQR as the cut off on both sides of the IQR (viz. 75th and 25th quantiles)\ndef detect_outliers_perc(df,column, ratio = 1.5):\n    lower,upper = np.percentile(df[column],25),np.percentile(df[column],75)\n    iqr = upper-lower\n    lower_bar,upper_bar = (lower - (ratio*iqr)),(upper + (ratio*iqr))\n    outliers = (df[column] < lower_bar) | (df[column] > upper_bar)\n    print(column,':','No of Outliers:',np.sum(outliers),';','% of outliers:',(np.sum(outliers)*100\/train_1.shape[0]))\n\nfor column in train_1.columns[:10]:\n    detect_outliers_perc(train_1,column)","09404c17":"# Create a copy df from train_1 so as to use the train_1 df for all EDA without impact of outliers.\ntrain_2 = train_1.copy()\n# Discard rows containing even one outlier for any of the features on that row.\ndef remove_outliers(df,column,lowperc=25,highperc=75,ratio=1.5):\n    lower,upper = np.percentile(df[column],lowperc),np.percentile(df[column],highperc)\n    iqr = upper-lower\n    lower_bar,upper_bar = (lower - (ratio*iqr)),(upper + (ratio*iqr))\n    outliers = (df[column] < lower_bar) | (df[column] > upper_bar)\n    #print('Number of Outliers:',np.sum(outliers))\n    df = df[~outliers]\n    print('After Removing outliers for',column,'New Shape is:',df.shape, 'Outlier rows removed:',np.sum(outliers))\n    \n    return df\nprint('Original shape of Data frame is:',train_1.shape)   \nfor column in train_2.columns[:10]:\n    train_2 = remove_outliers(train_2,column, ratio = 1.5)\nprint('Final Shape of Data frame is: ',train_2.shape)","b6b9c9e2":"plt.figure(figsize=(20,10))\nfor i,column in enumerate(train_2.columns[:10]):\n    plt.subplot(5,2,i+1)\n    sns.boxplot(train_2[column], palette = 'Set2')\n        \nplt.tight_layout() # use this to show up any hidden labels when the plots are originally drawn overlapping.","cf19751e":"sns.pairplot(train_1, hue = 'Cover_Type', palette = sns.color_palette('Set1',7))","bc1146a0":"sns.scatterplot(x='Slope', y = 'Elevation', hue = 'Cover_Type', data = train_1)","cc39dae2":"plt.figure(figsize=(15,15))\nfor i,column in enumerate(train_1.columns[1:10]):\n    #plt.figure(figsize=(6,6))\n    plt.subplot(3,3,i+1)\n    sns.scatterplot(x='Elevation', y = column, hue = 'Cover_Type', data = train_1, palette = sns.color_palette('Set1',7), alpha = 0.2)\n    plt.legend(frameon=False,loc='best', bbox_to_anchor = (1,0.5))\n    plt.tight_layout()\n    ","02755128":"plt.figure(figsize=(10,6))\nsns.heatmap(train_1.corr(),annot=True, cmap=sns.color_palette(\"BrBG\", 7), center = 0) # check for seaborn divergent color palettes to use for cmap.","9ef35077":"plt.figure(figsize=(10,6))\nimport numpy as np\ncorr = train_1.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n#with sns.axes_style(\"white\"):\nsns.heatmap(corr,annot=True, cmap=sns.color_palette(\"BrBG\", 7), center = 0, mask = mask)\n","31579a28":"## 1. Histograms - Plot for whole data by numerical feature\n\nWhile matplotlib can be quite heavy and procedural, Seaborn which is built on top of matplotlib, provides easier higher level plotting functions. Though i still fancy ggplot2 for super automatic plotting capabilities. But I am going to try to explore Seaborn as much as possible to create some sensible plots.\n\nSeaborn offers distplot() with which we can draw bar histograms, Kernel Density Estimation curves, Rug plots or all of them together. To customize with colors, line thickness etc., we can pass them in the kws arg as a dictionary. \n\n**Issue Alert** : The one thing i was not able to get to work was the subplot() function was not able to get me a grid of 5 rows and 2 columns as i would have liked, to avoid scrolling. Changing the figsize did not help much. Please leave a comment if you can provide a solution.\n\n**Issue Update** : I realized that i was calling plt.show() in each iteration of the loop, so it was printing only one plot in each row. Also i was setting the figsize in each iteration in the loop. once i was able to move the plt.show() and figsize setting outside the loop, the issue got solved.","ff39bee4":"So we have all 4 Wilderness Areas showing up here. ","fb58dce4":"Again we see that for Elevation, we can see only the cover types 4,5 and 7 easily separable. Other cover types are not easily separated even with a combination of numerical feature + Wild Area.\n\nHowever one useful information we can gather from these plots is that only 3 types of Forest Cover exist in Wild Area 2 and only 4 types in Wild Area 1. \n\nEven though For Wild Area 1, 5 and 7 are easily mutually separable as we observed even with uncategorized data , they are still quite overlapping with other cover types.\n\nWild Area 3 will probably be the most challenging to separate the cover types falling in that area as it supports 6 out of 7 forest types. So we may pay attention to the rate of misclassification that happens in this area.","b5983ebd":"## Import data\nImporting data into pandas dataframes is the first step in the Data Analysis pipeline. We import the training data csv file into a data frame. we use the index_col argument inside pd.read_csv() so that we can use the 'Id' column as the index for the data frame. Otherwise this will be taken as a feature when running machine learning algorithms or we just need to perform the additional step of dropping this column in a later step.\n\nLet us review the shape of the data frame to understand how many observations and features we have in our dataset. Also let us check out the feature names and data types. we can do all of this in one step using .info() on the data frame","128b576b":"\n\nInterestingly, the count of Soil_Type values is only 38\/40. So two of our Soil_type values are not present in our data at all to use for prediction. This is because the OHE features for these two Soil_Type values had only 0 for all observations and hence got removed when we did Reverse OHE. What this means is the corresponding two OHE features are not influencing the prediction of our training set and hence can be removed for model fitting purposes. But do note that we still may need to check how these two features are represented in the test data. But given that we are fitting using training data only, we can remove them from our final dataframe to be used for model fitting.\n\nSo Which two Soil Types are missing? Lets find out!","c083c47b":"Summary info on Soil Type - We can largely expect the below mapping of Soil Types and Cover Types\n\nSoil Type 8 : 2\n\nSoil Type 9 : 2\n\nSoil Type 12 : 2\n\nSoil Type 16 : 2, (4,6)\n\nSoil type 17 : 4,5,6\n\nSoil Type 19 : 1,(2,5)\n\nSoil Type 21 : mostly 1\n\nSoil Type 25 : only 2\n\nSoil type 35 : 7\n\nSoil Type 36 : 2,7\n\nSoil Type 37 - 7","d64eb4a8":"# What to Expect from this Kernel:\n1. Import, Summarize, Transform Data\n2. Visualize Data through Histograms, Bar plots, Box plots, Pair plots, Scatter plots, Heat Maps\n3. Challenges faced on the road and technical hints on how i solved these problems\n4. Detailed narrative of the overall EDA construction process\n5. What's up next..\n","2440696f":"Just as we guessed, all the Wilderness_Area* and Soil_Type* features are binary variables aka One-Hot-Encoded variables. 0 means the feature is absent for that observation and 1 indicates presence of feature for that observation. We can thank Kaggle for providing us clean data that we can instantly feed into the machine learning algorithms, but ofcourse there is still a lot of work before we get there.\n\nWe can also see that the Cover_Type target feature has 7 classes 1-7 and we have a perfect equal class distribution in the training set. Life is going too good to be true!!!!\n\nLet us look at our friends in the numerical feature space - the first 10 features of our data frame. we can get a bird's eye view of these by checking out the summary stats in a neat little table that .describe() method provides us. we will subset only the 10 numerical features so that we can fit all the columns in our page for easier glance through.","7e38b24e":"Looking at the min values, we see that Vertical_Distance_To_Hydrology has negative values while quite a few features have 0 as min value. At a later point, we need to check if the 0 values are normal or abnormally present in the data in which case, we need to think back if these are missing data encoded as 0 or the feature value is legally recorded as 0. \n\nWell then, lets get out of our shell and start visualizing our data","012a8c09":"**Important Note:** It seems still we have few outliers, this may be because as we delete outlier rows for one column at a time, the distribution for the next column changes and hence the outliers will be determined based on new rows for that column. \n\nOne way to avoid this is to get the outlier indices for each column upfront, get the unique indices across columns and then subset the data frame with these indices in one go. For now I will stick with what i have computed.","44911fda":"Note that Wild_Type and Soil_Type will be of dtype 'object' which is the default type for string values. Also Cover_Type is still integer. Let's do a quick preview of our EDA friendly data frame.","8b861dd4":"Given that Soil Type seems to directly relate to the type of vegetation growing in a region, and given the fact that we have 40 Soil Types compared to 7 Cover types, some of the soil types may be able to separate out the classifiers, especially when some soil types support only few cover types. lets take our best feature elevation to check which soil types may be good classifiers by themselves in collaboration with Elevation.\n\nElevation : \n\nSoil Type 1 - Cover Types 3,4,6 ; Count of Cover Types: 121-139-95 ; All are overlapping. No single separable class. Very Bad classifier.\n\nSoil Type 2 - 2,3,4,5,6 ; 3-283-94-61-182 ; 2 is insignificant. 4,5 are separable. 3 and 6 running through.No single separable class.. bad classifier.\n\nSoil Type 3 - 2,3,4,6 ; 12-133-799-18 ; 2 and 6 insignificant, 3 and 4 overlap significantly. No single separable class. bad classifier.\n\nSoil Type 4 - 1,2,3,4,5,6,7 ; 5-20-462-133-129-87-7 ; 1,2 and 7 are insignificant.4 is separable from 2 and 5. 6 and 3 running through. bad classifier.\n\nSoil Type 5 - 3,4,6 ;  55-39-71 ; All are overlapping. No single separable class. Very bad classifier.\n\nSoil Type 6 - 2,3,4,6 ; 7-248-244-151 ; 2 is insignificant. all overlapping. very bad clasifier.\n\nSoil Type 8 -  2 ; only 1 obs. keep note.\n\nSoil Type 9 - 1,2 ; 1-9 ; very low obs.maybe all are 2 only. keep note.\n\nSoil Type 10 - 1,2,3,4,5,6 ; 9-81-717-17-64-1101 ; 1 is insignificant. only 4 and 5 separable. Mostly overlapping. very Bad classifier.\n\nSoil Type 11 - 1,2,3,4,5,6 ; 5-67-89-24-154-67 ; 1 is insignificant ; 4 is separable from 1,3,5. bad classifier.\n\nSoil Type 12 - 1,2 ; 24-203 ; 90% cover type 2. shows promise.\n\nSoil Type 13 - 1,2,3,5,6 ; 17-84-4-305-66 ; 3 is insignificant. mostly overlapping. very bad classifier.\n\nSoil Type 14 - 3,4,6 ; 4-128-37 ; 3 is insignificant. All overlapping. very bad classifier.\n\nSoil Type 16 - 1,2,3,4,5,6 ; 9-14-5-40-9-37; 1,3,5 are insignificant ; 2 is probably separable from 4 and 6 ; shows promise.\n\nSoil Type 17 - 1,2,3,4,5,6 ; 2-7-34-350-131-88 ; 1 and 2 are insignificant and 3 to some extent. 4,5,6 are relatively separable. good classifier.\n\nSoil Type 18 - 2,5 ; 16-44 ;  fully overlapping. bad classifier.\n\nSoil Type 19 - 1,2,5 ; 15-13-18 ; 1 is probably separable from 2 and 5. good classifier.\n\nSoil Type 20 - 1,2,5,6 ; 41-55-6-37 ; 5 is insignificant ; Fully overlapping.bad classifier.\n\nSoil Type 21 - 1,7 ; 12-4 ; mostly 1. somewhat separable. good classifier.\n\nSoil Type 22 - 1,2,7 ; 275-54-16 ; significant overlap. bad classifier.\n\nSoil Type 23 - 1,2,5,6,7 ; 376-149-157-3-72; 5 and 7 separable. 1 and 2 running through. bad classifier.\n\nSoil Type 24 - 1,2,5,6,7 ; 128-72-11-15-31; 5 and 6 are insignificant. 1 and 2 running right through.bad classifier. \n\nSoil Type 25 - 2 ; only 1 obs - keep note. \n\nSoil Type 26 - 1,2,5 ; 7-19-28 ;  All overlapping. Bad classifier.\n\nSoil Type 27 - 1,2,7 ; 7-5-3 ; 1 and 7 clearly separable but 2 running right through. bad classifier.\n\nSoil Type 28 - 2,5 ; 6-3 ; overlapping. bad classifier.\n\nSoil Type 29 - 1,2,5,7 ; 407-554-254-76 ; 5 and 7 separable. 1 and 2 running right through. bad classifier.\n\nSoil Type 30 - 1,2,5,7 ; 81-144-480-20 ; 7 is insignificant ; 1 and 2 running right through. bad classifier.\n\nSoil Type 31 - 1,2,5,6,7 ; 114-97-87-7-27 ; 6 is insignificant ; 5 is separable from 1 and 7 ; 1 and 2 running through. bad classifier.\n\nSoil Type 32 - 1,2,3,5,6,7 ; 230-255-5-100-30-70; 3 is insignificant ; 5 and 7 separable 1 nd 2 running through. bad classifier.\n\nSoil Type 33 - 1,2,5,6,7 ; 184-184-115-66-67 ; 7 separable from 6 and 5 but 1 and 2 running through. bad classifier.\n\nSoil Type 34 - 2,5,6,7 ; 12-4-2-4 ; 5 and 7 separable. okay classifier.\n\nSoil Type 35 - 1,2,7 ; 3-1-98 ; 7 is dominant class - 98\/102 obs.others are outliers? good classifier.\n\nSoil Type 36 - 2,7 ; 2-8 ; fully separable - very good classifier.\n\nSoil Type 37 - 7 ; 34 ; fully separable - very good classifier.\n\nSoil Type 38 - 1,2,7 ; 80-7-641 ; 2 is insignificant ; all overlapping - bad classifier.\n\nSoil Type 39 - 1,2,7 ; 79-3-575 ; 2 is insignificant ; fully overlapping ; bad classifier.\n\nSoil Type 40 - 1,2,7 ; 49-3-407 ; 2 is insignificant ; all overlapping - bad classifier.","7a3bf87e":"As we can see from the heat map, the dark green and dark brown indicate stronger positive or negative relationship between any feature pair. The observations we made earlier are holding good.\n\nWe can further simplify the heat map by printing only the lower or upper triangle as the correlation matrix is symmetrical along the diagonal. This way there is no duplication of correlation coefficients, giving a more cleaner look. \n\nWe do this by creating a boolean mask (True\/False values) for the upper or the lower triangle and subsetting the matrix where mask is False (since the mask argument inside heatmap() excludes True value cells in the mask. we used np.triu to exclude upper triangle, we can use np.tril if we want to exclude lower triangle.\n","23531dc2":"### Heat Map with Lower Triangle layout","c927b845":"## 1. Combine all box plots in a single axes","9a83e9dc":"### Original plot with Issue mentioned in Point 1\nNote the categories are not all individual levels but a numeric range in the integer range of the levels.","213a6b85":"# Part 2: Histograms - Univariate Analysis of features\n\nOne of the first steps in EDA is to examine the distribution of values of each variable. For numeric variables - this will be visualized by a histogram\/distribution plot. For categorical variables, this will be a count plot or bar plot.\n\nThere are 10 numerical features (first 10) for which we can plot histograms, they are the first 10 columns of the data frame. But we can analyze histograms at multiple levels in a sequential manner.\n\n1. Plot histogram for each numeric feature for the whole data. This will give insight into overall data distribution of the numeric feature.  \n2. Break down the histogram by the target variable. This will help understand if the classes are easily separated by the features. Not all classes may be separated that easily. But maybe some classes are easier to distinguish. Knowing which classes have more tendency to overlap helps to focus our attention on these classes and try to dig into features that might help to separate them.\n3. Break down the histogram, by target variable, by categorical variables. Here we can visualize the impact of each categorical variable for its ability to separate the classes. For a given categorical variable, we can see which values correspond to which classes most of the times.This may also help us to understand which categorical values are useful features since each value will be a feature when the original categorical variable is one hot encoded into binary values 0 and 1 for each categorical value.","13e383b2":"## 2. Histograms: Plot faceted histograms for numerical features coloring by cover type\n\nIn order to find if the numerical features by themselves are able to separate the classes adequately, we need to visualize multiple histograms of each feature in the same plot each colored by its Cover_Type class. If the curves are all overlapping,separation of classes will pose challenges while if some curves are separable, atleast those classes will be easier to predict based on that feature value.\n\nWe can also use this information to pay special attention to those classes which are not easily separable and think of features that might help separating those classes.\n\n**Technical Hints:** Seaborn does not offer a hue argument in distplot(). so if we want to color histograms by a variable, we may need to use the FacetGrid() function to apply hue to the Grid axes and then map the Facet Grid to the distplot(). We hide the hist bars for better understanding of overlapping curves. While adding a legend, use frameon argument to remove the outline box of the legend for cleaner look.","5aee6a93":"# Data Fields\n* Elevation - Elevation in meters\n* Aspect - Aspect in degrees azimuth\n* Slope - Slope in degrees\n* Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n* Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n* Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n* Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n* Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n\nThe wilderness areas are:\n* 1 - Rawah Wilderness Area\n* 2 - Neota Wilderness Area\n* 3 - Comanche Peak Wilderness Area\n* 4 - Cache la Poudre Wilderness Area\n\nThe soil types are:\n\n* 1 Cathedral family - Rock outcrop complex, extremely stony.\n* 2 Vanet - Ratake families complex, very stony.\n* 3 Haploborolis - Rock outcrop complex, rubbly.\n* 4 Ratake family - Rock outcrop complex, rubbly.\n* 5 Vanet family - Rock outcrop complex complex, rubbly.\n* 6 Vanet - Wetmore families - Rock outcrop complex, stony.\n* 7 Gothic family.\n* 8 Supervisor - Limber families complex.\n* 9 Troutville family, very stony.\n* 10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n* 11 Bullwark - Catamount families - Rock land complex, rubbly.\n* 12 Legault family - Rock land complex, stony.\n* 13 Catamount family - Rock land - Bullwark family complex, rubbly.\n* 14 Pachic Argiborolis - Aquolis complex.\n* 15 unspecified in the USFS Soil and ELU Survey.\n* 16 Cryaquolis - Cryoborolis complex.\n* 17 Gateview family - Cryaquolis complex.\n* 18 Rogert family, very stony.\n* 19 Typic Cryaquolis - Borohemists complex.\n* 20 Typic Cryaquepts - Typic Cryaquolls complex.\n* 21 Typic Cryaquolls - Leighcan family, till substratum complex.\n* 22 Leighcan family, till substratum, extremely bouldery.\n* 23 Leighcan family, till substratum - Typic Cryaquolls complex.\n* 24 Leighcan family, extremely stony.\n* 25 Leighcan family, warm, extremely stony.\n* 26 Granile - Catamount families complex, very stony.\n* 27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n* 28 Leighcan family - Rock outcrop complex, extremely stony.\n* 29 Como - Legault families complex, extremely stony.\n* 30 Como family - Rock land - Legault family complex, extremely stony.\n* 31 Leighcan - Catamount families complex, extremely stony.\n* 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n* 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n* 34 Cryorthents - Rock land complex, extremely stony.\n* 35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n* 36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n* 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n* 38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n* 39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n* 40 Moran family - Cryorthents - Rock land complex, extremely stony.","2fcba595":"# Part 3: Box Plots - Visualizing Quartiles and Outliers in a single plot\n\nWe already mentioned in one of the histogram sections that we observed a number of skewed distributions. We need to take a closer look at the distribution of all of the features and also  review **outliers** in each of these distributions. We first look at the overall data for each feature to get a macro view of the features","b06d7c54":"Note that we tried to just plot all features in a box plot with same scale by simply passing the df to sns.boxplot. The result is that the features which are very small in comparison with the largest are almost flattened making any visualization of their quartiles impossible. Hence we need to plot them as sub plots which will then have their own scales for meaningful visualization.","5a5ee28e":"As we can see, Only Elevation histograms show some separation of classes. It may be interesting to plot Elevation vs other features in little more detail as scatter plots. To show more clearly the quantum of overlap of classes, we will use a high degree of transparency using alpha.\n\nAlso in the pair plots, we can take a first cut view of the nature of relationships between the different numeric features - correlation measure. if there is a correlation between two features, both the features may not be needed in the model and one can represent the other, depending on the strength of the relationship.The below observations can be made:\n\n1. Slope has a negative correlation with Hillside Noon\n2. Horizontal distance to hydrology and vertical distance to hydrology are positively correlated.\n3. Hill side 3 pm has a positive correlation with Hill side Noon and negative correlation with Hill side 3 PM.\n\nThere are signs of correlation for few other pairs of features but probably not as strong. A more certain way of examining the strength of relationship between features is to compute the pearson's correlation coefficient. Thankfully this is not just a mathematical exercise, we can infact visualize it in a pair grid with a heat map. But before we get there, lets finish up scatter plotting for relationships of interest based on our observation of the Pair plot.","f02a7c94":"# Up Next: Baseline Prediction Accuracy with full feature set\nWe have so far reviewed the available data without really looking into what the mentioned features mean in a real life scenario. However there are a bunch of questions that need to be given consideration:\n\nAre some of the numerical features really numerical features? \nAre some of them inter related? with interactions?\nAre there transformations of these features used in real life?\n\nFor the Soil Type feature, should we be really looking at 40 single Soil Type features or perhaps a lesser number of logically grouped set of Soil types that are more intuitively connected to the forest cover types\n\nTo get to the bottom of this, we may need to do some quick research in google and understand each of these features. Then we can re-engineer our feature list and get to the much awaited machine learning exercises.\n\nBefore we get to feature engineering, a caveat. Given that feature engineering is an intensive process with no clear markers for success without an iterative trial and error process, before we get further with feature engineering and selection, it is useful to take all of our simple features, do a quick model fitting with a generally performing ML algorithm to get a sense of baseline accuracy levels. Then we should think about raising the bar and reach for higher through feature engineering. \n\nI will intend to do this with Ensemble Tree based algorithms like Random Forest and few others - for the simple reason that they does not require any preprocessing, are resistant to outliers and are capable of achieving low variance with a lower bias as compared to non tree models.\n\nWith this context, I will move to creating a new kernel for Accuracy Baselining with full feature set using Ensemble Tree models.\n","855c251d":"# Part 4: Outlier Detection and Removal\nOutliers are observations that are not in the expected range of values when measuring a particular variable or feature. This may be because of data entry error but also could be because of some exceptional situation, leading to such an exceptional value. \n\nHow far they are from the 'normal' range can be defined by us to call that observation as an outlier. Typically, 1.5 * Inter Quartile range above the 75% Quartile and below the 25% Quartile are considered as Outliers. This, by no means, is set in stone, and we can choose harsher limits for outlier detection so that we do not lose these observations by just removing them from the data. We have to balance between 'losing data' and 'determining inaccurate relationships' when deciding on a strategy to address outliers.\n\nThe [Anscombe's Quartet](http:\/\/en.wikipedia.org\/wiki\/Anscombe's_quartet) is a nice way to understand the impact of outliers during model fitting. Not thinking about outliers can lead to wrong conclusions on the nature of relationships in the data.","8c68a985":"# Problem Statement\n\nA study was conducted in four wilderness areas within the beautiful Roosevelt National Forest of northern Colorado. These areas represent forests with very little human disturbances \u2013 the existing forest cover types there are more a result of ecological processes rather than forest management practices.\n\nThe study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. Each observation is a 30m x 30m patch. We are asked to predict an integer classification for the forest cover type. The seven types are:\n\n* 1 - Spruce\/Fir\n* 2 - Lodgepole Pine\n* 3 - Ponderosa Pine\n* 4 - Cottonwood\/Willow\n* 5 - Aspen\n* 6 - Douglas-fir\n* 7 - Krummholz\n\nThe training set (15120 observations) contains both features and the Cover_Type. The test set contains only the features. \n\nWe must predict the Cover_Type for every row in the test set (565892 observations).","10b9db32":"## 4. Box plots of each feature by target variable - Cover Type\n\nAs a matter of curiosity, I wanted to see if outliers have a pattern with Cover Types, that is, if outliers for specific features tend to occur more often with specific cover types. To check this, I drew the box plots with cover type as x-axis and feature as y-axis. Few points of note are\n\n1. Aspect feature has outliers only for Cover Type 4.Interestingly Cover Type 4 has very less outliers for most of the features.\n2. Cover Type 3,4 and 6 have more number of features without outliers \/ less outliers. So less extra ordinary reasons or measurement errors for these cover types.\n\nFinally we can see that Elevation feature by itself is able to separate certain classes very well. This is certainly an important feature. Other features do not have that clear a separation between the individual box plots by cover type, so by itself they may not be a good classifier, but remains to be seen if a combination of features can help with a better degree of classification atleast for certain classes.","0aaa6cf1":"## Summarize Numerical features","adcff4c5":"So we have 15120 observations and 55 columns or features. Also note that there seem to be no null entries, so no missing data mess to be sorted out.\n\nNote that the last column - 'Cover_Type' - is the target variable. Hence we really have 54 features based on which to predict the target variable - Cover_Type. Interestingly 'Cover_Type' is encoded as an integer data type rather than a categorical data type though it represents a classification label. However we can keep this as is for now.","aa1162bf":"## 2. Vertical box plots as sub plots as (2,5) to compress the view of the plots\n\nThis is good, but not the best way to grasp the numbers on the plot.","eb4e6c56":"## Outlier Removal\nTo remove all rows containing outliers for all columns, we need to filter out the outliers from data frame one column at a time. So we will run a loop on all numeric columns and for each iteration, that is, for each column, check the obs value against the outlier threshold and then filter out the TRUE rows. As we do this for each column in a cumulative fashion, we will end up with a much reduced data frame after getting clear of all outliers for all columns.\n\nNote that this is like an 'OR' applied for each row, i.e., in each row if there was even one outlier, that row will be removed. In order to lose not too much data, we may keep much a larger percentile threshold for filtering outliers. example 95%.\n\nIn the below function, we have multiple options to control outlier threshold\n1. Setting bounds for lower and higher quantiles based on which IQR ratio will apply. For default box plots, this is (25,75). We can even keep it (5,95)\n2. We can change the IQR Ratio for a given lower and higher quartile range.Default is 1.5 which is what standard box plots use.","e0850354":"### Modified Plot with Issue mentioned in Point 1 - solved. Here we use color palette method, but can also use legend = Full without specifying a color palette and let seaborn pick default color palette.","1a48a234":"# Part 1: Import,Transform, Summarize Data","bffc9742":"### Histograms by Cover Type and Soil Type\n\nWe will repeat the last exercise, this time with Soil Type as a conditioning variable.\n\n**Technical Hints:** Since we have 40 Soil Types, we need to use col_wrap with a limit of 4 to make legible plots.Also since the number of plots is reaching 2 digits, the sequencing of plots will not be smooth, so we set col_order to an ordered list of Soil Types.Finally when we make facet grid plots, the x-axis label for histogram appears only in the last row. To print it for each facet, we use the same loop for ax, to set the x-axis label each time for each facet plot with ax.set_xlabel(). To adjust the size of each facet, use height and aspect arguments of facet grid.","998a6689":"So that leaves us with Soil_Type15 and Soil_Type7. These 2 will later be removed before our machine learning tasks.","26954249":"## Load in the python libraries","09b5d54b":"As can be seen from the boxplots, the outliers are shown in dots. The features with large number of outliers seem to be:  \n\n1. Horizontal distance to hydrology\n2. Vertical distance to hydrology\n3. Horizontal distance to roadways\n4. Hillshade 9 am\n5. Hillshade noon\n6. Horizontal distance to fire points\n\nWe will validate this in coming sections on Outlier Detection and Removal, by calculating the outliers, which is, number of observations outside 1.5 * IQR for each feature as the box plot above shows.\n\n","f9a5483e":"Elevation seems to be the only feature that is able to separate some of the classes very well by itself. Also it seems classes 4,5 and 7 are easily separable.\n\nThe histograms of all other numeric variables are overlapping a lot hence not effective in separating the classes by themselves.","adf25f3d":"# Part 7: Heat Maps\nThere is quite no other visualization tool like colors on a plot. As we saw earlier, color coding target class enables us to check boundaries between different classes and if they are linearly separable, which is key while selecting a machine learning algorithm.\n\nHeat maps are color coded matrix of rectangular data. The values in each cell of the matrix will be color coded based on the magnitude of values.\n\nFor visualizing correlations between features, Heat map plots take in a square matrix of correlation coefficients between pairs of features on a pair Grid (A pair grid containing same set of features on X and Y axis so that relationship between any of the pairs of features can be plotted in each cell of the grid)\n\nIf we were to just get a matrix of correlation coefficients it will be quite hard to digest the information presented as well as selectively focus on the more important relationships containing feature pairs. In a heat map, we can choose a divergent color bar where middle values are in a neutral color and color progresses towards the dark bands on either side reaching the darkest colors at each ends of the color bar. \n\nWhen looking at correlation coefficients, we are usually looking for either strong positive or strong negative relationships, so we just need to focus on the darker cells in the matrix to focus on key relationships.\n\nLets get started with a heat map for correlation coefficients.","0cf26961":"### Original code with plt.figure and plt.show inside each iteration in the loop - Plots not showing as subplots","f1d404b0":"Seems like we have an equitable distribution of target classes, so no worries about having to do upsampling or downsampling, as in the case of class imbalance.","aba718c7":"# Part 5: Pair Plots\nSo far we analyzed univariate distributions which gave us a wealth of information on how the individual features data is distributed, the extent of outliers as well as how the range of values spread over various cover types. We saw that Elevation was the probably the only feature by itself that was able to separate atleast a few of the cover types very well. Does a combination of features provide good options to be able to classify? We can answer this question by drawing scatter plots which plot two features at a time. By conditioning on Cover Type using hue, we can see if we can identify isolated patches.\n\nInstead of starting with detailed scatter plots, we can do a bird's eye view of possible scatter plots and then drill down on those that seem interesting for further analysis.This can be achieved with pair plots available in seaborn. By passing a df, we can do all combinations of scatter plots plus histograms for each of the features. Scatter plots are for numeric data primarily, but the sns.pairplot() function is smart enough to take a data frame that contains both numeric as well as categorical features and pull out the numeric data only for creating the scatter plots. \n\nBelow, we plot by Cover Type so that we can visualize separation of classes.","bf51ff2a":"Taking a cue from the pair plots, we now plot Elevation against all other features with a very low alpha of 0.2 so that we can truly see the clustering of observations by cover type, while exposing the class overlap more than if they were solid markers.\n\n**Technical hints:** \n\n1. The Cover type data type is integer. If we simple use hue parameter, we will not get different colors at each level, rather a continuous shade of colors. Also not all levels will be color coded. So we can specify a color palette with the number of colors as per number of levels for the hue variable, which now results in proper colors for each level. **Edit **- Figured that if we specify **legend = 'full'**, **the legend is plotted for all possible values**, this seems to be a more automatic and effortless method than with the manual specification inside color palette.\n2. Made use to bbox_to_anchor to place the legend box outside the plot to keep the plot clean. \n3. To give space between plots and legend without overlap, used plt.tight_layout. This is a nice plot housekeeping tip.\n\n","c3e425c5":"## Summarize Categorical Features","4cafe8f4":"# Part 6: Scatter Plots","cac71d27":"I plan to use this kernel as a learning platform to experiment with various EDA techniques while also mentioning my experience of trying to make python functions and libraries work for my needs. I am also putting my learnings on the technical implementation issues i faced on the way and how i came to solve them so that it can serve as a useful tip for people reading this kernel. I have used seaborn as the primary plotting engine while using matplotlib for annotations, setting figure attributes etc.\n\nHappy reading and share your comments that i can use to improve this kernel!!!","ab797594":"Now we understand which soil types are more prevalent in the data. Let us go one step further and check which cover types are in each soil type. For this we need to create a stacked plot by categorical variable where each bar represents a soil type and within each soil type bar, different cover types category values are stacked.","f9f824d9":"Before we get to EDA, as a last step, lets check out the count of category values or levels for our categorical features - Cover_Type (target variable) and new categorical features Wild_Type and Soil_Type. We do this with .value_counts() applied to each of the 2 columns.","9e92dc0b":"### Plots showing as subplots correctly with plt.show and plt.figure outside the loop - Subplots shown properly now","c9c168ec":"We can observe how each column impacts the outlier rows getting removed in a cumulative fashion seeing the gradual change in shape of df (only the rows). Now that we have cleaned up the outliers, we can replot the box plots and should see the outlier situation has changed.","4f11f380":"What we observed was true, that the mentioned features had maximum outliers among the overall set.The maximum % of outliers were 5.5%, given IQR ratio of 1.5, for any of the features. Note that the function to detect outliers leaves the IQR ratio flexible, so we can change this ratio to define threshold for outliers. By choosing a liberal threshold (by increasing IQR ratio) we limit the risk of losing data.\n\nWe could also fill in the outlier data with a mean or median, we should evaluate the difference between this and removing outliers altogether, to weigh the impact of each.\n\nAs a next step, we should experiment with outlier thresholds, remove observations and redraw the box plots to see how the distribution and outlier % has fallen.","d344515f":"## Transform Data\n\nBefore we can pull up our sleeves and bang out some plots, we need to think about our data if it is friendly enough for making insightful plots. We have 10 numerical features, which can be used as such. But we have 4 Wilderness Area and 40 Soil Type features which are all containing binary values 0 and 1. This is not great for plotting especially if we want to slice and dice numerical features by conditioning on Wilderness Area or Soil Type, we cannot do that with One-Hot-Encoded (OHE) features. \n\nSo we need to revert the OHE features back into simpler nicer friendlier categorical features. As a first step we need to identify the OHE column indices so that we can apply reverse OHE on these columns. Pandas provides a nice function to do this. Name the column, it will return the index of that column in the data frame. This is a really cool function and help you not go wrong when you subset or transform specific columns of a data frame on your road to a cleaned up data frame. Enter .get_loc()","86fcbb65":"We observe from the heat map:\n1. Among positive correlations, Vertical and Horizontal distance to hydrology has the highest correlation, followed by Hillshade 3 PM with Aspect and Hillshade Noon.\n2. Among negative correlations, Hillshade 3pm and Hillshade Noon have the highest correlation, even the highest among all positive and negative correlations. Followed by Hillshade noon and Slope, hillshade 9 am and aspect.\n\nWe should probably consider combining these pairs of features in a way that we represent them as one feature. we dont know what and how that will be yet. To be given a thought when we get to feature engineering.","a5e234a9":"## 3. Horizontal Box plots as (5,2) to show wider and legible view of the data\n\nMapping values on X-axis is far more easier to understand where the quartiles are located as well as the outliers. I personally like this representation.","1e2846ed":"## Outlier Detection\nI created the below function to identify outliers in the training set.The IQR Ratio is set by default as 1.5, but we can play around with this to see how different Ratio thresholds gives different % of outliers in the data.","8416fb5f":"Okay, so we got the OHE column indices. For performing a Reverse OHE, pandas provides a simple method .idxmax(). When used this with axis = 1, which means travel along columns, it returns the column name with the highest value for each row. So a 1 for Soil_Type1 and 0 for all other Soil_Type features means we get the value as Soil_Type1. We do this for all OHE columns - once each for Wilderness_Area and Soil Type features and collect the values in a separate column called 'Wild_Area' and 'Soil_Type'. We may want to make a copy of the original data frame before transforming so that the original OHE data frame stays intact.\n\nCaution: Unlike R, in Python, we cannot simply copy one data frame to another with a simple assignment statement such as df1 = df2. If we do that in python, when we change df2, it also changes df1. So ensure to make a copy of our original data frame using .copy(). Now our original data is untouched and the OHE features are intact.\n\nOnce we got the two categorical features - Wild_Type and Soil_Type, we can simply drop the OHE columns leaving behind 10 numerical features and 3 categorical features - Wild_Type, Soil_Type and Cover_Type. ","fd302d99":"### Histograms by Cover Type and Wild Area\n\nNow we break down our earlier histogram colored by Cover Type, further by Wild Area - faceting by column - since they neatly fit into one line per feature. This will inform us on the prominent Cover Types that are found in each of the Wild Areas.\n\n**Technical Hints:** We use the col argument inside FacetGrid() to display the Wild Area wise histograms in 1 Row, 4 Columns. Plotting the legends in a Facet Grid plot is tricky as if we simply specify the label inside distplot() and call plt.legend() we will get the legend only on the last plot for every feature. To workaround this, for every feature, we loop around each axes and print the legend as we print each plot so that all column subplots are printed with a legend.","bfef1d6a":"We can see that for 'Elevation' - we have three modes. Maybe 3 classes are distinct and rest overlapping. we will investigate as we go.\n\nHorizontal and Vertical Distance to Hydrology have a large number of Zero values.Which means these observations are right at the water source. Curious to see which cover types do these fall under.\n\nHorizontal distance to Fire points - More points are closer to fire points\n\nwe can see that some of the features are highly skewed. While this can be a trait of the measured variable, there is also a chance that the presence of outliers may have skewed the data. If the outliers were to be removed, it can be verified if the same distribution still holds. We will discover and investigate outliers later using box plots which are probably the best form of visualization for plotting multiple distributions together and quickly checking for the quantum of outliers in the overall feature set.","8e3b1bb9":"### Regular Heatmap with rectangular layout","37b9d790":"## 3. Histograms: Visualize the Histograms by Cover Type and additionally by Wild Areas and Soil Types","5ad15372":"Let us take a deeper look at the 54 features we have. Prominent among them are Wilderness_Area* and Soil_Type* features.\nThere are 4 Wilderness_Area* features and 40 Soil_Type* features. Clearly points to categorical values for a more general Wilderness_Area and Soil_Type features. An easier way to check if these are categorical variables is to find out the unique values for the Soil Type and Wilderness Area features. We can use .value_counts() for each of these columns to check the distinct values.\nWe can also examine what are the different classes within 'Cover_Type' target variable.\n","1f5364df":"Though there is more overlap visible with a high degree of transparency, Elevation is still able to separate quite a number of points and is a valuable feature. I tried plotting other features against all other features and with alpha of 0.2, the colors are all over the place. Clearly no dominant sign for separating classes. If you are interested in verifing this, just replace the code by switching Elevation with any other feature of interest and pass the appropriate columns to compare against all other features.\n"}}