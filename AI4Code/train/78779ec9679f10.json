{"cell_type":{"a9347676":"code","9952b1a1":"code","67758396":"code","90e54ff3":"code","12e9d7dc":"code","8d6c177a":"code","c4e7c41b":"code","4395daaa":"code","f1f8589b":"code","ef0d5278":"code","3d2f69ee":"code","b98a383b":"code","73ac26ef":"code","e1d9add8":"code","6da1f419":"code","155e58ae":"code","39ccb436":"code","a00f7453":"code","095c36da":"code","f2830b44":"code","b3b51c29":"code","1c669a77":"code","6b88759d":"code","3bf28fb6":"code","fc8da8dd":"code","681db9c8":"code","0b3c3d16":"code","9d02a103":"code","ae5c5144":"code","ac5c6d1a":"code","f3130b93":"code","02998185":"code","f2f56ec3":"code","3f20b562":"code","2eff829a":"code","ea62f77f":"code","6e2c80f0":"code","528b9e51":"code","2320942f":"code","68e299b5":"code","35c5800b":"code","65d2603d":"code","b4279de4":"code","3020d2f7":"code","b204112f":"code","c14bb6a3":"code","05d46db5":"code","ee803501":"code","ce10ba27":"code","d6d283bc":"code","0f8bde22":"code","36b32cd1":"code","535c8507":"code","5380d530":"code","eaa0fbe9":"code","ee319da7":"code","36205cff":"code","6120a6ba":"code","69278a0f":"code","70fe8580":"code","b0aeee1f":"code","f9c97fae":"code","693736c0":"code","be9b7a73":"code","965a9655":"code","9ee199db":"code","cdf89723":"code","db706964":"code","2f8de37d":"code","d296a7d5":"code","1b5d308e":"code","0eba1bf1":"code","973b2d58":"code","bce1e03e":"code","5fde8121":"code","1c0f9c0d":"code","8318e3d7":"code","58214505":"code","0de82a6b":"code","050e3a7f":"code","5194ce6f":"code","b3c47494":"code","ef97687e":"code","0c2af068":"code","0e61a2ad":"code","01bbabb3":"code","760cf58c":"code","4d93f1ba":"code","08bb9344":"code","7927efb8":"code","1f609323":"code","b8ec204b":"code","fa06a48f":"code","18c860a7":"code","a3155d51":"markdown","e29ea1d8":"markdown","3da20674":"markdown","1f22e29e":"markdown","c1539c11":"markdown","86562314":"markdown","b5b2789f":"markdown","bca6e6bc":"markdown","b32f52d9":"markdown","da20283b":"markdown","cfa21da6":"markdown","e46125a8":"markdown","703bb181":"markdown","0a673eae":"markdown","618f6313":"markdown","94fb8e7a":"markdown","82a7e498":"markdown","fe88fb6a":"markdown","2bce79a0":"markdown","73a1dd18":"markdown","e4dbd1a7":"markdown","f647a793":"markdown","5a8deb20":"markdown","4ba1e076":"markdown","a5ea95d1":"markdown","89f5c2aa":"markdown","d2700b3d":"markdown","f9d2e06f":"markdown","6868a6d9":"markdown","bf4482c9":"markdown","f539f8ef":"markdown","a1664110":"markdown","af3e4c4d":"markdown","7b44c969":"markdown","cfd16784":"markdown","39357f4c":"markdown","cb9b58ff":"markdown","60a6d3fa":"markdown","8678a5be":"markdown","54062826":"markdown","48286be1":"markdown","94448f7a":"markdown","4c3bf6d4":"markdown","002e42c8":"markdown","cc69e3af":"markdown","b4c80cac":"markdown","36353052":"markdown"},"source":{"a9347676":"!ls ..\/input","9952b1a1":"PATH = '..\/input\/career-con-2019'","67758396":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport gc\ngc.enable()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain_x = pd.read_csv(PATH+'\/X_train.csv')\ntrain_y = pd.read_csv(PATH+'\/y_train.csv')","90e54ff3":"def prepare_data(t):\n    def f(d):\n        d=d.sort_values(by=['measurement_number'])\n        return pd.DataFrame({\n         'lx':[ d['linear_acceleration_X'].values ],\n         'ly':[ d['linear_acceleration_Y'].values ],\n         'lz':[ d['linear_acceleration_Z'].values ],\n         'ax':[ d['angular_velocity_X'].values ],\n         'ay':[ d['angular_velocity_Y'].values ],\n         'az':[ d['angular_velocity_Z'].values ],\n        })\n\n    t= t.groupby('series_id').apply(f)\n\n    def mfft(x):\n        return [ x\/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\n    t['lx_f']=[ mfft(x) for x in t['lx'].values ]\n    t['ly_f']=[ mfft(x) for x in t['ly'].values ]\n    t['lz_f']=[ mfft(x) for x in t['lz'].values ]\n    t['ax_f']=[ mfft(x) for x in t['ax'].values ]\n    t['ay_f']=[ mfft(x) for x in t['ay'].values ]\n    t['az_f']=[ mfft(x) for x in t['az'].values ]\n    return t\n\nt=prepare_data(train_x)\n\nt=pd.merge(t,train_y[['series_id','surface','group_id']],on='series_id')\nt=t.rename(columns={\"surface\": \"y\"})\n\ndef aggf(d, feature):\n    va= np.array(d[feature].tolist())\n    mean= sum(va)\/va.shape[0]\n    var= sum([ (va[i,:]-mean)**2 for i in range(va.shape[0]) ])\/va.shape[0]\n    dev= [ math.sqrt(x) for x in var ]\n    return pd.DataFrame({\n        'mean': [ mean ],\n        'dev' : [ dev ],\n    })\n\ndisplay={\n'hard_tiles_large_space':'r-.',\n'concrete':'g-.',\n'tiled':'b-.',\n\n'fine_concrete':'r-',\n'wood':'g-',\n'carpet':'b-',\n'soft_pvc':'y-',\n\n'hard_tiles':'r--',\n'soft_tiles':'g--',\n}\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(14, 8*7))\n#plt.margins(x=0.0, y=0.0)\n#plt.tight_layout()\n# plt.figure()\n\nfeatures=['lx_f','ly_f','lz_f','ax_f','ay_f','az_f']\ncount=0\n\nfor feature in features:\n    stat= t.groupby('y').apply(aggf,feature)\n    stat.index= stat.index.droplevel(-1)\n    b=[*range(len(stat.at['carpet','mean']))]\n\n    count+=1\n    plt.subplot(len(features)+1,1,count)\n    for i,(k,v) in enumerate(display.items()):\n        plt.plot(b, stat.at[k,'mean'], v, label=k)\n        # plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n   \n    leg = plt.legend(loc='best', ncol=3, mode=\"expand\", shadow=True, fancybox=True)\n    plt.title(\"sensor: \" + feature)\n    plt.xlabel(\"frequency component\")\n    plt.ylabel(\"amplitude\")\n\ncount+=1\nplt.subplot(len(features)+1,1,count)\nk='concrete'\nv=display[k]\nfeature='lz_f'\nstat= t.groupby('y').apply(aggf,feature)\nstat.index= stat.index.droplevel(-1)\nb=[*range(len(stat.at['carpet','mean']))]\n\nplt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\nplt.title(\"sample for error bars (lz_f, surface concrete)\")\nplt.xlabel(\"frequency component\")\nplt.ylabel(\"amplitude\")\n\nplt.show()","12e9d7dc":"del train_x, train_y \ngc.collect()","8d6c177a":"import os\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nimport math\n\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import *\n\nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers","c4e7c41b":"X_train = pd.read_csv(PATH+\"\/X_train.csv\")\nX_test = pd.read_csv(PATH+\"\/X_test.csv\")\ny_train = pd.read_csv(PATH+\"\/y_train.csv\")\nsub = pd.read_csv(PATH+\"\/sample_submission.csv\")","4395daaa":"print (\"Train: measurements = %d , features = %d\" % X_train.shape)\nX_train.head()","f1f8589b":"plt.figure(figsize=(15, 5))\nsns.countplot(y_train['surface'])\nplt.title('Target distribution', size=15)\nplt.show()","ef0d5278":"X_train.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_train = X_train.values.reshape((3810, 128, 10))\n\nX_test.drop(['row_id', \"series_id\", \"measurement_number\"], axis=1, inplace=True)\nX_test = X_test.values.reshape((3816, 128, 10))","3d2f69ee":"for j in range(2):\n    plt.figure(figsize=(15, 5))\n    plt.title(\"Target : \" + y_train['surface'][j], size=15)\n    for i in range(10):\n        plt.plot(X_train[j, :, i], label=i)\n    plt.legend()\n    plt.show()","b98a383b":"encode_dic = {'fine_concrete': 0, \n              'concrete': 1, \n              'soft_tiles': 2, \n              'tiled': 3, \n              'soft_pvc': 4,\n              'hard_tiles_large_space': 5, \n              'carpet': 6, \n              'hard_tiles': 7, \n              'wood': 8}\n\ndecode_dic = {0: 'fine_concrete',\n              1: 'concrete',\n              2: 'soft_tiles',\n              3: 'tiled',\n              4: 'soft_pvc',\n              5: 'hard_tiles_large_space',\n              6: 'carpet',\n              7: 'hard_tiles',\n              8: 'wood'}\n\ny_train = y_train['surface'].map(encode_dic).astype(int)","73ac26ef":"class Attention(Layer):\n    def __init__(self, step_dim, W_regularizer=None, b_regularizer=None, W_constraint=None, b_constraint=None, bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        assert len(input_shape) == 3\n        self.W = self.add_weight((input_shape[-1],), initializer=self.init, name='{}_W'.format(self.name), regularizer=self.W_regularizer, constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],), initializer='zero', name='{}_b'.format(self.name), regularizer=self.b_regularizer, constraint=self.b_constraint)\n        else:\n            self.b = None\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n        if self.bias: eij += self.b\n        eij = K.tanh(eij)\n        a = K.exp(eij)\n        if mask is not None: a *= K.cast(mask, K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","e1d9add8":"def make_model():\n    inp = Input(shape=(128, 10))\n    x = Bidirectional(CuDNNLSTM(32, return_sequences=True))(inp)\n    x = Attention(128)(x)\n    x = Dense(9, activation=\"softmax\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","6da1f419":"def k_folds(X, y, X_test, k=5):\n    folds = list(StratifiedKFold(n_splits=k).split(X, y))\n    y_test = np.zeros((X_test.shape[0], 9))\n    y_oof = np.zeros((X.shape[0]))\n    \n    for i, (train_idx, val_idx) in  enumerate(folds):\n        print(f\"Fold {i+1}\")\n        model = make_model()\n        model.fit(X[train_idx], y[train_idx], batch_size=64, epochs=75, \n                  validation_data=[X[val_idx], y[val_idx]], verbose=0)\n        \n        pred_val = np.argmax(model.predict(X[val_idx]), axis=1)\n        score = accuracy_score(pred_val, y[val_idx])\n        y_oof[val_idx] = pred_val\n        \n        print(f'Scored {score:.3f} on validation data')\n        \n        y_test += model.predict(X_test)\n        \n    return y_oof, y_test                               ","155e58ae":"y_oof, y_test = k_folds(X_train, y_train, X_test, k=5)","39ccb436":"print(f'Local CV is {accuracy_score(y_oof, y_train): .4f}')","a00f7453":"y_test = np.argmax(y_test, axis=1)\nsub['surface'] = y_test\nsub['surface'] = sub['surface'].map(decode_dic)\nsub.head()\nsub.to_csv('submission.csv', index=False)","095c36da":"gc.collect()","f2830b44":"!ls ..\/input","b3b51c29":"import numpy as np\nimport pandas as pd\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom seaborn import countplot,lineplot, barplot\n%matplotlib inline\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport random\nimport math\n\nfrom numpy.fft import *\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as FF\n\nle = preprocessing.LabelEncoder()\n\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\ngc.enable()\n","1c669a77":"PATH = \"..\/input\/career-con-2019\"","6b88759d":"testing = True #Set this to true for submission\/False for cross validation\nX_train = pd.read_csv(PATH+'\/X_train.csv')\ny_train = pd.read_csv(PATH+'\/y_train.csv')\nX_train = pd.merge(X_train,y_train,on='series_id')\nX_train['surface'] = le.fit_transform(X_train['surface'])\n\nif(testing):\n    X_test = pd.read_csv(PATH+'\/X_test.csv')\n    X_test['series_id'] = X_test['series_id']+3810\n    X_test['group_id'] = 0\n    X_test['surface'] = 0\n    frames = [X_train,X_test]\n    X_train = pd.concat(frames)\n    X_train.reset_index(drop=True,inplace=True)","3bf28fb6":"X_train.head()","fc8da8dd":"X_test.head()","681db9c8":"cols = list(X_train.columns.values)\ncols.remove('orientation_W')\ncols.insert(3,'orientation_W')\nX_train = X_train[cols]\nprint (\"New train data\")\nX_train.head()","0b3c3d16":"num_meas = 128\nnum_series = X_train['series_id'].nunique()\nprint (\"Number of measurements = \", num_meas)\nprint (\"Number of total sereis = \", num_series)","9d02a103":"def q_to_angle(q_val):\n    #We assume q_val is in this format: [qw, q1, q2, q3]\n    #And the quaternion is normalized\n    roll = np.arctan2(2*(q_val[0]*q_val[1] + q_val[2]*q_val[3]),1 - 2*(q_val[1]*q_val[1] + q_val[2]*q_val[2]))\n    pitch = np.arcsin(2*(q_val[0]*q_val[2] - q_val[3]*q_val[1]))\n    yaw = np.arctan2(2*(q_val[0]*q_val[3] + q_val[1]*q_val[2]),1 - 2*(q_val[2]*q_val[2] + q_val[3]*q_val[3]))\n    return np.array([roll, pitch, yaw])","ae5c5144":"quat_arr = np.array(X_train[['orientation_W','orientation_X','orientation_Y','orientation_Z']])\neuler_arr = np.zeros([quat_arr.shape[0],3])\nfor n,arr in enumerate(quat_arr):\n    euler_arr[n] = q_to_angle(arr)","ac5c6d1a":"X_train['roll'] = euler_arr[:,0]\nX_train['pitch'] = euler_arr[:,1]\nX_train['yaw'] = euler_arr[:,2]","f3130b93":"cols = list(X_train.columns.values)\ncols.remove('group_id')\ncols.append('group_id')\ncols.remove('surface')\ncols.append('surface')\nX_train = X_train[cols]\nprint (\"Train updated\")\nX_train.head()","02998185":"feat_cols = ['roll','pitch','yaw','angular_velocity_X','angular_velocity_Y','angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\nfeat_array = np.array(X_train[feat_cols])\nfeat_array = np.reshape(feat_array,[num_series,128,len(feat_cols)])\ngroup_array = np.array(X_train['group_id'])\ngroup_array = np.reshape(group_array,[num_series,128])\ngroup_array = group_array[:,0]\ntarget_array = np.array(X_train['surface'])\ntarget_array = np.reshape(target_array,[num_series,128])\ntarget_array = target_array[:,0]","f2f56ec3":"#Use the first order difference of the following features\n#Absolute Orientation features dont make sense to predict surface\ndelta_cols = ['roll','pitch','yaw']\nfor dc in delta_cols:\n    iia = feat_cols.index(dc)\n    np_arr = feat_array[:,:,iia]\n    roll_arr = np.copy(np_arr)\n    roll_arr[:,1:] = roll_arr[:,:-1]\n    np_arr = np_arr - roll_arr\n    feat_array[:,:,iia] = np_arr","3f20b562":"norm_cols = ['linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z','angular_velocity_X','angular_velocity_Y','angular_velocity_Z']\nfor norm in norm_cols:\n    iia = feat_cols.index(norm)\n    np_arr = feat_array[:,:,iia]\n    mean_arr = np.mean(np_arr,1)\n    mean_arr = np.expand_dims(mean_arr,1)\n    mean_arr = np.repeat(mean_arr,num_meas,1)\n    np_arr = np_arr - mean_arr\n    feat_array[:,:,iia] = np_arr\n","2eff829a":"def absfft(x):\n    return np.abs(np.fft.rfft(x))\n\nfeat_fft_array = np.copy(feat_array[:,:,3:])\nfeat_fft_array = np.apply_along_axis(absfft,1,feat_fft_array)","ea62f77f":"num_sensor = feat_array.shape[2]\nfor i in range(num_sensor):\n    mean_s = np.mean(feat_array[:,:,i])\n    sd_s = np.std(feat_array[:,:,i])\n    feat_array[:,:,i] = (feat_array[:,:,i]-mean_s)\/sd_s\n\nnum_sensor_fft = feat_fft_array.shape[2]\nfor i in range(num_sensor_fft):\n    mean_s = np.mean(feat_fft_array[:,:,i])\n    sd_s = np.std(feat_fft_array[:,:,i])\n    feat_fft_array[:,:,i] = (feat_fft_array[:,:,i]-mean_s)\/sd_s","6e2c80f0":"from keras.layers import Input,Dense, Dropout, BatchNormalization, SeparableConv1D, Reshape, LSTM, DepthwiseConv2D,AveragePooling2D, CuDNNLSTM, Concatenate\nfrom keras.models import Model\nfrom keras.backend import squeeze\nfrom keras.regularizers import l2","528b9e51":"kr = None\nnum_groups = np.unique(group_array).shape[0]\nnum_surfaces = np.unique(target_array).shape[0]\n\ndef get_net_with_fft_mag_only(dp):\n    inputs_t = Input(shape=(128,len(feat_cols)))\n    x = SeparableConv1D(32,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(inputs_t)\n    x = Dropout(dp)(x)\n    x = SeparableConv1D(64,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n    x = Dropout(dp)(x)\n    x = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n    x = Dropout(dp)(x)\n    x = SeparableConv1D(256,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(x)\n    x = Reshape((256,))(x)\n    x = Dropout(dp)(x)\n    x = Dense(64, activation='relu',kernel_regularizer=kr)(x)\n    x = Dropout(dp)(x)\n    x = Dense(64, activation='relu')(x)\n    \n    inputs_f = Input(shape=(feat_fft_array.shape[1],feat_fft_array.shape[2]))\n    y = SeparableConv1D(32,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(inputs_f)\n    y = Dropout(dp)(y)\n    y = SeparableConv1D(64,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n    y = Dropout(dp)(y)\n    y = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n    y = Dropout(dp)(y)\n    y = SeparableConv1D(128,8,4,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n    y = Dropout(dp)(y)\n    y = SeparableConv1D(256,8,2,'same',depth_multiplier=1,activation='relu',kernel_regularizer=kr)(y)\n    y = Reshape((256,))(y)\n    y = Dropout(dp)(y)\n    y = Dense(64, activation='relu',kernel_regularizer=kr)(y)\n    y = Dropout(dp)(y)\n    y = Dense(64, activation='relu')(y)\n    \n        \n    inputs = [inputs_t,inputs_f]\n    \n    z = Concatenate()([x,y])\n    z = Dense(64, activation='relu')(z)\n    predictions = Dense(num_surfaces, activation='softmax')(z)\n    model = Model(inputs=inputs, outputs=predictions)\n    model.compile(optimizer='adam',\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","2320942f":"from sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\ndepthwise = False\nfft_net = True\nif(not(testing)):\n    gkf = GroupKFold(3)\n    train_gen = gkf.split(X=feat_array,groups=group_array)\n    preds = np.zeros_like(target_array)\n    for train_idx,test_idx in train_gen:\n        #Test features\n        t_feats = feat_array[train_idx]\n        t_feats_fft = feat_fft_array[train_idx]\n        \n        #Validation features\n        v_feats = feat_array[test_idx]\n        v_feats_fft = feat_fft_array[test_idx]\n        \n        t_vals = target_array[train_idx]\n        v_vals = target_array[test_idx]\n        \n        pred_classes = np.zeros([v_vals.shape[0],num_surfaces,5])\n        for k in range(5): #5 time averaging to get more stable results\n            nnet = get_net_with_fft_mag_only(0.5)\n            nnet.fit(x=[t_feats,t_feats_fft],y=t_vals,batch_size=256,epochs=3000,validation_data=([v_feats,v_feats_fft],v_vals),verbose=2)\n            pred_classes[:,:,k] = nnet.predict([v_feats,v_feats_fft])\n        pred_classes = np.mean(pred_classes,axis=2)\n        pred_classes = np.argmax(pred_classes,axis=1)\n        preds[test_idx] = pred_classes\n        print('Val accuracy: ',accuracy_score(v_vals,pred_classes))\n        pred_classes = nnet.predict([t_feats,t_feats_fft])\n        pred_classes = np.argmax(pred_classes,axis=1)\n        print('Train accuracy: ',accuracy_score(t_vals,pred_classes))\n    print('5 Fold accuracy: ', accuracy_score(target_array,preds))\nelse:\n    t_feats = feat_array[:3810]\n    t_feats_fft = feat_fft_array[:3810]\n    t_vals = target_array[:3810]\n    v_feats = feat_array[3810:]\n    v_feats_fft = feat_fft_array[3810:]\n    pred_classes = np.zeros([v_feats.shape[0],num_surfaces,3])\n    for k in range(3):\n        nnet = get_net_with_fft_mag_only(0.5)\n        nnet.fit(x=[t_feats,t_feats_fft],y=t_vals,batch_size=256,epochs=3000,verbose=0)\n        pred_classes[:,:,k] = nnet.predict([v_feats,v_feats_fft])\n    pred_classes = np.mean(pred_classes,axis=2)\n    pred_classes = list(np.argmax(pred_classes,axis=1))\n    pred_classes = [le.inverse_transform([i])[0] for i in pred_classes]\n    sub_df = pd.read_csv(PATH+'\/sample_submission.csv')\n    sub_df['surface'] = pred_classes\n    sub_df.to_csv('submission.csv',index=False)","68e299b5":"gc.collect()","35c5800b":"from collections import Counter\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.layers import LSTM,Input,Dense,Flatten,SpatialDropout1D,Dropout,CuDNNLSTM,Reshape,Concatenate\nfrom keras.layers import Lambda,concatenate,BatchNormalization\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\nfrom keras import backend as K \nfrom sklearn.preprocessing import LabelEncoder","65d2603d":"directory = \"..\/input\/career-con-2019\/\"\nX_test_path = os.path.join(directory,\"X_test.csv\")\nX_train_path = os.path.join(directory,\"X_train.csv\")\nX_test_data = pd.read_csv(X_test_path)\nX_train_data = pd.read_csv(X_train_path)\ny_train_data = pd.read_csv(os.path.join(directory,\"y_train.csv\"))\nsample_submission = pd.read_csv(os.path.join(directory,\"sample_submission.csv\"))","b4279de4":"print (\"Train data: measurements = %d , features=%d\" %X_train_data.shape)\nX_train_data.head()","3020d2f7":"print (\"Test data: measurements = %d , features=%d\" %X_test_data.shape)\nX_test_data.head()","b204112f":"le = LabelEncoder()\nle.fit(list(y_train_data[\"surface\"]))\ny_train_dataset_for_nn = to_categorical(le.transform(list(y_train_data[\"surface\"])))","c14bb6a3":"def dataset_for_nn(X_dataset):\n    num_samples = X_dataset.shape[0]\/\/128\n    X_dataset_for_nn = np.zeros((num_samples,128,10))\n    for i in range(num_samples):\n        subset = np.array(X_dataset.iloc[i*128:(i+1)*128,3:])\n        X_dataset_for_nn[i,:,:] = subset\n    return X_dataset_for_nn","05d46db5":"X_train_for_nn = dataset_for_nn(X_train_data)\nX_test_for_nn = dataset_for_nn(X_test_data)","ee803501":"print (\"Train data for NN\")\nX_train_for_nn[0]","ce10ba27":"def freqs(dataset,width):\n    X = np.abs(fftpack.fft(dataset))\n    squeezed_dataset = []\n    for i in range(64\/\/width):\n        squeezed_dataset.append(np.mean(X[i*width:(i+1)*width]))\n    return squeezed_dataset","d6d283bc":"def X_features(X_dataset,width=3):\n    num_samples = len(list(set(X_dataset[\"series_id\"])))\n    num_cols = 64\/\/width\n    features = np.zeros((num_samples,40+10*num_cols))\n    for i in range(num_samples):\n        X_train_subset = np.array(X_dataset.iloc[i*128:(i+1)*128,3:])\n        features[i,:10] = np.mean(X_train_subset,axis=0)\n        features[i,10:20] = np.std(X_train_subset,axis=0)\n        features[i,20:30] = np.max(X_train_subset,axis=0)-np.min(X_train_subset,axis=0)\n        features[i,30:40] = X_train_subset[-1,:]-X_train_subset[0,:]\n        for j in range(X_train_subset.shape[1]):\n            features[i,40+j*num_cols:40+(j+1)*num_cols] = freqs(X_train_subset[:,j],width)\n    return features","0f8bde22":"X_train_features = X_features(X_train_data)\nX_test_features = X_features(X_test_data)","36b32cd1":"def LSTM_NN(drop):\n    inp = Input(shape=(128,10))\n    x = SpatialDropout1D(0.1)(inp)\n    inp_2 = Input(shape=(250,))\n    x_2 = Dense(250, input_shape=(250,), activation=\"sigmoid\")(inp_2)\n    x_2 = Dropout(drop)(x_2)\n    x_2 = Dense(120, activation=\"sigmoid\")(x_2)\n    x_2 = Dropout(drop)(x_2)\n    x_2 = Dense(60, activation=\"sigmoid\")(x_2)\n    x_2 = Dropout(drop)(x_2)\n    x_2 = BatchNormalization()(x_2)\n    x = CuDNNLSTM(units=200, return_sequences=True, return_state=False, go_backwards=False)(x)\n    x = Dropout(drop)(x)\n    x = CuDNNLSTM(units=100, return_sequences=False, return_state=False, go_backwards=False)(x)\n    x = concatenate([x,x_2])\n    x = Dropout(drop)(x)\n    outp = Dense(9, activation=\"sigmoid\")(x)\n    model = Model(inputs=[inp,inp_2], outputs=outp)\n    model.compile(loss='categorical_crossentropy', optimizer=Adam())\n    return model","535c8507":"%%time\nn_splits=5\nkfold = StratifiedKFold(n_splits=n_splits, random_state=10, shuffle=True)\ny_test = np.zeros((X_test_for_nn.shape[0],9*n_splits))\ntrain_preds = np.zeros((X_train_for_nn.shape[0],9))\nX = X_train_for_nn\nX_test = X_test_for_nn\nY = np.array(list(y_train_data[\"surface\"]))\nfor i, (train_index, valid_index) in enumerate(kfold.split(X, Y)):\n    X_train, X_val =  X[list(train_index),:,:],X[list(valid_index),:,:]\n    X_train_feat, X_val_feat = X_train_features[list(train_index),:],X_train_features[list(valid_index),:]\n    Y_train, Y_val = Y[list(train_index)], Y[list(valid_index)]\n    Y_train = to_categorical(le.transform(Y_train))\n    Y_val = to_categorical(le.transform(Y_val))\n    model = LSTM_NN(0.15)\n    model.fit([X_train,X_train_feat], Y_train, epochs=120, validation_data=([X_val,X_val_feat], Y_val), verbose=2) \n    y_pred = model.predict([X_val,X_val_feat], verbose=2)\n    y_test[:,i*9:(i+1)*9] = model.predict([X_test,X_test_features])\n    train_preds[list(valid_index),:] = np.squeeze(y_pred)","5380d530":"res_train = np.argmax(train_preds,axis=1)\nans_train = np.argmax(to_categorical(le.transform(Y)),axis=1)\nprint (\"CV score\",round(accuracy_score(res_train,ans_train),4))\n","eaa0fbe9":"def most_frequent(List): \n    occurence_count = Counter(List) \n    return occurence_count.most_common(1)[0][0]\n\nres_test_inter = np.zeros((y_test.shape[0],n_splits))\nres_test=[]\nfor i in range(n_splits):\n    inter_arr = y_test[:,i*9:(i+1)*9]\n    res_test_inter[:,i] = np.argmax(inter_arr,axis=1)\nfor j in range(y_test.shape[0]):\n    res_test.append(int(most_frequent(res_test_inter[j,:])))","ee319da7":"test_for_sub=le.inverse_transform(res_test)\nprint (test_for_sub[:5])\ntest_size = len(list(set(X_test_data[\"series_id\"])))\nY_test_pred_array = np.zeros((test_size,2))\nY_test_for_submission = pd.DataFrame(Y_test_pred_array,columns = [\"series_id\",\"surface\"])\nY_test_for_submission.iloc[:,0] = list(range(test_size))\nY_test_for_submission.iloc[:,1] = test_for_sub\nY_test_for_submission.to_csv(\"submission.csv\",index=None)\nY_test_for_submission.head()","36205cff":"gc.collect()","6120a6ba":"!ls ..\/input\/","69278a0f":"data_target = pd.read_csv(PATH+\"\/y_train.csv\")\ntrain_X = pd.read_csv(PATH+'\/X_train.csv').iloc[:,3:]\ntest_X  = pd.read_csv(PATH+'\/X_test.csv').iloc[:,3:]","70fe8580":"train_X.head()","b0aeee1f":"train_X = train_X.append(test_X)\ntrain_X = train_X.values.reshape(-1,128,10)","f9c97fae":"train_X[0]","693736c0":"test_X.head()","be9b7a73":"# Originally from Markus F's kernel: https:\/\/www.kaggle.com\/friedchips\/the-missing-link\ndef sq_dist(a,b):\n    ''' the squared euclidean distance between two samples '''\n    \n    return np.sum((a-b)**2, axis=1)\n\n\ndef find_run_edges(data, edge):\n    ''' examine links between samples. left\/right run edges are those samples which do not have a link on that side. '''\n\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n    \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4]) # distances to rest of samples\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list) # this is i's closest neighbor\n        if closest_i == i: # this might happen and it's definitely wrong\n            print('Sample', i, 'linked with itself. Next closest sample used instead.')\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4]) # now find closest_i's closest neighbor\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list) # here it is\n        if closest_rev == closest_i: # again a check\n            print('Sample', i, '(back-)linked with itself. Next closest sample used instead.')\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev): # we found an edge\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\n\ndef find_runs(data, left_edges, right_edges):\n    ''' go through the list of samples & link the closest neighbors into a single run '''\n    \n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","965a9655":"train_left_edges, train_left_linked  = find_run_edges(train_X, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(train_X, edge='right')\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","9ee199db":"train_runs = find_runs(train_X, train_left_edges, train_right_edges)","cdf89723":"lost_samples = np.array([ i for i in range(len(train_X)) if i not in np.concatenate(train_runs) ])\nprint(lost_samples)\nprint(len(lost_samples))\n","db706964":"find_run_edges(train_X[lost_samples], edge='left')[1][0]\nlost_run = np.array(lost_samples[find_runs(train_X[lost_samples], [0], [5])[0]])\ntrain_runs.append(lost_run)","2f8de37d":"first_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==821:\n            print(i)\n            first_double_surface = i","d296a7d5":"train_runs[first_double_surface]","1b5d308e":"new_train_runs = [821,  974,  328, 1548,  172,\n        355,  957, 1481, 1046, 1650,  857,  724,  164, 1092, 1017, 1300,\n       1212,  536,  531, 1032,  994, 1501,  588,  579, 1177,  812, 1333,\n       1253]\n\ntrain_runs[first_double_surface] = train_runs[first_double_surface][0:-len(new_train_runs)]","0eba1bf1":"train_runs.append(np.array(new_train_runs))","973b2d58":"second_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==3055:\n            print(i)\n            second_double_surface = i","bce1e03e":"train_runs[second_double_surface]","5fde8121":"new_train_runs2 = [3055, 3360, 3662,\n       3780, 3663, 3091, 3769, 3175, 1957, 2712, 2063, 2708, 3139, 2722]\n\ntrain_runs[second_double_surface] = train_runs[second_double_surface][0:-len(new_train_runs2)]","1c0f9c0d":"train_runs.append(np.array(new_train_runs2))","8318e3d7":"third_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==2484:\n            print(i)\n            third_double_surface = i","58214505":"train_runs[third_double_surface]","0de82a6b":"new_train_runs3 = [2484, 3062, 2290, 3517, 3293, 2651, 3767,\n       2029, 2558, 3580, 1874, 3373, 2514, 2308, 3160, 3161, 3613, 2511,\n       2469, 2990, 2780, 3756, 2376, 2616, 2540, 2039, 2219, 3743, 3198,\n       2584, 2752, 2304, 2887, 2841, 3480, 2517, 3020, 3424, 2027, 2652,\n       2648, 3433, 2359, 3392, 3164, 3798, 3642, 2713, 3405, 3673, 2369,\n       3411, 3595, 2242, 2307, 1897, 2834, 2350, 3795, 2948, 1856, 3486,\n       3353, 1966]\n\ntrain_runs[third_double_surface] = train_runs[third_double_surface][0:-len(new_train_runs3)]\ntrain_runs.append(np.array(new_train_runs3))","050e3a7f":"fourth_double_surface = 0\nfor i in range(0, len(train_runs)):\n    for x in train_runs[i]:\n        if x==3501:\n            print(i)\n            fourth_double_surface = i","5194ce6f":"train_runs[fourth_double_surface]","b3c47494":"new_train_runs4 = [3501, 2785]\ntrain_runs[fourth_double_surface] = train_runs[fourth_double_surface][0:-len(new_train_runs4)]\ntrain_runs.append(np.array(new_train_runs4))","ef97687e":"df_train_y = pd.DataFrame()\ndf_train_y['run_id'] = 0\ndf_train_y['run_pos'] = 0\n\nfor run_id in range(len(train_runs)):\n    for run_pos in range(len(train_runs[run_id])):\n        series_id = train_runs[run_id][run_pos]\n        df_train_y.at[ series_id, 'run_id'  ] = run_id\n        df_train_y.at[ series_id, 'run_pos' ] = run_pos\n\ndf_train_y.tail()","0c2af068":"df_train_y['index'] = df_train_y.index\ndf_train_y = df_train_y.sort_values('index')\ndf_train_y.rename(columns={'index':'series_id'}, inplace=True)\ndf_train_y['run_id'] = df_train_y['run_id'].apply(lambda x: int(x))\ndf_train_y['run_pos'] = df_train_y['run_pos'].apply(lambda x: int(x))\ndf_train_y.tail()","0e61a2ad":"run_id_train = df_train_y['run_id'][0:3810].values\nrun_id_test = df_train_y['run_id'][3810:7626].values","01bbabb3":"data_target['run_id'] = run_id_train\nmapping_leak = {}\nfor i in range(0,3810):\n    cur_data = data_target.iloc[i]\n    mapping_leak.update({cur_data['run_id']: cur_data['surface']})\n    \nunknown_run = []\nans_test = []\nknown_series = []\nunknown_series = []\n\nfor i in range(0,3816):\n    if run_id_test[i] in mapping_leak:\n        ans_test.append(mapping_leak[run_id_test[i]])\n        known_series.append(i)\n    else:\n        ans_test.append('unknown')\n        unknown_series.append(i)\n        unknown_run.append(run_id_test[i])","760cf58c":"print(\"Number of known series:{}\\nNumber of unknown series:{}\\n\".format(len(known_series),len(unknown_series)))","4d93f1ba":"sub = pd.read_csv(PATH+\"\/sample_submission.csv\")\nsub['surface'] = ans_test\nsub.head()","08bb9344":"!ls ..\/input\/robots-best-submission","7927efb8":"best = pd.read_csv('..\/input\/robots-best-submission\/mybest0.73.csv')\nmap_best_ans = {}\n\nfor i in range(0, best.shape[0]):\n    map_best_ans.update({best.iloc[i]['series_id'] : best.iloc[i]['surface'] })","1f609323":"result = []\nfor i in range(0, sub.shape[0]):\n    if (sub.surface[i] == 'unknown'):\n        result.append(map_best_ans[i])\n    else:\n        result.append(sub.surface[i])","b8ec204b":"sub.surface = result\nsub.to_csv('submission3.csv', index=False)\nsub.head()","fa06a48f":"sub.surface.value_counts()","18c860a7":"gc.collect()","a3155d51":"Like what Markus F said in his kernel [The missing link](https:\/\/www.kaggle.com\/friedchips\/the-missing-link)\n, we can reconstruct the original data before it's being splitted.\n\nApparently, some of the original data is being splitted between train and test data. So here we go:","e29ea1d8":"**CV Score**","3da20674":"**Input: Prepare data for the Neural Network**","1f22e29e":"### Run NN + Group K-fold","c1539c11":"Because some runs is linked between train and test data, we can essentially use train's target to map our test's target.","86562314":"Function for extracting Fourier transform with averaging","b5b2789f":"**Thanks Theo!**","bca6e6bc":"Normalize each 128-pt sample to ensure there is no group related information left in the samples","b32f52d9":"**Find the edges Part I**","da20283b":"Then, I just combine these data with my best submission (LB 0.73). I keep the known series target values while replacing the unknown one with values from my best submission.","cfa21da6":"## [My solution](https:\/\/www.kaggle.com\/ilhamfp31\/16-solution-0-76) by  Ilham Firdausi Putra","e46125a8":"**Output: encode\/decode target**","703bb181":"### Create NN Model","0a673eae":"Further normalization across the entire dataset to ensure NN inputs are zero-mean and unit standard deviation","618f6313":"**Submission**","94fb8e7a":"**Submission**","82a7e498":"<br>\n## [NN+LSTM,fft+features](https:\/\/www.kaggle.com\/skondrash\/nn-lstm-fft-features?scriptVersionId=12728595) by [Sergey K.](https:\/\/www.kaggle.com\/skondrash)","fe88fb6a":"Creating features","2bce79a0":"After we remove the double surface runs, let's add this knowledge to our train_y.","73a1dd18":"## [Simple Fourier Analysis](https:\/\/www.kaggle.com\/trohwer64\/simple-fourier-analysis) by [Thomas Rohwer](https:\/\/www.kaggle.com\/trohwer64)","e4dbd1a7":"**Exctracting test prediction (most frequent)**","f647a793":"**Load data**","5a8deb20":"## [Deep Learning Starter](https:\/\/www.kaggle.com\/theoviel\/deep-learning-starter) by [Theo Viel](https:\/\/www.kaggle.com\/theoviel)","4ba1e076":"**Looad data**","a5ea95d1":"**K-Folds**","89f5c2aa":"<br>\n**Load packages**","d2700b3d":"**Find the runs Part I**","f9d2e06f":"**Model**","6868a6d9":"\n# CareerCon 2019 - Help Navigate Robots\n## Top Solutions Compilation\n\n![](https:\/\/www.lextronic.fr\/imageslib\/4D\/0J7589.320.gif)\n\n---\n\nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.\n\nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n\n---\n\n> This is just a **compilation** of all the best solutions I have found, different types of analysis and posts (discussions).\n> **The purpose** of this is to have in a single kernel all the relevant information\/solutions of this competition, so that if in the future there is a similar competition this material can be used as baseline, and maybe the authors delete the kernel or make it private, so we would lose all that amazing information.\n> **Please** comment bellow if you want me to add something, or if I missed something important... and please support the original kernels and their authors.\n\nMy best\/original kernel in this competition is [#1 Smart Robots. Most Complete Notebook \ud83e\udd16](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-most-complete-notebook)\n\n### References\n\n- [#16 Solution](https:\/\/www.kaggle.com\/ilhamfp31\/16-solution-0-76) by [@ilhamfp31](https:\/\/www.kaggle.com\/ilhamfp31)\n- [Submission (Fourier, Neighbour Detection, SVM)](https:\/\/www.kaggle.com\/trohwer64\/submission-fourier-neighbour-detection-svm\/code) by [Thomas Rohwer](https:\/\/www.kaggle.com\/trohwer64)\n- [Starter Code for 3rd place Solution](https:\/\/www.kaggle.com\/prith189\/starter-code-for-3rd-place-solution)\n- https:\/\/www.kaggle.com\/ilhamfp31\/16-solution-0-76\n- https:\/\/www.kaggle.com\/whoiskk\/15-solution-private-0-77\n\n<br>","bf4482c9":"Neural network architecture LSTM","f539f8ef":"In this kernel, Theo directly feed the data into a **Recurrent Neural Network**. For fancyness, Theo added an **Attention Mechanism**.\n\n","a1664110":"# 2. Basic Kernels","af3e4c4d":"Rename index as series_id.","7b44c969":"<br>\n# 3. Top Solutions\n<br>\n## [The Missing Link...](https:\/\/www.kaggle.com\/friedchips\/the-missing-link) by [Markus F](https:\/\/www.kaggle.com\/friedchips)","cfd16784":"As you can see, we automatically get more 69% of our test target classified correctly!","39357f4c":"**Training model and prediction**","cb9b58ff":"Let's find the missing samples from test data","60a6d3fa":"As what have been said in Markus F's kernel, there is some train runs (4) having more than 1 surface. We need to separate in manually. In order to do this, we need to track the runs in which the mis-allocated series lies.","8678a5be":"# 1. Starter code: [#1 Smart Robots. Most Complete Notebook \ud83e\udd16](https:\/\/www.kaggle.com\/jesucristo\/1-smart-robots-most-complete-notebook)","54062826":"Creating X-datasets for LSTM - shape = (num_samples,128,10)","48286be1":"Now let's separate train and test.","94448f7a":"**Load data**","4c3bf6d4":"**Attention Layer**","002e42c8":"## [Starter Code for 3rd place Solution](https:\/\/www.kaggle.com\/prith189\/starter-code-for-3rd-place-solution\/notebook) by [Prithvi](https:\/\/www.kaggle.com\/prith189)","cc69e3af":"Converting class labels to binary matrix representation","b4c80cac":"**From quaternion to euler angles**\n\n![](https:\/\/www.researchgate.net\/publication\/283951857\/figure\/fig2\/AS:319897696522260@1453280962401\/Roll-pitch-yaw-angles-of-cars-and-other-land-based-vehicles-10.png)","36353052":"**Load packages**"}}