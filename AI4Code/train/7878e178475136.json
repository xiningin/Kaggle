{"cell_type":{"e2d0fdc4":"code","1c8b392c":"code","e5f61403":"code","f0e820f6":"code","030f94dd":"code","9c3296b1":"code","c1bb069c":"code","4423c6b8":"code","757d765b":"code","9cd7d315":"code","05d9d96d":"code","aea512cd":"code","0687492e":"code","8d0f7072":"code","26b34d38":"code","c98784bf":"code","90ba0885":"code","3f9f27d0":"code","38b91017":"code","991d8f14":"markdown","81e11fae":"markdown","b5a09c39":"markdown","f4f92772":"markdown","3bc0cc32":"markdown","84750006":"markdown","21ac620b":"markdown","507b0867":"markdown","f34d6a0f":"markdown","a0f990f4":"markdown","d94ee28b":"markdown","7d13294b":"markdown","30659afd":"markdown","23763550":"markdown","311652da":"markdown","5be0cd93":"markdown","c5080429":"markdown","bd171e36":"markdown","27761f1e":"markdown"},"source":{"e2d0fdc4":"from IPython.display import Image\nImage(filename =\"..\/input\/dszdecisiontree\/min-samples-split.png\", width=500, height=500)","1c8b392c":"import pandas as pd\nimport numpy as np","e5f61403":"df = pd.read_csv(\"..\/input\/\/dszdecisiontree\/bank-numeric.txt\")","f0e820f6":"df.head()","030f94dd":"features = df.drop(\"deposit_cat\", axis=1)\ntargets = df['deposit_cat']","9c3296b1":"features_names = list(features.columns)\ntargets_names = [\"Deposit_cat\"]","c1bb069c":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier","4423c6b8":"X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=0)","757d765b":"dt = DecisionTreeClassifier()","9cd7d315":"model = dt.fit(X_train, y_train)","05d9d96d":"predictions = model.predict(X_test)","aea512cd":"from sklearn import tree","0687492e":"import pydot\nimport graphviz","8d0f7072":"dot_data = tree.export_graphviz(\n         dt, \n         out_file=None,\n         feature_names=features_names,\n         filled=True, rounded=True,\n         proportion=False,\n         node_ids=True,\n         rotate=False\n        )  \ngraph = graphviz.Source(dot_data)  \ngraph","26b34d38":"dt = DecisionTreeClassifier(max_depth=6)\n\nmodel = dt.fit(X_train, y_train)\n\npredictions = model.predict(X_test)","c98784bf":"dot_data = tree.export_graphviz(\n         dt, \n         out_file=None,\n         feature_names=features_names,\n         filled=True, rounded=True,\n         proportion=False,\n         node_ids=True,\n         rotate=False\n        )  \ngraph = graphviz.Source(dot_data)  \ngraph","90ba0885":"from mlxtend.plotting import plot_decision_regions\nimport matplotlib.pyplot as plt","3f9f27d0":"def compara_modelos(maxdepth):\n    if maxdepth == 0:\n        dt = tree.DecisionTreeClassifier(random_state=1)\n    else:   \n        dt = tree.DecisionTreeClassifier(random_state=1, max_depth=maxdepth)\n    dt.fit(X_train, y_train)\n    train_score = dt.score(X_train, y_train)\n    test_score = dt.score(X_test, y_test)\n    return train_score,test_score","38b91017":"print('{:10} {:20} {:20}'.format('depth', 'Training score','Testing score'))\nprint('{:10} {:20} {:20}'.format('-----', '--------------','-------------'))\ndepth = np.linspace(15,0,16)\n\nfor profundidade in depth:\n    if profundidade != 0:\n        print('{:1}         {} '.format(profundidade,str(compara_modelos(profundidade))))\n    else:\n        print('{:1}         {} '.format('Full',str(compara_modelos(profundidade))))\n    ","991d8f14":"**7) Baseado no conhecimento aprendido sobre o par\u00e2metro min_samples_split e seu impacto na complexidade do modelo, utilize a fun\u00e7\u00e3o abaixo e plote as fronteiras para os valores de min_samples_split iguais a 1, 5 e 15. **","81e11fae":"**8) Atrav\u00e9s do conhecimento aprendido em aula, compare a performance dos modelos de arvore de decis\u00e3o variando o parametro max_depth em rela\u00e7\u00e3o a performance utilizando como valida\u00e7\u00e3o os dados de treinamento e teste. Para isso siga os passos dos itens abaixo. **","b5a09c39":"## def visualize_fronteiras(msamples_split):\n    X_train, X_test, y_train, y_test = train_test_split(features, targets, random_state=0)\n\n    clf = DecisionTreeClassifier(min_samples_split=msamples_split)\n    tree = clf.fit(X_train, y_train)\n\n    plt.figure(figsize=(8,5))\n    plot_decision_regions(X_train.values, y_train.values, clf=tree, legend=2)\n\n    plt.xlabel('sepal length [cm]')\n    plt.ylabel('petal length [cm]')\n    plt.title('Decision Tree on Bank')\n    plt.show()","f4f92772":"**6) Utilizando o dataset datasets\/bank-numeric.csv fa\u00e7a:**\n* 6.1 - Carregue a base de dados e separe os dados e as classes em vari\u00e1veis distintas.\n* 6.2 - Treine um modelo baseado em arvore de decis\u00e3o com seus parametros padr\u00e3o.\n* 6.3 - Utilizando o c\u00f3digo abaixo, fa\u00e7a as altera\u00e7\u00f5es necess\u00e1rias para renderizar de forma gr\u00e1fica a arvore utilizando o modelo treinado no item anterior (6.2).","3bc0cc32":"* Utilize o modelo treinado anteriormente com a base de dados bank-numeric.\n* Fa\u00e7a os ajustes necess\u00e1rios na fun\u00e7\u00e3o como altera\u00e7\u00e3o dos valores das v\u00e1ri\u00e1veis X e y.\n* N\u00e3o esque\u00e7a de importar a biblioteca ml_extend conforme aprendemos em aula.","84750006":"**3) Em compara\u00e7\u00e3o com modelos lineares, quando devemos usar modelos baseados em arvores de decis\u00e3o?**\n\n        Modelos lineares s\u00e3o muito usados para c\u00e1lculos de regress\u00e3o, j\u00e1 \u00e1rvores de decis\u00e3o s\u00e3o normalmente usadas para classifica\u00e7\u00e3o. Modelos lineares tamb\u00e9m s\u00e3o usados quando temos uma grande rela\u00e7\u00e3o entre as features e os alvos, j\u00e1 \u00e1rvores de decis\u00e3o podem ser usados para casos mais complexos.","21ac620b":"* 6.4 - Observou que a arvore ficou muito grande ? Explique como \u00e9 poss\u00edvel reduzir o tamanho da \u00e1rvore. Quais os parametros que influenciam diretamente na profundidade da arvore?\n\n        Podemos definir o min_samples_split  assim como min_samples_leaf\n* 6.5 - Baseado nisso, treine novamente o modelo, dessa vez, aumentando o par\u00e2metro max_depth e renderize novamente a arvore de forma gr\u00e1fica. \n        \n        Abaixo","507b0867":"**2) Explique de forma te\u00f3rica:<br>**\n* 2.1 - Como s\u00e3o geradas as \u00e1rvores de decis\u00e3o?\n\n        A partir de uma feature principal, os dados s\u00e3o divididos de acordo com seus valores que melhor dividem os dados. Divis\u00f5es posteriores s\u00e3o realizadas baseado no mesmo crit\u00e9rio, o que melhor divide os dados de acordo com os dados de treino. M\u00e9tricas como Gini e Entropia s\u00e3o m\u00e9tricas para o c\u00e1lculo de como dividir os dados\n        \n* 2.2 - Quais s\u00e3o as medidas utilizadas para calcular a impureza dos dados?\n        \n        Gini e Entropia\n        \n* 2.3 - Explique como funciona as estrat\u00e9gias para evitar Overfitting baseado em pre-poda e p\u00f3s-poda.\n   \n      Para evitar o overfitting, podemos definir o num\u00e9ro m\u00ednimo de dados para que seja criado uma divis\u00e3o, tamanho m\u00e1ximo da \u00e1rvore assim como o valor m\u00ednimo para que seja criada uma folha.","f34d6a0f":"# Cap\u00edtulo 06 - Machine Learning - Arvore de Decis\u00e3o\n# Data Science do Zero","a0f990f4":"**1) Baseado em seus conhecimentos sobre Arvores de Decis\u00e3o explique suas vantagens e desvantagens.**\n\n    As \u00e1rvores de decis\u00e3o s\u00e3o f\u00e1ceis de entender, n\u00e3o exige tanto pr\u00e9-processamento, pois aceita dados categ\u00f3ricos e num\u00e9ricos (apesar de comumente transformamos os dados). Por\u00e9m, \u00e1rvores muito grandes podem sofrer de overfitting ao ajustar muito os dados.","d94ee28b":"**5) Explique o funcionamento do par\u00e2metro min_samples_split.**\n* Lembre-se da imagem abaixo\n\n        Esse par\u00e2metro controla a menor quantidade de dados necess\u00e1rias para que seja criado uma divis\u00e3o","7d13294b":"*** Visualizando de forma gr\u00e1fica a arvore gerada***","30659afd":"## 6.2","23763550":"## Exerc\u00edcios","311652da":"## visualize_fronteiras(2)","5be0cd93":"## Error cause\n        \n        If you run the cells above as code, you'll get an error. The Decision Tree has lots of features, to creat a graph we most provide filler values,\n        more can be found here: http:\/\/rasbt.github.io\/mlxtend\/user_guide\/plotting\/plot_decision_regions\/","c5080429":"## 6.1","bd171e36":"**4) Explique o funcionamento do par\u00e2metro max_depth.**\n        \n        Define o tamanho m\u00e1ximo da \u00e1rvore","27761f1e":"* 8.1 - Carregue a base de dados bank-numeric. \n\n        J\u00e1 feito\n* 8.2 - Divida os dados em em treino e teste com o percentual de 30% para teste e 70% para treino.\n\n        J\u00e1 feito\n* 8.3 - Utilize a fun\u00e7\u00e3o compara_modelos() conforme o c\u00f3digo abaixo.\n\n        Abaixo\n* 8.4 - Fa\u00e7a as altera\u00e7\u00f5es necess\u00e1rias para utilizar os dados divididos anteriormente.\n    \n        Abaixo\n* 8.5 - Explique porque aumentamos o valor de max_depth tendemos a errar mais para o conjunto de teste e acertar mais no conjunto de treinamento.\n\n        Ao aumentarmos o valor de max_depth, aumentamos a complexidade da \u00e1rvore.\n        A \u00e1rvore ser\u00e1 capaz de capturar mais casos \u00fanicos. Apesar de ser bom para o treino,\n        a \u00e1rvore n\u00e3o conseguir\u00e1 generalizar bem para os dados de teste ou quaisquer dados novos."}}