{"cell_type":{"774833ce":"code","6bb316db":"code","59d88b0e":"code","647227cc":"code","80d7d5e7":"code","462c29f4":"code","d9297cd9":"code","53c7e3b6":"code","ef973d12":"code","28e2c76f":"code","cf58911f":"code","26ef666a":"code","923c9d5b":"code","fc57ad2b":"code","21895f46":"code","92ba6688":"code","02750084":"code","df7e5918":"markdown","09a1dbda":"markdown","ed6e604f":"markdown","a8d7639f":"markdown"},"source":{"774833ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6bb316db":"df = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv\")\n\nprint(\"Total number of docs \\t\\t\\t\\t\\t {}\".format(len(df)))\nprint(\"Number of docs with abstracts null \\t\\t\\t {}\".format(len(df[df['abstract'].isnull()])))\nprint(\"number of documents with no full text \\t\\t\\t {}\".format(len(df[df['has_full_text']==False])))\nprint(\"number of docs with no abstract and no full text \\t {}\".format(len(df[(df['abstract'].isnull()) & (df['has_full_text']==False)])))\nprint(\"Total count based on sha \\t\\t\\t\\t {}\".format(df['sha'].count()))\nprint(\"Number of doc with unique sha \\t\\t\\t\\t {}\".format(len(df['sha'].unique())))\nprint(\"Number of doc's with no sha \\t\\t\\t\\t {}\".format(len(df[df['sha'].isnull()])))","59d88b0e":"df=df[~df['abstract'].isnull()].reset_index(drop=True)\ndf=df[['sha','source_x','title','doi','abstract']]","647227cc":"import numpy as np \nimport pandas as pd\nimport re\nimport os\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nimport gensim\nimport fasttext\nimport fasttext.util\nfrom keras.preprocessing import text\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom nltk.cluster import KMeansClusterer\nfrom sklearn.cluster import DBSCAN\nimport seaborn as sns\nfrom sklearn.manifold import TSNE \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import OPTICS\nimport matplotlib.pyplot as plt","80d7d5e7":"embedding_dim=300\n\nfor i in sorted(os.scandir('..\/input\/fasttext-crawl-300d-2m'), key=lambda x: x.stat().st_size, reverse=True):\n    print(i.path)\n    \nmodel = gensim.models.KeyedVectors.load_word2vec_format(\"\/kaggle\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\")\n","462c29f4":"tokenizer = RegexpTokenizer(r'\\w+')\n#word_token_final_list_st =[]\n\ndef cleantext1(text):\n    lean1Text = re.sub('\\(.*?\\)','',text)\n    month = \"(January|February|March|April|May|June|July|August|September|October|November|December)\"\n    lean1Text = re.sub('[I|i]n\\s'+month+'[of\\s]\\d{4}','',lean1Text) #18 February 2020\n    lean1Text = re.sub('\\d{2,}\\s'+month,'',lean1Text)\n    lean1Text = re.sub('\\d{2,}','',lean1Text)\n    lean1Text = re.sub('BACKGROUND:',' ',lean1Text)\n    lean1Text=re.sub('&ndash;',' days to ',lean1Text)\n    lean1Text=re.sub('mid-',' ',lean1Text)\n    lean1Text=re.sub('\\s'+month+'\\sof\\s\\d{4}',' ',lean1Text)\n    lean1Text = re.sub('(\\s\\d{1,}\\sof\\s'+month+'\\sof\\s\\d{4})', ' ', lean1Text)\n    lean1Text = re.sub('(\\sof\\s'+month+'\\sof\\s\\d{4})', ' ', lean1Text)\n    lean1Text = re.sub('(\\s'+month+'\\sof\\s\\d{4})', ' ', lean1Text)\n    lean1Text = re.sub('r\\$[\\s+]?[\\d+]?[\\.]?[\\,]?[\\d{1,}]?[\\.]?[\\d{1,}]?[\\.]?[\\,]?[\\d{1,}]?',' ',lean1Text)\n    lean1Text = re.sub('-',' ',lean1Text)\n    lean1Text = re.sub('\\(.*?\\)',' ',lean1Text)\n    lean1Text = re.sub('\\s\\d\\.\\s',' ',lean1Text)\n    lean1Text = re.sub('\\s\\w\\.\\s',' ',lean1Text)\n    lean1Text = re.sub('(http[s]?...\\w*.?\\w*\\.?\\w*.*?\\s)',' ',lean1Text)\n    lean1Text= re.sub('\\.\\.+' , '.',lean1Text)\n    lean1Text= re.sub('\\.+', '.', lean1Text)\n    lean1Text= re.sub('\\s\\.+','.',lean1Text)\n    lean1Text=re.sub('\\s\\.','.',lean1Text)\n    lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n    lean1Text=re.sub('\\s{2,}',' ',lean1Text)\n    lean1Text=re.sub('\\.{2,}',' ',lean1Text)\n    lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n    lean1Text=re.sub('\\.\\s\\.','.',lean1Text)\n    lean1Text=re.sub('&ndash;',' days to ',lean1Text)\n    return lean1Text.strip()\n","d9297cd9":"df['cleaned_tex']=[cleantext1(x) for x in df['abstract'].tolist()]\ntrain,test= train_test_split(df, test_size=0.20, random_state=42)","53c7e3b6":"tokenizer= text.Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train['cleaned_tex'].tolist())\ntrain_word_index = tokenizer.word_index\ntrain_embedding_weights = np.zeros((len(train_word_index)+1, embedding_dim))\nele_to_pop=[]\nfor word,index in train_word_index.items():\n    if word in model:\n        train_embedding_weights[index,:]=model[word]\n    else:\n        train_embedding_weights[index,:]= np.zeros(embedding_dim)\n        ele_to_pop.append(word)\ntrain_embedding_weights= train_embedding_weights[~np.all(train_embedding_weights==0.0,axis=1)]\n\nfor name,key in list(train_word_index.items()):\n    if name in ele_to_pop:\n        del train_word_index[name]","ef973d12":"def ml_unsupervised(method,feature_matrix,numb_cluster=2):\n    if method=='kmeans_scikit':\n        km = KMeans(n_clusters=numb_cluster,max_iter=100)\n        km.fit(feature_matrix)\n        clusters = km.labels_\n        return km, clusters\n\n    if method=='optics':\n        db=OPTICS(min_samples=10)\n        db.fit(feature_matrix)\n        clusters= db.labels_\n        return db,clusters\n\ndef tsne_reduction(feature_matrix, convert_to_array='yes'):\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    if convert_to_array=='yes':\n        feature_matrix = tsne.fit_transform(feature_matrix.toarray())\n    else:\n        feature_matrix =  tsne.fit_transform(feature_matrix)\n    return feature_matrix\n\ndef pca_reduction(feature_matrix,convert_to_array='yes'):\n    pca = PCA(n_components=2, random_state=0)\n    if convert_to_array=='yes':  \n        feature_matrix = pca.fit_transform(feature_matrix.toarray())\n    else:\n        feature_matrix = pca.fit_transform(feature_matrix)\n    return feature_matrix","28e2c76f":"tsne_reduced_embedding_weights= tsne_reduction(train_embedding_weights,convert_to_array='no')\npca_reduced_embedding_weights= pca_reduction(train_embedding_weights,convert_to_array='no')","cf58911f":"from sklearn.metrics import silhouette_score\nfor i in range(2,7):\n    k_model,k_cluster= ml_unsupervised('kmeans_scikit',tsne_reduced_embedding_weights,numb_cluster=i)\n    sil_coeff = silhouette_score(tsne_reduced_embedding_weights, k_cluster, metric='euclidean')\n    print(sil_coeff)","26ef666a":"from sklearn.metrics import silhouette_score\nfor i in range(2,7):\n    k_model,k_cluster= ml_unsupervised('kmeans_scikit',pca_reduced_embedding_weights,numb_cluster=i)\n    sil_coeff = silhouette_score(pca_reduced_embedding_weights, k_cluster, metric='euclidean')\n    print(sil_coeff)","923c9d5b":"kmeans_scan_obj,kmemans_clusters = ml_unsupervised('kmeans_scikit',pca_reduced_embedding_weights)\nkmeans_sc_ts_obj,kmemans_sc_ts_clusters = ml_unsupervised('kmeans_scikit',tsne_reduced_embedding_weights)","fc57ad2b":"df['kmeans_scikit(pca)_cluster']=pd.Series(kmemans_clusters)\ndf['kmeans_scikit(tsne)_cluster']=pd.Series(kmemans_sc_ts_clusters)\ndf['x']=pd.Series(pca_reduced_embedding_weights[:,0])\ndf['y']=pd.Series(pca_reduced_embedding_weights[:,1])\ndf['x_n']=pd.Series(tsne_reduced_embedding_weights[:,0])\ndf['y_n']=pd.Series(tsne_reduced_embedding_weights[:,1])","21895f46":"import seaborn as sns\nf,axes = plt.subplots(2,1,figsize=(12,12),sharex=False,sharey=False,squeeze=False)\nsns.set()\ncenters1=np.array(kmeans_scan_obj.cluster_centers_)\ncenters2=np.array(kmeans_sc_ts_obj.cluster_centers_)\nsns.scatterplot(data=df,x=df['x'],y=df['y'],hue=df['kmeans_scikit(pca)_cluster'],ax=axes[0,0])\nsns.scatterplot(centers1[:,0], centers1[:,1], marker=\"x\", color='black',ax=axes[0,0])\nsns.scatterplot(data=df,x=df['x_n'],y=df['y_n'],hue=df['kmeans_scikit(tsne)_cluster'],ax=axes[1,0])\nsns.scatterplot(centers2[:,0], centers2[:,1], marker=\"x\", color='black',ax=axes[1,0])\nplt.show()","92ba6688":"print('By using the Kmeans-pca dimensionality reduction')\nprint('Number of abstracts in cluster 0 \\t {}'.format(len(df[df['kmeans_scikit(pca)_cluster']==0])))\nprint('Number of abstracts in cluster 1 \\t {}'.format(len(df[df['kmeans_scikit(pca)_cluster']==1])))\nprint('\\nBy using the Kmeans-tsne dimensionality reduction')\nprint('Number of abstracts in cluster 0 \\t {}'.format(len(df[df['kmeans_scikit(tsne)_cluster']==0])))\nprint('Number of abstracts in cluster 0 \\t {}'.format(len(df[df['kmeans_scikit(tsne)_cluster']==1])))","02750084":"# db=OPTICS(min_samples=50)\n# db.fit(tsne_reduced_embedding_weights)\n# clusters= db.labels_\n# d=db_clusters.tolist()\n# print(list(set(d)))","df7e5918":"* **Loading data**\n> * **Data understanding**\n> > *** for the particular motive of performing clustering**","09a1dbda":"**Intention:** \n> To check the kmean's clustering algorithm effeciency by inputing derived features from facebooks fasttext word embedding to cluster the abstracts.\n* > I here have used pca as well as tsne dimensionality reduction to check how effeciently the kmeans algorithm works on them to cluster\n> \n**Result:**\n> Although applying kmeans on tsne reduced features almost gave 53% of documents being classified in one cluster leaving the rest in another cluster, the PCA reduced feature gave 26% document in one cluster which i believe that this 26% can be worthy for which expert advice by domain expert is needed. Clearly starting from the cluster with less document brings conclusion of if the algorithm effeciency is worthy.\n\n>since the scikit also offers kmeans library and nltk as well, i decided to see which one gives better silhouette score. From this result, i decided the best library as well as the best \"number of cluster\" parameter\n\n>","ed6e604f":"### **Plot**","a8d7639f":"* **Preparing data for Clustering **\n> Since my first motive is to see if clustering documents based on abstracts works, i am remove the documents without any abstract.\n> The documents with no sha, contains text and abstracts that might be of some importance, so they are kept!"}}