{"cell_type":{"ef8a17a8":"code","41d45189":"code","7fac00bd":"code","9e4c9676":"code","31d0ae7d":"code","44881763":"code","acd36254":"code","a32cc581":"code","f25a9102":"code","02350204":"code","3454e5a7":"code","0c392945":"code","ad832ecb":"code","feed84c8":"code","27b57b5b":"code","0fc0cd3b":"code","ce5d0189":"code","960fc05e":"code","0975e18b":"code","eceece6f":"code","e600810b":"code","481a22ff":"code","887c3379":"code","fd2e2683":"code","1922735d":"code","507e98e2":"code","876c0213":"code","375ae793":"code","8da4d997":"code","a8152c4c":"code","5b3ded74":"code","6ff0a27c":"code","9807078b":"code","3870007a":"code","3d834b8b":"code","9da6d267":"code","7f18df42":"code","28a6176a":"code","13941f91":"code","34f6d2de":"code","aa7edc8e":"code","bf12b817":"code","eba70012":"code","78493bf2":"code","788a0c93":"markdown","49398c82":"markdown","ff5efe00":"markdown","435bb27d":"markdown","a43a16ae":"markdown","3fcd3985":"markdown","7db2d892":"markdown","a62d9f4a":"markdown","f0a30908":"markdown","7fa41292":"markdown","9dfb1b17":"markdown","a4ccabd5":"markdown","813ac346":"markdown","4d1f6baa":"markdown","f96ba6c9":"markdown","927b4da0":"markdown","d70c8f7b":"markdown","2152d33e":"markdown","7843d4e6":"markdown","f9766da3":"markdown"},"source":{"ef8a17a8":"# import \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_regression\nimport statsmodels.api as sm\nfrom statsmodels.graphics import utils\nfrom geopy.geocoders import Nominatim\nfrom geopy.exc import GeocoderTimedOut\nfrom itertools import combinations\nfrom sklearn.model_selection import train_test_split\nimport scipy as sp\nimport datetime\nimport sys\n\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().run_line_magic('matplotlib', 'inline')","41d45189":"# data load\ndf_train = pd.read_csv('..\/input\/train.csv')","7fac00bd":"df_train.shape","9e4c9676":"# data preprocessing\ntrain_ID = df_train['id']\n\nXvar = list(df_train.columns[3:])\nXvar.insert(0,'date')\nXvar.insert(0,'price')\n\ndf_train = pd.DataFrame(df_train, columns=Xvar) # \uadf8..\uadf8\ub0e5 \uce7c\ub7fc \uc21c\uc11c \ubc14\uafb8\uae30","31d0ae7d":"# simplify date variable\ndf_train['date'] = df_train['date'].apply(lambda x:x[:6]).astype(int)\ndf_train['date'] = df_train['date'].apply(lambda x: x - 201500 + 8 if x > 201500 else x - 201404)","44881763":"fig = plt.figure(figsize = (12,6))\n\nfig.add_subplot(1,2,1)\nres = sns.distplot(df_train['price'])\n\nfig.add_subplot(1,2,2)\nres = sns.distplot(np.log1p(df_train['price']))","acd36254":"# log transformation for target variable (for normalization)\ndf_train['price'] = np.log1p(df_train['price'])","a32cc581":"price_corr = df_train.corrwith(df_train['price'])\npd.DataFrame(price_corr, columns=['corr']).sort_values(['corr'],ascending=False).transpose()","f25a9102":"########### Check outliars\n\n## Cook's Distance\n# if D > 4\/(N-K-1) then outliar\nvar = Xvar[1:]\ndata = pd.concat([df_train['price'], df_train[var]], axis=1)\nmodel = sm.OLS(data['price'],data[var])\nresult = model.fit()\nsm.graphics.plot_leverage_resid2(result)\nplt.show()\n\n# \na = pd.merge(df_train.iloc[[1231,8756,8912]],\n             pd.DataFrame(df_train.mean(axis=0)).transpose(),\n             how='outer')\n# 1231: sqft_lot, view, sqft_lot15 => delete\n# 8756: bed\/bathroom, sqft_living, sqft_lot, sqft_above, ... -> Including!\n# 8912: bedrooms, bathrooms, sqft_living, sqft_lot, floors, view, grade, ... => delete\n\n\nvar = 'sqft_lot'\ndata = pd.concat([df_train['price'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(6, 4.5))\nfig = sns.regplot(x=var, y=\"price\", data=data)\n\n## check outliars of locations\nfig, axes = plt.subplots(figsize=(5,4))\nsns.regplot(data=df_train, x='long', y='lat',\n           fit_reg=False,\n           scatter_kws={'s': 15},\n           ax=axes)\naxes.grid(False)\naxes.set_title('Location of train data', fontsize=15)\nplt.show()","02350204":"# remove outliars\ndf_train = df_train.drop([1231,8912],0)","3454e5a7":"# scatterplot of input variables\nsns.set()\ncols = ['price', 'grade', 'sqft_living', 'sqft_living15', 'sqft_above', 'sqft_basement']\nsns.pairplot(df_train[cols], size = 2)\nplt.show()","0c392945":"# log transformation for input variables (normalization)\nskew_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15']\nfor c in skew_columns:\n    df_train[c] = np.log1p(df_train[c].values)","ad832ecb":"## variables\n# \uc9c0\ud558\uc2e4 \uc720\ubb34\ndf_train['is_basement'] = df_train['sqft_basement'].apply(lambda x: 0 if x == 0 else 1)\n# \uc7ac\uac74\ucd95 \uc5ec\ubd80\ndf_train['is_renovated'] = df_train['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\n# \uc7ac\uac74\ucd95\ub144\ub3c4\ndf_train['yr_renovated'] = df_train['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)\ndf_train['yr_renovated'] = df_train['yr_renovated'].fillna(df_train['yr_built'])","feed84c8":"## zipcode price \ud3c9\uade0\n# \ud3c9\ub2f9 \uac00\uaca9 : \uac00\uaca9\/\uc804\uccb4 \uc9d1 \ud3c9\uc218\ndf_train['per_price'] = df_train['price']\/(df_train['sqft_above'] + df_train['sqft_basement'])\n# zipcode \ub2f9 \ud3c9\ub2f9 \uac00\uaca9\uc758 \ud3c9\uade0, \ubd84\uc0b0\nzipcode_price = df_train.groupby(['zipcode'])['per_price'].agg({'mean','var'}).reset_index()\nzipcode_price.sort_values(['mean'], ascending=False)\ndf_train = pd.merge(df_train,zipcode_price,how='left',on='zipcode')\ndf_train['zipcode_mean'] = df_train['mean'] * (df_train['sqft_above'] + df_train['sqft_basement'])\ndf_train['zipcode_var'] = df_train['var'] * (df_train['sqft_above'] + df_train['sqft_basement'])\ndel df_train['mean']\ndel df_train['var']\ndel df_train['per_price']","27b57b5b":"def progressBar(value, endvalue, bar_length=20):\n    percent = float(value) \/ endvalue\n    arrow = '-' * int(round(percent * bar_length) - 1) + '>'\n    spaces = ' ' * (bar_length - len(arrow))\n\n    sys.stdout.write(\"\\rPercent: [{0}] {1}%\".format(arrow + spaces, int(round(percent * 100))))\n    sys.stdout.flush()\n\n# find location\ngeolocator = Nominatim()\ndef do_geocode(x):\n    try:\n        return geolocator.reverse(x)\n    except : #GeocoderTimedOut\n        return do_geocode(x)\n    \nstart = datetime.datetime.now()\nlocation = [None for i in range(df_train.shape[0])]\nfor i in range(df_train.shape[0]):\n    address = do_geocode(str(df_train['lat'][i])+','+str(df_train['long'][i]))\n    location[i] = address.address.find('Seattle')\n    progressBar(i, df_train.shape[0])\n\nis_seattle = pd.Series(location).apply(lambda x: 0 if x == -1 else 1)\ndf_train['is_seattle'] = pd.Series(is_seattle)\n\nend = datetime.datetime.now()\nprint(\"\\n\uac78\ub9b0\uc2dc\uac04 : \", end - start)","0fc0cd3b":"## Additive model\n'''# cross variables\nXvar_con = ['date', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors', 'sqft_above', #'sqft_basement'\n            'yr_built', 'yr_renovated', 'sqft_living15', 'sqft_lot15', 'zipcode_mean', 'zipcode_var', 'total_score']\ncross_var = list(combinations(Xvar_con, 2))\n# add\nfor var_idx in Xvar[1:]:\n    v1=var_idx+'_s2'; v2=var_idx+'_s3'; v3=var_idx+'_sq'; v4=var_idx+'_log'\n    df_train[v1] = df_train[var_idx]**2\n    df_train[v2] = df_train[var_idx]**3\n    df_train[v3] = np.sqrt(df_train[var_idx])\n    df_train[v4] = np.log1p(df_train[var_idx])\n\nfor var_idx in cross_var:\n    var_nm = var_idx[0]+'_'+var_idx[1]\n    df_train[var_nm] = df_train[var_idx[0]]*df_train[var_idx[1]]'''","ce5d0189":"df_train['total_score'] = (df_train['view']+1)*df_train['condition']*df_train['grade']\ndf_train['total_sqft_living'] = df_train['sqft_living']*df_train['sqft_living15']","960fc05e":"# temporary save\n#df_train.to_csv('..\/input\/train_revised.csv',index=False) #\ubcc0\uc218 \uc0dd\uc131\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub824\uc11c \uc784\uc2dc\ub85c \uc800\uc7a5\ud569\ub2c8\ub2e4...","0975e18b":"price_corr = df_train.corrwith(df_train['price'])\npd.DataFrame(price_corr, columns=['corr']).sort_values(['corr'],ascending=False).transpose()","eceece6f":"# Scaler\ndef minmaxscaler(x):\n    return (x-x.min())\/(x.max()-x.min())\n\ntrain = pd.DataFrame()\nfor var_idx in df_train.columns[1:]:\n    train[var_idx] = minmaxscaler(df_train[var_idx])","e600810b":"train.head()","481a22ff":"y = df_train['price']\n\n# train, test set\nX_train, X_test, y_train, y_test = train_test_split(train, y, test_size = 0.3, random_state = 42)\n\nX_train.head()","887c3379":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, BayesianRidge, ElasticNet, Lasso\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\nimport xgboost as xgb\nimport lightgbm as lgb","fd2e2683":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)\n\n#Validation function\nn_folds = 10\n\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","1922735d":"# 1* Ridge\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11, xmax = 16, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11, 16], [11, 16], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","507e98e2":"# 2* Lasso\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11, xmax = 16, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11, 16], [11, 16], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","876c0213":"# 4* ElasticNet\nelasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )\n\nprint(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_ela, y_train_ela - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_ela, y_test_ela - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 11, xmax = 16, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train, y_train_ela, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test, y_test_ela, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([11, 16], [11, 16], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(elasticNet.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()","375ae793":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0001, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0001, l1_ratio=0.085, random_state=3))\nKRR = KernelRidge(alpha=0.006, kernel='polynomial', degree=2, coef0=2.5)\nBRidge = BayesianRidge(n_iter=3000, tol=0.001, \n                       alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, \n                       compute_score=True, fit_intercept=True, normalize=False, copy_X=True, verbose=False)","8da4d997":"from keras import models\nfrom keras import layers\n\ndef build_model():\n    model = models.Sequential()\n    model.add(layers.Dense(64, activation='relu', input_shape=(train.shape[1],)))\n    model.add(layers.Dense(64, activation='relu'))\n    model.add(layers.Dense(1))  # activation \ud568\uc218 \uc5c6\uc74c. \uc784\uc758\uc758 \uac12 (\uc9d1 \uac12)\uc744 \ucd9c\ub825\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])   # mse: mean squared error , regression\uc744 \uc704\ud574 \ud754\ud788 \uc0ac\uc6a9\ud558\ub294 loss \ud568\uc218\n    return model\n\nkeras_model = build_model()","a8152c4c":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =40)\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.0001, eval_metric='rmse',\n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.0001, reg_lambda=0.8571,\n                             subsample=0.6, silent=1,\n                             random_state =41, nthread = -1)\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=30,boosting='gbdt',metric='rmse',\n                              learning_rate=0.01, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.9,\n                              bagging_freq = 5, feature_fraction = 0.8,\n                              feature_fraction_seed=9, bagging_seed=42,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 30)\nmodel_ada = AdaBoostRegressor(n_estimators=500, learning_rate=0.15)","5b3ded74":"pipe_xgb = Pipeline([('scl', StandardScaler()), ('xgb', xgb.XGBRegressor(random_state=42))])\n\n#param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nparam_grid = [{\n    'n_estimators': [1000, 2000, 3000],\n    'min_child_weight': [4,5], \n    'gamma': [i\/10.0 for i in range(1,10)],  \n    'subsample': [i\/10.0 for i in range(6,11)],\n    'colsample_bytree': [i\/10.0 for i in range(6,11)], \n    'max_depth': [2,3,4,5],\n}]\n\ngs = GridSearchCV(estimator=pipe_xgb, param_grid=param_grid,\n                  scoring='accuracy', cv=10, n_jobs=1)\n%time gs = gs.fit(train, y)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","6ff0a27c":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","9807078b":"averaged_models = AveragingModels(models = (GBoost, ENet, KRR, BRidge))\n#score = rmse_cv(averaged_models_reg)\n#print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3870007a":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","3d834b8b":"stacked_averaged_models = \\\nStackingAveragedModels(base_models = (averaged_models, model_xgb, model_lgb, model_ada), meta_model = lasso)\n\nscore = rmse_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","9da6d267":"# test data load\ndf_test = pd.read_csv('data\/test.csv')\n\n# data preprocessing\ntest_ID = df_test['id']\ndf_test['date'] = df_test['date'].apply(lambda x:x[:6]).astype(int)\ndf_test['date'] = df_test['date'].apply(lambda x: x - 201500 + 8 if x > 201500 else x - 201404)\n\n# remove outliars\n\n# log transformation for variables\nskew_columns = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement','sqft_living15', 'sqft_lot15']\nfor c in skew_columns:\n    df_test[c] = np.log1p(df_test[c].values)\n\n## variables\n# \uc9c0\ud558\uc2e4 \uc720\ubb34\ndf_test['is_basement'] = df_test['sqft_basement'].apply(lambda x: 0 if x == 0 else 1)\n# \uc7ac\uac74\ucd95 \uc5ec\ubd80\ndf_test['is_renovated'] = df_test['yr_renovated'].apply(lambda x: 0 if x == 0 else 1)\n# \uc7ac\uac74\ucd95\ub144\ub3c4\ndf_test['yr_renovated'] = df_test['yr_renovated'].apply(lambda x: np.nan if x == 0 else x)\ndf_test['yr_renovated'] = df_test['yr_renovated'].fillna(df_train['yr_built'])\n# zipcode price \ud3c9\uade0\uacfc \ubd84\uc0b0\ndf_test = pd.merge(df_test,zipcode_price,how='left',on='zipcode')\ndf_test['zipcode_mean'] = df_test['mean'] * (df_test['sqft_above'] + df_test['sqft_basement'])\ndf_test['zipcode_var'] = df_test['var'] * (df_test['sqft_above'] + df_test['sqft_basement'])\ndel df_test['mean']\ndel df_test['var']\n\n# \uc704\uce58 \uae30\ubc18\ngeolocator = Nominatim()\ndef do_geocode(x):\n    try:\n        return geolocator.reverse(x)\n    except : #GeocoderTimedOut\n        return do_geocode(x)\n\nstart = datetime.datetime.now()\nlocation = [None for i in range(df_test.shape[0])]\nfor i in range(df_test.shape[0]):\n    address = do_geocode(str(df_test['lat'][i])+','+str(df_test['long'][i]))\n    location[i] = address.address.find('Seattle')\n    progressBar(i, df_test.shape[0])\n    \nis_seattle = pd.Series(location).apply(lambda x: 0 if x == -1 else 1)\ndf_test['is_seattle'] = pd.Series(is_seattle)\nend = datetime.datetime.now()\nprint(\"\\n\uac78\ub9b0\uc2dc\uac04 : \", end - start)\n\n# interaction\ndf_test['total_score'] = (df_test['view']+1)*df_test['condition']*df_test['grade']\ndf_test['total_sqft_living'] = df_test['sqft_living']*df_test['sqft_living15']\n    \n# minmaxscale\ntest = pd.DataFrame(df_test['id'])\nfor var_idx in df_test.columns[1:]:\n    test[var_idx] = minmaxscaler(df_test[var_idx])\n    \ndf_test.to_csv(\"test_revised.csv\", index=False)","7f18df42":"df_test = df_test.join(pd.get_dummies(df_test['zipcode'], prefix='zipcode', drop_first = True))\ndel df_test['zipcode']\nprint(df_test.shape)","28a6176a":"test_ID = df_test['id']\ntest = pd.DataFrame()\nfor var_idx in df_test.columns[1:]:\n    test[var_idx] = minmaxscaler(df_test[var_idx])","13941f91":"test.head()","34f6d2de":"# averaged model\naveraged_models.fit(X_train.values, y_train)\naveraged_train_pred = averaged_models.predict(X_train.values)\naveraged_pred = np.expm1(averaged_models.predict(test.values))","aa7edc8e":"# averaged model\nstacked_averaged_models.fit(X_train.values, y_train)\nstacked_averaged_train_pred = stacked_averaged_models.predict(X_train.values)\nstacked_averaged_pred = np.expm1(stacked_averaged_models.predict(test.values))","bf12b817":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# XGBoost\nmodel_xgb.fit(X_train, y_train)\nxgb_train_pred = model_xgb.predict(X_train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmse(y_train, xgb_train_pred))\n\n# LightGBM\nmodel_lgb.fit(X_train, y_train)\nlgb_train_pred = model_lgb.predict(X_train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmse(y_train, lgb_train_pred))","eba70012":"## Ensemble prediction\nensemble = averaged_pred*0.50 + xgb_pred*0.25 + keras_model*0.10","78493bf2":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['price'] = ensemble\nsub.to_csv('submission.csv',index=False)","788a0c93":"## keras regression","49398c82":"Cook's distance\ub97c \uc774\uc6a9\ud558\uc5ec outliars\ub97c \uc81c\uac70\ud558\ub824 \ud588\uc73c\ub098 \ub108\ubb34 \ub9ce\uc740 \ub370\uc774\ud130\uac00 \uc9c0\uc6cc\uc838, \uac00\uc7a5 \ub208\uc5d0 \ub744\ub294 \ub450 \ub370\uc774\ud130\ub9cc \uc81c\uac70\ud558\ub824 \ud569\ub2c8\ub2e4.","ff5efe00":"## Hyperparameter tuning","435bb27d":"Seattle(\ub3c4\uc2dc)\uc774\uba74 \uac00\uaca9\uc774 \ub192\uc744 \uac83 \uac19\uc544 Seattle\uc5ec\ubd80\uc5d0 \uad00\ub828\ud55c \ubcc0\uc218\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\n(\ud558\uc9c0\ub9cc Seattle\uc774 \uc544\ub2cc \uadf8 \uc606\uc758 \uc791\uc740 \uad6c\uc5ed\uc774 \uac00\uc7a5 \ube44\uc2fc \uc9d1\uac12\uc744 \uac00\uc9c0\uc8e0..)","a43a16ae":"## Stacking\n\ucc38\uace0 \ucee4\ub110: https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","3fcd3985":"### EDA\n\uc5b4\ub290\uc815\ub3c4\uc758 EDA\ub294 skip \ud569\ub2c8\ub2e4..","7db2d892":"price\ub97c \uc81c\uc678\ud55c \ubaa8\ub4e0 \ubcc0\uc218\uc5d0 minmax scaling\uc744 \ud569\ub2c8\ub2e4.","a62d9f4a":"\ub2e4\uc74c\uacfc \uac19\uc740 \ud30c\uc0dd\ubcc0\uc218\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \uc774\ub294 Hyun woo Kim \ub2d8\uc758 \ucee4\ub110\uc744 \ub9ce\uc774 \ucc38\uace0\ud558\uc600\uc2b5\ub2c8\ub2e4! \u3160\u3160 \uc815\ub9d0 \uac10\uc0ac\ud569\ub2c8\ub2e4!\n\nhttps:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12","f0a30908":"\uc5ec\ub7ec \uad50\ud638\uc791\uc6a9\uc744 \uace0\ub824\ud55c \uacb0\uacfc \uc544\ub798\uc758 \ub450 \ubcc0\uc218\ub9cc\uc744 \uace0\ub824\ud569\ub2c8\ub2e4.\nview \ubcc0\uc218\ub294 0 \uad00\uce21\uce58\uac00 \ub9ce\uc544 1\uc744 \ub354\ud558\uc5ec \ub098\uba38\uc9c0 \ubcc0\uc218\uc640 \uacf1\ud569\ub2c8\ub2e4.","7fa41292":"## Kaggle Kernel\nby euphoria\n\n\ub9ce\uc740 \ubd84\ub4e4\uc758 \ub3c4\uc6c0 \ub355\ud0dd\uc5d0 \uccab \ucee4\ub110\uc774\ub124\uc694..! XD","9dfb1b17":"## Test","a4ccabd5":"(\uc774\ub807\uac8c \uc5f4\uc2ec\ud788 seattle \ubcc0\uc218\ub97c \ub9cc\ub4e4\uc5c8\uc9c0\ub9cc \uac00\uc7a5 \ube44\uc2fc \uacf3\uc740 seattle\uc774 \uc544\ub2c8\uc5c8\uc8e0..)\n\n+) \ub9cc\ub4dc\ub824\uace0 \ud588\uc73c\ub098 \uce90\uae00 \ucee4\ub110\uc5d0\uc11c \uacc4\uc18d \uc624\ub958\uac00 \ub098\uace0.. \ud06c\uac8c \uc758\ubbf8\uac00 \uc5c6\uc5b4\uc11c \ucee4\ub110\uc5d0\uc120 \ub9cc\ub4e4\uc9c0 \uc54a\ub294\ub2e4","813ac346":"### Regression - Regularization\n\ucc38\uace0 \ucee4\ub110: https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset","4d1f6baa":"## Data Preprocessing","f96ba6c9":"## Data Load","927b4da0":"\ub208\uc5d0 \ub744\uac8c \ubd84\ud3ec\uac00 \uce58\uc6b0\uce5c \ubcc0\uc218\ub4e4\uc740 \ub85c\uadf8 \ubcc0\ud658\uc744 \ucde8\ud569\ub2c8\ub2e4.","d70c8f7b":"## Boosting","2152d33e":"## Modeling","7843d4e6":"## Submission","f9766da3":"| Variable | Description | Variable | Description | Variable | Description |\n|---|:---|---|:---|---|:---|\n| ID          | \uc9d1\uc744 \uad6c\ubd84\ud558\ub294 \ubc88\ud638     |  floors       | \uc9d1\uc758 \uce35 \uc218 | yr_built | \uc9d1\uc744 \uc9c0\uc740 \ub144\ub3c4 |\n| date        | \uc9d1\uc744 \uad6c\ub9e4\ud55c \ub0a0\uc9dc       |  waterfront   | \uc9d1\uc758 \uc804\ubc29\uc5d0 \uac15\uc774 \ud750\ub974\ub294\uc9c0 \uc720\ubb34 | yr_renovated | \uc9d1\uc744 \uc7ac\uac74\ucd95\ud55c \ub144\ub3c4(0\uc740 \uc7ac\uac74\ucd95X) |\n| **price**   | ***Target*  \uc9d1 \uac00\uaca9** |  view         | \uc9d1\uc774 \uc5bc\ub9c8\ub098 \uc88b\uc544 \ubcf4\uc774\ub294\uc9c0\uc758 \uc815\ub3c4 | zipcode | \uc6b0\ud3b8\ubc88\ud638 |\n| bedrooms    | \uce68\uc2e4\uc758 \uc218              |  condition    | \uc9d1\uc758 \uc804\ubc18\uc801\uc778 \uc0c1\ud0dc | lat | \uc704\ub3c4 |\n| bathrooms   | \ud654\uc7a5\uc2e4 \uc218              | grade         | \uc9d1\uc758 \ub4f1\uae09* | long | \uacbd\ub3c4 |\n| sqft_living | \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8  | sqft_above    | \uc9c0\ud558\uc2e4\uc744 \uc81c\uc678\ud55c \ud3c9\ubc29 \ud53c\ud2b8 | sqft_living15 | 2015\ub144 \uae30\uc900 \uc8fc\uac70 \uacf5\uac04\uc758 \ud3c9\ubc29 \ud53c\ud2b8** |\n| sqft_lot    | \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8       | sqft_basement | \uc9c0\ud558\uc2e4\uc758 \ud3c9\ubc29 \ud53c\ud2b8 | sqft_lot15 | 2015\ub144 \uae30\uc900 \ubd80\uc9c0\uc758 \ud3c9\ubc29 \ud53c\ud2b8** |\n\n<center>* : King County grading \uae30\uc900\n    \n<center>** : \uc9d1\uc744 \uc7ac\uac74\ucd95\ud588\ub2e4\uba74, \ubcc0\ud654\uac00 \uc788\uc744 \uc218 \uc788\uc74c(\uadf8\ub7f0\ub370 \uc7ac\uac74\ucd95\ud55c \ub0a0 \ucd5c\ub300\uac00 2015\ub144\uc784)"}}