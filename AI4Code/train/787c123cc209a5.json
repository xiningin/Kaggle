{"cell_type":{"0b773378":"code","004d3bc6":"code","03f1e69d":"code","819b4910":"code","b50a0ac7":"code","c640f0be":"code","5e15f2f1":"code","73bf9538":"code","6947e5d7":"code","b319cdd6":"code","26f5d5cb":"code","ef30f56b":"code","69335011":"code","f5857d84":"code","ed613827":"code","a2bccfa1":"code","9eba7383":"code","a839ccb9":"code","d2a06c34":"code","b8bc53f8":"code","da9d1b79":"code","bfb30bea":"code","2dbf19ba":"code","5d57e353":"markdown","f9c63fa6":"markdown","abd7a0c4":"markdown","f9fea5bb":"markdown","9cf0c40a":"markdown","20f187e4":"markdown","ab9d9214":"markdown"},"source":{"0b773378":"# task link: https:\/\/www.kaggle.com\/c\/nlp-getting-started","004d3bc6":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\ntrain.head()\nlen(train)","03f1e69d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train['text'], train['target'], random_state=2)","819b4910":"len(X_train)","b50a0ac7":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english')\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","c640f0be":"y_test.head()","5e15f2f1":"print(len(y_test))\nprint(len(y_test.loc[y_test==1]),len(y_test.loc[y_test==1])\/len(y_test))\nprint(len(y_test.loc[y_test==0]),len(y_test.loc[y_test==0])\/len(y_test))\n","73bf9538":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True, stop_words='english')\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","6947e5d7":"print(len(X_test_cv.toarray()[0]))","b319cdd6":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions = naive_bayes.predict(X_test_cv)","26f5d5cb":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\nprint('Precision score: ', precision_score(y_test, predictions))\nprint('Recall score: ', recall_score(y_test, predictions))","ef30f56b":"from sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import train_test_split\nimport re\n\n\nX_train, X_test, y_train, y_test = train_test_split(train['text'], train['target'], random_state=1)\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\nX_train = [clean_text(x) for x in X_train]\nX_test = [clean_text(x) for x in X_test]\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'#?[a-z]+\\b', lowercase=True, stop_words='english')\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[a-z]+\\b)|(?:[@])|(?:#\\w+)', lowercase=True, stop_words='english')\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[a-z]+\\b)|(?:#\\w+)', lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[a-z]+\\b)', lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#@]+\\b)', lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#!@]+\\b)',analyzer='word',max_df=0.1, lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[a-z#!@]+\\b)',analyzer='word',max_df=0.1,max_features=3500, lowercase=True)\ncv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#!@]+\\b)',analyzer='word',max_df=0.1,max_features=3500, lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#!@]+\\b)',analyzer='word', lowercase=True)\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#]+\\b)',analyzer='word',max_df=0.1, lowercase=True)\n\n# cv = CountVectorizer(strip_accents='ascii', token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', lowercase=True)\n\nX_train_cv = cv.fit_transform(X_train)\nprint(X_train_cv.shape)\nX_test_cv = cv.transform(X_test)\n# print(len(cv.get_feature_names()))\n# print(cv.get_feature_names())","69335011":"\nfrom sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions = naive_bayes.predict(X_test_cv)\npredictions_train = naive_bayes.predict(X_train_cv)\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\nprint('Accuracy score(train): ', accuracy_score(y_train, predictions_train))\n\n","f5857d84":"# show some data samples\ncount = 0\nfor x,pred,y in zip(X_test,predictions,y_test):\n    count+=1\n    if pred!=y:\n        print(count,x,pred,y)\n        print('===')","ed613827":"# for submission\n\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col='id')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\n\n\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#!@]+\\b)',analyzer='word',max_df=0.1,max_features=3500, lowercase=True)\n\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\nX_train = [clean_text(x) for x in train['text']]\nX_test = [clean_text(x) for x in test['text']]\n\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)\n\nfrom sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, train['target'])\npredictions = naive_bayes.predict(X_test_cv)\n","a2bccfa1":"predictions[:10]\ntest['target'] = predictions\n\nsubmission_df = test[['target']]\n# submission_df.head()\nsubmission_df.to_csv('result_7_b.csv')","9eba7383":"X_train, X_test, y_train, y_test = train_test_split(train['text'], train['target'], random_state=1)\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\nX_train = [clean_text(x) for x in X_train]\nX_test = [clean_text(x) for x in X_test]\ncv = CountVectorizer(strip_accents='ascii', token_pattern=r'(?:\\b[\\w#!@]+\\b)',analyzer='word',max_df=0.1,max_features=3500, lowercase=True)\n\nX_train_cv = cv.fit_transform(X_train)\nprint(X_train_cv.shape)\nX_test_cv = cv.transform(X_test)\n\nfrom xgboost import XGBClassifier\n\nxgcls = XGBClassifier()\n\nxgcls.fit(X_train_cv, y_train)\npredictions = xgcls.predict(X_test_cv)\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))","a839ccb9":"\"\"\"\na simple bert based on huggingface and implemented on my own. \nthis classifier has been modified in a rush from my NER extractor, sorry if some code looks bad :(\n\"\"\"\nimport transformers\nimport torch\nfrom transformers import BertForSequenceClassification, AdamW, BertTokenizer, get_linear_schedule_with_warmup\nfrom tqdm.notebook import trange as tnrange\n\nclass BertClassifier:\n    def __init__(self,args):\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n        self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=len(args.tags))\n        self.args = args\n\n        if self.args.device == 'gpu':\n            self.model.to('cuda')\n    \n\n    def preprocess_texts(self, texts):\n        ids_list = []\n        for text in texts:\n            ids_list.append(self.preprocess_single_text( text, return_tensor=False))\n        return ids_list\n\n    def preprocess_single_text(self, text, return_tensor=True): \n        ids = self.tokenizer.encode(text,add_special_tokens=True)[:self.args.max_seq]\n\n        # padding\n        ids = self.padding(ids, self.args.max_seq)\n\n        if return_tensor:\n            return torch.tensor([ids])\n        else:\n            return ids\n\n    def padding(self, l, max_len, padding_id=0):\n        l = l[:max_len]+[0]*max([max_len-len(l),0])\n        return l\n\n\n    def preprocess_training_data(self,texts,labels):\n        if len(texts) != len(labels):\n            raise Exception('training data size not agree.')\n        \n        res_texts = []\n        res_labels = []\n\n        for i,_ in enumerate(texts):\n            test, label =  self.preprocess_single_training_data(texts[i],labels[i])\n            res_texts.append(test)\n            res_labels.append(label)\n\n        return torch.tensor(res_texts), torch.tensor(res_labels)\n\n\n\n\n\n    def preprocess_single_training_data(self,text,label):\n        text = self.tokenizer.encode(text,add_special_tokens=True)\n        # text = self.tokenizer.convert_tokens_to_ids(text)\n        return self.padding(text,self.args.max_seq), label\n    \n\n\n    def train(self, data, lables):\n\n        optimizer = AdamW(self.model.parameters(), correct_bias=False,lr=1e-5)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=self.args.num_training_steps)\n\n        self.model.zero_grad()\n        \n        epochs = tnrange(self.args.epoch)\n        for current_epoch in epochs:\n            iterations =  tnrange(len(lables)\/\/self.args.batch_size)\n            batch = self.make_bach(data, lables, self.args.batch_size)\n            for _ in iterations:\n                batch_data, batch_lables = next(batch)\n                self.model.train()\n\n                batch_data, batch_lables = self.preprocess_training_data(batch_data,batch_lables)\n\n                \n                if self.args.device == 'gpu':\n                    batch_data = batch_data.to('cuda')\n                    batch_lables = batch_lables.to('cuda')\n                loss, res = self.model(batch_data, labels=batch_lables)[:2]\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()\n                self.model.zero_grad()\n\n    def make_bach(self, data, lables, batch_size):\n        return (\n            [\n                data[i*batch_size:(i+1)*batch_size], lables[i*batch_size:(i+1)*batch_size]\n                ] \n        for i in range(len(data)\/\/batch_size)\n        )\n\n\n    def evaluate(self,test_tensor,labels):\n        predictions = self.predict(test_tensor)\n        return self.evaluate_with_metrics(predictions,labels)\n\n    \n    def predict(self,test):\n        test_ori = self.preprocess_texts(test)\n        test_tensor = torch.tensor(test_ori)\n\n        if self.args.device == 'gpu':\n            test_tensor = test_tensor.to('cuda')\n\n        self.model.eval()\n        with torch.no_grad():\n            outputs = self.model(test_tensor)\n            predictions = outputs[0]\n        prediction = torch.argmax(predictions,1)\n            \n        return prediction\n\n\n    def evaluate_with_metrics(self,predictions,labels):\n        return None\n        \n\n\nclass TestArgs:\n    def __init__(self):\n        self.tags = {0,1}\n\n        self.epoch = 4\n        self.batch_size = 16\n        self.max_seq = 256\n\n        # warm_up\n        # 7613 5709\n        self.warmup_steps = 5709*4\/\/16\/\/10\n        self.num_training_steps = 5709*4\/\/16\n\n        # gradient clip\n        self.max_grad_norm = 1\n\n        self.device = 'gpu'\n    \n    \n\n\n\n","d2a06c34":"len(train)","b8bc53f8":"\n# extractor = BertClassifier(TestArgs())\n# print(extractor.predict(['This is a pen for you! I am becoming a god.','I am a sample.']))\n# extractor.train(\n#     ('This is a pen.','I love playing games.','this is good')*100,\n#     ([1],[0],[1])*100,\n#     )\n# print(extractor.predict(('This is a pen.','I love playing games.','this is good')*10))\n\nimport re\nfrom sklearn.model_selection import train_test_split\ndef clean_text(text):\n    text = re.sub(r'https?:\/\/\\S+', '', text) # Remove link\n    text = re.sub(r'\\n',' ', text) # Remove line breaks\n    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n    return text\n\n\nextractor = BertClassifier(TestArgs())\nX_train, X_test, y_train, y_test = train_test_split(train['text'], train['target'], random_state=1)\n# X_train, y_train = train['text'], train['target']\n\nX_train = [clean_text(x) for x in X_train]\nX_test = [clean_text(x) for x in X_test]\n\n\ny_train = [[x] for x in y_train]\nprint(tuple(y_train)[:10])\n\nextractor = BertClassifier(TestArgs())\nextractor.train(tuple(X_train),tuple(y_train))","da9d1b79":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\ndef split(x, batch):\n    for i in range(0, len(x), batch):\n        yield(x[i:i+batch])\nbatched_test = split(X_test,16)\npredictions = []\nfor i,test_batch in enumerate(batched_test):\n    predictions += extractor.predict(tuple(test_batch)).tolist()\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\npredictions_train = []\nbatched_train = split(X_train,16)\nfor i,batch in enumerate(batched_train):\n    predictions_train += extractor.predict(tuple(batch)).tolist()\nprint('Accuracy score(train): ', accuracy_score(y_train, predictions_train))","bfb30bea":"predictions","2dbf19ba":"# for submission\n# accuracy varies from 81% to 82% looks OK\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col='id')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\n\nbatched_test = split(test['text'],16)\npredictions = []\nfor i,test_batch in enumerate(batched_test):\n    predictions += extractor.predict(tuple(test_batch)).tolist()\n\npredictions[:10]\ntest['target'] = predictions\n\nsubmission_df = test[['target']]\n# submission_df.head()\nsubmission_df.to_csv('result_8.csv')","5d57e353":"## textcnn\/bilstm\ntried offline, works worse than naive bayes baseline.","f9c63fa6":"# My Solution ","abd7a0c4":"## xgboost\n","f9fea5bb":"> ## bert","9cf0c40a":"## improvement: vocab","20f187e4":"The final submission is the result of BERT, which works better than any other results.","ab9d9214":"## baseline: naive base"}}