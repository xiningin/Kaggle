{"cell_type":{"dcc63cf6":"code","92eeeb16":"code","70a44f31":"code","cd969eb9":"code","f8226b38":"code","36d7f626":"code","b6ff32d8":"code","a9677c3b":"code","0f170603":"code","7dc9d64b":"code","0eb4f4ba":"code","6b964ae1":"code","22666891":"code","0489a1b3":"code","661b909d":"code","0ec54bd1":"code","aea55c0f":"code","ffb0ead1":"code","ce312687":"code","51b490b9":"markdown","40f6a02f":"markdown","9ca31837":"markdown","b81f029a":"markdown","0a42d92b":"markdown","e72f50df":"markdown","2bccda4c":"markdown","25772e8c":"markdown","ccbd6588":"markdown","00de7837":"markdown"},"source":{"dcc63cf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","92eeeb16":"data = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.head(10)","70a44f31":"data.info()","cd969eb9":"data.describe()","f8226b38":"#data1 visualization\nplt.scatter(data['Glucose'], data['Insulin'])\nplt.xlabel('Glucose')\nplt.ylabel('Insulin')\nplt.show()","36d7f626":"#data2 visualization\nplt.scatter(data['Glucose'], data['BloodPressure'])\nplt.xlabel('Glucose')\nplt.ylabel('BloodPressure')\nplt.show()","b6ff32d8":"# data1 create\ndata1 = data.loc[:, ['Glucose', 'Insulin']]\n# KMEANS 1\nfrom sklearn.cluster import KMeans\nkmeans1 = KMeans(n_clusters=2)\nkmeans1.fit(data1)\nlabels1 = kmeans1.predict(data1)\n\n# visualization\nplt.scatter(data['Glucose'], data['Insulin'], c = labels1)\nplt.xlabel('Glucose')\nplt.ylabel('Insulin')\nplt.show()","a9677c3b":"# data2 create\ndata2 = data.loc[:, ['Glucose', 'BloodPressure']]\n# KMEANS 2\nkmeans2 = KMeans(n_clusters=2)\nkmeans2.fit(data2)\nlabels2 = kmeans2.predict(data2)\n\n# visualization\nplt.scatter(data['Glucose'], data['BloodPressure'], c = labels2)\nplt.xlabel('Glucose')\nplt.ylabel('BloodPressure')\nplt.show()","0f170603":"df = pd.DataFrame({'labels':labels2, 'Outcome':data['Outcome']})\ncrosstab = pd.crosstab(df['labels'],df['Outcome'])\ncrosstab","7dc9d64b":"iner_list = np.empty(10)\nfor i in range(1,10):\n    kmeans2 = KMeans(n_clusters=i)\n    kmeans2.fit(data2)\n    iner_list[i] = kmeans2.inertia_\n\n# iner_list = iner_list%100\n# show the best number in graph\nplt.plot(range(0,10), iner_list,'-')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia\")\nplt.show()","0eb4f4ba":"data3 = data.drop('Outcome', axis = 1)\ndata3","6b964ae1":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nsscaler = StandardScaler()\nkmeans2 = KMeans(n_clusters=4)\npipeline = make_pipeline(sscaler, kmeans2)\npipeline.fit(data3)\n\n# cross table\nlabels = pipeline.predict(data3)\ndf = pd.DataFrame({'labels':labels, 'Outcome':data['Outcome']})\ncrosstab = pd.crosstab(df['labels'], df['Outcome'])\ncrosstab\n","22666891":"from scipy.cluster.hierarchy import linkage, dendrogram\n\nmerg = linkage(data3.iloc[200:220,:],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\nplt.show()","0489a1b3":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(n_clusters=2, affinity=\"euclidean\",linkage=\"ward\")\ncluster = hc.fit_predict(data3)\ndata3['Label'] = cluster\ndata3","661b909d":"plt.scatter(data3['Glucose'], data3['BloodPressure'], c = cluster)\nplt.xlabel('Glucose')\nplt.ylabel('BloodPressure')\nplt.show()","0ec54bd1":"plt.scatter(data['Glucose'], data['BloodPressure'], c = data['Outcome'])\nplt.xlabel('Glucose')\nplt.ylabel('BloodPressure')\nplt.show()","aea55c0f":"data3['Outcome'] = data['Outcome']\ndata3","ffb0ead1":"# We compare our labels results with base data's result.\ncorrect = []\nfor i in range(0,767):\n    if data3['Label'][i] == data3['Outcome'][i]:\n        correct.append(1)\n    else:\n        correct.append(0)\ncorrect[0:10] # -> if we find correctly 1, if not 0","ce312687":"print(\"Hierarchical Clustering Accuracy : \", (correct.count(1)\/data3['Label'].size)*100)","51b490b9":"As a result of this chart, we can choose the K value of 4.","40f6a02f":"<a id = \"3\"> <\/a><br>\n## Standarization","9ca31837":"<a id = \"5\"> <\/a><br>\n# Hierarchical Clustering","b81f029a":"This table shows us:\n* In Label 0 class We have 387 patients without diabetes and 103 patients with diabetes.\n* In Label 1 class We have 113 patients without diabetes and 165 patients with diabetes.","0a42d92b":"Now We need to find optimum K value for better result.","e72f50df":"<a id = \"2\"> <\/a><br>\n# KMeans Clustering","2bccda4c":"<a id = \"1\"> <\/a><br>\n# Dataset Features","25772e8c":"Now we try 2 different clustering. We will continue with data 2. Let's make cross table.","ccbd6588":"# INTRODUCTION\nHello.  I'm trying to learn unsupervised learning and these are my execises. I hope it help to you. If you find some bugs, please tell me. Thank you.\n\n1. [Dataset Features](#1)\n1. [KMeans Clustering](#2)\n1. [Standarization](#3)\n1. [Hierarchy](#4)\n1. [Hierarchical Clustering](#5)\n","00de7837":"<a id = \"4\"> <\/a><br>\n## Hierarchy"}}