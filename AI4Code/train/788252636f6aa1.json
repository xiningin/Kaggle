{"cell_type":{"9f4b974d":"code","dba12305":"code","a2459736":"code","c80fcbc1":"code","0be6e29f":"code","f1f40716":"code","e55cb237":"code","8ccab57d":"code","e350d048":"code","aacb4993":"code","1149ac5d":"code","48d2910a":"code","168c9d83":"code","9a0fd3e4":"code","010f6b12":"code","b5e2898d":"code","c2a30e17":"code","d3e122f6":"code","6be04e2b":"code","deadd2c1":"code","fa2358e3":"code","ef5abbff":"code","ca0a6f35":"code","35b9e9ee":"markdown","d8a1f80d":"markdown","40187e98":"markdown","6fd5f615":"markdown","84c03a0b":"markdown","1511a677":"markdown","72532577":"markdown"},"source":{"9f4b974d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dba12305":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","a2459736":"df = pd.read_csv('..\/input\/hard-drive-test-data\/harddrive.csv')\nprint(df.shape)\ndf.head()","c80fcbc1":"df = df.loc[:, ~df.isnull().all()]\nprint(df.shape)","0be6e29f":"# number of hdd\nprint(\"number of hdd:\", df['serial_number'].value_counts().shape) \n\n# number of different types of harddrives\nprint(\"number of different harddrives\", df['model'].value_counts().shape)","f1f40716":"failed_hdds = df.loc[df.failure==1][\"serial_number\"]\nlen(failed_hdds)","e55cb237":"df = df.loc[df[\"serial_number\"].isin(failed_hdds)]\ndf.shape","8ccab57d":"df[\"end_date\"] = df.groupby(\"serial_number\")[\"date\"].transform(\"max\")","e350d048":"df[\"end_date\"] = pd.to_datetime(df[\"end_date\"])\ndf[\"date\"] = pd.to_datetime(df[\"date\"])","aacb4993":"df[\"date_diff\"] = df[\"end_date\"] - df[\"date\"]\ndf[\"date_diff\"].describe()","1149ac5d":"df.head()","48d2910a":"df.isnull().sum()","168c9d83":"df[df[\"serial_number\"]=='Z300ZST1'].shape","9a0fd3e4":"df_notna = df[df.columns[~(df.isna().sum().values\/len(df) > 0.05)]]","010f6b12":"df_notna.dropna(inplace=True)","b5e2898d":"df = df_notna.copy()","c2a30e17":"df.reset_index(inplace=True, drop=True)","d3e122f6":"df.isnull().sum()","6be04e2b":"df.head()","deadd2c1":"df = df.drop(['date', 'serial_number', 'model','end_date'], axis=1)\ndf.head()","fa2358e3":"Y = df[\"date_diff\"].dt.days\nX = df.drop([\"date_diff\"],axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","ef5abbff":"from sklearn.ensemble import RandomForestRegressor\n\nRF_model=RandomForestRegressor(n_estimators=100,random_state=1)\nRF_model.fit(X_train, y_train)","ca0a6f35":"y_pred = RF_model.predict(X_test)\nRF_model.score(X_test, y_test)","35b9e9ee":"# \ud83d\uddd1\ufe0f Getting rid of NaN values","d8a1f80d":"# \u26d4\ufe0f HardDrive Failures - Random Forest ","40187e98":"# \ud83c\udfd7\ufe0f Dropping unwanted columns, Splitting Data and Training Model","6fd5f615":"# \u23f0 Finding the RUL (Remaining Useful Life)","84c03a0b":"# \ud83d\udce5 Importing needed libraries","1511a677":"## What is Random Forest Regession?\n\n<p>Random Forest Regression is a supervised learning algorithm that uses ensemble learning method for regression. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.<\/p>\n\n<center><img src = \"https:\/\/editor.analyticsvidhya.com\/uploads\/74060RF%20image.jpg\"><\/center>\n","72532577":"# \ud83d\udcc2 Loading dataset"}}