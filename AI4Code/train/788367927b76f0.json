{"cell_type":{"48f639fd":"code","33ba293a":"code","32e78344":"code","ff4471fe":"code","a217497f":"code","e05323a7":"code","2f309f0c":"code","a3436803":"code","74db9bac":"code","e810004c":"code","c376242b":"code","42de0858":"code","20db4f39":"code","b3f66725":"code","5607ac3f":"code","0e81230f":"code","aba51971":"code","ea10116b":"code","91271c90":"code","bbe3bd6c":"code","b08d4909":"code","03c78909":"code","1828fab7":"code","0d9773fd":"code","1f672de3":"code","ac5d2b7a":"code","5c83588f":"code","bd86c72c":"code","ad431fd3":"code","3358bf09":"code","d305b116":"code","7fec77f4":"code","710e3afa":"code","1779f1fb":"code","c0ea9dec":"code","1d033edc":"code","4189e6f9":"markdown","9d25a316":"markdown","4dfcc52f":"markdown"},"source":{"48f639fd":"# IMPORT modules\n# Turn GPU on\n\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport pprint\nfrom itertools import cycle, islice\nimport numpy as np\n\nfrom scipy.stats import multivariate_normal\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV,KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve, average_precision_score, auc\nfrom sklearn.utils.fixes import signature\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","33ba293a":"# Load MIMIC2 data \n\ndata = pd.read_csv('..\/input\/mimic3c\/mimic3c.csv')\nprint(\"With id\", data.shape)\n\ndata_full = data.drop('hadm_id', 1)\nprint(\"No id\",data_full.shape)\n","32e78344":"print(data_full.shape)\ndata_full.info()\ndata_full.describe()","ff4471fe":"data_full.head(10)","a217497f":"data_full.hist(bins=50, figsize=(20,15))\nplt.show()","e05323a7":"age_histogram = data_full.hist(column='age', bins=20, range=[0, 100])\nfor ax in age_histogram.flatten():\n    ax.set_xlabel(\"Age\")\n    ax.set_ylabel(\"Num. of Patients\")\nplt.show()\ndata_full.groupby('ExpiredHospital').size().plot.bar()\nplt.show()","2f309f0c":"# Label = ExpiredHospital\ny = data_full['ExpiredHospital']\nX = data_full.drop('ExpiredHospital', 1)\n\nX = X.drop('LOSdays', 1)\nX = X.drop('LOSgroupNum', 1)\nX = X.drop('AdmitDiagnosis', 1)\nX = X.drop('AdmitProcedure', 1)\nX = X.drop('marital_status', 1)\nX = X.drop('ethnicity', 1)\nX = X.drop('religion', 1)\nX = X.drop('insurance', 1)\n\nprint(\"y - Labels\", y.shape)\nprint(\"X - No Label No id \", X.shape)\nprint(X.columns)","a3436803":"# Check that all X columns have no missing values\nX.info()\nX.describe()","74db9bac":"data_full.groupby('ExpiredHospital').size().plot.bar()\nplt.show()\ndata_full.groupby('admit_type').size().plot.bar()\nplt.show()\ndata_full.groupby('admit_location').size().plot.bar()\nplt.show()","e810004c":"# MAP Text to Numerical Data with one-hot-encoding to convert categorical features to numerical\n\nprint(X.shape)\ncategorical_columns = [\n                    'gender',                     \n                    'admit_type',\n                    'admit_location'\n                      ]\nfor col in categorical_columns:\n    #if the original column is present replace it with a one-hot\n    if col in X.columns:\n        one_hot_encoded = pd.get_dummies(X[col])\n        X = X.drop(col, axis=1)\n        X = X.join(one_hot_encoded, lsuffix='_left', rsuffix='_right')\n        \nprint(X.shape)","c376242b":"print(X.columns)\n#print(X['VENTRICULOSTOMY          '])","42de0858":"print(data_full.shape)\nprint(X.shape)\n\nXnotNorm = X.copy()\nprint('XnotNorm ', XnotNorm.shape)\n\n#yFI = data_full.expired_icu\nynotNorm = y.copy()\nprint('ynotNorm ', ynotNorm.shape)","20db4f39":"# Normalize X\n\nx = XnotNorm.values #returns a numpy array\nscaler = preprocessing.StandardScaler()\nx_scaled = scaler.fit_transform(x)\nXNorm = pd.DataFrame(x_scaled, columns=XnotNorm.columns)\nprint(XNorm)\n","b3f66725":"# SPLIT into Train & Test\n\nX_train, X_test, y_train, y_test = train_test_split(XNorm, y, test_size=0.1, random_state=42)\nprint ('X_train: ', X_train.shape)\nprint ('X_test: ', X_test.shape)\nprint ('y_train: ', y_train.shape)\nprint ('y_test: ', y_test.shape)","5607ac3f":"# Test Models and evaluation metric\nseed = 7\nscoring = 'accuracy' \n\n# Spot Check Algorithms\nMymodels = []\n#Mymodels.append(('LogReg', LogisticRegression()))\nMymodels.append(('RandomForest', RandomForestClassifier()))\n#Mymodels.append(('SGDclassifier', SGDClassifier()))\n#Mymodels.append(('KNearestNeighbors', KNeighborsClassifier()))\n#Mymodels.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n#Mymodels.append(('GaussianNB', GaussianNB()))\n#Mymodels.append(('SVM', SVC()))\n\n# Evaluate each model in turn\nresults = []\nnames = []\nfor name, model in Mymodels:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg) ","0e81230f":"# Set the model according to above results\n\nmodel = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","aba51971":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","ea10116b":"# LEARNING CURVES Train \/ Validation\n\ntitle = \"Learning Curves \"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(model, title, XNorm, y, cv=cv, n_jobs=5)","91271c90":"# Model FINAL fit and evaluation on test\n\nmodel.fit(X_train, y_train)\nfinal_predictions = model.predict(X_test)\n\n#final_acc = accuracy(y_test, final_predictions)\n# Confusion matrix\n\nconf_mx = confusion_matrix(y_test, final_predictions)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint (recall, precision)","bbe3bd6c":"def plot_confusion_matrix(cm,target_names,title='Confusion matrix',cmap=None,\n                          normalize=False):\n    import itertools\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n","b08d4909":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['lived', 'died'],\n                      title        = \"Confusion Matrix\")","03c78909":"print ('precision ',round(precision_score(y_test, final_predictions),4))\nprint ('recall ',round(recall_score(y_test, final_predictions) ,4))\nprint ('accuracy ',round(accuracy_score(y_test, final_predictions),4))\nprint ('F1 score ',round(f1_score(y_test, final_predictions),4))","1828fab7":"# FEATURE IMPORTANCE \n\ntrainFinalFI = XNorm\nyFinalFI = y\nmodel.fit(trainFinalFI,yFinalFI)\n\nFI_model = pd.DataFrame({\"Feature Importance\":model.feature_importances_,}, index=trainFinalFI.columns)\nFI_model[FI_model[\"Feature Importance\"] > 0.01].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.xticks(rotation=90)\nplt.show()","0d9773fd":"# List of important features for model\nFI_model = pd.DataFrame({\"Feature Importance\":model.feature_importances_,}, index=trainFinalFI.columns)\nFI_model=FI_model.sort_values('Feature Importance', ascending = False)\nprint(FI_model[FI_model[\"Feature Importance\"] > 0.0025])","1f672de3":"# AUC\/ROC curves should be used when there are roughly equal numbers of observations for each class\n# Precision-Recall curves should be used when there is a moderate to large class imbalance\n\n# calculate AUC\nauc = roc_auc_score(y_test, final_predictions)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, final_predictions)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title('AUC for ROC')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","ac5d2b7a":"# Modify the raw final_predictions - prediction probs into 0 and 1\n\nPreds = final_predictions.copy()\n#print(len(Preds))\n#print(Preds)\nPreds[ np.where( Preds >= 0.5 ) ] = 1\nPreds[ np.where( Preds < 0.5 ) ] = 0\n\n# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, Preds)\n# calculate F1 score\nf1 = f1_score(y_test, Preds)\nprint('f1=%.3f' % (f1))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","5c83588f":"# NN MODEL\n\n# Use of DROPOUT\nmodel = models.Sequential()\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(30,)))\n#model.add(layers.BatchNormalization())\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2048, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nprint(model.summary())\n\n# FIT \/ TRAIN model\n\nNumEpochs = 100\nBatchSize = 16\n\nmodel.compile(optimizer=optimizers.Adam(lr=1e-5), loss='binary_crossentropy', metrics=['binary_accuracy'])\nhistory = model.fit(X_train, y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(X_test, y_test))\n\nresults = model.evaluate(X_test, y_test)\nprint(\"_\"*100)\nprint(\"Test Loss and Accuracy\")\nprint(\"results \", results)\nhistory_dict = history.history\nhistory_dict.keys()","bd86c72c":"# VALIDATION LOSS curves\n\nplt.clf()\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history_dict['loss']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# VALIDATION ACCURACY curves\n\nplt.clf()\nacc_values = history_dict['binary_accuracy']\nval_acc_values = history_dict['val_binary_accuracy']\nepochs = range(1, (len(history_dict['binary_accuracy']) + 1))\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\n","ad431fd3":"# Final Fit \/ Predict\n\n# NOTE final_predictions is a list of probabilities\n#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n#history = model.fit(X_train, y_train, epochs=NumEpochs, batch_size=BatchSize)\n\nfinal_predictions = model.predict(X_test)\nfinal_predictions.shape","3358bf09":"final_predictions","d305b116":"# Modify the raw final_predictions - prediction probs into 0 and 1\n\nPreds = final_predictions.copy()\n#print(len(Preds))\n#print(Preds)\nPreds[ np.where( Preds >= 0.5 ) ] = 1\nPreds[ np.where( Preds < 0.5 ) ] = 0\n#print(Preds)","7fec77f4":"\n# Confusion matrix\n\nconf_mx = confusion_matrix(y_test, Preds)\n\nTN = conf_mx[0,0]\nFP = conf_mx[0,1]\nFN = conf_mx[1,0]\nTP = conf_mx[1,1]\n\nprint ('TN: ', TN)\nprint ('FP: ', FP)\nprint ('FN: ', FN)\nprint ('TP: ', TP)\n\nrecall = TP\/(TP+FN)\nprecision = TP\/(TP+FP)\n\nprint (recall, precision)","710e3afa":"plot_confusion_matrix(conf_mx, \n                      normalize    = False,\n                      target_names = ['lived', 'died'],\n                      title        = \"Confusion Matrix\")","1779f1fb":"print ('precision ',precision_score(y_test, Preds))\nprint ('recall ',recall_score(y_test, Preds) )\nprint ('accuracy ',accuracy_score(y_test, Preds))\nprint ('F1 score ',f1_score(y_test, Preds))","c0ea9dec":"# AUC\/ROC curves should be used when there are roughly equal numbers of observations for each class\n# Precision-Recall curves should be used when there is a moderate to large class imbalance\n\n# calculate AUC\nauc = roc_auc_score(y_test, Preds)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, Preds)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\nplt.title('ROC ')\n# show the plot\nplt.show()","1d033edc":"# calculate precision-recall curve\nprecision, recall, thresholds = precision_recall_curve(y_test, Preds)\n# calculate F1 score\nf1 = f1_score(y_test, Preds)\n# calculate precision-recall AUC\n#auc = auc(recall, precision)\n# calculate average precision score\nap = average_precision_score(y_test, Preds)\nprint('f1=%.3f ap=%.3f' % (f1, ap))\n# plot no skill\nplt.plot([0, 1], [0.5, 0.5], linestyle='--')\n# plot the roc curve for the model\nplt.plot(recall, precision, marker='.')\n# show the plot\nplt.show()","4189e6f9":"# Optimize hyper params for one model\n\nmodel = RandomForestClassifier()\n\nparam_grid = [{},]\n\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(XNorm, y)\n\nprint(grid_search.best_estimator_)","9d25a316":"## Update 2021.09.08 ... A deeper look into the same issue, with medical concepts vectorized at\n### https:\/\/www.kaggle.com\/drscarlat\/predict-mortality-synthea-covid19\n\n\n\n.................................................\n* The original data is from MIMIC3 - Multiparameter Intelligent Monitoring in Intensive Care (deidentified DB) available freely from https:\/\/physionet.org\/\n* Each instance in the mldata.csv attached is one admission\n* Testing a theory I have, that one can predict Mortality just by the number of interactions betweeen patient and hospital per day, I've used the following features for the Mortality prediction as a Classification problem:\n* Age, Gender, Admission Type, Admission Source\n* Daily average number of: Labs, Micro labs, IV meds, Non-IV meds, Imaging Reports, Notes, Orders, Caregivers, Careunits, etc\n* The label is Hospital Mortality \n\nI've removed the LOS from the features as it may give a hint on the patient outcome as a leak from the future...\n\n* Random Forest vs NN on Accuracy, F1 score, ROC AUC","4dfcc52f":"# NN"}}