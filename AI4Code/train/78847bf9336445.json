{"cell_type":{"05af8f17":"code","ae314a30":"code","322ba387":"code","d6a3935b":"code","760d02bd":"code","9e542014":"code","033a872a":"code","43bfdd97":"code","b4973604":"code","be11ae45":"code","f32a6211":"code","00300da1":"code","12de6bb2":"code","90fe38e4":"code","89854c54":"code","36c82b2d":"code","03038ee1":"code","e3c78675":"code","492cdae4":"code","63c60529":"code","86f0c273":"code","70b6dfcd":"code","c0ca6d17":"code","6c0792c4":"code","b5950a9e":"code","c89c7f76":"code","08c19e7b":"code","4436a19d":"code","529e690e":"code","29bd327f":"code","1097a712":"code","4674722d":"code","9173ea7d":"code","6c57b7e2":"code","3408df66":"markdown","0315c34e":"markdown","83b374ee":"markdown","18184967":"markdown","1d086868":"markdown","4b68442f":"markdown","114b09e0":"markdown","a456cdf9":"markdown","a9392551":"markdown","394682c3":"markdown","2e467040":"markdown","6e177872":"markdown","6e58dca4":"markdown","960cbeda":"markdown","93d6e462":"markdown","b9c277fd":"markdown","05bbc9c3":"markdown","b265011e":"markdown","443ada85":"markdown","cd242988":"markdown","8bbdc5ed":"markdown","b0be9ac9":"markdown","4a8a9949":"markdown","8adef34c":"markdown","8d05b7d7":"markdown","78639462":"markdown","030b2cd6":"markdown","dc2ed042":"markdown","4aadd7ef":"markdown","76f90695":"markdown","f69687e2":"markdown","41bb9d6c":"markdown","ee69377d":"markdown","2dc74090":"markdown"},"source":{"05af8f17":"from tensorflow import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.layers import MaxPool2D,Convolution2D,Flatten,Dense,MaxPooling2D\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nfrom PIL import Image","ae314a30":"im1=Image.open('..\/input\/intel-image-classification\/seg_train\/seg_train\/buildings\/10029.jpg')\nim1","322ba387":"# Let's have a look at another image\nim2 = Image.open('..\/input\/intel-image-classification\/seg_train\/seg_train\/sea\/10087.jpg')\nim2","d6a3935b":"# Get the training and testing data\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255, \n                                   shear_range = 0.2, \n                                   zoom_range = 0.2, \n                                   horizontal_flip = True)\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntraining_set = train_datagen.flow_from_directory('..\/input\/intel-image-classification\/seg_train\/seg_train', \n                                                    target_size = (64, 64), \n                                                    batch_size = 32)\ntest_set = test_datagen.flow_from_directory('..\/input\/intel-image-classification\/seg_test\/seg_test',\n                                                target_size = (64, 64),\n                                                 batch_size = 32)","760d02bd":"# creating the model_zero_init\nmodel_zero_init=Sequential()\nmodel_zero_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='zeros',bias_initializer='zeros'))\nmodel_zero_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_zero_init.add(Flatten())\nmodel_zero_init.add(Dense(3000,activation='relu',kernel_initializer='zeros',bias_initializer='zeros'))\nmodel_zero_init.add(Dense(units=6,activation='softmax',  kernel_initializer='zeros', bias_initializer='zeros'))","9e542014":"model_zero_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","033a872a":"zero_model = model_zero_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","43bfdd97":"plt.plot([i for i in range(4)],zero_model.history['accuracy'])\nplt.plot([i for i in range(4)],zero_model.history['loss'])","b4973604":"# creating the model_one_init\nmodel_one_init=Sequential()\nmodel_one_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='ones',bias_initializer='ones'))\nmodel_one_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_one_init.add(Flatten())\nmodel_one_init.add(Dense(3000,activation='relu',kernel_initializer='ones',bias_initializer='ones'))\nmodel_one_init.add(Dense(units=6,activation='softmax',  kernel_initializer='ones', bias_initializer='ones'))","be11ae45":"model_one_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","f32a6211":"one_model = model_one_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","00300da1":"plt.plot([i for i in range(4)],one_model.history['accuracy'])\nplt.plot([i for i in range(4)],one_model.history['loss'])","12de6bb2":"# creating the model_random_init\nmodel_random_init=Sequential()\nmodel_random_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='random_normal',bias_initializer='random_normal'))\nmodel_random_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_random_init.add(Flatten())\nmodel_random_init.add(Dense(3000,activation='relu',kernel_initializer='random_normal',bias_initializer='random_normal'))\nmodel_random_init.add(Dense(units=6,activation='softmax',  kernel_initializer='random_normal', bias_initializer='random_normal'))","90fe38e4":"model_random_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","89854c54":"random_model = model_random_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","36c82b2d":"plt.plot([i for i in range(4)],random_model.history['accuracy'])\nplt.plot([i for i in range(4)],random_model.history['loss'])","03038ee1":"# creating the model_random_uni_init\nmodel_random_uni_init=Sequential()\nmodel_random_uni_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='random_uniform',bias_initializer='random_uniform'))\nmodel_random_uni_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_random_uni_init.add(Flatten())\nmodel_random_uni_init.add(Dense(3000,activation='relu',kernel_initializer='random_uniform',bias_initializer='random_uniform'))\nmodel_random_uni_init.add(Dense(units=6,activation='softmax',  kernel_initializer='random_uniform', bias_initializer='random_uniform'))","e3c78675":"model_random_uni_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","492cdae4":"random_uni_model = model_random_uni_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","63c60529":"plt.plot([i for i in range(4)],random_uni_model.history['accuracy'])\nplt.plot([i for i in range(4)],random_uni_model.history['loss'])","86f0c273":"# creating the model_truncated_init\nmodel_truncated_init=Sequential()\nmodel_truncated_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='truncated_normal',bias_initializer='truncated_normal'))\nmodel_truncated_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_truncated_init.add(Flatten())\nmodel_truncated_init.add(Dense(3000,activation='relu',kernel_initializer='truncated_normal',bias_initializer='truncated_normal'))\nmodel_truncated_init.add(Dense(units=6,activation='softmax',  kernel_initializer='truncated_normal', bias_initializer='truncated_normal'))","70b6dfcd":"model_truncated_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","c0ca6d17":"truncated_model = model_truncated_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","6c0792c4":"plt.plot([i for i in range(4)],truncated_model.history['accuracy'])\nplt.plot([i for i in range(4)],truncated_model.history['loss'])","b5950a9e":"# creating the model_glorot_init\nmodel_glorot_init=Sequential()\nmodel_glorot_init.add(Convolution2D(32,(3,3),input_shape=(64,64,3),activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_glorot_init.add(MaxPooling2D(pool_size=(2,2)))\nmodel_glorot_init.add(Flatten())\nmodel_glorot_init.add(Dense(3000,activation='relu',kernel_initializer='glorot_normal',bias_initializer='glorot_normal'))\nmodel_glorot_init.add(Dense(units=6,activation='softmax',  kernel_initializer='glorot_normal', bias_initializer='glorot_normal'))","c89c7f76":"model_glorot_init.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])","08c19e7b":"glorot_model = model_glorot_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 4,\n                        validation_data = test_set,\n                        validation_steps = 100)","4436a19d":"plt.plot([i for i in range(4)],glorot_model.history['accuracy'])\nplt.plot([i for i in range(4)],glorot_model.history['loss'])","529e690e":"# More about the model\ntf.keras.utils.plot_model(\n    model_glorot_init,\n    to_file=\"model.png\",\n    show_shapes=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False)","29bd327f":"glorot_model = model_glorot_init.fit_generator(training_set,\n                        steps_per_epoch = 100,\n                        epochs = 20,\n                        validation_data = test_set,\n                        validation_steps = 100)","1097a712":"plt.plot([i for i in range(20)],glorot_model.history['accuracy'])\nplt.plot([i for i in range(20)],glorot_model.history['loss'])","4674722d":"def image_prediction(path,model):\n    imm=Image.open(path)\n    imm=imm.resize((64,64))\n    x=np.array(imm)\n    x=np.expand_dims(x,axis=0)\n    classs=model.model.predict_classes(x)\n    l=os.listdir('..\/input\/intel-image-classification\/seg_train\/seg_train')\n    l.sort()\n    return l[classs[0]]","9173ea7d":"# Printing the image to be predicted\nim=Image.open('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10059.jpg')\nim","6c57b7e2":"image_prediction('..\/input\/intel-image-classification\/seg_pred\/seg_pred\/10059.jpg',glorot_model)","3408df66":"<div class=\"alert alert-block alert-info\" style=\"font-size:18px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp; Umm Xavier Model Looks the best of all . Let's train our model on it.:)<\/div>\n\n\n\n","0315c34e":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac This time we are gonna randomly initialize the weights and the bias of the model . Through the studies it has been seen that random initialization is a lot better than constant initialization like Zeros or Ones:)\ud83d\ude03<\/p><\/blockquote>\n\n\n\n","83b374ee":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Truncated Normal Initialization<\/center>\n<\/h1>\n<\/div>","18184967":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>He\/Xavier Initialization<\/center>\n<\/h1>\n<\/div>","1d086868":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Zero Initialization<\/center>\n<\/h1>\n<\/div>","4b68442f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Random Initialization<\/center>\n<\/h1>\n<\/div>","114b09e0":"![](data:image\/png;base64,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)","a456cdf9":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac If you think from a mathematical view point, a neural net is nothing but a chain of functions applied on top of each other. In these functions, we generally multiply an input vector with a weight vector and add a bias term to the product vector (think of broadcasting). We then pass the final vector through an activation function and then proceed from there.<br><br>\nIdeally we would want the values of the weight vector to be in such a way that they do not end up in causing a data loss in the input vector. Ultimately we are multiplying the weight vector with the input vector, so we need to be very careful. So, it\u2019s often a good practice to keep the values of the weight vector to be as small as possible but not very small so that they end up causing numerical instabilities.<br><br>\nSo we do uniform initialization of the weights:)\ud83d\ude03<\/p><\/blockquote>\n\n\n\n","a9392551":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","394682c3":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac Zero initialization means that we are gonna initialize every weight and bias as zero in each layer.\n    <br><br>\nHere is how our model will train with the weights and bias as zero :)\ud83d\ude03<\/p><\/blockquote>\n\n","2e467040":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Having a look at the data<\/center>\n<\/h1>\n<\/div>","6e177872":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac The goal of Xavier Initialization is to initialize the weights such that the variance of the activations are the same across every layer. This constant variance helps prevent the gradient from exploding or vanishing..:)\ud83d\ude03<\/p><\/blockquote>\n\n\n\n\n\n","6e58dca4":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp;The loss decrease certainly looks better, much better than throwing all zeros. The training and the validation accuracy also seemed to be in sync.<\/div>\n","960cbeda":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","93d6e462":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","b9c277fd":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac In this method we are gonna intialize every weight and bias as ones .\n    <br><br>\nHere is how our model will train with the weights and bias as ones :)\ud83d\ude03<\/p><\/blockquote>\n\n\n","05bbc9c3":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Getting the training and the validation data<\/center>\n<\/h1>\n<\/div>","b265011e":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Random Uniform Initialization<\/center>\n<\/h1>\n<\/div>","443ada85":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","cd242988":"<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp;The accuracy of the model is only 18 percent :(<br><br>This shows that zero initialization is no good since from this plot we can clearly see neither the loss of the model is going down nor the accuracy is increasing ...\n    <\/div>","8bbdc5ed":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Effect of weight initialization on a model<\/center>\n<\/h1>\n<\/div>","b0be9ac9":"<div class=\"alert alert-block alert-info\" style=\"font-size:18px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp;Although the losses are pretty much similar to the previous experiment (where weights initialized with ones) but the accuracy game has boosted up quite a lot:)<\/div>\n\n\n","4a8a9949":"## Reference : https:\/\/wandb.ai\/site\/articles\/the-effects-of-weight-initialization-on-neural-nets","8adef34c":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","8d05b7d7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>One Initialization<\/center>\n<\/h1>\n<\/div>","78639462":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5F9EA0;\n           font-size:90%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Compiling and fitting model<\/center>\n<\/h1>\n<\/div>","030b2cd6":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac Truncated normal weight initializer is the same as the random normal weight initializer but the main difference between the both is that values more than two standard deviations from the mean are discarded and re-drawn.:)\ud83d\ude03<\/p><\/blockquote>\n\n\n\n\n","dc2ed042":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Predicting some test data<\/center>\n<\/h1>\n<\/div>","4aadd7ef":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Importing the packages<\/center>\n<\/h1>\n<\/div>","76f90695":"<blockquote><p style=\"font-size:16px; color:#159364; font-family:verdana;\">\ud83d\udcac  A neural net can be viewed as a function with learnable parameters and those parameters are often referred to as weights and biases. Now, while starting the training of neural nets these parameters (typically the weights) are initialized in a number of different ways - sometimes, using contant values like 0\u2019s and 1\u2019s, sometimes with values sampled from some distribution (typically a unifrom distribution or normal distribution), sometimes with other sophisticated schemes like Xavier Initialization.\n<br><br>\nThe performance of a neural net depends a lot on how its parameters are initialized when it is starting to train. Moreover, if we initialize it randomly for each runs, it\u2019s bound to be non-reproducible (almost) and even not-so-performant too. On the other hand, if we initialize it with contant values, it might take it way too long to converge. With that, we also eliminate the beauty of randomness which in turn gives a neural net the power to reach a covergence quicker using gradient-based learning. We clearly need a better way to initialize it.\n<br><br>\nCareful initialization of weights not only helps us to develop more reproducible neural nets but also it helps us in training them better:) \ud83d\ude03<\/p><\/blockquote>","f69687e2":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#EA570E;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 10px;\n              color:white;\">\n    <center>Training Data With Xavier Initialization<\/center>\n<\/h1>\n<\/div>","41bb9d6c":"<div class=\"alert alert-block alert-info\" style=\"font-size:18px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp;As you can see the results are pretty similar to the random normal initializer .:)<\/div>\n\n\n\n","ee69377d":"![](https:\/\/i.pinimg.com\/originals\/8c\/40\/05\/8c4005377742272315e792545a9c93df.gif)\n","2dc74090":"<div class=\"alert alert-block alert-info\" style=\"font-size:18px; font-family:verdana; line-height: 1.7em;\">\n\ud83d\udccc &nbsp;The difference between random and zero initialization is huge . Zero initialization gave us highest accuracy of 18 percent on 4 epochs and Random initialization gave us 66 percent accuracy on just 4 epochs :)<\/div>\n\n"}}