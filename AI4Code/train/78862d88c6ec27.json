{"cell_type":{"a7fccc37":"code","dc636397":"code","1a0b2d65":"code","0df15652":"code","1d80c84a":"code","369f79fb":"code","d70b122e":"code","1da9a6b2":"code","2483502c":"code","4ae865b0":"code","e8cdd99c":"code","6040bcb8":"code","3cdff31e":"code","5ab6f76d":"code","0c743eeb":"code","4bd486e1":"code","1ad73f11":"code","233eeb8b":"code","327bade4":"code","fb8136d1":"code","9ae0171f":"code","31ad29d5":"code","b191404d":"code","333552b6":"code","2a43c60f":"code","479408e8":"code","c79a44bb":"code","87679689":"markdown","16a02f42":"markdown","6be9573e":"markdown","4d76c713":"markdown","1d30d104":"markdown","a20509ed":"markdown","5e77ef82":"markdown","feac161a":"markdown","d1062365":"markdown","7685361f":"markdown","39249152":"markdown","4071c9ee":"markdown"},"source":{"a7fccc37":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# read test file\ntest_data = pd.read_csv('..\/input\/restaurant-revenue-prediction\/test.csv.zip')\n# read training data\nraw_df = pd.read_csv('..\/input\/restaurant-revenue-prediction\/train.csv.zip')","dc636397":"print(f\"Shape of training data: {raw_df.shape}\\nShape of test dataset: {test_data.shape} \\\n    \\nFeatures available: {raw_df.columns}\")","1a0b2d65":"# To see all the columns in output this can be done.\npd.options.display.max_columns=None\n# To see all rows change max_columns with max_rows\n\nraw_df.head()","0df15652":"raw_df.isnull().sum().any()","1d80c84a":"raw_df.drop('Id',axis=1,inplace=True)","369f79fb":"sns.distplot(raw_df['revenue'],hist=False)\nplt.title('Distribution of Target variable')\nsns.despine(); #to remove top and right spines","d70b122e":"raw_df = raw_df[raw_df['revenue']<8e+06].copy()","1da9a6b2":"fig,ax = plt.subplots(1,2,figsize=(9,5))\nsns.set_style('darkgrid')\nsns.countplot(raw_df.Type,ax=ax[0])\nax[0].set_title('Train set')\nsns.countplot(test_data.Type,ax=ax[1])\nax[1].set_title('Test set');","2483502c":"#Create a checkpoint so that we can easily access original dataset\ndf = pd.concat([raw_df,test_data],axis=0)\n\n# Extracting month and year from date column\ndf['Open Date'] = pd.to_datetime(df['Open Date'])\ndf['launch_Month'] = [x.month for x in df['Open Date']]\ndf['launch_year'] = [x.year for x in df['Open Date']]\ndf.drop(['Id','Open Date'],axis=1,inplace=True)","4ae865b0":"sns.countplot(df['launch_Month'])\nplt.title('Month-wise no of launches');","e8cdd99c":"plt.figure(figsize=(15,6))\nsns.countplot(df['launch_year'],order=[1996,1997,1998,1999,2000,2002,2004,2005,2006,\n                                       2007,2008,2009,2010,2011,2012,2013,2014])\nplt.title('Year-wise no of launches',fontsize=15);","6040bcb8":"df['Type'].value_counts()","3cdff31e":"df['City Group'].value_counts()","5ab6f76d":"df['City'].value_counts()","0c743eeb":"df.drop('City',axis=1,inplace=True)","4bd486e1":"#converting other categorical columns\ndf['Type'] = df['Type'].map({'FC':0,'IL':1,'DT':2,'MB':3})\n\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\ndf['City Group'] = encoder.fit_transform(np.array(df['City Group']).reshape(-1,1))\ndf['City Group'] = df['City Group'].apply(int)","1ad73f11":"df.dropna().groupby('launch_Month')['revenue'].mean()","233eeb8b":"df.groupby('launch_year')['revenue'].mean()","327bade4":"# creating dummy variables\ndf.launch_year = df.launch_year.astype(str)\ndf.launch_Month = df.launch_Month.astype(str)\nyear_dummy = pd.get_dummies(df[['launch_year','launch_Month']],drop_first=True)\ndf = pd.concat([df,year_dummy],axis=1)\ndf.drop(['launch_year','launch_Month'],axis=1, inplace=True)","fb8136d1":"# Re-splitting train and test data\nprocessed_df = df.dropna(axis=0)\nprocessed_test_data = df[128:].drop('revenue',axis=1)\n# remember there were 137 rows in train data","9ae0171f":"processed_df.shape,processed_test_data.shape","31ad29d5":"#Check it once\nprocessed_df.head()","b191404d":"from sklearn.model_selection import train_test_split\nX=processed_df.drop('revenue',axis=1)\ny=df['revenue'][:128]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=12345)","333552b6":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nfrom sklearn.model_selection import cross_val_score\n\nregressors = {\n    'Linear Regression' : LinearRegression(),\n    'Logistic Regression' : LogisticRegression(),\n    'Decision Tree' : DecisionTreeRegressor(),\n    'Random Forest' : RandomForestRegressor(),\n    'Support Vector Machines' : SVR(),\n    'K-nearest Neighbors' : KNeighborsRegressor(),\n    'XGBoost' : XGBRegressor()\n}\nresults=pd.DataFrame(columns=['MAE','MSE','R2-score'])\nfor method,func in regressors.items():\n    func.fit(X_train,y_train)\n    pred = func.predict(X_test)\n    results.loc[method]= [mean_absolute_error(y_test,pred),\n                          mean_squared_error(y_test,pred),\n                          r2_score(y_test,pred)\n                         ]\nresults","2a43c60f":"from sklearn.model_selection import GridSearchCV\nparameters = {'learning_rate': [.03, 0.05, .07,.09], #so called `eta` value\n              'max_depth': [6,7,8,9],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500,700]}\n\nxgb_grid = GridSearchCV(XGBRegressor(),\n                        parameters,\n                        cv = 3,\n                        n_jobs = 5,\n                        verbose=True)\n\nxgb_grid.fit(X,y)\n\nprint(xgb_grid.best_score_)\nprint(xgb_grid.best_params_)","479408e8":"xgb=XGBRegressor(colsample_bytree=0.7,learning_rate=0.05,max_depth=7,min_child_weight=4,\n                n_estimators=500,subsample=0.7)\nxgb.fit(X,y)\npredicted_test_values = xgb.predict(processed_test_data)\nsubmission1 = pd.DataFrame(columns=['Id','Prediction'])\nsubmission1['Id'] = test_data['Id']\nsubmission1['Prediction'] = predicted_test_values\nsubmission1.to_csv('submission_xgb.csv',index=False)","c79a44bb":"knn=KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train,y_train)\npredicted_test_values2 = knn.predict(processed_test_data)\nsubmission2 = pd.DataFrame(columns=['Id','Prediction'])\nsubmission2['Id'] = test_data['Id']\nsubmission2['Prediction'] = predicted_test_values2\nsubmission2.to_csv('submission_knn.csv',index=False)","87679689":"If we remove outliers our target variable will follow a normal distribution.(with a little bit skew)","16a02f42":"* Year 1999 has recorded highest revenues.\n* Years 2000, 2013 and 2014 proved to be worst years for restaurants. Though number of observations is too low for year 2014, may be the big picture is differnt from this","6be9573e":"## Data Profiling\n\n### About the dataset:\n\nTFI has provided a dataset with 137 restaurants in the training set, and a test set of 100000 restaurants. The data columns include the open date, location, city type, and three categories of obfuscated data: Demographic data, Real estate data, and Commercial data. The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. ","4d76c713":"* There are no missing values.\n* Last column 'revenue' is our target column.\n* Features from P1 to P37 are all numerical features about which we know nothing specifically. So, I will directly feed them into my model.\n* Id column is redundant, I will drop it. To avoid it we can choose it as index column while reading data.i.e., using pd.read_csv('*filepath*',index_col=*col_name_or_positional_no*)\n* In feature Open date, I will focus on month and year and drop date values.\n* City, City Group and Type are categorical columns. To feed them to ML model they need to be converted into machine-readable form which is numerical form.","1d30d104":"Split train data further into training and testing sets to see performance of each ML model.\nThen, apply models.","a20509ed":"**Oops!** There are 63 different City values. I can create dummy variables to handle this feature but it will lead to so many columns. A good approach will be to reduce categories in this column. For example, instead of cities categories can be different zones or tier-1,tier-2 and so on.\nHere, I am dropping this column as there is not much information about which countries are involved. Also, the feature City Group covers effect of this feature as well.","5e77ef82":"With time, number of restaurants in the area is increasing due to several social factors.","feac161a":":(\n\nThere is not even asingle observation of type 'MB' in training set. This is a problem for our model.","d1062365":"## Preprocessing the data","7685361f":"## Modelling","39249152":"Second half of the year seems to witness more restaurant launches.","4071c9ee":"Revenue is a bit higher in months of Jan, Sept and Oct."}}