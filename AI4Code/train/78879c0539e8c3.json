{"cell_type":{"a71b7c0a":"code","0466faa8":"code","4a4ad9b6":"code","8d87896d":"code","bcfffeb9":"code","a2c0327f":"code","42f5a8af":"code","04add70c":"code","ccb5b2ed":"code","1450304d":"code","d01a28e0":"code","54a23863":"code","152244d9":"code","1234bf5a":"code","fefa7220":"code","d8fc8301":"code","0f418a3d":"code","2f86f602":"code","b190fc87":"markdown","8de0295c":"markdown","553b71be":"markdown","433ecd98":"markdown","81d19345":"markdown","66a88950":"markdown","c1171cab":"markdown","a8d69e13":"markdown","5724fbbb":"markdown","865be86b":"markdown"},"source":{"a71b7c0a":"import numpy as np\nimport pandas as pd\nimport string\nimport gensim\nfrom gensim.models import phrases, word2vec\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nmatplotlib.style.use('ggplot')\n\n%matplotlib inline","0466faa8":"df_IV = pd.read_table(\"..\/input\/SW_EpisodeIV.txt\", error_bad_lines=False, delim_whitespace=True, header=0, escapechar='\\\\')\ndf_V = pd.read_table(\"..\/input\/SW_EpisodeV.txt\", error_bad_lines=False, delim_whitespace=True, header=0, escapechar='\\\\')\ndf_VI = pd.read_table(\"..\/input\/SW_EpisodeVI.txt\", error_bad_lines=False, delim_whitespace=True, header=0, escapechar='\\\\')\n\npd.set_option('display.max_colwidth', -1)\ndf_IV.columns = ['speaker','text']\ndf_V.columns = ['speaker', 'text']\ndf_VI.columns = ['speaker', 'text']","4a4ad9b6":"df_IV.head(4)","8d87896d":"def prep_text(in_text):\n    return in_text.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split()","bcfffeb9":"df_IV['clean_text'] = df_IV.apply(lambda row: prep_text(row['text']), axis=1)\ndf_V['clean_text'] = df_V.apply(lambda row: prep_text(row['text']), axis=1)\ndf_VI['clean_text'] = df_VI.apply(lambda row: prep_text(row['text']), axis=1)\ndf_IV.head(5)","a2c0327f":"df = pd.concat([df_IV, df_V, df_VI])\n\nsentences = df.clean_text.values\n# for idx, row in df.iterrows():\n#     sentences.append(row['clean_text'])\n\ndf.head(5)","42f5a8af":"bigrams = phrases.Phrases(sentences)","04add70c":"print(bigrams[\"this is the death star\".split()])","ccb5b2ed":"bigrams[sentences]\n\nmodel = word2vec.Word2Vec(bigrams[sentences], size=50, min_count=3, iter=20)","1450304d":"model.wv.most_similar('death_star')","d01a28e0":"vocab = list(model.wv.vocab)\nlen(vocab)","54a23863":"X = model.wv[vocab]","152244d9":"pca = PCA(n_components=3, random_state=11, whiten=True)\nclf = pca.fit_transform(X)\n\ntmp = pd.DataFrame(clf, index=vocab, columns=['x', 'y', 'z'])\n\ntmp.head(3)","1234bf5a":"tmp = tmp.sample(150)","fefa7220":"fig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(tmp['x'], tmp['y'], tmp['z'], alpha = 0.5)\n\nfor word, row in tmp.iterrows():\n    x, y, z = row\n    pos = (x, y, z)\n    ax.text(x, y, z, s=word, size=8, zorder=1, color='k')\n    \nplt.title('w2v map - PCA')\nplt.show()","d8fc8301":"tsne = TSNE(n_components=3, random_state=11)\nclf = tsne.fit_transform(X)\n\ntmp = pd.DataFrame(clf, index=vocab, columns=['x', 'y', 'z'])\n\ntmp.head(3)","0f418a3d":"tmp = tmp.sample(150)","2f86f602":"fig = plt.figure(figsize=(15, 15))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(tmp['x'], tmp['y'], tmp['z'], alpha = 0.5)\n\nfor word, row in tmp.iterrows():\n    x, y, z = row\n    pos = (x, y, z)\n    ax.text(x, y, z, s=word, size=8, zorder=1, color='k')\n    \nplt.title('w2v map - t-SNE')\nplt.show()","b190fc87":"Lets train PCA over that","8de0295c":"and use that to create a large corpus for training","553b71be":"1108 is probably too many words to display nicely in a plot - we will thin this down a bit soon.\n\nWe need to now find the vector representing each word, we can do this like so","433ecd98":"as you can see, \"death star\" has been recognised as a bi-gram (indicated by the `_`)\n\nThis gives us something to train a w2v model over. There are a couple of important hyperparameters with gensim you need to think about when training a w2v model (I'm not convinced the defaults are great..):\n\n- size: this is the dimensionality of the vector space. The rule of thumb here is more dimensions requires more data and time to train, but also can pick up more information about the way words are used. It's typical to see this somewhere in the range of 50 to 300. We don't have that much text here so lets go with just 50\n- min_count: the min number of times a word appears before it's included in the output. 3 is perfectly ok given the size of this corpus\n- iter: this is often overlooked. You can think of it as epochs. The default is 5, which feels very small","81d19345":"First let's prep the data","66a88950":"In this kernel we're going to train a word2vec embedding for bigrams using Gensim and then plot the results in 3d using PCA and t-SNE","c1171cab":"I'm now going to take a random sample of words from the vocab, just 150. Another way to thin this down might be to only pick nouns or only pick the most common words","a8d69e13":"Lets print the same thing with t-SNE","5724fbbb":"Now we can use [gensim's phrases](https:\/\/radimrehurek.com\/gensim\/models\/phrases.html#module-gensim.models.phrases) to find bigrams","865be86b":"## plotting it\n\nSo let's plot it!\n\nWe're going to use dimensionality reduction to reduce the number of dimensions down from 50 to 3. Specifally we're going to use PCA and t-SNE\n\nFirst we need the words in the embedding"}}