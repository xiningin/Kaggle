{"cell_type":{"3d59cebf":"code","f085575d":"code","ed58a28d":"code","30338d3b":"code","3874da8d":"code","c059f945":"code","4839c6c6":"code","0c13c421":"code","7c14c320":"code","ec00ebc3":"code","71d95db7":"code","a481b8ef":"code","c84c0903":"code","e5db6889":"code","da792f29":"code","fb20d595":"code","01712cca":"code","dbbf45d0":"code","1273e147":"code","6cb3da4d":"code","ab82eb7a":"code","85f6137a":"code","f47be225":"code","3abbacd1":"code","b2ba552e":"code","6cb2f579":"code","e094c78e":"code","22f5cd88":"code","8316f716":"code","015dd4a8":"code","3387472c":"code","b1842e33":"code","5dbd27f4":"code","64dde552":"code","808b0f91":"code","4d75e06e":"code","91b70f09":"code","e3fd3aeb":"code","f3e11d59":"code","03748e03":"code","f3c58d7b":"code","439828bc":"code","feb9ddf3":"code","9912f168":"code","df8a5e51":"code","54feecef":"code","eac057ee":"code","7074b992":"code","b70d4a87":"code","6dcab393":"code","13044a6a":"code","1d1b0cbd":"code","337bf201":"code","4db9b163":"code","f4fa35ab":"code","43de6f70":"code","7d5b9940":"code","052f3483":"code","6b3ee040":"code","a5c4750e":"code","3a0bae8b":"code","2e8b66ee":"code","fa73b985":"code","47367dae":"code","35ce946f":"code","23f0f58c":"code","de86e67d":"code","72299bcc":"code","c810ea9e":"code","1e6c858a":"code","54cde862":"code","1a9fa408":"code","93e4b72d":"code","3d739a90":"code","ec434393":"code","d7a52270":"code","2e2491b1":"code","71c0dd88":"code","91e8ff6d":"code","3c6fc98e":"code","b0297770":"code","358b35c3":"code","a9570bba":"code","b4d06c36":"code","4af007df":"code","14a88640":"code","b9c3c404":"code","7b7a52a9":"code","c9c80e98":"code","d526fd24":"code","d7cac00e":"code","60dd6a08":"code","26c63406":"code","230d153e":"code","32b45d74":"code","7d89bd5f":"code","d59aeec3":"code","2ffa1381":"code","9b830c86":"markdown","955abd85":"markdown","86d9a37e":"markdown","426cd2c9":"markdown","b987e4ad":"markdown"},"source":{"3d59cebf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f085575d":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import scale\nimport statsmodels.api as sm","ed58a28d":"# To have proper visibility of all columns and rows setting these options\npd.set_option(\"display.max_rows\", 999)\npd.set_option(\"display.max_columns\", 999)","30338d3b":"# 01. Import Data\nf_train = '..\/input\/titanic\/train.csv'\ndf_train = pd.read_csv(f_train, index_col=0)\n\nf_new = '..\/input\/titanic\/test.csv'\ndf_test = pd.read_csv(f_new, index_col=0)","3874da8d":"df_train.head()","c059f945":"df_train.info()","4839c6c6":"df_test.info()","0c13c421":"df_train.describe()","7c14c320":"# Age has some null columns, we replace it with the median value of 28\ndf_train['Age'] = df_train['Age'].replace(np.nan,28 )","ec00ebc3":"df_train.describe()","71d95db7":"df_test.describe()","a481b8ef":"# Age has some null columns, we replace it with the median value of 27\ndf_test['Age'] = df_test['Age'].replace(np.nan,27 )","c84c0903":"# We replace Fare with the mean value\ndf_test['Fare'] = df_test['Fare'].replace(np.nan,35 )","e5db6889":"# For cabin we replace the numbers with just the letters. This replcaes null values too with 'n'\n\ndf_train['Cabin'] = df_train['Cabin'].astype(str).str[0]\ndf_test['Cabin'] = df_test['Cabin'].astype(str).str[0]","da792f29":"df_train.head()","fb20d595":"#df_train['Cabin'] = df_train['Cabin'].replace(np.nan,'None' )\n#df_test['Cabin'] = df_test['Cabin'].replace(np.nan,'None' )","01712cca":"df_train['Embarked'].value_counts()","dbbf45d0":"# We replace Embarked in Train set with the mode value\ndf_train['Embarked'] = df_train['Embarked'].replace(np.nan,'S' )","1273e147":"df_train['Cabin'].value_counts()","6cb3da4d":"df_test['Cabin'].value_counts()","ab82eb7a":"# Since some cabin types are very less, we can club them together to Other or letter 'O'\ndf_train['Cabin'] = df_train['Cabin'].replace('F', 'O')\ndf_train['Cabin'] = df_train['Cabin'].replace('G', 'O')\ndf_train['Cabin'] = df_train['Cabin'].replace('T', 'O')\ndf_train['Cabin'] = df_train['Cabin'].replace('A', 'O')\ndf_train['Cabin'].value_counts()","85f6137a":"# We club the cabin types for test set too\ndf_test['Cabin'] = df_test['Cabin'].replace('F', 'O')\ndf_test['Cabin'] = df_test['Cabin'].replace('G', 'O')\ndf_test['Cabin'] = df_test['Cabin'].replace('T', 'O')\ndf_test['Cabin'] = df_test['Cabin'].replace('A', 'O')\ndf_test['Cabin'].value_counts()","f47be225":"#Checking the survival rate\ndf_train['Survived'].value_counts()","3abbacd1":"df_train.info()","b2ba552e":"df_test.info()","6cb2f579":"df_train","e094c78e":"# I will now drop the Name and Ticket column as it does not have any use.\ndf_train = df_train.drop(columns=['Name','Ticket'])\ndf_test = df_test.drop(columns=['Name','Ticket'])","22f5cd88":"df_train.head()","8316f716":"df_test.head()","015dd4a8":"# Putting feature variable to X\nX = df_train.drop(['Survived'],axis=1)\n\n# Putting response variable to y\ny = df_train['Survived']","3387472c":"X.head()","b1842e33":"X.tail()","5dbd27f4":"y.head()","64dde552":"print(X.shape)\nprint(y.shape)","808b0f91":"# we create dummy values for the categorical columns\ncat_var = ['Sex', 'Cabin', 'Embarked']\ndummy = pd.get_dummies(X[cat_var], drop_first=True)\n#Adding the results to master dataframe\nX = pd.concat([X, dummy], axis=1)\n\n# Dropping the categorical variables for which dummy variables are present\nX = X.drop(cat_var, axis=1)\n\nX.head()","4d75e06e":"X.info()","91b70f09":"X['SibSp'].value_counts()","e3fd3aeb":"X['Parch'].value_counts()","f3e11d59":"X.info()","03748e03":"X.tail()","f3c58d7b":"# scaling the features\n\n# storing column names in cols\ncols = X.columns\nnum_cols = ['Pclass', 'Age', 'SibSp', 'Fare','Parch']\nX[num_cols] = pd.DataFrame(scale(X[num_cols]))\n#X.columns = cols\n#X.columns","439828bc":"X","feb9ddf3":"X.info()","9912f168":"# somehow the last row has null values, so we drop it\nX = X.drop(X.index[890])\nX.info()","df8a5e51":"y = y.drop(y.index[890])","54feecef":"# We will now do the split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","eac057ee":"X_train.shape","7074b992":"X_train.info()","b70d4a87":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","6dcab393":"# We use RFE for feature selection\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","13044a6a":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 6)             # running RFE with 10 variables as output\nrfe = rfe.fit(X_train, y_train)","1d1b0cbd":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","337bf201":"col = X_train.columns[rfe.support_]","4db9b163":"X_train.columns[~rfe.support_]","f4fa35ab":"#Assesing model with StatsModel\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","43de6f70":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","7d5b9940":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","052f3483":"y_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_Prob':y_train_pred})\ny_train_pred_final['PassengerId'] = y_train.index\ny_train_pred_final.head()","6b3ee040":"y_train_pred_final['predicted'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","a5c4750e":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","3a0bae8b":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","2e8b66ee":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","fa73b985":"# From curve we see that the optimum cutoff is 0.35\ny_train_pred_final['final_predicted'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.35 else 0)\n\ny_train_pred_final.head()","47367dae":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","35ce946f":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_predicted )\nconfusion2","23f0f58c":"df_train.head()","de86e67d":"# Putting feature variable to X\nX = df_train.drop('Survived',axis=1)\n\n# Putting response variable to y\ny = df_train['Survived']","72299bcc":"# we create dummy values for the categorical columns\ncat_var = ['Sex', 'Cabin', 'Embarked']\ndummy = pd.get_dummies(X[cat_var], drop_first=True)\n#Adding the results to master dataframe\nX = pd.concat([X, dummy], axis=1)\n\n# Dropping the categorical variables for which dummy variables are present\nX = X.drop(cat_var, axis=1)\n\nX.head()","c810ea9e":"# we create dummy values for the categorical columns for the test set too\ndummy = pd.get_dummies(df_test[cat_var], drop_first=True)\n#Adding the results to master dataframe\ndf_test = pd.concat([df_test, dummy], axis=1)\n\n# Dropping the categorical variables for which dummy variables are present\ndf_test = df_test.drop(cat_var, axis=1)\n\ndf_test.head()","1e6c858a":"from sklearn.tree import DecisionTreeClassifier\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=3)\ndt_default.fit(X, y)","54cde862":"pip install pydotplus","1a9fa408":"pip install graphviz\n","93e4b72d":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz\n\n# Putting features\nfeatures = list(X.columns[0:])\nfeatures","3d739a90":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","ec434393":"# fit\nrfc.fit(X_train,y_train)","d7a52270":"# Making predictions\npredictions = rfc.predict(X_test)","2e2491b1":"# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","71c0dd88":"# Let's check the report of our default model\nprint(classification_report(y_test,predictions))","91e8ff6d":"# Printing confusion matrix\nprint(confusion_matrix(y_test,predictions))","3c6fc98e":"print(accuracy_score(y_test,predictions))","b0297770":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [3,4,8,10],\n    'min_samples_leaf': range(3, 5, 10),\n    'min_samples_split': range(3, 5, 10),\n    'n_estimators': [5, 10, 20, 50, 100], \n    'max_features': [2, 3, 4, 5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","358b35c3":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","a9570bba":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","b4d06c36":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=3,\n                             min_samples_leaf=3, \n                             min_samples_split=3,\n                             max_features=3,\n                             n_estimators=5)","4af007df":"# fit\nrfc.fit(X_train,y_train)","14a88640":"# predict\npredictions = rfc.predict(X_test)","b9c3c404":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","7b7a52a9":"print(classification_report(y_test,predictions))","c9c80e98":"print(confusion_matrix(y_test,predictions))","d526fd24":"Y_pred = rfc.predict(df_test)\nrfc.score(X_train, y_train)\nacc_random_forest = round(rfc.score(X_train, y_train) * 100, 2)\nacc_random_forest","d7cac00e":"df_test.head()","60dd6a08":"Y_pred","26c63406":"submission = df_test[\"Pclass\"]","230d153e":"submission.head()","32b45d74":"submission = pd.DataFrame({\"Pclass\": df_test[\"Pclass\"],\"Survived\": Y_pred})\n#submission.to_csv('..\/output\/submission.csv', index=False)\nsubmission.head()","7d89bd5f":"# Need to drop Pclass as it is not needed\nsubission = submission.drop(columns=\"Pclass\")","d59aeec3":"submission.head()","2ffa1381":"submission.to_csv('..\/submission.csv', index=False)","9b830c86":"Age Cabin and Embarked ha null values. We need to impute them in the test set.\nWe also need to check the test set.","955abd85":"Creating a dataframe with the actual churn flag and the predicted probabilities","86d9a37e":"The test dataset has Age, Fare, and Cabin with null values.\nWe replace Age with median or mean in both dataset. When Cabin is null, it means No Cabin, so we replace it with None.\nFor Embarked, we need to check the data and maybe replace with the mode value.\n","426cd2c9":"Here accuracy is 78% with the Logistic Regression model.\n\nWe will try nce with Decision Tree","b987e4ad":"We now split the data to Test and train set."}}