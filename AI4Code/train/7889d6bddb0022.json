{"cell_type":{"09b7a1b7":"code","77c7c9e2":"code","030c94ae":"code","b0eb17c2":"code","16bae2d8":"code","ff3d0faf":"code","05131ae3":"code","9c2e9d2a":"code","e1bc4cd3":"code","0e86679f":"code","36e2296a":"code","e75054c4":"code","130bf408":"code","1a8a5681":"code","1cdbb5df":"code","61d9f86b":"code","2fc1c322":"code","346ee589":"code","ba7d5863":"code","06dfbc37":"code","b873bb87":"code","b8d09325":"code","4734f011":"code","e64a954f":"code","8bd15db6":"code","0930d448":"code","d1d10365":"markdown","2ba03583":"markdown","208adeda":"markdown","7d03db4d":"markdown","b97ff9af":"markdown","cc620be4":"markdown","a3423acb":"markdown","0a44b672":"markdown","f2af7b86":"markdown","08bdb079":"markdown","f2e5b5b3":"markdown","e96f38cb":"markdown","ad63a713":"markdown","67d17ba0":"markdown","56bc3ce7":"markdown","fda2d6d0":"markdown","72dcd601":"markdown","42e9f493":"markdown","291270d9":"markdown","991629ed":"markdown"},"source":{"09b7a1b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\n","77c7c9e2":"df_train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_train.head()  \n","030c94ae":"df_train.describe()  ","b0eb17c2":"df_train.isnull().sum()","16bae2d8":"neg, pos = np.bincount(df_train['Survived'])\nfig = make_subplots(rows=1, cols=2)\n\ntraces = [\n    go.Bar(\n        x=['Yes', 'No'], \n        y=[\n            len(df_train[df_train['Survived']==1]),\n            len(df_train[df_train['Survived']==0])\n        ], \n        name='Train Survived'\n    ),\n]\n\n\nfor i in range(len(traces)):\n    fig.append_trace(traces[i], (i \/\/ 2) + 1, (i % 2)  +1)\n\nfig.update_layout(\n    title_text='Train Survived distribution',\n    height=400,\n    width=400\n)\nfig.show()\n\n","ff3d0faf":"try:\n    del df_train['Name'], df_train['Ticket'], df_train['Embarked'], df_train['Cabin'], df_train['PassengerId']\n    del df_test['Name'], df_test['Ticket'], df_test['Embarked'], df_test['Cabin'], df_test['PassengerId']\n    \nexcept :\n    pass\n\ndf_train.loc[df_train['Sex'] == 'male', 'Sex'] = 1\ndf_train.loc[df_train['Sex'] == 'female', 'Sex'] = 2\ndf_train['Sex'] = df_train['Sex'].astype(int)\ndf_test.loc[df_test['Sex'] == 'male', 'Sex'] = 1\ndf_test.loc[df_test['Sex'] == 'female', 'Sex'] = 2\ndf_test['Sex'] = df_test['Sex'].astype(int)\n\ndf_train['Age'] = df_train['Age'].fillna(30)\ndf_test['Age'] = df_test['Age'].fillna(30)\n\ndf_train.head()","05131ae3":"f = plt.figure(figsize=(11, 13))\nplt.matshow(df_train.corr(), fignum=f.number)\nplt.xticks(range(df_train.shape[1]), df_train.columns, fontsize=14, rotation=75)\nplt.yticks(range(df_train.shape[1]), df_train.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","9c2e9d2a":"train_arr=df_train.values.tolist()\ndata=[x[1:] for x in train_arr]\nresponse=[x[0] for x in train_arr]\ndata = np.array(data, dtype='float')\nresponse = np.array(response, dtype='float')\n","e1bc4cd3":"# split into 60% for training and 640% for testing\nfrom sklearn.model_selection import train_test_split\n\ndata_training, data_testing, response_training, response_testing = train_test_split(data, response, test_size=0.5, random_state=42)","0e86679f":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndata_training = scaler.fit_transform(data_training)\ndata_testing = scaler.transform(data_testing)\n\ndata_training = np.clip(data_training, -5, 5)\ndata_testing = np.clip(data_testing, -5, 5)\n\n\nprint('Training labels shape:', response_training.shape)\nprint('Test labels shape:', response_testing.shape)\n\nprint('Training features shape:', data_training.shape)\nprint('Test features shape:', data_testing.shape)\n","36e2296a":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","e75054c4":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),\n      tf.keras.metrics.AUC(name='auc'),\n]\n\ndef make_model(metrics = METRICS, output_bias=None):\n  if output_bias is not None:\n    output_bias = tf.keras.initializers.Constant(output_bias)\n  model = tf.keras.Sequential([\n      tf.keras.layers.Dense(32, activation='relu'),\n      tf.keras.layers.Dropout(0.5),\n      tf.keras.layers.Dense(1, activation='sigmoid',\n                         bias_initializer=output_bias),\n  ])\n\n  model.compile(\n      optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n      loss=tf.keras.losses.BinaryCrossentropy(),\n      metrics=metrics)\n\n  return model","130bf408":"EPOCHS = 100\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)","1a8a5681":"model = make_model()\nhistory = model.fit(\n    data_training,\n    response_training,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    validation_data=(data_testing, response_testing))","1cdbb5df":"def plot_metrics(history):\n  metrics =  ['loss', 'auc', 'precision', 'accuracy']\n  for n, metric in enumerate(metrics):\n    name = metric.replace(\"_\",\" \").capitalize()\n    plt.subplot(2,2,n+1)\n    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n    plt.plot(history.epoch, history.history['val_'+metric],\n             color=colors[0], linestyle=\"--\", label='Val')\n    plt.xlabel('Epoch')\n    plt.ylabel(name)\n    if metric == 'loss':\n      plt.ylim([0, plt.ylim()[1]])\n    elif metric == 'auc':\n      plt.ylim([0.8,1])\n    else:\n      plt.ylim([0,1])\n\n    plt.legend()\n","61d9f86b":"mpl.rcParams['figure.figsize'] = (12, 10)\ncolors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n\nplot_metrics(history)","2fc1c322":"predict_train = model.predict_classes(data_training)\npredict_test = model.predict_classes(data_testing)","346ee589":"cm = confusion_matrix(response_testing, predict_test)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\n\nunique, counts = np.unique(response_testing, return_counts=True)\nprint(dict(zip(unique, counts)))\n\nunique, counts = np.unique(predict_test, return_counts=True)\nprint(dict(zip(unique, counts)))\n","ba7d5863":"weight_for_0 = (1 \/ neg)*(neg+pos)\/2.0 \nweight_for_1 = (1 \/ pos)*(neg+pos)\/2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))","06dfbc37":"weighted_model = make_model()\n\nweighted_history = weighted_model.fit(\n    data_training,\n    response_training,\n    epochs=EPOCHS,\n    validation_data=(data_testing, response_testing),\n    # The class weights go here\n    class_weight=class_weight) ","b873bb87":"plot_metrics(weighted_history)","b8d09325":"predict_weighted_train = weighted_model.predict_classes(data_training)\npredict_weighted_test = weighted_model.predict_classes(data_testing)","4734f011":"cm = confusion_matrix(response_testing, predict_weighted_test)\n\nax= plt.subplot()\nsns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\nax.set_xlabel('Predicted labels')\nax.set_ylabel('True labels')\n\nunique, counts = np.unique(response_testing, return_counts=True)\nprint(dict(zip(unique, counts)))\n\nunique, counts = np.unique(predict_weighted_test, return_counts=True)\nprint(dict(zip(unique, counts)))","e64a954f":"data_test=df_test.values.tolist()\ndata_test = np.array(data_test, dtype='float')\nprint(data_test[0:4])","8bd15db6":"prediction = model.predict_classes(data_test)\n\nid=['PassengerId']\nfor i in data_test:\n    id.append(int(i[0]))\n\ns=['Survived']\nresult = np.append(s, prediction[:, 0])\n\n\ncombined=np.vstack((id, result)).T\nprint(combined[0:4])","0930d448":"np.savetxt('Submission.csv', combined, delimiter=',', fmt='%s')","d1d10365":"Calculate class weights\n\nThe goal is to identify the customers interested in auto insurance, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class.","2ba03583":"<a id=\"2_2\"><\/a>\n<h1 style='border:0; color:black'>2.2. Training data and response correlation<h1>","208adeda":"The data is quiet balanced.","7d03db4d":"<a id=\"4\"><\/a>\n# Output","b97ff9af":"# Examine the class label imbalance","cc620be4":"<a id=\"1\"><\/a>\n<h2> Introduction<h2>","a3423acb":"<a id=\"3_1\"><\/a>\n\n# Define the model and the metrics","0a44b672":"We now check if theres is some data missing :","f2af7b86":"[](http:\/\/)<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2>Quick navigation<\/h2>\n\n[1. Introduction](#1)\n    \n[2. EDA : Exploratory Data Analysis](#2) \n    \n&nbsp;&nbsp;&nbsp;&nbsp;[2.1. Setup and examine the train dataset](#2_1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[2.2. Training data and response correlation](#2_2)   \n&nbsp;&nbsp;&nbsp;&nbsp;[2.3. Compare training and testing sets](#2_3)   \n    \n[3. Modeling (In progress)](#3)\n    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.1. Define the model and the metrics](#3_1)    \n&nbsp;&nbsp;&nbsp;&nbsp;[3.2. Set the correct initial bias](#3_2)   \n&nbsp;&nbsp;&nbsp;&nbsp;[3.3. Class weight](#3_3)  \n&nbsp;&nbsp;&nbsp;&nbsp;[3.4. Over sampling](#3_4)      \n    \n[4. Output](#4)    ","08bdb079":"<a id=\"3\"><\/a>\n<h2>3. Modeling<h2>","f2e5b5b3":"![](http:\/\/)<a id=\"2\"><\/a>\n<h2>EDA : Exploratory Data Analysis<\/center><h2>","e96f38cb":"If the model had predicted everything perfectly, this would be a diagonal matrix where values off the main diagonal, indicating incorrect predictions, would be zero. In this case, we can accept few false positives (89 cases in the matrix) meaning that we might ask a customer to subscribe to auto insurance even if he doesn't accept. However, we need to decrease as much as possible the false negative responses (18 783 cases) because they refer to the customers that want an auto subscription while the model flagged them as customers that don't want the auto insurance","ad63a713":"First, we need clean out data by convert text columns and removing irrelevant columns.\n\nWe will have to convert the sex of the passanger. Lets convert 'male' to 1 and 'female' to 2.\nWe need also to convert the Embarked column. Lets convert 'S' to 1, 'C' to 2 and 'Q' to 3\n\nThe data about the the Name and the ticket, the Port of Embarkation and the Cabin don't seem relevant. We can remove them.","67d17ba0":"# Setup\n \nWe  will first import the libraries needed.","56bc3ce7":"### Understanding useful metrics\n\nNotice that there are a few metrics defined above that can be computed by the model that will be helpful when evaluating the performance.\n\n\n\n*   **False** negatives and **false** positives are samples that were **incorrectly** classified\n*   **True** negatives and **true** positives are samples that were **correctly** classified\n*   **Accuracy** is the percentage of examples correctly classified\n>   $\\frac{\\text{true samples}}{\\text{total samples}}$\n*   **Precision** is the percentage of **predicted** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false positives}}$\n*   **Recall** is the percentage of **actual** positives that were correctly classified\n>   $\\frac{\\text{true positives}}{\\text{true positives + false negatives}}$\n*   **AUC** refers to the Area Under the Curve of a Receiver Operating Characteristic curve (ROC-AUC). This metric is equal to the probability that a classifier will rank a random positive sample higher than a random negative sample.\n\n\nRead more:\n*  [True vs. False and Positive vs. Negative](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/true-false-positive-negative)\n*  [Accuracy](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/accuracy)\n*   [Precision and Recall](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n*   [ROC-AUC](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)","fda2d6d0":"# Goal\n\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\n# Metric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\n# Submission File Format\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)","72dcd601":"<a id=\"3_3\"><\/a>\n# Class weights","42e9f493":"Normalize the input features using the sklearn StandardScaler. This will set the mean to 0 and standard deviation to 1.\n\nNote: The StandardScaler is only fit using the train_features to be sure the model is not peeking at the validation or test sets.","291270d9":"**There's no missing Data.**","991629ed":"<IMG align=\"center\" src=\"http:\/\/https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ffr.wikipedia.org%2Fwiki%2FTitanic&psig=AOvVaw2bM8kOIzurAJe7v6imSCPt&ust=1601021484779000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCLDrq4OsgewCFQAAAAAdAAAAABAD\" alt=\"Titanic\">\n"}}