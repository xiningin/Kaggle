{"cell_type":{"a91df4a1":"code","75ee0829":"code","3122398b":"code","52bbce0c":"code","9705608f":"code","640f95fb":"code","c942537c":"code","b474a7cc":"code","8ee6b47f":"code","5ce2e864":"code","e09e62a3":"markdown","e82d6b03":"markdown","101b919d":"markdown","8ad72b5d":"markdown","e38d5332":"markdown","8090944f":"markdown","7a0e6847":"markdown","3c09a7ad":"markdown","ada5ddc6":"markdown","ff0552ff":"markdown"},"source":{"a91df4a1":"! pip install -q https:\/\/github.com\/Borda\/kaggle_brain-tumor-3D\/archive\/refs\/heads\/main.zip\n! pip install -q https:\/\/github.com\/shijianjian\/EfficientNet-PyTorch-3D\/archive\/refs\/heads\/master.zip\n! pip install -q \"pytorch-lightning==1.3.8\"\n! pip uninstall -q -y wandb\n! ls -l \/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\n! nvidia-smi\n! mkdir \/kaggle\/temp\n\n%matplotlib inline\n%load_ext autoreload\n%autoreload 2\n\nimport kaggle_brain3d\nprint(kaggle_brain3d.__version__)","75ee0829":"import os\nimport glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nPATH_DATASET = \"\/kaggle\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"\nPATH_TEMP = \"\/kaggle\/temp\"\nSCAN_TYPES = (\"FLAIR\", \"T1w\", \"T1CE\", \"T2w\")\n\ndf_train = pd.read_csv(os.path.join(PATH_DATASET, \"train_labels.csv\"))\ndf_train[\"BraTS21ID\"] = df_train[\"BraTS21ID\"].apply(lambda i: \"%05d\" % i)\ndisplay(df_train.head())","3122398b":"_= df_train[\"MGMT_value\"].value_counts().plot(kind=\"pie\", title=\"label distribution\", autopct=\"%.1f%%\")","52bbce0c":"scans = [os.path.basename(p) for p in glob.glob(os.path.join(PATH_DATASET, \"train\", \"*\", \"*\"))]\n_= pd.Series(scans).value_counts().plot(kind=\"bar\", grid=True)","9705608f":"from ipywidgets import interact, IntSlider\n\nfrom kaggle_brain3d.utils import load_volume, interpolate_volume, show_volume\nfrom kaggle_brain3d.transforms import crop_volume\n\ndef interactive_show(volume_path: str, crop_thr: float):\n    print(f\"loading: {volume_path}\")\n    volume = load_volume(volume_path, percentile=0)\n    print(f\"sample shape: {volume.shape} >> {volume.dtype}\")\n    volume = interpolate_volume(volume)\n    print(f\"interp shape: {volume.shape} >> {volume.dtype}\")\n    volume = crop_volume(volume, crop_thr)\n    print(f\"crop shape: {volume.shape} >> {volume.dtype}\")\n    vol_shape = volume.shape\n    interact(\n        lambda x, y, z: plt.show(show_volume(volume, x, y, z)),\n        x=IntSlider(min=0, max=vol_shape[0], step=5, value=int(vol_shape[0] \/ 2)),\n        y=IntSlider(min=0, max=vol_shape[1], step=5, value=int(vol_shape[1] \/ 2)),\n        z=IntSlider(min=0, max=vol_shape[2], step=5, value=int(vol_shape[2] \/ 2)),\n    )\n\n\nPATH_SAMPLE_VOLUME = os.path.join(PATH_DATASET, \"train\", \"00005\", \"FLAIR\")\n\ninteractive_show(PATH_SAMPLE_VOLUME, crop_thr=1e-6)","640f95fb":"import os\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\n\nfrom kaggle_brain3d.data import BrainScansDataset\nfrom kaggle_brain3d.transforms import resize_volume\n\n# ==============================\n\nds = BrainScansDataset(\n    image_dir=os.path.join(PATH_DATASET, \"train\"),\n    df_table=os.path.join(PATH_DATASET, \"train_labels.csv\"),\n    crop_thr=None, cache_dir=PATH_TEMP,\n)\nfor i in tqdm(range(2)):\n    img = ds[i * 10][\"data\"]\n    img = resize_volume(img[0])\n    show_volume(img, fig_size=(12, 8))","c942537c":"from functools import partial\nimport rising.transforms as rtr\nfrom rising.loading import DataLoader, default_transform_call\nfrom rising.random import DiscreteParameter, UniformParameter\n\nfrom kaggle_brain3d.data import BrainScansDM  # , TRAIN_TRANSFORMS, VAL_TRANSFORMS\nfrom kaggle_brain3d.transforms import RandomAffine, rising_zero_mean\n\n# ==============================\n\n# Dataset >> mean: 0.13732214272022247 STD: 0.24326834082603455\nrising_norm = partial(rising_zero_mean, mean=0.137, std=0.243)\n\n# define transformations\nTRAIN_TRANSFORMS = [\n    rtr.Rot90((0, 1, 2), keys=[\"data\"], p=0.5),\n    rtr.Mirror(dims=DiscreteParameter([0, 1, 2]), keys=[\"data\"]),\n    RandomAffine(scale_range=(0.9, 1.1), rotation_range=(-10, 10), translation_range=(-0.1, 0.1)),\n    rising_norm,\n]\nVAL_TRANSFORMS = [\n    rising_norm,\n]\n\n# ==============================\n\ndm = BrainScansDM(\n    data_dir=PATH_DATASET,\n    scan_types=[\"FLAIR\"],\n    input_size=224,\n    crop_thr=1e-6,\n    batch_size=3,\n    cache_dir=PATH_TEMP,\n    # in_memory=True,\n    num_workers=2,\n    train_transforms=rtr.Compose(TRAIN_TRANSFORMS, transform_call=default_transform_call),\n    valid_transforms=rtr.Compose(VAL_TRANSFORMS, transform_call=default_transform_call),\n)\ndm.prepare_data(3)\ndm.setup()\nprint(f\"Training batches: {len(dm.train_dataloader())} and Validation {len(dm.val_dataloader())}\")\n\n# Quick view\nfor batch in dm.train_dataloader():\n    for i in range(2):\n        show_volume(batch[\"data\"][i][0], fig_size=(9, 6), v_min_max=(-1., 3.))\n    break","b474a7cc":"import logging\nfrom typing import Any, Optional, Sequence, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nfrom monai.networks.nets import EfficientNetBN\nfrom pytorch_lightning import LightningModule\nfrom torch import nn, Tensor\nfrom torch.optim import AdamW, Optimizer\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchmetrics import AUROC, F1\nfrom torchsummary import summary\n\n\nclass LitBrainMRI(LightningModule):\n\n    def __init__(\n        self,\n        net: Union[nn.Module, str] = \"efficientnet-b0\",\n        lr: float = 1e-3,\n        optimizer: Optional[Optimizer] = None,\n    ):\n        super().__init__()\n        if isinstance(net, str):\n            self.name = net\n            net = EfficientNetBN(net, spatial_dims=3, in_channels=1, num_classes=2)\n        else:\n            self.name = net.__class__.__name__\n        self.net = net\n        for _, param in self.net.named_parameters():\n            param.requires_grad = True\n        self.learning_rate = lr\n        self.optimizer = optimizer or AdamW(self.net.parameters(), lr=self.learning_rate)\n\n        self.train_auroc = AUROC(num_classes=2, compute_on_step=False)\n        self.train_f1_score = F1()\n        self.val_auroc = AUROC(num_classes=2, compute_on_step=False)\n        self.val_f1_score = F1()\n\n    def forward(self, x: Tensor) -> Tensor:\n        return self.net(x)\n\n    def compute_loss(self, y_hat: Tensor, y: Tensor):\n        return F.cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_idx):\n        img, y = batch[\"data\"], batch[\"label\"]\n        y_hat = self(img)\n        loss = self.compute_loss(y_hat, y)\n        self.log(\"train\/loss\", loss, prog_bar=False)\n        y_hat = F.softmax(y_hat)\n        self.log(\"train\/f1\", self.train_f1_score(y_hat, y), prog_bar=True)\n        self.train_auroc.update(y_hat, y)\n        try:  # ToDo: use balanced sampler\n            self.log('train\/auroc', self.train_auroc, on_step=False, on_epoch=True)\n        except ValueError:\n            pass\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        img, y = batch[\"data\"], batch[\"label\"]\n        y_hat = self(img)\n        loss = self.compute_loss(y_hat, y)\n        self.log(\"valid\/loss\", loss, prog_bar=False)\n        y_hat = F.softmax(y_hat)\n        self.log(\"valid\/f1\", self.val_f1_score(y_hat, y), prog_bar=True)\n        self.val_auroc.update(y_hat, y)\n        try:  # ToDo: use balanced sampler\n            self.log('valid\/auroc', self.val_auroc, on_step=False, on_epoch=True)\n        except ValueError:\n            pass\n\n    def configure_optimizers(self):\n        scheduler = CosineAnnealingLR(self.optimizer, self.trainer.max_epochs, 0)\n        return [self.optimizer], [scheduler]\n\n\n# ==============================\n\nmodel = LitBrainMRI(lr=5e-4)\n# summary(model, input_size=(1, 128, 128, 128))","8ee6b47f":"import pytorch_lightning as pl\n\nlogger = pl.loggers.CSVLogger(save_dir='logs\/', name=model.name)\nswa = pl.callbacks.StochasticWeightAveraging(swa_epoch_start=0.6)\nckpt = pl.callbacks.ModelCheckpoint(\n    monitor='valid\/auroc',\n    save_top_k=1,\n    filename='checkpoint\/{epoch:02d}-{valid_auroc:.4f}',\n    mode='max',\n)\n\n# ==============================\n\ntrainer = pl.Trainer(\n    # overfit_batches=5,\n    # fast_dev_run=True,\n    gpus=1,\n    callbacks=[ckpt , swa],  #\n    logger=logger,\n    max_epochs=10,\n    precision=16,\n    accumulate_grad_batches=24,\n    # val_check_interval=0.5,\n    progress_bar_refresh_rate=1,\n    log_every_n_steps=5,\n    weights_summary='top',\n    auto_lr_find=True,\n#     auto_scale_batch_size='binsearch',\n)\n\n# ==============================\n\n# trainer.tune(\n#     model, \n#     datamodule=dm, \n#     lr_find_kwargs=dict(min_lr=1e-5, max_lr=1e-3, num_training=20),\n#     # scale_batch_size_kwargs=dict(max_trials=5),\n# )\n# print(f\"Batch size: {dm.batch_size}\")\n# print(f\"Learning Rate: {model.learning_rate}\")\n\n# ==============================\n\ntrainer.fit(model=model, datamodule=dm)","5ce2e864":"metrics = pd.read_csv(f'{trainer.logger.log_dir}\/metrics.csv')\ndisplay(metrics.head())\n\naggreg_metrics = []\nagg_col = \"epoch\"\nfor i, dfg in metrics.groupby(agg_col):\n    agg = dict(dfg.mean())\n    agg[agg_col] = i\n    aggreg_metrics.append(agg)\n\ndf_metrics = pd.DataFrame(aggreg_metrics)\ndf_metrics[['train\/loss', 'valid\/loss']].plot(grid=True, legend=True, xlabel=agg_col)\ndf_metrics[['train\/f1', 'train\/auroc', 'valid\/f1', 'valid\/auroc']].plot(grid=True, legend=True, xlabel=agg_col)","e09e62a3":"See the dataset label distribution","e82d6b03":"## Prepare 3D model\n\nLightningModule is the core of PL, it wrappes all model related peaces, mainly:\n\n- the model\/architecture\/weights\n- evaluation metrics\n- configs for optimizer and LR cheduler","101b919d":"### Lightning DataModule\n\nIt is constric to wrap all data-related peaces and define Pytoch dataloder for Training \/ Validation \/ Testing phase.\n\nAt the end we show a few sample images from the fost training batch.","8ad72b5d":"For almost all scans we have all four types","e38d5332":"## Data exploration\n\nThese 3 cohorts are structured as follows: Each independent case has a dedicated folder identified by a five-digit number.\nWithin each of these \u201ccase\u201d folders, there are four sub-folders, each of them corresponding to each of the structural multi-parametric MRI (mpMRI) scans, in DICOM format.\nThe exact mpMRI scans included are:\n\n- **FLAIR**: Fluid Attenuated Inversion Recovery\n- **T1w**: T1-weighted pre-contrast\n- **T1Gd**: T1-weighted post-contrast\n- **T2**: T2-weighted","8090944f":"## Train a model\n\nLightning forces the following structure to your code which makes it reusable and shareable:\n\n- Research code (the LightningModule).\n- Engineering code (you delete, and is handled by the Trainer).\n- Non-essential research code (logging, etc... this goes in Callbacks).\n- Data (use PyTorch DataLoaders or organize them into a LightningDataModule).\n\nOnce you do this, you can train on multiple-GPUs, TPUs, CPUs and even in 16-bit precision without changing your code!","7a0e6847":"# Brain Tumor Classification with PyTorch\u26a1Lightning & EfficientNet 3D\n\nThe goal of this challenge is to Predict the status of a genetic biomarker important for brain cancer treatment.\n\nAll the code is refered from public repository: https:\/\/github.com\/Borda\/kaggle_brain-tumor-3D\nAny nice contribution is welcome!","3c09a7ad":"## Prepare dataset\n\n### Pytorch Dataset\n\nThe basic building block is traforming raw data to Torch Dataset.\nWe have here loading particular DICOM images into a volume and saving as temp\/cacher, so we do not need to take the very time demanding loading do next time - this boost the IO from about 2h to 8min\n\nAt the end we show a few sample images from prepared dataset.","ada5ddc6":"### Training progress","ff0552ff":"### Interactive view\n\nshowing particular scan in XYZ dimension\/slices"}}