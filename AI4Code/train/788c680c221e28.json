{"cell_type":{"d93ac62f":"code","f70f3400":"code","40a5d205":"code","7fbffa6c":"code","c99cd34d":"code","76dd262e":"code","d978a271":"code","797c24be":"code","e5b2d8ea":"code","c5480dbb":"code","485da8fc":"code","2e66932a":"code","972a2524":"code","b0a27a12":"code","c45b0c9b":"code","6d1dc259":"markdown","e0854ebb":"markdown","941f9828":"markdown","7bffbda5":"markdown","82662898":"markdown","52f1f8c6":"markdown","b2d6e503":"markdown","a7fb79c1":"markdown","2037429e":"markdown","46693668":"markdown","4e92004c":"markdown","89091a5b":"markdown","8ba8ba48":"markdown"},"source":{"d93ac62f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f70f3400":"\nsky=pd.read_csv('..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv')\nsky.head()","40a5d205":"pd.isnull(sky).sum()","7fbffa6c":"sky=sky.drop(['objid','rerun','specobjid','fiberid'], axis=1)","c99cd34d":"import seaborn as sns\ncorr=sky.corr() \nsns.heatmap(corr)","76dd262e":"sky=sky[['u','g','r','i','z','redshift','class']]","d978a271":"sky['class'].unique()","797c24be":"# Here we can see that Galaxies and Stars are more as compared to Quasars and so dataset is biased towards Galaxy and Star\nsns.countplot(x='class',data=sky, palette='plasma_r')","e5b2d8ea":"from sklearn.model_selection import train_test_split\ntrain, test= train_test_split(sky, test_size=0.3, random_state=100)\ntrain_x=train.drop('class',axis=1)\ntrain_y=train['class']\ntest_x=test.drop('class',axis=1)\ntest_y=test['class']\nprint(train_x.shape, train_y.shape, test_x.shape, test_y.shape)","c5480dbb":"Accuracy_df=pd.DataFrame(columns=['Classification Algo','Accuracy'])","485da8fc":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\ndt_model=DecisionTreeClassifier(random_state=100)\ndt_model.fit(train_x,train_y)\ndt_test_pred=dt_model.predict(test_x)\ndt_acc=round(accuracy_score(test_y, dt_test_pred)*100,2)\nAccuracy_df=Accuracy_df.append({'Classification Algo':'Decision Tree','Accuracy':dt_acc}, ignore_index=True)","2e66932a":"from sklearn.ensemble import RandomForestClassifier\nrf_model=RandomForestClassifier(random_state=100)\nrf_model.fit(train_x,train_y)\nrf_test_pred=rf_model.predict(test_x)\nrf_acc=round(accuracy_score(test_y,rf_test_pred)*100,2)\nAccuracy_df=Accuracy_df.append({'Classification Algo':'Random Forest','Accuracy':rf_acc}, ignore_index=True)","972a2524":"from sklearn.ensemble import AdaBoostClassifier\nab_model=AdaBoostClassifier(random_state=100)\nab_model.fit(train_x,train_y)\nab_test_pred=ab_model.predict(test_x)\nab_acc=round(accuracy_score(test_y, ab_test_pred)*100,2)\nAccuracy_df=Accuracy_df.append({'Classification Algo':'AdaBoost','Accuracy':ab_acc}, ignore_index=True)","b0a27a12":"from sklearn.neighbors import KNeighborsClassifier\nknn_model=KNeighborsClassifier()\nknn_model.fit(train_x, train_y)\nknn_test_pred=knn_model.predict(test_x)\nknn_acc=round(accuracy_score(test_y, knn_test_pred)*100,2)\nAccuracy_df=Accuracy_df.append({'Classification Algo':'KNN','Accuracy':knn_acc}, ignore_index=True)","c45b0c9b":"from sklearn.naive_bayes import GaussianNB\nnb_model=GaussianNB()\nnb_model.fit(train_x,train_y)\nnb_test_pred=nb_model.predict(test_x)\nnb_acc=round(accuracy_score(test_y, nb_test_pred)*100,2)\nAccuracy_df=Accuracy_df.append({'Classification Algo':'Naive bayes','Accuracy':nb_acc}, ignore_index=True)\nAccuracy_df","6d1dc259":"## Finding the correlation between the columns to fetch the best suitable features for classification.\n### Plotting the heatmap.","e0854ebb":"## Finding number of unique classes in 'class' column and will check if the classes are biased or not.","941f9828":"## Naive Bayes","7bffbda5":"## Decision Tree Classifier","82662898":"From the above correlation heatmap we can see that columns u, g, r, i, z and redshift are highly correlated. So, we'll use these columns for classification.","52f1f8c6":"There are no null values so does not need missing value imputation.","b2d6e503":"## AdaBoost","a7fb79c1":"## Splitting data into test(30% of data) and train(70% of data)","2037429e":"## Reading Dataset","46693668":"## K Nearest Neighbors","4e92004c":"## Random Forest Classifier","89091a5b":"## Dropping columns not relevant for training the model like IDs\n","8ba8ba48":"## Checking for the null values"}}