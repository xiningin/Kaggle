{"cell_type":{"11113238":"code","a2eb3b71":"code","9947eb39":"code","29cf939f":"code","bcd689a9":"code","8d9f75f2":"code","776f6118":"code","e283fc3b":"code","76ca8964":"code","eb226206":"code","17f857f1":"code","c3f24b11":"code","ca271c1d":"code","d5c7fc64":"code","dbd18b54":"code","c37867ac":"code","ff25399f":"code","47202a95":"code","92c9d67b":"code","7c9c43d7":"code","346809a4":"code","8220a0d5":"code","82e98b89":"code","5b2bbc7e":"code","30e78d7d":"code","fb9aa5af":"code","83b335db":"code","75dff648":"code","7bb28db0":"code","87456097":"code","4441951b":"code","895dfb58":"markdown","65b9831d":"markdown","54eccf9e":"markdown","107789c1":"markdown","7955f6ac":"markdown","8f048bab":"markdown","8fde6ea4":"markdown","c2e03df3":"markdown","263e712f":"markdown","1db4b1b6":"markdown","a045f75f":"markdown"},"source":{"11113238":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2eb3b71":"\n# Reading Training Data\ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',\n                         index_col='Id')\n\ntrain_data.head()","9947eb39":"train_data.info()","29cf939f":"test_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',\n                        index_col='Id')\ntest_data.head()","bcd689a9":"test_data.info()","8d9f75f2":"categorical_cols = [col for col in train_data.columns\n                    if train_data[col].dtype == object\n                    and train_data[col].nunique() <= 10]\n\n# We won't use columns that have more than 10 unique values","776f6118":"miss_categorical_cols_train = [col for col in categorical_cols\n                         if train_data[col].isnull().any()]\n\ntrain_data[miss_categorical_cols_train].info()","e283fc3b":"# More than 50% of these columns are missing values\ncategorical_cols.remove('PoolQC')\ncategorical_cols.remove('MiscFeature')\ncategorical_cols.remove('Fence')\ncategorical_cols.remove('Alley')","76ca8964":"miss_categorical_cols_test = [col for col in categorical_cols\n                         if test_data[col].isnull().any()]\n\ntrain_data[miss_categorical_cols_test].info()\n\nprint(\"\\nMissing Values columns in the test data that don't have missing values in train data:\\n\\n\"\n      ,list(set(miss_categorical_cols_test) - set(miss_categorical_cols_train)))","eb226206":"numerical_cols = [col for col in train_data.columns\n                   if train_data[col].dtype != object]\nnumerical_cols.remove('SalePrice')","17f857f1":"miss_numerical_cols_train = [col for col in numerical_cols\n                         if train_data[col].isnull().any()]\n\ntrain_data[miss_numerical_cols_train].info()","c3f24b11":"miss_numerical_cols_test = [col for col in numerical_cols\n                            if test_data[col].isnull().any()]\ntrain_data[miss_numerical_cols_test].info()\n\nprint(\"\\nMissing Values columns in the test data that don't have missing values in train data:\\n\\n\"\n      ,list(set(miss_numerical_cols_test) - set(miss_numerical_cols_train)))","ca271c1d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\npd.options.plotting.backend = \"plotly\"\n\nmy_cols = numerical_cols + categorical_cols\n\nmy_cols_with_price = my_cols + ['SalePrice']\ncorr_train = train_data[my_cols_with_price].corr()\ncorr_train[['SalePrice']].sort_values(by='SalePrice',ascending=False).style.background_gradient(cmap='viridis', axis=None)","d5c7fc64":"train_data['SalePrice'].hist()","dbd18b54":"train_data['LotArea'].hist()","c37867ac":"sns.set_style('darkgrid')\n\nplt.figure(figsize=(7,5))\nsns.regplot(x=train_data['MoSold'], \n            y=train_data['SalePrice'],\n            )","ff25399f":"numerical_cols.remove('MoSold')\nnumerical_cols.remove('YrSold')\nnumerical_cols.remove('LowQualFinSF')\nnumerical_cols.remove('MiscVal')\nnumerical_cols.remove('BsmtHalfBath')\nnumerical_cols.remove('BsmtFinSF2')","47202a95":"from sklearn.model_selection import train_test_split\n\nmy_cols = numerical_cols + categorical_cols\ny = train_data.SalePrice\nX = train_data[my_cols]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,\n                                                       random_state=0,\n                                                       train_size=0.8,\n                                                       test_size=0.2)","92c9d67b":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n\n# Preprocessing numerical data\nnumerical_transformer = Pipeline(steps=[\n                                        ('imputer_num', SimpleImputer(strategy='median')),\n                                        ('scaler_num', StandardScaler())\n])\n\n# Preprocessing categorical data\ncategorical_transformer = Pipeline(steps=[\n                                 ('imputer', SimpleImputer(strategy='constant')),\n                                 ('onehot', OneHotEncoder(handle_unknown='ignore')),\n                                 ('scaler', StandardScaler())\n])\n\n# Transforming all data\npreprocessor = ColumnTransformer(transformers=[\n                                ('num', numerical_transformer, numerical_cols),\n                                ('cat', categorical_transformer, categorical_cols)\n])","7c9c43d7":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Preprocessing numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing categorical data\ncategorical_transformer = Pipeline(steps=[\n                                 ('imputer', SimpleImputer(strategy='constant')),\n                                 ('onehot', OneHotEncoder(handle_unknown='ignore') )\n])\n\n# Transforming all data\npreprocessor = ColumnTransformer(transformers=[\n                                ('num', numerical_transformer, numerical_cols),\n                                ('cat', categorical_transformer, categorical_cols)\n])","346809a4":"train_X_prep = pd.DataFrame(preprocessor.fit_transform(train_X).todense())\nval_X_prep = pd.DataFrame(preprocessor.transform(val_X).todense())","8220a0d5":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_X_prep = pd.DataFrame(scaler.fit_transform(train_X_prep))\nval_X_prep = pd.DataFrame(scaler.transform(val_X_prep))","82e98b89":"train_X_prep.head()","5b2bbc7e":"train_X_prep.shape","30e78d7d":"from sklearn.decomposition import PCA\nimport plotly.graph_objects as go\nimport plotly.express as px\n\npca = PCA()\npca.fit(train_X_prep)\n\nfeatures = range(pca.n_components_)\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x = list(features),\n        y = 1 - pca.explained_variance_ratio_\n    )\n)\n\nfig.show()","fb9aa5af":"for i in features:\n    if 1 - pca.explained_variance_ratio_[i] > 0.999999:\n        n_components = i\n        break\n        \npca = PCA(n_components=n_components)\n\ntrain_X = pd.DataFrame(pca.fit_transform(train_X_prep))\nval_X = pd.DataFrame(pca.transform(val_X_prep))","83b335db":"from sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=0,\n                                 n_estimators=149)\n\n# Fitting the training data in the model\nrf_model.fit(train_X, train_y)\n\n# Predictions\npreds_rf = rf_model.predict(val_X)\n\n# Mean Absolute Error between the actual values and the predictions\nmae_rf = mean_absolute_error(val_y, preds_rf)\n\nprint(\"Validation MAE for Random Forest Regressor: %0.2f\"%(mae_rf))\n\n\n\n## XGB Regressor\nxgb_model = XGBRegressor(n_estimators=5000, \n                         learning_rate=0.005,\n                         n_jobs=4\n                         )\n\n# Fitting the training data in the model\nxgb_model.fit(train_X, train_y,early_stopping_rounds=5,\n              eval_set=[(val_X, val_y)],\n              verbose=False)\n\n# Predictions\npreds_xgb = xgb_model.predict(val_X)\n\n# Mean Absolute Error between the actual values and the predictions\nmae_xgb = mean_absolute_error(preds_xgb, val_y)\nprint(\"Validation MAE for XGBoost Regressor: %0.2f\" %mae_xgb)","75dff648":"xgb_model.best_ntree_limit","7bb28db0":"# Preprocess the training data, fit model\nmy_model_full_data = XGBRegressor(n_estimators=xgb_model.best_ntree_limit, \n                         learning_rate=0.05,\n                         n_jobs=4\n                         )\n\nfull_X = pd.concat([train_X, val_X], axis=0)\nfull_y = pd.concat([train_y, val_y], axis=0)\n\nmy_model_full_data.fit(full_X, full_y)","87456097":"test_X = test_data[my_cols].copy()\n\ntest_X_prep = pd.DataFrame(preprocessor.transform(test_X).todense())\ntest_X_prep = pd.DataFrame(scaler.transform(test_X_prep))\ntest_X = pd.DataFrame(pca.transform(test_X_prep))","4441951b":"test_preds = my_model_full_data.predict(test_X)\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': test_data.index,\n                       'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\n\noutput.head()","895dfb58":"# Submission","65b9831d":"# House Prices - Advanced Regression Techniques\n\n![image.png](attachment:image.png)\n\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n\n\n","54eccf9e":"# Training model with full data","107789c1":"# Notes\n- KNNImputer for NaN values\n- ![image.png](attachment:fd26c263-bc29-48ce-a1c3-72c3c2cc24da.png)","7955f6ac":"# Comparing different Regression Models","8f048bab":"# Numerical colums","8fde6ea4":"**Testing Data**","c2e03df3":"**Training Data**","263e712f":"# Categorical Columns","1db4b1b6":"According to the correlational dataframe, we will remove the columns that have a **absolute** correlation with SalePrice less than **0.04**.","a045f75f":"# Analysing data"}}