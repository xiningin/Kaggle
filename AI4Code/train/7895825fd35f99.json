{"cell_type":{"27531fb1":"code","71d5840d":"code","f2f78518":"code","c111db52":"code","ff4ee41c":"code","cb6ed654":"code","b7144368":"code","75ba24aa":"code","1d560fe9":"code","7c226758":"code","f4d33cdb":"code","7f288c45":"code","0b9ff5ac":"code","798a3773":"code","89f626b2":"markdown","7ed5eb52":"markdown","5c8851fb":"markdown","5c6e7aaf":"markdown","83823960":"markdown","000f8687":"markdown","58ae0181":"markdown","da800a2f":"markdown","69b635cf":"markdown","74f52d29":"markdown","4870833d":"markdown","70688875":"markdown","d621283c":"markdown","87f4ceff":"markdown"},"source":{"27531fb1":"! pip install pyphen\n\nimport pandas\nimport math\nimport torch\nimport json\nimport re\nimport pyphen\nimport string\nfrom IPython.display import clear_output\nfrom numpy import mean\nfrom glob import glob\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import Dataset, DataLoader","71d5840d":"class RegexTokenizer:\n    def __init__(self, regex=re.compile(r'\\w+|\\. |\\.$|\\b\\?|\\b\\!'), lower=False):\n        self.regex = regex\n        self.lower = lower\n\n    def tokenize(self, text):\n        text = text.lower() if self.lower else text\n        return self.regex.findall(text)\n\n\nclass SyllableTokenizer:\n    def __init__(self):\n        self.dic = pyphen.Pyphen(lang='en')\n\n    def tokenize(self, word):\n        return [\"#\" + s for s in self.dic.inserted(word).split(\"-\")]\n\n\nclass CharacterTokenizer:\n    def __init__(self):\n        pass\n\n    def tokenize(self, word):\n        return ['#' + char for char in list(word.replace('#', ''))]\n\n\nclass FirstPassTokenizer:\n    def __init__(self, frequency_dict, min_freq, sentence=RegexTokenizer(), word=SyllableTokenizer()):\n        self.sentence = sentence\n        self.word = word\n        self.frequency_dict = frequency_dict\n        self.min_freq = min_freq\n\n    def tokenize(self, text):\n        zeroth_pass = self.sentence.tokenize(text)\n        tokens = []\n        for potential_token in zeroth_pass:\n            try:\n                assert self.frequency_dict[potential_token] >= self.min_freq\n                tokens.append(potential_token)\n            except (AssertionError, KeyError):\n                tokens += self.word.tokenize(potential_token)\n        return tokens\n\n\nclass TextTokenizer:\n    def __init__(self, frequency_dict,  min_freq, first_pass, character=CharacterTokenizer()):\n        self.frequency_dict = frequency_dict\n        self.min_freq = min_freq\n        self.character = character\n        self.first_pass = first_pass\n        self.vocabulary = [token for token in frequency_dict.keys() if frequency_dict[token] >= min_freq]\n\n    def tokenize(self, text):\n        first_pass = self.first_pass.tokenize(text)\n        tokens = []\n        for potential_token in first_pass:\n            try:\n                assert potential_token in self.vocabulary\n                tokens.append(potential_token)\n            except (AssertionError, KeyError):\n                tokens += self.character.tokenize(potential_token)\n        return tokens\n","f2f78518":"class IntegerCoder:\n    def __init__(self, tokenizer, pad_string=\"[[PAD]]\", cls_string=\"[[CLS]]\"):\n        self.tokenizer = tokenizer\n        self.pad_string = pad_string\n        self.cls_string = cls_string\n        self.vocabulary = tokenizer.vocabulary + ['#{}'.format(char) for char in string.printable]\n        self.vocabulary.sort()\n        self.decode_dict = dict(enumerate(self.vocabulary))\n        if pad_string is not None:\n            self.decode_dict[max(self.decode_dict.keys()) + 1] = pad_string\n        if cls_string is not None:\n            self.decode_dict[max(self.decode_dict.keys()) + 1] = cls_string\n        self.encode_dict = {v: k for k, v in self.decode_dict.items()}\n        self.vocab_size = len(list(self.encode_dict.keys()))\n\n    def encode_word(self, word):\n        try:\n            return self.encode_dict[word]\n        except KeyError:\n            return self.vocab_size\n\n    def encode(self, obj):\n        if type(obj) == list:\n            return [self.encode(item) for item in obj]\n        elif type(obj) == str:\n            return self.encode_word(obj)\n        else:\n            raise TypeError(\n                \"Argument to self.encode must be a string, list of strings, or list of lists of strings, etc.\")\n\n    def decode_integer(self, integer):\n        return self.decode_dict[integer]\n\n    def decode(self, obj):\n        if type(obj) == list:\n            return [self.decode(item) for item in obj]\n        elif type(obj) == int:\n            return self.decode_integer(obj)\n        else:\n            raise TypeError(\"Argument to self.decode must be an int, list of ints, list of lists of ints, etc.\")\n            ","c111db52":"with open('..\/input\/frequency-dictionaries-for-biomedical-tokenizer\/text_frequencies.json', 'r') as f:\n    text_frequencies = json.load(f)\n\nwith open('..\/input\/frequency-dictionaries-for-biomedical-tokenizer\/first_pass_tokenizer_frequencies.json', 'r') as f:\n    first_pass_frequencies = json.load(f)\n\nfirst_pass_tokenizer = FirstPassTokenizer(text_frequencies, min_freq=10000)\n\ntokenizer = TextTokenizer(first_pass_frequencies, min_freq=10000, first_pass=first_pass_tokenizer)\n    \ntext_coder = IntegerCoder(tokenizer)","ff4ee41c":"def sequentialize(text, seq_length):\n    tokens = tokenizer.tokenize(text)\n    if len(tokens) >= seq_length:\n        return tokens[0:seq_length]\n    else:\n        return tokens + [text_coder.pad_string] * (seq_length - len(tokens))\n\n\ndef numericalize(text, seq_length=256):\n    return torch.tensor(text_coder.encode(sequentialize(str(text), seq_length)))\n\n\nprint(tokenizer.tokenize(\"This is a test of the numericalization function.\"))\nprint(numericalize(\"This is a test of the numericalization function.\", 15))","cb6ed654":"class MeshSamples(Dataset):\n    def __init__(self, directory, subdirectory, X_col='abstract', y_col=None):\n        self.directory = directory\n        self.subdirectory = subdirectory\n        self.X_col = X_col\n        self.y_col = y_col if y_col else self.directory\n        self.files = glob(\"..\/input\/data-for-learning-from-mesh-risk-factors\/{}\/{}\/*.csv\".format(self.directory, self.subdirectory))\n        self.dfs = [pandas.read_csv(f) for f in self.files]\n        self.lengths = [len(df) for df in self.dfs]\n        self.length = sum(self.lengths)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        a, b = self.get_coordinates(index)\n        df = self.dfs[a]\n        X = numericalize(df[self.X_col][b])\n        y = df[self.y_col][b]\n        return X, y\n        \n    def get_coordinates(self, index):\n        total = 0\n        for i, length in enumerate(self.lengths):\n            old_total = total\n            total += length\n            if index < total:\n                return i, index - old_total\n        return None\n        \n    \nms = MeshSamples('risk_factors', 'train')","b7144368":"dl = DataLoader(ms, batch_size=1024, shuffle=True, num_workers=0, pin_memory=True)\nnext(iter(dl))","75ba24aa":"\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) \/ d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n","1d560fe9":"\nclass TransformerClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_size, nhead, nhid, nlayers, dropout=0):\n        super(TransformerClassifier, self).__init__()\n        self.pos_encoder = PositionalEncoding(embedding_size, dropout)\n        encoder_layers = TransformerEncoderLayer(embedding_size, nhead, nhid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(vocab_size, embedding_size)\n        self.embedding_size = embedding_size\n        self.output = nn.Linear(embedding_size, 1)\n        self.sigmoid = nn.Sigmoid()\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.output.bias.data.zero_()\n        self.output.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        src = self.encoder(src) * math.sqrt(self.embedding_size)\n        src = self.pos_encoder(src)\n        encoded = self.transformer_encoder(src)\n        cls = encoded.mean(1)\n        output = self.sigmoid(self.output(cls)).flatten()\n        return output\n\nnum_types = len(text_coder.vocabulary) + 2\ntc = TransformerClassifier(num_types, 50, 1, 10, 1).to('cuda')\nprint(tc)","7c226758":"criterion = nn.BCELoss()\noptimizer = torch.optim.Adam(tc.parameters(), lr=0.01)\nlosses = []\naccuracies = []\nnum_batches = 500\n\nfor i, batch in enumerate(dl):\n    if i < num_batches:\n        X = batch[0].to('cuda')\n        y = batch[1].float().to('cuda')\n        y_pred = tc.forward(X)\n        loss = criterion(y_pred,y)\n        losses.append(loss.item())\n        accuracy = (((y_pred >= 0.5) & (y >= 0.5)).sum() + ((y_pred < 0.5) & (y < 0.5)).sum()).item()\n        accuracy = accuracy \/ y.shape[0]\n        accuracies.append(accuracy)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        clear_output()\n        print(\"Batch {} loss {} accuracy {} cumacc {}\".format(i + 1, loss.item(), accuracy, mean(accuracies)))\n    else:\n        break\n\ntorch.save(tc, 'tc.pt')\ntorch.save(tc.state_dict(), 'tc.pth')","f4d33cdb":"test_ms = MeshSamples('risk_factors', 'test')\ntest_dl = DataLoader(test_ms, batch_size=dl.batch_size, shuffle=False, num_workers=0, pin_memory=True)","7f288c45":"test_accuracies = []\n\nfor i, batch in enumerate(test_dl):\n    clear_output()\n    print(i)\n    X = batch[0].to('cuda')\n    y = batch[1].float().to('cuda')\n    y_pred = tc.forward(X)\n    accuracy = (((y_pred >= 0.5) & (y >= 0.5)).sum() + ((y_pred < 0.5) & (y < 0.5)).sum()).item() \/ test_dl.batch_size\n    test_accuracies.append(accuracy)\n\nprint(mean(test_accuracies))","0b9ff5ac":"cord = pandas.read_csv('..\/input\/cord19-abstracts\/cord.txt', sep='\\t')\n\nscores = []\n\nfor i, abstract in enumerate(cord['abstract']):\n    clear_output()\n    print(i)\n    numericalized = numericalize(abstract)\n    reshaped = numericalized.view(1, numericalized.shape[0]).to('cuda')\n    scores.append(tc.forward(reshaped).item())\n        \ncord['risk_factors'] = scores\n\ncord.to_csv('cord_enhanced.csv')","798a3773":"[title for title in cord.sort_values('risk_factors', ascending=False).head(1000)['title'] if 'COVID' in title]","89f626b2":"Now we instantiate those classes to create a tokenizer and coder for text input.","7ed5eb52":"## Evaluate model on held-out test set\n\nRecall that the datasets are balanced, so accuracy is meaningful.  Accuracy on the held-out test set is typically around 90%.","5c8851fb":"Now let's define an integer coder class to take the tokens and assign them unique integer values, allowing for padding and \"cls\" tokens a la BERT (which I'm not using here).","5c6e7aaf":"## What is a model like this good for?\n\nThere are a number of potential use cases for being able to predict relevance to a given MeSH term.  For the CORD-19 dataset in particular, one avenue we are exploring is using MeSH concept relevance ranking to refine and shrink the document set used by a question answering system.  Models like BERT and BioBERT are much more performant, and also give less noisy answers, if their contexts are derived from a more focused document set.  If we want to ask questions specifically about Risk Factors of COVID-19, we can first pre-filter our document set into a much smaller set -- say, the top thousand abstracts out of over 30,000 -- and go from there.  Here are the top results from using BioBERT to ask these top thousand CORD-19 abstracts the question, \"What are risk factors for COVID-19?\"\n\n![Ask.png](attachment:Ask.png)\n\nWe are only just beginning to experiment with the possible uses of models like this, but one thing that is clear is that we are not limited to Risk Factors.  For example, I've already deployed a model for the MeSH concept 'Drug Design', trained on over 100k abstracts, evaluated with over 91% accuracy, to create a literature collection containing potential drug targets.  Hopefully, others will find it useful to use NLP and learning from MeSH to help extract valuable information from biomedical literature.","83823960":"## Data loader\n\nOne aspect of PyTorch that I've been wanting to learn is how Dataset and DataLoader objects work.  In the past, I have done the work of loading my data and converting to tensors mostly from scratch, and probably in an unoptimized way compared to what comes bundled with PyTorch.  With some ease I was able to get a DataLoader which will abstracts and labels from multiple CSV files and create batched tensors of any given batch size.  You first create a Dataset sub-class, where you define the len and get_item methods.  In my case I needed to read from an arbitrarily large number of different CSVs (I created the data set in this way to avoid pulling overly large batches of documents from the database, and to keep file sizes down).  So when I initialize my custom Dataset sub-class, I load the CSVs in a given directory using pandas, then define get_item such that it takes a single index (e.g. get me the 1533rd data point) and locates the right CSV and index for that individual file (e.g., pull the 15th row of the 2nd CSV in the directory).  It then applies the tokenizer and integer coder to convert the text of each abstract into a tensor.  A DataLoader object, then, references this Dataset sub-class, and layers on top of it a \"batchification\" functionality.  Thus, I can create a DataLoader with a given batch size, and I can iterate through that object to pull batches of abstracts in tensor form (i.e., of dimension batch size x max sequence length).\n\nHere first is the Dataset sub-class, MeshSamples:","000f8687":"We now define the model, a transformer classifier, which looks like this:\n* Input layer\n* Embedding layer (50-dimensional)\n* Positional encodings (with max sequence length 256)\n* Transformer encoder to transform embeddings after positional enrichment (with just a single attention head, given this is a smaller model)\n* Document embedding layer which takes the mean of the transformed embeddings\n* Sigmoid output layer for binary class prediction (in this case where the binary label will be 1 for 'Risk Factors' being a topic, 0 for not.","58ae0181":"Now we make a DataLoader to do the batchification for us!","da800a2f":"## Tokenization and integer coding\n\nThis is not the focus of this exercise, but I was able to adapt some simple tokenizer classes from other work, and combine them to create a transformer-friendly frequency-thresholded word and word part tokenizer which is fit to biomedical English (using word and syllable frequencies from the MEDLINE database).  Two frequency dictionaries, one for word frequencies ('text_frequencies.json') and one for word and syllable frequencies ('first_pass_tokenizer_frequencies.json') are found in the 'cache' folder and are required to instantiate the tokenizer classes.  The basic idea is to iteratively create a tokenizer using the following steps:\n\n1. Create a \"first pass\" tokenizer which keeps word tokens (extracted with a simple regex) above a given frequency in MEDLINE.  Below that frequency threshold, use pyphen to split the words apart into syllables.\n2. Using frequencies of both word and syllable tokens on MEDLINE create with the first pass tokenizer, again use a frequency threshold to split apart infrequency syllable tokens into their component characters.\n\nYou'll see an example of its output below.  First, let's define the necessary tokenizer classes.","69b635cf":"## The model\n\nThere are lots of things I could have done at this point.  I ended up training a small transformer model, knowing that for binary text classification transformers are likely overkill. I did it the way I did it mainly as a learning exercise \u2013 another learning goal of mine was to play with the torch.nn implementation of TransformerEncoder.  I learned some good lessons about it, namely that (1) the TransformerEncoder does not \"come with\" an initial embedding layer \u2013 you need to add that to the model yourself, and (2) positional encodings (the additional embeddings that use sine\/cosine functions to encode where in the sequence a token is located) are also not included.  I used the PositionalEncodings implementation from PyTorch's transformer tutorial here:  https:\/\/pytorch.org\/tutorials\/beginner\/transformer_tutorial.html","74f52d29":"Below I define and give example outputs for two functions which use the above-defined classes to create sequences of tokens from text inputs, and also integer codings of them, with a given fixed sequence length.","4870833d":"Titles of records in the top 1000 'risk_factors'-sorted articles which contain \"COVID\":","70688875":"## Add inferences to CORD-19 abstract data\n\nSo now we have a model that will predict with (for this run) 89% accuracy whether 'Risk Factors' is a topic, which we can use to produce a \"risk_factors\" score between 0 and 1.  We close out this exercise by adding those scores to the abstract data ('cord.txt'), which was pulled from the 'metadata' file provided as part of the CORD-19 dataset.","d621283c":"# Identifying Risk Factors as a topic by learning from MeSH\n\n_Jon Scott Stevens, AbbVie RAIDERS_\n\nThe CORD-19 challenge surrounds information extraction of many stripes, but I was immediately drawn to the challenge of extracting information about risk factors. My fellow RAIDERs immediately had a number of great ideas including, (1) text mining the articles using ontologies for potential risk factors, (2) applying a question answering model such as bio-BERT to allow users to ask questions like \"is smoking a risk factor for COVID-19?\" and (3) applying simple search tools such as AWS Kendra to the data to allow users to search for phrases like \"risk factors.\" The question that occurred to me is whether any or all of these processes could be aided by pre-filtering the documents for their relevance to the concept of risk factors. The contribution of such a pre-filter is twofold.\n\n1.  It could refine the information that is extracted by getting rid of irrelevant documents thereby reducing noise\n\n2.  If the number of documents being extracted from is sufficiently shrunk, the system will become more performant \n\nI often find myself implementing some form of relevance-based pre-filtering in my projects for reason #2.  We want to speed up our systems and save on costs for fancy GPU hardware.\n\nIn the case of 'Risk Factors' we can determine relevance by learning from MeSH.\n\nhttps:\/\/www.nlm.nih.gov\/mesh\/meshhome.html\n\nI have a database of almost 30 million MEDLINE abstracts, most of which are tagged with MeSH terms, a good gold standard to use for normalized keyword annotations. There is a MeSH term 'Risk Factors'.  We can learn from the abstracts in this database to predict when 'Risk Factors' is tagged as a MeSH topic, and then apply that model to the CORD-19 abstracts, which often do not have MeSH \u2013 either because they are too new or because they are not on MEDLINE.  In doing this, we can, for example, ask questions about risk factors, and get answers much more quickly by reducing the set of answer-containing documents to only those which we are confident discuss some notion of risk factors.\n\nStep 1 of this exercise was to create a 'Risk Factors' dataset for training and evaluation, which I am not showing here.  The result of that process is a folder full of CSV files, each one balanced, with positive and negative samples for whether 'Risk Factors' is a MeSH term.  This is what is contained in the 'risk_factors' folder -- a classic binary text classification data set with over a million data points.  More than enough to train a decent model.\n\n## Import statements\n\nWe are using PyTorch for this exercise, including the torch.nn of TransformerEncoder to encode our abstracts.  In addition we are mostly using standard libraries, with the only esoteric one being 'pyphen', which separates out words into syllables, which I've used for my custom biomedical tokenizer, discussed briefly below.","87f4ceff":"## Train the model"}}