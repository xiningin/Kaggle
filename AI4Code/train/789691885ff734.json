{"cell_type":{"23848b02":"code","2425d6dd":"code","e0ccf9a1":"code","f5f14806":"code","0d92c27e":"code","e8c47a32":"code","0ddb7e1a":"code","ace10a8d":"code","3600fb3b":"code","bd17d211":"code","e0c71ad8":"code","4571cc28":"code","b4a9d1ca":"code","9d4245b8":"code","cc41a7a1":"code","07ccdd05":"code","ac61c367":"code","ef5444f5":"code","d32be473":"code","d672bf76":"code","1f4661c5":"code","cd3b4b4b":"code","93a1af24":"code","83ad9655":"code","4d6ae94e":"code","ada65086":"code","bf2328b3":"code","8c38ed88":"code","2e83a87d":"code","48f852ad":"code","c5f404ec":"code","318ce3c8":"code","14378d13":"code","08106a08":"code","09234eff":"code","e8690373":"code","8c4358d7":"code","6fccf2c4":"code","95b3af91":"code","9cca5426":"code","35b49ed5":"code","74b2c0a1":"code","4c3f2238":"code","cb89520c":"code","90bf7152":"code","bf35cbf7":"code","e49349d8":"code","0649d6b9":"code","1183eaff":"code","b8a74e70":"code","69447edf":"code","ab242ffc":"code","4116805e":"code","924a65bc":"code","00de8e1e":"code","f8e066f2":"code","432b081c":"code","baf14090":"code","9b6c4d5d":"code","16ad2661":"code","c90d5e7b":"code","1b212595":"code","f3cf81c1":"code","8e0dcd56":"code","6dcb4710":"code","2e3d7b33":"code","c66c01c6":"code","a56adb47":"code","63de2918":"code","a2c478f6":"code","749ac705":"code","24472213":"code","3e7ee0d8":"code","9aaf98de":"code","4523d40e":"code","9c960bcd":"code","9ad6c0e9":"code","31915516":"code","9a5ba24a":"code","aa75c52b":"code","234439ce":"code","cd5ae758":"code","b8a543d0":"code","c803967d":"code","26c5d254":"markdown","ffe56e11":"markdown","8140c16e":"markdown","b0bfb753":"markdown","4cbf24e0":"markdown","8e1b6070":"markdown","509bd174":"markdown","4e9240d1":"markdown","db37dee8":"markdown","fcf7520d":"markdown","71c8ef5c":"markdown","dcb97554":"markdown","04292d76":"markdown","fcc8d7da":"markdown","c7c1e549":"markdown","adfc18e7":"markdown","35b33864":"markdown","eb710eac":"markdown","1eaf457d":"markdown","42c020ac":"markdown","88d6c7ab":"markdown","12442019":"markdown","08c0a52e":"markdown","5d89fb92":"markdown","d63efe4e":"markdown","b14cd83e":"markdown","73dc3782":"markdown","829fbd14":"markdown","0d6ac330":"markdown","e14bb89c":"markdown","6ef66e6f":"markdown","460184a2":"markdown","712c3bcc":"markdown","5505a36c":"markdown","df4a4357":"markdown","ced3e586":"markdown","7b497f14":"markdown","18e01564":"markdown","94a5f8bc":"markdown","90cabf43":"markdown","25288a23":"markdown","6433d4a8":"markdown"},"source":{"23848b02":"!wget -c http:\/\/www.enterface.net\/enterface05\/docs\/results\/databases\/project2_database.zip","2425d6dd":"!unzip -b -n -q project2_database.zip","e0ccf9a1":"import io\nimport os\nimport gc # Garbage Collector para gerenciar mem\u00f3ria\n\nfrom tqdm import tqdm # barras de progresso no notebook\n\nimport subprocess # chamar programas externos a partir do python\nimport base64 # codificar v\u00eddeo em base 64\nfrom IPython.display import HTML, Video, Audio\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np # c\u00e1lculos nos vetores e matrizes!\nimport scipy.io.wavfile # ler arquivos wav","f5f14806":"#!conda install -c conda-forge -y librosa # instala o librosa e o ffmpeg para manuseio do \u00e1udio\n!conda install -y ffmpeg\n!apt-get install zip # instala no linux a ferramenta zip","0d92c27e":"root_dir = '.\/'\ntarget_audio_dir = '.\/audio_wav'\ntarget_video_dir = '.\/video_webm'\ntarget_npy_dir = '.\/audio_npy'\ntarget_npy_all_dir = '.\/audio_npy_all'\n\nif not os.path.exists(target_audio_dir):\n    os.mkdir(target_audio_dir, 755)\n        \nif not os.path.exists(target_video_dir):\n    os.mkdir(target_video_dir, 755)\n        \nif not os.path.exists(target_npy_dir):\n    os.mkdir(target_npy_dir, 755)\n        \nif not os.path.exists(target_npy_all_dir):\n    os.mkdir(target_npy_all_dir, 755)","e8c47a32":"def export_wav(root_dir, target_dir):\n    for dir_name, subdir_list, file_list in tqdm(os.walk(root_dir), total=1600):\n        for fname in file_list:\n            if fname.endswith('.avi'):\n                source = os.path.join(dir_name, fname)\n                target = os.path.join(target_dir, fname[:-3] + 'wav')\n                subprocess.call(['ffmpeg','-i', source, target])\n\nexport_wav(root_dir, target_audio_dir)","0ddb7e1a":"!ffmpeg -i .\/audio_wav\/s1_ha_1.wav -hide_banner","ace10a8d":"def list_files(startpath):\n    for dir_name, subdir_list, file_list in os.walk(root_dir):\n        level = dir_name.replace(startpath, '').count(os.sep)\n        indent = ' ' * 4 * (level)\n        print('{}{}\/'.format(indent, os.path.basename(dir_name)))\n        subindent = ' ' * 4 * (level + 1)\n        for fname in sorted(file_list):\n            if fname.endswith('.avi'):\n                print('{}{}'.format(subindent, fname))","3600fb3b":"list_files('.\/')","bd17d211":"# fixing file names\n!mv .\/audio_wav\/s16_su_3avi.wav .\/audio_wav\/s16_su_3.wav\n!mv .\/audio_wav\/s_3_fe_5.wav .\/audio_wav\/s3_fe_5.wav\n!mv .\/audio_wav\/s_3_ha_1.wav .\/audio_wav\/s3_ha_1.wav\n!mv .\/audio_wav\/s_3_ha_2.wav .\/audio_wav\/s3_ha_2.wav\n!mv .\/audio_wav\/s_3_ha_3.wav .\/audio_wav\/s3_ha_3.wav\n!mv .\/audio_wav\/s_3_ha_4.wav .\/audio_wav\/s3_ha_4.wav\n!mv .\/audio_wav\/s_3_ha_5.wav .\/audio_wav\/s3_ha_5.wav\n!mv .\/audio_wav\/s_3_sa_1.wav .\/audio_wav\/s3_sa_1.wav\n!mv .\/audio_wav\/s_3_sa_2.wav .\/audio_wav\/s3_sa_2.wav\n!mv .\/audio_wav\/s_3_sa_3.wav .\/audio_wav\/s3_sa_3.wav\n!mv .\/audio_wav\/s_3_sa_4.wav .\/audio_wav\/s3_sa_4.wav\n!mv .\/audio_wav\/s_3_sa_5.wav .\/audio_wav\/s3_sa_5.wav\n!mv .\/audio_wav\/s_3_su_1.wav .\/audio_wav\/s3_su_1.wav\n!mv .\/audio_wav\/s_3_su_2.wav .\/audio_wav\/s3_su_2.wav\n!mv .\/audio_wav\/s_3_su_3.wav .\/audio_wav\/s3_su_3.wav\n!mv .\/audio_wav\/s_3_su_4.wav .\/audio_wav\/s3_su_4.wav\n!mv .\/audio_wav\/s_3_su_5.wav .\/audio_wav\/s3_su_5.wav\n!mv .\/audio_wav\/s6_an.wav .\/audio_wav\/s6_an_all.wav\n!mv .\/audio_wav\/s6_di.wav .\/audio_wav\/s6_di_all.wav\n!mv .\/audio_wav\/s6_fe.wav .\/audio_wav\/s6_fe_all.wav\n!mv .\/audio_wav\/s6_ha.wav .\/audio_wav\/s6_ha_all.wav\n!mv .\/audio_wav\/s6_sa.wav .\/audio_wav\/s6_sa_all.wav\n!mv .\/audio_wav\/s6_su.wav .\/audio_wav\/s6_su_all.wav","e0c71ad8":"!ls -lh audio_wav\/","4571cc28":"def export_webm(root_dir, target_dir, export_list):\n    for dir_name, subdir_list, file_list in tqdm(os.walk(root_dir), total=1603):\n        for fname in file_list:\n            if fname in export_list:\n                source = os.path.join(dir_name, fname)\n                target = os.path.join(target_dir, fname[:-3] + 'webm')\n                subprocess.call(['ffmpeg','-i', source, target])","b4a9d1ca":"export_webm(root_dir, target_video_dir, ['s18_di_4.avi', 's29_ha_3.avi', 's43_fe_3.avi'])","9d4245b8":"!ls -lh video_webm\/","cc41a7a1":"def show_video_widget(video_path):\n    video = io.open(video_path, 'r+b').read()\n    encoded = base64.b64encode(video)\n    return HTML(data='''\n        <video controls>\n            <source src=\"data:video\/webm;base64,{0}\" type=\"video\/webm\" \/>\n        <\/video>'''.format(encoded.decode('ascii')))","07ccdd05":"show_video_widget('.\/video_webm\/s29_ha_3.webm')","ac61c367":"show_video_widget('.\/video_webm\/s18_di_4.webm')","ef5444f5":"show_video_widget('.\/video_webm\/s43_fe_3.webm')","d32be473":"T = 5.0 # segundos\nsample_rate = 8000 # amostras por segundo","d672bf76":"np.linspace(0, T, int(T*sample_rate), endpoint=False) # time variable","1f4661c5":"t = np.linspace(0, T, int(T * sample_rate), endpoint=False) # time variable\nsinal_1 = 0.5 * np.sin(2 * np.pi * 440 *t) #uma onda senoidal pura de 440 Hz (ISO 16! Veja mais em https:\/\/pt.wikipedia.org\/wiki\/L%C3%A1_440 )\nsinal_2 = 0.5 * np.sin(2 * np.pi * 800 *t) #uma onda senoidal pura de 1000 Hz\nsinais_1_2 = sinal_1 + sinal_2","cd3b4b4b":"Audio(sinal_1, rate=sample_rate) # Carrega a onda de 440Hz","93a1af24":"Audio(sinal_2, rate=sample_rate) # Carrega a onda de 800Hz","83ad9655":"Audio(sinais_1_2, rate=sample_rate) # Carrega a onda somada dos sinais de 440Hz e 800Hz","4d6ae94e":"plt.figure(figsize=(17, 4))\nplt.plot(sinal_1[:int(0.01*8000)])\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Sinal 1: Amplitude vs Tempo')\nplt.grid(True)\nplt.show()","ada65086":"plt.figure(figsize=(17, 4))\nplt.plot(sinal_2[:int(0.01*8000)])\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Sinal 2: Amplitude vs Tempo')\nplt.grid(True)\nplt.show()","bf2328b3":"plt.figure(figsize=(17, 4))\nplt.plot(sinais_1_2[:int(0.01*8000)])\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Sinais 1 e 2 somados: Amplitude vs Tempo')\nplt.grid(True)\nplt.show()","8c38ed88":"import librosa.display\n\nplt.figure(figsize=(15, 5))\n\nSINAL_1 = librosa.stft(sinal_1)\nSINAL_2 = librosa.stft(sinal_2)\nSINAIS_1_2 = librosa.stft(sinais_1_2)\n\nSINAL_1db = librosa.amplitude_to_db(np.abs(SINAL_1)) # 20 * log10(sinal)\nSINAL_2db = librosa.amplitude_to_db(np.abs(SINAL_2)) # 20 * log10(sinal)\nSINAIS_1_2db = librosa.amplitude_to_db(np.abs(SINAIS_1_2)) # 20 * log10(sinal)","2e83a87d":"librosa.display.specshow(SINAL_1db, sr=sample_rate, x_axis='time', y_axis='hz')","48f852ad":"librosa.display.specshow(SINAL_2db, sr=sample_rate, x_axis='time', y_axis='hz')","c5f404ec":"librosa.display.specshow(SINAIS_1_2db, sr=sample_rate, x_axis='time', y_axis='hz')","318ce3c8":"pre_emphasis = 0.95\nemphasized_signal = np.append(sinais_1_2[0], sinais_1_2[1:] - pre_emphasis * sinais_1_2[:-1])","14378d13":"plt.figure(figsize=(17, 4))\nplt.plot(emphasized_signal[:int(0.01*8000)])\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Sinais 1 e 2 somados no dom\u00ednio da Amplitude')\nplt.grid(True)\nplt.show()","08106a08":"plt.figure(figsize=(17, 4))\nplt.plot(sinais_1_2[:int(0.01*8000)])\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Original: Sinais 1 e 2 somados no dom\u00ednio da Amplitude')\nplt.grid(True)\nplt.show()","09234eff":"frame_size = 0.025\nframe_stride = 0.01","e8690373":"signal_length = len(emphasized_signal)\n\nframe_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n                                                                    \nnum_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) \/ frame_step))  # sempre temos pelo menos 1 frame","8c4358d7":"frame_length, frame_step, num_frames","6fccf2c4":"pad_signal_length = num_frames * frame_step + frame_length\nz = np.zeros((pad_signal_length - signal_length))\npad_signal = np.append(emphasized_signal, z) # Pad do sinal com zeros","95b3af91":"z, len(z)","9cca5426":"np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T","35b49ed5":"indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\nframes = pad_signal[indices.astype(np.int32, copy=False)]","74b2c0a1":"frames.shape","4c3f2238":"N = 200\nn = np.arange(0.0, N, 0.01)\n\ns = 0.54 - 0.46*np.cos(2*np.pi*n\/(N-1))\n\nplt.figure(figsize=(6, 4))\nplt.plot(n, s)\n\nplt.xlabel('Amostras')\nplt.ylabel('Amplitude')\nplt.title('Janela de Hamming')\nplt.grid(True)\nplt.show()","cb89520c":"for frame in range(len(frames)):\n    for sample in range(len(frames[frame])):\n        frames[frame][sample] *= 0.54 - 0.46 * np.cos((2 * np.pi * sample) \/ (frame_length - 1)) ","90bf7152":"NFFT = 512\n\nmag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude da FFT\npow_frames = ((1.0 \/ NFFT) * ((mag_frames) ** 2))  # Power spectrum","bf35cbf7":"print(mag_frames.shape, pow_frames.shape)","e49349d8":"plt.figure(figsize=(17, 6))\nplt.imshow(mag_frames.transpose(), extent=[0,497,0,257], origin='lower')\nplt.xlabel('Frames')\nplt.ylabel('Pot\u00eancia')\nplt.title('Power for FFT Points vs Frame Number')\nplt.grid(True)\nplt.show()","0649d6b9":"nfilt = 40\nlow_freq_mel = 0\nhigh_freq_mel = (2595 * np.log10(1 + (sample_rate \/ 2) \/ 700))  # Converte Hz para Escala Mel - SAMPLE RATE dividido por 2!\n\nprint (low_freq_mel, high_freq_mel)\n\nmel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Igualmente espa\u00e7ados na escala Mel\nhz_points = (700 * (10**(mel_points \/ 2595) - 1))  # Convert Mel to Hz\nbin = np.floor((NFFT + 1) * hz_points \/ sample_rate)\n\nfbank = np.zeros((nfilt, int(np.floor(NFFT \/ 2 + 1))))\nfor m in range(1, nfilt + 1):\n    f_m_minus = int(bin[m - 1])   # Ponto da Esquerda\n    f_m = int(bin[m])             # centro\n    f_m_plus = int(bin[m + 1])    # ponto da direita\n\n    for k in range(f_m_minus, f_m):\n        fbank[m - 1, k] = (k - bin[m - 1]) \/ (bin[m] - bin[m - 1])\n    for k in range(f_m, f_m_plus):\n        fbank[m - 1, k] = (bin[m + 1] - k) \/ (bin[m + 1] - bin[m])\n\nfilter_banks = np.dot(pow_frames, fbank.T)\nfilter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Estabilidade num\u00e9rica\nfilter_banks = 20 * np.log10(filter_banks)  # dB","1183eaff":"filter_banks.shape","b8a74e70":"plt.figure(figsize=(17, 10))\nplt.imshow(filter_banks.transpose(), extent=[0,497,0,40], origin='lower')\nplt.xlabel('Frames')\nplt.ylabel('Pot\u00eancia')\nplt.title('Power for FFT Points vs Frame Number')\nplt.grid(True)\nplt.show()","69447edf":"filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)","ab242ffc":"plt.figure(figsize=(17, 10))\nplt.imshow(filter_banks.transpose(), extent=[0,497,0,40], origin='lower')\nplt.xlabel('Frames')\nplt.ylabel('Pot\u00eancia')\nplt.title('Power for FFT Points vs Frame Number')\nplt.grid(True)\nplt.show()","4116805e":"filter_banks.shape","924a65bc":"filter_banks[0][:10]","00de8e1e":"pre_emphasis = 0.95\nframe_size = 0.025\nframe_stride = 0.01\nNFFT = 512\nnfilt = 40\n\nprintevery = 100\n\ndef preprocess_dataset(in_path, out_path):\n    # Cria uma pasta para os dados n\u00e3o processados\n    if not os.path.exists(out_path):\n        os.mkdir(out_path, 755);\n\n    files = [x for x in os.listdir(in_path) if x.endswith('.wav')]\n    \n    for idx, in_filename in enumerate(files):\n        audio_path = os.path.join(in_path, in_filename)\n        #print(audio_path)\n        if (0 == idx % printevery or idx == len(files)-1):\n            print('\\r Careregando arquivo {:s}: {:2d} de {:2d} arquivos'.format(files[idx], idx+1, len(files)))\n\n        # lendo o arquivo\n        sample_rate, signal = scipy.io.wavfile.read(audio_path)\n        emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n\n        # calculando o tamanho e n\u00famero dos frames\n        signal_length = len(emphasized_signal)\n        frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n        num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) \/ frame_step))  # sempre temos pelo menos 1 frame\n\n        # aplicando pad para o \u00faltimo frame\n        pad_signal_length = num_frames * frame_step + frame_length\n        z = np.zeros((pad_signal_length - signal_length))\n        pad_signal = np.append(emphasized_signal, z) # Pad do sinal com zeros\n        \n        # fatiando o sinal em frames\n        indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n        frames = pad_signal[indices.astype(np.int32, copy=False)]\n        \n        # aplicando a janela de hamming nos frames (a linha abaixo substitui as 3 linhas que seguem)\n        frames *= np.hamming(frame_length)\n        #for frame in range(len(frames)):\n        #    for sample in range(len(frames[frame])):\n        #        frames[frame][sample] *= 0.54 - 0.46 * np.cos((2 * np.pi * sample) \/ (frame_length - 1))\n        \n        # Calculando a magnitude e pot\u00eancia para cada frequ\u00eancia presente nos frames\n        mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n        pow_frames = ((1.0 \/ NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n\n        # Agora passou de dom\u00ednio do tempo para dom\u00ednio da frequ\u00eancia\n        \n        # Aplicando a transforma\u00e7\u00e3o de mel \n        low_freq_mel = 0\n        high_freq_mel = (2595 * np.log10(1 + (sample_rate \/ 2) \/ 700))  # Converte Hz para Escala Mel\n        #print (low_freq_mel, high_freq_mel)\n\n        mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Igualmente espa\u00e7ados na escala Mel\n        hz_points = (700 * (10**(mel_points \/ 2595) - 1))  # Convert Mel to Hz\n        bin = np.floor((NFFT + 1) * hz_points \/ sample_rate)\n\n        fbank = np.zeros((nfilt, int(np.floor(NFFT \/ 2 + 1))))\n        for m in range(1, nfilt + 1):\n            f_m_minus = int(bin[m - 1])   # esqurda\n            f_m = int(bin[m])             # centro\n            f_m_plus = int(bin[m + 1])    # direita\n\n            for k in range(f_m_minus, f_m):\n                fbank[m - 1, k] = (k - bin[m - 1]) \/ (bin[m] - bin[m - 1])\n            for k in range(f_m, f_m_plus):\n                fbank[m - 1, k] = (bin[m + 1] - k) \/ (bin[m + 1] - bin[m])\n\n        filter_banks = np.dot(pow_frames, fbank.T)\n        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Estabilidade num\u00e9rica\n        filter_banks = 20 * np.log10(filter_banks)  # dB\n\n        # Normalizando os filtros\n        filter_banks -= (np.mean(filter_banks, axis=0) + 1e-8)\n        \n        out_file = os.path.join(out_path, in_filename[:-3] + 'npy')\n        np.save(out_file, filter_banks)","f8e066f2":"preprocess_dataset(target_audio_dir, target_npy_dir)","432b081c":"ls -lh audio_npy\/","baf14090":"!mv .\/audio_npy\/s6_an_all.npy .\/audio_npy_all\/s6_an_all.npy\n!mv .\/audio_npy\/s6_di_all.npy .\/audio_npy_all\/s6_di_all.npy\n!mv .\/audio_npy\/s6_fe_all.npy .\/audio_npy_all\/s6_fe_all.npy\n!mv .\/audio_npy\/s6_ha_all.npy .\/audio_npy_all\/s6_ha_all.npy\n!mv .\/audio_npy\/s6_sa_all.npy .\/audio_npy_all\/s6_sa_all.npy\n!mv .\/audio_npy\/s6_su_all.npy .\/audio_npy_all\/s6_su_all.npy","9b6c4d5d":"!du -h --max-depth=1","16ad2661":"def encode_class(class_name, class_names): # cria um vetor one-hot para a classe\n    try:\n        idx = class_names.index(class_name)\n        enc = np.zeros(len(class_names))\n        enc[idx] = 1\n        return enc\n    except ValueError:\n        return Nonea","c90d5e7b":"encode_class('ha', ['sa', 'di', 'fe', 'an', 'ha', 'su'])","1b212595":"def get_melgram_dimensions_from_file(audio_filepath):\n    melgram = np.load(audio_filepath)\n    return melgram.shape","f3cf81c1":"get_melgram_dimensions_from_file('.\/audio_npy\/s10_an_1.npy')","8e0dcd56":"plt.figure(figsize=(17, 6))\nfilter_banks=np.load('.\/audio_npy\/s10_an_1.npy')\nplt.imshow(filter_banks.transpose(), extent=[0,497,0,40], origin='lower')\nplt.xlabel('Frames')\nplt.ylabel('Pot\u00eancia')\nplt.title('Power for FFT Points vs Frame Number')\nplt.grid(True)\nplt.show()","6dcb4710":"def get_num_melgram_frames_from_dir(dirpath):\n    files = [os.path.join(dirpath, f) for f in sorted(os.listdir(dirpath)) if os.path.isfile(os.path.join(dirpath, f)) and f.endswith('.npy')]\n    nframes = []\n    max_frames_melgram_in_dir = 0\n    for f in files:\n        nframes.append(max(max_frames_melgram_in_dir, np.load(f).shape[0]))    \n    return nframes","2e3d7b33":"#Obtem o n\u00famero de frames dos 15 primeiros arquivos\nget_num_melgram_frames_from_dir(target_npy_dir)[:15]","c66c01c6":"def plot_histogram(x, num_bins=50, xlabel='', ylabel='', title=''):\n    fig, ax = plt.subplots()\n\n    # cria o histograma\n    n, bins, patches = ax.hist(x, bins=num_bins)\n\n    # coloca os t\u00edtulos nos eixos e no plot\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    ax.set_title(title)\n\n    # previne o clipping do layout\n    fig.tight_layout()\n    plt.show()","a56adb47":"melgrams_sizes = get_num_melgram_frames_from_dir(target_npy_dir)\nplot_histogram(melgrams_sizes, 20, xlabel = 'Numero de Frames', ylabel = 'Densidade de Probabilidade', title='Histograma de frames')\nmin(melgrams_sizes), max(melgrams_sizes)","63de2918":"max_frames = 1366","a2c478f6":"def build_set(path, selected_files, nclasses, features_range, max_frames_len_out):   \n    print('* Gerando batch...')\n    print('\\tpath:', path)\n    print('Primeiros 10 arquivos selecionados', selected_files[:10])\n    print('\\tnclasses:', nclasses)\n    print('\\tfeatures_range:', features_range)\n    print('\\tmax_frames_len_out:',max_frames_len_out)\n    \n    mel_dims = get_melgram_dimensions_from_file(os.path.join(path, files[0]))\n    print('mel_dims:', mel_dims)\n    \n    features_range[0], features_range[1] = max(0, features_range[0]), min(mel_dims[1], features_range[1])\n    \n    nfiles_out = len(selected_files)\n    nfeatures_out = features_range[1] - features_range[0]\n\n    # pre-alocar mem\u00f3ria para velocidade\n    X = np.zeros((nfiles_out, max_frames_len_out, nfeatures_out))\n    Y = np.zeros((nfiles_out, nclasses))\n\n    print(X.shape, Y.shape)\n    # Construindo treinamento\n    \n    # Tamanho m\u00e1ximo de frames no input\n    max_frames_len_in = 0    \n        \n    for i, file_name in enumerate(selected_files):\n        \n        numpy_features_file_path = os.path.join(path, file_name)\n        max_frames_len_in = max(max_frames_len_in, len(numpy_features_file_path))\n        \n        fsubject = file_name.split('_')[0]\n        fclass = file_name.split('_')[1]\n        \n        aux = np.load(numpy_features_file_path)\n        X[i, 0:min(aux.shape[0], max_frames_len_out), 0:nfeatures_out] = aux[:, features_range[0]:features_range[1]] \n    \n        Y[i] = encode_class(fclass, class_names)\n        \n    print('Tamanho m\u00e1ximo de frames do melgrama no output:', max_frames_len_out)\n    print('Dataset Constru\u00eddo') \n    \n    \n    return X, Y","749ac705":"#preparando para construir os datasets de treino e valida\u00e7\u00e3o\nfiles = [f for f in sorted(os.listdir(target_npy_dir)) if os.path.isfile(os.path.join(target_npy_dir, f)) and f.endswith('.npy')]\nclasses = [f.split('_')[1] for f in files]\nclass_names = sorted(set(classes))\nsubjects = sorted(set([f.split('_')[0] for f in files]))\nprint(\"Quais classes est\u00e3o presentes no dataset:\", class_names)\nprint(\"Primeiras 15 classes:\", classes[:15])\nprint(\"Existem {} pessoas no dataset:\\n\\t{}\".format(len(subjects),subjects))","24472213":"x, y = build_set(target_npy_dir, files, 6, [0, 40], max_frames)","3e7ee0d8":"x.shape, y.shape","9aaf98de":"del x, y","4523d40e":"from keras.models import Sequential\nfrom keras.layers.core import Flatten, Dense, Dropout, Activation\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import CuDNNLSTM\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.advanced_activations import ELU\nfrom keras.layers.wrappers import Bidirectional, TimeDistributed\nfrom keras.engine.topology import Layer, InputSpec\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras import backend as K","9c960bcd":"# c\u00f3digo inspirado em https:\/\/github.com\/fchollet\/keras\/issues\/2151\nclass TemporalMeanPooling(Layer):\n    \"\"\"    \n    This is a custom Keras layer. This pooling layer accepts the temporal\n    sequence output by a recurrent layer and performs temporal pooling,\n    looking at only the non-masked portion of the sequence. The pooling\n    layer converts the entire variable-length hidden vector sequence\n    into a single hidden vector, and then feeds its output to the Dense\n    layer.\n\n    input shape: (nb_samples, nb_timesteps, nb_features)\n    output shape: (nb_samples, nb_features)\n    \"\"\"\n    def __init__(self, **kwargs):\n        super(TemporalMeanPooling, self).__init__(**kwargs)\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[2])    \n    \n    def call(self, x, mask=None): #mask: (nb_samples, nb_timesteps)\n        if mask is None:\n            mask = K.mean(K.ones_like(x), axis=-1)\n        ssum = K.sum(x,axis=-2) #(nb_samples, np_features)\n        mask = K.cast(mask,K.floatx())\n        rcnt = K.sum(mask,axis=-1,keepdims=True) #(nb_samples)\n        return ssum\/rcnt\n\n    def compute_mask(self, input, mask):\n        return None","9ad6c0e9":"def build_model(X, Y):\n    \n    input_shape = X.shape[1:]\n    nb_classes = Y.shape[1]\n    \n    model = Sequential()\n    model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True), input_shape=input_shape))\n    model.add(Dropout(0.2))\n    model.add(TemporalMeanPooling())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(nb_classes))\n    model.add(Activation('softmax'))\n    \n    return model","31915516":"nb_epoch = 50\nverbose = 0\n\nresultados = []\nsetups = []\nmelhor_val_acc = {}","9a5ba24a":"lindex_subject_out = range(len(subjects)) #Para testar todos os atores\nllr = [ 1e-03 ]\nlbatch_size = [ 64 ]\nlfeatures_range = [ [0, 40] ]\nlpath = [ target_npy_dir ]","aa75c52b":"def Train_Test_Audio_LOO(path, subjects, index_subject_out, lr, batch_size, features_range, build_batch_func, max_frames):\n\n    chosen_subject = subjects[index_subject_out]\n    print(\"Pessoa Escolhida: {} (id: {})\".format(chosen_subject, index_subject_out))\n\n    setup = [path, lr, batch_size, features_range, str(build_batch_func).split(' ')[1], chosen_subject]\n\n    files_train = [f for f in files if f.split('_')[0] != chosen_subject]\n    files_test = [f for f in files if f.split('_')[0] == chosen_subject]\n\n\n    if 'X_Train' in locals():\n        del X_train, Y_train, X_test, Y_test\n        \n    if 'model' in locals():\n        K.clear_session()\n        del model, optimizer\n        \n    gc.collect()\n\n\n    # get the data\n    print('Criando dataset de treino')\n    X_train, Y_train = build_batch_func(path, files_train, len(class_names), features_range, max_frames)   \n    \n    print('Criando dataset de teste')\n    X_test, Y_test = build_batch_func(path, files_test, len(class_names), features_range, max_frames)\n\n    print('X_train.shape: {}, Y_train.shape: {}'.format(X_train.shape, Y_train.shape))\n    print('X_test.shape: {}, Y_test.shape: {}'.format(X_test.shape, Y_test.shape))\n\n    # make the model\n    model = build_model(X_train, Y_train)\n    \n    optimizer = Adam(lr=lr)\n    \n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n\n    checkpoint_filepath = 'weights_{}_{}_{}_{}_{}_new.hdf5'.format(subjects[index_subject_out], lr, batch_size, '-'.join([str(x) for x in features_range]), str(build_batch_func).split(' ')[1])\n    checkpointer = ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n    \n    results = model.fit(X_train,\n                        Y_train,\n                        shuffle=True,\n                        batch_size=batch_size,\n                        epochs=nb_epoch,\n                        verbose=verbose,\n                        validation_data=(X_test, Y_test),\n                        callbacks=[checkpointer])\n\n    resultados.append(results)\n\n    # load weights\n    print('Carregando melhor modelo:')\n    model.load_weights(checkpoint_filepath)\n    print (setup)\n    score = model.evaluate(X_test, Y_test, verbose=1)\n    print('Test score:', score[0])\n    print('Test accuracy:', score[1])\n    \n    # plotando accuracy e loss de treino e valida\u00e7\u00e3o\n    plt.figure(1,figsize=(15,4))\n    plt.suptitle(\"Resultados para a pessoa: {}\".format(setup[-1]))\n    plt.subplot(121)\n    plt.title('Accuracy: {}'.format(\"%.3f\" % max(results.history['val_acc'])))\n    a, = plt.plot(results.history['acc'], 'b', label='train acc')\n    b, = plt.plot(results.history['val_acc'], 'r', label='val acc')\n    plt.legend(handles=[a, b])\n    plt.subplot(122)    \n    plt.title('Loss: {}'.format(\"%.4f\" % max(results.history['val_loss'])))\n    a, = plt.plot(results.history['loss'], 'g', label='train loss')\n    b, = plt.plot(results.history['val_loss'], 'k', label='val loss')\n    plt.legend(handles=[a, b])\n    plt.show()\n    \n    return setup, results","234439ce":"aux = []\nfor lr in llr:\n    for index_subject_out in lindex_subject_out:\n        for batch_size in lbatch_size:\n            for features_range in lfeatures_range:\n                for build_batch_func in [build_set]:\n                    aux.append(Train_Test_Audio_LOO(target_npy_dir, subjects, index_subject_out, lr, batch_size, features_range, build_batch_func, max_frames))","cd5ae758":"accs = []\n\nfor index_ in range(len(aux)):\n    setup, results = aux[index_]\n    results = resultados[index_]\n    #print (setup)\n    # plot the training accuracies\n    plt.figure(1,figsize=(15,4))\n    plt.suptitle(\"Resultados para a pessoa: {}\".format(setup[-1]))\n    plt.subplot(121)\n    plt.title('Accuracy: {}'.format(\"%.3f\" % max(results.history['val_acc'])))\n    a, = plt.plot(results.history['acc'], 'b', label='train acc')\n    b, = plt.plot(results.history['val_acc'], 'r', label='val acc')\n    plt.legend(handles=[a, b])\n    plt.subplot(122)    \n    plt.title('Loss: {}'.format(\"%.4f\" % max(results.history['val_loss'])))\n    a, = plt.plot(results.history['loss'], 'g', label='train loss')\n    b, = plt.plot(results.history['val_loss'], 'k', label='val loss')\n    plt.legend(handles=[a, b])\n    plt.show()\n    accs.append(max(results.history['val_acc']))\n    \nprint('=============================================')\nprint('Acur\u00e1cia M\u00e9dia: {}%'.format('%.2f' % (100*np.mean(np.array(accs)))))\nprint('Resultados: {}'.format(accs))\nprint('=============================================')\n","b8a543d0":"# para que o kernel do kaggle n\u00e3o retorne o erro de \"n\u00famero m\u00e1ximo de arquivos <= 5000\"\n#  n\u00f3s vamos comprimir os resultados do experimento e excluir os arquivos que n\u00e3o s\u00e3o mais necess\u00e1rios\n\n!zip -r audio_wav.zip audio_wav\n!rm -rf audio_wav\/\n\n!zip -r audio_npy.zip audio_npy\n!rm -rf audio_npy\/\n\n!zip -r video_webm.zip video_webm\n!rm -rf video_webm\/\n\n!zip -r audio_npy_all.zip audio_npy_all\n!rm -rf audio_npy_all\/\n\n!rm -rf enterface\\ database","c803967d":"# Jos\u00e9 Maia Neto <josemaianeto@hotmail.com>\n# Peterson Katagiri Zilli <peterson.zilli@gmail.com>","26c5d254":"### C. Aplicando uma janela de Hamming a cada frame\n\nA ideia aqui \u00e9 dar \u00eanfase \u00e0 informa\u00e7\u00e3o contida pr\u00f3xima ao centro de cada frame. Por isso vamos aplicar uma janela de Hamming sobre cada um dos frames.\n\nA janela \u00e9 calculada com a equa\u00e7\u00e3o:\n\n$$w[n] = 0.54 \u2212 0.46 cos ( \\frac{2\u03c0n}{N \u2212 1} )$$\nonde $0 \\leq n \\leq N - 1$, $N$ \u00e9 o tamanho da janela","ffe56e11":"### Fun\u00e7\u00f5es Auxiliares para Constru\u00e7\u00e3o do Dataset de Treinamento e Teste","8140c16e":"* criando os frames a partir do sinal","b0bfb753":"## A anatomia de um sistema complexo dentro de uma empresa\n\n<img src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/test.png\" alt=\"Drawing\" style=\"width: 700px;\"\/>\n","4cbf24e0":"## 1. Extrair Features\n\n### O que \u00e9 \"\u00c1udio\"? (E a import\u00e2ncia do conhecimento do dom\u00ednio do problema!)\n\nO dom\u00ednio do problema exige que saibamos mais sobre \u00c1udio: o que \u00e9, como \u00e9 representado e armazenado, que features podem ser geradas?\n\n### Sinal Anal\u00f3gico *vs* Digital\n\n#### Como \u00e9 o \u00c1udio Anal\u00f3gico\n\n\u00c9 um sinal cont\u00ednuo que representa uma onda de som. A figura que segue \u00e9 um desenho um sinal anal\u00f3gico.\n\n<div style=\"text-align:center\">\n    <img alt=\"Sinal Anal\u00f3gico\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/assets_sinal_analogico.png\" width=\"700px\"\/>\n    <font size=\"1\">Sinal Anal\u00f3gico<\/font>\n<\/div>\n<br>\n\nDuas dimens\u00f5es da figura:\n\n* **Tempo**: \u00e9 a medida do tempo tempo, oras.\n\n* **Amplitude**: \u00e9 a medida de mudan\u00e7a de um sinal durante um per\u00edodo.\n\n<div style=\"text-align:center\">\n    <img alt=\"Sinal Anal\u00f3gico\" src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/8a\/Sine_voltage.svg\" width=\"300px\"\/>\n    <font size=\"1\">Sinal Anal\u00f3gico<\/font>\n<\/div>\n<br>\n\nOlha na curva:\n1. Amplitude de pico\n2. Amplitude pico \u00e0 pico\n3. Amplitude RMS\n4. Per\u00edodo da onda\n\nFigura de [wikipedia](https:\/\/en.wikipedia.org\/wiki\/Amplitude)\n\n<br>","8e1b6070":"## Caso-\u00e0-caso ou em escala? A Necessidade de uma Plataforma para Escalar.\n\nPensar na implementa\u00e7\u00e3o de modelos a cada um dos casos de uso \u00e9 um bom come\u00e7o.\n\nEntretanto, se sua empresa espera fazer uso de modelos em escala, uma plataforma de modelagem e deploy de modelos pode ser uma boa ideia\n\n<div style=\"text-align:center\">\n    <img alt=\"Customer Service Strategy\" src=\"https:\/\/notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/michelangelo_0.png\" width=\"500px\"\/>\n    <font size=\"1\">https:\/\/eng.uber.com\/scaling-michelangelo\/<\/font>\n<\/div>\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"Customer Service Strategy\" src=\"https:\/\/notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/michelangelo_1.png\" width=\"500px\"\/>\n    <font size=\"1\">https:\/\/eng.uber.com\/scaling-michelangelo\/<\/font>\n<\/div>\n<br>\n<br>\n<br>","509bd174":"<div style=\"text-align:center\">\n    <img alt=\"Modelos Full\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/assets_ModeloSomente.png\" width=\"700px\"\/>\n    <font size=\"1\">Extra\u00e7\u00e3o de Features de \u00c1udio<\/font>\n<\/div>\n<br>","4e9240d1":"## Obten\u00e7\u00e3o dos Dados\n### Qual \u00e9 o nosso Conjunto de Dados?\n\n* **Qual a base de treinamento e valida\u00e7\u00e3o?**\nUtilizamos o corpus da eNTERFACE'05 emotion database [2], dispon\u00edvel [neste link](http:\/\/www.enterface.net\/results\/). S\u00e3o os dados apresentados no workshop da [eNTERFACE](http:\/\/www.enterface.net\/) de 2005, para o paper do projeto *Multimodal Caricatural Mirror*.\n\n* **Quais s\u00e3o os sentimentos que tem nessa base?**\nOs sentimentos s\u00e3o:\n    * **an**ger: raiva\n    * **di**sgust: nojo\n    * **fe**ar: medo\n    * **ha**ppiness: felicidade\n    * **sa**dness: tristeza\n    * **su**rprise: surpresa","db37dee8":"### Vamos fazer para todos os \u00c1udios!","fcf7520d":"<br>\n<br>\n<br>\n<br>\n<br>\n<center>Texto: <strong>A<\/strong><\/center>\n<br>\n<br>\n<br>\n<br>\n<br>\n<center>(ou <strong>AAA<\/strong>... no melhor dos casos.)<\/center>\n<br>\n<br>\n<br>\n<br>","71c8ef5c":"## Quem somos n\u00f3s?\n\n<img alt=\"Neto\" src=\"https:\/\/avatars1.githubusercontent.com\/u\/27365232?s=400&v=4\" width=\"200px\"\/>\n<strong>Jos\u00e9 Maia Neto<\/strong>: Graduado em Engenharia de Controle e Automa\u00e7\u00e3o com Mestrado em Engenharia El\u00e9trica pela UFMG. Atua como cientista de dados no Santander com foco em problemas envolvendo dados n\u00e3o estruturados. Atuou nesta mesma \u00e1rea no Ita\u00fa Unibanco.\n\n<img alt=\"Peterson\" src=\"https:\/\/avatars2.githubusercontent.com\/u\/13674573?s=460&v=5\" width=\"200px\"\/>\n<strong>Peterson Katagiri Zilli<\/strong>: Lidera um dos times de projetos de IA\/ML no Santander. Foi Coordenador do time de \u00c1udio Analytics no Ita\u00fa. Engenheiro da Computa\u00e7\u00e3o e Mestre pela UNICAMP. \u00c1vido leitor de capas, contracapas e sum\u00e1rios de livros. Adora um churrasco, dias frios e bons papers na m\u00e3o.","dcb97554":"## O que geralmente fazemos primeiro?\n\n* **Parceria com Consultorias**: N\u00e3o \u00e9 um passo obrigat\u00f3rio. Mas, se n\u00e3o temos o conhecimento das t\u00e9cnicas de desenvolvimento e deploy de modelos, podemos nos apoiar no conhecimento de terceiros para construir e entregar os primeiros casos. Se voc\u00ea julga que IA\/ML \u00e9 conhecimento central para o seu neg\u00f3cio, talvez voc\u00ea queira usar este per\u00edodo para aumentar o conhecimento do seu pr\u00f3prio time, utilizando a experi\u00eancia dos terceiros.\n\n* **Buscar Casos simples primeiro**: Geralmente come\u00e7amos a estudar o servi\u00e7o do cliente com t\u00e9cnicas mais simples, mais estudadas, e que nossos colaboradores dominam mais. Buscamos usar estas t\u00e9cnicas a partir de problemas mais simples para resolver ou mais estudados pelo mercado. Buscamos aqueles problemas que podem ser resolvidos por meio de dados estruturados. Problemas relacionados (Churning, propens\u00e3o de contrata\u00e7\u00e3o e outros) podem ser um bom caminho at\u00e9 chegar ao NPS.\n\n* **Quando j\u00e1 nos sentirmos preparados para a cria\u00e7\u00e3o e implanta\u00e7\u00e3o de outros cases, ainda mais complexos**: Sempre tenha no radar os resultados que s\u00e3o esperados pela implanta\u00e7\u00e3o dos casos, quais ser\u00e3o os acion\u00e1veis que ser\u00e3o habilitados e quais s\u00e3o os indicadores que ser\u00e3o medidos.\n\nMachine Learning n\u00e3o \u00e9 mist\u00e9rio e h\u00e1 v\u00e1rias ferramentas que podem te ajudar a organizar e priorizar seus Cases. Um exemplo \u00e9 o Machine Learning Model Canvas [link1](https:\/\/www.louisdorard.com\/machine-learning-canvas) [link2](https:\/\/medium.com\/louis-dorard\/from-data-to-ai-with-the-machine-learning-canvas-part-i-d171b867b047), do Louis Dorard\n\n<div style=\"text-align:center\">\n    <img alt=\"ML Model Canvas\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/MLMCanvas.jpeg\" width=\"500px\"\/>\n    <font size=\"1\">Machine Learning Model Canvas by Louis Dorard<\/font>\n<\/div>\n\nMesmo preparados, n\u00e3o devemos nos descuidar das bases, refor\u00e7ar cada passo do ciclo de desenvolvimento de modelos, e entender como implement\u00e1-lo de forma a \n\n### Buscar juntos o que j\u00e1 foi feito no mercado.\n\nMuitas aplica\u00e7\u00f5es de Machine Learning que buscam melhorar o servi\u00e7o aos clientes j\u00e1 foram implementadas e testadas pelas empresas no mercado. Algumas aplica\u00e7\u00f5es j\u00e1 s\u00e3o t\u00e3o comuns que j\u00e1 foram inclusive catalogadas, como no caso abaixo. Isto pode ajudar voc\u00ea a direcionar os esfor\u00e7os da sua empresa.\n\n* [Machine learning implementation strategy for a customer service center](https:\/\/cloudblogs.microsoft.com\/dynamics365\/bdm\/2018\/02\/07\/machine-learning-implementation-strategy-for-a-customer-service-center\/)\n\n<div style=\"text-align:center\">\n    <img alt=\"Customer Service Strategy\" src=\"https:\/\/notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/CustomerServiceMachineLearningImplementationStrategy.png\" width=\"500px\"\/>\n    <font size=\"1\">Customer Service ML Implementation Strategy by Microsoft<\/font>\n<\/div>","04292d76":"### B. Fatiando o \u00e1udio em Frames\n\nN\u00f3s seguimos o tamanho do frame de 25ms - $frame\\_size = 0.025$ - e stride de 10ms - $frame\\_stride = 0.010$. Ou seja, s\u00e3o 15 ms de overlap.\n\nTamanhos de Frames t\u00edpicos no processamento de \u00e1udio v\u00e3o de 20 ms a 40 ms com 40% \u00e0 60% de overlap entre frames consecutivos. ","fcc8d7da":"## 2. Construir um Modelo","c7c1e549":"## Como as empresas medem a satisfa\u00e7\u00e3o de seus clientes?\n\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/CustomerSatisfaction.png\" alt=\"Satisfa\u00e7\u00e3o do Cliente\" style=\"width: 350px;\"\/>\n    <font size=\"1\">Satisfa\u00e7\u00e3o do Cliente<\/font>\n<\/div>\n<br>\n\n\n### O que \u00e9 a satisfa\u00e7\u00e3o dos clientes?\n#### NPS\n\n**Net Promoter Score (NPS)** \n    \"Em uma escala de 0 a 10 qual a probabilidade de voc\u00ea recomendar a empresa para um amigo ou parente?\"\n\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/NPS.png\" alt=\"NPS\" style=\"width: 700px;\"\/>\n    <font size=\"1\">Net Promoter Score<\/font>\n<\/div>\n<br>\n\n#### Outras M\u00e9tricas?\n\n**Customer Satisfaction Score (CSAT)**\n    \"O qu\u00e3o satisfeito voc\u00ea est\u00e1 com a sua experi\u00eancia?\"\n\n**Customer Effort Score (CES)**\n    \"Quanto esfor\u00e7o foi requerido para utilizar o produto ou servi\u00e7o?\"\n\n## Por que medir NPS? Como isso vira um acion\u00e1vel?\n### O lado do Cliente e o Lado do Atendente\n\n\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/kpis1.png\" alt=\"KPIs de \u00e1udio\" style=\"width: 650px;\"\/>\n    <font size=\"1\">KPIs de \u00c1udio<\/font>\n<\/div>\n<br>\n\n**Indicadores de Qualidade**\n\n<div style=\"text-align:center\">\n    <img src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/kpis2.png\" alt=\"KPIs de \u00e1udio\" style=\"width: 650px;\"\/>\n    <font size=\"1\">KPIs de \u00c1udio<\/font>\n<\/div>\n<br>\n\n\n#### Ouvir a voz do Cliente\n* Personaliza\u00e7\u00e3o de atendimento\n* Melhoria dos servi\u00e7os e produtos da empresa\n\n#### Direcionamento do or\u00e7amento\n* Treinamento dos atendentes\n* Investimento em Sistemas de Software","adfc18e7":"### D. Calculando o Periodograma usando FFT\n\nVamos aplicar uma FFT de $N$-pontos em cada frame para calcular o espectro de frequ\u00eancias - Short-Time Fourier Transform (STFT), onde $N$ \u00e9 tipicamente 256 ou 512.\n\nUsamos $NFFT = 512$ e calculamos o periodograma usando a equa\u00e7\u00e3o:\n\n$$P = \\frac{|FFT(x_i)|^2}{N}$$\n\nonde $x_i$ \u00e9 o $i$-\u00e9simo frame no sinal $x$. Isto pode ser implementado com as linhas a seguir (usamos a fun\u00e7\u00e3o [rfft](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.fft.rfft.html) do numpy):","35b33864":"### Amplitude *vs* Frequ\u00eancia\n\n#### Sinais como Amplitude vs Tempo\n\nQuando reproduzimos os sons digitais, transformamos os sinais em ondas mec\u00e2nicas, que na verdade representam as varia\u00e7\u00f5es da amplitude no tempo.\n\nDesenhamos as varia\u00e7\u00f5es das amplitudes no tempo dos sinais 1, 2 e 1+2, seguindo 1\/8000 amostras por segundo","eb710eac":"# Muito Obrigado!","1eaf457d":"* Calculando o n\u00famero de frames","42c020ac":"# Como o uso Destes Modelos Acontece na Pr\u00e1tica de Grandes Empresas?","88d6c7ab":"* Preenchendo o sinal com Zeros para completar o \u00faltimo frame ","12442019":"## Os d\u00e9bitos t\u00e9cnicos de sistemas de ML\n\nH\u00e1 muitos desafios para a implanta\u00e7\u00e3o de sistemas que usam Machine Learning:\n\n* Necessidade de (re)treino\n* Monitora\u00e7\u00e3o dos resultados das predi\u00e7\u00f5es\n* Versionamento de c\u00f3digo fonte e dos modelos treinados\n* Integra\u00e7\u00e3o com bases de dados e versionamento da base de treinamento\n* ...\n\nE os desafios n\u00e3o param a\u00ed:\n\n<div style=\"text-align:center\">\n    <img alt=\"Customer Service Strategy\" src=\"https:\/\/notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/hiddenMLtechinicalDebts_0.png\" width=\"500px\"\/>\n    <font size=\"1\">https:\/\/papers.nips.cc\/paper\/5656-hidden-technical-debt-in-machine-learning-systems.pdf<\/font>\n<\/div>\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"Customer Service Strategy\" src=\"https:\/\/notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/hiddenMLtechinicalDebts_1.png\" width=\"800px\"\/>\n    <font size=\"1\">https:\/\/papers.nips.cc\/paper\/5656-hidden-technical-debt-in-machine-learning-systems.pdf<\/font>\n<\/div>\n<br>\n<br>\n<br>","08c0a52e":"### E. Computar Bancos de Filtros da Escala de Mel\n\nO \u00faltimo passo para computar os bancos de filtros \u00e9 aplicar filtros triangulares ao espectro de pot\u00eancia para extrair as bandas de frequ\u00eancia. Tipicamente aplicam-se 40 filtros, $nfilt = 40$, na escala Mel.\n\nO objetivo da escala Mel \u00e9 aproximar a percep\u00e7\u00e3o humana n\u00e3o linear do som - n\u00f3s diferenciamos melhor sons a baixas frequencias e menos os sons a altas frequencias.\n\nPara converter entre Hertz ($f$) e Mel ($m$) n\u00f3s usamos as seguintes equa\u00e7\u00f5es:\n\n$$m = 2595 \\log_{10} (1 + \\frac{f}{700})$$\n\n$$f = 700 (10^{m\/2595} - 1) $$\n\n![Filtros de Mel](http:\/\/haythamfayek.com\/assets\/posts\/post1\/mel_filters.jpg) fonte: http:\/\/haythamfayek.com\/2016\/04\/21\/speech-processing-for-machine-learning.html","5d89fb92":"# Como a An\u00e1lise de Sentimento em \u00c1udio pode nos Ajudar a Atender Melhor os Nossos Clientes? \n\n### TDC 2019 S\u00e3o Paulo \/ Trilha _Data Science_\n\n#### Por:[Jos\u00e9 Maia Neto](mailto:josemaianeto@hotmail.com) e [Peterson Katagiri Zilli](mailto:peterson.zilli@gmail.com?subject=TDC 2019)","d63efe4e":"### E com isso, Transformamos os \u00c1udios de Amplitude para Frequ\u00eancia vs Tempo!\n\n<div style=\"text-align:center\">\n    <img alt=\"Modelos Full\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/assets_InputSomente.png\" width=\"700px\"\/>\n    <font size=\"1\">Extra\u00e7\u00e3o de Features de \u00c1udio<\/font>\n<\/div>\n<br>","b14cd83e":"# E porque \u00c1udio?\n\nJ\u00e1 se perguntou o porqu\u00ea de se dar ao trabalho de descobrir emo\u00e7\u00f5es direto do _\u00e1udio_ ao inv\u00e9s de usar _texto_?\n\nSim, o texto est\u00e1 no e-mail, no SMS, nas redes de relacionamento e em tantos outros canais que nosso cliente usa...\n\nMas, nem tudo o que nosso cliente comunica pode ser expresso de forma direta num texto.\n\nVamos Testar?\n\n<div style=\"text-align:center\">\n    <img alt=\"Eureka!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/alex-sheldon-n6EdYIwpqp4-unsplash.jpg\" width=\"500px\"\/>\n    <font size=\"1\">Photo by Alex Sheldon on Unsplash<\/font>\n<\/div>\n<br>\n<div style=\"text-align:center\">\n    <img alt=\"Bung Jump!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/katika-bele-ut_DpeegS_E-unsplash.jpg\" width=\"500px\"\/>\n    <font size=\"1\">Photo by Katika Bele on Unsplash<\/font>\n<\/div>\n<br>\n<div style=\"text-align:center\">\n    <img alt=\"Bocejo!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/kyle-glenn-uFjORuZQczQ-unsplash.jpg\" width=\"500px\"\/>\n    <font size=\"1\">Photo by Kyle Glenn on Unsplash<\/font>\n<\/div>\n<br>\n<div style=\"text-align:center\">\n    <img alt=\"Chuva!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/gage-walker-6LT0b6LmCt4-unsplash.jpg\" width=\"400px\"\/>\n    <font size=\"1\">Photo by Gage Walker on Unsplash.<\/font>\n<\/div>\n<br>\n<div style=\"text-align:center\">\n    <img alt=\"Karate!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/soon-santos-Th0kRmcOyiY-unsplash.jpg\" width=\"500px\"\/>\n    <font size=\"1\">Photo by SOON SANTOS on Unsplash.<\/font>\n<\/div>","73dc3782":"### \u00c1udio Digital PCM\n\nUm PCM \u00e9 uma representa\u00e7\u00e3o do sinal anal\u00f3gico que amostra regularmente da amplitude do sinal anal\u00f3gico quantizado em intervalos uniformes de tempo.\n\n* **Profundidade em bits**: n\u00famero de n\u00edveis diferentes em que uma amostra pode ser quantizada.\n\n<div style=\"text-align:center\">\n    <img alt=\"Profundidade em bits\" src=\"http:\/\/digitalsoundandmusic.schwartzsound.com\/wp-content\/uploads\/2014\/05\/Figure-5.14-Wave-quantized-at-different-bit-depths.png\" width=\"800px\"\/>\n    <font size=\"1\">Profundidade em bits<\/font>\n<\/div>\n<br>\n\n\n* **Taxa de amostragem**: \u00e9 o n\u00famero de amostras por intervalo de tempo geralmente medida em amostras por segundo.\n\n<div style=\"text-align:center\">\n    <img alt=\"Taxa de amostragem\" src=\"http:\/\/artsites.ucsc.edu\/ems\/music\/tech_background\/TE-16\/teces_164.gif\" width=\"600px\"\/>\n    <font size=\"1\">Taxa de amostragem<\/font>\n<\/div>\n<br>\n\n\nOu seja, quanto maior a profundidade em bits e maior a taxa de amostragem, mais pr\u00f3ximo do sinal anal\u00f3gico.\n<div style=\"text-align:center\">\n    <img alt=\"Combina\u00e7\u00e3o entre amostragem e profundidade\" src=\"https:\/\/www.prosoundtraining.com\/site\/wp-content\/uploads\/2010\/01\/Vol36_Sep08_ThinkingDigitally-graph1.png\" width=\"600px\"\/>\n    <font size=\"1\">Combina\u00e7\u00e3o entre amostragem e profundidade<\/font>\n<\/div>\n<br>\n","829fbd14":"### Frequ\u00eancia na escala natural, escala log-Mel, o ouvido humano e o piano? (e o porque esta palestra est\u00e1 em DataScience e n\u00e3o em outra trilha...)\n\nO ouvido humano n\u00e3o distingue as frequencias com a mesma precis\u00e3o e n\u00e3o ouve todas as frequ\u00eancias com a mesma intensidade.\n\nA c\u00f3clea \u00e9 o org\u00e3o respons\u00e1vel pela \n\n![C\u00f3clea](https:\/\/4.bp.blogspot.com\/-gDjxQ4udLOY\/V6nOlVLh4cI\/AAAAAAAAI18\/EIO9BPwIP-4Pk-AxWjiFWyeoO6IgNpNrACLcB\/s1600\/ear-diagram-picture.jpg)\n\nA Percep\u00e7\u00e3o \u00e9 assim:\n\n<img src=\"https:\/\/s3.medel.com\/images\/triformance\/tonotopic-principal-of-the-cochlea.jpg\" alt=\"Percep\u00e7\u00e3o da Coclea versus distancia percorrida no canal\" style=\"width: 400px;\"\/>\n\n\n<img src=\"https:\/\/blog.medel.com\/wp-content\/uploads\/2015\/07\/CCC-Piano-Graphics-2011-EN.jpg\" alt=\"Percep\u00e7\u00e3o da Coclea comparada com o Piano\" style=\"width: 400px;\"\/>\n\n\nAssim, v\u00e1rias escalas baseadas na percep\u00e7\u00e3o surgiram no passado, a fim de simular este efeito natural que existe no humano. uma delas \u00e9 a [escala Mel](https:\/\/en.wikipedia.org\/wiki\/Mel_scale)\n\nEla segue este mapa entre as frequ\u00eancias naturais do som e as percebidas pelo ouvido humano:\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/aa\/Mel-Hz_plot.svg\" alt=\"Mels vs Hertz\" style=\"width: 800px;\"\/>","0d6ac330":"Abaixo, a onda original, antes da pr\u00e9-enfase, para comparar-mos.","e14bb89c":"### A. Aplicando Pr\u00e9-\u00eanfase\n\nVamos transformar o sinal para amplificar as frequ\u00eancias mais altas. A id\u00e9ia \u00e9 balanceramos as magnitudes do espectro das frequ\u00eancias (altas frequ\u00eancias geralmente tem menor amplitude)\n\nA transforma\u00e7\u00e3o \u00e9 $$y[t] = x[t] - \\alpha x[t-1]$$\n\nusamos $\\alpha = 0.95$","6ef66e6f":"# Como Criar Modelos para Tratamento de \u00c1udio\n\nOs modelos mais comuns de Machine Learning que tratam \u00e1udio geralmente trabalham em duas fases.\n\n1. **Extrair Features do \u00c1udio**: conjuntos de _features_ - como MFCC ou log-Mel s\u00e3o extra\u00eddos a partir do \u00e1udio cru,\n\n1. **Criar e Validar um Modelo a Partir das Features**: o modelo de ML \u00e9 treinado e validado\n\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"Modelos Full\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/assets_Modelo.png\" width=\"700px\"\/>\n    <font size=\"1\">Nosso Setup: Extra\u00e7\u00e3o de Features e Modelo de Classifica\u00e7\u00e3o de \u00c1udio<\/font>\n<\/div>\n<br>","460184a2":"### Entendendo os Dados Transformados","712c3bcc":"### F. Normalizar os bancos de filtros\n\nN\u00f3s normalizamos a informa\u00e7\u00e3o nos bancos de filtros - subtraindo a m\u00e9dia de cada coeficiente a partir de todos os frames - a fim de balancear o espectro de frequ\u00eancias e melhorar a raz\u00e3o Sinal\/Ru\u00eddo Signal-to-Noise (SNR).","5505a36c":"## Passo \u00e0 passo para transformar um sinal em frequ\u00eancias na escala log-mel\n\nVamos transformar esta onda dos sinais 1 e 2 somados para o dom\u00ednio da frequ\u00eancia. Para isso, vamos executar os passos:\n\n* A. Aplicar Pr\u00e9-\u00eanfase\n* B. Dividir o sinal em Frames\n* C. Aplicar uma Janela de Hamming em Cada Frame\n* D. Calcular o Peridograma usando FFT****\n* E. Computar Bancos de Filtros na Escala Mel\n* F. Normalizar os Bancos de Filtros","df4a4357":"### A import\u00e2ncia dos dados marcados e como colet\u00e1-los ou marc\u00e1-los\n\nOs modelos que usam DeepLearning geralmente necessitam de muitos dados para serem treinados, e \u00e9 importante que eles sejam marcados de forma correta para a tarefa que se deseja treinar.\n\nOs dados que usamos aqui foram coletados de forma a guardar o \"gabarito\" de que sentimento se referem.\n\nEntretanto, apesar de termos um conjunto muito grande de dados dispon\u00edveis na internet, para que possamos treinar modelos no regime de aprendizado supervisionado, \u00e9 importane que os marquemos da forma correta. Existem muitos servi\u00e7os e ferramentas de anota\u00e7\u00e3o dispon\u00edveis, como o Amazon Mechanical Turk(tm) e \"anotadores\" open-source como o VIA VGG Image Annotator.\n\n<div style=\"text-align:center\">\n    <img alt=\"Bung Jump!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/labelerGut.png\" alt=\"Satisfa\u00e7\u00e3o do Cliente\" style=\"width: 800px;\"\/>\n    <font size=\"1\">Tela de Marca\u00e7\u00e3o de Dados<\/font>\n<\/div>\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"Mechanical Turk\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/amazonMechanicalTurk.png\" width=\"400px\"\/>\n    <font size=\"1\">https:\/\/www.mturk.com\/<\/font>\n<\/div>\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"VIA\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/via_annotator.png\" width=\"400px\"\/>\n    <font size=\"1\">http:\/\/www.robots.ox.ac.uk\/~vgg\/software\/via\/<\/font>\n<\/div>\n<br>\n\n<div style=\"text-align:center\">\n    <img alt=\"VIA\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/via_annotator_audio.png\" width=\"400px\"\/>\n    <font size=\"1\">http:\/\/www.robots.ox.ac.uk\/~vgg\/software\/via\/<\/font>\n<\/div>\n<br>","ced3e586":"# Extraindo as Features de todos os \u00c1udios do Enterface05","7b497f14":"### E como estes sinais aparecem quando as vemos em termos das frequ\u00eancias presentes?","18e01564":"### Como a stft funciona?\n\nVoc\u00ea encontra uma descri\u00e7\u00e3o mais detalhada [aqui](https:\/\/pt.wikipedia.org\/wiki\/Transformada_de_Fourier_de_curto_termo)\n\nEntretanto, para o prop\u00f3sito deste tutorial, basta sabermos que o m\u00e9todo:\n1. 'Fatia' o sinal no dom\u00ednio do tempo - em pequenas fatias de 25ms, por exemplo.\n2. Aplica uma varia\u00e7\u00e3o da Transformada de Fourier para cada fatia do sinal no tempo.\n\nEsta transformada mapeia uma fun\u00e7\u00e3o real unidimensional no tempo $f(t)$ numa fun\u00e7\u00e3o complexa bi-dimensional $F(\\Omega,\\tau)$, na qual geralmente a grandeza correspondente \u00e0s vari\u00e1veis $t$ e $\\tau$ \u00e9 o tempo, e a grandeza correspondente \u00e0 vari\u00e1vel $\\Omega$ \u00e9 a chamada frequ\u00eancia angular.","94a5f8bc":"# Como Construir um Modelo para Classificar Sentimentos a Partir do \u00c1udio\n\n## Ciclo de vida de modelagem\n<div style=\"text-align:center\">\n    <img alt=\"Bung Jump!\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/ml-life-cycle.jpg\" width=\"800px\"\/>\n    <font size=\"1\">Imagem de https:\/\/milkandhoney.ai\/insights\/best-practice-approach-to-machine-learning-model-development<\/font>\n<\/div>\n\n## Formaliza\u00e7\u00e3o do Problema\n\nNa linguagem de praticantes do aprendizado de m\u00e1quina, tratamos aqui de um problema de classifica\u00e7\u00e3o com aprendizado supervisionado.\n\n* **Problema de Classifica\u00e7\u00e3o** [veja mais aqui](https:\/\/en.wikipedia.org\/wiki\/Machine_learning#Types_of_problems_and_tasks): significa que vamos classificar cada entrada em uma das classes\/sentimentos abordados. Dada uma entrada, a sa\u00edda do modelo \u00e9 a classe a que ela pertence!\n\n* **Aprendizado Supervisionado** [veja mais aqui](https:\/\/en.wikipedia.org\/wiki\/Supervised_learning): significa que nosso modelo classificador ser\u00e1 treinado com pares de (entrada, classe) na qual a classe da entrada \u00e9 informada. Estamos ensinando nosso modelo com exemplos!\n\n* **Qual o artigo que nos inspirou?** [Neste link](https:\/\/arxiv.org\/abs\/1706.02901) voc\u00ea encontra o artigo de Che-Wei Huang e Shrikanh S Narayanan, chamado *Characterizing Types of Convolution in Deep Convolutional Recurrent Neural Networks for Robust Speech Emotion Recognition*[1], publicado em 7 Jun 2017.\n\n* **Por que este artigo?** Este artigo prop\u00f5e um modelo de arquitetura bem simples, f\u00e1cil de implementar, e com resultados publicados em cima de um dataset livre, por isso podemos comparar os resultados que obtivermos com o do artigo. Mas, sim! H\u00e1 outros artigos por a\u00ed sobre classifica\u00e7\u00e3o de \u00e1udio, como [este](https:\/\/arxiv.org\/pdf\/1701.08071.pdf).\n\n* **O que desenvolvemos e como testamos?**\nNeste experimento n\u00f3s:\n    - Extra\u00edmos o \u00e1udio dos v\u00eddeos do dataset\n    - Extra\u00edmos features *log-mels* dos \u00e1udios\n    - Constru\u00edmos um classificador entre os 6 tipos de sentimentos (logo a gente vai falar mais sobre os dados e o que s\u00e3o estes 6 tipos de sentimentos!)\n    - Testamos o classificador num setup de *Leave One Subject Out*.\n\n* **Quais os resultados que chegamos?**\n    - Com este modelo simples, classificamos corretamente cerca de **~71%** das vezes (um classificador aleat\u00f3rio em base balanceada tem esperan\u00e7a de classificar ~16%)","90cabf43":"### Fun\u00e7\u00f5es Auxiliares para Obter a Marca\u00e7\u00e3o e a Dimens\u00e3o dos \u00c1udios","25288a23":"### Vamos Criar Ondas de \u00c1udio?","6433d4a8":"# Problema... Onde? O que vamos fazer aqui?\n\n<table>\n    <tr>\n        <td>\n            <div style=\"text-align:center\">\n                <font size=\"3\">Cliente<\/font>\n                <br>\n                <img alt=\"Man on Phone\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/austin-distel-qgdJX9mvMJI-unsplash.jpg\"\n                     width=\"300px\" style=\"-moz-transform: scale(-1, 1);\n-webkit-transform: scale(-1, 1);\n-o-transform: scale(-1, 1);\ntransform: scale(-1, 1);\nfilter: FlipH;\"\/>\n                <font size=\"1\">Photo by Austin Distel on Unsplash<\/font>\n                \n            <\/div>\n        <\/td>\n        <td>\n            <div style=\"text-align:center\">\n                <font size=\"3\">Atendente<\/font>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n                <img alt=\"Man on Service Desk Phone\" src=\"https:\/\/tdc2019assets-petersonzilli.notebooks.azure.com\/petersonzilli\/projects\/tdc2019assets\/raw\/proxyclick-visitor-management-system-wsHvGRpT8Eo-unsplash.jpg\" width=\"450px\"\/>\n                <font size=\"1\">Photo by Proxyclick Visitor Management System on Unsplash<\/font>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n                <br>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>\n    \n<br>\n<br>\n<br>\n\nO cliente tem uma d\u00favida ou um problema e liga para o SAC. Do outro lado da linha, um atendente ouve atentamente o \u00e1udio da liga\u00e7\u00e3o e resolve o problema. O atendente escreve o resumo do atendimento em algumas linhas de texto. O \u00e1udio \u00e9 gravado e guardado durante 5 anos por motivos regulat\u00f3rios.\n<br>\n<br>\n\nAcabou?\n<br>\n<br>\n\nEsta apresenta\u00e7\u00e3o vai mostrar como voc\u00ea pode alavancar o poder deste \u00e1udio puro para detectar automaticamente momentos de stress na liga\u00e7\u00e3o.\nApresentaremos a constru\u00e7\u00e3o de um modelo a partir de dados p\u00fablicos e tamb\u00e9m as dificuldades enfrentadas por n\u00f3s na implanta\u00e7\u00e3o de um modelo similar em bancos."}}