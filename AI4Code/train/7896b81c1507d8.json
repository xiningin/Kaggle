{"cell_type":{"e62fddba":"code","6cf5f7b5":"code","a4aca4a1":"code","be9072e1":"code","0afb6ec0":"code","771cfcd4":"code","f839bfb7":"code","7359717d":"code","bc948b71":"code","7995654f":"code","b8fdcb84":"code","c6b1822d":"code","053415e8":"code","92eb35a2":"code","15d84694":"code","64104d5a":"code","c6b577f5":"code","c96a5195":"code","02d3fa6f":"code","851803d2":"code","724171f4":"code","667c0ed2":"code","3f3a8a2a":"code","47a28b90":"code","36a25df5":"code","01f5491e":"markdown","7f2d4943":"markdown","382fcbb2":"markdown","0bf1e8dc":"markdown","48e1eac8":"markdown","20dec7a5":"markdown","decd7ff5":"markdown","e30558ca":"markdown","0f356372":"markdown","bf6fce15":"markdown","326f591a":"markdown"},"source":{"e62fddba":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6cf5f7b5":"train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')","a4aca4a1":"train.head()","be9072e1":"test.head()","0afb6ec0":"#calculate cardinalities of each categorical variables\ntrain.apply(pd.Series.nunique)","771cfcd4":"train['Cover_Type'].value_counts()","f839bfb7":"plt.title('Class Distribution')\nsns.countplot(data = train, x = 'Cover_Type')\nplt.show;","7359717d":"from sklearn.feature_selection import SelectKBest, f_classif\nX = train.drop(['Cover_Type', 'Id'], axis = 1)\ny = train['Cover_Type']\n#select 10 best features using SelectKBest class\nbestfeatures = SelectKBest(f_classif, k = 10)\nbestfeatures.fit(X,y)\ntrain_scores = pd.DataFrame(bestfeatures.scores_) \ntrain_cols = pd.DataFrame(X.columns)\n#concantenate the two dataframes for better visualization\nbest_features_scores = pd.concat([train_cols, train_scores], axis = 1)\nbest_features_scores.columns = ['feature', 'score']\n#best_features_scores.sort_values('score', ascending = False)\nbest_20_features = best_features_scores.nlargest(20, ['score'])\nbest_20_features\n","bc948b71":"plt.figure(figsize=(6,10))\nsns.barplot(x = 'score', y = 'feature', data = best_20_features[:10]);","7995654f":"from sklearn.ensemble import ExtraTreesClassifier\n\nselector = ExtraTreesClassifier()\nselector.fit(X,y)\nimportant_features = pd.Series(selector.feature_importances_, index=X.columns)\nplt.figure(figsize=(6,10))\nimportant_features.nlargest(10).plot(kind = 'barh');","b8fdcb84":"plt.figure(figsize=(12,8))\nbest_20_cols = list(best_20_features['feature'][:10])\nsns.heatmap(train[best_20_cols].corr(),annot = True, linewidth = 1.0)","c6b1822d":"# Separate training features from target feature\nX_reduced = train[best_20_cols]\ntest_reduced = test[best_20_cols]\ny = train['Cover_Type']\ntest_id = test['Id']","053415e8":"X_reduced.head()","92eb35a2":"test_reduced.head()","15d84694":"# import important libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","64104d5a":"# Split data into training and validation data\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, random_state = 22)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","c6b577f5":"rFModel_1 = RandomForestClassifier(n_estimators=1000)\nrFModel_1.fit(X_train, y_train)","c96a5195":"#print training accuracy\nprint('Training accuracy is :', round(rFModel_1.score(X_train, y_train),2)*100,'%')","02d3fa6f":"#print validation accuracy\npred = rFModel_1.predict(X_test)\nprint(\"The validation accuracy :\", round(accuracy_score(y_test, pred),2)*100,'%')","851803d2":"print('Get the list of hyperparameters used by the current model:\\n')\nrFModel_1.get_params()","724171f4":"from sklearn.model_selection import RandomizedSearchCV\nrFModel_2 = RandomForestClassifier(random_state = 22)\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","667c0ed2":"rf_random = RandomizedSearchCV(estimator = rFModel_2, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, \n                               random_state=22, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X, y)","3f3a8a2a":"pred_grid = rf_random.predict(X_test)\nprint(\"The validation accuracy for grid search :\", round(accuracy_score(y_test, pred_grid),2)*100,'%')","47a28b90":"test.head()","36a25df5":"#get the test data predictions\ntest = test.drop(['Id'], axis = 1)\npred_test = rf_random.predict(test)\npred_test_output = pd.DataFrame({'Id' : test_id, 'Cover_Type' : pred_test})\npred_test_output.to_csv('submission1.csv', index = False)","01f5491e":"# Read Data","7f2d4943":"### Univariate method","382fcbb2":"# Predictions","0bf1e8dc":"# Train The Model","48e1eac8":"### Feature Importance Method","20dec7a5":"## Selecting the best features","decd7ff5":"**A clear case of overfitting as as the validation model is 14% short of training model.**","e30558ca":"## Hyperparameters tuning with RandomSearch Grid","0f356372":"### Correlation Matrix with Heatmap","bf6fce15":"          wow! There are 9 features with over 100 unique values.","326f591a":"## Observe any inbalance in target feature."}}