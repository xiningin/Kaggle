{"cell_type":{"9547ec10":"code","f7191fd4":"code","0e73d8de":"code","cdf6d010":"code","e3a3225d":"code","2a22f841":"code","44f9019f":"code","27cba271":"code","d728ccb3":"code","6b62e850":"code","cca51285":"code","11d44809":"code","c765b763":"code","6e3714c6":"code","d56eacc9":"code","ed993a96":"code","91d4b680":"code","a2a8d63c":"code","ea51fd66":"code","05541e02":"code","d49bcc26":"code","2d36e1b4":"code","81822118":"markdown"},"source":{"9547ec10":"import sys\nimport gc\nimport time\nimport os\nimport logging\nfrom multiprocessing import Pool, current_process\nfrom multiprocessing import log_to_stderr, get_logger\nfrom tqdm import tqdm\nfrom numba import jit\n\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import LSTM,Dropout,Dense,TimeDistributed,Conv1D,MaxPooling1D,Flatten\nfrom keras.models import Sequential\nimport tensorflow as tf\n\nfrom IPython.display import display, clear_output\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")","f7191fd4":"import pyarrow.parquet as pq\nimport pandas as pd\nimport numpy as np","0e73d8de":"%%time \ntrain_set = pq.read_pandas('..\/input\/train.parquet').to_pandas()","cdf6d010":"%%time\nmeta_train = pd.read_csv('..\/input\/metadata_train.csv')","e3a3225d":"@jit('float32(float32[:,:], int32)')\ndef feature_extractor(x, n_part=1000):\n    lenght = len(x)\n    pool = np.int32(np.ceil(lenght\/n_part))\n    output = np.zeros((n_part,))\n    for j, i in enumerate(range(0,lenght, pool)):\n        if i+pool < lenght:\n            k = x[i:i+pool]\n        else:\n            k = x[i:]\n        output[j] = np.max(k, axis=0) - np.min(k, axis=0)\n    return output","2a22f841":"x_train = []\ny_train = []\nfor i in tqdm(meta_train.signal_id):\n    idx = meta_train.loc[meta_train.signal_id==i, 'signal_id'].values.tolist()\n    y_train.append(meta_train.loc[meta_train.signal_id==i, 'target'].values)\n    x_train.append(abs(feature_extractor(train_set.iloc[:, idx].values, n_part=400)))","44f9019f":"del train_set; gc.collect()","27cba271":"y_train = np.array(y_train).reshape(-1,)\nX_train = np.array(x_train).reshape(-1,x_train[0].shape[0])","d728ccb3":"def keras_auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","6b62e850":"n_signals = 1 #So far each instance is one signal. We will diversify them in next step\nn_outputs = 1 #Binary Classification","cca51285":"#Build the model\nverbose, epochs, batch_size = True, 15, 16\nn_steps, n_length = 40, 10\nX_train = X_train.reshape((X_train.shape[0], n_steps, n_length, n_signals))\n# define model\nmodel = Sequential()\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_signals)))\nmodel.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\nmodel.add(TimeDistributed(Dropout(0.5)))\nmodel.add(TimeDistributed(MaxPooling1D(pool_size=2)))\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(n_outputs, activation='sigmoid'))","11d44809":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras_auc])","c765b763":"model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)","6e3714c6":"model.save_weights('model1.hdf5')","d56eacc9":"#%%time\nmeta_test = pd.read_csv('..\/input\/metadata_test.csv')","ed993a96":"def process_chunk(arg):\n    start_index = arg['start_index']\n    chunk_size = arg['chunk_size']\n    \n    # Test set indices start at 8712\n    test_set_start = 8712\n    offset_index = (test_set_start + start_index)\n    \n    # Column name must be a string\n    subset_test = pq.read_pandas('..\/input\/test.parquet', columns=[str(offset_index + j) for j in range(chunk_size)]).to_pandas()    \n    x_test = []\n    for j in range(chunk_size):\n        subset_test_row = subset_test[str(offset_index + j)]\n        x_test.append(abs(feature_extractor(subset_test_row.values, n_part=400)))\n    return x_test","91d4b680":"# Define 21 chunks for processing the test set\n# on multiple cpus. I have choosen to process in chunks of 1000 (plus the remainder)\n# so as to keep within the kernels memory limit\nargs = []\nfor i in range(0, 20000, 1000):\n    args.append({\n        'start_index': i,\n        'chunk_size': 1000\n    })\n    \n# Add a chunk for the remainder\nargs.append({\n    'start_index': 20000,\n    'chunk_size': 337\n})\n\nn_cpu = processes=os.cpu_count()\nprint('n_cpu: ', n_cpu)\n\np = Pool(processes=n_cpu)\n\n# Map the chunk args to the the process_chunk function\nx_test_chunks = p.map(process_chunk, args)\nprint(f\"multi processing complete. len: {len(x_test_chunks)}\")\n\np.close()\np.join()","a2a8d63c":"x_test = [item for sublist in x_test_chunks for item in sublist]\nx_test = np.array(x_test)\nprint('x_test.shape: ', x_test.shape)\nX_test = x_test.reshape((x_test.shape[0], n_steps, n_length, n_signals))","ea51fd66":"del x_test_chunks","05541e02":"preds = model.predict(X_test)\npreds.shape","d49bcc26":"threshpreds = (preds>0.5)*1\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub.target = threshpreds\n\n# Gave me an LB score of ~0.450\nsub.to_csv('submission.csv',index=False)","2d36e1b4":"check_sub = pd.read_csv('submission.csv')\ncheck_sub.head(20)","81822118":"### This is the first part of @afajohn kernel with the addition of multiprocessing on the test set\nCheck out his brilliant kernel here https:\/\/www.kaggle.com\/afajohn\/cnn-lstm-for-signal-classification-lb-0-513\n\nAlso, many thanks to following kernels:\n- For shortening the signals with a simple feature extraction thanks to: https:\/\/www.kaggle.com\/ashishpatel26\/transfer-learning-in-basic-nn\n- For signal denoising and fft: https:\/\/www.kaggle.com\/theoviel\/fast-fourier-transform-denoising"}}