{"cell_type":{"54f84e96":"code","e466cd13":"code","f3bcff63":"code","384d6d61":"code","27028cab":"code","61c2bac9":"code","d4b48129":"code","9521373c":"markdown","07b8bfc2":"markdown","56484623":"markdown","91c04bb1":"markdown","25ca3858":"markdown","52f0b972":"markdown","d3aeb092":"markdown","95cd3ce5":"markdown"},"source":{"54f84e96":"import tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\nIMAGE_HEIGHT = 100\nIMAGE_WIDTH = 100\nIMAGE_CHANNELS = 3\nNETWORK_DEPTH = 4\n\ndata_dir = os.getcwd() + \"\/..\/input\/\" \ndata_dir += os.listdir(data_dir)[0] + \"\/fruits-360\/\"\ntrain_dir = data_dir + \"Training\/\"\nvalidation_dir = data_dir + \"Test\/\"\n\nbatch_size = 60\ninput_size = IMAGE_HEIGHT * IMAGE_WIDTH * NETWORK_DEPTH\nnum_classes = len(os.listdir(train_dir))\n# probability to keep the values after a training iteration\ndropout = 0.8\n\ninitial_learning_rate = 0.001\nfinal_learning_rate = 0.00001\nlearning_rate = initial_learning_rate\n\n# number of iterations to run the training\niterations = 75000\n# number of iterations after we display the loss and accuracy\nacc_display_interval = 1000\n# default number of iterations after we save the model\nsave_interval = 1000\nstep_display_interval = 100\n# use the saved model and continue training\nuseCkpt = False\n# placeholder for probability to keep the network parameters after an iteration\nkeep_prob = tf.placeholder(tf.float32, name='keep_prob')","e466cd13":"# -------------------- Write\/Read TF record logic --------------------\nclass ImageCoder(object):\n    \"\"\"Helper class that provides TensorFlow image coding utilities.\"\"\"\n\n    def __init__(self):\n        # Create a single Session to run all image coding calls.\n        self._sess = tf.Session()\n\n        # Initializes function that decodes RGB JPEG data.\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)\n\n    def decode_jpeg(self, image_data):\n        image = self._sess.run(self._decode_jpeg,\n                               feed_dict={self._decode_jpeg_data: image_data})\n        assert len(image.shape) == 3\n        assert image.shape[2] == 3\n        return image\n\ndef write_image_data(dir_name, tfrecords_name):\n    writer = tf.python_io.TFRecordWriter(tfrecords_name)\n    coder = ImageCoder()\n    image_count = 0\n    index = -1\n    classes_dict = {}\n\n    for folder_name in os.listdir(dir_name):\n        class_path = dir_name + '\/' + folder_name + '\/'\n        index += 1\n        classes_dict[index] = folder_name\n        for image_name in os.listdir(class_path):\n            image_path = class_path + image_name\n            image_count += 1\n            with tf.gfile.FastGFile(image_path, 'rb') as f:\n                image_data = f.read()\n                example = tf.train.Example(\n                    features = tf.train.Features(\n                        feature = {\n                            'label':tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n                            'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=[tf.compat.as_bytes(image_data)]))\n                        }\n                    )\n                )\n                writer.write(example.SerializeToString())\n    writer.close()\n    print(classes_dict)\n    return image_count, classes_dict\n\ndef parse_single_example(serialized_example):\n    features = tf.parse_single_example(\n        serialized_example,\n        features={\n            'image_raw': tf.FixedLenFeature([], tf.string),\n            'label': tf.FixedLenFeature([], tf.int64)\n        }\n    )\n    image = tf.image.decode_jpeg(features['image_raw'], channels=3)\n    image = tf.reshape(image, [100, 100, 3])\n    label = tf.cast(features['label'], tf.int32)\n    return image, label\n","f3bcff63":"def conv(input_tensor, name, kernel_width, kernel_height, num_out_activation_maps, stride_horizontal=1, stride_vertical=1, activation_fn=tf.nn.relu):\n    prev_layer_output = input_tensor.get_shape()[-1].value\n    with tf.variable_scope(name):\n        weights = tf.get_variable('weights', [kernel_height, kernel_width, prev_layer_output, num_out_activation_maps], tf.float32,\n                                  tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32))\n        biases = tf.get_variable(\"bias\", [num_out_activation_maps], tf.float32, tf.constant_initializer(0.0))\n        conv_layer = tf.nn.conv2d(input_tensor, weights, (1, stride_horizontal, stride_vertical, 1), padding='SAME')\n        activation = activation_fn(tf.nn.bias_add(conv_layer, biases))\n        return activation\n\ndef fully_connected(input_tensor, name, output_neurons, activation_fn=tf.nn.relu):\n    n_in = input_tensor.get_shape()[-1].value\n    with tf.variable_scope(name):\n        weights = tf.get_variable('weights', [n_in, output_neurons], tf.float32,\n                                  initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32))\n        biases = tf.get_variable(\"bias\", [output_neurons], tf.float32, tf.constant_initializer(0.0))\n        logits = tf.nn.bias_add(tf.matmul(input_tensor, weights), biases)\n        if activation_fn is None:\n            return logits\n        return activation_fn(logits)\n\ndef max_pool(input_tensor, name, kernel_height, kernel_width, stride_horizontal, stride_vertical):\n    return tf.nn.max_pool(input_tensor,\n                          ksize=[1, kernel_height, kernel_width, 1],\n                          strides=[1, stride_horizontal, stride_vertical, 1],\n                          padding='VALID',\n                          name=name)\n\ndef loss(logits, onehot_labels):\n    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=onehot_labels, name='xentropy')\n    loss = tf.reduce_mean(xentropy, name='loss')\n    return loss\n\ndef update_learning_rate(acc, learn_rate):\n    return learn_rate - acc * learn_rate * 0.9","384d6d61":"# perform data augmentation on images\n# add random hue and saturation\n# randomly flip the image vertically and horizontally\ndef augment_image(image):\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.random_hue(image, 0.02)\n    image = tf.image.random_saturation(image, 0.9, 1.2)\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    return build_hsv_grayscale_image(image)\n\n# converts the image from RGB to HSV and\n# adds a 4th channel to the HSV ones that contains the image in gray scale\n# for test just convert the image to HSV and add the gray scale channel\ndef build_hsv_grayscale_image(image):\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    gray_image = tf.image.rgb_to_grayscale(image)\n    image = tf.image.rgb_to_hsv(image)\n    rez = tf.concat([image, gray_image], 2)\n    return rez","27028cab":"def build_datasets(train_file, test_file, batch_size):\n    train_dataset = tf.data.TFRecordDataset(train_file).repeat()\n    train_dataset = train_dataset.map(parse_single_example).map(lambda image, label: (augment_image(image), label))\n    train_dataset = train_dataset.shuffle(buffer_size=10000, reshuffle_each_iteration=True)\n    train_dataset = train_dataset.batch(batch_size)\n    validation_dataset = tf.data.TFRecordDataset(train_file)\n    validation_dataset = validation_dataset.map(parse_single_example).map(lambda image, label: (build_hsv_grayscale_image(image), label))\n    validation_dataset = validation_dataset.batch(batch_size)\n    test_dataset = tf.data.TFRecordDataset(test_file)\n    test_dataset = test_dataset.map(parse_single_example).map(lambda image, label: (build_hsv_grayscale_image(image), label))\n    test_dataset = test_dataset.batch(batch_size)\n    return train_dataset, validation_dataset, test_dataset\n\ndef train_model(session, train_operation, loss_operation, correct_prediction, iterator_map):\n    global learning_rate\n    time1 = time.time()\n    train_iterator = iterator_map[\"train_iterator\"]\n    validation_iterator = iterator_map[\"validation_iterator\"]\n    validation_init_op = iterator_map[\"validation_init_op\"]\n    train_images_with_labels = train_iterator.get_next()\n    validation_images_with_labels = validation_iterator.get_next()\n    for i in range(1, iterations + 1):\n        batch_x, batch_y = session.run(train_images_with_labels)\n        batch_x = np.reshape(batch_x, [batch_size, input_size])\n        session.run(train_operation, feed_dict={X: batch_x, Y: batch_y})\n\n        if i % step_display_interval == 0:\n            time2 = time.time()\n            print(\"time: %.4f step: %d\" % (time2 - time1, i))\n            time1 = time.time()\n\n        if i % acc_display_interval == 0:\n            acc_value, loss = calculate_intermediate_accuracy_and_loss(session, correct_prediction, loss_operation,\n                                                                       validation_images_with_labels, validation_init_op, train_images_count)\n            learning_rate = update_learning_rate(acc_value, learn_rate=learning_rate)\n            print(\"step: %d loss: %.4f accuracy: %.4f\" % (i, loss, acc_value))\n        if i % save_interval == 0:\n            # save the weights and the meta data for the graph\n            saver.save(session, '.\/model.ckpt')\n            tf.train.write_graph(session.graph_def,'.\/', 'graph.pbtxt')\n\n\ndef calculate_intermediate_accuracy_and_loss(session, correct_prediction, loss_operation, test_images_with_labels, test_init_op, total_image_count):\n    sess.run(test_init_op)\n    loss = 0\n    predicted = 0\n    count = 0\n    total = 0\n    while total < total_image_count:\n        test_batch_x, test_batch_y = session.run(test_images_with_labels)\n        test_batch_x = np.reshape(test_batch_x, [-1, input_size])\n        l, p = session.run([loss_operation, correct_prediction], feed_dict={X: test_batch_x, Y: test_batch_y})\n        loss += l\n        predicted += np.sum(p)\n        count += 1\n        total += len(p)\n    return predicted \/ total_image_count, loss \/ count\n\n\ndef test_model(sess, pred, iterator, total_images, file_name):\n    images_left_to_process = total_images\n    total_number_of_images = total_images\n    images_with_labels = iterator.get_next()\n    correct = 0\n    while images_left_to_process > 0:\n        batch_x, batch_y = sess.run(images_with_labels)\n        batch_x = np.reshape(batch_x, [-1, input_size])\n        # the results of the classification is an array of 1 and 0, 1 is a correct classification\n        results = sess.run(pred, feed_dict={X: batch_x, Y: batch_y})\n        images_left_to_process = images_left_to_process - len(results)\n        correct = correct + np.sum(results)\n        print(\"Predicted %d out of %d; partial accuracy %.4f\" % (correct, total_number_of_images - images_left_to_process,\n                                                                 correct \/ (total_number_of_images - images_left_to_process)))\n    print(\"Final accuracy on %s data: %.8f\" % (file_name, correct \/ total_number_of_images))","61c2bac9":"def conv_net(input_layer, dropout):\n    # number of activation maps for each convolutional layer\n    number_of_act_maps_conv1 = 16\n    number_of_act_maps_conv2 = 32\n    number_of_act_maps_conv3 = 64\n    number_of_act_maps_conv4 = 128\n\n    # number of outputs for each fully connected layer\n    number_of_fcl_outputs1 = 1024\n    number_of_fcl_outputs2 = 256\n\n    input_layer = tf.reshape(input_layer, shape=[-1, IMAGE_HEIGHT, IMAGE_WIDTH, NETWORK_DEPTH])\n\n    conv1 = conv(input_layer, 'conv1', kernel_width=5, kernel_height=5, num_out_activation_maps=number_of_act_maps_conv1)\n    conv1 = max_pool(conv1, 'max_pool1', kernel_height=2, kernel_width=2, stride_horizontal=2, stride_vertical=2)\n\n    conv2 = conv(conv1, 'conv2', kernel_width=5, kernel_height=5, num_out_activation_maps=number_of_act_maps_conv2)\n    conv2 = max_pool(conv2, 'max_pool2', kernel_height=2, kernel_width=2, stride_horizontal=2, stride_vertical=2)\n\n    conv3 = conv(conv2, 'conv3', kernel_width=5, kernel_height=5, num_out_activation_maps=number_of_act_maps_conv3)\n    conv3 = max_pool(conv3, 'max_pool3', kernel_height=2, kernel_width=2, stride_horizontal=2, stride_vertical=2)\n\n    conv4 = conv(conv3, 'conv4', kernel_width=5, kernel_height=5, num_out_activation_maps=number_of_act_maps_conv4)\n    conv4 = max_pool(conv4, 'max_pool4', kernel_height=2, kernel_width=2, stride_horizontal=2, stride_vertical=2)\n\n    flattened_shape = np.prod([s.value for s in conv4.get_shape()[1:]])\n    net = tf.reshape(conv4, [-1, flattened_shape], name=\"flatten\")\n\n    fcl1 = fully_connected(net, 'fcl1', number_of_fcl_outputs1)\n    fcl1 = tf.nn.dropout(fcl1, rate=1 - dropout)\n\n    fcl2 = fully_connected(fcl1, 'fcl2', number_of_fcl_outputs2)\n    fcl2 = tf.nn.dropout(fcl2, rate=1 - dropout)\n\n    out = fully_connected(fcl2, 'out', num_classes, activation_fn=None)\n\n    return out","d4b48129":"# ------------------------------------------------------------\ntrain_images_count, fruit_labels = write_image_data(train_dir, \"train.tfrecord\")\ntest_images_count, _ = write_image_data(validation_dir, \"test.tfrecord\")\n# ------------------------------------------------------------\n\nwith tf.Session() as sess:\n    # placeholder for input layer\n    X = tf.placeholder(tf.float32, [None, input_size], name=\"X\")\n    # placeholder for actual labels\n    Y = tf.placeholder(tf.int64, [None], name=\"Y\")\n    \n    # build the network\n    logits = conv_net(input_layer=X, dropout=dropout)\n    # apply softmax on the final layer\n    prediction = tf.nn.softmax(logits)\n    \n    # calculate the loss using the predicted labels vs the expected labels\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n    # use adaptive moment estimation optimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n    train_op = optimizer.minimize(loss=loss)\n    \n    # calculate the accuracy for this training step\n    correct_prediction = tf.equal(tf.argmax(prediction, 1), Y)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    \n    # input tfrecord file\n    train_file = \"train.tfrecord\"\n    test_file = \"test.tfrecord\"\n    train_dataset, validation_dataset, test_dataset = build_datasets(train_file, test_file, batch_size)\n    \n    train_iterator = train_dataset.make_one_shot_iterator()\n    validation_iterator = tf.data.Iterator.from_structure(validation_dataset.output_types, validation_dataset.output_shapes)\n    validation_init_op = validation_iterator.make_initializer(validation_dataset)\n    iterator_map = {\"train_iterator\": train_iterator,\n                    \"validation_iterator\": validation_iterator,\n                    \"validation_init_op\": validation_init_op}\n    test_iterator = test_dataset.make_one_shot_iterator()\n\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n    sess.run(init)\n    # restore the previously saved value if we wish to continue the training\n    if useCkpt:\n        ckpt = tf.train.get_checkpoint_state(\".\")\n        saver.restore(sess, ckpt.model_checkpoint_path)\n\n    train_model(sess, train_op, loss, correct_prediction, iterator_map)\n    \n    test_model(sess, correct_prediction, test_iterator, test_images_count, test_file)\n    \n    sess.close()","9521373c":"In this section we run the training process, and after its completion, we test the resulted network on the test set.","07b8bfc2":"Here we define methods to perform data augmetation on the input images. Data augmentation is a good way to reduce overfitting on models. \n\n* *augment_image* applies the following operations on the train images\n    1. Alters the hue of the image\n    2. Alters the saturation of the image\n    3. Flips the image horizontally\n    4. Flips the image vertically\n    5. Applies the *build_hsv_grayscale_image* method on the result\n    \nBy altering the hue and saturation, we simulate having a larger variety of fruits in the images. The values with which we alter these properties are small, since in nature there is a small color variance between different fruits of the same species.\n\nFlipping the image horizontally and vertically helps prevent the use the orientation of the fruit as a feature when training. This should result in fruits being correctly classified regardless of their position in an image.\n   \n* *build_hsv_grayscale_image* converts the input image to HSV and adds the fourth grayscale channel","56484623":"Following, there are a few utility methods.\n * *conv* and *fully_connected* combine the TensorFlow method of defining a convolutional layer and a fully connected layer, respectively, adding the bias to the layer and applying a linear rectifier\n  * a convolutional layer consists of groups of neurons that make up kernels \n  * the kernels have a small size but they always have the same depth as the input \n  * the neurons from a kernel are connected to a small region of the input, called the receptive field, because it is highly inefficient to link all neurons to all previous outputs in the case of inputs of high dimensions such as images\n * *max_pool*, *loss* simplify the call to the respective TensorFlow methods\n * *update_learning_rate* dynamically adjusts the learning rate as training progresses","91c04bb1":"In this kernel we present the neural network used in the article [Fruit recognition from images using deep learning](https:\/\/www.researchgate.net\/publication\/321475443_Fruit_recognition_from_images_using_deep_learning).\n\nWe will cover the following parts:\n* Serializing the dataset images to tfrecord files\n* The structure of the neural network\n* Data augmentation done in order to improve the network's performance\n* Training and testing process\n\nThe code is accompanied by comments, which we hope provide better understanding of the logic behind the neural network.","25ca3858":"In this section we define the network structure. This is done according to the following image:\n\n![Network structure](https:\/\/i.imgur.com\/jiWkhAm.png \"Network structure\")\n\n* The first layer (Convolution #1) is a convolutional layer which applies 16 5 x 5 filters. On this layer we apply max pooling with a filter of shape 2 x 2 with stride 2 which specifies that the pooled regions do\nnot overlap (Max-Pool #1). This also reduces the width and height to 50 pixels each.\n\n* The second convolutional (Convolution #2) layer applies 32 5 x 5 filters  which  outputs  32  activation  maps.  We  apply  on  this  layer  the same kind of max pooling(Max-Pool #2) as on the first layer, \nshape 2 x 2 and stride 2.\n\n* The third convolutional (Convolution #3) layer applies 64 5 x 5 filters. Following is another max pool layer(Max-Pool #3) of shape 2 x 2 and stride 2.\n\n* The  fourth  convolutional  (Convolution  #4)  layer  applies  128  5  x  5 filters after which we apply a final max pool layer (Max-Pool #4).\n\n* Because of the four max pooling layers, the dimensions of the representation have each been reduced by a factor of 16, therefore the fifth layer, which is a fully connected layer(Fully Connected #1), \nhas 7 x 7 x 16 inputs.\n\n* This layer feeds into another fully connected layer (Fully Connected #2) with 1024 inputs and 256 outputs.\n\n* The last layer is a softmax loss layer (Softmax) with 256 inputs. The number of outputs is equal to the number of classes.\n\nWe tested several other configurations, however this one obtanied the best accuracy on the Test set.","52f0b972":"Here we define a helper class that converts the images into tfrecords file.  The tfrecord generation logic is roughly based on TensorFlow's own build_image_data.py script.\n\n* *write_image_data* method saves the image raw data together with the correct label for the image in a tfrecord file.\n * the label is encoded as an integer and is the number of the class folder from the Training folder\n * the image is saved as an aray of bytes\n\n* *parse_single_example* method reads from a tfrecord file and extracts the data and label from one image.","d3aeb092":"In the first section we define most of the variables we will further use in this program.\n* *IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS* - the image height, width and depth respectively;\n* *num_classes* - the number of different classes used\n * it is determined by counting the number of folders in the Training folder\n* *batch_size* - the number of images selected in each training\/testing step\n* *dropout* - the probability to keep a node in each training step \n * during training, at each iteration, some nodes are ignored with probability *1-dropout* \n * this results in a reduced network, which is then used for a forward or backward pass\n * dropout prevents neurons from developing co-dependency and, in turn, overfitting\n * outside of training, the dropout is ignored and the entire network is used for classifying\n* *iterations* - the number of steps for which the traning will be done\n* *acc_display_interval* - the number of iterations to train for before displaying the loss and accuracy of the network\n* *step_display_interval* - frequency of displaying the time spent \n* *save_interval* - the number of iterations to train for before saving the graph and checkpoint files","95cd3ce5":"* *build_datasets*  create datasets on the tfrecord files; one to retrieve data for training, one to show the evolution of the accuracy during training, one to evaluate the accuracy on the test set after the training\n    * the first dataset will apply data augmentation and shuffle its elements and will continuously queue new items - used for training\n    * the second dataset will iterate once over the training images - used for evaluating the loss and accuracy during the training; can be reset in order to evaluate as many times as needed\n    * the third dataset will iterate once over all the test images to calculate final accuracy\n    * *batch_size* represents the amount of images randomly selected in each batch\n* *train_model* performs the training of the configured neural network\n * prints the loss and accuracy for a step on start up and every *acc_display_interval* iterations\n * saves the model every *save_interval* iterations\n* *calculate_intermediate_accuracy_and_loss* calculates the loss and accuracy on the training dataset; used during training to monitor the performance of the network\n* *test_model* uses the test images to determine the accuracy of the trained model\n"}}