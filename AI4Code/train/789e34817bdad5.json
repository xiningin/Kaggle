{"cell_type":{"5b310aa8":"code","1d3c6dcd":"code","3a96664c":"code","f521db8b":"code","420d09f7":"code","7845b847":"code","cd081597":"code","c573dd40":"code","1f5f0123":"code","416c83ba":"code","08a0e7da":"code","9940550a":"code","ceb9361f":"code","25e8aa1e":"code","f51fc403":"markdown","a96e96b9":"markdown","ccbb1670":"markdown","4e74f832":"markdown","2a9c491f":"markdown","c83b7a21":"markdown","80d711cc":"markdown","402c4613":"markdown","9268aa70":"markdown","64b4ea30":"markdown","abffeb58":"markdown","78bbf5b1":"markdown","0143330b":"markdown","c4f32e58":"markdown","e8480400":"markdown","7b6f64cc":"markdown"},"source":{"5b310aa8":"def should_hit(player_total, dealer_card_val, player_aces):\n    \"\"\"Return True if the player should hit (request another card) given the current game\n    state, or False if the player should stay. player_aces is the number of aces the player has.\n    \"\"\"\n    return False","1d3c6dcd":"# SETUP. You don't need to worry for now about what this code does or how it works. \n# If you're curious about the code, it's available under an open source license at https:\/\/github.com\/Kaggle\/learntools\/\nfrom learntools.core import binder; binder.bind(globals())\nfrom learntools.python.ex3 import q7 as blackjack\n# Returns a message \"Sorry, no auto-checking available for this question.\" (You can ignore.)\nblackjack.check()\nprint('Setup complete.')","3a96664c":"blackjack.simulate_one_game()","f521db8b":"blackjack.simulate(n_games=50000)","420d09f7":"class Agent(object):\n    # Our policy that maps state to action parameterized by w\n    def policy(self, state):     \n        raise NotImplementedError('You need to overwrite the policy method.')\n        \n    def predict(self, *state):\n        return self.policy(state)\n    \n    def train(self, *state):\n        return self.policy(state)\n    \n    def store_reward(self, reward):\n        pass\n\n    def update(self):\n        pass\n\n    # Vectorized softmax Jacobian\n    @staticmethod\n    def softmax_grad(softmax):\n        s = softmax.reshape(-1,1)\n        return np.diagflat(s) - np.dot(s, s.T)","7845b847":"from random import shuffle\n\n# gfenerator for card drawing without repetition\ndef get_deck():\n    cards = (list(range(1, 9)) + [10] * 4 + [11]) * 4\n    while cards:\n        shuffle(cards)\n        yield cards.pop()\n\ndeck = get_deck()\nfor i in range(2):\n    print(next(deck))","cd081597":"def calculate_total(aces, partial):\n    total = partial\n    for i in range(aces):\n        if partial + 11 > 21:\n            total += 1          # if score > 21, aces have value 1\n        else:\n            total += 11         # atherwise, aces have value 11\n    return total\n\ndef make_environment(dealer_partial, dealer_aces, player_partial, player_aces):\n    return (calculate_total(player_aces, player_partial),\n        calculate_total(dealer_aces, dealer_partial),\n        player_aces)","c573dd40":"def simulate_game(agent, train=False):\n    # init scores and deck\n    dealer_partial = 0\n    dealer_aces = 0\n    player_partial = 0\n    player_aces = 0\n    deck = get_deck()\n    \n    # initial draw for the player\n    for _ in range(2):\n        card = next(deck)\n        if card == 11:\n            player_aces += 1\n        else:\n            player_partial += card\n            \n    # then for the dealer\n    card = next(deck)\n    if card == 11:\n        dealer_aces += 1\n    else:\n        dealer_partial += card\n    \n    # player's turn\n    # draw cards according to the provided policy\n    while getattr(agent, 'train' if train else 'predict')(*make_environment(dealer_partial, dealer_aces, player_partial, player_aces)):\n        card = next(deck)\n        if card == 11:\n            player_aces += 1\n            \n        else:\n            player_partial += card\n            \n        if calculate_total(player_aces, player_partial) > 21:\n            agent.store_reward(-1)  \n            return 0 # return 0 indicating house's victory\n        \n        agent.store_reward(0)\n        \n    # dealer's turn\n    while calculate_total(dealer_aces, dealer_partial) < 17:\n        card = next(deck)\n        if card == 11:\n            dealer_aces += 1\n        else:\n            dealer_partial += card\n            \n    # calculate totals\n    player_total = calculate_total(player_aces, player_partial)\n    dealer_total = calculate_total(dealer_aces, dealer_partial)\n    \n    # return 1 for player's victory, 0 otherwise\n    if dealer_total > 21 or player_total > dealer_total:\n        agent.store_reward(1)\n        return 1\n    \n    agent.store_reward(-1)\n    return 0","1f5f0123":"class Looser(Agent):\n    def predict(self, *state):\n        \"\"\"This should never win, for sanity check\"\"\"\n        return True\n\nfor i in range(1000):\n    assert not simulate_game(Looser())\n    \nprint('All good till here.')","416c83ba":"def simulate(agent, n_games=5, train=False):\n    player_victories = 0\n    for i in range(n_games):\n        player_victories += simulate_game(agent, train)\n        agent.update()\n        \n        if i % 1000 == 0:\n            print(f'{i}\/{n_games} - Player won {player_victories} out of {i} (win rate = {player_victories \/ (i + 1) * 100}%)', end='\\r')\n        \n    print(f'\\nPlayer won {player_victories} out of {n_games} (win rate = {player_victories \/ n_games * 100}%)')\n\nsimulate(Looser(), 1000)","08a0e7da":"import numpy as np\nnp.random.seed(1)\n\nclass REINFORCE(Agent):\n    '''\n    REINFORCE Policy Gradients agent with linear shallow model\n    https:\/\/homes.cs.washington.edu\/~todorov\/courses\/amath579\/reading\/PolicyGradient.pdf\n    https:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n    https:\/\/medium.com\/samkirkiles\/reinforce-policy-gradients-from-scratch-in-numpy-6a09ae0dfe12\n    '''\n    def __init__(self, state_dim, n_actions, learning_rate, gamma):\n        # Init weight\n        self.w = np.random.rand(state_dim, n_actions) * 0.1\n        self.n_actions = n_actions\n        self.lr = learning_rate\n        self.g = gamma\n        self.grads = []\n        self.rewards = []\n                   \n    @staticmethod\n    def preprocess_state(state):\n#         return np.array([state]).reshape((1, -1))\n        return np.array([state[0] \/ 21 - 0.5, state[1] \/ 21 - 0.5, state[2] \/ 4 - 0.5]).reshape((1, -1))\n        \n    # Our policy that maps state to action parameterized by w\n    def policy(self, state):\n        exp = np.exp(state.dot(self.w))\n        probs = exp \/ np.sum(exp)\n        action = np.random.choice(self.n_actions, p=probs[0])\n        return action, probs\n    \n    def predict(self, *state):\n        state = self.preprocess_state(state)\n        return np.argmax(self.policy(state)[1][0])\n        \n    def train(self, *state):\n        state = self.preprocess_state(state)\n        action, probs = self.policy(state)\n        dsoftmax = self.softmax_grad(probs)[action,:]\n        dlog = dsoftmax \/ probs[0, action]\n        grad = state.T.dot(dlog[None,:])\n        self.grads.append(grad)\n        return action\n    \n    def store_reward(self, reward):\n        # Compute gradient and save with reward in memory for our weight update\n        self.rewards.append(reward)\n\n    def update(self):\n        for i in range(len(self.grads)):\n            # Loop through everything that happend in the episode and update towards the log policy gradient times **FUTURE** reward\n            self.w += self.lr * self.grads[i] * sum([r * (self.g ** r) for t, r in enumerate(self.rewards[i:])])\n        self.grads = []\n        self.rewards = []","9940550a":"smart = REINFORCE(3, 2, 0.01, 0.9999)\nsimulate(smart, 200000, train=True)","ceb9361f":"simulate(smart, 50000, train=False)","25e8aa1e":"def should_hit(player_total, dealer_card_val, player_aces):\n    \"\"\"Return True if the player should hit (request another card) given the current game\n    state, or False if the player should stay. player_aces is the number of aces the player has.\n    \"\"\"\n    return smart.predict(player_total, dealer_card_val, player_aces)\n\nblackjack.simulate(n_games=50000)","f51fc403":"# The Blackjack Simulator\n\nRun the cell below to set up our simulator environment:","a96e96b9":"---\nThis exercise is from the **[Python Course](https:\/\/www.kaggle.com\/Learn\/python)** on Kaggle Learn.\n\nCheck out **[Kaggle Learn](https:\/\/www.kaggle.com\/Learn)**  for more instruction and fun exercises.","ccbb1670":"Lets simulate a bunch of games for our dummy agent.","4e74f832":"We will need some cards to play with:","2a9c491f":"The total calculation functions looks moreless like this:","c83b7a21":"Once you have run the set-up code, you can see the action for a single game of blackjack with the following line:","80d711cc":"We'll simulate games between your player agent and our own dealer agent by calling your function. So it must use the name `should_hit`.","402c4613":"# Your Turn\n\nWrite your own `should_hit` function in the cell below. Then run the cell and see how your agent did in repeated play.","9268aa70":"Ok Kaggle, lets see what we can do!\n\nLets first implement an Agent interface and a full blackjack learning environment for our agent to learn.","64b4ea30":"# Blackjack Rules\n\nWe'll use a slightly simplified version of blackjack (aka twenty-one). In this version, there is one player (who you'll control) and a dealer. Play proceeds as follows:\n\n- The player is dealt two face-up cards. The dealer is dealt one face-up card.\n- The player may ask to be dealt another card ('hit') as many times as they wish. If the sum of their cards exceeds 21, they lose the round immediately.\n- The dealer then deals additional cards to himself until either:\n    - The sum of the dealer's cards exceeds 21, in which case the player wins the round, or\n    - The sum of the dealer's cards is greater than or equal to 17. If the player's total is greater than the dealer's, the player wins. Otherwise, the dealer wins (even in case of a tie).\n\nWhen calculating the sum of cards, Jack, Queen, and King count for 10. Aces can count as 1 or 11. (When referring to a player's \"total\" above, we mean the largest total that can be made without exceeding 21. So A+8 = 19, A+8+8 = 17.)\n\n# The Blackjack Player\nYou'll write a function representing the player's decision-making strategy. Here is a simple (though unintelligent) example.\n\n**Run this code cell** so you can see simulation results below using the logic of never taking a new card.","abffeb58":"Now we can simulate one full game!","78bbf5b1":"# Discuss Your Results\n\nHow high can you get your win rate? We have a [discussion thread](https:\/\/www.kaggle.com\/learn-forum\/58735#latest-348767) to discuss your results.","0143330b":"Now we need some better policy than 'always lose'! Lets give it a try.\n\nWe are going to use some reinforcement learning technics, REINFORCE Policy Gradients to be more specific, because why not?","c4f32e58":"# Intro\n\nReady for a quick test of your logic and programming skills?\n\nIn today's micro-challenge, you will write the logic for a blackjack playing program.  Our dealer will test your program by playing 50,000 hands of blackjack. You'll see how frequently your program won, and you can discuss how your approach stacks up against others in the challenge.\n\n![Blackjack](http:\/\/www.hightechgambling.com\/sites\/default\/files\/styles\/large\/public\/casino\/table_games\/blackjack.jpg)","e8480400":"You can see how your player does in a sample of 50,000 games with the following command:","7b6f64cc":"Lets test our code for validity."}}