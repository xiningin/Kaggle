{"cell_type":{"c04d8667":"code","acc86970":"code","0a363397":"code","3ec66f67":"code","747d38a9":"code","458d5f3a":"code","75f162a9":"code","d9af5786":"code","1487d6d5":"code","01d0a35f":"code","3b3baecd":"code","c6613d14":"code","628cb708":"code","575257cb":"code","e61a31a1":"code","e824a158":"code","ce697c5b":"code","58854a9c":"code","47a2fd9a":"code","8edf1f82":"code","2618aa38":"code","a0a5c20e":"code","e5ebc585":"code","7f028343":"code","a9bf4a72":"code","13ad95cf":"code","5f49f341":"code","9ab97e9b":"code","57690c7a":"code","5c05ac5f":"code","a8e77f9e":"markdown","3389012a":"markdown","aa17f032":"markdown","e3c045e9":"markdown","d063a827":"markdown","2517155c":"markdown","c921efbb":"markdown","66ce09cb":"markdown","2d6d7e4f":"markdown","87e37dc3":"markdown","9b90ae55":"markdown","4de9c2cd":"markdown"},"source":{"c04d8667":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","acc86970":"import tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\nimport glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nfrom tensorflow.keras import layers\nimport time\nfrom IPython.display import clear_output\ntf.__version__","0a363397":"import time","3ec66f67":"(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n\ntrain_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\ntrain_images = (train_images - 127.5) \/ 127.5  # Normalize the images to [-1, 1]\n\n","747d38a9":"pd.DataFrame(train_labels[:50]).describe()","458d5f3a":"np.shape(train_images[1]), np.shape(train_labels[1])","75f162a9":"def make_generator_model(latent_dim=100, n_classes=10):\n\t# label input\n\tin_label = layers.Input(shape=(1,), name='label')\n\t# embedding for categorical input\n\tli = layers.Embedding(n_classes, 10)(in_label)\n\t# linear multiplication\n\tn_nodes = 7 * 7\n\tli = layers.Dense(n_nodes)(li)\n\t# reshape to additional channel\n\tli = layers.Reshape((7, 7, 1))(li)\n    \n    \n\t# image generator input\n\tin_lat = layers.Input(shape=(latent_dim,),name='noise')\n\t# foundation for 7x7 image\n\tn_nodes = 128 * 7 * 7\n\tgen = layers.Dense(n_nodes)(in_lat)\n\tgen = layers.LeakyReLU(alpha=0.2)(gen)\n\tgen = layers.Reshape((7, 7, 128))(gen)\n    \n    \n\t# merge image gen and label input\n\tmerge = layers.Concatenate()([gen, li])\n    \n    \n\t# upsample to 14x14\n\tgen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n\tgen = layers.LeakyReLU(alpha=0.2)(gen)\n\t# upsample to 28x28\n\tgen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n\tgen = layers.LeakyReLU(alpha=0.2)(gen)\n\t# output\n\tout_layer = layers.Conv2D(1, (7,7), activation='tanh', padding='same')(gen)\n\t# define model\n\tmodel = tf.keras.Model([in_lat, in_label], out_layer)\n    \n\treturn model\ngenerator = make_generator_model(100,10)\ngenerator.summary()","d9af5786":"#generator({'label':np.array([6,5,4]), 'noise':tf.random.normal([3,100])}, training=False)\n","1487d6d5":"def make_discriminator_model(in_shape=(28,28,1), n_classes=10):\n    in_label = layers.Input(shape=(1,), name='label') # label input\n    li = layers.Embedding(n_classes, 10)(in_label) # embedding for categorical input \n    n_nodes = in_shape[0] * in_shape[1] # scale up to image dimensions with linear activation\n\n    li = layers.Dense(n_nodes)(li) \n    li = layers.Reshape((in_shape[0], in_shape[1], 1))(li) # reshape to additional channel\n\n    in_image = layers.Input(shape=in_shape, name='image') # image input\n\n    merge = layers.Concatenate()([in_image, li]) # concat label as a channel\n\n    fe = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(merge)\n    fe = layers.LeakyReLU(alpha=0.2)(fe)\n\n    fe = layers.Conv2D(128, (3,3), strides=(2,2), padding='same')(fe)\n    fe = layers.LeakyReLU(alpha=0.2)(fe)\n\n    fe = layers.Flatten()(fe)\n\n    fe = layers.Dropout(0.4)(fe)\n\n    out_layer = layers.Dense(1, activation='sigmoid')(fe)\n\n    # define model\n    model = tf.keras.Model([in_image, in_label], out_layer)\n\n    # compile model\n    #opt = rf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n\n    #model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model\n\ndiscriminator = make_discriminator_model()\ndiscriminator.summary()","01d0a35f":"lbl = np.array([6])\nnoise = tf.random.normal([1,100])\ntest = generator({'label':lbl, 'noise':noise}, training=False)\nyo = discriminator({'label':lbl, 'image':test}, training=False)\nyo","3b3baecd":"def discriminator_loss(real_output, fake_output):\n    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n    total_loss = real_loss + fake_loss\n    return total_loss\n    ","c6613d14":"def generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)","628cb708":"cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)","575257cb":"generator_optimizer = tf.keras.optimizers.Adam(1e-4)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(1e-4)","e61a31a1":"@tf.function # compile function\ndef train_step(image_batch, label_batch):\n    noise = tf.random.normal([BATCH_SIZE, 100])\n    #fake_labels = tf.random.uniform(shape=(BATCH_SIZE,1), minval=0, maxval=9, dtype=tf.int32)\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator({'label':label_batch, 'noise':noise}, training=True)\n        \n        real_output = discriminator({'label':label_batch, 'image':image_batch}, training=True)\n        \n        fake_output = discriminator({'label':label_batch, 'image':generated_images}, training=True)\n        \n        # CALCULATE LOSS OF FAKE AND REAL IMAGES\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output, fake_output)\n        \n    # get gradients\n    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n    \n    # \n    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n","e824a158":"BUFFER_SIZE = 60000\nBATCH_SIZE = 256\n\nidx = np.random.permutation(len(train_images))\nx,y = train_images[idx], train_labels[idx]\n\nimage_dataset = tf.data.Dataset.from_tensor_slices(x[:-96]).batch(BATCH_SIZE)\nlabel_dataset = tf.data.Dataset.from_tensor_slices(y[:-96]).batch(BATCH_SIZE)\n\n# indices = tf.range(start=0, limit=tf.shape(image_dataset)[0], dtype=tf.int32)\n# idx = tf.random.shuffle(indices)\n# image_dataset = tf.gather(train_images, idx)\n# label_dataset = tf.gather(train_labels, idx)\n\n\n#dataset = zip(image_dataset, label_dataset).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","ce697c5b":"image_dataset","58854a9c":"np.size(label_dataset)","47a2fd9a":"seed = tf.random.normal([10, 100])\nseed_label = np.array([0,1,2,3,4,5,6,7,8,9])","8edf1f82":"def generate_and_save_images(model, epoch, test_input):\n  # Notice `training` is set to False.\n  # This is so all layers run in inference mode (batchnorm).\n  predictions = generator({'label':seed_label, 'noise':seed}, training=False)\n\n  fig = plt.figure(figsize=(4, 4))\n\n  for i in range(predictions.shape[0]):\n      plt.subplot(4, 4, i+1)\n      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n      plt.axis('off')\n\n  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n  plt.show()","2618aa38":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","a0a5c20e":"# checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n# \n","e5ebc585":"# for image_batch, label_batch in zip(image_dataset, label_dataset):\n#     #train_step(image_batch, label_batch)\n#     #display(plt.imshow(image_batch[9]), label_batch[9])\n#     display(len(image_batch))\n    \n","7f028343":"epochs = 200","a9bf4a72":"for epoch in range(epochs):\n    start = time.time()\n\n    for image_batch, label_batch in zip(image_dataset, label_dataset):\n        train_step(image_batch, label_batch)\n\n# Produce images for the GIF as you go\n    clear_output(wait=True)\n    generate_and_save_images(generator,\n                         epoch + 1,\n                         seed)\n\n# Save the model every 15 epochs\n    if (epoch + 1) % 15 == 0:\n      checkpoint.save(file_prefix = checkpoint_prefix)\n\n    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n\n# Generate after the final epoch\nclear_output(wait=True)\ngenerate_and_save_images(generator,\n                       epochs,\n                       seed)","13ad95cf":"# generator.save('.\/generator')\n# discriminator.save('.\/discriminator')","5f49f341":"# import shutil\n# shutil.make_archive('generator.zip', 'zip', '.\/generator')\n# shutil.make_archive('discriminator.zip', 'zip', '.\/discriminator')\n","9ab97e9b":"NUM = 3\n\nfor n in range(20):\n    test = generator({'label':np.array([NUM]), 'noise':tf.random.normal([1,100])}, training=False)\n    print(lbl)\n    plt.imshow(test[0])\n    plt.show()","57690c7a":"NUM = 2\n\nfor n in range(20):\n    test = generator({'label':np.array([NUM]), 'noise':tf.random.normal([1,100])}, training=False)\n    print(lbl)\n    plt.imshow(test[0])\n    plt.show()","5c05ac5f":"anim_file = 'cdcgan.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('image*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)\n","a8e77f9e":"#### This is just testing, so will just use popular data set (MNIST)","3389012a":"# **TRAINING**","aa17f032":"# Welcome, \n### This is my attempt at creating a conditional Deep Convolutional GAN (Generative Adverserial Network) using Tensorflow 2.0","e3c045e9":"# Time to create **GENERATOR** and **DISCRIMINATOR** model","d063a827":"### Very helpful diagram from matlab on how to train cGAN\n![](https:\/\/www.mathworks.com\/help\/examples\/nnet\/win64\/TrainConditionalGenerativeAdversarialNetworkCGANExample_02.png)","2517155c":"# TESTING AFTER TRAINING","c921efbb":"as you can see here we need to shuffle the image and label, me being stupid shuffle both of them separately and when i train the model, the model produces undesireable results. \nthe solution is to shuffle the indices and then apply those shuffled indices","66ce09cb":"# ######################################","2d6d7e4f":"# Get data and prepare the dataset","87e37dc3":"### Check data shape","9b90ae55":"## 2. The Discriminator\n#### cGAN uses the image and label as the input for the Discriminator as supposed to image only.","4de9c2cd":"### Generator Model Architecture:\nInstead of only noise input, we use label and noise input in cGAN, we then concantenate both of them and transpose it\n"}}