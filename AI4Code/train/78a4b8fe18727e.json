{"cell_type":{"346557b1":"code","f42fbb4a":"code","c19d9408":"code","b39d0fc4":"code","6df183a9":"code","93609bea":"code","529eeb16":"code","9ebc2ce0":"code","8173c878":"code","6d41e1ab":"markdown","1a22b081":"markdown","0aec6c94":"markdown","59b4e4c4":"markdown","c3efe566":"markdown"},"source":{"346557b1":"! pip install trax","f42fbb4a":"import os\n\nimport trax\nfrom trax import data\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as np","c19d9408":"# you need to run this cell twice in Kaggle\n# Reference: https:\/\/www.tensorflow.org\/datasets\/catalog\/ag_news_subset\ntrain_stream = data.TFDS('ag_news_subset', keys=('description', 'label'), train=True)()\neval_stream = data.TFDS('ag_news_subset', keys=('description', 'label'), train=False)()","b39d0fc4":"print(next(train_stream))","6df183a9":"data_pipeline = data.Serial(\n    data.Tokenize(vocab_file='en_8k.subword', keys=[0]),\n    data.Shuffle(),\n    data.FilterByLength(max_length=2048, length_keys=[0]),\n    data.BucketByLength(boundaries=[  32, 128, 512, 2048],\n                             batch_sizes=[512, 128,  32,    8, 1],\n                             length_keys=[0]),\n    data.AddLossWeights()\n)\ntrain_batches_stream = data_pipeline(train_stream)\neval_batches_stream = data_pipeline(eval_stream)\nexample_batch = next(train_batches_stream)\nprint(f'shapes = {[x.shape for x in example_batch]}')","93609bea":"model = tl.Serial(\n    tl.Embedding(vocab_size=8192, d_feature=50),\n    tl.Mean(axis=1),\n    tl.Dense(4),\n    tl.LogSoftmax()\n)\nmodel","529eeb16":"# Training task.\ntrain_task = training.TrainTask(\n    labeled_data=train_batches_stream,\n    loss_layer=tl.CrossEntropyLoss(),\n    optimizer=trax.optimizers.Adam(0.01),\n    n_steps_per_checkpoint=500,\n)\n\n# Evaluaton task.\neval_task = training.EvalTask(\n    labeled_data=eval_batches_stream,\n    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n    n_eval_batches=20  # For less variance in eval numbers.\n)\n\n# Training loop saves checkpoints to output_dir.\noutput_dir = os.path.expanduser('~\/output-dir\/')\n!rm -rf {output_dir}\ntraining_loop = training.Loop(model,\n                              train_task,\n                              eval_tasks=[eval_task],\n                              output_dir=output_dir)\n\n# Run 2000 steps (batches).\ntraining_loop.run(2000)","9ebc2ce0":"inputs, targets, weights = next(eval_batches_stream)","8173c878":"example_input = inputs[0]\nexpected_class = targets[0]\nexample_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')\nprint(f'example input_str: {example_input_str}')\nsentiment_log_probs = model(example_input[None, :])  # Add batch dimension.\nprint(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')\nprint(f'Expected class: {expected_class}')","6d41e1ab":"# Look at predictions","1a22b081":"# Model\n`trax` is really concise, you can use the library of layers available.","0aec6c94":"## The `trax` deep learning framework\n\nReference:\n- https:\/\/github.com\/google\/trax \n\n`trax` is coming out of the Google Brain team and is the latest iteration after almost a decade of work on TensorFlow, machine translation, and Tensor2Tensor. Being a new-comer in a somewhat crowded space (`keras`, `pytorch`, `thinc`), it has been able to learn from the mistakes or the best practices of those APIs.\nIn particular:\n- it is very concise\n- it runs on `TensorFlow` backend\n- it uses `Jax` to speed up tensor-based computation (instead of `numpy`)\n\n## The AG News dataset\n\nA great dataset to look into text classification. https:\/\/www.tensorflow.org\/datasets\/catalog\/ag_news_subset\n\n- 0 is \"World News\"\n- 1 is \"Sports News\"\n- 2 is \"Business News\"\n- 3 is \"Science-Technology News\"","59b4e4c4":"# Training\nFor training, there is the concep of a \"task\" which wraps the data, the optimiser, the metrics etc...","c3efe566":"# Dataset\nI'm not using the Kaggle dataset here, but rather the TensorFlow dataset as it is more convenient with `trax`.\n`trax` needs generators of data. Each element is a tuple (input, target) or (input, target, weight) (usually weight is =1 because all examples have the same importance)."}}