{"cell_type":{"1ed3d676":"code","6071d186":"code","0c3f38f7":"code","3d7ecc73":"code","403d72cc":"code","4edb3dc9":"code","54220b35":"code","32ba0c41":"code","844575b9":"code","95e38531":"code","fde57892":"code","87659e2c":"code","675b722b":"code","8f0a5ba5":"code","27a6cf3b":"code","18e53d99":"code","b15e7a3b":"code","303c7068":"code","9dcf7c23":"code","6536a47a":"code","1c82bca5":"code","514c8c3a":"code","fce0e6c5":"code","ff1b9b7a":"code","287c4cea":"code","c323120b":"code","6a8ae5d5":"code","6fe785f5":"code","dcd59a68":"code","2d51561b":"code","0c42e28a":"code","82743beb":"code","3c279cf3":"code","8aefedd9":"code","a00e86ce":"code","d9d33795":"code","01b731e9":"code","324885fc":"code","77928890":"code","641230b5":"code","3274d943":"code","a2752836":"code","a5c6e94e":"code","5d510074":"code","c294f428":"code","31978932":"code","e719609d":"code","0183fd03":"code","f134ec35":"code","9b9fec93":"code","474e4fbe":"code","4cbaa07b":"markdown","b6197f09":"markdown","0c4b30ac":"markdown","c16ef22c":"markdown","37ea8cc1":"markdown","87ab923d":"markdown","666171b3":"markdown","f1350371":"markdown","17556e11":"markdown","7dc20dee":"markdown","88253dc7":"markdown","dd29e4ac":"markdown","91379cb9":"markdown","2c1cf7c8":"markdown","c9066b1a":"markdown","f7757ddd":"markdown","d1e81dab":"markdown","3e7069d1":"markdown","96eaab7b":"markdown","4ca021d6":"markdown","ddb12219":"markdown","a1c2aa0f":"markdown","8abfe8fa":"markdown","0eea613f":"markdown","2e3136ef":"markdown","2b472cb5":"markdown","6627357f":"markdown","a5dddc54":"markdown","9daa8fcd":"markdown","4e6ea614":"markdown","69a79211":"markdown","238746ad":"markdown","ec91ebcc":"markdown","7b421d16":"markdown","174a0bf8":"markdown","b896693e":"markdown","c8a9c9c9":"markdown","3fca5251":"markdown","0dcdce85":"markdown","7d74018c":"markdown","8d426ca7":"markdown","c4282173":"markdown","de1b3028":"markdown","7356e844":"markdown"},"source":{"1ed3d676":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport scipy as sp\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,r2_score, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom xgboost import XGBRegressor\n \n\nimport warnings\nwarnings.filterwarnings('ignore')","6071d186":"df_train = pd.read_csv('..\/input\/used-cars-price-prediction\/train-data.csv')\ndf_test = pd.read_csv('..\/input\/used-cars-price-prediction\/test-data.csv')","0c3f38f7":"# checking the first five rows of training data\ndf_train.head()","3d7ecc73":"# dropping unnecessary column from both train and test dataset\ndf_train.drop('Unnamed: 0',axis=1,inplace=True)\ndf_test.drop('Unnamed: 0',axis=1,inplace=True)","403d72cc":"# checking the shape of data\ndf_train.shape","4edb3dc9":"# checking the info\ndf_train.info()","54220b35":"# checking for any missing value in data\ndf_train.isnull().sum()","32ba0c41":"# lets count uniques values in each column\nunique_cnt = list(map(lambda x: len(df_train[x].unique()), list(df_train)))\nprint('Unique counts in each column:')\ndict(zip(list(df_train),unique_cnt))","844575b9":"# Let's Drop those Rows which contains NULL values.\n# dropping New_Price as there are many cell which contains NULL value in this column.\ndf_train.drop('New_Price',axis=1,inplace=True)\n\ndf_train.dropna(inplace=True)\nprint(df_train.shape)","95e38531":"def remove_units(data,col = None):\n    for j in range(len(col)):\n        data[col[j]] = data[col[j]].str.split().str[0]\n    \nremove_units(df_train,col = ['Name','Mileage','Engine','Power'])","fde57892":"df_train.head()","87659e2c":"df_train.dtypes","675b722b":"# changing datatype from string to float\nfor col in ['Mileage','Engine']:\n    df_train[col] = df_train[col].astype(float)","8f0a5ba5":"df_train.Power.unique()","27a6cf3b":"df_train[df_train['Power'] == 'null']","18e53d99":"## dropping the rows having string null in Power column\nnull_index = df_train[df_train['Power'] == 'null'].index\ndf_train.drop(null_index,inplace=True)","b15e7a3b":"# lets check the shape again\ndf_train.shape","303c7068":"df_train['Power'] = df_train['Power'].astype(float)","9dcf7c23":"cols = ['Kilometers_Driven', 'Mileage', 'Engine', 'Power','Price']\n\nplt.figure(figsize=(12,6))\nsns.heatmap(df_train[cols].corr(),linewidths=0.5,cmap=\"plasma\", annot=True);\nplt.title(\"Correlation Plot\")\nplt.show()","6536a47a":"df_train['Price'].describe()","1c82bca5":"# lets check the distribution of target variable - price\nplt.figure(figsize=(15,6))\n\nf,ax = plt.subplots(1, sharex=True,)\nmean_price = df_train['Price'].mean()\nmedian_price = df_train['Price'].median()\nmode_price = df_train['Price'].mode().values[0]\n\nsns.distplot(df_train['Price'],ax = ax)\nax.axvline(mean_price, color='r', linestyle='--', label=\"Mean\")\nax.axvline(median_price, color='g', linestyle='-', label=\"Median\")\nax.axvline(mode_price, color='b', linestyle='-', label=\"Mode\")\n\nax.legend()\nplt.xlim()\nplt.show()","514c8c3a":"numerical_Features = ['Kilometers_Driven','Mileage', 'Engine', 'Power','Price']\nfig,axes = plt.subplots(2,3,figsize=(25,15))\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\nfig.suptitle('Distribution of Continous Variables', fontsize=30)\naxes = axes.ravel()\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\nfor i,col in enumerate(numerical_Features):\n    sns.distplot((df_train[col][~pd.isna(df_train[col])]), hist= False,ax=axes[i])\n    axes[i].set_title(col,fontdict = font, fontsize=25)    \nfig.delaxes(axes[-1])","fce0e6c5":"con_cols = ['Kilometers_Driven','Engine', 'Power', \"Price\"]\nfig,axes = plt.subplots(2,3,figsize=(20,15))\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\nfig.suptitle('Distribution of continous variables', fontsize=30)\naxes = axes.ravel()\n\nfor i,col in enumerate(con_cols):\n    sns.distplot((df_train[col][~pd.isna(df_train[col])]).apply(np.log), hist= False,ax=axes[i])\n    axes[i].set_title(col,fontdict = font, fontsize=25)  \nfig.delaxes(axes[-1])","ff1b9b7a":"sns.set(style=\"whitegrid\")\nplt.figure(figsize=(20,10))\ntotal = float(len(df_train))\nax = sns.countplot(x=\"Name\", data=df_train)\nplt.xticks(rotation=90)\nplt.title(\"Count Plot For Car's Brand\", fontsize=20)\nfor p in ax.patches:\n    percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n    x = p.get_x() + p.get_width() \/ 2.\n    y = p.get_height()\n    ax.annotate(percentage, (x, y),ha='center',va='bottom')\nplt.show()","287c4cea":"plt.figure(figsize=(40,10))\nsns.barplot(x=\"Location\", y=\"Price\", data=df_train,palette=\"muted\")\nplt.title(\"Price Across Cities\",fontdict = font, fontsize=40)\nplt.show()","c323120b":"plt.figure(figsize=(20, 10))\nfig = sns.boxplot(x='Year', y=\"Price\", data=df_train)\nfig.axis(ymin=0, ymax=165)\nplt.xticks(rotation=90)\nplt.show()","6a8ae5d5":"# lets create a feature age = current year - year\ncurrent_year = 2021\ndf_train['Age'] = current_year - df_train['Year']","6fe785f5":"# Plot Transmission vs Price\nplt.figure(figsize=(12, 8))\nsns.catplot(y='Price',x='Transmission',data= df_train.sort_values('Price',ascending=False),kind=\"boxen\",height=6, aspect=3)\nplt.show()","dcd59a68":"plt.figure(figsize=(12, 8))\nfig = sns.boxplot(x='Fuel_Type', y=\"Price\", data=df_train)\nfig.axis(ymin=0, ymax=165)\nplt.show()","2d51561b":"plt.figure(figsize=(12, 8))\nfig = sns.boxplot(x='Owner_Type', y=\"Price\", data=df_train)\nplt.show()","0c42e28a":"plt.figure(figsize=(25,10))\nsns.violinplot(x=df_train['Seats'], y=df_train['Price'].apply(np.log),width=1.5)    \nplt.title('Analysis of Price and Seats',fontdict = font, fontsize=25)\nplt.show()","82743beb":"print(df_train['Seats'].value_counts(dropna=False))","3c279cf3":"\ncols = ['Kilometers_Driven', 'Mileage', 'Engine', 'Power']\n\nfig,axes = plt.subplots(2,2,figsize=(25,15),sharey=True)\nfig.subplots_adjust(wspace=0.1, hspace=0.3)\nfig.suptitle('Scatter Plot Of Continous Variables vs Price',fontsize = 20, fontdict=font)\nfig.subplots_adjust(top=0.95)\n\naxes = axes.ravel()\n\nfor i,col in enumerate(cols):\n    #using log transformation\n    x = df_train[col].apply(np.log)\n    y = df_train['Price'].apply(np.log)\n    sns.scatterplot(x, y ,ax=axes[i])\n    ","8aefedd9":"df_train.dtypes","a00e86ce":"# Using get_dummies where data are not in any order and LabelEncoder when data is in order.\n\nordinal_col = 'Owner_Type'\nnominal_col = ['Location','Fuel_Type','Transmission']\n\ndf_train[ordinal_col].replace({\"First\":1,\"Second\":2,\"Third\": 3,\"Fourth & Above\":4},inplace=True)\ndummies = pd.get_dummies(df_train[nominal_col],drop_first=True)\n","d9d33795":"# concat train and dummies df\ndf_train = pd.concat([df_train,dummies],axis=1)\n\n# drop the original columns\ndf_train.drop(nominal_col,axis=1,inplace=True)\n\n# dropping brand names having lot of variation\ndf_train.drop('Name',axis=1,inplace=True)","01b731e9":"#So our target and numerical features except mileage are positive skewed so have to normalize them first.\n\ndef normalize(data,col=None):\n    for col in col:\n        data[col] = np.log1p(data[col])\n        \nnormalize(df_train,col = ['Price'])","324885fc":"# define MinMaxScaler\nscaler = MinMaxScaler()\n# transform data\ndf_train[['Kilometers_Driven','Engine','Mileage','Power']] = scaler.fit_transform(df_train[['Kilometers_Driven','Engine','Mileage','Power']])","77928890":"# I have decided to drop power beacuse it is correlated with engine.\n# also we have already 2 more feature that give us information about the motor: 'Engine', 'Mileage'.\ndf_train.drop('Power',axis=1,inplace=True)\ndf_train.drop('Year',axis=1,inplace=True)\n","641230b5":"df_train.head()","3274d943":"df_train.shape","a2752836":"df_test.head()","a5c6e94e":"print('Shape of Test Data Before Dropping Any Row: ',df_test.shape)\n# Let's Drop those Rows which contains NULL values.\n# dropping New_Price as there are many cell which contains NULL value in this column.\ndf_test.drop('New_Price',axis=1,inplace=True)\n\ndf_test.dropna(inplace=True)\n\nremove_units(df_test,col = ['Name','Mileage','Engine','Power'])\n\n# changing datatype from string to float\nfor col in ['Mileage','Engine']:\n    df_test[col] = df_test[col].astype(float)\n\n## dropping the rows having string null in Power column\nnull_index_test = df_test[df_test['Power'] == 'null'].index\ndf_test.drop(null_index_test,inplace=True)   \n\ndf_test['Power'] = df_test['Power'].astype(float)\n# lets create a feature age = current year - year\ncurrent_year = 2021\ndf_test['Age'] = current_year - df_test['Year']\n\n# Using get_dummies where data are not in any order and LabelEncoder when data is in order.\n\ndf_test[ordinal_col].replace({\"First\":1,\"Second\":2,\"Third\": 3,\"Fourth & Above\":4},inplace=True)\ndummies_test = pd.get_dummies(df_test[nominal_col],drop_first=True)\n\n# concat test and test dummies df\ndf_test = pd.concat([df_test,dummies_test],axis=1)\n\n# drop the original columns\ndf_test.drop(nominal_col,axis=1,inplace=True)\n\n# dropping brand names having lot of variation\ndf_test.drop('Name',axis=1,inplace=True) \n\n# normalize(df_test,col = ['Kilometers_Driven','Engine','Power'])\n# transform data\ndf_test[['Kilometers_Driven','Engine','Mileage','Power']] = scaler.transform(df_test[['Kilometers_Driven','Engine','Mileage','Power']])\n\n# I decided to drop Power because we have already 2 more feature that give us information about the motor: 'Engine', 'Mileage'.\ndf_test.drop('Power',axis=1,inplace=True) \ndf_test.drop('Year',axis=1,inplace=True)\nprint('Shape of Test Data After Dropping Rows: ',df_test.shape)","5d510074":"X = df_train.drop('Price',axis=1)\ny = df_train['Price']","c294f428":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)","31978932":"rf = RandomForestRegressor(n_estimators=300, max_depth = 20, random_state=50)\nrf.fit(X_train, y_train)\ny_pred_train = rf.predict(X_train)","e719609d":"print(\"MSE on training:\", mean_squared_error(y_train, y_pred_train))\n\ny_pred_valid = rf.predict(X_test)\n\nprint(\"MSE on validation:\", mean_squared_error(y_test, y_pred_valid))","0183fd03":"xgb = XGBRegressor()\n\nparam_grid = {\n    \"booster\": ['gbtree'],\n    \"eta\": [0.01],\n    \"gamma\": [0],\n    \"max_depth\": [5],\n    \"lambda\": [0],\n    \"alpha\": [0]\n}\n\nxgb_model = GridSearchCV(estimator=xgb, cv=5, param_grid=param_grid)\nxgb_model.fit(X_train, y_train)","f134ec35":"y_pred_train_xgb = xgb_model.predict(X_train)\n\nprint(\"MSE on training:\", mean_squared_error(y_train, y_pred_train_xgb))\n\ny_pred_valid_xgb = xgb_model.predict(X_test)\n\nprint(\"MSE on validation:\", mean_squared_error(y_test, y_pred_valid_xgb))","9b9fec93":"# All predictions by the model will then be in log values \n# we will need to take the antilog to get the actual value.\nrf_pred = np.floor(np.expm1(rf.predict(df_test)))","474e4fbe":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\n\nlr.fit(X_train,y_train)\npred_lr = lr.predict(X_test)\nprint(\"MSE on validation:\", mean_squared_error(y_test, pred_lr))","4cbaa07b":"**As we can see that there are missing values in some of the features.**","b6197f09":"* Interestly, mean price for 2-seater vehicle are way higher than the others. lets check the count of each category.","0c4b30ac":"### Prediction on Test Data","c16ef22c":"* From the above Plot, Coimbatore is highest in paying for used cars, and Jaipur and Kolkata lowest. This could mean coimbatore is paying comparitively higher prices or coimbatore is in forefront in buying expensive cars which pushes the city to the top and vice versa for Jaipur and Kolkata.","37ea8cc1":"* It is clear from the above figure that the Diesel cars are relatively more costly than the rest.","87ab923d":"**From Random Forest  we got a a very low MSE, so it will be a better model for us.**","666171b3":"#### Analysis of Price and Transmission","f1350371":"### **Exploratory Data Analysis**","17556e11":"### **Prepare Test Data**","7dc20dee":"* Cleary Maruti is most comman brand followed by Hyundai.","88253dc7":"#### Analysis of Price and Year","dd29e4ac":"### **Handling Categorical Features**","91379cb9":"#### Lets try one more model","2c1cf7c8":"**Dataset contains 6019 cars examples and has features like Location, Manufacture details, car features such as Fuel type, Engine, and usage parameters. Some features are numeric attached with units and they need to be removed.**","c9066b1a":"**You can see from the above result, a unique value 'null' which is creating a problem.**","f7757ddd":"#### Analysis of Price and Seats","d1e81dab":"### **Feature Engineering**","3e7069d1":"* 2 seater vehicle count is comparatively low (just 13), it cannot be universally accepted.","96eaab7b":"### **Handling Numerical Features**","4ca021d6":"### Random Forest","ddb12219":"#### Distribution of Continuous Variable- 'Kilometers_Driven','Mileage', 'Engine', 'Power'","a1c2aa0f":"#### Analysis of Price and Continuous Variable","8abfe8fa":"### **Importing Required Libraries**","0eea613f":"### **Loading Data**","2e3136ef":"* Majority of cars having price around 5 - 10 Lakh.\n* There are very minimum cars having price between 50 lakh to 160 lakh.\n* We can see that mean price is greater than median of price, also long tail of distribution is longer on right hand side as compared to left hand side which shows that distribution is positively skewed.","2b472cb5":"**When I tried to change Power to float an error occured (Can't convert str to float : null). so lets check the problem-**","6627357f":"#### Separate Dependent and Independent Variables","a5dddc54":"* Surprisingly Kilometers_Driven doesn't seem to influence target variable and Mileage negatively affects price which could be result of caustion.\n* Both Engine and Power have strong correlation with taget and Also with each other which is pretty inituitive as higher engine capacity imply higher power.\n* There exists a multicolinearity between Engine capacity and power.","9daa8fcd":"### **Model Building**","4e6ea614":"### **Data Cleaning**","69a79211":"#### Analysis of Price across cities","238746ad":"### XGBoost","ec91ebcc":"#### Distribution of Target variable - Price","7b421d16":"#### Analysis of Car Brand","174a0bf8":"# **Used Cars Price Predictions** ","b896693e":"* So with log, the distribution appears close to normal. So we can use this transformation for our numerical features and target variable and move ahead.\n* All predictions by the model will then be in log values and we will need to take the antilog to get the actual value.","c8a9c9c9":"#### Correlation Analysis","3fca5251":" * First time owners are more.","0dcdce85":"* We can see from the above plotted boxplot, as the year increases price of cars also increases. In short Price of Newest models is high compared to old models.","7d74018c":"* Except Milage, rest are highly positive skewed. We can transform it to represent a normal distribution.\n* Lets try with a very general transformation function log and see if that helps here.","8d426ca7":"#### Analysis of Price and Fuel Type","c4282173":"* Engine and Power has a strong association with the price.\n* Where as Price is not impacted much by Kilometer_driven and Mileage.","de1b3028":"#### Analysis of Price and Owner Type","7356e844":"#### Train-Test Split"}}