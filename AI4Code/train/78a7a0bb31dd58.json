{"cell_type":{"944addb7":"code","37da48c8":"code","13b7b83d":"code","97e3bc61":"code","462885fa":"code","85758676":"code","02889eed":"code","9a25a900":"code","62c892ab":"code","f13b2e43":"code","e17af555":"code","e4ad28fc":"code","ef2bf193":"code","21b2f975":"code","14d87ab5":"code","cb8fa4d3":"code","08324d41":"code","ed3a1803":"code","33a2c234":"code","994713a6":"code","e424043b":"code","4c8cacda":"code","1879e219":"markdown","e90ca3b3":"markdown","048fa6d1":"markdown","dcbad7aa":"markdown","f95d23b2":"markdown","9abfb262":"markdown","afddb6da":"markdown","0d5ba0a6":"markdown","3d89fb37":"markdown","b91878b3":"markdown","04da2ac6":"markdown","5592cdbf":"markdown","913cc6c5":"markdown","ed2ac5b0":"markdown","aa734d93":"markdown"},"source":{"944addb7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.cluster import KMeans,DBSCAN\nimport plotly.graph_objs as go\ndef RMSE(Y,Y_HAT):\n    return np.sqrt(mean_squared_error(Y_HAT,Y))\n\n\nplt.rc('figure',figsize=(20,11))\n\ndef set_seed(seed=31415):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()","37da48c8":"p_data = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')\np_data.head(3)\npcopy = p_data.copy()","13b7b83d":"plt.title('Amount Of Missing Values',fontsize=20)\nax = sns.heatmap(pd.DataFrame(p_data.isna().sum()),annot=True,fmt='d')\nplt.show()","97e3bc61":"info = p_data.describe()\ninfo.loc['median'] = p_data.median()\ninfo.loc['skew'] = p_data.skew()\ninfo.loc['kurtosis'] = p_data.kurt()\n\ninfo","462885fa":"plt.subplot(2,1,1)\nplt.title('Distribution Of Median age of a house within a block; a lower number is a newer building',fontsize=20)\nsns.kdeplot(p_data['housing_median_age'],color='teal')\nplt.show()\nplt.subplot(2,1,2)\nplt.title('Distribution Of Median house value for households within a block (measured in US Dollars)',fontsize=20)\nsns.kdeplot(p_data['median_house_value'],color='teal')\nplt.show()","85758676":"plt.subplot(2,1,1)\nplt.title('Distribution Of Room Numbers',fontsize=20)\nsns.kdeplot(p_data['total_rooms'])\nplt.show()\nplt.subplot(2,1,2)\nplt.title('Distribution Of Total Bedrooms',fontsize=20)\nsns.kdeplot(p_data['total_bedrooms'])\nplt.show()","02889eed":"plt.subplot(2,1,1)\nplt.title('Distribution Of The Total number of people residing within a block',fontsize=20)\nsns.kdeplot(p_data['population'],color='green')\nplt.show()\nplt.subplot(2,1,2)\nplt.title('Distribution Of Total number of households, a group of people residing within a home unit, for a block',fontsize=20)\nsns.kdeplot(p_data['households'],color='green')\nplt.show()","9a25a900":"plt.title('Distribution Of The income for households within a block of houses (measured in tens of thousands of US Dollars)',fontsize=20)\nsns.kdeplot(p_data['median_income'],color='teal')\nplt.show()","62c892ab":"ex.pie(p_data,names='ocean_proximity',title='Proportion of Locations of the house w.r.t ocean\/sea')","f13b2e43":"plt.subplot(2,1,1)\nplt.title('Normalized Distribution Of The Total number of people residing within a block',fontsize=20)\np_data['population'] =np.log(p_data['population'])\nsns.kdeplot(p_data['population'],color='green')\nplt.show()\nplt.subplot(2,1,2)\nplt.title('Normalized Distribution Of Total number of households, a group of people residing within a home unit, for a block',fontsize=20)\np_data['households'] =np.log(p_data['households'])\nsns.kdeplot(p_data['households'],color='green')\nplt.show()","e17af555":"plt.subplot(2,1,1)\nplt.title('Normalized Distribution Of Room Numbers',fontsize=20)\np_data['total_rooms'] =np.log(p_data['total_rooms'])\nsns.kdeplot(p_data['total_rooms'])\nplt.show()\nplt.subplot(2,1,2)\nplt.title('Normalized Distribution Of Total Bedrooms',fontsize=20)\np_data['total_bedrooms'] =np.log(p_data['total_bedrooms'])\nsns.kdeplot(p_data['total_bedrooms'])\nplt.show()","e4ad28fc":"ocean_prox_vec = pd.get_dummies(p_data['ocean_proximity']).drop(columns=['NEAR BAY'])\n\nN_COMPONENTS = 2\n\nSVM_T = TruncatedSVD(n_components=N_COMPONENTS)\ndc_mat = SVM_T.fit_transform(ocean_prox_vec)\n\ndesc_ex_var = np.cumsum(SVM_T.explained_variance_ratio_)\n\ntr1 = go.Scatter(x=np.arange(0,len(desc_ex_var)),y=desc_ex_var,name='Cumulative EV')\ntr2 = go.Scatter(x=np.arange(0,len(desc_ex_var)),y=SVM_T.explained_variance_ratio_,name='Individual Component Variance')\nfig = go.Figure(data=[tr1,tr2],\n          layout=dict(title='Ocean Proximity Explained Variance Ratio Using {} Components'.format(N_COMPONENTS),xaxis_title='# Componenets',yaxis_title='Total Variance Explained'))\n\nfig.show()\nop_vct = pd.DataFrame(dc_mat,columns=['Ocean_1','Ocean_2'])\np_data = pd.concat([p_data,op_vct],axis=1)","ef2bf193":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\n\n\ns_val =p_data.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False),\n    row=1, col=1\n)\n\n\ns_val =p_data.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Pearson And Spearman Correlations Between Features\")\nfig.show()","21b2f975":"plt.subplot(2,1,1)\nsns.regplot(data=p_data,x='median_income',y='median_house_value',line_kws=dict(color='red',label='Regression Line'))\nplt.legend()\nplt.show()\nplt.subplot(2,1,2)\nax = sns.regplot(data=p_data,x='Ocean_2',y='median_house_value',line_kws=dict(color='red',label='Regression Line'))\nplt.legend()\nplt.show()\n","14d87ab5":"km_model = KMeans(3)\ncp_data = p_data.copy()\nkm_model.fit(p_data[['median_income','population','households','total_rooms']])\ncp_data['label'] = km_model.labels_\n\n\nfig = make_subplots(\n    rows=3, cols=2,\n    #column_widths=[0.6, 0.4],\n    #row_heights=[0.6, 0.5],\n    \n    specs=[\n           [{\"type\": \"scatter3d\", \"rowspan\": 3}, {\"type\": \"histogram\"}],\n           [            None                    , {\"type\": \"histogram\"}],\n           [            None                    , {\"type\": \"histogram\"}]\n           \n          ])\n\nfig.add_trace(\n    go.Histogram(x=cp_data.query('label==0')['median_house_value'],name='label 0 median_house_price'),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Histogram(x=cp_data.query('label==1')['median_house_value'],name='label 1 median_house_price'),\n    row=2, col=2\n)\nfig.add_trace(\n    go.Histogram(x=cp_data.query('label==2')['median_house_value'],name='label 2 median_house_price'),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Scatter3d(x=cp_data['median_income'], y=cp_data['population'],z=cp_data['total_rooms'],mode='markers',name='Clusters',\n        marker=dict(\n        color=km_model.labels_,                \n        colorscale='Viridis',   \n        opacity=0.8\n    )),\n    row=1, col=1\n)\n\n\nfig.update_layout(scene = dict(\n                    xaxis_title='Median Income',\n                    yaxis_title='Population',\n                    zaxis_title='Total Rooms'),\n                    )\n\nfig.update_layout(title='Clustering Of Location And Distribution Of Cluster Median House Price')\nfig.show()","cb8fa4d3":"cp_data = pd.concat([cp_data,pd.get_dummies(cp_data['label']).drop(columns=[2])],axis=1)\nX = cp_data[['total_rooms','median_income','Ocean_2','Ocean_1','housing_median_age',0,1]].copy()\ny = cp_data['median_house_value']\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)","08324d41":"def evaluate_xgb(lr,train_x,test_x,train_y,test_y):\n    scores = []\n    for i in lr:\n        xgb_model = Pipeline(steps= [\n            ('scale',StandardScaler()),\n            ('xgb',XGBRegressor(random_state=42,verbose=False,learning_rate=i))\n        ])\n        xgb_model.fit(train_x,train_y)\n        scores.append(RMSE(xgb_model.predict(test_x),test_y))\n    return scores\n","ed3a1803":"xgb_scores =evaluate_xgb([0.09,0.08,0.07,0.05,0.03],train_x,test_x,train_y,test_y)","33a2c234":"plt.title('Different Learning Rates XGB Model RMSE',fontsize=20)\nax = sns.lineplot(x=np.arange(0,5),y=xgb_scores)\nplt.xlabel('Learning Rate')\nplt.ylabel(\"RMSE\")\nax.set_xticks(np.arange(0,5))\nax.set_xticklabels([0.09,0.08,0.07,0.05,0.03])\nplt.show()","994713a6":"xgb_model = Pipeline(steps= [\n    ('scale',StandardScaler()),\n    ('xgb',XGBRegressor(random_state=42,verbose=False,learning_rate=0.09))\n])\n\nxgb_model.fit(X,y)\nxgb_prediction = xgb_model.predict(X)\noutput = pd.DataFrame({'Actual':y,'Prediction':xgb_prediction})","e424043b":"plt.title('Current Model Residual Plot',fontsize=20)\nsns.residplot(x=xgb_prediction,y=y)\nplt.show()","4c8cacda":"fig = make_subplots(\n    rows=3, cols=2,subplot_titles=('','Actual','Predictions','Residuals'),\n    vertical_spacing=0.09,\n    specs=[[{\"type\": \"table\",\"rowspan\": 3}     ,{\"type\": \"scatter\"}] ,\n           [None                               ,{\"type\": \"scatter\"}]            ,           \n           [None                               ,{\"type\": \"scatter\"}]                           \n          ]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Actual\"])),\n        y=output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"],\n        mode=\"markers\",\n    ),\n    row=2, col=2\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=np.arange(0,len(output[\"Prediction\"])),\n        y=output[\"Prediction\"]-output[\"Actual\"],\n        mode=\"markers\",\n    ),\n    row=3, col=2\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['Prediction','Actual'],\n            font=dict(size=10),\n            align=\"left\"\n        ),\n        cells=dict(\n            values=[output[k].tolist() for k in output.columns],\n            align = \"left\")\n    ),\n    row=1, col=1\n)\n\n\n\nfig.add_shape(type=\"line\",\n    x0=0, y0=(output[\"Prediction\"]-output[\"Actual\"]).mean(), x1=len(output[\"Prediction\"]), y1=(output[\"Prediction\"]-output[\"Actual\"]).mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x3', \n        yref='y3'\n)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Prediction Evaluation\",\n)\n\nfig.show()","1879e219":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>In comparison to the 4 features, we saw prior to the median income feature (the plot above) we see that there is a slight positive skew but the distribution is fairly normal which is surprising considering we are dealing with median incomes.<\/span><\/p>","e90ca3b3":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Now that we applied a log transformation to the 4 features we saw above with the high right skewness we see that we are left with a fairly normal distriubtion.<\/span><\/p>","048fa6d1":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Future Directions<\/h3>\n","dcbad7aa":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We can clearly see that both features follow a multimodal distribution, meaning we have underlaying groups in our data.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We have no categories in our data set which may indicate the different groups so later in our analysis we will use clustering to try and divide the house value distribution into what will represent the potential underlaying groups.<\/span><\/p>","f95d23b2":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The 4 features above have a strong positive skew which most likely can be derived from the large gaps between house prices in comparison to the average.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will later transform the data in order to normalize our distribution.<\/span><\/p>","9abfb262":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">House Value Clustering<\/h3>\n","afddb6da":"<ol>\n    <li><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'> Evaluate different models (XGB was used in the following kernel by the task requirement)<\/span><\/li>\n    <li><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\">In-depth clustering (it is clear that there are underlying groups in our data can we successfully extract a proxy for those groups ?)<\/span><\/span><\/li>\n    <li><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'><span style=\"color: rgb(0, 0, 0); font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-style: initial; text-decoration-color: initial; float: none; display: inline !important;\">Consider model blending as a way to cover the different behaviors in our data and create an overall stable predictor (minimizing the patterns seen in the residual plot [ heteroskedasticity])<\/span> <\/span><\/li>\n<\/ol>","0d5ba0a6":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h3>\n","3d89fb37":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Libraries And Utilities<\/h3>\n","b91878b3":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>With the current layout besides having high values of RMSE when looking at the residual plot of our predictions, we can observe heteroskedasticity.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>The variance of our residuals isn&apos;t randomly distributed and we can see a certain behavior.<\/span><\/p>","04da2ac6":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">XGB Model Evaluation<\/h3>\n","5592cdbf":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Data Loading And Assessment<\/h3>\n","913cc6c5":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>\n","ed2ac5b0":"<a id=\"1.1\"><\/a>\n<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:200%;text-align:center;border-radius: 15px 50px;\">Correlation Assessment<\/h3>\n","aa734d93":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The ocean proximity feature was encoded into one-hot vectors and reduced to only 2 features which explain 80% of the variance in the feature.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We strive to use the minimum amount of features in our model and it is redundant to use 4 one-hot vectors when only 2 can be used.<\/span><\/p>"}}