{"cell_type":{"d2f44105":"code","a491a7d0":"code","2475e9dc":"code","db02c3ff":"code","d585d7ae":"code","df202064":"code","20fb20d7":"code","b496cb50":"code","25d7c85a":"code","28229993":"code","769fbcb9":"code","c1834b50":"code","552538fd":"code","80dad437":"code","35302c8d":"code","772b0f4f":"code","de667a49":"code","ebd637b0":"code","a9379c30":"code","6eff734e":"code","27596c12":"code","05d32640":"code","c6443221":"code","b1fdbcaa":"code","6e5c002f":"markdown","c40b9967":"markdown","83f690da":"markdown","0e3a70e8":"markdown","4fc022cf":"markdown","aa5046bb":"markdown","6421ec11":"markdown","c9fd207b":"markdown","354b8f86":"markdown","55dac47a":"markdown","02168193":"markdown","7581a71f":"markdown","72db7c8b":"markdown","245e5950":"markdown","8aaa476f":"markdown","ee113504":"markdown","759e4ed8":"markdown","c2d1580a":"markdown","02164958":"markdown","c16aa8eb":"markdown","4349bc82":"markdown"},"source":{"d2f44105":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a491a7d0":"from PIL import Image, ImageDraw\nimport random","2475e9dc":"train_metaData = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/train.csv')\ntrain_metaData.head()","db02c3ff":"test_metaData = pd.read_csv('..\/input\/tensorflow-great-barrier-reef\/test.csv')\ntest_metaData.head()","d585d7ae":"df_train = train_metaData.copy()\ntrain_dir = \"..\/input\/tensorflow-great-barrier-reef\/train_images\"\ndf_train['image_path'] = train_dir + \"\/video_\" + df_train['video_id'].astype(str) + \"\/\" + df_train['video_frame'].astype(str) + \".jpg\"\ndf_train.head()","df202064":"df_train['video_id'].value_counts()","20fb20d7":"plt.bar(x = df_train['video_id'].value_counts().index, height = df_train['video_id'].value_counts())","b496cb50":"df_train.info()","25d7c85a":"num_training_images = len(df_train)\nnum_training_images","28229993":"plt.figure(figsize = (20, 20))\nfor i in range(0, 10):\n    plt.subplot(5, 2, i+1)\n    index = random.randint(0, 23501)\n    img_path = df_train['image_path'].iloc[index]\n    img = Image.open(img_path)\n    plt.imshow(img)","769fbcb9":"img_video_2_10 = plt.imread('..\/input\/tensorflow-great-barrier-reef\/train_images\/video_2\/10.jpg')\nimg_video_2_10.shape","c1834b50":"df_train_annotated = df_train[df_train['annotations'] != '[]']\ndf_train_annotated","552538fd":"(len(df_train_annotated)\/ num_training_images) * 100","80dad437":"df_train.dtypes","35302c8d":"df_train['No_bbox'] = df_train['annotations'].apply(lambda x:x.count('{')) \ndf_train.head()","772b0f4f":"df_train['No_bbox'].value_counts()","de667a49":"plt.figure(figsize = (10,5))\nsns.countplot(x = df_train['No_bbox'])","ebd637b0":"df_train_annotated['No_bbox'] = df_train_annotated['annotations'].apply(lambda x:x.count('{')) \ndf_train_annotated.head()","a9379c30":"df_train_annotated[df_train_annotated['No_bbox'] >= 5]","6eff734e":"df_train_annotated[df_train_annotated['No_bbox'] >= 8]","27596c12":"# this module helps to find out programmatically what the current grammar looks like.\nimport ast\nast.literal_eval(df_train_annotated.iloc[2345].annotations)","05d32640":"def visualize_img_annots(df, id):\n    img_path = df['image_path'][id]\n    img = Image.open(img_path)\n    bounding_boxes = ast.literal_eval(df['annotations'].loc[id])\n                                         \n    for box in bounding_boxes:\n            shape = (box['x'], box['y'], box['x']+box['width'], box['y']+box['height'])\n            ImageDraw.Draw(img).rectangle(shape, outline=180, width=3)\n    display(img)","c6443221":"# Number of bounding boxes = 5\nvisualize_img_annots(df_train_annotated, id=19824)","b1fdbcaa":"# Number of bounding boxes = 11\nvisualize_img_annots(df_train_annotated, id = 9292)","6e5c002f":"* `video_id` - ID number of the video the image was part of. The video ids are not meaningfully ordered.\n* `video_frame` - The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n* `sequence` - ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n* `sequence_frame` - The frame number within a given sequence.\n* `image_id` - ID code for the image, in the format `{video_id}-{video_frame}`\n* `annotations` - The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in test.csv. A bounding box is described by the pixel coordinate `(x_min, y_min)` of its lower left corner within the image together with its `width` and `height` in pixels --> (COCO format).","c40b9967":"All images are of same size so need of resizing.  \nAs you see, Images are really blur and definitely needs to be enhanced before modelling.","83f690da":"Only about 20% of the data is annotated.","0e3a70e8":"# GOAL","4fc022cf":"# DATA VISUALIZATION","aa5046bb":"Doing this to get **indexes** of those images which contain greater number of bounding boxes for **visualization**.","6421ec11":"Doing the same for the annotated data","c9fd207b":"### 1. Predictions take the form of a bounding box together with a confidence score for each identified starfish. \n\n### 2. An image may contain zero or more starfish.\n\n### 3. This competition uses a hidden test set that will be served by an API to ensure you evaluate the images in the same order they were recorded within each video. When your submitted notebook is scored, the actual test data (including a sample submission) will be availabe to your notebook.","354b8f86":"### 1. Accurately identify starfish in real-time by building an object detection model trained on underwater videos of coral reefs.\n\n### 2. Predict the presence and position  crown-of-thorns starfish in sequences of underwater images taken at various times and locations around the Great Barrier Reef.","55dac47a":"# NEED","02168193":"Looking at random images from every video.","7581a71f":"# DATA DESCRIPTION","72db7c8b":"# IMPORTING LIBRARIES","245e5950":"To know more about **ast** library see:\n1. https:\/\/docs.python.org\/3\/library\/ast.html\n2. https:\/\/stackoverflow.com\/questions\/29552950\/when-to-use-ast-literal-eval\/29556591","8aaa476f":"# NOTEBOOK STILL IN PROGRESS","ee113504":"Number of training images","759e4ed8":"Now, we will look at the number of bounding boxes in each image because it is clearly stated that an image may conatin zero or more starfish.","c2d1580a":"Annotations contains the co-ordinate values for the bounding boxes where we will find starfishes.","02164958":"# INSTRUCTIONS","c16aa8eb":"### 1. Great Barrier Reef is under threat because of the overpopulation of one particular starfish \u2013 the coral-eating crown-of-thorns starfish (or COTS for short). \n\n### 2. Underwater cameras will collect thousands of reef images and AI technology could drastically improve the efficiency and scale at which reef managers detect and control COTS outbreaks.","4349bc82":"# [Tensorflow - Help Protect the Great Barrier Reef](https:\/\/www.kaggle.com\/c\/tensorflow-great-barrier-reef)\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31703\/logos\/header.png?t=2021-10-29-00-30-04\">"}}