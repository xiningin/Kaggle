{"cell_type":{"beebef0b":"code","9e023aeb":"code","299db537":"code","29bac1e0":"code","7fffdbb5":"code","74cf7676":"code","a3489cd6":"code","7fd554fb":"code","1be4465d":"code","9b7fafab":"code","87b9a5d4":"code","04eb466c":"code","b61bdfbd":"code","0f70d780":"code","e3115f14":"code","506f2b9b":"code","489ae749":"code","241c2128":"code","f503b180":"code","fa7afc13":"code","0980b993":"code","e7b3075a":"code","06aeff35":"code","b229bfad":"code","d979b47b":"code","d5711182":"code","72e9e712":"code","078d4d7c":"code","c9f0725f":"code","d3c221a3":"code","be868938":"code","862a722f":"code","72f5ac41":"code","7a8a27c1":"code","5877ddbb":"code","c51805e4":"code","2a83ea33":"code","d5e84b0b":"code","32a6940a":"code","9c390e62":"code","ecd57b57":"code","50432824":"code","68a497b9":"code","97030368":"code","8c68669d":"code","e788c4b4":"code","d9de43e1":"code","72acabb0":"code","6598ad1e":"code","9f97883f":"code","0685a548":"code","e6a48574":"code","93cd9d08":"markdown","ed8c086d":"markdown","58fa693f":"markdown","a4cdab5c":"markdown","ba278d75":"markdown","dfa1ad5b":"markdown","2274fe70":"markdown","6b6ec3d1":"markdown","35ca6d2b":"markdown","fb334981":"markdown","6a211f55":"markdown","add387a8":"markdown","2de42dae":"markdown","60b370fd":"markdown","5055635d":"markdown","e884c34b":"markdown","4349c991":"markdown","c34049a8":"markdown","867ba2b7":"markdown","c65be7a9":"markdown","379ec4a4":"markdown"},"source":{"beebef0b":"import numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col = 'Id')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col = 'Id')\n\ntrain.tail(10)","9e023aeb":"test.head(10)","299db537":"train.dtypes.unique()","29bac1e0":"train.select_dtypes(exclude = 'object').describe()","7fffdbb5":"train.select_dtypes(include = ['object']).describe()","74cf7676":"target = train.SalePrice.copy()\ntarget.describe()","a3489cd6":"print('In train data there are: {} categorical features;\\n\\t\\t\\t {} numerical features'.format(train.select_dtypes(include = ['object']).columns.size,\n                                                                                   train.drop('SalePrice', axis = 1).select_dtypes(exclude = ['object']).columns.size))\n\nprint('In test data there are: {} categorical features;\\n\\t\\t\\t {} numerical features.'.format(test.select_dtypes(include = ['object']).columns.size,\n                                                                                   test.select_dtypes(exclude = ['object']).columns.size))","7fd554fb":"(train.drop('SalePrice', axis = 1).columns).equals(test.columns)","1be4465d":"# Setting up Seaborn library\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom scipy import stats","9b7fafab":"sns.set_style('whitegrid')\n# plt.figure(figsize = (16,6))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (24, 6), gridspec_kw={'width_ratios': [3, 2]})\n\nsns.histplot(target, kde = True, color = 'red', stat = 'count', ax = ax1)\nax1.set_title('Histogram of SalePrice', fontsize = 16)\nstats.probplot(target, plot = sns.lineplot(ax = ax2))\nax2.set_title('Probability Plot of SalePrice', fontsize = 16)\nax2.get_lines()[0].set_color('red')\nax2.get_lines()[1].set_color('black')","87b9a5d4":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'red', stat = 'count')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'red')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'scatterplot':\n        for i, column_name in enumerate(data.drop(target, axis = 1).select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.scatterplot(x = data[column_name], y = data[target], color = 'red')\n            plot.set_xlabel(column_name, fontsize = 16)\n    if plot_type == 'boxplot_cat':\n        for i, column_name in enumerate(data.select_dtypes(include = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            sort = data.groupby([column_name])[target].median().sort_values(ascending = False) # This is here to make sure boxes are sorted by median\n            plot = sns.boxplot(x = data[column_name], y = data[target], order = sort.index, palette = 'Reds')\n            plot.set_xlabel(column_name, fontsize = 16)\n    plt.tight_layout()","04eb466c":"# numerical_data = train.drop('SalePrice', axis = 1).select_dtypes(exclude = 'object')\n    \nplot_grid(train.drop('SalePrice', axis = 1), fig_size = (20, 40), grid_size = (12, 3), plot_type = 'histplot')","b61bdfbd":"correlation = train.corr()\nplt.figure(figsize = (20,10))\nsns.heatmap(correlation.loc[::-1,::-1], \n            square = True, \n            vmax = 0.8,)","0f70d780":"# Heatmap of numerical features correlation with target sorted by value of correlation coefficient in descending order\nplt.figure(figsize = (40,20))\nsns.heatmap(correlation.sort_values(by = 'SalePrice', axis = 0, ascending = False).iloc[:,-1:], \n            square = True, \n            annot = True, \n            fmt = '.2f', \n            cbar = False,)","e3115f14":"# Heatmap for first n numerical features that correlate with target the most \nn = 20\nplt.figure(figsize = (32,10))\nsns.heatmap(train[correlation.nlargest(n, 'SalePrice').index].corr(), \n            annot = True, \n            fmt = '.2f', \n            square = True, \n            cbar = False,)","506f2b9b":"plot_grid(train, fig_size = (20, 40), grid_size = (12, 3), plot_type = 'scatterplot', target = 'SalePrice')","489ae749":"plot_grid(train.drop('SalePrice', axis = 1), fig_size = (20, 40), grid_size = (12, 3), plot_type = 'boxplot')","241c2128":"train.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","f503b180":"plot_grid(pd.concat([train[list(train.select_dtypes(include = 'object').nunique().sort_values(ascending = False).index)], \n                     target], axis = 1), \n          fig_size = (20, 40), grid_size = (15, 3), plot_type = 'boxplot_cat', target = 'SalePrice')","fa7afc13":"plot_grid(train[['Neighborhood', 'Exterior2nd', 'Exterior1st', 'SalePrice']], \n          fig_size = (20, 40), grid_size = (3, 1), \n          plot_type = 'boxplot_cat', target = 'SalePrice')","0980b993":"train_cleaning = train.drop('SalePrice', axis = 1).copy()\ntest_cleaning = test.copy()\ntrain_test = pd.concat([train_cleaning, test_cleaning])\nmissing_values = pd.concat([train_test.isnull().sum().sort_values(ascending = False),\n                            train_test.isnull().sum().sort_values(ascending = False).apply(lambda x: (x \/ train_test.shape[0]) * 100)],\n                            axis = 1, keys = ['Values missing', 'Percent of missing'])\nmissing_values[missing_values['Values missing'] > 0].style.background_gradient('Reds')","e7b3075a":"replace_zero = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF1', 'GarageCars', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea', 'BsmtFinSF2']\nreplace_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'BsmtExposure', 'BsmtCond', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType', 'Exterior2nd', 'Exterior1st']\nreplace_mode = ['Functional', 'Utilities', 'KitchenQual', 'SaleType', 'Electrical']\n\n# Replace null values in MSZoning according to MSSubClass\n# train_cleaning.MSZoning = train_cleaning.groupby('MSSubClass')['MSZoning'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntrain_cleaning[replace_zero] = train_cleaning[replace_zero].fillna(0)\n\ntrain_cleaning[replace_none] = train_cleaning[replace_none].fillna('None')\n\nfor col_name in replace_mode:\n    train_cleaning[col_name].replace(np.nan, train_cleaning[col_name].mode()[0], inplace = True)\n\n# Replace null values in test data separately from train data  \ntest_cleaning.MSZoning = test_cleaning.groupby('MSSubClass')['MSZoning'].apply(lambda x: x.fillna(x.mode()[0]))\n\ntest_cleaning[replace_zero] = test_cleaning[replace_zero].fillna(0)\n\ntest_cleaning[replace_none] = test_cleaning[replace_none].fillna('None')\n\nfor col_name in replace_mode:\n    test_cleaning[col_name].replace(np.nan, train_cleaning[col_name].mode()[0], inplace = True)","06aeff35":"train_cleaning.isnull().sum().max()","b229bfad":"test_cleaning.isnull().sum().max()","d979b47b":"# Converting some of the categorical values to numeric ones. Choosing similar values for closer groups to balance linear relations\nneigh_map = {'MeadowV': 1, \n             'IDOTRR': 1, \n             'BrDale': 1,\n             'BrkSide': 2,\n             'OldTown': 2,\n             'Edwards': 2,\n             'Sawyer': 3,\n             'Blueste': 3,\n             'SWISU': 3,\n             'NPkVill': 3,\n             'NAmes': 3,\n             'Mitchel': 4,\n             'SawyerW': 5,\n             'NWAmes': 5,\n             'Gilbert': 5,\n             'Blmngtn': 5,\n             'CollgCr': 5,\n             'ClearCr': 6,\n             'Crawfor': 6,\n             'Veenker': 7,\n             'Somerst': 7,\n             'Timber': 8,\n             'StoneBr': 9,\n             'NridgHt': 10,\n             'NoRidge': 10}\ntrain_cleaning['Neighborhood'] = train_cleaning['Neighborhood'].map(neigh_map).astype('int')\ntest_cleaning['Neighborhood'] = test_cleaning['Neighborhood'].map(neigh_map).astype('int')\n\n# Replacing misspelled values\ntest_cleaning['Exterior2nd'] = test_cleaning['Exterior2nd'].apply(lambda x: 'BrkComm' if (x == 'Brk Cmn') else 'CemntBd' if (x == 'CmentBd') else x)\ntrain_cleaning['Exterior2nd'] = train_cleaning['Exterior2nd'].apply(lambda x: 'BrkComm' if (x == 'Brk Cmn') else 'CemntBd' if (x == 'CmentBd') else x)\n# Creating new simple feature\ntrain_cleaning['ExteriorSame'] = (train_cleaning['Exterior1st'] == train_cleaning['Exterior2nd']).apply(lambda x: 1 if x == True else 0)\ntest_cleaning['ExteriorSame'] = (test_cleaning['Exterior1st'] == test_cleaning['Exterior2nd']).apply(lambda x: 1 if x == True else 0)\n\next1_map = {'None': 0, \n            'BrkComm': 1, \n            'AsphShn': 2,\n            'CBlock': 2,\n            'AsbShng': 3,\n            'WdShing': 4,\n            'Wd Sdng': 5,\n            'MetalSd': 5,\n            'Stucco': 6,\n            'HdBoard': 7,\n            'BrkFace': 8,\n            'Plywood': 8,\n            'VinylSd': 9,\n            'CemntBd': 10,\n            'Stone': 11,\n            'ImStucc': 12}\ntrain_cleaning['Exterior1st'] = train_cleaning['Exterior1st'].map(ext1_map).astype('int')\ntest_cleaning['Exterior1st'] = test_cleaning['Exterior1st'].map(ext1_map).astype('int')\n\next2_map = {'None': 0, \n            'BrkComm': 4, \n            'AsphShn': 3,\n            'CBlock': 1,\n            'AsbShng': 2,\n            'WdShing': 4,\n            'Wd Sdng': 3,\n            'Wd Shng': 3,\n            'MetalSd': 3,\n            'Stucco': 4,\n            'HdBoard': 5,\n            'BrkFace': 6,\n            'Plywood': 6,\n            'VinylSd': 9,\n            'CemntBd': 10,\n            'Stone': 7,\n            'ImStucc': 8,\n            'Other': 11}\ntrain_cleaning['Exterior2nd'] = train_cleaning['Exterior2nd'].map(ext2_map).astype('int')\ntest_cleaning['Exterior2nd'] = test_cleaning['Exterior2nd'].map(ext2_map).astype('int')","d5711182":"qual_map = {'None': 0, \n            'Po': 1, \n            'Fa': 2, \n            'TA': 3, \n            'Gd': 4, \n            'Ex': 5}\ntrain_cleaning['ExterQual'] = train_cleaning['ExterQual'].map(qual_map).astype('int')\ntest_cleaning['ExterQual'] = test_cleaning['ExterQual'].map(qual_map).astype('int')\n\ntrain_cleaning['ExterCond'] = train_cleaning['ExterCond'].map(qual_map).astype('int')\ntest_cleaning['ExterCond'] = test_cleaning['ExterCond'].map(qual_map).astype('int')\n\ntrain_cleaning['BsmtQual'] = train_cleaning['BsmtQual'].map(qual_map).astype('int')\ntest_cleaning['BsmtQual'] = test_cleaning['BsmtQual'].map(qual_map).astype('int')\n\ntrain_cleaning['BsmtCond'] = train_cleaning['BsmtCond'].map(qual_map).astype('int')\ntest_cleaning['BsmtCond'] = test_cleaning['BsmtCond'].map(qual_map).astype('int')\n\ntrain_cleaning['HeatingQC'] = train_cleaning['HeatingQC'].map(qual_map).astype('int')\ntest_cleaning['HeatingQC'] = test_cleaning['HeatingQC'].map(qual_map).astype('int')\n\ntrain_cleaning['KitchenQual'] = train_cleaning['KitchenQual'].map(qual_map).astype('int')\ntest_cleaning['KitchenQual'] = test_cleaning['KitchenQual'].map(qual_map).astype('int')\n\ntrain_cleaning['FireplaceQu'] = train_cleaning['FireplaceQu'].map(qual_map).astype('int')\ntest_cleaning['FireplaceQu'] = test_cleaning['FireplaceQu'].map(qual_map).astype('int')\n\ntrain_cleaning['GarageQual'] = train_cleaning['GarageQual'].map(qual_map).astype('int')\ntest_cleaning['GarageQual'] = test_cleaning['GarageQual'].map(qual_map).astype('int')\n\ntrain_cleaning['GarageCond'] = train_cleaning['GarageCond'].map(qual_map).astype('int')\ntest_cleaning['GarageCond'] = test_cleaning['GarageCond'].map(qual_map).astype('int')\n\nbsmtexposure_map = {'None': 0, \n                    'No': 1, \n                    'Mn': 2, \n                    'Av': 3, \n                    'Gd': 4}\ntrain_cleaning['BsmtExposure'] = train_cleaning['BsmtExposure'].map(bsmtexposure_map).astype('int')\ntest_cleaning['BsmtExposure'] = test_cleaning['BsmtExposure'].map(bsmtexposure_map).astype('int')\n\nfence_map = {'None': 0, \n             'MnWw': 1, \n             'GdWo': 2, \n             'MnPrv': 3, \n             'GdPrv': 4}\ntrain_cleaning['Fence'] = train_cleaning['Fence'].map(fence_map).astype('int')\ntest_cleaning['Fence'] = test_cleaning['Fence'].map(fence_map).astype('int')\n\nbsmf_map = {'None': 0,\n            'Unf': 1,\n            'LwQ': 2,\n            'Rec': 3,\n            'BLQ': 4,\n            'ALQ': 5,\n            'GLQ': 6}\ntrain_cleaning['BsmtFinType1'] = train_cleaning['BsmtFinType1'].map(bsmf_map).astype('int')\ntest_cleaning['BsmtFinType1'] = test_cleaning['BsmtFinType1'].map(bsmf_map).astype('int')\ntrain_cleaning['BsmtFinType2'] = train_cleaning['BsmtFinType2'].map(bsmf_map).astype('int')\ntest_cleaning['BsmtFinType2'] = test_cleaning['BsmtFinType2'].map(bsmf_map).astype('int')\n\ngaragef_map = {'None': 0,\n               'Unf': 1,\n               'RFn': 2,\n               'Fin': 3}\ntrain_cleaning['GarageFinish'] = train_cleaning['GarageFinish'].map(garagef_map).astype('int')\ntest_cleaning['GarageFinish'] = test_cleaning['GarageFinish'].map(garagef_map).astype('int')\n\npoolqc_map = {'None': 0, \n              'Fa': 2, \n              'TA': 3, \n              'Gd': 4, \n              'Ex': 5}\ntrain_cleaning['PoolQC'] = train_cleaning['PoolQC'].map(poolqc_map).astype('int')\ntest_cleaning['PoolQC'] = test_cleaning['PoolQC'].map(poolqc_map).astype('int')\n\nstr_all_map = {'None': 0, \n               'Grvl': 1, \n               'Pave': 2}\ntrain_cleaning['Street'] = train_cleaning['Street'].map(str_all_map).astype('int')\ntest_cleaning['Street'] = test_cleaning['Street'].map(str_all_map).astype('int')\ntrain_cleaning['Alley'] = train_cleaning['Alley'].map(str_all_map).astype('int')\ntest_cleaning['Alley'] = test_cleaning['Alley'].map(str_all_map).astype('int')\n\ncent_air_map = {'N': 0, \n                'Y': 1}\ntrain_cleaning['CentralAir'] = train_cleaning['CentralAir'].map(cent_air_map).astype('int')\ntest_cleaning['CentralAir'] = test_cleaning['CentralAir'].map(cent_air_map).astype('int')\n\npave_drive_map = {'N': 0, \n                  'P': 1,\n                  'Y': 2}\ntrain_cleaning['PavedDrive'] = train_cleaning['PavedDrive'].map(pave_drive_map).astype('int')\ntest_cleaning['PavedDrive'] = test_cleaning['PavedDrive'].map(pave_drive_map).astype('int')","72e9e712":"train_cleaning.select_dtypes(include = 'object').nunique().sort_values(ascending = False)","078d4d7c":"def get_outliers(X_y, cols):\n    \"\"\"\n    Custom function for dealing with outliers.\n    It takes: DataFrame of data, list of columns;\n    And it returns: list of unique indexes of outliers.(Also it outputs all outliers with indexes for each column)\n    (value is considered an outlier if absolute value of its z-score is > 3)\n    \"\"\"\n    outliers_index = []\n    for col in cols:\n        right_outliers = X_y[col][(X_y[col] - X_y[col].mean()) \/ X_y[col].std() > 3]\n        left_outliers = X_y[col][(X_y[col] - X_y[col].mean()) \/ X_y[col].std() < -3]\n        all_outliers = right_outliers.append(left_outliers)\n        outliers_index += (list(all_outliers.index))\n        print('{} right outliers:\\n{} \\n {} left outliers:\\n{} \\n {} has TOTAL {} rows of outliers\\n'.format(col, right_outliers, col, left_outliers, col, all_outliers.count()))\n    outliers_index = list(set(outliers_index)) # Removing duplicates\n    print('There are {} unique rows with outliers in dataset'.format(len(outliers_index)))\n    return outliers_index","c9f0725f":"cols = ['GrLivArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'YearRemodAdd']\nX_y = pd.concat([train_cleaning, target], axis = 1)\noutliers_index = get_outliers(X_y, cols)\nX_y = X_y.drop(outliers_index, axis = 0)\n\ntrain_cleaning = X_y.drop('SalePrice', axis = 1).copy()\ntarget_cleaned = X_y.SalePrice","d3c221a3":"train_cleaning","be868938":"train_test = pd.concat([train_cleaning, test_cleaning], keys = ['train', 'test'], axis = 0)\n\ntrain_test['TotalPorchSF'] = (train_test['OpenPorchSF'] + train_test['3SsnPorch'] + \n                              train_test['EnclosedPorch'] + train_test['ScreenPorch'] + train_test['WoodDeckSF'])\n\ntrain_test['TotalSF'] = (train_test['BsmtFinSF1'] + train_test['BsmtFinSF2'] + \n                         train_test['1stFlrSF'] + train_test['2ndFlrSF'] + \n                         train_test['TotalPorchSF'])\n\ntrain_test['TotalBathrooms'] = (train_test['FullBath'] + (0.5 * train_test['HalfBath']) + \n                                train_test['BsmtFullBath'] + (0.5 * train_test['BsmtHalfBath']))\n\ntrain_test['TotalRms'] = (train_test['TotRmsAbvGrd'] + train_test['TotalBathrooms'])\n\ntrain_test['YearSold'] = ((train_test['MoSold'] \/ 12) + train_test['YrSold']).astype('int')\n\ntrain_test['YearsAfterB'] = (train_test['YearSold'] - train_test['YearBuilt'])\n\ntrain_test['YearsAfterR'] = (train_test['YearSold'] - train_test['YearRemodAdd'])\n    \n# Merging quality and conditions\n\ntrain_test['TotalExtQual'] = (train_test['ExterQual'] + train_test['ExterCond'])\n\ntrain_test['TotalBsmtQual'] = (train_test['BsmtQual'] + train_test['BsmtCond'] + \n                               train_test['BsmtFinType1'] + train_test['BsmtFinType2'] + train_test['BsmtExposure'])\n\ntrain_test['TotalGrgQual'] = (train_test['GarageQual'] + train_test['GarageCond'] + train_test['GarageFinish'])\n\n# train_test['TotalPaved'] = (train_test['Street'] + train_test['Alley'] + train_test['PavedDrive'])\n\ntrain_test['TotalQual'] = (train_test['OverallQual'] + train_test['OverallCond'] + \n                           train_test['TotalExtQual'] + train_test['TotalBsmtQual'] + \n                           train_test['TotalGrgQual'] + train_test['KitchenQual'] + train_test['HeatingQC'] + \n                           train_test['FireplaceQu'] + train_test['PoolQC'] + train_test['Fence'] + \n                           train_test['CentralAir'])\n\n# Creating new features by using new quality indicators\n\ntrain_test['QualGr'] = train_test['TotalQual'] * train_test['GrLivArea']\n\ntrain_test['QualBsm'] = train_test['TotalBsmtQual'] * (train_test['BsmtFinSF1'] + train_test['BsmtFinSF2'])\n\ntrain_test['QualPorch'] = train_test['TotalExtQual'] * train_test['TotalPorchSF']\n\ntrain_test['QualExt'] = (train_test['TotalExtQual'] * \n                        (train_test['Exterior1st'] + train_test['Exterior2nd']) * train_test['MasVnrArea'])\n\ntrain_test['QualGrg'] = train_test['TotalGrgQual'] * train_test['GarageArea']\n\ntrain_test['QualFirepl'] = train_test['FireplaceQu'] * train_test['Fireplaces']\n\ntrain_test['QlLivArea'] = (train_test['GrLivArea'] - train_test['LowQualFinSF']) * (train_test['TotalQual'])\n\ntrain_test['QualSFNg'] = train_test['QualGr'] * train_test['Neighborhood']\n\ntrain_test['QualSF'] = train_test['TotalQual'] * train_test['TotalSF']\ntrain_test['QlSF'] = (train_test['TotalSF'] - train_test['LowQualFinSF']) * (train_test['TotalQual'])\ntrain_test['QualSFNg2'] = (train_test['QualGr'] + train_test['QualSF']) * train_test['Neighborhood']\ntrain_test['QualGrgNg'] = train_test['QualGrg'] * train_test['Neighborhood']","862a722f":"# Creating new simple features\n\ntrain_test['HasPool'] = train_test['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['Has2ndFloor'] = train_test['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasGarage'] = train_test['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasBsmt'] = train_test['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasFireplace'] = train_test['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasPorch'] = train_test['QualPorch'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasLotFr'] = train_test['LotFrontage'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['HasFence'] = train_test['Fence'].apply(lambda x: 1 if x > 0 else 0)\ntrain_test['WasRemod'] = (train_test['YearRemodAdd'] != train_test['YearBuilt']).apply(lambda x: 1 if x == True else 0)","72f5ac41":"to_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtFinType2',\n    'BsmtQual',\n    'GarageQual',\n    'GarageFinish',\n    'KitchenQual',\n    'HeatingQC',\n    'FireplaceQu',\n    'YearSold',\n    'MiscVal',\n    'MiscFeature',\n    'Alley',\n    'PoolArea',\n    'LowQualFinSF',\n]\ntrain_test.drop(columns = to_drop, inplace=True)","7a8a27c1":"# Visualizing new features\ntrain_cleaned = train_test.xs('train').copy()\n\nplot_grid(pd.concat([train_cleaned[['TotalPorchSF',\n                                    'TotalSF',\n                                    'TotalBathrooms',\n                                    'TotalRms',\n                                    'YearsAfterB',\n                                    'YearsAfterR',\n                                    'TotalExtQual',\n                                    'TotalBsmtQual',\n                                    'TotalGrgQual',\n                                    'TotalQual',\n                                    'QualGr',\n                                    'QualBsm',\n                                    'QualPorch',\n                                    'QualExt',\n                                    'QualGrg',\n                                    'QualFirepl',\n                                    'QlLivArea',\n                                    'QualSFNg',\n                                    'QualSF',\n                                    'QlSF',\n                                    'QualSFNg2',\n                                    'QualGrgNg']], target_cleaned], axis = 1), \n          fig_size = (20, 40), grid_size = (8, 3), plot_type = 'scatterplot', target = 'SalePrice')","5877ddbb":"fig = plt.figure(figsize = (32,16))\nfor i, col_name in enumerate(['HasPool',\n                              'Has2ndFloor',\n                              'HasGarage',\n                              'HasBsmt',\n                              'HasFireplace',\n                              'HasPorch',\n                              'HasLotFr',\n                              'HasFence',\n                              'WasRemod']):\n    fig.add_subplot(5, 2, i + 1)\n    plot = sns.boxplot(x = train_cleaned[col_name], y = target_cleaned, palette = 'Reds')\n    plot.set_xlabel(col_name, fontsize = 16)\nplt.tight_layout()","c51805e4":"# Visualizing all numeric\nplot_grid(train_cleaned, fig_size = (20, 60), grid_size = (25, 3), plot_type = 'histplot')","2a83ea33":"from scipy.stats import skew, boxcox_normmax\nfrom scipy.special import boxcox1p\n\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'Fence', 'TotalSF', 'TotalRms', 'TotalQual', 'TotalPorchSF',\n    'TotalBsmtQual', 'TotalGrgQual', 'QualPorch', 'QualFirepl', 'QualGr', \n    'QualGrg', 'QlLivArea', 'QualSFNg', 'QualExt',\n    'QualSF', 'QlSF', 'QualSFNg2', 'QualGrgNg', 'ExterCond', \n    'BsmtFinType1', 'BsmtCond', 'BsmtExposure', 'GarageCond',\n]\n\n# Finding skewness of the numerical features.\nskew_train = np.abs(train_cleaned[skewed].apply(lambda x: skew(x))).sort_values(ascending = False)\n\n# Filtering skewed features.\nhigh_skew_train = skew_train[skew_train > 0.3]\n\n# Taking column names of high skew.\nskew_columns_train = high_skew_train.index\n\ntest_cleaned = train_test.xs('test').copy()\n\n# Applying boxcox transformation to fix skewness.\nfor i in skew_columns_train:\n    lamb = boxcox_normmax(train_cleaned[i] + 1)\n    train_cleaned[i] = boxcox1p(train_cleaned[i], lamb)\n    test_cleaned[i] = boxcox1p(test_cleaned[i], lamb)\n    \nhigh_skew_train","d5e84b0b":"skew_train = np.abs(train_cleaned[skewed].apply(lambda x: skew(x))).sort_values(ascending = False)\nhigh_skew_train = skew_train[skew_train > 0.3]\nhigh_skew_train","32a6940a":"plot_grid(train_cleaned[skewed], fig_size = (20, 40), grid_size = (14, 3), plot_type = 'histplot')","9c390e62":"# categorical_features = [col_name for col_name in train_test.columns \n#                         if ((train_test[col_name].dtype == 'object' and train_test[col_name].nunique() < 10) \n#                             or (train_test[col_name].dtype in ['int64', 'float64']))]\ntrain_test_cleaned = pd.concat([train_cleaned, test_cleaned], keys = ['train', 'test'], axis = 0)\ntrain_test = pd.get_dummies(train_test_cleaned)\n\n# for col_name in train_test.columns:\n#     train_test[col_name] = (train_test[col_name] - train_test[col_name].mean()) \/ train_test[col_name].std()","ecd57b57":"# from mlxtend.preprocessing import minmax_scaling\n\n# train_test = minmax_scaling(train_test, columns = train_test.columns)\n\nX_train_full, X_test = train_test.xs('train'), train_test.xs('test')","50432824":"X_train_full","68a497b9":"y_train_full = np.log1p(target_cleaned)\ny_train_full","97030368":"fig, axs = plt.subplots(2, 2, figsize = (24, 12), gridspec_kw={'width_ratios': [3, 2]})\n\nsns.histplot(y_train_full, kde = True, color = 'red', stat = 'count', ax = axs[0, 0])\naxs[0, 0].set_title('Histogram of log transfomed SalePrice', fontsize = 16)\nstats.probplot(y_train_full, plot = sns.lineplot(ax = axs[0, 1]))\naxs[0, 1].set_title('Probability Plot of log transfomed SalePrice', fontsize = 16)\naxs[0, 1].get_lines()[0].set_color('red')\naxs[0, 1].get_lines()[1].set_color('black')\n\nsns.histplot(target, kde = True, color = 'red', stat = 'count', ax = axs[1, 0])\naxs[1, 0].set_title('Histogram of SalePrice', fontsize = 16)\nstats.probplot(target, plot = sns.lineplot(ax = axs[1, 1]))\naxs[1, 1].set_title('Probability Plot of SalePrice', fontsize = 16)\naxs[1, 1].get_lines()[0].set_color('red')\naxs[1, 1].get_lines()[1].set_color('black')\n\nfig.tight_layout()","8c68669d":"from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, ElasticNetCV\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# def scorer(y, y_pred):\n#     return mean_absolute_error(np.expm1(y_pred), np.expm1(y))\n\nmodel = LinearRegression()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of LinearRegression model: {}'.format(-scores.mean()))\n\nmodel = Lasso()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of Lasso model: {}'.format(-scores.mean()))\n\nmodel = Ridge()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of Ridge model: {}'.format(-scores.mean()))\n\nmodel = LGBMRegressor()\nscores = cross_val_score(model, X_train_full, y_train_full, scoring = 'neg_root_mean_squared_error', cv = 10)\n\nprint('Mean of RMSE values from 10-fold cross validation of LGBM model: {}'.format(-scores.mean()))\n\n# from sklearn.metrics import SCORERS # This two lines were written to see a list of possible strings for scoring\n# SCORERS.keys()","e788c4b4":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# def scorer(y, y_pred):\n#     return mean_absolute_error(np.expm1(y_pred), np.expm1(y))\ndef get_best_parameters(model, parameters, cv, search):\n    if (search == 'grid'):\n        grid = GridSearchCV(model, \n                            parameters,\n                            cv = cv, \n                            scoring = 'neg_root_mean_squared_error',\n                            n_jobs = -1)\n    \n    elif (search == 'randomized'):\n        grid = RandomizedSearchCV(model,\n                                  param_distributions = parameters,\n                                  n_iter = 100,\n                                  cv = cv, \n                                  scoring = 'neg_root_mean_squared_error',\n                                  n_jobs = -1)\n    \n    grid.fit(X_train_full, y_train_full)\n    return str(grid.best_params_)","d9de43e1":"ridge_params = {\n    'alpha': [0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 5.5, 6, \n              6.5, 7, 7.5, 8, 8.5, 9, 9.5, 10, 10.5]\n}\n\nlasso_params = {\n    'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, \n              0.0006, 0.0007, 0.0008, 0.0009]\n}\n\nelasticnet_params = {\n    'alpha' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n    'l1_ratio' : [0, 0.5, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, \n                  0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99, 1]\n}\n\nxgboost_params = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [100, 500, 1000, 2000, 3000],\n    'max_depth' : [3, 6, 9],\n    'min_child_weight' : [1, 5, 10, 20],\n    'reg_alpha' : [0.001, 0.01, 0.1],\n    'reg_lambda' : [0.001, 0.01, 0.1]\n}\n\nlightgbm_params = {\n    'max_depth' : [2, 5, 8, 10],\n    'learning_rate' : [0.001, 0.01, 0.1, 0.2],\n    'n_estimators' : [100, 300, 500, 1000, 1500],\n    'lambda_l1' : [0.0001, 0.001, 0.01],\n    'lambda_l2' : [0, 0.0001, 0.001, 0.01],\n    'feature_fraction' : [0.4, 0.6, 0.8],\n    'min_child_samples' : [5, 10, 20, 25]\n}\n\ngbr_params = {\n    'learning_rate' : [0.01, 0.1, 0.15, 0.3, 0.5],\n    'n_estimators' : [500, 1000, 1500, 2000, 2500, 3000, 3500],\n    'max_depth' : [3, 6, 9]\n}\n\ncbr_params = {\n    'n_estimators' : [100, 300, 500, 1000, 1300, 1600],\n    'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n    'l2_leaf_reg' : [0.001, 0.01, 0.1],\n    'random_strength' : [0.25, 0.5 ,1],\n    'max_depth' : [3, 6, 9],\n    'min_child_samples' : [2, 5, 10, 15, 20],\n    'rsm' : [0.5, 0.7, 0.9],\n}\n\nsvr_params = {\n    'svr__C' : [10, 10.5, 11, 11.5, 12, 12.5, 13, 13.5, 14, 14.5, 15, 15.5, 16,],\n    'svr__gamma' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n}\n\nridge = Ridge() \nlasso = Lasso() \nelasticnet = ElasticNet()\nxgboost = XGBRegressor(booster = 'gbtree', objective = 'reg:squarederror')\nlightgbm = LGBMRegressor(boosting_type = 'gbdt',objective = 'regression')\ngbr = GradientBoostingRegressor()\ncbr = CatBoostRegressor(loss_function = 'RMSE', allow_writing_files = False, logging_level='Silent')\nsvr = make_pipeline(StandardScaler(), SVR())\n\nestimators = [ridge,\n              lasso,\n              elasticnet,]\n#               svr, \n#               lightgbm, \n#               gbr, \n#               cbr, \n#               xgboost]\nlabels = ['Ridge',\n          'Lasso',\n          'Elasticnet',]\n#           'SVR',\n#           'LightGBM', \n#           'GBR', \n#           'CBR']\n#           'XGBoost']\nestimators_params = [ridge_params,\n                     lasso_params,\n                     elasticnet_params,]\n#                      svr_params\n#                      lightgbm_params, \n#                      gbr_params,\n#                      cbr_params]\n#                      xgboost_params]","72acabb0":"best_parameters = pd.DataFrame(columns = ['Model name', 'Best parameters'])\n\nfor i in range(len(estimators)):\n    best_parameters.loc[i, 'Model name'] = labels[i]\n    if (labels[i] in ['XGBoost', 'LightGBM', 'GBR', 'CBR']):\n        best_parameters.loc[i, 'Best parameters'] = get_best_parameters(estimators[i], \n                                                                    estimators_params[i], \n                                                                    cv = 10, \n                                                                    search = 'randomized')\n    else:\n        best_parameters.loc[i, 'Best parameters'] = get_best_parameters(estimators[i], \n                                                                    estimators_params[i], \n                                                                    cv = 10, \n                                                                    search = 'grid')","6598ad1e":"from sklearn.model_selection import cross_validate\n\ndef test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    scoring = 'neg_root_mean_squared_error',\n#                                     return_train_score = True,\n                                    n_jobs = -1)\n\n#         result_table.loc[row_index, 'Train RMSE'] = -cv_results['train_score'].mean()\n        result_table.loc[row_index, 'Test RMSE'] = -cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by=['Test RMSE'], ascending = True, inplace = True)\n\n    return result_table","9f97883f":"from ast import literal_eval # To convert string to dictionary\n\nlinear = LinearRegression()\nridge = Ridge(**literal_eval(best_parameters.loc[0, 'Best parameters']))\nlasso = Lasso(**literal_eval(best_parameters.loc[1, 'Best parameters']) )\nelasticnet = ElasticNet(**literal_eval(best_parameters.loc[2, 'Best parameters']))\n\nsvr = make_pipeline(StandardScaler(), SVR(C = 21,\n                                          epsilon = 0.0099, \n                                          gamma = 0.00017, \n                                          tol = 0.000121))\n\nlightgbm = LGBMRegressor(objective = 'regression',\n                         n_estimators = 3500,\n                         num_leaves = 5,\n                         learning_rate = 0.00721,\n                         max_bin = 163,\n                         bagging_fraction = 0.35711,\n                         n_jobs = -1,\n                         bagging_seed = 42,\n                         feature_fraction_seed = 42,\n                         bagging_freq = 7,\n                         feature_fraction = 0.1294,\n                         min_data_in_leaf = 8)\n\ngbr = GradientBoostingRegressor(n_estimators = 2900,\n                                learning_rate = 0.0161,\n                                max_depth = 4,\n                                max_features = 'sqrt',\n                                min_samples_leaf = 17,\n                                loss = 'huber',\n                                random_state = 42)\n\ncbr = CatBoostRegressor(loss_function = 'RMSE', \n                        allow_writing_files = False, \n                        logging_level='Silent')\n\nxgboost = XGBRegressor(learning_rate = 0.0139,\n                       n_estimators = 4500,\n                       max_depth = 4,\n                       min_child_weight = 0,\n                       subsample = 0.7968,\n                       colsample_bytree = 0.4064,\n                       nthread = -1,\n                       scale_pos_weight = 2,\n                       seed = 42,)\n\nestimators = [linear,\n              ridge, \n              lasso, \n              elasticnet, \n              svr,\n              lightgbm, \n              gbr, \n              cbr,] \n#               xgboost]\n\nlabels = ['Linear',\n          'Ridge', \n          'Lasso', \n          'Elasticnet',\n          'SVR', \n          'LightGBM', \n          'GBR', \n          'CBR',]\n#           'XGBoost']\n\nresults = test_estimators(X_train_full, y_train_full, estimators, labels, cv = 10)\nresults.style.background_gradient(cmap = 'Reds')","0685a548":"from sklearn.ensemble import StackingRegressor\n\nestimators = [\n    ('1', ridge),\n    ('2', lasso),\n    ('3', elasticnet),\n    ('4', lightgbm),\n    ('5', gbr),\n    ('6', cbr),\n    ('7', xgboost),\n    ('8', svr)\n]\n\nstacked = StackingRegressor(estimators = estimators, final_estimator = elasticnet, \n                            n_jobs = -1, verbose = 4, cv = 10)\nstacked.fit(X_train_full, y_train_full)\n\npredictions = np.floor(np.expm1(stacked.predict(X_test)))","e6a48574":"submission = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\nsubmission.to_csv('submission.csv', index = False)","93cd9d08":"# 6.3 Model stacking","ed8c086d":"# 2.1 Target variable and numerical data","58fa693f":"# 1. Meeting our data:","a4cdab5c":"# 4. Feature engineering","ba278d75":"# 3.3 Dealing with outliers","dfa1ad5b":"# 5. Data normalization and one-hot encoding","2274fe70":"# 2.2 Categorical data","6b6ec3d1":"# 2. Visualization and data analysis","35ca6d2b":"Finding the best parameters for all of these models using GridSearchCV and RandomizedSearchCV is time consuming, especially gradient boosting models. (in previous version of this kernel during commit it took almost 7 hours to compute parameters for all of these models excluding SVR and XGBoost)\n\nIf you want to try it yourself, then don't forget that Kaggle kernel stops working automatically after 9 hours of a session and that they are running on cloud machines so computation abilities are limited. It's better to download jupyter notebook and try it on your own computer. (also don't forget to set n_jobs parameter to -1 in both GridSearchCV and RandomizedSearchCV)\n\nSo here, for the sake of saving some time, I'm gonna find parameters only for 'Ridge', 'Lasso' and 'Elasticnet' models.\nFor other models I will use parameters from this great kernel:\nhttps:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking","fb334981":"Label encoding other features where it's appropriate. (you can check it by looking into dataset documentation)","6a211f55":"# 3. Data cleaning","add387a8":"Plotting categorical features sorted by cardinality in descending order.","2de42dae":"# 6.2 Models evaluations","60b370fd":"# 6.1 Parameter tuning","5055635d":"Comparing distributions of a log transformed target variable and just target variable.","e884c34b":"# 3.1 Dealing with null values","4349c991":"Dropping all of the features, I found out to be useless during exploratory data analysis.","c34049a8":"# Introduction\n\nHello there,\n\nFirst of all, if you are completely new to data science field I highly recommend checking out [kaggle courses](https:\/\/www.kaggle.com\/learn\/overview) to get started. Furthermore, I'd like to recommend a few amazing kernels about this particular competition:\n1. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n2. https:\/\/www.kaggle.com\/cheesu\/house-prices-1st-approach-to-data-science-process\n3. https:\/\/www.kaggle.com\/angqx95\/data-science-workflow-top-2-with-tuning\n4. https:\/\/www.kaggle.com\/datafan07\/top-1-approach-eda-new-models-and-stacking\n\nThese notebooks are amazing, and I learnt a ton from them so hope you will too :)\n\nIn this kernel you will find my approach to this regression problem.\n\nHere's a table of contents:\n\n1. Meeting our data\n\n2. Visualization and data analysis\n    \n    2.1 Target variable and numerical data\n    \n    2.2 Categorical data\n    \n3. Data cleaning\n\n    3.1 Dealing with null values\n    \n    3.2 Label encoding\n    \n    3.3 Dealing with outliers\n    \n4. Feature engineering\n\n5. Data normalization and one-hot encoding\n\n6. Creating and evaluating a model\n\n    6.1 Parameter tuning\n    \n    6.2 Models evaluations\n    \n    6.3 Model stacking","867ba2b7":"# 6. Creating and evaluating a model","c65be7a9":"Label encoding three categorical features with high cardinality.","379ec4a4":"# 3.2 Label encoding"}}