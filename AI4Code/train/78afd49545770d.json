{"cell_type":{"212d2056":"code","5739b83e":"code","e46d259d":"code","0d4624e7":"code","5dd9b2e9":"code","7e2060f7":"code","7d3c1e75":"code","e9117315":"code","dcae4cc1":"code","c0699970":"code","77190063":"code","53203151":"code","478e3a08":"code","c09b1239":"code","cdfcb97d":"code","957aab9a":"markdown"},"source":{"212d2056":"import numpy as np\nimport pandas as pd\nimport os\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nimport gc\nimport time\nplt.style.use('seaborn')\nsns.set(font_scale=1)\nfrom tqdm import tnrange, tqdm, tqdm_notebook","5739b83e":"train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')\nfake_ids = np.load('..\/input\/fakeids\/synthetic_samples_indexes.npy')\nids = np.arange(test.shape[0])\nreal_ids = list(set(ids) - set(fake_ids))\nreal_test = test.iloc[real_ids]\nfake_test = test.iloc[fake_ids]\nreal_test_id = real_test.ID_code","e46d259d":"features = [c for c in train.columns if c not in ['target', 'ID_code']]","0d4624e7":"df = pd.concat([train,real_test], axis = 0)\nfor feat in tqdm_notebook(features):\n    df[feat+'_var'] = df.groupby([feat])[feat].transform('var')\nfor feat in tqdm_notebook(features):\n    df[feat+'plus_'] = df[feat] + df[feat+'_var']\n    # df[feat+'minus_'] = df[feat] - df[feat+'_var'] this is not necessary\ndrop_features = [c for c in df.columns if '_var' in c]\ndf.drop(drop_features, axis=1, inplace=True)","5dd9b2e9":"train = df.iloc[:train.shape[0]]\nreal_test = df.iloc[train.shape[0]:]","7e2060f7":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target'] ","7d3c1e75":"print(train.shape)\nprint(real_test.shape)\nprint(len(features))","e9117315":"param = {\n    'bagging_freq': 5,  'bagging_fraction': 0.4,  'boost_from_average':'false',   \n    'boost': 'gbdt',    'feature_fraction': 0.04, 'learning_rate': 0.01,\n    'max_depth': -1,    'metric':'auc',             'min_data_in_leaf': 80,\n    'num_leaves': 13,  'num_threads': 8,            \n    'tree_learner': 'serial',   'objective': 'binary',       'verbosity': 1\n}","dcae4cc1":"folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4242)\noof = np.zeros(len(train))\npredictions = np.zeros(len(real_test))\nval_aucs = []\nfeature_importance_df = pd.DataFrame()","c0699970":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n    clf = lgb.train(param, trn_data, 100000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    val_aucs.append(roc_auc_score(target[val_idx] , oof[val_idx] ))\n    predictions += clf.predict(real_test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n","77190063":"mean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(target, oof)\nprint(\"Mean auc: %.9f, std: %.9f. All auc: %.9f.\" % (mean_auc, std_auc, all_auc))\n","53203151":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\nplt.figure(figsize=(14,124))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","478e3a08":"subreal = pd.DataFrame({\"ID_code\": real_test_id.values})\nsubreal['target']=predictions\nsub = pd.DataFrame({\"ID_code\": test.ID_code.values})","c09b1239":"finalsub = sub.set_index('ID_code').join(subreal.set_index('ID_code')).reset_index()\nfinalsub.fillna(0,inplace=True)\nfinalsub.to_csv(\"submission.csv\", index=False)","cdfcb97d":"from IPython.display import HTML\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","957aab9a":"**Update**: Thanks to MingGu and Jianfa for testing only use \"Plus\" features which also gives you 0.922. "}}