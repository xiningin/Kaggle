{"cell_type":{"2818641f":"code","12744d7d":"code","88ece811":"code","08781687":"code","484a8a56":"code","c959bfd6":"code","3699f239":"code","8f9d8fc6":"code","e39b12c2":"code","be6e3207":"code","f5a57950":"code","ed2dd2f3":"code","3873c5f9":"code","1c425121":"code","1a501aaa":"code","034ecba0":"code","40972241":"code","ec031144":"code","9df5afad":"code","a620f151":"code","0a8a383e":"code","3729eba9":"code","9a304683":"code","a6167271":"markdown","5e1355e9":"markdown","b2610345":"markdown"},"source":{"2818641f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12744d7d":"#load the data\ndata=pd.read_csv(\"\/kaggle\/input\/amazon-unlocked-mobilecsv\/Amazon_Unlocked_Mobile.csv\")","88ece811":"# Sample the data to speed up computation\n# Comment out this line to match with lecture\ndata = data.sample(frac=0.1, random_state=10)","08781687":"#drop nan values and rows with  rating =3\ndata.dropna(inplace=True)\ndata=data[data.Rating!=3]","484a8a56":"# adding a new column called Postively rated \"1\" for Ratings greater than 3 & \"0\"for others\ndata['Postively rated']=np.where(data.Rating>3,1,0)\ndata.head()","c959bfd6":"data['Postively rated'].mean()","3699f239":"#lets only take the Reviews & Postively rated columns for the review analysis\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(data.Reviews,data['Postively rated'],random_state=0)","8f9d8fc6":"print('X_train first entry:\\n\\n', X_train.iloc[0])\nprint('\\n\\nX_train shape: ', X_train.shape)","e39b12c2":"from sklearn.feature_extraction.text import CountVectorizer\n\n# Fit the CountVectorizer to the training data\nvect = CountVectorizer().fit(X_train)\n","be6e3207":"#looking at the vocaboulry of 2000 features,messy words with N.O's and miss spelling\nvect.get_feature_names()[::2000]","f5a57950":"len(vect.get_feature_names())","ed2dd2f3":"# transform the documents in the training data to a document-term matrix\nX_train_vectorized = vect.transform(X_train)\n\nX_train_vectorized","3873c5f9":"from sklearn.linear_model import LogisticRegression\n\n# Train the model\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)","1c425121":"from sklearn.metrics import roc_auc_score\n\n# Predict the transformed test documents\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","1a501aaa":"# get the feature names as numpy array\nfeature_names = np.array(vect.get_feature_names())\n\n# Sort the coefficients from the model\nsorted_coef_index = model.coef_[0].argsort()\n\n# Find the 10 smallest and 10 largest coefficients\n# The 10 largest coefficients are being indexed using [:-11:-1] \n# so the list returned is in order of largest to smallest\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","034ecba0":"#et's take a look at a few tricks for reducing the number of \n#features that might help improve our model's performance or reduce a refitting.\n#CountVectorizor and tf\u2013idf Vectorizor both take an argument, \n#mindf, which allows us to specify a minimum number of documents in which a token needs to appear to become part of the vocabulary.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\nvect = TfidfVectorizer(min_df=5).fit(X_train)\nlen(vect.get_feature_names())\n\n#This helps us remove some words that might appear in only a few and are unlikely to be useful predictors.\n#For example, here we'll pass in min_df = 5, which will remove any words from our vocabulary that \n#appear in fewer than five documents.\n\n#Looking at the length, we can see we've reduced the number of features ","40972241":"#now lets fit the model and check the accuracy\n\nX_train_vectorized = vect.transform(X_train)\n\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))\n\n#no improvment in the auc score","ec031144":"feature_names = np.array(vect.get_feature_names())\n\nsorted_tfidf_index = X_train_vectorized.max(0).toarray()[0].argsort()\n\nprint('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\nprint('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n\n#List of features with the smallest tf\u2013idf either commonly appeared across all reviews or only \n#appeared rarely in very long reviews.\n#List of features with the largest tf\u2013idf contains words which appeared frequently in a review, \n#but did not appear commonly across all reviews.","9df5afad":"#Looking at the smallest and largest coefficients from our new model, we can again see which words our \n#model has connected to negative and positive reviews.\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))\n\n#prob with our previous model is that \n#One problem with our previous bag-of-words approach is word order is disregarded. So, not an issue, phone is \n#working is seen the same as an issue, phone is not working\n#Our current model sees both of these reviews as negative reviews.","a620f151":"# Fit the CountVectorizer to the training data specifiying a minimum \n# document frequency of 5 and extracting 1-grams and 2-grams\nvect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n\nX_train_vectorized = vect.transform(X_train)\n\nlen(vect.get_feature_names())\n\n#To create these n-gram features, we'll pass in a tuple to the parameter ngram_range, where the values \n#correspond to the minimum length and maximum lengths of sequences.\n","0a8a383e":"#Keep in mind that, although n-grams can be powerful in capturing meaning, longer sequences \n#can cause an explosion of the number of features.\nmodel = LogisticRegression()\nmodel.fit(X_train_vectorized, y_train)\n\npredictions = model.predict(vect.transform(X_test))\n\nprint('AUC: ', roc_auc_score(y_test, predictions))","3729eba9":"feature_names = np.array(vect.get_feature_names())\n\nsorted_coef_index = model.coef_[0].argsort()\n\nprint('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\nprint('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))","9a304683":"# These reviews are now correctly identified\nprint(model.predict(vect.transform(['not an issue, phone is working',\n                                    'an issue, phone is not working'])))","a6167271":"# Tfidf\n\n**Term frequency-inverse document frequency** : allows us to weight terms based on how important they are to a document.High weight is given to terms that appear often in a particular document, but don't appear often in the corpus. Features with low tf\u2013idf are either commonly used across all documents or rarely used and only occur in long documents.\nSimilar to how we used CountVectorizer, we'll instantiate the tf\u2013idf vectorizer and fit it to our training data.","5e1355e9":"# CountVectorizer\nHere we need to convert text data into numeric representation with scikit learn \n\n**Bags of words approach**:simple and commonly used way to represent text for we in ML, which ignores structure & only counts how often each word occures \n\n* **count vectoriser**: allows us to the bag-of -words approach by counting the collection of text documents into matrix of token counts\n\n","b2610345":"# n-grams\n a way we can add some context by adding sequences of word features\n \n For example, bigrams, which count pairs of adjacent words, could give us features such as is working versus not working. And trigrams, which give us triplets of adjacent words, could give us features such as not an issue.\n"}}