{"cell_type":{"64b42ae4":"code","93257362":"code","b7a8553e":"code","a7390303":"code","5aead95f":"code","84bfa65a":"code","42c9ed3e":"code","2514bef9":"code","6b9326f9":"code","9ee89ea9":"code","bdb8659e":"code","eb2442e3":"code","a79400a4":"code","a7112c3f":"code","ec0a1af5":"code","8d5fe058":"code","91eee9d7":"code","2e484480":"code","1e6ef2cd":"code","b43ca3e1":"code","6e52a07e":"markdown","2862ef8b":"markdown","fb66d1e2":"markdown","ba678800":"markdown","e1c437d5":"markdown","cf697162":"markdown","a801188e":"markdown","dbe6b87d":"markdown","cf12be90":"markdown","34962de3":"markdown","c089e84c":"markdown","a1ef4bcc":"markdown","7281d9bf":"markdown","2e3c9e9d":"markdown"},"source":{"64b42ae4":"# 320203302 \u5218\u4e3a\u658c","93257362":"# \u6837\u672c\u6587\u4ef6\u8def\u5f84\nSAMPLE_FILE_PATH = \"..\/input\/the-oxfordiiit-pet-dataset\/images\/images\"\n\n# \u6837\u672c\u5206\u7c7b\u6807\u8bb0\u6587\u4ef6\nSAMPLE_ANNOTATION_FILE = \"..\/input\/the-oxfordiiit-pet-dataset\/annotations\/annotations\/list.txt\"\n\n# \u5206\u7c7b\u6570\u91cf\nNUM_SPECIES_CLASSES = 2  # \u7269\u79cd\u5206\u7c7b\nNUM_CAT_CLASSES = 12  # \u732b\u54c1\u79cd\u5206\u7c7b\nNUM_DOG_CLASSES = 25  # \u72d7\u54c1\u79cd\u5206\u7c7b\nNUM_ALL_CLASSES = 37  # \u5168\u90e8\u5206\u7c7b\n# \u9879\u76ee\u8f93\u51fa\u6587\u4ef6\u4fdd\u5b58\u76ee\u5f55\nOUTPUT_PATH = \".\/\"\n# \u8bad\u7ec3\u3001\u6821\u9a8c\u3001\u6d4b\u8bd5\u6570\u636e\u96c6HDF5\u6587\u4ef6\u7684\u8f93\u51fa\u8def\u5f84\nTRAIN_HDF5 = OUTPUT_PATH+\"train.hdf5\"\nVAL_HDF5 = OUTPUT_PATH+\"val.hdf5\"\nTEST_HDF5 = OUTPUT_PATH+\"test.hdf5\"\n# \u6bcf\u6279\u6b21\u6837\u672c\u6570\u91cf\nBATCH_SIZE = 128\n# \u6570\u636e\u96c6\u6837\u672cRGB\u5e73\u5747\u503c\u5b58\u50a8\u4f4d\u7f6e\u53ca\u6587\u4ef6\u540d\u79f0\nDATASET_MEAN_FILE = OUTPUT_PATH + \"\/rgb_mean.json\"\n# \u6a21\u578b\u4fdd\u5b58\u4f4d\u7f6e\u53ca\u6587\u4ef6\u540d\u79f0\nSPECIES_MODEL_FILE = OUTPUT_PATH + \"specise_model.h5\"\nCAT_MODEL_FILE = OUTPUT_PATH + \"cat_model.h5\"\nDOG_MODEL_FILE = OUTPUT_PATH + \"dog_model.h5\"\nALL_MODEL_FILE = OUTPUT_PATH + \"all_model.h5\"","b7a8553e":"import cv2\nimport imutils\nclass AspectAwarePreprocessor:\n    def __init__(self, width, height, inter=cv2.INTER_AREA):\n        self.width = width\n        self.height = height\n        self.inter = inter\n    def preprocess(self, image):\n        (h, w) = image.shape[:2]\n        dw = 0\n        dh = 0\n        if w < h:\n            image = imutils.resize(image, width=self.width, inter=self.inter)\n            dh = int((image.shape[0] - self.height) \/ 2.0)\n        else:\n            image = imutils.resize(image, height=self.height, inter=self.inter)\n            dw = int((image.shape[1] - self.width) \/ 2.0)\n        (h, w) = image.shape[:2]\n        image = image[dh:h - dh, dw:w - dw]\n        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)\n","a7390303":"import numpy as np\nimport cv2\nclass CroPreprocessor:\n    def __init__(self, width, height, horiz=True, inter=cv2.INTER_AREA):\n        self.width = width\n        self.height = height\n        self.horiz = horiz\n        self.inter = inter\n    def preprocessor(self, image):\n        crops = []\n        (h, w) = image.shape[:2]\n        coords = [\n            [0, 0, self.width, self.height],\n            [w - self.width, 0, w, self.height],\n            [w - self.width, h - self.height, w, h],\n            [0, h - self.height, self.width, h]]\n        dw = int(0.5 * (w - self.width))\n        dh = int(0.5 * (h - self.height))\n        coords.append([dw, dh, w - dw, h - dh])\n        for (startX, startY, endX, endY) in coords:\n            crop = image[startY:endY, startX:endX]\n            crop = cv2.resize(crop,\n                              (self.width, self.height),\n                              interpolation=self.inter)\n            crops.append(crop)\n            if self.horiz:\n                mirrors = [cv2.flip(c, 1) for c in crops]\n                crops.extend(mirrors)\n        return np.array(crops)","5aead95f":"from tensorflow.keras.callbacks import Callback\nimport os\nclass EpochCheckpoint(Callback):\n    def __init__(self, output_path, every=5, start_at=0):\n        super(Callback, self).__init__()\n        self.output_path = output_path\n        self.every = every\n        self.start_epoch = start_at\n    def on_epoch_end(self, epoch, logs={}):\n        if (self.start_epoch + 1) % self.every == 0:\n            p = os.path.sep.join([self.output_path,\n                                  \"epoch_{}.hdf5\".format(self.start_epoch + 1)])\n            self.model.save(p, overwrite=True)\n        self.start_epoch += 1","84bfa65a":"from keras.utils import np_utils\nimport numpy as np\nimport h5py\nclass HDF5DatasetGenerator:\n    def __init__(self,\n                 db_file,\n                 batch_size,\n                 preprocessors=None,\n                 aug=None,\n                 binarize=True,\n                 classes=2):\n        self.batchSize = batch_size\n        self.preprocessors = preprocessors\n        self.aug = aug\n        self.binarize = binarize\n        self.classes = classes\n        self.db = h5py.File(db_file)\n        self.numImages = self.db[\"labels\"].shape[0]\n\n    def generator(self, passes=np.inf):\n        epochs = 0\n        while epochs < passes:\n            for i in np.arange(0, self.numImages, self.batchSize):\n                images = self.db[\"images\"][i:i + self.batchSize]\n                labels = self.db[\"labels\"][i:i + self.batchSize]\n                if self.binarize:\n                    labels = np_utils.to_categorical(labels, self.classes)\n                if self.preprocessors is not None:\n                    processed_images = []\n                    for image in images:\n                        for p in self.preprocessors:\n                            image = p.preprocess(image)\n                        processed_images.append(image)\n                    images = np.array(processed_images)\n                if self.aug is not None:\n                    (images, labels) = next(self.aug.flow(images, labels, batch_size=self.batchSize))\n                yield images, labels\n            epochs += 1\n    def close(self):\n        self.db.close()\n","42c9ed3e":"import os\nimport h5py\nclass HDF5DatasetWriter:\n    def __init__(self, dims, output_path, data_key=\"images\", buf_size=1000):\n        if os.path.exists(output_path):\n            raise ValueError('\u60a8\u63d0\u4f9b\u7684\u8f93\u51fa\u6587\u4ef6{}\u5df2\u7ecf\u5b58\u5728\uff0c\u8bf7\u5148\u624b\u5de5\u5220\u9664!'.format(output_path))\n        self.db = h5py.File(output_path, \"w\")\n        self.data = self.db.create_dataset(data_key, dims, dtype=\"float\")\n        self.labels = self.db.create_dataset(\"labels\", (dims[0],), dtype=\"int\")\n        self.buf_size = buf_size\n        self.buffer = {\"data\": [], \"labels\": []}\n        self.idx = 0\n    def add(self, raw, label):\n        self.buffer[\"data\"].extend(raw)\n        self.buffer[\"labels\"].extend(label)\n        if len(self.buffer[\"data\"]) >= self.buf_size:\n            self.flush()\n    def flush(self):\n        i = self.idx + len(self.buffer[\"data\"])\n        self.data[self.idx:i] = self.buffer[\"data\"]\n        self.labels[self.idx:i] = self.buffer[\"labels\"]\n        self.idx = i\n        self.buffer = {\"data\": [], \"labels\": []}\n    def store_class_labels(self, class_labels):\n        dt = h5py.special_dtype(vlen=str)\n        label_dim = (len(class_labels),)\n        label_set = self.db.create_dataset(\"label_names\", label_dim, dtype=dt)\n        label_set[:] = class_labels\n    def close(self):\n        if len(self.buffer[\"data\"]) > 0:\n            self.flush()\n\n        self.db.close()\n","2514bef9":"import numpy as np\nimport matplotlib.pyplot as plt\nclass HistoryGraph:\n    def __init__(self, history, epochs, title, file_path):\n        self.history = history\n        self.epochs = epochs\n        self.title = title\n        self.file_path = file_path\n    def draw(self):\n        figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        figure.suptitle(self.title, fontsize=12)\n        figure.subplots_adjust(top=0.85, wspace=0.3)\n        epoch_list = list(range(1, self.epochs + 1))\n        ax1.plot(epoch_list,\n                 self.history.history['accuracy'],\n                 label='Train Accuracy')\n        ax1.plot(epoch_list,\n                 self.history.history['val_accuracy'],\n                 label='Validation Accuracy')\n        ax1.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax1.set_ylabel('Accuracy Value')\n        ax1.set_xlabel('Epoch #')\n        ax1.set_title('Accuracy')\n        ax1.legend(loc='best')\n        ax2.plot(epoch_list, self.history.history['loss'], label='Train Loss')\n        ax2.plot(epoch_list, self.history.history['val_loss'], label='Validation Loss')\n        ax2.set_xticks(np.arange(0, self.epochs + 1, 5))\n        ax2.set_ylabel('Loss Value')\n        ax2.set_xlabel('Epoch #')\n        ax2.set_title('Loss')\n        ax2.legend(loc=\"best\")\n        plt.savefig(self.file_path)\n        plt.close()","6b9326f9":"from keras.preprocessing.image import img_to_array\nclass ImageToArrayPreprocessor:\n    def __init__(self, data_format=None):\n        self.data_format = data_format\n    def preprocess(self, image):\n        return img_to_array(image, data_format=self.data_format)","9ee89ea9":"import cv2\nclass MeanPreprocessor:\n    def __init__(self, rMean, gMean, bMean):\n        self.rMean = rMean\n        self.gMean = gMean\n        self.bMean = bMean\n    def preprocess(self, image):\n        (B, G, R) = cv2.split(image.astype(\"float32\"))\n        R -= self.rMean\n        G -= self.gMean\n        B -= self.bMean\n        return cv2.merge([B, G, R])","bdb8659e":"from sklearn.feature_extraction.image import extract_patches_2d\nclass PatchPreprocessor:\n    def __init__(self, width, height):\n        self.width = width\n        self.height = height\n    def preprocess(self, image):\n        return extract_patches_2d(image,\n                                  (self.height, self.width), max_patches=1)[0]","eb2442e3":"import cv2\nclass SimplePreprocessor:\n    def __init__(self, width, height, inter=cv2.INTER_AREA):\n        self.width = width\n        self.height = height\n        self.inter = inter\n    def preprocess(self, image):\n        return cv2.resize(image, (self.width, self.height), interpolation=self.inter)","a79400a4":"import numpy as np\ndef top_accuracy(predicts, labels):\n    top1 = 0\n    top5 = 0\n    for (p, gt) in zip(predicts, labels):\n        p = np.argsort(p)[::-1]\n        if gt in p[:5]:\n            top5 += 1\n        if gt == p[0]:\n            top1 += 1\n    top1 \/= float(len(labels))\n    top5 \/= float(len(labels))\n    return (top1, top5)","a7112c3f":"from tensorflow.keras.callbacks import BaseLogger\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport json\nimport os\nclass TrainingMonitor(BaseLogger):\n    def __init__(self, fig_path, json_path=None, start_at=0):\n        super(TrainingMonitor, self).__init__()\n        self.fig_path = fig_path\n        self.json_path = json_path\n        self.start_at = start_at\n    def on_train_begin(self, logs={}):\n        self.history = {}\n        if self.json_path is not None:\n            if os.path.exists(self.json_path):\n                self.history = json.loads(open(self.json_path).read())\n                if self.start_at > 0:\n                    for k in self.history.keys():\n                        self.history[k] = self.history[k][:self.start_at]\n    def on_epoch_end(self, epoch, logs={}):\n        for (k, v) in logs.items():\n            log = self.history.get(k, [])\n            log.append(float(v))\n            self.history[k] = log\n        if self.json_path is not None:\n            f = open(self.json_path, \"w\")\n            f.write(json.dumps(self.history))\n            f.close()\n        if len(self.history[\"loss\"]) > 1:\n            N = np.arange(0, len(self.history[\"loss\"]))\n            plt.style.use(\"ggplot\")\n            plt.figure()\n            plt.plot(N, self.history[\"output_loss\"], label=\"train_loss\")\n            plt.plot(N, self.history[\"val_output_loss\"], label=\"val_loss\")\n            plt.plot(N, self.history[\"output_accuracy\"], label=\"train_acc\")\n            plt.plot(N, self.history[\"val_output_accuracy\"], label=\"val_acc\")\n            epochs = len(self.history[\"loss\"])\n            plt.title(\"Training Loss & Accuracy [Epoch {}]\".format(epochs))\n            plt.xlabel(\"Epoch #\")\n            plt.ylabel(\"Loss\/Accuracy\")\n            plt.legend()\n            plt.savefig(self.fig_path)\n            plt.close()","ec0a1af5":"import os \n# os.remove('.\/train.hdf5')\n# os.remove('.\/val.hdf5')\n# os.remove('.\/test.hdf5')\n","8d5fe058":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nimport os\nimport cv2\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport progressbar\nimport json\nfrom PIL import Image\nfrom imutils import paths\nsample_files=list(paths.list_images(SAMPLE_FILE_PATH))\nprint('..\/input\/the-oxfordiiit-pet-dataset\/images\/images\/american_pit_bull_terrier_16.jpg'.split(os.path.sep))\n\nremove_sample_files=[]\nfor file in sample_files:\n    image=cv2.imread(file)\n    if image is None:\n        remove_sample_files.append(file)\n        sample_files.remove(file)\nprint(remove_sample_files)\n\nsample_labels=[' '.join(s.split(os.path.sep)[-1].split('_')[:-1]) for s in sample_files]\n\n# \u5c06\u5168\u90e8\u6837\u672c\u6807\u7b7e\u8f6c\u6362\u4e3a\u6574\u578b\nle = LabelEncoder()\ny_sample = le.fit_transform(sample_labels)\n# \u83b7\u53d6\u5206\u7c7b\u6807\u7b7e\u540d\u79f0\u5217\u8868\nclass_labels = list(le.classes_)\n# \u5c06\u6837\u672c\u8fdb\u884c\u5206\u5272\uff0c80%\u4f5c\u4e3a\u8bad\u7ec3\u6837\u672c\uff0c\u5176\u4f59\u4e3a\u6821\u9a8c\u548c\u6d4b\u8bd5\u6837\u672c\nsplit = train_test_split(sample_files,\n                         y_sample,\n                         test_size=0.2,\n                         stratify=y_sample,\n                         random_state=42)\n(train_files, test_files, y_train, y_test) = split\n# \u5c06\u6d4b\u8bd5\u673a\u518d\u6b21\u5206\u5272  50%\u4e3a\u6821\u9a8c\u96c6\uff0c50%\u6d4b\u8bd5\u96c6\nsplit = train_test_split(test_files,\n                         y_test,\n                         test_size=0.5,\n                         stratify=y_test,\n                         random_state=42)\n(val_files, test_files, y_val, y_test) = split\nprint(\"[\u7cfb\u7edf\u4fe1\u606f] \u7edf\u8ba1\u6570\u636e\u96c6...\")\nprint(\"\u8bad\u7ec3\u6837\u672c\u6570\u91cf:\", len(train_files))\nprint(\"\u6821\u9a8c\u6837\u672c\u6570\u91cf:\", len(val_files))\nprint(\"\u6d4b\u8bd5\u6837\u672c\u6570\u91cf:\", len(test_files))\nprint(\"\u5408\u8ba1\u6837\u672c\u6570\u91cf:\", len(train_files) + len(val_files) + len(test_files))\nassert len(train_files) == len(train_files), \"\u8bad\u7ec3\u6837\u672c\u6570\u91cf\u548c\u8bad\u7ec3\u6807\u7b7e\u6570\u91cf\u4e0d\u76f8\u7b49\"\nassert len(val_files) == len(y_val), \"\u6821\u9a8c\u6837\u672c\u6570\u91cf\u548c\u6821\u9a8c\u6807\u7b7e\u6570\u91cf\u4e0d\u76f8\u7b49\"\nassert len(test_files) == len(y_test), \"\u6d4b\u8bd5\u6837\u672c\u6570\u91cf\u548c\u6d4b\u8bd5\u6807\u7b7e\u6570\u91cf\u4e0d\u76f8\u7b49\"\nprint(\"\u5206\u7c7b\u6570\u91cf:\", len(class_labels))\n# \u7edf\u8ba1\u6bcf\u4e2a\u7c7b\u522b\u6837\u672c\u6570\u91cf\u5206\u5e03\nclass_sample_distribution = []\nfor class_name in class_labels:\n    _class_name='_'.join(class_name.split(' '))\n    count_train = len([name for name in train_files if _class_name in name])\n    count_val = len([name for name in val_files if _class_name in name])\n    count_test = len([name for name in test_files if _class_name in name])\n    class_sample_distribution.append([class_name,\n                                      (count_train + count_val + count_test,\n                                       count_train,\n                                       count_val,\n                                       count_test)\n                                      ])\nprint(class_sample_distribution)\n# \u6784\u5efa\u4e00\u4e2a\u8bad\u7ec3\u3001\u6821\u9a8c\u3001\u6d4b\u8bd5\u6570\u636e\u96c6\u5217\u8868\uff0c\n# \u6bcf\u4e2a\u5143\u7d20\u7531\u6570\u636e\u96c6\u7c7b\u578b\u540d\u79f0\u3001\u5168\u90e8\u6837\u672c\u6587\u4ef6\u540d\u79f0\u3001\u5168\u90e8\u6837\u672c\u6574\u578b\u6807\u7b7e\u3001HDF5\u8f93\u51fa\u6587\u4ef6\u6784\u6210\ndataset = [(\"train\", train_files, y_train, TRAIN_HDF5),\n           (\"val\", val_files, y_val, VAL_HDF5),\n           (\"test\", test_files, y_test, TEST_HDF5)]\n# \u5b9e\u4f8b\u5316\u56fe\u50cf\u9884\u5904\u7406\u5668\uff08\u4fdd\u6301\u6bd4\u4f8b\u7f29\u653e\uff09\naap = AspectAwarePreprocessor(256, 256)\n# \u521d\u59cb\u5316R\u3001G\u3001B\u901a\u9053\u5747\u503c\n(R, G, B) = ([], [], [])\n# \u904d\u5386\u6570\u636e\u96c6\u5143\u7ec4\nfor (dType, files, labels, outputPath) in dataset:\n    # \u521b\u5efaHDF5\u5199\u5165\u5668\n    print(\"[\u7cfb\u7edf\u4fe1\u606f] \u6b63\u5728\u6784\u5efa{}...\".format(outputPath))\n    writer = HDF5DatasetWriter((len(files), 256, 256, 3), outputPath)\n    # \u521d\u59cb\u5316\u8fdb\u5ea6\u6761\n    widgets = [\"\u6784\u5efa\u6570\u636e\u96c6:\",\n               progressbar.Percentage(),\n               \" \", progressbar.Bar(), \"\", progressbar.ETA()]\n    pbar = progressbar.ProgressBar(maxval=len(files), widgets=widgets).start()\n    # \u904d\u5386\u6837\u672c\u56fe\u50cf\u6587\u4ef6\n    for (i, (file, label)) in enumerate(zip(files, labels)):\n        # \u8bfb\u5165\u56fe\u50cf\u6587\u4ef6\uff0c \u8fdb\u884c\u9884\u5904\u7406\n        # \u6807\u51c6\u5f52\u4e00\u5316\n        image = cv2.imread(file) \/ 255.0\n        # image=Image.open(file)\n        # image=np.array(image)\/255.0\n        # break\n        # \u7531\u4e8e\u6837\u672c\u56fe\u50cf\u7684\u5c3a\u5bf8\u3001\u7eb5\u6a2a\u6bd4\u662f\u4e0d\u56fa\u5b9a\u7684\n        # \u6240\u4ee5\u9700\u8981\u4e0b\u9762\u4fdd\u6301\u7eb5\u6a2a\u6bd4\u4e0d\u53d8\u7684\u7f29\u653e\u9884\u5904\u7406\u64cd\u4f5c\n        image = aap.preprocess(image)\n\n        # \u5982\u679c\u662f\u8bad\u7ec3\u96c6\uff0c \u5219\u83b7\u53d6\u6837\u672cR\u3001G\u3001B\u4e09\u4e2a\u901a\u9053\u7684\u5747\u503c\n        # \u5206\u522b\u6dfb\u52a0\u5230R\u3001G\u3001B\u5217\u8868\u4e2d\uff0c \u4ee5\u4fbf\u8ba1\u7b97\u8bad\u7ec3\u6837\u672c\u7684\u6574\u4f53R\u3001G\u3001B\u5747\u503c\n        if dType == \"train\":\n            (b, g, r) = cv2.mean(image)[:3]\n            R.append(r)\n            G.append(g)\n            B.append(b)\n        # \u5c06\u56fe\u50cf\u548c\u6807\u7b7e\u6dfb\u52a0\u5230HDF5\u6570\u636e\u96c6\u6587\u4ef6\n        writer.add([image], [label])\n        pbar.update(i)\n    if dType == \"train\":\n        writer.store_class_labels(class_labels)\n    # \u5173\u95edHDF5\u5199\u5165\u5668\n    pbar.finish()\n    writer.close()\n# \u521b\u5efa\u8bad\u7ec3\u6837\u672cR\u3001G\u3001B\u901a\u9053\u5747\u503c\u5b57\u5178\uff0c \u7136\u540e\u5c06\u5176\u5e8f\u5217\u5316\u5230json\u6587\u4ef6\nprint(\"[\u7cfb\u7edf\u4fe1\u606f] \u5e8f\u5217\u5316\u8bad\u7ec3\u6837\u672cR\u3001G\u3001B\u901a\u9053\u5747\u503c...\")\nD = {\"R\": np.mean(R), \"G\": np.mean(G), \"B\": np.mean(B)}\nwith open(DATASET_MEAN_FILE, \"w\") as f:\n    f.write(json.dumps(D))\n    f.close()\nprint(\"\u5199\u5165rgb\u901a\u9053\u5747\u503c\u6210\u529f\uff01\")","91eee9d7":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import MaxPool2D\nfrom keras.layers import Activation\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers import add\nfrom keras import backend as K\n\n\n# \u5b9a\u4e49ResNet50V2\u7c7b\nclass ResNet50V2:\n    @staticmethod\n    def residual_module(x,\n                        filters,\n                        kernel_size=3,\n                        stride=2,\n                        conv_shortcut=False):\n        \"\"\"\n        \u521b\u5efaResNet\u7684\u53c2\u6a21\u5757\n        Args:\n             x:                   \u53c2\u5dee\u6a21\u5757\u7684\u8f93\u5165\n             filters:             \u6838\u6570\n             kernel_size:         \u6838\u5927\u5c0f\uff0c\u7f3a\u7701\u4e3a3\n             stride:              \u7b2c\u4e00\u5377\u79ef\u5c42\u7684\u5377\u79ef\u6b65\u957f\n             conv_shortcut:       True\u65f6\u7528\u5377\u79ef\u77ed\u8def\uff0cFalse\u65f6\u4f7f\u7528\u7b49\u7ef4\u77ed\u8def\n\n        Returns:\n             \u751f\u6210\u7684\u53c2\u5dee\u6a21\u5757\n        \"\"\"\n\n        bn_axis = 3 if K.image_data_format() == 'channels_last' else 1\n\n        # #\u53c2\u5dee\u6a21\u5757\u7b2c\u4e00\u5377\u79ef\u5757:\u5148\u5f52\u4e00\u5316\uff0c\u518d\u6fc0\u52b1\uff0c\u518d\u5377\u79ef\n        preact = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n        preact = Activation('relu')(preact)\n        if conv_shortcut is True:\n            shortcut = Conv2D(4*filters,\n                              kernel_size=(1, 1),\n                              strides=stride,\n                              padding='valid',\n                              kernel_initializer='he_normal')(preact)\n            x = Conv2D(filters=filters,\n                       kernel_size=(1, 1),\n                       strides=stride,\n                       padding='valid',\n                       kernel_initializer='he_normal')(preact)\n\n        else:\n            shortcut = x\n            x = Conv2D(filters=filters,\n                       kernel_size=(1, 1),\n                       strides=(1, 1),\n                       kernel_initializer='he_normal')(preact)\n\n        # \u53c2\u5dee\u6a21\u5757\u7b2c\u4e8c\u5377\u79ef\u5757:\u5148\u5f52\u4e00\u5316\uff0c\u518d\u6fc0\u52b1\uff0c\u518d\u5377\u79ef\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n        x = Activation(\"relu\")(x)\n        x = Conv2D(filters=filters,\n                   kernel_size=kernel_size,\n                   strides=(1, 1),\n                   padding='same',\n                   kernel_initializer='he_normal')(x)\n\n        # \u53c2\u5dee\u6a21\u5757\u7b2c\u4e09\u5377\u79ef\u5757:\u5148\u5f52\u4e00\u5316\uff0c\u518d\u6fc0\u52b1\uff0c\u518d\u5377\u79ef\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n        x = Activation(\"relu\")(x)\n        # 1x1\u5377\u79ef\n        x = Conv2D(4*filters,\n                   kernel_size=(1, 1),\n                   strides=(1, 1),\n                   padding='valid',\n                   kernel_initializer='he_normal')(x)\n\n        # \u5c06\u6700\u540e\u7684\u5377\u79ef\u7ed3\u679c\u548c\u77ed\u8def\u5206\u652f\u5757\u76f8\u52a0\n        x = add([shortcut, x])\n\n        # \u8fd4\u56de\u53c2\u5dee\u5757\n        return x\n\n    @staticmethod\n    def stack(x, filters, blocks, stride1=2):\n        \"\"\"\n        \u751f\u6210\u4e00\u7ec4\u5806\u53e0\u7684\u53c2\u5dee\u5757\n        Args:\n            x:\n            filters:\n            blocks:\n            stride1:\n\n        Returns:\n            \u4e00\u7ec4\u5806\u53e0\u7684\u53c2\u5dee\u5757\n        \"\"\"\n        # 1\u3001\u6240\u6709\u6b8b\u5dee\u7f51\u6bb5\u7684\u7b2c\u4e00\u4e2a\u6b8b\u5dee\u6a21\u5757\u5747\u4f7f\u7528\u524d\u6fc0\u52b1\u5377\u79ef\u6b8b\u5dee\u6a21\u5757\n        # 2\u3001\u7b2c\u4e00\u4e2a\u6b8b\u5dee\u7f51\u6bb5\u7684\u7b2c\u4e00\u4e2a\u6b8b\u5dee\u6a21\u5757\u5377\u79ef\u6b65\u957f\u4e3a1\uff0c\u6240\u4ee5\u4e0d\u964d\u7ef4\n        # 3\u3001\u7b2c\u4e8c\u3001\u4e09\u3001\u56db\u6b8b\u5dee\u7f51\u6bb5\u7684\u7b2c\u4e00\u4e2a\u6b8b\u5dee\u6a21\u5757\u5377\u79ef\u6b65\u957f\u4e3a2\uff0c\u6240\u4ee5\u4ea7\u751f3\u6b21\u51cf\u534a\u964d\u7ef4\n        x = ResNet50V2.residual_module(x, filters, stride=stride1, conv_shortcut=True)\n\n        # \u6240\u6709\u6b8b\u5dee\u7f51\u6bb5\u7684\u975e\u7b2c\u4e00\u4e2a\u6b8b\u5dee\u6a21\u5757\u5747\u4f7f\u7528\u524d\u6fc0\u52b1\u7b49\u7ef4\u6b8b\u5dee\u6a21\u5757\n        for i in range(1, blocks):\n            x = ResNet50V2.residual_module(x, filters, conv_shortcut=False)\n        return x\n\n    @staticmethod\n    def build(width, height, depth, classes):\n        \"\"\"\n        \u6784\u9020\u53c2\u5dee\u7f51\u7edc\u6a21\u578b\n        Args:\n            width:\n            height:\n            depth:\n            classes:\n\n        Returns:\n            \u53c2\u5dee\u7f51\u7edc\u6a21\u578b\n        \"\"\"\n\n        # \u521d\u59cb\u5316\u8f93\u5165\u7ef4\u5ea6\u548c\u901a\u9053\u4f4d\u7f6e(channels_last, \u901a\u9053\u540e\u7f6e)\n        input_shape = (height, width, depth)\n        bn_axis = 3\n\n        # \u5982\u679c\u6211\u4eec\u7684\u6837\u672c\u4e3a\u901a\u9053\u524d(channels_first)\n        if K.image_data_format() == \"channels_first\":\n            input_shape = (depth, height, width)\n            bn_axis = 1\n\n        # \u8bbe\u7f6e\u7f51\u7edc\u7684\u8f93\u5165\n        inputs = Input(shape=input_shape)\n        x = Conv2D(64, 7, strides=2, padding='same', use_bias=True)(inputs)\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n        x = Activation('relu')(x)\n\n        x = MaxPool2D(3, strides=2, padding='same')(x)\n\n        x = ResNet50V2.stack(x, 64, 3, stride1=1)\n        x = ResNet50V2.stack(x, 128, 4, stride1=2)\n        x = ResNet50V2.stack(x, 256, 6, stride1=2)\n        x = ResNet50V2.stack(x, 512, 3, stride1=2)\n\n        x = BatchNormalization(axis=bn_axis, epsilon=1.001e-5)(x)\n        x = Activation('relu')(x)\n\n        # \u5168\u5c40\u5e73\u5747\u6c60\u5316\n        x = GlobalAveragePooling2D()(x)\n        # \u5168\u8fde\u63a5\n        x = Dense(classes, activation='softmax', name='output')(x)\n\n        # \u521b\u5efa\u6a21\u578b\n        model = Model(inputs, x, name=\"resnet50v2\")\n\n        # \u8fd4\u56de\u6a21\u578b\n        return model\nif __name__=='__main__':\n    model=ResNet50V2().build(width=224, height=224, depth=3, classes=37)\n    print(model.summary())","2e484480":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD\n# \u6837\u672c\u56fe\u50cf\u6587\u4ef6\u76ee\u5f55\nimage_base_path = SAMPLE_FILE_PATH\n\n\"\"\"\n\u6837\u672c\u6587\u4ef6\u5217\u8868\n\u6587\u4ef6\u540d                   \u5206\u7c7b\u7f16\u53f7          \u732b\u72d7\u7f16\u53f7           \u79cd\u5185\u5206\u7c7b\u7f16\u53f7\n                        (1-37)          (1:\u732b,2:\u72d7)       (\u732b:1-12,\u72d7:1-25)\nAbyssinian_100          1                1                1\nyorkshire_terrier_9     37               2                25\n\"\"\"\n\nsample_files = SAMPLE_ANNOTATION_FILE\n\n# \u521b\u5efa\u732b\u54c1\u79cd12\u5206\u7c7bDataFrame\nread_for = open(sample_files)\nsample_files = []\nsample_labels = []\nfor txt_line in read_for:\n    if txt_line[0] == '#':\n        continue\n    file_name, class_id, cat_dog_id, sub_class_id = txt_line.strip().split(' ')\n    if cat_dog_id == '1':\n        sample_files.append(file_name + '.jpg')\n        sample_labels.append(sub_class_id)\nread_for.close()\n\nsample_data_frame = pd.DataFrame({'filename': sample_files, 'label': sample_labels})\n# \u8bad\u7ec3\u53c2\u6570\u8bbe\u7f6e\nBATCH_SIZE = 64\nEPOCHS = 80\nLEARN_RATE = 0.01\nCLASSES = 12\n\n# \u6784\u9020\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668\ndata_gen = ImageDataGenerator(rescale=1.\/255.,\n                              rotation_range=20,\n                              zoom_range=0.15,\n                              width_shift_range=0.2,\n                              height_shift_range=0.2,\n                              shear_range=0.15,\n                              horizontal_flip=True,\n                              validation_split=0.05,\n                              fill_mode=\"nearest\")\n\n# \u521d\u59cb\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u751f\u6210\u5668\ntrainGen = data_gen.flow_from_dataframe(dataframe=sample_data_frame,\n                                        directory=image_base_path,\n                                        x_col='filename',\n                                        y_col='label',\n                                        target_size=(224, 224),\n                                        class_mode='categorical',\n                                        batch_size=BATCH_SIZE,\n                                        subset='training',\n                                        shuffle=True,\n                                        seed=42)\n# \u6784\u9020\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u6821\u9a8c\u6570\u636e\u751f\u6210\u5668\nval_gen = ImageDataGenerator(rescale=1.\/255.,\n                             validation_split=0.05)\n# \u521d\u59cb\u5316\u6821\u9a8c\u6570\u636e\u96c6\u751f\u6210\u5668\nval_Gen = val_gen.flow_from_dataframe(dataframe=sample_data_frame,\n                                      directory=image_base_path,\n                                      x_col='filename',\n                                      y_col='label',\n                                      target_size=(224, 224),\n                                      class_mode='categorical',\n                                      batch_size=BATCH_SIZE,\n                                      subset='validation',\n                                      shuffle=True,\n                                      seed=42)\n# \u7f16\u8bd1\u6a21\u578b\nopt = SGD(lr=LEARN_RATE, decay=1e-6, momentum=0.9, nesterov=True)\nmodel = ResNet50V2.build(224, 224, 3, classes=CLASSES)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n# \u8bad\u7ec3\u7f51\u7edc\nhistory = model.fit_generator(trainGen,\n                              steps_per_epoch=trainGen.n\/\/BATCH_SIZE,\n                              validation_data=val_Gen,\n                              validation_steps=val_Gen.n\/\/BATCH_SIZE,\n                              epochs=EPOCHS,\n                              max_queue_size=50,\n                              shuffle=True,\n                              verbose=1)\n# \u5c06\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4fdd\u5b58\u5230\u6587\u4ef6\nprint(\"[\u4fe1\u606f] \u4fdd\u5b58\u6a21\u578b...\")\nmodel.save(\"ResNet50V2.h5\", overwrite=True)\n# \u4fdd\u5b58\u8bad\u7ec3\u6027\u80fd\u56fe\ntitle = \"Cat Breeds ResNet50V2 Training Performance\"\ngraph_file = \"ResNet50V2_Training_Performance.pdf\"\nHistoryGraph(history, EPOCHS, title, graph_file).draw()","1e6ef2cd":"#\u5bfc\u5165\u5fc5\u8981\u7684\u5305\nfrom tensorflow.keras.applications import ResNet50V2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\n#\u521b\u5efaResNet50V2\u8fc1\u79fb\u5b66\u4e60\u7f51\u7edc\u6a21\u578b\n\ndef ResNet50V2_TL(input_shape=(224,224,3),classes=12):\n    '''\n    \u5b9e\u4f8b\u5316\u4e0d\u5305\u542b\u5206\u7c7b\u5c42\u7684ResNet50V2\u9884\u8bad\u7ec3\u9ed8\u8bb8\u51dd\uff0c\u7531include_top=False\u6307\u5b9a\uff0c\n    \u8be5\u6a21\u578b\u662f\u7531ImageNet\u6570\u636e\u96c6\u8bad\u7ec3\u7684\uff0c\u7531weights='imagenet\u2018\u6307\u5b9a\n    \u5b9e\u4f8b\u5316\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\n    '''\n    model_base=ResNet50V2(weights='imagenet',include_top=False,input_shape=input_shape)\n    #\u6211\u4eec\u53ea\u8bad\u7ec3\u9876\u90e8\u7684\u51e0\u5c42\uff08\u5206\u7c7b\u5c42\uff09\n    #\u9501\u4f4f\u6240\u6709ResNet50V2\u7684\u5377\u79ef\u5c42\n    for layer in model_base.layers:\n        layer.trainable=False\n    #\u65b0\u5efa\u4e00\u4e2a\u987a\u5e8f\u7f51\u7edc\u6a21\u578b\n    model=Sequential()\n\n    #\u6dfb\u52a0resnet50V2\u9884\u8bad\u7ec3\u6a21\u578b\n\n    model.add(model_base)\n\n    #\u5728\u6dfb\u52a0\u6211\u4eec\u81ea\u5df1\u7684\u5206\u7c7b\u5c42\n    model.add(Flatten())\n    model.add(Dense(512,activation='relu'))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dense(classes, activation='softmax'))\n\n    return  model\n\nif __name__=='__main__':\n    model=ResNet50V2_TL()\n    print(model.summary())\n","b43ca3e1":"# \u5bfc\u5165\u5fc5\u8981\u7684\u5305\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import SGD\n# \u6837\u672c\u56fe\u50cf\u6587\u4ef6\u76ee\u5f55\nimage_base_path = SAMPLE_FILE_PATH\n\n\"\"\"\n\u6837\u672c\u6587\u4ef6\u5217\u8868\n\u6587\u4ef6\u540d                   \u5206\u7c7b\u7f16\u53f7          \u732b\u72d7\u7f16\u53f7           \u79cd\u5185\u5206\u7c7b\u7f16\u53f7\n                        (1-37)          (1:\u732b,2:\u72d7)       (\u732b:1-12,\u72d7:1-25)\nAbyssinian_100          1                1                1\nyorkshire_terrier_9     37               2                25\n\"\"\"\n\nsample_files = SAMPLE_ANNOTATION_FILE\n\n# \u521b\u5efa\u732b\u54c1\u79cd12\u5206\u7c7bDataFrame\nread_for = open(sample_files)\nsample_files = []\nsample_labels = []\nfor txt_line in read_for:\n    if txt_line[0] == '#':\n        continue\n    file_name, class_id, cat_dog_id, sub_class_id = txt_line.strip().split(' ')\n    if cat_dog_id == '1':\n        sample_files.append(file_name + '.jpg')\n        sample_labels.append(sub_class_id)\nread_for.close()\n\nsample_data_frame = pd.DataFrame({'filename': sample_files, 'label': sample_labels})\n# \u8bad\u7ec3\u53c2\u6570\u8bbe\u7f6e\nBATCH_SIZE = 64\nEPOCHS = 80\nLEARN_RATE = 0.001\nCLASSES = 12\n\n# \u6784\u9020\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u5668\ndata_gen = ImageDataGenerator(rescale=1.\/255.,\n                              rotation_range=20,\n                              zoom_range=0.15,\n                              width_shift_range=0.2,\n                              height_shift_range=0.2,\n                              shear_range=0.15,\n                              horizontal_flip=True,\n                              validation_split=0.05,\n                              fill_mode=\"nearest\")\n\n# \u521d\u59cb\u5316\u8bad\u7ec3\u6570\u636e\u96c6\u751f\u6210\u5668\ntrainGen = data_gen.flow_from_dataframe(dataframe=sample_data_frame,\n                                        directory=image_base_path,\n                                        x_col='filename',\n                                        y_col='label',\n                                        target_size=(224, 224),\n                                        class_mode='categorical',\n                                        batch_size=BATCH_SIZE,\n                                        subset='training',\n                                        shuffle=True,\n                                        seed=42)\n# \u6784\u9020\u7528\u4e8e\u6570\u636e\u589e\u5f3a\u7684\u6821\u9a8c\u6570\u636e\u751f\u6210\u5668\nval_gen = ImageDataGenerator(rescale=1.\/255.,\n                             validation_split=0.05)\n# \u521d\u59cb\u5316\u6821\u9a8c\u6570\u636e\u96c6\u751f\u6210\u5668\nval_Gen = val_gen.flow_from_dataframe(dataframe=sample_data_frame,\n                                      directory=image_base_path,\n                                      x_col='filename',\n                                      y_col='label',\n                                      target_size=(224, 224),\n                                      class_mode='categorical',\n                                      batch_size=BATCH_SIZE,\n                                      subset='validation',\n                                      shuffle=True,\n                                      seed=42)\n# \u7f16\u8bd1\u6a21\u578b\nopt = SGD(lr=LEARN_RATE, decay=1e-6, momentum=0.9, nesterov=True)\nmodel = ResNet50V2_TL()\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n# \u8bad\u7ec3\u7f51\u7edc\nhistory = model.fit_generator(trainGen,\n                              steps_per_epoch=trainGen.n\/\/BATCH_SIZE,\n                              validation_data=val_Gen,\n                              validation_steps=val_Gen.n\/\/BATCH_SIZE,\n                              epochs=EPOCHS,\n                              max_queue_size=50,\n                              shuffle=True,\n                              verbose=1)\n# \u5c06\u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4fdd\u5b58\u5230\u6587\u4ef6\nprint(\"[\u4fe1\u606f] \u4fdd\u5b58\u6a21\u578b...\")\nmodel.save(\"ResNet50V2_Transfer_Learning_.h5\", overwrite=True)\n# \u4fdd\u5b58\u8bad\u7ec3\u6027\u80fd\u56fe\ntitle = \"Cat Breeds ResNet50V2 transfer learning Training Performance\"\ngraph_file = \"ResNet50V2_Transfer_Learning_Training_Performance.pdf\"\nHistoryGraph(history, EPOCHS, title, graph_file).draw()\n","6e52a07e":"\u56fe\u50cf\u7684\u968f\u673a\u88c1\u526a","2862ef8b":"ResNet50V2\u8fc1\u79fb\u7f51\u7edc\u6a21\u578b","fb66d1e2":"\u8bc4\u4ef7","ba678800":"\u6a21\u578b\u8bad\u7ec3\u76d1\u63a7\u5e76\u7ed8\u56fe","e1c437d5":"resnet50V2\u6a21\u578b\u8bad\u7ec3","cf697162":"ResNet50V2\u8fc1\u79fb\u5b66\u4e60\u7f51\u7edc\u6a21\u578b  \u4fee\u6539\u4e86\u5b66\u4e60\u7387\u4e3a0.001","a801188e":"\u7ef4\u5ea6\u91cd\u7f6e\u9884\u5904\u7406","dbe6b87d":"AspectAwarePreprocessor","cf12be90":"\u7edf\u4e00\u56fe\u50cf\u7684\u50cf\u7d20","34962de3":"CropPreprocessor","c089e84c":"hdf5\u6587\u4ef6\u7684\u751f\u6210","a1ef4bcc":"resnet50V2\u6a21\u578b\u7ed3\u6784","7281d9bf":"\u6837\u672c\u53bb\u5747\u503c\u5904\u7406","2e3c9e9d":"ResNet50V2\u8fc1\u79fb\u5b66\u4e60\u7f51\u7edc\u6a21\u578b"}}