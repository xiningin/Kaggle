{"cell_type":{"a3d92c03":"code","0143bd3a":"code","3661161d":"code","949b902b":"code","fbd75ed9":"code","3a193046":"code","b0ca8ffd":"code","b87dc7be":"code","2fe5fdc4":"code","f1b2a862":"code","4ea0ea51":"code","91d3e438":"code","493df7ef":"code","f525010e":"code","df6f9355":"code","b9af210f":"code","edca3094":"code","4d59fb5e":"code","c700d00a":"code","144658cd":"code","77ab13bc":"code","b740c72a":"code","fa5c6184":"code","2d511e27":"code","7ccc8d61":"code","ab51fd18":"code","db1eccc2":"code","00ed6ef9":"code","ec035377":"code","e7b5913c":"code","24fad5c3":"code","8810032a":"code","ab610ac3":"code","5280b0dd":"code","16c59947":"code","195bc264":"code","7d8d2188":"code","95e8e448":"code","51019ce4":"code","dd11dab3":"code","a3e02000":"code","8baf5925":"code","16afec7f":"code","98d22bd4":"code","c3ae679d":"code","b8a109c7":"code","d9b05d40":"code","cccfc395":"code","6f7acef8":"code","fedac465":"code","7f70fda9":"code","9dea3a4e":"code","0bdd34d1":"code","9b38390c":"code","b197880d":"code","c06190bb":"code","cdc20e5d":"code","9eb0a3fc":"code","14d23952":"code","5dbfac55":"code","1f7e2ead":"code","44a89ca5":"code","3badeeaa":"code","ad76cd01":"code","4d59ab33":"code","5a49440b":"markdown","70c1efde":"markdown","7b01acf4":"markdown","f39370cf":"markdown","18d60dda":"markdown","5fb3b7c3":"markdown","d477d4c5":"markdown","f873837d":"markdown","af9e7246":"markdown","01af6c25":"markdown","bf74bbcb":"markdown","6cbc5fc9":"markdown","454326a6":"markdown","e50e9c40":"markdown","c040dc6c":"markdown","a80c5ffe":"markdown","d77d7043":"markdown","c94199de":"markdown","bcbc61ba":"markdown","45693a01":"markdown","0d59fde1":"markdown","caf06c68":"markdown","1815f93c":"markdown","a0c7ad01":"markdown","9d92581e":"markdown","2ee3ea20":"markdown","5d5158b4":"markdown","79998cc3":"markdown","bbb169d1":"markdown","bf155a89":"markdown","595043ad":"markdown","380a4556":"markdown","38a5e3d3":"markdown","68f03b83":"markdown","dcf22324":"markdown","c4819654":"markdown","e972a0f1":"markdown","f760a4b9":"markdown","fdc104fe":"markdown","5c4bb761":"markdown","0922b875":"markdown","53a889fc":"markdown","4819506d":"markdown"},"source":{"a3d92c03":"# Loading Basic Libraries \nimport os\nimport gc   # No Version\nfrom pathlib import Path\n\nimport random\nimport six\nimport sys\nimport warnings\n\nimport dataclasses\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Any, Tuple, Union, List, Optional, Mapping\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\nimport yaml\nimport pickle","0143bd3a":"# Loading Data Processing Libraries \nimport numpy as np\nimport pandas as pd\n\nfrom logging import getLogger","3661161d":"# Loading Visualization Libraries \nfrom IPython.core.display import display, HTML\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as ptc\nimport seaborn as sns\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\n\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport plotly.io as pio","949b902b":"# Loading Imaging Libraries \nimport pydicom as pdc\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport albumentations as A","fbd75ed9":"# Loading Statistics Libraries \nimport scipy as sp","3a193046":"# Loading Models Libraries \nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom torch.nn import Linear\nfrom torch import nn, optim\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data.dataset import Dataset\n\nfrom ignite.engine import Events, Engine","b0ca8ffd":"#print(np.__version__)\n#print(sys.version_info)\n#print(Path.__version__)","b87dc7be":"#import types\n#def imports():\n#    for name, val in globals().items():\n#        if isinstance(val, types.ModuleType):\n#            yield val.__name__\n#list(imports())","2fe5fdc4":"!pip install detectron2 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.7\/index.html\n!pip install pytorch-pfn-extras timm","f1b2a862":"# Loading Loibraries after Installation\nimport pytorch_pfn_extras as ppe\nimport pytorch_pfn_extras.training.extensions as E\nfrom pytorch_pfn_extras.training import IgniteExtensionsManager\nfrom pytorch_pfn_extras.training.extension import Extension, PRIORITY_READER\nfrom pytorch_pfn_extras.training.manager import ExtensionsManager\n\nimport timm\nfrom detectron2.structures import BoxMode","4ea0ea51":"# To get python package version within jupyter notebook\nimport pkg_resources\nimport types\n\ndef get_imports():\n    for name, val in globals().items():\n        if isinstance(val, types.ModuleType):\n            # Split ensures you get root package, \n            # not just imported function\n            name = val.__name__.split(\".\")[0]\n\n        elif isinstance(val, type):\n            name = val.__module__.split(\".\")[0]\n\n        # Some packages are weird and have different\n        # imported names vs. system\/pip names. Unfortunately,\n        # there is no systematic way to get pip names from\n        # a package's imported name. You'll have to add\n        # exceptions to this list manually!\n        poorly_named_packages = {\n            \"PIL\": \"Pillow\",\n            \"sklearn\": \"scikit-learn\"\n        }\n        if name in poorly_named_packages.keys():\n            name = poorly_named_packages[name]\n\n        yield name\nimports = list(set(get_imports()))\n\n# The only way I found to get the version of the root package\n# from only the name of the package is to cross-check the names \n# of installed packages vs. imported packages\nrequirements = []\nfor m in pkg_resources.working_set:\n    if m.project_name in imports and m.project_name!=\"pip\":\n        requirements.append((m.project_name, m.version))\n\nfor r in requirements:\n    print(\"{}=={}\".format(*r))","91d3e438":"# Initialization of Configurations \nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_columns', 50)\npy.init_notebook_mode(connected=True)\npio.templates.default = \"plotly_dark\"\n%matplotlib inline","493df7ef":"train_dir = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\"\ntest_dir = \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test\"\n\ntrain_files = os.listdir(train_dir)\ntest_files = os.listdir(test_dir)\n\ntrain_df = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")\nsample_submission = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/sample_submission.csv\")","f525010e":"train_df","df6f9355":"train_df.isna().sum().to_frame().rename(columns={0:\"Nan_counts\"}).style.background_gradient(cmap=\"cool\")","b9af210f":"train_df.nunique().to_frame().rename(columns={0:\"Unique Values\"}).style.background_gradient(cmap=\"plasma\")","edca3094":"plt.figure(figsize=(26, 8))\nsns.countplot(x=\"class_name\", data=train_df)\nplt.title(\"Class Name Distribution\")\nplt.show()","4d59fb5e":"plt.figure(figsize=(8, 8))\nsns.countplot(x=\"class_id\", data=train_df)\nplt.title(\"Class ID Distribution\")\nplt.show()","c700d00a":"plt.figure(figsize=(8, 8))\nsns.countplot(x=\"rad_id\", data=train_df)\nplt.title(\"RAD ID Distribution\")\nplt.show()","144658cd":"plt.figure(figsize=(10, 10))\nsns.pairplot(train_df, hue='class_id')\nplt.show()","77ab13bc":"print(\"Number of train images: \", len(train_files))\nprint(\"Number of test images: \", len(test_files))","b740c72a":"print(\"Unique images in train_df: \", train_df.image_id.nunique())","fa5c6184":"for _ in range(3):\n    print(train_files[random.randint(0, len(train_files))])","2d511e27":"sample_fn = train_df.image_id.to_list()[random.randint(0, len(train_df))]\nsample = pdc.read_file(os.path.join(train_dir, sample_fn + \".dicom\"))\nsample","7ccc8d61":"def read_dicom_df(fn):\n    _ = pdc.read_file(os.path.join(train_dir, fn))\n    pass","ab51fd18":"def plot_pixel_array(data, figsize=(10,10)):\n    plt.figure(figsize=figsize)\n    plt.imshow(data, cmap=plt.cm.bone)\n    plt.show()","db1eccc2":"# ref kernel: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\ndef read_xray(path, voi_lut=True, fix_monochrome=True):\n    dcm_data = pdc.read_file(path)\n    \n    def show_dcm_info(data):\n        print(\"Gender :\", data.PatientSex)\n        if 'PixelData' in data:\n            rows = int(data.Rows)\n            cols = int(data.Columns)\n            print(\"Image size : {rows:d} x {cols:d}, {size:d} bytes\".format(\n                rows=rows, cols=cols, size=len(data.PixelData)))\n            if 'PixelSpacing' in data:\n                print(\"Pixel spacing :\", data.PixelSpacing)\n    \n    show_dcm_info(dcm_data)\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dcm_data.pixel_array, dcm_data)\n    else:\n        data = dcm_data.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dcm_data.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data","00ed6ef9":"print(\"Examining train images...\")\nfor _ in range(5):\n    fn = train_files[np.random.randint(0, len(train_files))]\n    file_path = os.path.join(train_dir, fn)\n    data = read_xray(file_path)\n    plot_pixel_array(data)","ec035377":"print(\"Examining test images...\")\nfor _ in range(5):\n    fn = test_files[np.random.randint(0, len(test_files))]\n    file_path = os.path.join(test_dir, fn)\n    data = read_xray(file_path)\n    plot_pixel_array(data)","e7b5913c":"for _ in range(10):\n    idx = np.random.randint(0, len(train_files))\n    img_id = train_df.loc[idx, 'image_id']\n    img = read_xray(os.path.join(train_dir, img_id+\".dicom\"))\n    plt.figure(figsize=(8, 14))\n    plt.imshow(img, cmap='gray')\n    plt.title(train_df.loc[idx, 'class_name'])\n    \n    if train_df.loc[idx, 'class_name'] != 'No finding':\n        bbox = [train_df.loc[idx, 'x_min'],\n                train_df.loc[idx, 'y_min'],\n                train_df.loc[idx, 'x_max'],\n                train_df.loc[idx, 'y_max']]\n        \n        patch = ptc.Rectangle((bbox[0], bbox[1]),\n                              bbox[2]-bbox[0],\n                              bbox[3]-bbox[1],\n                              ec='r', fc='none', lw=2.)\n        ax = plt.gca()\n        ax.add_patch(patch)","24fad5c3":"def save_yaml(filepath: str, content: Any, width: int = 120):\n    with open(filepath, \"w\") as f:\n        yaml.dump(content, f, width=width)","8810032a":"@dataclass\nclass Flags:\n    # General\n    debug: bool = True\n    outdir: str = \"results\/det\"\n    device: str = \"cuda:0\"\n\n    # Data config\n    imgdir_name: str = \"vinbigdata-chest-xray-resized-png-256x256\"\n    # split_mode: str = \"all_train\"  # all_train or valid20\n    seed: int = 111\n    target_fold: int = 0  # 0~4\n    label_smoothing: float = 0.0\n    # Model config\n    model_name: str = \"resnet18\"\n    model_mode: str = \"normal\"  # normal, cnn_fixed supported\n    # Training config\n    epoch: int = 20\n    batchsize: int = 8\n    valid_batchsize: int = 16\n    num_workers: int = 4\n    snapshot_freq: int = 5\n    ema_decay: float = 0.999  # negative value is to inactivate ema.\n    scheduler_type: str = \"\"\n    scheduler_kwargs: Dict[str, Any] = field(default_factory=lambda: {})\n    scheduler_trigger: List[Union[int, str]] = field(default_factory=lambda: [1, \"iteration\"])\n    aug_kwargs: Dict[str, Dict[str, Any]] = field(default_factory=lambda: {})\n    mixup_prob: float = -1.0  # Apply mixup augmentation when positive value is set.\n\n    def update(self, param_dict: Dict) -> \"Flags\":\n        # Overwrite by `param_dict`\n        for key, value in param_dict.items():\n            if not hasattr(self, key):\n                raise ValueError(f\"[ERROR] Unexpected key for flag = {key}\")\n            setattr(self, key, value)\n        return self","ab610ac3":"flags_dict = {\n    \"debug\": False,  # Change to True for fast debug run!\n    \"outdir\": \"results\/tmp_debug\",\n    # Data\n    \"imgdir_name\": \"vinbigdata-chest-xray-resized-png-256x256\",\n    # Model\n    \"model_name\": \"resnet18\",\n    # Training\n    \"num_workers\": 4,\n    \"epoch\": 15,\n    \"batchsize\": 8,\n    \"scheduler_type\": \"CosineAnnealingWarmRestarts\",\n    \"scheduler_kwargs\": {\"T_0\": 28125},  # 15000 * 15 epoch \/\/ (batchsize=8)\n    \"scheduler_trigger\": [1, \"iteration\"],\n    \"aug_kwargs\": {\n        \"HorizontalFlip\": {\"p\": 0.5},\n        \"ShiftScaleRotate\": {\"scale_limit\": 0.15, \"rotate_limit\": 10, \"p\": 0.5},\n        \"RandomBrightnessContrast\": {\"p\": 0.5},\n        \"CoarseDropout\": {\"max_holes\": 8, \"max_height\": 25, \"max_width\": 25, \"p\": 0.5},\n        \"Blur\": {\"blur_limit\": [3, 7], \"p\": 0.5},\n        \"Downscale\": {\"scale_min\": 0.25, \"scale_max\": 0.9, \"p\": 0.3},\n        \"RandomGamma\": {\"gamma_limit\": [80, 120], \"p\": 0.6},\n    }\n}","5280b0dd":"# args = parse()\nprint(\"torch\", torch.__version__)\nflags = Flags().update(flags_dict)\nprint(\"flags\", flags)\ndebug = flags.debug\noutdir = Path(flags.outdir)\nos.makedirs(str(outdir), exist_ok=True)\nflags_dict = dataclasses.asdict(flags)\nsave_yaml(str(outdir \/ \"flags.yaml\"), flags_dict)\n\n# --- Read data ---\ninputdir = Path(\"\/kaggle\/input\")\ndatadir = inputdir \/ \"vinbigdata-chest-xray-abnormalities-detection\"\nimgdir = inputdir \/ flags.imgdir_name\n\n# Read in the data CSV files\ntrain = pd.read_csv(datadir \/ \"train.csv\")\n# sample_submission = pd.read_csv(datadir \/ 'sample_submission.csv')","16c59947":"train.query(\"image_id == '50a418190bc3fb1ef1633bf9678929b3'\")","195bc264":"is_normal_df = train.groupby(\"image_id\")[\"class_id\"].agg(lambda s: (s == 14).sum()).reset_index().rename({\"class_id\": \"num_normal_annotations\"}, axis=1)\nis_normal_df.head()","7d8d2188":"num_normal_anno_counts = is_normal_df[\"num_normal_annotations\"].value_counts()\nnum_normal_anno_counts.plot(kind=\"bar\")\nplt.title(\"The number of 'No finding' annotations in each image\")","95e8e448":"num_normal_anno_counts_df = num_normal_anno_counts.reset_index()\nnum_normal_anno_counts_df[\"name\"] = num_normal_anno_counts_df[\"index\"].map({0: \"Abnormal\", 3: \"Normal\"})\nnum_normal_anno_counts_df","51019ce4":"normal_count = num_normal_anno_counts_df.iloc[0]['num_normal_annotations']\nabnormal_count = num_normal_anno_counts_df.iloc[1]['num_normal_annotations']\n\nprint(\"% of Normal X-Ray Images = \", round((normal_count\/(normal_count+abnormal_count))*100),3)\nprint(\"% of AbNormal X-Ray Images = \", round((abnormal_count\/(normal_count+abnormal_count))*100),3)","dd11dab3":"def get_vinbigdata_dicts(\n    imgdir: Path,\n    train_df: pd.DataFrame,\n    train_data_type: str = \"original\",\n    use_cache: bool = True,\n    debug: bool = True,\n    target_indices: Optional[np.ndarray] = None,\n):\n    debug_str = f\"_debug{int(debug)}\"\n    train_data_type_str = f\"_{train_data_type}\"\n    cache_path = Path(\".\") \/ f\"dataset_dicts_cache{train_data_type_str}{debug_str}.pkl\"\n    if not use_cache or not cache_path.exists():\n        print(\"Creating data...\")\n        train_meta = pd.read_csv(imgdir \/ \"train_meta.csv\")\n        if debug:\n            train_meta = train_meta.iloc[:500]  # For debug....\n\n        # Load 1 image to get image size.\n        image_id = train_meta.loc[0, \"image_id\"]\n        image_path = str(imgdir \/ \"train\" \/ f\"{image_id}.png\")\n        image = cv2.imread(image_path)\n        resized_height, resized_width, ch = image.shape\n        print(f\"image shape: {image.shape}\")\n\n        dataset_dicts = []\n        for index, train_meta_row in tqdm(train_meta.iterrows(), total=len(train_meta)):\n            record = {}\n\n            image_id, height, width = train_meta_row.values\n            filename = str(imgdir \/ \"train\" \/ f\"{image_id}.png\")\n            record[\"file_name\"] = filename\n            record[\"image_id\"] = image_id\n            record[\"height\"] = resized_height\n            record[\"width\"] = resized_width\n            objs = []\n            for index2, row in train_df.query(\"image_id == @image_id\").iterrows():\n                # print(row)\n                # print(row[\"class_name\"])\n                # class_name = row[\"class_name\"]\n                class_id = row[\"class_id\"]\n                if class_id == 14:\n                    # It is \"No finding\"\n                    # This annotator does not find anything, skip.\n                    pass\n                else:\n                    # bbox_original = [int(row[\"x_min\"]), int(row[\"y_min\"]), int(row[\"x_max\"]), int(row[\"y_max\"])]\n                    h_ratio = resized_height \/ height\n                    w_ratio = resized_width \/ width\n                    bbox_resized = [\n                        int(row[\"x_min\"]) * w_ratio,\n                        int(row[\"y_min\"]) * h_ratio,\n                        int(row[\"x_max\"]) * w_ratio,\n                        int(row[\"y_max\"]) * h_ratio,\n                    ]\n                    obj = {\n                        \"bbox\": bbox_resized,\n                        \"bbox_mode\": BoxMode.XYXY_ABS,\n                        \"category_id\": class_id,\n                    }\n                    objs.append(obj)\n            record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n            pickle.dump(dataset_dicts, f)\n\n    print(f\"Load from cache {cache_path}\")\n    with open(cache_path, mode=\"rb\") as f:\n        dataset_dicts = pickle.load(f)\n    if target_indices is not None:\n        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n    return dataset_dicts\n\ndef get_vinbigdata_dicts_test(\n    imgdir: Path, test_meta: pd.DataFrame, use_cache: bool = True, debug: bool = True,\n):\n    debug_str = f\"_debug{int(debug)}\"\n    cache_path = Path(\".\") \/ f\"dataset_dicts_cache_test{debug_str}.pkl\"\n    if not use_cache or not cache_path.exists():\n        print(\"Creating data...\")\n        # test_meta = pd.read_csv(imgdir \/ \"test_meta.csv\")\n        if debug:\n            test_meta = test_meta.iloc[:500]  # For debug....\n\n        # Load 1 image to get image size.\n        image_id = test_meta.loc[0, \"image_id\"]\n        image_path = str(imgdir \/ \"test\" \/ f\"{image_id}.png\")\n        image = cv2.imread(image_path)\n        resized_height, resized_width, ch = image.shape\n        print(f\"image shape: {image.shape}\")\n\n        dataset_dicts = []\n        for index, test_meta_row in tqdm(test_meta.iterrows(), total=len(test_meta)):\n            record = {}\n\n            image_id, height, width = test_meta_row.values\n            filename = str(imgdir \/ \"test\" \/ f\"{image_id}.png\")\n            record[\"file_name\"] = filename\n            # record[\"image_id\"] = index\n            record[\"image_id\"] = image_id\n            record[\"height\"] = resized_height\n            record[\"width\"] = resized_width\n            # objs = []\n            # record[\"annotations\"] = objs\n            dataset_dicts.append(record)\n        with open(cache_path, mode=\"wb\") as f:\n            pickle.dump(dataset_dicts, f)\n\n    print(f\"Load from cache {cache_path}\")\n    with open(cache_path, mode=\"rb\") as f:\n        dataset_dicts = pickle.load(f)\n    return dataset_dicts\n","a3e02000":"\"\"\"\nReferenced `chainer.dataset.DatasetMixin` to work with pytorch Dataset.\n\"\"\"\nimport numpy\n\nclass DatasetMixin(Dataset):\n\n    def __init__(self, transform=None):\n        self.transform = transform\n\n    def __getitem__(self, index):\n        \"\"\"Returns an example or a sequence of examples.\"\"\"\n        if torch.is_tensor(index):\n            index = index.tolist()\n        if isinstance(index, slice):\n            current, stop, step = index.indices(len(self))\n            return [self.get_example_wrapper(i) for i in\n                    six.moves.range(current, stop, step)]\n        elif isinstance(index, list) or isinstance(index, numpy.ndarray):\n            return [self.get_example_wrapper(i) for i in index]\n        else:\n            return self.get_example_wrapper(index)\n\n    def __len__(self):\n        \"\"\"Returns the number of data points.\"\"\"\n        raise NotImplementedError\n\n    def get_example_wrapper(self, i):\n        \"\"\"Wrapper of `get_example`, to apply `transform` if necessary\"\"\"\n        example = self.get_example(i)\n        if self.transform:\n            example = self.transform(example)\n        return example\n\n    def get_example(self, i):\n        \"\"\"Returns the i-th example.\n\n        Implementations should override it. It should raise :class:`IndexError`\n        if the index is invalid.\n\n        Args:\n            i (int): The index of the example.\n\n        Returns:\n            The i-th example.\n\n        \"\"\"\n        raise NotImplementedError\n","8baf5925":"class VinbigdataTwoClassDataset(DatasetMixin):\n    def __init__(self, dataset_dicts, image_transform=None, transform=None, train: bool = True,\n                 mixup_prob: float = -1.0, label_smoothing: float = 0.0):\n        super(VinbigdataTwoClassDataset, self).__init__(transform=transform)\n        self.dataset_dicts = dataset_dicts\n        self.image_transform = image_transform\n        self.train = train\n        self.mixup_prob = mixup_prob\n        self.label_smoothing = label_smoothing\n\n    def _get_single_example(self, i):\n        d = self.dataset_dicts[i]\n        filename = d[\"file_name\"]\n\n        img = cv2.imread(filename)\n        if self.image_transform:\n            img = self.image_transform(img)\n        img = torch.tensor(np.transpose(img, (2, 0, 1)).astype(np.float32))\n\n        if self.train:\n            label = int(len(d[\"annotations\"]) > 0)  # 0 normal, 1 abnormal\n            if self.label_smoothing > 0:\n                if label == 0:\n                    return img, float(label) + self.label_smoothing\n                else:\n                    return img, float(label) - self.label_smoothing\n            else:\n                return img, float(label)\n        else:\n            # Only return img\n            return img, None\n\n    def get_example(self, i):\n        img, label = self._get_single_example(i)\n        if self.mixup_prob > 0. and np.random.uniform() < self.mixup_prob:\n            j = np.random.randint(0, len(self.dataset_dicts))\n            p = np.random.uniform()\n            img2, label2 = self._get_single_example(j)\n            img = img * p + img2 * (1 - p)\n            if self.train:\n                label = label * p + label2 * (1 - p)\n\n        if self.train:\n            label_logit = torch.tensor([1 - label, label], dtype=torch.float32)\n            return img, label_logit\n        else:\n            # Only return img\n            return img\n\n    def __len__(self):\n        return len(self.dataset_dicts)","16afec7f":"dataset_dicts = get_vinbigdata_dicts(imgdir, train, debug=debug)\ndataset = VinbigdataTwoClassDataset(dataset_dicts)","98d22bd4":"index = 0\nimg, label = dataset[index]\nplt.imshow(img.cpu().numpy().transpose((1, 2, 0)) \/ 255.)\nplt.title(f\"{index}-th image: label {label}\")","c3ae679d":"class Transform:\n    def __init__(\n        self, hflip_prob: float = 0.5, ssr_prob: float = 0.5, random_bc_prob: float = 0.5\n    ):\n        self.transform = A.Compose(\n            [\n                A.HorizontalFlip(p=hflip_prob),\n                A.ShiftScaleRotate(\n                    shift_limit=0.0625, scale_limit=0.1, rotate_limit=10, p=ssr_prob\n                ),\n                A.RandomBrightnessContrast(p=random_bc_prob),\n            ]\n        )\n\n    def __call__(self, image):\n        image = self.transform(image=image)[\"image\"]\n        return image\n","b8a109c7":"aug_dataset = VinbigdataTwoClassDataset(dataset_dicts, image_transform=Transform())","d9b05d40":"index = 0\n\nn_images = 4\n\nfig, axes = plt.subplots(1, n_images, figsize=(16, 5))\nfor i in range(n_images):\n    # Each time the data is accessed, the result is different due to random augmentation!\n    img, label = aug_dataset[index]\n    ax = axes[i]\n    ax.imshow(img.cpu().numpy().transpose((1, 2, 0)) \/ 255.)\n    ax.set_title(f\"{index}-th image: label {label}\")\nplt.show()","cccfc395":"class Transform:\n    def __init__(self, aug_kwargs: Dict):\n        self.transform = A.Compose(\n            [getattr(A, name)(**kwargs) for name, kwargs in aug_kwargs.items()]\n        )\n\n    def __call__(self, image):\n        image = self.transform(image=image)[\"image\"]\n        return image","6f7acef8":"class CNNFixedPredictor(nn.Module):\n    def __init__(self, cnn: nn.Module, num_classes: int = 2):\n        super(CNNFixedPredictor, self).__init__()\n        self.cnn = cnn\n        self.lin = Linear(cnn.num_features, num_classes)\n        print(\"cnn.num_features\", cnn.num_features)\n\n        # We do not learn CNN parameters.\n        # https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html\n        for param in self.cnn.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        feat = self.cnn(x)\n        return self.lin(feat)\n","fedac465":"def build_predictor(model_name: str, model_mode: str = \"normal\"):\n    if model_mode == \"normal\":\n        # normal configuration. train all parameters.\n        return timm.create_model(model_name, pretrained=True, num_classes=2, in_chans=3)\n    elif model_mode == \"cnn_fixed\":\n        # normal configuration. train all parameters.\n        # https:\/\/rwightman.github.io\/pytorch-image-models\/feature_extraction\/\n        timm_model = timm.create_model(model_name, pretrained=True, num_classes=0, in_chans=3)\n        return CNNFixedPredictor(timm_model, num_classes=2)\n    else:\n        raise ValueError(f\"[ERROR] Unexpected value model_mode={model_mode}\")\n","7f70fda9":"def accuracy(y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n    \"\"\"Computes multi-class classification accuracy\"\"\"\n    assert y.shape[:-1] == t.shape, f\"y {y.shape}, t {t.shape} is inconsistent.\"\n    pred_label = torch.max(y.detach(), dim=-1)[1]\n    count = t.nelement()\n    correct = (pred_label == t).sum().float()\n    acc = correct \/ count\n    return acc\n\ndef accuracy_with_logits(y: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n    \"\"\"Computes multi-class classification accuracy\"\"\"\n    assert y.shape == t.shape\n    gt_label = torch.max(t.detach(), dim=-1)[1]\n    return accuracy(y, gt_label)","9dea3a4e":"def cross_entropy_with_logits(input, target, dim=-1):\n    loss = torch.sum(- target * F.log_softmax(input, dim), dim)\n    return loss.mean()\n","0bdd34d1":"class Classifier(nn.Module):\n    \"\"\"two class classfication\"\"\"\n\n    def __init__(self, predictor, lossfun=cross_entropy_with_logits):\n        super().__init__()\n        self.predictor = predictor\n        self.lossfun = lossfun\n        self.prefix = \"\"\n\n    def forward(self, image, targets):\n        outputs = self.predictor(image)\n        loss = self.lossfun(outputs, targets)\n        metrics = {\n            f\"{self.prefix}loss\": loss.item(),\n            f\"{self.prefix}acc\": accuracy_with_logits(outputs, targets).item()\n        }\n        ppe.reporting.report(metrics, self)\n        return loss, metrics\n\n    def predict(self, data_loader):\n        pred = self.predict_proba(data_loader)\n        label = torch.argmax(pred, dim=1)\n        return label\n\n    def predict_proba(self, data_loader):\n        device: torch.device = next(self.parameters()).device\n        y_list = []\n        self.eval()\n        with torch.no_grad():\n            for batch in data_loader:\n                if isinstance(batch, (tuple, list)):\n                    # Assumes first argument is \"image\"\n                    batch = batch[0].to(device)\n                else:\n                    batch = batch.to(device)\n                y = self.predictor(batch)\n                y = torch.softmax(y, dim=-1)\n                y_list.append(y)\n        pred = torch.cat(y_list)\n        return pred\n","9b38390c":"supported_models = timm.list_models()\nprint(f\"{len(supported_models)} models are supported in timm.\")\nprint(supported_models)","b197880d":"\"\"\"\nFrom https:\/\/github.com\/pfnet-research\/kaggle-lyft-motion-prediction-4th-place-solution\n\"\"\"\nclass EMA(object):\n    \"\"\"Exponential moving average of model parameters.\n\n    Ref\n     - https:\/\/github.com\/tensorflow\/addons\/blob\/v0.10.0\/tensorflow_addons\/optimizers\/moving_average.py#L26-L103\n     - https:\/\/anmoljoshi.com\/Pytorch-Dicussions\/\n\n    Args:\n        model (nn.Module): Model with parameters whose EMA will be kept.\n        decay (float): Decay rate for exponential moving average.\n        strict (bool): Apply strict check for `assign` & `resume`.\n        use_dynamic_decay (bool): Dynamically change decay rate. If `True`, small decay rate is\n            used at the beginning of training to move moving average faster.\n    \"\"\"  # NOQA\n\n    def __init__(\n        self,\n        model: nn.Module,\n        decay: float,\n        strict: bool = True,\n        use_dynamic_decay: bool = True,\n    ):\n        self.decay = decay\n        self.model = model\n        self.strict = strict\n        self.use_dynamic_decay = use_dynamic_decay\n        self.logger = getLogger(__name__)\n        self.n_step = 0\n\n        self.shadow = {}\n        self.original = {}\n\n        # Flag to manage which parameter is assigned.\n        # When `False`, original model's parameter is used.\n        # When `True` (`assign` method is called), `shadow` parameter (ema param) is used.\n        self._assigned = False\n\n        # Register model parameters\n        for name, param in model.named_parameters():\n            if param.requires_grad:\n                self.shadow[name] = param.data.clone()\n\n    def step(self):\n        self.n_step += 1\n        if self.use_dynamic_decay:\n            _n_step = float(self.n_step)\n            decay = min(self.decay, (1.0 + _n_step) \/ (10.0 + _n_step))\n        else:\n            decay = self.decay\n\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                new_average = (1.0 - decay) * param.data + decay * self.shadow[name]\n                self.shadow[name] = new_average.clone()\n\n    # alias\n    __call__ = step\n\n    def assign(self):\n        \"\"\"Assign exponential moving average of parameter values to the respective parameters.\"\"\"\n        if self._assigned:\n            if self.strict:\n                raise ValueError(\"[ERROR] `assign` is called again before `resume`.\")\n            else:\n                self.logger.warning(\n                    \"`assign` is called again before `resume`.\"\n                    \"shadow parameter is already assigned, skip.\"\n                )\n                return\n\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                self.original[name] = param.data.clone()\n                param.data = self.shadow[name]\n        self._assigned = True\n\n    def resume(self):\n        \"\"\"Restore original parameters to a model.\n\n        That is, put back the values that were in each parameter at the last call to `assign`.\n        \"\"\"\n        if not self._assigned:\n            if self.strict:\n                raise ValueError(\"[ERROR] `resume` is called before `assign`.\")\n            else:\n                self.logger.warning(\"`resume` is called before `assign`, skip.\")\n                return\n\n        for name, param in self.model.named_parameters():\n            if param.requires_grad:\n                assert name in self.shadow\n                param.data = self.original[name]\n        self._assigned = False\n","c06190bb":"\"\"\"\nFrom https:\/\/github.com\/pfnet-research\/kaggle-lyft-motion-prediction-4th-place-solution\n\"\"\"\nclass LRScheduler(Extension):\n    \"\"\"A thin wrapper to resume the lr_scheduler\"\"\"\n\n    trigger = 1, 'iteration'\n    priority = PRIORITY_READER\n    name = None\n\n    def __init__(self, optimizer: optim.Optimizer, scheduler_type: str, scheduler_kwargs: Mapping[str, Any]) -> None:\n        super().__init__()\n        self.scheduler = getattr(optim.lr_scheduler, scheduler_type)(optimizer, **scheduler_kwargs)\n\n    def __call__(self, manager: ExtensionsManager) -> None:\n        self.scheduler.step()\n\n    def state_dict(self) -> None:\n        return self.scheduler.state_dict()\n\n    def load_state_dict(self, to_load) -> None:\n        self.scheduler.load_state_dict(to_load)\n","cdc20e5d":"\ndef create_trainer(model, optimizer, device) -> Engine:\n    model.to(device)\n\n    def update_fn(engine, batch):\n        model.train()\n        optimizer.zero_grad()\n        loss, metrics = model(*[elem.to(device) for elem in batch])\n        loss.backward()\n        optimizer.step()\n        return metrics\n    trainer = Engine(update_fn)\n    return trainer\n","9eb0a3fc":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=flags.seed)\n# skf.get_n_splits(None, None)\ny = np.array([int(len(d[\"annotations\"]) > 0) for d in dataset_dicts])\nsplit_inds = list(skf.split(dataset_dicts, y))\ntrain_inds, valid_inds = split_inds[flags.target_fold]  # 0th fold\ntrain_dataset = VinbigdataTwoClassDataset(\n    [dataset_dicts[i] for i in train_inds],\n    image_transform=Transform(flags.aug_kwargs),\n    mixup_prob=flags.mixup_prob,\n    label_smoothing=flags.label_smoothing,\n)\nvalid_dataset = VinbigdataTwoClassDataset([dataset_dicts[i] for i in valid_inds])\n","14d23952":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=flags.batchsize,\n    num_workers=flags.num_workers,\n    shuffle=True,\n    pin_memory=True,\n)\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=flags.valid_batchsize,\n    num_workers=flags.num_workers,\n    shuffle=False,\n    pin_memory=True,\n)\n\ndevice = torch.device(flags.device)\n\npredictor = build_predictor(model_name=flags.model_name, model_mode=flags.model_mode)\nclassifier = Classifier(predictor)\nmodel = classifier\n# optimizer = optim.Adam(model.parameters(), lr=1e-3)\noptimizer = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=1e-3)\n\n# Train setup\ntrainer = create_trainer(model, optimizer, device)\n\nema = EMA(predictor, decay=flags.ema_decay)\n\ndef eval_func(*batch):\n    loss, metrics = model(*[elem.to(device) for elem in batch])\n    # HACKING: report ema value with prefix.\n    if flags.ema_decay > 0:\n        classifier.prefix = \"ema_\"\n        ema.assign()\n        loss, metrics = model(*[elem.to(device) for elem in batch])\n        ema.resume()\n        classifier.prefix = \"\"\n\nvalid_evaluator = E.Evaluator(\n    valid_loader, model, progress_bar=False, eval_func=eval_func, device=device\n)\n\n# log_trigger = (10 if debug else 1000, \"iteration\")\nlog_trigger = (1, \"epoch\")\nlog_report = E.LogReport(trigger=log_trigger)\nextensions = [\n    log_report,\n    E.ProgressBarNotebook(update_interval=10 if debug else 100),  # Show progress bar during training\n    E.PrintReportNotebook(),  # Show \"log\" on jupyter notebook  \n    # E.ProgressBar(update_interval=10 if debug else 100),  # Show progress bar during training\n    # E.PrintReport(),  # Print \"log\" to terminal\n    E.FailOnNonNumber(),  # Stop training when nan is detected.\n]\nepoch = flags.epoch\nmodels = {\"main\": model}\noptimizers = {\"main\": optimizer}\nmanager = IgniteExtensionsManager(\n    trainer, models, optimizers, epoch, extensions=extensions, out_dir=str(outdir),\n)\n# Run evaluation for valid dataset in each epoch.\nmanager.extend(valid_evaluator)\n\n# Save predictor.pt every epoch\nmanager.extend(\n    E.snapshot_object(predictor, \"predictor.pt\"), trigger=(flags.snapshot_freq, \"epoch\")\n)\n# Check & Save best validation predictor.pt every epoch\n# manager.extend(E.snapshot_object(predictor, \"best_predictor.pt\"),\n#                trigger=MinValueTrigger(\"validation\/module\/nll\",\n#                trigger=(flags.snapshot_freq, \"iteration\")))\n\n# --- lr scheduler ---\nif flags.scheduler_type != \"\":\n    scheduler_type = flags.scheduler_type\n    print(f\"using {scheduler_type} scheduler with kwargs {flags.scheduler_kwargs}\")\n    manager.extend(\n        LRScheduler(optimizer, scheduler_type, flags.scheduler_kwargs),\n        trigger=flags.scheduler_trigger,\n    )\n\nmanager.extend(E.observe_lr(optimizer=optimizer), trigger=log_trigger)\n\nif flags.ema_decay > 0:\n    # Exponential moving average\n    manager.extend(lambda manager: ema(), trigger=(1, \"iteration\"))\n\n    def save_ema_model(manager):\n        ema.assign()\n        torch.save(predictor.state_dict(), outdir \/ \"predictor_ema.pt\")\n        ema.resume()\n\n    manager.extend(save_ema_model, trigger=(flags.snapshot_freq, \"epoch\"))\n\n_ = trainer.run(train_loader, max_epochs=epoch)","5dbfac55":"torch.save(predictor.state_dict(), outdir \/ \"predictor_last.pt\")\ndf = log_report.to_dataframe()\ndf.to_csv(outdir \/ \"log.csv\", index=False)\ndf","1f7e2ead":"# --- Prediction ---\nprint(\"Training done! Start prediction...\")\n# valid data\nvalid_pred = classifier.predict_proba(valid_loader).cpu().numpy()\nvalid_pred_df = pd.DataFrame({\n    \"image_id\": [dataset_dicts[i][\"image_id\"] for i in valid_inds],\n    \"class0\": valid_pred[:, 0],\n    \"class1\": valid_pred[:, 1]\n})\nvalid_pred_df.to_csv(outdir\/\"valid_pred.csv\", index=False)\n\n# test data\ntest_meta = pd.read_csv(inputdir \/ \"vinbigdata-testmeta\" \/ \"test_meta.csv\")\ndataset_dicts_test = get_vinbigdata_dicts_test(imgdir, test_meta, debug=debug)\ntest_dataset = VinbigdataTwoClassDataset(dataset_dicts_test, train=False)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=flags.valid_batchsize,\n    num_workers=flags.num_workers,\n    shuffle=False,\n    pin_memory=True,\n)\ntest_pred = classifier.predict_proba(test_loader).cpu().numpy()\ntest_pred_df = pd.DataFrame({\n    \"image_id\": [d[\"image_id\"] for d in dataset_dicts_test],\n    \"class0\": test_pred[:, 0],\n    \"class1\": test_pred[:, 1]\n})\ntest_pred_df.to_csv(outdir\/\"test_pred.csv\", index=False)","44a89ca5":"# --- Test dataset prediction result ---\ntest_pred_df","3badeeaa":"sns.distplot(valid_pred_df[\"class0\"].values, color='green', label='valid pred')\nsns.distplot(test_pred_df[\"class0\"].values, color='orange', label='test pred')\nplt.title(\"Prediction results histogram\")\nplt.xlim([0., 1.])\nplt.legend()","ad76cd01":"# pred_2class = pd.read_csv(inputdir\/\"vinbigdata-2class-prediction\/2-cls test pred.csv\")  # LB 0.230\n# low_threshold = 0.0\n# high_threshold = 0.95\n\npred_2class = pd.read_csv(inputdir\/\"vinbigdata2classpred\/test_pred.csv\")\nlow_threshold = 0.0\nhigh_threshold = 0.976\npred_2class","4d59ab33":"NORMAL = \"14 1 0 0 1 1\"\n\npred_det_df = pd.read_csv(inputdir\/\"public-kernel-vinbigdata-detectron2-prediction-v9\/results\/20210110_train_all_500k_512\/submission.csv\")  # You can load from another submission.csv here too.\nn_normal_before = len(pred_det_df.query(\"PredictionString == @NORMAL\"))\nmerged_df = pd.merge(pred_det_df, pred_2class, on=\"image_id\", how=\"left\")\n\n# 1. p < low_threshold                   -> \"Keep\": Do nothing, Keep det prediction.\n# 2. low_threshold <= p < high_threshold -> \"Add\": Just \"Add\" Normal prediction\n# 3. high_threshold <= p                 -> \"Replace\": Replace with Normal prediction\n\nif \"target\" in merged_df.columns:\n    merged_df[\"class0\"] = 1 - merged_df[\"target\"]\n\nc0, c1, c2 = 0, 0, 0\nfor i in range(len(merged_df)):\n    p0 = merged_df.loc[i, \"class0\"]\n    if p0 < low_threshold:\n        # Keep, do nothing.\n        c0 += 1\n    elif low_threshold <= p0 and p0 < high_threshold:\n        # Add, keep \"det\" preds and add normal pred.\n        merged_df.loc[i, \"PredictionString\"] += f\" 14 {p0} 0 0 1 1\"\n        c1 += 1\n    else:\n        # Replace, remove all \"det\" preds.\n        merged_df.loc[i, \"PredictionString\"] = NORMAL\n        c2 += 1\n\nn_normal_after = len(merged_df.query(\"PredictionString == @NORMAL\"))\nprint(\n    f\"n_normal: {n_normal_before} -> {n_normal_after} with threshold {low_threshold} & {high_threshold}\"\n)\nprint(f\"Keep {c0} Add {c1} Replace {c2}\")\nsubmission_filepath = str(outdir \/ \"submission.csv\")\nsubmission_df = merged_df[[\"image_id\", \"PredictionString\"]]\nsubmission_df.to_csv(submission_filepath, index=False)\nprint(f\"Saved to {submission_filepath}\")","5a49440b":"Here mixup augmentation in the dataset. It makes interpolation of 2 images, with the label is also modified according to the mix ratio. \nmixup augmentation is especially useful when the number of data is limited.\nBecause it can make combination of any 2 images, instead of just using 1 image.\n\nI also added label smoothing feature. \nSometimes it is difficult to learn label is 0, 1. \nLabel smoothing changes its label 0 -> 0.01 & 1 -> 0.99. \nBy smoothing the label the loss surface becomes more \"soft\", and sometimes model can learn well.","70c1efde":"7.2.1 Visualizing Train and Test Images","7b01acf4":"15. Reference\n\n(1) Classifier Pipeline from the kernel of VinBigData 2-class classifier complete pipeline\n\n(2) Trained Detection Model from the Kernel of VinBigData detectron2 train\n\n(3) Training 2 Class Classifier from the Kernel of VinBigData \ud83c\udf1f2 Class Filter\ud83c\udf1f\n\n(4) VBD Chest X-ray Abnormalities Detection | EDA\ud83d\udcca\ud83d\udd34\n\n(5) Deep learning Pytorch in Computer Vision!\n\n(6) Image Augmentation Library developed by famous Kagglers\n\n(7) Pytorch-Image-Models with Petrained Weights\n\n(8) Pytorch-Ignite Traning\/Evaluation Abstraction Framework\n\n(9) Pytorch Feature-Rich Functionality on Ignite","f39370cf":"We have 15000 images for Train and 3000 images for Test. But, the length of train_df is 67914. So, are there any duplicacy or something? Let's have a sanity check on unique image_ids of train_df.","18d60dda":"**7. Exploratory Data Analysis**\n\n**7.1 Exploration of Train and Test Data**\n\nThe train set metadata, with one row for each object, including a class and a bounding box. \nSome images in both test and train have multiple objects.\n\nColumns\nimage_id - unique image identifier\nclass_name - the name of the class of detected object (or \"No finding\")\nclass_id - the ID of the class of detected object\nrad_id - the ID of the radiologist that made the observation\nx_min - minimum X coordinate of the object's bounding box\ny_min - minimum Y coordinate of the object's bounding box\nx_max - maximum X coordinate of the object's bounding box\ny_max - maximum Y coordinate of the object's bounding box","5fb3b7c3":"**8. Image Visualizaion and Augmentation with Albumentations**\n\nWhen you train CNN models, Image Augmentation is important to avoid model to overfit.\nThe below examples to use Albumentations to run image augmentation very easily\nAt first, I will define pytorch Dataset class for this competition, which can be also used later in the training.","d477d4c5":"So around 70% of the data is actually \"Normal\" X-ray images.\n\nBut about 30% of the images need thoracic abnormality location detection.","f873837d":"**4. Data Preparation**\n\nPreprocessing x-ray image format (dicom) into normal png image format is already done by @xhlulu in the below discussion:\n - [Multiple preprocessed datasets: 256\/512\/1024px, PNG and JPG, modified and original ratio](https:\/\/www.kaggle.com\/c\/vinbigdata-chest-xray-abnormalities-detection\/discussion\/207955).\n\nSo I will just use the dataset \n- [VinBigData Chest X-ray Resized PNG (256x256)](https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-chest-xray-resized-png-256x256) to skip the preprocessing and focus on modeling part. \n\nPlease upvote the dataset as well!","af9e7246":"**10. Model Training**","01af6c25":"# Table of Contents\n\n1. Introduction\n2. Objectives\n3. Data\n4. Dataset Preparation\n5. Installation\n6. Exploratory Data Analysis\n7. Methods\n8. Image Visualizaion and Augmentation with Albumentations\n9. Model Building\n10. Model Training\n11. Model Testing and Validation\n12. Results\n13. Discussion\n14. Furture Improvement\n15. References\n16. Appendices","bf74bbcb":"**13. Discussion**\n\n(1) General\n - The baseline submission of score is 0.141\n - Augmentation is very important hyperparameter to improve model's performance, and \n   you want to experiment with various configurations.\n   updated `Transform` function is written to **support all the augmentations implemented in \n   albumentations**.\n   You can specify `aug_kwargs` from external configuration inside `Configuration`\n - **Exponential Moving Average of model weights** is calculated by `EMA` class during \n   training, together with showing its validation loss. We can usually obtrain more stable \n   models with EMA\n - Obtrained training history results really easily by just accessing `LogReport` class, which \n   is useful for managing a lot of experiments during kaggle competitions.\n - Now, the score improved by about **0.8, which is significant**!\n\n(2) Details\n - Tried to change several `low_threshold` value and lower `low_threshold` achieved better \n   results. So it may be okay to set `low_threshold=0.0` which means **always add \"No finding\"    prediction with the predicted probability.**<br\/>\n - Noticed that setting `high_threshold` value less than 1 is important, which means we have a \n   benefit to **remove abnormality predictions** for the images which is highly likely to be  \n   normal.<br\/>\n - Curently used [training kernel](https:\/\/www.kaggle.com\/corochann\/vinbigdata-detectron2-\n   train) with abnormal image during training and model tend to produce more abnormal boxes. \n - Better to include normal images for training to learn where there is **no** abnormality. \n   <br\/>\n - Try to **include \"No finding\" class during detection training** (by adding virtual \"No \n   finding\" boxes, or by adding global classifier together with the detection).","6cbc5fc9":"# VinBigData Chest X-ray Abnormalities Classifier","454326a6":"It could confirm that **always 3 radiologists opinions match** for normal - abnormal diagnosis.\n\nNoticed that it does not apply for the other classes. \ni.e., 3 radiologists opinions sometimes do not match for the other class of thoracic abnormalities.","e50e9c40":"That's all!\n\n<h3 style=\"color:red\">If this kernel helps you, please upvote to keep me motivated \ud83d\ude01<br>Thanks!<\/h3>","c040dc6c":"**5. Installation**\n\nDetectron2 is not pre-installed in this kaggle docker, so we need to install it in here. \n\nWe need to know CUDA and pytorch version to install correct `detectron2`.","a80c5ffe":"What kind of models are supported in the `timm` library?","d77d7043":"**10. Model Training**","c94199de":"**11. Model Validation**","bcbc61ba":"7.2 Exploration of Normal and Abnromal Classes\n\nWe need to check how many normal class exist in the training data.\nIt is classified as \"class_name = No finding\" and \"class_id = 14\".\n\nHowever you need to be careful that 3 radiologists annotated for each image, \nso you can find 3 annotations as you can see below.","45693a01":"Is there an image that the 3 radiologists' opinions differ?\n\nLet's check number of \"No finding\" annotations for each image.\nIf the opinions are in complete agreement the number of \"No finding\" annotations, then it should be \n**0 -> Abnormal(all radiologists does not think this is normal)\" or \n\"1 -> Normal(all radiologists think this is normal)\"**.","0d59fde1":"**11. Model Testing and Validation**","caf06c68":"To use augmentation, you can just define dataset with the `Transform` function.","1815f93c":"You can access each image and its label (0=Normal, 1=Abnormal) by just access `dataset` with index.","a0c7ad01":"It's confirmed that we have one or more than one ground truths in our train dataset. Now, we got a lot to explore, such as:\n\nWhether the boxes are overlapped on a single image? What about different classes? Are they over-lapped too for different classes? In a single image, what's the maximum number of class (or taregt present)? etc.\n\nWe will continue to explore these aspects and try to come with some insights, but let's worry about DICOM at the first place. Let's read out some image file names from both the train and test folders.","9d92581e":"**11. Model Testing and Validation**\n\nWhen we have few data, running stable evaluation is very important. \nWe can use cross validation to reduce validation error standard deviation.\n\nI will use **[`StratifiedKFold`]** to keep the balance between normal\/abnormal ratio same for the train & validation dataset.\n\nAccording to [this discussion](https:\/\/www.kaggle.com\/c\/vinbigdata-chest-xray-abnormalities-detection\/discussion\/208837#1139712), using multi label stratified kfold https:\/\/github.com\/trent-b\/iterative-stratification may be more stable.","2ee3ea20":"**2. Objective**\n\nExisting methods of interpreting chest X-ray images classify them into a list of findings. There is currently no specification of their locations on the image which sometimes leads to inexplicable results. A solution for localizing findings on chest X-ray images is needed for providing doctors with more meaningful diagnostic assistance\n\nIn this competition, we are classifying common thoracic lung diseases and localizing critical findings. This is an **object detection and classification** problem from chest x-ray image (radiographs)","5d5158b4":"**6. Methods (Implementation)**\n\nLoading Libraries","79998cc3":"**16. Appendices**\n\n- There are several other X-ray images in this research field.<br\/>\n- The paper of this competition dataset [\"VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations\u201d.](https:\/\/arxiv.org\/pdf\/2012.15029.pdf)\nsummarizes the existing public datasets.\n![image.png](attachment:image.png)","bbb169d1":"Let's visualize, looks good. <br\/>\nYou can see each image looks different (rotated, brightness is different etc...) even if it is generated from the same image :)","bf155a89":"**3. Data**\n**3.1 Intro**\n\nIn this competition:\n\n* Task: Automatically localize and classify **14 types of thoracic abnormalities** from chest radiographs.\n* Dataset: Consisting of **18,000 scans: 15,000 train images** and will be evaluated on a test set of **3,000 images**.\n\nFor each test image, you will be predicting a bounding box and class for all findings. If you predict that there are no findings, you should create a prediction of \"14 1 0 0 1 1\" (14 is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0).\n\nThe images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying","595043ad":"Flags Class summarizes all the configuratoin available during the training. \n\nThe users can change various hyperparameters to experiment improving your models","380a4556":"**7.2 Exploration of DICOM**\n\nDICOM files can be exchanged between two entities that are capable of receiving image and patient data in DICOM format.\n\nThe different devices come with DICOM Conformance Statements which state which DICOM classes they support. \nThe standard includes a file format definition and a network communications protocol that uses TCP\/IP to communicate between systems.\n\nLet's read one sample and grind it.","38a5e3d3":"**12. Results**\n\nWe can submit **14 prob 1 1 0 0** where the `prob` is the normal probability in the discussion [[Scoring bug] Improve your LB score by 0.053, just adding \"14 1 0 0 1 1\"](https:\/\/www.kaggle.com\/c\/vinbigdata-chest-xray-abnormalities-detection\/discussion\/211971)!\n\nPropose new post processing (similar to [this](https:\/\/www.kaggle.com\/c\/vinbigdata-chest-xray-abnormalities-detection\/discussion\/211971#1157809) by @pestipeti):\n\nHere `p` is the **normal probability**.\n\n1. `p < low_threshold`                   -> Do nothing, Keep det prediction.\n2. `low_threshold <= p < high_threshold` -> Just \"Add\" Normal prediction, **keep** detection prediction.\n3. `high_threshold <= p`                 -> Replace with Normal prediction with normal score 1.0, **remove** all detection predictoin.","68f03b83":"14.Future Improvement\n\n(1) Changing training configurations by just changing Flags (flags_dict) configuration.\n\nData\nimgdir_name: You can use different preprocessed image introduced in [Multiple preprocessed datasets: 256\/512\/1024px, PNG and JPG, modified and original ratio]\nModel\nmodel_name: You can try various kinds of models timm library support, by just changing model_name.\nTraining\nepoch, batch_size, scheduler_type etc: Try changing these hyperparamters, to see the difference!\nAugmentation: Please modify Transform class to add your augmentation, it's easy to support more augmentations with albumentations library.\n(2) Checking training loss\/training accuracy: If it is almost same with validation loss\/accuracy and it is not accurate enough, model's representation power may be not enough, or data augmentation is too strong. You can try more deeper models, decrease data augmentation or using more rich data (high-resolution image).\n\n(3) Checking training loss\/validation loss difference: If validation loss is very high compared to training loss, it is a sign of overfitting. Try using smaller models, increase data augmentation or apply regularization (dropout etc).","dcf22324":"**3.2 Dataset information**\n\nThe dataset comprises 18,000 postero-anterior (PA) CXR scans in DICOM format, which were de-identified to protect patient privacy. All images were labeled by a panel of experienced radiologists for the presence of 14 critical radiographic findings as listed below:\n\n\n\nThe \"No finding\" observation (14) was intended to capture the absence of all findings above.","c4819654":"Now creating the dataset is just easy as following:","e972a0f1":"To run augmentation on this image, I will define `Transform` class which is applied each time the data is accessed.","f760a4b9":"7.2.2 Checking the Localizations","fdc104fe":"**13. Discussion**\n\n**Extensions** - Each role:\n - **`ProgressBar` (`ProgressBarNotebook`)**: Shows training progress in formatted style.\n - **`LogReport`**: Logging metrics reported by `ppe.reporter.report` (see `LyftMultiRegressor` for reporting point) method and save to **log** file. It automatically collects reported value in each iteration and saves the \"mean\" of reported value for regular frequency (for example every 1 epoch).\n - **`PrintReport` (`PrintReportNotebook`)**: Prints the value which `LogReport` collected in formatted style.\n - **`Evaluator`**: Evaluate on validation dataset.\n - **`snapshot_object`**: Saves the object. Here the `model` is saved in regular interval `flags.snapshot_freq`. Even you quit training using Ctrl+C without finishing all the epoch, the intermediate trained model is saved and you can use it for inference.\n - **`LRScheduler`**: You can insert learning rate scheduling with this extension, together with the regular interval call specified by `trigger`. Here cosine annealing is applied (configured by Flags) by calling `scheduler.step()` every iteration.\n - **`observe_lr`**: `LogReport` will check optimizer's learning rate using this extension. So you can follow how the learning rate changed through the training","5c4bb761":"**1. Introduction**\n\nWhen you have a broken arm, radiologists help save the day\u2014and the bone. These doctors diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. **The interpretation of chest X-rays can lead to medical misdiagnosis**, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe\/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.\n\nThe annotations were collected via VinBigData's web-based platform, VinLab. Details on building the dataset can be found in the organizer's recent paper \u201cVinDr-CXR: An open dataset of chest X-rays with radiologist's annotations\u201d.und in the organizer's recent paper \u201cVinDr-CXR: An open dataset of chest X-rays with radiologist's annotations\u201d.","0922b875":"**9. Model Building(CNN Models)**\n\nI will use `timm` this time. You can just re-use latest research results without hustle.","53a889fc":"So, for a single patient(yes, there is one-one mapping in patients and image_ids, so there is no duplicates or more than one images per patient, as confirmed by the organizers in the discussion forum), we have following entities:\n\nPatient's Sex Samples per Pixel Photometric Interpretation Rows Columns Pixel Spacing Bits Allocated Bits Stored High Bit Pixel Representation Window Center Window Width Rescale Intercept Rescale Slope Lossy Image Compression Pixel Data","4819506d":"434 models are supported including **resnet** related models, **efficientnet**\nYou may wonder which model should be used?\nI will go with `resnet18` as a baseline at first, and try using more deeper\/latest models in the experiment."}}