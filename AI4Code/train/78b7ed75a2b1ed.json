{"cell_type":{"a9872ae7":"code","9c29ebf1":"code","840992cc":"code","2afc70e1":"code","2de5ac89":"code","c8ea1212":"code","fe75f346":"code","24fae5ce":"code","59ad617c":"code","0c9881a6":"markdown","418d196e":"markdown","bda5484d":"markdown","3431ad17":"markdown","8d64420e":"markdown"},"source":{"a9872ae7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c29ebf1":"import pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics.pairwise import cosine_similarity","840992cc":"df=pd.read_excel('..\/input\/exerciseCB.xlsx',sheet_name='CB - Simply Unary',header=1,index_col=0)","2afc70e1":"#Subsetting the initial dataframe\nq_data = df.iloc[:20,:10]\nuser_data=df.iloc[:20,13:17].fillna(0)\nanswer_data=df.iloc[:20,18:22].fillna(0)\ncopy_1=q_data.copy()\ncopy_2=q_data.copy()","2de5ac89":"#I am going to iterate through users and follow the steps of \n#1) multiplying questions with user like\/dislikes, \n#2) creating a user profile by summing the columns from step 1,\n#3) multiplying this user profile in step 2 with initial table of questions\nn=1\nresults=pd.DataFrame()\nfor i in user_data.columns:\n    for col in q_data.columns:\n        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col])\n        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n    #Storing the results and then initiating empty lists again to have a fresh start for every user\n    results[n]=product\n    user_profile=[]\n    product=[]\n    n=n+1\nresults.index=user_data.index \ntop = pd.DataFrame()\nfor r in results.columns:\n    ranking = results.loc[:,r].sort_values(ascending = False)\n    top[r]=ranking.index\ntop.columns=user_data.columns\n#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\nprint(top.head(5))","c8ea1212":"#Exactly the same process with weights calculated row-wise to leverage on the number of topics in one question\nweights=1\/copy_1.sum(axis=1)\nn=1\nresults=pd.DataFrame()\ncopy_2=q_data.copy()\nfor i in user_data.columns:\n    for col in q_data.columns:\n        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*weights)\n        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n    results[n]=product\n    user_profile=[]\n    product=[]\n    n=n+1\n#Simply sorting values and making it look clean by having same column names\nresults.index=user_data.index \ntop = pd.DataFrame()\nfor r in results.columns:\n    ranking = results.loc[:,r].sort_values(ascending = False)\n    top[r]=ranking.index\ntop.columns=user_data.columns\n#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\nprint(top.head(5))","fe75f346":"#I am going to iterate through users and follow the steps of \n#1) multiplying questions with user like\/dislikes, \n#2) getting weights based on number of topics in a question\n#3) getting IDF from frequency of occurance of a topic in all questions\n#4) to get the user profile I used IDF, weights and question data\n#5) multiplying this user profile in step 4 with initial table of questions\nlist2=[]\nlist3=[]\n#Getting IDF\nfor i in q_data.index:\n    list1=1\/sum(q_data.loc[i,:])\n    list2.append(list1)\nfor i in q_data.columns:\n    list1=np.log10(20\/sum(q_data.loc[:,i]))\n    list3.append(list1)\nlist3=pd.DataFrame(list3).transpose()\nlist3.columns=q_data.columns\ncopy_2=q_data.copy()\nn=1\nresults=pd.DataFrame()\n#Getting user_profile by using questions data IDF and weights\nfor i in user_data.columns:\n    for col in q_data.columns:\n        copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n        user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n        product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n    results[n]=product\n    user_profile=[]\n    product=[]\n    n=n+1\n#Simply sorting values and making it look clean by having same column names\nresults.index=user_data.index \ntop = pd.DataFrame()\nfor r in results.columns:\n    rank = results.loc[:,r].sort_values(ascending = False)\n    top[r]=rank.index\ntop.columns=user_data.columns\n#For User 4 I am recommending random questions because we have no data on the preference and feedback, we can also exclude that user but I think it is better to recommend smth then understand what the user likes\nprint(top.head(5))\n","24fae5ce":"#Initialising empty lists\nlist2=[]\nlist3=[]\ncopy_1=q_data.copy()\n#Obtaining IDF by duplicating the same steps described in the previous method\nfor i in q_data.index:\n    list1=1\/sum(q_data.loc[i,:])\n    list2.append(list1)\nfor i in q_data.columns:\n    list1=np.log10(20\/sum(q_data.loc[:,i]))\n    list3.append(list1)\nlist3=pd.DataFrame(list3).transpose()\nlist3.columns=q_data.columns\nn=1\nresults=pd.DataFrame()\ncopy_2=q_data.copy()\n#Resolving cold start issue by returning the average of other users for the new user\nfor i in user_data.columns:\n    if ((user_data.loc[:,i].mean())==0 and (user_data.loc[:,i].std())==0):\n        results.loc[:,n]=results.loc[:,results.columns!=n].mean(axis=1)\n    else:\n        copy_2=q_data.copy()\n        #Product of User Feedback, IDF, Weights and topic column \n        for col in q_data.columns:\n            copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n            user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n            product=cosine_similarity(copy_1, user_profile).sum(axis=1)\n        results[n]=product\n        user_profile=[]\n        product=[]\n        n=n+1\n#Simply sorting values and making it look clean by having same column names\nresults.index=user_data.index   \ntop=pd.DataFrame()\nfor r in results.columns:\n    ranking=results.loc[:,r].sort_values(ascending = False)\n    top[r]=ranking.index\ntop.columns=user_data.columns\nprint(top.head(5))","59ad617c":"#In the Notebook I will explain the steps but the rationale can be found in the PDF\n#Rescaling the data in User Answers to logically merge it with User Feedback\ncopy_3=q_data.copy()\nanswer_data.columns=user_data.columns\nV1=pd.DataFrame(preprocessing.scale(answer_data))\na_copy=answer_data.copy()\n\n#After scaling some data is distorted in particluar the zero values, hence I take two steps to recover them\n#1) I create a copy table which holds duplicates\n#2) I multiply values with that table to get values back because they hold important info to us\nfor a in a_copy.columns:\n    for c in a_copy.index:\n        if (a_copy.loc[c,a]!=0):\n            a_copy.loc[c,a]=1\nm_product=np.multiply(V1,a_copy)\nm_product.columns=answer_data.columns\nm_product.index=answer_data.index\na_data=m_product.copy()\n#I am following the logic from previous methods and multiplying the user data with answer data which was molded from user feedback and answers\nfor n in user_data.columns:\n    for p in user_data.index:\n        if (user_data.loc[p,n]==1):\n            if (a_data.loc[p,n]!=0):\n                user_data.loc[p,n]=user_data.loc[p,n]*a_data.loc[p,n]\n                #Here I have a condition for negative values because if I don't treat them specially I will receive a positive based on two negatives, which does not correctly mirror the User Profile\n            elif(user_data.loc[p,n]== -1):\n                if (a_data.loc[p,n]<0):\n                    user_data.loc[p,n]=(-1)*user_data.loc[p,n]*a_data.loc[p,n]\n                if(answer_data.loc[p,n]>0):\n                    user_data.loc[p,n]=(-1)*user_data.loc[p,n]*a_data.loc[p,n]\n                #Here I have one more condition for zeros because we want to cover for cases when the person did not like\/dislike but still gave a good answer, I wanted to count this in \n            elif(user_data.loc[p,n]== 0):\n                if (a_data.loc[p,n]!=0):\n                    user_data.loc[p,n]=a_data.loc[p,n]  \n#Initializing empty lists\nlist2=[]\nlist3=[]\n#Assigning weights based on frequency of occurence of a topic\nfor i in q_data.index:\n    list1=1\/sum(q_data.loc[i,:])\n    list2.append(list1)\n#Computing IDF\nfor i in q_data.columns:\n    list1=np.log10(20\/sum(q_data.loc[:,i]))\n    list3.append(list1)\nlist3=pd.DataFrame(list3).transpose()\nlist3.columns=q_data.columns\nn=1\nresults=pd.DataFrame()\n#Here I make sure that for users who have a cold start, hence a st.dev of zero average is taken from other users' predictions\nfor i in user_data.columns:\n    if ((user_data.loc[:,i].std())==0):\n        results.loc[:,n]=results.loc[:,results.columns!=n].mean(axis=1)\n    else:\n        copy_2=q_data.copy()\n        #Same methodology as in previous steps of using product of user profile(in this case a new one), weights and question data\n        for col in q_data.columns:\n            copy_2[col]=np.where(q_data.loc[:,col]==0,q_data.loc[:,col],user_data.loc[:,i]*q_data.loc[:,col]*list2*list3.loc[0,col])\n            user_profile=pd.DataFrame(copy_2.sum(axis=0)).transpose()\n            product=cosine_similarity(copy_3, user_profile).sum(axis=1)\n        results[n]=product\n        user_profile=[]\n        product=[]\n        n=n+1\nresults.index=user_data.index \n#Simply sorting values and making it look clean by having same column names\ntop=pd.DataFrame()\nfor r in results.columns:\n    ranking=results.loc[:,r].sort_values(ascending = False)\n    top[r]=ranking.index\ntop.columns=user_data.columns\nprint(top.head(5))","0c9881a6":"**Hybrid Challenge**","418d196e":"**Simple Unary**","bda5484d":"**Hybrid Switching**","3431ad17":"**Weighted Units**","8d64420e":"**IDF**"}}