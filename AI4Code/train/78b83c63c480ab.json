{"cell_type":{"e91b9228":"code","e86d4321":"code","cd0ef148":"code","d6fb422d":"code","20caad8c":"code","ea70b286":"code","eee19074":"code","ef4c06ff":"code","25fcaf07":"code","851541a0":"code","d219e311":"code","d0fcafbd":"code","3dd42019":"code","46ed968e":"code","c43bc207":"code","8cd2904f":"code","0162378d":"code","f296363d":"code","91c3cccf":"code","bd385ffc":"code","207fe0ac":"code","efada967":"code","d59407fe":"code","58a261b2":"code","a13ce03b":"code","01a3b228":"code","cbc79f48":"code","0f9de2f6":"code","0bfc47df":"code","7006750d":"code","9f0b3e90":"code","1fbb4c0f":"code","3104b57b":"code","69bf2de6":"code","a1edecd5":"code","0b1f692f":"markdown"},"source":{"e91b9228":"from keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50 \n#,NASNetLarge\nfrom keras.layers import Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam ,RMSprop\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.vgg16 import VGG16","e86d4321":"image_width, image_height = 224,224\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=20,\n      shear_range=0.2,\n      zoom_range=0.2,\n      width_shift_range=0.3,\n      height_shift_range=0.3,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255.)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    \"..\/input\/skin-cencer-dataset-images\/train\",\n                    batch_size=128,\n                    class_mode='categorical',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)     \n\ntest_generator =  test_datagen.flow_from_directory(\n                    \"..\/input\/skin-cencer-dataset-images\/test\",\n                    batch_size=128,\n                    class_mode='categorical',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)\n","cd0ef148":"train_steps = train_generator.n \/\/ train_generator.batch_size\n\nprint(train_steps)","d6fb422d":"test_steps = test_generator.n \/\/ test_generator.batch_size\n\nprint(test_steps)","20caad8c":"#rs = ResNet50(weights='imagenet', include_top=True, input_shape=(image_width, image_height, 3))","ea70b286":"rs = ResNet50(include_top=True,\n                 weights= None,\n                 input_tensor=None,\n                 input_shape=(image_width, image_height, 3),\n                 pooling='avg',\n                 classes=2)","eee19074":"for layers in (rs.layers):\n    layers.trainable = False","ef4c06ff":"rs.layers","25fcaf07":"model = Sequential()\n\n# Add the ResNet50 convolutional base model\nmodel.add(rs)\n\n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.summary()\n\n","851541a0":"lr = 1e-6\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])","d219e311":"mcp = ModelCheckpoint('modelResNe.h5', verbose=1)","d0fcafbd":"es = EarlyStopping(patience=2,verbose=1)","3dd42019":"history = model.fit(train_generator,steps_per_epoch=train_steps,epochs=30,validation_data=test_generator,validation_steps=test_steps,verbose=1,callbacks=[mcp,es])","46ed968e":"model.evaluate(test_generator, verbose=1, steps=test_steps)","c43bc207":"import matplotlib.pyplot as plt\n\naccuracy      = history.history['accuracy']\nval_accuracy  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, accuracy)\nplt.plot(epochs, val_accuracy)\nplt.title('Training and validation accuracy')\nplt.figure()\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Training and validation loss')","8cd2904f":"train_generator.class_indices","0162378d":"import numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\nimage_width, image_height = 224,224\n\nimg = image.load_img('..\/input\/skin-cencer-dataset-images\/data\/train\/benign\/1002.jpg', target_size=(image_width, image_height))\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis = 0)\nimg\/= 255.\nmodel = load_model('modelResNe.h5')\nresult = model.predict(img)\ntrain_generator.class_indices\nif result[0][0] == 0:\n    prediction = 'Malignant'\nelse:\n    prediction = 'Benign'\nprint(prediction)","f296363d":"prediction = model.predict_classes(img)\nprediction","91c3cccf":"model.save('modelResNe.h5')","bd385ffc":"vvg = VGG16(input_shape=(224,224, 3), include_top=False,weights = 'imagenet')","207fe0ac":"vvg.summary()","efada967":"for layers in (vvg.layers):\n    layers.trainable = False","d59407fe":"model2 = Sequential()\n\n# Add the vvg convolutional base model\nmodel2.add(vvg)\n\n# Add new layers\nmodel2.add(Flatten())\nmodel2.add(Dense(4096 , activation='relu'))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(4096 , activation='relu'))\nmodel2.add(Dropout(0.3))\nmodel2.add(Dense(4096, activation='relu'))\nmodel2.add(Dropout(0.4))\nmodel2.add(Dense(2, activation='softmax'))\n\nmodel2.summary()","58a261b2":"#z = layers.Flatten()(pretrained_model.output)\n \n#z  = layers.Dense(4096 , activation='relu')(z)\n#z  = layers.Dense(4096 , activation='relu')(z)\n \n   \n#z = layers.Dense(2, activation='softmax')(z)","a13ce03b":"#model2 = Model(vvg.input, z)","01a3b228":"#model2.summary()","cbc79f48":"lr = 1e-4\nmodel2.compile(loss='categorical_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])","0f9de2f6":"mcp2 = ModelCheckpoint('modelVVG.h5', verbose=1)","0bfc47df":"es2 = EarlyStopping(patience=2,verbose=1)","7006750d":"#history = model.fit(train_generator,epochs=10,batch_size = 128)\n","9f0b3e90":"history = model2.fit(train_generator,steps_per_epoch=train_steps,epochs=30,validation_data=test_generator,validation_steps=test_steps,verbose=1,callbacks=[mcp2,es2])","1fbb4c0f":"model2.evaluate(test_generator, verbose=1, steps=test_steps)","3104b57b":"import matplotlib.pyplot as plt\n\naccuracy      = history.history['accuracy']\nval_accuracy  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, accuracy)\nplt.plot(epochs, val_accuracy)\nplt.title('Training and validation accuracy')\nplt.figure()\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Training and validation loss')","69bf2de6":"import numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\nimage_width, image_height = 224,224\n\nimg = image.load_img('..\/input\/skin-cencer-dataset-images\/test\/malignant\/1026.jpg', target_size=(image_width, image_height))\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis = 0)\nimg\/= 255.\nmodel = load_model('modelVVG.h5')\nresult = model.predict(img)\ntrain_generator.class_indices\nif result[0][0] == 0:\n    prediction = 'Malignant'\nelse:\n    prediction = 'Benign'\nprint(prediction)","a1edecd5":"model.save('modelVVG.h5')","0b1f692f":"### Skin cancer\n\nSkin cancers are cancers that arise from the skin. They are due to the development of abnormal cells that have the ability to invade or spread to other parts of the body.\n\n[![image.png](attachment:image.png)](http:\/\/)\n\n\n\n\n\n\n\n\n\n"}}