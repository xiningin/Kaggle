{"cell_type":{"4de03a34":"code","f598114a":"code","4f119d09":"code","fed408e4":"code","92b99358":"code","0d8b2a9f":"code","b5e04a44":"code","1a9d9ce8":"code","cbb26256":"code","d774c733":"code","c958472f":"code","705d4092":"code","2beb31ec":"code","9535984a":"markdown","c9e87df1":"markdown","14b78707":"markdown","26daecbc":"markdown","61c8283d":"markdown","45cd6971":"markdown","73a18de9":"markdown"},"source":{"4de03a34":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f598114a":"df = pd.read_csv(\"..\/input\/ad-data\/Advertising.csv\")\ndf.head(5)","4f119d09":"df['total_spend'] = df['TV'] + df['radio'] + df['newspaper']\nfinal_df = pd.concat([df['total_spend'],df['sales']],axis=1)\nfinal_df.head(5)\n","fed408e4":"#Defining x_i and y_i from our dataframe\nx_i = final_df['total_spend'] \ny_i = final_df['sales']","92b99358":"from sklearn.preprocessing import StandardScaler","0d8b2a9f":"scaler = StandardScaler()\nx_i = np.array(x_i) # This is done because Standard scaler requires data as np arrays\nx_i = x_i.reshape(-1,1) # Reshaping as scaler needs 2D data\n\n# Scaling Part\nscaler.fit(x_i)\nscaled_x_i = scaler.transform(x_i)\n\n# Reshaping again for future use.(plots)\nscaled_x_i = scaled_x_i.reshape(-1)\n\n# Reassigning the data\nx_i = final_df['total_spend'] \ny_i = final_df['sales']","b5e04a44":"# This is the block which performs gradient descent.\n\n# Feel free to play with this hyperparameters. \nepoch = 25 # How many times?\nstep_size = 0.1 # Also known as learning rate\n\n# Initializing m and c to 0. \nm = 0     \nc = 0\n\n# Initializing some empty lists (plotting purpose)\nmse=[]\ncoef_m=[]\ncoef_c=[]\n\nwhile epoch>=0:\n    \n    random_index = np.random.randint(0,200) # This is where we bring in Stochasticity. \n\n    # Length of the data\n    N = 1 #             Due to the Stochastic nature of our computation\n\n    y_hat = m*scaled_x_i[random_index] + c \n    y_hat_actual = m*scaled_x_i + c\n    \n    J = (1\/200)*sum((y_i - y_hat_actual)**2)  \n    mse.append(J)\n    \n    J_m = (1\/N)*(2*scaled_x_i[random_index]*(y_hat-y_i[random_index]))   # Calculated Gradient with respect to m.\n    J_c = (1\/N)*(2*(y_hat-y_i[random_index]))                            # Calculated Gradient with respect to c.\n    \n    # Updating the m and c to achieve minimum error\n    m = m - step_size*J_m \n    coef_m.append(m)  # Storing the values of m and c to plot.\n    \n    c = c - step_size*J_c\n    coef_c.append(c)  # Storing the values of m and c to plot.\n    \n    # Loop condition\n    epoch=epoch-1","1a9d9ce8":"# Thanks to Stephen Welch - Welch Labs\n\n# Interactive Visualization of coefficients m and c converging.\nfrom ipywidgets import interact\ndef plotGD(epoch=2):\n    fig = plt.figure(figsize=(12,10))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(coef_m[0:epoch], mse[0:epoch], linestyle='-', marker='o', linewidth=2, markeredgecolor='none',color='black')\n    plt.grid(1)\n    plt.title('Relation of $m$ with error')\n    plt.xlabel('Values of $m$')\n    plt.ylabel('Error')\n    #plt.xlim([3.5, 4.6])\n    #plt.ylim([40, 250])\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(coef_c[0:epoch], mse[0:epoch], linestyle='-', marker='o', linewidth=2, markeredgecolor='none',color='black')\n    plt.grid(1)\n    plt.title('Relation of $m$ with error')\n    plt.xlabel('Values of $c$')\n    plt.ylabel('Error')\n    #plt.xlim([3.5, 4.6])\n    #plt.ylim([40, 250])\n\ninteract(plotGD, epoch=(1, 25));\n#Please Go to Edit mode for interative Animation","cbb26256":"def plotGD_animate(epoch=2):\n    fig = plt.figure(figsize=(12,6),dpi=100) \n    sns.scatterplot(x=x_i,y=y_i)\n    \n    y_hat = coef_m[epoch]*scaled_x_i + coef_c[epoch] # Its important to pass scaled values of x_i since we calculated in that way. \n    \n    plt.plot(x_i,y_hat,color='red',linestyle='-', linewidth=2, markeredgecolor='none')\n    plt.grid(1)\n    plt.title('Advertisement Data')\n    plt.xlabel('Total Spend in Lakh')\n    plt.ylabel('Total Sale Generated in Crores Rs.')\n\ninteract(plotGD_animate, epoch=(1, 25));\n#Please Go to Edit mode for interative Animation","d774c733":"# Try to use animate\n%matplotlib inline\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(x=x_i,y=y_i)\n\nplt.text(300, 5, \"I am Learning!!\", fontsize = 22,bbox = dict(facecolor = 'yellow', alpha = 0.5))\nplt.text(300, 8, \"Stochastically\", fontsize = 22,bbox = dict(facecolor = 'yellow', alpha = 0.5))\nplt.grid(1)\nplt.title('Advertisement Data')\nplt.xlabel('Total Spend in Lakh')\nplt.ylabel('Total Sale Generated in Crores Rs.')\n\ni = list(range(0,epoch))\nx = scaled_x_i\ny = m*x + c\nline, = ax.plot(x_i, y,color='red')\n\n\ndef animate(i):\n    line.set_ydata(coef_m[i]*x + coef_c[i])  # update the data.\n    return line,\n\nani = animation.FuncAnimation(fig, animate, interval=100, blit=True, save_count=25);\nplt.close()\nHTML(ani.to_jshtml())\n","c958472f":"ani.save('animation_stochastic.gif', writer='imagemagick', fps=20)","705d4092":"# Getting the value of m and c where error (mse) was minimum\nk = mse.index(min(mse))\nm = coef_m[k]\nc = coef_c[k]\n# Rescaling m and c to fit in our previous data\nalpha = scaler.scale_[0]  \n\n# Rescaling \nm_new = m\/scaler.scale_[0]\nc_new = c\/scaler.scale_[0]\n\nprint(\"m =\", m_new,\"and c =\", c_new) # These are the values of m and c. \nprint('These m and c values are found in epoch number:',k)","2beb31ec":"RMSE = np.sqrt(min(mse))\nprint(RMSE)","9535984a":"# Rescaling  - Just to scale back in the original domain. ","c9e87df1":"# Visualize for yourself how the curve is learning. :)","14b78707":"# Some more Visualization","26daecbc":"# Linear Regression - Gradient Stochastic descent method.\n\nIn this method we iteratively calculate the value of $m$ and $c$ such that it reduces the mean of squares of the error.\nBut with a twist. \nThat is in each iteration (also called epoch), only the gradient evaluated at a **single point** - $x_{i}[random\\_index]$ instead of evaluating at the set of all samples.This reduces a lot of computation.  [Wiki](https:\/\/en.wikipedia.org\/wiki\/Stochastic_gradient_descent)\n\nSince we have only one sample all the summation job is gone. (and N=1 now)\n\nSo now our equations are like this, \n\n$$\nJ = (y_{i}[random\\_index]-\\hat{y})^{2}\n$$\n\nPlugging in the nature of our prediction equation i.e $\\hat{y} = mx_{i}[random\\_index] + c$\n$$\nJ = (y_{i}-(mx_{i}[random\\_index] + c))^{2}\n$$\n\nThis J is generally called the **Cost Function**. \n\nNow our aim is to minimise it. \n\nPartial derivatives will be ,\n$$\n\\frac{\\partial J}{\\partial m} = 2x_{i}[random\\_index](\\hat{y}-y_{i}[random\\_index])\n$$\n\n\n$$\n\\frac{\\partial J}{\\partial c} = 2(\\hat{y}-y_{i}[random\\_index])\n$$\n\nLets get started!\n","61c8283d":"## Without scaling the code was not converging. - So I am using the Standard Scaler.\n(Any help in understanding why this exactly happens is highly appreciated.I am still not able to find a convincing answer.)","45cd6971":"# Evaluation.","73a18de9":"# Summary\n\nStochasting gradientr descent simplified calculation to such an extent and this didnt even affect our RMSE of previous experiments(in part1 and part2 we had RMSE of 2.58 ) . This is so cool. And just an image for you.[Image](https:\/\/drive.google.com\/file\/d\/186hl55psf7lMBHXCan-ikvV8nS9w1ubK\/view?usp=sharing)"}}