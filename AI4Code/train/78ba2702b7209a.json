{"cell_type":{"197b2158":"code","93d25b80":"code","e0a3ff71":"code","91fb8496":"code","30242ad3":"code","3fc82880":"code","cd64a253":"code","3f5745f9":"code","92be7c68":"code","674fbf8a":"code","ba8b7ca1":"code","b7e96ff3":"code","8be158ae":"code","0ad42a27":"code","03be18c3":"code","82b84b86":"code","dfe35db3":"code","3ca562f8":"code","2a23b9c2":"code","7618db82":"code","cba0f6a4":"code","bc8d6b2b":"code","2ea322a8":"code","928b5c00":"code","d795d387":"code","5fbf7460":"code","1085040f":"code","a70aab1a":"code","c38e8623":"code","25f9915e":"code","b75edf4b":"code","360bb8b6":"code","0727ac48":"code","df426046":"code","57d43582":"code","5dbf85a3":"code","a9dd56df":"markdown","87884075":"markdown","80a77da6":"markdown","ef18f093":"markdown","72ef16ec":"markdown","ba4b522f":"markdown","44801933":"markdown","35cfee14":"markdown","03ad01af":"markdown","ffddbb0f":"markdown","2b247b4c":"markdown","28e61130":"markdown","af30d02d":"markdown","92848def":"markdown","4704ce12":"markdown","131f11e3":"markdown","0b0129ca":"markdown","5fe8a364":"markdown","6cbca440":"markdown","59b21573":"markdown","347b7b96":"markdown","e6545f6b":"markdown","b331eaa2":"markdown"},"source":{"197b2158":"!pip install transformers","93d25b80":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom collections import defaultdict\nfrom textwrap import wrap\n\n# Torch ML libraries\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Misc.\nimport warnings\nwarnings.filterwarnings('ignore')","e0a3ff71":"# Set intial variables and constants\n%config InlineBackend.figure_format='retina'\n\n# Graph Designs\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\n\n# Random seed for reproducibilty\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\n\n# Set GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","91fb8496":"df = pd.read_csv('..\/input\/google-play-store-reviews\/reviews.csv')\ndf.shape","30242ad3":"# Let's have a look at the data \ndf.head()","3fc82880":"# Let's check for missing values \ndf.isnull().sum()","cd64a253":"# Let's have a look at the class balance.\nsns.countplot(df.score)\nplt.xlabel('review score');","3f5745f9":"# Function to convert score to sentiment\ndef to_sentiment(rating):\n    \n    rating = int(rating)\n    \n    # Convert to class\n    if rating <= 2:\n        return 0\n    elif rating == 3:\n        return 1\n    else:\n        return 2\n\n# Apply to the dataset \ndf['sentiment'] = df.score.apply(to_sentiment)","92be7c68":"# Plot the distribution\nclass_names = ['negative', 'neutral', 'positive']\nax = sns.countplot(df.sentiment)\nplt.xlabel('review sentiment')\nax.set_xticklabels(class_names)","674fbf8a":"# Set the model name\nMODEL_NAME = 'bert-base-cased'\n\n# Build a BERT based tokenizer\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)","ba8b7ca1":"# Some of the common BERT tokens\nprint(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\nprint(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we\u2019re doing classification\nprint(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\nprint(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set ","b7e96ff3":"# Store length of each review \ntoken_lens = []\n\n# Iterate through the content slide\nfor txt in df.content:\n    tokens = tokenizer.encode(txt, max_length=512)\n    token_lens.append(len(tokens))","8be158ae":"# plot the distribution of review lengths \nsns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count')","0ad42a27":"MAX_LEN = 160","03be18c3":"class GPReviewDataset(Dataset):\n    # Constructor Function \n    def __init__(self, reviews, targets, tokenizer, max_len):\n        self.reviews = reviews\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    # Length magic method\n    def __len__(self):\n        return len(self.reviews)\n    \n    # get item magic method\n    def __getitem__(self, item):\n        review = str(self.reviews[item])\n        target = self.targets[item]\n        \n        # Encoded format to be returned \n        encoding = self.tokenizer.encode_plus(\n            review,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        return {\n            'review_text': review,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'targets': torch.tensor(target, dtype=torch.long)\n        }","82b84b86":"df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\ndf_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n\nprint(df_train.shape, df_val.shape, df_test.shape)","dfe35db3":"def create_data_loader(df, tokenizer, max_len, batch_size):\n    ds = GPReviewDataset(\n        reviews=df.content.to_numpy(),\n        targets=df.sentiment.to_numpy(),\n        tokenizer=tokenizer,\n        max_len=max_len\n    )\n    \n    return DataLoader(\n        ds,\n        batch_size=batch_size,\n        num_workers=0\n    )","3ca562f8":"# Create train, test and val data loaders\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)","2a23b9c2":"# Examples \ndata = next(iter(train_data_loader))\nprint(data.keys())\n\nprint(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","7618db82":"# Load the basic BERT model \nbert_model = BertModel.from_pretrained(MODEL_NAME)","cba0f6a4":"# Build the Sentiment Classifier class \nclass SentimentClassifier(nn.Module):\n    \n    # Constructor class \n    def __init__(self, n_classes):\n        super(SentimentClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(MODEL_NAME)\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    \n    # Forward propagaion class\n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert(\n          input_ids=input_ids,\n          attention_mask=attention_mask\n        )\n        #  Add a dropout layer \n        output = self.drop(pooled_output)\n        return self.out(output)","bc8d6b2b":"# Instantiate the model and move to classifier\nmodel = SentimentClassifier(len(class_names))\nmodel = model.to(device)","2ea322a8":"# Number of hidden units\nprint(bert_model.config.hidden_size)","928b5c00":"# Number of iterations \nEPOCHS = 10\n\n# Optimizer Adam \noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\n# Set the loss function \nloss_fn = nn.CrossEntropyLoss().to(device)","d795d387":"# Function for a single training iteration\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        correct_predictions += torch.sum(preds == targets)\n        losses.append(loss.item())\n        \n        # Backward prop\n        loss.backward()\n        \n        # Gradient Descent\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","5fbf7460":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            \n            # Get model ouptuts\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            \n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n            \n            correct_predictions += torch.sum(preds == targets)\n            losses.append(loss.item())\n            \n    return correct_predictions.double() \/ n_examples, np.mean(losses)","1085040f":"%%time\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    \n    # Show details \n    print(f\"Epoch {epoch + 1}\/{EPOCHS}\")\n    print(\"-\" * 10)\n    \n    train_acc, train_loss = train_epoch(\n        model,\n        train_data_loader,\n        loss_fn,\n        optimizer,\n        device,\n        scheduler,\n        len(df_train)\n    )\n    \n    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n    \n    # Get model performance (accuracy and loss)\n    val_acc, val_loss = eval_model(\n        model,\n        val_data_loader,\n        loss_fn,\n        device,\n        len(df_val)\n    )\n    \n    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n    print()\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    # If we beat prev performance\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","a70aab1a":"# Plot training and validation accuracy\nplt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\n\n# Graph chars\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","c38e8623":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\n\ntest_acc.item()","25f9915e":"def get_predictions(model, data_loader):\n    model = model.eval()\n\n    review_texts = []\n    predictions = []\n    prediction_probs = []\n    real_values = []\n\n    with torch.no_grad():\n        for d in data_loader:\n            texts = d[\"review_text\"]\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n\n            # Get outouts\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n\n            review_texts.extend(texts)\n            predictions.extend(preds)\n            prediction_probs.extend(outputs)\n            real_values.extend(targets)\n\n    predictions = torch.stack(predictions).cpu()\n    prediction_probs = torch.stack(prediction_probs).cpu()\n    real_values = torch.stack(real_values).cpu()\n\n    return review_texts, predictions, prediction_probs, real_values","b75edf4b":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n    model,\n    test_data_loader\n)","360bb8b6":"print(classification_report(y_test, y_pred, target_names=class_names))","0727ac48":"def show_confusion_matrix(confusion_matrix):\n    hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n    plt.ylabel('True sentiment')\n    plt.xlabel('Predicted sentiment');\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","df426046":"review_text = \"I love completing my todos! Best app ever!!!\"","57d43582":"encoded_review = tokenizer.encode_plus(\n    review_text,\n    max_length=MAX_LEN,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    return_attention_mask=True,\n    return_tensors='pt',\n)","5dbf85a3":"input_ids = encoded_review['input_ids'].to(device)\nattention_mask = encoded_review['attention_mask'].to(device)\n\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\nprint(f'Review text: {review_text}')\nprint(f'Sentiment  : {class_names[prediction]}')","a9dd56df":"### Import Libraries and Set the intial variables","87884075":"There are missing values in some of the columns but Content and score don't have a missing value. We can also look at the class balance. \n\nWe will be alloting three classes:- \n1. Positive (Score: 4-5)\n2. Neutral (Score: 3)\n3. Negative (Score: 1-2)","80a77da6":"### Model Evaluation","ef18f093":"We can see that the most relevant column for us is content and replyContent and the score as well.","72ef16ec":"Write a function to evaluate model performance","ba4b522f":"We can see that we have more positive classes than negative and low number of neutral class. I have kept neutral less to focus more on positive and negative classes. Let's allot classes based on scores now. \n\n* 0 - negative\n* 1 - neutral \n* 2 - positive","44801933":"### Load the data","35cfee14":"Define a helper function to get predictions from our models. This is similar to the evaluation function, except that we\u2019re storing the text of the reviews and the predicted probabilities","03ad01af":"## Predicting on raw text","ffddbb0f":"This confirms that our model is having difficulty classifying neutral reviews. It mistakes those for negative and positive at a roughly equal frequency.\n\nThat\u2019s a good overview of the performance of our model.","2b247b4c":"## Sentiment Classification with BERT and Hugging Face\n\nWe\u2019ll use the basic BertModel and build our sentiment classifier on top of it. Let\u2019s load the model","28e61130":"BERT works with fixed-length sequences. We\u2019ll use a simple strategy to choose the max length. Let\u2019s store the token length of each review.","af30d02d":"# Sentiment Analysis using BERT\n\nBERT (Bidirectionnal Encoder Representations for Transformers) is a \u201cnew method of pre-training language representations\u201d developed by Google and released in late 2018.","92848def":"### Training Phase\n\nwe\u2019ll use the AdamW optimizer provided by Hugging Face. It corrects weight decay. We\u2019ll also use a linear scheduler with no warmup","4704ce12":"### Preparing Torch Dataset\n\nTo enter data into a PyTorch, we need a more robust data generator class. We will return the review text as well to validate our predictions easily. ","131f11e3":"Create a dataloader to release data in batches.","0b0129ca":"#### Model Characterstics","5fe8a364":"Write the training Loop and store the best training state.","6cbca440":"We use a dropout layer for some regularization and a fully-connected layer for our output. We are returning the raw output of the last layer since that is required for the cross-entropy loss function in PyTorch to work. Create an instance and move it to the GPU","59b21573":"The above took a lot of time but it's finally working. Now, we can plot the training and validation accuracy.","347b7b96":"Most of the reviews seem to contain less than 120 tokens, but we\u2019ll be on the safe side and choose a maximum length of 160. ","e6545f6b":"Create a 80% train data and 10% test and 10% validation data","b331eaa2":"## Data Preprocessing\n\nMachine Learning models don\u2019t work with raw text. You need to convert text to numerical representation. BERT requires even more attention when it comes to this representation. \n\nHere are the requirements:\n\n* Add special tokens to separate sentences and do classification\n* Pass sequences of constant length (introduce padding)\n* Create array of 0s (pad token) and 1s (real token) called attention mask\n\nBERT offers a few model architectures and I will be using one of them combined with manual preprocessing. I am using the cased version which considers GREAT and great to be to different entities and BAD might be given more focus than bad.  \n\nThe tokenizer will break the sentence into words and give numerical values to each word. "}}