{"cell_type":{"92c5d476":"code","edb5015f":"code","346d3943":"code","eead17ef":"code","0f138ff8":"code","2b7d9e0e":"code","87ae2bcd":"code","fda969a7":"code","985b4bfa":"code","239e8fcb":"code","b7ab90f0":"code","c91ff630":"code","fc181299":"code","1d15e515":"code","bc10ae2f":"code","eee8f58b":"code","8b580afd":"code","cb5a678a":"code","cb3ce667":"code","dad55c64":"code","b2eb37ef":"code","e8fbb59a":"code","cb54d0d0":"code","0bce2cc4":"code","9a748dd9":"code","e416cf57":"code","43fbdb97":"code","445d9864":"code","67d060d4":"code","a8715690":"code","5de1a096":"code","c9a87c43":"code","52afba02":"code","00a65edf":"code","77e1ed00":"code","07c8876e":"code","cf58b10c":"code","0d950ace":"code","f048f1f4":"code","dd1ff103":"code","4b8d9f4d":"code","16b5632d":"code","227952f0":"code","4938d02b":"code","25cf1cd0":"code","f9b6745e":"code","38a7e3db":"code","66b0f768":"code","2223d4a6":"markdown","b2cad71b":"markdown","b041b37a":"markdown","5b82d4f1":"markdown","a2ab545a":"markdown","f2c08015":"markdown","a6518646":"markdown","a115fa04":"markdown","4c38049c":"markdown","0162f9af":"markdown","3dfad40f":"markdown","a4498fac":"markdown","b09cb6aa":"markdown","f06f31a4":"markdown","f4c5381e":"markdown","048b9b11":"markdown","555bb9ef":"markdown","ce2ab815":"markdown","0268b47c":"markdown","8420f632":"markdown","597f01e5":"markdown","20f2ba98":"markdown","de45b8cb":"markdown","50f095d9":"markdown","726b6587":"markdown","657daef1":"markdown","e22b0899":"markdown","832608b9":"markdown","4c9da056":"markdown","751ddfda":"markdown","c9ef201b":"markdown","8c2fe4b6":"markdown","9db947f2":"markdown","b6f6fe15":"markdown","ece799d9":"markdown","170074c7":"markdown","a021c136":"markdown"},"source":{"92c5d476":"# Importing Packages\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom random import sample\nfrom scipy.stats import skew\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBRegressor\nfrom scipy.special import boxcox1p\nfrom datetime import datetime as dt\npd.set_option('display.max_rows', 3000)\npd.set_option('display.max_columns', 1500)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor","edb5015f":"# Ignoring Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","346d3943":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntrain.head(10)","eead17ef":"test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\ndf = pd.concat((train, test)).reset_index(drop = True)","0f138ff8":"df.columns = df.columns.str.lower()\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()\ncols = list(df.columns)","2b7d9e0e":"print('List of columns with datatypes \"int\" and \"float\" in original data : \\n\\n'\n      f'{np.array(df.select_dtypes(include = [\"int\", \"float\"]).columns)} \\n\\n\\n'\n      'List of columns with datatypes \"object\" in original data : \\n\\n'\n      f'{np.array(df.select_dtypes(include = [\"object\"]).columns)} \\n\\n\\n'\n      'List of columns having missing values in original data : \\n\\n'\n      f'{np.array([col for col in cols if df[col].isnull().sum() > 0])}')","87ae2bcd":"df.info()","fda969a7":"Count_ = df['utilities'].value_counts().sort_index()\nfig, ax = plt.subplots(figsize = [20, 10])\nfig.subplots_adjust(top = 0.93)\nfig.suptitle('Counts of Each type of Utilities in the Dataset', size = 25, fontweight = 'bold')\nsns.barplot(Count_.index, Count_.values, ax = ax)\nfor index_, value_ in enumerate(Count_):\n    ax.text(index_, value_ + 12, str(value_), color = 'black', fontweight = 'bold', size = 15)\nax.set_xlabel('Utilities', size = 20)\nax.set_ylabel('Frequency', size = 20)\nplt.show()","985b4bfa":"non_existence_cols = ['alley', 'bsmtqual', 'bsmtcond', 'bsmtexposure', 'bsmtfintype1', 'bsmtfintype2', 'fence', 'fireplacequ', \n                      'garagetype', 'garagefinish', 'garagequal', 'garagecond', 'masvnrtype', 'miscfeature', 'poolqc']\nzero_cols = ['bsmtfinsf1', 'bsmtfinsf2', 'bsmtunfsf', 'totalbsmtsf', 'bsmtfullbath', 'bsmthalfbath', 'garagecars', \n             'garagearea', 'masvnrarea']\nmode_cols = ['mszoning', 'exterior1st', 'exterior2nd', 'electrical', 'kitchenqual', 'functional', 'saletype']\ndrop_cols = ['garageyrblt', 'utilities']","239e8fcb":"for col in zero_cols: df[col].fillna(0, axis = 0, inplace = True)\nfor col in mode_cols: df[col].fillna(df[col].mode()[0], axis = 0, inplace = True)\ndf.drop(drop_cols, axis = 1, inplace = True)\ndf['lotfrontage'] = df.groupby('neighborhood')['lotfrontage'].transform(lambda x: x.fillna(x.median()))","b7ab90f0":"encoding_dict = {\n    'lotshape'       :   {'Reg' : 3, 'IR1' : 2, 'IR2' : 1, 'IR3' : 0}, \n    'landslope'      :   {'Gtl' : 0, 'Mod' : 1, 'Sev' : 2}, \n    'exterqual'      :   {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'Po' : 0}, \n    'extercond'      :   {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'Po' : 0}, \n    'bsmtqual'       :   {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, np.nan : 0}, \n    'bsmtcond'       :   {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, np.nan : 0}, \n    'bsmtexposure'   :   {'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, np.nan : 0}, \n    'bsmtfintype1'   :   {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, np.nan : 0}, \n    'bsmtfintype2'   :   {'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, np.nan : 0}, \n    'heatingqc'      :   {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'Po' : 0}, \n    'centralair'     :   {'Y' : 1, 'N' : 0}, \n    'kitchenqual'    :   {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, 'Po' : 0}, \n    'fireplacequ'    :   {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, np.nan : 0}, \n    'garagequal'     :   {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, np.nan : 0}, \n    'garagecond'     :   {'Ex' : 5, 'Gd' : 4, 'TA' : 3, 'Fa' : 2, 'Po' : 1, np.nan : 0}, \n    'paveddrive'     :   {'Y' : 2, 'P' : 1, 'N' : 0}, \n    'poolqc'         :   {'Ex' : 4, 'Gd' : 3, 'TA' : 2, 'Fa' : 1, np.nan : 0}, \n    'fence'          :   {'GdPrv' : 4, 'MnPrv' : 3, 'GdWo' : 2, 'MnWw' : 1, np.nan : 0}, \n    'garagefinish'   :   {'Fin' : 3, 'RFn' : 2, 'Unf' : 1, np.nan : 0}, \n}","c91ff630":"ordinal_cols = list(encoding_dict.keys())\nnominal_cols = ['mssubclass', 'mszoning', 'street', 'alley', 'landcontour', 'lotconfig', 'neighborhood', 'condition1', 'condition2', \n                'bldgtype', 'housestyle', 'roofstyle', 'roofmatl', 'exterior1st', 'exterior2nd', 'masvnrtype', 'foundation', \n                'heating', 'electrical', 'functional', 'garagetype', 'miscfeature', 'saletype', 'salecondition']\nyear_cols = ['yearbuilt', 'yearremodadd']\nother_cols = ['id', 'saleprice']\nactual_numerical_cols = list(set(cols) - (set(nominal_cols) | set(ordinal_cols) | set(drop_cols) | set(year_cols) | set(other_cols)))","fc181299":"for col in actual_numerical_cols:\n    sns.scatterplot(x = 'saleprice', y = col, data = df)\n    plt.show()","1d15e515":"sns.boxplot(y = df['saleprice'])\nplt.title('Box Plot of \"saleprice\"', size = 15, fontweight = 'bold')\nplt.show()","bc10ae2f":"outlier_indices = df[((df['1stflrsf'] > 4000)     &  (df['saleprice'] < 300000))    | \n                     ((df['lotfrontage'] > 300)   &  (df['saleprice'] < 300000))    | \n                     ((df['totalbsmtsf'] > 5000)  &  (df['saleprice'] < 300000))    | \n                     ((df['grlivarea'] > 4000)    &  (df['saleprice'] < 300000))    | \n                     ((df['bsmtfinsf1'] > 5000)   &  (df['saleprice'] < 300000))    | \n                     (df['saleprice'] > 500000)].index\ndf.drop(outlier_indices, axis = 0, inplace = True)","eee8f58b":"plt.figure(figsize = [20, 8])\nsns.distplot(df['saleprice'])\nplt.xlabel(f'Sale Price, Skewness : {skew(df[\"saleprice\"].dropna()):.2f}', size = 20)\nplt.title('Distribution of Sale Price', size = 25, fontweight = 'bold')\nplt.show()","8b580afd":"print(f'No. of Values more than 0 in \"saleprice\" column is {(df[\"saleprice\"] > 0).sum()} where the No. of values in the column is {df[\"saleprice\"].count()}.')","cb5a678a":"df['saleprice'] = np.log(df['saleprice'])\nplt.figure(figsize = [20, 8])\nsns.distplot(df['saleprice'])\nplt.xlabel(f'Sale Price, Skewness : {skew(df[\"saleprice\"].dropna()):.2f}', size = 20)\nplt.title('Distribution of Sale Price', size = 25, fontweight = 'bold')\nplt.show()","cb3ce667":"print(df[actual_numerical_cols].apply(lambda x: skew(x.dropna())))","dad55c64":"skewed_cols = df[actual_numerical_cols].apply(lambda x: skew(x.dropna())) # Compute Skewness\nskewed_cols = skewed_cols[abs(skewed_cols) > 0.75].sort_values(ascending = False).index","b2eb37ef":"print('Skewness of the following columns are : \\n')\nfor col in skewed_cols:\n    print(f'{col} \\t: {skew(df[col].dropna()):.2f}')","e8fbb59a":"print(f'Number of Strictly Positive Values in each Column : \\n {(df[skewed_cols] > 0).sum()}')","cb54d0d0":"for col in skewed_cols:\n    df[col] = np.array(boxcox1p(df[col], 0.15)).reshape(-1,1)\n    print(f'{col} \\t: {skew(df[col].dropna()):.2f}')","0bce2cc4":"df.replace(encoding_dict, inplace = True)","9a748dd9":"for col in nominal_cols:\n    df = pd.get_dummies(df, columns = [col], prefix = [col], prefix_sep = '_', dummy_na = False)","e416cf57":"df_train, df_test = df[df['id'].isin(train['id'])].reset_index(drop = True), df[df['id'].isin(test['id'])].reset_index(drop = True)","43fbdb97":"print(f'shape of whole data   :  {df.shape} \\n'\n      f'shape of train data   :  {df_train.shape} \\n'\n      f'shape of test data    :  {df_test.shape}')","445d9864":"x_train, y_train = df_train.drop(['id', 'saleprice'], axis = 1).reset_index(drop = True), df_train['saleprice'].reset_index(drop = True)\nx_test, y_test = df_test.drop(['id', 'saleprice'], axis = 1).reset_index(drop = True), df_test['saleprice'].reset_index(drop = True)\n\nprint('Shape of : \\n'\n      f'\\t   x_train : {x_train.shape} \\t&\\t y_train : {y_train.shape}\\n'\n      f'\\t   x_test  : {x_test.shape} \\t&\\t y_test  : {y_test.shape}\\n')","67d060d4":"validate_indices = sample(list(df_train.index), int(df_train.shape[0]*0.2))\nvalidate__id = df_train.iloc[validate_indices,:]['id'].reset_index(drop = True)\ntrain__id = df_train.drop(validate_indices, axis = 0)['id'].reset_index(drop = True)","a8715690":"x__train, x__validate = x_train.drop(validate_indices, axis = 0).reset_index(drop = True), x_train.iloc[validate_indices,].reset_index(drop = True)\ny__train, y__validate = y_train.drop(validate_indices, axis = 0).reset_index(drop = True), y_train.iloc[validate_indices,].reset_index(drop = True)\n\nprint('Shape of : \\n'\n      f'\\t   x_train : {x__train.shape} \\t&\\t y_train : {y__train.shape}\\n'\n      f'\\t   x_test  : {x__validate.shape} \\t&\\t y_test  : {y__validate.shape}\\n')","5de1a096":"y__validate = pd.Series(np.exp(y__validate), name = 'saleprice')","c9a87c43":"def Calc_Cross_Validation_Score(model, n_split, Predictor, Response):\n    \"\"\"\n    This function will help to calculate Cross Validation Score for a particular model.\n    \n    Parameters\n    ----------\n    model : model object <Required>\n        The model for which Cross Validation Score is to be calculated.\n    \n    n_split : int <Required>\n        The number of splitting of the data.\n        \n    Predictor : pandas.core.frame.DataFrame <Required>\n        A pandas dataframe containing all the predictor variables.\n        \n    Response : pandas.core.series.Series <Required>\n        A pandas series object containing the response variable.\n    \"\"\"\n    X, Y = Predictor, Response\n    kf = KFold(n_splits = n_split, shuffle = True).get_n_splits(pd.concat((X, Y), axis = 1).values)\n    cross_val_score_ = -cross_val_score(model, X, Y, scoring = \"neg_root_mean_squared_error\", cv = kf)\n    return(cross_val_score_)","52afba02":"def Calc_MAPE(Observed, Predicted):\n    \"\"\"\n    This function will help to calculate the Mean Absolute Percentage Error (MAPE) for the prediction of a model.\n    \n    Parameters\n    ----------\n    Observed : pandas.core.series.Series <Required>\n        A pandas series object containing the observed value of the response variable.\n        \n    Predicted : pandas.core.series.Series <Required>\n        A pandas series object containing the predicted value of the response variable.\n    \"\"\"\n    Observed, Predicted = np.array(Observed), np.array(Predicted)\n    return(np.mean(np.abs((Observed - Predicted) \/ Observed)))","00a65edf":"def Model_Fit(X_Train, X_Test, Y_Train, Y_Test, Model, Prediction_Plot = True, Print_Metrices = True):\n    \"\"\"\n    This function will help to fit a given Classification Model with given Predictor and Response variables .\n    \n    Parameters\n    ----------\n    X_Train : pandas.core.frame.DataFrame <Required>\n        A pandas dataframe containing all the predictor variables of train set.\n        \n    X_Test : pandas.core.frame.DataFrame <Required>\n        A pandas dataframe containing all the predictor variables of train set.\n        \n    Y_Train : pandas.core.series.Series <Required>\n        A pandas series object containing the response variable of test set.\n        \n    Y_Test : pandas.core.series.Series <Required>\n        A pandas series object containing the response variable of test set.\n        \n    Model : str <Required>\n        The name of the model (without parentheses) in string format.\n        \n    Prediction_Plot : bool, default = True\n        Whether to show the \"Predicted vs Actual\" and the \"Predicted vs Redial\" plot or not.\n        \n    Print_Metrices : bool, default = True\n        Whether to print the different evaluation metrics for the prediction of the model or not.\n    \"\"\"\n    \n    Models = {\n        'LinearRegression' : LinearRegression(), 'Ridge' : Ridge(), 'Lasso' : Lasso(), \n        'XGBRegressor' : XGBRegressor(), 'ElasticNet' : ElasticNet(), \n        'RandomForestRegressor' : RandomForestRegressor(), 'GradientBoostingRegressor' : GradientBoostingRegressor()\n    }\n    \n    if Model in Models.keys():\n        kf = KFold(n_splits = 10, shuffle = True).get_n_splits(pd.concat((X_Train, Y_Train), axis = 1).values)\n        model_ = Models[Model]\n        model_.fit(X_Train, Y_Train)\n        Y_Pred = pd.Series(np.exp(model_.predict(X_Test)), name = 'saleprice')\n        Residual = Y_Test - Y_Pred\n        MSE = mse(Y_Test, Y_Pred)\n        RMSE = np.sqrt(MSE)\n        MAPE = Calc_MAPE(Y_Test, Y_Pred)\n        Cross_Val_Score = Calc_Cross_Validation_Score(model_, 10, pd.concat((X_Train, X_Test), axis = 0), pd.concat((Y_Train, Y_Test), axis = 0))\n        CV = Cross_Val_Score.std() \/ Cross_Val_Score.mean()\n        \n        if Prediction_Plot:\n            fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = [20, 5])\n            fig.subplots_adjust(wspace = .25, top = 0.8)\n            axs = axs.ravel()\n            fig.suptitle(f'For the Predictions using {Model} Regression Model', size = 25, fontweight = 'bold')\n            axs[0].scatter(Y_Pred, Y_Test)\n            axs[0].set_xlabel('Predicted', size = 15)\n            axs[0].set_ylabel('Actual', size = 15)\n            axs[0].set_title('Predicted vs Actual', size = 20)\n            axs[1].scatter(Y_Pred, Residual)\n            axs[1].set_xlabel('Predicted', size = 15)\n            axs[1].set_ylabel('Residual', size = 15)\n            axs[1].set_title('Predicted vs Residual', size = 20)\n            plt.show()\n        \n        if Print_Metrices:\n            print(\n                'For Ridge Regression Model : \\n\\n'\n                f'\\t Mean Square Error (MSE)                   :     {MSE}\\n'\n                f'\\t Root Mean Square Error (RMSE)             :     {RMSE}\\n'\n                f'\\t Mean Absolute Percentage Error (MAPE)     :     {MAPE}\\n'\n                f'\\t Mean of RMSE                              :     {Cross_Val_Score.mean()}\\n'\n                f'\\t Standard Deviation of RMSE                :     {Cross_Val_Score.std()}\\n'\n                f'\\t Coefficient of Variation  is              :     {CV}\\n')\n            \n        Output = {'Model Name'                       :   Model, \n                  'Fitted Model'                     :   model_, \n                  'Predicted Values'                 :   Y_Pred, \n                  'Mean Square Error'                :   MSE, \n                  'Root Mean Square Error'           :   RMSE, \n                  'Mean Absolute Percentage Error'   :   MAPE, \n                  'Cross Validation Score'           :   Cross_Val_Score, \n                 }\n        return(Output)\n    else:\n        print(f'Please Enter a Valid Model Name. Available Models are : \\n\\t {list(Models.keys())}')        ","77e1ed00":"Models = {'Ridge'                       :   Ridge(), \n          'Lasso'                       :   Lasso(), \n          'XGBRegressor'                :   XGBRegressor(), \n          'ElasticNet'                  :   ElasticNet(), \n          'RandomForestRegressor'       :   RandomForestRegressor(), \n          'GradientBoostingRegressor'   :   GradientBoostingRegressor()\n         }","07c8876e":"Predictions, MSE, RMSE, MAPE, Cross_Val_Score = pd.DataFrame(), pd.Series(), pd.Series(), pd.Series(), pd.Series()\nFitted_Models = dict()\nfor Model in Models:\n    print(f'Model : {Model} \\n\\n')\n    Model_Info = Model_Fit(X_Train = x__train, X_Test = x__validate, Y_Train = y__train, Y_Test = y__validate, \n                          Model = Model, Prediction_Plot = True, Print_Metrices = False)\n    Fitted_Models[Model] = Model_Info['Fitted Model']\n    Predictions[Model] = Model_Info['Predicted Values']\n    MSE[Model] = Model_Info['Mean Square Error']\n    RMSE[Model] = Model_Info['Root Mean Square Error']\n    MAPE[Model] = Model_Info['Mean Absolute Percentage Error']\n    Cross_Val_Score[Model] = Model_Info['Cross Validation Score']","cf58b10c":"Predictions.head()","0d950ace":"Metrics_ = pd.concat([MSE, RMSE, MAPE, Cross_Val_Score.apply(lambda x: x.mean())], axis = 1)\nMetrics_ = pd.DataFrame(Metrics_.values, index = Metrics_.index , columns = ['MSE','RMSE','MAPE','Mean Cross-Validation Score'])\nMetrics_.sort_values(ascending = True, by = 'MAPE')","f048f1f4":"Top_3_Models = MAPE.sort_values(ascending = True)[:3].index","dd1ff103":"coef_ = np.arange(0, 1.001, 0.01)\nrmse__ = np.inf\ncoef_1_, coef_2_, coef_3_ = 0, 0, 0\nfor coef_1 in coef_:\n    for coef_2 in coef_:\n        for coef_3 in coef_:\n            if coef_1 + coef_2 + coef_3 == 1:\n                Final_Pred = (Predictions[Top_3_Models[0]] * coef_1) + (Predictions[Top_3_Models[1]] * coef_2) + (Predictions[Top_3_Models[2]] * coef_3)\n                _rmse_ = np.sqrt(mse(y__validate, Final_Pred))\n                if _rmse_ < rmse__ : coef_1_, coef_2_, coef_3_, rmse__ = coef_1, coef_2, coef_3, _rmse_\n            else:\n                continue","4b8d9f4d":"print('To make an Ensemble Model, the Weights of different models are as follows: \\n\\n'\n      f'\\t Weight of {Top_3_Models[0]} Regression Model is  :   {coef_1_} \\n'\n      f'\\t Weight of {Top_3_Models[1]} Regression Model is  :   {coef_2_} \\n'\n      f'\\t Weight of {Top_3_Models[2]} Regression Model is  :   {coef_3_} \\n'\n     )","16b5632d":"Final_Prediction = (Predictions[Top_3_Models[0]] * coef_1_) + (Predictions[Top_3_Models[1]] * coef_2_) + (Predictions[Top_3_Models[2]] * coef_3_)\nFinal_Prediction = pd.Series(Final_Prediction, name = 'saleprice')","227952f0":"print('The Metrices for the Final Predictions are as follows : \\n\\n'\n      f'\\t MSE  : {mse(y__validate, Final_Prediction)}\\n'\n      f'\\t RMSE : {np.sqrt(mse(y__validate, Final_Prediction))}\\n'\n      f'\\t MAPE : {np.mean(np.abs((y__validate - Final_Prediction) \/ y__validate))}\\n'\n     )","4938d02b":"fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize = [20, 5])\nfig.subplots_adjust(wspace = .25, top = 0.8)\naxs = axs.ravel()\nfig.suptitle('For the Final Predictions using Three Regression Models', size = 25, fontweight = 'bold')\naxs[0].scatter(Final_Prediction, y__validate)\naxs[0].set_xlabel('Predicted', size = 15)\naxs[0].set_ylabel('Actual', size = 15)\naxs[0].set_title('Predicted vs Actual', size = 20)\naxs[1].scatter(Final_Prediction, (y__validate - Final_Prediction))\naxs[1].set_xlabel('Predicted', size = 15)\naxs[1].set_ylabel('Residual', size = 15)\naxs[1].set_title('Predicted vs Residual', size = 20)\nplt.show()","25cf1cd0":"Model_1 = Fitted_Models[Top_3_Models[0]]\nPrediction_1 = pd.Series(np.exp(Model_1.predict(x_test)), name = 'saleprice')\nModel_2 = Fitted_Models[Top_3_Models[1]]\nPrediction_2 = pd.Series(np.exp(Model_2.predict(x_test)), name = 'saleprice')\nModel_3 = Fitted_Models[Top_3_Models[2]]\nPrediction_3 = pd.Series(np.exp(Model_3.predict(x_test)), name = 'saleprice')","f9b6745e":"Ensembled_Prediction = (coef_1_ * Prediction_1) + (coef_2_ * Prediction_2) + (coef_3_ * Prediction_3)\nSubmission = pd.concat([test['id'], pd.Series(Ensembled_Prediction)], axis = 1)\nSubmission = pd.DataFrame(Submission.values, columns = ['Id', 'SalePrice'])","38a7e3db":"Submission.head()","66b0f768":"Submission.to_csv('Submission.csv', index = False)","2223d4a6":"First 10 Records","b2cad71b":"Now, let's find out the portion where ambiguous values are present with the help of above Scatter Plots and Box Plot and get the indices of those values which will be helpful to remove those values as outliers.","b041b37a":"Now, let's encode the __Ordinal Columns__. For this purpose the following _'encoding_dict'_ is being created which will be helpful to replace the _string values_ in the __Ordinal Columns__ by _numeric values_.","5b82d4f1":"Now, let's take a look at the Skewness of other _Numerical Columns_.\n> From the following table, it can be observed that there are __Many Highly Skewed__ variable.","a2ab545a":"Now, let's take a look at the Scatter Plots of _'saleprice'_ and all the other _Numerical Columns_ to get a clear idea of __Presence of Outliers__.","f2c08015":"## <center>Predictive Modelling<\/center>","a6518646":"## <center>Prediction for Test Data<\/center>","a115fa04":"Let's treat the __Missing Values__ according to the methodologies mentioned above.","4c38049c":"Let's take a look at the __Predicted Values__ from different _Regression Models_.","0162f9af":"Now, let's form a loop which will perform model fitting on different __Regression Models__.","3dfad40f":"Now, let's take a look at the Distribution of the __Response Variable__ , _'saleprice'_.\n> So, from the following Distribution Plot, it can be seen that the values of _'saleprice'_ are much larger, the range of the values is $(34900, 485000)$ and also the Skewness is much higher, more than $1$.","a4498fac":"Now, let's segregate the whole dataset into _Train_ and _Test_ set again with the help of _'id'_ column, and then drop this particular column and separate the _'saleprice'_ column from the Predictor Variables.","b09cb6aa":"## <center>Context<\/center>\nIf a home buyer is asked to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east - west railroad. But this dataset proves that much more influences price negotiations than the number of bedrooms or a white - picket fence. With $79$ explanatory variables describing almost every aspect of residential homes in Ames, Iowa.","f06f31a4":"## <center>Model Ensembling<\/center>","f4c5381e":"So, many variable in _'skewed_cols'_ has zero values, which implies __Box - Cox Transformation__ can not be applied directly in this Dataset to convert the variables into Normal Distribution as this particular transformation requires all positive values. So, instead of Box-Cox Transformation, __*boxcox1p* Transformation__ is being applied here to perform the mentioned task.\n> From the following table, it can be seen that, Skewness of many variables in _'skewed_cols'_ have been reduced significantly which will help in the prediction of house prices.","048b9b11":"Now, let's form the __Final Prediction__ of the _Ensemble Model_ as a __Linear Combination__ of the predictions of the _Top 3 Models_.","555bb9ef":"Basic Informations","ce2ab815":"Now, let's check the counts of each type of Utilities in the Dataset.\n> From the following Bar Diagram, it can be noticed that, _'AllPub'_ occurs in every records except only one which leads to variation nearly about $0$ and hence, not important for modelling.","0268b47c":"Now, let's make different lists of column names with respect to different methodologies to be applied on those columns in order to treat Missing Values.<br><\/br>\nNow, according to _Data Description_ :\n> - _non_existence_cols_ are those columns for which __'NA' is a category__. For this set of columns, replacement is not being done at this stage, as later in the _.get_dummies()_ method of _pandas_ class, _dummy_na = False_ parameter will automatically remove those __'NA'__ values.\n> - _zero_cols_ are those columns for which __Missing Values are to be replaced by zero ($0$)__. \n> - _mode_cols_ are those columns for which __Missing Values are to be replaced by Mode of that particular column__.\n> - _drop_cols_ are those columns which will be __Dropped__ from the dataset as _'garageyrblt'_ has a high correlation with the _'yearbuilt'_ column and in the _'utilities'_ column all the records have same value except only one.\n> - For the _'lotfrontage'_ column, __Missing Values are to be replaced by Median of that particular column for each *'neighborhood'*__.","8420f632":"### Observations from the Data\n\n- The Dataset has $1460$ number of records.\n- There are $19$ columns having missing values out of which $3$ columns having not more than $10$ missing values, $10$ columns having number of missing values between $10$ to $100$, $2$ columns having number of missing values between $100$ to $1000$ and the rest $4$ columns have more than $1000$ missing values.\n- Total number of column is $80$ in which, _SalePrice_ is the response variable and _Id_ is the identification number of each house. So there are all total $79$ predictor variable to be account for. So, modelling and prediction over this data may need some feature selection or dimensity reduction technique to be applies on the data in order to get rid of the less important predictor variables.\n- There are $3$ columns with float datatype, $35$ with integer datatype and $43$ with object datatype which means there are $38$ Numerical Variables and $43$ Categorical Variables in this data, which need to be taken care of.\n    - So, Categorical Variables can be of two types, _Nominal_ and _Ordinal_.\n    - There may be Categorical Variables which are in Numerical forms.\n    - From the Data Description, it can be seen that, __NA__ is a category for many Categorical Variables. So, missing value in Categorical Variables should be replaced by __NA__ initially.","597f01e5":"Now, let's transform back the values of Response variable in the Validation set to its Original Scale.","20f2ba98":"Now, let's encode the __Categorical Variables__. <br>\nThe __Ordinal Variables__ are being encoded with the help of _'encoding_dict'_ created earlier, and for the __Nominal Variables__ , __Dummy Variables__ are being created using the _.get_dummies()_ method of _pandas_ class with _dummy_na = False_ parameter, so that the __Missing Values__ we kept earlier, will be removed at this stage.","de45b8cb":"Also let's take a look at the Box Plot of _'saleprice'_ for the same above reason.","50f095d9":"Now, Here are different _metrics_ corresponding to the predictions of the above models.","726b6587":"Now, Finally let's look at the __Predicted vs Actual__ & __Predicted vs Residual__ for the _Final Predition_.","657daef1":"To reduce the variation in the values, let's apply __log(x) Transformation__ , as all the values of the column are more than $0$.\n> So, from the following Distribution Plot, it can be seen that the values of 'saleprice' have been reduced very much, the range which was previously $(34900, 485000)$, has now become $(10.5, 13)$ and also the Skewness is reduced to $- 0.04$, which is nearly $0$.","e22b0899":"Let's make few different lists of column names based on their types of values where,\n> - _ordinal_cols_ is the list of columns for which the values of the coulmns are __Ordinal__ in nature.\n> - _nominal_cols_ is the list of columns for which the values of the coulmns are __Nominal__ in nature.\n> - _year_cols_ is the list of columns for which the values of the coulmns are __No. of Years__.\n> - _other_cols_ is the list of columns of _'id'_ and _'saleprice'_ , _'id'_ is __Unnecessary for Modelling, but Important for Identification__ and _'saleprice'_ is the __Response Variable__.\n> - _actual_numerical_cols_ is the list of columns for which the values of the coulmns are __Actually Numerical__ in nature.","832608b9":"## <center>Data Pre - Processing<\/center>","4c9da056":"Now, let's take a random sample from the _Train_ set, and keep this aside as the __Validation__ set in order to evaluate the Predictions.","751ddfda":"Now, let's take a look at the _Metices_ for the __Final Prediction_.","c9ef201b":"Let's find out __Optimal Weights__ for the __Top 3 Models__.","8c2fe4b6":"Now, let's take a look at the Skewness of the _'skewed_cols'_.\n> So, from the following table, it can be noticed that, there are many columns having skewness more than $0.75$ in terms of absolute value.","9db947f2":"Here is the list of __Regression Models__ which will be applied on the Final Dataset and the Best Model will be chosen on the basis of __Root Mean Squared Error__.","b6f6fe15":"So, now let's take the columns, where absolute value of Skewness is more than $0.75$, _'skewed_cols'_.","ece799d9":"## <center>Data Overview<\/center>","170074c7":"Now, let's define a function which will help to fit different __Regression Models__.","a021c136":"Now, let's select the __Top 3 Models__ on the basis of __Mean Absolute Percentage Error (MAPE)__ , to create an _Ensemble Model_."}}