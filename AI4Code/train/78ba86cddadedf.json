{"cell_type":{"790d1514":"code","c183705d":"code","a565d0d2":"code","f959832b":"code","652f03d5":"code","7784b423":"code","5c475b87":"code","8f775734":"code","ffbd7fc6":"code","d338158b":"code","52b2d24c":"code","27e23e69":"code","2459bd59":"code","de40b3c2":"code","6b348e98":"code","4a24ecc3":"code","94561fb5":"code","fb2b3dba":"code","b7e1bd21":"code","6c5faa79":"code","cfe66be5":"code","cee2bb9c":"code","ef3f2ca9":"code","5c55d2fb":"code","4e9499e6":"code","582ea3f8":"code","4689a5a3":"code","f5c28f13":"code","bed974e4":"code","f2743403":"code","e874b21d":"code","8139d68f":"code","c986cd91":"code","b07346dd":"code","e8010bb4":"code","fe36d701":"code","7c5002f4":"code","19307a95":"code","0d346bd2":"code","08bfb313":"code","40dccd18":"code","6eed88e8":"code","fb89f9c5":"code","a2da0f28":"code","051107fd":"code","705580bd":"code","91d9fc3e":"code","0b806d5c":"code","e6cfe48f":"code","a089b005":"code","a4f96b62":"code","f5c7791c":"markdown","02a50860":"markdown","23194043":"markdown","311793c2":"markdown","0a3e6c29":"markdown","19170964":"markdown","dc9932be":"markdown","350ff0d0":"markdown","c8548bf9":"markdown","dff52bb6":"markdown","f681f878":"markdown","b77881ec":"markdown","2462feeb":"markdown","05f53286":"markdown","7d34dcba":"markdown","d3e8d661":"markdown","f9e0f9bb":"markdown","4a8a0369":"markdown","861841ea":"markdown","9b2e046d":"markdown"},"source":{"790d1514":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c183705d":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","a565d0d2":"train.head()","f959832b":"df1 = train[['PassengerId','Survived','Pclass','Sex','Age','SibSp','Parch','Fare']]","652f03d5":"df1.head()","7784b423":"df1.shape","5c475b87":"p = df1.isnull().sum().sum()\nprint(p)","8f775734":"df1_dl=df1.dropna(axis = 0)","ffbd7fc6":"q = df1_dl.isnull().sum().sum()\nprint(q)","d338158b":"df1_dl['Sex_value'] = df1_dl['Sex']","52b2d24c":"df1_dl","27e23e69":"df1_dl.loc[df1_dl['Sex'] == 'male','Sex_value'] = 1\ndf1_dl.loc[df1_dl['Sex'] == 'female','Sex_value'] = 0","2459bd59":"df1_dl","de40b3c2":"df1_train = df1_dl[['Survived','Pclass','Age','SibSp','Parch','Fare','Sex_value']]","6b348e98":"df1_train","4a24ecc3":"x_train = df1_train.iloc[:,1:]\ny_train = df1_train.iloc[:,0]","94561fb5":"x_train.head()","fb2b3dba":"y_train.head()","b7e1bd21":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","6c5faa79":"test","cfe66be5":"df2 = test[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Fare']]\nG_S = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","cee2bb9c":"df2","ef3f2ca9":"G_S","5c55d2fb":"df2['Survived'] = G_S['Survived']","4e9499e6":"df2","582ea3f8":"m = df2.isnull().sum().sum()\nprint(m)","4689a5a3":"df2_dl=df2.dropna(axis = 0)","f5c28f13":"n = df2_dl.isnull().sum().sum()\nprint(n)","bed974e4":"df2_dl","f2743403":"df2_dl['Sex_value'] = df2_dl['Sex']\ndf2_dl.loc[df2_dl['Sex'] == 'male','Sex_value'] = 1\ndf2_dl.loc[df2_dl['Sex'] == 'female','Sex_value'] = 0","e874b21d":"df2_dl","8139d68f":"df2_test = df2_dl[['Pclass','Age','SibSp','Parch','Fare','Sex_value','Survived']]","c986cd91":"df2_test","b07346dd":"x_test = df2_test.iloc[:,0:6]\ny_test = df2_test.iloc[:,6]","e8010bb4":"x_test","fe36d701":"y_test","7c5002f4":"print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)","19307a95":"x1 = x_train.isnull().sum().sum()\ny1 = y_train.isnull().sum().sum()\nx2 = x_test.isnull().sum().sum()\ny2 = y_test.isnull().sum().sum()\nprint(x1,y1,x2,y2)","0d346bd2":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\nclf.fit(x_train,y_train)","08bfb313":"clf.score(x_test,y_test)","40dccd18":"clf.predict(x_test[301:306])","6eed88e8":"y_test[301:306]","fb89f9c5":"from sklearn import svm\n\nclf = svm.SVC()\nclf.fit(x_train,y_train)","a2da0f28":"clf.score(x_test,y_test)","051107fd":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(x_train,y_train)","705580bd":"clf.score(x_test,y_test)","91d9fc3e":"import tensorflow as tf\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Dense(256, activation='relu', input_shape = [x_train.shape[1]]),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(128, activation='relu'),  \n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(64, activation='relu'),  \n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(1)                     \n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\n\nmodel.compile(optimizer=optimizer,\n              loss='mse',\n              metrics=['mae','mse'])","0b806d5c":"print(model.summary())","e6cfe48f":"tf.keras.utils.plot_model(\n    model,\n    to_file='model.png',\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir='TB',\n)","a089b005":"class PrintDot(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs):\n        if epoch % 100 == 0: print('')\n        print('.', end='')\n\nEPOCHS = 2000\n\nhistory = model.fit(\n    x_train, y_train,\n    epochs=EPOCHS, \n    validation_data=(x_test, y_test), \n    verbose=0,\n    callbacks=[PrintDot()],\n)","a4f96b62":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","f5c7791c":">>5.2 RandomForestClassifier","02a50860":"**4 Predict the Particular Data**","23194043":"Add the column 'Survived' of dataframe 'Survived' into dataframe 'df2' in order to delete the rows which contain missing data conveniently.","311793c2":"Try to use '1' or '0' to describe 'Sex' instead of strings.\n* 1 -> male\n* 0 -> female","0a3e6c29":"**2.1 Prepare the traing data**","19170964":"Count the number of 'NaN'","dc9932be":"**1 Gather the Data**\n\nAdd the dataset of Titanic into the programe","350ff0d0":"**2.2 Prepare the test data**\n* Almost the same as preparing the training data","c8548bf9":"**5 Other Models**\n\n* 5.1 SVM\n* 5.2 RandomForestClassifier","dff52bb6":"Split features and labels","f681f878":"Check the shape of dataframe ","b77881ec":"Make sure there is no NaN in our data frame","2462feeb":"Make sure there is no missing data.","05f53286":"Set up 'df1_train' without column 'Sex'","7d34dcba":"**2 Data Preparation**\n\n* Split our data into test training data and test data.\n* Split both training data and test data into features data and label data\n* Handle with the missing data(Handle with the problems caused by NaN)","d3e8d661":"* Set up a new column named 'Sex_value' \n* Set the 'Sex_value' of every passenger according to 'Sex'","f9e0f9bb":">>5.1 SVM","4a8a0369":"In my opinion, filling the 'NaN' with other data which are fake or calculated artificially will decrease the accuracy of our prediction model in this case. So I choose to delete the rows which contain the missing data.","861841ea":"In my opinion, 'Name' and 'Cabin' and 'Embarked' have nothing to do with the possibility of survival of the passengers. So I set up 'df1' with the rest of the features of the passengers.","9b2e046d":"**3 Train and Evaluate the Model**\n\nLet's see how LogisticRegression perform in this case "}}