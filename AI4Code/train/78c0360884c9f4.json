{"cell_type":{"d5adb909":"code","1a25b987":"code","43b2035e":"code","08e20a8f":"code","fd007719":"code","2a18fb12":"code","2952e4d7":"code","1a0907dc":"code","a74107f9":"code","ec532bd1":"code","43e72a4f":"code","977a8f5f":"code","ad8b3c6a":"code","29a098a4":"code","5b067741":"code","d2cd2207":"markdown","ac60f4dc":"markdown"},"source":{"d5adb909":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n#import warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1a25b987":"### load dataset\nx_l= np.load('..\/input\/X.npy')\nY_l= np.load('..\/input\/Y.npy')\n\nimg_size=64\nplt.subplot(1,2,1)\nplt.imshow(x_l[260].reshape(img_size,img_size)) # x_l i\u00e7inden 260. resmi g\u00f6ster\nplt.axis('off') # x ve y eksenlerni g\u00f6sterme sadece resmi g\u00f6ster\nplt.subplot(1,2,2)\nplt.imshow(x_l[900].reshape(img_size,img_size))\nplt.axis('off')","43b2035e":"#Join a sequence of arrays along a row axis\nX=np.concatenate((x_l[204:409],x_l[822:1027]),axis=0) #from 0 to 204 is zero sign and from 205 to 410 is one sign\n#64*64 l\u00fck bir resimden 410 tane\n#x_l resimlerden olu\u015fan bir dizi. Bu dizi i\u00e7inden 205 er rasimlik iki ayr\u0131 resim dizisini (0 ve 1 resimleri) al\u0131p tek dizide birle\u015ftiriyorum\n#Tek sat\u0131rl\u0131k 410 resimden olu\u015fan bir resim vekt\u00f6r\u00fc elde etmi\u015f oldum ()\n(410, 64, 64)","08e20a8f":"z=np.zeros(205) #205 lik bir 0 dizisi. Hen\u00fcz (205,1) boyutunda de\u011fil. reshape etmek gerekir.\no=np.ones(205) #205 lik bir 1 dizisi. Hen\u00fcz (205,1) boyutunda de\u011fil. reshape etmek gerekir.\nY=np.concatenate((z,o),axis=0) #z ve o dizilerini x ekseni boyunca birle\u015ftir. (410,) boyutunda bir dizi elde edilir.\nprint(Y.shape) #deneme. \nY=Y.reshape(X.shape[0],1) #0 lardan 205 tane 1' lerden 205 tane olmak \u00fczere 410 tane class ((410,1)'lik vekt\u00f6r)\nprint(\"X shape: \", X.shape)\nprint(\"Y shape: \", Y.shape)","fd007719":"#Then lets create x_train, y_train,x_test,y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y,test_size=0.15,random_state=42)\n#Modelin %15 i test gerisi train, random state ile random yapma i\u015flemini ayn\u0131 de\u011ferlere sabitledik\nnumber_of_train= X_train.shape[0] #\nnumber_of_test= X_test.shape[0]\n","2a18fb12":"X_train_flatten=X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2]) #Resimleri tek boyutlu bir dizi haline getirdik\nX_test_flatten= X_test.reshape(number_of_test,X_test.shape[1]*X_test.shape[2]) #Resimleri tek boyutlu bir dizi haline getirdik.\n#X ve y korrdinatlar\u0131n\u0131 tek vekt\u00f6r haline getirdik. 64*64 l\u00fck matris 4096 l\u0131k vekt\u00f6r oldu.\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","2952e4d7":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","1a0907dc":"#initialize parameters\n#so what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w=np.full((dimension,1),0.01)\n    b=0.0\n    return w,b","a74107f9":"# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","ec532bd1":"y_head = sigmoid(0)\ny_head","43e72a4f":"#forward propagaation steps:\n#find z= w.T*x+b\n#y_head=sigmoid(z)\n#loss(error)= loss(y,y_head)\n#cost=sum(loss)\n\n#In backward propagation we will use y_head that found in forward propagation\n#Therefore instead of writing backward propagation, lets combine both forward and backward\ndef forward_backward_propagation(w,b,x_train,y_train):\n    z= np.dot(w.T,x_train)+b\n    y_head= sigmoid(z) #probabilistic 0-1\n    loss= -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost= (np.sum(loss))\/x_train.shape[1] #x_train.shape[1] is for scaling\n    #backward propagation\n    derivative_weight= (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] #x_train.shape[1] is for scaling\n    derivative_bias= np.sum(y_head-y_train)\/x_train.shape[1] #x_train.shape[1] is for scaling\n    gradients= {\"derivative_weight\":derivative_weight, \"derivative_bias\":derivative_bias}\n        \n    return cost,gradients\n    ","977a8f5f":"#Updating parameters\ndef update(w,b,x_train,y_train,learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    #updating(learning) parameters is number_of_iteration\n    for i in range(number_of_iteration):\n        #make forward and backward propagation and find cost and gradients\n        cost,gradients= forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        #lets update\n        w=w-learning_rate*gradients[\"derivative_weight\"]\n        b=b-learning_rate*gradients[\"derivative_bias\"]\n        if i%10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n        #we update(learn) parameters weight and bias\n    parameters= {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation=\"vertical\")\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,cost_list\n    #parameters, gradients, cost_list = update(w,b,x_train, y_train, learning rate= 0.009,number_of_iteration=200)     ","ad8b3c6a":"#prediction\ndef predict(w,b,x_test):\n    #x_test is a input for forward propagation\n    z=sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction=np.zeros((1,x_test.shape[1]))\n    #if z is bigger than 0.5 our prediction is sign one (y_head=1)\n    #if z is smaller than 0.5 our prediction is sign zero(y_head=0)\n    for i in range (z.shape[1]):\n        if z[0,i]<=0.5:\n            Y_prediction[0,i]=0\n        else:\n             Y_prediction[0,i]=1\n    return Y_prediction\n#predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","29a098a4":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    #initialize\n    dimension=x_train.shape[0] #4096\n    print(dimension)\n    w,b = initialize_weights_and_bias(dimension)\n    #do not change learning rate\n    parameters, gradients,cost_list= update(w,b,x_train,y_train,learning_rate,num_iterations)\n    \n    y_prediction_test= predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train= predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n    \n    #Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_train- y_train))*100))\n    print(\"test accuracy: {} %\".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))","5b067741":"logistic_regression(x_train,y_train,x_test, y_test, learning_rate=0.01, num_iterations=150)","d2cd2207":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","ac60f4dc":"**Overview the Data Set**\n* We will use \"sign language digits data set\" for this tutorial"}}