{"cell_type":{"5aacbfdc":"code","f6a62892":"code","50d466fc":"code","91174e7c":"code","ccd86cd2":"code","87003c59":"code","7b9d72b6":"code","79513ca6":"code","f3f73e19":"code","7789039c":"code","aa237c51":"code","fdc0d7a5":"code","a55c5cf1":"code","60e42916":"code","2f9d27c8":"code","36b319a9":"code","793b1667":"code","283e381f":"code","1ace7ed8":"code","b2f51dfa":"code","976d5981":"code","108c7037":"code","b9c34564":"code","804334fd":"code","4b4c6b97":"markdown","29872dc9":"markdown","39536dfe":"markdown","d625c62d":"markdown","91387853":"markdown","d5002662":"markdown","8341d5d1":"markdown","4c938918":"markdown","7524d3f1":"markdown","0828d119":"markdown","5a1d8a16":"markdown","7e5a69d8":"markdown","1ef07ce5":"markdown","265f0c3f":"markdown","91ed03d2":"markdown","6de49e49":"markdown","5b18ab46":"markdown","334b92a8":"markdown","b904a8af":"markdown","fc516df7":"markdown","9d191538":"markdown","6a942671":"markdown","cc0a20d9":"markdown"},"source":{"5aacbfdc":"import pandas as pd\n\ndf = pd.read_csv('D:\/UTA\/Fall-2020\/DM\/TermProject\/archive\/bgg-15m-reviews.csv')\ndel df['Unnamed: 0']\n","f6a62892":"#Dropping rows with missing reviews \ntemp_dataset = df.dropna().reset_index(drop=True)\ntemp_dataset\n","50d466fc":"dataset = temp_dataset[['comment', 'rating']].copy()\ndataset.columns = ['Reviews','Rating']\ndataset","91174e7c":"import nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\n\ndef sentence_tokenize(text):\n    sentences = nltk.sent_tokenize(text)\n    return sentences\n    \n\ndef remove_html_tags(text):\n    pattern = re.compile('<.*?>')\n    new_text = re.sub(pattern, '', text)\n    return new_text\n\ndef remove_numbers(text):\n    text = re.sub('\\w*\\d\\w*', \"\",text)\n    return text\n\ndef word_tokenize(text):\n    #remove punctuations\n    tokeniser = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    tokens = (tokeniser.tokenize(text))\n    return tokens\n    \ndef lemmatization(text):\n    lemmatiser = WordNetLemmatizer()\n    lemmas = [lemmatiser.lemmatize(token, pos='v') for token in text]\n    return lemmas\n    \ndef remove_stopwords(text):\n    stopwords = nltk.corpus.stopwords.words('english')\n    new_text = [word for word in text if word not in stopwords]\n    return new_text\n\ndef list_to_string(str2):  \n    str1 = \" \"   \n    return (str1.join(str2)) \n    ","ccd86cd2":"lower_case_dataset = pd.DataFrame(dataset.Reviews.apply(lambda x: x.lower()))\nreviews_without_htmltags_df =  pd.DataFrame(lower_case_dataset.Reviews.apply(lambda x: remove_html_tags(x)))\nreviews_without_htmltags_df =  pd.DataFrame(reviews_without_htmltags_df.Reviews.apply(lambda x: remove_numbers(x)))\nreviews_without_htmltags_df","87003c59":"reviews_without_htmltags_df['Reviews_sentence_tokenized'] = reviews_without_htmltags_df['Reviews']\nreviews_without_htmltags_df['Reviews_sentence_tokenized'] = reviews_without_htmltags_df['Reviews'].apply(lambda x: sentence_tokenize(x))\n\n#word tokenizing\nreviews_without_htmltags_df['Reviews_word_tokenized'] = reviews_without_htmltags_df['Reviews']\nreviews_without_htmltags_df['Reviews_word_tokenized'] = reviews_without_htmltags_df['Reviews'].apply(lambda x: word_tokenize(x))\n\n#removing stop words\nreviews_without_htmltags_df['Reviews_without_stopwords'] = reviews_without_htmltags_df['Reviews_word_tokenized'].apply(lambda x: remove_stopwords(x))\nreviews_without_htmltags_df","7b9d72b6":"#performing lemmatization as a preprocessing step\nreviews_lemmatized = pd.DataFrame(reviews_without_htmltags_df['Reviews_without_stopwords'].apply(lambda x: lemmatization(x)))","79513ca6":"reviews_lemmatized.columns = ['Reviews']\nreviews_lemmatized","f3f73e19":"dataset['Reviews'] = reviews_lemmatized['Reviews'].apply(lambda x: list_to_string(x))\n#dataset['Reviews'] = reviews_lemmatized","7789039c":"import numpy as np\n#Round the ratings to the nearest integer\nnewdf = dataset['Rating'].astype(np.int64)\ndataset['Rating'] = newdf\n#final dataset with pre-processed reviews\ndataset","aa237c51":"import numpy as np\n\ntrain_df,test_df = np.split(dataset, [int(.8*len(dataset))])\nprint(\"training: \",train_df.shape)\nprint(\"test: \",test_df.shape)\nY_train = train_df['Rating'].tolist() #ratings for the train dataset\nY_test = test_df['Rating'].tolist() # ratings for the test dataset","fdc0d7a5":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# corpus = train_df['Reviews'].tolist()\n# vectorizer = TfidfVectorizer(analyzer = 'word',use_idf = True)\n# X = vectorizer.fit_transform(corpus)\n# print(vectorizer.get_feature_names())\n# print(X.shape)","a55c5cf1":"from sklearn.feature_extraction.text import CountVectorizer\n# Create an instance of CountfVectorizer\nvectoriser = CountVectorizer(max_features=5000) # max features is set to 5000 for better accuracy\n# Fit to the data and transform to feature matrix\nX_train = vectoriser.fit_transform(train_df['Reviews'])\nX_test = vectoriser.transform(test_df['Reviews'])","60e42916":"print(X_train)","2f9d27c8":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB(alpha = 1.0e-10)\nclf.fit(X_train, Y_train)\n\nY_pred = clf.predict(X_test)#testing the predictions for test dataset once model has been trained\ntest_df['Predictions'] = Y_pred\nprint(test_df)\naccuracy = clf.score(X_test, np.array(Y_test))\nprint(\"Accuracy of test data predictions: \",accuracy*100,\"%\")","36b319a9":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nY_true = np.array(Y_test)\nmse = mean_squared_error(Y_true, Y_pred)\nprint(\"Accuracy:\",accuracy_score(Y_true, Y_pred)*100,\"%\")\nprint(\"Mean Squared error:\",mse)","793b1667":"from sklearn.model_selection import GridSearchCV\nparams = {'alpha': np.array(np.linspace(0,1,100))}\nmultinomial_nb_grid = GridSearchCV(MultinomialNB(), param_grid=params, n_jobs=3, cv=5, verbose=5,scoring='accuracy')\nmultinomial_nb_grid.fit(X_train, Y_train)\nprint('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, Y_train))\nprint('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, Y_true))\nprint('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\nprint('Best Parameters : ',multinomial_nb_grid.best_params_)\nresults_NB = pd.DataFrame(multinomial_nb_grid.cv_results_['params'])\nresults_NB['test_score'] = multinomial_nb_grid.cv_results_['mean_test_score']\nresults_NB","283e381f":"#ind = params['alpha'].index(multinomial_nb_grid.best_params_['alpha'])\nind = np.where(params['alpha'] == multinomial_nb_grid.best_params_['alpha'])\nprint(ind)","1ace7ed8":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = (15,7)\n\nfig, ax = plt.subplots(1) \nax.plot(results_NB['alpha'], results_NB['test_score'],'ro-')\nax.set_title('Hyperparameter Tuning')\nax.set(xlabel='Alpha', ylabel='Accuracy')\nax.set_xticks(ind)\nax.set_xticklabels([\"Min\"])\nplt.legend(loc=\"upper right\")","b2f51dfa":"# from sklearn import svm\n# clf = svm.SVC()\n# clf.fit(X_train, train_df['Rating'])\n# accuracy = clf.score(X_test, y)\n# print(accuracy)","976d5981":"review = input(\"Enter a review:\")\nX_test = vectoriser.transform([review])\npred = clf.predict(X_test)\nprint(\"The estimated rating is: \", str(pred[0]))\nprint(clf.predict_proba(vectoriser.transform([review])))","108c7037":"X_train_final = vectoriser.fit_transform(dataset['Reviews'])","b9c34564":"clf = MultinomialNB(alpha = 1.0e-10)\nclf.fit(X_train_final, dataset['Rating'])","804334fd":"import pickle\npickle.dump(clf, open('D:\/UTA\/Fall-2020\/DM\/TermProject\/NaiveBayesClassifier', 'wb'))\nwith open('D:\/UTA\/Fall-2020\/DM\/TermProject\/Vectorizer', 'wb') as fin:\n        pickle.dump(vectoriser, fin)","4b4c6b97":"<h3>Links<\/h3>\n\n - Blog post : https:\/\/pxm5568.uta.cloud\/img\/Maitreyee_02.html \n - Working model is deployed at: [http:\/\/pragnyam.pythonanywhere.com\/](http:\/\/pragnyam.pythonanywhere.com\/ )\n - GitHub link : [https:\/\/github.com\/Pragnyashree\/RatingEstimator](https:\/\/github.com\/Pragnyashree\/RatingEstimator)\n  ","29872dc9":"<h4>Processed Dataset<\/h4>\n<p>The ratings are rounded to the nearest integer to get a rating in the scale of 1-10. This cleaned and pre-processed dataset is then used for training and testing our algorithm<\/p>","39536dfe":"<h4>Lemmatization<\/h4>\n<p>Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. <\/p>","d625c62d":"<h3>References<\/h3>\n\n - Sklearn documentation: [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)\n - [https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html)\n - [https:\/\/machinelearningmastery.com\/save-load-machine-learning-models-python-scikit-learn\/](https:\/\/machinelearningmastery.com\/save-load-machine-learning-models-python-scikit-learn\/)\n\n<p>I did not use any references for the model implementation. Just followed official documentation above and process followed in assignment 3.<\/p>","91387853":"<h3> HYPERPARAMETER TUNING <\/h3>\n<p>Hyperparameter tuning is done on the smoothing parameter alpha for Multinomial Naive Bayes. The best accuracy obtained against the best alpha value is then used in training the final model.5 fold cross validation is performed and the best accuracy obtained can be seen as 30.9% for this model.For this process sklearn GridSearchCV method is used.<\/p>\n<p>Best results are obtained for alpha=0.0 but recommended alpha value is 1.0e-10 so that is used for training the algorithm<\/p>","d5002662":"<h3> Splitting dataset into train and test <\/h3>\n<p>We split the dataset in the ratio 4:1 to get the training and the test dataset respectively<\/p>","8341d5d1":"<h4>At this step changing the words in the reviews to lowercase, removing of certain patterns and numbers is performed<\/h4>","4c938918":"<h3>Rating estimation for a sample review<\/h3>","7524d3f1":"<h3>CREATING DOCUMENT MATRIX<\/h3>\n<p>In order to get a vocabulary of words with their frequencies we need to utilize sklearn's <a href = \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer<\/a>. This gives us a dictionary of words with their corresponding frequencies in each document in vector form. We need the output in a matrix form in order to pass it as an input to our models. <\/p>\n<p>Here the max_features is set to 5000 to get a better accuracy and ignore the less frequent words<\/p>\n","0828d119":"<h3>Exporting vectorizer and model to use in deployment server<\/h3>\n<p>Pickle is used to save our model to be used externally. The mentioned files are used in pythonanywhere along with the deployed application.<\/p>","5a1d8a16":"<h3> NAIVE BAYES MODEL IMPLEMENTATION<\/h3>\n<p>This classifier has two probabilities: P(class) which is the probability an input will produce a certain class, and P(input_condition|class) is the probability an input feature has a certain value, given the class. Otherwise, default probability is 0. Multinomial Na\u00efve bayes implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts<\/p>\n<p>The <a href = \"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\"> multinomial naive bayes model<\/a> provided by sklearn is implemented here. At first the model was executed with default alpha= 1. After performing hyperparameter tuning the alpha is updated to get the best accuracy 27.3%.<\/p>","7e5a69d8":"<h4>Dropping rows that have missing comments(NaN)<\/h4>\n\n<p>This step is essential because we have to get a dataset that contains reviews in order to train our model. So removal of ratings without review is executed.<\/p>","1ef07ce5":"<h3>REVIEWS RATING ESTIMATOR<\/h3>","265f0c3f":"<p>Use the 'Reviews' and the corresponding 'Rating' columns as the dataset<\/p>","91ed03d2":"<h3>Challenges faced<\/h3>\n\n - Due to large dataset model execution time was very high so could not implement SVM for the entire dataset. \n - Dataset had missing values, reduced the dataset size by removing rows with NaN reviews.\n - The pre-processing step was removed after it was observed that the accuracy is improved by 1% without pre-processing like lemmatization and stop words removal.","6de49e49":"<h3> Reading the data <\/h3>\n<p>The first step is to read the data and convert it to a pandas dataframe<\/p>","5b18ab46":"<p> This is a project that estimates ratings based on customer reviews.The purpose of such an application is to help identify ratings for reviews where the user ratings are missing. This rating could be used to further analyze the success rate of your product.<\/p><br>\n<p>The dataset used for this project is a boardgamegeek review dataset available at Kaggle here: \n    <a href=\"https:\/\/www.kaggle.com\/jvanelteren\/boardgamegeek-reviews\">Dataset<\/a> This model will also work for other kinds of dataset for e.g. Movie review dataset for IMDB, Restaurant review dataset etc. The various steps involved in the creation of the model are described in detail over the ipynb.<\/p>","334b92a8":"<h3> PREPROCESSING <\/h3>\n\n<p>For preprocessing and cleaning the data in the reviews column, NLTK libraries are used for removing punctuations, stopwords, numbers and lemmatization of the filtered reviews.<\/p>","b904a8af":"<h4>This is the input of our model in matrix form.<\/h4>","fc516df7":"<h3>Training with the complete dataset<\/h3>\n<p>Now that we have a hyper parameter to get the best results the entire dataset is used to train the model to estimate ratings in the application for best results<\/p>","9d191538":"<h4>Hyperparameter tuning plot<\/h4>","6a942671":"<h3>PERFORMANCE EVALUATION <\/h3>\n<p>For evaluation of the algorithm: accuracy and mean squared error is used as performance measure. Since this is a classification for 10 different classes, the accuracy can be low. So the mean squared error will give us idea how close to the original rating was our predicted rating.<\/p>","cc0a20d9":"<h4>Here sentence tokenization,word tokenization and removal of stop words is performed<\/h4>"}}