{"cell_type":{"c1755a9e":"code","b99d16aa":"code","639acdef":"code","11c7c151":"code","0816454c":"code","ff1c7889":"code","f2c12b97":"code","b63d5e64":"code","ca305138":"code","d42a5381":"code","9f32e31a":"code","95d4514b":"code","99fd332a":"code","dc16b86d":"code","661b1654":"code","504da197":"code","561bdc7a":"code","3419fd0f":"code","1f08d6c6":"code","b29b07b8":"code","cd296ea4":"code","f7ce5e8f":"code","107a168d":"markdown","4f81269a":"markdown","b02ef49a":"markdown","f78924e7":"markdown","eb9e2cce":"markdown","957b93df":"markdown","78b00ba2":"markdown","9da18f71":"markdown","efe9e409":"markdown","b3a9cca0":"markdown","393cc375":"markdown","7d477432":"markdown","5c840176":"markdown","1e5aa3f7":"markdown","0514ee30":"markdown","109b3292":"markdown","4913240b":"markdown","8e0742fd":"markdown","28fa04e0":"markdown","07525ece":"markdown","540b6c1f":"markdown","7bf5d25f":"markdown","064fa0c0":"markdown","7199773c":"markdown","5daa92d8":"markdown","6247d3dd":"markdown","201a550f":"markdown","daea8db0":"markdown","5a2dc51b":"markdown","be0e2aae":"markdown","88a574a0":"markdown","a549a016":"markdown","319a1f8e":"markdown","64f3508d":"markdown","1eead6de":"markdown","95ca0bcf":"markdown","6972bd38":"markdown","b62f4d12":"markdown","6f886290":"markdown","c9ca22c7":"markdown","7ee0af1b":"markdown","985527a4":"markdown"},"source":{"c1755a9e":"!pip install dabl","b99d16aa":"# for basic operations\nimport numpy as np \nimport pandas as pd \n\n# for data visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# for getting the file path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# for avoiding warnings\nimport warnings\nwarnings.filterwarnings('ignore')","639acdef":"# reading the data\ndata = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\n# lets check the shape of the dataset\ndata.shape","11c7c151":"# lets check the head of the dataset\npd.set_option('max_columns', 82)\ndata.head()","0816454c":"plt.rcParams['figure.figsize'] = (15, 6)\nplt.style.use('fivethirtyeight')\n\nimport dabl\ndabl.plot(data, target_col = 'SalePrice')","ff1c7889":"# let's check the columns in the dataset\ndata.columns","f2c12b97":"# lets check the different neighborhoods\ndata['Neighborhood'].value_counts()","b63d5e64":"# total number of houses in the neighborhood\nall_houses = data.shape[0]\nprint(\"Total Number of Houses in the Neighborhood :\", all_houses)","ca305138":"# total number of houses in the Old town neighborhood\nhouses_in_OldTown = data[data['Neighborhood'] == 'OldTown'].shape[0]\nprint(\"Total Number of Houses in the Old Town Road :\", houses_in_OldTown)\n","d42a5381":"# lets find the probability of picking a House in the Old Town\nprobability = (houses_in_OldTown\/all_houses)*100\nprint('Probability of picking a house in OldTown: {0:.2f}'.format(probability )+'%')","9f32e31a":"## Enter condtional probability code\ncond_prob = (houses_in_OldTown\/all_houses) * ((houses_in_OldTown - 1)\/(all_houses - 1)) \nprint(\"The Probability of Picking a House in Old Town and again picking a house from the same neighborhood is {0:.9f}\".\n      format(cond_prob*100))","95d4514b":"plt.rcParams['figure.figsize'] = (11, 4)\nplt.style.use('fivethirtyeight')\n\nplt.xticks(rotation=30)\nsns.distplot(data['SalePrice'])\nplt.title('Distribution of Target Column')\nplt.show()","99fd332a":"# lets take seed so that everytime the random values come out to be constant\nnp.random.seed(6)\n\n# lets take 500 sample values from the dataset of 1460 values\nsample_ages = np.random.choice(a= data['SalePrice'], size=500)\n\n# getting the sample mean\nprint (\"Sample mean:\", sample_ages.mean() )          \n\n# getting the population mean\nprint(\"Population mean:\", data['SalePrice'].mean())","dc16b86d":"# lets check Central Limit theorem for this data\n\n# provides capability to define function with partial arguments\nfrom functools import partial\n\n# number of samples to average over.\nn=np.array([1, 2, 3, 5, 10, 100, 200])\n\n# number of times samples of size n are taken. Try varying this number.\nN = 1000\n\n# number of bin boundaries on plots\nnobb=101\n\n# mean of exponential distribution\nexp_mean=3\n\n# parameters of beta distribution\na,b=0.7,0.5 \n\ndist=[partial(np.random.random), \n      partial(np.random.exponential, exp_mean),\n      partial(np.random.beta, a, b)]\n\n# lets define the title names.\ntitle_names=[\"Flat\",\n             \"Exponential (mean=%.1f)\" % exp_mean, \n             \"Beta (a=%.1f, b=%.1f)\" % (a,b)]\n\n# ranges of the three distributions\ndrange=np.array([[0,1],[0,10],[0,1]]) \n\n# means of the three distributions\nmeans=np.array([0.5,exp_mean,a\/(a+b)])\n\n# variances of the three distributions\nvar=np.array([1\/12,exp_mean**2,a*b\/((a+b+1)*(a+b)**2)]) \n\n# generates random samples in the specified ranges for the respective distributions.\nbinrange=np.array([np.linspace(p,q,nobb) for p,q in drange]) \nln,ld=len(n),len(dist)\nplt.figure(figsize=((ld*4)+1,(ln*2)+1))\n\n # loop over number of n samples to average over\nfor i in range(ln):\n     # loop over the different distributions\n    for j in range(ld):\n        plt.subplot(ln,ld,i*ld+1+j)\n        plt.hist(np.mean(dist[j]((N,n[i])),1),binrange[j])\n        plt.xlim(drange[j])\n        if j==0:\n            plt.ylabel('n=%i' % n[i],fontsize=15)        \n        if i==0:\n            plt.title(title_names[j], fontsize=15)\n        else:\n            clt=(1\/(np.sqrt(2*np.pi*var[j]\/n[i])))*np.exp(-(((binrange[j]-means[j])**2)*n[i]\/(2*var[j])))\n            plt.plot(binrange[j],clt,'y',linewidth=2)     \nplt.show()","661b1654":"# lets import the scipy package\nimport scipy.stats as stats\nimport math\n\n# lets seed the random values\nnp.random.seed(10)\n\n# lets take a sample size\nsample_size = 1000\nsample = np.random.choice(a= data['SalePrice'],\n                          size = sample_size)\nsample_mean = sample.mean()\n\n# Get the z-critical value*\nz_critical = stats.norm.ppf(q = 0.95)  \n\n # Check the z-critical value  \nprint(\"z-critical value: \",z_critical)                                \n\n# Get the population standard deviation\npop_stdev = data['SalePrice'].std()  \n\n# checking the margin of error\nmargin_of_error = z_critical * (pop_stdev\/math.sqrt(sample_size)) \n\n# defining our confidence interval\nconfidence_interval = (sample_mean - margin_of_error,\n                       sample_mean + margin_of_error)  \n\n# lets print the results\nprint(\"Confidence interval:\",end=\" \")\nprint(confidence_interval)\nprint(\"True mean: {}\".format(data['SalePrice'].mean()))","504da197":"np.random.seed(12)\n\nsample_size = 500\n\nintervals = []\nsample_means = []\n\nfor sample in range(25):\n    sample = np.random.choice(a= data['SalePrice'], size = sample_size)\n    sample_mean = sample.mean()\n    sample_means.append(sample_mean)\n\n     # Get the z-critical value* \n    z_critical = stats.norm.ppf(q = 0.975)         \n\n    # Get the population standard deviation\n    pop_stdev = data['SalePrice'].std()  \n\n    stats.norm.ppf(q = 0.025)\n\n    margin_of_error = z_critical * (pop_stdev\/math.sqrt(sample_size))\n\n    confidence_interval = (sample_mean - margin_of_error,\n                           sample_mean + margin_of_error)  \n    \n    intervals.append(confidence_interval)\n    \n\nplt.figure(figsize=(13, 9))\n\nplt.errorbar(x=np.arange(0.1, 25, 1), \n             y=sample_means, \n             yerr=[(top-bot)\/2 for top,bot in intervals],\n             fmt='o')\n\nplt.hlines(xmin=0, xmax=25,\n           y=data['SalePrice'].mean(), \n           linewidth=2.0,\n           color=\"red\")\nplt.title('Confidence Intervals for 25 Trials', fontsize = 20)\nplt.show()","561bdc7a":"# lets import z test from statsmodels\nfrom statsmodels.stats.weightstats import ztest\n\nz_statistic, p_value = ztest(x1 = data[data['Neighborhood'] == 'OldTown']['SalePrice'],\n                             value = data['SalePrice'].mean())\n\n# lets print the Results\nprint('Z-statistic is :{}'.format(z_statistic))\nprint('P-value is :{:.50f}'.format(p_value))","3419fd0f":"print('No of houses in Stone Brook: {}'\\\n      .format(data['Neighborhood'].value_counts()['StoneBr']))\n","1f08d6c6":"stats.ttest_1samp(a= data[data['Neighborhood'] == 'StoneBr']['SalePrice'],               # Sample data\n                 popmean= data['SalePrice'].mean())  # Pop mean","b29b07b8":"national = pd.DataFrame([\"white\"]*100000 + [\"hispanic\"]*60000 +\\\n                        [\"black\"]*50000 + [\"asian\"]*15000 + [\"other\"]*35000)          \n\nminnesota = pd.DataFrame([\"white\"]*600 + [\"hispanic\"]*300 + \\\n                         [\"black\"]*250 +[\"asian\"]*75 + [\"other\"]*150)\n\nnational_table = pd.crosstab(index=national[0], columns=\"count\")\nminnesota_table = pd.crosstab(index=minnesota[0], columns=\"count\")\n\nprint( \"National\")\nprint(national_table)\nprint(\" \")\nprint( \"Minnesota\")\nprint(minnesota_table)","cd296ea4":"observed = minnesota_table\n\nnational_ratios = national_table\/len(national)  # Get population ratios\n\nexpected = national_ratios * len(minnesota)   # Get expected counts\n\nchi_squared_stat = (((observed-expected)**2)\/expected).sum()\n\nprint(chi_squared_stat)","f7ce5e8f":"# Let's test if knowing LandContour which is the overall flatness of the property tells us anything about the price\n\n# For this let's divide the SalePrice in three buckets - High, Medium, Low\n\nimport scipy.stats as sp\ndef compute_freq_chi2(x,y):\n    freqtab = pd.crosstab(x,y)\n    print(\"Frequency table\")\n    print(\"============================\")\n    print(freqtab)\n    print(\"============================\")\n    chi2, pval, dof, expected = sp.chi2_contingency(freqtab)\n    print(\"ChiSquare test statistic: \",chi2)\n    print(\"p-value: \",pval)\n    return\n\n\nprice = pd.qcut(data['SalePrice'], 3, labels = ['High', 'Medium', 'Low'])\ncompute_freq_chi2(data.LandContour, price)","107a168d":"### Chi Square Test\n\nThe term \"chi-squared test,\" also written as \u03c7\u00b2 test, refers to certain types of statistical hypothesis tests that are valid to perform when the test statistic is chi-squared distributed under the null hypothesis. Often, however, the term is used to refer to Pearson's chi-squared test and variants thereof.\n\n***A chi-squared goodness of fit tests whether the distribution of sample categorical data matches an expected distribution.***\n\nFor example, \n* *you could use a chi-squared goodness-of-fit test to check whether the race demographics of members at your church or school match that of the entire population of your country*.\n* *you could check whether the computer browser preferences of your friends match those of Internet uses as a whole.*\n\n* *When working with categorical data the values the observations themselves aren't of much use for statistical testing because categories like \"male\", \"female,\" and \"other\" have no mathematical meaning.*","4f81269a":"#### Let's generate some fake demographic data for U.S. and Minnesota and walk through the chi-square goodness of fit test to check whether they are different:","b02ef49a":"* **Good Fit**: If the significance value that is p-value associated with chi-square statistics is 0.002, there is very strong evidence of rejecting the null hypothesis of no fit. It means good fit.","f78924e7":"## Simple Probability Distribution\n\n* Lets Take an example, that we throw a dice of containing six faces.\n* so, there are Total no. of Combinations = 6*6 = 36\n\nLet\u2019s see how:\n\n2 {(1,1)} => 1\/36\n\n3 {(1,2),(2,1)} => 2\/36\n\n4 {(2,2),(3,1),(1,3)} => 3\/36\n\n5 {(1,4),(4,1),(2,3),(3,2)} => 4\/36\n\n6 {(3,3),(1,5),(5,1),(2,4),(4,2)} => 5\/36\n\n7 {(1,6),(6,1),(2,5),(5,2),(3,4),(4,3)} => 6\/36\n\n8 {(2,6),(6,2),(3,5),(5,3),(4,4)} => 5\/36\n\n9 {(3,6),(6,3),(5,4),(4,5)} => 4\/36\n\n10 {(4,6),(6,4),(5,5)} => 3\/36\n\n11 {(5,6),(6,5)} => 2\/36\n\n12 {(6,6)} = > 1\/36","eb9e2cce":"* **Let's check out the Probability of picking a house in the Neighborhood - \"OldTown\"**\n\n\nNo. of houses in OldTown\/Total no. of houses\nLet's go through this in Python","957b93df":"* Let's take an example to better understand the meaning of z-score\n    * Let's Suppose the average height of a Student in a class is 1.4 meters\n    * In that same class one of the students is 1.85m tall\n    * You can see on the bell curve that 1.85m is 3 standard deviations from the mean of 1.4.\n    * so, the student with 1.85m height has a **z-score\" of 3.0**.","78b00ba2":"![image.png](attachment:image.png)","9da18f71":"## PDF and PMF\n\n* The probability distribution for a discrete random variable is the **probability mass function** for that variable and similarly and if our random variable takes continuous values the distribution is called a **probability density function**.\n\n* In the previous what we plotted was the Probability Mass Function of a Discrete Random Variable (X which is the sum of two fair dies)\n\n* One of the most common Probability Distribution Functions is the Normal Distribution.\n","efe9e409":"* The low p-value tells us that the two variables aren't independent and knowing the LandContour of a house does tells us something about its SalePrice.\n\n**The frequency distribution reflects this**\n* Houses that are Near Flat\/Level(Lvl) have an equal distribution of SalePrice.\n* On the other hand houses that are at a Hillside i.e., Significant slope from side to side (HLS) have almost thrice as much houses with low price than high prices.","b3a9cca0":"* If the P value if less than 0.05, then we can reject our null hypothesis against the alternate hypothesis.\n\n* **The Probability of getting the given distribution of houseprices in OldTown under the assumption that its mean, is the same as the mean of all house prices.**","393cc375":"![image.png](attachment:image.png)","7d477432":"## Statistical Inference\n\n* This **subset** of the population is nothing but the Sample data\n\n* We carry out various tests on the Sample to gain insight on the larger population out there!\n\n* Therefore Statistical inference is the process of analyzing sample data to gain insight into the population from which the data was collected and to investigate differences between different data samples.\n\nThe sample mean is usually not exactly the same as the population mean. This difference can be caused by many factors including poor survey design, biased sampling methods and the randomness inherent to drawing a sample from a population.\n\n","5c840176":"![image.png](attachment:image.png)","1e5aa3f7":"## Chi-Squared Goodness of fit Test","0514ee30":"## Confidence Interval\n\n**Confidence Interval (CI)** is a type of estimate computed from the statistics of the observed data. This proposes a range of plausible values for an unknown parameter (for example, the mean). The interval has an associated confidence level that the true parameter is in the proposed range.","109b3292":"* The distribution for our target variable aka SalePrice doesn't resemble a normal distribution, it is skewed to the right\n* If we remove the outliers, it'd somewhat resemble a Normal Dstribution","4913240b":"### P Value\n\n* In statistical hypothesis testing, **the p-value or probability value** is the probability of obtaining test results at least as extreme as the results actually observed during the test, assuming that the null hypothesis is correct. \n\n* So now say that we have put a significance (\u03b1) = 0.05\n* This means that if we see a p-value of lesser than 0.05, we reject our Null and accept the Alternative to be true\n","8e0742fd":"## Z-Score\n\n* The number of standard deviations from the mean is also called the \"Standard Score\", \"sigma\" or \"z-score\".","28fa04e0":"## Type 1 and Type 2 Error\n\n* In statistical hypothesis testing, a type I error is the rejection of a true null hypothesis, while a type II error is the non-rejection of a false null hypothesis\n\n### Type 1  and Type 2 Error Example\n\nFor example, let's look at the trail of an accused criminal. The null hypothesis is that the person is innocent, while the alternative is guilty. \n* A Type 1 error in this case would mean that the person is not found innocent and is sent to jail, despite actually being innocent.\n* A Type 2 Erroe Example In this case would be, the person is found innocent and not sent to jail despite of him being guilty in real.\n","07525ece":"### Checking for Skewness of the data\n\n* We Generally check Askewness in the Target Columns of the data.\n* Skewness is a state of distribution where the distribution is highly biased towards the right or left side of the plot.","540b6c1f":"\n### Another way to test: Gosset's (Student's) t-test","7bf5d25f":"### Effect of LandContour on SalePrice","064fa0c0":"## Inference","7199773c":"# Introduction to Inferential Statistics\n\n## Introduction to Probability\n\n* Basic Probability\n* Conditional Probability\n* Simple Probability Distribution\n* Probability Mass Function (p.m.f) & Probability Density Function (p.d.f)\n* Normal Distribution\n* Normal Distribution & Standard Deviation\n* Concept of Z-score\n\n## Introduction to Inference\n\n* Sample Mean & Population Mean\n* Statistical Inference\n* Central Limit Theorem\n* Confidence Intervals\n* Interpretation Of Confidence Interval\n* Hypothesis Testing\n* Why Null Hypothesis ?\n* Alternate Hypothesis\n* P-Value\n* t-test\n* Type I and Type II error\n* Chi-squared Goodness of fit test\n* Chi-sqaured Test of Independence","5daa92d8":"## Normal Distrution\n\n* Normal distribution, also known as the Gaussian distribution, is a probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean. In graph form, normal distribution will appear as a bell curve.","6247d3dd":"![image.png](attachment:image.png)","201a550f":"\n**Are house prices in OldTown really different from the House Prices of Other Neighborhoods?**","daea8db0":"For any 2 events A & B, the probability that at least one occurs is the sum of their individual probabilities minus the probability of their intersection. i.e\n\n\n$$ P(A\\cup B) = P(A) + P(B) - P(A\\cap B) $$\n\nExample\n\nThe National Sleep Foundation reports that around 3% of the American population has sleep-breathing issues. They also report that around 10% of the American population has restless leg syndrome. Does this imply that 13% of people will have at least one of these problems?\n\nAnswer: No, the events can occur simultaneously and so are not mutually exclusive. To elaborate:","5a2dc51b":"## Chi-Sqaured Test of Independence\n\nIndependence is a key concept in probability that describes a situation where knowing the value of one variable tells you nothing about the value of another.\n\nFor instance, the month you were born probably doesn't tell you anything which web browser you use, so we'd expect birth month and browser preference to be independent.\n\nOn the other hand, your month of birth might be related to whether you excelled at sports in school, so month of birth and sports performance might not be independent.\n\nThe chi-squared test of independence tests whether two categorical variables are independent.","be0e2aae":"### Rules\n\n* The Probability that an event occurs with certainty is 1\n* The Probability that an event will not occur surely is 0\n* The Probability of the complement of an event is 1 minus the probability of that event.\n\n* The probability of at least 1 of 2 (or more) things that can not simultaneously occur (mutually exclusive) is the sum of their respective probabilities\n\n* **Mutually exclusive is a statistical term describing two or more events that cannot occur simultaneously. For example, it is impossible to roll a five and a three on a single die at the same time.**","88a574a0":"### Sample Mean and population Mean\n\n* Let's consider a sample of 500 houses at random from 1460 houses and plot it's mean\n* But the mean of these 500 houses can be near or pretty far away from the mean of the 1460 houses calculated earlier.","a549a016":"The 95% confidence interval defines a range of values that you can be 95% certain contains the population mean. With large samples, you know that mean with much more precision than you do with a small sample, so the confidence interval is quite narrow when computed from a large sample.","319a1f8e":"## Conditional Probability\n\n* There are 10 candies in a bag: 5 green, and 5 blue.\n\n* What is the probability of getting 3 blue candies in a row?\n\n* The probability of getting the first blue candy is 5\/10, or 1\/2.\n\nWhen we pick a blue candy, though, we remove it from the bag. We're left with 9 candies in total with (5-1 =)4 Blue ones.\nSo the probability of getting another blue is 4\/9.\nSimilarly, the probability of picking a third blue candy is 3\/8\n\nSince we're calculating the probability of picking 1 Blue Candy AND 1 Blue Candy AND 1 Blue Candy\n\n* Our final probability is **1\/2 * 4\/9 * 3\/8, or .0833**. So, there is an 8.3% chance of picking three blue candies in a row.\nSimple tricks: Whenever you have to verbally say AND (like we just did above), you will want to MULTIPLY the probabilities\nWhenever you have to verbally say OR, you will want to ADD the probabilities.\n\n\n**GIVEN that we have the probability of picking a house in \"OldTown\" neighborhood, we go a step further and AGAIN pick a house from the SAME neighborhood ?**","64f3508d":"## Hypothesis Testing\n\n* $Statistical Hypothesis$, sometimes called confirmatory data analysis, is a hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables. A statistical hypothesis test is a method of statistical inference.\n\n### Null Hypothesis\n\n* In Inferential Statistics, **The Null Hypothesis is a general statement or default position that there is no relationship between two measured phenomena or no association among groups.**\n\n* Statistical hypothesis tests are based on a statement called the null hypothesis that assumes nothing interesting is going on between whatever variables you are testing.\n\n* Therefore, in our case the Null Hypothesis would be:\n**The Mean of House Prices in OldTown is not different from the houses of other neighborhoods**\n\n### Alternate Hypothesis\n\n* The alternate hypothesis is just an alternative to the null. For example, if your null is **I'm going to win up to 1000** then your alternate is **I'm going to win more than 1000.** Basically, you're looking at whether there's enough change (with the alternate hypothesis) to be able to reject the null hypothesis\n\n###  The Null Hypothesis is assumed to be true and Statistical evidence is required to reject it in favor of an Alternative Hypothesis.\n\n\n1. Once you have the null and alternative hypothesis in hand, you choose a significance level (often denoted by the Greek letter \u03b1). The significance level is a probability threshold that determines when you reject the null hypothesis.\n\n2. After carrying out a test, if the probability of getting a result as extreme as the one you observe due to chance is lower than the significance level, you reject the null hypothesis in favor of the alternative.\n\n3. This probability of seeing a result as extreme or more extreme than the one observed is known as the p-value.","1eead6de":"### Now, let's also see if house prices in Stone Brook neighborhood are different from the houses in the rest of the neighborhoods.","95ca0bcf":"* The T-test is a statistical test used to determine whether a numeric data sample differs significantly from the population or whether two samples differ from one another.\n* A z-test assumes a sample size >30 to work, but what if our sample is less than 30?\n* A t-test solves this problem and gives us a way to do a hypothesis test on a smaller sample.\n* Now, let's also see if house prices in Stone Brook neighborhood are different from the houses in the rest of the neighborhoods.","6972bd38":"* Notice that the true mean is contained in our interval.\n* A confidence interval of 95% would mean that if we take many samples and create confidence intervals for each of them, 95% of our samples' confidence intervals will contain the true population mean.\n* Now, let's create several confidence intervals and plot them to get a better sense of what it means to \"capture\" the true mean","b62f4d12":"* The p-value in this case again is low and we can reject our null hypothesis","6f886290":"## Basic Probability\n\n* Let's start with a simple example: Say, we flip a fair coin\n\n* Intuitively, there's a 50% chance of getting heads, and a 50% chance of getting tails. This is because there are only two possible outcomes, and each event is equally likely.\n\n* Therefore, we can say that the Probability of getting a Heads is 0.5. Similarly, Probability of getting a Tails is 0.5\n\n* Probability can roughly be described as **the chance of an event or sequence of events occurring**.\n\n* **Experiment** \u2013 are the uncertain situations, which could have multiple outcomes. A coin toss is an experiment.\n* **Outcome** is the result of a single trial. So, if head lands, the outcome of or coin toss experiment is \u201cHeads\u201d\n* **Event** is one or more outcomes from an experiment. \u201cTails\u201d is one of the possible events for this experiment.","c9ca22c7":"## Central Limit Theorem\n\nThe central limit theorem (CLT) is a statistical theory that states that given a sufficiently large sample size from a population with a finite level of variance, the mean of all samples from the same population will be approximately equal to the mean of the population. Furthermore, all of the samples will follow an approximate normal distribution pattern, with all variances being approximately equal to the variance of the population divided by each sample's size.","7ee0af1b":"\nIn the graphs above the yellow curve is the predicted Gaussian distribution from the Central Limit Thereom. Notice that the rate of convergence of the sample mean to the Gaussian depends on the original parent distribution. Also,\n\nthe mean of the Gaussian distribution is the same as the original parent distribution,\nthe width of the Gaussian distribution varies with sample size as $1\/\\sqrt{n}$.","985527a4":"* It is easily visible that 95% of the times the blue lines(the sample meean) overlaps with the red line(the true mean), also 5% of the times it is expected to not overlap with the red line(the true mean)."}}