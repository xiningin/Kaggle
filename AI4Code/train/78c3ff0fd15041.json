{"cell_type":{"898360c8":"code","3e1cbcf0":"code","79aeec41":"code","84d1d20b":"code","9f534137":"code","f96ef275":"code","5256d107":"code","9d0bc180":"code","76c472b9":"code","ea19c65e":"code","e70a8a7c":"code","83c94140":"code","15dda91d":"code","067b45ae":"code","76c4ae23":"code","07e0d185":"code","5ce9a9fa":"markdown","7595c76a":"markdown","06b5058e":"markdown","3de2c2ec":"markdown","9ad7f660":"markdown","44d0cb44":"markdown","1ad093c7":"markdown","acc9094b":"markdown","4b2fb4cd":"markdown","a22f49d8":"markdown","6e911cd6":"markdown","3f2ad4c5":"markdown","872de6e1":"markdown","6796652c":"markdown","a05e84d3":"markdown"},"source":{"898360c8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb\nfrom sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nimport seaborn as sns\nfrom sklearn.naive_bayes import GaussianNB\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3e1cbcf0":"#Raw_data\nRaw_data = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')","79aeec41":"Raw_data","84d1d20b":"Raw_data.info()","9f534137":"HeartDisease = Raw_data[Raw_data['HeartDisease'] == 1]\nRaw_data['Count'] = 1","f96ef275":"categorical_columns = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\nfig, axes = plt.subplots(2, 3, figsize=(20, 15))\nfig.suptitle('The Categorical feature')\nfor i in range(2):\n    for j in range(3):\n        if 3*i + j <= 6 :\n            group = Raw_data.groupby(by = f'{categorical_columns[3*i + j]}').sum()\n            sns.barplot(y = group['Count'],x = group.index, alpha = 0.3, color = 'g', label = '# of people', ax = axes[i, j])\n            sns.barplot(y = group['HeartDisease'],x = group.index, alpha = 0.3, color = 'r', label = '# of people disease', ax = axes[i, j])\n            axes[i, j].set_title(f'The feature : {categorical_columns[3*i + j]}')\n            axes[i, j].set_ylabel('# of people')\n            axes[i, j].legend()\n        else:\n            break","5256d107":"numeric_data = Raw_data.drop(columns = categorical_columns)\nnumeric_data = numeric_data.drop(columns = ['Count', 'HeartDisease'])\nnumeric_columns = numeric_data.columns\nfig, axes = plt.subplots(2, 3, figsize=(20, 15))\nfig.suptitle('The Numeric feature histplot')\nfor i in range(2):\n    for j in range(3):\n        if 3*i + j <= 4 :\n            group = Raw_data.groupby(by = f'{numeric_columns[3*i + j]}').sum()\n            sns.histplot(data = Raw_data, x = Raw_data[f'{numeric_columns[3*i + j]}'], color = 'g', alpha = 0.3, label = '# of people ', kde=True, ax = axes[i, j])\n            sns.histplot(data = HeartDisease , x = HeartDisease[f'{numeric_columns[3*i + j]}'], color = 'r', alpha = 0.3, label = '# of peoplt get disease', kde=True, ax = axes[i, j])\n            axes[i, j].set_title(f'The feature : {numeric_columns[3*i + j]}')\n            axes[i, j].set_ylabel('# of people')\n            axes[i, j].legend()\n        else:\n            break","9d0bc180":"fig, axes = plt.subplots(2, 3, figsize=(20, 15))\nfig.suptitle('The Numeric feature boxplot')\nfor i in range(2):\n    for j in range(3):\n        if 3*i + j <= 4 :\n            sns.boxplot(y = Raw_data[f'{numeric_columns[3*i + j]}'],x = Raw_data['HeartDisease'], ax = axes[i, j])\n            axes[i, j].set_title(f'The feature : {numeric_columns[3*i + j]}')","76c472b9":"#Dummies the categorical columns:\nRaw_data['FastingBS'] = Raw_data['FastingBS'].apply(str) # For categorical\nRaw_Data = pd.get_dummies(Raw_data)","ea19c65e":"from sklearn.model_selection import train_test_split\n\ndata = Raw_Data.drop(columns = 'HeartDisease')\ntarget = Raw_Data['HeartDisease']\n\n\nx_train, x_test, y_train, y_test = train_test_split(data, target, train_size = 0.8, random_state = 5)","e70a8a7c":"def model_fit(x_train, x_test, y_train, y_test):\n    \n    #LogisticRegression\n    LR = LogisticRegression(solver='liblinear', C = 1)\n    LR.fit(x_train, y_train)\n    print('1.LogisticRegression score (test) :', LR.score(x_test, y_test))\n          \n    #RandomForestClassifier\n    RFC = RandomForestClassifier(n_estimators = 140, criterion = 'entropy', max_depth = 7, random_state = 3)\n    RFC.fit(x_train, y_train)\n    print('2.RandomForestClassifier score (test) :', RFC.score(x_test, y_test))\n          \n    #DecisionTreeClassifier\n    DTC = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\n    DTC.fit(x_train,y_train)\n    print('3.DecisionTreeClassifier score (test) :', DTC.score(x_test, y_test))\n    \n    #KNeighborsClassifier\n    KNC = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski')\n    KNC.fit(x_train, y_train)\n    print('4.KNeighborsClassifier score (test) :', KNC.score(x_test, y_test))\n          \n    #SVC for linear\n    SVC_linear = LinearSVC(C = 0.1)\n    SVC_linear.fit(x_train, y_train)\n    print('5. SVC for linear score (test) :', SVC_linear.score(x_test, y_test))\n          \n    #SVC for rbf\n    SVC_rbf = SVC(kernel='rbf', C = 10, gamma = 0.01)\n    SVC_rbf.fit(x_train, y_train)\n    print('6. SVC for rbf score (test) :', SVC_rbf.score(x_test, y_test))\n    #XGBC\n    XG = xgb.XGBClassifier(objective = 'binary:logistic' , learning_rate = 0.05,use_label_encoder=False,\n                n_estimators = 50).fit(x_train, y_train)\n    print('7. XGBClassifier score(test):', XG.score(x_test, y_test))\n    \n    #GradientBoostClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    GBC = GradientBoostingClassifier(random_state = 5, learning_rate = 0.03)\n    GBC.fit(x_train, y_train)\n    print('8. GradientBoostClassifier score(test):', GBC.score(x_test, y_test))\n    return LR, RFC, DTC, KNC, SVC_linear, SVC_rbf, XG, GBC","83c94140":"Model = model_fit(x_train, x_test, y_train, y_test)","15dda91d":"ML_model = ['LogisticRegression', 'RandomForestClassifier', 'DecisionTreeClassifier', 'KNeighborsClassifier', \n            'SVC for linear ', 'SVC for rbf ', 'XGBClassifier score', 'GradientBoostingClassifier']\nscore_num = []\nfor i in range(8):\n  score_num.append(Model[i].score(x_test, y_test))\n\nplt.figure(figsize = (15, 10))\nplt.xlabel('Score')\nplt.ylabel('ML Model')\nplt.title('The Score in Seven ML Model')\nsns.barplot(x = score_num, y = ML_model)","067b45ae":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import plot_model, to_categorical\ntrain_Data = np.array(data)\ntrain_Target = np.array(target)\nmodel = Sequential()\nmodel.add(Dense(64, activation = 'relu', input_shape = (data.shape[1],)))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Dense(16, activation = 'relu'))\nmodel.add(Dropout(rate = 0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))\nmodel.compile(optimizer= 'adam', \n              loss = 'binary_crossentropy', \n              metrics = ['accuracy'])\n\nhistory = model.fit(train_Data, train_Target, epochs = 100, validation_split = 0.2, batch_size = 16, verbose = 0)","76c4ae23":"df_DL = pd.DataFrame(history.history)\ndf_DL.head()\n\nplt.plot(df_DL.index, df_DL['loss'], label = 'loss')\nplt.plot(df_DL.index, df_DL['val_loss'], label = 'Val_loss')\nplt.xlabel( 'Epochs')\nplt.ylabel('Binary_crossentropy')\nplt.title('DL loss function')\nplt.legend()\nplt.show()\nplt.clf()\n\nplt.plot(df_DL.index, df_DL['accuracy'], label = 'accuracy')\nplt.plot(df_DL.index, df_DL['val_accuracy'], label = 'Val_accuracy')\nplt.xlabel( 'Epochs')\nplt.ylabel('Accuracy')\nplt.title('DL Accuracy process')\nplt.legend()\nplt.show()\nplt.clf()","07e0d185":"score_num.append(model.evaluate(train_Data ,train_Target)[1])\nML_model = ['LogisticRegression', 'RandomForestClassifier', 'DecisionTreeClassifier', 'KNeighborsClassifier',\n            'SVC for linear ', 'SVC for rbf ', 'XGBClassifier score', 'GradientBoostingClassifier', 'DeepLearning']\nplt.figure(figsize = (15, 10))\nplt.xlabel('Score')\nplt.ylabel('ML Model')\nplt.title('The Score in Seven ML Model')\nsns.barplot(x = score_num, y = ML_model)","5ce9a9fa":"## 3-5. Numeric_columns's data analysis by boxplot","7595c76a":"## 7. The Best Model : GradientBoostingClassifie!","06b5058e":"# 2. Data exploration","3de2c2ec":"## 5.1 Split the train and test","9ad7f660":"## 3-3. Categorical_columns's data analysis by barplot","44d0cb44":"## 5-2. Model Selection","1ad093c7":"## 3-1. Categorical features analysis : Barplot\n\n## 3-2. Numeric features analysis : Histplot, Boxplot","acc9094b":"## 6-1. Deep Learning","4b2fb4cd":"# 1. Read the data","a22f49d8":"## 6-2. Model Compasion (8's ML Model + Deep Learning)","6e911cd6":"# 4. Data Preprocessing","3f2ad4c5":"# 5. Machine Learning Model ","872de6e1":"# 6. Model Compasion (8's ML Model)","6796652c":"# 3. EDA :\n## All features = categorical features + numeric features","a05e84d3":"## 3-4. Numeric_columns's data analysis by histplot"}}