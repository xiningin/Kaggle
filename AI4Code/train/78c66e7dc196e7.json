{"cell_type":{"420f219b":"code","c428ee65":"code","b9b708a4":"code","db499420":"code","5143edb7":"code","d92b71f9":"code","6b3b7917":"code","9cff49c4":"code","16a3cb2d":"code","e45e03c2":"code","0b17b5eb":"code","f8c5b2db":"code","3b7fbe6a":"code","d261b870":"code","e0bd9a9e":"code","b4c08aa4":"code","7eb463b1":"code","fcd8dfe2":"code","74d9f74b":"code","0b020c77":"code","410b9a1f":"code","239087d7":"code","f935741c":"code","b55767c7":"code","8c64586c":"code","018cf1d2":"code","a45e245d":"code","f5d7079c":"code","41a87f88":"code","7feb7868":"code","1c9d1cf1":"code","7c66923e":"code","18ca6a92":"code","ebe4171a":"code","21e3dc79":"code","8f00d5c0":"code","f7ab209e":"code","bb606e8c":"code","80148202":"code","8aa22bea":"code","6debee47":"code","35dd5258":"code","43ca7e6a":"markdown","1c380422":"markdown","f195cdcc":"markdown","42f175c5":"markdown","89014661":"markdown","7cac7921":"markdown","ccf467f7":"markdown","5950699d":"markdown","486bcbd7":"markdown","273f95cc":"markdown","8c4d87cd":"markdown","2152c2de":"markdown","54a4b93c":"markdown","5f6f3e58":"markdown","acf79fd8":"markdown","262a7f5f":"markdown","2abf216b":"markdown","9d9d9b5e":"markdown","f2533737":"markdown","7f6da85d":"markdown","a5aeeb22":"markdown","88f5f211":"markdown","90d3e73f":"markdown","901f8d60":"markdown"},"source":{"420f219b":"__author__ = \"Victor Xu\"\n__email__ = \"victor.c.xu@gmail.com\"\n__website__ = \"victorxu.me\"\n\n__powerpoint_presentation__ = \"https:\/\/www.dropbox.com\/s\/ezenffm4bbrutar\/grocer.pdf?dl=0\"\n__write_up__ = \"coming soon\"\n\n__copyright__ = \"Copyright 2019, Victor Xu\"","c428ee65":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n#from src.data_loader import load_monthly_data ","b9b708a4":"abr_month = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n            'aug', 'sep', 'oct', 'nov', 'dec']\n\nabr_month = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n            'aug', 'sep', 'oct', 'nov', 'dec']\n\ndef load_monthly_data():\n    \"\"\"Loads monthly sales data from  csv\"\"\"\n\n    monthly = pd.read_csv(\n        '..\/input\/total-sale-2018-yearly-data-of-grocery-shop\/SELL_1.csv',\n        delimiter=';',\n        decimal=',',\n        encoding='latin-1')\n    monthly = monthly.rename(columns={\n        'Date': 'date',\n        'PKod': 'sku',\n        'Pgroup': 'group',\n        'Pname': 'name',\n        'Pquantity': 'quantity',\n        'pce_zn': 'unit_cogs',\n        'pwa_zn': 'agg_cogs',  # aggregate cogs for this sku\n        'pce_sn': 'unit_revenue',\n        'pwn_sn': 'agg_rev',  # aggregate revenue for this sku\n        'pmarza': 'gross_margin',\n        'pmarzajedn': 'unit_contribution',\n        'pkwmarza': 'agg_dollar_contribution',\n        'pudzmarza': 'share_of_margin'\n    })\n\n    monthly = monthly.drop(['pwa_sn', 'pce_sb', 'pwa_sb', 'pudzsb'], axis=1)\n    monthly.group = monthly.group.str.lower()\n    monthly.name = monthly.name.str.lower()\n    monthly.date = pd.to_datetime(monthly.date, format=\"%d.%m.%Y\")\n    monthly.unit_cogs = monthly.unit_cogs.str.replace(\n        ',', '.').str.replace(\n        ' ', '')\n\n    monthly.group = monthly.group.replace(\n        {\"ketch_concetrate_mustard_majo_horseradish\": \"sauce\"})\n\n    return monthly\n\n\nmonthly = load_monthly_data()\n\nmonthly = load_monthly_data()","db499420":"monthly.head()","5143edb7":"monthly.info()","d92b71f9":"# Drop empty rows\nidx_to_drop = monthly.unit_cogs[monthly.unit_cogs.str.len() == 0].index\nmonthly = monthly.drop(idx_to_drop)\n\n# Convert to numeric\nmonthly.unit_cogs =  monthly.unit_cogs.astype('float')\n\n# Add aggregate revenue column\nmonthly['agg_rev'] = monthly['quantity'] * monthly['unit_revenue']","6b3b7917":"monthly.isnull().sum()","9cff49c4":"monthly.duplicated().sum()","16a3cb2d":"monthly.columns","e45e03c2":"categorical_cols = ['group']\nnumeric_cols = list(monthly.columns)\nnumeric_cols.remove('group')","0b17b5eb":"monthly.describe(include = ['O'])","f8c5b2db":"monthly.describe(include = [np.number])","3b7fbe6a":"# Getting the full list of categories and the number of skus within each category\ncategory_sku_count = monthly[['sku','group']].drop_duplicates().groupby('group').count().sort_values(by='sku', ascending=False)\ncategory_annual_contribution = monthly[['group','agg_dollar_contribution']].drop_duplicates().groupby('group').sum().sort_values(by='agg_dollar_contribution', ascending=False)\ncategory_sku_annual_contrib = category_sku_count.join(category_annual_contribution)\n\n# calculating aggregate dollar revenue\nmonthly['agg_revenue'] = monthly['quantity'] * monthly['unit_revenue']","d261b870":"labels = list(category_sku_annual_contrib.index)\nx = np.arange(len(labels))\nwidth = .35\n\nfig, ax1 = plt.subplots(figsize=(17,6))\nax1.bar(x - width\/2, category_sku_count.sku, width, color=['orange'])\nax2 = ax1.twinx()\nax2.bar(x+ width\/2, category_annual_contribution.agg_dollar_contribution, width)\n\nax1.legend(['SKU'])\nax2.legend(['Contribution'],loc=2)\n\nax1.set_ylabel('# of SKUs')\nax2.set_ylabel('Aggregate Annual Contribution (PLN)')\n\nax1.set_xticks(x)\nax1.set_xticklabels(labels, minor=False, rotation=90);","e0bd9a9e":"def get_mom_growth(df):\n    \"\"\"Get the month over month growth rate for input data\"\"\"\n    \n    growth_df = df \/ df.shift(1, axis=1) - 1\n    return growth_df.iloc[:,1:]\n\n\ndef top_n_cat_by_contrib_idx(n):\n    \"\"\"Returns sorted n largest product categories index (pd.Int64Index) by annual aggregate revenue\"\"\"\n    \n    return category_contribution_by_month.sum(axis=1).sort_values(ascending=False).iloc[:n].index\n\n\ndef ma(df, n):\n    \"\"\"Returns a moving average dataframe based on given period\"\"\"\n    \n    return df.rolling(window=n, axis=1).mean().iloc[:,n-1:]\n\n\ndef cat_by_month(value, aggfunc='sum'):\n    \"\"\"Returns df aggregated on value (fn parameter). Returned df has product categories as rows \n    and month of year as columns\"\"\"\n    \n    df = monthly.pivot_table(value, index='group', columns=['date'], aggfunc=aggfunc)\n    df = df.fillna(0)\n    df.columns = abr_month\n    return df\n\n\n# These functions have some dependent dataframes\ncategory_contribution_by_month = cat_by_month('agg_revenue') - cat_by_month('agg_cogs')","b4c08aa4":"fig1, axs = plt.subplots(4, 4, figsize=(18,18), sharex=False)\ni=0\n\nmonthly_items_sold_by_category = cat_by_month('sku','count').T\nmonthly_items_sold_by_category = monthly_items_sold_by_category[list(top_n_cat_by_contrib_idx(17))]\n\ncategories_ex_vegetables = list(monthly_items_sold_by_category.columns.values)\ncategories_ex_vegetables.remove('vegetables')\n\n# Plotting the subplots\nfor row in range(9):\n    for col in range(4):\n        try:\n            cat = categories_ex_vegetables[i]\n            i += 1\n\n            x = monthly_items_sold_by_category.vegetables.values\n            y = monthly_items_sold_by_category[cat].values\n            sns.regplot(x=x, y=y, ax=axs[row][col])\n            axs[row][col].title.set_text(cat)\n            \n            if col == 0:\n                axs[row][col].set_ylabel('# of Items Sold')\n                \n            if row == 3:\n                axs[row][col].set_xlabel('# of Grocery Items Sold')            \n        except IndexError:\n            # Done\n            break","7eb463b1":"fig = plt.figure(figsize=(12, 9))\n\nproduct_categories = list(monthly_items_sold_by_category.columns.values)\nsns.heatmap(monthly_items_sold_by_category[list(top_n_cat_by_contrib_idx(10))].corr(), \n            cmap=sns.color_palette(\"RdBu_r\",50)[::-1], \n            annot=True)\n\nplt.xticks(rotation=45)\nplt.show()","fcd8dfe2":"# Making the sales correlation matrix between groceries and other categories\ncorr_df = pd.DataFrame(np.corrcoef(monthly_items_sold_by_category.values, rowvar=False), columns = monthly_items_sold_by_category.columns)\ncorr_df.index = monthly_items_sold_by_category.columns\ncorr_df = corr_df[['vegetables']].sort_values('vegetables', ascending=False)\n\ncorr_df.head()","74d9f74b":"# Aggregate dollar contribution by category\ncontribution_by_category = monthly[['group','agg_dollar_contribution']].groupby(by='group').sum()\ncontribution_by_category = contribution_by_category.sort_values(by='agg_dollar_contribution', ascending=False)\ncontribution_by_category = contribution_by_category.join(corr_df)\ncontribution_by_category = contribution_by_category.rename(columns={'vegetables':'corr_w_vegetables'})\n\n# Maximum estimated financial impact by counting everything correlated with grocery sales as sales lift\ncontribution_by_category['max_est_fin_impact'] = contribution_by_category['agg_dollar_contribution'] * contribution_by_category['corr_w_vegetables']\n\n# Measuring the category's importance by calculating its share of total store's contribution pool\ncontribution_by_category['cat_total_contrib_share'] =  contribution_by_category['agg_dollar_contribution'] \/ contribution_by_category['agg_dollar_contribution'].sum()\n\n# Contribution counted as sales lift as a percentage of total store's contribution\ncontribution_by_category['lift_contrib_share'] =  contribution_by_category['max_est_fin_impact'] \/ contribution_by_category['agg_dollar_contribution'].sum()\n\nmax_lift = contribution_by_category.loc[contribution_by_category['lift_contrib_share'] > 0, 'lift_contrib_share'].sum()\n\nprint(\"Max estimated lift is {0:.1f}% counting fresh produce sales, and {1:.1f}% without.\".format(max_lift * 100, (max_lift - .1) * 100))","0b020c77":"# Looking at the dataframe containing sales lift data\ncontribution_by_category.sort_values('max_est_fin_impact', ascending=False).head()","410b9a1f":"# Grabbing data for skus sold and revenue per month\nmonthly_aggregate_skus_sold = cat_by_month('quantity').sum(axis=0)\nmonthly_aggregate_revenue = cat_by_month('agg_revenue').sum(axis=0)\nmonthly_aggregate_revenue.index = monthly_aggregate_skus_sold.index\n\n# plotting side by side\nrev_sku_fig = plt.figure(figsize=(12,6))\nax1 = rev_sku_fig.add_subplot(111)\nax1.plot(monthly_aggregate_skus_sold)\nax1.set_ylabel('# of Items Sold')\n\nax2 = ax1.twinx()\nax2.plot(monthly_aggregate_revenue, 'r-')\nax2.set_ylabel('Monthly Revenue (PLN)')\n\nax1.set_title(\"# of Items Sold and Monthly Revenue\")\n\nax1.legend(['Items Sold'])\nax2.legend(['Monthly Rev'],loc=2)\n\nfor tl in ax2.get_yticklabels():\n    tl.set_color('r')","239087d7":"# Products, a df conaining SKU, product group and product name\nproducts = monthly[['group','sku','name']].drop_duplicates().set_index('sku')\nproducts.head()","f935741c":"monthly_sku_revenue = monthly[['date','sku','agg_revenue']].pivot_table('agg_revenue', index='sku', columns='date')\nmonthly_sku_revenue = monthly_sku_revenue.fillna(0)\nmonthly_sku_revenue.columns = abr_month\nmonthly_sku_revenue['annual_total'] = monthly_sku_revenue.sum(axis=1)\nmonthly_sku_revenue = monthly_sku_revenue.sort_values('annual_total', ascending=False)\n\n# Calculating each SKU's share of overall revenue\nproduct_performance = monthly_sku_revenue.join(products)\nproduct_performance['rev_share'] = product_performance['annual_total'] \/ product_performance['annual_total'].sum()","b55767c7":"# Sort by most performent SKU based on annual contribution and calculating cumulative revenue\nproduct_performance['cumulative_rev'] = product_performance['rev_share']\n\ncumulative_revenue_share = 0.0\nfor idx, row in product_performance.iterrows():\n    product_performance.loc[idx,'cumulative_rev'] = cumulative_revenue_share + row['rev_share']\n    cumulative_revenue_share += row['rev_share']\n    \n# Making a pricing table to hold product pricing information\npricing = monthly[['sku','unit_cogs','unit_revenue']]\n# In case there are multiple prices for a single SKU, we'll take the mean\npricing = pricing.groupby('sku').mean()\n\n# Joining dfs together\nproduct_performance = product_performance.join(pricing)\nprint(f'there are {product_performance.shape[0]} SKUs sold by the store within the last yr, \\\nbut many also has not made a sale in months')","8c64586c":"product_performance.head()","018cf1d2":"# Plotting Cumulative Revenue\ncumulative_revenue = pd.DataFrame(product_performance['cumulative_rev'].values)\n\nplt.figure(figsize=(9,5))\nplt.plot(cumulative_revenue.values)\nplt.title('Cumulative Revenue by nth Top SKUs')\nplt.xlabel(\"n'th SKUs\")\nplt.ylabel(\"Cumulative Revenue as % of Total\");","a45e245d":"def get_sku_status(df, n_months, active=True):\n    \"\"\"Returns the index of active or inactive SKUs. A inactive SKU is one that has not made a\n    sale in the last n_months.\n    \n    Args:\n        df: df\n            product dataframe containing product sku, cumulative revenue, unit cogs, and unit revenue\n        \n        n_months: int\n            A SKU that has not made a sale in n_months will be marked as inactive\n            \n        active: bool\n            Returns active SKU indexes when set to True, returns inactive SKU indexes when False\n            \n    \n    Returns:\n        idx: pd.Int64Index\n            Pandas Row Index of active or inactive SKUs\n    \"\"\"\n    start_month = abr_month[-n_months]\n    df['rev_last_n_months'] = df.loc[:, start_month:\"dec\"].sum(axis=1)\n    \n    if active:\n        return df[df['rev_last_n_months'] > 0].index\n    \n    else:\n        return df[df['rev_last_n_months'] <= 0].index\n    \n\n\ndef working_capital_used_by_underperfoming_products(df, rev_to_keep, liquidation_discount, inventory_per_sku):\n    \"\"\"Calculate the amount of working capital used by non-performent SKUs, and the amount of capital\n    that can be released from liduidating these non-performent SKUs. You can specify the amount of\n    revenues you wish to retain. (IE, find the long tail SKUs that constitues the latst 10% of revenue)\n    \n    Args:\n        df: df\n            product dataframe containing product sku, culative revenue, unit cogs, and unit revenue\n            \n        rev_to_keep: float \n            Amount of revenue to *keep*, ranges from 0 to 1, 1 being keeping all revenue and not \n            dropping any skus. Similarily, 0 being keeping no revenue and dropping all SKUs\n        \n        liquidation_discount: float\n            A discount (0 to 1) applied to retail price to clear inventory. It is used to calcualte\n            amount of capital that can be freed up via inventory liquidation\n    \n    Returns:\n        working_cap_invested: float\n            Amount of working capital invested in non-performent SKUs\n        \n        working_cap_freed: float\n            Amount of working capital to be freed by liquidating stock at liquidation discount\n            \n        num_skus_dropped: int\n            Number of SKUs dropped\n            \n        idx_skus_dropped: df.Int64Index\n            dataframe index of SKUs that will be dropped\n            \n    \"\"\"    \n    # Get a list of active SKUs (SKU that made sales in last 3 months)\n    active_skus_idx = get_sku_status(df, 3, active=True)\n    df = df.loc[active_skus_idx,:].copy()\n    \n    active_skus = df.shape[0]\n    \n    # Calculating working capital and liquidation value\n    df['working_cap'] = df['unit_cogs'] * inventory_per_sku\n    df['liquidation_cf'] = df['unit_revenue'] * liquidation_discount * inventory_per_sku\n    \n    # return totals for given revene share to keep\n    df_to_liquidate = df[df['cumulative_rev'] >= rev_to_keep]\n    total_wc = df_to_liquidate['working_cap'].sum()\n    total_liquidation_cf = df_to_liquidate['liquidation_cf'].sum()\n    \n    num_skus_dropped = df_to_liquidate.shape[0]\n    num_skus_dropped_frac = num_skus_dropped \/ active_skus * 100\n    \n    idx_skus_dropped = df_to_liquidate.index\n    \n    print(\"Total working capital invested in SKUs to be liquidated is ${0:.0f} PLN\".format(total_wc))\n    print(\"Total amount of cash generated from liquidating underperforming SKUs is ${0:.0f} PLN\".format(total_liquidation_cf))\n    print(\"Total number of active SKUs to be dropped from store is {0:.0f}, representing {1:.0f}% of your SKUs\".format(num_skus_dropped, num_skus_dropped_frac))\n    \n    return total_wc, total_liquidation_cf, num_skus_dropped, idx_skus_dropped","f5d7079c":"_, _, _, idx_skus_dropped = working_capital_used_by_underperfoming_products(product_performance, .9, 0.35, 5);","41a87f88":"def explore_dropped_sku_categories(product_performance, idx_skus_dropped, n_months):\n    \"\"\"\n    Calculates number of SKUs to be dropped in each product category and compare it to the total\n    number of SKUs in that produce category\n    \n    Args:\n        product_performance: df\n            product dataframe containing product sku, culative revenue, unit cogs, and unit revenue\n            \n        idx_skus_dropped: pd.Int64Index\n            index conaining skus to be dropped\n            \n        n_months: int\n            Threshold in months used in marking a item as inactive. Every SKU that has made \n            a sale in the last n_months are considered active, and as such, is assumed to be\n            currently in inventory. Active SKUs are counted towards category SKU total. Inactive\n            SKUs are not.\n            \n    \"\"\"\n    skus_dropped = product_performance.loc[idx_skus_dropped,:].groupby('group').count()\n    skus_dropped = pd.DataFrame(skus_dropped.name.sort_values(ascending=False))\n    skus_dropped.columns = ['skus_dropped']\n    \n    active_skus_idx = get_sku_status(product_performance, 3, active=True)\n    active_skus = product_performance.loc[active_skus_idx,:]\n\n    # Count the categories\n    active_skus_per_category = active_skus.groupby('group').count()\n    active_skus_per_category = pd.DataFrame(active_skus_per_category.name)\n    active_skus_per_category.columns = ['cat_total']\n\n    # Join the dfs and normalize\n    df = skus_dropped.join(active_skus_per_category)\n    df['frac_dropped'] = df.skus_dropped \/ df.cat_total\n    \n    return df","7feb7868":"explore_dropped_sku_categories(product_performance, idx_skus_dropped, 3).sort_values('frac_dropped', ascending=False)","1c9d1cf1":"store_by_month = monthly[['date','quantity','agg_rev', 'agg_dollar_contribution']].groupby('date').sum()\nstore_by_month['gross_margin'] = store_by_month['agg_dollar_contribution'] \/ store_by_month['agg_rev']\nstore_by_month.head()","7c66923e":"# Loser look at margins\nstore_by_month['gross_margin'].plot()\nplt.ylabel('Margin')\nplt.title(\"Store Wide Margin by Month\");","18ca6a92":"# Contribution magin for each category, broek down by months\n# Contribution margin = 1 - aggregate_cogs \/ aggregate_revenue\ncategory_contribution_margin_by_month = 1 - cat_by_month('agg_cogs') \/ cat_by_month('agg_revenue')\n\n# Store baseline performance\nstore_baseline_margin = 1 - cat_by_month('agg_cogs').sum() \/ cat_by_month('agg_revenue').sum()\n\ncategory_contribution_margin_by_month.head()","ebe4171a":"# We want to subtract category growth by baseline store level performance to see which category is \n# Over performing, and which category is underperforming relative to baseline.\n\n# Pandas doesn't support broadcasting so we'll make the df ourselves\nbase_row = pd.DataFrame(store_baseline_margin).T\nto_append = []\nfor idx in list(category_contribution_margin_by_month.index):\n    to_append.append(base_row.rename({0:idx}))\n\nbroadcasted_baseline_margin = pd.concat(to_append)","21e3dc79":"# Month over month margin change\ncategory_margin_performance = get_mom_growth(category_contribution_margin_by_month)\ncategory_margin_performance = category_margin_performance.sort_values('jun', ascending=False).loc[top_n_cat_by_contrib_idx(8),:]\n\n# Month over month margin change over baseline (store wide m\/m margin change)\ncategory_margin_performance_over_baseline = get_mom_growth(category_contribution_margin_by_month) - get_mom_growth(broadcasted_baseline_margin)\ncategory_margin_performance_over_baseline = category_margin_performance_over_baseline.sort_values('jun', ascending=False).loc[top_n_cat_by_contrib_idx(7),:]","8f00d5c0":"# Using 3 month moving average to smooth out the volatility a bit more and make the trend more appearant\n\nfig, ax = plt.subplots(1, figsize=(12,6))\nma(category_margin_performance_over_baseline, 3).T.plot(ax=ax)\nax.set_title(\"M\/M Category Contribution Margin Change, 3 Month Moving Average\")\nax.grid(axis='y')\nax.set_xticklabels(category_margin_performance.T.index[1:]);","f7ab209e":"# Seems like the positive margin expansion has been casued by the margin expansion of fresh produce\n# Explore more by looking at volume change\ncategory_quantity_performance = get_mom_growth(cat_by_month('quantity')).loc[top_n_cat_by_contrib_idx(8),:]\n\nfig, ax = plt.subplots(1, figsize=(12,6))\nma(category_quantity_performance, 3).T.plot(ax=ax)\nax.set_title(\"M\/M Category Volume Change, 3 Month Moving Average\")\nax.grid(axis='y')\nax.set_xticklabels(category_quantity_performance.T.index[1:]);","bb606e8c":"def get_best_product_for_quarter(q, top_n=200):\n    \"\"\"Get a list of best performing products for a given quarter\n    \n    Args:\n        q: int\n            Quarter of the year, 1 to 4\n            1: Start of Jan - end of Mar\n            2: Start of Apr - end of Jun\n            3: Start of Jul - end of Sept\n            4: Start of Oct - end of Dec\n        \n        top_n: int\n            Get up to top nth SKU\n    \n    Returns:\n        df: DataFrame\n            df containg best performing SKUs\n    \"\"\"\n    df = product_performance.copy()\n    df['q1_contrib'] = df.loc[:,'jan':'mar'].sum(axis=1)\n    df['q2_contrib'] = df.loc[:,'apr':'jun'].sum(axis=1)\n    df['q3_contrib'] = df.loc[:,'jul':'sep'].sum(axis=1)\n    df['q4_contrib'] = df.loc[:,'oct':'dec'].sum(axis=1)\n    \n    quarters = {1:'q1_contrib', 2:'q2_contrib', 3:'q3_contrib', 4:'q4_contrib'}\n    \n    quarter = quarters[q]\n    \n    return df.sort_values(quarter, ascending=False).iloc[:top_n,:]","80148202":"get_best_product_for_quarter(1).head()","8aa22bea":"# What categories do top SKUs in Q1 belong to?\n# If you do this for all 4 quarters you will see these rankings stays relatively constant, \n# meaning all categories exhibit similar seasonality trends\nget_best_product_for_quarter(1).group.value_counts().head(7)","6debee47":"# Getting a list of products that have significant sales. This will make sure we have meaningful month over month\n# growth metrhics. We'll use 1500 PLN annual sales as the bar\n\ndef get_highest_growth_skus(top_n, min_annual_pln, month):\n    \"\"\"Returns a dataframe with highest growth skus\n    \n    Args:\n        top_n: int\n            Get top n high growth skus\n            \n        min_annual_pln: int\n            Minimum annual sales in PLN the sku needs to make in order for it to be considered as a potential\n            high growth candidate\n            \n        month: str\n            Abbrevated month to rank the high growth skus\n            \n    Returns:\n        df: DataFrame\n            df containing high growth skus sorted by the month of choice, values in df are 2 month \n            moving average of dollar contribution growth\n    \n    \"\"\"\n    significant_skus_idx = product_performance[product_performance.annual_total > min_annual_pln].index\n\n    # High Growth Products\n    high_growth_skus = ma(get_mom_growth(product_performance.copy().loc[:,'jan':'dec']), 2)\n\n    high_growth_skus = high_growth_skus.loc[significant_skus_idx,:].sort_values(month, ascending=False)\n    \n    high_growth_skus = high_growth_skus.join(product_performance[['group','name']])\n    \n    return high_growth_skus","35dd5258":"# Getting the highest growth SKUs for jun, next year this time you can promote these a bit more, put them on\n# better shelf locations etc...\n\nget_highest_growth_skus(200, 1500, 'jun').head()","43ca7e6a":"### Visualizing product categories and their dollar contribution","1c380422":"# SKU Analysis\n\nGoal:\n- Find the best performing and upcoming product SKUs and use them to drive cash flow generation and foot traffic to the store.\n\n### Anchor Products \/ Highest Performing SKUs\nAnchor products are high performence products that generates the bulk of store's free cash flow. Make sure these products are displayed in the middle of the isle, where people are most likely to notice and buy them. Also, spread them around the store to drive foot traffic evenly throughout the store, with the goal of getting the customer to take a whole tour of the store before leaving. \n\nResults:\n- See output below\n\n### High Growth Products\nResults:\n- See output below","f195cdcc":"### Estimating Grocery Sales Lift\nThe ceiling sales lift is calculaed by multiplying the r^2 between fresh produce and the other category by the total dollar contribution of the other category. That is, we will attribute all correlated sales as produce's sales lift. Realistically sales lift should be much lower than this ceiling figure.","42f175c5":"Notice unit_cogs are strings, lets convert it to float","89014661":"## Sales Trends, Best Performing Categories & SKUs","7cac7921":"#### Making a Product Table","ccf467f7":"### Summarize numerical and categorical variables","5950699d":"### Check N\/A and duplicates","486bcbd7":"### Understanding Sales Trend\n\nGoal:\n- Understand the reasons behind the current sales trend\n- Figure out which categories are well positioned for growth, and which look the least favorable\n\nImplication\n- In the winter months you can exploit the fact that people don't want to hop into the car and go to the large supermarkets, and increase price on staple categories such as fresh produce, cheese, and bread.\n- Generally speaking you should increase the margins on produce, the data suggests it has low price elasticity.\n- Consider expanding drink and beer selection during the summer months, and perhaps run more promotions during winter months to counter cyclicality\n\nHow:\n- By examing the changes in the following statistics for each product category. We'll use median for all statistics because this dataset has many outliers\n    - Median selling price (are customers switching to lower end products?)\n    - median contribution margin (absorbing the cost increase? Change in pricing power?)\n    - Total volume (Are people trading up\/down or not buying at all?)\n    \n    \nResults:\n#### First look at store level performance\n\n- There is significant margin expansion in the second half of the year, thanks to\n    - Fresh produce\n    - Cheese\n    \n    \n- Margin has been weighted down by\n    - Bread\n\n#### Category Specific Trends\n**Revenue Trends**\n- **Fresh produce has relative low price elasticity** so the data suggest you do have some pricing power here, likely due to the store's convinence factor. You can explore upping your margins while closely monitoring sales volume.\n- All categories are cyclical\n- Drop in revenue towards the second half of the year was largely due to slowing sales of drink_juice and beer.\n- This dataset exhibits strong cyclicality trends in your significant categories. Beer ad drink_juice's seasonality trends are dragging down store's revenue generation in colder months.\n\n**Product Rotation**\n- It is difficult to advise on product rotation when all the categories exhibit the same seasonality trends, that is, universaly lower sales during winter months. As such, it is critical that the new market positioning incorporates products that sells well during winter months. Alternatively, you can have a strong and differentiated positioning selling a consumer staple that shows strong demand throughout the year.\n\n**Seasonal Product Categories**\n\nsorted most seasonal to lease\n- Drinks and Juice\n- Beer\n- Sweets\n- Cigarettes\n- Diary and Cheese\n- Fresh produce\n- Vodka & Spirit\n- Bread\n\n","273f95cc":"## Understanding Fresh Produce Business Dynamics\n\nMotivation:\n- Estimate the financial value of keeping the fresh produce section\n\nLimitations & Assumptions:\n- Due to the limitations of the dataset; specifically, the lack of receipt level data, it is difficult to accurately estimate how much sales lift fresh produce crates.\n- Equally unfortunate, we don't have financial statements for the store, and therefore we can't know for sure if produce is a loss leader, but it most likely is due to industry competitive dynamics and the store profile.\n\nResult:\n- Not strongly correlated with any major categories\n- Maximum sales lift ~25% of total sales, ~ 15% is probably realistic since grocery is likely a loss leader and not all correlated sales are attributable to groceries.\n- Will build a new attribution model when receipt level data becomes available\n\nHow:\n- The method we use will set a upper limit on Produce's sales lift, by counting anything that is positively correlated with fresh produce sales.\n\n\n### Visualizing Grocery Sales Correlation \/w Other Categories\n\nWe will only visualize the top 17 categories since they account more than ~90% of all sales","8c4d87cd":"#### Cumulative Revenue by nth Top Performing SKUs","2152c2de":"### Identify numerical and categorical variables","54a4b93c":"### Again with a heatmap","5f6f3e58":"### Defining Some Helper Functions","acf79fd8":"# Business Problem Definition\n\nThis EDA notebook examines financial and operational performance of the grocer from historical montly sales data. \n\nProducts and product categories are examined and operation changes are recommended based on historical data and industry competative dynamics. \n\nFor a complete write up, including the analysis of strategic alternatives and specific recommendations, please see the **PPT** presentation. \n\nFor complete PPT presentation please see https:\/\/www.dropbox.com\/s\/ezenffm4bbrutar\/grocer.pdf?dl=0\n\n\n### Business Objectives\n#### Short term objectives - focus on business understanding\n- Time frame < 1 year\n- Make mom & pop management comprehend the competitive landscape\n- Communicate insights from sales data and adopt industry best practice in sourcing, inventory management etc\u2026\n- Find defensible market positioning amid worsening competitive environment\n\n#### Medium term objectives - business transformation\n- Time frame 1 - 2 years\n- Execute on repositioning\n- Adopt industry best practices\n- Use data as competitive advantage\n\n\n### Summary of Recommendations (Taken from PPT)\n\n<img src=\"https:\/\/imgur.com\/RVs4W8R.jpg\" alt=\"Drawing\" style=\"width: 700px;\"\/>","262a7f5f":"## Finding Under-Performing Categories\n\n### How many SKUs can we drop and still keep 90% of our revenue? How much cash can we raise by selling stock?\n\nResult:\n- We can drop 60% of our total SKUs and still keep 90% of our total revenue\n- Generate $14,000 PLN via stock liqudiation\n- Cash and freed up space can be used to upgrade\/renovate store\n\n\n- **Biggest non-performing categories are**\n    - Household chemicals (chemistry)\n    - General\n    - Wine\n    - Sweets\n    - Bulk cookies\n  \n  \n- ** Categories that has expansion potential **\n    - Bread\n    - Vodka \/ Spirits\n    - Beer\n    - Cigarettes\n\nImplication:\n- Lots of excess shelf space in the store, which gives us the opportunity to dramatically adjust the store layout, product display, and to introduce new product categories etc...\n\nAssumptions:\n- 35% discount on retail when liquidating stock\n- 5 items in stock per SKU","2abf216b":"#### Finding Growth Categories and Products\nYou can then use this to find trending product and better match consumer demand with stock levels","9d9d9b5e":"#### Margin Analysis by Category","f2533737":"#### Dropping Underperforming SKUs & Assess Financial Impact\n#### 90% of Store's Revenue is made by ~ 40% of SKUs, 50% by ~10% of SKUs\n- There are about 4k active SKUs\n- This means we can strip many underperforming products and do something else with the space we have. We will also be able to raise some capital through the sale of inventory","7f6da85d":"### Examine the data","a5aeeb22":"### Load the Data & Cleaning\n\n**Data Files**\n\nmonthly_sale_by_sku.csv\n\n    Monthly sales data with SKU number, products groups, names, amount sold, cost of goods sold, and contribution margin. \n    Data from year 2018. Products group in English, products names in Polish.\n    \n    Original file name from Kaggle is Sale1.csv. File is renamed but data is not altered.\n\nSetting up the dataframe containing monthly sales data, including sku, cost of goods sold (COGS), contribution margin etc...","88f5f211":"### Use .info() to see lengths and dtypes","90d3e73f":"#### Product Rotation: Getting the best performing SKU for each quarter of the year\nYou can then use this to advise on product rotation throughout the year","901f8d60":"#### SKUs sold Vs. Revenue\n\nResults:\n- Total revenue highly correlated with number of items sold, with the exception of December. Consumers were trading up during December and not other time of year. This could be due to the Christmas Holidays."}}