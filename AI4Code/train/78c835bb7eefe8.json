{"cell_type":{"f7e73c9e":"code","6130e51f":"code","0e751626":"code","a7936a88":"code","a6e37274":"code","32424f01":"code","1e56328e":"code","3dbf9fe2":"code","83e3a132":"code","7bb90240":"code","0f902b9c":"code","af2f885a":"code","982cc810":"code","2b6b013c":"code","dd6fe57d":"code","738a7897":"code","c3273a5b":"code","f9f42699":"code","48a2a0e6":"code","10f9ce56":"code","3119f94a":"code","4cbf462f":"code","5f1bdbea":"code","44f28272":"code","16bf28ec":"code","b867f240":"code","5344ab0c":"code","5fe8b2d1":"code","86d524be":"code","aa3c198f":"code","1283d379":"code","0ce78606":"code","898c22dd":"code","b5af882b":"code","340048fb":"code","77970653":"code","3bbdf7d8":"code","fb3b68f4":"code","a3ea2e74":"code","95121673":"code","f6617f67":"code","382c84be":"code","1a4506ea":"code","a3e39256":"markdown","fd0e7f4f":"markdown","e37fd101":"markdown","710ba209":"markdown","563573f6":"markdown","5fedad86":"markdown"},"source":{"f7e73c9e":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom keras.layers import Dropout\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport cv2\nfrom glob import glob\nimport seaborn as sns\nsns.set()\nimport keras\nimport sklearn\nimport skimage\nfrom skimage.transform import resize\nimport time\nimport random\nfrom skimage.color import rgb2gray\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import accuracy_score\n","6130e51f":"train = \"..\/input\/autistic-children-data-set-traintestvalidate\/train\"\n\ntest = \"..\/input\/autistic-children-data-set-traintestvalidate\/test\"\n\nval = '..\/input\/autistic-children-data-set-traintestvalidate\/valid'\n\n\n\nLOAD_FROM_IMAGES = True\n\ndef get_data(folder):\n    x = []\n    y = []\n    for folderName in os.listdir(folder):\n        if not folderName.startswith(\".\"):\n            if folderName in [\"non_autistic\"]:\n                label = 0\n            elif folderName in [\"autistic\"]:\n                label = 1\n            else:\n                label = 2\n            for image_filename in tqdm(os.listdir(folder +\"\/\" +folderName+\"\/\")):\n                img_file = cv2.imread(folder + \"\/\" +folderName + \"\/\" + image_filename)\n                if img_file is not None:\n                    img_file = skimage.transform.resize(img_file,(128,128,3), mode = \"constant\",anti_aliasing=True)\n                    #img_file = rgb2gray(img_file)\n                    img_arr = np.asarray(img_file)\n                    x.append(img_arr)\n                    y.append(label)\n    x = np.asarray(x)\n    y = np.asarray(y)\n    return x,y\n\nif LOAD_FROM_IMAGES:\n    X_train,y_train = get_data(train)\n    X_test, y_test = get_data(test)\n    X_val,y_val = get_data(val)\n    \n    np.save(\"xtrain.npy\",X_train)\n    np.save(\"ytrain.npy\",y_train)\n    np.save(\"xtest.npy\",X_test)\n    np.save(\"ytest.npy\",y_test)\n    np.save(\"xval.npy\",X_val)\n    np.save(\"yval.npy\",y_val)\nelse:\n    X_train = np.load(\"xtrain.npy\")\n    y_train = np.load(\"ytrain.npy\")\n    X_test = np.load(\"xtest.npy\")\n    y_test = np.load(\"ytest.npy\")\n    X_val = np.load(\"xval.npy\")\n    y_val = np.load(\"yval.npy\")\n    ","0e751626":"def plot_histogram(a):\n    plt.figure(figsize=(12,6))\n    plt.subplot(1,2,1)\n    plt.hist(a.ravel(),bins=255)\n    plt.subplot(1,2,2)\n    plt.imshow(a,vmin=0,vmax=1)\n    plt.show()\n    \n    \nplot_histogram(X_train[1])","a7936a88":"plot_histogram(X_test[1])","a6e37274":"plot_histogram(X_val[1])","32424f01":"glob_img = glob(\"..\/input\/autistic-children-data-set-traintestvalidate\/train\/autistic\/**\")\n\ndef plot(images):\n    z = random.sample(images,3)\n    plt.figure(figsize=(20,20))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(z[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(z[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(z[2]))\n\nplot(glob_img)","1e56328e":"print(\"Autism\")\n\nglob_img = glob(\"..\/input\/autistic-children-data-set-traintestvalidate\/train\/autistic\/**\")\ni_=0\nplt.rcParams[\"figure.figsize\"] =(20.0,20.0)\nplt.subplots_adjust(wspace=0,hspace=1)\nfor i in  glob_img[0:20]:\n    img=cv2.imread(i)\n    img=cv2.resize(img,(128,128))\n    plt.subplot(5,5,i_+1)\n    plt.imshow(img);plt.axis(\"off\")\n    i_ +=1","3dbf9fe2":"glob_img = glob(\"..\/input\/autistic-children-data-set-traintestvalidate\/train\/non_autistic\/**\")\n\ndef plot(images):\n    z = random.sample(images,3)\n    plt.figure(figsize=(20,20))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(z[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(z[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(z[2]))\n\nplot(glob_img)","83e3a132":"print(\"No Autism\")\n\nglob_img = glob(\"..\/input\/autistic-children-data-set-traintestvalidate\/train\/non_autistic\/**\")\ni_=0\nplt.rcParams[\"figure.figsize\"] =(20.0,20.0)\nplt.subplots_adjust(wspace=0,hspace=1)\nfor i in  glob_img[0:20]:\n    img=cv2.imread(i)\n    img=cv2.resize(img,(128,128))\n    plt.subplot(5,5,i_+1)\n    plt.imshow(img);plt.axis(\"off\")\n    i_ +=1","7bb90240":"plt.figure(figsize=(8,4))\n\nmap_characters = {0:\"non_autistic\",1:\"autistic\"}\ndict_characters = map_characters\n\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df[\"labels\"]\ndist=lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)","0f902b9c":"model = models.Sequential()\n\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\",input_shape=(128,128,3)))\nmodel.add(layers.MaxPooling2D((2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2,2)))\n\nmodel.add(layers.Conv2D(64,(3,3),activation=\"relu\"))\nmodel.add(layers.MaxPooling2D((2,2)))\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(128,activation=\"relu\"))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(layers.Dense(2,activation=\"softmax\"))","af2f885a":"model.summary()","982cc810":"model.compile(optimizer = \"adam\" , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","2b6b013c":"batch_size=8\nepochs=200","dd6fe57d":"from tensorflow.keras.callbacks import EarlyStopping\n\ncustom_early_stopping = EarlyStopping(\n    monitor='accuracy', \n    patience=50,\n    restore_best_weights=True\n)","738a7897":"history =  model.fit(X_train, y_train, batch_size=batch_size,epochs=epochs,\n                              verbose=1,\n                              validation_data =(X_val,y_val),callbacks=[custom_early_stopping])","c3273a5b":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","f9f42699":"y_test_pred = model.predict(X_test)","48a2a0e6":"y_pred = (y_test_pred > 0.5)\ny_pred=np.argmax(y_pred, axis=1)","10f9ce56":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","3119f94a":"print(classification_report(y_test, y_pred)) ","4cbf462f":"def confusion(y_test,y_test_pred,X):\n    names=['non_autistic','autistic']\n    cm=confusion_matrix(y_test,y_test_pred)\n    f,ax=plt.subplots(figsize=(10,10))\n    sns.heatmap(cm,annot=True,linewidth=.5,linecolor=\"r\",fmt=\".0f\",ax=ax)\n    plt.title(X, size = 25)\n    plt.xlabel(\"y_pred\")\n    plt.ylabel(\"y_true\")\n    ax.set_xticklabels(names)\n    ax.set_yticklabels(names)\n    plt.show()\n\n    return","5f1bdbea":"confusion(y_test,y_pred,\"CNN\")","44f28272":"class LRA(keras.callbacks.Callback):\n    best_weights=model.get_weights() # set a class vaiable so weights can be loaded after training is completed\n    def __init__(self, patience=2, threshold=.95, factor=.5):\n        super(LRA, self).__init__()\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        self.count=0\n        msg='\\n Starting Training - Initializing Custom Callback'\n        print_in_color (msg, (244, 252, 3), (55,65,80))\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy        \n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            if acc>self.highest_tracc: # training accuracy improved in the epoch\n                msg= f'\\n training accuracy improved from  {self.highest_tracc:7.2f} to {acc:7.2f} learning rate held at {lr:9.6f}'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=model.get_weights() # traing accuracy improved so save the weights\n                count=0 # set count to 0 since training accuracy improved\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss                    \n            else:  # training accuracy did not improve check if this has happened for patience number of epochs if so adjust learning rate\n                if self.count>=self.patience -1:\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    if v_loss<self.lowest_vloss:\n                        self.lowest_vloss=v_loss\n                    msg=f'\\nfor epoch {epoch +1} training accuracy did not improve for {self.patience } consecutive epochs, learning rate adjusted to {lr:9.6f}'\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                else:\n                    self.count=self.count +1\n                    msg=f'\\nfor  epoch {epoch +1} training accuracy did not improve, patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))\n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            if v_loss< self.lowest_vloss: # check if the validation loss improved\n                msg=f'\\n for epoch {epoch+1} validation loss improved from  {self.lowest_vloss:7.4f} to {v_loss:7.4}, saving best weights'\n                print_in_color(msg, (0,255,0), (55,65,80))\n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved               \n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    self.lr=self.lr * self.factor\n                    msg=f' \\nfor epoch {epoch+1} validation loss failed to improve for {self.patience} consecutive epochs, learning rate adjusted to {self.lr:9.6f}'\n                    self.count=0 # reset counter\n                    print_in_color(msg, (255,0,0), (55,65,80))\n                    tf.keras.backend.set_value(model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                else: \n                    self.count =self.count +1 # increment the count\n                    msg=f' \\nfor epoch {epoch+1} validation loss did not improve patience count incremented to {self.count}'\n                    print_in_color(msg, (255,255,0), (55,65,80))","16bf28ec":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat))\n    print('\\33[0m') # returns default print color to back to black\n    return","b867f240":"epochs=100","5344ab0c":"callbacks=[LRA()]\nmodel.fit(X_train, y_train,epochs=epochs,\n                              verbose=1,\n                              validation_data =(X_val,y_val),callbacks=callbacks,shuffle=True)","5fe8b2d1":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","86d524be":"y_test_pred = model.predict(X_test)","aa3c198f":"y_pred_lra = (y_test_pred > 0.5)\ny_pred_lra=np.argmax(y_pred_lra, axis=1)","1283d379":"print(classification_report(y_test, y_pred)) ","0ce78606":"confusion(y_test,y_pred_lra,\"CNN with LRA\")","898c22dd":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","b5af882b":"class LRA(keras.callbacks.Callback):\n    reset=False\n    count=0\n    stop_count=0\n    tepochs=0\n    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,batches, initial_epoch):\n        super(LRA, self).__init__()\n        self.model=model\n        self.patience=patience # specifies how many epochs without improvement before learning rate is adjusted\n        self.stop_patience=stop_patience\n        self.threshold=threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n        self.factor=factor # factor by which to reduce the learning rate\n        self.dwell=dwell\n        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initiallearning rate and save it in self.lr\n        self.highest_tracc=0.0 # set highest training accuracy to 0\n        self.lowest_vloss=np.inf # set lowest validation loss to infinity\n        #self.count=0 # initialize counter that counts epochs with no improvement\n        #self.stop_count=0 # initialize counter that counts how manytimes lr has been adjustd with no improvement  \n        self.initial_epoch=initial_epoch \n        self.batches=batches\n        #self.epochs=epochs\n        best_weights=self.model.get_weights() # set a class vaiable so weights can be loaded after training is completed        \n        msg=' '\n        if freeze==True:\n            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n        else:\n            msgs=f' Starting training using base model { model_name} training all layers '            \n        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n    def on_train_begin(self, logs=None):\n        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor', 'Duration', 'Batch')\n        print_in_color(msg, (244,252,3), (55,65,80)) \n        \n    def on_train_batch_begin(self, batch, logs=None):\n        msg='{0:83s}{1:4s}of {2:5s}'.format(' ', str(batch), str(self.batches))\n        print(msg, '\\r', end='') # prints over on the same line to show running batch count\n        \n        \n    def on_epoch_begin(self,epoch, logs=None):\n        self.now= time.time()\n        \n    def on_epoch_end(self, epoch, logs=None):  # method runs on the end of each epoch\n        later=time.time()\n        duration=later-self.now \n        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n        current_lr=lr\n        v_loss=logs.get('val_loss')  # get the validation loss for this epoch\n        acc=logs.get('accuracy')  # get training accuracy \n        v_acc=logs.get('val_accuracy')\n        loss=logs.get('loss')\n        #print ( '\\n',v_loss, self.lowest_vloss, acc, self.highest_tracc)\n        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n            monitor='accuracy'\n            if acc>self.highest_tracc: # training accuracy improved in the epoch                \n                self.highest_tracc=acc # set new highest training accuracy\n                LRA.best_weights=self.model.get_weights() # traing accuracy improved so save the weights\n                self.count=0 # set count to 0 since training accuracy improved\n                self.stop_count=0 # set stop counter to 0\n                if v_loss<self.lowest_vloss:\n                    self.lowest_vloss=v_loss\n                color= (0,255,0)\n                self.lr=lr\n            else: \n                # training accuracy did not improve check if this has happened for patience number of epochs\n                # if so adjust learning rate\n                if self.count>=self.patience -1:\n                    color=(245, 170, 66)\n                    self.lr= lr* self.factor # adjust the learning by factor\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    self.count=0 # reset the count to 0\n                    self.stop_count=self.stop_count + 1\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space                        \n                    else:\n                        if v_loss<self.lowest_vloss:\n                            self.lowest_vloss=v_loss                                    \n                else:\n                    self.count=self.count +1 # increment patience counter                    \n        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n            monitor='val_loss'\n            if v_loss< self.lowest_vloss: # check if the validation loss improved \n                self.lowest_vloss=v_loss # replace lowest validation loss with new validation loss                \n                LRA.best_weights=self.model.get_weights() # validation loss improved so save the weights\n                self.count=0 # reset count since validation loss improved  \n                self.stop_count=0  \n                color=(0,255,0)\n                self.lr=lr\n            else: # validation loss did not improve\n                if self.count>=self.patience-1:\n                    color=(245, 170, 66)\n                    self.lr=self.lr * self.factor # adjust the learning rate                    \n                    self.stop_count=self.stop_count + 1 # increment stop counter because lr was adjusted \n                    self.count=0 # reset counter\n                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) # set the learning rate in the optimizer\n                    if self.dwell:\n                        self.model.set_weights(LRA.best_weights) # return to better point in N space\n                else: \n                    self.count =self.count +1 # increment the patience counter                    \n                if acc>self.highest_tracc:\n                    self.highest_tracc= acc\n        msg=f'{str(epoch+1):^3s}\/{str(LRA.tepochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{self.lr:^9.5f}{monitor:^11s}{duration:^8.2f}'\n        print_in_color (msg,color, (55,65,80))\n        if self.stop_count> self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n            print_in_color(msg, (0,255,0), (55,65,80))\n            self.model.stop_training = True # stop training","340048fb":"epochs =50\npatience= 1 # number of epochs to wait to adjust lr if monitored value does not improve\nstop_patience =25 # number of epochs to wait before stopping training if monitored value does not improve\nthreshold=.9 # if train accuracy is < threshhold adjust monitor accuracy, else monitor validation loss\nfactor=.3 # factor to reduce lr by\ndwell=True # experimental, if True and monitored metric does not improve on current epoch set  modelweights back to weights of previous epoch\nfreeze=False # if true free weights of  the base model\nbatch_size=16\ncallbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n                   factor=factor,dwell=dwell, model_name=model, freeze=freeze, batches=batch_size,initial_epoch=0 )]\nLRA.tepochs=epochs  # used to determine value of last epoch for printing\n","77970653":"model.fit(X_train, y_train,epochs=epochs,\n                              verbose=0,\n                              validation_data =(X_val,y_val),callbacks=callbacks,shuffle=True)","3bbdf7d8":"score = model.evaluate(X_test, y_test, batch_size=batch_size, verbose=1)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","fb3b68f4":"y_test_pred = model.predict(X_test)","a3ea2e74":"y_pred_n_lra = (y_test_pred > 0.5)\ny_pred_n_lra=np.argmax(y_pred_n_lra, axis=1)","95121673":"print(classification_report(y_test, y_pred_n_lra)) ","f6617f67":"confusion(y_test,y_pred_n_lra,\"CNN with the New LRA\")","382c84be":"confusion(y_test,y_pred_lra,\"CNN with LRA\")","1a4506ea":"confusion(y_test,y_pred,\"CNN\")","a3e39256":"# Autism","fd0e7f4f":"# WIth LRA","e37fd101":"### Thanks @gpiosenka\n","710ba209":"# No Autism","563573f6":"## With the new LRA","5fedad86":"### Thanks @Shruti_Iyyer"}}