{"cell_type":{"3854685d":"code","1c168a96":"code","a0262ef1":"code","de276ba8":"code","023a2b5c":"code","9230afb2":"code","d05b302d":"code","06c97bdc":"code","8bde72cf":"code","b58303d5":"code","07cbd83b":"code","66e4bd0b":"code","d22d170c":"code","430be633":"code","901df072":"code","03391c81":"code","614dd592":"code","ce9edbea":"markdown","5e062b62":"markdown","2f050ff7":"markdown","198d68fb":"markdown","eb0a7b53":"markdown","4014420e":"markdown","d7a0808d":"markdown","b9c900ad":"markdown","829c10ce":"markdown","f41fa3c3":"markdown","65d32298":"markdown","1b394b91":"markdown","26852d5d":"markdown","923ec23a":"markdown"},"source":{"3854685d":"from sklearn.preprocessing import StandardScaler\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport os\nimport warnings\n\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))","1c168a96":"df = pd.read_csv('..\/input\/Mall_Customers.csv')\ndf.head()","a0262ef1":"df.info()","de276ba8":"df.rename(index=str, columns={'Annual Income (k$)': 'Income',\n                              'Spending Score (1-100)': 'Score'}, inplace=True)\ndf.head()","023a2b5c":"# Let's see our data in a detailed way with pairplot\nX = df.drop(['CustomerID', 'Gender'], axis=1)\nsns.pairplot(df.drop('CustomerID', axis=1), hue='Gender', aspect=1.5)\nplt.show()","9230afb2":"from sklearn.cluster import KMeans\n\nclusters = []\n\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(X)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(3, 140000), xytext=(3, 50000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","d05b302d":"# 3 cluster\nkm3 = KMeans(n_clusters=3).fit(X)\n\nX['Labels'] = km3.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 3))\nplt.title('KMeans with 3 Clusters')\nplt.show()","06c97bdc":"# Let's see with 5 Clusters\nkm5 = KMeans(n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('KMeans with 5 Clusters')\nplt.show()","8bde72cf":"fig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(121)\nsns.swarmplot(x='Labels', y='Income', data=X, ax=ax)\nax.set_title('Labels According to Annual Income')\n\nax = fig.add_subplot(122)\nsns.swarmplot(x='Labels', y='Score', data=X, ax=ax)\nax.set_title('Labels According to Scoring History')\n\nplt.show()","b58303d5":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(X)\n\nX['Labels'] = agglom.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nplt.title('Agglomerative with 5 Clusters')\nplt.show()","07cbd83b":"from scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \n\ndist = distance_matrix(X, X)\nprint(dist)","66e4bd0b":"Z = hierarchy.linkage(dist, 'complete')","d22d170c":"plt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size=12, orientation='right')","430be633":"Z = hierarchy.linkage(dist, 'average')\nplt.figure(figsize=(18, 50))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size =12, orientation = 'right')","901df072":"from sklearn.cluster import DBSCAN \n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\n\nX['Labels'] = db.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]))\nplt.title('DBSCAN with epsilon 11, min samples 6')\nplt.show()\n","03391c81":"from sklearn.cluster import MeanShift, estimate_bandwidth\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.1)\nms = MeanShift(bandwidth).fit(X)\n\nX['Labels'] = ms.labels_\nplt.figure(figsize=(12, 8))\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', np.unique(ms.labels_).shape[0]))\nplt.plot()\nplt.title('MeanShift')\nplt.show()","614dd592":"fig = plt.figure(figsize=(20,15))\n\n##### KMeans #####\nax = fig.add_subplot(221)\n\nkm5 = KMeans(n_clusters=5).fit(X)\nX['Labels'] = km5.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('KMeans with 5 Clusters')\n\n\n##### Agglomerative Clustering #####\nax = fig.add_subplot(222)\n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(X)\nX['Labels'] = agglom.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'],\n                palette=sns.color_palette('hls', 5), s=60, ax=ax)\nax.set_title('Agglomerative with 5 Clusters')\n\n\n##### DBSCAN #####\nax = fig.add_subplot(223)\n\ndb = DBSCAN(eps=11, min_samples=6).fit(X)\nX['Labels'] = db.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'], s=60,\n                palette=sns.color_palette('hls', np.unique(db.labels_).shape[0]), ax=ax)\nax.set_title('DBSCAN with epsilon 11, min samples 6')\n\n\n##### MEAN SHIFT #####\nax = fig.add_subplot(224)\n\nbandwidth = estimate_bandwidth(X, quantile=0.1)\nms = MeanShift(bandwidth).fit(X)\nX['Labels'] = ms.labels_\nsns.scatterplot(X['Income'], X['Score'], hue=X['Labels'], style=X['Labels'], s=60,\n                palette=sns.color_palette('hls', np.unique(ms.labels_).shape[0]), ax=ax)\nax.set_title('MeanShift')\n\nplt.tight_layout()\nplt.show()","ce9edbea":"## Wrap Up All in One Place\n\nLet's visualize all the algorithms we used so far and see their clustering distributions.","5e062b62":"## Density Based Clustering (DBSCAN)\n\nMost of the traditional clustering techniques, such as k-means, hierarchical and fuzzy clustering, can be used to group data without supervision. \n\nHowever, when applied to tasks with arbitrary shape clusters, or clusters within cluster, the traditional techniques might be unable to achieve good results. That is, elements in the same cluster might not share enough similarity or the performance may be poor.\nAdditionally, Density-based Clustering locates regions of high density that are separated from one another by regions of low density. Density, in this context, is defined as the number of points within a specified radius.\n\nIn this part, the main focus will be manipulating the data and properties of DBSCAN and observing the resulting clustering.\n\n### Modeling\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise. This technique is one of the most common clustering algorithms  which works based on density of object.\nThe whole idea is that if a particular point belongs to a cluster, it should be near to lots of other points in that cluster.\n\nIt works based on two parameters: Epsilon and Minimum Points  \n__Epsilon__ determine a specified radius that if includes enough number of points within, we call it dense area  \n__minimumSamples__ determine the minimum number of data points we want in a neighborhood to define a cluster.","2f050ff7":"Elbow method tells us to select the cluster when there is a significant change in inertia. As we can see from the graph, we can say this may be either 3 or 5. Let's see both results in graph and decide.\n\n###  Creating the Visual Plots","198d68fb":"# Clustering Algortihms\n\nIn Machine Learning, the types of Learning can broadly be classified into three types: \n\n1. Supervised Learning,  \n2. Unsupervised Learning and   \n3. Semi-supervised Learning.  \n\nAlgorithms belonging to the family of Unsupervised Learning have no variable to predict tied to the data. Instead of having an output, the data only has an input which would be multiple variables that describe the data. This is where clustering comes in.\n\nClustering is the task of grouping together a set of objects in a way that objects in the same cluster are more similar to each other than to objects in other clusters. Similarity is a metric that reflects the strength of relationship between two data objects. Clustering is mainly used for exploratory data mining. It has manifold usage in many fields such as machine learning, pattern recognition, image analysis, information retrieval, bio-informatics, data compression, and computer graphics.\n","eb0a7b53":"From the above plot we see that gender has no direct relation to segmenting customers. That's why we can drop it and move on with other features which is why we will X parameter from now on.","4014420e":"## Conclusions\n\nThe Scikit learn official website provides the math behind the algorithms and if you're not sure which algorithm you want to use check their usage [here](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html)\n\nDon't forget to upvote if you like my kernel :)","d7a0808d":"We can clearly see our clusters as we indicated before.\n\n## Hierarchical Clustering\n\n## Agglomerative\n\nWe will be looking at a clustering technique, which is <b>Agglomerative Hierarchical Clustering<\/b>. Agglomerative is the bottom up approach which is more popular than Divisive clustering. <br> <br>\nWe will also be using Complete Linkage as the Linkage Criteria. <br>\n\nThe <b> Agglomerative Clustering <\/b> class will require two inputs:\n<ul>\n    <li> <b>n_clusters<\/b>: The number of clusters to form as well as the number of centroids to generate. <\/li>\n    <li> <b>linkage<\/b>: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. <\/li>\n    <ul> \n        <li> Value will be: 'complete' <\/li> \n        <li> <b>Note<\/b>: It is recommended that try everything with 'average' as well <\/li>\n    <\/ul>\n<\/ul>","b9c900ad":"\n### Dendrogram Associated for the Agglomerative Hierarchical Clustering\nRemember that a <b>distance matrix<\/b> contains the <b> distance from each point to every other point of a dataset <\/b>. <br>\nWe can use the function <b> distance_matrix, <\/b> which requires <b>two inputs<\/b>. \nRemember that the distance values are symmetric, with a diagonal of 0's. This is one way of making sure your matrix is correct. <br>","829c10ce":"A Hierarchical clustering is typically visualized as a dendrogram as shown in the following cell. Each merge is represented by a horizontal line. The y-coordinate of the horizontal line is the similarity of the two clusters that were merged, where cities are viewed as singleton clusters. \nBy moving up from the bottom layer to the top node, a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering. ","f41fa3c3":"Using the <b> linkage <\/b> class from hierarchy, pass in the parameters:\n<ul>\n    <li> The distance matrix <\/li>\n    <li> 'complete' for complete linkage <\/li>\n<\/ul>","65d32298":"## K-Means \n\nThere are many models for **clustering** out there. We will be going through most popular ones. Despite its simplicity, the **K-means** is vastly used for clustering in many data science applications, especially useful if you need to quickly discover insights from **unlabeled data**. In this notebook, we see how to use k-Means for customer segmentation.","1b394b91":"As we can see DBSCAN doesn't perform very well because the density in our data is not that strong. Label -1 means outliers so it will appear most as outliers. We may have performed better if we had had a bigger data.\n\n## Mean Shift Algorithm\n\nMeanShift clustering aims to discover blobs in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\n\nThe algorithm automatically sets the number of clusters, instead of relying on a parameter bandwidth, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided estimate_bandwidth function, which is called if the bandwidth is not set.","26852d5d":"We used __complete__ linkage for our case, let's change it to __average__ linkage to see how the dendogram changes.","923ec23a":"By judging from the plots we could say that 5 cluster seems better than the 3 ones. As this is a unsupervised problem we can't really know for sure which one is the best in real life but by looking at the data it's safe to say that 5 would be our choice. \n\nWe can analyze our 5 clusters in detail now:\n\n- `Label 0` is low income and low spending\n- `Label 1` is high income and high spending\n- `Label 2` is mid income and mid spending\n- `Label 3` is high income and low spending\n- `Label 4` is low income and high spending\n\nAlso let's see them more clearly with swarmplot:"}}