{"cell_type":{"954971e4":"code","2725ca83":"code","728de115":"code","6c877663":"code","86890559":"code","87f9379f":"code","e7684349":"code","05c8bcb3":"code","d5660f0a":"code","d4b444a0":"code","0a092216":"code","9145a96c":"code","fafe2a53":"code","db33e994":"code","4bb9970c":"code","20e099b8":"code","d8fee06d":"code","77e4305d":"code","917f7278":"code","feef3142":"code","55a4e56d":"code","a6cf0488":"code","4c8aa1be":"code","ab2ed1a4":"code","26aff1f9":"code","c8417248":"code","8057663d":"code","27f5f999":"code","57f18856":"code","9a794cb7":"code","c010c623":"code","337ae11e":"code","ec89652d":"code","f1fe76fa":"markdown","f3c93d11":"markdown","ad3f4438":"markdown","fbe40b5c":"markdown","7a8dc7d8":"markdown","7bd95406":"markdown","82db3f0b":"markdown","d501945b":"markdown"},"source":{"954971e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2725ca83":"# Importing the pandas library\nimport numpy as np\nimport pandas as pd\n\n# loading the data\npath = \"\/kaggle\/input\/bbc-fulltext-and-category\/bbc-text.csv\"\ndata = pd.read_csv(path)","728de115":"# printing out few elements\ndata.head()","6c877663":"# Basic details about the dataset\ndata.describe()","86890559":"classes = data['category'].unique()\nprint(\"Different Categories:\",classes)","87f9379f":"data['category'].value_counts()","e7684349":"# importing the matplotlib plots\nimport seaborn as sns\n\nsns.countplot(x='category',data = data)","05c8bcb3":"#importing libraries\nimport nltk\nimport re\nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.stem import WordNetLemmatizer ","d5660f0a":"# loading the stop words list, stemmer and lemmatizer\nstop_words = set(stopwords.words(\"english\")) \nstemmer = PorterStemmer() \nlemmatizer = WordNetLemmatizer() \nprint(stop_words)","d4b444a0":"# methods to perform preprocessing\ndef process_words(text):\n    # tokenize the text\n    words = text.split()\n    new_words_list = []\n    \n    for word in words:\n        # only add words which are not stop words\n        if word not in stop_words:\n            word = stemmer.stem(word)\n            word = lemmatizer.lemmatize(word, pos ='v')\n            new_words_list.append(word)\n    \n    # concatenate the string\n    return \" \".join(new_words_list)\n\n\ndef preprocess(text):\n    # convert to lower case\n    text = text.lower()\n    \n    # replace non-alphabets with null\n    text = re.sub('[^a-zA-Z ]','',text)\n    \n    #remove stop words\n    text = process_words(text)\n    \n    return text","0a092216":"sample = data['text'][50]\n\nprint(\"Sample Text before Pre-Processing:\\n\",sample)","9145a96c":"pre_sample = preprocess(sample)\nprint(\"Sample Text after Pre-Processing:\\n\",pre_sample)","fafe2a53":"# apply preprocessing on entire dataset\nx = data['text'].apply(lambda x:preprocess(x))\nprint(x[:1])","db33e994":"# convert the input text to suitable features\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\ncount_vectorizer = CountVectorizer()\ntfidf_vectorizer = TfidfVectorizer()\n\nx_counts = count_vectorizer.fit_transform(x)\nx_tfidf = tfidf_vectorizer.fit_transform(x)","4bb9970c":"new_x = x_counts","20e099b8":"# create labels for target\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder \n\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform( data['category'] )\nprint(\"Label Encodings:\",y)","d8fee06d":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(new_x,y,test_size=0.15)\nprint(\"Size of Training data:\",x_train.shape[0])\nprint(\"Size of Testing data:\",x_test.shape[0])","77e4305d":"# Loading the libraires\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","917f7278":"nb_model = MultinomialNB()\nlogistic_model = LogisticRegression()\ndec_tree_model = DecisionTreeClassifier()","feef3142":"# training the model\nnb_model.fit(x_train,y_train)\nlogistic_model.fit(x_train,y_train)\ndec_tree_model.fit(x_train,y_train)","55a4e56d":"def check_accuracy(model,x_test,y_test):\n    total = x_test.shape[0]\n    count =  0\n    res = model.predict(x_test)\n    for i in range(total):\n        y_true = y_test[i]\n        if y_true == res[i]:\n            count+=1\n    return count\/total\n\nprint(\"Naive Bayes Accuracy:\",check_accuracy(nb_model,x_test,y_test))\nprint(\"Logistic Regression Accuracy:\",check_accuracy(logistic_model,x_test,y_test))\nprint(\"Desicion Tree Accuracy:\",check_accuracy(dec_tree_model,x_test,y_test))\n","a6cf0488":"sample_data = \"actor injured while shooting and the movie got cancelled\"\nprocessed_data = preprocess(sample_data)\nx_sample = count_vectorizer.transform([processed_data])\n\n# get results\nnb_result =  nb_model.predict(x_sample)\nlog_result = logistic_model.predict(x_sample)\ntree_result = dec_tree_model.predict(x_sample)\n\n# get labels\nnb_result = label_encoder.inverse_transform(nb_result)\nlog_result = label_encoder.inverse_transform(log_result)\ntree_result = label_encoder.inverse_transform(tree_result)\n\nprint(\"Naive Bayes Result:\",nb_result)\nprint(\"Logistic Regession Result:\",log_result)\nprint(\"Decision Tree Result:\",tree_result)","4c8aa1be":"input_shape = x_train[0].shape[1]\noutput_shape = 5\nprint(\"Input Shape:\",input_shape)\nprint(\"Output Shape:\",output_shape)","ab2ed1a4":"#create one hot encoding's for the labels\nonehotencoder = OneHotEncoder() \nonehotencoder.fit(y.reshape(-1, 1))\nlabels = onehotencoder.transform(y.reshape(-1, 1)).toarray()\nprint(\"One hot Vector:\\n\",labels)","26aff1f9":"x_train, x_test, y_train, y_test = train_test_split(new_x,labels,test_size=0.15)","c8417248":"# import library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout","8057663d":"model = Sequential()\nmodel.add(Dense( 1024 , activation='sigmoid',  input_dim = input_shape  ))\nmodel.add(Dropout(0.2))\nmodel.add(Dense( 512  , activation='sigmoid' ))\nmodel.add(Dropout(0.2))\nmodel.add(Dense( 256   , activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128   , activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense( 64   , activation='sigmoid'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense( 5, activation='softmax'))","27f5f999":"model.summary()","57f18856":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","9a794cb7":"history = model.fit(x_train,y_train, epochs = 10, batch_size=32, validation_data = (x_test,y_test)  )","c010c623":"import matplotlib.pyplot as plt\nh = history\nplt.plot(h.history['accuracy'])\nplt.plot(h.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.show()\n\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('Model Loss')\nplt.show()","337ae11e":"sample_data = \"actor injured while shooting and the movie got cancelled\"\nprocessed_data = preprocess(sample_data)\nx_sample = count_vectorizer.transform([processed_data])\n\n# get results\nmodel_result = list(model.predict(x_sample)[0])\n\nprint(model_result)\n\n# get labels\nmax_ = max(model_result)\nindex = model_result.index(max_)\nresult = label_encoder.inverse_transform([index])\n\nprint(\"Model Result:\",result)","ec89652d":"sample_data = \"Issues in china after the government tries to introduce a new law system \"\nprocessed_data = preprocess(sample_data)\nx_sample = count_vectorizer.transform([processed_data])\n\n# get results\nmodel_result = list(model.predict(x_sample)[0])\n\nprint(model_result)\n\n# get labels\nmax_ = max(model_result)\nindex = model_result.index(max_)\nresult = label_encoder.inverse_transform([index])\n\nprint(\"Model Result:\",result)","f1fe76fa":"**Creating the models with Keras**","f3c93d11":"**Splitting the dataset**","ad3f4438":"**About the Data**","fbe40b5c":"**Data Pre-Processing**\n","7a8dc7d8":"**Creating the models using scikit-learn**","7bd95406":"*Important Links*\n\n**Text Classification**\n\n[Learn More](https:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python\/)\n\n**Data Handling**\n\n[Python and Numpy](http:\/\/cs231n.github.io\/python-numpy-tutorial\/)\n\n[Pandas](https:\/\/www.guru99.com\/python-pandas-tutorial.html)\n\n**Data Visualization**\n\n[Seaborn](https:\/\/www.journaldev.com\/18583\/python-seaborn-tutorial)\n\n[Matplotlib](https:\/\/www.edureka.co\/blog\/python-matplotlib-tutorial\/)\n\n**Processing**\n\n[Pre-Processing](https:\/\/medium.com\/@datamonsters\/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)\n\n[NLTK](https:\/\/www.guru99.com\/nltk-tutorial.html)\n\n[RegEx](https:\/\/www.w3schools.com\/python\/python_regex.asp)","82db3f0b":"**Validating the Model**","d501945b":"**Loading the Data**\nPandas is Python library providing high-performance, easy-to-use data structures and data analysis tools."}}