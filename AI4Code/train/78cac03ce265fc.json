{"cell_type":{"d71f4d8b":"code","47923891":"code","bc1e36d7":"code","8438e102":"code","da23e434":"code","ada86f51":"code","2f090b10":"code","68d72afe":"code","ead38fd8":"code","a45deb9d":"code","14d2d458":"code","fa4c1290":"code","b283e4a8":"code","3da993a4":"code","71555403":"code","b9ad7c53":"code","b3dbb445":"code","fb29ccc0":"code","97cb31cb":"code","abaf31f2":"code","8b7e63e4":"code","0c3a0508":"code","f8083a13":"code","d90d22a2":"code","708111bf":"code","348c16b5":"code","ad0cd507":"code","eaaa0fa7":"code","f2c068bf":"code","81c09a79":"code","8025e3d5":"code","036fa3d7":"code","d9aa2066":"code","2e7f5615":"code","e25f9ddb":"code","80af4b07":"code","eaae264f":"code","36138928":"code","60ff8c6e":"code","f732dd5f":"code","c8503c3b":"code","d7990c44":"code","e1402030":"code","1ac7ea4b":"code","fcd5b673":"code","8bfb37c0":"code","b2a5cac7":"code","9e542142":"code","7bacb467":"code","1ffc3b68":"code","6da8b939":"code","2fb75e68":"code","76f2fa36":"code","ad85fe47":"code","7ad3f467":"code","e1532f70":"code","ef4cbea8":"code","6d45cf90":"code","bce082b7":"code","536b88d2":"code","fea96d5c":"code","8ac337db":"code","e18df4f7":"code","7f06d8fa":"code","a02e69c1":"code","d4ad4a71":"code","8ac1a1c8":"code","f93e799d":"code","f079fea2":"code","7fe24145":"code","e5be8e22":"code","1d4d7398":"code","dbe4593f":"code","2d914877":"code","ddecb2a4":"code","11ef2b29":"markdown","5b2126a3":"markdown","46ef2aac":"markdown","4bb318c8":"markdown","9eec02d7":"markdown","af246b35":"markdown","49cdba76":"markdown","a8dec8d8":"markdown","220cfd85":"markdown","ced0b4f4":"markdown","bee3190a":"markdown","840a9833":"markdown","8d36e4af":"markdown","b266ab78":"markdown","7ea46a56":"markdown","07bced07":"markdown","091ef8f1":"markdown","a8d9e69d":"markdown","616f4c83":"markdown","cfcdd4ac":"markdown","853dd528":"markdown","31fb3904":"markdown","168bf36f":"markdown","7abb4230":"markdown","e1ed0ed8":"markdown","19e37e65":"markdown","5ad36d1c":"markdown","684f395c":"markdown","0e1bb8e4":"markdown","5fe147af":"markdown","41a7b09a":"markdown","fd4e92f0":"markdown","3aaa5cb6":"markdown","291cfa47":"markdown","c745a3c6":"markdown"},"source":{"d71f4d8b":"import numpy as np\nnp.random.seed(13)\nfrom matplotlib import pyplot as plt\nimport itertools\nimport scipy\nimport sklearn.decomposition\nimport sklearn.manifold\nimport json\nimport gc","47923891":"EMBEDDING_SIZE = 8\nMIN_OCCURRENCES = 10","bc1e36d7":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","8438e102":"(market_data, news_data) = env.get_training_data()","da23e434":"%%time\nnews_data['subjects_tuples'] = news_data['subjects'].copy()\nsubjects_cats = news_data['subjects_tuples'].cat.categories\nsubjects_cats = [eval(c.replace(\"{\", \"(\").replace(\"}\", \")\")) for c in subjects_cats]\nnews_data['subjects_tuples'].cat.categories = subjects_cats\n\ndel subjects_cats","ada86f51":"news_data[['assetCodes', 'time', 'subjects', 'subjects_tuples']].head(3)","2f090b10":"%%time\nnews_data['audiences_tuples'] = news_data['audiences'].copy()\naudiences_cats = news_data['audiences_tuples'].cat.categories\naudiences_cats = [eval(c.replace(\"{\", \"(\").replace(\"}\", \")\")) for c in audiences_cats]\nnews_data['audiences_tuples'].cat.categories = audiences_cats\n\ndel audiences_cats","68d72afe":"news_data[['assetCodes', 'time', 'audiences', 'audiences_tuples']].head(3)","ead38fd8":"def pd_categorical_to_dummies(series, min_occurrences=0):\n    \n    features_cats_evals = series.cat.categories\n    unique_features = list(set(itertools.chain(*features_cats_evals)))\n    \n    num_unique_features = len(unique_features)\n    \n    features_map = {k:v for v, k in enumerate(unique_features)}\n    features_cats_factorized = [[features_map[k] for k in l] for l in features_cats_evals]\n    \n    features_lengths = [\n        len(features_cats_factorized[i]) \n        for i in series.cat.codes\n    ]\n    \n    features_cats_rows = np.arange(series.shape[0]).repeat(features_lengths)\n    \n    features_cats_cols = np.array([\n        v for c in \n        series.cat.codes\n        for v in features_cats_factorized[c]\n    ])\n    \n    total_length = len(features_cats_cols)\n    \n    dummies = scipy.sparse.coo_matrix(\n        (np.ones(total_length, dtype=np.bool), (features_cats_rows, features_cats_cols)),\n        shape=(series.shape[0], num_unique_features),\n        dtype=np.bool\n    )\n    \n    dummies = dummies.tocsr()\n    \n    m = dummies.sum(axis=0).A[0] > min_occurrences\n    \n    dummies = dummies[:, m]\n    unique_features = [a for a, mm in zip(unique_features, m) if mm == 1]\n    \n    return dummies, unique_features","a45deb9d":"def get_similar(w, embeddings, features, max_features=10):\n    \n    i = features.index(w)\n    v = embeddings[i]\n    similarities = (embeddings @ v)\n    \n    ii = np.argsort(similarities)[::-1]\n    \n    similarities = similarities[ii[:max_features + 1]]\n    \n    similar_words = [features[j] for j in ii[:max_features + 1]]\n        \n    m = similarities > 1 - 1e-6\n        \n    assert w in np.array(similar_words)[m]\n    \n    similar_words.remove(w)\n    \n    return similar_words","14d2d458":"%%time\nsubject_dummies, subjects = pd_categorical_to_dummies(\n    news_data['subjects_tuples'],\n    MIN_OCCURRENCES\n)","fa4c1290":"subject_dummies.shape, len(subjects)","b283e4a8":"svd_reducer = sklearn.decomposition.TruncatedSVD(\n    n_components=EMBEDDING_SIZE,\n    algorithm='randomized',\n    n_iter=5,\n    random_state=None,\n    tol=0.0\n)","3da993a4":"%%time\nsvd_reducer.fit(subject_dummies.T)","71555403":"%%time\n_subject_embeddings = svd_reducer.transform(subject_dummies.T)","b9ad7c53":"assert np.abs(_subject_embeddings.sum(axis=1)).min() != 0","b3dbb445":"subject_embeddings = _subject_embeddings\/np.linalg.norm(_subject_embeddings, axis=1)[:, np.newaxis]\n# subject_embeddings[np.isnan(subject_embeddings)] = 0","fb29ccc0":"assert np.isnan(subject_embeddings).sum() == 0","97cb31cb":"%%time\nsubject_tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(subject_embeddings)\nprint(subject_tsne.shape)","abaf31f2":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.scatter(subject_tsne[:, 0], subject_tsne[:, 1], s=1)\n\nfor i, txt in enumerate(subjects):\n    if i % 20 == 0:\n        ax.annotate(txt, (subject_tsne[i, 0], subject_tsne[i, 1]), fontsize=10)\n\nplt.show()","8b7e63e4":"\" \".join(get_similar(\"FUND\", subject_embeddings, subjects))","0c3a0508":"\" \".join(get_similar(\"GOLF\", subject_embeddings, subjects))","f8083a13":"\" \".join(get_similar(\"EPMICS\", subject_embeddings, subjects))","d90d22a2":"\" \".join(get_similar(\"TWAVE\", subject_embeddings, subjects))","708111bf":"with open(\"subjects.json\", \"w\") as f:\n    json.dump(subjects, f)\nnp.save(\"subject_embeddings.npy\", subject_embeddings)","348c16b5":"if False:\n    del subject_dummies, subjects, subject_embeddings\ndel subject_tsne, _subject_embeddings, svd_reducer\ngc.collect()","ad0cd507":"%%time\naudience_dummies, audiences = pd_categorical_to_dummies(\n    news_data['audiences_tuples'], \n    MIN_OCCURRENCES\n)","eaaa0fa7":"audience_dummies.shape, len(audiences)","f2c068bf":"svd_reducer = sklearn.decomposition.TruncatedSVD(\n    n_components=EMBEDDING_SIZE,\n    algorithm='randomized',\n    n_iter=5,\n    random_state=None,\n    tol=0.0\n)","81c09a79":"%%time\nsvd_reducer.fit(audience_dummies.T)","8025e3d5":"%%time\n_audience_embeddings = svd_reducer.transform(audience_dummies.T)","036fa3d7":"assert np.abs(_audience_embeddings.sum(axis=1)).min() != 0","d9aa2066":"audience_embeddings = _audience_embeddings\/np.linalg.norm(_audience_embeddings, axis=1)[:, np.newaxis]\n# audience_embeddings[np.isnan(audience_embeddings)] = 0","2e7f5615":"assert np.isnan(audience_embeddings).sum() == 0","e25f9ddb":"audience_tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(audience_embeddings)\nprint(audience_tsne.shape)","80af4b07":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.scatter(audience_tsne[:, 0], audience_tsne[:, 1], s=1)\n\nfor i, txt in enumerate(audiences):\n    if i % 5 == 0:\n        ax.annotate(txt, (audience_tsne[i, 0], audience_tsne[i, 1]))\n\nplt.show()","eaae264f":"\" \".join(get_similar(\"OIL\", audience_embeddings, audiences))","36138928":"\" \".join(get_similar(\"MTL\", audience_embeddings, audiences))","60ff8c6e":"\" \".join(get_similar(\"NZP\", audience_embeddings, audiences))","f732dd5f":"\" \".join(get_similar(\"FN\", audience_embeddings, audiences))","c8503c3b":"with open(\"audiences.json\", \"w\") as f:\n    json.dump(audiences, f)\nnp.save(\"audience_embeddings.npy\", audience_embeddings)","d7990c44":"if False:\n    del audience_dummies, audiences\ndel audience_embeddings, audience_tsne, _audience_embeddings, svd_reducer\ngc.collect()","e1402030":"audience_subject_map = {}\nnum = 5\n\nglobal_subject_proportions = subject_dummies.sum(axis=0).A[0]\/subject_dummies.shape[0]\n\nfor i in range(len(audiences)):\n    m = audience_dummies[:, i].A[:, 0]\n    a = audiences[i]\n    \n    c = subject_dummies[m].sum(axis=0).A[0]\n    p = c\/subject_dummies[m].shape[0]\n    # s = np.abs(p - global_subject_proportions)\n    s = np.clip(p - global_subject_proportions, 0, np.inf)\n    \n    ii = np.argsort(s)[::-1][:num]\n    \n    subs = np.array(subjects)[ii].tolist()\n    cnts = c[ii]\n    \n    # print(a, subs)\n    \n    audience_subject_map[a] = subs\n    \n    #break","1ac7ea4b":"audience_subject_map['OIL']","fcd5b673":"audience_subject_map['MTL'] # Metal??","8bfb37c0":"audience_subject_map['FN'] # Finland ??","b2a5cac7":"audience_subject_map['NZP'] # New Zealand ??","9e542142":"# subject_embeddings = np.load(\"subject_embeddings.npy\")","7bacb467":"_audience_embeddings_using_sub = []\n\nfor i in range(len(audiences)):\n    a = audiences[i]\n    \n    subs = audience_subject_map[a]\n    \n    ii = [i for i, s in enumerate(subjects) if s in subs]\n    \n    e = subject_embeddings[ii].mean(axis=0)\n    \n    _audience_embeddings_using_sub.append(e)\n    \n_audience_embeddings_using_sub = np.array(_audience_embeddings_using_sub)\n\nprint(_audience_embeddings_using_sub.shape)","1ffc3b68":"audience_embeddings_using_sub = _audience_embeddings_using_sub\/np.linalg.norm(_audience_embeddings_using_sub, axis=1)[:, np.newaxis]","6da8b939":"audience_using_sub_tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(\n    audience_embeddings_using_sub\n)\nprint(audience_using_sub_tsne.shape)","2fb75e68":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.scatter(audience_using_sub_tsne[:, 0], audience_using_sub_tsne[:, 1], s=1)\n\nfor i, txt in enumerate(audiences):\n    if i % 2 == 0:\n        ax.annotate(txt, (audience_using_sub_tsne[i, 0], audience_using_sub_tsne[i, 1]))\n\nplt.show()","76f2fa36":"\" \".join(get_similar(\"OIL\", audience_embeddings_using_sub, audiences))","ad85fe47":"\" \".join(get_similar(\"MTL\", audience_embeddings_using_sub, audiences))","7ad3f467":"\" \".join(get_similar(\"FN\", audience_embeddings_using_sub, audiences))","e1532f70":"\" \".join(get_similar(\"NZP\", audience_embeddings_using_sub, audiences))","ef4cbea8":"with open(\"audience_subject_map.json\", \"w\") as f:\n    json.dump(audience_subject_map, f)\n    \nnp.save(\"audience_embeddings_using_sub.npy\", audience_embeddings_using_sub)","6d45cf90":"import gensim","bce082b7":"model_audience = gensim.models.Word2Vec(\n    size=EMBEDDING_SIZE, #10,\n    window=99999,\n    sg=1,\n    hs=0,\n    min_count=MIN_OCCURRENCES,\n    workers=4,\n    compute_loss=True\n)","536b88d2":"model_audience.build_vocab(news_data['audiences_tuples'].values)","fea96d5c":"%%time\nmodel_audience.train(\n    sentences=news_data['audiences_tuples'],\n    epochs=1,\n    total_examples=news_data.shape[0],\n    compute_loss=True,   \n)\n\nmodel_audience.get_latest_training_loss()","8ac337db":"model_audience.wv.similar_by_word(\"OIL\")","e18df4f7":"model_audience.wv.similar_by_word(\"MTL\")","7f06d8fa":"model_audience.wv.similar_by_word(\"NZP\")","a02e69c1":"%%time\naudience_word2vec_tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(\n    model_audience.wv.vectors_norm\n)\nprint(audience_word2vec_tsne.shape)","d4ad4a71":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.scatter(audience_word2vec_tsne[:, 0], audience_word2vec_tsne[:, 1], s=1)\n\nfor i, txt in enumerate(model_audience.wv.index2word):\n    if i % 2 == 0:\n        ax.annotate(txt, (audience_word2vec_tsne[i, 0], audience_word2vec_tsne[i, 1]))\n\nplt.show()","8ac1a1c8":"with open(\"audience_skipgram.json\", \"w\") as f:\n    json.dump(model_audience.wv.index2word, f)\nnp.save(\"audience_skipgram_embeddings.npy\", model_audience.wv.vectors_norm)","f93e799d":"model_subject = gensim.models.Word2Vec(\n    size=EMBEDDING_SIZE, #10,\n    window=99999,\n    sg=1,\n    hs=0,\n    min_count=MIN_OCCURRENCES,\n    workers=4,\n    compute_loss=True\n)","f079fea2":"model_subject.build_vocab(news_data['subjects_tuples'].values)","7fe24145":"%%time\nmodel_subject.train(\n    sentences=news_data['subjects_tuples'],\n    epochs=1,\n    total_examples=news_data.shape[0],\n    compute_loss=True,   \n)\n\nmodel_subject.get_latest_training_loss()","e5be8e22":"model_subject.wv.similar_by_word(\"FUND\")","1d4d7398":"model_subject.wv.similar_by_word(\"EPMICS\")\n# COMDIS    Communicable Diseases\n# SL        Sierra Leone","dbe4593f":"%%time\nsubjects_word2vec_tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(model_subject.wv.vectors_norm)\nprint(subjects_word2vec_tsne.shape)","2d914877":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\nax.scatter(subjects_word2vec_tsne[:, 0], subjects_word2vec_tsne[:, 1], s=1)\n\nfor i, txt in enumerate(model_subject.wv.index2word):\n    if i % 20 == 0:\n        ax.annotate(txt, (subjects_word2vec_tsne[i, 0], subjects_word2vec_tsne[i, 1]))\n\nplt.show()","ddecb2a4":"with open(\"subjects_skipgram.json\", \"w\") as f:\n    json.dump(model_subject.wv.index2word, f)\nnp.save(\"subject_skipgram_embeddings.npy\", model_subject.wv.vectors_norm)","11ef2b29":"The audiences and subjects columns in the news data are two of the more difficult features to work with. This notebook aims to explore these features, primarily by using embeddings to see how the terms relate to one another.","5b2126a3":"The subjects embedding appears to have done a reasonable job at grouping similar terms together (based on a few examples).\n\nLets save the results.","46ef2aac":"The TWAVE subject is related to natural disasters","4bb318c8":"## Load Data","9eec02d7":"The NZP appear to be related to New Zealand","af246b35":"It is less clear what the MTL audience is related to but assuming it is to do with metal the commodities subject is of note:\n  \n    COM    Commodities    \n    BLR    Content produced in Bangalore\n    FIN    Financials (Legacy)","49cdba76":"### Convert the audience and subject features to tuples\n\nThe audiences and subjects featurs are strings, to make the processing easier we will convert them to tuples","a8dec8d8":"## Word2Vec Skip-Gram Audience Embedding\n\nAnother method we can try, is to use the skip-gram word2vec model to learn the vectors.","220cfd85":"Again, lets have a look at some similar terms for a selection of audiences","ced0b4f4":"The FUND subject is related to other banking and fund subjects","bee3190a":"## Word2Vec Skip-Gram Subject Embedding\n\nFor completeness, lets learn vectors for the subjects using the skip-gram model.\n\nWARNING: this will take several minutes (~15mins)","840a9833":"    FUND \tFunds\n    BANK \tBanks (TRBC)\n    PVE \t Private Equity Funds\n    BSVC \tBanking Services (TRBC)","8d36e4af":"Its still hard to tell if the audience vectors created using the subjects are any good.\n\nAgain, lets save the results","b266ab78":"## Subject Embedding\n\nLets start by creating an embedding for the subjects","7ea46a56":"    TWAVE \tTsunami\n    WLDPWS    Wind Power? (WINPWR = Wind Farms)\n    QUAK \t Earthquakes\n    VIO \t  Civil Unrest\n    TRD       International Trade","07bced07":"    EPMICS   Epidemics\n    INFDIS   Infectious Diseases\n    WOMHEA   Women's Health\n    COMDIS   Communicable Diseases","091ef8f1":"## Audience Embedding using Subject Map\n\nThe embedding learnt for the audiences was not particularly encouraging. Since we have some confidence about the quality of the subjects data lets take a look at which subjects best describe each audience.","a8d9e69d":"## Create Embeddings\n\nTo create the embedding, first we will create a multi-hot encoded array, and then use dimensionality reduction to obtain vectors of our desired size (EMBEDDING_SIZE). To perform the multi-hot encoding we will use the following function.","616f4c83":"The FN audience seems to be related to European countries:\n\n    FI    Finland\n    NORD  Nordic States","cfcdd4ac":"### Visualise\n\nAs with the subjects embedding we can use TSNE to visualise the vectors.\n\nUnlike for the subjects, I have been unable to find any resources to describe what each of the audiences mean, which makes analysing the results harder.","853dd528":"### Audience Embedding\n\nNext we can perform the same steps on the audience data","31fb3904":"### Visualise\n\nNow we can use the vectors to see how subjects relate to one another. The first thing we will do is use TSNE to visualise the embedding.\n\nThere are two sources on the internet that describe what some of the subjects relate to:\n\n [s3.amazonaws.com\/tr-liaison-documents\/public\/Reuters_News_Topics_External.xls](http:\/\/s3.amazonaws.com\/tr-liaison-documents\/public\/Reuters_News_Topics_External.xls)\n\n https:\/\/liaison.reuters.com\/tools\/topic-codes\n \n Of the two the first, the spreadsheet, appears to be the more complete list.","168bf36f":"Again, we will save the results","7abb4230":"There appears to be a strong pattern in which subjects best describe each audience, so let try and use the subject vectors to create an audience vector.","e1ed0ed8":"Next we can use cosine similarity to find which subjects are similar to a given subject","19e37e65":"    GOLF \tGolf\n    PREV     Previews \/ Schedules \/ Diaries\n    GEN      General News\n    ODD \t Human Interest \/ Brights \/ Odd News\n    ICEH \tIce Hockey","5ad36d1c":"As before, it is no clearer if these results are any better.\n\nAgain, we will save the results.","684f395c":"It is more difficult to tell how good the vectors are at grouping similar audiences compared with the subjects due to the lack of any information on what the abbreviations mean. However, looking at the results I would have expected to be able to see some more obvious patterns.\n\nLets save the audience embedding and see if we can use the subjects, which we have some confidence in, to get an understand of what some of the audience abbreviations mean.","0e1bb8e4":"Normalise the vectors","5fe147af":"    MTL    Metal\n    OIL    Oil","41a7b09a":"The EPMICS subject is related to diseases and health issues","fd4e92f0":"The OIL audience appears to be related to energy subjects:\n\n    COM    Commodities\n    NRG    Energy Markets\n    ENR    Energy (Legacy)","3aaa5cb6":"This is another helper function we will use later","291cfa47":"Normalise the vectors","c745a3c6":"The GOLF subject is related to other sports and more general news"}}