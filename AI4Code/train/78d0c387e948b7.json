{"cell_type":{"8f8127cf":"code","cefe3d03":"code","30e9837e":"code","a8b0a2bb":"code","74a078db":"code","01e94cf7":"code","985c24d4":"code","131537e9":"code","e641929c":"code","19610b46":"code","c8581387":"code","0f950bb3":"code","2a252ac4":"code","e1c037fc":"code","313dc34a":"code","28e3da43":"code","5ea7f6f5":"code","461b5a45":"code","45ae8ca2":"code","8ffcbeb4":"code","66eb92e6":"code","e4bf9f61":"code","5b23aa2e":"code","a7ccf70d":"code","ab6dc000":"code","5ee42346":"code","391b4a5d":"code","8b81dbeb":"code","2394557e":"code","10721b9c":"code","258c9d49":"code","fefc01f6":"code","509b8f9e":"code","9ef8f3d3":"code","bf7252e5":"code","b443399a":"code","3addc71c":"code","05b3c77b":"code","a340f59d":"code","a0a13f37":"code","4137eb06":"code","554d4d17":"code","477d6371":"code","7d09b815":"code","b5d1a07f":"code","376df7ba":"code","75d52ce1":"code","06d2c79b":"code","abb8f506":"code","f2912d0e":"code","d2d8a070":"code","49bbd31a":"code","ec501d26":"code","42433811":"code","08b30725":"code","9506b91e":"code","50e8ee28":"code","fc29dac9":"code","b59bbba8":"code","203eeb0a":"markdown","4bd54753":"markdown","0a7f26c6":"markdown","c03769ce":"markdown","09c6428e":"markdown","98f36302":"markdown","97d04237":"markdown","5cc35bf6":"markdown","d77c821b":"markdown","71757247":"markdown","9a8f7093":"markdown","b5c3d90f":"markdown","63a023d9":"markdown","55009949":"markdown"},"source":{"8f8127cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cefe3d03":"#ML Librarires \nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport  matplotlib.pyplot as plt\n\n\nplt.style.use('seaborn')\n%matplotlib inline","30e9837e":"#Data Read\ndf=pd.read_csv('..\/input\/parkinsons-disease-classification\/pd_speech_features.csv',index_col=0, delimiter=',', skiprows=1)\n","a8b0a2bb":"df","74a078db":"df = df.loc[:,~df.columns.duplicated()]","01e94cf7":"df=df.sample(frac=1).reset_index(drop=True)","985c24d4":"df.apply(lambda x: sum(x.isnull()),axis=0)","131537e9":"df.info()","e641929c":"df.describe()","19610b46":"#Plotting data \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px","c8581387":"#checking the target variable countplot\nsns.countplot(data=df,x = 'class',palette='plasma')","0f950bb3":"sns.set(rc={'figure.figsize':(10,8)})\nfig = sns.countplot(x = \"class\" , data = df)\nplt.xlabel(\"class\")\nplt.ylabel(\"Count\")\nplt.title(\"Class Count\")\nplt.grid(True)\nplt.show(fig)","2a252ac4":"\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split","e1c037fc":"df.columns","313dc34a":"#Box Plotting All features distribution corresponding Target column\ni=1\nplt.figure(figsize=(40,40))\nfor c in df.columns[:49]:\n    plt.subplot(10,5,i)\n    plt.title(f\"Boxplot of {c}\",fontsize=16)\n    plt.yticks(fontsize=12)\n    plt.xticks(fontsize=12)\n    sns.boxplot(y=df[c],x=df['class'])\n    i+=1\nplt.show()","28e3da43":"#checking the target variable countplot\n\nplt.figure(figsize=(25,15))\nsns.set_style('white')\nsns.countplot(x='class', data = df, palette='GnBu')\nsns.despine(left=True)","5ea7f6f5":"dataX=df.drop('class',axis=1)\ndataY=df['class']","461b5a45":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.15,random_state=42)","45ae8ca2":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","8ffcbeb4":"dims = X_train.shape[1]\nprint(dims, 'dims')","66eb92e6":"dims = X_test.shape[1]\nprint(dims, 'dims')","e4bf9f61":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","5b23aa2e":"def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):\n    plt.figure(figsize=(5,5))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],linestyle='--')\n    plt.axis('tight')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","a7ccf70d":"from sklearn.neighbors  import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV,cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n","ab6dc000":"from sklearn.decomposition import PCA\npca=PCA(n_components=50)\nX_train=pca.fit_transform(X_train)\nX_test=pca.transform(X_test)","5ee42346":"print('X_train',X_train.shape)\nprint('X_test',X_test.shape)\nprint('y_train',y_train.shape)\nprint('y_test',y_test.shape)","391b4a5d":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(C=0.1,penalty='l2',random_state=42)\nlr.fit(X_train,y_train)\n\ny_pred=lr.predict(X_test)\n\n\ny_proba=lr.predict_proba(X_test)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\n#print('Hata Oran\u0131 :',r2_score(y_test,y_pred))\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"Logistic TRAIN score with \",format(lr.score(X_train, y_train)))\nprint(\"Logistic TEST score with \",format(lr.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","8b81dbeb":"knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)\nknn.fit(X_train,y_train)\n\ny_pred=knn.predict(X_test)\n\ny_proba=knn.predict_proba(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplot_roc_(false_positive_rate,true_positive_rate,roc_auc)\n\nfrom sklearn.metrics import r2_score,accuracy_score\n\nprint('Accurancy Oran\u0131 :',accuracy_score(y_test, y_pred))\nprint(\"KNN TRAIN score with \",format(knn.score(X_train, y_train)))\nprint(\"KNN TEST score with \",format(knn.score(X_test, y_test)))\nprint()\n\ncm=confusion_matrix(y_test,y_pred)\nprint(cm)\nsns.heatmap(cm,annot=True)\nplt.show()","2394557e":"X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.15,random_state=42)","10721b9c":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","258c9d49":"# Fitting Logistic Regression To the training set \nfrom sklearn.linear_model import LogisticRegression   \n  \nclassifier = LogisticRegression(penalty='l2',solver='lbfgs',class_weight='balanced', max_iter=1000,random_state = 42) \nclassifier.fit(X_train, y_train)\n","fefc01f6":"y_pred = classifier.predict(X_test)\n","509b8f9e":"# making confusion matrix between \n#  test set of Y and predicted value. \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred) \nprint (cm)","9ef8f3d3":"from sklearn.metrics import classification_report,accuracy_score\nprint(classification_report(y_test,y_pred))\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred)*100)\n\nprint(y_pred)","bf7252e5":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis","b443399a":"names = [ \"MLP-Neural Net\", \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    MLPClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()\n]","3addc71c":"from sklearn.model_selection import cross_val_score\n\n# iterate over classifiers\nresults = {}\nfor name, clf in zip(names, classifiers):\n    scores = cross_val_score(clf, X_train, y_train, cv=5)\n    results[name] = scores","05b3c77b":"for name, scores in results.items():\n    print(\"%20s | Accuracy: %0.2f%% (+\/- %0.2f%%)\" % (name, 100*scores.mean(), 100*scores.std() * 2))","a340f59d":"from sklearn.model_selection import GridSearchCV\n\nclf = SVC(kernel=\"linear\")\n\n# prepare a range of values to test\nparam_grid = [\n  {'C': [.01, .1, 1, 10], 'kernel': ['linear']},\n ]\n\ngrid = GridSearchCV(estimator=clf, param_grid=param_grid)\ngrid.fit(X_train, y_train)\nprint(grid)","a0a13f37":"# summarize the results of the grid search\nprint(\"Best score: %0.2f%%\" % (100*grid.best_score_))\nprint(\"Best estimator for parameter C: %f\" % (grid.best_estimator_.C))","4137eb06":"seed = 42\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import classification_report, roc_auc_score, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier,\\\n                            BaggingClassifier,VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline","554d4d17":"# split the data into train and test\ndef split_data(X, Y, seed=42, train_size=0.8):\n    xtrain, xtest, ytrain, ytest = train_test_split(X, Y, train_size=train_size, random_state = seed, stratify=Y)\n    xtrain, xtest = preprocess(xtrain, xtest)\n    return (xtrain, xtest, ytrain, ytest)\n\n# preprocess the data for training\ndef preprocess(x1, x2=None):\n    sc = StandardScaler()\n    x1 = pd.DataFrame(sc.fit_transform(x1), columns=x1.columns)\n    if x2 is not None:\n        x2 = pd.DataFrame(sc.transform(x2), columns=x2.columns)\n        return (x1,x2)\n    return x1\n\n# for model evaluation and training\ndef eval_model(model, X, Y, seed=1):\n    xtrain, xtest, ytrain, ytest = split_data(X, Y)\n    model.fit(xtrain, ytrain)\n    \n    trainpred = model.predict(xtrain)\n    trainpred_prob = model.predict_proba(xtrain)\n    testpred = model.predict(xtest)\n    testpred_prob = model.predict_proba(xtest)\n    \n    print(\"Train ROC AUC : %.4f\"%roc_auc_score(ytrain, trainpred_prob, multi_class='ovr'))\n    print(\"\\nTrain classification report\\n\",classification_report(ytrain, trainpred))\n    \n    ### make a bar chart for displaying the wrong classification of one class coming in which other class\n    \n    print(\"\\nTest ROC AUC : %.4f\"%roc_auc_score(ytest, testpred_prob, multi_class='ovr'))\n    print(\"\\nTest classification report\\n\",classification_report(ytest, testpred))\n    \ndef plot_importance(columns, importance):\n    plt.bar(columns, importance)\n    plt.show()","477d6371":"#Feature Extraction, Importance & Splitting\n\nY= df['class']\n\nX = df.drop(['class'],axis = 1)","7d09b815":"X_sc = preprocess(X)","b5d1a07f":"model_logr = LogisticRegression(random_state=seed,n_jobs=-1)\nmodel_nb = GaussianNB()\nmodel_dt = DecisionTreeClassifier(random_state=seed)\nmodel_dt_bag = BaggingClassifier(model_dt, random_state=seed, n_jobs=-1)\nmodel_ada = AdaBoostClassifier(random_state=seed)\nmodel_gbc = GradientBoostingClassifier(random_state=seed)\nmodel_rf = RandomForestClassifier(random_state=seed, n_jobs=-1)\nmodel_xgb = XGBClassifier(random_state=seed)\nmodel_lgbm = LGBMClassifier(random_state=seed, n_jobs=-1)\nmodel_knn = KNeighborsClassifier(n_jobs=-1)","376df7ba":"models = []\nmodels.append(('LR',model_logr))\nmodels.append(('NB',model_nb))\nmodels.append(('DT',model_dt))\nmodels.append(('Bag',model_dt_bag))\nmodels.append(('Ada',model_ada))\nmodels.append(('GBC',model_gbc))\nmodels.append(('RF',model_rf))\nmodels.append(('XGB',model_xgb))\nmodels.append(('LGBM',model_lgbm))\nmodels.append(('KNN',model_knn))","75d52ce1":"cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    scores = cross_val_score(model, X_sc, Y, scoring='f1_weighted', cv=cv, n_jobs=-1)\n    accuracy = scores.mean()\n    std = scores.std()\n    print(f\"{name} : Mean ROC {accuracy} STD:({std})\")\n    results.append(scores)\n    names.append(name)","06d2c79b":"fig, ax = plt.subplots(figsize=(12,6))\nax.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","abb8f506":"print(X_train.shape , y_train.shape)\nprint(X_test.shape , y_test.shape)","f2912d0e":"#Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfrom keras.layers import Dense, Dropout , BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.optimizers import RMSprop, Adam\n\n#tf \nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n","d2d8a070":"model = Sequential()\nmodel.add(Dense(64,input_dim=X_train.shape[1],activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(256,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(512,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss = 'binary_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])","49bbd31a":"model.summary()","ec501d26":"from keras.callbacks import EarlyStopping, ModelCheckpoint","42433811":"fBestModel = 'best_model.h5' \nearly_stop = EarlyStopping(monitor='val_loss', patience=2, verbose=1) \nbest_model = ModelCheckpoint(fBestModel, verbose=0, save_best_only=True)\n\nmodel.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=100, \n          batch_size=62, verbose=True, callbacks=[best_model, early_stop])","08b30725":"score = model.evaluate(X_test, y_test, verbose=1)\nprint('Accuracy: ', score[1]*100)\nprint( 'loss:', score[0]*100)","9506b91e":"prediction = model.predict(X_test)","50e8ee28":"prediction = (prediction > 0.5)","fc29dac9":"from sklearn import metrics","b59bbba8":"print(metrics.classification_report(y_test, prediction))","203eeb0a":"Research Projects Details; https:\/\/github.com\/sohel-ccse?tab=projects","4bd54753":"# Part 3 for Algorithms","0a7f26c6":"# Future Work & Suggestions : \n* If u  want to improve this work.You can focus on Class distribution ..\n* definetly it'll improve..Bcz class distribuion is imbalanced .\n* And do the hyperameter tuning for Deep Neural Networks","c03769ce":"#  Withou PCA Analysis & Using Machine Learning Algorithms; Part 2 for ML Algorithms","09c6428e":"# Running the algorithms","98f36302":"# Part 1 for ML Algorithms","97d04237":"# Data Read, Data Visualization,EDA Analysis,Data Pre-Processing,Data Splitting","5cc35bf6":"Imbalanced data distribution for target class.","d77c821b":"With PCA Analysis","71757247":"# Creating array of models","9a8f7093":"# Part 1: Method-ML Algorithams","b5c3d90f":"N.B. = I prefer to use models without outlier & imbalanced treatment, in many cases it can improve the model performance. But it also leads to change of information which might alter real\/practical situations","63a023d9":"Data Splitting","55009949":" # Using Deep Neural Networks ; Part4"}}