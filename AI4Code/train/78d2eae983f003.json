{"cell_type":{"1329cece":"code","6a814e2c":"code","077a38ea":"code","5eeb8603":"code","f33f2bd6":"code","dbc455bd":"code","53b30570":"code","221c2f67":"code","54f8608a":"markdown","e3f261bb":"markdown","d38f27c4":"markdown","938d873b":"markdown","20d6353e":"markdown","2b7e8a1c":"markdown","0db480de":"markdown"},"source":{"1329cece":"import json\nimport pickle\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nimport warnings\nfrom bayes_opt import BayesianOptimization\nimport sys\n\n# Silence SettingWithCopy warnings\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")","6a814e2c":"random_state = 42\n\nprint('Loading data...')\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\n\ntrain_data, val_data = train_test_split(data, test_size=0.15, random_state=random_state)\ntest_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)\nfeatures = data.columns\n\n\ny_train = train_data.pop('SalePrice')\ny_eval = val_data.pop('SalePrice')\n\nn_samples, n_features = data.values.shape\n\nprint('Data loaded')","077a38ea":"features_numeric = []\nfeatures_categorical = []\n\nfor feature in train_data.columns:\n    if len(data[feature].unique()) > 4 and pd.api.types.is_numeric_dtype(data[feature]):\n        features_numeric.append(feature)\n        train_data[feature] = train_data[feature].astype('float32')\n        val_data[feature] = val_data[feature].astype('float32')\n        test_data[feature] = test_data[feature].astype('float32')\n\n    else:\n        features_categorical.append(feature)\n        train_data[feature] = train_data[feature].astype('category')\n        val_data[feature] = val_data[feature].astype('category')\n        test_data[feature] = test_data[feature].astype('category')","5eeb8603":"feature_names = list(train_data.columns)\n\nX_train = lgb.Dataset(train_data, y_train, feature_name=feature_names, categorical_feature=features_categorical, free_raw_data=False)\nX_val = lgb.Dataset(val_data, y_eval, feature_name=feature_names, categorical_feature=features_categorical, free_raw_data=False)","f33f2bd6":"params = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 1E-2,\n    'feature_fraction': 0.8,\n    'early_stopping_rounds': 500,\n    'random_state': random_state\n}\n","dbc455bd":"print('Starting training...')\n\ngbm = lgb.train(params,\n                    X_train,\n                    num_boost_round=5000,\n                    valid_sets=X_val,  # validation data\n                    feature_name=feature_names, verbose_eval=True)\n","53b30570":"def RMSLE(y_true:np.ndarray, y_pred:np.ndarray) -> np.float64:\n    \"\"\"\n        The Root Mean Squared Log Error (RMSLE) metric \n        \n        :param y_true: The ground truth labels given in the dataset\n        :param y_pred: Our predictions\n        :return: The RMSLE score\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\ny_val_pred = gbm.predict(val_data)\nprint(f'Loss: {RMSLE(y_eval, y_val_pred):0.3f}')","221c2f67":"y_pred = gbm.predict(test_data)\n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv', index_col=0)\nsubmission['SalePrice'] = y_pred\nsubmission.to_csv('submission.csv')","54f8608a":"We can use Microsoft's [lightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html) package to get a good score (top ~30%) without any preprocessing or feature engineering, thanks to it's ability to handle categorical variables and missing values implicitly. Here we will get a score of ~0.133 in three easy steps and under 20s of execution time!! ","e3f261bb":"Not bad for an entirely unoptimized model! If we scored this well on the public test set we'd be in the top 33%. \n(We won't; since we used this as our early stopping criteria we're slightly overfit to our validation set.) \n\nAnd finally, we predict on the test data:","d38f27c4":"## Conclusion\n\nlightGBM is my go-to baseline model for most classification and regression tasks as it is very fast (this workbook runs in under a minuite) and can provide very strong results without any preprocessing, scaling or imputation. Too get even better results, we can consider:\n\n1. K folds: The most obvious improvement we could make is to do k-fold cross validation and ensemble each trained model.\n\n2. Feature engineering and imputation: Missing values can be intelligently imputed: some values (like lot size or home square feet) could be imputed to the median of k nearest neighbors, while others could impute to a default or null value. Advanced features from regressions or clustering could also be used. \n\n3. Hyperparameter optimization: lightGBM has [many hyperparameters](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html), which can be easily tuned using the excellent [BayesOpt](https:\/\/github.com\/rmcantin\/bayesopt) package.\n\n4. Oversampling: While it isn't that popular in regression tasks, we can use [oversampling techniques](https:\/\/towardsdatascience.com\/repurposing-traditional-resampling-techniques-for-regression-tasks-d1a9939dab5d?gi=fcc6bb8eadad) to increase our coverage of very high and very low priced homes. \n\n5. Custom loss function: We could also, in principle, set the loss function to RMSLE, the metric used in the competition, but I actually haven't figured out how to do that yet. \n\nThanks for reading!","938d873b":"## 1. Import data","20d6353e":"## 2. Build dataset\n\nWe define the categorical variables. We cast all numeric variables with over four unique variables to floats, and set the remaining variables to categorical","2b7e8a1c":"We define the metric used in the contest - root-mean-squared log error - and estimate our score","0db480de":"## 3. Train model\n\nWe train a lightGBM using mostly default hyperparameters, setting the target, metric, a low learning rate, and a high early stopping patience. We also set a `bagging_fraction` and `feature_fraction` of 0.8 to minimize overfitting."}}