{"cell_type":{"593686a3":"code","a77be82f":"code","f4a445f4":"code","b4ea4b2f":"code","aeef7d20":"code","de574a70":"code","c92e7302":"code","098a8e46":"code","5bf9a083":"code","ab3ed9e4":"code","3a0d6714":"code","0ac959a4":"code","7be3672d":"code","df01f6d9":"code","8602c86c":"code","1206ae92":"code","3dc5fa57":"code","7047aa16":"code","9f60036b":"code","c31ba681":"code","32d92f37":"code","916cb6d9":"code","c78afa2e":"code","f16548cc":"code","97528979":"code","5878f9af":"code","de4ea5cb":"code","dc040100":"code","19b28d89":"code","0751ee6f":"code","3057cdce":"code","71c8658d":"markdown","8d221da9":"markdown","56438f58":"markdown","b5b5ea58":"markdown","0e2662b3":"markdown","2eca7cd2":"markdown","95653c78":"markdown","7351c7b3":"markdown","7ec1358d":"markdown"},"source":{"593686a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a77be82f":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nimport xgboost as xgb\n\n%config InlineBackend.figure_format = 'retina' \n%matplotlib inline","f4a445f4":"train_set = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', header=0)\ntest_set =pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', header=0)","b4ea4b2f":"train_set.head()","aeef7d20":"train_set.info()","de574a70":"train_set.describe()","c92e7302":"full_data = pd.concat((train_set.loc[:,'MSSubClass':'YrSold'],test_set.loc[:,'MSSubClass':'YrSold']))","098a8e46":"matplotlib.rcParams['figure.figsize'] = (20, 6)\nprices = pd.DataFrame({\"price\":train_set[\"SalePrice\"], \"log(price + 1)\":np.log1p(train_set[\"SalePrice\"])})\nprices.hist()","5bf9a083":"#log transform:\ntrain_set[\"SalePrice\"] = np.log1p(train_set[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_feats = full_data.dtypes[full_data.dtypes != \"object\"].index\n\nskewed_feats = train_set[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nfull_data[skewed_feats] = np.log1p(full_data[skewed_feats])","ab3ed9e4":"full_data = pd.get_dummies(full_data) # Converting categorical variable into dummy\/indicator variables.","3a0d6714":"full_data = full_data.fillna(full_data.mean()) # Filling NA's with the mean of the column.","0ac959a4":"full_data #just checking","7be3672d":"#creating matrices for sklearn:\nx_train = full_data[:train_set.shape[0]]\nx_test = full_data[train_set.shape[0]:]\ny = train_set.SalePrice","df01f6d9":"def rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, x_train, y, scoring=\"neg_mean_squared_error\", cv = 5)) #simple cross-validation\n    return(rmse)","8602c86c":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(x_train, y)","1206ae92":"r = rmse_cv(model_lasso).mean()\nprint(r)","3dc5fa57":"model_ridge = Ridge()","7047aa16":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() \n            for alpha in alphas]","9f60036b":"cv_ridge = pd.Series(cv_ridge, index = alphas)\nprint(cv_ridge.min())","c31ba681":"matplotlib.rcParams['figure.figsize'] = (20, 6)\ncv_ridge.plot(title = \"Change of Error\", color=\"r\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\")","32d92f37":"coef = pd.Series(model_lasso.coef_, index = x_train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","916cb6d9":"imp_coef = pd.concat([coef.sort_values().head(10),coef.sort_values().tail(10)])\n\nmatplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\nimp_coef.plot(kind = 'barh',color='g')\nplt.title(\"Coefficients in the Lasso Model\")","c78afa2e":"plt.scatter(train_set.GrLivArea, train_set.SalePrice, c = \"c\",)\nplt.title(\"GrLivArea vs SalePrice\", size=20)\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","f16548cc":"#removing outliers\ntrain_set = train_set[train_set.GrLivArea < 8.25]\ntrain_set = train_set[train_set.SalePrice < 13]\ntrain_set = train_set[train_set.SalePrice > 10.75]\ntrain_set.drop(\"Id\", axis=1, inplace=True)","97528979":"matplotlib.rcParams['figure.figsize'] = (25.0, 15.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(x_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\", color=\"m\")","5878f9af":"dtrain = xgb.DMatrix(x_train, label = y)\ndtest = xgb.DMatrix(x_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","de4ea5cb":"matplotlib.rcParams['figure.figsize'] = (20, 8)\nmodel.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n","dc040100":"model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(x_train, y)","19b28d89":"xgb_preds = np.expm1(model_xgb.predict(x_test))\nlasso_preds = np.expm1(model_lasso.predict(x_test))\n\npredictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")","0751ee6f":"preds = (0.7*lasso_preds) + (0.3*xgb_preds)","3057cdce":"solution = pd.DataFrame({\"id\":test_set.Id, \"SalePrice\":preds})\nsolution.to_csv(\"housePrice_solution.csv\", index = False)","71c8658d":"Now it's time for Ridge.","8d221da9":"### *The Why:*\n**Logarithmic transformation** is a convenient means of transforming a highly skewed variable into a more normalized dataset. When modeling variables with non-linear relationships, the chances of producing errors may also be skewed negatively. In theory, we want to produce the smallest error possible when making a prediction, while also taking into account that we should not be overfitting the model. Overfitting occurs when there are too many dependent variables in play that it does not have enough generalization of the dataset to make a valid prediction. Using the logarithm of one or more variables improves the fit of the model by transforming the distribution of the features to a more normally-shaped bell curve. So I will use log. transform as below.\n - ------------------------------------------------\n### *Neden Log. Transformation Kullan\u0131yoruz:*\n\n**Logaritmik d\u00f6n\u00fc\u015f\u00fcm**, \u00e7arp\u0131kl\u0131\u011f\u0131 y\u00fcksek olan bir de\u011fi\u015fkeni daha da normalle\u015ftirilmi\u015f bir veri k\u00fcmesine d\u00f6n\u00fc\u015ft\u00fcrmede uygun bir yoldur. Do\u011frusal olmayan ili\u015fkilere sahip de\u011fi\u015fkenleri modellerken, hata \u00fcretme \u015fans\u0131 da olumsuz y\u00f6nde e\u011filebilir. Teorik olarak, bir tahminde bulunurken m\u00fcmk\u00fcn olan en k\u00fc\u00e7\u00fck hatay\u0131 \u00fcretmek istiyoruz ve ayn\u0131 zamanda modele fazla uymamam\u0131z gerekti\u011fini de hesaba kat\u0131yoruz. Overfitting, \u00e7ok fazla ba\u011f\u0131ml\u0131 de\u011fi\u015fken oldu\u011funda ge\u00e7erli bir tahmin yapmak i\u00e7in veri setinin yeterli genellemesine sahip olmad\u0131\u011f\u0131nda meydana gelir. Bir veya daha fazla de\u011fi\u015fkenin logaritmas\u0131n\u0131 kullanmak, \u00f6zelliklerin da\u011f\u0131l\u0131m\u0131n\u0131 daha normal \u015fekilli bir \u00e7an e\u011frisine d\u00f6n\u00fc\u015ft\u00fcrerek modelin uyumunu iyile\u015ftirir. Bu y\u00fczden a\u015fa\u011f\u0131daki gibi log. transform kullanaca\u011f\u0131m.\n","56438f58":"The most important positive feature is **\"GrLivArea\"** (the above ground area by area square feet). So let's visualize the relationship between \"GrLivArea\" and \"SalePrice\" with scatter plot.","b5b5ea58":"An important characteristic of *'Lasso regression'* is that it eliminates the weights of the least important features (ie makes it zero). In other words, Lasso regression automatically applies a \"feature selection\" and outputs a discrete model (a discrete model means models with only a few weights different from zero). So let's also look at how many variables Lasso has chosen and how many it has eliminated accordingly.\n- --\n*'Lasso regresyonu'*nun \u00f6nemli bir karakteristi\u011fi, en az \u00f6nemli \u00f6zniteliklerin (features) a\u011f\u0131rl\u0131klar\u0131n\u0131 elemektedir (\u00f6rne\u011fin s\u0131f\u0131r yapmaktad\u0131r.). Ba\u015fka bir deyi\u015fle, Lasso regresyonu otomatik olarak \u201c\u00f6znitelik se\u00e7imi\u201d (feature selection) uygulamaktad\u0131r ve \u00e7\u0131kt\u0131 olarak ayr\u0131k bir model (sparse model) vermektedir (ayr\u0131k bir modelden kas\u0131t, yaln\u0131zca bir ka\u00e7 a\u011f\u0131rl\u0131\u011f\u0131n s\u0131f\u0131rdan farkl\u0131 oldu\u011fu modeller). O halde buna g\u00f6re ka\u00e7 tane de\u011fi\u015fken se\u00e7ip ka\u00e7 tanesini elimine etti\u011fine de bakal\u0131m. ","0e2662b3":"Let's add an xgboost model to our linear model to see if we can improve our score:","2eca7cd2":"TO DO \n1. I'll make the features more normal \n1. I'll create Dummy variables for the categorical features\n1. Finally I'll replace tehe NaN's with the mean of their respective columns","95653c78":"Let's look at the residuals as well. A **residual** is the difference between the \"observed y-value\" and the \"predicted y-value\". It is the vertical distance from the actual plotted point to the point on the regression line. \n- ---\n**Residual**, \"g\u00f6zlenen y de\u011feri\" ile \"tahmin edilen y de\u011feri\" aras\u0131ndaki farkt\u0131r. Yani bu kabaca,noktan\u0131n ger\u00e7ek yerinin regresyon \u00e7izgisi \u00fczerindeki yerine dikey mesafesidir.","7351c7b3":"I will try both ***'Lasso Regularization'*** and ***'Ridge Regularization'*** and choose whichever model gives the lowest error. <br>Let's try Lasso first.","7ec1358d":"As we can see, the error was lower when we used the Lasso.\n* **Lasso RMSE** -> 0.12368100235260548 \n* **Ridge RMSE** -> 0.12819301188873156\n"}}