{"cell_type":{"6eae03d9":"code","e80fbbf4":"code","9f8d30c5":"code","8176ff2c":"code","03480354":"code","0508514f":"code","3a2cf614":"code","3bb61ec3":"code","cc87a63b":"code","3548c4e2":"markdown","31b7ecb2":"markdown","65b6c027":"markdown","34af0bda":"markdown","bfea25c2":"markdown","283e54ea":"markdown","475b1a8a":"markdown","a6009ed5":"markdown","d1938bca":"markdown"},"source":{"6eae03d9":"#load come basic package\nimport pandas as pd\nimport numpy as np\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\n#deep learning package\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\n\nimport matplotlib.pyplot as plt","e80fbbf4":"seed=123\ndata=pd.read_csv('..\/input\/SP500.csv')  \n\n#delete the date column\ndata=data.drop(['Date'],axis=1)\n\npred_result=pd.DataFrame()\n\n#here we use 80% data for training and validation, and 20% data for testing\n#training and validation set\nX_train=data.drop(['LABEL'],axis=1).loc[:1958,:].values\ny_train=np.asarray(data.loc[:1958,'LABEL'].values).astype('float64')\n\n#test set\nX_test=data.drop(['LABEL'],axis=1).loc[1959:,:].values\ny_test=np.asarray(data.loc[1959:,'LABEL']).astype('float64')\n\n#here we normalized the features\n#range from 0 to 1\nscaler=MinMaxScaler().fit(X_train)\nX_train_norm=scaler.transform(X_train)\nX_test_norm=scaler.transform(X_test)","9f8d30c5":"Model_Deep_DNN=Sequential()\nModel_Deep_DNN.add(layers.Dense(32,input_shape=(X_train.shape[1],)))\nModel_Deep_DNN.add(layers.Dropout(0.1))\nModel_Deep_DNN.add(layers.Dense(32))\nModel_Deep_DNN.add(layers.Dropout(0.1))\nModel_Deep_DNN.add(layers.Dense(32))\nModel_Deep_DNN.add(layers.Dropout(0.1))\nModel_Deep_DNN.add(layers.Dense(1,activation='sigmoid'))\n\nModel_Deep_DNN.compile(optimizer=RMSprop(),loss='binary_crossentropy',metrics=['accuracy'])\nhistory=Model_Deep_DNN.fit(X_train,y_train,batch_size=128,epochs=100,validation_split=0.2)\n\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training acc')\nplt.plot(epochs,val_acc,'ro',label='Validation acc')\nplt.plot(epochs,loss,'b',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='Validation loss')\n\nplt.legend() \nplt.figure()","8176ff2c":"timestep=1\n#reshape the data for 3d input\nX_deep_matrix=np.append(X_train,X_test,axis=0)\n#transform the matrix data to tensor data\nX_deep_tensor=np.empty((X_deep_matrix.shape[0]-timestep+1,timestep,X_deep_matrix.shape[1]))\n\nfor i in range(timestep-1,X_deep_matrix.shape[0]):\n    X_deep_tensor[i-timestep+1]=X_deep_matrix[i-timestep+1:i+1,:]\n\ndel X_deep_matrix\n#generate training and test sets for deep learning algorithm    \nX_train_deep=X_deep_tensor[:X_deep_tensor.shape[0]-y_test.shape[0]]\nX_test_deep=X_deep_tensor[X_deep_tensor.shape[0]-y_test.shape[0]:]\n\ndel X_deep_tensor\n\ny_train_deep=y_train[timestep-1:len(y_train)]","03480354":"Model_Deep_RNN=Sequential()\nModel_Deep_RNN.add(layers.SimpleRNN(32,dropout=0.1,recurrent_dropout=0.1,return_sequences=True,\n                                    input_shape=(X_train_deep.shape[1],X_train_deep.shape[2])))\nModel_Deep_RNN.add(layers.SimpleRNN(32,dropout=0.1,recurrent_dropout=0.1,return_sequences=True))\nModel_Deep_RNN.add(layers.SimpleRNN(32,dropout=0.1,recurrent_dropout=0.1,return_sequences=False))\nModel_Deep_RNN.add(layers.Dense(1,activation='sigmoid'))\n\nModel_Deep_RNN.compile(optimizer=RMSprop(),loss='binary_crossentropy',metrics=['accuracy'])\nhistory=Model_Deep_RNN.fit(X_train_deep,y_train_deep,batch_size=128,epochs=100,validation_split=0.2)\n\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training acc')\nplt.plot(epochs,val_acc,'ro',label='Validation acc')\nplt.plot(epochs,loss,'b',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='Validation loss')\n\nplt.legend() \nplt.figure()","0508514f":"Model_Deep_BRNN=Sequential()\nModel_Deep_BRNN.add(layers.Bidirectional(layers.SimpleRNN(5,dropout=0.1,recurrent_dropout=0.1,return_sequences=True,\n                                     input_shape=(X_train_deep.shape[1],X_train_deep.shape[2]))))\nModel_Deep_BRNN.add(layers.Bidirectional(layers.SimpleRNN(5,dropout=0.1,recurrent_dropout=0.1,return_sequences=True)))\nModel_Deep_BRNN.add(layers.Bidirectional(layers.SimpleRNN(5,dropout=0.1,recurrent_dropout=0.1,return_sequences=False)))\nModel_Deep_BRNN.add(layers.Dense(1,activation='sigmoid'))\n\nModel_Deep_BRNN.compile(optimizer=RMSprop(),loss='binary_crossentropy',metrics=['accuracy'])\nhistory=Model_Deep_BRNN.fit(X_train_deep,y_train_deep,batch_size=128,epochs=100,validation_split=0.2)\n\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training acc')\nplt.plot(epochs,val_acc,'ro',label='Validation acc')\nplt.plot(epochs,loss,'b',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='Validation loss')\n\nplt.legend() \nplt.figure()","3a2cf614":"Model_Deep_RNNLSTM=Sequential()\nModel_Deep_RNNLSTM.add(layers.LSTM(5,dropout=0.5,recurrent_dropout=0.5,return_sequences=True,\n                                    input_shape=(X_train_deep.shape[1],X_train_deep.shape[2])))\nModel_Deep_RNNLSTM.add(layers.LSTM(5,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\nModel_Deep_RNNLSTM.add(layers.LSTM(5,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\nModel_Deep_RNNLSTM.add(layers.Dense(1,activation='sigmoid'))\n\nModel_Deep_RNNLSTM.compile(optimizer=RMSprop(),loss='binary_crossentropy',metrics=['accuracy'])\nhistory=Model_Deep_RNNLSTM.fit(X_train_deep,y_train_deep,batch_size=128,epochs=20,validation_split=0.2)\n\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training acc')\nplt.plot(epochs,val_acc,'ro',label='Validation acc')\nplt.plot(epochs,loss,'b',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='Validation loss')\n\nplt.legend() \nplt.figure()","3bb61ec3":"#RNN-GRU\nModel_Deep_RNNGRU=Sequential()\nModel_Deep_RNNGRU.add(layers.GRU(3,dropout=0.5,recurrent_dropout=0.5,return_sequences=True,\n                                    input_shape=(X_train_deep.shape[1],X_train_deep.shape[2])))\nModel_Deep_RNNGRU.add(layers.GRU(3,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\nModel_Deep_RNNGRU.add(layers.GRU(3,dropout=0.5,recurrent_dropout=0.5,return_sequences=False))\nModel_Deep_RNNGRU.add(layers.Dense(1,activation='sigmoid'))\n\nModel_Deep_RNNGRU.compile(optimizer=RMSprop(),loss='binary_crossentropy',metrics=['accuracy'])\nhistory=Model_Deep_RNNGRU.fit(X_train_deep,y_train_deep,batch_size=128,epochs=20,validation_split=0.2)\n\nacc=history.history['acc']\nval_acc=history.history['val_acc']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(1,len(acc)+1)\n\nplt.plot(epochs,acc,'bo',label='Training acc')\nplt.plot(epochs,val_acc,'ro',label='Validation acc')\nplt.plot(epochs,loss,'b',label='Training loss')\nplt.plot(epochs,val_loss,'r',label='Validation loss')\n\nplt.legend() \nplt.figure()","cc87a63b":"pred_Deep=pd.concat([pd.DataFrame(Model_Deep_RNN.predict(X_test_deep)),\n                     pd.DataFrame(Model_Deep_BRNN.predict(X_test_deep)),\n                     pd.DataFrame(Model_Deep_RNNLSTM.predict(X_test_deep)),\n                     pd.DataFrame(Model_Deep_RNNGRU.predict(X_test_deep))],axis=1)\n\npred_Deep[pred_Deep>0.5]=1\npred_Deep[pred_Deep<=0.5]=0\n\npred_result=pd.concat([pd.DataFrame(y_test),pred_Deep],axis=1)\n\n#rename\npred_result.columns=['LABEL','RNN_Pred','BRNN_Pred','RNNLSTM_Pred','RNNGRU_Pred']\n\npred_result","3548c4e2":"**RNN-GRU**","31b7ecb2":"Transform the 2-dimension data to the 3-dimension data (here the timestep =1)","65b6c027":"Any issues about the reproduce\n\nhttps:\/\/github.com\/keras-team\/keras\/issues\/2280\n\nhttps:\/\/stackoverflow.com\/questions\/42412660\/non-deterministic-gradient-computation","34af0bda":"**RNN-LSTM**","bfea25c2":"**RNN**","283e54ea":"**DNN**","475b1a8a":"**bidirectional RNN (BRNN)**","a6009ed5":"**preparation for the normal 2-dimension data**","d1938bca":"Notice that it is just a simple example in the deep-learning areas. The purpose of this kernel is to implement deep-learning algorithms for financial asset forecasting (e.g. stock index price movement). Any discussion is encouraged.\n\nThis kernal will be updating continuously..."}}