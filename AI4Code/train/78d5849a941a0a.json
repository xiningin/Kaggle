{"cell_type":{"fe0256c7":"code","4f31b50f":"code","d26a54c0":"code","796f12f5":"code","8016435b":"code","6c5f5f13":"code","0dd7e0df":"code","4316b722":"code","03936c7b":"code","afbb9e0b":"code","5fb5d1e7":"code","8472699e":"code","18e7413f":"code","8a4b9862":"code","ef40401c":"code","6dd1f3be":"code","60fe10e3":"code","b3f9e455":"code","78f9a7a8":"code","7c16a678":"code","dc624f5a":"code","649b848d":"code","72144703":"code","03827d02":"code","d08c2a4e":"code","8d0f9055":"code","f1930004":"code","3ec166c1":"code","07a5e9ba":"code","745f26c0":"code","50e7c4ff":"code","73aaccff":"code","336d9310":"code","7211182e":"code","dde88a0a":"code","609070c4":"code","fb2bc52a":"code","b127d3c5":"markdown","454ab654":"markdown","f697b6fb":"markdown","1008275e":"markdown","bb1d864a":"markdown","efecb383":"markdown","6ddad5a8":"markdown","6078de6d":"markdown","3a757268":"markdown","4a5b9bd4":"markdown","223001fd":"markdown","a1daffe4":"markdown","0e38cd21":"markdown","7a539c98":"markdown","8280c554":"markdown","69bb6247":"markdown","ece11c11":"markdown"},"source":{"fe0256c7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import shapiro\nfrom scipy.stats import randint\nfrom imblearn.over_sampling import SMOTE","4f31b50f":"data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndata.head()","d26a54c0":"data.describe()","796f12f5":"data.info()","8016435b":"data.isnull().sum()","6c5f5f13":"def data_pipeline(data):\n    data[\"Age\"].fillna((data[\"Age\"].mean()), inplace=True)\n    data = data.drop([\"Name\", \"Ticket\"], axis=1)\n    data.Sex.replace(to_replace=dict(female=1, male=0), inplace=True)\n    data.Embarked.replace(to_replace=dict(Q=1, C=2, S=3), inplace=True)\n    data[\"Embarked\"].fillna((data[\"Embarked\"].mean()), inplace=True)\n    data['Deck'] = data['Cabin'].str.extract('([A-Za-z])', expand=False)\n    data.Deck.replace(to_replace=dict(A=1, B=2, C=3, D=4, E=5, F=6, G=7, T=8), inplace=True)\n    data[\"Deck\"].fillna((data[\"Deck\"].median()), inplace=True)\n    data = data.drop([\"Cabin\"], axis=1)\n    data[\"Fare\"].fillna((data[\"Fare\"].mean()), inplace=True)\n    \n    return data","0dd7e0df":"data = data_pipeline(data)","4316b722":"data","03936c7b":"plt.figure(figsize=(15, 10))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(data))\nsns.stripplot(x=\"variable\", y=\"value\", data=pd.melt(data), color=\"orange\", jitter=0.2, size=2.5)\nplt.grid()","afbb9e0b":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\na=1\nplt.figure(figsize=(20, 10))\nfor i in data.columns:\n    plt.subplot(4, 3, a)\n    sns.distplot(data[i])\n    a += 1\nplt.show()","5fb5d1e7":"# Shapiro-Wilk Test\nfor col in data:\n    stat, p = shapiro(data[col])\n    print(\"----------------------------------------------\")\n    print(col)\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    # interpret\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)')\n    else:\n        print('Sample does not look Gaussian (reject H0)')","8472699e":"# Visualize correlations of each column (not necessary but for interest)\ncorrelations = data.corr(method=\"pearson\")\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlations, vmin= -1, cmap=\"coolwarm\", annot=True)","18e7413f":"plt.figure(figsize=(20, 8))\n\nplt.subplot(1, 2, 1)\nplt.ylim(0, 600)\nsns.countplot(data=data, x=\"Survived\")\n\n\nplt.subplot(1, 2, 2)\nfig_2 = sns.countplot(data=data, x=\"Sex\", hue=\"Survived\")\nfig_2.set_xticklabels([\"male\", \"female\"])\nplt.ylim(0, 600)\nplt.show(fig_2.containers[0])\nplt.show(fig_2.containers[1])\n\nplt.show()\n","8a4b9862":"plt.figure(figsize=(12, 8))\nsns.histplot(data=data, x=\"Age\", hue=\"Survived\", element=\"step\", kde=True)","ef40401c":"plt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.ylim(0, 550)\nfig_2 = sns.countplot(data=data, x=\"Pclass\")\nplt.bar_label(fig_2.containers[0])\n\n\nplt.subplot(1, 2, 2)\nfig_2 = sns.countplot(data=data, x=\"Pclass\", hue=\"Survived\")\nfig_2.set_xticklabels([\"1\", \"2\", \"3\"])\nplt.ylim(0, 550)\nplt.show(fig_2.containers[0])\nplt.show(fig_2.containers[1])","6dd1f3be":"plt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.ylim(0, 650)\nsns.countplot(data=data, x=\"SibSp\")\n\n\nplt.subplot(1, 2, 2)\nfig_4 = sns.countplot(data=data, x=\"SibSp\", hue=\"Survived\")\nplt.ylim(0, 650)\nplt.show(fig_4.containers[0])\nplt.show(fig_4.containers[1])\n","60fe10e3":"plt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.ylim(0, 700)\nsns.countplot(data=data, x=\"Parch\")\n\n\nplt.subplot(1, 2, 2)\nfig_4 = sns.countplot(data=data, x=\"Parch\", hue=\"Survived\")\nplt.ylim(0, 700)\nplt.show(fig_4.containers[0])\nplt.show(fig_4.containers[1])\n","b3f9e455":"x = data.drop(columns=[\"Survived\"])\ny = data[\"Survived\"]\n\n# Show distribution of 0 and 1\ny.value_counts()","78f9a7a8":"sm = SMOTE(random_state=42)\nx, y = sm.fit_resample(x, y)\ny.value_counts()","7c16a678":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=14)","dc624f5a":"from sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier, BaggingClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\n%matplotlib inline\n\n# Create empty list and append each model to list\nmodels = []\nmodels.append((\"SVC\", SVC(random_state=14)))\nmodels.append((\"SVM\", LinearSVC(random_state=14)))\nmodels.append((\"LOGR\", LogisticRegression(solver=\"liblinear\", random_state=14)))\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append((\"KNN\", KNeighborsClassifier()))\nmodels.append((\"CART\", DecisionTreeClassifier(random_state=14)))\nmodels.append((\"NB\", GaussianNB()))\nmodels.append((\"DT\", DecisionTreeClassifier(random_state=14)))\nmodels.append((\"RF\", RandomForestClassifier(random_state=14)))\nmodels.append((\"ET\", ExtraTreesClassifier(random_state=14)))\nmodels.append((\"GB\", GradientBoostingClassifier(random_state=14)))\nmodels.append((\"BC\", BaggingClassifier(random_state=14)))\n\n# Empty list for results of the evaluation\nmodel_results = []","649b848d":"# Function: for each element in model list there will be an evaluation -> Results will be added to results df\ndef train_all_models(models):\n    i = 1\n    plt.figure(figsize=(25, 15))\n    for method, model in models:\n        model.fit(x_train, y_train)\n        test_pred = model.predict(x_test)\n\n        f_score = model.score(x_test, y_test)\n        model_results.append((method, f_score))\n\n        plt.subplot(3, 4, i)\n        plt.subplots_adjust(hspace=0.3, wspace=0.3)\n        sns.heatmap(confusion_matrix(y_test, test_pred), annot=True, cmap=\"Greens\")\n        plt.title(model, fontsize=14)\n        plt.xlabel('Test', fontsize=12)\n        plt.ylabel('Predict', fontsize=12)\n        df = pd.DataFrame(model_results).transpose()\n        i+=1\n\n# Show confusion matrix for each trained model \n    plt.show()\n    df = pd.DataFrame(model_results)\n    return df","72144703":"# Sort results df for later visualizations    \nbest_models = train_all_models(models)\nbest_models = best_models.sort_values([1], ascending=False)","03827d02":"best_models","d08c2a4e":"y_pos = np.arange(len(best_models[0]))\nplt.figure(figsize=(10, 6))\nplt.bar(y_pos, best_models[1], color=(0.2, 0.4, 0.6, 0.6))\nplt.xticks(y_pos, best_models[0])\nplt.title('F-Score of all trained models')\nplt.xlabel('Model Type')\nplt.ylabel('F-Score')\nplt.show()","8d0f9055":"# Take top 3 models and define new -> for randomized search cv\ntop3_RF = RandomForestClassifier()\ntop3_ET = ExtraTreesClassifier()\ntop3_GB = GradientBoostingClassifier()\n\ntop3_RF.fit(x_train, y_train)\ntop3_ET.fit(x_train, y_train)\ntop3_GB.fit(x_train, y_train)","f1930004":"from sklearn.model_selection import RandomizedSearchCV\n\n# Grid Search for RandomForesClassifier\ngrid_param_RF = {\n    \"n_estimators\": randint(low=1, high=100),\n    \"max_depth\": randint(low=10, high=100),\n    \"max_features\": randint(low=1, high=4)\n}\n\nRF_grid_search = RandomizedSearchCV(estimator=top3_RF, param_distributions=grid_param_RF, cv= 10, verbose=1, random_state=14)\nRF_grid_search.fit(x_train, y_train)\n\nRF_best_grid = RF_grid_search.best_estimator_\nprint(RF_best_grid)\nprint(RF_grid_search.best_score_)","3ec166c1":"# Grid Search for ExtraTreesClassifier\ngrid_param_ET = {\n    \"n_estimators\": randint(low=1, high=100),\n    \"max_depth\": randint(low=10, high=100),\n    \"max_features\": randint(low=1, high=4)\n}\n\nET_grid_search = RandomizedSearchCV(estimator=top3_ET, param_distributions=grid_param_ET, cv= 10, verbose=1, random_state=14)\nET_grid_search.fit(x_train, y_train)\n\nET_best_grid = ET_grid_search.best_estimator_\n\nprint(ET_best_grid)\nprint(ET_grid_search.best_score_)","07a5e9ba":"# Grid Search for GradientBoostingClassifier\ngrid_param_GB = {\n    \"n_estimators\": randint(low=1, high=100),\n    \"max_depth\": randint(low=10, high=100),\n    \"max_features\": randint(low=1, high=4)\n}\n\nGB_grid_search = RandomizedSearchCV(estimator=top3_GB, param_distributions=grid_param_GB, cv= 10, verbose=1, random_state=14)\nGB_grid_search.fit(x_train, y_train)\n\nGB_best_grid = GB_grid_search.best_estimator_\n\nprint(GB_best_grid)\nprint(GB_grid_search.best_score_)","745f26c0":"# Show results of grid search\nprint(RF_best_grid, \"\\n Score: \", RF_grid_search.best_score_, \"\\n ------------------------------------\")\nprint(ET_best_grid, \"\\n Score: \", ET_grid_search.best_score_, \"\\n ------------------------------------\")\nprint(GB_best_grid, \"\\n Score: \", GB_grid_search.best_score_, \"\\n ------------------------------------\")","50e7c4ff":"# ExtraTreesClassifier is best one, create model and learn again with defined parameters of grid search\nbest_model = RandomForestClassifier(max_depth=20, max_features=1, n_estimators=62)\nbest_model.fit(x, y)","73aaccff":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","336d9310":"test_data = data_pipeline(test_data)\n\ntest_data.head()","7211182e":"test_data.isnull().sum()","dde88a0a":"test_data[\"Survived\"] = best_model.predict(test_data)","609070c4":"test_data[['PassengerId', 'Survived']].to_csv('submission.csv', index=False)","fb2bc52a":"submission = pd.read_csv(\".\/submission.csv\")\nsubmission.shape","b127d3c5":"#### Persons which are in the Age Group 30 are died the most but not in percentage:","454ab654":"# Preprocessing and Classification","f697b6fb":"#### According to the Classes 1-3 (1 = 1st class (best class) -> upper levels of the ship, 2 = 2nd class -> middle level of the ship, 3 = 3rd class -> lower levels of the ship) this means that passengers in the 3rd class had it harder to get on top than 1st class passengers:","1008275e":"### Visualization","bb1d864a":"# Data Preprocessing","efecb383":"### Outlier Visualization, Interpretation and Handling","6ddad5a8":"### Outlier handling\nWe do not handle any outliers for the following reason:\n- a woman, with age 70 in the 3rd class has it a lot harder than a man, with 30 ages and in the 1st class\n\nThis statement will be proven in the section visualizations...","6078de6d":"# Taking best Model and train it again with best Hyperparameters","3a757268":"#### On the basis of the knowledge gained from above the data in columns are not normal distributed.","4a5b9bd4":"### Prepare the models for testing","223001fd":"# Using the best_model for test.csv","a1daffe4":"#### Hypothesis 1: Not one of the columns are normal distriuted\n\nThis will be evaluated by using the p-value measurement:\n\n${\\displaystyle p=2\\min\\{\\Pr(T\\geq t\\mid H_{0}),\\Pr(T\\leq t\\mid H_{0})\\}}$ for a two-sided test. If distribution ${\\displaystyle T}$ is symmetric about zero, then ${\\displaystyle p=\\Pr(|T|\\geq |t|\\mid H_{0})}{\\displaystyle p=\\Pr(|T|\\geq |t|\\mid H_{0})}$","0e38cd21":"#### More Men than Women died:","7a539c98":"# Titanic Prediction\/Classification","8280c554":"# Data Interpretation and Visualization","69bb6247":"### Hyperparameter Tuning of best 3 models","ece11c11":"#### First of all we split the Survived column from the other columns and use SMOTE to make the \"Survived\" column distribution equal"}}