{"cell_type":{"733d5ffd":"code","a71e7702":"code","e6040a46":"code","26af6ea4":"code","1449eee4":"code","d4576c34":"code","7f4b58dd":"code","48064111":"code","ea702ad1":"code","cf662893":"code","80191880":"code","582c3829":"code","cb59cc6c":"code","3b94ada5":"code","99249d7d":"code","fffe4e79":"code","7a56b9af":"code","a6614a46":"code","c4988cb7":"code","68b7584e":"code","87432c03":"code","adcdc72d":"code","97a47a96":"code","2d1b9281":"code","4a0fc26d":"code","b65ac07f":"code","ab5edc31":"code","6d3b7d08":"markdown","025e1855":"markdown","1aa5a927":"markdown","b5f28af3":"markdown","65ac9bd7":"markdown","d7fc613e":"markdown","45a0cbf3":"markdown","76d7e6ff":"markdown","837e78ef":"markdown","00719e27":"markdown","9f0e80b4":"markdown","5e366b06":"markdown","43e7a739":"markdown","2e2f4e80":"markdown","efdb3ed8":"markdown","7b530b19":"markdown","9449e9de":"markdown","74ebed09":"markdown","d594c2fc":"markdown","cb51c1b5":"markdown"},"source":{"733d5ffd":"import numpy as np\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\nfrom keras.layers import Dense, LSTM, Input, Embedding, Dropout, Bidirectional\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils, get_file, to_categorical\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow_addons as tfa\n\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom tqdm import tqdm_notebook as tqdm\nimport random","a71e7702":"path = get_file(\n    'hobbit.txt',\n    'https:\/\/archive.org\/stream\/TheHobbitJ.R.R.Tolkien\/The%20Hobbit%20-%20J.%20R.%20R.%20Tolkien_djvu.txt'\n)","e6040a46":"with open(path, encoding=\"utf-8\") as f:\n    text = f.read()\n\ntext = text.lower()\n\nsoup = BeautifulSoup(text, 'lxml')\ntext = soup.text.replace('\\n', ' ')\n\ntext = \" \".join(text.split())","26af6ea4":"begin_train = [(i.start(), i.end()) for i in re.finditer('an unexpected party', text)]\nend_train = [(i.start(), i.end()) for i in re.finditer('parts between elves and dwarves and men.', text)]","1449eee4":"train_start, train_end = begin_train[-1][0], end_train[-1][1]\ntrain_start, train_end","d4576c34":"train = text[train_start - 10 : train_end]\ntrain[:50], train[-50:]","7f4b58dd":"all_chapter = [(i.start(), i.end()) for i in re.finditer('chapter', train)]\n\nfor i in range(len(all_chapter)):\n    print(train[all_chapter[i][0] : all_chapter[i][0] + 50])","48064111":"all_chapter = [re.search('(?<=chapter )(\\w+)', train[all_chapter[i][0] : all_chapter[i][0] + 50]).group(1) for i in range(len(all_chapter))]\n\nall_chapter.remove('and')\nall_chapter","ea702ad1":"for next_word in all_chapter:\n    substring = 'chapter ' + next_word + ' '\n    train = train.replace(substring, '||| ')","cf662893":"train[:50]","80191880":"train = re.sub('([!>#$ %&()*+,-.\/:;<=>?@[\\][\\\\]\u25a0\u2022^_`\u201c\u201d\u2019\u2018\u00a3\u00ac{}~\u2014])', r' \\1 ', train)\ntrain = re.sub('\\s{2,}', ' ', train)","582c3829":"train[:100]","cb59cc6c":"train[-1191:]","3b94ada5":"test = train[-1191:]\ntrain = train[:-1191]","99249d7d":"tokenizer = Tokenizer(char_level=False, filters='')\ntokenizer.fit_on_texts([train])\ntotal_words = len(tokenizer.word_index) + 1\n# total_words = len(tokenizer.word_index)\ntrain = tokenizer.texts_to_sequences([train])[0]","fffe4e79":"total_words","7a56b9af":"window = 30\nstep = 1","a6614a46":"train_sentences = []\npredict_char = []\n\nfor ind in tqdm(range(0, len(train) - window, step)):\n    train_sentences.append(train[ind : ind + window])\n    predict_char.append(train[ind + window])","c4988cb7":"train_sentences = np.array(train_sentences)\npredict_char = to_categorical(predict_char, num_classes=total_words)","68b7584e":"test = tokenizer.texts_to_sequences([test])[0]\n\ntest_sentences = np.array([test[ind : ind + window] for ind in tqdm(range(0, len(test) - window, step))])\nlen(test_sentences)","87432c03":"text_in = Input(shape=(None,))\nx = Embedding(total_words, 250)(text_in)\nx = Bidirectional(LSTM(256))(x)\n# x = Bidirectional(LSTM(256, return_sequences=True))(text_in)\n# x = LSTM(256)(x)\nx = Dropout(0.2)(x)\ntext_out = Dense(total_words, activation='softmax')(x)","adcdc72d":"model = Model(text_in, text_out)\noptimizer = Adam(lr=1e-2)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\nmodel.summary()","97a47a96":"checkpoint_path = 'bestmodel.hdf5'\n\ncheckpoint = ModelCheckpoint(checkpoint_path, monitor='categorical_crossentropy', verbose=0, save_best_only=True, mode='min')\n\n# decrease learning rate\nscheduler = LearningRateScheduler(lambda epoch, lr: lr * 0.9, verbose=1)\n\n# meaningless, since without validation\n# early_stop = EarlyStopping(monitor='val_categorical_crossentropy', min_delta=0, patience=3, mode='min', verbose=1)\n\ntqdm_callback = tfa.callbacks.TQDMProgressBar(\n    leave_epoch_progress=False, \n    leave_overall_progress=True, \n    show_epoch_progress=False,\n    show_overall_progress=True\n)\n\ncallbacks_list = [\n    checkpoint, \n    scheduler,\n#     early_stop,\n    tqdm_callback\n]","2d1b9281":"history = model.fit(train_sentences, predict_char, epochs=15, batch_size=128, callbacks=callbacks_list, verbose=1, shuffle=True)","4a0fc26d":"def redistribution(predict, temp=0.5, eps=0):\n    predict = np.asarray(predict).astype(np.float64)\n    predict = np.log(predict + eps) \/ temp\n    exp = np.exp(predict)\n    return np.argmax(np.random.multinomial(1, exp \/ np.sum(exp), 1))\n\nredistribution([0.2, 0.3, 0.5]), redistribution([0.2, 0.7, 0.1])","b65ac07f":"len_generate = 50","ab5edc31":"# any\nsent = random.choice(test_sentences)\n    \nfor i in range(len_generate):\n    pred = model.predict(np.reshape(sent[-window:], (1, window)), verbose=0)[0]\n    pred = redistribution(pred, temp=1)\n    \n    word = tokenizer.index_word[pred]\n    sent = np.append(sent, pred)\n\n    \nprint('Initial:')\n\nfor i, word in enumerate(sent):\n    # split initial and predicted parts\n    if i == window:\n        print('\\nPredict:')\n    print(tokenizer.index_word[word], end=' ')","6d3b7d08":"Number of generated words","025e1855":"### Separation of many punctuation marks and symbols with a space","1aa5a927":"## Sampling","b5f28af3":"## Import libraries","65ac9bd7":"And replacement with |||","d7fc613e":"### Deleting \"chapters\"","45a0cbf3":"## Predict","76d7e6ff":"### Indices","837e78ef":"Weighting the logarithm of probabilities with the parameter","00719e27":"### Beginning and end of the story","9f0e80b4":"### Tokenizer","5e366b06":"Delete \"chapter and\"","43e7a739":"Tokenizer application for test part","2e2f4e80":"Word count and selection step","efdb3ed8":"### Separation of the test part (so that the neural network does not see)","7b530b19":"## LSTM","9449e9de":"## Loading a book","74ebed09":"## Preprocessing text","d594c2fc":"Roman numerals were read as \"i\", \"x\", so there are 19 chapters. Except \"chapter and\" - a phrase from the story","cb51c1b5":"Into categorical format"}}