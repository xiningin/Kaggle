{"cell_type":{"cf585197":"code","a2d595e6":"code","f19ebbd4":"code","ce3d0cea":"code","2be2531d":"code","11692144":"code","469fb799":"code","fce96e31":"code","34132070":"code","d3d9b9d7":"code","fa00d75b":"code","c1dc0fd1":"code","00bc85f6":"code","077dfd47":"code","bfe13a07":"code","e900f18c":"code","d83e792f":"code","0d768ce5":"code","380877d3":"code","87e0d679":"markdown","ecd93308":"markdown","2bd750c7":"markdown","4447b22c":"markdown","719c569b":"markdown","8963ff2c":"markdown","dac2c65c":"markdown","85b3bc7c":"markdown","e0a553ee":"markdown","d64fdd53":"markdown","8f813161":"markdown","1ec2cc0f":"markdown","5e70aa8a":"markdown","74338a6e":"markdown","e3b8e105":"markdown","9f07ce4c":"markdown","d2c90143":"markdown","2f21659e":"markdown","eb0d8a2d":"markdown"},"source":{"cf585197":"# Install TensorFlow\n# !pip install -q tensorflow-gpu==2.0.0-rc0\n\ntry:\n  %tensorflow_version 2.x  # Colab only.\nexcept Exception:\n  pass\n\nimport tensorflow as tf\nprint(tf.__version__)","a2d595e6":"# More imports\nfrom tensorflow.keras.layers import Input, Dense, LeakyReLU, Dropout, \\\n  BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import SGD, Adam\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sys, os","f19ebbd4":"mnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n# map inputs to (-1, +1) for better training\nx_train, x_test = x_train \/ 255.0 * 2 - 1, x_test \/ 255.0 * 2 - 1\nprint(\"x_train.shape:\", x_train.shape)","ce3d0cea":"# Flatten the data\nN, H, W = x_train.shape\nD = H * W\nx_train = x_train.reshape(-1, D)\nx_test = x_test.reshape(-1, D)","2be2531d":"# Dimensionality of the latent space\nlatent_dim = 100","11692144":"def build_generator(latent_dim):\n  i = Input(shape=(latent_dim,))\n  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n  x = BatchNormalization(momentum=0.7)(x)\n  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n  x = BatchNormalization(momentum=0.7)(x)\n  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n  x = BatchNormalization(momentum=0.7)(x)\n  x = Dense(D, activation='tanh')(x)\n\n  model = Model(i, x)\n  return model","469fb799":"def build_discriminator(img_size):\n  i = Input(shape=(img_size,))\n  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n  x = Dense(1, activation='sigmoid')(x)\n  model = Model(i, x)\n  return model","fce96e31":"# Build and compile the discriminator\ndiscriminator = build_discriminator(D)\ndiscriminator.compile(\n    loss='binary_crossentropy',\n    optimizer=Adam(0.0002, 0.5),\n    metrics=['accuracy'])\n\n# Build and compile the combined model\ngenerator = build_generator(latent_dim)\n\n# Create an input to represent noise sample from latent space\nz = Input(shape=(latent_dim,))\n\n# Pass noise through generator to get an image\nimg = generator(z)\n\n# Make sure only the generator is trained\ndiscriminator.trainable = False\n\n# The true output is fake, but we label them real!\nfake_pred = discriminator(img)\n\n# Create the combined model object\ncombined_model = Model(z, fake_pred)\n\n# Compile the combined model\ncombined_model.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))","34132070":"# Config\nbatch_size = 32\nepochs = 30000\nsample_period = 200 # every `sample_period` steps generate and save some data\n\n\n# Create batch labels to use when calling train_on_batch\nones = np.ones(batch_size)\nzeros = np.zeros(batch_size)\n\n# Store the losses\nd_losses = []\ng_losses = []\n\n# Create a folder to store generated images\nif not os.path.exists('gan_images'):\n  os.makedirs('gan_images')","d3d9b9d7":"# A function to generate a grid of random samples from the generator\n# and save them to a file\ndef sample_images(epoch):\n  rows, cols = 5, 5\n  noise = np.random.randn(rows * cols, latent_dim)\n  imgs = generator.predict(noise)\n\n  # Rescale images 0 - 1\n  imgs = 0.5 * imgs + 0.5\n\n  fig, axs = plt.subplots(rows, cols)\n  idx = 0\n  for i in range(rows):\n    for j in range(cols):\n      axs[i,j].imshow(imgs[idx].reshape(H, W), cmap='gray')\n      axs[i,j].axis('off')\n      idx += 1\n  fig.savefig(\"gan_images\/%d.png\" % epoch)\n  plt.close()","fa00d75b":"for epoch in range(epochs):\n  ###########################\n  ### Train discriminator ###\n  ###########################\n  \n  # Select a random batch of images\n  idx = np.random.randint(0, x_train.shape[0], batch_size)\n  real_imgs = x_train[idx]\n  \n  # Generate fake images\n  noise = np.random.randn(batch_size, latent_dim)\n  fake_imgs = generator.predict(noise)\n  \n  # Train the discriminator\n  # both loss and accuracy are returned\n  d_loss_real, d_acc_real = discriminator.train_on_batch(real_imgs, ones)\n  d_loss_fake, d_acc_fake = discriminator.train_on_batch(fake_imgs, zeros)\n  d_loss = 0.5 * (d_loss_real + d_loss_fake)\n  d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n  \n  \n  #######################\n  ### Train generator ###\n  #######################\n  \n  noise = np.random.randn(batch_size, latent_dim)\n  g_loss = combined_model.train_on_batch(noise, ones)\n  \n  # do it again!\n  noise = np.random.randn(batch_size, latent_dim)\n  g_loss = combined_model.train_on_batch(noise, ones)\n  \n  # Save the losses\n  d_losses.append(d_loss)\n  g_losses.append(g_loss)\n  \n  if epoch % 100 == 0:\n    print(f\"epoch: {epoch+1}\/{epochs}, d_loss: {d_loss:.2f}, \\\n      d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n  \n  if epoch % sample_period == 0:\n    sample_images(epoch)","c1dc0fd1":"plt.plot(g_losses, label='g_losses')\nplt.plot(d_losses, label='d_losses')\nplt.legend()","00bc85f6":"!ls gan_images","077dfd47":"from skimage.io import imread\na = imread('gan_images\/0.png')\nplt.imshow(a)","bfe13a07":"a = imread('gan_images\/1000.png')\nplt.imshow(a)","e900f18c":"a = imread('gan_images\/5000.png')\nplt.imshow(a)","d83e792f":"a = imread('gan_images\/10000.png')\nplt.imshow(a)","0d768ce5":"a = imread('gan_images\/20000.png')\nplt.imshow(a)","380877d3":"a = imread('gan_images\/29800.png')\nplt.imshow(a)","87e0d679":"# Load in the data","ecd93308":"# How do GANs work?","2bd750c7":"# Compile both models in preparation for training","4447b22c":"# Main training loop","719c569b":"# Introduction","8963ff2c":"Here are some example images produced by State of the Art.\n![](https:\/\/github.com\/yvtsanlevy\/KaggleProjects\/blob\/main\/images\/GAN1.JPG?raw=true)","dac2c65c":"since the discriminator must be able to tell the difference between a real and fake images.\n\nThis is a binary classification problem for every input.\n\nThat discriminator can only predict one of two categories, real or fake.\n\nAs you know, for binary classification, the Carex lost function is the binary cross entropy.\n\nWhat about for the generator?\n\nWhat is the last function?\n\nThe basic idea is this.\n\nWe have our generator network and our generator network feeds into our discriminator network. From this perspective, this is just one gigantic neural network.\n\nWhat we can do is just like with transfer learning, we can freeze the layers in the discriminator so that when we run gradient descent, only the generator network gets trained next.\n\nWe want the generator to get better at producing real looking images.\n\nAnd so our lost function will remain the binary cross entropy, except that we are going to switch the labels.\n\nIn other words, let's say for the discriminator, real is one, in effect zero.\n\nWe're going to pass in fake images, but make the target one such that we're encouraging the discriminator to identify these fake images as real. But the discriminator weights are frozen, so they won't change when we do this.\n\nOnly the generator weights are trained.","85b3bc7c":"https:\/\/towardsdatascience.com\/a-basic-intro-to-gans-generative-adversarial-networks-c62acbcefff3\n\nThe Lazy Programmer","e0a553ee":"Believe it or not, these are not real people, but rather images generated by a computer.\n\nOne thing you can do if you want to see a few more examples and observe for yourself, the power of GAN's is to check out a few websites that display such examples.\n\nOne website is https:\/\/thispersondoesnotexist.com\/. This one is simple. You go to the page and it shows you a randomly generated face. You can refresh the page to see more faces.\n\nAnother one, which I really like is https:\/\/www.whichfaceisreal.com\/.\n\nThis one is more like a game. So you're shown two images and one is real, one is fake and you have to pick the right one.\n\nSo check these out for yourself.","d64fdd53":"# Get the discriminator model","8f813161":"# Get the generator model","1ec2cc0f":"Generative Adversarial Networks, or GANs for short, are an approach to generative modeling using deep learning methods, such as convolutional neural networks.\n\nGenerative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.","5e70aa8a":"# with Gan's, what is the input?","74338a6e":"We know that neural networks must have both inputs and outputs for the discriminator.\n\nThis is the input as an image, and the output is a prediction about whether the input image is real or fake.\n\nFor the generator. This will be a little strange.\n\nWe know that the output should be an image because its job is to generate images.\n\nBut what is its input?\n\nIn fact, the input to the generator is nothing but noise, what we are going to do is generate noise from a multi very standard normal and the generator will learn to map this noise to an image.\n","e3b8e105":"# References","9f07ce4c":"Gan's , a system of two neural networks, each of which has their own objective?\n\nThe objective of the first known network is to generate images.\n\nThe objective of the second neural network is to discriminate between real and fake images.\n\nThis is all you need to understand how GANs get their name.\n\nThey are generative because you are generating images.\n\nThey are adversarial because you have two neural networks which oppose each other in an adversarial manner.\n\n#They have opposite goals.#\n\nA goal of the first known network is to generate images that look real, and the goal of the second neural network is to detect real and fake images.\n\n![](https:\/\/github.com\/yvtsanlevy\/KaggleProjects\/blob\/main\/images\/GAN2.JPG?raw=true)","d2c90143":"# with Gan's, what is the lost function?","2f21659e":"# Train the GAN","eb0d8a2d":"# lets start"}}