{"cell_type":{"395371a8":"code","7cd69932":"code","2458bacb":"code","9c25bac5":"code","efc3320c":"code","d414ef18":"code","e600b700":"code","078f7121":"code","69916196":"code","28fafae8":"code","332af836":"code","0c3bdf9d":"code","ab5d8898":"code","945ccc73":"code","7a5e647a":"code","b7ea1404":"code","3264aecd":"code","1c845504":"code","cf24838e":"code","f1773902":"code","ef46cf00":"markdown","746433e9":"markdown","95a0c94c":"markdown","af186f5e":"markdown","dc85b9b7":"markdown","7b3efe16":"markdown","642dfb3d":"markdown","4c55c31a":"markdown","d5dd7dd3":"markdown","3ab5102f":"markdown","d08f9385":"markdown","64c4cd34":"markdown"},"source":{"395371a8":"import numpy as np\nimport matplotlib.pyplot as plt \nimport pandas as pd  \nimport seaborn as sns \n\nfrom sklearn.metrics import mean_squared_log_error\n\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom catboost import CatBoostRegressor\nfrom datetime import datetime\nfrom skmultilearn.problem_transform import ClassifierChain","7cd69932":"train = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/test.csv')\n# load submission\npreds = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')","2458bacb":"# Dropping the last row which is 2011-01-01 00:00:00\ntrain=train.loc[~(train['date_time']=='2011-01-01 00:00:00')].reset_index(drop=True)","9c25bac5":"all_data = pd.concat([train, test])\n# convert to datatime format\nall_data['date_time'] = pd.to_datetime(all_data['date_time'])\nall_data.head()","efc3320c":"#Year, Month, Day, Hour, Is_WeekEnd, SMC\nall_data['Hour'] = all_data['date_time'].dt.hour\nall_data[\"working_hours\"] =  all_data[\"Hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\nall_data[\"is_weekend\"] = (all_data[\"date_time\"].dt.dayofweek >= 5).astype(\"int\")\nall_data['satday'] = (all_data.date_time.dt.weekday==5).astype(\"int\")\nall_data['Hour'] = all_data.date_time.dt.hour*60+all_data.date_time.dt.minute\nall_data['time'] = all_data['date_time'].astype(np.int64)\/\/10**9\nall_data[\"SMC\"] = (all_data[\"absolute_humidity\"] * 100) \/ all_data[\"relative_humidity\"]","d414ef18":"import math\n\nall_data['sensor_6'] = (all_data['sensor_2'] - all_data['sensor_5']) \/ all_data['sensor_5']\nall_data['sensor_7'] = (all_data['sensor_3'] - all_data['sensor_4']) \/ all_data['sensor_4']\nall_data['is_odd'] = (all_data['sensor_4'] < 646) & (all_data['absolute_humidity'] < 0.238)\nall_data['day'] = all_data.date_time.dt.weekday\n\nfor periods in [3, 6]:\n        all_data[f'dt-{periods}'] = all_data['deg_C'] - all_data['deg_C'].shift(periods=periods, \n                                                                                fill_value=0)\ndiff = all_data['date_time'] - min(all_data['date_time'])\ntrend = diff.dt.days\n\nall_data['f1s'] = np.sin(trend * 2 * math.pi \/ (365 * 1)) \nall_data['f1c'] = np.cos(trend * 2 * math.pi \/ (365 * 1))\nall_data['f2s'] = np.sin(2 * math.pi * trend \/ (365 * 2)) \nall_data['f2c'] = np.cos(2 * math.pi * trend \/ (365 * 2)) \nall_data['f3s'] = np.sin(2 * math.pi * trend \/ (365 * 3)) \nall_data['f3c'] = np.cos(2 * math.pi * trend \/ (365 * 3)) \nall_data['f4s'] = np.sin(2 * math.pi * trend \/ (365 * 4)) \nall_data['f4c'] = np.cos(2 * math.pi * trend \/ (365 * 4)) \nall_data['fh1s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\nall_data['fh1c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 1))\nall_data['fh2s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\nall_data['fh2c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 2))\nall_data['fh3s'] = np.sin(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))\nall_data['fh3c'] = np.cos(diff.dt.seconds * 2 * math.pi \/ ( 3600 * 24 * 3))","e600b700":"all_data['deg_C_is_odd'] = all_data['deg_C'].between(22.0, 26.5).astype('int')\nall_data['sensor_3_is_odd'] = all_data['sensor_3'].between(1500, 2100).astype('int')","078f7121":"train.hist(bins=15,figsize=(15,10),color='gray')\nplt.show()","69916196":"from scipy import stats\n#Corelation of the Target Variable.\ntarget=['target_benzene','target_carbon_monoxide','target_nitrogen_oxides']\nplt.figure(figsize = (10, 8))\n\nfor t in target:\n    fig, ax = plt.subplots(figsize=(8,4))\n    plt.xlabel(t); plt.ylabel(t + 'Target Density'); \n    plt.title('Distribution of Target');\n    \n    sns.kdeplot(train[t] , label = t,color=\"black\",)\n    \n    plt.grid()\n    plt.show()\n    \n    fig, ax = plt.subplots(figsize=(8,4))\n    res = stats.probplot(train[t],plot=plt)\n    plt.show()\n    ","28fafae8":"# Reference : https:\/\/www.kaggle.com\/anjalianupam\/tps-july-eda\n\ntarget=['target_benzene','target_carbon_monoxide','target_nitrogen_oxides']\ncolor=['red','black','blue']\ni = 0\nfor t in target:\n    my_alpha=0.25\n    fig, ax = plt.subplots(figsize=(16,4))\n    ax.scatter(all_data['date_time'].apply(lambda x: str(x).split(\" \")[0]), all_data[t], alpha=my_alpha, \n               color=color[i], s=4)\n    ax.xaxis.set_major_locator(plt.MaxNLocator(20)) # reduce number of x-axis labels\n    plt.xticks(rotation=90)\n    plt.title(t)\n    plt.grid()\n    plt.show()\n    i+=1","332af836":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","0c3bdf9d":"# Split train data\nmonths = all_data[\"date_time\"].dt.month[:len(train)]\nall_data.drop(columns = 'date_time', inplace = True)\ntrain_data = all_data.loc[~all_data.target_nitrogen_oxides.isnull()].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\ny = all_data.loc[~all_data.target_nitrogen_oxides.isnull()][['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']]\n\ny_log=np.log1p(y)\n\n# Split test data\ntest_data = all_data.loc[all_data.target_nitrogen_oxides.isnull()].drop(columns = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides'])\ntrain_data.shape,y.shape,test_data.shape,all_data.shape","ab5d8898":"#histogram and normal probability plot\nfrom scipy.stats import norm\ntarget=['target_benzene','target_carbon_monoxide','target_nitrogen_oxides']\nfor t in target:\n    fig = plt.figure()\n    sns.distplot(y_log[t], fit=norm);\n    res = stats.probplot(y_log[t], plot=plt)","945ccc73":"train_data.columns","7a5e647a":"# cols = ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1', 'sensor_2', 'sensor_3', \n#  'sensor_4', 'sensor_5', 'working_hours', 'is_weekend', 'time']\n\ncols = ['deg_C', 'relative_humidity', 'absolute_humidity', 'sensor_1',\n       'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5','sensor_6', 'sensor_7',\n        'is_odd',\n        #'deg_C_is_odd', 'sensor_3_is_odd',\n       'dt-3', 'dt-6', 'f1s', 'f1c',\n       'f2s', 'f2c', 'f3s', 'f3c', 'f4s', 'f4c', \n        'fh1s', 'fh1c', 'fh2s','fh2c', 'fh3s', 'fh3c',\n        'is_weekend', 'working_hours','time', 'SMC',]\n\n# train_data = train_data[cols]\n# test_data = test_data[cols]\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(train_data[cols])\ntrain_data_s = scaler.transform(train_data[cols])\ntest_data_s = scaler.transform(test_data[cols])","b7ea1404":"# https:\/\/www.kaggle.com\/andy6804tw\/catboost-13feature-cross-validation\ncb_params = [\n                {'learning_rate': 0.010169009412219588,\n                 'l2_leaf_reg': 8.908337085912136,\n                 'bagging_temperature': 8.384477224270551,\n                 'random_strength': 1.950237493637981,\n                 'depth': 6,\n                 'grow_policy': 'Lossguide',\n                 'leaf_estimation_method': 'Newton'},\n                {'learning_rate': 0.166394867169309,\n                 'l2_leaf_reg': 8.704675157564441,\n                 'bagging_temperature': 3.340826164726799,\n                 'random_strength': 1.538518016574368,\n                 'depth': 2,\n                 'grow_policy': 'Depthwise',\n                 'leaf_estimation_method': 'Newton'},\n                {'learning_rate': 0.028141156076957437,\n                 'l2_leaf_reg': 3.116523267336638,\n                 'bagging_temperature': 4.420661209459851,\n                 'random_strength': 1.8011752694610028,\n                 'depth': 6,\n                 'grow_policy': 'Depthwise',\n                 'leaf_estimation_method': 'Newton'},\n            ]","3264aecd":"model_preds = 0\ntarget_names=y_log.columns\n\n# for i, target in enumerate(target_names):\n#     print(f\"\\nTraining for {target}...\")\nclassifier = ClassifierChain(CatBoostRegressor(random_state=2021,\n                                 thread_count=4,\n                                 verbose=False,\n                                 loss_function='RMSE',\n                                 eval_metric='RMSE',\n                                 od_type=\"Iter\",\n                                 early_stopping_rounds=500,\n                                 #use_best_model=True,\n                                 iterations=10000,\n                                 **cb_params[0]))\n\nclassifier.fit(train_data_s, y_log.values)\n\nmodel_preds = np.expm1(pd.DataFrame.sparse.from_spmatrix(classifier.predict(test_data_s)))        ","1c845504":"preds = pd.read_csv('..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv')\npreds[target_names] = model_preds","cf24838e":"preds.to_csv('submission.csv', index=False)","f1773902":"# finish","ef46cf00":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Intepretations.<br>\n    1. We Observe that the target variables shows peakedness and a positive skew.<br>\n    2. It would be good to apply a Log transformation to this data.<br><\/div>","746433e9":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Intepretations.<br>\n    1. As we have noted above scaling the data will be appropriate.<br>\n    2. We have also applied a Log Transformation to the target variable<br><\/div>","95a0c94c":"#### *Reference & Credits:* \nhttps:\/\/www.kaggle.com\/andy6804tw\/catboost-13feature-cross-validation\n<br>\nhttps:\/\/www.kaggle.com\/anjalianupam\/tps-july-eda\n<br>\nhttps:\/\/www.kaggle.com\/hiro5299834\/tps-jul-2021-votingregressor-leaveonegroupout\n<br>\nhttps:\/\/www.kaggle.com\/alexryzhkov\/tps-lightautoml-baseline-with-pseudolabels","af186f5e":"## \ud83d\udc4d UpVote if you find helpful.","dc85b9b7":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Intepretations.<br>\n1.    Sensor data for Sensor 1, 2 and 3 is positively skewed.<br>\n2.     So is the Target data<br>\n3.     You also see a peaks in the Sensor data.<br><\/div>","7b3efe16":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Intepretations.<br>\n    1. There appears to be a corelation of the peaks and lows with months.<br>\n    2. Later part of the year Nitrogen Oxides significantly rise<br>\n    3. There is also a notable dip in the month of August across all target variables<br><\/div>","642dfb3d":"### Data Visualization","4c55c31a":"<div class=\"alert alert-block alert-info\"> \ud83d\udccc Intepretations.<br>\n    1. Sensor 3 data appears to have a high negative corelation with teh target variable and other sensor data.<br>\n    2. Sensor 1, 2 and 5 appears to have a positive corelation with the target and other Sensor data\n    <br><\/div>","d5dd7dd3":"## Split Train and Test","3ab5102f":"### Data Features","d08f9385":"<p style=\"padding: 4 px;color:black;\">\n        <h3 style=\"padding: 3px;color:black;\" > Data Features <\/h3>\n        <h3 style=\"padding: 3px;color:black;\" > Data Visualization <\/h3>        \n        <h3 style=\"padding: 3px;color:black;\"> Model Training <\/h3>\n        <h3 style=\"padding: 3px;color:black;\"> Submission <\/h3>\n<\/p>","64c4cd34":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#000066;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"padding: 15px;\n              color:white;\">Tabular Playground Series - July 2021 <\/h1>\n\n<\/div>"}}