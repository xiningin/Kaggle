{"cell_type":{"1504063a":"code","706bef6a":"code","bb559147":"code","cd484637":"code","6baef959":"code","39fa5890":"code","e13162cf":"code","f5bbe1a6":"code","28e7cc74":"code","3ddd3e2d":"code","2d7b4995":"code","954d3f91":"code","c298b115":"code","66752293":"code","7f3aadd5":"code","bcbde841":"code","d6b539a1":"code","0ee7690a":"code","d5a15743":"code","f2f68791":"code","25c89076":"code","96f9602f":"code","ea96fa2c":"markdown","50c7a269":"markdown","0b5cce39":"markdown","7f553dba":"markdown","47651ffc":"markdown","c5246fa8":"markdown","87b56eaa":"markdown","075e2896":"markdown","91882ac1":"markdown","115c93aa":"markdown","8b0d8241":"markdown","0bdc65a1":"markdown","376c3fab":"markdown","4d28dc24":"markdown","6dc6a87f":"markdown","ea8290b7":"markdown"},"source":{"1504063a":"import re\nimport pickle\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","706bef6a":"# Importing the dataset\nDATASET_COLUMNS  = [\"sentiment\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding=DATASET_ENCODING , names=DATASET_COLUMNS)\n\n# Removing the unnecessary columns.\ndataset = dataset[['sentiment','text']]\n# Replacing the values to ease understanding.\ndataset['sentiment'] = dataset['sentiment'].replace(4,1)\n\n# Plotting the distribution for dataset.\nax = dataset.groupby('sentiment').count().plot(kind='bar', title='Distribution of data',\n                                               legend=False)\nax.set_xticklabels(['Negative','Positive'], rotation=0)\n\n# Storing data in lists.\ntext, sentiment = list(dataset['text']), list(dataset['sentiment'])","bb559147":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n\n## Defining set containing all stopwords in english.\nstopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']","cd484637":"from nltk.stem import WordNetLemmatizer\n    ","6baef959":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer\n    wordLemm = WordNetLemmatizer()\n    \n    # Regex patterns\n    urlPattern = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            # If word not in stopwordlist\n            if len(word)>1:\n                \n                word = wordLemm.lemmatize(word)\n            \n                tweetwords += (word+' ')\n        processedText.append(tweetwords)\n        \n    return processedText","39fa5890":"import time\nt = time.time()\nprocessedtext = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","e13162cf":"data_neg = processedtext[:800000]\nplt.figure(figsize=(20,20))\nwc = WordCloud(max_words = 1000, width = 1600, height = 800, collocations=False).generate(\" \".join(data_neg))\nplt.imshow(wc)","f5bbe1a6":"data_pos = processedtext[800000:]\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n              collocations=False).generate(\" \".join(data_pos))\nplt.figure(figsize = (20,20))\nplt.imshow(wc)","28e7cc74":"from sklearn.model_selection import train_test_split","3ddd3e2d":"X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n                                                    test_size = 0.05, random_state = 0)\nprint(f'Data Split done.')","2d7b4995":"from sklearn.feature_extraction.text import TfidfVectorizer","954d3f91":"vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=50000)\nvectorizer.fit(X_train)\nprint(f'Vectorizer fitted')\nprint('No. of feature_words: ',len(vectorizer.get_feature_names()))","c298b115":"X_train = vectorizer.transform(X_train)\nX_test = vectorizer.transform(X_test)\nprint(f'Data Transformed')","66752293":"from sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.linear_model import LogisticRegression","7f3aadd5":"from sklearn.metrics import confusion_matrix, classification_report ","bcbde841":"import seaborn as sns","d6b539a1":"def model_evaluate(model):\n    y_pred = model.predict(X_test)\n    \n    # classification report\n    print(classification_report(y_test, y_pred))\n    \n    # confusion report\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    \n    categories = ['Negative', 'Positive']\n    \n    group_names = ['True Neg', 'False Pos','False Neg','True Pos']\n    \n    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() \/ np.sum(cf_matrix)]\n\n    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","0ee7690a":"BNBmodel = BernoulliNB(alpha = 2)\nBNBmodel.fit(X_train, y_train)\nmodel_evaluate(BNBmodel)","d5a15743":"SVCmodel = LinearSVC()\nSVCmodel.fit(X_train, y_train)\nmodel_evaluate(SVCmodel)","f2f68791":"LRmodel = LogisticRegression(C =2, max_iter=1000, n_jobs=1)\nLRmodel.fit(X_train, y_train)\nmodel_evaluate(LRmodel)","25c89076":"file = open('vectoriser-ngram-(1,2).pickle','wb')\npickle.dump(vectorizer, file)\nfile.close()\n\nfile = open('Sentiment-LR.pickle','wb')\npickle.dump(LRmodel, file)\nfile.close()\n\nfile = open('Sentiment-BNB.pickle','wb')\npickle.dump(BNBmodel, file)\nfile.close()","96f9602f":"def load_models():\n    '''\n    Replace '..path\/' by the path of the saved models.\n    '''\n    \n    # Load the vectoriser.\n    file = open('..path\/vectoriser-ngram-(1,2).pickle', 'rb')\n    vectoriser = pickle.load(file)\n    file.close()\n    # Load the LR Model.\n    file = open('..path\/Sentiment-LRv1.pickle', 'rb')\n    LRmodel = pickle.load(file)\n    file.close()\n    \n    return vectoriser, LRmodel\n\ndef predict(vectoriser, model, text):\n    # Predict the sentiment\n    textdata = vectorizer.transform(preprocess(text))\n    sentiment = model.predict(textdata)\n    \n    # Make a list of text with sentiment.\n    data = []\n    for text, pred in zip(text, sentiment):\n        data.append((text,pred))\n        \n    # Convert the list into a Pandas DataFrame.\n    df = pd.DataFrame(data, columns = ['text','sentiment'])\n    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n    return df\n\nif __name__==\"__main__\":\n    # Loading the models.\n    #vectoriser, LRmodel = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"I hate our president\",\n            \"I Love you.\",\n            \"Yes! We can win\"]\n    \n    df = predict(vectorizer, LRmodel, text)\n    print(df.head())","ea96fa2c":"# Creating and Evaluating Models\nCreating 3 different types of model of our sentimental analysis probelms.\n\n- **Bernoulli Naive Baye(Bernoulli)**\n- **Linear Support Vector Classificatio (LinearSVC)**\n- **Logistic Regression (LR)**\n","50c7a269":"**Word-Cloud for Negative tweets**","0b5cce39":"## Splitting the data","7f553dba":"**Text Preprocessing** is traditionally an important step for Natural Language Processing (NLP) tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better.\n\nThe Preprocessing steps taken are:\n\n1. Lower Casing: Each text is converted to lowercase. Replacing URLs: Links starting with \"http\" or \"https\" or \"www\" are replaced by \"URL\".\n2. Replacing Emojis: Replace emojis by using a pre-defined dictionary containing emojis along with their meaning. (eg: \":)\" to \"EMOJIsmile\")\n3. Replacing Usernames: Replace @Usernames with word \"USER\". (eg: \"@Kaggle\" to \"USER\")\n4. Removing Non-Alphabets: Replacing characters except Digits and Alphabets with a space.\n5. Removing Consecutive letters: 3 or more consecutive letters are replaced by 2 letters. (eg: \"Heyyyy\" to \"Heyy\")\n6. Removing Short Words: Words with length less than 2 are removed.\n7. Removing Stopwords: Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. (eg: \"the\", \"he\", \"have\")\n8. Lemmatizing: Lemmatization is the process of converting a word to its base form. (e.g: \u201cGreat\u201d to \u201cGood\u201d)","47651ffc":"# Saving the model","c5246fa8":"The dataset being used is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the Twitter API. The tweets have been annotated (0 = Negative, 4 = Positive) and they can be used to detect sentiment.\n\n[The training data isn't perfectly categorised as it has been created by tagging the text according to the emoji present. So, any model built using this dataset may have lower than expected accuracy, since the dataset isn't perfectly categorised.]\n\nIt contains the following 6 fields:\n\nsentiment: the polarity of the tweet (0 = negative, 4 = positive)\nids: The id of the tweet (2087)\ndate: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\nflag: The query (lyx). If there is no query, then this value is NO_QUERY.\nuser: the user that tweeted (robotickilldozr)\ntext: the text of the tweet (Lyx is cool)\nWe require only the sentiment and text fields, so we discard the rest.\n\nFurthermore, we're changing the sentiment field so that it has new values to reflect the sentiment. (0 = Negative, 1 = Positive)","87b56eaa":"## Importing Libraries","075e2896":"## Importing Dataset","91882ac1":"We can clearly see that the **Logistic Regression Model** performs the best out of all the different models that we tried. It achieves nearly 82% accuracy while classifying the sentiment of a tweet.\n\nAlthough it should also be noted that the BernoulliNB Model is the fastest to train and predict on. It also achieves 80% accuracy while calssifying.","115c93aa":"## Logistic Regression Model","8b0d8241":"## LinearSVC Model","0bdc65a1":"# Twitter Sentimental Analysis","376c3fab":"## BernoulliNB Model","4d28dc24":"## Analysing the data\n\nNow we're going to analyse the preprocessed data to get an understanding of it. We'll plot **Word Clouds** for **Positive and Negative** tweets from our dataset and see which words occur the most.","6dc6a87f":"## Evaluation Model Function","ea8290b7":"**Word-Cloud for Positive tweets**"}}