{"cell_type":{"7dba5552":"code","0263eed6":"code","f32e7fa8":"code","ee8e8daa":"code","81c46286":"code","54866542":"code","53fbbd9c":"code","dabf7794":"code","f7abddc1":"code","70e72ce0":"code","e579a2f6":"code","30d46461":"markdown","c7b8dc38":"markdown","71e55a8b":"markdown","58f5ea29":"markdown"},"source":{"7dba5552":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n  \nimport torch\nimport torchvision\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0263eed6":"## UNET Implementation Taken From : https:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/image_segmentation\/semantic_segmentation_unet\/model.py\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\nclass UNET(nn.Module):\n    def __init__(\n            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n    ):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in reversed(features):\n            self.ups.append(\n                nn.ConvTranspose2d(\n                    feature*2, feature, kernel_size=2, stride=2,\n                )\n            )\n            self.ups.append(DoubleConv(feature*2, feature))\n\n        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n        skip_connections = []\n\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n        skip_connections = skip_connections[::-1]\n\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx\/\/2]\n\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx+1](concat_skip)\n\n        return self.final_conv(x)\n\ndef test():\n    x = torch.randn((3, 3, 161, 161))\n    model = UNET(in_channels=3, out_channels=1)\n    preds = model(x)\n    #assert preds.shape == x.shape\n    print(preds.shape)\n    print(x.shape)\n\nif __name__ == \"__main__\":\n    test()\n","f32e7fa8":"## Dataloader\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport numpy as np\n\n\nclass FaceSegmentationDataset(Dataset):\n    def __init__(self , image_dir , mask_dir , df  , transform = None):\n        self.image_dir = image_dir\n        self.df = df\n        self.images = self.df['Image_Name']\n        self.mask_dir = mask_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    \n    \n    def __getitem__(self , index):\n        img_id  = self.images[index]\n        img_path = os.path.join(self.image_dir, img_id)\n        #l_eye_mask_path = os.path.join(self.mask_dir , img_id[-5::-1][::-1].zfill(5)+\"_\"+\"l\"+\"_eye.png\")\n        r_eye_mask_path= os.path.join(self.mask_dir , img_id[-5::-1][::-1].zfill(5)+\"_\"+\"r\"+\"_eye.png\")\n       # l_lip_mask_path = os.path.join(self.mask_dir , img_id[-5::-1][::-1].zfill(5)+\"_\"+\"l\"+\"_lip.png\")\n       # u_lip_mask_path = os.path.join(self.mask_dir , img_id[-5::-1][::-1].zfill(5)+\"_\"+\"u\"+\"_lip.png\")\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        #l_eye_mask = np.array(Image.open(l_eye_mask_path).convert(\"L\") , dtype=np.float32)\n        r_eye_mask = np.array(Image.open(r_eye_mask_path).convert(\"L\") ,dtype=np.float32)\n       # l_lip_mask = np.array(Image.open(l_lip_mask_path).convert(\"L\"), dtype=np.float32)\n       # u_lip_mask = np.array(Image.open(u_lip_mask_path).convert(\"L\"), dtype=np.float32)\n\n        r_eye_mask[r_eye_mask == 255.0] = 1.0\n       # r_eye_mask[r_eye_mask == 255.0] = 1.0\n       # l_lip_mask[l_lip_mask == 255.0] = 1.0\n       # u_lip_mask[u_lip_mask == 255.0] = 1.0\n\n        #masks  = [l_eye_mask, r_eye_mask  ,l_lip_mask, u_lip_mask  ]\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=r_eye_mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n            \n        ## get item will return images and their corresponding augmented masks not just single masks\n        return image , mask\n\n            \n    ","ee8e8daa":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nIMAGE_HEIGHT = 384\nIMAGE_WIDTH = 384\n\ntrain_transform = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Rotate(limit=35, p=1.0),\n            #A.HorizontalFlip(p=0.5),\n            #A.VerticalFlip(p=0.1),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\nval_transforms = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n    \n","81c46286":"from torch.utils.data import Dataset  , DataLoader\nimport torchvision.transforms as transforms\n\ndef get_loader(\n    image_dir,\n    mask_dir ,\n    df , \n    transform ,\n    batch_size = 16 ,\n    num_workers = 8 ,\n    shuffle = True , \n    pin_memory = True\n\n):\n    dataset =FaceSegmentationDataset(image_dir , mask_dir  , df , transform = transform)\n    \n    loader = DataLoader(\n                dataset = dataset ,\n                batch_size = batch_size ,\n                num_workers = num_workers , \n                shuffle = shuffle ,\n                pin_memory = pin_memory , \n                \n    \n    \n    )\n    return loader","54866542":"import torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n\n## Hyperparams of Model\n\nLEARNING_RATE  = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 16\n\nNUM_EPOCHS  = 2\n\nNUM_WORKERS = 4\n\nIMAGE_HEIGHT = 384\nIMAGE_WIDTH = 384\n\nPIN_MEMORY = True\nLOAD_MODEL = False\n\nIMAGE_DIR  = \"..\/input\/celebamaskhq\/CelebAMask-HQ\/CelebA-HQ-img\"\nMASK_DIR  = \"..\/input\/celeb-m-hqmaskscollated\/CelebAHQ-Mask-Images\"\n\n","53fbbd9c":"def train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())","dabf7794":"## utils\ndef save_checkpoint(state, filename=\"my_checkpoint_r_eye.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])","f7abddc1":"## utils \ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) \/ (\n                (preds + y).sum() + 1e-8\n            )\n\n    print(\n        f\"Got {num_correct}\/{num_pixels} with acc {num_correct\/num_pixels*100:.2f}\"\n    )\n    print(f\"Dice score: {dice_score\/len(loader)}\")\n    model.train()\n\n","70e72ce0":"def main():\n    train_transform = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Rotate(limit=35, p=1.0),\n            #A.HorizontalFlip(p=0.5),\n            #A.VerticalFlip(p=0.1),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\n    val_transforms = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n    model = UNET(in_channels = 3 , out_channels = 1).to(DEVICE)\n    \n    loss_fn  = nn.BCEWithLogitsLoss().to(device = DEVICE)\n    optimizer = optim.Adam(model.parameters() , lr = LEARNING_RATE)\n    \n    train_df = pd.read_csv(\"..\/input\/face-segmentation-dataloader\/train_data_four_masks.csv\")\n    valid_df = pd.read_csv(\"..\/input\/face-segmentation-dataloader\/val_data_four_masks.csv\")\n    #train_df = train_df.sample(n=5000).reset_index(drop = True) #test\n    train_loader = get_loader(image_dir  = IMAGE_DIR , mask_dir = MASK_DIR , df = train_df , transform  =train_transform )\n    val_loader = get_loader(image_dir = IMAGE_DIR , mask_dir = MASK_DIR , df = valid_df , transform = val_transforms )\n    \n    if LOAD_MODEL:\n        load_checkpoint(torch.load(\"my_checkpoint_r_eye.pth.tar\"), model)\n    \n    check_accuracy(val_loader, model, device=DEVICE)\n    \n    scaler = torch.cuda.amp.GradScaler()\n\n    for epochs in range(NUM_EPOCHS):\n        train_fn(train_loader , model , optimizer , loss_fn , scaler)\n        \n        # save model\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\":optimizer.state_dict(),\n        }\n        save_checkpoint(checkpoint)\n        \n        check_accuracy(val_loader, model, device=DEVICE)\n        \n       \n        \n    #del model , loss_fn , train_df , train_loader , scaler ","e579a2f6":"if __name__ == \"__main__\":\n    main()","30d46461":"Will be using Unet Model , its computationally faster to train and SOTA in Segmentation tasks ","c7b8dc38":"## Training and Validation Loop","71e55a8b":"## Dataset and Dataloaders","58f5ea29":"## Model : UNET"}}