{"cell_type":{"fa5e591f":"code","dbfb22da":"code","b9e0a70c":"code","a421d949":"code","dfb51e32":"code","56bff728":"code","8850415c":"code","2a15fa55":"code","d92a40a2":"code","384b7f28":"code","82b67a77":"code","a80fa150":"code","7c820362":"code","f50024e4":"code","540c7e43":"code","7ec7ad44":"code","3f853f7b":"code","0336ff89":"code","ad72df9b":"code","ceb43b4f":"code","af4ba198":"code","741994f4":"code","85a38ac1":"code","4643cf8a":"code","33f03c87":"code","8a5d9fcd":"code","446ed046":"code","85c546f3":"code","b60e5939":"markdown","692201c0":"markdown","bddd90f8":"markdown","6e28f549":"markdown","47da1606":"markdown","ba4f6305":"markdown","58b4b93e":"markdown","bf797bce":"markdown","85b5b8b5":"markdown","4d84290a":"markdown","7d83f0d5":"markdown","caa1e161":"markdown","a39cbce3":"markdown","666f575b":"markdown","d9cb0687":"markdown","0bf7461b":"markdown","5814f7b8":"markdown","8d5ab9c5":"markdown","3f7485fb":"markdown","539be87b":"markdown","9f07b69b":"markdown","566d8ef1":"markdown","b19b05a6":"markdown","227e2d54":"markdown","fa95ee81":"markdown","ed07f195":"markdown","bb9da9a2":"markdown"},"source":{"fa5e591f":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport tensorflow_addons as tfa","dbfb22da":"class Config:\n    image_size = 128\n    input_shape = [image_size, image_size, 3]\n    learning_rate = 4e-4\n    weight_decay = 0.0001\n    batch_size = 128\n    num_classes = 1\n    num_epochs = 30\n    patch_size = 16\n    num_patches = (image_size \/\/ patch_size) ** 2\n    projection_dim = 64\n    num_heads = 4\n    scale_factor = 100.0\n    transformer_units = [\n        projection_dim * 2,\n        projection_dim\n    ]\n    transformer_layers = 8\n    mlp_head_units = [2048, 1024]\n    tabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']","b9e0a70c":"def display_images(images, row_count, column_count):\n    fig, axs = plt.subplots(row_count, column_count, figsize=(10,10))\n    for i in range(row_count):\n        for j in range(column_count):\n            axs[i,j].imshow(images[i * column_count + j])\n            axs[i,j].axis('off')\n    plt.show()","a421d949":"def preprocess_image(image_url):\n    image_string = tf.io.read_file(image_url)\n    image = tf.image.decode_jpeg(image_string, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    #image = tf.image.central_crop(image, 1.0)\n    image = tf.image.resize(image, (Config.image_size, Config.image_size))\n    return image","dfb51e32":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x","56bff728":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(y_true * Config.scale_factor - y_pred * Config.scale_factor)))","8850415c":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","2a15fa55":"train.head()","d92a40a2":"train[\"file_path\"] = train[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/train\/\" + identifier + \".jpg\")\ntest[\"file_path\"] = test[\"Id\"].apply(lambda identifier: \"..\/input\/petfinder-pawpularity-score\/test\/\" + identifier + \".jpg\")","384b7f28":"train.head()","82b67a77":"train[\"Pawpularity\"].hist()","a80fa150":"item_width = 5\ndata = train[train.Pawpularity >= 90]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","7c820362":"item_width = 5\ndata = train[train.Pawpularity <= 10]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","f50024e4":"item_width = 5\ndata = train[(train.Pawpularity >= 40) & (train.Pawpularity <= 60)]\nimage_urls = data.iloc[np.random.choice(data.shape[0], item_width ** 2)][\"file_path\"]\nfor images in tf.data.Dataset.from_tensor_slices((image_urls)).map(preprocess_image).batch(item_width ** 2):\n    display_images(images.numpy(), item_width, item_width)","540c7e43":"train[\"Pawpularity\"] \/= Config.scale_factor","7ec7ad44":"def preprocess(image_url, tabular):\n    image =  preprocess_image(image_url)\n    return (image, tabular[1:]), tabular[0]","3f853f7b":"augmentation_layer = keras.Sequential([\n    keras.layers.Input(Config.input_shape),\n    #keras.layers.experimental.preprocessing.RandomRotation(factor=0.02),\n    keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n    keras.layers.experimental.preprocessing.RandomZoom(height_factor=0.2, width_factor=0.2),\n])","0336ff89":"class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super(Patches, self).__init__()\n        self.patch_size = patch_size\n        \n    def call(self, images):\n        batch_size = tf.shape(images)[0]\n        patches = tf.image.extract_patches(\n            images = images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        patch_dims = patches.shape[-1]\n        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n        return patches","ad72df9b":"index = np.random.choice(train.shape[0])\nplt.figure(figsize=(4, 4))\nimage = preprocess_image(tf.constant(train.iloc[index][\"file_path\"], dtype=tf.string))\nprint(image.shape)\nplt.imshow(np.squeeze(image))\nplt.axis(\"off\")\n\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(Config.image_size, Config.image_size)\n)\nprint(resized_image.shape)\npatches = Patches(Config.patch_size)(resized_image)\nprint(f\"Image size: {Config.image_size} X {Config.image_size}\")\nprint(f\"Patch size: {Config.patch_size} X {Config.patch_size}\")\nprint(f\"Patches per image: {patches.shape[1]}\")\nprint(f\"Elements per patch: {patches.shape[-1]}\")\n\nn = int(np.sqrt(patches.shape[1]))\nplt.figure(figsize=(4, 4))\nfor i, patch in enumerate(patches[0]):\n    ax = plt.subplot(n, n, i + 1)\n    patch_img = tf.reshape(patch, (Config.patch_size, Config.patch_size, 3))\n    plt.imshow(patch_img.numpy())\n    plt.axis(\"off\")","ceb43b4f":"class PatchEncoder(layers.Layer):\n    \n    def __init__(self, num_patches, projection_dim):\n        super(PatchEncoder, self).__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(projection_dim)\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded","af4ba198":"def create_vision_transformer(use_tabular_inputs=False):\n    \n    tabular_inputs = tf.keras.Input(len(Config.tabular_columns))\n    # Inputs\n    inputs = layers.Input(shape=Config.input_shape)\n    # Data Augmentation\n    augmented = augmentation_layer(inputs)\n    # Patches\n    patches = Patches(Config.patch_size)(augmented)\n    encoder_patches = PatchEncoder(Config.num_patches, Config.projection_dim)(patches)\n    \n    for _ in range(Config.transformer_layers):\n        # Layer Normalization 1\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n        # Multi-Head Attention Layer\n        attention_output = layers.MultiHeadAttention(\n            num_heads=Config.num_heads, \n            key_dim=Config.projection_dim,\n            dropout=0.1\n        )(x1, x1)\n        # Skip Connnection 1\n        x2 = layers.Add()([attention_output, encoder_patches])\n        \n        # Layer Normalization 2\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        \n        # MLP\n        x3 = mlp(x3, hidden_units=Config.transformer_units, dropout_rate=0.1)\n        \n        # Skip Connnection 2\n        encoder_patches = layers.Add()([x3, x2])\n    \n    representation = layers.LayerNormalization(epsilon=1e-6)(encoder_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    \n    features = mlp(representation, hidden_units=Config.mlp_head_units, dropout_rate=0.5)\n    \n    if use_tabular_inputs:\n        image_x = layers.Dense(128, activation=tf.nn.gelu)(features)\n        tabular_x = mlp(tabular_inputs, hidden_units=[16] * 10, dropout_rate=0.5)\n        x = tf.keras.layers.Concatenate(axis=1)([image_x, tabular_x])\n    else:\n        x = features\n    \n    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n    \n    model = keras.Model(inputs=[inputs, tabular_inputs], outputs=outputs)\n    return model","741994f4":"model =  create_vision_transformer(True)\ntf.keras.utils.plot_model(model, show_shapes=True)","85a38ac1":"model.summary()","4643cf8a":"image = np.random.normal(size=(1, Config.image_size, Config.image_size, 3))\ntabular = np.random.normal(size=(1, len(Config.tabular_columns)))\nprint(image.shape, tabular.shape)\nprint(model((image, tabular)).shape)","33f03c87":"tf.keras.backend.clear_session()\nmodels = []\nhistorys = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=997)\n# For the current random state, 5th fold can generate a better validation rmse and faster convergence.\ntrain_best_fold = False\nbest_fold = 4\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_best_fold and index != best_fold:\n        continue\n    x_train = train.loc[train_indices, \"file_path\"]\n    tabular_train = train.loc[train_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    x_val= train.loc[val_indices, \"file_path\"]\n    tabular_val = train.loc[val_indices, [\"Pawpularity\"] + Config.tabular_columns]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True,\n        save_weights_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=20\n    )\n   \n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=10, \n        min_lr=1e-7\n    )\n\n    callbacks = [checkpoint, reduce_lr, early_stop]\n    \n    loss = tf.keras.losses.BinaryCrossentropy()\n\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=Config.learning_rate,\n        weight_decay=Config.weight_decay\n    )\n    train_ds = tf.data.Dataset.from_tensor_slices((x_train, tabular_train)).map(preprocess).shuffle(512).batch(Config.batch_size).cache().prefetch(1)\n    val_ds = tf.data.Dataset.from_tensor_slices((x_val, tabular_val)).map(preprocess).batch(Config.batch_size).cache().prefetch(1)\n    # You can choose whether to use tabular inputs\n    model = create_vision_transformer(use_tabular_inputs=True)\n    model.compile(loss=loss, optimizer=optimizer, metrics=[rmse, \"mae\", \"mape\"])\n    history = model.fit(train_ds, epochs=Config.num_epochs, validation_data=val_ds, callbacks=callbacks)\n    for metrics in [(\"loss\", \"val_loss\"), (\"mae\", \"val_mae\", \"rmse\", \"val_rmse\"), (\"mape\", \"val_mape\")]:\n        pd.DataFrame(history.history, columns=metrics).plot()\n        plt.show()\n    model.load_weights(checkpoint_path)\n    historys.append(history)\n    models.append(model)","8a5d9fcd":"def preprocess_test_data(image_url, tabular):\n    print(image_url, tabular)\n    image = preprocess_image(image_url)\n    # 0 won't be used in prediction, but it's needed in this senario or the tabular variable is treated as label.\n    return (image, tabular), 0","446ed046":"test_ds = tf.data.Dataset.from_tensor_slices((test[\"file_path\"], test[Config.tabular_columns])).map(preprocess_test_data).batch(Config.batch_size).cache().prefetch(1)","85c546f3":"use_best_result = False\nif use_best_result:\n    if train_best_fold:\n        best_model = models[0]\n    else:\n        best_fold = 0\n        best_score = 10e8\n        for fold, history in enumerate(historys):\n            for val_rmse in history.history[\"val_rmse\"]:\n                if val_rmse < best_score:\n                    best_score = val_rmse\n                    best_fold = fold\n        print(\"Best Score:%.2f Best Fold: %d\"%(best_score, best_fold + 1))\n        best_model = models[best_fold]\n    sample_submission[\"Pawpularity\"] = Config.scale_factor * best_model.predict(test_ds).reshape(-1)\n    sample_submission.to_csv(\"submission.csv\", index=False)\nelse:\n    total_results = []\n    for model in models:\n        total_results.append(model.predict(test_ds).reshape(-1))\n    results = np.mean(total_results, axis=0).reshape(-1)\n    sample_submission[\"Pawpularity\"] = Config.scale_factor * results\n    sample_submission.to_csv(\"submission.csv\", index=False)","b60e5939":"This Model accepts images with shape (image_size, image_size, 3) and tabular information (if needed) with shape (12) as input. It generates output with shape (1). ","692201c0":"## Reference\n- [Image classification with Vision Transformer](https:\/\/keras.io\/examples\/vision\/image_classification_with_vision_transformer\/)\n- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https:\/\/arxiv.org\/abs\/2010.11929)","bddd90f8":"### Patch Creation Layer","6e28f549":"### Preprocess function","47da1606":"### Display images","ba4f6305":"### Preprocess images","58b4b93e":"### Images has median scores","bf797bce":"### Data Augmentation","85b5b8b5":"### Custom RMSE function that can calculate Pawpularity Score correctly after Normalization","4d84290a":"### Normalize Pawpularity Score from 0 to 1.","7d83f0d5":"When We Submit the result, don't forget to multiply the result by 100.","caa1e161":"# Pawpularity Prediction: Vision Transformer from Scratch\n\n## Table of Contents\n- Summary\n- Setup\n- Configuration\n- Helpers\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Submission\n- Reference\n\n## Summary\n* Create Vision Transformer from scratch, modify the Vision Transformer so that it can also accept Tabular inputs. Tabular inputs is optional, with the Functional API, it's easy to control whether or not to receive Tabular inputs.\n* Change Regression Problem to Classification Problem. Normalize the Pawpularity score from 0 to 1 and use BinaryCrossEntropy as loss function.\n* Apply Data Augmentation to dataset during Training.\n\n\n## Setup","a39cbce3":"### Patch Encoder Layer","666f575b":"### Images that has High Score","d9cb0687":"### Model Training\nI will use tensorflow Dataset here to preprocess and cache tensors, first epoch is very slow because it's preprocessing data; after that, it would be must faster.","0bf7461b":"### Images with Low Scores","5814f7b8":"## Import datasets","8d5ab9c5":"## Helpers","3f7485fb":"## Configuration","539be87b":"Let's understand what Patch Creation Layer do. It simply split the image to NxN grid.","9f07b69b":"Let's have a big picture of how this Model looks like.","566d8ef1":"### Multi Layer Perceptron","b19b05a6":"## Submission","227e2d54":"## Model Development","fa95ee81":"### Vision Transformer Model","ed07f195":"## Data Preprocessing","bb9da9a2":"**If you find my notebook useful, give me an upvote.**"}}