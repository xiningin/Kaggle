{"cell_type":{"9a89866a":"code","98196c93":"code","5dd37d50":"markdown","d89e0d49":"markdown"},"source":{"9a89866a":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport collections\n\nbasePath = \"..\/input\/optiver-realized-volatility-prediction\/\"\n\nbookTrainFolder  = os.path.join(basePath, \"book_train.parquet\") # Folder for book data  \n\nactivity = np.zeros(600) # An array to count the number of orderbook changes that happened at each second\n\nfor f in os.listdir(bookTrainFolder):\n    parquetFile = os.listdir(os.path.join(bookTrainFolder, f))[0]\n\n    df = pd.read_parquet(os.path.join(bookTrainFolder, f, parquetFile)).to_numpy()\n\n    counts = collections.Counter(df[:, 1].astype(int)) # Count the total orderbook changes of one stock at each second\n    \n    for second in counts:\n        activity[second] += counts[second] # Add activity of all the stocks together\n\nplt.bar(np.arange(600), activity, width=1.0) # Plot activity\nplt.show()","98196c93":"plt.bar(np.arange(25), activity[-25:], width=1.0) # Plot activity of the last 25 seconds\nplt.show()","5dd37d50":"Many will have noticed that the mean of $|log{WAP_0 \\over 1}|$ (the change of the first calculated wap compared to a wap of 1) is greater than the mean of $|log{WAP_n \\over WAP_{n-1}}|$ (the change of any two neighboring waps). If the buckets are normalized to 1, this should not be the case. \nMatteo stated that:\n> a random amount of seconds is dropped at the beginning of each time_id.\n\nThis means, for some buckets, much more seconds actually have passed since point in time where the normalization to 1 has happend. Which also explains the bigger price movement in the beginning. \n\nIn this competition I have tried to shift buckets to the \"right\" where all $second\\_in\\_bucket<599$ as these buckets are the only ones that have been possibly shifted. If we make the assumption that most stocks have a slight correlation (especially during bigger market movements), we can minimize $\\epsilon$ in the following equation for any two distinct stocks $A$ and $B$ within a $time\\_id$, to make an approximation of the bucket shifts:\n\n$$\\epsilon_{AB} = \\sum\\limits_{second=1}^{599} {\\Big({log\\big({WAP_{stockA_{second}} \\over WAP_{stockA_{second - 1}}}\\big) - log\\big({WAP_{stockB_{second}} \\over WAP_{stockB_{second - 1}}}\\big)\\Big)}^2}$$\n\nIn practice I have tried a few different things; For example I selected the stocks within a $time\\_id$, that have the highest $seconds\\_in\\_bucket$ value (the least shiftable stocks) and used these stocks as scale for the other stocks. I then minimzed the sum of $\\epsilon_{AB}$ for all possible stocks $A$ and $B$, where $A$ is a stock from the scaling group and $B$ is a stock from the group of the other stocks. I also choose to use a small moving average of the log returns instead of the raw log returns, in case the correlation between stocks was slightly offset.\n\nShifting the buckets like this has the disadvantage, that stocks that are negatively correlated will not align very well.\n\n# Did this help my score?\n\nSadly I was not able to see any noticable improvements in my score, but I kept this method of preprocessing in my final models anyway. I also haven't fully explored all the possible methods to align the buckets in time, so I wonder if any of you were able to improve by messing with the alignment of the buckets.\n","d89e0d49":"# How are the buckets shifted in time\n\nThroughout this competition I was not enitrely sure, how the exact sacling and shifting process of the orderbook and trade data happened.\nIn the discussion forum I came across these two posts by Matteo and Jiashen:\n\n> Hi, a random amount of seconds is dropped at the beginning of each time_id. That is meant to make the data harder to revert for someone with access to the full dataset.\n> The normalization happens at the beginning of each time bucket by scaling the WAP to 1, so small deviations in price can be explained because of it.\n\nhttps:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/249474#1400435\n\n> Hi @stassl , thank you very much for your question. Yes, you are right. If, at the second when the bucket starts there is no book update, we will rebase the second_in_bucket field which means the time sync for that stock on second level breaks. However if a stock has a book update on that second, then there is no rebase hence the sync does not break. Now in hindsight we should admit that this part of the design is not ideal --- something we also learned as competition host.\n\nhttps:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/251775#1386295\n\nIf I understand theses statements correctly, it means that there is no way to differentiate between the following two orderbooks, after rebasing them to 0:\n\nBook 1\n\n|  second| wap |\n| --- | --- |\n|  100| 1.4 |\n|  220| 1.5 |\n|  400| 1.2 |\n\nBook 2\n\n|  second| wap |\n| --- | --- |\n|  200| 1.4 |\n|  320| 1.5 |\n|  500| 1.2 |\n\nas both get rebased to\n\n|  second| wap |\n| --- | --- |\n|  000| 1.4 |\n|  120| 1.5 |\n|  300| 1.2 |\n\nThis means, a competitor might be able to gain a significant advantage when shifting the buckets by the right amount of seconds during preprocessing stage. When we plot the number occurences of each $seconds\\_in\\_bucket$ value, we can see that the $seconds\\_in\\_bucket$ close to the end of a bucket get lower. On the other hand the first few $seconds\\_in\\_bucket$ have more occurences. This makes sense, as a bucket which originally has no update at second 0, can't have an update at second 599 later, as the bucket will be shifted.\n\n"}}