{"cell_type":{"fa61382d":"code","14c7ca6b":"code","d87a46ad":"code","7d9f1c5a":"code","f8dbea48":"code","f344db31":"code","2b1bc957":"code","80a5373d":"code","575c033d":"code","701bf9fb":"code","d9d789fd":"code","4e100f11":"code","c2e91e36":"code","e3373886":"code","6d6edc1d":"code","47b185e1":"code","7b283717":"code","f27be804":"code","a923c0e5":"code","202c06f3":"code","16de9374":"code","4446830c":"code","571a01b0":"code","d8f5dfff":"code","c64bc527":"code","15b8b748":"code","052e6995":"code","ba3ad1fb":"code","c40e5e48":"code","ff6e51a0":"code","815c1a72":"code","d308fc9b":"code","1ec8ee0f":"code","75ff80c0":"code","ada3d2a9":"code","bbaed169":"code","09f3fd7e":"code","ddd69031":"markdown","a165de26":"markdown","2bfbb12f":"markdown","c350b53c":"markdown","ace8f229":"markdown","d5cdbf4a":"markdown","1e1f2754":"markdown","b5fb26ba":"markdown","5a2e70d1":"markdown","ca6b5c00":"markdown","688955a8":"markdown"},"source":{"fa61382d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14c7ca6b":"from matplotlib import pyplot as plt","d87a46ad":"df = pd.read_csv(\"\/kaggle\/input\/world-happiness\/2019.csv\")","7d9f1c5a":"df\nscores = df[\"Score\"]","f8dbea48":"df = df.drop(columns = [\"Overall rank\", \"Country or region\", \"Score\"])","f344db31":"df = (df - np.min(df)) \/ (np.max(df) - np.min(df))","2b1bc957":"from sklearn.cluster import KMeans\nwcss = []\nfor i in range(1,10):\n    km = KMeans(n_clusters=i).fit(df)\n    wcss.append(km.inertia_)\nplt.plot(np.arange(1,len(wcss)+1), wcss)","80a5373d":"km = KMeans(n_clusters=3).fit(df)","575c033d":"df[\"label\"] = km.predict(df)","701bf9fb":"happy_num = df[\"label\"][0]\nunhappy_num = df[\"label\"][155]\nnormal_num = -1\nfor i in range(3):\n    if i != happy_num and i != unhappy_num:\n        normal_num = i","d9d789fd":"happy_num, unhappy_num, normal_num","4e100f11":"df.label.values","c2e91e36":"plt.scatter(df[df.label == happy_num].index,scores[df[df.label == happy_num].index], color=\"yellow\", label = \"Happy\")\nplt.scatter(df[df.label == normal_num].index,scores[df[df.label == normal_num].index], color=\"blue\", label = \"Normal\")\nplt.scatter(df[df.label == unhappy_num].index,scores[df[df.label == unhappy_num].index], color=\"red\", label = \"Unhappy\")\nplt.legend()\nplt.title(\"KMean Clustering\")\nplt.ylabel(\"Scores\")\nplt.show()","e3373886":"from sklearn.cluster import AgglomerativeClustering\nward = AgglomerativeClustering(n_clusters=3,linkage='ward').fit(df)\nh = ward.labels_","6d6edc1d":"h = pd.DataFrame(h)\nhappy_num2 = h[0][0]\nunhappy_num2 = h[0][155]\nnormal_num2 = -1\nfor i in range(3):\n    if i != happy_num2 and i != unhappy_num2:\n        normal_num2 = i\nplt.scatter(h[h[0] == happy_num2].index,scores[h[h[0] == happy_num2].index], color=\"yellow\", label = \"Happy\")\nplt.scatter(h[h[0] == normal_num2].index,scores[h[h[0] == normal_num2].index], color=\"blue\", label = \"Normal\")\nplt.scatter(h[h[0] == unhappy_num2].index,scores[h[h[0] == unhappy_num2].index], color=\"red\", label = \"Unhappy\")\nplt.legend()\nplt.title(\"Hierarchical Clustering\")\nplt.ylabel(\"Scores\")\nplt.show()","47b185e1":"def flag(x):\n    if x == happy_num:\n        return \"Happy\"\n    elif x == normal_num:\n        return \"Normal\"\n    return \"Unhappy\"\ndf[\"happiness\"] = df[\"label\"].apply(lambda x : flag(x))\ndf","7b283717":"from sklearn.model_selection import train_test_split\ny = df[\"label\"]\nx = df.drop(columns = [\"happiness\",\"label\"])\ny.values","f27be804":"y1 = y.apply(lambda x : 1 if x == happy_num else 0)\nx_test, x_train, y_test, y_train = train_test_split(x, y1, test_size = 0.2)","a923c0e5":"from sklearn.linear_model import LogisticRegression\nhappy_predict = LogisticRegression().fit(x_train, y_train)","202c06f3":"y2 = y[y != happy_num].apply(lambda x : 1 if x == normal_num else 0)","16de9374":"x2 = df[df[\"label\"] != happy_num].drop(columns = [\"happiness\", \"label\"])\nx_test, x_train, y_test, y_train = train_test_split(x2, y2, test_size = 0.2)\nnormal_predict = LogisticRegression().fit(x_train, y_train)","4446830c":"happies = happy_predict.predict(x)\nhappies","571a01b0":"normals = normal_predict.predict(x)\nnormals","d8f5dfff":"normals.shape","c64bc527":"result = np.full(normals.shape, -1)","15b8b748":"result","052e6995":"for i in range(happies.shape[0]):\n    if happies[i] == 1:\n        result[i] = happy_num\nresult","ba3ad1fb":"for i in range(normals.shape[0]):\n    if result[i] == happy_num:\n        continue\n    elif normals[i] == 1:\n        result[i] = normal_num\n    else:\n        result[i] = unhappy_num\nresult","c40e5e48":"df[\"predict_label\"] = result.astype(int)","ff6e51a0":"df","815c1a72":"accuracy = (df[\"label\"] == df[\"predict_label\"])","d308fc9b":"print(\"Accuracy : %\",accuracy.values.sum() \/ accuracy.size * 100)","1ec8ee0f":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size =0.2)","75ff80c0":"y_train","ada3d2a9":"from sklearn.neighbors import KNeighborsClassifier","bbaed169":"from sklearn.model_selection import GridSearchCV\nknn_cv = GridSearchCV(KNeighborsClassifier(), {\"n_neighbors\" : np.arange(1,50)}, cv = 10)\nknn_cv.fit(x_train, y_train)\nbest_n = knn_cv.best_params_[\"n_neighbors\"]\nknn_cv.best_params_","09f3fd7e":"knn = KNeighborsClassifier(n_neighbors=best_n).fit(x_train,y_train)\nprint(\"KNN Accuracy : %\",knn.score(x_test, y_test)*100)","ddd69031":"* After clustering we will check the result with scores list","a165de26":"![Untitled.png](attachment:Untitled.png)","2bfbb12f":"Now let's look hierarchical clustering","c350b53c":"* elbow point is 3[](http:\/\/)","ace8f229":"Lets find best neighbor number for KNN","d5cdbf4a":"* first divide happy and others (normal, unhappy)","1e1f2754":"* We must remove happy countries from data for normal prediction","b5fb26ba":"* we can split our data happy, normal, unhappy","5a2e70d1":"* Happy predict find just happy countries","ca6b5c00":"With K-Nearest Learning","688955a8":"Normalization"}}