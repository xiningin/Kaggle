{"cell_type":{"31ecbf66":"code","2d7286b2":"code","037ff442":"code","5047690b":"code","e4b14a0f":"code","2bbb20ed":"code","436c16fd":"code","ceb93dac":"code","0544183a":"code","1e67f0aa":"code","887e7e7f":"code","c0a0ca7c":"code","4016f073":"code","83c3c109":"code","d8422812":"code","48bd521c":"code","1cbbd7ed":"code","a5b8d097":"code","bb1af426":"code","44fd1935":"code","d985dfc3":"code","41284cfc":"code","305d8e62":"code","29099998":"code","f0925d47":"code","9b054213":"code","dff53fea":"code","cb412217":"code","3841ac30":"code","1ef22151":"code","c63cd663":"code","14a84bc0":"code","cea00c3a":"code","89411386":"code","2e7819ae":"code","95665070":"code","608080a9":"code","5d8a2178":"code","302a2724":"code","af086837":"code","2876e40b":"code","a1cd69b5":"code","63a984c4":"markdown","9aa8154b":"markdown","f0e880b8":"markdown","3073b425":"markdown","6f273e3e":"markdown","9618f501":"markdown","3fc97907":"markdown","f76efba0":"markdown","c1b83969":"markdown","a004e670":"markdown","654e2d47":"markdown","af29fcd7":"markdown","ea858a9d":"markdown","303fa8c7":"markdown","9e8c3dd0":"markdown","82d88cc5":"markdown","160dae94":"markdown","a5c8b3de":"markdown","a2de7420":"markdown","6b0bb6ca":"markdown","45a5887d":"markdown","db84c62f":"markdown","100943b4":"markdown","e5cde741":"markdown","c2e59cf5":"markdown","ef031a4e":"markdown","19e95855":"markdown","aabb58e8":"markdown","e3a68c8f":"markdown","94704508":"markdown","7cb7303e":"markdown","4e4210eb":"markdown","39ae4899":"markdown","b2fafe54":"markdown","764633fe":"markdown"},"source":{"31ecbf66":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","2d7286b2":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndata.shape","037ff442":"data.head()","5047690b":"data.info()","e4b14a0f":"data.columns","2bbb20ed":"data.describe()","436c16fd":"#knowing the data type of each column\ndata.dtypes","ceb93dac":"#We can count the target variables with this method \ndata.target.value_counts()","0544183a":"plt.figure(figsize=(8,6))\nsns.set_style('whitegrid')\nsns.countplot(x='target',data=data)","1e67f0aa":"sns.countplot(x='sex', data=data, palette=\"pastel\")\nplt.xlabel(\"Sex (0 = female, 1= male)\") # Representing values \nplt.show()","887e7e7f":"plt.figure(figsize=(8,6))\nsns.set_style('dark')\nsns.countplot(x='target',hue='sex',data=data,palette='RdBu_r')\nplt.title(\"The distribution of Sex column in relation to the Target column\")","c0a0ca7c":"plt.figure(figsize=(8,6))\nsns.set_style('dark')\nsns.countplot(x='target',hue='thal',data=data)\nplt.title(\"The Distribution of thal in relation to the Target column\")","4016f073":"#Bar Plot and this is providing distribution of age\nsns.barplot(x=data.age.value_counts()[:10].index,y=data.age.value_counts()[:10].values)\nplt.xlabel('Age')\nplt.ylabel('Number of People')\nplt.title('Age Calculation')\nplt.show()\n","83c3c109":"# Bar chart for age with sorted index\nplot = data[data.target == 1].age.value_counts().sort_index().plot(kind = \"bar\", figsize=(15,5), fontsize = 15)\nplot.set_title(\"Age Distribution\", fontsize = 15)\nplt.ioff()","d8422812":"data['age'].plot(kind='hist',bins=30,color='red',figsize= (16,7))","48bd521c":"#We will practise how can we use 'lineplot'\ndata.chol.plot(kind=\"line\",color=\"green\",label=\"chol\",grid=True,linestyle=\":\")\ndata.thalach.plot(kind=\"line\",color=\"purple\",label=\"thalach\",grid=True)\ndata.age.plot(kind=\"line\",color=\"pink\",label=\"age\",grid=True)\ndata.trestbps.plot(kind=\"line\",color=\"orange\",label=\"trestbps\",grid=True)\nplt.legend(loc=\"upper right\") #legend: puts feature label into plot\nplt.xlabel(\"indexes\")\nplt.ylabel(\"Features\")\nplt.title(\"Heart Diseases\")\nplt.show()","1cbbd7ed":"#We will practise how can we use Scatter Plot\n#I want to compare 'age' and 'chol'. Is there any connection with these features?\ndata.plot(kind=\"scatter\", x=\"age\", y=\"chol\", alpha= 0.5, color=\"brown\")\nplt.xlabel(\"age\")\nplt.ylabel(\"chol\")\nplt.title(\"age and chol with Scatter Plot\")\nplt.show()","a5b8d097":"#Lets find type of heart attack by all ages. (cp values)\nage_list=list(data.age.unique())\n#cp_list= list(data.cp.unique())\ncp_zero=[]\ncp_one=[]\ncp_two=[]\ncp_three=[]\nfor i in age_list:\n    x= data[data['age']==i]\n    cp_zero.append(sum(x.cp==0)\/len(x))\n    cp_one.append(sum(x.cp==1)\/len(x))\n    cp_two.append(sum(x.cp==2)\/len(x))\n    cp_three.append(sum(x.cp==3)\/len(x))\n#Visualization\nf,ax= plt.subplots(figsize=(15,9))\nsns.barplot(y=cp_zero,x=age_list,color='purple',alpha=0.5,label='Type 0')\nsns.barplot(y=cp_one,x=age_list,color='green',alpha=0.7,label='Type 1')\nsns.barplot(y=cp_two,x=age_list,color='yellow',alpha=0.6,label='Type 2')\nsns.barplot(y=cp_three,x=age_list,color='blue',alpha=0.6,label='Type 3')\n\nax.legend(loc='lower right',frameon=True)\nax.set(xlabel='Ages', ylabel='Cp values', title='Type of heart attack by age')","bb1af426":"#Distribution for all numeric variables \n\n\nfor i in data.columns:\n    plt.hist(data[i])\n    plt.title(i)\n    plt.show()","44fd1935":"sns.factorplot(x=\"slope\", y =\"target\", data=data, kind=\"bar\", size=3)\nplt.show()","d985dfc3":"sns.factorplot(x=\"exang\", y =\"target\", data=data, kind=\"bar\", size=3)\nplt.show()","41284cfc":"sns.factorplot(x=\"restecg\", y =\"target\", data=data, kind=\"bar\", size=3)\nplt.show()","305d8e62":"g = sns.FacetGrid(data, row=\"target\")\ng.map(sns.distplot, \"age\", bins=25)\nplt.show()","29099998":"g = sns.FacetGrid(data, row=\"target\")\ng.map(sns.distplot, \"chol\", bins=25)\nplt.show()","f0925d47":"#Sorted Chol values by age\nage_list= list(data.age.unique())\nchol_ratio=[]\nfor i in age_list:\n    x=data[data['age']==i]\n    chol_rate=sum(x.chol)\/len(x)\n    chol_ratio.append(chol_rate)\ndatac= pd.DataFrame({'age_list': age_list,'chol_ratio': chol_ratio})\nnew_index=(datac['chol_ratio'].sort_values(ascending=False)).index.values\nsorted_data=datac.reindex(new_index)\nsorted_data.head()","9b054213":"#Sorted trestbps values by age\nage_list= list(data.age.unique())\ntbps_ratio=[]\nfor i in age_list:\n    x=data[data['age']==i]\n    tbps_rate=sum(x.trestbps)\/len(x)\n    tbps_ratio.append(tbps_rate)\ndatat= pd.DataFrame({'age_list': age_list,'tbps_ratio': tbps_ratio})\nnew_index=(datat['tbps_ratio'].sort_values(ascending=False)).index.values\nsorted_data2=datat.reindex(new_index)\nsorted_data2.head()","dff53fea":"#We have values in two different ranges so I normalized them.\nsorted_data['chol_ratio']=sorted_data['chol_ratio']\/max(sorted_data['chol_ratio'])\nsorted_data2['tbps_ratio']=sorted_data2['tbps_ratio']\/max(sorted_data2['tbps_ratio'])\ndata_all=pd.concat([sorted_data,sorted_data2['tbps_ratio']],axis=1)\ndata_all.sort_values('chol_ratio',inplace=True)\ndata_all.head()","cb412217":"#Visualization with point plot\nf,ax1 = plt.subplots(figsize=(12,10))\nsns.pointplot(x=data_all['age_list'],y=data_all['chol_ratio'],data_all=data_all,color='lime',alpha=0.8)\nsns.pointplot(x=data_all['age_list'],y=data_all['tbps_ratio'],data_all=data_all,color='purple',alpha=0.8)\nplt.text(40,0.58,'chol ratio',color='lime',fontsize=18,style='normal')\nplt.text(40,0.55,'trestbps ratio',color='purple',fontsize=18,style='normal')\nplt.xlabel('Ages',fontsize=15,color='orange')\nplt.ylabel('Values',fontsize=15,color='orange')\nplt.title('Chol vs Trestbps Values',fontsize=20,color='orange')\nplt.grid()","3841ac30":"#I want to learn chest pain type (cp) according in heart data.\nlabels= data.cp.value_counts().index\ncolors=[\"orange\",\"pink\",\"brown\",\"gray\"]\nexplode= [0,0,0,0]\nsizes=data.cp.value_counts().values\n#Visualization with pie plot\nplt.figure(figsize=(7,7))\nplt.pie(sizes,explode=explode,labels=labels,colors=colors,autopct='%1.1f%%')\nplt.title('Chest pain type(cp) according to Heart Dataset',color=\"blue\",fontsize=15)","1ef22151":"#correlation matrix to find out the most related features to the \"target\" \ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cmap=\"coolwarm\", vmax=.8, square=True, annot=True);","c63cd663":"#it is too much features to look at, let's look at the top 10 features related to the target column\n# Top 10 Heatmap\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'target')['target'].index\ncm = np.corrcoef(data[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","14a84bc0":"def outlier_detect(feature, data):\n    outlier_index = []\n\n    for each in feature:\n        Q1 = np.percentile(data[each], 25)\n        Q3 = np.percentile(data[each], 75)\n        IQR = Q3 - Q1\n        min_quartile = Q1 - 1.5*IQR\n        max_quartile = Q3 + 1.5*IQR\n        outlier_list = data[(data[each] < min_quartile) | (data[each] > max_quartile)].index\n        outlier_index.extend(outlier_list)\n        \n    outlier_index = Counter(outlier_index)\n    #If there are three or more outlier data features we must delete them. (n)\n    outlier_data = list(i for i, n in outlier_index.items() if n > 3)\n    return outlier_data","cea00c3a":"outlier_data = outlier_detect([\"age\",\"trestbps\",\"restecg\",\"thalach\",\"chol\",\"oldpeak\"], data)\ndata.loc[outlier_data]","89411386":"data = data.drop(outlier_data, axis=0).reset_index(drop=True)","2e7819ae":"#missing data\ntotal = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","95665070":"# example of grid searching key hyperparametres for logistic regression\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n# define dataset\nX = data.drop('target', axis=1)\ny = data.target\n\n# define models and parameters\nmodel = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01, 0.001]\n\n# define grid search\ngrid = dict(solver=solvers, penalty=penalty, C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","608080a9":"from sklearn.linear_model import RidgeClassifier\n\n# define models and parameters\nmodel = RidgeClassifier()\nalpha = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n\n# define grid search\ngrid = dict(alpha=alpha)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","5d8a2178":"from sklearn.neighbors import KNeighborsClassifier\n\n# define models and parameters\nmodel = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","302a2724":"from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nx_sc = StandardScaler()\nX_std = x_sc.fit_transform(X)\n\n# define model and parameters\nmodel = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01]\ngamma = [0.001, 0.01, 0.1, 1, 10]\n\n# define grid search\ngrid = dict(kernel=kernel, C=C, gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X_std, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","af086837":"from sklearn.ensemble import BaggingClassifier\n\n# define models and parameters\nmodel = BaggingClassifier()\nn_estimators = [10, 100, 1000, 1500]\n\n# define grid search\ngrid = dict(n_estimators=n_estimators)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","2876e40b":"from sklearn.ensemble import RandomForestClassifier\n\n# define models and parameters\nmodel = RandomForestClassifier()\nn_estimators = [10, 100, 1000, 1500]\nmax_features = ['sqrt', 'log2']\n\n# define grid search\ngrid = dict(n_estimators=n_estimators, max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","a1cd69b5":"from sklearn.ensemble import GradientBoostingClassifier\n\n# define models and parameters\nmodel = GradientBoostingClassifier()\nn_estimators = [10, 100, 1000]\nlearning_rate = [0.001, 0.01, 0.1]\nsubsample = [0.5, 0.7, 1.0]\nmax_depth = [3, 7, 9]\n\n# define grid search\ngrid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, \n            max_depth=max_depth)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, \n                           scoring='accuracy',error_score=0, iid=True)\ngrid_result = grid_search.fit(X, y)\n\n# summarize results\nprint(f\"Best: {grid_result.best_score_:.3f} using {grid_result.best_params_}\")","63a984c4":"# Summary\n* in this notebook we have learnded very interesting topics such as:\n\n**Comprehensive EDA Processes**\n* Data Underestanding\n* Data visualization\n* Feature Analysis\n* Correlation Matrix\n* Data Preprocessing and Prepration\n    1. Outlier detection\n    2. Missing Values\n    \n**Comprehensive Building Modelas and hyperparameters Tuning**\n* Building Different Model like:\n    1. Logistic Regression\n    2. Ridge Classifier\n    3. K-Nearest Neighbors (KNN)\n    4. Support Vector Machine (SVM)\n    5. Bagged Decision Trees (Bagging)\n    6. Random Forest\n    7. Stochastic Gradient Boosting\n* hyperparameters Tuning\n    1. Grid Search\n    2. Randomized Search","9aa8154b":"# Data Underestanding ","f0e880b8":"# Age & target","3073b425":"# 2. Ridge Classifier\n\nRidge regression is a penalized linear regression model for predicting a numerical value.\n\nNevertheless, it can be very effective when applied to classification.\n\nPerhaps the most important parameter to tune is the regularization strength (alpha). A good starting point might be values in the range `[0.1 to 1.0]`\n\n- alpha in `[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]`","6f273e3e":"# slope & target","9618f501":"# Outlines\n\n**Comprehensive EDA Processes**\n* Data Underestanding\n* Data visualization\n* Feature Analysis\n* Correlation Matrix\n* Data Preprocessing and Prepration\n    1. Outlier detection\n    2. Missing Values\n    \n**Comprehensive Building Modelas and hyperparameters Tuning**\n* Building Different Models like:\n    1. Logistic Regression\n    2. Ridge Classifier\n    3. K-Nearest Neighbors (KNN)\n    4. Support Vector Machine (SVM)\n    5. Bagged Decision Trees (Bagging)\n    6. Random Forest\n    7. Stochastic Gradient Boosting\n* hyperparameters Tuning\n    1. Grid Search\n    2. Randomized Search","3fc97907":"# Classification Heart Disease\n* In this notebook, we will do a alot of data viasualization and classification of the heart disease\n","f76efba0":"# 3. K-Nearest Neighbors (KNN)\n\nThe most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n\nTest values between at least `1` and `21`, perhaps just the odd numbers.\n\n- n_neighbors in `[1 to 21]`\n\nIt may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n\n- metric in `[\u2018euclidean\u2019, \u2018manhattan\u2019, \u2018minkowski\u2019]`\n\nIt may also be interesting to test the contribution of members of the neighborhood via different weightings (weights).\n\n- weights in `[\u2018uniform\u2019, \u2018distance\u2019]`","c1b83969":"# Buidling the model","a004e670":"# chol & target","654e2d47":"# 1. Logistic Regression\n\nLogistic regression does not really have any critical hyperparameters to tune.\n\nSometimes, you can see useful differences in performance or convergence with different solvers (solver).\n- **solver** in `[\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019]`\n\nRegularization (penalty) can sometimes be helpful.\n- **penalty** in `[\u2018none\u2019, \u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019]`\n\n**Note:** not all solvers support all regularization terms.\n\n- The **C** parameter controls the penality strength, which can also be effective. **C** in `[100, 10, 1.0, 0.1, 0.01]`\n\nThe example below demonstrates grid searching the key hyperparameters for LogisticRegression on a synthetic binary classification dataset.","af29fcd7":"# Data Visualization","ea858a9d":"* Great, we do not have missing values","303fa8c7":"* Let's plot the target variables in a Bar chart to get more underestanding of it","9e8c3dd0":"# Import the necessary libraries","82d88cc5":"# 7. Stochastic Gradient Boosting\n\nAlso called Gradient Boosting Machine (GBM) or named for the specific implementation, such as XGBoost.\n\nThe gradient boosting algorithm has many parameters to tune.\n\nThere are some parameter pairings that are important to consider. The first is the learning rate, also called shrinkage or eta (`learning_rate`) and the number of trees in the model (`n_estimators`). Both could be considered on a log scale, although in different directions.\n\n- learning_rate in `[0.001, 0.01, 0.1]`\n- n_estimators `[10, 100, 1000]`\n\nAnother pairing is the number of rows or subset of the data to consider for each tree (`subsample`) and the depth of each tree (`max_depth`). These could be grid searched at a 0.1 and 1 interval respectively, although common values can be tested directly.\n\n- subsample in `[0.5, 0.7, 1.0]`\n- max_depth in `[3, 7, 9]`","160dae94":"* Now, Let's plot the sex column to see how Male and Female categories are distibuted\n* Luckinly we have the amazing library Seaborn to see the relationshipp between the male and female","a5c8b3de":"# Correlation Matrix","a2de7420":"# Outlier Detection for our dataset\n![download.png](attachment:download.png)\n\n* Q1 = 1.Quartile 25%\n* Q2 = 2.Quartile 50% (median)\n* Q3 = 3.Quartile 75%\n* IQR = Q3 - Q1\n* Outlier data = (Q1 - 1.5 IQR ) U (Q3 + 1.5 IQR)","6b0bb6ca":"# 4. Support Vector Machine (SVM)\n\nThe SVM algorithm, like gradient boosting, is very popular, very effective, and provides a large number of hyperparameters to tune.\n\nPerhaps the first important parameter is the choice of kernel that will control the manner in which the input variables will be projected. There are many to choose from, but `linear`, `polynomial`, and `RBF` are the most common, perhaps just linear and RBF in practice.\n\n- kernels in `[\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019]`\n\nIf the polynomial kernel works out, then it is a good idea to dive into the degree hyperparameter.\n\nAnother critical parameter is the `penalty (C)` that can take on a range of values and has a dramatic effect on the shape of the resulting regions for each class. A log scale might be a good starting point.\n\n- C in `[100, 10, 1.0, 0.1, 0.001]`","45a5887d":"# Missing Values","db84c62f":"# Hope, you enjoy reading my notebook\n# I would appreciate any comments and supports for improving ","100943b4":"# 5. Bagged Decision Trees (Bagging)\n\nThe most important parameter for bagged decision trees is the number of trees (n_estimators).\n\nIdeally, this should be increased until no further improvement is seen in the model.\n\nGood values might be a log scale from 10 to 1,000.\n\n- n_estimators in `[10, 100, 1000]`","e5cde741":"## Grid Search\nAll you need to do in GridSearch is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values, using cross-validation.\n\n## Randomized Search\nthe grid search approach is fine when you are exploring relatevely few combinations, but when the hyperparameter search space is large, it is often preferable to use `RandomizedSearchCV` instead. This technique evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:\n- If you let the randomized search run for 1000 iterations it will explore 1000 different values for each hyperparameter.\n- You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations.","c2e59cf5":" Great, we do not have outlier data in our dataset","ef031a4e":"# Gaining some information about the dataset\n","19e95855":"# Data Prepration","aabb58e8":"# exang & target","e3a68c8f":"# Feature Analysis","94704508":"Great! we do not have imbalanced data in the target variables","7cb7303e":"* age\n* sex\n* chest pain type (4 values)\n* resting blood pressure\n* serum cholestoral in mg\/dl\n* fasting blood sugar > 120 mg\/dl\n* resting electrocardiographic results (values 0,1,2)\n* maximum heart rate achieved\n* exercise induced angina\n* oldpeak = ST depression induced by exercise relative to rest\n* the slope of the peak exercise ST segment\n* number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","4e4210eb":"# 6. Random Forest\n\nThe most important parameter is the number of random features to sample at each split point (max_features).\n\nYou could try a range of integer values, such as 1 to 20, or 1 to half the number of input features.\n\n- max_features `[1 to 20]`\n\nAlternately, you could try a suite of different default value calculators.\n\n- max_features in `[\u2018sqrt\u2019, \u2018log2\u2019]`\n\nAnother important parameter for random forest is the number of trees (n_estimators).\n\nIdeally, this should be increased until no further improvement is seen in the model.\n\nGood values might be a log scale from 10 to 1,000.\n\n- n_estimators in `[10, 100, 1000]`","39ae4899":"# Loading the dataset into the kernel","b2fafe54":"Lets compare\n\n* age\n* chol (serum cholestoral in mg\/dl),\n* trestbps values (resting blood pressure) and find the correlation!","764633fe":"# restecg & target"}}