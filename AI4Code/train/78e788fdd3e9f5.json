{"cell_type":{"dbb26232":"code","ae30580b":"code","39dbcc7f":"code","cf386262":"code","9f72fd2f":"code","ebf65d9d":"code","badef66e":"code","66813522":"code","7e188b44":"code","10b435af":"code","99baa070":"code","62a825c5":"code","c3b32afa":"code","8d73176d":"code","3609b177":"code","79cfdae1":"code","0c244188":"code","2892bf97":"code","abb9bdee":"code","428518c7":"code","3f1e167f":"code","e47586b5":"code","ec5aca75":"code","d95966fa":"code","a2d751a0":"code","eb951d52":"code","a537f804":"code","cdc78991":"code","66620abe":"code","18b0ec3f":"code","c5c42629":"code","631fb163":"code","06b9989c":"code","b81132da":"code","9cb5bb2e":"code","344446d4":"code","f78acaf7":"code","84fd5a42":"code","8eb19c16":"code","9da8de5d":"code","7a6a8d1c":"code","e9121476":"code","b36faa3b":"code","10ac015c":"code","6f639dd1":"code","9eb9eebf":"code","2318fb13":"code","898368e8":"code","46eb7cdc":"code","271425b4":"markdown","fef53c4d":"markdown","96e70a17":"markdown","c4d5cc3e":"markdown","7efafa2f":"markdown","273aa7cf":"markdown","c9827f2b":"markdown","aca9dbda":"markdown","d02a7d6f":"markdown","9f9043d0":"markdown","803df2b4":"markdown","a5ef56ea":"markdown","e9d9cd20":"markdown","e0dbe8e4":"markdown","75162642":"markdown","39f04471":"markdown","3e90b1fd":"markdown","dc6224ab":"markdown","07fa0339":"markdown","82468d24":"markdown"},"source":{"dbb26232":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ae30580b":"import pandas as pd\nimport numpy as np\nimport operator \nimport re\nimport gc\nimport keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')","39dbcc7f":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","cf386262":"df = pd.concat([train, test])","9f72fd2f":"print(\"Total number of examples: \", df.shape[0])","ebf65d9d":"!pip install pytorch-pretrained-bert","badef66e":"import torch\nfrom pytorch_pretrained_bert import BertTokenizer\n\n# Load pre-trained model tokenizer (vocabulary)\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","66813522":"with open(\"vocabulary.txt\", 'w') as f:\n    \n    # For each token...\n    for token in tokenizer.vocab.keys():\n        \n        # Write it out and escape any unicode characters.            \n        f.write(token + '\\n')","7e188b44":"# this buids the vocab. of our dataset\ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","10b435af":"vocab = build_vocab(df['text'])","99baa070":"list(vocab.keys())[:10]","62a825c5":"# this check how much of our vocab is similar to the BERT vocab.\ndef check_coverage(vocab, embeddings_index):\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.3%} of vocab'.format(len(known_words) \/ len(vocab)))\n    print('Found embeddings for  {:.3%} of all text'.format(nb_known_words \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words","c3b32afa":"print(\"BERT\")\noov_bert = check_coverage(vocab, tokenizer.vocab)","8d73176d":"oov_bert[:10]","3609b177":"tokenizer.vocab[\"I\"]","79cfdae1":"tokenizer.vocab[\"i\"]","0c244188":"df['lowered_text'] = df['text'].apply(lambda x: x.lower())","2892bf97":"vocab_lower = build_vocab(df['lowered_text'])\nprint(\"BERT EMBEDDINGS\")\noov_bert = check_coverage(vocab_lower, tokenizer.vocab)","abb9bdee":"oov_bert[:25]","428518c7":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","3f1e167f":"def known_contractions(embed):\n    known = []\n    for contract in contraction_mapping:\n        if contract in embed:\n            known.append(contract)\n    return known","e47586b5":"print(\"- Known Contractions -\")\nprint(\"   BERT :\")\nprint(known_contractions(tokenizer.vocab))","ec5aca75":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","d95966fa":"df['treated_text'] = df['lowered_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","a2d751a0":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","eb951d52":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'","a537f804":"def unknown_punct(embed, punct):\n    unknown = ''\n    for p in punct:\n        if p not in embed:\n            unknown += p\n            unknown += ' '\n    return unknown","cdc78991":"print(\"BERT :\")\nprint(unknown_punct(tokenizer.vocab, punct))","66620abe":"punct_mapping = { '\u00e0': 'a'}","18b0ec3f":"oov_bert[:10]","c5c42629":"tokenizer.vocab[\"amp\"]","631fb163":"print(tokenizer.tokenize(\"thunderstorm\"))\nprint(tokenizer.tokenize(\"11-year-old\"))\nprint(tokenizer.tokenize(\"@youtube\"))\nprint(tokenizer.tokenize(\"\\x89\u00fb_\"))","06b9989c":"bad_words = []\nfor i in range(len(oov_bert)):\n    if oov_bert[i][0][0] ==\"\\x89\":\n        bad_words.append(oov_bert[i])","b81132da":"bad_dict = {}\nfor i in range(len(bad_words)):\n    bad_dict[bad_words[i][0]] = \"\"","9cb5bb2e":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' {p} ')\n    \n    specials = bad_dict  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","344446d4":"df['treated_text'] = df['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","f78acaf7":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","84fd5a42":"oov_bert[:25]","8eb19c16":"for i in oov_bert[:25]:\n    print(tokenizer.tokenize(i[0]))","9da8de5d":"explicit_mapping = {\"\\x89\u00fb\": \"\", \"mh370\" : \"flight\", \"legionnaires\": \"pneumonia\", \n                   \"derailment\": \"railway accident\", \"inundated\": \"flood\", \"deluged\": \"flood\", \n                   \"curfew\": \"stay at home\",\"obliteration\": \"destruction\", \n                   \"quarantine\": \"prevent the spread of disease\", \"lol\": \"laugh\", \n                   \"obliterate\": \"destroy\", \"hijacking\": \"seize\", \"detonation\": \"explosion\", \n                   \"electrocuted\": \"killed\", \"destroyd\": \"destroyed\"}","7a6a8d1c":"def explicit_changes(text, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    return text","e9121476":"df['treated_text'] = df['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","b36faa3b":"vocab = build_vocab(df['treated_text'])\nprint(\"BERT : \")\noov_bert = check_coverage(vocab, tokenizer.vocab)","10ac015c":"oov_bert[:20]","6f639dd1":"# lower\ntrain['treated_text'] = train['text'].apply(lambda x: x.lower())\n# clean contractions\ntrain['treated_text'] = train['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\n# clean special chars - this is optional as most of the punct. are in BERT embed.\ntrain['treated_text'] = train['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n# cleaning some word\ntrain['treated_text'] = train['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","9eb9eebf":"train.head()","2318fb13":"test['treated_text'] = test['text'].apply(lambda x: x.lower())\ntest['treated_text'] = test['treated_text'].apply(lambda x: clean_contractions(x, contraction_mapping))\ntest['treated_text'] = test['treated_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\ntest['treated_text'] = test['treated_text'].apply(lambda x: explicit_changes(x, explicit_mapping))","898368e8":"test.head()","46eb7cdc":"# Saving out work\ntrain.to_csv(\"train_BERT_preprocessed.csv\")\ntest.to_csv(\"test_BERT_preprocessed.csv\")","271425b4":"## Dealing with Contractions","fef53c4d":"Now let us check what's missing in our vocab. ","96e70a17":"## Now let us clean the special Characters.","c4d5cc3e":"**I will be adding more stuff soon!** <br>\nSo stay in touch.","7efafa2f":"Again, that's a great improvment.","273aa7cf":"First Faults appearing are: \n* Contractions \n* Punctuations\n* Some words like **\\x89\u00fb\u00f2**\n> Let's correct that.","c9827f2b":"Now we have done most of our job in preprocessing our data.","aca9dbda":"Above example clearly demonstrate that the word **I** is not in vocab, but the word **i** is this is because we have imported `bert-base-uncased`. Let's make the text lower.","d02a7d6f":"That's a significant amount of improvement. ","9f9043d0":"For those top 25 words let us do something.","803df2b4":"Oh Shit! Contractions doesn't exist in BERT embeddings. <br>\nThis is really is big problem for us. Let fix it up! ","a5ef56ea":"As we can see above some of them are understood by tokenizer & for rest of them let us explicitly make a mapping dictionary.","e9d9cd20":"Most of the work will be done by our tokenizer that will tokenize the words. For eg. thunderstorm can be broken up as thunder + storm ","e0dbe8e4":"That's great only 1 unknown punct. from our punct. set.","75162642":"## What we are going to do?\nPre-Processing the data, so as to make it similar to BERT Vocab.","39f04471":"Now let us delete some words like \\x89...","3e90b1fd":"## Importing BERT Vocab.","dc6224ab":"<h2 style = \"color:red\" >Please Upvote, if you like this kernel.<\/h2>","07fa0339":"## Lowering the text","82468d24":"A lot of material for this kernel is taken from [Theo Viel](https:\/\/www.kaggle.com\/theoviel) kernel on [Improve your Score with Text Preprocessing](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-text-preprocessing-v2). <br>\n\n**Any feedback would be greatly appreciated. Thank you**"}}