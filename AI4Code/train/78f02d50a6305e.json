{"cell_type":{"64f1d8d4":"code","945d8067":"code","f0680fd9":"code","476e7fb3":"code","04f358ed":"code","aa38e4ca":"code","97e82f2d":"code","1f4fa411":"code","ebd86e84":"code","9b77e4ec":"code","ccf3a018":"code","df92424f":"code","639f1467":"code","72079316":"code","f8c75fd0":"code","86aca1c6":"code","daa4cd87":"code","b44283cb":"code","e6933f2b":"code","805801d1":"code","0d13c981":"markdown","aca47531":"markdown","16306cc3":"markdown","9bc624da":"markdown","c8cbd2d0":"markdown","b594ea34":"markdown","b29d3e76":"markdown"},"source":{"64f1d8d4":"pip install pycaret --quiet","945d8067":"# import packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport pycaret\nfrom pycaret.regression import *\nprint('PyCaret: %s' % pycaret.__version__)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint('Matplotlib: {}'.format(matplotlib.__version__))\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f0680fd9":"# read in competition data\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","476e7fb3":"# create experiment to setup pycaret environment and define pre-processing pipeline\n# set silent=True only after you checked the data types! this removes the user prompt and allows to run \"save & run all\", i.e store the notebook output\npc_experiment = setup(data=df_train, target=df_train.columns[-1], \n                      normalize = True, transformation = True, \n                      silent= True, log_experiment=True, experiment_name=\"TPS8_normalize\")","04f358ed":"# let's see which models are available for regression in Pycaret\n#models()","aa38e4ca":"# let's have a look at the normalized and transformed data\nget_config('X_train')","97e82f2d":"get_config('y_train')","1f4fa411":"# compare different baseline models with 5 fold cross-validation, I ran this only once due to the execution time\n#best_3_models = compare_models(fold=5, n_select=3)\n\n\n# RSME with defaults\n# catboost   7.8681\n# lightgbm   7.8729\n# gbr        7.8866\n# ... lr     7.9386\n\n# RSME with normalize=True and transform=True\n# catboost   7.8569\n# lightgbm   7.8619\n# gbr        7.8746\n# lr         7.8914","ebd86e84":"# create model - this function uses the default hyperparameters\n# you actually don't need this line if you run everything in one session. best_model would already return the trained best model\n# see here: https:\/\/towardsdatascience.com\/5-things-you-are-doing-wrong-in-pycaret-e01981575d2a\ncatboost = create_model('catboost')\nlightgbm =  create_model('lightgbm')\nlr = create_model('lr') # use linear regression (4th place) instead of gbr (3rd place), because gbr has huge training time","9b77e4ec":"# have a look at the used hyperparameters\nprint(lightgbm)\nprint(lr)\nprint(catboost) # for catboost this does not give the hyperparameteres, use the line below instead\ncatboost.get_all_params()","ccf3a018":"# compare the predictions (yhat) to the ground truth (y)\nmodellist = [catboost, lightgbm, lr]\n# fig, ax = plt.subplots(1, 3, figsize=(20,10)) # I could not figure out how to get the three plots in one row, plot_model does not support using ax=i\n\nplot_model(catboost, plot = 'error')","df92424f":"plot_model(lightgbm, plot = 'error')","639f1467":"plot_model(lr, plot = 'error')","72079316":"# build voting regressor that uses the top 3 models and averages the individual predictions\n# the resulting blended model can be used like any other model in pycaret\nblender_top3 = blend_models(estimator_list = modellist)","f8c75fd0":"# prediction on validation\/holdout set to assess model performance\nholdout_predictions = predict_model(blender_top3)\nholdout_predictions.head()","86aca1c6":"# prediction on validation\/holdout set to assess model performance\nholdout_predictions_cb = predict_model(catboost)\nholdout_predictions_cb.head()","daa4cd87":"# finalize model: re-train on whole training data (=Transformed Train Set (70%)+ Transformed Test Set (30%))\nfinal_model = finalize_model(blender_top3)","b44283cb":"# predict for test set\ntest_predictions = predict_model(final_model, data=df_test)\ntest_predictions.head()","e6933f2b":"# check which format the submission needs to have\nsample_submission.head()","805801d1":"# prepare submission file\nsubmission = test_predictions[['id', 'Label']]\nsubmission = submission.rename(columns={\"Label\": \"loss\"})\nsubmission.to_csv('submission_blender.csv', index=False)\nsubmission.head()","0d13c981":"# About\nThis time I decided to follow a completely differnt approach than in the last Tabular Playground Competitions: Use Auto ML Tools.\n\nI will NOT do EDA. \n\nI will NOT learn anything about the data. \n\nI will just throw it into an Auto ML Tool and observe the result. In this notebook, it's **Pycaret**.\n\n(Somehow I already feel bad... I love EDA... but hey, I think it's time to try out something new this time)\n\n.................\n\n**Update**: in Version 9 I started to play with pycarets preprocessing pipeline to see if I can improve the score. Additionally I included a voting regressor that averages the predictions of 3 models","aca47531":"**Conclusions**:\n- I learned a lot about pycaret\n- I feel strange, because I still don't know much about the data I used\n- The tuning process in Pycaret did not give the results I hoped for. I did not understand yet why.\n- I am impressed about the results: Looking at the leaderboard the public score is well in line with what the other competitors get. And I did not spend any time on the data itself, just on learning to use pycaret\n\n**What are your experiences with using Auto ML tools? I'd be happy if you comment below.**","16306cc3":"The data has been split into Transformed Train Set and Transformed Test Set. The split was done with the default of 70\/30. The Transformed Test Set (30%) is the validation set (holdout set) that is used later to assess model performance. It's important to note that the validation set is not used for cross validation. Cross validation is done on the Transformed Training Set (70%) ([see here](https:\/\/github.com\/pycaret\/pycaret\/blob\/master\/tutorials\/Regression%20Tutorial%20Level%20Beginner%20-%20REG101.ipynb)). Pycaret will use 10-fold cross validation.\n\nWe can see here that Pycaret did not create any additional features (like it would e.g. do when one hot encoding categorical features): The Transformed Train Set and the Transformed Test Set have still 100 features. ","9bc624da":"**Oberservations while waiting for pycaret to finish model evaluation:**\n\nThe first three quaters on the processing bar passed quite quickly. Then the visible progress stalled. \"Estimator\" is changing from time to time... so it seems to be still running.\n\nPycaret runs algorithms I haven't even heard of...\n\nI have time to work out, clean and read a book while waiting.\n\n....................................................................................Now, it's done and the winners are: **CatBoost, LightGBM and GBR**!","c8cbd2d0":"I checked quickly the data types and that there are no missing values. Nothing more. Now let's throw the data into pycaret.","b594ea34":"I am not really satisfied with the RSME of the blended model. It is worse than the RSME of catboost on the untransformed dataset (which was 7.8658). So let's see what catboost alone gets with the transformed dataset. (-> 7.8767). This is basically the same as with blending. So, at least on the holdout set, blending seems to have no big effect.","b29d3e76":"The models are getting more and more conservative, none of them is able to predict higher values."}}