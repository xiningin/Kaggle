{"cell_type":{"d1825571":"code","4edcd7bf":"code","f6448359":"code","7448b2b7":"code","c07778e9":"code","becdadf5":"code","5af439f3":"code","eb70ebd6":"code","b31ff038":"code","36fbf791":"code","4c820cd8":"code","806308f4":"code","ef812462":"code","fe908664":"code","6386c96e":"code","ba63a901":"code","2ec04d15":"code","35cbac40":"code","26a6ab87":"code","427264c6":"markdown","d5d0f837":"markdown","e934087f":"markdown","ab2bb353":"markdown","9568d9f9":"markdown","a629f0c3":"markdown","6b8ef62a":"markdown"},"source":{"d1825571":"import re\nimport math\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom kaggle_datasets import KaggleDatasets\nprint(tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\nGCS_PATH=KaggleDatasets().get_gcs_path('512x512-melanoma-tfrecords-70k-images')","4edcd7bf":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set.\n    # On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f6448359":"SEED = 42\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\n\nIMAGE_SIZE = [512,512]\n\nLR = 0.00004\n\nEPOCHS = 25\n\nWARMUP = 5\n\nWEIGHT_DECAY = 0\n\nLABEL_SMOOTHING = 0.05\n\nTTA = 4","7448b2b7":"def seed_everything(SEED):\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n\nseed_everything(SEED)\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH+'\/train*')\n\nTRAINING_FILENAMES,VALIDATION_FILENAMES=train_test_split(TRAINING_FILENAMES,test_size=0.2,random_state=SEED)\n\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH+'\/test*')","c07778e9":"print(len(TRAINING_FILENAMES),len(VALIDATION_FILENAMES))","becdadf5":"def decode_augument_image(image_data,seed=SEED):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.bfloat16) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    image = tf.image.rot90(image,k=np.random.randint(4))\n    image = tf.image.random_flip_left_right(image,seed=seed)\n    image = tf.image.random_flip_up_down(image,seed=seed)\n    return image\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.bfloat16) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*IMAGE_SIZE, 3]) # explicit size needed for TPU\n    return image\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),  \n        \"sex\": tf.io.FixedLenFeature([], tf.int64), \n        \"anatom_site_general_challenge\" : tf.io.FixedLenFeature([] , tf.int64),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_augument_image(example['image'])\n    age = tf.cast(example['age_approx'], tf.bfloat16)\n    sex = tf.cast(example['sex'], tf.bfloat16)\n    asg = tf.cast(example['anatom_site_general_challenge'] , tf.bfloat16)\n    target = tf.cast(example['target'], tf.int32)\n    return (image, tf.stack([age,sex,asg])),target\n\ndef read_valid_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),  \n        \"sex\": tf.io.FixedLenFeature([], tf.int64), \n        \"anatom_site_general_challenge\" : tf.io.FixedLenFeature([] , tf.int64),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),  \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    age = tf.cast(example['age_approx'], tf.bfloat16)\n    sex = tf.cast(example['sex'], tf.bfloat16)\n    asg = tf.cast(example['anatom_site_general_challenge'] , tf.bfloat16)\n    target = tf.cast(example['target'], tf.int32)\n    return (image, tf.stack([age,sex,asg])),target\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"age_approx\": tf.io.FixedLenFeature([], tf.int64),  \n        \"sex\": tf.io.FixedLenFeature([], tf.int64), \n        \"anatom_site_general_challenge\" : tf.io.FixedLenFeature([] , tf.int64),\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    age = tf.cast(example['age_approx'], tf.bfloat16)\n    sex = tf.cast(example['sex'], tf.bfloat16)\n    asg = tf.cast(example['anatom_site_general_challenge'] , tf.bfloat16)\n    idnum = example['image_name']\n    return (image,tf.stack([age,sex,asg])),idnum# returns a dataset of image(s)\n\ndef load_dataset(filenames, labeled=True, ordered=False, valid=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    if labeled:\n      dataset = dataset.map(read_labeled_tfrecord,num_parallel_calls=AUTO) \n    elif labeled and valid:\n      dataset = dataset.map(read_valid_labeled_tfrecord,num_parallel_calls=AUTO)\n    else:\n      dataset = dataset.map(read_unlabeled_tfrecord,num_parallel_calls=AUTO)\n\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES ,labeled=True,valid=False)\n    dataset = dataset.shuffle(SEED)\n    dataset = dataset.batch(BATCH_SIZE,drop_remainder=True)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(ordered=False):\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled=True ,valid=True, ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=True):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, valid=False, ordered=ordered)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TRAINING_IMAGES = count_data_items(TRAINING_FILENAMES)\nNUM_VALID_IMAGES = count_data_items(VALIDATION_FILENAMES)\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nSTEPS_PER_EPOCH = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nVALIDATION_STEPS = NUM_VALID_IMAGES \/\/ BATCH_SIZE\nprint('Dataset: {} training images ,{} validation images,{} unlabeled test images'.format(NUM_TRAINING_IMAGES,NUM_VALID_IMAGES,NUM_TEST_IMAGES))\nprint(\"STEPS_PER_EPOCH are {}\".format(STEPS_PER_EPOCH))\nprint(\"validation Steps are {}\".format(VALIDATION_STEPS))","5af439f3":"!pip install -q efficientnet","eb70ebd6":"from tensorflow.keras import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom efficientnet.tfkeras import *","b31ff038":"def create_model():\n    base_model=EfficientNetB7(include_top=False,weights='imagenet',pooling='avg',input_shape=(*IMAGE_SIZE,3))\n    base_model.trainable=False\n    inp1=Input(shape=(*IMAGE_SIZE,3))\n    inp2=Input(shape=(3,))\n    X=base_model(inp1,training=False)\n    #X=GlobalAveragePooling2D()(X)\n    Z=Dense(512,activation='relu')(inp2)\n    Z=BatchNormalization()(Z)\n    Z=Dropout(0.4)(Z)\n    Z=Dense(1024,activation='relu')(Z)\n    Z=BatchNormalization()(Z)\n    Z=Dropout(0.4)(Z)\n    X=Concatenate()([X,Z])\n    X=Dense(1024,activation='relu')(X)\n    X=BatchNormalization()(X)\n    X=Dropout(0.4)(X)\n    Y=Dense(1,activation='sigmoid')(X)\n    return Model(inputs=[inp1,inp2],outputs=Y)","36fbf791":"with strategy.scope():\n    model = create_model()\n    \n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = LABEL_SMOOTHING),\n                  metrics=[tf.keras.metrics.AUC(),'accuracy'])\n    \n    model.summary()","4c820cd8":"tf.keras.utils.plot_model(model,show_layer_names=True,show_shapes=True)","806308f4":"def get_cosine_schedule_with_warmup(lr,num_warmup_steps, num_training_steps, num_cycles=0.5):\n    \"\"\"\n    Modified version of the get_cosine_schedule_with_warmup from huggingface.\n    (https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) \/ float(max(1, num_warmup_steps))) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nlr_schedule = get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)\n\nes=tf.keras.callbacks.EarlyStopping(monitor='val_auc',mode='max',patience=3,verbose=1)","ef812462":"model.fit(get_training_dataset(),\n          epochs=EPOCHS,\n          steps_per_epoch=STEPS_PER_EPOCH,\n          validation_data=get_validation_dataset(),\n          validation_steps=VALIDATION_STEPS,\n          callbacks=[es,lr_schedule],\n          verbose=2\n         )","fe908664":"import h5py\n\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n\nmodel.save_weights(\"mo.h5\")\nprint(\"Saved model to disk\")","6386c96e":"def display_training_curves(training, validation, title, subplot):\n    \"\"\"\n    Source: https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu\n    \"\"\"\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","ba63a901":" display_training_curves(\n        model.history.history['loss'], \n        model.history.history['val_loss'], \n        'loss', 211)\n display_training_curves(\n        model.history.history['accuracy'], \n        model.history.history['val_accuracy'], \n        'accuracy',212)\n display_training_curves(\n     model.history.history['auc'],\n     model.history.history['val_auc'],\n     'auc score ', 311)","2ec04d15":"y_pred = model.predict(get_validation_dataset())\n\n\ny_true = np.array([\n    target.numpy() for _, target in iter(get_validation_dataset().unbatch())])\n\nprint(y_true.shape)\nprint(y_pred.shape)","35cbac40":"from sklearn.metrics import roc_auc_score\nmetric_score=roc_auc_score(y_true,y_pred)\nprint(metric_score)","26a6ab87":"def predictions():\n    test_ds=get_test_dataset(ordered=True)\n\n    test_ds_features=test_ds.map(lambda feat,imname:(feat,0)).batch(BATCH_SIZE) #Getting the features of the Test_ds \n\n    preds=model.predict(test_ds_features) #predicting with the model\n\n    test_ids_ds = test_ds.map(lambda img, imname: imname)\n\n    test_ids = next(iter(test_ids_ds.batch(NUM_TEST_IMAGES))).numpy().astype('U')\n\n    prediction_df=pd.DataFrame({'image_name':test_ids ,'target':np.concatenate(preds)}) #writing to Dataframs\n\n    prediction_df.to_csv(\"submission.csv\",index=False) #Generating CSV file\n    \npredictions()","427264c6":"# Training ","d5d0f837":"**Hyper parameters**","e934087f":"**Hardware Detection**","ab2bb353":"#  Generate Predictions","9568d9f9":"**Data Splitting**","a629f0c3":"**Imports**","6b8ef62a":"# Preparing the dataset"}}