{"cell_type":{"0892d320":"code","dbf4e61b":"code","f828e868":"code","c11da06f":"code","6e94a72e":"code","002bfc17":"code","2d361e3a":"code","ea6739bb":"code","fd80941f":"code","a7a4ce29":"code","8ace79e6":"code","90c2ce9a":"code","fd264c09":"code","4b019052":"code","2d2df1b9":"code","3cb0d27f":"code","dd04d1d6":"code","b5f43f46":"code","b0d9c40b":"code","00483df5":"code","be12eb42":"code","37c390ef":"code","adef0fc9":"code","73fe02f4":"code","7a3b5140":"markdown","587e097d":"markdown","86f639c9":"markdown"},"source":{"0892d320":"import numpy as np\nimport pandas as pd\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","dbf4e61b":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPool2D, Conv3D, Bidirectional, LSTM, GRU, Add\nfrom tensorflow.keras.layers import MaxPool3D, UpSampling2D, GlobalMaxPool2D, GlobalMaxPool3D\nfrom tensorflow.keras.layers import GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalAveragePooling3D, Conv2DTranspose, Concatenate\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Input\n\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import to_categorical, plot_model\n\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.applications import NASNetMobile, Xception, DenseNet121, MobileNetV2, InceptionV3, InceptionResNetV2, vgg16, resnet50, inception_v3, xception, DenseNet201\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import CSVLogger\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import metrics\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import Sequence\nimport math\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import jaccard_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import RobustScaler\nfrom scipy import stats\n\nimport seaborn as sns\n\nimport skimage\nfrom skimage.transform import rotate\n\nfrom tqdm.notebook import tqdm\nfrom datetime import datetime\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, roc_curve, mean_absolute_error\nimport numpy as np\nimport os\nimport cv2\nimport pandas as pd\n# import imutils\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport urllib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom PIL import Image\n\nimport tensorflow_addons as tfa\n\nfrom IPython.display import HTML\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_color_lut\nimport re\n\nimport itertools\n\nfrom sklearn.utils import shuffle","f828e868":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","c11da06f":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv').astype(np.float32)\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv').astype(np.float32)","6e94a72e":"train","002bfc17":"def add_features(df):\n    df = df.copy()\n    \n    df_group = df.groupby(['breath_id'])\n    \n    feature_list = ['u_in', 'time_step', 'cross']\n    \n    df['cross']= df['u_in'] * df['u_out']\n    df['area_out']= df['time_step'] * df['u_out']\n    df['area'] = df['time_step'] * df['u_in']\n    \n    # add\n    df['air_flow_rate'] = df['u_out'] - (df['u_in']\/100.0)\n    df['air_flow_area'] = df['air_flow_rate'] * df['time_step']\n    print(\"Step-1...Completed\")\n    \n    # cumsum cummean\n    df['one'] = 1\n    df['count'] = df_group['one'].cumsum()\n    for feature in feature_list:\n        df[f'{feature}_cumsum'] = df_group[feature].cumsum()\n        df[f'{feature}_cummean'] = df[f'{feature}_cumsum'] \/ df['count']\n        \n    print(\"Step-2 cumsum cummean ...Completed\")\n    # lagging\n    use_lags = 4\n    for lag in range(1, use_lags+1):\n        for feature in feature_list:\n            # lag \n            df[f'{feature}_lag_{lag}'] = df_group[feature].shift(lag)\n            # inverse lag\n            df[f'{feature}_lag_inverse_{lag}'] = df_group[feature].shift(-lag)\n\n            # dif lag\n            df[f'{feature}_lag_diff_{lag}'] = df[feature] - df[f'{feature}_lag_{lag}']\n\n            # dif inverse lag\n            df[f'{feature}_lag_inverse_diff_{lag}'] = df[feature] - df[f'{feature}_lag_inverse_{lag}']\n\n            df = df.drop(columns=[f'{feature}_lag_{lag}', f'{feature}_lag_inverse_{lag}'])\n        \n    df = df.fillna(0)\n    print(\"Step-3 lagging ...Completed\")\n    \n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    df = df.fillna(0)\n    print(\"Step-4 categorical ...Completed\")\n    \n    rolling_list = [15]\n    for roll in rolling_list:\n        for feature in feature_list:\n            df[[f'{roll}_{feature}_sum',f'{roll}_{feature}_min',\n                f'{roll}_{feature}_max',f'{roll}_{feature}_mean']] = (df_group[feature]\\\n                                                                  .rolling(window=roll,min_periods=1)\\\n                                                                  .agg({f'{roll}_{feature}_sum':'sum',\n                                                                        f'{roll}_{feature}_min':'min',\n                                                                        f'{roll}_{feature}_max':'max',\n                                                                        f'{roll}_{feature}_mean':'mean'})\\\n                                                                   .reset_index(level=0,drop=True))\n    \n    print(\"Step-5 Sliding window...Completed\")\n    print()\n    \n    df = df.fillna(0)\n    df = df.drop(['id', 'breath_id','one','count'], axis=1)\n    \n    return df.astype(np.float16)","2d361e3a":"%%time\ntrain = add_features(train)\ntest = add_features(test)\ntrain","ea6739bb":"train.info()","fd80941f":"import gc\ngc.collect()","a7a4ce29":"targets = train[['pressure']].to_numpy().reshape(-1, 80)","8ace79e6":"train = train.drop(columns=['pressure'])","90c2ce9a":"%%time\ncorr = train.corr()","fd264c09":"threshold = 1\ncorr = np.abs(corr)\ndrop_columns = set()\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= threshold:\n            drop_columns.add(corr.columns[j])\n            \ndrop_columns = list(drop_columns)\ntrain = train.drop(columns=drop_columns)\ntest = test.drop(columns=drop_columns)\nprint(f'Columns drop size : {len(drop_columns)}')","4b019052":"scaler = RobustScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","2d2df1b9":"import pickle\npickle.dump(scaler, open('scaler.pickle','wb'))","3cb0d27f":"train = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, test.shape[-1])","dd04d1d6":"gc.collect()","b5f43f46":"with tpu_strategy.scope():\n    def dnn_model():\n        x_input = Input(shape=(train.shape[-2:]))\n\n        x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n        x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n        x3 = Bidirectional(LSTM(units=256, return_sequences=True))(x2)\n        x4 = Bidirectional(LSTM(units=128, return_sequences=True))(x3)\n\n        z2 = Bidirectional(GRU(units=256, return_sequences=True))(x2)\n        z3 = Bidirectional(GRU(units=128, return_sequences=True))(Add()([x3, z2]))\n        z4 = Bidirectional(GRU(units=64, return_sequences=True))(Add()([x4, z3]))\n\n        x = Concatenate(axis=2)([x4, z2, z3, z4])\n\n        x = Dense(units=128, activation='selu')(x)\n\n        x_output = Dense(units=1)(x)\n\n        model = Model(inputs=x_input, outputs=x_output,\n                      name='DNN_Model')\n\n        model.compile(optimizer='adam',\n                      loss='mae'\n        )\n\n        return model","b0d9c40b":"model = dnn_model()\nmodel.summary()","00483df5":"plot_model(\n    model, \n    to_file='Google_Brain_Keras_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)","be12eb42":"with tpu_strategy.scope():\n    \n    nfold = 5\n    VERBOSE = 0\n    kf = KFold(n_splits=nfold, shuffle=True, random_state=2021)\n    history_list = []\n    test_preds = []\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        model = dnn_model()\n\n        lr = ReduceLROnPlateau(monitor='val_loss', factor=0.75, \n                               patience=10, verbose=VERBOSE)\n        \n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n        chk_point = ModelCheckpoint(f'.\/Bidirect_LSTM_model_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_loss', verbose=VERBOSE, \n                                    save_best_only=True, mode='min')\n\n        es = EarlyStopping(monitor=\"val_loss\", patience=50, \n                           verbose=VERBOSE, mode=\"min\", \n                           restore_best_weights=True)\n        \n        tqdm_callback = tfa.callbacks.TQDMProgressBar(show_epoch_progress=False)\n        \n        history = model.fit(X_train, y_train, \n                          validation_data=(X_valid, y_valid), \n                          epochs=300,\n                          verbose=VERBOSE,\n                          batch_size=BATCH_SIZE, \n                          callbacks=[tqdm_callback, lr, chk_point, es])\n        \n        history_list += [history]\n        \n        y_true = y_valid.squeeze().reshape(-1, 1)\n        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n        score = mean_absolute_error(y_true, y_pred)\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        \n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())","37c390ef":"submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsubmission[\"pressure\"] = sum(test_preds)\/nfold\nsubmission.to_csv('submission_mean.csv', index=False)\nsubmission","adef0fc9":"submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsubmission[\"pressure\"] = np.median(test_preds, axis=0)\nsubmission.to_csv('submission_median.csv', index=False)\nsubmission","73fe02f4":"for fold, history in enumerate(history_list):\n    print(f'Fold : {fold+1}')\n    plt.figure(figsize=(15,7))\n    for name in ['loss', 'val_loss']:\n        plt.plot(history.history[name])\n    plt.legend(['loss', 'val_loss'])\n    plt.show()\n","7a3b5140":"# Model","587e097d":"* Fold-1 | OOF Score: 0.16721594333648682\n* Fold-2 | OOF Score: 0.17076881229877472\n* Fold-3 | OOF Score: 0.16922391951084137\n* Fold-4 | OOF Score: 0.16878041625022888\n* Fold-5 | OOF Score: 0.16836853325366974","86f639c9":"# Train"}}