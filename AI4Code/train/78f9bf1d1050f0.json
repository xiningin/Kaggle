{"cell_type":{"30d29084":"code","b0c2f31b":"code","d110a720":"code","5c33bab3":"code","052276ea":"code","d3f19654":"code","536de1ac":"code","2f8214a2":"code","fa8a4f9a":"code","a593249a":"code","a8c21a88":"code","b60b8bf3":"code","2969934e":"code","782fa853":"code","734bb3a0":"code","d94c0770":"code","bf162d43":"code","776c09a6":"code","8efa2c8c":"code","8ecabcc7":"markdown","31c329f4":"markdown","7dce1035":"markdown","0dcd44d1":"markdown","5cea9b3a":"markdown","79172376":"markdown","b432cc54":"markdown","6ae40624":"markdown"},"source":{"30d29084":"from fastai.core import *\nPath.read_csv = lambda o: pd.read_csv(o)\ninput_path = Path(\"\/kaggle\/input\/data-science-bowl-2019\")\npd.options.display.max_columns=200\npd.options.display.max_rows=200\ninput_path.ls()","b0c2f31b":"sample_subdf = (input_path\/'sample_submission.csv').read_csv()\nspecs_df = (input_path\/\"specs.csv\").read_csv()\ntrain_df = (input_path\/\"train.csv\").read_csv()\ntrain_labels_df = (input_path\/\"train_labels.csv\").read_csv()\ntest_df = (input_path\/\"test.csv\").read_csv()","d110a720":"assert set(train_df.installation_id).intersection(set(test_df.installation_id)) == set()","5c33bab3":"train_with_features_part1 = pd.read_feather(\"..\/input\/dsbowl2019-feng-part1\/train_with_features_part1.fth\")","052276ea":"train_with_features_part1.shape, test_df.shape, train_labels_df.shape","d3f19654":"train_with_features_part1.head()","536de1ac":"test_df.head()","2f8214a2":"# there shouldn't be any common installation ids between test and train \nassert set(train_df.installation_id).intersection(set(test_df.installation_id)) == set()","fa8a4f9a":"from fastai.tabular import *\nimport types\n\nstats = [\"median\",\"mean\",\"sum\",\"min\",\"max\"]\nUNIQUE_COL_VALS = pickle.load(open(\"..\/input\/dsbowl2019-feng-part1\/UNIQUE_COL_VALS.pkl\", \"rb\"))\nlist(UNIQUE_COL_VALS.__dict__.keys())","a593249a":"# add accuracy_group unique vals\nUNIQUE_COL_VALS.__dict__['accuracy_groups'] = np.unique(train_with_features_part1.accuracy_group)\nUNIQUE_COL_VALS.accuracy_groups\npickle.dump(UNIQUE_COL_VALS, open( \"UNIQUE_COL_VALS.pkl\", \"wb\" ))","a8c21a88":"def target_encoding_stats_dict(df, by, targetcol):\n    \"get target encoding stats dict, by:[stats]\"\n    _stats_df = df.groupby(by)[targetcol].agg(stats)   \n    _d = dict(zip(_stats_df.reset_index()[by].values, _stats_df.values))\n    return _d","b60b8bf3":"def _value_counts(o, freq=False): return dict(pd.value_counts(o, normalize=freq))\ndef countfreqhist_dict(df, by, targetcol, types, freq=False):\n    \"count or freq histogram dict for categorical targets\"\n    types = UNIQUE_COL_VALS.__dict__[types]\n    _hist_df = df.groupby(by)[targetcol].agg(partial(_value_counts, freq=freq))\n    _d = dict(zip(_hist_df.index, _hist_df.values))\n    for k in _d: _d[k] = array([_d[k][t] for t in types]) \n    return _d","2969934e":"countfreqhist_dict(train_with_features_part1, \"title\", \"accuracy_group\", \"accuracy_groups\")","782fa853":"f1 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_incorrect\")\nf2 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"num_correct\")\nf3 = partial(target_encoding_stats_dict, by=\"title\", targetcol=\"accuracy\")\nf4 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_incorrect\")\nf5 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"num_correct\")\nf6 = partial(target_encoding_stats_dict, by=\"world\", targetcol=\"accuracy\")\n\nf7 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf8 = partial(countfreqhist_dict, by=\"title\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)\nf9 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=False)\nf10 = partial(countfreqhist_dict, by=\"world\", targetcol=\"accuracy_group\", types=\"accuracy_groups\",freq=True)","734bb3a0":"from sklearn.model_selection import KFold\n# create cross-validated indexes\nunique_ins_ids = np.unique(train_with_features_part1.installation_id)\ntrain_val_idxs = KFold(5, random_state=42).split(unique_ins_ids)","d94c0770":"feature_dfs = [] # collect computed _val_feats_dfs here\nfor train_idxs, val_idxs  in train_val_idxs:\n    # get train and val dfs\n    train_ins_ids, val_ins_ids = unique_ins_ids[train_idxs], unique_ins_ids[val_idxs]\n    _train_df = train_with_features_part1[train_with_features_part1.installation_id.isin(train_ins_ids)]\n    _val_df = train_with_features_part1[train_with_features_part1.installation_id.isin(val_ins_ids)]\n    assert (_train_df.shape[0] + _val_df.shape[0]) == train_with_features_part1.shape[0]\n    # compute features for val df\n    _idxs = _val_df['title'].map(f1(_train_df)).index\n    feat1 = np.stack(_val_df['title'].map(f1(_train_df)).values)\n    feat2 = np.stack(_val_df['title'].map(f2(_train_df)).values)\n    feat3 = np.stack(_val_df['title'].map(f3(_train_df)).values)\n    feat4 = np.stack(_val_df['world'].map(f4(_train_df)).values)\n    feat5 = np.stack(_val_df['world'].map(f5(_train_df)).values)\n    feat6 = np.stack(_val_df['world'].map(f6(_train_df)).values)\n    feat7 = np.stack(_val_df['title'].map(f7(_train_df)).values)\n    feat8 = np.stack(_val_df['title'].map(f8(_train_df)).values)\n    feat9 = np.stack(_val_df['world'].map(f9(_train_df)).values)\n    feat10 = np.stack(_val_df['world'].map(f10(_train_df)).values)\n    # create dataframe with same index for later merge\n    _val_feats = np.hstack([feat1, feat2, feat3, feat4, feat5, feat6, feat7, feat8, feat9, feat10])\n    _val_feats_df = pd.DataFrame(_val_feats, index=_idxs)\n    _val_feats_df.columns = [f\"targenc_feat{i}\"for i in range(_val_feats_df.shape[1])]\n    feature_dfs.append(_val_feats_df)","bf162d43":"train_feature_df = pd.concat(feature_dfs, 0)","776c09a6":"train_with_features_part2 = pd.concat([train_with_features_part1, train_feature_df],1)","8efa2c8c":"train_with_features_part2.to_feather(\"train_with_features_part2.fth\")","8ecabcc7":"### Feature Engineering with part 2\n\nIn this notebook we implement cross validated target encoding features.\n\n**To go back to previous modeling notebook:** [Part 1 Modeling Notebook](https:\/\/www.kaggle.com\/keremt\/fastai-model-part1-regression\/)\n\n**To skip and go to next modeling notebook:** [Part 2 Modeling Notebook](https:\/\/www.kaggle.com\/keremt\/fastai-model-part2-regression\/)","31c329f4":"### Read data","7dce1035":"### Imports\n\nWe will use fastai v1","0dcd44d1":"### end","5cea9b3a":"Train (also train labels) and test doesn't have any common installation ids. This means that we can't use past assessment target information for a given user, e.g. we can't use how a child did in his\/her previous to predict for future assessment results. \n\nInstead we need to create global features from target information, e.g. using categorical target encoding and similar techniques.","79172376":"### Compute Features for Train","b432cc54":"Load part 1 data too","6ae40624":"### Cross Validated Target Encoding\n\nWe will use cv based target encoding for using label information. For more information see really nice documentation here in [h2o.ai](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-munging\/target-encoding.html#holdout-type). For these calculations we will use `train_with_features_part1` and calculate stats on different target information by different categorical data. Note that we will not use `event_ids` since it has a 1:1 mapping `titles` adding no extra information, `media_types` since always `Assesment` and `event_codes` since it's always `2000`. \n\n**Disclaimer:** Target encoding on the following categories can be not that effective since their cardinality is low.\n\nValue: num_correct x num_incorrect x accuracy x hist of accuracy group\n\nBy: Title x World \n"}}