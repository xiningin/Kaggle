{"cell_type":{"97cdb720":"code","0b75314b":"code","f3b62285":"code","f682dffb":"code","081e7668":"code","ce347144":"code","4653adbd":"code","5a8b4f9a":"code","eff850f0":"code","36ecc4a7":"code","b175c650":"code","2b30100d":"code","d7a45277":"code","24e2dd51":"code","fb96dc57":"code","6b22bb25":"code","01c19e1e":"code","5d0918ac":"code","65ca3d0b":"code","0238103f":"code","c0a455d7":"code","ac6b996e":"code","f137144f":"code","c59249cd":"code","b2b445a7":"code","579f1fb2":"code","96845795":"code","051e4997":"code","0a3d52b4":"code","b77c343c":"code","7e12cdaa":"code","0fbad337":"code","2f1098ad":"code","68f86bfa":"code","a2231d72":"code","cec0015f":"code","413dbbfd":"code","de194a67":"code","8602a87c":"code","5abdf6db":"code","9781e6da":"code","e84968eb":"code","ec148ab5":"code","2d28b722":"code","56b358ab":"code","b514ee08":"code","3123c0c1":"markdown","261e7d68":"markdown","3529e2df":"markdown","eda357b2":"markdown","eb281f8c":"markdown","d19a6ea0":"markdown","5e6bb123":"markdown","e7f31267":"markdown","ef9984e8":"markdown","ec53abfc":"markdown","7d323512":"markdown","e66e345e":"markdown","7f94a9d6":"markdown","4e359564":"markdown","6aea16bc":"markdown","1757e28c":"markdown","bda1bb97":"markdown","46d9a51a":"markdown","978a7e65":"markdown","e5fa47d3":"markdown","94a4610f":"markdown","3139e8d2":"markdown","8b27fe71":"markdown","3f4ea3a9":"markdown","75354d21":"markdown","5e0efaea":"markdown","728390e2":"markdown","68fb4ee7":"markdown","88b9c798":"markdown","0f2795cf":"markdown","0a80d4d2":"markdown","569b6d83":"markdown","e5af7571":"markdown","b9b9cb51":"markdown","7a30906d":"markdown","d92aee39":"markdown","6265808d":"markdown","4f7a5ffe":"markdown","b7284c3a":"markdown","33db3672":"markdown","5cff8724":"markdown","f7d7a001":"markdown","f58c5250":"markdown","26d5c591":"markdown","007dc7e2":"markdown","487ad788":"markdown","9b02baa5":"markdown","2422facd":"markdown","d478649b":"markdown","a8adaaf2":"markdown"},"source":{"97cdb720":"import os\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\nfrom termcolor import colored\n\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\n\nimport re\nimport nltk\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical \nfrom nltk.corpus import stopwords\nfrom nltk.stem import *\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.metrics import confusion_matrix","0b75314b":"train_data=pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding=\"latin1\")\ntest_data=pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding=\"latin1\")","f3b62285":"train_data.shape","f682dffb":"train_data.drop_duplicates()\ntrain_data.shape","081e7668":"train_data.isnull().sum()","ce347144":"print(train_data.Sentiment.unique())\nprint(train_data.Sentiment.value_counts())","4653adbd":"groups = np.array(train_data.Sentiment.value_counts())\ngroups_cat = ['Positive','Negative','Neutral','Extremely Positive','Extremely Negative']\nplt.pie(groups, labels = groups_cat,startangle=90, autopct='%1.1f%%')\nplt.axis('equal')\nplt.show()","5a8b4f9a":"location = train_data['Location'].value_counts().nlargest(n=15)\n\nfig = px.bar(y=location.values,\n       x=location.index,\n       orientation='v',\n       color=location.index,\n       text=location.values,\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"Location\",\n                  yaxis_title=\"Count\",\n                  title=\"Top 15 Locations with tweet count\")\nfig.show()","eff850f0":"def india_tweet(data):\n    count_pos =0\n    count_neg =0\n    count_neu = 0\n    for i in range(len(data)):\n        if ((data['Sentiment'][i] == 'Positive' or data[\"Sentiment\"][i] == \"Extremely Positive\") and data['Location'][i] == 'India'):\n            count_pos = count_pos + 1\n        elif ((data['Sentiment'][i] == 'Negative' or data[\"Sentiment\"][i] == \"Extremely Negative\") and data['Location'][i] == 'India'):\n            count_neg = count_neg + 1\n        elif  ((data['Sentiment'][i] == 'Neutral') and data['Location'][i] == 'India'):\n            count_neu = count_neu + 1\n        else:\n            0\n    return [count_pos,count_neg,count_neu]\n            \n    ","36ecc4a7":"df=india_tweet(train_data)\ngroups_cat_1 = ['Positive','Negative','Neutral']\nplt.pie(df, labels = groups_cat_1,startangle=90, autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('India Sentiment in Twitter')\nplt.show()","b175c650":"def mentions_finder(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ntrain_data['mentions'] = train_data['OriginalTweet'].apply(lambda x:mentions_finder(x))\nallMentions = list(train_data[(train_data['mentions'] != None) & (train_data['mentions'] != \"\")]['mentions'])\nallMentions = [tag.lower() for tag in allMentions]\nmentions_df = dict(Counter(allMentions))\ntop_mentions_df = pd.DataFrame(list(mentions_df.items()),columns = ['word','count']).reset_index(drop=True).sort_values('count',ascending=False)[:20]\ntop_mentions_df.head()\n\nfig = px.bar(x=top_mentions_df['word'],y=top_mentions_df['count'],\n       orientation='v',\n       color=top_mentions_df['word'],\n       text=top_mentions_df['count'],\n       color_discrete_sequence= px.colors.qualitative.Bold)\n\nfig.update_traces(texttemplate='%{text:.2s}', \n                  textposition='outside', \n                  marker_line_color='rgb(8,48,107)', \n                  marker_line_width=1.5, \n                  opacity=0.7)\n\nfig.update_layout(width=1000, \n                  showlegend=False, \n                  xaxis_title=\"People\/Organizations\",\n                  yaxis_title=\"Count\",\n                  title=\"Top @mentions in Covid19 Tweets\")\nfig.show()","2b30100d":"neg = train_data[train_data['Sentiment']=='Negative']['OriginalTweet'].str.split().map(lambda x: len(x))\npos = train_data[train_data['Sentiment']=='Positive']['OriginalTweet'].str.split().map(lambda x: len(x))\nneu = train_data[train_data['Sentiment']=='Neutral']['OriginalTweet'].str.split().map(lambda x: len(x))\nex_pos = train_data[train_data['Sentiment']=='Extremely Positive']['OriginalTweet'].str.split().map(lambda x: len(x))\nex_neg = train_data[train_data['Sentiment']=='Extremely Negative']['OriginalTweet'].str.split().map(lambda x: len(x))\n\n\nfig = make_subplots(rows=2, cols=3)\n\nfig.add_trace(\n    go.Histogram(x=list(neg), name='Negative Tweets'),\n    row=1, \n    col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=list(pos), name='Positive Tweets'),\n    row=1, \n    col=2,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(neu), name='Neutral Tweets'),\n    row=1, \n    col=3,\n)\n\nfig.add_trace(\n    go.Histogram(x=list(ex_neg), name='Extremely Negative Tweets'),\n    row=2, \n    col=1\n)\nfig.add_trace(\n    go.Histogram(x=list(ex_pos), name='Extremely Positive Tweets'),\n    row=2, \n    col=2\n)\n\nfig.update_layout(title_text=\"Word Count\")\nfig.show()\n","d7a45277":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\n\ndf_pos = train_data[train_data['Sentiment']==\"Positive\"]\ndf_neg = train_data[train_data['Sentiment']==\"Negative\"]\ndf_neu = train_data[train_data['Sentiment']==\"Neutral\"]\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df_pos.OriginalTweet: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n   \n\nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greens\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive Sentiment',fontsize=35);\n\ncomment_words = ''\n\nfor val in df_neg.OriginalTweet: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\n\n\n\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Reds\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)  \nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\n\n\ncomment_words = ''\nfor val in df_neu.OriginalTweet: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud3 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greys\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutal Sentiment',fontsize=35);\n","24e2dd51":"train_data.drop(['UserName', 'ScreenName', 'Location', 'TweetAt','mentions'], axis=1, inplace=True)\ntest_data.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1, inplace=True)","fb96dc57":"train_data.head()","6b22bb25":"train_data.dtypes","01c19e1e":"train_data['Sentiment'] = train_data['Sentiment'].astype('category')\ntest_data['Sentiment'] = test_data['Sentiment'].astype('category')","5d0918ac":"train_data['Sentiment']= train_data['Sentiment'].cat.codes\ntest_data['Sentiment'] = test_data['Sentiment'].cat.codes","65ca3d0b":"train_data.head()","0238103f":"def original_tweet(data):\n    processed = data.str.replace(r\"#(\\w+)\", \"\") # replace #tag\n    processed = processed.str.replace(r'https?:\/\/\\S+', \" \") # replace url\n    processed = processed.str.replace(r\"\\r\\r\\n\", \"\") # replace \\r\\n\\r\n    processed = processed.str.replace(\".\", \"\") # Replace dot\n    processed = processed.str.replace(r'^\\s+|\\s+?$', '') # Remove lead and trail space\n    processed = processed.str.replace(r'\\s+', ' ') # Remove whitespace\n    processed = processed.str.replace(\"'\", \"o\")\n    processed = processed.str.replace(r'[^\\w\\d\\s]', '') # remove punctuation\n    processed = processed.str.replace(r'[0-9]', '') #remove number\n    return processed.str.lower()","c0a455d7":"clean_train = original_tweet(train_data[\"OriginalTweet\"])\nclean_test = original_tweet(test_data[\"OriginalTweet\"])","ac6b996e":"print(colored(\"Before Cleaning\\n\",\"grey\"))\nprint(colored(train_data['OriginalTweet'][3],\"red\"))\nprint(\"-----------------------------------------------------------------\")\nprint(colored(\"After Cleaning\\n\",\"grey\"))\nprint(colored(clean_train[3],\"green\"))","f137144f":"stopwords = set(STOPWORDS)  # stopwords\n\nclean_train = clean_train.apply(lambda x:\" \".join(term for term in x.split() if term not in stopwords)) # clean train\nclean_test = clean_test.apply(lambda x:\" \".join(term for term in x.split() if term not in stopwords)) # clean test","c59249cd":"print(colored(\"Before Cleaning\\n\",\"grey\"))\nprint(colored(train_data['OriginalTweet'][3],\"red\"))\nprint(\"-----------------------------------------------------------------\\n\")\nprint(colored(\"After Cleaning\\n\",\"grey\"))\nprint(colored(clean_train[3],\"green\"))","b2b445a7":"fig, (ax1) = plt.subplots(1, 1, figsize=[30, 15])\n\ndf_pos = clean_train\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in df_pos: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n   \n\nwordcloud1 = WordCloud(width = 500, height = 500, \n                background_color ='white',\n                colormap=\"Greens\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Most Used Words',fontsize=35);\n\n","579f1fb2":"wn = WordNetLemmatizer() # Using Lemmatization\n\nclean_train = clean_train.apply(lambda x:\" \".join([wn.lemmatize(word) for word in x.split()]))\nclean_test = clean_test.apply(lambda x:\" \".join([wn.lemmatize(word) for word in x.split()]))","96845795":"print(colored(\"Before Cleaning\",\"yellow\"))\nprint(colored(train_data['OriginalTweet'][3],\"red\"))\nprint(\"-----------------------------------------------------------------\")\nprint(colored(\"After Cleaning\",\"yellow\"))\nprint(colored(clean_train[3],\"green\"))","051e4997":"max_seq_length = np.max(clean_train.apply(lambda tweet: len(tweet))) # max sequence length\nmax_seq_length\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(clean_train)\nvocab_length = len(tokenizer.word_index) + 1\nvocab_size = vocab_length\nembedding_dim = 16\nmax_length = max_seq_length\npad_type = \"pre\"","0a3d52b4":"train_inputs = tokenizer.texts_to_sequences(clean_train)\ntrain_inputs = pad_sequences(train_inputs, maxlen=max_seq_length, padding=pad_type)\n\ntest_inputs = tokenizer.texts_to_sequences(clean_test)\ntest_inputs = pad_sequences(test_inputs, maxlen=max_seq_length, padding=pad_type)","b77c343c":"print(\"Example after tokenization\")\ntrain_inputs[3]","7e12cdaa":"y_train = train_data['Sentiment']\ny_test = test_data['Sentiment']","0fbad337":"y_train = to_categorical(y_train, 5, dtype =\"uint8\")\ny_test = to_categorical(y_test, 5, dtype =\"uint8\")","2f1098ad":"print(y_train.shape)","68f86bfa":"X = train_inputs\ny = y_train\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)","a2231d72":"model=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(10000,128,input_length=X_train.shape[1]),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(16,activation='relu'),\n    tf.keras.layers.Dense(8,activation='relu'),\n    tf.keras.layers.Dense(5,activation='softmax')\n])\nmodel.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])","cec0015f":"model.summary() # summary of the model","413dbbfd":"num_epochs = 10\nhistory = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))","de194a67":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"training accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"training loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()","8602a87c":"model.save('.\/baseline1')","5abdf6db":"final=list(np.argmax(model.predict(X_test),axis=1))\nrounded_labels=np.argmax(y_test, axis=1)\ncm=confusion_matrix(rounded_labels, final)\nsns.heatmap(cm\/np.sum(cm), annot=True, \n            fmt='.2%', cmap='Blues')","9781e6da":"model_2=tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(1000,128,input_length=X_train.shape[1]),\n    tf.keras.layers.LSTM(512),\n    tf.keras.layers.Dense(256,activation='relu'),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dense(32,activation='relu'),\n    tf.keras.layers.Dense(5,activation='softmax')\n])\nmodel_2.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=['accuracy'])\n    ","e84968eb":"model_2.summary() # summary of the model","ec148ab5":"num_epochs = 10\nhistory_2 = model_2.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test))","2d28b722":"accuracy = history_2.history['accuracy']\nval_accuracy = history_2.history['val_accuracy']\nloss = history_2.history['loss']\nval_loss = history_2.history['val_loss']\nepochs = range(len(accuracy))\n\nplt.plot(epochs, accuracy, \"b\", label=\"training accuracy\")\nplt.plot(epochs, val_accuracy, \"r\", label=\"validation accuracy\")\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, \"b\", label=\"training loss\")\nplt.plot(epochs, val_loss, \"r\", label=\"validation loss\")\nplt.legend()\nplt.show()","56b358ab":"model_2.save('.\/baseline1')","b514ee08":"final=list(np.argmax(model_2.predict(X_train),axis=1))\nrounded_labels_1=np.argmax(y_train, axis=1)\ncm=confusion_matrix(rounded_labels_1, final)\nsns.heatmap(cm\/np.sum(cm), annot=True, \n            fmt='.2%', cmap='Blues')","3123c0c1":"### **Model Building**","261e7d68":"### **Workflow**","3529e2df":"### Prediction of Model","eda357b2":"### ****Splitting the data ****","eb281f8c":"**Top Mentions in twitter**","d19a6ea0":"### **Accuracy**","5e6bb123":"### Import library","e7f31267":"### **Top 15 Countires tweeted**","ef9984e8":"### Accuracy of about 75% achieved for LSTM model","ec53abfc":"### **Thus for new dataset [Testing Dataset] => 67% accuracy has obtain**","7d323512":"1. The dataset contains 41157 rows and 6 columns\n2. Location is the only column with missing values. Basically, it is a sentiment analysis and we can remove all other independent variables(Username,Screen Name,Location,TweetAt)\n3. All the tweets are unique. i.e. Every User has Tweeted a Tweet.\n4. There exist no Duplication of samples \n5. Basically there are 3 sentiment and divided into 5. [Extremely Positive, Positive, Neutral, Negative, Extremely Negative]\n*  Positive - 27.8 %\n*  Negative - 24.1 %\n*  Neutral - 18.7 %\n*  Extremely Positive - 16.1 %\n*  Extremely Negative - 13.3 %\n6. Top 5 tweeted states about corona are\n* London\n* NYC\n* Washington\n* Los Angeles\n* India\n7. Top 5 mention person about corona are\n* Donald Trump\n* Youtube\n* Tesco\n* Amazon\n8. Sentiment in India are\n* Positive - 43.4 %\n* Neutral - 19.4 %\n* Negative - 37.3 %","e66e345e":"### **Categorical Encoding**","7f94a9d6":"-----------------------------------------------------------------------------------------------------","4e359564":"#### Removing duplicate tweets if any present","6aea16bc":"### **Tokenization** - separating a piece of text into smaller units","1757e28c":"### Distribution chart","bda1bb97":"### Reading the data","46d9a51a":"**As of total 43.8 % has positive sentiment in the tweet, 37.4 % has negative impact and 18.7% has neutral sentiment in the tweet.**","978a7e65":"### Baseline Model","e5fa47d3":"1. Visualizations were done to know about the data clearly.\n* The dataset is balanced one. (ie: Equal amount of positive and Negative dataset)\n* Tweet was largely observed in USA and UK\n2. Pre-processing is done to convert the unstructural dataset to a structred dataset\n3. Deep Neural Network Model and LSTM model is built to predict the new tweets to know about their sentiments.\n4. The model accuracy is 75%\n5. LSTM has a over fitting problem and Deep Neural Network works fine with an accuracy of about 70 %. At this moment, Deep Neural Network works fine and predicts well compared to LSTM. ","94a4610f":"--------------------------------------------------------------------------------------------------------","3139e8d2":"#### **Storage of Model**","8b27fe71":"### **Cleaning Data**","3f4ea3a9":"### **To Preprocess the tweet**","75354d21":"-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","5e0efaea":"## Conclusion","728390e2":"------------------------------------------------------------------------------------------------------","68fb4ee7":"**1. The baseline model is build using a Deep Neural Network with**\n* Training Accuracy - 80 % \n* Validation Accuracy - 70% ","88b9c798":"![WORKFLOW.drawio.png](attachment:914c90d2-7833-40d2-b4e2-0c7d3df3f644.png)","0f2795cf":"### **Introduction**","0a80d4d2":"### **Lemmitization** - To ensuring that the root word belongs to the language","569b6d83":"The main objective of this problem is to classify the sentiment of the tweet related to the COVID-19. At first, Visulizations are done to infer about the data. Tweets [ Unstructure data ] are converted into numbers [ structured data] by removing stop words, punctuations and  by tokenization and padding. Baseline model is build using Deep Neural Network.  LSTM network was also build for advancement. Accuracy is calculated using training and testing accuracy, confusion matrix. From this model, we can predict new tweet with following steps.","e5af7571":"### Building a **LSTM model**","b9b9cb51":"#### **The most common used words are online shopping, grocery, toilet papers, panic buying. This infer the demand of toilet papers in USA at the moment of lockdown period**","7a30906d":"### **Conclusion from EDA and start of the process**","d92aee39":"**India's Sentiment**","6265808d":"# Text Classification on Tweets about COVID-19","4f7a5ffe":"#### Change the String type categorical variables into number wise","b7284c3a":"## **Agenda**\n**1. Introduction**\n\n**2. Workflow**\n\n**3. Visualization**\n\n**4. Pre Processing**\n\n**5. Model Building**\n\n**6. Prediction and Accuracy Testing**\n\n**7. Conclusion**","33db3672":"### Accuracy","5cff8724":"-----------------------------------------------------------------------------------------------------------------------","f7d7a001":"==========================================================================================================================================================================================================","f58c5250":"-----------------------------------------------------------------------------------------------","26d5c591":"#### From the above two cells we says that the data doesnot contain any duplicate values","007dc7e2":"## **EDA and Visualization**","487ad788":"### Remove Stop Words","9b02baa5":"So, There is null value present only in Location","2422facd":"These are some commonly used words before cleaing data.","d478649b":"----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------","a8adaaf2":"Word Count for each sentiment"}}