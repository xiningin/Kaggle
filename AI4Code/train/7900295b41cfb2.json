{"cell_type":{"13ca7aa4":"code","eba92e85":"code","6f61a1ca":"code","f9328be7":"code","518b1110":"code","07ca037b":"code","ced61876":"code","956ae176":"code","28897b23":"code","6bf5e716":"code","8abe1306":"code","5d53b91c":"code","7061f277":"code","0b1fc560":"code","c079e9fe":"code","5bb8fc46":"code","9f58b8fa":"code","06e5a66a":"code","13037bf6":"code","63f80d3d":"code","0b1878c3":"code","1ead7dc4":"code","eb16f75d":"code","99645531":"code","41eab0a4":"code","26bd9b95":"code","780f678c":"code","14aecf95":"code","6725b77a":"code","fe789b32":"code","d45a263e":"code","e07401ad":"code","33d1e8f1":"code","b93a9205":"code","a7e34696":"code","213de819":"code","84f8d2f5":"code","fdc9bed7":"code","2ff84725":"code","404d52e7":"code","a97fe0f3":"code","8123ce05":"code","669b4820":"code","17edfa00":"code","24ceed72":"code","b9f3df97":"code","9ad5a8e0":"code","36f9e0c5":"code","1a176025":"code","33ec8dee":"code","9d4cdd79":"code","376b9e32":"code","8aba07a5":"code","74622ec3":"code","88151bf1":"code","e884d1fd":"code","b29f81c3":"code","bb44fad8":"code","1f519511":"code","1eafcfdb":"code","c57d22e0":"code","f7c33a51":"code","d0527e29":"code","6af6d59a":"code","5b3635f2":"code","bd01ae23":"code","ab216b47":"code","a40f6c9c":"code","36f66237":"code","b57faa0a":"code","cb3a34b6":"code","3a0c50b2":"code","162f925f":"code","4b03cc60":"code","b122ef8d":"code","da522c92":"code","315dde13":"code","ab4e810f":"code","24369869":"code","efb4d9fc":"code","ce69cdb4":"code","c85a2cc3":"code","21aa79e6":"code","dc29f900":"code","5016cae2":"code","a777da9e":"code","de326eb6":"code","4a9b57ae":"code","9c8ee693":"code","e9e89323":"code","d32e97bc":"code","5b519db8":"code","780d2e4b":"code","760a6206":"code","333d870c":"code","8ad2c18b":"code","da9919ef":"code","5c740775":"code","e63aaaf1":"code","30c2a815":"code","1ef799ed":"code","3d45646c":"code","02bee0e4":"code","ccfd635f":"code","893119e2":"code","1e76f342":"code","eec4b23e":"code","168bd2df":"code","4f7a7f71":"code","2f9fdb05":"code","f13ffc56":"code","b57f98cc":"code","cd05d505":"code","9419606f":"code","b0eb4c31":"code","fb6dd41c":"code","0d8ee7e0":"code","905dff0d":"code","8cbd7f7a":"code","64397843":"code","e9702ab2":"code","bddd63a9":"code","1bd7ba1e":"code","f4ed6bf9":"code","ccd0a678":"code","67351d85":"code","cde31648":"code","88260ea3":"code","0a71a748":"code","7c54f88a":"code","31d6020f":"code","922b800e":"code","aa62ee77":"code","7ac9834a":"code","f7de85b6":"code","96efe680":"code","7e25f455":"code","dce8f1fd":"code","cd83987c":"code","938f619d":"code","db215b26":"code","50ab1d28":"code","844282c2":"code","d0e361df":"code","6590fc26":"code","200960d2":"code","2c135e97":"code","66293c1e":"code","aeee61fb":"code","3693eb85":"code","b17faf87":"code","d9a5b0af":"code","8fdfcb7d":"code","458556c7":"code","06e0b807":"code","907f07bc":"code","9cf97c8e":"code","2b3cff43":"code","f983a894":"code","e8af1f05":"code","f236c903":"code","1bbe0b35":"code","0ceaa23d":"code","b53d430c":"code","5bb30a90":"code","32ff117d":"code","c4f53fc9":"code","825b0cd3":"code","b628fc84":"code","0d77b0b6":"code","28e89821":"code","b442f24d":"code","ed18c177":"code","75a6b7a3":"code","6ea2aad6":"code","a97156e4":"code","b97d7e7f":"code","99ae4f19":"code","bef407a2":"code","c8552de3":"code","78fa08db":"code","90cd827c":"code","67e3d6d5":"code","0897a7ed":"markdown","a528c1f5":"markdown","641ae175":"markdown","5ead8ead":"markdown","1e2de1a2":"markdown","ace29455":"markdown","cd76ed14":"markdown","557d7722":"markdown","c1b1832f":"markdown","cf2afb16":"markdown","709b1f9c":"markdown","9a25e21a":"markdown","62b49ad5":"markdown","b1236552":"markdown","34ebea6b":"markdown","85eb3e81":"markdown","f56c40e1":"markdown","9ec77230":"markdown","0b243f17":"markdown","39e8d3d9":"markdown","3c696121":"markdown","6eb934e7":"markdown","62723a94":"markdown","93ff8dc8":"markdown","14e9efa0":"markdown","2d1a95b8":"markdown","3ca4915f":"markdown","8ae0d151":"markdown","46a1a686":"markdown","8cf17a0d":"markdown","cacd664c":"markdown","0cbd2cf4":"markdown","4e06722c":"markdown","c88c5799":"markdown","70812d81":"markdown","11ea70b3":"markdown","ffb9a5c4":"markdown","a699b2b0":"markdown","e3fac1f5":"markdown","25a4912f":"markdown","e8b88fd4":"markdown","2c0a2d72":"markdown","18d7120b":"markdown","edb73c18":"markdown","988c4671":"markdown","aa75ffa2":"markdown","7d831a4e":"markdown","acfdb1f4":"markdown","6ff876ff":"markdown","6499b207":"markdown","725176be":"markdown","44d4dfa6":"markdown","97d5e23c":"markdown","0b4a8489":"markdown","6189b296":"markdown"},"source":{"13ca7aa4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport re\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\npd.options.display.max_columns = None\npd.options.display.max_rows = 80\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eba92e85":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","6f61a1ca":"def inv_y(trans_y):\n    return np.exp(trans_y)\ndef check_null(df, n_head=30):\n    return df.isna().sum().sort_values(ascending=False).head(n_head)\ndef score(model, train_x, train_y, val_x, val_y):\n    score = {}\n    model.fit(train_x, train_y)\n    preds = model.predict(val_x)\n    score['mae'] = mean_absolute_error(inv_y(preds), inv_y(val_y))\n    score['mse'] = mean_squared_error(inv_y(preds), inv_y(val_y))\n    return score","f9328be7":"train","518b1110":"test.head()","07ca037b":"train.info()","ced61876":"train.describe()","956ae176":"train['GrLivArea'].describe()","28897b23":"train.describe(include='O')","6bf5e716":"pd.DataFrame(train.isnull().sum().sort_values(ascending=False)).reset_index()[:20]","8abe1306":"plt.figure(figsize=(10, 5))\nplt.xticks(np.arange(min(train.YearBuilt.tolist()), max(train.YearBuilt.tolist())+1, 7.0))\nplt.bar(train.YearBuilt, train.SalePrice)\n# 1. older buiding lower house price -> YearBuilt vs. SalePrice ?","5d53b91c":"train[['YearBuilt', 'SalePrice']].groupby(['YearBuilt']).mean().sort_values(by='SalePrice', ascending=False)","7061f277":"plt.bar(train.SaleType, train.SalePrice)\ntrain[['SaleType', 'SalePrice']].groupby(['SaleType'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n# 2. SaleType vs. SalePrice relation with interest or new may relatively higher price?","0b1fc560":"plt.bar(train.SaleCondition, train.SalePrice)\ntrain[['SaleCondition', 'SalePrice']].groupby(['SaleCondition'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)","c079e9fe":"train['ExtraFeature'] = train.MiscFeature.notnull().astype('int')\nplt.bar(train.ExtraFeature, train.SalePrice)\ntrain[['ExtraFeature', 'SalePrice']].groupby(['ExtraFeature'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n# 3. have value in MiscFeature may increase house price as it contain several services?\n# no, nothing extra is twice of having extra in price\n# can drop MiscFeature columns, but need to consider as 1406\/1460,almost 96% do not have values","5bb8fc46":"train[train.PoolArea!=0]\n# 4. pool also another indicator of higher price\n# 1453\/1460 do not have poolarea, 7 has, also not high price, not good indicator\n# can drop PoolArea, PoolQC columns","9f58b8fa":"# 5. OverallCond\/OverallQual feature store as numerical feature, should be converted to onehot encoder \nplt.bar(train.OverallCond, train.SalePrice)\ntrain[['OverallCond', 'SalePrice']].groupby(['OverallCond'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)","06e5a66a":"plt.bar(train.OverallQual, train.SalePrice)\ntrain[['OverallQual', 'SalePrice']].groupby(['OverallQual'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n# overallQuality as good indicator, overallCond may be dropped","13037bf6":"plt.figure(figsize=(23, 5))\nplt.bar(train.Neighborhood, train.SalePrice)\ntrain[['Neighborhood', 'SalePrice']].groupby(['Neighborhood'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n# 6. Neighbohood could be good indicator of house price\n# can use label encoder as there is ordinal ","63f80d3d":"plt.bar(train.LandSlope, train.SalePrice)\ntrain[['LandSlope', 'SalePrice']].groupby(['LandSlope'], as_index=False).mean().sort_values(by='SalePrice', ascending=False)\n# 7. LandSlope involve slope may lead expense of others increase, overall houseprice increase?\n# Gtl has highest value&Sev has lowest value,  but overall Sev has higher value","0b1878c3":"corr = train.corr()\nplt.subplots(figsize=(14, 12))\nsns.heatmap(corr)","1ead7dc4":"corr['SalePrice'].sort_values(ascending=False).head(15)","eb16f75d":"corr['SalePrice']['Fireplaces']","99645531":"corr[corr.mask(np.eye(len(corr), dtype=bool)).abs()>0.5].SalePrice.sort_values(ascending=False)","41eab0a4":"# exploring \neda_potential_features = ['YearBuilt', 'SaleType', 'OverallQual', 'Neighborhood', 'LandSlope']\neda_drop_features = ['SaleCondition', 'MiscFeature', 'ExtraFeature', 'PoolArea', 'OverallCond']","26bd9b95":"# potential features according correlation \ncorr_potential_features = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF',\n                      '1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']","780f678c":"a = 'Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType,ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 Heating HeatingQC CentralAir Electrical,Functional,GarageQual GarageCond PavedDrive, PoolQC Fence MiscFeature, YrSold SaleType SaleCondition'\nremain_feature_list = [i for i in re.split(' |,', a) if len(i)]\nremain_feature_list","14aecf95":"def evaluation_model(model, train_x, train_y, valid_x, valid_y):\n    score = {}\n    model.fit(train_x, train_y)\n    valid_prediction = model.predict(valid_x)\n    score['mae'] = mean_absolute_error(valid_y, valid_prediction)\n    score['mse'] = mean_squared_error(valid_y, valid_prediction)\n    score['r2'] = r2_score(valid_y, valid_prediction)\n    return score","6725b77a":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ny = train.SalePrice\nX = train.loc[:, train.columns!='SalePrice']","fe789b32":"x_train, x_valid, y_train, y_valid = train_test_split(X, y, random_state=1)","d45a263e":"x_train = x_train.select_dtypes(exclude='O')\nx_valid = x_valid.select_dtypes(exclude='O')","e07401ad":"drop_col = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\nx_train = x_train.drop(drop_col, axis=1)\nx_valid = x_valid.drop(drop_col, axis=1)","33d1e8f1":"# baseline with DecisionTreeRegressor\nmodel = DecisionTreeRegressor(random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","b93a9205":"# baseline with RandomForestRegressor\nmodel = RandomForestRegressor(random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","a7e34696":"# n_estimators\nn_est = [i for i in range(290, 330, 10)]\nmae_scores= []\nmse_scores = []\nr2_scores = []\nfor i in n_est:\n    model = RandomForestRegressor(n_estimators=i, random_state=1)\n    mae_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mse'])\n    r2_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['r2'])\n    \n# plt.plot(n_est, mae_scores, label='mae score')\n# plt.plot(n_est, mse_scores, label='mse score')\nplt.plot(n_est, r2_scores, label='r2 score')","213de819":"model = RandomForestRegressor(n_estimators=310, random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","84f8d2f5":"# depth\ndepth_ = [i for i in range(8, 17)]\nmae_scores= []\nmse_scores = []\nr2_scores = []\nfor i in depth_:\n    model = RandomForestRegressor(n_estimators=310, max_depth=i, random_state=1)\n    mae_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mse'])\n    r2_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['r2'])\n    \n# plt.plot(depth_, mae_scores, label='mae score')\n# plt.plot(depth_, mse_scores, label='mse score')\nplt.plot(depth_, r2_scores, label='r2 score')","fdc9bed7":"model = RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","2ff84725":"test.Id","404d52e7":"x_train","a97fe0f3":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ny = train.SalePrice\nX = train.loc[:, train.columns!='SalePrice']\nX = X.select_dtypes(exclude='O')\ndrop_col = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\nX = X.drop(drop_col, axis=1)\nmodel_on_full_data = RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1)\nmodel_on_full_data = model.fit(X, y)","8123ce05":"test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\nId = test.Id\ntest = test.select_dtypes(exclude='O')\ndrop_col = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\ntest = test.drop(drop_col, axis=1)\ntest.fillna(0, inplace=True)","669b4820":"test_preds = model_on_full_data.predict(test)\noutput = pd.DataFrame({'Id': Id, 'SalePrice': test_preds})\noutput.to_csv('houseprice_test1.csv', index=False)","17edfa00":"# another try with whole dataset k-fold\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\nx_train = train.select_dtypes(exclude='O')\ndrop_col = ['LotFrontage', 'MasVnrArea', 'GarageYrBlt']\nx_train = x_train.drop(drop_col, axis=1)","24ceed72":"model = RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1)\nscore = {}\nscore['mae'] = np.mean(cross_val_score(model, x_train, train.SalePrice, cv=5, scoring=\"neg_mean_absolute_error\"))\nscore['mse'] = np.mean(cross_val_score(model, x_train, train.SalePrice, cv=5, scoring=\"neg_mean_squared_error\")) \nscore['r2'] = np.mean(cross_val_score(model, x_train, train.SalePrice, cv=5))\nscore","b9f3df97":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","9ad5a8e0":"corr_potential_features = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF',\n                      '1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd']\ntrain[corr_potential_features].describe()\ntrain[corr_potential_features].info()","36f9e0c5":"train[corr_potential_features].isnull().any()","1a176025":"X = train[corr_potential_features]\ny = train.SalePrice\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, random_state=1)","33ec8dee":"# previous best model to see how's performance on new features\nmodel = RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","9d4cdd79":"model = RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1)\ntest_preds = model.predict()","376b9e32":"# baseline with DecisionTreeRegressor\nmodel = DecisionTreeRegressor(random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","8aba07a5":"# baseline with RandomForestRegressor\nmodel = RandomForestRegressor(random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","74622ec3":"# n_estimators\nn_est = [i for i in range(360, 500, 20)]\nmae_scores= []\nmse_scores = []\nr2_scores = []\nfor i in n_est:\n    model = RandomForestRegressor(n_estimators=i, random_state=1)\n    mae_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mse'])\n    r2_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['r2'])\n    \n# plt.plot(n_est, mae_scores, label='mae score')\n# plt.plot(n_est, mse_scores, label='mse score')\nplt.plot(n_est, r2_scores, label='r2 score')","88151bf1":"model = RandomForestRegressor(n_estimators=450, random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","e884d1fd":"# depth\ndepth_ = [i for i in range(8, 17)]\nmae_scores= []\nmse_scores = []\nr2_scores = []\nfor i in depth_:\n    model = RandomForestRegressor(n_estimators=450, max_depth=i, random_state=1)\n    mae_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['mse'])\n    r2_scores.append(evaluation_model(model, x_train, y_train, x_valid, y_valid)['r2'])\n    \n# plt.plot(depth_, mae_scores, label='mae score')\n# plt.plot(depth_, mse_scores, label='mse score')\nplt.plot(depth_, r2_scores, label='r2 score')","b29f81c3":"model = RandomForestRegressor(n_estimators=450, max_depth=15, random_state=1)\nevaluation_model(model, x_train, y_train, x_valid, y_valid)","bb44fad8":"model = RandomForestRegressor(n_estimators=450, max_depth=15, random_state=1)\nscore = {}\nscore['mae'] = np.mean(cross_val_score(model, train[corr_potential_features], train.SalePrice, cv=5, scoring=\"neg_mean_absolute_error\"))\nscore['mse'] = np.mean(cross_val_score(model, train[corr_potential_features], train.SalePrice, cv=5, scoring=\"neg_mean_squared_error\")) \nscore['r2'] = np.mean(cross_val_score(model, train[corr_potential_features], train.SalePrice, cv=5))\nscore","1f519511":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ny = train.SalePrice\nX = train[corr_potential_features]\nmodel_on_full_data = RandomForestRegressor(n_estimators=450, max_depth=15, random_state=1)\nmodel_on_full_data = model_on_full_data.fit(X, y)","1eafcfdb":"test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\nId = test.Id\ntest = test[corr_potential_features]\ntest.fillna(0, inplace=True)","c57d22e0":"test_preds = model_on_full_data.predict(test)\noutput = pd.DataFrame({'Id': Id, 'SalePrice': test_preds})\noutput.to_csv('houseprice_test2.csv', index=False)","f7c33a51":"home_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col=0)","d0527e29":"home_data","6af6d59a":"home_data.shape","5b3635f2":"home_data.select_dtypes(exclude=['O']).columns","bd01ae23":"len(home_data.select_dtypes(exclude=['O']).columns)","ab216b47":"home_data.select_dtypes(exclude=['O']).describe().round(decimals=2)","a40f6c9c":"home_data.select_dtypes(include=['O']).columns","36f66237":"len(home_data.select_dtypes(include=['O']).columns)","b57faa0a":"home_data.select_dtypes(include='O').describe()","cb3a34b6":"target = home_data.SalePrice\nplt.figure()\nsns.distplot(target)\nplt.title('Distribution of SalePrice')\nplt.show()","3a0c50b2":"sns.distplot(np.log(target))\nplt.title('Distribution of Log-transfromed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","162f925f":"print(f'Target has a skew of {str(target.skew().round(decimals=2))}, while the log-transofrmed SalePrice improves the skew to {np.log(target).skew().round(decimals=2)}')","4b03cc60":"num_attr = home_data.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\nfig = plt.figure(figsize=(15, 18))\nskewness = {}\nfor i in range(len(num_attr.columns)):\n    skewness[num_attr.columns[i]] = num_attr[num_attr.columns[i]].skew().round(decimals=2)\n    fig.add_subplot(9, 4, i+1)\n    try:\n        sns.distplot(num_attr.iloc[:, i].dropna())\n        plt.xlabel(num_attr.columns[i])\n    except:\n        plt.xlabel('ANOM_'+num_attr.columns[i])\n        print(f'Anomalies in these columns {num_attr.columns[i]}')\n        pass\n\nplt.tight_layout()\nplt.show()","b122ef8d":"for k,v in skewness.items():\n    if v > 1:\n        print(k, v)","da522c92":"fig = plt.figure(figsize=(12, 18))\nfor i in range(len(num_attr.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.boxplot(y=num_attr.iloc[:, i])\n\nplt.tight_layout()\nplt.show()","315dde13":"fig = plt.figure(figsize=(12, 18))\nfor i in range(len(num_attr.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_attr.iloc[:, i], target)\n\nplt.tight_layout()\nplt.show()","ab4e810f":"correlation = home_data.corr()\nf, ax = plt.subplots(figsize=(14,12))\nplt.title('Correlation of numerical attributes')\nsns.heatmap(correlation)\nplt.show()","24369869":"correlation['SalePrice'].sort_values(ascending=False).head(15)","efb4d9fc":"corr_columns = []\nfor i in correlation:\n    corr_columns.append(i)","ce69cdb4":"fig = plt.figure(figsize=(15, 18))\nfor i in range(len(correlation)):\n    fig.add_subplot(9, 5, i+1)\n    sns.scatterplot(home_data[corr_columns[i]], target)\n    plt.title(f'Corr to SalePrice= {str(np.round(correlation.SalePrice[corr_columns[i]], decimals=3))}')\nplt.tight_layout()\nplt.show()","c85a2cc3":"num_attr.isna().sum().sort_values(ascending=False).head()","21aa79e6":"cat_columns = home_data.select_dtypes(include='O').columns\ncat_columns","dc29f900":"fig = plt.figure(figsize=(18, 25))\nfor i in range(len(cat_columns)):\n    fig.add_subplot(11, 4, i+1)\n    sns.boxplot(home_data[cat_columns[i]], target)\n\nplt.tight_layout()\nplt.show()","5016cae2":"fig = plt.figure(figsize=(18, 25))\nfor i in range(len(cat_columns)):\n    fig.add_subplot(11, 4, i+1)\n    sns.countplot(home_data[cat_columns[i]])\n\nplt.tight_layout()\nplt.show()","a777da9e":"home_data[cat_columns].isna().sum().sort_values(ascending=False).head(17)","de326eb6":"home_data_copy = home_data.copy()\ncol_fil_none = ['PoolQC',\n 'MiscFeature',\n 'Alley',\n 'Fence',\n 'FireplaceQu',\n 'GarageCond',\n 'GarageQual',\n 'GarageFinish',\n 'GarageType',\n 'BsmtFinType2',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtQual',\n 'BsmtCond', 'MasVnrType']\nhome_data_copy['MasVnrArea'] = home_data_copy.MasVnrArea.fillna(0)\nfor col in col_fil_none:\n    home_data_copy[col] = home_data_copy[col].fillna('None')\nhome_data_copy.isna().sum().sort_values(ascending=False).head()","4a9b57ae":"# home_data_copy = home_data_copy.drop(home_data_copy['LotFrontage']\n#                                      [home_data_copy['LotFrontage']>200].index)\n# home_data_copy = home_data_copy.drop(home_data_copy['LotArea']\n#                                      [home_data_copy['LotArea']>100000].index)\n# home_data_copy = home_data_copy.drop(home_data_copy['BsmtFinSF1']\n#                                      [home_data_copy['BsmtFinSF1']>4000].index)\n# home_data_copy = home_data_copy.drop(home_data_copy['TotalBsmtSF']\n#                                      [home_data_copy['TotalBsmtSF']>6000].index)\n# home_data_copy = home_data_copy.drop(home_data_copy['1stFlrSF']\n#                                      [home_data_copy['1stFlrSF']>4000].index)\n# home_data_copy = home_data_copy.drop(home_data_copy.GrLivArea\n#                                      [(home_data_copy['GrLivArea']>4000) & \n#                                       (target<300000)].index)\n# home_data_copy = home_data_copy.drop(home_data_copy.LowQualFinSF\n#                                      [home_data_copy['LowQualFinSF']>550].index)","9c8ee693":"home_data_copy['LotFrontage'] = home_data_copy.drop(home_data_copy['LotFrontage'][home_data_copy['LotFrontage']>200].index)\nhome_data_copy['LotArea'] = home_data_copy.drop(home_data_copy['LotArea'][home_data_copy['LotArea']>100000].index)\nhome_data_copy['BsmtFinSF1'] = home_data_copy.drop(home_data_copy['BsmtFinSF1'][home_data_copy['BsmtFinSF1']>4000].index)\nhome_data_copy['BsmtFinSF2'] = home_data_copy.drop(home_data_copy['BsmtFinSF2'][home_data_copy['BsmtFinSF2']>1500].index)\nhome_data_copy['TotalBsmtSF'] = home_data_copy.drop(home_data_copy['TotalBsmtSF'][home_data_copy['TotalBsmtSF']>4000].index)\nhome_data_copy['1stFlrSF'] = home_data_copy.drop(home_data_copy['1stFlrSF'][home_data_copy['1stFlrSF']>4000].index)\nhome_data_copy['GrLivArea'] = home_data_copy.drop(home_data_copy['GrLivArea'][(home_data_copy['GrLivArea']>4000) & (target<300000)].index)\nhome_data_copy['WoodDeckSF'] = home_data_copy.drop(home_data_copy['WoodDeckSF'][home_data_copy['WoodDeckSF']>750].index)\nhome_data_copy['OpenPorchSF'] = home_data_copy.drop(home_data_copy['OpenPorchSF'][home_data_copy['OpenPorchSF']>400].index)\nhome_data_copy['EnclosedPorch'] = home_data_copy.drop(home_data_copy['EnclosedPorch'][home_data_copy['EnclosedPorch']>400].index)\nhome_data_copy['OpenPorchSF'] = home_data_copy.drop(home_data_copy['OpenPorchSF'][home_data_copy['OpenPorchSF']>400].index)\nhome_data_copy['MiscVal'] = home_data_copy.drop(home_data_copy['MiscVal'][home_data_copy['MiscVal']>5000].index)","e9e89323":"home_data_copy.isna().sum().sort_values(ascending=False).head(15)","d32e97bc":"home_data_copy['SalePrice'] = np.log(home_data_copy['SalePrice'])\nhome_data_copy = home_data_copy.rename(columns={'SalePrice': 'SalePrice_log'})\nhome_data_copy","5b519db8":"correlation['SalePrice'].sort_values(ascending=False)","780d2e4b":"home_data_copy.isna().sum().sort_values(ascending=False).head(15)","760a6206":"my_imputer = SimpleImputer(strategy='most_frequent')\nimputed_home_data_copy = pd.DataFrame(my_imputer.fit_transform(home_data_copy), index=home_data_copy.index)\nimputed_home_data_copy.columns = home_data_copy.columns","333d870c":"imputed_home_data_copy.isna().sum().sort_values(ascending=False).head(15)","8ad2c18b":"col_drop = ['SalePrice_log', 'MSSubClass', 'YrSold', 'MiscVal', 'MoSold' , 'GarageArea', 'GarageYrBlt', 'YearRemodAdd']\n\n# home_data_copy['LotFrontage'] = home_data_copy.LotFrontage.fillna(home_data_copy['LotFrontage'].mode()[0])\n# home_data_copy['Electrical'] = home_data_copy.Electrical.fillna(home_data_copy['Electrical'].mode()[0])\n\nX = imputed_home_data_copy.drop(col_drop, axis=1)\ny = imputed_home_data_copy.SalePrice_log","da9919ef":"X.info()","5c740775":"home_data_copy.select_dtypes(include='O')","e63aaaf1":"X.isna().sum().sort_values(ascending=False).head(10)","30c2a815":"# one-hot encoding to all categorical columns so far TODO: only with less than 10 unique values, greater than 10 deal with label encoder\nX = pd.get_dummies(X)\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, random_state=1)","1ef799ed":"# my_imputer = SimpleImputer()\n# imputed_x_train = pd.DataFrame(my_imputer.fit_transform(x_train), index=x_train.index)\n# imputed_x_train.columns = x_train.columns\n# imputed_x_valid = pd.DataFrame(my_imputer.transform(x_valid), index=x_valid.index)\n# imputed_x_valid.columns = x_valid.columns","3d45646c":"x_train.isna().sum().sort_values(ascending=False).head(10)","02bee0e4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","ccfd635f":"mae_compare = pd.Series()\nmae_compare.index.name = 'Algorithm'","893119e2":"model_1 = DecisionTreeRegressor(random_state=1)\nmae_compare['DecisionTree'] = score(model_1, x_train, y_train, x_valid, y_valid)['mae']","1e76f342":"model_2 = RandomForestRegressor(random_state=1)\nmae_compare['RandomForest'] = score(model_2, x_train, y_train, x_valid, y_valid)['mae']","eec4b23e":"model_3 = LinearRegression()\nmae_compare['LinearRegression'] = score(model_3, x_train, y_train, x_valid, y_valid)['mae']","168bd2df":"model_4 = XGBRegressor()\nmae_compare['XGBoost'] = score(model_4, x_train, y_train, x_valid, y_valid)['mae']","4f7a7f71":"mae_compare.sort_values()","2f9fdb05":"# imputer = SimpleImputer()\n# imputed_X = pd.DataFrame(imputer.fit_transform(X), index=X.index)\n# imputed_X.columns = X.columns\nrmse_compare = pd.DataFrame({'Algorithm': [], 'RMSE': [], 'Error std': []})","f13ffc56":"rmse = np.sqrt(-cross_val_score(model_1, X, y, scoring='neg_mean_squared_error', cv=10))\nrmse_compare.loc[0] = ('DecisionTree', rmse.mean(), rmse.std())","b57f98cc":"rmse = np.sqrt(-cross_val_score(model_2, X, y, scoring='neg_mean_squared_error', cv=10))\nrmse_compare.loc[1] = ('RandomForest', rmse.mean(), rmse.std())","cd05d505":"rmse = np.sqrt(-cross_val_score(model_3, X, y, scoring='neg_mean_squared_error', cv=10))\nrmse_compare.loc[2] = ('LinearRegression', rmse.mean(), rmse.std())","9419606f":"rmse = np.sqrt(-cross_val_score(model_4, X, y, scoring='neg_mean_squared_error', cv=10))\nrmse_compare.loc[3] = ('XGBoost', rmse.mean(), rmse.std())","b0eb4c31":"rmse_compare.sort_values(by='RMSE')","fb6dd41c":"# Tuning RandomForestRegressor\n# param_grid = [{'n_estimators': [50, 100, 150, 200, 250], 'max_depth': [2, 4, 8, 10]}]\n# top_reg = XGBRegressor()\n\n# # -------------------------------------------------------\n# grid_search = GridSearchCV(top_reg, param_grid, cv=5, \n#                            scoring='neg_mean_squared_error')\n\n# grid_search.fit(imputed_X, y)\n\n# grid_search.best_params_","0d8ee7e0":"# n_estimators\nn_est = [i for i in range(25, 100, 10)]\nmae_scores= []\nmse_scores = []\nfor i in n_est:\n    model = RandomForestRegressor(n_estimators=i, random_state=1)\n    mae_scores.append(score(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, x_train, y_train, x_valid, y_valid)['mse'])\n    \nplt.plot(n_est, mae_scores, label='mae score')\n# plt.plot(n_est, mse_scores, label='mse score')","905dff0d":"# depth\ndepth_ = [i for i in range(12, 24)]\nmae_scores= []\nmse_scores = []\nfor i in depth_:\n    model = RandomForestRegressor(n_estimators=55, max_depth=i, random_state=1)\n    mae_scores.append(score(model, x_train, y_train, x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, x_train, y_train, x_valid, y_valid)['mse'])\n    \nplt.plot(depth_, mae_scores, label='mae score')\n# plt.plot(depth_, mse_scores, label='mse score')","8cbd7f7a":"model = RandomForestRegressor(n_estimators=55, random_state=1)\nscore(model, x_train, y_train, imputed_x_valid, y_valid)","64397843":"model = RandomForestRegressor(n_estimators=55, max_depth=19, random_state=1)\nscore(model, x_train, y_train, imputed_x_valid, y_valid)","e9702ab2":"model = LinearRegression()\nscore(model, x_train, y_train, imputed_x_valid, y_valid)","bddd63a9":"home_data.isna().sum().sort_values(ascending=False).head(20)","1bd7ba1e":"test_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=0)\ntest_X = test_data.copy()\n\nfor col in col_fil_none:\n    test_X[col] = test_X[col].fillna('None')\n\nif 'SalePrice_log' in col_drop:\n    col_drop.remove('SalePrice_log')\n\ntest_X = test_data.drop(col_drop, axis=1)\ntest_X['LotFrontage'] = test_X['LotFrontage'].fillna(test_X['LotFrontage'].mode()[0])\ntest_X['MasVnrArea'] = test_X['MasVnrArea'].fillna(0)\ntest_X['BsmtHalfBath'] = test_X['BsmtHalfBath'].fillna(test_X['BsmtHalfBath'].mode()[0])\ntest_X['BsmtFullBath'] = test_X['BsmtFullBath'].fillna(test_X['BsmtFullBath'].mode()[0])\ntest_X['GarageCars'] = test_X['GarageCars'].fillna(test_X['GarageCars'].mode()[0])\ntest_X['BsmtFinSF1'] = test_X['BsmtFinSF1'].fillna(test_X['BsmtFinSF1'].mode()[0])\ntest_X['BsmtFinSF2'] = test_X['BsmtFinSF2'].fillna(test_X['BsmtFinSF2'].mode()[0])\ntest_X['BsmtUnfSF'] = test_X['BsmtUnfSF'].fillna(test_X['BsmtUnfSF'].mode()[0])\ntest_X['TotalBsmtSF'] = test_X['TotalBsmtSF'].fillna(test_X['TotalBsmtSF'].mode()[0])\n\ntest_X = pd.get_dummies(test_X)\n# final_train, final_test = X.align(test_X, join='left', axis=1)\n# final_test_imputed = pd.DataFrame(my_imputer.transform(test_X), index=test_X.index)\n# final_test_imputed.columns = final_test.columns\n# final_train_imputed = pd.DataFrame(my_imputer.fit_transform(final_train), index=final_train.index)\n# final_train_imputed.columns = final_train.columns\n\nfinal_model = RandomForestRegressor(n_estimators=55, max_depth=19, random_state=1)\ndrop_x_train = X.drop([i for i in X.columns if i not in test_X.columns], axis=1)\nfinal_model.fit(drop_x_train, y)\n\ntest_preds = final_model.predict(test_X)","f4ed6bf9":"output = pd.DataFrame({'Id': test_data.index, 'SalePrice': inv_y(test_preds)})\noutput.to_csv('houseprice_test5.csv', index=False)","ccd0a678":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=0)\n\ntrain.shape, test.shape","67351d85":"train.isna().sum().sort_values(ascending=False).head(20)","cde31648":"train.select_dtypes(exclude='O').isna().sum().sort_values(ascending=False).head()","88260ea3":"train.select_dtypes(include='O').isna().sum().sort_values(ascending=False).head(20)","0a71a748":"missing_col = ['PoolQC',\n 'MiscFeature',\n 'Alley',\n 'Fence',\n 'FireplaceQu',\n 'LotFrontage',\n 'GarageType',\n 'GarageCond',\n 'GarageFinish',\n 'GarageQual',\n 'GarageYrBlt',\n 'BsmtFinType2',\n 'BsmtExposure',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtFinType1',\n 'MasVnrArea',\n 'MasVnrType',\n 'Electrical']\n\nmissing_num_col = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea']\nmissing_cat_col = [i for i in missing_col if i not in missing_num_col]","7c54f88a":"train_copy = train.copy()\ntest_copy = test.copy()\n\ntarget = train.SalePrice","31d6020f":"def drop_outliers(df):\n    '''drop all outstanding rows, which leads more columns with missing values'''\n    df['LotFrontage'] = df['LotFrontage'].drop(df['LotFrontage'][df['LotFrontage']>200].index)\n    df['LotArea'] = df['LotArea'].drop(df['LotArea'][df['LotArea']>100000].index)\n    df['BsmtFinSF1'] = df['BsmtFinSF1'].drop(df['BsmtFinSF1'][df['BsmtFinSF1']>4000].index)\n#     df['BsmtFinSF2'] = df['BsmtFinSF2'].drop(df['BsmtFinSF2'][df['BsmtFinSF2']>1500].index)\n    df['TotalBsmtSF'] = df['TotalBsmtSF'].drop(df['TotalBsmtSF'][df['TotalBsmtSF']>5000].index)\n#     df['1stFlrSF'] = df['1stFlrSF'].drop(df['1stFlrSF'][df['1stFlrSF']>4000].index)\n    df['GrLivArea'] = df['GrLivArea'].drop(df['GrLivArea'][df['GrLivArea']>4000].index)\n\n#     df['GrLivArea'] = df['GrLivArea'].drop(df['GrLivArea'][(df['GrLivArea']>4000) & (target<300000)].index)\n#     df['LowQualFinSF'] = df['LowQualFinSF'].drop(df['LowQualFinSF'][df['LowQualFinSF']>550].index)\n#     df['WoodDeckSF'] = df['WoodDeckSF'].drop(df['WoodDeckSF'][df['WoodDeckSF']>750].index)\n#     df['OpenPorchSF'] = df['OpenPorchSF'].drop(df['OpenPorchSF'][df['OpenPorchSF']>400].index)\n#     df['EnclosedPorch'] = df['EnclosedPorch'].drop(df['EnclosedPorch'][df['EnclosedPorch']>400].index)\n#     df['OpenPorchSF'] = df['OpenPorchSF'].drop(df['OpenPorchSF'][df['OpenPorchSF']>400].index)\n#     df['MiscVal'] = df['MiscVal'].drop(df['MiscVal'][df['MiscVal']>5000].index)\n    return df","922b800e":"train_copy = drop_outliers(train_copy)\n# test_copy = drop_outliers(test_copy)\n\ntrain_copy","aa62ee77":"train_copy.isna().sum().sort_values(ascending=False).head(30)","7ac9834a":"missing_col_2 = ['PoolQC',\n 'MiscFeature',\n 'Alley',\n 'Fence',\n 'FireplaceQu',\n 'LotFrontage',\n 'GarageCond',\n 'GarageQual',\n 'GarageType',\n 'GarageYrBlt',\n 'GarageFinish',\n 'BsmtExposure',\n 'BsmtFinType2',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtFinType1',\n 'MasVnrType',\n 'MasVnrArea',\n 'OpenPorchSF',\n 'LotArea',\n 'GrLivArea',\n 'MiscVal',\n 'TotalBsmtSF',\n 'LowQualFinSF',\n '1stFlrSF',\n 'Electrical',\n 'WoodDeckSF',\n 'BsmtFinSF1',\n 'EnclosedPorch']\n\nnew_missing_num_col = [i for i in missing_col_2 if i not in missing_col]\nnew_missing_num_col","f7de85b6":"train_copy.describe(include='O')","96efe680":"drop_col = ['PoolQC', 'MiscFeature', 'OverallCond', \n            'MSSubClass', 'YrSold', 'MiscVal', 'MoSold' , 'GarageArea', 'GarageYrBlt', 'GarageCond', 'YearRemodAdd',\n            'LotShape','Condition2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2',\n             'Alley', 'FireplaceQu','Fence', 'LotConfig', 'Condition2', 'RoofStyle', 'Exterior2nd', 'MasVnrType', 'ExterCond', 'BsmtCond']","7e25f455":"train = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col=0)\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=0)\ntarget = train.SalePrice\ntrain_copy = train.copy()\ntest_copy = test.copy()","dce8f1fd":"highly_corr = ['GarageYrBlt','TotRmsAbvGrd','1stFlrSF','GarageCars']\nmany_missing_val = ['PoolQC','MiscFeature','Alley']\nless_corr_target = ['MoSold','YrSold']\n\ncol = train.columns\nmost_one_value = []  # ['Street', 'Utilities','Condition2','RoofMatl','Heating','LowQualFinSF','3SsnPorch','PoolArea','MiscVal']\nfor i in col:\n    counts = train[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(train) * 100 > 96:\n        most_one_value.append(i)\n        \ntotal_drop_cols = highly_corr + many_missing_val + less_corr_target + most_one_value\ntrain_copy = train_copy.drop(total_drop_cols, axis=1)\ntest_copy = test_copy.drop(total_drop_cols, axis=1)\ntrain_copy.shape, test_copy.shape","cd83987c":"train_copy = drop_outliers(train_copy)\ncheck_null(train_copy, 20)","938f619d":"X = train_copy.loc[:, train_copy.columns!='SalePrice']\nlog_target = np.log(target)","db215b26":"x_train, x_valid, y_train, y_valid = train_test_split(X, log_target, random_state=1)","50ab1d28":"cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\nx_train[cat] = x_train[cat].fillna(\"NA\")\nx_valid[cat] = x_valid[cat].fillna(\"NA\")\n# train_copy[cat] = train_copy[cat].fillna(\"NA\")\ntest_copy[cat] = test_copy[cat].fillna(\"NA\")\n\ncols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\", \"GrLivArea\"]\nx_train[cols] = x_train.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\nx_valid[cols] = x_valid.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\n# train_copy[cols] = train_copy.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\ntest_copy[cols] = test_copy.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\n\n#for correlated relationship\nx_train['LotArea'] = x_train.groupby('Neighborhood')['LotArea'].transform(lambda x: x.fillna(x.mean()))\nx_train['LotFrontage'] = x_train.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nx_train['GarageArea'] = x_train.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nx_train['MSZoning'] = x_train.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nx_valid['LotArea'] = x_valid.groupby('Neighborhood')['LotArea'].transform(lambda x: x.fillna(x.mean()))\nx_valid['LotFrontage'] = x_valid.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\nx_valid['GarageArea'] = x_valid.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\nx_valid['MSZoning'] = x_valid.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# train_copy['LotArea'] = train_copy.groupby('Neighborhood')['LotArea'].transform(lambda x: x.fillna(x.mean()))\n# train_copy['LotFrontage'] = train_copy.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\n# train_copy['GarageArea'] = train_copy.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\n# train_copy['MSZoning'] = train_copy.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\ntest_copy['LotArea'] = test_copy.groupby('Neighborhood')['LotArea'].transform(lambda x: x.fillna(x.mean()))\ntest_copy['LotFrontage'] = test_copy.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\ntest_copy['GarageArea'] = test_copy.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))\ntest_copy['MSZoning'] = test_copy.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n#numerical\ncont = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\nx_train[cont] = x_train[cont].fillna(x_train[cont].mean())\nx_valid[cont] = x_valid[cont].fillna(x_valid[cont].mean())\n# train_copy[cont] = train_copy[cont].fillna(train_copy[cont].mean())\ntest_copy[cont] = test_copy[cont].fillna(test_copy[cont].mean())\n\nprint(check_null(x_train, 5))\nprint(check_null(x_valid, 5))\n# print(check_null(train_copy, 5))\nprint(check_null(test_copy, 5))","844282c2":"x_train['MSSubClass'] = x_train['MSSubClass'].apply(str)\nx_valid['MSSubClass'] = x_valid['MSSubClass'].apply(str)\n# train_copy['MSSubClass'] = train_copy['MSSubClass'].apply(str)\ntest_copy['MSSubClass'] = test_copy['MSSubClass'].apply(str)","d0e361df":"# Mapping Ordinal Features\nordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\nfintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\nexpose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\nfence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}\nord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\nfor col in ord_col:\n    x_train[col] = x_train[col].map(ordinal_map)\n    x_valid[col] = x_valid[col].map(ordinal_map)\n#     train_copy[col] = train_copy[col].map(ordinal_map)\n    test_copy[col] = test_copy[col].map(ordinal_map)\n    \nfin_col = ['BsmtFinType1','BsmtFinType2']\nfor col in fin_col:\n    x_train[col] = x_train[col].map(fintype_map)\n    x_valid[col] = x_valid[col].map(fintype_map)\n#     train_copy[col] = train_copy[col].map(fintype_map)\n    test_copy[col] = test_copy[col].map(fintype_map)\n\nx_train['BsmtExposure'] = x_train['BsmtExposure'].map(expose_map)\nx_train['Fence'] = x_train['Fence'].map(fence_map)\nx_valid['BsmtExposure'] = x_valid['BsmtExposure'].map(expose_map)\nx_valid['Fence'] = x_valid['Fence'].map(fence_map)\n# train_copy['BsmtExposure'] = train_copy['BsmtExposure'].map(expose_map)\n# train_copy['Fence'] = train_copy['Fence'].map(fence_map)\ntest_copy['BsmtExposure'] = test_copy['BsmtExposure'].map(expose_map)\ntest_copy['Fence'] = test_copy['Fence'].map(fence_map)","6590fc26":"# feature engineer\nx_train['TotalLot'] = x_train['LotFrontage'] + x_train['LotArea']\nx_train['TotalBsmtFin'] = x_train['BsmtFinSF1'] + x_train['BsmtFinSF2']\nx_train['TotalSF'] = x_train['TotalBsmtSF'] + x_train['2ndFlrSF']\nx_train['TotalBath'] = x_train['FullBath'] + x_train['HalfBath']\nx_train['TotalPorch'] = x_train['OpenPorchSF'] + x_train['EnclosedPorch'] + x_train['ScreenPorch']\nx_valid['TotalLot'] = x_valid['LotFrontage'] + x_valid['LotArea']\nx_valid['TotalBsmtFin'] = x_valid['BsmtFinSF1'] + x_valid['BsmtFinSF2']\nx_valid['TotalSF'] = x_valid['TotalBsmtSF'] + x_valid['2ndFlrSF']\nx_valid['TotalBath'] = x_valid['FullBath'] + x_valid['HalfBath']\nx_valid['TotalPorch'] = x_valid['OpenPorchSF'] + x_valid['EnclosedPorch'] + x_valid['ScreenPorch']\n# train_copy['TotalLot'] = train_copy['LotFrontage'] + train_copy['LotArea']\n# train_copy['TotalBsmtFin'] = train_copy['BsmtFinSF1'] + train_copy['BsmtFinSF2']\n# train_copy['TotalSF'] = train_copy['TotalBsmtSF'] + train_copy['2ndFlrSF']\n# train_copy['TotalBath'] = train_copy['FullBath'] + train_copy['HalfBath']\n# train_copy['TotalPorch'] = train_copy['OpenPorchSF'] + train_copy['EnclosedPorch'] + train_copy['ScreenPorch']\ntest_copy['TotalLot'] = test_copy['LotFrontage'] + test_copy['LotArea']\ntest_copy['TotalBsmtFin'] = test_copy['BsmtFinSF1'] + test_copy['BsmtFinSF2']\ntest_copy['TotalSF'] = test_copy['TotalBsmtSF'] + test_copy['2ndFlrSF']\ntest_copy['TotalBath'] = test_copy['FullBath'] + test_copy['HalfBath']\ntest_copy['TotalPorch'] = test_copy['OpenPorchSF'] + test_copy['EnclosedPorch'] + test_copy['ScreenPorch']\n\ncolum = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','2ndFlrSF','WoodDeckSF','TotalPorch']\nfor col in colum:\n    col_name = col+'_bin'\n    x_train[col_name] = x_train[col].apply(lambda x: 1 if x > 0 else 0)\n    x_valid[col_name] = x_valid[col].apply(lambda x: 1 if x > 0 else 0)\n#     train_copy[col_name] = train_copy[col].apply(lambda x: 1 if x > 0 else 0)\n    test_copy[col_name] = test_copy[col].apply(lambda x: 1 if x > 0 else 0)\n    \nx_train.shape, x_valid.shape, test_copy.shape","200960d2":"remaining_cat_features = x_train.select_dtypes(include='O').columns\n# remaining_cat_features = train_copy.select_dtypes(include='O').columns","2c135e97":"OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_encoder.fit(x_train[remaining_cat_features])\n# OH_encoder.fit(train_copy[remaining_cat_features])\ncolumn_name = OH_encoder.get_feature_names()\n\nOH_col_x_train = pd.DataFrame(OH_encoder.transform(x_train[remaining_cat_features]), columns=column_name, index=x_train.index)\nOH_col_x_valid = pd.DataFrame(OH_encoder.transform(x_valid[remaining_cat_features]), columns=column_name, index=x_valid.index)\n# OH_col_train = pd.DataFrame(OH_encoder.transform(train_copy[remaining_cat_features]), columns=column_name, index=train_copy.index)\nOH_col_test = pd.DataFrame(OH_encoder.transform(test_copy[remaining_cat_features]), columns=column_name, index=test_copy.index)\n\nnum_x_train = x_train.drop(remaining_cat_features, axis=1)\nnum_x_valid = x_valid.drop(remaining_cat_features, axis=1)\n# num_train = train_copy.drop(remaining_cat_features, axis=1)\nnum_test = test_copy.drop(remaining_cat_features, axis=1)\n\nOH_x_train = pd.concat([num_x_train, OH_col_x_train], axis=1)\nOH_x_valid = pd.concat([num_x_valid, OH_col_x_valid], axis=1)\n# OH_train = pd.concat([num_train, OH_col_train], axis=1)\nOH_test = pd.concat([num_test, OH_col_test], axis=1) \nOH_x_train.shape, OH_x_valid.shape, OH_test.shape","66293c1e":"# train_copy = pd.get_dummies(train_copy)\n# test_copy = pd.get_dummies(test_copy)","aeee61fb":"X = OH_train.loc[:, OH_train.columns!='SalePrice']\nlog_target = np.log(target)","3693eb85":"if X.shape[1] != OH_test.shape[1]:\n    col_not_test = [i for i in X.columns if i not in OH_test.columns]\n    col_not_train = [i for i in OH_test.columns if i not in X.columns]\n    drop_cols = col_not_test + col_not_train\n    print('Start dropping...')\n    X = X.drop(col_not_test, axis=1)\n    OH_test = OH_test.drop(col_not_train, axis=1)\n    \nX.shape, OH_test.shape","b17faf87":"x_train, x_valid, y_train, y_valid = train_test_split(X, log_target, test_size=0.4, random_state=1)","d9a5b0af":"# n_estimators\nn_est = [i for i in range(90, 300, 10)]\nmae_scores= []\nmse_scores = []\nfor i in n_est:\n    model = RandomForestRegressor(n_estimators=i, random_state=1)\n    mae_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mse'])\n    \nplt.plot(n_est, mae_scores, label='mae score')","8fdfcb7d":"# depth\ndepth_ = [i for i in range(23,35)]\nmae_scores= []\nmse_scores = []\nfor i in depth_:\n    model = RandomForestRegressor(n_estimators=80, max_depth=i, random_state=1)\n    mae_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mse'])\n    \nplt.plot(depth_, mae_scores, label='mae score')\n# plt.plot(depth_, mse_scores, label='mse score')","458556c7":"model = RandomForestRegressor(n_estimators=80, max_depth=25, random_state=1)\nscore(model, OH_x_train, y_train, OH_x_valid, y_valid)","06e0b807":"model = RandomForestRegressor(n_estimators=80, max_depth=14, random_state=1)\nscore(model, OH_x_train, y_train, OH_x_valid, y_valid)","907f07bc":"X.shape, log_target.shape, OH_test.shape","9cf97c8e":"total_x = pd.concat([OH_x_train,OH_x_valid]).sort_index()\ntotal_x.shape, log_target.shape, OH_test.shape","2b3cff43":"final_model = RandomForestRegressor(n_estimators=80, max_depth=14, random_state=1)\nfinal_model.fit(OH_x_train, y_train)\npreds = final_model.predict(OH_test)\noutput = pd.DataFrame({'Id': test.index, 'SalePrice': inv_y(preds)})\noutput.to_csv('houseprice_test18.csv', index=False)","f983a894":"train_copy['GarageYrBlt'] = train_copy['GarageYrBlt'].fillna(0)\ntrain_copy['MasVnrArea'] = train_copy['MasVnrArea'].fillna(0)\ntest_copy['GarageYrBlt'] = test_copy['GarageYrBlt'].fillna(0)\ntest_copy['MasVnrArea'] = test_copy['MasVnrArea'].fillna(0)\n\ntrain_copy = train_copy.drop(drop_col, axis=1)\ntest_copy = test_copy.drop(drop_col, axis=1)\n\ntrain_copy.shape, test_copy.shape","e8af1f05":"# can be done after split dataset or before k-fold cross-val\nmissing_col_fil_none = list(set(missing_cat_col) - set([i for i in missing_cat_col if i in drop_col]+['Electrical']))\nmissing_col_fil_mode = ['Electrical']\nmissing_col_fil_median = list(set(new_missing_num_col)-set([i for i in drop_col if i in new_missing_num_col])) +['LotFrontage']","f236c903":"X = train_copy.loc[:, train_copy.columns != 'SalePrice']","1bbe0b35":"trans_target = np.log(target)","0ceaa23d":"x_train, x_valid, y_train, y_valid = train_test_split(X, trans_target, random_state=1)\nx_train.shape, x_valid.shape","b53d430c":"imputed_none_x_train = x_train.copy()\nimputed_none_x_valid = x_valid.copy()\nimputed_none_test = test_copy.copy()\nfor col in missing_col_fil_none:\n    imputed_none_x_train[col] = x_train[col].fillna('None')\n    imputed_none_x_valid[col] = x_valid[col].fillna('None')\n    imputed_none_test[col] = test_copy[col].fillna('None')","5bb30a90":"imputed_mode_x_train = imputed_none_x_train.copy()\nimputed_mode_x_valid = imputed_none_x_valid.copy()\nimputed_mode_test = imputed_none_test.copy()\nfor col in missing_col_fil_mode:\n    imputed_mode_x_train[col] = imputed_none_x_train[col].fillna(imputed_none_x_train[col].mode()[0])\n    imputed_mode_x_valid[col] = imputed_none_x_valid[col].fillna(imputed_none_x_valid[col].mode()[0])\n    imputed_mode_test[col] = imputed_mode_test[col].fillna(imputed_mode_test[col].mode()[0])","32ff117d":"imputed_median_x_train = imputed_mode_x_train.copy()\nimputed_median_x_valid = imputed_mode_x_valid.copy()\nimputed_median_test = imputed_mode_test.copy()\nfor col in missing_col_fil_median:\n    imputed_median_x_train[col] = imputed_mode_x_train[col].fillna(imputed_mode_x_train[col].mean())\n    imputed_median_x_valid[col] = imputed_mode_x_valid[col].fillna(imputed_mode_x_valid[col].mean())\n    imputed_median_test[col] = imputed_median_test[col].fillna(imputed_median_test[col].mean())","c4f53fc9":"check_null(imputed_median_x_train).head()","825b0cd3":"# check_null(imputed_median_test)\ntest_imputer = SimpleImputer(strategy='most_frequent')\nimputed_median_test = pd.DataFrame(test_imputer.fit_transform(imputed_median_test), index=imputed_median_test.index, columns=imputed_median_test.columns)","b628fc84":"train_copy.describe(include='O')","0d77b0b6":"# ordinal_features = ['MSZoning','Neighborhood', 'Condition1', 'HouseStyle', 'Functional',\n#                     'ExterQual', 'BsmtQual','KitchenQual', 'FireplaceQu', 'GarageQual', 'SaleType']\ncat_features = [ 'Neighborhood', 'RoofMatl', 'Exterior1st', 'HeatingQC']\nremaining_cat_features = list(set(imputed_median_x_train.select_dtypes(include='O').columns) - set(ordinal_features))\nordinal_features = list(set(imputed_median_x_train.select_dtypes(include='O').columns) - set(cat_features))","28e89821":"label_x_train = imputed_median_x_train.copy()\nlabel_x_valid = imputed_median_x_valid.copy()\nlabel_test = imputed_median_test.copy()\n\nlabel_encoder = LabelEncoder()\nfor col in ordinal_features:\n    label_x_train[col] = label_encoder.fit_transform(imputed_median_x_train[col])\n    label_x_valid[col] = label_encoder.transform(imputed_median_x_valid[col])\n    label_test[col] = label_encoder.transform(imputed_median_test[col])","b442f24d":"label_x_train.shape, label_test.shape","ed18c177":"remaining_cat_features = cat_features","75a6b7a3":"OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_encoder.fit(label_x_train[remaining_cat_features])\ncolumn_name = OH_encoder.get_feature_names()\nOH_col_train = pd.DataFrame(OH_encoder.transform(label_x_train[remaining_cat_features]), columns=column_name, index=label_x_train.index)\nOH_col_valid = pd.DataFrame(OH_encoder.transform(label_x_valid[remaining_cat_features]), columns=column_name, index=label_x_valid.index)\nOH_col_test = pd.DataFrame(OH_encoder.transform(label_test[remaining_cat_features]), columns=column_name, index=label_test.index)","6ea2aad6":"num_x_train = label_x_train.drop(remaining_cat_features, axis=1)\nnum_x_valid = label_x_valid.drop(remaining_cat_features, axis=1)\nnum_test = label_test.drop(remaining_cat_features, axis=1)\nOH_x_train = pd.concat([num_x_train, OH_col_train], axis=1)\nOH_x_valid = pd.concat([num_x_valid, OH_col_valid], axis=1)\nOH_test = pd.concat([num_test, OH_col_test], axis=1) ","a97156e4":"OH_x_train.shape, OH_x_valid.shape, OH_test.shape","b97d7e7f":"# n_estimators\nn_est = [i for i in range(50, 150, 10)]\nmae_scores= []\nmse_scores = []\nfor i in n_est:\n    model = RandomForestRegressor(n_estimators=i, random_state=1)\n    mae_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mse'])\n    \nplt.plot(n_est, mae_scores, label='mae score')","99ae4f19":"# depth\ndepth_ = [i for i in range(5, 20)]\nmae_scores= []\nmse_scores = []\nfor i in depth_:\n    model = RandomForestRegressor(n_estimators=80, max_depth=i, random_state=1)\n    mae_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mae'])\n    mse_scores.append(score(model, OH_x_train, y_train, OH_x_valid, y_valid)['mse'])\n    \nplt.plot(depth_, mae_scores, label='mae score')\n# plt.plot(depth_, mse_scores, label='mse score')","bef407a2":"model = RandomForestRegressor(n_estimators=80, random_state=1)\nscore(model, OH_x_train, y_train, OH_x_valid, y_valid)","c8552de3":"total_x = pd.concat([OH_x_train, OH_x_valid])\ntotal_y = pd.concat([y_train, y_valid])","78fa08db":"total_x.shape, OH_test.shape","90cd827c":"final_model = RandomForestRegressor(n_estimators=80, random_state=1)\nfinal_model.fit(total_x, total_y)","67e3d6d5":"preds = final_model.predict(OH_test)\noutput = pd.DataFrame({'Id': test.index, 'SalePrice': inv_y(preds)})\noutput.to_csv('houseprice_test12.csv', index=False)","0897a7ed":"### Done first try\n     - RandomForestRegressor(n_estimators=310, max_depth=12, random_state=1) reach best mae score=16735.62160553494\n    - no feature engineer, just non-null numerical features\n### Score\n    - local = {'mae': 16806.679666371307, 'mse': 638147857.8412026, 'r2': 0.9049417278546259}\n    - submitted=17416.92709 (model not on full train dataset, on splitted dataset)\n    - submitted=16602.87879 (model on full train dataset) \n****\n---------------","a528c1f5":"### Notes for Data Cleaning & Preprocessing:\n    * PoolQC          1453\n    * MiscFeature     1406\n    * Alley           1369\n    * Fence           1179\n    * FireplaceQu      690\n    * GarageCond        81\n    * GarageQual        81\n    * GarageFinish      81\n    * GarageType        81\n    * BsmtFinType2      38\n    * BsmtExposure      38\n    * BsmtFinType1      37\n    * BsmtQual          37\n    * BsmtCond          37 \n- Above missing values as houses do not have them, so it can be filled with 'None' value\n- MasVnrType has 8 missing value same as analysis from numerical columns [MasVnrArea       8], likely house do not have, can be filled with 'None'\n- Electrical only has one missing value, could be filled with mode? or can be filled by imputed value\n\n----------\n\n### 2. Data Cleaning & Preprocessing\n\n   ### 2.1 Dealing with missing\/null values","641ae175":"* Previous best model did not get better score than all-in non-null numerical features \n* **Test to see change model parameters to see whether it achive better score?****","5ead8ead":"### Visualization of categorical attributes versus target","1e2de1a2":"### Numerical columns within the dataset","ace29455":"### Frequency distribution of attributes","cd76ed14":"### 2.2 Addressing outliers","557d7722":"### Distribution of numerical attributes\nAnomaly 'scott'value inside numerical columns which lead column to become 'object', can not draw graph","c1b1832f":"### Split dataset \n- apply log transformation on target\n- fill null\/missing value ","cf2afb16":"### Missing\/null values in categorical columns","709b1f9c":"### 1.1 Preliminary observations","9a25e21a":"b. Within bivariate analysis, using scatter plots. Outliers have y-values that are unusual in relation to other observations with similar x-values. [draw scatter plot with x value numerical attributes, y value target]","62b49ad5":"Done fill missing values \n\n----------\n\n### Encode categorical features\n- ordinal pattern in following features can be encoded by label encoder library from sklearn\n    - Condition1\tCondition2\n    - ExterQual\tExterCond\n    - BsmtQual\tBsmtCond\n    - KitchenQual\n    - FireplaceQu\n    - GarageQual\tGarageCond\n \n- remaining features will be encoded by one-hot\n","b1236552":"### Outliers\nBased on previous eda:\n- LotArea may contain outliers as its max value is greater than living area max\n- MiscVal may contain outliers as its max takes 2% of overall house price \n\nBased on scatter plots vesus target and univariate box plots:\n- LotFrontage, LotArea, BsmtFinSF1, BsmtFinSF2, TotalBsmtSF, 1stFlrSF,GrLivArea  WoodDeckSF, OpenPorchSF, EnclosedPorch, OpenPorchSF, MiscVal all have few outliers inside","34ebea6b":"### Missing value","85eb3e81":"### Notes for Data Cleaning & Preprocessing\nBased on the above scatter plots, there will be few **outliers**.\n\n### Access correlations amongst attributes\n- correlation values may be heavily influenced by single outliers\n- \"use linear regression for modelling, it is necessary to remove correlated variables to improve model\"\n","f56c40e1":"### Find Outliers\nVisualization of data may support the discouvery of possible outliers within the data.\n\na. Within univariate analysis, using box plots. Outliers are obersvations more than the IQR beyond the upper or lower quartile. [draw box plot with features only] ","9ec77230":"with reference to the target, the top correlated attributes are:","0b243f17":"- LotArea.max = 215245.000000 > max of living area=5642.000000 -> outliers inside LotArea\n- miscellaneous.max = 15500.00 take 2%of max house price, may be outliers\n--------------------","39e8d3d9":"### Categorical columns within the dataset","3c696121":"### Notes for Data Cleaning& Preprocessing\nUni-modal(only on peak), skewed distributions could potentially be log transformed: \\\nLotFrontage 2.16 \\\nLotArea 12.21 \\\n1stFlrSF 1.38 \\\nGrLivArea 1.37 \\\nOpenPorchSF 2.36 \n\nThis list exclude just found anomaly columns \n\n\n\n\n\n","6eb934e7":"Dataset has 1460 rows\/instances, with data from 80 attributes. \\\nOut of 80 attributes, one is the target that model should predict. \\\nHence, there are 79 attributes that may be used for fetaure selection\/engineering.","62723a94":"### 1.2 Exploring numerical columns\n\nSkew of target column","93ff8dc8":"### Done second try\n    - model = RandomForestRegressor(n_estimators=450, max_depth=15, random_state=1) reach best mae \n    - based on corr>0.5 features, 10 in total, also numerical features\n### Score\n    - submitted=18275.18270 (model on full train dataset)  [16602.87879 scores on fool way]\n****\n---------------\n\n**Sum up**\n- only use features selected based on correlation not good ideas\n- also obevious more features works better, all-in ways select 34 features works way better than correlation one 10 features\n- learn new knowledge that use highly-correlated features may lead algorithm in reduction performance, so it might be another reason it does not work well","14e9efa0":"### Notes for Data Cleaning& Preprocessing:\n- Perform log transformation on SalePrice\n- Feature varibales that are skewed should also be investigated to assess whether they require transformation","2d1a95b8":"### 2.3 Transforming data to reduce skew\n- apply log transformance to target\n- skewed features not sure to be applied transformance?","3ca4915f":"### !!! can log price, so that decrease large gap\n    > import sklearn\n    > sorted(sklearn.metrics.SCORERS.keys())","8ae0d151":"### score\n1.(dummies)\n- local={'mae': 16031.252141969784, 'mse': 671466340.1984859}\n- whole dataset public=16686.46623\n- splited dataset public=17187.45583\n\n2.(onehot)\n- local={'mae': 15988.569774769829, 'mse': 652963562.4143941}\n- whole dataset public=16495.94609\n- splited dataset public=17156.43993\n\n3.(encode after split)\n- local={'mae': 16256.248618206168, 'mse': 870189119.5109609}\n- whole dataset public=16511.17029\n- splited dataset public=17125.60170\n\n\n\n","46a1a686":"Based on the above correlation list, may drop columns with few high correlation and low correlation\n- MSSubClass, YrSold, MiscVal, MoSold columns with low correlations, also less relative meaning respect to price\n- GarageArea, GarageYrBlt, YearRemodAdd columns with high correlations\n","8cf17a0d":"There are 43 categorical columns, with the following characteristics:","cacd664c":"model = RandomForestRegressor(n_estimators=55, max_depth=19, random_state=1) \\\nScore:\n- local = {'mae': 16094.723902085541, 'mse': 613561500.485585}\n* whole dataset public=16838.68658 with fill null value by imputation \n* splited dataset public=17132.26901 with fill null value by mode \n* whole dataset public=16557.23485 with fill null value by mode\n","0cbd2cf4":"## Second try\n****Selected features try out -> use features from correlation > 0.5 **","4e06722c":"### Final model to train and predict","c88c5799":"- [**PoolQC** 1453] missing value, as most house do not have pool, PoolArea associate with null value in PoolQC will be 0, not valuable column, can consider to be dropped\n- [**MiscFeature** 1406] missing value+previous eda, will consider to be dropped\n- [Alley 1369], [Fence 1179], [FireplaceQu 690], [GarageCond 81], [GarageQual 81],\n[GarageFinish      81],[GarageType        81],[BsmtFinType2      38],[BsmtExposure      38], [BsmtFinType1      37],[BsmtQual          37],[BsmtCond          37],[MasVnrType         8], [MasVnrType         8] has null\/missing value due to house do not have such property\n- [LotFrontage    259], [Electrical         1]just missing value could be filled with median value ","70812d81":"### Finish data preprocessing \n- drop columns\n- fill missing values\n- encode categorical features\n\n-----------\n","11ea70b3":"LinearRegression -> array([-15195.62996958,  -5435.65571427,   4269.11704437,   8620.28303583,\n       -10844.0793762 ,   8620.10634931,  -9786.6950593 ,   8620.05514625,\n         8620.24941952, -15195.63135819,   8620.12919005, -10844.76489403,\n       -10844.8...)","ffb9a5c4":"### Cross-validation","a699b2b0":"Assumptions:\n1. older buiding lower house price -> YearBuilt vs. SalePrice ?\n2. SaleType vs. SalePrice relation with interest or new may relatively higher price?\n3. have value in MiscFeature may increase house price as it contain several services?\n4. pool also another indicator of higher price\n5. OverallCond\/OverallQual feature store as numerical feature, should be converted to onehot encoder \n6. Neighbohood could be good indicator of house price\n7. LandSlope involve slope may lead expense of others increase, overall houseprice increase?","e3fac1f5":"## Helper functions","25a4912f":"corr\u6ca1\u6709\u5bf9\u6bd4\u7684feature list\n1. *'Street\tAlley\tLotShape\tLandContour\tUtilities\tLotConfig\tLandSlope\tNeighborhood\tCondition1\tCondition2\tBldgType\tHouseStyle RoofStyle\tRoofMatl\tExterior1st\tExterior2nd\tMasVnrType,ExterQual\tExterCond\tFoundation\tBsmtQual\tBsmtCond\tBsmtExposure\tBsmtFinType1\tHeating\tHeatingQC\tCentralAir\tElectrical,Functional,GarageQual\tGarageCond\tPavedDrive, PoolQC\tFence\tMiscFeature, YrSold\tSaleType\tSaleCondition'*","e8b88fd4":"### Notes for Feature Selection & Engineering\nBased on the scatter plots and correlation figures, excluding attributes with low corr with Price and unclear non-linear correlation\n\n### Missing\/null values in numerical columns","2c0a2d72":"### Sum up\n- all missing cat col with be filled with none value except Electrical filled with mode value **[3]**\n- GarageYrBlt and MasVnrArea will be filled with 0, GarageYrBlt will be dropped due to high corr **[1]**\n- [LotFrontage] + [new created missing num col] this time will be filled with median value **[3]**\n- columns will be droped: **[2]** \n    - 'PoolQC', 'MiscFeature', 'OverallCond' based on eda\n    - 'MSSubClass', 'YrSold', 'MiscVal', 'MoSold' , 'GarageArea', 'GarageYrBlt', 'YearRemodAdd' based on correlation, low corr and high corr\n    - 'LotShape','Condition2', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2' less relative to SalePrice\n----  \n## Fifth\n\n### **new**\n\nDrop\n- with mostly 1 value\n    *     BsmtFinSF2\n    *     LowQualFinSF\n    *     EnclosedPorch\n    *     3SsnPorch\n    *     ScreenPorch\n    *     PoolArea\n    *     MiscVal\n        \n- Highly Correlated variables(remove the highly correlated features to avoid the problem of multicollinearity):\n\n    * **GarageYrBlt** and YearBuilt\n    * **TotRmsAbvGrd** and GrLivArea\n    * **1stFlrSF** and TotalBsmtSF\n    * **GarageArea** and GarageCars\n\n\n- remove features that is not very useful in prediction due to many missing values ->  PoolQC, MiscFeature, Alley\n- remove features that does not have any linear relationship with target -> MoSold and YrSold\n- user defined threshold at 96% of a column has the same value -> ['Street', 'Utilities','Condition2','RoofMatl','Heating','LowQualFinSF','3SsnPorch','PoolArea','MiscVal']\n\nOutlier\n* LotFrontage >200\n* LotArea >100000\n* BsmtFinSF1 >4000\n* TotalBsmtSF >5000\n* GrLivArea >4000\n\nFill missing value \n- fill with 'NA'\n    - ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Fence',\"KitchenQual\",\n       \"HeatingQC\",'ExterQual','ExterCond']\n- fill cat features with mode value \n    - cols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\n    - groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.mode()[0]))\n- fill num features with mean value\n    - cols = [\"BsmtHalfBath\", \"BsmtFullBath\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\"]\n    - fillna([cols].mean())\n    \n    *** certain features like LotFrontage and GarageArea have wide variance in their distribution ***\n        - group these features by Neighborhoods to impute the respective mean values\n        - cols = ['LotFrontage', 'GarageArea']\n        - groupby('Neighborhood')[cols].transform(lambda x: x.fillna(x.mean()))\n        - X['MSZoning'] = X.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))  \n        -  change **MSZoning** data type to string\n\nMapping Ordinal Features\n- ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA':0}\n* fintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\n* expose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n* fence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}","18d7120b":"### 1. Exploratory Data Analysis\n    a. gain a preliminary understanding of available data\n    b. check for missing or null values\n    c. find potential outliers\n    d. assess correlations amongst attributes\/features\n    e. check for data skew","edb73c18":"## First fool try model\n* All non-null numerical features included\n* Use default 0.25 split train and valid to see score\n* Model: DecisionTreeRegressor, RandomForestRegressor","988c4671":"The above shows that there appears to be 37 numerical columns, including the target, SalePrice. \\\nNotes: \n- It is possible that there are numerical columns that have data in the form of discrete, and limited number of values. Such columns may also be interpreted as categorical data.\n- Some potential anomalies in certain rows of the dataset may cause the column data type to become an 'object'. How can this be checked efficiently?\n\nThe 37 numerical columns have the following general characteristics:\n","aa75ffa2":"### 5. Select of best algorithm & Fine-tuning\nscikit-learn's function GridSearchCV for the best combination of hyperparameters\n> from sklearn.model_selection import GridSearchCV\n\nSince based on the above, best model is LinearRegression, so there is no need to fine-tuning","7d831a4e":"### 4. Machine Learning Algorithms\nNote: target is now log(SalePrice), need to inverse-transform to obtain SalePrice\n\n1. ****Import machine learning modules****","acfdb1f4":"## Fourth try \n- redo steps of data cleaning & preprocessing\n- **subtract** features","6ff876ff":"### 3. Feature Selection & Engineering\n\n**Considering highly-correlated features** \\\n\nAs feeding highly-correlated features to machine learning algorithms may cause a reduction in performance.\n","6499b207":"Reading input file","725176be":"score:\n1.\n- local={'mae': 16933.62258001099, 'mse': 749120003.017304}\n- whole dataset public=16859.41583\n- splited dataset public=17389.47447\n\n2.\n* local={'mae': 16881.53840913917, 'mse': 732597699.0359966}\n* whole dataset public=16907.88673\n* splited dataset public=17351.57037\n\n3.\n- local={'mae': 16855.237488145245, 'mse': 737464308.3875}\n- whole dataset public=17468.15694\n\n4.\n- local={'mae': 16641.50345231994, 'mse': 722266958.4715488}\n- whole dataset public=16838.68548\n\n5.\n- local={'mae': 16898.424212746104, 'mse': 733921988.9858397}\n- whole dataset public=16707.65655\n\n","44d4dfa6":"## Try others notebook","97d5e23c":"### Notes for Data Cleaning & Processing\n- LotFrontage    259\n- GarageYrBlt     81\n- MasVnrArea       8 \\\n\nAbove missing columns, could be filled with imputed value?\n\n### 1.3 Exploring categorical columns","0b4a8489":"TODO list\n* all in features as baseline to see model accuracy\n* \u51cf\u6cd5\u505afeature\n* \u518d\u8bd5\u7528corr\u7ed9\u7684feature","6189b296":"Show scatter plots for each numerical attribute and correlation value:"}}