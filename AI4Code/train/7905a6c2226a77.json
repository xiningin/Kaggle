{"cell_type":{"0e2e238d":"code","24375150":"code","a15cdca9":"code","70d854da":"code","17306782":"code","83ef9e09":"code","01ab099e":"code","397ab504":"code","53c3afd9":"code","609fca4d":"code","5399a47b":"code","9d5adca8":"code","2024708d":"code","56ba276d":"code","8c10eba3":"code","d899a5c1":"code","cf9a99b9":"code","e5e5ce6a":"code","c3e5a3dd":"code","864fa680":"code","3fc90b6a":"code","29053cf3":"code","1bfae813":"code","5e223280":"code","338a1779":"code","54492b3d":"code","0dc6ff6a":"markdown","87689246":"markdown","57140c8a":"markdown","c677b047":"markdown","a9ad2c65":"markdown","06398b2c":"markdown","e7c5d40d":"markdown","f8a6e123":"markdown","2f64a906":"markdown","d1957ecb":"markdown","36e0c08f":"markdown","d000181a":"markdown","0d5ef568":"markdown"},"source":{"0e2e238d":"import pandas as pd\nimport numpy as np\n#import codecs\nimport matplotlib.pyplot as plt\nimport itertools\n","24375150":"# Import Data\n\ninput_file = open(\"..\/input\/nlp-starter-test\/socialmedia_relevant_cols.csv\", \"r\",encoding='utf-8', errors='replace')\n\n# read_csv will turn CSV files into dataframes\nquestions = pd.read_csv(input_file)","a15cdca9":"# to clean data\ndef normalise_text (text):\n    text = text.str.lower()\n    text = text.str.replace(r\"\\#\",\"\")\n    text = text.str.replace(r\"http\\S+\",\"URL\")\n    text = text.str.replace(r\"@\",\" \")\n    text = text.str.replace(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \")\n    text = text.str.replace(\"\\s{2,}\", \" \")\n    return text","70d854da":"questions[\"text\"]=normalise_text(questions[\"text\"])\n#could save to another file","17306782":"import spacy\nnlp = spacy.load(\"en\")","83ef9e09":"doc = questions[\"text\"].apply(nlp)","01ab099e":"questions.choose_one.value_counts()","397ab504":"max_sent_len=max(len(doc[i]) for i in range(0,len(doc)))\nprint(max_sent_len)\n\nvector_len=len(doc[0][0].vector)\nprint(vector_len)","53c3afd9":"tweet_matrix=np.zeros((len(doc),max_sent_len,vector_len))\nprint(tweet_matrix[0:2,0:3,0:4]) #test print","609fca4d":"for i in range(0,len(doc)):\n    for j in range(0,len(doc[i])):\n        tweet_matrix[i][j]=doc[i][j].vector","5399a47b":"list_labels = np.array(questions[\"class_label\"])\nprint(list_labels.shape[0])\nprint(tweet_matrix.shape[0])","9d5adca8":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data import random_split","2024708d":"#if you need to convert numpy ndarray to tensor explicitely\n#tweet_matrix = torch.from_numpy(tweet_matrix)","56ba276d":"#for GPU - CUDA\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assuming that we are on a CUDA machine, this should print a CUDA device:\n\nprint(device)","8c10eba3":"len_for_split=[int(tweet_matrix.shape[0]\/4),int(tweet_matrix.shape[0]*(3\/4))]\nprint(len_for_split)","d899a5c1":"test, train=random_split(tweet_matrix,len_for_split)","cf9a99b9":"test.dataset.shape","e5e5ce6a":"# Hyperparameters\nnum_epochs = 10\nnum_classes = 3\nlearning_rate = 0.001\nbatch_size=100","c3e5a3dd":"# to transform the data and labels\nclass MyDataset(Dataset):\n    def __init__(self, data, target, transform=None):\n        self.data = torch.from_numpy(data).float()\n        self.target = torch.from_numpy(target).long()\n        self.transform = transform\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        \n        if self.transform:\n            x = self.transform(x)\n        \n        return x, y\n    \n    def __len__(self):\n        return len(self.data)","864fa680":"#load labels #truncating total data to keep batch size 100\nlabels_train=list_labels[train.indices[0:8100]]\nlabels_test=list_labels[test.indices[0:2700]]\n\n#load train data\ntraining_data=train.dataset[train.indices[0:8100]].astype(float)\n#training_data=training_data.unsqueeze(1)\n\n#load test data\ntest_data=test.dataset[test.indices[0:2700]].astype(float)\n#test_data=test_data.unsqueeze(1)\n\ndataset_train = MyDataset(training_data, labels_train)\ndataset_test = MyDataset(test_data, labels_test)\n\n\n#loading data batchwise\ntrain_loader = DataLoader(dataset=dataset_train, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=dataset_test, batch_size=batch_size, shuffle=False)\n","3fc90b6a":"## setting up the CNN network\n\n#arguments(input channel, output channel, kernel size, strides, padding)\n            \n            #layer 1 : \n            # height_out=(h_in-F_h)\/S+1=(72-5)\/1+1=68\n            # width_out=(w_in-F_w)\/S+1=(384-35)\/1+1=350\n            # no padding given\n            # height_out=68\/2=34 \n            # width_out=350\/5=70\n            \n            #layer 2:\n            # height_out=(h_in-F_h)\/S+1=(34-5)\/1+1=30\n            # width_out=(w_in-F_w)\/S+1=(70-5)\/1+1=66\n            # height_out=30\/2=15 \n            # width_out=66\/2=33\n\n\nclass ConvNet(nn.Module):\n    def __init__(self):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 15, kernel_size=(5,35), stride=1,padding=0), \n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=(2,5), stride=(2,5)))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(15, 30, kernel_size=5, stride=1, padding=0),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.drop_out = nn.Dropout()\n        self.fc1 = nn.Linear(15 * 33 * 30, 5000)\n        self.fc2 = nn.Linear(5000, 100)\n        self.fc3 = nn.Linear(100,3)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.drop_out(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.fc3(out)\n        return(out)","29053cf3":"#creating instance of our ConvNet class\nmodel = ConvNet()\nmodel.to(device) #CNN to GPU\n\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\n#CrossEntropyLoss function combines both a SoftMax activation and a cross entropy loss function in the same function\n\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","1bfae813":"# Train the model\ntotal_step = 8100\/batch_size\nloss_list = []\nacc_list = []\n\n\nfor epoch in range(num_epochs):\n    for i, (data_t, labels) in enumerate(train_loader):\n        data_t=data_t.unsqueeze(1)\n        data_t, labels = data_t.to(device), labels.to(device)\n        \n        # Run the forward pass\n        outputs = model(data_t)\n        loss = criterion(outputs, labels)\n        loss_list.append(loss.item())\n        #print(\"==========forward pass finished==========\")\n            \n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        #print(\"==========backward pass finished==========\")\n        \n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list.append(correct \/ total)\n    \n        if (i+1) % 10 == 0:\n            print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),(correct \/ total) * 100))\n        \n        \n        ","5e223280":"## evaluating model\n\n# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for data_t, labels in test_loader:\n        data_t=data_t.unsqueeze(1)\n        data_t, labels = data_t.to(device), labels.to(device)\n        \n        outputs = model(data_t)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Test Accuracy of the model: {} %'.format((correct \/ total) * 100))\n\n","338a1779":"import matplotlib.pyplot as plt","54492b3d":"plt.xlabel(\"runs\")\nx_len=list(range(len(acc_list)))\nplt.axis([0, max(x_len), 0, 1])\nplt.title('result of convNet')\nloss=np.asarray(loss_list)\/max(loss_list)\nplt.plot(x_len, loss, 'r.', x_len, acc_list, 'b.')\nplt.show\n","0dc6ff6a":"# Sentence Classification using CNN\n\n## The word embeddings have been created using spaCy (english language model)\n## The CNN used is quite simple \n*  Layer 1 has 15 filters of size 5 x 35 , with stride 1\n    * followed by Maxpool\n*  Layer2 has 30 filters\n    * followed by Maxpool\n* ### followed by 3 densely connected layers\n\n\n## The results are not very accurate and I will be working on improving both the network and hyperparamters\n","87689246":"## **Data Pre Processing**\n*  Cleaning the data\n*  Building word embeddings","57140c8a":"### Embedding using spaCy ","c677b047":"### Training the model","a9ad2c65":"### Plot a graph to trace model performance","06398b2c":"### create labels","e7c5d40d":"### Train test split","f8a6e123":"### Declare Hyperparameters","2f64a906":"### Setting up the CNN model","d1957ecb":" ### Form 3D vectors for converting to use in CNNs","36e0c08f":"### Evaluating the model","d000181a":"### Load Data\n\nThe dataset is loaded in batches with the Dataset class and Dataloader Module from torch.utils.data","0d5ef568":"## **CNN in Pytorch**"}}