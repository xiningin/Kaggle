{"cell_type":{"2e824ef8":"code","40a201c6":"code","51d17e8a":"code","b3f0e215":"code","5a2c3cf6":"code","9a77539c":"code","f45f01db":"code","c1b63b25":"code","7865040d":"code","95d82d2e":"code","4fc3f1fd":"code","b3d0258f":"code","1c3aeae8":"code","0be626f4":"code","3700edd1":"code","f1fe05bb":"code","71f7fe57":"code","69e3b1d1":"code","4d6e95b4":"code","ff7d55c2":"code","eeee9eaa":"code","dc126fce":"code","9d78c390":"code","12b3f799":"markdown","4d35524c":"markdown","27365b3c":"markdown","bcad3e9b":"markdown","ea36a05a":"markdown","f9b8e1a2":"markdown","fcbd4b08":"markdown","33c2493b":"markdown","2607c0bd":"markdown","42db4696":"markdown","14154822":"markdown","9887ee8d":"markdown","895d25b5":"markdown"},"source":{"2e824ef8":"import pandas as pd\nimport numpy as np\n#pd.set_option(\"max_columns\" , None)\npd.set_option(\"max_rows\" , None)\npd.set_option(\"display.float_format\", lambda x: \"%.4f\" % x)\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import roc_auc_score\nimport optuna","40a201c6":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\ntrain1 = train.copy()\ntest1 = test.copy()","51d17e8a":"train.head()","b3f0e215":"test.head()","5a2c3cf6":"submission.head()","9a77539c":"train.info()","f45f01db":"print(f\" In Train : {train.shape[0]} obs and {train.shape[1]} features \\n In Test : {test.shape[0]} obs and {test.shape[1]} features\")","c1b63b25":"target = train.claim\ntest = test.drop([\"id\"] , axis = 1)\ntrain = train.drop([\"id\", \"claim\"] , axis = 1)","7865040d":"train.describe().T","95d82d2e":"target.describe()","4fc3f1fd":"print(pd.isnull(train).values.any())\nprint(pd.isnull(test).values.any())\nprint(pd.isnull(target).values.any())","b3d0258f":"fig = px.histogram(target, x = target, color = target)\nfig.update_layout(\n    title_text = \"Target Distribution\",\n    xaxis_title_text = \"Value\",\n    yaxis_title_text = \"Count\",\n    bargap = 0.1)\nfig.show()","1c3aeae8":"plt.figure(1, figsize = (12,7))\nplt.title(\"Target distribution\", color = \"orange\", fontsize = 15)\ntarget.value_counts().plot.pie(autopct = \"%1.4f%%\");","0be626f4":"fig = plt.figure(figsize = (30,60))\nax = fig.gca()\nhist = train.hist(bins = 50, layout = (24,5),\n                       color = \"r\", alpha = 0.5,  ax = ax)","3700edd1":"fig = plt.figure(figsize = (15, 60))\nfor i in range(len(train.columns.tolist()[:-1])):\n    plt.subplot(24,5,i+1)\n    sns.set_style(\"ticks\")\n    plt.title(train.columns.tolist()[:-1][i], size = 12, fontname = \"monospace\")\n    a = sns.boxplot(train[train.columns.tolist()[:-1][i]], linewidth = 2.5 , color = \"lightgreen\")\n    plt.ylabel(\"\")\n    plt.xlabel(\"\")\n    plt.xticks(fontname = \"monospace\")\n    plt.yticks([])\n    for j in [\"right\", \"left\", \"top\"]:\n        a.spines[j].set_visible(False)\n        a.spines[\"bottom\"].set_linewidth(1.2)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","f1fe05bb":"fig, ax = plt.subplots(figsize=(20 , 15))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype = np.bool))\n\nsns.heatmap(corr,square = True, center = 0, \n            linewidth = 0.2, cmap = \"coolwarm\",\n           mask = mask, ax = ax) \n\nax.set_title(\"Feature Correlation Matrix\", loc = \"left\")\nplt.show()","71f7fe57":"plt.figure(figsize=(25, 6))\ntrain1.corr()[\"claim\"][:-1].plot(kind = \"bar\", grid = True)\nplt.title(\"Feature Correlation Table\" , fontdict = {\"fontsize\": 20});","69e3b1d1":"Imputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\n\ndf1 = pd.DataFrame(Imputer.fit_transform(train))\ndf2 = pd.DataFrame(Imputer.fit_transform(test))\ndf1.columns = train.columns\ndf2.columns = test.columns\n\ntrain_ = df1\ntest_ = df2","4d6e95b4":"print(pd.isnull(train_).values.any())\nprint(pd.isnull(test_).values.any())","ff7d55c2":"sc = StandardScaler()\ndf_standardize = train_.copy()\ndf_standardize_test = test_.copy()\ndf_standardize[df_standardize.columns.tolist()] = sc.fit_transform(df_standardize[df_standardize.columns.tolist()])\ndf_standardize_test[df_standardize_test.columns.tolist()] = sc.fit_transform(df_standardize_test[df_standardize_test.columns.tolist()])","eeee9eaa":"df_standardize.head()","dc126fce":"df_standardize_test.head()","9d78c390":"features = train.columns[:10].tolist()\nfor i in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(train[i], color = \"red\", bins = 30, alpha = 0.3, label = \"Skew = %s\" %(str(round(train[i].skew(),3))) )\n    ax[0].set_title(str(i))   \n    ax[0].legend(loc = 0)\n    ax[1].hist(df_standardize[i], color = \"green\", bins = 30, alpha = 0.3, label = \"Skew = %s\" %(str(round(df_standardize[i].skew(),3))) )\n    ax[1].set_title(str(i)+ \"  After scaling\")\n    ax[1].legend(loc = 0)\n    plt.show()","12b3f799":"Weak Correlation ! In other words, there is no correlation.","4d35524c":"![](http:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/26479\/logos\/header.png?t=2021-04-09-00-55-58)\n\n#### <span style=\"color: orange; font-family: Segoe UI; font-size: 1.7em; font-weight: 300;\">Simple Tabular Playground Series - Sep 2021<\/span>\n\n\n**Bugra Sebati E. - September - 2021**","27365b3c":"Focus Correlation","bcad3e9b":"It looks like good. In classification problems, the distribution of the target variable is important.\n\nNow, let's look independent variables distributions.","ea36a05a":"#### **ROC Curve**\n\nAUC - ROC curve is a performance measurement for the classification problems at various threshold settings. ROC is a probability curve and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes. Higher the AUC, the better the model is at predicting 0 classes as 0 and 1 classes as 1. By analogy, the Higher the AUC, the better the model is at distinguishing between patients with the disease and no disease.\nThe ROC curve is plotted with TPR against the FPR where TPR is on the y-axis and FPR is on the x-axis.\n\n![](http:\/\/miro.medium.com\/max\/451\/1*pk05QGzoWhCgRiiFbz-oKQ.png)","f9b8e1a2":"#### **INTRODUCTION**\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are anonymized, they have properties relating to real-world features.\n\n**Eval Metric** : Submissions are evaluated on area under the **ROC curve** between the predicted probability and the observed target.","fcbd4b08":"#### Import Data & Libraries","33c2493b":"\nIf you like it , dont forget to upvote ! :) **Thanks !**","2607c0bd":"Next step is Standard Scaler method for data normalize.","42db4696":"As can be seen we have missing values. We should solve this problem. We will solve later this problem with **Simple Imputer** method. This is basic and easy method. We can use also other methods.\n\nNow, we focus to target variable !","14154822":"Let's meet the dataset ! ","9887ee8d":"Now, We will use **Simple Imputer** method for missing values.","895d25b5":"If we want to see differences , we can see in the before-after graphs.\n\nLet's look at the transformation of first 10 variables."}}