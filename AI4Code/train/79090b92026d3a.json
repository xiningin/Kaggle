{"cell_type":{"42511341":"code","c0f795d6":"code","265509ba":"code","309de2a7":"code","50dfb53a":"code","fdde79e4":"code","ebed606f":"code","69d6cb8f":"code","e9e1f86e":"code","4bc39bbd":"markdown","389ac2da":"markdown","9ce4f0db":"markdown","0609e960":"markdown","3aed2bb2":"markdown","408814c7":"markdown"},"source":{"42511341":"# Import Libraries\nimport sys\nimport numpy as np\nimport random\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint","c0f795d6":"import os\nprint(os.listdir(\"..\/input\/textdata\"))","265509ba":"# Load Dataset\nfilename    = '..\/input\/textdata\/shakespeare.txt'\ntext        = open(filename, encoding='utf-8').read()\ntext        = text.lower()\nprint('corpus length:', len(text))\n\n# Find all the unique characters\nchars        = sorted(list(set(text)))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\nvocab_size   = len(chars)\n\nprint(\"List of unique characters : \\n\", chars)\nprint(\"Number of unique characters : \\n\", vocab_size)\nprint(\"Character to integer mapping : \\n\", char_indices)","309de2a7":"# Preprocessing Dataset\nmax_seq_len = 40 # cut text in semi-redundant sequences of max_seq_len characters\nstep = 3 \nsentences = [] # list_X\nnext_chars= [] # list_Y\n\nfor i in range(0, len(text) - max_seq_len, step):\n    sentences.append(text[i: i + max_seq_len])\n    next_chars.append(text[i + max_seq_len])\nprint('nb sequences:', len(sentences))\n\nnum_sequences  = len(sentences)\nprint(\"Number of sequences: \", num_sequences)\nprint(sentences[0])","50dfb53a":"print('Vectorization...')\ntrain_X = np.zeros((len(sentences), max_seq_len, len(chars)), dtype=np.bool)\ntrain_Y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        train_X[i, t, char_indices[char]] = 1\n    train_Y[i, char_indices[next_chars[i]]] = 1\n\nprint(train_X.shape)\nprint(train_Y.shape)\nprint(max_seq_len, vocab_size)","fdde79e4":"# Build Model\ninput_shape = (max_seq_len, vocab_size)\n\nmodel = Sequential()\nmodel.add(LSTM(64, input_shape=input_shape))\nmodel.add(Dense(len(chars), activation='softmax'))","ebed606f":"# Compile Model\nadam = Adam(lr=0.0005)\nmodel.compile(optimizer=adam, loss='mse')\nmodel.summary()","69d6cb8f":"# Train Model\nnum_epochs = 10\nbatch_size = 128\n#model_path = \"textgen-lstm.h5\"\n#checkpoint = ModelCheckpoint(model_path, monitor='loss', save_best_only=True, verbose=1, mode='min')\n#callbacks_list = [checkpoint]\n\nmodel.fit(train_X, train_Y, epochs = num_epochs, batch_size = batch_size, verbose=1) #, callbacks=callbacks_list)","e9e1f86e":"# Generate Text\ndef sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\ndef generate_text():\n    start_index = random.randint(0, len(text) - max_seq_len - 1)\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print('----- diversity:', diversity)\n        generated = ''\n        sentence = text[start_index: start_index + max_seq_len]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x_pred = np.zeros((1, max_seq_len, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n\ngenerate_text()","4bc39bbd":"## Train Model","389ac2da":"## Preprocess Dataset","9ce4f0db":"# Text Generation LSTM","0609e960":"## Load Dataset","3aed2bb2":"## Generate Text","408814c7":"## Build Model"}}