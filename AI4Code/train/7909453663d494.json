{"cell_type":{"ba27c647":"code","a03c2c4c":"code","25cbf4f1":"code","1967e615":"code","99bd6531":"code","c9be9b78":"code","d34c6e91":"code","a2217948":"code","a7dd6ab1":"code","87c732b2":"code","fca8264f":"code","a2af884c":"code","04cf18db":"code","1fbec853":"code","94010fdf":"markdown","cd5830d2":"markdown","e72b1665":"markdown","c65ab9c8":"markdown","aba9cc1f":"markdown","d71b052f":"markdown","38caf2c9":"markdown","9b856b7e":"markdown","bdb2bdef":"markdown","710f32dd":"markdown","a2b072c2":"markdown","20122c66":"markdown","fe1acf9f":"markdown","5f1fccbe":"markdown","a128d382":"markdown","e7697269":"markdown","c4c41b24":"markdown","d4ddbdff":"markdown","309c9e4c":"markdown"},"source":{"ba27c647":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","a03c2c4c":"(xtrain, ytrain), (xtest, ytest) = keras.datasets.cifar10.load_data()\nxtrain = xtrain.reshape(-1, 100, 32, 32, 3).astype(\"float32\") \/ 255.0\nxtest = xtest.reshape(-1, 32, 32, 3).astype(\"float32\") \/ 255.0\nytrain = tf.one_hot(ytrain, 10).numpy().reshape(-1, 100, 10)\nytest = tf.one_hot(ytest, 10).numpy().reshape(-1, 10)","25cbf4f1":"classifier = keras.Sequential([\n    layers.InputLayer(input_shape=[32,32,3]),\n    layers.Conv2D(32, 3),\n    layers.Conv2D(32, 3),\n    layers.LeakyReLU(0.1),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.3),\n\n    layers.Conv2D(64, 3),\n    layers.Conv2D(64, 3),\n    layers.LeakyReLU(0.1),\n    layers.Dropout(0.3),\n\n    layers.Conv2D(128, 3),\n    layers.LeakyReLU(0.1),\n    layers.Dropout(0.3),\n\n    layers.Conv2D(256, 3),\n    layers.LeakyReLU(0.1),\n    layers.Dropout(0.3),\n\n    layers.Flatten(),\n    layers.Dense(512),\n    layers.LeakyReLU(0.2),\n    layers.Dropout(0.3),\n    layers.Dense(10, activation='softmax')\n])\n        \nclassifier.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy']) \nclassifier.summary()\n","1967e615":"masker = keras.Sequential([\n    layers.InputLayer(input_shape=(32,32,3)),\n    layers.Conv2D(32, 3, strides=1, padding='same'),\n    layers.LeakyReLU(0.1),\n    layers.Conv2D(32, 3, strides=1, padding='same'),\n    layers.LeakyReLU(0.1),\n    layers.Conv2D(32, 3, strides=1, padding='same'),\n    layers.LeakyReLU(0.1),\n    layers.Conv2D(32, 3, strides=1, padding='same'),\n    layers.LeakyReLU(0.1),\n    layers.Conv2D(1, 3, strides=1, padding='same', activation='sigmoid') \n    # Since we want outputs of 1 or 0, we use the sigmoid activation\n])\n\nmasker.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy']) \nmasker.summary()","99bd6531":"# This function technically is not differentiable at x=0.5, \n# but has a derivative of 0 at all other points, so we're using a custom gradient\n@tf.custom_gradient         \ndef binary_round(x):\n    binary_result = tf.math.greater(x, tf.constant(0.5, dtype=tf.float32))\n    binary_result = tf.cast(binary_result, tf.float32)\n    \n    # custom gradient\n    def grad(upstream): \n        return upstream * 0\n    \n    return binary_result, grad","c9be9b78":"def mask(indices, inputs):\n    indices = tf.repeat(binary_round(indices), 3, axis=-1)\n    return tf.math.multiply(indices, inputs)","d34c6e91":"batch_size = 100","a2217948":"# Optimizers for classifier and masker\nclassifier_opt = keras.optimizers.Adam(learning_rate=0.001)#, momentum=0.9)\nmasker_opt = keras.optimizers.Adam(learning_rate=0.001)#, momentum=0.9)\n\n# Metric for classifier\nclassifier_metric_train = keras.metrics.CategoricalAccuracy()\nclassifier_metric_test = keras.metrics.CategoricalAccuracy()\n\n# Loss function for classifier\nclassifier_loss_fn = keras.losses.CategoricalCrossentropy()\n\n# Loss function for masker\nBCE = keras.losses.BinaryCrossentropy()\ndef masker_loss_fn(masked_input, classifier_loss, alpha, zeros=tf.zeros([batch_size,32,32,1])):\n    masker_loss = BCE(zeros, masked_input)\n    masker_loss = tf.math.add(classifier_loss, tf.math.multiply(alpha, masker_loss))\n    return masker_loss","a7dd6ab1":"epochs = 10\nbatches = xtrain.shape[0]\nalpha = 0.1 # this is our alpha to control the strength of the BCE term\n\n# We will record the classifier and masker's losses and classifier accuracy\nclassifier_trainloss_hist = []\nclassifier_testloss_hist = []\nmasker_trainloss_hist = []\nmasker_testloss_hist = []\nclassifier_acc_train = []\nclassifier_acc_test = []\n\nfor epoch in range(epochs):\n    print(f'Epoch {epoch+1}')\n    for xbatch, ybatch in tqdm(zip(xtrain, ytrain), total=batches): # tqdm lets us make a progress bar\n        \n        with tf.GradientTape() as classifier_tape, tf.GradientTape() as masker_tape:\n            masked_input = masker(xbatch) # Mask the inputs\n            masked_input_round = mask(masked_input, xbatch) # Round the masked inputs\n            classifier_pred = classifier(masked_input_round) # Run classifier on masked inputs\n            \n            # Get losses of classifier first and then masker next\n            classifier_loss = classifier_loss_fn(ybatch, classifier_pred)\n            masker_loss = masker_loss_fn(masked_input_round, classifier_loss, alpha)\n        \n        # Backprop\n        classifier_grad = classifier_tape.gradient(classifier_loss, classifier.trainable_weights)\n        classifier_opt.apply_gradients(zip(classifier_grad, classifier.trainable_weights))\n        \n        masker_grad = masker_tape.gradient(masker_loss, masker.trainable_weights)\n        masker_opt.apply_gradients(zip(masker_grad, masker.trainable_weights))\n        \n    # Append Classifier Losses\n    eval_cut = 10000 # I'm slicing the training set to a smaller size because I get memory errors\n    eval_xtrain = xtrain.reshape([-1,32,32,3])[:eval_cut]\n    eval_ytrain = ytrain.reshape([-1,10])[:eval_cut]\n    \n    masked_train_input = mask(masker(eval_xtrain), eval_xtrain) # Training Loss\n    train_loss_acc = classifier.evaluate(masked_train_input, eval_ytrain, verbose=False) # Training Loss\n    classifier_trainloss = train_loss_acc[0] # Training Loss\n    classifier_trainloss_hist.append(classifier_trainloss) # Training Loss\n    \n    classifier_testpred = classifier(mask(masker(xtest), xtest)) # Test Loss\n    classifier_testloss = classifier_loss_fn(ytest, classifier_testpred) # Test Loss\n    classifier_testloss_hist.append(classifier_testloss) # Test Loss\n    \n    # Append Masker Losses\n    zeros = tf.zeros([eval_xtrain.shape[0],32,32,1]) # Training Loss\n    masker_trainloss = masker_loss_fn(masked_train_input, classifier_trainloss, alpha, zeros) # Training Loss\n    masker_trainloss_hist.append(masker_loss) # Training Loss\n    \n    masked_test_input =  mask(masker(xtest), xtest) # Test Loss\n    zeros = tf.zeros([xtest.shape[0],32,32,1]) # Test Loss\n    masker_testloss = masker_loss_fn(masked_test_input, classifier_testloss, alpha, zeros) # Test Loss\n    masker_testloss_hist.append(masker_testloss) # Test Loss\n        \n    # Compute classifier accuracy\n\n    classifier_metric_test.update_state(ytest, classifier_testpred)\n    classifier_train_acc = train_loss_acc[1]\n    classifier_test_acc = classifier_metric_test.result()\n    print(f'Classifier training accuracy: {classifier_train_acc}')\n    print(f'Classifier test accuracy: {classifier_test_acc}')\n    classifier_acc_train.append(classifier_train_acc)\n    classifier_acc_test.append(classifier_test_acc)","87c732b2":"plt.plot(classifier_trainloss_hist)\nplt.plot(classifier_testloss_hist)","fca8264f":"plt.plot(classifier_acc_train)\nplt.plot(classifier_acc_test)","a2af884c":"print(f'The model acheived a maximmum accuracy of {round(max(classifier_acc_test).numpy()*100, 2)}%')","04cf18db":"images = 10\nindex = 15\n\nplt.figure(figsize=(20,4))\nfor i in range(images):\n    ax = plt.subplot(2, images, i+1+images)\n    image = xtrain[i+index,0].reshape([-1,32,32,3])\n    m = mask(masker(image), image)[0]\n    plt.imshow(m)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    ax = plt.subplot(2, images, i+1)\n    plt.imshow(image[0])\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)","1fbec853":"masker.save_weights('masker.h5')","94010fdf":"## Build Classifier","cd5830d2":"## Masker Model","e72b1665":"## Load CIFAR-10","c65ab9c8":"We will be training the CNN (which we will call classifier) on CIFAR-10 images, but the input to this classifier will be a modified image. This modified image has portions cut out from it, or \"masked\", which is determined by another neural network. We will call this neural network \"masker\", since it masks out portions of the original image. The masker will aim to delete as many as possible portions of the input image whilst having the classifier maintain a high accuracy.\n\n**How does the masking process work?**\n\n1. The masker takes the input image, and determines which pixels (indices in an array) will be removed. If the pixel (all three RGB channels as a whole) is to be removed, it will output a value of 0, otherwise, it will output 1. It is like turning a pixel on (1) or off (0). This will be done to all the pixels in the image, so the output of the masker is the same size (or dimensions\/shape) as the input.\n2. The output from the masker is then multiplied element-wise to the input image. This results in portions of the input image being cut out.\n\n![image.png](attachment:7c954a94-726b-4293-9103-d1409f181155.png)","aba9cc1f":"We will build a simple convolutional neural network to classify CIFAR-10 images.","d71b052f":"Here we will formulate the loss function for the masker. We want it to delete as many as possible portions or pixels from the image, so an optimum output would be an array (same dimensions as the input image) of all zeros (all \"off\" values). We can use binary crossentropy (BCE) for this. \n\n$masker loss = BCE(zero array, masker output)$\n\nBut of course this would result in the masker outputting an array (image) of all zeros, or a black image, causing the classifier have a high loss, essentially just guessing its prediction. Therefore we want to add to this loss function the loss of the classifier, so that the masker learns to delete as many as possible portions, while minimizing the classifier loss.\n\n$masker loss = BCE(zero array, masker output) + classifier loss$\n\nWe can control the strength of the $BCE$ term by multiplying a constant $\\alpha$ to it:\n\n$masker loss = \\alpha [BCE(zero array, masker output)] + classifier loss$","38caf2c9":"**Function to round inputs to 0 or 1**\n\nBecuase the masker model outputs a values between 0 to 1 but never exactly 0 or 1, we want a function to round the masker output to become 0 or 1 (1 if x > 0.5, 0 otherwise). The function looks like this:\n\n![image.png](attachment:0a7c1fa6-186d-4297-8640-331302238e96.png)","9b856b7e":"## Mask Function","bdb2bdef":"This element-wise multiplies the indices containing values 0 or 1 with the input image","710f32dd":"We can see that despite portions of the image removed, the model achieves a 45% accuracy.","a2b072c2":"**Mask Function**","20122c66":"**Custom Training Loop**","fe1acf9f":"## Plot images and masked images","5f1fccbe":"Let's see the classifier losses and accuracy:","a128d382":"## Training","e7697269":"## The process:","c4c41b24":"It's quite interesting to see that the model achieves quite a higher accuracy than you'd expect on masked images like these. Looking at the masked images, it seems that the left and bottom borders of the image are mainly not masked. \n\nThis phenomenon is called \"overinterpretation\", which is actually a problem in machine learning.\n\nTo learn more, check out MIT's take on this: [Nonsense can make sense to machine-learning models](https:\/\/news.mit.edu\/2021\/nonsense-can-make-sense-machine-learning-models-1215)\n\n","d4ddbdff":"## Importing required libraries","309c9e4c":"**Neural networks have shown to be very robust in numerous tasks, but to what extent? To test this, I will be making a convolutional neural network to classify CIFAR-10 images, but portions of the image keep disappearing each timestep in training. Can the neural network keep up and still perform well?**"}}