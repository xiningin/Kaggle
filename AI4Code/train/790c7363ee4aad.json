{"cell_type":{"4547fc40":"code","93a9ab3e":"code","553efcf7":"code","cc2e1a1a":"code","d212d789":"code","36192913":"code","f398cdca":"code","93e0ddbd":"code","635fc546":"code","123deadb":"code","04c2714a":"code","1aa5cd51":"code","fe4ebdd7":"code","26be9f82":"code","d6bacad1":"code","70e78f7b":"code","51bef3d2":"code","c2f084bb":"code","b1d2a510":"code","5ce8de4f":"code","8ad18cba":"code","b9772675":"code","d005bbdf":"code","05d01a6f":"code","972195ab":"code","3e584d76":"code","54a35182":"code","b81c1726":"markdown","ed7967fd":"markdown","4b125f29":"markdown","275240c6":"markdown","00ecda51":"markdown","3928b48e":"markdown","360a59f5":"markdown","50457ad7":"markdown","e86beb1e":"markdown","4f8a384c":"markdown","9ba0404b":"markdown"},"source":{"4547fc40":"import os\nimport numpy as np\nimport pandas as pd\nimport sklearn as skl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","93a9ab3e":"base_path = '\/kaggle\/input\/tabular-playground-series-apr-2021\/'\ntrain_path = os.path.join(base_path, 'train.csv')\ntest_path = os.path.join(base_path, 'test.csv')\nsample_submission_path = os.path.join(base_path, 'sample_submission.csv')\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\nsample_submission = pd.read_csv(sample_submission_path)","553efcf7":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","cc2e1a1a":"train.head()","d212d789":"train.describe()","36192913":"train.info()","f398cdca":"cols_to_drop = ['Name', 'PassengerId']\n\ntrain.drop(cols_to_drop, axis=1, inplace=True)\n\n# keep ids for submission \ntest_indexes = test['PassengerId']\ntest.drop(cols_to_drop, axis=1, inplace=True)","93e0ddbd":"target_col = 'Survived'\n\ncategorical_cols = list(train.loc[:,train.dtypes == \"object\"].columns)\nnumerical_cols = list(train.loc[:,train.dtypes != \"object\"].columns)\n\nnumerical_cols.remove(target_col)","635fc546":"train.Cabin.fillna('N', inplace=True)\ntest.Cabin.fillna('N', inplace=True)\n\ntrain.Cabin = train.Cabin.map(lambda x: x[0])\ntest.Cabin = test.Cabin.map(lambda x: x[0])\n\n\ntrain.Ticket = train.Ticket.fillna('N').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'N')\ntest.Ticket = test.Ticket.fillna('N').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'N')","123deadb":"train.shape","04c2714a":"axes = []\ncols = list(categorical_cols)\ncols.extend(['Pclass', 'SibSp', 'Parch'])\ncols.remove('Ticket')\nprint(cols)\nnum_of_cols = len(cols)\nfig = plt.figure(figsize=(12, 12 * num_of_cols))\n\nfor i in range(num_of_cols):    \n    plot = train[cols[i]].value_counts()\n  \n    ax = fig.add_subplot(num_of_cols, 1, i + 1)\n    axes.append(ax)\n    \n    sns.countplot(data=train, x=cols[i], hue=target_col, ax=ax)\n    ax.legend()","1aa5cd51":"def plot_face_grid(x):\n    g = sns.FacetGrid(train, col=target_col, height=6)\n    g.map(sns.kdeplot, x, shade=True).add_legend()","fe4ebdd7":"plot_face_grid('Age')","26be9f82":"plot_face_grid('Fare')","d6bacad1":"# Apply Chi-Squared test on the next verison of the notebook","70e78f7b":"def remove_target(data, target):\n    ret = data[target]\n    data.drop([target], axis=1, inplace=True)\n    return ret","51bef3d2":"from sklearn.preprocessing import LabelEncoder\nclass ModifiedLabelEncoder(LabelEncoder):\n\n    def fit_transform(self, y, *args, **kwargs):\n        return super().fit_transform(y).reshape(-1, 1)\n\n    def transform(self, y, *args, **kwargs):\n        return super().transform(y).reshape(-1, 1)","c2f084bb":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\ntrain_y = remove_target(train, target_col)\n\n\ncategorical_cols.remove('Ticket')\nlabel_cols = ['Ticket']\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('std_scaler', StandardScaler())\n    ])\n\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),#, fill_value='None')),\n        ('one_hot', OneHotEncoder(handle_unknown='ignore'))\n    ])\n\nlab_pipeline = Pipeline([\n    ('label_encoder', ModifiedLabelEncoder())\n])\n\nfull_pipeline = ColumnTransformer([\n        ('num', num_pipeline, numerical_cols),\n        ('cat', cat_pipeline, categorical_cols),\n        ('lab', lab_pipeline, label_cols)\n    ])\n    \n\ntrain = full_pipeline.fit_transform(train, train_y)\ntest = full_pipeline.fit_transform(test)","b1d2a510":"train.shape","5ce8de4f":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom lightgbm.sklearn import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_validate\n\n\nfrom scipy.stats import uniform\nfrom random import randint\n\nnp.random.seed(123)\n\nmodels = [\n    (CatBoostClassifier(verbose=False), 'CatBoost'),\n    (AdaBoostClassifier(), 'AdaBoost'),\n    (RandomForestClassifier(), 'RandomForest'),\n    (GaussianNB(), 'NB'),\n    (ExtraTreesClassifier(), 'ExtraTreesClassifier'),\n    (LogisticRegression(max_iter=600), 'LogisticRegression'),\n    (KNeighborsClassifier(), 'KNeighbors'),\n    (XGBClassifier(use_label_encoder=False), 'XGB'),\n    (LGBMClassifier(), 'LGBM')\n]","8ad18cba":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n\ndef train_and_evaluate(model, x, y):\n    model, name = model\n    results = cross_validate(model, x, y)\n    results = results['test_score']\n    print(name)\n    print(results)\n    print('mean: ', results.mean())\n    print('std: ', results.std(), '\\n')","b9772675":"for model in models:\n    train_and_evaluate(model, train, train_y)","d005bbdf":"parameters = {\n    \"n_estimators\": list(range(20, 500)),\n    \"learning_rate\": uniform(0.001, 0.199),\n    \"max_depth\": [-1, 2, 4, 8, 16],\n    \"min_data_per_group\": list(range(2,1000)),\n    \"num_leaves\": list(range(2, 200)),\n    \"bagging_freq\": list(range(1, 8)),\n    \"max_bin\": list(range(2, 200)),\n    \"lambda_l1\": uniform(0.001, 0.199),\n    \"lambda_l2\": uniform(0.001, 0.199),\n    \"feature_fraction\" : [i\/100 if i\/100 < 1 else 1. for i in range(71,120)],\n    \"bagging_fraction\" : uniform(0.01, 0.99)\n    } \n\nmodel = LGBMClassifier(extra_trees=True, verbose=-1)\nrandomsearch = RandomizedSearchCV(model,\n                                  param_distributions=parameters,\n                                  n_iter=25,\n                                  cv=4,\n                                  random_state=1,\n                                  scoring='accuracy',\n                                  return_train_score=True,\n                                  refit='Accuracy',\n                                  )\n\nr = randomsearch.fit(train,train_y)\nscores = r.cv_results_\nbest = r.best_estimator_\n","05d01a6f":"# print all models tried with the parameters used and gained performace\nfor mean_score, params in sorted(list(zip(scores[\"mean_test_score\"], scores[\"params\"])), key = lambda x: x[0]):\n    print(mean_score, params)","972195ab":"result = best.predict(test)","3e584d76":"df = pd.DataFrame(zip(test_indexes, result))\ndf.columns = ['PassengerId', 'Survived']\ndf.set_index('PassengerId', inplace=True)\ndf.head()","54a35182":"df.to_csv('submission.csv')","b81c1726":"## Summary\n\n### I hope you liked this notebook. If you have any suggestions, please share them in the comments.\n\n### List of the todo things to improve final accuracy:\n- Try pseudo labels. This works really well on this dataset. It may increase your final accuracy by > 1% (You first train the initial model to predict labels for the test set and later on you use a combined test and training set for training the final model) \n- Try Stacking. (Train many different models and use the probabilities returned by them as an input to the final decision model, read more here https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html.\n- Build the NN.\n- Experiment with feature engineering. ","ed7967fd":"### As on the previous chart, data seems to be a bit skew to the right","4b125f29":"# Modle selection","275240c6":"### As we can see, catboost and LGBM performs the best. We will focus on finding best parameters for the second one. If you want to get better result you may increase n_iters to try bigger amount of combinations.","00ecda51":"### As expected sex seems to have a big impact. Embarked, Cabin and Pclass also give some valueble insight. Looking into SibSp and Parth columns it is hard to evaluate because of not equaly distributed labels(~45% survived and ~55% not survived)\n### Now lets inspect numerical values with respect to the target value","3928b48e":"### The best result I got has accuracy slightly above 78.2%. Fortunately, because of cross-validation, we did not overfit and it gives us almost 80% for the submission\n","360a59f5":"### Let's plot categorical and descrete values data with respect to Survived","50457ad7":"### After initial data inspection it seems like Name, PassengerId columns won't be needed. Cabin and Ticket columns need to be transformed to bu useful.","e86beb1e":"### Now let's prepare data for the modeling\n### You may experiment with different strategies for SimpleImputer and with Scalers","4f8a384c":"<img src=\"https:\/\/faithmag.com\/sites\/default\/files\/styles\/article_full\/public\/2018-09\/titanic2.jpg?h=6521bd5e&itok=H8td6QVv\" alt=\"drawing\" height=\"600\" width=\"600\"\/>\n\n# **Titanic dataset is one of the best known for those who start their journey with ML.**\n### In this notebook, I will show you an easy approach to obtain nearly 80%. This score may be easily improved by a couple of methods which I will mention at the end. The purpose of this work is not to find the best possible solution for this problem but a simple one that performs decently.\n\n## Please upvote my work if you find it helpful. Happy reading :)","9ba0404b":"### It seems like older people were more likely to survive compering to young adults"}}