{"cell_type":{"779559bb":"code","7f1b1987":"code","09644fb8":"code","5b33a5fc":"code","74072678":"code","aef845d3":"code","63236677":"code","8f4cc34c":"code","23556b44":"code","43f4e9f1":"code","2a4ea783":"code","e1ff0c35":"code","f2819545":"code","e99f11c1":"code","e312bfc7":"code","6a620704":"code","34f1d883":"code","d4599d1c":"markdown","119d17cf":"markdown","ab4acd61":"markdown","c5cc45a6":"markdown","2a061f6a":"markdown","62f556cd":"markdown","7af06ca4":"markdown","6aa47fca":"markdown","f622d543":"markdown","cf9141d9":"markdown","99ef88f7":"markdown","16158fa7":"markdown","04e6ad94":"markdown","5a406931":"markdown","ba101130":"markdown","029d601e":"markdown","72316894":"markdown","2ff99302":"markdown"},"source":{"779559bb":"import os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nimport plotly.offline\n#import cufflinks as cf\n#cf.go_offline()\n#cf.set_config_file(offline=False, world_readable=True)\n\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","7f1b1987":"!ls ..\/input\/","09644fb8":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \", train_df.shape)\nprint(\"Test shape : \", test_df.shape)","5b33a5fc":"train_df.head()","74072678":"train_df[\"target2\"]=(train_df[\"target\"]>0.5)","aef845d3":"target1=[]\nfor i,row in train_df.iterrows():\n    if row[\"target2\"]==False:\n        target1.append(0)\n    else:\n        target1.append(1)  ","63236677":"train_df[\"final_target\"]=target1","8f4cc34c":"import re\ncorpus = []\nfor i in range(0, train_df.shape[0]):\n    review = re.sub('[.]', '', train_df['comment_text'][i])\n    corpus.append(review)","23556b44":"train_df['comment_text']=corpus","43f4e9f1":"## Taken from SRKs Kernel\n## target count ##\ncnt_srs = train_df['final_target'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","2a4ea783":"## Taken from SRKs Kernel\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"comment_text\"], title=\"Word Cloud of Comments\")","e1ff0c35":"## Taken from SRKs Kernel\nfrom collections import defaultdict\ntrain1_df = train_df[train_df[\"final_target\"]==1]\ntrain0_df = train_df[train_df[\"final_target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from non-toxic comments ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from toxic comments ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of non-toxic comments\", \n                                          \"Frequent words of toxic comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()","f2819545":"## Taken from SRKs Kernel\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of non-toxic comments\", \n                                          \"Frequent bigrams of toxic comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","e99f11c1":"## Taken from SRKs Kernel\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"comment_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"comment_text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of non-toxic comments\", \n                                          \"Frequent trigrams of toxic comments\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","e312bfc7":"## Taken from SRKs Kernel\n## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"comment_text\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"comment_text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","6a620704":"## Taken from SRKs Kernel\n## Truncate some extreme values for better visuals ##\ntrain_df['num_words'].loc[train_df['num_words']>60] = 60 #truncation for better visuals\ntrain_df['num_punctuations'].loc[train_df['num_punctuations']>10] = 10 #truncation for better visuals\ntrain_df['num_chars'].loc[train_df['num_chars']>350] = 350 #truncation for better visuals\n\nf, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='final_target', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='final_target', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='final_target', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\n#plt.ylabel('Number of punctuations in text', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","34f1d883":"plt.figure(figsize=(8, 6))\nsns.heatmap(train_df.select_dtypes(include=['float64']).corr(),cmap=\"YlGnBu\")","d4599d1c":"**Data files provided:**","119d17cf":"**Correlation Matrix**","ab4acd61":"**As usual, the data is imbalanced**","c5cc45a6":"**Setting a threshold of 0.5 according to competition rules and changing the target to binary**","2a061f6a":"**Observation:**\nHigh correlation of the target with \"insult\".","62f556cd":"**Inference:**\n1. Text meta features don't say much about the toxicity of a comment","7af06ca4":"**Trigram Frequency plot of Comments**","6aa47fca":"**Observations:**\n1. Most of the top bigrams are similar among both classes like \"donald trump\" and \"united states\"\n2. Top bigrams for non-toxic comments are \"health care\" and \"many people\"\n3. Top bigrams for toxic comments are \"white people\" and \"white house\"","f622d543":"**Observations:**\n1. Most of the top words are similar among both classes like \"people\",\"will\",\"one\" and \"trump\"\n2. Top words for non-toxic comments are \"many\" and \"good\"\n3. Top words for toxic comments are \"go\" and \"want\"","cf9141d9":"**Word Frequency plot of Comments**","99ef88f7":"**Word Cloud**","16158fa7":"**Importing libraries**","04e6ad94":"**Observations:**\n1. Some of the top trigrams are similar among both classes like \"ha ha ha\", \"president united states\"\n2. Top trigrams for non-toxic comments are \"health care system\" and \"oil and gas\"\n3. Top trigrams for toxic comments are \"black lives matter\" and \"make america great\"\n\n**Also \"...\" indicates people just love to not complete sentences.**","5a406931":"**Some meta features:**\n1. Number of words in the text\n2. Number of unique words in the text\n3. Number of characters in the text\n4. Number of stopwords\n5. Number of punctuations\n6. Number of upper case words\n7. Number of title case words\n8. Average length of the words","ba101130":"**Removing \".\" as they were appearing in the most frequently occuring words**","029d601e":"**Bigram Frequency plot of Comments**","72316894":"**Future Work:**\n1. Plan to add visualizations for additional toxicity subtype attributes\n2. Plan to add build a simple model","2ff99302":"**About my notebook:**\n* The main objective of the notebook is to explore the data.\n* The notebook is greatly inspired from SRK's kernel on QIQC (https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc)(Most code snippets for visualizations have been directly copied from his kernel and thus individual reference hasn't been mentioned)\n\n**About the competition:**\n* The main aim of the competition is to detect toxic comments and minimize unintended model bias"}}