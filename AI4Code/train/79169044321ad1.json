{"cell_type":{"d5e30dcd":"code","188672d8":"code","3d54ebc8":"code","7d676aaf":"code","9ab6a101":"code","89d7c9ca":"code","31df7d3c":"code","ff8c1168":"code","33b37393":"code","a5b1bc98":"code","ca0e4307":"code","01ca6a8f":"code","79233540":"code","eace35cf":"code","d7008567":"code","f2ad0ae9":"code","da855ac7":"code","6912d8cc":"code","d9f547fa":"code","815c7d34":"code","52a9c820":"code","df020020":"code","3466799e":"code","f91f597b":"code","ff6a7016":"code","f3afe548":"markdown","354b576d":"markdown","cb60387e":"markdown","72a80789":"markdown","4d106cd4":"markdown","b2cbc512":"markdown"},"source":{"d5e30dcd":"import glob\nimport os \nimport string\nfrom pathlib import Path\n\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom torchvision.models import resnet18\nimport matplotlib.pyplot as plt\nimport collections\n\nfrom IPython.display import clear_output\n%matplotlib inline\nplt.style.use('seaborn')","188672d8":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(0)","3d54ebc8":"def plot_loss(epoch: int, \n              train_losses: list, \n              val_losses: list, \n              n_steps: int = 100):\n    \n    # clear previous graph\n    clear_output(True)\n    # making titles\n    train_title = f'Epoch:{epoch} | Train Loss:{np.mean(train_losses[-n_steps:]):.6f}'\n    val_title = f'Epoch:{epoch} | Val Loss:{np.mean(val_losses[-n_steps:]):.6f}'\n\n    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n    ax[0].plot(train_losses)\n    ax[1].plot(val_losses)\n\n    ax[0].set_title(train_title)\n    ax[1].set_title(val_title)\n\n    plt.show()\n\ndef print_prediction(model, dataset, device, label_converter):\n    idx = np.random.randint(len(dataset))\n    path = dataset.pathes[idx]\n    \n    with torch.no_grad():\n        model.eval()\n        img, target_text = dataset[idx]\n        img = img.unsqueeze(0)\n        logits = model(img.to(device))\n        \n    pred_text = decode_prediction(logits.cpu(), label_converter)\n\n    img = np.asarray(Image.open(path).convert('L'))\n    title = f'Truth: {target_text} | Pred: {pred_text}'\n    plt.imshow(img)\n    plt.title(title)\n    plt.axis('off');","7d676aaf":"class strLabelConverter(object):\n\n    def __init__(self, alphabet: str, ignore_case: bool = True):\n        self._ignore_case = ignore_case\n        if self._ignore_case:\n            alphabet = alphabet.lower()\n        self.alphabet = alphabet + '-'  # for `-1` index\n\n        self.char2idx = {}\n        for i, char in enumerate(alphabet):\n            self.char2idx[char] = i + 1\n        self.idx2char = {idx: char for char, idx in self.char2idx.items()}\n\n    def encode(self, text):\n        if isinstance(text, str):\n            text = [\n                self.char2idx[char.lower() if self._ignore_case else char]\n                for char in text\n            ]\n            length = [len(text)]\n        elif isinstance(text, collections.Iterable):\n            length = [len(s) for s in text]\n            text = ''.join(text)\n            text, _ = self.encode(text)\n        return (torch.IntTensor(text), torch.IntTensor(length))\n\n    def decode(self, t, length, raw=False):\n        if length.numel() == 1:\n            length = length[0]\n            assert t.numel() == length, \"text with length: {} does not match declared length: {}\".format(t.numel(), length)\n            if raw:\n                return ''.join([self.alphabet[i - 1] for i in t])\n            else:\n                char_list = []\n                for i in range(length):\n                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n                        char_list.append(self.alphabet[t[i] - 1])\n                return ''.join(char_list)\n        else:\n            # batch mode\n            assert t.numel() == length.sum(), \"texts with length: {} does not match declared length: {}\".format(t.numel(), length.sum())\n            texts = []\n            index = 0\n            for i in range(length.numel()):\n                l = length[i]\n                texts.append(\n                    self.decode(\n                        t[index:index + l], torch.IntTensor([l]), raw=raw))\n                index += l\n        return texts\n\n\ndef decode_prediction(logits: torch.Tensor, \n                      label_converter: strLabelConverter) -> str:\n    tokens = logits.softmax(2).argmax(2)\n    tokens = tokens.squeeze(1).numpy()\n    \n    # convert tor stings tokens\n    tokens = ''.join([label_converter.idx2char[token] \n                      if token != 0  else '-' \n                      for token in tokens])\n    tokens = tokens.split('-')\n    \n    # remove duplicates\n    text = [char \n            for batch_token in tokens \n            for idx, char in enumerate(batch_token)\n            if char != batch_token[idx-1] or len(batch_token) == 1]\n    text = ''.join(text)\n    return text","9ab6a101":"class blockCNN(nn.Module):\n    def __init__(self, in_nc, out_nc, kernel_size, padding, stride=1):\n        super(blockCNN, self).__init__()\n        self.in_nc = in_nc\n        self.out_nc = out_nc\n        self.kernel_size = kernel_size\n        self.padding = padding\n        # layers\n        self.conv = nn.Conv2d(in_nc, out_nc, \n                              kernel_size=kernel_size, \n                              stride=stride, \n                              padding=padding)\n        self.bn = nn.BatchNorm2d(out_nc)\n        \n    def forward(self, batch, use_bn=False, use_relu=False, \n                use_maxpool=False, maxpool_kernelsize=None):\n        batch = self.conv(batch)\n        if use_bn:\n            batch = self.bn(batch)\n        if use_relu:\n            batch = F.relu(batch)\n        if use_maxpool:\n            assert maxpool_kernelsize is not None\n            batch = F.max_pool2d(batch, kernel_size=maxpool_kernelsize, stride=2)\n        return batch\n\nclass blockRNN(nn.Module):\n    def __init__(self, in_size, hidden_size, out_size, bidirectional, dropout=0):\n        super(blockRNN, self).__init__()\n        self.in_size = in_size\n        self.hidden_size = hidden_size\n        self.out_size = out_size\n        self.bidirectional = bidirectional\n        # layers\n        self.gru = nn.GRU(in_size, hidden_size, bidirectional=bidirectional)\n        \n    def forward(self, batch, add_output=False):\n        batch_size = batch.size(1)\n        outputs, hidden = self.gru(batch)\n        out_size = int(outputs.size(2) \/ 2)\n        if add_output:\n            outputs = outputs[:, :, :out_size] + outputs[:, :, out_size:]\n        return outputs","89d7c9ca":"class CaptchaDataset(Dataset):\n    def __init__(self, img_dir: str):\n        pathes = os.listdir(img_dir)\n        abspath = os.path.abspath(img_dir)\n        self.img_dir = img_dir\n        self.pathes = [os.path.join(abspath, path) for path in pathes]\n        self.list_transforms = transforms.Compose([transforms.ToTensor()])\n        \n    def __len__(self):\n        return len(self.pathes)\n    \n    def __getitem__(self, idx):\n        path = self.pathes[idx]\n        text = self.get_filename(path)\n        img = Image.open(path).convert('RGB')\n        img = self.transform(img)\n        return img, text\n    \n    def get_filename(self, path: str) -> str:\n        return os.path.basename(path).split('.')[0].lower().strip()\n    \n    def transform(self, img) -> torch.Tensor:\n        return self.list_transforms(img)","31df7d3c":"TRAIN_DIR = Path('\/kaggle\/input\/captcha-data\/data\/train')\nVAL_DIR = Path('\/kaggle\/input\/captcha-data\/data\/val')\nBATCH_SIZE = 8\nN_WORKERS = 4","ff8c1168":"alphabet = string.ascii_lowercase + string.digits\nlabel_converter = strLabelConverter(alphabet)\nalphabet","33b37393":"train_dataset = CaptchaDataset(TRAIN_DIR)\nval_dataset = CaptchaDataset(VAL_DIR)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n                              num_workers=N_WORKERS, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, \n                            num_workers=N_WORKERS, shuffle=False)","a5b1bc98":"imgs, texts = iter(val_dataloader).next()\nprint(imgs.shape, len(texts))","ca0e4307":"def weights_init(m):\n    classname = m.__class__.__name__\n    if type(m) in [nn.Linear, nn.Conv2d, nn.Conv1d]:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","01ca6a8f":"class CRNN(nn.Module):\n    def __init__(self, hidden_size: int, \n                 vocab_size: int, \n                 bidirectional: bool = True, \n                 dropout: float = 0.5):\n        super(CRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.bidirectional = bidirectional\n        # make layers\n        # convolutions \n        resnet = resnet18(pretrained=True)\n        modules = list(resnet.children())[:-3]\n        self.resnet = nn.Sequential(*modules)\n        #PRETRAINED^\n        \n        \n        \n        self.cn6 = blockCNN(256, 256, kernel_size=3, padding=1)\n        # RNN + Linear\n        self.linear1 = nn.Linear(1024, 256)\n        self.gru1 = blockRNN(256, hidden_size, hidden_size,\n                             dropout=dropout, \n                             bidirectional=bidirectional)\n        self.gru2 = blockRNN(hidden_size, hidden_size, vocab_size,\n                             dropout=dropout,\n                             bidirectional=bidirectional)\n        self.linear2 = nn.Linear(hidden_size * 2, vocab_size)\n        \n    def forward(self, batch: torch.Tensor):\n        batch_size = batch.size(0)\n        # convolutions\n        batch = self.resnet(batch)\n        batch = self.cn6(batch, use_relu=True, use_bn=True)\n        # make sequences of image features\n        batch = batch.permute(0, 3, 1, 2)\n        n_channels = batch.size(1)\n        batch = batch.view(batch_size, n_channels, -1)\n        batch = self.linear1(batch)\n        # rnn layers\n        batch = self.gru1(batch, add_output=True)\n        batch = self.gru2(batch)\n        # output\n        batch = self.linear2(batch)\n        batch = batch.permute(1, 0, 2)\n        return batch","79233540":"hidden_size = 256\nvocab_size = len(alphabet) + 1 # extra character for blank symbol\nbidirectional = True\ndropout = 0.1\nweight_decay = 1e-5\nmomentum = 0.9\nclip_norm = 5\nmax_epoch = 50\nprint(vocab_size)","eace35cf":"crnn = CRNN(hidden_size=hidden_size, vocab_size=vocab_size, \n            bidirectional=bidirectional, dropout=dropout).to(device)\ncrnn(imgs.to(device)).shape","d7008567":"lr = 0.02\noptimizer = torch.optim.SGD(crnn.parameters(), lr=lr, nesterov=True, \n                            weight_decay=weight_decay, momentum=momentum)\ncritertion = nn.CTCLoss(blank=0)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, patience=5)","f2ad0ae9":"def validation(model, val_losses, label_converter):\n    with torch.no_grad():\n        model.eval()\n        for batch_img, batch_text in val_dataloader:\n            logits = crnn(batch_img.to(device))\n            val_loss = calculate_loss(logits, batch_text, label_converter)\n            val_losses.append(val_loss.item())\n    return val_losses","da855ac7":"def calculate_loss(logits, texts, label_converter):\n    # get infomation from prediction\n    device = logits.device\n    input_len, batch_size, vocab_size = logits.size()\n    # encode inputs\n    logits = logits.log_softmax(2)\n    encoded_texts, text_lens = label_converter.encode(texts)\n    logits_lens = torch.full(size=(batch_size,), fill_value=input_len, dtype=torch.int32)\n    # calculate ctc\n    loss = critertion(logits, encoded_texts, \n                      logits_lens.to(device), text_lens)\n    return loss","6912d8cc":"epoch = 0","d9f547fa":"train_losses = []\nval_losses = []\nval_epoch_len = len(val_dataset) \/\/ BATCH_SIZE\nval_epoch_len","815c7d34":"try:\n    while epoch <= max_epoch:\n        crnn.train()\n        for idx, (batch_imgs, batch_text) in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            logits = crnn(batch_imgs.to(device))\n            # calculate loss\n            train_loss = calculate_loss(logits, batch_text, label_converter)\n            if np.isnan(train_loss.detach().cpu().numpy()):\n                continue\n            train_losses.append(train_loss.item())\n            # make backward\n            train_loss.backward()\n\n            nn.utils.clip_grad_norm_(crnn.parameters(), clip_norm)\n            optimizer.step()\n\n        val_losses = validation(crnn, val_losses, label_converter)\n        \n        # printing progress\n        plot_loss(epoch, train_losses, val_losses)\n        print_prediction(crnn, val_dataset, device, label_converter)\n        \n        scheduler.step(val_losses[-1])\n        epoch += 1\nexcept KeyboardInterrupt:\n    pass","52a9c820":"# saving model\ntorch.save(crnn.state_dict(), '\/kaggle\/working\/crnn.pt')","df020020":"def acc_calc(model, dataset, label_converter) -> float:\n    acc = 0\n    with torch.no_grad():\n        model.eval()\n        for idx in range(len(dataset)):\n            img, text = dataset[idx]\n            logits = model(img.unsqueeze(0).to(device))\n            pred_text = decode_prediction(logits.cpu(), label_converter)\n            \n            if pred_text == text:\n                acc += 1\n            \n    return acc \/ len(dataset)","3466799e":"print_prediction(crnn, val_dataset, device, label_converter)","f91f597b":"val_acc = acc_calc(crnn, val_dataset, label_converter)\ntrain_acc = acc_calc(crnn, train_dataset, label_converter)\nprint('Validaton Accuracy: ', val_acc)\nprint('Training Accuracy: ', train_acc)","ff6a7016":"crnn","f3afe548":"## <center>  Dataset","354b576d":"## <center>  Model Parameters and Initialization","cb60387e":"## <center> Dataset Params","72a80789":"## <center> Model","4d106cd4":"## <center> Training","b2cbc512":"#### <center> Calculating Accuracy and printing prediction"}}