{"cell_type":{"0ea7769b":"code","a46cff7e":"code","b3398428":"code","62086c61":"code","147209fb":"code","a580096a":"code","51bd2311":"code","48ae9b24":"code","7df0fa51":"markdown","cc13cbd4":"markdown","6dda5666":"markdown","4100f7f3":"markdown","f16def01":"markdown","76b56b94":"markdown","109a754a":"markdown","b5c9082d":"markdown","b5a2a51d":"markdown"},"source":{"0ea7769b":"\"\"\"\nthis is what implements the restricted boltzmann machine it self with a twist\nwe talk about that specific recommender systems such as using contrastive divergence\nit may look a little complex and it is, I mean we're basicly making a brain here\nthat can keep track of hundreds of different people and guess what movies they might like \ngiven a hundred thousand ratings.\n\"\"\"\nimport numpy as np\nimport tensorflow as tf\n\nclass RBMs(object):\n    \n    def __init__(self, visibleDimensions, epochs=20, hiddenDimensions=50, ratingValues=10, learningRate=0.001, batchSize=100):\n        \"\"\"\n        our init function takes several different parameters, hyperparameteres if you will,\n        that give us control over how the learning RMB works.\n        It turns out these values are pretty important for how well it all works.\n        Visible dimensions is kind of non-negotiable. It's the product of the number of movies \n        and how many distinct rating values you have, as we discussed earlier.\n        In other words, a pretty big number. Epochs is how many iterations you're going to take \n        on the forward and backward passes while we try to minimize the error between\n        our actual rating values and the reconstructed predicted values.\n        That is, how many times will we train our RBM across all of the users in our training data\n        and hopes that it will converges weights and biases we can use for future predictions.\n        HiddenDimentions is the munber of hidden neurons in the system which can be surprisingly small,\n        comparing to number of visible neurons.\n        Next we have the number of distinc rating values since our data actualy contains half star\n        rating values, we have 10 possible rating values and not just 5.\n        We then have the learning rate which controls how quickly we attemp to converge on each iteration\n        this need to be balanced so you find the right values in reasonable number of steps without skipping the best\n        results in the process.\n        And finally we have the batch size which control how many users we process at a time while training.\n        We store all of this within the RBM objects so we can use them in other functions. \n        \"\"\"\n        self.visibleDimensions = visibleDimensions\n        self.epochs = epochs\n        self.hiddenDimensions = hiddenDimensions\n        self.ratingValues = ratingValues\n        self.learningRate = learningRate\n        self.batchSize = batchSize\n        \n                \n    def Train(self, X):\n        \"\"\"\n        The train function is what we call when we want to create a RBM with weights and biases that will\n        allow us to reconstruct any users rating for any item.\n        It takes in an array called X which is a 2D array that has a row of rating values for every user.\n        Those rating values are flatten binary values one for each possible rating type like we showed in introduction.\n        So this input array contains a row for every user and every row contains binary data for the number of movies,\n        times to 10 possible rating values we have. We're using Tensorflow so we start by reseting everything\n        in case you try to train the same RBM more than once.\n        Next we call MakeGraph function which we'll look at soon it contains the RBM model itself.\n        We then Create a Tensorflow session to run the RBM within, iterate over every epoch,\n        shuffling the users everytime, and divided into baches of users that can run together as we converge\n        better weight and biases values both hidden biases and visible biases.\n        \"\"\"\n        for epoch in range(self.epochs):\n            np.random.shuffle(X)\n            \n            trX = np.array(X)\n            for i in range(0, trX.shape[0], self.batchSize):\n                epochX = trX[i:i+self.batchSize]\n                self.MakeGraph(epochX)\n\n            print(\"Trained epoch \", epoch)\n\n\n    def GetRecommendations(self, inputUser):\n        \"\"\"\n        GetRecommendations function is how we get preditcions for a given user using a RBM\n        that's already been trained and has a set of optimized weights and biases in place.\n        The input here is an array of every rating type for every movie for one user, where that users known\n        ratings is filled in.\n        We define a simple RBM here for this purpose with just a couple of lines.\n        Our forward pass creates a hidden layer by multiplying the visible layer X with our weights and adding\n        in the bias terms.\n        We then reconstruct visible layer by multiplying the hissen layer by the same weights and adding\n        in the visible bias terms we computing during training, that's the backward pass.\n        What's left are the reconstructed rating values for every movie which pass back for further processing.\n        \"\"\"\n        feed = self.MakeHidden(inputUser)\n        rec = self.MakeVisible(feed)\n        return rec[0]       \n\n    def MakeGraph(self, inputUser):\n        \"\"\"\n        MakeGraph function, the base function in all of this!\n        First we set a cosisting random seed, so we consisting results from the run.\n        Next we need set up placeholders in tensorflow for nodes, weight and biases that make up our RBM.\n        We start with X which is our visible nodes remeber we have one for every possible rating value\n        for every possible movie that product contains our visible dimentions property.\n        Even though we'll automatically be binary data the RBM will place floating point values in there in \n        backward pass so we need to convert back to binary, so the type is float32. \n        Next we set up out weights and this must be initialized random data but it can't be just any random data.\n        in this case -4.0 times to squared 6 over the root of the total number of nodes is a right thing to do.\n        and that gives us the magnitude of the positive and negative extence of the initial random weights.\n        Next we set aside the bias terms.Remember in a RBM  we do both forward and backward pass and we end up\n        with bias terms on both hidden and visible layers as a result. so we have both hidden bias tensor \n        associated with hidden nodes and a visible bias tensor associated with the visible nodes and we\n        initialize them all to zero. \n        Notice we've made X, weights, hidden bias and visible bias all members of the RBM class itself,\n        because these are the parts of a trained RBM we need to keep around even after makegraph is called.\n        We'll need those to reconstruct the RBM for making predictions later. \n        Next we need to set up the specifics of the forward and backward passes and it's both what's needed \n        to allow system to converge using contrastive divergence while it is learning.\n        hProb0 is a new tensor that represents the values of the hidden nodes at the end of the forward pass.\n        we multiply the visible nodes and X which contain training data from a given user by our weights which\n        initially are random but get better with each batch retrain with. Then we add in the hidden bias terms,\n        apply a sigmoid activation function and we have got our hidden layer output of forward pass.\n        Before contrastive divergence we need to perform Gibbs sampling of those hidden probabilities\n        coming out of the hidden nodes. We have a new tensor, hSample, which is applying to ReLU\n        activatin function to the sign of the hidden probabilities with a random value subtractive from them\n        This has the effect of selecting a random sample of the complete set of hidden node values.\n        So the weight for that sampled forward pass is what we actually store in a tensor where naming forward\n        and computing those weights is what's makes it all happen.\n        In backward pass where we reconstruct the visible layer by running the RBM in reverse from\n        the hidden nodes outputs.\n        The tensor V takes our sample output from the forward pass which it's name is hSample, multiplying by\n        weights and adds in the visible bias terms.\n        \n        We have to deal with missing ratings and translate this scores to get reconstructed in the binary values,\n        define actual rating classifications.\n        \n        First Let's deal with missing ratings\n        We know that we dont want to predect anything at all but training for movies users didn't rate since\n        we don't have a rating to compare predictions against. This is the sparse data problem we described\n        earlier. So to handle that we'll create a mask from the original ratings from the user that exist in the X tensor.\n        We start by making sure of that everything in our mask is either zero or one by applying the sign function\n        to it then we'll reshape our mask tensor in the 3 Dimentions one for each user in our batch, one for each movie\n        and one for each set of 10 values that represent of a single rating.\n        So at this point each indivisual set of 0 to 9 rating values might contain all zeros which will indicates\n        a missing rating that does not exist in training data at all or there might be single value of one in it\n        which would indicate an actual rating value. To figure out which it is, we'll apply the reduce_max function on line 79.\n        It finds the maximum value in each rating. So, missing ratings end up getting reduced to the value zero\n        and actual ratings get reduced to the value one. So, we have each set of 10 binary rating values organized\n        together for each movie.Then we multiply the reconstructed visible layer by our mask, which has the effect\n        of leaving reconstructed data for ratings we actually have data for and leaving all the missing ratings as zero.\n        We then apply the soft_max function to the results, which will only do anything meaningful as soft_max units,\n        but it does the job. In some ways, it might have been easier to code this algorithm up without using TensorFlow,\n        but then, we couldn't really use our GPU to do it quickly. Alright, finally, we tie everything together\n        and tell TensorFlow what we want it to do in every batch. We run the forward and backward passes and\n        update the weights. Then we updates the hidden bias terms, trying to minimize error between the hidden node outputs\n        from the original data and from the reconstructed data and then update the visible bias terms, \n        trying to minimize error between the original training data and the reconstructed visible data.\n        \n        Then, we stick it all inside self.update, which is ultimately what gets run when we call our train function.\n    \n        \"\"\"\n\n        # Initialize weights randomly\n        maxWeight = -4.0 * np.sqrt(6.0 \/ (self.hiddenDimensions + self.visibleDimensions))\n        self.weights = tf.Variable(tf.random.uniform([self.visibleDimensions, self.hiddenDimensions], minval=-maxWeight, maxval=maxWeight), tf.float32, name=\"weights\")\n        \n        self.hiddenBias = tf.Variable(tf.zeros([self.hiddenDimensions], tf.float32, name=\"hiddenBias\"))\n        self.visibleBias = tf.Variable(tf.zeros([self.visibleDimensions], tf.float32, name=\"visibleBias\"))\n        \n        # Perform Gibbs Sampling for Contrastive Divergence, per the paper we assume k=1 instead of iterating over the \n        # forward pass multiple times since it seems to work just fine\n        \n        # Forward pass\n        # Sample hidden layer given visible...\n        # Get tensor of hidden probabilities\n        hProb0 = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n        # Sample from all of the distributions\n        hSample = tf.nn.relu(tf.sign(hProb0 - tf.random.uniform(tf.shape(hProb0))))\n        # Stitch it together\n        forward = tf.matmul(tf.transpose(inputUser), hSample)\n        \n        # Backward pass\n        # Reconstruct visible layer given hidden layer sample\n        v = tf.matmul(hSample, tf.transpose(self.weights)) + self.visibleBias\n        \n        # Build up our mask for missing ratings\n        vMask = tf.sign(inputUser) # Make sure everything is 0 or 1\n        vMask3D = tf.reshape(vMask, [tf.shape(v)[0], -1, self.ratingValues]) # Reshape into arrays of individual ratings\n        vMask3D = tf.reduce_max(vMask3D, axis=[2], keepdims=True) # Use reduce_max to either give us 1 for ratings that exist, and 0 for missing ratings\n        \n        # Extract rating vectors for each individual set of 10 rating binary values\n        v = tf.reshape(v, [tf.shape(v)[0], -1, self.ratingValues])\n        vProb = tf.nn.softmax(v * vMask3D) # Apply softmax activation function\n        vProb = tf.reshape(vProb, [tf.shape(v)[0], -1]) # And shove them back into the flattened state. Reconstruction is done now.\n        # Stitch it together to define the backward pass and updated hidden biases\n        hProb1 = tf.nn.sigmoid(tf.matmul(vProb, self.weights) + self.hiddenBias)\n        backward = tf.matmul(tf.transpose(vProb), hProb1)\n    \n        # Now define what each epoch will do...\n        # Run the forward and backward passes, and update the weights\n        weightUpdate = self.weights.assign_add(self.learningRate * (forward - backward))\n        # Update hidden bias, minimizing the divergence in the hidden nodes\n        hiddenBiasUpdate = self.hiddenBias.assign_add(self.learningRate * tf.reduce_mean(hProb0 - hProb1, 0))\n        # Update the visible bias, minimizng divergence in the visible results\n        visibleBiasUpdate = self.visibleBias.assign_add(self.learningRate * tf.reduce_mean(inputUser - vProb, 0))\n\n        self.update = [weightUpdate, hiddenBiasUpdate, visibleBiasUpdate]\n        \n    def MakeHidden(self, inputUser):\n        hidden = tf.nn.sigmoid(tf.matmul(inputUser, self.weights) + self.hiddenBias)\n        self.MakeGraph(inputUser)\n        return hidden\n    \n    def MakeVisible(self, feed):\n        visible = tf.nn.sigmoid(tf.matmul(feed, tf.transpose(self.weights)) + self.visibleBias)\n        #self.MakeGraph(feed)\n        return visible\n","a46cff7e":"from surprise import AlgoBase\nfrom surprise import PredictionImpossible\nimport numpy as np\n\n\nclass RBMAlgorithm(AlgoBase):\n\n    def __init__(self, epochs=20, hiddenDim=100, learningRate=0.001, batchSize=100, sim_options={}):\n        AlgoBase.__init__(self)\n        self.epochs = epochs\n        self.hiddenDim = hiddenDim\n        self.learningRate = learningRate\n        self.batchSize = batchSize\n\n    def softmax(self, x):\n        return np.exp(x) \/ np.sum(np.exp(x), axis=0)\n\n    def fit(self, trainset):\n        AlgoBase.fit(self, trainset)\n\n        numUsers = trainset.n_users\n        numItems = trainset.n_items\n\n        trainingMatrix = np.zeros([numUsers, numItems, 10], dtype=np.float32)\n\n        for (uid, iid, rating) in trainset.all_ratings():\n            adjustedRating = int(float(rating)*2.0) - 1\n            trainingMatrix[int(uid), int(iid), adjustedRating] = 1\n\n        # Flatten to a 2D array, with nodes for each possible rating type on each possible item, for every user.\n        trainingMatrix = np.reshape(trainingMatrix, [trainingMatrix.shape[0], -1])\n\n        # Create an RBM with (num items * rating values) visible nodes\n        rbm = RBMs(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n        rbm.Train(trainingMatrix)\n\n        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n        for uiid in range(trainset.n_users):\n            if (uiid % 50 == 0):\n                print(\"Processing user \", uiid)\n            recs = rbm.GetRecommendations([trainingMatrix[uiid]])\n            recs = np.reshape(recs, [numItems, 10])\n\n            for itemID, rec in enumerate(recs):\n                # The obvious thing would be to just take the rating with the highest score:\n                #rating = rec.argmax()\n                # ... but this just leads to a huge multi-way tie for 5-star predictions.\n                # The paper suggests performing normalization over K values to get probabilities\n                # and take the expectation as your prediction, so we'll do that instead:\n                normalized = self.softmax(rec)\n                rating = np.average(np.arange(10), weights=normalized)\n                self.predictedRatings[uiid, itemID] = (rating + 1) * 0.5\n\n        return self\n\n\n    def estimate(self, u, i):\n\n        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n            raise PredictionImpossible('User and\/or item is unkown.')\n\n        rating = self.predictedRatings[u, i]\n\n        if (rating < 0.001):\n            raise PredictionImpossible('No valid prediction exists.')\n\n        return rating\n\n    \n","b3398428":"import os\nimport csv\nimport sys\nimport re\n\nfrom surprise import Dataset\nfrom surprise import Reader\n\nfrom collections import defaultdict\n\nclass MovieLens:\n\n    movieID_to_name = {}\n    name_to_movieID = {}\n    ratingsPath = '\/kaggle\/input\/movielens\/ratings.csv'\n    moviesPath = '\/kaggle\/input\/movielens\/movies.csv'\n    def loadMovieLensLatestSmall(self):\n\n        # Look for files relative to the directory we are running from\n        os.chdir(os.path.dirname(sys.argv[0]))\n\n        ratingsDataset = 0\n        self.movieID_to_name = {}\n        self.name_to_movieID = {}\n\n        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n\n        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n\n        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n                movieReader = csv.reader(csvfile)\n                next(movieReader)  #Skip header line\n                for row in movieReader:\n                    movieID = int(row[0])\n                    movieName = row[1]\n                    self.movieID_to_name[movieID] = movieName\n                    self.name_to_movieID[movieName] = movieID\n\n        return ratingsDataset\n\n    def getUserRatings(self, user):\n        userRatings = []\n        hitUser = False\n        with open(self.ratingsPath, newline='') as csvfile:\n            ratingReader = csv.reader(csvfile)\n            next(ratingReader)\n            for row in ratingReader:\n                userID = int(row[0])\n                if (user == userID):\n                    movieID = int(row[1])\n                    rating = float(row[2])\n                    userRatings.append((movieID, rating))\n                    hitUser = True\n                if (hitUser and (user != userID)):\n                    break\n\n        return userRatings\n\n    def getPopularityRanks(self):\n        ratings = defaultdict(int)\n        rankings = defaultdict(int)\n        with open(self.ratingsPath, newline='') as csvfile:\n            ratingReader = csv.reader(csvfile)\n            next(ratingReader)\n            for row in ratingReader:\n                movieID = int(row[1])\n                ratings[movieID] += 1\n        rank = 1\n        for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n            rankings[movieID] = rank\n            rank += 1\n        return rankings\n\n    def getGenres(self):\n        genres = defaultdict(list)\n        genreIDs = {}\n        maxGenreID = 0\n        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n            movieReader = csv.reader(csvfile)\n            next(movieReader)  #Skip header line\n            for row in movieReader:\n                movieID = int(row[0])\n                genreList = row[2].split('|')\n                genreIDList = []\n                for genre in genreList:\n                    if genre in genreIDs:\n                        genreID = genreIDs[genre]\n                    else:\n                        genreID = maxGenreID\n                        genreIDs[genre] = genreID\n                        maxGenreID += 1\n                    genreIDList.append(genreID)\n                genres[movieID] = genreIDList\n        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n        for (movieID, genreIDList) in genres.items():\n            bitfield = [0] * maxGenreID\n            for genreID in genreIDList:\n                bitfield[genreID] = 1\n            genres[movieID] = bitfield\n\n        return genres\n\n    def getYears(self):\n        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n        years = defaultdict(int)\n        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n            movieReader = csv.reader(csvfile)\n            next(movieReader)\n            for row in movieReader:\n                movieID = int(row[0])\n                title = row[1]\n                m = p.search(title)\n                year = m.group(1)\n                if year:\n                    years[movieID] = int(year)\n        return years\n\n    def getMiseEnScene(self):\n        mes = defaultdict(list)\n        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n            mesReader = csv.reader(csvfile)\n            next(mesReader)\n            for row in mesReader:\n                movieID = int(row[0])\n                avgShotLength = float(row[1])\n                meanColorVariance = float(row[2])\n                stddevColorVariance = float(row[3])\n                meanMotion = float(row[4])\n                stddevMotion = float(row[5])\n                meanLightingKey = float(row[6])\n                numShots = float(row[7])\n                mes[movieID] = [avgShotLength, meanColorVariance, stddevColorVariance,\n                   meanMotion, stddevMotion, meanLightingKey, numShots]\n        return mes\n\n    def getMovieName(self, movieID):\n        if movieID in self.movieID_to_name:\n            return self.movieID_to_name[movieID]\n        else:\n            return \"\"\n\n    def getMovieID(self, movieName):\n        if movieName in self.name_to_movieID:\n            return self.name_to_movieID[movieName]\n        else:\n            return 0\n","62086c61":"from surprise.model_selection import train_test_split\nfrom surprise.model_selection import LeaveOneOut\nfrom surprise import KNNBaseline\n\nclass EvaluationData:\n\n    def __init__(self, data, popularityRankings):\n\n        self.rankings = popularityRankings\n\n        #Build a full training set for evaluating overall properties\n        self.fullTrainSet = data.build_full_trainset()\n        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n\n        #Build a 75\/25 train\/test split for measuring accuracy\n        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n\n        #Build a \"leave one out\" train\/test split for evaluating top-N recommenders\n        #And build an anti-test-set for building predictions\n        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n        for train, test in LOOCV.split(data):\n            self.LOOCVTrain = train\n            self.LOOCVTest = test\n\n        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n\n        #Compute similarty matrix between items so we can measure diversity\n        sim_options = {'name': 'cosine', 'user_based': False}\n        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n        self.simsAlgo.fit(self.fullTrainSet)\n\n    def GetFullTrainSet(self):\n        return self.fullTrainSet\n\n    def GetFullAntiTestSet(self):\n        return self.fullAntiTestSet\n\n    def GetAntiTestSetForUser(self, testSubject):\n        trainset = self.fullTrainSet\n        fill = trainset.global_mean\n        anti_testset = []\n        u = trainset.to_inner_uid(str(testSubject))\n        user_items = set([j for (j, _) in trainset.ur[u]])\n        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n                                 i in trainset.all_items() if\n                                 i not in user_items]\n        return anti_testset\n\n    def GetTrainSet(self):\n        return self.trainSet\n\n    def GetTestSet(self):\n        return self.testSet\n\n    def GetLOOCVTrainSet(self):\n        return self.LOOCVTrain\n\n    def GetLOOCVTestSet(self):\n        return self.LOOCVTest\n\n    def GetLOOCVAntiTestSet(self):\n        return self.LOOCVAntiTestSet\n\n    def GetSimilarities(self):\n        return self.simsAlgo\n\n    def GetPopularityRankings(self):\n        return self.rankings\n","147209fb":"import itertools\n\nfrom surprise import accuracy\nfrom collections import defaultdict\n\nclass RecommenderMetrics:\n\n    def MAE(predictions):\n        return accuracy.mae(predictions, verbose=False)\n\n    def RMSE(predictions):\n        return accuracy.rmse(predictions, verbose=False)\n\n    def GetTopN(predictions, n=10, minimumRating=0.0):\n        topN = defaultdict(list)\n\n\n        for userID, movieID, actualRating, estimatedRating, _ in predictions:\n            if (estimatedRating >= minimumRating):\n                topN[int(userID)].append((int(movieID), estimatedRating))\n\n        for userID, ratings in topN.items():\n            ratings.sort(key=lambda x: x[1], reverse=True)\n            topN[int(userID)] = ratings[:n]\n\n        return topN\n\n    def HitRate(topNPredicted, leftOutPredictions):\n        hits = 0\n        total = 0\n\n        # For each left-out rating\n        for leftOut in leftOutPredictions:\n            userID = leftOut[0]\n            leftOutMovieID = leftOut[1]\n            # Is it in the predicted top 10 for this user?\n            hit = False\n            for movieID, predictedRating in topNPredicted[int(userID)]:\n                if (int(leftOutMovieID) == int(movieID)):\n                    hit = True\n                    break\n            if (hit) :\n                hits += 1\n\n            total += 1\n\n        # Compute overall precision\n        return hits\/total\n\n    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n        hits = 0\n        total = 0\n\n        # For each left-out rating\n        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n            # Only look at ability to recommend things the users actually liked...\n            if (actualRating >= ratingCutoff):\n                # Is it in the predicted top 10 for this user?\n                hit = False\n                for movieID, predictedRating in topNPredicted[int(userID)]:\n                    if (int(leftOutMovieID) == movieID):\n                        hit = True\n                        break\n                if (hit) :\n                    hits += 1\n\n                total += 1\n\n        # Compute overall precision\n        return hits\/total\n\n    def RatingHitRate(topNPredicted, leftOutPredictions):\n        hits = defaultdict(float)\n        total = defaultdict(float)\n\n        # For each left-out rating\n        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n            # Is it in the predicted top N for this user?\n            hit = False\n            for movieID, predictedRating in topNPredicted[int(userID)]:\n                if (int(leftOutMovieID) == movieID):\n                    hit = True\n                    break\n            if (hit) :\n                hits[actualRating] += 1\n\n            total[actualRating] += 1\n\n        # Compute overall precision\n        for rating in sorted(hits.keys()):\n            print (rating, hits[rating] \/ total[rating])\n\n    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n        summation = 0\n        total = 0\n        # For each left-out rating\n        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n            # Is it in the predicted top N for this user?\n            hitRank = 0\n            rank = 0\n            for movieID, predictedRating in topNPredicted[int(userID)]:\n                rank = rank + 1\n                if (int(leftOutMovieID) == movieID):\n                    hitRank = rank\n                    break\n            if (hitRank > 0) :\n                summation += 1.0 \/ hitRank\n\n            total += 1\n\n        return summation \/ total\n\n    # What percentage of users have at least one \"good\" recommendation\n    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n        hits = 0\n        for userID in topNPredicted.keys():\n            hit = False\n            for movieID, predictedRating in topNPredicted[userID]:\n                if (predictedRating >= ratingThreshold):\n                    hit = True\n                    break\n            if (hit):\n                hits += 1\n\n        return hits \/ numUsers\n\n    def Diversity(topNPredicted, simsAlgo):\n        n = 0\n        total = 0\n        simsMatrix = simsAlgo.compute_similarities()\n        for userID in topNPredicted.keys():\n            pairs = itertools.combinations(topNPredicted[userID], 2)\n            for pair in pairs:\n                movie1 = pair[0][0]\n                movie2 = pair[1][0]\n                innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))\n                innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n                similarity = simsMatrix[innerID1][innerID2]\n                total += similarity\n                n += 1\n\n        if (n > 0):\n            S = total \/ n\n            return (1-S)\n        else:\n            return 0\n\n    def Novelty(topNPredicted, rankings):\n        n = 0\n        total = 0\n        for userID in topNPredicted.keys():\n            for rating in topNPredicted[userID]:\n                movieID = rating[0]\n                rank = rankings[movieID]\n                total += rank\n                n += 1\n        return total \/ n\n","a580096a":"class EvaluatedAlgorithm:\n\n    def __init__(self, algorithm, name):\n        self.algorithm = algorithm\n        self.name = name\n\n    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n        metrics = {}\n        # Compute accuracy\n        if (verbose):\n            print(\"Evaluating accuracy...\")\n        self.algorithm.fit(evaluationData.GetTrainSet())\n        predictions = self.algorithm.test(evaluationData.GetTestSet())\n        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n\n        if (doTopN):\n            # Evaluate top-10 with Leave One Out testing\n            if (verbose):\n                print(\"Evaluating top-N with leave-one-out...\")\n            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())\n            # Build predictions for all ratings not in the training set\n            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n            # Compute top 10 recs for each user\n            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n            if (verbose):\n                print(\"Computing hit-rate and rank metrics...\")\n            # See how often we recommended a movie the user actually rated\n            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)\n            # See how often we recommended a movie the user actually liked\n            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n            # Compute ARHR\n            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n\n            #Evaluate properties of recommendations on full training set\n            if (verbose):\n                print(\"Computing recommendations with full data set...\")\n            self.algorithm.fit(evaluationData.GetFullTrainSet())\n            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n            if (verbose):\n                print(\"Analyzing coverage, diversity, and novelty...\")\n            # Print user coverage with a minimum predicted rating of 4.0:\n            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted,\n                                                                   evaluationData.GetFullTrainSet().n_users,\n                                                                   ratingThreshold=4.0)\n            # Measure diversity of recommendations:\n            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n\n            # Measure novelty (average popularity rank of recommendations):\n            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted,\n                                                            evaluationData.GetPopularityRankings())\n\n        if (verbose):\n            print(\"Analysis complete.\")\n\n        return metrics\n\n    def GetName(self):\n        return self.name\n\n    def GetAlgorithm(self):\n        return self.algorithm\n","51bd2311":"class Evaluator:\n\n    algorithms = []\n\n    def __init__(self, dataset, rankings):\n        ed = EvaluationData(dataset, rankings)\n        self.dataset = ed\n\n    def AddAlgorithm(self, algorithm, name):\n        alg = EvaluatedAlgorithm(algorithm, name)\n        self.algorithms.append(alg)\n\n    def Evaluate(self, doTopN):\n        results = {}\n        for algorithm in self.algorithms:\n            print(\"Evaluating \", algorithm.GetName(), \"...\")\n            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n\n        # Print results\n        print(\"\\n\")\n\n        if (doTopN):\n            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n            for (name, metrics) in results.items():\n                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n        else:\n            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n            for (name, metrics) in results.items():\n                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n\n        print(\"\\nLegend:\\n\")\n        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n        if (doTopN):\n            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n            print(\"           for a given user. Higher means more diverse.\")\n            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n\n    def SampleTopNRecs(self, ml, testSubject=85, k=10):\n\n        for algo in self.algorithms:\n            print(\"\\nUsing recommender \", algo.GetName())\n\n            print(\"\\nBuilding recommendation model...\")\n            trainSet = self.dataset.GetFullTrainSet()\n            algo.GetAlgorithm().fit(trainSet)\n\n            print(\"Computing recommendations...\")\n            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n\n            predictions = algo.GetAlgorithm().test(testSet)\n\n            recommendations = []\n\n            print (\"\\nWe recommend:\")\n            for userID, movieID, actualRating, estimatedRating, _ in predictions:\n                intMovieID = int(movieID)\n                recommendations.append((intMovieID, estimatedRating))\n\n            recommendations.sort(key=lambda x: x[1], reverse=True)\n\n            for ratings in recommendations[:10]:\n                print(ml.getMovieName(ratings[0]), ratings[1])\n","48ae9b24":"from surprise import NormalPredictor\nfrom surprise.model_selection import GridSearchCV\n\nimport random\n\ndef LoadMovieLensData():\n    ml = MovieLens()\n    print(\"Loading movie ratings...\")\n    data = ml.loadMovieLensLatestSmall()\n    print(\"\\nComputing movie popularity ranks so we can measure novelty later...\")\n    rankings = ml.getPopularityRanks()\n    return (ml, data, rankings)\n\nnp.random.seed(0)\nrandom.seed(0)\n\n# Load up common data set for the recommender algorithms\n(ml, evaluationData, rankings) = LoadMovieLensData()\n\nprint(\"Searching for best parameters...\")\nparam_grid = {'hiddenDim': [20, 10], 'learningRate': [0.1, 0.01]}\ngs = GridSearchCV(RBMAlgorithm, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(evaluationData)\n\n# best RMSE score\nprint(\"Best RMSE score attained: \", gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])\n\n# Construct an Evaluator to, you know, evaluate them\nevaluator = Evaluator(evaluationData, rankings)\n\nparams = gs.best_params['rmse']\nRBMtuned = RBMAlgorithm(hiddenDim = params['hiddenDim'], learningRate = params['learningRate'])\nevaluator.AddAlgorithm(RBMtuned, \"RBM - Tuned\")\n\nRBMUntuned = RBMAlgorithm()\nevaluator.AddAlgorithm(RBMUntuned, \"RBM - Untuned\")\n\n# Just make random recommendations\nRandom = NormalPredictor()\nevaluator.AddAlgorithm(Random, \"Random\")\n\n# Fight!\nevaluator.Evaluate(True)\n\nevaluator.SampleTopNRecs(ml)","7df0fa51":"# RBM Tuning & Bake Off Module","cc13cbd4":"# Recommender Metrics :","6dda5666":"# Restricted Boltzmann Machines for Recommender Systems\n\n## Introduction\nIt's been in use since 2007, long before AI had its big resurgence, but it's still a commonly cited paper and a technique that's still in use today.\nA few years ago, Netflix confirmed they were still using RBM's as part of their recommender system that's in production. Let's learn how it works.\nFirst of all, if you're serious about using RBM's for recommendations, I recommend tracking down this [paper](https:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/rbmcf.pdf) so you can study it later once you understand the general concepts. It's from a team from the University of Toronto and was published in the proceedings of the 24th International Conference on Machine Learning in 2007.\n\n### Restricted Boltzmann Machines (RBMs)\nRBM's are really one of the simplest neural networks. It's just two layers, a visible layer and a hidden layer. We train it by feeding our training data into the visible layer in a forward pass and training weights and biases between them during back propagation. An activation function such as ReLU is used to produce the output from each hidden neuron. \n![image.png](attachment:image.png)\n\nWhy are they called Restricted Boltzmann Machines? Well they are restricted because neurons in the same layer can't communicate with each other directly. There are only connections between the two different layers. That's just what you do these days with modern neural networks, but that restriction didn't exist with earlier Boltzmann Machines back when AI was still kind of floundering as a field. And RBM's weren't invented by a guy named Boltzmann. The name refers to the Boltzmann distribution function they use for their sampling function. RBM's are actually credited to Geoffrey Hinton, who was a professor at Carnegie Mellon University at the time. The idea actually dates back to 1985. So RBM's get trained by doing a forward pass, which we just described, and then a backward pass where the inputs get reconstructed. We do this iteratively over many epochs just like when we train a deep neural network until it converges on a set of weights and biases that minimizes the error.\nLet's take a closer look at that backward pass.\n![image.png](attachment:image.png)\n\nDuring the backward pass we are trying to reconstruct the original input by feeding back the output of the forward pass back through the hidden layer and seeing what values we end up with out of the visible layer. Since those weights are initially random, there can be a big difference between the inputs we started with and the ones we reconstruct. In the process we end up with another set of bias terms, this time on the visible layer. So we share weights between both the forward and backward passes, but we have two sets of biases. The hidden bias that's used in the forward pass and the visible bias that's used in this backward pass. We can then measure the error we end up with and use that information to adjust the weights a little bit during the next iteration to try and minimize that error.\n\nYou can also construct multi layer RBM's that are akin to modern deep learning architectures. The main difference is that we read the output of an RBM on the lower level during a backward pass as opposed to just taking the outputs on the other side like we would with a modern neural network.\n\nSo this all works well when you have a complete set of training data, for example applying an RBM to the same MNIST handwriting recognition problem we looked at in this [notebook](https:\/\/www.kaggle.com\/alirezanematolahy\/handwriting-recognition-with-keras) is a straightforward thing to do. \n\nWhen you applying RBMs or neural networks in general to recommender systems though, things get wierd. The problem is that we have sparse training data, very sparse in most cases. How do you train a neural network when most of your input nodes have no data to work with?","4100f7f3":"# Evaluated Algorithm :","f16def01":"# RBM Algorithm :","76b56b94":"# RBMs for recommender systems\n\nAdapting a RBM for say recommender movies giving 5 star ratings data requires a few twists to the generic RBM architecture we just described.\n![image.png](attachment:image.png)\n\nLet's take a step back and think about what we're doing here The general idea is to use each individual user in our training data as a set of inputs into our RBM to help train it. So we process each user as part of a batch during training, looking at their ratings for every movie they rated. So our visible nodes represent ratings for a given user on every movie, and we're trying to learn weights and biases that let us reconstruct ratings for user movie pairs we don't know yet. \n\nFirst of all our visible units aren't just simple nodes taking in a single input. Ratings are really categorical data. So we actually want to treat each individual rating as five nodes, one for each possible rating value. So let's say the first rating we have on our training data is the five star rating. That would be represented as four nodes with the value of zero and one with the value of one as represented here. Then we have a couple of ratings that are missing for user item pairs that are unknown and need to be predicted, then we have a three star rating represented like this with a one in the third slot. When we're done training the RBM, we'll have a set of weights and biases that should allow us to reconstruct ratings for any user. So to use it to predict ratings for a new user, we just run it once again using the known ratings of the user we're interested in. We run those through in the forward pass then back again in the backward pass and end up with reconstructed rating values for that user. We can then run soft max on each group of five rating values to translate the output back into a five star rating for every item. But again, the big problem is that the data we have is sparse. If we are training an RBM on every possible combination of users and movies, most of that data will be missing, because most movies have not been rated at all by a specific user. We want to predict user ratings for every movie, though so we need to leave space for all of them. That means if we have n movies, we end up with n times five visible nodes and for any given user, most of them are undefined and empty. \nWe deal with this by excluding any missing ratings from processing while we're training the RBM. This is kind of a tricky thing to do, because most frameworks built for deep learning such as TensorFlow assume you want to process everything in parallel all the time. \nSparse data isn't something they were really built for originally, but there are ways to trick it into doing what we want. But notice that we've only drawn lines between visible units that actually have known ratings data in them and the hidden layer. \nSo as we're training our RBM with a given user's known ratings, we only attempt to learn the weights and biases used for the movies that user actually rated. As we iterate through training on all of the other users, we fill in the other weights and biases as we go.\n\nFor the sake of completeness I should point out that TensorFlow actually does have a sparse tensor these days you can use and there are other frameworks, such as [Amazon's Destiny system](https:\/\/aws.amazon.com\/blogs\/big-data\/generating-recommendations-at-amazon-scale-with-apache-spark-and-amazon-dsstne\/) that are designed to construct more typical deep neural networks with sparse data.\nRBM's will probably become a thing of the past now that we can treat sparse data in the same manner as complete data with modern neural networks and we will examine that in more depth in this notebook.\nThe other twist is how to best train an RBM that contains huge amounts of sparse data. Gradient descent needs a very efficient expectation function to optimize on and for recommender systems this function is called contrastive divergence. At least that's the function the paper on the topic uses successfully. The math behind it gets a bit complicated, but the basic idea is that it samples probability distributions during training, using something called a Gibbs sampler. We only train it on the ratings that actually exist, but reuse the resulting weights and biases across other users to fill in the missing ratings we want to predict. So let's look at some code that actually implements an RBM on our movie lens data and play around with it.","109a754a":"# Preparing data :","b5c9082d":"# Evaluation Data :","b5a2a51d":"# Evaluator Module :"}}