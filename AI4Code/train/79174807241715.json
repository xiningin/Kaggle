{"cell_type":{"9f6b6865":"code","38e26309":"code","9a3286fe":"code","37f23100":"code","3f3f4f11":"code","17c58ac3":"code","a6119579":"code","a3aafc61":"code","c4eba135":"code","d2650597":"code","aa53d1e5":"code","7139126f":"code","a50bf457":"code","f9fa5152":"code","788349ab":"code","ca6448c8":"code","79e68755":"code","8481bd6b":"code","d7df8b5f":"code","11d4d6f0":"code","bbc91410":"code","8e7f08c1":"code","1bba5279":"code","34f4f171":"code","47a84056":"code","c3c6eae3":"code","ff43fc89":"code","870c7d83":"code","20aad286":"code","e72dc213":"code","23beb0e7":"markdown","fdb83fc7":"markdown","87b54a3f":"markdown","4481bf10":"markdown","7994ef0d":"markdown","79a2f89c":"markdown","2b24234a":"markdown","c611a3ae":"markdown","3d92e60c":"markdown","38a4d931":"markdown","5be7cd66":"markdown","ff680de1":"markdown","a252d16e":"markdown","4a96892a":"markdown","28fcee5e":"markdown","588a538b":"markdown","c5c4ca79":"markdown","5f9847b1":"markdown","218d2ead":"markdown","ed8e849a":"markdown","d5f9899b":"markdown","7f313696":"markdown","ffb3b416":"markdown","205febcd":"markdown","b20eb546":"markdown"},"source":{"9f6b6865":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nplt.rcParams['figure.figsize']=(12,5)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras import regularizers\n#!pip install openpyxl","38e26309":"df_car = pd.read_csv(\"..\/input\/car-price-prediction\/CarPrice_Assignment.csv\")\n#data_car = pd.read_excel(\"..\/input\/car-price-prediction\/Data Dictionary - carprices.xlsx\")","9a3286fe":"df_car.head()","37f23100":"print(df_car.shape)\nprint(df_car.size)","3f3f4f11":"df_car.info()","17c58ac3":"df_car.describe()","a6119579":"df_car.drop(columns = ['car_ID'], inplace= True)","a3aafc61":"plt.figure(figsize=(16,10))\nsns.heatmap(df_car.select_dtypes(include=['int','float']).corr(),annot=True)","c4eba135":"df_car.columns","d2650597":"# Coverting categorical data to dummy variables\ncar_dummies = pd.get_dummies(df_car,columns=['symboling','CarName', 'fueltype', 'aspiration', 'doornumber','carbody', \n                                             'drivewheel','enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem'])","aa53d1e5":"car_dummies.describe()","7139126f":"# Training Data\nnp.random.seed(11111) \nmsk = np.random.rand(len(car_dummies)) < 0.72\nX_train = car_dummies[msk]\nX_test = car_dummies[~msk]","a50bf457":"print(len(X_train))\nprint(len(X_test))","f9fa5152":"# Target Data \ny_train = X_train.pop('price')\ny_test = X_test.pop('price')","788349ab":"{X_train.columns.get_loc(c): c for idx, c in enumerate(X_train.columns)}","ca6448c8":"X_mean = X_train.iloc[:,0:13].mean(axis=0) # taking mean of training data\nX_train.iloc[:,0:13] -= X_mean # subtracting the mean from training data\nX_std = X_train.iloc[:,0:13].std(axis=0) # taking std of training data\nX_train.iloc[:,0:13] \/= X_std # dividing train data by std\nX_test.iloc[:,0:13] -= X_mean # subrating the mean from testing data\nX_test.iloc[:,0:13] \/= X_std # dividing test data by std","79e68755":"y_mean = y_train.mean() \ny_train -= y_mean\ny_std = y_train.std()\ny_train \/= y_std\ny_test -= y_mean\ny_test \/= y_std","8481bd6b":"X_train = np.asarray(X_train).astype(float)\nX_test = np.asarray(X_test).astype(float)\n\ny_train = np.asarray(y_train).astype(float)\ny_test = np.asarray(y_test).astype(float)","d7df8b5f":"print(len(X_train))\nprint(len(y_train))\nprint(len(X_test))\nprint(len(y_test))","11d4d6f0":"def build_model():\n    model = Sequential()\n    model.add(Dense(80 , activation='relu', input_shape=(X_train.shape[-1],))) # Input Layer\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(40 , activation='relu'))\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(20 , activation='relu'))\n    model.add(Dropout(0.5)) # Dropout Layer\n    model.add(Dense(10 , activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) # Compiling Model\n    return model","bbc91410":"build_model().summary()","8e7f08c1":"import numpy as np\nk =  4 # no of folds\nnum_val_samples = len(X_train) \/\/ k\nnum_epochs = 100\nall_scores_relu = []\nfor i in range(k):\n    print('processing fold #', i)\n    val_X = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n    val_y = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n    partial_train_data = np.concatenate([X_train[:i * num_val_samples],X_train[(i + 1) * num_val_samples:]],  axis=0)\n    # print(partial_train_data)\n    partial_train_targets = np.concatenate([y_train[:i * num_val_samples],y_train[(i + 1) * num_val_samples:]],axis=0)\n    model = build_model()\n    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=0)\n    val_mse, val_mae = model.evaluate(val_X, val_y, verbose=0)\n    all_scores_relu.append(val_mae)","1bba5279":"val_mse","34f4f171":"all_scores_relu","47a84056":"model_relu = build_model()\nmodel_relu.fit(X_train, y_train,epochs= 80, batch_size=1, verbose=0)\ntest_mse_score, test_mae_score = model_relu.evaluate(X_test, y_test)","c3c6eae3":"# MSE Score\ntest_mse_score","ff43fc89":"# MAE Score\ntest_mae_score","870c7d83":"x_relu = model_relu.predict(X_test[5].reshape(1,X_test.shape[1]))","20aad286":"x_relu * y_std + y_mean","e72dc213":" y_test[5] * y_std + y_mean ","23beb0e7":"**Note** the network with the mse loss function\u2014mean squared error,\nthe square of the difference between the predictions and the targets. This is a widely\nused loss function for regression problems.","fdb83fc7":"### If this Kernel helped you in any way, some <span style=\"color:red\">UPVOTES !!!<\/span> would be very much appreciated.","87b54a3f":"#### Dropping Useless Column","4481bf10":"# Car Price Prediction Using Keras","7994ef0d":"<a id=\"5\"><\/a> <br>\n## Splitting the Data\nSplitting data into training and testing data.","79a2f89c":"<a id=\"3\"><\/a> <br>\n## Data Visualization","2b24234a":"### Changing Data Type To Float","c611a3ae":"Price is highly(positively) correlated with wheelbase, carlength, carwidth, curbweight and enginesize. And negatively correlated with citympg and highwaympg.","3d92e60c":"#### Below are the steps which we will be basically following:\n\n1. [Step 1: Reading and Understanding the Data](#1)\n1.  [Step 2: Cleaning the Data](#2)\n    - Missing Value check\n    - Data type check\n    - Duplicate check\n1. [Step 3: Data Visualization](#3)\n    - Heatmap\n1. [Step 4: Data Preprocessing](#4) \n   - One-Hot Encoding\n1. [Step 5: Splitting the Data into Training and Testing Sets](#5)\n1. [Step 6: Normalizing the Data](#6)\n1. [Step 7: Building a Model](#7)\n1. [Step 8: K-Fold Validation](#8)\n1. [Step 9: Training](#9)\n1. [Step 10: Model Evaluation](#10)\n   - MSE Score\n1. [Step 11: Prediction](#11)","38a4d931":"<a id=\"6\"><\/a> <br>\n## Normalizing the Data\nHere we are normalizing data by subtracting data by mean of the data and then dividing by standard deviation of the data.","5be7cd66":"<a id=\"2\"><\/a> <br>\n## Cleaning the Data","ff680de1":"<a id=\"4\"><\/a> <br>\n## Data Preprocessing","a252d16e":"<a id=\"1\"><\/a> <br>\n## Loading Data","4a96892a":"### Actual Value","28fcee5e":" **Note** that here we will use the reverse process of Normalization to retrieve our values of price in thousand of dollars i.e. x = (y - mean)\/ std ==>> we will calculate y = x * std + mean and then we will compare it with our target values.","588a538b":"Firstly, we will import all the required libraries.","c5c4ca79":"#### Checking Shape and Size","5f9847b1":"<a id=\"11\"><\/a> <br>\n## Prediction","218d2ead":"##### Validation MSE","ed8e849a":"## Setting-up Envoirnment ","d5f9899b":"<a id=\"7\"><\/a> <br>\n## Building a Model","7f313696":"<a id=\"8\"><\/a> <br>\n## K- Fold Validation","ffb3b416":"<a id=\"9\"><\/a> <br>\n## Training\nHere we will call model and train on the training data and evaluate on the test data.","205febcd":"<a id=\"10\"><\/a> <br>\n## Model Evaluation","b20eb546":"There is no missing value."}}