{"cell_type":{"6c1f912f":"code","f14b637c":"code","f1b6f538":"code","f2282cd8":"code","19a6fd45":"code","457aa93d":"code","37f965ce":"code","d43f2ce8":"code","a8a26e2a":"code","551407cb":"code","9137adba":"code","b6aed0d1":"code","d8c11f52":"code","d2ba25d6":"code","c7991b2a":"code","66a0c970":"code","e456a9b3":"code","44eb99c3":"code","1f0925e7":"code","c4d07f0c":"markdown","11f6e32e":"markdown","3e791b3c":"markdown","b24a28bb":"markdown","69c83da2":"markdown"},"source":{"6c1f912f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))","f14b637c":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","f1b6f538":"#print information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\ntrain_df.info()","f2282cd8":"#Generates descriptive statistics of a dataset, excluding NaN values.\ntrain_df.describe()","19a6fd45":"train_df.head()","457aa93d":"test_df.info()","37f965ce":"test_df.describe()","d43f2ce8":"test_df.head()","a8a26e2a":"## Number of words in the comment_text ##\ntrain_df[\"num_words\"] = train_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\ntrain_df.head()","551407cb":"#minimum length of a comment in train data\nmin(train_df.num_words)","9137adba":"#maximum length of comment\nmax(train_df.num_words)","b6aed0d1":"#print rows where comment contains single token in train data\ntrain_df[train_df.num_words==1][:3]","d8c11f52":"train_df[train_df.num_words==1411]","d2ba25d6":"toxic_count = train_df.iloc[:,2:-1].sum(axis=0)\narr_toxic_count = toxic_count.values\nindex = toxic_count.index.values","c7991b2a":"#plot toxic count for each toxic class\nimport matplotlib.pyplot as plt\nx_pos = [i for i,_ in enumerate(index)]\nplt.figure(figsize=(5, 3))\nplt.bar(x_pos,arr_toxic_count,align='edge')\nplt.xlabel(\"Toxic Severity levels\")\nplt.ylabel(\"comments count\")\nplt.title(\"comment count of various toxic Severity levels\")\nplt.xticks(x_pos, index,rotation=90)\n\nplt.show()","66a0c970":"# column wise sum of toxic labels for each row\ntrain_df['toxic_score'] = train_df.iloc[:,2:-2].sum(axis=1)\ntrain_df.head()","e456a9b3":"train_df[train_df['toxic_score']>0][:5]","44eb99c3":"toxic_observation_count = len(train_df[train_df['toxic_score']>0])\nnon_toxic_observation_count = train_df.shape[0]-toxic_observation_count\ntoxicity_type = [\"toxic\", \"non-toxic\"]\ncomments_count= [toxic_observation_count, non_toxic_observation_count]","1f0925e7":"#plot toxic count for each toxic class\nimport matplotlib.pyplot as plt\nx_pos = [i for i,_ in enumerate(toxicity_type)]\nplt.figure(figsize=(3, 3))\nplt.bar(x_pos,comments_count,align='edge')\nplt.xlabel(\"toxicity Type\")\nplt.ylabel(\"comments count\")\nplt.title(\"comment count for toxic vs non-toxic\")\nplt.xticks(x_pos, toxicity_type,rotation=90)\n\nplt.show()","c4d07f0c":"maximum number of comments are present in category toxic. Comments can have more than one category also i.e.  same comment can be toxic, obscene, and threat","11f6e32e":"There are no missing values in any column of train data.\n\nno. of rows = 159571\n\nno. of columns = 8\n\nSimilary, we can check for test dataset","3e791b3c":"The count of non-toxic observation is 143346 and count of toxic data is 16225","b24a28bb":"The length of longest comment is 1411. It contains repeated text \" I AM AN LOSER ====\". It is single comment with longest length. It is toxic as its label is 1.","69c83da2":"The above table shows comments which contains single word. All these comments are non-toxic as binary label is 0 for all the columns(toxic, severe_toxic, obscene, threat, insult, identity_hate). these columns tells about the toxic behaviour of comment. If comment is labeled as 1, it is considered toxic."}}