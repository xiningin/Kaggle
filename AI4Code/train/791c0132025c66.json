{"cell_type":{"53d87df0":"code","210e2579":"code","c974af26":"code","1d33fdc6":"code","5b782e86":"code","dbf12636":"code","e274f004":"code","867a5260":"code","e795fc7f":"code","798e1c66":"code","58b630a7":"markdown","01db4a2a":"markdown","9ed26489":"markdown","a6ac67ed":"markdown"},"source":{"53d87df0":"#include libraries\n%matplotlib inline\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.constraints import maxnorm\n%load_ext autoreload\n%autoreload 2","210e2579":"#set target folder to create plots\nimport os\nTARGET_DIR = \"\/kaggle\/working\/figures\"\nif not os.path.isdir(TARGET_DIR):\n    os.mkdir(TARGET_DIR)","c974af26":"def standardize_scale(*args):\n    \"\"\" Standardize the feature matrix x by multipling by 10E9 -> values will be >0 and around 1.\n    \"\"\"\n    #set scale manualy to 10e9\n    scale = np.power(10,9)\n    \n    for x in args:\n        yield x*scale","1d33fdc6":"print(\"Start_Program\")","5b782e86":"\"\"\"Choose which version of the data set should be used\n- reduce: set this option equal true, if you want to use just the upper left quarter of the\n          dataset containing the full 64x64 matrice\n          \n          if set to false the full matrice will be used to train the model\n- quarter: set this value to true, if you wish to use the dataset containing just the upper\n           left quarter of the matrice in higher resolution (64x64 for the quarter)\n\"\"\"\nreduce_ = False\nquarter = True\n#abort if both options are equal true\nassert not (quarter and reduce_)\n#set the file path\n#if working on kaggle\nx_file_path = \"..\/data-input\/20191205\/cloak_mat_cs64{}_2.csv\".format(\"\" if quarter else \"full\")\n#if working on local computer (not recommend, unless you can use GPU, oterwise it will be really slow)\n#x_file_path = \"..\/data\/cloak_mat_cs64{}_2.csv\".format(\"\" if quarter else \"full\")\n\n#setting file name extensions for plots and pickled lists\nannotation = \"_quarter_64\" if quarter else \"\"\nreduce_str = \"_reduced\" if reduce_ else \"\"","dbf12636":"# Load data (each row corresponds to one sample)\nx_train = np.loadtxt('..\/input\/quarter\/x_train{}.csv'.format(annotation + reduce_str), dtype=np.float64, delimiter=',')\nx_test  = np.loadtxt('..\/input\/quarter\/x_test{}.csv'.format( annotation + reduce_str), dtype=np.float64, delimiter=',')\n\n# Reshape x to recover its 2D content\nside_length = 32 if reduce_ else 64\nx_train = x_train.reshape(x_train.shape[0], side_length, side_length, 1)\nx_test = x_test.reshape(x_test.shape[0], side_length, side_length, 1)\nprint(x_train.shape)\nprint(x_test.shape)\n\n# Load labels:\ny_train = np.loadtxt('..\/input\/quarter\/y_train.csv', dtype=np.float64, delimiter=',')\ny_test = np.loadtxt('..\/input\/quarter\/y_test.csv', dtype=np.float64, delimiter=',')\n\n# Transform the labels y so that min(y) == 0 and max(y) == 1. Importantly, y_train and y_test must be considered jointly.\ny_train, y_test = standardize_scale(y_train, y_test)","e274f004":"from tensorflow.keras import datasets, layers, models\nfrom tensorflow import keras\nimport numpy as np\nfrom keras.layers import LeakyReLU\n\n#activation_fct = \"relu\" #\"leakyrelu\nnb_epochs = 50\n\n#fixed values from first exploratory search:\nn_filters = 15\nk_sizes = 5\nnb_layers = 4\ndense_neurons = 1\n\n#fixed argument found in second grid search\n#dim 0\nactivation_list = [\"sigmoid\"]\n#dim 1\nfilter_mult_list = [3]\n#dim 2\nactivation_last_list = [\"relu\"]\n#dim 3\ndim3_list = [1]\n\n#define 4 4D Vectors to store test and train error (mae and mse)\nrmse_test = np.ones((len(activation_list),len(filter_mult_list),len(activation_last_list),len(dim3_list)))\nrmse_train = np.ones((len(activation_list),len(filter_mult_list),len(activation_last_list),len(dim3_list)))\n\nrmae_test = np.ones((len(activation_list),len(filter_mult_list),len(activation_last_list),len(dim3_list)))\nrmae_train = np.ones((len(activation_list),len(filter_mult_list),len(activation_last_list),len(dim3_list)))\n\n## keep structure of old program, but as all list have a single element no iteration will be done\n\n#iterate over dim 0\nfor dim0, activation_fct in enumerate(activation_list):\n    print(\"entering_loop_of_dim_0_with_activation_equal_{}\".format(activation_fct))\n    \n    for dim1, filter_mult in enumerate(filter_mult_list):\n        print(\"entering_loop_of_dim_1_with_filter_mult_equal_{}\".format(filter_mult))\n        \n        for dim2,activation_last in enumerate(activation_last_list):\n            print(\"entering_loop_of_dim_2_with_activation_last_equal_{}\".format(activation_last))\n            \n            for dim3, b in enumerate(dim3_list):\n                print(\"entering_loop_of_dim_3_with_dim3_equal_{}\".format(b))\n                \n                # defining the neural network\n                print(\"build_model\")             \n                model = models.Sequential()\n                \n                #add first convolutional layer\n                \"\"\"if reduce_:\n                    model.add(layers.Conv2D(n_filters, kernel_size =(k_sizes, k_sizes), strides=(1, 1), activation=activation_fct, input_shape=(32, 32, 1), padding = \"same\"))\n                else:\"\"\"\n                if(activation_fct == \"leakyrelu.1\"):\n                    model.add(layers.Conv2D(n_filters, kernel_size =(k_sizes, k_sizes), strides=(1, 1), input_shape=(64, 64, 1), padding = \"same\"))\n                    keras.layers.LeakyReLU(alpha=0.1)\n                elif(activation_fct == \"leakyrelu.2\"):\n                    model.add(layers.Conv2D(n_filters, kernel_size =(k_sizes, k_sizes), strides=(1, 1), input_shape=(64, 64, 1), padding = \"same\"))\n                    keras.layers.LeakyReLU(alpha=0.2)\n                elif(activation_fct == \"leakyrelu.4\"):\n                    model.add(layers.Conv2D(n_filters, kernel_size =(k_sizes, k_sizes), strides=(1, 1), input_shape=(64, 64, 1), padding = \"same\"))\n                    keras.layers.LeakyReLU(alpha=0.4)\n                else:\n                    model.add(layers.Conv2D(n_filters, kernel_size =(k_sizes, k_sizes), strides=(1, 1), activation=activation_fct, input_shape=(64, 64, 1), padding = \"same\"))\n                \n                layer = 1\n                while layer < nb_layers:\n                    # reduce image size by maxpooling\n                    model.add(layers.MaxPooling2D(pool_size =(2, 2), strides =(2, 2)))\n                    # add another convolutional layer\n                    if(activation_fct == \"leakyrelu.1\"):\n                        model.add(layers.Conv2D(int(n_filters*filter_mult), kernel_size =(k_sizes, k_sizes), strides=(1, 1), padding = \"same\"))            \n                        keras.layers.LeakyReLU(alpha=0.1)\n                    elif(activation_fct == \"leakyrelu.2\"):\n                        model.add(layers.Conv2D(int(n_filters*filter_mult), kernel_size =(k_sizes, k_sizes), strides=(1, 1), padding = \"same\"))            \n                        keras.layers.LeakyReLU(alpha=0.2)\n                    elif(activation_fct == \"leakyrelu.4\"):\n                        model.add(layers.Conv2D(int(n_filters*filter_mult), kernel_size =(k_sizes, k_sizes), strides=(1, 1), padding = \"same\"))            \n                        keras.layers.LeakyReLU(alpha=0.4)\n                    else:\n                        model.add(layers.Conv2D(int(n_filters*filter_mult), kernel_size =(k_sizes, k_sizes), strides=(1, 1), activation=activation_fct, padding = \"same\"))\n                    layer += 1;\n                    \n                #add a dense neural network at the end\n                model.add(layers.Flatten()) \n                #add a layer with a single neuron\n                if(activation_last == \"leakyrelu.1\"):\n                    model.add(layers.Dense(1))\n                    keras.layers.LeakyReLU(alpha=0.1)\n                elif(activation_last == \"leakyrelu.2\"):\n                    model.add(layers.Dense(1))\n                    keras.layers.LeakyReLU(alpha=0.2)\n                elif(activation_last == \"leakyrelu.4\"):\n                    model.add(layers.Dense(1))\n                    keras.layers.LeakyReLU(alpha=0.4)\n                else:\n                    model.add(layers.Dense(1, activation=activation_last))\n                \n                print(\"copile_model:\")\n                #run model and save results in vector\n                model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error', 'mae']) \n        \n                #print(model.summary())\n            \n                print(\"sucessfully_compiled_fit:\")\n                history = model.fit(x_train, y_train, epochs = nb_epochs)\n                \n                #rmse_test[dim0][dim1][dim2][dim3] = np.sqrt(history.history['val_mean_squared_error'][-1])\n                #rmse_train[dim0][dim1][dim2][dim3] = np.sqrt(history.history['mean_squared_error'][-1])\n                \n                #rmae_test[dim0][dim1][dim2][dim3] = np.sqrt(history.history['val_mae'][-1])\n                #rmae_train[dim0][dim1][dim2][dim3] = np.sqrt(history.history['mae'][-1])\n\n\n","867a5260":"\"\"\"evaluate the model on the test set\nThis values have never been used to train or validate the model and give thus and\nunbiased result\n\"\"\"\nmse_loss, mse_metric, mae_metric = model.evaluate(x_test, y_test)\nprint(np.isclose(mse_loss, mse_metric))\nprint(\"RMSE on test set:\", np.sqrt(mse_metric))\nprint(\"MAE on test set: \", mae_metric)\n","e795fc7f":"print(\"end_of_code\")","798e1c66":"#generate a zip of the produced figures\nimport shutil\nshutil.make_archive(\"\/kaggle\/working\/output_figures_RMSE\", 'zip', TARGET_DIR)","58b630a7":"### Model definition","01db4a2a":"### Pre-process, split into test and train, etc. (run once)","9ed26489":"## Model","a6ac67ed":"# Convolutional neural network"}}