{"cell_type":{"b1328273":"code","7581ab23":"code","79b8275c":"code","60f2b40d":"code","27636fbd":"code","a64866a0":"code","4f236d50":"code","2d7dd989":"code","eb25bddb":"code","954c024f":"code","de377abc":"code","1df5a204":"code","8dc2849d":"code","8ad43b89":"markdown","bf695971":"markdown","c0e593a1":"markdown","52c140ba":"markdown","34f3ba62":"markdown","af4c4531":"markdown","3f49b17a":"markdown","8b3f2a3d":"markdown","f3d97b9a":"markdown","53f9255c":"markdown"},"source":{"b1328273":"import numpy as np \nimport pandas as pd\nimport nltk\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom toolz import compose\nfrom itertools import combinations\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error","7581ab23":"## train_data\ntrain_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\n\n##test_data\ntest_data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\n\n\ntrain_data.head()","79b8275c":"fig,axes = plt.subplots(nrows=1,ncols=2,figsize=(30,10))\nsns.histplot(train_data['target'],ax=axes[0],kde=True)\nsns.histplot(train_data['standard_error'],ax=axes[1],kde=True)\n\naxes[0].set(title='Target distribution')\naxes[0].set(title='Standard error distribution')","60f2b40d":"## drop unnecessary columns\ntrain_data.drop(['url_legal','license','id'],inplace=True,axis=1)","27636fbd":"X = train_data['excerpt']\ny = train_data['target']\n\n## splitting into train and test\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42)\n","a64866a0":"### code to create skipgrams\nclass SkipGramVectorizer(CountVectorizer):\n    \n    def __init__(self, k=1, **kwds):\n        super(SkipGramVectorizer, self).__init__(**kwds)\n        self.k=k\n    \n    def build_sent_analyser(self, preprocess, stop_words, tokenize):\n        return lambda sent : self._word_skip_grams(compose(tokenize, preprocess, self.decode)(sent),stop_words)\n    \n    def build_analyzer(self):    \n        preprocess = self.build_preprocessor()\n        stop_words = self.get_stop_words()\n        tokenize = self.build_tokenizer()\n        sent_analyse = self.build_sent_analyser(preprocess, stop_words, tokenize)\n        return lambda doc : self._sent_skip_grams(doc, sent_analyse)\n    \n    def _sent_skip_grams(self, doc, sent_analyze):\n        skip_grams = []\n        for sent in nltk.sent_tokenize(doc):\n            skip_grams.extend(sent_analyze(sent))\n        return skip_grams\n    \n    def _word_skip_grams(self, tokens, stop_words=None):\n        # handle stop words\n        if stop_words is not None:\n            tokens = [w for w in tokens if w not in stop_words]\n        \n        min_n, max_n = self.ngram_range\n        k = self.k\n        if max_n != 1:\n            original_tokens = tokens\n            if min_n == 1:\n                # no need to do any slicing for unigrams\n                # just iterate through the original tokens\n                tokens = list(original_tokens)\n                min_n += 1\n            else:\n                tokens = []\n\n            n_original_tokens = len(original_tokens)\n\n            # bind method outside of loop to reduce overhead\n            tokens_append = tokens.append\n            space_join = \" \".join\n\n            for n in range(min_n,min(max_n + 1, n_original_tokens + 1)):\n                for i in range(n_original_tokens - n + 1):\n                    # k-skip-n-grams\n                    head = [original_tokens[i]]                    \n                    for skip_tail in combinations(original_tokens[i+1:i+n+k], n-1):\n                        tokens_append(space_join(head + list(skip_tail)))\n        return tokens","4f236d50":"## tokenizer function\n\nlemm = nltk.wordnet.WordNetLemmatizer()\ndef tokenizer(text):\n    text = text.lower()\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    toks = nltk.tokenize.word_tokenize(text)\n    tok_ret = []\n    for tok in toks:\n        if tok != \"\":\n            tok_ret.append(tok)\n    return tok_ret","2d7dd989":"## english stopwords\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))","eb25bddb":"from sklearn.pipeline import Pipeline\npipe = Pipeline([\n                    ('vectorizer',SkipGramVectorizer(ngram_range=(1,3), k=1,stop_words=stop_words,tokenizer = tokenizer)),\n                    ('clf',LinearRegression())\n                \n                ])\npipe.fit(X_train,y_train)\npreds = pipe.predict(X_test)","954c024f":"print(\"MSE : \" + str(mean_squared_error(y_test,preds)))\nprint(\"\\n\")\nprint(\"RMSE : \" + str(np.sqrt(mean_squared_error(y_test,preds))))","de377abc":"all_v = pipe[0].transform(X_train).toarray().sum(axis=0)\nall_grams = {}\nfor gram,value in zip(pipe[0].get_feature_names(),all_v):\n    \n    total_grams = len(gram.split(\" \"))\n    if total_grams not in all_grams:\n        all_grams[total_grams] = []\n        \n    all_grams[total_grams].append((gram,value))\n\nfig,axes = plt.subplots(2,2,figsize=(30,15))\nfig.tight_layout(w_pad=20, h_pad=5)\nrow = 0\nfor index,key in enumerate(all_grams.keys()):\n    \n    sorted_grams = sorted(all_grams[key],key = lambda x : x[1],reverse=True)[:10]\n    \n    x = [tup[0] for tup in sorted_grams]\n    y = [tup[1] for tup in sorted_grams]\n    \n    if index % 2 == 0 and index!=0:\n        row+=1\n    ax = sns.barplot(x=y,y=x,ax=axes[row][index%2],palette='plasma',orient='h')\n    ax.set(title = \"Top \" + str(key) + \" grams\")\n    #ax.set_xticklabels(labels=x,rotation=45,ha='right')","1df5a204":"test_data['target'] = pipe.predict(test_data['excerpt'])\ntest_data.drop(['url_legal','license','excerpt'],inplace=True,axis=1)\ntest_data.to_csv('\/kaggle\/working\/submission.csv',index=False)\n","8dc2849d":"test_data.head()","8ad43b89":"# <center> CommonLit Readability Prize <\/center>\n ","bf695971":"#### Preprocessing \n* I am writing a tokeniser function which - \n  * uses word tokenize\n  * keeps only alphabets\n  * lemmatizes all the tokens.","c0e593a1":"#### **Building model using Sklearn Pipeline**\n\n<br>\n\n* We will be using Linear Regression as a baseline to predict the grading difficuly.","52c140ba":"#### Submission File","34f3ba62":"\n* To-Do\n  * To predict the complexity of reading passages for grade 3-12 classroom use.\n  \n  \n* About data - \n >  * id - unique ID for excerpt\n >  * url_legal - URL of source - this is blank in the test set.\n >  * license - license of source material - this is blank in the test set.\n >  * excerpt - text to predict reading ease of\n >  * target - reading ease\n >  * standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n \n \n* Special Notes - \n * url_legal, license and standard error are not available for test data.","af4c4531":"### EDA","3f49b17a":"### **Imports**","8b3f2a3d":"#### Addition of Skip-Grams\n\n* Instead of using normal n-grams (continuous), I will be overriding sklearn's countvectorizer to give us skip-grams. There are two parameters required - \n  * **ngram_range** - is a tuple of (min_number, max_number) which represents.\n    * min_number - minimum number of words you need (if 1 - unigram, 2 - bigram etc etc.)\n    * max_number - maximum number of words you need (if 1 - unigram, 2 - bigram etc etc.)\n    \n  <br>\n    \n  * **k** - number of skips in between\n  \n  * Ex - if sentence is **The forecast says there will be rain tomorrow.**, **k = 2** and **ngram_range = (1,3)**. Then, tokens would be - \n  \n  ['be',\n 'be rain',\n 'be rain tomorrow',\n 'be tomorrow',\n 'forecast',\n 'forecast says',\n 'forecast says be',\n 'forecast says there',\n 'forecast says will',\n 'forecast there',\n 'forecast there be',\n 'forecast there will',\n 'forecast will',\n 'forecast will be',\n 'rain',\n 'rain tomorrow',\n 'says',\n 'says be',\n 'says be rain',\n 'says there',\n 'says there be',\n 'says there rain',\n 'says there will',\n 'says will',\n 'says will be',\n 'says will rain',\n 'the',\n 'the forecast',\n 'the forecast says',\n 'the forecast there',\n 'the forecast will',\n 'the says',\n 'the says there',\n 'the says will',\n 'the there',\n 'the there will',\n 'there',\n 'there be',\n 'there be rain',\n 'there be tomorrow',\n 'there rain',\n 'there rain tomorrow',\n 'there will',\n 'there will be',\n 'there will rain',\n 'there will tomorrow',\n 'tomorrow',\n 'will',\n 'will be',\n 'will be rain',\n 'will be tomorrow',\n 'will rain',\n 'will rain tomorrow',\n 'will tomorrow']","f3d97b9a":"### Reading Data","53f9255c":"#### **Plotting top words**"}}