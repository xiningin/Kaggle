{"cell_type":{"74a073cd":"code","5bd06026":"code","7b7e2078":"code","680ab183":"code","20ef438a":"code","639855d6":"code","e2062dd7":"code","71683091":"code","6a42072d":"code","955b3fb2":"code","a0deb38c":"code","52716093":"code","70b9b1c1":"code","f8d3e64e":"code","a4d7b035":"code","4860da84":"code","db00d5e3":"code","3a12f69c":"code","2a8ab7b9":"code","00dde5e9":"code","36ac952a":"code","92ab6bd9":"code","4d0ecdb2":"code","8985fa1d":"code","1645dbb3":"code","465940cd":"code","a1be23d5":"code","06e83da3":"code","2282e113":"code","2c26ede1":"code","36383c40":"code","36bc3ea7":"code","978dc458":"code","737be567":"code","71516c93":"code","0260f0e7":"code","f07ef438":"code","5121ca9e":"code","1039f224":"code","14736aa2":"code","a73b4de5":"code","d11b495e":"code","ef2ef06d":"code","49b16ffc":"code","5148d379":"code","aae4c949":"code","2df9b6d7":"code","1572ac57":"code","e2dfc4cb":"code","b9804716":"code","65313466":"code","e61dfe4d":"code","ef289443":"code","68289d44":"code","b9a2f079":"code","e2351a3f":"code","230c94c5":"code","bfb0ffb4":"code","99e32f09":"code","db6e8fa8":"code","d42106c7":"code","8e5c0c1c":"code","68e73d03":"code","801e0163":"code","f35812fe":"code","dd375f86":"code","9c364904":"code","6ff4df7d":"code","3d3adb03":"code","ef4a1e54":"code","550dd7d9":"code","8362fe77":"code","d0e598d8":"code","e16abe5b":"code","2ed73724":"code","18423230":"code","3e7d4380":"code","f0a5b449":"code","fb052ebe":"code","da05271f":"code","d92d6d63":"markdown","bdde10e6":"markdown","33815ca8":"markdown","9d98222b":"markdown","73646594":"markdown","7aa5cf6b":"markdown","2a87bea9":"markdown","5f777ba9":"markdown","00f9ccc9":"markdown","8f4a15f5":"markdown","7bf32404":"markdown","b442a188":"markdown","f7c0e418":"markdown","ddd208be":"markdown","898bdb55":"markdown","023a3872":"markdown","2de95250":"markdown","5095cd53":"markdown","c1a89c3b":"markdown","925101d4":"markdown","dfe72cd4":"markdown","8bf7a252":"markdown","1f715a8b":"markdown","c3182e0c":"markdown","daa7c793":"markdown","48daff48":"markdown","40211d99":"markdown","d57e2d27":"markdown","ef9bb512":"markdown","2d972a87":"markdown","82f1bc4d":"markdown","2ee990b5":"markdown","3226f8af":"markdown","4c6b204a":"markdown","b548efa2":"markdown","c8e676c9":"markdown","2c4fa5f3":"markdown","a3a20284":"markdown","3140fee5":"markdown","4634971e":"markdown","aebea667":"markdown","66ce4a8a":"markdown","108da236":"markdown","3fea4593":"markdown","ed987c87":"markdown","05d59d67":"markdown","77f4228c":"markdown","8691e4fd":"markdown","64df7bbe":"markdown","74650262":"markdown","614cf858":"markdown","b570fb5e":"markdown","efda022b":"markdown","e82384b6":"markdown","a323ddfb":"markdown","cb1a0987":"markdown","ba877ffb":"markdown","97b64896":"markdown","71ec72d3":"markdown","8b26e7d0":"markdown"},"source":{"74a073cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd\nfrom imblearn.over_sampling import ADASYN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras import backend as K\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom imblearn.over_sampling import SMOTE\n#from imblearn.over_sampling import BorderlineSMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer,f1_score\nimport sklearn\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras import regularizers\nfrom keras.callbacks import EarlyStopping\n#from keras_lr_finder import LRFinder","5bd06026":"data = pd.read_csv('..\/input\/creditcard.csv')\ndf = data.copy()","7b7e2078":"df.head()","680ab183":"df.dtypes","20ef438a":"df.columns","639855d6":"df.describe()","e2062dd7":"df.isnull().sum()","71683091":"print(df[df['Class']==1].shape)\nprint(df[df['Class']==0].shape)","6a42072d":"sns.countplot(x='Class', data=df);\nplt.title('0: Non-Fraud Transaction\\n1: Fraud Transaction', fontsize=14)","955b3fb2":"print('Non-Frauds Transaction', round(len(df[df['Class']==0])\/len(df)*100,2), '% of the dataset')\nprint('Frauds Transaction', round(len(df[df['Class']==1])\/len(df)*100,2), '% of the dataset')","a0deb38c":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","52716093":"Fraud = data[data['Class']==1]\n\nNormal = data[data['Class']==0]\n\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(Fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(Normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","70b9b1c1":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount by class')\nax1.scatter(Fraud.Time, Fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(Normal.Time, Normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","f8d3e64e":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\" Correlation Matrix\", fontsize=14)","a4d7b035":"print(df.corr())","4860da84":"plt.figure(figsize=(20,20))\nsns.boxplot(data = df)\nplt.show()","db00d5e3":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1","3a12f69c":"((df < (Q1 - 3 * IQR)) | ( df> (Q3 + 3 * IQR))).sum()","2a8ab7b9":"#Select only the  PCA transformed features.\nv_features = df.iloc[:,1:29].columns","00dde5e9":"plt.figure(figsize=(12,28*4))\ngs = gridspec.GridSpec(28, 1)\nfor i, cn in enumerate(df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(df[cn][df.Class == 1], bins=50)\n    sns.distplot(df[cn][df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    plt.legend(df[\"Class\"])\n    ax.set_title('histogram of feature: ' + str(cn))\n    \nplt.show()","36ac952a":"print(df.var())","92ab6bd9":"rob_scaler = RobustScaler()","4d0ecdb2":"df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n#df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","8985fa1d":"scaled_amount = df['scaled_amount']\n\ndf.drop(['scaled_amount'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)","1645dbb3":"print(df.var())","465940cd":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\" Correlation Matrix\", fontsize=14)","a1be23d5":"df.head()","06e83da3":"X = df.drop('Class', axis=1)\ny = df['Class']","2282e113":"df['Class'].value_counts()","2c26ede1":"X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=123,stratify = y)","36383c40":"y_train_val.value_counts()","36bc3ea7":"y_test.value_counts()","978dc458":"X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=123, stratify = y_train_val)","737be567":"y_val.value_counts()","71516c93":"y_train.value_counts()","0260f0e7":"X_train.shape","f07ef438":"X_val.shape","5121ca9e":"y_val.value_counts()","1039f224":"y_train.value_counts()","14736aa2":"#from imblearn.under_sampling import RandomUnderSampler","a73b4de5":"#rus = RandomUnderSampler(random_state = 42)","d11b495e":"# #An undersampled dataset is also created to tune the parameters\n# Ytrain_df = pd.DataFrame(y_train)\n# Ytrain_df = Ytrain_df.rename(columns={0: 'Class'})\n# Train_set= pd.merge(X_train, Ytrain_df, how='inner', left_index=True, right_index=True)\n# Train_set.head()\n\n# Train_set.shape\n\n# length_frauds=len(Train_set[Train_set.Class==1])\n# length_frauds\n\n# fraud_df= Train_set.loc[Train_set['Class'] == 1]\n# fraud_df.head()\n\n# non_fraud_df= Train_set.loc[Train_set['Class'] == 0]\n# #Randomly select the same number of records as the Frauds\n# non_frauds=non_fraud_df.sample(n=length_frauds)\n# non_frauds.shape\n\n# non_frauds.head()\n\n# #Appending both Frauds and Non-Frauds\n# undersampled_df= fraud_df.append(non_frauds)\n# #Randomly shuffling all the instances\n# undersampled_df = sklearn.utils.shuffle(undersampled_df)\n# undersampled_df.head()\n\n# X_undersample = undersampled_df.drop('Class', axis=1)\n# Y_undersample = undersampled_df['Class']\n# print(X_undersample.shape)\n# print(Y_undersample.shape)\n# X_undersample.head()\n\n# X_undersample.to_csv('X_undersample.csv', index=False)\n# pd.DataFrame(Y_undersample).to_csv('Y_undersample.csv', index=False)","ef2ef06d":"#X_undersample, Y_undersample = rus.fit_resample(X_train, y_train)","49b16ffc":"# print(X_undersample.shape)","5148d379":"# Y_undersample.value_counts()","aae4c949":"# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import GridSearchCV\n# from sklearn.metrics import make_scorer, f1_score","2df9b6d7":"# def create_model(nodes,init_mode):\n#     model = Sequential()\n#     model.add(Dense(nodes, input_dim = len(X.columns), kernel_initializer = init_mode,activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(1,activation='sigmoid'))\n#     model.compile(keras.optimizers.Adam, loss='binary_crossentropy')\n#     return(model)\n\n# model = KerasClassifier(build_fn = create_model,batch_size = 512,epochs = 30, verbose = 1)\n# nodes = [29,35,40,45,50,55,64]\n# init_mode = ['he_normal','he_uniform','uniform','zero']\n# param_grid = dict(nodes = nodes, init_mode = init_mode)\n# grid = GridSearchCV(estimator = model, param_grid = param_grid,scoring = make_scorer(f1_score))\n# grid_res = grid.fit(X_undersample,Y_undersample)","1572ac57":"# print(\"Best: %f using %s\" % (grid_res.best_score_, grid_res.best_params_))\n# means = grid_res.cv_results_['mean_test_score']\n# stds = grid_res.cv_results_['std_test_score']\n# params = grid_res.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","e2dfc4cb":"sm = SMOTE(random_state=2)\nX_train_resample, y_train_resample = sm.fit_sample(X_train, y_train)","b9804716":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_resample.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_resample.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_resample==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_resample==0)))","65313466":"model = keras.Sequential()\nmodel.add(Dense(64, input_dim=len(X.columns), kernel_initializer = 'he_normal',activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","e61dfe4d":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","ef289443":"from matplotlib import pyplot as plt\nimport math\nfrom keras.callbacks import LambdaCallback\nimport keras.backend as K\nclass LRFinder:\n    \"\"\"\n    Plots the change of the loss function of a Keras model when the learning rate is exponentially increasing.\n    See for details:\n    https:\/\/towardsdatascience.com\/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n        self.losses = []\n        self.lrs = []\n        self.best_loss = 1e9\n\n    def on_batch_end(self, batch, logs):\n        # Log the learning rate\n        lr = K.get_value(self.model.optimizer.lr)\n        self.lrs.append(lr)\n\n        # Log the loss\n        loss = logs['loss']\n        self.losses.append(loss)\n\n        # Check whether the loss got too large or NaN\n        if math.isnan(loss) or loss > self.best_loss * 4:\n            self.model.stop_training = True\n            return\n\n        if loss < self.best_loss:\n            self.best_loss = loss\n\n        # Increase the learning rate for the next batch\n        lr *= self.lr_mult\n        K.set_value(self.model.optimizer.lr, lr)\n\n    def find(self, x_train, y_train, start_lr, end_lr, batch_size=64, epochs=1):\n        num_batches = epochs * x_train.shape[0] \/ batch_size\n        self.lr_mult = (end_lr \/ start_lr) ** (1 \/ num_batches)\n\n        # Save weights into a file\n        self.model.save_weights('tmp.h5')\n\n        # Remember the original learning rate\n        original_lr = K.get_value(self.model.optimizer.lr)\n\n        # Set the initial learning rate\n        K.set_value(self.model.optimizer.lr, start_lr)\n\n        callback = LambdaCallback(on_batch_end=lambda batch, logs: self.on_batch_end(batch, logs))\n\n        self.model.fit(x_train, y_train,\n                        batch_size=batch_size, epochs=epochs,\n                        callbacks=[callback])\n\n        # Restore the weights to the state before model fitting\n        self.model.load_weights('tmp.h5')\n\n        # Restore the original learning rate\n        K.set_value(self.model.optimizer.lr, original_lr)\n\n    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n        \"\"\"\n        Plots the loss.\n        Parameters:\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n        \"\"\"\n        plt.ylabel(\"loss\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n\n    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n        \"\"\"\n        Plots rate of change of the loss function.\n        Parameters:\n            sma - number of batches for simple moving average to smooth out the curve.\n            n_skip_beginning - number of batches to skip on the left.\n            n_skip_end - number of batches to skip on the right.\n            y_lim - limits for the y axis.\n        \"\"\"\n        assert sma >= 1\n        derivatives = [0] * sma\n        for i in range(sma, len(self.lrs)):\n            derivative = (self.losses[i] - self.losses[i - sma]) \/ sma\n            derivatives.append(derivative)\n\n        plt.ylabel(\"rate of loss change\")\n        plt.xlabel(\"learning rate (log scale)\")\n        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n        plt.xscale('log')\n        plt.ylim(y_lim)","68289d44":"lr_finder = LRFinder(model)\nlr_finder.find(X_train, y_train, 0.0001, 1, 512, 5)","b9a2f079":"lr_finder.plot_loss()\nplt.show()","e2351a3f":"callbacks = [EarlyStopping(monitor='val_loss',mode='min',patience=2,restore_best_weights=True)]","230c94c5":"    model = Sequential()\n    model.add(Dense(64, input_dim=len(X.columns),kernel_initializer = 'he_normal', activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))  \n    model.compile(keras.optimizers.Adam(lr=0.001),loss = 'binary_crossentropy',metrics=['accuracy'])\n    model.summary()","bfb0ffb4":"results_control_arm_f1 = []\n#run_count =0;\n#check = 0.0;\nfor i in range(0,70):\n    model = Sequential()\n    model.add(Dense(64, input_dim=len(X.columns),kernel_initializer = 'he_normal', activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))  \n    model.compile(keras.optimizers.Adam(lr=0.001),loss = 'binary_crossentropy',metrics=['accuracy'])\n    history = model.fit(X_train,y_train,callbacks=callbacks,epochs=30,validation_data=(X_val,y_val),batch_size=512)\n    print(i)\n    y_test_pred = model.predict(X_test) > 0.5\n    f1 = f1_score(y_test,y_test_pred)  \n    results_control_arm_f1.append(f1)","99e32f09":"print(results_control_arm_f1)\nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(results_control_arm_f1), np.std(results_control_arm_f1)))","db6e8fa8":"print(np.mean(results_control_arm_f1))","d42106c7":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n","8e5c0c1c":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","68e73d03":"from sklearn.metrics import confusion_matrix\ny_test_pred= model.predict(X_test) > 0.5\ncm = confusion_matrix(y_test, y_test_pred)\nprint(cm)","801e0163":"sns.heatmap(cm, annot=True, fmt='.0f')\nplt.show()","f35812fe":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_test_pred))","dd375f86":"from sklearn.metrics import roc_curve,auc\nfpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\nplt.plot ([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label = 'Deep Learning')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Deep Learning')\nprint('Control Arm Area under the curve = '+ str(auc(fpr, tpr)))\nplt.show()","9c364904":"model = Sequential()\nmodel.add(Dense(64, input_dim=len(X.columns),kernel_initializer = 'he_normal', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64,kernel_initializer = 'he_normal', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))  \nmodel.compile(keras.optimizers.Adam(lr=0.001),loss = 'binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","6ff4df7d":"results_experiment_arm_f1 = []\n#run_count =0;\n#check = 0.0;\nfor i in range(0,70):\n    model = Sequential()\n    model.add(Dense(64, input_dim=len(X.columns),kernel_initializer = 'he_normal', activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(64,kernel_initializer = 'he_normal', activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='sigmoid'))  \n    model.compile(keras.optimizers.Adam(lr=0.001),loss = 'binary_crossentropy',metrics=['accuracy'])\n    history = model.fit(X_train,y_train,callbacks=callbacks,epochs=30,validation_data=(X_val,y_val),batch_size=512)\n    print('Value'+str(i))\n    y_test_pred = model.predict(X_test) > 0.5\n    f1 = f1_score(y_test,y_test_pred)  \n    results_experiment_arm_f1.append(f1)\n    \nprint(results_experiment_arm_f1)\nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(results_experiment_arm_f1), np.std(results_experiment_arm_f1)))\n","3d3adb03":"print(results_experiment_arm_f1)\nprint(np.mean(results_experiment_arm_f1))","ef4a1e54":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()","550dd7d9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","8362fe77":"from sklearn.metrics import confusion_matrix\ny_test_pred= model.predict(X_test) > 0.5\ncm = confusion_matrix(y_test, y_test_pred)\nprint(cm)","d0e598d8":"sns.heatmap(cm, annot=True, fmt='.0f')\nplt.show()","e16abe5b":"from sklearn.metrics import roc_curve,auc\nfpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\nplt.plot ([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label = 'Deep Learning')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Deep Learning')\nprint('Experimental Arm Area under the curve = '+ str(auc(fpr, tpr)))\nplt.show()","2ed73724":"mean_control_f1 = pd.DataFrame(results_control_arm_f1).mean()\nprint(\"Mean Control F1: {}\".format(mean_control_f1))\n\nmean_experimental_f1 = pd.DataFrame(results_experiment_arm_f1).mean()\nprint(\"Mean Experimental F1: {}\".format(mean_experimental_f1))","18423230":"std_control_f1 = pd.DataFrame(results_control_arm_f1).std()\nprint(\"Standard Deviation of Control F1 Results: {}\".format(std_control_f1))\n\nstd_experimental_f1 = pd.DataFrame(results_experiment_arm_f1).std()\nprint(\"Standard Deviation of Experimental F1 Results: {}\".format(std_experimental_f1))","3e7d4380":"results_f1= pd.concat([pd.DataFrame(results_control_arm_f1), pd.DataFrame(results_experiment_arm_f1)], axis=1)\nresults_f1.columns = ['Control', 'Experimental']","f0a5b449":"results_f1.hist(density=True)","fb052ebe":"from scipy import stats\n\nalpha = 0.05;\n\ns, p = stats.normaltest(results_control_arm_f1)\nif p < alpha:\n  print('Control data is not normal')\nelse:\n  print('Control data is normal')\n\ns, p = stats.normaltest(pd.DataFrame(results_experiment_arm_f1))\nif p < alpha:\n  print('Experimental data is not normal')\nelse:\n  print('Experimental data is normal')","da05271f":"from scipy.stats import wilcoxon\ns, p = stats.wilcoxon(pd.DataFrame(results_experiment_arm_f1)[0], pd.DataFrame(results_control_arm_f1)[0])\n\nif p < 0.05:\n  print('null hypothesis rejected, significant difference between the data-sets')\nelse:\n  print('null hypothesis accepted, no significant difference between the data-sets')","d92d6d63":"Confusion Matrix","bdde10e6":"**H0  the null hypothesis:** insufficient evidence to support hypothesis.\n\n**H1 the alternate hypothesis**: evidence suggests the hypothesis is likely true.","33815ca8":"## Data Analyis ","9d98222b":"In the experiment if there is a significant affect on the performance of the neural network by changing the number of hidden layer, then H1 hypothesis will be accepted otherwise rejected.","73646594":"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. The early  stopping will monitor the validation loss and will stop the epoch as soon as minimum loss is found or loss has increased after decreasing.","7aa5cf6b":"The model is a experimental arm in the experiment. It consists of 1 input layer with 29 input neurons, 2 hidden layer each with 64 neurons and 1 output layer with 1 neuron. For initial weights, he_normal tectnique is used. The dropout rate is 0.5, which means that in each epoch  50% of the network neuron will be droped out for model regularization and prevent overfitting. Adam optimizaer with Learning rate is 0.001 is used","2a87bea9":"Credit card dataset is divided into 3 dataset. Training dataset, validation dataset and testing dataset.  ","5f777ba9":"## Distribution of the data","00f9ccc9":"## Result Analysis","8f4a15f5":"## Over Sampling","7bf32404":"Due to high imbalance dataset, we will be using SMOTE for balancing the training dataset Synthetic Minority Oversampling (SMOTE) works by creating synthetic observations based upon the existing minority observations (Chawla et al., 2002).  SMOTE will not be applied to test and validation dataset as it will cause over fitting. ","b442a188":"The model is a control arm in the experiment. It consists of 1 input layer with 29 input neurons, 1 hidden layer with 64 neurons and 1 output layer with 1 neuron. For initial weights, he_normal tectnique is used. The dropout rate is 0.5, which means that in each epoch  50% of the network neuron will be droped out for model regularization and prevent overfitting. Adam optimizaer with Learning rate is 0.001 is used","f7c0e418":"For every iteration( 0 - 70), the model produces a different or almost similar f1_scorce that is appended in results_control_arm_f1 array. Then the average of of the f1_score is calculated.","ddd208be":"Normal (orange) and fraud (blue) transaction show some differences in distribution in different  features. But none of the features can completely separate the two types of transactions.\n\nI have decided not to exclude any feature for neural network as more features generally helps neural network accuracy.","898bdb55":"## Experimental Design","023a3872":"# Implementation ","2de95250":"The standard deviation of the results in each arm of the experiment","5095cd53":"### Loading Data","c1a89c3b":"## PCA Transformed Features\n### Next, let's take a look at the PCA transformed features.","925101d4":"Time and Amount feature are  skewed ","dfe72cd4":"First the credit card dataset is divided into training dataset and testing dataset. 80% of the dataset is training dataset and 20% dataset is testing dataset. Due to high imbalance dataset, stratify is used to equaly split the ratio of each class ( 0 and 1 ) in training dataset and testing dataset. ","8bf7a252":"After applying robust scaler on amount feature (scaled_amount) , the variance in amount feature (scaled_amount) has decreased.","1f715a8b":"### Control Arm Neural Network","c3182e0c":"scipy.stats.normaltest() function will return two variables, one called the statistic and the p-value. The p-value which is the probability of the hypothesis test. A p-value is  always between 0 and 1, indicates the strength of evidence against the null hypothesis. A smaller p-value indicates greater evidence against null hypothesis, whilst a larger p-value indicates weaker evidence against the null hypothesis.","daa7c793":"f the p-value is less than 0.05, the null hypothesis has been rejected and the samples are likely not from a normal distribution. Otherwise, the null hypothesis cannot be rejected, and the samples are likely from a normal distribution.","48daff48":"## Wilcoxon signed-rank test","40211d99":"There is some correlation among the features in the dataset. e.g V2 is highly negatively  correlated with Amount","d57e2d27":"The loss graph against epoch is plotted. It can be seen that the loss  on testing dataset is almost the same as loss achieved during training which validates the training of the network.","ef9bb512":"Then training dataset is divided into training dataset and validation dataset. 80% of the dataset is training dataset and 20% dataset is validation dataset. Stratify is used to equaly split the ratio of each class ( 0 and 1 ) in training dataset and validation dataset. ","2d972a87":"### Interquartile Range (IQR) method","82f1bc4d":"Above figure shows that Amount feature has high outliers when compared to other features in the dataset. Now using Interquartile range we can count the number of outliears of each feature in the dataset.","2ee990b5":"Artificail neural network have many different hyperparameter that effect the performance of the neural network. I have used GridSearchCV function to find hyperparamter such as number of neaurons in hidden layer and weight initializer that help in improving overall network performance. Due high computation cost and time of GridSearchCV method, I have unsampled the training dataset to achieve result in short period of time but still it take 2 to 3 hours to run GridSearchCV method. I am commenting GridSearchCV method as  if not commented, commiting code on kaggle is taking alot of time and crashing.","3226f8af":"**Hypothesis**: A neural network classifier\u2019s performance on the credit card dataset is affected by the number of hidden layer.","4c6b204a":"### Basic Data Exploration","b548efa2":"The accuracy graph against epoch is plotted. It can be seen that the accuracy  on testing dataset is almost the same as accuracy achieved during training.","c8e676c9":"As per GridSearchCV(), there should be 64 neurons in the hidden layer and he_normal technique is used for weight initialization ","2c4fa5f3":"### Principal Component Analysis: It is a process that uses orthogonal transformation to reduce the dimensionality of the dataset and convert correlated into linealy uncorrelated variables","a3a20284":"Comparing the mean performance of the control arm against the experimental arm","3140fee5":" Amount and Time feature is droped from the dataset. The Amount feature after applying robust scalar is included in the data with the column name scaled_amount.","4634971e":"## Pre-processing Data","aebea667":"# CREDIT CARD FRAUD DETECTION","66ce4a8a":"### Checking For Imbalance Dataset","108da236":"The Amount and Time feature has high variance while the features from V1 to V28 have very low variance because of Principle Component Analysis (PCA) . Time feature could be dropped as it has Time column values only represent difference between each transaction and first tranaction .Feature scaling is required in order to reduce variance in Amount feature.","3fea4593":"### The dataset is highly imbalance. Number of non-fraud transaction are 284315 and number of fraud transaction are 492 in the dataset.","ed987c87":"### Importing Required Libaries","05d59d67":"After applying SMOTE,  training dataset is balanced as there are equal number of fraud ( 1 = 181961 ) and non-fraud ( 0 = 181961 ) transaction.","77f4228c":"Confusion Matrix","8691e4fd":"There is no clear trend of transaction amount vs time for both normal and frauld class.","64df7bbe":"#### Robust Scalar: In statistics, a robust measure of scale is a robust statistic that quantifies the statistical dispersion in a set of numerical data. The most common such statistics are the interquartile range and the median absolute deviation.","74650262":"**GridSearchCV for deciding hyperparameter for model","614cf858":"After applying robust scaler correlation between some feature can be seen.\n\n**Negative Correlations:** V17, V14, V12 and V10 are negatively correlated to class. (correlation less then 0)\n\n**Positive Correlations:** V2, V4, V11, and V19 are positively correlated to class. (correlation greater then 0)","b570fb5e":"## Visualising the results","efda022b":"### Experimental Arm Neural Network ","e82384b6":"Learinig rate is another important hyperparameter that effects the training and optimization of the neural network. I am using LRFinder class to find the optimal learning rate for the model.","a323ddfb":"### Checking For Missing Values","cb1a0987":"From the above figure, it can be seen amount in fraud traction is low. There are more number of transaction of small amount as compare to large amounts in non-fraud transactions.","ba877ffb":"Comparing data using box ploting","97b64896":"### Detecting outliears in credit card dataset","71ec72d3":"As per LRFinder graph, it can been seen that adam optimizer perform best at 0.001 learning rate ","8b26e7d0":"#### Applying  Robust Scaling on  Amount column and removing Time column. As Amount feature has high number of outliers,  so robust scalar is used because it uses the interquartile range method for standardization, so it is robust to outliers or extreme outliers."}}