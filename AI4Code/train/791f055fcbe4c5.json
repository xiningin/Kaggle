{"cell_type":{"e8fb1620":"code","d6bdeac8":"code","55ec091e":"code","ba9e7321":"code","4828e3a1":"code","13c512e6":"code","fb122090":"code","74f20b67":"code","1d5dbc50":"code","b8f7c12c":"code","1ab3c763":"code","1cbc3d55":"code","87e9d64f":"code","00420112":"code","89d44ab4":"code","72639e55":"code","11fbca16":"code","746333ac":"code","e1ea9e50":"code","26b658e1":"code","7afbed0e":"code","8cf691f5":"code","e27ae571":"code","75f610ad":"code","361fcb0b":"code","8844d841":"code","ed9aacd6":"code","7dfcf820":"code","b6e1893e":"code","f31e06e1":"code","e6ad7716":"code","c1f6b312":"code","223c5ec8":"code","be2b995e":"code","f8ac326e":"code","35a269e9":"code","2fa45493":"code","cf84c6e4":"code","d80f6b1f":"code","2dfce270":"code","1e7d580f":"code","28c4e8a2":"code","85dc5c91":"code","2fd0157d":"markdown","147ee238":"markdown","633bf2ad":"markdown","98aa1c3b":"markdown","56de20a7":"markdown","9ea03ef9":"markdown","a6f866c3":"markdown"},"source":{"e8fb1620":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6bdeac8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler\n\n# models for clf\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB","55ec091e":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","ba9e7321":"train_df.head(3)","4828e3a1":"test_df.head(3)","13c512e6":"train_df.info()","fb122090":"train_df.corr()","74f20b67":"# helper to drop col\ndef drop_column(dataframe, column_list):\n    new_df = dataframe.drop(columns = column_list)\n    return new_df\n\ndef get_model_score_and_save(y_test, y_pred, clf_name, result_dict):\n    score_dict = {}\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    score_dict[\"accuracy\"] = accuracy\n    score_dict[\"precision\"] = precision\n    score_dict[\"recall\"] = recall\n    score_dict[\"f1_score\"] = f1\n    \n    result_dict[clf_name] = score_dict","1d5dbc50":"passenger_Id_df = test_df[\"PassengerId\"]  # needed for submission\n\ntrain_df = drop_column(train_df, [\"PassengerId\"])\ntest_df = drop_column(test_df, [\"PassengerId\"])\n\ntrain_df.head(3)","b8f7c12c":"fig, _ = plt.subplots(figsize = (9,9))\n\ntrain_corr = train_df.corr()\n\nsns.heatmap(train_corr, annot=True)","1ab3c763":"train_df.head(3)","1cbc3d55":"# turn cat data to num\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\nlb = LabelBinarizer()\n\ntrain_df[\"Sex\"] = lb.fit_transform(train_df[\"Sex\"])\ntest_df[\"Sex\"] = lb.transform(test_df[\"Sex\"])","87e9d64f":"sns.barplot(x = train_df[\"Sex\"], y = train_df[\"Survived\"])","00420112":"# get number of nan rows per column\ntrain_df.isnull().sum()","89d44ab4":"# drop the Cabin col totally, due to too many nan rows and also seem to not be off use\ntrain_df = drop_column(train_df, [\"Cabin\"])\ntest_df = drop_column(test_df, [\"Cabin\"])\n\ntrain_df.head(2)","72639e55":"# drop ticket col too\n\ntrain_df = drop_column(train_df, [\"Ticket\"])\ntest_df = drop_column(test_df, [\"Ticket\"])\n\ntrain_df.head(2)","11fbca16":"# fill the nan rows in embarked col with most frequent class\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode().iloc[0])\n\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].fillna(test_df[\"Embarked\"].mode().iloc[0])\n\ntrain_df.isnull().sum()","746333ac":"# maked embarked cat data to num\nmapping_dict = {\n    \"S\" : 0,\n    \"C\" : 1,\n    \"Q\" : 2\n}\n\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].map(mapping_dict).astype(int)\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].map(mapping_dict).astype(int)\n\ntrain_df.head(4)","e1ea9e50":"train_df.corr()","26b658e1":"# since simblings onBoard and Parents on board are correlated,merge to redcue dims\n\ntrain_df[\"FamilyCount\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\n\ntest_df[\"FamilyCount\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]\n\ntrain_df.head()","7afbed0e":"train_df = drop_column(train_df , [\"SibSp\", \"Parch\"])\ntest_df = drop_column(test_df, [\"SibSp\", \"Parch\"])\n\ntrain_df.head()","8cf691f5":"# drop the name col\n\ntrain_df = drop_column(train_df , [\"Name\"])\ntest_df = drop_column(test_df, [\"Name\"])\n\ntrain_df.head()","e27ae571":"# seems being with max fare price are more likely to survive\n\ntrain_df[train_df[\"Fare\"] == train_df[\"Fare\"].max()].tail(10)","75f610ad":"# replace nan values\ntrain_df.isna().sum()\n\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(train_df[\"Age\"].mean(axis = 0))\n\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(test_df[\"Age\"].mean(axis = 0))\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(test_df[\"Fare\"].mean(axis = 0))","361fcb0b":"test_df.isna().sum()","8844d841":"std = StandardScaler()\n\n# train_df[[\"Age\", \"Fare\"]] = std.fit_transform(train_df[[\"Age\", \"Fare\"]])\n# test_df[[\"Age\", \"Fare\"]] = std.fit_transform(test_df[[\"Age\", \"Fare\"]])","ed9aacd6":"# drop Embarked\ntrain_df = drop_column(train_df, [\"Embarked\"])\ntest_df = drop_column(test_df, [\"Embarked\"])","7dfcf820":"y = train_df[\"Survived\"]\n\nx = drop_column(train_df, [\"Survived\"])\n\nx = std.fit_transform(x)\n\ntest_df = std.transform(test_df)","b6e1893e":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.02)","f31e06e1":"# class weight specified since more class for dead (0)than alive(1)\n\nrandom_trees = RandomForestClassifier(n_estimators = 450, max_depth=15, max_leaf_nodes=16, \n                                      random_state=100, n_jobs = -1, class_weight={0:0.616, 1:0.384})\n\nrandom_trees.fit(x_train, y_train)","e6ad7716":"performance_dict = {}\n\nrandom_tree_pred = random_trees.predict(x_test)\n\nget_model_score_and_save(y_test, random_tree_pred, \"random_trees\", performance_dict)","c1f6b312":"performance_dict[\"random_trees\"]","223c5ec8":"ada_boost = AdaBoostClassifier(DecisionTreeClassifier(random_state=100, max_depth = 15),\n                        n_estimators=600, learning_rate=1.0)\n\nada_boost.fit(x_train, y_train)\n\nada_boost_pred = ada_boost.predict(x_test)\n\nget_model_score_and_save(y_test, ada_boost_pred, \"ada_boost\", performance_dict)\n\nperformance_dict","be2b995e":"# I'm going with random forest for now, maybe later on after experimenting I will go with other models\n\ntest_df_predictions = random_trees.predict(test_df)","f8ac326e":"test_df_predictions.shape","35a269e9":"random_forest_predictions_df = pd.DataFrame({\n    \"Passengerid\": passenger_Id_df,\n    \"Survived\": test_df_predictions\n})\n\nrandom_forest_predictions_df\nrandom_forest_predictions_df.to_csv(\"predictions.csv\", index=False)","2fa45493":"random_forest_predictions_df","cf84c6e4":"import xgboost\n\nxgb = xgboost.XGBClassifier(\n    learning_rate = 0.4,\n    n_estimators = 200,\n    max_depth = 5,\n    subsample = 0.9,\n    colsample_bytree = 0.5,\n    gamma = 1,  \n)\n\neval_set = [(x_train, y_train), (x_test, y_test)]\neval_metric = [\"logloss\", \"auc\"]\n# 2. Fit the model\nhistory = xgb.fit(x_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)","d80f6b1f":"xgb_pred = xgb.predict(x_test)\n\nget_model_score_and_save(y_test, xgb_pred, \"xgb\", performance_dict)\n\n\nperformance_dict\n\n# not giving me what I want actually","2dfce270":"gradient_boost = GradientBoostingClassifier()\n\nparam_grid = {\n    \"learning_rate\": [0.5, 0.6, 0.8, 0.9, 1.0],\n    \"n_estimators\": [300, 400, 450, 500],\n    \"max_depth\": [10, 15, 17, 20,30, 40],\n    \"max_leaf_nodes\": [16, 18, 20, 22]\n}\n\n\ngrid_search_clf = GridSearchCV(gradient_boost, param_grid=param_grid, cv=3, n_jobs = -1)\n\ngrid_search_clf.fit(x_train, y_train)","1e7d580f":"grid_search_clf.best_params_, grid_search_clf.best_score_","28c4e8a2":"gradient_boost_pred = grid_search_clf.predict(x_test)\n\nget_model_score_and_save(y_test, gradient_boost_pred, \"gradient_boost_clf\", performance_dict)\n\nperformance_dict","85dc5c91":"train_df.groupby(\"Survived\").count()","2fd0157d":"**Male = 1 and Female = 0**","147ee238":"**Ada Boost**","633bf2ad":"**Traning without scaling ordinal data**","98aa1c3b":"***EDA***","56de20a7":"**Gradient Boosting**","9ea03ef9":"**using XGboost**","a6f866c3":"**Random Trees**"}}