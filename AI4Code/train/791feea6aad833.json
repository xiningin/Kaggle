{"cell_type":{"c3d5de62":"code","1799cf99":"code","9ae45496":"code","e5de67b2":"code","c66d4cca":"code","acdc3ed4":"code","787bb393":"code","7cabe75d":"code","81e52281":"code","8be3aead":"code","23264e7a":"code","bb31ecda":"code","36e82d29":"code","aa06e509":"code","221a833b":"code","e6d3b50b":"code","dc465a52":"code","0e5d76ef":"code","1e03f14b":"markdown","cd265420":"markdown","80c119a7":"markdown","3ed229c5":"markdown"},"source":{"c3d5de62":"import torch\nimport torchvision\nfrom torchvision import transforms, datasets\n\nimport matplotlib.pyplot as plt\nimport numpy as np","1799cf99":"#both training and testing dataset is coverted to tensor\n\ntrain = datasets.MNIST('', train=True, \n                      download=True, transform=transforms.Compose(\n                      [transforms.ToTensor()]))\n\ntest = datasets.MNIST('', train=False, \n                      download=True, transform=transforms.Compose(\n                      [transforms.ToTensor()]))","9ae45496":"trainLoader = torch.utils.data.DataLoader(train, batch_size=64, shuffle=True)\ntestLoader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)","e5de67b2":"next(iter(trainLoader))","c66d4cca":"#unpacking\nX, y = next(iter(trainLoader))","acdc3ed4":"plt.imshow(X[1].reshape(28,-1), cmap='gray')","787bb393":"print(y[1])","7cabe75d":"total = 0\ncounter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n\nfor i in trainLoader:\n    Xs, ys = i\n    for y in ys:\n        counter[int(y)] += 1\n        total += 1\n        \nprint(counter)\nprint(total)","81e52281":"import torch.nn as nn\nimport torch.nn.functional as F","8be3aead":"class LinNets(nn.Module):\n    def __init__(self):\n        super(LinNets, self).__init__()\n        self.fc1 = nn.Linear(1*28*28, 32)\n        self.fc2 = nn.Linear(32, 32)\n        self.fc3 = nn.Linear(32, 16)\n        self.fc4 = nn.Linear(16, 10)\n    \n    #play with activation function\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        \n        #research about softmax function\n        return F.softmax(x, dim=1)\n    \nmodel = LinNets()\nmodel","23264e7a":"# 1, 784 | 784,32 | 32, 32 | 32,16 | 16,8 | 8,10 => 1,10\n\nrandomX = torch.randn(28, 28)\nrandomX = randomX.reshape(1,-1)\n\n","bb31ecda":"import pandas as pd","36e82d29":"y = model(randomX) #training\n\ny = pd.DataFrame(y.detach().numpy(), columns=[0,1,2,3,4,5,6,7,8,9])\ny #prediction","aa06e509":"columns=[0,1,2,3,4,5,6,7,8,9]\n\nplt.bar(columns, y.T[0])","221a833b":"for ind, param in enumerate(model.parameters()):\n    print(f'layer{ind}: ', param)","e6d3b50b":"import time","dc465a52":"#training in GPU\n\nstart = time.time()\ndevice = torch.device('cuda')\n\nmodel = model.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=0.01)\n\nlosses = []\n\nEPOCHS = 5\n\nfor i in range(EPOCHS):\n    for X,y in trainLoader:\n        X = X.to(device)\n        y = y.to(device)\n        \n        y_pred = model(X.reshape(-1,28*28))\n        loss = F.cross_entropy(y_pred,y)\n        \n        #gradient descent or back propagation\n        loss.backward()\n        optim.step()\n    \n    print(loss)\n\nend = time.time()\nprint('total Time: ', end-start)","0e5d76ef":"#training in CPU\n\nstart = time.time()\ndevice = torch.device('cpu')\n\nmodel = model.to(device)\noptim = torch.optim.Adam(model.parameters(), lr=0.01)\n\nEPOCHS = 5\n\nfor i in range(EPOCHS):\n    for X,y in trainLoader:\n        X = X.to(device)\n        y = y.to(device)\n        \n        y_pred = model(X.reshape(-1,28*28))\n        loss = F.cross_entropy(y_pred,y)\n        \n        #gradient descent or back propagation\n        loss.backward()\n        optim.step()\n    \n    print(loss)\n\nend = time.time()\nprint('total Time: ', end-start)","1e03f14b":"## Loss graph","cd265420":"# Building a neural net","80c119a7":"## Visualisation","3ed229c5":"## Optimization\n"}}