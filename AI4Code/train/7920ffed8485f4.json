{"cell_type":{"61b331ec":"code","0153f39f":"code","83f5c662":"code","778c9e3f":"code","4b0492dd":"code","6a06c8fa":"code","be823168":"code","636fb7e0":"code","82da959e":"code","82d75384":"code","d2c883d8":"code","33f0ca1d":"code","6fb1ef27":"code","839bafb2":"code","c65b25b6":"code","37a64a83":"code","b3900cea":"code","21c57980":"code","a00cbcc1":"code","124212e3":"code","7c77902d":"code","6e13dd5a":"code","13a05929":"code","74f37228":"code","51dc7a65":"code","80c32a08":"code","645ff481":"code","6dbdb3fe":"code","19fb9e5d":"code","d073050d":"code","e599062f":"code","1122c0df":"code","ad6e30eb":"code","68b40f03":"code","754f7468":"code","51fb5bb7":"code","f0a063e1":"code","ed82ba6d":"code","9d853ce0":"code","6356bc4d":"code","233153f1":"code","56e4fa50":"code","3c086fcf":"code","232564dc":"code","c4a38eb4":"code","78f2cf48":"code","514a58f8":"code","eec74978":"code","f45626a3":"code","e625d425":"code","17885198":"code","c77752a3":"code","9a9382e6":"code","0c44c7f0":"code","76090188":"code","d13b1aa0":"code","58876fd4":"markdown","fd6e92c1":"markdown","f5656f2e":"markdown","3d80c759":"markdown","db3d41cd":"markdown","01e898e1":"markdown","7f2a2fe3":"markdown","aba0d021":"markdown","65a6eca0":"markdown","f7bb8743":"markdown","dbe59939":"markdown","ab6a2648":"markdown"},"source":{"61b331ec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.impute import KNNImputer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0153f39f":"path =\"\/kaggle\/input\/kagg-train\/kaggle_train.csv\"\npath1 =\"\/kaggle\/input\/kagg-test\/kaggle_test.csv\"\nclass DataFrame_Loader():\n\n    \n    def __init__(self):\n        \n        print(\"Loadind DataFrame\")\n        \n    def read_csv(self,data):\n        self.df = pd.read_csv(data)\n        \n    def load_csv(self):\n        return self.df","83f5c662":"load= DataFrame_Loader()","778c9e3f":"load.read_csv(path)","4b0492dd":"dftrain = load.load_csv()\ndftrain.head()","6a06c8fa":"dftest = load.load_csv()\ndftest.head()","be823168":"class DataFrame_Information():\n    \n\n    def __init__(self):\n        \n        print(\"Attribute Information object created\")\n        \n        \n        \n    def Attribute_information(self,df):\n        \n        \"\"\"\n        This method will give us a basic\n        information of the dataframe like\n        Count of Attributes,Count of rows,\n        Numerical Attributes, Categorical \n        Attributes, Factor Attributes etc..\n        \"\"\"\n    \n        data_info = pd.DataFrame(\n                                columns=['No of observation',\n                                        'No of Variables',\n                                        'No of Numerical Variables',\n                                        'No of Factor Variables',\n                                        'No of Categorical Variables',\n                                        'No of Logical Variables',\n                                        'No of Date Variables',\n                                        'No of zero variance variables'])\n\n\n        data_info.loc[0,'No of observation'] = df.shape[0]\n        data_info.loc[0,'No of Variables'] = df.shape[1]\n        data_info.loc[0,'No of Numerical Variables'] = df._get_numeric_data().shape[1]\n        data_info.loc[0,'No of Factor Variables'] = df.select_dtypes(include='category').shape[1]\n        data_info.loc[0,'No of Logical Variables'] = df.select_dtypes(include='bool').shape[1]\n        data_info.loc[0,'No of Categorical Variables'] = df.select_dtypes(include='object').shape[1]\n        data_info.loc[0,'No of Date Variables'] = df.select_dtypes(include='datetime64').shape[1]\n        data_info.loc[0,'No of zero variance variables'] = df.loc[:,df.apply(pd.Series.nunique)==1].shape[1]\n\n        data_info =data_info.transpose()\n        data_info.columns=['value']\n        data_info['value'] = data_info['value'].astype(int)\n\n\n        return data_info\n\n    def __get_missing_values(self,data):\n        \n        \"\"\"\n        It is a Private method, so it cannot \n        be accessed by object outside the \n        class. This function will give us \n        a basic information like count \n        of missing values\n        \"\"\"\n        \n        #Getting sum of missing values for each feature\n        missing_values = data.isnull().sum()\n        #Feature missing values are sorted from few to many\n        missing_values.sort_values(ascending=False, inplace=True)\n        \n        #Returning missing values\n        return missing_values\n        \n    def Agg_Tabulation(self,data):\n        \n        \n        \"\"\"\n        This method is a extension of \n        schema will gives the aditional \n        information about the data\n        like Entropy value, Missing \n        Value Percentage and some observations\n        \"\"\"\n        \n        print(\"=\" * 110)\n        print(\"Aggregation of Table\")\n        print(\"=\" * 110)\n        table = pd.DataFrame(data.dtypes,columns=['dtypes'])\n        table1 =pd.DataFrame(data.columns,columns=['Names'])\n        table = table.reset_index()\n        table= table.rename(columns={'index':'Name'})\n        table['No of Missing'] = data.isnull().sum().values    \n        table['No of Uniques'] = data.nunique().values\n        table['Percent of Missing'] = ((data.isnull().sum().values)\/ (data.shape[0])) *100\n        table['First Observation'] = data.loc[0].values\n        table['Second Observation'] = data.loc[1].values\n        table['Third Observation'] = data.loc[2].values\n        for name in table['Name'].value_counts().index:\n            table.loc[table['Name'] == name, 'Entropy'] = round(stats.entropy(data[name].value_counts(normalize=True), base=2),2)\n        return table\n    \n        print(\"=\" * 110)\n        \n    def __iqr(self,x):\n        \n        \n        \"\"\"\n        It is a private method which \n        returns you interquartile Range\n        \"\"\"\n        return x.quantile(q=0.75) - x.quantile(q=0.25)\n\n    def __outlier_count(self,x):\n        \n        \n        \"\"\"\n        It is a private method which \n        returns you outlier present\n        in the interquartile Range\n        \"\"\"\n        upper_out = x.quantile(q=0.75) + 1.5 * self.__iqr(x)\n        lower_out = x.quantile(q=0.25) - 1.5 * self.__iqr(x)\n        return len(x[x > upper_out]) + len(x[x < lower_out])\n\n    def num_count_summary(self,df):\n        \n        \n        \"\"\"\n        This method will returns \n        you the information about\n        numerical attributes like\n        Positive values,Negative Values\n        Unique count, Zero count \n        positive and negative inf-\n        nity count and count of outliers\n        etc \n        \n        \"\"\"\n        \n        df_num = df._get_numeric_data()\n        data_info_num = pd.DataFrame()\n        i=0\n        for c in  df_num.columns:\n            data_info_num.loc[c,'Negative values count']= df_num[df_num[c]<0].shape[0]\n            data_info_num.loc[c,'Positive values count']= df_num[df_num[c]>0].shape[0]\n            data_info_num.loc[c,'Zero count']= df_num[df_num[c]==0].shape[0]\n            data_info_num.loc[c,'Unique count']= len(df_num[c].unique())\n            data_info_num.loc[c,'Negative Infinity count']= df_num[df_num[c]== -np.inf].shape[0]\n            data_info_num.loc[c,'Positive Infinity count']= df_num[df_num[c]== np.inf].shape[0]\n            data_info_num.loc[c,'Missing Percentage']= df_num[df_num[c].isnull()].shape[0]\/ df_num.shape[0]\n            data_info_num.loc[c,'Count of outliers']= self.__outlier_count(df_num[c])\n            i = i+1\n        return data_info_num\n    \n    def statistical_summary(self,df):\n        \n        \n        \"\"\"\n        This method will returns \n        you the varoius percentile\n        of the data including count \n        and mean\n        \"\"\"\n    \n        df_num = df._get_numeric_data()\n\n        data_stat_num = pd.DataFrame()\n\n        try:\n            data_stat_num = pd.concat([df_num.describe().transpose(),\n                                       pd.DataFrame(df_num.quantile(q=0.10)),\n                                       pd.DataFrame(df_num.quantile(q=0.90)),\n                                       pd.DataFrame(df_num.quantile(q=0.95))],axis=1)\n            data_stat_num.columns = ['count','mean','std','min','25%','50%','75%','max','10%','90%','95%']\n        except:\n            pass\n\n        return data_stat_num","636fb7e0":"info = DataFrame_Information()","82da959e":"info.Attribute_information(dftrain)","82d75384":"info.Agg_Tabulation(dftrain)","d2c883d8":"info.num_count_summary(dftrain)","33f0ca1d":"info.statistical_summary(dftrain)","6fb1ef27":"class DataFrame_Preprocessor():\n    \n\n    def __init__(self):\n        print(\"Preprocessor object created\")\n        \n        \n    def __split_numbers_chars(self,row):\n        head = row.rstrip('0123456789')\n        tail = row[len(head):]\n        return head, tail\n    \n    def reverse_one_hot_encode(self,dataframe, start_loc, end_loc, numeric_column_name):\n        dataframe['String_Column'] = (dataframe.iloc[:, start_loc:end_loc] == 1).idxmax(1)\n        dataframe['Tuple_Column'] = dataframe['String_Column'].apply(self.__split_numbers_chars)\n        dataframe[numeric_column_name] = pd.to_numeric(dataframe['Tuple_Column'].apply(lambda x: x[1]),errors='coerce')\n        dataframe.drop(columns=['String_Column','Tuple_Column'], inplace=True)\n","839bafb2":"Preprocessor = DataFrame_Preprocessor()","c65b25b6":"Preprocessor.reverse_one_hot_encode(dftrain,14,54,'soil_type')\nPreprocessor.reverse_one_hot_encode(dftrain,10,14,'wilderness')\ndftrain.head()","37a64a83":"Preprocessor.reverse_one_hot_encode(dftest,14,54,'soil_type')\nPreprocessor.reverse_one_hot_encode(dftest,10,14,'wilderness')\ndftest.head()","b3900cea":"col_list = ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type7','Soil_Type8','Soil_Type9','Soil_Type10',\n  'Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type15','Soil_Type16','Soil_Type17','Soil_Type18','Soil_Type19',\n  'Soil_Type20','Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type26','Soil_Type27','Soil_Type28',\n  'Soil_Type29','Soil_Type30','Soil_Type31','Soil_Type32','Soil_Type33','Soil_Type34','Soil_Type35','Soil_Type36','Soil_Type37',\n  'Soil_Type38','Soil_Type39','Soil_Type40']\n\ncol_list1 = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n\nclass Column_Dopper():\n\n    \n    def __init__(self):\n        print(\"Column Dopper object created\")\n    \n    \n    def dropper(self,x):\n        \n        \"\"\"\n        This method helps\n        to drop the columns\n        in our original \n        dataframe which is \n        available in the \n        col_list and return \n        us final dataset\n        \"\"\"\n        \n        data=[]\n        for i in x.columns:\n            if i not in col_list:\n                data.append(i)\n        return x[data]\n","21c57980":"col_drop = Column_Dopper()","a00cbcc1":"dftrain = col_drop.dropper(dftrain)\ndftrain.head()","124212e3":"dftest = col_drop.dropper(dftest)\ndftest.head()","7c77902d":"class DataFrame_Feature_Engineering():\n\n    def __init__(self):\n        print(\"Feature Engineering object created\")\n        \n    def Make_Features_for_Train(self,dftrain):\n        \n        dftrain['Hydro_fire'] = dftrain['Horizontal_Distance_To_Fire_Points'] +  dftrain['Horizontal_Distance_To_Hydrology']\n        dftrain['Hydro_Road'] = dftrain['Horizontal_Distance_To_Roadways'] +  dftrain['Horizontal_Distance_To_Hydrology']\n        dftrain['Road_fire'] = dftrain['Horizontal_Distance_To_Fire_Points'] +  dftrain['Horizontal_Distance_To_Roadways']\n        dftrain['Hydro_Road_sub'] = np.abs(dftrain['Horizontal_Distance_To_Roadways'] -  dftrain['Horizontal_Distance_To_Hydrology'])\n        dftrain['Hydro_fire_sub'] = np.abs(dftrain['Horizontal_Distance_To_Fire_Points'] -  dftrain['Horizontal_Distance_To_Hydrology'])\n        dftrain['Road_fire_sub'] = np.abs(dftrain['Horizontal_Distance_To_Fire_Points'] -  dftrain['Horizontal_Distance_To_Roadways'])\n        dftrain['RAD_SLOPE'] = dftrain['Slope'].apply(lambda x: x*(np.pi\/180))\n        dftrain['RAD_Aspect'] = dftrain['Aspect'].apply(lambda x: x*(np.pi\/180))\n        dftrain['EL_DIS'] = dftrain['Elevation'] - dftrain['Horizontal_Distance_To_Hydrology']*0.2\n        dftrain['EL_Fire'] = dftrain['Elevation'] - dftrain['Horizontal_Distance_To_Fire_Points']*0.2\n        dftrain['EL_Road'] = dftrain['Elevation'] - dftrain['Horizontal_Distance_To_Roadways']*0.2\n        \n        \n    def Make_Features_for_Test(self,dftest):\n        \n        dftest['Hydro_fire'] = dftest['Horizontal_Distance_To_Fire_Points'] +  dftest['Horizontal_Distance_To_Hydrology']\n        dftest['Hydro_Road'] = dftest['Horizontal_Distance_To_Roadways'] +  dftest['Horizontal_Distance_To_Hydrology']\n        dftest['Road_fire'] = dftest['Horizontal_Distance_To_Fire_Points'] +  dftest['Horizontal_Distance_To_Roadways']\n        dftest['Hydro_fire_sub'] = np.abs(dftest['Horizontal_Distance_To_Fire_Points'] -  dftest['Horizontal_Distance_To_Hydrology'])\n        dftest['Hydro_Road_sub'] = np.abs(dftest['Horizontal_Distance_To_Roadways'] -  dftest['Horizontal_Distance_To_Hydrology'])\n        dftest['Road_fire_sub'] = np.abs(dftest['Horizontal_Distance_To_Fire_Points'] -  dftest['Horizontal_Distance_To_Roadways'])\n        dftest['RAD_SLOPE'] = dftest['Slope'].apply(lambda x: x*(np.pi\/180))\n        dftest['RAD_Aspect'] = dftest['Aspect'].apply(lambda x: x*(np.pi\/180))\n        dftest['EL_DIS'] = dftest['Elevation'] - dftest['Horizontal_Distance_To_Hydrology']*0.2\n        dftest['EL_Fire'] = dftest['Elevation'] - dftest['Horizontal_Distance_To_Fire_Points']*0.2\n        dftest['EL_Road'] = dftest['Elevation'] - dftest['Horizontal_Distance_To_Roadways']*0.2","6e13dd5a":"FE = DataFrame_Feature_Engineering()","13a05929":"FE.Make_Features_for_Train(dftrain)\ndftrain.head()","74f37228":"FE.Make_Features_for_Test(dftest)\ndftest.head()","51dc7a65":"class DataFrame_numerical_Imputer():\n    \n\n    def __init__(self):\n        print(\"numerical_Imputer object created\")\n\n        \n   \n    def KNN_Imputer(self,df):\n        \n        \"\"\"\n        This method is for\n        imputation, behalf\n        of all methods KNN\n        imputation performs\n        well, hence this method\n        will helps to impute\n        missing values in \n        dataset\n        \"\"\"\n        \n        knn_imputer = KNNImputer(n_neighbors=5)\n        df.iloc[:, :] = knn_imputer.fit_transform(df)\n        return df","80c32a08":"imputer = DataFrame_numerical_Imputer()","645ff481":"dftrain = imputer.KNN_Imputer(dftrain)\ndftrain.head()","6dbdb3fe":"dftrain =  dftrain.drop(['Id'],axis=1)\ndftrain.head()","19fb9e5d":"from sklearn.model_selection import KFold, cross_val_score,RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nx = dftrain.drop(['Cover_Type'],axis=1)\ny = dftrain['Cover_Type']\nx_train,x_test,y_train,y_test=train_test_split(x\\\n                ,y,test_size=0.30,random_state=42)\n\nclass Model_Selector():\n    \n    \n\n    def __init__(self,n_estimators=100,\\\n            random_state=42,max_depth=10):\n        print(\"Model Selector object created\")\n        \n    \"\"\"\n    This method helps to select\n    the best machine learning \n    model to compute the relationship\n    betweem i\/p and d\/p variable\n    \n    \"\"\"    \n        \n        \n    def Classification_Model_Selector(self,df):\n        seed = 42\n        models = []\n        models.append((\"LR\", LogisticRegression()))\n        models.append((\"RF\", RandomForestClassifier(n_estimators=100,\\\n            random_state=42,max_depth=10)))\n        models.append((\"KNN\", KNeighborsClassifier()))\n        models.append((\"CART\", DecisionTreeClassifier()))\n        models.append((\"XGB\", XGBClassifier()))\n        result = []\n        names = []\n        scoring = 'accuracy'\n        seed = 42\n        \n        \n\n        for name, model in models:\n            kfold = KFold(n_splits = 5, random_state =seed)\n            cv_results = cross_val_score(model, x_train,\\\n                    y_train, cv = kfold, scoring = scoring)\n            result.append(cv_results)\n            names.append(name)\n            msg = (name, cv_results.mean(), cv_results.std())\n            print(msg)\n            \n            \n            \n        fig = plt.figure(figsize = (8,4))\n        fig.suptitle('Algorithm Comparison')\n        ax = fig.add_subplot(1,1,1)\n        plt.boxplot(result)\n        ax.set_xticklabels(names)\n        plt.show()","d073050d":"MS = Model_Selector()","e599062f":"MS.Classification_Model_Selector(dftrain)","1122c0df":"class Data_Modelling():\n    \n\n    def __init__(self,n_estimators,\n                    max_depth,\n                    min_samples_split,\n                    min_samples_leaf,\n                    max_leaf_nodes,\n                    bootstrap,\n                    class_weight,\n                    min_child_weight,\n                    learning_rate,\n                    Subsample,\n                    Alpha,\n                    Lamda,\n                    random_state,\n                    criterion):\n        \n        self.n_estimators = 500\n        self.max_depth = 5\n        self.min_samples_split = 3\n        self.min_samples_leaf = 3\n        self.max_leaf_nodes = None\n        self.bootstrap = True\n        self.class_weight = 'balanced'\n        self.min_child_weight = 3\n        self.learning_rate = 0.07\n        self.Subsample = 0.7\n        self.Alpha = 0\n        self.Lamda = 1.5\n        self.random_state = 29 \n        self.criterion = 'entropy'\n        \n        print(\"Data Modelling object created\")\n        \n        \n    def Random_Forest_Model(self,df):\n        \n        Classifier = RandomForestClassifier(n_estimators = 500,\n                    max_depth = 5,\n                    min_samples_split = 3,\n                    min_samples_leaf = 3,\n                    max_leaf_nodes = None,\n                    bootstrap = True,\n                    class_weight= 'balanced',\n                    criterion = 'entropy')\n        \n        Classifier.fit(x_train,y_train)\n        \n        RF_pred=Classifier.predict(x_test)\n        \n        print(metrics.accuracy_score(y_test, RF_pred))\n        \n        print(metrics.confusion_matrix(y_test, RF_pred))\n        \n        print(metrics.classification_report(y_test, RF_pred))\n        \n    def Extreme_Gradient_Boosting_Model(self,df):\n        \n        XGB_Classifier = XGBClassifier(n_estimators = 500,\n                    learning_rate = 0.07,\n                    max_depth = 5,\n                    min_child_weight = 3,\n                    random_state = 29,\n                    Subsample = 0.7,\n                    Alpha = 0,\n                    Lamda = 1.5)\n        \n        XGB_Classifier.fit(x_train,y_train)\n        \n        XGB_pred=XGB_Classifier.predict(x_test)\n        \n        print(metrics.accuracy_score(y_test, XGB_pred))\n        \n        print(metrics.confusion_matrix(y_test, XGB_pred))\n        \n        print(metrics.classification_report(y_test, XGB_pred))","ad6e30eb":"Basemodell = Data_Modelling(500,5,3,3,None,True,'balanced',3,0.07,0.7,0,1.5,29,'entropy')","68b40f03":"Basemodell.Random_Forest_Model(dftrain)","754f7468":"Basemodell.Extreme_Gradient_Boosting_Model(dftrain)","51fb5bb7":"class Model_Classifier_HyperParameter_Tuning():\n    \n\n    def __init__(self):\n        \n        print(\"HyperParameter_Tuning object created\")\n        \n    class XGB_Classifier_HyperParameter_Tuning():\n    \n\n        def __init__(self):\n\n            print(\"XGB HyperParameter_Tuning object created\")\n\n\n        def Fit_XGB_HyperParameter_Tuner(self,dftrain):\n            \n\n            xgb_clf = XGBClassifier(tree_method = \"exact\", predictor = \"cpu_predictor\",\n                                        objective = \"multi:softmax\")\n\n\n            parameters = {\"learning_rate\": [0.1, 0.01, 0.001],\n                           \"gamma\" : [0.01, 0.1, 0.3, 0.5, 1, 1.5, 2],\n                           \"max_depth\": [2, 4, 7, 10],\n                           \"colsample_bytree\": [0.3, 0.6, 0.8, 1.0],\n                           \"subsample\": [0.2, 0.4, 0.5, 0.6, 0.7],\n                           \"reg_alpha\": [0, 0.5, 1],\n                           \"reg_lambda\": [1, 1.5, 2, 3, 4.5],\n                           \"min_child_weight\": [1, 3, 5, 7],\n                           \"n_estimators\": [100, 250, 500, 1000]}\n\n            from sklearn.model_selection import RandomizedSearchCV\n\n            xgb_rscv = RandomizedSearchCV(xgb_clf, param_distributions = parameters, scoring = \"f1_micro\",\n                                         cv = 3, random_state = 29 )\n\n            # Fit the model\n            model_xgboost = xgb_rscv.fit(x_train, y_train)\n            return model_xgboost\n        \n        \n        def XGB_Get_Best_Prams(self):\n            \n            print(\"Learning Rate: \", Xgb_model.best_estimator_.get_params()[\"learning_rate\"])\n            print(\"Gamma: \", Xgb_model.best_estimator_.get_params()[\"gamma\"])\n            print(\"Max Depth: \", Xgb_model.best_estimator_.get_params()[\"max_depth\"])\n            print(\"Subsample: \", Xgb_model.best_estimator_.get_params()[\"subsample\"])\n            print(\"Max Features at Split: \", Xgb_model.best_estimator_.get_params()[\"colsample_bytree\"])\n            print(\"Alpha: \", Xgb_model.best_estimator_.get_params()[\"reg_alpha\"])\n            print(\"Lamda: \", Xgb_model.best_estimator_.get_params()[\"reg_lambda\"])\n            print(\"Minimum Sum of the Instance Weight Hessian to Make a Child: \",Xgb_model.best_estimator_.get_params()[\"min_child_weight\"])\n            print(\"Number of Trees: \", Xgb_model.best_estimator_.get_params()[\"n_estimators\"])\n\n\n        \n        def get_classification_report(self,modelname,y_test):\n            \n            \n            \n            ypred = modelname.predict(x_test)\n            report = metrics.classification_report(y_test, ypred,output_dict=True)\n            print(metrics.confusion_matrix(y_test, ypred))\n            print(metrics.accuracy_score(y_test, ypred))\n            df_classification_report = pd.DataFrame(report).transpose()\n            return df_classification_report\n        \n        class RF_Classifier_HyperParameter_Tuning():\n    \n\n            def __init__(self):\n\n                print(\"RF HyperParameter_Tuning object created\")\n\n\n            def Fit_RF_HyperParameter_Tuner(self,dftrain):\n                \n                \n\n                param_grid = {\"max_depth\": [1, 3, 5, 7, 9, 10],\n                              \"max_features\": [1, 3, 10, 20,40, 50,80],\n                              \"min_samples_split\": [1, 3, 10, 15, 20],\n                              \"min_samples_leaf\": [1, 3, 5, 10],\n                              \"bootstrap\": [True, False],\n                              \"criterion\": [\"gini\", \"entropy\"],\n                              \"n_estimators\": [100, 250, 500, 1000]}\n\n                clf = RandomForestClassifier(random_state=29, class_weight='balanced', n_jobs=-1)\n                model = RandomizedSearchCV(clf, param_grid, scoring = 'f1_micro', cv=3)\n\n                model.fit(x_train, y_train)\n\n                return model\n\n            def RF_Get_Best_Prams(self):\n                \n                \n                \n\n                print(\"n_estimators: \", RF_model.best_estimator_.get_params()[\"n_estimators\"])\n                print(\"Max Depth: \", RF_model.best_estimator_.get_params()[\"max_depth\"])\n                print(\"min_samples_split: \", RF_model.best_estimator_.get_params()[\"min_samples_split\"])\n                print(\"min_samples_leaf: \", RF_model.best_estimator_.get_params()[\"min_samples_leaf\"])\n                print(\"max_leaf_nodes: \", RF_model.best_estimator_.get_params()[\"max_leaf_nodes\"])\n                print(\"bootstrap: \", RF_model.best_estimator_.get_params()[\"bootstrap\"])\n                print(\"class_weight: \", RF_model.best_estimator_.get_params()[\"class_weight\"])\n                print(\"criterion: \",RF_model.best_estimator_.get_params()[\"criterion\"])\n                print(\"Number of Trees: \", RF_model.best_estimator_.get_params()[\"n_estimators\"])\n\n            def Evaluation_Report(self,modelname,y_test):\n                \n                \n                return HP_XGB.get_classification_report(RF_model,y_test)","f0a063e1":"HP_XGB = Model_Classifier_HyperParameter_Tuning().XGB_Classifier_HyperParameter_Tuning()\nHP_RF = Model_Classifier_HyperParameter_Tuning().XGB_Classifier_HyperParameter_Tuning().RF_Classifier_HyperParameter_Tuning()","ed82ba6d":"Xgb_model = HP_XGB.Fit_XGB_HyperParameter_Tuner(dftrain)\nXgb_model","9d853ce0":"HP_XGB.XGB_Get_Best_Prams()","6356bc4d":"HP_XGB.get_classification_report(Xgb_model,y_test)","233153f1":"RF_model = HP_RF.Fit_RF_HyperParameter_Tuner(dftrain)\nRF_model","56e4fa50":"HP_RF.RF_Get_Best_Prams()","3c086fcf":"HP_RF.Evaluation_Report(RF_model,y_test)","232564dc":"from sklearn.feature_selection import RFE\nfrom catboost import CatBoostRegressor\n\nclass Feature_Selection():\n\n    def __init__(self,n_estimators,\n                    max_depth,\n                    min_samples_split,\n                    min_samples_leaf,\n                    max_leaf_nodes,\n                    bootstrap,\n                    class_weight,\n                    random_state,\n                    criterion):\n        \n        self.n_estimators = 500\n        self.max_depth = 5\n        self.min_samples_split = 5\n        self.min_samples_leaf = 3\n        self.max_leaf_nodes = None\n        self.bootstrap = True\n        self.class_weight = 'balanced'\n        self.random_state = 29\n        self.criterion = 'entropy'\n        print(\"Feature Selection object created\")\n        \n    def Classification_Feature_Selector(self,data):\n        estimator = RandomForestClassifier(n_estimators = 500,\n                    max_depth = 5,\n                    min_samples_split = 5,\n                    min_samples_leaf = 3,\n                    max_leaf_nodes = None,\n                    bootstrap = True,\n                    class_weight= 'balanced',\n                    random_state = 29,\n                    criterion = 'entropy')\n        \n        selector = RFE(estimator,6,step=1)\n        selector = selector.fit(x_train,y_train)\n        rank =pd.DataFrame(selector.ranking_,\\\n                        columns=['Importance'])\n        Columns = pd.DataFrame(x_train.columns,\\\n                            columns=['Columns'])\n        Var = pd.concat([rank,Columns],axis=1)\n        Var.sort_values([\"Importance\"], axis=0,\\\n                    ascending=True, inplace=True) \n        return Var\n    \n    def Feature_visualizer(self,data):\n        RF_Selector = RandomForestClassifier(n_estimators = 500,\n                      max_depth = 5,\n                      min_samples_split = 5,\n                      min_samples_leaf = 3,\n                      max_leaf_nodes = None,\n                      bootstrap = True,\n                      class_weight= 'balanced',\n                      random_state = 29,\n                      criterion = 'entropy')\n        \n        RF_Selector = RF_Selector.fit(x_train,y_train)\n        importances = RF_Selector.feature_importances_\n        std = np.std([tree.feature_importances_ for tree \\\n                          in RF_Selector.estimators_],\n                         axis=0)\n        indices = np.argsort(importances)[::-1]\n\n            # Print the feature ranking\n        print(\"Feature ranking:\")\n        for f in range(x_train.shape[1]):\n            print(\"%d. feature %d (%f)\" % (f + 1, indices[f],\\\n                                        importances[indices[f]]))\n\n            # Plot the feature importances of the forest\n\n        plt.figure(1, figsize=(14, 13))\n        plt.title(\"Feature importances\")\n        plt.bar(range(x_train.shape[1]), importances[indices],\n                   color=\"g\", yerr=std[indices], align=\"center\")\n        plt.xticks(range(x_train.shape[1]), \\\n                    x_train.columns[indices],rotation=90)\n        plt.xlim([-1, x_train.shape[1]])\n        plt.show()    ","c4a38eb4":"FSS = Feature_Selection(500,5,5,3,None,True,'balanced',29,'entropy')","78f2cf48":"FSS.Classification_Feature_Selector(dftrain)","514a58f8":"FSS.Feature_visualizer(dftrain)","eec74978":"col_list = ['Elevation','EL_DIS','Road_fire','EL_Fire','soil_type','EL_Road','Horizontal_Distance_To_Roadways','Cover_Type']\ncol_list1 = ['Elevation','EL_DIS','Road_fire','EL_Fire','soil_type','EL_Road','Horizontal_Distance_To_Roadways']\nclass Column_Dopper_After_Feature_Selction():\n\n    \n    def __init__(self):\n        print(\"Column Dopper object created\")\n    \n    \n    def dropper(self,x):\n        \n        \"\"\"\n        This method helps\n        to drop the columns\n        in our original \n        dataframe which is \n        available in the \n        col_list and return \n        us final dataset\n        \"\"\"\n        \n        data=[]\n        for i in x.columns:\n            if i in col_list:\n                data.append(i)\n        return x[data]\n","f45626a3":"CD = Column_Dopper_After_Feature_Selction()","e625d425":"New_dftrain = CD.dropper(dftrain)\nNew_dftrain.head()","17885198":"New_dftest = CD.dropper(dftest)\nNew_dftest.head()","c77752a3":"x1 = New_dftrain.drop(['Cover_Type'],axis=1)\ny1 = New_dftrain['Cover_Type']\nx_train1,x_test1,y_train1,y_test1=train_test_split(x1\\\n                ,y1,test_size=0.30,random_state=42)\n\nRF_Selector = RandomForestClassifier(n_estimators = 250,\n                    max_depth = 10,\n                    min_samples_split = 3,\n                    min_samples_leaf = 3,\n                    max_leaf_nodes = None,\n                    bootstrap = True,\n                    class_weight= 'balanced',\n                    criterion = 'entropy')\nRF_Selector = RF_Selector.fit(x_train1,y_train1)\nrf_pred = RF_Selector.predict(New_dftest)\nrf_pred","9a9382e6":"pred = pd.DataFrame(rf_pred,columns=['Pred'])\npred.head()","0c44c7f0":"import joblib\njoblib.dump(RF_Selector,  'RF_Modeljob.pkl',compress=3)","76090188":"joblib.__version__","d13b1aa0":"RF_Selector = joblib.load('RF_Modeljob.pkl')\nRF_Selector","58876fd4":"# Forest Cover Classification","fd6e92c1":"## Prediction for Original Test Data","f5656f2e":"# Missing Value Imputation","3d80c759":"# Model Selection","db3d41cd":"# Exploratory Data Analysis","01e898e1":"### Explaination of the data\n\nOur dataset has `54` features and `1` target variable `'Cover_Type'`. From `54` features, `10` are `numeric` and `44` are `catrgorical`. From `44` categorical, `40` are of `Soil_Type` and `4` of `Wilderness_Area`.\n\nWe have been provided the names of all `Soil_Type` and `Wilderness_Areas` for this dataset. The table below lists all the names with respect to their feature names in the column:\n\nThis information is available on [Kaggle](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset), [UCI](https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/covtype\/covtype.info) but for convenience I have documented here too.\n\n<br><br>\n\n| Feature Name | Names |\n| ------------ | ----- |\n| Wilderness_Area1 | Rawah Wilderness Area |\n| Wilderness_Area2 | Neota Wilderness Area |\n| Wilderness_Area3 | Comanche Wilderness Area |\n| Wilderness_Area4 | Cache La Poudre Wilderness Area |\n| Soil_Type1 | Cathedral family - Rock outcrop complex, extremely stony |\n| Soil_Type2 | Vanet - Ratake families complex, very stony |\n| Soil_Type3 | Haploborolis - Rock outcrop complex, rubbly |\n| Soil_Type4 | Ratake family - Rock outcrop complex, rubbly |\n| Soil_Type5 | Vanet family - Rock outcrop complex, rubbly |\n| Soil_Type6 | Vanet - Wetmore families - Rock outcrop complex, stony |\n| Soil_Type7 | Gothic family |\n| Soil_Type8 | Supervisor - Limber families complex |\n| Soil_Type9 | Troutville family, very stony |\n| Soil_Type10 | Bullwark - Catamount families - Rock outcrop complex, rubbly |\n| Soil_Type11 | Bullwark - Catamount families - Rock land complex, rubbly |\n| Soil_Type12 | Legault family - Rock land complex, stony |\n| Soil_Type13 | Catamount family - Rock land - Bullwark family complex, rubbly |\n| Soil_Type14 | Pachic Argiborolis - Aquolis complex |\n| Soil_Type15 | _unspecified in the USFS Soil and ELU Survey_ |\n| Soil_Type16 | Cryaquolis - Cryoborolis complex |\n| Soil_Type17 | Gateview family - Cryaquolis complex |\n| Soil_Type18 | Rogert family, very stony |\n| Soil_Type19 | Typic Cryaquolis - Borohemists complex |\n| Soil_Type20 | Typic Cryaquepts - Typic Cryaquolls complex |\n| Soil_Type21 | Typic Cryaquolls - Leighcan family, till substratum complex |\n| Soil_Type22 | Leighcan family, till substratum, extremely bouldery |\n| Soil_Type23 | Leighcan family, till substratum, - Typic Cryaquolls complex. |\n| Soil_Type24 | Leighcan family, extremely stony |\n| Soil_Type25 | Leighcan family, warm, extremely stony |\n| Soil_Type26 | Granile - Catamount families complex, very stony |\n| Soil_Type27 | Leighcan family, warm - Rock outcrop complex, extremely stony |\n| Soil_Type28 | Leighcan family - Rock outcrop complex, extremely stony |\n| Soil_Type29 | Como - Legault families complex, extremely stony |\n| Soil_Type30 | Como family - Rock land - Legault family complex, extremely stony |\n| Soil_Type31 | Leighcan - Catamount families complex, extremely stony |\n| Soil_Type32 | Catamount family - Rock outcrop - Leighcan family complex, extremely stony |\n| Soil_Type33 | Leighcan - Catamount families - Rock outcrop complex, extremely stony |\n| Soil_Type34 | Cryorthents - Rock land complex, extremely stony |\n| Soil_Type35 | Cryumbrepts - Rock outcrop - Cryaquepts complex |\n| Soil_Type36 | Bross family - Rock land - Cryumbrepts complex, extremely stony |\n| Soil_Type37 | Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony |\n| Soil_Type38 | Leighcan - Moran families - Cryaquolls complex, extremely stony |\n| Soil_Type39 | Moran family - Cryorthents - Leighcan family complex, extremely stony |\n| Soil_Type40 | Moran family - Cryorthents - Rock land complex, extremely stony |\n| Cover_Type | Target Variable - It contain mixture of all Families and yet to be Classified |\n","7f2a2fe3":"# Feature Engineering","aba0d021":"# Data Modelling","65a6eca0":"# Data Preprocessing","f7bb8743":"# PROJECT GOAL\n<br>\n\nThe **Requirement** of a Business Problem **is to develop a predictive model to Analyse and to classify the Coverage type of the forest**  using **Classification Analysis** with **Python**.\n\n\n# Description\n\nA short description I want to give of how I am going to solve this project before starting. Our goal in this project is to classify which forest type it is from the data given.\n\nThis study area includes 4 Wilderness Areas located in the Roosevelt National Forest of Northern Colorado. These area represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological process rather than forest management practices.\n\n Each observation is 30m x 30m forest cover type determined from US Forest Service (USFS) Region 2 Resource Information System (RIS) data. Independent variables were derived from the data originally obtained from US Geological Survey (USGS) and USFS data.\n\nI have been given a total of 54 attributes\/features, (excluding 1 target variable) these attributes contain Binary and Quantative attributes, and I need to predict which Forest Cover-Type is it from the given features.\n\nI will first explore the data, visualize it, know what the data wants to tell us. Remove any missing values and features that have null values and scale the data within a specific range.\n\n Also perform dimensionality reduction procedure where I will use 4 models to tell us which are useful in order to predict the target variable, and then using features which gives us hgih score in the most models. Those 4 Models are:\n        Extra Trees Classifier (ETC)\n        Random Forest (RF)\n        AdaBoost Classifier (ADBC)\n        Gradient Boosting Classifier (GBC)\n\nSplit the data 75%-25%, train-test set respectively. Will use 10 K-fold Cross Validation on train set.\n\nFeed the training data to the Naive Bayes (Our Benchmark Model) and evaluate the result.\n\nTraining will be done on the Solution Models that I have chose, those are:-\n\n        K-Nearest Neighbour (KNN),\n        Random Forest (RF),\n        Stochastic Gradient Descent Classifier (SGDC),\n        Extra Trees Classifier (ETC),\n        Logistic Regression (LG)\n\n    Scores will be evaluated with Accuracy and F1 score metrics.\n\nChoosing the best model from above based on metrics scores and testing that model on the test set.\n","dbe59939":"# Data Modelling\n\n## HyperParameter Tuning","ab6a2648":"## Serializing Model"}}