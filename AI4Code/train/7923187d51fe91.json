{"cell_type":{"0cfa1ef7":"code","3b8472a6":"code","4aa9c8b2":"code","e3e523f3":"code","02cc01dc":"code","20776b3a":"code","c108cd85":"code","fbd4e46a":"code","8081c57f":"code","795a0ba6":"code","46ee8130":"code","f3141cdc":"code","9e081e34":"code","49b16ed7":"code","3d74f4f5":"code","c0130101":"code","17e4bc45":"code","b5f5ddf9":"code","1396ee15":"code","44b24afb":"code","2d5caba5":"code","59594ae5":"code","44b2a756":"code","4456cd82":"code","ae9c72ed":"code","f0b684cf":"code","e321e988":"code","23abfb37":"code","b2f6b260":"code","01e274d2":"code","c55b39ef":"code","0b1677a9":"code","841d3679":"code","81a481b9":"code","47cf9858":"code","35121c97":"code","10f2a30d":"code","2fefc459":"code","1b76be4e":"code","253989ba":"code","ffedcae9":"code","5443463b":"code","00e436ff":"code","8076166f":"code","f53d7e24":"code","c0e630a9":"code","590963ee":"code","c753e50a":"code","ce54f81b":"code","28a9a585":"code","6151c216":"code","e6ff210e":"code","a1405b4e":"code","f525b29b":"code","ba25f778":"code","c12db866":"code","c689c3c4":"code","3e1466ea":"code","8ad60170":"code","2cc0d412":"code","03d4f95a":"code","f0944d7a":"code","a0f32739":"code","85cf364e":"code","cdfe66d0":"code","fc4b3c7b":"code","afef61e0":"code","a01cf5ae":"code","a695f310":"code","27fcc009":"code","5136db32":"code","529ba8c5":"code","3170aa75":"code","536ee185":"code","da0d30dc":"code","63d659f7":"code","f18b9695":"code","0a6d20a5":"code","95e86d72":"code","d02d8901":"code","5aac947d":"code","39341722":"code","93989b3e":"code","8962a07b":"code","5c9880a1":"code","e4ac2099":"code","227c09cf":"code","a8932a5d":"code","d9bc3068":"code","a303984e":"code","d09b79f3":"code","e42e4309":"code","6334a845":"code","d02a4a44":"code","faa137cc":"code","12ea2166":"code","112ac130":"code","8b439fcd":"code","304e96a4":"code","5789d71e":"code","26876a65":"code","651db285":"code","b7403de4":"code","1b68577c":"code","5c0a6139":"code","386258a4":"code","95595fae":"code","1e839030":"code","31a7081a":"code","896d0cba":"code","083c34bc":"code","a6d3e290":"code","2172383b":"code","bf10cf0d":"code","561d82d2":"code","9b84f7c6":"code","207b9ed7":"code","f014ec18":"code","03b685ef":"code","e7b447ad":"code","27d5c781":"code","efedc3ac":"code","9690b18b":"code","2e3038a3":"code","b5c71063":"markdown","7be70cc9":"markdown","62c6ea1d":"markdown","5ee9839a":"markdown","73845e71":"markdown","3bc66f99":"markdown","03c0b37b":"markdown","2348cacf":"markdown","7ba5fbfe":"markdown","cf6b06cb":"markdown","41549465":"markdown","3fce9f3e":"markdown","7bdda450":"markdown","1362b4ff":"markdown","a759f533":"markdown","410297da":"markdown","dc0c8223":"markdown","9ceaa4c1":"markdown","f5aeff76":"markdown","97def54e":"markdown","560cfdfe":"markdown","e963304e":"markdown","2b23a9e9":"markdown","0de59fe4":"markdown","fbaac121":"markdown","b7bfdd5e":"markdown","86079ce2":"markdown","5811ea03":"markdown"},"source":{"0cfa1ef7":"\n# print(\"\\n... IMPORTS STARTING ...\\n\")\n# print(\"\\n\\tVERSION INFORMATION\")\n# # Machine Learning and Data Science Imports\n# import tensorflow as tf; print(f\"\\t\\t\u2013 TENSORFLOW VERSION: {tf.__version__}\");\n# import tensorflow_addons as tfa; print(f\"\\t\\t\u2013 TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\n\n\n# # Built In Imports\n# from kaggle_datasets import KaggleDatasets\n# from collections import Counter\n# from datetime import datetime\n# from glob import glob\n# import warnings\n# import requests\n# import imageio\n# import IPython\n# import urllib\n# import zipfile\n# import pickle\n# import random\n# import shutil\n# import string\n# import math\n# import time\n# import gzip\n# import ast\n# import sys\n# import io\n# import gc\n# import re\n\n# # Visualization Imports\n# from matplotlib.colors import ListedColormap\n# import matplotlib.patches as patches\n# import plotly.graph_objects as go\n# import plotly.express as px\n# import seaborn as sns\n# import matplotlib; print(f\"\\t\\t\u2013 MATPLOTLIB VERSION: {matplotlib.__version__}\");\n# import plotly\n# import PIL\n    \n# print(\"\\n\\n... IMPORTS COMPLETE ...\\n\")\n\n","3b8472a6":"# TRAIN_CSV_PATH = \"\/kaggle\/input\/siim-covid19-updated-train-labels\/updated_train_labels.csv\"\n# SS_CSV_PATH = \"\/kaggle\/input\/siim-covid19-updated-train-labels\/updated_sample_submission.csv\"\n\n# print(\"\\n\\nCOMBINED AND EXPLODED TRAIN DATAFRAME\\n\\n\")\n# train_df = pd.read_csv(TRAIN_CSV_PATH)\n# display(train_df)\n\n# print(\"\\n\\nSAMPLE SUBMISSION DATAFRAME\\n\\n\")\n# ss_df = pd.read_csv(SS_CSV_PATH)\n# display(ss_df)\n\n","4aa9c8b2":"import pandas as pd\n","e3e523f3":"study_df = pd.read_csv(\"..\/input\/siim-covid19-detection\/train_study_level.csv\")\n","02cc01dc":"study_df.head()","20776b3a":"study_df.sum()","c108cd85":"study_df.describe()","fbd4e46a":"study_df[~study_df.id.str.contains(\"study\")]","8081c57f":"study_df[\"id\"] = study_df[\"id\"].str.replace(\"_study\", \"\")\n","795a0ba6":"study_df.head()","46ee8130":"import os\n","f3141cdc":"def get_absolute_file_paths(directory):\n    all_abs_file_paths = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            all_abs_file_paths.append(os.path.abspath(os.path.join(dirpath, f)))\n    return all_abs_file_paths","9e081e34":"from tqdm.notebook import tqdm; tqdm.pandas();\n","49b16ed7":"study_df[\"study_dir\"] = \"\/kaggle\/input\/siim-covid19-detection\/train\/\"+study_df[\"id\"]\nstudy_df[\"images_per_study\"] = study_df.study_dir.progress_apply(lambda x: len(get_absolute_file_paths(x)))\n# study_df[\"images_per_study\"] = study_df.study_dir.apply(lambda x: len(get_absolute_file_paths(x)))\n","3d74f4f5":"study_df.head()","c0130101":"# study_df.images_per_study.describe()\nstudy_df.images_per_study.value_counts()\n","17e4bc45":"row = study_df.iloc[0]","b5f5ddf9":"row","1396ee15":"# ! ls \/kaggle\/input\/siim-covid19-detection\/train\/ | grep 00086460a852\n! ls \/kaggle\/input\/siim-covid19-detection\/train\/00086460a852\/9e8302230c91\/65761e66de9f.dcm\n","44b24afb":"multiple_images_per_study_df = study_df[study_df.images_per_study>1].reset_index(drop=True)\n","2d5caba5":"multiple_images_per_study_df.head()","59594ae5":"multiple_images_per_study_df.shape","44b2a756":"import matplotlib.pyplot as plt\n","4456cd82":"image_df = pd.read_csv(\"\/kaggle\/input\/siim-covid19-detection\/train_image_level.csv\")\n","ae9c72ed":"image_df.head()","f0b684cf":"all_image_ids = image_df.id.str.replace(\"_image\", \"\")\nbbox_image_ids = image_df.dropna().id.str.replace(\"_image\", \"\")","e321e988":"# all_image_ids\nlen(all_image_ids)","23abfb37":"all_image_ids[:10]","b2f6b260":"# bbox_image_ids\nlen(bbox_image_ids)","01e274d2":"bbox_image_ids[:10]","c55b39ef":"import numpy as np; print(f\"\\t\\t\u2013 NUMPY VERSION: {np.__version__}\");\n","0b1677a9":"import pydicom\nfrom pydicom import dcmread\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n","841d3679":"# # Installs\n# !cp \/kaggle\/input\/gdcm-conda-install\/gdcm.tar .\n# !tar -xvzf gdcm.tar\n# !conda install --offline .\/gdcm\/gdcm-2.8.9-py37h71b2a6d_0.tar.bz2\n# !rm -rf .\/gdcm.tar\n","81a481b9":"'''\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    \"\"\" Convert dicom file to numpy array \n    \n    Args:\n        path (str): Path to the dicom file to be converted\n        voi_lut (bool): Whether or not VOI LUT is available\n        fix_monochrome (bool): Whether or not to apply monochrome fix\n        \n    Returns:\n        Numpy array of the respective dicom file \n        \n    \"\"\"\n    # Use the pydicom library to read the dicom file\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to \n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n        \n    # The XRAY may look inverted\n    #   - If we want to fix this we can\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    \n    # Normalize the image array and return\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n'''\n\n\ndef dicom2array_2(fname, target_size=512, use_clahe=True, clip_limit=2., grid_size=(8,8)):\n    dicom = pydicom.dcmread(fname)\n    \n    try:\n        data = dicom.pixel_array\n    except Exception as err:\n        print('exception seen=', err)\n        data = None\n    \n#     data = apply_voi_lut(dicom.pixel_array, dicom)\n#     im = data - np.min(data)\n#     im = 255. * im \/ np.max(im)\n    \n#     if dicom.PhotometricInterpretation == \"MONOCHROME1\": # check for inverted image\n#         im = 255. - im\n    \n#     if use_clahe:\n#         clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n#         climg = clahe.apply(im.astype('uint8'))\n#         img = Image.fromarray(climg.astype('uint8'), 'L')\n#     else:\n#         img = Image.fromarray(im.astype('uint8'), 'L')\n#     org_size = img.size\n    \n#     if max(img.size) > target_size:\n#         img.thumbnail((target_size, target_size), Image.ANTIALIAS)\n    \n#     return np.asarray(img)\n    return data\n","47cf9858":"import cv2\n","35121c97":"from PIL import Image\n","10f2a30d":"for ix, case in multiple_images_per_study_df.head(20).iterrows():\n    print('new case= \\n ', case)\n    dir_path = case.study_dir\n    image_paths = get_absolute_file_paths(dir_path)\n    \n    selected = []\n    rejected = []\n    \n    if len(image_paths) <= 4:\n        plt.figure(figsize=(18,4))\n        print('\\n dir_path=', dir_path)\n        \n        study_id = dir_path.rsplit('\/', 1)[1]\n        print('\\n study_id=', study_id)\n        \n        \n        plt.suptitle(f\"\\n\\nSTUDY: {study_id}\", fontsize=16, fontweight=\"bold\")\n        print('\\n image_paths=', image_paths)\n        \n        for j, x in enumerate(image_paths):\n            print(j, x)\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n                    selected.append(x)\n                    \n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n                \n            data = dicom2array_2(x)\n            \n            if data is not None:\n                plt.subplot(1,4,j+1)\n                plt.imshow(data)\n                plt.axis(False)\n                plt.title(title, fontweight=\"bold\")\n                \n        print('\\n selected=', selected)\n        selected_img = selected[0]\n        selected_img_id = selected_img.split('\/')[-1]\n        selected_img_id = selected_img_id.replace('.dcm', '_image')\n        print('selected_img_id=', selected_img_id)\n\n        docs_img_selected = image_df[(image_df.StudyInstanceUID == study_id) & (image_df.id == selected_img_id)] \n        print('\\n docs_img_selected =\\n', docs_img_selected.iloc[0])\n        \n            \n    else:\n        print('not processing this right now')\n    \n    '''\n    elif len(image_paths)<=8:\n        plt.figure(figsize=(18,8))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('\/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(2,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n    else:\n        plt.figure(figsize=(18,12))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('\/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(3,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.title(title, fontweight=\"bold\")\n            plt.axis(False)\n    '''\n\n    plt.tight_layout()\n    plt.show()\n#     break\n\n    print('===============================\\n')","2fefc459":"study_df.head()","1b76be4e":"\nfor dir_path in study_df.study_dir.values:\n    print('dir_path=', dir_path)\n    image_paths = get_absolute_file_paths(dir_path)\n    if len(image_paths)<=4:\n        plt.figure(figsize=(18,4))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('\/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            print(j,x)\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(1,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n    elif len(image_paths)<=8:\n        plt.figure(figsize=(18,8))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('\/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(2,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n    else:\n        plt.figure(figsize=(18,12))\n        plt.suptitle(f\"\\n\\nSTUDY: {dir_path.rsplit('\/', 1)[1]}\", fontsize=16, fontweight=\"bold\")\n        for j, x in enumerate(image_paths):\n            if any(True for xx in all_image_ids if xx in x):\n                title = \"\\nINCLUDED IN IMAGE LEVEL\"\n                if any(True for xx in bbox_image_ids if xx in x):\n                    title += \" - W\/ BBOX!!!\"\n            else:\n                title = \"xxxxxxxxxxxxxxxxxxxxxxx\"\n            plt.subplot(3,4,j+1)\n            plt.imshow(dicom2array_2(x))\n            plt.title(title, fontweight=\"bold\")\n            plt.axis(False)\n    \n    plt.tight_layout()\n    plt.show()\n    break","253989ba":"study_df.head()","ffedcae9":"study_df.columns","5443463b":"study_df['all_sum'] = study_df.apply(lambda row: row['Negative for Pneumonia'] + \n                                     row['Typical Appearance'] + row['Indeterminate Appearance']\n                                    + row['Atypical Appearance'], axis=1)","00e436ff":"study_df.head()","8076166f":"study_df['all_sum'].value_counts()","f53d7e24":"image_df.shape","c0e630a9":"image_df.head()","590963ee":"image_df[['image_id','image_type']]= image_df['id'].str.split('_',expand=True)","c753e50a":"image_df.head()","ce54f81b":"image_df.StudyInstanceUID.describe()","28a9a585":"image_df.image_id.describe()","6151c216":"image_df.image_type.describe()","e6ff210e":"image_df[image_df.StudyInstanceUID == '0fd2db233deb']","a1405b4e":"image_df[image_df.StudyInstanceUID == '0fd2db233deb'].iloc[3].label","f525b29b":"image_df[image_df.StudyInstanceUID == '0fd2db233deb'].iloc[3].label","ba25f778":"image_df[image_df.StudyInstanceUID == '0fd2db233deb'].iloc[3].label.split()\n","c12db866":"image_df[image_df.StudyInstanceUID == '0fd2db233deb'].iloc[3].boxes","c689c3c4":"eval(image_df[image_df.StudyInstanceUID == '0fd2db233deb'].iloc[3].boxes)","3e1466ea":"study_df[study_df.id == '0fd2db233deb']","8ad60170":"study_df[study_df.id == '0fd2db233deb'].iloc[0].study_dir","2cc0d412":"all_labels = list(image_df.label)\nall_labels_list = [k.split(' ') for k in all_labels]\n","03d4f95a":"# all_labels_list[:3]","f0944d7a":"possible_lens = [len(k) for k in all_labels_list]","a0f32739":"possible_values = set([k[0] for k in all_labels_list])","85cf364e":"possible_values","cdfe66d0":"d = {12:0, 6:0, 18:0, 24:0, 30:0, 48:0}\nfor k in possible_lens:\n#     print(k)\n    d[k] += 1\n","fc4b3c7b":"d","afef61e0":"image_df['boxes'].fillna(0,inplace=True)","a01cf5ae":"image_df.head()","a695f310":"image_df['len_boxes'] = image_df['boxes'].apply(lambda row: len(eval(row)) if row else 0)","27fcc009":"image_df.head()","5136db32":"image_df.len_boxes.value_counts()","529ba8c5":"image_df['label_type'] = image_df.label.str.split(expand=True)[0]","3170aa75":"image_df.head()","536ee185":"image_df.label_type.value_counts()","da0d30dc":"image_df.head()","63d659f7":"study_df.head()","f18b9695":"print(image_df.columns,study_df.columns)","0a6d20a5":"image_study_df = pd.merge(image_df, \n                          study_df, \n                          left_on='StudyInstanceUID',\n                          right_on='id')","95e86d72":"image_study_df.shape","d02d8901":"image_study_df.head()","5aac947d":"# image_study_df['study_id'] = image_study_df['id_y']","39341722":"image_study_df.head()","93989b3e":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb']","8962a07b":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3]","5c9880a1":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3].study_dir","e4ac2099":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3].image_id","227c09cf":"study_dir = '\/kaggle\/input\/siim-covid19-detection\/train\/0fd2db233deb'","a8932a5d":"! ls  \/kaggle\/input\/siim-covid19-detection\/train\/0fd2db233deb\/","d9bc3068":"! ls -r \/kaggle\/input\/siim-covid19-detection\/train\/0fd2db233deb\/*\/","a303984e":"! ls \/kaggle\/input\/siim-covid19-detection\/train\/0fd2db233deb\/9025f953c3d2\/26f643772090.dcm","d09b79f3":"path = '\/kaggle\/input\/siim-covid19-detection\/train\/0fd2db233deb\/9025f953c3d2\/26f643772090.dcm'","e42e4309":"    # Use the pydicom library to read the dicom file\n    dicom = pydicom.read_file(path)\n","6334a845":"data = dicom.pixel_array\n","d02a4a44":"data","faa137cc":"data.shape","12ea2166":"            plt.imshow(dicom2array_2(x))\n            plt.axis(False)\n            plt.title(title, fontweight=\"bold\")\n","112ac130":"import matplotlib.pyplot as plt\nplt.imshow(data, cmap='gray')\nplt.show()","8b439fcd":"# this can help with increasing the contrast\nfrom skimage import exposure\nequ_img = exposure.equalize_hist(data)\nplt.imshow(equ_img, cmap='gray')\nplt.show()","304e96a4":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3]","5789d71e":"eval(image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3].boxes)","26876a65":"image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3].label","651db285":"box1 = image_study_df[image_study_df['StudyInstanceUID']=='0fd2db233deb'].iloc[3].label.split()","b7403de4":"box1","1b68577c":"img = data","5c0a6139":"img = cv2.rectangle(img,(int(float(box1[2])), int(float(box1[3]))), \n                    (int(float(box1[4])), int(float(box1[5]))),\n                    color=(200, 0, 0), thickness=15)","386258a4":"plt.imshow(img, cmap='gray')\nplt.show()","95595fae":"equ_img = exposure.equalize_hist(data)\n\nequ_img = cv2.rectangle(equ_img,(int(float(box1[2])), int(float(box1[3]))), \n                    (int(float(box1[4])), int(float(box1[5]))),\n                    color=(200, 0, 0), thickness=15)","1e839030":"plt.imshow(equ_img, cmap='gray')\nplt.show()","31a7081a":"image_study_df.shape","896d0cba":"# image_study_df[image_study_df.boxes == 0].head()","083c34bc":"# image_study_df[image_study_df.boxes == 0].shape","a6d3e290":"# image_study_df[image_study_df.boxes != 0].shape","2172383b":"# image_study_df[image_study_df.boxes != 0].head()","bf10cf0d":"nobox_docs = image_study_df[image_study_df.boxes == 0]","561d82d2":"box_docs = image_study_df[image_study_df.boxes != 0]","9b84f7c6":"nobox_docs.shape","207b9ed7":"box_docs.shape","f014ec18":"nobox_docs.head()","03b685ef":"box_docs.head()","e7b447ad":"nobox_docs.study_id.describe()","27d5c781":"nobox_docs['Negative for Pneumonia'].value_counts()","efedc3ac":"nobox_docs['Typical Appearance'].value_counts()","9690b18b":"box_docs['Negative for Pneumonia'].value_counts()","2e3038a3":"box_docs['Typical Appearance'].value_counts()","b5c71063":"### viz  another image","7be70cc9":"### So for each test study, possible labels:\n\n'Negative for Pneumonia' - 1676 have this - 27%\n\n\n'Typical Appearance' - 2855 have this - 47%\n\n\n'Indeterminate Appearance' - 1049 have this - 17% \n\n'Atypical Appearance' - 474 have this - 7%\n\n\n#### Overall Multi-class(4), unbalanced classification problem\n\n","62c6ea1d":"### This work is adaped from existing notebooks of:\n\nhttps:\/\/www.kaggle.com\/dschettler8845\/covid-detection-studies-with-multiple-images-viz\n\nhttps:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/240878\n\nhttps:\/\/www.kaggle.com\/c\/siim-covid19-detection\/discussion\/246597\n\nfor getting started and also see multi-image cases.\n\n-----------------------\n\nhttps:\/\/www.kaggle.com\/jaideepvalani\/basic-exploration-eda-duplicate-nonduplicate\n\nhttps:\/\/www.kaggle.com\/devanshchowdhury\/eda-understand-data\n\nfor further EDA on study\/image level rows\n\n\n-----------------------\n\nhttps:\/\/www.kaggle.com\/aleksandramowio\/getting-started-simple-eda\n\nto read image data\/voxels\n\n\n\n-----------------------\n\n","5ee9839a":"### Background\n\nIn this competition, we are identifying and localizing COVID-19 abnormalities on chest radiographs. <br>**This is an object detection and classification problem.**\n\nFor each test image, you will be predicting a bounding box and class for all findings. \n* If you predict that there are no findings, you should create a prediction of **`\"none 1 0 0 1 1\"`** \n    * \"none\" is the class ID for no finding, and this provides a one-pixel bounding box with a confidence of 1.0\n\n\nTo make a prediction of one of the above labels, create a prediction string similar to the \"none\" class above: \n* i.e. **`atypical 1 0 0 1 1`**\n\n---\n\n**MESSAGE FROM THE COMPETITION HOST ON LABEL AND BBOX DETAILS:**\n\nIn this challenge, the chest radiographs (CXRs) were categorized using a specific grading schema, based on a published paper:\n\n[**Litmanovich DE, Chung M, Kirkbride RR, Kicska G, Kanne JP. Review of chest radiograph findings of COVID-19 pneumonia and suggested reporting language. Journal of thoracic imaging. 2020 Nov 14;35(6):354-60.**](https:\/\/journals.lww.com\/thoracicimaging\/Fulltext\/2020\/11000\/Review_of_Chest_Radiograph_Findings_of_COVID_19.4.aspx)\n\nPer the grading schema, chest radiographs are classified into one of four categories, which are mutually exclusive:\n\n1. **Typical Appearance**: Multifocal bilateral, peripheral opacities with rounded morphology, lower lung\u2013predominant distribution\n2. **Indeterminate Appearance**: Absence of typical findings AND unilateral, central or upper lung predominant distribution\n3. **Atypical Appearance**: Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity\n4. **Negative for Pneumonia**: No lung opacities\n\nBounding boxes were placed on lung opacities, whether typical or indeterminate. Bounding boxes were also placed on some atypical findings including solitary lobar consolidation, nodules\/masses, and cavities. Bounding boxes were not placed on pleural effusions, or pneumothoraces. No bounding boxes were placed for the negative for pneumonia category.\n\nIn cases of multiple adjacent opacities, we opted for one large bounding box, rather than multiple adjacent smaller boxes, to improve consistency in the labeling.\n\nAnnotators did have access to the COVID status for each patient, but were asked to adhere to the grading system above irrespective of the status. As such, some patients who were COVID negative still had chest radiographs with typical appearances. Similarly, some patients who were COVID positive had atypical appearances, or were negative for pneumonia (no lung opacities), because the grading system is based off the chest radiographic findings alone.\n\nThe goal in this challenge is to determine the appropriate category for each radiograph, as well as localize the lung opacities with a bounding box prediction.\n\n---\n\nThe images are in DICOM format, which means they contain additional data that might be useful for visualizing and classifying.\nNote that the images are in **DICOM** format, which means they contain additional data that might be useful for visualizing and classifying.\n\n![Example Radiographs](https:\/\/i.imgur.com\/QWmbhXx.png)\n\n<br>\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">DATASET INFORMATION<\/b>\n\nThe **train dataset** comprises **`6,334`** chest scans in **DICOM** format, which were de-identified to protect patient privacy. \n\nNote that all images are stored in paths with the form **`study\/series\/image`**. \n* The **`study`** ID here relates directly to the study-level predictions\n* the **`image`** ID is the ID used for image-level predictions\n\nThe **test dataset** is of roughly the same scale as the training dataset. \n* As this is a kernels only competition we shsould plan accordingly\n* i.e. we should be able to infer on the entirety of the training dataset within the submission kernel\n\n<br>\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">DATA FILES<\/b>\n> **`train_study_level.csv`**\n> * **`id`** - unique study identifier\n> * **Negative for Pneumonia** - **`1`** if the study is negative for pneumonia, **`0`** otherwise\n> * **Typical Appearance** - **`1`** if the study has this appearance, **`0`** otherwise\n> * **Indeterminate Appearance**  - **`1`** if the study has this appearance, **`0`** otherwise\n> * **Atypical Appearance**  - **`1`** if the study has this appearance, **`0`** otherwise\n\n> **`train_image_level.csv`**\n> * **`id`** - unique image identifier\n> * **`boxes`** - bounding boxes in easily-readable dictionary format\n> * **`label`** - the correct prediction label for the provided bounding boxes","73845e71":"### images without boxes","3bc66f99":"### dig bit more into imagelevel df","03c0b37b":"<br>\n\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: darkred; background-color: #ffffff;\">1.2  THE GOAL<\/h2>\n\n---\n\nIn this competition, you\u2019ll identify and localize COVID-19 abnormalities on chest radiographs. In particular, you'll categorize the radiographs as one of a possible **`4`** categories. \n\nIn this competition, we are making predictions at both a study (multi-image) and image level.\n* **`negative for pneumonia`** or **`typical`**, **`indeterminate`**, or **`atypical`** \n     \nYou'll work with a dataset consisting of **`8,781`** scans that have been annotated by experienced radiologists. You can train your model with **`6,334`** independently-labeled images and you will be evaluated on a test set of **`2,447`** images. \n\nThe challenge uses the standard PASCAL VOC 2010 mean Average Precision (mAP) at IoU > 0.5.\n* Note that the linked document describes VOC 2012, which differs in some minor ways (e.g. there is no concept of \"difficult\" classes in VOC 2010). The P\/R curve and AP calculations remain the same.\n\n","2348cacf":"### for this need to read dicom files ","7ba5fbfe":"### so bbox\/label distribution\n\nalmost 50% have 1 bbox (can also be none), \n\n50% have 2 bbox (should not be none)\n\nno other label possible","cf6b06cb":"### these image might not be of same size - and require resizing","41549465":"### 3-level folder hierarchy here\n\nstudy_id\/folder?\/image_id.dcm","3fce9f3e":"### study can have multi images.","7bdda450":"### so we have 6.3k images belonging to 6k cases\n\n1 case has 9 images","1362b4ff":"### so opacity is one form of indeterminate appearance?\n\nyes - Bounding boxes were placed on lung opacities, whether typical or indeterminate","a759f533":"### visualise a dicom file ?","410297da":"### look at 2 dfs together?","dc0c8223":"### There are 4294 images with boxes and 2040 images without any boxes.","9ceaa4c1":"### Is it  a Multilabel Multiclass problem?\n\nNo","f5aeff76":"### so if any1 image in case is opaque, complete case becomes opaque\/indeterminate","97def54e":"### TODO:\n\nplay with few more examples to see opacity vs. none\n\nthink of classifier","560cfdfe":" ### ADDITIONAL INFORMATION ON ABNORMALITIES\n\nNegative for Pneumonia =  No lung opacities\n\nTypical Appearance = Multifocal bilateral, peripheral opacities with rounded morphology, lower lung\u2013predominant distribution\n\nIndeterminate Appearance = Absence of typical findings AND unilateral, central or upper lung predominant distribution\n\nAtypical Appearance = Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity","e963304e":"### where is train data?","2b23a9e9":"### similar + finer bbox info  now","0de59fe4":"### if you have a box, you can't be negative for pneumonia","fbaac121":"### how to read an image data","b7bfdd5e":"### how many image labels are possible?\n\nso far seen opacity and none\n","86079ce2":"### Is it  a Multilabel Multiclass problem?\n\n","5811ea03":"### also add bboxes now"}}