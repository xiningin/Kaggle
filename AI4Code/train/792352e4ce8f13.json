{"cell_type":{"a51e8eda":"code","531ebe2c":"code","ded3a79c":"code","d9a4d99e":"code","af42a80f":"code","8a24670f":"code","b7921f53":"code","45011fe8":"code","7199476b":"code","5b05dd50":"code","fb3e13dc":"code","2e8abef9":"code","d20241de":"code","de90b45e":"code","b436403c":"code","ff067718":"code","cbebf72f":"code","e52ccca6":"code","82534e39":"code","351d35d9":"code","7ec6ba08":"code","c7343379":"code","32317cb5":"code","d6142a1d":"code","247718db":"code","329c3198":"markdown"},"source":{"a51e8eda":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input,Dropout,BatchNormalization,LSTM,Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nimport traitlets\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\n# from sklearn.metrics import roc_auc_score\nwarnings.simplefilter(\"ignore\")","531ebe2c":"df = pd.read_csv(\"\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv\")\ndf.head()","ded3a79c":"df=df.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator','Time'],axis=1)\n# df.head()","d9a4d99e":"df['Score'].value_counts().plot.pie()","af42a80f":"def sentiment_rating(rating):\n    # Replacing ratings of 1,2,3 with 0 (not good) and 4,5 with 1 (good)\n    if(int(rating) == 1 or int(rating) == 2 or int(rating) == 3):\n        return 0\n    else: \n        return 1\ndf.Score= df.Score.apply(sentiment_rating) \n# df.Score.head()\ndf.Score.value_counts().plot.pie(autopct='%1.1f%%')","8a24670f":"df_pos=df[df.Score==1]\ndf_neg=df[df.Score==0]\ndf_pos = df_pos.sample(n=len(df_neg))\ndf=pd.concat((df_pos,df_neg))\ndf.head()","b7921f53":"X=df.drop(['Score'],axis=1)\n\ny=df['Score']\nX.Text=X.Text.apply(str)\nX.Summary=X.Summary.apply(str)\nX['sumtext']=X[['Summary','Text']].agg(' '.join, axis=1)\nX.head()","45011fe8":"y.value_counts().plot.pie(autopct='%1.1f%%')","7199476b":"X=X.values\ny=y.values","5b05dd50":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,shuffle=True)\nX_train.shape,X_test.shape","fb3e13dc":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1,shuffle=True)\nX_train.shape,X_val.shape","2e8abef9":"from sklearn.utils.class_weight import compute_class_weight\nc_w=compute_class_weight(class_weight='balanced',classes=np.unique(y_train),y=y_train)\nc_w","d20241de":"# First load the real tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Save the loaded tokenizer locally\nsave_path = '\/kaggle\/working\/distilbert_base_uncased\/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased\/vocab.txt', lowercase=True)\nfast_tokenizer","de90b45e":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","b436403c":"classes,counts=np.unique(y_test, return_counts=True)\nprint('Min Acc(test): ', np.max(counts)\/np.sum(counts))\nclasses,counts=np.unique(y_val, return_counts=True)\nprint('Min Acc(val): ', np.max(counts)\/np.sum(counts))","ff067718":"x_train = fast_encode(X_train[:,2], fast_tokenizer, maxlen=128)\nx_val = fast_encode(X_val[:,2], fast_tokenizer, maxlen=128)\nx_test = fast_encode(X_test[:,2], fast_tokenizer, maxlen=128)","cbebf72f":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Create strategy from tpu\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n","e52ccca6":"batch_size=64\n# AUTO = tf.data.experimental.AUTOTUNE\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_val, y_val))\n    .batch(batch_size)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = [(\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)]","82534e39":"from tensorflow.keras.optimizers import Adam\ndef build_model(transformer, max_len=128):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    x = BatchNormalization()(cls_token)\n    #x = tf.expand_dims(x, axis=-1)\n#     x = Bidirectional(LSTM(64,return_sequences=True))(x)\n#     x = Bidirectional(LSTM(64))(x)\n    x = Dropout(0.25)(x)\n    x = Dense(256,activation='relu')(x)\n    x = Dense(256,activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x = Dense(512,activation='relu')(x)\n    x = Dense(512,activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x = BatchNormalization()(x)\n    out = Dense(1, activation='sigmoid')(x)\n    \n    \n    model = Model(inputs=input_word_ids, outputs=out)\n\n    \n    model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","351d35d9":"with strategy.scope():\n    transformer_layer = transformers.TFBertModel.from_pretrained('distilbert-base-uncased')\n    model = build_model(transformer_layer, max_len=128)\n    #for layer in model.layers[:2]:\n    #    layer.trainable = False\nmodel.summary()","7ec6ba08":"def build_lrfn(lr_start=0.000001, lr_max=0.000002, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","c7343379":"lrfn = build_lrfn()\nlr_cb = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n# checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.h5',verbose=1,monitor='val_acc',mode='auto',save_best_only=True)","32317cb5":"train_history = model.fit(train_dataset,steps_per_epoch=150,verbose=1,callbacks=[lr_cb],validation_data=valid_dataset,epochs=50)","d6142a1d":"acc = train_history.history['accuracy']\nval_acc = train_history.history['val_accuracy']\nloss = train_history.history['loss']\nval_loss = train_history.history['val_loss']\nepochs = range(len(acc))\n\nfig, axes = plt.subplots(1, 2, figsize=(15,5))\n\naxes[0].plot(epochs, acc, 'r-', label='Training Accuracy')\naxes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')\naxes[0].set_title('Training and Validation Accuracy')\naxes[0].legend(loc='best')\n\naxes[1].plot(epochs, loss, 'r-', label='Training Loss')\naxes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')\naxes[1].set_title('Training and Validation Loss')\naxes[1].legend(loc='best')\n\nplt.show()","247718db":"from keras.models import load_model\nfrom sklearn.metrics import accuracy_score\n# model=load_model('best_model.h5')\ny_pred=model.predict(x_test)\naccuracy_score(np.round(y_pred),y_test)","329c3198":"Fork from [here](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras)"}}