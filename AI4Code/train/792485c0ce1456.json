{"cell_type":{"7d665d15":"code","062c077c":"code","4176c090":"code","b617929f":"code","9dd9b1c1":"code","f3d59cf8":"code","466ee0ef":"code","7de0afce":"code","96cd6e5d":"code","aff297de":"code","8ea148fe":"code","cb80ddf2":"code","cd2b268c":"code","d2a1f19f":"code","d7f83990":"code","b720d6b2":"code","3cbf23d0":"code","afb17643":"code","5c281fa5":"code","b397764c":"code","44bb5c7d":"code","119f2fb0":"markdown","c4c7c42a":"markdown","8d1a6879":"markdown","200c66ef":"markdown","0881803d":"markdown","d9c3b6e4":"markdown","c2a12aaa":"markdown","af36ec13":"markdown","c83da615":"markdown","79874b6b":"markdown","0bed506c":"markdown","c5961905":"markdown","d2780246":"markdown","1aa8b819":"markdown","af593e18":"markdown","b23d006a":"markdown","5356ec1c":"markdown","6845a39f":"markdown","d8f2bac3":"markdown"},"source":{"7d665d15":"# normal required libraries \nimport pandas as pd\nimport numpy as np\n\n# libraries for visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# sklearn import for data pre-processing\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer, KNNImputer\n\n# sklearn import for LogisticRegression and RandomForest algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\n# sklearn import to create pipelines\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# ignore warnings( not suggested for real-life projects)\nimport warnings\nwarnings.filterwarnings('ignore')","062c077c":"# as the data is in Excel format, installing excel reading engine\n!pip install openpyxl","4176c090":"# reading from  data sheet of excel\ndata = pd.read_excel(r'..\/input\/ecommerce-customer-churn-analysis-and-prediction\/E Commerce Dataset.xlsx', sheet_name = 'E Comm')\ndata.head()","b617929f":"# dividing independent and dependent features into separate variables \n\nX = data.drop(['Churn'], axis = 1)  # independent features\ny = data.Churn                      # dependent features","9dd9b1c1":"# spliting the data into train and test data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 123)","f3d59cf8":"# defining the type of feature\/columns\n\n# below two features need to be dropped as they won't help in modelling\ndrop_features = ['CustomerID', 'Gender']\n\n# below code is to find the categorical and numerical features based on datatype \n# although this is very basic logic\ncategorical_features = X.columns[X.dtypes == object].tolist()\nnumeric_features = X.columns[X.dtypes != object].tolist()\n\n# removing these two from features lists\nnumeric_features.remove('CustomerID')\ncategorical_features.remove('Gender')\n\nfeature_ls = numeric_features.copy()\n","466ee0ef":"drop_transformer = ColumnTransformer(transformers = [('drop_columns' , 'drop', drop_features)], \n                                   remainder = 'passthrough' )","7de0afce":"pipeline = Pipeline([('drop_transformer' ,drop_transformer)])\npipeline.fit(X_train) \npipeline.transform(X_train)","96cd6e5d":"# first create a normal method\ndef mp_to_phone(input_df):\n    input_df['PreferredLoginDevice'] = input_df['PreferredLoginDevice'].replace(['Mobile Phone', 'Phone'], 'XPhone')\n    return input_df\n\n# create a class for transformer\nclass NameChangeTransformer:\n    def __init__(self, function):\n        self.function = function\n        \n    # create a transform method and call you normal method\n    def transform(self, input_df, **transform_params):\n        return self.function(input_df)\n    # create a fit method, here in our case we do not need to return anything\n    def fit(self, X, y= None, **fit_params):\n        return self","aff297de":"pipeline = Pipeline([\n    ('name_changer' ,NameChangeTransformer(mp_to_phone)),\n    ('drop_columns', drop_transformer)\n])\n\npipeline.fit(X_train)\npipeline.transform(X_train)","8ea148fe":"## Creating separate transformer for categorical and numerical data\n\n\n# within list, we are writing transformation we want \nnumerical_transformer = Pipeline([\n    ('numerical_mean_imputer', SimpleImputer()),\n    ('scale' , StandardScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('encoding', OneHotEncoder( drop = 'first')),\n    ('categorical_mode_imputer', SimpleImputer(strategy = 'most_frequent'))\n])","cb80ddf2":"col_transformer = ColumnTransformer([\n    ('Drop', 'drop' , drop_features),\n    ('numerical_transformer', numerical_transformer, numeric_features),\n    ('categorical_transformer', categorical_transformer, categorical_features),\n    \n],\n    remainder = 'drop'\n)","cd2b268c":"pipeline = Pipeline([ ('NAME_CHANGER' ,NameChangeTransformer(mp_to_phone)),  # this should be first as this will return data with columns name\n                     ('ALL_COLUMNS_TRANSFORMATION', col_transformer),        # here we get array so we do not have column names\n                    ])","d2a1f19f":"pipeline","d7f83990":"temp_X_train = X_train\npipeline.fit(X_train)\ntrans_data = pd.DataFrame(pipeline.transform(temp_X_train)).copy()\ntrans_data","b720d6b2":"onehotcod_col = pipeline.named_steps['ALL_COLUMNS_TRANSFORMATION'].transformers_[2][1].named_steps['encoding'].get_feature_names(categorical_features)","3cbf23d0":"pipeline.named_steps['ALL_COLUMNS_TRANSFORMATION'].transformers_[1][1].named_steps['scale'].mean_","afb17643":"pipeline.named_steps['ALL_COLUMNS_TRANSFORMATION'].transformers_[1][1].named_steps['scale'].var_","5c281fa5":"feature_ls.extend(onehotcod_col.tolist())\ntrans_data.columns = feature_ls\ntrans_data","b397764c":"pipeline = Pipeline([\n    ('NAME_CHANGER' ,NameChangeTransformer(mp_to_phone)),  \n    ('ALL_COLUMNS_TRANSFORMATION', col_transformer), \n    ('LOGISTIC_REGRESSION' , LogisticRegression())\n])","44bb5c7d":"pipeline.fit(X_train, y_train)\nprint(pipeline.score(X_train, y_train))\nprint(pipeline.score(X_test, y_test))\ny_pred = pipeline.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred), annot = True, fmt = \"2d\")","119f2fb0":"**.fit()** will create a pipeline, to apply the transformation we need to use **.transform()**. \n\n*This is similar to what we normally do with sklearn preprocessor.","c4c7c42a":"### Here, you can see the data after transformation applied via Pipeline.","8d1a6879":"## Starting with Transformers","200c66ef":"**Now, going in more depth, we can create pipeline for separate puposes and then append them later.**\n\nBelow we are going to create two separate pipeline for Numeric and Categorical transformation, in next step we will take these pipeline and will create column transformers with them.","0881803d":"You can see the transformed data with column name as well.","d9c3b6e4":"Hi there!\nIn this notebook I am going to **use Scikit-learn Pipeline**, pipeline makes overall process quite flawless to use, also while moving model to production, **pipeline makes deployment process easy**, we can directly use pipeline pickle and we do not need to write pre-processing code again at deployment site. \n---\nI will talking more about the individual functions I use them in this notebook.","c2a12aaa":"Once pipeline is created with our model, we can go ahead with simple flow that we perform with normal code.\nBelow is the example for same.","af36ec13":"### Transformers are used for data modifications and transformations on data.","c83da615":"Also, in above case we have used one default option drop, that is we are quoting that, \notherwise we can directly write eg., ('transformation_1' , StandardScaler(), features)","79874b6b":"## Model - Estimator Code\n\nNow that we have completed transformation pipeline, now we can get into machine learning model, currently I am using Logistic Regression for this example.\n\n___\nThis will be the final pipeline, we are going to combine all and will add Logistic Regression as last, as this should done after pre-processing. ","0bed506c":"**Here, I am not concered about the EDA and other thing as this notebook is more about the Pipeline, so I will directly dive into the Pipeline code.**","c5961905":"In below cell output, we can see all the pipelines and column transformers along with the columns name and transformations.","d2780246":"So, the last output does not have the column name and that could be one issue, for that we have methods to get intermediate steps output.\n\n**This is very tricky to understand like once we have used named_steps and why then index\nnamed_steps are for pipeline, once you are reached to pipeline\nthen you will get the specific transformer of that pipeline by transformer_ keyword with index, again that transformer itself is a pipeline, thats why again names_steps**","1aa8b819":"Next step is to feed the custom transformer as usual.","af593e18":"Below, I am including one Column Transformer and 2 Pipeline that have been created, also as we are using ColumnTransformer here, we will also pass the columns name where we want to apply these transformation.\n\n\n*Also, last argument remainder will delete all the columns that are not part of columns list that we have provided for different transformations.","b23d006a":"Few more examples..","5356ec1c":"### How to create a custom transformer?!\n\n- There would be cases where we need to create few transformers that are not available by default, for that we have options to create custom transformers.","6845a39f":"\nThis is single column transformer, here within ColumnTransformer, we have argument transformers where we provide all the *transformations* we want and these are written within list as tuple.\nWithin tuple, \n- First argument is name that we want to give\n- Second is the transformations that we want\n- Third where we tell in which all columns we want to apply this transformer(transformations)\n\n*Remainder parameter is to tell what to do with columns that are not the part of transformation but are in dataframe.\n","d8f2bac3":"Again, we are creating main Pipeline for all the data transformation."}}