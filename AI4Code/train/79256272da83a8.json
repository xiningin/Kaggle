{"cell_type":{"249608c4":"code","5c1e5d44":"code","68625831":"code","55ad34f0":"code","fed9fd6d":"code","04b2c421":"code","a473fc38":"code","e6557cf6":"code","a3769a70":"code","fef592e6":"code","93cfab93":"code","cb58c823":"code","adbf0850":"code","1246e4f3":"code","297f9ce4":"code","af16d16f":"code","9800fa18":"code","99b6906a":"code","1371048f":"code","9f5dabbc":"code","09fc1dda":"code","8009b792":"code","f6839ec3":"code","de44bec3":"code","76f4a7dc":"code","16ad590c":"code","75498539":"code","53620c09":"code","0b4fe423":"markdown","824b8a17":"markdown","f11511b3":"markdown","d4643e2d":"markdown","aee3f788":"markdown","f1ae9c70":"markdown","56857e43":"markdown","0370f264":"markdown","2f6d472d":"markdown","9ed47478":"markdown","5fa763e2":"markdown","42a26eda":"markdown","32d4b4e2":"markdown","996afb6f":"markdown","d054d50f":"markdown","010c6299":"markdown","27528b6f":"markdown","dc8818f7":"markdown","01abc372":"markdown","63bbf91b":"markdown","6d236277":"markdown","68033525":"markdown","47e34149":"markdown","baa01de5":"markdown","25462cd0":"markdown","5d386db4":"markdown","1d602645":"markdown","6410fe1f":"markdown","c9f264c0":"markdown","d96d8a3e":"markdown","b36c0923":"markdown","401804b5":"markdown"},"source":{"249608c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c1e5d44":"filepath='..\/input\/sms-spam-collection-dataset\/spam.csv'\ndf=pd.read_csv(filepath, encoding='latin-1')\ndf.head()","68625831":"df=df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1)\ndf=df.rename(columns={'v1':'labels','v2': 'sms'})\ndf.head()","55ad34f0":"df['labels']=df.labels.map({'spam':0, 'ham':1})\ndf.head()","fed9fd6d":"df.shape","04b2c421":"df['length']=df['sms'].apply(len)\ndf.head()","a473fc38":"plt.figure(figsize=(16,6))\nsns.distplot(a=df['length'],kde=False)\nplt.legend()","e6557cf6":"message=df[df['length']==910]['sms'].iloc[0]\nmessage","a3769a70":"message={\"\"\"\nFor me the love should start \n         with attraction.i should feel that \n         I need her every time around me.she should be the first thing which comes in my thoughts.\n         I would start the day and end it with her.she should be there every time I dream.love will be \n         then when my every breath has her name.my life should happen around her.my life will be named to her.\n         I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.\n         I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that \n         my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when \n         I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.\n         will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later.\n\"\"\"}\nlower_case=[]\nfor i in message:\n    lower_case=[i.lower() for i in message]\n    print(lower_case)","fef592e6":"sans_punctuation = []\nimport string\n\nfor i in lower_case:\n    sans_punctuation.append(i.translate(str.maketrans('', '', string.punctuation)))\nprint(sans_punctuation)","93cfab93":"preprocessed_documents = []\nfor i in sans_punctuation:\n     preprocessed_documents=[[w for w in i.split()] for i in message]\nprint(preprocessed_documents)","cb58c823":"import pprint\nfrom collections import Counter\nfrequency_num=[]\n\nfor i in preprocessed_documents:\n    frequency_count=Counter(i)\n    frequency_num.append(frequency_count)\npprint.pprint(frequency_num)","adbf0850":"count_vector=CountVectorizer()\nprint(count_vector)","1246e4f3":"count_vector.fit(message)\nvoc=count_vector.get_feature_names()\nvoc","297f9ce4":"doc_to_array=count_vector.transform(voc).toarray()\ndoc_to_array","af16d16f":"frequency_matrix = pd.DataFrame(doc_to_array, \n                                columns = count_vector.get_feature_names())\nfrequency_matrix","9800fa18":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","99b6906a":"X_train, X_test, y_train, y_test = train_test_split(df['sms'],df['labels'],random_state=1)\nprint('Number of rows in the total set: {}'.format(df.shape[0]))\nprint('Number of rows in the training set: {}'.format(X_train.shape[0]))\nprint('Number of rows in the test set: {}'.format(X_test.shape[0]))","1371048f":"training_data=count_vector.fit_transform(X_train)\ntesting_data=count_vector.transform(X_test)","9f5dabbc":"mnb=MultinomialNB()\nmnb.fit(training_data, y_train)\n\npredictions=mnb.predict(testing_data)\nmnb_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))\n\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","09fc1dda":"dtc=DecisionTreeClassifier()\ndtc.fit(training_data,y_train)\n\npredictions=dtc.predict(testing_data)\ndtc_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test, predictions)))\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","8009b792":"rfc=RandomForestClassifier()\nrfc.fit(training_data,y_train)\n\npredictions=rfc.predict(testing_data)\nrfc_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test,predictions)))\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","f6839ec3":"knn=KNeighborsClassifier()\nknn.fit(training_data, y_train)\n\npredictions=knn.predict(testing_data)\nknn_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test,predictions)))\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","de44bec3":"bgc=BaggingClassifier()\nbgc.fit(training_data, y_train)\n\npredictions=bgc.predict(testing_data)\nbgc_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test,predictions)))\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","76f4a7dc":"#AdaBoost\nadb=AdaBoostClassifier()\nadb.fit(training_data, y_train)\npredictions=adb.predict(testing_data)\nadb_accuracy = accuracy_score(y_test,predictions)\nprint('Accuracy score: ', format(accuracy_score(y_test,predictions)))\nprint('precision score: ', format(precision_score(y_test,predictions)))\nprint('recall score: ', format(recall_score(y_test,predictions)))\nprint('f1 score: ', format(f1_score(y_test,predictions)))","16ad590c":"clf=(mnb_accuracy,dtc_accuracy,rfc_accuracy,knn_accuracy,bgc_accuracy,adb_accuracy)\nplt.figure(figsize=(16,6))\nsns.distplot(a=clf, hist=True)\nplt.xlabel('Accuracy scores')\nplt.title('Accuracy comparison')\nplt.legend()","75498539":"sns.barplot(data=clf)\nplt.title('Accuracy estimates')\nplt.legend()","53620c09":"cm=true_positive, false_negative, false_positive, true_negative = confusion_matrix(y_test,predictions).ravel()\nprint('True positive : ',true_positive)\nprint('False negative : ',false_negative)\nprint('False positive : ',false_positive)\nprint('True negative : ',true_negative)\n\naccuracy = (true_positive + true_negative)\/(true_positive + false_negative + false_positive + true_negative)\nprint('General accuracy : ',accuracy)","0b4fe423":"So our model has a very good predictions on messages that are spam or ham and using certain algorithms like AdaBoost can enhance the accuracy of the model.","824b8a17":"**Tokenization**","f11511b3":"KNN","d4643e2d":"Now we will implement Bag of Words which will count the number of words based on their frequency distribution and that binary number will be fed for Machine Learning model\n","aee3f788":"Let's try the above with CountVectorizer tool ","f1ae9c70":"Now, using count_vector i have converted the words to vocabulary as well","56857e43":"\nwe convert the message words to array form","0370f264":"Now we will use punctutation for sorting out the sentences","2f6d472d":"Bar plot for all model accuracies","9ed47478":"# Training and Testing the model","5fa763e2":"# Spam Classification\n","42a26eda":"We start with using lowercase for all the words in the above sentence","32d4b4e2":"Bagging Classifer and AdaBoost","996afb6f":"Let's Get started, I have used datasets from\nUCI Spam dataset : https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset","d054d50f":"# Data Visualization (Part-1)","010c6299":"# Assigning Binary Values","27528b6f":"Table created ","dc8818f7":"Now, here we start with spam classification so we will allotting binary values to labels so that Machine Learning model can work efficiently in predicting the results","01abc372":"Using Decision Trees","63bbf91b":"**Accuracy Plots estimations **","6d236277":"**Confusion Matrix**","68033525":"Any suggestions feel free to comment","47e34149":"RandomForest Classifier","baa01de5":"# Steps taken\n* Load the libraries\n* Data Cleaning\n* Assigning Binary Values to Labels\n* Data Visualization (Part-1)\n* LowerCasing, Punctuation removing and Vocabulary modifications\n* Counting The Occurence of Words\n* Training, Testing Part of the model\n* Data Visualization (Part-2)","25462cd0":"# Data Cleaning\nWe, start with dropping columns with missing values","5d386db4":"# Data Visualization (Part-2)","1d602645":"Now we begin with counting the numbers as how much is their frequency","6410fe1f":"We fix our response values for spam and ham","c9f264c0":"# Counting The Occurence of Words","d96d8a3e":"Hope you enjoyed this!!","b36c0923":"# Loading the libraries","401804b5":"# LowerCasing, Punctuations and Vocab. modifications"}}