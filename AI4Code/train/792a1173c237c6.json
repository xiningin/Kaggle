{"cell_type":{"e65f4c28":"code","f8f7dc9d":"code","3a13df1b":"code","533956cc":"code","db5dddcc":"code","da662e7c":"code","929db7de":"code","c262f18c":"code","f92c24cc":"code","6dc933b7":"code","c8b412e9":"code","2d25376c":"code","eade1911":"code","31f7f41a":"code","4c704850":"code","a3ed0d12":"code","4e947cf8":"code","8251d63a":"code","261df3b9":"code","c1d10cbe":"code","a0138ca8":"code","3b346079":"code","9ffe7c21":"code","622ed13d":"code","33f0d4a9":"code","f27f28ba":"code","d010a1a6":"code","94b23bce":"code","099ba4e4":"code","53f4bb3d":"code","11979480":"code","c1a25cd0":"code","3a77bfd7":"code","2751149a":"code","8cec765e":"code","9722eecf":"code","cdf64ec0":"code","41bc3281":"code","6ac2c92f":"code","b52a425a":"code","9a027616":"code","2573d5cb":"code","76274d7e":"code","02c1f0aa":"code","7e161cf1":"code","fc27dc1f":"code","6e48735b":"code","ca4dea3b":"code","41c31596":"code","a51135ae":"code","f7098310":"code","282f2818":"code","aa421f19":"markdown","9df8121f":"markdown","909ea98e":"markdown","87184557":"markdown","ae50b45f":"markdown","491b5099":"markdown","21437c0b":"markdown","492ca446":"markdown"},"source":{"e65f4c28":"import cv2\nfrom scipy.signal import correlate2d, convolve2d\n\nimport numpy as np\n\nfrom IPython.display import clear_output\nimport PIL\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch.optim as optim\n\nfrom IPython.display import clear_output\n\nfrom collections import defaultdict\n\nimport seaborn as sn\nimport pandas as pd\n#from torchsummary import summary","f8f7dc9d":"def view_classify(img, ps):\n    ps = ps.cpu().data.numpy().squeeze()\n\n    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze(), cmap='gray')\n    ax1.axis('off')\n    ax2.barh(np.arange(10), ps)\n    ax2.set_aspect(0.1)\n    ax2.set_yticks(np.arange(10))\n    ax2.set_yticklabels(np.arange(10))\n    ax2.set_title('class probability')\n    ax2.set_xlabel('probability')\n    ax2.set_ylabel('class')\n    ax2.set_xlim(0, 1.1)\n\n    plt.tight_layout()","3a13df1b":"def show_features(features):\n    if len(features.shape) < 4:\n        batch, num_feature = features.shape[:2]\n        for i, feature in enumerate(features):\n            plt.subplot(1, num_feature, i+1)\n            plt.imshow(feature.numpy().transpose(1,2,0))\n    else:\n        batch, num_feature = features.shape[:2]\n        for i, element in enumerate(features):\n            for j, feature in enumerate(element):\n                plt.subplot(batch, num_feature, i * num_feature + j + 1)\n                plt.imshow(feature.numpy())","533956cc":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","db5dddcc":"train=pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv')","da662e7c":"# tt_data=pd.concat([train, test], ignore_index=True)\n# tt_data=tt_data.sample(frac=1)\n# test=tt_data.sample(frac=0.1)\n# train=tt_data.drop(test.index)\n# test=test.reset_index(drop=True)\n# train=train.reset_index(drop=True)","929db7de":"print('train',train.shape)\nprint('test',test.shape)","c262f18c":"train_data=train.drop('label',axis=1)\ntrain_targets=train['label']\n\ntest_data=test.drop('label',axis=1)\ntest_targets=test['label']","f92c24cc":"train_data=torch.from_numpy(train_data.values).float().view(train_data.shape[0],28,28)\ntrain_targets=torch.from_numpy(train_targets.values).long().view(train_data.shape[0])\n\ntest_data=torch.from_numpy(test_data.values).float().view(test_data.shape[0],28,28)\ntest_targets=torch.from_numpy(test_targets.values).long().view(test_data.shape[0])","6dc933b7":"num, height, width = np.array(train_data).shape\nimg_min = np.array(train_data).min()\nimg_max = np.array(train_data).max()\nimg_norm_mean = np.array(train_data, dtype=float).mean() \/ img_max\nimg_std = np.sqrt(np.sum((np.array(train_data) \/ img_max  - img_norm_mean) ** 2) \/ (num * height * width))\nprint(img_min, img_max, img_norm_mean, img_std)","c8b412e9":"print('train set shape: ', train_data.shape)\nprint('train labels shape: ', train_targets.shape)","2d25376c":"print('test set shape: ', test_data.shape)\nprint('test labels shape: ', test_targets.shape)","eade1911":"plt.title('train dataset distribution')\nplt.hist(train_targets);","31f7f41a":"plt.title('test dataset distribution')\nplt.hist(test_targets);","4c704850":"class KannadaDataSet(torch.utils.data.Dataset):\n    def __init__(self, images, labels, transforms = None):\n        self.images = images\n        self.labels = labels\n        self.transforms = transforms\n         \n    def __len__(self):\n        return self.images.shape[0]\n    \n    def __getitem__(self, i):\n        data = np.array(self.images.iloc[i,:]).astype(np.uint8).reshape(28,28,1)\n        \n        if self.transforms:\n            data = self.transforms(data)\n            \n        return (data, self.labels[i])","a3ed0d12":"train_orig_T = transforms.Compose(([\n    transforms.ToTensor(),\n#     transforms.Normalize((img_norm_mean,), (img_std,))\n]))\ntrain_aug_T = transforms.Compose(([\n    transforms.ToPILImage(),\n    transforms.RandomCrop(28),\n    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n#     transforms.Normalize((img_norm_mean,), (img_std,))\n]))","4e947cf8":"test_T = transforms.Compose(([\n    transforms.ToTensor(),\n#     transforms.Normalize((img_norm_mean,), (img_std,))\n]))","8251d63a":"orig_train_set=KannadaDataSet(train.drop('label',axis=1), train['label'],train_orig_T)\naug_train_set=KannadaDataSet(train.drop('label',axis=1), train['label'],train_aug_T)\ntrain_set = torch.utils.data.ConcatDataset([orig_train_set,aug_train_set])\ntest_set=KannadaDataSet(test.drop('label',axis=1), test['label'], test_T) ","261df3b9":"train_loader = torch.utils.data.DataLoader(train_set, \n                                           batch_size=128, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(test_set,\n                                          batch_size=128, \n                                          shuffle=False)\n\nclasses = ('0', '1', '2', '3', '4', '5', '6', '7', '8', '9')","c1d10cbe":"images, labels = next(iter(train_loader))\nimages, labels = images[:8], labels[:8]\nplt.imshow(torchvision.utils.make_grid(images)[0,:,:], cmap='gray')\nprint(labels)","a0138ca8":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, \n                               out_channels=12, \n                               kernel_size=5, \n                               padding=0)\n        self.bn1 = nn.BatchNorm2d(12)\n        self.conv2 = nn.Conv2d(in_channels=12, \n                               out_channels=32, \n                               kernel_size=3, \n                               padding=0)\n        self.bn2 = nn.BatchNorm2d(32)\n        self.fc1 = nn.Linear(32 * 5 * 5, 240)\n        self.fc2 = nn.Linear(240, 168)\n        self.fc3 = nn.Linear(168, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = F.relu(x)\n        #x = F.max_pool2d(x,2)\n        x = F.avg_pool2d(x,2)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = F.relu(x)\n        #x = F.max_pool2d(x,2)\n        x = F.avg_pool2d(x,2)\n        x = x.view(-1, 32 * 5 * 5)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        x = F.log_softmax(x, dim=1)\n        return x\n    \n    def features_2(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n#         x = F.max_pool2d(F.relu(self.conv1(x)),2)\n#         x = F.max_pool2d(F.relu(self.conv2(x)),2)\n        return x\n    \n    def features_1(self, x):\n        x = F.relu(self.conv1(x))\n#         x = F.max_pool2d(F.relu(self.conv1(x)),2)\n        return x","3b346079":"net = Net().to(device)\n# summary(net, (1, 28, 28))\nnet","9ffe7c21":"plt.figure(figsize=(15,10))\nshow_features(net.features_1(images.to(device)).detach().cpu())","622ed13d":"plt.figure(figsize=(15,10))\nshow_features(net.features_2(images.to(device)).detach().cpu())","33f0d4a9":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9)\n\nepoch=0\n\ncurve_x=[]\nloss_curve_y=[]\nv_loss_curve_y=[]\nacc_curve_y=[]","f27f28ba":"num_epochs=5\n\ne=0\nwhile e < num_epochs:\n    epoch+=1\n    e+=1\n    \n#     if e == 3:\n#         optimizer.param_groups[0]['lr'] = 1e-5\n\n#     if e == 5:\n#         optimizer.param_groups[0]['lr'] = 1e-8\n        \n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        \n        if i != 0 and i % 100 == 0:    \n            validation_loss = 0.\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                net.eval()\n\n                for v_data in test_loader:\n                    v_inputs, v_labels = v_data[0].to(device), v_data[1].to(device)\n\n                    v_outputs = net(v_inputs)\n                    v_loss = criterion(v_outputs, v_labels)\n\n                    validation_loss+=v_loss.item()\n\n                    _, predicted = torch.max(v_outputs.data, 1)\n                    total += v_labels.size(0)\n                    correct += (predicted == v_labels).sum().item()\n            net.train()\n        #     clear_output()              \n            print('epoch %d, step %5d training loss: %.3f validation loss: %.3f test acc: %.3f' %\n              (epoch, i, running_loss \/ 1000, validation_loss\/1000, 100 * correct \/ total))\n            \n            curve_x.append(len(curve_x))\n            loss_curve_y.append(running_loss\/1000)\n            v_loss_curve_y.append(validation_loss\/1000)\n            acc_curve_y.append(100 * correct \/ total)\n            \n            running_loss = 0.0\n\nprint('finish')","d010a1a6":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\naxes[0].plot(curve_x,v_loss_curve_y, label='validation')\naxes[0].plot(curve_x,loss_curve_y, '--',label='train')\naxes[0].set_title('validation loss')\naxes[0].set_xlabel(\"iterations\")\naxes[0].set_ylabel(\"loss\")\naxes[0].set_xticks([])\naxes[0].legend()\naxes[1].plot(curve_x,acc_curve_y)\naxes[1].set_title('test acc')\naxes[1].set_xlabel(\"iterations\")\naxes[1].set_ylabel(\"acc\")\naxes[1].set_xticks([])\nfig.tight_layout()\nplt.show()","94b23bce":"plt.figure(figsize=(15,10))\nshow_features(net.features_1(images.to(device)).detach().cpu())","099ba4e4":"plt.figure(figsize=(15,10))\nshow_features(net.features_2(images.to(device)).detach().cpu())","53f4bb3d":"images, labels = next(iter(test_loader))\nimages, labels = images.to(device), labels.to(device)\nimages, labels = images[:8], labels[:8]\noutputs = net(images)\n_, predicted = torch.max(outputs, 1)\n\nplt.imshow(torchvision.utils.make_grid(images.cpu())[0,:,:], cmap='gray')\nprint('gt:', labels)\nprint('predict:', predicted)","11979480":"correct = 0\ntotal = 0\n\nerrors_imgs=[]\nerrors_labels=[]\nconfusion_matrix = torch.zeros(10, 10)\nerr_confusion_matrix = torch.zeros(10, 10)\ncorrect_samples={i:None for i in range(10)}\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data[0].to(device), data[1].to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        errors_imgs.extend(images[predicted != labels])\n        errors_labels.extend(zip(predicted[predicted != labels], labels[predicted != labels]))\n        \n        for i in [i for i in correct_samples if type(correct_samples[i]) != torch.Tensor]:\n            ci = images[(predicted == labels) & (i == labels)]\n            if ci.shape[0] > 0:\n                correct_samples[i] = ci[0]\n        for t, p in zip(labels.view(-1), predicted.view(-1)):\n            confusion_matrix[t.long(), p.long()] += 1\n            if t.long() != p.long():\n                err_confusion_matrix[t.long(), p.long()] += 1\nerrors_imgs=torch.stack(errors_imgs)\nerrors_labels=np.array([(p.item(), t.item())for p, t in errors_labels])\nprint('accuracy of the network on the test images: %d %%' % (100 * correct \/ total))\n#torch.save(net, 'torch_kannada_mnist_model.pt')  ","c1a25cd0":"print('errors: ',len(errors_labels))","3a77bfd7":"df_cm = pd.DataFrame(confusion_matrix.numpy(), range(10), range(10))\n\nplt.figure(figsize = (20,20))\nsn.set(font_scale=2)\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 14}, fmt='g')","2751149a":"df_cm = pd.DataFrame(err_confusion_matrix.numpy(), range(10), range(10))\n\nplt.figure(figsize = (20,20))\nsn.set(font_scale=2)\nsn.heatmap(df_cm, annot=True,annot_kws={\"size\": 14}, fmt='g')","8cec765e":"sn.set(font_scale=1)\nsn.set_style(\"whitegrid\", {'axes.grid' : False})","9722eecf":"f=lambda i, a: ({k: len(v) for k, v in a.items()} if [a[x].append(i) for x in i] else {})","cdf64ec0":"errors=f(errors_labels[:,1],defaultdict(list))","41bc3281":"plt.figure(figsize=(6,4))\nplt.barh(*zip(*errors.items()))\nplt.yticks(range(10))\nplt.xlabel('erorrs')\nplt.ylabel('labels')\nplt.show()","6ac2c92f":"plt.figure(figsize=(15,10))\nshow_features(net.features_1(errors_imgs[:8]).detach().cpu())","b52a425a":"plt.figure(figsize=(15,10))\nshow_features(net.features_2(errors_imgs[:8]).detach().cpu())","9a027616":"for img_indx in range(10):\n    pred_digit=errors_labels[img_indx][0].item()\n    true_digit=errors_labels[img_indx][1].item()\n    print('PREDICTED: ', pred_digit, '!=','TRUE: ', true_digit)\n    ps = net.forward(errors_imgs[img_indx].unsqueeze(0))\n    view_classify(errors_imgs[img_indx].cpu(), torch.softmax(ps,dim=1))\n    plt.show()\n    \n    print(f'FEATURES_1 OF FALSE {true_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_1(errors_imgs[img_indx:img_indx+1].to(device)).detach().cpu())\n    plt.show()\n\n    print(f'FEATURES_1 OF TRUE {true_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_1(correct_samples[true_digit].unsqueeze(0).to(device)).detach().cpu())\n    plt.show()\n    \n    print(f'FEATURES_1 OF TRUE {pred_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_1(correct_samples[pred_digit].unsqueeze(0).to(device)).detach().cpu())\n    plt.show()\n    \n    print(f'FEATURES_2 OF FALSE {true_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_2(errors_imgs[img_indx:img_indx+1].to(device)).detach().cpu())\n    plt.show()\n    \n    print(f'FEATURES_2 OF TRUE {true_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_2(correct_samples[true_digit].unsqueeze(0).to(device)).detach().cpu())\n    plt.show()\n    \n    print(f'FEATURES_2 OF TRUE {pred_digit}')\n    plt.figure(figsize=(15,10))\n    show_features(net.features_2(correct_samples[pred_digit].unsqueeze(0).to(device)).detach().cpu())\n    plt.show()","2573d5cb":"# net=torch.load('torch_kannada_mnist_model.pt')\n# net.eval()","76274d7e":"kaggle=pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/test.csv')","02c1f0aa":"kaggle.shape","7e161cf1":"kaggle=kaggle.drop('id', axis=1)","fc27dc1f":"kaggle=torch.from_numpy(kaggle.values)","6e48735b":"kaggle=kaggle.view(kaggle.shape[0],28,28).to(device)","ca4dea3b":"# T = transforms.Compose([\n#     transforms.Normalize((img_norm_mean,), (img_std,))\n# ])\n# kaggle=T(kaggle)","41c31596":"predictions=[]\nwith torch.no_grad():\n    for i, data in enumerate(kaggle):\n        images = data.to(device)\n        outputs = net(images.float().unsqueeze(0).unsqueeze(0))\n        _, predicted = torch.max(outputs.data, 1)\n        predictions.append([i,predicted.item()])","a51135ae":"predictions=pd.DataFrame(predictions)","f7098310":"predictions.columns=['id','label']","282f2818":"predictions.to_csv('submission.csv', index=False)","aa421f19":"# errors examples","9df8121f":"# errors features","909ea98e":"# heatmap","87184557":"# errors heatmap","ae50b45f":"# errors distribution","491b5099":"# kaggle submission","21437c0b":"# LeNet - 5","492ca446":"# training curves"}}