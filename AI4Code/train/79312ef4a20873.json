{"cell_type":{"97d4b2b0":"code","f5b7d876":"code","0976080c":"code","f8752849":"code","e4b656db":"code","a6d1b0ce":"code","ba4e61e7":"code","40a7e5cf":"code","977d6d84":"code","db7dfe46":"code","601cb12b":"code","3cc66029":"code","5f2bfa15":"code","84e06743":"code","956ffcf6":"code","33a4b790":"code","cf7103c5":"code","383c55b5":"code","cdd16a0b":"code","e0a51f89":"code","3fbbdb9e":"code","b71e5d44":"code","62736679":"code","fdb4379f":"code","f35a8508":"code","adf4120e":"code","7b14f30d":"code","0a9c0fd7":"code","cc53af11":"code","19b63cad":"code","0f48d3c9":"code","fb60486f":"code","b0b95dd0":"code","6aea8906":"code","2772f840":"code","129d3d4a":"code","a0971140":"code","7de217b4":"code","232bf3d9":"markdown","0fdfdbb4":"markdown","89dcc39b":"markdown","949ab61a":"markdown","3bf04b3f":"markdown","6c348815":"markdown","d1c6cc93":"markdown","5413eaf8":"markdown","1d7bbe9a":"markdown","a3e37b9b":"markdown","dcbe2439":"markdown","9269a27f":"markdown","a4f7cb70":"markdown","cfb6bc6b":"markdown","e94529b0":"markdown","b6c65405":"markdown","e27e2f40":"markdown","a3136e5b":"markdown","8817eb4a":"markdown","7e498e3a":"markdown","ffacadd2":"markdown","733174c9":"markdown","b01a52dc":"markdown","4586b5da":"markdown","38426798":"markdown","16c354dd":"markdown","09a4d2f0":"markdown","23cba017":"markdown","4e05fd86":"markdown","cf9e86f9":"markdown","51a59c79":"markdown","36b44f46":"markdown","994f0aa4":"markdown","e43213cc":"markdown","e60e6688":"markdown","be0a060b":"markdown"},"source":{"97d4b2b0":"import pandas as pd, numpy as np, os, gc, matplotlib.pyplot as plt, seaborn as sb, re, warnings, calendar, sys\nfrom numpy import arange\nget_ipython().run_line_magic('matplotlib', 'inline')\nwarnings.filterwarnings('ignore'); np.set_printoptions(suppress=True); pd.options.mode.chained_assignment = None\npd.set_option('display.float_format', lambda x: '%.3f' % x); pd.options.display.max_rows = 15\nglobal directory; directory = '..\/input'\n\ndef files(): return os.listdir(directory)\n\ndef read_clean(data):\n    data.columns = [str(x.lower().strip().replace(' ','_')) for x in data.columns]\n    seen = {}; columns = []; i = 0\n    for i,x in enumerate(data.columns):\n        if x in seen: columns.append(x+'_{}'.format(i))\n        else: columns.append(x)\n        seen[x] = None\n        \n    for x in data.columns[data.count()\/len(data) < 0.0001]: del data[x];\n    gc.collect();\n    try: data = data.replace({'':np.nan,' ':np.nan});\n    except: pass;\n    \n    if len(data) < 10000: l = len(data);\n    else: l = 10000;\n    sample = data.sample(l);size = len(sample);\n    \n    for x in sample.columns:\n        ints = pd.to_numeric(sample[x], downcast = 'integer', errors = 'coerce')\n        if ints.count()\/size > 0.97:\n            minimum = ints.min()\n            if minimum > 0: data[x] = pd.to_numeric(data[x], downcast = 'unsigned', errors = 'coerce')\n            else: data[x] = pd.to_numeric(data[x], downcast = 'integer', errors = 'coerce')\n        else:\n            floats = pd.to_numeric(sample[x], downcast = 'float', errors = 'coerce')\n            if floats.count()\/size > 0.97: data[x] = pd.to_numeric(data[x], downcast = 'float', errors = 'coerce')\n            else:\n                dates = pd.to_datetime(sample[x], errors = 'coerce')\n                if dates.count()\/size > 0.97: data[x] = pd.to_datetime(data[x], errors = 'coerce')\n    return data.reset_index(drop = True)\n\ndef read(x):\n    '''Kaggle Reading in CSV files.\n    Just type read('file.csv'), and you'll get back a Table.'''\n    \n    file = '{}\/{}'.format(directory,x)\n    try:     data = pd.read_csv(file)\n    except:  data = pd.read_csv(file, encoding = 'latin-1')\n    return read_clean(data)\n\ndef tally(column, minimum = 0, top = None, graph = False, percent = False, multiple = False, lowercase = False, min_count = 1):\n    '''Provides a tally count of all values in a COLUMN.\n        1. minimum  =  (>0)          Least count of item to show.\n        2. top      =  (-1,>0)       Only show top N objects\n        3. graph    =  (False,True)  Show bar graph of results\n        4. percent  =  (False,>0)    Instead of showing counts, show percentages of total count\n        \n       multiple = False\/True.\n       If True, counts and tallies objects in list of lists (Count Vectorizer)\n       \n       lowercase = True \/ False.\n       If True, lowers all text firsrt. So A == a\n       \n       min_count >= 1\n       If a column sum for tag has less than min_count, discard whole column\n    '''\n    if multiple == False:\n        counts = column.value_counts().astype('uint')\n        counts = counts[counts >= minimum][:top]\n        counts = pd.DataFrame(counts).reset_index()\n        counts.columns = [column.name, 'tally']\n        if percent: \n            counts['tally'] \/= counts['tally'].sum()\/100\n            counts['tally'] = counts['tally']\n        if graph:\n            C = counts[::-1]\n            C.plot.barh(x = column.name, y = 'tally', legend = False); plt.show();\n        return counts\n    else:\n        from sklearn.feature_extraction.text import CountVectorizer\n        column = column.fillna('<NAN>')\n        if type(column.iloc[0]) != list: column = column.apply(lambda x: [x])\n        counter = CountVectorizer(lowercase = lowercase, tokenizer = lambda x: x, dtype = np.uint32, min_df = min_count)\n        counter.fit(column)\n        counts = pd.DataFrame(counter.transform(column).toarray())\n        counts.columns = [column.name+'_('+str(x)+')' for x in counter.get_feature_names()]\n        return counts\n    \n    \ndef describe(data):\n    '''Provides an overview of your data\n        1. dtype    =  Column type\n        2. missing% =  % of the column that is missing\n        3. nunique  =  Number of unique values in column\n        4. top3     =  Top 3 most occuring items\n        5. min      =  Minimum value. If not a number column, then empty\n        6. mean     =  Average value. If not a number column, then empty\n        7. median   =  Middle value. So sort all numbers, and get middle. If not a number column, then empty\n        8. max      =  Maximum value. If not a number column, then empty\n        9. sample   =  Random 2 elements\n        10. name    =  Column Name\n    '''\n    dtypes = dtype(data)\n    length = len(data)\n    missing = ((length - data.count())\/length*100)\n    \n    N = [];    most3 = []\n    for dt,col in zip(dtypes,data.columns):\n        if dt != 'datetime':\n            U = data[col].value_counts()\n            N.append(len(U))\n            if U.values[0] > 1: most3.append(U.index[:3].tolist())\n            else: most3.append([]);\n        else: N.append(0); most3.append([]);\n            \n    df = pd.concat([dtypes, missing], 1)\n    df.columns = ['dtype','missing%']\n    df['nunique'] = N; df['top3'] = most3\n    \n    numbers = list(data.columns[df['dtype'].isin(('uint','int','float'))])\n    df['min'] = data.min()\n    df['mean'] = data[numbers].mean()\n    df['median'] = data[numbers].median()\n    df['max'] = data.max()\n    df['sample'] = data.apply(lambda x : x.sample(2).values.tolist())\n    df['name'] = list(data.columns)\n    return df.sort_values(['missing%', 'nunique', 'dtype'], ascending = [False, False, True]).reset_index(drop = True)\n\n\ndef Checker(x):\n    if type(x) is pd.DataFrame: return 0\n    elif type(x) is pd.Series: return 1\n    else: return -1\n\ndef columns(data): return list(data.columns)\ndef rows(data): return list(data.index)\ndef index(data): return list(data.index)\ndef head(data, n = 10): return data.head(n)\ndef tail(data, n = 10): return data.tail(n)\ndef sample(data, n = 10): return data.sample(n)\n\ndef dtype(data):\n    what = Checker(data)\n    if what == 0:\n        dtypes = data.dtypes.astype('str')\n        dtypes = dtypes.str.split(r'\\d').str[0]\n    else:\n        dtypes = str(data.dtypes)\n        dtypes = re.split(r'\\d', dtypes)[0]\n    return dtypes\n\ndef mean(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].mean()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.mean()\n        else: return np.nan\n    else:\n        try:     return np.nanmean(data)\n        except:  return np.nan\n        \ndef std(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].std()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.std()\n        else: return np.nan\n    else:\n        try:     return np.nanstd(data)\n        except:  return np.nan\n        \ndef var(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].var()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.var()\n        else: return np.nan\n    else:\n        try:     return np.nanvar(data)\n        except:  return np.nan\n        \ndef log(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        x = np.log(data[numbers])\n        x[np.isinf(x)] = np.nan\n        return pd.Series(x)\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        else: return np.nan\n    else:\n        try:\n            x = np.log(data)\n            x[np.isinf(x)] = np.nan\n            return x\n        except:  return np.nan\n        \ndef median(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].median()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.median()\n        else: return np.nan\n    else:\n        try:     return np.nanmedian(data)\n        except:  return np.nan\n        \ndef minimum(data):\n    what = Checker(data)\n    if what == 0:      return data.min()\n    elif what == 1:    return data.min()\n    else:              return np.min(data)\n        \ndef maximum(data):\n    what = Checker(data)\n    if what == 0:      return data.max()\n    elif what == 1:    return data.max()\n    else:              return np.max(data)\n    \ndef missing(data):\n    what = Checker(data)\n    if what >= 0:      return pd.isnull(data)\n    else:              return np.isnan(data)\n    \ndef count(data):\n    what = Checker(data)\n    if what >= 0:      return data.count()\n    else:              return len(data)\n    \ndef nunique(data):\n    what = Checker(data)\n    if what >= 0:      return data.nunique()\n    else:              return len(np.unique(data))\n    \ndef unique(data):\n    if type(data) is pd.DataFrame:\n        uniques = []\n        for x in data.columns:\n            uniques.append(data[x].unique())\n        df = pd.Series(uniques)\n        df.index = data.columns\n        return df\n    elif type(data) is pd.Series: return data.unique()\n    else:              return np.unique(data)\n    \ndef total(data):\n    what = Checker(data)\n    _dt = ('uint','int','float')\n    if what == 0:\n        dtypes = dtype(data)\n        numbers = data.columns[dtypes.isin(_dt)]\n        return data[numbers].sum()\n    elif what == 1:\n        dtypes = dtype(data)\n        if dtypes in _dt: return data.sum()\n        else: return np.nan\n    else:\n        try:     return np.nansum(data)\n        except:  return np.nan\n        \ndef time_number(date): return hours(date)+minutes(date)\/60+seconds(date)\/60**2\ndef hours_minutes(date): return hours(date)+minutes(date)\/60\ndef hours(date): return date.dt.hour\ndef minutes(date): return date.dt.minute\ndef seconds(date): return date.dt.second\ndef month(date): return date.dt.month\ndef year(date): return date.dt.year\ndef day(date): return date.dt.day\ndef weekday(date): return date.dt.weekday\ndef leap_year(date): return year(date).apply(calendar.isleap)\ndef date_number(date): return year(date)+month(date)\/12+day(date)\/(365+leap_year(date)*1)\ndef year_month(date): return year(date)+month(date)\/12\n\ndef hcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 1)\n\ndef vcat(*columns):\n    cols = []\n    for c in columns:\n        if c is None: continue;\n        if type(c) in (list, tuple): \n            for i in c:\n                if type(i) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(i))\n                else: cols.append(i)\n        elif type(c) not in (pd.DataFrame, pd.Series): cols.append(pd.Series(c))\n        else: cols.append(c)\n    return pd.concat(cols, 0)\n\ndef melt(data, columns):\n    '''Converts a dataset into long form'''\n    return data.melt(id_vars = columns)\n    \ndef tabulate(*columns, method = 'count'):\n    '''Splits columns into chunks, and counts the occurences in each group.\n        Remember - tabulate works on the LAST column passed.\n        Options:\n            1. count            = Pure Count in group\n            2. count_percent    = Percentage of Count in group\n            3. mean             = Mean in group\n            4. median           = Median in group\n            5. max              = Max in group\n            6. min              = Min in group\n            7. sum_percent      = Percentage of Sum in group\n        Eg:\n            Apple | 1\n            ---------\n            Orange| 3\n            ---------\n            Apple | 2\n            ---------\n        Becomes:\n            Apple | 1 | 1\n            -------------\n                  | 2 | 1\n            -------------\n            Orange| 3 | 1\n        \n        NOTE --------\n            method can be a list of multiple options.\n    '''\n    if type(method) in (list, tuple):\n        xs = []\n        for x in method:\n            g = tabulate(*columns, method = x)\n            xs.append(g)\n        xs = hcat(xs)\n        xs = xs.T.drop_duplicates().T\n        return read_clean(xs)        \n    else:\n        def percent(series):\n            counts = series.count()\n            return counts.sum()\n\n        data = hcat(*columns)\n        columns = data.columns.tolist()\n\n        if method in ('count', 'count_percent'):\n            groups = data.groupby(data.columns.tolist()).apply(lambda x: x[data.columns[-1]].count())\n\n            if method == 'count_percent':\n                groups = groups.reset_index()\n                groups.columns = list(groups.columns[:-1])+['Group_Count']\n                right = data.groupby(columns[:-1]).count().reset_index()\n                right.columns = list(right.columns[:-1])+['Group_Sum']\n\n                groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n                groups['Percent%'] = groups['Group_Count']\/groups['Group_Sum']*100\n                groups = groups[columns+['Percent%']]\n                return groups\n\n        elif method == 'mean': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].mean())\n        elif method == 'median': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].median())\n        elif method == 'max': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].max())\n        elif method == 'min': groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].min())\n        elif method == 'sum_percent':\n            groups = data.groupby(data.columns.tolist()[:-1]).apply(lambda x: x[data.columns[-1]].sum()).reset_index()\n            groups.columns = list(groups.columns[:-1])+['Group_Count']\n            right = data.groupby(columns[:-1]).sum().reset_index()\n            right.columns = list(right.columns[:-1])+['Group_Sum']\n\n            groups = pd.merge(left = groups, right = right, left_on = columns[:-1], right_on = columns[:-1])\n            groups['Sum%'] = groups['Group_Count']\/groups['Group_Sum']*100\n            groups = groups[cols+['Sum%']]\n            return groups\n        else:\n            print('Method does not exist. Please choose count, count_percent, mean, median, max, min, sum_percent.'); return None;\n        #except: print('Method = {}'.format(method)+' cannot work on Object, Non-Numerical data. Choose count.'); return None;\n\n        groups = pd.DataFrame(groups)\n        groups.columns = [method]\n        groups.reset_index(inplace = True)\n        return groups\n\n\ndef sort(data, by = None, how = 'ascending', inplace = False):\n    ''' how can be 'ascending' or 'descending' or 'a' or 'd'\n    It can also be a list for each sorted column.\n    '''\n    replacer = {'ascending':True,'a':True,'descending':False,'d':False}\n    if by is None and type(data) is pd.Series:\n        try:    x = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n        return data.sort_values(ascending = x, inplace = inplace)\n    elif type(how) is not list:\n        try:    how = replacer[how]\n        except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    else:\n        for x in how: \n            try:    x = replacer[x]\n            except: print(\"how can be 'ascending' or 'descending' or 'a' or 'd'\"); return None;\n    return data.sort_values(by, ascending = how, inplace = inplace)\n\ndef keep(data, what, inplace = False):\n    '''Keeps data in a column if it's wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple,np.array,np.ndarray): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[~need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[~need] = np.nan\n        return df\n\ndef remove(data, what, inplace = False):\n    '''Deletes data in a column if it's not wanted.\n    Everything else is filled with NANs'''\n    if type(what) not in (list,tuple): what = [what]\n    need = data.isin(what)\n    if inplace: \n        df = data\n        df.loc[need] = np.nan\n    else: \n        df = data.copy()\n        df.loc[need] = np.nan\n        return df\n    \n    \ndef ternary(data, condition, true, false = np.nan, inplace = False):\n    '''C style ternary operator on column.\n    Condition executes on column, and if true, is filled with some value.\n    If false, then replaced with other value. Default false is NAN.'''\n    try:\n        execute = 'data {}'.format(condition)\n        series = eval(execute)\n        try: series = series.map({True:true, False:false})\n        except: series = series.replace({True:true, False:false})\n        return series\n    except: print('Ternary accepts conditions where strings must be enclosed.\\nSo == USD not allowed. == \"USD\" allowed.'); return False;\n\n    \ndef locate(data, column):\n    '''Use ternary to get result and then filter with notnull'''\n    if dtype(column) == 'bool': return data.loc[column]\n    return data.loc[column.notnull()]\n    \n    \ndef query(data, column = None, condition = None):\n    '''Querying data based on conditions'''\n    if type(column) is str:\n        cond = f'data[\"{column}\"]{condition}'\n    else:\n        cond = f'column{condition}'\n    return data.loc[eval(cond)]\n\n\ndef keep_top(x, n = 5):\n    '''Keeps top n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:n].values)\n    return df\n\ndef keep_bot(x, n = 5):\n    '''Keeps bottom n (after tallying) in a column'''\n    df = keep(x, tally(x)[x.name][:-n].values)\n    return df\n\n\ndef remove_outlier(x, method = 'iqr', range = 1.5):\n    '''Removes outliers in column with methods:\n        1. mean     =    meean+range (normally 3.5)\n        2. median   =    median+range (normally 3.5)\n        3. iqr      =    iqr+range (normally 1.5)\n    '''\n    i = x.copy()\n    if method == 'iqr':\n        first = np.nanpercentile(x, 0.25)\n        third = np.nanpercentile(x, 0.75)\n        iqr = third-first\n        i[(i > third+iqr*range) | (i < first-iqr*range)] = np.nan\n    else:\n        if method == 'mean': mu = np.nanmean(x)\n        else: mu = np.nanmedian(x)\n        std = np.nanstd(x)\n        i[(i > mu+std*range) | (i < mu-std*range)] = np.nan\n    return i\n\n\ndef cut(x, bins = 5, method = 'range'):\n    '''Cut continuous column into parts.\n        Method options:\n            1. range\n            2. quantile (number of quantile cuts)'''\n    if method == 'range': return pd.cut(x, bins = bins, duplicates = 'drop')\n    else: return pd.qcut(x, q = bins, duplicates = 'drop')\n    \n    \ndef plot(x, y = None, colour = None, column = None, data = None, size = 5, top = 10, wrap = 4, \n         subset = 5000, method = 'mean', quantile = True, bins = 10,\n         style = 'lineplot', logx = False, logy = False, logc = False, power = 1):\n    '''Plotting function using seaborn and matplotlib\n        Options:\n        x, y, colour, column, subset, style, method\n        \n        Plot styles:\n            1. boxplot\n            2. barplot\n            3. tallyplot (counting number of appearances)\n            4. violinplot (boxplot just fancier)\n            5. lineplot (mean line plot)\n            6. histogram\n            7. scatterplot (X, Y must be numeric --> dates will be converted)\n            8. bivariate (X, Y must be numeric --> dates will be converted)\n            9. heatmap (X, Y will be converted into categorical automatically --> bins)\n            10. regplot (X, Y must be numeric --> dates will be converted)\n    '''\n    if type(x) in (np.array,np.ndarray): x = pd.Series(x); x.name = 'x';\n    if type(y) in (np.array,np.ndarray): y = pd.Series(y); y.name = 'y';\n    if type(column) in (np.array,np.ndarray): column = pd.Series(column); column.name = 'column';\n    if type(colour) in (np.array,np.ndarray): colour = pd.Series(colour); colour.name = 'colour';\n        \n    if type(x) == pd.Series: \n        data = pd.DataFrame(x); x = x.name\n        if type(x) is not str:\n            data.columns = [str(x)]\n            x = str(x)\n    if method == 'mean': estimator = np.nanmean\n    elif method == 'median': estimator = np.nanmedian\n    elif method == 'min': estimator = np.min\n    elif method == 'max': estimator = np.max\n    else: print('Wrong method. Allowed = mean, median, min, max'); return False;\n    #----------------------------------------------------------\n    sb.set(rc={'figure.figsize':(size*1.75,size)})\n    dtypes = {'x':None,'y':None,'c':None,'col':None}\n    names = {'x':None,'y':None,'c':None,'col':None}\n    xlim = None\n    #----------------------------------------------------------\n    if data is not None:\n        if type(x) is str: x = data[x];\n        if type(y) is str: y = data[y]; \n        if type(colour) is str: colour = data[colour]; \n        if type(column) is str: column = data[column]; \n    if type(x) is str: print('Please specify data.'); return False;\n    #----------------------------------------------------------\n    if x is not None:\n        dtypes['x'] = dtype(x); names['x'] = x.name\n        if dtypes['x'] == 'object': x = keep_top(x, n = top)\n        elif dtypes['x'] == 'datetime': x = date_number(x)\n        if logx and dtype(x) != 'object': x = log(x)\n    if y is not None: \n        dtypes['y'] = dtype(y); names['y'] = y.name\n        if dtypes['y'] == 'object': y = keep_top(y, n = top)\n        elif dtypes['y'] == 'datetime': y = date_number(y)\n        if logy and dtype(y) != 'object': y = log(y)\n    if colour is not None:\n        dtypes['c'] = dtype(colour); names['c'] = colour.name\n        if dtypes['c'] == 'object': colour = keep_top(colour, n = top)\n        elif dtypes['c'] == 'datetime': colour = date_number(colour)\n        if logc and dtype(colour) != 'object': colour = log(colour)\n    if column is not None:\n        dtypes['col'] = dtype(column); names['col'] = column.name\n        if dtypes['col'] == 'object': column = keep_top(column, n = top)\n        elif dtypes['col'] == 'datetime': column = date_number(column)\n    #----------------------------------------------------------\n    df = hcat(x, y, colour, column)\n    if subset > len(df): subset = len(df)\n    df = sample(df, subset)\n    #----------------------------------------------------------\n    if column is not None:\n        if dtype(df[names['col']]) not in ('object', 'uint',' int') and nunique(df[names['col']]) > top: \n            if quantile: df[names['col']] = cut(df[names['col']], bins = bins, method = 'quantile')\n            else: df[names['col']] = cut(df[names['col']], bins = bins, method = 'range')\n    \n    try: df.sort_values(names['y'], inplace = True);\n    except: pass;\n    #----------------------------------------------------------\n    replace = {'boxplot':'box', 'barplot':'bar', 'tallyplot':'count', 'violinplot':'violin', \n               'lineplot': 'point', 'histogram':'lv'}\n    \n    if style == 'histogram' and y is None:\n        plot = sb.distplot(df[names['x']].loc[df[names['x']].notnull()], bins = bins)\n    elif style == 'lineplot' and y is None:\n        plot = plt.plot(df[names['x']]);\n        plt.show(); return;\n    elif style == 'barplot' and y is None:\n        plot = df.sort_values(names['x']).plot.bar();\n        plt.show(); return;\n    elif style in replace.keys():\n        if dtype(df[names['x']]) not in ('object', 'uint',' int') and nunique(df[names['x']]) > top: \n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n        \n        if names['col'] is not None:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator, col_wrap = wrap)\n        else:\n            plot = sb.factorplot(x = names['x'], y = names['y'], hue = names['c'], data = df, kind = replace[style], col = names['col'],\n                             n_boot = 1, size = size, estimator = estimator)\n            \n        for ax in plot.axes.flatten(): \n            for tick in ax.get_xticklabels(): \n                tick.set(rotation=90)\n    \n    elif style == 'heatmap':\n        if dtype(df[names['x']]) != 'object'and nunique(df[names['x']]) > top:\n            if quantile: df[names['x']] = cut(df[names['x']], bins = bins, method = 'quantile')\n            else: df[names['x']] = cut(df[names['x']], bins = bins, method = 'range')\n                \n        if dtype(df[names['y']]) != 'object'and nunique(df[names['y']]) > top:\n            if quantile: df[names['y']] = cut(df[names['y']], bins = bins, method = 'quantile')\n            else: df[names['y']] = cut(df[names['y']], bins = bins, method = 'range')     \n\n        df = tabulate(df[names['x']], df[names['y']]).pivot(index = names['x'], columns = names['y'], values = 'count')\n        plot = sb.heatmap(df, cmap=\"YlGnBu\")\n\n        \n    elif dtype(df[names['x']]) == 'object' or dtype(df[names['y']]) == 'object':\n            print('{} can only take X = number and Y = number.'.format(style)); return False;\n        \n    elif style  in ('regplot', 'scatterplot'):\n        if column is None: col_wrap = None\n        else: col_wrap = wrap\n        if style == 'regplot': reg = True\n        else: reg = False\n        \n        plot = sb.lmplot(x = names['x'], y = names['y'], hue = names['c'], data = df, col = names['col'],\n                             n_boot = 2, size = size, ci = None, scatter_kws={\"s\": 50,'alpha':0.5},\n                        col_wrap = col_wrap, truncate = True, fit_reg = reg, order = power)\n        plot.set_xticklabels(rotation=90)\n        \n    elif style == 'bivariate':\n        plot = sb.jointplot(x = names['x'], y = names['y'], data = df, dropna = True, size = size, kind = 'reg',\n                           scatter_kws={\"s\": 50,'alpha':0.5}, space = 0)\n    plt.show()\n    \ndef remove(x, what):\n    return replace(x, what, '')\n    \ndef notnull(data, loc = None):\n    '''Returns the items that are not null in a column \/ dataframe'''\n    if loc is not None:\n        return data.loc[loc.notnull()]\n    else:\n        return data.loc[data.notnull().sum(1) == data.shape[1]]\n    \n    \ndef exclude(data, col):\n    '''Only returns a dataframe where the columns in col are not included'''\n    if type(col) is str: col = [col]\n    columns = list(data.columns)\n    leave = list(set(columns) - set(col))\n    return data[leave]\n\n################### -----------------------------------------------------------------#######################\n#Recommendation Systems\ndef pivot(index, columns, values):\n    '''Creates a table where rows = users, columns = items, and cells = values \/ ratings'''\n    from scipy.sparse import dok_matrix\n    S = dok_matrix((nunique(index), nunique(columns)), dtype=np.float32)\n    \n    mins = np.abs(np.min(values))+1\n    indexM = {}\n    for i,x in enumerate(unique(index)): indexM[x] = i;\n    columnsM = {}\n    for i,x in enumerate(unique(columns)): columnsM[x] = i;\n        \n    for i,c,v in zip(index, columns, values+mins): S[indexM[i],columnsM[c]] = v;\n    \n    S = S.toarray(); S[S == 0] = np.nan; S -= mins\n    S = pd.DataFrame(S)\n    S.index = indexM.keys(); S.columns = columnsM.keys();\n    return S\n\ndef row_operation(data, method = 'sum'):\n    '''Apply a function to a row\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n    '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(1)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(1)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(1)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 1)\n    x.name = 'row_operation'\n    return x\n\n\ndef col_operation(data, method = 'sum'):\n    '''Apply a function to a column\n        Allowed functions:\n            1. sum\n            2. median\n            3. mean\n            4. max\n            5. min\n            6. count\n            7. count_percent\n            8. sum_percent\n            9. mean_zero         (Mean but zeroes arent counted)\n            10. count_zero       (Count but zeroes arent counted)\n        Own functions are also allowed.\n        '''\n    if method in ('sum','median','mean','max','min','count'):\n        x = eval('data.{}(0)'.format(method))\n    elif method in ('count_percent', 'sum_percent'):\n        x = eval('data.{}(0)'.format(method.split('_')[0]))\n        x \/= x.sum()\n        x *= 100\n    elif method in ('mean_zero', 'count_zero'):\n        df = data.copy()\n        df[df == 0] = np.nan\n        x = eval('df.{}(0)'.format(method.split('_')[0]))\n    else: return data.apply(method, axis = 0)\n    x.name = 'col_operation'\n    return x\n\n    \ndef random(obj, n = 1, p = None):\n    if p is not None:\n        if type(p) is pd.Series: p = p.values\n        if p.sum() > 2: p \/= 100\n    return list(np.random.choice(obj, size = n, replace = False, p = p))\n\ndef row(data, n): return data.loc[n]\n\ndef distances(source, target):\n    '''Returns all distances between target and source (L2)'''\n    Y = np.tile(target.values, (source.shape[0],1))\n    nans = np.isnan(Y)\n    X = source.values; X[np.isnan(X)] = 0;\n    Y[nans] = 0;\n    diff = X - Y;\n    diff[nans] = 0;\n    d = np.linalg.norm(diff, axis = 1)\n    j = pd.Series(d)\n    j.index = source.index\n    return j","f5b7d876":"files()","0976080c":"ratings = read('ratings.csv')","f8752849":"head(ratings, 5)","e4b656db":"describe(ratings)","a6d1b0ce":"plot(data = ratings, x = 'rating', style = 'histogram')","ba4e61e7":"tally(ratings['user_id'])","40a7e5cf":"head( tally(ratings['user_id']) )","977d6d84":"user_ratings = tally(ratings['user_id'])\nuser_ratings = user_ratings['tally']","db7dfe46":"plot(x = user_ratings, style = 'histogram', data = user_ratings)","601cb12b":"by_user = tabulate(ratings['user_id'], ratings['rating'], method = 'mean')\nhead(by_user)","3cc66029":"plot(x = 'mean', data = by_user, style = 'histogram')","5f2bfa15":"by_book = tabulate(ratings['book_id'], ratings['rating'], method = 'mean')\nhead(by_book)","84e06743":"plot(x = 'mean', data = by_book, style = 'histogram')","956ffcf6":"users = tally(ratings['user_id'])\nhead(users)","33a4b790":"mappings = pivot(ratings['user_id'], ratings['book_id'], ratings['rating'])","cf7103c5":"head(mappings)","383c55b5":"mean_users = row_operation(mappings, 'mean')\nplot(mean_users, style = 'histogram')","cdd16a0b":"mean_books = col_operation(mappings, 'mean')\nplot(mean_books, style = 'histogram')","e0a51f89":"random(columns(mappings))","3fbbdb9e":"random(columns(mappings), 10)","b71e5d44":"means = col_operation(mappings, 'mean')\ncounts = col_operation(mappings, 'count')","62736679":"sorted_means = sort(means, how = 'descending')\nhead(sorted_means)","fdb4379f":"sorted_counts = sort(counts, how = 'descending')\nhead(sorted_counts)","f35a8508":"score = means*log(counts)\nscore_sort = sort(score, how = 'descending')\nhead(score_sort)","adf4120e":"print(index(score_sort)[0])\nprint(index(score_sort)[0:20])","7b14f30d":"user_read = row_operation(mappings, 'count')\nuser_read = sort(user_read, how = 'ascending')","0a9c0fd7":"head(user_read)","cc53af11":"over_40 = locate(mappings, user_read > 40)","19b63cad":"row(mappings, 563)","0f48d3c9":"user_563 = row(mappings, 563)\nds = distances(  over_40,  user_563 )","fb60486f":"ds = sort(ds, how = 'ascending')\nhead(ds)","b0b95dd0":"row(mappings, index(ds)[0])","6aea8906":"recommendations = row(mappings, index(ds)[0])\nscore = sort(recommendations, how = 'descending')\n\nhead(score)","2772f840":"index(score)[0]","129d3d4a":"#_______________CHANGE USER BELOW_________________________\nuser = 1000\n\n#Checking______________(DO NOT change below)______________\ntry: x = row(mappings, user);\nexcept: print('No user exists');\n    \nds = distances(over_40, x)\nds = sort(ds, how = 'ascending')\nrecommendations = row(mappings, index(ds)[0:10])\n\nmean_recommendations = col_operation(recommendations, 'mean')\ncount_recommendations = col_operation(recommendations, 'count')\nlog_count_recommendations = log(count_recommendations)\n\nscore = mean_recommendations*log_count_recommendations\nscore = sort(score, how = 'descending')\n\nprint(head(score, 10))\nprint('Recommendations are: {}'.format(index(score)[0:10]))","a0971140":"#Your code goes here.\n","7de217b4":"#Your code goes here.\n","232bf3d9":"<a id='Content'><\/a>\n<h1> 1. Reading Data <\/h1>","0fdfdbb4":"**3. Recommending \"closest user's\" books (manual - nearest neighbors)**\n\nNow, lets MANUALLY recommend books.\n\nWhat we do, is given a user A, find the CLOSEST user B out of all users U such that we can recommend all the books user B read (the best ones)\n\nSay we get user A, what we essentially do, is find 10 users that are similar, and average all their ratings.\n\nNow, let's find which user has the LEAST read books (we want to recommend books to them.)\n\nUse ROW_OPERATION and COUNT","89dcc39b":"Now, let's get the AVERAGE RATING per USER!\n\nLet's use TABULATE and METHOD = MEAN","949ab61a":"Let's see which users are the MOST and LEAST readers.\n\nWe want to recommend books to the LEAST readers (our goal)","3bf04b3f":"We can also figure out each user's average rating using ROW_OPERATION.\n\nThe HISTOGRAM should look exactly the same as the ones we did before.","6c348815":"<a id='Lab'><\/a>\n<h1> 5. Lab Questions <\/h1>\n\n\n<img src=\"https:\/\/previews.123rf.com\/images\/christianchan\/christianchan1503\/christianchan150300425\/37144675--now-it-s-your-turn-note-pinned-on-cork-.jpg\" style=\"width: 300px;\"\/>","d1c6cc93":"Let's see how many books each user reads in a HISTOGRAM.\n\nUse TALLY ","5413eaf8":"First, check the files in the database:","1d7bbe9a":"<a id='Plot'><\/a>\n<h1> 2. Plotting and Describing Data <\/h1>\n\nLet's draw a HISTOGRAM of ratings using PLOT","a3e37b9b":"So, the best book for user 563 is:","dcbe2439":"Now, we want to find the DISTANCE between all users U against user A.\n\nDistance is calculated using the EUCLIDEAN distance or L2 norm. This is the straight line distance between points in any N dimensions.\n\nThe shorter the straight line distance, the CLOSER the user.\n\nBelow, you can see the straight line EUCLIDEAN distance from A to B\n\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-e73d01f18d0b4a2f57ff2206a3863c10-c\" style=\"width: 300px;\"\/>\n\nRemember, let's get user 563 against over_40\n\nNow, the actual process is below: (WHITE means no rating).\n\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1TtaUhOCGKHngpntfmWylDq2m4zVuGrKX\" style=\"width: 500px;\"\/>","9269a27f":"Let's sort both means, counts by DESCENDING order using SORT, since we want the highest rating and most popular","a4f7cb70":"**1. Recommending random books**\n\nThis is the fastest and cheapest approach (though not accurate and useful)\n\nLiterally, when a user comes, check what they haven't read, and recommend a random book.\n\nWe firstly want all unique book ids. Use COLUMNS, and then RANDOM to choose a random book.\n\nEach student's random output will be different (since random choosing 1)","cfb6bc6b":"(2) Find which **TOP 100** books have the highest **average rating**. Then, recommend from those top 100 **ANY 30 RANDOM books**.","e94529b0":"Once you are done, and satisfied with your work, let the tutor mark you.\n\nNote, if you can't get it, it's fine. Marks are awarded for trying the questions out. We don't mind if the output is wrong.\n\n<h2>ASSIGNMENT 1 (week 3 to 5) 7 marks<\/h2>\n\nThis night (Friday Week 3), you will receive a Kaggle link to **Assignment 1**. It's worth 7 marks, and super easy :).\n\nThe stuff you learnt in week 1, 2 and today will be enough to complete Assignment 1 entirely.\n\nIt's due in 2 weeks time on Saturday week 5 10pm. We need you to submit a URL \/ LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.\n\n<h2>TABLEAU<\/h2>\n\nNext week, we will be using **TABLEAU**. Please download it. If you have trouble, please ask us.","b6c65405":"<a id='Filter'><\/a>\n<h1> 4. COLLOBORATIVE FILTERING <\/h1>\n\nLet's do some recommendations!\n\nWe will be doing 6 major ways of recommendation:\n\n1. Recommending random books\n\n2. Recommending most viewed & most popular --> weighted\n\n3. Recommending \"closest user's\" books (manual nearest neighbors)\n\n4. Recommending \"closest user's\" books (nearest neighbors)","e27e2f40":"<a id='Pivot'><\/a>\n<h1> 3. Pivot Tables in Recommendation Algorithms <\/h1>\n\nSo we have 53,000 users and 10,000 books.\n\nWe want to build a large table where rows = each user, columns = each book, and we want this ratings per user table.","a3136e5b":"Let's see a description of the data using DESCRIBE","8817eb4a":"<h2> Welcome to MARK5826 Week 3!<\/h2>\n\nLecturer In Charge: Junbum Kwon;Teaching Assistant: Daniel Han-Chen & James Lin\n\nIn week 2, we focused on data analysis.\n\n<h2>AIM<\/h2>\n\nThis week (week 3), we are starting on Recommendation Algorithms!\n\nA Recommendation Algorithm is a system which gives an educated guess to what a user \/ customer might like to read \/ buy \/ see.\n\nNetflix, Youtube, Amazon all use these technologies, and they are very important in the Marketing World.\n\nGiven past data about book ratings and user likes, estimate in the future what users would like to read.\n\nThe methods we are using are called **COLLOBORATIVE FILTERING**.\n\n**CONTENT FILTERING** is content search based.\n\n<h2>TABLEAU<\/h2>\n\nNext week, we will be using **TABLEAU**. Please download it. If you have trouble, please ask us.\n\n<h2>ASSIGNMENT 1 (week 3 to 5) 7 marks<\/h2>\n\nThis night (Friday Week 3), you will receive a Kaggle link to **Assignment 1**. It's worth 7 marks, and super easy :).\n\nThe stuff you learnt in week 1, 2 and today will be enough to complete Assignment 1 entirely.\n\nIt's due in 2 weeks time on Saturday week 5 10pm. We need you to submit a URL \/ LINK to your assignment notebook on Kaggle.\n\nIT MUST BE **PUBLIC**. Also, download the Kaggle file, and upload it to Moodle by 10pm Saturday week 5.\n\n<h2> Suggestions from previous weeks <\/h2>\n\n1. **|Slides|** We will upload the slides through email \/ moodle before the lectures!\n2. **|Applications|** Week 1 was intro. Week 2+ will be all industry applications.\n3. **|Lab Delivery|**  Speaking will be slower now (and content has been cut back a lot. We will explain more --> more quality over quantity).\n4. **|Too fast|** We will go slowly on the hard parts of the class. In fact, we will go through how to do Lab Questions as a class.\n5. **|Heavy Content|** Sorry about the overwhelming content! I'm sure from week 2 onwards, the code you'll see should look familiar.\n6. **|Slow Computers|** Some people's computers are slow. We have decided to optimise the code below (removing superfluous code)\n7. **|Heavy Content|** Lab Organisation has been improved, with one streamlined marking system.\n\n\n<h2>Week Topics<\/h2>\n\n(You can click the links below)\n\n[SKIP BELOW CODE TO CONTENT](#Content)\n<hr>\n1.[Reading Data](#Content)\n\n2.[Plotting and Describing Data](#Plot)\n\n3.[Pivot Tables in Recommendation Algorithms](#Pivot)\n\n4.[COLLOBORATIVE FILTERING](#Filter)\n\n5.[Lab Questions](#Lab)\n","7e498e3a":"Given the distances, we want to SORT ASCENDING and get maybe the top 1 person with the LEAST distances.","ffacadd2":"How about PER BOOK?","733174c9":"Let's save the # of books into a variable, a plot a HISTOGRAM","b01a52dc":"Likewise, let's use a HISTOGRAM to see the per user averages","4586b5da":"(1) Recommend or get **TOP 30 BEST AVERAGE RATED** books to users.","38426798":"Likewise, we can also figure out each book's average rating using COL_OPERATION.\n\nThe HISTOGRAM should look exactly the same as the ones we did before.","16c354dd":"Let's get user 563, and the targets are users who have >40 read books.\n\nFirst, we need to LOCATE all users > 40 books.","09a4d2f0":"Clearly, the approximate median \/ average rating is around 4-5.","23cba017":"Anyways, to make a table rows = each user id, column = each book id, and each cell = rating, use PIVOT\n\nIt'll take some time, so be patient.","4e05fd86":"[<h1>CLICK to SKIP BELOW CODE TO CONTENT<\/h1>](#Content)","cf9e86f9":"Now, the issue now is sometimes 1 person rated 1 book only with rating = 5. It'll be high on the list.\n\nSo, what happens if this happens? If 1 person rates it 5, should be recommend it to new users?\n\nDefinitely not! Popularity also matters!\n\nSo, we need to combine POPULARITY AND RATINGS into 1 measure.\n\nWe will use the rating * LOG(popularity).\n\nWhy LOG? It's because the more popular, the better, BUT too large numbers means popularity takes over.\n\n<img src=\"https:\/\/drive.google.com\/uc?id=1VE4XOpCi5Qf3uLMzeWVYOPUVyQYhXCMS\" style=\"width: 300px;\"\/>","51a59c79":"Ratings.csv is the list of all ratings of a user \/ book","36b44f46":"Now, select a random user. I'll select user 1000.\n\n(You can change the user number below. HOWEVER - sometimes a user DOESNT exist, so be careful)\n<img src=\"https:\/\/static1.squarespace.com\/static\/5087cbb1e4b0f16d02a0d011\/t\/53a435e3e4b07e5a2fce9baf\/1403270627196\/\" style=\"width: 300px;\"\/>","994f0aa4":"We can also choose say 10 random books","e43213cc":"Index means to get actual book.\n\nTo get say top 20 books, use 0:20 to index 0 to 20","e60e6688":"Now, get the top 1 person  in mappings using INDEX and ROW","be0a060b":"**2. Recommending most viewed & most popular --> weighted**\n\nNext, we want to recommend books which are the most popular.\n\nSo, we want to take the MEAN \/ AVERAGE of all the COLUMNS (Books).\n\nLet's do that using COL_OPERATION and MEAN.\n\nNote - we also want to COUNT how many ratings each book got - (so get popularity score --> the more people read it - the more popular)"}}