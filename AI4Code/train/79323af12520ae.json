{"cell_type":{"510b5d45":"code","86096b32":"code","9c204427":"code","e3a46812":"code","c5d98b4b":"code","1fbdcd32":"code","72c8fc59":"code","dae2d9dd":"code","889d45ee":"code","aa973863":"code","574561d2":"code","99a34576":"code","955b6e2e":"code","8ea2eca3":"code","5bc94fdb":"code","25aa510a":"code","7db915d9":"code","dec2e8db":"code","d51ba737":"code","c187f053":"code","d38e5492":"code","dd07cae4":"code","e674ee60":"code","55a49517":"code","6b73ab77":"code","0b8a27bd":"code","b71726dd":"code","263bdcb6":"code","d792567f":"code","3002ecc9":"code","3c605d80":"code","e67a758f":"code","369f5d99":"code","d2059e99":"code","96dee2cd":"code","614e5e5b":"code","9e6ececa":"code","2524a41a":"code","f0650446":"markdown","f22029c5":"markdown","54cf2ce7":"markdown","84903ab8":"markdown","e3ea84cd":"markdown","787248d7":"markdown","e73aa5e0":"markdown","07e45d9b":"markdown","42a00327":"markdown","0ed3badf":"markdown","f6d1d4b6":"markdown","d2d5a352":"markdown","66db33d4":"markdown","b688e7bb":"markdown","7b67002a":"markdown","9393a910":"markdown","ae1a807d":"markdown","ca5239bf":"markdown","1aba815f":"markdown"},"source":{"510b5d45":"!pip install bioinfokit","86096b32":"!pip install seaborn --upgrade","9c204427":"# basic packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import sample\nimport gc\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\n# for PCA\nfrom bioinfokit.visuz import cluster\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport janestreet","e3a46812":"sns.__version__","c5d98b4b":"train_df = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\nfeatures_df = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\nexample_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')\n\nprint('Train dataset shape: {}'.format(train_df.shape))\nprint('Features dataset shape: {}'.format(features_df.shape))\nprint('Example test dataset shape: {}'.format(example_test.shape))","1fbdcd32":"print('Head of the train data:')\ntrain_df.head()","72c8fc59":"print('Train dataset dtypes: \\n{}'.format(train_df.dtypes.value_counts()))\nprint('-'*20)\nprint('Features dataset dtypes: \\n{}'.format(features_df.dtypes.value_counts()))\nprint('-'*20)\nprint('Example test dtypes: \\n{}'.format(example_test.dtypes.value_counts()))","dae2d9dd":"print('Columns with NaN (Train): %d' %train_df.isna().any().sum())\nprint('Columns with NaN (Features): %d' %features_df.isna().any().sum())\nprint('Columns with NaN (Example test): %d' %example_test.isna().any().sum())","889d45ee":"NaN_train = pd.Series(train_df.isna().sum()[train_df.isna().sum() > 0].\n                      sort_values(ascending = False) \/ len(train_df) * 100)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 10))\n\nsns.barplot(y = NaN_train.index[:30], x = NaN_train[:30], \n            edgecolor = 'black', alpha = 0.8,\n            palette = sns.color_palette(\"viridis\", len(NaN_train[:30])))\nplt.title('NaN values of train dataset (30 columns)', size = 13)\nplt.xlabel('NaN values (%)')\nplt.show()","aa973863":"NaN_test = pd.Series(example_test.isna().sum()[example_test.isna().sum() > 0].\n                      sort_values(ascending = False) \/ len(example_test) * 100)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 10))\n\nsns.barplot(y = NaN_test.index[:30], x = NaN_test[:30], \n            edgecolor = 'black', alpha = 0.8,\n            palette = sns.color_palette(\"viridis\", len(NaN_test[:30])))\nplt.title('NaN values of test dataset (30 columns)', size = 13)\nplt.xlabel('NaN values (%)')\nplt.show()","574561d2":"plt.figure(figsize=(10, 5))\nplt.title('Date', size = 15)\n\nsns.histplot(data = train_df, x = 'date',\n             edgecolor = 'black',\n             palette = \"viridis\")\nplt.xlabel('')\nplt.show()","99a34576":"def my_plot(feat, ax = None):\n    if ax != None:\n        sns.histplot(data = train_df, x = feat,\n                     palette = \"viridis\", ax = ax)\n        ax.set_xlabel('')\n        ax.set_title(f'{feat}')\n    else:\n        sns.histplot(data = train_df, x = feat,\n                     palette = \"viridis\")\n        plt.xlabel('')\n        plt.title(f'{feat}')","955b6e2e":"fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 10))\nsns.set_style(\"whitegrid\")\nplt.suptitle('resp 1-4 columns', size = 15)\n\nmy_plot('resp_1', ax1)\nmy_plot('resp_2', ax2)\nmy_plot('resp_3', ax3)\nmy_plot('resp_4', ax4)\n\nplt.show()","8ea2eca3":"plt.figure(figsize=(8, 5))\n\nmy_plot('resp')\nplt.show()","5bc94fdb":"plt.figure(figsize=(10, 5))\nplt.title('Weight', size = 15)\n\nsns.histplot(data = train_df, x = 'weight',\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('')\nplt.show()","25aa510a":"print('Rows with weight==0: \\t %d' %len(train_df[train_df.weight == 0]))","7db915d9":"train_df.ts_id.nunique() == len(train_df)","dec2e8db":"sample_df = sample(list(train_df.columns[7:]), 4)\n\nfig, ax = plt.subplots(2, 2, figsize=(16, 8))\nsns.set_style(\"whitegrid\")\nplt.suptitle('Random feature columns', size = 15)\n\nmy_plot(sample_df[0], ax[0, 0])\nmy_plot(sample_df[1], ax[0, 1])\nmy_plot(sample_df[2], ax[1, 0])\nmy_plot(sample_df[3], ax[1, 1])\n\nplt.show()","d51ba737":"features_df","c187f053":"features_tags = features_df.apply(lambda x: x[x == True].count(), axis = 1) \\\n    .sort_values(ascending = False).astype('str')\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 5))\n\nsns.histplot(x = features_tags,\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('Tags count')\nplt.show()","d38e5492":"pca = PCA()\npca_out = pca.fit(StandardScaler().fit_transform(train_df.iloc[:, 7:-1]\n                                                 .dropna()))","dd07cae4":"comp = pca_out.components_\nnum_pc = pca_out.n_features_\npc_list = [\"PC\" + str(i) for i in list(range(1, num_pc + 1))]\ncomp_df = pd.DataFrame.from_dict(dict(zip(pc_list, comp)))\ncomp_df['variable'] = train_df.iloc[:, 7:-1].columns.values\ncomp_df = comp_df.set_index('variable')\n\ncomp_df.head(10).style.background_gradient(cmap = 'viridis')","e674ee60":"plt.figure(figsize=(12, 8))\nplt.title('Corelation matrix of 10 first feature columns (Train dataset)', size = 15)\n\nsns.heatmap(comp_df.iloc[:10, :10], annot = True, cmap = 'Spectral')\nplt.show()","55a49517":"cluster.screeplot(obj = [pc_list[:20], pca_out.explained_variance_ratio_[:20]], \n                  show = True, dim = (16, 5), axlabelfontsize = 13)","6b73ab77":"# PCA loadings plots\n# 2D\ncluster.pcaplot(x = comp[0], y = comp[1], \n                labels = range(0, 129, 1), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2),\n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2),\n                show = True, dim = (10, 8), axlabelfontsize = 13)\n\n# 3D\ncluster.pcaplot(x = comp[0], y = comp[1], z = comp[2],  \n                labels = range(0, 129, 1), \n                var1 = round(pca_out.explained_variance_ratio_[0]*100, 2), \n                var2 = round(pca_out.explained_variance_ratio_[1]*100, 2), \n                var3 = round(pca_out.explained_variance_ratio_[2]*100, 2),\n                show = True, dim = (14, 10), axlabelfontsize = 13)","0b8a27bd":"test_pca = PCA()\ntest_pca_out = test_pca.fit(StandardScaler()\n                            .fit_transform(example_test.iloc[:, 2:-1]\n                                           .dropna()))","b71726dd":"comp_test = test_pca_out.components_\ntest_num_pc = test_pca_out.n_features_\ntest_pc_list = [\"PC\" + str(i) for i in list(range(1, test_num_pc + 1))]\ncomp_test_df = pd.DataFrame.from_dict(dict(zip(test_pc_list, comp_test)))\ncomp_test_df['variable'] = example_test.iloc[:, 2:-1].columns.values\ncomp_test_df = comp_test_df.set_index('variable')\n\ncomp_test_df.head(10).style.background_gradient(cmap = 'viridis')","263bdcb6":"plt.figure(figsize=(12, 8))\nplt.title('Corelation matrix of 10 first feature columns (Test dataset)', size = 15)\n\nsns.heatmap(comp_test_df.iloc[:10, :10], annot = True, cmap = 'Spectral')\nplt.show()","d792567f":"cluster.screeplot(obj = [test_pc_list[:20], \n                         test_pca_out.explained_variance_ratio_[:20]], \n                  show = True, dim = (16, 5), axlabelfontsize = 13)","3002ecc9":"# PCA loadings plots\n# 2D\ncluster.pcaplot(x = comp_test[0], y = comp_test[1], \n                labels = range(0, 129, 1), \n                var1 = round(test_pca_out.explained_variance_ratio_[0]*100, 2),\n                var2 = round(test_pca_out.explained_variance_ratio_[1]*100, 2),\n                show = True, dim = (10, 8), axlabelfontsize = 13)\n\n# 3D\ncluster.pcaplot(x = comp_test[0], y = comp_test[1], z = comp_test[2],  \n                labels = range(0, 129, 1), \n                var1 = round(test_pca_out.explained_variance_ratio_[0]*100, 2), \n                var2 = round(test_pca_out.explained_variance_ratio_[1]*100, 2), \n                var3 = round(test_pca_out.explained_variance_ratio_[2]*100, 2),\n                show = True, dim = (14, 10), axlabelfontsize = 13)","3c605d80":"# Loading prediction work space\nenv = janestreet.make_env()\niter_test = env.iter_test()","e67a758f":"# Preparing the data\ntrain_df = train_df[train_df['weight'] != 0]\ntrain_df['action'] = ((train_df['weight'].values * train_df['resp']\n                       .values) > 0).astype('int')\n\nX_train = train_df.loc[:, train_df.columns.str.contains('feature')]\ny_train = train_df.loc[:, 'action']\n\nX_train = X_train.fillna(-999)","369f5d99":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 5))\n\nsns.histplot(x = y_train.astype('str'),\n             edgecolor = 'black',\n             palette = \"viridis\", binwidth = 1)\nplt.xlabel('Action')\nplt.show()","d2059e99":"del train_df\ngc.collect()","96dee2cd":"# X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, \n#                                                 train_size = 0.85, \n#                                                 random_state = 0)","614e5e5b":"# params = {'n_estimators': 1000,\n#           'max_depth': 12,\n#           'subsample': 0.9,\n#           'learning_rate': 0.05,\n#           'missing': -999,\n#           'random_state': 0,\n#           'tree_method': 'gpu_hist'}\n\n# model = XGBClassifier(**params)\n\n# model.fit(X_tr, y_tr)","9e6ececa":"# print('ROC AUC score: %.3f' \n#       %roc_auc_score(y_valid, model.predict(X_valid)))","2524a41a":"# for (test_df, sample_prediction_df) in iter_test:\n#     X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n#     X_test.fillna(-999)\n#     preds = model.predict(X_test)\n#     sample_prediction_df.action = preds\n#     env.predict(sample_prediction_df)","f0650446":"# EDA","f22029c5":"Almost all columns in train and test datasets are numeric. Features dataset all have bool dtype.","54cf2ce7":"We have balanced targets.","84903ab8":"Let's look at the distribution of 'date' column.","e3ea84cd":"And also 'resp' and 'weight' columns, which together represents a return on the trade.","787248d7":"There is no significant difference between 'train' and 'example_test' datasets. Both have three PCs with importance over 10% and some number of less importance (around 5%). For future prediction, I'll use 10 PCs firstly.","e73aa5e0":"Positive and negative values in component loadings reflect the positive and negative correlation of the variables with then PCs.","07e45d9b":"We should keep only the PCs which explain the most variance. The eigenvalues for PCs can help to retain the number of PCs. It will be useful for future predictions.","42a00327":"## Work in progress...","0ed3badf":"# The second part of notebook: \n## [Jane Street Market Prediction: Baseline (Part 2)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/jane-street-market-prediction-baseline-part-2)","f6d1d4b6":"# Baseline prediction\n\nNow, we'll make a simple prediction, without complicated data preprocessing and feature engineering. We'll use XGBClassifier as a terrific simple but strong algorithm.\n\nFor the training process, we need feature columns with not zero weight values. As a prediction target ('action') we'll use a feature that contains 'weight' and 'resp' columns.","d2d5a352":"Dataset represents a set of bool values. Let's check some of the most frequent features.","66db33d4":"We should look at the 'features' dataset too.","b688e7bb":"Also, let's look at the test data example.","7b67002a":"Checking \"ts_id\" for unique values. The result must be 'True'.","9393a910":"# Loading and a first look at the data","ae1a807d":"# Jane Street Market Prediction\n![janestreet](https:\/\/www.janestreet.com\/assets\/logo_horizontal.png)\n\n### \u201cBuy low, sell high.\u201d It sounds so easy\u2026.\n\nIn reality, trading for profit has always been a difficult problem to solve, even more so in today\u2019s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n\n## See also the second part of this notebook:\n\n## [Jane Street Market Prediction: Baseline (Part 2)](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/jane-street-market-prediction-baseline-part-2)","ca5239bf":"#### Model evaluation","1aba815f":"# PCA"}}