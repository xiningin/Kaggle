{"cell_type":{"04006a04":"code","85922a0e":"code","5a75b4d1":"code","7124e8a9":"code","8edce716":"code","2ae7a5eb":"code","599329cf":"code","e14a9e37":"code","5fc1dc04":"code","eb90408f":"code","51c1141e":"code","954057c7":"code","9a4e426c":"code","db746758":"code","c9b3c995":"code","7e007e9d":"code","ea61e480":"code","e408b80a":"code","6d26062e":"code","dd39dc37":"code","9cd76677":"code","f7cc9777":"code","e4546ff8":"code","2d5b1e53":"markdown","7fa0fbbd":"markdown","78ef3b41":"markdown","64465d5b":"markdown","10bfb90b":"markdown","998d8fd4":"markdown","f81531dd":"markdown","38055875":"markdown","0b6e103a":"markdown","769dfc57":"markdown","2daeb074":"markdown","72dcaa47":"markdown","4cf51f49":"markdown","fa5e2ec7":"markdown","fe47b595":"markdown","fe132194":"markdown","f636434f":"markdown","56ce3be9":"markdown","940285fd":"markdown","a04a79e8":"markdown","6b190b91":"markdown","f177f47d":"markdown","90db4caf":"markdown","0a9787f9":"markdown","764d4e4a":"markdown","6ce36f01":"markdown"},"source":{"04006a04":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Another data visualisation (prettier than matplotlib)\nimport itertools\n\nimport math # We need this module to use mathematical function such as EXP()\n\nfrom sklearn.model_selection import train_test_split # Because we will split the training data into a training set and a validation set.\nfrom sklearn.metrics import confusion_matrix # Usefull at the end to see where the model makes mistakes\n\nfrom keras.utils.np_utils import to_categorical # Because we will convert lebels to one-hot-encoding\nfrom keras.models import Sequential # The type of keras model we'll be using\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, Activation # All the different layers that we need to build the model\nfrom keras.optimizers import RMSprop # Optimizer for the model\nfrom keras.preprocessing.image import ImageDataGenerator # This will help to make data augmentation and batches of images\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping # To make a model a bit more sophisticated\n# Import to loas the previous callbacks from TensorFlow and not from keras !\n\nsns.set(palette='muted') # To build pretty graphs","85922a0e":"!ls \/kaggle\/input\/ # The dataset is already attached to this notebook, we can just load the train and test sets form there","5a75b4d1":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\n\nprint('train shape is: ' + str(df_train.shape))\nprint('test shape is: ' + str(df_test.shape))","7124e8a9":"df_train.head()","8edce716":"y_train = df_train['label'] # y_train will contains the labels\nX_train = df_train.drop('label', axis = 1) # X_train contains the data to build the images","2ae7a5eb":"print('shape of the training label dataset: ' + str(y_train.shape))\nprint('shape of the training image dataset: ' + str(X_train.shape))","599329cf":"X_train.loc[0].value_counts().head() # Looks like pixels values range from 0 to 255. We'll need to normalize these. Moreover, their type is *int64* we need to convert them to float.\n# On the fisrt picture we can see that 687 out of the 784 pixels are completely black\n# And that around 30 are very bright\n# The others should be due to some kind of halo effect where pixels close to the bright ones are a bit ligthen up","e14a9e37":"X_train \/= 255.0\ndf_test \/= 255.0 # Test data should always be normalized the exact same way the training data have been normalized.","5fc1dc04":"X_full = X_train # We'll need this to have a dataset with all the input data\ny_full = y_train","eb90408f":"y_label = y_train # Just keeping a copy of the original labels before the one-hot-encoding\ny_train = to_categorical(y_train, num_classes = 10)\ny_full = to_categorical(y_full, num_classes = 10)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=42) # Here we are creating the training set and the validation set","51c1141e":"def reshape_data(data):\n    return(data.values.reshape(-1,28,28,1))","954057c7":"X_train = reshape_data(X_train)\nX_test = reshape_data(df_test)\nX_val = reshape_data(X_val)\nX_full = reshape_data(X_full)","9a4e426c":"def decode(label): # Since we have encoded the labels,we need to decode it to access to the original number.\n    maxi = max(label)\n    for i in range(len(label)+1):\n        if label[i] == maxi:\n            return(i)\n    \nplt.figure()\nfor i in range(0,9):\n    plt.subplot(330 + 1 + i) # Create the 3x3 image grid\n    plt.axis('off')\n    plt.imshow(X_train[i][:,:,0], cmap='gray')\n    plt.title('number = ' + str(decode(y_train[i])))\nplt.show()","db746758":"datagen = ImageDataGenerator(rotation_range=11, # Rotating randomly the images up to 25\u00b0\n                             width_shift_range=0.08, # Moving the images from left to right\n                             height_shift_range=0.08, # Then from top to bottom\n                             shear_range=0.10, \n                             zoom_range=0.07, # Zooming randomly up to 20%\n                             zca_whitening=False,\n                             horizontal_flip=False, \n                             vertical_flip=False,\n                            fill_mode = 'nearest')\n\ndatagen.fit(X_train) # Very important to fit the Generator on the data\n\nfor X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n    for i in range(0,9):\n        plt.subplot(330 + 1 + i)\n        plt.axis('off')\n        plt.imshow(X_batch[i][:,:,0], cmap='gray')\n        plt.title('number = ' + str(decode(y_batch[i])))\n    break\n# Since we are now batching, we won't get the exact same images from the last exemple.","c9b3c995":"def scheduler(epoch, lr):\n    if epoch < 15: # For the first 10 epochs, the learning rate is not changed\n        return(lr)\n    elif 15 < epoch < 30: # After ten is decreases exponentially\n        return(lr*math.exp(-0.1))\n    else:\n        return(lr*math.exp(-0.2)) # And then decreases even more, still exponentially\n\nLRScheduler = LearningRateScheduler(scheduler)","7e007e9d":"earlystopper = EarlyStopping(monitor='loss', min_delta =0, patience=6, verbose=1, mode='min',restore_best_weights=True) # If after 8 epochs (*patience=8*), the validation loss haven't decreased at all (*min_delta=0), the training stage is stopped","ea61e480":"import tensorflow as tf\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","e408b80a":"def create_model():\n    with strategy.scope(): # This line of code is very important if you want to run your full code on TPU or GPU. Make sur the model is defined with the \"with\" loop.\n        \n        model = Sequential([\n                    Conv2D(filters = 32, kernel_size = (3,3), activation ='relu', input_shape = (28 ,28 ,1)), # Important to specify the shape of the input data in the first layer.\n                    Conv2D(filters = 32, kernel_size = (3,3), activation ='relu'), # The kernel_size is the grid that will stop at every possible location to extract a patch of surrounding features\n                    BatchNormalization(),\n                    Activation('relu'),\n                    MaxPool2D(pool_size=(2,2)),\n                    Dropout(0.05), # We are shunting down 25% of the nodes randomly\n\n                    Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'), # Same as block 1 but with 64 nodes\n                    Conv2D(filters = 64, kernel_size = (3,3), activation ='relu'),\n                    BatchNormalization(),\n                    Activation('relu'),\n                    MaxPool2D(pool_size=(2,2)),\n                    Dropout(0.12),\n                    Flatten(), # Important to start using the 1D fully connected part of the layer\n\n                    Dense(256, activation='relu'), # Creating a layer with 256 nodes\n                    BatchNormalization(),\n                    Activation('relu'),\n\n                    Dense(128, activation='relu'),\n                    BatchNormalization(),\n                    Activation('relu'),\n\n                    Dense(84, activation='relu'),\n                    BatchNormalization(),\n                    Activation('relu'),\n                    Dropout(0.06),\n\n                    Dense(10, activation='softmax') # We need to end the model with a Dense layer composed of 10 nodes (because 10 numbers from 0 to 9) and with a softmax activation to get a probability\n        ])\n\n    model.compile(optimizer =RMSprop(lr=1e-4) , loss = \"categorical_crossentropy\", metrics=[\"acc\"])\n    \n    return(model)\n\ncreate_model().summary()","6d26062e":"EPOCHS = 80 # Number of time the model will see the data\nBATCH_SIZE = 86 # Number of images per batch\nSTEPS_PER_EPOCH = X_train.shape[0] \/\/ BATCH_SIZE # Since we are batching, we need to specify when the model should consider that one epoch has been processed.\n# A proper way to fix this parameter is to have a number of step equal to the number of data\nCALLBACKS = [LRScheduler, earlystopper] # List of the previously defined callbacks\n\nmodel = create_model() # Here we build the model by calling the previous function.\n\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=BATCH_SIZE),\n                              validation_data = (X_val,y_val),\n                              epochs = EPOCHS,\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)","dd39dc37":"acc = history.history['acc'][3:] # We won't display the 3 first epochs in order to have a more precise view of the last points.\nval_acc = history.history['val_acc'][3:]\nloss = history.history['loss'][3:]\nval_loss = history.history['val_loss'][3:]\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show() ","9cd76677":"# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n\nsns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='BuPu')\nplt.title('Confusion matrix MNIST')","f7cc9777":"datagen.fit(X_full) # Very important to fit the Generator on the data\n\nmodel_submission = create_model()\n\nmodel_submission.fit_generator(datagen.flow(X_full,y_full, batch_size=BATCH_SIZE), # No more validation set for the accuracy here !\n                              epochs = EPOCHS,\n                              verbose = 2,\n                              steps_per_epoch=STEPS_PER_EPOCH,\n                              callbacks=CALLBACKS)","e4546ff8":"# predict results\nresults = model_submission.predict(X_test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","2d5b1e53":"# 3. Data visualisation\nOne important part of this notebook is dedicated to the technique of *Data augmentation*. This is one of the best and easiest way to mitigate overfitting. In this section it is proposed to see the impact of *data augmentation* on the images.\n## 3.1 Without augmentation\nLet's have a look at those pictures without any transformation:","7fa0fbbd":"# 5. Running the model","78ef3b41":"## 2.1 Load the dataset","64465d5b":"Now it's time to create the model. Here I am creating it through a function since in my opinion, it is a good habbit (you can easily implement parameters to tune your model that way).","10bfb90b":"The train dataset is composed of 42000 samples and 785 rows. Each row corresponds to one pixel of the 28x28 image execpt the firt column that is dedicated to the label. The test dataset is composed of 784 rows because we can't access the label since it is for submission.\nLet's normalize these dataset and reshape them.","998d8fd4":"For now we just have created two huge datasets but we need to set apart the images and their labels for the train set. One can also notice that for now, datasets are not composed of images but of arrays of 784 pixels.\n\nThe train set is composed of one more column (feature) because it corresponds to the prediction.","f81531dd":"### Running on TPU or GPU\nIn order to train our model faster, we will use a TPU or a GPU. The next section is the code needed to do this. Make sure you have selection an accelerator (TPU or GPU) and that the notebook is connected to internet.","38055875":"## 2.2 Data Exploration","0b6e103a":"## 6.2 Confusion matrix\nThis will help us to understand where the model is making mistakes.","769dfc57":"Once we are satisfied with our model and parameters, it's time to recreate a new model and train it on the whole data available (Indeed, previously we used only 80% of the whole input data !)","2daeb074":"## 2.3 Data normalization\nIt's important, before feeding data to the model, to have them in the same range. Having heterogeneous data would lead to make the learning stage harder. Here we'll just divide each pixels value by 255.0 (the *.0* is important to create float numbers). However, one very common way to normalize data is to standardize it by substracting the mean of the feature and divide by the standard deviation.","72dcaa47":"# 7. Submission\n## 7.1 Building a brand new model on the whole dataset","4cf51f49":"### 4.1.2 EarlyStopping","fa5e2ec7":"This is no surprise that the model performs better on the validation set since we are unsing data augmentation technique. The model trains on more difficult data than the one we use to evaluate it.","fe47b595":"## 4.2 CNN\nThis is where we start building the model. Many different layers have been used here to reach a high accuracy but it is possible to reach very good accuracy (98%) with less. Let's cover quicly what these layer do:\n- Conv2D: This is a very powerful layer that is able to learn local patterns (edges, circles etc...) and that is translation invariant. It opperates as a filter to detect those patterns.\n- MaxPooling2D: This layer is used to make downsampling. The aim is to reduce the number of feature-map coefficient to process while applying spatial-filter hierarchies via a *max* tensor operator\n- BatchNormalization: This is a way to normalize the activation (the output) of the previous layers\n- Activation: Just an activation function layer\n- Dropout: This is another way to mitigate overfitting. The aim of this layer is to randomly shut down a speficied proportion of nodes.\n- Flatten: As the name says, this layer is used to go from 2D activations to 1D. This is mandatory to use Dense layers.\n- Dense: Basic fully connected layer.\n\nThis model in first structured with a stack of Conv2D and MaxPool2D layers and then there is a densely connected part","fe132194":"# Data Augmentation on the MNIST Dataset\nThis Dataset was made to present how to simply build a very effective convolutionnal network for the MNIST digit classification problem. It adresses these particular points:\n- Data visualisation\n- how to make data augmentation\n- Building a convolutional model from scartch\n- Evaluating a model\n\nThis will be the structuration of the following notebook:\n- 1. Import the relevant libraries\n- 2. Access the data\n    - 2.1. Load the Dataset\n    - 2.2 Data exploration\n    - 2.3 Data normalization\n    - 2.4 Reshaping the data\n- 3. Data Visualisation\n    - 3.1 Without data augmentation\n    - 3.2 With data augmentation\n- 4. Build the model\n    - 4.1 Creating the callbacks\n        - 4.1.1 LearningRateScheduler\n        - 4.1.2 EarlyStopping\n    - 4.2 CNN\n- 5. Running the model\n- 6. Evaluation\n    - 6.1 Accuracy and loss through epochs\n    - 6.2 Confusion matrix\n- 7. Submission\n\nWhile reading this, please keep in mind that I am not an expert myself. I am just trying to share what I learned from other kaggle competition notebooks, comments, and external sources.","f636434f":"# 6. Evaluation\n## 6.1 Accuracy and loss through epochs","56ce3be9":"We can clearly identify the images to the corresponding numbers. However, without modifying it, the model will only see the same pictures again and gain. As a result, it will be very efficient classifying these images but will do poorly on new images since it lost its ability to generalize (this is overfitting). By creating artificially new images from the original ones, we are challenging the model to learn in a more general way. It will take more time for the model to learn but it will be way more accurate on classifying new images.","940285fd":"# 1. Import the relevant libraries\nHere we load everything we need to use later","a04a79e8":"## 3.2 With augmentation\nThanks to Keras *ImageDataGenerator* classe, data augmentation is made very easy. It takes a bunch of arguments that changes the images randomly in the specified range. Some where used here, for instance to rotate a bit the images, to shift them from left to right, up and down, to zoom etc... Another important function of the *ImageDataGenerator* is that it allows to create batch of images. The previous modifications will affect only some pictures and will be fed immediatly to the model. This is a way to save a lot of memory !","6b190b91":"Thanks to *history* we can access some very usefull information on how the model learned through all the epochs and display the very usefull validation curves and loss curves.","f177f47d":"## 8. Submission","90db4caf":"# 4. Build the model\n## 4.1 Creating the callbacks\n*From Keras website* \"These are object that can perform actions at various stages of training (at the start or end of an epoch, before or after a batch etc.)\" There are not mandatory but they will help to create a more complete model. Here we'll create one that decreases the learning rate after a specific number of epochs to have a very accurate learning and one to stop the training session if the model stops improving\n### 4.1.1 LearningRateScheduler","0a9787f9":"# 2. Access the dataset\nBecause this notebook is created for a kaggle competition, the dataset is already attached to it","764d4e4a":"Now everything is fine. We can start looking at the train data and let the test data behind until the submission.","6ce36f01":"Let's store the predictions and the images (pixel columns for now) in different datasets:"}}