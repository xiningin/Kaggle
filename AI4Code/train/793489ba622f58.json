{"cell_type":{"f01a4f2d":"code","89c0b034":"code","cdb8202f":"code","14ad3508":"code","1ae89f9f":"code","908b503f":"code","9050beb0":"code","ca666224":"code","4621586f":"code","b0d83ae4":"code","5313b129":"code","38d7592d":"code","ba904e0c":"code","24f032c4":"code","60205013":"code","5467ba87":"code","84780108":"code","6ab9ee89":"code","0d4059a6":"code","bb3031f5":"code","51587b27":"markdown","8b326fda":"markdown","3052d397":"markdown","00f19542":"markdown","636e19fd":"markdown","234f63d3":"markdown","0199aa4a":"markdown","1da3e422":"markdown","5415b48b":"markdown","b99b32ac":"markdown","78e87192":"markdown","15296961":"markdown","64443a99":"markdown","25cc04d7":"markdown","c3f3fcdd":"markdown"},"source":{"f01a4f2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89c0b034":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\nsns.set(rc={f'figure.figsize':(12,10)})","cdb8202f":"from collections import Counter","14ad3508":"train_filepath = '\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv'\ntest_filepath = '\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv'","1ae89f9f":"train_df = pd.read_csv(train_filepath)\ntest_df = pd.read_csv(test_filepath)","908b503f":"columns = [col for col in train_df.columns if col not in ['id', 'claim']]","9050beb0":"full_dataset_size = train_df.shape[0] + test_df.shape[0]\nprint(f'Total size of both datasets: {full_dataset_size}')\nprint(f'Train data contains {train_df.shape[0]} rows ({round(train_df.shape[0]\/full_dataset_size*100)}% of all data) and {train_df.shape[1]} columns')\nprint(f'Test data contains {test_df.shape[0]} rows ({round(test_df.shape[0]\/full_dataset_size*100)}% of all data) and {test_df.shape[1]} columns')","ca666224":"pd. set_option(\"display.max_columns\", None)","4621586f":"train_df.head(3)","b0d83ae4":"test_df.head(3)","5313b129":"print(f'Total missing values in training set is {sum(train_df.isna().sum())}')\nprint(f'Total missing values in testing set is {sum(test_df.isna().sum())}')","38d7592d":"train_df.describe()","ba904e0c":"test_df.describe()","24f032c4":"train_df['claim'].isna().value_counts()","60205013":"'''\nGot inspiration from:\n1. https:\/\/www.kaggle.com\/harshsharma511\/titanic-eda-visualization-top-ensemble-models\n2. https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/\n'''\n\ndef outliers_itq_method(data, n, columns):\n    outlier_indices = []\n    \n    for col in columns:\n        q25, q75 = np.percentile(data[col], 25), np.percentile(data[col], 75)\n        iqr = q75 - q25\n        \n        cut_off = iqr * 1.5\n        lower, upper = q25 - cut_off, q75 + cut_off\n        \n        outliers = data[(data[col] < lower) | (data[col] > upper)].index\n        \n        outlier_indices.extend(outliers)\n        \n    outlier_indices = Counter(outlier_indices)\n    \n    final_outliers = list(k for k, v in outlier_indices.items() if v > n)\n    \n    return final_outliers","5467ba87":"train_df_outliers = outliers_itq_method(train_df, 2, columns)\nprint(f'outliers found: {len(train_df_outliers)}')","84780108":"# saving testing data ids\nids = test_df['id'].copy()\n\n# dropping id columns\ntrain_df.drop('id', axis=1, inplace=True)\ntest_df.drop('id', axis=1, inplace=True)","6ab9ee89":"sns.barplot(x=train_df['claim'].value_counts(), y=train_df['claim'], ci=False, orient='h')","0d4059a6":"train_corr = train_df.corr()\n\nmask = np.triu(np.ones_like(train_corr, dtype=bool))\n\nsns.heatmap(train_corr, mask=mask, center=0, square=True, cbar_kws={'shrink':.5})","bb3031f5":"fig, axis = plt.subplots(59,2,figsize=(24,200))\n\nfor i, col in enumerate(columns):\n    sns.histplot(train_df[col].values, kde=True, ax=axis[(i \/\/ 2),(i % 2)]).set(title=str(i+1))","51587b27":"## Outliers","8b326fda":"## Observations\n\n1. All features are of continues values in both datasets;\n2. Both datasets have 118 features and both contains id column. For this I will need to remove id column from train dataset and save id column from test dataset for later submissions;\n3. Total rows in both columns are over 1.4M with 957k in training data and 493k in test data;\n4. We have 1.8M missing values in training dataset and 936k missing values in testing dataset, will need to first, check how many columns have missing values in each row, then set a threshold and remove rows which exeeds the threshhold, lastyle prepare a pipeline for imputing values on remaining columns;\n5. Our target column is named claim;\n6. Some columns contains data which is small, some that is in thousands, will need to scale it. This applies to both datasets;\n7. Target columns has no missing values (which is nice);\n8. Using IQR outliers detection technique found no outliers;\n\n","3052d397":"### Observations\nCan see that claims distribution is balanced meaning both approved and denied claims have similar entries, which approved claimes having very small advantage","00f19542":"## 2. Correlations","636e19fd":"## First insights","234f63d3":"# Tabular Playground Series - Sep 2021\n\n## Dataset\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.\n\n## Data Description\n\nFor this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\n## Evaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","0199aa4a":"# Loading Data","1da3e422":"# Explanatory Data Analysis","5415b48b":"# Imports","b99b32ac":"## 1. Claims count","78e87192":"# Visualisation","15296961":"## Dropping id columns","64443a99":"### Observations\n\nMajority of features are skewed, so I will probobly will try log transformation before further scaling.","25cc04d7":"## 3. Distributions of features","c3f3fcdd":"### Observations\n\nMajority of correlation is between -.01 and .01."}}