{"cell_type":{"b28c0280":"code","45dc30bb":"code","46044d90":"code","e5c86afa":"code","1b010191":"code","50d0806b":"markdown","4ac9d0dc":"markdown","a5bfe778":"markdown","b4af4f7c":"markdown","1172ea9a":"markdown","c39f94f6":"markdown","4c198ddd":"markdown","5da4a512":"markdown"},"source":{"b28c0280":"\n# Setup. Import libraries and load dataframes for Movielens data.\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport random\n\ntf.set_random_seed(1); np.random.seed(1); random.seed(1) # Set random seeds for reproducibility\n\ninput_dir = '..\/input\/movielens-preprocessing'\nratings_path = os.path.join(input_dir, 'rating.csv')\n\nratings_df = pd.read_csv(ratings_path, usecols=['userId', 'movieId', 'rating', 'y'])\ndf = ratings_df\n\nmovies_df = pd.read_csv(os.path.join(input_dir, 'movie.csv'), usecols=['movieId', 'title'])","45dc30bb":"movie_embedding_size = user_embedding_size = 8\n\n# Each instance consists of two inputs: a single user id, and a single movie id\nuser_id_input = keras.Input(shape=(1,), name='user_id')\nmovie_id_input = keras.Input(shape=(1,), name='movie_id')\nuser_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n                                       input_length=1, name='user_embedding')(user_id_input)\nmovie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n                                        input_length=1, name='movie_embedding')(movie_id_input)\n\ndotted = keras.layers.Dot(2)([user_embedded, movie_embedded])\nout = keras.layers.Flatten()(dotted)\n\nmodel = keras.Model(\n    inputs = [user_id_input, movie_id_input],\n    outputs = out,\n)\nmodel.compile(\n    tf.train.AdamOptimizer(0.001),\n    loss='MSE',\n    metrics=['MAE'],\n)\nmodel.summary(line_length=88)","46044d90":"history = model.fit(\n    [df.userId, df.movieId],\n    df.y,\n    batch_size=5000,\n    epochs=20,\n    verbose=0,\n    validation_split=.05,\n);","e5c86afa":"\n# Save the model to disk. (We'll be reusing it in a later exercise)\nmodel.save('factorization_model.h5')","1b010191":"\n# Load up the training stats we saved to disk in the previous tutorial\nhistory_dir = '..\/input\/embedding-layers'\npath = os.path.join(history_dir, 'history-1.csv')\nhdf = pd.read_csv(path)\n\nfig, ax = plt.subplots(figsize=(15, 8))\nc1 = 'blue'\nax.plot(history.epoch, history.history['val_mean_absolute_error'], '--', label='Validation MAE', color=c1)\nax.plot(history.epoch, history.history['mean_absolute_error'], label='Training MAE', color=c1)\n\nc2 = 'orange'\nax.plot(hdf.epoch, hdf.val_mae, '--', label='Validation MAE (DNN)', color=c2)\nax.plot(hdf.epoch, hdf.train_mae, label='Training MAE (DNN)', color=c2)\nax.set_xlabel('Epoch')\nax.set_ylabel('Mean Absolute Error')\nax.set_xlim(left=0)\nbaseline_mae = 0.73\nax.axhline(baseline_mae, ls='-.', label='Baseline', color='#002255', alpha=.5)\nax.grid()\nfig.legend();","50d0806b":"# Matrix factorization for recommendation problems\n\nIn the previous lesson, we trained a model to predict the ratings assigned to movies by users in the [MovieLens dataset](https:\/\/www.kaggle.com\/grouplens\/movielens-20m-dataset\/home). As a reminder the model looked something like this:\n\n![Imgur](https:\/\/i.imgur.com\/Z1eVQu9.png)\n\nWe look up an embedding vector for the movie and user, concatenate them together. Then we add some hidden layers. Finally these come together at a single output node to predict a rating.\n\nIn this lesson, I'll show a simpler architecture for solving the same problem: **matrix factorization**. And simpler can be a very good thing! Sometimes a simple model will converge quickly to an adequate solution, where a more complicated model might overfit or fail to converge.\n\nHere's what our matrix factorization model will look like:\n\n![Imgur](https:\/\/i.imgur.com\/lUzvCHj.png)","4ac9d0dc":"Let's train it.","a5bfe778":"Let's compare the error over time for this model to the deep neural net we trained in the previous lesson:","b4af4f7c":"# Your turn!\n\nHead over to [the Exercises notebook](https:\/\/www.kaggle.com\/kernels\/fork\/1598589) to get some hands-on practice working with matrix factorization.\n### P.S...\n\nThis course is still in beta, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this lesson](https:\/\/form.jotform.com\/82826168584267), I'd greatly appreciate it. You can also leave public feedback in the comments below, or on the [Learn Forum](https:\/\/www.kaggle.com\/learn-forum).\n","1172ea9a":"Our new, simpler model (in blue) is looking pretty good.\n\nHowever, even though our embeddings are fairly small, both models suffer from some obvious overfitting. That is,  the error on the training set - the solid lines - is significantly better than on the unseen data. We'll work on addressing that very soon in the exercise.","c39f94f6":"# Why?\n\nThere's an intuitive interpretation that supports the decision to combine our embedding vectors in this way. Suppose the dimensions of our movie embedding space correspond to the following axes of variation:\n\n- Dimension 1: How action-packed?\n- Dimension 2: How romantic?\n- Dimension 3: How mature is the intended audience?\n- Dimension 4: How funny is it?\n\nHence, *Twister*, an action-packed disaster movie, has a positive value of 1.0 for $m_1$.\n\nWhat does this imply about the meaning of our user vectors? Remember that $m_1u_1$ is one of the terms we add up to get our predicted rating. So if $u_1$ is 1.0, it will increase our predicted rating by 1 star (vs. $u_1 = 0$). If $u_1 = .5$, our predicted rating goes up half a star. If $u_1$ is -1, our predicted rating goes down a star.\n\nIn plain terms $u_1$ tells us 'how does this user feel about action?'. Do they love it? Hate it? Or are they indifferent?\n\nStanley's vector tells us he's a big fan of romance and comedy, and slightly dislikes action and mature content. What if we give him a movie that's similar to the last one except that it has lots of romance?\n\n$$\\mathbf{m_{Titanic}} = \\begin{bmatrix} 1.0 & 1.1 & 0.3 & -0.1 \\end{bmatrix} $$\n\nIt's not hard to predict how this affects our rating output. We're giving Stanley more of what he likes, so his predicted rating increases.\n\n\\begin{align}\n\\ \\mathrm{predicted\\_rating(Stanley, Titanic)} &= \\mathbf{m_{Titanic}} \\cdot \\mathbf{u_{Stanley}} + 3.5 \\\\\n&= (1.0 \\cdot -0.2) + (1.1 \\cdot 1.5) + (0.3 \\cdot -0.1) + (-0.1 \\cdot 0.9) + 3.5 \\\\\n&= 4.83 \\text{ stars}\n\\end{align}\n\n> **Aside:** In practice, the meaning of the dimensions of our movie embeddings will not be quite so clear-cut, but it remains true that the meaning of our movie embedding space and user embedding space are fundamentally tied together: $u_i$ will always represent \"how much does this user like movies that have the quality represented by $m_i$?\". (Hopefully this also gives some more intuition for why the movie embedding space and user embedding space have to be the same size for this technique.)\n\n\n# Implementing it","4c198ddd":"# Dot Products\n\nLet's review a bit of math. If you're a linear algebra pro, feel free to skip this section.\n\nThe dot product of two length-$n$ vectors $\\mathbf{a}$ and $\\mathbf{b}$ is defined as:\n\n$$\\mathbf{a}\\cdot\\mathbf{b}=\\sum_{i=1}^n a_ib_i=a_1b_1+a_2b_2+\\cdots+a_nb_n$$\n\nThe result is a single scalar number (not a vector).\n\nThe dot product is only defined for vectors *of the same length*. This means we need to use the same size for movie embeddings and user embeddings.\n\nAs an example, suppose we've trained embeddings of size 4, and the movie *Twister* is represented by the vector:\n\n$$\\mathbf{m_{Twister}} = \\begin{bmatrix} 1.0 & -0.5 & 0.3 & -0.1 \\end{bmatrix} $$\n\nAnd the user Stanley is represented by:\n\n$$\\mathbf{u_{Stanley}} = \\begin{bmatrix} -0.2 & 1.5 & -0.1 & 0.9 \\end{bmatrix} $$\n\nWhat rating do we think Stanley will give to *Twister*? We can calculate our model's output as:\n\n\\begin{align}\n\\ \\mathbf{m_{Twister}} \\cdot \\mathbf{u_{Stanley}} &= (1.0 \\cdot -0.2) + (-0.5 \\cdot 1.5) + (0.3 \\cdot -0.1) + (-0.1 \\cdot 0.9) \\\\\n&= -1.07\n\\end{align}\n\nBecause we're training on a a centered version of the rating column, our model's output is on a scale where 0 = the overall average rating in the training set (about 3.5). So we predict that Stanley will give *Twister* $3.5 + (-1.07) = 2.43$ stars.","5da4a512":"The code to create this model is similar to the code we wrote in the previous lesson, except I combine the outputs of the user and movie embedding layers using a `Dot` layer (instead of concatenating them, and piling on dense layers)."}}