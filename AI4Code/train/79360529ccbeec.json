{"cell_type":{"1908fe8a":"code","f546e4b0":"code","b57c7326":"code","e8491b85":"code","2a326ffa":"code","8838a846":"code","cae9ecbf":"code","e72a141b":"code","05169074":"code","72fa0b21":"code","637ae422":"code","c7b59bde":"code","eeb39dde":"code","63fa40f5":"code","76f5d59e":"code","b51117b8":"code","e4f5e219":"code","b6efe7b5":"code","39a53bcf":"code","5e20b7ea":"code","9d2d8b81":"code","9707a0ce":"code","29e0e127":"code","a194965a":"code","3d4f6fd3":"code","c18fa6dd":"code","053756c8":"code","7f752757":"code","24e786c5":"code","fb4b34e5":"code","04533d07":"code","b12a4ac6":"code","935e4ac6":"code","2d5affc0":"code","86813880":"code","df609e3a":"code","dfbdd59b":"code","e8cdc845":"code","48c16e8f":"code","c384ec3f":"code","f919f3bf":"code","33fab465":"code","ed0f6c59":"code","04f0fbf3":"code","94262183":"code","65587516":"code","9cf8f071":"code","aac47162":"code","c8dee3e0":"code","15fb7b9c":"code","045e7617":"code","8d451bd9":"code","c7aea542":"code","798eafff":"code","29806282":"code","64ee66d5":"code","b5d2a600":"code","34686d6a":"code","96dd0304":"code","993d2f1b":"code","127ba5f2":"code","09e7ac71":"code","bc17a372":"code","67e4a2d2":"code","1bcb6133":"code","d22eccf3":"code","8376cdd3":"code","58b7d8ab":"code","8d88daee":"code","b866b5e2":"code","12c6efc0":"code","66522ab6":"code","e0e94b05":"code","1cb07b6b":"code","8312cc54":"code","d5fb84d6":"code","c2d5be86":"code","e5adac2d":"code","d97461c2":"code","16f78a99":"code","8b7c2e88":"code","c3be3727":"code","8adb9518":"code","a2b1a65e":"code","752c7dc0":"markdown","7333f61e":"markdown","78fa18ae":"markdown","172f6ae4":"markdown","acc9f4f8":"markdown","ce0210cf":"markdown","757ce308":"markdown","f2187b40":"markdown","6e3aad3d":"markdown","59a67005":"markdown","accbe015":"markdown","3bff8290":"markdown","27d83e07":"markdown","75de2242":"markdown","8206932b":"markdown","4d94429a":"markdown","45fbf1e8":"markdown","80443350":"markdown","b8a6f670":"markdown","28a3e629":"markdown","15e8912f":"markdown","45b4603e":"markdown","056da0e2":"markdown","5b8c8410":"markdown","94d30292":"markdown","f76aaa5e":"markdown","8c45be8b":"markdown","43ac7782":"markdown","c6ce6156":"markdown","0bd04db5":"markdown","0758d08a":"markdown","a4833f4c":"markdown","d1087465":"markdown","5db7f740":"markdown","8ec53c8d":"markdown","d0277182":"markdown","312ee654":"markdown","2a6d38f6":"markdown","a0d90eca":"markdown","bad1206e":"markdown","e26a2ff2":"markdown","f1aa9f22":"markdown","e1432462":"markdown","b97188a9":"markdown","ac3b1239":"markdown","6b8604e8":"markdown","94af2d84":"markdown","148a3763":"markdown","72673821":"markdown"},"source":{"1908fe8a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels as sm\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.model_selection import learning_curve, cross_val_score, train_test_split, cross_val_predict, cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import roc_auc_score, precision_score, plot_confusion_matrix, recall_score\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, ElasticNet\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBRFClassifier, XGBClassifier, plot_importance \nfrom imblearn.combine import SMOTEENN \nfrom pprint import pprint\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.metrics import classification_report_imbalanced, geometric_mean_score\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.ensemble import RUSBoostClassifier, BalancedRandomForestClassifier, BalancedBaggingClassifier,EasyEnsembleClassifier\nimport os\nfrom sklearn.naive_bayes import MultinomialNB\nfrom warnings import filterwarnings\nfrom keras import Sequential, layers, Input, callbacks, utils, regularizers\nimport tensorflow as tf","f546e4b0":"plt.style.use('fivethirtyeight')\nsns.set_style('whitegrid')\nfilterwarnings('ignore')","b57c7326":"file = '..\/input\/machine-predictive-maintenance-classification\/predictive_maintenance.csv'","e8491b85":"maintenance = pd.read_csv(file)","2a326ffa":"maintenance.head()","8838a846":"maintenance.info()","cae9ecbf":"plt.figure(figsize=(15,8))\ng=sns.countplot(x='Target', data=maintenance)\nfor i, u in enumerate(maintenance['Target'].value_counts().values):\n    g.text(i, u, str(u), bbox=dict(facecolor='green', alpha=0.5))\nplt.title('Machine maintenance decision.')\nplt.show()","e72a141b":"maintenance.drop(columns=['UDI'], inplace=True)","05169074":"maintenance.describe()","72fa0b21":"maintenance.corr()","637ae422":"maintenance.skew()","c7b59bde":"maintenance.kurtosis()","eeb39dde":"fig, ax = plt.subplots(2, 2, figsize=(15,10))\nsns.histplot(data=maintenance, x='Air temperature [K]', kde=True, ax=ax[0,0])\nsns.histplot(data=maintenance, x='Process temperature [K]', kde=True, ax=ax[0,1])\nsns.boxplot(data=maintenance, x='Air temperature [K]', ax=ax[1,0])\nsns.boxplot(data=maintenance, x='Process temperature [K]', ax=ax[1,1])\nplt.show()","63fa40f5":"fig1, ax1 = plt.subplots(2, 3, figsize=(15,10))\nsns.histplot(data=maintenance, x='Rotational speed [rpm]', kde=True, ax=ax1[0,0])\nsns.histplot(data=maintenance, x='Torque [Nm]', kde=True, ax=ax1[0,1])\nsns.histplot(data=maintenance, x='Tool wear [min]', kde=True, ax=ax1[0,2])\nsns.boxplot(data=maintenance, x='Rotational speed [rpm]', ax=ax1[1,0])\nsns.boxplot(data=maintenance, x='Torque [Nm]', ax=ax1[1,1])\nsns.boxplot(data=maintenance, x='Tool wear [min]', ax=ax1[1,2])\nplt.show()","76f5d59e":"plt.figure(figsize=(15,5))\nh=sns.countplot(x='Type', data=maintenance, order=['L','M','H'])\nfor i, u in enumerate(maintenance['Type'].value_counts().values):\n    h.text(i, u, str(u), bbox=dict(facecolor='green', alpha=0.5))\nplt.title('Product type')\nplt.show()","b51117b8":"plt.figure(figsize=(15,5))\nmachine_failure = maintenance[maintenance['Failure Type'] != 'No Failure']\nhg=sns.countplot(x='Failure Type', data=machine_failure, order=['Heat Dissipation Failure', 'Power Failure',\n                                                              'Overstrain Failure', 'Tool Wear Failure',\n                                                              'Random Failures'])\nfor i, u in enumerate(machine_failure['Failure Type'].value_counts().values):\n    hg.text(i, u, str(u), bbox=dict(facecolor='green', alpha=0.5))\nplt.title('Machine Failure Type')\nplt.show()","e4f5e219":"plt.figure(figsize=(15,5))\nsns.scatterplot(x='Air temperature [K]', y='Process temperature [K]',\n            hue='Target', alpha=0.75, data=maintenance)\n#plt.xlabel('Air temperature [K]')\n#plt.ylabel('Process temperature [K]')\nplt.show()","b6efe7b5":"plt.figure(figsize=(15,5))\nsns.scatterplot(x='Rotational speed [rpm]', y='Torque [Nm]', hue='Target', alpha=0.85, data=maintenance)\nplt.show()","39a53bcf":"maintenance['Power [W]'] = maintenance['Torque [Nm]']*(2*np.pi*maintenance['Rotational speed [rpm]']\/60.0)\nmaintenance['Overstrain [minNm]'] = maintenance['Torque [Nm]'] * maintenance['Tool wear [min]']\nmaintenance['Heat dissipation [rpminK]'] = abs(maintenance['Air temperature [K]'] -maintenance['Process temperature [K]'])*maintenance['Rotational speed [rpm]']","5e20b7ea":"maintenance.tail(3)","9d2d8b81":"plt.figure(figsize=(10,8))\nsns.heatmap(maintenance.corr(), annot=True, center=0)\nplt.title('Machine maintenance correlation with new features')\nplt.show()","9707a0ce":"maintenance[['Power [W]', 'Overstrain [minNm]', 'Heat dissipation [rpminK]']].describe()","29e0e127":"maintenance[['Power [W]', 'Overstrain [minNm]', 'Heat dissipation [rpminK]']].plot.box(subplots=True,\n                                                                                      figsize=(15,5))\nplt.suptitle('Machine Failure boxplot')\nplt.show()","a194965a":"power = maintenance[maintenance['Failure Type'].isin(['Power Failure', 'Random Failures'])]\noverstrain = maintenance[maintenance['Failure Type'].isin(['Overstrain Failure', 'Random Failures'])]\nheat_dissipation = maintenance[maintenance['Failure Type'].isin(['Heat Dissipation Failure', 'Random Failures'])]\ntool_wear = maintenance[maintenance['Failure Type'].isin(['Tool Wear Failure', 'Random Failures'])]","3d4f6fd3":"_, ax2 = plt.subplots(1, 4, figsize=(16,4))\nsns.histplot(x='Power [W]',data=power, kde=True, ax=ax2[0])\nsns.histplot(x='Overstrain [minNm]',data=overstrain, kde=True, ax=ax2[1])\nsns.histplot(x='Heat dissipation [rpminK]', data=heat_dissipation, ax=ax2[2], kde=True)\nsns.histplot(x='Tool wear [min]', data=tool_wear, ax=ax2[3], kde=True)\nplt.show()","c18fa6dd":"_, ax3 = plt.subplots(1, 4, figsize=(16,4))\ndf = maintenance[maintenance['Failure Type'] == 'No Failure']\nsns.histplot(x='Power [W]',data=df  , kde=True, ax=ax3[0])\nsns.histplot(x='Overstrain [minNm]',data=df, kde=True, ax=ax3[1])\nsns.histplot(x='Heat dissipation [rpminK]', data=df, ax=ax3[2], kde=True)\nsns.histplot(x='Tool wear [min]', data=df, ax=ax3[3], kde=True)\nplt.suptitle('Machine No Failure histogram')\nplt.show()","053756c8":"_, ax4  = plt.subplots(1, 4, figsize=(16,4))\nsns.boxplot(x='Power [W]',data=df , ax=ax4[0])\nsns.boxplot(x='Overstrain [minNm]', data=df, ax=ax4[1])\nsns.boxplot(x='Heat dissipation [rpminK]', data=df, ax=ax4[2])\nsns.boxplot(x='Tool wear [min]', data=df, ax=ax4[3])\nplt.suptitle('Machine No Failure boxplot')\nplt.show()","7f752757":"plt.figure(figsize=(15, 5))\nsns.scatterplot(y='Power [W]', x='Torque [Nm]', hue='Failure Type', data=maintenance)\nplt.show()","24e786c5":"plt.figure(figsize=(15, 5))\nsns.scatterplot(y='Power [W]', x='Heat dissipation [rpminK]', hue='Failure Type', data=maintenance)\nplt.show()","fb4b34e5":"plt.figure(figsize=(15, 5))\nsns.scatterplot(y='Overstrain [minNm]', x='Tool wear [min]', hue='Failure Type', data=maintenance)\nplt.show()","04533d07":"plt.figure(figsize=(15, 5))\nsns.scatterplot(x='Rotational speed [rpm]', y='Heat dissipation [rpminK]', hue='Failure Type', data=maintenance)\nplt.show()","b12a4ac6":"data =  maintenance.drop(columns=['Product ID','Type', 'Failure Type'])","935e4ac6":"matcorr = data.corr()\nvif = pd.Series(np.linalg.inv(matcorr.to_numpy()).diagonal(), index=data.columns, name='vif_factor' )","2d5affc0":"vif.reset_index()","86813880":"cols_selected = ['Process temperature [K]', 'Torque [Nm]', 'Tool wear [min]']","df609e3a":"# We compute new vif\nmatcorr_ = data[cols_selected].corr()\nvif_ = pd.Series(np.linalg.inv(matcorr_.to_numpy()).diagonal(), index=matcorr_.columns, name='vif_factor' )","dfbdd59b":"vif_.reset_index()","e8cdc845":"Xdata = data[cols_selected] #\ntarget = maintenance['Target'] #","48c16e8f":"xtrain, xtest, ytrain, ytest = train_test_split(Xdata, target, stratify=target, random_state=0, test_size=0.2)","c384ec3f":"print(f'Xtrain shape: {xtrain.shape} ytrain shape: {ytrain.shape}.')","f919f3bf":"def ensemble_sampler_learning(X, y):\n    \n    ens_learners = {'bagg':BalancedBaggingClassifier(base_estimator=ExtraTreeClassifier(), random_state=0,\n                                                    n_jobs=-1),\n                   'rus': RUSBoostClassifier(base_estimator=ExtraTreeClassifier(), random_state=0),\n                   'rfc':BalancedRandomForestClassifier(random_state=0, n_jobs=-1),\n                   'easy':EasyEnsembleClassifier(base_estimator=ExtraTreeClassifier(), \n                                                 random_state=0, n_jobs=-1 )}\n    results = {}\n    imb_results = {}\n    \n    X, Xvalid, y, yvalid = train_test_split(X, y, stratify=y, random_state=42, train_size=0.66)\n    \n    print(\"Cross validation\")\n    print('=====================================================================')\n    for u in ens_learners.keys():\n        model = make_pipeline(RobustScaler(), PCA(n_components=0.95), ens_learners[u])\n        cv_results = cross_validate(model, X, y=y, cv=5, n_jobs=-1, scoring=\"roc_auc\",\n                                     return_train_score=True, return_estimator=True)\n        print('Learner', u)\n        print(f\"Training roc_auc mean +\/- std. dev.: \"\n             f\"{cv_results['test_score'].mean():.3f} +\/- \"\n             f\"{cv_results['test_score'].std():.3f}\")\n        print('\\n')\n        \n        auc = []\n        score = []\n        for foldid, cv_model in enumerate(cv_results['estimator']):\n            ypred = cv_model.predict(Xvalid)\n            auc.append(roc_auc_score(yvalid, ypred))\n            score.append(geometric_mean_score(yvalid, ypred, average='binary'))\n        \n            results[u] = auc\n            imb_results[u] = score\n        \n    return results, imb_results","33fab465":"auc_result_sampler, results_sampler = ensemble_sampler_learning(xtrain.values, ytrain)","ed0f6c59":"for u in auc_result_sampler.keys():\n    print(f'{u}: auc = {np.mean(auc_result_sampler[u])} +\/- {np.std(auc_result_sampler[u])}')","04f0fbf3":"for u in results_sampler.keys():\n    print(f'{u}: G-mean = {np.mean(results_sampler[u])} +\/- {np.std(results_sampler[u])}')","94262183":"pipe_imbalanced_ = Pipeline([('scaler', RobustScaler()),\n                            ('BRFC', BalancedRandomForestClassifier(n_jobs=-1, random_state=0))])","65587516":"params = {'BRFC__n_estimators': [100, 500, 1000], 'BRFC__criterion': [\"gini\", 'entropy'],\n          'BRFC__max_depth': [1,2, 3, 4, 5],\n         'BRFC__max_features': ['auto','sqrt','log2']}","9cf8f071":"#gridsearch = GridSearchCV(estimator=pipe_imbalanced_, param_grid=params, \n    #                      scoring='roc_auc', cv=5, refit=True, verbose=2, n_jobs=-1)","aac47162":"#%%time\n#gridsearch.fit(xtrain, ytrain)","c8dee3e0":"#gridsearch.best_params_","15fb7b9c":"#gridsearch.best_score_","045e7617":"pipe_imbalanced = Pipeline([('scaler', RobustScaler()),\n                            ('BRFC', BalancedRandomForestClassifier(n_jobs=-1,\n                                                                    random_state=0,\n                                criterion='gini', max_depth=5,\n                                                                   max_features='sqrt'))])","8d451bd9":"pipe_imbalanced.fit(xtrain.values, y=ytrain)","c7aea542":"print(classification_report_imbalanced(ytrain, pipe_imbalanced.predict(xtrain)))","798eafff":"plot_confusion_matrix(pipe_imbalanced, xtrain, ytrain, labels=[0, 1])\nplt.grid(False)\nplt.title('Training: Confusion matrix.')\nplt.show()","29806282":"print(f'roc_auc training: {roc_auc_score(ytrain, pipe_imbalanced.predict(xtrain))}')","64ee66d5":"ypred = pipe_imbalanced.predict(xtest.values)","b5d2a600":"print(classification_report_imbalanced(ytest, ypred))","34686d6a":"plot_confusion_matrix(pipe_imbalanced, xtest, ytest, labels=[0, 1])\nplt.grid(False)\nplt.title('Evaluation: Confusion matrix.')\nplt.show()","96dd0304":"print(f'roc_auc test: {roc_auc_score(ytest, pipe_imbalanced.predict(xtest))}')","993d2f1b":"failure_data = maintenance[maintenance['Failure Type'] != 'No Failure']","127ba5f2":"fdata = failure_data[cols_selected]\nftarget = failure_data['Failure Type']","09e7ac71":"print(f'failure data shape: {fdata.shape}. failure type target shape = {ftarget.shape}.')","bc17a372":"#we convert \nftarget = ftarget.astype(\"category\")\nftarget.cat.categories =[0,1,2,3,4]\nftarget = ftarget.astype('int')","67e4a2d2":"fdata.plot.box(subplots=True, figsize=(15,5))\nplt.show()","1bcb6133":"#We split our data.\nfxtrain, fxtest, fytrain, fytest = train_test_split(fdata, ftarget, stratify=ftarget, random_state=0,\n                                                    test_size=0.2)","d22eccf3":"def best_enSamplerLearner(X, y):\n\n    ens_learners = {'bagg':BalancedBaggingClassifier(base_estimator=ExtraTreeClassifier(), random_state=0,\n                                                    n_jobs=-1),\n                   'rus': RUSBoostClassifier(base_estimator=ExtraTreeClassifier(), random_state=0),\n                   'rfc':BalancedRandomForestClassifier(random_state=0, n_jobs=-1),\n                   'easy':EasyEnsembleClassifier(base_estimator=ExtraTreeClassifier(), \n                                                 random_state=0, n_jobs=-1 )}\n    print(\"Cross validation\")\n    print('=====================================================================')\n    for u in ens_learners.keys():\n        model = make_pipeline(RobustScaler(),  ens_learners[u])\n        cv_results = cross_validate(model, X, y=y, cv=10, n_jobs=-1,\n                                     return_train_score=False, return_estimator=True)\n        \n        train_score = []\n        for foldid, cv_model in enumerate(cv_results['estimator']):\n            y_pred = cv_model.predict(X)\n            train_score.append(geometric_mean_score(y, y_pred))\n        print(u)\n        print(f'cross validation G-mean = {np.mean(train_score)} +\/- {np.std(train_score)}.')\n        print('\\n')","8376cdd3":"best_enSamplerLearner(fxtrain, fytrain)","58b7d8ab":"fparams = {'BRFC__n_estimators': [100, 200], 'BRFC__criterion': [\"gini\", 'entropy'],\n          'BRFC__max_depth': [1, 2, 3, 4, 5, 6,7],\n         'BRFC__max_features': ['auto','sqrt','log2']}","8d88daee":"imbal_pipe_failure_ = Pipeline([('scaler', RobustScaler()),\n                            ('BRFC', BalancedRandomForestClassifier(n_jobs=-1, random_state=0))])","b866b5e2":"#fgridsearch = GridSearchCV(estimator=imbal_pipe_failure_, param_grid=fparams, \n                          #scoring='balanced_accuracy', cv=5, refit=True, verbose=1, n_jobs=-1)","12c6efc0":"#%%time\n#fgridsearch.fit(fxtrain, fytrain)","66522ab6":"#fgridsearch.best_params_","e0e94b05":"#fgridsearch.best_score_","1cb07b6b":"imbalanced_pipe_failure = Pipeline([('scaler', RobustScaler()),\n                            ('BRFC', BalancedRandomForestClassifier(n_jobs=-1, random_state=0,\n                                                                   criterion='entropy',max_depth=5,\n                                                                   max_features='auto', n_estimators=100))])","8312cc54":"imbalanced_pipe_failure.fit(fxtrain, fytrain)","d5fb84d6":"print(f'Training G-mean = {geometric_mean_score(fytrain, imbalanced_pipe_failure.predict(fxtrain))}')","c2d5be86":"print(classification_report_imbalanced(fytrain, imbalanced_pipe_failure.predict(fxtrain)))","e5adac2d":"plot_confusion_matrix(imbalanced_pipe_failure, fxtrain, fytrain, labels=[0,1,2,3,4])\nplt.grid(False)\nplt.show()","d97461c2":"print(f'Test G-mean = {geometric_mean_score(fytest, imbalanced_pipe_failure.predict(fxtest))}')","16f78a99":"print(classification_report_imbalanced(fytest, imbalanced_pipe_failure.predict(fxtest)))","8b7c2e88":"plot_confusion_matrix(imbalanced_pipe_failure, fxtest, fytest, labels=[0,1,2,3,4])\nplt.grid(False)\nplt.show()","c3be3727":"def MPM_model_decision(input_data=None):\n    \"\"\"\n    input_data: 1d-dimensional array data\n    \n    return Failure and Failure Type\n    \"\"\"\n    #\n    failure_type = sorted(failure_data['Failure Type'].unique().tolist())\n    ypred = pipe_imbalanced.predict(input_data)\n    prob  = pipe_imbalanced.predict_proba(input_data)[0]\n    if ypred == 0:\n        print(f\"Decision = {'No Failure'} with probability = {prob[ypred][0]}\")\n    else:\n        y_pred = imbalanced_pipe_failure.predict(input_data)[0]\n        prob   = imbalanced_pipe_failure.predict_proba(input_data)[0]\n        print(f'Decision = {failure_type[y_pred]} with probability = {prob[y_pred]}')","8adb9518":"print(f'Consider we have data: {np.array([[309.1 ,4.6,143]])}; the information of one product.')\nMPM_model_decision(np.array([[309.1, 4.6, 143]]))","a2b1a65e":"print(f'Consider we have data: {np.array([[308.0,42.8,0.0]])}; the information of one product.')\nMPM_model_decision(np.array([[308.0,42.8,0.0]]))","752c7dc0":"For cross validation result, we see rfc have score greater than bagging, rus and easy. ","7333f61e":"### Evaluation","78fa18ae":"### Relation plotting","172f6ae4":"# UpNext\nNext work, we will see how we are going to improve the two models.","acc9f4f8":"Test G-mean is less than training G-mean. I think that dataset for failure are too small. ","ce0210cf":"# Data mining technique\n\nThe objective is to Simulate dataset to predict machine failure and its type. To do so, we select feature \nby using variance inflation factor (vif). The VIF is a measure of colinearity among predictor variables within a multiple regression. A high VIF indicates that the associated indepedent variable is highly collinear with other variables.\n","757ce308":"the vif factor of all features indicates that each feature are very related. Then we cannot take this.\nWe select ","f2187b40":"Let's boxplot our failure data to see how it is. ","6e3aad3d":"### Machine Failure: Descriptive analysis and visualization ","59a67005":"- Machine fails if 3500 W < power < 9000 W (the outlier values gets a power failure).\n- Machine fails if oversstrain > 11,000 minNm (the outlier values gets a overstrain failure).\n- Machine fails if Heat dissipation < 11,868 rpmK (intlier values get heat dissipation failure).","accbe015":"# Machine Predictive Maintenance (MPM)\n\n**Maintenance** it is  functional checks, servicing, repairing or replacing of necessary devices, equipment, machinery, building infrastructure, and supporting utilities in industrial, business, and residential installations.\n\nThe different type of maintenance are\n- Preventive maintenance, corrective maintenance and predictive maintenance.\n\n**Predictive maintenance** use sensor data to monitor a system, then continuously evaluates it against historical trends to predict failure before it occurs.\n\n**Preventive maintenance** which consists of intervening on a piece of equipment before it is faulty, in order to try to prevent any breakdown\n\n**Corrective maintenance** which consists of intervening on a piece of equipment when it is faulty.\n\nIn this notebook, we interest a predictive maintenance. We are going to build a machine learning model.\n\nTo do so, our work are subdivided as follows\n\n- [Exploratory data analysis](#eda)\n- [Data mining process](#dm)\n\nLet's go.","3bff8290":"To find a good learner in this cases, we take G-mean as a measure.","27d83e07":"Product type L are mojorities in the datatsets.","75de2242":"### Evaluation","8206932b":"## Simple visualization","4d94429a":"## Relation between feature ","45fbf1e8":"### Training","80443350":"### Cross validation","b8a6f670":"If we consider only a condition to get power failure, overstrain failure and heat dissipation failure, our machine maintenance can be determined due to random phenomena maintenance is non-determined.","28a3e629":"Some feature are correlated with other feature. Power and Torque have  opposite trends. Overstrain an tool wear have same trends.","15e8912f":"Air temperature and Process temperature are correlated due to $\\Delta T$ = Air temperature - Process temperature.\n\nrotational speed and torque are also correlated. power = torque x rotational speed  ","45b4603e":"Now, we see that the vif factor of these three features is equal to 1 this means that all feature are independant.","056da0e2":"In failure data, Torque contains again outlier values.  ","5b8c8410":"Only rotational speed and torque have outlier. Some product have high rotational speed and torque than other.","94d30292":"# Data preparation","f76aaa5e":"roc_auc test is greater than roc_auc training. Our model does not overfit. That's is good. We can improve it. ","8c45be8b":"**Summary**\n- Majorities of the external datapoints in each chart indicates that machine are failure.\n- With a condition of different failure type, decision for maintaining machine is random.\n- Target are very unbalanced: 96.61% of the datasets are no failure and 3.39% of the datasets are failure.  \n- New feature describes well a datasets.","43ac7782":"We can said that Balanced RandomForest Classifier is the good learner to predict Machine Failure.","c6ce6156":"vif factor is a diagonal of the inverse matrix correlation.","0bd04db5":"Our classification report imbalanced is not bad.","0758d08a":"## Feature engineering\n\nWe create another feature such that **Power [W]**, **Overstrain [minNm]**, **Heat dissipation [rpminK]**.","a4833f4c":"## Build model with ensemble of sampler to predict Machine Failure.\n\nIn this part, our measure are f1-score, G-mean, roc_auc, precision and recall. We know that our dataseet are imbalanced. ","d1087465":"## Descriptive analysis","5db7f740":"#### Find best parameters.\n","8ec53c8d":"Target are very unbalanced. 96.61% of the datasets are no failure and 3.39% of the datasets are failure. ","d0277182":"# Build model to predict Machine Failure Type.\n\nHere, we consider model1 give Failure as respond now, we need to know which Failure Type it is.\nWe are selected only datapoint where Machine type is Failure.","312ee654":"### Find best parameters","2a6d38f6":"Classification report imbalanced for test dataset give some good result. Nice.","a0d90eca":"# Put all together.\n\nNow, we have two models: **model I for Machine Failure prediction** and **model II for Failure Type prediction**.","bad1206e":"**Be free to share, comment and download.**","e26a2ff2":"This training result show that Balanced RandomForest Classifier give good roc_auc score than other learners.","f1aa9f22":"Machine generates more heat dissipation losses more power.","e1432462":"# Modeling\n\nIn this part we are build model to predict Failure and Failure Type. Our strategy is to find model that predict Failure after we find another model which predict a Failure type if Machine Failure == Failure.  ","b97188a9":"Air temperature and Process temperature have good obsevation i.e no outlier exist.","ac3b1239":"### Training","6b8604e8":"We choose again BalancedRandomForest classifier","94af2d84":"Let's check correlation","148a3763":"# Exploratory data analysis\n\nHere, we divide data in five independent failure modes (tool wear failure, heat dissipation failure, power failure, overstrain failure, random failure). We describe each failures modes.\n- **tool wear failure (TWF)**: the tool will be replaced of fail at a randomly selected tool wear time between 200 and 240 mins (120 times in our dataset). At this point in time, the tool is replaced 69 times, and fails 51 times (randomly assigned).\n- **heat dissipation failure (HDF)**: heat dissipation causes a process failure, if the difference between air- and process temperature is below 8.6 K and the tool\u00e2\u20ac\u2122s rotational speed is below 1380 rpm. This is the case for 115 data points.\n- **power failure (PWF)**: the product of torque and rotational speed (in rad\/s) equals the power required for the process. If this power is below 3500 W or above 9000 W, the process fails, which is the case 95 times in our dataset.\n- **overstrain failure (OSF)**: if the product of tool wear and torque exceeds 11,000 minNm for the L product variant (12,000 M, 13,000 H), the process fails due to overstrain. This is true for 98 datapoints.\n- **random failures (RNF)**: each process has a chance of 0,1 % to fail regardless of its process parameters. This is the case for only 5 datapoints, less than could be expected for 10,000 datapoints in our dataset.\n\nWhat follows, we are going to see how many process fails.","72673821":"Now we start realtion plotting."}}