{"cell_type":{"140e0854":"code","e2cb01f3":"code","aaf94cd5":"code","c1b77cf0":"code","ce7721a7":"code","f7cf983f":"code","f057ca06":"code","604fa7e5":"code","198f5a6d":"code","f728b88c":"code","80233667":"code","2576a217":"code","0dcb9c23":"code","586c40d4":"code","61f78de9":"code","d57601b1":"code","68d87c8c":"code","707c1102":"code","41a1fb5f":"code","e57ee967":"code","7ce7510f":"code","3f260294":"code","8182595b":"code","a842dd7e":"code","a834d1bb":"code","4131674f":"code","4de31c1c":"code","b8fd6079":"code","80340086":"code","2b897ab6":"code","318999f6":"code","bcc9cfbc":"code","517aa7b8":"code","5a0e6dee":"code","3abd7f33":"code","cb8d2d90":"code","bb47d9c5":"code","bf2ceb7d":"code","3de5777c":"code","8c1313c1":"code","8948eb30":"code","f54ba67d":"code","ba42afe3":"code","4d3ae244":"code","787e6bc5":"code","eafb8b20":"code","a2a567de":"code","8b7d4dd9":"code","282ca080":"code","25b8ddea":"code","378e5cb9":"code","53f78a3d":"markdown","12e559d1":"markdown","f99e4c0e":"markdown","edae11fe":"markdown","3f52bb60":"markdown","14a2a00e":"markdown","9d65aba4":"markdown","edbbd9e5":"markdown","63f40ced":"markdown","58687675":"markdown","adbf0aee":"markdown","0e030f56":"markdown","86d5ed97":"markdown","a15eb74f":"markdown","8d518d8d":"markdown","04ce3a7b":"markdown","b6269072":"markdown","55f1759a":"markdown","f0efb942":"markdown","ff1ccede":"markdown","b3981d42":"markdown","d8d9f05c":"markdown","303d0dda":"markdown","d458968f":"markdown","0c848392":"markdown","bf5de0ce":"markdown","be7a9d5d":"markdown","4775b740":"markdown","853be124":"markdown","51fbe952":"markdown","cb35903a":"markdown","eb7a2bf5":"markdown","029ef02f":"markdown","0bdf5163":"markdown","4d6ce97e":"markdown","b74bab02":"markdown","71c9b533":"markdown","84da9c41":"markdown","e5cdb905":"markdown","1be507de":"markdown","4d1fe6bc":"markdown","35f89318":"markdown","6cc29c30":"markdown","28d0d48c":"markdown","04137d92":"markdown","d08be7de":"markdown","29bb02a5":"markdown","d884a7be":"markdown","978cec4f":"markdown","73c0d0a4":"markdown","f3cd8b92":"markdown","b8aa8331":"markdown","c9e5e57b":"markdown","c4d00931":"markdown","219b944d":"markdown"},"source":{"140e0854":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, r2_score, median_absolute_error, max_error, explained_variance_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, plot_importance \n\nimport lightgbm as lgb\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout\nfrom keras.optimizers import RMSprop \nfrom keras.callbacks import EarlyStopping \nimport keras\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport threading\nimport re\nimport os\n\nfiles = {}\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files[filename] = os.path.join(dirname, filename)\n\nTrainData =  pd.read_csv(files['train.csv'])\nTestData =  pd.read_csv(files['test.csv'])\n\nsns.set()","e2cb01f3":"traind = TrainData.copy()\ntestd = TestData.copy()\ntd_merged = traind.append(testd, ignore_index=True)\ntd_merged.info() ","aaf94cd5":"# Retrieve title from Name, we will use it as a feature\nfor index, person in td_merged.iterrows():\n    found = re.match(r\".+ (.+\\.).*\", person.Name)\n    if found:\n        td_merged.loc[index, 'Title'] = found.group(1)","c1b77cf0":"td_filtered = td_merged.drop(['PassengerId','Name',],axis=1)","ce7721a7":"# According to publicly available report, those 2 ladies embarked in Southampton\ntd_filtered[['Embarked']] = td_filtered[['Embarked']].fillna('S')","f7cf983f":"# There's only 1 missing fare for a senior citezen, we'll use a median fare\n# among other people on board of the same age\nmissing_fare = float(td_filtered[['Pclass','Age','Fare']]\n                     [(td_filtered.Age > 50) & (td_filtered.Pclass == 3)]\n                     .groupby(['Pclass']).median().Fare)\ntd_filtered[['Fare']] = td_filtered[['Fare']].fillna(missing_fare)","f057ca06":"td_filtered.Cabin = td_filtered.Cabin[td_filtered.Cabin.notnull()].apply(lambda c: c[0])","604fa7e5":"pd.options.mode.chained_assignment = None\nmasters = td_filtered[(td_filtered.Title == 'Master.')]\nmasters_without_age = masters[(masters.Age.isnull())]\nmaster_mean_age = masters[(masters.Age.notnull())]['Age'].mean()\nmasters_without_age['Age'] = master_mean_age\ntd_filtered.update(masters_without_age)","198f5a6d":"td_filtered.groupby('Title').size()","f728b88c":"td_filtered.Title[(td_filtered.Title.isin(['Countess.', 'Jonkheer.', 'Sir.', 'Dame.', 'L.', 'Lady.', 'Don.', 'Dona.']))] = 'High rank'","80233667":"td_filtered.Title[(td_filtered.Title.isin(['Major.', 'Col.', 'Capt.']))] = 'Military'","2576a217":"td_filtered.Title[(td_filtered.Title.isin(['Rev.']))] = 'Religion'","0dcb9c23":"td_filtered.Title[(td_filtered.Title.isin(['Dr.']))] = 'Academic'","586c40d4":"td_filtered.Title[(td_filtered.Title.isin(['Mr.','Ms.']))] = 'Unknown'","61f78de9":"td_filtered.Title[(td_filtered.Title.isin(['Miss.','Mlle.']))] = 'Unmarried'","d57601b1":"td_filtered.Title[(td_filtered.Title.isin(['Mrs.','Mme.']))] = 'Married'","68d87c8c":"td_filtered.Title[(td_filtered.Title.isin(['Master.'])) | (td_filtered.Age < 18)] = 'Child'\n","707c1102":"td_filtered.groupby('Title').size()","41a1fb5f":"td_family_size = td_filtered.copy()\ntd_family_size['FamilySize'] = (td_filtered.SibSp + td_filtered.Parch + 1)\ntd_filtered['FamilyCategory'] = pd.cut(td_family_size.FamilySize,[0,1,2,3,4],labels=['Single','Small','Medium','Large'],duplicates='drop')\ntd_filtered.groupby(['FamilyCategory','Pclass']).size().unstack()","e57ee967":"categorical_features = ['Sex','Cabin','Embarked', 'Title','FamilyCategory','Ticket']\nencoders = dict()\nfor feature in categorical_features:\n    enc = LabelEncoder()\n    encoders[feature] = enc\n    td_filtered.loc[:,feature] = enc.fit_transform(td_filtered[feature].astype(str))","7ce7510f":"pd.set_option('precision',2)\nplt.figure(figsize=(10, 8))\nsns.heatmap(td_filtered.corr())\nplt.suptitle(\"Pearson Correlation Heatmap\")\nplt.show();\ntd_filtered.corr()","3f260294":"ax = sns.catplot(x='Sex', y='Age', hue='Survived', data=td_filtered, kind=\"box\", col='Pclass',\n               palette=['lightpink','lightgreen'])","8182595b":"td_with_age = td_filtered[(~td_filtered.Age.isnull())]\ntd_without_age = td_filtered[(td_filtered.Age.isnull())].drop(['Age'], axis=1)\n\nX = td_with_age.drop(['Age'], axis=1)\nY = td_with_age.Age\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=10)\n","a842dd7e":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': {'l2', 'l1'},\n    'learning_rate': 0.05,\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 5,\n    'num_threads': threading.active_count(),\n}\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nmodelLGB = lgb.train(params,\n                lgb_train,\n                num_boost_round=50,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=10, verbose_eval=0)\n\npredictions = modelLGB.predict(X_test, num_iteration=modelLGB.best_iteration)\n\nprint('R2 Score (best is 1.0): %s' % r2_score(y_test.to_numpy(), predictions))\nprint('MedAE (the smaller the better): %s' % median_absolute_error(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('Max Error: %s' % max_error(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('ExpVar score (best is 1.0): %s' % explained_variance_score(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('RMSE:', mean_squared_error(y_test, predictions) ** 0.5)\n\nfeature_importance = pd.DataFrame()\nfeature_importance['Score'] = modelLGB.feature_importance()\nfeature_importance['Feature'] = modelLGB.feature_name()\nfeature_importance = feature_importance.sort_values(by='Score',ascending=False)\n\ntemp = pd.DataFrame()\ntemp['Actual'] = y_test\ntemp['Predicted'] = predictions\n\nax = sns.regplot(x='Actual',y='Predicted',data=temp)","a834d1bb":"ax = sns.barplot(x='Score',y='Feature',data=feature_importance, palette=\"Purples_d\", orient='h')","4131674f":"modelXGB = XGBRegressor(n_estimators=100)\nmodelXGB.fit(X_train, y_train, early_stopping_rounds=10, \n             eval_set=[(X_test, y_test)], verbose=False)\npredictions = modelXGB.predict(X_test)\nprint('R2 Score (best is 1.0): %s' % r2_score(y_test.to_numpy(), predictions))\nprint('MedAE (the smaller the better): %s' % median_absolute_error(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('Max Error: %s' % max_error(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('ExpVar score (best is 1.0): %s' % explained_variance_score(y_true=y_test.to_numpy(),y_pred=predictions))\nprint('RMSE:', mean_squared_error(y_test, predictions) ** 0.5)\n\ntemp = pd.DataFrame()\ntemp['Actual'] = y_test\ntemp['Predicted'] = predictions\nsns.regplot(x='Actual',y='Predicted',data=temp)\n\nplot_importance(modelXGB)\n\nplt.show()","4de31c1c":"predictions = modelXGB.predict(td_without_age)\ntd_age_restored = td_without_age.copy()\ntd_age_restored.loc[:,'Age'] = predictions\ntd_filtered.update(td_with_age.append(td_age_restored))\ntd_filtered.info()","b8fd6079":"td_with_cabin = td_filtered[(td_filtered.Cabin.notna())].drop(['Survived'], axis=1)\ntd_without_cabin = td_filtered[(td_filtered.Cabin.isna())].drop(['Cabin','Survived'], axis=1)\n\nX = td_with_cabin.drop(['Cabin'], axis=1)\n\ninput_features = X.columns.values\nn_input_features = len(input_features)\nn_output_feature = len(td_with_cabin.Cabin.unique())\n\nY = td_with_cabin.Cabin\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=10)","80340086":"modelNN = Sequential([\n    Dense(n_input_features, input_dim=n_input_features, activation='relu'),\n    Dense(n_input_features * 4, activation='relu'),\n    Dense(n_input_features * 4, activation='relu'),\n    Dropout(0.1),\n    Dense(n_output_feature, activation='softmax'),\n])\nmodelNN.compile(\n    loss='sparse_categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodelNN.summary()\nmodelNN.fit(\n    X_train, y_train,\n    epochs=30,\n    validation_data=(X_test, y_test),\n    verbose = 1\n)\nloss, accuracy = modelNN.evaluate(X_test, y_test)\nprint(\"Accuracy: %s\" % accuracy)\nprint(\"Loss: %s\" % loss)","2b897ab6":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\nparams = { \n    'objective': 'multi:softmax',\n    'learning_rate': 0.3,\n    'max_depth': 6,\n    'eta': 0.1,\n    'nthread': threading.active_count(),\n    'num_class': n_output_feature,\n}\nmodelXGB = xgb.train(params=params,dtrain=dtrain,num_boost_round=100)\npredictions = modelXGB.predict(dtest)","318999f6":"print('Accuracy: %s' % accuracy_score(y_test, predictions))\nprint('Error rate: %s' % (np.sum(predictions != y_test) \/ y_test.shape[0]))\nprint('MSE: %s' % mean_squared_error(y_test, predictions))\nprint('MedAE: %s' % median_absolute_error(y_true=y_test,y_pred=predictions))\nprint('Max Error: %s' % max_error(y_true=y_test,y_pred=predictions))\nprint('ExpVar score: %s' % explained_variance_score(y_true=y_test,y_pred=predictions))","bcc9cfbc":"predictions = modelXGB.predict(xgb.DMatrix(td_without_cabin))\nmissing_cabins = np.array(predictions,dtype=int)\ntd_cabins_restored = td_without_cabin.copy()\ntd_cabins_restored.loc[:,'Cabin'] = missing_cabins\ntd_filtered.update(td_with_cabin.append(td_cabins_restored))\ntd_filtered['Survived'] = TrainData.Survived\ntd_filtered.info()","517aa7b8":"training_data = td_filtered[(~td_filtered.Survived.isnull())].drop(['Fare'],axis=1)\ntesting_data = td_filtered[(td_filtered.Survived.isnull())].drop(['Survived','Fare'],axis=1)\n\nX = training_data.drop(['Survived'],axis=1)\nY = training_data.Survived\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=10)","5a0e6dee":"param = {\n    'objective': 'binary',\n    'learning_rate': 0.05,\n    'feature_fraction': 1,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 5,\n    'num_threads': threading.active_count(),\n}\nparam['metric'] = ['auc', 'binary_logloss']\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\nmodelLGB = lgb.train(param, lgb_train, num_boost_round=50, valid_sets=lgb_eval, early_stopping_rounds=10)","3abd7f33":"print('Accuracy: %s' % modelLGB.best_score['valid_0']['auc'])\nprint('Binary logloss: %s' % modelLGB.best_score['valid_0']['binary_logloss'])\n\nfeature_importance = pd.DataFrame()\nfeature_importance['Score'] = modelLGB.feature_importance()\nfeature_importance['Feature'] = modelLGB.feature_name()\nfeature_importance = feature_importance.sort_values(by='Score',ascending=False)\n\nax = sns.barplot(x='Score',y='Feature',data=feature_importance, palette=\"Purples_d\", orient='h')","cb8d2d90":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\nparams = { \n    'objective': 'binary:logistic',\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'eta': 0.01,\n    'nthread': threading.active_count(),\n}\nmodelXGB = xgb.train(params=params,dtrain=dtrain,num_boost_round=40)\npredictions = modelXGB.predict(dtest)","bb47d9c5":"print('Accuracy: %s' % accuracy_score(y_test, predictions.round()))\nprint('MSE: %s' % mean_squared_error(y_test, predictions.round()))","bf2ceb7d":"xgb.to_graphviz(modelXGB)","3de5777c":"plot_importance(modelXGB)\nplt.show()","8c1313c1":"modelKNN = KNeighborsClassifier(n_neighbors=10)\nmodelKNN.fit(X_train, y_train)\npredictions = modelKNN.predict(X_test)","8948eb30":"print('Accuracy: %s' % accuracy_score(y_test, predictions.round()))\nprint('MSE: %s' % mean_squared_error(y_test, predictions.round()))","f54ba67d":"neurons=len(testing_data.columns.values)\nmodelNN = Sequential([\n    Dense(neurons * 8, input_dim=neurons, activation='relu'),\n    Dense(neurons * 3, activation='relu'),\n    Dense(neurons * 1, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\nmodelNN.compile(\n    loss='mse',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nmodelNN.fit(\n    X_train, y_train,\n    epochs=5, batch_size=10,\n    validation_data=(X_test, y_test),\n    verbose = 1\n)","ba42afe3":"loss, accuracy = modelNN.evaluate(X_test, y_test)\nprint(\"Accuracy: %s\" % accuracy)\nprint(\"Loss: %s\" % loss)","4d3ae244":"modelCNN = Sequential([\n    Conv1D(filters=neurons, kernel_size=2, input_shape=(neurons,1)),\n    MaxPooling1D(pool_size=2),\n    Dropout(0.1),\n    Flatten(),\n    Dense(neurons * 2, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\nmodelCNN.compile(\n   loss = 'mse',\n   optimizer = 'adam',\n   metrics = ['accuracy']\n)\n\nX_train_reshaped = X_train.to_numpy().reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test_reshaped = X_test.to_numpy().reshape(X_test.shape[0], X_test.shape[1], 1)\n\nmodelCNN.fit(\n    X_train_reshaped, y_train,\n    epochs=5,\n    validation_data=(X_test_reshaped, y_test),\n    verbose = 1\n)","787e6bc5":"loss, accuracy = modelCNN.evaluate(X_test_reshaped, y_test)\nprint(\"Accuracy: %s\" % accuracy)\nprint(\"Loss: %s\" % loss)","eafb8b20":"TestDataReshaped = testing_data.to_numpy().reshape(testing_data.shape[0],testing_data.shape[1],1)\nOutputCNN = testing_data.copy()\nOutputCNN[\"Survived\"] = modelCNN.predict_classes(TestDataReshaped)\nOutputCNN['PassengerId'] = td_merged.PassengerId.astype(int)","a2a567de":"OutputNN = testing_data.copy()\nOutputNN[\"Survived\"] = modelNN.predict_classes(testing_data.to_numpy())\nOutputNN['PassengerId'] = td_merged.PassengerId.astype(int)","8b7d4dd9":"OutputXGB = testing_data.copy()\nOutputXGB[\"Survived\"] = np.array(\n    modelXGB.predict(\n        xgb.DMatrix(testing_data.to_numpy(), feature_names=testing_data.columns.values)\n    ).round()\n    ,dtype=int)\nOutputXGB['PassengerId'] = td_merged.PassengerId.astype(int)","282ca080":"OutputKNN = testing_data.copy()\nOutputKNN[\"Survived\"] = modelKNN.predict(testing_data.to_numpy())\nOutputKNN.Survived = OutputKNN.Survived.astype(int)\nOutputKNN['PassengerId'] = td_merged.PassengerId.astype(int)","25b8ddea":"OutputLGB = testing_data.copy()\nOutputLGB[\"Survived\"] = np.array(modelLGB.predict(testing_data.to_numpy(), num_iteration=modelLGB.best_iteration).round(),dtype=int)\nOutputLGB['PassengerId'] = td_merged.PassengerId.astype(int)","378e5cb9":"OutputLGB[['PassengerId', 'Survived']].to_csv('LGB.csv', index=False)\nOutputXGB[['PassengerId', 'Survived']].to_csv('XGB.csv', index=False)\nOutputCNN[['PassengerId', 'Survived']].to_csv('CNN.csv', index=False)\nOutputNN[['PassengerId', 'Survived']].to_csv('NN.csv', index=False)\nOutputKNN[['PassengerId', 'Survived']].to_csv('KNN.csv', index=False)","53f78a3d":"The result produced by the model is somewhat not satisfying at all for this task!","12e559d1":"### Use `Cabin`'s first letter as sector identifier on the ship","f99e4c0e":"##### Model overview\n\nNot the best result, indeed. This test was done for the sake of experiment. This domain of a problem is not suitable for CNN as the data set isn't large enough, so the model ended up underfitted.","edae11fe":"Let's see what we do have now after grouping:","3f52bb60":"### Classification of `Survived` passengers","14a2a00e":"##### Model overview","9d65aba4":"#### Child\n\n- *Master.* belongs to boys (assuming under 18)\n- We will assign to all kids under 18 a title `Child`","edbbd9e5":"# Feature engineering","63f40ced":"## Feature recovery","58687675":"# Tools we would need","adbf0aee":"##### Model overview\n\nWe achieved quite a good result:\n","0e030f56":"# The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n# Abstract\n\nUse various techniques of Feature Engineering, Classification and Regression algorithms to achieve more accurate predictions.\n\nWe will be utilising ensemble learning by applying several algorithms interchangeably.\n\nThe best score achived on Kaggle using techniques below is `79.904%`.","86d5ed97":"### Fill missing `Fare` value","a15eb74f":"### Regression analysis of `Age`\n\nFor this model we are going to drop `Ticket` feature here, as it reduces error rate of prediction.","8d518d8d":"#### Gradient boosting with LightGBM","04ce3a7b":"### Closer look on `Gender`, `Age` and `Class` ","b6269072":"#### Married woman\n\n- *Mrs.* is a title of married or widowed woman in English\n\n- *Mme.* stands for Madame in French","55f1759a":"### Restore age by title\n\nIt's a fact that title `Master` belongs to boys.\n\n>The abbreviation Mr. has been in use since the fifteenth century, it is a variant of the word master. Master is still occasionally used as a title for a boy, there is no abbreviation.\n\nAt this point we are going to assign an average kid age to those kids with absent `Age`.","f0efb942":"### Feed testing data set","ff1ccede":"### Aggregate redundant titles\n\nLet's see how many titles we have, and what meaning each of them connotate to understand what can be grouped under the same category.","b3981d42":"Now we have all of our missing values recovered except the only one `Survived`. \n\nLet's jump to our final goal -- guess who survived and who did not.","d8d9f05c":"#### Unmarried woman\n\n- *Miss.* in English\n- *Mlle.* stands for Mademoiselle in French","303d0dda":"#### Unknown marital status\n\n- *Mr.*\/*Ms.* in English\n","d458968f":"#### Binary logistic regression with `XGBoost`\n\nDespite the name of the algorithm suggests the word \"regression\" we're going to perform classification underneath.\n\nIn order to answer to the question like \"Did this passenger survive the disaster?\" -- the possible answer can contain only 2 possible outcomes: `True` or `False`.\n\nLet's see what rate of accuracy we can achieve with this model.","0c848392":"### Classification of missing `Cabin` (sector of the ship) values","bf5de0ce":"### Retrieve Title from Name","be7a9d5d":"#### Populating missing `Age` values with results of our prediction","4775b740":"#### Trying to classify with XGBoost","853be124":"## Interpolation of missing values\n\nOne can notice that in traing and testing data sets combined there are:\n- **1014** passengers with missing `Cabin`\n- **418** passengers with missing `Survived` value\n- **263** passengers with missing `Age`\n- **2** passengers with missing `Embarked` port\n- **1** record with missing `Fare` value\n\nWe will try to approximate those values later.","51fbe952":"#### Convolutional Neural Network with 1x1 dimension (`Keras` & `TF`)","cb35903a":"#### Gradient boosting with XGBoost","eb7a2bf5":"##### Model overview","029ef02f":"#### KNN - K Nearest Neighbours with `scikit-learn`","0bdf5163":"#### Military\n\n- *Major.*, *Col.*, *Capt.*","4d6ce97e":"# Titanic: Machine Learning from Disaster","b74bab02":"##### Model overview\n\nExcellent result - 95% of accuracy!","71c9b533":"#### Predicting with multiple layer Neural Networks (Keras & TF)","84da9c41":"### Save predictions to file","e5cdb905":"From age and gender insights we can observe that passengers of the First class got the best survival rate followed by the Second class and the Third as the last one. Probably because passengers of the 3rd class ended up to be locked in their cabins, that might have been done to reduce sinking rate of the ship so that more lifes could have be saved.","1be507de":"## Feature relationship analysis\n\nIn this step we will discover how different features correlate with each other and prepare a set of important variables to feed to our predictive models later.\n\n### Heatmap","4d1fe6bc":"### Setting family category","35f89318":"#### Religion\n\n- *Rev.* stands for the Reverend, a Christian cleric such as an pastor or priest. \n","6cc29c30":"#### Neural network with multiple layers (`Keras` & `TF`)","28d0d48c":"#### Related to a royalty or possession of a high rank\n\n- _Earl._ and _Countess._ are in the third degree of the U.K. Peerage System known as British nobility (e.g. Prince Edward, Earl of Wessex, of his wife Countess of Wessex).\n\n- _Jonkheer._ of person related to the Dutch Nobility System\n\n- _Sir._\/_Dame._ of person knighted by the Queen.\n\n- *L.*(Lord)\/_Lady_., _Don._\/_Dona._ - in the U.K., Barons, viscounts, earls, marquesses and their female counterparts can all be referred to as lord or lady instead of their full title, as can their children.\n","04137d92":"##### Decision tree","d08be7de":"#### Academic\n\n- *Dr.* of a person who has obtained a doctorate (e.g. PhD).\n","29bb02a5":"Apart from `Name`, we are also dropping `PassengerId` to avoid our models allocating excessive weights to irrelevant features.","d884a7be":"#### Populating predicted values","978cec4f":"##### Model overview","73c0d0a4":"Looks like **XGBoost** does a better job at regression. \n\nWe will go with the results produced by **XGBoost** in this case as the predictions indicates slightly lower error rates comparing to **LightGBM**","f3cd8b92":"### Fill missing `Embarked` values","b8aa8331":"#### Binary logistic regression with `LightGBM`","c9e5e57b":"##### Model overview\n\nQuite a surprise -- KNN and Gradient Boosting ended up predicting with the same rate of accuracy!","c4d00931":"## Categorical feature encoding\n* `Sex` - we have discovered only 2 possible genders in the data: _male_ and _female_\n* `Ticket` - some tickets are reoccurring, perhaps shared among passengers tagging along in group (family members, tourists, friends, etc.)\n* `Cabin` - Some cabins are also shared among passengers\n* `Embarked` - this feature has 3 ports, some of the passengers (2 ladies from _Southampton_ ) have this field missing, we will manually populate those entries, as this information is publically available\n* `Title` - We will extract title (_Mr._ , _Ms._ , _Mrs._ , etc.) from the name. Perhaps, it's worth to have a look at surname, and play around with that feature to predict survival rate of particular family members but for now we will drop it and use title only.\n* `FamilyCategory` - _Single_, _Small_, _Medium_ and _Large_ accordingly with its size","219b944d":"##### Feature importance\n\n- From my point of view it doesn't look so obvious why our model picked up `Ticket` as the most important feature. My guess here would be that passengers in group has the best chance to escape the sinking ship. But strange that eliminating this feature gives worse accuracy score. So we will keep it as it is.\n\n- Secondly, looks like `Age` contributes heavily to survival probability after whether the passenger was in group or alone. This drives to conclusion that passengers of particular age group have higher chances to survive.\n\n- `Fare` and `Cabin` sector are the next deciders in this game of dice.\n\n- Class and gender don't play so crucial role as it might appeared in the beginning.\n\n- Apperantly a particular `Title` or whether the passenger is a parent or children (`Parch`) contribute very little to importance"}}