{"cell_type":{"01d147f7":"code","4629fe13":"code","12f08a71":"code","5b1a5a92":"code","4f471c19":"code","503b012f":"code","3fec3964":"code","20e41c40":"code","e00917ba":"code","721d61ff":"code","a78a54c3":"code","36c92f62":"code","8bc85c4f":"code","8b4e4e58":"code","80d392b0":"code","d8b2993f":"code","9f83e8f6":"code","5c09e902":"code","3bd5797c":"code","7d8b49f5":"code","9fceca2b":"code","b18f340b":"code","84ca7e9f":"markdown","4b2a869d":"markdown","b461b242":"markdown","092c61b2":"markdown","d5997c1c":"markdown","df58130c":"markdown","ee87b77c":"markdown"},"source":{"01d147f7":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport time\nimport cv2\nimport PIL.Image\nimport random\nfrom sklearn.metrics import accuracy_score\nfrom tqdm.notebook import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport albumentations\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nfrom sklearn.metrics import roc_auc_score\n%matplotlib inline\nimport seaborn as sns\nfrom pylab import rcParams\nfrom warnings import filterwarnings\nfrom sklearn.preprocessing import LabelEncoder\nimport math\nimport glob\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfilterwarnings(\"ignore\")\nimport torchvision.models as models\n\n\ndevice = torch.device('cuda')","4629fe13":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False # set True to be faster\n    print(f'Setting all seeds to be {seed} to reproduce...')\nseed_everything(42)","12f08a71":"image_size = 224\nbatch_size = 16\nn_worker = 4\ninit_lr = 3e-4\nn_epochs = 6 # from my experiments, use > 25 when margin = 0.5\nfold_id = 0\nholdout_id = 0\nvalid_every = 5\nsave_after = 10\nmargin = 0.5 # 0 for faster convergence, larger may be beneficial\nsearch_space = np.arange(40, 100, 10) # in my experiments, thresholds should be between 40 - 90 (\/100) for cosine similarity\nuse_amp = False # todo: figure how to work with pytorch native amp\ndebug = True # set this to False to train in full\nkernel_type = 'baseline'\nmodel_dir = '.\/weights\/'\ndata_dir = '..\/input\/shopee-product-matching\/train_images'\n# data_dir = '..\/input\/train_images'\n\n! mkdir $model_dir","5b1a5a92":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_train['file_path'] = df_train.image.apply(lambda x: os.path.join(data_dir, x))\ndf_train.head(5)","4f471c19":"gkf = GroupKFold(n_splits=5)\ndf_train['fold'] = -1\nfor fold, (train_idx, valid_idx) in enumerate(gkf.split(df_train, None, df_train.label_group)):\n    df_train.loc[valid_idx, 'fold'] = fold","503b012f":"le = LabelEncoder()\ndf_train.label_group = le.fit_transform(df_train.label_group)","3fec3964":"transforms_train = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2)),\n    albumentations.HueSaturationValue(p=0.5, hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2),\n    albumentations.ShiftScaleRotate(p=0.5, shift_limit=0.0625, scale_limit=0.2, rotate_limit=20),\n    albumentations.CoarseDropout(p=0.5),\n    albumentations.Normalize()\n])\n\ntransforms_valid = albumentations.Compose([\n    albumentations.Resize(image_size, image_size),\n    albumentations.Normalize()\n])","20e41c40":"class SHOPEEDataset(Dataset):\n    def __init__(self, df, mode, transform=None):\n        \n        self.df = df.reset_index(drop=True)\n        self.mode = mode\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        row = self.df.loc[index]\n        img = cv2.imread(row.file_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform is not None:\n            res = self.transform(image=img)\n            img = res['image']\n                \n        img = img.astype(np.float32)\n        img = img.transpose(2,0,1)\n        \n        if self.mode == 'test':\n            return torch.tensor(img).float()\n        else:\n            return torch.tensor(img).float(), torch.tensor(row.label_group).float()","e00917ba":"dataset = SHOPEEDataset(df_train, 'train', transform = transforms_train)\nrcParams['figure.figsize'] = 15,5\nfor i in range(2):\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        idx = i*5 + p\n        img, label = dataset[idx]\n        axarr[p].imshow(img.transpose(0,1).transpose(1,2).squeeze())\n        axarr[p].set_title(label.item())","721d61ff":"class ArcModule(nn.Module):\n    def __init__(self, in_features, out_features, s = 10, m = margin):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_normal_(self.weight)\n\n        self.cos_m = math.cos(m)\n        self.sin_m = math.sin(m)\n        self.th = torch.tensor(math.cos(math.pi - m))\n        self.mm = torch.tensor(math.sin(math.pi - m) * m)\n\n    def forward(self, inputs, labels):\n        cos_th = F.linear(inputs, F.normalize(self.weight))\n        cos_th = cos_th.clamp(-1, 1)\n        sin_th = torch.sqrt(1.0 - torch.pow(cos_th, 2))\n        cos_th_m = cos_th * self.cos_m - sin_th * self.sin_m\n        # print(type(cos_th), type(self.th), type(cos_th_m), type(self.mm))\n        cos_th_m = torch.where(cos_th > self.th, cos_th_m, cos_th - self.mm)\n\n        cond_v = cos_th - self.th\n        cond = cond_v <= 0\n        cos_th_m[cond] = (cos_th - self.mm)[cond]\n\n        if labels.dim() == 1:\n            labels = labels.unsqueeze(-1)\n        onehot = torch.zeros(cos_th.size()).cuda()\n        labels = labels.type(torch.LongTensor).cuda()\n        onehot.scatter_(1, labels, 1.0)\n        outputs = onehot * cos_th_m + (1.0 - onehot) * cos_th\n        outputs = outputs * self.s\n        return outputs","a78a54c3":" models.resnet101(True)","36c92f62":"class SHOPEEDenseNet(nn.Module):\n\n    def __init__(self, channel_size, out_feature, dropout=0.5, backbone='densenet121', pretrained=True):\n        super(SHOPEEDenseNet, self).__init__()\n        self.channel_size = channel_size\n        self.out_feature = out_feature\n        \n        if backbone == 'resnet18':\n            self.backbone = models.resnet18(True)\n            self.in_features = self.backbone.fc.in_features\n            self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n            self.fc1 = nn.Linear(self.in_features * 7 * 7 , self.channel_size)\n        if backbone == 'resnet101':\n            self.backbone = models.resnet101(True)\n            self.in_features = self.backbone.fc.in_features\n            self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n            self.fc1 = nn.Linear(self.in_features * 7 * 7 , self.channel_size)\n        print(self.backbone)\n        \n        self.margin = ArcModule(in_features=self.channel_size, out_features = self.out_feature)\n        self.bn1 = nn.BatchNorm2d(self.in_features)\n        self.dropout = nn.Dropout2d(dropout)\n        self.bn2 = nn.BatchNorm1d(self.channel_size)\n        \n    def forward(self, x, labels=None):\n        features = self.backbone(x)\n        features = self.dropout(features)\n        features = features.view(features.size(0), -1)\n        # print(features.shape)\n        features = self.fc1(features)\n        features = F.normalize(features)\n        if labels is not None:\n            return self.margin(features, labels)\n        return features\n    \n    def test(self):\n        x = torch.rand(1, 3, 224, 224).cuda()\n        print(self.forward(x))","8bc85c4f":"model = SHOPEEDenseNet(512, df_train.label_group.nunique(), backbone='resnet101')\nmodel.to(device);","8b4e4e58":"model.test()","80d392b0":"def train_func(train_loader):\n    model.train()\n    bar = tqdm(train_loader)\n    if use_amp:\n        scaler = torch.cuda.amp.GradScaler()\n    losses = []\n    for batch_idx, (images, targets) in enumerate(bar):\n        images, targets = images.to(device), targets.to(device).long()\n        \n        if debug and batch_idx == 100:\n            print('Debug Mode. Only train on first 100 batches.')\n            break\n            \n        if use_amp:\n            with torch.cuda.amp.autocast():\n                logits = model(images, targets)\n                loss = criterion(logits, targets)\n            scaler.scale(loss).backward()\n            if ((batch_idx + 1) %  accumulation_step == 0) or ((batch_idx + 1) == len(train_loader)):\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n        else:\n            logits = model(images, targets)\n            loss = criterion(logits, targets)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n        losses.append(loss.item())\n        smooth_loss = np.mean(losses[-30:])\n\n        bar.set_description(f'loss: {loss.item():.5f}, smth: {smooth_loss:.5f}')\n\n    loss_train = np.mean(losses)\n    return loss_train\n\n\ndef valid_func(valid_loader):\n    model.eval()\n    bar = tqdm(valid_loader)\n\n    PROB = []\n    TARGETS = []\n    losses = []\n    PREDS = []\n\n    with torch.no_grad():\n        for batch_idx, (images, targets) in enumerate(bar):\n            images, targets = images.to(device), targets.to(device).long()\n            logits = model(images, targets)\n\n            PREDS += [torch.argmax(logits, 1).detach().cpu()]\n            TARGETS += [targets.detach().cpu()]\n\n            loss = criterion(logits, targets)\n            losses.append(loss.item())\n           \n            bar.set_description(f'loss: {loss.item():.5f}')\n\n    PREDS = torch.cat(PREDS).cpu().numpy()\n    TARGETS = torch.cat(TARGETS).cpu().numpy()\n    accuracy = (PREDS==TARGETS).mean()\n   \n    loss_valid = np.mean(losses)\n    return loss_valid, accuracy\n\ndef generate_test_features(test_loader):\n    model.eval()\n    bar = tqdm(test_loader)\n    \n    FEAS = []\n    TARGETS = []\n\n    with torch.no_grad():\n        for batch_idx, (images) in enumerate(bar):\n            images = images.to(device)\n            features = model(images)\n            FEAS += [features.detach().cpu()]\n    FEAS = torch.cat(FEAS).cpu().numpy()\n    return FEAS","d8b2993f":"def row_wise_f1_score(labels, preds):\n    scores = []\n    for label, pred in zip(labels, preds):\n        n = len(np.intersect1d(label, pred))\n        score = 2 * n \/ (len(label)+len(pred))\n        scores.append(score)\n    return scores, np.mean(scores)","9f83e8f6":"def find_threshold(df, lower_count_thresh, upper_count_thresh, search_space):\n    '''\n    Compute the optimal threshold for the given count threshold.\n    '''\n    score_by_threshold = []\n    best_score = 0\n    best_threshold = -1\n    for i in tqdm(search_space):\n        sim_thresh = i\/100\n        selection = ((FEAS@FEAS.T) > sim_thresh).cpu().numpy()\n        matches = []\n        oof = []\n        for row in selection:\n            oof.append(df.iloc[row].posting_id.tolist())\n            matches.append(' '.join(df.iloc[row].posting_id.tolist()))\n        tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n        df['target'] = df.label_group.map(tmp)\n        scores, score = row_wise_f1_score(df.target, oof)\n        df['score'] = scores\n        df['oof'] = oof\n        \n        selected_score = df.query(f'count > {lower_count_thresh} and count < {upper_count_thresh}').score.mean()\n        score_by_threshold.append(selected_score)\n        if selected_score > best_score:\n            best_score = selected_score\n            best_threshold = i\n            \n#     plt.title(f'Threshold Finder for count in [{lower_count_thresh},{upper_count_thresh}].')\n#     plt.plot(score_by_threshold)\n#     plt.axis('off')\n#     plt.show()\n    print(f'Best score is {best_score} and best threshold is {best_threshold\/100}')","5c09e902":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr = init_lr)\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.000001, max_lr=0.1)","3bd5797c":"df_train_this = df_train[df_train['fold'] != fold_id]\ndf_valid_this = df_train[df_train['fold'] == fold_id]\n\ndf_valid_this['count'] = df_valid_this.label_group.map(df_valid_this.label_group.value_counts().to_dict())\n\ndataset_train = SHOPEEDataset(df_train_this, 'train', transform = transforms_train)\ndataset_valid = SHOPEEDataset(df_valid_this, 'test', transform = transforms_valid)\n\ntrain_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = n_worker)\nvalid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, num_workers = n_worker)","7d8b49f5":"search_space = np.arange(40, 100, 10)","9fceca2b":"debug = False\nfor epoch in range(n_epochs):\n    scheduler.step()\n    loss_train = train_func(train_loader)\n    if epoch % valid_every == 0: \n        print('Now generating features for the validation set to simulate the submission.')\n        FEAS = generate_test_features(valid_loader)\n        FEAS = torch.tensor(FEAS).cuda()\n        print('Finding Best Threshold in the given search space.')\n        find_threshold(df = df_valid_this, \n               lower_count_thresh = 0, \n               upper_count_thresh = 999,\n               search_space = search_space)\n        if epoch >= save_after:\n            torch.save(model.state_dict(), f'{model_dir}{kernel_type}_fold{fold_id}_densenet_{image_size}_epoch{epoch}.pth')","b18f340b":"np.argsort([0.5, 0.4, 0.8])[-2]","84ca7e9f":"## Train","4b2a869d":"## Transforms","b461b242":"## Model","092c61b2":"## Utils","d5997c1c":"## Configuration","df58130c":"## Dataset","ee87b77c":"## Make Folds"}}