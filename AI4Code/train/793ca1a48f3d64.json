{"cell_type":{"267959f4":"code","463544eb":"code","9fc2dd1e":"code","870502f5":"code","6fc4885c":"code","60e85a25":"code","dadc94af":"code","710d6701":"code","217b79b0":"code","15c8b10b":"code","13590673":"code","9796c0b7":"code","e7efc66b":"code","3b4df23c":"code","5adef228":"code","6750f8c7":"code","f7483e3f":"code","27e23b5d":"code","74a0e45d":"code","be454d21":"code","71513c92":"code","80f5c178":"code","dadfccc2":"markdown","a80eb525":"markdown","306f3170":"markdown","12276117":"markdown","4c8c4f98":"markdown","07115743":"markdown","4b17baf8":"markdown","fc990129":"markdown","fc4871e6":"markdown","0d275dca":"markdown","d6a65f6f":"markdown","e5e49ef4":"markdown","4c349fe1":"markdown","b53a42bf":"markdown","5c1be2a3":"markdown","37aed958":"markdown","44906785":"markdown","d5fb9c90":"markdown","8abda6d6":"markdown","ffb2d0b0":"markdown","0b093f50":"markdown","1769fb4e":"markdown","87cb28ca":"markdown","2eceaca4":"markdown","b34f9f35":"markdown","daa7ab26":"markdown","5111d063":"markdown","0d9e6bcf":"markdown","4f5b72d9":"markdown","8692f26b":"markdown","cd8d55f5":"markdown"},"source":{"267959f4":"#EDA and plotting libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","463544eb":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.head()","9fc2dd1e":"# What do the classes we're trying to predict look like?\nax = df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"blue\", \"orange\"], title=\"Prevalence of Heart Disease in the data set\")\n\ndf[\"target\"].value_counts()","870502f5":"df.info()","6fc4885c":"df.isna().sum()","60e85a25":"df.describe()","dadc94af":"pd.crosstab(df.target, df.sex)\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", color=[\"blue\", \"orange\"], title=\"Prevalence of heart disease in Males vs. Females\")\nplt.ylabel(\"Occurences of heart disease\")\nplt.xlabel(\"0 - No Disease, 1 - Disease\")\nplt.legend([\"Female\", \"Male\"]);","710d6701":"plt.figure(figsize=(10,6))\n\n#Scatter plot with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1], \n           color = [\"blue\"])\nplt.title(\"Maximum Heart Rate Achieved vs. Age of patients (Positive results)\")\n\n#Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           color = [\"Orange\"])\n                    \n                    \n                    \nplt.title(\"Maximum Heart Rate Achieved vs. Age of patients (Negative results)\")\nplt.legend([\"Male\",\"Female\"])\nplt.xlabel(\"Age of patient\")\nplt.ylabel(\"Max heart rate achieved\");","217b79b0":"pd.crosstab(df.cp, df.target)\npd.crosstab(df.target, df.cp).plot(kind=\"bar\", color=[\"blue\", \"orange\", \"Red\", \"Green\"])\nplt.legend([\"Typical Angina\", \"Atypical Angina\", \"Non-Anginal\", \"Asymptomatic\"])\nplt.title(\"Occurences of heart disesase per chest pain type\");","15c8b10b":"corr_matrix = df.corr()\nplt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix, annot=True, fmt=\".2f\")","13590673":"X = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\n#split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n","9796c0b7":"one_hot = OneHotEncoder(handle_unknown='ignore')\ncat_features = [\"cp\", \"thal\"]\ntransformer = ColumnTransformer([(\"one_hot\", one_hot, cat_features)], remainder=\"passthrough\")\nX_train_onehot = pd.DataFrame(transformer.fit_transform(X_train))\nX_test_onehot = pd.DataFrame(transformer.fit_transform(X_test))\n","e7efc66b":"scaler = MinMaxScaler()\nX_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_onehot))\nX_test_scaled = pd.DataFrame(scaler.fit_transform(X_test_onehot))","3b4df23c":"models = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\ndef fit_and_score(models, X_train, X_test, y_train, Y_test):\n    \"\"\"\n    Fit and scores given machine learning models on the given input data\n    \"\"\"\n    \n    #set random seed for reproducibility\n    np.random.seed(42)\n    #make dictionary to keeo scores\n    model_scores = {}\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","5adef228":"model_scores = fit_and_score(models, X_train_scaled, X_test_scaled, y_train, y_test)\nmodel_scores","6750f8c7":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","f7483e3f":"#Tuning KNN\ntrain_scores = []\ntest_scores = []\nnp.random.seed(42)\n#Create a different list for n neighbors\nneighbors = range(1,21)\nknn = KNeighborsClassifier()\n\n#loop over values for neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    knn.fit(X_train_scaled, y_train)\n    \n    train_scores.append(knn.score(X_train_scaled, y_train))\n    \n    test_scores.append(knn.score(X_test_scaled, y_test))\n\nplt.plot(neighbors, train_scores, label = \"Train Score\")\nplt.plot(neighbors, test_scores, label = \"Test Score\")\nplt.legend();\nprint(f\"The max accuracy for KNN is {max(test_scores)*100:.2f}% \")","27e23b5d":"# create a hyperparameter grid for logisticRegression\n\nlog_reg_grid = {\"C\":np.logspace(-4, 4, 20),\n               \"solver\": [\"liblinear\"]}\n\n#create grid for RandomForest\n\nrf_grid = {\"n_estimators\":np.arange(10, 1000, 50),\n          \"max_depth\":[None, 3, 5, 10],\n          \"min_samples_split\":np.arange(2,20,2),\n          \"min_samples_leaf\": np.arange(1,20,2)}","74a0e45d":"#Tune Logistic Regression\n\nnp.random.seed(42)\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions = log_reg_grid,\n                               cv=5, \n                               n_iter=20,\n                               verbose=True)\n#fit hyperparamter search model for Logistic Regression\nrs_log_reg.fit(X_train_scaled, y_train)","be454d21":"rs_log_reg.score(X_test_scaled, y_test)","71513c92":"np.random.seed(42)\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(), \n                          param_distributions=rf_grid, \n                          cv=5,\n                          n_iter=20,\n                          verbose=2)\nrs_rf.fit(X_train_scaled, y_train)\n","80f5c178":"rs_rf.best_params_\nrs_rf.score(X_test_scaled, y_test)","dadfccc2":"### Correlation Matrix","a80eb525":"If a patient in the dataset is female, there is a (72\/24+72) = 75% chance that patient has heart disease. \nFor males (93\/114+93) = 45% chance that a male in the dataset has heart disease.","306f3170":"It looks like there are a lot of patients without heart disease who suffer from Angina, while the most common form of pain for those with heart disease is Non-Anginal Pain","12276117":"Original data from: https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease\n\nData in CSV form from: https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\nThe data has 76 total columns, however only 14 of them are used in published experiments. I will also be using this 14 column subset.\n\n1. age\n2. sex\n3. chest pain type (4 values)\n4. resting blood pressure\n5. serum cholestoral in mg\/dl\n6. fasting blood sugar > 120 mg\/dl\n7. resting electrocardiographic results (values 0,1,2)\n8. maximum heart rate achieved\n9. exercise induced angina\n10. oldpeak = ST depression induced by exercise relative to rest\n11. the slope of the peak exercise ST segment\n12. number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target","4c8c4f98":"By changing the number of neighbors, I'm able to improve the accuracy of the KNN algorithm a bit but not incredibly much. This is largely inefficient hyperparamter tuneing. We'll come back to this in a bit. Let's take a look at the other two models now.","07115743":"# Using RandomizedSearchCV to tune hyperparameters","4b17baf8":"# Hyperparameter tuning","fc990129":"This implies that all the columns are numeric. This is technically true, but some columns are categorically represented by numbers. 'cp' (chest pain) and 'thal' (Thalium stress test result) are categorical columns, and we'll need to encode them as such later on. ***** Categorical Transformation not implemented as of 4\/12\/21 at 11:43 PM, I completely forgot and now I am very tired. Should be fixed by this time on 4\/13\/21****","fc4871e6":"Again, about the same score as before the tuning. This doesn't mean that tuning isn't going to help, just that it hasn't helped in this specific case","0d275dca":"## Age vs Max Heart Rate for Heart Disease","d6a65f6f":"# Now let's actually do some modelling","e5e49ef4":"# EDA","4c349fe1":"# Predicting Heart Disease\nThis notebook uses several Python based Machine Leanrning and data science tools to predict whether a patient has heart disesase. This is not a diagnostic tool.\n\nThe following approach will be used\n1. Problem Definition (see above)\n2. Data Exploration\n3. Evaluation \n4. What features contribute most\n5. Modelling the data\n6. Validation and Improvement","b53a42bf":"Based on the correlation matrix, there are only a few features that don't correlate somewhat (positively or negatively). Maybe we can remove chol and fbs from the training and test data later? This is something to look into, at the very least.","5c1be2a3":"### Quick plot of these results","37aed958":"# One Hot Encoding to handle categorical data","44906785":"What kind of data do we have?","d5fb9c90":"### EDA Checklist\n\n1. What questions are we trying to solve?\n2. What kind of data do we have?\n3. What's missing?\n4. What are the outliers?","8abda6d6":"Accuracy is not necessarily the best measure of a model, but it isn't a waste of time to look at it either. Initially, this shows RandomForest doing the \"best\" and Logistic Regression doing the \"worst\". Now with a baseline, I can tune the models.","ffb2d0b0":"In this case, it's tough to say that any correlation exists. There appears to be a general downward trend for both positve cases and negative cases.","0b093f50":"I'm going to scale the data to a 0-1 scale. While technically not needed for all algorithms, it doesn't hurt and will allow other algorithms to be used if we so choose. Be sure to scale train and test splits separately, to avoid leakage.","1769fb4e":"Any missing values?","87cb28ca":"## Load Data","2eceaca4":"## Does chest pain correlate to heart disease?","b34f9f35":"No missing values, which saves us a bit of imputing later on. ","daa7ab26":"Testing on 3 different models\n1. K Nearest Neighbors\n2. Random Forest\n3. Logistic Regression\n\nScaling was really done for the sake of K Nearest Neighbors, which I believe requires some sort of scaling","5111d063":"How does heart disease scale with the sex of the patients in the study? Male = 1, Female = 0","0d9e6bcf":"It looks like the two classes are (relatively) even. 0 = no heart disease, 1 = heart disease. Can we do better than roughly a coinflip?","4f5b72d9":"It looks like just about the same score. Trying RandomForest now","8692f26b":"# Imports","cd8d55f5":"For the cell below, thalach is maximum heart rate achieved during the study"}}