{"cell_type":{"bb34c63d":"code","91330678":"code","70ddb0fe":"code","40df4224":"code","45817ba4":"code","8dd471f1":"code","897e67c5":"code","4cb379a5":"code","2619ff5c":"code","adf19179":"code","f30b012b":"code","d49934c3":"code","feea56c1":"code","004a7f48":"code","5a448408":"code","ef5d7811":"code","fc8164fd":"code","f27c291e":"code","acab7f4c":"code","91184a09":"code","69e4656c":"code","19f5cbaa":"code","04b7ce86":"code","79d227e5":"code","1158539a":"code","2c3cbf77":"code","6f6f1fbd":"code","109d9b61":"code","306fbc60":"code","251f6035":"code","e5bca8ad":"code","6037c59e":"code","59442491":"code","89d63dea":"code","aa1cfd0a":"code","91a09709":"code","fecb852b":"code","d3efb1f5":"code","cba2e8e4":"code","3f187f32":"code","0811ab84":"code","ee4e1c49":"markdown","19b4c022":"markdown","b3d336c8":"markdown","68f679a9":"markdown","7e54306e":"markdown","53d031c7":"markdown","afbf012c":"markdown","fd567eb4":"markdown","e90aa459":"markdown","869d38b5":"markdown","044f9074":"markdown","063742f6":"markdown","389741e1":"markdown","03fa322b":"markdown","a8a911a3":"markdown","e00356f6":"markdown","abd64823":"markdown","d2f437d3":"markdown","d822962e":"markdown","2854947c":"markdown","7a3dc7f3":"markdown","bf98484f":"markdown","a501809e":"markdown","6d6a6280":"markdown","82f3dff1":"markdown","c28a8315":"markdown","7833c7e7":"markdown","f4fcb519":"markdown","b67770d0":"markdown","e8e61130":"markdown","bd5c2afa":"markdown","1f807f29":"markdown"},"source":{"bb34c63d":"import pandas as pd\n#pandas\nimport numpy as np\n#numpy\nimport matplotlib.pyplot as plt\n#matplotlib\nimport seaborn as sns\n#seaborn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n#sklearn\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\n#keras\nimport tensorflow as tf\n#tensorflow\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n#nltk\nimport re\nimport os\n#other useful stuff\nfrom wordcloud import WordCloud, STOPWORDS\n#wordclouds and cloud stopwords\nimport warnings\nwarnings.filterwarnings(\"ignore\")","91330678":"data = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding = 'latin', header=None)","70ddb0fe":"data.head(10)","40df4224":"data = data.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'})","45817ba4":"data.head(1)","8dd471f1":"missing_data = data.isna().sum().sort_values(ascending=False)\npercentage_missing = round((data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)*100,2)\nmissing_info = pd.concat([missing_data,percentage_missing],keys=['Missing values','Percentage'],axis=1)\nmissing_info.style.background_gradient()","897e67c5":"pd.set_option('display.max_colwidth', -1)\ndata[data['target']==0]['content'].head()","4cb379a5":"data[data['target']==4]['content'].head","2619ff5c":"data['target'] = data['target'].replace([0, 4],['Negative','Positive'])","adf19179":"fig = plt.figure(figsize=(8,8))\ntargets = data.groupby('target').size()\ntargets.plot(kind='pie', subplots=True, figsize=(10, 8), autopct = \"%.2f%%\", colors=['red','green'])\nplt.title(\"Pie chart of different classes of tweets\",fontsize=16)\nplt.ylabel(\"\")\nplt.legend()\nplt.show()","f30b012b":"data['target'].value_counts()","d49934c3":"data['length'] = data.content.str.split().apply(len)","feea56c1":"fig = plt.figure(figsize=(14,7))\n\nax1 = fig.add_subplot(122)\nsns.distplot(data[data['target']=='Positive']['length'], ax=ax1,color='green')\ndescribe = data.length[data.target=='Positive'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for positive sentiment tweets.', fontsize=16)\n\nplt.show()","004a7f48":"fig = plt.figure(figsize=(14,7))\n\nax1 = fig.add_subplot(122)\nsns.distplot(data[data['target']=='Negative']['length'], ax=ax1,color='red')\ndescribe = data.length[data.target=='Negative'].describe().to_frame().round(2)\n\nax2 = fig.add_subplot(121)\nax2.axis('off')\nfont_size = 14\nbbox = [0, 0, 1, 1]\ntable = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\ntable.set_fontsize(font_size)\nfig.suptitle('Distribution of text length for negative sentiment tweets.', fontsize=16)\n\nplt.show()","5a448408":"plt.figure(figsize=(14,7))\ncommon_keyword=sns.barplot(x=data[data['target']=='Positive']['username'].value_counts()[:10].index, \\\n                           y=data[data['target']=='Positive']['username'].value_counts()[:10],palette='viridis')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Positive tweet frequency',fontsize=12)\nplt.title('Top 10 users who publish positive tweets',fontsize=16)\nplt.show()","ef5d7811":"data[data['username']=='what_bugs_u'].head()","fc8164fd":"plt.figure(figsize=(14,7))\ncommon_keyword=sns.barplot(x=data[data['target']=='Negative']['username'].value_counts()[:10].index, \\\n                           y=data[data['target']=='Negative']['username'].value_counts()[:10],palette='Spectral')\ncommon_keyword.set_xticklabels(common_keyword.get_xticklabels(),rotation=90)\ncommon_keyword.set_ylabel('Negative tweet frequency',fontsize=12)\nplt.title('Top 10 users who publish negative tweets',fontsize=16)\nplt.show()","f27c291e":"data[data['username']=='lost_dog'].head()","acab7f4c":"plt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(data[data.target=='Positive'].content))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in positive sentiment tweets.',fontsize=20)\nplt.show()","91184a09":"plt.figure(figsize=(14,7))\nword_cloud = WordCloud(stopwords = STOPWORDS, max_words = 200, width=1366, height=768, background_color=\"white\").generate(\" \".join(data[data.target=='Negative'].content))\nplt.imshow(word_cloud,interpolation='bilinear')\nplt.axis('off')\nplt.title('Most common words in negative sentiment tweets.',fontsize=20)\nplt.show()","69e4656c":"data.drop(['id','date','query','username','length'], axis=1, inplace=True)","19f5cbaa":"data.head()","04b7ce86":"data.target = data.target.replace({'Positive': 1, 'Negative': 0})","79d227e5":"english_stopwords = stopwords.words('english')\n#base of english stopwords\nstemmer = SnowballStemmer('english')\n#stemming algorithm\nregex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n#regex for mentions and links in tweets","1158539a":"def preprocess(content, stem=False):\n  content = re.sub(regex, ' ', str(content).lower()).strip()\n  tokens = []\n  for token in content.split():\n    if token not in english_stopwords:\n      tokens.append(stemmer.stem(token))\n  return \" \".join(tokens)","2c3cbf77":"data.content = data.content.apply(lambda x: preprocess(x))","6f6f1fbd":"data.head()","109d9b61":"train, test = train_test_split(data, test_size=0.1, random_state=44)","306fbc60":"print('Train dataset shape: {}'.format(train.shape))\nprint('Test dataset shape: {}'.format(test.shape))","251f6035":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.content)  \nvocab_size = len(tokenizer.word_index) + 1 \nmax_length = 50","e5bca8ad":"sequences_train = tokenizer.texts_to_sequences(train.content) \nsequences_test = tokenizer.texts_to_sequences(test.content) \n\nX_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\nX_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\ny_train = train.target.values\ny_test = test.target.values","6037c59e":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('\/kaggle\/input\/glove6b100dtxt\/glove.6B.100d.txt')\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\n    \nglove_file.close()\n\nembeddings_matrix = np.zeros((vocab_size, embedding_dim))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[index] = embedding_vector","59442491":"embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)","89d63dea":"num_epochs = 10\nbatch_size = 1000","aa1cfd0a":"model = Sequential([\n        embedding_layer,\n        tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Bidirectional(LSTM(128)),\n        tf.keras.layers.Dropout(0.3),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid'),\n    ])","91a09709":"model.summary()","fecb852b":"tf.keras.utils.plot_model(model, show_shapes=True)","d3efb1f5":"model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size = batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)","cba2e8e4":"y_pred = model.predict(X_test)\ny_pred = np.where(y_pred>0.5, 1, 0)","3f187f32":"print(classification_report(y_test, y_pred))","0811ab84":"#History for accuracy\nplt.figure(figsize=(10,5))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train accuracy', 'Test accuracy'], loc='lower right')\nplt.show()\n# History for loss\nplt.figure(figsize=(10,5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train loss', 'Test loss'], loc='upper right')\nplt.suptitle('Accuracy and loss for second model')\nplt.show()","ee4e1c49":"Now, columns of dataset are much more informative. ","19b4c022":"### **Missing values**\n\n\nMissing data is common occurance in datasets, therefore it is recommended to check if a data set contains missing values before starting any analysis. ","b3d336c8":"### **Tokenization**\n\nIt is a particular kind of document segmentation. It does breaks up text into smaller chunks or segments called tokens. A tokenizer breaks unstructured data, natural language text, into chunks of information that can be counted as discrete elements. After this operation these counts of token occurences in particular document can be used as a vector representing given document.","68f679a9":"### **Wordclouds**\n\nBy creating word clouds for two classes, we can visualize what words were repeated most often for positive and negative classes. We don't want to show stopwords so i took base of stopwords from nltk library and i passed it to WordCloud function","7e54306e":"Based on the analysis of the tweet length it was concluded that the maximum length for tokenization equal to 50 will be sufficient","53d031c7":"### **Length of tweet content**\n\nBased on this analysis, we can find out the length of tweets for two particular classes of tweets.","afbf012c":"# **Model test harness** \n\nThe proposed model architecture will be tested on the following parameters:\n\n*   **loss** = \"binnary_crossentropy\" (due to binary classification problem)\n*   **optimizer** = Adam(learning_rate=0.001) (may be changed after seeing the learning graph)\n*   **metrics** = \"accuracy\" (due to binary classification problem)\n*   **number of epochs** = 10 (due to the large training data set)\n*   **batch size** = 1000 (in order to accelerate learning time)","fd567eb4":"# **Exploratory data analysis**\n","e90aa459":"Fortunately, dataset is free of missing values. ","869d38b5":"Based on the contet posted by the user, it can be concluded that this is not a regular user but it is just a bot.","044f9074":"As we can see dataset is perfectly balanced with the same numbers of occurrences for both classes. It is also worth mentioning that the data is not skewed which will certainly make modeling easier. ","063742f6":"By looking at the top part of the dataset we can learn a lot from it. We can indicate which column refers to. Therefore, we can describe them briefly:\n\n\n*   0 - target of sentiment\n*   1 - id of user\n*   2 - date of tweet\n*   3 - unnecessary column, in each row contains 'NO_QUERY'\n*   4 - nickname of author\n*   5 - content of tweet   ","389741e1":"### **Renaming column names**\n\n\nBecause of the numerical column names, it will be more convenient to work with a dataset with predefined column names.\n","03fa322b":"Seems like what_bugs_u is kind of a user who is in good mood really often.","a8a911a3":"Adding new column to dataset with length of particular tweets.","e00356f6":"### **Train test split**\nDue to the rather large size of the dataset 160000 tweets will be enough for testing. ","abd64823":"### **Targets**\n\n\n\nFirstly let's see what the classes of the individual tweets are about.","d2f437d3":"Based on the word cloud, it can be deduced that the most repeated words in tweets with negative sentiment are words such as: quot, lol, today which are the same as for positive sentiment class. However, there are also word occurrences from which negative sentiment of a tweet can be inferred such as: miss, sorry, hate etc.","d822962e":"### **Word embeddings using GloVe**\n\nWord embeddings provide a dense representation of words and their relative meanings. Embedding Matrix is a maxtrix of all words and their corresponding embeddings. Embedding matrix is used in embedding layer in model to embedded a token into it's vector representation, that contains information regarding that token or word.\n\nEmbedding vocabulary is taken from the tokenizer and the corresponding vectors from embedding model, which in this case is GloVe model. GloVe stand for Global Vectors for Word Representation \nand it is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nBelow was used pretrained GloVe embeddings from world known Stanford vector files. The smallest available file contains embeddings created for tiny 6 billions of tokens.  ","2854947c":"Replacing Positive and Negative labels with 1 and 0 respectively.","7a3dc7f3":"Based on the word cloud, it can be deduced that the most repeated words in tweets with positive sentiment are words such as: love, quot, lol, haha, thank, today. ","bf98484f":"Head of data after stemming and removing https. ","a501809e":"### **Most commonly tweeting users**\n","6d6a6280":"### **Content cleaning**\n\nStemming - it does refers to the process which goal is to reduce words into thier base form. In case of our problem for classification it is very important ooperation as we need to focus on the meaning of particular word. For instance words: *Running, Runned, Runner* all can reduce to the stem *Run*. Below we have used the base of english stopwords and stemming algorithm from nltk library. ","82f3dff1":"### Conclusions\n\n*   Presetned model performed quite good but there is also visible overfitting, learning rate increase or other hyperparameter tuning should help\n*   Larger dropout layers also could help with overfitting.\n*   Another improvement might be to use a more complex embedding layer. For example: GloVe 300d.\n*   High accuracy was not achieved for the training data which may indicate an unrepresentative training dataset. ","c28a8315":"# **Data preparing**\n### **Dropping unnecessary columns**\n\n\nThere are a lot of unnecessary columns in the following dataset. The task is to classify the semantics of a tweet, so all columns except the target and content columns are unnecessary. ","7833c7e7":"By reading the content of the tweets, we can conclude that they have a rather negative message, so class 0 refers to negative sentiments tweets.","f4fcb519":"By reading the content of the tweets, we can conclude that they have a rather positive message, so class 4 refers to positive sentiments tweets.","b67770d0":"It seems that lost dog bots are rather in a bad mood.","e8e61130":"Changing labels from 0 and 4 for more informative labels for further analysis. ","bd5c2afa":"In such a large dataset, tweets belonging to two classes are almost the same lengths. However, the average tweet length for the negative class is about 0.8 words longer.","1f807f29":"# **Model - Embedding + Stacked LSTM**\n\n\nModel consisted of layers build with lstm cells. With such a large amount of data, the model is computationally complex making the training process take a while. Furthermore, model regularization layers will reduce the possible overfitting which was present in the simpler models tested."}}