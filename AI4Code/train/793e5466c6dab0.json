{"cell_type":{"857edfe1":"code","c5f9f521":"code","82c4f36c":"code","d586f976":"code","5bb11ff9":"code","6cc62a16":"code","01d835e7":"code","dad4a423":"code","fab93ddd":"code","64e81dc0":"code","1090000e":"code","b2511f1a":"code","c3a11aee":"code","7d7d07b7":"code","c4298a1c":"code","458ba88b":"code","aab8728c":"code","b01bfaf8":"code","e851bbad":"code","0c79eb3b":"code","081ab10a":"code","e814fa3d":"code","5f9c65a7":"code","ce5f19a6":"code","0a4200e5":"code","0117ddc4":"markdown","36f7145f":"markdown","dea86c06":"markdown","42ea7d3d":"markdown","98ab1024":"markdown","6252ca01":"markdown","9fa2c443":"markdown","fef576dd":"markdown","393cb95c":"markdown","48093d13":"markdown","3fa96b1a":"markdown","e8898922":"markdown","a4273f47":"markdown","23313bad":"markdown","cfc0a84a":"markdown","c763dc07":"markdown","bfeae5c9":"markdown","c7a013a9":"markdown","808a1bf7":"markdown","1a4b6ea7":"markdown","088aadf9":"markdown","908c5350":"markdown","2c275460":"markdown","a5e13d9c":"markdown","f95c3af9":"markdown","aaed8ce4":"markdown","8cfb27a8":"markdown","514d7dbe":"markdown","a5f96184":"markdown","f4149e53":"markdown","cd797cc5":"markdown","b8db1707":"markdown","5f4413ed":"markdown","c5eda495":"markdown","e8652d12":"markdown","2bdc6bf7":"markdown","78c433a3":"markdown","7f7d0dc3":"markdown","224fd7a6":"markdown","25b7d78b":"markdown","2b158530":"markdown","718625dd":"markdown","12db0800":"markdown","67fdd79e":"markdown"},"source":{"857edfe1":"import pandas as pd\nimport plotly.express as px \nimport os\nimport matplotlib.pyplot as plt\nimport missingno as msno\nimport datetime\nfrom plotly import graph_objects as go\nfrom plotly.subplots import make_subplots","c5f9f521":"data=pd.read_csv('..\/input\/ecommerce-events-history-in-cosmetics-shop\/2020-Jan.csv')","82c4f36c":"data.head()","d586f976":"#seperating timezone\ndata[\"timezone\"]= data[\"event_time\"].str.rsplit(\" \", n=1,expand = True)[1]\ndata[\"event_time\"]= data[\"event_time\"].str.rsplit(\" \", n=1,expand = True)[0]\ndata[\"event_time\"]=pd.to_datetime(data[\"event_time\"])\n\n#creating date,time,hours,weekday,weeknum columns\ndata[\"date\"]=data['event_time'].dt.date\ndata[\"time\"]=data['event_time'].dt.time\ndata[\"hours\"]=data['event_time'].dt.hour\ndata[\"weekday\"]=data['event_time'].dt.weekday\ndata['weeknum']=data['event_time'].dt.isocalendar().week\n\n#changing weekday to string and adding 'week_' prefix to weeknum\ndata['weeknum'] = 'week_' + data['weeknum'].astype(str)\ndata['weekday']= data['weekday'].replace({0:'Mon',1:'Tues',2:'Wed',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'})","5bb11ff9":"data.head()\n","6cc62a16":"data.info()","01d835e7":"data.describe()","dad4a423":"returned_orders=data[data['price']<0]['price'].count()\nreturned_orders_perc=returned_orders\/(data['price'].count())\n\nprint(\"There are %2d returned orders which is %.5f of total orders.\" %(returned_orders,round(returned_orders_perc,5)))","fab93ddd":"data=data[data['price']>=0]\n\n#Checkinng how much missing values are present in the dataset.\n\n# Calculate the Percentage of missing values in All columns\nperc=data.isnull().sum() * 100 \/ len(data)\nprint(round(perc,2))","64e81dc0":"#Visualising in matrix form\nmsno.matrix(data)","1090000e":"#Visualising as bar graph\nmsno.bar(data)","b2511f1a":"#checking number of unique values in dataframe\ndata.nunique()","c3a11aee":"data.event_type.unique()","7d7d07b7":"#grouping and preparing data for funnel visualisation\n\ndata_funnel=data[data['event_type']!='remove_from_cart'].groupby(['event_type'],as_index=False)['event_time'].count()\ndata_funnel.columns=['event_type','# events']\ndata_funnel.sort_values('# events', inplace=True,ascending=False)\ndata_funnel.reset_index(drop=True,inplace=True)\ndata_funnel['percent']=data_funnel['# events']\/(data_funnel['# events'][0].sum())*100\ndata_funnel","c4298a1c":"#plotly to visualise funnel\nfig = go.Figure(go.Funnel(\n    y = data_funnel[\"event_type\"],\n    x = data_funnel[\"# events\"],\n    customdata=data_funnel[\"percent\"],\n    texttemplate= \"<b>%{label}: <\/B>%{value:.2s}\"+\"<br><b>% of Total:<\/b> %{customdata:.2f}%\",\n    textposition='inside',\n    marker = {\"color\": [\"lightyellow\", \"lightsalmon\", \"tan\"]}\n    ))\nfig.update_yaxes(visible=False)\nfig.update_layout(template='simple_white',     \n                  title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Customer Funnel for Purchase Journey\"})\nfig.show()","458ba88b":"datahour=data.groupby(['hours','weeknum'],as_index=False)['price'].count()\ndatahour.columns=['hours','weeknum','price']","aab8728c":"#Visualisation\nfig = px.area(datahour, x='hours', y=\"price\",color='weeknum')\nfig.update_layout(template='simple_white',     \n                title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Customer's Hourly Website Views\"},\n                xaxis = dict(\n                    title_text='hours',\n                    tickmode = 'linear',\n                    tick0 = 0,\n                    dtick = 2),\n                 yaxis = dict(\n                    title_text='Visitors'))\nfig.show()","b01bfaf8":"datadate=data[data['event_type']=='purchase'].groupby(['date'],as_index=False)['price'].sum()\ndatadateh=data[data['event_type']=='purchase'].groupby(['date'],as_index=False)['price'].count()\ndatadateh['avg_ticket']=datadate['price']\/datadateh['price']\ndatadate.columns=['date','price']","e851bbad":"#Visualisation\nfig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"Bar\"}],\n           [{\"type\": \"Scatter\"}]])\n\nfig.add_trace(go.Bar(x=datadate['date'], y=datadate[\"price\"],name='Sales'),\n             row=1,col=1)\nfig.add_trace(go.Scatter(x=datadateh['date'], y=datadateh['price'],\n                    mode='lines+markers',\n                    name='No of Purchases'),\n              row=1,col=1)\nfig.add_trace(go.Scatter(x=datadateh['date'], y=datadateh['avg_ticket'],\n                    mode='lines+markers',\n                    name='Avg Ticket Size'),\n              row=2,col=1)\nfig.update_layout(template='simple_white',     \n                title={'xanchor': 'center',\n                         'yanchor': 'top',        \n                         'y':0.9,\n                         'x':0.5,\n                         'text':\"Daily sales\"}\n                 )\nfig.update_yaxes(title_text='Sales\/No of Purchases',ticks=\"inside\", row=1)\nfig.update_yaxes(title_text='Avg Ticket Size',ticks=\"inside\", row=2)\nfig.update_xaxes(title_text='Date',ticks=\"inside\")\nfig.show()","0c79eb3b":"#preprare data for further Visualisation\ndatadatehour=data[data['event_type']=='purchase'].groupby(['date','hours'],as_index=False)['price'].sum()\ndatadatehour.columns=['date','hours','price']\ndatadatehour['hours']=datadatehour['hours'].astype(str)\ndatadatehour['date']=datadatehour['date'].astype(str)","081ab10a":"fig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"histogram2d\"}],\n           [{\"type\": \"histogram2dcontour\"}]])\nfig.add_trace(\n    go.Histogram2d(\n        x = datadatehour[\"date\"],\n        y = datadatehour[\"hours\"],\n        z = datadatehour[\"price\"],\n        colorbar=dict(len=0.5, y=0.8,title=\"Overall Sales\"),\n        histfunc = \"sum\",\n        colorscale = \"cividis\",\n        nbinsx = 31,\n        nbinsy=24),\n    row=1,col=1)\nfig.add_trace(\n    go.Histogram2dContour(\n        x = datadatehour[\"date\"],\n        y = datadatehour[\"hours\"],\n        z = datadatehour[\"price\"],\n        colorbar=dict(len=0.5, y=0.25,title=\"Overall Sales\",tickmode=\"array\",\n        tickvals=[20000,40000,60000,80000,100000,120000],),\n        histfunc = \"sum\",\n        showlegend=False,\n        colorscale = \"cividis\",\n        contours = dict(\n            showlabels = True),\n        nbinsx=31,\n        nbinsy=24),\n    row=2,col=1)\n\n\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':1,\n            'x':0.5,\n           'text':\"Heatmap vs Contourplot<\/br><\/br><\/br>Hourly sales during Jan\"}\n)\nfig.update_yaxes(title_text='Hours',ticks=\"inside\")\nfig.update_xaxes(title_text='Date',ticks=\"inside\")\nfig.show()","e814fa3d":"fig = make_subplots(\n    rows=2, cols=1,\n    column_widths=[1.0],\n    row_heights=[0.5, 0.5],\n    specs=[[{\"type\": \"histogram2d\"}],\n           [{\"type\": \"histogram2dcontour\"}]])\nfig.add_trace(\n    go.Histogram2d(\n        x = datahour[\"hours\"],\n        y = datahour[\"weeknum\"],\n        z = datahour[\"price\"],\n        colorbar=dict(len=0.5, y=0.8,title=\"Overall Sales\"),\n        histfunc = \"sum\",\n        colorscale='cividis',\n        nbinsx = 31,\n        nbinsy=7\n    ),\n    row=1,col=1)\nfig.add_trace(\n    go.Histogram2dContour(\n        x = datahour[\"hours\"],\n        y = datahour[\"weeknum\"],\n        z = datahour[\"price\"],\n        colorbar=dict(len=0.5, y=0.20,title=\"Overall Sales\",tickmode=\"array\",\n        tickvals=[20000,40000,60000],\n                     ),\n        histfunc = \"sum\",\n        showlegend=False,\n        colorscale='cividis',\n        contours = dict(\n            showlabels = True),\n        nbinsx=31,\n        nbinsy=7\n    ),\n    row=2,col=1)\n\n\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':0.999,\n            'x':0.5,\n           'text':\"Heatmap vs Contourplot<\/br><\/br><\/br>Hourly sales by weekdays\"}\n)\nfig.update_yaxes(title_text='Weekdays',ticks=\"inside\")\nfig.update_xaxes(title_text='Hours',ticks=\"inside\")\nfig.show()","5f9c65a7":"data.head()","ce5f19a6":"#grouping based on user_id,date,hours,weekday,weeknum\ndata_user=data[data['event_type']=='purchase'].groupby(['user_id','date','hours','weekday','weeknum']).agg({'price':['sum','count']}).reset_index()\n\n#converting columns from multi index to single index\ndata_user.columns=data_user.columns.to_flat_index()\ndata_user=data_user.rename(columns={('price', 'sum'):'purchased_value',('price', 'count'):'no_of_purchases',('user_id',''):'user_id',\n                          ('date',''):'date',('hours',''):'hours',('weekday',''):'weekday',('weeknum',''):'weeknum'})\n\n#checking whether columns are updated correctly\ndata_user.head()","0a4200e5":"fig = make_subplots(\n    rows=1, cols=2,\n    column_widths=[0.5, 0.5],\n    row_heights=[1.0],\n    specs=[[{\"type\": \"Violin\"},\n           {\"type\": \"Violin\"}]])\nfig = fig.add_trace(go.Violin(y=data_user['no_of_purchases'],\n                            name='No of Purchases',\n                            box_visible=True,\n                            meanline_visible=True),\n                       row=1,col=1)\nfig = fig.add_trace(go.Violin(y=data_user['purchased_value'],\n                            name='Ticked Size',\n                            box_visible=True,\n                            meanline_visible=True),\n                       row=1,col=2)\nfig.update_layout(\n    template=\"simple_white\",\n    margin=dict(r=50, t=50, b=50, l=50),\n    height=600,\n    showlegend=False,\n    yaxis_title=\"Count\",\n    title={'xanchor': 'center',\n           'yanchor': 'top',        \n            'y':0.95,\n            'x':0.5,\n           'text':\"Summary Statistics for Number of Purchases and Ticket size of purchases\"}\n)\nfig.show()","0117ddc4":"In order to find the average ticket size, averge number of purchases by user, lets group the data by user_id.","36f7145f":"## 3.Hourly Website Traffic<a id=\"12\"><\/a>","dea86c06":"Now lets use plotly too visualise the customer funnel","42ea7d3d":"### B. Funnel Visualisation<a id=\"11\"><\/a>","98ab1024":"### B. Hourly Website Traffic Visualisation<a id=\"14\"><\/a>","6252ca01":"### B. Visualising Daily sales,Ticket Size and # of Orders<a id=\"17\"><\/a>","9fa2c443":"# Data Cleaning and EDA\n<a id=\"2\"><\/a>","fef576dd":"## 1.Reading Data\n<a id=\"3\"><\/a>","393cb95c":"### C. Data Visualisation - Hourly Sales (by Week)<a id=\"21\"><\/a>","48093d13":"###  A. Data Prep<a id=\"23\"><\/a>","3fa96b1a":"This notebook contains Exploratory Data Analysis and some data visualisation using plotly package. The data used here is the Ecommerce behaviour data for a medium cosmetics online store and I have chosen only January data for the analysis.\n    \nLink to the dataset -> [https:\/\/www.kaggle.com\/mkechinov\/ecommerce-events-history-in-cosmetics-shop](http:\/\/)\n\nLets begin.\n    ","e8898922":"### A.Data Prep\n<a id=\"10\"><\/a>","a4273f47":"## 2.Parsing Datetime and Creating necessary Datetime columns\n<a id=\"4\"><\/a>","23313bad":"As we can see from above info() method, all the datatypes are correctly available. Hours is available as int64. Since hours is only used for grouping and other data analysis related works, its okay to have it in int64 format. Lets quickly check the minimum, average, maximum and other statics related to the numerical columns.","cfc0a84a":"# Table of Contents","c763dc07":"###  A. Data Prep<a id=\"16\"><\/a>","bfeae5c9":"## 4.Daily Sales,Ticket Size and Number of Orders<a id=\"15\"><\/a>","c7a013a9":"# Data Analysis and Data Visualisation\n<a id=\"7\"><\/a>","808a1bf7":"## 2.Customer Purchase Funnel\n<a id=\"9\"><\/a>","1a4b6ea7":"## 3.Checking datatypes and range for filtering unrelavent data\n<a id=\"5\"><\/a>","088aadf9":"### B. Data Visualisation - Daily Sales (by Hour)<a id=\"20\"><\/a>","908c5350":"### B. Visualising summary statistics<a id=\"24\"><\/a>","2c275460":"###  A. Data Prep<a id=\"19\"><\/a>","a5e13d9c":"We can see that there are only 4 types of event_types in the data. So lets check the differnt types and will create a data funnel as how customers will go thorugh the purchase funnel.","f95c3af9":"0.00001% almost negligible.Might be result of poor data mining. Lets remove those records and save our dataset.","aaed8ce4":"We can see that there are some entries which have negative prices. This might be a return orders. Lets just try to see how many orders are returned.","8cfb27a8":"As we can see from above graph, there are two peaks when sales were high. This happened on Jan 27th and Jan 28th,2020. This might be due to some specific promotional campaigns.The number of sales is also high on these days but the average ticket size is not that low by which we can infer that,even if there is any promotional campaigns running on Jan 27 and Jan 28, the discount is not that low affecting the average ticket size.","514d7dbe":"Lets check the number of Unique values in the dataframe","a5f96184":"As we can see from the above graph, there are usually two peaks in a day which happens around 10 AM to 2PM and then again the peak starts from 6PM to 8PM. Knowing the marketing spent through out the day, and conversion rate arouund these hours, we can target campaigns (especially conversion campaigns) to run specifically targeting highly converting hours.\n\nThis peak hours generally translates to lunch break, and post work. Thus having high traffic at these hours makes sense.","f4149e53":"We can confirm the same high volume of sales on Jan 27th and 28th in above countour or heatmap as well. From above visualisation, it is also clear that generally the sales happen around 9AM to 1PM. But in last week, there are sales happening in the evening as well.","cd797cc5":"\n***This notebook I have created to practice Explarotary data analysis and to practise some data visualisation. Feel free to share your feedbacks with me. If you like my work,Please upvote. This will help me be motivated***","b8db1707":"When we combine sales weekwise and visualise, as we already predicted sales happen around 9AM to 2PM in the morning and in evening from 4PM To 8PM (mmajorly from 6PM to 8PM)\n\nWhen combining weekly sales, majority of sales happened in week3 and week4 ( because in January 2020, there are 4 days in week1 and 6days in week5 and rest of the week has 7 days. If we compensate the one missing day in week5, we will have higher sales in last week as well.","5f4413ed":"* [Import Packages](#1)\n* [Data Cleaning and EDA](#2)\n    1. [Reading Data](#3)\n    2. [Parsing Datetime and Creating necessary Datetime columns](#4)\n    3. [Checking datatypes and range for filtering unrelavent data](#5)\n    4. [Checking Null data](#6)\n* [Data Analysis and Data Visualisation](#7)\n    1. [Checking the Unique Values](#8)\n    2. [Customer Purchase Funnel](#9)\n        1. [Data Prep](#10)\n        2. [Funnel Visualisation](#11)\n    3. [Hourly Website Traffic](#12)\n        1. [Data Prep](#13)\n        2. [Hourly Website Traffic Visualisation](#14)\n    4. [Daily Sales,Ticket Size and Number of Orders](#15)\n        1. [Data Prep](#16)\n        2. [Visualising Daily sales,Ticket Size and # of Orders](#17)\n    5. [Hourly sales During Jan - Heatmap Vs Countour](#18)\n        1. [Data Prep](#19)\n        2. [Data Visualisation - Daily Sales (by Hour)](#20)\n        3. [Data Visualisation - Hourly Sales (by Week)](#21)\n    6. [Summary Statistics for # of Orders and Ticket size](#22)\n        1. [Data Prep](#23)\n        2. [Visualising summary statistics](#24)\n    ","c5eda495":"## 5.Hourly sales During Jan - Heatmap Vs Countour<a id=\"18\"><\/a>","e8652d12":"We can clearly see that Category_code and brand has majority of missing values. user_session has very minimal missing value. Lets visually see the missing value in the dataframe as follows:","2bdc6bf7":"Event_time column has string \"UTC\" in it. So split the actual datetime and UTC seperate. Then convert the Event_time column into datetime datatype. Also, create new columns called date,time,hour,weekday,weeknum from the \"event_time\" column. Lets also change the weekday format from number to string like mon,tues,etc..","78c433a3":"# Import Packages\n<a id=\"1\"><\/a>\n**Lets import all the necessary packages that we are going to use**","7f7d0dc3":"###  A. Data Prep<a id=\"13\"><\/a>","224fd7a6":"## 6.Summary Statistics for # of Orders and Ticket size <a id=\"22\"><\/a>","25b7d78b":"As we can see there are four event_types out of which \"Remove_from_cart\" is a event which all users might not go through during their journey. So lets remove that particular event before we group and make our data ready for visualising the funnel","2b158530":"## 1.Checking the Unique Values\n<a id=\"8\"><\/a>","718625dd":"As we can see from above graph, on average 8 products have been purchased where as 50% of people have purchased 6 products in the month of January. 25%(since INR Q3 is 10) of people have atleast purchased 10 products. Similarly, on average, the ticket size is 40.7 \ud835\udc4e\ud835\udc5b\ud835\udc5150 . 25%(since INR Q3 is 10) of people have spent minimum of 50.1$ as ticket size.","12db0800":"This notebook consists of some Exploratory data analysis, Basic web analysis and data visualisation. For simplicity, will be using only Jan data.","67fdd79e":"## 4.Checking Null data\n<a id=\"6\"><\/a>"}}