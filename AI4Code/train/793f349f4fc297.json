{"cell_type":{"5022f236":"code","f6f968ee":"code","914f5fd3":"code","9e10abd5":"code","4a98d41b":"code","09940b39":"code","2be3e94f":"code","376b29a7":"code","76d65c19":"markdown","745d283f":"markdown","b5f22d45":"markdown","593624a6":"markdown","1b535f20":"markdown","300b6622":"markdown","77fd0bf0":"markdown","1a33e22e":"markdown","77e1ceba":"markdown","d3de921d":"markdown","3906e9d9":"markdown","e881021e":"markdown","a98bd70a":"markdown","36f57937":"markdown","21e38875":"markdown","673a150e":"markdown"},"source":{"5022f236":"import statsmodels.api as sm","f6f968ee":"data = sm.datasets.fair.load_pandas()","914f5fd3":"data.exog.head()","9e10abd5":"data.endog.head()","4a98d41b":"y, X = data.endog, data.exog","09940b39":"X = sm.add_constant(X)","2be3e94f":"res = sm.OLS(y, X).fit()","376b29a7":"res.summary()","76d65c19":"#### ANOVA Interpretation\n\nAnalysis of Variance (ANOVA) - <a href = \"http:\/\/www.stat.yale.edu\/Courses\/1997-98\/101\/anovareg.htm\"> ANOVA <\/a> is a way to find out if the regression model is significant or not. This will be explained by hypothesis testing which will be done using F-Test. And the null hypothesis is H0: m=0 or \u03b2 = 0, and the alternate hypothesis is HA: m \u22600 or \u03b2\u22600. In other words, it indicates the probability that all the coefficients in our regression output are actually zero or not, which indicates if X explains the behaviour of Y. \n\nBefore understanding the statistics, below are the few definition related to statistics:\n\n\nIt explained by below statistics:\n\n<ul>\n     <li><i>DF<\/i>- Degree of Freedom is the number of values in the final calculation that are free to vary without violating any constraint. <\/li>\n   <li> <i> Df Model: 8 <\/i> - Degree of Freedom for Model or Regression. It is calculated as number of parameters or dependent variables used in model, and denoted by 'k'. \n    <li> <i> Df Residuals:\t6357 <\/i> - Degree of Freedom for Residuals in model. It is calculated as sample size minus the number of parameters being estimated (including constant), so it becomes df(Residual) = n - (k+1) or df(Residual) = n - k - 1.<\/li>\n    <li><i> F-statistic:46.06<\/i> - The F-statistic is the test statistic for F-tests. In general, it is a ratio of two quantities that are expected to be roughly equal under the null hypothesis, which produces an F-statistic of approximately 1. So in order to reject the null hypothesis, we need a <b> high F-value <\/b>.\n    <\/li>\n  <li><i> Prob (F-statistic): 1.47e-72<\/i> - The value of Prob(F) is the probability that the null hypothesis for the model is true (i.e., that all of the regression coefficients are zero). Compare the value against alpha value which is 0.05 by default. If its <b> less than alpha value, we reject the null hypothesis <\/b>. In other words, one or more regression coefficients are not zero. Else, we fail to reject the null hypothesis. In our case, we reject the null hypothesis as Prob(F) value is less than the alpha value.\n    <\/li>  \n    <li> <i> R-squared:\t0.055 <\/i> - R-squared is expressed as a value between 0 and 1, with 1 signaling perfect correlation or linear relationship between X and Y, and zero signaling no correlation at all. In other words, R-squared value is 1 means the predicted value will be equal to corresponding actual value and residual will be zero. Generally, we need to target for <b>high R-squared value <\/b>.\n<\/li> \n    <li> <i> Adj. R-squared: 0.054 <\/i> - Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of estimators in the model. It will either equal or less than R-squared value. With every new estimator (relevant or not), R-squared value will get incremented. But Adjusted R-square value increases, if the new estimator improves the model. Else it get decreases, if the new estimator affects the model. In case of <b> multiple linear regression, consider Adjusted R-squared value.<\/b>\n<\/li>\n   <\/ul>","745d283f":"In our case, the exog are stored in the X variable which are explanatory or independent variables. Similarly the y variable contains the endog, which is response or dependent variable","b5f22d45":"### Interpret the output of Model\n\nPrint out the statistics and interpret this regression model","593624a6":"Hi Guys,\n\nHere to share my thoughts on interpretation of basic linear regression model output.\n\n<b>So, let's get started and build a model.<\/b>\n","1b535f20":"Number of observations: 6366\n\nNumber of variables: 9\n\nVariable name definitions:\n\n    rate_marriage   : How rate marriage, 1 = very poor, 2 = poor, 3 = fair,4 = good, 5 = very good\n    age             : Age\n    yrs_married     : No. years married. Interval approximations. See original paper for detailed \n                      explanation.\n    children        : No. children\n    religious       : How relgious, 1 = not, 2 = mildly, 3 = fairly, 4 = strongly\n    educ            : Level of education, 9 = grade school, 12 = high school, 14 = some college, \n                      16 = college graduate, 17 = some graduate school, 20 = advanced degree\n    occupation      : 1 = student, 2 = farming, agriculture; semi-skilled,or unskilled worker; \n                      3 = white-colloar; 4 = teacher counselor social worker, nurse; artist, writers;\n                      technician, skilled worker, 5 = managerial,administrative, business, \n                      6 = professional with advanced degree\n    occupation_husb : Husband's occupation. Same as occupation.\n    affairs         : measure of time spent in extramarital affairs","300b6622":"## Done :)\n\nFeel free to share your comments.","77fd0bf0":"Statsmodels does not add a constant by default. So, add constant to get intercept value (denoted as b) in our regression model","1a33e22e":"### Import Libraries","77e1ceba":"### Train the Model\n\nNow its time to build and train our regression model using basic model known as <a href = \"https:\/\/en.wikipedia.org\/wiki\/Ordinary_least_squares\" > Ordinary Least Squares (OLS) <\/a>","d3de921d":"### Load a dataset","3906e9d9":"### Linear regression","e881021e":"<b>Now let's explore our dataset using <\/b><a href=\"https:\/\/www.statsmodels.org\/stable\/endog_exog.html\">exog and endog<\/a>","a98bd70a":"<b>Linear regression<\/b> is a statistical model that examines the linear relationship between two (Simple Linear Regression ) or more (Multiple Linear Regression) variables \u2014 a dependent variable and independent variable(s). Linear relationship basically means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) too.\n\nA linear relationship between variables Y and X is represented by this equation:\n\nY = m*X + b\n(OR)\nY = \u03b20+\u03b21*X\n\nIn this equation, y is the dependent variable \u2014 or the variable we are trying to predict or estimate; X is the independent variable \u2014 the variable we are using to make predictions\n\n<a href = \"https:\/\/towardsdatascience.com\/simple-and-multiple-linear-regression-in-python-c928425168f9\">Click here <\/a> for more details.","36f57937":"#### Coefficient Interpretation for Single Estimated paremter\n<ul>\n    <li> <i> coef <\/i> - These are predicted coefficient values of estimators or X variables including Intercept (labeled as const). It will be inferred as \"Change in one unit of estimator(denoted as x), will increase (if sign is positive) or decrease (if sign is negative) the Y by coef value, when all other estimators are kept constant.\" Example: If age is increase by 1, the time spent in affairs will be reduced by '0.4205', assuming all other varaibles are kept constant.         \n   \n   In case of intercept, the value refers as the average value of Y, when X is zero.  \n   <\/li> \n   <li><i> std err <\/i> - Standard error gives indication of how much predicted ceofficient is likely to vary from the corresponding population parameter. \n    <\/li>\n    <li><i> t <\/i> - T-test is to testing the statistical significance of the coefficient by doing hypothesis testing. For the given parameter, null hypothesis is H0: \u03b2 = 0, and the alternate hypothesis is HA: \u03b2\u22600. In other words, it indicates the probability that the actual coefficient is actually zero or not, which indicates if the given estimated parameter having any impact on the behaviour of Y. To perform this test, t-statistic is calcuated by dividing the predicted coefficient value and the corresponding standard error.\n    <\/li>\n    <li> <i> P>|t| <\/i> - Calculate the p-value of t-statistic using two-tailed test. This value is the probability that the null hypothesis is true (i.e., that the actual coefficient is zero). Similar to Prob(F-statistic), Compare the value against alpha value which is 0.05 by default. Estimators whose p-value is very high, they are not statistical significant in explaining the behaviour of Y.<\/li> \n    <li> <i> [0.025\t0.975] <\/i> - This is 95% confidence interval of predicted coefficient. In other words, there is 95% chance that the actual coefficient will be in this range. It is calculated as \n    \n    [coef + t-critical * std err, coef - t-critical * std err]\n    \n    where, t-critical for 95% or 0.05 alpha value is 1.96.\n<\/ul>","21e38875":"## Check out the data\nLets use python in-build dataset - <a href = \"https:\/\/www.statsmodels.org\/devel\/datasets\/generated\/fair.html\"> Affairs dataset <\/a>","673a150e":"#### Basic Interpretation\n<ul>\n    <li> <i> Dep. Variable:\taffairs<\/i> - Name of the dependent Variable. <\/li>\n    <li> <i> Model:\tOLS, Method: Least Squares<\/i> - Ordinary Least Squares (OLS) model used to build regression model. This model uses method to minimizes the sum of the squared residuals (SSR), where residual is the difference between the observed  or predicted value and the corresponding actual value of dependent variable.<\/li>\n    <li> <i> Date:\tMon, 17 Aug 2020, Time:\t13:03:22 <\/i> - Date and time when model is created <\/li>\n    <li> <i> No. Observations: 6366 <\/i> - No of rows present in dataset. Also, referred as sample size and denoted by 'n'. <\/li>\n    \n \n<\/ul>\n    "}}