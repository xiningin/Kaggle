{"cell_type":{"a8f32515":"code","47d156d7":"code","e4971d28":"code","f3e79a6e":"code","0db3ff17":"code","48241219":"code","6f9c3fc2":"code","12faf5bf":"code","e11c5cee":"code","8d536c9d":"code","8fc56a51":"code","11a6315e":"code","a01430bc":"code","6301753f":"code","ebe3bbe9":"code","9e70d46a":"code","80121bbc":"code","e0c74893":"code","376c82b4":"code","0c4821d4":"code","85950686":"code","51945e87":"code","78a9bb94":"code","18a2d870":"code","bc846bde":"code","ba324bb5":"code","7d519a48":"code","6103b4a4":"code","459f103f":"code","be078aa9":"code","4696b517":"code","f2418c67":"code","732ee422":"code","3c867063":"code","21faba2a":"code","b18842bd":"code","2f11e00a":"code","aefe071f":"code","ac4c2469":"code","fc4738f8":"code","93de54d4":"code","7c6ba4af":"code","563d346b":"code","b5846661":"code","48f60676":"code","6bed4868":"code","61be5179":"code","9606f932":"code","f380fff7":"code","3354c419":"code","48052287":"code","315e80ae":"markdown","4cbe8200":"markdown","0c278919":"markdown","feefa02b":"markdown","9d6fc176":"markdown","9e351900":"markdown"},"source":{"a8f32515":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport gc","47d156d7":"#data path\nPATH = '\/kaggle\/input\/instacart-market-basket-analysis\/'","e4971d28":"#Reading the datasets.\naisles = pd.read_csv(PATH + 'aisles.csv')\nproducts = pd.read_csv(PATH + 'products.csv')\norder_products_train = pd.read_csv(PATH + 'order_products__train.csv')\norder_products_prior = pd.read_csv(PATH + 'order_products__prior.csv')\ndepartments = pd.read_csv(PATH + 'departments.csv')\norders = pd.read_csv(PATH + 'orders.csv')","f3e79a6e":"#creating a datframe that will contain only prior information\nop = pd.merge(orders, order_products_prior, on='order_id', how='inner')\nop.head()","0db3ff17":"#Total number of orders placed by each users\nusers = op.groupby(by='user_id')['order_number'].aggregate('max').to_frame('u_num_of_orders').reset_index()\nusers.head()","48241219":"#average number of products bought by the user in each purchase.\n\n#1. First getting the total number of products in each order.\ntotal_prd_per_order = op.groupby(by=['user_id', 'order_id'])['product_id'].aggregate('count').to_frame('total_products_per_order').reset_index()\ntotal_prd_per_order.head(10)","6f9c3fc2":"#2. Getting the average products purchased by each user\navg_products = total_prd_per_order.groupby(by=['user_id'])['total_products_per_order'].mean().to_frame('u_avg_prd').reset_index()\navg_products.head()","12faf5bf":"#deleting the total_prd_per_order dataframe\ndel total_prd_per_order","e11c5cee":"#dow of most orders placed by each user\nfrom scipy import stats\ndow = op.groupby(by='user_id')['order_dow'].agg(lambda x: stats.mode(x)[0]).to_frame('dow_most_orders_u').reset_index()\ndow.head()","8d536c9d":"#hour of day when most orders placed by each user\nfrom scipy import stats\nhod = op.groupby(by='user_id')['order_hour_of_day'].agg(lambda x: stats.mode(x)[0]).to_frame('hod_most_orders_u').reset_index()\nhod.head()","8fc56a51":"#merging the user created features.\n\n#1. merging avg_products with users\nusers = users.merge(avg_products, on='user_id', how='left')\n#deleting avg_products\ndel avg_products\nusers.head()","11a6315e":"#2. merging dow with users.\nusers = users.merge(dow, on='user_id', how='left')\n#deleting dow\ndel dow\nusers.head()","a01430bc":"#3. merging hod with users\n#2. merging dow with users.\nusers = users.merge(hod, on='user_id', how='left')\n#deleting dow\ndel hod\nusers.head()","6301753f":"#number of time a product was purchased.\nprd = op.groupby(by='product_id')['order_id'].agg('count').to_frame('prd_count_p').reset_index()\nprd.head()","ebe3bbe9":"#products reorder ratio.\nreorder_p = op.groupby(by='product_id')['reordered'].agg('mean').to_frame('p_reordered_ratio').reset_index()\nreorder_p.head()","9e70d46a":"#merging the reorder_p with prd\nprd = prd.merge(reorder_p, on='product_id', how='left')\n#deleting reorder_p\ndel reorder_p\nprd.head()","80121bbc":"#how many times a user bought the same product.\nuxp = op.groupby(by=['user_id', 'product_id'])['order_id'].agg('count').to_frame('uxp_times_bought').reset_index()\nuxp.head()","e0c74893":"#reorder ratio of the user for each product.\nreorder_uxp = op.groupby(by=['user_id', 'product_id'])['reordered'].agg('mean').to_frame('uxp_reordered_ratio').reset_index()\nreorder_uxp.head()","376c82b4":"#merging the two dataframes into one\nuxp = uxp.merge(reorder_uxp, on=['user_id', 'product_id'], how='left')\n#deleting reorder_uxp\ndel reorder_uxp\nuxp.head()","0c4821d4":"#merging users df into uxp\ndata = uxp.merge(users, on='user_id', how='left')\ndata.head()","85950686":"#merging products df into data\ndata = data.merge(prd, on='product_id', how='left')\ndata.head()","51945e87":"#deleting unwanted dfs\ndel [users, prd, uxp]","78a9bb94":"#keeping only the train and test eval set from the orders dataframe.\norder_future = orders.loc[((orders.eval_set == 'train') | (orders.eval_set == 'test')), ['user_id', 'eval_set', 'order_id']]\norder_future.head()","18a2d870":"#merging the order_future with the data.\ndata = data.merge(order_future, on='user_id', how='left')\ndata.head()","bc846bde":"#preparing the train df.\ndata_train = data[data.eval_set == 'train']\ndata_train.head()","ba324bb5":"#merging the information from the order_proucts_train df into the data_train.\ndata_train = data_train.merge(order_products_train[['product_id', 'order_id', 'reordered']], on=['product_id', 'order_id'], how='left')\ndata_train.head()","7d519a48":"#filling the NAN values\ndata_train.reordered.fillna(0, inplace=True)","6103b4a4":"#setting user_id and product_id as index.\ndata_train = data_train.set_index(['user_id', 'product_id'])\n\n#deleting eval_set, order_id as they are not needed for training.\ndata_train.drop(['eval_set', 'order_id'], axis=1, inplace=True)","459f103f":"data_train.head()","be078aa9":"#preparing the test dataset.\ndata_test = data[data.eval_set == 'test']\ndata_test.head()","4696b517":"#deleting unwanted columns\ndata_test.drop(['eval_set', 'order_id'], axis=1, inplace=True)","f2418c67":"#setting user_id and product_id as index.\ndata_test = data_test.set_index(['user_id', 'product_id'])","732ee422":"data_test.head()","3c867063":"#deleting unwanted df\ndel [aisles, departments, order_products_prior, order_products_train, orders, order_future, data] #order_future, data","21faba2a":"#resetting index\ndata_train.reset_index(inplace=True)\ndata_test.reset_index(inplace=True)","b18842bd":"data_train.head()","2f11e00a":"#merging the aisles and department ids to with the train and test data\ndata_train = data_train.merge(products[['product_id', 'aisle_id']], on='product_id', how='left')\ndata_test = data_test.merge(products[['product_id', 'aisle_id']], on='product_id', how='left')","aefe071f":"#department\ndata_train = data_train.merge(products[['product_id', 'department_id']], on='product_id', how='left')\ndata_test = data_test.merge(products[['product_id', 'department_id']], on='product_id', how='left')","ac4c2469":"#setting user_id and product_id as index.\ndata_test = data_test.set_index(['user_id', 'product_id'])\n#setting user_id and product_id as index.\ndata_train = data_train.set_index(['user_id', 'product_id'])","fc4738f8":"#Building a XGBoost model.\n\n#importing the package.\nimport xgboost as xgb\n\n#splitting the train data into training and testing set.\nX_train, y_train = data_train.drop('reordered', axis=1), data_train.reordered\n\n#setting boosters parameters\nparameters = {\n    'eavl_metric' : 'logloss',\n    'max_depth' : 5,\n    'colsample_bytree' : 0.4,\n    'subsample' : 0.8\n}\n\n#instantiating the model\nxgb_clf = xgb.XGBClassifier(objective='binary:logistic', parameters=parameters, num_boost_round=10)\n\n# TRAIN MODEL\nmodel = xgb_clf.fit(X_train, y_train)\n\n#FEATURE IMPORTANCE - GRAPHICAL\nxgb.plot_importance(model)","93de54d4":"#predicting on the testing data\ny_pred = xgb_clf.predict(data_test).astype('int')\n\n#setting a threshold.\ny_pred = (xgb_clf.predict_proba(data_test)[:, 1] >= 0.21).astype('int')\ny_pred[0:10]","7c6ba4af":"#saving the prediction as a new column in data_test\ndata_test['prediction'] = y_pred\ndata_test.head()","563d346b":"# Reset the index\nfinal = data_test.reset_index()\n# Keep only the required columns to create our submission file (for chapter 6)\nfinal = final[['product_id', 'user_id', 'prediction']]\n\ngc.collect()\nfinal.head()","b5846661":"#Creating a submission file\norders = pd.read_csv(PATH + 'orders.csv')\norders_test = orders.loc[orders.eval_set == 'test', ['user_id', 'order_id']]\norders_test.head()","48f60676":"#merging our prediction with orders_test\nfinal = final.merge(orders_test, on='user_id', how='left')\nfinal.head()","6bed4868":"#remove user_id column\nfinal = final.drop('user_id', axis=1)","61be5179":"#convert product_id as integer\nfinal['product_id'] = final.product_id.astype(int)\n\n## Remove all unnecessary objects\ndel orders\ndel orders_test\ngc.collect()\n\nfinal.head()","9606f932":"d = dict()\nfor row in final.itertuples():\n    if row.prediction== 1:\n        try:\n            d[row.order_id] += ' ' + str(row.product_id)\n        except:\n            d[row.order_id] = str(row.product_id)\n\nfor order in final.order_id:\n    if order not in d:\n        d[order] = 'None'\n        \ngc.collect()\n\n#We now check how the dictionary were populated (open hidden output)\nd","f380fff7":"#Convert the dictionary into a DataFrame\nsub = pd.DataFrame.from_dict(d, orient='index')\n\n#Reset index\nsub.reset_index(inplace=True)\n#Set column names\nsub.columns = ['order_id', 'products']\n\nsub.head()","3354c419":"sub.to_csv('sub.csv', index=False)","48052287":"gc.collect()","315e80ae":"# Creating train and test dataset.","4cbe8200":"# Creating features related to the products using product_id.","0c278919":"# Creating Predictive model","feefa02b":"# Creating features related to the users. i.e using user_id","9d6fc176":"# Creating user-product features.","9e351900":"# Merging all the features into data DF."}}