{"cell_type":{"8dca74ef":"code","ba1c2cea":"code","dd59529e":"code","a93257fe":"code","5fb906d1":"code","198a660c":"code","57acd08e":"code","106dfb91":"code","5f31b9c4":"code","81e81a7f":"code","5b48629d":"markdown","06264c9d":"markdown","4fef3106":"markdown","0531a668":"markdown","cee9adee":"markdown","631ed185":"markdown","b07f2732":"markdown","f67b0bdf":"markdown","2301531d":"markdown","0ff0c07d":"markdown","804a1799":"markdown"},"source":{"8dca74ef":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom keras.layers import LeakyReLU\nimport numpy as np\nfrom typing import Tuple\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, LeakyReLU, Dropout\nfrom tensorflow.python.ops.gen_dataset_ops import PrefetchDataset\n\nfrom numpy.random import randn\nfrom numpy.random import randint\nfrom keras.datasets.cifar10 import load_data\nfrom matplotlib import pyplot","ba1c2cea":"BUFFER_SIZE = 5000\nBATCH_SIZE = 128\nnoise_dim = 100\nimg_rows, img_cols, channels = 32, 32, 3\nEPOCHS = 200","dd59529e":"# load and prepare cifar10 training images\ndef load_real_samples(BUFFER_SIZE: int, BATCH_SIZE: int) -> Tuple[np.ndarray, PrefetchDataset]:\n    (X_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\n    X_train = X_train[np.where(y_train == 1)[0]]\n    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, channels).astype('float32')\n    X_train = (X_train - 127.5) \/ 127.5  # Normalize the images to [-1, 1]\n\n    # creation of a tensorflow dataset\n    dataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(buffer_size=BUFFER_SIZE)  # (shuffle => repeat => batch)\n    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(1) # prefech (add in cache x batch during processing an other one)\n    return X_train, dataset\n","a93257fe":"(X_train, y_train), (_, _) = tf.keras.datasets.cifar10.load_data()\nplt.imshow(X_train[np.where(y_train == 1)[0][0]])\nplt.axis('off')","5fb906d1":"def define_generator(noise_dim: int) -> Sequential:\n    model = Sequential()\n    # foundation for 4x4 image\n    n_nodes = 256 * 4 * 4\n    model.add(Dense(n_nodes, input_dim=noise_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization())\n    model.add(Reshape((4, 4, 256)))\n    # 8x8\n    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization())\n    # 16x16\n    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization())\n    # 32x32\n    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(layers.BatchNormalization())\n    # output layer\n    model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n    return model","198a660c":"generator = define_generator(noise_dim)\n\nnoise = tf.random.normal([1, noise_dim])\ngenerated_image = generator(noise, training=False)\n\nplt.imshow(generated_image[0, :, :, 0], cmap='gray')\nplt.axis('off')\n","57acd08e":"def define_discriminator(in_shape=(32,32,3)) -> Sequential:\n    model = Sequential()\n    # 32x32\n    model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n    model.add(LeakyReLU(alpha=0.2))\n    # 16x16\n    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    # 8x8\n    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    # 4x4\n    model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n    model.add(LeakyReLU(alpha=0.2))\n    # classifier\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation='sigmoid'))\n    # compile model\n    opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model","106dfb91":"def define_gan(g_model: Sequential, d_model: Sequential) -> Sequential:\n    # make weights in the discriminator not trainable\n    d_model.trainable = False\n    model = Sequential([g_model, d_model])\n    # compile model\n    opt = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt)\n    return model","5f31b9c4":"# Generate images based just on noise\ndef generation_images(BATCH_SIZE: int, noise_dim: int, generator: Sequential) -> tf.Tensor:\n    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n    generated_image = generator(noise, training=True)\n    return generated_image\n\n\n# Save and plot images\ndef generate_and_save_images(static_noise: tf.Tensor, epoch: int, generator: Sequential):\n    generated_images = generator(static_noise, training=False)\n    plt.figure(figsize=(10, 10))\n    \n    for i in range (6*6):\n        plt.subplot(6, 6, i+1)\n        plt.imshow(generated_images[i, :, :, 0], cmap='gray')\n        plt.axis('off')\n        \n    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch+1))\n    plt.show()\n    \n\ndef train_model(EPOCHS: int, BATCH_SIZE: int, X_train: np.ndarray, dataset: PrefetchDataset, noise_dim: int,\n                static_noise: tf.Tensor, model: Sequential):\n    generator, discriminator = model.layers\n\n    for epoch in range(EPOCHS):\n        print(f\"Currently on Epoch {epoch+1}\")\n        i = 0\n        # For every batch in the dataset\n        for X_batch in dataset:\n            i=i+1\n            # show where the algo is \n            if i%100 == 0:\n                print(f\"\\tCurrently on batch number {i} of {len(X_train)\/\/BATCH_SIZE}\")\n                \n            ###### TRAINING THE DISCRIMINATOR \n\n            # Generate images based just on noise input\n            gen_images = generation_images(BATCH_SIZE, noise_dim, generator)\n\n            # Concatenate Generated Images against the Real Ones\n            X_fake_vs_real = tf.concat([gen_images, tf.dtypes.cast(X_batch,tf.float32)], axis=0)\n\n            # Set to zero for fake images and 0.9 for real images\n            y1 = tf.constant([[0.]] * BATCH_SIZE + [[0.9]] * BATCH_SIZE)\n\n            # Train the discriminator on this batch\n            d_loss = discriminator.train_on_batch(X_fake_vs_real, y1)\n\n            ###### TRAINING THE GENERATOR\n\n            # Create some noise\n            noise = tf.random.normal(shape=[BATCH_SIZE, noise_dim])\n\n            # We want discriminator to believe that fake images are real\n            y2 = tf.constant([[1.]] * BATCH_SIZE)\n\n            g_loss = model.train_on_batch(noise, y2)\n   \n        # save image every 5 epochs\n        if (epoch+1) % 5 == 0:\n            print(f'\\t Discriminator Loss: {d_loss} \\t\\t Generator Loss: {g_loss}')\n            generate_and_save_images(static_noise, epoch, generator)\n        # Save model every 50 epochs\n        if (epoch+1) % 50 == 0:\n            filename = 'generator_cars_model_%03d.h5' % (epoch+1)\n            generator.save(filename)","81e81a7f":"# create the discriminator\nd_model = define_discriminator()\n# create the generator\ng_model = define_generator(noise_dim)\n# create the gan\ngan_model = define_gan(g_model, d_model)\n# load data \nX_train, dataset = load_real_samples(BUFFER_SIZE, BATCH_SIZE)\n# static noise\nstatic_noise = tf.random.normal([BATCH_SIZE, noise_dim])\n# train model\ntrain_model(EPOCHS, BATCH_SIZE, X_train, dataset, noise_dim, static_noise, gan_model)","5b48629d":"#  Load data","06264c9d":"# Launch the training","4fef3106":"**Plot 1 image**","0531a668":"# Train model","cee9adee":"# Define parameters","631ed185":"# Create Generator model","b07f2732":"Create one image only based on noise","f67b0bdf":"# Create Discriminator model","2301531d":"Choose only 1 label of data : y_train == 1 (cars) \n\nX_train = X_train[np.where(y_train == 1)[0]]","0ff0c07d":"# Combine both models","804a1799":"# Import"}}