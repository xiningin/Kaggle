{"cell_type":{"4b482736":"code","888aa546":"code","b8475f52":"code","b146f42e":"code","44d7ba24":"code","d7aacf04":"code","0b320d5f":"code","ea380d18":"code","bcfdec38":"code","3ec18fa8":"code","c7c485f3":"code","6e7fc3e2":"code","c8f7ee50":"code","24b61146":"code","421e12c4":"code","2b39dee4":"code","6571eb88":"code","c81d4295":"code","89b13827":"code","c1f02ad6":"code","82fcbc1a":"code","28ac9743":"code","253de533":"code","a21f6d46":"code","da9d56b5":"code","b4864169":"code","fa2c3463":"code","b8ec0903":"code","f86dab8c":"code","b4a22aab":"code","e99e7a30":"code","70ed3af7":"code","e8e0588f":"code","363b6af4":"code","1348c403":"code","34ddb8b5":"code","bc9d96ad":"code","210024de":"code","c302b9f1":"code","d9b96a6d":"code","280e803e":"code","f295ff1e":"code","61a55b95":"code","bd6054ed":"code","41f597da":"code","7b8a8b80":"code","50a3b45d":"code","8fa702e8":"code","240371a1":"code","c31eaba8":"code","e321dbf7":"code","014e77e7":"code","8e426fc6":"code","bff15992":"code","e731e3e9":"code","34a8983f":"code","3391d244":"code","83e9fcac":"code","f7e5fe62":"code","8b013b43":"code","19c0f248":"code","43dc0316":"code","d1ff71cb":"code","72606829":"code","3d81f712":"code","773378cd":"code","9d4e1253":"code","82720119":"code","326ac0ee":"code","ee30b7b4":"code","8b9679f9":"code","78a5eb85":"code","9fe28606":"code","062f14b7":"code","bbbd08fd":"code","6cc5cabc":"code","fec1e0f0":"code","b7261a42":"code","68db877a":"markdown","e0a93974":"markdown","911d3657":"markdown","27c74bf3":"markdown","84ea1f07":"markdown","f4263c16":"markdown","cb86f8ef":"markdown","76da3b2b":"markdown","a64d8f21":"markdown","7a0be1d5":"markdown","381b8835":"markdown","27846073":"markdown","36423648":"markdown","217603bb":"markdown","64fbeed0":"markdown","a9f321a5":"markdown","84b81514":"markdown","74c985db":"markdown","9c04c59a":"markdown","7eed3f64":"markdown","b5c4fd8a":"markdown","40cb38be":"markdown","aa98b0d7":"markdown","79c04317":"markdown","4e8e85a5":"markdown","0a0227c7":"markdown","aea973ba":"markdown","9e5384c2":"markdown","24c3c14d":"markdown"},"source":{"4b482736":"#importing required libraries and packages\nimport numpy as np\nnp.random.seed(0)\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.metrics import mean_squared_error as mse\n\npd.set_option('max_columns', 100)","888aa546":"#Function Definitions\n\n#Function to get the VIFs for all the variables in a dataframe\ndef getvif(df):\n    if 'const' in list(df.columns):\n        df1=df.drop('const', axis=1) \n    else:\n        df1 = df.copy()\n    vif=pd.DataFrame()\n    vif['Features'] = df1.columns\n    vif['VIF'] = [variance_inflation_factor(df1.values, i) for i in range(df1.shape[1])]\n    vif['VIF'] = round(vif.VIF,2)\n    vif = vif.sort_values(by = 'VIF', ascending = False)\n    return vif\n\n\n#Function to print Model Evaluation parameters\ndef modeleval(y_actual, y_pred, x_test):\n    print('R2 Score of Model')\n    print(r2_score(y_true = y_actual, y_pred = y_pred))\n    print('\\nMean Absolute Error')\n    print(mae(y_true = y_actual, y_pred = y_pred))\n    print('\\nMean Squared Error')\n    print(mse(y_true = y_actual, y_pred = y_pred))\n    print('\\nRoot Mean Squared Error')\n    print(np.sqrt(mse(y_true = y_actual, y_pred = y_pred)))\n    print('\\nAdjusted R-squared')\n    print(1 - (1-r2_score(y_true = y_actual, y_pred = y_pred))*(len(y_actual)-1)\/(len(y_actual)-x_test_m6.shape[1]-1))","b8475f52":"#importing the data\ndf = pd.read_csv('..\/input\/bike-sharing-demand\/day.csv')\ndf.head()","b146f42e":"df.shape","44d7ba24":"df.info()","d7aacf04":"df.describe()","0b320d5f":"#There are no null values\ndf.isnull().sum()","ea380d18":"#Dropping columns we would not be needing\/using\n#Instant is just an identifier\n#dteday is the date and can be analyzed using other columns in the dataset, so it is redundant\n#casual and registered are part of cnt\ndf.drop(['instant','dteday','casual','registered'], axis = 1, inplace = True)","bcfdec38":"#Getting a pairplot for the entire dataset\npp = sns.pairplot(df)\nfig = pp.fig\nfig.savefig(\"output.png\")","3ec18fa8":"#Visalizing correlation between different varaibles\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(df.corr(), annot=True, ax=ax)\nplt.show()","c7c485f3":"#We see a high correlation between temp and atemp. Both of those together will not be crucial to the model and will lead to a very high (maybe infinite) VIF. \n#Hence, dropping atemp\ndf.drop(['atemp'], axis = 1, inplace = True)","6e7fc3e2":"#Visalizing categorical variables\nplt.figure(figsize = (20,20))\ncatcols = ['season','yr','holiday','weekday','workingday','weathersit']\nfor i in range(1,7):\n    plt.subplot(2,3,i)\n    sns.boxplot(x = catcols[i-1], y = 'cnt', data = df)\n\nplt.savefig('box')    ","c8f7ee50":"#Let's see the AVERAGE ridership across categorical variables\nplt.figure(figsize=(16,16))\nplt.subplot(2,3,i)\nfor i in range(1,7):\n    plt.subplot(2,3,i)\n    df.groupby(catcols[i-1])['cnt'].mean().plot.barh()\n    plt.xlabel('Average Ridership')\nplt.savefig('barh.png')\nplt.show()\n","24b61146":"#Let's see the TOTAL ridership across variables\nplt.figure(figsize=(16,16))\nplt.subplot(2,3,i)\nfor i in range(1,7):\n    plt.subplot(2,3,i)\n    df.groupby(catcols[i-1])['cnt'].sum().plot.barh()\nplt.show()","421e12c4":"df.head()","2b39dee4":"#changing the integer encodings to string for the season, mnth, weekday and creating dummy variables for all of these (since they do not have an associated cardinality)\ndf.season = df.season.map({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\nseasons = pd.get_dummies(df.season, drop_first = True)\ndf.mnth = df.mnth.map({1:'jan', 2:'feb', 3:'mar', 4:'apr',5:'may', 6:'jun', 7:'jul', 8:'aug',9:'sep', 10:'oct', 11:'nov', 12:'dec'})\nmonths = pd.get_dummies(df.mnth, drop_first = True)\ndf.weekday = df.weekday.map({1:'mon',2:'tue',3:'wed',4:'thu',5:'fri',6:'sat',0:'sun'})\nweekdays = pd.get_dummies(df.weekday, drop_first = True)\ndf.weathersit = df.weathersit.map({1:'clear',2:'cloudy',3:'light rain',4:'heavy rain'})\nweather = pd.get_dummies(df.weathersit, drop_first = True)","6571eb88":"#Inspecting the created dummy variables (already dropped the first one)","c81d4295":"seasons.head()","89b13827":"months.head()","c1f02ad6":"weekdays.head()","82fcbc1a":"weather.head()","28ac9743":"#Let's add these dummy variables to the data frame and remove the original columns which are now dummified\ndf = pd.concat([df,seasons,months,weekdays,weather], axis = 1)\ndf.drop(['season','mnth','weekday','weathersit'],axis = 1, inplace = True)\ndf.head()","253de533":"#Splitting the data in 70:30 ratio\ndf_train, df_test = train_test_split(df, train_size = 0.7, test_size = 0.3, random_state = 100)","a21f6d46":"df_train.shape","da9d56b5":"df_test.shape","b4864169":"#Now that we have split the data, we can work on scaling the variables in the train data set.\n#We will be using the MinMaxScaler\nscaler = MinMaxScaler()\nto_scale = ['temp', 'hum', 'windspeed','cnt']\n#fitting the scaler on the training dataset only\ndf_train[to_scale] = scaler.fit_transform(df_train[to_scale])","fa2c3463":"df_train.head()","b8ec0903":"df_train.describe()","f86dab8c":"#Visualizing correlation between all the variables now in the data\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(df_train.corr(), annot = True, ax = ax)\nplt.show()","b4a22aab":"#defining x_train and y_train, x_test and y_test\nx_train = df_train.drop(['cnt'], axis=1)\ny_train = df_train[['cnt']]\n\nx_test = df_test.drop(['cnt'], axis = 1)\ny_test = df_test[['cnt']]","e99e7a30":"#Perform Recursive Feature Elimination \nlm = LinearRegression()\nlm.fit(x_train, y_train)\n\nrfe = RFE(lm, 15) #We will set number of output variables to 15\nrfe = rfe.fit(x_train , y_train)\nlist(zip(x_train.columns, rfe.support_ , rfe.ranking_ ))","70ed3af7":"#Let's see which columns were selected\ncols_selected = x_train.columns[rfe.support_]\ncols_selected","e8e0588f":"#And these columns would be eliminated\nx_train.columns[~rfe.support_]","363b6af4":"#Creating another dataframe with only the retained variables from x_train\nx_train_rfe = x_train[cols_selected]","1348c403":"#Let's see the correlation heatmap once again for these variables\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(x_train_rfe.corr(), annot = True, ax = ax)\nplt.show()","34ddb8b5":"#Adding a constant term since sklearn does not automatically add constant to the model\nx_train_rfe = sm.add_constant(x_train_rfe)","bc9d96ad":"#Running the linear model on the now ready data\nlm = sm.OLS(y_train, x_train_rfe).fit()\nprint(lm.summary())","210024de":"#Checking VIF\ngetvif(x_train_rfe)","c302b9f1":"#Dropping hum since it has a very high VIF and most of the impact from this variable can be explained by other variables \nx_train_rfe2 = x_train_rfe.drop('hum',axis=1)","d9b96a6d":"#Building new model without hum variable\nlm2 = sm.OLS(y_train, x_train_rfe2).fit()\nprint(lm2.summary())","280e803e":"#Checking VIF for variables in new model\ngetvif(x_train_rfe2)","f295ff1e":"#Dropping holiday\nx_train_rfe3 = x_train_rfe2.drop('holiday', axis = 1)","61a55b95":"lm3 = sm.OLS(y_train, x_train_rfe3).fit()\nprint(lm3.summary())","bd6054ed":"#Checking VIF again \ngetvif(x_train_rfe3)","41f597da":"#Dropping variable jan since it is turning out to be realtively less significant\nx_train_rfe4 = x_train_rfe3.drop('jan', axis = 1)","7b8a8b80":"lm4 = sm.OLS(y_train, x_train_rfe4).fit()\nprint(lm4.summary())","50a3b45d":"#checking VIFs\ngetvif(x_train_rfe4)","8fa702e8":"#Dropping hul since it realtively less significant\nx_train_rfe5 = x_train_rfe4.drop('jul', axis = 1)","240371a1":"lm5 = sm.OLS(y_train, x_train_rfe5).fit()\nprint(lm5.summary())","c31eaba8":"#Checking VIFs\ngetvif(x_train_rfe5)","e321dbf7":"#Removing spring variable due to a higher VIF and lower significance\nx_train_rfe6 = x_train_rfe5.drop('spring', axis=1)","014e77e7":"lm6 = sm.OLS(y_train, x_train_rfe6).fit()\nprint(lm6.summary())","8e426fc6":"getvif(x_train_rfe6)","bff15992":"#Let's see the correlation heatmap once again for these variables\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(x_train_rfe6.corr(), annot = True, ax = ax)\nplt.show()","e731e3e9":"#predicting y_train using the model 7 \ny_train_pred = lm6.predict(x_train_rfe6)","34a8983f":"# Plotting y_train and y_train_pred to understand the variance from actual results in the train data\n#This verifies the assumption of linearity of variables x and y (and hence linearity of the model)\n\nfig = plt.figure()\nsns.regplot(x=y_train, y=y_train_pred)\nfig.suptitle('y_train vs y_pred')             \nplt.xlabel('y_train')                          \nplt.ylabel('y_pred')     ","3391d244":"#Lets see the distribution of error terms in the training set\n#it is coming out as normal\nplt.figure(figsize=(8,8))\nsns.distplot((y_train - y_train_pred.values.reshape(-1,1)))\nplt.xlabel('Residuals')\nplt.show()","83e9fcac":"#Checking residuals with a qqplot, the points are either on or close to the 45 degree line, indicating that the distribution is normal\nsm.qqplot(lm6.resid, line='45',fit=True)","f7e5fe62":"#checking multicollinearity in the final model using test data.\n#The max value of VIF is 4.76, which is acceptable.\n#This indicates there is very little multicollinearity between our selected variables and the assumption is met.\ngetvif(x_train_rfe6)","8b013b43":"#Mean of residuals is also very close to zero\n(y_train - y_train_pred.values.reshape(-1,1)).mean()","19c0f248":"#We can also plot a regression line through our data to get a clear picture of the spread\n#this helps us understand that error terms are independent of each other and mostly have a constant variance\nsns.regplot(x=y_train_pred, y = (y_train - y_train_pred.values.reshape(-1,1))['cnt'])\nplt.title('Spread of Residuals in Train Data')\nplt.xlabel('Target Variable Prediction')\nplt.ylabel('Residual Value')\nplt.show()","43dc0316":"#Visualzing residual terms\nres_train = y_train - y_train_pred.values.reshape(-1,1)\nsns.scatterplot(x=res_train.index, y=res_train.values.reshape(-1,))\nplt.show()","d1ff71cb":"#We will only use the transform method and not fit_transform since fitting is done on the train set only\ndf_test[to_scale] = scaler.transform(df_test[to_scale])","72606829":"df_test.describe()","3d81f712":"### Dividing into x_test and y_test\ny_test = df_test.pop('cnt')\nx_test = df_test","773378cd":"y_test.head()","9d4e1253":"x_test.head()","82720119":"#adding constant for using sm\nx_test_sm = sm.add_constant(x_test)","326ac0ee":"#Creating a predictor dataframe by only retaining the variables we retained in our model (Model 6)\nx_test_m6 = x_test_sm[(x_train_rfe6.columns)]","ee30b7b4":"x_test_m6.head()","8b9679f9":"#Making predictions using Model 6\ny_test_pred_m6 = lm6.predict(x_test_m6)","78a5eb85":"#Performance of model 6 on test data\nmodeleval(y_test,y_test_pred_m6, x_test_m6)","9fe28606":"# Plotting y_test and y_pred to understand the variance from actual results in the test data\n#This verifies the assumption of linearity of variables x and y (and hence linearity of the model)\n\nfig = plt.figure()\nsns.regplot(x=y_test, y=y_test_pred_m6)\nfig.suptitle('y_test vs y_pred')             \nplt.xlabel('y_test')                          \nplt.ylabel('y_pred')      ","062f14b7":"#Error terms are approximately normally distributed in the test data as well, thus meeting the assumption of linear regression\nsns.distplot(((y_test - y_test_pred_m6)))\nplt.xlabel('Residuals')\nplt.show()","bbbd08fd":"#Mean of residuals is also very close to zero\n(y_test - y_test_pred_m6).mean()","6cc5cabc":"#checking multicollinearity in the final model using test data.\n#The max value of VIF is 4.99, which is acceptable.\n#This indicates there is very little multicollinearity between our selected variables and the assumption is met.\ngetvif(x_test_m6)","fec1e0f0":"#We can also plot a regression line through this data to get a clear picture of the spread\n#this helps us understand that error terms are independent of each other and mostly have a constant variance\nsns.regplot(x=y_test_pred_m6, y = y_test - y_test_pred_m6)\nplt.title('Spread of Residuals in Test Data')\nplt.xlabel('Target Variable Prediction')\nplt.ylabel('Residual Value')\nplt.show()","b7261a42":"#Parameters of the final model\nlm6.params.sort_values()","68db877a":"### Scaling numerical variables on test set using the scaler used on train set","e0a93974":"# Visualizing the data","911d3657":"# Model 6","27c74bf3":"# Model 2","84ea1f07":"# Making Predictions on Test Data","f4263c16":"# Rescaling features","cb86f8ef":"# Model 1","76da3b2b":"# Residual Analysis","a64d8f21":"##### The R-squared is now 0.843. ","7a0be1d5":"##### Seeing the VIFs, it seems that temp has a very high VIF and we should drop it. But since the temperature of the day can play a crucial role in infulencing the target variable, we will instead be dropping holiday since it is the variable which is least significant in the model","381b8835":"# Performing train\/test split on the data","27846073":"# Preparing the data for model building","36423648":"### The distribution of error terms is approaching a normal distribution, this implies that our model is perfoming well in terms of assumptions of linear regression","217603bb":"**Problem Statement**\n\nA bike-sharing provider has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n* Which variables are significant in predicting the demand for shared bikes.\n* How well those variables describe the bike demands\n* Based on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n**Business Goal:**\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","64fbeed0":"##### We see that there are many variables with a considerably high VIF. \n##### We will start dropping variables from our model now, one by one, in an attempt to make the model simpler (less variables), while still maintaining a good r squared \n##### We will also be using the p-value (significance) of the varible as a criteria to determine if we can retain a variable","a9f321a5":"#### The model shows an r-squared value of 0.836 on the training data and 0.795 on the test data, which is decent. The RMSE is 0.09 which is also considerably low, given that all our variables were scaled between 0 and 1. The adjusted r-squared reported by the model on the test data is 0.785\n#### Overall, this seems to be a good model.","84b81514":"#  Starting EDA on the data set","74c985db":"Based on the paremters selected in the final model, we can make the following suggestions to the business to explain the impact of various variables on the total ridership","9c04c59a":"# Building the Model","7eed3f64":"## Model Evaulation","b5c4fd8a":"# Conclusion","40cb38be":"#### Error terms do appear to be independent of one another with an almost constant variance","aa98b0d7":"## Verifying Assumptions of Linear Regression\n\nWe will test for the following assumptions of linear regression: \n1. Linear relationship between X and Y\n2. Error terms are normally distributed (not X, Y)\n3. Error terms are independent of each other\n4. Error terms have constant variance (homoscedasticity)","79c04317":"We can safely select the Model 6 as our final model since it has performed well on the test data and has an r-squared value of 0.795 and an RMSE of 0.098. ","4e8e85a5":"##### The model still has a good R-squared value of 0.842","0a0227c7":"* Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n    * The above listed weather conditions have a heavy negative impact on the ridership. Whenever the weather situation is one of those listed above, the company can expect their ridership to go down. \n* Temperature\n    * The temperature of the day has a good positive effect on the ridership, and is the most significant variable in determining the count of riders using the company's services. The higher the temperature, the more the ridership will be. For every unit of increase in temperature, keeping all other variables constant, the ridership (cnt) would go up by 0.55, which is considerably large. \n* Year\n    * Since we have analyzed historical data, it has been observed that the ridership in general was higher in 2019 as compared to 2018. This might mean that there were some factors that the company had changed in 2019 that were not present in 2018\n* Windspeed\n    * The windspeed of the day also has a negative impact on ridership. People tend to not rent bikes on windy days. \n* Winter\n    * The ridership is expected go slightly up in winter months as compared to summer. Since temperature is also a crucial factor, warm winter days are turning out to be the best for the company's business.","aea973ba":"# Model 3","9e5384c2":"# Model 5","24c3c14d":"# Model 4"}}