{"cell_type":{"1d1f2afc":"code","acfe8d7a":"code","64aa240c":"code","35ffbb17":"code","1ba4edf5":"code","e187d49a":"code","cb0c5ed6":"code","9f2ff6d8":"code","282ffcc7":"code","452c9947":"code","a0439549":"code","1ca74831":"code","ace2e834":"code","d2187a14":"code","481ebb68":"code","9505df70":"code","fdbaaa80":"code","b726b2fb":"code","f0ee9ca2":"code","d10e67ff":"code","2e994a9a":"code","ff5595c2":"code","014175a8":"code","7e2aab73":"code","f6184db5":"code","f1ae7516":"code","c5f8a84e":"code","bff0193d":"code","aed70178":"code","4545f329":"code","e106c1a3":"code","c9035aa5":"code","470b16a8":"code","ceece9df":"code","9535ff49":"code","5120ae06":"code","ae2b0e5e":"code","2104fbda":"code","b9e6280c":"code","1c39deed":"code","333c6349":"code","a9edb65a":"code","446fa41b":"code","98578f9e":"code","c34b62e7":"code","911e229a":"code","c47aca4b":"code","92526598":"code","9942f59a":"markdown","f063b978":"markdown","a0ddef12":"markdown","6d2f1a2d":"markdown","b1adcef2":"markdown","fd0e8725":"markdown","195b3f0f":"markdown","7fc7619d":"markdown","029c3885":"markdown","db516214":"markdown","1885baef":"markdown","a5e0b8b3":"markdown","2dd25f3b":"markdown","a45f3afd":"markdown","28ddee4e":"markdown"},"source":{"1d1f2afc":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport string\nfrom string import digits\nimport re\nfrom sklearn.utils import shuffle\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Input, Dense,Embedding\nfrom keras.models import Model,load_model\nfrom keras.utils import plot_model\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import model_from_json\nimport pickle as pkl\nimport numpy as np","acfe8d7a":"with open('mar.txt','r') as f:\n  data = f.read()\n\n# len(data)","64aa240c":"# we need to clean the data\nuncleaned_data_list = data.split('\\n')","35ffbb17":"len(uncleaned_data_list)","1ba4edf5":"uncleaned_data_list = uncleaned_data_list[:38695]","e187d49a":"len(uncleaned_data_list)","cb0c5ed6":"english_word = []\nmarathi_word = []\ncleaned_data_list = []\nfor word in uncleaned_data_list:\n  english_word.append(word.split('\\t')[:-1][0])\n  marathi_word.append(word.split('\\t')[:-1][1])","9f2ff6d8":"len(english_word), len(marathi_word)","282ffcc7":"language_data = pd.DataFrame(columns=['English','Marathi'])\nlanguage_data['English'] = english_word\nlanguage_data['Marathi'] = marathi_word","452c9947":"# saving to csv\nlanguage_data.to_csv('language_data.csv', index=False)","a0439549":"language_data = pd.read_csv('language_data.csv')","1ca74831":"language_data.head()","ace2e834":"language_data.tail()","d2187a14":"english_text = language_data['English'].values\nmarathi_text = language_data['Marathi'].values","481ebb68":"english_text[0], marathi_text[0]","9505df70":"len(english_text), len(marathi_text)","fdbaaa80":"english_text_ = [x.lower() for x in english_text]\nmarathi_text_ = [x.lower() for x in marathi_text]","b726b2fb":"type(english_text_), type(marathi_text_)","f0ee9ca2":"english_text_ = [re.sub(\"'\",'',x) for x in english_text_]\nmarathi_text_ = [re.sub(\"'\",'',x) for x in marathi_text_]","d10e67ff":"def remove_punc(text_list):\n  table = str.maketrans('', '', string.punctuation)\n  removed_punc_text = []\n  for sent in text_list:\n    sentance = [w.translate(table) for w in sent.split(' ')]\n    removed_punc_text.append(' '.join(sentance))\n  return removed_punc_text\nenglish_text_ = remove_punc(english_text_)\nmarathi_text_ = remove_punc(marathi_text_)","2e994a9a":"# removing the digits from english sentances\nremove_digits = str.maketrans('', '', digits)\nremoved_digits_text = []\nfor sent in english_text_:\n  sentance = [w.translate(remove_digits) for w in sent.split(' ')]\n  removed_digits_text.append(' '.join(sentance))\nenglish_text_ = removed_digits_text\n\n# removing the digits from the marathi sentances\nmarathi_text_ = [re.sub(\"[\u0968\u0969\u0966\u096e\u0967\u096b\u096d\u096f\u096a\u096c]\",\"\",x) for x in marathi_text_]\nmarathi_text_ = [re.sub(\"[\\u200d]\",\"\",x) for x in marathi_text_]\n\n# removing the stating and ending whitespaces\nenglish_text_ = [x.strip() for x in english_text_]\nmarathi_text_ = [x.strip() for x in marathi_text_]","ff5595c2":"# removing the starting and ending whitespaces\nenglish_text_ = [x.strip() for x in english_text_]\nmarathi_text_ = [x.strip() for x in marathi_text_]","014175a8":"# Putting the start and end words in the marathi sentances\nmarathi_text_ = [\"start \" + x + \" end\" for x in marathi_text_]","7e2aab73":"# manipulated_marathi_text_\nmarathi_text_[0], english_text_[0]","f6184db5":"len(marathi_text_),len(english_text)","f1ae7516":"X = english_text_\nY = marathi_text_","c5f8a84e":"X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size = 0.1)\nlen(X_train),len(y_train), len(X_test), len(y_test)","bff0193d":"X[0], Y[0]","aed70178":"X_train[0], y_train[0]","4545f329":"# preparing data for the word embedding\ndef Max_length(data):\n  max_length_ = max([len(x.split(' ')) for x in data])\n  return max_length_\n\n#Training data\nmax_length_english = Max_length(X_train)\nmax_lenght_marathi = Max_length(y_train)\n\n#Test data\nmax_length_english_test = Max_length(X_test)\nmax_lenght_marathi_test = Max_length(y_test)","e106c1a3":"max_lenght_marathi, max_length_english","c9035aa5":"\ndef tokenizer_(text_data):\n  tokenizer = Tokenizer()\n  tokenizer.fit_on_texts(text_data)\n  return tokenizer\n\ntokenizer_input = tokenizer_(X_train)\nvocab_size_input = len(tokenizer_input.word_index) + 1\ntokenizer_target = tokenizer_(y_train)\nvocab_size_target = len(tokenizer_target.word_index) + 1","470b16a8":"with open('tokenizer_input.pkl','wb') as f:\n  pkl.dump(tokenizer_input,f)\n\nwith open('tokenizer_target.pkl','wb') as f:\n  pkl.dump(tokenizer_target,f)\npkl.dump(tokenizer_input, open('tokenizer_input.pkl', 'wb'))\npkl.dump(tokenizer_target, open('tokenizer_target.pkl', 'wb'))","ceece9df":"vocab_size_input,vocab_size_target","9535ff49":"# encoder_data_input = []\n# decoder_data_input = []\n# decoder_target_input = []\n# for i,(input_text,target_text) in enumerate(zip(X_train,y_train)):\n#   # Encoder input sequence\n#   # print(input_text)\n#   sequence_input = tokenizer_input.texts_to_sequences([input_text])\n#   # print(sequence_input)\n#   pad_seq_encoder =  pad_sequences(sequence_input, maxlen=max_length_english, padding='post')\n#   # print(pad_seq_encoder.shape)\n#   encoder_data_input.extend(pad_seq_encoder)# Always extend else it will make it 3 dimensional data\n\n#   #Decoder input sequence\n#   # print(target_text)\n#   sequence_target = tokenizer_target.texts_to_sequences([target_text])\n#   # print(sequence_target)\n#   pad_seq_decoder =  pad_sequences(sequence_target, maxlen=max_lenght_marathi, padding='post')\n#   decoder_data_input.extend(pad_seq_decoder)# Always extend else it will make it 3 dimensional data\n\n#   #Decoder target sequence\n#   # print(\"orignal target word:\",target_text)\n#   word = target_text.split(' ')\n#   word.pop(0)\n#   # print(word)\n#   decoder_target_word = ' '.join(word)\n#   # print(\"after pop target word: \", decoder_target_word)\n#   # print(decoder_target_word)\n#   decoder_target_word_one_hot = one_hot(decoder_target_word,vocab_size_target)\n#   # sequence_target_decoder = tokenizer_target.texts_to_sequences([decoder_target_word])\n#   # print(sequence_target_decoder)\n#   # pad_seq_decoder_target =  pad_sequences(sequence_target_decoder, maxlen=max_lenght_marathi, padding='post')\n#   decoder_target_input.append(np.array(decoder_target_word_one_hot))\n#   # break","5120ae06":"def generator_batch(X= X_train,Y=y_train, batch_size=128):\n  while True:\n    for j in range(0, len(X), batch_size):\n      encoder_data_input = np.zeros((batch_size,max_length_english),dtype='float32') #metrix of batch_size*max_length_english\n      decoder_data_input = np.zeros((batch_size,max_lenght_marathi),dtype='float32') #metrix of batch_size*max_length_marathi\n      decoder_target_input = np.zeros((batch_size,max_lenght_marathi,vocab_size_target),dtype='float32') # 3d array one hot encoder decoder target data\n      for i, (input_text,target_text) in enumerate(zip(X[j:j+batch_size],Y[j:j+batch_size])):\n        for t, word in enumerate(input_text.split()):\n          encoder_data_input[i,t] = tokenizer_input.word_index[word] # Here we are storing the encoder \n                                                                     #seq in row here padding is done automaticaly as \n                                                                     #we have defined col as max_lenght\n        for t, word in enumerate(target_text.split()):\n          # if word == 'START_':\n          #   word = 'start'\n          # elif word == 'END_':\n          #   word = 'end'\n          decoder_data_input[i,t] = tokenizer_target.word_index[word] # same for the decoder sequence\n          if t>0:\n            decoder_target_input[i,t-1,tokenizer_target.word_index[word]] = 1 #target is one timestep ahead of decoder input because it does not have 'start tag'\n      # print(encoder_data_input.shape())\n      yield ([encoder_data_input,decoder_data_input],decoder_target_input)\n","ae2b0e5e":"latent_dim = 50\n# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None,),name=\"encoder_inputs\")\nemb_layer_encoder = Embedding(vocab_size_input,latent_dim, mask_zero=True)(encoder_inputs)\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(emb_layer_encoder)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,),name=\"decoder_inputs\")\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\nemb_layer_decoder = Embedding(vocab_size_target,latent_dim, mask_zero=True)(decoder_inputs)\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(emb_layer_decoder, initial_state=encoder_states)\ndecoder_dense = Dense(vocab_size_target, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","2104fbda":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","b9e6280c":"plot_model(model, to_file='train_model.png', show_shapes=True)","1c39deed":"train_samples = len(X_train)\nval_samples = len(X_test)\nbatch_size = 128\nepochs = 50","333c6349":"model.fit_generator(generator = generator_batch(X_train, y_train, batch_size = batch_size),\n                    steps_per_epoch = train_samples\/\/batch_size,\n                    epochs=epochs)","a9edb65a":"model_json = model.to_json()\nwith open(\"model_2.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# serialize weights to HDF5\nmodel.save_weights(\"model_weight_5.h5\")\nprint(\"Saved model to disk\")","446fa41b":"\n# loading the model architecture and asigning the weights\njson_file = open('model_2.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel_loaded = model_from_json(loaded_model_json)\n# load weights into new model\nmodel_loaded.load_weights(\"model_weight_5.h5\")","98578f9e":"latent_dim = 50\n#inference encoder\nencoder_inputs_inf = model_loaded.input[0] #Trained encoder input layer\nencoder_outputs_inf, inf_state_h, inf_state_c = model_loaded.layers[4].output # retoring the encoder lstm output and states\nencoder_inf_states = [inf_state_h,inf_state_c]\nencoder_model = Model(encoder_inputs_inf,encoder_inf_states)","c34b62e7":"#inference decoder\n# The following tensor will store the state of the previous timestep in the \"starting the encoder final time step\"\ndecoder_state_h_input = Input(shape=(latent_dim,)) #becase during training we have set the lstm unit to be of 50\ndecoder_state_c_input = Input(shape=(latent_dim,))\ndecoder_state_input = [decoder_state_h_input,decoder_state_c_input]\n\n# # inference decoder input\ndecoder_input_inf = model_loaded.input[1] #Trained decoder input layer\n# decoder_input_inf._name='decoder_input'\ndecoder_emb_inf = model_loaded.layers[3](decoder_input_inf)\ndecoder_lstm_inf = model_loaded.layers[5]\ndecoder_output_inf, decoder_state_h_inf, decoder_state_c_inf = decoder_lstm_inf(decoder_emb_inf, initial_state =decoder_state_input)\ndecoder_state_inf = [decoder_state_h_inf,decoder_state_c_inf]\n#inference dense layer\ndense_inf = model_loaded.layers[6]\ndecoder_output_final = dense_inf(decoder_output_inf)# A dense softmax layer to generate prob dist. over the target vocabulary\n\ndecoder_model = Model([decoder_input_inf]+decoder_state_input,[decoder_output_final]+decoder_state_inf)\n\n","911e229a":"with open('tokenizer_input.pkl','rb') as f:\n  tokenizer_input = pkl.load(f)\nwith open('tokenizer_target.pkl','rb') as f:\n  tokenizer_target = pkl.load(f)\n# Creating the reverse mapping to get the word from the index in the sequence\nreverse_word_map_input = dict(map(reversed, tokenizer_input.word_index.items()))\nreverse_word_map_target = dict(map(reversed, tokenizer_target.word_index.items()))","c47aca4b":"# Code to predct the input sentences translation\ndef decode_seq(input_seq):\n  # print(\"input_seq=>\",input_seq)\n  state_values_encoder = encoder_model.predict(input_seq)\n  # intialize the target seq with start tag\n  target_seq = np.zeros((1,1))\n  target_seq[0, 0] = tokenizer_target.word_index['start']\n  # print(\"target_seq:=>\",target_seq)\n  stop_condition = False\n  decoder_sentance = ''\n  # print(\"Beforee the while loop\")\n  while not stop_condition:\n    sample_word , decoder_h,decoder_c= decoder_model.predict([target_seq] + state_values_encoder)\n    # print(\"sample_word: =>\",sample_word)\n    sample_word_index = np.argmax(sample_word[0,-1,:])\n    # print(\"sample_word_index: \",sample_word_index)\n    decoder_word = reverse_word_map_target[sample_word_index]\n    decoder_sentance += ' '+ decoder_word\n    # print(\"decoded word:=>\",decoder_word)\n    # print(len(decoder_sentance))\n    # print(\"len(decoder_sentance) > 70: \",len(decoder_sentance) > 70)\n    # print('decoder_word == \"end\"',decoder_word == 'end')\n    # print(decoder_word == 'end' or len(decoder_sentance) > 70)\n    # stop condition for the while loop\n    if (decoder_word == 'end' or \n        len(decoder_sentance) > 70):\n        stop_condition = True\n        # print(\"from if condition\")\n    # target_seq = np.zeros((1,1))\n    target_seq[0, 0] = sample_word_index\n    # print(target_seq)\n    state_values_encoder = [decoder_h,decoder_c]\n  return decoder_sentance\n","92526598":"for i in range(100):\n  sentance = X_test[i]\n  original_target = y_test[i]\n  input_seq = tokenizer_input.texts_to_sequences([sentance])\n  pad_sequence = pad_sequences(input_seq, maxlen= 30, padding='post')\n  # print('input_sequence =>',input_seq)\n  # print(\"pad_seq=>\",pad_sequence)\n  predicted_target = decode_seq(pad_sequence)\n  print(\"Test sentance: \",i+1)\n  print(\"sentance: \",sentance)\n  print(\"origianl translate:\",original_target[6:-4])\n  print(\"predicted Translate:\",predicted_target[:-4])\n  print(\"==\"*50)\n","9942f59a":"# Model","f063b978":"## Data cleaning ","a0ddef12":"## some sentance translation from english sentance","6d2f1a2d":"# Inference","b1adcef2":"## Saving the model into Json","fd0e8725":"## loading the saved tokenizer for the prediction","195b3f0f":"## Data preparing for encoder and decoder","7fc7619d":"# Importing libraries","029c3885":"## Code for generating the translated sentance","db516214":"## Text Preprocessing","1885baef":"# Loading the data","a5e0b8b3":"## Loading the model and weight from Json","2dd25f3b":"# Data spliting","a45f3afd":"# Cleaning and preprocessing the data","28ddee4e":"# Creating the dataframe and saving the data into csv"}}