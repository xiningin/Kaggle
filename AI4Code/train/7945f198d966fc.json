{"cell_type":{"e3125c54":"code","1c02851b":"code","28efe3ac":"code","355aad45":"code","d7cd9c0e":"code","3362f307":"code","94ed1ed2":"code","05e31761":"code","f2af836a":"code","4752ef5d":"code","29f25c0d":"code","ea22095f":"code","9847929a":"code","13e23f28":"code","5e0b640c":"code","e68c55e0":"code","bc806e07":"code","e7c885f9":"code","77fee696":"code","61bf6e0f":"markdown","2780181f":"markdown","3e00d947":"markdown","60198429":"markdown","900f0429":"markdown","fb02af61":"markdown","e77528bc":"markdown","16c73070":"markdown","54dfff9e":"markdown","6cad551c":"markdown","2ae853b8":"markdown","79ebbd71":"markdown","bbd4fd0f":"markdown"},"source":{"e3125c54":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom ipywidgets import IntProgress\nfrom IPython.display import display","1c02851b":"def DataImport(filename):\n    \"\"\"\n    Arguments:\n        filename -- python str (string) containing the name of the csv file to read the data\n    Returns:\n        X -- data, numpy array of shape (number of variables, number of examples)\n        Y -- true \"label\" vector (for example: containing 0 if no survived, 1 if survived), shape (1, number of examples)\n    \"\"\"\n    Table = pd.read_csv(\"..\/input\/titanic\/\"+filename+\".csv\")\n    Table = Table.drop([\"Cabin\",\"Name\",\"Ticket\"],axis=1)\n    Table[\"Fare\"] = Table[\"Fare\"].replace(np.nan,32)\/512.\n    Table[\"Age\"] = Table[\"Age\"].replace(np.nan,30)\/100.\n    Table[\"Embarked\"] = Table[\"Embarked\"].replace(np.nan,\"C\")\n    Table[\"Embarked\"] = Table[\"Embarked\"].replace([\"C\",\"Q\",\"S\"],[0,1,2])\/2.\n    Table[\"Sex\"] = Table[\"Sex\"].replace([\"male\",\"female\"],[1,0])\n    Table[\"Pclass\"] = 1. - Table[\"Pclass\"]\/3.\n    Table[\"SibSp\"] = Table[\"SibSp\"]\/10.\n    if \"Survived\" in Table.columns:\n      X = Table.drop([\"PassengerId\",\"Survived\"],axis=1).to_numpy().T\n      Y = Table[\"Survived\"].to_numpy().reshape((1,-1))\n    else:\n      X = Table.drop([\"PassengerId\"],axis=1).to_numpy().T\n      Y = Table[\"PassengerId\"].to_numpy().reshape((1,-1))\n    return X, Y","28efe3ac":"X_data, Y_data = DataImport(\"train\")\nprint(\"X.shape=\",str(X_data.shape))\nprint(\"Y.shape=\",str(Y_data.shape))\nprint(\"X(0).T=\",str(X_data[:,0].T))\nprint(\"Y(0).T=\",str(Y_data[:,0].T))","355aad45":"list_permute = np.random.permutation(X_data.shape[1])\nlist_train, list_validation = list_permute[:600], list_permute[600:]\nX_train, Y_train = X_data[:,list_train], Y_data[:,list_train]\nX_validation, Y_validation = X_data[:,list_validation], Y_data[:,list_validation]","d7cd9c0e":"# import numpy as np\n# import matplotlib.pyplot as plt\n# from ipywidgets import IntProgress\n# from IPython.display import display\nclass NeuralNetwork:\n    def __init__(self,layer_dims,activations,cost_function):\n        \"\"\"\n        Arguments:\n        layer_dims   -- python array (list) containing the dimensions of each layer in our network\n        activations  -- python array (list) containing the activation function of each layer in our network\n        cost_function-- python str containing the cost function of our network\n        Returns:\n        parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                        W -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                        b -- bias vector of shape (layer_dims[l], 1)\n        \"\"\"\n        L = len(layer_dims)\n        self.cost_function = cost_function\n        self.parameters = {}\n        self.activations, self.caches, self.grads = [], [], []\n        for l in range(1,L):\n            self.parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n            self.parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n            self.activations.append(activations[l-1])\n        \n    def activation_forward(self,A_prev, W, b, activation):\n        \"\"\"\n        Arguments:\n        A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n        W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n        b -- bias vector, numpy array of shape (size of the current layer, 1)\n        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\" or \"tanh\"\n        Returns:\n        A -- the output of the activation function, also called the post-activation value \n        cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n                 stored for computing the backward pass efficiently\n        \"\"\"\n        Z = np.dot(W,A_prev)+b\n        linear_cache = (A_prev, W, b)\n        if activation == \"sigmoid\":\n            A = 1\/(1+np.exp(-Z));\n        elif activation == \"relu\":\n            A = np.maximum(0,Z)\n        elif activation == \"tanh\":\n            A = np.tanh(Z)\n        activation_cache = Z\n        cache = (linear_cache, activation_cache)\n        return A, cache\n    \n    def activation_backward(self,dA, cache, activation):\n        \"\"\"\n        Arguments:\n        dA -- post-activation gradient for current layer l \n        cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n        activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\", \"relu\" or \"tanh\"\n        Returns:\n        dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n        dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n        db -- Gradient of the cost with respect to b (current layer l), same shape as b\n        \"\"\"\n        linear_cache, activation_cache = cache\n        Z = activation_cache\n        if activation == \"sigmoid\":\n            s = 1\/(1+np.exp(-Z))\n            dZ = dA * s * (1-s)\n        elif activation == \"relu\":\n            dZ = np.array(dA, copy=True)\n            dZ[Z <= 0] = 0\n        elif activation == \"tanh\":\n            s = np.tanh(Z)\n            dZ = dA*(1-s**2)\n        A_prev, W, b = linear_cache\n        m = A_prev.shape[1]\n        dW = 1\/m*np.dot(dZ,A_prev.T)\n        db = 1\/m*np.sum(dZ,axis=1,keepdims=True)\n        dA_prev = np.dot(W.T,dZ)\n        return dA_prev, dW, db\n    \n    def forward(self,X):\n        \"\"\"\n        Arguments:\n        X -- data, numpy array of shape (input size, number of examples)\n        Returns:\n        AL -- last post-activation value\n        caches -- list of caches containing: every cache of activation_forward()\n        \"\"\"\n        self.caches = []\n        A = X\n        L = len(self.parameters) \/\/ 2\n        for l in range(1, L+1):\n            A_prev = A\n            activation = self.activations[l-1]\n            A, cache = self.activation_forward(A_prev, self.parameters[\"W\"+str(l)], self.parameters[\"b\"+str(l)], activation)\n            self.caches.append(cache)\n        AL = A\n        return AL\n    \n    def compute_cost(self, AL, Y):\n        \"\"\"\n        Arguments:\n        AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n        Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n        Returns:\n        cost -- cross-entropy cost\n        \"\"\"\n        m = Y.shape[1]\n        if self.cost_function == \"cross-entropy\":\n                cost = (1.\/m) * (-np.dot(Y,np.log(AL+1e-8).T) - np.dot(1-Y, np.log(1-AL+1e-8).T))\n        return cost\n    \n    def backward(self, AL, Y):\n        \"\"\"\n        Arguments:\n        AL -- probability vector, output of the forward propagation (L_model_forward())\n        Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n        caches -- list of caches containing: every cache of linear_activation_forward() \n        Returns:\n        grads -- A dictionary with the gradients\n                 grads[\"dA\" + str(l)] = ... \n                 grads[\"dW\" + str(l)] = ...\n                 grads[\"db\" + str(l)] = ... \n        \"\"\"\n        grads = {}\n        L = len(self.caches)\n        m = AL.shape[1]\n        Y = Y.reshape(AL.shape)\n        if self.cost_function == \"cross-entropy\":\n            dAL = - (np.divide(Y, AL+1e-8) - np.divide(1 - Y, 1 - AL+1e-8))\n        grads[\"dA\" + str(L)] = dAL\n        for l in reversed(range(L)):\n            current_cache, activation = self.caches[l], self.activations[l]\n            dA_prev_temp, dW_temp, db_temp = self.activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation)\n            grads[\"dA\" + str(l)] = dA_prev_temp\n            grads[\"dW\" + str(l + 1)] = dW_temp\n            grads[\"db\" + str(l + 1)] = db_temp\n        self.grads = grads\n    \n    def update_parameters(self, learning_rate):\n        \"\"\"\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients, output of L_model_backward\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n                      parameters[\"W\" + str(l)] = ... \n                      parameters[\"b\" + str(l)] = ...\n        \"\"\"\n        grads = self.grads\n        L = len(self.parameters) \/\/ 2\n        for l in range(L):\n            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n        \n    def train(self, X, Y, batch_size=None, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n        \"\"\"\n        Arguments:\n        X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n        Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n        learning_rate -- learning rate of the gradient descent update rule\n        num_iterations -- number of iterations of the optimization loop\n        print_cost -- if True, it prints the cost every 100 steps\n\n        Returns:\n        parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        if not batch_size == None:\n            m = X.shape[1] \/\/ batch_size\n            X_batch, Y_batch = [], []\n            for ii in range(m-1):\n                X_batch.append(X[:,ii*batch_size:(ii+1)*batch_size])\n                Y_batch.append(Y[:,ii*batch_size:(ii+1)*batch_size])\n            X_batch.append(X[:,(m-1)*batch_size:])\n            Y_batch.append(Y[:,(m-1)*batch_size:])\n        else:\n            X_batch, Y_batch = [X], [Y]\n        if print_cost:\n          f = IntProgress(min=0, max=num_iterations\/\/1000)\n          display(f)\n        costs = []\n        for i in range(0, int(num_iterations)):\n            cost_iteration = []\n            for ii in range(len(X_batch)):\n                X_mini, Y_mini = X_batch[ii], Y_batch[ii]\n                AL= self.forward(X_mini)\n                cost = self.compute_cost(AL,Y_mini)\n                cost_iteration.append(cost)\n                self.backward(AL,Y_mini)\n                self.update_parameters(learning_rate)\n            if print_cost and i % 1000 == 0:\n                costs.append(np.average(cost_iteration))\n                f.value += 1\n        if print_cost:\n            costs.append(np.average(cost_iteration))\n            plt.plot(np.squeeze(costs))\n            plt.ylabel('cost')\n            plt.xlabel('iterations (per thousands)')\n            plt.title(\"Learning rate = \" + str(learning_rate))\n            plt.show()\n        else:\n            print(\"training done:\",num_iterations,\"iterations\")","3362f307":"def Predict(AL):\n  return (AL>0.5).astype(int)\n\ndef Predict_accuracy(AL,Y):\n  Yhat = Predict(AL)\n  accuracy = 1 - np.average(np.abs(Yhat-Y))\n  return accuracy, Yhat","94ed1ed2":"Logistic = NeuralNetwork((7,1),[\"sigmoid\"],\"cross-entropy\")\n\nprint(\"Error with training data:\",np.squeeze(Logistic.compute_cost( Logistic.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(Logistic.compute_cost( Logistic.forward(X_validation), Y_validation )))\n\nacc_t, _ = Predict_accuracy(Logistic.forward(X_train),Y_train)\nacc_v, _ = Predict_accuracy(Logistic.forward(X_validation),Y_validation)\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)","05e31761":"Logistic.train(X_train,Y_train,learning_rate=1e-2,num_iterations=10e3,\\\n               batch_size=200,print_cost=True)","f2af836a":"print(\"Error with training data:\",np.squeeze(Logistic.compute_cost( Logistic.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(Logistic.compute_cost( Logistic.forward(X_validation), Y_validation )))\n\nacc_t, _ = Predict_accuracy(Logistic.forward(X_train),Y_train)\nacc_v, pred_v = Predict_accuracy(Logistic.forward(X_validation),Y_validation)\n\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)\nprint(np.concatenate((pred_v.T,Y_validation.T),axis=1)[:10,:])","4752ef5d":"NN = NeuralNetwork((7,3,1),[\"relu\",\"sigmoid\"],\"cross-entropy\")\n\nprint(\"Error with training data:\",np.squeeze(NN.compute_cost( NN.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(NN.compute_cost( NN.forward(X_validation), Y_validation )))\n\nacc_t, pred_t = Predict_accuracy(NN.forward(X_train),Y_train)\nacc_v, pred_v = Predict_accuracy(NN.forward(X_validation),Y_validation)\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)","29f25c0d":"NN.train(X_train,Y_train,learning_rate=1e-2,num_iterations=10e3, \\\n         batch_size=200,print_cost=True)","ea22095f":"print(\"Error with training data:\",np.squeeze(NN.compute_cost( NN.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(NN.compute_cost( NN.forward(X_validation), Y_validation )))","9847929a":"acc_t, pred_t = Predict_accuracy(NN.forward(X_train),Y_train)\nacc_v, pred_v = Predict_accuracy(NN.forward(X_validation),Y_validation)\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)\nprint(np.concatenate((pred_v.T,Y_validation.T,),axis=1)[:10,:])","13e23f28":"NN_deep = NeuralNetwork((7,6,3,1),[\"tanh\",\"relu\",\"sigmoid\"],\"cross-entropy\")\n\nprint(\"Error with training data:\",np.squeeze(NN_deep.compute_cost( NN_deep.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(NN_deep.compute_cost( NN_deep.forward(X_validation), Y_validation )))\n\nacc_t, _ = Predict_accuracy(NN_deep.forward(X_train),Y_train)\nacc_v, _ = Predict_accuracy(NN_deep.forward(X_validation),Y_validation)\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)","5e0b640c":"NN_deep.train(X_train,Y_train,learning_rate=1e-1,num_iterations=10e3,\\\n               batch_size=200,print_cost=True)","e68c55e0":"print(\"Error with training data:\",np.squeeze(NN_deep.compute_cost( NN_deep.forward(X_train), Y_train )))\nprint(\"Error with validation data:\",np.squeeze(NN_deep.compute_cost( NN_deep.forward(X_validation), Y_validation )))\n\nacc_t, pred_t = Predict_accuracy(NN_deep.forward(X_train),Y_train)\nacc_v, pred_v = Predict_accuracy(NN_deep.forward(X_validation),Y_validation)\nprint(\"Accuracy of model with training data:\",acc_t*100)\nprint(\"Accuracy of model with validation data:\",acc_v*100)\nprint(np.concatenate((pred_v.T,Y_validation.T,),axis=1)[:10,:])","bc806e07":"X_test, Y_test = DataImport(\"test\")\nprint(\"X.shape=\",str(X_test.shape))\nprint(\"X(0).T=\",str(X_test[:,0].T))","e7c885f9":"predict_NN_deep = Predict(NN_deep.forward(X_test))\nprint(predict_NN_deep[:,:5])","77fee696":"data = {\n    \"PassengerId\": np.squeeze(Y_test).tolist(),\n    \"Survived\": np.squeeze(predict_NN_deep).tolist(),\n}\nresults = pd.DataFrame(data,columns=[\"PassengerId\",\"Survived\"])\nresults.to_csv('..\/working\/results.csv',index=False)\nresults.head()","61bf6e0f":"### Deep Neural Network","2780181f":"### Deep Neural Network","3e00d947":"# Titanic Machine Learning (TitanicML)","60198429":"## 5. Neural Network Class\n### Class definition","900f0429":"## 3. Libraries\nFirst, let's run the cell below to import all the libraries that it will need during this commitment. \n- [numpy](https:\/\/numpy.org) is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.  \n- [pandas](https:\/\/pandas.pydata.org\/) is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n- [matplotlib](https:\/\/matplotlib.org)  is a plotting library for the Python programming language and its numerical mathematics extension NumPy.","fb02af61":"## 2. Data\nThe competition gives access to two similar datasets, which has passenger information.  \nOne dataset is \"train.csv\" and contains the details of a group of the passenger aboard the titanic (891 to be exact), likewise reveals if they survived or not.  \nThe second dataset is \"test.csv\" and contains the details of 418 passengers but does not include if they survived.  \nThe job is to predict these outcomes.  \n<table>\n<tbody>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><th><b>Key<\/b><\/th><\/tr>\n<tr><td>survival<\/td> <td>Survival<\/td> <td>0 = No, 1 = Yes<\/td><\/tr>\n<tr><td>pclass<\/td><td>Ticket class<\/td><td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td><\/tr>\n<tr><td>sex<\/td><td>Sex<\/td><td><\/td><\/tr>\n<tr><td>Age<\/td><td>Age in years<\/td><td><\/td><\/tr>\n<tr><td>sibsp<\/td><td># of siblings \/ spouses aboard the Titanic<\/td><td><\/td><\/tr>\n<tr><td>parch<\/td><td># of parents \/ children aboard the Titanic<\/td><td><\/td><\/tr>\n<tr><td>ticket<\/td><td>Ticket number<\/td><td><\/td><\/tr>\n<tr><td>fare<\/td><td>Passenger fare<\/td><td><\/td><\/tr>\n<tr><td>cabin<\/td><td>Cabin number<\/td><td><\/td><\/tr>\n<tr><td>embarked<\/td><td>Port of Embarkation<\/td><td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td><\/tr>\n<\/tbody>\n<\/table>","e77528bc":"### Neural Network 1 Hidden Layer","16c73070":"### results","54dfff9e":"## 4. Preprocess Data\n\nPrevious to build the model, the data need to process the information.  \nBeginning with removing the Cabin, Name, and Ticket columns, replacing the missing data with default values, renewing tags with numerical representations, and separating the data and the labels. This process is implemented in the DataImport function.\n\nThis above function is utilized on the \"train.csv\" file, extracting the information to build the model.\n\nHow the \"test.csv\" file does not have the information about the passenger survived or not, it is necessary to have validation data, which allows examining the model performance. Due to this, the data extracted from \"train.csv\" have to shuffle and divided into two parts.","6cad551c":"### Predict Function","2ae853b8":"## 7. Applying on test.csv\n### Loading the data of \"test.csv\"","79ebbd71":"## 1. Introduction\nThis notebook tries to solve the \"Titanic: Machine Learning from Disaster\" competition of [Kaggle](https:\/\/www.kaggle.com\/). The objective is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.  \nThe challenge is to build a predictive model, which shows what sort of people is more like to survive. This model has to analyze the information about each passenger (ie name, age, gender, socio-economic class, etc).","bbd4fd0f":"## 6. Different Models\n### Logistic Neural Network"}}