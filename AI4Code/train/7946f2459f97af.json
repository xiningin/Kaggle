{"cell_type":{"433ff285":"code","d1a9e77e":"code","2b93462b":"code","5c007cc7":"code","30d51d3b":"code","d915bc4f":"code","9cb006d7":"code","82b718c5":"code","3b57b147":"code","1243e215":"code","a9a0c366":"code","f7914f21":"code","2098ee75":"markdown","d1d12b91":"markdown","a65bdf6c":"markdown"},"source":{"433ff285":"# General imports\nimport numpy as np\nimport pandas as pd\nimport os, sys, gc, time, warnings, pickle, psutil, random\nfrom tqdm import tqdm\n\nimport seaborn as sns # data visualization library  \nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings('ignore')","d1a9e77e":"# Load data\ngrid_df = pd.read_pickle('\/kaggle\/input\/m5-simple-fe\/grid_part_1.pkl')\ngrid_df.head()","2b93462b":"def gap_finder(ts):\n    \n    # this function finds gaps and calculates their length:\n    # note ts: 0 = day with sales, 1 = days with 0 sales\n    \n    for i, gap in enumerate(ts):\n        if gap == 0: \n            continue\n        elif i!=0: \n            ts[i] += ts[i-1]\n            if ts[i-1]!=0: ts[i-1] = -1\n    return ts","5c007cc7":"# Note: in 'gap' column: 1 is a day without sales:\ngrid_df['gaps'] = (~(grid_df['sales'] > 0)).astype(int)\ntotal_days = 1941\n\nprods = list(grid_df.id.unique())\ns_list = [] #list to hold gaps in days\ne_list = [] #list to hold expected values of gaps\np_list = [] #list to hold avg probability of no sales\n\n# original 1 hour version\n#for prod_id in tqdm(prods):\n#    \n#    # extract gap_series for a prod_id\n#    m = grid_df.id==prod_id\n#    sales_gaps = grid_df.loc[m,'gaps']\n#    \n#    # calculate initial probability\n#    zero_days = sum(sales_gaps)\n#    p = zero_days\/total_days\n#    \n#    # find and mark gaps\n#    sales_gaps[:] = gap_finder(sales_gaps.values.copy())\n#    sales_gaps = sales_gaps.astype(int).replace(-1,np.nan).fillna(method='backfill').fillna(method='ffill')\n#    s_list += [sales_gaps]\n\n# magic x8 speed booster thanks to @nadare\nfor prod_id, df in tqdm(grid_df.groupby(\"id\")):   \n    # extract gap_series for a prod_id\n    sales_gaps = df.loc[:,'gaps']\n\n    # calculate initial probability\n    zero_days = sum(sales_gaps)\n    p = zero_days\/total_days\n\n    # find and mark gaps\n    accum_add_prod = np.frompyfunc(lambda x, y: int((x+y)*y), 2, 1)\n    sales_gaps[:] = accum_add_prod.accumulate(df[\"gaps\"], dtype=np.object).astype(int)\n    sales_gaps[sales_gaps < sales_gaps.shift(-1)] = np.NaN\n    sales_gaps = sales_gaps.fillna(method=\"bfill\").fillna(method='ffill')\n    s_list += [sales_gaps]\n    \n    # calculate E\/total_days for all possible gap lengths:\n    gap_length = sales_gaps.unique()\n    \n    d = {length: ((1-p**length)\/(p**length*(1-p)))\/365 for length in gap_length}\n    sales_E_years = sales_gaps.map(d)\n    \n    # cut out supply_gap days and run recursively\n    p1 = 0\n    while p1 < p:\n        \n        if p1!=0:\n            p=p1\n        \n        # once in 100 years event; change to your taste here\n        gap_days = sum(sales_E_years>100)\n            \n        p1 = (zero_days-gap_days+0.0001)\/(total_days-gap_days)\n        \n        d = {length: ((1-p1**length)\/(p1**length*(1-p1)))\/365 for length in gap_length}\n        sales_E_years = sales_gaps.map(d)\n        \n    # add results to list it turns out masked replacemnt is a very expensive operation in pandas, so better do it in one go\n    e_list += [sales_E_years]\n    p_list += [pd.Series(p,index=sales_gaps.index)]","30d51d3b":"# add it to grid_df in one go fast!:\ngrid_df['gap_days'] = pd.concat(s_list)\ngrid_df['gap_e'] = pd.concat(e_list)\ngrid_df['sale_prob'] = pd.concat(p_list)\n##45664\n# Dump to pickle:\ngrid_df.to_pickle('grid_part_1_gaps.pkl')","d915bc4f":"# becuase we have some really extreme values lets take a log:\ngrid_df['gap_e_log10'] = np.log10((grid_df['gap_e'].values+1))","9cb006d7":"# e over 100 years does not make much sense\nm = grid_df['gap_e_log10']>2\ngrid_df.loc[m,'gap_e_log10']=2\n\n# take a subsample to vizualise:\nnp.random.seed(19)\ndepts = list(grid_df.dept_id.unique())\n\nprod_list = []\nfor d in depts:\n    prod_by_dept=grid_df['item_id'][grid_df.dept_id == d].unique()\n    prod_list += list(np.random.choice(prod_by_dept,5))\n    \nm = grid_df.item_id.isin(prod_list)\nviz_df = grid_df[m]\nviz_df.head()","82b718c5":"v_df = viz_df.pivot(index='d', columns='id', values='gap_e_log10')\nv_df = v_df.reindex(sorted(v_df.columns), axis=1)","3b57b147":"v_df.describe()","1243e215":"f, ax = plt.subplots(figsize=(15, 20))\ntemp = sns.heatmap(v_df, cmap='Reds')\nplt.show()","a9a0c366":"grid_df.head()","f7914f21":"#Finally lets calculate the proportion of non random gaps in original dataset.\n# as mentioned by @Amphi2 we should have dropped last 28 days, so lets substract them:\n\n(sum(grid_df['gap_e_log10'] >= 2) - 3049*10*28)\/grid_df.shape[0]","2098ee75":"# M5 - calculating 'out_of_stock' feature 640x faster\n\nThe is notebook builds 'out_of_stock' feature 80x faster thanks to [math guys here](https:\/\/math.stackexchange.com\/questions\/364038\/expected-number-of-coin-tosses-to-get-five-consecutive-heads\/1030482#1030482)\n\nIn the [previous notebook](https:\/\/www.kaggle.com\/sibmike\/m5-out-of-stock-feature\/), I layed out how **we can distinguish between zero demand and zero supply**. Unfortunately, it would take roughly 90 hours to calulate the feature with the help of simulations. This notebook calculates the feature in less then ~~an hour~~ 8 minutes because instead of using simulations we can use a formula to calculate **expected number of days to randomly get a gap of length N** (in other words N consecutive days with zero sales). \n\nOur gap problem is similar to **\"Expected Number of Coin Tosses to Get Five Consecutive Heads\"** problem which is discussed and solved [here](https:\/\/math.stackexchange.com\/a\/1834771). The remaining logic is the same: \n1. assume probability of sale on a day, p0 = sale_days\/total_days\n2. for gap of length N find expected number of days to get it randomly E0 = E(N,p0)\n3. mark it as supply_gap if E0\/365 > 100 (i.e. it takes **more than 100 years to observe the gap ranomly**)\n4. cut out supply_gaps from probability calculations: p1 = (sale_days-supply_gap_days)\/(total_days-supply_gap_days)\n5. run recursively till p1 stops decreasing\n\nOnce in 100 years might be a little conservative threshold, so you can easily change this parameter to your taste. Formula for p1 can also be adjusted. Instead of constructing a boolean feature I have mapped gap days to E\/total_days value and saved p value in a separate column. Have fun!\n\nPS: As you will see in the end even with the 100 years threshold, **non random gaps account for over 23% of the dataset**. (That is if I have not messed up somewhere;)\n\n**_PPS: Special thanks to @nadare for outstanding 8x booster that cut time from 1 hr to under 8 mins!_**","d1d12b91":"### mini EDA\nAs you can see there is a variety of non-random gaps. Large patches when a product disappears from all stores. Gaps when several products disappear. And lots of Store specific supply failures.","a65bdf6c":" [Discussion here.](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/138085#790628)"}}