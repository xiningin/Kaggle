{"cell_type":{"1df85038":"code","2f45a809":"code","7ff5c48a":"code","d9b45d1e":"code","542faa37":"code","0d85c0cd":"code","f39380ba":"code","8ecd0469":"code","f1f55350":"code","9232b8aa":"code","5a781276":"code","412b73e0":"code","584dbb39":"code","f2457bcd":"code","828db25a":"code","0f025c62":"code","cbfdd162":"code","66dff17f":"code","3721d6cb":"code","cd958c2f":"code","9745570f":"markdown","f43bee8a":"markdown","65f1b03c":"markdown","7cd7b654":"markdown","4bb39403":"markdown","291e6224":"markdown","3de5a58c":"markdown","05c4db1f":"markdown","21fbe25a":"markdown","9f769fc5":"markdown","96d1a1f0":"markdown","f74194f5":"markdown","d5ace394":"markdown","d1507458":"markdown","027f330f":"markdown","a9e6e16a":"markdown","1acebf93":"markdown","84710f07":"markdown","2d2f623e":"markdown","571dd254":"markdown","8ce9cf78":"markdown","32c8db6d":"markdown","6152508b":"markdown","12da86c6":"markdown","87a38d4b":"markdown","6960de04":"markdown"},"source":{"1df85038":"import os\nimport numpy as np \nimport pandas as pd \nfrom datetime import datetime\nimport time\nimport random\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nimport matplotlib.pyplot as plt\n\n# torch\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\n\n# torchvision\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\n# sklearn\nfrom sklearn.model_selection import StratifiedKFold\n\n# CV\nimport cv2\n\n# Albumenatations\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Glob\nfrom glob import glob\n\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.","2f45a809":"# Lets check our Hardware first\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n    print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n    print('and then re-execute this cell.')\nelse:\n    print(gpu_info)","7ff5c48a":"n_folds = 5\nseed = 2020\nnum_classes = 2\nBATCH_SIZE = 4\nLR = 0.05\n\n## Use more then 20 Epochs, we go for 20 epochs for demonstration purposes\nEPOCHS = 20\nimg_size_training = 800\n\n## Choose your optimizers:\nAdam = False\nif Adam: \n    Adam_config = {\"lr\" : 0.001, \"betas\" : (0.9, 0.999), \"eps\" : 1e-08}\nelse:\n    SGD_config = {\"lr\" : 0.001, \"momentum\" : 0.9, \"weight_decay\" : 0.001}","d9b45d1e":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nseed_everything(seed)","542faa37":"bboxes_df = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nbboxes_df.head()","0d85c0cd":"# get an array of arrays of all bboxes in the form [[x], [y], [w], [h]]\nbboxs = np.stack(bboxes_df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep = ',')))\n# save the values in the respective column\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    bboxes_df[column] = bboxs[:,i]\nbboxes_df.drop(columns=['bbox'], inplace=True)\nbboxes_df.head()","f39380ba":"# Creating stratified Folds\nskf = StratifiedKFold(n_splits = n_folds, shuffle = True, random_state = seed)\n\ndf_folds = bboxes_df[['image_id']].copy()\ndf_folds.loc[:, 'bbox_count'] = 1\n\n# get number of bboxes for each image_id\ndf_folds = df_folds.groupby('image_id').count()\n\n# add [source] column\ndf_folds.loc[:, 'source'] = bboxes_df[['image_id', 'source']].groupby('image_id').min()['source']\n\n# concat two arays of strings: source and (_bboxcount \/\/ 15) to separate into folds\ndf_folds.loc[:, 'stratify_group'] = np.char.add(\n    df_folds['source'].values.astype(str),\n    df_folds['bbox_count'].apply(lambda x: f'_{x \/\/ 15}').values.astype(str))\ndf_folds.loc[:, 'fold'] = 0\n\nfor fold_number, (train_index, val_index) in enumerate(skf.split(        \n        X = df_folds.index,\n        y = df_folds['stratify_group'])):\n    df_folds.loc[df_folds.iloc[val_index].index, 'fold'] = fold_number","8ecd0469":"DIR_TRAIN = '..\/input\/global-wheat-detection\/train'\n\nclass WheatDataset(Dataset):\n    def __init__(self,image_ids,dataframe,transforms = None):\n        self.image_ids = image_ids\n        self.df = dataframe\n        self.transforms = transforms\n        \n        \n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \n    def __getitem__(self,index):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        image = cv2.imread(f'{DIR_TRAIN}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        \n        # converting from Coco to Pascal_voc format        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        # calculate area of bbox\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # there is only one target class: wheat \n        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)\n        \n        \"\"\"\n        What is 'iscrowd' meaning and doing?:\n        iscrowd: For single, seperable objects iscrowd is set to zero.\n        If we want to do segmentation, for a closeby group or collection of objects\n        in the image, we set iscrowd=1, in which case RLE is used.\n        \"\"\"\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            # if this creates issues, use target['boxes'] = torch.as_tensor(sample['bboxes']) instead of below line\n            # target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n            target['boxes'] = torch.as_tensor(sample['bboxes'])\n            target['boxes'] = target['boxes'].float()\n        \n        return image, target, image_id","f1f55350":"## Albumentations\n\ndef get_train_transforms():\n    return A.Compose(\n        [   A.OneOf([\n                A.HueSaturationValue(hue_shift_limit = 0.2, \n                                     sat_shift_limit = 0.2,\n                                     val_shift_limit = 0.2,\n                                     p = 0.3), \n            \n                A.RandomBrightnessContrast(brightness_limit = 0.2,                                             \n                                           contrast_limit = 0.2,\n                                           p = 0.3),\n                # RGB shift normally expects not-normalized images, so make sure to normalize the RGB shift!\n                A.RGBShift(r_shift_limit = 20\/255, \n                           g_shift_limit = 20\/255, \n                           b_shift_limit = 10\/255,\n                           p = 0.3)\n            ], \n            p = 0.2),\n         \n            A.OneOf([\n                A.RandomGamma(gamma_limit = (80, 120),\n                              p = 0.3),\n                A.Blur(p = 0.6),\n                A.GaussNoise(var_limit = (0.01, 0.05), mean = 0, p = 0.05),\n                A.ToGray(p = 0.05)\n                ],\n                p = 0.1),\n\n            A.OneOf([\n                A.HorizontalFlip(p = 1), \n                A.VerticalFlip(p = 1),  \n                A.Transpose(p = 1),                \n                A.RandomRotate90(p = 1)\n                ], \n                p = 0.7),  \n         \n            A.RandomFog(fog_coef_lower = 0.1,\n                        fog_coef_upper = 0.2,\n                        p = 0.02),   \n         \n            A.RandomSizedBBoxSafeCrop(img_size_training, \n                                      img_size_training, \n                                      p = 0.05),         \n            A.Resize(height = img_size_training, \n                     width = img_size_training, \n                     p = 1),\n         \n            A.Cutout(num_holes = random.randint(1, 6),\n                     max_h_size = 64, \n                     max_w_size = 64,\n                     fill_value = 0, \n                     p = 0.15),\n         \n            ToTensorV2(p = 1.0),\n        ],\n        p = 1.0, bbox_params = {'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transforms():\n    return A.Compose([\n        A.Resize(height = img_size_training,\n                 width = img_size_training,\n                 p = 1),\n        \n        ToTensorV2(p = 1.0)\n    ], bbox_params = {'format': 'pascal_voc', 'label_fields': ['labels']})","9232b8aa":"## In several occasions in the next sections we will need to create a dataset and a dataloader.\n# In this section we create helper functions for this to stay DRY (dont repeast yourself).\n\ndef get_validation_datset(df):\n    return WheatDataset(\n    image_ids = df.index.values,\n    dataframe = bboxes_df,\n    transforms = get_valid_transforms()\n    )\n    \ndef get_train_datset(df):    \n    return WheatDataset(\n    image_ids = df.index.values,\n    dataframe = bboxes_df,\n    transforms = get_train_transforms()\n    )\n\ndef get_validation_data_loader(valid_dataset, batch_size = 16):\n    return DataLoader(\n    valid_dataset,\n    batch_size = batch_size,\n    shuffle = False,\n    num_workers = 4,\n    collate_fn = collate_fn\n    )    \n\ndef get_train_data_loader(train_dataset, batch_size = 16):\n    return DataLoader(\n    train_dataset,\n    batch_size = batch_size,\n    shuffle = False,\n    num_workers = 4,\n    collate_fn = collate_fn\n    ) \n\ndef collate_fn(batch):\n    return tuple(zip(*batch))","5a781276":"## Lets check some augmentations!\n\n# get a fold\ndf_train = df_folds[df_folds['fold'] != 0]\n\n# create train dataset and data-loader\ntrain_dataset = get_train_datset(df_train)\ntrain_data_loader = get_train_data_loader(train_dataset, batch_size = 16)\n\ndevice=torch.device('cuda')\nimages, targets, image_ids = next(iter(train_data_loader))\n\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n# plot some augmentations!\nfig, ax = plt.subplots(figsize=(20, 20),  \n                       nrows = 4,\n                       ncols = 4)\nfor i in range (16):    \n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1,2,0).cpu().numpy()\n    for box in boxes:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n    \n    ax[i \/\/ 4][i % 4].imshow(sample)   ","412b73e0":"def get_model():    \n    # load pre-trained model incl. head\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n    # get number of input features for the classifier custom head\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    return model","584dbb39":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    # initialize and reset to zero\n    def __init__(self):\n        self.reset()\n\n    # reset everything to zero    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    # update loss after each batch\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","f2457bcd":"def train_fn(data_loader,model,optimizer,device,scheduler,epoch):\n    # set pytorch to train mode, in which the model will return losses\n    model.train()\n        \n    summary_loss = AverageMeter()\n    \n    tk0 = tqdm(data_loader, total = len(data_loader))\n    \n    # actual training steps\n    for step, (images, targets, image_ids) in enumerate(tk0):   \n        \n        # if targets is empty, create empty target tensor        \n        if len(targets) < 1:\n            target['boxes'] = torch.zeros((0, 4))\n            \n        # prepare data\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]        \n\n        # calculate loss        \n        loss_dict = model(images, targets)        \n        losses = sum(loss for loss in loss_dict.values())\n                \n        # resets gradients of all optimized Tensors before next training step\n        optimizer.zero_grad() \n        \n        # computes derivative of the loss w.r.t. the parameters \n        losses.backward() \n        \n        # updates the parameter based on the gradients from previous step\n        optimizer.step()\n        \n        # updates the scheduler if it's defined\n        if scheduler is not None:\n            scheduler.step()\n            \n        summary_loss.update(losses.item(),BATCH_SIZE)\n        tk0.set_postfix(loss = summary_loss.avg)\n        \n    return summary_loss","828db25a":"def calculate_iou(gt, pr, form='pascal_voc') -> float:\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt\/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n\n    return overlap_area \/ union_area\n\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred, form = form)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fn = (gts.sum(axis=1) > 0).sum()\n\n    return tp \/ (tp + fp + fn)\n\n\n\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n    \"\"\"Calculates image precision.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n\n    for threshold in thresholds:\n        precision_at_threshold = calculate_precision(gts.copy(), \n                                                     preds, \n                                                     threshold = threshold,\n                                                     form = form,\n                                                     ious = ious)\n        image_precision += precision_at_threshold \/ n_threshold\n\n    return image_precision\n","0f025c62":"def eval_fn(data_loader, model, device):\n    model.eval()\n    summary_loss_eval = AverageMeter()\n    iou_thresholds = [x for x in np.arange(0.5, 0.76, 0.05)]\n    validation_image_precisions = []\n    \n    with torch.no_grad():\n        \n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for step, (images, targets, image_ids) in enumerate(tk0):\n            \n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            \"\"\"\n            In model.train() mode, model(images)  is returning losses.\n            We are using model.eval() mode --> it will return boxes and scores. \n            \"\"\"\n            outputs = model(images)\n\n            for i, image in enumerate(images):\n                boxes = outputs[i]['boxes'].data.cpu().numpy()\n                scores = outputs[i]['scores'].data.cpu().numpy()\n                gt_boxes = targets[i]['boxes'].cpu().numpy()\n                preds_sorted_idx = np.argsort(scores)[::-1]\n                preds_sorted = boxes[preds_sorted_idx]\n                image_precision = calculate_image_precision(preds_sorted,\n                                                            gt_boxes,\n                                                            thresholds = iou_thresholds,\n                                                            form = 'pascal_voc')\n                validation_image_precisions.append(image_precision)\n\n    valid_prec = np.mean(validation_image_precisions)\n        \n    return valid_prec","cbfdd162":"def run(fold):\n    summary_loss = AverageMeter()\n    \n    # split data in training and validation set based on the fold\n    df_train = df_folds[df_folds['fold'] != fold]\n    df_valid = df_folds[df_folds['fold'] == fold]\n    \n    # get data loaders\n    train_dataset = get_train_datset(df_train)\n    train_data_loader = get_train_data_loader(train_dataset, batch_size = BATCH_SIZE)\n    \n    validation_dataset = get_validation_datset(df_valid)\n    validation_data_loader = get_validation_data_loader(validation_dataset, batch_size = 16)\n          \n    # get GPU device\n    device = torch.device('cuda')\n    \n    # get model\n    model = get_model()\n    model = model.to(device)\n    \n    # get all trainable parameters for the optimizer\n    params = [p for p in model.parameters() if p.requires_grad]\n    \n    # get the configured optimizer\n    if Adam:\n        optimizer = torch.optim.Adam(params, **Adam_config)\n    else:\n        optimizer = torch.optim.SGD(params, **SGD_config)\n        \n    best_precision = 0.01\n    for epoch in range(EPOCHS):\n        \n        # reset loss for each new epoch\n        summary_loss.reset()\n                                \n        train_loss = train_fn(train_data_loader, \n                              model, optimizer, \n                              device, \n                              scheduler = None, \n                              epoch = epoch)\n        \n        valid_precision = eval_fn(validation_data_loader, \n                                  model,\n                                  device)\n        \n        print('|EPOCH {}| TRAIN_LOSS {}| VAL_PRECISION {}|'.format(\n            epoch+1,\n            train_loss.avg,\n            valid_precision))\n        \n        if valid_precision > best_precision:\n            best_precision = valid_precision\n            print('Best model found for Epoch {}'.format(epoch+1))\n            torch.save(model.state_dict(), f'FRCNN_best_{fold}.pth')","66dff17f":"run(fold = 0)","3721d6cb":"def view_sample(df_valid,model,device):\n    '''\n    Code merged from Peter's & KnowNothing's great Kernels\n    https:\/\/www.kaggle.com\/pestipeti\/pytorch-starter-fasterrcnn-train\n    https:\/\/www.kaggle.com\/tanulsingh077\/end-to-end-object-detection-with-transformers-detr\n    '''\n    \n    validation_dataset = get_validation_datset(df_valid)\n    validation_data_loader = get_validation_data_loader(validation_dataset, batch_size = 16)\n    \n    # not needed\n    # images, targets, image_ids = next(iter(validation_data_loader))\n    # images = list(img.to(device) for img in images)\n    # targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    #_,h,w = images[0].shape # for de normalizing images\n    \n    images, targets, image_ids = next(iter(validation_data_loader))\n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    boxes_gt = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[0].permute(1,2,0).cpu().numpy()\n    \n    model.to(device)\n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n    #model.cuda()\n\n    outputs = model(images)\n    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n    \n    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n    # paint ground_truth boxes (red)\n    for box in boxes_gt:\n        cv2.rectangle(sample,\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (220, 0, 0), 3)\n        \n    boxes = outputs[0]['boxes'].data.cpu().numpy()\n    scores = outputs[0]['scores'].data.cpu().numpy()\n          \n    # paint predicted boxes (blue)\n    pred_box_count = 0\n    for box,p in zip(boxes,scores):        \n        if p >0.5:\n            pred_box_count += 1\n            color = (0,0,220) \n            cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  color, 2)\n    \n    \n    print(f\"Predicted {pred_box_count} BBoxes (blue); Number of GT BBoxes (red) : {len(boxes_gt)}\")\n    ax.set_axis_off()\n    ax.imshow(sample)       \n\n  ","cd958c2f":"model = get_model()\n# load the best saved weights\nmodel.load_state_dict(torch.load(\".\/FRCNN_best_0.pth\"))\n# show a prediction\nview_sample(df_folds[df_folds['fold'] == 0],\n            model = model,\n            device = torch.device('cuda'))","9745570f":"### EDITS\/REWORKS:\nV12: Corrections, enhanced readability\n\nV10: Typo correction, slight changes & additions  \n\nV9: fixed a bug in Inference (used predictions for the wrong image) --> now predictions amtch ground-truth much better","f43bee8a":"It's always strongly recommended to check your input pictures into your model after augmentations, to ensure that not strange things happen.\n\nI've seen (and also accidently used)  \n```A.RGBShift(r_shift_limit = 20, \n            g_shift_limit = 20,\n            b_shift_limit = 10,\n            p = 0.3)```    \nin a training pipeline for normalized pictures, and guess what happend: They turned in completely yellow, blue or red. Why? Because RGBSHift expectes not-normalized pictures! So be sure you normalize RGB shift to get valid input data -->  \n```A.RGBShift(r_shift_limit = 20\/255, \n            g_shift_limit = 20\/255,\n            b_shift_limit = 10\/255, \n            p = 0.3)```","65f1b03c":"## Importing dependencies\n<span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> from  <\/span> coffee  <span style=\"color:darkgreen ;font-family: Impact; font-size:13;\"> import  <\/span> ***** ","7cd7b654":"As mentioned in the introduction, we are using the default [ResNet50 pytroch implementation](https:\/\/kite.com\/python\/docs\/torchvision.models.detection.fasterrcnn_resnet50_fpn) here.\nWe will start with defining a function to load and get the model, as we will be doing this several times in the later sections. Next, we will define an AverageMeter, tracking of the loss of our model.\nThen we can finally set up the training.","4bb39403":"In order to select the best model, epoch & weights, we need to create a valid competition metric.\nIn this case, we take the very well coded version from [this notebook](https:\/\/www.kaggle.com\/pestipeti\/competition-metric-details-script). ","291e6224":"# Creating Dataset\n## Let's raise a glass* to create the Dataset!\n![champaign-clipart-champagne-glass-6.png](attachment:champaign-clipart-champagne-glass-6.png)\n\n*class","3de5a58c":"### The model works as expected: \nThe loss is continuously dropping, the VAL_PRECISION is continuously increasing.\nUsing more epochs will lead to much better results and only the best model is saved in your OUTPUT folder.","05c4db1f":"And then unpack the bbox coordinates into seperate columns x, y, w, h.","21fbe25a":"# Sample visualization of predictions\n\nIn this step we load the best saved model and make our predictions.","9f769fc5":"# Evaluation Function","96d1a1f0":"# Training Function\n\n","f74194f5":"In this section we will do the actual training, comining the previous defined functions to a full pipeline.","d5ace394":"# Validate Augmentations","d1507458":"# Seed Everything\n\nSeeding everything for reproducible results.","027f330f":"As expected, using less than 20 Epochs is not enough. Good results can be achived with 40+ Epochs, mostly using Colab for longer notebook availability.","a9e6e16a":"# Augmentations\nDue to the limited size of the dataset and the fact, that Imagenet pre-trained FasterRCNNs don't know a lot about Wheathead-Detection yet, we will use a lot of different augmentations.\nBut: More augmentations are not always leading to better results. Also the augmentations need to be somewhat reasonable. So be careful to not augment every single picture multiple times: Using Albumentations ```OneOf``` method helps a lot here.\nUsing [Cutmix](https:\/\/arxiv.org\/abs\/1905.04899) in this competition is a very good idea to ensure good regulariazion\/generalization of your network.\n\nHowever, there are always things you should not do:\n![](https:\/\/pbs.twimg.com\/media\/Ea4T7DxWsAEi4Ga.jpg)\n","1acebf93":"# Engine","84710f07":"![WHEAT_whatsapp-min.jpg](attachment:WHEAT_whatsapp-min.jpg)","2d2f623e":"# Conclusion\n\nAs Yolov5 is not really allowed due to its GPLv3 license (see [discussions](https:\/\/www.kaggle.com\/c\/global-wheat-detection\/discussion\/163433)), most probably FasterRCCN, EfficientDet or a combination of their results will win this competition. \n\nThank you for reading!","571dd254":"# Faster RCNN Intro\n## Architectural overview\n![Architektur+#.PNG](attachment:Architektur+#.PNG)\n\n\n","8ce9cf78":"# Preparing the Data\n\nFor creating a cross-validation strategy we use code from Alex's awesome [kernel](https:\/\/www.kaggle.com\/shonenkov\/training-efficientdet).\nWe will create a cross-validation split in which the folds are made by preserving the percentage of samples (= stratisfied!) for each of the following \"classes\":\n* number of boxes \n* source\n\nFor this we first check the data:","32c8db6d":"## Quick Intro: Dear Farmgirls, dear Farmboys..\nthis is my first object detection Kaggle competition! \nPlease feel free to give me some hints for improvements in all categories: making code more \"pythonic\", better graphics, ideas for pre & post-processing, etc.\n","6152508b":"# Model\n\n","12da86c6":"## How Faster-RCNN works:  \n\n1) **Region Proposal Network** (RPN) step 1: Run the image through a CNN to get a **Feature Map** and project Anchors or BBoxes onto this map.  \n\n2) (RPN) step 2: Keep only the best anchors (e.g. top 2000), and discard the rest. This is achieved using only anchors with IoU < 0.4 for background and > 0.7 for objects and after that  applying non-max suppression.  \n\n3) Do **Region of interest** pooling ([ROI Pooling](https:\/\/blog.deepsense.ai\/region-of-interest-pooling-explained\/): this leads to uniformly sized outputs which can be used for classification and BBOX regression  \n\n4) For the **Classification** task ( = is it background or object?) binary cross entropy (BCE) is used to calculate the loss.   \n\n5) The **BBOX regressor** calculates offsets based on the anchors from step 2) and the regression loss is calculated.  \n\n\nMore in depth literatur on how anchor boxes are created and how ROI pooling works in detail can be found here:\nhttps:\/\/tryolabs.com\/blog\/2018\/01\/18\/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection\/  \nhttps:\/\/medium.com\/@smallfishbigsea\/faster-r-cnn-explained-864d4fb7e3f8  \nhttps:\/\/medium.com\/@whatdhack\/a-deeper-look-at-how-faster-rcnn-works-84081284e1cd  \nhttps:\/\/towardsdatascience.com\/fasterrcnn-explained-part-1-with-code-599c16568cff    \n\n\n## Special info about the pytroch implementation we are going to use.\n* As you can see in many [github implementations](https:\/\/github.com\/pytorch\/vision\/blob\/d2c763e14efe57e4bf3ebf916ec243ce8ce3315c\/torchvision\/models\/detection\/faster_rcnn.py#L227) and the (kinda hard to find) [official pytroch implementation](https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/models\/detection\/faster_rcnn.html#fasterrcnn_resnet50_fpn), the minimum Image size for FasterRCNN is 800 and the maximum is 1333. So resizing to below 800 (most notebooks use 512 for training) before feeding it in the network is not only unnecessary computational cost, but also decreasing the information contained in the pictures.\n* Images should not be normalized during augmentation-process or in your Dataset-class. Subtracting the mean and dividing by std. deviation is automatically done in the [GeneralizedRCNNTransform](https:\/\/github.com\/pytorch\/vision\/blob\/39021408587eb252ebb842a54195d840d6d76095\/torchvision\/models\/detection\/transform.py)-Class which is used by the model itself! \n","87a38d4b":"# Competition description\nOpen up your pantry and you\u2019re likely to find several wheat products. Indeed, your morning toast or cereal may rely upon this common grain. Its popularity as a food and crop makes wheat widely studied. To get large and accurate data about wheat fields worldwide, plant scientists use image detection of \"wheat heads\"\u2014spikes atop the plant containing grain. These images are used to estimate the density and size of wheat heads in different varieties. Farmers can use the data to assess health and maturity when making management decisions in their fields.\nHowever, accurate wheat head detection in outdoor field images can be visually challenging. There is often overlap of dense wheat plants, and the wind can blur the photographs. Both make it difficult to identify single heads. Additionally, appearances vary due to maturity, color, genotype, and head orientation. Finally, because wheat is grown worldwide, different varieties, planting densities, patterns, and field conditions must be considered.\n\nWheat is a staple across the globe, which is why this competition must account for different growing conditions. Models developed for wheat phenotyping need to be able to generalize between environments. If successful, researchers can accurately estimate the density and size of wheat heads in different varieties. With improved detection farmers can better assess their crops, ultimately bringing cereal, toast, and other favorite dishes to your table.\n\n### What should I expect the data format to be?\nThe Global Wheat Head Dataset is led by nine research institutes from seven countries: the University of Tokyo, Institut national de recherche pour l\u2019agriculture, l\u2019alimentation et l\u2019environnement, Arvalis, ETHZ, University of Saskatchewan, University of Queensland, Nanjing Agricultural University, and Rothamsted Research.\nThe Data needs to be submitted in Coco-style format: x, y, width and height.\n\n### What am I predicting?\nIn this competition, you\u2019ll detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, you will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. You will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.\n\n","6960de04":"# Configuration\n\n## Basic configuration for this model\n\nBelow you can change the base configuration, the impact on CV and LB score is quite high! Be careful to not choose a too high learning rate."}}