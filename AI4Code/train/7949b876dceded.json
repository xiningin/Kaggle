{"cell_type":{"8552decb":"code","a1a5b6ce":"code","f307933e":"code","0252e0a3":"code","ceddbe91":"code","ce87a1d9":"code","51e70ac8":"code","4b66c729":"code","86a60827":"code","ba0728c0":"code","34be204a":"code","9eae29ae":"code","13674f30":"code","2675e8b1":"code","c84c8c1f":"code","0fe4e2e9":"code","31f570d4":"code","43155c00":"code","3f118244":"code","641934b7":"code","13b633ff":"code","8e896f79":"code","05ec0747":"code","ad05a94f":"code","890a7555":"code","9ab60506":"code","ddc7ec85":"code","de8f3bf5":"code","8accb463":"code","a03700c4":"code","1cfbd543":"code","88e9910c":"code","01dd5e6f":"code","63b949c1":"code","5e6823a2":"code","40e58d66":"code","6f6544d9":"code","9eb9ef21":"code","e0bf35c4":"code","e4bdc2f4":"code","c763ba28":"code","e057cb29":"code","d46a79d9":"code","5cf9256b":"code","e013c716":"code","ad08587d":"code","c483e294":"code","43611e4e":"code","59574de8":"code","1990ff74":"code","717b0ade":"code","2e795d49":"code","b648136c":"code","1e6dc1f2":"code","718c2cdd":"code","d3e3725d":"code","2eb82b20":"code","6e98e4d1":"code","7c437177":"code","3e58140a":"code","19efa61b":"code","df3d4ce3":"code","9c0872b4":"code","c828f799":"code","93ed9af8":"code","1e9841d9":"code","e470bcd0":"code","9f43ceea":"code","127a9791":"markdown","77bb3c4f":"markdown","140d1cf8":"markdown","4dd8bc0d":"markdown","b2713c8e":"markdown","b139f687":"markdown","1a083887":"markdown","09f68c0d":"markdown","e5c42b79":"markdown","177dfc35":"markdown","be6f50f9":"markdown","4375c783":"markdown","04aa02fe":"markdown","ddfb5ce0":"markdown","c3c31ac5":"markdown","ccebd85e":"markdown","22a37310":"markdown","14b9cf51":"markdown","672f13e0":"markdown","0db8ee8a":"markdown","e4118f9f":"markdown","3bc2cfc7":"markdown","5f7d175e":"markdown","1523c07f":"markdown","da1b28de":"markdown","4a182eae":"markdown","f85eccb5":"markdown","c3217041":"markdown","0be62195":"markdown","0056b55c":"markdown","1523d4d8":"markdown","3ab47895":"markdown","a26b4013":"markdown","243cc704":"markdown","cc1e7509":"markdown","0dbec1fc":"markdown","30296216":"markdown","226494c6":"markdown","0405f559":"markdown","560d29d1":"markdown","cc2ab92c":"markdown"},"source":{"8552decb":"#import the needed packages\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import classification_report,roc_auc_score, confusion_matrix, accuracy_score,recall_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split,GridSearchCV\n#from sklearn.cross_validation import cross_val_predict\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import make_scorer\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nimport scipy.stats as stats\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\n%matplotlib inline\nimport warnings\n\nimport sklearn\nimport scipy\n\nimport sys\nimport os\n\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","a1a5b6ce":"import warnings\nwarnings.filterwarnings(\"ignore\")","f307933e":"# Import the data to bank_df\nbank_df=pd.read_csv(\"..\/input\/Bank_Personal_Loan_Modelling.csv\")","0252e0a3":"bank_df.head(10)","ceddbe91":"# Shape of training and test data set\ndef dataframe_shape(df):\n    print(\"The dataframe has %d rows\" %df.shape[0])\n    print(\"The dataframe has %d columns\" %df.shape[1])\n\ndataframe_shape(bank_df)","ce87a1d9":"# Columns\/Feature in dataset\npd.DataFrame(bank_df.columns,index=None,copy=False).T","51e70ac8":"# First 3 observation\nbank_df.head(3) # you can choose any number of rows by changing the number inside head function. Default it shows 5","4b66c729":"# Last 3 observation\nbank_df.tail(3) # you can choose any number of rows by changing the number inside tail function. Default it shows 5","86a60827":"# Random 3 observation\nbank_df.sample(3) # you can choose any number of rows by changing the number inside sample function. Default it shows 1","ba0728c0":"# datatypes present into training dataset\ndef datatypes_insight(data):\n    display(data.dtypes.to_frame().T)\n    data.dtypes.value_counts().plot(kind=\"barh\")\n\ndatatypes_insight(bank_df)","34be204a":"# Missing value identification\n\ndef Nan_value(data):\n    display(data.apply(lambda x: sum(x.isnull())).to_frame().T)\n    ##data.apply(lambda x: sum(x.isnull())).plot(kind=\"barh\")\n\nNan_value(bank_df)","9eae29ae":"# Ploting the NAN values if any.\nsns.heatmap(bank_df.isna(),yticklabels=False,cbar=False,cmap='viridis')","13674f30":"# Unique values in features\ndef unique_data(data):\n    display(data.apply(lambda x: len(x.unique())).to_frame().T)\n    data.apply(lambda x: len(x.unique())).plot(kind=\"barh\")\n\nunique_data(bank_df)","2675e8b1":"# check for imbalance dataset\nfig, ax = plt.subplots(nrows=1, ncols=2,squeeze=True)\nfig.set_size_inches(14,6)\nfrequency_colums= pd.crosstab(index=bank_df[\"Personal Loan\"],columns=\"count\")\nfrequency_colums.plot(kind='bar',ax=ax[0],color=\"c\",legend=False,rot=True,fontsize=10)\nfrequency_colums.plot(kind='pie',ax=ax[1],subplots=True,legend=False,fontsize=10,autopct='%.2f')\nax[0].set_title('Frequency Distribution of Dependent variable: Survived',fontsize=10)\nax[1].set_title('Pie chart representation of Dependent variable: Survived',fontsize=10)\n\n#adding the text labels\nrects = ax[0].patches\nlabels = frequency_colums[\"count\"].values\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax[0].text(rect.get_x() + rect.get_width()\/2, height +1,label, ha='center', va='bottom',fontsize=10)\nplt.show()","c84c8c1f":"#statistical analysis of data set\nbank_df.describe().T","0fe4e2e9":"def distploting(df):\n    col_value=df.columns.values.tolist()\n    sns.set(context='notebook',style='whitegrid', palette='dark',font='sans-serif',font_scale=1.2,color_codes=True)\n    \n    fig, axes = plt.subplots(nrows=7, ncols=2,constrained_layout=True)\n    count=0\n    for i in range (7):\n        for j in range (2):\n            s=col_value[count+j]\n            #axes[i][j].hist(df[s].values,color='c')\n            sns.distplot(df[s].values,ax=axes[i][j],bins=30,color=\"c\")\n            axes[i][j].set_title(s,fontsize=17)\n            fig=plt.gcf()\n            fig.set_size_inches(8,20)\n            plt.tight_layout()\n        count=count+j+1\n        \n             \ndistploting(bank_df)","31f570d4":"bank_df[['CreditCard', 'Personal Loan']].groupby(['CreditCard'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","43155c00":"bank_df[['Online', 'Personal Loan']].groupby(['Online'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","3f118244":"bank_df[['Family', 'Personal Loan']].groupby(['Family'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","641934b7":"bank_df[['Education', 'Personal Loan']].groupby(['Education'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","13b633ff":"bank_df[['CD Account', 'Personal Loan']].groupby(['CD Account'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","8e896f79":"bank_df[['Securities Account', 'Personal Loan']].groupby(['Securities Account'], as_index=False).mean().sort_values(by='Personal Loan', ascending=False)","05ec0747":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'Income', bins=20)","ad05a94f":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'Mortgage', bins=20)","890a7555":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'CCAvg', bins=20)","9ab60506":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'Age', bins=20)","ddc7ec85":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'Experience', bins=20)","de8f3bf5":"grid = sns.FacetGrid(bank_df, col='Personal Loan', row='Education', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Income', alpha=.5, bins=20)\ngrid.add_legend();","8accb463":"grid = sns.FacetGrid(bank_df, col='Personal Loan', row='Family', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Income', alpha=.5, bins=20)\ngrid.add_legend();","a03700c4":"grid = sns.FacetGrid(bank_df, col='Personal Loan', row='Online', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Income', alpha=.5, bins=20)\ngrid.add_legend();","1cfbd543":"grid = sns.FacetGrid(bank_df, col='Personal Loan', row='CreditCard', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Income', alpha=.5, bins=20)\ngrid.add_legend();","88e9910c":"grid = sns.FacetGrid(bank_df, col='Personal Loan', row='Family', size=2.5, aspect=1.6)\ngrid.map(plt.hist, 'Mortgage', alpha=.5, bins=20)\ngrid.add_legend();","01dd5e6f":"# Compare the Age, Exp and Education for the person\npd.DataFrame(bank_df[bank_df[\"Experience\"]>0][[\"Age\",\"Education\",\"Experience\"]].sort_values(\"Age\")).head()","63b949c1":"#Lets see if we have any relationship bewteen Exp and Age\ndf = pd.DataFrame(bank_df.groupby(\"Age\").mean()[\"Experience\"]).reset_index()\nfig.set_size_inches(20,6)\nsns.lmplot(x='Age',y='Experience',data=df)\nplt.ylabel(\"Experience(Mean)\")\nplt.title(\"Mean Experience by Age\")\nplt.show()","5e6823a2":"# From the plot, we can see Age and Experience has linear relationship.\n#In data set the value was correct but it was captured with wrong sign.let replace the values with absolute value.\nbank_df[\"Experience\"] = bank_df[\"Experience\"].apply(abs)","40e58d66":"bank_df[\"PP_income_M\"] = (((bank_df[\"Income\"]*1000)\/12)-((bank_df[\"CCAvg\"]*1000)\/12))","6f6544d9":"g = sns.FacetGrid(bank_df, col='Personal Loan')\ng.map(plt.hist,'PP_income_M', bins=20)","9eb9ef21":"bank_df = bank_df.drop(['ID','ZIP Code'], axis=1)","e0bf35c4":"corr = bank_df.corr()\n\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nplt.figure(figsize=(20,15))\nsns.heatmap(corr, mask=mask,annot=True,square=True,cmap=\"coolwarm\")","e4bdc2f4":"plt.figure(figsize=(20, 20))\nsns.pairplot(bank_df,hue=\"Personal Loan\")","c763ba28":"from sklearn.preprocessing import MinMaxScaler,StandardScaler,robust_scale\nscaler = StandardScaler();\n\ncolscal=['Age', 'Experience', 'Income', 'CCAvg','PP_income_M']\n\nscaler.fit(bank_df[colscal])\nscaled_bank_df = pd.DataFrame(scaler.transform(bank_df[colscal]),columns=colscal)\n\nbank_df =bank_df.drop(colscal,axis=1)\nbank_df = scaled_bank_df.join(bank_df)","e057cb29":"X=bank_df[['Age','Experience', 'Income', 'Family', 'CCAvg', 'Education', 'Securities Account', 'CD Account', 'Online',\n       'CreditCard','PP_income_M']]\ny=bank_df[\"Personal Loan\"]","d46a79d9":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=101)","5cf9256b":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","e013c716":"predict = logmodel.predict(X_test.values)\npredictProb = logmodel.predict_proba(X_test.values)\nacc_log=round(metrics.accuracy_score(predict,y_test)*100,2)","ad08587d":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(logmodel, open(filename, 'wb'))","c483e294":"print(\"**\"*40)\nprint('The accuracy of the Logistic is',metrics.accuracy_score(predict,y_test))\nprint(\"__\"*40)\nprint(\"confusion_matrix :\\n\",confusion_matrix(y_test, predict))\nprint(\"__\"*40)\nprint(\"\\nclassification_report :\\n\",classification_report(y_test, predict))\nprint(\"__\"*40)\nprint('Recall Score',recall_score(y_test, predict))\nprint('ROC AUC :', roc_auc_score(y_test, predictProb[:,1]))\nprint('Accuracy :',accuracy_score(y_test, predict))\nprint(\"**\"*40)","43611e4e":"score1 =cross_val_score(X=X,y=y,estimator=logmodel,scoring=\"recall\",cv=10)\nscore2 =cross_val_score(X=X,y=y,estimator=logmodel,scoring=\"roc_auc\",cv=10)\nscore3 =cross_val_score(X=X,y=y,estimator=logmodel,scoring=\"accuracy\",cv=10)\nscore4 =cross_val_score(X=X,y=y,estimator=logmodel,scoring=\"f1\",cv=10)\nscore5 =cross_val_score(X=X,y=y,estimator=logmodel,scoring=\"average_precision\",cv=10)","59574de8":"print(\"**\"*40)\nprint(\"Logistic Regression Cross Validation:\")\nprint(\"\\nCross Validation Recall :\",score1.mean())\nprint(\"Cross Validation Roc Auc :\",score2.mean())\nprint(\"Cross Validation accuracy :\",score3.mean())\nprint(\"Cross Validation f1 :\",score4.mean())\nprint(\"Cross Validation average_precision :\",score5.mean())\nprint(\"**\"*40)","1990ff74":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)","717b0ade":"predict = knn.predict(X_test.values)\npredictProb = knn.predict_proba(X_test.values)\nacc_knn=round(metrics.accuracy_score(predict,y_test)*100,2)","2e795d49":"print(\"**\"*40)\nprint('The accuracy of the KNN is',metrics.accuracy_score(predict,y_test))\nprint(\"__\"*40)\nprint(\"confusion_matrix :\\n\",confusion_matrix(y_test, predict))\nprint(\"__\"*40)\nprint(\"\\nclassification_report :\\n\",classification_report(y_test, predict))\nprint(\"__\"*40)\nprint('Recall Score',recall_score(y_test, predict))\nprint('ROC AUC :', roc_auc_score(y_test, predictProb[:,1]))\nprint(\"**\"*40)","b648136c":"score1 =cross_val_score(X=X,y=y,estimator=knn,scoring=\"recall\",cv=10)\nscore2 =cross_val_score(X=X,y=y,estimator=knn,scoring=\"roc_auc\",cv=10)\nscore3 =cross_val_score(X=X,y=y,estimator=knn,scoring=\"accuracy\",cv=10)\nscore4 =cross_val_score(X=X,y=y,estimator=knn,scoring=\"f1\",cv=10)\nscore5 =cross_val_score(X=X,y=y,estimator=knn,scoring=\"average_precision\",cv=10)","1e6dc1f2":"print(\"KNN Cross Validation:\")\nprint(\"**\"*40)\nprint(\"\\nCross Validation Recall :\",score1.mean())\nprint(\"Cross Validation Roc Auc :\",score2.mean())\nprint(\"Cross Validation accuracy :\",score3.mean())\nprint(\"Cross Validation f1 :\",score4.mean())\nprint(\"Cross Validation average_precision :\",score5.mean())\nprint(\"**\"*40)","718c2cdd":"from sklearn.model_selection import GridSearchCV\nk = np.arange(1,10,1)","d3e3725d":"parameters = {'n_neighbors': k, \n              'weights': [\"uniform\",\"distance\"], \n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n             }\n\nacc_scorer = make_scorer(accuracy_score)","2eb82b20":"grid_obj = GridSearchCV(knn, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)","6e98e4d1":"print(\"**\"*40)\nprint('The accuracy of the KNN is',metrics.accuracy_score(predict,y_test))","7c437177":"predict = grid_obj.predict(X_test.values)\npredictProb = grid_obj.predict_proba(X_test.values)","3e58140a":"print(\"**\"*40)\nprint('The accuracy of the KNN with GridSearchCV is',metrics.accuracy_score(y_test,predict))\nprint(\"__\"*40)\nprint(\"confusion_matrix :\\n\",confusion_matrix(y_test, predict))\nprint(\"__\"*40)\nprint(\"\\nclassification_report :\\n\",classification_report(y_test, predict))\nprint(\"__\"*40)\nprint('Recall Score',recall_score(y_test, predict))\nprint('ROC AUC :', roc_auc_score(y_test, predictProb[:,1]))\nprint('Accuracy :',accuracy_score(y_test, predict))\nprint(\"**\"*40)","19efa61b":"from sklearn import model_selection\n# subsetting just the odd ones\nneighbors = list(np.arange(1,20,2))\n\n# empty list that will hold cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores =model_selection.cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())","df3d4ce3":"# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint (\"The optimal number of neighbors is %d\" % optimal_k)\n\n# plot misclassification error vs k\nplt.plot(neighbors,MSE)\nlocator = matplotlib.ticker.MultipleLocator(2)\nplt.gca().xaxis.set_major_locator(locator)\nformatter = matplotlib.ticker.StrMethodFormatter(\"{x:.0f}\")\nplt.gca().xaxis.set_major_formatter(formatter)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","9c0872b4":"gb = GaussianNB()\ngb.fit(X_train, y_train)","c828f799":"predict = gb.predict(X_test)\npredictProb = gb.predict_proba(X_test)\nacc_nb=round(metrics.accuracy_score(predict,y_test)*100,2)","93ed9af8":"print(\"**\"*40)\nprint('The accuracy of the Na\u00efve Bayes is',metrics.accuracy_score(predict,y_test))\nprint(\"__\"*40)\nprint(\"confusion_matrix :\\n\",confusion_matrix(y_test, predict))\nprint(\"__\"*40)\nprint(\"\\nclassification_report :\\n\",classification_report(y_test, predict))\nprint(\"__\"*40)\nprint('Recall Score',recall_score(y_test, predict))\nprint('ROC AUC :', roc_auc_score(y_test, predictProb[:,1]))\nprint('Accuracy :',accuracy_score(y_test, predict))\nprint(\"**\"*40)","1e9841d9":"score1 =cross_val_score(X=X,y=y,estimator=gb,scoring=\"recall\",cv=10)\nscore2 =cross_val_score(X=X,y=y,estimator=gb,scoring=\"roc_auc\",cv=10)\nscore3 =cross_val_score(X=X,y=y,estimator=gb,scoring=\"accuracy\",cv=10)\nscore4 =cross_val_score(X=X,y=y,estimator=gb,scoring=\"f1\",cv=10)\nscore5 =cross_val_score(X=X,y=y,estimator=gb,scoring=\"average_precision\",cv=10)","e470bcd0":"print(\"Na\u00efve Bayes Cross Validation:\")\nprint(\"**\"*40)\nprint(\"\\nCross Validation Recall :\",score1.mean())\nprint(\"Cross Validation Roc Auc :\",score2.mean())\nprint(\"Cross Validation accuracy :\",score3.mean())\nprint(\"Cross Validation f1 :\",score4.mean())\nprint(\"Cross Validation average_precision :\",score5.mean())\nprint(\"**\"*40)","9f43ceea":"models = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression','Naive Bayes'],\n    'Score': [acc_knn, acc_log, acc_nb, \n              ]})\nmodels.sort_values(by='Score', ascending=False)","127a9791":"## Background:\n\nThis case is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are liability customers (depositors) with varying size of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\n\nBank provided data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.","77bb3c4f":"# Assumtions based on data analysis\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\n#### Correlating.\n\nWe want to know how well does each feature correlate with personal loan acceptance. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n#### Creating.\n\n1. We may want to engineer the Mortgage & income feature to see the mortgage raitio vs income(monthly).\n2. We may want to engineer the Income & CCAvg feature to see the CCAvg raitio vs income(monthly).\n3. We may want to create new feature for Age and Experience bands. This turns a continous numerical feature into an ordinal categorical feature.","140d1cf8":"#### These values classify the samples into sets of similar samples. Within categorical features are the values nominal, ordinal, ratio, or interval based.\nAmong other things this helps us select the appropriate plots for visualization.\n### Categorical feature:\n###### ordinal:\n- Family\n- Education\n###### nominal:\n- ID\n- Zip Code\n- Securities Account\n- CD Account\n- Online\n- Credit Card\n### Numerical feature:\n- Age\n- Experience\n- Income\n- CCAvg\n- Mortage","4dd8bc0d":"## Create new Feature data:","b2713c8e":"##### Observation: \n1. Most of the customer of the Bank does not have Credit_card.\n2. Customer Having high income,who does not have credit card,has higher rate of loan offer acceptance.","b139f687":"##### Observation by analyzing pivoting features:\n1. **CD_Account: **We observe significant correlation (~0.5) among CD_Account=1 and Personal_Loan Accepted We decide to include this feature in our model.\n\n2. **Securities Account\/Education\/Family: **We observe mild correlation (>.1) with Personal_Loan Accepted.We decide to include these features in our model.\n\n3. **Credit_Card\/Online(NetBanking Facility): **We observe less correlation (less than .1) with Personal_Loan Accepted.We may would  like to exclude both these features from our model.","1a083887":"# Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set.","09f68c0d":"# Data Cleaning\/Wrangle data:\n\nWe have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.","e5c42b79":"**Observation: **Customer having high income(Yearly income>120K,does not have net-banking(Online) facility.","177dfc35":"### Logistic Regration:","be6f50f9":"# Na\u00efve Bayes with KFold cross validation:","4375c783":"## Initial Data Analysis:","04aa02fe":"##### Observation: \n1. Most customer having less than 100K anual income & Education qualification is undergraduate , Rejected the loan.\n2. Mejority customers of bank are having less than <100K anual income and Education qualification is undergraduate.","ddfb5ce0":"## Univariate Analysis:","c3c31ac5":"##### Observations.\n\n1. Customer having less income (Income<=100K) had high rejection rate.\n2. Customer having 0 Mortgage had high rejection rate.\n3. Customer having low CCAvg mostly rejected Personal loan offer. Custer having CCAvg between 2.5 to 6 has higher rate of acceptance of the offer\n4. Most Customers are in 35-55 age range.\n5. Most Customers are in 15-35 Experience range\n\n##### Decisions.\n1. We should consider Income & Mortgage in our model training.\n2. We should band age and Experience group may be.","ccebd85e":"# K-Nearest-Neighbors with GridSearchCV","22a37310":"# Na\u00efve Bayes:","14b9cf51":"## Problem Statement:\nThe department wants to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.\n","672f13e0":"# Analyze by visualizing data\nNow we can continue confirming some of our assumptions using visualizations for analyzing the data.Let us start by understanding correlations between numerical features and our solution goal (Personal loan accepted).","0db8ee8a":"# Model:","e4118f9f":"It is a good practice to understand the data first and try to gather as many insights from it. EDA is all about making sense of data in hand.","3bc2cfc7":"### Data Descripsion\t\t\t\t\t\t\t\n![image.png](attachment:image.png)","5f7d175e":"##### Observation: \nExperience minimum is -3.Experience can not be negetive value.We will treat this condition later.Assumption is Experience and Age are related.","1523c07f":"Categorical:Online,CreditCard,Securities Account,CD Account.\nOrdinal: Education & Family.","da1b28de":"# Test_Train Split","4a182eae":"# Co-relation Map","f85eccb5":"# References\nThis notebook has been created based on great work done solving the Titanic competition and other sources.\n1. https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions .\n2. https:\/\/www.kaggle.com\/iconoclash\/personal-loan-dataset-binary-classification.","c3217041":"## Correcting by dropping features:\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\n\nBased on our assumptions and decisions we want to drop the ID and Zip features.","0be62195":"# K-Nearest-Neighbors:","0056b55c":"**Observation: \n**Single(Family Size-1\/Couples(Family Size-2) people are mejority of the customer of Thera Bank.","1523d4d8":"##### Correcting by imputing the Data:\nExperience feature we saw some negetive value. Lets fix that by compareing with Age.","3ab47895":"### Correlating numerical and ordinal features\nWe can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.","a26b4013":"##### Create perhead income:","243cc704":"# K-Nearest-Neighbors with KFold Cross Validation","cc1e7509":"## Bi-Variate Analysis - With Pivot Table for Catagorical variable:","0dbec1fc":"## Observation\/Inferance from IDA- Initial Data Analysis:\n1. Dataset has 5000 records with 14 features\/variable. Data is not huge.\n2. In Dataset we have mostly Integer data types.\n3. There is NO missing value present in dataset.Thats great.\n4. Dataset is likely to be imbalanced dataset.There ratio of Opted Personal Loan vs not opted is less than 90:10.We may need to find out a way to balance those 2 class.","30296216":"## Data Visualization and Insight-EDA(Exploratory Data Analysis)","226494c6":"### Data Distribution of each feature:","0405f559":"# Logistic Regration with KFold Cross Validation:","560d29d1":"Initial data analysis in primary step for data analytics. Mostly its cover as part of EDA. But as name suggest,EDA is exploritary data analysis is done to analyze each of feature in data set to get some inferance or for the Hypothesis.\n\nIDA on other hand perform to get familiar with data set. To identify the dependent and independent variable in data set. IDA step consists of :\n\n    1. Shape of the data. Row and Column count.\n    2. Get to know datatypes of the features of the dataset.\n    3. Initial descritive analysis.\n    4. Check if the missing values are present.\n    4. Check if the data set in balanced dataset or not.","cc2ab92c":"# Feature Scaling:"}}