{"cell_type":{"3334fcf5":"code","4969b319":"code","3b66e1ea":"code","73db80d9":"code","c3c72a2b":"code","1ff2d203":"code","21b674b1":"code","239479f4":"code","48d2bab6":"code","b4047945":"code","2792b70b":"markdown","a6747420":"markdown","09bfb693":"markdown","646b525f":"markdown","23626f09":"markdown","a0946c37":"markdown","5c0eeb37":"markdown","b2a62a71":"markdown","de8311fc":"markdown","5dec9eea":"markdown"},"source":{"3334fcf5":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","4969b319":"class LinearRegression:\n        \n    def forward(self, x):\n    \n        # shape(#samples, 1)\n        return ((np.dot(x, self.weight)) + self.bias)\n    \n    def leastSquare(self, y, y_hat):\n        \n        # shape(#samples, 1)\n        return (y_hat - y)**2\n    \n    def cost(self, y, y_hat):\n        \n        m = y.shape[0]\n        \n        # scalar\n        return (np.sum(self.leastSquare(y, y_hat)))\/(2*m)\n        \n    def train(self, x, y, alpha, epoch, random_state=-1):\n        \n        # x : shape(#samples, #features)\n        # y : shape(#samples, 1)\n        \n        m, n = x.shape[0], x.shape[1]\n        \n        if random_state != -1:\n            np.random.seed(random_state)\n        \n        # shape(#features, 1)\n        self.weight = np.random.randn(n,1)\n\n        # shape(1,1)\n        self.bias = np.zeros((1,1))\n        \n        self.epoch = epoch\n        \n        self.cost_list = []\n        \n        for i in range(epoch):\n            \n            # shape(#samples, 1)\n            y_hat = self.forward(x)\n    \n            # scalar\n            loss = self.cost(y, y_hat)\n\n            self.cost_list.append(loss)\n\n            # Gradient\n            # dL_dw : dLoss\/dweight (#features, 1)\n            dL_dw = (np.dot(x.T, (y_hat - y)))\/m\n\n            # dL_db : dLoss\/dbias (1, 1)\n            dL_db = np.sum((y_hat - y)\/m)\n\n            # shape(#features, 1)\n            self.weight = self.weight - (alpha * dL_dw)\n\n            # shape(1, 1)\n            self.bias = self.bias - (alpha * dL_db)\n            \n    def plot_convergence(self):\n        \n        plt.plot([i for i in range(self.epoch)], self.cost_list)\n        plt.xlabel('Epochs'); plt.ylabel('Mean Squared Error')\n        \n    def predict(self, x_test):\n        \n        # shape(#samples, 1)\n        return self.forward(x_test)","3b66e1ea":"def randomDataset(m, n, random_state=-1):\n    \n    if random_state != -1:\n        np.random.seed(random_state)\n        \n    x = np.random.randn(m, n)\n    slope = np.random.randn(n, 1)\n    epsilon = np.random.randn(1, 1)\n    y = np.dot(x, slope) + epsilon\n    print(slope, epsilon)\n    \n    return x, y","73db80d9":"def train_test_split(x, y, size=0.2, random_state=-1):\n    \n    if random_state != -1:\n        np.random.seed(random_state)\n        \n    x_val = x[:int(len(x)*size)]\n    y_val = y[:int(len(x)*size)]\n    x_train = x[int(len(x)*size):]\n    y_train = y[int(len(x)*size):]\n    \n    return x_train, y_train, x_val, y_val","c3c72a2b":"import numpy as np\n\ndef rss(y, y_hat):\n    \n    return np.sum((y-y_hat)**2)\n\ndef tss(y):\n    \n    return np.sum((y-y.mean())**2)\n\ndef r2(y, y_hat):\n    \n    return (1 - (rss(y, y_hat)\/tss(y)))\n\ndef rmse(y, y_hat):\n    \n    return np.sqrt(np.mean((y-y_hat)**2))","1ff2d203":"m = 1000\nn = 2\nx, y = randomDataset(m, n, random_state=0)","21b674b1":"x_train, y_train, x_val, y_val = train_test_split(x, y, size=0.2, random_state=0)","239479f4":"l = LinearRegression()","48d2bab6":"alpha = 0.1\nepoch = 20\nl.train(x_train, y_train, alpha, epoch, random_state=0)\nl.plot_convergence()\nl.weight, l.bias","b4047945":"y_hat = l.predict(x_val)\nprint(rmse(y_val, y_hat), r2(y_val, y_hat))","2792b70b":"#### Split data","a6747420":"## Linear Regression","09bfb693":"### Metrics","646b525f":"#### Train","23626f09":"#### Create data","a0946c37":"## Train Model on Dummy data","5c0eeb37":"### Evaluate on validation data","b2a62a71":"### Utils","de8311fc":"alpha : learning rate\n\nepoch : number of iterations","5dec9eea":"m : number of samples\n\nn : number of features"}}