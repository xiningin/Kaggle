{"cell_type":{"156059e9":"code","3e561048":"code","5472c1d9":"code","599fb5e2":"code","07a82a24":"code","07233173":"code","7702cc97":"code","7ceac831":"code","3b0f5c6f":"code","f5ffbe7c":"code","60e97a82":"code","e36e3b03":"code","4f0fd045":"code","9edcbe4e":"code","ec6d65ba":"code","2ff3cb29":"code","9b5edde6":"code","0115ca6c":"code","938f3a6b":"code","1fb8ec53":"code","5bdec8fd":"code","0fe46905":"code","ac1b50c9":"code","bf567ae5":"code","6f623862":"code","3005d010":"code","90fb2167":"code","6a3f397b":"code","8a6df899":"code","adba9dd9":"code","3830a5d2":"code","5d55c718":"code","4a76857a":"code","232eb7c2":"code","60119488":"code","6ee72d17":"code","1dda55dc":"code","e13d045b":"code","22d62ff5":"code","e4b9ffeb":"code","c35199b0":"code","b9f7059b":"code","d9b8d8c2":"code","f0d21946":"code","cc8a0bf1":"code","94b00fe3":"code","b507be7b":"code","8b0a251b":"code","7b135322":"code","ecdeeede":"code","69eb9497":"code","97e68106":"code","f395f9e7":"markdown","99bfacb7":"markdown","3786bd67":"markdown","3eece418":"markdown","b64d0a57":"markdown","c161d9da":"markdown","7ee41b94":"markdown","c94e6098":"markdown","7a6f6b24":"markdown","567a7627":"markdown","bddda4c4":"markdown","b85628b1":"markdown","34ab275b":"markdown","17902817":"markdown","03f9dddb":"markdown","14cf127c":"markdown","b62fb212":"markdown","7fcb8ec6":"markdown","76056a69":"markdown"},"source":{"156059e9":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.fasttext import FastText\nfrom smart_open import open, os\nimport logging\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Image\n\nfrom scipy import spatial\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en')","3e561048":"# Word2Vec Window\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/w2v_window.png\", width=600)","5472c1d9":"# Document and Word Vectors\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/word2vec_approaches.png\", width=600)","599fb5e2":"# CBOW Models\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/shallow_net.png\", width=600)","07a82a24":"Image('https:\/\/towardsml.files.wordpress.com\/2018\/06\/capture11.png')","07233173":"# Word2Vec Hidden Layer\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/word2vec_hidden_layer.png\", width=400)","7702cc97":"# Document and Word Vectors\n# Image(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/word_vector.png\", width=400)\nImage('https:\/\/i.imgur.com\/R8VLFs2.png', width= 500)","7ceac831":"# Word2Vec Architecture\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/w2v_architecture.png\", width=600)","3b0f5c6f":"# Multinomial Logistic Regression\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/multinomial_logistic_regression.png\", width=600)","f5ffbe7c":"Image('http:\/\/mccormickml.com\/assets\/word2vec\/matrix_mult_w_one_hot.png',width=400)","60e97a82":"# Softmax\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/softmax.png\", width=600)","e36e3b03":"a= np.array([2.0,1.0,0.1])\nnp.exp(a)","4f0fd045":"np.exp(a)\/ sum(np.exp(a))","9edcbe4e":"# Word2Vec Architecture\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/cross_entropy.png\", width=600)","ec6d65ba":"# Cross Entropy Example\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/cross_entropy_example.png\", width=600)","2ff3cb29":"0 * ln(0.1)+ 1* ln(0.5)+0*ln(0.4)","9b5edde6":"# Log Loss (i.e. binary cross entropy)\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/log_loss.png\", width=600)","0115ca6c":"Image('https:\/\/jalammar.github.io\/images\/word2vec\/queen-woman-girl-embeddings.png',width= 800)","938f3a6b":"corpus = [\n  'Text of the first document.',\n  'Text of the second document made longer.',\n  'Number three.',\n  'This is number four.',\n]\n\n# we need to pass splitted sentences to the model\ntokenized_sentences = [simple_preprocess(sentence) for sentence in corpus]\ntokenized_sentences","1fb8ec53":"# build a word2vec model\nmodel = gensim.models.Word2Vec(tokenized_sentences, min_count=1)\nprint(model)","5bdec8fd":"import logging\n\n# add logging to view training info from gensim\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","0fe46905":"# build a word2vec model with logging turned on\nmodel = gensim.models.Word2Vec(tokenized_sentences, min_count=1)\nprint(model)","ac1b50c9":"import os\nos.listdir('..\/input\/usi-nlp-practicum-2')","bf567ae5":"df = pd.read_csv('..\/input\/usi-nlp-practicum-2\/imdb_train.csv').sample(1000).reset_index(drop=True)\ndf.head()","6f623862":"nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\nimport re\ndef cleaning(doc):\n    doc=nlp(doc)\n    txt = [token.lemma_ for token in doc if not token.is_stop]\n    txt=' '.join(txt)\n    txt=re.sub(\"[^A-Za-z']+\", ' ', str(txt)).lower()\n    # Word2Vec uses context words to learn the vector representation of a target word,\n    # if a sentence is only one or two words long,\n    # the benefit for the training is very small\n    return txt.split()","3005d010":"from tqdm import tqdm\ntqdm.pandas()","90fb2167":"df['clean_review']=df['review'].progress_apply(lambda x:cleaning(x))\ndf['clean_review'].head()","6a3f397b":"from gensim.models import Word2Vec","8a6df899":"# build the same model, making the 2 steps explicit\n# start with an empty model, no training occurs yet\n\nnew_model = Word2Vec(\n      size=300\n    , window=5\n    , min_count=5\n    , sg=1\n)","adba9dd9":"%%time\n\n# learn the vocabulary\nnew_model.build_vocab(df['clean_review'])","3830a5d2":"print('epochs: {}'.format(new_model.epochs))\nprint('corpus count: {}'.format(new_model.corpus_count))","5d55c718":"%%time\n\n# train the model\nnew_model.train(\n      df['clean_review']\n    , total_examples=new_model.corpus_count\n    , epochs=5 #new_model.epochs\n)","4a76857a":"# save the model\nSAVED_EMBEDDINGS_PATH=\"w2v.model\"\nnew_model.save(SAVED_EMBEDDINGS_PATH)\n# open the model\nnew_model = gensim.models.Word2Vec.load(SAVED_EMBEDDINGS_PATH)","232eb7c2":"print(list(new_model.wv.vocab)[0:100])","60119488":"# create a word vector from a trained work\nprint(new_model.wv['comedy'])","6ee72d17":"new_model.most_similar(positive=['movie'], topn=10)","1dda55dc":"from itertools import combinations\ncompare_words = ['movie', 'film', 'good', 'awesome']\n\nfor t1, t2 in combinations(compare_words, 2):\n    print('t1: {} | t2: {} | simiarity_score: {}'.format(t1, t2, new_model.wv.similarity(t1,t2)))","e13d045b":"# Document and Word Vectors\nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/man_to_king_w2v.png\", width=800)","22d62ff5":"# Paragraph Vector - Distributed Memory (PV_DM) Model \nImage(\"https:\/\/s3.amazonaws.com\/nlp.practicum\/pv_dm.png\", width=600)","e4b9ffeb":"#Import all the dependencies\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument","c35199b0":"train_corpus= [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(df['clean_review'][:1000])]","b9f7059b":"from gensim.models import doc2vec\n\nd2v = doc2vec.Doc2Vec(\n      vector_size=300\n    , epochs=20\n    , dm=0\n)\n\nprint(d2v)","d9b8d8c2":"d2v.build_vocab(train_corpus)","f0d21946":"%%time\n\nd2v.train(\n    train_corpus,\n    total_examples=d2v.corpus_count,\n    epochs=d2v.epochs\n)","cc8a0bf1":"# view a document vector\nd2v.infer_vector('This was one of the most beautiful movies I have ever seen'.split()).shape","94b00fe3":"similar_doc = d2v.docvecs.most_similar('2')\nsimilar_doc","b507be7b":"embedding_file='..\/input\/usi-nlp-practicum-2\/glove.6B.50d.txt'","8b0a251b":"embeddings_dict = {}\nwith open(embedding_file, 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vector = np.asarray(values[1:], \"float32\")\n        embeddings_dict[word] = vector","7b135322":"len(embeddings_dict)","ecdeeede":"embeddings_dict[\"king\"].shape","69eb9497":"def find_closest_embeddings(embedding):\n    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.euclidean(embeddings_dict[word], embedding))","97e68106":"print(find_closest_embeddings(embeddings_dict[\"king\"])[1:3])","f395f9e7":"Resources to create content:\n- [Word embeddings: exploration, explanation, and exploitation (with code in\u00a0Python)](https:\/\/towardsdatascience.com\/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795)\n- [The amazing power of word\u00a0vectors](https:\/\/blog.acolyer.org\/2016\/04\/21\/the-amazing-power-of-word-vectors\/)\n- [Don\u2019tcount,predict! Asystematiccomparisonof context-countingvs.context-predictingsemanticvectors\n](http:\/\/clic.cimec.unitn.it\/marco\/publications\/acl2014\/baroni-etal-countpredict-acl2014.pdf)\n- [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/)\n- [Taming Text with the SVD](ftp:\/\/ftp.sas.com\/techsup\/download\/EMiner\/TamingTextwiththeSVD.pdf)","99bfacb7":"[YouTube Video- what is a Neural Network?](https:\/\/youtu.be\/aircAruvnKk?t=228)","3786bd67":"##### Cross Entropy\n\nIn a linear model, we have a scalar prediction, thus we can use an evaluation metric like MSE to evaluate the model.\n\nWhen you\u2019re using softmax, however, your output is a vector. One vector is the probability values from the output units. You can also express your data labels as a vector using what\u2019s called one-hot encoding.\n\nThis just means that you have a vector the length of the number of classes, and the label element is marked with a 1 while the other labels are set to 0.\n\n`y=[0,0,0,0,1,0,0,0,0,0]`\n\nAnd our output prediction vector could be something like\n\n`y=[0.047,0.048,0.061,0.07,0.330,0.062,0.001,0.213,0.013,0.150]`\n\nWe want our error to be proportional to how far apart these vectors are. To calculate this distance, we\u2019ll use the cross entropy. Then, our goal when training the network is to make our prediction vectors as close as possible to the label vectors by minimizing the cross entropy.\n\nSOURCE: https:\/\/towardsdatascience.com\/deep-learning-concepts-part-1-ea0b14b234c8","3eece418":"**Pretrained Embeddings (GLove)**","b64d0a57":"### Word embedding\n\"Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with much higher dimension.\n\nMethods to generate this mapping include neural networks, dimensionality reduction on the word co-occurrence matrix, probabilistic models, and explicit representation in terms of the context in which words appear.\n\nWord and phrase embeddings, when used as the underlying input representation, have been shown to boost the performance in NLP tasks such as syntactic parsing and sentiment analysis.\"\n\n##### There are many techniques to create Word Embeddings. Some of the popular ones are:\n\n- Binary Encoding\n- TF Encoding\n- TF-IDF Encoding\n- Latent Semantic Analysis Encoding\n- Topic Modeling\n- Word2Vec Encoding\n\n##### Why do we need Word Embeddings?\n\"Many machine learning algorithms and almost all Deep Learning Architectures are incapable of processing strings or plain text in their raw form. They require numbers as inputs to perform any sort of job, be it classification, regression etc. in broad terms. And with the huge amount of data that is present in the text format, it is imperative to extract knowledge out of it and build applications.\"\n\n\"So a natural language modelling technique like Word Embedding is used to map words or phrases from a vocabulary to a corresponding vector of real numbers. As well as being amenable to processing by ML algorithms, this vector representation has two important and advantageous properties:\n\n- **Dimensionality Reduction** - it is a more efficient representation\n- **Contextual Similarity** - it is a more expressive representation\"\n\n### Techniques (Count versus Predictive)\nCount-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word. Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).\"\n\n##### Word2Vec\nWord2Vec is a more recent model that embeds words in a lower-dimensional vector space using a shallow neural network. The result is a set of word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings. For example, strong and powerful would be close together and strong and Paris would be relatively far. There are two versions of this model based on skip-grams (SG) and continuous-bag-of-words (CBOW), both implemented by the gensim Word2Vec class.\n\nWord2Vec uses a trick you may have seen elsewhere in machine learning. We\u2019re going to train a simple neural network with a single hidden layer to perform a certain task, but then we\u2019re not actually going to use that neural network for the task we trained it on! Instead, the goal is actually just to learn the weights of the hidden layer\u2013we\u2019ll see that these weights are actually the \u201cword vectors\u201d that we\u2019re trying to learn.\n\nThe network is going to learn the statistics from the number of times each pairing shows up. So, for example, the network is probably going to get many more training samples of (\u201cSoviet\u201d, \u201cUnion\u201d) than it is of (\u201cSoviet\u201d, \u201cSasquatch\u201d). When the training is finished, if you give it the word \u201cSoviet\u201d as input, then it will output a much higher probability for \u201cUnion\u201d or \u201cRussia\u201d than it will for \u201cSasquatch\u201d.\n\n##### Word2Vec - Skip-gram Model\nThe skip-gram word2vec model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the synthetic task of given an input word, giving us a predicted probability distribution of nearby words to the input. A virtual one-hot encoding of words goes through a 'projection layer' to the hidden layer; these projection weights are later interpreted as the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings.\n\n##### Word2Vec - Continuous-bag-of-words Model\nContinuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The synthetic training task now uses the average of multiple input context words, rather than a single word as in skip-gram, to predict the center word. Again, the projection weights that turn one-hot words into averageable vectors, of the same width as the hidden layer, are interpreted as the word embeddings.\n\nSOURCE: \n- https:\/\/en.wikipedia.org\/wiki\/Word_embedding\n- [An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/)\n- [What is word embedding in deep learning?](https:\/\/www.quora.com\/What-is-word-embedding-in-deep-learning)\n- [How to Develop Word Embeddings in Python with Gensim](https:\/\/machinelearningmastery.com\/develop-word-embeddings-python-gensim\/)\n- [Advances in Pre-Training Distributed Word Representations](https:\/\/arxiv.org\/pdf\/1712.09405.pdf)\n- [Vector Representations of Words](https:\/\/www.tensorflow.org\/tutorials\/word2vec)\n- [Word2Vec Tutorial - The Skip-Gram Model](http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/)","c161d9da":"Seperate the training of the model in 3 steps:\n\n`Word2Vec()`:\n\nIn this first step, I set up the parameters of the model one-by-one.\nI do not supply the parameter sentences, and therefore leave the model uninitialized, purposefully.\n\n`.build_vocab()`:\n\nHere it builds the vocabulary from a sequence of sentences and thus initialized the model.\nWith the loggings, I can follow the progress and even more important, the effect of min_count and sample on the word corpus. I noticed that these two parameters, and in particular sample, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n\n`.train()`:\nFinally, trains the model.\nThe loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously.","7ee41b94":"**The parameters:**\n* `min_count` = int - Ignores all words with total absolute frequency lower than this - (2, 100)\n* `window` = int - The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n* `size` = int - Dimensionality of the feature vectors. - (50, 300)\n* `sample` = float - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial. - (0, 1e-5)\n* `alpha` = float - The initial learning rate - (0.01, 0.05)\n* `min_alpha` = float - Learning rate will linearly drop to min_alpha as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n* `negative` = int - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n* `workers` = int - Use these many worker threads to train the model (=faster training with multicore machines)","c94e6098":"##### Distributed Representations of Words and Phrases and their Compositionality\n\nThere are three innovations in this second paper:\n\n- Treating common word pairs or phrases as single \u201cwords\u201d in their model.\n- Subsampling frequent words to decrease the number of training examples.\n- Modifying the optimization objective with a technique they called \u201cNegative Sampling\u201d, which causes each training sample to update only a small percentage of the model\u2019s weights.\n\n##### Phrases\nEach pass only looks at combinations of 2 words, but you can run it multiple times to get longer phrases. So, the first pass will pick up the phrase \u201cNew_York\u201d, and then running it again will pick up \u201cNew_York_City\u201d as a combination of \u201cNew_York\u201d and \u201cCity\u201d.\n\n##### Subsampling\nWord2Vec implements a \u201csubsampling\u201d scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the word\u2019s frequency.\n\nIf we have a window size of 10, and we remove a specific instance of \u201cthe\u201d from our text:\n\nAs we train on the remaining words, \u201cthe\u201d will not appear in any of their context windows.\nWe\u2019ll have 10 fewer training samples where \u201cthe\u201d is the input word.\n\n##### Negative Sampling \nTraining a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network.\n\nAs we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!\n\nNegative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here\u2019s how it works.\n\nWhen training the network on the word pair (\u201cfox\u201d, \u201cquick\u201d), recall that the \u201clabel\u201d or \u201ccorrect output\u201d of the network is a one-hot vector. That is, for the output neuron corresponding to \u201cquick\u201d to output a 1, and for all of the other thousands of output neurons to output a 0.\n\nWith negative sampling, we are instead going to randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for. (In this context, a \u201cnegative\u201d word is one for which we want the network to output a 0 for). We will also still update the weights for our \u201cpositive\u201d word (which is the word \u201cquick\u201d in our current example).\n\n\nSOURCE:\n- [Distributed Representations of Words and Phrases and their Compositionality:](https:\/\/arxiv.org\/pdf\/1310.4546.pdf)","7a6f6b24":"##### Input\nWhen training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when you evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).\n\n##### The Hidden Layer\nFor our example, we\u2019re going to say that we\u2019re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).","567a7627":"### Explore Embeddings","bddda4c4":"### word2Vec Hyperparameters\n\n- **size:** (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n- **window:** (default 5) The maximum distance between a target word and words around the target word.\n- **min_count:** (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n- **sg:** (default 0) The training algorithm, either CBOW (0) or skip gram (1).","b85628b1":"### Train Embeddings","34ab275b":"Inferring a Vector\nOne important thing to note is that you can now infer a vector for any piece of text without having to re-train the model by passing a list of words to the model.infer_vector function. This vector can then be compared with other vectors via cosine similarity.","17902817":"### `Doc2Vec`\n\nDoc2vec (aka paragraph2vec, aka sentence embeddings) modifies the word2vec algorithm to unsupervised learning of continuous representations for larger blocks of text, such as sentences, paragraphs or entire documents.\n\n##### `Paragraph Vector - Distributed Memory (PV-DM)`\nThis is the Paragraph Vector model analogous to Continuous-bag-of-words Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of inferring a center word based on context words and a context paragraph. A paragraph is a context for all words in the paragraph, and a word in a paragraph can have that paragraph as a context.\n\n##### `Paragraph Vector - Distributed Bag of Words (PV-DBOW)`\nThis is the Paragraph Vector model analogous to Skip-gram Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n\nParagraph Vector, aka gensim Doc2Vec\nThe straightforward approach of averaging each of a text's words' word-vectors creates a quick and crude document-vector that can often be useful. However, Le and Mikolov in 2014 introduced the Paragraph Vector, which usually outperforms such simple-averaging.\n\nSOURCE: \n- [Doc2vec tutorial](https:\/\/rare-technologies.com\/doc2vec-tutorial\/)\n- [Distributed Representations of Sentences and Documents:](https:\/\/cs.stanford.edu\/~quocle\/paragraph_vector.pdf)\n- [Representations for Language: From Word Embeddings to Sentence Meanings\n](https:\/\/nlp.stanford.edu\/manning\/talks\/Simons-Institute-Manning-2017.pdf)\n- [A gentle introduction to\u00a0Doc2Vec](https:\/\/medium.com\/scaleabout\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)\n- [Gensim Doc2vec Tutorial on the IMDB Sentiment Dataset](https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/develop\/docs\/notebooks\/doc2vec-IMDB.ipynb)","03f9dddb":"**Training the model**\nGensim Word2Vec Implementation:\nWe use Gensim implementation of word2vec: https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html","14cf127c":"##  Doc2Vec","b62fb212":"### Word Embeddings \n\n###### Author: Alex Sherman (alsherman@deloitte.com) | Vikas Kumar (vikkumar@deloitte.com)\n\n\n##### Agenda\n- Word2Vec\n- cbow\n- skig-gram\n- doc2vec","7fcb8ec6":"**Cleaning:**\nWe are lemmatizing and removing the stopwords and non-alphabetic characters for each line of dialogue.**","76056a69":"### Save Embeddings"}}