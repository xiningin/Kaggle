{"cell_type":{"316a45bd":"code","300e8c36":"code","b0f92531":"code","fc427cca":"code","3674b6d6":"code","7e30950f":"code","8be272c6":"code","af8bab97":"code","26a6bbdd":"code","3c3304a6":"code","ab91409d":"code","689e15e1":"code","11de5ed5":"code","1db71e4f":"code","c2b5eb70":"code","ecde163f":"code","9838bfcc":"code","92e6e3ff":"code","5177954c":"code","7fc776ed":"code","1f8df904":"code","be458967":"code","21d3443c":"code","ffa18a00":"code","92cc8c39":"code","56dd745c":"code","3fa05fa1":"code","3d722a42":"code","e200c95c":"code","f0035c9d":"markdown","4e019a57":"markdown","1d1ea01d":"markdown","332d288f":"markdown","80fb002b":"markdown","cf290aa6":"markdown","cf1cbdb9":"markdown","cd6d8ece":"markdown","faf1d760":"markdown","3797b7b9":"markdown","344f00b0":"markdown"},"source":{"316a45bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","300e8c36":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport random","b0f92531":"clean = pd.read_csv('..\/input\/jane-street-data-prep\/clean.csv')","fc427cca":"clean = clean[clean['weight'] != 0] # include them in the analysis later","3674b6d6":"clean['action'] = ((clean['resp'].values) > 0).astype(int) # variable to predict\nclean.head()","7e30950f":"features = [column for column in clean.columns if \"feature\" in column]","8be272c6":"clean.fillna(method='ffill')","af8bab97":"# features.remove('')\nlen(features)","26a6bbdd":"clean.shape","3c3304a6":"clean['resp'] = (((clean['resp'].values)* clean['weight']) > 0).astype(int) # computes the weighted return of each trade\nclean['resp_1'] = (((clean['resp_1'].values) * clean['weight']) > 0).astype(int)\nclean['resp_2'] = (((clean['resp_2'].values) * clean['weight']) > 0).astype(int)\nclean['resp_3'] = (((clean['resp_3'].values) * clean['weight']) > 0).astype(int)\nclean['resp_4'] = (((clean['resp_4'].values) * clean['weight']) > 0).astype(int)","ab91409d":"features_mean = np.mean(clean[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']","689e15e1":"X_train = clean[features].values\n\ny_train = np.stack([(clean[column] > 0).astype('int') for column in resp_cols]).T","11de5ed5":"params = {\"num_leaves\": 300,\n       \"max_bin\": 450,\n       \"feature_fraction\": 0.52,\n       \"bagging_fraction\": 0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\": 0.052,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"}","1db71e4f":"models = [] # list of models to train\n\nfor i in range(y_train.shape[1]): # each model tries to predict each resp column separately\n    xtr, xval, ytr, yval = train_test_split(X_train,\n                                            y_train[:, i],\n                                            test_size=0.2,\n                                            stratify=y_train[:, i]) # use a time series split\n   \n    d_train = lgbm.Dataset(xtr,label=ytr)\n    \n    d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)\n    \n    clf = lgbm.train(params,\n                     d_train,\n                     valid_sets=[d_train,d_eval],\n                     num_boost_round=1000,\n                     early_stopping_rounds=50,\n                     verbose_eval=50)\n    \n    models.append(clf)","c2b5eb70":"fig,ax = plt.subplots(figsize=(25,50))\n\nlgbm.plot_importance(clf, ax=ax,importance_type='gain',max_num_features=130) # for the last classifier\n\nplt.show()","ecde163f":"# necessary imports\n\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import pipeline\n\nfrom functools import partial\nfrom skopt import space\nfrom skopt import gp_minimize","9838bfcc":"def optimize(params, param_names, x, y):\n    params = dict(zip(param_names, params))\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n        \n    return -1.0 * np.mean(accuracies)","92e6e3ff":"if __name__ == '__main__':\n    X = X_train\n    y = y_train\n    \n    param_space = [\n        space.Integer(3, 15, name='max_depth'),\n        space.Integer(100, 600, name='n_estimators'),\n        space.Categorical(['gini', 'entropy'], name='criterion'),\n        space.Real(0.01, 1, prior='uniform', name='max_features')\n    ]\n    \n    param_names = [\n        'max_depth',\n        'n_estimators',\n        'criterion',\n        'max_features'\n    ]\n    \n    optimization_function = partial(\n        optimize,\n        param_names=param_names,\n        x=X,\n        y=y\n    )","5177954c":"result = gp_minimize(\n    optimization_function,\n    dimensions=param_space,\n    n_calls=15,\n    n_random_starts=10\n    verbose=10\n)\n\nprint(dict(zip(param_names, result.x)))","7fc776ed":"from hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope","1f8df904":"def optimize(params, x, y):\n    model = ensemble.RandomForestClassifier(**params)\n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n        \n    return -1.0 * np.mean(accuracies)","be458967":"if __name__ == '__main__':\n    X = X_train\n    y = y_train\n    \n    param_space = {\n        'max_depth': scope.int(hp.quniform('max_depth', 3, 15, 1)),\n        'n_estimators': scopre.int(hp.quniform('n_estimators', 100, 600, 1)),\n        'criterion': hp.choice('criterion', ['gini', 'entropy']),\n        'max_features': hp.uniform('max_features', 0.01, 1)\n    }\n    \n    trial = Trials()\n    \n    optimization_function = partial(optimize, x=X, y=y)","21d3443c":"result = gp_minimize(\n    fn=optimization_function,\n    space=param_space,\n    algo=tpe.suggest,\n    max_evals=15,\n    trials=trials\n)\n\nprint(dict(zip(param_names, result.x)))","ffa18a00":"import optuna","92cc8c39":"def optimize(trial, x, y):\n    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n    n_estimators = trial.suggest_int('n_estimators', 100, 1500)\n    max_depth = trial.suggest_int('max_depth', 3, 15)\n    max_features = trial.suggest_uniform('max_features', 0.01, 1.0)\n    \n    model = ensemble.RandomForestClassifier(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        max_features=max_features,\n        criterion=criterion\n    )\n    \n    kf = model_selection.StratifiedKFold(n_splits=5)\n    accuracies = []\n    for idx in kf.split(X=x, y=y):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = x[train_idx]\n        ytrain = y[train_idx]\n        xtest = x[test_idx]\n        ytest = y[test_idx]\n        \n        model.fit(xtrain, ytrain)\n        preds = model.predict(xtest)\n        fold_acc = metrics.accuracy_score(ytest, preds)\n        accuracies.append(fold_acc)\n    \n     return -1.0 * np.mean(accuracies)","56dd745c":"if __name__ == '__main__':\n    X = X_train\n    y = y_train\n    \n    optimization_function = partial(optimize, x=X, y=y)\n    \n    study = optuna.create_study(direction='minimize')\n    study.optimize(optimization_function, n_trials=15)","3fa05fa1":"result = gp_minimize(\n    fn=optimization_function,\n    space=param_space,\n    algo=tpe.suggest,\n    max_evals=15,\n    trials=trials\n)\n\nprint(dict(zip(param_names, result.x)))","3d722a42":"def objective(trial):\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25)\n    dtrain = lgbm.Dataset(train_x, label=train_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    gbm = lgbm.train(param, dtrain)\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","e200c95c":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","f0035c9d":"### Bayesian hyperparameter optimization with Gaussian process","4e019a57":"Inspired from Kamal's great work (https:\/\/www.kaggle.com\/kamalnaithani\/lightgbm-stock-prediction-1-1). This notebook will be used to better understand future predictions with neural nets.","1d1ea01d":"Example from optuna's webpage + Abhishek Thakur's youtube channel","332d288f":"### Remove less important features","80fb002b":"### Train and test sets","cf290aa6":"### Preparing the dataset","cf1cbdb9":"### Tuning hyperparameters with Hyperopt","cd6d8ece":"### Feature importance","faf1d760":"### Model","3797b7b9":"Example from optuna's website","344f00b0":"### Tuning hyperparameters with Optuna"}}