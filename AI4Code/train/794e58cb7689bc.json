{"cell_type":{"2372f0b5":"code","cc2acdf2":"code","bde7759a":"code","48e3d6a2":"code","cd62f5d5":"code","66f543a7":"code","d904e76b":"code","94daceea":"code","c3cfe800":"code","aa316fcc":"code","1679187c":"code","b8667716":"code","bc0420f0":"code","44b551d0":"code","f795f144":"code","c1f49d2e":"code","dcd8ac16":"code","fe6d6ba7":"code","cd37a106":"code","697b6923":"code","b7d8ea58":"code","d5318b25":"code","3c9e05ba":"code","441e311d":"code","a11e63a5":"code","ba95b19f":"code","d9c41dab":"code","334039b1":"code","0ac13fa4":"code","887361eb":"code","af513f83":"code","3a4e52db":"code","bb3fe274":"code","be6ffb9b":"code","e8dae6d7":"code","b02a2baf":"code","415bf293":"code","fcffa2ca":"code","806cf49f":"code","6b597525":"code","95fee104":"code","2f0ffa46":"code","3412caa8":"code","dc0e2159":"code","6dd1c2e2":"code","2b3f11e9":"code","08da6ba1":"code","d800b8c3":"code","45699df4":"code","5dc8ad43":"code","5871c4a7":"code","69233d96":"code","674082de":"code","c8bb502e":"code","c61e300f":"code","f07a4596":"code","771d8d04":"code","12670764":"code","30343755":"code","0234517c":"code","55a75419":"code","b61e814a":"code","22407e4d":"code","324e1da0":"code","a2121264":"code","5f82a239":"code","02b85904":"code","51521ac3":"code","bbb6ef96":"code","6ccfaf16":"code","c3fc5c7b":"code","55e39d7b":"code","7557f030":"code","005c7de1":"code","a9850adc":"code","8cfac56f":"code","d66ac2b1":"code","e7b1db93":"code","7f73b67e":"code","76e5d1e3":"code","6f68a6b6":"code","baaa9333":"code","72ed35f5":"code","a7901114":"code","f418e09c":"code","9f612ff5":"code","3dcc5006":"code","61d85493":"code","d41f10e7":"code","33390bbd":"code","317fc9e7":"code","a7552ac2":"code","eb6d8c95":"code","5db9b465":"code","92e2076c":"code","e252868f":"code","f07ca3a7":"code","d86d5c64":"code","29c970ca":"code","cc0593b6":"code","cea5bf91":"code","a2723782":"code","9ea2abb4":"code","a577ab7d":"code","656b0b1f":"code","deec4c67":"code","bc3669d6":"markdown","662a4271":"markdown","96a4d102":"markdown","d1a50644":"markdown","c1162cc3":"markdown","902ac241":"markdown","4867e5d9":"markdown","13607a40":"markdown","90cc1a1c":"markdown","810fa682":"markdown","3e3dd66c":"markdown","8025db83":"markdown","a8fbaa5a":"markdown","c8b8afb1":"markdown","abac3d23":"markdown","20b0c4a0":"markdown","0657036a":"markdown","8c06ff90":"markdown","d9e869af":"markdown","30c90ede":"markdown","46c9a7e5":"markdown","2041ad29":"markdown","ddfae06f":"markdown","a6fccf1f":"markdown","5d363f18":"markdown","df175f76":"markdown","e3d272f6":"markdown","dedec655":"markdown","2919d690":"markdown","bba81eec":"markdown","e0f1c5fa":"markdown","4a5776d2":"markdown","02befd3b":"markdown","61f3dcdf":"markdown","51f31608":"markdown","7ca2f5e4":"markdown","62f490bc":"markdown","3006ced3":"markdown","aaf983a3":"markdown","09bd29c8":"markdown","cc549cfd":"markdown","29d56f27":"markdown","db90a656":"markdown","ea2e574d":"markdown","893f5185":"markdown","59e5bb30":"markdown","b1c7a63b":"markdown","73a41684":"markdown","1cb40bcc":"markdown","2e7a59f5":"markdown","8ed8d0ba":"markdown","49075e94":"markdown","03b9a5e1":"markdown","1dba52d8":"markdown"},"source":{"2372f0b5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cc2acdf2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nimport plotly.express as px\nfrom itertools import product\nimport warnings\nimport statsmodels.api as sm\nplt.style.use('seaborn-darkgrid')\n\n#matplotlib inline","bde7759a":"# Reading the csv file\nbitstamp = pd.read_csv(\"\/kaggle\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv\")\nbitstamp.head()","48e3d6a2":"bitstamp.info()","cd62f5d5":"# Converting the Timestamp column from string to datetime\nbitstamp['Timestamp'] = [datetime.fromtimestamp(x) for x in bitstamp['Timestamp']]","66f543a7":"\nbitstamp.head()","d904e76b":"print('Dataset Shape: ',  bitstamp.shape)","94daceea":"bitstamp.set_index(\"Timestamp\").Weighted_Price.plot(figsize=(14,7), title=\"Bitcoin Weighted Price\")","c3cfe800":"#calculating missing values in the dataset\n\nmissing_values = bitstamp.isnull().sum()\nmissing_per = (missing_values\/bitstamp.shape[0])*100\nmissing_table = pd.concat([missing_values,missing_per], axis=1, ignore_index=True) \nmissing_table.rename(columns={0:'Total Missing Values',1:'Missing %'}, inplace=True)\nmissing_table","aa316fcc":"#testing missing value methods on a subset\n\npd.set_option('display.max_rows', 1500)\n\na = bitstamp.set_index('Timestamp')\n\na = a['2019-11-01 00:15:00':'2019-11-01 02:24:00']\n\na['ffill'] = a['Weighted_Price'].fillna(method='ffill') # Imputation using ffill\/pad\na['bfill'] = a['Weighted_Price'].fillna(method='bfill') # Imputation using bfill\/pad\na['interp'] = a['Weighted_Price'].interpolate()         # Imputation using interpolation\n\na","1679187c":"def fill_missing(df):\n    ### function to impute missing values using interpolation ###\n    df['Open'] = df['Open'].interpolate()\n    df['Close'] = df['Close'].interpolate()\n    df['Weighted_Price'] = df['Weighted_Price'].interpolate()\n\n    df['Volume_(BTC)'] = df['Volume_(BTC)'].interpolate()\n    df['Volume_(Currency)'] = df['Volume_(Currency)'].interpolate()\n    df['High'] = df['High'].interpolate()\n    df['Low'] = df['Low'].interpolate()\n\n    print(df.head())\n    print(df.isnull().sum())","b8667716":"fill_missing(bitstamp)","bc0420f0":"#created a copy \nbitstamp_non_indexed = bitstamp.copy()","44b551d0":"bitstamp = bitstamp.set_index('Timestamp')\nbitstamp.head()","f795f144":"ax = bitstamp['Weighted_Price'].plot(title='Bitcoin Prices', grid=True, figsize=(14,7))\nax.set_xlabel('Year')\nax.set_ylabel('Weighted Price')\n\nax.axvspan('2018-12-01','2019-01-31',color='red', alpha=0.3)\nax.axhspan(17500,20000, color='green',alpha=0.3)","c1f49d2e":"#Zooming in\n\nax = bitstamp.loc['2017-10':'2019-03','Weighted_Price'].plot(marker='o', linestyle='-',figsize=(15,6), title=\"Oct-17 to March-19 Trend\", grid=True)\nax.set_xlabel('Month')\nax.set_ylabel('Weighted_Price')","dcd8ac16":"sns.kdeplot(bitstamp['Weighted_Price'], shade=True)","fe6d6ba7":"plt.figure(figsize=(15,12))\nplt.suptitle('Lag Plots', fontsize=22)\n\nplt.subplot(3,3,1)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=1) #minute lag\nplt.title('1-Minute Lag')\n\nplt.subplot(3,3,2)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=60) #hourley lag\nplt.title('1-Hour Lag')\n\nplt.subplot(3,3,3)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=1440) #Daily lag\nplt.title('Daily Lag')\n\nplt.subplot(3,3,4)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=10080) #weekly lag\nplt.title('Weekly Lag')\n\nplt.subplot(3,3,5)\npd.plotting.lag_plot(bitstamp['Weighted_Price'], lag=43200) #month lag\nplt.title('1-Month Lag')\n\nplt.legend()\nplt.show()","cd37a106":"hourly_data = bitstamp.resample('1H').mean()\nhourly_data = hourly_data.reset_index()\n\nhourly_data.head()","697b6923":"bitstamp_daily = bitstamp.resample(\"24H\").mean() #daily resampling","b7d8ea58":"import plotly.express as px\n\nbitstamp_daily.reset_index(inplace=True)\nfig = px.line(bitstamp_daily, x='Timestamp', y='Weighted_Price', title='Weighted Price with Range Slider and Selectors')\nfig.update_layout(hovermode=\"x\")\n\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(count=2, label=\"2y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n            \n        ])\n    )\n)\nfig.show()","d5318b25":"plot_ = bitstamp_daily.set_index(\"Timestamp\")[\"2017-12\"]","3c9e05ba":"import plotly.graph_objects as go\n\nfig = go.Figure(data=go.Candlestick(x= plot_.index,\n                    open=plot_['Open'],\n                    high=plot_['High'],\n                    low=plot_['Low'],\n                    close=plot_['Close']))\nfig.show()","441e311d":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","a11e63a5":"fill_missing(bitstamp_daily)","ba95b19f":"plt.figure(figsize=(15,12))\nseries = bitstamp_daily.Weighted_Price\nresult = seasonal_decompose(series, model='additive',period=1)\nresult.plot()","d9c41dab":"acf = plot_acf(series, lags=50, alpha=0.05)\nplt.title(\"ACF for Weighted Price\", size=20)\nplt.show()","334039b1":"plot_pacf(series, lags=50, alpha=0.05, method='ols')\nplt.title(\"PACF for Weighted Price\", size=20)\nplt.show()","0ac13fa4":"stats, p, lags, critical_values = kpss(series, 'ct')","887361eb":"print(f'Test Statistics : {stats}')\nprint(f'p-value : {p}')\nprint(f'Critical Values : {critical_values}')\n\nif p < 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","af513f83":"def adf_test(timeseries):\n    print ('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n       dfoutput['Critical Value (%s)'%key] = value\n    \n    print (dfoutput)\n    \n    if p > 0.05:\n        print('Series is not Stationary')\n    else:\n        print('Series is Stationary')","3a4e52db":"adf_test(series)","bb3fe274":"df = bitstamp_daily.set_index(\"Timestamp\")","be6ffb9b":"df.reset_index(drop=False, inplace=True)\n\nlag_features = [\"Open\", \"High\", \"Low\", \"Close\",\"Volume_(BTC)\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index()\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index()\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index()\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index()\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index()\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index()\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Timestamp\", drop=False, inplace=True)\ndf.head()","e8dae6d7":"df[\"month\"] = df.Timestamp.dt.month\ndf[\"week\"] = df.Timestamp.dt.week\ndf[\"day\"] = df.Timestamp.dt.day\ndf[\"day_of_week\"] = df.Timestamp.dt.dayofweek\ndf.head()","b02a2baf":"df_train = df[df.Timestamp < \"2020\"]\ndf_valid = df[df.Timestamp >= \"2020\"]\n\nprint('train shape :', df_train.shape)\nprint('validation shape :', df_valid.shape)","415bf293":"!pip install pmdarima","fcffa2ca":"import pmdarima as pm","806cf49f":"exogenous_features = ['Open_mean_lag3',\n       'Open_mean_lag7', 'Open_mean_lag30', 'Open_std_lag3', 'Open_std_lag7',\n       'Open_std_lag30', 'High_mean_lag3', 'High_mean_lag7', 'High_mean_lag30',\n       'High_std_lag3', 'High_std_lag7', 'High_std_lag30', 'Low_mean_lag3',\n       'Low_mean_lag7', 'Low_mean_lag30', 'Low_std_lag3', 'Low_std_lag7',\n       'Low_std_lag30', 'Close_mean_lag3', 'Close_mean_lag7',\n       'Close_mean_lag30', 'Close_std_lag3', 'Close_std_lag7',\n       'Close_std_lag30', 'Volume_(BTC)_mean_lag3', 'Volume_(BTC)_mean_lag7',\n       'Volume_(BTC)_mean_lag30', 'Volume_(BTC)_std_lag3',\n       'Volume_(BTC)_std_lag7', 'Volume_(BTC)_std_lag30', 'month', 'week',\n       'day', 'day_of_week']","6b597525":"model = pm.auto_arima(df_train.Weighted_Price, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.Weighted_Price, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Forecast_ARIMAX\"] = forecast","95fee104":"df_valid[[\"Weighted_Price\", \"Forecast_ARIMAX\"]].plot(figsize=(14, 7))","2f0ffa46":"from sklearn.metrics import mean_squared_error, mean_absolute_error\n\nprint(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.Weighted_Price, df_valid.Forecast_ARIMAX)))\n\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.Weighted_Price, df_valid.Forecast_ARIMAX))","3412caa8":"model.plot_diagnostics()","dc0e2159":"# Load FB Prophet\nfrom fbprophet import Prophet","6dd1c2e2":"# Resampling originial data to day level and forward fill the missing values\ndaily_data = bitstamp.resample(\"24H\").mean() #daily resampling\nfill_missing(daily_data)","2b3f11e9":"# Renaming the column names accroding to Prophet's requirements\n\ndaily_data_fb = daily_data.reset_index()[['Timestamp','Weighted_Price']].rename({'Timestamp':'ds','Weighted_Price':'y'}, axis=1)\ndaily_data_fb.head()","08da6ba1":"split_date = \"2020-01-01\"\ntrain_filt = daily_data_fb['ds'] <= split_date\ntest_filt = daily_data_fb['ds'] > split_date\n\ntrain_fb = daily_data_fb[train_filt]\ntest_fb = daily_data_fb[test_filt]","d800b8c3":"print(\"train data shape :\", train_fb.shape)\nprint(\"test data shape :\", test_fb.shape)","45699df4":"model_fbp = Prophet()\nfor feature in exogenous_features:\n    model_fbp.add_regressor(feature)\n\nmodel_fbp.fit(df_train[[\"Timestamp\", \"Weighted_Price\"] + exogenous_features].rename(columns={\"Timestamp\": \"ds\", \"Weighted_Price\": \"y\"}))\n\nforecast = model_fbp.predict(df_valid[[\"Timestamp\", \"Weighted_Price\"] + exogenous_features].rename(columns={\"Timestamp\": \"ds\"}))\nforecast.head()","5dc8ad43":"df_valid[\"Forecast_Prophet\"] = forecast.yhat.values","5871c4a7":"# Plot Our Predictions\nfig1 = model_fbp.plot(forecast)","69233d96":"model_fbp.plot_components(forecast)","674082de":"# Plotting changepoints\nfrom fbprophet.plot import add_changepoints_to_plot\nfig = model_fbp.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), model_fbp, forecast)","c8bb502e":"df_valid[[\"Weighted_Price\", \"Forecast_Prophet\"]].plot(figsize=(14, 7))","c61e300f":"test_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet'])\ntest_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet']))\n\nprint(f\" Prophet's MAE : {test_mae}\")\nprint(f\"Prophet's RMSE : {test_rmse}\")","f07a4596":"# #Keeping only relevant columns\n# forecast_df = forecast[['ds','yhat','yhat_lower','yhat_upper' ]]\n# forecast_df\n\n# # Plotting the predicted values -> yhat\n# forecast_df.set_index('ds')['yhat'].plot()\n\n# # Plotting both actual and the predicted values -> y and yhat\n# pd.concat([daily_data_fb.set_index('ds')['y'],forecast_df.set_index('ds')['yhat']], axis=1).plot()  \n\n# test = forecast_df[forecast_df['ds']>= \"2020-01-02\"]\n# df_concat = test.merge(test_fb,how='inner',on=\"ds\")\n# df_concat.plot(x=\"ds\", y=[\"y\", \"yhat\"])\n\n# # Checking Deltas\n# deltas = model.params['delta'].mean(0)\n# deltas\n\n# # Plotting changepoints\n\n# fig = plt.figure()\n# ax = fig.add_subplot(111)\n# ax.bar(range(len(deltas)), deltas)\n# ax.grid(True, which='major', c='red', ls='-', lw=1, alpha=0.2)\n# ax.set_ylabel('Rate change')\n# ax.set_xlabel('changepoint')\n# fig.tight_layout()\n\n# # Checking model changepoints\n\n# model.changepoints","771d8d04":"from sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nplt.style.use('fivethirtyeight')\n\nfrom datetime import datetime","12670764":"X_train, y_train = df_train[exogenous_features], df_train.Weighted_Price\nX_test, y_test = df_valid[exogenous_features], df_valid.Weighted_Price","30343755":"reg = xgb.XGBRegressor()","0234517c":"## Hyper Parameter Optimization Grid\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n \"max_depth\"        : [1, 3, 4, 5, 6, 7],\n \"n_estimators\"     : [int(x) for x in np.linspace(start=100, stop=2000, num=10)],\n \"min_child_weight\" : [int(x) for x in np.arange(3, 15, 1)],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n \"subsample\"        : [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1],\n \"colsample_bytree\" : [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n \"colsample_bylevel\": [0.5, 0.6, 0.7, 0.8, 0.9, 1],  \n \n}","55a75419":"model  = RandomizedSearchCV(    \n                reg,\n                param_distributions=params,\n                n_iter=10,\n                n_jobs=-1,\n                cv=5,\n                verbose=3,\n)","b61e814a":"model.fit(X_train, y_train)","22407e4d":"print(f\"Model Best Score : {model.best_score_}\")\nprint(f\"Model Best Parameters : {model.best_estimator_.get_params()}\")","324e1da0":"model.best_estimator_","a2121264":"df_train['Predicted_Weighted_Price'] = model.predict(X_train)\n\ndf_train[['Weighted_Price','Predicted_Weighted_Price']].plot(figsize=(15, 5))","5f82a239":"df_valid['Forecast_XGBoost'] = model.predict(X_test)\n\noverall_data = pd.concat([df_train, df_valid], sort=False)","02b85904":"overall_data[['Weighted_Price','Forecast_XGBoost']].plot(figsize=(15, 5))","51521ac3":"df_valid[['Weighted_Price','Forecast_XGBoost']].plot(figsize=(15, 5))","bbb6ef96":"from sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score","6ccfaf16":"train_mae = mean_absolute_error(df_train['Weighted_Price'], df_train['Predicted_Weighted_Price'])\ntrain_rmse = np.sqrt(mean_squared_error(df_train['Weighted_Price'], df_train['Predicted_Weighted_Price']))\ntrain_r2 = r2_score(df_train['Weighted_Price'], df_train['Predicted_Weighted_Price'])\n\nprint(f\"train MAE : {train_mae}\")\nprint(f\"train RMSE : {train_rmse}\")\nprint(f\"train R2 : {train_r2}\")","c3fc5c7b":"test_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost'])\ntest_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost']))\ntest_r2 = r2_score(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost'])\n\nprint(f\"test MAE : {test_mae}\")\nprint(f\"test RMSE : {test_rmse}\")\nprint(f\"test R2 : {test_r2}\")","55e39d7b":"df_valid[[\"Weighted_Price\", \"Forecast_ARIMAX\", \"Forecast_Prophet\", \"Forecast_XGBoost\"]].plot(figsize=(14,7))","7557f030":"arimax_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_ARIMAX']))\nfbp_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet']))\nxgb_rmse = np.sqrt(mean_squared_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost']))\n\narimax_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_ARIMAX'])\nfbp_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_Prophet'])\nxgb_mae = mean_absolute_error(df_valid['Weighted_Price'], df_valid['Forecast_XGBoost'])\n\n\nprint(\"ARIMAX RMSE :\", arimax_rmse)\nprint(\"FB Prophet RMSE :\", fbp_rmse)\nprint(\"XGBoost RMSE :\", xgb_rmse)\n\nprint(\"\\nARIMAX MAE :\", arimax_mae)\nprint(\"FB Prophet MAE :\", fbp_mae)\nprint(\"XGBoost MAE :\", xgb_mae)","005c7de1":"price_series = bitstamp_daily.reset_index().Weighted_Price.values\nprice_series","a9850adc":"price_series.shape","8cfac56f":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0, 1))\nprice_series_scaled = scaler.fit_transform(price_series.reshape(-1,1))","d66ac2b1":"price_series_scaled, price_series_scaled.shape","e7b1db93":"train_data, test_data = price_series_scaled[0:2923], price_series_scaled[2923:]","7f73b67e":"test_data","76e5d1e3":"# train_data = train_data.reshape(-1,1)\n# test_data = test_data.reshape(-1,1)","6f68a6b6":"train_data.shape, test_data.shape","baaa9333":"def windowed_dataset(series, time_step):\n    dataX, dataY = [], []\n    for i in range(len(series)- time_step-1):\n        a = series[i : (i+time_step), 0]\n        dataX.append(a)\n        dataY.append(series[i+ time_step, 0])\n        \n    return np.array(dataX), np.array(dataY)","72ed35f5":"X_train, y_train = windowed_dataset(train_data, time_step=100)\nX_test, y_test = windowed_dataset(test_data, time_step=100)","a7901114":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","f418e09c":"#reshape inputs to be [samples, timesteps, features] which is requred for LSTM\n\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n\nprint(X_train.shape) \nprint(X_test.shape)","9f612ff5":"print(y_train.shape) \nprint(y_test.shape)","3dcc5006":"#Create Stacked LSTM Model\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout","61d85493":"# Initialising the LSTM\nregressor = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50, return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth LSTM layer and some Dropout regularisation\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')","d41f10e7":"regressor.summary()","33390bbd":"# Fitting the RNN to the Training set\nhistory = regressor.fit(X_train, y_train, validation_split=0.1, epochs = 50, batch_size = 32, verbose=1, shuffle=False)","317fc9e7":"plt.figure(figsize=(16,7))\nplt.plot(history.history[\"loss\"], label= \"train loss\")\nplt.plot(history.history[\"val_loss\"], label= \"validation loss\")\nplt.legend()","a7552ac2":"#Lets do the prediction and performance checking\n\ntrain_predict = regressor.predict(X_train)\ntest_predict = regressor.predict(X_test)","eb6d8c95":"#transformation to original form\n\ny_train_inv = scaler.inverse_transform(y_train.reshape(-1, 1))\ny_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\ntrain_predict_inv = scaler.inverse_transform(train_predict)\ntest_predict_inv = scaler.inverse_transform(test_predict)","5db9b465":"plt.figure(figsize=(16,7))\nplt.plot(y_train_inv.flatten(), marker='.', label=\"Actual\")\nplt.plot(train_predict_inv.flatten(), 'r', marker='.', label=\"Predicted\")\nplt.legend()","92e2076c":"plt.figure(figsize=(16,7))\nplt.plot(y_test_inv.flatten(), marker='.', label=\"Actual\")\nplt.plot(test_predict_inv.flatten(), 'r', marker='.', label=\"Predicted\")\nplt.legend()","e252868f":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ntrain_RMSE = np.sqrt(mean_squared_error(y_train, train_predict))\ntest_RMSE = np.sqrt(mean_squared_error(y_test, test_predict))\ntrain_MAE = np.sqrt(mean_absolute_error(y_train, train_predict))\ntest_MAE = np.sqrt(mean_absolute_error(y_test, test_predict))\n\n\nprint(f\"Train RMSE: {train_RMSE}\")\nprint(f\"Train MAE: {train_MAE}\")\n\nprint(f\"Test RMSE: {test_RMSE}\")\nprint(f\"Test MAE: {test_MAE}\")","f07ca3a7":"test_data.shape","d86d5c64":"lookback = len(test_data) - 100\nx_input=test_data[lookback:].reshape(1,-1)\nx_input.shape","29c970ca":"x_input","cc0593b6":"lookback, len(test_data)","cea5bf91":"temp_input=list(x_input)\ntemp_input=temp_input[0].tolist()\ntemp_input","a2723782":"len(temp_input)","9ea2abb4":"# demonstrate prediction for next 100 days\nfrom numpy import array\n\nlst_output=[]\nn_steps=100\ni=0\nwhile(i<30):\n    \n    if(len(temp_input)>100):\n        #print(temp_input)\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        x_input=x_input.reshape(1,-1)\n        x_input = x_input.reshape((1, n_steps, 1))\n        #print(x_input)\n        yhat = regressor.predict(x_input, verbose=0)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.extend(yhat[0].tolist())\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps,1))\n        yhat = regressor.predict(x_input, verbose=0)\n        print(yhat[0])\n        temp_input.extend(yhat[0].tolist())\n        print(len(temp_input))\n        lst_output.extend(yhat.tolist())\n        i=i+1\n    \n\nprint(lst_output)","a577ab7d":"len(price_series_scaled)","656b0b1f":"df_=price_series_scaled.tolist()\ndf_.extend(lst_output)\nplt.plot(df_)","deec4c67":"plt.figure(figsize=(14,7))\ndf_invscaled=scaler.inverse_transform(df_).tolist()\nplt.plot(df_invscaled)","bc3669d6":"It appears that the Timestamp column is being treated as a integer rather than as dates. To fix this, we\u2019ll use the fromtimestamp() function which converts the arguments to dates.","662a4271":"Post time series decomposition we don't observe any seasonality. Also, there is no constant mean, variance and covariance, hence the series is **Non Stationary.** \nWe will perform statistical tests like KPSS and ADF to confirm our understanding.\n\nBut first, let's plot ACF and PACF graphs.","96a4d102":"# Rolling windows\n\nTime series data can be noisy due to high fluctuations in the market. As a result, it becomes difficult to gauge a trend or pattern in the data. \n\nAs we\u2019re looking at daily data, there\u2019s quite a bit of noise present. It would be nice if we could average this out by a week, which is where a rolling mean comes in. \n\nA rolling mean, or moving average, is a transformation method which helps average out noise from data. It works by simply splitting and aggregating the data into windows according to function, such as mean(), median(), count(), etc. For this example, we\u2019ll use a rolling mean for 3, 7 and 30 days.","d1a50644":"# Visualising the Time Series data","c1162cc3":"No Null values in the final output. Now we will move to **Exploratory Data Analysis**.\n\nFirst, we will create a copy of the dataset as in the next step we will be setting index as our Timestamp column. Its main advatange is to be able to query the datset fast and also helps in quering and filtering the dataset. Also, a lot of plots require our dataset to be indexed.","902ac241":"Coefficients values for lag>5 are statistically not significant and their impact on the model is minimal, except a few spikes at 8,11,22 and beyond.","4867e5d9":"**To summarize what happened above:**\n\n* data.resample() is used to resample the stock data.\n* The \u20181H\u2019 stands for hourly frequency, and denotes the offset values by which we want to resample the data.\n* mean() indicates that we want the average stock price during this period.\n\nThe offset values list can be found in the pandas [documentation](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#offset-aliases).","13607a40":"<a id=\"subsection-eight\"><\/a>\n# XG Boost","90cc1a1c":"<a id=\"subsection-one\"><\/a>\n# A first look at Bitcoin Prices\n\nLet\u2019s check what the first 5 lines of our time-series data look like:","810fa682":"**Narrative**\n\n* **Black dots** : the actual data points in our dataset.\n* **Deep blue line** : the predicted forecast\/the predicted values\n* **Light blue line** : the boundaries","3e3dd66c":"So there is a downward trend in stock prices from Dec-17 onwards till March 2019. We will investigate it further by investigation and with some findings during that period.","8025db83":"**Narrative**\n* **yhat** : the predicted forecast\n* **yhat_lower** : the lower border of the prediction\n* **yhat_upper**: the upper border of the prediction","a8fbaa5a":"There has been a increase in Bitcoin's weighted price except a slump in late 2018 and early 2019. Also, we can  observe a spike in weighted price in December 2017. We shall use Pandas to investigate it further in the coming sections.","c8b8afb1":"![bitcoin.jpg](attachment:bitcoin.jpg)","abac3d23":"Let us now look at the datatypes of the various components.","20b0c4a0":"# Comparing Models","0657036a":"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\n\nIt works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well. \n\n# How does Prophet work?\nThe procedure makes use of a decomposable time series model with three main model components: trend, seasonality, and holidays.\n\n**y(t) = g(t) + s(t) + h(t) + e(t)**\n\n- **g(t)** trend models non-periodic changes; linear or logistic\n- **s(t)** seasonality represents periodic changes; i.e. weekly, monthly, yearly\n- **h(t)** ties in effects of holidays; on potentially irregular schedules \u2265 1 day(s)\n\nThe error term e(t) represents any idiosyncratic changes which are not accommodated by the model; later we will make the parametric assumption that e(t) is normally distributed. Facebook paper can be referenced for more details [here](http:\/\/peerj.com\/preprints\/3190.pdf#section.1)\n\nLet's start modelling!","8c06ff90":"<a id=\"subsection-five\"><\/a>\n# ADF Test\n\nThe only difference here is the Null hypothesis which is just opposite of KPSS.\n\nThe null hypothesis of the test is the presence of **unit root**, that is, the series is **non-stationary**.","d9e869af":"Here, we we will opt for a ***hold-out based validation***. \n\nHold-out is used very frequently with time-series data. In this case, we will select all the data for 2020 as a hold-out and train our model on all the data from 2012 to 2019. ","30c90ede":"<a id=\"subsection-three\"><\/a>\n\n# Time resampling\n\nExamining stock price data for every single day isn\u2019t of much use to financial institutions, who are more interested in spotting market trends. To make it easier, we use a process called time resampling to aggregate data into a defined time period, such as by month or by quarter. Institutions can then see an overview of stock prices and make decisions according to these trends.\n\nThe pandas library has a .resample() function which resamples such time series data. The resample method in pandas is similar to its groupby method as it is essentially grouping according to a certain time span. The resample() function looks like this:","46c9a7e5":"<a id=\"subsection-seven\"><\/a>\n# Facebook Prophet\n\n![prophet.png](attachment:prophet.png)","2041ad29":"# Conclusion\n\n\nKPSS says series is not stationary and ADF says series is stationary. It means series is **difference stationary**, we will use **differencing** to make series stationary.","ddfae06f":"<a id=\"subsection-four\"><\/a>\n# KPSS Test\n\nThe KPSS test, short for, Kwiatkowski-Phillips-Schmidt-Shin (KPSS), is a type of Unit root test that tests for the stationarity of a given series around a deterministic trend.\n\nHere, the null hypothesis is that the series is **stationary**.\n\nThat is, if p-value is < signif level (say 0.05), then the series is non-stationary and vice versa.","a6fccf1f":"* **Long Short Term Memory networks** \u2013 usually just called \u201cLSTMs\u201d \u2013 are a special kind of RNN, capable of learning long-term dependencies. \n* LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn like RNNs!\n* All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single tanh layer.\n* Also, they don't suffer from problems like **vanishing\/exploding gradient descent**. \n\nTo master the theory, refer this amazing article [here](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/)","5d363f18":"<a id=\"section-three\"><\/a>\n\n# Exploratory Data Analysis\n\n**Visualizing the weighted price using markers**\n\nWhen working with time-series data, a lot can be revealed through visualizing it.\nIt is possible to add markers in the plot to help emphasize the specific observations or specific events in the time series.","df175f76":"# Simple Candlestick Graph","e3d272f6":"# Imputations Techniques for non Time Series Problems\n\nImputation refers to replacing missing data with substituted values.There are a lot of ways in which the missing values can be imputed depending upon the nature of the problem and data. Dependng upon the nature of the problem, imputation techniques can be broadly they can be classified as follows:\n\n**Basic Imputation Techniques**\n* 'ffill' or 'pad' - Replace NaNs with last observed value\n* 'bfill' or 'backfill' - Replace NaNs with next observed value\n*  Linear interpolation method","dedec655":"<a id=\"subsection-nine\"><\/a>\n# LSTM\n![LSTM.JPG](attachment:LSTM.JPG)","2919d690":"<a id=\"section-four\"><\/a>\n# Time Series Decomposition & Statistical Tests\n\nWe can decompose a time series into trend, seasonal amd remainder components, as mentioned in the earlier section. The series can be decomposed as an additive or multiplicative combination of the base level, trend, seasonal index and the residual.\nThe seasonal_decompose in statsmodels is used to implements the decomposition.\n\nWe will then perform some statistical tests like [KPSS](http:\/\/en.wikipedia.org\/wiki\/KPSS_test) and [Augmented Dickey\u2013Fuller](http:\/\/en.wikipedia.org\/wiki\/Augmented_Dickey%E2%80%93Fuller_test) tests to check stationarity. ","bba81eec":"# Table of Contents\n\n* [Introduction](#section-one)\n    - [First look](#subsection-one)\n* [Handling Missing Values](#section-two)\n* [Exploratory Data Analysis](#section-three)\n    - [Lag Plots](#subsection-two)\n    - [Time Resampling](#subsection-three)\n* [Time Series Decomposition & Statistical Tests](#section-four)\n    - [KPSS Test](#subsection-four)\n    - [ADF Test](#subsection-five)\n* [Feature Extraction](#section-five)\n* [Model Building](#section-six)\n    - [ARIMA](#subsection-six)\n    - [FB Prophet](#subsection-seven)\n    - [XG Boost](#subsection-eight)\n    - [LSTM](#subsection-nine)\n* [Model Selection](#section-seven)\n* [Next Steps](#section-eight)","e0f1c5fa":"<a id=\"subsection-two\"><\/a>\n\n# Visualizing using Lag Plots\n\nLag plot are used to observe the autocorrelation. These are crucial when we try to correct the trend and stationarity and we have to use smoothing functions. Lag plot helps us to understand the data better.","4a5776d2":"***Welcome to this notebook!***\n\nFeel free to fork for your own learning and edit the code or use in your own submissions. If you found this notebook enriched and helps improving your learning in the slightest, please ***Upvote*** this notebook as an encouragement for me to continue writing notebooks! :)","02befd3b":"Let's ensure there are no missing values before you perform statistical tests.","61f3dcdf":"<a id=\"subsection-six\"><\/a>\n# ARIMA Model\n\nARIMA is an acronym that stands for AutoRegressive Integrated Moving Average. It is a class of model that captures a suite of different standard temporal structures in time series data.This acronym is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n\n* **AR: Autoregression** A model that uses the dependent relationship between an observation and some number of lagged observations.\n* **I: Integrated** The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n* **MA: Moving Average** A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\nA standard notation is used of ARIMA(p,d,q) where the parameters are substituted with integer values to quickly indicate the speci\fc ARIMA model being used.\n\n* **p**: The number of lag observations included in the model, also called the lag order.\n* **d**: The number of times that the raw observations are differenced, also called the degree of dfferencing.\n* **q**: The size of the moving average window, also called the order of moving average.\n\nHappy further reading [here](http:\/\/en.wikipedia.org\/wiki\/Autoregressive_integrated_moving_average)","51f31608":"<a id=\"section-seven\"><\/a>\n# Model Selection and Prediction\n\nWe observed remarkable results using LSTMs. They really work a lot better for most sequence tasks! \n\nLet's predict weighted price for next 30 days. ","7ca2f5e4":"**Benefits :**\n\nSo, what are the key benefits of calculating a moving average or using this rolling mean method? Our data becomes a lot less noisy and more reflective of the trend than the data itself.","62f490bc":"<a id=\"section-six\"><\/a>\n# Model Building","3006ced3":"Let's sense check above stated 3 techniques on a subset to compare the output:","aaf983a3":" <a id=\"section-one\"><\/a>\n# Introduction \n\nThe first Bitcoin protocol and proof of concept was published in a Whitepaper in 2009 by a shadowy individual or group under the pseudonym Satoshi Nakamoto. Nakamoto, left the project in late 2010. Other developers took over and the Bitcoin community has since grown exponentially.\n\nBitcoin emerged out of the 2008 global economic crisis when big banks were caught misusing borrowers' money, manipulating the system, and charging exorbitant fees. To address such issues, Bitcoin creators wanted to put the owners of bitcoins in-charge of the transactions, eliminate the middleman, cut high interest rates and transaction fees, and make transactions transparent. They created a distributed network system, where people could control their funds in a transparent way.\n\nHowever, there are issues with bitcoins such as hackers breaking into accounts, high volatility of bitcoins, and long transaction delays. Considering the volatility it's always challenging to predict the bitcoin price. \n\nIn this notebook, we will be deep diving into the dataset, perform some EDA, feature engineering and will predict bitcoin price using Stochastic, Machine Learning and Deep Learning models. ","09bd29c8":"![missing.JPG](attachment:missing.JPG)","cc549cfd":"Let's extract time and date features from the Date column.","29d56f27":"# Plotting using Plotly\n\nPlotly allows us to make interactve charts which are pretty useful in financial analysis. \n\n* The **range-sliders** can be used to zoom-in and zoom-out.\n* The **range-selectors** can be used to select the range.","db90a656":"We can see that there is a positive correlation for minute, hour and daily lag plots. We observe absolutely no correlation for month lag plots.\n\nIt makes sense to re-sample our data atmost at the Daily level, thereby preserving the autocorrelation as well. ","ea2e574d":"# Interpreting KPSS test results\n\nThe output of the KPSS test contains 4 things:\n\n* The KPSS statistic\n* p-value\n* Number of lags used by the test\n* Critical values\n\nThe **p-value** reported by the test is the probability score based on which you can decide whether to reject the null hypothesis or not. If the p-value is less than a predefined alpha level (typically 0.05), we reject the null hypothesis.\n\nThe **KPSS statistic** is the actual test statistic that is computed while performing the test.\n\nThe number of **lags** reported is the number of lags of the series that was actually used by the model equation of the kpss test.\n\nIn order to reject the null hypothesis, the test statistic should be greater than the provided critical values. If it is in fact higher than the target critical value, then that should automatically reflect in a low p-value.\nThat is, if the p-value is less than 0.05, the kpss statistic will be greater than the 5% critical value.","893f5185":"<a id=\"section-eight\"><\/a>\n# Next Steps\n\n- We can increase the number of epochs to refine our model performance, you can increase epochs to 100 and can the results. Also, number of lag features can be increased beyond 100 to help learning the model. \n- I will use  LSTM Autoencoders for Anomaly Detection in Time Series.\n\nPlease keep an eye on this notebook and ***Upvote*** if you learnt something new from this :)","59e5bb30":"The Auto ARIMAX model seems to do a fairly good job in predicting the bitcoin price given data till the previous day. Can other models beat this benchmark?","b1c7a63b":"<a id=\"section-two\"><\/a>\n# Handling Missing Values in Time-series Data\n\nIt is very common for a time-series data to have missing data. The first step is to detect the count\/percentage of missing values in every column of the dataset. This will give an idea about the distribution of missing values. Let's check this in our dataset:","73a41684":"# Visualising using KDEs\n\nSummarizing the data with Density plots to see where the mass of the data is located.","1cb40bcc":"<a id=\"section-five\"><\/a>\n# Feature Extraction","2e7a59f5":"# About the Bitcoin Data\u00b6\n\nIncluded here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. It consists time period of Jan 2012 to September 2020, with minute to minute updates of OHLC (Open, High, Low, Close), Volume in BTC and indicated currency, and weighted bitcoin price. \n\nNow that our data has been converted into the desired format, let\u2019s take a look at its various columns for further analysis.\n\n* The **Open and Close** columns indicate the opening and closing price on a particular day.\n* The **High and Low** columns provide the highest and the lowest price on a particular day, respectively.\n* The **Volume** column tells us the total volume of traded on a particular day.\n* The **Weighted price** is a trading benchmark used by traders that gives the weighted price a security has traded at throughout the day, based on both volume and price. It is important because it provides traders with insight into both the trend and value of a security. To read more about how Weighted price is calculated, click [here](http:\/\/bitcointalk.org\/index.php?topic=2777198.0). ","8ed8d0ba":"**Regression metrics**\n\nThe sklearn.metrics module implements several loss, score, and utility functions to measure regression performance. We will apply some of them:\n\n1. **Mean absolute error** : The mean_absolute_error function computes mean absolute error, a risk metric corresponding to the expected value of the absolute error loss or L1-norm loss.\n\n2. **Mean squared error** : The mean_squared_error function computes mean square error, a risk metric corresponding to the expected value of the squared error.\n \n3. **R\u00b2 score, the coefficient of determination** : R-squared says how good your model fits the data. R-squared  closer to 1.0 says that the model fits the data quite well, whereas closer 0 means  that model isn\u2019t that good. R-squared can also be negative when the model just  makes absurd predictions. ","49075e94":"The above graph shows that effect barely detoriate over time, so past values affect the present ones. The more lags we include, the better our model will fit the dataset, now the risk is coefficients might predict the dataset too well, cause an overfitting.\nIn our model, we always try to include only those lags which have a direct effect on our present value. Hence, let's try PACF.","03b9a5e1":"# Important Note on Cross Validation\n\nTo measure the performance of our forecasting model, We typically want to split the time series into a training period and a validation period. This is called fixed partitioning. \n\n* If the time series has some seasonality, you generally want to ensure that each period contains a whole number of seasons. For example, one year, or two years, or three years, if the time series has a yearly seasonality. \nYou generally don't want one year and a half, or else some months will be represented more than others. \n \n* We'll train our model on the training period, we'll evaluate it on the validation period. Here's where you can experiment to find the right architecture for training. And work on it and your hyper parameters, until you get the desired performance, measured using the validation set. Often, once you've done that, you can retrain using both the training and validation data.And then test on the test(or forecast) period to see if your model will perform just as well.\n \n* And if it does, then you could take the unusual step of retraining again, using also the test data. But why would you do that? Well, it's because the test data is the closest data you have to the current point in time. And as such it's often the strongest signal in determining future values. If your model is not trained using that data, too, then it may not be optimal.\n\nHere, we we will opt for a ***hold-out based validation***. \n\nHold-out is used very frequently with time-series data. In this case, we will select all the data for 2020 as a hold-out and train our model on all the data from 2012 to 2019. \n","1dba52d8":"**Imputation using Linear Interpolation method**\n\nTime series data has a lot of variations against time. Hence, imputing using backfill and forward fill isn't the best possible solution to address the missing value problem. A more apt alternative would be to use interpolation methods, where the values are filled with incrementing or decrementing values.\n\n[Linear interpolation](https:\/\/www.lexjansen.com\/nesug\/nesug01\/ps\/ps8026.pdf) is an imputation technique that assumes a linear relationship between data points and utilises non-missing values from adjacent data points to compute a value for a missing data point.\n\nRefer to the official documentation for a complete list of interpolation strategies [here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.interpolate.html)\n\nIn our dataset, we will be performing Linear interpolation on the missing value columns."}}