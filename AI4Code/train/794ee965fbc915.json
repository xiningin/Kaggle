{"cell_type":{"55054e11":"code","57c9cda9":"code","eb6f3641":"code","cb6e6b5c":"code","2c2fb237":"code","d9173023":"code","61998b35":"code","545dbfb2":"code","45e30202":"code","70d39f44":"code","f471eade":"code","96842472":"code","1e0c12c1":"code","b9b58170":"code","f2598848":"markdown","74d77303":"markdown","6945301a":"markdown","ba559bbb":"markdown","7492cba8":"markdown","6dbf5275":"markdown","c837dcff":"markdown","d1fd7ef4":"markdown","43e6e1f6":"markdown","21a8ff43":"markdown","09533dfa":"markdown","efce4277":"markdown","25c45558":"markdown"},"source":{"55054e11":"# Necessary Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # For plot\nfrom sklearn.model_selection import train_test_split # For splitting data as train and test\nfrom sklearn.linear_model import LogisticRegression # Sklearn library - Logistic Regression\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","57c9cda9":"# Read csv\ndata = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndata.head()","eb6f3641":"data.info()","cb6e6b5c":"# Lets check our output\nprint(\"Maximum Quality Value:\",data.quality.max())\nprint(\"Minimum Quality Value:\",data.quality.min())\nprint(data.quality.head())","2c2fb237":"data.head()","d9173023":"data.quality = [1 if each >= 7 else 0 for each in data.quality] # if quality is 7 or 8 it is good quality which means \"1\"\nprint(data.info())\nprint(data.head())\ny = data.quality.values\nx_data = data.drop([\"quality\"],axis=1)","61998b35":"# Normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values\nx.head()","545dbfb2":"# Test - Train Split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test: \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test: \",y_test.shape)","45e30202":"# Parameter initialize and sigmoid function\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n# w,b = initialize_weights_and_bias(30)\n\ndef sigmoid(z):\n    \n    y_head = 1\/(1+ np.exp(-z))\n    return y_head\n\n# print(sigmoid(0))","70d39f44":"def forward_backward_propagation(w,b,x_train,y_train):\n    \n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","f471eade":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 5 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","96842472":"# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","1e0c12c1":"# logistic_regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    \n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100)","b9b58170":"# Logistic Regression with Sklearn \nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","f2598848":"##  Optimization Algorithm with Gradient Descent\n* Well, now we know what is our cost that is error.\n* Therefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction.\n* Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them.\n* In order to decrease cost, we need to update weights and bias.\n* In other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.\n* Lets make an example:\n    * We have w = 5 and bias = 0 (so ignore bias for now). Then we make forward propagation and our cost function is 1.5.\n    * It looks like this. (red lines)\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/dAaYJH\/7.jpg\" alt=\"7\" border=\"0\"><\/a>\n    * As you can see from graph, we are not at minimum point of cost function. Therefore we need to go through minimum cost. Okey, lets update weight. ( the symbol := is updating)\n    * w := w - step. The question is what is this step? Step is slope1. Okey, it looks remarkable. In order to find minimum point, we can use slope1. Then lets say slope1 = 3 and update our weight. w := w - slope1 => w = 2.\n    * Now, our weight w is 2. As you remember, we need to find cost function with forward propagation again. \n    * Lets say according to forward propagation with w = 2, cost function is 0.4. Hmm, we are at right way because our cost function is decrease. We have new value for cost function that is cost = 0.4. Is that enough? Actually I do not know lets try one more step.\n    * Slope2 = 0.7 and w = 2. Lets update weight w : = w - step(slope2) => w = 1.3 that is new weight. So lets find new cost.\n    * Make one more forward propagation with w = 1.3 and our cost = 0.3. Okey, our cost even decreased, it looks like fine but is it enough or do we need to make one more step? The answer is again I do not know, lets try.\n    * Slope3 = 0.01 and w = 1.3. Updating weight w := w - step(slope3) => w = 1.29 ~ 1.3. So weight does not change because we find minimum point of cost function. \n    * Everything looks like good but how we find slope? If you remember from high school or university, in order to find slope of function(cost function) at given point(at given weight) we take derivative of function at given point. Also you can ask that okey well we find slope but how it knows where it go. You can say that it can go more higher cost values instead of going minimum point. The asnwer is that slope(derivative) gives both step and direction of step. Therefore do not worry :)\n    * Update equation is this. It says that there is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with  \u03b1 learning rate. Then update weight. (In order to explain I ignore bias but these all steps will be applied for bias)\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hYTTJH\/8.jpg\" alt=\"8\" border=\"0\"><\/a>\n    * Now, I am sure you are asking what is learning rate that I mentioned never. It is very simple term that determines learning rate. Hovewer there is tradeoff between learning fast and never learning. For example you are at Paris(current cost) and want to go Madrid(minimum cost). If your speed(learning rate) is small, you can go Madrid very slowly and it takes too long time. On ther other hand, if your speed(learning rate) is big, you can go very fast but maybe you make crash and never go to Madrid. Therefore, we need to choose wisely our speed(learning rate).\n    * Learning rate is also called hyperparameter that need to be chosen and tuned. I will explain it more detailed in artificial neural network with other hyperparameters. For now just say learning rate is 1 for our previous example.\n  \n* I think now you understand the logic behind forward propagation(from weights and bias to cost) and backward propagation(from cost to weights and bias to update them). Also you learn gradient descent. Before implementing the code you need to learn one more thing that is how we take derivative of cost function according to weights and bias. It is not related with python or coding. It is pure mathematic. There are two option first one is to google how to take derivative of log loss function and second one is even to google what is derivative of log loss function :) I choose second one because I cannot explain math without talking :) \n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","74d77303":"Now lets put them all together.","6945301a":"## Initializing parameters\n\n* The first step is multiplying each x values with their own weights.\n* The question is that what is the initial value of weights?\n* There are some techniques but to not make problem more difficult initial weights were selected as 0.01.\n* Also initial bias is 0.\n* Lets write some code.","ba559bbb":"* Up to this point we learn our parameters. It means we fit the data. \n* In order to predict we have parameters. Therefore, lets predict.\n* In prediction step we have x_test as a input and while using it, we make forward prediction.","7492cba8":"### If quality is 7 or 8 it is good quality which means \"1\" otherwise \"0\" which means bad quality","6dbf5275":"# Logistic Regression\n* Logistic Regression can be used as classification model when we talk about binary classification (0 and 1 outputs).\n* Logistic regression is actually a very simple neural network.","c837dcff":"* As you see, our output (Wine Quality) value is between 3 and 8. So, It is not binary value.\n* You can think that we shouldn't use Logistic Regression Classification for this dataset because the output is not binary value. But, We can convert it to binary. I will make quality 7 or higher getting classified as 'good\/1' and the remainder as 'not good\/0.\n## - Let's start","d1fd7ef4":"This notebook was prepared with the help of DATAI Team Machine Learning Udemy Course and DATAI Team Deep Learning Tutorial for Beginners Notebook on Kaggle.\n\n# Red Wine Quality \n### Input variables (based on physicochemical tests):\n* fixed acidity\n* volatile acidity\n* citric acid\n* residual sugar\n* chlorides\n* free sulfur dioxide\n* total sulfur dioxide\n* density\n* pH\n* sulphates\n* alcohol\n\n\n### Output variable (based on sensory data):\n* quality (score between 0 and 10)","43e6e1f6":"* As we write sigmoid method and calculate y_head. Lets learn what is loss(error) function\n* Loss function has an important job in that it must faithfully distill all aspects of the model down into a single number in such a way that improvements in that number are a sign of a better model. Let's say we have a prediction that is correct or not correct and how do we check whether it is correct or not? The answer is with loss(error) function:\n* Mathematical expression of log loss(error) function is that: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>\n* It says that if you make wrong prediction, loss(error) becomes big.\n* After that, we find the cost function that is summation of loss function. Each input creates loss function. Cost function is summation of loss functions that is created by each input.","21a8ff43":"## Summary\nWhat we did at this first part:\n    \n* Initialize parameters weight and bias\n* Forward propagation\n* Loss function\n* Cost function\n* Backward propagation (gradient descent)\n* Prediction with learnt parameters weight and bias\n* Logistic regression with sklearn\n\nIn conclusion, in first part we learned the math behind logistic regression classification and wrote the code. In second part, we used sklearn library for logistic regression classification. \n    \n### Our accuracy without using library is 85.3125 %, with library 86.875 %","09533dfa":"## Forward Propagation\n* ### The all steps from X to cost is called forward propagation\n\n    * z = (w.T)x + b => in this equation we know x that is array, we know w (weights) and b (bias) so the rest is calculation. (T is transpose)\n    * Then we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n    * Then we calculate loss(error) function.\n    * Cost function is summation of all loss(error).\n    * Lets start with z and the write sigmoid definition(method) that takes z as input parameter and returns y_head(probability)","efce4277":"### Now lets look at computation graph of logistic regression\n![image.png](attachment:image.png)\n* Parameters are weight and bias.\n* Weights: coefficients of each pixels\n* Bias: intercept\n* z = (w.t)x + b => z equals to (transpose of weights times input x) + bias\n* In an other saying => z = b + x1w1 + x2w2 + ... + x_n*w_n\n* y = sigmoid(z)\n* Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n\n### Why we use sigmoid function?\n* It gives probabilistic result\n* It is derivative so we can use it in gradient descent algorithm.\n* Lets make example:\n* Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification result is 1 with 90% probability.\n* Now lets start with from beginning and examine each component of computation graph more detailed.","25c45558":"* We learn logic behind simple neural network(logistic regression) and how to implement it.\n* Now that we have learned logic, we can use sklearn library which is easier than implementing all steps with hand for logistic regression.\n\n## Logistic Regression with Sklearn\n* In sklearn library, there is a logistic regression method that ease implementing logistic regression."}}