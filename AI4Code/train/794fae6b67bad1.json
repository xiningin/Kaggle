{"cell_type":{"f6204395":"code","c356d765":"code","8f2a34c3":"code","13d40ff3":"code","ded7b7a1":"code","c75becb5":"code","729441dd":"code","e391b42e":"code","9dcf08eb":"code","c3026bd0":"code","63c6a3a1":"code","336fcf7a":"code","fd07b372":"code","fe91407a":"code","ba21627b":"code","984bd030":"code","1434adef":"code","61fa611e":"code","17e65a4d":"code","dcaeb1b4":"code","108721a1":"code","84f8d806":"code","53734001":"markdown","a77d534e":"markdown","b8ee0cc9":"markdown","d2444bb3":"markdown","f4a78c21":"markdown","e18466c3":"markdown","e91dc9cd":"markdown","e844c4c1":"markdown","72bed91e":"markdown"},"source":{"f6204395":"\nimport io\nimport os\nimport math\nimport copy\nimport pickle\nimport zipfile\nfrom textwrap import wrap\nfrom pathlib import Path\nfrom itertools import zip_longest\nfrom collections import defaultdict\nfrom urllib.error import URLError\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\n\nimport numpy as np\nimport pandas as pd","c356d765":"\ndef try_download(url, download_path):\n    archive_name = url.split('\/')[-1]\n    folder_name, _ = os.path.splitext(archive_name)\n    \n    try:\n        r = urlopen(url)\n    except URLError as e:\n        print('Cannot download the data. Error: %s' % s)\n        return \n\n    assert r.status == 200\n    data = r.read()\n\n    with zipfile.ZipFile(io.BytesIO(data)) as arch:\n        arch.extractall(download_path)\n        \n    print('The archive is extracted into folder: %s' % download_path)\n\n# It also includes reading the data\ndef read_data(path):\n    files = {}\n    for filename in path.glob('*'):\n        if filename.suffix == '.csv':\n            files[filename.stem] = pd.read_csv(filename)\n        elif filename.suffix == '.dat':\n            if filename.stem == 'ratings':\n                columns = ['userId', 'movieId', 'rating', 'timestamp']\n            else:\n                columns = ['movieId', 'title', 'genres']\n            data = pd.read_csv(filename, sep='::', names=columns, engine='python',encoding = \"ISO-8859-1\")\n            files[filename.stem] = data\n    return files['ratings'], files['movies']","8f2a34c3":"# 1M Movielens dataset URL\narchive_url = f'http:\/\/files.grouplens.org\/datasets\/movielens\/ml-1m.zip'\ndownload_path = Path.home() \/ 'data' \/ 'movielens'\n\n# Download dataset\ntry_download(archive_url, download_path)\n\n# Pick one of the available folders and we are done\nratings, movies = read_data(download_path \/'ml-1m')","13d40ff3":"ratings.sample(5)","ded7b7a1":"movies.sample(5)","c75becb5":"print('There are {} rows of data from {} users'.format(len(ratings), len(ratings.userId.unique())))","729441dd":"from sklearn.preprocessing import LabelEncoder\n\ndef create_df(df):\n\n    # Encoding users and movies as integers\n    user_ids=df['userId'].unique().tolist()\n    user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n    userencoded2user = {i: x for i, x in enumerate(user_ids)}\n    user_enc = LabelEncoder()\n    df['user'] = user_enc.fit_transform(df['userId'].values)\n    \n\n    movie_ids = df[\"movieId\"].unique().tolist()\n    movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n    movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n    movie_enc = LabelEncoder()\n    df['movie'] = movie_enc.fit_transform(df['movieId'].values)\n\n    # Number of unique users and movies\n    n_users = df['userId'].nunique()\n    n_movies = df['movieId'].nunique()\n\n    df['rating'] = df['rating'].values.astype(np.float32)\n\n    # Min and Max ratings from the database\n    # We will use these for normalization\n    min_rating = min(df['rating'])\n    max_rating = max(df['rating'])\n\n    return n_users, n_movies, min_rating, max_rating, user2user_encoded, userencoded2user, movie2movie_encoded, movie_encoded2movie, df\n\n","e391b42e":"from sklearn.model_selection import train_test_split\n\n# Create our df by calling create_df function & create X,y for split\nn_users, n_movies, min_rating, max_rating, user2user_encoded, userencoded2user, movie2movie_encoded, movie_encoded2movie, df =create_df(ratings)\n\nX = df[['user', 'movie']].values\ny = df['rating'].values # our target\n\n\n# Split by %90-%10 for train and test as we have 1M row data. \n# Test will have 100k row data.\n# We could do %95-%5 but i prefer that way. As 50k row would be enough for test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, random_state=10)\n\n# We didn't normalize our target(y) for first try, we will do it later\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","9dcf08eb":"print(f'Embeddings: {n_users} users, {n_movies} movies')\nprint(f'Dataset(user&movie ids) shape: {df.shape}')\nprint(f'Target(ratings) shape: {y.shape}')\n\nprint(f'We will use user and movie as features and rating as the target of our embedding based model.')","c3026bd0":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\n\nfrom keras.models import Model\nfrom keras.layers import Input, Reshape, Dot\nfrom keras.regularizers import l2\n\nimport matplotlib.pyplot as plt\n\n# Make users and movies into separate arrays in the training and test data for Keras input\nX_train_array = [X_train[:, 0], X_train[:, 1]]\nX_test_array = [X_test[:, 0], X_test[:, 1]]\n\n# EmbeddingLayer as a calling class\nclass EmbeddingLayer:\n    def __init__(self, n_items, n_factors):\n        self.n_items = n_items\n        self.n_factors = n_factors\n    \n    def __call__(self, Embedded):\n        Embedded = layers.Embedding(self.n_items, \n                             self.n_factors, \n                             embeddings_initializer='he_normal',\n                             embeddings_regularizer=l2(1e-6))(Embedded)\n        Embedded = Reshape((self.n_factors,))(Embedded)\n        return Embedded\n\n\ndef rec_Model(n_users, n_movies, embedding_size=50):\n    #Creating an embedding layer having embedding vectors of user and item ids.\n    user = Input(shape=(1,))\n    u = EmbeddingLayer(n_users, embedding_size)(user)\n        \n    movie = Input(shape=(1,))\n    m = EmbeddingLayer(n_movies, embedding_size)(movie)\n    \n    # Dot product for interactions between users and items\n    interaction = Dot(axes=1)([u, m])\n\n    # We used Adam for optimizer as learning_rate=0.001\n    rec_model = Model(inputs=[user, movie], outputs=interaction)\n    opt = Adam(learning_rate=0.001)\n\n    # Computing loss as mean squared error\n    rec_model.compile(loss='mean_squared_error', optimizer=opt)\n\n    return rec_model #recommender model ","63c6a3a1":"model = rec_Model(n_users, n_movies)\nmodel.summary()","336fcf7a":"history1 = model.fit(x=X_train_array, \n                      y=y_train, \n                      batch_size=1000, \n                      epochs=10, \n                      verbose=1, \n                      validation_data=[X_test_array, y_test])","fd07b372":"plt.plot(history1.history[\"loss\"])\nplt.plot(history1.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","fe91407a":"from keras.layers import Activation, Concatenate, Dense, Dropout\n\n\ndef NNmodel(n_users, n_movies, embedding_size=50):\n    user = Input(shape=(1,))\n    u = EmbeddingLayer(n_users, embedding_size)(user)\n    \n    movie = Input(shape=(1,))\n    m = EmbeddingLayer(n_movies, embedding_size)(movie)\n    \n    # Concat on the embedding layers and dropout layer\n    interaction = Concatenate()([u, m])\n    interaction = Dropout(0.05)(interaction)\n\n    # Dense layer with relu and sigmoid activation functions for interaction    \n    interaction = Dense(10, kernel_initializer='he_normal')(interaction)\n    interaction = Activation('relu')(interaction)\n    interaction = Dropout(0.5)(interaction)\n    \n    interaction = Dense(1, kernel_initializer='he_normal')(interaction)\n    interaction = Activation('sigmoid')(interaction)\n\n    # Again used Adam with lr=0.001\n    NNmodel = Model(inputs=[user, movie], outputs=interaction)\n    opt = Adam(learning_rate=0.001)\n    NNmodel.compile(loss='mean_squared_error', optimizer=opt)\n\n    return NNmodel","ba21627b":"model2 = NNmodel(n_users, n_movies)\nmodel2.summary()","984bd030":"history2 = model2.fit(x=X_train_array, \n                      y=y_train, \n                      batch_size=1000, \n                      epochs=10, \n                      verbose=1, \n                      validation_data=(X_test_array, y_test))","1434adef":"plt.plot(history2.history[\"loss\"])\nplt.plot(history2.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","61fa611e":"\ny2 = df[\"rating\"].apply(lambda x: (x - min_rating) \/ (max_rating - min_rating)).values\n\n# Again split data\nX_train, X_test, y2_train, y2_test = train_test_split(X, y2, train_size=0.9, random_state=10)\n\nX_train.shape, X_test.shape, y2_train.shape, y2_test.shape\n\n\n","17e65a4d":"model3 = NNmodel(n_users, n_movies)\nmodel3.summary()","dcaeb1b4":"history3 = model3.fit(x=X_train_array, \n                      y=y2_train, \n                      batch_size=1000, \n                      epochs=5, \n                      verbose=1, \n                      validation_data=(X_test_array, y2_test))","108721a1":"plt.plot(history3.history[\"loss\"])\nplt.plot(history3.history[\"val_loss\"])\nplt.title(\"model loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\nplt.show()","84f8d806":"# Let us get a random user and see the top recommendations for that user\nuser_id = df.userId.sample(1).iloc[0]\n\n# We need movies df this time\nmovie_df=movies\nmovies_watched_by_user = df[df.userId == user_id]\n\n# We have movies watched by user, so we need movies not watched\nmovies_not_watched = movie_df[~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)][\"movieId\"]\nmovies_not_watched2 = list(set(movies_not_watched).intersection(set(movie2movie_encoded.keys())))\nmovies_not_watched3 = [[movie2movie_encoded.get(x)] for x in movies_not_watched2]\n\n# Prepare user movie array for model prediction\nuser_encoder = user2user_encoded.get(user_id)\nuser_movie_array = np.hstack(([[user_encoder]] * len(movies_not_watched3), movies_not_watched3))\nuser_movie_array2= [user_movie_array[:, 0], user_movie_array[:, 1]]\n\n# Predict movies for user to recommend\nratings_for_user= model3.predict(user_movie_array2).flatten()\n\n# Sort predicted movies\ntop_ratings_indices = ratings_for_user.argsort(axis=0)[-5:][::-1]\n\n# Get movie ids\nrecommended_movie_ids = [movie_encoded2movie.get(movies_not_watched3[x][0]) for x in top_ratings_indices]\n\n\n# Print movies which are recommended by our model\nprint(\"----\" * 11)\nprint(f\"Top 5 movie recommendations for user: {user_id}\")\nprint(\"----\" * 11)\nrecommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\nfor row in recommended_movies.itertuples():\n    print(row.title, \":\", row.genres)\n","53734001":"Let's do some preprocessing for out data.\n\nPrepare it as df and encode users and movies as integer indices.\n\nHere we used sklearn's LabelEncoder for this process.","a77d534e":"Our loss dropped from 0.79 to 0.051 and with only just 5 epochs","b8ee0cc9":"Now we will start our collaborative filtering model\n\nImporting tensorflow and keras","d2444bb3":"## Movie Recommendation For a User By Our Model","f4a78c21":"Data download code starts here:","e18466c3":"Now we add neural network to our model with a dropout and concatenatation\n\nActually I tried with more dense layers and dropouts but it didn't further improve our score","e91dc9cd":"Let's quickly check what we have in the dataset:\n","e844c4c1":"Here we start by importing libraries for getting the data.\n\nActually, I used data by uploading Google Colab from my laptop, \nbecause of my laptop's lack of ram.\n\nTherefore, in order to be consistent and provide integrity in the code when you run, I searched and find this small downloading code to download Movielens 1M dataset. \n\nHere are the download code's imports:","72bed91e":"Now final trick\n\nWe will try to normalize our target and let's see what will our NNmodel do\n\nNormalize the target between 0 and 1. Makes it easy to train."}}