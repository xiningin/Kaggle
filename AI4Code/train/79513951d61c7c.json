{"cell_type":{"7c26a507":"code","beadce84":"code","d1315b1a":"code","1e04bf3c":"code","2244eadc":"code","36dc3139":"code","f3b55824":"code","c092ddfb":"code","1b513207":"code","1ad25fe8":"code","456feb05":"code","d08a34fa":"code","3789cab7":"code","3278791f":"code","47aab976":"code","0f67fa82":"code","f96110cb":"code","dfb278c1":"code","2d7aad82":"code","f6445d72":"code","8d049455":"code","417971a1":"code","a0123b84":"code","40fd4ba8":"code","18bb5949":"code","89242bfc":"code","63c3dd72":"code","b1608ec1":"code","99eafb3e":"code","710862e3":"code","c6f49ec7":"code","678f1bc3":"code","abc69548":"code","6dd2228c":"code","da259af5":"code","d4577827":"code","83c86f30":"code","1260a0aa":"code","3655cb34":"code","bd2cdc36":"code","6f3535b4":"code","4bf62564":"code","962ac8a5":"code","5e0ebe3a":"code","1ac1ad5e":"markdown","31f28b63":"markdown","774f3b06":"markdown","0b19b871":"markdown","e763e6d6":"markdown","4e8e3e7f":"markdown","120f11c2":"markdown","cf3ba134":"markdown","281d7e5c":"markdown","5a1260cf":"markdown","0642ca22":"markdown","31c24bdf":"markdown","ff3fb2d4":"markdown","30282c64":"markdown","ccfd69fa":"markdown","0cabfa05":"markdown","4602938b":"markdown","bcb239b5":"markdown"},"source":{"7c26a507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","beadce84":"# creating a dataframe out of embol_train.json\ndf = pd.read_json('\/kaggle\/input\/github-bugs-prediction\/embold_train.json')\ndf['combined'] = df['title']+'. '+df['body']\ndf.head()","d1315b1a":"df_bug = df[df['label']==0]\ndf_feature = df[df['label']==1]\ndf_question = df[df['label']==2]","1e04bf3c":"import nltk\nimport re\nimport string\n\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    return text","2244eadc":"from string import punctuation\nfrom nltk.corpus import stopwords\n\ndef punctuation_stopwords_removal(git_text):\n    # filters charecter-by-charecter : ['h', 'e', 'e', 'l', 'o', 'o', ' ', 'm', 'y', ' ', 'n', 'a', 'm', 'e', ' ', 'i', 's', ' ', 'p', 'u', 'r', 'v', 'a']\n    remove_punctuation = [ch for ch in git_text if ch not in punctuation]\n    # convert them back to sentences and split into words\n    remove_punctuation = \"\".join(remove_punctuation).split()\n    filtered_git_text = [word.lower() for word in remove_punctuation if word.lower() not in stopwords.words('english')]\n    return filtered_git_text","36dc3139":"from collections import Counter\nimport plotly.express as px\n\ndef plot_most_common_words(df_category, category):\n    df_category['combined'] = df_category['combined'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\n    df_category['combined'] = df_category['combined'].apply(lambda x: clean_text(x))\n    \n    df_category[\"combined\"] = df_category[\"combined\"].apply(punctuation_stopwords_removal)\n    \n    word_list = []\n    \n    for i, j in df_category.iterrows():\n        for word in j['combined']:\n            word_list.append(word)\n        \n    count_dict = Counter(word_list)\n    most_common_words_df = pd.DataFrame(count_dict.most_common(20), columns=['word', 'count'])\n    fig = px.histogram(most_common_words_df,\n                       x='word', \n                       y='count',\n                       title='Most common terms used while refering to a GitHub {}'.format(category),\n                       color_discrete_sequence=['#843B62'] )\n    fig.show()","f3b55824":"# taking only a subset; since huge dataset\nplot_most_common_words(df_bug[:10000], 'Bugs \ud83d\udd77')","c092ddfb":"plot_most_common_words(df_feature[:7000], 'Features \ud83d\udca1')","1b513207":"plot_most_common_words(df_question[:7000], 'Questions \u2049\ufe0f')","1ad25fe8":"\"\"\"\nlabel 0: Bug\nlabel 1: Feature\nlabel 2: Question\n\"\"\"\ndf['combined'] = df['combined'].apply(lambda x: x.replace(\"\\\\r\", \"\"))\ndf['combined'] = df['combined'].apply(lambda x: clean_text(x))\ndf.head()","456feb05":"df.drop(['title', 'body'], axis=1, inplace=True)\ndf.head()","d08a34fa":"import torch\nimport torch.nn as nn\n\nimport transformers\nfrom transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader","3789cab7":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","3278791f":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","47aab976":"#loading our BERT model\nBERT_UNCASED = '\/kaggle\/input\/bert-base-uncased'","0f67fa82":"#loading the pre-trained BertTokenizer\ntokenizer = BertTokenizer.from_pretrained(BERT_UNCASED)","f96110cb":"# some basic operations to understand how BERT converts a sentence into tokens and then into IDs\nsample_body = 'script stopped adding videos saenzramiro abc xyz'\ntokens = tokenizer.tokenize(sample_body)\ntoken_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f' Sentence: {sample_body}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","dfb278c1":"# using encode_plus to add special tokens : [CLS]:101, [SEP]:102, [PAD]:0\nencodings = tokenizer.encode_plus(\n            sample_body,\n            max_length=32,\n            add_special_tokens=True,\n            return_token_type_ids=False,\n            pad_to_max_length=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n)\n\nencodings.keys()","2d7aad82":"print('Input IDs : {}'.format(encodings['input_ids'][0]))\nprint('\\nAttention Mask : {}'.format(encodings['attention_mask'][0]))","f6445d72":"MAX_LENGTH = 512","8d049455":"class GitHubCommitMessages(Dataset):\n    \n    def __init__(self, commit_message, label, tokenizer, max_len):\n        self.commit_message = commit_message\n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.commit_message)\n    \n    def __getitem__(self, item):\n        commit_message = str(self.commit_message[item])\n        label = self.label[item]\n        \n        encoding = self.tokenizer.encode_plus(\n        commit_message,\n        add_special_tokens=True,\n        max_length=self.max_len,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors='pt')\n        return {\n        'commit_message': commit_message,\n         'input_ids': encoding['input_ids'],\n         'attention_mask': encoding['attention_mask'],\n         'label': torch.tensor(label, dtype=torch.long)\n          }","417971a1":"from sklearn.model_selection import train_test_split","a0123b84":"df.head()","40fd4ba8":"df = df[:2000]","18bb5949":"df.shape","89242bfc":"training_data, testing_data = train_test_split(\n    df,\n    test_size=0.1,\n    random_state=RANDOM_SEED\n)\n\ntesting_data, validation_data = train_test_split(\n    testing_data,\n    test_size=0.5,\n    random_state=RANDOM_SEED\n)","63c3dd72":"training_data.shape, testing_data.shape, validation_data.shape","b1608ec1":"def create_data_loader(data, tokenizer, max_len, batch_size):\n    \n    ds = GitHubCommitMessages(commit_message=data.combined.to_numpy(),\n    label=data.label.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len)\n    \n    return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4)\n\n\nBATCH_SIZE = 16\ntrain_data_loader = create_data_loader(training_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\ntesting_data_loader = create_data_loader(testing_data, tokenizer, MAX_LENGTH, BATCH_SIZE)\nval_data_loader = create_data_loader(validation_data, tokenizer, MAX_LENGTH, BATCH_SIZE)","99eafb3e":"df = next(iter(train_data_loader))\ndf.keys()","710862e3":"df['input_ids'].squeeze().shape, df['attention_mask'].squeeze().shape, df['label'].shape","c6f49ec7":"print('commit_message  : ', df['commit_message'][0])\nprint('input_ids : ', df['input_ids'].squeeze()[0])\nprint('attention_mask : ', df['attention_mask'].squeeze()[0])\nprint('label : ', df['label'][0])","678f1bc3":"bert_model = BertModel.from_pretrained(BERT_UNCASED)","abc69548":"last_hidden_state, pooled_output = bert_model(\n  input_ids=encodings['input_ids'],\n  attention_mask=encodings['attention_mask']\n)","6dd2228c":"last_hidden_state.shape, pooled_output.shape","da259af5":"class BugPredictor(nn.Module):\n    \n    def __init__(self, n_classes):\n        super(BugPredictor, self).__init__()\n        self.bert_model = BertModel.from_pretrained(BERT_UNCASED)\n        self.dropout = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert_model.config.hidden_size, n_classes)\n        \n    def forward(self, input_ids, attention_mask):\n        _, pooled_output = self.bert_model(\n        input_ids=input_ids,\n        attention_mask = attention_mask\n        )\n        output = self.dropout(pooled_output)\n        return self.out(output)\n        ","d4577827":"\"\"\"\nlabel 0: Bug\nlabel 1: Feature\nlabel 2: Question\n\"\"\"\nclass_names = [0, 1, 2]\nbug_predictor_model = BugPredictor(len(class_names))\nbug_predictor_model = bug_predictor_model.to(device)","83c86f30":"EPOCHS = 5\n\noptimizer = AdamW(bug_predictor_model.parameters(), lr=2e-5, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss().to(device)","1260a0aa":"def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    \n    losses = []\n    correct_predictions = 0\n    \n    for d in data_loader:\n        input_ids = d['input_ids'].squeeze().to(device)\n        attention_mask = d['attention_mask'].squeeze().to(device)\n        targets = d['label'].to(device)\n\n        outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, targets)\n        \n        correct_predictions += torch.sum(preds==targets)\n        losses.append(loss.item())\n        \n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","3655cb34":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    \n    losses = []\n    correct_predictions = 0\n    \n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d['input_ids'].squeeze().to(device)\n            attention_mask = d['attention_mask'].squeeze().to(device)\n            targets = d['label'].to(device)\n\n            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, targets)\n\n            correct_predictions += torch.sum(preds==targets)\n            losses.append(loss.item())\n    \n    return correct_predictions.double()\/n_examples, np.mean(losses)","bd2cdc36":"%%time\nfrom collections import defaultdict\n\nhistory = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}\/{}'.format(epoch+1,EPOCHS))\n    print('-' * 10)\n    \n    train_acc, train_loss = train_model(bug_predictor_model, train_data_loader, loss_fn, optimizer, device, scheduler, len(training_data))\n    \n    print('Train loss : {} accuracy : {}'.format(train_loss, train_acc))\n    \n    val_acc, val_loss = eval_model(bug_predictor_model, val_data_loader, loss_fn, device, len(validation_data))\n    \n    print('Validation loss : {} accuracy : {}'.format(val_loss, val_acc))\n    \n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    \n    if val_acc > best_accuracy:\n        print('Saving the best model ...')\n        torch.save(bug_predictor_model.state_dict(), 'best_model.bin')\n        best_accuracy = val_acc\n    ","6f3535b4":"sample_bug_message = \"Script stopped adding video's. A recent change in the youtube layout broke the script. Probably caused by element names being altered.\"","4bf62564":"class_names = ['bug', 'feature', 'question']","962ac8a5":"def predict_git_category(sample_message, model):\n    encoded_message = tokenizer.encode_plus(sample_bug_message, max_length=MAX_LENGTH, add_special_tokens=True, return_token_type_ids=False, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n    input_ids = encoded_message['input_ids'].to(device)\n    attention_mask = encoded_message['attention_mask'].to(device)\n    \n    output = model(input_ids=input_ids, attention_mask=attention_mask)\n    _, prediction_idx = torch.max(output, dim=1)\n        \n    return class_names[prediction_idx]\n","5e0ebe3a":"print('Sample bug message : ', sample_bug_message)\nprint('Predicted GitHub Category : ', predict_git_category(sample_bug_message, bug_predictor_model))","1ac1ad5e":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Creating training, testing and validation data<\/h1>","31f28b63":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Class for our GitHub messages<\/h1>","774f3b06":"1. https:\/\/curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/\n2. https:\/\/huggingface.co\/transformers\/\n3. http:\/\/jalammar.github.io","0b19b871":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Future Improvements<\/h1>","e763e6d6":"![image.png](attachment:image.png)","4e8e3e7f":"In this notebook, I have implemented a BERT-based model that can classify a GitHub title and body into three categories: Bugs, Features and Questions. Here, I have employed BERT-base uncased model for performing the classification task. The notebook has been organized as follows:\n\n1. Exploratory Data Analysis\n     * Most common terms employed when referring to a GitHub *Bug*\n     * Most common terms employed when referring to a GitHub *feature*\n     * Most common terms employed when referring to a GitHub *questions*\n\n2. Model Architecture\n3. Preprocessing GitHub messages\n4. Importing Transformer, PyTorch library and constructing the BERT model.\n5. Class definition of GitHub messages\n6. Creating training, testing and validation data\n7. Creating data loaders\n8. Class for BERT-based GitHub Bug Predictor model\n9. Training the model\n10. Results and evaluation\n11. Future improvements\n12. References","120f11c2":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Model Architecture<\/h1>","cf3ba134":"<h1 style=\"background-color:#7AE7C7;text-align:center\">References<\/h1>","281d7e5c":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Results and Evaluation<\/h1>","5a1260cf":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Training the model<\/h1>","0642ca22":"<h1 style=\"background-color:#7AE7C7;text-align:center\">GitHub Bug Prediction Model using BERT<\/h1>","31c24bdf":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Class for our BERT-based GitHub Bug Predictor model<\/h1>","ff3fb2d4":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Importing Transformer, PyTorch Libraries and constructing BERT model<\/h1>","30282c64":"1. Due to low computational resources,  was not able to leverage the entire dataset for training the BERT model. \n2. I still need to employ BERT large model to test its accuracy on GitHub message classification use-case.\n3. The model was trained on only 5 epochs, which I feel can be increased.\n4. The dataset provided has rich textual information. This can be leveraged for other NLP tasks such as natural language generation. ","ccfd69fa":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Exploratory Data Analysis<\/h1>","0cabfa05":"<h4>So our BERT-based GitHub bug predictor model achieved training accuracy of 96.5% and validation accuracy of 85%. Let us see some sample Git messages and categorize them into bugs, features and questions based on our model!<h4>","4602938b":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Preprocessing GitHub Messages<\/h1>","bcb239b5":"<h1 style=\"background-color:#7AE7C7;text-align:center\">Creating data loaders<\/h1>"}}