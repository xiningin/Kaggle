{"cell_type":{"4812d07f":"code","89209cfd":"code","55711817":"code","c17c6586":"code","42d8465f":"code","34a7a85c":"code","879d3764":"code","170cd64d":"code","f5d9176c":"code","4106c4a3":"code","077f7c7c":"code","9ee5451b":"code","1b27d01f":"code","469a9318":"code","aea68b56":"code","51236b41":"code","bb1a13a2":"code","0b6b52a1":"code","a1b27399":"code","5432e4bf":"code","1214a7e5":"code","ff57b282":"code","d7149351":"code","f68bf407":"code","fe17e5b2":"code","5aec2dcc":"code","2a1c8a5b":"code","008d8a17":"code","e5c45d30":"code","23068058":"code","600805ca":"code","cb96caa6":"code","8e7437e9":"code","597c95a4":"code","87a64b88":"code","61ec121b":"code","bbdb5146":"code","63427ee5":"code","c0372d86":"code","263d8144":"code","a851a7ab":"code","18854cdf":"markdown","178818da":"markdown","c2f2d5e4":"markdown","dc545fbf":"markdown","a03a3e87":"markdown","e6d5ddf3":"markdown","a922ab29":"markdown","64026900":"markdown","ec86ddd0":"markdown","0bfdb941":"markdown","9ba03c34":"markdown","a9cfbd88":"markdown","a7a05d0c":"markdown","76d11618":"markdown","23bcd5cd":"markdown","a55fd9c0":"markdown","7d499c76":"markdown","13880aa5":"markdown","fadfdfd7":"markdown","6ea38755":"markdown","560d1197":"markdown","6c159e4e":"markdown","6725fc22":"markdown","cfe927c5":"markdown","d0d0e53f":"markdown","1b321c63":"markdown","85577a14":"markdown","8d8d63b8":"markdown","574af4be":"markdown","46fc785f":"markdown","36bb0903":"markdown","3f25bb90":"markdown","f11c7e3d":"markdown","3a29a608":"markdown","3fb8693d":"markdown","50275047":"markdown","015ddc34":"markdown"},"source":{"4812d07f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89209cfd":"#importing libraries\nimport pandas as pd\nfrom datetime import datetime\nfrom dateutil.tz import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n","55711817":"#Importing the dataset\ndata = pd.read_csv('\/kaggle\/input\/SolarEnergy\/SolarPrediction.csv')","c17c6586":"#Checking which data is available in the dataset and which data-type is associated to each column of the dataset\ndata.info()","42d8465f":"#Checking if there are missing values\ndata.isnull().sum()","34a7a85c":"#Converting UNIX time to datetime object\ndata['Date']= pd.to_datetime(data['UNIXTime'],unit='s')\n\n#Setting the right timezone to the datetime object\ndata['Date'] = data['Date'].dt.tz_localize('UTC').dt.tz_convert('HST')","879d3764":"#Extracting date from Data column\ndata['Data'] = pd.to_datetime(data['Data']).dt.date\n\n#Converting Sunrise and Sunset columns into datetime.time objects\ndata['TimeSunRise'] = pd.to_datetime(data['TimeSunRise']).dt.time\ndata['TimeSunSet'] = pd.to_datetime(data['TimeSunSet']).dt.time\n\n#Creating new sunset\/sunrise columns featuring also the right date\ndata['sunrise_time'] = data.apply(lambda row: pd.datetime.combine(row['Data'], row['TimeSunRise']), axis = 1)\ndata['sunset_time'] = data.apply(lambda row: pd.datetime.combine(row['Data'], row['TimeSunSet']), axis = 1)\n\n#Adding approriate timezone\ndata['sunrise_time'] = data['sunrise_time'].dt.tz_localize('HST')\ndata['sunset_time'] = data['sunset_time'].dt.tz_localize('HST')","170cd64d":"#Setting 'Date' as index\ndata.set_index('Date', inplace = True)\n\n#Sorting by the index\ndata.sort_index()\n\ndata.drop(columns = ['Data', 'Time', 'TimeSunRise',\n                    'TimeSunSet'], inplace = True)","f5d9176c":"#Inspecting the first rows of the dataset\ndata.head()","4106c4a3":"data_one_day = data.loc['2016-09-29':'2016-09-30',:]\n\nplt.figure(figsize = (12,3))\nplt.plot(data_one_day.Radiation, 'o', markerfacecolor = 'w')\n\n#Plotting vertical line at sunrise\nplt.axvline(data_one_day.sunrise_time.iloc[0], label = 'Sunrise time', color = 'blue')\n\n#Plotting vertical line at sunset\nplt.axvline(data_one_day.sunset_time.iloc[0], label = 'Sunset time', color = 'red') \n\n#Adjusting timezone of x-axis\nplt.gca().xaxis_date('HST')\n\nplt.legend()\nplt.show()","077f7c7c":"#Analysing the ranges of the various features of the datset\ndata.describe()","9ee5451b":"fig, ax = plt.subplots(nrows =2, ncols = 6, figsize = (25, 10))\n\nsns.distplot(data.Radiation, ax = ax[0,0])\nax[0,0].set_xlabel('Solar radiation [W\/m^2]', fontsize = 14)\n\nsns.distplot(data.Temperature, ax = ax[0,1])\nax[0,1].set_xlabel('Temperature [F]', fontsize = 14)\n\nsns.distplot(data.Pressure, ax = ax[0,2])\nax[0,2].set_xlabel('Pressure [Hg]', fontsize = 14)\n\nsns.distplot(data.Humidity, ax = ax[0,3])\nax[0,3].set_xlabel('Humidity [%]', fontsize = 14)\n\nsns.distplot(data.Speed, ax = ax[0,4])\nax[0,4].set_xlabel('Wind speed [miles\/h]', fontsize = 14)\n\nsns.distplot(data['WindDirection(Degrees)'], ax = ax[0,5])\nax[0,5].set_xlabel('Wind direction [Degrees]', fontsize = 14)\n\n\nsns.boxplot(data.Radiation, ax = ax[1,0])\nax[1,0].set_xlabel('Solar radiation [W\/m^2]', fontsize = 14)\n\nsns.boxplot(data.Temperature, ax = ax[1,1])\nax[1,1].set_xlabel('Temperature [F]', fontsize = 14)\n\nsns.boxplot(data.Pressure, ax = ax[1,2])\nax[1,2].set_xlabel('Pressure [Hg]', fontsize = 14)\n\nsns.boxplot(data.Humidity, ax = ax[1,3])\nax[1,3].set_xlabel('Humidity [%]', fontsize = 14)\n\nsns.boxplot(data.Speed, ax = ax[1,4])\nax[1,4].set_xlabel('Wind speed [miles\/h]', fontsize = 14)\n\nsns.boxplot(data['WindDirection(Degrees)'], ax = ax[1,5])\nax[1,5].set_xlabel('Wind direction [Degrees]', fontsize = 14)\n\nfig.suptitle('Distribution and box plot of the various features', fontsize = 22)\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\n\nplt.show()","1b27d01f":"#Creation of the median dataset\ndata_median = data.resample('H').median().dropna()","469a9318":"#Extraction of the data for a five-day period\ndata_5 = data.loc['2016-10-03':'2016-10-08',:]\ndata_5_median = data_median.loc['2016-10-03':'2016-10-08',:]\n\n\nfig, ax = plt.subplots(nrows =6, ncols = 1, figsize = (23,25))\n\nax[0].plot(data_5.Radiation,'o', markerfacecolor='w')\nax[0].plot(data_5_median.Radiation, linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[0].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\nax[0].legend(fontsize = 14)\n\nax[1].plot(data_5.Temperature,'o', markerfacecolor='w')\nax[1].plot(data_5_median.Temperature, linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[1].set_ylabel('Temperature [F]', fontsize = 14)\nax[1].legend(fontsize = 14)\n\nax[2].plot(data_5.Pressure,'o', markerfacecolor='w')\nax[2].plot(data_5_median.Pressure, linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[2].set_ylabel('Pressure [Hg]', fontsize = 14)\nax[2].legend(fontsize = 14)\n\nax[3].plot(data_5.Humidity,'o', markerfacecolor='w')\nax[3].plot(data_5_median.Humidity, linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[3].set_ylabel('Humidity [%]', fontsize = 14)\nax[3].legend(fontsize = 14)\n\nax[4].plot(data_5.Speed,'o', markerfacecolor='w')\nax[4].plot(data_5_median.Speed, linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[4].set_ylabel('Wind Speed [miles\/h]', fontsize = 14)\nax[4].legend(fontsize = 14)\n\nax[5].plot(data_5['WindDirection(Degrees)'],'o', markerfacecolor='w')\nax[5].plot(data_5_median['WindDirection(Degrees)'], linewidth = 1.5, color = 'red', label = 'Hourly median')\nax[5].set_ylabel('Wind direction [degrees]', fontsize = 14)\nax[5].legend(fontsize = 14)\n\nfig.suptitle('Trend of the various parameters over a five-day period', fontsize = 22)\nfig.tight_layout(rect=[0, 0.03, 1, 0.97])\n\nplt.show()","aea68b56":"#Converting sunrise and sunset times into timestamp\ndata['sunrise_timestamp'] = data.apply(lambda row: datetime.timestamp(row['sunrise_time']), axis = 1)\ndata['sunset_timestamp'] = data.apply(lambda row: datetime.timestamp(row['sunset_time']), axis = 1)\n\n#Creating a column containing the number of daily light hours\ndata['Hours_of_light'] = (data['sunset_timestamp'] - data['sunrise_timestamp'])\/60\/60\n\n#Creating column describing current time relative to sunrise\/sunset\ndata['Rel_time'] = (data['UNIXTime']- data['sunrise_timestamp'])\/(data['sunset_timestamp']-data['sunrise_timestamp'])","51236b41":"#Removing non-necessary columns\ndata.drop(columns = ['UNIXTime','sunrise_timestamp', 'sunset_timestamp', \n                     'sunset_time', 'sunrise_time'], inplace = True)","bb1a13a2":"#Plotting a heatmap of the various features in the dataset\nfig, ax = plt.subplots(figsize = (6,6))\nsns.heatmap(data.corr(), annot = True, cmap = 'YlGnBu')\nfig.suptitle('Correlation matrix', fontsize = 16)\nplt.show()","0b6b52a1":"fig, ax = plt.subplots(nrows =2, ncols = 4, figsize = (23,8))\n\nax[0,0].plot(data.Temperature, data.Radiation,'o', markerfacecolor='w')\nax[0,0].set_xlabel('Temperature [F]', fontsize = 14)\nax[0,0].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[0,1].plot(data.Pressure, data.Radiation,'o', markerfacecolor='w')\nax[0,1].set_xlabel('Pressure [Hg]', fontsize = 14)\nax[0,1].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[0,2].plot(data.Humidity, data.Radiation,'o', markerfacecolor='w')\nax[0,2].set_xlabel('Humidity [%]', fontsize = 14)\nax[0,2].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[0,3].plot(data.Hours_of_light, data.Radiation,'o', markerfacecolor='w')\nax[0,3].set_xlabel('Hours of light [h]', fontsize = 14)\nax[0,3].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[1,0].plot(data.Rel_time, data.Radiation,'o', markerfacecolor='w')\nax[1,0].set_xlabel('Rel_time', fontsize = 14)\nax[1,0].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[1,1].plot(data.Speed, data.Radiation,'o', markerfacecolor='w')\nax[1,1].set_xlabel('Wind speed [miles\/h]', fontsize = 14)\nax[1,1].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nax[1,2].plot(data['WindDirection(Degrees)'], data.Radiation,'o', markerfacecolor='w')\nax[1,2].set_xlabel('Wind direction [degrees]', fontsize = 14)\nax[1,2].set_ylabel('Radiation [W\/m^2]', fontsize = 14)\n\nfig.delaxes(ax[1,3])\n\nfig.suptitle('Scatter plots of the solar radiation as a function of the various features', fontsize = 22)\nfig.tight_layout()\nfig.subplots_adjust(top=0.88)\n\nplt.show()","a1b27399":"#Renaming dataset\ndf = data\n\n#Splitting dataset into labels and features\nX = df.drop(columns = 'Radiation')\ny = df.Radiation","5432e4bf":"#Splitting data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   test_size = 0.3,\n                                                   random_state = 42)","1214a7e5":"#Initiating linear regression model\nlr = LinearRegression()\n\n#Training the linear regression model\nlr.fit(X_train, y_train)\n\n#Carrying out prediction with the linear model\nlr_predict_train = lr.predict(X_train)\nlr_predict_test = lr.predict(X_test)","ff57b282":"#Checking the performance of the linear model\n\n#Squared error\nprint('Linear model, R^2 training set:{:.2f}'.format(r2_score(y_train, lr_predict_train)))\nprint('Linear model, R^2 test set:{:.2f}'.format(r2_score(y_test,lr_predict_test)))\n\n#Mean squared error (MSE)\nprint('Linear model, MSE training set:{:.2f}'.format(MSE(y_train, lr_predict_train)))\nprint('Linear model, MSE test set:{:.2f}'.format(MSE(y_test,lr_predict_test)))","d7149351":"#Carrying out predictions with linear model for the 5-day period\nX_five = X.loc['2016-10-03':'2016-10-08',:]\ny_five_lr = lr.predict(X_five)\n\n\nfig, ax = plt.subplots(figsize = (23,5))\n\nax.plot(data_5.Radiation,'o', markerfacecolor='w')\nax.plot(data_5.index, y_five_lr, linewidth = 1.5, color = 'red', label = 'Linear model prediction')\nax.set_ylabel('Radiation [W\/m^2]', fontsize = 14)\nax.legend(fontsize = 14)\n\nplt.show()","f68bf407":"#Initiating Random Forest regressor\nrf_model = RandomForestRegressor(random_state = 42)\n\n#Define the grid of hyperparameters\nparams_rf = {\n    'n_estimators': [500, 600, 700],\n    'max_depth': [5, 6, 7],\n    'min_samples_leaf': [0.075, 0.05, 0.025],\n    'max_features': ['log2', 'sqrt']   \n}\n\n\n#Initiate Grid search\ngrid_rf = GridSearchCV(estimator = rf_model,\n                       param_grid = params_rf,\n                       cv = 3,\n                       scoring = 'neg_mean_squared_error',\n                       verbose = 1,\n                       n_jobs = -1)","fe17e5b2":"#Fitting the grid search\ngrid_rf.fit(X_train, y_train)","5aec2dcc":"#Extracting best hyperparameters\nrf_best_hyperparams = grid_rf.best_params_\nprint('Best hyperparameters for RF: \\n', rf_best_hyperparams)","2a1c8a5b":"#Extracting best rf model\nrf = grid_rf.best_estimator_","008d8a17":"#Checking if there is overfitting through the use of Cross validation\nrf_MSE_CV = -cross_val_score(rf, X_train, y_train,\n                            cv = 10, \n                            scoring = 'neg_mean_squared_error',\n                            n_jobs = -1)","e5c45d30":"#Computing Random Forest predictions in the traning and test sets\nrf_predict_train = rf.predict(X_train)\nrf_predict_test = rf.predict(X_test)\n","23068058":"#Computing the MSE in the traning set, test set, and cross-validation procedure\nprint('CV MSE for RF:{:.2f}'.format(rf_MSE_CV.mean()))\nprint('Train MSE for RF:{:.2f}'.format(MSE(y_train,rf_predict_train)))\nprint('Test MSE for RF:{:.2f}'.format(MSE(y_test,rf_predict_test)))","600805ca":"#Computing the R^2 in the traning set and test set for the Random Forest Regressor\nprint('Random Forest, R^2 score training set:{:.2f}'.format(r2_score(y_train, rf_predict_train)))\nprint('Random Forest, R^2 score test set:{:.2f}'.format(r2_score(y_test,rf_predict_test)))","cb96caa6":"#Initiating Gradient Boosting regressor\ngb_model = GradientBoostingRegressor(random_state = 42)\n\n#Define the grid of hyperparameters\nparams_gb = {\n    'n_estimators': [200, 300, 600],\n    'max_depth': [2, 3,5],\n    'min_samples_leaf': [0.125, 0.1, 0.075],\n    'max_features': ['log2', 'sqrt']   \n}\n\n\n#Initiate Grid search\ngrid_gb = GridSearchCV(estimator = gb_model,\n                       param_grid = params_gb,\n                       cv = 3,\n                       scoring = 'neg_mean_squared_error',\n                       verbose = 1,\n                       n_jobs = -1)","8e7437e9":"#Fitting the grid search\ngrid_gb.fit(X_train, y_train)","597c95a4":"#Extracting best hyperparameters\ngb_best_hyperparams = grid_gb.best_params_\nprint('Best hyperparameters for GB: \\n', gb_best_hyperparams)","87a64b88":"#Extracting best gb model\ngb = grid_gb.best_estimator_","61ec121b":"#Checking if there is overfitting through the use of Cross validation\ngb_MSE_CV = -cross_val_score(gb, X_train, y_train,\n                            cv = 10, \n                            scoring = 'neg_mean_squared_error',\n                            n_jobs = -1)","bbdb5146":"#Computing Gradient Boosting predictions on the train and test stes\ngb_predict_train = gb.predict(X_train)\ngb_predict_test = gb.predict(X_test)\n","63427ee5":"#GB CV MSE\nprint('CV MSE for GB:{:.2f}'.format(gb_MSE_CV.mean()))\nprint('Train MSE for GB:{:.2f}'.format(MSE(y_train,gb_predict_train)))\nprint('Test MSE for GB:{:.2f}'.format(MSE(y_test,gb_predict_test)))","c0372d86":"#Computing the R^2 in the traning set and test set for the Gradient Boosting Regressor\nprint('Gradient Boosting, R^2 score training set:{:.2f}'.format(r2_score(y_train, gb_predict_train)))\nprint('Gradient Boosting, R^2 score test set:{:.2f}'.format(r2_score(y_test,gb_predict_test)))","263d8144":"#Plotting feature importances for Random Forest and Gradient boosting\n\n#Creating a pd.Series of feature importances\nimportances_rf = pd.Series(rf.feature_importances_, index = X.columns)\nimportances_gb = pd.Series(gb.feature_importances_, index = X.columns)\n\n#Sorting importances\nsorted_importances_rf = importances_rf.sort_values()\nsorted_importances_gb = importances_gb.sort_values()\n\n#Plotting sorted importances\nfig, ax = plt.subplots(ncols = 2, figsize = (27,7))\nsorted_importances_rf.plot(kind = 'barh', color = 'lightblue', ax = ax[0])\nsorted_importances_gb.plot(kind = 'barh', color = 'lightblue', ax = ax[1])\nax[0].set_title('Random Forest Regressor')\nax[1].set_title('Gradient Boosting Regressor')\nfig.suptitle('Feature importances in the two ML models', fontsize = 24)\nplt.show()","a851a7ab":"#Computing predictions of the two ML models in the 5-day period\ny_five_rf = rf.predict(X_five)\ny_five_gb = gb.predict(X_five)\n\nfig, ax = plt.subplots(figsize = (23,10))\n\nax.plot(data_5.Radiation,'o', markerfacecolor='w')\nax.plot(data_5.index, y_five_rf, linewidth = 1.5, color = 'red', label = 'Random Forest prediction')\nax.plot(data_5.index, y_five_gb, linewidth = 1.5, color = 'orange', label = 'Gradient Boosting prediction')\n\nax.set_ylabel('Radiation [W\/m^2]', fontsize = 14)\nax.legend(fontsize = 14)\n\nplt.show()","18854cdf":"# Prediction of the solar radiation with tree-based models","178818da":"The preliminary investigations indicated that the solar radiation is related to the various features in a non-linear way (except for the ambient temperature). It is however interesting to check how the use of a linear model will perform in this setting. In addition, the performance of the linear model can be used as a benchmark when considering more complex ML models.\n\nIn order to train a linear ML model, the dateset is divided into features (X) and the variable to be predicted (y = Solar radiation) and then split into train and test sets. Splitting the data into train and test sets enables to check the accuracy of the ML model when predicting non-previously seen data.","c2f2d5e4":"A preliminary analysis of the dataset indicates that there are no missing values, and therefore there is no need to understand how to deal with potential missing records.\n\nIt is possible to notice, however, that some features are not represented by the right class: the various dates are included as \"objects\", and hence they should be converted to \"datetime\" objects to facilitate their handling during the study.","dc545fbf":"Looking at the distribution of the data it is possible to conclude that most features have a skewed distribution, except for the wind directions, which is characterized by three peaks.\n\nAs it was possible to assume, roughtly 50 % of values of the solar radiation are located in the range between 0 W\/^2 and 250 W\/m^2 (there is no or little solar radiation at night). With respect to the wind speed, it seems that the high wind speeds areextreme outliers in a distribution that has most of its values in the range between 0 miles\/h and 20 miles\/h.","a03a3e87":"The correlation matrix indicates a positive linear correlation between the ambient temperature and the solar radiation (coefficient = 0.73). No clear linear correlation appears for the other features, and the second highest correlation value is identified for the humidity (yet it is only of -0.23).\n\nAs a second step in the correlation analysis, it is possible to draw scatter plots showing the distribution of the values of the various features as a function of the value of the target parameter (solar radiation). This allows to identify potential non-linear trends present. ","e6d5ddf3":"# Prediction of the solar radiation with a linear model","a922ab29":"In this section, the dataset is analysed to identify whether there are missing values and whether all the data is identified by the correct data-type. After these evaluations are completed, more sophisticated analyses can be carried out on the dataset.","64026900":"The scatter plots suggest the following:\n* It is confirmed a linear correlation between solar radiation and ambient temperature;\n* It seems that the highest values of the solar radiation are taking place when the ambient pressure is the highest;\n* The Rel_time feature is correctly implemented: the solar radiation is > 0 between sunrise (Rel_time=0) and sunset (Rel_time=1);\n* It seems that the maximum present solar radiation decreases for high wind speeds.","ec86ddd0":"Given that Random Forest and Gradient Boosting are complex models, the chances of overfitting the training set are considerable. When a model is overfitting the training set, it does fit not only its trends, but also its noise. Therefore, an overfitted model will perform poorly on unseen data.\n\nA way to check whether the model overfits the training set is to carry out a cross-validation procedure. During the cross-validation procedure, the training set is split into k folds, and the model is trained k times using a different portion of data from the training set. Each trained model will have a different prediction accuracy. The average error of the models trained during the cross validation procedure is defined \"cross-validation\" error.\n\nTwo scenarios are possible:\n1. The cross-validation error is greater than the training set error -> in this case the model is overfitting the training set;\n2. The cross-validation error is similar to the training set error -> it is possible to assume that the model is not overfitting the training set and will perform similarly on unseen data.\n\n","0bfdb941":"# Prediction of solar radiation data given weather conditions","9ba03c34":"The objective of the preliminary data analysis is to get a sense of how the data looks like, and to confirm whether the data actually makes sense (i.e. it would be questionable to identify negative values for the Solar radiation).\n\nThe first step of the preliminary data analysis is therefore to check the ranges of the various features of the dataset, and to do a cross-check whether these ranges are reasonable.","a9cfbd88":"After carrying out a preliminary data analysis of the dataset, it is time to define which features will be used when building the ML regression model, and to carry out correlation analyses aimed at identifying if there are clear patterns (linear or non-linear) between the variable to be predicted (the solar radiation), and the features.\n\nAs a first step, it is reasonable to consider that all the information contained in the dataset are useful for the prediction of the target variable. Therefore, the following features will be considered:\n1. Temperature\n2. Pressure\n3. Humidity\n4. Wind speed\n5. Wind direction\n\nOn top of that, it is also important to include features giving an indication of the time and date, because the solar radiation changes according to the solar position in the sky, and to the duration of the solar day. For this reason, two new features are included in the dataset: the relative time of the day (Rel_time), and the duration of the solar day (Hours of light).\n\nThe relative time of the day is defined as follows:\n(current time - sunrise_time)\/(sunrise_time - sunset_time)\n\nIt assumes the following values:\n* < 0 before sunrise\n* = 0 at sunrise\n* '>' 0 but < 1 between sunrise and sunset\n* = 1 at sunset\n* '>' 1 after sunset\n\nThe duration of the solar day is instead computed by subtracting the sunrise time from the sunset time.\n\nAll the calculations are carried out by referring to the UNIX time, therefore the duration of the day is later divided by 3600 to have a value in hours.","a7a05d0c":"The visual representation indicates that the linear model is capable of predicting where the peak solar radiation is located, and to match (in most cases) the maximum value of the solar radiation. However, it seems that it is not capable of correctly predicting the periods where the solar radiation is zero.\n\nThis suggests that more complex model are required in order to have a more comprehensive understanding of the relationship between the solar radiation and the selected features.","76d11618":"The same procedure carried out for the Random Forest regressor is now carried out for the Gradient Boosting regressor.","23bcd5cd":"As a more complex class of models to predict the solar radiation, tree-based models are selected. This class of models is characterized by two main advantages:\n1. They are capable of capturing non-linear relationships between features and labels\n2. They do not require feature scaling (i.e. regularization)\n\nTwo models are here selected for prediction: Random Forest and Grandient Boosting. Both models are ensamble methods. This means that they are based on the combination of a set of simple models, which together lead to a more robust and accurate prediction model. \n\nGiven that this set of models is more complex than the simple linear regression, some parameters (hyper-parameters) have to be user-specified before the model can be trained. The proper selection of these parameter is of uttermost importance in order to attain a suitable model. This selection process is generally called \"hyper-parameter tuning\" and it is here carried out by means of a grid-search approach: a list of potential values for the various hyper-parameters is defined and then multiple models are trained as a way to identify the best performing set of hyper-parameters.\n\nThe first model to be considered is Random Forest.","a55fd9c0":"As a last step in the preliminary data analysis, it makes good sense to plot the data for limited range of time. In this case, a five-day period is selected.\n\nPlotting the data enables to have an understanding regarding the variability of data itself during the day, which could provide some interesting insights to be accounted for when proceeding with the building of a ML model to predict the solar radiation.\n\nAside from the data, also the hourly-median of the data is represented in the following plots. This allows for an easier identification of potential patterns. The median is selected over the mean, because it is less affected by the presence of potential outliers.","7d499c76":"Tree-based models allow for extracting the importance that the various features have in determining the regression model. Having a look at the importance of the various features, it is possible to understand whether the two models allocated the same importance to the the various parameters.","13880aa5":"The ranges here identified look reasonable. In particular:\n* The Solar radiation assumes only positive values, and has a maximum value of 1600 W\/m^2 (this is reasonable as the average solar radiation is estimated to be of around 1361 W\/m^2);\n* The temperature ranges from 30.4 F to 71 F (This corresponds in a range between -1 C to 21 C);\n* The pressure variates very little, and in any case has a value of around 1 bar;\n* The Humidity has values over 100 %, but only very slightly. This can be considered acceptable for the scope of this work;\n* Wind direction is correctly in the range from 0 to 360 degrees. Notice that the direction 0 degrees and 360 degrees are the same measurement, hence some data transformation could be required to correctly account for this phenomena;\n* Wind speed is always positive, and its maximum value (40.5 miles\/hour or 18 m\/s) is reasonable as it corresponds to a grade 8 of the Beaufort scale.","fadfdfd7":"This notebook presentes a data analysis on a four-month dataset collected at the HI-SEAS weather station (Hawaii). The sampling rate is of around 5 minutes and, the collected variables are the following:\n* Solar radiation [W\/m^2]\n* Temperature [F]\n* Atmospheric pressure [Hg]\n* Humidity [%]\n* Wind speed [miles\/h]\n* Wind direction [degrees]\n\nThe objective of the evaluations is to derive a ML model to forecast the available solar radiation as a function of the available features.","6ea38755":"# Data cleaning and preparation","560d1197":"As a first step, the UNIXTime is converted into a datetime object, and the right timezone is allocated to this feature (when converting UNIX time to datetime, the UTC timezone is assumed, and there is therefore the need to update this info, because the data is collected under the HST timezone).","6c159e4e":"A dataset containing weather data was analyzed and regression models were built with the objective of predicting the solar radiation given the weather data. The best performing model was a Gradient Boosting regressor (R^2 = 0.89) and the most significant features were found to be the ambient temperature and a parameter indicating the relative time in relation to the sunrise\/sunset times.\n\nSuggestions for improving the analysis\/predictions are the following:\n1. Carry out a more estensive hyper-parameter tuning procedure (a simple gride search of few parameters was here carried out);\n2. Consider to limit the evaluations to the only hours of the day when the solar radiation is present (this allows to build a model that does not have to account for the night hours);\n3. Include uncertainty bonds in the predicted solar radiation;\n4. Consider the use of ML models that account for the fact that the data is available as a time-series (i.e. SARIMAX models).","6725fc22":"If the data handling has been carried out correctly, then it would be reasonable to expect that the solar radiation, for any considered day, would be approximately zero before the sunrise time, and after the sunset time. It is possible to check this by means of a graphical inspection.","cfe927c5":"As a second step of the preliminary data analysis, it is reasonable to check for the distribution of the data, in order to understand how the various data is allocated between the lower and upper limits. This can be carried out by plotting either a distribution plot or a boxplot. \n\nBoth are plotted in this case, as they enable to have a more comprehensive understanding of the data.","d0d0e53f":"The training MSE and cross-validation MSE are fairly similar and, thus, it is possible to conclude that the model is not overfitting the data.","1b321c63":"The training MSE and cross-validation MSE are fairly similar and, thus, it is possible to conclude that the model is not overfitting the data.","85577a14":"Now that all the features have been defined and included in the dataset, it is time to identify if there are clear patterns between the features and the target parameter. One way to do so is to plot the correlation matrix, which displays whether there is a linear correlation between the various variables. \n\nA value close to 1 indicates a strongly linear positive correlation, while a value close to -1 indicates a strongly negative linear correlation. \n\nValues around zero indicate that there is no linear correlation, but do not exclude that other kind of correlations are present (i.e. exponential, more than linear, quadratic, etc..)","8d8d63b8":"The plot suggests that the various dates have been correctly manipulated. It is now possible to proceed with the preliminary data analysis of the dataset.","574af4be":"Now that the date columns have been correctly identified as datetime objects, it is suitable to set the newly created \"Date\" column as index, sort the data by the index (ascending order), and drop the non-required columns.","46fc785f":"The plots indicate that the RF and GB selected the same order of importance for the available features, and that the most important features are the ambient temperature and the Rel_time (the variable that describes in which phase of the day the considered time-point is).\n\nLastly, a graphical understanding of how the two model performe is possible by plotting the forecasted solar radiation over a five-day period.","36bb0903":"Looking at the plots it is possible to deduce the following:\n1. The data for the Temperature, humidity, and wind speed seems to assume only discrete values. This could be connected with the type of sensors used for the data campaing;\n2. The pressure data seems to follow some clear pattern in which high and low pressure values interchange each other;\n3. The wind speed data is extremely volatile. The high volatility could make this feature a less \"certain\" one when carrying out the regression analysis;\n4. As expected, solar radiation is constant at zero during the night, but high variability is experienced during the day-hours;\n5. The wind direction data is volatile, but clear trends can be identified. Sometimes the variation of the measurements between 0 degrees and 360 degrees create a sense of \"change\" of the wind direction, which in practice is not there.","3f25bb90":"# Preliminary Data Analysis","f11c7e3d":"As a second step, the SunRiseTime and SunSetTime columns are adjusted. These columns only contain information about the sunrise and sunset time, while it would be beneficial to have them containing both the time and the date.\n\nIn order to update this columns, it is possible to proceed as follows:\n1. Convert the \"SunRiseTime\" and \"SunSetTime\" columns into a datetime.time object\n2. Convert the \"Data\" column into a datetime.date object\n3. Combine the datetime.time and datetime.date objects through the function pd.datetime.combine()\n\nAfter doing so, similarly to what done previously, the correct timezone is allocated to the data.","3a29a608":"The graphical inspection, indicates that the Gradient boosting regressor leads to the most accurate predictions. This could be expected due to the previously shown performance indicators:\n* R^2 -> 0.9 for Gradient Boosting, and 0.77 for Random Forest\n* MSE -> 10,000 for Gradient boosting, and 23,000 for Random Forest\n\nLooking at the plots it seems that RF regressor is characterized by the following drawbacks:\n1. It is characterized by high variance in the prediction\n2. It seems to systematically overestimate the solar radiation after the sunset\n\nBoth aspects could be improved by refining the hyper-parameter tuning procedure (not all possible parameters were screened).\n","3fb8693d":"The results indicate a relatively poor fitting (R squared or around 0.6). In addition, it is possible to notice that the model have similar scores for the train and test sets, this indicates that the model does not overfit the training data, and that its performance on the test set is comparable to the one on the training set.\n\nAs a way to have a visual understanding of the performance of the model, it is possible to plot the predicted and real solar radiation data over a five day-period.","50275047":"# Conclusions and recommendations for further analyses","015ddc34":"# Feature engineering and correlation analysis"}}