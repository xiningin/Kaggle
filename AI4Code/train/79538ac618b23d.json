{"cell_type":{"961c2f5e":"code","0e7f2c78":"code","278149f0":"code","fb7ca3de":"code","d0d77051":"code","ec5d2d0a":"code","4ae5c5eb":"code","261c758f":"code","bc115303":"code","4f8f0a35":"code","35ebfb74":"code","65d1737c":"code","cfd01b79":"code","596f94d3":"code","c306a4c7":"code","c8f56aa3":"code","9f962468":"code","5c10e683":"code","7a9281a1":"code","5cca4cd4":"code","4fb42300":"code","a8982e0c":"code","898fb589":"code","65a06b43":"code","31c140d2":"code","ac7222e0":"code","a40679ce":"code","283a9635":"code","33a283a3":"code","64c79bfe":"code","be7670d8":"code","2bcad357":"code","ba673ded":"code","0ff34a02":"code","2c435abc":"code","c7aebf8f":"markdown","d183dcc8":"markdown","32cdfd23":"markdown","1858ca21":"markdown","017d1d27":"markdown","eca511b3":"markdown","9f809a42":"markdown","589ba14c":"markdown","b4126529":"markdown","bb15d34b":"markdown","05496266":"markdown","e80143ea":"markdown","e27d0c82":"markdown","f31b7f08":"markdown","221b16ef":"markdown","991412a3":"markdown","ca5e4bed":"markdown","6ac7c9ca":"markdown","1a2cc354":"markdown","6da3fb04":"markdown","beb2a23a":"markdown","16f08e1f":"markdown","2a6ce14d":"markdown","dc1ceee0":"markdown","e8696680":"markdown","28da33f1":"markdown","f0529fe6":"markdown"},"source":{"961c2f5e":"import numpy as np\nimport pandas as pd\nimport os\nfrom os import listdir\nimport cv2\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nfrom PIL import Image\nfrom glob import glob\nfrom skimage.io import imread\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torch.optim as optim\n\nimport time\nimport copy\nfrom tqdm import tqdm_notebook as tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nprint('Imports complete')","0e7f2c78":"# Model Parameters\nnum_epochs = 10\nbatch_size = 128\nnum_classes = 2\nlearning_rate = 0.002\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","278149f0":"base_dir = '..\/input\/histopathologic-cancer-detection\/'\nprint(os.listdir(base_dir))","fb7ca3de":"labels = pd.read_csv(base_dir + \"train_labels.csv\")\nlabels.head()","d0d77051":"labels.shape","ec5d2d0a":"labels.info()","4ae5c5eb":"train_path = base_dir + \"train\/\"\ntest_path = base_dir + \"test\/\"\ntrain_files = listdir(train_path)\ntest_files = listdir(test_path)","261c758f":"train_files[:5]","bc115303":"test_files[:5]","4f8f0a35":"# Number of images in train and test\nprint(\"Train size: \", len(train_files))\nprint(\"Test size: \", len(test_files))","35ebfb74":"print((len(train_files)\/(len(train_files)+len(test_files)))*100, (len(test_files)\/(len(train_files)+len(test_files)))*100)","65d1737c":"sub = pd.read_csv(base_dir + \"sample_submission.csv\")\nsub.head()","cfd01b79":"sub.shape","596f94d3":"sub.info()","c306a4c7":"plt.pie(labels.label.value_counts(), labels=['No Cancer', 'Cancer'], colors=['#90EE91', '#F47174'], autopct='%1.1f')\nplt.show()","c8f56aa3":"positive_images = np.random.choice(labels[labels.label==1].id, size=50, replace=False)\nnegative_images = np.random.choice(labels[labels.label==0].id, size=50, replace=False)","9f962468":"fig, ax = plt.subplots(5, 10, figsize=(20,10))\n\nfor n in range(5):\n    for m in range(10):\n        img_id = positive_images[m + n*10]\n        image = Image.open(train_path + img_id + \".tif\")\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)\n        ax[n,m].tick_params(labelbottom=False, labelleft=False)","5c10e683":"fig, ax = plt.subplots(5, 10, figsize=(20,10))\n\nfor n in range(5):\n    for m in range(10):\n        img_id = negative_images[m + n*10]\n        image = Image.open(train_path + img_id + \".tif\")\n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)\n        ax[n,m].tick_params(labelbottom=False, labelleft=False)","7a9281a1":"train, val = train_test_split(labels, stratify=labels.label, test_size=0.1)\nprint(len(train), len(val))","5cca4cd4":"fig, ax = plt.subplots(1, 2, figsize=(10,4))\n\nsns.countplot(train.label, palette=\"Blues\", ax=ax[0])\nax[0].set_title(\"Train dataset\")\nfor i, rows in enumerate(train['label'].value_counts().values):\n    ax[0].annotate(int(rows), xy=(i, rows), ha='center')\nsns.countplot(val.label, palette=\"Greens\", ax=ax[1])\nax[1].set_title(\"Validation dataset\")\nfor i, rows in enumerate(val['label'].value_counts().values):\n    ax[1].annotate(int(rows), xy=(i, rows), ha='center')","4fb42300":"class CancerDataset(Dataset):\n    \n    def __init__(self, df_data, data_dir = '.\/', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name + '.tif')\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","a8982e0c":"transform_train = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.RandomHorizontalFlip(), \n                                  transforms.RandomVerticalFlip(),\n                                  transforms.RandomRotation(20), \n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ntransform_val = transforms.Compose([transforms.ToPILImage(),\n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])\n\ntransform_test = transforms.Compose([transforms.ToPILImage(), \n                                  transforms.ToTensor(),\n                                  transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])])","898fb589":"train_dataset = CancerDataset(df_data=train, data_dir=train_path, transform=transform_train)\nval_dataset = CancerDataset(df_data=val, data_dir=train_path, transform=transform_val)\ntest_dataset = CancerDataset(df_data=sub, data_dir=test_path, transform=transform_test)","65a06b43":"train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","31c140d2":"print(len(train_dataloader), len(val_dataloader), len(test_dataloader))","ac7222e0":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        \n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(3, 32, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(32),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv2 = nn.Sequential(\n                        nn.Conv2d(32, 64, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(64),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv3 = nn.Sequential(\n                        nn.Conv2d(64, 128, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv4 = nn.Sequential(\n                        nn.Conv2d(128, 256, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv5 = nn.Sequential(\n                        nn.Conv2d(256, 512, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.fc=nn.Sequential(\n                nn.Linear(512*3*3, 256),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm1d(256),\n                nn.Dropout(0.4),\n                nn.Linear(256, num_classes))\n        \n    def forward(self,x):\n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=self.conv3(x)\n        x=self.conv4(x)\n        x=self.conv5(x)\n#        print(x.shape)\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","a40679ce":"model = CNN().to(device)\nprint(model)","283a9635":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)","33a283a3":"train_losses = []\nval_losses = []\ntrain_auc = []\nval_auc = []\ntrain_auc_epoch = []\nval_auc_epoch = []\nbest_acc = 0.0\nmin_loss = np.Inf\n\nsince = time.time()\n\nfor e in range(num_epochs):\n    \n    train_loss = 0.0\n    val_loss = 0.0\n    \n    # Train the model\n    model.train()\n    for i, (images, labels) in enumerate(tqdm(train_dataloader, total=int(len(train_dataloader)))):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # Loss and accuracy\n        train_loss += loss.item()\n        y_actual = labels.data.cpu().numpy()\n        y_pred = outputs[:,-1].detach().cpu().numpy()\n        train_auc.append(roc_auc_score(y_actual, y_pred))\n    \n    # Evaluate the model\n    model.eval()\n    for i, (images, labels) in enumerate(tqdm(val_dataloader, total=int(len(val_dataloader)))):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Loss and accuracy\n        val_loss += loss.item()\n        y_actual = labels.data.cpu().numpy()\n        y_pred = outputs[:,-1].detach().cpu().numpy()\n        val_auc.append(roc_auc_score(y_actual, y_pred))\n    \n    # Average losses and accuracies\n    train_loss = train_loss\/len(train_dataloader)\n    val_loss = val_loss\/len(val_dataloader)\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    training_auc = np.mean(train_auc)\n    validation_auc = np.mean(val_auc)\n    train_auc_epoch.append(training_auc)\n    val_auc_epoch.append(validation_auc)\n    \n    # Updating best validation accuracy\n    if best_acc < validation_auc:\n        best_acc = validation_auc\n        \n    # Saving best model\n    if min_loss >= val_loss:\n        torch.save(model.state_dict(), 'best_model.pt')\n        min_loss = val_loss\n    \n    print('EPOCH {}\/{}'.format(e+1, num_epochs))\n    print('-' * 10)\n    print(\"Train loss: {:.6f}, Train AUC: {:.4f}\".format(train_loss, training_auc))\n    print(\"Validation loss: {:.6f}, Validation AUC: {:.4f}\\n\".format(val_loss, validation_auc))\n\ntime_elapsed = time.time() - since\nprint('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best validation accuracy: {:4f}'.format(best_acc))","64c79bfe":"plt.figure(figsize=(20,5))\nplt.plot(train_losses, '-o', label=\"train\")\nplt.plot(val_losses, '-o', label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss change over epoch\")\nplt.legend()","be7670d8":"plt.figure(figsize=(20,5))\nplt.plot(train_auc_epoch, '-o', label=\"train\")\nplt.plot(val_auc_epoch, '-o', label=\"val\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy over epoch\")\nplt.legend()","2bcad357":"model.load_state_dict(torch.load('best_model.pt'))","ba673ded":"model.eval()\n\npredictions = []\n\nfor i, (images, labels) in enumerate(tqdm(test_dataloader, total=int(len(test_dataloader)))):\n    images = images.to(device)\n    labels = labels.to(device)\n    \n    outputs = model(images)\n    pred = outputs[:,1].detach().cpu().numpy()\n    \n    for j in pred:\n        predictions.append(j)","0ff34a02":"sub['label'] = predictions\nsub.to_csv('submission.csv', index=False)\nsub.info()","2c435abc":"test_images = np.random.choice(sub.id, size=50, replace=False)     \n\nfig, ax = plt.subplots(5, 10, figsize=(20,10))\n\nfor n in range(5):\n    for m in range(10):\n        img_id = test_images[m + n*10]\n        image = Image.open(test_path + img_id + \".tif\")\n        pred = sub.loc[sub['id'] == img_id, 'label'].values[0]\n        label = \"Cancer\" if(pred >= 0.5) else \"Healthy\"  \n        ax[n,m].imshow(image)\n        ax[n,m].grid(False)\n        ax[n,m].tick_params(labelbottom=False, labelleft=False)\n        ax[n,m].set_title(\"Label: \" + label)","c7aebf8f":"I have split the train data into train and validation sets in the ratio 9:1.\n\n**Plotting the positive and negative ratio in train and val sets**","d183dcc8":"### Loss and Optimizer\n\nThis task is a binary classification problem that has two classes, 1 for cancer positive images and 0 for cancer negative images. For loss function, I have used cross entropy loss.\nI have used adam for optimizer.","32cdfd23":"**Accuracy trend**","1858ca21":"### Visualizing healthy and cancer patches","017d1d27":"### Data Augmentation\n\nNow to increase the data size, I have applied transformation like flipping and rotation to the train dataset, and then converted the datasets into tensors.","eca511b3":"# Defining the Model <a class=\"anchor\" id=\"model\"><\/a>\n\nI am using a CNN as the model with 5 layers.","9f809a42":"Printing the training model.","589ba14c":"# Preparation & Understanding the data structure <a class=\"anchor\" id=\"prep\"><\/a>\n\n### Importing packages","b4126529":"**Healthy patches**","bb15d34b":"# Data Preprocessing <a class=\"anchor\" id=\"data\"><\/a>\n\n### Splitting the data into train and validation sets","05496266":"# Training the Model <a class=\"anchor\" id=\"train\"><\/a>\n\nBuilding the training loop for the model. It prints the loss and accuracy for training and validation after each epoch.\nFor accuracy, I have calculated the area under the ROC curve between the predicted probability and the observed target.\nThe losses and accuracies are also saved in an array for further evaluation of the model.","e80143ea":"### Plotting training history\n\n**Loss Convergence**","e27d0c82":"### Loading the best model","f31b7f08":"### Creating pytorch dataloader\n\n* The training data is shuffled after epochs so that the batches in the epochs are different every time and the model doesn't learn in a specific sequence.\n* The last batch is dropped as it might contain less images than the batch size.","221b16ef":"### Modifying the submission file\n\nNow I am using the predictions made by the model to create a submission file.","991412a3":"This file contains the ids of images for training and their labels for cancer. ","ca5e4bed":"### Loading and understanding the data structure","6ac7c9ca":"The directories train and test contain the actual images with 79.3% and 20.7% of the total images respectively.","1a2cc354":"**Cancer patches**","6da3fb04":"This file contains the ids of test images and all the labels are set to 0. We need to modify the labels in this file according to our predictions.","beb2a23a":"**Analysis**\n\nVisualising cancerous and healthy patches, it is hard to identify metastatic cancer for an untrained eye. One observation could be that the healthy patches have higher contrast than the cancerous patches. However, this observation doesn't seem to be applicable on all the images. It would be interesting to see what criterion pathologists use for identification of metastatic cancer!","16f08e1f":"# Making & Visualising Predictions <a class=\"anchor\" id=\"pred\"><\/a>\n\n### Predictions on test dataset\n\nI have used my best model to make predictions on the test dataset.","2a6ce14d":"### Configurations","dc1ceee0":"## Table of Contents\n\n1. [Preparation & Understanding the data structure](#prep)\n2. [Exploratory Data Analysis](#eda)\n3. [Data Preprocessing](#data)\n4. [Defining the Model](#model)\n5. [Training the Model](#train)\n6. [Making & Visualising Predictions](#pred)","e8696680":"### Custom Dataset\n\nI have created a dataset that loads an image patch, converts it to RGB, performs the augmentation if it's desired, and returns the image and its label.","28da33f1":"### Visualising predictions\n\nFirst I have written a function to convert the image from tensor and then displayed some of the test images along with their predicted result. For a probability less than 0.5, images are labelled 'Healthy', otherwise they are labelled 'Cancer'.","f0529fe6":"# Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"><\/a>\n\n### Visualizing the number of patches with cancer vs without cancer."}}