{"cell_type":{"5b58c947":"code","650f17a9":"code","9588914a":"code","2f177210":"code","605ca92f":"code","54671284":"code","c0eda930":"code","27e85a3b":"code","30241970":"code","d3a4ca2e":"code","6e2b0e12":"code","181563dd":"code","67cce873":"code","1d15b41f":"code","712845b0":"code","f0c862bf":"code","625f3f9e":"code","583ca249":"code","a893b73e":"code","ec622a4d":"markdown","a172e662":"markdown","f26c7def":"markdown","b561eba9":"markdown","08848e55":"markdown","7d66daa7":"markdown","e0f3df21":"markdown","7e8e36dd":"markdown"},"source":{"5b58c947":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","650f17a9":"## Most Important\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\nfrom tensorflow.keras import regularizers\n\n## less Important\nfrom functools import partial\nimport os\nfrom scipy import stats\nimport missingno as msno\nimport joblib\nimport tarfile\nimport shutil\nimport urllib\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout\n\n## Sklearn\nfrom sklearn import datasets\n## Preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n## Metrics\nfrom sklearn.metrics import accuracy_score\n\n## tensorflow & Keras\nimport tensorflow as tf    ## i will use tf for every thing and for keras using tf.keras","9588914a":"train_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/train.csv')\ntrain_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/train')\n\n## read these all training images paths as Series\ntrain_images_paths = pd.Series(sorted(list(train_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntrain_images_paths.head()","2f177210":"img_key_value = {}\nfor value in train_labels['label'].unique():\n    img_key_value[value] = train_labels[train_labels['label']==value].index[0]\n    \nimg_index = list(img_key_value.values())\nimg_label = list(img_key_value.keys())\n\nfig, ax = plt.subplots(4, 7, figsize=(12, 8))\n\ni = 0\nfor row in range(4):\n    for col in range(7):\n        plt.sca(ax[row, col])\n        plt.title(f'label = {img_label[i]}')\n        img = plt.imread(train_images_paths.iloc[img_index[i]])\n        plt.imshow(img)\n        plt.axis('off')\n        i+=1","605ca92f":"print('Number of Instances in train_set =>', len(train_images_paths))\nprint('Number of Instances in train_labels =>', len(train_labels))\n\nprint()\n\nimg = plt.imread(train_images_paths.iloc[img_index[0]])\nprint('shape of each Image is =>', img.shape)","54671284":"train_full_labels = train_labels['label'].values\ntrain_full_set = np.empty((13440, 32, 32, 3), dtype=np.float32)  #take only the first 3 channels\n\nfor idx, path in enumerate(train_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    train_full_set[idx] = img\n    \nprint('train_full_set.shape =>', train_full_set.shape)\nprint('train_full_labels.shape =>', train_full_labels.shape)","c0eda930":"# import cv2\n# R_90_c = np.array([cv2.rotate(i, cv2.ROTATE_90_CLOCKWISE)  for i in train_full_set])\n# R_90_cc= np.array([cv2.rotate(i, cv2.ROTATE_90_COUNTERCLOCKWISE)  for i in train_full_set])\n\n# import numpy\n# train_full_set = numpy.concatenate((\n#                 train_full_set.reshape(13440, 32, 32, 3),\n#                 R_90_c,\n#                 R_90_cc,\n#                ), \n#           axis=0).reshape(40320, 32, 32, 3)\n\n# # X_test = X_test.reshape(3360, 32, 32, 3)\n# # y_train = np.repeat(y_train, 8 , axis=0)\n\n# a = [train_full_labels for i in range(3)]\n# train_full_labels = numpy.concatenate(a)\n# len(train_full_labels)","27e85a3b":"X_train, X_valid, y_train, y_valid = train_test_split(train_full_set, train_full_labels, \n                                                      test_size=0.15, shuffle=True, random_state=42,stratify=train_full_labels)\n\nprint('X_train.shape =>', X_train.shape)\nprint('X_valid.shape =>', X_valid.shape)\nprint('y_train.shape =>', y_train.shape)\nprint('y_valid.shape =>', y_valid.shape)","30241970":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu',input_shape=(32, 32, 3)),\n    Dropout(0.3),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'),\n    Dropout(0.3),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n    tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', ),\n    Dropout(0.3),\n    tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n    tf.keras.layers.Flatten(),\n \n    # FC6 Fully Connected Layer\n    tf.keras.layers.Dense(units = 84, activation = 'relu'),\n    Dropout(0.3),\n    \n   \n    \n    #tf.keras.layers.GlobalAveragePooling2D(),\n    \n    tf.keras.layers.Dense(29, activation='softmax')\n \n])","d3a4ca2e":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nearly_stopp = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)","6e2b0e12":"history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), \n                    epochs=100, batch_size=32, callbacks=[early_stopp])","181563dd":"pd.DataFrame(history.history).plot(figsize=(10, 6));","67cce873":"loss_all_data, acc_all_data = model.evaluate(train_full_set, train_full_labels, verbose=0)\nprint('loss_all_data =>', loss_all_data)\nprint('acc_all_data =>', acc_all_data)","1d15b41f":"test_labels = pd.read_csv('..\/input\/arabic-hwr-ai-pro-intake1\/test.csv')\ntest_images = Path(r'..\/input\/arabic-hwr-ai-pro-intake1\/test')\n\n## read these all training images paths as Series\ntest_images_paths = pd.Series(sorted(list(test_images.glob(r'*.png'))), name='Filepath').astype(str)\n\ntest_images_paths.head()","712845b0":"print('Number of Instances in test_set is', len(test_images_paths))","f0c862bf":"test_full_set = np.empty((3360, 32, 32, 3), dtype=np.float32)  #take only the first 3 channels\n\nfor idx, path in enumerate(test_images_paths):\n    img = plt.imread(path)\n    img = img[:,:,:3]\n    test_full_set[idx] = img\n    \nprint('test_full_set.shape =>', test_full_set.shape)","625f3f9e":"predictions=model.predict(test_full_set)\npred = [np.argmax(i) for i in predictions]\ntest_labels['label']=pred","583ca249":"test_labels","a893b73e":"test_labels[['id', 'label']].to_csv('\/kaggle\/working\/submission_new6.csv', index=False)","ec622a4d":"## Data Preprocessing","a172e662":"## Model Training","f26c7def":"`Only for training here`","b561eba9":"## Explore the Data","08848e55":"## Split the Data","7d66daa7":"## Done :D","e0f3df21":"## Loading the Data and Look at the Big Picture","7e8e36dd":"## Evaluation on Testing DataSet"}}