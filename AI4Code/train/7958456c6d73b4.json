{"cell_type":{"0a423e65":"code","7d846583":"code","c3b56af9":"code","1c521bfa":"code","5e5de6ca":"code","2b696296":"code","e445333e":"code","a23535b4":"code","476b6f79":"code","a984dc5f":"code","72dfa70d":"code","a7a7df9f":"code","c8a1c6c4":"code","d6896c22":"code","6e579d95":"code","95570e2d":"code","fd8d5810":"code","1e9b789d":"code","e74e64e6":"code","a6dce97e":"code","45e94ed9":"code","89dde951":"code","43e536e4":"code","6bbbf662":"code","7281b802":"code","1b9241d7":"code","3768d7a8":"code","0beb43d5":"code","52762ea2":"code","b137ebb4":"code","d26e1276":"code","2e26d855":"code","9ba4138e":"code","f2743cbf":"code","c6105df4":"code","bc3fdc4d":"code","b919368a":"code","00ddf732":"code","92a7ba49":"code","6def8d8a":"markdown","a3f5c03b":"markdown","00a768e5":"markdown","16b961c7":"markdown","dc3c2df4":"markdown","01e7d3ff":"markdown","b2369b2c":"markdown","4cfd037c":"markdown","5166ba77":"markdown","8fc66111":"markdown","948cdcb3":"markdown","491b3019":"markdown","1815c246":"markdown","1c968fd1":"markdown","5d0dea96":"markdown","9b9994c7":"markdown","a6151091":"markdown","a87803cc":"markdown","d2169982":"markdown","748fecd8":"markdown","627cfbba":"markdown","ebefb518":"markdown","100fea22":"markdown","f986b0e4":"markdown","4baef6d1":"markdown","3cde4213":"markdown"},"source":{"0a423e65":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt, rcParams, style\nstyle.use('seaborn-darkgrid')\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom plotly import express as px, graph_objects as go\n\nfrom statsmodels.tsa.deterministic import DeterministicProcess, CalendarFourier\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, Normalizer, MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, BaggingRegressor\n\nimport gc\ngc.enable()\nfrom warnings import filterwarnings, simplefilter\nfilterwarnings('ignore')\nsimplefilter('ignore')","7d846583":"rcParams['figure.figsize'] = (12, 9) # Konfigurasi jendela figure","c3b56af9":"train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv',\n                    parse_dates = ['date'], infer_datetime_format = True,\n                    dtype = {'store_nbr' : 'category',\n                             'family' : 'category'},\n                    usecols = ['date', 'store_nbr', 'family', 'sales'])\ntrain['date'] = train.date.dt.to_period('D')\ntrain = train.set_index(['date', 'store_nbr', 'family']).sort_index()\ntrain","1c521bfa":"test = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv',\n                   parse_dates = ['date'], infer_datetime_format = True)\ntest['date'] = test.date.dt.to_period('D')\ntest = test.set_index(['date', 'store_nbr', 'family']).sort_values('id')\ntest","5e5de6ca":"calendar = pd.DataFrame(index = pd.date_range('2013-01-01', '2017-08-31')).to_period('D')\noil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\noil['avg_oil'] = oil['dcoilwtico'].rolling(7).mean()\ncalendar = calendar.join(oil.avg_oil)\ncalendar['avg_oil'].fillna(method = 'ffill', inplace = True)\ncalendar.dropna(inplace = True)","2b696296":"# Plotting oil price\n_ = sns.lineplot(data = oil.dcoilwtico.to_timestamp())","e445333e":"_ = plot_pacf(calendar.avg_oil, lags = 12) # Lagplot oil price (Feature Engineering)","a23535b4":"n_lags = 3\nfor l in range(1, n_lags + 1) :\n    calendar[f'oil_lags{l}'] = calendar.avg_oil.shift(l)\ncalendar.dropna(inplace = True)\ncalendar","476b6f79":"lag = 'oil_lags1'\nplt.figure()\nsns.regplot(x = calendar[lag], y = calendar.avg_oil)\nplt.title(f'corr {calendar.avg_oil.corr(calendar[lag])}')\nplt.show()","a984dc5f":"hol = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv',\n                  parse_dates = ['date'], infer_datetime_format = True,\n                  index_col = 'date').to_period('D')\nhol = hol[hol.locale == 'National'] # I'm only taking National holiday so there's no false positive.\nhol = hol.groupby(hol.index).first() # Removing duplicated holiday at the same date\nhol","72dfa70d":"calendar = calendar.join(hol) # Joining calendar with holiday dataset\ncalendar['dofw'] = calendar.index.dayofweek # Weekly day\ncalendar['wd'] = 1\ncalendar.loc[calendar.dofw > 4, 'wd'] = 0 # If it's saturday or sunday then it's not Weekday\ncalendar.loc[calendar.type == 'Work Day', 'wd'] = 1 # If it's Work Day event then it's a workday\ncalendar.loc[calendar.type == 'Transfer', 'wd'] = 0 # If it's Transfer event then it's not a work day\ncalendar.loc[calendar.type == 'Bridge', 'wd'] = 0 # If it's Bridge event then it's not a work day\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == False), 'wd'] = 0 # If it's holiday and the holiday is not transferred then it's holiday\ncalendar.loc[(calendar.type == 'Holiday') & (calendar.transferred == True), 'wd'] = 1 # If it's holiday and transferred then it's not holiday\ncalendar = pd.get_dummies(calendar, columns = ['dofw'], drop_first = True) # One-hot encoding (Make sure to drop one of the columns by 'drop_first = True')\ncalendar = pd.get_dummies(calendar, columns = ['type']) # One-hot encoding for type holiday (No need to drop one of the columns because there's a \"No holiday\" already)\ncalendar.drop(['locale', 'locale_name', 'description', 'transferred'], axis = 1, inplace = True) # Unused columns\ncalendar","a7a7df9f":"#calendar['wd_lag1'] = calendar.wd.shift(1)\n#calendar['wd_fore1'] = calendar.wd.shift(-1).fillna(0)\n#calendar.dropna(inplace = True)\n#calendar","c8a1c6c4":"y = train.unstack(['store_nbr', 'family']).loc['2016-06':'2017']\nfamily = {c[2] for c in train.index}\nfor f in family :\n    ax = y.loc(axis = 1)['sales', :, f].plot(legend = None)\n    ax.set_title(f)","d6896c22":"sdate = '2017-04-30' # Start and end of training date\nedate = '2017-08-15'","6e579d95":"school_season = [] # Feature for school fluctuations\nfor i, r in calendar.iterrows() :\n    if i.month in [4, 5, 8, 9] :\n        school_season.append(1)\n    else :\n        school_season.append(0)\ncalendar['school_season'] = school_season\ncalendar","95570e2d":"y = train.unstack(['store_nbr', 'family']).loc[sdate:edate]\nfourier = CalendarFourier(freq = 'W', order = 4)\ndp = DeterministicProcess(index = y.index,\n                          order = 1,\n                          seasonal = False,\n                          constant = False,\n                          additional_terms = [fourier],\n                          drop = True)\nx = dp.in_sample()\nx = x.join(calendar)\nx","fd8d5810":"print(y.isna().sum().sum())\ndisplay(y)","1e9b789d":"xtest = dp.out_of_sample(steps = 16) # 16 because we are predicting next 16 days\nxtest = xtest.join(calendar)\nxtest","e74e64e6":"def make_lags(x, lags = 1) : #Fungsi untuk membuat fitur lags\n    lags = lags\n    x_ = x.copy()\n    for i in range(lags) :\n        lag = x_.shift(i + 1)\n        x = pd.concat([x, lag], axis = 1)\n    return x","a6dce97e":"from joblib import Parallel, delayed\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import mean_squared_log_error as msle\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.svm import SVR\nfrom sklearn.multioutput import MultiOutputRegressor\n\nlnr = LinearRegression(fit_intercept = True, n_jobs = -1, normalize = True)\nlnr.fit(x, y)\n\nyfit_lnr = pd.DataFrame(lnr.predict(x), index = x.index, columns = y.columns).clip(0.)\nypred_lnr = pd.DataFrame(lnr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n\nsvr = MultiOutputRegressor(SVR(C = 0.2, kernel = 'rbf'), n_jobs = -1)\nsvr.fit(x, y)\n\nyfit_svr = pd.DataFrame(svr.predict(x), index = x.index, columns = y.columns).clip(0.)\nypred_svr = pd.DataFrame(svr.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\n\nyfit_mean = pd.DataFrame(np.mean([yfit_svr.values, yfit_lnr.values], axis = 0), index = x.index, columns = y.columns).clip(0.)\nypred_mean = pd.DataFrame(np.mean([ypred_lnr.values, ypred_svr.values], axis = 0), index = xtest.index, columns = y.columns).clip(0.)\n\ny_ = y.stack(['store_nbr', 'family'])\ny_['lnr'] = yfit_lnr.stack(['store_nbr', 'family'])['sales']\ny_['svr'] = yfit_svr.stack(['store_nbr', 'family'])['sales']\ny_['mean'] = yfit_mean.stack(['store_nbr', 'family'])['sales']\n\nprint('='*70, 'Linear Regression', '='*70)\nprint(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['lnr']))))\nprint('LNR RMSLE :', np.sqrt(msle(y, yfit_lnr)))\nprint('='*70, 'SVR', '='*70)\nprint(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['svr']))))\nprint('SVR RMSLE :', np.sqrt(msle(y, yfit_svr)))\nprint('='*70, 'Mean', '='*70)\nprint(y_.groupby('family').apply(lambda r : np.sqrt(msle(r['sales'], r['mean']))))\nprint('Mean RMSLE :', np.sqrt(msle(y, yfit_mean)))","45e94ed9":"from sklearn.metrics import mean_absolute_error as mae\n\nprint('='*70, 'Linear Regression', '='*70)\nprint(y_.groupby('family').apply(lambda r : mae(r['sales'], r['lnr'])))\nprint('LNR MAE :', mae(y, yfit_lnr))\nprint('='*70, 'SVR', '='*70)\nprint(y_.groupby('family').apply(lambda r : mae(r['sales'], r['svr'])))\nprint('SVR MAE :', mae(y, yfit_svr))\nprint('='*70, 'Mean', '='*70)\nprint(y_.groupby('family').apply(lambda r : mae(r['sales'], r['mean'])))\nprint('Mean MAE :', mae(y, yfit_mean))","89dde951":"true_low = [2]\npred_low = [4]\n\nprint('RMSLE for low value :', np.sqrt(msle(true_low, pred_low)))\nprint('MAE for low value :', mae(true_low, pred_low))\n\ntrue_high = [255]\npred_high = [269]\n\nprint('RMSLE for high value :', np.sqrt(msle(true_high, pred_high)))\nprint('MAE for high value :', mae(true_high, pred_high))","43e536e4":"display(x, xtest)","6bbbf662":"ypred_svr","7281b802":"fam = 'BOOKS'\nnbr = '1'\nplt.rcParams['figure.figsize'] = (15, 9)\nplt.figure()\ny.loc(axis = 1)['sales', nbr, fam].plot()\nyfit_lnr.loc(axis = 1)['sales', nbr, fam].plot(label = 'Linear Regression')\n#yfit_svr.loc(axis = 1)['sales', nbr, fam].plot(label = 'SVR')\n#yfit_mean.loc(axis = 1)['sales', nbr, fam].plot(label = 'Mean')\n#y.mean(axis = 1).plot()\n#yfit_lnr.median(axis = 1).plot(label = 'Linear Regression')\n#yfit_svr.median(axis = 1).plot(label = 'SVR')\n#yfit_mean.mean(axis = 1).plot(label = 'Mean')\nplt.legend()\nplt.show()","1b9241d7":"ymean = yfit_lnr.append(ypred_lnr)\nschool = ymean.loc(axis = 1)['sales', :, 'SCHOOL AND OFFICE SUPPLIES']\nymean = ymean.join(school.shift(1), rsuffix = 'lag1') # I'm also adding school lag for it's cyclic yearly.\nx = x.loc['2017-05-01':]","3768d7a8":"ymean.loc['2017-08-16':]","0beb43d5":"x = x.join(ymean) # Concating linear result\nxtest = xtest.join(ymean)\ndisplay(x, xtest)","52762ea2":"y = y.loc['2017-05-01':]\ny","b137ebb4":"print(y.isna().sum().sum())","d26e1276":"display(x, xtest)","2e26d855":"from joblib import Parallel, delayed\nimport warnings\n\n# Import necessary library\nfrom sklearn.linear_model import Ridge, LinearRegression, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# SEED for reproducible result\nSEED = 5\n\nclass CustomRegressor():\n    \n    def __init__(self, n_jobs=-1, verbose=0):\n        \n        self.n_jobs = n_jobs\n        self.verbose = verbose\n        \n        self.estimators_ = None\n        \n    def _estimator_(self, X, y):\n    \n        warnings.simplefilter(action='ignore', category=FutureWarning)\n        \n        if y.name[2] == 'SCHOOL AND OFFICE SUPPLIES': # Because SCHOOL AND OFFICE SUPPLIES has weird trend, we use decision tree instead.\n            r1 = ExtraTreesRegressor(n_estimators = 225, n_jobs=-1, random_state=SEED)\n            r2 = RandomForestRegressor(n_estimators = 225, n_jobs=-1, random_state=SEED)\n            b1 = BaggingRegressor(base_estimator=r1,\n                                  n_estimators=10,\n                                  n_jobs=-1,\n                                  random_state=SEED)\n            b2 = BaggingRegressor(base_estimator=r2,\n                                  n_estimators=10,\n                                  n_jobs=-1,\n                                  random_state=SEED)\n            model = VotingRegressor([('et', b1), ('rf', b2)]) # Averaging the result\n        else:\n            ridge = Ridge(fit_intercept=True, solver='auto', alpha=0.75, normalize=True, random_state=SEED)\n            svr = SVR(C = 0.2, kernel = 'rbf')\n            \n            model = VotingRegressor([('ridge', ridge), ('svr', svr)]) # Averaging result\n        model.fit(X, y)\n\n        return model\n\n    def fit(self, X, y):\n        from tqdm.auto import tqdm\n        \n        \n        if self.verbose == 0 :\n            self.estimators_ = Parallel(n_jobs=self.n_jobs, \n                                  verbose=0,\n                                  )(delayed(self._estimator_)(X, y.iloc[:, i]) for i in range(y.shape[1]))\n        else :\n            print('Fit Progress')\n            self.estimators_ = Parallel(n_jobs=self.n_jobs, \n                                  verbose=0,\n                                  )(delayed(self._estimator_)(X, y.iloc[:, i]) for i in tqdm(range(y.shape[1])))\n        return\n    \n    def predict(self, X):\n        from tqdm.auto import tqdm\n        if self.verbose == 0 :\n            y_pred = Parallel(n_jobs=self.n_jobs, \n                              verbose=0)(delayed(e.predict)(X) for e in self.estimators_)\n        else :\n            print('Predict Progress')\n            y_pred = Parallel(n_jobs=self.n_jobs, \n                              verbose=0)(delayed(e.predict)(X) for e in tqdm(self.estimators_))\n        \n        return np.stack(y_pred, axis=1)","9ba4138e":"%%time\n\nmodel = CustomRegressor(n_jobs=-1, verbose=1)\nmodel.fit(x, y)\ny_pred = pd.DataFrame(model.predict(x), index=x.index, columns=y.columns)","f2743cbf":"display(y_pred)\nprint(y_pred.isna().sum().sum())","c6105df4":"from sklearn.metrics import mean_squared_log_error\ny_pred = y_pred.stack(['store_nbr', 'family']).clip(0.)\ny_ = y.stack(['store_nbr', 'family']).clip(0.)\n\ny_['pred'] = y_pred.values\nprint(y_.groupby('family').apply(lambda r : np.sqrt(np.sqrt(mean_squared_log_error(r['sales'], r['pred'])))))\n# Looking at error\nprint('RMSLE : ', np.sqrt(np.sqrt(msle(y_['sales'], y_['pred']))))","bc3fdc4d":"y_pred.isna().sum()","b919368a":"ypred = pd.DataFrame(model.predict(xtest), index = xtest.index, columns = y.columns).clip(0.)\nypred","00ddf732":"ypred = ypred.stack(['store_nbr', 'family'])\nypred","92a7ba49":"sub = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/sample_submission.csv')\nsub['sales'] = ypred.values\nsub.to_csv('submission.csv', index = False) # Submit\nsub","6def8d8a":"As you can see, with RMSLE, the best model is the averaging of linear regression and SVR.\n\nBut, in MAE, Linear Regression has the least loss than Mean. What does it mean?","a3f5c03b":"# Submission","00a768e5":"# Fetching holiday dataset","16b961c7":"# Dependent Variable Viz","dc3c2df4":"We make date in calendar from beginning of train until last date of test.\n\nWe also concatenate calendar with oil price.","01e7d3ff":"# Import Library","b2369b2c":"## Please upvote before fork!!!\nThis notebook is a hyperparameter-ed version from [BIZEN's](https:\/\/www.kaggle.com\/hiro5299834) [Notebook](https:\/\/www.kaggle.com\/hiro5299834\/store-sales-ridge-voting-bagging-et-bagging-rf)\nPlease upvote that notebook too if you find it useful :)","4cfd037c":"You can see that oil price is only high at 2013 to 2014, however in 2015 it's starting to go down.\n\nSo, because we only predict 16 data points we will only need the training data from at least 2015","5166ba77":"Because in RMSLE we are applying log, that means higher the value, the lower the deviation.\n\nLet me show you","8fc66111":"Thank you!!!","948cdcb3":"# DeterministicProcess","491b3019":"Graphs above are the visualization of each product","1815c246":"# Fetching dataset","1c968fd1":"This is the model I use, as I said I'm taking it from [BIZEN](https:\/\/www.kaggle.com\/hiro5299834) and modifying it.","5d0dea96":"You can concat linear regression's prediction with the training data, this is called blending.","9b9994c7":"# Evaluation","a6151091":"I'm not gonna use validation data because the data we have is not much and because we are using linear-based algorithm so only using training would be fine.","a87803cc":"# Adding lags","d2169982":"# Calendar Engineering","748fecd8":"You can see that max value for making a lags is up to 5, but you can take whatever you want.\n\nI'm taking 3 lags of oil","627cfbba":"# Model Creation","ebefb518":"# Feature Engineering for holiday","100fea22":"As you can see, RMSLE will have higher \"tolerance\" for higher value meanwhile Mean Absolute Error will go as usual.\n\nTherefore, I will take the Linear Regression's result instead because it has lower MAE and MAE is reliable because it's robust to outlier","f986b0e4":"# Correlation plot","4baef6d1":"# Using LinearRegression to make a generalized line (It's usually called blending.)","3cde4213":"All seems good."}}