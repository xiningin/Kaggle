{"cell_type":{"d154ee3b":"code","82594dc2":"code","42d46c7b":"code","219eaf08":"code","9bfc20d2":"code","2f3a9000":"code","727c1b05":"code","b56d990c":"code","eeaf5d78":"code","70d9b8e7":"markdown","c06c55f3":"markdown","c536c871":"markdown","900f5590":"markdown","fa70cd19":"markdown","0bd53c22":"markdown","1ac3e923":"markdown"},"source":{"d154ee3b":"#mathematical analysis\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nimport cv2\n#preprocessing\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\n","82594dc2":"#\ntrain_data = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest_data = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nsample = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')\nprint('Shape of input data :',train_data.shape,test_data.shape,sample.shape)\nprint('Identify the Null values : ',train_data.isnull().sum())\ntrain_data.sample(2)\n","42d46c7b":"#add extra one columns\ntrain_data['kfold']=-1\n#Distributing the data 5 shares\nkfold = model_selection.KFold(n_splits=10, shuffle= True, random_state = 42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kfold.split(X=train_data)):\n    #print(fold,train_indicies,valid_indicies)\n    train_data.loc[valid_indicies,'kfold'] = fold\n\n    \nprint(train_data.kfold.value_counts()) #total data 300000 = kfold split :5 * 60000\n\n#output of train folds data\ntrain_data.to_csv(\"trainfold_10.csv\",index=False)","219eaf08":"train = pd.read_csv(\".\/trainfold_10.csv\")\n# Plot dataframe\nheat = train_data.corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(heat)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(heat, annot=False, mask=mask, cmap=\"RdYlGn\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","9bfc20d2":"train.columns","2f3a9000":"#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    xgb_params = {\n        'learning_rate': 0.2113303692287,\n        'subsample': 0.12703520389320402,\n        'colsample_bytree': 0.2566392406542389,\n        'max_depth': 2,\n        'booster': 'gbtree', \n        'reg_lambda': 0.0005172374569093787,\n        'reg_alpha': 0.001273145009879541,\n        'random_state':256,\n        'n_estimators':30000\n        \n        \n    }\n    \n    model= XGBRegressor(**xgb_params,\n                       tree_method='gpu_hist',\n                       predictor='gpu_predictor',\n                       gpu_id=0)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=False)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","727c1b05":"#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    #lgb parameters\n    params_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    'subsample': 0.95312,\n    \"metric\": \"rmse\",\n    'learning_rate': 0.11635,\n    \"max_depth\": 2,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107,\n    'njobs':4\n    }\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_val = lgb.Dataset(xvalid, yvalid)\n    \n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=1000)\n    \n   \n    preds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\n    test_pre = model.predict(xtest,num_iteration=model.best_iteration)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","b56d990c":"from catboost import CatBoostRegressor\n#store the final_prediction data and score\nfinal_predictions = []\nscore= []\n\n#features(categorical and numerical datas separate)\nuseful_features = [c for c in train.columns if c not in (\"Id\",\"Pawpularity\",\"kfold\")]\nobject_cols = [col for col in useful_features]\n#numerical_cols = [col for col in useful_features]\ntest = test_data[useful_features]\n\nfor fold in range(10):\n    xtrain = train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = test.copy()\n    \n    ytrain = xtrain.Pawpularity\n    yvalid = xvalid.Pawpularity\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    #ordinal encode categorical colums and standardscaler is applied (mean0,sd=1)\n    ordinal_encoder = OrdinalEncoder()\n    \n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n\n    #Model hyperparameter of XGboostRegressor\n    #catboost model\n    catpara={\n        'subsample': 0.95312,\n        'learning_rate': 0.0011356,\n        \"max_depth\": 6,\n        \"min_data_in_leaf\":77,\n        'random_state':42,\n        'n_estimators':8000,\n        'rsm':0.5,\n        'l2_leaf_reg': 0.02247766515106271\n    }\n    \n    model=CatBoostRegressor(**catpara)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=1000)\n    model.fit(xtrain,ytrain,early_stopping_rounds=100,eval_set=[(xvalid,yvalid)],verbose=False)\n    preds_valid = model.predict(xvalid)\n    \n    #Training model apply the test data and predict the output\n    test_pre = model.predict(xtest)\n    final_predictions.append(test_pre)\n    \n    #Rootmeansquared output\n    rms = mean_squared_error(yvalid,preds_valid,squared=False)\n    \n    score.append(rms)\n    #way of output is display\n    print(f\"fold:{fold},rmse:{rms}\")\n\n#mean of repeation of fold data and identify the  mean and standard deviation \nprint(np.mean(score),np.std(score))","eeaf5d78":"#prediction of data\npreds = np.mean(np.column_stack(final_predictions),axis=1)\nprint(preds)\nsample.Pawpularity = preds\nsample.to_csv(\"submission.csv\",index=False)\nprint(\"success\")","70d9b8e7":"# KFOLD","c06c55f3":"# Import Necessary library","c536c871":"# Read data","900f5590":"# Visualize to Null data","fa70cd19":"## ***About this Competition***\n\n### ***In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.***\n","0bd53c22":"# XGBRegressor","1ac3e923":"# Prediction output"}}