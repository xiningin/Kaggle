{"cell_type":{"45d03e51":"code","cd6fdf19":"code","ead84e67":"code","4d583dad":"code","1e773aa6":"code","895cfa75":"code","6ad65a58":"code","40121a19":"code","cf9fe254":"code","e90acd12":"code","55978461":"code","fa77573b":"code","e91b8949":"code","2d2f2127":"code","75a189c0":"code","4f4b9e75":"code","f8825a0a":"code","54bcfccd":"code","7b72ba22":"code","ed100b4c":"code","fe13e105":"code","be0a85a3":"code","e68e4120":"code","fc77d4d9":"code","85b3fbe4":"code","cc5df22a":"code","5253e5c1":"code","face4d39":"code","86e398fe":"code","69646ba8":"code","d31db694":"code","1fd1845a":"code","c6407dd4":"code","286211f9":"code","e8dd4f33":"code","571556b1":"code","bfc381cb":"code","73db1ad1":"code","1b138faa":"code","9d7068d6":"code","6fe24569":"code","5ca64cfa":"code","e1ae3646":"code","d6174df8":"code","e53f5695":"code","aeaa20b8":"code","cab5fe4e":"code","5c18cbcf":"code","9d17a3b1":"code","b08a400a":"code","754f4a5d":"code","f0c203ef":"code","37ab72f6":"code","3df1fdb0":"code","597984ac":"code","c365ee9d":"code","4ff414e3":"code","acda4bf9":"code","f300cb28":"code","025cce11":"markdown","534383af":"markdown","1243b046":"markdown","b4240086":"markdown","96074d6f":"markdown","11ab3c8f":"markdown","2b53de73":"markdown","65e4799b":"markdown","edba292a":"markdown","332fe0dc":"markdown","47565eaa":"markdown","c4e723f7":"markdown","edc0bcee":"markdown","8a0db6d7":"markdown","f1c75dfc":"markdown","f46da870":"markdown","50a8291a":"markdown","a703cae1":"markdown","7607a80c":"markdown","0f53a166":"markdown","446abe8c":"markdown","728f16e5":"markdown","b4b22e00":"markdown","68f5ae5c":"markdown","c6bed902":"markdown","aac3a8c7":"markdown","6d9f6776":"markdown","f1fc2541":"markdown","e360858b":"markdown","0350f584":"markdown","5dcb6d8a":"markdown","3b7ba1f6":"markdown","ac6c6a23":"markdown","e1273675":"markdown","7df705b9":"markdown","e29528c2":"markdown","d8829088":"markdown","8a22ebf7":"markdown","c09647d8":"markdown","18b785fc":"markdown"},"source":{"45d03e51":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport math, time, datetime\n# from math import sqrt\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import colors\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.model_selection import (train_test_split, GridSearchCV, cross_val_score)\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, scale\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.metrics import (confusion_matrix, mean_squared_error, r2_score, classification_report,\n                            roc_auc_score, roc_curve, precision_recall_curve, auc, log_loss)\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import mutual_info_classif","cd6fdf19":"df_first = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test_first = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_gender_sub = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\ndf_first.head()","ead84e67":"fig, ax = plt.subplots(figsize = (12, 2))\nax.barh(df_first['Survived'].unique(), df_first['Survived'].value_counts(), align='center', color=['red', 'green'])\nax.text(530, 0, df_first['Survived'].value_counts()[0], ha='center', va='center', color='w', size=20)\nax.text(320, 1, df_first['Survived'].value_counts()[1], ha='center', va='center', color='w', size=20)\nax.set_yticks(df_first['Survived'].unique())\nax.set_yticklabels(df_first['Survived'].unique())\nax.invert_yaxis()\nax.set_ylabel('Survived')\nax.set_title('How many people survived?')\n\nplt.show()","4d583dad":"df_first.info()\nprint(\"----------------------------\")\ndf_test_first.info()","1e773aa6":"## plot graphic of missing values\nmsno.matrix(df_first, figsize=(12, 6))","895cfa75":"# plot graphic of missing values\nmsno.matrix(df_test_first, figsize=(12, 6))","6ad65a58":"# drop unnecessary columns, which won't be used in analysis and prediction\ndf = df_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)\ndf_test = df_test_first.drop(['PassengerId','Name','Ticket', 'Cabin'], axis=1)","40121a19":"# Checking for missing values in train data\ndf.isnull().sum()","cf9fe254":"# Checking for missing values in test data\ndf_test.isnull().sum()","e90acd12":"# getting the Pclass of the Fare missing value\nPclass_fare_mis = df_test[df_test['Fare'].isnull()]['Pclass'].values[0]\nprint(Pclass_fare_mis)\nprint(\"----------------------------\")\n# getting the mean of Fare for the Pclass 3\ndf_Pclass_mean = df_test.groupby(['Pclass']).mean().loc[Pclass_fare_mis,'Fare']\n\n# fill NaN values in Fare column with mean\ndf_test.loc[152, 'Fare'] = df_Pclass_mean\n\ndf_test.isnull().sum()","55978461":"# getting the Pclass of the Embarked missing value\nPclass_embarked_mis = df[df['Embarked'].isnull()]['Pclass'].values\nprint(Pclass_embarked_mis)\nprint(\"----------------------------\")\n# distribution in Embarked by their Pclass\nprint(df[df['Pclass'] == 1]['Embarked'].value_counts())\n\n# for Pclass = 1, the most occurring values are S and C. \n# Therefore, one is filled as 'S', and the other as 'C'. \ndf.loc[61, 'Embarked'] = 'S'\ndf.loc[829, 'Embarked'] = 'C'\n\ndf.isnull().sum()","fa77573b":"mis_val_female = df[df['Sex'] == 'female']['Age'].isna().sum()\nmis_val_male = df[df['Sex'] == 'male']['Age'].isna().sum()\n\nprint(f'Missing values in Age for female: {mis_val_female}')\nprint(f'Missing values in Age for male: {mis_val_male}')","e91b8949":"def generate_random_numbers(df):\n    nan_count = df.isna().sum()\n    \n    ax = df.hist(bins=10, density=True, stacked=True, color='teal', alpha=0.6)\n    ax1 = df.plot(kind='kde', color='teal')\n    \n    # mean age\n    df_mean = df.mean(skipna=True)\n    # median age\n    df_median = df.median(skipna=True)\n    # std age\n    df_std = df.std(skipna=True)\n        \n    # getting density peak values\n    density = stats.gaussian_kde(df.dropna())\n    xs = np.linspace(df.min(),df.max(),200)\n    ys = density(xs)\n    index = np.argmax(ys)\n    max_y = ys[index]\n    max_x = xs[index]\n    \n    # density peak plot\n    ax.axvline(max_x, 0, 1, color='blue', label='peak(x): {:.2f}'.format(max_x))\n    \n    # once the median is closer than the mean to the x value of the density peak, median will be used.\n    # otherwise, the mean will be used to fill in the missing values by generated random numbers.\n    if abs(max_x - df_mean) <=  abs(max_x - df_median):\n        rand_numbers = np.random.randint(df_mean - df_std, df_mean + df_std, \n                                   size = nan_count)\n        ax.axvline(df_mean, 0, 1, color='red', label='mean: {:.2f}'.format(df_mean))\n        # we will generate random numbers using the mean {between (mean - std) & (mean + std)}\n        ax.axvspan(df_mean - df_std, df_mean + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n    else: \n        rand_numbers = np.random.randint(df_median - df_std, df_median + df_std, \n                                   size = nan_count)\n        ax.axvline(df_median, 0, 1, color='red', label='median: {:.2f}'.format(df_median))\n        \n        # we will generate random numbers using the median {between (median - std) & (median + std)}\n        ax.axvspan(df_median - df_std, df_median + df_std, 0, 1, \n                   color='red', alpha=0.2, label='random numbers \\ninterval')\n\n    ax.set(xlabel='Age')\n    ax.legend(title='Female')\n    plt.show()\n\n    return (rand_numbers)","2d2f2127":"df_female_age = df[df['Sex'] == 'female']['Age']\ndf_female_rand_numbers = generate_random_numbers(df_female_age)\n\ndf_female_null = (df[df['Age'].isnull()]['Sex'] == 'female')\ndf_female_ind_null = df_female_null[df_female_null].index\ndf.loc[df_female_ind_null, 'Age'] = df_female_rand_numbers\n\nprint(df.isnull().sum())","75a189c0":"df_male_age = df[df['Sex'] == 'male']['Age']\ndf_male_rand_numbers = generate_random_numbers(df_male_age)\n\ndf_male_null = (df[df['Age'].isnull()]['Sex'] == 'male')\ndf_male_ind_null = df_male_null[df_male_null].index\ndf.loc[df_male_ind_null, 'Age'] = df_male_rand_numbers\n\nprint(df.isnull().sum())","4f4b9e75":"df_test_female_age = df_test[df_test['Sex'] == 'female']['Age']\ndf_test_female_rand_numbers = generate_random_numbers(df_test_female_age)\n\ndf_test_female_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'female')\ndf_test_female_ind_null = df_test_female_null[df_test_female_null].index\ndf_test.loc[df_test_female_ind_null, 'Age'] = df_test_female_rand_numbers\n\nprint(df_test.isnull().sum())","f8825a0a":"df_test_male_age = df_test[df_test['Sex'] == 'male']['Age']\ndf_test_male_rand_numbers = generate_random_numbers(df_test_male_age)\n\ndf_test_male_null = (df_test[df_test['Age'].isnull()]['Sex'] == 'male')\ndf_test_male_ind_null = df_test_male_null[df_test_male_null].index\ndf_test.loc[df_test_male_ind_null, 'Age'] = df_test_male_rand_numbers\n\nprint(df_test.isnull().sum())","54bcfccd":"## Create categorical variable for traveling alone\ndf['TravelAlone']=np.where((df[\"SibSp\"]+df[\"Parch\"])>0, 0, 1)\ndf.drop('SibSp', axis=1, inplace=True)\ndf.drop('Parch', axis=1, inplace=True)\ndf.head()","7b72ba22":"## Create categorical variable for traveling alone\ndf_test['TravelAlone']=np.where((df_test[\"SibSp\"]+df_test[\"Parch\"])>0, 0, 1)\ndf_test.drop('SibSp', axis=1, inplace=True)\ndf_test.drop('Parch', axis=1, inplace=True)\ndf_test.head()","ed100b4c":"col_order = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone', 'Survived']\ndf=df[col_order]\ndf.head()","fe13e105":"for col in df:\n    unique_vals = np.unique(df[col])\n    nr_values = len(unique_vals)\n    if nr_values < 10:\n        print(f'The number of values for feature {col} :{nr_values} -- {unique_vals}')\n    else:\n        print(f'The number of values for feature {col} :{nr_values}')","be0a85a3":"axes = pd.plotting.scatter_matrix(df, alpha=0.7, figsize=(14, 8), diagonal='kde')\nfor ax in axes.flatten():\n    ax.xaxis.label.set_rotation(0)\n    ax.yaxis.label.set_rotation(90)\n    ax.yaxis.label.set_ha('right')\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.gcf().subplots_adjust(wspace=0, hspace=0)\nplt.show()","e68e4120":"def subplot_graph(df, features):\n    f_count = len(features)\n    cols = f_count\n    if f_count < 4:\n        rows = 1\n    else:\n        rows = math.ceil(f_count\/3)\n        cols = 3\n                \n    # Set up the matplotlib figure\n    fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n        \n    sub_rows = 0\n    sub_cols = 0    \n    for f in features:\n        if sub_cols > cols-1:\n            sub_cols = 0\n            sub_rows += 1\n            \n        unique_vals = len(np.unique(df[f]))\n        if unique_vals < 10:\n            df_sub = df.groupby(df.columns[-1])[f].value_counts().sort_index(level=0)\n            x_no = list(df_sub.xs(0, level=0).index)\n            y_no = list(df_sub.xs(0, level=0).values)\n            x_yes = list(df_sub.xs(1, level=0).index)\n            y_yes = list(df_sub.xs(1, level=0).values)\n            list_no = list(zip(x_no, y_no))\n            list_yes = list(zip(x_yes, y_yes))\n            df_no = pd.DataFrame(list_no)\n            df_yes = pd.DataFrame(list_yes)\n\n            df_plot = pd.merge(df_no, df_yes, on=df_no.columns[0], how=\"outer\")\n            df_plot = df_plot.fillna(0)\n            try:\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                   label='1', color='steelblue')\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n            except:\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,1], label='0', color='firebrick')\n                axes[sub_rows, sub_cols].bar(df_plot.iloc[:,0], df_plot.iloc[:,2], bottom=df_plot.iloc[:,1], \n                                             label='1', color='steelblue')  \n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        else:    \n            df_3 = df[df[df.columns[-1]] == 0][f]\n            df_4 = df[df[df.columns[-1]] == 1][f]\n            try:\n                df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_cols].set_title(f)\n                axes[sub_cols].legend(title=df.columns[-1])\n                axes[sub_cols].set_xlim(xmin=0)\n            except:\n                df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n                df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n                axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n                axes[sub_rows, sub_cols].set_title(f)\n                axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n                axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n            \n        sub_cols += 1\n        \n    fig.tight_layout()\n    fig.show()\n\nfeatures = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'TravelAlone']\nsubplot_graph(df, features)","fc77d4d9":"f_count = len(features)\ncols = f_count\nif f_count < 4:\n    rows = 1\nelse:\n    rows = math.ceil(f_count\/3)\n    cols = 3\n    \n# Set up the matplotlib figure\nfig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n\n\nsub_rows = 0\nsub_cols = 0    \nfor f in features:\n    if sub_cols > cols-1:\n        sub_cols = 0\n        sub_rows += 1\n        \n    \n    unique_vals = len(np.unique(df[f]))\n    if unique_vals < 10:\n        df_yy = pd.DataFrame()\n        df1 = df.groupby([f])[df.columns[-1]].value_counts().sort_index(level=0)\n        df_uniq = df1.index.levels[0]\n        f_uniqs = []\n        sur_rates = []\n        for uniq in df_uniq:\n            try:\n                df2 = df1.xs(uniq, level=0)\n                df_sum = df2.sum()\n                df_lev_1_each = df2[1]\n                sur_rate = round((df_lev_1_each \/ df_sum)*100, 1)\n                sur_rates.append(sur_rate)\n                f_uniqs.append(uniq)\n            except:\n                pass\n\n        df_xx = pd.DataFrame(sur_rates, index=f_uniqs, columns=[f])\n        df_yy = pd.concat([df_yy, df_xx], axis=1)\n        \n        df_sub = df_yy[f].dropna()\n#         print(df_sub)\n\n        try:\n            axes[sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n        except:\n            axes[sub_rows, sub_cols].bar(df_sub.index, df_sub.values, color='steelblue')\n            axes[sub_rows, sub_cols].set(ylabel=\"Survived (%)\")\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].set_xticks(df[f].unique().tolist(), minor=False)\n    else:    \n        df_3 = df[df[df.columns[-1]] == 0][f]\n        df_4 = df[df[df.columns[-1]] == 1][f]\n        try:\n            df_3.plot(ax=axes[sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_cols].set_title(f)\n            axes[sub_cols].legend(title=df.columns[-1])\n            axes[sub_cols].set_xlim(xmin=0)\n        except:\n            df_3.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='red', label='0')\n            df_4.plot(ax=axes[sub_rows, sub_cols], kind='kde', color='steelblue', label='1')\n            axes[sub_rows, sub_cols].hist(df_3, bins=20, density=True, stacked=True, color='lightgreen')\n            axes[sub_rows, sub_cols].set_title(f)\n            axes[sub_rows, sub_cols].legend(title=df.columns[-1])\n            axes[sub_rows, sub_cols].set_xlim(xmin=-5, xmax=100)\n        \n    sub_cols += 1\n    \nfig.tight_layout()\nfig.show()","85b3fbe4":"def create_heatmap(hm, figsize=(7, 6)):\n    fig, ax = plt.subplots(figsize=figsize)\n\n    im = ax.imshow(hm, \n    #                vmin=0, vmax=10, \n                   cmap='viridis', aspect='auto')\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax)\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(hm.columns)))\n    ax.set_yticks(np.arange(len(hm.columns)))\n    # ... and label them with the respective list entries\n    ax.set_xticklabels(hm.columns)\n    ax.set_yticklabels(hm.columns)\n\n    # Turn spines off and create white grid.\n    ax.spines[:].set_visible(False)\n    ax.set_xticks(np.arange(hm.shape[1]+1)-.5, minor=True)\n    ax.set_yticks(np.arange(hm.shape[0]+1)-.5, minor=True)\n    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(hm.columns)):\n        for j in range(len(hm.columns)):\n            hm_val = round(hm.values[i, j], 2)\n            if hm_val > 0.85:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"black\", size=16)\n            else:\n                text = ax.text(j, i, hm_val,\n                               ha=\"center\", va=\"center\", color=\"w\", size=16)\n\n    fig.tight_layout()\n    plt.show()","cc5df22a":"hm = df.corr()\ncreate_heatmap(hm)","5253e5c1":"X = df[df.columns[:-1]]\ny = df[df.columns[-1]]\nX_kaggle = df_test.copy()\nprint(X.shape)\nprint(X_kaggle.shape)","face4d39":"# There is no categorical columns in the dataframe\ncategorical_cols = list(set(X.columns) - set(X._get_numeric_data().columns))\ncategorical_cols","86e398fe":"numerical_cols = list(X._get_numeric_data().columns)\nnumerical_cols","69646ba8":"categorical_cols + ['Pclass', 'TravelAlone']","d31db694":"from sklearn.compose import make_column_transformer\n\ncolumn_trans = make_column_transformer((OneHotEncoder(), categorical_cols + ['Pclass', 'TravelAlone']),\n                                      remainder='passthrough')\nX_trans = column_trans.fit_transform(X)\nX_kaggle_trans = column_trans.fit_transform(X_kaggle)\n\ncols = []\nfor i in column_trans.transformers_[0][2]:\n    cols_enc = sorted(X[i].unique())\n    for col_enc in cols_enc:\n        col_name = str(i) + '_' + str(col_enc)\n        cols.append(col_name)\n        \nfor i in column_trans.transformers_[1][2]:\n    col_name = X.columns[i]\n    cols.append(col_name)\n    \nX_encoded = pd.DataFrame(X_trans, columns=cols)\nX_kaggle_encoded = pd.DataFrame(X_kaggle_trans, columns=cols)\nprint(X_encoded.shape)\nprint(X_kaggle_encoded.shape)\nX_encoded.head()","1fd1845a":"X_kaggle_encoded.head()","c6407dd4":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X_encoded)\nX_encoded.columns[var_thres.get_support()]\n\nconstant_columns = [column for column in X_encoded.columns\n                    if column not in X_encoded.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","286211f9":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, train_size = 0.8, test_size=0.2, random_state=42)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","e8dd4f33":"hm_X_train = X_train.corr()\ncreate_heatmap(hm_X_train, figsize=(10, 6))","571556b1":"#  to select highly correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                \n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","bfc381cb":"corr_features = correlation(X_train, 0.85)\ncorr_features","73db1ad1":"X_train = X_train.drop(corr_features, axis=1)\nX_test = X_test.drop(corr_features, axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(corr_features, axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","1b138faa":"# determine the mutual information\nmutual_info = mutual_info_classif(X_train.values, y_train)\nmutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","9d7068d6":"X_train = X_train.drop(['Embarked_C', 'Embarked_Q'], axis=1)\nX_test = X_test.drop(['Embarked_C', 'Embarked_Q'], axis=1)\nX_kaggle_encoded = X_kaggle_encoded.drop(['Embarked_C', 'Embarked_Q'], axis=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(X_kaggle_encoded.shape)","6fe24569":"def model_evaluation(cols):\n    param_grid = [{\n    'criterion': ['entropy', 'gini'],\n    'splitter': ['best', 'random'],\n    'random_state': [0, 5, 10, 20, 42],\n    'max_depth': [None, 5, 10, 15, 20, 25],\n    }]\n\n    optimal_param = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy', verbose=0)\n    optimal_param.fit(X_test[cols], y_test)\n    dt = optimal_param.best_estimator_\n#     print(cols)\n#     print(optimal_param.best_estimator_)\n\n\n#     dt = DecisionTreeClassifier(criterion='entropy', max_depth=4, random_state=0, splitter='random')    \n    dt.fit(X_train[cols], y_train)\n    y_pred_train = dt.predict(X_train[cols])\n    y_pred_test = dt.predict(X_test[cols])\n\n#     print(\"Test Accuracy: {:.1%}\".format(dt.score(X_test[cols],y_test)))\n#     print(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\n#     print(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\n#     print('='*10)\n\n    dt_acc = dt.score(X_test[cols],y_test)\n    dt_mse = mean_squared_error(y_test, y_pred_test)\n    dt_r2 = r2_score(y_test, y_pred_test)\n    \n    return dt, dt_acc, dt_mse, dt_r2","5ca64cfa":"import itertools\n\nstuff = X_train.columns\n\nopt_param = []\nbest_cols = []\nbest_acc = []\nbest_mse = []\nbest_r2 = []\nfor L in range(0, len(X_train)+1):\n    for subset in itertools.combinations(stuff, L):\n        sub_list = list(subset)\n        if len(sub_list) > 2:\n            dt, dt_acc, dt_mse, dt_r2 = model_evaluation(sub_list)\n            best_cols.append(sub_list)\n            best_acc.append(dt_acc)\n            best_mse.append(dt_mse)\n            best_r2.append(dt_r2)\n            opt_param.append(dt)","e1ae3646":"best_zip = zip(opt_param, best_cols, best_acc, best_mse, best_r2)\ndf_best_acc = pd.DataFrame(best_zip, columns=['dt', 'columns', 'accuracy', 'mse', 'r2'])\ndf_best_acc = df_best_acc.sort_values('accuracy', ascending = False).reset_index(drop=True)\npd.set_option('max_colwidth', -1)\ndf_best_acc.head(10)","d6174df8":"cols_final = ['Embarked_S', 'Sex_female', 'Pclass_2', 'Pclass_3', 'TravelAlone_0', 'Age', 'Fare']\n\nX_train_final = X_train[cols_final]\nX_test_final = X_test[cols_final]\nX_kaggle_final = X_kaggle_encoded[cols_final]\n\nprint(X_train_final.shape)\nprint(X_test_final.shape)\nprint(X_kaggle_final.shape)","e53f5695":"dt = DecisionTreeClassifier(random_state=0, criterion = 'entropy', max_depth = 10, splitter='random')    \ndt.fit(X_train_final, y_train)\ny_pred_train = dt.predict(X_train_final)\npred_proba_train = dt.predict_proba(X_train_final)\n\nprint(\"Train Accuracy: {:.1%}\".format(dt.score(X_train_final,y_train)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_train, y_pred_train)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_train, y_pred_train)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_train, y_pred_train))\nprint('='*10)\nprint(classification_report(y_train, y_pred_train))","aeaa20b8":"y_pred_test = dt.predict(X_test_final)\npred_proba_test = dt.predict_proba(X_test_final)\n\nprint(\"Test Accuracy: {:.1%}\".format(dt.score(X_test_final,y_test)))\nprint(\"mean_squared_error: {:.3}\".format(mean_squared_error(y_test, y_pred_test)))\nprint(\"r2_score: {:.3}\".format(r2_score(y_test, y_pred_test)))\nprint('='*10)\nprint('confusion_matrix')\nprint(confusion_matrix(y_test, y_pred_test))\nprint('='*10)\nprint(classification_report(y_test, y_pred_test))","cab5fe4e":"def confusion_matrix_func(cm, cm_title):\n    fig, ax = plt.subplots(figsize=(4, 4))\n\n    # Plot the heatmap\n    im = ax.imshow(cm, interpolation='nearest', cmap='Reds', aspect='auto')\n\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(len(cm.tolist())))\n    ax.set_yticks(np.arange(len(cm.tolist())))\n\n\n    thresh = cm.max() \/ 1.5\n    # Loop over data dimensions and create text annotations.\n    for i in range(len(cm.tolist())):\n        for j in range(len(cm.tolist())):\n            text = ax.text(j, i, cm.tolist()[i][j],\n                           ha=\"center\", va=\"center\", size=16,\n                           color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n\n    plt.xlabel('Actual value', size=16)\n    plt.ylabel('Predicted value', size=16)\n    plt.title(cm_title, size=20, x=0.2, y=1.2)\n    plt.show()\n# https:\/\/matplotlib.org\/stable\/gallery\/images_contours_and_fields\/image_annotated_heatmap.html","5c18cbcf":"def Confusion_matrix_metrics(TP, FP, FN, TN):\n    # Sensitivity, hit rate, recall, or true positive rate\n    TPR = TP \/ (TP + FN)\n    print('The True Positive Rate is: {:.2%}'.format(TPR))\n    # Specificity, selectivity or true negative rate (TNR)\n    TNR = TN \/ (TN + FP)\n    print('The True Negative Rate is: {:.2%}'.format(TNR))\n    print('='*10)\n\n    # accuracy (ACC)\n    ACC = (TP + TN) \/ (TP + TN + FP + FN)\n    print('The Accuracy is: {:.2%}'.format(ACC))\n    # balanced accuracy (BA)\n    BA = (TPR + TNR) \/ 2\n    print('The Balanced Accuracy is: {:.2%}'.format(BA))\n    print('='*10)\n\n    # Precision or positive predictive value\n    PPV = TP \/ (TP + FP)\n    print('The Precision is: {:.2%}'.format(PPV))\n    # negative predictive value (NPV)\n    NPV = TN \/ (TN + FN)\n    print('The Negative Predictive Value is: {:.2%}'.format(NPV))\n    # false discovery rate (FDR)\n    FDR = 1 - PPV\n    print('The False Discovery Rate is: {:.2%}'.format(FDR))\n    # false omission rate (FOR)\n    FOR = 1 - NPV\n    print('The False Omission Rate is: {:.2%}'.format(FOR))\n    print('='*10)\n\n    # prevalence threshold (PT)\n    PT = (math.sqrt(TPR*(1 - TNR)) + TNR - 1)\/(TPR + TNR - 1)\n    print('The Prevalence Threshold is: {:.2}'.format(PT))\n    # F1 score\n    F1 = 2*TP \/ (2*TP + FP + FN)\n    print('The F1 Score is: {:.2}'.format(F1))\n    # Matthews correlation coefficient (MCC) or phi coefficient\n    MCC = ((TP*TN) - (FP*FN)) \/ math.sqrt((TP + FP)*(TP + FN)*(TN + FP)*(TN + FN))\n    print('The Matthews Correlation Coefficient is: {:.2}'.format(MCC))\n    print('='*10)\n\n    # False positive rate or False alarm rate\n    FPR = FP \/ (FP + TN)\n    print('The False positive rate is: {:.2}'.format(FPR))\n    # False negative rate or Miss Rate\n    FNR = FN \/ (FN + TP)\n    print('The False Negative Rate is: {:.2%}'.format(FNR))","9d17a3b1":"cm = confusion_matrix(y_train, y_pred_train).T\nconfusion_matrix_func(cm, cm_title=\"Confusion Matrix_Train\")","b08a400a":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","754f4a5d":"cm_test = confusion_matrix(y_test, y_pred_test).T\nconfusion_matrix_func(cm_test, cm_title=\"Confusion Matrix_Test\")","f0c203ef":"# Calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\nTP, FP, FN, TN = cm_test.ravel()\nConfusion_matrix_metrics(TP, FP, FN, TN)","37ab72f6":"# TRAIN\n# calculate scores\nlr_auc = roc_auc_score(y_train, pred_proba_train[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, thresholds = roc_curve(y_train, pred_proba_train[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc = pd.DataFrame({\n    'False Positive Rate': lr_fpr,\n    'True Positive Rate': lr_tpr\n}, index=thresholds)\ndf_roc.index.name = \"Thresholds\"\ndf_roc.columns.name = \"Rate\"\n\n\n# TEST\n# calculate scores\nlr_auc_test = roc_auc_score(y_test, pred_proba_test[:, 1])\n# summarize scores\n# print('Logistic: ROC AUC=%.3f' % (lr_auc_test))\n# calculate roc curves\nlr_fpr_test, lr_tpr_test, thresholds_test = roc_curve(y_test, pred_proba_test[:, 1])\n\n# Evaluating model performance at various thresholds\ndf_roc_test = pd.DataFrame({\n    'False Positive Rate': lr_fpr_test,\n    'True Positive Rate': lr_tpr_test\n}, index=thresholds_test)\ndf_roc_test.index.name = \"Thresholds\"\ndf_roc_test.columns.name = \"Rate\"\n\n\n# GRAPH\n# Set up the matplotlib figure\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# row=0, col=0\naxes[0, 0].plot(df_roc.iloc[:,0], df_roc.iloc[:,1], color='red', linewidth=2, \n                label=f'AUC={lr_auc:.2f}')\naxes[0, 0].fill_between(df_roc.iloc[:,0], df_roc.iloc[:,1], 0, color='red', alpha=0.3)\naxes[0, 0].plot(df_roc_test.iloc[:,0], df_roc_test.iloc[:,1], color='black', linewidth=2, \n                label=f'AUC_test={lr_auc_test:.2f}')\naxes[0, 0].plot([0, 1], [0, 1], color='green', linestyle='--', linewidth=1,\n                label='No Skill')\n\n# index of the first threshold for which the sensibility > 0.90\nidx = np.min(np.where(lr_tpr > 0.90))\naxes[0, 0].plot([0,lr_fpr[idx]], [lr_tpr[idx],lr_tpr[idx]], 'k--', color='blue')\naxes[0, 0].plot([lr_fpr[idx],lr_fpr[idx]], [0,lr_tpr[idx]], 'k--', color='blue')\n# Annotation\naxes[0, 0].annotate('(%.2f, %.2f)'%(lr_fpr[idx], lr_tpr[idx]),\n            (lr_fpr[idx], lr_tpr[idx]), \n            xytext =(-2 * 50, -30),\n            textcoords ='offset points',\n            bbox = dict(boxstyle =\"round\", fc =\"0.8\"), \n            arrowprops = dict(arrowstyle = \"->\"))\n\naxes[0, 0].set_xlabel('False Positive Rate', size=12)\naxes[0, 0].set_ylabel('True Positive Rate (recall)', size=12)\naxes[0, 0].legend(title='kNN')\naxes[0, 0].set_title('ROC curve', color='red', size=14)\n\n# row=0, col=1\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"True Positive Rate\"][1:], color='blue', linewidth=2, \n                label='TPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"True Positive Rate\"][1:], color='black', linewidth=2, \n                label='TPR_test')\naxes[0, 1].plot(df_roc.index[1:], df_roc[\"False Positive Rate\"][1:], color='orange', linewidth=2, \n                label='FPR')\naxes[0, 1].plot(df_roc_test.index[1:], df_roc_test[\"False Positive Rate\"][1:], color='black', linewidth=2, \n                label='FPR_test')\n\naxes[0, 1].set_xlabel('Threshold', size=12)\naxes[0, 1].legend()\naxes[0, 1].set_title('TPR and FPR at every threshold', color='red', size=14)\n\n# row=1, col=0\nprecision, recall, thresholds = precision_recall_curve(y_test, pred_proba_test[:, 1])\n\naxes[1, 0].plot(recall, precision, color='green', linewidth=2, \n                label=f'PR_Curve (AUC={auc(lr_fpr, lr_tpr):.2f})')\naxes[1, 0].fill_between(recall, precision, 0, color='green', alpha=0.3)\n\naxes[1, 0].set_xlabel('Recall', size=12)\naxes[1, 0].set_ylabel('Precision', size=12)\naxes[1, 0].legend()\naxes[1, 0].set_title('Precision-Recall Curve', color='red', size=14)\n\nfig.tight_layout()\nfig.show()","3df1fdb0":"# Running Log loss on training\nprint('The Log Loss on Training is: {:.2}'.format(log_loss(y_train, pred_proba_train[:, 1])))\n\n# Running Log loss on testing\nprint('The Log Loss on Testing Dataset is: {:.2}'.format(log_loss(y_test, pred_proba_test[:, 1])))","597984ac":"df_gender_sub.head()","c365ee9d":"submission = df_gender_sub.drop('Survived', axis=1)\ny_pred_kaggle = dt.predict(X_kaggle_final)\nsubmission['Survived'] = y_pred_kaggle\nsubmission.head()","4ff414e3":"# Are our test and submission dataframes the same length?\nif len(submission) == len(df_gender_sub):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","acda4bf9":"# Convert submisison dataframe to csv for submission to csv for Kaggle submisison\nsubmission.to_csv('titanic_submission_dt.csv', index=False)\nprint('Submission CSV is ready!')","f300cb28":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"titanic_submission_dt.csv\")\nsubmissions_check.head()","025cce11":"<a id='1'><\/a>\n## 1 Importing packages","534383af":"<a id='42'><\/a>\n### 4_2 Check categorical columns","1243b046":"#### 3_2_2 Test data","b4240086":"{'Sex_male'} {'Sex_female'} and {'TravelAlone_0'} {'TravelAlone_1'} are highly correlated features in the dataset.","96074d6f":"<a id='45'><\/a>\n### 4_5 Separate the dataset into train and test","11ab3c8f":"<a id='49'><\/a>\n### 4.9. roc curve and auc","2b53de73":"<a id='5'><\/a>\n## 5. Submission","65e4799b":"<a id='48'><\/a>\n### 4.8. Confusion matrix","edba292a":"#### 3_2_1 Train data","332fe0dc":"<a id='32'><\/a>\n### 3_2 Combine SibSp and Parch for Simplicity","47565eaa":"<a id='464'><\/a>\n#### 4_6_4 Feature_Selection - Final","c4e723f7":"<a id='461'><\/a>\n#### 4_6_1 Feature Selection - Drop Features Using Pearson Correlation","edc0bcee":"**The goal** is to predict the target variable(Survived) using logistic regression.","8a0db6d7":"<a id='46'><\/a>\n### 4_6 Feature Selection","f1c75dfc":"<a id='463'><\/a>\n#### 4_6_3 Feature_Selection - Evaluation of all column combinations","f46da870":"<a id='33'><\/a>\n### 3_3 Move the dependent column to the end","50a8291a":"## Index\n\n[1 Importing packages](#1)<br>\n[2 Read CSV train\/test files into DataFrame](#2)<br>\n[3 Data Preprocessing](#3)<br>\n    <ul>\n        <li>[3_1 Missing Values](#31)<\/li>\n            <ul><li>[3_1_1 Fare - Missing Values](#311)<\/li>\n            <li>[3_1_2 Embarked - Missing Values](#12)<\/li>\n            <li>[3_1_3 Age - Missing Values](#313)<\/li><\/ul>\n        <li>[3_2 Combine SibSp and Parch for Simplicity](#32)<\/li>\n        <li>[3_3 Move the dependent column to the end](#33)<\/li>\n        <li>[3_4 Investigate all the elements within each Feature](#34)<\/li>\n        <li>[3_5 Exploratory Data Analysis](#35)<\/li>\n    <\/ul>\n[4 Regressions and Results](#4)<br>\n    <ul>\n        <li>[4_1 Separate the dataset](#41)<\/li>\n        <li>[4_2 Check categorical columns](#42)<\/li>\n        <li>[4_3 One_Hot_Encoding](#43)<\/li>\n        <li>[4_4 Check zero variance features](#44)<\/li>\n        <li>[4_5 Separate the dataset into train and test](#45)<\/li>\n        <li>[4_6 Feature Selection](#46)<\/li>\n            <ul><li>[4_6_1 Feature Selection - Drop Features Using Pearson Correlation](#461)<\/li>\n            <li>[4_6_2 Feature_Selection - Mutual information](#462)<\/li>\n            <li>[4_6_3 Feature_Selection - Feature_Selection - Evaluation of all column combinations](#463)<\/li>\n            <li>[4_6_4 Feature_Selection - Final](#464)<\/li><\/ul>\n        <li>[4_7 Evaluating The Decision Trees Model](#47)<\/li>\n        <li>[4_8 Confusion matrix](#48)<\/li>\n        <li>[4_9 roc curve and auc](#49)<\/li>\n        <li>[4_10 Logarithmic loss](#410)<\/li>\n[5. Submission](#5)<br>","a703cae1":"<a id='44'><\/a>\n### 4_4 Check zero variance features","7607a80c":"<a id='34'><\/a>\n### 3_4 Investigate all the elements within each Feature ","0f53a166":"<a id='43'><\/a>\n### 4_3 One-Hot Encoding","446abe8c":"pclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","728f16e5":"<a id='31'><\/a>\n### 3_1 Missing Values","b4b22e00":"<a id='312'><\/a>\n#### 3_1_2 Embarked - Missing Values","68f5ae5c":"PLife -> Life is unfortunately not fair. If you're a 3rd class person in this world, it's a chance for you to survive.\n\nSex -> Positive discrimination has been made against women here as well.\n\nAge -> The age distribution for survivors and deceased is very similar except for children. The travelers strived for the survival of the children.\n\nFare -> Passengers who pay lower fare seem less likely to survive.\n\nEmbarked -> Although the number of boarders in Southhampton is higher than those in Cherbourg, the survival rate of Cherbourg is higher. This is probably related to the socioeconomic situation.","c6bed902":"<a id='321'><\/a>\n#### 3_1_1 Fare - Missing Values","aac3a8c7":"<a id='35'><\/a>\n### 3_5 Exploratory Data Analysis","6d9f6776":"<a id='4'><\/a>\n## 4 Regressions and Results","f1fc2541":"##### 3_1_2_1 Missing values in train data","e360858b":"##### 3_1_3_1 Missing values in train data","0350f584":"<a id='462'><\/a>\n#### 4_6_2 Feature_Selection - Mutual information","5dcb6d8a":"<a id='3'><\/a>\n## 3 Data Preprocessing","3b7ba1f6":"<a id='47'><\/a>\n### 4.7. Evaluating The Decision Trees Model","ac6c6a23":"![titanic_data_dict.png](attachment:titanic_data_dict.png)","e1273675":"Mutual information (MI) measures the dependency between the variables. Higher values mean higher dependency.","7df705b9":"##### 3_1_1_1 Missing values in test data","e29528c2":"<a id='410'><\/a>\n### 4.10. Logarithmic loss","d8829088":"##### 3_1_3_2 Missing values in test data","8a22ebf7":"<a id='2'><\/a>\n## 2 Read CSV train\/test files into DataFrame","c09647d8":"<a id='41'><\/a>\n### 4_1 Separate the dataset","18b785fc":"<a id='313'><\/a>\n#### 3_1_3 Age - Missing Values"}}