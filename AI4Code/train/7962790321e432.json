{"cell_type":{"3e98e1fd":"code","0df9bceb":"code","f9d54cf9":"code","8b0b4343":"code","e62adfa2":"code","a5da6ea6":"code","2c6f44c5":"code","85b7a374":"code","f6720bd1":"code","c110e83f":"code","a580d872":"code","d6163ee6":"code","1983d52e":"code","6e6569ed":"code","144923a2":"code","f86ffedd":"code","1a56fec8":"code","86098d36":"code","8303a260":"code","8eef8c74":"code","7c56d9b4":"code","27c1c61a":"code","0c858c52":"code","d6372cb5":"code","c6520546":"code","805c9ba9":"code","2898cb15":"code","c04bf7b8":"code","77559a37":"code","a40e90c7":"code","4cdba645":"code","10e18e70":"code","d27b22a9":"markdown","617b1174":"markdown","716090ea":"markdown","6648ebfd":"markdown","da03f5d9":"markdown","e12d6ecf":"markdown","a94f434b":"markdown","02822200":"markdown","e55f3871":"markdown","54fe370a":"markdown","da47fd56":"markdown","2e23fd2f":"markdown","a794b113":"markdown","69c44702":"markdown","6a04d20c":"markdown","9dcec814":"markdown","f611ca38":"markdown","1afa2934":"markdown","0b027358":"markdown"},"source":{"3e98e1fd":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport lightgbm as lgb\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\nfrom tqdm import tqdm_notebook as tqdm\nfrom scipy.sparse import csr_matrix\nfrom joblib import Parallel, delayed\nimport random\nfrom statsmodels.tsa.api import SimpleExpSmoothing\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0df9bceb":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns: #columns\u6bce\u306b\u51e6\u7406\n        col_type = df[col].dtypes\n        if col_type in numerics: #numerics\u306e\u30c7\u30fc\u30bf\u578b\u306e\u7bc4\u56f2\u5185\u306e\u3068\u304d\u306b\u51e6\u7406\u3092\u5b9f\u884c. \u30c7\u30fc\u30bf\u306e\u6700\u5927\u6700\u5c0f\u5024\u3092\u5143\u306b\u30c7\u30fc\u30bf\u578b\u3092\u52b9\u7387\u7684\u306a\u3082\u306e\u306b\u5909\u66f4\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n","f9d54cf9":"def read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    \n    sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    \n    sales_train_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0], sales_train_val.shape[1]))\n    \n    submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    \n    return calendar, sell_prices, sales_train_val, submission","8b0b4343":"import IPython\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","e62adfa2":"calendar, sell_prices, sales_train_val, submission = read_data()","a5da6ea6":"# \u4e88\u6e2c\u671f\u9593\u3068item\u6570\u306e\u5b9a\u7fa9\nNUM_ITEMS = sales_train_val.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","2c6f44c5":"def explanatory_variables(df):\n    \n    df['ex1'] = df['snap_CA'] + df['snap_TX'] + df['snap_WI']\n    df['ex2'] = 1 * (pd.notnull(df['event_name_1']) | pd.notnull(df['event_name_2']))\n\n    return df\n\n\ncalendar = explanatory_variables(calendar).pipe(reduce_mem_usage)","85b7a374":"calendar.head()","f6720bd1":"nrows = 365 * 2 * NUM_ITEMS","c110e83f":"#\u52a0\u5de5\u524d  \ndisplay(sales_train_val.head(5))","a580d872":"sales_train_val = pd.melt(sales_train_val,\n                                     id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')","d6163ee6":"#\u52a0\u5de5\u5f8c  \ndisplay(sales_train_val.head(5))\nprint('Melted sales train validation has {} rows and {} columns'.format(sales_train_val.shape[0],\n                                                                            sales_train_val.shape[1]))","1983d52e":"sales_train_val = sales_train_val.iloc[-nrows:,:]","6e6569ed":"# seperate test dataframes\n\n# submission file\u306eid\u306evalidation\u90e8\u5206\u3068, ealuation\u90e8\u5206\u306e\u540d\u524d\u3092\u53d6\u5f97\ntest1_rows = [row for row in submission['id'] if 'validation' in row]\ntest2_rows = [row for row in submission['id'] if 'evaluation' in row]\n\n# submission file\u306evalidation\u90e8\u5206\u3092test1, ealuation\u90e8\u5206\u3092test2\u3068\u3057\u3066\u53d6\u5f97\ntest1 = submission[submission['id'].isin(test1_rows)]\ntest2 = submission[submission['id'].isin(test2_rows)]\n\n# test1, test2\u306e\u5217\u540d\u306e\"F_X\"\u306e\u7b87\u6240\u3092d_XXX\"\u306e\u5f62\u5f0f\u306b\u5909\u66f4\ntest1.columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\ntest2.columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n# test2\u306eid\u306e'_evaluation'\u3092\u7f6e\u63db\n#test1['id'] = test1['id'].str.replace('_validation','')\ntest2['id'] = test2['id'].str.replace('_evaluation','_validation')\n\n# sales_train_val\u304b\u3089id\u306e\u8a73\u7d30\u90e8\u5206(item\u3084department\u306a\u3069\u306eid)\u3092\u91cd\u8907\u306a\u304f\u4e00\u610f\u306b\u53d6\u5f97\u3002\nproduct = sales_train_val[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n\n# id\u3092\u30ad\u30fc\u306b\u3057\u3066, id\u306e\u8a73\u7d30\u90e8\u5206\u3092test1, test2\u306b\u7d50\u5408\u3059\u308b.\ntest1 = test1.merge(product, how = 'left', on = 'id')\ntest2 = test2.merge(product, how = 'left', on = 'id')\n\n# test1, test2\u3092\u3068\u3082\u306bmelt\u51e6\u7406\u3059\u308b.\uff08\u58f2\u4e0a\u6570\u91cf:demand\u306f0\uff09\ntest1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\ntest2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                var_name = 'day', value_name = 'demand')\n\n# validation\u90e8\u5206\u3068, evaluation\u90e8\u5206\u304c\u308f\u304b\u308b\u3088\u3046\u306bpart\u3068\u3044\u3046\u5217\u3092\u4f5c\u308a\u3001 test1,test2\u306e\u30e9\u30d9\u30eb\u3092\u4ed8\u3051\u308b\u3002\nsales_train_val['part'] = 'train'\ntest1['part'] = 'test1'\ntest2['part'] = 'test2'\n\n# sales_train_val\u3068test1, test2\u306e\u7e26\u7d50\u5408.\ndata = pd.concat([sales_train_val, test1, test2], axis = 0)\n\n# memory\u306e\u958b\u653e\ndel test1, test2, sales_train_val\n\n# delete test2 for now(6\/1\u4ee5\u524d\u306f, validation\u90e8\u5206\u306e\u307f\u63d0\u51fa\u306e\u305f\u3081.)\ndata = data[data['part'] != 'test2']\n\ngc.collect()","144923a2":"#calendar\u306e\u7d50\u5408\n# drop some calendar features(\u4e0d\u8981\u306a\u5909\u6570\u306e\u524a\u9664:weekday\u3084wday\u306a\u3069\u306fdatetime\u5909\u6570\u304b\u3089\u5f8c\u307b\u3069\u4f5c\u6210\u3067\u304d\u308b\u3002)\ncalendar.drop(['weekday', 'wday', 'month', 'year'], \n              inplace = True, axis = 1)\n\n# notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)(day\u3068d\u3092\u30ad\u30fc\u306bdata\u306b\u7d50\u5408)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndata.drop(['d', 'day'], inplace = True, axis = 1)\n\n# memory\u306e\u958b\u653e\ndel calendar\ngc.collect()\n\n#sell price\u306e\u7d50\u5408\n# get the sell price data (this feature should be very important)\ndata = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\nprint('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n\n# memory\u306e\u958b\u653e\ndel sell_prices\ngc.collect()","f86ffedd":"data.head(3)","1a56fec8":"display(data.head())","86098d36":"data = data[['date', 'demand', 'id', 'ex1', 'ex2', 'sell_price']]\n\n# going to evaluate with the last 28 days\nx_train = data[data['date'] <= '2016-03-27']\ny_train = x_train['demand']\nx_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\ny_val = x_val['demand']\ntest = data[(data['date'] > '2016-04-24')]\n\n#data\u306e\u524a\u9664\uff08\u30e1\u30e2\u30ea\u306e\u524a\u9664\uff09\n#del data\n#gc.collect()","8303a260":"weight_mat = np.c_[np.identity(NUM_ITEMS).astype(np.int8), #item :level 12\n                   np.ones([NUM_ITEMS,1]).astype(np.int8), # level 1\n                   pd.get_dummies(product.state_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.cat_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.store_id.astype(str) + product.dept_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.item_id.astype(str),drop_first=False).astype('int8').values,\n                   pd.get_dummies(product.state_id.astype(str) + product.item_id.astype(str),drop_first=False).astype('int8').values\n                   ].T\n\nweight_mat_csr = csr_matrix(weight_mat)\ndel weight_mat; gc.collect()\n\ndef weight_calc(data,product):\n\n    # calculate the denominator of RMSSE, and calculate the weight base on sales amount\n    \n    sales_train_val = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    \n    d_name = ['d_' + str(i+1) for i in range(1913)]\n    \n    sales_train_val = weight_mat_csr * sales_train_val[d_name].values\n    \n    # calculate the start position(first non-zero demand observed date) for each item \/ \u5546\u54c1\u306e\u6700\u521d\u306e\u58f2\u4e0a\u65e5\n    # 1-1914\u306eday\u306e\u6570\u5217\u306e\u3046\u3061, \u58f2\u4e0a\u304c\u5b58\u5728\u3057\u306a\u3044\u65e5\u3092\u4e00\u65e60\u306b\u3057\u30010\u30929999\u306b\u7f6e\u63db\u3002\u305d\u306e\u3046\u3048\u3067minimum number\u3092\u8a08\u7b97\n    df_tmp = ((sales_train_val>0) * np.tile(np.arange(1,1914),(weight_mat_csr.shape[0],1)))\n    \n    start_no = np.min(np.where(df_tmp==0,9999,df_tmp),axis=1)-1\n    \n    \n    # denominator of RMSSE \/ RMSSE\u306e\u5206\u6bcd\n    weight1 = np.sum((np.diff(sales_train_val,axis=1)**2),axis=1)\/(1913-start_no)\n    \n    # calculate the sales amount for each item\/level\n    df_tmp = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    df_tmp['amount'] = df_tmp['demand'] * df_tmp['sell_price']\n    df_tmp =df_tmp.groupby(['id'])['amount'].apply(np.sum).values\n    \n    weight2 = weight_mat_csr * df_tmp \n\n    weight2 = weight2\/np.sum(weight2)\n    \n    del sales_train_val\n    gc.collect()\n    \n    return weight1, weight2\n\n\nweight1, weight2 = weight_calc(data,product)\n\ndef wrmsse(preds, data):\n    \n    preds = preds.astype('int32')\n    \n    # actual obserbed values \/ \u6b63\u89e3\u30e9\u30d9\u30eb\n    y_true = data.get_label()\n    y_true = y_true.astype('int32')\n    \n    # number of columns\n    num_col = len(y_true)\/\/NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) \/ \u63a8\u8ad6\u306e\u7d50\u679c\u304c 1 \u6b21\u5143\u306e\u914d\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u76f4\u3059\n    reshaped_preds = np.array(preds).reshape(num_col, NUM_ITEMS).T\n    reshaped_true = np.array(y_true).reshape(num_col, NUM_ITEMS).T\n    \n    x_name = ['pred_' + str(i) for i in range(num_col)]\n    x_name2 = [\"act_\" + str(i) for i in range(num_col)]\n          \n    train = np.array(weight_mat_csr*np.c_[reshaped_preds, reshaped_true])\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) \/ weight1) * weight2)\n    \n    return 'wrmsse', score, False\n\ndef wrmsse_simple(preds, data):\n    \n    # actual obserbed values \/ \u6b63\u89e3\u30e9\u30d9\u30eb\n    y_true = data.get_label()\n    \n    # number of columns\n    num_col = len(y_true)\/\/NUM_ITEMS\n    \n    # reshape data to original array((NUM_ITEMS*num_col,1)->(NUM_ITEMS, num_col) ) \/ \u63a8\u8ad6\u306e\u7d50\u679c\u304c 1 \u6b21\u5143\u306e\u914d\u5217\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u76f4\u3059\n    reshaped_preds = np.array(preds).reshape(num_col, NUM_ITEMS).T\n    reshaped_true = np.array(y_true).reshape(num_col, NUM_ITEMS).T\n    \n    train = np.c_[reshaped_preds, reshaped_true]\n    \n    weight2_2 = weight2[:NUM_ITEMS]\n    weight2_2 = weight2_2\/np.sum(weight2_2)\n    \n    score = np.sum(\n                np.sqrt(\n                    np.mean(\n                        np.square(\n                            train[:,:num_col] - train[:,num_col:])\n                        ,axis=1) \/  weight1[:NUM_ITEMS])*weight2_2)\n    \n    return 'wrmsse', score, False","8eef8c74":"# useful dataset for speeding up the following calculations\nuids = pd.concat([pd.Series(np.arange(30490)), pd.Series(x_train['id'].unique())], axis=1)\nuids.columns = ['idx', 'id']\ndemand = x_train.merge(uids, on='id').sort_values(by=['idx', 'date'], axis=0)['demand']\nnum_vals = int(len(demand)\/30490)","7c56d9b4":"# helper function\ndef intervals(ts):\n    y = np.zeros(len(ts))\n    k = 0\n    counter = 0\n    for tmp in range(len(ts)):\n        if(ts[tmp]==0):\n            counter = counter + 1\n        else:\n            k = k + 1\n            y[k] = counter\n            counter = 1\n    y = np.array(y)\n    y[np.isnan(y)] = 1\n    y = y[y > 0]\n    return y","27c1c61a":"# Simple Exponential Smoothing\ndef SES(ts, alpha=0.1, h=56):\n    if ts is None or len(ts)==0:\n        return np.zeros(h)\n    \n    y = np.zeros(len(ts)+1)\n    y[0] = ts[0]\n        \n    for t in range(len(ts)):\n        y[t+1] = alpha*ts[t]+(1-alpha)*y[t]\n  \n    return np.concatenate([y[0:len(ts)], np.repeat(y[-1:],h)])","0c858c52":"# 1. Naive (LB: 1.46378)\ny_pred_Naive = np.tile(x_train[x_train['date'] == '2016-03-27']['demand'], 28)\ny_test_Naive = np.tile(x_train[x_train['date'] == '2016-03-27']['demand'], 28)\nwrmsse_Naive = wrmsse(y_pred_Naive, lgb.Dataset(x_val, y_val))[1]\nprint(f'WRMSSE for Naive method: {wrmsse_Naive}')\ndel y_pred_Naive","d6372cb5":"# 2. Seasonal Naive (LB: 0.86967)\ny_pred_sNaive = np.tile(x_train[(x_train['date'] > '2016-03-20') & (x_train['date'] <= '2016-03-27')]['demand'], 4)\ny_test_sNaive = np.tile(x_train[(x_train['date'] > '2016-03-20') & (x_train['date'] <= '2016-03-27')]['demand'], 4)\nwrmsse_sNaive = wrmsse(y_pred_sNaive, lgb.Dataset(x_val, y_val))[1]\nprint(f'WRMSSE for Seasonal Naive method: {wrmsse_sNaive}')\ndel y_pred_sNaive","c6520546":"# 3. Simple Exponential Smoothing (LB: 1.07202)\ndef mySES(ts):\n    if ts is None or len(ts)==0:\n        ts = np.asarray([0, 0])\n    \n    start_period = np.argmin(ts!=0)\n    ts = ts[start_period:]\n    MSE = {}\n    for alpha in np.arange(0.1,0.3,0.1):\n        MSE[str(alpha)] = np.mean(np.square(SES(ts, alpha)[0:len(ts)] - ts))\n    opt_alpha = float(list(MSE.keys())[np.argmin(MSE.values())])\n    return SES(ts, opt_alpha)[-56:]\n    \n\ny_pred_SES = Parallel(n_jobs=-1)(delayed(mySES)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_SES = np.concatenate(y_pred_SES)\ny_pred_SES = y_pred_SES.reshape([56, 30490], order='F')\ny_test_SES = y_pred_SES[28:56,:].reshape([28*30490,1])\ny_pred_SES = y_pred_SES[0:28,:].reshape([28*30490,1])\ny_test_SES = np.concatenate(y_test_SES)\ny_pred_SES = np.concatenate(y_pred_SES)\nwrmsse_SES = wrmsse(y_pred_SES, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for SES method: {wrmsse_SES}\")\ndel y_pred_SES","805c9ba9":"# 4. Moving Averages (LB: 1.09815)\ndef MA(ts, h=56):\n    mse = np.ones(14)*np.inf\n    for k in np.arange(2,16):\n        y = np.repeat(np.nan, len(ts))\n        for i in np.arange(k+1,len(ts)+1):\n            y[i-1] = np.mean(ts[(i-k-1):i])\n        mse[k-2] = np.mean(np.square(y[~np.isnan(y)]-ts[~np.isnan(y)]))\n        k = np.argmin(mse)+2\n    forecast = np.repeat(np.mean(ts[-k:]), h)\n    return forecast\n\n\ny_pred_MA = Parallel(n_jobs=-1)(delayed(MA)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_MA = np.concatenate(y_pred_MA)\ny_pred_MA = y_pred_MA.reshape([56, 30490], order='F')\ny_test_MA = y_pred_MA[28:56,:].reshape([28*30490,1])\ny_pred_MA = y_pred_MA[0:28,:].reshape([28*30490,1])\ny_test_MA = np.concatenate(y_test_MA)\ny_pred_MA = np.concatenate(y_pred_MA)\nwrmsse_MA = wrmsse(y_pred_MA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for MA method: {wrmsse_MA}\")\ndel y_pred_MA","2898cb15":"# 5. Croston's (LB: 1.05648)\ndef Croston(ts, h=56, alpha=0.1, debias=1.0):\n    yd = np.mean(SES(ts[ts!=0])[-56:])\n    yi = np.mean(SES(intervals(ts))[-56:])\n    return np.repeat(yd\/yi, h)*debias\n\n\ny_pred_Croston = Parallel(n_jobs=-1)(delayed(Croston)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_Croston = np.concatenate(y_pred_Croston)\ny_pred_Croston = y_pred_Croston.reshape([56, 30490], order='F')\ny_test_Croston = y_pred_Croston[28:56,:].reshape([28*30490,1])\ny_pred_Croston = y_pred_Croston[0:28,:].reshape([28*30490,1])\ny_test_Croston = np.concatenate(y_test_Croston)\ny_pred_Croston = np.concatenate(y_pred_Croston)\nwrmsse_Croston = wrmsse(y_pred_Croston, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for Croston's method: {wrmsse_Croston}\")\ndel y_pred_Croston","c04bf7b8":"# 6. Optimized Croston's (LB: 1.05804)\ndef optCroston(ts, h=56, debias=1.0):\n    yd = np.mean(mySES(ts[ts!=0])[-56:])\n    yi = np.mean(mySES(intervals(ts))[-56:])\n    return np.repeat(yd\/yi, h)*debias\n\n\ny_pred_optCroston = Parallel(n_jobs=-1)(delayed(optCroston)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_optCroston = np.concatenate(y_pred_optCroston)\ny_pred_optCroston = y_pred_optCroston.reshape([56, 30490], order='F')\ny_test_optCroston = y_pred_optCroston[28:56,:].reshape([28*30490,1])\ny_pred_optCroston = y_pred_optCroston[0:28,:].reshape([28*30490,1])\ny_test_optCroston = np.concatenate(y_test_optCroston)\ny_pred_optCroston = np.concatenate(y_pred_optCroston)\nwrmsse_optCroston = wrmsse(y_pred_optCroston, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for Optimized Croston's method: {wrmsse_optCroston}\")\ndel y_pred_optCroston","77559a37":"# 7. Syntetos-Boylan Approximation (LB: 1.09166)\ny_pred_SBA = Parallel(n_jobs=-1)(delayed(lambda x: Croston(x, debias=0.95))(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_SBA = np.concatenate(y_pred_SBA)\ny_pred_SBA = y_pred_SBA.reshape([56, 30490], order='F')\ny_test_SBA = y_pred_SBA[28:56,:].reshape([28*30490,1])\ny_pred_SBA = y_pred_SBA[0:28,:].reshape([28*30490,1])\ny_test_SBA = np.concatenate(y_test_SBA)\ny_pred_SBA = np.concatenate(y_pred_SBA)\nwrmsse_SBA = wrmsse(y_pred_SBA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for SBA method: {wrmsse_SBA}\")\ndel y_pred_SBA","a40e90c7":"# 8. Teunter-Syntetos-Babai method (LB: 1.06812)\n# This should be optimized over the smoothing parameters by grid search\ndef TSB(ts,extra_periods=56,alpha=0.4,beta=0.4):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), probability(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n    # Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1\/(1 + first_occurence)\n    f[0] = p[0]*a[0]\n                 \n    # Create all the t+1 forecasts\n    for t in range(0,cols): \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = beta*(1) + (1-beta)*p[t]  \n        else:\n            a[t+1] = a[t]\n            p[t+1] = (1-beta)*p[t]       \n        f[t+1] = p[t+1]*a[t+1]\n        \n    # Future Forecast\n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    #df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return f[-extra_periods:]\n\n\ny_pred_TSB = Parallel(n_jobs=-1)(delayed(TSB)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_TSB = np.concatenate(y_pred_TSB)\ny_pred_TSB = y_pred_TSB.reshape([56, 30490], order='F')\ny_test_TSB = y_pred_TSB[28:56,:].reshape([28*30490,1])\ny_pred_TSB = y_pred_TSB[0:28,:].reshape([28*30490,1])\ny_test_TSB = np.concatenate(y_test_TSB)\ny_pred_TSB = np.concatenate(y_pred_TSB)\nwrmsse_TSB = wrmsse(y_pred_TSB, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for TSB method: {wrmsse_TSB}\")\ndel y_pred_TSB","4cdba645":"# 9. Aggregate-Disaggregate Intermittent Demand Approach (LB: 1.07268)\ndef ADIDA(ts, h=56):\n    a1 = np.ceil(np.mean(intervals(ts)))\n    idx = [0 if (np.isnan(a1)) or (a1==0) else -int((len(ts) \/\/ int(a1)) * a1)][0]\n    if np.isnan(a1):\n        a1 = 1\n    agg_ser = pd.Series(ts[idx:]).rolling(int(a1)).sum()\n    agg_ser = agg_ser[np.arange(0, len(agg_ser), int(a1))] # non-overlapping\n    agg_ser[np.isnan(agg_ser)] = np.mean(agg_ser)\n    forecast = mySES(agg_ser.values)[-56:]\/a1\n    return forecast\n\n\ny_pred_ADIDA = Parallel(n_jobs=-1)(delayed(ADIDA)(\n        demand[i*num_vals:(i+1)*num_vals].values\n    ) for i in tqdm(range(30490)))\ny_pred_ADIDA = np.concatenate(y_pred_ADIDA)\ny_pred_ADIDA = y_pred_ADIDA.reshape([56, 30490], order='F')\ny_test_ADIDA = y_pred_ADIDA[28:56,:].reshape([28*30490,1])\ny_pred_ADIDA = y_pred_ADIDA[0:28,:].reshape([28*30490,1])\ny_test_ADIDA = np.concatenate(y_test_ADIDA)\ny_pred_ADIDA = np.concatenate(y_pred_ADIDA)\nwrmsse_ADIDA = wrmsse(y_pred_ADIDA, lgb.Dataset(x_val, y_val))[1]\nprint(f\"WRMSSE for ADIDA method: {wrmsse_ADIDA}\")\ndel y_pred_ADIDA\n","10e18e70":"# stochastic ensemble\ndel demand, x_train, y_train, x_val, y_val \ngc.collect()\n\nprobs = 1.0\/np.array([wrmsse_Naive, wrmsse_sNaive, wrmsse_SES, wrmsse_MA, \n                    wrmsse_Croston, wrmsse_optCroston, wrmsse_SBA, wrmsse_TSB, wrmsse_ADIDA]) \/ \\\n    sum(1.0\/np.array([wrmsse_Naive, wrmsse_sNaive, wrmsse_SES, wrmsse_MA, \n                    wrmsse_Croston, wrmsse_optCroston, wrmsse_SBA, wrmsse_TSB, wrmsse_ADIDA]))\nprobs = np.cumsum(probs)\n\nrs = [random.random() for i in range(30490*28)]\ndef f(rs): \n    return np.argmax(rs<probs)\nrcols = np.vectorize(f)(rs)\n\ntest['demand'] = np.select([rcols==0, rcols==1, rcols==2, rcols==3, rcols==4, rcols==5, rcols==6, rcols==7, rcols==8], \n         [y_test_Naive, y_test_sNaive, y_test_SES, y_test_MA, y_test_Croston, y_test_optCroston, y_test_SBA, y_test_TSB, y_test_ADIDA])\ntest.fillna(0, inplace=True)\n\npredictions = test[['id', 'date', 'demand']]\npredictions = predictions.pivot(index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\nevaluation_rows = [row for row in submission['id'] if 'evaluation' in row]\nevaluation = submission[submission['id'].isin(evaluation_rows)]\n\nvalidation = submission[['id']].merge(predictions, on = 'id')\nfinal = pd.concat([validation, evaluation])\nfinal.to_csv('submission.csv', index = False)\n\n","d27b22a9":"\u4e88\u6e2c\u90e8\u5206\u306esubmission file\u3092\u540c\u3058\u304fmelt\u51e6\u7406\u3057\u3001sales_train_val\u3068\u3064\u306a\u3052\u308b\u3002  \n\u51e6\u7406\u306e\u6ce8\u610f\u70b9:  \n* submission file\u306e\u5217\u540d\u3092\"d_xx\"\u5f62\u5f0f\u306b\u5909\u66f4\u3059\u308b. submission file\u3067\u7e26\u306b\u7d50\u5408\u3055\u308c\u305fvalidation\u3068evaluation\u3092\u4e00\u5ea6\u5206\u5272\u3057\u3001\u305d\u308c\u305e\u308c\u3053\u3068\u306a\u308b28\u65e5\u9593\u306e\u5217\u540d\"d_xx\"\u3092\u305d\u308c\u305e\u308c\u4ed8\u4e0e\u3002\n* submission file\u306b\u306f, id\u306e\u8a73\u7d30\uff08item, department, state\u7b49\uff09\u304c\u7121\u3044\u305f\u3081id\u3092\u30ad\u30fc\u306b, sales validation\u304b\u3089\u53d6\u5f97\u3057\u305fproduct\u3092\u7d50\u5408\n* test2\u306f\u30016\/1\u307e\u3067\u4e0d\u8981\u306a\u305f\u3081\u524a\u9664","617b1174":"## train\/test\u306e\u5206\u5272\u3068model\u306e\u63a8\u5b9a","716090ea":"### 3.1\u3068\u540c\u69d8\u306b\u4e88\u6e2c\u90e8\u5206(validation\/evaluation\u90e8\u5206)\u306emelt\u51e6\u7406\u3057, \u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u7d50\u5408\u3059\u308b. \u51fa\u529b\u306fdata\u3068\u3044\u3046\u5909\u6570.","6648ebfd":"### 5 data\u304b\u3089\u7279\u5fb4\u91cf\u751f\u6210\n* groupby & transofrm\u306e\u5909\u63db\u65b9\u6cd5\u306f\u3053\u3061\u3089\u3092\u53c2\u7167:https:\/\/qiita.com\/greenteabiscuit\/items\/132e0f9b1479926e07e0\n* shift\/rolling\u306a\u3069\u306e\u5f79\u5272\u306f\u3053\u3061\u3089\u3092\u53c2\u7167:https:\/\/note.nkmk.me\/python-pandas-rolling\/ (\u3053\u3053\u3067melt\u304c\u3046\u307e\u304f\u52b9\u304d\u307e\u3059\u3002)\n\u3000\u30e9\u30b0\u5909\u6570\u3084\u904e\u53bb\u306e\u5e73\u5747\u5024\u306a\u3069\u306e\u7279\u5fb4\u91cf\u304c\u751f\u6210\u3067\u304d\u308b\u3002\n* \u5909\u6570\u306f, \u3059\u3079\u3066lag\u309228\u4ee5\u4e0a\u306b\u3057\u3066, F1~F28\u306e\u4e88\u6e2c\u30921\u3064\u306e\u30e2\u30c7\u30eb\u3067\u8868\u73fe\u3059\u308b\u306e\u304c\u76ee\u7684\u3002\n* TODO\uff1a\u7279\u5fb4\u91cf\u306e\u751f\u6210\u65b9\u6cd5\u306f\u8272\u3005\u5909\u66f4\u53ef\u80fd. Shift\u3084Rolling\u306e\u5024\u306e\u5909\u66f4\u306a\u3069\u306a\u3069","da03f5d9":"read_data\u306f\u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\u3068, reduce_mem_usage\u306e\u9069\u7528\u3092\u884c\u3046\u95a2\u6570","e12d6ecf":"pandas\u306emelt\u3092\u4f7f\u3044demand(\u58f2\u4e0a\u6570\u91cf)\u3092\u7e26\u306b\u4e26\u3079\u308b.  \n* pandas\u306emelt\u306f https:\/\/qiita.com\/ishida330\/items\/922caa7acb73c1540e28\u3000\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002\n* data\u306e\u884c\u6570\u304c\u83ab\u5927\u306b\u306a\u308b\u306e\u3067, Kaggle Notebook\u306ememory\u5236\u9650\u3092\u8003\u616e\u3057\u3001nrows\u3067\u76f4\u8fd1365*2\u65e5\u5206\uff082\u5e74\u5206\uff09\u306e\u30c7\u30fc\u30bf\u306b\u9650\u5b9a\uff08TODO:\u74b0\u5883\u306b\u5fdc\u3058\u3066\u671f\u9593\u3092\u5909\u66f4\uff09","a94f434b":"## data\u52a0\u5de5 ","02822200":"### 1.\u6700\u521d\u306b\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u51e6\u7406","e55f3871":"### 2.sales_train_validation\u306emelt\u51e6\u7406  \n\uff08\u6642\u7cfb\u5217\u306e\u7279\u5fb4\u91cf\u304c\u4f5c\u308a\u3084\u3059\u3044\u3088\u3046\u306b, id\u6bce\u306b\u6a2a\u306b\u4e26\u3093\u3060\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u3092\u3001\uff08id , \u6642\u7cfb\u5217\uff09\u3067\u7e26\u306b\u5909\u63db\uff09","54fe370a":"As [@kaushal2896](https:\/\/www.kaggle.com\/kaushal2896) suggested in [this comment](https:\/\/www.kaggle.com\/harupy\/m5-baseline#770558), encode the categorical columns before merging to prevent the notebook from crashing even with the full dataset. [@harupy](https:\/\/www.kaggle.com\/harupy) also use this encoding suggested in [m5-baseline](https:\/\/www.kaggle.com\/harupy\/m5-baseline).  \n\u30e1\u30e2\u30ea\u306e\u52b9\u7387\u5229\u7528\u306e\u305f\u3081, \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u3092\u3042\u3089\u304b\u3058\u3081Label encoding.","da47fd56":"## WRMSSE calculation","2e23fd2f":"reduce_mem_usage\u306f\u3001\u30c7\u30fc\u30bf\u306e\u30e1\u30e2\u30ea\u3092\u6e1b\u3089\u3059\u305f\u3081\u306b\u30c7\u30fc\u30bf\u578b\u3092\u5909\u66f4\u3059\u308b\u95a2\u6570\u3067\u3059\u3002  https:\/\/qiita.com\/hiroyuki_kageyama\/items\/02865616811022f79754\u3000\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002","a794b113":"### 4.data\u306bcalendar\/sell_prices\u3092\u7d50\u5408","69c44702":"LightGBM\u306eMetric\u3068\u3057\u3066, WRMSSE\u306e\u52b9\u7387\u7684\u306a\u8a08\u7b97\u3092\u884c\u3046\u3002\u3042\u304f\u307e\u3067, 28day-lag\u30671\u3064\u306e\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u3059\u308b\u3068\u304d\u306bLGBM\u3067\u52b9\u7387\u7684\u306aWRMSSE\u306e\u8a08\u7b97\u3092\u884c\u3046\u5834\u5408\u3067\u3042\u308b\u3002\n* weight_mat\u3068\u3044\u30460 or 1\u306e\u758e\u884c\u5217\u3067\u3001\u52b9\u7387\u7684\u306baggregation level\u3092\u884c\u5217\u7a4d\u3067\u8a08\u7b97\u51fa\u6765\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u308b","6a04d20c":"To be continued ...","9dcec814":"Pandas\u306edataFrame\u3092\u304d\u308c\u3044\u306b\u8868\u793a\u3059\u308b\u95a2\u6570","f611ca38":"## function\u306e\u5b9a\u7fa9","1afa2934":"Implementation of the statistical benchamark forecasting techniques from the [M5 Participants Guide](https:\/\/mofc.unic.ac.cy\/m5-competition\/).\n\nThis kernel is based on [for_Japanese_beginner(with WRMSSE in LGBM))](https:\/\/www.kaggle.com\/girmdshinsei\/for-japanese-beginner-with-wrmsse-in-lgbm). Thanks [@Tsuru](https:\/\/www.kaggle.com\/girmdshinsei).  \n\nFinal submission is a stochastic ensemble of the statistical benchmarks. Contains some parallelization.","0b027358":"## module import"}}