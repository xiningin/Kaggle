{"cell_type":{"b4082e26":"code","adbff01f":"code","b0bdfa3f":"code","64ac9d65":"code","cb39a0ee":"code","3c750798":"code","f19d79ab":"code","f67b41e0":"code","f8a811c8":"code","254af962":"code","63e69cbe":"code","6c7e9e96":"code","47ecf812":"code","33e67eab":"code","cfc82fc0":"code","a10e6ea7":"code","f623c1ee":"code","793dc52a":"code","fe070ac4":"code","95862245":"code","3805b38b":"code","89a9e1e6":"code","8232a120":"code","638d722f":"code","0bf08549":"code","a7290ad1":"code","43647db2":"code","06e37401":"markdown","a8171b1f":"markdown","69f87e92":"markdown","01c847c6":"markdown","a5ff95d5":"markdown","8b3df123":"markdown","47bc2f8b":"markdown","2054198b":"markdown","2226dffa":"markdown","f8735f27":"markdown","3bab77ac":"markdown","8b6fe8b7":"markdown","6c2d7763":"markdown","0a382e8a":"markdown","f9a37642":"markdown","b98d155f":"markdown","a31e693e":"markdown","5d5cb9cd":"markdown","cfa8dd5a":"markdown","4fc4b87c":"markdown","35c1d0aa":"markdown","ceb91ca8":"markdown","5421aea5":"markdown","89ed6186":"markdown","3621b60a":"markdown","f9f752c1":"markdown","be5f233e":"markdown","e16f7c8c":"markdown","66c4af5e":"markdown","3979a3b8":"markdown","8e6b6bbb":"markdown","5f71cbff":"markdown","49f71d61":"markdown","f29e6fa9":"markdown"},"source":{"b4082e26":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport tensorflow_datasets as tfds\nimport pathlib\nimport os\nfrom sklearn.model_selection import train_test_split","adbff01f":"data_path = pathlib.Path(r\"..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset\")","b0bdfa3f":"#list of pathes for all images \nall_images = list(data_path.glob(r'*\/*\/*.jpg')) + list(data_path.glob(r'*\/*\/*.png'))\n\nimages = []\nlabels = []\n\n#looping through the pathes to extract pathes and labels\nfor item in all_images:\n    \n    path = os.path.normpath(item)\n    splits = path.split(os.sep)\n    \n    if 'GT' not in splits[-2]:\n    \n        images.append(item)\n    \n        label = splits[-2]\n        labels.append(label)","64ac9d65":"# Dataframe with two columns: image_path, label \nimage_pathes = pd.Series(images).astype(str)\nlabels = pd.Series(labels)\n\ndataframe = pd.concat([image_pathes, labels], axis=1)\n\ndataframe.columns = ['images', 'labels']\n\ndataframe.head()","cb39a0ee":"fig, axes = plt.subplots(nrows=3, ncols=5, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(dataframe.images[i]))\n    ax.set_title(dataframe.labels[i])\n    \nplt.show()","3c750798":"#Shuffle the dataframe rows and split it to train, val, test splits","f19d79ab":"shuffled_dataframe = dataframe.sample(frac = 1)","f67b41e0":"all_train, test = train_test_split(shuffled_dataframe, test_size=0.15, random_state=0)\ntrain, val = train_test_split(all_train, test_size=0.17, random_state=0)","f8a811c8":"training_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0,\n                                                                   rotation_range=40,\n                                                                   zoom_range=0.2,\n                                                                   width_shift_range=0.2,\n                                                                   height_shift_range=0.2,\n                                                                   shear_range=0.2,\n                                                                   horizontal_flip=True,\n                                                                   vertical_flip=True)\n\ntraining_generator = training_data_gen.flow_from_dataframe(dataframe=train,\n                                                           x_col='images', y_col='labels',\n                                                           target_size=(224, 224),\n                                                           color_mode='rgb',\n                                                           class_mode='categorical',\n                                                           batch_size=64)\n#------------------------------------------------------------------------------------------------------\nval_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\nvalidation_generator = val_data_gen.flow_from_dataframe(dataframe=val,\n                                                        x_col='images', y_col='labels',\n                                                        target_size=(224, 224),\n                                                        color_mode='rgb',\n                                                        class_mode='categorical',\n                                                        batch_size=64)\n#------------------------------------------------------------------------------------------------------\ntest_data_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1\/255.0)\ntest_generator = test_data_gen.flow_from_dataframe(dataframe=test,\n                                                   x_col='images', y_col='labels',\n                                                   target_size=(224, 224),\n                                                   color_mode='rgb',\n                                                   class_mode='categorical',\n                                                  batch_size=64)","254af962":"mlp_model = tf.keras.models.Sequential()\n\n#Flatten layer\nmlp_model.add(tf.keras.layers.Flatten(input_shape=(224, 224, 3)))\n\n# 3 Hidden Layers with (256, 256, 128) neurons and relu activation function\nmlp_model.add(tf.keras.layers.Dense(256, activation='relu'))\n# dropout layer to reduce the overfitting \nmlp_model.add(tf.keras.layers.Dropout(0.4))\nmlp_model.add(tf.keras.layers.Dense(256, activation='relu'))\nmlp_model.add(tf.keras.layers.Dense(128, activation='relu'))\n\n# output layer with 9 neurons and softmax activation function\nmlp_model.add(tf.keras.layers.Dense(9, activation = 'softmax'))","63e69cbe":"# we can se the network using: \n\ntf.keras.utils.plot_model(mlp_model,\n                          show_shapes=True,\n                          show_dtype=True,\n                          show_layer_names=True)","6c7e9e96":"mlp_model.summary()","47ecf812":"mlp_model.compile(loss='categorical_crossentropy',\n                 optimizer='rmsprop',\n                 metrics=['acc'])","33e67eab":"mlp_model.fit(training_generator, \n             steps_per_epoch=24, \n             validation_data=validation_generator,\n             validation_steps=20,\n             epochs=5)","cfc82fc0":"from IPython.display import Image\nImage(url='https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRwed5zvnSDt0zrFd_gf-kUIMoF7Nm6FXIwDw&usqp=CAU',\n      width=750,\n      height=500)","a10e6ea7":"Image(url='https:\/\/www.researchgate.net\/profile\/Lavender-Jiang-2\/publication\/343441194\/figure\/fig2\/AS:921001202311168@1596595206463\/Basic-CNN-architecture-and-kernel-A-typical-CNN-consists-of-several-component-types.ppm',\n      width=750,\n      height=500)","f623c1ee":"Image(url='https:\/\/i.stack.imgur.com\/CQtHP.gif',\n      width=750,\n      height=500)","793dc52a":"Image(url='https:\/\/miro.medium.com\/max\/658\/0*jLoqqFsO-52KHTn9.gif',\n      width=750,\n      height=500)\n\n#Source: https:\/\/towardsdatascience.com\/cnn-part-i-9ec412a14cb1","fe070ac4":"Image(url='https:\/\/res.cloudinary.com\/practicaldev\/image\/fetch\/s--nUoflRuG--\/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880\/https:\/\/i.ibb.co\/kG5vPdn\/final-cnn.png',\n      width=750,\n      height=500)\n\n#Source: https:\/\/towardsdatascience.com\/cnn-part-i-9ec412a14cb1","95862245":"Image(url='https:\/\/nico-curti.github.io\/NumPyNet\/NumPyNet\/images\/maxpool.gif',\n      width=750,\n      height=500)","3805b38b":"# Building the model \ncnn_model = tf.keras.models.Sequential()\n\n#----------------------------------------------------------------------------------------------\n\n#Conv layer: 32 filters of size (3, 3), withstrides = 1 and relu activation\ncnn_model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides=1, \n                                     activation='relu', input_shape=(224,224,3)))\n#max-poolig layer with pool_size of (2,2)\ncnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#----------------------------------------------------------------------------------------------\n\n#Conv layer: 64 filters of size (3, 3), withstrides = 1 and relu activation\ncnn_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1, \n                                    activation='relu'))\n#max-poolig layer with pool_size of (2,2)\ncnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#----------------------------------------------------------------------------------------------\n\n#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\ncnn_model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=1, \n                                    activation='relu'))\n#max-poolig layer with pool_size of (2,2)\ncnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#----------------------------------------------------------------------------------------------\n\n#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\ncnn_model.add(tf.keras.layers.Conv2D(128, kernel_size=(3, 3), strides=1, \n                                    activation='relu'))\n#max-poolig layer with pool_size of (2,2)\ncnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#----------------------------------------------------------------------------------------------\n\n#Conv layer: 128 filters of size (3, 3), withstrides = 1 and relu activation\ncnn_model.add(tf.keras.layers.Conv2D(256, kernel_size=(3, 3), strides=1, \n                                    activation='relu'))\n#max-poolig layer with pool_size of (2,2)\ncnn_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n\n#----------------------------------------------------------------------------------------------\n#Flattening the output of the last pooling layer \ncnn_model.add(tf.keras.layers.Flatten())\n\n#cnn_model.add(tf.keras.layers.GlobalAveragePooling2D())\n\n\n#Fully connected layer with 256 units and relu activation\ncnn_model.add(tf.keras.layers.Dense(256, activation='relu'))\n\n#Dropout layer to lower the overfitting with dropuot rate of rate 0.4\ncnn_model.add(tf.keras.layers.Dropout(0.4))\n\n#Fully connected layer with 9 units and softmax activation\ncnn_model.add(tf.keras.layers.Dense(9, activation = 'softmax'))","89a9e1e6":"cnn_model.summary()","8232a120":"cnn_model.compile(loss='categorical_crossentropy',\n                 optimizer='rmsprop',\n                 metrics=['acc'])","638d722f":"history = cnn_model.fit(training_generator, \n                         steps_per_epoch=99, \n                         validation_data=validation_generator,\n                         validation_steps=20,\n                         epochs=25)","0bf08549":"cnn_train_loss = history.history['loss']\ncnn_val_loss = history.history['val_loss']\n\n\nplt.plot(history.epoch, cnn_train_loss, label='Training Loss')\nplt.plot(history.epoch, cnn_val_loss, label='Validation Loss')\nplt.grid(True)\nplt.legend()","a7290ad1":"train_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nplt.plot(history.epoch, train_acc, label='Training Accuracy')\nplt.plot(history.epoch, val_acc, label='Validation Accuracy')\nplt.grid(True)\nplt.legend()","43647db2":"cnn_model.evaluate(test_generator)","06e37401":"Compile the model you can learn more about loss function and optimization at: <a href=\"https:\/\/www.kaggle.com\/general\/265294\">Loss Function & Optimization.<\/a>","a8171b1f":"Hello everyone and welcome. \n\nIn this notebook I will explain the convolutional neural networks, how it's work and why we use it in computer vision problems \ninstead of using the traditional Multi-Layer Perceptron (MLP). \n\nFirst we will try to classify the image using traditional MLP (The results will be bad \u2639\ufe0f) and explain why the results was \nbas when we use the MLP in image classification. \n\nNext we will see the components of Convolutional Neural Network CNN and see how it works and why it works better than the MLP\nin image classification problems. \n\nFinally we will see an example of CNN and compaire its result the the MLP. ","69f87e92":"**Next we will use Convolutional neural networks to train the classifier**","01c847c6":"**Strides and Padding**","a5ff95d5":"**INPUT   >   CONV   >   POOL   >   CONV   >   POOL   >   CONV   >   POOL >  FC >   FC (softmax)**","8b3df123":"As we can see the number of parameters is reduced from 38,635,273 in the MLP to 2,066,313 in CNN. reducing the parameters to 5.2% of the number of parameters in the MLP !!!","47bc2f8b":"Filters are parameters that learned during the training process. some filters can detect vertical edges while other might \ndetect horizontal edges and other detect different things. \n\nso we can see that each filter produce its own feature map, the conv layer has many of those filters so the number of filters\nin the conv layer determine the number of feature maps that produced after applying this layer on the input. And the number \nof filters represent the depth of the output of the layer. As we increase the number of filters, the complexity of the network\nincrease and enable use to detect more complex features. \n\n\nThis kernel is just a matrix of weights that  are learned during the training process this filter is works by sliding over\nthe image to extract features. kernels are almost always squares and can have different sizes (3x3), (5x5), (7x7) it is an\nhyperparameter that you can tune and the performance will be different based on the problem that you are solving. ","2054198b":"We will first discuss the use of MLP in image classification problems. \n\n- MLP consists of **Input Layer** and the input must be 1D vector so the first thing we shoud do is to flatten the input images form 2D matrix to 1D vector to train the network. We will use keras Flatten layer that takes 2D image matrix and convert it  into a 1D vector. <br>\n\n\n- The second thing we add is the **hidden layers**, you can add as much as you want, but generally it depends on the complexity of the data and the task you have, and we eill choose the activation function, Relu function performs the best in Hidden layers. <br> \n\n\n- Finally the **output layer**. The number of neurons is equal to the number of classes we have, and the activation function  is Softmax since we have a multi-class classification  problem. <br>","2226dffa":"We can see the Trainable params which they are the parameters that will be adjusted and learned during the training process. <br>\n\n\n- The first hidden layer has (224 * 224*3 nodes in the input layer) * (256 nodes in this hidden layer) + 256 bias terms = 38535424 learnable parameters. <br>\n\n\n- The second Hidden layer has (256 nodes in the previous hidden layer) * (256 nodes in this hidden layer) + 256 bias terms =  **65792 learnable parameters.** <br>\n\n\n- The last hidden layer has (256 nodes in the previous hidden layer) * (128 nodes in this hidden layer) + 128 bias terms =  **32896 learnable parameters.**  <br>\n\n\n- Finally the output layer has (128 nodes in the previous hidden layer) * (9 nodes in the output layer) + 9 bias terms =  **1161 learnable parameters.** <br>\n\n\nThe total number of Trainable params: **24,983,305** and this is a huge number \ud83d\udc94 to such small neural network. What will happen if we add more and more layers or we resize the images to larger sizes, there will be tens of millions of parameters. ","f8735f27":"As we can see we get a much better accuracy when we use the convolutional network \n\nwe can get much better accuracy when we use the transfer learning which will be the nect topic to discuss","3bab77ac":"Convolutional Layer is the basis of the Convolutional Networks, the units here is the **filters** and also called kernels and they works by sliding the filter over the input image, apply some processing at each location and store the result in the new processed image as we see in the figure below. \n\nIn CNN these filters are the weights, their values are intialized randomly and learned during the training process. The area of the image that the filter process on is called receptive field. \n\n\nThe math here is very simple, at each location the filter stop we multiply each pixel value in the receptive field by the corresponding pixel in the filter and then sum them to get the value of the center pixel in the new image as we see in the figure below. \n\nIf the images are colored lets say with size of (520, 520, 3) where 3 is the number of channels (Red, Green and Blue (RGB)) the filter also will be (3, 3, 3) where the last 3 is number of channels in the input image. We have filter of size (3,3) for each channel in the input image and for each channel we do the same calculation then add the results of the filter processing on the 3 channels. \n\nIn sime application colors are very important to identify objects. ","8b6fe8b7":"You can learn more about MLP at: \n    \n- <a href=\"https:\/\/www.kaggle.com\/general\/265058\">Multilayer Perceptron (MLP).<\/a>  <br>\n\n\n- <a href=\"https:\/\/www.kaggle.com\/general\/265073\">summary for Multilayer Perceptron (MLP) and backpropagation algorithm.<\/a> <br>\n\n\n- <a href=\"https:\/\/www.kaggle.com\/general\/265279\">Activation function, why do we need activation function in neural networks.<\/a> <br>\n\n\n- <a href=\"https:\/\/www.kaggle.com\/general\/265294\">Loss Function & Optimization.<\/a>","6c2d7763":"In pooling we also have a window with specified size that sliding on the feature map but without weights, the window take the \nmax pixel value and ignore the rest values in max pooling, or take the average of the values in average pooling, so the pooling\nlayer help us to keep the important information and pass them to the next layer. \n\n**Where we put the pooling layers in the CNN?**\n\nusually pooling layer are placed after one or two convolutional  layers.\n\nthe output of the final pooling layer after a sequence  of conv and pool layers will be for example (7, 7, 120) where the 120 id the depth (number of feature maps) now this output is ready for classification but first we need to flatten it then feeding it to the fully connected layers. \n\nPooling layers don't  have any parameters to train. ","0a382e8a":"## 3.2- Convolutional Layers","f9a37642":"So **to summarize** what we learn until now, we have an input image that passed directly to the network without flattening it, the the first part (the feature extractor) extract features that represent the input image and give us what we call it a feature map , and finally this feature map to the classifier that consists of Fully connected layers that perform the classification. ","b98d155f":"## 3.5 Image classification using CNN","a31e693e":"## 3.1 Introduction","5d5cb9cd":"Example: if the input image is 224,224,3 to a Conv layer with 64 filters of size (3,3), stride of 1, and no padding ('valid'). The output will be of size {(224 + 0 -3 \/1) +1 = 222} (222, 222, 64) where the 3 in the input image size is the RGB channels and the 64 in the output means 64 feature map produced by the 64 filters. ","cfa8dd5a":"The network consist of 2 main parts: \n\n1- Feature extraction part <br>\n2- classification part <br>\n\n- The image is passed to the CNN layers to extract features that called **feature map**, through this process the image dimension will shrinks and the number of feature  maps will increase. \n\n- The output of the final layer in the CNN is the feature that was extracted form the image and it represent the image and also this is what we are going to classify. Flatten layer used to flatten the feature map and then pass it to a fully connected layer for classification. \n\n- The earlier layers in the CNN will learn basic features such as edges and the later layers will learn more complex features such as a wheel of a car or the nose of a cat. ","4fc4b87c":"# 1- Input pipeline","35c1d0aa":"The Convolutional Neural Networks (CNN) has an input layer and a stack of hidden layers then the output layer just like the Fully Connected Network, the weights are randomly inialized, activation function is applied, we use back propagation and optimization algorithms to update the weights. The CNN does not need FLatten layer, it takes the whole image matrix as an input. ","ceb91ca8":"**In the next notebook I will explain the transfer learning in computer vision problems.**\n\n\n**Thank you for reading, I hope you enjoyed and benefited from it.**\n\n**If you have any questions or notes please leave it in the commont section.**\n\n**If you like this notebook please press upvote and thanks again.**","5421aea5":"We will loss the spatial features of the image  when we flattening the image to 2D vector, we will loss \na lot of information and the network does not relate the pixel values to each other when it is trying to find patterns \nthats why we get a very bad accuracy when we use MLP in such problem. \n\n**Why??** \n\n\n**loss of information** \ud83e\udd15\n\nwhen we Flatten the image to be a 1D vector, the pixel values that present the fish \ud83d\udc1f will be distributed in a certain way \nin the vector lets say in the left side of the image, if we have a new image that has the same object but in different \nlocation in the image, the neural network will not recognize it because different neurons need to fires in order to recognize \nthe fish, the neural network will have no idea that this is the same fish. But why it does better than that on the MNIST data set, because MNIST data are well prepared  for this task. The MLP will not learn the fish shape. \n\n\n**very large number of parameters** \ud83e\udd15\n\nAnother problem with the MLP is that it is an Fully connected layer, where every node in the layer is connected to all nodes \nof the previous layer and all nodes in the next layer. You saw that with this simple network we have more that 24 million parameters to learn, with more complex network and larger image size we will end up with billions of parameters to train and it is very computationally  expensive. ","89ed6186":"## 3.3- Pooling Layers","3621b60a":"So **to summarize** what we learn until now, the first component  of the CNN is the Conv layer, in this layer we have a lot of hyperparameters that we can tune to control the output of it. The Conv layers contain filters (kernels) that contain the weights that learned during training, these filters works by sliding them over the image and each filter detect a certain  feature, where the filters in the first layers detect simple things like edges and when we go deeper in the network, the filters will detect more complex features. Each filter in the layer produce a feature map and the feature maps of all filters in the layer are concatenated and this is the output of the layer. to control the shape of th output of the layer we have two additional hyperparameters, the stride which control the amount by which the filter slides  over the image and the padding where we add zeros around the border of the image. ","f9f752c1":"**Convolutional Neural Networks (CNN)** are locally connected, each node is connected to amall subset of the previous units.\nAnd it can learn the object features such as a car wheel or a cat nose. ","be5f233e":"As we can see from the figure above the conv operation output is smaller that the input, we can control the shape \nof the output of this operation  by the **stride and padding**\n\nStride is the amount by which the filter slides  over the image, for example if stride = 1 then the filter will slide one pixel at a time, and if stride = 2 it will slide 2 pixel at a time. \n\n\npadding: We add zeros around the border of the image to preserve the spatial size of the image so we can train a deeper network and prevent losing  information from the edges of the image. \n\nthe values for padding is either  2: 'same' where we add zeros around the image border in which the size of the output image is the same as the input image. 2: 'valid' which means without padding. ","e16f7c8c":"# 3- Image classification using Convolutional Neural Networks (CNN)","66c4af5e":"So as I saied the network consists of 2 main parts: the CNN and the FCN \n    \nThe CNN consist of **two main components:**\n    \n    1- Convolutional Layer (CONV)\n    2- Pooling Layer  (POOL)","3979a3b8":"# 2- Image classification using Multi Layer Perceptron (MLP)","8e6b6bbb":"## 3.4- Fully Conected Layers","5f71cbff":"So the full architecture consists of Convolutional Layers , Pooling Layers and Fully Connected Layers. ","49f71d61":"We pass the image through our feature extractor that composed of convolutional and pooling layer and give us the feature \nthat we will pass through the FCL for classification. \n\nin This stage, we flatten the output of the feature extractor lets say for example its (7, 7, 40) wen we flattening it we get a vector\nof size 7*7*40 = 1960, then we feed this vector to a fully connected  layers (we can use for example one layer with 256 units\nwith relu activation function and other layer with 9 units with softmax activation function for classification)\n\nthe number of units in the last layer will equal to the number of classes where each node represents  the probability of each class.","f29e6fa9":"As we go deeper in the ConvNet the number of the feature maps (depth) is increase which lead to the increase to the number \nof parameters that will increase the computational  power and memory needed. \n\nPooling layer reduce the size of the feature map (not the depth it reduce the height and the width) with two types of pooling\nmax pooling and Average pooling. "}}