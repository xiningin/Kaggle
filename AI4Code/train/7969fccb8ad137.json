{"cell_type":{"82e8528e":"code","c4950ada":"code","f4371f73":"code","767380ec":"code","408f378e":"code","95ef2d19":"code","53a97279":"code","7ab503ab":"code","dcf24648":"code","aa1c4e5c":"code","bbc319df":"code","4c341b52":"code","00f158fb":"code","62614831":"code","0a761c10":"code","fa0270d6":"code","5ec0a5d0":"markdown","ad76d38e":"markdown","a2a54726":"markdown","6cb626fd":"markdown","46efebe0":"markdown","c7d197e6":"markdown","0be730c8":"markdown","913d8c8f":"markdown","ec3b01f6":"markdown","fdc3f0df":"markdown","b88ff165":"markdown","2f6b84fb":"markdown","41916419":"markdown","f53818f1":"markdown","fb7193bb":"markdown","94d13974":"markdown","4962b4c1":"markdown","8e12d766":"markdown","9822cd6e":"markdown","8d30b5d5":"markdown","5e84afda":"markdown"},"source":{"82e8528e":"# Load Libraries\n\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import libraries for data transformation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n\n# Import classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn import metrics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc\n\n\n\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nprint(os.listdir(\"..\/input\"))\n","c4950ada":"df  = pd.read_csv('..\/input\/mushrooms.csv')\ndf.head()","f4371f73":"df.describe()","767380ec":"df['class'].value_counts(normalize = True) # Check Class Balance","408f378e":"analysis = [df.shape, df.columns, df.info(), df.isnull().sum()]\nfor j in analysis:\n    print(j,'\\n----------------------------------------------------------------------------\\n')","95ef2d19":"fig, axes = plt.subplots(nrows=5, ncols=4, figsize=(25, 25),sharey=True)\nfig.subplots_adjust(hspace=1.2, wspace=0.6)\n\n\nfor ax, col in zip(axes[0], df.iloc[:,1:].columns):\n    pd.crosstab(df[col], df['class']).plot.bar(stacked=True, ax = ax)\n    plt.xlabel(col, fontsize=18)\n    #ax.set_title(col)\nfor ax, col in zip(axes[1], df.iloc[:,5:].columns):\n    pd.crosstab(df[col], df['class']).plot.bar(stacked=True, ax = ax)\n    plt.xlabel(col, fontsize=18)\n    #ax.set_title(col)\nfor ax, col in zip(axes[2], df.iloc[:,9:].columns):\n    pd.crosstab(df[col], df['class']).plot.bar(stacked=True, ax = ax)\n    plt.xlabel(col, fontsize=18)\n    #ax.set_title(col)\nfor ax, col in zip(axes[3], df.iloc[:,13:].columns):\n    pd.crosstab(df[col], df['class']).plot.bar(stacked=True, ax = ax)\n    plt.xlabel(col, fontsize=18)\n    #ax.set_title(col)\nfor ax, col in zip(axes[4], df.iloc[:,17:].columns):\n    pd.crosstab(df[col], df['class']).plot.bar(stacked=True, ax = ax)\n    plt.xlabel(col, fontsize=18)\n    #ax.set_title(col)\n\n\n#handles, labels = ax.get_legend_handles_labels()\n#fig.legend(handles, labels, loc='upper center')\nplt.tight_layout()\n","53a97279":"df.groupby(['gill-attachment','ring-number','veil-color'])['class'].value_counts(normalize=True).unstack()","7ab503ab":"data = df.drop(['gill-attachment','ring-number','veil-color','veil-type'], axis=1)\ndata.shape","dcf24648":"y = data.iloc[:,0]\ndata_X = data.drop('class', axis=1)","aa1c4e5c":"# Label Encoding\nle=LabelEncoder()\ny=le.fit_transform(y)\n\n#One-Hot encoding\nX = pd.DataFrame()\nfor variables in data_X:\n    dummy_var = pd.get_dummies(data_X[variables],prefix=variables)\n    X = pd.concat([X,dummy_var], axis=1)\n\nX.head()","bbc319df":"X.shape","4c341b52":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=101)","00f158fb":"lr = LogisticRegression()\nlr.get_params().keys()","62614831":"start = time.clock()\n\nparameters = {'solver':('liblinear', 'newton-cg', 'lbfgs', 'sag'), \n              'C': np.logspace(1,0.1,10),\n             'max_iter': [200],\n             'n_jobs':[-1]}\n\nclf = GridSearchCV(lr, parameters, cv=5, verbose=1)\nbest_model = clf.fit(X, y)\n  \nprint('Time taken (in secs) to tune hyperparameters: {:.2f}'.format(time.clock() - start))","0a761c10":"y_pred = clf.predict(X_test)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\nprint(clf.best_estimator_)\nprint('\\n---------------------------------------------------------------------------------')\nprint('Confusion matrix of logistic regression classifier on test set: \\n{}'.format(metrics.confusion_matrix(y_test,y_pred)))\nprint('\\n---------------------------------------------------------------------------------')\nprint('Accuracy of logistic regression classifier on test set: {:.10f}'.format(clf.score(X_test, y_test)))\nprint('\\n---------------------------------------------------------------------------------')\nprint('ROC of logistic regression classifier on test set: {:.10f}'.format(roc_auc))\n","fa0270d6":"plt.figure()\nlw = 2\nplt.plot(false_positive_rate[2], true_positive_rate[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)'% roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","5ec0a5d0":"Now we have 19 variables including one class label. Next we need to address catgeorical variables, one of the most popular way of handling categorical variable is one-hot encoding.","ad76d38e":"### 2.3. Split data into training and testing","a2a54726":"There are 8124 rows and 23 columns in the dataset of which one is lalbel. The dataset is clean with no missing data. The class variable has two unique classes (P for poisonous and E for edible). The class label looks very balanced with similar number of mushrooms classified under each label. The entire dataset is categorical (text data), we will have to convert them to nominal type.","6cb626fd":"* From the above cross tab plots we can see that some of the group in the variables show clear demarcation between classes for poisonous and edible for example : in odour , f class belongs to poisonous and m to edible\n* However most of the others share the groups. \n* Veil typehas only group. This variable is not useful in our analysis, we can remove it.\n* veil-colour, ring-number and gill-attachment also seems to exhibit same behaviour as Veil-type.Only f class exists we can validate it below with cross tab.","46efebe0":"### 1.2. Analyze redundant columns","c7d197e6":"#### 1. Find estimator parameters","0be730c8":"### Plot ROC Curve","913d8c8f":"# 1. Exploratory Data Analysis","ec3b01f6":"#### 2. Tune hyperparameters","fdc3f0df":"The number of variables increased from 18 to 107 with one-hot encoding. Since all the variables were categorical, we one-hot encoded all of them. Hence there is no need for standarding the data.","b88ff165":"### 2.2. Tranform X and y","2f6b84fb":"# 3. Data Mining","41916419":"The data is now ready for modelling, we will try different models like linear, non-linear, parametric, non-parametric etc in this project.","f53818f1":"## Parametric Models\n[In statistics, a parametric model or parametric family or finite-dimensional model is a particular class of statistical models. Specifically, a parametric model is a family of probability distributions that has a finite number of parameters.](https:\/\/en.wikipedia.org\/wiki\/Parametric_model)\n\nIn this project we will run hyparameter tuning for all the parametric models. scikit-learn has a package called GridSearchCV, which takes the dictionary of hypermeters and provides the best parameter.","fb7193bb":"2.2.1. **One-Hot encoding for X vars and Label encoding for y var**\n\nIn this method each category value is converted into a new column and assigned a 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly. However this can increase the dimensions exponentially depending on the number of categories and categorical columns we are dealing with.","94d13974":"### **1.1. Visualize cross tab output","4962b4c1":"# 2. Data Transformation","8e12d766":"As observed, most of the classes in these groups are redundant since the entire group belongs to one class. Lets remove all these variables --> 'gill-attachment', 'ring-number', 'veil-color' and 'veil-type'. These variables will not help us classify data into either classes.","9822cd6e":"### Logistic Regression\n[In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)\n","8d30b5d5":"### 2.1. Drop variables","5e84afda":"#### 3. Best model accuracy"}}