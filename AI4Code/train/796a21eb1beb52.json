{"cell_type":{"12e8bf3f":"code","e334bf39":"code","2c9d0452":"code","1b3a9a8d":"code","aa68d5ee":"code","4942ad83":"code","e91f52c3":"code","342daa81":"code","8ff14299":"code","04878f21":"code","c6eb3dcc":"code","8025ffab":"code","bb331d52":"code","44603189":"code","97814e4f":"code","5bebb1b9":"code","dd9fed46":"code","345acb6d":"code","f5be6fd5":"code","e8b7d4a3":"code","b7341800":"code","4d7f3975":"code","56ecae5d":"code","7a7b7c88":"code","d81e1e6d":"code","d59d5414":"code","bdad22c5":"code","d6e652f7":"code","2d873ee3":"code","8918ddbe":"code","67a5395c":"code","bf1ac0c3":"code","a342896f":"code","13dece7b":"code","bb983369":"code","8acb2b21":"code","7436b2bc":"code","68cd1b45":"code","49d2282c":"markdown","03ddb0bb":"markdown","2546a4b9":"markdown","6c47617b":"markdown","eb8a2796":"markdown","023078e5":"markdown","fea92eb4":"markdown","e93a0ad7":"markdown","9ef60f2a":"markdown","1227110b":"markdown","c46d5100":"markdown","201611fc":"markdown","4a11d07c":"markdown","ee75f741":"markdown","c1ee715b":"markdown","d3e65bdf":"markdown","268a31b1":"markdown","b9a56995":"markdown","c3aeb1de":"markdown","c15a68d0":"markdown","13c1fbe5":"markdown","df70111f":"markdown","e5fac5f2":"markdown","95e21ed4":"markdown","d141771a":"markdown","e03e39bd":"markdown","304d164a":"markdown","e339d29d":"markdown","e677b280":"markdown","a7fd66b7":"markdown","e257cfc5":"markdown","bcceced6":"markdown","f7e3df74":"markdown","70e154ba":"markdown","7a5a13a6":"markdown","43869375":"markdown","dd4b32cc":"markdown","9252250c":"markdown","b77cd209":"markdown","9e2bb14d":"markdown","4d80ea27":"markdown","deda4077":"markdown","25e4c680":"markdown","9dff1aba":"markdown","ee2bf1df":"markdown","d97465eb":"markdown","00727cc8":"markdown","8953da6b":"markdown","e898bb41":"markdown","3f9a2c34":"markdown","24eb5d4a":"markdown","49ccc4b5":"markdown","52ee6a0b":"markdown","dbd293cb":"markdown","74e92672":"markdown","c1437c8c":"markdown","bacb97c8":"markdown","28f177df":"markdown","ea142e0b":"markdown","b91026eb":"markdown","c1577c15":"markdown","ba19e52c":"markdown","1b4003d1":"markdown","aef747e6":"markdown","7c4d62b1":"markdown","77350591":"markdown","644caf03":"markdown"},"source":{"12e8bf3f":"# import necessary libraries \nimport nltk\nimport string\nimport re","e334bf39":"def lowercase_text(text): \n    return text.lower() \n  \ninput_str = \"Weather is too Cloudy.Possiblity of Rain is High,Today!!\"\nlowercase_text(input_str) ","2c9d0452":"# For Removing numbers \ndef remove_num(text): \n    result = re.sub(r'\\d+', '', text) \n    return result \n  \ninput_s = \"You bought 6 candies from shop, and 4 candies are in home.\"\nremove_num(input_s) ","1b3a9a8d":"pip install inflect","aa68d5ee":"# import the library \nimport inflect \nq = inflect.engine() \n  \n# convert number into text \ndef convert_num(text): \n    # split strings into list of texts \n    temp_string = text.split() \n    # initialise empty list \n    new_str = [] \n  \n    for word in temp_string: \n        # if text is a digit, convert the digit \n        # to numbers and append into the new_str list \n        if word.isdigit(): \n            temp = q.number_to_words(word) \n            new_str.append(temp) \n  \n        # append the texts as it is \n        else: \n            new_str.append(word) \n  \n    # join the texts of new_str to form a string \n    temp_str = ' '.join(new_str) \n    return temp_str \n  \ninput_str = 'You bought 6 candies from shop, and 4 candies are in home.'\nconvert_num(input_str)","4942ad83":"# let's remove punctuation \ndef rem_punct(text): \n    translator = str.maketrans('', '', string.punctuation) \n    return text.translate(translator) \n  \ninput_str = \"Hey, Are you excited??, After a week, we will be in Shimla!!!\"\nrem_punct(input_str) ","e91f52c3":"# importing nltk library\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nnltk.download('stopwords')\nnltk.download('punkt')\n  \n# remove stopwords function \ndef rem_stopwords(text): \n    stop_words = set(stopwords.words(\"english\")) \n    word_tokens = word_tokenize(text) \n    filtered_text = [word for word in word_tokens if word not in stop_words] \n    return filtered_text \n  \nex_text = \"Data is the new oil. A.I is the last invention\"\nrem_stopwords(ex_text)","342daa81":"#importing nltk's porter stemmer \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nstem1 = PorterStemmer() \n  \n# stem words in the list of tokenised words \ndef s_words(text): \n    word_tokens = word_tokenize(text) \n    stems = [stem1.stem(word) for word in word_tokens] \n    return stems \n  \ntext = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\ns_words(text)","8ff14299":"from nltk.stem import wordnet \nfrom nltk.tokenize import word_tokenize \nlemma = wordnet.WordNetLemmatizer()\nnltk.download('wordnet')\n# lemmatize string \ndef lemmatize_word(text): \n    word_tokens = word_tokenize(text) \n    # provide context i.e. part-of-speech(pos)\n    lemmas = [lemma.lemmatize(word, pos ='v') for word in word_tokens] \n    return lemmas \n  \ntext = 'Data is the new revolution in the World, in a day one individual would generate terabytes of data.'\nlemmatize_word(text)","04878f21":"import nltk\nnltk.download('punkt')","c6eb3dcc":"# importing tokenize library\nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag \nnltk.download('averaged_perceptron_tagger')\n  \n# convert text into word_tokens with their tags \ndef pos_tagg(text): \n    word_tokens = word_tokenize(text) \n    return pos_tag(word_tokens) \n  \npos_tagg('Are you afraid of something?') ","8025ffab":"# downloading the tagset  \nnltk.download('tagsets') \n  \n# extract information about the tag \nnltk.help.upenn_tagset('PRP')","bb331d52":"#importing libraries\nfrom nltk.tokenize import word_tokenize  \nfrom nltk import pos_tag \n  \n# here we define chunking function with text and regular \n# expressions representing grammar as parameter \ndef chunking(text, grammar): \n    word_tokens = word_tokenize(text) \n  \n    # label words with pos \n    word_pos = pos_tag(word_tokens) \n  \n    # create chunk parser using grammar \n    chunkParser = nltk.RegexpParser(grammar) \n  \n    # test it on the list of word tokens with tagged pos \n    tree = chunkParser.parse(word_pos) \n      \n    for subtree in tree.subtrees(): \n        print(subtree) \n    #tree.draw() \n      \nsentence = 'the little red parrot is flying in the sky'\ngrammar = \"NP: {<DT>?<JJ>*<NN>}\"\nchunking(sentence, grammar) ","44603189":"#Importing tokenization and chunk\nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, ne_chunk \nnltk.download('maxent_ne_chunker')\nnltk.download('words')\n  \ndef ner(text): \n    # tokenize the text \n    word_tokens = word_tokenize(text) \n  \n    # pos tagging of words \n    word_pos = pos_tag(word_tokens) \n  \n    # tree of word entities \n    print(ne_chunk(word_pos)) \n  \ntext = 'Brain Lara scored the highest 400 runs in a test match which played in between WI and England.'\nner(text) ","97814e4f":"import re\nsent = \"iNeuron13, Data is a new fuel\"\nr2 = re.findall(r\"^\\w+\",sent)\nprint(r2)","5bebb1b9":"import re\n\nprint((re.split(r'\\s','We splited this sentence')))","dd9fed46":"import re\n\nlists = ['icecream images', 'ink immitated', 'inner peace']\n\nfor i in lists:\n    q = re.match(\"(i\\w+)\\W(i\\w+)\", i)\n    \n    if q:\n        print((q.groups()))","345acb6d":"import re\n\npattern = [\"playing\", \"iNeuron\"]\ntext = \"Raju is playing outside.\"\n\nfor p in pattern:\n    print(\"You're looking for '%s' in '%s'\" %(p, text), end = ' ')\n    \n    if re.search(p, text):\n        print('Found match!')\n        \n    else:\n        print(\"no match found!\")","f5be6fd5":"import re\n\nkgf = \"Gaurav@iNeuron.ai, Nilesh@iNeuron.ai, Jay@iNeuron.ai, Vikash@iNeuron.ai\"\n\nemails = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', kgf)\n\nfor e in emails:\n    print(e)","e8b7d4a3":"import re\n\naa = \"\"\"iNeuron13\nMachine\nLearning\"\"\"\n\nq1 = re.findall(r\"^\\w\", aa)\nq2 = re.findall(r\"^\\w\", aa, re.MULTILINE)\nprint(q1)\nprint(q2)","b7341800":"from nltk.tokenize.regexp import WhitespaceTokenizer\nm = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"","4d7f3975":"tokens = WhitespaceTokenizer().tokenize(m)\nprint(len(tokens))","56ecae5d":"tokens","7a7b7c88":"my_vocab = set(tokens)\nprint(len(tokens))","d81e1e6d":"my_vocab","d59d5414":"my_st = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"","bdad22c5":"from nltk.tokenize.regexp import WordPunctTokenizer","d6e652f7":"m_t = WordPunctTokenizer().tokenize(my_st)\n\nprint(len(m_t))","2d873ee3":"m_t","8918ddbe":"my_vocab = set(m_t)\nprint(len(my_vocab))","67a5395c":"my_vocab ","bf1ac0c3":"#from nltk.book import *\nimport nltk\n#nltk.download('gutenberg')\nprint(\"\\n\\n\\n\")\ntext1 = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\nfreqDist = nltk.FreqDist(word_tokenize(text1))\nprint(freqDist)","a342896f":"print(freqDist[\"person\"])","13dece7b":"words = freqDist.keys()\nprint(type(words))","bb983369":"print(len(words))","8acb2b21":"freqDist.plot(15)","7436b2bc":"from nltk import FreqDist\n \nsent = \"'There is no need to panic. We need to work together, take small yet important measures to ensure self-protection,' the Prime Minister tweeted.\"\n \ntext_list = sent.split(\" \")\n \nfreqDist = FreqDist(text_list)\nwords = list(freqDist.keys())\n \nprint(freqDist['need'])","68cd1b45":"freqDist.plot(20)","49d2282c":"Likewise, you can also use other Python flags like re.U (Unicode), re.L (Follow locale), re.X (Allow Comment), etc.\n\n","03ddb0bb":"You see many Python RegEx methods and functions take an optional arguemnet flag.This flag can modify the meaning of the given regeEx pattern.\n\nVarious flags used in python include.","2546a4b9":"Text normalization has even been effective for analyzing highly unstructured clinical texts where physicians take notes in non-standard ways. We have also found it useful for topic extraction where near synonyms and spelling differences are common (like 'topic modelling', 'topic modeling', 'topic-modeling', 'topic-modelling').\n\nUnlike stemming and lemmatization, there is not a standard way to normalize texts. It typically depends on the task. For e.g, the way you would normalize clinical texts would arguably be different from how you normalize text messages.\n\nSome of the common approaches to text normalization include dictionary mappings, statistical machine translation (SMT) and spelling-correction based approaches.","6c47617b":"A Regex or we called it as regular expression, it is a type of object will help you out to extract information from any string data by searching through text and find it out what you need.Whether it's punctuation, numbers, letters, or even white spaces, RegEx will allow you to check and match any of the character combination in strings.\n\nFor example, suppose you need to match the format of a email addresses or security numbers. You can utilize RegEx to check the pattern inside the text strings and use it to replace another substring.\n\nFor instance, a RegEx could tell the program to search for the specific text from the string and then to print out the output accordingly. Expressions can include Text matching, Repetition of words,Branching,pattern-composition.\n\nPython supports RegEx through libraries. In RegEx supports for various things like Identifiers, Modifiers, and White Space.","eb8a2796":"# Chunking","023078e5":"Suppose you want to do frequency distribution based on your own personal text. Let's get started,","fea92eb4":"We use re.findall() module is when you wnat to iterate over the lines of the file, it'll do like list all the matches in one go. Here in a example, we would like to fetch email address from the list and we want to fetch all emails from the list, we use re.findall() method.","e93a0ad7":"# Using re.findall() for text","9ef60f2a":"We should either remove the numbers or convert those numbers into textual representations. We use regular expressions(re) to remove the numbers.","1227110b":"In the above example, we defined the grammar by using the regular expression rule. This rule tells you that NP(noun phrase) chunk should be formed whenever the chunker find the optional determiner(DJ) followed by any no. of adjectives and then a NN(noun).\n\n\nLibraries like Spacy and TextBlob are best for chunking.","c46d5100":"![image.png](attachment:1c7a4246-6474-4c8d-a955-a9b5bb4a4167.png)","201611fc":"What is Frequency distribution? This is basically counting words in your texts.To give a brief example of how it works,","4a11d07c":"Data preprocessing is an essential step in building a Machine Learning model and depending on how well the data has been preprocessed; the results are seen.\nIn NLP, text preprocessing is the first step in the process of building a model.","ee75f741":"# Let's look the example for re.M or Multiline Flags","c1ee715b":"n the class nltk.text.Text function do the same stuff, so what is the difference? The difference is that with FreqDist you can create your own texts without the necessity of converting your text to nltk.text.Text class.\n\nAnd the other usual functon is plot. Plot will do like it displays the most used words in your text. So, if you want to see 15 most used words in the text , For example like:","d3e65bdf":"Above tokenizer also split the words into tokens:","268a31b1":"One of the most important function in FreqDist is the .keys() function. Let us see what will it give in a below code.","b9a56995":"![image.png](attachment:24da6117-bc25-4c0f-b8a1-f404b0b3667c.png)","c3aeb1de":"Now we are going to perform the same operation but with the different tokenizer.","c15a68d0":"# Parts of Speech (POS) Tagging","13c1fbe5":"We do lowercase the text to reduce the size of the vocabulary of our text data.","df70111f":"A RegEx is commonly used to search for a pattern in the text. This method takes a RegEx pattern and a string and searches that pattern with the string.\n\nFor using re.search() function, you need to import re first. The search() function takes the \"pattern\" and \"text\" to scan from our given string and returns the match object when the pattern found or else not match.","e5fac5f2":"# Text lowercase","95e21ed4":"# Effects of normalization","d141771a":"import re\n\n\nre library in Python is used for string searching and manipulation.\nWe also used it frequently for web scraping.\nExample for w+ and ^ Expression\n^: Here in this expression matches the start of a string.\nw+: This expression matches for the alphanumeric characters from inside the string.\nHere, we will give one example of how you can use \"w+\" and \"^\" expressions in code. re.findall will cover in next parts,so just focus on the \"w+\" and \"^\" expression.\n\nLet's have an example \"iNeuron13, Data is a new fuel\", if we execute the code we will get \"iNeuron13\" as a result.","e03e39bd":"# Text normalization","304d164a":"I am assuming you have the understanding of tokenization,the first figure we can calculate is the word frequency.By word frequency we can find out how many times each tokens appear in the text. When talking about word frequency, we distinguished between types and tokens.Types are the distinct words in a corpus, whereas tokens are the words, including repeats. Let's see how this works in practice.\n\nLet's take an example for better understanding:\n\n\u201cThere is no need to panic. We need to work together, take small yet important measures to ensure self-protection,\u201d the Prime Minister tweeted.\n\nHow many tokens and types are there in above sentences?\n\nLet's use Python for calculating these figures. First, tokenize the sentence by using the tokenizer which uses the non-alphabetic characters as a separator.\n\n","e339d29d":"It is used to extract information from unstructured text. It is used to classy the entities which is present in the text into categories like a person, organization, event, places, etc. This will give you a detail knowledge about the text and the relationship between the different entities.","e677b280":"In the first line, you don't have to import nltk.book to use the FreqDist class.\n\nWe then declare sent and text_list variables. The variable sent is your custom text and the variable text_list is a list that contains all the words of your custom text.You can see that we used sent.split(\" \") to separate the words.\n\nThen you have the variable freqDist and words. freqDist is an object of the FreqDist class is for the text you have given and words is the list of all keys of freqDist.\n\nThe last line of code is where you print your results. In this example, your code will print the count of the word \u201cneed\u201d.\n\nIf you replace \u201cneed\u201d with \u201cPrime\u201d, you can see that it will return 1 instead of 2. This is because nltk indexing is case-sensitive. To avoid this, you can use the .lower() function in the variable text.","a7fd66b7":"# Remove default stopwords","e257cfc5":"# Word Count","bcceced6":"# Stemming","f7e3df74":"From Stemming we will process of getting the root form of a word. Root or Stem is the part to which inflextional affixes(like -ed, -ize, etc) are added. We would create the stem words by removing the prefix of suffix of a word. So, stemming a word may not result in actual words.\n\nFor Example: Mangoes ---> Mango\n\n         Boys ---> Boy\n\n         going ---> go\nIf our sentences are not in tokens, then we need to convert it into tokens. After we converted strings of text into tokens, then we can convert those word tokens into their root form. These are the Porter stemmer, the snowball stemmer, and the Lancaster Stemmer. We usually use Porter stemmer among them.","70e154ba":"# Python Flags","7a5a13a6":"In the multiline flag the pattern character \"^\" matches the first character of the string and the begining of the each line. While the small \"w\" is used to mark the space with characters.When you run the code first variable \"q1\" prints out the character \"i\" only and while using the Multiline flag will give the result of all first character of all strings.","43869375":"Chunking is the process of extracting phrases from the Unstructured text and give them more structure to it. We also called them shallow parsing.We can do it on top of pos tagging. It groups words into chunks mainly for noun phrases. chunking we do by using regular expression.","dd4b32cc":"# Personal Frequency Distribution","9252250c":"# Remove numbers","b77cd209":"# You can also convert the numbers into words. This could be done by using the inflect library.","9e2bb14d":"# Understanding Regex","4d80ea27":"# Finding Pattern in the text(re.search())","deda4077":"# Frequency distribution","25e4c680":"In the tect pre-processing highly overlooked step is text normalization. The text normalization means the process of transforming the text into the canonical(or standard) form. Like, \"ok\" and \"k\" can be transformed to \"okay\", its canonical form.And another example is mapping of near identical words such as \"preprocessing\", \"pre-processing\" and \"pre processing\" to just \"preprocessing\".\n\nText normaliztion is too useful for noisy textssuch as social media comments, comment to blog posts, text messages, where abbreviations, misspellings, and the use out-of-vocabulary(oov) are prevalent.","9dff1aba":"We'll import different tokenizer:","ee2bf1df":"# Named Entity Recognition","d97465eb":"The class FreqDist works like a dictionary where keys are the words in the text and the values are count associated with that word. For example, if you want to see how many words \"person\" are in the text, you can type as:","00727cc8":"We remove punctuations because of that we don't have different form of the same word. If we don't remove punctuations, then been, been, and been! will be treated separately.","8953da6b":"Similarly, there are series of regular expression in Python that you can use in various ways like \\d,\\D,$,.,\\b, etc.","e898bb41":"In the above example NNP stands for Proper noun, PRP stands for personal noun, IN as Preposition. We can get all the details pos tags using the Penn Treebank tagset.","3f9a2c34":"Stopwords are words that do not contribute to the meaning of the sentence. Hence, they can be safely removed without causing any change in the meaning of a sentence. The NLTK(Natural Language Toolkit) library has the set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens.","24eb5d4a":"In the Above example, we look for two literal strings \"playing\", \"iNeuron\" and in text string we had taken \"Raju is playing outside.\". For \"playing\" we got the match and in the output we got \"Found Match\", while for the word \"iNeuron\" we didn't got any match. So,we got no match found for that word.","49ccc4b5":"Note: If we remove '+' sign from \\w, the output will change and it'll give only first character of the first letter, i.e [i]","52ee6a0b":"Supose we have textual data available, we need to apply many of pre-processing steps to the data to transform those words into numerical features that work with machine learning algorithms.\n\nThe pre-processing steps for the problem depend mainly on the domain and the problem itself.We don't need to apply all the steps for every problem.\n\nHere, we're going to see text preprocessing in Python. We'll use NLTK(Natural language toolkit) library here.","dbd293cb":"The pos(parts of speech) explain you how a word is used in a sentence. In the sentence, a word have different contexts and semantic meanings. The basic natural language processing(NLP) models like bag-of-words(bow) fails to identify these relation between the words. For that we use pos tagging to mark a word to its pos tag based on its context in the data. Pos is also used to extract rlationship between the words.","74e92672":"Note in the above we had used a slightly different syntax for importing the module. You'll recognize by now the variable assignment.","c1437c8c":"# Lemmatization","bacb97c8":"# What is RegEx and why is it important?","28f177df":"As stemming, lemmatization do the same but the only difference is that lemmatization ensures that root word belongs to the language. Because of the use of lemmatization we will get the valid words. In NLTK(Natural language Toolkit), we use WordLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization.So, we added pos(parts-of-speech) as a parameter.","ea142e0b":"# Use RegEx methods","b91026eb":"# Remove Punctuation","c1577c15":"# RegEx Syntax","ba19e52c":"After running above code, it'll give as class 'dict_keys', in the other words, you get a list of all the words in your text.\n\nAnd you want to see how many words are there in the text,","1b4003d1":"As you're a software developer, you have probably encountered regular expressions many times and got consufed many times with these daunting set of characters grouped together like this:\n\n\nAnd you may wondered what this is all about?\n\nRegular Expressions(Regx or RegExp) are too useful in stepping up your algorithm game and this will make you a better problem solver. The structure of Regx can be intimidating at first, but it is very rewarding once you got all the patterns and implement them in your work properly.","aef747e6":"Example of \\s expression in re.split function\n\n\n\"s:\" This expression we use for creating a space in the string.\nTo understand better this expression we will use the split function in a simple example. In this example, we have to split each words using the \"re.split\" function and at the same time we have used \\s that allows to parse each word in the string seperately.","7c4d62b1":"The \"re\" packages provide several methods to actually perform queries on an input string. We will see different methods which are\n\nre.match()\n\n\n\nre.search()\n\n\n\nre.findall()\n\n\n\nNote: Based on the RegEx, Python offers two different primitive operations. This match method checks for the match only at the begining of the string while search checks for a match anywhere in the string.\n\nUsing re.match()\nThe match function is used to match the RegEx pattern to string with optional flag. Here, in this \"w+\" and \"\\W\" will match the words starting from \"i\" and thereafter ,anything which is not started with \"i\" is not identified. For checking match for each element in the list or string, we run the for loop.","77350591":"# Natural Language Toolkit (NLTK)","644caf03":"What is the difference between the above approaches? In the first one, vocabulary ends up containing \"words\" and \"words.\" as two distinct words; whereas in second example \"words\" is a token type and \".\" (i.e. the dot) is split into a separate token and this results into a new token type in addition to \"words\"."}}