{"cell_type":{"a9290248":"code","beeecc02":"code","587a324d":"code","cafd7d46":"code","0e6e493c":"code","c4b8e121":"code","233ed0ca":"markdown","aa9125f5":"markdown","46280ae0":"markdown","edebe245":"markdown"},"source":{"a9290248":"# run me first!\n# import the necessary packages\n# You don't need to change any of this, but feel free to read it\nimport cv2          # we need OpenCV\nimport numpy as np  # and Numpy\nfrom matplotlib import pyplot as plt  # for showing images\nimport pandas as pd\nimport requests  # for loading the images\nimport random    # for random pixel values\n\n# utility function for loading an image from any URL\ndef load_image_from_url(url: str):\n    r = requests.get(url)\n    image = np.asarray(bytearray(r.content), dtype=\"uint8\")\n    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n    return image\n\n# utility function to display an image\ndef show_image(img):\n    plt.figure(figsize=(6*img.shape[1] \/ img.shape[0], 6))\n    plt.axis('off')\n    if np.ndim(img) == 2:\n        if img.dtype == np.uint8:\n            plt.imshow(img, cmap='gray', vmin=0, vmax=255)\n        else:\n            plt.imshow(img, cmap='gray', vmin=0, vmax=1)\n    else:\n        plt.imshow(img[:,:,::-1])","beeecc02":"img = cv2.imread('..\/input\/images\/LukeTLJ.png')\ngreyscale = 1.0 * cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ngreyscale = np.float32(greyscale)\n\ndst = greyscale\n\n# TODO: dst should be the result of cornerHarris. Check the docs\n\nimg[dst>0.01*dst.max()]=[0,0,255]\nshow_image(img)","587a324d":"# load images\nimgL = cv2.resize(cv2.imread('..\/input\/images\/Left.jpg'), (0,0), fx=0.5, fy=0.5)\nimgR = cv2.resize(cv2.imread('..\/input\/images\/Right.jpg'), (0,0), fx=0.5, fy=0.5)\n\n# create ORB detector\norb = cv2.ORB_create()\n\n# detect\nkpL, desL = orb.detectAndCompute(imgL,None)\n# TODO: detect features in the right image, similarly to how it's done on the left image\nkpR, desR = ([], [])\n\n# visualse features by drawing red circles around them\nimg_kL = cv2.drawKeypoints(imgL, kpL, None, color=(0,0,255))\n# TODO: visualise features on the right image as well\nimg_kR = np.zeros_like(imgR)\n\n\nshow_image(np.concatenate((img_kL, img_kR), axis=1))","cafd7d46":"# using a brute force matcher and hamming distance.\n# You can try FLANN matchers if interested\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\nmatches = bf.match(desL,desR)\n\n# TODO: this visualisation is really messy, as weak matches are alos shown. \n# First, sort the matches list by increasing distance (you can use Python's sorted() method \n# with the key being the distance). Then only use the first 20 items of the matches list for visualisation\nimg3 = cv2.drawMatches(imgL,kpL,imgR,kpR,matches[:], None, flags=2)\nshow_image(img3)","0e6e493c":"# using ransac to find the best homography transform, then store it in H\nleft_pts = np.float32([ kpL[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\nright_pts = np.float32([ kpR[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\nH, mask = cv2.findHomography(right_pts, left_pts, cv2.RANSAC, 5.0)\n\n# now visualise how the right image would look when projected onto the left image\nh,w,d = imgL.shape\npts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\ndst = cv2.perspectiveTransform(pts,H)\nimgRinL = imgL.copy()\nimgRinL = cv2.polylines(imgRinL,[np.int32(dst)],True,255,3, cv2.LINE_AA)\nshow_image(imgRinL)","c4b8e121":"# projecting right image onto the destination\ndst = cv2.warpPerspective(imgR, H, ((imgL.shape[1] + imgR.shape[1]), imgL.shape[0]))\n\n# TODO: copy the pixels of the left image onto the top-left corner of dst\n\n# Extension: to reduce seamlines, compute a weighted average of the pixels, rather than\n# simple overwriting them\n\n#visualise\nshow_image(dst)","233ed0ca":"# Images using OpenCV (part 3)\nThe following exercises will help you get some hands-on experience with images (bitmaps) using the duly popular Open Computer Vision (OpenCV) library.\n\nOpenCV doesn't use Pandas to store its pixels, instead it relies on a similarly convenient library called Numpy (np).\n\nFor documentation, take a look here:\nOpenCV: https:\/\/docs.opencv.org\/4.1.2\/\n\nThis notebook walks you through feature detection including high level features and panorama stitching","aa9125f5":"## Salient features\nAn image contains thousands to millions of pixels, but not all of them are equally important. Features are specific parts which are potentially more easy to recognise (e.g. corners, edges). When a feature is unique, we call it salient.\n\nYour first task is to complete the code below to find some salient features in the image of your choice.\n\nCheck the tutorial to help you get started: https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_features_harris\/py_features_harris.html","46280ae0":"## Panorama stitching\nPanorama stitching is a great example of making sense and aligning two sets of unstructured data.\nThis requires solving the *correspondence problem*, finding sets of features which overlap. For features, we will use ORB features (https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_orb\/py_orb.html)\n\nThe steps involved:\n1. find all ORB features in both left and right images\n2. try to match the features\n3. find the projection that transforms one image into the co-ordinates of the other\n4. apply the projection to stitch the images together\n\nThe next four code blocks solve these problems respectively.","edebe245":"You should now see that a number of features have been detected on both images. You notice that there are a number of them which are only in one of the images, but there should be some overlap (around the trees and on the rocks above them). Next, we need to match these."}}