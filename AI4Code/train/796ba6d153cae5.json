{"cell_type":{"a143de8d":"code","bd212717":"code","746b0b8d":"code","2e8d9ccb":"code","42a24e37":"code","bfec554e":"code","e63d8529":"code","416b8cb4":"code","a0cce745":"code","e918bfa9":"code","920fd066":"code","8faf1d98":"code","1194c826":"code","3831e22e":"code","2889ccfd":"code","99adf2aa":"code","bb223a31":"code","1ab7bea2":"code","633581d6":"code","8061e43c":"code","96cd1349":"code","7d768ec5":"code","54cb4e9b":"code","9c2b75a9":"code","1eb605b1":"code","e93d110b":"code","ec068798":"code","4c5dfdc5":"code","40c750e8":"code","aaa65f1f":"code","9d2e9b29":"code","d0779b87":"code","797aa655":"code","80dd5994":"code","840f3199":"code","a2ac68dc":"code","88b7f2fd":"code","d9d8de9e":"code","9e01011d":"code","f2011398":"code","123313aa":"code","02769e22":"code","adcb302b":"code","e7cfbd35":"code","e03782d5":"code","cce28654":"code","ce28d7c6":"code","f0dc1eca":"code","dde3b488":"code","45b24f98":"code","d0b2ef59":"code","b2b0dd0d":"code","84c4492a":"code","b9b0a545":"code","271c4f11":"code","53f9ccf0":"code","41a98c20":"code","76968b71":"code","f432a0d6":"code","5b14d928":"code","bd8dada8":"code","2552a919":"code","3e00a0aa":"code","7f85dafd":"code","15593d52":"code","09958102":"code","c9f06c60":"code","29404d55":"code","1c3c9aa4":"code","6d432b1e":"code","9bc32e48":"code","03e72681":"code","88ae9bfc":"code","63f1b649":"code","2dbda87e":"code","1215a9a2":"code","921fe6c6":"code","56608810":"code","e82809a0":"code","41f44324":"code","b575bced":"code","0142fa7d":"code","640dff98":"code","f5007cfb":"code","d5c42845":"code","c15da698":"code","979b7ac3":"code","fa769959":"code","d74ad254":"code","02fd7705":"code","6da2d092":"code","9ee09a64":"code","88733f7e":"code","dc14cba6":"code","a04e91e2":"markdown","aedd8da3":"markdown","34e3836f":"markdown","9f6dc946":"markdown","ff4b4d77":"markdown","8dad41a3":"markdown","661b9381":"markdown","88ba2040":"markdown","ad4d7dce":"markdown","a900d39c":"markdown","275144ee":"markdown","790cfe4d":"markdown","0adbac86":"markdown","fe1c7d01":"markdown","b371a629":"markdown","7773064b":"markdown","39550658":"markdown","d04e8219":"markdown","ab3a21ad":"markdown","937364dd":"markdown","f1524b5b":"markdown","659605c7":"markdown","46ba61d5":"markdown","2693310e":"markdown","17525475":"markdown","3957b45a":"markdown","cdf1a0c1":"markdown","856726c8":"markdown","2a55cac2":"markdown","d9b81fd8":"markdown","48f38000":"markdown","7bc33b41":"markdown","cab84dee":"markdown","6c4788da":"markdown","5f6c98a0":"markdown","9c1bd023":"markdown","f7f8ecb8":"markdown","b77637cc":"markdown","fd1e5abb":"markdown","957d16e6":"markdown","9f7a8012":"markdown","96c966bc":"markdown","cf403e2c":"markdown","6544982d":"markdown","e277b9a9":"markdown","84939dc2":"markdown","b3239324":"markdown","813b4c87":"markdown","80c92a49":"markdown","7b438baa":"markdown","ec37e94d":"markdown","5c2f921b":"markdown","a23a60e8":"markdown","283620bb":"markdown","d42a943c":"markdown","1c320df1":"markdown","6e2d40a0":"markdown","c8cb19ae":"markdown","4b718a75":"markdown","020e3ea0":"markdown","dd4ca48e":"markdown","16a191c1":"markdown","7f54b39b":"markdown"},"source":{"a143de8d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd212717":"!pip install texthero","746b0b8d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport texthero as hero\nfrom scipy import stats\nimport plotly.express as px\nimport datetime as dt\nimport plotly.graph_objs as gog\nimport plotly.offline as pyoff\nimport plotly.graph_objects as go\nimport seaborn as sns","2e8d9ccb":"df = pd.read_csv(\"..\/input\/online-retail-ii-uci\/online_retail_II.csv\")\ndf.head(5)","42a24e37":"df.info()","bfec554e":"df.isnull().sum()","e63d8529":"#Percentage of null with reference to entire dataset\nfor i in df:\n    print(\"percentage of null values for \" + str(i) + \" is\")\n    value=(df[i].isnull().sum()\/df.shape[0]) * 100\n    print(round(value,4))\n    print(\"\\n\")","416b8cb4":"df = df.drop(index=df[df['Description'].isnull()].index)\n\ndf.isnull().sum()","a0cce745":"df_customerid_null= df[df['Customer ID'].isnull()]\n","e918bfa9":"df_customerid_null['InvoiceDate'] = pd.to_datetime(df_customerid_null['InvoiceDate'])","920fd066":"#Time Features for data with null customerID values\ndf_customerid_null['Year'] = df_customerid_null[\"InvoiceDate\"].apply(lambda x: x.year)\ndf_customerid_null['Month'] = df_customerid_null[\"InvoiceDate\"].apply(lambda x: x.month)\ndf_customerid_null['Weekday'] = df_customerid_null[\"InvoiceDate\"].apply(lambda x: x.weekday())\ndf_customerid_null['Hour'] = df_customerid_null[\"InvoiceDate\"].apply(lambda x: x.hour)","8faf1d98":"df_customerid_null['Num']=1\ndf_customerid_null.head(5)","1194c826":"#Time features for original df with no null values\ndf_no_null = df.drop(index=df[df['Customer ID'].isnull()].index)\n\ndf_no_null.isnull().sum()","3831e22e":"df_no_null['Num']=1\ndf_no_null.head(5)","2889ccfd":"df_no_null['InvoiceDate'] = pd.to_datetime(df_no_null['InvoiceDate'])","99adf2aa":"#Time Features for data without null customerID values\ndf_no_null['Year'] = df_no_null[\"InvoiceDate\"].apply(lambda x: x.year)\ndf_no_null['Month'] = df_no_null[\"InvoiceDate\"].apply(lambda x: x.month)\ndf_no_null['Weekday'] = df_no_null[\"InvoiceDate\"].apply(lambda x: x.weekday())\ndf_no_null['Hour'] = df_no_null[\"InvoiceDate\"].apply(lambda x: x.hour)","bb223a31":"hourly_sales = df_customerid_null.groupby('Hour')['Num'].sum().sort_index(ascending=True)\nx_hourly_sales = hourly_sales.index \ny_hourly_sales = hourly_sales.values\nplt.bar(x_hourly_sales, y_hourly_sales, label = 'hourly sales_null')\nplt.legend()","1ab7bea2":"hourly_sales_no_null = df_no_null.groupby('Hour')['Num'].sum().sort_index(ascending=True)\nx_hourly_sales_no_null = hourly_sales_no_null.index \ny_hourly_sales_no_null = hourly_sales_no_null.values\nplt.bar(x_hourly_sales_no_null, y_hourly_sales_no_null, label = 'hourly sales_no_null')\nplt.legend()","633581d6":"weekday_sales = df_customerid_null.groupby('Weekday')['Num'].sum().sort_index(ascending=True)\nx_weekday = weekday_sales.index \ny_weekday = weekday_sales.values\nplt.bar(x_weekday, y_weekday, label = 'weekday sales')\nplt.legend()","8061e43c":"weekday_sales_no_null = df_no_null.groupby('Weekday')['Num'].sum().sort_index(ascending=True)\nx_weekday_sales_no_null = weekday_sales_no_null.index \ny_weekday_sales_no_null = weekday_sales_no_null.values\nplt.bar(x_weekday_sales_no_null, y_weekday_sales_no_null, label = \"weekday sales_no_null\")\nplt.legend()","96cd1349":"monthly_sales = df_customerid_null.groupby('Month')['Num'].sum().sort_index(ascending=True)\nx_month = monthly_sales.index \ny_month = monthly_sales.values\nplt.bar(x_month, y_month, label = 'monthly sales')\nplt.legend()","7d768ec5":"monthly_sales_no_null = df_no_null.groupby('Month')['Num'].sum().sort_index(ascending=True)\nx_monthly_sales_no_null = monthly_sales_no_null.index \ny_monthly_sales_no_null = monthly_sales_no_null.values\nplt.bar(x_monthly_sales_no_null, y_monthly_sales_no_null, label = \"monthly sales_no_null\")\nplt.legend()","54cb4e9b":"yearly_sales = df_customerid_null.groupby('Year')['Num'].sum().sort_index(ascending=True)\nx_year = yearly_sales.index \ny_year = yearly_sales.values\nplt.bar(x_year, y_year, label = 'yearly sales')\nplt.legend()","9c2b75a9":"yearly_sales_no_null = df_no_null.groupby('Year')['Num'].sum().sort_index(ascending=True)\nx_yearly_sales_no_null = yearly_sales_no_null.index \ny_yearly_sales_no_null = yearly_sales_no_null.values\nplt.bar(x_yearly_sales_no_null, y_yearly_sales_no_null, label = \"yearly sales_no_null\")\nplt.legend()","1eb605b1":"df = df.drop(index=df[df['Customer ID'].isnull()].index)","e93d110b":"# checking for duplicate transactions\nprint('Number of duplicate entries: {}'.format(df_no_null.duplicated().sum()))\ndf.drop_duplicates(inplace = True)","ec068798":"df.describe(include='all')","4c5dfdc5":"negquantity = df[df['Quantity'] < 0]\nnegquantity.head(20)","40c750e8":"df['order_cancelled'] = df['Invoice'].apply(lambda x:int('C' in x))\n# display(df[:5])\n\nnum_order_cancelled = df['order_cancelled'].sum()\ntotal = df.shape[0]\nprint('Number of orders cancelled: {}\/{} ({:.2f}%) '.format(num_order_cancelled, total, num_order_cancelled\/total*100))\n\n ","aaa65f1f":"invoice = df[df['Customer ID'] == 16321.0]\ninvoice","9d2e9b29":"df_check = df[df['Quantity'] < 0][['StockCode','Description','Quantity','InvoiceDate','Price','Customer ID']]\niterations=0\nfor index, col in  df_check.iterrows():\n    if df[(df['Description'] == col[1]) & (df['Quantity'] == -col[2]) & (df['Customer ID'] == col[4])].shape[0] == 0:\n        if(iterations<200):\n            iterations+=1\n        elif(iterations<240):\n            print(\"----------Does not match----------\")\n            print(df_check.loc[index])\n            iterations+=1\n        else:\n            break\n\n#         break","d0779b87":"df.loc[(df[\"Customer ID\"]==14695) & (df[\"StockCode\"]==\"90214L\") ]","797aa655":"df_discount=df[df[\"Description\"]==\"Discount\"]\ndf_discount","80dd5994":"df_cleaned = df.copy(deep = True)\ndf_cleaned['QuantityCanceled'] = 0\n\nentry_to_remove = [] ; doubtful_entry = []\n\nfor index, col in  df.iterrows():\n    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n    df_test = df[(df['Customer ID'] == col['Customer ID']) &\n                         (df['StockCode']  == col['StockCode']) & \n                         (df['InvoiceDate'] < col['InvoiceDate']) & \n                         (df['Quantity']   > 0)].copy()\n    #_________________________________\n    # Cancelation WITHOUT counterpart\n    if (df_test.shape[0] == 0): \n        doubtful_entry.append(index)\n    #________________________________\n    # Cancelation WITH a counterpart\n    elif (df_test.shape[0] == 1): \n        index_order = df_test.index[0]\n        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n        entry_to_remove.append(index)        \n    #______________________________________________________________\n    # Various counterparts exist in orders: we delete the last one\n    elif (df_test.shape[0] > 1): \n        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n        for ind, val in df_test.iterrows():\n            if val['Quantity'] < -col['Quantity']: continue\n            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n            entry_to_remove.append(index) \n            break     ","840f3199":"total = df_cleaned.shape[0]\nlen_doubtful_entry=len(doubtful_entry)\n\nprint(\"entry_to_remove: {} ({:.2f}%)\".format(len(entry_to_remove),(len(entry_to_remove)\/total)*100))\nprint(\"doubtful_entry: {} ({:.2f}%)\".format(len(doubtful_entry),(len(doubtful_entry)\/total)*100))","a2ac68dc":"df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\ndf_cleaned.drop(doubtful_entry, axis = 0, inplace = True)\nremaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\nprint(\"Number of entries not yet deleted: {}\".format(remaining_entries.shape[0]))\nremaining_entries[:5]","88b7f2fd":"df_cleaned['TotalPrice'] = df_cleaned['Price'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\ndf_cleaned.sort_values('Customer ID')[:5]","d9d8de9e":"temp = df_cleaned.groupby(by=['Customer ID', 'Invoice'], as_index=False)['TotalPrice'].sum()\nbasket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n\ntemp = df_cleaned.groupby(by=['Customer ID', 'Invoice'], as_index=False)\n\nbasket_price = basket_price[basket_price['Basket Price'] > 0]\nbasket_price.sort_values('Customer ID')[:6]","9e01011d":"price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\ncount_price = []\nfor i, price in enumerate(price_range):\n    if i == 0: continue\n    val = basket_price[(basket_price['Basket Price'] < price) &\n                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n    count_price.append(val)\n     \nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(11, 6))\ncolors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\nlabels = [ '{}<x<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\nsizes  = count_price\nexplode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\nax.pie(sizes, explode = explode, labels=labels, colors = colors,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow = False, startangle=0)\nax.axis('equal')\nf.text(0.5, 1.01, \"Distribution of individual purchases (x)\", ha='center', fontsize = 18);","f2011398":"df_cleaned.to_csv('df_cleaned.csv')\n# df_cleaned = pd.read_csv(\"..\/input\/output\/df_cleaned.csv\")","123313aa":"z = np.abs(stats.zscore(df_cleaned['TotalPrice']))\nthreshold = 3\n\ndf_cleaned_outliers = df_cleaned.copy(deep=True)\ndf_cleaned_outliers['Outliers'] = z\n\ndf_cleaned_outliers[df_cleaned_outliers['Outliers']>threshold]","02769e22":"len_outlier=df_cleaned_outliers[df_cleaned_outliers['Outliers']>threshold]","adcb302b":"len_outlier=df_cleaned_outliers[df_cleaned_outliers['Outliers']>threshold].shape[0]\ntotal = df_cleaned.shape[0]\n\nprint(\"entry_to_remove: {} ({:.2f}%)\".format(len_outlier,(len_outlier\/total)*100))","e7cfbd35":"df_cleaned.drop(df_cleaned_outliers[df_cleaned_outliers['Outliers']>threshold].index, axis = 0, inplace = True)","e03782d5":"# product_df = df_cleaned.drop(columns=['StockCode', 'Invoice', 'Customer ID', 'Price', 'Quantity', 'InvoiceDate', 'Country'])\ndf_cleaned['Description_clean'] = df_cleaned['Description'].pipe(hero.clean)","cce28654":"tw = hero.visualization.top_words(df_cleaned['Description_clean']).head(40)\n\nfig = px.bar(tw)\nfig.show()","ce28d7c6":"df_cleaned['InvoiceDate'] = pd.to_datetime(df_cleaned['InvoiceDate'])\ndf_cleaned['Year'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.year)\ndf_cleaned['Month'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.month)\n# df_cleaned['MonthYear'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.strftime(\"%B %Y\"))\ndf_cleaned['Weekday'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.weekday())\n# df_cleaned['Day'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.day)\ndf_cleaned['Hour'] = df_cleaned[\"InvoiceDate\"].apply(lambda x: x.hour)","f0dc1eca":"df_cleaned_1=df_cleaned[df_cleaned[\"Month\"]== 1]\ndf_cleaned_2=df_cleaned[df_cleaned[\"Month\"]== 2]\ndf_cleaned_3=df_cleaned[df_cleaned[\"Month\"]== 3]\ndf_cleaned_4=df_cleaned[df_cleaned[\"Month\"]== 4]\ndf_cleaned_5=df_cleaned[df_cleaned[\"Month\"]== 5]\ndf_cleaned_6=df_cleaned[df_cleaned[\"Month\"]== 6]\ndf_cleaned_7=df_cleaned[df_cleaned[\"Month\"]== 7]\ndf_cleaned_8=df_cleaned[df_cleaned[\"Month\"]== 8]\ndf_cleaned_9=df_cleaned[df_cleaned[\"Month\"]== 9]\ndf_cleaned_10=df_cleaned[df_cleaned[\"Month\"]== 10]\ndf_cleaned_11=df_cleaned[df_cleaned[\"Month\"]== 11]\ndf_cleaned_12=df_cleaned[df_cleaned[\"Month\"]== 12]","dde3b488":"\n\ntw_1 = hero.visualization.top_words(df_cleaned_1['Description_clean']).head(20)\ntw_2 = hero.visualization.top_words(df_cleaned_2['Description_clean']).head(20)\ntw_3 = hero.visualization.top_words(df_cleaned_3['Description_clean']).head(20)\n\ntw_4 = hero.visualization.top_words(df_cleaned_4['Description_clean']).head(20)\ntw_5 = hero.visualization.top_words(df_cleaned_5['Description_clean']).head(20)\ntw_6 = hero.visualization.top_words(df_cleaned_6['Description_clean']).head(20)\n\ntw_7 = hero.visualization.top_words(df_cleaned_7['Description_clean']).head(20)\ntw_8 = hero.visualization.top_words(df_cleaned_8['Description_clean']).head(20)\ntw_9 = hero.visualization.top_words(df_cleaned_9['Description_clean']).head(20)\n\ntw_10 = hero.visualization.top_words(df_cleaned_10['Description_clean']).head(20)\ntw_11 = hero.visualization.top_words(df_cleaned_11['Description_clean']).head(20)\ntw_12 = hero.visualization.top_words(df_cleaned_12['Description_clean']).head(20)\n\nfig, axes = plt.subplots(4, 3, figsize=(18, 36))\n# plt.xticks(rotation=90)\n# fig.suptitle('Initial Pokemon - 1st Generation')\n\nsns.barplot(ax=axes[0,0], x=tw_1.index, y=tw_1.values)\naxes[0,0].set_title(\"January\")\naxes[0,0].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[0,1], x=tw_2.index, y=tw_2.values)\naxes[0,1].set_title(\"February\")\naxes[0,1].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[0,2], x=tw_3.index, y=tw_3.values)\naxes[0,2].set_title(\"March\")\naxes[0,2].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[1,0], x=tw_4.index, y=tw_4.values)\naxes[1,0].set_title(\"April\")\naxes[1,0].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[1,1], x=tw_5.index, y=tw_5.values)\naxes[1,1].set_title(\"May\")\naxes[1,1].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[1,2], x=tw_6.index, y=tw_6.values)\naxes[1,2].set_title(\"June\")\naxes[1,2].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[2,0], x=tw_7.index, y=tw_7.values)\naxes[2,0].set_title(\"July\")\naxes[2,0].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[2,1], x=tw_8.index, y=tw_8.values)\naxes[2,1].set_title(\"August\")\naxes[2,1].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[2,2], x=tw_9.index, y=tw_9.values)\naxes[2,2].set_title(\"September\")\naxes[2,2].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[3,0], x=tw_10.index, y=tw_10.values)\naxes[3,0].set_title(\"October\")\naxes[3,0].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[3,1], x=tw_11.index, y=tw_11.values)\naxes[3,1].set_title(\"November\")\naxes[3,1].tick_params(labelrotation=90)\n\nsns.barplot(ax=axes[3,2], x=tw_12.index, y=tw_12.values)\naxes[3,2].set_title(\"December\")\naxes[3,2].tick_params(labelrotation=90)","45b24f98":"df_cleaned['InvoiceDate'].max()","d0b2ef59":"compared_date=dt.datetime(2011,12,10)","b2b0dd0d":"df_rfm=df_cleaned.copy()","84c4492a":"custom_aggregation={}\ncustom_aggregation[\"InvoiceDate\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"Customer ID\"] = lambda x:x.iloc[0]\ncustom_aggregation[\"TotalPrice\"] = \"sum\"\ndf_rfm_final = df_rfm.groupby(\"Invoice\").agg(custom_aggregation)","b9b0a545":"df_rfm_final[\"Recency\"] = compared_date - df_rfm_final[\"InvoiceDate\"]\ndf_rfm_final[\"Recency\"] = pd.to_timedelta(df_rfm_final[\"Recency\"]).astype(\"timedelta64[D]\")\ndf_rfm_final.head(5)\n","271c4f11":"#plot a recency histogram\n\nplot_data = [\n    go.Histogram(\n        x=df_rfm_final['Recency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Recency'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","53f9ccf0":"custom_aggregation = {}\n\ncustom_aggregation[\"Recency\"] = [\"min\"]\ncustom_aggregation[\"InvoiceDate\"] = lambda x: len(x)\ncustom_aggregation[\"TotalPrice\"] = \"sum\"\ndf_rfm_final_final = df_rfm_final.groupby(\"Customer ID\").agg(custom_aggregation)","41a98c20":"df_rfm_final_final.head(5)","76968b71":"df_rfm_final_final.columns = [\"min_recency\", \"frequency\", \"monetary_value\"]","f432a0d6":"df_rfm_final_final.head(5)","5b14d928":"#plot a recency histogram FOR CUSTOMERS\n\nplot_data = [\n    go.Histogram(\n        x=df_rfm_final_final['min_recency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Recency'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","bd8dada8":"#plot a frequency histogram\n\nplot_data = [\n    go.Histogram(\n        x=df_rfm_final_final['frequency']\n    )\n]\n\nplot_layout = go.Layout(\n        title='Frequency'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","2552a919":"plot_data = [\n    go.Histogram(\n        x=df_rfm_final_final['monetary_value']\n    )\n]\n\nplot_layout = go.Layout(\n        title='monetary_value'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","3e00a0aa":"df_rfm_final_copy=df_rfm_final_final.copy()","7f85dafd":"def RScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 1\n    elif x <= d[p][0.50]:\n        return 2\n    elif x <= d[p][0.75]: \n        return 3\n    else:\n        return 4\n    \ndef FMScore(x,p,d):\n    if x <= d[p][0.25]:\n        return 4\n    elif x <= d[p][0.50]:\n        return 3\n    elif x <= d[p][0.75]: \n        return 2\n    else:\n        return 1","15593d52":"df_rfm_final_copy.head(5)","09958102":"quantiles = df_rfm_final_copy.quantile(q=[0.25,0.5,0.75])\nquantiles = quantiles.to_dict()","c9f06c60":"df_rfm_final_copy['r_quartile'] = df_rfm_final_copy['min_recency'].apply(RScore, args=('min_recency',quantiles,))\ndf_rfm_final_copy['f_quartile'] = df_rfm_final_copy['frequency'].apply(FMScore, args=('frequency',quantiles,))\ndf_rfm_final_copy['m_quartile'] = df_rfm_final_copy['monetary_value'].apply(FMScore, args=('monetary_value',quantiles,))\ndf_rfm_final_copy.head()","29404d55":"df_rfm_final_copy['RFMScore'] = df_rfm_final_copy.r_quartile.map(str) + df_rfm_final_copy.f_quartile.map(str) + df_rfm_final_copy.m_quartile.map(str)\ndf_rfm_final_copy['RFMScore'] = df_rfm_final_copy['RFMScore'].astype(int)\ndf_rfm_final_copy.head()","1c3c9aa4":"df_rfm_final_copy['OverallScore'] = (df_rfm_final_copy['r_quartile'] + df_rfm_final_copy['f_quartile'] + df_rfm_final_copy['m_quartile'])\/3\n\n#not sure of the line below\n# df_rfm_final_copy.groupby('OverallScore')['Recency','Frequency','Revenue'].mean()","6d432b1e":"df_rfm_final_copy.head(5)","9bc32e48":"#not sure on the segment distribution\ndf_rfm_final_copy['Segment'] = 'High-Value'\ndf_rfm_final_copy.loc[df_rfm_final_copy['OverallScore']>2,'Segment'] = 'Mid-Value' \ndf_rfm_final_copy.loc[df_rfm_final_copy['OverallScore']>3,'Segment'] = 'Low-Value' ","03e72681":"total_count=df_rfm_final_copy['min_recency'].count()\nhigh_value_count=df_rfm_final_copy[df_rfm_final_copy['Segment'] == 'High-Value']['min_recency'].count()\nmid_value_count=df_rfm_final_copy[df_rfm_final_copy['Segment'] == 'Mid-Value']['min_recency'].count()\nlow_value_count=df_rfm_final_copy[df_rfm_final_copy['Segment'] == 'Low-Value']['min_recency'].count()\n\nprint(\"high_value_count: {} ({:.2f}%)\".format(high_value_count,(high_value_count\/total_count)*100))\nprint(\"mid_value_count: {} ({:.2f}%)\".format(mid_value_count,(mid_value_count\/total_count)*100))\nprint(\"low_value_count: {} ({:.2f}%)\".format(low_value_count,(low_value_count\/total_count)*100))","88ae9bfc":"df_rfm_final_copy['monetary_value'].describe()","63f1b649":"df_rfm_final_copy['frequency'].describe()","2dbda87e":"#Revenue vs Frequency\ntx_graph = df_rfm_final_copy.copy()\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['frequency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['monetary_value'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['frequency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['monetary_value'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['frequency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['monetary_value'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Revenue\"},\n        xaxis= {'title': \"Frequency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","1215a9a2":"#Revenue Recency\n\ntx_graph = df_rfm_final_copy.copy()\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['monetary_value'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['monetary_value'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['monetary_value'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Revenue\"},\n        xaxis= {'title': \"Recency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)\n\n","921fe6c6":"# Revenue vs Frequency\ntx_graph = df_rfm_final_copy.copy()\n\nplot_data = [\n    go.Scatter(\n        x=tx_graph.query(\"Segment == 'Low-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'Low-Value'\")['frequency'],\n        mode='markers',\n        name='Low',\n        marker= dict(size= 7,\n            line= dict(width=1),\n            color= 'blue',\n            opacity= 0.8\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'Mid-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'Mid-Value'\")['frequency'],\n        mode='markers',\n        name='Mid',\n        marker= dict(size= 9,\n            line= dict(width=1),\n            color= 'green',\n            opacity= 0.5\n           )\n    ),\n        go.Scatter(\n        x=tx_graph.query(\"Segment == 'High-Value'\")['min_recency'],\n        y=tx_graph.query(\"Segment == 'High-Value'\")['frequency'],\n        mode='markers',\n        name='High',\n        marker= dict(size= 11,\n            line= dict(width=1),\n            color= 'red',\n            opacity= 0.9\n           )\n    ),\n]\n\nplot_layout = go.Layout(\n        yaxis= {'title': \"Frequency\"},\n        xaxis= {'title': \"Recency\"},\n        title='Segments'\n    )\nfig = go.Figure(data=plot_data, layout=plot_layout)\npyoff.iplot(fig)","56608810":"df_rfm_final_final_copy= df_rfm_final_copy.drop(['r_quartile','f_quartile','m_quartile'], 1)","e82809a0":"df_rfm_final_final_copy['Customer_type']=0\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.Segment == \"High-Value\"), 'Customer_type'] = 'Good Customers'\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.Segment == \"Mid-Value\"), 'Customer_type'] = 'Average Customers'\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.Segment == \"Low-Value\"), 'Customer_type'] = 'Bad Customers'\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == 111), 'Customer_type'] = 'Best Customers'\n\nfor num in [112, 113, 114, 212, 213, 214, 312, 313, 314, 412, 413, 414] : \n    df_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == num), 'Customer_type'] = 'Loyal Customers'\n    \nfor num in [121, 131, 141, 221, 231, 241, 321, 331, 341, 421, 431, 441] : \n    df_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == num), 'Customer_type'] = 'Big Spenders'\n    \ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == 311), 'Customer_type'] = 'Almost Lost'\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == 411), 'Customer_type'] = 'Lost Customers'\n\ndf_rfm_final_final_copy.loc[(df_rfm_final_final_copy.RFMScore == 444), 'Customer_type'] = 'Lost Cheap Customers'","41f44324":"df_rfm_final_final_copy","b575bced":"df_combine=df_cleaned.copy()","0142fa7d":"df_combine.head(5)","640dff98":"df_good_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Good Customers']\ndf_good_customers_merge=pd.merge(df_good_customers, df_combine, left_index=True, right_index=True)","f5007cfb":"good_customers_list=df_good_customers.index.values\nmask = df_combine['Customer ID'].isin(good_customers_list)\ndf_good_customers_merge=df_combine.loc[mask]\ndf_good_customers_dataset=df_good_customers_merge.groupby(['Year','Month']).sum()\ndf_good_customers_dataset.index = [df_good_customers_dataset.index.get_level_values(0), df_good_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_good_customer_testing=df_good_customers_dataset.droplevel(level=0)","d5c42845":"df_good_customers = df_good_customer_testing['TotalPrice']\nx_df_good_customers = df_good_customers.index \ny_df_good_customers = df_good_customers.values\nplt.bar(x_df_good_customers, y_df_good_customers, label = \"monthly sales_good_customer\",color=\"springgreen\")\nplt.xticks(rotation=90)\nplt.legend()","c15da698":"df_average_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Average Customers']\ndf_average_customers_merge=pd.merge(df_average_customers, df_combine, left_index=True, right_index=True)\n\naverage_customers_list=df_average_customers.index.values\nmask = df_combine['Customer ID'].isin(average_customers_list)\ndf_average_customers_merge=df_combine.loc[mask]\ndf_average_customers_dataset=df_average_customers_merge.groupby(['Year','Month']).sum()\ndf_average_customers_dataset.index = [df_average_customers_dataset.index.get_level_values(0), df_average_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_average_customer_testing=df_average_customers_dataset.droplevel(level=0)\n\ndf_average_customers = df_average_customer_testing['TotalPrice']\nx_df_average_customers = df_average_customers.index \ny_df_average_customers = df_average_customers.values\nplt.bar(x_df_average_customers, y_df_average_customers, label = \"monthly sales_average_customer\",color=\"yellow\")\nplt.xticks(rotation=90)\nplt.legend()","979b7ac3":"df_bad_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Bad Customers']\ndf_bad_customers_merge=pd.merge(df_bad_customers, df_combine, left_index=True, right_index=True)\n\nbad_customers_list=df_bad_customers.index.values\nmask = df_combine['Customer ID'].isin(bad_customers_list)\ndf_bad_customers_merge=df_combine.loc[mask]\ndf_bad_customers_dataset=df_bad_customers_merge.groupby(['Year','Month']).sum()\ndf_bad_customers_dataset.index = [df_bad_customers_dataset.index.get_level_values(0), df_bad_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_bad_customer_testing=df_bad_customers_dataset.droplevel(level=0)\n\ndf_bad_customers = df_bad_customer_testing['TotalPrice']\nx_df_bad_customers = df_bad_customers.index \ny_df_bad_customers = df_bad_customers.values\nplt.bar(x_df_bad_customers, y_df_bad_customers, label = \"monthly sales_bad_customer\",color=\"firebrick\")\nplt.xticks(rotation=90)\nplt.legend()","fa769959":"df_best_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Best Customers']\ndf_best_customers_merge=pd.merge(df_best_customers, df_combine, left_index=True, right_index=True)\n\nbest_customers_list=df_best_customers.index.values\nmask = df_combine['Customer ID'].isin(best_customers_list)\ndf_best_customers_merge=df_combine.loc[mask]\ndf_best_customers_dataset=df_best_customers_merge.groupby(['Year','Month']).sum()\ndf_best_customers_dataset.index = [df_best_customers_dataset.index.get_level_values(0), df_best_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_best_customer_testing=df_best_customers_dataset.droplevel(level=0)\n\ndf_best_customers = df_best_customer_testing['TotalPrice']\nx_df_best_customers = df_best_customers.index \ny_df_best_customers = df_best_customers.values\nplt.bar(x_df_best_customers, y_df_best_customers, label = \"monthly sales_best_customer\",color=\"lawngreen\")\nplt.xticks(rotation=90)\nplt.legend()","d74ad254":"df_loyal_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Loyal Customers']\ndf_loyal_customers_merge=pd.merge(df_loyal_customers, df_combine, left_index=True, right_index=True)\n\nloyal_customers_list=df_loyal_customers.index.values\nmask = df_combine['Customer ID'].isin(loyal_customers_list)\ndf_loyal_customers_merge=df_combine.loc[mask]\ndf_loyal_customers_dataset=df_loyal_customers_merge.groupby(['Year','Month']).sum()\ndf_loyal_customers_dataset.index = [df_loyal_customers_dataset.index.get_level_values(0), df_loyal_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_loyal_customer_testing=df_loyal_customers_dataset.droplevel(level=0)\n\ndf_loyal_customers = df_loyal_customer_testing['TotalPrice']\nx_df_loyal_customers = df_loyal_customers.index \ny_df_loyal_customers = df_loyal_customers.values\nplt.bar(x_df_loyal_customers, y_df_loyal_customers, label = \"monthly sales_loyal_customer\",color=\"pink\")\nplt.xticks(rotation=90)\nplt.legend()","02fd7705":"df_big_spenders_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Big Spenders']\ndf_big_spenders_customers_merge=pd.merge(df_big_spenders_customers, df_combine, left_index=True, right_index=True)\n\nbig_spenders_customers_list=df_big_spenders_customers.index.values\nmask = df_combine['Customer ID'].isin(big_spenders_customers_list)\ndf_big_spenders_customers_merge=df_combine.loc[mask]\ndf_big_spenders_customers_dataset=df_big_spenders_customers_merge.groupby(['Year','Month']).sum()\ndf_big_spenders_customers_dataset.index = [df_big_spenders_customers_dataset.index.get_level_values(0), df_big_spenders_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_big_spenders_customer_testing=df_big_spenders_customers_dataset.droplevel(level=0)\n\ndf_big_spenders_customers = df_big_spenders_customer_testing['TotalPrice']\nx_df_big_spenders_customers = df_big_spenders_customers.index \ny_df_big_spenders_customers = df_big_spenders_customers.values\nplt.bar(x_df_big_spenders_customers, y_df_big_spenders_customers, label = \"monthly sales_big_spenders_customer\",color=\"aqua\")\nplt.xticks(rotation=90)\nplt.legend()","6da2d092":"df_almost_lost_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Almost Lost']\ndf_almost_lost_customers_merge=pd.merge(df_almost_lost_customers, df_combine, left_index=True, right_index=True)\n\nalmost_lost_customers_list=df_almost_lost_customers.index.values\nmask = df_combine['Customer ID'].isin(almost_lost_customers_list)\ndf_almost_lost_customers_merge=df_combine.loc[mask]\ndf_almost_lost_customers_dataset=df_almost_lost_customers_merge.groupby(['Year','Month']).sum()\ndf_almost_lost_customers_dataset.index = [df_almost_lost_customers_dataset.index.get_level_values(0), df_almost_lost_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_almost_lost_customer_testing=df_almost_lost_customers_dataset.droplevel(level=0)\n\ndf_almost_lost_customers = df_almost_lost_customer_testing['TotalPrice']\nx_df_almost_lost_customers = df_almost_lost_customers.index \ny_df_almost_lost_customers = df_almost_lost_customers.values\nplt.bar(x_df_almost_lost_customers, y_df_almost_lost_customers, label = \"monthly sales_almost_lost_customer\",color=\"darkviolet\")\nplt.xticks(rotation=90)\nplt.legend()","9ee09a64":"df_lost_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Lost Customers']\ndf_lost_customers_merge=pd.merge(df_lost_customers, df_combine, left_index=True, right_index=True)\n\nlost_customers_list=df_lost_customers.index.values\nmask = df_combine['Customer ID'].isin(lost_customers_list)\ndf_lost_customers_merge=df_combine.loc[mask]\ndf_lost_customers_dataset=df_lost_customers_merge.groupby(['Year','Month']).sum()\ndf_lost_customers_dataset.index = [df_lost_customers_dataset.index.get_level_values(0), df_lost_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_lost_customer_testing=df_lost_customers_dataset.droplevel(level=0)\n\ndf_lost_customers = df_lost_customer_testing['TotalPrice']\nx_df_lost_customers = df_lost_customers.index \ny_df_lost_customers = df_lost_customers.values\nplt.bar(x_df_lost_customers, y_df_lost_customers, label = \"monthly sales_lost_customer\",color=\"blue\")\nplt.xticks(rotation=90)\nplt.legend()","88733f7e":"df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Lost Cheap Customers']","dc14cba6":"df_lost_cheap_customers=df_rfm_final_final_copy[df_rfm_final_final_copy['Customer_type']=='Lost Cheap Customers']\ndf_lost_cheap_customers_merge=pd.merge(df_lost_cheap_customers, df_combine, left_index=True, right_index=True)\n\nlost_cheap_customers_list=df_lost_cheap_customers.index.values\nmask = df_combine['Customer ID'].isin(lost_cheap_customers_list)\ndf_lost_cheap_customers_merge=df_combine.loc[mask]\ndf_lost_cheap_customers_dataset=df_lost_cheap_customers_merge.groupby(['Year','Month']).sum()\ndf_lost_cheap_customers_dataset.index = [df_lost_cheap_customers_dataset.index.get_level_values(0), df_lost_cheap_customers_dataset.index.map('{0[1]}\/{0[0]}'.format)]\ndf_lost_cheap_customer_testing=df_lost_cheap_customers_dataset.droplevel(level=0)\n\ndf_lost_cheap_customers = df_lost_cheap_customer_testing['TotalPrice']\nx_df_lost_cheap_customers = df_lost_cheap_customers.index \ny_df_lost_cheap_customers = df_lost_cheap_customers.values\nplt.bar(x_df_lost_cheap_customers, y_df_lost_cheap_customers, label = \"monthly sales_lost_cheap_customer\",color=\"black\")\nplt.xticks(rotation=90)\nplt.legend()","a04e91e2":"## 1.2 Duplicate values\nI will be removing duplicate entries.","aedd8da3":"# 5. Data visualization\n\nBased on the customer segmentation we have done previously, we will now take a closer look at the individual customer type.","34e3836f":"Low-value customers spend too little (cannot even see on the map. Range is from 0 to 2200 in Revenue. The greater the frequency, the lower the revenue. \n\nMid-value customers spend more than low-value customers, range around 0 to 30000 in Revenue. The same trend is also present where the greater the frequency, the lower the revenue.\n\nHigh-value customers spend alot more than mid-value customers , range around 0 to 500000 in Revenue. The trend is different for this case as the amount of revenue increases when frequency increases, which makes sense.","9f6dc946":"There seems to be a drop in average customer monthly sales from 2010 to 2011, especially in the decrease in sales for 10\/2011 and 11\/2011 when compared to 10\/2010 and 11\/2010.","ff4b4d77":"# 3 Feature engineering - Time features (for exploratory analysis .. )\n\nPreviously we have done some feature engineering regarding the description, time, price etc. Here we will be doing feature engineering specifically for the time features, which we will be using in the later sections. It is noted that this process has been done previously to compare the time distribution between the invoices with null values and the invoices without null values. The difference this time is that it will be done on a cleaned dataset.\n\nThe time features are:\n* Year\n* Month\n* MonthYear\n* Weekday\n* Day\n* Hour","8dad41a3":"Since the percentage of null values for Description is very low (0.4105%), i will drop these null values. As for CustomerID since the percentage of null values is quite high (22.7669%), before dropping these values, i will perform some exploratory analysis to look at the distribution of the null values for customerID. ","661b9381":"Overview, main points covered\n\n1. Cleaning data\n\n    * check for missing data \n    -> \"Customer ID\"\n    * duplicate values\n    * negative values\n    * doubtful entry -> cancellation without counterpart ['Quantity Cancelled']\n    * TotalPrice = Price * (Quantity - Quantity Cancelled) \n    \n    -> Removing TotalPrice < 0 (Cannot have more quantities cancelled than bought initially)\n    * Removal of outliers (delete transaction with more than 10 times of standard deviation)\n    \n    \n\n2. Feature engineering - Product tagging (for exploratory analysis for marketing)\n\n    * cleaning the description using texthero -> hero.clean, \n    * hero.top_words \n    * product colour \n\n\n3. Feature engineering - Time features (for exploratory analysis .. )\n\n    * Year\n    * Month\n    * MonthYear\n    * Weekday\n    * Day\n    * Hour\n    \n4. RFM principle\n\n    * Recency(R): Days since last purchase\n    \n    -> NOW = df.datetime(2011,12,10)\n    \n    -> Recency = NOW - rfmTable[\"Invoice Date\"]\n    \n    -> min_recency (0-25th:1, 26-50th:2, 51-75th:3, 76-100th:4 percentile)\n    \n    -> The smaller the number, the more recent the last purchase is, the better\n    * Frequency(F): Total number of purchases\n    \n    -> custom_aggregation[\"InvoiceDate\"] = lambda x: len(x)\n    -> 0-25th:4, 26-50th:3, 51-75th:2, 76-100th percentile:1 \n    * Monetary value (M): Total money this customer spent    \n       \n    -> custom_aggregation[\"TotalPrice\"] = \"sum\"\n    -> 0-25th:4, 26-50th:3, 51-75th:2, 76-100th percentile:1 \n    * Customer segmentation\n    \n    -> segmented_rfm['RFMScore'] = segmented_rfm.r_quartile.map(str) + segmented_rfm.f_quartile.map(str) + segmented_rfm.m_quartile.map(str)\n    \n5. Data visualization\n    * Total Sales\n    * Countries sold\n    -> Customer distribution by country\n    * Customers' segments\n    * Most sold products\n    * Best products monthly sale\n    * Time features \n    \n    -> Hourly sales\n    \n    -> Weekday sales\n    \n    -> Day of the month sales\n    * Frequency distribution\n    \n    -> Recency\n    -> Frequency\n    -> Monetary\n    * Box plot    \n    \n6. Clustering (The elbow method)","88ba2040":"I think there is some problems with the dataset regarding the distribution of weekday sales. There seems to be significantly less invoices on fridays compared to the other days.\n\nThe weekday distribution for the first graph is quite different from the second graph. For the first graph, the day with the highest and lowest(excluding friday) number of invoices are sunday and saturdays. For the second graph, the day with the highest and lowest(excluding friday) number of invoices are wednesday and thursday respectively.","ad4d7dce":"## 4.1 Recency - feature engineering","a900d39c":"In the above function, I checked the two cases:\n\n1. a cancel order exists without counterpart\n2. there's at least one counterpart with the exact same quantity","275144ee":"We will be dropping the cancellation orders with counterpart as it is justified. Since the doubtful entries (cancellation without counterpart) covers a small percentage (0.24%), we will also drop those entries too.\n\nNow we will check to see the number of entries that correspond to cancellations and that have not been deleted with the previous filter:","790cfe4d":"As you can see from the table above, for the first 9 rows, we can see cancellations made by the same person (Customer ID: 16321.0). I decide to see whether there are orders bought before with exactly the same attributes with exception of invoice number and invoiceDate. It is noted this is not present for the first 9 rows as the dataset starts at 2009-12-01 and hence the orders are likely to be bought previously on a date earlier and thus not present in the dataset.\n","0adbac86":"This is what the dataframe looks like now after I have add the attributes \"r_quartile\",\"f_quartile\",\"m_quartile\",\"RFMScore\" and \"OverallScore\".","fe1c7d01":"\nThe loyal customer monthly sale dropped alittle from 2010 to 2011. It is most likely due to the poor marketing campaign targetted at the customer segment.","b371a629":"The graph is expected where most customers' monetary value belongs in the 0-1000 dollar range and the number of customers belonging to the higher monetary value brackets sharply decrease.","7773064b":"We will be combining the individual quartile numbers for recency, frequency and monetary value together.","39550658":"Since date is not really very important we are just going to take the first date of the single purchase","d04e8219":"These are the cases where the number of cancellations are more than the purchase, which is wierd. \n\n** For now, we will not yet delete these. ** To be edited in the future","ab3a21ad":"## Can save the file here","937364dd":"It seems like the sales made by good customers is increasing from 2010 to 2011 comparatively across all months, which is a good sign. The most noticable would be the increase in sales would be 9\/2011, 10\/2011 and 11\/2011 compared to 9\/2010, 10\/2010 and 11\/2010. This trend is not present for all the other customer segments. Working with the marketing team would be useful in determining the reason for this improvement in sales and this info could be use to further increase sales made by good customers for the following years.","f1524b5b":"## 1.5 Removal of outliers\n\nWe determine the cut-off for identifying outliers as more than 3 standard deviations from the mean.","659605c7":"The stock codes for the wrong matching is quite unexpected. Most of the StockCodes are expected(contains all numbers) but some of the stockCodes contains alphabets e.g. A, B, C, D, E, G, L, J, P, S, W which are located at the end of the StockCode. Upon closer inspection at the description, the alphabets are related to the product. For example, if you look at 233th-240th mismatch invoices, they all contain alphabets at the end of their stockCode and the alphabets in this case refers to the letters on the \"BLING KEY RING\". ","46ba61d5":"Sort according to the RFM scores from the best customers (score 111):\n![image.png](attachment:image.png)\nSource: Blast Analytics Marketing","2693310e":"Since we have already obtained the RFMScore, we will drop the attributes, \"r_quartile\",\"f_quartile\" and \"m_quartile\".","17525475":"Upon closer inspection, I just want to check whether the particular customer ID == 14695 where the stockCode ends with the letter \"L\", as you can see the order is not matched.","3957b45a":"### 1.3.4 Data cleaning for cancellation\n\nSince some of the cancellations are not matched, we are going to do datacleaning specifically to only those that matched. It is also important to note that for some of the orders that are not matched, it is most likely due to the fact that the order is placed before 2009-12-01 which is the date that the dataset starts.\n\n** The below code takes 1 hour 20 mins to load **","cdf1a0c1":"Below shows the distribution of the total price of individual purchases through the use of pie chart:","856726c8":"The bad customer monthly sale dropped drastically at 12\/2010. Since these are bad customers who are of low value, it may be a waste of resources for the marketing team to target the segment to increase in sales and it would be more efficient to target higher value customers.","2a55cac2":"## 2.1 Cleaning the description","d9b81fd8":"### 1.3.2 Cancelled orders\n\n2.3% of the orders are cancelled.","48f38000":"https:\/\/towardsdatascience.com\/data-driven-growth-with-python-part-2-customer-segmentation-5c019d150444\n\nWe can also create an overall score using kmeans-clustering","7bc33b41":"Below shows the distribution of recency. The distribution looks alittle bit wierd as for days 340-349, the number of invoices with the particular range of recency is 0. The distribution of recency follows a periodic trend where there is a periodic period of peaks and troughs. A possible explanation could be due to the holiday season. If you refer to the month distribution bar graph in 1.1.2, you can see that the amount of invoices made (which is correlated to sales) peaks in the later part of the year and slumps at the start of the year, therefore it is able to justify the periodic trend of the bar plot for recency.","cab84dee":"renaming the columns","6c4788da":"# 2. Feature engineering - Product tagging (for exploratory analysis for marketing)\n","5f6c98a0":"We are able to see that \"set\", \"red\", \"bag\", \"heart\" and \"pink\" are the most common words that appear in the description. However, a better approach will be to look at the top words that appear for respective months since some months have special holidays that may alter the consumer's spending patterns. Below show the top words that appear in the description for each month.","9c1bd023":"# 4 RFM principle\n\nRFM stands for Recency, Frequency and Monetary value respectively.\n\n* Recency(R): Days since last purchase\n* Frequency(F): Total number of purchases\n* Monetary value (M): Total money this customer spent\n\nRFM is a model based on customer segmentation and it helps to divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.","f7f8ecb8":"As you can see from the two graphs, the hourly time distribution for the data with null customerID values is significantly different from the hourly time distribution for the data with no null values.\n\nFor example, for the 12th hour, the number of invoices is the highest in the second graph (data with no null values) whereas there is low numbers of invoices shown in the first graph.\n\nThe time distribution for the second graph is logical as the number of invoices is at its peak at around 12pm - 1pm which is understandable since most people will be awake and economic activity is the highest.\n\nThe time distribution for the first graph is alittle confusing as it peaks between 3pm-4pm which is the off-peak hour.","b77637cc":"For the monthly sale, the distribution is relatively similar, where there are troughs in the early parts of the year and peaks in the late parts of the year.","fd1e5abb":"It is noted that for negative quantity numbers, the invoice number contains the letter \"C\", which indicates a cancellation.","957d16e6":"## 1.4 TotalPrice of each purchase\n\nI created a new variable to indicate the total price of each purchase. We will be sorting the df according to customerID.","9f7a8012":"As we can see as the dates increase, the number of customers decrease, This shows that there is actually a high number of returning customers\/frequent customers, which is a good sign or it could also mean that alot of the customers are new customers (this is most likely the case when you look at the frequency graph below)","96c966bc":"It can be seen that the vast majority of orders concern relatively large purchases given that 44% of purchases are between 200 and 500. Low cost purhcases (e.g. 0<x<50 and 50<x<100) form only a small component of individual purchases, 7 percent and 6 percent respectively.","cf403e2c":"\nTo verify that this is true for all the invoices, I decide to locate the entries that indicate a negative quantity and check if there is an invoice with the exact same attributes except invoice number and invoiceDate.\n\nSince we are aware that for invoices made on date 2009-12-01 is most likely not valid due to the fact that it is most likely bought before 2009-12-01 and hence not present in the dataset, we will just take a look at the 200th-240th wrong matching entries.","6544982d":"The frequency graph shows that alot of customers only went to the store a few times and there are very little loyal cusstomers (people who frequent the shop)","e277b9a9":"### 1.3.3 Discounted orders\nSome of the negative quanity invoices are discounted too, as we can see in the description == \"Discount\".","84939dc2":"It seems like the sales made by big spenders customers is increasing from 2010 to 2011 comparatively across all months, which is a good sign. The most noticable would be the increase in sales would be 9\/2011, 10\/2011 and 11\/2011 compared to 9\/2010, 10\/2010 and 11\/2010. This trend is not present for all the other customer segments. Working with the marketing team would be useful in determining the reason for this improvement in sales and this info could be use to further increase sales made by big spenders customers for the following years. ","b3239324":"I think need to merge both datasets","813b4c87":"Each entry of the dataframe indicates prizes for a single kind of product. Hence, orders are split on several lines. I collect all the purchases made during a single order to recover the total order prize:","80c92a49":"Surprisingly quanity has a negative number.","7b438baa":"In the dataframe, products are uniquely identified through the StockCode variable. A shrort description of the products is given in the Description variable. In this section, I intend to use the content of this latter variable in order to group the products into different categories.","ec37e94d":"Overall Score: average out the quartile numbers","5c2f921b":"We will first look at the description of the dataset, especially the minimum and maximum values of the features.","a23a60e8":"## 4.2 Frequency & Monetary Value - feature engineering","283620bb":"Based on the dataset the maximum invoice date is '2011-12-09 12:50:00'. Therefore, we set the compared_date used to calculate the recency of the invoice as 2011-12-10 which is a day after the maximum date.","d42a943c":"I also created a new attribute called \"Segment\" which segments the customers according to their OverallScore value respectively","1c320df1":"### 1.1.2 Exploratory analysis for missing data\n\nBar plots to compare the time distribution for df with null customerID values and df without null values.\n\nList of time distributions undertaken:\n1. Year\n2. Month\n3. Weekday\n4. Hour","6e2d40a0":"# 1. Data Cleaning\n## 1.1 Missing data\n### 1.1.1 Checking for missing data\nAs you can see from the results, there are missing data present for the description and customerID","c8cb19ae":"For the yearly sale, there is just three years and therefore it hard to make a good comparison. The overall distribution is quite similar where there are the least invoices for 2009 and relatively similar numbers for 2010 and 2011.\n\nI will remove the null values for customerID for now.","4b718a75":"## 4.3 Customer segregation\n\nHere we will start segregating based on the percentile of the attribute. The attibutes are min_recency, frequency and monetary_value for 1. Recency, 2. Frequency and 3. Monetary value respectively. Based on the respective percentile, we will be allocating a score from 1 to 4. 1 would be the best score and 4 would be the worst.\n\n* For 1. Recency, the smaller the min_recency, the more recent the last purchase is, and hence it is better. \n\n        -> min_recency (0-25th:1, 26-50th:2, 51-75th:3, 76-100th:4 percentile)\n\n* For 2. Frequency and 3. Monetary value, the larger their respective attributes the better.\n\n        -> 0-25th:4, 26-50th:3, 51-75th:2, 76-100th percentile:1 ","020e3ea0":"## 1.3 Negative values\n### 1.3.1 Looking at the negative values","dd4ca48e":"Create another table for better backup incase code is wrong.","16a191c1":"# Getting to know the attributes\n\nThe dataframe contains 8 attributes:\n* Invoice: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n* StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n* Description: Product (item) name. Nominal.\n* Quantity: The quantities of each product (item) per transaction. Numeric.\n* InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n* Price: Unit price. Numeric. Product price per unit in sterling (\u00c2\u00a3).\n* CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n* Country: Country name. Nominal. The name of the country where a customer resides.\n","7f54b39b":"It seems like the sales made by best customers is increasing from 2010 to 2011 comparatively across all months, which is a good sign. The most noticable would be the increase in sales would be 9\/2011, 10\/2011 and 11\/2011 compared to 9\/2010, 10\/2010 and 11\/2010. This trend is not present for all the other customer segments. Working with the marketing team would be useful in determining the reason for this improvement in sales and this info could be use to further increase sales made by best customers for the following years. This is very important as these customers are of the highest value."}}