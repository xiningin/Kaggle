{"cell_type":{"f01bb25e":"code","b3069005":"code","c4cadb0d":"code","fea9ef3a":"code","5cd843a1":"code","43d33d67":"code","bad380a5":"code","3d26b12b":"code","969629a3":"code","821abc7a":"code","d02fb605":"code","b6078068":"code","74953c96":"code","aec02b75":"code","b48e3abc":"code","ef3718e7":"code","6c538703":"code","15f21785":"code","f4215dca":"code","7f08ef4a":"code","1463db3e":"code","bfcdd4d0":"code","250681e0":"code","557f8b67":"code","b6802b70":"code","1b148a57":"code","1047a664":"code","77138313":"code","79a127f6":"code","ed044029":"code","f7b0e5e4":"code","33788e45":"code","6a547b9d":"code","bc161fc2":"code","dc92407a":"code","3007bc39":"code","be08f7fc":"code","6191c028":"code","e4362a7d":"code","28f9a0f8":"code","7f6719bf":"code","3dfe4515":"code","4219e37e":"code","a087ecad":"code","a843c229":"code","238301dc":"code","1ef3370a":"code","4e05d8d7":"code","c05fdfd1":"code","e90ab510":"code","9390d856":"code","a3361a12":"code","3bc795f1":"code","85102893":"code","c1e04cb9":"code","d4a1d6e8":"code","9a51354f":"code","641233e4":"code","9a3058a5":"code","676850e2":"code","7c118576":"code","61a8dc84":"code","ad6a38a0":"code","e5627655":"code","7fc59215":"code","b5094042":"code","8ac6e4bf":"markdown","16fcfacb":"markdown","24f457da":"markdown","e30d88c0":"markdown","0b068f33":"markdown","c0bd3462":"markdown","e3254e50":"markdown","fa08c53e":"markdown"},"source":{"f01bb25e":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')","b3069005":"iris = pd.read_csv(\"..\/input\/irisdata\/iris_data.csv\")","c4cadb0d":"iris.head()","fea9ef3a":"iris.info()","5cd843a1":"iris.Species.value_counts()","43d33d67":"iris.describe()","bad380a5":"import matplotlib.pyplot as plt\n%matplotlib inline\nsns.FacetGrid(iris,hue='Species',size=5)\\\n.map(plt.scatter,'Sepal.Length','Sepal.Width')\\\n.add_legend()","3d26b12b":"df = pd.DataFrame(iris) ","969629a3":"df.isnull().sum()","821abc7a":"print(iris[iris.isnull().any(axis=1)].head())","d02fb605":"df.mean()","b6078068":"df.mode()","74953c96":"df.median()","aec02b75":"df['Sepal.Length'] = df['Sepal.Length'].fillna(df['Sepal.Length'].mean())","b48e3abc":"df['Sepal.Width'] = df['Sepal.Width'].fillna(df['Sepal.Width'].mean())","ef3718e7":"df['Petal.Length'] = df['Petal.Length'].fillna(df['Petal.Length'].mean())","6c538703":"df.isnull().sum()","15f21785":"iris_setosa = iris[iris['Species'] == 'setosa']\niris_versicolor = iris[iris['Species'] == 'versicolor']\niris_virginica = iris[iris['Species'] == 'virginica']\n","f4215dca":"sns.FacetGrid(iris, hue=\"Species\", height=5) \\\n   .map(sns.distplot, \"Sepal.Length\") \\\n   .add_legend();\nsns.FacetGrid(iris, hue=\"Species\", height=5) \\\n   .map(sns.distplot, \"Sepal.Width\") \\\n   .add_legend();\nsns.FacetGrid(iris, hue=\"Species\", height=5) \\\n   .map(sns.distplot, \"Petal.Length\") \\\n   .add_legend();\nsns.FacetGrid(iris, hue=\"Species\", height=5) \\\n   .map(sns.distplot, \"Petal.Width\") \\\n   .add_legend();\nplt.show()","7f08ef4a":"plt.figure(figsize=(10,10))\ncounts, bin_edges = np.histogram(iris_setosa['Petal.Length'],\n                                 bins = 10, density = True)\npdf = counts\/sum(counts)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label = 'Setosa PDF')\nplt.plot(bin_edges[1:], cdf, label = 'Setosa CDF')\ncounts, bin_edges = np.histogram(iris_versicolor['Petal.Length'],\n                                 bins = 10, density = True)\npdf = counts\/sum(counts)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label = 'Versicolor PDF')\nplt.plot(bin_edges[1:], cdf, label = 'Versicolor CDF')\ncounts, bin_edges = np.histogram(iris_virginica['Petal.Length'],\n                                 bins = 10, density = True)\npdf = counts\/sum(counts)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label = 'Virginica PDF')\nplt.plot(bin_edges[1:], cdf, label = 'Virginica CDF')\nplt.legend()\nplt.show()","1463db3e":"df","bfcdd4d0":"df1 = df.iloc[:,:4]","250681e0":"df1","557f8b67":"df2 = df.iloc[:,4:]","b6802b70":"df2.head()","1b148a57":"df2=df2.replace(to_replace =\"setosa\", \n                 value =0)\ndf2=df2.replace(to_replace =\"versicolor\", \n                 value =1)\ndf2=df2.replace(to_replace =\"virginica\", \n                 value =2)","1047a664":"df2.head()","77138313":"from  sklearn import  datasets\niris=datasets.load_iris()","79a127f6":"x=df1\ny=df2","ed044029":"print(x.shape)\nprint(y.shape)","f7b0e5e4":"df1.head()","33788e45":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.5)","6a547b9d":"from sklearn import tree\nclassifier=tree.DecisionTreeClassifier()","bc161fc2":"# from sklearn import neighbors\n# classifier=neighbors.KNeighborsClassifier()","dc92407a":"classifier.fit(x_train,y_train)","3007bc39":"predictions=classifier.predict(x_test)","be08f7fc":"predictions","6191c028":"from sklearn.metrics import accuracy_score\nacc=(accuracy_score(y_test,predictions))\nprint('The accuracy of Decision Tree Classifier is:', acc)","e4362a7d":"from sklearn import svm\nfrom sklearn import metrics \nsv = svm.SVC() #select the algorithm\nsv.fit(x_train,y_train) # we train the algorithm with the training data and the training output\ny_pred = sv.predict(x_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","28f9a0f8":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns","7f6719bf":"tf.compat.v1.disable_eager_execution()","3dfe4515":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()","4219e37e":"seed = 1234\nnp.random.seed(seed)\ntf.set_random_seed(seed)","a087ecad":"dataset = pd.read_csv('..\/input\/irisdataset\/Iris_Dataset.csv')","a843c229":"dataset.head()","238301dc":"g=sns.pairplot(dataset,size = 2.5)","1ef3370a":"cols = dataset.columns\nfeatures = cols[0:4]\nlabels = cols[4]\nprint(features)\nprint(labels)","4e05d8d7":"data_norm = pd.DataFrame(dataset)\n\nfor feature in features:\n    dataset[feature] = (dataset[feature] - dataset[feature].mean())\/dataset[feature].std()","c05fdfd1":"print(\"Averages\")\nprint(dataset.mean())\n\nprint(\"\\n Deviations\")\nprint(pow(dataset.std(),2))","e90ab510":"#Shuffle The data\nindices = data_norm.index.tolist()\nindices = np.array(indices)\nnp.random.shuffle(indices)\nX = data_norm.reindex(indices)[features]\ny = data_norm.reindex(indices)[labels]","9390d856":"dataset = pd.get_dummies(dataset, columns=['Species']) #\u00a0One Hot Encoding\nvalues = list(dataset.columns.values)","a3361a12":"y = dataset[values[-3:]]\ny = np.array(y, dtype='float32')\nX = dataset[values[1:-3]]\nX = np.array(X, dtype='float32')","3bc795f1":"# Shuffle Data\nindices = np.random.choice(len(X), len(X), replace=False)\nX_values = X[indices]\ny_values = y[indices]","85102893":"# Creating a Train and a Test Dataset\ntest_size = 10\nX_test = X_values[-test_size:]\nX_train = X_values[:-test_size]\ny_test = y_values[-test_size:]\ny_train = y_values[:-test_size]","c1e04cb9":"sess = tf.Session()\ninterval = 25\nepoch = 500","d4a1d6e8":"X_data = tf.placeholder(shape=[None, 4], dtype=tf.float32)\ny_target = tf.placeholder(shape=[None, 3], dtype=tf.float32)","9a51354f":"hidden_layer_nodes = 8","641233e4":"# nn layer\nw1 = tf.Variable(tf.random_normal(shape=[4,hidden_layer_nodes])) \nb1 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes]))   \nw2 = tf.Variable(tf.random_normal(shape=[hidden_layer_nodes,3])) \nb2 = tf.Variable(tf.random_normal(shape=[3]))","9a3058a5":"hidden_output = tf.nn.relu(tf.add(tf.matmul(X_data, w1), b1))\nfinal_output = tf.nn.softmax(tf.add(tf.matmul(hidden_output, w2), b2))","676850e2":"loss = tf.reduce_mean(-tf.reduce_sum(y_target * tf.log(final_output), axis=0))","7c118576":"optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)","61a8dc84":"init = tf.global_variables_initializer()\nsess.run(init)","ad6a38a0":"# Training\nls = {}\nprint('Training the model...')\nfor i in range(1, (epoch + 1)):\n    sess.run(optimizer, feed_dict={X_data: X_train, y_target: y_train})\n    if i % interval == 0:\n        l = sess.run(loss, feed_dict={X_data: X_train, y_target: y_train})\n        print('Epoch', i, '|', 'Loss:', l)\n        ls[i] = l","e5627655":"import matplotlib.pyplot as plt\niterations = list(ls.keys())\ncosts = list(ls.values())\nplt.plot(iterations,costs)  \nplt.ylabel('cost')\nplt.xlabel('iterations (per fifties)')\nplt.title(\"Learning rate = \" + str(.001)) \nplt.show() ","7fc59215":"fo = []","b5094042":"#\u00a0Prediction\nfor i in range(len(X_test)):\n    print('Actual:', y_test[i], 'Predicted:', np.rint(sess.run(final_output, feed_dict={X_data: [X_test[i]]})))\n    fo.append(np.rint(sess.run(final_output, feed_dict={X_data: [X_test[i]]})))","8ac6e4bf":"## Neural Network Classification","16fcfacb":"## univarent","24f457da":"## SVM Classification","e30d88c0":"https:\/\/medium.com\/bycodegarage\/a-comprehensive-guide-on-handling-missing-values-b1257a4866d1","0b068f33":"### Dealing with null values","c0bd3462":"## Decisioin Tree Classification","e3254e50":"https:\/\/medium.com\/analytics-vidhya\/exploratory-data-analysis-uni-variate-analysis-of-iris-data-set-690c87a5cd40","fa08c53e":"## classification"}}