{"cell_type":{"b58571ff":"code","8d988d2b":"code","f42fc35c":"code","3992320f":"code","49b2ff9a":"code","d30207e2":"code","a5520ba5":"code","d1a54756":"code","be0b92ec":"code","b682b176":"code","e49e65c7":"code","3d235bae":"code","a9903469":"code","0b24b3b7":"code","05e750a8":"code","dabe9a0a":"code","d685163b":"code","dfa17a50":"code","22b8f7e4":"code","6f796e4a":"code","c10fc7da":"code","fec8aefb":"code","6d0494ee":"code","bd1e3732":"code","ce1e1f07":"code","37ec26c4":"code","1922841b":"code","af3c9ec8":"code","785d8a30":"code","7bae991c":"code","2a04a4c4":"code","4cc1fe7d":"code","dab3f731":"code","bebf4f99":"code","8ba09623":"code","090f64cf":"code","ee0cc48c":"code","fdd726cf":"code","726275bb":"markdown","e589e0ea":"markdown","12f46d85":"markdown","8a0a9443":"markdown","d1f3465d":"markdown","929ae114":"markdown","2f0e7b30":"markdown","f57ae256":"markdown","85d7a243":"markdown","0c4b957d":"markdown","8c36630b":"markdown","2c652e4a":"markdown","a817ef43":"markdown","623d96f5":"markdown","2ba4945b":"markdown","c1d4007b":"markdown","e5b637a6":"markdown","664e0d72":"markdown","b331706b":"markdown","3ef51eb6":"markdown","09bd75a2":"markdown","2532be66":"markdown","a212295d":"markdown","9d00badf":"markdown","bf1a3afa":"markdown","e9332cb1":"markdown","2849ec69":"markdown","6c0f0701":"markdown","83bb7a8a":"markdown","45e34b3e":"markdown","39ee6680":"markdown","21be9a25":"markdown","ac7afcb4":"markdown","cb7a289f":"markdown","04ff4142":"markdown"},"source":{"b58571ff":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nplt.style.use('ggplot')\nsns.set_style()","8d988d2b":"data = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv', sep=',')","f42fc35c":"data.head()","3992320f":"print('Rows     :', data.shape[0])\nprint('Columns  :', data.shape[1])\nprint('\\nFeatures :', data.columns.tolist())\nprint('\\nUnique values :\\n', data.nunique())","49b2ff9a":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Valores em falta', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n    \n# Valores que est\u00e3o faltando nos dados\nmissing_values = missing_values_table(data)","d30207e2":"data.info()","a5520ba5":"#Data Manipulation\n\n#Replacing spaces with null values in total charges column\ndata['TotalCharges'] = data[\"TotalCharges\"].replace(\" \", np.nan)\n\n#Dropping null values from total charges column which contain .15% missing data \ndata = data[data[\"TotalCharges\"].notnull()]\ndata = data.reset_index()[data.columns]\n\n#convert to float type\ndata[\"TotalCharges\"] = data[\"TotalCharges\"].astype(float)","d1a54756":"data.info()","be0b92ec":"f, axes = plt.subplots(2, 3, sharey=False, sharex=False, figsize=(10,4))\n\ndata['OnlineSecurity'].value_counts(ascending=True).plot.barh(title='OnlineSecurity', ax=axes[0,0])\ndata['OnlineBackup'].value_counts(ascending=True).plot.barh(title='OnlineBackup', ax=axes[0,1])\ndata['DeviceProtection'].value_counts(ascending=True).plot.barh(title='DeviceProtection', ax=axes[0,2])\ndata['TechSupport'].value_counts(ascending=True).plot.barh(title='TechSupport', ax=axes[1,0])\ndata['StreamingTV'].value_counts(ascending=True).plot.barh(title='StreamingTV', ax=axes[1,1])\ndata['StreamingMovies'].value_counts(ascending=True).plot.barh(title='StreamingMovies', ax=axes[1,2])\nplt.tight_layout()","b682b176":"replace_cols = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport','StreamingTV', 'StreamingMovies', 'MultipleLines']\n\nfor i in replace_cols :\n    data[i]  = data[i].replace({'No internet service' : 'No'})\n    data[i]  = data[i].replace({'No phone service' : 'No'})","e49e65c7":"f, axes = plt.subplots(2, 3, sharey=False, sharex=False, figsize=(10,4))\n\ndata['OnlineSecurity'].value_counts(ascending=True).plot.barh(title='OnlineSecurity', ax=axes[0,0])\ndata['OnlineBackup'].value_counts(ascending=True).plot.barh(title='OnlineBackup', ax=axes[0,1])\ndata['DeviceProtection'].value_counts(ascending=True).plot.barh(title='DeviceProtection', ax=axes[0,2])\ndata['TechSupport'].value_counts(ascending=True).plot.barh(title='TechSupport', ax=axes[1,0])\ndata['StreamingTV'].value_counts(ascending=True).plot.barh(title='StreamingTV', ax=axes[1,1])\ndata['StreamingMovies'].value_counts(ascending=True).plot.barh(title='StreamingMovies', ax=axes[1,2])\nplt.tight_layout()","3d235bae":"#replace values\ndata[\"SeniorCitizen\"] = data[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})","a9903469":"f, axes = plt.subplots(3, 2, sharey=False, sharex=False, figsize=(12,12))\n\ndata['Contract'].value_counts(ascending=True, normalize=True).plot.barh(title='Contracts', ax=axes[0,0])\ndata['SeniorCitizen'].value_counts(ascending=True, normalize=True).plot.barh(title='SeniorCitizen', ax=axes[0,1])\nsns.countplot(x='gender', hue='Contract', data=data, orient='v', ax=axes[1,0])\nsns.countplot(x='Contract', hue='SeniorCitizen', data=data, orient='v', ax=axes[1,1])\n\ndata.groupby('gender')['SeniorCitizen'].value_counts(ascending=True, normalize=True).plot.barh(title='SeniorCitizen by gender', ax=axes[2,0])\ndata.groupby('Contract')['gender'].value_counts(ascending=True, normalize=False).unstack().plot.bar(title='Contracts by gender', ax=axes[2,1])\nplt.tight_layout()","0b24b3b7":"data['Contract'].value_counts(ascending=True, normalize=True)","05e750a8":"#Tenure to categorical column\ndef tenure_lab(data):    \n    if data['tenure'] <= 12 :\n        return \"Tenure_0-12\"\n    elif (data['tenure'] > 12) & (data['tenure'] <= 24 ):\n        return \"Tenure_12-24\"\n    elif (data['tenure'] > 24) & (data['tenure'] <= 48) :\n        return \"Tenure_24-48\"\n    elif (data['tenure'] > 48) & (data['tenure'] <= 60) :\n        return \"Tenure_48-60\"\n    elif data['tenure'] > 60 :\n        return \"Tenure_60-gt\"\ndata[\"tenure_group\"] = data.apply(lambda data:tenure_lab(data), axis = 1)","dabe9a0a":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=False, sharex=False, figsize=(10,4))\n\ndata['tenure'].hist(density=False, ax=ax1)\ndata['tenure_group'].value_counts(ascending=False, sort=True, normalize=True).sort_index().plot.bar(ax=ax2)\n\nax1.set_xlabel(r'Tenure')\nax1.set_ylabel(r'# months with the company')\nplt.tight_layout()","d685163b":"order_tenure = list(data['tenure_group'].value_counts(ascending=False, sort=True, normalize=True).sort_index().index)","dfa17a50":"data[data['Churn']=='Yes']['TotalCharges'].hist();\ndata[data['Churn']=='No']['TotalCharges'].hist();","22b8f7e4":"f, axes = plt.subplots(3, 2, sharey=False, sharex=False, figsize=(16,16))\n\nsns.countplot(x='tenure_group', hue='Churn', data=data, order=order_tenure, orient='v', ax=axes[0,0])\nsns.countplot(x='Churn', hue='gender', data=data, orient='v', ax=axes[0,1])\nsns.countplot(x='Churn', hue='SeniorCitizen', data=data, orient='v', ax=axes[1,0])\nsns.distplot(data[data['Churn']=='Yes']['MonthlyCharges'], kde=False, hist=True, norm_hist=False, ax=axes[1,1], label='Churn')\nsns.distplot(data[data['Churn']=='No']['MonthlyCharges'], kde=False, hist=True, norm_hist=False, ax=axes[1,1], label='Non-Churn')\n\nsns.distplot(data[data['Churn']=='Yes']['TotalCharges'], kde=False, hist=True, norm_hist=False, ax=axes[2,0], label='Churn')\nsns.distplot(data[data['Churn']=='No']['TotalCharges'], kde=False, hist=True, norm_hist=False, ax=axes[2,0], label='Non-Churn')\n\npivot = pd.pivot_table(data, values=['MonthlyCharges', 'TotalCharges'], index=['tenure_group', 'Churn'])\n\npivot.unstack()['TotalCharges'].plot.bar(ax=axes[2,1], title='Average Total Charges by Tenure groups')\n\n\naxes[0,0].set_title('Churn customers by Tenure groups')\naxes[0,1].set_title('Churn customers by gender')\naxes[1,0].set_title('Churn customers by SeniorCitizen')\naxes[1,1].set_title('Distribution of MonthlyCharges by Churn')\naxes[2,0].set_title('Distribution of TotalCharges by Churn')\n\naxes[1,1].legend()\naxes[2,0].legend()\naxes[2,1].legend()\nplt.tight_layout()","6f796e4a":"from sklearn.preprocessing import LabelEncoder\n\ndataobject=data.select_dtypes(['object'])\n\ndef uni(columnname):\n    print(columnname,\"--\" ,data[columnname].unique())\n\nfor i in range(1,len(dataobject.columns)):\n    uni(dataobject.columns[i])\n    \ndef labelencode(columnname):\n    data[columnname] = LabelEncoder().fit_transform(data[columnname])\n    \nfor i in range(1,len(dataobject.columns)):\n    labelencode(dataobject.columns[i])\n        \nfor i in range(1,len(dataobject.columns)):\n     uni(dataobject.columns[i])","c10fc7da":"df = data.copy()\ndrop_list = ['customerID', 'gender', 'Dependents', 'PhoneService', 'DeviceProtection', 'TechSupport',\n             'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'PaymentMethod']\n\ndf['Contract_0'] = ((df['Contract']==0).values).astype(int)\ndf['Contract_1'] = ((df['Contract']==1).values).astype(int)\ndf['Contract_2'] = ((df['Contract']==2).values).astype(int)\n\ndf['tenure_0'] = ((df['tenure_group']==0).values).astype(int)\ndf['tenure_1'] = ((df['tenure_group']==1).values).astype(int)\ndf['tenure_2'] = ((df['tenure_group']==2).values).astype(int)\ndf['tenure_3'] = ((df['tenure_group']==3).values).astype(int)\ndf['tenure_4'] = ((df['tenure_group']==4).values).astype(int)\n\ndf = df.drop(drop_list, axis=1)\ndf = df.drop(['Contract', 'tenure', 'tenure_group'], axis=1)","fec8aefb":"f, (ax1,ax2) = plt.subplots(1, 2, sharey=False, sharex=False, figsize=(16,6))\n\ncorr = df.corr(method='pearson')\n\nsns.heatmap(data.corr(method='pearson'), cmap=plt.cm.inferno_r, ax=ax1)\nsns.heatmap(corr, cmap=plt.cm.inferno_r, ax=ax2)\nplt.tight_layout()","6d0494ee":"from sklearn.model_selection import train_test_split\ndata_to_train = data.drop(['customerID'], axis=1).copy()\n\ntrain_set, validation_set = train_test_split(data_to_train.copy(), test_size=0.30)\n\n#to perform cross-validation\ntarget_to_train = data_to_train['Churn']\ndata_to_train = data_to_train.drop(['Churn'], axis=1)\n\n#to perform holdout-set\ntrain_target = train_set['Churn']\ntrain_set = train_set.drop(['Churn'], axis=1)\n\nvalidation_target = validation_set['Churn']\nvalidation_set = validation_set.drop(['Churn'], axis=1)","bd1e3732":"x, y = train_set, train_target","ce1e1f07":"from sklearn.linear_model import Lasso, LassoCV\n\nlassocv = LassoCV(alphas=None, cv=40, max_iter=100000, normalize=True)\nlassocv.fit(data_to_train, target_to_train);","37ec26c4":"from sklearn.ensemble import RandomForestClassifier\n\n# Fit RandomForest Classifier\nclf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nclf.fit(x, y);","1922841b":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x, y);","af3c9ec8":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='linear', C=0.01) \nsvm.fit(x, y);","785d8a30":"f, axes = plt.subplots(2, 2, sharey=False, sharex=False, figsize=(16,10))\n\nimp1 = pd.Series(data=clf.feature_importances_, index=x.columns).sort_values(ascending=False)\nimp2 = pd.Series(data=lassocv.coef_, index=x.columns).sort_values(ascending=False)\nimp3 = pd.Series(data=logreg.coef_[0], index=x.columns).sort_values(ascending=False)\nimp4 = pd.Series(data=svm.coef_[0], index=x.columns).sort_values(ascending=False)\n\nsns.barplot(y=imp1.index, x=imp1.values, orient='h', ax=axes[0,0])\nsns.barplot(y=imp2.index, x=imp2.values, orient='h', ax=axes[0,1])\nsns.barplot(y=imp3.index, x=imp3.values, orient='h', ax=axes[1,0])\nsns.barplot(y=imp4.index, x=imp4.values, orient='h', ax=axes[1,1])\n\n\n\naxes[0,0].set_title(\"Feature importance using Random Forest\")\naxes[0,1].set_title(\"Feature importance using LASSO\")\naxes[1,0].set_title(\"Feature importance using Logistic Regression\")\naxes[1,1].set_title(\"Feature importance using Support Vector Machine\")\n\nplt.tight_layout();","7bae991c":"from sklearn.model_selection import cross_val_score","2a04a4c4":"cross_validation_pred_random_forest = cross_val_score(clf, data_to_train, target_to_train, cv=5)\ncross_validation_pred_logreg = cross_val_score(logreg, data_to_train, target_to_train, cv=5)\ncross_validation_pred_svm = cross_val_score(svm, data_to_train, target_to_train, cv=5)","4cc1fe7d":"validation_pred_random_forest = clf.predict(validation_set)\nvalidation_pred_logreg = logreg.predict(validation_set)\nvalidation_pred_svm = svm.predict(validation_set)","dab3f731":"from sklearn.metrics import accuracy_score\nacc_random_forest = accuracy_score(validation_target, validation_pred_random_forest)\nacc_logreg = accuracy_score(validation_target, validation_pred_logreg)\nacc_svm = accuracy_score(validation_target, validation_pred_svm)\n\nprint('Random Forest Accuracy:          ', acc_random_forest*100)\nprint('Logistic Regression Accuracy:    ', acc_logreg*100)\nprint('SVM Accuracy:                    ', acc_svm*100)\n\nprint('\\nRandom Forest CV Accuracy:       ', cross_validation_pred_random_forest.mean()*100)\nprint('Logistic Regression CV Accuracy: ', cross_validation_pred_logreg.mean()*100)\nprint('SVM CV Accuracy:                 ', cross_validation_pred_svm.mean()*100)","bebf4f99":"from sklearn.metrics import confusion_matrix\n\nf, axes = plt.subplots(1, 3, sharey=False, sharex=False, figsize=(16,5))\n\naxes[0].set_title(\"Confusion Matrix to Random Forest\")\naxes[1].set_title(\"Confusion Matrix to Logistic Regression\")\naxes[2].set_title(\"Confusion Matrix to SVM\")\n\nconfusion_matrix1 = confusion_matrix(validation_target, validation_pred_random_forest)\nconfusion_matrix2 = confusion_matrix(validation_target, validation_pred_logreg)\nconfusion_matrix3 = confusion_matrix(validation_target, validation_pred_svm)\n\nsns.heatmap(confusion_matrix1, ax=axes[0], annot=True)\nsns.heatmap(confusion_matrix2, ax=axes[1], annot=True)\nsns.heatmap(confusion_matrix3, ax=axes[2], annot=True)\n\nplt.tight_layout();","8ba09623":"df.head()","090f64cf":"train_set, validation_set = train_test_split(df.copy(), test_size=0.30)\n\n#to perform holdout-set\ntrain_target = train_set['Churn']\ntrain_set = train_set.drop(['Churn'], axis=1)\n\nvalidation_target = validation_set['Churn']\nvalidation_set = validation_set.drop(['Churn'], axis=1)\n\nx, y = train_set, train_target","ee0cc48c":"clf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nclf.fit(x, y);\n\nlogreg = LogisticRegression()\nlogreg.fit(x, y);\n\nsvm = SVC(kernel='linear') \nsvm.fit(x, y);\n\nvalidation_pred_random_forest = clf.predict(validation_set)\nvalidation_pred_logreg = logreg.predict(validation_set)\nvalidation_pred_svm = svm.predict(validation_set)\n\nacc_random_forest = accuracy_score(validation_target, validation_pred_random_forest)\nacc_logreg = accuracy_score(validation_target, validation_pred_logreg)\nacc_svm = accuracy_score(validation_target, validation_pred_svm)","fdd726cf":"print('Random Forest Accuracy:          ', acc_random_forest*100)\nprint('Logistic Regression Accuracy:    ', acc_logreg*100)\nprint('SVM Accuracy:                    ', acc_svm*100)","726275bb":"We have to replace **No internet service** to **No** and the above features `OnlineSecurity`, `OnlineBackup`, `DeviceProtection`, `TechSupport`, `StreamingTV`, `StreamingMovies`.","e589e0ea":"We look for first informations about our data, the size of our dataset, features, and the number of differents labels in each feature.","12f46d85":"The most of customers doesn't have additional products, choosing simple plans that doesn't contains special products.","8a0a9443":"Our score decrease to 2%, we are suffering of underfitting in this case and we have to add more features. A probably feature that we want to have is a score given by the customer. This feature can give us a better accuracy to our models.","d1f3465d":"Here we predict the labels to `Churn` feature using our `validation set`","929ae114":"We will replace the labels in `SeniorCitizen` changing **1 to Yes** and **0 to No**. This gives us a better understand about next data visualizations.","2f0e7b30":"* Next we will do encoding with in our variables to categorical values, this will permite us to create some correlation matrix to analyse how ours variables are relationed.","f57ae256":"**How the client may use this analysis to improve his customer acquisition?**\n\nThe answer isn't simple but there are some attempts that can help us. To improve customers acquisitions the companie have to build a solid relationship *company-customer*. For example, the most important conclusion in our analysis is that the churn rate is increased by the `MonthlyCharges` and `TotalCharges` that represents the same thing, in other words, when customers pay more the churn rate increases. In our analysis we saw that we customers that have contracts until one year have the most churn rate, they chose by month-to-month contracts. Why people give up to continue with the company?\n\nTo encourage our customers we can create some **reward mechanism**, for example:\n\n* Customers who chose one-year contracts can earn some discounts, for example, we can start the contract with the same value to month-to-month contract and decrease some percentage of the values over the months.\n* We saw that people who have more optional services, as optical fiber pay more and people who pays more is more likely to churn the company. So, we can create some packages with most used optional services and encourage our customers to chose it, give some discounts.\n* People who chose to pay with online methods can be encouraged, because they decrease expenses to company, and maybe earn some discounts.\n\nThe basic thing to do is build a solid relationship *company-customer* to show to our customer that he is a part of the company, and that how much they consume more discounts they will have. Probably its can decrease that churn rate to the company.","85d7a243":"We can look for the number of contracts that are distributed by customers, trying to summarize how them chose the services:","0c4b957d":"Once we know our customer, we have to identify customers who are churn or non-churn","8c36630b":"In cell above we looking for missing values, or nun-numerical values, NaN. We can't indentify none until we look carefully","2c652e4a":"**1)** Before we train our model, we pre-processed our data. We done some cleasing, some labels corrections, and after all that, we categorized numerically our data. In some features we had ambiguous informations, for example, in `OnlineSecurity`  we had three differents labels, ***YES, NO and NO_INTERNET_SERVICE,*** but the correct labels are **YES or NO**. So, we correct these kind of informations in other features. After that we categorized the labels **NO or YES** to **0 or 1**. We done this kind of preprocessing all over the dataset.","a817ef43":"<a name=\"prediction\"><\/a>\n# Prediction","623d96f5":"<a name=\"exploratory\"><\/a> \n# Exploratory Data Analysis","2ba4945b":"First we're looking for possible issues when we load the data set as type convertion and missing values. Our first task here is motivated by data cleansing and type convertions to be used in future trainnig set.","c1d4007b":"***Pearson correlation coefficient***\n\nIn statistics, the Pearson correlation coefficient is a measure of the linear correlation between two variables X and Y. Owing to the Cauchy\u2013Schwarz inequality it has a value between +1 and \u22121, where 1 is total positive linear correlation, 0 is no linear correlation, and \u22121 is total negative linear correlation. Pearson's coefficient is the covariance of the two variables divided by the product of their standard deviations. \n\nPearson's coefficient when applied to a sample is commonly represented by the letter $r$ and can assume values between -1 and 1.\n\n* ${\\displaystyle r =1}$ Means a positive correlation between two variables, in other words if one vary the second one varies too.\n* ${\\displaystyle r =-1}$ Means a negative correlation between two variables, in other words if one vary the second one varies in the opposite way.\n* ${\\displaystyle r =0}$ Represents a zero correlation, one variable doesn't depend of second one linearly.\n\n$$r_{XY} = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^n(X_i - \\overline{X})^2}\\sqrt{\\sum_{i=1}^n(Y_i - \\overline{Y})^2}}$$","e5b637a6":"To improve our accuracy lets do some feature engineering and drop some features and create other important ones.\n\n**We done a previous dataframe with our required features, df**. This dataset can give us a better score in our models because we dropped some features and rearrange others","664e0d72":"Above we plotted the correlation matrix with our features, some of these features doesn't contribute to us to understand the root-cause of churn, so we dropped some and keeped the features that we think it's important to us.\n\nTo exemplify, we look for the correlation coefficient between `tenure` and `TotalCharges`, it can be a little obvious because the time a customer contracted a service is highly related with the charges that he will pay. The coefficient is `corr['tenure']['TotalCharges']= 0.83`, showing high correlation between this two features.","b331706b":"**4)** Bias and variance dilemma. Here is the most important thing to understand about our results. We've trained our models, chose our hyperparameters but\nwe need a way to validate that our model and our hyperparameters are a good fit to the data. When you trained a model and the model fits very well, with accuracy about 98%, for example, maybe there are some problems with. Maybe your model are suffering of **high variance problem**, its happens when your model fits your data very well, **overfitting** your model. But, when you try to predict something that is little different of your train set you got a reasonable error.\n\nOn the other hand, when you trained model and the model fits very poorly, with low accuracy, and don't matter how big your train dataset is, you always take low accuracy, probabily you are suffering of **underfitting** problem. In this case we need to add new features to our train set to improve our accuracy.\n\nBellow we have a picture that explains itself, relationing the bias and variance dilemma.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*xwtSpR_zg7j7zusa4IDHNQ.png)\n\nOur approach to solve this problem consists in two methods **holdout sets** and **cross-validation**. Holdout set consists in hold back some subset of the dataset from the training of the model, and then use this holdout set to check the model performance. This was the tecnique that we used here in our models, besides Linear Regression Lasso that we used cross-validation.\n\nOne disadvantage of using a holdout set for model validation is that we have lost a portion of our dataset to the model training. Sometimes holdout-set can cause some problems, because we lose a portion of our data to train our model, and when the dataset is small it can lead us to bias or variance problems. One way to address this is to use cross-validation; that is, to do a sequence of fits where each subset of the data is used both as a training set and as a validation set. \n\nWe used here holdout set to attempt avoid underfitting and overfitting problems. After we will drop some features in a attempt to improve our scores.\n\nIn next cells we see how our models scores and we can understand better how models.","3ef51eb6":"When we plotted `churn` variable with the features we chose, we observed how customers are categorized, for example, there aren't diferences between gender. Other conclusion is how much older a customer is less churn they are.\n\nTo finish, we can visualize how ours variables are correlated. To do that we can calculate the ***Pearson correlation coefficient***.","09bd75a2":"**5)** Bellow we look for how accurate our models are. The main validation technique used here was holdout sets, but to compare with cross-validation, we have calculated the accuracy using cross-validation for comparation. ","2532be66":"With a little cleanising we dropped some values from dataset. These dropped data contains `white spaces` and we have to change to NaN values. After we dropped this customers from our dataset.","a212295d":"We have to plot an histogram for the `tenure` variable, which represent the number of months the customer has stayed with the company. Beside, we look for the same behaviour but now looking for years.","9d00badf":"Here we can compute the accuracy of each model:","bf1a3afa":"About 31% of customers stay with the company about one year, this period is the most common. Only 20% are greater than or equal to four years, in other words, only 20% continues in the company before four years of contract.","e9332cb1":"To understand the chart above, remember that we are looking for how each feature contributes to gives us a good result to our target feature `Churn`. When we use `LASSO`, `SVM` and `Logistic Regression`, we can see that some features have a negative relation to our predicted variable `Churn`, while others have positive relation. Negative relation means that likeliness of churn decreases with that variable.\n\n**To summarize:**\n\nTotal charges, internet services and seniority, for example, can lead to higher churn rates. This is interesting because although fibre optic services are faster, customers are likely to churn because of it, probably because you have to pay more, contributing to total charges.","2849ec69":"**2)** One of the methods to identify the most importants features is try to fit our data in a Linear Regression. In this case I'll use linear regression with penalties to find the variables that are more important to linear regression. Besides that, I'll try Random Forest Algorithm, Logistic Regression and Support Vector Machine to do the same thing, but with differents approachs.","6c0f0701":"# Table of Contents\n\n* [Exploratory Data Analysis](#exploratory):\n    - [Data Cleansing and Initial Analysis](#cleasing)\n    \n* [Prediction](#prediction)\n    - [Feature Importance](#prediction)\n    - [Correlation Matrix](#prediction)\n    - [Model choosing](#prediction)\n    - [DevOPS](#prediction)\n    \n* Conclusion","83bb7a8a":"**6)** To build a scalable machine learning model we *have to identify our objective to build a application to our client*. In our analysis we trained a model that predicts if the customer will be a churn or non-churn customer, so we trained a static model. In the figures below we exemplify a production-chain to create machine-learning applications:\n\n<img src='http:\/\/barnraisersllc.com\/wp-content\/uploads\/2018\/09\/CRISP_en.png' width=\"260\" height=\"210\" class=\"center\" alt=\"\"><img src='https:\/\/qph.fs.quoracdn.net\/main-qimg-8cdd3bee2eaed6a42bcd075f55ba00d2' width=\"330\" height=\"312\" class=\"center\" alt=\"\"> \n\nWhen we create a machine learning model we should ask ourself about the best way to deploy our model to a production enviroment:\n\n* How will I evaluate the model\u2019s performance in production? (and how will I collect and store the additional data from interactions)\n* How frequently do I plan on re-training my model?\n* What are the data preprocessing needs?\n* Will the format of the production input data differ drastically from the model training data? Will it come in batches or as a stream?\n* Does the model need to be able to run offline?\n\nTo create an application we have to export our Python model, using a `pickle` library, this library makes it easy to serialize the models into files that I create. To deploy our model we can create a server, in this case a chose to create a `FLASK` server to run my model. The Flask Python framework allows us to create web servers in record time.\n\nOnce we had setup our server we can add our model to server. To do that we have to load our model in our application. Our application receives an array of features and put in our model. The model's prediction is then sent back to the user. In `python` we code something like this:\n```python\nimport pickle\nimport flask\n\napp = flask.Flask(__name__)\n\n#getting our trained model from a file we created earlier\nmodel = pickle.load(open(\"model.pkl\",\"r\"))\n\n@app.route('\/predict', methods=['POST'])\ndef predict():\n    #grabbing a set of wine features from the request's body\n    feature_array = request.get_json()['feature_array']\n    \n    #our model rates the wine based on the input array\n    prediction = model.predict([feature_array]).tolist()\n    \n    #preparing a response object and storing the model's predictions\n    response = {}\n    response['predictions'] = prediction\n    \n    #sending our response object back as json\n    return flask.jsonify(response)\n```\nOur application is running as an on-demand offline application. To create an real time application, with a dynamic training set, that increase and improve our prediction, for example Amazon search engines, we can chose an auto-machine learning framework as **Cloud AutoML** from Google.","45e34b3e":"Again, there are some features that contains ambiguous informations, for example, `OnlineSecurity` contains 3 differents labels, but the correct labels are *YES* or *NO*.","39ee6680":"There are some features that contains `Python object`, and we have to convert this columns to a valid type format. One feature that contains an important information is `TotalCharges` which contains charges values and we need to convert to numeric type. ","21be9a25":"The most accurated model was Logistic Regression with about 80% of accuracy when we use *holdout sets* (probably it's can run different in others machine, because I don't set a static sed). To improve this accuracy we can do some **feature engineering**, dropping some features and create others to improve our model.\n\nTo visualize, we can plot the `confusion matrix` to our models","ac7afcb4":"<a name=\"cleasing\"><\/a>\n## Data Cleansing and Initial Analysis","cb7a289f":"In the chart above we were looking for how distributed our contracts are divided by gender, by senior citizen, and by kind of contracts. The most of contracts are specified as **Month-to-month** about 55% of contracts. About the `SeniorCitizen` we have just 15% of seniors citizen. When we look for SeniorCitizen by gender we have about the same percentage of male and female who contracts services.\n\nNow we have to look about how the `tenure` feature are categorized. If we have the entire information about how customers buy products we can analyse the most important features to calculate churn.\n\nThe feature `tenure` specifies the number of months the customer has stayed with the company, we can categorize this in years.","04ff4142":"**3)** We chose three differents algorithms to predict the `Churn` feature, Random Forest, Logistic Regression and Support Vector Machine. Bellow we'll give some information of why we used them. We are trying to predict a categorical variable which represent a customer is `Churn` or `Non-Churn`, then to do that we had to trainned our classifiers with three differents algorithms, bellow we will explain our choice.\n\n***Random Forest:***\n\nRandom forests are a powerful method with several advantages and can fit our model very well, we give some advantages and disadvantages bellow:\n\n* Both training and prediction are very fast, because of the simplicity of the underlying decision trees. In addition, both tasks can be straightforwardly parallelized, because the individual trees are entirely independent entities.\n* The multiple trees allow for a probabilistic classification.\n* The nonparametric model is extremely flexible, and can thus perform well on tasks that are under-fit by other estimators.\n* A primary disadvantage of random forests is that the results are not easily interpretable: that is, if you would like to draw conclusions about the meaning of the classification model, random forests may not be the best choice.\n\n***Logistic Regression:***\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary) as in our case to predict `Churn` or `Non-Churn`. Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\nTo summarize, logistic regression allow us to predict labels of a target feature, even though the feature depends of several other features, giving us good prediction.\n\n***Support Vector Machine:***\n\nSupport vector machines are a powerful classification method for a number of reasons:\n\n* Their dependence on relatively few support vectors means that they are very compact models, and take up very little memory.\n* Once the model is trained, the prediction phase is very fast.\n* Because they are affected only by points near the margin, they work well with high-dimensional data\u2014even data with more dimensions than samples, which is a challenging regime for other algorithms.\n* Their integration with kernel methods makes them very versatile, able to adapt to many types of data.\n\nHowever, SVMs have several disadvantages as well:\n\n* The scaling with the number of samples $N$ is $\\mathcal{O}[N^3]$ at worst, or $\\mathcal{O}[N^2]$ for efficient implementations. For large numbers of training samples, this computational cost can be prohibitive.\n* The results are strongly dependent on a suitable choice for the softening parameter $C$. This must be carefully chosen via cross-validation, which can be expensive as datasets grow in size.\n* The results do not have a direct probabilistic interpretation. This can be estimated via an internal cross-validation, but this extra estimation is costly.\n\nBased on this explanations, we chose this three algorithms to perform about our dataset, every one has advantages and disadvantages also. SVM is a good algorithm but sometimes needs much time to train. Random Forest are very good but sometimes can lead us to overfitting problems. Lastly, Logistic Regression is always a good method to classifications problems combined with Cross Validation can give us best results."}}