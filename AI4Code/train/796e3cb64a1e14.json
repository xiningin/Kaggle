{"cell_type":{"d07f4648":"code","a3d862db":"code","3ff6ac50":"code","5183697f":"code","37a5dfe9":"code","326d9da4":"code","9f25ef8d":"code","d66abf31":"code","97895e46":"code","65ffcc8f":"code","3b32bb53":"code","84b6eb67":"code","76e9ac72":"code","e581b8b4":"code","d93fa429":"code","ccef47a0":"code","c4a56a47":"code","5c11157c":"code","354d11f3":"code","73ee2dc4":"code","f010b539":"code","e411c801":"code","0b79b0ef":"code","d0e815a3":"code","0c505c73":"code","a3da1ddd":"code","f8b01b91":"code","ebbaee48":"markdown"},"source":{"d07f4648":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (12, 12) ### Setting the size of the Plots\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a3d862db":"#### Lets load the file\nmerchant_df = pd.read_csv(\"..\/input\/merchants.csv\")","3ff6ac50":"#### Lets take a quick look at the basic info. some of the columns have missing values\nmerchant_df.info()","5183697f":"#### Next lets look at the numeric values and see if everything is ok. \nmerchant_df.describe()","37a5dfe9":"#### We can see that 3 columns have the maximum value as inf, Lets see how many rows are affected\nprint(merchant_df[merchant_df['avg_purchases_lag3']==float('Inf')].merchant_id.count())\nprint(merchant_df[merchant_df['avg_purchases_lag6']==float('Inf')].merchant_id.count())\nprint(merchant_df[merchant_df['avg_purchases_lag12']==float('Inf')].merchant_id.count())\nmerchant_df[merchant_df['avg_purchases_lag3']==float('Inf')]","326d9da4":"#### We can see that only 3 rows are affected. we can drop these rows\nmerchant_df = merchant_df.drop(merchant_df.index[merchant_df.avg_purchases_lag3 ==float('Inf')],0)\nmerchant_df.describe()","9f25ef8d":"#### Lets look at handling some of the outliers in the sales lag \/ purchase lag fields\nmerchant_df.plot.scatter(x='avg_sales_lag3', y='avg_purchases_lag3')\nplt.show()","d66abf31":"### we can see only 16 records are getting affected. Lets drop these\n### We can repeat the scatter plot for avg_sales_lag6, avg_sales_lag12 as well\nprint(merchant_df[merchant_df['avg_sales_lag3']>20000].merchant_id.count())\nmerchant_df = merchant_df.drop(merchant_df.index[merchant_df.avg_sales_lag3>20000],0)","97895e46":"merchant_df.plot.scatter(x='avg_sales_lag6', y='avg_purchases_lag6')\nplt.show()","65ffcc8f":"### we can see only 16 records are getting affected. Lets drop these\n### We can repeat the scatter plot for avg_sales_lag6, avg_sales_lag12 as well\nprint(merchant_df[merchant_df['avg_sales_lag6']>20000].merchant_id.count())\nmerchant_df = merchant_df.drop(merchant_df.index[merchant_df.avg_sales_lag6>20000],0)","3b32bb53":"merchant_df.plot.scatter(x='avg_sales_lag12', y='avg_purchases_lag12')\nplt.show()","84b6eb67":"### we can see only 16 records are getting affected. Lets drop these\n### We can repeat the scatter plot for avg_sales_lag6, avg_sales_lag12 as well\nprint(merchant_df[merchant_df['avg_sales_lag12']>20000].merchant_id.count())\nmerchant_df = merchant_df.drop(merchant_df.index[merchant_df.avg_sales_lag12>20000],0)","76e9ac72":"merchant_df.describe()","e581b8b4":"##### Next Lets handle the 4 categorical values\n### category_1, category_4  have Y or N we can replace this with 0 \/ 1\n### most_recent_sales_range, most_recent_purchases_range are kind of ranking of some sort with A>B>C>D>E, we can replace these with 5>4>3>2>1\nclean_up_categoricals = {'category_1':{'Y':1, 'N':0},\n                         'category_4' :{'Y':1, 'N':0},\n                        'most_recent_sales_range' : {'A':5,'B':4,'C':3,'D':2,'E':1},\n                        'most_recent_purchases_range' : {'A':5,'B':4,'C':3,'D':2,'E':1}}\nmerchant_df.replace(clean_up_categoricals, inplace=True)\nmerchant_df.head(5)","d93fa429":"##### Finally lets replace the null values with the respective means of the columns\nmerchant_df = merchant_df.fillna(merchant_df.mean())\nmerchant_df.head(5)","ccef47a0":"#### Next we can extract our X component out of this - All fields excluding the merchant_id\nX = merchant_df\nX = X.drop('merchant_id',1)\nX.head(2)\nX.info()","c4a56a47":"#### Lets use the standard scaler to scale all our columns\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX = sc_X.fit_transform(X)","5c11157c":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(X, test_size = 0.2, random_state = 0)\n","354d11f3":"# this is the size of our encoded representations\nencoding_dim = 6  # we need our 21 columns to be encoded as 6\n# this is our input placeholder\ninput_shape = Input(shape=(21,))\n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_shape)\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(21, activation='sigmoid')(encoded)\n# this model maps an input to its reconstruction\nautoencoder = Model(input_shape, decoded)\n# this model maps an input to its encoded representation\nencoder = Model(input_shape, encoded)","73ee2dc4":"# create a placeholder for an encoded (32-dimensional) input\nencoded_input = Input(shape=(encoding_dim,))\n# retrieve the last layer of the autoencoder model\ndecoder_layer = autoencoder.layers[-1]\n# create the decoder model\ndecoder = Model(encoded_input, decoder_layer(encoded_input))","f010b539":"#### Compile the autoencoder\nautoencoder.compile(optimizer='adadelta', loss='MSE')","e411c801":"#### Train the autoencoder\nautoencoder.fit(X_train, X_train,\n                epochs=100,\n                batch_size=256,\n                shuffle=True,\n                validation_data=(X_test, X_test))","0b79b0ef":"#### Get our encoded data\nencoded_data = encoder.predict(X)\nencoded_data.shape","d0e815a3":"#### Lets find the optimal number of clusters using the elbow method by plotting wcss against the number of clusters\nfrom sklearn.cluster import KMeans\nwcss = []\n\nfor i in range (1,11):\n    kmeans = KMeans(n_clusters = i, init = \"k-means++\", max_iter = 300,\n                    n_init = 10, random_state = 0)\n    kmeans.fit(encoded_data)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1,11),wcss)\nplt.title(\"The Elbow Method\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()\n","0c505c73":"##### 4 seems to be a good number of clusters\n\nkmeans = KMeans(n_clusters = 4, init = \"k-means++\", max_iter = 300,\n                    n_init = 10, random_state = 0)\nykmeans = kmeans.fit_predict(encoded_data)","a3da1ddd":"plt.scatter(encoded_data[ykmeans==0,0], encoded_data[ykmeans==0,1], s = 10, c='red', label = \"1\")\nplt.scatter(encoded_data[ykmeans==1,0], encoded_data[ykmeans==1,1], s = 10, c='blue', label = \"2\")\nplt.scatter(encoded_data[ykmeans==2,0], encoded_data[ykmeans==2,1], s = 10, c='green', label = \"3\")\nplt.scatter(encoded_data[ykmeans==3,0], encoded_data[ykmeans==3,1], s = 10, c='cyan', label = \"4\")\n\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1], s= 100, c=\"yellow\", label = \"Centroids\")\nplt.title(\"Clusters of Clients\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.legend()\nplt.show()\n","f8b01b91":"##### Finally Lets Create a new dataframe that will have just 2 columns - the merchant_id and the corresponding cluster id\nmerchant_id = merchant_df['merchant_id']\nprint(merchant_id.shape)\nprint(ykmeans.shape)\n\nnew_df = pd.DataFrame()\nnew_df['merchant_id'] = merchant_df['merchant_id']\nnew_df['cluster_id'] = ykmeans\nprint(new_df.info())\nnew_df.to_csv('merchant_id_clusters.csv', index=False)","ebbaee48":"This notebook is an attempt to explore the merchants.csv file.\n1.  Do basic pre-processing\n2. Reduce dimensionality using an auto-encoder.\n3. Cluster Merchants based on the reduced dimensions"}}