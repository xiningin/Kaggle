{"cell_type":{"e67c577d":"code","5fab2d29":"code","ef01f5d4":"code","7d0b1fb1":"code","2957916e":"code","d029fa77":"code","361614de":"code","9bfd7d70":"code","d7d29ebf":"code","cbbab18e":"code","85d0719e":"code","b25f3958":"code","2b6973e6":"code","5de781a4":"code","3f31cf4f":"code","6afc9a56":"code","7b083fe3":"code","6a5aee77":"code","0474d0f6":"code","2328d919":"code","bfd79d74":"code","1aa5f556":"code","8d9b8d72":"code","6d59e54d":"code","e36724e0":"code","e0c7da75":"code","bc1e4e97":"code","cc56b401":"code","03fe1ee7":"code","980f7852":"code","9a8b2c7c":"code","69828ef1":"code","de0b2f77":"code","573106b0":"code","6f2eb83d":"code","3952c629":"code","769ab2e3":"code","c55bca2a":"code","eb677ad6":"code","f2b5007b":"code","7e32c644":"code","b45aab9b":"code","9ebd103f":"code","14568a62":"code","13ba90a9":"code","698649a7":"code","d3b3c053":"code","2e52dcd3":"code","c623b011":"code","9f9b5171":"code","14380af8":"code","8cf80563":"code","50e92c51":"code","f126ab05":"code","3f162c44":"code","d4f42a10":"code","a4a026f1":"code","10d63b5b":"code","c497de36":"code","9013e8aa":"code","e05ca386":"code","38e0f340":"code","8c6018e0":"code","1edf669e":"code","764ea69f":"code","f08b46ef":"code","0e16dfa5":"code","7d322f5e":"code","96d8f284":"code","e288d8f3":"code","ca79e80e":"code","cb27087a":"code","6692ba57":"code","89f5fc9d":"code","33f0f6d0":"code","52c6376f":"code","6862ce1e":"code","c7db1982":"code","2c7194a5":"code","07d11a9c":"code","4ac11e77":"code","ee03e7cf":"code","7703b732":"code","3ed160d2":"code","702ab449":"code","2827bbc2":"code","70cf95e1":"code","1c361e21":"code","5bd83da5":"code","ee0557ad":"code","7011ee4a":"code","03456c8a":"code","f5112556":"code","8e2af4a6":"code","532db508":"markdown","f4aacfc9":"markdown","b675f474":"markdown","7a83e9bc":"markdown","d746ffee":"markdown","e40bf91e":"markdown","90306483":"markdown","319e4d0c":"markdown","9fe8b803":"markdown","2fda4dd4":"markdown","b53dc474":"markdown","8e6aa8c2":"markdown","79599f19":"markdown","0813261e":"markdown","08ea098e":"markdown","ac39cde4":"markdown","849ffc44":"markdown","ce11b691":"markdown","87292247":"markdown","071a674a":"markdown","fb545567":"markdown","867ec3b1":"markdown","e6b96d8b":"markdown","643086e8":"markdown","74ca33a7":"markdown","bece7f2f":"markdown","49b8a72b":"markdown","a5cd5554":"markdown","48ff1ee4":"markdown","eadf8686":"markdown","ad701ac2":"markdown","03ce2d91":"markdown","2acedf24":"markdown","83807a32":"markdown","a20f77d6":"markdown","4de64cd1":"markdown","8c3e6e52":"markdown","88852a23":"markdown","83b41304":"markdown","7279b766":"markdown","6bd1707f":"markdown","157ddbe1":"markdown","8206816f":"markdown","426f6ed6":"markdown","d6e97422":"markdown","59f06a0e":"markdown","7a42114a":"markdown","55a56ad8":"markdown","da7d82ce":"markdown","b1a3b351":"markdown","b51ea183":"markdown","4fe02797":"markdown","b2b3e243":"markdown","528a7d44":"markdown","421676a3":"markdown","52c2bcfc":"markdown","307eebcf":"markdown","4548c336":"markdown","29f6cb56":"markdown","9c2d07ba":"markdown","81677790":"markdown","d5608ad1":"markdown","5f7233bb":"markdown","916738b7":"markdown","1a2d68a4":"markdown","51d15981":"markdown","57d2aae4":"markdown","c3014ba5":"markdown","b16b062b":"markdown","a224017b":"markdown","6bce3b68":"markdown","1cbc2088":"markdown","88441551":"markdown","41778231":"markdown","ac37db15":"markdown","7e4fd81b":"markdown","7e846ed8":"markdown","32174851":"markdown","41b13bc7":"markdown","19fbca82":"markdown","b18d6806":"markdown","65f9ece2":"markdown","90e80e7f":"markdown","6c31fdcb":"markdown","356fe2c3":"markdown","c4acad13":"markdown","0114a873":"markdown","964d54b1":"markdown","31434514":"markdown","68d63618":"markdown"},"source":{"e67c577d":"#Importing all libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#building model\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LogisticRegression\n\n#model evaluation\nfrom sklearn.feature_selection import RFE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_curve\n\n#model validation\nfrom sklearn.model_selection import train_test_split\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data display coustomization\npd.set_option('display.max_columns', 100)","5fab2d29":"telecom = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","ef01f5d4":"# Now we have one data frame consisting all data. Now we will see first five rows of the new data frame\ntelecom.head()","7d0b1fb1":"#prinitng shape of the dataset\nr,c = telecom.shape\nprint(f\"Shape of telecom dataset: {telecom.shape}\")\nprint(f\"Number of rows: {r}\")\nprint(f\"Number of columns: {c}\")","2957916e":"# let's look at the some statistics of the dataframe\ntelecom.describe()","d029fa77":"# Let's look at the data type of each feature\ntelecom.info()","361614de":"# Defining method to convert them\ndef binary_map(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\nbin_var =  ['PhoneService', 'PaperlessBilling', 'Churn', 'Partner', 'Dependents']\n\n# Applying the method to the data frame\ntelecom[bin_var] = telecom[bin_var].apply(binary_map)","9bfd7d70":"#creating dummies for gender and dropping first column because single column can capture the whole data\ngender = pd.get_dummies(telecom['gender'], drop_first=True)\n\n# Merging the above results with telecom data frame \ntelecom = pd.concat([telecom, gender], axis=1)","d7d29ebf":"#printing first 5 rows of the data frame after converting Binary variables\ntelecom.head()","cbbab18e":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(telecom[['Contract', 'PaymentMethod', 'InternetService']], drop_first=True)\n\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom, dummy1], axis=1)","85d0719e":"telecom.head()","b25f3958":"# Creating dummy variables for the remaining categorical variables and dropping the level with big names.\n\n# Creating dummy variables for the variable 'MultipleLines'\nml = pd.get_dummies(telecom['MultipleLines'], prefix='MultipleLines')\n# Dropping MultipleLines_No phone service column\nml1 = ml.drop(['MultipleLines_No phone service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ml1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineSecurity'.\nos = pd.get_dummies(telecom['OnlineSecurity'], prefix='OnlineSecurity')\nos1 = os.drop(['OnlineSecurity_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,os1], axis=1)\n\n# Creating dummy variables for the variable 'OnlineBackup'.\nob = pd.get_dummies(telecom['OnlineBackup'], prefix='OnlineBackup')\nob1 = ob.drop(['OnlineBackup_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ob1], axis=1)\n\n# Creating dummy variables for the variable 'DeviceProtection'. \ndp = pd.get_dummies(telecom['DeviceProtection'], prefix='DeviceProtection')\ndp1 = dp.drop(['DeviceProtection_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,dp1], axis=1)\n\n# Creating dummy variables for the variable 'TechSupport'. \nts = pd.get_dummies(telecom['TechSupport'], prefix='TechSupport')\nts1 = ts.drop(['TechSupport_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ts1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingTV'.\nst =pd.get_dummies(telecom['StreamingTV'], prefix='StreamingTV')\nst1 = st.drop(['StreamingTV_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,st1], axis=1)\n\n# Creating dummy variables for the variable 'StreamingMovies'. \nssm = pd.get_dummies(telecom['StreamingMovies'], prefix='StreamingMovies')\nssm1 = ssm.drop(['StreamingMovies_No internet service'], 1)\n# Adding the results to the telecom dataframe\ntelecom = pd.concat([telecom,ssm1], axis=1)","2b6973e6":"telecom.InternetService.value_counts()","5de781a4":"telecom.PhoneService.value_counts()","3f31cf4f":"telecom.head()","6afc9a56":"# We have created dummies for the below variables, so we can drop them\ntelecom = telecom.drop(['Contract','PaymentMethod','gender','MultipleLines','InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n       'TechSupport', 'StreamingTV', 'StreamingMovies'], 1)","7b083fe3":"#The varaible TotalCharges is of String data type so converting it into float type\ntelecom['TotalCharges'] = pd.to_numeric(telecom[\"TotalCharges\"].replace(\" \",\"\"),downcast=\"float\")","6a5aee77":"#checking data types of variables \ntelecom.info()","0474d0f6":"#Plot Box Plot for all there continuous variables \n\nplt.figure(figsize=(15,3))\nplt.subplot(1,3,1)\nsns.boxplot(telecom[[\"tenure\"]])\nplt.title(\"Tenure\",size=15)\n\nplt.subplot(1,3,2)\nsns.boxplot(telecom[[\"MonthlyCharges\"]])\nplt.title(\"MonthlyCharges\",size=15)\n\nplt.subplot(1,3,3)\nsns.boxplot(telecom[[\"TotalCharges\"]])\nplt.title(\"TotalCharges\",size=15)","2328d919":"# Adding up the missing values (column-wise)\ntelecom.isnull().sum()","bfd79d74":"# Removing NaN TotalCharges rows\ntelecom = telecom[~np.isnan(telecom['TotalCharges'])]","1aa5f556":"# Checking again for missing values (column-wise)\ntelecom.isnull().sum()","8d9b8d72":"# Correlation Matrix\nplt.figure(figsize = (20,10))\nsns.heatmap(round(telecom.corr(),1),annot = True)\nplt.show()","6d59e54d":"#dropping the highly correlated variables\n\ntelecom.drop(['MultipleLines_No','OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No',\n                       'StreamingTV_No','StreamingMovies_No'],axis=1,inplace=True)","e36724e0":"#customerID column is of no use for the model so we drop that column also\nX = telecom.drop(['Churn','customerID'], axis=1)\nX.head()","e0c7da75":"y = telecom['Churn']\ny.head()","bc1e4e97":"# Splitting the dataset into training set and testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","cc56b401":"#we will standardize continous variables only and not categorical variables\nsc = StandardScaler()\nsc.fit(X_train[['tenure','MonthlyCharges','TotalCharges']])\nX_train[['tenure','MonthlyCharges','TotalCharges']] = sc.transform(X_train[['tenure','MonthlyCharges','TotalCharges']])\nX_train.head()","03fe1ee7":"### Checking the \np = (sum(y_train)\/len(y_train))\nprint(f\"p: {p}\")\n\nk = X_train.shape[1]\nprint(f\"k: {k}\")\n\nN = 10 * k \/ p\nprint(f\"N: {int(N)}\")","980f7852":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","9a8b2c7c":"logreg = LogisticRegression()\nrfe = RFE(logreg, 15)\nrfe = rfe.fit(X_train, y_train)","69828ef1":"#top 15 columns returned by RFE\ncol = X_train.columns[rfe.support_]\ncol","de0b2f77":"#building the model with top 15 features which we got from RFE\nX_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","573106b0":"# Getting the predicted values on the train set and showing first 10 predictions in terms of probabilities\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","6f2eb83d":"#reshaping the predicted array\ny_train_pred = y_train_pred.values.reshape(-1)","3952c629":"#creating data frame with actual churn and predicted probablilities\ny_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\ny_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.head()","769ab2e3":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","c55bca2a":"#Checking Accuracy of the model\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","eb677ad6":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f2b5007b":"#We will drop variables one by one, droping MonthlyCharges column\ncol = col.drop('MonthlyCharges',1)\ncol","7e32c644":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","b45aab9b":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","9ebd103f":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","14568a62":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","13ba90a9":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","698649a7":"#We are droping MultipleLines_Yes variable as it is insignificant\n#We give priority to p-value than VIF\ncol = col.drop('MultipleLines_Yes',1)\ncol","d3b3c053":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","2e52dcd3":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","c623b011":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","9f9b5171":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","14380af8":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","8cf80563":"# Let's drop TotalCharges since it has a high VIF\ncol = col.drop('TotalCharges')\ncol","50e92c51":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","f126ab05":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)\ny_train_pred_final['Churn_Prob'] = y_train_pred\ny_train_pred[:10]","3f162c44":"# Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\ny_train_pred_final.head()","d4f42a10":"# Let's check the overall accuracy.\nprint(\"Accuracy (Training Set): \",round(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted),4))","a4a026f1":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","10d63b5b":"# Let's take a look at the confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nconfusion","c497de36":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     3270        |     365\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      604        |     683\")","9013e8aa":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","e05ca386":"#Accuracy of the final model\naccuracy = (TN + TP)\/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP \/ float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN \/ float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","38e0f340":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","8c6018e0":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['Probability','Accuracy','Sensitivity','Specificity'])\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","1edf669e":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='Probability', y=['Accuracy','Sensitivity','Specificity'])\nplt.xlabel(\"Thresh-hold\")\nplt.ylabel(\"Scores\")\nplt.title(\"Sensitivity and Specificity Trade-off\",size=15)\nplt.show()","764ea69f":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","f08b46ef":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","0e16dfa5":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     2787        |     848\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      288        |     999\")","7d322f5e":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","96d8f284":"#Accuracy of the final model\naccuracy = (TN + TP)\/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP \/ float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN \/ float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","e288d8f3":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    #plt.savefig(\"E:\/1. NITW\/Project 4th Sem\/ROC Curve.jpg\")\n    plt.show()\n    \n    return None","ca79e80e":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )\n\ndraw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","cb27087a":"# Precision of the final model\nprecision = TP \/ float(TP+FP)\nprint(\"Precision of the model: \",round(precision,3))\n\n# Recall of the final model\nrecall = TP \/ float(TP+FN)\nprint(\"Recall of the model: \",round(recall,3))","6692ba57":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","89f5fc9d":"plt.plot(thresholds, p[:-1], \"g-\",label=\"Precision\")\nplt.plot(thresholds, r[:-1], \"r-\",label=\"Recall\")\nplt.xlabel(\"Thresh-hold\")\nplt.ylabel(\"Scores\")\nplt.title(\"Precision and Recall Trade-off\",size=15)\nplt.legend()\nplt.show()","33f0f6d0":"X_test[['tenure','MonthlyCharges','TotalCharges']] = sc.transform(X_test[['tenure','MonthlyCharges','TotalCharges']])","52c6376f":"X_test = X_test[col]\nX_test.head()","6862ce1e":"X_test_sm = sm.add_constant(X_test)\ny_test_pred = res.predict(X_test_sm)","c7db1982":"y_test_pred[:10]","2c7194a5":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","07d11a9c":"# Let's see the head\ny_pred_1.head()","4ac11e77":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","ee03e7cf":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","7703b732":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","3ed160d2":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","702ab449":"y_pred_final.head()","2827bbc2":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","70cf95e1":"# Let's see the head of y_pred_final\ny_pred_final.head()","1c361e21":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.3 else 0)","5bd83da5":"y_pred_final.head()","ee0557ad":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","7011ee4a":"print(\"Predicted     |  Not Churn (0)  |  Churn (1)\")\nprint(\"Actual        |                 | \")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Not Churn (0) |     1144        |     384\")\nprint(\"--------------|-----------------|----------------\")\nprint(\"Churn     (1) |      163        |     419\")","03456c8a":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","f5112556":"#Accuracy of the final model\naccuracy = (TN + TP)\/float(TN+FN+TP+FP)\nprint(\"Accuracy of the model: \",round(accuracy,3))\n\n# Sensitivity of the final model\nsensitivity = TP \/ float(TP+FN)\nprint(\"Sensitivity of the model: \",round(sensitivity,3))\n\n# Specificity of the final model\nspecificity = TN \/ float(TN+FP)\nprint(\"Specificity of the model: \",round(specificity,3))","8e2af4a6":"model  = pd.DataFrame({\"Features\": X_train_sm.columns,\"Coefficient\":res.params.values})\nmodel[\"Odds_Ratio\"] = model[\"Coefficient\"].apply(lambda x: np.exp(x))\nmodel[[\"Coefficient\",\"Odds_Ratio\"]] = model[[\"Coefficient\",\"Odds_Ratio\"]].apply(lambda x: round(x,2))\nmodel[\"Perc_Impact\"] = model[\"Odds_Ratio\"].apply(lambda x: (x-1)*100)\nmodel","532db508":"# Step 5: Feature Scaling\n\nMost of the times, your dataset will contain features highly varying in magnitudes, units and range. But since, most of the machine learning algorithms use Eucledian distance between two data points in their computations. To supress this effect, we need to bring all features to the same level of magnitudes.\n\nThere are several ways for performing Feature Scaling, here we will be using `Standard Scalar` or `Standardization`\n\\begin{equation*}\n\\mathbf{X_*}   = \\frac{X - Mean}{Standard Deviation}\n\\end{equation*}","f4aacfc9":"### Checking Accuracy of the Model","b675f474":"We have completed all steps for solving a classification problem. We have seen that the model we built gives good accuracy score of 77% on the Training dataset and 74% on the Testing dataset along with other metrics. For this problem we preffered to use Sensitivity and Specificity metrics for the evauation. We have also seen impact of each variable on the probability of churn. Below are few observations about the model:\n\n- A customer with long term contracts like One year and Two Year are less likely to churn than the customer having Monthly contract.\n\n- A customer who is associated with the company from longer time is less likely to churn than a customer who is associated from few months. Reason can be the customer is happy with the services and wishes to continue with them.\n\n- Customer using Internet Services, Fiber Optics, Streaming TV and Movies servies are more likely to churn than customer who are not using these services. Reason can be company not providing good Internet services and need to work on that.\n\n- Customer who have opted for Payment Method through Credit Card or Mailed check are less likely to churn then other customers.\n\nOverall, company need to provide better internet services and other services associated with internet to retain their customers.","7a83e9bc":"# Step 4: Splitting the Dataset","d746ffee":"##### Making predictions on the Training Set","e40bf91e":"### Checking VIFs","90306483":"# Telecom Churn Case Study\n\nIn this notebook, we are going to work on a Telecom company case study where using the past information we need to build a model that can predict whether a particular customer will  switch to different service provider or not (Churn or not). So for us the variable of interest is `Churn` which will tell us whether or not a particular customer has churned. It is a binary variable, 1 means that the customer has churned and 0 means the customer has not churned.\n\nCompany also needs to know the factors (variables) which influences the Churn variable and how much they impact individually. This will help the company to improve those area to retain their customers. Company needs a descent model that can predict good percentage of Churn and Non-Churn customer correctly.\n\nWe will build a Logistic Regression model for this problem because it is easy to interpret and this will help company in decision making better.","319e4d0c":"### Precision and Recall Tradeoff\n\nLet's look at the Precision and Recall trade-off now","9fe8b803":"We can notice that p-value of `MultipleLines_Yes` is 0.07 (> 0.05).\n\nThis means that variable is insignificant for us and hence we can drop it.","2fda4dd4":"We can notice that ROC curve look very good and **Area under the curve (AUC)** is `0.85` which is a very good score and tells the goodness of the model.\n\nTherefore, all this shows that our the model we build using Training dataset fits goon on that and the optimal value of the thresh-hold gives us good scores.","b53dc474":"This means we need to have minimum **879** samples to build a Logistic Regression model and in the `X_train` we have **4922** samples i.e. we can build a Logistic model.","8e6aa8c2":"## ROC Curve","79599f19":"We are done with data processing steps,  now the data is ready to fetch in the model.\n\nWe will first split the dataset into towo part:\n- X = All independent variables\n- y = Dependent variable `Churn`\n\nThen we will split the dataset into Training set and Testing set:\n- Training Set: Model is build using this dataset\n- Testing Set: Model Validation is done using this set\n\nWe will be doing In-sample validation for this problem. Training set and Testing set will be in ratio 7:3 respectively. ","0813261e":"`Male` column represents the gender column now, 1 = Male and 0 = Female","08ea098e":"## Building Second Model\n\nBuilding second model after selecting top 15 features from RFE","ac39cde4":"#### Dropping the repeated Variables","849ffc44":"### Checking Accuracy of the Model","ce11b691":"So overall the accuracy hasn't dropped much.","87292247":"We will perform following steps in this notebook:\n\n- **Step 1**: Importing Libraries\n- **Step 2**: Exploring Data Frame\n- **Step 3**: Data Preparation\n- **Step 4**: Splitting the Dataset\n- **Step 5**: Feature Scaling\n- **Step 6**: Model Building\n- **Step 7**: Model Evaluation\n- **Step 8**: Model Validation\n- **Step 9**: Model Interpretation\n- **Step 10**: Conclusion","071a674a":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","fb545567":"#### Converting Binary Variable gender (Male\/ Female) into (1\/ 0)","867ec3b1":"Now we split the dataset into Training set and Testing set with ratio 7:3","e6b96d8b":"# Step 6: Model Building\n\nNow we will build Logistic Regression model using `statsmodel` and `sklearn` libraries.","643086e8":"There are a few variables with high VIF. It's best to drop these variables as they aren't helping much with prediction and unnecessarily making the model complex. The variable 'PhoneService' has the highest VIF. So let's start by dropping that.","74ca33a7":"##### Now selecting only those columns which we used to build the final model","bece7f2f":"# Step 1: Importing Libraries","49b8a72b":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","a5cd5554":"### Checking the Correlation among Variables.\nWe are using Pearson's correlation to compute correlation matrix","48ff1ee4":"### Coverting Multinomial Variables by creating dummy variables\n\nA dummy variable is a numeric variable that represents categorical data.\n\nTechnically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values. As a practical matter, regression results are easiest to interpret when dummy variables are limited to two specific values, 1 or 0. Typically, 1 represents the presence of a qualitative attribute, and 0 represents the absence.\n\n\n##### Avoid the Dummy Variable Trap\nWhen defining dummy variables, a common mistake is to define too many variables. If a categorical variable can take on k values, it is tempting to define k dummy variables. Resist this urge. Remember, you only need k - 1 dummy variables.\n\nA kth dummy variable is redundant; it carries no new information. And it creates a severe multicollinearity problem for the analysis.","eadf8686":"### Checking for Missing Values","ad701ac2":"Accuracy of the model looks good with **81%**. Let's check Variable Inflation Factors (VIF) to check multicollinearity because Pearson's correlation calculates one-to-one correlation only. However, VIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable. ","03ce2d91":"### Checking VIFs","2acedf24":"## Building Third Model\n\nBuilding third model after dropping `MonthlyCharges`variable.","83807a32":"##### Let's first scale down the continous varibales using Standardization.\n\nWe have already trained the scalar on the `Training Set`, now we just need to transform the `Testing Set` using the same scalar.","a20f77d6":"Now we don't have any missing values","4de64cd1":"#### Observations\n\nWe can see that thresh-hold value of `0.5` would be preffered to choose if we use `Precision` and `Recall` for model evaluation.\n\nWe can in the graph that the curve for precision is quite jumpy towards the end. This is because the denominator of precision, i.e. (TP+FP) is not constant as these are the predicted values of 1s. And because the predicted values can swing wildly, we get a very jumpy curve.","8c3e6e52":"##### Comparing Actual Churn and Predicted Churn on Training set ","88852a23":"#### All variables have a good value of VIF and none of the variable is insignificant. Therefore we don't need to drop any more variables and we can proceed with this model.","83b41304":"Now let's look at some different metrics for evaluation which we come accross in the theoritical part. However for this dataset we will refer `Sensitivity` and `Specificity` only.","7279b766":"## Building First Model\n\nBuilding first model with all selected varaiables","6bd1707f":"## Precision and Recall","157ddbe1":"We can notice that p-value for all variables are < 0.05","8206816f":"Optimal thresh-hold probability is that probability where we get balanced sensitivity and specificity","426f6ed6":"##### Precision \n\nIt tells us the percentage of 1\u2019s predicted correctly out of total 1\u2019s predicted.\n\nPrecision = TP \/ (TP + FP)\n\n##### Recall\n\nIt tell us the 1\u2019s predicted correctly out of total actual 1\u2019s. It's basically sensitivity.\n\nRecall = TP \/ (TP + FN)","d6e97422":"## Finding the Optimal Thresh-hold value","59f06a0e":"We can notice that p-value of all features is < 0.05","7a42114a":"# Step 9: Model Interpretation\n\nNow we reached to the final step in this notebook which is interpretation of model coefficients and making the final conclusion.","55a56ad8":"## Building Fourth Model\n\nBuilding Fourth model after dropping `MultipleLines_Yes` variable","da7d82ce":"### Model Evaluation (Testing Set)","b1a3b351":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","b51ea183":"### Let's look at the Confustion Matrix, Accuracy, Sensitivity and Specificity for the final optimal thresh-hold value","4fe02797":"##### Making predictions on the Training Set","b2b3e243":"We dropped `No phone service` and `No internet service` because they are already included from columns `InternetService` and `PhoneService` as we can see below","528a7d44":"# Step 3: Data Preparation","421676a3":"### Feature Selection Using RFE\n\nSelecting top 15 features for the model out of 23 features using RFE","52c2bcfc":"Now we can notice that, we have got `Sesitivity` score along with good `Accuracy` and `Specificity`.\n\nOur model performance is similar on all three metrics, now let's look at the **Receiver Operating Characteristic (ROC) curve** of the model.\n\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","307eebcf":"## Building Fifth Model\n\nBuilding fifth model after dropping `TotalCharges` variable","4548c336":"### Making Predictions using the trained model","29f6cb56":"From the above Box Plots we can see that there are no outliers.","9c2d07ba":"#### Converting Binary Variables (Yes\/ No) into (1\/ 0)","81677790":"We have seen `MultipleLines_Yes` variable is insignificant due to p-value and `TotalCharges` has high VIF value. However, we will drop only one variable at a time and there we give priority to p-value more than VIF.\n\nTherefore, we will drop `MultipleLines_Yes`","d5608ad1":"### Let's look at the Confustion Matrix, Accuracy, Sensitivity and Specificity for the final optimal thresh-hold value","5f7233bb":"We can notice that p-value for all variables are < 0.05","916738b7":"## Confusion Metrix","1a2d68a4":"### Checking VIFs","51d15981":"The accuracy is still practically the same.","57d2aae4":"#### Final Probabilities corrosponding to the customerID","c3014ba5":"We have transformed all the variables, now next step is to check for outlier in the dataset.","b16b062b":"We will perform Coarse tuning and Fine tuning technique to do `Feature Selection` and select best features for the model\n\n- **Coarse Tuning**: Recursive Feature Elimination (RFE) \n- **Fine Tuning**: Variable Inflation Factors (VIF) and p-value","a224017b":"##### Creating new column 'final_predicted' with 1 if Churn_Prob > 0.3 else 0\n\nChoosing the optimal thresh-hold value","6bce3b68":"### Observations\n\nWe can notice that, for the Testing Set, all three metrics shows similar score as observed in the Training Set.\n\n**Training Set**\n- Accuracy of the model:  0.769\n- Sensitivity of the model:  0.776\n- Specificity of the model:  0.767\n\n**Testing Set**\n- Accuracy of the model:  0.741\n- Sensitivity of the model:  0.72\n- Specificity of the model:  0.749\n\nWe can notice that scores are similar for both Training and Testing Set, this shows that the model we build using Training Set also fits good and generalizes on the Testing Set.\n\nHence, we are ready to deploy the model and make predicitions and decisions using that.\n\nHowever, from time to time we need to monitor its performance and if the accuracy drops on the new data then we need to rebuild the model using new data.","1cbc2088":"# Step 7: Model Evaluation","88441551":"### Checking for Outliers","41778231":"##### Creating new column 'Predicted' with 1 if Churn_Prob > 0.5 else 0","ac37db15":"##### Making predictions on the Training Set","7e4fd81b":"# Step 2: Exploring Data Frame","7e846ed8":"### Checking VIFs","32174851":"### Observations\n\n**Tenure:**\n\n- Coefficient: -.90\n- Odds Ratio: 0.41\n\n*Tenure is continous variable which was standarized using Standard Scalar. Therefore, for 1 stardardized unit increase the odds of getting churned reduces by 59%. We know that 1 stardardized unit of tenure is equal to 24.5 months, therefore for increase in tenure by 24.5 months will lead to decrease in customer getting churned by 59%.*\n\n**PaperlessBilling**\n\n- Coefficient: 0.35\n- Odds Ratio: 1.42\n\n*The odds of a customer to get churned in case he\/ she has opted for Paperless Billing are 1.42 higher than in case of Not opted for Paperless Billing, considering every other variable same.*\n*In terms of percentage change, the odds of customer with Paperless Billing getting churned is 42% higher than the odds of customer with not Paperless Billing getting churned.*\n\n**SeniorCitizen**\n\n- Coefficient: 0.47\n- Odds Ratio: 1.60\n\n*The odds of a Senior Citizen customer to get churned are 1.60 higher than in case of non-Senior Citizen, considering every other variable same.*\n*In terms of percentage change, the odds of a Senior Citizen customer getting churned is 60% higher than the odds of not Senior Citizen customer getting churned.*\n\n**Contract_One year**\n\n- Coefficient: -0.74\n- Odds Ratio: 0.48\n\n*The odds of a customer with One Year contract to get churned are 0.52 lower than in case of customer not having One Year contract, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with One Year contract getting churned is 52% lesser than the odds of a customer not having One Year contract getting churned.*\n\n**Contract_Two year**\n\n- Coefficient: -1.31\n- Odds Ratio: 0.27\n\n*The odds of a customer with Two Year contract to get churned are 0.73 lower than in case of customer not having Two Year contract, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Two Year contract getting churned is 73% lesser than the odds of a customer not having Two Year contract getting churned.*\n\n**PaymentMethod_Credit card (automatic)**\n\n- Coefficient: -0.39\n- Odds Ratio: 0.68\n\n*The odds of a customer with Automatic Payment via Credit Card to get churned are 0.32 lower than in case of customer not having Automatic Payment via Credit Card, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Automatic Payment via Credit Card getting churned is 32% lesser than the odds of a customer not having Automatic Payment via Credit Card getting churned.*\n\n**PaymentMethod_Credit card (automatic)**\n\n- Coefficient: -0.39\n- Odds Ratio: 0.68\n\n*The odds of a customer with Automatic Payment via Credit Card to get churned are 0.32 lower than in case of customer not having Automatic Payment via Credit Card, considering every other variable same.*\n*In terms of percentage change, the odds of a customer with Automatic Payment via Credit Card getting churned is 32% lesser than the odds of a customer not having Automatic Payment via Credit Card getting churned.*\n\n**PaymentMethod_Mailed check**\n\n\n- Coefficient: -0.34\n- Odds Ratio: 0.71\n\n*The odds of a customer with have enabled Payment Method via Mail Check to get churned are 0.29 lower than in case of customer not enabled Payment Method via Mail Check, considering every other variable same.*\n*In terms of percentage change, the odds of a customer enabled Payment Method via Mail Check getting churned is 29% lesser than the odds of a customer not enabled Payment Method via Mail Check getting churned.*\n\n**InternetService_Fiber Optic**\n\n- Coefficient: 0.86\n- Odds Ratio: 2.37\n\n*The odds of a customer with having Fiber Optic service to get churned are 2.37 higher than in case of customer not having Fiber Optic service, considering every other variable same.*\n*In terms of percentage change, the odds of a customer having Fiber Optic service getting churned is 137% higher than the odds of a customer not having Fiber Optic service getting churned.*\n\n**InternetService_No**\n\n- Coefficient: -0.97\n- Odds Ratio: 0.38\n\n*The odds of a customer with Not having Interner Services to get churned are 0.62 lower than in case of customer having Interner Services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Not having Interner Services getting churned is 62% lesser than the odds of a customer having Interner Services getting churned.*\n\n**TechSupport_Yes**\n\n- Coefficient: -0.41\n- Odds Ratio: 0.67\n\n*The odds of a customer with having Tech Support to get churned are 0.33 lower than in case of customer not having Tech Support , considering every other variable same.*\n*In terms of percentage change, the odds of a customer having Tech Support  getting churned is 33% lesser than the odds of a customer not having Tech Support getting churned.*\n\n**StreamingTV_Yes**\n\n- Coefficient: 0.35\n- Odds Ratio: 1.41\n\n*The odds of a customer with Streaming TV services to get churned are 1.41 higher than in case of customer not having Streaming TV services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Streaming TV services getting churned is 41% higher than the odds of a customer not having Streaming TV services getting churned.*\n\n**StreamingMovies_Yes**\n\n- Coefficient: 0.25\n- Odds Ratio: 1.28\n\n*The odds of a customer with Streaming Movies services to get churned are 1.28 higher than in case of customer not having Streaming Movies services, considering every other variable same.*\n*In terms of percentage change, the odds of a customer Streaming Movies services getting churned is 28% higher than the odds of a customer not having Streaming Movies services getting churned.*","41b13bc7":"This means we have **21 features** about a customer including target variable `Churn` and we have details for **7043 customers**.\n\nBrief description about each feature (column) is given below:\n1. `customerID`: The unique ID of each customer\n2. `tenure`: Number of monthscustomer has been using the service\n3. `PhoneService`: Whether a customer has a Phone services or not (Yes, No)\n4. `Contract`: The contract term of the customer (Month-to-month, One year, Two year)\n5. `PaperlessBilling`: Whether a customer has opted for paperless billing (Yes, No)\n6. `PaymentMethod`: The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))\n7. `MonthlyCharges`: Specifies the money paid by a customer each month\n8. `TotalCharges`: The total money paid by the customer to the company\n9. `Churn`: This is the target variable which specifies if a customer has churned or not (Yes, No)\n10. `gender`: The gender of a person (Male, Female)\n11. `SeniorCitizen`: Whether a customer can be classified as a senior citizen (1=Yes, 0=No)\n12. `Partner`: Whether the customer has a partner or not (Yes, No)\n13. `Dependents`: Whether the customer has dependents(children\/ retired parents) or not (Yes, No)\n14. `MultipleLines`: Whether the customer has multiple lines or not (Yes, No, No phone service)\n15. `InternetService`: Customer\u2019s internet service provider (DSL, Fiber optic, No)\n16. `OnlineSecurity`: Whether the customer has online security or not (Yes, No, No internet service)\n17. `OnlineBackup`: Whether the customer has online backup or not (Yes, No, No internet service)\n18. `DeviceProtection`: Whether the customer has device protection or not (Yes, No, No internet service)\n19. `TechSupport`: Whether the customer has tech support or not (Yes, No, No internet service)\n20. `StreamingTV`: Whether the customer has streaming TV or not (Yes, No, No internet service)\n21. `StreamingMovies`: Whether the customer has streaming movies or not (Yes, No, No internet service)","19fbca82":"# Step 10: Conclusion","b18d6806":"# Step 8: Model Validation\n\nWe are done with the model building and evaluation steps, now let's check for the stability of the model.\n\nWhether the model gives similar scores on the `Testing Set` also. We are using `In-sample` validation for this problem. ","65f9ece2":"We can notice that we have got very good Accuracy and Specificity score, however, Sensitivity score is not that good.\n\nIt means that our model is not able to capture `churned` customer very well and this can be a problematic for the business. We wish to capture them properly, but How?\n\n**Remember**: We declared the customer as churn (1) or not churn (0) from probabilities based on some arbitrary thresh-hold. We chose that thresh-hold to be 0.5 i.e. any customer with prob > 0.5 marked as churn (1) else not churn (0).\n\nTherefore, now we need to find optimal value of the thresh-hold so that our model can capture churn customer well.","90e80e7f":"### Checking Accuracy of the Model","6c31fdcb":"#### Variables are of different types, which are categorized below\n\n- **Categorical**:\n    - **Binary (7)**: `SeniorCitizen`, `gender`, `Partner`, `Dependents`, `PhoneService`, `PaperlessBilling`, and `Churn`\n    \n    - **Multimonial (11)**: `CustomerID`,\u00a0`MultipleLines`,`InternetService`,\u00a0`OnlineSecurity`,\u00a0`OnlineBackup`,\u00a0`DeviceProtection`,\u00a0`TechSupport`,\u00a0`StreamingTV`, `StreamingMovies`, `Contract`, `PaymentMethod`\n    \n    \n- **Continuous(3)**: `TotalCharges\u00a0`, `MonthlyCharges` and `Tenure`\u00a0","356fe2c3":"The accuracy is still practically the same.","c4acad13":"It means that 11\/7043 = 0.001561834 i.e 0.1%, best is to remove these observations from the analysis","0114a873":"### Minimum samples required for building a Logistic Regression Model\n\nFor multivariable logistic regression, Peduzzi, Concato, Kemper, Holford, & Feinstein (1996) suggested a very simple guideline for a minimum number of cases for logistic regression study.\n\n**p:** Smallest of the proportions of negative or positive cases in the population\n\n**k:** Number of Independent variables\n\\begin{equation*}\n\\mathbf{N}   = \\frac{10*k}{p}\n\\end{equation*}","964d54b1":"#### From the above curve ,we can notice that 0.3 is the optimum thresh-hold value","31434514":"### Checking Accuracy of the Model","68d63618":"## Metrics for Evaluation"}}