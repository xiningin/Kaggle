{"cell_type":{"78ca581d":"code","5d8c4401":"code","c5e99c61":"code","f41e180f":"code","611f364b":"code","3e95eadd":"code","060526d1":"code","9189550d":"code","826c7075":"code","efe27806":"code","2f6968ba":"code","2ef0341f":"code","4032e2d9":"code","305fab6f":"code","51404e4a":"code","caa4107d":"code","c2c1a6ff":"code","900a7ea6":"code","23298016":"code","561aedf6":"code","03f0f9ec":"code","50ae5c23":"code","a5a67c8b":"code","a943d73f":"code","8306a051":"code","350a5300":"code","a72dcb19":"code","ec20a14a":"code","103687af":"code","8209b101":"code","b48306a9":"code","b6ca72d3":"code","0dec0536":"code","fd282e16":"code","12413228":"code","22cb3b23":"code","3261f1db":"code","fb4d4725":"code","507ec455":"code","6e57bf5e":"code","7df633ea":"code","cd7e10a6":"code","17fc2134":"code","bca0def1":"code","e1e72b3f":"code","45643db5":"code","67424fb6":"code","37a124da":"code","7e66fb12":"code","9cf2e300":"code","f8d2fe55":"code","35077435":"code","3373f886":"code","ae309192":"code","fa4d5888":"code","df18ae37":"code","dd39a34d":"code","f18ce19d":"code","047f6fde":"code","163b9e8d":"code","4c2ceb7c":"code","1cb7364e":"code","442ad949":"code","b76ca5fa":"code","63243e3a":"code","d0d09aad":"code","8c98359c":"code","05f4e729":"markdown","857f7b59":"markdown","ddd33710":"markdown","9e4d83c2":"markdown","5bba9337":"markdown","442f8b96":"markdown","86a442f5":"markdown"},"source":{"78ca581d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%matplotlib inline\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport xgboost\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,SpatialDropout1D\nfrom tensorflow.keras.layers import LSTM,Dropout\nfrom keras.layers import Bidirectional\nfrom tensorflow.keras.optimizers import RMSprop,Adam\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5d8c4401":"train=pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv\")","c5e99c61":"train.head()","f41e180f":"test.head()","611f364b":"def drop(te):\n    te.drop(\"id\",axis=1,inplace=True)","3e95eadd":"drop(train)","060526d1":"drop(test)","9189550d":"train.isnull().any()","826c7075":"test.isnull().any()","efe27806":"train[\"label\"].value_counts()","2f6968ba":"sns.countplot(\"label\",data=train)","2ef0341f":"tweettoken = TweetTokenizer(strip_handles=True, reduce_len=True)","4032e2d9":"stemmer=PorterStemmer()","305fab6f":"collect=[]\ncollecttest=[]\ndef preprocess(t,kpc):\n    tee=re.sub('[^a-zA-Z]',\" \",t)\n    tee=tee.lower()\n    res=tweettoken.tokenize(tee)\n    for i in res:\n        if i in stopwords.words('english'):\n            res.remove(i)\n    rest=[]\n    for k in res:\n        rest.append(stemmer.stem(k))\n    ret=\" \".join(rest)\n    if kpc==1:\n        collect.append(ret)\n    elif kpc==0:\n        collecttest.append(ret)","51404e4a":"def splitpro(t,q,m):\n         for j in range(q):\n                 preprocess(t[\"tweet\"].iloc[j],m)","caa4107d":"splitpro(train,31962,1)","c2c1a6ff":"splitpro(test,17197,0)","900a7ea6":"len(collect)","23298016":"len(collecttest)","561aedf6":"len(test)","03f0f9ec":"collect[:5]","50ae5c23":"collecttest[:5]","a5a67c8b":"val=train[\"label\"].values","a943d73f":"val","8306a051":"def bow(ll):\n    cv=CountVectorizer(max_features=200)\n    x=cv.fit_transform(ll).toarray()\n    return x\n    ","350a5300":"y=bow(collect)","a72dcb19":"y[0]","ec20a14a":"len(y[0][:])","103687af":"from imblearn.under_sampling import NearMiss","8209b101":"tt=NearMiss()\nx_us,y_us=tt.fit_sample(y,val)","b48306a9":"x_us.shape","b6ca72d3":"(x_train,x_test,y_train,y_test) = train_test_split(x_us,y_us, train_size=0.80, random_state=42)","0dec0536":"x_train","fd282e16":"rnd_clf=RandomForestClassifier(n_estimators=200,random_state=42)","12413228":"rnd_clf.fit(x_train,y_train)","22cb3b23":"rnd_clf.score(x_test,y_test)","3261f1db":"mm=[300,400,500,600]\nfor i in mm:\n    rnd_clf=RandomForestClassifier(n_estimators=i,random_state=42)\n    rnd_clf.fit(x_train,y_train)\n    t=rnd_clf.score(x_test,y_test)\n    print(t)\n    print(\"*\"*40)","fb4d4725":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(x_train,y_train)\nclf.score(x_train,y_train)","507ec455":"params={\n    \"eta\":[0.01,0.2],\n    \"min_child_weight\":[1,2,3,4,5,6,7,8,9,10],\n    \"max_depth\":[3,6,8,10,15,20,25,30],\n    \"gamma\":[0.0,0.1,0.2,0.3,0.4,0.5,0.6],\n    \"subsample\":[0.5,0.6,0.7,0.8,0.9],\n    \"colsample_bytree\":[0.6,0.7,0.8,0.8],\n    \"reg_alpha\":[0,0.001,0.005,0.01,0.05],\n    \"learning_rate\":[0.05,0.10,0.15,0.20,0.25,0.30],\n    \"max_leaf_nodes\":[8,16,24,32,40],\n}","6e57bf5e":"from sklearn.model_selection import RandomizedSearchCV","7df633ea":"classifier=xgboost.XGBClassifier()","cd7e10a6":"random=RandomizedSearchCV(classifier,param_distributions=params,n_iter=5,scoring=\"roc_auc\",cv=5,verbose=3)","17fc2134":"random.fit(x_train,y_train)","bca0def1":"random.best_estimator_","e1e72b3f":"classifier=xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, eta=0.01, gamma=0.1,\n              gpu_id=-1, importance_type='gain', interaction_constraints='',\n              learning_rate=0.2, max_delta_step=0, max_depth=25,\n              max_leaf_nodes=40, min_child_weight=1,\n              monotone_constraints='()', n_estimators=100, n_jobs=0,\n              num_parallel_tree=1, random_state=0, reg_alpha=0.05, reg_lambda=1,\n              scale_pos_weight=1, subsample=0.6, tree_method='exact',\n              validate_parameters=1, verbosity=None)","45643db5":"classifier.fit(x_train,y_train)\nclassifier.score(x_test,y_test)","67424fb6":"from sklearn import svm\nC = [1,10,20,25,30,35,40,50]","37a124da":"for i in C:\n    svc = svm.SVC(kernel='linear', C=i)\n    svc.fit(x_train,y_train)\n    t=svc.score(x_test,y_test)\n    print(t)","7e66fb12":"for i in C:\n    svc = svm.SVC(kernel='rbf', C=i)\n    svc.fit(x_train,y_train)\n    t=svc.score(x_test,y_test)\n    print(t)","9cf2e300":"oneh=[]\noneht=[]\ndef hot(cc,k):\n    for i in cc:\n        if k==1:\n            oneh.append(one_hot(i,10000))\n        elif k==0:\n            oneht.append(one_hot(i,10000))","f8d2fe55":"hot(collect,1)","35077435":"hot(collecttest,0)","3373f886":"len(oneh[0])","ae309192":"oneh[:1]","fa4d5888":"len(oneh)","df18ae37":"len(oneht)","dd39a34d":"max=0\nfor i in oneh:\n    tq=len(i)\n    if tq>max:\n        max=tq\nprint(max)","f18ce19d":"sent=40\nemoneh=pad_sequences(oneh,padding=\"pre\",maxlen=sent)\nemoneht=pad_sequences(oneht,padding=\"pre\",maxlen=sent)","047f6fde":"emoneh[:1]","163b9e8d":"emoneht[:1]","4c2ceb7c":"xtrain,xtest,ytrain,ytest=train_test_split(emoneh,val,train_size=0.80,random_state=42)","1cb7364e":"model=Sequential()\nmodel.add(Embedding(10000,100,input_length=sent))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, activation='softmax'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","442ad949":"model.summary()","b76ca5fa":"model.fit(xtrain,ytrain,validation_data=(xtest,ytest),epochs=50,batch_size=300,verbose=2)","63243e3a":"ytest","d0d09aad":"val","8c98359c":"model.predict(emoneht)","05f4e729":"Lets check if their is any null value","857f7b59":"# DATA BALANCING AND GOING FORWARD WITH ML ALGORITHMS","ddd33710":"# DATA PREPROCESSING","9e4d83c2":"lEt initiliaze our stemmer and tokenizer and do some data processing","5bba9337":"Data is imbalanced we will balanced it later before sending it to our Model","442f8b96":"Balancing imbalanced dataset","86a442f5":"# GOING FORWARD WITH KERAS"}}