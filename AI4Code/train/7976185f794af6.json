{"cell_type":{"df95b32a":"code","39af0f14":"code","b5309ccc":"code","a37e11f7":"code","af3967f5":"code","c5bd7cb8":"code","7f7c5b67":"code","91af852c":"code","24b82930":"markdown","8e8ca2f3":"markdown","bfb40301":"markdown","e078cdfc":"markdown"},"source":{"df95b32a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39af0f14":"import matplotlib.pyplot as plt\nimport seaborn as sns","b5309ccc":"train_df = pd.read_csv(os.path.join(dirname, 'train.csv'))","a37e11f7":"seq = train_df['standard_error']\nfilter_zero = seq != 0\n\nprint(f'mean standard_error: {seq[filter_zero].mean():.4f}')\nprint(f'min standard_error: {seq[filter_zero].min():.4f}')\nprint(f'max standard_error: {seq[filter_zero].max():.4f}')\n\nsns.histplot(seq[filter_zero])\nplt.title('Distribution of S.E.')","af3967f5":"import random\n\nrsme_public = []\nseq = train_df['standard_error']\n\nfor i in range(10000):\n    np.random.seed(123 + i)\n    sampled = seq.sample(n=int(0.3 * 2000), replace=True, random_state=234 + i).values\n    noize = np.array(list(map(lambda e: np.random.normal(0, e, 1)[0], sampled)))\n    rsme_public.append(np.sqrt(np.square(noize).mean()))\n    assert len(noize) == 600\n\nsns.histplot(rsme_public)\nplt.title('#sample dataset = 600')","c5bd7cb8":"print(f'p2.5 = {np.quantile(rsme_public, 0.025):.4f}')\nprint(f'p97.5 = {np.quantile(rsme_public, 0.975):.4f}')\nprint(f'mean = {np.mean(rsme_public):.4f}')\nprint(f'std = {np.std(rsme_public):.4f}')","7f7c5b67":"import random\n\nrsme_private = []\nseq = train_df['standard_error']\n\nfor i in range(10000):\n    np.random.seed(123 + i)\n    sampled = seq.sample(n=int(2000), replace=True, random_state=234 + i).values\n    noize = np.array(list(map(lambda e: np.random.normal(0, e, 1)[0], sampled)))\n    rsme_private.append(np.sqrt(np.square(noize).mean()))\n    assert len(noize) == 2000\n\nsns.histplot(rsme_private)\nplt.title('#sample dataset = 2000')","91af852c":"print(f'p2.5 = {np.quantile(rsme_private, 0.025):.4f}')\nprint(f'p97.5 = {np.quantile(rsme_private, 0.975):.4f}')\nprint(f'mean = {np.mean(rsme_private):.4f}')\nprint(f'std = {np.std(rsme_private):.4f}')","24b82930":"## Result\n\nIn the above experiments, can say:\n\n1. the theoretical lower bound of the public LB score is around 0.46-0.52 (for 95% confidence)\n2. the theoretical lower bound of the private LB score is around 0.48-0.51 (for 95% confidence)\n\n### Note\n\nThis notebook\u2019s argument is just examining the general feature of RSME loss if the residual errors are Gaussian distribution N(0, \u03c3) [1]. (Strictly saying, they are slightly different because standard error is not constant.)\n\nBesides, the result relies on the correctness of the standard error of the Bradley-Terry estimation of the target value. I wonder if this errors truly are the theoretical lower bound or not. At least, the target must contains some uncertainty, and this notebook shows how RSME changes if the uncertainty is around 0.49.\n\n[1] https:\/\/zenn.dev\/bilzard\/articles\/f62c762a0016b9","8e8ca2f3":"# Rough Argument about the Theoretical Lowerbound of the LB Score\n\nIn this notebook, I argued rough argument about theoretical lower bound for the task.\n\nConsidering how the host created the target value[1], it's natural to say that the target value already contains the uncertainty which corresponds to the `standard_error`.\n\nSo I conducted a naive experiment:\n\n1. generate Gaussian noize (sigma = standard error)\n2. resample 600, 2000 samples from the Gausian noize (these corresponds to the # of dataset in public and the private test datasets)\n3. calculate RSME error (this corresponds to the hypothesis that the model estimates the targets in 100% accuracy)\n4. iterate 1-3 for 10000 times and argure with the variance of RSME\n\n---\n\n[1] https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423","bfb40301":"First, naive estimation of RSME of 100% accuracy model is just calculate the mean of standard_error:","e078cdfc":"Then, let's do the experiment with 10000 iteration.\n\n1. sampling of 600 datasets\n2. sampling of 2000 datasets"}}