{"cell_type":{"14c96364":"code","211d79bd":"code","929a79c4":"code","cc518a8c":"code","9cc0987e":"code","f0a830e4":"code","73964cdc":"code","f95ce954":"code","7eec1467":"code","0611761a":"code","d8cf5876":"code","bb02a48a":"code","517ba28f":"code","ef83040e":"code","bbc1f3cb":"code","1406d0e8":"code","6cb49702":"code","32ee3102":"code","6ac57013":"code","d7bcca15":"code","6f6e9534":"code","ac7970f6":"code","cb01ec0a":"code","ba2aebbb":"code","4ce75385":"code","f677fa6d":"code","9e2102d3":"code","56305891":"code","a1180c57":"code","83ffe3d3":"code","088d600c":"code","82d63a40":"code","d5d678c1":"code","7a644a0c":"code","38f0cbdd":"code","229669ac":"code","0c0d07a9":"code","9f48d37c":"code","ba1b9e07":"code","d27885f1":"code","b2565818":"code","276e9744":"code","7dd5a884":"code","d12befca":"code","fb227dbf":"code","a89a8fd3":"code","becc4803":"code","953eb702":"code","c7fbb13d":"code","18ee0c1f":"code","3eb5da4d":"code","7d35412c":"code","945640b8":"code","fc7b5b0b":"code","b4bc9929":"code","6567ec85":"code","8c1b8db6":"code","cf280048":"code","53d35fe4":"code","00c726be":"code","3b1b8fab":"code","12c0004e":"code","4b5f7462":"code","e4d66224":"code","6d2cc899":"code","d61f4dd3":"code","792ff54b":"code","1ce2309c":"code","ef545fdc":"code","15c6d250":"code","fec52319":"code","e0dd8431":"code","26f5c9ef":"code","7d4c82c7":"code","8803b2ec":"code","9fa6dda4":"code","5d66edcf":"code","004530b7":"code","48f94352":"code","9ac54246":"markdown","90b6b9e7":"markdown","c6d5ed2f":"markdown","dd3b2172":"markdown","f9915ded":"markdown","c01b32ee":"markdown","126baf5c":"markdown","6f8c5cdc":"markdown","23acee12":"markdown","33ba71d7":"markdown","7a6e9ea9":"markdown","a88a12ad":"markdown","c1e278ff":"markdown","43365bd9":"markdown","31dadf8d":"markdown","053f0126":"markdown","19afef33":"markdown","73e8ae73":"markdown","fe738f3c":"markdown","beebc07b":"markdown","e8b8db07":"markdown","5f2601af":"markdown","0a54f7ad":"markdown","1ee54897":"markdown","b665c7c0":"markdown","3d80a149":"markdown","676208cc":"markdown","75f8eced":"markdown"},"source":{"14c96364":"import numpy as np\nimport pandas as pd\nimport os\nfrom keras import models, layers, Sequential\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier","211d79bd":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","929a79c4":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","cc518a8c":"## A merged dataset for common manipulations\nmerged_df = pd.concat((train_df.drop(['Survived'], axis = 1), test_df))\nmerged_df.head()","9cc0987e":"print(train_df.shape)\nprint(test_df.shape)\nprint(merged_df.shape)","f0a830e4":"train_df.info()","73964cdc":"import seaborn as sb\nfrom matplotlib import pyplot as plt","f95ce954":"train_df.groupby(['Pclass'])['Survived'].sum() \/ train_df.groupby(['Pclass'])['Survived'].count()","7eec1467":"sb.barplot(x = 'Pclass', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"Pclass\")\nplt.show()","0611761a":"train_df.groupby(['Sex'])['Survived'].sum() \/ train_df.groupby(['Sex'])['Survived'].count()","d8cf5876":"sb.barplot(x = 'Sex', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"Sex\")\nplt.show()","bb02a48a":"train_df.groupby(['SibSp'])['Survived'].sum() \/ train_df.groupby(['SibSp'])['Survived'].count()","517ba28f":"sb.barplot(x = 'SibSp', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"SibSp\")\nplt.show()","ef83040e":"train_df.groupby(['Parch'])['Survived'].sum() \/ train_df.groupby(['Parch'])['Survived'].count()","bbc1f3cb":"sb.barplot(x = 'Parch', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"Parch\")\nplt.show()","1406d0e8":"train_df['Family'] = train_df['SibSp'] + train_df['Parch']","6cb49702":"train_df.groupby(['Family'])['Survived'].sum() \/ train_df.groupby(['Family'])['Survived'].count()","32ee3102":"sb.barplot(x = 'Family', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"Family\")\nplt.show()","6ac57013":"train_df.drop(['Family'], axis = 1, inplace = True) #Dropping it here. We will use it as a custom feature later","d7bcca15":"ticket_num_records = train_df.groupby(['Ticket']).size().sort_values(ascending=False).to_dict()\ntrain_df.groupby(['Ticket']).size().sort_values(ascending=False).head()","6f6e9534":"train_df['Companion'] = train_df['Ticket'].apply(lambda x: ticket_num_records[x])","ac7970f6":"train_df.groupby(['Companion'])['Survived'].sum() \/ train_df.groupby(['Companion'])['Survived'].count()","cb01ec0a":"sb.barplot(x = 'Companion', y = 'Survived', data = train_df)\nplt.ylabel(\"Survived\")\nplt.xlabel(\"Companion\")\nplt.show()","ba2aebbb":"train_df.drop(['Companion'], axis = 1, inplace = True)","4ce75385":"train_df.groupby(pd.cut(train_df[\"Fare\"], np.arange(0, 350, 25)))['Survived'].sum() \/ train_df.groupby(pd.cut(train_df[\"Fare\"], np.arange(0, 350, 25)))['Survived'].count()","f677fa6d":"train_df[\"Cabin\"].unique()","9e2102d3":"train_df['CabinId'] = train_df['Cabin'].apply(lambda x: 'None' if pd.isna(x) else x[0])\ntrain_df.groupby(['CabinId'])['Survived'].sum() \/ train_df.groupby(['CabinId'])['Survived'].count()","56305891":"train_df.drop(['CabinId'], axis = 1, inplace = True)","a1180c57":"train_df['Embarked'].unique()","83ffe3d3":"train_df.groupby(['Embarked'])['Survived'].sum() \/ train_df.groupby(['Embarked'])['Survived'].count()","088d600c":"import re\n\ntrain_df['Name'].apply(lambda x: re.compile('.+?[,][\\s](.*?)[\\.][\\s].+').findall(x)[0]).unique()","82d63a40":"train_df['Title'] = train_df['Name'].apply(lambda x: re.compile('.+?[,][\\s](.*?)[\\.][\\s].+').findall(x)[0])\ntrain_df.groupby(['Title'])['Survived'].sum() \/ train_df.groupby(['Title'])['Survived'].count()","d5d678c1":"train_df.groupby(['Title'])['Survived'].count()","7a644a0c":"train_df.drop(['Title'], axis = 1, inplace = True)","38f0cbdd":"np.nanmean(train_df['Age'])","229669ac":"np.nanmean(train_df[train_df['Name'].str.contains('Master')]['Age'])","0c0d07a9":"np.nanmean(train_df[train_df['Name'].str.contains('Miss')]['Age'])","9f48d37c":"#Adding Title as a feature\nmerged_df['Title'] = merged_df['Name'].apply(lambda x: re.compile('.+?[,][\\s](.*?)[\\.][\\s].+').findall(x)[0])","ba1b9e07":"merged_df.head()","d27885f1":"#We will take the mean values from training data, as per convention\n\nboymean = np.nanmean(train_df[train_df['Name'].str.contains('Master.')]['Age'])\ngirlmean = np.nanmean(train_df[train_df['Name'].str.contains('Miss.')]['Age'])\nmeanage = np.nanmean(train_df['Age'])","b2565818":"merged_df['Age'] = np.where(np.isnan(merged_df['Age']) & (merged_df['Title'] == 'Master'), boymean, merged_df['Age'])\nmerged_df['Age'] = np.where(np.isnan(merged_df['Age']) & (merged_df['Title'] == 'Miss'), girlmean, merged_df['Age'])\nmerged_df['Age'] = merged_df['Age'].fillna(meanage)","276e9744":"# Replacing the only missing Fare value with mean Fare. Then converting it to a binary feature\n\nmerged_df['Fare'] = merged_df['Fare'].fillna(np.nanmedian(merged_df['Fare']))\nmerged_df['Fare'] = merged_df['Fare'].apply(lambda x: 1 if x > 75.0 else 0)","7dd5a884":"# Reformatting the Cabin column\n\nmerged_df['Cabin'] = merged_df['Cabin'].apply(lambda x: 0 if pd.isna(x) else 1)","d12befca":"# Filling two empty Embarked data with an arbitrary character\n\nmerged_df['Embarked'] = merged_df['Embarked'].fillna('N')","fb227dbf":"merged_df['Family'] = merged_df['Parch'] + merged_df['SibSp']","a89a8fd3":"merged_df.info()","becc4803":"merged_df.head()","953eb702":"# Removing features that I don't think has any significance\n\nmerged_df.drop(['Name', 'Ticket', 'SibSp', 'Parch'], axis = 1, inplace = True)","c7fbb13d":"merged_df.head()","18ee0c1f":"train_df.groupby(pd.cut(train_df[\"Age\"], np.arange(0, 100, 20)))['Survived'].sum() \/ train_df.groupby(pd.cut(train_df[\"Age\"], np.arange(0, 100, 20)))['Survived'].count()","3eb5da4d":"maxAge = train_df['Age'].max()\nminAge = train_df['Age'].min()\nmerged_df['Age'] = (merged_df['Age'] - minAge)\/(maxAge - minAge)","7d35412c":"merged_df.head()","945640b8":"merged_df['Age'].max()","fc7b5b0b":"dummiesPclass = pd.get_dummies(merged_df['Pclass'], prefix = 'Pclass')\nmerged_df = pd.concat([merged_df, dummiesPclass], axis=1)\nmerged_df.head()","b4bc9929":"dummiesFare = pd.get_dummies(merged_df['Fare'], prefix = 'Fare')\nmerged_df = pd.concat([merged_df, dummiesFare], axis=1)\nmerged_df.head()","6567ec85":"merged_df.groupby(['Title'])['PassengerId'].count()","8c1b8db6":"merged_df['Title'] = merged_df['Title'].apply(lambda x: 'Miss' if (x in ['Mlle', 'Mme', 'Ms']) else x)\nmerged_df['Title'] = merged_df['Title'].apply(lambda x: 'Mrs' if (x in ['Dona', 'Lady']) else x)\nmerged_df['Title'] = merged_df['Title'].apply(lambda x: 'Mr' if (x == 'Rev') else x)\nmerged_df['Title'] = merged_df['Title'].apply(lambda x: x if (x in ['Master', 'Mr', 'Mrs', 'Miss']) else 'Other')","cf280048":"merged_df.groupby(['Title'])['PassengerId'].count()","53d35fe4":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nmerged_df['Title'] = le.fit_transform(merged_df['Title'])","00c726be":"dummiesTitle = pd.get_dummies(merged_df['Title'], prefix = 'Title')\nmerged_df = pd.concat([merged_df, dummiesTitle], axis=1)\nmerged_df.head()","3b1b8fab":"merged_df['Sex'] = le.fit_transform(merged_df['Sex'])\ndummiesSex = pd.get_dummies(merged_df['Sex'], prefix = 'Sex')\nmerged_df = pd.concat([merged_df, dummiesSex], axis=1)\nmerged_df.head()","12c0004e":"dummiesCabin = pd.get_dummies(merged_df['Cabin'], prefix = 'Cabin')\nmerged_df = pd.concat([merged_df, dummiesCabin], axis=1)\nmerged_df.head()","4b5f7462":"merged_df['Embarked'] = le.fit_transform(merged_df['Embarked'])\ndummiesEmbarked = pd.get_dummies(merged_df['Embarked'], prefix = 'Embarked')\nmerged_df = pd.concat([merged_df, dummiesEmbarked], axis=1)\nmerged_df.head()","e4d66224":"merged_df.head()","6d2cc899":"merged_df['Family'] = merged_df['Family'].apply(lambda x: 'N' if x == 0 else ('S' if x < 4 else 'L'))","d61f4dd3":"merged_df['Family'] = le.fit_transform(merged_df['Family'])\ndummiesFamily = pd.get_dummies(merged_df['Family'], prefix = 'Family')\nmerged_df = pd.concat([merged_df, dummiesFamily], axis=1)\nmerged_df.head()","792ff54b":"merged_df.drop(['Pclass', 'Sex', 'Fare', 'Cabin', 'Embarked', 'Title', 'Family'], axis = 1, inplace = True)","1ce2309c":"merged_df.head()","ef545fdc":"train_df_x = merged_df[:891]\ntest_df_x = merged_df[891:]","15c6d250":"print(test_df_x.shape)\nprint(test_df.shape)","fec52319":"train_df_y = train_df['Survived']","e0dd8431":"train_df = train_df_x.copy()\ntrain_df['Survived'] = train_df_y\ntrain_df.drop(['PassengerId'], axis = 1, inplace = True)\ntrain_df.head()","26f5c9ef":"from sklearn.model_selection import train_test_split\n\ndef train_and_test(model_specific_tasks, df, it = 20):\n    accsum = 0\n    minacc = 1.0\n    maxacc = 0\n    \n    for i in range(it):\n        print('Iteration: ', (i + 1), end = '\\r')\n        train, test = train_test_split(df, test_size=0.2)\n\n        train_x = train.drop(['Survived'], axis=1)\n        test_x = test.drop(['Survived'], axis=1)\n\n        train_y = train['Survived']\n        test_y = test['Survived']\n\n        train_x = np.asarray(train_x).astype('float32')\n        train_y = np.asarray(train_y).astype('float32')\n\n        acc = model_specific_tasks(train_x, train_y, test_x, test_y)\n        accsum += acc\n        minacc = acc if acc < minacc else minacc\n        maxacc = acc if acc > maxacc else maxacc\n        \n    print('Avg. accuracy: ', (accsum \/ it))\n    print('Min. accuracy: ', minacc)\n    print('Max. accuracy: ', maxacc)","7d4c82c7":"def logistic_reg(train_x, train_y, test_x, test_y):\n    model = LogisticRegression(solver='sag', max_iter=1000)\n    model.fit(train_x, train_y)\n    return model.score(test_x, test_y)","8803b2ec":"train_and_test(logistic_reg, train_df.copy(), it = 50)","9fa6dda4":"def rfc(train_x, train_y, test_x, test_y):\n    model = RandomForestClassifier(n_estimators=100)\n    model.fit(train_x, train_y)\n    return model.score(test_x, test_y)","5d66edcf":"train_and_test(rfc, train_df.copy())","004530b7":"def nn(train_x, train_y, test_x, test_y):\n    model = Sequential()\n    model.add(layers.Dense(32, activation='relu', input_shape = (22,)))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(8, activation='relu'))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    model.fit(train_x, train_y, epochs=150, batch_size=16, verbose = 0)\n    \n    return model.evaluate(test_x, test_y, verbose = 0)[1]","48f94352":"train_and_test(nn, train_df.copy(), it = 10)","9ac54246":"The relation between survival and the number of siblings or spouses is showing an interesting relation than the people with none. Those who had their spouse or their 1 or 2 siblings with them, had a better odds of survival. More siblings (one is considered to not have more than one spouse with them :p ) actually decreases the chance of survival.","90b6b9e7":"## Importing necessary tools that we might need","c6d5ed2f":"Our target class is 'Survived'. I will check individual correlation of the independent variables with our target class. After that I will try some mutated versions of the independent variables.","dd3b2172":"## Preprocessing\n\nI will work with the merged data for this step.","f9915ded":"### Neural network","c01b32ee":"This graph only indicates that, minors with their parents or people with few number of children survived more than people with no child or parents. But the relationship is definitely not very strong.\n\nI am going to merge them together. Let's see what happens.","126baf5c":"Seems like this information does not represent much correlation with survival. It should not harm the model either.","6f8c5cdc":"What I did here is, mapped people with how many of their companions embarked on the ship with same ticket. We are assuming the ticket numbers are unique.\n\nThis relation is very similar to the one with family size. I think I will skip one of them.","23acee12":"Now, we will one hot encode all categorical features","33ba71d7":"## EDA","7a6e9ea9":"In preprocessing stage, I will fill the NaN values accordingly.","a88a12ad":"Now, there are a lot of missing values in Age column. We could have used mean age from the training dataset.","c1e278ff":"We need to label encode the Sex, Cabin and Title column first. But before that, let's get rid of some statistically insignificant categorie.","43365bd9":"People who had cabins, have much higher survival ratio. A binary feature (1 if has cabin, 0 otherwise) should suffice.","31dadf8d":"But, for some of them, we can use a more targeted approach. The Master title is used for minors. Data proves it too.","053f0126":"So, there is a good correlation between passenger class and survival.\n\nI will get to the 'Name' later, as it might not be very straightforward.","19afef33":"### Logistic Regression","73e8ae73":"Apart from people with 5 more members in their family, there's a steady relationship between the number of family members and the survival. People with small family survived more than those who were alone or who had more family members.","fe738f3c":"Let's categorize the Family column into none (N), small(S) and large(L).","beebc07b":"Same goes with the title Miss","e8b8db07":"Apart from Master, Miss, Mr and Mrs, others are not that much statistically signifigant. I will get that to later.","5f2601af":"Sex is also showing a very significant correlation.\n\nAge has some missing values. We will get to it later.","0a54f7ad":"## Classification\n\nI will try some classifiers, along with a deep learning model. First, we need to split our dataset again.","1ee54897":"As it shows, the survival ratio is not showing any significant correlation. We will just perform min-max normalization.","b665c7c0":"### Random Forest Classifier","3d80a149":"We want to either normalize or make Age column as ranged data. Let's see if the ranged data makes sense.","676208cc":"I have distributed the prices in ranges to have a more clear idea about this column's relation with survival. One thing is clear from here, people who paid ticket prices over 75 dollars had better odds of survival. I think I will have a binary feature (more than \\$75, less than or eq. $75) in place of 'Fare', as that will be more meaningful.","75f8eced":"## Importing datasets"}}