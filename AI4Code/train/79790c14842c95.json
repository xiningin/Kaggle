{"cell_type":{"201ffea1":"code","c3731707":"code","9ed34899":"code","ad651be6":"code","f1ce937b":"code","4858804e":"code","731e86c4":"code","b5ec085a":"code","3943170f":"code","b91aa3e1":"code","cf05dcfb":"code","24b8575a":"code","e365a806":"code","0fd210f4":"code","f0e506ca":"code","989442a2":"code","4c1941f6":"code","713a0f65":"code","b595d2b1":"code","b1f38a59":"code","9c1d93eb":"code","de60c7c6":"code","eaf36aff":"code","5e83911f":"code","db6603ea":"code","6aaa9c48":"code","0a7cca7a":"code","be9415e8":"code","8a49c42a":"code","89dc9d80":"code","b3ee6863":"code","a9eace68":"code","5b722631":"code","ecab9777":"code","582eb827":"code","a0e510bc":"code","1033003e":"code","61fcc296":"code","1e84e25d":"code","04d38765":"code","734f933a":"code","ea24c355":"code","c9d64540":"code","fe7d689e":"code","5c1ce08e":"code","45426637":"code","efb1ca2d":"code","127065c5":"code","b23823a5":"code","546788c1":"code","1764eaf4":"code","9c1b4c58":"code","cd5585c1":"code","d0b7bc99":"code","a005c1b4":"code","322898dc":"code","626f46f3":"code","becc1a63":"code","55569453":"code","cc744479":"code","7f3acca4":"markdown","0ce84144":"markdown","92014e49":"markdown","9d30cd7c":"markdown","0fa8d01c":"markdown","5952a9fc":"markdown","b66b6d0a":"markdown","131952f1":"markdown","d0f238c7":"markdown","52c8503b":"markdown","27aeccdb":"markdown","2df02833":"markdown","539520b8":"markdown","0a83ac94":"markdown","c6c954af":"markdown","2fb969d4":"markdown","4ac24a1b":"markdown","1c107852":"markdown","a742a4af":"markdown","d6e10fdb":"markdown","27b9eb1e":"markdown","93b90a16":"markdown","955e96ad":"markdown","42f9f791":"markdown","ea7e5343":"markdown","df9f50da":"markdown","446e0685":"markdown","12914794":"markdown","49949ec1":"markdown","256609e0":"markdown","07508af7":"markdown","350d9ee5":"markdown","2e05c9c8":"markdown","58bc9099":"markdown","23587235":"markdown","257643c6":"markdown","5e0e6baf":"markdown","ab76a419":"markdown","80053151":"markdown","bc3d6821":"markdown","6e846337":"markdown","147d6edb":"markdown","92c6a883":"markdown","908311ab":"markdown","da293e99":"markdown","2d54b8ae":"markdown","fbc96051":"markdown","50c344cc":"markdown","f2645b85":"markdown","2ba88dcb":"markdown","ef5b5ffa":"markdown","e06e087e":"markdown","97c8272e":"markdown","62d7e26a":"markdown","20e93d62":"markdown"},"source":{"201ffea1":"from IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n<\/style>\n\"\"\")","c3731707":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom prettytable import PrettyTable\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer \nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport math\nimport matplotlib.cm as cm\nfrom matplotlib import rcParams\nfrom collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport re\nimport time\nimport string\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Conv1D, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.tokenize import RegexpTokenizer\nfrom wordcloud import WordCloud\nimport glob\n\n%matplotlib inline","9ed34899":"cols = ['text']\nplaintext_files=pd.DataFrame()\naddress = glob.glob('\/kaggle\/input\/clinical-documents-on-syndromes-disease\/plaintext_data\/*.txt')\ncount=0\nfor i in address:\n    with open(i, \"rb\") as data_of_files:\n        data=pd.read_csv(data_of_files, sep='\\t', header=None, names=cols)\n        plaintext_files=pd.concat([plaintext_files,data['text']], axis=0)\n        count=count+1\n        if count==493:\n            break  \n\nplaintext_files=plaintext_files.rename(columns={0:'text'})          \nplaintext_files.head(10)","ad651be6":"english_punctuations = string.punctuation\npunctuations_list = english_punctuations\ndef cleaning_punctuations(text):\n    translator = str.maketrans('', '', punctuations_list)\n    return text.translate(translator)","f1ce937b":"plaintext_files[\"text\"] = plaintext_files[\"text\"].apply(lambda x: cleaning_punctuations(x))","4858804e":"def cleaning_characters(text): \n    \n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    \n    return text","731e86c4":"plaintext_files[\"text\"] = plaintext_files[\"text\"].apply(lambda x: cleaning_characters(x))\nplaintext_files","b5ec085a":"plaintext_files= plaintext_files.text.str.split(expand=True).stack()\nplaintext_files","3943170f":"ground_truth=pd.read_csv('\/kaggle\/input\/clinical-documents-on-syndromes-disease\/ground_truth_annotation_file.csv')\nground_truth.head()","b91aa3e1":"li=list(ground_truth['text'])\nli","cf05dcfb":"Processed_data=pd.DataFrame({\n    'Sentence#': plaintext_files.index.get_level_values(0) + 1, \n    'Word': plaintext_files.values, \n    'Tag': plaintext_files.map(dict(zip(ground_truth.text, ground_truth['class']))).fillna(0).values\n})","24b8575a":"Processed_data.head()","e365a806":"Processed_data.tail()","0fd210f4":"Processed_data.columns","f0e506ca":"print('lenght of Processed_data is', len(Processed_data))","989442a2":"Processed_data.shape","4c1941f6":"Processed_data.info()","713a0f65":"Processed_data.dtypes","b595d2b1":"np.sum(Processed_data.isnull().any(axis=1))","b1f38a59":"print('Count of columns in the Processed_data is:  ', len(Processed_data.columns))","9c1d93eb":"print('Count of rows in the Processed_data is:  ', len(Processed_data))","de60c7c6":"current=len(Processed_data)\nprint('Rows of Processed_data before Delecting ', current)","eaf36aff":"now=len(Processed_data)\nprint('Rows of Processed_data before Delecting ', now)","5e83911f":"diff=current-now\nprint('Duplicated rows are ', diff)","db6603ea":"sns.countplot(data= Processed_data, x = \"Tag\")\nplt.show()","6aaa9c48":"Processed_data[\"Tag\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()","0a7cca7a":"Processed_data[\"Tag\"].value_counts()","be9415e8":"Processed_data.loc[Processed_data['Tag'] == 'Others', 'Word'].head()","8a49c42a":"Processed_data.loc[Processed_data['Tag'] == 'Disease_Syndrome', 'Word'].head()","89dc9d80":"Processed_data.loc[Processed_data['Tag'] == 0, 'Word'].head()","b3ee6863":"Processed_data['Word']=Processed_data['Word'].str.lower()","a9eace68":"st = nltk.PorterStemmer()\ndef stemming_on_text(Processed_data):\n    text = [st.stem(word) for word in Processed_data]\n    return Processed_data\n\nProcessed_data[\"Word\"] = Processed_data[\"Word\"].apply(lambda x: stemming_on_text(x))","5b722631":"lm = nltk.WordNetLemmatizer()\ndef lemmatizer_on_text(Processed_data):\n    text = [lm.lemmatize(word) for word in Processed_data]\n    return Processed_data\n\nProcessed_data[\"Word\"] = Processed_data[\"Word\"].apply(lambda x: lemmatizer_on_text(x))","ecab9777":"aa=Processed_data[Processed_data['Tag']=='Others']\nbb=Processed_data[Processed_data['Tag']=='Disease_Syndrome']\nProcessed_data=pd.concat([aa,bb])\nProcessed_data","582eb827":"sns.countplot(data= Processed_data, x = \"Tag\")\nplt.show()","a0e510bc":"Processed_data[\"Tag\"].value_counts().head(7).plot(kind = 'pie', autopct='%1.1f%%', figsize=(8, 8)).legend()","1033003e":"Processed_data['Tag']=Processed_data['Tag'].replace('Others',1)\nProcessed_data['Tag']=Processed_data['Tag'].replace('Disease_Syndrome',2)","61fcc296":"X = Processed_data.Word\nY = Processed_data['Tag']\nle=LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","1e84e25d":"max_words = 1500\nmax_len = 300\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X)\nsequences = tok.texts_to_sequences(X)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","04d38765":"sequences_matrix","734f933a":"X_train, X_test, y_train, y_test = train_test_split(sequences_matrix, Y, test_size=0.30, random_state=2)","ea24c355":"def RNN_model():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","c9d64540":"model = RNN_model()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['acc'])","fe7d689e":"history=model.fit(X_train,y_train,batch_size=500,epochs=20,\n          validation_split=0.1)\nprint('Training finished !!')","5c1ce08e":"def Training_and_validation_plot(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\nTraining_and_validation_plot(history)","45426637":"val_acc = history.history['val_acc']\nacc = history.history['acc']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, val_acc, 'g', label = 'Validation Accuracy')\nplt.title('Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\ny_arrow = max(val_acc)\nx_arrow = val_acc.index(y_arrow) + 1\nplt.annotate(str(y_arrow)[:6],\n             (x_arrow, y_arrow),\n             xytext=(x_arrow + 5, y_arrow + .02),\n             arrowprops=dict(facecolor='green', shrink=0.05))","efb1ca2d":"y_pred1 = model.predict(X_test)\ny_pred1=(y_pred1 > 0.5)\naccr = model.evaluate(X_test,y_test)\nrnn_acc=accr[1]\nprint('Test set\\n   Accuracy: {:0.3f}'.format(accr[1]))","127065c5":"print('\\n')\nprint(\"Precision, Recall, F1\")\nprint('\\n')\nCR=classification_report(y_test, y_pred1)\nprint(CR)\nprint('\\n')","b23823a5":"print('\\n')\nprint(\"confusion matrix\")\nprint('\\n')\nCR=confusion_matrix(y_test, y_pred1)\nprint(CR)","546788c1":"fpr, tpr, thresholds = roc_curve(y_test, y_pred1)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='AUC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","1764eaf4":"RF = RandomForestClassifier()","9c1b4c58":"params = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","cd5585c1":"Grid_RN = RandomizedSearchCV(estimator = RF, param_distributions = params)\nGrid_RN.fit(X_train, y_train)","d0b7bc99":"Grid_RN.best_params_","a005c1b4":"RF_R=Grid_RN.predict(X_test)\nRF_acc=Grid_RN.score(X_test, y_test)\nprint('Accuracy score= {:.2f}'.format(Grid_RN.score(X_test, y_test)))","322898dc":"print('\\n')\nprint(\"Precision, Recall, F1\")\nprint('\\n')\nCR=classification_report(y_test, RF_R)\nprint(CR)\nprint('\\n')","626f46f3":"print('\\n')\nprint(\"confusion matrix\")\nprint('\\n')\nCR=confusion_matrix(y_test, RF_R)\nprint(CR)","becc1a63":"fpr, tpr, thresholds = roc_curve(y_test, RF_R)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nplt.plot(fpr, tpr, color='darkorange', lw=1, label='AUC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', lw=1.5)\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('AUC CURVE')\nplt.legend(loc=\"lower right\")\nplt.show()","55569453":"x = PrettyTable()\nprint('\\n')\nprint(\"Comparison of RNN and Random Forest Results\")\nx.field_names = [\"Model\", \"Accuracy\"]\n\nx.add_row([\"RNN model\",  round(rnn_acc,2)])\nx.add_row([\"Random Forest model\", round(RF_acc,3)])\n\nprint(x)\nprint('\\n')","cc744479":"Acc = [['Results of RNN & RF', rnn_acc, RF_acc]]\nAcc = pd.DataFrame (Acc, columns = ['Comparison of RNN and RF models','RNN Model', 'RF Mocdel'])\nAcc.set_index(\"Comparison of RNN and RF models\",drop=True,inplace=True)\nAcc.plot(kind='bar',figsize=(15, 10)).legend(bbox_to_anchor=(1, 1))","7f3acca4":"###  Making the sentences of plain text sentences into words in each row","0ce84144":"#### Split data into training and testing\n- 70% training data \n- 30% testing data\n","92014e49":"### Words tagged as Others","9d30cd7c":"#### Precision, Recall, F1","0fa8d01c":"We trained recurrent neural network RNN and random forest on this data. As we can see that Random Forest Model did not performed well but RNN outperformed with highets accuracy. Initially RNN was not giving good results but we tuned the parameters in order to get the good results. \n\nRNN model is not giving only high accuracy but also performing well on the other evaluation metrics which are confusion matrix, precision, recall, F1 measure and also can see the AUC curve which is showing high positive rate of classification. ","5952a9fc":"#### Coloumns\/features in Processed_data","b66b6d0a":"#### Rows and columns in the Processed_dataset","131952f1":"#### Training and validating \n\n- from 70% of the training data, we used 10% for validation","d0f238c7":"#### Checking Null values","52c8503b":"#### Length of Processed_data","27aeccdb":"# Conclusion","2df02833":"# Implementing RNN model\n- building the RNN with Hyper parameter\n- Tunning the hyper parameter accroding to the problem \n- We used 9 layers in the RNN model in which one is input layer and one is output layer. We used word embedding and LSTM layers to learn the patterns even better. We used RNN because it remembers the each and every information though the time. RNN model save the previous information and take the better decisions for classification. ","539520b8":"#### AUC curve plot","0a83ac94":"#### Getting inpout and output features","c6c954af":"#### Processed_data information","2fb969d4":"#### Accuracy","4ac24a1b":"#### Five top records of Processed_data","1c107852":"<h1 style=\"background-color:#FF85A3;font-size:20px;color:#00033E;font-weight : bold\">\u2705 Importing Required Libraries<\/h1>","a742a4af":"### Making text text in lower case","d6e10fdb":"# Comparison of RNN and Random Forest Results ","27b9eb1e":"<div class=\"alert alert-block alert-success\">  \n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h1><strong><center>RF Model<\/center><\/strong><\/h1>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <i><\/i>\n<\/div>","93b90a16":"### Cleaning special character ","955e96ad":"# Implementing Random Forest model\n- Random forest model works in the tree form. Various trees save the information of input features and decides the output class and then finally combine all the trees, calculate the weights and classify. RF model add adds additional randomness to the model, while growing the trees. Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features. This results in a wide diversity that generally results in a better model. ","42f9f791":"#### Encoding the Others as 0 and Disease_Syndrome as 1","ea7e5343":"### Best parameters","df9f50da":"#### Confusion Matrix","446e0685":"# Importing plaintext data files","12914794":"#### Count of the class","49949ec1":"#### Shape of Processed_data","256609e0":"### Taking only Others and Disease_Syndrome","07508af7":"#### Checking duplicate Processed_data","350d9ee5":"#### AUC curve plot","2e05c9c8":"# Exploratory data analysis","58bc9099":"#### Processed_data types of all coloumns","23587235":"### Taking the text in list","257643c6":"#### Precision, Recall, F1","5e0e6baf":"### Matching the ground_truth with the plain text and creating the sentence, word and tag","ab76a419":"### Applying Stemming","80053151":"# Data Preparation","bc3d6821":"# Importing ground truth annotation file","6e846337":"#### Training and validation plot","147d6edb":"# Testing and calculating Accuracy","92c6a883":"### Training the Algorithm with Random Search for Hyper parameter Optimization","908311ab":"#### Accuracy","da293e99":"#### Highest Validation accuracy point","2d54b8ae":"#### Confusion Matrix","fbc96051":"### Applying Lemmatizer\n- We used the lemmatization because it helps to achieve the roots of the words. Since we are working on the words basis so it is very important to know each word with its root not many words which give the same meaning. ","50c344cc":"### Words tagged as Disease_Syndrome      ","f2645b85":"#### Count of the class","2ba88dcb":"We applied different techniques on the data preparation and then features extraction. We used TF-IDF technique to extract the features from the text but the models was not giving good results. We changed the features extraction technique to Sequance extractor using tensorflow and we got the good results. Data preprocessing, data extraction and hyper parameter tuning helped to boost the perofrmance of RNN but Random forest did not performed well. \n\n\nWell about the hyper parameters tuning, we don\u2019t need to set different hyper parameters, train the model and show the results. \nTo do this what we need to do, we tune the hyper parameters according to problem, we train and test and know how the model giving results in deep learning. And in Random forest model, we can use GRID SEARCH approach that train the model on all the given hyper parameters and select the best parameters to train the final model.\n\nHyper parameters for RNN,  \nWe choose the different number of parameters but after tuning, we got best hyper parameters to get the good results. We used Sigmoid activation function as we need only one output. Sigmoid function helps to get one output for binary problem. We used binary cross entropy as our problem is to classify only two classes. RMSPROP optimizer helps to get the good results. We change and test different epochs and batch sizes but 20 epochs and 500 batch size is good to get the good results as well. \n\n\nHyper parameters for RF,  \nTo get the best parameters of random forest model, we used grid search which take different parameters every time and train the model. Finally grid search train the model on the best parameters. These are the best parameters for RF model:\n{'n_estimators': 200,\n 'max_features': 'sqrt',\n 'max_depth': 8,\n 'criterion': 'gini'}\n\nWe got good results on the RNN because it remember each and every information of previous times.","ef5b5ffa":"#### Features Extraction using sequance extractor \n- Tensorflow sequence extractor transforms each word in text to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. By doing this, it use the words with top frequency as features. Since we used the tokenizor, so the words for sequence extractor taken into account which are known by the tokenizor. \n- We selected the 1500 maximum words window to extract from the data using tensorflow text to sequences method. The words are on one side as single feature vector and maximum length of word is 300. ","e06e087e":"#### Five last records of Processed_data","97c8272e":"<div class=\"alert alert-block alert-info\">  \n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h1><strong><center>RNN Model<\/center><\/strong><\/h1>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <h6>...<\/h6>\n    <i><\/i>\n<\/div>","62d7e26a":"### Words tagged as 0      ","20e93d62":"### Cleaning and removing punctuations "}}