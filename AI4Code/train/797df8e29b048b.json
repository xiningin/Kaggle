{"cell_type":{"1d9ac0fd":"code","7690c57c":"code","84cf094a":"code","a3878b0c":"code","b8734192":"code","e33e1614":"code","dc92c6ea":"code","65f65e8d":"code","40d447a8":"code","2fc080ae":"code","7ea9320b":"code","faeef4bf":"code","02175f63":"code","4070604c":"code","35c79cc8":"code","2588a418":"markdown","2b78d43b":"markdown"},"source":{"1d9ac0fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7690c57c":"# importing 'orders.csv' and sorting it by 'user_id' and 'order_number'\norders=pd.read_csv('..\/input\/orders.csv')\norders=orders[['user_id','order_number', 'order_id', 'eval_set', 'order_dow',\n       'order_hour_of_day', 'days_since_prior_order']].sort_values(['user_id','order_number'])\n#first order of user_id==1 can be seen as below:\nprint(orders.head())","84cf094a":"#Examining the different SETs in the data.\nprint(orders['eval_set'].unique())\n#We can drop the 'test' SET\norders.drop(orders[orders['eval_set']=='test'].index,inplace=True)\nprint(orders['eval_set'].unique())","a3878b0c":"#We need to map all the orders based on 'order_id' and 'eval_set'. But before we need to import both SETs of 'order_products_*.csv'\norder_prior=pd.read_csv('..\/input\/order_products__prior.csv')\nprint(order_prior.head())","b8734192":"order_train=pd.read_csv('..\/input\/order_products__train.csv')\nprint(order_train.head())","e33e1614":"p_orders=orders[orders['eval_set']=='prior'].drop('eval_set',axis=1)\ndf_prior=pd.merge(p_orders,order_prior,left_on='order_id',right_on='order_id')\ndf_prior['eval_set']='prior'\ndf_prior.head()","dc92c6ea":"t_orders=orders[orders['eval_set']=='train'].drop('eval_set',axis=1)\ndf_train=pd.merge(t_orders,order_train,left_on='order_id',right_on='order_id')\ndf_train['eval_set']='test'\ndf_train.head()","65f65e8d":"df=pd.concat([df_prior,df_train],ignore_index=True)\ndf=df.sort_values(['user_id','order_number','add_to_cart_order'])\ndf.head()","40d447a8":"#deleting the dataframes that arent needed\ndel orders\ndel order_prior\ndel order_train\ndel p_orders\ndel t_orders\ndel df_prior\ndel df_train","2fc080ae":"#importing 'products.csv', 'aisles.csv' and 'department.csv'\nproducts=pd.read_csv('..\/input\/products.csv')\naisles=pd.read_csv('..\/input\/aisles.csv')\ndepartments=pd.read_csv('..\/input\/departments.csv')","7ea9320b":"#Merging products and aisles\nproducts_aisles_df= pd.merge(products,aisles,left_on='aisle_id',right_on='aisle_id').sort_values('product_id')\nprint(products_aisles_df.head())","faeef4bf":"#Merging products_aisles_df with departments to get 'products_df'\nproducts_df=pd.merge(products_aisles_df,departments,left_on='department_id',right_on='department_id')\nprint(products_df.head())","02175f63":"#deleting the dataframes not needed\ndel products\ndel aisles\ndel departments\ndel products_aisles_df","4070604c":"final_df=pd.merge(df,products_df,left_on='product_id',right_on='product_id')\nfinal_df=final_df.sort_values(['user_id','order_number','add_to_cart_order'])\nfinal_df.head(20)","35c79cc8":"final_df.to_hdf('final.hdf','final_df',mode='w',Table=True)","2588a418":"## For beginners, having trouble with huge amount of data files and just want to get the joined data, I have created this kernel to ease your pain.\n\n**You can directly download the 'hdf' file  from the OUTPUT TAB  OR include it in your kernel and read it using**  `df=pd.read_hdf('final.hdf', key='final_df', mode='r')` ","2b78d43b":"# Making one DataFrame of all the date.\n### Let's have a look at the different files we have in our data and their respective columns.\n\n**1. aisles.csv**:\nShape: 134x2\n* `aisle_id`: aisle identifier\n* `aisle`: the name of the aisle\n\n**2. department.csv**:\nShape: 21x2\n* `department_id`: department identifier\n* `department`: the name of the department\n\n**(3,4). order_products__SET.csv**:\n* `order_id`: foreign key\n* `product_id`: foreign key\n* `add_to_cart_order`: order in which each product was added to cart\n* `reordered`: 1 if this product has been ordered by this user in the past, 0 otherwise\n\nwhere `SET` is one of the four following evaluation sets (`eval_set` in `orders`):\n* `\"prior\"`: orders prior to that users most recent order (~32.4mx4 )\n* `\"train\"`: training data supplied to participants (1.38mx4)\n\n**5. orders.csv**:\n* `order_id`: order identifier\n* `user_id`: customer identifier\n* `eval_set`: which evaluation set this order belongs in (see `SET` described below)\n* `order_number`: the order sequence number for this user (1 = first, n = nth)\n* `order_dow`: the day of the week the order was placed on\n* `order_hour_of_day`: the hour of the day the order was placed on\n* `days_since_prior`: days since the last order, capped at 30 (with NAs for `order_number` = 1)\n\n(source:https:\/\/gist.github.com\/jeremystan\/c3b39d947d9b88b3ccff3147dbcf6c6b\/data_description.mds)"}}