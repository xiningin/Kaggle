{"cell_type":{"7eface1a":"code","d2989993":"code","92b78b5b":"code","38ab3399":"code","6107ab87":"code","9cbbe5cf":"code","91fed173":"code","2d17476e":"code","d38714f5":"code","1d2f56e6":"code","56bb9cd8":"markdown","43c4d1f5":"markdown","09e3bc61":"markdown","6447662e":"markdown","9a3e7129":"markdown","c7e7de5b":"markdown","5e05b2ed":"markdown","0306472b":"markdown","1a05e1aa":"markdown","2336ea20":"markdown"},"source":{"7eface1a":"!pip install keras-tuner","d2989993":"import cv2\nimport time\nimport keras\nimport pickle\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom kerastuner.tuners import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameter\n\ndf = pd.read_csv(\"..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")","92b78b5b":"X = df.drop('target_class',axis=1)\ny = df['target_class']\n\n#normalize full data\nX = preprocessing.scale(X)\n\n#split intro training and testing\nx_train,x_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=50)\n\n#Convert all data samples into numpy arrays\nx_train = np.array(x_train) \nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)   \n","38ab3399":"#Create Simple NN\ndef tune_model(hp):\n    model = tf.keras.models.Sequential()\n    \n    model.add(tf.keras.layers.Dense(hp.Int('dense_units',\n                                             min_value=32,\n                                             max_value=256,\n                                             step=32),\n                                             activation='relu'))\n    \n        \n    for i in range(hp.Int('n_layers', 1,4)):\n        model.add(tf.keras.layers.Dense(hp.Int(f'dense_{i}_units',\n                                             min_value=32,\n                                             max_value=256,\n                                             step=32),\n                                             activation='relu'))\n        \n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model\n\ntuner = RandomSearch(\n    tune_model,\n    objective='val_accuracy',\n    max_trials=10,\n    executions_per_trial=2)\n\ntuner.search(\n    x=x_train,\n    y=y_train,\n    epochs=3,\n    batch_size=32,\n    validation_data=(x_test,y_test)\n    )\n    \nprint(tuner.get_best_hyperparameters()[0].values)\nprint(tuner.get_best_models()[0].summary)","6107ab87":"def create_model():\n    model = keras.models.Sequential([\n            keras.layers.Dense(160, activation='relu'),\n            keras.layers.Dense(224, activation='relu'),\n            keras.layers.Dense(256, activation='relu'),\n            keras.layers.Dense(96, activation='relu'),\n            keras.layers.Dense(1,activation='sigmoid')\n            ])\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model\n\nmodel = create_model()\n\nhistory = model.fit(x_train,y_train,batch_size=32,epochs=30,verbose=True)\nscores = model.evaluate(x_test,y_test,verbose=1)\nprint(scores[1])\n# 0.9793","9cbbe5cf":"from sklearn.ensemble import RandomForestClassifier #import the model library\nrf = RandomForestClassifier()\nparameters = {'n_estimators':[10,50,100,200],'max_depth':[2,4,6,8]}\n\nclf = GridSearchCV(rf, parameters)\nclf.fit(x_train,y_train)\nsorted(clf.cv_results_.keys())\nprint(clf.best_params_)\n\nrf = RandomForestClassifier(n_estimators=200, max_depth=8,random_state=0) # sitting model parameters\nprint(\"test accuracy: {} \".format(rf.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(rf.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set\n# 0.9807","91fed173":"from sklearn import linear_model #import the model library\nlogreg =linear_model.LogisticRegression()\nparameters = {'solver':('lbfgs','sag','saga'),'max_iter':[100,250,500,750]}\n\nclf = GridSearchCV(logreg,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)\n\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 250,solver='saga') # sitting model parameters\nprint(\"test accuracy: {} \".format(logreg.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(logreg.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set\n#0.9801","2d17476e":"from sklearn import tree #import the model library\ndt = tree.DecisionTreeClassifier() # sitting model\nparameters = {'max_depth':[2,4,6,8]}\n\nclf = GridSearchCV(dt,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)\n\ndt = tree.DecisionTreeClassifier(max_depth=4)\nprint(\"test accuracy: {} \".format(dt.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(dt.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set\n\n#0.9779","d38714f5":"from sklearn.neighbors import KNeighborsClassifier #import the model library\nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':[1,3,5,7]}\n\nclf = GridSearchCV(knn,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)\n\nneigh = KNeighborsClassifier(n_neighbors=7) # sitting model parameters\nprint(\"test accuracy: {} \".format(neigh.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(neigh.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set\n\n#0.9793","1d2f56e6":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = XGBClassifier()\nparameters = {'max_depth':[2,4,6,8,10],'learning_rate':[0.1,0.001,0.0001],'n_estimators':[50,100,150,200,250,300,350,400]}\n\nclf = GridSearchCV(model,parameters)\nclf.fit(x_train,y_train)\nprint(clf.best_params_)\n\nmodel = XGBClassifier(learning_rate=0.1,max_depth=4,n_estimators=250)\nprint(\"test accuracy: {} \".format(model.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(model.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set\n\n#0.9798\n","56bb9cd8":"<h3>Random Forest Classifier<\/h3> ","43c4d1f5":"<h3>Decision Tree Classifier<\/h3> ","09e3bc61":"<h2>Predicting Pulsar Star with Different Models with Tuners<\/h2>\n\n<h4>Essentially, a binary-classification problem<\/h4>\n\n<p>\n    This kernel uses the simple Pulsar Star dataset to analyze and predict correct classification using 7 different models. All models yielded over 97% accuracy, with the highest accuracy attained was 98.07% - using Random Forest Search.     \n<\/p>\n\n<h3>The models used:<\/h3>\n    <li>Keras simple dense neural network<\/li>\n    <li>Random Forest Search<\/li>\n    <li>Logistic Regression<\/li>\n    <li>Decision Tree Classifier<\/li>\n    <li>KNN<\/li>\n    <li>SVM<\/li>\n    <li>XGBoost<\/li>   \n\n<h3>Tuners Used:<\/h3>\n    <li>Keras Tuner V1.0.0<\/li>\n    <li>SciKit Learn GridSearchCV<\/li>\n    \n    \nAlso used AMD PlaidML Back-end for GPU Processing","6447662e":"<h3>KNN<\/h3>","9a3e7129":"<h3><\/h3>","c7e7de5b":"Now, let's explore all the other models using Grid Search to optimize some parameters for highest accuracy\n","5e05b2ed":"<h4>\n    First let's start with the simple neural network.\n<\/h4>\n\n<p>\n    Since this is a binary classification problem, we are going to keep the model simple. Just using Dense layers, let's use Keras Tuner to find the hyper-parameters for our model\n<\/p>","0306472b":"<h3>Logistic Regression<\/h3>","1a05e1aa":"Now with the output parameters let's create the actual model","2336ea20":"<h3>XGBoost<\/h3>"}}