{"cell_type":{"96ac85c9":"code","ff97831a":"code","915ffe19":"code","46633edf":"code","d12d5ff5":"code","77e8921a":"code","33c80224":"code","0514617c":"code","34b108b1":"code","41e4b9cb":"code","ec799e62":"code","750e41cf":"code","e8618736":"code","fe248351":"code","d818283c":"code","45e8b7de":"code","0d564054":"code","882dc61a":"code","b6237716":"code","0e086030":"code","23502d33":"code","b6c1e186":"markdown","10cfdcfd":"markdown","aa341426":"markdown","ae222a58":"markdown","c19165a3":"markdown","a5e018da":"markdown","438a4717":"markdown","1d88e27b":"markdown","d044784c":"markdown","9229acb5":"markdown","65588e56":"markdown","d0398d63":"markdown"},"source":{"96ac85c9":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ff97831a":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n    '''\n    The function does not return, but transforms the input pd.DataFrame\n    \n    Encodes the Costa Rican Household Poverty Level data \n    following studies in https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-in-the-data\n    and the insight from https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403#359631\n    \n    The following columns get transformed: edjefe, edjefa, dependency, idhogar\n    The user most likely will simply drop idhogar completely (after calculating houshold-level aggregates)\n    '''\n    \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n\n    \ndef do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean', 'count'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean', 'sum']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n    \ndef convert_OHE2LE(df):\n    tmp_df = df.copy(deep=True)\n    for s_ in ['pared', 'piso', 'techo', 'abastagua', 'sanitario', 'energcocinar', 'elimbasu', \n               'epared', 'etecho', 'eviv', 'estadocivil', 'parentesco', \n               'instlevel', 'lugar', 'tipovivi',\n               'manual_elec']:\n        if 'manual_' not in s_:\n            cols_s_ = [f_ for f_ in df.columns if f_.startswith(s_)]\n        elif 'elec' in s_:\n            cols_s_ = ['public', 'planpri', 'noelec', 'coopele']\n        sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n        #deal with those OHE, where there is a sum over columns == 0\n        if 0 in sum_ohe:\n            print('The OHE in {} is incomplete. A new column will be added before label encoding'\n                  .format(s_))\n            # dummy colmn name to be added\n            col_dummy = s_+'_dummy'\n            # add the column to the dataframe\n            tmp_df[col_dummy] = (tmp_df[cols_s_].sum(axis=1) == 0).astype(np.int8)\n            # add the name to the list of columns to be label-encoded\n            cols_s_.append(col_dummy)\n            # proof-check, that now the category is complete\n            sum_ohe = tmp_df[cols_s_].sum(axis=1).unique()\n            if 0 in sum_ohe:\n                 print(\"The category completion did not work\")\n        tmp_cat = tmp_df[cols_s_].idxmax(axis=1)\n        tmp_df[s_ + '_LE'] = LabelEncoder().fit_transform(tmp_cat).astype(np.int16)\n        if 'parentesco1' in cols_s_:\n            cols_s_.remove('parentesco1')\n        tmp_df.drop(cols_s_, axis=1, inplace=True)\n    return tmp_df","915ffe19":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","46633edf":"def process_df(df_):\n    # fix categorical features\n    encode_data(df_)\n    #fill in missing values based on https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n    for f_ in ['v2a1', 'v18q1', 'meaneduc', 'SQBmeaned']:\n        df_[f_] = df_[f_].fillna(0)\n    df_['rez_esc'] = df_['rez_esc'].fillna(-1)\n    # do feature engineering and drop useless columns\n    return do_features(df_)\n\ntrain = process_df(train)\ntest = process_df(test)","d12d5ff5":"def train_test_apply_func(train_, test_, func_):\n    test_['Target'] = 0\n    xx = pd.concat([train_, test_])\n\n    xx_func = func_(xx)\n    train_ = xx_func.iloc[:train_.shape[0], :]\n    test_  = xx_func.iloc[train_.shape[0]:, :].drop('Target', axis=1)\n\n    del xx, xx_func\n    return train_, test_","77e8921a":"train, test = train_test_apply_func(train, test, convert_OHE2LE)","33c80224":"train.info(max_cols=20)","0514617c":"cols_2_ohe = ['eviv_LE', 'etecho_LE', 'epared_LE', 'elimbasu_LE', \n              'energcocinar_LE', 'sanitario_LE', 'manual_elec_LE',\n              'pared_LE']\ncols_nums = ['age', 'meaneduc', 'dependency', \n             'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total',\n             'bedrooms', 'overcrowding']\n\ndef convert_geo2aggs(df_):\n    tmp_df = pd.concat([df_[(['lugar_LE', 'idhogar']+cols_nums)],\n                        pd.get_dummies(df_[cols_2_ohe], \n                                       columns=cols_2_ohe)],axis=1)\n    #print(pd.get_dummies(train[cols_2_ohe], \n    #                                   columns=cols_2_ohe).head())\n    #print(tmp_df.head())\n    #print(tmp_df.groupby(['lugar_LE','idhogar']).mean().head())\n    geo_agg = tmp_df.groupby(['lugar_LE','idhogar']).mean().groupby('lugar_LE').mean().astype(np.float32)\n    geo_agg.columns = pd.Index(['geo_' + e + '_MEAN' for e in geo_agg.columns.tolist()])\n    \n    #print(gb.T)\n    del tmp_df\n    return df_.join(geo_agg, how='left', on='lugar_LE')\n\ntrain, test = train_test_apply_func(train, test, convert_geo2aggs)","34b108b1":"X = train.query('parentesco1==1')\n\n# pull out the target variable\ny = X['Target'] - 1 # this is done to bing input labels [1,2,3,4] in agreement with lightgbm [0,1,2,3]\nX = X.drop(['Target'], axis=1)","41e4b9cb":"#cols_2_drop = ['agg18_estadocivil1_MEAN', 'agg18_estadocivil3_COUNT', 'agg18_estadocivil4_COUNT', 'agg18_estadocivil5_COUNT', 'agg18_estadocivil6_COUNT', 'agg18_estadocivil7_COUNT', 'agg18_instlevel1_COUNT', 'agg18_instlevel2_COUNT', 'agg18_instlevel3_COUNT', 'agg18_instlevel4_COUNT', 'agg18_instlevel5_COUNT', 'agg18_instlevel6_COUNT', 'agg18_instlevel7_COUNT', 'agg18_instlevel8_COUNT', 'agg18_instlevel9_COUNT', 'agg18_parentesco10_COUNT', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_COUNT', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_COUNT', 'agg18_parentesco12_MEAN', 'agg18_parentesco1_COUNT', 'agg18_parentesco2_COUNT', 'agg18_parentesco3_COUNT', 'agg18_parentesco4_COUNT', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_COUNT', 'agg18_parentesco6_COUNT', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_COUNT', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_COUNT', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_COUNT', 'fe_people_weird_stat', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'v14a', 'v18q']\ncols_2_drop = ['abastagua_LE', 'agg18_estadocivil1_MEAN', 'agg18_instlevel6_MEAN', 'agg18_parentesco10_MEAN', 'agg18_parentesco11_MEAN', 'agg18_parentesco12_MEAN', 'agg18_parentesco4_MEAN', 'agg18_parentesco5_MEAN', 'agg18_parentesco6_MEAN', 'agg18_parentesco7_MEAN', 'agg18_parentesco8_MEAN', 'agg18_parentesco9_MEAN', 'fe_people_not_living', 'fe_people_weird_stat', 'geo_elimbasu_LE_3_MEAN', 'geo_elimbasu_LE_4_MEAN', 'geo_energcocinar_LE_0_MEAN', 'geo_energcocinar_LE_1_MEAN', 'geo_energcocinar_LE_2_MEAN', 'geo_epared_LE_0_MEAN', 'geo_epared_LE_2_MEAN', 'geo_etecho_LE_2_MEAN', 'geo_eviv_LE_0_MEAN', 'geo_hogar_mayor_MEAN', 'geo_hogar_nin_MEAN', 'geo_manual_elec_LE_1_MEAN', 'geo_manual_elec_LE_2_MEAN', 'geo_manual_elec_LE_3_MEAN', 'geo_pared_LE_0_MEAN', 'geo_pared_LE_1_MEAN', 'geo_pared_LE_3_MEAN', 'geo_pared_LE_4_MEAN', 'geo_pared_LE_5_MEAN', 'geo_pared_LE_6_MEAN', 'geo_pared_LE_7_MEAN', 'hacapo', 'hacdor', 'mobilephone', 'parentesco1', 'parentesco_LE', 'rez_esc', 'techo_LE', 'v14a', 'v18q']\n\nX.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)\ntest.drop((cols_2_drop+['idhogar']), axis=1, inplace=True)","ec799e62":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=314, stratify=y)","750e41cf":"X_test.info(max_cols=20)","e8618736":"from sklearn.metrics import f1_score\ndef evaluate_macroF1_lgb(truth, predictions):  \n    # this follows the discussion in https:\/\/github.com\/Microsoft\/LightGBM\/issues\/1483\n    pred_labels = predictions.reshape(len(np.unique(truth)),-1).argmax(axis=0)\n    f1 = f1_score(truth, pred_labels, average='macro')\n    return ('macroF1', f1, True) \n\nimport lightgbm as lgb\nfit_params={\"early_stopping_rounds\":300, \n            \"eval_metric\" : evaluate_macroF1_lgb, \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': False,\n            'categorical_feature': 'auto'}\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return max(lr, min_learning_rate)\n\nfit_params['callbacks'] = [lgb.reset_parameter(learning_rate=learning_rate_power_0997)]\n","fe248351":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(12, 20), \n             'min_child_samples': sp_randint(40, 100), \n             #'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.75, scale=0.25), \n             'colsample_bytree': sp_uniform(loc=0.8, scale=0.15)#,\n             #'reg_alpha': [0, 1e-3, 1e-1, 1, 10, 50, 100],\n             #'reg_lambda': [0, 1e-3, 1e-1, 1, 10, 50, 100]\n            }","d818283c":"#This parameter defines the number of HP points to be tested\nn_HP_points_to_test = 100\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nclf = lgb.LGBMClassifier(max_depth=-1, learning_rate=0.05, objective='multiclass',\n                         random_state=314, silent=True, metric='None', \n                         n_jobs=4, n_estimators=5000, class_weight='balanced')\n\ngs = RandomizedSearchCV(\n    estimator=clf, param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='f1_macro',\n    cv=5,\n    refit=True,\n    random_state=314,\n    verbose=True)","45e8b7de":"_ = gs.fit(X_train, y_train, **fit_params)","0d564054":"print(\"PERFORMANCE IMPROVES FROM TOP TO BOTTOM\")\nprint(\"Valid+-Std     Train  :   Parameters\")\nfor i in np.argsort(gs.cv_results_['mean_test_score'])[-5:]:\n    print('{1:.3f}+-{3:.3f}     {2:.3f}   :  {0}'.format(gs.cv_results_['params'][i], \n                                    gs.cv_results_['mean_test_score'][i], \n                                    gs.cv_results_['mean_train_score'][i],\n                                    gs.cv_results_['std_test_score'][i]))\n\nopt_parameters = gs.best_params_","882dc61a":"clf_final = lgb.LGBMClassifier(**clf.get_params())\nclf_final.set_params(**opt_parameters)\n\ndef learning_rate_power_0997(current_iter):\n    base_learning_rate = 0.1\n    min_learning_rate = 0.02\n    lr = base_learning_rate  * np.power(.997, current_iter)\n    return max(lr, min_learning_rate)\n\n#Train the final model with learning rate decay\nfit_params['verbose'] = 200\n_ = clf_final.fit(X_train, y_train, **fit_params)#, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_power_0997)])","b6237716":"y_subm = pd.read_csv('..\/input\/sample_submission.csv')","0e086030":"y_subm['Target'] = clf_final.predict(test) + 1","23502d33":"from datetime import datetime\nnow = datetime.now()\nglobal_score = f1_score(y_test, clf_final.predict(X_test), average='macro')\n\nsub_file = 'submission_LGB_{:.4f}_{}.csv'.format(global_score, str(now.strftime('%Y-%m-%d-%H-%M')))\n\ny_subm.to_csv(sub_file, index=False)","b6c1e186":"## Use test subset for early stopping criterion\n\nThis allows us to avoid overtraining and we do not need to optimise the number of trees\n","10cfdcfd":"# Read in the data and clean it up","aa341426":"# Complete optimisation of a LightGBM model using random search\nFeatures that are illustrated in this kernel:\n- a bit of data cleaning following https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-in-the-data and https:\/\/www.kaggle.com\/mlisovyi\/missing-values-in-the-data\n- **gradient-boosted decision trees** using _**LightGBM**_ package\n- **early stopping** in _**LightGBM**_ model training using **F1 macro score** to avoid overfotting\n- **learning rate decay** in _**LightGBM**_ model training to improve convergence to the minimum\n- **hyperparameter optimisation** of the model using random search in cross validation with F1 macro score\n- submission preparation\nThis kernel inherited ideas and SW solutions from other public kernels and in such cases I will post direct references to the original product, that that you can get some additional insights from the source.","ae222a58":"# Geo aggregates","c19165a3":"# VERY IMPORTANT\n> Note that **ONLY the heads of household are used in scoring**. All household members are included in test + the sample submission, but only heads of households are scored.","a5e018da":"The actual search for the optimal parameters","438a4717":"# Set up HyperParameter search\n\nWe use random search, which is more flexible and more efficient than a grid search\nDefine the distribution of parameters to be sampled from\n","1d88e27b":"# Fit the final model with learning rate decay","d044784c":"# Model fitting with HyperParameter optimisation\n\nWe will use LightGBM classifier - LightGBM allows to build very sophysticated models with a very short training time.","9229acb5":"Let's print the 'top 5 parameter configurations","65588e56":"The following categorical mapping originates from [this kernel](https:\/\/www.kaggle.com\/mlisovyi\/categorical-variables-encoding-function)","d0398d63":"# Prepare submission"}}