{"cell_type":{"c326b9bd":"code","c83622f9":"code","457a59e3":"code","27e2229e":"code","a9cc9091":"code","a8f4778c":"code","0b98a231":"code","38a5fe07":"code","eaf6203a":"code","2521a4ac":"code","649db71c":"code","03631436":"code","e536fd05":"code","1ef27787":"code","b531e977":"code","29cc8045":"code","01af627a":"code","82097846":"code","526b3e90":"code","943e9cce":"code","14dc94d2":"code","a19c6034":"code","16e40c77":"code","ba40239b":"code","abfdf0a6":"code","70c614fe":"code","4b21061b":"code","9ad126e2":"code","e4f7b6ce":"code","41f2333c":"code","bd916272":"code","f7ba80f8":"code","408b46da":"code","f1f00094":"code","92749b10":"code","47e8ea80":"code","6e174aee":"code","cc77a354":"code","b33f5e75":"code","7e828be3":"code","dd6a2341":"code","7998289d":"code","236c7bdf":"code","603f5c7f":"code","d3602566":"code","14ce22d8":"code","1dd19980":"code","244bca52":"code","6f0819a6":"code","136eb868":"code","47e5fc69":"markdown","57f1f1ac":"markdown","9f44b016":"markdown","f19127d7":"markdown"},"source":{"c326b9bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c83622f9":"import seaborn as sns \n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew","457a59e3":"#importing test and train data\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ntrain_ID = df_train['Id']\ntest_ID = df_test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ndf_train.drop(\"Id\", axis = 1, inplace = True)\ndf_test.drop(\"Id\", axis = 1, inplace = True)","27e2229e":"df_test.head()","a9cc9091":"df_train.head()","a8f4778c":"print(df_train.shape)\nprint(df_test.shape)","0b98a231":"train_cols = df_train.columns\ntest_cols = df_test.columns\n\ntrain_not_test = train_cols.difference(test_cols)\ntrain_not_test\n","38a5fe07":"full_data.info()","eaf6203a":"df_train[['GrLivArea','SalePrice', 'YearBuilt', 'PoolArea','GarageArea','GarageCars','KitchenAbvGr','BedroomAbvGr', 'TotRmsAbvGrd']].describe()","2521a4ac":"sns.scatterplot(x=df_train['SalePrice'], y=df_train['GrLivArea'])","649db71c":"df_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<500000)].index)","03631436":"sns.scatterplot(x=df_train['SalePrice'], y=df_train['GrLivArea'])","e536fd05":"sns.distplot(df_train['SalePrice'] , fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","1ef27787":"fig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","b531e977":"df_train['SalePrice'] = np.log(df_train['SalePrice']) #log\n\nsns.distplot(df_train['SalePrice'] , fit=norm)\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n","29cc8045":"fig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","01af627a":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\nfull_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nfull_data.drop(['SalePrice'], axis=1, inplace=True)\nfull_data.shape","82097846":"figure = plt.figure(figsize=(12,6))\nsns.heatmap(full_data.isnull(),yticklabels='')","526b3e90":"missingdata = full_data.isnull().sum()\n\nmissingdata = missingdata.drop(missingdata[missingdata == 0].index).sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'missing_count':missingdata})\n\nmissing_data","943e9cce":"print(full_data['GarageYrBlt'].mean())\nprint(full_data['GarageYrBlt'].median())\nprint(full_data['Electrical'].mode())\nprint(full_data['MSZoning'].mode())","14dc94d2":"full_data['Utilities'].value_counts().plot(kind='bar',figsize=[10,3])\nfull_data['Utilities'].value_counts() ","a19c6034":"full_data[\"PoolQC\"] = full_data[\"PoolQC\"].fillna(\"None\")\nfull_data[\"MiscFeature\"] = full_data[\"MiscFeature\"].fillna(\"None\")\nfull_data[\"Alley\"] = full_data[\"Alley\"].fillna(\"None\")\nfull_data[\"Fence\"] = full_data[\"Fence\"].fillna(\"None\")\nfull_data[\"FireplaceQu\"] = full_data[\"FireplaceQu\"].fillna(\"None\")\nfull_data['GarageYrBlt'] = full_data['GarageYrBlt'].fillna(full_data['GarageYrBlt'].mean())\nfull_data['Electrical'] = full_data['Electrical'].fillna(full_data['Electrical'].mode()[0]) # most common used SBrkr \nfull_data['MSZoning'] = full_data['MSZoning'].fillna(full_data['MSZoning'].mode()[0]) #most common used RL\nfull_data[\"MasVnrType\"] = full_data[\"MasVnrType\"].fillna(\"None\")\nfull_data[\"RoofMatl\"] = full_data[\"RoofMatl\"].fillna(\"None\")\nfull_data[\"Exterior1st\"] = full_data[\"Exterior1st\"].fillna(full_data['Exterior1st'].mode()[0])\nfull_data[\"Exterior2nd\"] = full_data[\"Exterior2nd\"].fillna(full_data['Exterior2nd'].mode()[0])\nfull_data[\"Fireplaces\"] = full_data[\"Fireplaces\"].fillna(0)\nfull_data[\"MasVnrArea\"] = full_data[\"MasVnrArea\"].fillna(0)\nfull_data[\"TotRmsAbvGrd\"] = full_data[\"TotRmsAbvGrd\"].fillna(\"None\")\nfull_data[\"Functional\"] = full_data[\"Functional\"].fillna(\"Typ\")\nfull_data[\"SaleType\"] = full_data[\"SaleType\"].fillna(full_data['SaleType'].mode()[0])\nfull_data[\"GarageArea\"] = full_data[\"GarageArea\"].fillna(0)\nfull_data[\"GarageCars\"] = full_data[\"GarageCars\"].fillna(0)\nfull_data['KitchenQual'] = full_data['KitchenQual'].fillna(full_data['KitchenQual'].mode()[0])\nfull_data[\"LotFrontage\"] = full_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfull_data = full_data.drop(['Utilities'], axis=1) #because there are two types of utilities and no need such a column\n","16e40c77":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n            full_data[col] =full_data[col].fillna('None')\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n            full_data[col] = full_data[col].fillna('None')\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n            full_data[col] = full_data[col].fillna(0)","ba40239b":"missingdata = full_data.isnull().sum()\n\nmissingdata = missingdata.drop(missingdata[missingdata == 0].index).sort_values(ascending=False)\n\nmissing_data = pd.DataFrame({'missing_count':missingdata})\n\nmissing_data","abfdf0a6":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n","70c614fe":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(full_data[c].values)) \n    full_data[c] = lbl.transform(list(full_data[c].values))\n\n# shape        \nprint('Shape full_data: {}'.format(full_data.shape))","4b21061b":"full_data = pd.get_dummies(full_data)\nprint(full_data.shape)","9ad126e2":"train = full_data[:ntrain]\ntest = full_data[ntrain:]","e4f7b6ce":"test.head()","41f2333c":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","bd916272":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","f7ba80f8":"score = rmsle_cv(lasso)\nprint(\"Lasso RMSE:\", (score.mean()))","408b46da":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","f1f00094":"score = rmsle_cv(GBoost)\nprint(\"GBoost score:\", (score.mean()))","92749b10":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","47e8ea80":"score = rmsle_cv(model_lgb)\nprint(\"LGB score:\", (score.mean()))","6e174aee":"y_lr = df_train.SalePrice\nX_lr = train","cc77a354":"X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(\n                          X_lr, y_lr, random_state=42, test_size=.33)","b33f5e75":"from sklearn import linear_model\nlr = linear_model.LinearRegression()","7e828be3":"model = lr.fit(X_lr_train, y_lr_train)","dd6a2341":"print (\"R^2 is: \\n\", model.score(X_lr_test, y_lr_test))","7998289d":"predictions = model.predict(X_lr_test)","236c7bdf":"print ('RMSE is: \\n', mean_squared_error(y_lr_test, predictions))","603f5c7f":"feats = test.select_dtypes(\n        include=[np.number]).interpolate()\n\npredictions = model.predict(feats)\n\nfinal_predictions = np.exp(predictions)","d3602566":"print ('RMSE is: \\n', mean_squared_error(y_lr_test,final_predictions))","14ce22d8":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","1dd19980":"lasso.fit(train, y_train)\nlasso_train_pred = lasso.predict(train)\nlasso_pred = np.expm1(lasso.predict(test.values))\nprint(rmsle(y_train, lasso_train_pred))","244bca52":"GBoost.fit(train, y_train)\nGBoost_train_pred = GBoost.predict(train)\nGBoost_pred = np.expm1(GBoost.predict(test.values))\nprint(rmsle(y_train, GBoost_train_pred))","6f0819a6":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","136eb868":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = GBoost_pred\nsub.to_csv('submission.csv',index=False)","47e5fc69":"* # GBoost ","57f1f1ac":"### Linear Regression","9f44b016":"* # Lasso","f19127d7":"### Full Data"}}