{"cell_type":{"a0e466d8":"code","c64a2f2a":"code","a800d8d0":"code","bf219514":"code","738021aa":"code","9a605fe0":"code","d670922a":"code","d9b82f82":"code","20ed4375":"code","79292ff0":"code","1af5ae67":"code","e74cd7f5":"code","980f601f":"code","577d11c6":"code","07b5f223":"code","d8b9abbc":"code","f5e04620":"code","4c7b1f79":"code","2275219a":"code","bdd00f0b":"code","b1304d35":"code","70f88b01":"code","9df3649f":"code","e2b64841":"code","7acfe9d1":"code","f1a0162f":"code","6a4f5cfa":"code","e3d0ed28":"code","2ba6085b":"code","e34c9e76":"code","62bd4c3d":"code","c592fcaa":"code","8560aada":"code","13befe1b":"code","1bb8f283":"code","edb1b5bf":"code","b6cc1344":"code","2d2c0306":"code","6a9cc4c6":"code","a1f67b65":"code","d23d9cab":"code","d180870d":"code","e4abe87a":"code","3e27bab8":"code","0f031da0":"code","d26929ac":"code","f8620f1b":"code","00521680":"code","b3f220e8":"code","9b792a4f":"code","83700f06":"code","5e49f71f":"code","f943b062":"code","530f7d76":"code","1c052142":"code","2602db03":"code","452b44ec":"code","3f1bf1b5":"code","73e35b19":"code","ae57918a":"code","37f2da40":"code","15ba4247":"code","fbab283b":"code","476a6327":"code","db06d98b":"code","fd145523":"code","8949b04b":"code","40164de4":"code","23628fcc":"code","94891d30":"code","7a2a7b2b":"code","a8b0854e":"code","9801b85a":"code","5eaf51f1":"code","d47f3798":"code","26525d47":"markdown","f9d8ee41":"markdown","b31dd6f5":"markdown","115ce659":"markdown","2bcc6380":"markdown","bdbecabd":"markdown","3f7bbffe":"markdown","81ef0528":"markdown","05e93ec4":"markdown","301a8aaf":"markdown","d0eba199":"markdown","5d1e485a":"markdown","5f533e56":"markdown","37d39a43":"markdown","4fc34082":"markdown","15ef45e5":"markdown","03b2b0e7":"markdown","6c3a5434":"markdown","589acc59":"markdown","474ef6f8":"markdown","d8c2e898":"markdown","c32570cd":"markdown","b4493e3b":"markdown","07fa5e11":"markdown","8ffbcd77":"markdown","be6c3f9a":"markdown","754a628a":"markdown","824cceea":"markdown","b18308e5":"markdown","576d59ff":"markdown","967765ee":"markdown","6b280d1a":"markdown","2cb62a1b":"markdown","0d79c204":"markdown","50657628":"markdown","fc54f324":"markdown","928a2d59":"markdown","2bf713ff":"markdown","063b42f3":"markdown","e38c16b5":"markdown"},"source":{"a0e466d8":"import numpy as np\nfrom numpy import random\nrandom.seed(12345)","c64a2f2a":"from sklearn import metrics\nfrom sklearn.metrics import r2_score\nimport sklearn","a800d8d0":"from sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.linear_model import Lasso","bf219514":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%config InlineBackend.figure_format = 'png'\nsns.set()\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly\nimport plotly.graph_objs as go\nfrom scipy.stats import chi2_contingency\nfrom sklearn.linear_model import Ridge\n\ninit_notebook_mode(connected=True)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","738021aa":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler","9a605fe0":"%config InlineBackend.figure_format = 'retina'","d670922a":"data=pd.read_csv(\"\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv\")\ndata.head()","d9b82f82":"data.info()","20ed4375":"data.describe()","79292ff0":"data.columns.unique()","1af5ae67":"columns1={'gender':'gender','race\/ethnicity':'race','parental level of education':'parent_ed_level','lunch':'lunch_type','test preparation course':'test_prep','math score':'math','reading score':'reading','writing score':'writing'}\ndata.rename(columns=columns1,inplace=True)","e74cd7f5":"data.head()","980f601f":"data['total']=data['math']+data['reading']+data['writing']","577d11c6":"data['parent_ed_level'].unique()","07b5f223":"data['parent_ed_level']=data['parent_ed_level'].replace(['some high school'],'high school')\ndata['parent_ed_level'].unique()","d8b9abbc":"data['lunch_type']=data['lunch_type'].replace(['free\/reduced'],'reduced')","f5e04620":"data.head()","4c7b1f79":"data.info()","2275219a":"data[['math','reading','writing','total']].hist(figsize=(12,9))","bdd00f0b":"#function to create a boxplot\ndef boxplot(column,dataf):\n    plt.figure(figsize=(10,4))\n    sns.boxplot(x=column,data=dataf)","b1304d35":"nums=['math','reading','writing','total']\nfor i in nums:\n    boxplot(i,data)","70f88b01":"cat_list=['gender','lunch_type','test_prep']\nfor col in cat_list:\n    plt.figure(figsize=(8,5))\n    sns.countplot(x=col,data=data)","9df3649f":"plt.figure(figsize=(8,5))\nplt.xticks(rotation=45)\nsns.countplot(x='parent_ed_level',data=data,order=['high school','some college',\"associate's degree\",\"bachelor's degree\",\"master's degree\"])","e2b64841":"plt.figure(figsize=(8,5))\nplt.xticks(rotation=45)\nsns.countplot(x='race',data=data,order=['group C','group D',\"group B\",\"group E\",\"group A\"])","7acfe9d1":"def hue(a,b):\n    plt.figure(figsize=(8,5))\n    if a=='parent_ed_level':\n        plt.xticks(rotation=45)\n    sns.countplot(x=a,data=data,hue=b)","f1a0162f":"hue('parent_ed_level','gender')","6a4f5cfa":"hue('parent_ed_level','race')","e3d0ed28":"corr_matrix=data[nums].corr()\nplt.figure(figsize=(8,5))\nsns.heatmap(corr_matrix,annot=True)","2ba6085b":"data.drop(['total'],axis=1,inplace=True)","e34c9e76":"new_data=data.copy()","62bd4c3d":"new_data.drop(['writing'],inplace=True,axis=1)","c592fcaa":"new_data.head()","8560aada":"hue_list=['gender','lunch_type','test_prep']\nfor hue in hue_list:\n    plt.figure(figsize=(12,5))\n    sns.lmplot(x='reading',y='math',data=new_data,hue=hue,fit_reg=True,markers=['o','x'])","13befe1b":"#creating a function to execute chisq test for independence\ndef chisq(col1,col2):\n    #create a contingency table\n    table=pd.crosstab(new_data[col1],new_data[col2])\n    #get chi_Sq statistics,p-value,degrees of freedom and expected frequencies.\n    stat, p, dof, expected = chi2_contingency(table)\n    #set significance level\n    alpha=0.05\n    if p<=0.05:\n        print('Features are associated')\n    else:\n        print('Features are not associated')","1bb8f283":"chisq('gender','lunch_type')\nchisq('gender','parent_ed_level')\nchisq('gender','race')\nchisq('gender','test_prep')\nchisq('lunch_type','test_prep')\nchisq('lunch_type','parent_ed_level')\nchisq('lunch_type','race')\nchisq('parent_ed_level','race')\nchisq('parent_ed_level','test_prep')\nchisq('race','test_prep')","edb1b5bf":"new_data.drop(['test_prep'],axis=1,inplace=True)","b6cc1344":"new_data.head()","2d2c0306":"label=LabelEncoder()\ncat_list=['gender','race','lunch_type','parent_ed_level']\nfor col in cat_list:\n    new_data[col]=label.fit_transform(new_data[col])","6a9cc4c6":"new_data['race'].value_counts()","a1f67b65":"y=new_data['math']\nnew_data.drop(['math'],axis=1,inplace=True)","d23d9cab":"new_data['reading']=new_data['reading']\/100.0","d180870d":"y=y\/100.0","e4abe87a":"corr_matrix=new_data.corr()\nsns.heatmap(corr_matrix,annot=True)","3e27bab8":"#creating a function that prints all relevant regression metrics when called\ndef reg_metrics(actual,predicted):\n    mae=metrics.mean_absolute_error(actual,predicted)\n    mse=metrics.mean_squared_error(actual,predicted)\n    rmse=np.sqrt(metrics.mean_squared_error(actual,predicted))\n    r2=r2_score(actual,predicted)\n    print(\"MAE:\",mae)\n    print(\"MSE:\",mse)\n    print(\"RMSE:\",rmse)\n    print(\"R2:\",r2)","0f031da0":"#creating a function to plot a histogram of the residual values.\ndef residual_plot(actual,predicted):\n    plt.figure(figsize=(10,6))\n    plt.xlabel('Residual error value')\n    plt.title('Residual Plot',size=13)\n    plt.hist(actual-predicted)\n#plotting the residual error plot","d26929ac":"#training algorithm with the dataset containing outliers\nX_train, X_holdout, y_train, y_holdout = train_test_split(new_data.values,y,test_size=0.3,random_state=17)\nreg=LinearRegression(normalize=True)\nreg.fit(X_train,y_train)\npred=reg.predict(X_holdout)","f8620f1b":"coef_df = pd.DataFrame(reg.coef_, new_data.columns, columns=['coefficients'])\ncoef_df","00521680":"intercept=reg.intercept_\nintercept","b3f220e8":"residual_plot(y_holdout,pred)","9b792a4f":"plt.figure(figsize=(8,6))\nplt.xlabel('predicted')\nplt.ylabel('actual')\nsns.scatterplot(pred,y_holdout)","83700f06":"reg_metrics(y_holdout,pred)","5e49f71f":"ridge=Ridge()\ncv=RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid=dict()\ngrid['alpha']=np.arange(0.0,1.0,0.01)\nsearch=GridSearchCV(ridge, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1,refit=False)\nresult=search.fit(X_train,y_train)\nprint('MAE: %.3f' % result.best_score_)\nprint('Config: %s' % result.best_params_)","f943b062":"ridge=Ridge(alpha=0.04)\nridge.fit(X_train,y_train)\npred1=ridge.predict(X_holdout)\nreg_metrics(y_holdout,pred1)","530f7d76":"residual_plot(y_holdout,pred1)","1c052142":"lasso=Lasso(normalize=True)\ncv=RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid=dict()\ngrid['alpha']=np.arange(0.0,2.0,0.01)\nsearch=GridSearchCV(lasso,grid, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1,refit=False)\nresult=search.fit(X_train,y_train)\nprint('MAE: %.3f' % result.best_score_)\nprint('Config: %s' % result.best_params_)\n","2602db03":"lasso=Lasso(normalize=True,alpha=0)\nlasso.fit(X_train,y_train)\npred2=lasso.predict(X_holdout)\nreg_metrics(y_holdout,pred2)","452b44ec":"from sklearn.ensemble import RandomForestRegressor","3f1bf1b5":"forest=RandomForestRegressor(n_estimators=100)\nforest.fit(X_train,y_train)\npred3=forest.predict(X_holdout)\nreg_metrics(y_holdout,pred3)","73e35b19":"from sklearn.model_selection import GridSearchCV","ae57918a":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [3,4,5],\n    'max_features': [3,4,5],\n    'min_samples_leaf': [3,4,5],\n    'min_samples_split': [8,10],\n    'n_estimators': [100, 200]\n}\nforest_cv=RandomForestRegressor(criterion='mae')\n# Instantiate the grid search model\ngrid=GridSearchCV(estimator=forest_cv, param_grid=param_grid, cv=6, n_jobs=-1, verbose=2)","37f2da40":"grid.fit(X_train,y_train)","15ba4247":"grid.best_params_","fbab283b":"forest_cv=RandomForestRegressor(criterion='mae',bootstrap=True,max_depth=5,max_features=4,min_samples_leaf=3,min_samples_split=10,n_estimators=100)\nforest_cv.fit(X_train,y_train)\npred4=forest_cv.predict(X_holdout)","476a6327":"reg_metrics(y_holdout,pred4)","db06d98b":"residual_plot(y_holdout,pred4)","fd145523":"X1_train, X1_holdout, y1_train, y1_holdout = train_test_split(new_data,y,test_size=0.3,random_state=17)","8949b04b":"from sklearn.preprocessing import OneHotEncoder","40164de4":"ohe=OneHotEncoder(sparse=False,handle_unknown='ignore')\nOH_cols_train = pd.DataFrame(ohe.fit_transform(X1_train[['gender']]))\nOH_cols_holdout = pd.DataFrame(ohe.transform(X1_holdout[['gender']]))\n\nOH_cols_train.index = X1_train.index\nOH_cols_holdout.index = X1_holdout.index\n\nnum_X_train = X1_train.drop(['gender'], axis=1)\nnum_X_holdout = X1_holdout.drop(['gender'], axis=1)\n\nOH_X_train = pd.concat([num_X_train, OH_cols_train],axis=1)\nOH_X_holdout = pd.concat([num_X_holdout, OH_cols_holdout],axis=1)","23628fcc":"reg_ohe=LinearRegression(normalize=True)\nreg_ohe.fit(OH_X_train,y_train)\npred_ohe=reg_ohe.predict(OH_X_holdout)","94891d30":"reg_metrics(y_holdout,pred_ohe)","7a2a7b2b":"forest_ohe=RandomForestRegressor(n_estimators=100)\nforest_ohe.fit(OH_X_train,y_train)\npred_ohe1=forest_ohe.predict(OH_X_holdout)\nreg_metrics(y_holdout,pred_ohe1)","a8b0854e":"param_grid_ohe = {\n    'bootstrap': [True],\n    'max_depth': [4,5],\n    'max_features': [4,5,6],\n    'min_samples_leaf': [3,4,5],\n    'min_samples_split': [8,10],\n    'n_estimators': [100, 200]\n}\nforest_cv_ohe=RandomForestRegressor(criterion='mae')\n# Instantiate the grid search model\ngrid_ohe=GridSearchCV(estimator=forest_cv_ohe, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)","9801b85a":"grid_ohe.fit(OH_X_train,y_train)","5eaf51f1":"grid_ohe.best_params_","d47f3798":"for_cv_ohe=RandomForestRegressor(bootstrap=True,max_depth=5,max_features=5,min_samples_leaf=4,min_samples_split=10,n_estimators=100)\nfor_cv_ohe.fit(OH_X_train,y_train)\npred6=for_cv_ohe.predict(OH_X_holdout)\nreg_metrics(y_holdout,pred6)","26525d47":"# PREDICTING STUDENT PERFORMANCE IN EXAMS USING REGRESSION ALGORITHMS","f9d8ee41":"In this we look at one feature at a time. Finding out the distribution of that feature while ignoring other features is usually done here.","b31dd6f5":"As we can see,Hyperparameter tuning improved the performance of the model on unseen data. But it is still slightly better in performance than Linear regression. ","115ce659":"CROSS VALIDATION ON THE TRAINING DATASET TO DETERMINE ALPHA VALUE","2bcc6380":"**VISUAL DATA ANALYSIS**","bdbecabd":"Out of the 3 scatterplots with different classification bases, gender and lunch_type seem the most effective on getting a good math score. So I will eliminate test_prep as one of the predicting features and go ahead with gender and lunch_type.","3f7bbffe":"# LINEAR REGRESSION","81ef0528":"# UNIVARIATE ANALYSIS","05e93ec4":"# **RANDOM FOREST REGRESSOR**","301a8aaf":"# LINEAR REGRESSION IN ONE-HOT ENCODED DATASET","d0eba199":"Here we consider two features at a time and test the null hypothesis which assumes that the 2 features are independent against the alternative hypothesis that those 2 features are dependent.","5d1e485a":"Lasso and Ridge regression performed slightly worse than the Linear Regression model. Hence I choose my Linear Regression model to predict future unseen data. It manages to explain around 84% of the variance in the data with a MAE of around 5 marks.\n\nUpdations will be made to the notebook.","5f533e56":"# ONE HOT ENCODING CATEGORICAL FEATURES.\n","37d39a43":"# RIDGE REGRESSION","4fc34082":"**Renaming Columns**","15ef45e5":"In the scatterplot that has hue parameter set as gender, we can see that male students typically have higher math scores than female students. This could be used to One Hot encode the gender feature later.","03b2b0e7":"The Ridge regression model performed slightly worse than the Linear regression model.","6c3a5434":"Correlation of \"total\" with \"Reading\" and \"writing\" is very high so we drop the total column.","589acc59":"Hyperparameter tuning:\n","474ef6f8":"This dataset consists of data related to students of a particular grade and their scores in Maths, Reading and Writing specified out of 100. I will be using various regression algorithms to predict math score by using the features available in the dataset.","d8c2e898":"# VARIABLE DESCRIPTIONS:\n\n1) gender: specifies gender of the student(male\/female)\n\n2) race: specifies race of the student(group A,group B,group C)\n\n3) parental level of education: specifies highest educational qualification of any parent of each student\n\n4) lunch_type: standard\/reduced,the type of lunch package selected for the student\n\n5) test_prep: specifies if the test preparation course was completed by the student or not\n\n6) math_score: specifies score in math(our target variable)\n\n7) reading_score: specifies score in reading\n\n8) writing_score: specifies score in writing\n\nAll scores are taken out of 100.","c32570cd":"By correlation coefficient we can conclude that there is no high-level of correlation between any of the features.","b4493e3b":"# RANDOM FOREST REGRESSOR ON ONE HOT ENCODED DATA","07fa5e11":"Without applying techniques to improve the regressor model, the RandomForest regressor performs worse than the Lasso and Linear regression models, giving an MAE of about 5.7 marks and an R2 score of 0.786.\n\nI will now apply GridSearchCV to figure out the best parameters to predict Math score.","8ffbcd77":"Defining the metrics obtained above:\n\n1) MAE is just the Mean Absolute Error, ie the mean of residual errors mathematically given by sum(abs(Y(actual)-Y(pred)))\/no.of predictions\n\n2) MSE is the Mean Square Error, ie the square of the MAE.\n\n3) RMSE is the square root of MSE.\n\n4) R2 score returns a value that tells you how much of the variation in the target variable has been captured by the features used to predict said target variable.","be6c3f9a":"# MULTIVARIATE ANALYSIS","754a628a":"Every distribution plotted above is Left-Skewed.","824cceea":"# CHI-SQUARE TEST FOR FEATURE SELECTION","b18308e5":"BOXPLOTS","576d59ff":"Now for feature engineering and feature selection to predict math score of students, I will create a copy of the original dataset and manipulate features there.","967765ee":"CONCLUSION: LINEAR REGRESSION MODEL WILL BE USED TO MAKE PREDICTIONS OF MATH SCORE OF STUDENTS","6b280d1a":"Since every pair of categorical features is independent we do not eliminate any of these for our feature selection process.","2cb62a1b":"As we can see \"some high school\" and \"high school\" represent the same level of education, we can replace the former with a single value namely \"high school\".","0d79c204":"Encoding the categorical features","50657628":"# LASSO REGRESSION","fc54f324":"In both cases gridsearch has improved model performance but still Linear regression gives the maximum R2 score and the least MAE.","928a2d59":"**Data Wrangling**","2bf713ff":"Dataset has zero null values with 8 columns and 1000 rows.","063b42f3":"#Interpreting boxplots","e38c16b5":"The best evaluation metrics for predicting marks would be the MAE because a student's grade in any subject can change by even 1 mark. So we will consider MAE and RMSE to be our top priorities to ensure closest possible predictions"}}