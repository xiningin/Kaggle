{"cell_type":{"6f210fbe":"code","54605aaf":"code","9bb40368":"code","8a7b1d01":"code","d1461cfd":"code","67123883":"code","ab88e98f":"code","a8ca642d":"code","b21eea37":"code","2d0e4fa0":"markdown","ea14481e":"markdown","2581db36":"markdown","a8a41ca8":"markdown","b9b8cf45":"markdown","d92deeaa":"markdown","34867916":"markdown","18f1a246":"markdown","ed951d52":"markdown","88fd4106":"markdown"},"source":{"6f210fbe":"import numpy as np\n\nimport tensorflow as tf\n\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\nprint(\"Version: \", tf.__version__)\nprint(\"Eager mode: \", tf.executing_eagerly())\nprint(\"Hub version: \", hub.__version__)\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")","54605aaf":"# Split the training set into 60% and 40%, so we'll end up with 15,000 examples\n# for training, 10,000 examples for validation and 25,000 examples for testing.\ntrain_data, validation_data, test_data = tfds.load(\n    name=\"imdb_reviews\", \n    split=('train[:60%]', 'train[60%:]', 'test'),\n    as_supervised=True)","9bb40368":"train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\ntrain_examples_batch","8a7b1d01":"train_labels_batch","d1461cfd":"embedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[], \n                           dtype=tf.string, trainable=True)\nhub_layer(train_examples_batch[:3])","67123883":"model = tf.keras.Sequential()\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.summary()","ab88e98f":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","a8ca642d":"history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=20,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)","b21eea37":"results = model.evaluate(test_data.batch(512), verbose=2)\n\nfor name, value in zip(model.metrics_names, results):\n    print(\"%s: %.3f\" % (name, value))","2d0e4fa0":"- Now, let\u2019s print the first 10 labels from the data set:","ea14481e":"Tranining The Text Classification Model\n- Train the model for 20 epochs in mini-sets of 512 samples. These are 20 iterations on all the samples of the tensors x_train and y_train. During training, monitor model loss and accuracy on the 10,000 samples in the validation set:","2581db36":"- Now build the model on the complete dataset:","a8a41ca8":"- Now, let\u2019s get started with this task of text classification with TensorFlow by importing some necessary libraries","b9b8cf45":"- Data Exploration\nLet\u2019s have a look at the data to figure out what we are going to work with. I will simply print the first 10 samples from the dataset:","d92deeaa":"- Although the dataset I am using here is available online to download, but I will simply load the data using TensorFlow. It means you don\u2019t need to download the dataset from any external sources. Now, I will simply load the data and split it into training and test sets:","34867916":"Evaluating The Model\n- And let\u2019s see how the text classification model works. \n- Two values \u200b\u200bwill be returned. \n- Loss and accuracy rate:","18f1a246":"Compile The Model\n- Now, I will compile the model by using the loss function and the adam optimizer:","ed951d52":"# Text Classification with TensorFlow\n- I\u2019ll walk you through the basic application of transfer learning with TensorFlow Hub and Keras. I will be using the IMDB dataset which contains the text of 50,000 movie reviews from the internet movie database. These are divided into 25,000 assessments for training and 25,000 assessments for testing. The training and test sets are balanced in a way that they contain an equal number of positive and negative reviews.","88fd4106":"# Building Text Classification Model\nTo build a model for the task of Text Classification with TensorFlow, I will use a pre-trained model provided by TensorFlow which is known by the name TensorFlow Hub. Let\u2019s first create a Keras layer that uses a TensorFlow Hub model to the embed sentences, and try it out on some sample input:"}}