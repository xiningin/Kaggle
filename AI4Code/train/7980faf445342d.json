{"cell_type":{"9015171b":"code","3df3ddfe":"code","de694525":"code","43c5fe26":"code","3d3984f5":"code","71313eea":"code","a874bf62":"code","ddc08bd1":"code","9ba3730e":"code","1b5be228":"code","5ffd6202":"code","0f3d271c":"code","8a62f864":"code","cb15c52e":"code","e1ce081f":"code","59f3f47d":"code","cba2659e":"code","7a8b5b3c":"markdown","657a539b":"markdown","2673de56":"markdown","52475d53":"markdown","33d1575a":"markdown","6ecff240":"markdown","7be488dd":"markdown","f6f303e3":"markdown","df219a5c":"markdown","3ce80751":"markdown","c6c08972":"markdown","48f2b256":"markdown","e15ef9e1":"markdown","89eb62ff":"markdown","35b97c30":"markdown","0d9aa313":"markdown","86952299":"markdown","654fecaf":"markdown","16524ee4":"markdown","f385a9c7":"markdown","d82214e8":"markdown","4a185b1a":"markdown","fdb82091":"markdown","7ea30c1a":"markdown","ad26cc45":"markdown"},"source":{"9015171b":"from IPython.display import Image\nimport os\nImage(\"..\/input\/employee\/Feature Significance.JPG\")","3df3ddfe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de694525":"#import modules\n\n#import libraries for data handling\nimport os\nimport pandas as pd # for dataframes\nimport numpy as np\n\n#import for visualization\nimport seaborn as sns # for plotting graphs\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting graphs\n%matplotlib inline\n\n#import for Linear regression\nfrom sklearn.linear_model import LinearRegression\n\n#import warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\n","43c5fe26":"basetable1 = pd.read_excel('\/kaggle\/input\/employee\/employee_churn.xls')\nbasetable1.head(5)","3d3984f5":"# correcting column names\nbasetable1 = basetable1.rename(columns={'left': 'target', 'Work_accident': 'work_accident'})\nbasetable1 = basetable1.rename(columns={'Departments ': 'departments'})\nbasetable1.info()","71313eea":"basetable1.head()","a874bf62":"# Create the dummy variable\ndummies_salary = pd.get_dummies(basetable1[\"salary\"], drop_first = True)\n\n# Add the dummy variable to the basetable\nbasetable2 = pd.concat([basetable1, dummies_salary], axis = 1)\n\n# Delete the original variable from the basetable\ndel basetable2[\"salary\"]\n\n# Create the dummy variable\ndummies_departments = pd.get_dummies(basetable2[\"departments\"], drop_first = True)\n\n\n# Add the dummy variable to the basetable\nbasetable2 = pd.concat([basetable2, dummies_departments], axis = 1)\n\n# Delete the original variable from the basetable\ndel basetable2[\"departments\"]\n\n\nbasetable2.head()","ddc08bd1":"# chck if there are any NUll values\nbasetable2.isnull().sum()","9ba3730e":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\")\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n#modeling_data\n\n# Machine Learning \nFI_predictor = basetable2.drop([\"target\"], 1)\nFI_target = basetable2[\"target\"]\n\nclf = RandomForestClassifier(n_estimators=50, max_features='sqrt')\nclf = clf.fit(FI_predictor, FI_target)\n\n#have a look at the importance of each feature.\nfeatures = pd.DataFrame()\nfeatures['feature'] = FI_predictor.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\n##features.set_index('feature', inplace=True)\n\n\n# Initialize the matplotlib figure\nf, ax = plt.subplots(figsize=(10, 10))\n\n# Load the dataset\nFI = features.sort_values(\"importance\", ascending=False)\n\n# Plot the total crashes\nsns.set_color_codes(\"pastel\")\nsns.barplot(x=\"importance\", y=\"feature\", data=FI,\n            label=\"importance\", color=\"b\")\n\n\n# Add a legend and informative axis label\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\nax.set(xlim=(0, 0.4), ylabel=\"\",\n       xlabel=\"Feature Importance\")\nsns.despine(left=True, bottom=True)","1b5be228":"# Checking absolute values of Feature importance\nFI","5ffd6202":"# Import the GB and accuracy modules\n#Import Gradient Boosting Classifier model\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n# define auc function\ndef gbacc(variables, target, basetable):\n    predictions = 0 # reset the value\n    auc= 0 # reset the value\n    \n    X = basetable[variables]\n    Y = basetable[target]\n    #Create Gradient Boosting Classifier\n    gb = GradientBoostingClassifier()\n    gb.fit(X, Y)\n    predictions = gb.predict(X)\n    gb_acc = metrics.accuracy_score(Y, predictions)\n    return(gb_acc)\n","0f3d271c":"gb_acc = gbacc([\"satisfaction_level\", \"promotion_last_5years\",\"number_project\"],[\"target\"], basetable2)\nprint(round(gb_acc,2))","8a62f864":"def next_best_v(current_variables,candidate_variables, target, basetable):\n    best_gbacc = -1\n    best_variable = None\n    \n\t# Calculate the auc score of adding v to the current variables\n    for v in candidate_variables:\n        gbacc_v = gbacc(current_variables + [v],target, basetable)\n        \n\t\t# Update best_auc and best_variable adding v led to a better auc score\n        if gbacc_v >= best_gbacc:\n            best_gbacc = gbacc_v\n            best_variable = v\n            \n    return best_variable\n","cb15c52e":"next_variable = next_best_v([\"satisfaction_level\",\"last_evaluation\"], [\"number_project\",\"average_montly_hours\",\"time_spend_company\",\"work_accident\",\"promotion_last_5years\",\"low\",\"medium\",\"RandD\",\"accounting\",\"hr\",\"management\",\"marketing\",\"product_mng\",\"sales\",\"support\",\"technical\"], [\"target\"], basetable2)\nprint(next_variable)\n\n\n","e1ce081f":"candidate_variables = [\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\",\"work_accident\",\"promotion_last_5years\",\"low\",\"medium\",\"RandD\",\"accounting\",\"hr\",\"management\",\"marketing\",\"product_mng\",\"sales\",\"support\",\"technical\"]\ncurrent_variables = []\ntarget = [\"target\"]\n\nmax_number_variables = 8\nnumber_iterations = min(max_number_variables, len(candidate_variables))\nresult = []\nfor i in range(0,number_iterations):\n    \n    next_variable = next_best_v(current_variables,candidate_variables,target,basetable2)\n    \n    current_variables = current_variables + [next_variable]\n    candidate_variables.remove(next_variable)\n    #result.append((gb_acc))\n##print(gb_acc)\nprint(current_variables)\n\n","59f3f47d":"srt_variable = [\"satisfaction_level\",\"last_evaluation\",\"number_project\",\"average_montly_hours\",\"time_spend_company\",\"work_accident\",\"promotion_last_5years\",\"low\",\"medium\",\"RandD\",\"accounting\",\"hr\",\"management\",\"marketing\",\"product_mng\",\"sales\",\"support\",\"technical\"]\ncurrent_variables = []\ntarget = [\"target\"]\nresult = []\n\nfor v in srt_variable:\n        gbacc_v = gbacc(current_variables + [v],target, basetable2)\n        \n        #print(gbacc_v)\n        #print(v)\n        result.append((v,gbacc_v))\n        \n\npd.DataFrame(result)\ndf = pd.DataFrame(result, columns =['Variable', 'Model_Accuracy']) \ndf   ","cba2659e":"import matplotlib.pyplot as plt\n\n\nsns.set(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\n\ndf1 = df.sort_index(axis = 1)\nax = sns.catplot(x=\"Variable\", y=\"Model_Accuracy\",kind = \"point\",  data=df1)\n\nplt.xticks(rotation=90)\n    \n# Show plot\nplt.show()","7a8b5b3c":"There are only Low & Medium Salary and now High. The data seems to be real :)","657a539b":"# Selecting Features based on Model Accuracy","2673de56":"Function is created and now its time to check the AUC score.","52475d53":"If we have to find more number of features that we can add. We can repeat the process and keep adding to the list.","33d1575a":"Beautiful! Satisfaction Level, Time spent and Number of projects are our top 3 feature that are influcing the target feature Churn.","6ecff240":"WOW! 92% . thats great with 3 features. which would be the next feature to add to the list? Lets check next bst feature.","7be488dd":"We will use AUC score for evaluating model accuracy.\nAUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).\nAUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n# More the AUC scrore better the model is!\n\n* More details on sklearn : [link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html)\n* and google developer: [link](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/roc-and-auc)","f6f303e3":"We see that Model Accuracy improves to an extent but flattens out after 6th Features. \n# We conclude, Finding Feature Significance is important for model accuracy.","df219a5c":"# Approach for Feature selection: STEP FORWARD APPROACH\n* Calculate AUC score\n* Find next best Variable\n* Find List of Best Features","3ce80751":"# **Choosing best set of features to make Models**","c6c08972":"# Now its time to check Feature significance","48f2b256":"Fetch data for analysis","e15ef9e1":"Good! there are no missing values.","89eb62ff":"First we would run a model with Random Forest and check the importance of features. Quick Visualization would give us a clear picture.","35b97c30":"**Data Clean-up**","0d9aa313":"Beore we start with analysis. we would do some basic clean up.\n* Correcting Column names\n* Creating Categorical variable\n* Checking if there are any missing values","86952299":"Using data from dataset: https:\/\/www.kaggle.com\/liujiaqi\/hr-comma-sepcsv","654fecaf":"Here we have 2 sets of features. Current features and Candidate features. We will bring features one by one and check the accuracy of model. We will Select a feature that would give us the best score.","16524ee4":"That is cool! we get our next best feature is time_spend_company.","f385a9c7":"1. Quick look at the missing values","d82214e8":"We got the list of top 8 features that would create best model. \nThis makes me curious see how the model accuracy would be impacted as we keep adding the features.","4a185b1a":"# Beautiful!","fdb82091":"Model accuracy is highly dependent on Features selected in the process. More is not always good. We would explore the importance of features in a model and a handy code to choose the best set of features.","7ea30c1a":"Data looks fine. Since we would be using Gradient Boosting and Randon Forest for models, we need to convert departments & Salary feature to categorical features.","ad26cc45":"Good! we have the data."}}