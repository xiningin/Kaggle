{"cell_type":{"4bb8c4cd":"code","615350b5":"code","8d5c7515":"code","b472b57b":"code","4dc23f27":"code","da0a246a":"code","a682f2e7":"code","49639dac":"code","09cd2e24":"code","88021303":"code","db58da9f":"code","264fad12":"code","a3d2c68f":"markdown","4efbcec1":"markdown","904617e3":"markdown"},"source":{"4bb8c4cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","615350b5":"curr_dir = '\/kaggle\/input\/nyt-comments\/'\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        break\narticle_df.head()","8d5c7515":"hdline = []\nhdline.extend(article_df.headline)\nhdline = [h for h in hdline if h not in \"Unknown\"]\nlen(hdline)","b472b57b":"import string\ndef clean(text):\n    token = \"\".join(t for t in text if t not in string.punctuation).lower()\n    return token","4dc23f27":"df = [clean(x) for x in hdline]","da0a246a":"# Generating Sequence of N-gram Tokens\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer()\ndef get_seq(df):\n    # Tokenization \n    tokenizer.fit_on_texts(df)\n    tot_words = len(tokenizer.index_word) +1\n    \n    # Convert Data to sequence\n    input_seq = []\n    for line in df:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram = token_list[:i+1]\n            input_seq.append(n_gram)\n    return input_seq, tot_words\n","a682f2e7":"input_seq, tot_words = get_seq(df)\ninput_seq[:10]\n","49639dac":"from keras.preprocessing.sequence import pad_sequences\nimport keras.utils as ku \ndef pad_seq(input_seq):\n    max_seq_len = max([len(x) for x in input_seq])\n    input_sequences = np.array(pad_sequences(input_seq, maxlen=max_seq_len, padding='pre'))\n    x, y = input_sequences[:, :-1], input_sequences[:, -1]\n    y = ku.to_categorical(y, num_classes=tot_words)\n    return x,y,max_seq_len\n\nx, y, max_seq_len = pad_seq(input_seq)","09cd2e24":"from keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\ndef create_model(max_seq_len, tot_words):\n    inp_length = max_seq_len - 1\n    model = Sequential()\n    model.add(Embedding(tot_words, 10, input_length=inp_length))\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    model.add(Dense(tot_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_seq_len, tot_words)\nmodel.summary()","88021303":"model.fit(x, y, epochs=100, verbose=5)\n","db58da9f":"\ndef generate_text(seed_text, next_words, model, max_seq_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","264fad12":"print (generate_text(\"united states\", 10, model, max_seq_len))\nprint (generate_text(\"preident trump\", 10, model, max_seq_len))\nprint (generate_text(\"donald trump\", 10, model, max_seq_len))\nprint (generate_text(\"india and china\", 10, model, max_seq_len))\nprint (generate_text(\"new york\", 10, model, max_seq_len))\nprint (generate_text(\"science and technology\", 5, model, max_seq_len))","a3d2c68f":"# Text Generation LSTM : \nIn this kernel, I will be using the dataset of New York Times Comments and Headlines to train a text generation language model which can be used to generate News Headlines\n\n","4efbcec1":"###  Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. We will create N-grams sequence as predictors and the next word of the N-gram as label.","904617e3":"### Model Creation:"}}