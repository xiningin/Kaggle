{"cell_type":{"39b826ad":"code","db8737b0":"code","fdd15e1f":"code","267816a2":"code","652bc0d5":"code","d5d012ff":"code","2fd79c11":"code","63d6b4aa":"code","236e8a29":"code","e735a171":"code","a74c6a3a":"code","68e32c1d":"code","837ce334":"code","3a08e441":"code","ff67c13d":"code","010f7a10":"code","9c650ae8":"code","73ebdef6":"code","ae637ee5":"code","c3b5830a":"code","ac64c2ff":"code","75d2a7f8":"code","618f4f7b":"code","dfe53573":"code","11aec216":"code","f10eafdf":"code","f1be2851":"code","da0102dc":"code","c5a3477c":"code","830fcf12":"code","5a3bad69":"code","38c83e67":"code","501002b7":"code","422b0069":"code","28e16d1b":"code","aa201658":"code","e6245d4a":"code","fa8336c4":"code","4cabb66a":"code","dcbd212e":"code","26445e50":"code","654c5bae":"markdown","e7bac248":"markdown","e724f9eb":"markdown","a0360011":"markdown","ba3dbaf3":"markdown","2a92f560":"markdown","ee505b20":"markdown","1f16784b":"markdown","2c62bbed":"markdown","8ba498a3":"markdown","36faac5e":"markdown","f7c5c1b5":"markdown","2c4b94dc":"markdown","2c9e8bfd":"markdown","81cb8c0d":"markdown","3ca6fd5e":"markdown","c8b22483":"markdown","8e72f495":"markdown","a0f27d93":"markdown","0ab46bb6":"markdown","ca6113fb":"markdown"},"source":{"39b826ad":"%load_ext autoreload\n%autoreload 2","db8737b0":"! pip install pyvis","fdd15e1f":"\n\nimport pandas as pd \nimport numpy as np \nfrom datetime import datetime\nimport sys\nimport ast\n\nimport plotly.express as px\n\n#import nltk\n#from nltk.corpus import stopwords\n#import spacy\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nimport networkx as nx\nfrom networkx.algorithms.components.connected import connected_components\n\nimport json\nimport dask.bag as db\n\nimport sys\nimport os\n\nsys.path.append(\"..\")\n\nfrom pathlib import Path\n\nimport json\n\n\nfrom itertools import combinations\nfrom collections import Counter\nfrom itertools import chain\nimport random\n\nfrom tqdm.notebook import tqdm, trange\nimport time    # to be used in loop iterations\n\nimport multiprocessing\nimport smart_open\n\nfrom gensim.models.word2vec import Word2Vec\n\nfrom pyvis.network import Network\n\nfrom IPython.core.display import display, HTML\n\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.metrics.pairwise import cosine_similarity,cosine_distances\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\n","267816a2":"# Extract Only the AI , ML PAPERS\ndef extractArxivData(categories=['stat.ML','cs.AI'],year=None,raw_data_path=\"..\/data\/raw\/\",save_extracted_filename=\"..\/data\/processed\/AI_ML.json\"):\n    \"\"\" This function extracts data for the given set of categories and save the data into the save_extracted_filename path \"\"\"\n    records=db.read_text(raw_data_path+\"\/*.json\").map(lambda x:json.loads(x))\n    docs = (records.filter(lambda x:any(ele in x['categories'] for ele in categories)==True))\n    extract_latest_version=lambda x:x['versions'][-1][\"created\"]\n    if year!=None:\n        docs=docs.filter(lambda x:int(extract_latest_version(x).split(\" \")[3])>=year)\n\n    get_metadata = lambda x: {'id': x['id'],\n                  'title': x['title'],\n                  'category':x['categories'],\n                  'abstract':x['abstract'],\n                 'version':x['versions'][-1]['created'],\n                         'doi':x[\"doi\"],\n                         'authors_parsed':x['authors_parsed']}\n                        \n    data=docs.map(get_metadata).to_dataframe().compute()\n\n    ## Creating authors fields by joining first and last nmes in authors_parsed columns.\n    data['authors']=data['authors_parsed'].apply(lambda authors:[(\" \".join(author)).strip() for author in authors])\n\n    print(\"Number of Records Extracted for Given Set of Categories \",data.shape[0])\n    Path(os.path.dirname(save_extracted_filename)).mkdir(parents=True, exist_ok=True)\n    data.to_json(save_extracted_filename,orient=\"records\")\n    return data\n","652bc0d5":"RAW_DATA_PATH=\"..\/input\/arxiv\/\"\n","d5d012ff":"## Collect data for Papers published in ['stat.ML','cs.AI'] since year 2015.\ndata=extractArxivData(categories=['stat.ML','cs.AI'],year=2015,raw_data_path=RAW_DATA_PATH,save_extracted_filename=\"AI_ML_since2015.json\")","2fd79c11":"data['author_pairs']=data['authors'].apply(lambda x:list(combinations(x, 2)))\ndata.head()","63d6b4aa":"def flattenList(nested_list):\n    flat_list = [item for sublist in nested_list for item in sublist]\n    return flat_list\n","236e8a29":"ai_authors=pd.DataFrame(flattenList(data['authors'].tolist())).rename(columns={0:'authors'})\npapers_by_authors=ai_authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False)\npapers_by_authors.shape","e735a171":"papers_by_authors['Number of Papers Published'].describe()\n","a74c6a3a":"## Keeping Authors who have published more than 2 Papers\nnodes_to_keep=papers_by_authors.loc[papers_by_authors['Number of Papers Published']>2,'authors'].tolist()\nlen(nodes_to_keep)","68e32c1d":"\nauthors_pairs=data['author_pairs'].tolist()\nauthors_edge_list=[item for sublist in authors_pairs for item in sublist]\nauthors_weighted_edge_list=list(Counter(authors_edge_list).items())\nauthors_weighted_edge_list=[(row[0][0],row[0][1],row[1]) for idx,row in enumerate(authors_weighted_edge_list)]\nauthors_weighted_edge_list[0:10]","837ce334":"G1=nx.Graph()\nG1.add_weighted_edges_from(authors_weighted_edge_list)\nprint(len(G1.nodes()))","3a08e441":"## From the complete Graph, create a subgraph, with only the nodes to keep\nsub_g=nx.subgraph(G1,nodes_to_keep)\nG=nx.Graph(sub_g)\nprint(len(G.nodes()))\nisolated_node=nx.isolates(G)\nlen(list(isolated_node))","ff67c13d":"G.remove_nodes_from(list(nx.isolates(G)))\nlen(G.nodes)","010f7a10":"del G1, sub_g","9c650ae8":"print(\"Number of Nodes in Author Graph \",len(G.nodes()))\nprint(\"Number of Edges in AUthor Graph \",len(G.edges()))","73ebdef6":"def getRandomWalk(graph,node,length_of_random_walk):\n    \"\"\" This function takes NetworkX Graph and a Node and generate random walk for a given length \n    \n    Returns the random walk (list of nodes traversed)\n\n    Note: The same node may occcur more than once in a Random Walk.\n    \"\"\"\n    start_node=node\n    current_node=start_node\n    random_walk=[node]\n    for i in range(0,length_of_random_walk):\n        ## Choose a random neighbour of the current node\n        \n        current_node_neighbours=list(graph.neighbors(current_node))\n        chosen_node=random.choice(current_node_neighbours)\n        current_node=chosen_node\n        random_walk.append(current_node)\n    return random_walk\n\n\n","ae637ee5":"### For every Node in the Graph, get randomwalks . For eahc node, let us get random walks say around 10 times each of path length 10\nnum_sampling=10\nrandom_walks=[]\nlength_of_random_walk=10\nfor node in tqdm(G.nodes(),desc=\"Iterating Nodes\"):\n    \n    for i in range(0,num_sampling):\n        random_walks.append(getRandomWalk(G,node,length_of_random_walk))\n\n\n        ","c3b5830a":"deepwalk_model=Word2Vec(sentences=random_walks,window=5,sg=1,negative=5,vector_size=128,epochs=20,compute_loss=True)\n","ac64c2ff":"deepwalk_model.save(\"deepwalk_since2015.model\")","75d2a7f8":"def getSimilarNodes(model,node):\n    \"\"\"\n    This function takes deepwalk model and a node\n    \n    Returns the top 10 nodes (author) similar to the given node \n    \"\"\"\n    similarity=model.wv.most_similar(node)\n    similar_nodes=pd.DataFrame()\n    similar_nodes['Similar_Node']=[row[0] for i,row in enumerate(similarity)]\n    similar_nodes['Similarity_Score']=[row[1] for i,row in enumerate(similarity)]\n    similar_nodes['Source_Node']=node\n    return similar_nodes\n\n","618f4f7b":"getSimilarNodes(deepwalk_model,\"Bengio Yoshua\")","dfe53573":"ai_authors=pd.DataFrame(flattenList(data['authors'].tolist())).rename(columns={0:'authors'})\npapers_by_authors=ai_authors.groupby(['authors']).size().reset_index().rename(columns={0:'Number of Papers Published'}).sort_values(\"Number of Papers Published\",ascending=False)\npapers_by_authors","11aec216":"def getCoAuthorshipNetwork(graph,initial_nodes):\n    \"\"\"\n    This function takes a Graph and list of initial nodes \n    \n    Returns the set of immediate neighbours of these nodes\n\n    \"\"\"\n    total_neighbours=0\n    nodes_set=[initial_nodes]\n    for node in initial_nodes:\n        #print(node)\n        neighbours=list(graph.neighbors(node))\n        total_neighbours=total_neighbours+len(neighbours)\n        \n        nodes_set.append(neighbours)\n    print(total_neighbours)\n    nodes_set=flattenList(nodes_set)\n    return list(set(nodes_set))\n\n","f10eafdf":"\ncoauthor_nodes=getCoAuthorshipNetwork(G,papers_by_authors['authors'].tolist()[4:10])\nprint(\"Number of CoAuthor Nodes \",len(coauthor_nodes))","f1be2851":"coauthor_subgraph=nx.subgraph(G,coauthor_nodes)\nprint(\"number of edges in the CoAuthor Subgraph \",len(coauthor_subgraph.edges()))","da0102dc":"nx.write_gexf(coauthor_subgraph, \"CoAuthor_Subgraph_Author4to10.gexf\")","c5a3477c":"#coauthor_subgraph=nx.read_gexf(\"CoAuthor_Subgraph_Top50Author.gexf\")\nprint(\"number of edges in the CoAuthor Subgraph \",len(coauthor_subgraph.edges()))","830fcf12":"pyvis_nt=Network(notebook=True,height='800px', width='100%',heading='')\n\nprint(\"Creating PyVis from NetworkX\")\npyvis_nt.from_nx(coauthor_subgraph)\n\nprint(\"Saving PyVis Graph\")\npyvis_nt.show(\"Author4to10_CoAuthorGraph.html\")\n\n","5a3bad69":"def getCosineDistanceMatrix(vectors):\n    '''\n    This function takes list of vectors or numpy array \n    \n    Returns the pairwise cosine similarity matrix\n    '''\n    if type(vectors)==list:\n        X=np.asarray(vectors)\n    elif type(vectors)==np.ndarray:\n        X=vectors\n    else:\n        print(\"Error in Data Type . Need to Pass list or numpy array as input argument\")\n        return []\n    cosine_dist=cosine_distances(X)\n    return cosine_dist\n","38c83e67":"coauthor_nodes=list(coauthor_subgraph.nodes)\nprint(\"Number of CoAuthor Subgraph Nodes\",len(coauthor_nodes))","501002b7":"coauthor_embeddings=[deepwalk_model.wv[node] for node in coauthor_nodes]","422b0069":"coauthor_embeddings=np.asarray(coauthor_embeddings)\n\nprint(coauthor_embeddings.shape)","28e16d1b":"cosine_dist=getCosineDistanceMatrix(coauthor_embeddings)\n","aa201658":"sse=[]\nk_list=[]\nfor k in range(2,20):\n   \n    km=KMeans(n_clusters=k)\n    km.fit(cosine_dist)\n    sse.append(km.inertia_)\n\n\n","e6245d4a":"plt.figure(figsize=(12,6))\nplt.plot([i for i in range(2,20)],sse)\n","fa8336c4":"km=KMeans(n_clusters=7)\ncoauthor_clusters=km.fit_predict(cosine_dist)\ncoauthor_cluster_dict={node:str(coauthor_clusters[idx]) for idx,node in enumerate(coauthor_nodes)}\nnx.set_node_attributes(coauthor_subgraph,coauthor_cluster_dict,\"group\")","4cabb66a":"pyvis_nt=Network(notebook=True,height='600px', width='100%',heading='Author Network')\n\nprint(\"Creating PyVis from NetworkX\")\npyvis_nt.from_nx(coauthor_subgraph)\npyvis_nt.toggle_physics(True)\nprint(\"Saving PyVis Graph\")\n#pyvis_nt.show_buttons()\n#pyvis_nt.set_options('var options = {\"edges\": { \"color\": { \"inherit\": true },\"smooth\": false},\"physics\": {\"hierarchicalRepulsion\": { \"centralGravity\": 0 },\"minVelocity\": 0.75, \"solver\": \"hierarchicalRepulsion\",\"timestep\": 0.18}}')\n\n\npyvis_nt.show(\"Author4to10_CoAuthorGraph_Clustered.html\")\n","dcbd212e":"bengio_nodes=getCoAuthorshipNetwork(G,['Bengio Yoshua'])\nbengio_network=nx.subgraph(G,bengio_nodes)\nprint(\"Number of Nodes in Bengio Network \",len(bengio_network.nodes()))\nprint(\"Number of Edges in Bengio Network \",len(bengio_network.edges()))","26445e50":"bengio_nt=Network(notebook=True,height='800px', width='100%',heading='Bengio Network')\n\nprint(\"Creating PyVis from NetworkX\")\nbengio_nt.from_nx(bengio_network)\nbengio_nt.toggle_physics(True)\n#bengio_nt.enable_physics(True)\nprint(\"Saving PyVis Graph\")\n\nbengio_nt.show_buttons()\nbengio_nt.show(\"Bengio_CoAuthorGraph.html\")","654c5bae":"We can see some clusters already. Lets see if embeddings is able to seperate these clusters\n### Cluster the Nodes - based on Embeddings\n\nIdea is to see if similar nodes belong to the same cluster. ","e7bac248":"We can see that the embeddings has done a pretty good job at clustering the network. \n\n### Visualising Bengio Yoshuas Network","e724f9eb":"# Load the Libraries","a0360011":"### Let us look at the CoAuthorship Network of the top popular Authors\n\n","ba3dbaf3":"The top 3 similar authors have recently published a paper **Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing its Gradient Estimator Bias**. They have published lot of work together in the recent years. \n\nJo Jason since 2019 has published 7 papers with Yoshua\n","2a92f560":"The data now is similar to list of words in a sentence and we can use gensim to create Node Embedding Model - here each author is a Node and Node is similar to word in a sentence","ee505b20":"### Creating the Graph on the Complete Data","1f16784b":"# Introduction\n\nIn this we will look at Arxiv papers published in ML and AI since the year 2015- with focus of Co-Author Network.We will use Deep Walk (which is a concept based of Word Embeddings) to cluster author networks.\nThis kind of embedding a Graph, can help in applications like clustering, Link PRediction etc.\n\nFor Visualising the Graph we will use pyvis","2c62bbed":"### APply K-Means tp select the optimal number of clusters","8ba498a3":"Lets get the graph of these top  Authors with their first step neighbours.We consider the authors ranked 4th to 10th to help visualise the clusters better","36faac5e":"## For our Analysis, we will consider authors who have published papers after 2015 and published more than 2 papers.\n","f7c5c1b5":"### Generate a Co-Author Graph from the complete Graph","2c4b94dc":"# Extract the Data from Kaggle ","2c9e8bfd":"### Filtering the Graph, to keep nodes (authors) who have atleast published 3 papers. We will also remove any isolated nodes in the generated network","81cb8c0d":"### Generating the Edges of the Co-Author Network","3ca6fd5e":"## Implementing Deep Walk\n\n**Deep walk uses the concept of Random Walks to assign an embedding to each node in the network.** \n\n1. In Random Walk, given a node we pick one of its neighbours at random and move to this node and from this node again choose another node among its neighbours at random. This continues for a fixed number of steps. \n\n\n\n2. Once we have random walks generated for every node in the network, in DeepWalk the next step is to predict probability of visiting node \"v\" on a random walk starting from node \"u\". \n \n3. This is very similar to the Skip-Gram model used in Word2Vec Model in NLP, wherein we try to predict the neighbouring words given a particular target word.\n\n\n### Define Function for Random Walk\n","c8b22483":"## Load the Data\n","8e72f495":"### Visualise the generated network","a0f27d93":"## Lets Look who are similar authors ","0ab46bb6":"# Creating a Co-Author Network\n\nFor the set of papers extracted, for every pair of authors an edge is to be created. The Edge weight will be the number of papers the two authors have collabrated on. ","ca6113fb":"Lets pick 7 clusters and update Node Attribute of the coAuthor Subgraph "}}