{"cell_type":{"42251ef5":"code","4b9c86ca":"code","d37b3cb8":"code","e5f53f43":"code","5e41cf8d":"code","4620fd72":"code","880382ba":"code","8840f209":"code","71d939f2":"code","4b9276bf":"code","50a1bbfc":"code","a91de3a3":"code","e0d29e49":"code","a14faabc":"code","cbde80ac":"code","973739d9":"code","8dad0fc3":"code","54b5ebbb":"code","3d12f527":"code","f937722e":"code","61893edd":"code","67d5cc33":"code","6bee61d2":"code","4b864a2a":"code","fd0fd09f":"code","4cb67b71":"code","d32ca4a6":"code","244d5449":"code","066718ea":"code","877241cb":"code","e80056c8":"code","936039d4":"code","d4b5f12d":"code","0aa79737":"code","8f90c9bb":"code","90cdb5f3":"code","09fd9271":"code","69087cde":"markdown","372b037a":"markdown","5b023cf5":"markdown","b0eaba24":"markdown","0667cd0d":"markdown","99321d50":"markdown","97aec3a4":"markdown","6fadbda5":"markdown","10ea4937":"markdown"},"source":{"42251ef5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b9c86ca":"!pip install contractions","d37b3cb8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly import graph_objs as go\nimport plotly.figure_factory as ff\n\n\n#Text Preprocessing libraries\nimport nltk\nnltk.download('stopwords')\nimport re \nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.tokenize import word_tokenize\nimport contractions\nfrom nltk.stem import SnowballStemmer\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\n\n\nimport xgboost as xgb\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\n\nnltk.download('wordnet')\nnltk.download('punkt')","e5f53f43":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_red = \"#eb345b\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"","5e41cf8d":"df = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head()","4620fd72":"df.info()","880382ba":"print(\"-------Shape of data--------\")\ndf.shape","8840f209":"#Copying the data \ndf_copy = df.copy()","71d939f2":"df['is_sarcastic'].value_counts()","4b9276bf":"df['source_of_text'] = df['article_link'].apply(lambda x: re.findall(r'\\w+', x)[2])\ndf.head()","50a1bbfc":"df = df.drop(['article_link'],axis = 1)","a91de3a3":"df.head()","e0d29e49":"px.pie(df,names='is_sarcastic',labels=['Sarcastic','Acclaim'],title='Sarcasam Vs Acclaim',template='plotly_dark')","a14faabc":"# prettier graphs!\nplt.style.use('ggplot')","cbde80ac":"target_counts=df['source_of_text'].value_counts()\nplt.figure(figsize = (15,7))\nsns.barplot(y=target_counts,x=target_counts.index)\nplt.title(\"Counting the values in Source column\",fontsize = 24)\nplt.ylabel('Sample')\nplt.xlabel('Target')","973739d9":"df['message_len'] = df['headline'].apply(lambda x: len(x.split(' ')))\ndf.head()","8dad0fc3":"sarcastic_df = df[df['is_sarcastic'] == 1]['message_len'].value_counts().sort_index()\nnot_sarcastic_df = df[df['is_sarcastic'] == 0]['message_len'].value_counts().sort_index()\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=sarcastic_df.index,\n    y=sarcastic_df.values,\n    name='Sarcastic',\n    fill='tozeroy',\n    marker_color=primary_red,\n))\nfig.add_trace(go.Scatter(\n    x=not_sarcastic_df.index,\n    y=not_sarcastic_df.values,\n    name='Acclaim',\n    fill='tozeroy',\n    marker_color=primary_blue,\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Data Roles in Different Fields<\/span>'\n)\nfig.update_xaxes(range=[0, 70])\nfig.show()","54b5ebbb":"df['headline']=df['headline'].str.lower()\n# Code to remove the Hashtags from the text\ndf['headline']=df['headline'].apply(lambda x:re.sub(r'\\B#\\S+','',x))\n# Code to remove the links from the text\ndf['headline']=df['headline'].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n# Code to remove the Special characters from the text \ndf['headline']=df['headline'].apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n# Code to substitute the multiple spaces with single spaces\ndf['headline']=df['headline'].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n# Code to remove all the single characters in the text\ndf['headline']=df['headline'].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n# Remove the twitter handlers\ndf['headline']=df['headline'].apply(lambda x:re.sub('@[^\\s]+','',x))","3d12f527":"stop_words = stopwords.words('english')\nmore_stopwords = ['u', 'im', 'c']\nstop_words = stop_words + more_stopwords\n\ndef remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text\n    \ndf['headline_clean'] = df['headline'].apply(remove_stopwords)\ndf.head()","f937722e":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\ndf['tokenized'] = df['headline_clean'].apply(lambda x: tokenization(x.lower()))\ndf.head()","61893edd":"sarcastic = df[df['is_sarcastic']==0]['headline_clean']\nsarcastic[:10]","67d5cc33":"not_sarcastic = df[df['is_sarcastic']==1]['headline_clean']\nnot_sarcastic[:10]","6bee61d2":"stemmer = nltk.SnowballStemmer(\"english\")\n\ndef stemm_text(text):\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text\n\ndf['headline_clean'] = df['headline_clean'].apply(stemm_text)\ndf.head()","4b864a2a":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 0].headline_clean))\nplt.imshow(wc , interpolation = 'bilinear')","fd0fd09f":"plt.figure(figsize = (20,20)) # Text that is Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.is_sarcastic == 1].headline_clean))\nplt.imshow(wc , interpolation = 'bilinear')","4cb67b71":"all_words=[]\nfor i in range(len(df['tokenized'])):\n    a=df['tokenized'][i]\n    for i in a:\n        all_words.append(i)\nall_words=pd.Series(np.array(all_words))\n\ncommon_words=all_words.value_counts()[:100].rename_axis('Common Words').reset_index(name='count')\n\nfig = px.treemap(common_words, path=['Common Words'], values='count',template= \"plotly_dark\",title='100 Most Common Words In Headline')\nfig.show()","d32ca4a6":"#Spliting it into training and testing \nX = df['headline_clean']\ny = df['is_sarcastic']\n\n# Split into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y,test_size = 0.2,random_state=42)\nprint(len(x_train), len(y_train))\nprint(len(x_test), len(y_test))","244d5449":"pipe = Pipeline([\n    ('bow', CountVectorizer()), \n    ('tfid', TfidfTransformer()),  \n    ('model', xgb.XGBClassifier(\n        use_label_encoder=False,\n        eval_metric='auc',\n    ))\n])\nfrom sklearn import metrics\n\n# Fit the pipeline with the data\nMODEL = pipe.fit(x_train, y_train)\n\ny_pred_class = pipe.predict(x_test)\ny_pred_train = pipe.predict(x_train)\n\nprint('Train: {}'.format(metrics.accuracy_score(y_train, y_pred_train)))\nprint('Test: {}'.format(metrics.accuracy_score(y_test, y_pred_class)))","066718ea":"cm = metrics.confusion_matrix(y_test, y_pred_class)\ncm = pd.DataFrame(cm , index = ['Not Sarcastic','Sarcastic'] , columns = ['Not Sarcastic','Sarcastic'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"seismic_r\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' ,\n            xticklabels = ['Not Sarcastic','Sarcastic'] , yticklabels = ['Not Sarcastic','Sarcastic'])","877241cb":"vocab_size = 3000\nmax_len = 500\nembedding_dim = 16\noov_tok = \"<OOV>\"\npadding_type = \"post\"\ntrunc_type = \"post\"\ntraining_size = 20000","e80056c8":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\ntok = Tokenizer(num_words=vocab_size)\ntok.fit_on_texts(x_train)\nsequences = tok.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","936039d4":"# define a DNN model with an Embedding layer\nfrom tensorflow import keras\nfrom keras import layers\nimport tensorflow as tf\n\nmodel = keras.Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n                         layers.GlobalAveragePooling1D(),\n                         layers.Dropout(0.3),\n                         layers.Dense(16, activation=\"relu\"),\n                         layers.Dense(1, activation=\"sigmoid\")])\n                         \nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n                         \nmodel.summary()\nkeras.utils.plot_model(model)","d4b5f12d":"#Early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nrlrp=ReduceLROnPlateau(monitor='val_loss', patience=2,factor=0.01, min_lr=0.00001)","0aa79737":"batch_size=16\nepochs=20","8f90c9bb":"history_model = model.fit(\n    x=sequences_matrix,\n    y=y_train,\n    validation_data=(x_test, y_test),\n    validation_split=0.1,\n    batch_size=batch_size,\n    epochs=epochs,\n    shuffle=True,\n    verbose=1,\n    callbacks=[early_stopping,rlrp]\n)","90cdb5f3":"#Let's plot the curve for loss,val_loss,accuracy,val_accuracy\ndef plot_loss_nd_accuracy(history):\n    history_df=pd.DataFrame(history)\n    history_df.loc[0:,['loss','val_loss']].plot()\n    history_df.loc[0:,['accuracy','val_accuracy']].plot()","09fd9271":"plot_loss_nd_accuracy(history_model.history)","69087cde":"# ***Proprocessing the data***","372b037a":"# XGB CLASSIFIER","5b023cf5":"# **--------------------Up-Vote-----------------------**","b0eaba24":"# ***Importing the libraries***","0667cd0d":"\n**Contractions** are shortened version of words or syllables. They often exist in either written or spoken forms in the English language. These shortened versions or contractions of words are created by removing specific letters and sounds.","99321d50":"# ***Word Cloud***","97aec3a4":"# ***Loading the news-headlines-dataset***","6fadbda5":"# ***Model Building with keras and Tensorflow***","10ea4937":"# THE TOP 100 MOST FREQUENTLY OCCURING WORDS IN THE NEWS HEADLINE DATA"}}