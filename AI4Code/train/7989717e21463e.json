{"cell_type":{"94ebe756":"code","400cccdf":"code","55842571":"code","93727c95":"code","2aa24053":"code","ba1c0083":"code","f337e0f3":"code","a1e7ec11":"code","266b439a":"code","80ad9a44":"code","6579a87d":"code","dc5c17d4":"code","b949f002":"markdown","b3d75d52":"markdown","747b6c77":"markdown","61ad759d":"markdown","a75ed86a":"markdown","7de86e72":"markdown","b0229db2":"markdown","1d7dfa69":"markdown","6559b136":"markdown","a0b21b7a":"markdown","6e71803a":"markdown"},"source":{"94ebe756":"import os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O\nimport datetime","400cccdf":"path = '..\/input\/ashrae-energy-prediction'\n\nfor dirname, _, filenames in os.walk(path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55842571":"def reduce_mem(df):\n    result = df.copy()\n    for col in result.columns:\n        col_data = result[col]\n        dn = col_data.dtype.name\n        if not dn.startswith(\"datetime\"):\n            if dn == \"object\":  # only object feature has low cardinality\n                result[col] = pd.to_numeric(col_data.astype(\"category\").cat.codes, downcast=\"unsigned\")\n            elif dn.startswith(\"int\") | dn.startswith(\"uint\"):\n                if col_data.min() >= 0:\n                    result[col] = pd.to_numeric(col_data, downcast=\"unsigned\")\n                else:\n                    result[col] = pd.to_numeric(col_data, downcast='integer')\n            else:\n                result[col] = pd.to_numeric(col_data, downcast='float')\n    return result\n\ndef add_lag_features(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n    return weather_df\n\ndef _delete_bad_sitezero(X, y):\n    cond = (X.timestamp > '2016-05-20') | (X.site_id != 0) | (X.meter != 0)\n    X = X[cond]\n    y = y.reindex_like(X)\n    return X.reset_index(drop=True), y.reset_index(drop=True)\n\ndef _extract_temporal(X, train=True):\n    X['hour'] = X.timestamp.dt.hour\n    X['weekday'] = X.timestamp.dt.weekday\n    if train:\n        # include month to create validation set, to be deleted before training\n        X['month'] = X.timestamp.dt.month \n    # month and year cause overfit, could try other (holiday, business, etc.)\n    return reduce_mem(X)","93727c95":"def load_data(source='train'):\n    assert source in ['train','test']\n    df = pd.read_csv(f'{path}\/{source}.csv', parse_dates=['timestamp'])\n    return reduce_mem(df)\n\ndef load_building():\n    df = pd.read_csv(f'{path}\/building_metadata.csv').fillna(-1)\n    return reduce_mem(df)\n\ndef load_weather(source='train', fix_timezone=True, impute=True, add_lag=True):\n    assert source in ['train','test']\n    df = pd.read_csv(f'{path}\/weather_{source}.csv', parse_dates=['timestamp'])\n    if fix_timezone:\n        offsets = [5,0,9,6,8,0,6,6,5,7,8,6,0,7,6,6]\n        offset_map = {site: offset for site, offset in enumerate(offsets)}\n        df.timestamp = df.timestamp - pd.to_timedelta(df.site_id.map(offset_map), unit='h')\n    if impute:\n        site_dfs = []\n        for site in df.site_id.unique():\n            if source == 'train':\n                new_idx = pd.date_range(start='2016-1-1', end='2016-12-31-23', freq='H')\n            else:\n                new_idx = pd.date_range(start='2017-1-1', end='2018-12-31-23', freq='H')\n            site_df = df[df.site_id == site].set_index('timestamp').reindex(new_idx)\n            site_df.site_id = site\n            for col in [c for c in site_df.columns if c != 'site_id']:\n                site_df[col] = site_df[col].interpolate(limit_direction='both', method='linear')\n                site_df[col] = site_df[col].fillna(df[col].median())\n            site_dfs.append(site_df)\n        df = pd.concat(site_dfs)\n        df['timestamp'] = df.index\n        df = df.reset_index(drop=True)\n        \n    if add_lag:\n        df = add_lag_features(df, window=3)\n    \n    return reduce_mem(df)\n\ndef merged_dfs(source='train', fix_timezone=True, impute=True, add_lag=False):\n    df = load_data(source=source).merge(load_building(), on='building_id', how='left')\n    df = df.merge(load_weather(source=source, fix_timezone=fix_timezone, impute=impute, add_lag=add_lag),\n                 on=['site_id','timestamp'], how='left')\n    if source == 'train':\n        X = df.drop('meter_reading', axis=1)  \n        y = np.log1p(df.meter_reading)  # log-transform of target\n        return X, y\n    elif source == 'test':\n        return df","2aa24053":"X_train, y_train = merged_dfs(add_lag=False)\nX_train.head()","ba1c0083":"# preprocessing\nX_train, y_train = _delete_bad_sitezero(X_train, y_train)\nX_train = _extract_temporal(X_train)\n\n# remove timestamp and other unimportant features\nto_drop = ['timestamp','sea_level_pressure','wind_direction','wind_speed']\nX_train.drop(to_drop, axis=1, inplace=True)\n\ngc.collect()","f337e0f3":"df_train = pd.concat([X_train, y_train], axis=1)\n\ndel X_train, y_train\ngc.collect()\n\ndf_train.info()","a1e7ec11":"df_train.to_hdf('preprocessing_no_lag.h5', index=False, key='train')\ndel df_train","266b439a":"X_test = merged_dfs(source='test', add_lag=False)\nX_test = _extract_temporal(X_test, train=False)\nX_test.drop(columns=['timestamp']+to_drop, inplace=True)\ngc.collect()\n\nX_test.head()","80ad9a44":"X_test.info()","6579a87d":"X_test.to_hdf('preprocessing_no_lag.h5', index=False, key='test')","dc5c17d4":"X_test2 = pd.read_hdf('preprocessing_no_lag.h5','test')\nX_test2.info()","b949f002":"# Training data","b3d75d52":"# Utilities","747b6c77":"We are now ready to load `df_train` and `X_test` in any other kernel to start modelling! If you are interested, check out the kernel at:\n\n- https:\/\/www.kaggle.com\/michelezoccali\/ashrae-with-fast-ai-part-2","61ad759d":"Now we add the test DataFrame to the same HDF5 file.","a75ed86a":"We try without lag features first, as unsure if the TabularPandas instance can deal with the added features without adding enough overhead to fill the RAM.","7de86e72":"This kernel performs the preprocessing of the training and test data for future use. \n\nEven with careful memory management, some operations on the large ASHRAE datasets are too demanding. Let's spread the work across multiple notebooks. This kernel is thus part of the series which further includes:\n\n- https:\/\/www.kaggle.com\/michelezoccali\/ashrae-with-fast-ai-part-2 (training)\n- https:\/\/www.kaggle.com\/michelezoccali\/ashrae-with-fast-ai-part-3 (inference)\n\nNB: Kaggle allows you to reference the static output of a kernel from other instances, as long as you remember to commit the notebook and save the output.","b0229db2":"# Imports","1d7dfa69":"An important part of the preprocessing involves careful type downcasting for memory management. Saving to a csv file does not preserve dtypes, hence we save in HDF5 format. We can add multiple frames to the save HDF5 file as long as we specify a key for retrieval.","6559b136":"# ASHRAE with fast.ai, Part 1: Preprocessing","a0b21b7a":"# Test data","6e71803a":"Let us verify that HDF5 files did indeed preserve dtypes."}}