{"cell_type":{"5396940b":"code","0a81b1de":"code","370989d1":"code","783bee2f":"code","f678cba2":"code","522201af":"code","71babd41":"code","166c86f0":"code","2dc7d239":"code","804c7d12":"code","4f1681f8":"code","1748d185":"code","0aa7c36f":"code","845250c9":"code","d1836eb4":"code","133b95f5":"code","9258dbc4":"code","2020ea1b":"code","c31fa32e":"markdown","3efb4e62":"markdown","0f8c7491":"markdown","8432b8c0":"markdown","a8926e1d":"markdown","0f011cd1":"markdown","f6c49883":"markdown","1dac2f33":"markdown","a6f6e3c9":"markdown","37a4c71a":"markdown","508d9033":"markdown","f337cac9":"markdown","c4422862":"markdown","8adb4759":"markdown","8a8e9860":"markdown","5be25e2f":"markdown","23f66df2":"markdown","af5655a8":"markdown","7914bae1":"markdown","e37d3dfb":"markdown","1cfe0619":"markdown","549c625a":"markdown","bf06f35a":"markdown","1b9147a2":"markdown","311c89e9":"markdown","b38194f9":"markdown"},"source":{"5396940b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.io as pio\n\npio.templates.default = \"plotly_dark\"\n\nimport sklearn\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import GroupKFold\nfrom typing import List, Tuple\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(rows=1, cols=2)","0a81b1de":"PATH = '..\/input\/trends-assessment-prediction\/'\nfnc = pd.read_csv(PATH+'fnc.csv')\nts = pd.read_csv(PATH+'train_scores.csv')\nloading = pd.read_csv(PATH+'loading.csv')","370989d1":"id1 = fnc[\"Id\"]\nfnc = fnc.drop([\"Id\"], axis=1)\nfnc.head(3)","783bee2f":"ts.head(3)","f678cba2":"loading.head(3)","522201af":"import plotly.graph_objects as go\n\nx, y, z = np.array(ts[\"age\"]), np.array(ts[\"domain1_var1\"]), np.array(ts[\"domain1_var2\"])\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array\/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n)])\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","71babd41":"x, y, z = np.array(ts[\"age\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\n\nfig = go.Figure(data=[go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array\/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n)])\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","166c86f0":"from plotly.subplots import make_subplots\n\nx, y, z = np.array(ts[\"domain1_var1\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\nx2, y2, z2 = np.array(ts[\"domain1_var2\"]), np.array(ts[\"domain2_var1\"]), np.array(ts[\"domain2_var2\"])\n\nfig = make_subplots(rows=1, cols=1)\n\nfig.add_trace(go.Scatter3d(\n    x=x,\n    y=y,\n    z=z,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array\/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    )\n))\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","2dc7d239":"fig = make_subplots(rows=1, cols=1)\n\nfig.add_trace(go.Scatter3d(\n    x=x2,\n    y=y2,\n    z=z2,\n    mode='markers',\n    marker=dict(\n        size=12,\n        color=z,                # set color to an array\/list of desired values\n        colorscale='Viridis',   # choose a colorscale\n        opacity=0.8\n    ),\n))\n\n\n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","804c7d12":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)","4f1681f8":"pd.DataFrame(transformed_pca)","1748d185":"pca.explained_variance_ratio_","0aa7c36f":"from sklearn.decomposition import IncrementalPCA\npca = IncrementalPCA(n_components=5)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)","845250c9":"pd.DataFrame(transformed_pca)","d1836eb4":"pca.explained_variance_ratio_","133b95f5":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\nsvd.fit(fnc)\nx = pd.DataFrame(svd.fit_transform(fnc))\nx","9258dbc4":"skewValue = x.skew(axis=1)\nskewValue","2020ea1b":"from sklearn.decomposition import IncrementalPCA\npca = IncrementalPCA(n_components=5)\npca.fit(fnc)\ntransformed_pca = pca.fit_transform(fnc)\npd.DataFrame(transformed_pca).to_csv('PCAData.csv', index=False)","c31fa32e":"From the official page, the descriptions we get for each of the data files are as follows:\n\n+ **fnc.csv** (corresponding to neuron networks)\n```\n    + SCN - Sub-cortical Network\n    + ADN - Auditory Network\n    + SMN - Sensorimotor Network\n    + VSN - Visual Network\n    + CON - Cognitive-control Network    \n    + DMN - Default-mode Network\n    + CBN - Cerebellar Network\n```\n+ **loading.csv** (parts of the brain)\n```\n    + IC_01 - Cerebellum\n    + IC_07 - Precuneus+PCC\n    + IC_05 - Calcarine\n    + IC_16 - Middle Occipital?\n    + IC_26 - Inf+Mid Frontal\n    + IC_06 - Calcarine\n    + IC_10 - MTG\n    + IC_09 - IPL+AG\n    + IC_18 - Cerebellum\n    + IC_12 - SMA\n    + IC_24 - IPL+Postcentral\n    + IC_15 - STG\n    + IC_13 - Temporal Pole\n    + IC_17 - Cerebellum\n    + IC_02 - ACC+mpfc\n    + IC_08 - Frontal\n    + IC_03 - Caudate\n    + IC_21 - Temporal Pole + Cerebellum\n    + IC_28 - Calcarine\n    + IC_11 - Frontal\n    + IC_20 - MCC\n    + IC_30 - Inf Frontal\n    + IC_22 - Insula + Caudate\n    + IC_29 - MTG\n    + IC_14 - Temporal Pole + Fusiform\n```","3efb4e62":"### Truncated SVD","0f8c7491":"There is a very minimal difference between our first attempt at PCA and this one. Honestly, I didn't expect the difference to be too much between these two.","8432b8c0":"# <center>TRENDS: PCA and Dimensionality Reduction<\/center>\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1537731%2Fa5fdbe17ca91e6713d2880887232c81a%2FScreen%20Shot%202019-12-09%20at%2011.25.31%20AM.png?generation=1575920121028151&alt=media)\n\n---\n\n*Author: **Nikhil Praveen***","a8926e1d":"So, not off to a great start. Let's look at the other PCA methods.","0f011cd1":"One one hand, there is a lot of variance between components 0 and 1. What's the explained variance ratio?","f6c49883":"## Starting the EDA","1dac2f33":"Now let's look at the domain2 variables:","a6f6e3c9":"---\n\n# Please upvote if you liked this kernel!\n\n---","37a4c71a":"## Data description","508d9033":"In this competition, we are asked to predict multiple assessments as well as age from many different features given in the data. Let's look at our data files:","f337cac9":"## Table of contents\n\n* 1. Introduction to the data\n```\n    + Imports\n    + Data description\n```\n* 2. Overview of Methods\n* 3. Simple EDA\n   ```\n   + Starting the EDA\n   + Some fancy Plotly visuals\n   + Dimensionality reduction techniques\n       + PCA\n           + Normal PCA\n           + Incremental PCA\n           + Kernel PCA\n       + TruncatedSVD\n       + NMF\n       + Selection\n   + What to infer?\n   ```","c4422862":"# 1. Introduction","8adb4759":"## Dimensionality reduction\n\nThe main one we have to reduce is the data from `fnc.csv` as well as the data from `loading.csv`. For this purpose, we shall try PCA and TruncatedSVD on this dataset and pick which one is better.","8a8e9860":"What about the relation between domain1 and domain2 variables? That would be interesting.","5be25e2f":"## Fancy Plotly\n\nIf we want to look at building models, we will have to look at fancy Plotly instead of good old-fashioned Seaborn and Matplotlib.\n\nOur first 3d plot explores the relationships between `age` and the two `domain1_var` variables.","23f66df2":"# 2. Overview of methods\n\nIn this notebook, I will use the following methods:\n* **Dimensionality reduction**:\n    We have high dimensionality here, so I will try to apply: \n        * PCA\n        * TruncatedSVD\n        \n*NOTE: THE FOLLOWING WILL COME IN A LATER KERNEL.*\n* **Feature maker**<br>\n    I shall use sklearn's `BaseEstimator` class and add some dummy methods to make it work in the pipeline. Here's some sample code for the feature maker:\n    ```\n    class FeatureMaker(BaseEstimator, TransformerMixin):\n        def __init__(self, df):\n            '''And so on and so forth and what have'''\n        def fit(self):\n            pass\n        def transform():\n            # and so on and so forth\n    ```\n* **Imputer**<br>\n    After performing the feature engineering with our simple function, the Imputer from `sklearn` will be able to handle the many missing values.\n* **K-Fold**<br>\n    It is pretty obvious why one would use CV in a Kaggle competition.\n* **HistGradient Boosting Regressor**<br>\n    OK, this is an experimental `sklearn` regressor which I have decided to attempt in this competition.","af5655a8":"#### 1. Normal PCA","7914bae1":"It seems like Incremental PCA is the best choice here for dimensionality reduction. Let's perform PCA on the full dataset:","e37d3dfb":"It seems that the first component is carrying most of the weight in both PCA and SVD. This leads to the absence of a well-balanced dataset. Let's check for skew:","1cfe0619":"# 3. Simple EDA","549c625a":"First, we look at the dataset:","bf06f35a":"#### 2. Incremental PCA","1b9147a2":"### PCA","311c89e9":"There is virtually **no variance** between these PCA and IncrementalPCA. Looks like it's time to move on to our good old friend TruncatedSVD.","b38194f9":"## Imports"}}