{"cell_type":{"38ae52e9":"code","537bd629":"code","f10ec3a7":"code","988085c2":"code","ba40f5a2":"code","9c77da08":"code","769c033e":"code","8e4b7989":"code","dee6cd17":"code","6b3458bf":"code","42165d79":"code","b7a41e56":"code","6d45936d":"code","96bcdae6":"code","fc7dd3d7":"code","4a3c3b6a":"code","0f1f1fcf":"code","e305ef17":"code","c239df06":"code","0ec57856":"code","568cdfb4":"code","2b463ffa":"code","bb2e4dee":"code","ccff92ce":"code","3aaa0ce9":"code","d257ef46":"code","b02dce04":"code","da59e64d":"markdown","0c256267":"markdown","e0984b9c":"markdown","4ec41085":"markdown","a7f9a04d":"markdown","f8133d00":"markdown","365c66a9":"markdown","ada66387":"markdown","66e28809":"markdown","de9c7013":"markdown","078ef92c":"markdown","07cb2545":"markdown","ce0a74ee":"markdown","0af558b2":"markdown","ed37f215":"markdown","20b273d5":"markdown","e656ac9c":"markdown","4d0f8c37":"markdown","fcf0cc55":"markdown","cc7b6bcc":"markdown","2450de98":"markdown","1eb90bab":"markdown","24802a07":"markdown","4a595bdf":"markdown","65737257":"markdown","082f2cc5":"markdown","55662531":"markdown","3318d4ac":"markdown","9ae2f8cc":"markdown"},"source":{"38ae52e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# scipy.special for sigmoid function\nimport scipy.special\n\n# visualizations\nimport matplotlib.pyplot\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","537bd629":"# file imports\ndf_mnist_test = pd.read_csv('..\/input\/mnist-in-csv\/mnist_test.csv')\ndf_mnist_train = pd.read_csv('..\/input\/mnist-in-csv\/mnist_train.csv')\n\n\n# load the mnist train data CSV into a list\ntraining_data_file = open('..\/input\/mnist-in-csv\/mnist_train.csv','r')\ntraining_data_list = training_data_file.readlines()\ntraining_data_file.close()\n\n\n# load the mnist test data CSV file into a list\ntest_data_file = open(\"..\/input\/mnist-in-csv\/mnist_test.csv\", 'r')\ntest_data_list = test_data_file.readlines()\ntest_data_file.close()\n","f10ec3a7":"df_mnist_test","988085c2":"print(\"Length of the train set: \" + str(len(df_mnist_train)))\nprint(\"Length of the test set: \" + str(len(df_mnist_test)))\n","ba40f5a2":"all_values[:]","9c77da08":"# split the CSV-file values\nall_values = training_data_list[1].split(',')\n\n# reshape the comma seperated values into an 28 x 28 array\nimage_array = np.asfarray(all_values[1:]).reshape((28,28))\n\nprint(image_array)\n\n","769c033e":"# visualize this 28x28 reshaped array\nmatplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')","8e4b7989":"# scaling of the initial array values to reduce zero calculations inside the NN.\nscaled_input = (np.asfarray(all_values[1:]) \/ 255.0 * 0.99) + 0.01\nprint(scaled_input)","dee6cd17":"# neural network class definition\nclass neuralNetwork:\n    \n    # initialise the neural network\n    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate):\n        # set number of nodes in each layer\n        self.inodes = inputnodes\n        self.hnodes = hiddennodes\n        self.onodes = outputnodes\n\n        # link weight matrices, \n        #      wih and who\n        \n        # 1: mean value of the normal distribution - 0.0\n        # 2: standard deviation - based on the root of nodes of the upcomming layer ->\n        #     pow(self.hnodes, -0.5) --- exponent -0.5 is equal to root of \n        # 3: last param builds the grid of the array (self.hnodes, self.inodes)\n        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n\n        # learning rate \n        self.lr = learningrate\n\n        # activation function - sigmoid function\n        self.activation_function = lambda x: scipy.special.expit(x)\n        \n        pass\n    \n    #train the neural network\n    def train(self, inputs_list, targets_list):\n        # convert inputs list to 2d array\n        inputs = np.array(inputs_list, ndmin=2).T\n        targets = np.array(targets_list, ndmin=2).T\n        \n        # calculate signals into hindden layer\n        hidden_inputs = np.dot(self.wih, inputs)\n        # calculate the signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # calculate signals into final output layer\n        final_inputs = np.dot(self.who, hidden_outputs)\n        # calculate the signals emerging form final output layer\n        final_outputs = self.activation_function(final_inputs)\n        \n        \n        # BACKPROPAGATION #\n        \n        # error is the (target - actual)\n        output_errors = targets - final_outputs\n        \n        # hidden layer error is the output_error, split by weights, recombined at hidden nodes\n        hidden_errors = np.dot(self.who.T, output_errors) \n        \n        # update the weights for the links between the hidden and output layers\n        self.who += self.lr * np.dot((output_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n        \n        # update the weights for the links between the input and hidden layers\n        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n        \n        pass\n    \n    #query the neural network\n    def query(self, inputs_list):\n        # convert input list to 2d array\n        inputs = np.array(inputs_list, ndmin=2).T\n        \n        # calcuclate signals into hidden layer\n        hidden_inputs = np.dot(self.wih, inputs)\n        # calculate signals emerging from hidden layer\n        hidden_outputs = self.activation_function(hidden_inputs)\n        \n        # calculate signals  into final output layer\n        final_inputs = np.dot(self.who, hidden_outputs)\n        # calculate signals emerging from final output layer\n        final_outputs = self.activation_function(final_inputs)\n        \n        return final_outputs\n        ","6b3458bf":"# number of nodes\ninput_nodes = 784\nhidden_nodes = 100\noutput_nodes = 10\n\n# learning rate with 0.1\nlearning_rate = 0.1\n\n# create an instance of neuralnetwork\nn = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)","42165d79":"# train the neural network\n\n# epochs -> the number of times the training data set is used for training\nepochs = 5\n\nfor e in range(epochs):\n    # go through all records in the training data set\n    for record in training_data_list[1:]:\n\n        # split the record by the ',' commas\n        all_values = record.split(',')\n\n        # scale and shift the inputs\n        inputs = (np.asfarray(all_values[1:]) \/ 255.0 * 0.99) + 0.01\n\n        # create the target output values, \n        # -> an array out of output_nodes elements (all will receive the values 0.01, ...\n        targets = np.zeros(output_nodes) + 0.01\n\n        # ... except the desired label which will be set to 0.99 here) \n        # -> all_values[0] is the target label for this record.\n        targets[int(all_values[0])] = 0.99\n\n        n.train(inputs, targets)\n\n        pass\n    pass\n","b7a41e56":"# What is Inside the Training Elements from the Training Code up there?!\n# This part here is just for visualizing the values insides the single elements\n# of the calculation up there, to get a better understanding what is happening in the code.\nprint ( \"output_nodes                   ->    \", output_nodes)  \nprint ( \"np.zeros(output_nodes) + 0.01  ->    \", np.zeros(output_nodes) + 0.01)  \nprint ( \"targets[int(all_values[0])]    ->    \", targets[int(all_values[0])])\nprint ( \"int(all_values[0])             ->    \", int(all_values[0]) )  \n","6d45936d":"# get the first test record\nall_values = test_data_list[1].split(',')\nprint(\"Testrecord for handwritten number: \" + str(all_values[0]))","96bcdae6":"image_array = np.asfarray(all_values[1:]).reshape((28,28))\n\nmatplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation = 'None')","fc7dd3d7":"n.query((np.asfarray(all_values[1:]) \/ 255.0 * 0.99) + 0.01)","4a3c3b6a":"# test the neural network \n\n# scorecard for how wel the network performs, initially empty\nscorecard = []\n\n# go through all the records in the test data set\nfor record in test_data_list[1:]:\n    # split the record by the ',' commas\n    all_values =  record.split(',')\n    \n    # correct answer is first value\n    correct_label = int(all_values[0])\n    # print(correct_label, \"correct label\")\n    \n    # scale and shift the inputs\n    inputs =  (np.asfarray(all_values[1:]) \/ 255.0 * 0.99) + 0.01\n    \n    # query the network\n    outputs = n.query(inputs)\n    \n    # the index of the highest value corresponds to the label\n    label = np.argmax(outputs)\n    # print(label, \"network's answer\")\n     \n    # append correct or incorrect to list\n    if (label == correct_label):\n        \n        # network's answer matches correct answer, add 1 to scorecard \n        scorecard.append(1)\n    \n    else:\n        # network's answer does not match correct answer, add 0 to scorecard\n        scorecard.append(0)\n    \n    pass","0f1f1fcf":"# calculate the performance score, the fraction of correct answers \nscorecard_array = np.asfarray(scorecard)\nprint(\"performance = \", scorecard_array.sum() \/ scorecard_array.size)","e305ef17":"import scipy.misc\nimport imageio\nimport glob","c239df06":"own_handwritten_digits = []\n\n\nfor image_file_name in glob.glob('..\/input\/own-handwritten-digits\/*.png'):\n    label = int(image_file_name[-5:-4])\n        \n    # read picture file\n    img_array =  imageio.imread(image_file_name, as_gray = True)\n    \n    # print(label)\n    \n    # the mnist-dataset pictures are stored in a contrariwise greyscale way which means 0 is white and 255 is black\n    # and not as usual 0 is black and 255 is white. This leads us to the subtraction of 255 -> we make the uploaded picture equal to the rest of mnist-digits\n    img_data = 255.0 - img_array.reshape(784)\n       \n    # rescaling the image pixels betwenn 0.01 and 1.0\n    img_data = (img_data \/ 255.0 * 0.99) + 0.01\n\n    record = np.append(label, img_data)\n    # print(record)\n    own_handwritten_digits.append(record)\n    pass","0ec57856":"# visualizing handwritten digit\nmatplotlib.pyplot.imshow(own_handwritten_digits[0][1:].reshape(28,28), cmap='Greys', interpolation = 'None')","568cdfb4":"# result\nnp.argmax(n.query(own_handwritten_digits[0][1:]))","2b463ffa":"# visualizing handwritten digit\nmatplotlib.pyplot.imshow(own_handwritten_digits[1][1:].reshape(28,28), cmap='Greys', interpolation = 'None')","bb2e4dee":"# result\nnp.argmax(n.query(own_handwritten_digits[1][1:]))","ccff92ce":"# visualizing handwritten digit\nmatplotlib.pyplot.imshow(own_handwritten_digits[3][1:].reshape(28,28), cmap='Greys', interpolation = 'None')","3aaa0ce9":"# result\nnp.argmax(n.query(own_handwritten_digits[3][1:]))","d257ef46":"# visualizing first handwritten digit\nmatplotlib.pyplot.imshow(own_handwritten_digits[2][1:].reshape(28,28), cmap='Greys', interpolation = 'None')","b02dce04":"# result\nnp.argmax(n.query(own_handwritten_digits[2][1:]))","da59e64d":"### Imports","0c256267":"### Whats is Inside the Code Snippet","e0984b9c":"### Test on own data\nIn this chapter, I will visualize the digits first and query the neural networks on it afterward.","4ec41085":"### Visualization of the 28x28-Array \"Picture\"","a7f9a04d":"## Training\nThis chapter is about train the neural network via several executions or epochs. ","f8133d00":"#### Digit 6","365c66a9":"# Introduction\nThe MNIST-Database is a large dataset of 70.000 handwritten digits that are mainly used for training several image processing systems, especially in the deeplearning field.\n\nThe dataset consists digits from 0 to 9 in a 28x28 pixel based format. The values are stored in a comma separted file in which each row has 785 values (28x28 = 784) where every row represents a different number that is initially displayed in the first value of the row (784 + 1). \n\nIf you are interested in the MNIST-Dataset more, you will find detailed information on Yann LeCun's website [here](http:\/\/yann.lecun.com\/exdb\/mnist\/). Next to the dataset explanation you will find a list of benchmarks for different classifiers and pre-processing approaches that were used with this dataset.\n\nThis work here is based on the very good explanation by Tariq Rashid about neural networks, so thank you for that great work.\n","ada66387":"## Neural Network Class Definition","66e28809":"#### Digit 8","de9c7013":"### Testrecord Visualization","078ef92c":"### Neural Network's Testrecord Result\nThe result shows an array with 10 values where the highest value represents the predicted digit by corresponding array index. The array starts with 0 and ends with 9. \nThe highest output value in this case is the eighth value which is the index with the number 7. The neural network's result of our testrecord seems to be correct. ","07cb2545":"## Initialize Neural Network\n##### Input Nodes\nThe input nodes are based on the 28x28 pixel input for every digit \"picture\" which makes in total 784 pixel input nodes.\n\n##### Hidden Nodes\nThe number of the hidden nodes here where chosen randomly. They should not be too high or higher than the input nodes number, as we want to make the network to concentrate the values and find patterns in  inderlying levels of the input values. Just like that we do not want to choose a number too low, to avoid reducing the neural network's ability to find any feature or pattern in the input. With less hidden nodes the neural network would not be capable of creating an own understanding of the MNIST-dataset.\n\n##### Output Nodes\nThe output nodes represent our targets we want to determine, the digits from 0 to 9 (10 values).\n\n##### Learning Rate \nThe learning rate is for fine-tuning and adjusting the performance of the neural network's pattern recognition and its overall accuracy. This part here is where try and error takes place.","ce0a74ee":"#### Result - Digit 7","0af558b2":"#### Digit 5","ed37f215":"# Neural Network - Detection of Handwritten Characters\nIn this chapter there will be the class definition for the neural network object as well as its initialization, training and testing.","20b273d5":"## Test on own handwritten digits\nIn this chapter I uploaded some handwritten digits to test the neural network on unknown, really new data. I created (wrote) four numbers with different interferences like a shadowed background or broken lines in the digits sign, which shall be identified correctly by the neural network. I am very interested in the results. \nLet' take a look.\n","e656ac9c":"## Visualize the Number(s)\nAs previously mentioned the MNIST-Dataset consists 28x28 pixel based information of digits, that are rowbased stored in a comma separated file (in this example). Therefore we need to split the values and bring them into shapes of 28x28-array to visualize and work with the \"pictures\".\n\nEvery field of the 28x28-array will represent a number value that is correspondent to a grey scaled color between 0 and 255.\n\nThe following will show you a reshaped(28x28) and visualized row from the CSV-file.","4d0f8c37":"#### Result - Digit 8","fcf0cc55":"#### Digit 7","cc7b6bcc":"### Reshaped Input","2450de98":"#### Result - Digit 6","1eb90bab":"### Test with Scorecard\nThis scorecard will show all comparison between the correct values and the predicted ones.","24802a07":"### Neural Network's Performance","4a595bdf":"## Testing\nFirst there will be a comparison between the trained neural network's result and the correct answer based on a single record. After that there will be the scorecard based testing where all results are compared with the correct values from the dataset. This will lead to the neural network's accuracy performance.","65737257":"### Conclusion on Handwritten Digit Detection\nWell this worked ok. The digit 7 and 8 were identified correctly. Digit 6 was very close with the result 5 but digit 5 instead was identified as an 8. It looks quite after an 8 according to the grey striped background which probably lead the neural network to that decision. The grey striped background in the digit 8 sign didn't affected the neural network in its decision, so I guess there is still a need to train the NN more :D \n\nI will work on this.\n\nThanks for reading\/visiting.\n","082f2cc5":"### Import Handwritten Digits","55662531":"In the following code snippet we are scaling of the initial values to avoid 0 and 1 calculations inside the NN.","3318d4ac":"#### Result - Digit 5","9ae2f8cc":"### Test and Training Dataset Volume"}}