{"cell_type":{"ca3a46c2":"code","867f69fb":"code","e160317d":"code","100cda83":"code","05804fbf":"code","25e35bf5":"code","f16d71da":"code","76f53984":"code","e23ef615":"code","0350a68c":"code","3be50d40":"code","db076719":"code","293791e4":"code","d9e7651e":"code","0323f5c1":"code","98ac1284":"code","2a18e43e":"code","db72885d":"code","096c36be":"code","2f9a07b0":"code","d3e91ee1":"code","caba76e1":"markdown","0ef9b427":"markdown","baa0e067":"markdown","23b1ac97":"markdown","1e2d9a32":"markdown","4b59d75f":"markdown","37f0a0bf":"markdown","1c68d787":"markdown","62a5d4bc":"markdown","af5fc674":"markdown","079d27dc":"markdown","f93183a5":"markdown","51cc5247":"markdown","57a6efbe":"markdown","65060353":"markdown","46c1f575":"markdown","e8b2bb8b":"markdown","1794daa1":"markdown","e56b483f":"markdown","b8639e13":"markdown","890d1b67":"markdown","b414b139":"markdown","64911e5a":"markdown","56446b58":"markdown","0da34771":"markdown","77c24db3":"markdown","76fb302d":"markdown","daec81e1":"markdown","945e564a":"markdown","5048e25f":"markdown","9d141ab7":"markdown","e602dbbf":"markdown","37dedf45":"markdown","d4ff587a":"markdown","b656d344":"markdown","b9d9b69e":"markdown","d0d7fef4":"markdown","de4ef7f5":"markdown","6040c931":"markdown","2fd5caf3":"markdown","29da49d6":"markdown","c59b01b1":"markdown","705499d4":"markdown","eb51992d":"markdown","4fa68091":"markdown","2ba71a0c":"markdown","3d49066f":"markdown","a217b528":"markdown","6c7ba3e7":"markdown","c573d176":"markdown","7846852b":"markdown","20b134bf":"markdown"},"source":{"ca3a46c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","867f69fb":"## Loading In Data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head() #visualize data\ntrain_data.head() #visualize data\n","e160317d":"women = train_data.loc[train_data.Sex == \"female\"][\"Survived\"]\nrate_women = sum(women)\/len(women) #getting women rate\nprint(\"The rate of women that survived: \", round(rate_women*100, 2), \"%\")","100cda83":"men = train_data.loc[train_data.Sex == \"male\"][\"Survived\"]\nrate_men = sum(men)\/len(men) #getting women rate\nprint(\"The rate of men that survived: \", round(rate_men*100, 2), \"%\")","05804fbf":"#Create a graph that compares these rates\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nsex = [\"Women\", \"Men\"]\nrates = [rate_women, rate_men]\nax.bar(sex, rates)\nplt.ylabel(\"Survival Rate\")\nplt.xlabel(\"Sex of the Passenger\")\nplt.title(\"Survival Rates Between Women and Men\")\n\n","25e35bf5":"#we will be using the random forest model \nfrom sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"] # pulling the output of training data set\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"] # selecting our features\nX = pd.get_dummies(train_data[features]) # pulling out the input data we will be training on\nX_test = pd.get_dummies(test_data[features]) # pulling out the input data we will be using to test\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) # bulding the machine learning model\nmodel.fit(X, y) # training the model\npredictions = model.predict(X_test) # creating a prediction based on our test inputs\noutput = pd.DataFrame({\"PassenderId\" : test_data.PassengerId, \"Survived\" : predictions}) # storing prediction in table\noutput.to_csv(\"my_sub.csv\", index = False) # putting it into a csv file for submission ","f16d71da":"import matplotlib.pyplot as plt\n%matplotlib inline\n#lets look at age\n#infant 0-6  \n#children 7-11 (my roommates)\n#teenager 12-19\n#Rohit, Trenton, Lauren 20-40 (not so old people)\n#Heath 41-60 (old people)\n#elderly 61+\n\nage = train_data.loc[train_data.Age > 0][\"Survived\"]\n\ninfant = age.loc[train_data.Age < 7]\nchildren = age.loc[train_data.Age.between(7, 11)]\nteenager = age.loc[train_data.Age.between(12, 19)]\nadult = age.loc[train_data.Age.between(20, 40)]\nmiddleage = age.loc[train_data.Age.between(41, 60)]\nelderly = age.loc[train_data.Age > 61]\n\n#survival rates\nrate_infant = sum(infant)\/len(infant)\nrate_children = sum(children)\/len(children)\nrate_teenager = sum(teenager)\/len(teenager)\nrate_adult = sum(adult)\/len(adult)\nrate_middleage = sum(middleage)\/len(middleage)\nrate_elderly = sum(elderly)\/len(middleage)\nprint(\"The rate of infants that survived: \", round(rate_infant*100, 2), \"%\")\nprint(\"The rate of children that survived: \", round(rate_children*100, 2), \"%\")\nprint(\"The rate of teenager that survived: \", round(rate_teenager*100, 2), \"%\")\nprint(\"The rate of adult that survived: \", round(rate_adult*100, 2), \"%\")\nprint(\"The rate of middleage that survived: \", round(rate_middleage*100, 2), \"%\")\nprint(\"The rate of elderly that survived: \", round(rate_elderly*100, 2), \"%\")\n\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nage_range = [\"Infant\", \"Children\", \"Teenager\", \"Adult\", \"Middle Age\", \"Elderly\"]\nrates = [rate_infant, rate_children, rate_teenager, rate_adult, rate_middleage, rate_elderly]\nax.bar(age_range, rates)\nplt.ylabel(\"Survival Rate\")\nplt.xlabel(\"Age Ranges\")\nplt.title(\"Survival Rate Among Age Ranges\")\n","76f53984":"#clean cells where Age is empty\n#use Imputation method from the Intermiediate Machine Learning Course to fill in the mising Age data\n\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\nage_train = train_data.drop(['Name', 'Cabin', 'Embarked', 'Ticket', 'Sex'], axis = 1)\n#age_train.head()\nimputed_age_train = pd.DataFrame(my_imputer.fit_transform(age_train))\n\n#imputation removed column names, put them back\nimputed_age_train.columns = age_train.columns\n#imputed_age_train.head(15)\n\n#replace missing ages in training data with our imputed ages\ntrain_data['Age'] = imputed_age_train['Age']\n#train_data.head()\ntrain_data.to_csv(\"testout.csv\", index = False)\n#output.to_csv(\"my_sub.csv\", index = False) # putting it into a csv file for submission \n\n\n#now we will fill in the test data to match, this may be considered baking the data, but this does not \n#determine the actual results of if a passenger survived or not\n\nmy_imputer2 = SimpleImputer()\nage_test = test_data.drop(['Name', 'Cabin', 'Embarked', 'Ticket', 'Sex'], axis = 1)\nimputed_age_test = pd.DataFrame(my_imputer.fit_transform(age_test))\n#then we change the test data\nimputed_age_test.columns = age_test.columns\n#imputed_age_test.head(15)\ntest_data['Age'] = imputed_age_test['Age']\n#test_data.head(15)","e23ef615":"#gather data on fares\n#age = train_data.loc[train_data.Pclass > 0][\"Survived\"]\nfirst_class = train_data.loc[train_data.Pclass == 1][\"Survived\"]\nsecond_class = train_data.loc[train_data.Pclass == 2][\"Survived\"]\nthird_class = train_data.loc[train_data.Pclass == 3][\"Survived\"]\n\n\nfirst_class.describe()\n#216 passengers bought first-class tickets\n#184 passengers bought second-class tickets\n#491 passengers bought third-class tickets\n\n#survival rates of each class:\nrate_first = sum(first_class)\/len(first_class)\nrate_second = sum(second_class)\/len(second_class)\nrate_third = sum(third_class)\/len(third_class)\n\nprint(\"The rate of first-class passengers that survived: \", round(rate_first*100, 2), \"%\")\nprint(\"The rate of second-class passengers that survived: \", round(rate_second*100, 2), \"%\")\nprint(\"The rate of third-class passengers that survived: \", round(rate_third*100, 2), \"%\")\n\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nclasses = [\"First-Class\", \"Second-Class\", \"Third-Class\"]\nrates = [rate_first, rate_second, rate_third]\nax.bar(classes, rates)\nplt.ylabel(\"Survival Rate\")\nplt.xlabel(\"Ticket Classes\")\nplt.title(\"Survival Rate Among Ticket-Classes\")\n","0350a68c":"#Add a Relatives column to Train File\nSibSp_data = train_data[\"SibSp\"]\nParch_data = train_data[\"Parch\"]\nnumRelatives = SibSp_data + Parch_data\ntrain_data[\"Relatives\"]= numRelatives\n#train_data.head(15)\n\n#Add a Relatives column to Test File\nSibSp_data1 = test_data[\"SibSp\"]\nParch_data1 = test_data[\"Parch\"]\nnumRelatives1 = SibSp_data1 + Parch_data1\ntest_data[\"Relatives\"]= numRelatives1\n#test_data.head(15)\n\n#Plot data to analyze trends among relatives\nsolo = train_data.loc[train_data.Relatives == 0][\"Survived\"]\none = train_data.loc[train_data.Relatives == 1][\"Survived\"]\ntwo = train_data.loc[train_data.Relatives == 2][\"Survived\"]\nthree = train_data.loc[train_data.Relatives == 3][\"Survived\"]\nfour = train_data.loc[train_data.Relatives == 4][\"Survived\"]\nfive = train_data.loc[train_data.Relatives == 5][\"Survived\"]\nsix = train_data.loc[train_data.Relatives == 6][\"Survived\"]\nseven = train_data.loc[train_data.Relatives == 7][\"Survived\"]\neight = train_data.loc[train_data.Relatives == 8][\"Survived\"]\nnine = train_data.loc[train_data.Relatives == 9][\"Survived\"]\nten = train_data.loc[train_data.Relatives == 10][\"Survived\"]\n\n#survival rates of each Relative class:\nrate_solo = sum(solo)\/len(solo)\nrate_one = sum(one)\/len(one)\nrate_two = sum(two)\/len(two)\nrate_three = sum(three)\/len(three)\nrate_four = sum(four)\/len(four)\nrate_five = sum(five)\/len(five)\nrate_six = sum(six)\/len(six)\nrate_seven = sum(seven)\/len(seven)\n#rate_eight = sum(eight)\/len(eight)\n#rate_nine = sum(nine)\/len(nine)\nrate_ten = sum(ten)\/len(ten)\n\n#there were no passengers with eight or nine relatvies on board, we we cut them from our graphical display\n\n#plot analysis and make conclusions\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nclasses = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\" , \"7\", \"10\"]\nrates = [rate_solo, rate_one, rate_two, rate_three, rate_four, rate_five, rate_six, rate_seven, rate_ten]\nax.bar(classes, rates)\nplt.ylabel(\"Survival Rate\")\nplt.xlabel(\"Number of Relatives\")\nplt.title(\"Survival Rate Between Passengers and their Number of Relatives On Board\")\n","3be50d40":"from sklearn.tree import DecisionTreeClassifier\nimport numpy\n#We need to build the data set we want to use. This will be selecting the features we want to use\n\n#setting up data\ny_dec = train_data[\"Survived\"]\ndec_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_dec = pd.get_dummies(train_data[dec_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\ny_dec.head(15)\n# now we will build the model\ndec_model = DecisionTreeClassifier(random_state = 1)\ndec_model.fit(X_dec, y_dec)\ny_dec.to_csv(\"y_dec.csv\", index = False)\n\n# getting testing data\nX_test = pd.get_dummies(test_data[dec_features]) # pulling out the input data we will be using to test\nX_test.to_csv(\"X_test.csv\", index = False)\n#now we will predict the data\ndec_predictions = dec_model.predict(X_test)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data = pd.DataFrame(dec_predictions)\n\nprediction_data.columns = [\"Survived\"]\n#prediction_data[\"Survived\"] = numpy.around(prediction_data[[\"Survived\"]], 0)\nprediction_data.head(15)\ndec_predictions = prediction_data[\"Survived\"]\n\n\noutput_dec = pd.DataFrame({\"PassengerId\" : test_data.PassengerId, \"Survived\" : dec_predictions}) \noutput_dec.to_csv(\"Dec_Submission.csv\", index = False)\n","db076719":"from sklearn.ensemble import RandomForestClassifier\nimport numpy\n#We need to build the data set we want to use. This will be selecting the features we want to use\n\n#setting up data\ny_for = train_data[\"Survived\"]\nfor_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_for = pd.get_dummies(train_data[for_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\ny_for.head(15)\n# now we will build the model\nfor_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nfor_model.fit(X_for, y_for)\ny_for.to_csv(\"y_for.csv\", index = False)\n\n# getting testing data\nX_test = pd.get_dummies(test_data[for_features]) # pulling out the input data we will be using to test\nX_test.to_csv(\"X_test.csv\", index = False)\n#now we will predict the data\nfor_predictions = for_model.predict(X_test)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data_for = pd.DataFrame(for_predictions)\nprediction_data_for.columns = [\"Survived\"]\n#prediction_data_for[\"Survived\"] = numpy.around(prediction_data_for[[\"Survived\"]], 0)\nprediction_data.head(15)\nfor_predictions = prediction_data_for[\"Survived\"]\n\n\noutput_for = pd.DataFrame({\"PassengerId\" : test_data.PassengerId, \"Survived\" : for_predictions}) \noutput_for.to_csv(\"For_Submission.csv\", index = False)","293791e4":"from sklearn.neural_network import MLPClassifier\n\nimport numpy\n#We need to build the data set we want to use. This will be selecting the features we want to use\n\n#setting up data\ny_MLP = train_data[\"Survived\"]\nMLP_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_MLP = pd.get_dummies(train_data[MLP_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\ny_MLP.head(15)\n# now we will build the model\nMLP_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,), random_state=1,max_iter=400, )\nMLP_model.fit(X_MLP, y_MLP)\ny_MLP.to_csv(\"y_for.csv\", index = False)\n\n# getting testing data\nX_test = pd.get_dummies(test_data[MLP_features]) # pulling out the input data we will be using to test\nX_test.to_csv(\"X_test.csv\", index = False)\n#now we will predict the data\nMLP_predictions = MLP_model.predict(X_test)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data_MLP = pd.DataFrame(MLP_predictions)\nprediction_data_MLP.columns = [\"Survived\"]\n#prediction_data_for[\"Survived\"] = numpy.around(prediction_data_for[[\"Survived\"]], 0)\nprediction_data.head(15)\nMLP_predictions = prediction_data_MLP[\"Survived\"]\n\n\noutput_MLP = pd.DataFrame({\"PassengerId\" : test_data.PassengerId, \"Survived\" : MLP_predictions}) \noutput_MLP.to_csv(\"MLP_Submission.csv\", index = False)","d9e7651e":"# setting up testing rig\n\n#splitting data\ntemp = np.array_split(train_data, 2)\n#organizing data\ntrain = temp[0]\ntest = temp[1]\n\n#splitting up training data\ny = train[\"Survived\"] # pulling the output of training data set \nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"Relatives\"] # selecting our features \nX = pd.get_dummies(train[features]) # pulling out the input data we will be training on\n\n\n#now we need a way to determine accuracy \n#for simplicly we will just look direclty at the output of the model vs the test survived or not survived array\n\ndef accuracyTest(prediction, true):\n    ### these will be pandas of the same size\n    prediction_temp = pd.DataFrame(prediction)\n    true_temp = pd.DataFrame(true)\n    \n    if len(prediction) != len(true):\n        print(\"Arrays are not same length!\")\n        return\n    \n    # reset indcies to guarantee compare\n    prediction_temp = prediction_temp.reset_index().drop([\"index\"], axis=1)\n    true_temp =  true_temp.reset_index().drop([\"index\"], axis=1)\n    \n    dif = prediction_temp.compare(true_temp);\n    percent = len(dif)\/len(true_temp)\n    print(\"Accuracy is: \", (1-percent))\n    return 1-percent\n    \n#Testing function \nmock_prediction =  pd.DataFrame(np.array([[1], [0], [1]]))\nmock_true = pd.DataFrame(np.array([[1], [1], [1]]))\naccuracyTest(mock_prediction, mock_true) #looks great! It should work with our data if there is only a survived column \n\n\n\n","0323f5c1":"#now lets try with our MLP model \n#lets try it with our previous predictor \n\n\n#setting up data\ny_MLP = train[\"Survived\"]\nMLP_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_MLP = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\n\n# now we will build the model\nMLP_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,), random_state=1, max_iter=400,  )\nMLP_model.fit(X_MLP, y_MLP)\n\n# getting testing data\nX = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be using to test\n#now we will predict the data\nMLP_predictions = MLP_model.predict(X)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data_MLP = pd.DataFrame(MLP_predictions)\nprediction_data_MLP.columns = [\"Survived\"]\nY_test = train[\"Survived\"]\nY_test = Y_test.reset_index().drop([\"index\"], axis=1)\n\naccuracyTest(prediction_data_MLP, Y_test)\n","98ac1284":"#setting up data\ny_MLP = train[\"Survived\"]\nMLP_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_MLP = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\n\n# now we will build the model\nMLP_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(3,), random_state=1, max_iter=400, activation=\"logistic\" )\nMLP_model.fit(X_MLP, y_MLP)\n\n# getting testing data\nX = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be using to test\n#now we will predict the data\nMLP_predictions = MLP_model.predict(X)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data_MLP = pd.DataFrame(MLP_predictions)\nprediction_data_MLP.columns = [\"Survived\"]\nY_test = train[\"Survived\"]\nY_test = Y_test.reset_index().drop([\"index\"], axis=1)\n\naccuracyTest(prediction_data_MLP, Y_test)","2a18e43e":"def TrainandTestMLP(alphaInput = 1e-5, hiddenLayers = (3,), maxIterations=400):\n    #setting up data\n    y_MLP = train[\"Survived\"]\n    MLP_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\n    X_MLP = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be training on\n    #checking to make sure data set looks right\n    #X_dec.head()\n\n    # now we will build the model\n    MLP_model = MLPClassifier(solver='lbfgs', alpha=alphaInput, hidden_layer_sizes=hiddenLayers, random_state=1, max_iter=maxIterations, activation=\"logistic\" )\n    MLP_model.fit(X_MLP, y_MLP)\n\n    # getting testing data\n    X = pd.get_dummies(train[MLP_features]) # pulling out the input data we will be using to test\n    #now we will predict the data\n    MLP_predictions = MLP_model.predict(X)\n\n    #prediction calculated percent survial rate, so anything equal to or above 50 did survive \n    #data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\n    prediction_data_MLP = pd.DataFrame(MLP_predictions)\n    prediction_data_MLP.columns = [\"Survived\"]\n    Y_test = train[\"Survived\"]\n    Y_test = Y_test.reset_index().drop([\"index\"], axis=1)\n    print(f\"Alpha = {alphaInput} | HiddenLayers = {hiddenLayers} | Max Iterations = {maxIterations} | Num Iterations: {MLP_model.n_iter_}\")\n    return accuracyTest(prediction_data_MLP, Y_test), MLP_model.n_iter_\n   ","db72885d":"# testing code above\n\nTrainandTestMLP() ","096c36be":"maxAccuracy = -1;\nhiddenLayerIdeal = -1;\nnumIters = -1;\nfor i in range(1, 15):\n    temp, num = TrainandTestMLP(hiddenLayers = (i,), maxIterations =5000)\n    if(temp > maxAccuracy):\n        maxAccuracy = temp\n        hiddenLayerIdeal = (i,)\n        numIters = num;\n\nprint(f\"\\n Highest Accuracy was: {maxAccuracy} with {hiddenLayerIdeal} hidden layers with {numIters} iterations\")\n    ","2f9a07b0":"maxAccuracy = -1;\nalphaIdeal = -1;\nnumIters = -1;\nfor i in range(1, 10):\n    temp, num = TrainandTestMLP(alphaInput = 10**(i-1) * 1e-5, hiddenLayers = (13,), maxIterations =15000)\n    if(temp > maxAccuracy):\n        maxAccuracy = temp\n        alphaIdeal = 10**(i-1) * 1e-5\n        numIters = num;\n\nprint(f\"\\n Highest Accuracy was: {maxAccuracy} with a {alphaIdeal} alpha with {numIters} iterations\")","d3e91ee1":"from sklearn.neural_network import MLPClassifier\n\nimport numpy\n#We need to build the data set we want to use. This will be selecting the features we want to use\n\n#setting up data\ny_MLP = train_data[\"Survived\"]\nMLP_features = [\"Sex\", \"Pclass\", \"Age\", \"Relatives\"]\nX_MLP = pd.get_dummies(train_data[MLP_features]) # pulling out the input data we will be training on\n#checking to make sure data set looks right\n#X_dec.head()\ny_MLP.head(15)\n# now we will build the model\nMLP_model = MLPClassifier(solver='lbfgs', alpha=.01, hidden_layer_sizes=(13,), random_state=1,max_iter=30000, activation=\"logistic\" )\nMLP_model.fit(X_MLP, y_MLP)\ny_MLP.to_csv(\"y_for.csv\", index = False)\n\n# getting testing data\nX_test = pd.get_dummies(test_data[MLP_features]) # pulling out the input data we will be using to test\nX_test.to_csv(\"X_test.csv\", index = False)\n#now we will predict the data\nMLP_predictions = MLP_model.predict(X_test)\n\n#prediction calculated percent survial rate, so anything equal to or above 50 did survive \n#data[['par1','par2']] = numpy.around(data[['par1','par2']], 2)\nprediction_data_MLP = pd.DataFrame(MLP_predictions)\nprediction_data_MLP.columns = [\"Survived\"]\n#prediction_data_for[\"Survived\"] = numpy.around(prediction_data_for[[\"Survived\"]], 0)\nprediction_data.head(15)\nMLP_predictions = prediction_data_MLP[\"Survived\"]\n\n\noutput_MLP = pd.DataFrame({\"PassengerId\" : test_data.PassengerId, \"Survived\" : MLP_predictions}) \noutput_MLP.to_csv(\"Final_MLP_Submission.csv\", index = False)","caba76e1":"# **Methods of Execution**\n\nOur plan of action is to analyze the data given to us by the Kaggle competition, use the organized data we analyzed on three different machine models, then pick the machine model that is the most accurate. Finally, we will tune the model to receive more accurate results. \n\n1) Analyze Data\n\n2) Compare the Accuracy of Each Model's Predictions\n\n3) Tune the Selected Model to Improve Predction Accuracy","0ef9b427":"In this section we will use the categories we analayzed above to train machine learning models to try to predict the outcome of a passenger surving with moderately high accuracy. \n\nThe categories we will be choosing are as follows: \"Sex\", \"Age\", \"Class\", and the \"Number of Relatives\" a passenger has aboard the *Titanic*. \n\nWe will try three different machine learning models to see which one will give us the most accurate results. Then, after we determine which one is the best fit for our data, we will attempt to tune the model to increase the accuracy of our model's predictions. \n\n","baa0e067":"# *Random Forest Classifier*\n\nThe random forest model is constructed of several decision trees that each come to their own conclusion as to whether or not a passenger will survive the shipwreck. In the same method as above, the decision trees will make their conclusions based off of the categorical data that we train them on. Once each tree comes to a conclusion, the random forest will consider each decision tree's \"vote\" to decomocratically decide the passenger's fate; i.e. the passenger's outcome with the most amount of votes is the random forest's prediction.","23b1ac97":"# *Activation Function*\n\nOne of the most obvious things that suck out to us is that we are not using a great forward activator function for our ouptut. By default the activator function of the MLP is the Relu function. This looks at values from 0 to inf. A more suitable function is the Sigmoid function. This one has outputs between 0 and 1. In this library it is called the \"logistic\" activator.\n\nLets try it!","1e2d9a32":"# *Digging Deeper*\n\nAn overview of the MLP:\n\nMLP what are its attributes?\n- Neruons\n- Forward Activators\n- Backpropagation\n- Hidden Layers\n- Regularization (Alpha)","4b59d75f":"# **Conclusions**\n\nOur approach to completing this final project was to learn data analysis and visualization skills through our participation in the NC State Datahon, and then apply those skills to a machine learning competition through Kaggle. After taking the top prize in the undergraduate category at the Datathon, we entered a Kaggle competition where we were tasked to determine if a passenger would survive on the *Titanic* given certain characteristics of that person. Using the skills we had learned in the previous competition, we used data analysis and visualization on the passenger data to determine which features would be useful in building a machine learning model. We tested three different models in our analysis; those models were the decision tree classifier, the random forest classifier, and the multi-layer perceptron (MLP). After splitting our training data to explore the accuracy of each model given the same number of features, we chose to implement a greedy MLP algorithm.To further increase the accuracy of our model\u2019s predictions, we changed the activator function to a sigmoid function. Then, we tuned the alpha and hidden layer values of the MLP.\n\nAfter tuning, we achieved an accuracy score that was better than assigning survival by gender. We ended up with a model that had an alpha value of 01., 13 hidden layers, and had 13328 iterations. It could predict who died and lived on the *Titanic* with 79.9% accuracy based on the person\u2019s age, sex, class, and number of relatives. In the process of tuning the data we were also able to get a 26.29% increase in accuracy on the split data. In the future, better tuning or even the exploration of different models could increase our competition accuracy. Throughout the project, we gained significant experience in data analysis, python programming, and training machine learning models. Please see our \u201cWhat I learned section below\u201d for more details! \n","37f0a0bf":"# *Sex*","1c68d787":"*Competition Score\/Model Accuracy:*\n\n![image.png](attachment:a213b3ad-398e-443d-a6fa-ff3c13fd604d.png)\n\n","62a5d4bc":"# Introduction\n\n\nFor this project we will be exploring machine learning concepts through the \"*Titanic* - Machine Learning From Disaster\" competition. The goal of the competition is to accurately predict which passengers have survived and which passengers have perished based on their characteristics.\n\nWe have been given a data set through Kaggle to use with machine learning models to predict which passengers survived. Although it would be fantastic to reach great heights on the copmetition's leaderboard, our objective is to learn about how different models of machine learning are used to recognize patterns and their underlying mechanics. The Kaggle competition will be used as a medium for this learning and to track our progress. \n\n","af5fc674":"# *Alpha Tuning*","079d27dc":"In this section we will pick our best model and tune its paremeters to see if we can get an even higher accuracy in the competition. This requires a more in depth look at the model and data to better understand how we can tune it more efficently rather than randomly guessing values. \n\nIn this case we are going to use the Multi-Layer Perceptron model.","f93183a5":"Some of this data may not be useful to train on. Most of the models later on rely on numerical data so data such as \"Name\", \"Embark\", \"Cabin\" may not be useful in our application. It is worth noting that there are methods of extracting numerical data from non numerical data given, we will explore this with the \"Sex\" data to be able to process it in our code. \n\nIn this section we end up exploring the categories of \"Sex\", \"Age\", \"Pclass\", and \"SibSp & Parch\".\n","51cc5247":"With the machine learning algorithm we revceived a prediction accuracy score of 77.51%. This score is slightly better than the one prediction made off of the sex of the passengers alone. This improvement in score is expected because we introduced more data for a model to train on; this meas that if the extra datasets are not random noise we should see a higher accuracy rate.","57a6efbe":"# *Multi-Layer Perceptron Model*\n\nA multi-layer perceptron model (MLP) is a type of nueral network that uses supervised learning that, given a set of features, can learn a non-linear function approximation for classification or regression; in our case, we are using our MLP for classifying if a passenger will survive the *Titanic* shipwreck.\n\nIn the following code, we train the MLP on the test dataset to learn a non-linear function. During training, we feed in our selected features for each passenger (Age, Sex, Class, and Relatives), and then whether or not that passenger survived (indicated as a 0 or 1). Through Backpropogation and the use of hidden layers, the MLP then refines the non-linear function to classify if a passenger will survive the shipwreck.\n\n","65060353":"It looks like having 13 hidden layers gets us a really great accuracy! It is about 2% higher than before, so taking a greedy approach we will be using 13 hidden layers when tuning alpha. It is important to note that for the 2% increase in accuracy we had a 2853% increase in iterations! Although this was very high, it only took about half a second to train the model.  ","46c1f575":"# *Faster Tuning at a Cost*\n\nDue to the competition capping the number of attempts to 10 and for tuning purposes, we will split the data in half and test on it to see if our tuning is effective. \n\nIt is worth noting that because we are training with a smaller set of data (half the size) we may have lower accuracy, or may make changes that improve our local accuracy, but does not improve our Kaggle competion accuracy. As mentioned before our main goal is to increase our understanding not necessary get a higher accuracy on the Kaggle competition.\n\n\nIn this section we will also be deep diving into the math and parameters of a MLP classifer. ","e8b2bb8b":"Great looks like it worked! ","1794daa1":"# *Hidden Layer Tuning*","e56b483f":"# **Tuning**","b8639e13":"# *The Decision Tree Classifier Model:*\n    \nFor our first model we will use the Decision Tree Classifier Model. It is the first model that is explored in the Kaggle machine learing courses. A Decision Tree Classifier model breaks each category down until a conditon for output is reached. Each breakdown of category is a parent node with a child (except for at the root), and the end leaf nodes are decision nodes that have an output. This output will be a 0 if the tree determines if the passenger did not survive the shipwreck, and a 1 if the passenger lived to see another day.\n   ","890d1b67":"Just creating a CSV file based on the survival rates of each sex leads to a 76.56% death prediction rate. This would imply that forming predicitons based on gender alone, we would be able to predict the survival rate of the members onboard with 76.55% accuracy.\n\nThis will be a good baseline on how accurate our models are later on.","b414b139":"# **What I learned** \n\n**Trenton Wallis:** \n\nBeginning with the Datathon, I gained experience using QGIS and analyzing real world data. As a team we also presented to the competition and won first place against 100s of other students. I think this was due to the amount we learned about data analysis during the competition. Some of the main focus of data analysis during the datathon was filtering which data was actually important, and understanding visual trends in the data.Through the competition, I also was able to brush up on my python programming skills as well as my data visualization skills, which would be useful later. Next our team moved over to the Kaggle competition. Through the competition I learned more about basic data analyzed through Kaggle courses, but also through practicing it on our data. Kaggle also had great resources on the basics of python and on basic and intermediate machine learning. This gave me some intuition on how we should approach our project, and gave me valuable skills using the sklearn and matplotlib libraries that I have never used before. The courses introduced two machine learning models. When first entering the competition that is something that I never considered, that there are different models for different types of data. In the Kaggle competition I explored the basics of the decision tree classifier model, the random forest classifier model, and the multi layer perceptron model. I also became aware of the differences between a regressor model and a classifier model. After taking a greedy approach, we choose the MLP model. Conducting lots of research on the MLP model, I learned more about what exactly neurons are, more about forward activiors and their different types, back propagation, nonlinearity in terms of a machine learning model, hidden layers, and regularization. One of the most important aspects of machine learning that I learned was how abstract yet accessible it can be. Early on, I trained models that were 4 lines of code, but after splitting data, picking a model, and tuning the parameters it can easily become 100s of lines of code.\n\n**Lauren White:** \n\nI wanted to use this project as a way to explore mathematical concepts, ideas, and experiences that I have never been exposed to in the past; I can say with full confidence that I am very pleased with what I have learned throughout these past few weeks. The Datathon was the first university-wide competition that I had ever participated in, and the results were even greater than I could have imagined. The competition introduced me to advanced forms of data analysis and visualization with workshops on QGIS, Python, R, and Excel. Using these tools to analyze cyclist safety in Raleigh, Trenton and I won the top prize in the undergraduate category. Then, I was able to expand upon these skills, and the machine learning skills learned in ECE 301, in the Kaggle competition. Before this experience, I had very little knowledge of how to actually program anything in Python. After taking Kaggle\u2019s Python course and a lot of trial and error on writing the script of the project, I feel much more confident in my ability to apply Python to other projects and courses in the future. I feel that if I had had previous experience with the plotting and data analysis libraries that Python has, I would have done our MATLAB homework in Python instead. I have a heavy background in C and C++ programming, so Python ended up being more intuitive for me to write in than MATLAB. \n\nThroughout ECE 301, I felt that I understood the machine learning concepts that we had learned in class just well enough to be able to regurgitate it back onto an exam. Through taking the beginner and intermediate Kaggle courses, I have not only solidified, but expanded upon the material that we had covered in class. Learning about, and then implementing, the decision tree classifier, random forest classifier, and multilayer-perceptron models built upon the linear regression and neural network models that we had learned in class. Most importantly, learning about those models solidified the mathematical concepts that we had covered during our lectures and homeworks. Post the completion of this project, I cannot wait to take more of Kaggle\u2019s classes on Python and machine learning, and then apply that knowledge to other competitions.\n","64911e5a":"Next, the survival rate of men aboard the *Titanic*.","56446b58":"# **Getting Started**\n\nAt the top of this notebook hit the \"Run All\" button. It should look like:\n\n![image.png](attachment:12bebd5a-eee4-40dc-8b1e-711500b4700b.png)\n\nThis will let you display all of the plots and text necessary for understanding our thought processes.","0da34771":"# *Age*\n\nWe are now going to look at different age groups to see what might be a strong predictor for a passenger's survival rate aboard the shipwreck. We split the Age data into several age groups of human development.\n\nOur classifications of the different age groups are as follows:\n* Infants: 0-6  \n* Children: 7-11 \n* Teenagers: 12-19\n* Adults: 20-40 \n* Middle Aged: 41-60 \n* Elderly: 61+","77c24db3":"An alpha value of 0.01 seems to have given us a slightly higher accuracy. There is an accuracy increase of .67%, with a 295.6 % increase in iterations. Unfortunately, we made be seeing the limit of our greedy approach. Let's see how the model does in the Kaggle submission to see if our improvements transferred over. ","76fb302d":"![image.png](attachment:eee2a6be-75c9-4d63-b934-383addb0a8e3.png)","daec81e1":"# **Machine Learning**","945e564a":"As we can see from above we are given data on the passengers \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", and \"Embark\".\n\nWhat do these categories mean?\n\nPclass - Class of the Passenger \n\nName - Name of the Passenger\n\nSex = Male or Female\n\nAge = Age of the Passenger\n\nSibSp = Number of Spouses or Siblings the Passenger had on the *Titanic* \n\nParCh = Number of Children or Parents the Passenger had on the *Titanic*\n\nTicket = Ticket Number\n\nFare = Cost of Ticket\n\nCabin = Cabin Number or Location on the *Titanic*\n\nEmbark = Which dock the passenger embarked from. The docks included are Southhampton, Queenstown, and Cherbourg.","5048e25f":" # **Division of Labor**\n \n **Lauren**: Sex, Class and Relatives data analysis, building and describing the decision tree and random forest models, MLP activator function change, splitting the training data for internal verification, reference compilation and formatting \n \n **Trenton**: Age data analysis, MLP attribute description, building the MLP model, MLP iterative approaches (alpha and hidden layers), built the function for internal verification\n \n **Collaborative**:\n Datathon analysis and presentation, Kaggle Python and machine learning courses, formatting and editing, introduction, table of contents, and conclusion.","9d141ab7":"*Competition Score\/Model Accuracy:*\n\n![image.png](attachment:9936d798-ec34-47d7-b16d-6e8350636c42.png)\n\n\n**Conclusion**\nOur results were not that great, espically when we could just corrilate sex with survial rate and get a better score....","e602dbbf":"**Conclusion:**\nPassengers with first-class tickets were significantly more likely to survive the shipwreck than passengers with second-class and third-class tickets. Notice the these rates exclude sex. This means that regardless of being a male or female, a passenger with a first-class ticket has a 63% chance of surviving the shipwreck. \n\nOut of curiosity, we explored how many of the first class passengers were male, and how many were female. 122 were male, and 94 were female. As we saw in our analaysis of sex and survival rate, men had a 19% chance of surviving the shipwreck. However, here we see that men make up 56.5% of first-class, and that first-class passengers had a 63% chance of survival. Therefore, we can conclude that men in first-class had a much higher chance of survival than men with second-class and third-class tickets. \n\nClass appears to be another strong indicator survival rate on the *Titanic*, so we will include that in our model. ","37dedf45":"# *Class*\n","d4ff587a":"# ***Titanic* Submission - ECE 301 Final Project**\nBy: Trenton Wallis and Lauren White\n\nECE 301\n\nDr. Robert Heath","b656d344":"# *Code Clean Up*","b9d9b69e":"Well, it looks like after all that work we ended up with a less ideal percent accuracy. This is most likely due to overfitting on our data. This could possibly be from having too many hidden layers or even not having a high enough alpha parameter. In the future, these values could be more finely tuned, or even more, features could be added. The MLP model also has other parameters that could be adjusted to achieve a better accuracy score.\n\nThis demonstrates one of the biggest challenges of machine learning, trying to build a model on a finite set of data that could be used on an infinite set of data. In the future, we could learn better techniques in training models, or even explore models that train on this data more efficiently with higher accuracy. ","d0d7fef4":"The construction of the *Titanic* was such that the cabins and amenities that a passenger had access to were significantly influenced by which \"class\" of ticket that passenger had purchased. For example, many of second and third class cabins were on the back and lower portions of the ship, which more susceptible to damage from the impact with the iceberg. Additionally, it would be more difficult to rescue passengers from the lower decks and bring them topside. As such, we decided to analyze \"PClass\" and determine if there was any correlation between the class of a passenger's ticket and their chance at surviving the shipwreck.  ","de4ef7f5":"**Conclusion:** \nA passenger is significantly more likely to survive the shipwreck if they have between one and three relatives aboard the *Titanic*.","6040c931":"# *Proof Of Concept Accuracy Testing*\n\nBelow we will split up the data ","2fd5caf3":"# **References**\n\n\u201c1.17. Neural network models (supervised)\u00b6,\u201d scikit. [Online]. Available: https:\/\/scikit-learn.org\/stable\/modules\/neural_networks_supervised.html. [Accessed: 29-Apr-2021]. \n\n\u201cLearn Intermediate Machine Learning Tutorials,\u201d Kaggle. [Online]. Available: https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning. [Accessed: 29-Apr-2021].\n\n\u201cLearn Intro to Machine Learning Tutorials,\u201d Kaggle. [Online]. Available: https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning. [Accessed: 29-Apr-2021]. \n\n\u201cLearn Python Tutorials,\u201d Kaggle. [Online]. Available: https:\/\/www.kaggle.com\/learn\/python. [Accessed: 29-Apr-2021]. \n\nM. A. Nielsen, \u201cNeural Networks and Deep Learning,\u201d Neural networks and deep learning, 01-Jan-1970. [Online]. Available: http:\/\/neuralnetworksanddeeplearning.com\/chap1.html. [Accessed: 29-Apr-2021].\n\nS. Ronaghan, \u201cDeep Learning: Overview of Neurons and Activation Functions,\u201d Medium, 26-Jul-2018. [Online]. Available: https:\/\/srnghn.medium.com\/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4#:~:text=What%20is%20a%20neuron%3F,to%20become%20the%20neuron's%20output. [Accessed: 30-Apr-2021]. \n\nS. Ronaghan, \u201cDeep Learning: Which Loss and Activation Functions should I use?,\u201d Medium, 01-Aug-2019. [Online]. Available: https:\/\/towardsdatascience.com\/deep-learning-which-loss-and-activation-functions-should-i-use-ac02f1c56aa8?gi=fb710a4c7b9a. [Accessed: 30-Apr-2021]. \n\nWhat is backpropagation really doing? | Deep learning, chapter 3. YouTube, 2017. ","29da49d6":"# **Data Anlaysis**","c59b01b1":"Now lets try testing the accuracy test on our MLP model to get a base line.","705499d4":" # *Sanity Check*\n\nApplying a machine learning algorithm to other classes below, we chose to use Pclass, Sex, SibSp, and Parch as our features, since there is an example to compare our output. ","eb51992d":"# *Attributes Descriptions*\n\n**Neurons**:\n\n![image.png](attachment:e4b7490e-47a3-4595-bd27-47b89f2aa06c.png)\n\nPhoto Credit: https:\/\/srnghn.medium.com\/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4#:~:text=What%20is%20a%20neuron%3F,to%20become%20the%20neuron's%20output.\n\nA neuron, simply put, is a node (or function) that takes in several different inputs. Each input can have a predefined weight (that changes as the model learns) when entering the neuron. If different weights are assigned to the same inputs it will most likely have created a different output from the neuron. Each neuron may also have a bias, which is a constant value that is added or subtracted from the inputs and their weights to further tune the neuron. Neurons in the field of machine learning are also called perceptrons (hence the name Multilayer Perceptron MLP). The true power of perceptrons is when you begin to build layers of them as you see below. \n\n![image.png](attachment:bec4fce3-8ef1-4994-9076-a697d680634d.png)\n\nPhoto Credit: https:\/\/srnghn.medium.com\/deep-learning-overview-of-neurons-and-activation-functions-1d98286cf1e4#:~:text=What%20is%20a%20neuron%3F,to%20become%20the%20neuron's%20output.\n\nThis may look confusing, but really it's just each perceptron taking one input from each other perceptron, from left to right. One change in input on one perceptron could end up affecting the result of all the perceptrons taking an input from it, hence making the neural network nonlinear. It builds complex relationships that are hard to see for humans. In relation to the competition, this makes sense, each category has some weight on if that person survives or not. Ideally, the model can build complex relationships between each category and eliminate the noise to produce a higher survival rate. \n\n**Foward Activators**:\n\nEach perceptron is considered a function, as mentioned above. These functions are called Foward Activators, this is because they feed an output to the next perceptron, or if it is in the output layer, the output. Different activators can be used for different approaches based on the expected output of the data. In our case, we expect a binary output, a 1 for survived or a 0 for died. It would not make sense to have a decimal value or a negative value. \n\nLet's go over a few of the active functions to build some intuition:\n\nLinear: A straight line (sometimes polynomial) model. This is a great example of what we did with the bass and perch data. We saw that as a fish's dimensions increase its weight tends to increase. This is great for output values you expect to be between -inf and inf. \n\nRelu: Linear but does not have negative output values. Negative values will be equated to 0. This would be an even better model for the bass and perch if there seems to be an initial shift in the y value due to unclean or wrong data. This could potentially can cause negative weight values which doesn't make sense.\n\nSigmoid: A nonlinear function that outputs a 0 or a 1. This is useful for determining if something is true or false, or has a binary output. For example, the output could be if a passenger of a certain ship died or not based on their characteristics.... Hmmm this might come in handy later.\n\n\n**Backpropagation**: \n\nBackprogration is the key to MLP machine learning. Put simply it is the way the model trains to minimize the error in getting the answer we want. Say for a machine learning model with one layer, initially the model will randomly guess at the output. It will then randomize the weights and biases and get another output. It will then look at each difference in weight and bias to see if the change helped or hurt the success rate. Weights and biases that helped increase the successrate may be increased while the others may be decreased. We also cannot forget about our inputs both the ones we feed in and the inputs that are fed into perceptrons from other perceptrons. Where the term backpropagation starts to seem relevant is when we look at the error of the output with the next left layer's weights and biases to that output. Then that perceptron we saw as an input can be considered an output which may change the weights and biases from the perceptrons feeding into it to help get the correct answer. This would work backwards from output to the rightmost layer, to the second rightmost layer etc to the input. \n\n\n**Hidden Layers:**\n\nA hidden layer is a group of perceptrons that share the same input perceptrons. This does not include the input and output layer. Each layer may have at least one percepton. Determining how many layers and how many perceptrons each layer has can be a bit tricky. Many sources online use graphics drawing lines of separation, while some say it is an iterative empirical approach. One source even mentions specific criteria based on the number of outputs and the number of features. \n\n**Regularization:**\n\nRegularization in terms of our MLP model is a way to decrease overfitting. It is expressed by our alpha parameter. The higher the parameter the more heavily weighted data is penalized while data that has less weight is rewarded. \n\n\n","4fa68091":"First, we will analyze the survival rates between men and women aboard the *Titanic*.\nThe survival rate of the women aboard are as follows:","2ba71a0c":"Yikes! That is not as good as our accuracy we submited when we first tested the model. One reason for this may be that we are using half the amount of inputs. Lets take a deeper dive at what the MLP model is doing and some paramters we can change to try to get better accuracy. This way we can understand the underlying problem, and find a more ideal solution. ","3d49066f":"# *Iterative Tuning*\n\nFirst we will begin with the number of hidden layers to see what our maximum accuracy is with which layer. We will go ahead and take a greedy approach. After picking the best layers we will again take a greedy approach tuning the regularization paramter alpha. ","a217b528":"# *Relatives*\n\n**Passenger Relationships: Sibling\/Spouse relationships and Parents\/Children relationships**\n\nNow we will analyze if a passenger's relationship with other passengers will affect their survival rate aboard the *Titanic*.\n\nOur smallest number of relationships to other passengers is 0, and the largest is 9.\n\nPlan: Analyze if a passenger is more likely to survuve the shipwreck based on the number of relatives they have on board.\n","6c7ba3e7":"# **Table of Contents**\n\n**Introduction** \n- Project Description\n\n**Methods of Execution**\n- Analysing the Data \n- Picking a Model\n- Tuning the Model \n\n**Getting Started:**\n- Reading in Datasets\n- Hit the \"Run All\"\n\n**Data Analysis**\n- General description\n- Sex\n- Age\n- Pclass\n- Number of Relatives\n\n**Machine Learning Models**\n- General Description\n\n**Decision Tree**\n- Decision Tree Description\n- Decision Tree Model Code\n- Decision Tree Model Accuracy\n\n**Random Forest Tree**\n- Random Forest Tree Description\n- Random Forest Model Code\n- Random Forest Model Accuracy\n\n**Multi-layer Perceptron (MLP)**\n- MLP Description\n- MLP Model Code\n- MLP Accuracy\n\n**Tuning**\n- Digging Deeper\n- Tuning Parameters\n- Testing \n- Resulting Model\n\n**Conclusion**\n- Project Summary\n- What I Learned\n\n\n**Contributions**\n\n**References**\n- IEEE Citations","c573d176":"Look at that! Already we can see a significant improvement with a single change. We went from 60.7% all the way to 83.4%~\n\nIt is hard to gain intution on what the alpha and the amount of hidden layers should be. It may be appropiate to find which ones work best through an iterative empirical approach. Below we will put the above code into a function to simply code and give a better description on what exaclty we are changing. ","7846852b":"**Conclusion:**\nBeing elderly during the shipwreck was almost certainly a death sentence. Regardless of sex, infants were the most likely to survive the shipwreck, with a survival rate of 70.21%.\n\nAge seems to be a good indicator for infants and elderly survial rates, so we will add this category to our machine learning models.","20b134bf":"*Competition Score\/Model Accuracy:*\n\n![image.png](attachment:41339cdc-cc26-4f11-a20c-7529af47e369.png)\n"}}