{"cell_type":{"32f1a11e":"code","df3cedba":"code","f069b049":"code","9b0db0e2":"code","c9d56f3d":"code","7c76a841":"code","4ed939ff":"code","9ef7b2c3":"code","8eee8a94":"code","08dbd7df":"code","43086501":"code","6cbb628a":"code","64dc405c":"code","c1786338":"code","76408fbb":"code","12cc0190":"code","38d2a91d":"code","b13cd749":"code","2ec776a4":"code","c524b526":"code","89f1cfbd":"code","82c2a84a":"code","36fcef57":"code","d4b923c2":"code","0c83a414":"code","381ef4d7":"code","15725e75":"code","410f5ad6":"code","19c5c85f":"code","355efa4f":"code","c055940f":"code","8911455f":"code","081c51e5":"code","4cd8b9f5":"code","f71c18b5":"code","cb847164":"code","2adb4bca":"code","ab7313fe":"code","6ea1e34e":"code","b1ca6e93":"code","13b67b6f":"code","454f5a61":"code","312d5dc0":"code","2134177f":"code","4b7030ec":"code","d6b8cf98":"code","21c85be4":"code","ccdc180b":"code","132d77f4":"code","b7f4d8d8":"code","5f6853af":"code","e2cc3b91":"code","e25e33e9":"code","eaed5ec9":"code","0b0630b8":"code","79a408d4":"code","34ad7f94":"code","c1039108":"code","9d9ba0f2":"code","63ea9c04":"code","ec98a951":"markdown","0ccbfdab":"markdown","df88ed25":"markdown","bd945916":"markdown","3d520570":"markdown","e9a802af":"markdown","f1803531":"markdown","79e2d8fc":"markdown","aa82e7a9":"markdown","f290fc75":"markdown","ecea6599":"markdown","de8c4cdf":"markdown","5e760e26":"markdown","8d3592f3":"markdown","bef38691":"markdown","99c60427":"markdown","a3d3b08f":"markdown","a96e51f5":"markdown","fecda9b3":"markdown","ea959579":"markdown","c574cb56":"markdown","63cde810":"markdown","ba4c9d05":"markdown","d0d9173e":"markdown","47b8c074":"markdown","2f60cc62":"markdown","3ca7858f":"markdown","a2c64ea2":"markdown","5e62f63e":"markdown","21f7931f":"markdown","a81ae644":"markdown","7578cc01":"markdown","152836cd":"markdown","d8c577da":"markdown","69a32e09":"markdown"},"source":{"32f1a11e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df3cedba":"#importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly\nplotly.offline.init_notebook_mode(connected = True)\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split as tts,RandomizedSearchCV,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,plot_confusion_matrix","f069b049":"#data loading\nst=pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\nst.head()","9b0db0e2":"#checking number of rows and columns of dataset\nst.shape","c9d56f3d":"st.info()","7c76a841":"st.isnull().sum()","4ed939ff":"#creating a copy of original dataset for treating missing values\nst_copy=st.copy(deep=True)","9ef7b2c3":"st_copy['ever_married']=st_copy['ever_married'].replace({'Yes':1,'No':0})\nst_copy=pd.get_dummies(st_copy,drop_first=True)","8eee8a94":"plt.figure(figsize=(20,20))\nsns.heatmap(st_copy.corr(),annot=True)","08dbd7df":"st['gender'].value_counts(normalize=True)","43086501":"st['gender']=st['gender'].replace('Other','Female')","6cbb628a":"#dropping missing data \nst=st.dropna()","64dc405c":"#dropping unnecessary columns\nst.drop(columns='id',inplace=True)","c1786338":"#function to observe values in each categorical feature\ndef value_viz(feature,title):\n    return px.pie(st,feature,title=title)","76408fbb":"value_viz('gender','Distribution Of Gender')","12cc0190":"value_viz('gender','Distribution Of Gender')","38d2a91d":"value_viz('hypertension','Distribution of people with High Blood Pressure')","b13cd749":"value_viz('heart_disease','Distribution of People having Heart Disease')","2ec776a4":"value_viz('ever_married','Distribution of people who are married')","c524b526":"value_viz('work_type','Distribution of people\\'s work type')","89f1cfbd":"value_viz('Residence_type','Distribution of where people live')","82c2a84a":"value_viz('smoking_status','Distribution of people who smoke')","36fcef57":"value_viz('stroke','Distribution of people having stroke')","d4b923c2":"plt.figure(figsize=(20,5))\nsns.histplot(st['age'])\nplt.xticks(range(0,100,10))\nplt.title(\"Distribution of Age\")","0c83a414":"plt.figure(figsize=(20,5))\nsns.histplot(st['avg_glucose_level'])\nplt.title('Distribution of Average Glucose Level')\nplt.xticks(range(0,300,25))","381ef4d7":"plt.figure(figsize=(20,5))\nsns.histplot(st['bmi'])\nplt.title('Distribution of Body Mass Index')\nplt.xlabel('BMI in kg\/m2')\nplt.xticks(range(0,100,10))","15725e75":"plt.figure(figsize=(20,5))\nsns.boxplot(x='age',data=st)","410f5ad6":"plt.figure(figsize=(20,5))\nsns.boxplot(x='bmi',data=st)","19c5c85f":"plt.figure(figsize=(20,5))\nsns.boxplot(x='avg_glucose_level',data=st)","355efa4f":"#function to find outliers\ndef iqr_outliers(df):\n    out=[]\n    q1 = df.quantile(0.25)\n    q3 = df.quantile(0.75)\n    iqr = q3-q1\n    Lower_tail = q1 - 1.5 * iqr\n    Upper_tail = q3 + 1.5 * iqr\n    for i in df:\n        if i > Upper_tail or i < Lower_tail:\n            out.append(i)\n    return out","c055940f":"d=iqr_outliers(st['bmi'])","8911455f":"#finding minimum of outliers in bmi\nd.sort()\nd[0]","081c51e5":"e=iqr_outliers(st['avg_glucose_level'])","4cd8b9f5":"#finding minimum of outliers in avg_glucose_level\ne.sort()\ne[0]","f71c18b5":"#median imputation in bmi\nmed=st.bmi.median()\nfor i in st.bmi:\n    if i>=47.6:\n        st.bmi=st.bmi.replace(i,med)","cb847164":"#median imputation in avg_glucose_level\nmed=st.avg_glucose_level.median()\nfor i in st.avg_glucose_level:\n    if i>=168.68:\n        st.avg_glucose_level=st.avg_glucose_level.replace(i,med)","2adb4bca":"st.shape #201 outliers have been removed","ab7313fe":"st.describe()","6ea1e34e":"y=st.stroke\nX=st.drop('stroke',axis=1)","b1ca6e93":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\nX['gender']=le.fit_transform(X['gender'])\nX['ever_married']=le.fit_transform(X['ever_married'])\nX['work_type']=le.fit_transform(X['work_type'])\nX['Residence_type']=le.fit_transform(X['Residence_type'])\nX['smoking_status']=le.fit_transform(X['smoking_status'])","13b67b6f":"X.head()","454f5a61":"#splitting the original dataset\nX_train_or,X_test_or,Y_train_or,Y_test_or=tts(X,y,test_size=0.25,random_state=27)","312d5dc0":"#separating numerical and categorical for feature selection\nnumerical=X_train_or[['age','avg_glucose_level','bmi']]\ncategorical=X_train_or.drop(columns=numerical.columns)","2134177f":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\ntest = SelectKBest(score_func=chi2, k='all')\ntest.fit(categorical, Y_train_or)\nfor i in range(len(test.scores_)):\n    print('Feature %s: %f' % (categorical.columns[i], test.scores_[i]))","4b7030ec":"from sklearn.feature_selection import RFE\nmodel = DecisionTreeClassifier()\nrfe = RFE(model, 2)\nfit = rfe.fit(numerical, Y_train_or)\nprint(\"Num Features: %s\" % (fit.n_features_))\nprint(\"Selected Features: %s %s\" % (fit.support_,numerical.columns))\nprint(\"Feature Ranking: %s\" % (fit.ranking_))","d6b8cf98":"X_train_or=X_train_or[['hypertension','heart_disease','avg_glucose_level','bmi']]\nX_test_or=X_test_or[['hypertension','heart_disease','avg_glucose_level','bmi']]","21c85be4":"#using standard scaler to scale training data and applying it to testing data\nsc=StandardScaler()\nX_train_scaled_or=sc.fit_transform(X_train_or)\nX_test_scaled_or=sc.transform(X_test_or)","ccdc180b":"rf=RandomForestClassifier(random_state=25)\nrf.fit(X_train_or,Y_train_or)\npred=rf.predict(X_test_scaled_or)\nplot_confusion_matrix(rf,X_test_scaled_or,Y_test_or,cmap=plt.cm.Blues,normalize='all')\nprint(classification_report(pred,Y_test_or))","132d77f4":"st['stroke'].value_counts()","b7f4d8d8":"#splitting before applying smote\nX_train,X_test,Y_train,Y_test=tts(X,y,test_size=0.25,random_state=27)","5f6853af":"X_train=X_train[['avg_glucose_level','bmi','hypertension','heart_disease']]\nX_test=X_test[['avg_glucose_level','bmi','hypertension','heart_disease']]","e2cc3b91":"#using SMOTE to generate synthetic examples in target variables \nover = SMOTE(random_state=27)\nunder = RandomUnderSampler(random_state=27)\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\nX_train_balanced, Y_train_balanced = pipeline.fit_resample(X_train, Y_train)","e25e33e9":"Y_train_balanced.value_counts()","eaed5ec9":"sc=StandardScaler()\nX_train_scaled=sc.fit_transform(X_train_balanced)\nX_test_scaled=sc.transform(X_test)","0b0630b8":"#function to fit models\ndef model(model):\n    mod=model\n    mod.fit(X_train_scaled,Y_train_balanced)\n    mod_pred=mod.predict(X_test_scaled)\n    plot_confusion_matrix(mod,X_test_scaled,Y_test,cmap=plt.cm.Blues,normalize='all')\n    print(classification_report(mod_pred,Y_test))","79a408d4":"model(LogisticRegression(random_state=25)) ","34ad7f94":"model(DecisionTreeClassifier(random_state=25))","c1039108":"model(KNeighborsClassifier())","9d9ba0f2":"model(XGBClassifier(use_label_encoder=False,random_state=25))","63ea9c04":"model(RandomForestClassifier(random_state=25)) ","ec98a951":"***Since the dataset contains most instances of an negative stroke(4700) so there might be a possibility that the model builded on this dataset classifies the person having stroke as the person who don't have stroke(known as False Negative).***\n\n*So to avoid this handling of imbalanced dataset is mandatory in classification models.*","0ccbfdab":"* *Most of the individuals of the dataset are of age 40 and above.*\n\n* *age is normally distributed.*","df88ed25":"* *Accuracy is 96% but the model is unable to correctly classify people who suffer from a stroke.*\n\n* *The model classifies people who have a stroke as people who don't have a stroke.*\n\n* *This is due to imbalance of target variable as majority of values consists of people who don't suffer from a stroke and therefore the model learns that.*","bd945916":"***KNN CLASSIFIER***","3d520570":"***MORE TECHNIQUES TO IMPROVE CLASSIFICATION WILL BE EXPLORED IN NEXT UPDATE.***","e9a802af":"## FEATURE SELECTION","f1803531":"## ENCODING VARIABLES","79e2d8fc":"## OBSERVING CONTINUOUS VARIABLES","aa82e7a9":"***XGBOOST CLASSIFIER***","f290fc75":"## MODEL BUILDING AND EVALUATION(ORIGINAL DATASET)","ecea6599":"***DECISION TREE CLASSIFIER***","de8c4cdf":"*Outliers are present in both bmi and avg_glucose_level.*","5e760e26":"## MODEL BUILDING AND EVALUATION(AFTER SMOTE)","8d3592f3":"## OUTLIER REMOVAL","bef38691":"## OUTLIER DETECTION","99c60427":"***LOGISTIC REGRESSION***","a3d3b08f":"## SCALING DATA(ORIGINAL DATASET)","a96e51f5":"***RANDOM FOREST CLASSIFIER*** ","fecda9b3":"***BMI*** *column has 201 missing values.*","ea959579":"## SCALING DATA(AFTER SMOTE)","c574cb56":"*Since **'bmi'** feature don't show high correlation with any of the variables in the dataset neither it is related  by the characteristics of the missing data itself.Therefore the values in bmi are* ***Missing Completely At Random(MCAR).***","63cde810":"* *Most of the individuals of the dataset has BMI index between 20-30 kilogram\/metresq.*\n \n* *bmi is also normally distributed with some outliers*","ba4c9d05":"***hypertension and heart_disease are the most important categorical features.***","d0d9173e":"## OBSERVING CATEGORICAL VARIABLES","47b8c074":"*Since the proportion of values having **Other** is very less **(0.0196%)** so we can impute it to female(as it was the most frequent occuring element).*","2f60cc62":"*It can be seen that it is an imbalanced dataset having people without stroke as 95.7% and remaining 4.26% having stroke*","3ca7858f":"***avg_glucose_level and bmi are the most important numerical features.***","a2c64ea2":"***'bmi'*** *has maximum value of **47.5** and **'avg_glucose_level'** has maximum value of **168.15**.Therefore it can be seen that all the outliers from both of the features have been removed.*","5e62f63e":"*Hence we can see that now the stroke feature is balanced in training dataset containing equal sets of both 0 and 1 after using SMOTE.*","21f7931f":"***STEPS-***\n\n* *We first find the outliers in each of the feature.*\n* *Then we will find out the minimum(starting value) value from outliers.*\n* *Then values above this minimum value will be replaced by median (as it is not affected by outliers).*","a81ae644":"*Since it is an imbalanced dataset we will use Random Forest Classifier to see the predictions on original dataset.*","7578cc01":"## HANDLING IMBALANCED DATA","152836cd":"No outliers in age feature.","d8c577da":"* *Most of the individuals have 75-100 average glucose levels.*\n\n* *avg_glucose_level has right skewed distribution.*","69a32e09":"## TREATING MISSING VALUES"}}