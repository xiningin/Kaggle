{"cell_type":{"ec7500bb":"code","3b34f98e":"code","d59aeef0":"code","e5ec7fb6":"code","9e826d80":"code","ae9831f8":"code","638a91a0":"code","fbf3b69b":"code","316d5682":"code","608858b6":"code","a1de337f":"code","bc5e12c9":"code","e3a03929":"code","f0f26830":"code","1168b220":"code","03dc9c3a":"code","ba003f87":"code","1e6e3901":"code","9c411886":"code","ceb93fd7":"code","a598af6c":"code","8bc39e19":"code","642a9a34":"code","a24d9590":"code","e51b65c4":"code","9ea2e5eb":"code","350fe587":"code","3013affb":"code","8ce68d19":"code","2c1f1fc5":"code","86aeb22b":"code","c503e6b0":"code","6e6566de":"code","0a258b83":"code","f29170e7":"code","7ef57455":"code","d0acdade":"code","3f2a8917":"code","eef70e0f":"markdown","6d770b70":"markdown","b614ab63":"markdown","63752fa3":"markdown","e12d4ec7":"markdown","508d10cc":"markdown","cb4245a3":"markdown","7e318adf":"markdown","0c796178":"markdown","c9c1cf62":"markdown","062a9a6d":"markdown","8c34b0a7":"markdown","f3469085":"markdown","add423d4":"markdown","68fade23":"markdown","0a425b2c":"markdown","b27cef0d":"markdown","4dd89709":"markdown","0f6abee0":"markdown","5b86d5f5":"markdown","7cfcb9c7":"markdown","7ca03a96":"markdown","748848c6":"markdown","f78e61dd":"markdown","b1685a88":"markdown","aacc7d1e":"markdown","e466ebc5":"markdown","c9d21f59":"markdown","f6cb8854":"markdown","4ba83d63":"markdown","7c6b9e57":"markdown","d0b908df":"markdown","a5783121":"markdown","d6905a57":"markdown","29f2e43b":"markdown","72356bbc":"markdown","1cfb6dbc":"markdown","6199f841":"markdown","4148cd1f":"markdown","1346548c":"markdown","87ed10bf":"markdown","050090d1":"markdown","ac89cbfb":"markdown","de8b6931":"markdown"},"source":{"ec7500bb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scikitplot.estimators import plot_learning_curve\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.utils.vis_utils import plot_model\n\n#  Models\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as ctb\n\n\n# Preprocessing\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Scoring\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix\n\n# Hyperparameters and features importance\nfrom sklearn.model_selection import GridSearchCV\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# remove verison errors\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)","3b34f98e":"path = '\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/'\n\ndf_heart = pd.read_csv(path + 'heart.csv')","d59aeef0":"df_heart.shape","e5ec7fb6":"df_heart.info()","9e826d80":"df_heart.sample(15)","ae9831f8":"df_heart.isnull().sum().sum()","638a91a0":"df_heart.duplicated().sum()","fbf3b69b":"df_heart.drop_duplicates(inplace=True)","316d5682":"categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exng','slp', 'caa','thall'] # 8\ncontinous_cols = ['age', 'trtbps', 'chol','thalachh', 'oldpeak'] # 5\nlabel_col = ['output']\n\nname_change = {\n    'sex': {'0': 'female', '1': 'male'}, \n    'fbs': {'0': 'false', '1': 'true'}, \n    'exng': {'0': 'no', '1': 'yes'},\n    'cp': {'0': 'typical angina', '1': 'atypical angina', '2': 'non-anginal pain', '3': 'asymptomatic'},\n    'restecg': {'0': 'normal', '1': 'having ST-T wave abnormality', '2': 'showing probable or definite left ventricular hypertrophy'},\n    'caa': {'0': '0 vessels', '1': '1 vessels', '2': '2 vessels', '3': '3 vessels', '4': '4 vessels'}, \n    'slp': {'0': '0', '1': '1', '2': '2'},\n    'thall': {'0': '0', '1': '1', '2': '2', '3': '3'}, \n}","608858b6":"df_heart[continous_cols].describe().T","a1de337f":"mycolors = ['red', 'blue', 'brown', 'orange']","bc5e12c9":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    data = df_heart[x]\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(f'Distribution of {x} variable', fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.ylabel('Count', fontsize=16)\n    sns.histplot(data, bins = 50, kde=50);\n    cnt += 1","e3a03929":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.ylabel('Density', fontsize=16)\n    sns.kdeplot(data=df_heart, x=x, hue=\"output\", fill=True, common_norm=False, alpha=.5, linewidth=0);\n    cnt += 1","f0f26830":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    data = df_heart[x]\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    sns.boxplot(data = data);\n    sns.despine(offset=10, trim=True);\n    cnt += 1","1168b220":"cnt = 0\nmax_in_row = 1\nfor x in categorical_cols:\n    val1 = df_heart[x].value_counts().index\n    val1 = [name_change[x][str(val)] for val in val1]\n    cnt1 = df_heart[x].value_counts().values\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.bar(val1, cnt1, color=mycolors);\n    cnt += 1","03dc9c3a":"cnt = 0\nmax_in_row = 1\nfor x in categorical_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    sns.kdeplot(data=df_heart, x=x, hue=\"output\", fill=True, common_norm=False, alpha=.5, linewidth=0,);\n    cnt += 1","ba003f87":"f1 = df_heart['output'].map(lambda x:  '1 = more chance of heart attack' if x == 1 else '0 = less chance of heart attack')\n\nplt.figure(figsize=(18,10))\nval = f1.value_counts().index\ncnt = f1.value_counts().values\n\nplt.title('Count of the target', size=20)\nplt.tick_params(labelsize=16)\nplt.ylabel('Count', size=16)\nplt.xlabel('output', size=16)\nplt.bar(val, cnt, color = mycolors);\nplt.show()","1e6e3901":"plt.figure(figsize = (24, 24))\nsns.heatmap(df_heart.corr(), cmap = \"coolwarm\", annot=True, fmt='.1f', linewidths=0.1);\nplt.yticks(rotation=0, size=16)\nplt.xticks(size=16)\nplt.title('Correlation Matrix', size=26)\nplt.show()","9c411886":"plt.figure(figsize = (24, 24))\nsns.heatmap(df_heart.corr()>=0.4, cmap = \"coolwarm\", annot=True, fmt='.1f', linewidths=0.1);\nplt.yticks(rotation=0, size=16)\nplt.xticks(size=16)\nplt.title('Correlation Matrix', size=26)\nplt.show()","ceb93fd7":"sns.pairplot(df_heart, hue='output');","a598af6c":"# df_heart = pd.get_dummies(df_heart, columns = categorical_cols)\n\nX = df_heart.drop(['output'],axis=1)\ny = df_heart['output']\n\n# split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2021)\n\nX_train_raw = X_train.copy()\nX_test_raw = X_test.copy()\n\nX_train_norm = X_train.copy()\nX_test_norm = X_test.copy()\n\nX_train_stand = X_train.copy()\nX_test_stand = X_test.copy()\n\nX_train_own = X_train.copy()\nX_test_own = X_test.copy()\n\nscaler = StandardScaler()\nX_train_stand[continous_cols] = scaler.fit_transform(X_train_stand[continous_cols])\nX_test_stand[continous_cols] = scaler.transform(X_test_stand[continous_cols])\n\nnorm = MinMaxScaler()\nX_train_norm[continous_cols] = norm.fit_transform(X_train_norm[continous_cols])\nX_test_norm[continous_cols] = norm.transform(X_test_norm[continous_cols])\n\nX_train_own[\"age\"]= np.log(X_train.age)\nX_train_own[\"trtbps\"]= np.log(X_train.trtbps)\nX_train_own[\"chol\"]= np.log(X_train.chol)\nX_train_own[\"thalachh\"]= np.log(X_train.thalachh)\n\nX_test_own[\"age\"]= np.log(X_test.age)\nX_test_own[\"trtbps\"]= np.log(X_test.trtbps)\nX_test_own[\"chol\"]= np.log(X_test.chol)\nX_test_own[\"thalachh\"]= np.log(X_test.thalachh)","8bc39e19":"def train_model(model, X, y):\n    model.fit(X, y)\n    return model\n\n\ndef predict_model(model, X, proba=False):\n    if ~proba:\n        y_pred = model.predict(X)\n    else:\n        y_pred_proba = model.predict_proba(X)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n\n    return y_pred\n\n\nlist_scores = []\n\ndef run_model(name, model, X_train, X_test, y_train, y_test, fc, proba=False):\n    print(name)\n    print(fc)\n    \n    model1 = train_model(model, X_train, y_train)\n    y_pred = predict_model(model1, X_test, proba)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print(y_pred,'\\n')\n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    print(classification_report(y_test, y_pred))\n    \n    \n    plot_confusion_matrix(model, X_test, y_test, cmap='Blues');    \n    plt.show()\n    plot_learning_curve(model, X_train, y_train, cv=3);    \n    plt.show()\n    \n    list_scores.append({'Model Name': name, 'Feature Scaling':fc, 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1':f1})","642a9a34":"feature_scaling = {\n    'Raw':(X_train_raw, X_test_raw, y_train, y_test),\n    'Normalization':(X_train_norm, X_test_norm, y_train, y_test),\n    'Standardization':(X_train_stand, X_test_stand, y_train, y_test),\n    'Own':(X_train_own, X_test_own, y_train, y_test),\n}","a24d9590":"model_svc = SVC(kernel='linear', C=1, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('SVC', model_svc, X_train, X_test, y_train, y_test, fc_name)","e51b65c4":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    svm = SVC()\n    parameters = { 'C':np.arange(1,5,1),'gamma':[0.001, 0.005, 0.01, 0.05, 0.09, 0.1, 0.2, 0.5,1]}\n    searcher = GridSearchCV(svm, parameters)\n    \n    run_model('Tuning SVC', searcher, X_train, X_test, y_train, y_test, fc_name )","9ea2e5eb":"logreg = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('Logistic Regression', logreg, X_train, X_test, y_train, y_test, fc_name, proba=True)","350fe587":"for fc_name, value in feature_scaling.items():\n    scores_1 = []\n    X_train, X_test, y_train, y_test = value\n    \n    for i in range(1,50):\n        knn = KNeighborsClassifier(n_neighbors = i)\n        knn.fit(X_train, y_train)\n        \n        scores_1.append(accuracy_score(y_test, knn.predict(X_test)))\n    \n    max_val = max(scores_1)\n    max_index = np.argmax(scores_1) + 1\n    \n    knn = KNeighborsClassifier(n_neighbors = max_index)\n    knn.fit(X_train, y_train)\n\n    run_model(f'KNeighbors Classifier n_neighbors = {max_index}', knn, X_train, X_test, y_train, y_test, fc_name)","3013affb":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    \n    dt = DecisionTreeClassifier()\n    \n    parameters = { 'max_depth':np.arange(1,5,1),'random_state':[2021]}\n    searcher = GridSearchCV(dt, parameters)\n    \n    run_model('DecisionTree Classifier', searcher, X_train, X_test, y_train, y_test, fc_name )","8ce68d19":"rf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('RandomForest Classifier', rf, X_train, X_test, y_train, y_test, fc_name)","2c1f1fc5":"gbt = GradientBoostingClassifier(n_estimators = 50, max_depth=2, subsample=0.8, max_features=0.2, random_state=2021)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('GradientBoosting Classifier', gbt, X_train, X_test, y_train, y_test, fc_name)","86aeb22b":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    xgb_model = xgb.XGBClassifier(n_estimators = 50, max_depth=3, random_state=2021, use_label_encoder=False, eval_metric='mlogloss')\n        \n    run_model('XGBoost Classifier', xgb_model, X_train, X_test, y_train, y_test, fc_name)","c503e6b0":"lgbm_model = lgbm.LGBMClassifier(max_depth = 3, n_estimators=50, subsample=0.8, random_state=2021)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('Lightgbm Classifier', lgbm_model, X_train, X_test, y_train, y_test, fc_name)","6e6566de":"cat_model = ctb.CatBoostClassifier(n_estimators = 80, depth=3, subsample=0.8, random_state=2021, verbose=0)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n\n    run_model('CatBoost Classifier',cat_model, X_train, X_test, y_train, y_test, fc_name)","0a258b83":"def callbacks(name): \n    return [ \n        EarlyStopping(monitor = 'loss', patience = 7), \n        ReduceLROnPlateau(monitor = 'loss', patience = 4), \n        ModelCheckpoint(f'..\/working\/{name}.hdf5', save_best_only=True) # saving the best model\n    ]\n\ndef draw_learning_curve(history, keys=['accuracy', 'loss']):\n    plt.figure(figsize=(20,8))\n    for i, key in enumerate(keys):\n        plt.subplot(1, 2, i + 1)\n        sns.lineplot(x = history.epoch, y = history.history[key])\n        sns.lineplot(x = history.epoch, y = history.history['val_' + key])\n        plt.title('Learning Curve')\n        plt.ylabel(key.title())\n        plt.xlabel('Epoch')\n        plt.legend(['train', 'test'], loc='best')\n    plt.show()","f29170e7":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    X_train_nn, y_train_nn = X_train, y_train\n    \n    X_test_nn, y_test_nn = X_test, y_test\n    y_train_nn = to_categorical(y_train_nn)\n    y_test_nn = to_categorical(y_test_nn)\n    num_feats = X_train_nn.shape[1]\n    num_classes = 2\n\n    model = Sequential([\n\n            Dense(700, input_dim = num_feats, activation='relu'),\n            Dropout(0.7),\n            BatchNormalization(),\n\n            Dense(num_classes, activation='softmax')\n        ])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n#     model.summary()\n\n    learning_history = model.fit(X_train_nn, y_train_nn,\n              batch_size = 32, epochs = 100, verbose = 0,\n              callbacks = callbacks('mlp'),\n              validation_data = (X_test_nn, y_test_nn)\n            );\n    \n    model = load_model('..\/working\/mlp.hdf5')\n    y_pred = model.predict(X_test_nn, verbose = 0)\n    y_pred = np.argmax(y_pred, axis = 1)\n\n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n\n    print(y_pred,'\\n')\n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    print(classification_report(y_test, y_pred))\n\n    draw_learning_curve(learning_history)\n\n    list_scores.append({'Model Name': 'Neural Network', 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1':f1, 'Feature Scaling':fc_name})","7ef57455":"df_scores = pd.DataFrame(list_scores)\ndf_scores.style.highlight_max(color = 'lightgreen', axis = 0)","d0acdade":"model_svc = SVC(kernel='linear', C=1, random_state=2021)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('SVC', model_svc, X_train, X_test, y_train, y_test, fc_name)","3f2a8917":"model_svc = SVC(kernel='linear', C=1, random_state=2021)\nmodel_svc.fit(X_train, y_train)\n\nimp = PermutationImportance(model_svc, random_state = 2021).fit(X_train, y_train)\neli5.show_weights(imp, feature_names = X_train.columns.values, top = 15)","eef70e0f":"## Summary","6d770b70":"# The Purpose of notebook\n\nIn this notebook, I will analyze a dataset of people who have been tested for heart disease.\n\nLink to [Orginal dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease)\n","b614ab63":"# About this dataset\n- `Age` : Age of the patient\n\n- `Sex` : Sex of the patient (0 = female; 1 = male)\n\n- `exng`: exercise induced angina (1 = yes; 0 = no)\n\n- `caa`: number of major vessels (0-3)\n\n- `cp` : Chest Pain type\n\n    - Value 0: typical angina\n    - Value 1: atypical angina\n    - Value 2: non-anginal pain\n    - Value 3: asymptomatic\n    \n    \n- `trtbps` : resting blood pressure (in mm Hg)\n\n- `chol` : cholestoral in mg\/dl fetched via BMI sensor\n\n- `fbs` : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n- `restecg` : resting electrocardiographic results\n\n    - Value 0: normal\n    - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n    \n    \n- `thalachh` : maximum heart rate achieved\n\n- `slp`: the slope of the peak exercise ST segment\n    - Value 0: upsloping\n    - Value 1: flat\n    - Value 2: downsloping\n\n- `oldpeak`:  ST depression induced by exercise relative to rest\n\n- `thall`: Thallium Stress Test result ~ (0,3)\n\n- `target` : 0 = less chance of heart attack, 1 = more chance of heart attack\n\n","63752fa3":"## Additionaly we check Neural Network","e12d4ec7":"### We can see that our previous conclusions concur with the importance of features with the model.","508d10cc":"## Conclusion\n1.`sex`:\n   * Male (`sex` = 1 ) has higher chance of heart attack\n   \n2.`cp`:\n   * People with non-anginal pain (`cp` = 2 ) have higher chances of heart attack.\n   \n3.`restecg`:\n   * People with having ST-T wave abnormality (`restecg` = 1 ) have higher chance of heart attack.\n\n4.`exng`:\n   * People with no exercise induced angina (`exng` = 0 ) have higher chance of heart attack.\n\n5.`slp`:\n   * People with the downslope of the peak exercise ST segment (`slp` = 2 ) have higher chance of heart attack.\n\n6.`caa`:\n   * People with 0 major vessels have a very higher chance of heart attack \n\n7.`thall`:\n   * People with thall = 2 have higher chance of heart attack","cb4245a3":"## Barplot of the categorical features","7e318adf":"#### We learned a lot of interesting knowledge about heart disease.","0c796178":"# Fact\n**Heart disease** is the leading cause of death for men, women, and people of most racial and ethnic groups in the United States.\n\nOne person dies every **36** seconds in the United States from cardiovascular disease.\n\nAbout 655,000 Americans die from **heart disease** each year\u2014that's 1 in every 4 deaths.\n\nReferences:\n- [ https:\/\/www.cdc.gov\/heartdisease\/facts.htm ]","c9c1cf62":"## Boxplot of continuous features","062a9a6d":"#### Creating and training NN","8c34b0a7":"#### I divide features to categorical, continous and label columns","f3469085":"#### There are more men than women in the data set","add423d4":"## Imports libs ","68fade23":"## Pairplot according to target variable","0a425b2c":"# Training model","b27cef0d":"### A functions that makes life easier","4dd89709":"## Running some models on this data","0f6abee0":"## Correlation Matrix","5b86d5f5":"#### There are no NaN values in the dataset.","7cfcb9c7":"## Sample data","7ca03a96":"#### As we can see, the variables weekly correlate with each other","748848c6":"# Heart Attack Analysis","f78e61dd":"## Checking missing values","b1685a88":"## Conclusion\n1.`age`:\n   * Most people get a heart attack at the age of 50.\n   \n2.`chol`:\n   * People with higher cholesterol are less likely to get a heart attack.\n   \n3.`thalachh`:\n   * People with a higher maximum heart rate are more likely to have a heart attack.\n","aacc7d1e":"#### I would love to know your comments and note about this.\n\n#### If you liked it, make sure to vote :)\n\n#### I'm going to make the next notebook soon.","e466ebc5":"#### Colors to charts","c9d21f59":"## Count of the target ","f6cb8854":"#### Most important features for model","4ba83d63":"## Summary scores","7c6b9e57":"## Checking duplicates","d0b908df":"### Distribution of continuous features","a5783121":"# Exploratory data analysis","d6905a57":"## Removing duplicate","29f2e43b":"![](https:\/\/www.mainstreetfamilycare.com\/wp-content\/uploads\/2018\/10\/iStock-heart-health.jpg)","72356bbc":"#### Make one-hot encoding for caterical columns and simple scaler train data","1cfb6dbc":"### The best model for this dataset - `SVC`","6199f841":"## Statistics continous columns","4148cd1f":"## The size of the dataset","1346548c":"#### `draw_learning_curve` - function to drawing learning curve history learning neural network\n#### `callbacks` - function to generate unique callback to NN","87ed10bf":"## Basic info about data","050090d1":"## Loading the dataset","ac89cbfb":"#### In dataset we have more cases with option `1`","de8b6931":"<font size=\"6\">\n    <div style=\"text-align: center\"> <b> Author <\/b> <\/div>\n<\/font>\n\n<font size=\"5\">\n    <div style=\"text-align: center\"> J\u0119drzej <\/div>\n    <div style=\"text-align: center\"> Dudzicz <\/div>\n<\/font>"}}