{"cell_type":{"3a0ddbc1":"code","b938a2ac":"code","df2a0a81":"code","a4f741cf":"code","afb3bad6":"code","d31e18a3":"code","b4db6da5":"code","6c9cc750":"code","c15cba4c":"code","4f051ec9":"code","a55503b6":"code","838f0a9d":"code","ba3df16c":"code","718f2037":"code","f7092c9a":"code","6dc8b39b":"code","8ea4ab34":"code","d05cedb4":"code","8fc0ff84":"code","4353d1a2":"code","fdf53d56":"code","cc66dd20":"code","4eba759b":"code","43799dba":"code","9526e05c":"code","f50a195b":"code","fee277fb":"code","4c7b5fb7":"code","4b7e5c61":"code","4bfcd8c1":"code","d5bb4941":"code","ffb98f83":"code","b88a72d4":"code","9b26f04f":"code","f38ecf75":"code","fde582a7":"code","dfee45b3":"code","6bdc93f5":"code","c0beb9bf":"code","164ef70d":"code","fca26f47":"code","5f1effb4":"code","af3d30eb":"code","6a686daa":"markdown","51cb4f11":"markdown","9718068d":"markdown","8d70565a":"markdown","ab17e669":"markdown","e3375567":"markdown","4ec855c3":"markdown","bebfa0e2":"markdown","56c63222":"markdown","26048410":"markdown","a2440fee":"markdown","ba0d7a8e":"markdown","9d5c3161":"markdown","a12a52df":"markdown","2a2056f6":"markdown","6866cd63":"markdown","bca17587":"markdown","156c672e":"markdown","2cc66ddb":"markdown","ac7a67c7":"markdown","9e1805a1":"markdown","ed2ff453":"markdown","52db9d73":"markdown","6dc9012e":"markdown","4a699fc2":"markdown","5cd76118":"markdown","69d21c04":"markdown","4c268dc1":"markdown","b609f408":"markdown","c4d6d19b":"markdown","9bb36a34":"markdown","0b78710a":"markdown","7dce33f8":"markdown","1262925f":"markdown","b24d157f":"markdown","2cad7aa4":"markdown","7243ec3a":"markdown","b2476422":"markdown","4833b3de":"markdown","b97cb7d5":"markdown","9dc87658":"markdown","25be1c5a":"markdown","93d59b60":"markdown","d624a145":"markdown","17f99045":"markdown","8dbd8c36":"markdown"},"source":{"3a0ddbc1":"import pandas as pd\n\n# Import all of my data\nsales_train = pd.read_csv(\"..\/input\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nitems = pd.read_csv(\"..\/input\/items.csv\")\nitem_categories = pd.read_csv(\"..\/input\/item_categories.csv\")\nshops = pd.read_csv(\"..\/input\/shops.csv\")\n\n\n# This step just helps me loop over these variables in the future\n# I'm lazy and always try to write less code\nfrom collections import namedtuple\nDataset = namedtuple(\"Dataset\", \"name df\")\ndatasets = [Dataset(name=\"Sales train\", df=sales_train),\n            Dataset(name=\"Test\", df=test),\n            Dataset(name=\"Items\", df=items),\n            Dataset(name=\"Item categories\", df=item_categories),\n            Dataset(name=\"Shops\", df=shops)]","b938a2ac":"# Get the size info\nfor d in datasets:\n    print(d.name + \": \" + str(d.df.shape))","df2a0a81":"sales_train.sample(random_state=4)","a4f741cf":"test.sample(random_state=4)","afb3bad6":"items.sample(random_state=4)","d31e18a3":"item_categories.sample(random_state=4)","b4db6da5":"shops.sample(random_state=4)","6c9cc750":"# Get the names of columns\nprint(\"Column names for each dataset\")\nfor d in datasets:\n    print('{:<16}'.format(d.name + \":\"), end=\" \")\n    print(*d.df.columns.tolist(), sep=\", \")","c15cba4c":"# Get some samples of values\nfor d in datasets:\n    cols = d.df.columns\n    print(d.name + \" column examples\")\n    for c in cols:\n        print('    ' + c + \": \", end = \" \")\n        print(*d.df[c].sample(5, random_state=4), sep=\"; \")","4f051ec9":"# Check the types\nfor d in datasets:\n    print(\"\u2022 \" + d.name + \" types\")\n    print(d.df.dtypes, end=\"\\n\\n\")","a55503b6":"# Convert date to a datetime\nsales_train[\"date\"] = pd.to_datetime(sales_train[\"date\"], format='%d.%m.%Y')","838f0a9d":"# Check if all of the item count days are ints\nall(sales_train[\"item_cnt_day\"] == sales_train[\"item_cnt_day\"].astype(int))","ba3df16c":"# Looks for NAs\nfor d in datasets:\n    print(\"\u2022 \" + d.name + \" number of missing values\")\n    print(d.df.isna().sum(), end=\"\\n\\n\")","718f2037":"# Looks for duplicates\nfor d in datasets:\n    print(d.name + \" any duplicates?\", end=\" \")\n    print(d.df.duplicated().any())","f7092c9a":"sales_train[sales_train.duplicated(keep=False)]","6dc8b39b":"sales_train = sales_train.drop_duplicates(keep='first')","8ea4ab34":"def merge_data(df):\n    \"\"\"Merges data from the inputed dataframe to return shop and item details.\n    \n    Paramters\n    ---------\n    df : DataFrame\n        Train or test dataframe to merge lookup values to\n    \n    Returns\n    -------\n    DataFrame\n        Inputted dataframe with columns shop_name, item_name,\n            item_category_id, and item_category_name appended to it\n    \n    Raises\n    ------\n    Exception\n        If there is a value in the inputted df that is not present in the lookup dataframes\n    \"\"\"\n    # validate ensures that the keys are unique in the left dataset\n    shop_merge = pd.merge(df, shops,\n                          on=\"shop_id\",\n                          how=\"left\",\n                          validate=\"m:1\",\n                          indicator=\"_shop_merge\")\n    # This exception catches if the lookup could not be found in the right datatset\n    if any(shop_merge[\"_shop_merge\"]==\"left_only\"):\n        raise Exception(\"Could not lookup value for shop\")\n\n    item_merge = pd.merge(shop_merge, items, \n                          on=\"item_id\", \n                          how=\"left\", \n                          validate=\"m:1\", \n                          indicator=\"_item_merge\")\n    if any(item_merge[\"_item_merge\"]==\"left_only\"):\n        raise Exception(\"Could not lookup value for item\")\n    \n    item_cat_merge = pd.merge(item_merge, item_categories, \n                              on=\"item_category_id\", \n                              how=\"left\", \n                              validate=\"m:1\", \n                              indicator=\"_cat_merge\")\n    if any(item_cat_merge[\"_cat_merge\"]==\"left_only\"):\n        raise Exception(\"Could not lookup value for item category\")\n\n    # Drop merge columns\n    item_cat_merge = item_cat_merge.drop(columns=[\"_shop_merge\", \"_item_merge\", \"_cat_merge\"])\n    return item_cat_merge \n\nsales_train_merge = merge_data(sales_train)\ntest_merge = merge_data(test)","d05cedb4":"sales_train_merge.sample(random_state=4)","8fc0ff84":"from matplotlib import pyplot\nsales_train_merge.plot.scatter(x=\"date_block_num\", y=\"item_price\")\npyplot.show()","4353d1a2":"sales_train_merge.loc[sales_train_merge[\"item_price\"]>300000]","fdf53d56":"sales_train_merge.loc[sales_train_merge[\"item_id\"]==6066]","cc66dd20":"sales_train_merge.plot.scatter(x=\"date_block_num\", y=\"item_cnt_day\")\npyplot.show()","4eba759b":"sales_train_merge.loc[sales_train_merge[\"item_cnt_day\"] > 500].sort_values(\"item_cnt_day\", ascending=False)","43799dba":"print(\"Min date \", min(sales_train_merge[\"date\"]))\nprint(\"Max date \", max(sales_train_merge[\"date\"]))","9526e05c":"(sales_train_merge[\"item_cnt_day\"] >= 0 ).all()","f50a195b":"sales_train_merge.loc[sales_train_merge[\"item_cnt_day\"] < 0]","fee277fb":"sales_train_merge.loc[sales_train_merge[\"item_cnt_day\"]==0]","4c7b5fb7":"sales_train_merge.loc[sales_train_merge[\"item_price\"] < 0 ]","4b7e5c61":"sales_train_merge.loc[(sales_train_merge[\"item_id\"] == 2973 ) & (sales_train_merge[\"shop_id\"] == 32 )]","4bfcd8c1":"sales_train_merge.loc[sales_train_merge[\"item_price\"] < 0, \"item_price\"] = None\nsales_train_merge[\"item_price\"] = sales_train_merge[\"item_price\"].fillna(sales_train_merge[\"item_price\"].median())","d5bb4941":"months = sales_train_merge[\"date\"].dt.month - 1\nyears = (sales_train_merge[\"date\"].dt.year - 2013)*12\n\nmy_dateblock = months + years\nall(my_dateblock == sales_train_merge[\"date_block_num\"])","ffb98f83":"train_set = set([k for k in sales_train_merge[[\"shop_id\", \"item_id\"]].itertuples(index=False)])\ntest_set = set([k for k in test_merge[[\"shop_id\", \"item_id\"]].itertuples(index=False)])\nnew = test_set - train_set\nprint(\"New or never sold item + shop combinations in test\", len(new), \"out of\", len(test_set))","b88a72d4":"test_merge.columns","9b26f04f":"sales_train_merge[[\"item_price\", \"item_cnt_day\"]].describe()","f38ecf75":"sales_train_agg = sales_train_merge.groupby([\"item_id\", \"date_block_num\"])[\"item_cnt_day\"].sum()\nsales_train_agg.describe()","fde582a7":"pyplot.figure(1, figsize=(12,6))\npyplot.suptitle(\"Distribution of units sold\")\npyplot.subplot(121)\nxlims = (-20, 100)  # set x-limits\nsales_train_agg.hist(range=xlims, bins=12)\npyplot.subplot(122)\nsales_train_agg.to_frame().boxplot()\npyplot.show()","dfee45b3":"sales_train_agg.to_frame().boxplot()\nax = pyplot.gca()\nax.set_ylim(xlims)\npyplot.show()","6bdc93f5":"print(\"Number of unique shops: {0:d}\".format(sales_train_merge[\"shop_id\"].nunique()))\nprint(\"Number of unique item names: {0:d}\".format(sales_train_merge[\"item_name\"].nunique()))\nprint(\"Number of unique item ids: {0:d}\".format(sales_train_merge[\"item_id\"].nunique()))\nprint(\"Number of unique categories: {0:d}\".format(sales_train_merge[\"item_category_id\"].nunique()))","c0beb9bf":"pyplot.figure(figsize=(16,8))\nsales_train_merge.groupby(\"date\")[\"item_cnt_day\"].sum().plot()\npyplot.show()","164ef70d":"pyplot.figure(figsize=(16,8))\nsales_train_merge.groupby(\"date_block_num\")[\"item_cnt_day\"].sum().plot()\npyplot.show()","fca26f47":"pyplot.figure(figsize=(16,8))\nsales_train_merge.groupby(pd.Grouper(key=\"date\", freq=\"A\"))[\"item_cnt_day\"].sum().plot.bar()\npyplot.show()","5f1effb4":"# There's too many to plot, so let's just look at top-selling categories\ntopselling = sales_train_merge.groupby(\"item_category_id\")[\"item_cnt_day\"].sum().nlargest(10).index\nsales_topselling = sales_train_merge.loc[sales_train_merge[\"item_category_id\"].isin(topselling)]\nsales_topselling.groupby([\"date_block_num\", \"item_category_id\"])[\"item_cnt_day\"].sum().unstack().plot(figsize=(16,8))\npyplot.show()","af3d30eb":"sales_topselling[\"item_category_name\"].unique()","6a686daa":"This makes the spikes and trend even more apparent and less noisy. Even more apparent (and always good to know how to use grouper in pandas):","51cb4f11":"Definitely make sure you don't save anything with an ASCII encoding or it will screw up on the Russian characters","9718068d":"Hmm, no?","8d70565a":"For better or for worse, these are pretty slim datasets in terms of number of features. Thus, if we want to use some ML models as opposted to time series, we may want to do a fair amount of feature engineering.","ab17e669":"These seem fine. Now what about the item_cnt_day? Are they all positive?","e3375567":"# What information is contained in each dataset?\n\n## Size and Format\n\nWe have been given 6 datasets, all as csv's, I downloaded and unzipped them all to get the size.\n\n| Dataset Name  | Size (unzipped) | \n|---------------|-----------------|\n|sales_train    |94.6 MB          |\n|test           |3.2 MB           |\n|items          |1.6 MB           |\n|item_catgories |4 KB             |\n|shops          |3 KB             |\n\nGreat news! I can do all of the processing locally probably and just use the data in the raw csv format. That's a relief.\n\nOkay, now let's get it into Python. Of course, I'll use pandas to start exploring. If you don't know pandas very well, you can pick up [Wes McKinney's book](https:\/\/www.amazon.com\/Python-Data-Analysis-Wrangling-IPython\/dp\/1491957662).","4ec855c3":"Let's quickly calculate some basic statistics using pandas's describe function","bebfa0e2":"Sure enough, date is an object instead of a datetime. Also, I'm a little curious that item_cnt_day is a float instead of an int. How can you sell just half an item? I will check my intuition","56c63222":"Now, I mentioned that we could merge these datasets together, let's do that and make sure we wouldn't be missing any data between the datasets. E.g. if the sales_train dataset had a shop_id = 9000, but there was not shop 9000 in the shops dataset, then I will be missing some info.\n\nThe following code will merge the train and test datasets to the lookup datasets items, shops, and item_categories. It will raise exceptions if a lookup fails.","26048410":"I noticed 'Grand Theft Auto V' in there, the release date of which was [14 April 2015](https:\/\/support.rockstargames.com\/articles\/202423276\/Grand-Theft-Auto-V-PC-Release-Date-And-Time), so I guess that explains the high quantity that day. Clearly a new release of an item will lead to large sales. Let's keep that in mind when building our model.\n\nThe item category 80 is for service tickets, so I expect this was probably when tickets were first released.\n\nThe top-selling item is for some delivery service: https:\/\/boxberry.ru\/en\/\n\n\nOkay, most of these outliers seem defensible so I'm not going to modify anything","a2440fee":"There are so many ways to view this data, by shop, by item id, by category, by day, by month, etc. Let's start with the big picture and view the number of units sold across time.","ba0d7a8e":"Now, let's check some assumptions. First, let's make sure the dates appear to be in the correct range","9d5c3161":"We can also see on these plots above that very few items sell more than 20 items in a month.","a12a52df":"It seems like there are probably some returns in there. It is good to know that. Sometimes, it may make sense to predict returns and sales separately. In this case, all of the data has been aggregated together so we won't be able to do that. ","2a2056f6":"3\/4 of items are price below 1000. It also looks most items only set 1 per shop per day. It probably makes more sense to look at this information at a monthly level across all stores since that's what we need to forecast.","6866cd63":"These correspond to \n'Cinema - Blu-Ray', 'Music - Local Production CD',\n'Games - XBOX 360', 'Games - PS3',\n'PC Games - Additional Editions',\n'PC Games - Standard Editions', 'Cinema DVD',\n'Gifts - Board Games (Compact)',\n'Gifts - Bags, Albums, Mousepad', 'Games - PS4'\n\nThanks to Google Translate","bca17587":"This is some kind of software program. Maybe it was just a mistake? Let's see if there are any other entries","156c672e":"I'm going to make sure that the dateblock values are correctly calculated before I use them for anything","2cc66ddb":"There are huge spikes around December. There is definitely some seasonal variation. There is an overall decline in sales. Let's see what it looks like when we view it from a monthly view.","ac7a67c7":"## Examples\nLook at some examples of rows so we have some sense of what we will be looking at","9e1805a1":"It's kind of hard to read the boxplot, so let's zoom in closer","ed2ff453":"No other entries, a quick google finds the software's site: http:\/\/www.radmin.com\/ordering\/, and looking at the conversion of 1 USD to RUB, it looks like 300k is actually reasonable for over 500 licenses. That's like $5k USD. Okay, so I guess it's fine. ","52db9d73":"That's a big proportion. It may just be because most of these items never sold previously at those stores, but it will be trickier if it is because a store never carried them before. If it's the latter, then we might want to forecast at a higher level than item + shop.","6dc9012e":"Hope this helped you consider some new things to look for when doing EDA! Happy modeling!","4a699fc2":"Part of this can be explained that the data for 2015 ends in October and doesn't include the final months, and we have seen how sales tend to spike in December.\n\nNow let's see sales at a category-level","5cd76118":"Okay, I am satisifed that the item_cnt_day is always whole integer numbers. ","69d21c04":"Great! Everything merged without issues and I'm not missing any keys. I'm going to be using these datasets going forward. Here's a quick preview:","4c268dc1":"What?! There's an item with a price over 300k?","b609f408":"Going back to the [Data documentation](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/data), I know what these fields are\n\n|Source         |Column            |Description\n|---------------|------------------|-----------\n|sales_train    |date              |date of sales presumably, in dd.mm.yyyy\n|sales_train    |date_block_num    |consecutive month number. January 2013 is 0, February 2013 is 1\n|sales_train    |shop_id           |unique identifier of a shop\n|sales_train    |item_id           |unique identifier of a product\n|sales_train    |item_price        |current price of an item\n|sales_train    |item_cnt_day      |number of products sold\n|test           |ID                |an ID that represents a (shop,item) tuple in test set\n|test           |shop_id           |unique identifier of a shop\n|test           |item_id           |unique identifier of a product\n|items          |item_name         |item name\n|items          |item_id           |unique identifier of a product\n|items          |item_category_id  |unique identifier of item category\n|item_categories|item_category_name|name of item category\n|item_categories|item_category_id  |unique identifier of item category\n|shops          |shop_name         |shop name\n|shops          |shop_id           |unique identifier of a shop\n\nSo great! No descriptions are missing and I more or less know what everything is. My only confusion was the ID in the test is not a concatenation of the shop id and item id. It's just a different unique identifier.\n\nThe predictor value in this case will be *item_cnt_day*. Note that predictions need to be at a monthly level per item. The train data looks like the test set but the test set does not have item price, so we need to consider that before building a model.\n\nIt looks like my sales_train and test can be merged with items, item_categories, and shops to get the shop name, item category name, and item name if I want it.","c4d6d19b":"## Plots","9bb36a34":"## Quality check\n\n**Never assume that the data is clean cause it's probably not**. You're probably going to spend more time cleaning up data than modeling, but it will probably improve your results more than a model ever could. Don't skimp on this step!","0b78710a":"Now this is not the plotting section, but I'm going to use scatterplot to find outliers for *item_cnt_day* and *item_price*, if there are any. Boxplots could also be helpful here too but I am trying to view the data as rawly as possible for now","7dce33f8":"Darn, let's figure out what this\/these duplicate(s) is\/are","1262925f":"Looks like there we don't have information about items where nothing is sold, so it's not clear if it's because the item wasn't carried or if the item just didn't sell.","b24d157f":"## Statistics","2cad7aa4":"Let's look closer at some of these values","7243ec3a":"In my job, I would maybe try to track down or talk to the team who possesses this data to ask why there are duplicates. Given, this is a kaggle competition, it's hard to ask someone. But I can make one of two decisions: either this is duplicated rows, and item 20130 only sold 1 item on 05.01.2013 in shop 54, or it actually sold 2 and it wasn't aggregated correctly. I'm going to assume it's just a duplicate and drop these extra rows.","b2476422":"## Column fields","4833b3de":"AMAZING!!!","b97cb7d5":"# A Robust and Thorough Exploratory Data Analysis\nAfter reading the rules of the challenge and adhering to the terms, the beginning of every challenge should start with setting up your environment (I usually use [miniconda](https:\/\/docs.conda.io\/en\/latest\/miniconda.html)), directory structure, and obtaining all of the needed data.\n\n_**You should then spend at least several hours getting to know the data very well. This will save you time in the long run**_. It will help identify potentital data problems and might curate new modeling ideas.\n\nYou can spend a very long time on EDA, but at the very least, be able to answer the following questions:\n## What information is contained in each dataset?\n1. What is the size of the stored data? This will affect how you may store, access, and process the data. Working with MB of data vs TB require different tools and hardware.\n    1. Number of rows\n    1. Number of columns\n    1. How is it stored?\n1. Look at a couple of example rows to get a better sense of what it looks like\n1. Column fields\n    1. What are the descriptions?\n    1. Which are unique allegedly? (Always good to verify!)\n    1. Which columns will be feature variables and which one is the predictor variable\n    1. If you have multiple datasets, how can the data be joined together? Are there common columns that you can join the datasets to?\n    1. Some examples of values for each column\n    1. Types: does anything need to be converted? E.g. strings into dates, strings into floats, etc.\n    1. Any metadata explaining possible encodings? E.g. in sku numbers, the first two digits may represent a retail's department for that particular item. There may be more information contained in some of the columns.\n    \n## What is the quality?\n1. Any missing data? Can it be inputted or should it be dropped?\n1. Any duplicates?\n1. Outliers? Should they be removed or adjusted?\n1. Unusual values? E.g. negative values for price, or very old dates, etc. What assumptions can be made? What cannot be corrected?\n1. Check any assumptions if possible, are values actually unique?\n\n## What are some basic statistics? \n1. Mean\n1. Median\n1. Quartiles\n1. Count of distinct values\n\n## What does the data look like when plotted?\n1. Histograms\/Density plots\n1. Box plots\n1. Scatter plots\n1. Line plots\n\nNow let's get started! Feel free to add additional tips in the columns--I'll try to update it here.\n\n..................................................................................................................................................................................................................................","9dc87658":"Looking at the items, it seems that this store sells CDs, games, and DVDs. Google-translate of a couple of the categories shows \u041f\u043e\u0434\u0430\u0440\u043a\u0438 - \u041c\u044f\u0433\u043a\u0438\u0435 \u0438\u0433\u0440\u0443\u0448\u043a\u0438 is 'Gifts - soft toys', \u041f\u043e\u0434\u0430\u0440\u043a\u0438 - \u041e\u0442\u043a\u0440\u044b\u0442\u043a\u0438, \u043d\u0430\u043a\u043b\u0435\u0439\u043a\u0438 is 'Gifts - Cards, stickers'. So it seems to sell an array of items.\n\nMoving onwards, you have noticed at this point that the dates are in a different format than what pandas usually prints, let's check the types","25be1c5a":"Let's fill this problematic value with the median","93d59b60":"This is more helpful. From these values, we should expect that each item will only sell several units per month. The mean (15.6) is much greater than the median (4) indicating a large skew. Let's plot to visualize the distribution.","d624a145":"A good purchase","17f99045":"Swell, nothing unexpected there\n\nLast, I just want to get an idea of how many new combinations of show and items are in the test set compared to the training set","8dbd8c36":"This just doesn't seem right."}}