{"cell_type":{"e2e4921c":"code","71542283":"code","6023b4e4":"code","d2cc5503":"code","eed01587":"code","85ba62e2":"code","3a8ac904":"code","187cb5bf":"code","ce1b292c":"code","89d04fdd":"code","7ec46fef":"code","0fb61f4d":"code","967d5214":"code","95e2cdf0":"code","88a3bd89":"code","5c0045fe":"code","539513e7":"code","b12719c2":"code","14ea4f29":"code","cd618554":"code","866a1c30":"code","ac76cfe8":"code","ee242649":"code","d41fff65":"code","13f07a44":"markdown"},"source":{"e2e4921c":"#Import revelant libraries\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport re\n#Downloading revelant content\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\nfrom nltk.stem.porter import *\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n# Tensorflow (Model)\nimport tensorflow as tf","71542283":"# Renaming Columns name \ncolnames=['TweetId', 'Entity', 'Output', 'Tweet']\n\ndata = pd.read_csv('..\/input\/twitter-entity-sentiment-analysis\/twitter_validation.csv', names=colnames, header=None)","6023b4e4":"#Main list consists of cleaned data\nmain = []\n\n# Storing all punctuations using RE library like !;,\"% etc\nre_puncs = re.compile('[%s]' % re.escape(string.punctuation))\n# Storing all stop words like a, an, the, when, there, this etc\nstop_word  = set(stopwords.words('english'))\n# Making Lemmatizing object\nlem = WordNetLemmatizer()\n# Using Porter Stemmer\np_stem = PorterStemmer()\n\n# Traversing whole dataset\nfor i in tqdm(range(len(data['Tweet']))):\n    # Tokenization\n    tokens = word_tokenize(str(data['Tweet'][i]))\n    # Converting all characters to lower case\n    tokens = [w.lower() for w in tokens]\n    # Remove all punctuations from sentenses\n    tokens = [re_puncs.sub('', w) for w in tokens]\n    # Checking all words is alphabets or not\n    tokens = [i for i in tokens if i.isalpha()]\n    # Removing all stop words from the sentenses\n    tokens = [w for w in tokens if w not in stop_word]\n    # Doing Lemmatizing of words\n    tokens = [lem.lemmatize(w) for w in tokens]\n    # Stemming process\n    tokens = [p_stem.stem(w) for w in tokens]\n    # Finally convert to string\n    r = ' '.join(tokens)\n    # Storing the final string into main list\n    main.append(r)","d2cc5503":"# Display the first five rows of dataframe\ndata.head()","eed01587":"# Assigning a new column in the dataframe\ndata['Preprocess_Tweet'] = main\n# Dropping old column which consists unstructred data =\ndata = data.drop('Tweet', axis = 1)","85ba62e2":"# Display the first five rows of dataframe\ndata.head()","3a8ac904":"# Converting the main into Vector using CountVectorizer and then convert it to array\ncnt = CountVectorizer(analyzer=\"word\")\nX = cnt.fit_transform(main).toarray()","187cb5bf":"# Unique values in output column\ndata['Output'].unique()","ce1b292c":"# Checking any NULL values\ndata['Output'].isnull().sum()","89d04fdd":"# As output has four output so we convert labels to binary numbers\nenc = OneHotEncoder(handle_unknown='ignore')\nip = np.array(data['Output'])\nip = ip.reshape(-1, 1)\ny = enc.fit_transform(ip).toarray()","7ec46fef":"#Printing X (independent values)\nX","0fb61f4d":"# Priniting (Dependent values)\ny","967d5214":"# Splitting data into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)","95e2cdf0":"# Display training set (Independent Values)\nX_train","88a3bd89":"# Display training set (dependent values)\ny_train","5c0045fe":"X_train.shape","539513e7":"from sklearn.feature_extraction.text import TfidfTransformer","b12719c2":"tf_insta = TfidfTransformer()","14ea4f29":"X_train_filter = tf_insta.fit_transform(X_train).toarray()","cd618554":"X_train_filter.shape","866a1c30":"X_train_filter.shape","ac76cfe8":"y_train.shape","ee242649":"#Test model\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(1024, input_dim=3786, activation='relu'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(4, activation='softmax')\n])\nmodel.compile(\n     loss='categorical_crossentropy',\n     optimizer='adam',\n     metrics=['accuracy']\n)","d41fff65":"h = model.fit(X_train_filter, y_train, epochs=10)","13f07a44":"## Importing Dataset\n\n**Dataset consists of two types set:**\n* Training Set\n* Test or Validation Set"}}