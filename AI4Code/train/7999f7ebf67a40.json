{"cell_type":{"e57ce005":"code","7b8252a3":"code","61d8fdf9":"code","5863d504":"code","21b9f1ed":"code","400d5dba":"code","d73782b6":"code","0b27397a":"code","c358ecea":"code","4c9eb93d":"code","6379938f":"code","a0387ff8":"code","776b5ba8":"code","93e0d834":"code","02463ebe":"code","9f772411":"code","4fd92eb3":"code","00120ac4":"code","fa76a4b7":"code","48cb1943":"code","fd268fee":"code","4b621247":"code","07b0121e":"code","d958e505":"code","e62ce981":"code","6385640a":"code","abc9a8d1":"code","6baf0899":"code","e678f655":"code","222b4eac":"code","8eb99ef1":"code","48462995":"code","7b127194":"code","a81ab300":"code","95061833":"code","fc40d527":"code","195de87f":"code","c13301b8":"markdown","9140e49b":"markdown","251d4c03":"markdown","d33e656f":"markdown","9022b24b":"markdown","4f940946":"markdown","bb018ed6":"markdown","08886ab0":"markdown","948797ab":"markdown","ca7c36ff":"markdown","0a9cd6dd":"markdown","cdd5c251":"markdown","d1dc3f32":"markdown","67e442b7":"markdown","8165b434":"markdown","28d93e71":"markdown","aa30ede4":"markdown","4ed8cb93":"markdown","44fd9148":"markdown","97a6e353":"markdown","88680a95":"markdown"},"source":{"e57ce005":"from IPython.display import Image\nImage(\"\/kaggle\/input\/ufc245\/UFC 245.jpg\")","7b8252a3":"import re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.options.display.max_columns = None\npd.options.display.max_rows = None\nimport sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))","61d8fdf9":"df = pd.read_csv('\/kaggle\/input\/ufcdata\/data.csv')\n\nb_age = df['B_age']  #  we replace B_age to put it among B features \ndf.drop(['B_age'], axis = 1, inplace = True)\ndf.insert(76, \"B_age\", b_age)\n\ndf_fe = df.copy() #  We make a copy of the dataframe for the feature engineering part later\n\ndf.head(5)","5863d504":"print(df.shape)\nlen(df[df['Winner'] == 'Draw'])","21b9f1ed":"last_fight = df.loc[0, ['date']]\nprint(last_fight)","400d5dba":"limit_date = '2001-04-01'\ndf = df[(df['date'] > limit_date)]\nprint(df.shape)","d73782b6":"print(\"Total NaN in dataframe :\" , df.isna().sum().sum())\nprint(\"Total NaN in each column of the dataframe\")\nna = []\nfor index, col in enumerate(df):\n    na.append((index, df[col].isna().sum())) \nna_sorted = na.copy()\nna_sorted.sort(key = lambda x: x[1], reverse = True) \n\nfor i in range(len(df.columns)):\n    print(df.columns[na_sorted[i][0]],\":\", na_sorted[i][1], \"NaN\")","0b27397a":"from sklearn.impute import SimpleImputer\n\nimp_features = ['R_Weight_lbs', 'R_Height_cms', 'B_Height_cms', 'R_age', 'B_age', 'R_Reach_cms', 'B_Reach_cms']\nimp_median = SimpleImputer(missing_values=np.nan, strategy='median')\n\nfor feature in imp_features:\n    imp_feature = imp_median.fit_transform(df[feature].values.reshape(-1,1))\n    df[feature] = imp_feature\n\nimp_stance = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimp_R_stance = imp_stance.fit_transform(df['R_Stance'].values.reshape(-1,1))\nimp_B_stance = imp_stance.fit_transform(df['B_Stance'].values.reshape(-1,1))\ndf['R_Stance'] = imp_R_stance\ndf['B_Stance'] = imp_B_stance","c358ecea":"print('Number of features with NaN values :', len([x[1] for x in na if x[1] > 0]))","4c9eb93d":"na_features = ['B_avg_BODY_att', 'R_avg_BODY_att']\ndf.dropna(subset = na_features, inplace = True)\n\ndf.drop(['Referee', 'location'], axis = 1, inplace = True)","6379938f":"print(df.shape)\nprint(\"Total NaN in dataframe :\" , df.isna().sum().sum())","a0387ff8":"df.info()","776b5ba8":"list(df.select_dtypes(include=['object', 'bool']))","93e0d834":"print(df['B_draw'].value_counts())\nprint(df['R_draw'].value_counts())\ndf.drop(['B_draw', 'R_draw'], axis=1, inplace=True)","02463ebe":"df = df[df['Winner'] != 'Draw']\ndf = df[df['weight_class'] != 'Catch Weight']","9f772411":"plt.figure(figsize=(50, 40))\ncorr_matrix = df.corr(method = 'pearson').abs()\nsns.heatmap(corr_matrix, annot=True)","4fd92eb3":"sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1)\n                 .astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\nprint(sol[0:10])","00120ac4":"#  i = index of the fighter's fight, 0 means the last fight, -1 means first fight\ndef select_fight_row(df, name, i): \n    df_temp = df[(df['R_fighter'] == name) | (df['B_fighter'] == name)]  # filter df on fighter's name\n    df_temp.reset_index(drop=True, inplace=True) #  as we created a new temporary dataframe, we have to reset indexes\n    idx = max(df_temp.index)  #  get the index of the oldest fight\n    if i > idx:  #  if we are looking for a fight that didn't exist, we return nothing\n        return \n    arr = df_temp.iloc[i,:].values\n    return arr\n\nselect_fight_row(df, 'Amanda Nunes', 0) #  we get the last fight of Amanda Nunes","fa76a4b7":"# get all active UFC fighters (according to the limit_date parameter)\ndef list_fighters(df, limit_date):\n    df_temp = df[df['date'] > limit_date]\n    set_R = set(df_temp['R_fighter'])\n    set_B = set(df_temp['B_fighter'])\n    fighters = list(set_R.union(set_B))\n    return fighters","48cb1943":"fighters = list_fighters(df, '2017-01-01')\nprint(len(fighters))","fd268fee":"def build_df(df, fighters, i):      \n    arr = [select_fight_row(df, fighters[f], i) for f in range(len(fighters)) if select_fight_row(df, fighters[f], i) is not None]\n    cols = [col for col in df] \n    df_fights = pd.DataFrame(data=arr, columns=cols)\n    df_fights.drop_duplicates(inplace=True)\n    df_fights['title_bout'] = df_fights['title_bout'].replace({True: 1, False: 0})\n    df_fights.drop(['R_fighter', 'B_fighter', 'date'], axis=1, inplace=True)\n    return df_fights\n\ndf_train = build_df(df, fighters, 0)\ndf_test = build_df(df, fighters, 1)","4b621247":"df_train.head(5)","07b0121e":"print(df_train.shape)\nprint(df_test.shape)","d958e505":"print(len(df_train[df_train['Winner'] == 'Blue']))\nprint(len(df_train[df_train['Winner'] == 'Red']))\nprint(len(df_test[df_test['Winner'] == 'Blue']))\nprint(len(df_test[df_test['Winner'] == 'Red']))","e62ce981":"from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\nfrom sklearn.compose import make_column_transformer\n\npreprocessor = make_column_transformer((OrdinalEncoder(), ['weight_class', 'B_Stance', 'R_Stance']), remainder='passthrough')\n\n# If the winner is from the Red corner, Winner label will be encoded as 1, otherwise it will be 0 (Blue corner)\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(df_train['Winner'])\ny_test = label_encoder.transform(df_test['Winner'])\n\nX_train, X_test = df_train.drop(['Winner'], axis=1), df_test.drop(['Winner'], axis=1)","6385640a":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","abc9a8d1":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\n\n# Random Forest composed of 100 decision trees. We optimized parameters using cross-validation and GridSearch tool paired together\nrandom_forest = RandomForestClassifier(n_estimators=100, \n                                       criterion='entropy', \n                                       max_depth=10, \n                                       min_samples_split=2,\n                                       min_samples_leaf=1, \n                                       random_state=0)\n\nmodel = Pipeline([('encoding', preprocessor), ('random_forest', random_forest)])\nmodel.fit(X_train, y_train)\n\n# We use cross-validation with 5-folds to have a more precise accuracy (reduce variation)\naccuracies = cross_val_score(estimator=model, X=X_train, y=y_train, cv=5)\nprint('Accuracy mean : ', accuracies.mean())\nprint('Accuracy standard deviation : ', accuracies.std())\n\ny_pred = model.predict(X_test)\nprint('Testing accuracy : ', accuracy_score(y_test, y_pred), '\\n')\n\ntarget_names = [\"Blue\",\"Red\"]\nprint(classification_report(y_test, y_pred, labels=[0,1], target_names=target_names))","6baf0899":"#from sklearn.model_selection import GridSearchCV\n#parameters = [{'random_forest__n_estimators': [10, 50, 100, 500, 1000],\n#               'random_forest__criterion': ['gini', 'entropy'],\n#               'random_forest__max_depth': [5, 10, 50],\n#               'random_forest__min_samples_split': [2, 3, 4],\n#               'random_forest__min_samples_leaf': [1, 2, 3],\n#              }]\n#model = Pipeline([('encoding', preprocessor), ('random_forest', RandomForestClassifier())])\n\n#grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring='accuracy', cv=5, n_jobs=-1)\n#grid_search = grid_search.fit(X_train, y_train)\n#best_accuracy = grid_search.best_score_\n\n#best_params = grid_search.best_params_\n#print('Best accuracy : ', best_accuracy)\n#print('Best parameters : ', best_params)","e678f655":"from sklearn.metrics import confusion_matrix\n\n# The confusion matrix looks like the shape below:\n# [TN FN\n#  FP TP]\ncm = confusion_matrix(y_test, y_pred) \nax = plt.subplot()\nsns.heatmap(cm, annot = True, ax = ax, fmt = \"d\")\nax.set_xlabel('Actual')\nax.set_ylabel('Predicted')\nax.set_title(\"Confusion Matrix\")\nax.xaxis.set_ticklabels(['Blue', 'Red'])\nax.yaxis.set_ticklabels(['Blue', 'Red'])","222b4eac":"feature_names = [col for col in X_train]\nfeature_importances = model['random_forest'].feature_importances_\nindices = np.argsort(feature_importances)[::-1]\nn = 30 # maximum feature importances displayed\nidx = indices[0:n] \nstd = np.std([tree.feature_importances_ for tree in model['random_forest'].estimators_], axis=0)\n\n#for f in range(n):\n#    print(\"%d. feature %s (%f)\" % (f + 1, feature_names[idx[f]], feature_importances[idx[f]])) \n\nplt.figure(figsize=(30, 8))\nplt.title(\"Feature importances\")\nplt.bar(range(n), feature_importances[idx], color=\"r\", yerr=std[idx], align=\"center\")\nplt.xticks(range(n), [feature_names[id] for id in idx], rotation = 45) \nplt.xlim([-1, n]) \nplt.show()","8eb99ef1":"from sklearn.tree import export_graphviz\nfrom subprocess import call\nfrom IPython.display import Image\n\ntree_estimator = model['random_forest'].estimators_[10]\nexport_graphviz(tree_estimator, \n                out_file='tree.dot', \n                filled=True, \n                rounded=True)\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\nImage(filename = 'tree.png')","48462995":"def predict(df, pipeline, blue_fighter, red_fighter, weightclass, rounds, title_bout=False): \n    \n    #We build two dataframes, one for each figther \n    f1 = df[(df['R_fighter'] == blue_fighter) | (df['B_fighter'] == blue_fighter)].copy()\n    f1.reset_index(drop=True, inplace=True)\n    f1 = f1[:1]\n    f2 = df[(df['R_fighter'] == red_fighter) | (df['B_fighter'] == red_fighter)].copy()\n    f2.reset_index(drop=True, inplace=True)\n    f2 = f2[:1]\n    \n    # if the fighter was red\/blue corner on his last fight, we filter columns to only keep his statistics (and not the other fighter)\n    # then we rename columns according to the color of  the corner in the parameters using re.sub()\n    if (f1.loc[0, ['R_fighter']].values[0]) == blue_fighter:\n        result1 = f1.filter(regex='^R', axis=1).copy() #here we keep the red corner stats\n        result1.rename(columns = lambda x: re.sub('^R','B', x), inplace=True)  #we rename it with \"B_\" prefix because he's in the blue_corner\n    else: \n        result1 = f1.filter(regex='^B', axis=1).copy()\n    if (f2.loc[0, ['R_fighter']].values[0]) == red_fighter:\n        result2 = f2.filter(regex='^R', axis=1).copy()\n    else:\n        result2 = f2.filter(regex='^B', axis=1).copy()\n        result2.rename(columns = lambda x: re.sub('^B','R', x), inplace=True)\n        \n    fight = pd.concat([result1, result2], axis = 1) # we concatenate the red and blue fighter dataframes (in columns)\n    fight.drop(['R_fighter','B_fighter'], axis = 1, inplace = True) # we remove fighter names\n    fight.insert(0, 'title_bout', title_bout) # we add tittle_bout, weight class and number of rounds data to the dataframe\n    fight.insert(1, 'weight_class', weightclass)\n    fight.insert(2, 'no_of_rounds', rounds)\n    fight['title_bout'] = fight['title_bout'].replace({True: 1, False: 0})\n    \n    pred = pipeline.predict(fight)\n    proba = pipeline.predict_proba(fight)\n    if (pred == 1.0): \n        print(\"The predicted winner is\", red_fighter, 'with a probability of', round(proba[0][1] * 100, 2), \"%\")\n    else:\n        print(\"The predicted winner is\", blue_fighter, 'with a probability of ', round(proba[0][0] * 100, 2), \"%\")\n    return proba","7b127194":"predict(df, model, 'Kamaru Usman', 'Colby Covington', 'Welterweight', 5, True) ","a81ab300":"predict(df, model, 'Max Holloway', 'Alexander Volkanovski', 'Featherweight', 5, True) ","95061833":"predict(df, model, 'Amanda Nunes', 'Germaine de Randamie', \"Women's Bantamweight\", 5, True)","fc40d527":"predict(df, model, 'Jose Aldo', 'Marlon Moraes', 'Bantamweight', 3, False)","195de87f":"predict(df, model, 'Urijah Faber', 'Petr Yan', 'Bantamweight', 3, False)","c13301b8":"The last fight (and ufc event) recorded on this dataset was on the 8th November of 2019.","9140e49b":"Before April 2001, there were almost no rules in UFC (no judges, no time limits, no rounds, etc.). It's up to this precise date that UFC started to implement a set of rules known as \"Unified Rules of Mixed Martial Arts\" in accordance with the New Jersey State Athletic Control Board in United States. Therefore, we delete all fights before this major update in UFC's rules history. ","251d4c03":"# Data Cleaning","d33e656f":"We get a mean accuracy of 0.53 on the cross_val_score and 0.7 for the f1-score. The discrepancy between cross and test score is maybe due to the fact that the model does well in this particular test set but doesn't generalize well. At least the mean accuracy on train data is higher than a random choice (0.5) and that's a good point as MMA is a very uncertain sport. ","9022b24b":"There are 135 quantitative features and 8 categorical features. \nLet's see which features are categorical features.","4f940946":"Remind that Blue => 0 and Red => 1 for the target value (Winner).","bb018ed6":"## Predictions\n\nLet's make predictions on the next UFC event introducing one waited fight on the main card between Kamaru Usman and Colby Covington (event occuring the 15\/12\/2019, UFC 245).","08886ab0":"# Feature Engineering","948797ab":"We delete all NaN rows from na_features columns. We also drop Referee and location columns as they are useless (no predictive power) for the model we want to build. ","ca7c36ff":"# UFC prediction\n\nThe purpose of this notebook is to predict the result of UFC matches (UFC 245 event is the latest one as I'm curently writting this introduction). \n\nBut first, let's speak a little bit about the context of this dataset.  \nThe UFC is nowadays the biggest mixed martial arts competition organization in the world in terms of views and prestige. The UFC's fighters are usually considered as the best in the world, they often came from different organisations and got promoted there after a long road. People have always wanted to predict the winner of those kind of fights and even more today as bettings on such games are now becoming huge. To be able to predict (showing probabilities) the final result of a UFC match could be a good betting decision helper and therefore making fights even more exciting to watch. \n\nThe special thing about this dataset to keep in mind is that each row is a compilation of both fighter statistics UP UNTIL THIS FIGHT (and this is very important to understand !). ","0a9cd6dd":"The train and test sets are pretty well balanced. We don't need to apply sampling techniques here.","cdd5c251":"We build a new DataFrame by adding the last fight of every active UFC fighters and we build another Dataframe by adding the second last fight of every active UFC fighter. ","d1dc3f32":"# Random Forest Model\n\nRandom Forest is a tree-based model and hence does not require feature scaling. Those algorithm computations aren't based on distance (euclidian distance or whatever), therefore, normalizing data is useless.","67e442b7":"We can notice that variables which are higly correlated between each other are the \"attempted\" and \"landed\" ones. Intuitively, the more strikes a fighter attempts, the more strikes are actually landed to his opponent.  \nFor instance, the higher the average significant head strikes \"landed of attempted\" for one fighter, the higher the average significant strikes \"landed of attempted\" he will get.  \n\nThough, we can't only keep \"landed\" variables because we need to know the accuracy of the fighter (whether on leg kicks, head or body strikes, submission , clinches, etc.). Maybe he attempts a lot of shots but has a hard time touching his opponent.  \nWe will try to transform some features and hence reduce the number of variables to build a more comprehensive and lighter dataframe later. The goal is to keep main informations while avoiding dependence between features. But first, we are going to use all our dataframe features on the model. \n\nWe can also notice that the number of rounds is highly correlated to the title bout. The rule is that for a title bout, the fights must last 5 rounds maximum and only 3 for a non title bout. But UFC changed rules to allow non title bout fights to last 5 rounds (those on the main cards acctually). ","8165b434":"Let's create a column transformer to encode each categorical feature.  \nWe will rather use ordinal encoder than one-hot-encoder as it leads to a poorer accuracy on Random Forest model when dummie features get high. Here is a good explanation why : https:\/\/towardsdatascience.com\/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769.  ","28d93e71":"Most NaN values can be explained because of empty statistics of new fighters joining UFC and fighting for their first time. Actually, they get NaN values until their first fight, so according to how the dataset is built, those statistics are filled in their second fight. \n\nAs there are just a few, we replace weight, height, age and reach NaN values by the median value obtained from each of those features. \nWe also replace 'B_Stance' and 'R_Stance' NaN values by the mode which in our case is \"Orthodox\" stance. ","aa30ede4":"# Data Preprocessing","4ed8cb93":"We delete B_draw and R_draw columns as all their values are fixed to 0 (constant). We still need to keep date to filter our dataframe and also the rest of the categorical features. ","44fd9148":"As we only need the last fights of UFC fighters to get their last updated statistics and hence feed it into our model, we don't need to keep previous fights of active fighters as it won't make any differences in the model's performance.  \nIn another words, the dataset train\/test split strategy will be the following :\n1. We train our model on every fighter's fight at t-1 where t is the last fight of the fighter. \n2. We then test our model on every fighter's fight at t. \n3. We drop row fights at moments t-2, t-3, etc.","97a6e353":"There are no big significant features that could explain by themselves the model results. Almost all variables have an impact on the model even if it's on a very small proportion.  \nBut we can notice that age, takedowns attempts, current lose streak (if the fighter is on a big loose streak, he has highest chance to loose again), clinched landed, significant strikes that opponents are doing on fighters play a slightly bigger role which makes sense.  \n\n\nLet's visualize now the construction of a random single Decision Tree among the Forest. ","88680a95":"We also delete every match where the result is a draw (equality), indeed we don't want to add an additional class to our target variable. As we have already seen earlier, only 83 fights were a draw in the dataset. Now we only have a winner and a looser.  We delete 'Catch Weight' rows from weight_class feature as those fights are anecdotals. "}}