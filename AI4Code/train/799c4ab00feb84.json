{"cell_type":{"52c6ceea":"code","3a73e3a3":"code","2d2aa9f4":"code","73090371":"code","8c43e9a2":"code","dcd51ccc":"code","8a3de5bb":"code","d6aa13ca":"code","45b860e1":"code","b2dc565d":"code","4be664e5":"code","66c6ae1c":"code","56444ea2":"code","87bddb49":"code","5c0c931e":"code","3e621716":"code","74483d03":"code","6e1fe3b6":"code","2193ce21":"code","0039e4ef":"code","ea4ef7a7":"code","7e979393":"code","caec7e30":"code","3d1a019a":"code","8c920390":"code","798c5987":"code","c0c697d4":"code","6b17a4d0":"code","55423824":"code","eef3423a":"code","6a7a1797":"code","012565b5":"code","c5526983":"code","4942c7c8":"code","9d75706a":"code","41ece24a":"code","b9067d5c":"code","c18712f7":"code","ef5f8f59":"code","9dfcf5b5":"code","14815c18":"code","832170be":"code","480373de":"code","4e236c59":"code","77e7c26f":"code","03699f10":"code","e6f35af9":"code","26652640":"code","ab05a318":"code","7b53d9fb":"code","2d6b66ec":"code","2d426657":"code","fe0e5f44":"code","a08c8162":"code","608bbb34":"code","ff1ffecb":"code","b12a4e5e":"code","07f96c55":"code","1f2d7ad9":"code","a3af8f2d":"code","8b7cab5d":"code","7a9c3cf8":"code","ae4df7a9":"code","23279886":"code","c8888576":"code","9a7ebb93":"code","6f65bd51":"code","3cebf2f8":"code","63215d6d":"code","3274bd13":"code","3e0c6c06":"code","1eb19a8b":"code","8c4abc7a":"code","d2eb7272":"code","62af1c5d":"code","92a28f85":"code","9888f69e":"code","6e0cde5f":"code","c27f4aad":"code","23cb383e":"code","3596f0da":"code","9a55632a":"code","c7fe2b88":"code","aba218b8":"code","b4270795":"code","caeade0e":"code","4547eceb":"code","64220a9a":"code","90b72d96":"code","4fee3541":"code","fdd71519":"code","bfb88070":"code","e23dc5a4":"code","f4183a6b":"code","f3ceefde":"code","215f5280":"code","bed69d59":"markdown","47300cd5":"markdown","da807e5d":"markdown","48546634":"markdown","0f078f1e":"markdown","d2c88cdf":"markdown","e7909b49":"markdown","12a00314":"markdown","408238eb":"markdown","eae1ca13":"markdown","329294de":"markdown","87af0987":"markdown","f3a22377":"markdown","0d530d86":"markdown","9b6f1146":"markdown","d660765e":"markdown","e1691458":"markdown","9ffe4c40":"markdown","5bb23d4c":"markdown","3e4770ee":"markdown","ad80cd69":"markdown","9eaafd6b":"markdown","65ea5a1e":"markdown","86002579":"markdown","509f76c5":"markdown","cf9f5655":"markdown","b778cafe":"markdown","45ed1796":"markdown","d96cd661":"markdown","d0690e69":"markdown","91c44a40":"markdown","50ad1cfe":"markdown","dea23b24":"markdown","6ab3082c":"markdown","045e8c1a":"markdown","cbd41f78":"markdown","95b2881b":"markdown","8cf41dfe":"markdown","9b3199f2":"markdown","c32e804a":"markdown","6594a3cb":"markdown","346110d1":"markdown","b225a002":"markdown","058d250c":"markdown","e13008d2":"markdown","2c30acc7":"markdown","87094f0e":"markdown","c40c603f":"markdown","56cc5e15":"markdown","7512f76d":"markdown","41665049":"markdown","145b6bde":"markdown","bd251d50":"markdown","8e7281ab":"markdown","b7c33f39":"markdown","48fcc6ab":"markdown","8e7be8fb":"markdown","a403c300":"markdown","68828582":"markdown"},"source":{"52c6ceea":"from IPython.display import display\nimport datetime\nimport gc\nimport itertools\nimport json\nimport operator\nimport os\nimport pandas as pd\nimport pickle\nimport pprint\nimport numpy as np\nimport re\nimport seaborn as sns\nimport spacy\nimport torch\nimport torch.optim as optim\n\nfrom collections import Counter, deque\nfrom pytorch_pretrained_bert import BertAdam\nfrom sklearn.base import clone\nfrom sklearn.metrics import (\n    accuracy_score,\n    log_loss,\n    make_scorer,\n    mean_squared_error\n)\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    StratifiedKFold,\n    cross_val_score,\n    train_test_split\n)\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom skorch.callbacks import (\n    Callback,\n    Checkpoint,\n    EpochScoring,\n    LRScheduler,\n    ProgressBar,\n    TrainEndCheckpoint\n)\n\nfrom utils.callbacks import SummarizeParameters\nfrom utils.data import load_training_test_data\nfrom utils.dataframe import (\n    categories_from_column,\n    column_list_to_category_flags,\n    count_json_in_dataframes,\n    count_ngrams_up_to_n,\n    drop_columns_from_dataframes,\n    map_categorical_column_to_category_ids,\n    normalize_categories,\n    normalize_description,\n    numerical_feature_engineering_on_dataframe,\n    parse_address_components,\n    remove_outliers,\n    remap_column,\n    remap_columns_with_transform,\n    remap_date_column_to_days_before,\n    remove_small_or_stopwords_from_ranking\n)\nfrom utils.featurize import (\n    featurize_for_tabular_models,\n    featurize_for_tree_models,\n)\nfrom utils.gc import gc_and_clear_caches\nfrom utils.doc2vec import (\n    column_to_doc_vectors\n)\nfrom utils.model import (\n    basic_logistic_regression_pipeline,\n    basic_xgboost_pipeline,\n    basic_adaboost_pipeline,\n    basic_extratrees_pipeline,\n    basic_svc_pipeline,\n    basic_random_forest_pipeline,\n    expand_onehot_encoding,\n    format_statistics,\n    get_prediction_probabilities_with_columns,\n    get_prediction_probabilities_with_columns_from_predictions,\n    prediction_accuracy,\n    write_predictions_table_to_csv,\n    rescale_features_and_split_into_continuous_and_categorical,\n    split_into_continuous_and_categorical,\n    test_model_with_k_fold_cross_validation,\n    train_model_and_get_validation_and_test_set_predictions\n)\nfrom utils.language_models.bert import (\n    BertClassifier,\n    BertForSequenceClassification,\n    TensorTuple,\n    GRADIENT_ACCUMULATION_STEPS,\n    WARMUP_PROPORTION,\n    bert_featurize_data_frames,\n    create_bert_model,\n    create_bert_model_with_tabular_features,\n)\nfrom utils.language_models.descriptions import (\n    descriptions_to_word_sequences,\n    generate_bigrams,\n    generate_description_sequences,\n    maybe_cuda,\n    postprocess_sequences,\n    token_dictionary_seq_encoder,\n    tokenize_sequences,\n    torchtext_create_text_vocab,\n    torchtext_process_texts,\n    words_to_one_hot_lookups,\n)\nfrom utils.language_models.featurize import (\n    featurize_sequences_from_dataframe,\n    featurize_sequences_from_sentence_lists,\n)\nfrom utils.language_models.fasttext import (\n    FastText,\n    FastTextWithTabularData\n)\nfrom utils.language_models.simple_rnn import (\n    CheckpointAndKeepBest,\n    LRAnnealing,\n    NoToTensorInLossClassifier,\n    SimpleRNNPredictor,\n    SimpleRNNTabularDataPredictor\n)\nfrom utils.language_models.split import (\n    shuffled_train_test_split_by_indices,\n    simple_train_test_split_without_shuffle_func,\n    ordered_train_test_split_with_oversampling\n)\nfrom utils.language_models.textcnn import (\n    TextCNN,\n    TextCNNWithTabularData\n)\nfrom utils.language_models.ulmfit import (\n    load_ulmfit_classifier_with_transfer_learning_from_data_frame,\n    train_ulmfit_model_and_get_validation_and_test_set_predictions,\n    train_ulmfit_classifier_with_gradual_unfreezing\n)\nfrom utils.language_models.visualization import (\n    preview_tokenization,\n    preview_encoded_sentences\n)\nfrom utils.report import (\n    generate_classification_report_from_preds,\n    generate_classification_report\n)\n\nnlp = spacy.load(\"en\")","3a73e3a3":"torch.cuda.is_available()","2d2aa9f4":"(ALL_TRAIN_DATAFRAME, TEST_DATAFRAME) = \\\n  load_training_test_data(os.path.join('data', 'train.json'),\n                          os.path.join('data', 'test.json'))\nTRAIN_INDEX, VALIDATION_INDEX = train_test_split(ALL_TRAIN_DATAFRAME.index, test_size=0.1)\nTRAIN_DATAFRAME = ALL_TRAIN_DATAFRAME.iloc[TRAIN_INDEX].reset_index()\nVALIDATION_DATAFRAME = ALL_TRAIN_DATAFRAME.iloc[VALIDATION_INDEX].reset_index()\nTEST_DATAFRAME = TEST_DATAFRAME.reset_index(drop=True)","73090371":"ALL_TRAIN_DATAFRAME.head()","8c43e9a2":"ALL_TRAIN_DATAFRAME.describe()","dcd51ccc":"TEST_DATAFRAME.head()","8a3de5bb":"TEST_DATAFRAME.describe()","d6aa13ca":"CORE_NUMERICAL_COLUMNS = ['bathrooms', 'bedrooms', 'price', 'latitude', 'longitude']","45b860e1":"NUMERICAL_QUANTILES = {\n    'bathrooms': (0.0, 0.999),\n    'bedrooms': (0.0, 0.999),\n    'latitude': (0.01, 0.99),\n    'longitude': (0.01, 0.99),\n    'price': (0.01, 0.99)\n}","b2dc565d":"sns.pairplot(remove_outliers(ALL_TRAIN_DATAFRAME[CORE_NUMERICAL_COLUMNS],\n                             NUMERICAL_QUANTILES))","4be664e5":"sns.pairplot(remove_outliers(TEST_DATAFRAME[CORE_NUMERICAL_COLUMNS],\n                             NUMERICAL_QUANTILES))","66c6ae1c":"TRAIN_DATAFRAME = remove_outliers(TRAIN_DATAFRAME, NUMERICAL_QUANTILES)","56444ea2":"TRAIN_DATAFRAME.head()","87bddb49":"normalized_categories = sorted(normalize_categories(categories_from_column(TRAIN_DATAFRAME, 'features')))\nnormalized_categories[:50]","5c0c931e":"most_common_ngrams = sorted(count_ngrams_up_to_n(\" \".join(normalized_categories), 3).most_common(),\n                            key=lambda x: (-x[1], x[0]))\nmost_common_ngrams[:50]","3e621716":"most_common_ngrams = sorted(list(remove_small_or_stopwords_from_ranking(most_common_ngrams, nlp, 3)),\n                            key=lambda x: (-x[1], x[0]))\nmost_common_ngrams[:50]","74483d03":"TRAIN_DATAFRAME = column_list_to_category_flags(TRAIN_DATAFRAME, 'features', list(map(operator.itemgetter(0), most_common_ngrams[:100])))\nVALIDATION_DATAFRAME = column_list_to_category_flags(VALIDATION_DATAFRAME, 'features', list(map(operator.itemgetter(0), most_common_ngrams[:100])))\nTEST_DATAFRAME = column_list_to_category_flags(TEST_DATAFRAME, 'features', list(map(operator.itemgetter(0), most_common_ngrams[:100])))","6e1fe3b6":"TRAIN_DATAFRAME.head(5)","2193ce21":"TRAIN_DATAFRAME = remap_date_column_to_days_before(TRAIN_DATAFRAME, \"created\", \"created_days_ago\", datetime.datetime(2017, 1, 1))\nVALIDATION_DATAFRAME = remap_date_column_to_days_before(VALIDATION_DATAFRAME, \"created\", \"created_days_ago\", datetime.datetime(2017, 1, 1))\nTEST_DATAFRAME = remap_date_column_to_days_before(TEST_DATAFRAME, \"created\", \"created_days_ago\", datetime.datetime(2017, 1, 1))","0039e4ef":"TRAIN_DATAFRAME[\"created_days_ago\"].head(5)","ea4ef7a7":"INTEREST_LEVEL_MAPPINGS = {\n    \"high\": 0,\n    \"medium\": 1,\n    \"low\": 2\n}\n\nTRAIN_DATAFRAME = remap_column(TRAIN_DATAFRAME, \"interest_level\", \"label_interest_level\", lambda x: INTEREST_LEVEL_MAPPINGS[x])\nVALIDATION_DATAFRAME = remap_column(VALIDATION_DATAFRAME, \"interest_level\", \"label_interest_level\", lambda x: INTEREST_LEVEL_MAPPINGS[x])\n# The TEST_DATAFRAME does not have an interest_level column, so we\n# instead add it and replace it with all zeros\nTEST_DATAFRAME[\"label_interest_level\"] = 0","7e979393":"TRAIN_DATAFRAME[\"label_interest_level\"].head(5)","caec7e30":"((BUILDING_ID_UNKNOWN_REMAPPING,\n  BUILDING_CATEGORY_TO_BUILDING_ID,\n  BUILDING_CATEGORY_TO_BUILDING_ID),\n (TRAIN_DATAFRAME,\n  VALIDATION_DATAFRAME,\n  TEST_DATAFRAME)) = map_categorical_column_to_category_ids(\n    'building_id',\n    'building_id_category',\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n    min_freq=40\n)","3d1a019a":"((MANAGER_ID_UNKNOWN_REMAPPING,\n  MANAGER_ID_TO_MANAGER_CATEGORY,\n  MANAGER_CATEGORY_TO_MANAGER_ID),\n (TRAIN_DATAFRAME,\n  VALIDATION_DATAFRAME,\n  TEST_DATAFRAME)) = map_categorical_column_to_category_ids(\n    'manager_id',\n    'manager_id_category',\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n    min_freq=40\n)","8c920390":"(TRAIN_DATAFRAME,\n VALIDATION_DATAFRAME,\n TEST_DATAFRAME) = parse_address_components(\n    [\n        \"display_address\",\n        \"street_address\"\n    ],\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n)","798c5987":"((DISP_ADDR_ID_UNKNOWN_REMAPPING,\n  DISP_ADDR_ID_TO_DISP_ADDR_CATEGORY,\n  DISP_ADDR_CATEGORY_TO_DISP_ADDR_ID),\n (TRAIN_DATAFRAME,\n  VALIDATION_DATAFRAME,\n  TEST_DATAFRAME)) = map_categorical_column_to_category_ids(\n    'display_address_normalized',\n    'display_address_category',\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n    min_freq=40\n)","c0c697d4":"(TRAIN_DATAFRAME,\n VALIDATION_DATAFRAME,\n TEST_DATAFRAME) = count_json_in_dataframes(\n    \"photos\",\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n)","6b17a4d0":"NUMERICAL_COLUMNS = CORE_NUMERICAL_COLUMNS + [\n    'photos_count'\n]","55423824":"(TRAIN_DATAFRAME,\n VALIDATION_DATAFRAME,\n TEST_DATAFRAME) = remap_columns_with_transform(\n    'description',\n    'clean_description',\n    normalize_description,\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME,\n)","eef3423a":"DROP_COLUMNS = [\n    'id',\n    'index',\n    'created',\n    'building_id',\n    'clean_description',\n    'description',\n    'features',\n    'display_address',\n    'display_address_normalized',\n    # We keep listing_id in the dataframe\n    # since we'll need it later\n    # 'listing_id',\n    'manager_id',\n    'photos',\n    'street_address',\n    'street_address_normalized',\n    'interest_level',\n]","6a7a1797":"(FEATURES_TRAIN_DATAFRAME,\n FEATURES_VALIDATION_DATAFRAME,\n FEATURES_TEST_DATAFRAME) = drop_columns_from_dataframes(\n    DROP_COLUMNS,\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME\n)","012565b5":"FEATURES_TRAIN_DATAFRAME.head(5)","c5526983":"FEATURIZED_NUMERICAL_COLUMNS = CORE_NUMERICAL_COLUMNS + [\"photos_count\", \"label_interest_level\"]","4942c7c8":"sns.pairplot(remove_outliers(FEATURES_TRAIN_DATAFRAME[FEATURIZED_NUMERICAL_COLUMNS],\n                             NUMERICAL_QUANTILES))","9d75706a":"sns.pairplot(remove_outliers(FEATURES_VALIDATION_DATAFRAME[FEATURIZED_NUMERICAL_COLUMNS],\n                             NUMERICAL_QUANTILES))","41ece24a":"sns.pairplot(remove_outliers(FEATURES_TEST_DATAFRAME[FEATURIZED_NUMERICAL_COLUMNS[:-1]],\n                             NUMERICAL_QUANTILES))","b9067d5c":"CATEGORICAL_FEATURES = {\n    'building_id_category': len(BUILDING_CATEGORY_TO_BUILDING_ID),\n    'manager_id_category': len(MANAGER_ID_TO_MANAGER_CATEGORY),\n    'display_address_category': len(DISP_ADDR_ID_TO_DISP_ADDR_CATEGORY)\n}","c18712f7":"TRAIN_LABELS = FEATURES_TRAIN_DATAFRAME['label_interest_level']\nVALIDATION_LABELS = FEATURES_VALIDATION_DATAFRAME['label_interest_level']","ef5f8f59":"def train_logistic_regression_model(data_info,\n                                    featurized_train_data,\n                                    featurized_validation_data,\n                                    train_labels,\n                                    validation_labels,\n                                    train_param_grid_optimal=None):\n    pipeline = basic_logistic_regression_pipeline(featurized_train_data,\n                                                  train_labels,\n                                                  CATEGORICAL_FEATURES,\n                                                  param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline\n\n\ndef predict_with_sklearn_estimator(model, data):\n    return model.predict(data), model.predict_proba(data)\n","9dfcf5b5":"(LOGISTIC_REGRESSION_MODEL_VALIDATION_PROBABILITIES,\n LOGISTIC_REGRESSION_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tabular_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_logistic_regression_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'C': [1.0],\n            'class_weight': [None],\n            'penalty': ['l2']\n        }\n    )\n)","14815c18":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        LOGISTIC_REGRESSION_MODEL_TEST_PROBABILITIES,\n    ),\n    'renthop_logistic_regression_submissions.csv'\n)","832170be":"def train_xgboost_model(data_info,\n                        featurized_train_data,\n                        featurized_validation_data,\n                        train_labels,\n                        validation_labels,\n                        train_param_grid_optimal=None):\n    pipeline = basic_xgboost_pipeline(featurized_train_data,\n                                      train_labels,\n                                      tree_method=(\n                                          # 'gpu_hist' turned out to be a lot slower\n                                         'hist'\n                                      ),\n                                      param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline","480373de":"(XGBOOST_MODEL_VALIDATION_PROBABILITIES,\n XGBOOST_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tree_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_xgboost_model,\n        predict_with_sklearn_estimator,\n        # Determined by Grid Search, above\n        train_param_grid_optimal={\n            'colsample_bytree': [1.0],\n            'gamma': [1.5],\n            'max_depth': [5],\n            'min_child_weight': [1],\n            'n_estimators': [200],\n            'subsample': [0.6]\n        }\n    )\n)","4e236c59":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        XGBOOST_MODEL_TEST_PROBABILITIES,\n    ),\n    'xgboost_submissions.csv'\n)","77e7c26f":"def train_rf_model(data_info,\n                   featurized_train_data,\n                   featurized_validation_data,\n                   train_labels,\n                   validation_labels,\n                   train_param_grid_optimal=None):\n    pipeline = basic_random_forest_pipeline(featurized_train_data,\n                                            train_labels,\n                                            # Determined by Grid Search, above\n                                            param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline","03699f10":"(RF_MODEL_VALIDATION_PROBABILITIES,\n RF_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tree_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_rf_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'bootstrap': [False],\n            'max_depth': [5],\n            'min_samples_leaf': [1],\n            'n_estimators': [100]\n        }\n    )\n)","e6f35af9":"def train_adaboost_model(data_info,\n                         featurized_train_data,\n                         featurized_validation_data,\n                         train_labels,\n                         validation_labels,\n                         train_param_grid_optimal=None):\n    pipeline = basic_adaboost_pipeline(featurized_train_data,\n                                       train_labels,\n                                       # Determined by Grid Search, above\n                                       param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline","26652640":"(ADABOOST_MODEL_VALIDATION_PROBABILITIES,\n ADABOOST_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tree_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_adaboost_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'learning_rate': [0.1],\n            'n_estimators': [100]\n        }\n    )\n)","ab05a318":"def train_extratrees_model(data_info,\n                           featurized_train_data,\n                           featurized_validation_data,\n                           train_labels,\n                           validation_labels,\n                           train_param_grid_optimal=None):\n    pipeline = basic_extratrees_pipeline(featurized_train_data,\n                                         train_labels,\n                                         # Determined by Grid Search, above\n                                         param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline","7b53d9fb":"(EXTRATREES_MODEL_VALIDATION_PROBABILITIES,\n EXTRATREES_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tree_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_extratrees_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'bootstrap': [False],\n            'max_depth': [5],\n            'min_samples_leaf': [1],\n            'n_estimators': [200]\n        }\n    )\n)","2d6b66ec":"def train_svc_model(data_info,\n                    featurized_train_data,\n                    featurized_validation_data,\n                    train_labels,\n                    validation_labels,\n                    train_param_grid_optimal=None):\n    pipeline = basic_svc_pipeline(featurized_train_data,\n                                  train_labels,\n                                  # Determined by Grid Search, above\n                                  param_grid_optimal=train_param_grid_optimal)\n    pipeline.fit(featurized_train_data, train_labels)\n    print(\"Best parameters {}\".format(pipeline.best_params_))\n    return pipeline","2d426657":"(SVC_MODEL_VALIDATION_PROBABILITIES,\n SVC_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_for_tabular_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_svc_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'C': [1.0],\n            'gamma': ['scale'],\n            'kernel': ['rbf']\n        }\n    )\n)","fe0e5f44":"((TRAIN_FEATURES_CONTINUOUS,\n  TRAIN_FEATURES_CATEGORICAL),\n (VALIDATION_FEATURES_CONTINUOUS,\n  VALIDATION_FEATURES_CATEGORICAL),\n (TEST_FEATURES_CONTINUOUS,\n  TEST_FEATURES_CATEGORICAL)) = rescale_features_and_split_into_continuous_and_categorical(CATEGORICAL_FEATURES,\n                                                                                           FEATURES_TRAIN_DATAFRAME,\n                                                                                           FEATURES_VALIDATION_DATAFRAME,\n                                                                                           FEATURES_TEST_DATAFRAME)","a08c8162":"TRAIN_LABELS_TENSOR = torch.tensor(TRAIN_LABELS.values).long()\nVALIDATION_LABELS_TENSOR = torch.tensor(VALIDATION_LABELS.values).long()","608bbb34":"TRAIN_FEATURES_CONTINUOUS_TENSOR = torch.tensor(TRAIN_FEATURES_CONTINUOUS).float()\nTRAIN_FEATURES_CATEGORICAL_TENSOR = torch.tensor(TRAIN_FEATURES_CATEGORICAL).long()","ff1ffecb":"VALIDATION_FEATURES_CONTINUOUS_TENSOR = torch.tensor(VALIDATION_FEATURES_CONTINUOUS).float()\nVALIDATION_FEATURES_CATEGORICAL_TENSOR = torch.tensor(VALIDATION_FEATURES_CATEGORICAL).long()","b12a4e5e":"TEST_FEATURES_CONTINUOUS_TENSOR = torch.tensor(TEST_FEATURES_CONTINUOUS).float()\nTEST_FEATURES_CATEGORICAL_TENSOR = torch.tensor(TEST_FEATURES_CATEGORICAL).long()","07f96c55":"preview_tokenization(TRAIN_DATAFRAME[\"description\"][:10])","1f2d7ad9":"preview_encoded_sentences(TRAIN_DATAFRAME[\"description\"][:10])","a3af8f2d":"def featurize_for_rnn_language_model(*dataframes):\n    data_info, model_datasets = featurize_sequences_from_dataframe(*dataframes)\n    return data_info, model_datasets\n\n\ndef train_rnn_model(data_info,\n                    featurized_train_data,\n                    featurized_validation_data,\n                    train_labels,\n                    validation_labels,\n                    train_param_grid_optimal=None):\n    word_to_one_hot, one_hot_to_word = data_info\n    train_word_description_sequences, train_word_sequences_lengths = featurized_train_data\n    model = NoToTensorInLossClassifier(\n        SimpleRNNPredictor,\n        module__encoder_dimension=100, # Number of encoder features\n        module__hidden_dimension=50, # Number of hidden features\n        module__dictionary_dimension=len(one_hot_to_word), # Dictionary dimension\n        module__output_dimension=3,\n        module__dropout=0.1,\n        lr=1e-2,\n        batch_size=256,\n        optimizer=optim.Adam,\n        max_epochs=4,\n        module__layers=2,\n        train_split=simple_train_test_split_without_shuffle_func(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='rnn_lang_checkpoint'),\n            TrainEndCheckpoint(dirname='rnn_lang_checkpoint',\n                               fn_prefix='rnn_train_end_')\n        ]\n    )\n    model.fit((train_word_description_sequences,\n               train_word_sequences_lengths),\n              maybe_cuda(train_labels))\n    \n    return model","8b7cab5d":"(SIMPLE_RNN_MODEL_VALIDATION_PROBABILITIES,\n SIMPLE_RNN_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_for_rnn_language_model,\n        train_rnn_model,\n        predict_with_sklearn_estimator\n    )\n)","7a9c3cf8":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        SIMPLE_RNN_MODEL_TEST_PROBABILITIES,\n    ),\n    'simple_rnn_model_submissions.csv'\n)","ae4df7a9":"def splice_into_datasets(datasets, append):\n    return tuple(list(d) + list(a) for d, a in zip(datasets, append))\n\ndef featurize_for_rnn_tabular_model(*dataframes):\n    data_info, model_datasets = featurize_sequences_from_dataframe(*dataframes)\n    # Need to wrap each dataset in outer list so that splicing works correctly\n    return data_info, splice_into_datasets(model_datasets,\n                                           ((TRAIN_FEATURES_CONTINUOUS_TENSOR,\n                                             TRAIN_FEATURES_CATEGORICAL_TENSOR),\n                                            (VALIDATION_FEATURES_CONTINUOUS_TENSOR,\n                                             VALIDATION_FEATURES_CATEGORICAL_TENSOR),\n                                            (TEST_FEATURES_CONTINUOUS_TENSOR,\n                                             TEST_FEATURES_CATEGORICAL_TENSOR )))\n\n\ndef train_rnn_tabular_model(data_info,\n                            featurized_train_data,\n                            featurized_validation_data,\n                            train_labels,\n                            validation_labels,\n                            train_param_grid_optimal=None):\n    word_to_one_hot, one_hot_to_word = data_info\n    _, _, train_continuous, train_categorical = featurized_train_data\n    model = NoToTensorInLossClassifier(\n        SimpleRNNTabularDataPredictor,\n        module__encoder_dimension=100, # Number of encoder features\n        module__hidden_dimension=50, # Number of hidden features\n        module__dictionary_dimension=len(one_hot_to_word), # Dictionary dimension\n        module__output_dimension=3,\n        module__dropout=0.1,\n        module__continuous_features_dimension=train_continuous.shape[1],\n        module__categorical_feature_embedding_dimensions=[\n            (CATEGORICAL_FEATURES[c], 80) for c in CATEGORICAL_FEATURES\n        ],\n        lr=1e-2,\n        batch_size=256,\n        optimizer=optim.Adam,\n        max_epochs=4,\n        module__layers=2,\n        train_split=simple_train_test_split_without_shuffle_func(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='rnn_lang_checkpoint'),\n            TrainEndCheckpoint(dirname='rnn_lang_checkpoint',\n                               fn_prefix='rnn_train_end_')\n        ]\n    )\n    model.fit(featurized_train_data, maybe_cuda(train_labels))\n    \n    return model","23279886":"(SIMPLE_RNN_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n SIMPLE_RNN_TABULAR_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_for_rnn_tabular_model,\n        train_rnn_tabular_model,\n        predict_with_sklearn_estimator\n    )\n)","c8888576":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        SIMPLE_RNN_TABULAR_MODEL_TEST_PROBABILITIES,\n    ),\n    'simple_rnn_model_tabular_data_submissions.csv'\n)","9a7ebb93":"def featurize_dataframe_sequences_for_fasttext(*dataframes):\n    sequences_lists = tokenize_sequences(\n        *tuple(list(df['clean_description']) for df in dataframes)\n    )\n\n    text = torchtext_create_text_vocab(*sequences_lists,\n                                       vectors='glove.6B.100d')\n    \n    return text, torchtext_process_texts(*postprocess_sequences(\n        *sequences_lists,\n        postprocessing=generate_bigrams\n    ), text=text)\n\n\ndef train_fasttext_model(data_info,\n                         featurized_train_data,\n                         featurized_validation_data,\n                         train_labels,\n                         validation_labels,\n                         train_param_grid_optimal=None):\n    embedding_dim = 100\n    model = NoToTensorInLossClassifier(\n        FastText,\n        lr=0.001,\n        batch_size=256,\n        optimizer=optim.Adam,\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='fasttext_checkpoint'),\n            TrainEndCheckpoint(dirname='fasttext_tabular_checkpoint',\n                               fn_prefix='fasttext_train_end_')\n        ],\n        max_epochs=6,\n        train_split=shuffled_train_test_split_by_indices(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        module__encoder_dimension=embedding_dim, # Number of encoder features\n        module__dictionary_dimension=len(data_info.vocab.itos), # Dictionary dimension\n        module__output_dimension=3,\n        module__dropout=0.8,\n        module__pretrained=data_info.vocab.vectors\n    )\n    model.fit(featurized_train_data, maybe_cuda(train_labels))\n    return model","6f65bd51":"(FASTTEXT_MODEL_VALIDATION_PROBABILITIES,\n FASTTEXT_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_dataframe_sequences_for_fasttext,\n        train_fasttext_model,\n        predict_with_sklearn_estimator\n    )\n)","3cebf2f8":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        FASTTEXT_MODEL_TEST_PROBABILITIES,\n    ),\n    'fasttext_model_submissions.csv'\n)","63215d6d":"def featurize_for_fasttext_tabular_model(*dataframes):\n    data_info, model_datasets = featurize_dataframe_sequences_for_fasttext(*dataframes)\n    # Need to wrap each dataset in outer list so that splicing works correctly\n    return data_info, splice_into_datasets(tuple([x] for x in model_datasets),\n                                           ((TRAIN_FEATURES_CONTINUOUS_TENSOR,\n                                             TRAIN_FEATURES_CATEGORICAL_TENSOR),\n                                            (VALIDATION_FEATURES_CONTINUOUS_TENSOR,\n                                             VALIDATION_FEATURES_CATEGORICAL_TENSOR),\n                                            (TEST_FEATURES_CONTINUOUS_TENSOR,\n                                             TEST_FEATURES_CATEGORICAL_TENSOR)))\n\n\ndef train_fasttext_tabular_model(data_info,\n                                 featurized_train_data,\n                                 featurized_validation_data,\n                                 train_labels,\n                                 validation_labels,\n                                 train_param_grid_optimal=None):\n    embedding_dim = 100\n    _, train_features_continuous, _ = featurized_train_data\n    model = NoToTensorInLossClassifier(\n        FastTextWithTabularData,\n        lr=0.001,\n        batch_size=256,\n        optimizer=optim.Adam,\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='fasttext_checkpoint'),\n            TrainEndCheckpoint(dirname='fasttext_tabular_checkpoint',\n                               fn_prefix='fasttext_train_end_')\n        ],\n        max_epochs=6,\n        train_split=shuffled_train_test_split_by_indices(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        module__encoder_dimension=embedding_dim, # Number of encoder features\n        module__dictionary_dimension=len(data_info.vocab.itos), # Dictionary dimension\n        module__output_dimension=3,\n        module__dropout=0.8,\n        module__pretrained=data_info.vocab.vectors,\n        module__continuous_features_dimension=train_features_continuous.shape[1],\n        module__categorical_feature_embedding_dimensions=[\n            (CATEGORICAL_FEATURES[c], 80) for c in CATEGORICAL_FEATURES\n        ],\n    )\n    model.fit(featurized_train_data, maybe_cuda(train_labels))\n    return model","3274bd13":"(FASTTEXT_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n FASTTEXT_TABULAR_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_for_fasttext_tabular_model,\n        train_fasttext_tabular_model,\n        predict_with_sklearn_estimator\n    )\n)","3e0c6c06":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        FASTTEXT_TABULAR_MODEL_TEST_PROBABILITIES,\n    ),\n    'fasttext_tabular_model_submissions.csv'\n)","1eb19a8b":"def featurize_dataframe_sequences_for_textcnn(*dataframes):\n    sequences_lists = tokenize_sequences(\n        *tuple(list(df['clean_description']) for df in dataframes)\n    )\n\n    text = torchtext_create_text_vocab(*sequences_lists,\n                                       vectors='glove.6B.100d')\n    \n    return text, tuple(text.process(sl).transpose(0, 1) for sl in sequences_lists)\n\ndef train_textcnn_model(data_info,\n                        featurized_train_data,\n                        featurized_validation_data,\n                        train_labels,\n                        validation_labels,\n                        train_param_grid_optimal=None):\n    embedding_dim = 100\n    model = NoToTensorInLossClassifier(\n        TextCNN,\n        lr=0.001,\n        batch_size=64,\n        optimizer=optim.Adam,\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='textcnn_checkpoint'),\n            TrainEndCheckpoint(dirname='textcnn_tabular_checkpoint',\n                               fn_prefix='textcnn_train_end_')\n        ],\n        max_epochs=10,\n        train_split=shuffled_train_test_split_by_indices(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        module__encoder_dimension=embedding_dim, # Number of encoder features\n        module__dictionary_dimension=len(data_info.vocab.itos), # Dictionary dimension\n        module__output_dimension=3,\n        module__n_filters=10,\n        module__filter_sizes=(3, 4, 5),\n        module__dropout=0.8,\n        module__pretrained=data_info.vocab.vectors\n    )\n    model.fit(featurized_train_data, maybe_cuda(train_labels))\n    return model","8c4abc7a":"(TEXTCNN_MODEL_VALIDATION_PROBABILITIES,\n TEXTCNN_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_dataframe_sequences_for_textcnn,\n        train_textcnn_model,\n        predict_with_sklearn_estimator\n    )\n)","d2eb7272":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        TEXTCNN_MODEL_TEST_PROBABILITIES,\n    ),\n    'textcnn_model_submissions.csv'\n)","62af1c5d":"def featurize_dataframes_for_textcnn_tabular_model(*dataframes):\n    data_info, model_datasets = featurize_dataframe_sequences_for_textcnn(*dataframes)\n    return data_info, splice_into_datasets(tuple([x] for x in model_datasets),\n                                           ((TRAIN_FEATURES_CONTINUOUS_TENSOR,\n                                             TRAIN_FEATURES_CATEGORICAL_TENSOR),\n                                            (VALIDATION_FEATURES_CONTINUOUS_TENSOR,\n                                             VALIDATION_FEATURES_CATEGORICAL_TENSOR),\n                                            (TEST_FEATURES_CONTINUOUS_TENSOR,\n                                             TEST_FEATURES_CATEGORICAL_TENSOR)))\n\ndef train_textcnn_tabular_model(data_info,\n                                featurized_train_data,\n                                featurized_validation_data,\n                                train_labels,\n                                validation_labels,\n                                train_param_grid_optimal=None):\n    embedding_dim = 100\n    _, train_features_continuous, _ = featurized_train_data\n    model = NoToTensorInLossClassifier(\n        TextCNNWithTabularData,\n        lr=0.001,\n        batch_size=256,\n        optimizer=optim.Adam,\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            LRAnnealing(),\n            LRScheduler(),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='fasttext_checkpoint'),\n            TrainEndCheckpoint(dirname='fasttext_tabular_checkpoint',\n                               fn_prefix='fasttext_train_end_')\n        ],\n        max_epochs=10,\n        train_split=shuffled_train_test_split_by_indices(0.3),\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        module__encoder_dimension=embedding_dim, # Number of encoder features\n        module__dictionary_dimension=len(data_info.vocab.itos), # Dictionary dimension\n        module__output_dimension=3,\n        module__n_filters=100,\n        module__filter_sizes=(3, 4, 5),\n        module__dropout=0.8,\n        module__pretrained=data_info.vocab.vectors,\n        module__continuous_features_dimension=train_features_continuous.shape[1],\n        module__categorical_feature_embedding_dimensions=[\n            (CATEGORICAL_FEATURES[c], 80) for c in CATEGORICAL_FEATURES\n        ],\n    )\n    model.fit(featurized_train_data, maybe_cuda(train_labels))\n    return model","92a28f85":"(TEXTCNN_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n TEXTCNN_TABULAR_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_dataframes_for_textcnn_tabular_model,\n        train_textcnn_tabular_model,\n        predict_with_sklearn_estimator\n    )\n)","9888f69e":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        TEXTCNN_TABULAR_MODEL_TEST_PROBABILITIES,\n    ),\n    'textcnn_tabular_model_submissions.csv'\n)","6e0cde5f":"gc_and_clear_caches(None)","c27f4aad":"def flatten_params(params):\n    return {\n        k: v[0] if isinstance(v, list) else v for k, v in params.items()\n    }","23cb383e":"BERT_MODEL = 'bert-base-uncased'\n\n\ndef featurize_bert_lang_features(*dataframes):\n    return _, tuple(\n        tuple(torch.stack(x) for x in zip(*features))\n        for features in bert_featurize_data_frames(BERT_MODEL, *dataframes)\n    )\n        \n\ndef train_bert_lang_model(data_info,\n                          featurized_train_data,\n                          featurized_validation_data,\n                          train_labels,\n                          validation_labels,\n                          train_param_grid_optimal=None):\n    model = BertClassifier(\n        module=create_bert_model(BERT_MODEL, 3),\n        optimizer__warmup=WARMUP_PROPORTION,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        optimizer=BertAdam,\n        lr=6e-5,\n        len_train_data=int(len(featurized_train_data[0])),\n        num_labels=3,\n        batch_size=16,\n        train_split=shuffled_train_test_split_by_indices(0.1),\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='bert_lang_checkpoint')\n        ],\n    )\n    \n    if not train_param_grid_optimal:\n        # As sugested by Maksad, need to do a hyperparameter search\n        # here to get good results.\n        param_grid = {\n            \"batch_size\": [16, 32],\n            \"lr\": [6e-5, 3e-5, 3e-1, 2e-5],\n            \"max_epochs\": [3, 4]\n        }\n        search = GridSearchCV(model,\n                              param_grid,\n                              cv=1,\n                              refit=False,\n                              scoring=make_scorer(log_loss,\n                                                  greater_is_better=False,\n                                                  needs_proba=True))\n        search.fit(TensorTuple(featurized_train_data), train_labels)\n\n        print('Best params {}'.format(search.best_params_))\n        # Now re-fit the estimator manually, using the best params -\n        # we do this manually since we need a different view over\n        # the training data to make it work\n        best = clone(search.estimator, safe=True).set_params(**search.best_params_)\n        best.fit(featurized_train_data, train_labels)\n        return best\n    else:\n        model = clone(model, safe=True).set_params(**flatten_params(train_param_grid_optimal))\n        model.fit(featurized_train_data, train_labels)\n        return model","3596f0da":"(BERT_LANG_MODEL_VALIDATION_PROBABILITIES,\n BERT_LANG_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS,\n        VALIDATION_LABELS,\n        featurize_bert_lang_features,\n        train_bert_lang_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'lr': [2e-05],\n            'max_epochs': [4],\n            'batch_size': [32]\n        }\n    )\n)","9a55632a":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        BERT_LANG_MODEL_TEST_PROBABILITIES,\n    ),\n    'bert_model_submissions.csv'\n)","c7fe2b88":"def featurize_bert_tabular_features(*dataframes):\n    model_datasets = splice_into_datasets(tuple([x] for x in bert_featurize_data_frames(BERT_MODEL, *dataframes)),\n                                          ((TRAIN_FEATURES_CONTINUOUS_TENSOR,\n                                            TRAIN_FEATURES_CATEGORICAL_TENSOR),\n                                           (VALIDATION_FEATURES_CONTINUOUS_TENSOR,\n                                            VALIDATION_FEATURES_CATEGORICAL_TENSOR),\n                                           (TEST_FEATURES_CONTINUOUS_TENSOR,\n                                            TEST_FEATURES_CATEGORICAL_TENSOR)))\n    return _, tuple(\n        tuple(torch.stack(x) for x in zip(*[\n            tuple(list(bert_features) + [continuous, categorical])\n            for bert_features, continuous, categorical in zip(features,\n                                                              continuous_tensor,\n                                                              categorical_tensor)\n        ]))\n        for features, continuous_tensor, categorical_tensor in model_datasets\n    )\n\n\ndef train_bert_tabular_model(data_info,\n                             featurized_train_data,\n                             featurized_validation_data,\n                             train_labels,\n                             validation_labels,\n                             train_param_grid_optimal=None):\n    batch_size = 16\n    _, _, _, continuous_features, categorical_features = featurized_train_data\n    model = BertClassifier(\n        module=create_bert_model_with_tabular_features(\n            BERT_MODEL,\n            continuous_features.shape[1],\n            [\n                (CATEGORICAL_FEATURES[c], 80) for c in CATEGORICAL_FEATURES\n            ],\n            3\n        ),\n        len_train_data=int(len(featurized_train_data[0])),\n        optimizer__warmup=WARMUP_PROPORTION,\n        device='cuda' if torch.cuda.is_available() else 'cpu',\n        optimizer=BertAdam,\n        num_labels=3,\n        batch_size=batch_size,\n        train_split=shuffled_train_test_split_by_indices(0.3),\n        callbacks=[\n            SummarizeParameters(),\n            EpochScoring(scoring='accuracy'),\n            ProgressBar(),\n            CheckpointAndKeepBest(dirname='bert_lang_checkpoint')\n        ],\n    )\n    \n    if not train_param_grid_optimal:\n        # As sugested by Maksad, need to do a hyperparameter search\n        # here to get good results.\n        param_grid = {\n            \"batch_size\": [16, 32],\n            \"lr\": [6e-5, 3e-5, 3e-5, 2e-5],\n            \"max_epochs\": [3, 4]\n        }\n        search = GridSearchCV(model,\n                              param_grid,\n                              cv=2,\n                              refit=False,\n                              scoring=make_scorer(log_loss,\n                                                  greater_is_better=False,\n                                                  needs_proba=True))\n        search.fit(TensorTuple(featurized_train_data), train_labels)\n\n        print('Best params {}'.format(search.best_params_))\n        # Now re-fit the estimator manually, using the best params -\n        # we do this manually since we need a different view over\n        # the training data to make it work\n        best = clone(search.estimator, safe=True).set_params(**search.best_params_)\n        best.fit(featurized_train_data, train_labels)\n        return best\n    else:\n        model = clone(model, safe=True).set_params(**flatten_params(train_param_grid_optimal))\n        model.fit(featurized_train_data, train_labels)\n        return model","aba218b8":"(BERT_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n BERT_TABULAR_MODEL_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        TRAIN_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TEST_DATAFRAME,\n        VALIDATION_DATAFRAME,\n        TRAIN_LABELS_TENSOR,\n        VALIDATION_LABELS_TENSOR,\n        featurize_bert_tabular_features,\n        train_bert_tabular_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'lr': [2e-05],\n            'max_epochs': [4],\n            'batch_size': [32]\n        }\n    )\n)","b4270795":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        BERT_TABULAR_MODEL_TEST_PROBABILITIES,\n    ),\n    'bert_tabular_model_submissions.csv'\n)","caeade0e":"from utils.language_models.ulmfit import train_ulmfit_model_and_get_validation_and_test_set_predictions\n\n(ULMFIT_VALIDATION_PROBABILITIES,\n ULMFIT_TEST_PROBABILITIES) = train_ulmfit_model_and_get_validation_and_test_set_predictions(\n    TRAIN_DATAFRAME,\n    VALIDATION_DATAFRAME,\n    TEST_DATAFRAME\n)","4547eceb":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        ULMFIT_TEST_PROBABILITIES\n    ),\n    'ulmfit_submissions.csv'\n)","64220a9a":"(STACKED_VALIDATION_PREDICTIONS_TRAINING_SET,\n STACKED_VALIDATION_PREDICTIONS_VALIDATION_SET,\n _,\n VALIDATION_SPLIT_VALIDATION_DATAFRAME,\n TRAINING_SPLIT_FEATURES_VALIDATION_DATAFRAME,\n VALIDATION_SPLIT_FEATURES_VALIDATION_DATAFRAME,\n STACKED_VALIDATION_PREDICTIONS_LABELS_TRAINING_SET,\n STACKED_VALIDATION_PREDICTIONS_LABELS_VALIDATION_SET) = train_test_split(\n    np.column_stack([\n        LOGISTIC_REGRESSION_MODEL_VALIDATION_PROBABILITIES,\n        XGBOOST_MODEL_VALIDATION_PROBABILITIES,\n        RF_MODEL_VALIDATION_PROBABILITIES,\n        ADABOOST_MODEL_VALIDATION_PROBABILITIES,\n        EXTRATREES_MODEL_VALIDATION_PROBABILITIES,\n        SVC_MODEL_VALIDATION_PROBABILITIES,\n        SIMPLE_RNN_MODEL_VALIDATION_PROBABILITIES,\n        SIMPLE_RNN_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n        FASTTEXT_MODEL_VALIDATION_PROBABILITIES,\n        FASTTEXT_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n        TEXTCNN_MODEL_VALIDATION_PROBABILITIES,\n        TEXTCNN_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n        BERT_LANG_MODEL_VALIDATION_PROBABILITIES,\n        BERT_TABULAR_MODEL_VALIDATION_PROBABILITIES,\n        ULMFIT_VALIDATION_PROBABILITIES,\n    ]),\n    VALIDATION_DATAFRAME,\n    FEATURES_VALIDATION_DATAFRAME,\n    VALIDATION_LABELS,\n    stratify=VALIDATION_LABELS,\n    test_size=0.1\n)\n\nSTACKED_TEST_PREDICTIONS_TEST_SET = np.column_stack([\n    LOGISTIC_REGRESSION_MODEL_TEST_PROBABILITIES,\n    XGBOOST_MODEL_TEST_PROBABILITIES,\n    RF_MODEL_TEST_PROBABILITIES,\n    ADABOOST_MODEL_TEST_PROBABILITIES,\n    EXTRATREES_MODEL_TEST_PROBABILITIES,\n    SVC_MODEL_TEST_PROBABILITIES,\n    SIMPLE_RNN_MODEL_TEST_PROBABILITIES,\n    SIMPLE_RNN_TABULAR_MODEL_TEST_PROBABILITIES,\n    FASTTEXT_MODEL_TEST_PROBABILITIES,\n    FASTTEXT_TABULAR_MODEL_TEST_PROBABILITIES,\n    TEXTCNN_MODEL_TEST_PROBABILITIES,\n    TEXTCNN_TABULAR_MODEL_TEST_PROBABILITIES,\n    BERT_LANG_MODEL_TEST_PROBABILITIES,\n    BERT_TABULAR_MODEL_TEST_PROBABILITIES,\n    ULMFIT_TEST_PROBABILITIES,\n])","90b72d96":"def identity_unpack(*args):\n    return _, args\n\n(XGBOOST_MODEL_STACKED_VALIDATION_PROBABILITIES,\n XGBOOST_MODEL_STACKED_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_TRAINING_SET),\n        pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_VALIDATION_SET),\n        pd.DataFrame(STACKED_TEST_PREDICTIONS_TEST_SET),\n        VALIDATION_SPLIT_VALIDATION_DATAFRAME,\n        STACKED_VALIDATION_PREDICTIONS_LABELS_TRAINING_SET.reset_index(drop=True),\n        STACKED_VALIDATION_PREDICTIONS_LABELS_VALIDATION_SET.reset_index(drop=True),\n        identity_unpack,\n        train_xgboost_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'colsample_bytree': [0.8],\n            'gamma': [2],\n            'max_depth': [3],\n            'min_child_weight': [5],\n            'n_estimators': [100],\n            'subsample': [0.8]\n        }\n    )\n)","4fee3541":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        XGBOOST_MODEL_STACKED_TEST_PROBABILITIES\n    ),\n    'xgboost_stacked_submissions.csv'\n)","fdd71519":"(LOGISTIC_REGRESSION_MODEL_STACKED_VALIDATION_PROBABILITIES,\n LOGISTIC_REGRESSION_MODEL_STACKED_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_TRAINING_SET),\n        pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_VALIDATION_SET),\n        pd.DataFrame(STACKED_TEST_PREDICTIONS_TEST_SET),\n        VALIDATION_SPLIT_VALIDATION_DATAFRAME,\n        STACKED_VALIDATION_PREDICTIONS_LABELS_TRAINING_SET.reset_index(drop=True),\n        STACKED_VALIDATION_PREDICTIONS_LABELS_VALIDATION_SET.reset_index(drop=True),\n        identity_unpack,\n        train_logistic_regression_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'C': [1.0],\n            'class_weight': [None],\n            'penalty': ['l2']\n        }\n    )\n)","bfb88070":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        LOGISTIC_REGRESSION_MODEL_STACKED_TEST_PROBABILITIES\n    ),\n    'logistic_stacked_submissions.csv'\n)","e23dc5a4":"(GUIDED_XGBOOST_MODEL_STACKED_VALIDATION_PROBABILITIES,\n GUIDED_XGBOOST_MODEL_STACKED_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        pd.concat((pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_TRAINING_SET),\n                   TRAINING_SPLIT_FEATURES_VALIDATION_DATAFRAME.reset_index().drop([\"index\"], axis=1)), axis=1),\n        pd.concat((pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_VALIDATION_SET),\n                   VALIDATION_SPLIT_FEATURES_VALIDATION_DATAFRAME.reset_index().drop([\"index\"], axis=1)), axis=1),\n        pd.concat((pd.DataFrame(STACKED_TEST_PREDICTIONS_TEST_SET),\n                   FEATURES_TEST_DATAFRAME.reset_index().drop(\"index\", axis=1)), axis=1),\n        VALIDATION_SPLIT_VALIDATION_DATAFRAME,\n        STACKED_VALIDATION_PREDICTIONS_LABELS_TRAINING_SET.reset_index(drop=True),\n        STACKED_VALIDATION_PREDICTIONS_LABELS_VALIDATION_SET.reset_index(drop=True),\n        featurize_for_tree_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_xgboost_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'colsample_bytree': [0.6],\n            'gamma': [2],\n            'max_depth': [3],\n            'min_child_weight': [1],\n            'n_estimators': [100],\n            'subsample': [0.8]\n        }\n    )\n)","f4183a6b":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        GUIDED_XGBOOST_MODEL_STACKED_TEST_PROBABILITIES\n    ),\n    'guided_xgboost_stacked_submissions.csv'\n)","f3ceefde":"(GUIDED_LOGISTIC_MODEL_STACKED_VALIDATION_PROBABILITIES,\n GUIDED_LOGISTIC_MODEL_STACKED_TEST_PROBABILITIES) = gc_and_clear_caches(\n    train_model_and_get_validation_and_test_set_predictions(\n        pd.concat((pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_TRAINING_SET),\n                   TRAINING_SPLIT_FEATURES_VALIDATION_DATAFRAME.reset_index().drop([\"index\"], axis=1)), axis=1),\n        pd.concat((pd.DataFrame(STACKED_VALIDATION_PREDICTIONS_VALIDATION_SET),\n                   VALIDATION_SPLIT_FEATURES_VALIDATION_DATAFRAME.reset_index().drop([\"index\"], axis=1)), axis=1),\n        pd.concat((pd.DataFrame(STACKED_TEST_PREDICTIONS_TEST_SET),\n                   FEATURES_TEST_DATAFRAME.reset_index().drop(\"index\", axis=1)), axis=1),\n        VALIDATION_SPLIT_VALIDATION_DATAFRAME,\n        STACKED_VALIDATION_PREDICTIONS_LABELS_TRAINING_SET.reset_index(drop=True),\n        STACKED_VALIDATION_PREDICTIONS_LABELS_VALIDATION_SET.reset_index(drop=True),\n        featurize_for_tabular_models(DROP_COLUMNS, CATEGORICAL_FEATURES),\n        train_logistic_regression_model,\n        predict_with_sklearn_estimator,\n        train_param_grid_optimal={\n            'C': [1.0],\n            'class_weight': [None],\n            'penalty': ['l2']\n        }\n    )\n)","215f5280":"write_predictions_table_to_csv(\n    get_prediction_probabilities_with_columns_from_predictions(\n        FEATURES_TEST_DATAFRAME['listing_id'],\n        GUIDED_LOGISTIC_MODEL_STACKED_TEST_PROBABILITIES\n    ),\n    'guided_logistic_stacked_submissions.csv'\n)","bed69d59":"### 2.8 Cleaning Description\nThe text in the descriptions are pretty messy. We can clean it up by applying some normalization (eg, removing repeated symbols, normalizing whitespace, etc)","47300cd5":"## 3) Fitting models","da807e5d":"### 4 Ensembling\n\nNow that we have all of our models and various predictions, we can\nensemble them together in the form of one big logistic regression model or gradient boosted tree to work out what the \"true\" classes are based on how all the different models were voting.\n\nNotice that the confusion matrix for each of the models was quite\ndifferent - this indicates that each model is probably more biased towards certain features. We can get the predictions for all of our training data and put them together into another dataset which we then apply XGBoost and a linear model to. In principle this allows us to decide amongst the learners by comparing their probability distributions.\n\nWe also only do the stacking on the validation set by splitting the validation set again into training and validation data - we don't use the  original training data since that's what the learners themselves were trained on.","48546634":"### 2.9 Drop unnecessary columns","0f078f1e":"### 3.6.1 FastText","d2c88cdf":"Finally, test set: we don't have the labels here (obviously), but we do have the count of photos. We get similar-ish distributions, properties with a higher photo count tend to be a little more expensive, but not much new information there.","e7909b49":"Now we need to encode our tokens as **padded** sequences of integers such that we have a matrix of length [n $\\times$ max_len].\n\nTo start our with, PyTorch takes one-hot encoded data as a single array of integers, where each integer specifies the index into some sparse vector where a $1$ will be set. Of course, if you have such sparse vectors, you can save a lot of time on the multiplication by just picking the right dimension and ignoring all the zero ones, which is exactly what happens internally.\n\nThe reason why we need padding is for computational efficiency reasons - we want to push a large batch of sentences on to the GPU for parallel computation all, but in order for this to work we need to pass the GPU a big square matrix. This means that the matrix will have at least as many columns as the maximum number of tokens in a sentence, where every other shorter sentence will be padded by a special \"<PAD>\" token. We also keep the length of every unpadded sentence in a separate vector - we'll see later that this is used by torch as an optimization to prevent the RNN from running over all the padding tokens within a batch.","12a00314":"There's quite a few words here that don't add much value. We can remove them by consulting a list of stopwords","408238eb":"Now that we have made our data nicer to work with, we can drop all the text-only columns and keep a \"features\" dataset, eg one that can be fed into our models (with a little extra work)","eae1ca13":"Note: It is likely that this kernel won't run on the Kaggle infrastructure, you can view it here: https:\/\/github.com\/smspillaz\/aalto-CS-kaggle-competitions\/blob\/master\/two-sigma-rental-interest\/kernel.ipynb","329294de":"Right now the interest level is encoded on a scale of \"Low, Medium, High\". The competition\nwants us to classify the entries in to each, so we assign a label","87af0987":"## 1 Exploratory Data Analysis","f3a22377":"## 1.2 Outlier Removal","0d530d86":"# Two Sigma: Rental Interest Competition","9b6f1146":"The test set is distributed in a simialr way, though it has more properties to the north and south east.","d660765e":"Now we can try out a few models and see what works well for the data that\nwe have so far.","e1691458":"### 3.6 Neural Net (Text Classification) Approaches","9ffe4c40":"#### 4.3.2 Guided Ensemble - Logistic Regression","5bb23d4c":"Now that we have our slightly tidied up categories, we can create some n-grams and count their frequency","3e4770ee":"Now we convert the labels into tensors, but we only put the labels\non the GPU (not the rest of the data, we'd run out of memory). Skorch\nwill conveniently put each batch on the GPU for us, so we don't need to\nworry about that.","ad80cd69":"For convenience, pull out the labels for training","9eaafd6b":"Let's clean up the categories and put them into a sensible vector. Unfortunately the categories are a bit of a mess - since the user can specify what categories they want there isn't much in the way of consistency between categories.\n\nSome of the patterns that we frequently see in the categories are:\n - Separating category names with \"**\"\n - Mix of caps\/nocaps\n - Some common themes, such as:\n   - \"pets\"\n   - \"office\"\n   - \"living room\"\n   - \"garden\"\n   - \"common area\"\n   - \"storage\"\n   - \"no pets\"\n   - \"parking\"\n   - \"bicycle\"\n   - \"doorman\"\n   - etc\n\nTo deal with this, lets pull out all of the categories and normalize them\nby removing excess punctuation, normalizing for whitespace, lowercasing, and counting for certain n-grams.","65ea5a1e":"### 4.3 Guided Ensembles\n\nWe can also try to \"guide\" the ensembles by concatenating our features and then doing predictions based on that.\n\nUnfortunately this seems to be more of a distraction than a help - we actually perform *worse* on validation set once we start introducing our original data back in.","86002579":"Let's see what this table looks like. We'll display the head of the table which shows its features","509f76c5":"### 4.1 Ensembling with XGBoost","cf9f5655":"## 3.3 Random Forest\n\nRandom forest is basically an ensemble of lots of decision trees and we average out the results from each tree.","b778cafe":"### 3.6.5 ULMFiT\nThis is basically transfer learning onto an AWD-LSTM (used by fastai).\n\nWe have to use the fastai API here directly, since the best\nimplementation I found of this was by Jeremy Howard himself.\n\nNote before we ran the notebook, we used `train_lm.py` (pointing it\nto the training data) to fine-tune the existing language model,\nwhich in principle is good at English (WikiText-103) and made transferred\nwhat we knew about English word prediction to predicting RentHop\ndescriptions, fine tuning on the RentHop description task. This is\ndifferent from just using word vectors, since you get the benefit\nof the entire language model and contextual information, not just\nthe vectors themselves.\n\nIt is critical here that we load the same vocabulary used to fine\ntune the network into the model when we load in the weights. Also,\nwe need to do the train-test split ourselves, because there\nis a bug in the library where the vocabulary is only computed\non the training set and not the validation set, meaning that if the\nsplits are random you could miss words.\n\nUnfortunately, the fastai API is very involved, making it difficult\nto wrap with skortch without breaking stuff, so we have to use it\nin a slightly different way to do the same thing. We can also only\nreport statistics on the first batch, hopefully that should be enough.","45ed1796":"## 2 Data Cleaning and Feature Engineering","d96cd661":"## 1.1 Initial Data Visualization\n\nTake all the numerical features and show some statistics on all of them.\n\nImmediately we can tell the following:\n - Price seems to have a pretty high range on the log scale, going from $40 \\to 449000$. That's not particularly helpful since we think this is probably a pretty important feature, so lets filter out some of the super-high priced stuff.\n - Most properties are within the sweet spot of \"-74 to -73\" longitude and \"40.70 to 40.80 latitude\". There's a few others that aren't, so maybe better to filter those out.\n - Clearly there is some bogus data. Some properties have a lat\/long of 0 which is incorrect. Unfortunately this also exists on the test set, but there are probably so few that we don't care.","d0690e69":"We compare the validation split to the test data above to ensure that that we're sampling from a similar distribution. On the whole, we appear to be - the distribution of labels is about the same and this seems to have leaked into the distribution of proeprties geogrpahically (though the test data seems to have more properties from the northeastern peninsula). With a bit more time we probably could have tried to address this problem by also stratifying the validation split so that we included properties from that region, too.","91c44a40":"### 4.2 Ensembling with Logistic Regression","50ad1cfe":"### 3.6.4 BERT\n\nBERT is a big ol' attentional model.\n\nWe use the pretrained version of BERT (eg `bert-base-uncased`) and the\ncorresponding tokenizer - `bert_featurize_dataframe` reads from\n`clean_description` in the dataframe and tokenizes sentences in the\nsame way that the pre-trained model was tokenized and should in principle\ndo the word vector mapping for us.\n\nEssentially what we are doing here is fine-tuning the classification\nlayers of BERT.","dea23b24":"### 2.3 Cleaning up interest_level","6ab3082c":"#### 3.6.1 Simple RNN\/LSTM","045e8c1a":"### 3.1 Logistic Regression\n\nThis is just baseline Logistic Regression. The 'C' parameter is a regularization strength.\n\nThe 'penalty' paramter specifies the regularization penalty to be applied. 'l2' is the default, which basically\nprevents any one particular weight from getting too large. 'l1' promotes sparse solutions.","cbd41f78":"Lets remove our outliers the training set.","95b2881b":"A key concept here is that we compute the bigrams of a sentence and then apply them to the end of the sentence.\n\nThen we just put the whole thing through linear layers after average pooling, the average pooling takes into account the big-grams.","8cf41dfe":"Check GPU support","9b3199f2":"Now that we have these, we can probably take 100 most common and arrange\nthem into category flags for our table","c32e804a":"### 2.4 Cleaning up building_id, manager_id","6594a3cb":"## 3.4 ExtraTrees\n\nExtratrees is also another ensembling based model. It stands for \"Extremely Randomized Trees\". Its main property is that it reduces variance for a small incrase in bias.","346110d1":"Here we leverage the actual text in the \"description\" field to try and do the classification, both independently and on top of the tabular data using Neural Nets with PyTorch.\n\nBefore we do that, it will be convenient to split our\ndata into continuous and categorical sections (as the categorical\nsections will be put through independent embeddings in each\nmodel) and tensorify some of our data, so lets do that now.\n\nNote that the continuous data needs to be scaled to have zero mean\nand unit variance - this prevents saturation of activation units\nin the upper classification layers.","b225a002":"Before we start putting our data into the RNN, lets tokenize our descriptions. In order to do this we'll be using fastai's Tokenizer class. We can preview the result of tokenization below","058d250c":"## 3.4 Adaboost\n\nIn this approach, we combine several \"weak\" classifiers into a \"strong\" classifier. It is actually a meta-learning technique, though we use a decision tree as the base model.","e13008d2":"### 2.7 Feature Engineering on Numerical Columns\nSome models can't do simple math, but ratios or additions\/subtractions\nbetween things might be important. Lets do that now for all of our\nnumerical data, but only for XGBoost, later","2c30acc7":"We can also do something useful with the listing date - it may be better to say how many days ago the property was listed - older properties are probably going to get a lot less interest than newer properties.","87094f0e":"### 3.2 XGBoost\n\nXGBoost is a histogram based model that applies boosting to an ensemble of weaker learners (decision trees). It generally performs quite well on Kaggle competitions and is also another baseline to use.","c40c603f":"### 2.10 Visualizing Again\n\nWe can do the pairplots again now that we have featurized a little bit more and also to validate that our train\/test split makes sense.\n\nIts hard to visualize categorical features in a pairplot so we don't do that. Instead, we visualize the photos_count and label_interest level to see if any feature in particular is highly correlated with the interest level.\n\nWe find that at least on both the training and validation data, nothing *really* is, except perhaps the latitude and longitude, indicating that location seems to be the most important factor in determining how much interest a property gets.\n\nAlso, the numbert of photos is *negatively* correlated with interest (properties with more photos) had less interest overall.\n\nThe bottom right hand corner of the pairplot matrix tells us something particularly useful which is the class (im)balance. We have lots of properties with low interest (2) and few properties with high interest. This will be a challenge for us to deal with later. We tried over-sampling by duplicating but that didn't really help validation scores at all. There are also other oversampling techniques like SMOTE but they don't work with non-numeric data.","56cc5e15":"### 2.1) Cleaning up categories","7512f76d":"### 2.2 Cleaning up listing_date","41665049":"Here we roll our own simple LSTM classifier using Skorch and PyTorch.\n\nThe `SimpleRNNPredictor` model just takes a batch of encoded word-encoded\nsentences and runs them through a Bi-LSTM, then has a fully connected\nlayer sitting on top of the final \"hidden\" output of the LSTM (think\nof the hidden state being passed through the LSTM along with each encoded input for the entire sentence all the way up to the sentence length). Then we just predict the class based on the final hidden state.\n\nWhat Skorch does here is it implements the training loop, allows us\nto add \"hooks\" (for instance, scoring on every epoch, a progress bar,\ncheckpointing so that we only keep our best model by validation loss,\ncyclic learning rate scheduling and learning rate annealing (we reduce the learning rate if our validation loss is not going down)). It also wraps the model in an sklearn estimator-like API, so we can use it just like any other estimator.\n\nNote that the NoToTensorInLossClassifier implements a few fixes on top\nof NeuralNetClassifier - in particular `predict_proba` takes the exponent of the returned probabilities since we return log-softmax probabilities that get passed to `NLLLoss`.","145b6bde":"### 2.5 Parsing and Separating Out Address Components\nSome properties might be in the same neighbourhood, the same street or\npart of the same building. If we separate out the address components then\nwe might be able to get some more meaningful feature groupings.\n\nWe first parse all the components into their own columns and then map them into categories (dropping them later on).","bd251d50":"#### 4.3.1 Guided Ensemble - XGBoost","8e7281ab":"We can then run a pairplot analysis on both the training and test dataframes and notice that while there isn't much correlation between price and amenities, there is quite a big correlation between things like price and location. Properties in pricier areas tend to have more amenities.\n\nThe cell for the correlation between latitude and longitude looks a little bit like the Greater New York area.\n\nPrice wise, most properties are sitting in the $2000-4000 range, steeply dropping off after that. Also, there are definitely \"pricey\" and \"cheap\" areas, with most of the cheaper properties sitting in the -73.85 to -73.95 longitude.","b7c33f39":"### 3.6.3 TextCNN\nHere we use a convolutional neural network on the text.\n\nWe kind of have to think of text like an \"image\". The horizontal\ndimension is just each sentence (of variable length, but they all have\npadding at the end). Then on the vertical axis, we have the word vectors\nand we move our window across. Idea here is that similar to images, the\norder between words and between word vector components probably matters,\nso we take into account conceptual space and sentence space in the text\nimage domain.","48fcc6ab":"`building_id` and `manager_id` look a bit useless to us on the outside, but according to https:\/\/www.kaggle.com\/den3b81\/some-insights-on-building-id they are actually quite predictive of interest since 20% of the manager make up 80% of the rentals (we can also see this in their writing style as well).\n\nSince there aren't too many managers or buildings in total, we can convert these into category ID's where we'll pass them through an embedding later on.\n\nNote that we need to do this over both dataframes - since there could\nbe some managers that are in the test dataframe which are not in the training dataframe and vice versa.\n\nNote that we want to lump all the \"misc\" buildings and managers together\ninto a single building or manager since listings by \"non-property managers\" or \"non-frequently-rented-buildings\" are different from ones run by property managers.","8e7be8fb":"### 3.5 SVC\n\nWith Support Vector Machines we can make use of kernel functions in order to try and have non-linear fits on our data. For instance, below we use the radial-basis-function kernel, though it doesn't perform as well as we would like.","a403c300":"### 2.6 Counting Number of Photos\nThe number of photos a place has might be predictive of its interest as well, so lets at least count the number of photos.","68828582":"## 5 Conclusions\n\nWith all the work that we did, we were able to reduce the validation loss from a baseline linear model $\\approx 0.63$ to a much better $\\approx 0.537$.\n\nOn the Kaggle I scored $\\approx 0.58836$. Not great, but also not too bad either. Possibly other generated submissions may have scored better.\n\nEnsembling provided us the biggest benefit - we were able to combine all the knowledge we gained from different learners over different parts of the training data to make better classifications over the validation data.\n\nI was able to apply quite a lot of the knowledge I gained from this course into forming my model. In particular:\n * Where to look for guidance on the competition (eg, the forums, winning kernels, etc)\n * A better exploratory data analysis using Seaborn crossplots to look at correlation between the numerical features.\n * Feature engineering on numerical data by performing simple math, this helps decision tree based models that can't do this kind of transformation inherently.\n * Correct usage of sklearn - even though I wasn't able to use Pipelines very effectively, I was able to make use of meta-estimators like grid-search and leverage the estimator API.\n * Usage of ensembling methods - as stated before, the use of ensembling over may different methods provided quite a large benefit. I am sure that this benefit would have been even greater if we used bagging or boosting on the actual estimators themselves with subsets of the training data, though I didn't have enough time to implement that properly.\n * Through my own research, I learned quite a lot about language models and text classification approaches (FastText, TextCNN, BERT)."}}