{"cell_type":{"0b2b7855":"code","efc36aa1":"code","096cba06":"code","7627ddee":"code","d18705ce":"code","23f13e66":"code","804bf448":"code","a7b3e8a2":"code","39ca33b5":"code","f7f410ea":"code","2211664d":"code","d408a722":"markdown"},"source":{"0b2b7855":"import os\nimport librosa, librosa.display\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras import layers, models","efc36aa1":"sample_music = '..\/input\/gtzan-musicspeech-collection\/music_wav\/bagpipe.wav'\nsample_speech = '..\/input\/gtzan-musicspeech-collection\/speech_wav\/acomic.wav'\n\nsr = 22050\nsamplesNum = sr*30\nn_fft=255\nhop_length=128\n\ndef wave_plot(path, sr):\n  signal, sr = librosa.load(path, sr=sr)\n  librosa.display.waveplot(signal, sr=sr)\n  plt.title('Signal in time domian')\n  plt.xlabel('Time')\n  plt.ylabel('Amp')\n  plt.show()\n  return signal\n\ndef ftt_plot(signal, samplesNum):\n  magnitude = np.abs(np.fft.fft(signal))[:samplesNum\/\/2]\n  freq = np.linspace(0, sr, samplesNum)[:samplesNum\/\/2]\n  plt.plot(freq, magnitude)\n  plt.title('Signal in frequancy domian')\n  plt.xlabel('Freq')\n  plt.ylabel('Magnitude')\n  plt.show()\n\ndef spectrogram_plot(signal, n_fft, hop_length):\n    spectrogram = np.abs(librosa.core.stft(signal, n_fft=n_fft, hop_length=hop_length))\n    librosa.display.specshow(librosa.amplitude_to_db(spectrogram))\n    plt.title('Signal spectrogram')\n    plt.xlabel('Time')\n    plt.ylabel('Freq')\n    plt.colorbar()\n\n    plt.show()","096cba06":"# Explore a music sample\nsignal = wave_plot(sample_music, sr=sr)\nftt_plot(signal, samplesNum=samplesNum)\nspectrogram_plot(signal, n_fft=n_fft, hop_length=hop_length)","7627ddee":"# Explore a speech sample\nsignal = wave_plot(sample_speech, sr=sr)\nftt_plot(signal, samplesNum=samplesNum)\nspectrogram_plot(signal, n_fft=n_fft, hop_length=hop_length)","d18705ce":"def get_spectrogram(signal, frame_length, frame_step):\n    spectrogram = tf.signal.stft(signal, frame_length=frame_length, frame_step=frame_step)\n    spectrogram = tf.abs(spectrogram)\n    return tf.expand_dims(spectrogram, -1)","23f13e66":"sample_music = '..\/input\/gtzan-musicspeech-collection\/music_wav\/bagpipe.wav'\nsample_speech = '..\/input\/gtzan-musicspeech-collection\/speech_wav\/acomic.wav'\n\nlabels_index = {'music': 0, 'speech': 1}\nnum_labels = 2\ndata_dir = '..\/input\/gtzan-musicspeech-collection'\ndata_lst = []\n\nfor label in labels_index.keys():\n    class_dir = os.path.join(data_dir, label+'_wav')\n    aud_titles = os.listdir(class_dir)\n    for aud_title in aud_titles:\n        aud_dir = os.path.join(class_dir, aud_title)\n        signal, sr = librosa.load(aud_dir)\n        spectrogram = get_spectrogram(signal, frame_length=255, frame_step=128)\n        y = labels_index[label]\n        data_lst.append((spectrogram, y))\n\nprint('Number of samples: ', len(data_lst))\nprint('Spectrogram shape: ', data_lst[0][0].shape)\nprint('Spectrogram class: ', data_lst[0][1])","804bf448":"random.shuffle(data_lst)\n\nX=[]\ny=[]\n\nfor sample in data_lst:\n    X.append(sample[0])\n    y.append(sample[1])\n  \nX = tf.stack(X)\ny = tf.stack(y)\n\nprint('Spectrogram tensor: ', X.shape)\nprint('Labels tensor: ', y.shape)","a7b3e8a2":"input_shape = X[0].shape\n\nmodel = models.Sequential([\n    layers.Input(shape=input_shape),\n    preprocessing.Resizing(128, 128), \n    layers.Conv2D(32, 3, activation='relu'),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.MaxPooling2D(),\n    layers.Dropout(0.25),\n    layers.Flatten(),\n    layers.Dense(128, activation='relu'),\n    layers.Dropout(0.5),\n    layers.Dense(num_labels, activation='softmax'),\n])","39ca33b5":"model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=.001*.9),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n    metrics=['accuracy'],\n)","f7f410ea":"batch_size = 32\nepochs = 10\nhistory = model.fit(X, y, batch_size=batch_size, epochs=epochs, validation_split=.25)","2211664d":"# Plot the loss and accuracy curves for training and validation\nfig, ax = plt.subplots(2,1, figsize=(15,7))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training Accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation Accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","d408a722":"* Carefully studying the difference between Speech and Music Spectrograms, one can observe that in music spectrogram there is wider range of frequancies across time axis(x), Hence a well-trained simple cnn architecture can easily distinguish the difference between Speech and Music with a spectrogram as its input."}}