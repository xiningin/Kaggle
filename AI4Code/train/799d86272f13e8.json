{"cell_type":{"e319a93d":"code","36ae3fbc":"code","f3274d4d":"code","a09e53b1":"code","686b8d51":"code","7c531013":"code","96f98b1e":"code","d7c32a6a":"code","01ecb4a1":"code","80e91f98":"code","bb37c8b5":"code","01b550a8":"code","6c407012":"code","ce81c4ca":"code","84caaa1d":"code","c6db41ac":"code","87dd7171":"code","c38d183f":"code","8bd55724":"code","cf26e7d7":"code","2b7c06c7":"code","33a0344b":"code","eb823399":"code","b767c354":"code","3d3b5211":"code","a406afff":"code","1dc7601f":"code","76990746":"code","6193a2d2":"markdown","bda0a1c6":"markdown","bd97aabd":"markdown","e2a2c704":"markdown","fa38da84":"markdown","1b5fe14d":"markdown","5d4350e0":"markdown","b7dfd76a":"markdown","2d9beb5a":"markdown","9352cd15":"markdown","a84ef8ce":"markdown","ed080d73":"markdown","e4e3c8b9":"markdown","3e7c6fde":"markdown","9c61d418":"markdown","6362b3b5":"markdown","bbb0cbba":"markdown","fa56b7bf":"markdown"},"source":{"e319a93d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nfrom keras.preprocessing import image\nfrom tqdm import tqdm \nimport time\nimport cv2\nimport datetime\nimport xgboost as xgb\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\n!pip install yfinance\nimport yfinance as yf\nfrom sklearn.svm import SVR\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.applications.vgg16 import VGG16\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARIMA","36ae3fbc":"covid_data = pd.read_csv(\"\/kaggle\/input\/ntt-data-global-ai-challenge-06-2020\/COVID-19_and_Price_dataset.csv\")\nts = pd.read_csv(\"\/kaggle\/input\/ntt-data-global-ai-challenge-06-2020\/Crude_oil_trend_From1986-01-02_To2020-06-08.csv\")\nts[\"Diff\"] = ts[\"Price\"].diff().dropna()","f3274d4d":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\naxes[0].plot(ts[\"Price\"])\naxes[0].set_title(\"Oil Price\")\naxes[1].plot(ts[\"Diff\"])\naxes[1].set_title(\"Oil Price Differences\")\nfig.tight_layout()","a09e53b1":"f = plt.figure(figsize=(24, 16))\n\nfor i in range(0,6):\n    im = image.load_img('\/kaggle\/input\/ntt-data-global-ai-challenge-06-2020\/NTL-dataset\/tif\/Italy-20200' + str(i+1) + '01.tif', color_mode='grayscale').convert(\"L\")\n    it = np.array(im)\n    f.add_subplot(3, 2, i+1)\n    plt.imshow(it)\n    \nplt.show()","686b8d51":"# model = VGG16(weights='imagenet', include_top=False)\n\ndef get_datetime_img(imgstr):\n    date = datetime.datetime(int(imgstr[-12:-8]),int(imgstr[-8:-6]),int(imgstr[-6:-4]))\n    return date\n                                 \ndef get_datetime_dtset(i):\n    date = datetime.datetime(int(i.split(\"-\")[0]),int(i.split(\"-\")[1]),int(i.split(\"-\")[2]))\n    return date\n\ndef get_img_features(img_):\n    img = image.load_img(img_, target_size=(224, 224)) \n    x = image.img_to_array(img) \n    x = np.expand_dims(x, axis=0) \n    x = preprocess_input(x) \n    features = model.predict(x) \n    # print(features.shape)\n    features_reduce = features.flatten() \n    return features_reduce\n\ndef get_cont_bright(img_):\n    img = image.load_img(img_, target_size=(224, 224),color_mode='grayscale') \n    img = np.array(img)\n    bright = np.sum(img>220)\n    cont = np.amax(img)-np.amin(img)\n    return bright, cont\n    \ndef get_img_dataset():\n    img_features = covid_data[\"Date\"]\n    for dtset in [\"Italy\", \"USA\", \"China\"]: #, \"France\"]:\n        feature_list = [dtset+str(x) for x in [\"Brightness\",\"Contrast\"]] # range(0,25088)\n        tmp = pd.DataFrame(0, index=pd.Series(range(0,110)), columns=feature_list)\n        img_features = pd.concat([img_features, tmp], axis=1)\n        for dirname, _, filenames in os.walk('\/kaggle\/input'):\n            for filename in sorted(filenames):\n                if dtset in filename and \".tif\" in filename:\n                    # feats = get_img_features(os.path.join(dirname, filename))\n                    bright, cont = get_cont_bright(os.path.join(dirname, filename))\n                    for j in img_features[\"Date\"]:\n                        if j == \"2019-12-31\" and \"20200101\" in filename:\n                            # img_features.loc[img_features.Date==j,feature_list] = feats\n                            img_features.loc[img_features.Date==j,dtset+\"Brightness\"] = bright\n                            img_features.loc[img_features.Date==j,dtset+\"Contrast\"] = cont\n                        if get_datetime_img(filename) == get_datetime_dtset(j):\n                            # img_features.loc[img_features.Date==j,feature_list] = feats\n                            img_features.loc[img_features.Date==j,dtset+\"Brightness\"] = bright\n                            img_features.loc[img_features.Date==j,dtset+\"Contrast\"] = cont\n        print(\"Finished:\",  dtset)\n    return img_features","7c531013":"train_ft = get_img_dataset()","96f98b1e":"# Plotting extracted features\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n\ntrain_ft.plot(kind='line',x='Date',y='ItalyBrightness', color='red', ax=ax[0])\ntrain_ft.plot(kind='line',x='Date',y='USABrightness', color='blue', ax=ax[0])\ntrain_ft.plot(kind='line',x='Date',y='ChinaBrightness', color='green', ax=ax[0])\n\ntrain_ft.plot(kind='line',x='Date',y='ItalyContrast', color='red', ax=ax[1])\ntrain_ft.plot(kind='line',x='Date',y='USAContrast', color='blue', ax=ax[1])\ntrain_ft.plot(kind='line',x='Date',y='ChinaContrast', color='green', ax=ax[1])\n\nplt.show()","d7c32a6a":"Yahoo_indeces=[\"^GSPC\",\"^DJI\",\"NQ=F\",\"^FTSE\",\"^GDAXI\",\"^ISEQ\",\"^N225\",\"^XU100\",\n               \"^FCHI\",\"IMOEX.ME\",\"^OMX\",\"^OSEAX\",\"XIU.TO\",\"^BVSP\",\"^MXX\",\"^BSESN\",\"^SSE50\",\n               \"^KS11\",\"^NZ50\",\"^AXJO\",\"^BFX\",\"^ATX\",\"^PSI20\",\"BTCUSD=X\",\"EURUSD=X\",\n              \"MSFT\",\"AAPL\",\"AMZN\",\"FB\",\n        \"BLK\",\"JNJ\",\"V\",\"PG\",\"UNH\",\n        \"JPM\",\"INTC\",\"HD\",\"MA\",\"VZ\",\n        \"PFE\",\"T\",\"MRK\",\"NVDA\",\"NFLX\",\"DIS\",\n        \"CSCO\",\"PEP\",\"XOM\",\"BAC\",\"WMT\",\"ADBE\",\"CVX\",\"BA\",\"KO\",\"CMCSA\",\"ABT\",\"WFC\",\n         \"BMY\",\"CRM\",\"AMGN\",\"TMO\",\"LLY\",\"COST\",\"MCD\",\"MDT\",\"ORCL\",\"ACN\",\"NEE\",\"NKE\",\"UNP\",\"AVGO\",\"PM\",\"IBM\",\"LMT\",\"QCOM\",\"DAL\",\n              \"ETHUSD=X\", \"JPY=X\",\"GBPUSD=X\",\"VXX\",\"^VIX\"]\n\n\nStock_indeces=[\"S&P500\",\"DowJones\",\"NASDAQ100\",\"FTSE100\",\"DAX\",\"ISEQ\",\"NIKKEI225\",\"BIST100\",\n               \"CAC40\",\"MOEX\",\"OMXS30\",\"Oslo BorsAll-Share\",\"TSX\",\"IBOVESPA\",\"IPCMexico\",\"SHANGHAI50\",\n               \"SENSEX\",\"KOSPI\",\"NZX50\",\"ASX200\",\"BEL20\",\"ATX\",\"PSI20\",\"BTC\/USD\",\"EUR\/USD\",\n              \"Microsoft Corporation\",\"Apple Inc.\",\"Amazon\",\"Facebook\",\"BlackRock Inc.\",\"Johnson & Johnson\",\n             \"Visa Inc.\",\"Procter & Gamble\",\"UnitedHealth Group\",\"JPMorgan\",\n            \"Intel Corporation\",\"Home Depot Inc.\",\"Mastercard\",\"Verizon Communications Inc.\",\"Pfizer\",\"AT&T Inc.\",\"Merck & Co. Inc\",\n             \"NVIDIA Corporation\",\"Netflix Inc.\",\"Walt Disney Company\",\"Cisco Systems Inc.\",\n            \"PepsiCo Inc.\",\"Exxon Mobil Corporation\",\"Bank of America Corp\",\"Walmart Inc.\",\"Adobe Inc.\",\"Chevron Corporation\",\n             \"Boeing Company\",\"Coca-Cola Company\",\"Comcast Corporation Class A\",\"Abbott Laboratories\",\"Wells Fargo & Company\",\n            \"Bristol-Myers Squibb Company\",\"Salesforce\",\"Amgen Inc.\",\"Thermo Fisher Scientific Inc.\",\"Eli Lilly and Company\",\n             \"Costco Wholesale Corporation\",\"McDonald's Corporation\",\"Medtronic Plc\",\"Oracle Corporation\",\"Accenture Plc Class A\"\n             ,\"NextEra Energy Inc.\",\n            \"NIKE Inc. Class B\",\"Union Pacific Corporation\",\"Broadcom Inc.\",\"Philip Morris International\",\"International Business Machines Corporation\"\n             ,\"Lockheed Martin Corporation\",\"QUALCOMM Incorporated\",\"Delta Air\",\"Ethereum-USD\",\"Yen-USD\",\"Pound-USD\",\n              \"VIX-1Month\",\"VIX\"]\n\nfor i in range(len(Stock_indeces)):\n    data = yf.download(Yahoo_indeces[i], start=\"2019-01-02\", end=\"2020-08-31\")\n    stock_hist=pd.DataFrame(data[['Close','Volume']])\n    stock_hist['Stock_indeces']=Stock_indeces[i]\n    if i == 0:\n        stock_history=stock_hist\n    else:\n        stock_history=pd.concat([stock_history,stock_hist])","01ecb4a1":"stock_history['date']=stock_history.index\ncountry_indeces=stock_history.pivot_table(index='date', columns='Stock_indeces', values='Close',fill_value=0)\ncountry_indeces['Date']=country_indeces.index\ncountry_indeces=country_indeces.reset_index(drop=True, inplace=False)\ncountry_indeces['Date']=country_indeces['Date'].dt.strftime('%Y-%m-%d') #date column in downloaded data is not in wanted type.So I changed its type in here","80e91f98":"# Using covid world cases and deaths as features as well\ndata = covid_data[[\"Price\", \"Date\", 'World_total_cases', 'World_total_deaths']]\ndata = pd.merge(data, country_indeces, on='Date')\ndata = pd.merge(data, train_ft, on=\"Date\")\n\ndata[\"Price\"] = data[\"Price\"].diff()\n\ndata = data.drop(\"Date\", axis=1) \n\ndata.head()","bb37c8b5":"def series_timedep(data, n_in=1, n_out=1, dropnan=True):\n    n_vars = 1 if type(data) is list else data.shape[1]\n    df = DataFrame(data)\n    cols, names = list(), list()\n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n        else:\n            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n\n\n# load dataset\nvalues = data\n# ensure all data is float\nvalues = values.astype('float32')\n\n# normalize features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\n\nreframed = series_timedep(scaled, 0, 1)\ntrain_x, train_y, test_x, test_y = reframed.iloc[:81, 1:].values, reframed.iloc[:81, 0].values, reframed.iloc[81:, 1:].values, reframed.iloc[81:, 0].values","01b550a8":"model_xg = xgb.XGBRegressor(colsample_bytree=0.5,\n                 gamma=0,                 \n                 learning_rate=0.005,\n                 max_depth=5,\n                 min_child_weight=0.25,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=1,\n                 reg_lambda=1,\n                 subsample=1,\n                 seed=777) \n\nmodel_xg.fit(train_x, train_y)","6c407012":"# make a prediction\nyhat_xg = model_xg.predict(test_x)\nyhat_xg = yhat_xg.reshape((len(yhat_xg), 1))\n# invert scaling for forecast\ninv_yhat_xg = np.concatenate((yhat_xg, test_x), axis=1)\ninv_yhat_xg = scaler.inverse_transform(inv_yhat_xg)\ninv_yhat_xg = inv_yhat_xg[:, 0]\n# invert scaling for actual\ntest_y = test_y.reshape((len(test_y), 1))\ninv_y = np.concatenate((test_y, test_x), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:, 0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat_xg))\nprint('Test RMSE: %.3f' % rmse)","ce81c4ca":"model_svm = SVR(kernel='rbf', C=1e5, gamma=\"scale\")\n\nmodel_svm.fit(train_x, train_y)","84caaa1d":"# make a prediction\nyhat_svm = model_svm.predict(test_x)\nyhat_svm = yhat_svm.reshape((len(yhat_svm), 1))\n# invert scaling for forecast\ninv_yhat_svm = np.concatenate((yhat_svm, test_x), axis=1)\ninv_yhat_svm = scaler.inverse_transform(inv_yhat_svm)\ninv_yhat_svm = inv_yhat_svm[:, 0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat_svm))\nprint('Test RMSE: %.3f' % rmse)","c6db41ac":"# reshape input to be 3D [samples, timesteps, features]\nLSTMtrain_x = train_x.reshape((train_x.shape[0], 1, train_x.shape[1]))\nLSTMtest_x = test_x.reshape((test_x.shape[0], 1, test_x.shape[1]))\nprint(LSTMtrain_x.shape, train_y.shape, LSTMtest_x.shape, test_y.shape)","87dd7171":"# design network\ndropout = 0.1\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(100, return_sequences=True, input_shape=(LSTMtrain_x.shape[1], LSTMtrain_x.shape[2]), dropout=dropout, recurrent_dropout=dropout))\nmodel_lstm.add(LSTM(50, return_sequences=True, dropout=dropout, recurrent_dropout=dropout))\nmodel_lstm.add(LSTM(25, dropout=dropout, recurrent_dropout=dropout))\nmodel_lstm.add(Dense(1))\nmodel_lstm.compile(loss='mse', optimizer='adam')","c38d183f":"# fit network\nhistory = model_lstm.fit(LSTMtrain_x, train_y, epochs=1000, validation_data=(LSTMtest_x, test_y), batch_size=8, verbose=0, shuffle=False)\n# plot history\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","8bd55724":"# make a prediction\nyhat_lstm = model_lstm.predict(LSTMtest_x)\n# invert scaling for forecast\ninv_yhat_lstm = np.concatenate((yhat_lstm, test_x), axis=1)\ninv_yhat_lstm = scaler.inverse_transform(inv_yhat_lstm)\ninv_yhat_lstm = inv_yhat_lstm[:, 0]\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat_lstm))\nprint('Test RMSE: %.3f' % rmse)","cf26e7d7":"#plot acf and pacf\nplot_acf(ts[\"Diff\"][-1000:], lags=50)\nplot_pacf(ts[\"Diff\"][-1000:], lags=50)\nplt.show()","2b7c06c7":"model_arima = ARIMA(ts[\"Diff\"][-1000:].to_numpy(), order=(1,0,1))\nmodel_arima = model_arima.fit()\nprint(model_arima.summary())","33a0344b":"# plot residual errors\nresiduals = pd.DataFrame(model_arima.resid)\nresiduals.plot()\nplt.show()\nresiduals.plot(kind='kde')\nplt.show()\nprint(residuals.describe())","eb823399":"ts_train = ts[\"Diff\"][-1000:].to_numpy()\nyhat_arima = []\n\nfor i in range(len(inv_y)):\n    model_arima1step = ARIMA(ts_train, order=(1,0,1)).fit(disp=-1)\n    yhat_arima1step = model_arima1step.forecast(1)[0]\n    yhat_mix1step = (yhat_arima1step + inv_yhat_lstm[i] + inv_yhat_svm[i] + inv_yhat_xg[i])\/4\n    ts_train = np.append(ts_train, yhat_mix1step, axis=0)\n    yhat_arima.append(yhat_mix1step)\n\nyhat_arima = np.array(yhat_arima).reshape((len(yhat_svm), ))\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, yhat_arima))\nprint('Test RMSE: %.3f' % rmse)","b767c354":"ensemble_preds = (yhat_arima + inv_yhat_lstm + inv_yhat_svm + inv_yhat_xg)\/4\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, ensemble_preds))\nprint('Test RMSE: %.3f' % rmse)","3d3b5211":"base = covid_data[\"Price\"][0]\norig = []\n\nfor i in range(1, len(covid_data[\"Price\"])+1):\n    orig.append(round(base + sum(data[\"Price\"][1:i]),6))\n\n# check there is no mistake in undoing the differencing\nprint((orig == round(covid_data[\"Price\"], 6)).all())","a406afff":"def get_price(preds):\n    base_test = covid_data[\"Price\"][81]\n    oil_preds = []\n    for i in range(len(preds)):\n        oil_preds.append(round(base_test + sum(preds[0:i]),6))\n    return oil_preds\n\nfinal_preds = get_price(ensemble_preds)\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(orig[82:], final_preds))\nprint('Test RMSE: %.3f' % rmse)","1dc7601f":"# Our test data is the last month of available data\n\nplt.plot(orig[82:], color=\"black\", label=\"Oil Price\")\nplt.plot(final_preds, color=\"red\", label=\"Ensemble Prediction\")\nplt.plot(get_price(yhat_arima), color=\"green\", label=\"ARIMA Prediction\")\nplt.plot(get_price(inv_yhat_lstm), color=\"yellow\", label=\"LSTM Prediction\")\nplt.plot(get_price(inv_yhat_xg), color=\"violet\", label=\"XGBoost Prediction\")\nplt.plot(get_price(inv_yhat_svm), color=\"blue\", label=\"SVM Prediction\")\nplt.legend()\nplt.show()","76990746":"sub = pd.read_csv(\"\/kaggle\/input\/ntt-data-global-ai-challenge-06-2020\/sampleSubmission.csv\")\nsub[\"Price\"] = final_preds\nsub.to_csv(\"sampleSubmission.csv\",index=False)","6193a2d2":"# **Index**\n\n* [Loading packages](#packages)\n* [Data and Plots](#dataandplots)\n* [Getting Features](#gettingfeatures)\n* [XGBoost](#xgboost)\n* [SVM](#svm)\n* [LSTM](#lstm)\n* [ARIMA](#arima)\n* [Ensembling](#ensemblingresults)\n* [Final Comments](#finalcomments)","bda0a1c6":"## **Data and Plots**\n<div id=\"dataandplots\">\n<\/div>","bd97aabd":"## **Getting Features**\n<div id=\"gettingfeatures\">\nThis is the same procedure as the notebook mentioned at the beginning does. Only a few other indeces where added like the VIX, a volatility index and a few other currencies' exchange rates.\n<\/div>","e2a2c704":"In this particular case, as the model needs more data to work properly, we took the last 1000 time values, and looked at the first lags values for acf and pacf. The first plot shows that there is a strong AR component in the model, as there is dependency on the previous values. the pacf plot shows also that a weak MA component is present.\n\nThe model results below show that our assumptions were correct and the presence of both components is statistically significant.","fa38da84":"The images change greatly over time, in fact for the first day of each month, we can look at how the images of Italy get darker until March, and in April and May more lights are present. It should also be noted that sme images are whited out because of the presence of clouds.\n\n\nThe next step is to feature engineer the images into usable features by the models. Two ways are shown below, the one commented out utilizes a CNN pretrained model (VGG16), and the flattened output of the last layer of the model is used to create the features. Overall it creates 25088 features for each image.\n\nThe other method involves getting the images' brightness (values greater than 220, this value is not set though), and contrast (difference between max pixel value and min pixel value in the image).\n\nThe images are also rescaled to 224x224, specifically for the VGG model input, but as they have different shape, it may still be useful to do it for the brightness and contrast features.\nThe other functions allow us to assign the image to the correct data in the dataset.\n\n\nAs the VGG method takes some time (and spoiler: it doesn't change much), we will use brightness and contrast as features.","1b5fe14d":"Now that we have all our features and datasets in order, we can merge them all through the date column, and then remove it for the modelling.","5d4350e0":"There are some peaks in brightness simultaneously all over the world every month. It would be interesting to try to understand why, but it falls out of current analysis. As for the contrast, it is almost always 255, except for one day in China. This feature is most likely irrelevant.","b7dfd76a":"## **ARIMA**\n<div id=\"arima\">\nARIMA is one of the basic tools for time series analysis and has a long history of work behind it, but it is still very much used. ARIMA is a parametric modelling approach where the current value at time t is modelled as a function of past values and linear combination of error terms. The integration part means the differencing that we already performed.\n  <br>\n  The plots of the Autocorrelation function (acf) and Partial Autocorrelation function (pacf) help us in understanding the structure of the model based on the data.\n<\/div>","2d9beb5a":"## **Ensembling results**\n<div id=\"ensemblingresults\">\nSimply by taking the mean of all our predictions. A different weighting scheme can be thought for this step.\n<\/div>","9352cd15":"## **Loading packages**\n<div id=\"packages\">\n<\/div>","a84ef8ce":"The plot of the residuals indicates that the model fits that data quite well, as it is Gaussian distributed and centered around 0, but the presence of some outlier is quite obvious. In very volatile time series such as oil prices, this is to be expected.\n\nBelow, we model the data to obtain one-step predictions, while basically using the previous predictions as a means to convey external information to the model. If that would not have been done, as time passes the fit of this particular model would be very poor.","ed080d73":"# **Introduction**\n\nWelcome to this notebook on multiple Machine Learning and Statistical methods to tackle the task of predicting future oil prices, or more in general to forecast on multivariate time series.\n\nThe target variable for the models, however, will not be the oil prices, but the first order difference. This is done to remove the trend from the series, as it stabilizes the mean. A very useful analysis of this was done in another notebook which is linked below. At the end, though, we will undo the transformation and get the final results in the same scale.\n\nBefore the modelling aspect, we will use images, the covid data and stock and currency prices. Although NTL images may not be correlated with the current task, I think it's still valuable to show a couple of way to integrate them into an eventual model. Thus, the task of choosing which features are relevant will be left for another time. ;)\n\nFor the current task, 3 different ML approaches(XGBoost, Support Vector Machine (SVM) and Long Short Term Memory (LSTM)) will be shown and one statistical parametric approach (Autoregressive Integrated Moving Average (ARIMA) model).\n\nI would also like to thank the work of other people that shared their notebooks, if you find any interesting content, **please remember to upvote the useful notebooks!**\n\n[https:\/\/www.kaggle.com\/snndemirhan\/stock-prices-vs-oil-prices](https:\/\/www.kaggle.com\/snndemirhan\/stock-prices-vs-oil-prices)\n\n[https:\/\/www.kaggle.com\/sixxx6\/notes-on-time-series-analysis](https:\/\/www.kaggle.com\/sixxx6\/notes-on-time-series-analysis)","e4e3c8b9":"## **Support Vector Regression**\n<div id=\"svm\">\nSVMs are mostly used for classification problems, however they can also be used for regression. Different kernel types were tried on the data, but the standard radial basis kernel had the best results. Other hyperparameters were not tested.\n<\/div>","3e7c6fde":"## **XGBoost**\n<div id=\"xgboost\">\nFirst we will scale the data to 0-1, to improve the[](http:\/\/) model performance and then we will divide into train and test set, with about a 75%-25% split. The series_timedep function also allows us to add temporal dependence into the data by adding columns at previous time lags. For now we will use the same time values only to predict.\n  <br>\n  XGBoost is one of the most used algorithms for ML, it can handle any type of numerical data, categorical data if encoded as number and  NA values, moreover it is very fast. Hyperparameter tuning for this model, such as grid search was not performed.\n<\/div>","9c61d418":"## **LSTM**\n<div id=\"lstm\">\nLSTMs are a kind of neural network, particularly RNNs, which can model long-term dependency. As such they are widely used for time series forecasting, but also in other settings (NLP, Speech Recognition, etc.).\n<\/div>","6362b3b5":"As expected the original time series has very strong external trends, which are very difficult for the models to pick up, while the differenced time series is centered around 0 and stationary. This time series is much more easy to model, and to check it, I encourage you to try in the later parts of the notebook, to change the target variable to the original price. The results will vary greatly!\n\nNext let's start working on the other features our models will use to help predict the changes to oil prices.","bbb0cbba":"## **Final Comments**\n<div id=\"finalcomments\">\nThe predictions of our models are very close to the actual ones, up until 10 days, after that they seem to predict a faster fall of the oil prices. A simple way to improve the results, would of course be a better choice of the features, like including stocks and commodities prices which relate very heavily to the WTI oil price (such as Brent, which basically follows the same distribution as WTI), and removing uncorrelated ones(image features, unrelated stock prices), but the aim of this notebook is to show some of the main tools for forecasting time series.  \n  <br>\n  As data is added, the performances of the models should improve, as right now the data is very few. If one decided not to use the covid and image data, a bigger dataset could be easily used as well.\nAlso model finetuning for the ML models should be performed to improve their respective predictions.   \n  <br>\n  Some more updates may be coming in the next weeks, so stay tuned!\n<\/div>","fa56b7bf":"To get the actual RMSE on daily oil price, first we will check that our function to undo the differencing actually works, and the we will apply it to the predictions and check the final results."}}