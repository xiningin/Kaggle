{"cell_type":{"4f8e9e55":"code","a43e8de9":"code","940b35dc":"code","fb895a52":"code","5048b312":"code","d4724e49":"code","5c86beed":"code","bd96a841":"code","88231f94":"code","2eb41796":"code","45932778":"code","353a01f4":"code","d8671763":"code","4d386888":"code","c5a5c015":"code","b97ec1d5":"code","7c69b6e2":"code","19d285fc":"code","3a495796":"code","60e98fc9":"code","1cf092fb":"code","7bece862":"code","a6ac800a":"markdown","080b978e":"markdown","b6bb7945":"markdown","893c8e91":"markdown","3658977a":"markdown","4413e799":"markdown","4000697e":"markdown","cff49db7":"markdown","209ca209":"markdown","bc33f670":"markdown","d41947f6":"markdown","88e172a6":"markdown","bd741851":"markdown","c7f9ccc7":"markdown","391c5b06":"markdown","ea8aa670":"markdown","8ed1f2ce":"markdown","1db6ac95":"markdown","7425ad48":"markdown","0fb3b106":"markdown","9be1857a":"markdown","a2048dd8":"markdown","20a7fa9f":"markdown","da305657":"markdown","d53e72ee":"markdown","a1f7fec9":"markdown","2681e37d":"markdown","eb6d2e5a":"markdown","86f90204":"markdown","657aef7b":"markdown","4f61da5e":"markdown","ee02b2ca":"markdown","ddbe7d39":"markdown","457d46b7":"markdown","4c5bd322":"markdown","203f579d":"markdown","db48b980":"markdown","15e9a900":"markdown","8ac26ff1":"markdown","4578f6bd":"markdown","58452cce":"markdown","89cad9dd":"markdown","11096246":"markdown","e768d4fb":"markdown","dbe0d681":"markdown","2273eca7":"markdown","ecbaa4a1":"markdown","950f775f":"markdown","3d158e23":"markdown","3cb2fa32":"markdown"},"source":{"4f8e9e55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import boxcox\nfrom collections import Counter\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.svm import LinearSVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a43e8de9":"X = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ny = X['target']\n\nX = X.drop(['enrollee_id', 'target'], axis=1)\nX['major_discipline'][X['major_discipline']=='Business Degree'] = 'Business'","940b35dc":"X","fb895a52":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['experience'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       explode=[0]*14+[0.12]+[0.06]+[0]*7, shadow=True, rotatelabels=0.75)\nplt.title('Work experience (in years)', fontsize=20)\nplt.show()","5048b312":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['company_size'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       shadow=True, explode=[0.075]+[0]*8, rotatelabels=0.75)\nplt.title('Company size (number of people)', fontsize=20)\nplt.show()","d4724e49":"fig, ax = plt.subplots(figsize=(10, 10))\ncount = Counter(X['last_new_job'])\nplt.pie(count.values(), labels=count.keys(), labeldistance=0.75, autopct=lambda p:f'{p:.2f}%',\n       explode=[0.05]+[0]*6, shadow=True)\nplt.title('Number of years between last and current job', fontsize=20)\nplt.show()","5c86beed":"names = [['gender', 'relevent_experience'], ['enrolled_university', 'education_level'], \n ['major_discipline', 'company_type']]\ncolours = ['red', 'green', 'blue', 'purple', 'orange', 'skyblue']\n\nfig, axes = plt.subplots(3, 2, figsize=(15, 17))\nfig.tight_layout(h_pad=3)\n\nfor i in names:\n    for name, ax in zip(i, axes[names.index(i)]):\n        col = X[name].fillna('NaN')\n        count = Counter(col)\n        count = pd.Series(count).sort_values(ascending=False)\n        bars = ax.bar(count.keys(), count, color=colours[list(axes.flatten()).index(ax)])\n        for bar in bars:\n            label = count[list(bars).index(bar)]\n            ax.text(bar.get_x() + bar.get_width()\/2., bar.get_height(), label, ha='center', \n                     va='bottom', fontsize=15)  \n        ax.set_title(name, fontsize=15)\n        ax.xaxis.label.set_size(50)\n        plt.xticks(rotation='vertical')","bd96a841":"X['experience'] = X['experience'].fillna('-1')\nX['company_size'] = X['company_size'].fillna('-1')\nX['last_new_job'] = X['last_new_job'].fillna('-1')","88231f94":"cat_cols = ['gender', 'relevent_experience', 'enrolled_university', 'education_level', \n            'major_discipline', 'experience', 'company_size', 'company_type', 'last_new_job']\nordinal = ['experience', 'company_size', 'last_new_job']\n\nfor col in cat_cols:\n    if col in ordinal:\n        le = LabelEncoder()\n        X[col] = le.fit_transform(X[col])\n    else:\n        dummies = pd.get_dummies(X[col])\n        for d_col in dummies:\n            X[col+' '+d_col] = dummies[d_col]\n        X = X.drop(col, axis=1)\n                           \nX['city'] = [int(city[5:]) for city in X['city']]","2eb41796":"for col in X.columns[:6]:\n    normal = X[col]\n    transforms = [[normal, 'Normal', 'lightblue'], \n                  [(normal+1).transform(np.log), 'Log Transform', 'lightgreen'], \n                  [boxcox(normal+1)[0], 'Box Cox', 'pink']]\n    fig, axes = plt.subplots(2, len(transforms), figsize=(20, 12))\n    \n    for ax in axes[0]:\n        transform = transforms[list(axes[0]).index(ax)]\n        pd.DataFrame(transform[0]).hist(ax=ax, color=transform[2])\n        ax.set_title('')\n        ax.set_xlabel(transform[1], fontsize=15)\n        \n        deciles = pd.Series(transform[0]).quantile([.1, .2, .3, .4, .5, .6, .7, .8, .9])\n        for pos in np.array(deciles).reshape(1, -1)[0]:\n            handle = ax.axvline(pos, color='darkblue', linewidth=1)\n        ax.legend([handle], ['deciles'])\n        \n    for ax in axes[1]:\n        transform = transforms[list(axes[1]).index(ax)]\n        ax.boxplot(transform[0])\n        ax.set_title('')\n        ax.set_xlabel(transform[1], fontsize=15)\n        \n    axes[0][int(np.floor(len(transforms)\/2))].set_title(col, pad=25, fontsize=30)\n    plt.show()","45932778":"X['city'] = X['city']\nX['city_development_index'] = boxcox(X['city_development_index']+1)[0]\nX['experience'] = boxcox(X['experience']+1)[0]\nX['company_size'] = (X['company_size']+1).transform(np.log)\nX['last_new_job'] = (X['last_new_job']+1).transform(np.log)\nX['training_hours'] = boxcox(X['training_hours']+1)[0]","353a01f4":"fig, ax = plt.subplots(1, 1, figsize=(13, 8))\nfor name in ['city', 'training_hours']:\n    col = X[name]\n    X[name] = np.digitize(col, np.arange(col.min(), col.max(), (col.max()-col.min())\/20))\n    X[name].hist(alpha=0.65, legend=True)\n    plt.title('Binned Distribution', fontsize=20)\n        \nplt.show()\nX = X.drop(['city', 'training_hours'], axis=1)","d8671763":"count = Counter(y)\nplt.bar(['1', '0'], count.values(), color='blue')\nplt.title('Distribution of y')\nplt.xlabel('Class')\nplt.ylabel('Number of samples')\nplt.show()","4d386888":"smote = SMOTE()\nX, y = smote.fit_resample(X, y)","c5a5c015":"count = Counter(y)\nplt.bar(['1', '0'], count.values(), color='blue')\nplt.title('Distribution of y')\nplt.xlabel('Class')\nplt.ylabel('Number of samples')\nplt.show()","b97ec1d5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","7c69b6e2":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","19d285fc":"sns.heatmap(X.corr())\nplt.title('Correlation of features', fontsize=15, pad=10)\nplt.show()","3a495796":"pca = PCA(n_components=29).fit(X_train)\nevr = pca.explained_variance_ratio_\nplt.bar(range(len(evr)), evr)\nplt.title('Explained variance ratio for features')\nplt.xlabel('Features')\nplt.ylabel('Explained variance ratio')\nplt.show()","60e98fc9":"pca = PCA(n_components=26)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","1cf092fb":"classifiers = [['XGBoost', XGBClassifier()], ['Random Forest', RandomForestClassifier()], \n               ['Linear SVC', LinearSVC(dual=False)], ['SGD', SGDClassifier()]]\nscores = []\ncross_vals = []\nroc_aucs = []\n\nfor classifier in classifiers:\n    name = classifier[0]\n    model = classifier[1]\n    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    score = model.score(X_test, y_test)\n    cross_val = cross_val_score(model, X_test, y_test).mean()\n    roc_auc = roc_auc_score(y_test, y_pred)\n    \n    scores.append(score)\n    cross_vals.append(cross_val)\n    roc_aucs.append(roc_auc)\n    \n    print(name)\n    print('model score:    ', score)\n    print('cross val score:', cross_val)\n    print('ROC AUC score:  ', roc_auc)\n    if classifier != classifiers[-1]:\n        print('')","7bece862":"fig, axes = plt.subplots(1, 3, figsize=(18, 7))\nmetrics = [scores, cross_vals, roc_aucs]\nmetric_names = ['model score', 'cross validation score', 'ROC AUC score']\nnames = ['XGBoost', 'Random Forest', 'Linear SVC', 'SGD']\ncolours = ['red', 'lightgreen', 'blue']\n\nfor metric in metrics:\n    index = metrics.index(metric)\n    ax = axes.flatten()[index]\n    bars = ax.bar(names, metric, color=colours[index])\n    for bar in bars:\n        label = str(metric[list(bars).index(bar)])[:4]\n        ax.text(bar.get_x() + bar.get_width()\/2, bar.get_height(), label, ha='center', \n                va='bottom', fontsize=16)\n    ax.set_title(metric_names[index], fontsize=20)\nplt.show()","a6ac800a":"As for the **education level**, 60% are graduates, 23% have a master's and 11% only finished high school.","080b978e":"<font color='purple' size=6 style=\"font-family:garamond;\">Dimensionality reduction<\/font>","b6bb7945":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Leaving job prediction & EDA<\/font>","893c8e91":"<font color='darkblue' size=5 style=\"font-family:georgia\">Thank you for reading my notebook.<\/font>","3658977a":"Then we use a Standard Scaler to scale our X train and test data so that it can be more useful to our classifier.","4413e799":"For the **gender** column, we can see that there are significantly more men than any other gender. All of the non-male samples combined make up of less than one half of the amount of males in the dataset.","4000697e":"The amount of people who have had **relevent experience** is more than twice the amount who have not.","cff49db7":"Welcome to this notebook, where today we will be performing EDA and predicting whether a person will leave their job.","209ca209":"<font color='purple' size=6 style=\"font-family:garamond;\">Bar graphs<\/font>","bc33f670":"<font color='purple' size=6 style=\"font-family:garamond;\">Splitting sets and sampling<\/font>","d41947f6":"The correlation of the columns in our data is shown below using a heatmap. There don't seem to be any strong connections between them that jump out to us, which is a good sign because if they were correlated then that would mean that the probability of us having a useless feature would be greater.","88e172a6":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Data Transformation<\/font>","bd741851":"The final feature which we will visualise using a pie chart is the 'last_new_job'. This tells us how many years have passed between the last and the current job. Over 40% of people have had one year between their previous and current job, then 17.17% have had a gap of more than four years and for 15.14% of the candidates, two years have passed.","c7f9ccc7":"<font color='purple' size=6 style=\"font-family:garamond;\">Resampling<\/font>","391c5b06":"Our conclusions are that the 'city' is best left alone, the 'city_development_index', 'experience' and 'training_hours' features should be transformed using box cox and the 'company_size' and 'last_new_job' work best with a log transform.","ea8aa670":"In the **major discipline**, a substantial amount of people are involved in STEM, as only 24% are involved in any other discipline and 76% are in STEM.","8ed1f2ce":"Another very useful piece of data cleaning that we will perform is dimensionality reduction, in which we remove useless features so that our model can have greater accuracy.","1db6ac95":"Subsequently, we now take a look at six different columns: '**gender**', '**relevent_experience**', '**enrolled_university**', '**education_level**', '**major_discipline**', '**company_type**'.","7425ad48":"<img src=\"https:\/\/miro.medium.com\/max\/1000\/1*WACiczYwdWTnJ94mnizS4Q.jpeg\" width=\"400px\">","0fb3b106":"Another useful technique to change the data is binning, which reduces the amount of unique classes in the columns to a specified amount by grouping certain ranges together.","9be1857a":"Afterwards, we plot the distribution of the 'city', 'city_development_index', 'experience', 'company_size', 'last_new_job','training_hours' using histograms and box plots. Also, we check how they fare when transformed using a log transform and a box cox.","a2048dd8":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Creating a classifier<\/font>","20a7fa9f":"Now we move onto transforming our data in certain ways so that it can be inputted into a model for predictions.","da305657":"Now the number of samples per class in y are equal, so the predictor can have a higher accuracy.","d53e72ee":"The first step is to acquire our dataset and put it in an 'X' variable. We extract the 'target' feature out of X and assign a variable 'y' to it.","a1f7fec9":"<font color='purple' size=6 style=\"font-family:garamond;\">Categorical to numerical<\/font>","2681e37d":"As seen below, we visualise 'experience' using a pie chart, which tells us how many years the candidate has been working. More than 17% of the people have been working for more than 20 years, 7.46% have worked for 5 years and 7.32% have had 4 years.","eb6d2e5a":"The 'company_size' tells us the range of how many employees were in the companies. In this column, a lot of our samples are missing (almost two fifths!), as indicated by the 'nan'. Though, the largest group out of the recorded samples is 50-99, followed by 100-500 (13.42%) and 10,000+ (10.54%).","86f90204":"The final step is to create a machine learning classifier which will accurately predict our data.","657aef7b":"<img src=\"http:\/\/www.rgitaa.com\/wp-content\/uploads\/2018\/12\/2.png\" width=\"500\">","4f61da5e":"<font color='purple' size=6 style=\"font-family:garamond;\">Missing data<\/font>","ee02b2ca":"<font color='purple' size=6 style=\"font-family:garamond;\">Pie charts<\/font>","ddbe7d39":"The variables we choose to bin are 'city' and 'training_hours'. The graph below shows us not just their compared distribution, but also how many different classes they have - 20, as shown by the maximum value on the x axis.","457d46b7":"<font color='darkblue' size=7 style=\"font-family:garamond;\">Categorical feature visualisation<\/font>","4c5bd322":"<font color='darkblue' size=5 style=\"font-family:georgia;\">If you enjoyed this notebook and found it helpful, please upvote it and give feedback as it would help me make more of these.<\/font>","203f579d":"The first part of our EDA is displaying the different categorical features in our dataset. We take three columns: '**experience**', '**company_size**' and '**last_new_job**' and plot them out using pie charts.","db48b980":"<font color='purple' size=6 style=\"font-family:garamond;\">Log and Box Cox<\/font>","15e9a900":"An extremely common practice among people who wish to predict data is to split the X and y into train and test sets, which is what we do in the cell below. The test set has 20% of data, while the train has 80%.","8ac26ff1":"<font color='purple' size=6 style=\"font-family:garamond;\">Gathering data<\/font>","4578f6bd":"<font color='purple' size=6 style=\"font-family:garamond;\">Binning<\/font>","58452cce":"It seems that the ensemble algorithms: XGBoost and Random Forest perform the best on our dataset.","89cad9dd":"The vast majority of people (13817 candidates) have not **enrolled** to any university, followed by 3757 candidates at a full time course and 1198 at a part time course.","11096246":"Furthermore, we will transform our categorical columns to numerical ones. We use a LabelEncoder for the ordinal (ordered) features and a get_dummies\/One Hot Encoder for the nominal (unordered) features.","e768d4fb":"More than half of candidates (51%) were involved in a Pvt Ltd **company type**, while 32% of the samples are missing and 5% were in a funded startup.","dbe0d681":"<img src=\"http:\/\/adcengineers.com\/wp-content\/uploads\/2016\/06\/careers.jpg\" width=\"1000px\">","2273eca7":"In the next cell, we use a PCA to determine how much each feature contributes to our dataset. The first column has 12% of explained variance ratio, and we can also see that 25 other variables also have some contribution.","ecbaa4a1":"We start off by replacing the null values in 'experience', 'company_size' and 'last_new_job' with -1's. This is because these columns are ordinal (ordered) and need to have a replacement for any missing data.","950f775f":"We use a PCA to pull out the best 26 components and apply it to our X train and test.","3d158e23":"The distribution of y has a lot more samples in '0' than in '1'. Therefore, we will use the SMOTE class to resample our data.","3cb2fa32":"Here, we test four predictors: XGBoost, Random Forest, Linear SVC and SGD Classifier. We use the model, cross validation and ROC AUC scores in order to evaluate which model performs the best."}}