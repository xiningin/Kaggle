{"cell_type":{"ca999396":"code","60522dd5":"code","5ea5f291":"code","e4e47b8b":"code","308a366e":"code","159e11e1":"code","63a8b7a6":"code","0faebcf1":"code","a9c2a1da":"code","0c207b0b":"code","7aa09592":"code","fd24942c":"code","2979becf":"code","f95bdaee":"code","e21dc792":"code","031a456c":"code","d27ec670":"markdown"},"source":{"ca999396":"import os\nimport sys\nimport json\nimport datetime\nimport numpy as np\nimport skimage.draw\nimport random\n\n\nimport math\n\nimport skimage.io\nimport matplotlib\nimport matplotlib.pyplot as plt","60522dd5":"SAMPLES_DIR = '\/kaggle\/input\/maskrcnnusg\/dataset'\n\nDATA_DIR = '\/kaggle\/input\/maskrcnnusg'\n\n# Directory to save logs and trained model\nROOT_DIR = '\/kaggle\/working'\nTEST_DATA = '\/kaggle\/input\/testimage\/testdata\/'\nIMAGE_DIR = '\/kaggle\/input\/testimage\/testdata\/'","5ea5f291":"!git clone https:\/\/www.github.com\/matterport\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n#!python setup.py -q install","e4e47b8b":"# Import Mask RCNN\nprint(os.path.join(ROOT_DIR, '..\/Mask_RCNN'))\nsys.path.append(os.path.join(ROOT_DIR, '..\/Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","308a366e":" !wget --quiet https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n #!ls -lh mask_rcnn_coco.h5\n\n COCO_WEIGHTS_PATH = '\/kaggle\/working\/Mask_RCNN\/mask_rcnn_coco.h5'","159e11e1":"# print(os.listdir(\".\/\"))\n# Directory to save logs and model checkpoints, if not provided\n# through the command line argument --logs\n# DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\") #\/kaggle\/working\/logs\nDEFAULT_LOGS_DIR = '\/kaggle\/working'\n# print(os.path.join(ROOT_DIR, \"logs\"))","63a8b7a6":"class BalloonConfig(Config):\n    \"\"\"Configuration for training on the toy  dataset.\n    Derives from the base Config class and overrides some values.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"class\"\n\n    # We use a GPU with 12GB memory, which can fit two images.\n    # Adjust down if you use a smaller GPU.\n    IMAGES_PER_GPU = 2\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 2  # Background + benign + malignant\n\n    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 100\n\n    # Skip detections with < 90% confidence\n    DETECTION_MIN_CONFIDENCE = 0.9","0faebcf1":"############################################################\n#  Dataset\n############################################################\n\nclass BalloonDataset(utils.Dataset):\n\n    def load_balloon(self, dataset_dir, subset):\n        \"\"\"Load a subset of the Balloon dataset.\n        dataset_dir: Root directory of the dataset.\n        subset: Subset to load: train or val\n        \"\"\"\n        # Add classes. We have only one class to add.\n        self.add_class(\"class\", 1, \"benign\")\n        self.add_class(\"class\", 2, \"malignant\")\n        \n\n        # Train or validation dataset?\n        assert subset in [\"train\", \"test\"]\n        dataset_dir = os.path.join(dataset_dir, subset)\n\n        # Load annotations\n        # VGG Image Annotator saves each image in the form:\n        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n        #   'regions': {\n        #       '0': {\n        #           'region_attributes': {},\n        #           'shape_attributes': {\n        #               'all_points_x': [...],\n        #               'all_points_y': [...],\n        #               'name': 'polygon'}},\n        #       ... more regions ...\n        #   },\n        #   'size': 100202\n        # }\n        # We mostly care about the x and y coordinates of each region\n        annotations = json.load(open(os.path.join(dataset_dir, \"via_region_data.json\")))\n        annotations = list(annotations.values())  # don't need the dict keys\n\n        # The VIA tool saves images in the JSON even if they don't have any\n        # annotations. Skip unannotated images.\n        annotations = [a for a in annotations if a['regions']]\n\n        # Add images\n        for a in annotations:\n            # Get the x, y coordinaets of points of the polygons that make up\n            # the outline of each object instance. There are stores in the\n            # shape_attributes (see json format above)\n            polygons = [r['shape_attributes'] for r in a['regions'].values()]\n\n            # load_mask() needs the image size to convert polygons to masks.\n            # Unfortunately, VIA doesn't include it in JSON, so we must read\n            # the image. This is only managable since the dataset is tiny.\n            image_path = os.path.join(dataset_dir, a['filename'])\n            image = skimage.io.imread(image_path)\n            height, width = image.shape[:2]\n\n            self.add_image(\n                \"class\",\n                image_id=a['filename'],  # use file name as a unique image id\n                path=image_path,\n                width=width, height=height,\n                polygons=polygons)\n\n    def load_mask(self, image_id):\n        \"\"\"Generate instance masks for an image.\n       Returns:\n        masks: A bool array of shape [height, width, instance count] with\n            one mask per instance.\n        class_ids: a 1D array of class IDs of the instance masks.\n        \"\"\"\n        # If not a balloon dataset image, delegate to parent class.\n        image_info = self.image_info[image_id]\n        if image_info[\"source\"] != \"class\":\n            return super(self.__class__, self).load_mask(image_id)\n\n        # Convert polygons to a bitmap mask of shape\n        # [height, width, instance_count]\n        info = self.image_info[image_id]\n        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"polygons\"])],\n                        dtype=np.uint8)\n        for i, p in enumerate(info[\"polygons\"]):\n            # Get indexes of pixels inside the polygon and set them to 1\n            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])\n            mask[rr, cc, i] = 1\n\n        # Return mask, and array of class IDs of each instance. Since we have\n        # one class ID only, we return an array of 1s\n        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n\n    def image_reference(self, image_id):\n        \"\"\"Return the path of the image.\"\"\"\n        info = self.image_info[image_id]\n        if info[\"source\"] == \"class\":\n            return info[\"path\"]\n        else:\n            super(self.__class__, self).image_reference(image_id)                  ","a9c2a1da":"def train(model):\n    \"\"\"Train the model.\"\"\"\n    # Training dataset.\n    dataset_train = BalloonDataset()\n    dataset_train.load_balloon(SAMPLES_DIR, \"train\")\n    dataset_train.prepare()\n\n    # Validation dataset\n    dataset_val = BalloonDataset()\n    dataset_val.load_balloon(SAMPLES_DIR, \"test\")\n    dataset_val.prepare()\n\n    # *** This training schedule is an example. Update to your needs ***\n    # Since we're using a very small dataset, and starting from\n    # COCO trained weights, we don't need to train too long. Also,\n    # no need to train all layers, just the heads should do it.\n    print(\"Training network heads\")\n    model.train(dataset_train, dataset_val,\n                learning_rate=config.LEARNING_RATE,\n                epochs=15,\n                layers='heads')  \n    \n#     model.save('64x3-CNN.model')\n\n# checkpoint = ModelCheckpoint(\"models\/{}.model\".format(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) #","0c207b0b":"if __name__ == '__main__':\n    print('Train')\n    \n    config = BalloonConfig()\n    \n    model = modellib.MaskRCNN(mode=\"training\", config=config,\n                          model_dir=DEFAULT_LOGS_DIR)\n    \n    weights_path = COCO_WEIGHTS_PATH\n#     COCO_WEIGHTS_PATH = '..\/..\/..\/mask_rcnn_coco.h5'\n    \n    # Find last trained weights\n    # weights_path = model.find_last()[1]\n    \n    \n    model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n    \n    train(model)","7aa09592":"# remove files to allow committing (hit files limit otherwise)\n#!rm -rf \/kaggle\/working\/Mask_RCNN","fd24942c":"# from IPython.display import HTML\n\n# def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n#     html = '<a href={filename}>{title}<\/a>'\n#     html = html.format(title=title,filename=filename)\n#     return HTML(html)\n\n# # create a link to download the dataframe which was saved with .to_csv method\n# # create_download_link(filename='\/kaggle\/working\/logs\/balloon20190325T1747\/mask_rcnn_balloon_0001.h5')\n# create_download_link(filename='balloon20190326T1301\/mask_rcnn_balloon_0001.h5')","2979becf":"import matplotlib.pyplot as plt\nargs_weights = \"\/kaggle\/input\/testimage\/testdata\/mask_rcnn_class_0015.h5\"\nargs_logs = DEFAULT_LOGS_DIR\nargs_image = \"\/kaggle\/input\/image55\/image\/\"\nargs_video = None\n\n# Configurations\nclass InferenceConfig(BalloonConfig):\n    # Set batch size to 1 since we'll be running inference on\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\nconfig = InferenceConfig()\nconfig.display()\n\n# Create model\nmodel = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=args_logs)\n\n# Load weights\nprint(\"Loading weights \", args_weights)\nmodel.load_weights(args_weights, by_name=True)\n","f95bdaee":"def color_splash(image, mask):\n    \"\"\"Apply color splash effect.\n    image: RGB image [height, width, 3]\n    mask: instance segmentation mask [height, width, instance count]\n\n    Returns result image.\n    \"\"\"\n    # Make a grayscale copy of the image. The grayscale copy still\n    # has 3 RGB channels, though.\n    gray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n    # Copy color pixels from the original color image where mask is set\n    if mask.shape[0] > 0:\n        # We're treating all instances as one, so collapse the mask into one layer\n        mask = (np.sum(mask, -1, keepdims=True) >= 1)\n        splash = np.where(mask, image, gray).astype(np.uint8)\n    else:\n        splash = gray.astype(np.uint8)\n    return splash\n\n\ndef detect_and_color_splash(model, image_path=None, video_path=None):\n    assert image_path or video_path\n\n\n    # Run model detection and generate the color splash effect\n    print(\"Running on {}\".format(args_image))\n    # Read image\n    image = skimage.io.imread(args_image + os.listdir(args_image)[0])\n    # Detect objects\n    r = model.detect([image], verbose=1)[0]\n    # Color splash\n    splash = color_splash(image, r['masks'])\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'],\n                             r['scores'])\n    file_name = 'splash.png'\n    # Save output\n    #file_name = \"splash_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n    #skimage.io.imsave(file_name, splash)\n    return image, splash","e21dc792":"#inference of your model\n\n# Configurations\nclass InferenceConfig(BalloonConfig):\n    # Set batch size to 1 since we'll be running inference on\n    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\nconfig = InferenceConfig()\nconfig.display()\n\n# Create model\nmodel = modellib.MaskRCNN(mode=\"inference\", config=config,\n                              model_dir=DEFAULT_LOGS_DIR)\n\n# Load your weights\nargs_your_weights = \"\/kaggle\/input\/testimage\/testdata\/mask_rcnn_class_0015.h5\" #\"path to your weight\" #TODO\nprint(\"Loading your weights \", args_your_weights)\nmodel.load_weights(args_weights, by_name=True)\n\n#inference\nclass_names = ['BG', 'benign', 'malignant']\nfile_names = next(os.walk(IMAGE_DIR))[2]\nimage = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n\nresults = model.detect([image], verbose=1)\n\n# Visualize results\nr = results[0]\nvisualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                            class_names, r['scores'])\nvisualize.display_instances(image)","031a456c":"image_src, image_splash = detect_and_color_splash(model, image_path=args_image, video_path=args_video)\n#show\nplt.figure(num='astronaut2',figsize=(16,16))\nplt.subplot(6,6,1)\nplt.imshow(image_src[:,:,:])\nplt.subplot(6,6,2)\nplt.imshow(image_splash[:,:,:])\n","d27ec670":"### Install Matterport's Mask-RCNN model from github.\nSee the [Matterport's implementation of Mask-RCNN](https:\/\/github.com\/matterport\/Mask_RCNN)."}}