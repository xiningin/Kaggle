{"cell_type":{"15779e7f":"code","521a2af9":"code","511b4b93":"code","45ab2c96":"code","0e9cccaf":"code","98c778a5":"code","e8f9d7a3":"code","63fa19f3":"code","6b6ca5bf":"code","0eaa3714":"code","1fa7675d":"code","f0e87237":"code","649bf035":"code","a643e56d":"code","a326f3b2":"code","1495cdb1":"code","2299f89a":"code","50adf6bf":"code","e57374c4":"code","84f22043":"code","50be9a33":"code","43d6e8c9":"code","0c5643e4":"code","fe15d8b8":"code","691de786":"code","9017bed1":"code","412681d0":"code","287fb171":"markdown","bb85d4bb":"markdown","9c4c02ae":"markdown","eb91332a":"markdown","5521f3c0":"markdown","f44ea0d2":"markdown","402f027c":"markdown","670d15d0":"markdown","4e0867d2":"markdown","0e8fbbc6":"markdown","c7b71793":"markdown","b9a0fc9c":"markdown","26fd1b65":"markdown","c4f4c615":"markdown","17825332":"markdown","66f6550f":"markdown","3bb53d44":"markdown","8614de3b":"markdown","ab6f1ef6":"markdown","c6133bb7":"markdown","1f2cefeb":"markdown","c3593a08":"markdown","3a9f6d0b":"markdown","f556c55e":"markdown"},"source":{"15779e7f":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n","521a2af9":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression","511b4b93":"df=pd.read_csv(\"..\/input\/heart.csv\")","45ab2c96":"df.head()","0e9cccaf":"df.describe()","98c778a5":"sns.countplot(x=\"target\", data=df)","e8f9d7a3":"y = df.target.values","63fa19f3":"x_data = df.drop([\"target\"],axis=1)\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","6b6ca5bf":"x_data","0eaa3714":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state=56)","1fa7675d":"lr=LogisticRegression()","f0e87237":"#Training the model\nlr.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))\nlr_score=lr.score(x_test,y_test)","649bf035":"#Testing the model\ny_prediction = lr.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","a643e56d":"#Visualizing the results\nsns.heatmap(cm, annot=True)\nplt.xlabel(\"Predictions Y\")\nplt.ylabel(\"Actual Y\")\nplt.show()","a326f3b2":"from sklearn.neighbors import KNeighborsClassifier","1495cdb1":"knn=KNeighborsClassifier()\n#Training the model\nknn.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(knn.score(x_test,y_test)))\n\nknn_prediction_score=knn.score(x_test,y_test)","2299f89a":"#Testing the model\ny_prediction = knn.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","50adf6bf":"#Visualizing the result\nsns.heatmap(cm, annot=True)\nplt.xlabel(\"Predictions Y\")\nplt.ylabel(\"Actual Y\")\nplt.show()","e57374c4":"from sklearn.svm import SVC","84f22043":"svm = SVC()\n#Training the model\nsvm.fit(x_train,y_train)\n# prediction and accuracy \nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\nsvm_score = svm.score(x_test,y_test)","50be9a33":"#Testing the model\ny_prediction = knn.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","43d6e8c9":"#Visualizing the results\nsns.heatmap(cm, annot=True)\nplt.xlabel(\"Predictions Y\")\nplt.ylabel(\"Actual Y\")\nplt.show()","0c5643e4":"from sklearn.tree import DecisionTreeClassifier\n\n","fe15d8b8":"dt = DecisionTreeClassifier()\n#Training the Model\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))\n\ndt_score=dt.score(x_test,y_test)","691de786":"#Testing the model\ny_prediction = knn.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","9017bed1":"#Visulaizing the results\nsns.heatmap(cm, annot=True)\nplt.xlabel(\"Predictions Y\")\nplt.ylabel(\"Actual Y\")\nplt.show()","412681d0":"class_name = (\"Logistic Regression\",\"KNN\",\"SVM\",\"Decision Tree\")\nclass_score = (lr_score,knn_prediction_score,svm_score,dt_score)\ny_pos= np.arange(len(class_score))\ncolors = (\"red\",\"gray\",\"purple\",\"green\")\nplt.figure(figsize=(20,12))\nplt.bar(y_pos,class_score,color=colors)\nplt.xticks(y_pos,class_name,fontsize=20)\nplt.yticks(np.arange(0.00, 1.05, step=0.05))\nplt.ylabel('Accuracy')\nplt.title(\" Confusion Matrix Comparision of the Classes\",fontsize=15)\nplt.savefig('graph.png')\nplt.show()","287fb171":"**Decision Tree**","bb85d4bb":"**Selection of Algorithm**","9c4c02ae":"> Data preprocessing is a data mining technique that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and\/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues. Data preprocessing prepares raw data for further processing.\n> \n> Data preprocessing is used database-driven applications such as customer relationship management and rule-based applications (like neural networks).","eb91332a":"**Conclusion**\n> 1. *Svm wins in this case by a very small margin,but the commpetition is very close.*\n> 2. *Svm shows the highesst accuracy of 80%.*","5521f3c0":"**Classification using KNN**","f44ea0d2":"> Representation Used for Logistic Regression\n> \n> Logistic regression uses an equation as the representation, very much like linear regression.\n> \n> Input values (x) are combined linearly using weights or coefficient values (referred to as the Greek capital letter Beta) to predict an output value (y). A key difference from linear regression is that the output value being modeled is a binary values (0 or 1) rather than a numeric value.\n> \n> Below is an example logistic regression equation:\n> \n> y = e^(b0 + b1*x) \/ (1 + e^(b0 + b1*x))\n> \n> Where y is the predicted output, b0 is the bias or intercept term and b1 is the coefficient for the single input value (x). Each column in your input data has an associated b coefficient (a constant real value) that must be learned from your training data.\n> \n> The actual representation of the model that you would store in memory or in a file are the coefficients in the equation (the beta value or b\u2019s).","402f027c":"> The  choice  of  which  specific  learning  algorithm  we  should  use  is  a  critical  step.  Once  preliminary  testing  is  judged  to  be  satisfactory,  the  classifier  (mapping  from  unlabeled  instances  to  classes)  is  available  for  routine  use.  The  classifier\u2019s  evaluation  is  most  often  based  on  prediction accuracy (the percentage of correct prediction divided by the total number of predictions). There are at least  three  techniques  which  are  used  to  calculate  a  classifier\u2019s   accuracy.   One   technique   is   to   split   the   training set by using two-thirds for training and the other third  for  estimating  performance.  In  another  technique,  known as cross-validation, the training set is divided into mutually  exclusive  and  equal-sized  subsets  and  for  each  subset  the  classifier  is  trained  on  the  union  of  all  the  other subsets. The average of the error rate of each subset is therefore an estimate of the error rate of the classifier. Leave-one-out   validation   is   a   special   case   of   cross   validation.  All  test  subsets  consist  of  a  single  instance.  This  type  of  validation  is,  of  course,  more  expensive  computationally,   but   useful   when   the   most   accurate   estimate of a classifier\u2019s error rate is required.","670d15d0":"> ***Classification is a technique for determining class the dependent belongs to based on the one or more independent variables. ***\n> There  are  several  applications  for  Machine  Learning  (ML),  the  most  significant  of  which  is  data  mining.  People   are   often   prone   to   making   mistakes   during   analyses    or,    possibly,    when    trying    to    establish    relationships  between  multiple  features.  This  makes  it  difficult  for  them  to  find  solutions  to  certain  problems.  Machine  learning  can  often  be  successfully  applied  to  these problems, improving the efficiency of systems and the designs of machines\n\n> Classification predicts the category the data belongs to.\n\n> eg: Spam Detection, Churn Prediction, Sentiment Analysis,Dog Breed Detection.","4e0867d2":"**Classification using Logisitic Regression **","0e8fbbc6":"*In this Paper i am using various machine learning techniques for analyzing the dataset and comparing different models to figure out which model is performing the best in this case.*","c7b71793":"> K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970\u2019s as a non-parametric technique.\n\n> K-NN is a non-parametric, lazy learning algorithm. It classifies new cases based on a similarity measure (e.g. distance functions).\n\n> A case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a distance function. If K = 1, then the case is simply assigned to the class of its nearest neighbor\n> KNN works well with a small number of input variables (p), but struggles when the number of inputs is very large.","b9a0fc9c":"> Content\n> \n> \n> Attribute Information:\n>  1. age\n>  2. sex\n>  3. chest pain type (4 values)\n>  4. resting blood pressure\n>  5. serum cholestoral in mg\/dl\n>  6. fasting blood sugar > 120 mg\/dl\n>  7. resting electrocardiographic results (values 0,1,2)\n>  8. maximum heart rate achieved\n>  9. exercise induced angina\n>  10. oldpeak = ST depression induced by exercise relative to rest\n>  11. the slope of the peak exercise ST segment\n>  12. number of major vessels (0-3) colored by flourosopy\n>  13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n> \n> The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been \"processed\", that one containing the Cleveland database. All four unprocessed files also exist in this directory.\n> \n> To see Test Costs (donated by Peter Turney), please see the folder \"Costs\" ","26fd1b65":"**SVM : Support Vector Machine**","c4f4c615":"**About the Dataset**","17825332":"> Support Vector is used for both regression and Classification. It is based on the concept of decision planes that define decision boundaries. A decision plane(hyperplane) is one that separates between a set of objects having different class memberships.\n\n> It performs classification by finding the hyperplane that maximizes the margin between the two classes with the help of support vectors.\n\n> \u201cSupport Vector Machine\u201d (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well (look at the below snapshot).","66f6550f":"> Logistic regression is kind of like linear regression but is used when the dependent variable is not a number, but something else (like a Yes\/No response). Its called Regression but performs classification as based on the regression it classifies the dependent variable into either of the classes.\n\nLogistic Function\n\n> Logistic regression is named for the function used at the core of the method, the logistic function.\n> \n> The logistic function, also called the sigmoid function was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. It\u2019s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\n> 1 \/ (1 + e^-value)","3bb53d44":"**Comparision**","8614de3b":"**                Classification**","ab6f1ef6":"> Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. It follows Iterative Dichotomiser 3(ID3) algorithm structure for determining the split.\n> Decision Tree uses Entropy and Information Gain to construct a decision tree.\n\n> **Entropy**\n\n> Entropy is the degree or amount of uncertainty in the randomness of elements or in other words it is a measure of impurity.\n> Entropy\n> \n> Intuitively, it tells us about the predictability of a certain event. Entropy calculates the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has an entropy of one.\n> \n> **Information Gain**\n\n> It measures the relative change in entropy with respect to the independent attribute. It tries to estimate the information contained by each attribute. Constructing a decision tree is all about finding the attribute that returns the highest information gain (i.e., the most homogeneous branches).","c6133bb7":"**Preprocessing Dataset**","1f2cefeb":"[](http:\/\/https:\/\/qph.fs.quoracdn.net\/main-qimg-05edc1873d0103e36064862a45566dba)","c3593a08":"> On Comparision of performance of each of these algorithm we can determine the algorithm which gives maximum accuracy and ca be used for production.","3a9f6d0b":"> Context\n> \n> This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The \"goal\" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.","f556c55e":"> > **Practically applying Logistic Regression on our Dataset **"}}