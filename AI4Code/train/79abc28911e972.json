{"cell_type":{"d42cd27e":"code","a4515c43":"code","41669c6a":"code","ef3fc595":"code","6e6a608f":"code","a02ff093":"code","f8116f7e":"code","23a2e012":"code","46182d9a":"code","9ba1a8f7":"code","ea60dc91":"code","15e45282":"code","f2840925":"code","69f93236":"code","4c17ce1f":"code","3448bbf0":"code","6c876370":"code","7aba1f63":"code","7a589822":"code","9c76a8d2":"code","9dcd1f1b":"code","d1691e8b":"code","a4dc4005":"code","01079ec8":"markdown","30d6338d":"markdown","8519a3e9":"markdown"},"source":{"d42cd27e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4515c43":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\n\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nsns.set(rc={'figure.figsize':(8,5)})","41669c6a":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","ef3fc595":"train.head()","6e6a608f":"#Time for some Exploratory data analysis.","a02ff093":"sns.countplot(data = train, x=train['Response'])","f8116f7e":"fig, a=plt.subplots(2,2, figsize=(12, 8))\nsns.distplot(train['Age'], kde=False, ax=a[0, 0])\nsns.countplot(data=train, x=train['Gender'], ax=a[0, 1])\nsns.countplot(data=train, x=train['Gender'], hue=train['Response'], ax=a[1, 0])\nsns.countplot(data=train, x=train['Gender'], hue=train['Previously_Insured'], ax=a[1, 1])","23a2e012":"fig, a=plt.subplots(2,2, figsize=(12, 8))\nsns.countplot(data=train, x=train['Vehicle_Damage'], hue=train['Response'], ax=a[0, 0])\nsns.countplot(data=train, x=train['Vehicle_Damage'], hue=train['Gender'], ax=a[0, 1])\nsns.countplot(data=train, x=train['Vehicle_Age'], hue=train['Response'], ax=a[1, 0])\nsns.countplot(data=train, x=train['Response'], ax=a[1, 1])","46182d9a":"sns.set(rc={'figure.figsize':(12,10)})\nsns.heatmap(train.corr(), annot=True)","9ba1a8f7":"X = train.drop(['Response', 'id', 'Driving_License'], axis = 1)\ny = train['Response']","ea60dc91":"cat = []\nnum = []\n\n\ndef feature(df):\n    \n    \n    for i in df.columns:\n        if df[i].dtype == 'object':\n            cat.append(i)\n        else:\n            num.append(i)\n","15e45282":"mms = MinMaxScaler()\ndef scale(num):\n    \n    num_new = pd.DataFrame(mms.fit_transform(num))\n    num_new.columns = num.columns\n    return num_new","f2840925":"def ohe(cat):\n    cat_new = pd.DataFrame(pd.get_dummies(cat, drop_first = True))\n    return cat_new\n","69f93236":"feature(X)","4c17ce1f":"X_new = pd.concat([scale(X[num]), ohe(X[cat])], axis=1)\nX_new","3448bbf0":"X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.20, random_state = 100)\n\nprint(y_train.value_counts())\nprint(y_test.value_counts())","6c876370":"smote = SMOTE()\n\nX_sm, y_sm = smote.fit_sample(X_train, y_train)\n\nprint(X_sm.shape)\nprint(y_sm.value_counts())","7aba1f63":"def mod(any_model):\n    \n    model = any_model\n    \n    model.fit(X_sm, y_sm)\n    \n    y_pred = model.predict(X_test)\n    \n    print(roc_auc_score(y_test, y_pred)) \n    print(classification_report(y_pred, y_test))\n    print(confusion_matrix(y_pred, y_test))","7a589822":"import re\n\nregex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n\nX_sm.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_sm.columns.values]\nX_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n\nmodel = XGBClassifier(objective=\"binary:logistic\", random_state=42)\n\nmodel.fit(X_sm, y_sm)\n\ny_pred = model.predict(X_test)\n\nprint(roc_auc_score(y_test, y_pred))","9c76a8d2":"mod(LogisticRegression())","9dcd1f1b":"mod(RandomForestClassifier(n_estimators=200))","d1691e8b":"mod(KNeighborsClassifier(n_neighbors=20, weights='uniform', n_jobs=-1))","a4dc4005":"from sklearn.naive_bayes import GaussianNB\n\nmod(GaussianNB())","01079ec8":"SMOTE will help in increaing the number of minority record. Although, SMOTE Oversampling usies nearest neighbour concept, there is a chance of data leakage which might affect the test data. So it is better to oversample only the train data.","30d6338d":"The data is imbalanced. We'll need to fix this before i make the model. SMOTE oversampling can be done.\nAlthough even undersampling with hyperparameter tuning can be done and give a appritiable accuracy, auc-roc score.\n","8519a3e9":"People who did not have insurance previouly were indeed intrested in buying now."}}