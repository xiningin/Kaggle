{"cell_type":{"9135d0a6":"code","19282152":"code","4078e098":"code","e081f3ae":"code","02441bd5":"code","cdeca964":"code","ebfe40eb":"code","07e29ba2":"code","e600370b":"code","1a3ed9d9":"code","3ad979a9":"code","a62d29f7":"code","d6cda36b":"code","527b46d7":"code","9042ceac":"code","40671024":"code","c9dc7f40":"code","5fc38227":"code","7e10c7e7":"code","1b9b7b87":"code","b625e378":"code","733095a7":"code","7955406f":"code","b1d18187":"code","73e9215a":"markdown","47f51547":"markdown","2791d028":"markdown"},"source":{"9135d0a6":"#Follow Kaggle's way to load datasets\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","19282152":"#change the dataset into Pandas dataframe\nimport pandas as pd\ndataset = pd.read_csv(\"\/kaggle\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data.csv\")\n#anyway check the raw data\ndataset.tail()","4078e098":"#check the info of the dataset\ndataset.info()","e081f3ae":"#As you know, there are 26 alphabets from A to Z. \n#The first colummn shows labels comprised of \"0 to 25\", in short, the correct answers.\n#For example, \"0\" is \"A\". \"25\" is \"Z\".\n#Process loading dataset and change it into float 32 data, and name the first colummn \"label\".\ndataset.astype('float32')\ndataset.rename(columns={'0':'label'}, inplace=True)\n# Now X is \"Explanatory variable\", y is \"Taget\".\nX = dataset.drop('label',axis = 1)\ny = dataset['label']","02441bd5":"alphabets=\"abcdefghijklmnopqrstuvwxyz\"\nletter_name=[]\n[letter_name.append(i) for i in alphabets]\nname_tag = pd.DataFrame(letter_name)","cdeca964":"#import two libraries, matplot, seaborn.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(15,5))\nsns.distplot(y,kde=False)","ebfe40eb":"#import numpy and give a seed. \nimport numpy as np\nnp.random.seed(2)\n#Show 3 letter at random and convert them into gray scale letters. \nfor i in range(3):\n    plt.imshow(X.iloc[np.random.randint(0,372449)].values.reshape(28,28),cmap='Greys')\n    plt.show()","07e29ba2":"# load some libraries that I frequently use. \nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","e600370b":"#Now, reload the dataset and change it into \"numpy array\".\ndataset = np.loadtxt('\/kaggle\/input\/az-handwritten-alphabets-in-csv-format\/A_Z Handwritten Data.csv', delimiter=',')\n#Divide dataset into two parts, one is \"Explanatory variable\", the other is \"Taget\".\nX = dataset[:,0:784]\nY = dataset[:,0]\n#Split the \"X,Y\" data into the ratio of 7:3, 3 is the test size. \n(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.3, random_state=2)\n#Reshape the data and change it into float 32 as usual.\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n#The pixcel of the data is comprised from 0 to 255. 0 is white,255 is black.\n#Now normalize the data from 0 to 1 without some libraries, in a simple way.\nX_train = X_train \/ 255\nX_test = X_test \/ 255","1a3ed9d9":"# One-Hot-Encoding of the target.\nY_train = np_utils.to_categorical(Y_train)\nY_test = np_utils.to_categorical(Y_test)\n# Define the classification of 26 alphabets.\nnum_classes = Y_test.shape[1]","3ad979a9":"#Build an ordinary \"Deep Learning\" model with CNN and maxpooling by using Keras.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n#Choose an optimizer and compile the model.\nmodel.compile(optimizer = Adam(learning_rate = 0.01), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n#And print the summary of the model.\nprint(model.summary())","a62d29f7":"#Fit the train data of model 1.\nTrain1=model.fit(X_train, Y_train,batch_size=128, epochs=5)","d6cda36b":"#Check the test data of the model 1.\nTest1=model.fit(X_test, Y_test,batch_size=128, epochs=5)","527b46d7":"#Build the second model to look for best or better models.\n#Many people says \"Adam\" is way better than RMSprop. But I wanna try it just in case.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\nprint(model.summary())\n#Now the second model's optimizer is \"RMSprop\".\nmodel.compile(optimizer = \"rmsprop\", loss = 'categorical_crossentropy', metrics = ['accuracy'])","9042ceac":"#Check the second model named the model 2.\nTrain2=model.fit(X_train, Y_train,batch_size=128, epochs=5)","40671024":"#Check the test data of the second model too.\nTest2=model.fit(X_test, Y_test,batch_size=128, epochs=5)","c9dc7f40":"#Evaluate the two models by using two metrics, loss and accuracy.\nmetrics = ['loss', 'accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    #set subplots to show the result\n    plt.subplot(1, 2, i+1)\n    #Titles of subplots are \"loss\" and \"accuracy\"\n    plt.title(metric) \n    plt_train1 = Train1.history[metric] \n    plt_test1 = Test1.history[metric]\n    plt_train2 = Train2.history[metric]\n    plt_test2 = Test2.history[metric] \n    #plot them all\n    plt.plot(plt_train1, label='train1') \n    plt.plot(plt_train2, label='train2') \n    plt.plot(plt_test1, label='test1') \n    plt.plot(plt_test2, label='test2') \n    plt.legend() \nplt.show()","5fc38227":"#I wanna try Dilution technique also know as Dropout. So I build the third model.\nmodel = Sequential()\nmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n#Print the model\nprint(model.summary())\n#Compile the model\nmodel.compile(optimizer = \"rmsprop\", loss = 'categorical_crossentropy', metrics = ['accuracy'])","7e10c7e7":"#Now check the both data of the third model in one time.\nTrain3=model.fit(X_train, Y_train,batch_size=128, epochs=5)\nTest3=model.fit(X_test, Y_test,batch_size=128, epochs=5)","1b9b7b87":"#Evaluate the two models by using two metrics, loss and accuracy.\nmetrics = ['loss', 'accuracy']\n#show the evaluation result by using matoplot.\nplt.figure(figsize=(10, 5))\n#Use \"For Loop\".\nfor i in range(len(metrics)):\n    metric = metrics[i]\n    plt.subplot(1, 2, i+1)\n    plt.title(metric) \n    plt_train1 = Train1.history[metric] \n    plt_test1 = Test1.history[metric]\n    plt_train2 = Train2.history[metric]\n    plt_test2 = Test2.history[metric] \n    plt_train3 = Train3.history[metric]\n    plt_test3 = Test3.history[metric] \n    plt.plot(plt_train1, label='train1') \n    plt.plot(plt_train2, label='train2') \n    plt.plot(plt_train3, label='train3') \n    plt.plot(plt_test1, label='test1') \n    plt.plot(plt_test2, label='test2') \n    plt.plot(plt_test3, label='test3') \n    plt.legend() \nplt.show()","b625e378":"#Now I came into a conclusion that the second model shows the best performance.\n#So I chose the model 2 for this case. ","733095a7":"#Here is the prediction sample.\nplt.imshow(X_test[[18]].reshape(28,28),cmap='Greys')","7955406f":"#Let's predict.\nprediction=model.predict(X_test[[18]]) \nprediction","b1d18187":"#Preparation for this predction. \nalphabets=\"abcdefghijklmnopqrstuvwxyz\"\nlist1=[]\n[list1.append(i) for i in range(26)]\nlist2=[]\n[list2.append(i) for i in alphabets]\ndic = dict(zip(list1, list2))\n#Let's check the result.\nprint(\"The answer is\",dic[np.argmax(prediction)],\". :-)\")\n","73e9215a":"Now I know so many letter \"O\".","47f51547":"One of Kaggle buddies asked me a question how to predict. I should have mentioned in my notebook. Let's update this notebook.\n","2791d028":"Now I know the common understanding \"The Adam is way better than RMSprop.\" is wrong in this case. "}}