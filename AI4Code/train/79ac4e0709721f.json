{"cell_type":{"840a60e2":"code","adea15be":"code","ec5a235d":"code","837c9ecf":"code","839d63df":"code","ca38fc15":"code","9772486a":"code","9b40ce08":"code","b0e7bda8":"code","d55be51f":"code","e537c3dc":"code","4184d067":"code","25f7d8bd":"code","9850d102":"code","3538cf3b":"code","9a9df021":"code","10e66564":"code","26b5102e":"code","be71c5c4":"code","1a496e24":"code","9c22322e":"code","94ac4b91":"code","5dbaa2df":"code","28ee27d2":"code","4952fe5b":"code","3fa29fed":"code","03e88204":"code","4e174d61":"code","0bb5c770":"code","ecae8cb5":"code","95698795":"code","3608cb6b":"code","88a53ff1":"code","e6a1f620":"code","c0f492fc":"code","ef723374":"code","2c69e560":"code","69b71d62":"code","ac7147fd":"code","db6216db":"code","6dc3c593":"code","bbcbe6bd":"code","06d97b22":"code","9455c684":"code","b91c6556":"code","b9daa455":"code","bc6c11ff":"code","b869c86f":"code","9e01ba57":"code","3a34d595":"code","253d2f08":"code","4267c15b":"code","032cbfcc":"code","dc3d95f2":"code","c456d108":"code","6da815c2":"markdown","67b545c4":"markdown","4896d0b8":"markdown","81501108":"markdown","5d09b913":"markdown","76da6c6b":"markdown","cbd996bf":"markdown","aa3e1886":"markdown","d64d1794":"markdown","4bacb784":"markdown","7e143626":"markdown","e1400acf":"markdown","0d218db0":"markdown","8a41e444":"markdown","c072bce3":"markdown","e032d261":"markdown","fc2d1ee1":"markdown","26cde7be":"markdown","ef33b08e":"markdown","a351b438":"markdown","f3a09bbb":"markdown","fe1adf31":"markdown","fe11d379":"markdown","37c23ab5":"markdown","10b499f9":"markdown","bed59fde":"markdown","865fd4f5":"markdown","c7c18dbd":"markdown","8bf19167":"markdown","237185d0":"markdown","e39ff02a":"markdown","6bf79134":"markdown","b5f371d0":"markdown","37296ee1":"markdown","861a3d59":"markdown"},"source":{"840a60e2":"import os\nimport pandas as pd\nimport numpy as np\nimport datetime \nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, Normalizer, OneHotEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn import metrics\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans, SpectralClustering\nfrom sklearn.pipeline import make_pipeline\n\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\nsns.set_theme(style=\"whitegrid\")\npd.set_option(\"display.max_columns\", 30)","adea15be":"marketing_campaign_data = \"..\/input\/customer-personality-analysis\/marketing_campaign.csv\"","ec5a235d":"records = pd.read_csv(marketing_campaign_data, delimiter='\\t', index_col=['ID'])","837c9ecf":"records.head()","839d63df":"records.isna().sum().to_frame()","ca38fc15":"records = records.dropna(axis=0)","9772486a":"records = records.drop(['Z_CostContact', 'Z_Revenue'],axis=1)","9b40ce08":"records.info()","b0e7bda8":"records.describe().T","d55be51f":"records[records['Income'] == 666666.0]","e537c3dc":"records[records['MntWines'] >= 1000]","4184d067":"records[records['MntWines'] >= 1000].describe().T","25f7d8bd":"records[records['MntMeatProducts'] >= 1000]","9850d102":"records[records['Income'] > 200000]","3538cf3b":"records = records[records['Income'] < 200000]","9a9df021":"records.describe().T","10e66564":"fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(ax=axes[0], x=records['Education'].value_counts().index, y=records['Education'].value_counts().values, palette='flare', order=['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD'])\naxes[0].set_title('Categorical Distribution of Education Level')\naxes[0].set_xlabel('Education Level')\naxes[0].set_ylabel('Number of Customers')\n\nsns.barplot(ax=axes[1], x=records['Marital_Status'].value_counts().index, y=records['Marital_Status'].value_counts().values, palette='pastel', order=['Single', 'Together', 'Married', 'Divorced', 'Widow', 'Alone', 'Absurd', 'YOLO'])\naxes[1].set_title('Categorical Distribution of Marital Status')\naxes[1].set_xlabel('Marital Status')\naxes[1].set_ylabel('Number of Customers')\n\nplt.show()","26b5102e":"fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(ax=axes[0], x=records[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum().index, y=records[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum().values, palette='rainbow')\naxes[0].set_title('Amounts Spent on Products')\naxes[0].set_xlabel('Products')\naxes[0].set_xticklabels(['Wines', 'Fruits', 'Meat', 'Fish', 'Sweet', 'Gold'])\naxes[0].set_ylabel('Amounts')\n\nsns.barplot(ax=axes[1], x=records['Complain'].value_counts().index, y=records['Complain'].value_counts().values, palette='GnBu', order=[1, 0])\naxes[1].set_title('Categorical Distribution of Complain')\naxes[1].set_xlabel('Complains')\naxes[1].set_xticklabels(['Yes', 'No'])\naxes[1].set_ylabel('Number of Customers')\n\nplt.show()","be71c5c4":"records[records['Complain'] == 1]","1a496e24":"records[records['Complain'] == 1].describe().T","9c22322e":"fig, axes = plt.subplots(1, 2, figsize=(24, 6))\n\nsns.barplot(ax=axes[0], x=records[['NumWebPurchases','NumCatalogPurchases', 'NumStorePurchases']].sum().index, y=records[['NumWebPurchases','NumCatalogPurchases', 'NumStorePurchases']].sum().values, palette='rainbow')\naxes[0].set_title('Categorical Distribution of Purchases made through')\naxes[0].set_xlabel('Purchases made through')\naxes[0].set_ylabel('Number of Customers')\n\nsns.barplot(ax=axes[1], x=records[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].sum().index, y=records[['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']].sum().values, palette='rainbow')\naxes[1].set_title('Categorical Distribution of Compaign Acceptances')\naxes[1].set_xlabel('Compaign Acceptances')\naxes[1].set_ylabel('Number of Customers')\n\nplt.show()","94ac4b91":"fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n\nsns.histplot(ax=axes[0], x=(pd.to_datetime(records['Dt_Customer']).max() - pd.to_datetime(records['Dt_Customer'])).dt.days, kde=True, bins=40)\naxes[0].set_xlabel(\"Number of Days since Join\")\n\nsns.histplot(ax=axes[1], x=(records['Recency']), kde=True, bins=40, color='orange')\naxes[1].set_xlabel(\"Recency\")\n\nsns.histplot(ax=axes[2], x=(records['Year_Birth']), kde=True, bins=40, color='green')\naxes[2].set_xlabel(\"Year of Birth\")\n\nplt.show()","5dbaa2df":"fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n\nsns.histplot(ax=axes[0], x=(records['Income']), kde=True, bins=40)\naxes[0].set_xlabel(\"Income\")\n\nsns.histplot(ax=axes[1], x=(records['Income']), kde=True, bins=40, hue=records['Kidhome'])\naxes[1].set_xlabel(\"Distribution of Income Grouped by Number of Kids at Home\")\n\nsns.histplot(ax=axes[2], x=(records['Income']), kde=True, bins=40, hue=records['Teenhome'], palette='Set2_r')\naxes[2].set_xlabel(\"Distribution of Income Grouped by Number of Teens at Home\")\n\nplt.show()","28ee27d2":"plt.figure(figsize=(24, 8))\nsns.boxplot(x='Education', y='Income', hue='Marital_Status', data=records, palette='pastel', width=0.5, order=['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD'])\nplt.show()","4952fe5b":"records['Marital_Status'].value_counts()","3fa29fed":"plt.figure(figsize=(24, 8))\nsns.boxplot(x='Education', y='Recency', hue='Marital_Status', data=records, palette='pastel', width=0.5, order=['Basic', '2n Cycle', 'Graduation', 'Master', 'PhD'])\nplt.show()","03e88204":"sns.pairplot(\n    data=records[['Income', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']],\n    diag_kind='kde',\n    kind='reg',\n    plot_kws={\n        'scatter_kws': {'alpha': 0.5, 's': 1},\n        'line_kws': {'color': 'salmon'}\n    },\n    corner=True\n)","4e174d61":"plt.figure(figsize=(28,28))\n\nmask = ~((records.corr() <= -.3) | (records.corr() >= .3)).values\nmask[np.triu_indices_from(mask, k=1)] = True\n\nsns.heatmap(records.corr(), mask=mask, cmap='OrRd', center=0, vmin=-1, vmax=1, fmt='.2f', annot=True, annot_kws={'fontsize': 12}, linewidth=1, linecolor='grey', square=True)\nplt.show()","0bb5c770":"records.head(5)","ecae8cb5":"print(pd.to_datetime(records['Dt_Customer']).min())\nprint(pd.to_datetime(records['Dt_Customer']).max())","95698795":"records['Age'] = pd.to_datetime(records['Dt_Customer']).max().year - records['Year_Birth']\nrecords['NumDaysJoin'] = (pd.to_datetime(records['Dt_Customer']).max() - pd.to_datetime(records['Dt_Customer'])).dt.days\nrecords['TotalSpending'] = records[['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']].sum(axis=1)\nrecords['NumKidsTeens'] = records[['Kidhome', 'Teenhome']].sum(axis=1)","3608cb6b":"data = records.copy().drop(columns=['Dt_Customer'])\ndata.head()","88a53ff1":"gaussian_mixture_settings = {\n    'n_components': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'covariance_type': ['full', 'tied', 'diag', 'spherical'],\n    'max_iter': 2000,\n    'random_state': 42,\n    'least_group': 4\n}\n\nkmeans_settings = {\n    'n_clusters': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n    'max_iter': 2000,\n    'algorithm': ['full', 'elkan'],\n    'random_state': 42,\n    'least_group': 4\n}\n\npreprocessing_channel = make_pipeline(\n    StandardScaler(),\n    Normalizer(norm='l2')\n)\n\nuse_features = ['Income', 'TotalSpending', 'NumDaysJoin']","e6a1f620":"def gaussian_mixture_model(data, settings, verbose=1):\n    \n    param_dict = {}\n    best_model = None\n    best_silhouette = -1\n    best_labels = None\n    best_params = None\n    \n    # grid search - for components and covariance type\n    for n_comp in settings['n_components']:\n        if n_comp >= settings['least_group']:\n            for cov_type in settings['covariance_type']:\n                if verbose == 1:\n                    print(f\"train model with n_components={n_comp} and covariance_type={cov_type}\")\n                model = make_pipeline(\n                    preprocessing_channel,\n                    GaussianMixture(\n                        n_components=n_comp, \n                        covariance_type=cov_type,\n                        max_iter=settings['max_iter'],\n                        random_state=settings['random_state']\n                    )\n                )\n\n                model.fit(data)\n                labels = model.predict(data)\n\n                _silhouette = metrics.silhouette_score(\n                    preprocessing_channel.fit_transform(data),\n                    labels\n                )\n\n                params = {\n                    'labels': labels,\n                    'silhouette_score': _silhouette,\n                    'n_components': n_comp,\n                    'covariance_type': cov_type,\n                    'max_iter': settings['max_iter'],\n                    'random_state': settings['random_state']\n                }\n\n                if _silhouette > best_silhouette:\n                    best_model = model\n                    best_silhouette = _silhouette\n                    best_params = params\n                    best_labels = labels\n\n                param_dict['n=' + str(n_comp) + '_' + cov_type] = params\n\n    return best_labels, best_silhouette, best_params, best_model, param_dict\n\ndef kmeans_model(data, settings, verbose=1):\n    \n    param_dict = {}\n    best_model = None\n    best_silhouette = -1\n    best_labels = None\n    best_params = None\n    \n    # grid search for number of clusters and K-Means algorithm\n    for n_cluster in settings['n_clusters']:\n        if n_cluster >= settings['least_group']:\n            for algorithm in settings['algorithm']:\n                if verbose == 1:\n                    print(f\"train model with n_clusters={n_cluster} and algorithm={algorithm}\")\n                model = make_pipeline(\n                    preprocessing_channel,\n                    KMeans(\n                        n_clusters=n_cluster,\n                        algorithm=algorithm,\n                        max_iter=settings['max_iter'],\n                        random_state=settings['random_state']\n                    )\n                )\n\n                model.fit(data)\n                labels = model.predict(data)\n\n                _silhouette = metrics.silhouette_score(\n                    preprocessing_channel.fit_transform(data),\n                    labels\n                )\n\n                params = {\n                    'labels': labels,\n                    'silhouette_score': _silhouette,\n                    'n_clusters': n_cluster,\n                    'algorithm': algorithm,\n                    'max_iter': settings['max_iter'],\n                    'random_state': settings['random_state']\n                }\n\n                if _silhouette > best_silhouette:\n                    best_model = model\n                    best_silhouette = _silhouette\n                    best_params = params\n                    best_labels = labels\n\n                param_dict['n=' + str(n_cluster) + '_' + algorithm] = params\n            \n    return best_labels, best_silhouette, best_params, best_model, param_dict","c0f492fc":"k_labels, k_score, _, k_model, k_param_dict = kmeans_model(data[use_features], kmeans_settings, verbose=0)\ng_labels, g_score, _, g_model, g_param_dict = gaussian_mixture_model(data[use_features], gaussian_mixture_settings, verbose=0)","ef723374":"print(f\"KMeans' best score: {k_score}\")\nprint(f\"Gaussian Mixture' best score: {g_score}\")","2c69e560":"labels, score, model, param_dict = k_labels, k_score, k_model, k_param_dict\nmodel","69b71d62":"data['Clusters'] = labels","ac7147fd":"age_segment_labels = ['Young', 'Adult', 'Mature', 'Senior']\nage_segment_intval = [data['Age'].min(), 25, 40, 60, data['Age'].max()]\nincome_segment_labels = ['Low Income', 'Low to Medium Income', 'Medium to High Income', 'High Income']\nseniority_segment_labels = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\nproducts_buyer_segment_labels = ['Low Frequent Buyer', 'Frequent Buyer', 'Biggest Buyer']\n\ndata['AgeGroup'] = pd.cut(data['Age'], bins=age_segment_intval, labels=age_segment_labels).astype(\"object\")\ndata['IncomeGroup'] = pd.qcut(data['Income'], q=4, labels=income_segment_labels).astype(\"object\")\ndata['Seniority'] = pd.qcut(data['NumDaysJoin'], q=4, labels=seniority_segment_labels).astype(\"object\")\ndata['Wines'] = pd.qcut(data['MntWines'][data['MntWines']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")\ndata['Fruits'] = pd.qcut(data['MntFruits'][data['MntFruits']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")\ndata['Meat'] = pd.qcut(data['MntMeatProducts'][data['MntMeatProducts']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")\ndata['Fish'] = pd.qcut(data['MntFishProducts'][data['MntFishProducts']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")\ndata['Sweets'] = pd.qcut(data['MntSweetProducts'][data['MntSweetProducts']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")\ndata['Gold'] = pd.qcut(data['MntGoldProds'][data['MntGoldProds']>0],q=[0, .25, .75, 1], labels=products_buyer_segment_labels).astype(\"object\")","db6216db":"customer_profile = data[['Clusters', 'Age', 'AgeGroup', 'Seniority', 'Education', 'Marital_Status', 'Income', 'IncomeGroup', 'Complain', 'Wines', 'Fruits', 'Meat', 'Fish', 'Sweets', 'Gold', 'TotalSpending', 'NumDaysJoin','NumKidsTeens', 'Recency']].copy()","6dc3c593":"customer_profile.groupby('Clusters')[['Income', 'NumDaysJoin', 'TotalSpending']].describe().T","bbcbe6bd":"customer_profile['Clusters'] = customer_profile['Clusters'].map({\n    0: 'High_Income_New_or_Discover_Customer_HighSpending',\n    1: 'Relative_Low_Income_Old_Customer_LowSpending',\n    2: 'Relative_High_Income_Old_Customer_HighSpending',\n    3: 'Moderate_Income_New_or_Discover_Customer_LowSpending'\n})\n\ncustomer_profile['HasChildren'] = np.where(customer_profile['NumKidsTeens'] > 0, 'Yes', 'No')","06d97b22":"customer_profile.head()","9455c684":"fig = plt.figure(figsize=(28, 28))\nax = fig.add_subplot(111, projection='3d')\n\nfor c in list(customer_profile['Clusters'].unique()):\n    ax.scatter(\n        customer_profile[customer_profile['Clusters'] == c]['NumDaysJoin'],\n        customer_profile[customer_profile['Clusters'] == c]['Income'],\n        customer_profile[customer_profile['Clusters'] == c]['TotalSpending'],\n        s=200\n    )\n\nax.set_xlabel('Number of Days that Customer Enrolled with the Company')\nax.set_ylabel('Income')\nax.set_zlabel('Total Spending')\n\nplt.show()","b91c6556":"rule_extraction_data = customer_profile.drop(columns=['TotalSpending', 'NumDaysJoin', 'NumKidsTeens', 'Recency', 'Age', 'Income'])","b9daa455":"rule_extraction_data.head()","bc6c11ff":"rule_extraction_data = pd.get_dummies(rule_extraction_data)","b869c86f":"frequent_items = apriori(rule_extraction_data, use_colnames=True, min_support=0.08, max_len=11)\nrules = association_rules(frequent_items, metric='lift', min_threshold=1)","9e01ba57":"rules.head()","3a34d595":"product = 'Wines'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_wines_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_wines_buyer[['antecedents', 'consequents', 'confidence']].head(10)","253d2f08":"product = 'Fruits'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_fruits_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_fruits_buyer[['antecedents', 'consequents', 'confidence']].head(10)","4267c15b":"product = 'Meat'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_meat_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_meat_buyer[['antecedents', 'consequents', 'confidence']].head(10)","032cbfcc":"product = 'Fish'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_fish_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_fish_buyer[['antecedents', 'consequents', 'confidence']].head(10)","dc3d95f2":"product = 'Sweets'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_sweets_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_sweets_buyer[['antecedents', 'consequents', 'confidence']].head(10)","c456d108":"product = 'Gold'\nsegment = 'Biggest Buyer'\ntarget = product + '_' + segment\nbiggest_gold_buyer = rules[rules['consequents'].astype(str).str.contains(target, na=False)].sort_values(by='confidence', ascending=False)\nbiggest_gold_buyer[['antecedents', 'consequents', 'confidence']].head(10)","6da815c2":"<p style=\"text-align: justify\">Now, let's look at the distribution of the income, the recency, and the year of birth of the customers.<\/p>","67b545c4":"<p style=\"text-align: justify\">Below is the summary statistics for each cluster in terms of income, number of days enrolled, and spending. It will be used to interpret the clusters.<\/p>","4896d0b8":"<p style=\"text-align: justify\">Then, by visualizing the total amounts spent on each products across all the customers, we can see that the wines and meat products are the majority. In addition to this, we can see that there is a small amount of customers made complains before.<\/p>","81501108":"<p style=\"text-align: justify\"> \nThere are 115 customers spent on wines with an amount over 1000 in the past 2 years at the time the data is collected. We can see that the average income of these customers is $76493.31 which is sitting above the 3rd quatile of the income.<\/p>","5d09b913":"<p style=\"text-align: justify\">Let's prepare the data for rule extraction. We will use the Apriori algorithm to extract frequent items, and then extract rules based on those frequent item set. Since the Apriori algorithm only taking 0 - 1 vector, we need to drop numeric values (alrady converted to group in the previous step) and perform one-hot encoding on those groups.<\/p>","76da6c6b":"<p style=\"text-align: justify\">Now, all the rules are extracted, we need to filter over it to get the rules that we are interested in.<\/p>","cbd996bf":"<p style=\"text-align: justify\">The distribution of income presents a difference among customers with different number of children or teens at home.<\/p>","aa3e1886":"# Postprocessing & Customer Analysis","d64d1794":"<p style=\"text-align: justify\">Let's convert Year_Birth, Dt_Customer to Age, and Number_of_Days_Enrolled such that these features can be used by the clustering algorithm. We also created a feature 'Total Spending' to summarize the amounts spent on all products by the customers in the last two years.<\/p>","4bacb784":"<p style=\"text-align: justify\">Let's drop the records with missing values as well as the two columns that we don't really have an idea about it meanings.<\/p>  ","7e143626":"<p style=\"text-align: justify\">By inspecting the column types, we can see that all the numerical features are type of integer, except for the income.<\/p>  ","e1400acf":"<p style=\"text-align: justify\">Let's inspect the group of customers who made the complains before.<\/p>","0d218db0":"<p style=\"text-align: justify\">we take the one that results in the highest silhouette score as the model for our further analysis.<\/p>","8a41e444":"<p style=\"text-align: justify\">3D visualization of the clustering result.<\/p>","c072bce3":"<p style=\"text-align: justify\">Then, let's look at the categorical distributions of education level and the marital status of the customers. We can see that majority of the customers have a graduation degree, and most of them are in relationship of together or married.<\/p>","e032d261":"# Feature Engineering","fc2d1ee1":"# EDA & Data Cleaning","26cde7be":"<p style=\"text-align: justify\">From the boxplot, we can see that different marital status doesn't present too much differences from income, while the customers with 'basic' education level significantly differs from the other groups in terms of income. For the marital status, since only a small amount of customers are presence in some groups (e.g. Alone, Absurd, YOLO), it might be not suitable to conclude there is a real difference between them and the other groups in terms of income.<\/p>","ef33b08e":"<p style=\"text-align: justify\">we can visualize the correlation matrix to better see the correlation between features.<\/p>","a351b438":"<p style=\"text-align: justify\">Let's drop this record since the reason behind this high income value is uncertain<\/p>","f3a09bbb":"<p style=\"text-align: justify\">The preprocessing step includes Standardization and Normalization, to convert the data to have zero mean, unit variance, and unit norm. These two processes will be encapsulated to a pipeline that will be chained with the model. Below are two settings that will be used by a grid search to explore the parameters for the model that resulting in the highest silhouette score.<\/p>","fe1adf31":"# Preprocessing & Clustering Pipeline","fe11d379":"<p style=\"text-align: justify\">create serveral features to describe customers in terms of groups.<\/p>","37c23ab5":"<p style=\"text-align: justify\">From the pair plot below, we can see that there are moderate to high positive correlations between incomes and the amounts spent on products. Also, there are also positive correlations between the amount spent on each product.<\/p>","10b499f9":"<p style=\"text-align: justify\">We can also see that only one customer has income greater than $200,000<\/p>","bed59fde":"<p style=\"text-align: justify\">The income column contains 24 missing values. Also, we can also see two columns 'Z_CostContact' and 'Z_Revenue' which haven't been described by the metadata.<\/p>  ","865fd4f5":"The rules are already sorted by confidence, we can take the top 10 rules after filtering it, to get the description in terms of rules for the top 10 buyers for wines, meat, fruit, sweets, gold, and fish.","c7c18dbd":"<p style=\"text-align: justify\">Let's first start with exploring the features in the dataset. Based on the data description, the features can be grouped into infomation about customers, information about the products, promotions, and the place where the purchases were made through.<\/p>  \n\n[Data Metadata](https:\/\/www.kaggle.com\/imakash3011\/customer-personality-analysis)","8bf19167":"<p style=\"text-align: justify\">The customer with income of $66,666.00 enrolled with the company on February 6th, 2013. The customer visit the company's website 6 times a month, and the last purchased was made 23 days ago at the time the data is collected.<\/p>  ","237185d0":"<p style=\"text-align: justify\">Only 4 customers spent on meat products with amount greater than or equal to 1000.<\/p>","e39ff02a":"# Customer Personality Analysis and Segmentation\n\n<p style=\"text-align: justify;\">Customer Personality Analysis can help a company to better understand its customers and makes it easier for them to plan sales strategies or to modify their products based on the needs, behaviors and concerns of customers under various segmentations. This notebook aims to analyze the <strong>underlying customer segments<\/strong> and find out <strong>the top 10 buyers<\/strong> for each product.<\/p>\n\n# Table of Contents\n\n1. EDA & Data Cleaning  \n2. Feature Engineering  \n3. Preprocessing & Clustering Pipeline  \n4. Cluster Visualization & Postprocessing  \n5. Association Rule Mining  \n6. Conclusion  \n\n\n**Acknowledgement:** \n[Customer Personality Analysis with Python - Aman Kharwal](https:\/\/thecleverprogrammer.com\/2021\/02\/08\/customer-personality-analysis-with-python\/)\n\n<p style=\"text-align: justify;\">Please feel free to comment if you observed any error, wording issues, misinterpretation, or you had any recommandations. Thanks!<\/p>","6bf79134":"<p style=\"text-align: justify\">Let's look at the summary statistics of the data. We can see that the maximum income is much higher than the income at the 3rd quatile (almost 10 times greater). Also, the maximum amount spent on wines and meat products (i.e. MntWines, MntMeatProduct) are significantly grater than the one spent on other products.<\/p>  ","b5f371d0":"<p style=\"text-align: justify\">After that, we can see most of the purchases are placed in store, and website as the second. And around 370 of out 2215 customers accepted the offer in the last campaign.<\/p>","37296ee1":"<p style=\"text-align: justify\">There are total 21 customers made complains before. These customers have relatively high consumption on wines and meat products, and the recency ranged from around a week to 2 and a half month.<\/p>","861a3d59":"<p style=\"text-align: justify\">we can find that the customers enrolled with the company between 2012 and 2014<\/p>"}}