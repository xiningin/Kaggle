{"cell_type":{"debc7292":"code","9948cf1d":"code","5ca8ba30":"code","26b1b078":"code","fb01c31c":"code","8fa27fd7":"code","04e4e943":"code","33697d7e":"code","12e51182":"code","b00dc073":"code","3db241f0":"code","cdf1db49":"code","521566b9":"code","848f7f12":"code","45a96240":"code","65241a48":"code","2c9a8c2c":"code","ebe019c9":"code","f6a06cfc":"code","cc0ea5e2":"code","9cc29450":"code","ebd5397d":"code","2f03c667":"code","6054dd0a":"code","bb7b08a5":"code","93e8dfac":"code","ccc5b91e":"code","948313a8":"code","6ba30ec9":"code","a0c5bc2e":"code","c153855f":"code","a5f15f40":"code","d2e13842":"code","6176cc00":"code","4ff8fc33":"code","9e734893":"code","3630677f":"markdown","f0c907e6":"markdown","52131c43":"markdown","6d0be15a":"markdown","32188273":"markdown","9560df99":"markdown","930575c8":"markdown","926dcbd3":"markdown","097ae42b":"markdown","225dd00a":"markdown","3b097c35":"markdown","cdeab4fa":"markdown","5de3d590":"markdown","d28a49bb":"markdown","36ff3fce":"markdown","7deeef14":"markdown","b160803d":"markdown","62d6e281":"markdown","a8769b0e":"markdown","e5fab6e7":"markdown","d43671ec":"markdown","b1752066":"markdown","c9d822ea":"markdown"},"source":{"debc7292":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9948cf1d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\n\n\nimport keras\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras \nfrom keras import backend as K\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import LSTM,Dense,Bidirectional,Input\nfrom keras.models import Model\nimport torch\nimport transformers","5ca8ba30":"df=pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head()","26b1b078":"df.describe()","fb01c31c":"sns.set(style = \"darkgrid\" , font_scale = 1.2)\nsns.countplot(df.sentiment)","8fa27fd7":"df.isna().sum() # Checking for any missing values","04e4e943":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","33697d7e":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop and i.strip().lower().isalpha():\n            final_text.append(i.strip().lower())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['review']=df['review'].apply(denoise_text)","12e51182":"df.sentiment.replace(\"positive\" , 1 , inplace = True)\ndf.sentiment.replace(\"negative\" , 0 , inplace = True)\ndf.head()","b00dc073":"sns.set(style = \"white\" , font_scale = 1.2)","3db241f0":"plt.figure(figsize = (20,20)) # Positive Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 1].review))\nplt.imshow(wc , interpolation = 'bilinear')","cdf1db49":"plt.figure(figsize = (20,20)) # Negative Review Text\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.sentiment == 0].review))\nplt.imshow(wc , interpolation = 'bilinear')","521566b9":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['sentiment']==1]['review'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Text with Good Reviews')\ntext_len=df[df['sentiment']==0]['review'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Text with Bad Reviews')\nfig.suptitle('Characters in texts')\nplt.show()","848f7f12":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=df[df['sentiment']==1]['review'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Text with Good Reviews')\ntext_len=df[df['sentiment']==0]['review'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Text with Bad Reviews')\nfig.suptitle('Words in texts')\nplt.show()","45a96240":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['sentiment']==1]['review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Text with Good Reviews')\nword=df[df['sentiment']==0]['review'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Text with Bad Reviews')\nfig.suptitle('Average word length in each text')","65241a48":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\ncorpus = get_corpus(df.review)\ncorpus[:5]","2c9a8c2c":"from collections import Counter\ncounter = Counter(corpus)\nmost_common = counter.most_common(10)\nmost_common = dict(most_common)\nmost_common","ebe019c9":"def get_top_text_ngrams(corpus, n, g):\n    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","f6a06cfc":"most_common_uni = get_top_text_ngrams(df.review,20,1)\nmost_common_uni = dict(most_common_uni)\ntemp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\ntemp[\"Common_words\"] = list(most_common_uni.keys())\ntemp[\"Count\"] = list(most_common_uni.values())\nfig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","cc0ea5e2":"most_common_bi = get_top_text_ngrams(df.review,20,2)\nmost_common_bi = dict(most_common_bi)\ntemp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\ntemp[\"Common_words\"] = list(most_common_bi.keys())\ntemp[\"Count\"] = list(most_common_bi.values())\nfig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Commmon Bigrams in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","9cc29450":"most_common_tri = get_top_text_ngrams(df.review,20,3)\nmost_common_tri = dict(most_common_tri)\ntemp = pd.DataFrame(columns = [\"Common_words\" , 'Count'])\ntemp[\"Common_words\"] = list(most_common_tri.keys())\ntemp[\"Count\"] = list(most_common_tri.values())\nfig = px.bar(temp, x=\"Count\", y=\"Common_words\", title='Commmon Trigrams in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","ebd5397d":"x_train,x_test,y_train,y_test = train_test_split(df.review,df.sentiment,random_state = 0 , stratify = df.sentiment)","2f03c667":"from tokenizers import BertWordPieceTokenizer\n# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\nfast_tokenizer","6054dd0a":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=400):\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","bb7b08a5":"x_train = fast_encode(x_train.values, fast_tokenizer, maxlen=400)\nx_test = fast_encode(x_test.values, fast_tokenizer, maxlen=400)","93e8dfac":"def build_model(transformer, max_len=400):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","ccc5b91e":"bert_model = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')","948313a8":"model = build_model(bert_model, max_len=400)\nmodel.summary()","6ba30ec9":"history = model.fit(x_train,y_train,batch_size = 32 ,validation_data=(x_test,y_test),epochs = 3)","a0c5bc2e":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","c153855f":"epochs = [i for i in range(3)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(20,10)\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\nax[0].set_title('Training & Testing Accuracy')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\nax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\nax[1].set_title('Training & Testing Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Loss\")\nplt.show()","a5f15f40":"pred = model.predict(x_test)\npred[:5]","d2e13842":"pred = np.round(pred).astype(int)\npred[:5]","6176cc00":"print(classification_report(y_test, pred, target_names = ['Bad Reviews','Good Reviews']))","4ff8fc33":"cm = confusion_matrix(y_test,pred)\ncm","9e734893":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Bad Reviews','Good Reviews'] , yticklabels = ['Bad Reviews','Good Reviews'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","3630677f":"# TRAINING THE MODEL","f0c907e6":"# Description of Dataset\n\n**IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\nThis is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. It consists of a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So,we have to predict the number of positive and negative reviews using either classification or deep learning algorithms.So here we will use BERT and train it for classifying reviews as positive\/negative correctly.**","52131c43":"**Unigram Analysis**","6d0be15a":"# ANALYSIS AFTER TRAINING OF MODEL","32188273":"**Source Credits - https:\/\/towardsml.com\/2019\/09\/17\/bert-explained-a-complete-guide-with-theory-and-tutorial\/**","9560df99":"**BASIC DATA CLEANING**","930575c8":"**SO, WE CAN SEE THAT THE DATASET IS BALANCED**","926dcbd3":"**WORDCLOUD FOR NEGATIVE TEXT (LABEL - 0)**","097ae42b":"**Number of characters in texts**","225dd00a":"# DATA VISUALIZATION AND PREPROCESSING","3b097c35":"**PLS UPVOTE THIS NOTEBOOK IF YOU LIKE IT! THANKS FOR YOUR TIME !** ","cdeab4fa":"# BERT WORKING\n\n**BERT relies on a Transformer (the attention mechanism that learns contextual relationships between words in a text). A basic Transformer consists of an encoder to read the text input and a decoder to produce a prediction for the task. Since BERT\u2019s goal is to generate a language representation model, it only needs the encoder part. The input to the encoder for BERT is a sequence of tokens, which are first converted into vectors and then processed in the neural network. But before processing can start, BERT needs the input to be massaged and decorated with some extra metadata:**\n\n**1. Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the first sentence and a [SEP] token is inserted at the end of each sentence.**\n\n**2. Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the encoder to distinguish between sentences.**\n\n**3. Positional embeddings: A positional embedding is added to each token to indicate its position in the sentence.**","5de3d590":"**WORDCLOUD FOR POSITIVE TEXT (LABEL - 1)**","d28a49bb":"# IMPORTING THE DATASET","36ff3fce":"**Number of words in each text**","7deeef14":"**Fast Encoding**","b160803d":"# LOADING THE NECESSARY LIBRARIES","62d6e281":"**Bigram Analysis**","a8769b0e":"**Trigram Analysis**","e5fab6e7":"![image.png](attachment:image.png)","d43671ec":"![image.png](attachment:image.png)","b1752066":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","c9d822ea":"**Average word length in a text**"}}