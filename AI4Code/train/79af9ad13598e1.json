{"cell_type":{"4fdf1ee9":"code","6a32a747":"code","fae9ebe1":"code","cc0d65c5":"code","f12d8c54":"code","a2842d51":"code","eaf02add":"code","bf74be5c":"code","e900abee":"code","ce0d7582":"code","48becf34":"code","f4566704":"code","135af536":"code","284a14ec":"code","fedd2948":"code","2c87749d":"code","c52465ef":"code","dd4bfa2a":"code","90dd6da1":"code","6ad4ddda":"code","135c6f26":"code","a7fac955":"code","3db3fb1f":"code","40ce8c1a":"code","7ec933a1":"code","70c1590e":"code","e6481148":"code","287d59c4":"code","42c93e8f":"code","905141f9":"code","e9017661":"code","8d585a31":"code","6e6423b9":"code","db5a3e2c":"code","c21cabfe":"code","a24526af":"code","36615b5b":"code","57031ba8":"code","c21f786e":"code","773e4f42":"code","85e4d89e":"code","ec604a34":"code","599d8747":"code","aa6666ff":"code","901f4304":"code","b387a2e7":"code","f41328ca":"code","39aa28d7":"code","c9387e6f":"code","2af088a3":"markdown","2cdb08ad":"markdown","3a46ac08":"markdown","a8d83365":"markdown","fa022594":"markdown","02929724":"markdown","8c4ed017":"markdown","3e8a01bc":"markdown","863980a6":"markdown","75c1c520":"markdown","f296d42d":"markdown","d728dbe4":"markdown","e0322eb8":"markdown","b7033dac":"markdown","9bbb6156":"markdown","90e41135":"markdown","c44f3f1a":"markdown","8b7b2860":"markdown","40d41c71":"markdown","f9e809bf":"markdown","289bbffa":"markdown"},"source":{"4fdf1ee9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a32a747":"directory = \"..\/input\/home-depot-product-search-relevance\"\npickle_directory = \"..\/input\/btl-home-depot-product-search-relevance\/\"","fae9ebe1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nimport pickle\nimport keras as kr\nfrom keras.preprocessing import text, sequence\nfrom keras import layers, models, optimizers\nfrom keras.layers import *\nimport xgboost\nimport gensim\nfrom time import time\nfrom gensim.models import KeyedVectors\nfrom gensim.utils import simple_preprocess, tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport spacy\nimport re\nimport math \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder","cc0d65c5":"train = pd.read_csv(directory + \"\/train.csv.zip\", encoding = \"ISO-8859-1\", index_col= \"id\")\nprint(train.shape)\ntrain.head(5)","f12d8c54":"test = pd.read_csv(directory + \"\/test.csv.zip\", encoding = \"ISO-8859-1\", index_col= \"id\")\nprint(test.shape)\ntest.head(5)","a2842d51":"attributes=pd.read_csv(directory + \"\/attributes.csv.zip\")\nprint(attributes.shape)\nattributes.head(5)","eaf02add":"hight_relevance = train.loc[train[\"relevance\"] == 3]\nhight_relevance[['product_title', 'search_term']].head()","bf74be5c":"product_descriptions = pd.read_csv('..\/input\/home-depot-product-search-relevance\/product_descriptions.csv.zip')\ntrain = pd.merge(train, product_descriptions, on='product_uid')\ntest = pd.merge(test, product_descriptions, on='product_uid')","e900abee":"train.head()","ce0d7582":"train.iloc[0]['search_term'] + '\/' + train.iloc[0]['product_title'] + '\/'+ train.iloc[0]['product_description']","48becf34":"distribution = train['relevance'].value_counts().sort_index()\nplt.figure(figsize=(6, 4))\nplt.ylabel('count')\nplt.xlabel('value of relevance')\ndistribution.plot(kind = 'bar')\nplt.show()","f4566704":"distribution_train = train.search_term.str.split().apply(len).value_counts().sort_index()\ndistribution_test = test.search_term.str.split().apply(len).value_counts().sort_index()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True)\nfig.suptitle('Number of word in search term of train and test set')\n\ndistribution_train.plot(kind = 'bar', sharex = 1, figsize = (7,5), ax = ax1, title = 'train')\ndistribution_test.plot(kind = 'bar', colormap = 'jet', figsize = (7, 5), ax = ax2, title = 'test')","135af536":"distribution_train = train.product_description.str.split().apply(len).divmod(10)[0].value_counts().nlargest(40).sort_index()\ndistribution_test = test.product_description.str.split().apply(len).divmod(10)[0].value_counts().nlargest(40).sort_index()\nfig, (ax1, ax2) = plt.subplots(2, sharex=True)\nfig.suptitle('Number of word in product description of train and test set')\n\ndistribution_train.plot(kind = 'bar', sharex = 1, figsize = (10,5), ax = ax1, title = 'train')\ndistribution_test.plot(kind = 'bar', colormap = 'jet', figsize = (10, 5), ax = ax2, title = 'test')","284a14ec":"print(len(train),' ',len(test))\ntest.tail()","fedd2948":"brand_attributes = attributes[attributes.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"brand\"})\nmaterial_attributes = attributes[attributes.name == \"Material\"][[\"product_uid\", \"value\"]].rename(columns={\"value\": \"material\"})\nbrand_attributes.drop_duplicates(subset=['product_uid'], inplace = True)\nmaterial_attributes.drop_duplicates(subset=['product_uid'], inplace = True)\ntrain = pd.merge(train, brand_attributes, on='product_uid',  how='left').fillna('')\ntest = pd.merge(test, brand_attributes, on='product_uid',  how='left').fillna('')\ntrain = pd.merge(train, material_attributes, on='product_uid',  how='left').fillna('')\ntest = pd.merge(test, material_attributes, on='product_uid',  how='left').fillna('')","2c87749d":"print(len(train),' ',len(test))\ntest.tail()","c52465ef":"strNum = {'zero': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'seven': 7, 'eight': 8, 'nine': 9}\ndef str_stem(s): \n    if isinstance(s, str):\n        #fix strong.Bullet strongBullet\n        s = re.sub(r\"(\\w)\\.([A-Z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n        #stem unit of measurement\n        s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1in. \", s)\n        s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1lb. \", s)\n        s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1sq.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1cu.ft. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1gal. \", s)\n        s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1oz. \", s)\n        s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1cm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1mm. \", s)\n        s = re.sub(r\"([0-9]+)( *)(\u00b0|degrees|degree)\\.?\", r\"\\1 deg. \", s)\n        s = re.sub(r\"([0-9]+)( *)(v|volts|volt)\\.?\", r\"\\1 volt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(wattage|watts|watt)\\.?\", r\"\\1 watt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \", s)\n        s = re.sub(r\"([0-9]+)( *)(qquart|quart)\\.?\", r\"\\1 qt. \", s)\n        s = re.sub(r\"([0-9]+)( *)(hours|hour|hrs.)\\.?\", r\"\\1 hr \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons\/min.|gallons\/min)\\.?\", r\"\\1 gal. per min. \", s)\n        s = re.sub(r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons\/hour|gallons\/hr)\\.?\", r\"\\1 gal. per hr \", s)\n        # Deal with special characters\n        s = s.replace(\"$\",\" \")\n        s = s.replace(\"?\",\" \")\n        s = s.replace(\"&nbsp;\",\" \")\n        s = s.replace(\"&amp;\",\"&\")\n        s = s.replace(\"&#39;\",\"'\")\n        s = s.replace(\"\/>\/Agt\/>\",\"\")\n        s = s.replace(\"<\/a<gt\/\",\"\")\n        s = s.replace(\"gt\/>\",\"\")\n        s = s.replace(\"\/>\",\"\")\n        s = s.replace(\"<br\",\"\")\n        s = s.replace(\"<.+?>\",\"\")\n        s = s.replace(\"[ &<>)(_,;:!?\\+^~@#\\$]+\",\" \")\n        s = s.replace(\"'s\\\\b\",\"\")\n        s = s.replace(\"[']+\",\"\")\n        s = s.replace(\"[\\\"]+\",\"\")\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"+\",\" \")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\",\",\"\") #could be number \/ segment later\n        s = s.replace(\"-\",\" \")\n        s = s.replace(\"\/\/\",\"\/\")\n        s = s.replace(\"\/\",\" \")\n        s = s.replace(\"..\",\".\")\n        s = s.replace(\" \/ \",\" \")\n        s = s.replace(\" \\\\ \",\" \")\n        s = s.replace(\".\",\" . \")\n        s = s.replace(\"  \",\" \")\n        s = re.sub(r\"(^\\.|\/)\", r\"\", s)\n        s = re.sub(r\"(\\.|\/)$\", r\"\", s)\n        s = re.sub(r\"([0-9])([a-z])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])([0-9])\", r\"\\1 \\2\", s)\n        s = re.sub(r\"([a-z])( *)\\.( *)([a-z])\", r\"\\1 \\4\", s)\n        s = re.sub(r\"([a-z])( *)\/( *)([a-z])\", r\"\\1 \\4\", s)\n        s = s.replace(\"*\",\" xbi \")\n        s = s.replace(\" by \",\" xbi \")\n\n        s = (\" \").join([str(strNum[z]) if z in strNum else z for z in s.split(\" \")])\n        s = \" \".join([re.sub('[^A-Za-z0-9-.\/]', ' ', word) for word in s.lower().split()])\n        return s\n    else:\n        return \"null\"","dd4bfa2a":"#TODO : test SymSpell\n!pip install pyspellchecker","90dd6da1":"from spellchecker import SpellChecker\nspell = SpellChecker()","6ad4ddda":"stop_words = set(stopwords.words('english')) \nwordnet_lemmatizer = WordNetLemmatizer()\ngensim.utils.tokenize\ndef preprocessing(doc, correction = 0):\n    words = []\n    doc  = str_stem(doc)\n    for word in doc.split(' '):\n        if word not in stop_words:\n            if correction == 1:\n                if len(spell.unknown([word])):\n                    word = spell.correction(word)\n            word1 = wordnet_lemmatizer.lemmatize(word, pos = \"n\")\n            word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n            word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n            words.append(word3)\n    return ' '.join(words)","135c6f26":"%%time\n\ntrain.search_term = [preprocessing(i, correction = 1) for i in train.search_term]\ntrain.product_description = [preprocessing(i) for i in train.product_description]\ntrain.brand = [preprocessing(i) for i in train.brand]\ntrain.material = [preprocessing(i) for i in train.material]\ntrain.product_title = [preprocessing(i) for i in train.product_title]\n\ntest.search_term = [preprocessing(i, correction = 1) for i in test.search_term]\ntest.product_description = [preprocessing(i) for i in test.product_description]\ntest.brand = [preprocessing(i) for i in test.brand]\ntest.material = [preprocessing(i) for i in test.material]\ntest.product_title = [preprocessing(i) for i in test.product_title]\n\n# train = pd.read_pickle(pickle_directory + \"proccessed_train\")\n# test = pd.read_pickle(pickle_directory + \"proccessed_test\")\n\ntrain.to_pickle(\"proccessed_train\")\ntest.to_pickle(\"proccessed_test\")","a7fac955":"features = pd.concat((train, test), axis=0, ignore_index=True)\nprint(str(len(train)) + ' ' + str(len(test)) + ' ' + str(len(features)))\ndel train\ndel test","3db3fb1f":"features['brand'] = features['brand'].replace(np.nan, '', regex=True)\nfeatures['material'] = features['material'].replace(np.nan, '', regex=True)","40ce8c1a":"from fuzzywuzzy import fuzz\n#with description\nfeatures[\"partial_ratio_desc\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_desc\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_desc\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\n#with tittle\nfeatures[\"partial_ratio_tittle\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_tittle\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_tittle\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\n#with brand name\nfeatures[\"partial_ratio_brand\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_brand\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_brand\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\n#with material\nfeatures[\"partial_ratio_material\"] = [fuzz.partial_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"token_set_ratio_material\"] = [fuzz.token_set_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"token_sort_ratio_material\"] = [fuzz.token_sort_ratio(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]","7ec933a1":"pip install textdistance","70c1590e":"import textdistance\n#with description\nfeatures[\"jaccard_similar_desc\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_desc\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_desc\"] = [textdistance.mra(features[\"search_term\"][i], features[\"product_description\"][i]) for i in range(0, len(features))]\n#with title\nfeatures[\"jaccard_similar_title\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_title\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_title\"] = [textdistance.mra(features[\"search_term\"][i], features[\"product_title\"][i]) for i in range(0, len(features))]\n#with brand\nfeatures[\"jaccard_similar_brand\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_brand\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_brand\"] = [textdistance.mra(features[\"search_term\"][i], features[\"brand\"][i]) for i in range(0, len(features))]\n#with material\nfeatures[\"jaccard_similar_material\"] = [textdistance.jaccard(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"levenshtein_similarz_material\"] = [textdistance.levenshtein(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]\nfeatures[\"mra_similar_material\"] = [textdistance.mra(features[\"search_term\"][i], features[\"material\"][i]) for i in range(0, len(features))]","e6481148":"from nltk.corpus import brown\nembed_model = gensim.models.Word2Vec(brown.sents())\nembed_model.save('brown.embedding')\nmodel = gensim.models.Word2Vec.load('brown.embedding')","287d59c4":"print(model.wv.similarity(\"humanity\", \"angel\"), ' ', model.wv.similarity(\"humanity\", \"evil\"))\nprint(model.wv.similarity(\"cat\", \"alien\"), ' ', model.wv.similarity(\"cat\", \"human\"))\n","42c93e8f":"%%time\ndef embeding_similarity_calculator(s, t, i):\n    _sum = 0\n    avg = 0\n    if len(s.split()) == 0 :\n        return 0\n    for s_word in s.split():\n        _max = 0\n        for t_word in t.split():\n            if ((s_word in model.wv) and (t_word in model.wv)):\n                _max = max(_max, model.wv.similarity(s_word, t_word))\n        _sum += _max\n    avg = _sum\/ len(s.split())\n    return avg\nfeatures[\"word_ebed_similarity\"] = [embeding_similarity_calculator(features[\"search_term\"][i], features[\"product_description\"][i], i) for i in range(0, len(features))]","905141f9":"\ntfidf_vect = TfidfVectorizer(analyzer='char_wb', ngram_range = (3,3), max_features = 1500)\ntfidf_des = tfidf_vect.fit_transform(features.product_description).toarray()\ntfidf_search = tfidf_vect.transform(features.search_term).toarray()\n\n# infile = open(pickle_directory + \"tfidf_des\",'rb')\n# tfidf_des = pickle.load(infile)\n# infile = open(pickle_directory + \"tfidf_search\",'rb')\n# tfidf_search = pickle.load(infile)\n\noutfile = open(\"tfidf_des\",'wb')\npickle.dump(tfidf_des, outfile)\noutfile = open(\"tfidf_search\",'wb')\npickle.dump(tfidf_search, outfile)","e9017661":"from scipy.spatial import distance\nfeatures[\"tfidf_cosine_distance\"] = [distance.cosine(tfidf_search[i], tfidf_des[i]) for i in range(0, len(tfidf_des))]","8d585a31":"# infile = open(\"..\/input\/btl-home-depot-product-search-relevance\/features_18_4\",'rb')\n# features = pickle.load(infile)\noutfile = open(\"features_18_4\",'wb')\npickle.dump(features, outfile)","6e6423b9":"features = pd.DataFrame(features).fillna(0)\ndf_train = features.iloc[:74067]\ndf_test = features.iloc[74067:]\ny_train = df_train['relevance']\ndf_train = df_train.drop(columns=['product_title','product_description','brand','material','search_term','relevance'])\ndf_test = df_test.drop(columns=['product_title','product_description','brand','material','search_term', 'relevance'])","db5a3e2c":"print(len(df_train), ' ',len(df_test))","c21cabfe":"df_train = df_train.sort_values('product_uid')","a24526af":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\n\ndef modelEvaluate(model, X_train, y_train, x_val, y_val, label_encode = None):\n    model.fit(X_train, y_train)\n    pred = model.predict(x_val)\n    _pred = pred\n    _y_val = y_val\n    if label_encode is not None:\n        _y_val = label_encode.inverse_transform(_y_val)\n        _pred = label_encode.inverse_transform(_pred)\n    mae, mse = (mean_absolute_error(_pred, _y_val),mean_squared_error(_pred, _y_val))\n    return (mae, mse)","36615b5b":"from sklearn.model_selection import KFold\n\ndef crossValidate(model, label_encoder = None):\n    mae_score = []\n    mse_score = []\n    kf = KFold(n_splits=4)\n    kf.get_n_splits(df_train)\n    for train_index, test_index in kf.split(df_train):\n        X, X_test = df_train.iloc[train_index], df_train.iloc[test_index]\n        y, y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n        (mae, mse) = modelEvaluate(model, X, y, X_test, y_test, label_encode = label_encoder)\n        mae_score.append(mae)\n        mse_score.append(mse)\n    return [sum(mae_score)\/len(mae_score), math.sqrt(sum(mse_score)\/len(mse_score))]","57031ba8":"%%time\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn import linear_model\n\nresult = []\nresult.append(('RandomForest',crossValidate(RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42))))\nresult.append(('XGBoost',crossValidate(XGBRegressor(colsample_bytree=0.4,       \n                 learning_rate=0.1,\n                 max_depth=6,\n                 n_estimators=700,                                                                    \n                 reg_alpha=0.075,\n                 reg_lambda=0.045,\n                 subsample=0.6,\n                 seed=42))))\nresult.append(('GradientBoosting',crossValidate(GradientBoostingRegressor(n_estimators=700, max_depth=6, random_state=42))))\nresult.append(('ETR',crossValidate(ExtraTreesRegressor(n_estimators=700, max_depth=5, random_state=42))))\nresult.append(('Lasso',crossValidate(linear_model.Lasso(alpha=0.05))))\nresult.append(('ElasticNet',crossValidate(linear_model.ElasticNet(alpha=0.05))))\nresult.append(('Ridge',crossValidate(linear_model.Ridge(alpha=0.05))))","c21f786e":"result","773e4f42":"%%time\nfrom sklearn import linear_model\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import BayesianRidge\nestimators = [  ('RandomForest', RandomForestRegressor(n_estimators=700, max_depth=6, random_state=42)),\n                ('XGBoost', XGBRegressor(colsample_bytree=0.4,learning_rate=0.1,max_depth=6,n_estimators=700,reg_alpha=0.075,reg_lambda=0.045,subsample=0.6,seed=42)),                                                                    \n                ('Lasso',linear_model.Lasso(alpha=0.05)),\n                ('ElasticNet',linear_model.ElasticNet(alpha=0.05)),\n                ('Ridge',linear_model.Ridge(alpha=0.05))\n             ]\nstacked_reg = StackingRegressor(\n        estimators=estimators,\n        final_estimator=BayesianRidge()\n    )\nresult.append(('StackedBayesianRidge',crossValidate(stacked_reg)))","85e4d89e":"labels = []\nmae = []\nrmse = []\nfor _name, _score in result:\n    labels.append(_name)\n    mae.append(_score[0])\n    rmse.append(_score[1])\n\n#Plotting\nx = np.arange(len(labels))  # the label locations\nwidth = 0.3  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(15,6))\nrects1 = ax.bar(x - width\/2, mae, width, label='MAE')\nrects2 = ax.bar(x + width\/2, rmse, width, label='RMSE')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","ec604a34":"type(y_train)","599d8747":"%%time\nle = LabelEncoder()\ny_train = pd.Series(data = le.fit_transform(y_train))\ny_train = y_train.replace({1:2, 3:4, 5:6})\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nresult_classifier = []\nresult_classifier.append(('ETR_c',crossValidate(ExtraTreesClassifier(n_estimators=700,max_depth = 6, class_weight = 'balanced_subsample', random_state=42),le)))\nresult_classifier.append(('RF_c',crossValidate(RandomForestClassifier(n_estimators=700,max_depth = 6, class_weight = 'balanced_subsample', random_state=42),le)))\nresult_classifier.append(('PA_c',crossValidate(PassiveAggressiveClassifier(max_iter = 2000, early_stopping = True, class_weight = 'balanced'),le)))\nresult_classifier.append(('Perceptron_c',crossValidate(Perceptron(max_iter = 2000,penalty = 'l2', class_weight = 'balanced', early_stopping = True, random_state=42),le)))","aa6666ff":"y_train.replace({1:2, 3:4, 5:6, 7:8, 9:10, 11:12}).value_counts()","901f4304":"labels = []\nmae = []\nrmse = []\nfor _name, _score in result_classifier:\n    labels.append(_name)\n    mae.append(_score[0])\n    rmse.append(_score[1])\n\n#Plotting\nx = np.arange(len(labels))  # the label locations\nwidth = 0.3  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(15,6))\nrects1 = ax.bar(x - width\/2, mae, width, label='MAE')\nrects2 = ax.bar(x + width\/2, rmse, width, label='RMSE')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by models')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","b387a2e7":"y_train = features.iloc[:74067]['relevance']","f41328ca":"stacked_reg.fit(df_train, y_train)\npred = stacked_reg.predict(df_test)\npred.shape","39aa28d7":"submission =pd.read_csv(directory + \"\/sample_submission.csv.zip\")\npred = [min(3,i) for i in pred]\npred = [max(1, i) for i in pred]\nsubmission['relevance'] = pred\nsubmission.to_csv('submission.csv', index=False)","c9387e6f":"rf = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42)\nrf.fit(df_train, y_train)\nfi = pd.DataFrame({'feature': df_train.columns, 'importance': rf.feature_importances_}).sort_values(by='importance', ascending=False)\nfi = fi.reset_index()\nfi.head(20)","2af088a3":"### Remove stop words, tokenization, lemmatization. ","2cdb08ad":"# Data Pre-processing","3a46ac08":"## Feature importance","a8d83365":"# Data exploration","fa022594":"## String similarity feature","02929724":"## Make submission","8c4ed017":"# Feature engineering","3e8a01bc":"### Spelling correctors","863980a6":"Extract brand name and material information from attributes file","75c1c520":"# Importing libraries","f296d42d":"## Word embedding feature","d728dbe4":"## Regressor","e0322eb8":"## Metric to evaluate models : MAE, RMSE","b7033dac":"## Processing the description","9bbb6156":"## Class weight +  classifier","90e41135":"## TF-IDF similarity measure feature at char level","c44f3f1a":"## String matching feature ","8b7b2860":"# Model","40d41c71":"## Plotting","f9e809bf":"Open source : https:\/\/gist.github.com\/susanli2016\/b83d148de7394821509bd5172d2c96d3","289bbffa":"Join table 'product_descriptions' and 'train', 'test'"}}