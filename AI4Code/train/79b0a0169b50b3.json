{"cell_type":{"7f93b32e":"code","198c088a":"code","03b81a47":"code","c080cec1":"code","2b4b708b":"code","d521d60f":"code","db951f21":"markdown","25a67ec1":"markdown","daa5e397":"markdown"},"source":{"7f93b32e":"#import libraries\nimport os\nos.environ['TFF_CPP_MIN_LOG_LEVEL'] = \"2\"   # to reduce warnings\n\nimport numpy as np\nimport sys\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# To check GPU \nphysical_devices = tf.config.list_physical_devices('GPU')\ntf.config.experimental.set_memory_growth(physical_devices[0], True)\n\nimg_height = 224\nimg_width = 224\nbatch_size = 32\n\npath = r\"\/kaggle\/input\/facemask-dataset-covid1910k-images-2-folders\/FaceMask-Dataset-covid-19\/\" # path where our subfolders lies\n","198c088a":"\nds_train = tf.keras.preprocessing.image_dataset_from_directory(\n    path,\n    validation_split = 0.2, # we use 20% data as validation data\n    subset = \"training\",    # for training\n    seed=123,\n    image_size = (224,224),\n    batch_size = batch_size\n    )\n\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    path,\n    validation_split = 0.2,\n    subset='validation', # for validation\n    seed=123,\n    image_size=(224,224),\n    batch_size = batch_size\n    )","03b81a47":"# Data augmentation\nnum_classes = 2\n\ndata_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)\n","c080cec1":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = ds_train.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)  #this deals with cache memory and we use shuffle for shuffling the data\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n\n\n\n\n# model building\n\nmodel = keras.models.Sequential([\n  data_augmentation,    # we define previously(for data augmentation)\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])\n\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n","2b4b708b":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","d521d60f":"# check our model state\n\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","db951f21":"**Hey everyone**\n\n\n**some time we realized that dataset is too large to handle with our small RAM. because of small size of RAM, Most of the time our kernel broken.So for dealing that problem,Tensorflow provide some method and today we will discuss about them.I hope you would like it.**\n\n\n**So lets Start**","25a67ec1":"\n**we can get better result if we increase the epoches and also make some changes in Sequential model. This notebook is just for give you a brief introduction about tensorflow Input pipelines and how it works**\n\n\n**Note: This method is use when we have data in subfolders. There are still other methods.we will discuss about them in next notebook**\n\n\n**Thank you. I hope you like this notebook**","daa5e397":"**we use \"image_dataset_from_directory\" method which is one of a off disk method for loading the data**\n"}}