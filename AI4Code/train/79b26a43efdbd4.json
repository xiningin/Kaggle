{"cell_type":{"0ba420fa":"code","c2553f4d":"code","58c80740":"code","2f81125d":"code","3186febb":"code","0f7fd782":"code","89fc0f60":"code","5d7fc484":"code","ac97e6d5":"code","7a3dd2ae":"code","b7aa0c2f":"code","d3832874":"code","e639fe9d":"code","6d96ad85":"code","0597b44d":"code","700ae794":"code","e2519d99":"code","3b89aedd":"code","3c3900b6":"code","f1117ffd":"code","996cc62a":"code","63e80f76":"code","6f5f6483":"code","543fd632":"code","fef059bd":"code","2208d82e":"code","407b2883":"code","9076c2ce":"code","31c8465c":"code","286f80c9":"code","4a22536f":"code","eee0a576":"code","9ac84a57":"code","08cd2a1f":"code","65e0bb3f":"code","b4344a1d":"code","a1c5eafb":"code","cfb6b078":"code","9b8934e9":"code","d23d051a":"code","a6b6a4a9":"code","b34b81ec":"code","d7a4a468":"code","96854a74":"code","d9557ab8":"code","294f3ef7":"code","db7e616a":"code","0a16b002":"code","4cea1003":"code","23dab2f9":"code","964c8de7":"code","8a11ca14":"code","d27a9882":"code","83cee61c":"code","48584f80":"code","f2d1d278":"code","cf56ad87":"code","39a3b7fd":"code","138f33bc":"code","9391ca64":"code","9079ace4":"code","18a73adf":"code","26c2437f":"code","9e607d63":"code","fe46445f":"code","c8dde3ae":"code","61d35839":"markdown","7b23fac9":"markdown","ac8a0232":"markdown","668ada3d":"markdown","a0d4cb8a":"markdown","357a9298":"markdown","c72a3ecc":"markdown","2dc09dd6":"markdown","a7c094e7":"markdown"},"source":{"0ba420fa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c2553f4d":"import matplotlib.pyplot as plt\nimport re\n\n# Remove the useless url tag \ndef remove_url(raw_str):\n    clean_str = re.sub(r'http\\S+', '', raw_str)\n    return clean_str","58c80740":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntrain_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","2f81125d":"#Randomization\nstate = 1\ntrain_df = train_df.sample(frac=1,random_state=state)\ntest_df = test_df.sample(frac=1,random_state=state)\ntrain_df.reset_index(inplace=True, drop=True) \ntest_df.reset_index(inplace=True, drop=True) ","3186febb":"train_df.head()","0f7fd782":"test_df.head()","89fc0f60":"#Credit: Gunes Evitan\n#https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-full-cleaning#4.-Embeddings-&-Text-Cleaning\ndef clean(tweet):\n    \n    # Punctuations at the start or end of words    \n    #for punctuation in \"#@!?()[]*%\":\n    #    tweet = tweet.replace(punctuation, f' {punctuation} ').strip()\n        \n    #tweet = tweet.replace('...', ' ... ').strip()\n    #tweet = tweet.replace(\"'\", \" ' \").strip()        \n    \n    # Special characters\n    tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n    tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n    tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n    tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n    tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n    tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n    tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n    tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n    tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n    tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n    \n    # Contractions\n    tweet = re.sub(r\"he's\", \"he is\", tweet)\n    tweet = re.sub(r\"there's\", \"there is\", tweet)\n    tweet = re.sub(r\"We're\", \"We are\", tweet)\n    tweet = re.sub(r\"That's\", \"That is\", tweet)\n    tweet = re.sub(r\"won't\", \"will not\", tweet)\n    tweet = re.sub(r\"they're\", \"they are\", tweet)\n    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n    tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"What's\", \"What is\", tweet)\n    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n    tweet = re.sub(r\"There's\", \"There is\", tweet)\n    tweet = re.sub(r\"He's\", \"He is\", tweet)\n    tweet = re.sub(r\"It's\", \"It is\", tweet)\n    tweet = re.sub(r\"You're\", \"You are\", tweet)\n    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n    tweet = re.sub(r\"you've\", \"you have\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n    tweet = re.sub(r\"we're\", \"we are\", tweet)\n    tweet = re.sub(r\"what's\", \"what is\", tweet)\n    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n    tweet = re.sub(r\"we've\", \"we have\", tweet)\n    tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n    tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n    tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n    tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n    tweet = re.sub(r\"who's\", \"who is\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n    tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n    tweet = re.sub(r\"would've\", \"would have\", tweet)\n    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n    tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n    tweet = re.sub(r\"We've\", \"We have\", tweet)\n    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n    tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n    tweet = re.sub(r\"they've\", \"they have\", tweet)\n    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n    tweet = re.sub(r\"should've\", \"should have\", tweet)\n    tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n    tweet = re.sub(r\"where's\", \"where is\", tweet)\n    tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n    tweet = re.sub(r\"They're\", \"They are\", tweet)\n    tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n    tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n    tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n    tweet = re.sub(r\"let's\", \"let us\", tweet)\n    \n    # Character entity references\n    tweet = re.sub(r\"&gt;\", \">\", tweet)\n    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n        \n    # Typos, slang and informal abbreviations\n    tweet = re.sub(r\"w\/e\", \"whatever\", tweet)\n    tweet = re.sub(r\"w\/\", \"with\", tweet)\n    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n    tweet = re.sub(r\"<3\", \"love\", tweet)\n    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n    tweet = re.sub(r\"8\/5\/2015\", \"2015-08-05\", tweet)\n    tweet = re.sub(r\"chest\/torso\", \"chest \/ torso\", tweet)\n    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n    tweet = re.sub(r\"8\/6\/2015\", \"2015-08-06\", tweet)\n    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n    \n    # Separating other punctuations\n    tweet = re.sub(r\"MH370:\", \"MH370 :\", tweet)\n    tweet = re.sub(r\"PM:\", \"Prime Minister :\", tweet)\n    tweet = re.sub(r\"Legionnaires:\", \"Legionnaires :\", tweet)\n    tweet = re.sub(r\"Latest:\", \"Latest :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"News:\", \"News :\", tweet)\n    tweet = re.sub(r\"derailment:\", \"derailment :\", tweet)\n    tweet = re.sub(r\"attack:\", \"attack :\", tweet)\n    tweet = re.sub(r\"Saipan:\", \"Saipan :\", tweet)\n    tweet = re.sub(r\"Photo:\", \"Photo :\", tweet)\n    tweet = re.sub(r\"Funtenna:\", \"Funtenna :\", tweet)\n    tweet = re.sub(r\"quiz:\", \"quiz :\", tweet)\n    tweet = re.sub(r\"VIDEO:\", \"VIDEO :\", tweet)\n    tweet = re.sub(r\"MP:\", \"MP :\", tweet)\n    tweet = re.sub(r\"UTC2015-08-05\", \"UTC 2015-08-05\", tweet)\n    tweet = re.sub(r\"California:\", \"California :\", tweet)\n    tweet = re.sub(r\"horror:\", \"horror :\", tweet)\n    tweet = re.sub(r\"Past:\", \"Past :\", tweet)\n    tweet = re.sub(r\"Time2015-08-06\", \"Time 2015-08-06\", tweet)\n    tweet = re.sub(r\"here:\", \"here :\", tweet)\n    tweet = re.sub(r\"fires.\", \"fires .\", tweet)\n    tweet = re.sub(r\"Forest:\", \"Forest :\", tweet)\n    tweet = re.sub(r\"Cramer:\", \"Cramer :\", tweet)\n    tweet = re.sub(r\"Chile:\", \"Chile :\", tweet)\n    tweet = re.sub(r\"link:\", \"link :\", tweet)\n    tweet = re.sub(r\"crash:\", \"crash :\", tweet)\n    tweet = re.sub(r\"Video:\", \"Video :\", tweet)\n    tweet = re.sub(r\"Bestnaijamade:\", \"bestnaijamade :\", tweet)\n    tweet = re.sub(r\"NWS:\", \"National Weather Service :\", tweet)\n    tweet = re.sub(r\".caught\", \". caught\", tweet)\n    tweet = re.sub(r\"Hobbit:\", \"Hobbit :\", tweet)\n    tweet = re.sub(r\"2015:\", \"2015 :\", tweet)\n    tweet = re.sub(r\"post:\", \"post :\", tweet)\n    tweet = re.sub(r\"BREAKING:\", \"BREAKING :\", tweet)\n    tweet = re.sub(r\"Island:\", \"Island :\", tweet)\n    tweet = re.sub(r\"Med:\", \"Med :\", tweet)\n    tweet = re.sub(r\"97\/Georgia\", \"97 \/ Georgia\", tweet)\n    tweet = re.sub(r\"Here:\", \"Here :\", tweet)\n    tweet = re.sub(r\"horror;\", \"horror ;\", tweet)\n    tweet = re.sub(r\"people;\", \"people ;\", tweet)\n    tweet = re.sub(r\"refugees;\", \"refugees ;\", tweet)\n    tweet = re.sub(r\"Genocide;\", \"Genocide ;\", tweet)\n    tweet = re.sub(r\".POTUS\", \". POTUS\", tweet)\n    tweet = re.sub(r\"Collision-No\", \"Collision - No\", tweet)\n    tweet = re.sub(r\"Rear-\", \"Rear -\", tweet)\n    tweet = re.sub(r\"Broadway:\", \"Broadway :\", tweet)\n    tweet = re.sub(r\"Correction:\", \"Correction :\", tweet)\n    tweet = re.sub(r\"UPDATE:\", \"UPDATE :\", tweet)\n    tweet = re.sub(r\"Times:\", \"Times :\", tweet)\n    tweet = re.sub(r\"RT:\", \"RT :\", tweet)\n    tweet = re.sub(r\"Police:\", \"Police :\", tweet)\n    tweet = re.sub(r\"Training:\", \"Training :\", tweet)\n    tweet = re.sub(r\"Hawaii:\", \"Hawaii :\", tweet)\n    tweet = re.sub(r\"Selfies:\", \"Selfies :\", tweet)\n    tweet = re.sub(r\"Content:\", \"Content :\", tweet)\n    tweet = re.sub(r\"101:\", \"101 :\", tweet)\n    tweet = re.sub(r\"story:\", \"story :\", tweet)\n    tweet = re.sub(r\"injured:\", \"injured :\", tweet)\n    tweet = re.sub(r\"poll:\", \"poll :\", tweet)\n    tweet = re.sub(r\"Guide:\", \"Guide :\", tweet)\n    tweet = re.sub(r\"Update:\", \"Update :\", tweet)\n    tweet = re.sub(r\"alarm:\", \"alarm :\", tweet)\n    tweet = re.sub(r\"floods:\", \"floods :\", tweet)\n    tweet = re.sub(r\"Flood:\", \"Flood :\", tweet)\n    tweet = re.sub(r\"MH370;\", \"MH370 ;\", tweet)\n    tweet = re.sub(r\"life:\", \"life :\", tweet)\n    tweet = re.sub(r\"crush:\", \"crush :\", tweet)\n    tweet = re.sub(r\"now:\", \"now :\", tweet)\n    tweet = re.sub(r\"Vote:\", \"Vote :\", tweet)\n    tweet = re.sub(r\"Catastrophe.\", \"Catastrophe .\", tweet)\n    tweet = re.sub(r\"library:\", \"library :\", tweet)\n    tweet = re.sub(r\"Bush:\", \"Bush :\", tweet)\n    tweet = re.sub(r\";ACCIDENT\", \"; ACCIDENT\", tweet)\n    tweet = re.sub(r\"accident:\", \"accident :\", tweet)\n    tweet = re.sub(r\"Taiwan;\", \"Taiwan ;\", tweet)\n    tweet = re.sub(r\"Map:\", \"Map :\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"150-Foot\", \"150 - Foot\", tweet)\n    tweet = re.sub(r\"failure:\", \"failure :\", tweet)\n    tweet = re.sub(r\"prefer:\", \"prefer :\", tweet)\n    tweet = re.sub(r\"CNN:\", \"CNN :\", tweet)\n    tweet = re.sub(r\"Oops:\", \"Oops :\", tweet)\n    tweet = re.sub(r\"Disco:\", \"Disco :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"Grows:\", \"Grows :\", tweet)\n    tweet = re.sub(r\"projected:\", \"projected :\", tweet)\n    tweet = re.sub(r\"Pakistan.\", \"Pakistan .\", tweet)\n    tweet = re.sub(r\"ministers:\", \"ministers :\", tweet)\n    tweet = re.sub(r\"Photos:\", \"Photos :\", tweet)\n    tweet = re.sub(r\"Disease:\", \"Disease :\", tweet)\n    tweet = re.sub(r\"pres:\", \"press :\", tweet)\n    tweet = re.sub(r\"winds.\", \"winds .\", tweet)\n    tweet = re.sub(r\"MPH.\", \"MPH .\", tweet)\n    tweet = re.sub(r\"PHOTOS:\", \"PHOTOS :\", tweet)\n    tweet = re.sub(r\"Time2015-08-05\", \"Time 2015-08-05\", tweet)\n    tweet = re.sub(r\"Denmark:\", \"Denmark :\", tweet)\n    tweet = re.sub(r\"Articles:\", \"Articles :\", tweet)\n    tweet = re.sub(r\"Crash:\", \"Crash :\", tweet)\n    tweet = re.sub(r\"casualties.:\", \"casualties .:\", tweet)\n    tweet = re.sub(r\"Afghanistan:\", \"Afghanistan :\", tweet)\n    tweet = re.sub(r\"Day:\", \"Day :\", tweet)\n    tweet = re.sub(r\"AVERTED:\", \"AVERTED :\", tweet)\n    tweet = re.sub(r\"sitting:\", \"sitting :\", tweet)\n    tweet = re.sub(r\"Multiplayer:\", \"Multiplayer :\", tweet)\n    tweet = re.sub(r\"Kaduna:\", \"Kaduna :\", tweet)\n    tweet = re.sub(r\"favorite:\", \"favorite :\", tweet)\n    tweet = re.sub(r\"home:\", \"home :\", tweet)\n    tweet = re.sub(r\"just:\", \"just :\", tweet)\n    tweet = re.sub(r\"Collision-1141\", \"Collision - 1141\", tweet)\n    tweet = re.sub(r\"County:\", \"County :\", tweet)\n    tweet = re.sub(r\"Duty:\", \"Duty :\", tweet)\n    tweet = re.sub(r\"page:\", \"page :\", tweet)\n    tweet = re.sub(r\"Attack:\", \"Attack :\", tweet)\n    tweet = re.sub(r\"Minecraft:\", \"Minecraft :\", tweet)\n    tweet = re.sub(r\"wounds;\", \"wounds ;\", tweet)\n    tweet = re.sub(r\"Shots:\", \"Shots :\", tweet)\n    tweet = re.sub(r\"shots:\", \"shots :\", tweet)\n    tweet = re.sub(r\"Gunfire:\", \"Gunfire :\", tweet)\n    tweet = re.sub(r\"hike:\", \"hike :\", tweet)\n    tweet = re.sub(r\"Email:\", \"Email :\", tweet)\n    tweet = re.sub(r\"System:\", \"System :\", tweet)\n    tweet = re.sub(r\"Radio:\", \"Radio :\", tweet)\n    tweet = re.sub(r\"King:\", \"King :\", tweet)\n    tweet = re.sub(r\"upheaval:\", \"upheaval :\", tweet)\n    tweet = re.sub(r\"tragedy;\", \"tragedy ;\", tweet)\n    tweet = re.sub(r\"HERE:\", \"HERE :\", tweet)\n    tweet = re.sub(r\"terrorism:\", \"terrorism :\", tweet)\n    tweet = re.sub(r\"police:\", \"police :\", tweet)\n    tweet = re.sub(r\"Mosque:\", \"Mosque :\", tweet)\n    tweet = re.sub(r\"Rightways:\", \"Rightways :\", tweet)\n    tweet = re.sub(r\"Brooklyn:\", \"Brooklyn :\", tweet)\n    tweet = re.sub(r\"Arrived:\", \"Arrived :\", tweet)\n    tweet = re.sub(r\"Home:\", \"Home :\", tweet)\n    tweet = re.sub(r\"Earth:\", \"Earth :\", tweet)\n    tweet = re.sub(r\"three:\", \"three :\", tweet)\n    \n    # Hashtags and usernames\n    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n    tweet = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", tweet)\n    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n    \n    return tweet","5d7fc484":"from wordcloud import STOPWORDS\n\nstopwords = [\"they've\", 'k', 'whom', \"he'll\", 'could', 'itself', 'hence',  \"when's\", 'where', 'through',\n             'was', 'its', 'into', 'however', \"she'd\", \"we'd\", 'they', 'below', 'again', \"she'll\", \"he's\",\n             'did', 'my', 'are', 'our', \"where's\", 'above', 'ever', 'yourself', \"i'd\", 'just', 'we', \"i'll\",\n             'it', 'from', \"we'll\", 'that', 'he', 'cannot', \"you're\", 'her', 'this', \"why's\", 'once',\n             'am', 'ourselves', 'out', 'get', 'would', 'up', \"it's\", 'same', 'these',  \"you've\", 'such', 'between',\n             'himself', 'also', \"that's\", 'r', 'www', 'all', \"what's\", 'if', 'http', 'herself',   'after', 'had',\n             'has', 'your', 'while', 'other', 'their', 'shall', 'more', 'off', 'as', 'hers', 'with',   'over',\n             'by',  'there', \"we're\", \"they'll\", 'any', 'to',  'no',  'about',  'both', \"he'd\", 'only', 'here', \n             'than', 'what', 'been', 'does', \"we've\", 'theirs', 'being', 'ought', \"they'd\", 'few', 'you', 'under', \n             'since',  'can', 'them', 'at', 'else', 'each', 'ours', 'therefore', 'most', 'before', 'then', 'his', 'me',\n             'a', 'further', \"how's\", 'during', 'of', 'like', 'on',  'themselves', 'why', 'those', 'in', 'too', 'she',\n             'because', \"i've\", \"let's\", 'how', 'very', \"you'd\", 'own', 'but', \"she's\", 'i', 'yourselves', 'down', 'should',\n             'and', 'do', 'or', 'were', 'some', 'an', \"who's\", 'otherwise', 'be', 'him', 'myself', 'have', 'which', \"there's\",\n             \"i'm\", 'when', 'doing', \"you'll\", 'com', 'for', 'who', 'yours', 'until', 'the', 'is', 'so', \"they're\", \"here's\", \n             'nor', 'having', 'will', 'may', 'one', 'now']","ac97e6d5":"def preprocessor2(text): \n    text = text.replace('%20',' ')\n    text = text.lower()\n    text = text.replace(\"n't\",\"nt\")\n    text = re.sub(r\"\\s\\w\\s\", \"\", text)\n    text = re.sub(r\"\\s\\d\\s\", \"\", text)\n    #emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n    #text = (re.sub('[^a-zA-Z0-9_]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n    return text","7a3dd2ae":"a= \"i don't like you 2 a r!\"\na=preprocessor2(a)\nprint(a)","b7aa0c2f":"train_df['text'] = train_df['text'].apply(lambda x:remove_url(x))\ntrain_df['keyword_cleaned'] = train_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df['location_cleaned'] = train_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df['text_cleaned'] = train_df['text'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","d3832874":"test_df['text'] = test_df['text'].apply(lambda x:remove_url(x))\ntest_df['keyword_cleaned'] = test_df['keyword'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['location_cleaned'] = test_df['location'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['text_cleaned'] = test_df['text'].copy().apply(lambda x : clean(str(x))).apply(lambda x : preprocessor2(x)).apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","e639fe9d":"train_df.head()","6d96ad85":"test_df.head()","0597b44d":"# Target count\nfig, ax = plt.subplots(figsize = (8,5))\npd.value_counts(train_df['target']).plot(kind=\"bar\")\nax.set_title('Target Count')\nax.set_ylabel('Frequency')\nax.grid(True)\nplt.show()","700ae794":"#Top 20 Locations with most target occurance\nloc_pos = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != '') & (train_df['target'] == 1)]['location_cleaned'].value_counts()\nloc_neg = train_df[(train_df.location_cleaned != 'nan') & (train_df.location_cleaned != '') & (train_df['target'] == 0)]['location_cleaned'].value_counts()\n\nloc_pos_dict = loc_pos[:20].to_dict()\nloc_neg_dict = loc_neg[:20].to_dict()\n\nnames0 = list(loc_neg_dict.keys())\nvalues0 = list(loc_neg_dict.values())\nnames1 = list(loc_pos_dict.keys())\nvalues1 = list(loc_pos_dict.values())\n\n#Graph\nfig, (ax1, ax2) = plt.subplots(figsize = (20,5), nrows=1, ncols=2)\n\nax1.bar(range(len(loc_pos_dict)),values1,tick_label=names1)\nax1.set_xticklabels(names1, rotation=\"vertical\")\nax1.set_ylim(0, 100)\nax1.grid(True)\nax1.set_title('Location with most Pos target')\nax1.set_ylabel('Frequency')\n\nax2.bar(range(len(loc_neg_dict)),values0,tick_label=names0)\nax2.set_xticklabels(names0, rotation=\"vertical\")\nax2.set_ylim(0, 100)\nax2.grid(True)\nax2.set_title('Location with most Neg target')\nax2.set_ylabel('Frequency')","e2519d99":"#As we can see some same meaning words using different abbreviation, so that we try to make a function to align these words\ndef preprocessor3(text):\n    text = re.sub(r'^washington d c ', \"washington dc\", text)\n    text = re.sub(r'^washington +[\\w]*', \"washington dc\", text)\n    text = re.sub(r'^new york +[\\w]*', \"new york\", text)\n    text = re.sub(r'^nyc$', \"new york\", text)\n    text = re.sub(r'^chicago +[\\w]*', \"chicago\", text)\n    text = re.sub(r'^california +[\\w]*', \"california\", text)\n    text = re.sub(r'^los angeles +[\\w]*', \"los angeles\", text)\n    text = re.sub(r'^san francisco +[\\w]*', \"san francisco\", text)\n    text = re.sub(r'^london +[\\w]*', \"london\", text)\n    text = re.sub(r'^usa$', \"united states\", text)\n    text = re.sub(r'^us$', \"united states\", text)\n    text = re.sub(r'^uk$', \"united kingdom\", text)\n    \n    return text\n\ndef preprocessor4(text):\n    abb = ['ak', 'al', 'az', 'ar', 'ca', 'co',\n           'ct', 'de', 'dc', 'fl', 'ga', 'hi',\n           'id', 'il', 'in', 'ia', 'ks', 'ky',\n           'la', 'me', 'mt', 'ne', 'nv', 'nh',\n           'nj', 'nm', 'ny', 'nc', 'nd', 'oh',\n           'ok', 'or', 'md', 'ma', 'mi', 'mn',\n           'ms', 'mo', 'pa', 'ri', 'sc', 'sd',\n           'tn', 'tx', 'ut', 'vt', 'va', 'wa',\n           'wv', 'wi', 'wy']\n    \n    for i in abb:\n        text = re.sub(r'^{0}$'.format(i), '', text)\n        \n    return text ","3b89aedd":"train_df['location_cleaned'] = train_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\ntrain_df['text_cleaned'] = train_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\n\ntest_df['location_cleaned'] = test_df['location_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))\ntest_df['text_cleaned'] = test_df['text_cleaned'].copy().apply(lambda x : preprocessor3(x)).apply(lambda x : preprocessor4(x))","3c3900b6":"loc_pos_dict = loc_pos[:].to_dict()\nloc_neg_dict = loc_neg[:].to_dict()\n\nloc_list = list(loc_pos_dict.keys()) + list(loc_neg_dict.keys())\nunique_loc = []\nlower_loc = []\nfor x in loc_list:\n    if x not in unique_loc:\n        unique_loc.append(x)\n        \nfor x in unique_loc:\n    lower_loc.append(x.lower().replace(',','').replace('.',''))","f1117ffd":"from wordcloud import WordCloud,ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#stopwords = list(STOPWORDS)+['will','may','one','now','nan','don'] #+ lower_loc\n\nclass wc_base2:\n    def __init__(self, data):\n        self.temp = data.apply(lambda x: ' '.join([word for word in x.split()]))\n        self.text = \" \".join(word for word in data)\n        self.wordlist = []\n        \n    def plot_wc(self, mask=None, max_words=200, figure_size=(20,10), title=None, stopwords=stopwords):\n\n        print (\"There are {} words in the combination of all review.\".format(len(self.text)))\n\n        wordcloud = WordCloud(background_color='black',\n                        stopwords=stopwords,\n                        max_words = max_words,\n                        collocations=False,\n                        random_state = 10,\n                        width = 800,\n                        height =400)\n\n        wordcloud.generate(self.text)\n         \n        self.wordlist = list(wordcloud.words_.keys())\n\n        plt.figure(figsize=figure_size)\n        plt.imshow(wordcloud)\n        plt.title(title)\n        plt.axis(\"off\")","996cc62a":"print(stopwords)","63e80f76":"wc = wc_base2(train_df[train_df.target == 0].text_cleaned)\nwc.plot_wc(title=\"Word Cloud of tweets with Negative target\")\nwc_s = set(wc.wordlist)","6f5f6483":"wc2 = wc_base2(train_df[train_df.target == 1].text_cleaned)\nwc2.plot_wc(title=\"Word Cloud of tweets with Positive target\")\nwc2_s = set(wc2.wordlist)","543fd632":"train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df['text_cleaned'] = test_df['text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))","fef059bd":"train_df.text_cleaned.head()","2208d82e":"train_df['index_no'] = train_df.index\ntrain_df['sent_w_index'] = train_df['text_cleaned'] + ' ' + train_df['index_no'].astype('str')","407b2883":"train_df.head()","9076c2ce":"train_df.sent_w_index[1]","31c8465c":"def random_swap(text):\n    text_list = text.split()\n    seed = int(text_list[-1])\n\n    text_list=text_list[:-1]\n    text_length = len(text_list)\n   \n    np.random.seed(seed)\n    try:\n        a = np.random.randint(0, text_length,size=2)\n    except:\n        return\n    #print(a)\n\n    temp_a = text_list[a[0]]\n    temp_b = text_list[a[1]]\n    \n    text_list[a[0]] = temp_b\n    text_list[a[1]] = temp_a\n    \n    redo = ' '.join([str(i) for i in text_list])\n   \n    return redo    \n\ndef random_del(text):\n    text_list = text.split()\n    seed = int(text_list[-1])\n\n    text_list=text_list[:-1]\n    text_length = len(text_list)\n   \n    np.random.seed(seed)\n    try:\n        a = np.random.randint(0, text_length,size=1)\n    except:\n        return\n    \n    text_list.pop((a[0]))\n    \n    redo = ' '.join([str(i) for i in text_list])\n      \n    return redo    ","286f80c9":"train_df['da_text_cleaned'] = train_df['sent_w_index'].apply(lambda x:random_swap(x))\ntrain_df['da_text_cleaned2'] = train_df['sent_w_index'].apply(lambda x:random_del(x))","4a22536f":"train_df.drop(['index_no','sent_w_index'], axis=1,inplace=True)","eee0a576":"train_df.head()","9ac84a57":"#temp_df1 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned))\n#temp_df2 = list(zip(train_df.target,train_df.keyword_cleaned,train_df.location_cleaned,train_df.da_text_cleaned2))","08cd2a1f":"#x = pd.DataFrame(temp_df1, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n#y = pd.DataFrame(temp_df2, columns =['target','keyword_cleaned','location_cleaned','text_cleaned'])\n\n#train_df.drop(['da_text_cleaned','da_text_cleaned2'], axis=1,inplace=True)\n\n#z = pd.concat([train_df,x,y], axis=0, join='outer', ignore_index=False, keys=None, sort = False)\n#z = z[['id','keyword_cleaned','location_cleaned','text_cleaned','target']].copy()\n#z.reset_index(inplace=True, drop=True) ","65e0bb3f":"#train_df = z \ntest_df = test_df[['id','keyword_cleaned','location_cleaned','text_cleaned']].copy()","b4344a1d":"#Randomization\nstate = 1\ntrain_df = train_df.sample(frac=1,random_state=state)\ntrain_df.reset_index(inplace=True, drop=True) ","a1c5eafb":"train_df['text_cleaned'] = train_df['text_cleaned'].apply(lambda x : str(x))","cfb6b078":"train_df.info()","9b8934e9":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\ntop_word = 35000\n\ntext_lengths = [len(x.split()) for x in (train_df.text_cleaned)]\n#text_lengths = [x for x in text_lengths if x < 50]\nplt.hist(text_lengths, bins=25)\nplt.title('Histogram of # of Words in Texts')\n\ntok = Tokenizer(num_words=top_word)\ntok.fit_on_texts((train_df['text_cleaned']+train_df['keyword_cleaned']+train_df['location_cleaned']))\n\nmax_words = max(text_lengths) + 1\nmax_words_ky = max([len(x.split()) for x in (train_df.keyword_cleaned)]) + 1\nmax_words_lc = max([len(x.split()) for x in (train_df.location_cleaned)]) + 1\nprint(\"top_word: \", str(top_word))\nprint(\"max_words: \", str(max_words))\nprint(\"max_words_ky: \", str(max_words_ky))\nprint(\"max_words_lc: \", str(max_words_lc))","d23d051a":"#Training set\n\nX_train_tx = tok.texts_to_sequences(train_df['text_cleaned'])\nX_train_ky = tok.texts_to_sequences(train_df['keyword_cleaned'])\nX_train_lc = tok.texts_to_sequences(train_df['location_cleaned'])\n\nX_test_tx = tok.texts_to_sequences(test_df['text_cleaned'])\nX_test_ky = tok.texts_to_sequences(test_df['keyword_cleaned'])\nX_test_lc = tok.texts_to_sequences(test_df['location_cleaned'])\n\n\nY_train = train_df['target']\n\nprint('Found %s unique tokens.' % len(tok.word_index))","a6b6a4a9":"from keras.utils import to_categorical\n# One-hot category\nY_train = to_categorical(Y_train)\nprint(\"Y_train.shape: \", Y_train.shape)","b34b81ec":"X_train_tx = sequence.pad_sequences(X_train_tx, maxlen=max_words)\nX_train_ky = sequence.pad_sequences(X_train_ky, maxlen=max_words_ky)\nX_train_lc = sequence.pad_sequences(X_train_lc, maxlen=max_words_lc)\n\nX_test_tx = sequence.pad_sequences(X_test_tx, maxlen=max_words)\nX_test_ky = sequence.pad_sequences(X_test_ky, maxlen=max_words_ky)\nX_test_lc = sequence.pad_sequences(X_test_lc, maxlen=max_words_lc)\n\nprint(\"X_train_tx.shape: \", X_train_tx.shape)\nprint(\"X_train_ky.shape: \", X_train_ky.shape)\nprint(\"X_train_lc.shape: \", X_train_lc.shape)","d7a4a468":"from numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\nglove_file = open('\/kaggle\/input\/glove6b\/glove.6B.300d.txt', encoding=\"utf8\")","96854a74":"for line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    \n    embeddings_dictionary [word] = vector_dimensions\n\nglove_file.close()","d9557ab8":"embedding_dim = 300\nembedding_matrix = zeros((top_word, embedding_dim))\nfor word, index in tok.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","294f3ef7":"from keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, Embedding, LSTM, Input, Conv2D, MaxPooling2D,Conv1D,MaxPooling1D\nfrom keras.layers import Bidirectional,  Reshape, Flatten, GRU\nfrom keras.layers.merge import concatenate","db7e616a":"input1 = Input(shape=(max_words,))\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words, trainable=False)(input1)\ndropout1 = Dropout(0.2)(embedding_layer1)\nlstm1_1 = LSTM(128,return_sequences = True)(dropout1)\nlstm1_2 = LSTM(128,return_sequences = True)(lstm1_1)\nlstm1_2a = LSTM(128,return_sequences = True)(lstm1_2)\nlstm1_3 = LSTM(128)(lstm1_2a)\n\ninput2 = Input(shape=(max_words_ky,))\nembedding_layer2 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_ky, trainable=False)(input2)\ndropout2 = Dropout(0.2)(embedding_layer2)\nlstm2_1 = LSTM(64,return_sequences = True)(dropout2)\nlstm2_2 = LSTM(64,return_sequences = True)(lstm2_1)\nlstm2_3 = LSTM(64)(lstm2_2)\n\ninput3 = Input(shape=(max_words_lc,))\nembedding_layer3 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_lc, trainable=False)(input3)\ndropout3 = Dropout(0.2)(embedding_layer3)\nlstm3_1 = LSTM(32,return_sequences = True)(dropout3)\nlstm3_2 = LSTM(32,return_sequences = True)(lstm3_1)\nlstm3_3 = LSTM(32)(lstm3_2)\n\nmerge = concatenate([lstm1_3, lstm2_3,lstm3_3])\n\ndropout = Dropout(0.8)(merge)\ndense1 = Dense(256, activation='relu')(dropout)\ndense2 = Dense(128, activation='relu')(dense1)\noutput = Dense(2, activation='softmax')(dense2)\nmodel1 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel1.summary()","0a16b002":"input1 = Input(shape=(max_words,))\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words, trainable=False)(input1)\nlstm1_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer1)\nlstm1_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm1_1)\nlstm1_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm1_1a)\nres = Reshape((-1, X_train_tx.shape[1], 100))(lstm1_1b)\nconv1 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res)\npool1 = MaxPooling2D(pool_size=(2,2))(conv1)\nflat1 = Flatten()(pool1)\n\ninput2 = Input(shape=(max_words_ky,))\nembedding_layer2 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_ky, trainable=False)(input2)\nlstm2_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer2)\nlstm2_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm2_1)\nlstm2_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm2_1a)\nres2 = Reshape((-1, X_train_ky.shape[1], 100))(lstm2_1b)\nconv2 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res2)\npool2 = MaxPooling2D(pool_size=(2,2))(conv2)\nflat2 = Flatten()(pool2)\n\ninput3 = Input(shape=(max_words_lc,))\nembedding_layer3 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=max_words_lc, trainable=False)(input3)\nlstm3_1 = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(embedding_layer3)\nlstm3_1a = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm3_1)\nlstm3_1b = Bidirectional(LSTM(100, return_sequences=True,dropout = 0.2))(lstm3_1a)\nres3 = Reshape((-1, X_train_lc.shape[1], 100))(lstm3_1b)\nconv3 = Conv2D(100, (3,3), padding='same',activation=\"relu\")(res3)\npool3 = MaxPooling2D(pool_size=(2,2))(conv3)\nflat3 = Flatten()(pool3)\n\nmerge = concatenate([flat1, flat2, flat3])\n\ndropout = Dropout(0.4)(merge)\ndense1 = Dense(256, activation='relu')(dropout)\ndense2 = Dense(128, activation='relu')(dense1)\noutput = Dense(2, activation='softmax')(dense2)\nmodel2 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel2.summary()","4cea1003":"input1 = Input(shape=(max_words,))\ninput2 = Input(shape=(max_words_ky,))\ninput3 = Input(shape=(max_words_lc,))\n\nmerge = concatenate([input1, input2, input3])\n\nembedding_layer1 = Embedding(top_word, embedding_dim, weights=[embedding_matrix], input_length=42, trainable=False)(merge)\nlstm1_1 = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(embedding_layer1)\nlstm1_1a = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1)\n#lstm1_1b = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1a)\nlstm1_1b = Bidirectional(LSTM(128, return_sequences=True,dropout = 0.2))(lstm1_1a)\n\n#res2 = Reshape((-1, 40, 256))(lstm1_1b)\nconv2 = Conv1D(64, 3, padding='same',activation=\"relu\")(lstm1_1b)\npool2 = MaxPooling1D(pool_size=2)(conv2)\nconv3 = Conv1D(64, 3, padding='same',activation=\"relu\")(pool2)\npool3 = MaxPooling1D(pool_size=2)(conv3)\nflat2 = Flatten()(pool3)\n\ndense1 = Dense(256, activation='relu')(flat2)\ndropout = Dropout(0.8)(dense1)\ndense2 = Dense(128, activation='relu')(dropout)\noutput = Dense(2, activation='softmax')(dense2)\nmodel3 = Model(inputs=[input1,input2,input3], outputs=output)\nmodel3.summary()","23dab2f9":"from keras.optimizers import Adam","964c8de7":"optimizer1 = Adam(lr = .0001, beta_1 = .9, beta_2 = .999, epsilon = 1e-10, decay = .0, amsgrad = False)","8a11ca14":"optimizer2 = Adam(lr = .0001, beta_1 = .9, beta_2 = .999, epsilon = 1e-10, decay = .0, amsgrad = False)","d27a9882":"model1.compile(loss=\"binary_crossentropy\", optimizer=optimizer1,\n              metrics=[\"accuracy\"])","83cee61c":"model2.compile(loss=\"binary_crossentropy\", optimizer=optimizer2,\n              metrics=[\"accuracy\"])","48584f80":"from keras.callbacks import EarlyStopping, ReduceLROnPlateau\nes = EarlyStopping(monitor='val_loss', mode='min',verbose=1, patience = 4)\nlearning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy', patience = 2, verbose = 1, \n                                           factor = 0.5, min_lr = 1e-8, cooldown=1)\n#history = model.fit([X_train_tx,X_train_ky], Y_train, validation_split=0.2, epochs=30, batch_size=64, verbose=2, callbacks=[es])\nhistory = model2.fit([X_train_tx,X_train_ky,X_train_lc], Y_train, validation_split=0.2, epochs=20, batch_size=16, verbose=2, callbacks=[es, learning_rate_reduction])","f2d1d278":"history2 = model1.fit([X_train_tx,X_train_ky,X_train_lc], Y_train, validation_split=0.2, epochs=20, batch_size=16, verbose=2, callbacks=[es, learning_rate_reduction])","cf56ad87":"def result_eva (loss,val_loss,acc,val_acc):\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    \n    epochs = range(1,len(loss)+1)\n    plt.plot(epochs, loss,'b-o', label ='Training Loss')\n    plt.plot(epochs, val_loss,'r-o', label ='Validation Loss')\n    plt.title(\"Training and Validation Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    epochs = range(1, len(acc)+1)\n    plt.plot(epochs, acc, \"b-o\", label=\"Training Acc\")\n    plt.plot(epochs, val_acc, \"r-o\", label=\"Validation Acc\")\n    plt.title(\"Training and Validation Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","39a3b7fd":"result_eva(history.history['loss'], history.history['val_loss'], history.history['accuracy'], history.history['val_accuracy'])","138f33bc":"result_eva(history2.history['loss'], history2.history['val_loss'], history2.history['accuracy'], history2.history['val_accuracy'])","9391ca64":"model2.save('nlp_disaster.h5')","9079ace4":"model1.save('nlp_disaster2.h5')","18a73adf":"from keras.models import load_model\n\nmodel = Model()\nmodel = load_model('nlp_disaster.h5')\n#model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])","26c2437f":"#Y_pred = model.predict([X_test_tx,X_test_ky], batch_size=64, verbose=2)\nY_pred = model.predict([X_test_tx,X_test_ky,X_test_lc], batch_size=16, verbose=2)\nY_pred = np.argmax(Y_pred,axis=1)\n\npred_df = pd.DataFrame(Y_pred, columns=['target'])\nresult = pd.concat([test_df,pred_df], axis=1, join='outer', ignore_index=False, keys=None, sort = False)\nresult = result[['id','target']]\nprint(Y_pred)","9e607d63":"result.to_csv('sample_submission.csv',index=False)","fe46445f":"Y_pred = model1.predict([X_test_tx,X_test_ky,X_test_lc], batch_size=16, verbose=2)\nY_pred = np.argmax(Y_pred,axis=1)\n\npred_df = pd.DataFrame(Y_pred, columns=['target'])\nresult = pd.concat([test_df,pred_df], axis=1, join='outer', ignore_index=False, keys=None, sort = False)\nresult = result[['id','target']]\nprint(Y_pred)","c8dde3ae":"result.to_csv('sample_submission2.csv',index=False)","61d35839":"## 3. Understanding of the training dataset - basic","7b23fac9":"## 4. Understanding of the training dataset - word cloud visualization","ac8a0232":"## **0: Library and function import**","668ada3d":"## ** 2: Data Preprocessing**","a0d4cb8a":"### Improved Model -LSTM+CNN - action in keywords, locations and texts","357a9298":"## **1: Data Loading**","c72a3ecc":"## 5.1 Build a Simple deep learning model  ","2dc09dd6":"### Standard Model - action in keywords, locations and texts","a7c094e7":"## 5.0 Data Augmentation -- EDA (Random  Swap - RS) and EDA (Random  Deletion - RD) - for training set"}}