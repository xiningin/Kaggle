{"cell_type":{"dba06fc8":"code","a5a12819":"code","9a0dce72":"code","863bcd08":"code","b6751e09":"code","5cb5092f":"code","7c8c5d36":"code","557e8022":"code","c6d3f61f":"code","c15853d6":"code","2e597b87":"code","ad139f9c":"code","f16ac00b":"code","fc1cdf69":"code","8fa3c01f":"code","0f02797f":"code","11b31f16":"code","d363b873":"code","752afb23":"code","6514206d":"code","efe10bff":"code","0b7c8ea6":"code","c99ab7e6":"code","a8d5dfbc":"code","bdc2ad89":"code","e7eff60c":"code","a1f37247":"code","44b02e41":"code","e18f98fe":"code","0d27a873":"code","945c21d8":"code","a36ed0a7":"code","3a459103":"code","6500d8b1":"code","5d1dcc3a":"code","0a6c218e":"code","b3d7f131":"code","03507e8d":"code","f04aa7e8":"code","ce30fcb2":"code","f9956791":"code","61f2485e":"code","668e8e18":"code","2144f140":"code","ddfb4651":"code","2bc807d2":"code","c78dd706":"code","91a4afeb":"code","1c96fd5a":"code","541a5cbf":"code","eeb43cec":"code","26aa12f4":"code","dea07cd0":"code","42b72ecf":"code","1179b691":"code","af782eeb":"code","b2097da0":"code","8232b41d":"code","da4f2f4c":"code","f029c1a7":"code","a9158e54":"code","32efc915":"code","0b5be80a":"code","ee2c79b9":"code","70e8b073":"code","b7b4f961":"code","4cc75a75":"code","253b50ab":"code","46e7b5be":"code","465e3dda":"code","48475c84":"code","98775da1":"code","7c56f424":"code","56dc0c4d":"code","9829bb94":"code","639f4235":"code","54852b20":"code","88d852f6":"code","ec2b556b":"code","fc538cfd":"code","c1180c18":"code","dacdc4c8":"code","835ab664":"code","15418727":"code","d238825f":"code","e173bd3f":"code","3851bc66":"code","57a4428d":"code","4512fef5":"code","fc2c6d49":"code","ebb039a4":"code","bc543483":"code","70a75e9f":"code","2ec2ed6f":"code","7bf8c359":"code","64b9d395":"code","8d847315":"code","713c1f76":"code","6c219beb":"code","44b501e9":"code","83223543":"code","6e26c26b":"code","fdad1e38":"code","7d6a9c9d":"code","4b81daf8":"code","05b7f5a8":"code","b8d68b3e":"markdown","b6977236":"markdown","b291b6f4":"markdown","906b2eb5":"markdown","aa0b647e":"markdown","bd44d5e8":"markdown","9e9af0f5":"markdown","88cec42b":"markdown","1abccbe1":"markdown","2046c989":"markdown","cb7e2a7b":"markdown","046bfe2d":"markdown","fa51eeef":"markdown","6ad86e86":"markdown","44042fff":"markdown","985214a4":"markdown","1b0094de":"markdown","6c4de2d3":"markdown","dd7dba16":"markdown","9a1cc6de":"markdown","9ae101c0":"markdown","f9970273":"markdown","bd230187":"markdown","020a2142":"markdown","3a051a51":"markdown","a6244f95":"markdown","6ad103af":"markdown","088c7519":"markdown","dd36c60d":"markdown","20ab4dcf":"markdown","1e4c6c49":"markdown","4e24512d":"markdown","0ec86388":"markdown"},"source":{"dba06fc8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nimport math\n\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import ensemble, tree, linear_model\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport missingno as msno\n\n#Model Train\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom lightgbm import LGBMRegressor\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5a12819":"os.listdir('\/kaggle\/input\/house-prices-advanced-regression-techniques')","9a0dce72":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","863bcd08":"train.shape, test.shape","b6751e09":"train.info()","5cb5092f":"train.isnull().sum().sum()","7c8c5d36":"train.describe()","557e8022":"train.describe(include='O')","c6d3f61f":"def column_unique(col_list):\n    for column_name in train.columns:\n        if train[column_name].nunique() < 35 and train[column_name].dtypes == 'int64':\n            unique_category = len(train[column_name].unique())\n            print(f'Feature {column_name} with dtype discrete has {unique_category} unique categories')\n        elif train[column_name].dtypes == 'object':\n            unique_category = len(train[column_name].unique())\n            print(f'Feature {column_name} with dtype object has {unique_category} unique categories')\n        else:\n            dtype = train[column_name].dtypes\n            print(f'Feature {column_name} is of dtype {dtype}')","c15853d6":"column_unique(train.columns)","2e597b87":"# for column_name in train.columns:\n#     if train[column_name].nunique() < 35 and train[column_name].dtypes == 'int64':\n#         unique_category = len(train[column_name].unique())\n#         print(\"Feature '{column_name}' has '{unique_category}' unique categories\".format(column_name = column_name,\n#                                                                                          unique_category=unique_category))\n# for column_name in train.columns:\n#     if train[column_name].dtypes == 'object':\n#         unique_category = len(train[column_name].unique())\n#         print(\"Feature '{column_name}' has '{unique_category}' unique categories\".format(column_name = column_name,\n#                                                                                          unique_category=unique_category))\n# for column_name in test.columns:\n#     if test[column_name].dtypes == 'object':\n#         unique_category = len(test[column_name].unique())\n#         print(\"Features in test set '{column_name}' has '{unique_category}' unique categories\".format(column_name = column_name, unique_category=unique_category))","ad139f9c":"# Making Lists of different dataframe types, categorising by categorical, discrete and numerical \n# functions takes dataframe as an input and returns three lists of each type\n# access using indexes\n# made it just for fun\n\n# def feature_type_identifier(df):\n#     cat_cols = train.select_dtypes('object').columns\n#     dis_cols = [feature for feature in train.columns if train[feature].nunique() < 25 and train[feature].dtypes == 'int64' ]\n#     num_cols = [feature for feature in train.columns if train[feature].nunique() > 25]\n#     return cat_cols, dis_cols, num_cols","f16ac00b":"cat_cols = list(train.select_dtypes('object').columns)\ndis_cols = [feature for feature in train.columns if train[feature].nunique() < 25 and train[feature].dtypes == 'int64' ]\nnum_cols = [feature for feature in train.columns if train[feature].nunique() > 25]","fc1cdf69":"train[cat_cols].isnull().sum()","8fa3c01f":"def missing_data(df):\n    total = df.isnull().sum()\n    percent = round(df.isnull().sum() \/ df.shape[0]* 100)\n    missing_info = pd.concat([total, percent], axis = 1, keys=['Total', 'Percent']).sort_values(by='Percent', ascending=False)\n    missing_info = missing_info[missing_info['Total'] > 0]\n    return missing_info","0f02797f":"missing_data(train)","11b31f16":"msno.matrix(train)","d363b873":"msno.bar(train)","752afb23":"msno.heatmap(train)","6514206d":"def plotting_charts_hist_qq_boxp(df, feature):\n    style.use('fivethirtyeight')\n    fig = plt.figure(constrained_layout=True, figsize= (12,8))\n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    ax1 = fig.add_subplot(grid[0,:2])\n    # Histogram\n    ax1.set_title('Histogram')\n    sns.distplot(df[feature], norm_hist=True, ax=ax1)\n    # QQ Plot\n    ax2 = fig.add_subplot(grid[1, :2])\n    ax2.set_title('QQ_PLOT')\n    stats.probplot(df[feature], plot= ax2)\n    ## Boxplot\n    ax3 = fig.add_subplot(grid[:,2])\n    ax3.set_title('Box Plot')\n    sns.boxplot(df[feature], orient='v', ax =ax3)\nplotting_charts_hist_qq_boxp(train, 'SalePrice')","efe10bff":"# Skewness\nprint('Skewness: {}'.format(train['SalePrice'].skew()))\nprint('Kurtosis: {}'.format(train['SalePrice'].kurt()))","0b7c8ea6":"train.corr()['SalePrice'].sort_values(ascending = False)","c99ab7e6":"# Visualizing Outliers\n# style.use('ggplot')\nfig, axes = plt.subplots(ncols=2, nrows=0, figsize =(12,120))\n# sns.color_palette(\"husl\", 8)\nplt.subplots_adjust(right=2)\n# plt.subplots_adjust(top=2)\nplot_list = (x for x in num_cols if x not in ['Id' , 'SalePrice'])\nfor i, feature in enumerate(plot_list, 1):\n    plt.subplot(len(num_cols), 3,i)\n    sns.scatterplot(x = 'SalePrice', y = feature, data=train)\n    plt.ylabel('{}'.format(feature), size=15)\n    plt.xlabel('SalePrice', size=15)\nplt.show()","a8d5dfbc":"# Correlation between SalePrice and numerical features\ntrain.corr()['SalePrice'][num_cols].sort_values(ascending = False)","bdc2ad89":"# Scatter Plot\ndef scatter_plotter(indep, dep):\n#     style.use('ggplot')\n    plt.subplots(figsize=(8,6))\n    sns.scatterplot(x = indep, y = dep)","e7eff60c":"scatter_plotter(train.GrLivArea, train.SalePrice)\ntrain.corr()['SalePrice']['GrLivArea']","a1f37247":"scatter_plotter(train.GarageArea, train.SalePrice)\ntrain.corr()['SalePrice']['GarageArea']","44b02e41":"scatter_plotter(train.TotalBsmtSF, train.SalePrice)\ntrain.corr()['SalePrice']['TotalBsmtSF']","e18f98fe":"fig, (ax1,ax2) = plt.subplots(figsize = (8,6), ncols=2, sharey=False)\nsns.scatterplot(train.GrLivArea, train.SalePrice, ax=ax1)\nsns.regplot(train.GrLivArea, train.SalePrice, ax=ax1)\nsns.scatterplot(train.MasVnrArea, train.SalePrice, ax=ax2)\nsns.regplot(train.MasVnrArea, train.SalePrice, ax=ax2)","0d27a873":"sns.residplot(train.MasVnrArea, train.SalePrice)","945c21d8":"train.corr()['SalePrice'][dis_cols].sort_values(ascending=False)","a36ed0a7":"\n# for feature in dis_cols:\n#     data=train.copy()\n#     data.groupby(feature)['SalePrice'].median().plot.bar()\n#     plt.xlabel(feature)\n#     plt.ylabel('SalePrice')\n#     plt.title(feature)\n#     plt.show()","3a459103":"# CountPlots\n\nfig, axes = plt.subplots(ncols=2, nrows=0, figsize =(12,240))\nsns.color_palette(\"husl\", 8)\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\nfor i, feature in enumerate(dis_cols+cat_cols, 1):\n    plt.subplot(len(dis_cols+cat_cols), 3,i)\n    sns.countplot(train[feature])\n    plt.xlabel(f'{feature}', size=15)\n    plt.ylabel('Count', size=15)\nplt.show()","6500d8b1":"# Categorised Mean\/Median\n\nfig, axes = plt.subplots(ncols=2, nrows=0, figsize =(12,120))\nsns.color_palette(\"husl\", 8)\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\nfor i, feature in enumerate(dis_cols+cat_cols, 1):\n    plt.subplot(len(dis_cols+cat_cols), 3,i)\n    train.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(f'{feature}', size=15)\n    plt.ylabel('Count', size=15)\nplt.show()","5d1dcc3a":"# Boxplots\n\nfig, axes = plt.subplots(ncols=2, nrows=0, figsize =(12,120))\nsns.color_palette(\"husl\", 8)\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\nfor i, feature in enumerate(dis_cols+cat_cols, 1):\n    plt.subplot(len(dis_cols+cat_cols), 3,i)\n    sns.boxplot(y = 'SalePrice', x = feature, data= train)\n    plt.xlabel(f'{feature}', size=15)\n    plt.ylabel('Count', size=15)\nplt.show()","0a6c218e":"object_df = pd.concat([train[cat_cols], train['SalePrice']], axis=1)","b3d7f131":"ix=1\nfig = plt.figure(figsize = (8,6))\nfor c in list(object_df.columns):\n    if ix <= 3:\n        if c != 'SalePrice':\n            ax1 = fig.add_subplot(2,3,ix)\n            sns.countplot(data = object_df, x=c, ax = ax1)\n            ax2 = fig.add_subplot(2,3,ix+3)\n            sns.boxplot(data=object_df, x=c, y='SalePrice', ax=ax2)\n#             sns.violinplot(data=object_df, x=c, y='SalePrice', ax=ax2)\n#             sns.swarmplot(data = object_df, x=c, y ='SalePrice', color = 'k', alpha = 0.4, ax=ax2)\n           \n    ix = ix +1\n    if ix == 4:\n        fig= plt.figure(figsize = (8,6))\n        ix =1","03507e8d":"# discrete_feature=[feature for feature in numeric_features if len(train[feature].unique())<25 and feature not in year_feature+['Id']]\n# print(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","f04aa7e8":"# HeatMap\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), \n            cmap=sns.diverging_palette(20, 220, n=200), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","ce30fcb2":"train_v1 = train.copy()","f9956791":"# train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n# train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n# train.reset_index(drop=True, inplace=True)","61f2485e":"train['SalePrice'] = np.log1p(train['SalePrice'])","668e8e18":"plotting_charts_hist_qq_boxp(train, 'SalePrice')","2144f140":"y = train['SalePrice'].reset_index(drop=True)\n## Remove Id and save target variable as y\ntrain = train.drop(['Id', 'SalePrice'], axis=1)\ntest = test.drop(['Id'], axis=1)","ddfb4651":"## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((train, test)).reset_index(drop = True)","2bc807d2":"all_data.shape","c78dd706":"missing_data(all_data)","91a4afeb":"missing_data(all_data)['Percent'].plot.bar(color=\"b\")","1c96fd5a":"for feature in ['MoSold', 'YrSold', 'MSSubClass']:\n    all_data[feature] = all_data[feature].apply(str)","541a5cbf":"all_data[['MoSold', 'YrSold', 'MSSubClass']].info()","eeb43cec":"# Assume typical unless deductions are warranted (from the data description)\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\n# Fillna with modes as these columns has very less missing data\nmode_feats = list(missing_data(all_data[cat_cols])[missing_data(all_data[cat_cols])['Total'] <2].index)\nfor feature in mode_feats:\n    all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])","26aa12f4":"missing_data(all_data[['Exterior2nd', 'Exterior1st', 'Electrical', 'KitchenQual', 'SaleType']])","dea07cd0":"all_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","42b72ecf":"## Some missing values are intentionally left blank, for example: In the Alley feature \n## there are blank values meaning that there are no alley's in that specific house. \nnone_available = [ \"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor feature in none_available:\n    all_data[feature] = all_data[feature].fillna('None')","1179b691":"none_available2 =  ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageYrBlt',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor feature in none_available2:\n    all_data[feature] = all_data[feature].fillna(0)","af782eeb":"## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))","b2097da0":"all_data[all_data['Utilities'].isnull()]","8232b41d":"all_data.Utilities.value_counts()","da4f2f4c":" missing_data(all_data)","f029c1a7":"all_data['Utilities'] = all_data['Utilities'].fillna(all_data['Utilities'].mode()[0])","a9158e54":"missing_data(all_data)","32efc915":"plot_list","0b5be80a":"for x in all_data.columns:\n    if all_data[x].dtype in ('int64', 'float64') and x not in ('Id', 'SalePrice'):\n        print(all_data[x].dtype)\n        print(x)","ee2c79b9":"plot_list = [x for x in all_data.columns if all_data[x].dtype in ('int64', 'float64')]","70e8b073":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_data[plot_list] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","b7b4f961":"len(all_data.dtypes[all_data.dtypes != 'object'].index)","4cc75a75":"skew_cols = all_data.dtypes[all_data.dtypes != 'object'].index\nskewness = all_data[skew_cols].apply(lambda x: skew(x)).sort_values(ascending =False)\nskewness = skewness[abs(skewness) > 0.5]\nhigh_skew = pd.DataFrame({'Skew' : skewness })\nhigh_skew_cols = high_skew.index","253b50ab":"skew(all_data.YearBuilt)","46e7b5be":"high_skew_cols","465e3dda":"high_skew_cols","48475c84":"# Normalize skewed features\nfor i in high_skew_cols:\n    all_data[i] = boxcox1p(all_data[i], boxcox_normmax(all_data[i] + 1))","98775da1":"all_data['OverallCond'].head()","7c56f424":"# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=all_data[plot_list] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","56dc0c4d":"# #Creating More Features\n\n# all_data['BsmtFinType1_Unf'] = 1*(all_data['BsmtFinType1'] == 'Unf')\n# all_data['HasWoodDeck'] = (all_data['WoodDeckSF'] == 0) * 1\n# all_data['HasOpenPorch'] = (all_data['OpenPorchSF'] == 0) * 1\n# all_data['HasEnclosedPorch'] = (all_data['EnclosedPorch'] == 0) * 1\n# all_data['Has3SsnPorch'] = (all_data['3SsnPorch'] == 0) * 1\n# all_data['HasScreenPorch'] = (all_data['ScreenPorch'] == 0) * 1\n# all_data['YearsSinceRemodel'] = all_data['YrSold'].astype(int) - all_data['YearRemodAdd'].astype(int)\n# all_data['Total_Home_Quality'] = all_data['OverallQual'] + all_data['OverallCond']\n# all_data = all_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n# all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n# all_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\n\n# all_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] +\n#                                  all_data['1stFlrSF'] + all_data['2ndFlrSF'])\n# all_data['Total_Bathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n#                                all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n# all_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n#                               all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n#                               all_data['WoodDeckSF'])\n# all_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n# all_data['2ndFlrSF'] = all_data['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n# all_data['GarageArea'] = all_data['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n# all_data['GarageCars'] = all_data['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n# all_data['LotFrontage'] = all_data['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n# all_data['MasVnrArea'] = all_data['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n# all_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\n# all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n# all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n# all_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n# all_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n# all_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","9829bb94":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\nall_data = logs(all_data, log_features)","639f4235":"all_data.shape","54852b20":"all_data = pd.get_dummies(all_data).reset_index(drop=True)\nall_data.shape","88d852f6":"# Remove any duplicated column names\nall_data = all_data.loc[:,~all_data.columns.duplicated()]","ec2b556b":"all_data.shape","fc538cfd":"train_clean = all_data.iloc[:len(y), :]\ntest_clean = all_data.iloc[len(y):, :]\ntrain_clean.shape, y.shape, test_clean.shape","c1180c18":"X_train, X_test, y_train, y_test = train_test_split(train_clean, y, train_size=0.75, shuffle=True, random_state=1)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","dacdc4c8":"train_check = train_clean.copy()","835ab664":"train_check['mean_sale_price'] = y_train.mean()","15418727":"# Fit and Predict on X_test\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)","d238825f":"print (f' Train Score is {lr.score(X_train, y_train)}')\nprint (f' Test Score is {lr.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {mse}')","e173bd3f":"alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]","3851bc66":"from sklearn.linear_model import Lasso \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    lasso_reg = Lasso(alpha= i, normalize=True)\n    ## fit the model. \n    lasso_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","57a4428d":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","4512fef5":"lasso_reg = Lasso(alpha=0.0001 , normalize=True)\n## fit the model. \nlasso_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = lasso_reg.predict(X_test)","fc2c6d49":"print (f' Train Score is {lasso_reg.score(X_train, y_train)}')\nprint (f' Test Score is {lasso_reg.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {mse}')","ebb039a4":"from sklearn.linear_model import Ridge \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    ridge_reg = Ridge(alpha= i, normalize=True)\n    ## fit the model. \n    ridge_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = ridge_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","bc543483":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","70a75e9f":"ridge_reg = Ridge(alpha=0.4 , normalize=True)\n## fit the model. \nridge_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = ridge_reg.predict(X_test)","2ec2ed6f":"print (f' Train Score is {ridge_reg.score(X_train, y_train)}')\nprint (f' Test Score is {ridge_reg.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {mse}')","7bf8c359":"kf = KFold(n_splits=12, random_state=42, shuffle=True)\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=train_clean):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","64b9d395":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","8d847315":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","713c1f76":"lgb_model_full_data = lightgbm.fit(train_clean, y)\nlgb_model_full_data.predict(test_clean)","6c219beb":"from sklearn.datasets import make_moons\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\n# Step1: Create data set\n# X, y = make_moons(n_samples=10000, noise=.5, random_state=0)\n# Step2: Split the training test set\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Step 3: Fit a Decision Tree model as comparison","44b501e9":"# Load Library\nclf = DecisionTreeRegressor()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_test, y_test)","83223543":"# Step 4: Fit a Random Forest model, \" compared to \"Decision Tree model, accuracy go up by 5%\nclf = RandomForestRegressor(n_estimators=100, max_features=\"auto\",random_state=0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_test, y_test)","6e26c26b":"# Step 5: Fit a AdaBoost model, \" compared to \"Decision Tree model, accuracy go up by 10%\nclf = AdaBoostRegressor(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_test, y_test)","fdad1e38":"# Step 6: Fit a Gradient Boosting model, \" compared to \"Decision Tree model, accuracy go up by 10%\nclf = GradientBoostingRegressor(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nclf.score(X_test, y_test)","7d6a9c9d":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.shape","4b81daf8":"submission.iloc[:,1] = np.floor(np.expm1(clf.predict(test_clean)))","05b7f5a8":"submission.to_csv(\"submission_GradientBoost.csv\", index=False)","b8d68b3e":"Focusing on Sales Price. Need to check if it follows a normal distribution so that we can easily fit into a multi regression model.","b6977236":"# **\ud83d\udcca A.Exploratory Data Analysis**","b291b6f4":"# C. Feature Engineering - Creating New Features","906b2eb5":"## 4. Missing Data Assessment","aa0b647e":"Test Codes","bd44d5e8":"# Handling Missing Values\nWill be using all_data from here on","9e9af0f5":"### Observations\n1. Target variable is not normally distributed\n2. Target Variable is highly right-skewed.\n3. There are outliers too.\n\nNote to Self : SalePrice needs tranformation","88cec42b":"# Submission","1abccbe1":"## 6. Bivariate Analaysis","2046c989":"## Ridge Regression\nL1 Regularization","cb7e2a7b":"Assumptions of Linear Regression\n1. Linearity \n2. Homoscedasticity\n3. Independence Errors\n4. Multivariate Normality\n5. No or little multicollinearity","046bfe2d":"Test Codes\n","fa51eeef":"## 1. General Data Statistics","6ad86e86":"Likely a case of overfitting as the model is performing better on Train Set but slightly less on test(validation) set","44042fff":"## Fixing Skewness","985214a4":"## Check for Regression Assumptions\nBy fitting a Regression Line","1b0094de":"## 3. Create dtype lists","6c4de2d3":"# B. Feature Engineering","dd7dba16":"Two distant values for GrLivArea showing outlier nature, should be deleted!","9a1cc6de":"Other Models","9ae101c0":"## 5. Univariate Analysis","f9970273":"## Creating Dummy Variables","bd230187":"## Ridge Regression\nL2 Regularization","020a2142":"### 6B. Discrete + Categorical Features Bivariate Analysis","3a051a51":"What heatmap gave me:\n 1. 1stFloorSF and TotalBsmtSF are highly correlated, both features depicts the same attribute basically. Take only one forward.\n 2. GarageArea and GarageCars are highly correlated, both shows the same feature about the house. Only take one forword.\n 3. TotRmAbvGrd and GrLivArea are highly correlated.\n ","a6244f95":"There you go, SalePrice looking off the charts. (Pun Intended)","6ad103af":"### Observations from above exercise:\n1. Our target variable shows an unequal level of variance across most predictor variables (for GrLivArea, TotalBsmtSF, 1stFloorSF, MasVnrArea). This phenomenon is called Heteroscedasticity and is a red flag for multiple linear regression. Will make the variable homoscedastic later on in df manipulation part.\n2. Outliers are present in GrLivArea, TotalBsmtSF, 1stFloorSF, MasVnrArea and should be handled.\n3. SalePrice vs. GrLivArea seem to follow a trend, which can be explained by saying that \"As the prices increased, so did the area.\n4. Will remove outliers from GrLivArea later in dataframe manipulation part.","088c7519":"## Linear Regression Model","dd36c60d":"## 2. Value counts for discrete and categorical features","20ab4dcf":"## 7. Multivariate Analysis","1e4c6c49":"### 6A. Numerical Features Bivariate Analysis","4e24512d":"# D. Modeling","0ec86388":"# \ud83d\udcd6 Read Files "}}