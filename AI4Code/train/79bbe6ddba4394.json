{"cell_type":{"20230e3d":"code","b6d43872":"code","e001539f":"code","1ef97ff5":"code","0b7deebc":"code","29954e89":"code","e152bdb0":"code","4152b78f":"code","d52de7eb":"code","52eebfe3":"code","50ba1e55":"code","894c44f9":"code","4c18653b":"code","ea180aca":"code","d83c99fa":"code","a8b2bd44":"code","11355efa":"code","5ba10499":"code","6051c2f0":"code","5cc0fb85":"markdown","816b895c":"markdown","975939f0":"markdown","879b5017":"markdown","79dfd72c":"markdown"},"source":{"20230e3d":"# Libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import SubsetRandomSampler","b6d43872":"images_gray = np.load('..\/input\/image-colorization\/l\/gray_scale.npy')\n#stack the ab images \nstacked_ab = np.concatenate([np.load('..\/input\/image-colorization\/ab\/ab\/ab1.npy') ,\n                             np.load('..\/input\/image-colorization\/ab\/ab\/ab2.npy') , \n                             np.load('..\/input\/image-colorization\/ab\/ab\/ab3.npy')] , axis=0)","e001539f":"class ColorDataSet(Dataset):\n    def __init__(self , idx_set ):\n        self.idx_set = idx_set\n        self.transform_rgb = transforms.Compose([\n            transforms.ToPILImage() ,\n            transforms.Resize(299),\n            transforms.CenterCrop(299),\n            transforms.ToTensor() , \n            transforms.Normalize(mean = [0.485, 0.456, 0.406] , std = [0.229, 0.224, 0.225])\n        ])\n        self.transofrm_ab = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(224),\n            transforms.CenterCrop(224),\n            transforms.ToTensor() ,\n            transforms.Normalize(mean = [0.5 , 0.5] , std = [0.5 , 0.5])\n        ])\n        self.transofrm_gray = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(224),\n            transforms.CenterCrop(224),\n            transforms.ToTensor() ,\n            transforms.Normalize(mean = [0.0 ] , std = [1.0 ])\n        ])\n        \n    def __len__(self):\n\n        return (len(self.idx_set))\n    \n    def __getitem__(self , index):\n        \"\"\"\n        1. obtain the lab and gray scale according to the index\n        2. remove the l channel from lab space\n        3. apply the tensor transformations\n        \n        \"\"\"\n        img_index = self.idx_set[index]\n        gray_img = images_gray[img_index]\n        gray_dup = np.copy(gray_img)\n        gray_rgb = cv2.cvtColor(gray_dup , cv2.COLOR_GRAY2RGB)\n        ab_img = stacked_ab[img_index]\n        #resize the lab space to the -1 and 1\n        ab_max = float(ab_img.max())\n        ab_min = float(ab_img.min())\n        #ab_img_transformed = 2*(ab_img - ab_min)\/(ab_max-ab_min+1.0) -1.0\n        ab_img_transformed = ab_img\n        #apply the tensor transformations\n        gray_tensor = self.transofrm_gray(gray_img)\n        ab_tensor = self.transofrm_ab(ab_img_transformed)\n        rgb_tensor = self.transform_rgb(gray_rgb)\n        \n        sample ={\n            'rgb_img' : rgb_tensor ,\n            'ab_img' : ab_tensor , \n            'gray_img': gray_tensor\n        }\n        \n        return sample","1ef97ff5":"\"\"\"\ndefine the train and validation data loaders with the randomsubsetsampler sampling\n\"\"\"\n\nidx_list = list(np.arange(len(images_gray)))\ntrain_val_split = 0.3\n#define the random seed \nnp.random.seed(0)\n#define the margin of split\nidx_split = int(train_val_split*len(images_gray))\n#shuffle the index list\nnp.random.shuffle(idx_list)\n\ntrain_idx = idx_list[idx_split:]\nval_idx = idx_list[:idx_split]\n\ntrain_set = ColorDataSet(idx_set=train_idx)\nvalid_set = ColorDataSet(idx_set=val_idx)\n\ntrain_loader = DataLoader(train_set , batch_size=32 , shuffle=False )\nvalid_loader = DataLoader(valid_set , batch_size=32 , shuffle=False)","0b7deebc":"train_data = next(iter(train_loader))","29954e89":"gray_img = train_data['gray_img'][0].numpy().squeeze(0)\nab_img = train_data['ab_img'][0].numpy()","e152bdb0":"mean = [0.5 , 0.5]\nstd = [0.5 , 0.5]\ndef viz_tensor(gray_img , ab_img):\n    gray_img = gray_img[0].numpy().squeeze(0)\n    ab_img = ab_img[0].numpy()\n    mean_ab =[0.0 , 0.0]\n    std_ab = [1.0 , 1.0]\n    img = np.zeros((224, 224, 3))\n    ab_img = np.transpose(ab_img , (1,2,0))\n    #ab_img = ab_img * std_ab + mean_ab\n    ab_img =( ab_img* std) + mean\n    img = np.zeros((224, 224, 3))\n    img[:, :, 0] = gray_img *255\n    img[:, :, 1:] = ab_img *255\n    img = img.astype('uint8')\n    img = cv2.cvtColor(img, cv2.COLOR_LAB2RGB)\n    plt.imshow(img)\n    plt.title(\"Reconstructed image\")\n    plt.show()","4152b78f":"viz_tensor(gray_img , ab_img)","d52de7eb":"image_gray = images_gray[15705]\nimage_lab = stacked_ab[15705]\n\n# Initializing with zeros ( or any random number)\nimg_1 = np.zeros((224, 224, 3))\nimg_1[:, :, 0] = image_gray\nimg_1[:, :, 1:] = image_lab\n\n# Changing the data type of the img array to 'uint8'\nimg_1 = img_1.astype('uint8')\n# doesn't have values between 0-1. infact it ranges till 255\nimg_1 = cv2.cvtColor(img_1, cv2.COLOR_LAB2RGB)\n\n\nprint('Gray scale image')\nplt.imshow(image_gray)\nplt.show()\n\nprint('Recreated image')\nplt.imshow(img_1)\nplt.show()","52eebfe3":"from torchvision import models\nimport torch.nn as nn\nimport torch","50ba1e55":"device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nclass Model_Colarize(nn.Module):\n    def __init__(self , in_channels , out_channels ):\n        super(Model_Colarize , self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \"\"\"\n        Model Architecture\n        \n        encoder ->> 1 -> 64 -> 128 -> 128(downsample) -> 256 -> 256(downsample) -> 512 -> 512 ->256 -> fusion\n        decoder -> 256 -> 128 -> 64(upsample) -> 64 -> 32(upsample) -> 2\n        \n        \"\"\"\n        #_____________________________________ Encoder _________________________________________________________________\n        \n        self.conv_64 = Model_Colarize._conv_block(self.in_channels , 64 , kernel=3 , stride=1  , padding=1 , dropout=0.2)\n        self.conv_128 = Model_Colarize._conv_block(64 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.2)\n        self.conv_128_down = Model_Colarize._conv_block(128 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.3 , downsample=True)\n        self.conv_256 = Model_Colarize._conv_block(128 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.conv_256_down = Model_Colarize._conv_block(256 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.3 , downsample=True)\n        self.conv_512_1 = Model_Colarize._conv_block(256 , 512 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.conv_512_2 = Model_Colarize._conv_block(512 , 512 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        \n        #_________________________________ Fusion ___________________________________________________________________________\n        \n        self.conv_256_fusion = Model_Colarize._conv_block(512 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.conv_1256 = Model_Colarize._conv_block(1256 , 256 , kernel=1 , stride=1 , padding=0 , dropout=0.3)\n        \n        #____________________________________ Decoder __________________________________________________________________\n        \n        self.conv_128_up = Model_Colarize._conv_block(256 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.conv_64_up = Model_Colarize._upsample_block(128 , 64 )\n        self.conv_64_nup = Model_Colarize._conv_block(64 ,64 , kernel=3 , stride=1 , padding=1 , dropout=0.4)\n        self.conv_32_up = Model_Colarize._upsample_block(64 , 32 )\n        \n        #_______________________________________ Result ___________________________________________________________________\n        \n        self.result = Model_Colarize._conv_block(32 , 2 , kernel=3 , stride=1 , padding=1 , dropout=0.5)\n        \n        \n    def forward(self, x , x_res101):\n        \n        c64_out =  self.conv_64(x)\n        c128_out = self.conv_128(c64_out)\n        c128_down = self.conv_128_down(c128_out)\n        c256_out =  self.conv_256(c128_down)\n        c256_down = self.conv_256_down(c256_out)\n        c512_1_out = self.conv_512_1(c256_down)\n        c512_2_out = self.conv_512_2(c512_1_out)\n        c256_fuse_in = self.conv_256_fusion(c512_2_out)\n        \n        fuse_layer = torch.cat([c256_fuse_in , x_res101] , dim=1)\n        conv_fuse_out = self.conv_1256(fuse_layer)\n        \n        c128_up_out = self.conv_128_up(conv_fuse_out)\n        c64_up_out = self.conv_64_up(c128_up_out)\n        c64_nup_out = self.conv_64_nup(c64_up_out)\n        c32_up_out = self.conv_32_up(c64_nup_out)\n        \n        result = self.result(c32_up_out)\n        \n        return result\n        \n        \n        \n    @staticmethod\n    def _conv_block(in_channels , out_channels , kernel  , stride , padding ,dropout , downsample=False , batch_norm=True):\n        layers=[]\n        conv = nn.Conv2d(in_channels=in_channels , out_channels= out_channels , \n                         kernel_size=kernel , stride=stride , \n                         padding=padding)\n        \n        layers.append(conv)\n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_channels)\n            layers.append(bn)\n            \n        relu = nn.ReLU()\n        layers.append(relu)\n        dropout = nn.Dropout(p=dropout)\n        layers.append(dropout)\n        \n        if(downsample):\n            maxpool = nn.MaxPool2d(kernel_size=2 , stride=2)\n            layers.append(maxpool)\n            \n        return nn.Sequential(*layers)\n        \n    @staticmethod\n    def _upsample_block(in_channels , out_channels , kernel=4 , stride=2 , padding=1 , dropout=0.3 , batch_norm=True):\n        layers=[]\n        up_conv = nn.ConvTranspose2d(in_channels=in_channels , out_channels=out_channels , \n                                    kernel_size=kernel , stride=stride , padding=padding)\n        layers.append(up_conv)\n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_channels)\n            layers.append(bn)\n            \n        relu = nn.ReLU()\n        layers.append(relu)\n        \n        return nn.Sequential(*layers)","894c44f9":"class FusionLayer(nn.Module):\n    def __init__(self):\n        super(FusionLayer , self).__init__()\n        self.res101 = models.resnet101(pretrained=True)\n        for params in self.res101.parameters():\n            params.requires_grad = False\n            \n    def forward(self , x  , dim1 , dim2):\n        \n        x = self.res101(x)\n        x = x.unsqueeze(2)\n        x = x.repeat(1, 1 , dim1*dim2)\n        batch_size  , last_dim = x.shape[0] , x.shape[1]\n        x = x.view(batch_size ,last_dim , dim1 , dim2 )\n        \n        return x","4c18653b":"class FwdModel(nn.Module):\n    def __init__(self , in_channels , out_channels):\n        super(FwdModel , self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        \"\"\"\n        Model Architecture\n        \n        encoder ->> 1 -> 64 -> 128 -> 128(downsample) -> 256 -> 256(downsample) -> 512 -> 512 ->256 -> fusion\n        decoder -> 256 -> 128 -> 64(upsample) -> 64 -> 32(upsample) -> 2\n        \n        \"\"\"\n        #_____________________________________ Encoder _________________________________________________________________\n        \n        self.conv_64 = FwdModel._conv_block(self.in_channels , 64 , kernel=3 , stride=1  , padding=1 , dropout=0.2)\n        self.conv_128 = FwdModel._conv_block(64 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.2)\n        self.conv_128_down = FwdModel._conv_block(128 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.3 , downsample=True)\n        self.conv_256 = FwdModel._conv_block(128 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.conv_256_down = FwdModel._conv_block(256 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.3 , downsample=True)\n        self.conv_512 = FwdModel._conv_block(256 , 512 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        \n        #_________________________________ Fusion ___________________________________________________________________________\n        \n        self.conv_1024_res = FwdModel._conv_block(512 , 1024 , kernel=3 , stride=1 , padding=1 , dropout=0.3)\n        self.res_block1 = FwdModel._res_block(1024 , 1024)\n        self.res_block2 = FwdModel._res_block(1024 , 1024)\n        self.res_block3 = FwdModel._res_block(1024 , 1024)\n        self.conv_256_res = FwdModel._conv_block(1024 , 256 , kernel=3 , stride=1 , padding=1 , dropout=0.4)\n        \n        #____________________________________ Decoder __________________________________________________________________\n        \n        self.conv_128_up = FwdModel._conv_block(256 , 128 , kernel=3 , stride=1 , padding=1 , dropout=0.4)\n        self.conv_64_up = FwdModel._upsample_block(128 , 64 )\n        self.conv_64_nup = FwdModel._conv_block(64 ,64 , kernel=3 , stride=1 , padding=1 , dropout=0.4)\n        self.conv_32_up =FwdModel._upsample_block(64 , 32 )\n        \n        #_______________________________________ Result ___________________________________________________________________\n        \n        self.result = FwdModel._conv_block(32 , 2 , kernel=3 , stride=1 , padding=1 , dropout=0.5)\n        \n        \n    def forward(self, x ):\n        \n        c64_out =  self.conv_64(x)\n        c128_out = self.conv_128(c64_out)\n        c128_down = self.conv_128_down(c128_out)\n        c256_out =  self.conv_256(c128_down)\n        c256_down = self.conv_256_down(c256_out)\n        c512_out = self.conv_512(c256_down)\n        \n        conv_res_in = self.conv_1024_res(c512_out)\n        res1_out = self.res_block1(conv_res_in)\n        res2_in = res1_out + conv_res_in\n        res2_out = self.res_block2(res2_in)\n        res3_in = res2_out + res2_in\n        res3_out = self.res_block3(res3_in)\n        res_out = res3_in + res3_out\n        \n        conv_res_out  = self.conv_256_res(res_out)\n        \n        c128_up_out = self.conv_128_up(conv_res_out)\n        c64_up_out = self.conv_64_up(c128_up_out)\n        c64_nup_out = self.conv_64_nup(c64_up_out)\n        c32_up_out = self.conv_32_up(c64_nup_out)\n        \n        result = self.result(c32_up_out)\n        \n        return result\n        \n        \n        \n    @staticmethod\n    def _conv_block(in_channels , out_channels , kernel  , stride , padding ,dropout , downsample=False , batch_norm=True):\n        layers=[]\n        conv = nn.Conv2d(in_channels=in_channels , out_channels= out_channels , \n                         kernel_size=kernel , stride=stride , \n                         padding=padding)\n        \n        layers.append(conv)\n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_channels)\n            layers.append(bn)\n            \n        relu = nn.ReLU()\n        layers.append(relu)\n        dropout = nn.Dropout(p=dropout)\n        layers.append(dropout)\n        \n        if(downsample):\n            maxpool = nn.MaxPool2d(kernel_size=2 , stride=2)\n            layers.append(maxpool)\n            \n        return nn.Sequential(*layers)\n        \n    @staticmethod\n    def _upsample_block(in_channels , out_channels , kernel=4 , stride=2 , padding=1 , dropout=0.3 , batch_norm=True):\n        layers=[]\n        up_conv = nn.ConvTranspose2d(in_channels=in_channels , out_channels=out_channels , \n                                    kernel_size=kernel , stride=stride , padding=padding)\n        layers.append(up_conv)\n        if(batch_norm):\n            bn = nn.BatchNorm2d(out_channels)\n            layers.append(bn)\n            \n        relu = nn.ReLU()\n        layers.append(relu)\n        \n        return nn.Sequential(*layers)\n    \n    @staticmethod\n    def _res_block(in_channels , out_channels ):\n        \"\"\"\n        3 by 3 conv layer\n        bn leaky relu\n        3 by 3 conv layer\n        bn leaky relu\n        \"\"\"\n        layers = nn.Sequential(\n        \n            nn.Conv2d(in_channels , out_channels , kernel_size=3 , stride=1 , padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(negative_slope=0.2),\n            nn.Conv2d(out_channels , out_channels , kernel_size=3 , stride=1 , padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(negative_slope=0.2)\n        )\n        \n        return layers","ea180aca":"#define the trainig models\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nfusion_model = FusionLayer()\ncolorize_model = Model_Colarize(in_channels=1 , out_channels=2)\nFwd_model = FwdModel(in_channels=1 , out_channels=2)\nRefine_model = FwdModel(in_channels=5 , out_channels=2)","d83c99fa":"class RMSLoss(nn.Module):\n    def __init__(self , eps = 1e-6):\n        super(RMSLoss , self).__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n        \n    def forward(self , y_pred , y_target):\n        # prediction -->> y_target\n        loss = torch.sqrt(self.mse(y_pred , y_target) + self.eps)\n        \n        return loss","a8b2bd44":"\"\"\"\ndeint the loss functions and the optimizers for the models this is a RMS error because we need to predict the exact ab values with ground truth\n\"\"\"\nimport torch.optim as optim\n\nloss_colorize = RMSLoss().to(device)\noptim_colorize = optim.Adam(colorize_model.parameters() , lr=0.001)\n\nloss_fwd = RMSLoss().to(device)\noptim_fwd = optim.Adam(Fwd_model.parameters() , lr=0.001 )\n\nloss_refine = RMSLoss().to(device)\noptim_refine = optim.Adam(Refine_model.parameters() , lr=0.001)","11355efa":"from collections  import deque\nEPOCHS = 20\nmaxlen=800\ntrain_col_loss = deque(maxlen=maxlen)\ntrain_fwd_loss = deque(maxlen=maxlen)\ntrain_ref_loss = deque(maxlen=maxlen)\nval_col_loss = deque(maxlen=maxlen)\nval_fwd_loss = deque(maxlen=maxlen)\nval_ref_loss = deque(maxlen=maxlen)\ntotal_col_loss =[]\ntotal_fwd_loss =[]\ntotal_ref_loss =[]\ntotal_val_col_loss = []\ntotal_val_fwd_loss = []\ntotal_val_ref_loss = []\nprint_evvery = 800\ndim1=56\ndim2=56","5ba10499":"fusion_model.to(device)\ncolorize_model.to(device)\nFwd_model.to(device)\nRefine_model.to(device)","6051c2f0":"\"\"\"\n1. iterate the train data loader and get the 3dim gray img , ab img , gray img\n2. compute the fusion embeddings usig res101\n3. fwd forward the colorize model and fwd model to gather predicted ab images\n4. feed forward the refine model to refine the results and predict the ab image\n\n\"\"\"\nfrom torch.autograd import Variable\n\nfor i_epoch in range(EPOCHS):\n    \n    epoch_col_loss =0\n    epoch_fwd_loss =0\n    epoch_ref_loss =0\n    epoch_val_col_loss = 0\n    epoch_val_fwd_loss = 0\n    epoch_val_ref_loss = 0\n    \n    for idx , data in enumerate(train_loader) :\n        gray_rgb = data['rgb_img'].to(device)\n        gray_img = data['gray_img'].to(device)\n        ab_img = data['ab_img'].to(device)\n        \n        # 1 step  compute the res101 embeddings\n        res101_out = fusion_model(gray_rgb , dim1 , dim2)\n        # 2 step feed forward the colorize model\n        colorize_out = colorize_model(gray_img , res101_out)\n        # 3 step feed forward the gray img in fwd model\n        fwd_out = Fwd_model(gray_img)\n        # 4 step concat the fwd out and colorize out\n        refine_in = torch.cat([colorize_out , fwd_out , gray_img] , dim=1)\n        \n        \"\"\"\n        since ou refine model input depend on the col and fwd model then it create a path to all way to the other models hence we ned to detach \n        the input to that model input from the autograd and again redefine the input with Variable type\n        \"\"\"\n        refine_in = refine_in.detach()\n        refine_in = Variable(refine_in.data, requires_grad=True)\n    \n        # 5 step feed forward the refine model\n        refine_out = Refine_model(refine_in)\n        \n        # 6 step compute loss values\n        col_loss = loss_colorize(colorize_out , ab_img)\n        fwd_loss = loss_fwd(fwd_out , ab_img)\n        ref_loss = loss_refine(refine_out , ab_img)\n        \n        # 7 reset the optimizers\n        optim_colorize.zero_grad()\n        optim_fwd.zero_grad()\n        optim_refine.zero_grad()\n        \n        # 8 step backprop the loss\n        col_loss.backward()\n        fwd_loss.backward()\n        ref_loss.backward()\n        \n        # 9 step step on the optimizer\n        optim_colorize.step()\n        optim_fwd.step()\n        optim_refine.step()\n        \n        # add loss vals\n        epoch_col_loss += col_loss.to('cpu').detach().item()\n        epoch_fwd_loss += fwd_loss.to('cpu').detach().item()\n        epoch_ref_loss += ref_loss.to('cpu').detach().item()\n        \n        train_col_loss.append(col_loss.to('cpu').item())\n        train_fwd_loss.append(fwd_loss.to('cpu').item())\n        train_ref_loss.append(ref_loss.to('cpu').item())\n        \n        if((idx+1)%print_evvery==0):\n            print(\"Epoch : {}  Col loss : {:.6f} Fwd loss : {:.6f} Ref loss : {:.6f}\".format(\n                i_epoch , \n                np.mean(train_col_loss) ,\n                np.mean(train_fwd_loss) ,\n                np.mean(train_ref_loss) \n            ))\n            total_col_loss.append(np.mean(train_col_loss))\n            total_fwd_loss.append(np.mean(train_fwd_loss))\n            total_ref_loss.append(np.mean(train_ref_loss))\n            \n        if((idx+1)%3000 == 0):\n            viz_tensor(gray_img.to('cpu').detach() , refine_out.to('cpu').detach())\n      \n    colorize_model.eval()\n    Fwd_model.eval()\n    Refine_model.eval()\n    with torch.no_grad():\n        for idx , data in enumerate(valid_loader) :\n\n            gray_rgb = data['rgb_img'].to(device)\n            gray_img = data['gray_img'].to(device)\n            ab_img = data['ab_img'].to(device)\n\n            # 1 step  compute the res101 embeddings\n            res101_out = fusion_model(gray_rgb)\n            # 2 step feed forward the colorize model\n            colorize_out = colorize_model(gray_img)\n            # 3 step feed forward the gray img in fwd model\n            fwd_out = Fwd_model(gray_img)\n            # 4 step concat the fwd out and colorize out\n            refine_in = torch.cat([colorize_out , fwd_out , gray_img] , dim=1)\n            # 5 step feed forward the refine model\n            refine_out = Refine_model(refine_in)\n\n            # 6 step compute loss values\n            col_loss = loss_colorize(colorize_out , ab_img)\n            fwd_loss = loss_fwd(fwd_out , ab_img)\n            ref_loss = loss_refine(refine_out , ab_img)\n\n            # add loss vals\n            epoch_val_col_loss += col_loss.to('cpu').detach()\n            epoch_val_fwd_loss += fwd_loss.to('cpu').detach()\n            epoch_val_ref_loss += ref_loss.to('cpu').detach()\n\n            val_col_loss.append(col_loss.to('cpu').item())\n            val_fwd_loss.append(fwd_loss.to('cpu').item())\n            val_ref_loss.append(ref_loss.to('cpu').item())\n\n            if((idx+1)%print_evvery==0):\n                print(\"Epoch : {}  Col loss : {:.6f} Fwd loss : {:.6f} Ref loss : {:.6f}\".format(\n                    i_epoch , \n                    np.mean(val_col_loss) ,\n                    np.mean(val_fwd_loss) ,\n                    np.mean(val_ref_loss) \n                ))\n                total_val_col_loss.append(np.mean(val_col_loss))\n                total_val_fwd_loss.append(np.mean(val_fwd_loss))\n                total_val_ref_loss.append(np.mean(val_ref_loss))\n        \n    colorize_model.train()\n    Fwd_model.train()\n    Refine_model.train()","5cc0fb85":"# Simple visualization","816b895c":"# Define Refinement and FeedFwd Model","975939f0":"# Define the model Coloarization","879b5017":"# Define training loop","79dfd72c":"# Define the Fusion Model"}}