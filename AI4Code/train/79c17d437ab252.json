{"cell_type":{"9cbfdaca":"code","df62a4ab":"code","a8801691":"code","551ab140":"code","ceae6327":"code","da1d7a0b":"code","215f9aa6":"code","93af8d08":"code","146f1b34":"code","c88f5033":"code","9f87dcc9":"code","a8f6d4a7":"code","426e0424":"code","7fc79e31":"code","d5315218":"code","9fc95702":"code","8f7a25c3":"code","b07e5ae9":"code","060826cb":"code","b4609179":"code","492b70a8":"code","4c59c249":"code","0f8dc2ff":"code","7fd92903":"code","d3f52232":"code","571947ee":"code","7b26bf66":"code","5e40c06e":"code","7bf1432f":"code","5d0aa985":"code","a60f8cc1":"code","5d0334e8":"code","44d830fe":"code","1881b391":"code","880f5407":"code","150db148":"code","e9b0c454":"code","9fdbf486":"code","fc04604e":"code","ff749b52":"code","4af2b192":"code","ed9347b3":"code","c79a5efb":"code","e1fd4dc1":"code","8e83e237":"code","bd0833b1":"code","f17d61f9":"code","b3611e63":"code","09bd09e0":"code","0737f52b":"code","3d24b3d0":"code","6625c3b3":"code","8a9d7d0a":"code","6745ee1a":"code","d52e318b":"code","52eb44fc":"code","04df2320":"code","4d162ae7":"code","e382bc50":"code","f1723e21":"code","842ce6bb":"code","93198a89":"code","8010b4a2":"code","3af8671d":"code","1ba3fa92":"code","90100d35":"code","fc172cb1":"markdown","76859aed":"markdown","24306146":"markdown","97fb4f45":"markdown","6fe2c343":"markdown","3f503ebf":"markdown","32477f81":"markdown","c5ff2137":"markdown","e5babf9b":"markdown","2486f4ff":"markdown","7e9b6a79":"markdown","ce71945e":"markdown","5e7bca37":"markdown","669a31ab":"markdown","e2c3132f":"markdown","76cdb1ec":"markdown","c7ee9476":"markdown","fb990861":"markdown","214975f7":"markdown","2b650df7":"markdown","496762a4":"markdown","f84829fb":"markdown","c4609b8e":"markdown","8d2cc781":"markdown","b529a998":"markdown","27159e90":"markdown","b2aedec6":"markdown","08efeedb":"markdown","6bfcd545":"markdown","8f25fabb":"markdown","fe1ce730":"markdown","4aca077e":"markdown","509fe026":"markdown","90ac9397":"markdown"},"source":{"9cbfdaca":"# basic libraries\n\nimport glob\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nfrom matplotlib.image import imread\nimport seaborn as sns\n\n# warnings & offline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.offline as pyo\n\n# files & pics imports\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom os import listdir\nfrom os.path import isfile, join\n\n\n# Data Preprocessing & model tools:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import plot_tree\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix,roc_curve, roc_auc_score, accuracy_score\n\n# Models:\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\n\n# A model that I learned by myself: CatBoost + Plotly\n\nfrom catboost import CatBoostClassifier\nimport plotly.graph_objects as go\nimport plotly.io as pio\nimport plotly.express as px\n\n# Clustering:\n\nfrom sklearn.cluster import KMeans\n\n# PCA:\n\nfrom sklearn.decomposition import PCA\n\n# Scaling:\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Cross Validation:\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# OpenCv & keras image preprocessing\n    \nimport cv2\nfrom keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator","df62a4ab":"# here's an example of cat pictures with a variety of simple manipulations:\n\nimage = imread('..\/input\/dogs-vs-cats-csv\/cat.0.jpg')\nimage_flip = cv2.flip(image, 1)\nimage90 = cv2.flip(image, cv2.ROTATE_90_CLOCKWISE)\nimage180 = cv2.rotate(image, 1)\nimage_trans = cv2.transpose(image)\n\nfig, axes = plt.subplots(1,5, figsize = (10,10))\naxes = axes.ravel()\naxes[0].imshow(image, cmap=plt.get_cmap('gray'))\naxes[0].axis('off')\naxes[0].set_title(\"Original\", size=15)\naxes[1].imshow(image_flip, cmap=plt.get_cmap('gray'))\naxes[1].axis('off')\naxes[1].set_title(\"Flipped\", size=15)\naxes[2].imshow(image90, cmap=plt.get_cmap('gray'))\naxes[2].axis('off')\naxes[2].set_title(\"180 & Flipped\", size=15)\naxes[3].imshow(image180, cmap=plt.get_cmap('gray'))\naxes[3].axis('off')\naxes[3].set_title(\"180\", size=15)\naxes[4].imshow(image_trans, cmap=plt.get_cmap('gray'))\naxes[4].axis('off')\naxes[4].set_title(\"Transposed\", size=15)\n\nplt.show()\n","a8801691":"df = pd.read_csv(r'..\/input\/dogs-vs-cats-csv\/train.csv') # our full data frame of train\ndf.sample(n = 4, random_state = 4).sort_values(by = 'label')","551ab140":"target = df['label'] # the feature we would like to predict, the label of picture\ndata = df.drop(['label'], axis = 1) # we will drop y from x, because we want to predict it","ceae6327":"# we will convert out targets list into a pandas df so we will add it to the data df\nlabels = pd.DataFrame(df['label'])\nlabels.sample(n = 4, random_state = 4).sort_values(by = 'label')","da1d7a0b":"df.shape # 23000 pics + 23000 flipped pics, 6075 pixels and 1 label column","215f9aa6":"# now we will create a df for out test\ntest = pd.read_csv(r'..\/input\/dogs-vs-cats-csv\/test.csv')    \n\ny_test = pd.DataFrame(test['label'])    \nX_test = test.drop(['label'], axis = 1)\nX_test.head()","93af8d08":"fig, axes = plt.subplots(3, 3, figsize = (10,10))\naxes = axes.ravel()\n\nfor i in range(9):\n    axes[i].imshow(data.values.reshape((data.shape[0], 45, 45, 3))[i*2550])\n    axes[i].set_title(\"Label: \" + str(df.label[i*2550]), size=15)\n    axes[i].axis('off')\nplt.show()","146f1b34":"# simple mapping of the labels into numbers\n\nmapping = {'cat':0, 'dog':1}\ndf.label = df.label.map(mapping)\ny_test = y_test.label.map(mapping)\ndf.sample(2, random_state=555)","c88f5033":"# A function to show the labels when we it needed\n\ndef num_to_name(label):\n    labeled = label.copy()\n    mapping = {0 :'Cat',\n    1 :'Dog'}\n    labeled = label.map(mapping)\n    return labeled","9f87dcc9":"df.describe()","a8f6d4a7":"df[df['label']==0].describe() # for cats","426e0424":"df[df['label']==1].describe() # for dogs","7fc79e31":"df.isnull().sum().sum() # the dataset has no NaN values","d5315218":"mapping = {'cat':0, 'dog':1}\ntarget = target.map(mapping)","9fc95702":"num_to_name(target).value_counts() # count the examples we have in each lable","8f7a25c3":"# a simple counter graph\n\nplt.subplots(figsize = (6,4))\nplt.title(\"Labels Counter\", size=20)\nfig = sns.countplot(num_to_name(target))","b07e5ae9":"X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.15, random_state=18)","060826cb":"# from 64 to 32: to avoid infinity cases with high numbers\n\nX_train = X_train.astype(np.float32)\nX_val = X_val.astype(np.float32)\nX_test = X_test.astype(np.float32)","b4609179":"X_train = X_train \/ 255\nX_val = X_val \/ 255\nX_test = X_test \/ 255","492b70a8":"pca = PCA() # all 6075 features\npca.fit(X_train)","4c59c249":"# Explained variance graph\n\nexp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\nfig = px.area(\n    title = \"Explained variance as a function of the number of dimensions:\",\n    x=range(1, exp_var_cumul.shape[0] + 1),\n    y=exp_var_cumul * 100,\n    labels={\"x\": \"# of Pixels\", \"y\": \"Explained Variance\"},\n    width = 1000 ,\n    height = 500\n)\n\nfig.show()","0f8dc2ff":"pca = PCA(n_components=0.95) # we can also try using svd_solver=\"randomized\"\nX_train_reduced = pca.fit_transform(X_train)\nX_val_reduced = pca.transform(X_val)\npca.n_components_","7fd92903":"# A three-dimensional graph depicting the way our data is interpreted, plotly does it easily for us \ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter_3d(\n    X_train_reduced, x=0, y=1, z=2, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n)\nfig.show()","d3f52232":"# 2D version: with x and y\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=0, y=1, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","571947ee":"# 2D version: with y and z\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=1, y=2, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()\n","7b26bf66":"# 2D version: with z and y\ntotal_var = pca.explained_variance_ratio_.sum() * 100\nfig = px.scatter(\n    X_train_reduced, x=2, y=1, color = num_to_name(y_train),\n    title=f'Total Explained Variance: {total_var:.2f}%',\n    labels={'0': 'PC 1', '1': 'PC 2'}\n)\nfig.show()","5e40c06e":"X_train = pd.DataFrame(X_train_reduced)\nX_val = pd.DataFrame(X_val_reduced)","7bf1432f":"bayes = GaussianNB()\nbayes.fit(X_train, y_train)\nbayes","5d0aa985":"y_pred = bayes.predict(X_val)\nbayes_acc = accuracy_score(y_val, y_pred)\nbayes_acc","a60f8cc1":"print (classification_report(y_val, y_pred))","5d0334e8":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn","44d830fe":"y_pred = knn.predict(X_val)\nknn_acc = accuracy_score(y_val, y_pred)\nknn_acc","1881b391":"print (classification_report(y_val, y_pred))","880f5407":"cat = CatBoostClassifier(logging_level='Silent')\ncat.fit(X_train, y_train)\ncat","150db148":"y_pred = cat.predict(X_val)\ny_pred_cat = y_pred.copy()\ncat_acc = accuracy_score(y_val, y_pred)\ncat_acc","e9b0c454":"print (classification_report(y_val, y_pred))","9fdbf486":"#rfc = RandomForestClassifier(n_estimators=10)\n#ada = AdaBoostClassifier(n_estimators=100,learning_rate= 0.1, base_estimator=rfc)\nada = AdaBoostClassifier()\nada.fit(X_train, y_train)\nada","fc04604e":"y_pred = ada.predict(X_val)\nada_acc = accuracy_score(y_val, y_pred)\nada_acc","ff749b52":"print (classification_report(y_val, y_pred))","4af2b192":"gb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)\ngb","ed9347b3":"y_pred = gb.predict(X_val)\ngb_acc = accuracy_score(y_val, y_pred)\ngb_acc","c79a5efb":"print (classification_report(y_val, y_pred))","e1fd4dc1":"xgb = XGBClassifier(use_label_encoder =False)\nxgb.fit(X_train, y_train)\nxgb","8e83e237":"y_pred = xgb.predict(X_val)\nxgb_acc = accuracy_score(y_val, y_pred)\nxgb_acc","bd0833b1":"print (classification_report(y_val, y_pred))","f17d61f9":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf","b3611e63":"y_pred = rf.predict(X_val)\nrf_acc = accuracy_score(y_val, y_pred)\nrf_acc","09bd09e0":"print (classification_report(y_val, y_pred))","0737f52b":"stacking = StackingClassifier(estimators=[('gxb',xgb),('rf',rf)],final_estimator=cat)\nstacking.fit(X_train,y_train)\nstacking","3d24b3d0":"y_pred = stacking.predict(X_val)\nstacking_acc = accuracy_score(y_val, y_pred)\nstacking_acc","6625c3b3":"print (classification_report(y_val, y_pred))","8a9d7d0a":"clf1 = xgb\nclf2 = cat\nclf3 = rf","6745ee1a":"sv = VotingClassifier(estimators=[\n        ('xgb', clf1), ('cat', clf2)], voting='soft') # , ('rf', clf3)\nsv.fit(X_train, y_train)\nsv","d52e318b":"y_pred = sv.predict(X_val)\nsv_acc = accuracy_score(y_val, y_pred)\nsv_acc","52eb44fc":"print (classification_report(y_val, y_pred))","04df2320":"acc_list = {'Model':  ['Naive Bayes', 'KNN','CatBoost', 'AdaBoost', 'Gradient Boosting', 'XGBoost','Random Forest','Stacking', 'Soft Voting'],\n        'Accuracy': [bayes_acc,knn_acc,cat_acc,ada_acc,gb_acc,xgb_acc,rf_acc,stacking_acc,sv_acc],\n        }","4d162ae7":"# bar graph as a conclusion to the models\nfig = go.Figure(data=[\n    go.Bar(name='train set', x=acc_list['Model'], y=acc_list['Accuracy'],text=np.round(acc_list['Accuracy'],2),textposition='outside',marker_color='lightsalmon'),\n])\nfig.update_layout(barmode='group',title_text='Accuracy Comparison On Different Models',yaxis=dict(\n        title='Accuracy'))\nfig.show()","e382bc50":"# we would like to present confusion matrix for our best model:\n\ncm = confusion_matrix(y_val, y_pred_cat)\nplt.figure(figsize=(6,6))\nplt.imshow(cm, interpolation='nearest', cmap = plt.cm.coolwarm)\nplt.title('Confusion matrix - CatBoost Classifier', size = 15)\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Cat','Dog'], rotation=45, size = 12)\nplt.yticks(tick_marks, ['Cat','Dog'], size = 12)\nplt.tight_layout()\nplt.ylabel('Actual label', size = 15)\nplt.xlabel('Predicted label', size = 15)\nwidth, height = cm.shape\nfor x in range(width):\n    for y in range(height):\n        plt.annotate(str(cm[x][y]), xy=(y, x), \n        horizontalalignment='center',\n        verticalalignment='center')","f1723e21":"# ROC curve for our best model at validation:\n\nfpr, tpr, _= roc_curve(y_val, y_pred_cat)\nauc= roc_auc_score(y_val, y_pred_cat)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='blue')\nplt.box(False)\nplt.title('ROC CURVE CatBoost - Validation', size=15)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndct_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","842ce6bb":"# we will concat our X,y train & validation for cross validation for our best model - catboost\n\nX = pd.concat([X_train, X_val], axis = 0)\ny = pd.concat([y_train, y_val], axis = 0)\n\n# now the dataset is united again, we can split it 5 time for cv score\n\npipe = Pipeline(steps=[('CatBoost', cat)])\nsearch = GridSearchCV(pipe, param_grid={'CatBoost__iterations':[100,250,500]}, cv=5) # 5 cv on iterations\nsearch.fit(X, y) # we will apply cv on our whole data because we would like our model to see more examples\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","93198a89":"X_test = pca.transform(X_test)\ny_pred = search.predict(X_test)","8010b4a2":"accuracy_score(y_test, y_pred)","3af8671d":"print (classification_report(y_test, y_pred))","1ba3fa92":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,6))\nplt.imshow(cm, interpolation='nearest', cmap = plt.cm.coolwarm)\nplt.title('Confusion matrix - Test', size = 15)\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Cat','Dog'], rotation=45, size = 12)\nplt.yticks(tick_marks, ['Cat','Dog'], size = 12)\nplt.tight_layout()\nplt.ylabel('Actual label', size = 15)\nplt.xlabel('Predicted label', size = 15)\nwidth, height = cm.shape\nfor x in range(width):\n    for y in range(height):\n        plt.annotate(str(cm[x][y]), xy=(y, x), \n        horizontalalignment='center',\n        verticalalignment='center')","90100d35":"# ROC curve for our best model at test:\n\nfpr, tpr, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(fpr, tpr, label=\"auc=\"+str(auc), color='red')\nplt.box(False)\nplt.title('ROC CURVE CatBoost - TEST', size=15)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate (Recall)')\nplt.grid(True)\nplt.show()\n\ndct_auc = round(auc,3)*100\n \nprint(f\"The score for the ROC Curve is: {round(auc,3)*100}%\")","fc172cb1":"##### Because the test folder has no labels, I created a test folder by myself by taking 1000 images of each type and putting them in a folder named 'test'.","76859aed":"### Scaling:","24306146":"##### Let's continue to deal with the data:","97fb4f45":"##### Since most of the images are different from each other, some in a minor way and some in a very significant way, I estimate that we need to use as much data as possible in order to be successful in the classification process. It is I prefer to take 0.15 percent of the data for the test. We will also do cross validation so I guess we will not run into overfitting issues anyway. \n\n##### Another assessment I have, and it is perhaps most important of all, is that although we doubled the amount of data by 2 by adding mirror images, our model could not learn from one image to its mirror image. Therefore, I estimate that even though I took a small test and multiplied my data with supposedly the \"same\" data, there will be no overfitting problem here.","6fe2c343":"##### Link to kaggle: https:\/\/www.kaggle.com\/c\/dogs-vs-cats","3f503ebf":"##### After reducing the num of the features, we will convert X_train & X_validation to pandas data frames:","32477f81":"##### We would like to create an array which will contain our images:","c5ff2137":"##### Our data is divided equally. So a dummy classifier model will give us 50 percent success.","e5babf9b":"# Dogs VS Cats \n#####  @ Haim Goldfisher","2486f4ff":"### Soft Voting","7e9b6a79":"##### It can be seen that our model is relatively balanced. That is, it was able to classify dogs a little better than cats, but the difference is negligible.","ce71945e":"As we can see, CatBoost has the best results with 72.2 % of success","5e7bca37":"## KNN","669a31ab":"## Gradient Boosting","e2c3132f":"### Random Forest","76cdb1ec":"## Cross Validation by Pipeline & Grid Search","c7ee9476":"### XGBoost","fb990861":"## AdaBoost","214975f7":"##### Some important details before uploading the datasets:\n\n* After a lot of trials and errors, this is the best option of dataset that I can think about. I tried to use gray scale's data set but the color has great impact on the model's performents, so I gave up on the idea of gray pictures. In this case, each pixel in each picture has RGB values (3), which multiply the amount of pixels by three. However, after dimensionality reduction by PCA, the number of features we take is much more lower.\n\n\n* Due to https:\/\/nanonets.com\/blog\/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2\/ we would like to create more data by flip our pictures. I wanted to understand the logic of Deep Learning methods which can bring 97% success so I find this article. I have to add that I tried a lot of options of rotatitons and transforms, but 'flip' provides the best results.","2b650df7":"#### Since we don't have labels for the test1 folder, I took 1000 images of each type and transferred them to a test folder. We would like to load this folder","496762a4":"### Stacking","f84829fb":"##### Since we have 256 pixels (0-255), we can divide them into 255 in order to achieve the desired range values (0-1).","c4609b8e":"##### We can see that voting model didn't improve our model. It can be explained by the fact that xgb and catboost are both gradinet boosting alogrithms. In that case, their strategy is a little bit the same. In this case, catboost did better job, so we don't need xgb.","8d2cc781":"## Data Processing","b529a998":"##### After we have finished uploading all our data and converting it to CSV files, we would like to see how the images are really displayed through the rows in our data frame.","27159e90":"## Naive Bayes","b2aedec6":"# Dimensionality Reduction: PCA","08efeedb":"# Test","6bfcd545":"##### The beautiful results we got by the cross validation show that even though we doubled the amount of images, the model remained balanced. Turns out the idea to flipping the photos was successful.","8f25fabb":"# Models","fe1ce730":"## CatBoost","4aca077e":"# To sum up, our CatBoost model had 70.15% of success on the Test data. It did it by using 930 of 6075 features.","509fe026":"##### Some important things to talk about before we get started. Unlike the previous notebook, this time it would not be right to use ICA. As we saw at the beginning of the notebook, the animals in each picture are in a different place. It is not possible here to select a few important pixels but to identify the animal in each image. Plus, since I want to get good results, I'm willing to accept a large amount of features. We will go for as standard a dimension reduction as possible (apparently n_components = 0.95). That means even if we are left with a relatively high amount of features it is fine\n\n##### We would like to find out optimal n_components value","90ac9397":"### Training \/ Testing Split:"}}