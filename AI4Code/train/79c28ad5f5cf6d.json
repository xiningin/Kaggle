{"cell_type":{"652a3ea6":"code","f2a2e070":"code","55dee749":"code","c7e32ae6":"code","c5e2a1d2":"code","3edb3f19":"code","53981f7e":"code","8c9ac6f6":"code","e5ca9936":"code","7e98f883":"code","cd387949":"code","4e694d9c":"code","031f1ef7":"code","7e26827b":"code","d84664d6":"code","608eb216":"code","9964402d":"code","7652b052":"code","183442ff":"code","1ca20c66":"code","d9d68836":"code","c6298d6c":"code","dfd1072f":"code","ae14fd37":"code","e1eca75b":"code","53f6743c":"code","1052c578":"markdown","a8e2dca1":"markdown","77a50231":"markdown","8bdc0c0e":"markdown","f6607989":"markdown","57683c85":"markdown","833b1698":"markdown","0940f84f":"markdown","2e4eefc2":"markdown","30ae9ac6":"markdown","ffa0f9a1":"markdown","df874764":"markdown","b8b908fa":"markdown","17966b04":"markdown","6748ac90":"markdown","e16af976":"markdown","d415383b":"markdown","6c01d556":"markdown"},"source":{"652a3ea6":"# Importing the libaries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPool2D\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\nfrom keras.optimizers import SGD\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks.callbacks import EarlyStopping,ModelCheckpoint,CSVLogger\nfrom livelossplot import PlotLossesKeras\nfrom keras.applications import VGG16,VGG19\nfrom keras.models import Model\nimport os\nimport zipfile\nimport glob\nimport shutil\nprint(os.listdir('..\/input\/dogs-vs-cats'))","f2a2e070":"# Unipping the data\nzip_files = glob.glob('..\/input\/*\/*.zip')\nprint(zip_files)\n\n# extract file into a temp folder\ndef extract_zip(file):\n    with zipfile.ZipFile(file,\"r\") as zip_ref:\n        zip_ref.extractall(\"data\")\n        \n# extract both train and test1 zip\nfor files in zip_files:\n    extract_zip(files)","55dee749":"# Checking the new files along with number of samples\nprint(os.listdir('data'))\nprint(len(os.listdir('\/kaggle\/working\/data\/train')),'Training Samples')\nprint(len(os.listdir('\/kaggle\/working\/data\/test1')),'Testing Samples')","c7e32ae6":"# Checking the names of few data samples\nprint(os.listdir('\/kaggle\/working\/data\/train')[:5])","c5e2a1d2":"plt.figure(figsize=(10,10)) # specifying the overall grid size\n# define location of dataset\nfolder = '\/kaggle\/working\/data\/train'\nfor i in range(25):\n    plt.subplot(5,5,i+1)    # the number of images in the grid is 5*5 (25)\n    filename = folder + '\/dog.' + str(i) + '.jpg'\n\t# load image pixels\n\timage = imread(filename)\n\t# plot raw pixel data\n    plt.imshow(image)\n\nplt.show()","3edb3f19":"# plot dog photos from the dogs vs cats dataset\n\n# define location of dataset\nfolder = '\/kaggle\/working\/data\/train'\n# plot first few images\nfor i in range(9):\n\t# define subplot\n\tplt.subplot(330 + 1 + i)\n\t# define filename\n\tfilename = folder + '\/dog.' + str(i) + '.jpg'\n\t# load image pixels\n\timage = imread(filename)\n\t# plot raw pixel data\n\tplt.imshow(image, aspect = 'auto')\n# show the figure\nplt.show()","53981f7e":"# plot cat photos from the dogs vs cats dataset\n\n# define location of dataset\nfolder = '\/kaggle\/working\/data\/train'\n# plot first few images\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # define filename\n    filename = folder + '\/cat.' + str(i) + '.jpg'\n    # load image pixels\n    image = imread(filename)\n    # plot raw pixel data\n    plt.imshow(image, aspect = 'auto')\n# show the figure\nplt.show()","8c9ac6f6":"# create new directories for training data and validation data\ndataset_home = 'dataset\/'\nsubdirs = ['training_set\/', 'validation_set\/']\nfor subdir in subdirs:\n\t# create label subdirectories\n\tlabeldirs = ['dogs\/', 'cats\/']\n\tfor labldir in labeldirs:\n\t\tnewdir = dataset_home + subdir + labldir\n\t\tos.makedirs(newdir, exist_ok=True)","e5ca9936":"# create new directory for test data\ntestdir = dataset_home + 'testing_set'\nos.makedirs(testdir, exist_ok= True)\ndirs = '\/kaggle\/working\/dataset\/testing_set\/' + 'Test'\nos.makedirs(dirs,exist_ok = True)","7e98f883":"print(os.listdir(dataset_home))","cd387949":"# Dividng the training data into two parts, one for training and other for validation\n\n# seed random number generator\nfrom random import random\nfrom random import seed\nseed(1)\n# define ratio of pictures to use for validation\nvalidation_ratio = 0.25\n# copy training dataset images into subdirectories\nfrom shutil import copyfile\ntraining_source = '\/kaggle\/working\/data\/train\/'\nfor file in os.listdir(training_source):\n\tsrc = training_source + '\/' + file\n\tdestination = 'training_set\/'\n\tif random() < validation_ratio:\n\t\tdestination = 'validation_set\/'\n\tif file.startswith('cat'):\n\t\tdst = dataset_home + destination + 'cats\/'  + file\n\t\tcopyfile(src, dst)\n\telif file.startswith('dog'):\n\t\tdst = dataset_home + destination + 'dogs\/'  + file\n\t\tcopyfile(src, dst)","4e694d9c":"# Deleting the images in test folder if needed in order to get results on more random images\na = glob.glob('\/kaggle\/working\/dataset\/testing_set\/Test\/*.jpg')\nfor j in a:\n    os.remove(j)","031f1ef7":"# Creating some samples for classifying images using the build model\ntest_source = \"\/kaggle\/working\/data\/test1\"\n#print(os.listdir(test_source)[:5])\ntest_dest = \"\/kaggle\/working\/dataset\/testing_set\/Test\"\n# Selecting 10 random files from the test main folder\nrandom_samples = np.random.choice(os.listdir(test_source),10)\nfor file in random_samples:\n    new_src = test_source + '\/' + file\n    shutil.copy(new_src,test_dest)\nprint(os.listdir(test_dest))","7e26827b":"# Building the CNN model\nmodel = Sequential()\n\nmodel.add(Conv2D(32, (3, 3), padding='same', input_shape=(200,200,3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel.add(Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(units = 256, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units = 256, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n            optimizer=RMSprop(lr=0.0001),\n            metrics=['accuracy'])","d84664d6":"# Preparing the data and performing Data Augmentation\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n                                    shear_range=0.2,\n                                    zoom_range=0.2,\n                                    horizontal_flip=True)\n\nvalidation_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_set = train_datagen.flow_from_directory('dataset\/training_set',\n                                                target_size=(200, 200),\n                                                batch_size=64,\n                                                class_mode='binary')\nvalidation_set = validation_datagen.flow_from_directory('dataset\/validation_set',\n                                                target_size=(200, 200),\n                                                batch_size=64,\n                                                class_mode='binary')\ntest_set = test_datagen.flow_from_directory('dataset\/testing_set',\n                                                target_size=(200, 200),\n                                                batch_size=1,class_mode = 'binary',shuffle = False)","608eb216":"# Creating h5 file for storing best_model data\nwith open('best_model.h5', 'w') as best:\n    pass\n# Creating CSV file for storing losses\nwith open('CSVLogs.csv','w') as loss:\n    pass","9964402d":"# Creating the callback functions\nes = EarlyStopping(monitor='val_loss',mode= 'min',verbose=1,patience=10,restore_best_weights=True)\nmc = ModelCheckpoint('best_model.h5',monitor='val_loss',mode= 'min',verbose=1,save_best_only=False)\ncsv = CSVLogger('CSVLogs.csv',separator=',', append=False)","7652b052":"# Fitting the model to data\noutput = model.fit_generator(train_set, steps_per_epoch=(18697\/\/64),epochs= 20,\n                             validation_data = validation_set, validation_steps= (6303\/\/64),\n                             callbacks=[es,mc,csv,PlotLossesKeras()])","183442ff":"# Evalauting the model\nloss,score = model.evaluate_generator(validation_set,steps = (6303\/\/64), verbose = 0)\nprint('accuracy : %.3f'% (np.round(score*100,1)))","1ca20c66":"# Testing the model\nprobabilities = model.predict_generator(test_set)","d9d68836":"# Plotting the probabilities along with images:\nfor index, probability in enumerate(probabilities):\n    image_path = \"\/kaggle\/working\/dataset\/testing_set\/\" + test_set.filenames[index]\n    img = imread(image_path)\n    plt.imshow(img)\n    if probability > 0.5:\n        plt.title(\"%.2f\" % (probability[0]*100) + \"% dog\")\n    else:\n        plt.title(\"%.2f\" % ((1-probability[0])*100) + \"% cat\")\n    plt.show()","c6298d6c":"# Making new single predictions\ndef detect_image(filepath):\n    test_image = load_img(filepath, target_size = (200,200))\n    plt.imshow(test_image)\n    test_image = img_to_array(test_image)\n    test_image = np.expand_dims(test_image, axis = 0)\n    result = model.predict(test_image)\n    train_set.class_indices\n    if result[0][0] == 1:\n        prediction = 'dog'\n    else:\n        prediction = 'cat'\n    plt.title(prediction)\n\ndetect_image('\/kaggle\/working\/dataset\/testing_set\/Test\/5490.jpg')\n","dfd1072f":"# save model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n# save weights to HDF5\nwith open(\"model.h5\",'w') as h5_file:\n    pass\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","ae14fd37":"# Loading the VGG model using Transfer Learning\ntransfer_model = VGG16(include_top = False, weights = 'imagenet',input_shape = (200,200,3))\nfor layer in transfer_model.layers:\n    layer.trainable = False\n# Defining the output layers\nflat = Flatten()(transfer_model.layers[-1].output)\nclass1 = Dense(units = 256, activation='relu')(flat)\nclass2 = Dense(units = 256, activation = 'relu')(class1)\noutput = Dense(units = 1, activation='sigmoid')(class2)\n# define new model\ntransfer_model = Model(inputs=transfer_model.inputs, outputs=output)\ntransfer_model.compile(loss='binary_crossentropy',\n            optimizer=RMSprop(lr=0.001),\n            metrics=['accuracy'])","e1eca75b":"transfer_model.fit_generator(train_set, steps_per_epoch=(18697\/\/64),epochs= 10,\n                             validation_data = validation_set, validation_steps= (6303\/\/64),\n                             callbacks = [csv,PlotLossesKeras()])","53f6743c":"# Evalauting the model\nloss,score = transfer_model.evaluate_generator(validation_set,steps = (6303\/\/64), verbose = 0)\nprint('accuracy : %.3f'% (np.round(score*100,1)))","1052c578":"## *Evaluating and Testing the model*","a8e2dca1":"## *Performing the data augmentation to prevent overfitting*","77a50231":"## *Creating files for storing model*","8bdc0c0e":"## *Fitting the VGG19 model to the data*","f6607989":"## *Evaluating the VGG19 model results*","57683c85":"## *Plotting results along with the image*","833b1698":"## *Building the Convolutional Neural Network (CNN) model*","0940f84f":"## *Fitting the CNN model to data*","2e4eefc2":"## *Unzipping the Data*","30ae9ac6":"## *Loading the VGG19 model for transfer learning*","ffa0f9a1":"## *Creating folders for Training, Validation and Testing Data*","df874764":"---","b8b908fa":"## *Dividing the training data into two parts*","17966b04":"## *Saving our CNN model along with weights*","6748ac90":"# **Building a Convolutional Neural Network Using Deep Learning**\nIn this notebook, we will be building a **CNN model** used to classify images of dogs and cats using **keras library and tensorflow backend**. Also, we will be using transfer learning to load a predefined model VGG16 and use it to classify the images as well. The various steps that we will take in order to build the models are as follows:\n\n![image.png](attachment:image.png)\n* Preparing the Data\n\n\n    1. Importing the libraries and mounting the data.\n    2. Unzipping the data\n    3. Display random images of cats and dogs from data\n    4. Creating Folders for training, validaton and testing data\n    5. Dividing the training data into two parts\n        \n* Building the CNN model\n* Performing Data Augmentation on training and test data\n* Defining the Callback functions\n* Fitting the CNN model to data\n* Evaluating and predicting the results\n* Saving the model and weights\n* Loading the VGG16 model for transfer learning\n* Fitting the VGG16 model to data\n* Evaluating the VGG16 model results\n","e16af976":"## *Building the Callback Functions*","d415383b":"##          ***Importing the libraries and mounting the data***","6c01d556":"## *Display random images of cats and dogs from data*"}}