{"cell_type":{"0da91be4":"code","d2c77215":"code","041e2f28":"code","41254515":"code","13ba599f":"code","93fdd736":"code","c80e57df":"code","c7b1c7a3":"code","b4075c21":"code","2d79d9d6":"code","e65a50e0":"code","8c4848df":"code","81bb6a44":"code","fdf76471":"code","e9470c4b":"code","ca309c8f":"code","23348be5":"code","d7b7fdf8":"code","67b6f946":"code","e5d037c8":"code","6d0e20f2":"code","167bf191":"code","a998c8c4":"code","52297067":"code","17ea8d36":"code","279fbdeb":"code","88737137":"code","f8a78b83":"code","bcc5b3e3":"code","12fe18c8":"code","f0f6791c":"code","3106e01f":"code","f509298e":"code","7f10c190":"code","086574a3":"code","3e161941":"code","e9371aef":"code","1accd23c":"code","7d1ecd02":"code","148e7662":"code","614a0738":"markdown","5815c137":"markdown","e9813a80":"markdown","c1f3ee4d":"markdown","7e2ba265":"markdown","ea716a7c":"markdown","f78d4d97":"markdown","cbee7097":"markdown","c496362e":"markdown","73b153f9":"markdown","f7a800a2":"markdown","43e2afc6":"markdown","15ea867a":"markdown","36645c50":"markdown","8744f56a":"markdown","3ef5f0da":"markdown","98009fc8":"markdown","de8be0b1":"markdown","aee2bd36":"markdown","7f02479a":"markdown","90ee35d2":"markdown","17a060e6":"markdown","100060cf":"markdown","ba0a4315":"markdown","b04bea1a":"markdown","2a68522f":"markdown","ff7603f5":"markdown","d765b96a":"markdown","2b378af1":"markdown","87349f54":"markdown","8cb7ac85":"markdown","9cb22e74":"markdown","d428c446":"markdown","bf1321c8":"markdown","7de3e011":"markdown","b44ef2c7":"markdown","e1720e2f":"markdown","42de201c":"markdown","310bb61a":"markdown","b13fc66c":"markdown","57d8c0db":"markdown","825e01f3":"markdown","e888c148":"markdown","2dbde02b":"markdown","15878f3e":"markdown","d8a4274d":"markdown","8aeaf234":"markdown","ebe2387c":"markdown","841fa1f5":"markdown","b9d624b3":"markdown","fe7d05d7":"markdown"},"source":{"0da91be4":"# Core\nimport numpy as np\nimport pandas as pd\npd.set_option('display.float_format', lambda x: '%.1f' % x)\npd.get_option(\"display.max_columns\", 55)\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport statistics\nimport time\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Models\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","d2c77215":"# Save to df\ntrain_data=pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv', index_col='Id')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv', index_col='Id')\n\n# save for submission\ntest_index=test_data.index\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","041e2f28":"print('Number of null values in training set:',train_data.isnull().sum().sum())\nprint('')\nprint('Number of null values in test set:',test_data.isnull().sum().sum())","41254515":"train_data.describe()","13ba599f":"# Save to df\n#pseudo_label_df=pd.read_csv('..\/input\/tps-dec-pseudolabels\/pseudo_label_df.csv', index_col='Id')\npseudo_label_df=pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v2.csv', index_col='Id')\n\n# Concatenate\nnew_train_data=pd.concat([train_data, pseudo_label_df], axis=0)\n\n# Remove pseudolabel samples from test set\npseudo_label_index=pseudo_label_df.index\nnew_test_data=test_data.drop(pseudo_label_index, axis=0)\n\n# Save for submission\nnew_test_data_index=new_test_data.index\npseudo_label_preds_df=pd.DataFrame({'Id': pseudo_label_index,\n                       'Cover_Type': pseudo_label_df['Cover_Type']}).reset_index(drop=True)\n\n\n# Shape and preview\nprint('New train data df shape:',new_train_data.shape)\nprint('New test data df shape:',new_test_data.shape)\nnew_train_data.tail()","93fdd736":"# Figure size \nplt.figure(figsize=(12,6))\n\n# Countplot\nsns.countplot(new_train_data.Cover_Type)\n\n# Aesthetics\nplt.title('Distribution of labels in training set', fontsize=15)","c80e57df":"print('Number of samples with label 1 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==1]))\nprint('Number of samples with label 2 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==2]))\nprint('Number of samples with label 3 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==3]))\nprint('Number of samples with label 4 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==4]))\nprint('Number of samples with label 5 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==5]))\nprint('Number of samples with label 6 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==6]))\nprint('Number of samples with label 7 is', len(new_train_data.Cover_Type[new_train_data.Cover_Type==7]))","c7b1c7a3":"new_train_data.drop(new_train_data[new_train_data.Cover_Type==5].index, axis=0, inplace=True)","b4075c21":"sns.heatmap(new_train_data.iloc[:,:10].corr())","2d79d9d6":"fig, axes = plt.subplots(5, 2, figsize=(14, 30))\n\ni = 0\nfor subaxes in axes:\n    for axis in subaxes:\n        new_train_data.hist(column = new_train_data.columns[i], bins = 50, ax=axis)\n        plt.title(train_data.columns[i]+'\\n')\n        i = i+1","e65a50e0":"# Obtain continuous features\nfeature_variables = new_train_data.iloc[:,:10].columns.values.tolist()\n\n# Plot boxplot and distribution plot against pawpularity for each feature (excluding Id)\nfor i in feature_variables:\n    plt.figure(figsize=(14,5))\n    sns.violinplot(x='Cover_Type', y=i, data=new_train_data)\n    plt.title(i, fontsize=20)\n    fig.show()","8c4848df":"# Specify features to clip\nmask_features=['Slope','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology',\n              'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points']\n\n# Clip negative values\nnew_train_data[mask_features]=new_train_data[mask_features].clip(lower=0)\nnew_test_data[mask_features]=new_test_data[mask_features].clip(lower=0)","81bb6a44":"# Project training aspect angles onto [0,360]\nnew_train_data['Aspect'][new_train_data['Aspect'] < 0] += 360\nnew_train_data['Aspect'][new_train_data['Aspect'] >= 360] -= 360\n\n# Project test aspect angles onto [0,360]\nnew_test_data['Aspect'][new_test_data['Aspect'] < 0] += 360\nnew_test_data['Aspect'][new_test_data['Aspect'] >= 360] -= 360","fdf76471":"# l1 (aka Manhattan) distance to Hydrology\nnew_train_data['l1_Hydrology'] = np.abs(new_train_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_train_data['Vertical_Distance_To_Hydrology'])\nnew_test_data['l1_Hydrology'] = np.abs(new_test_data['Horizontal_Distance_To_Hydrology']) + np.abs(new_test_data['Vertical_Distance_To_Hydrology'])","e9470c4b":"# Euclidean distance to Hydrology (training set)\nnew_train_data[\"ED_to_Hydrology\"] = np.sqrt((new_train_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                        (new_train_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n\n# Euclidean distance to Hydrology (test set)\nnew_test_data[\"ED_to_Hydrology\"] = np.sqrt((new_test_data['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + \n                                       (new_test_data['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)","ca309c8f":"# Clips hillshades 0 to 255 index\nhillshades = [col for col in train_data.columns if col.startswith('Hillshade')]\n\n# Clip df's\nnew_train_data[hillshades] = new_train_data[hillshades].clip(0, 255)\nnew_test_data[hillshades] = new_test_data[hillshades].clip(0, 255)","23348be5":"# Soil type count\nsoil_features = [x for x in new_train_data.columns if x.startswith(\"Soil_Type\")]\nnew_train_data[\"Soil_Type_Count\"] = new_train_data[soil_features].sum(axis=1)\nnew_test_data[\"Soil_Type_Count\"] = new_test_data[soil_features].sum(axis=1)\n\n# Wilderness area count\nwilderness_features = [x for x in new_train_data.columns if x.startswith(\"Wilderness_Area\")]\nnew_train_data[\"Wilderness_Area_Count\"] = new_train_data[wilderness_features].sum(axis=1)\nnew_test_data[\"Wilderness_Area_Count\"] = new_test_data[wilderness_features].sum(axis=1)","d7b7fdf8":"# Figure size \nplt.figure(figsize=(12,6))\n\n# Countplot\nsns.countplot(new_train_data['Soil_Type_Count'])\n\n# Aesthetics\nplt.title('Soil type count', fontsize=15)","67b6f946":"# Train df\nnew_train_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_train_data.drop('Soil_Type15', axis=1, inplace=True)\n\n# Test df\nnew_test_data.drop('Soil_Type7', axis=1, inplace=True)\nnew_test_data.drop('Soil_Type15', axis=1, inplace=True)","e5d037c8":"new_train_data.dtypes","6d0e20f2":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","167bf191":"new_train_data=reduce_mem_usage(new_train_data)\nnew_test_data=reduce_mem_usage(new_test_data)","a998c8c4":"# Labels\ny=new_train_data.Cover_Type\n\n# Features\nX=new_train_data.drop('Cover_Type', axis=1)","52297067":"scaler = StandardScaler()\nX=scaler.fit_transform(X)\ntest_data_preprocessed = scaler.transform(new_test_data)","17ea8d36":"# Encode labels to lie in range 0 to 5\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)","279fbdeb":"del train_data, test_data, scaler\ndel pseudo_label_df, new_train_data, new_test_data\ndel fig, axes, i\ndel feature_variables, mask_features, hillshades\ndel soil_features,wilderness_features","88737137":"'''\n# Start timer\nstart = time.time()\n\n# Define model\nRF_model=RandomForestClassifier(n_estimators=100, random_state=0)\n\n# Train classifier\nRF_model.fit(X_train, y_train)\n\n# Make predictions\nRF_preds=RF_model.predict(test_data)\n\n# Stop timer\nstop = time.time()\n\n# Print training and prediction time\nprint(f'Time: {round((stop - start),3)} seconds')\n'''","f8a78b83":"'''\n# Retrieve most likely classes\nRF_pred_classes = np.argmax(RF_preds,axis=1)\n\n# Map predicted classes to Cover_Type classes\nRF_pred_classes+=1\nRF_pred_classes[RF_pred_classes>=5]+=1\n\n# Accuracy on validation set\naccuracy_score(RF_model.predict(X_valid),y_valid)\n'''","bcc5b3e3":"'''\n# Start timer\nstart = time.time()\n\n# Define model\nXGB_model=XGBClassifier(n_estimators=200, learning_rate=0.05, random_state=0)\n\n# Train classifier\nXGB_model.fit(X_train,train_data.Cover_Type[y_train.index])\n\n# Make predictions\nXGB_preds=XGB_model.predict(test_data)\n\n# Stop timer\nstop = time.time()\n\n# Print training and prediction time\nprint(f'Time: {round((stop - start),3)} seconds')\n'''","12fe18c8":"'''\n# Accuracy on validation set\naccuracy_score(XGB_model.predict(X_valid),train_data.Cover_Type[y_valid.index])\n'''","f0f6791c":"'''\n# Start timer\nstart = time.time()\n\n# Define model\nLGBM_model=LGBMClassifier(random_state=0)  #LGBM is faster than XGBoost\n\n# Train classifier\nLGBM_model.fit(X_train,train_data.Cover_Type[y_train.index])\n\n# Make predictions\nLGBM_preds=LGBM_model.predict(test_data)\n\n# Stop timer\nstop = time.time()\n\n# Print training and prediction time\nprint(f'Time: {round((stop - start),3)} seconds')\n'''","3106e01f":"'''\n# Accuracy on validation set\naccuracy_score(LGBM_model.predict(X_valid),train_data.Cover_Type[y_valid.index])\n'''","f509298e":"'''\n# Start timer\nstart = time.time()\n\n# Define model\nCAT_model=CatBoostClassifier(random_state=0) #Catboost is faster than XGBoost\n\n# Train classifier\nCAT_model.fit(X_train,train_data.Cover_Type[y_train.index], verbose=False)\n\n# Make predictions\nCAT_preds=np.squeeze(CAT_model.predict(test_data))\n\n# Stop timer\nstop = time.time()\n\n# Print training and prediction time\nprint(f'Time: {round((stop - start),3)} seconds')\n'''","7f10c190":"'''\n# Accuracy on validation set\naccuracy_score(CAT_model.predict(X_valid),train_data.Cover_Type[y_valid.index])\n'''","086574a3":"# Define model\ndef build_model():\n    model = keras.Sequential([\n\n        # hidden layer 1\n        layers.Dense(units=256, activation='relu', input_shape=[X.shape[1]], kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 2\n        layers.Dense(units=256, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.3),\n\n        # hidden layer 3\n        layers.Dense(units=128, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n        \n        # hidden layer 4\n        layers.Dense(units=64, activation='relu', kernel_initializer='lecun_normal'),\n        layers.Dropout(rate=0.2),\n\n        # output layer\n        layers.Dense(units=6, activation='softmax')\n    ])\n    \n    # Define loss, optimizer and metric\n    model.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n    \n    return model","3e161941":"# Define early stopping callback on validation loss\nearly_stopping = callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=20,\n    restore_best_weights=True,\n)\n\n# Reduce learning rate when validation loss plateaus\nreduce_lr = callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)","e9371aef":"FOLDS = 10\nEPOCHS = 200\nBATCH_SIZE = 250\n\ntest_preds = np.zeros((1, 1))\nscores = []\n\ncv = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=0)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n    # Start timer\n    start = time.time()\n    \n    # get training and validation sets\n    X_train, X_valid = X[train_idx], X[val_idx]\n    y_train, y_valid = y[train_idx], y[val_idx]\n\n    # Build and train model on tpu\n    model = build_model()\n    model.fit(\n        X_train,\n        y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=EPOCHS,\n        batch_size=BATCH_SIZE,\n        callbacks=[early_stopping, reduce_lr],\n        verbose=False\n    )\n\n    # Make predictions and get measure accuracy\n    y_pred = np.argmax(model.predict(X_valid), axis=1)\n    score = accuracy_score(y_valid, y_pred)\n    scores.append(score)\n    \n    # Store predictions\n    test_preds = test_preds + model.predict(test_data_preprocessed)\n    \n    # Stop timer\n    stop = time.time()\n    \n    # Print accuracy and time\n    print(f\"Fold {fold} - Accuracy: {score}, Time: {round((stop - start)\/60,1)} mins\")\n    \nprint('')\nprint(f\"Mean Accuracy: {np.mean(scores)}\")","1accd23c":"# Soft voting to ensemble predictions\ntest_preds = np.argmax(test_preds, axis=1)\n\n# Recover class labels\npred_classes = encoder.inverse_transform(test_preds)","7d1ecd02":"# Save new predictions to df\nnew_test_preds_df=pd.DataFrame({'Id': new_test_data_index, \n                                'Cover_Type': pred_classes})\n\n# Concatenate with pseudolabels\nfinal_preds=pd.concat([new_test_preds_df, pseudo_label_preds_df])\n\n# Sort by id\nfinal_preds=final_preds.sort_values(by='Id', ascending=True)\n\n# Check format\nfinal_preds.head(10)","148e7662":"# Save to csv\nfinal_preds.to_csv('submission.csv', index=False)","614a0738":"# Data","5815c137":"# XGBoost","e9813a80":"Aspect values represent angles between 0 and 360 degrees so we should project them onto [0,360] to make any patterns easier to learn.","c1f3ee4d":"**Labels and features:**","7e2ba265":"This suggests the continuous features are fairly independent of each other, which is good.","ea716a7c":"# Neural network","f78d4d97":"# Submission","cbee7097":"Update: I am using [Remek's](https:\/\/www.kaggle.com\/remekkinas) pseudolabels because his are better haha.","c496362e":"**Aspect**","73b153f9":"We have the horizontal and vertical distances to Hydrology so we can use these to calculate the l1 (aka Manhattan) or euclidean distance.","f7a800a2":"**Drop label 5**","43e2afc6":"The distribution is far from uniform which could bias our models. This is especially important if the test data does not have a similar distribution of labels.","15ea867a":"**Observations:**\n* There aren't enough points in class 5 to plot a distribution. This means it will the hardest to predict.\n* Elavation seems to be the best feature for separating the classes. The other features all have similar in-class distributions.","36645c50":"**Feature correlations**","8744f56a":"**Reduce memory usage**","3ef5f0da":"**Explore label distribution**","98009fc8":"**Drop features with 0 variance**","de8be0b1":"**Observations:**\n* Aspect is a compass direction and so values should lie between 0 to 360 degrees (see [here](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data)).\n* Several features have negative values when they shouldn't. ","aee2bd36":"**Callbacks**","7f02479a":"**Number of soil & wilderness types**","90ee35d2":"**Remove unwanted negative values**","17a060e6":"Everything is int64.","100060cf":"In other scripts, I trained 5 classifiers (RF, XGB, LGBM, CAT, ANN) to make predictions. I saved the predictions that all models agreed on to a dataframe (pseudo_label_df). I will add these samples (originally from the test set) to the training set. I will then train an ANN on this new bigger training set to hopefully increase its acurracy and predict the labels for the remaing test set. ","ba0a4315":"# LightGBM","b04bea1a":"# Catboost","2a68522f":"For the features below it does not make physical sense to include negative numbers.","ff7603f5":"# Random Forest","d765b96a":"# EDA","2b378af1":"**Check for null values**","87349f54":"**Hillshade**","8cb7ac85":"The objective to predict the cover type of a forest given features like elavation, soil type etc. There are 7 different cover types to predict in total.\n\n![https:\/\/th.bing.com\/th\/id\/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1](https:\/\/th.bing.com\/th\/id\/OIP.PcAN1kc44gDpHowTie715gHaD4?pid=ImgDet&rs=1)","9cb22e74":"# Memory","d428c446":"**Distribution of continuous features**","bf1321c8":"**Acknowledgments:**\n* [Confusion matrices](https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart) by [AmbrosM](https:\/\/www.kaggle.com\/ambrosm).\n* [Feature engineering](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373) by [Gulshan Mishra](https:\/\/www.kaggle.com\/gulshanmishra).\n* [Memory usage](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844) by [Luca Massaron](https:\/\/www.kaggle.com\/lucamassaron).\n* [Ensembling](https:\/\/www.kaggle.com\/odins0n\/tps-dec-eda-modeling\/notebook#Modeling) by [Sanskar Hasija\n](https:\/\/www.kaggle.com\/odins0n).\n* [Pseudolabelling](https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95661\/notebook) by [Remek Kinas](https:\/\/www.kaggle.com\/remekkinas).","7de3e011":"**Data types**","b44ef2c7":"Wow, there is only 1 sample with the label 5, compared to millions of samples with label 2. Let's drop the entry with label 5; one sample is not enough information to accurately predict that class.","e1720e2f":"**Distance to Hydrology**","42de201c":"# Pre-process data","310bb61a":"4 million training samples is a lot! I would expect a neural network to do well on this dataset so I will try that first.\n\nNote also that all the data is numeric. In particular the first 10 features are continuous whereas the rest are binary categorical. The categorical features follow a one-hot encoding scheme. There are 4 types of wilderness areas and 40 soil types. ","b13fc66c":"**Scale data**","57d8c0db":"# Introduction","825e01f3":"**Save memory**","e888c148":"# Cross validation","2dbde02b":"From [ArcMap](https:\/\/desktop.arcgis.com\/en\/arcmap\/10.3\/manage-data\/raster-and-images\/hillshade-function.htm): \"A hillshade is a grayscale 3D representation of the surface, with the sun's relative position taken into account for shading the image.\" \n\nThis means all Hillshade values should lie in the range [0, 255] because it corresponds to a greyscale image.","15878f3e":"# Feature engineering","d8a4274d":"**Soft voting**","8aeaf234":"# Pseudolabeling","ebe2387c":"**Label encoding**","841fa1f5":"**Distribution by label**","b9d624b3":"**Summary of data**","fe7d05d7":"# Libraries"}}