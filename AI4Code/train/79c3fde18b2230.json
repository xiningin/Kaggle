{"cell_type":{"d21504c6":"code","7ad3859d":"code","d8a73c14":"code","6abbe5d5":"code","a07acfc0":"code","d4f1a01f":"code","fed3fac6":"code","d5e1c399":"code","100a47a1":"code","a5239725":"code","a70e2f22":"code","af417221":"code","3ce53726":"code","b5d0f1f1":"code","7c5210b0":"code","dbad2f86":"code","2c6fb988":"code","95ca85bd":"code","a24bb393":"code","1fc60667":"code","5c9f851e":"code","b4da9ccf":"code","ec2dc6f7":"code","7b5e31d3":"code","5ba9030e":"code","71184546":"code","034052ad":"code","393bacb7":"code","baffff5d":"code","796b745e":"code","62fbe76c":"code","600e5986":"code","e6032406":"code","592ab36b":"code","f0b2ba46":"code","84a09966":"code","0690e5c8":"code","b1aeb6db":"code","528a55d6":"code","8df466b6":"code","4239272d":"code","205e51a1":"code","585f05ae":"code","d6d0ed36":"code","55f4156b":"code","bf63c612":"code","330ff6df":"code","7b8a79b6":"code","bba8958a":"code","45fd4425":"code","d6ddf620":"code","5e327310":"code","8c058681":"code","1c458576":"code","474ac66f":"code","98ee708f":"code","cb07e6b4":"code","c53e7c32":"code","adb8d3ca":"code","2752f1ea":"code","997ad288":"code","7ff81246":"code","517d8bd9":"code","770ccf68":"code","97976792":"code","fcc87bca":"code","d195374a":"code","0357ec94":"code","68866cc5":"code","291a8824":"code","382cefb2":"markdown","16b79fc7":"markdown","de364f3e":"markdown","1a752e42":"markdown","3299d32a":"markdown","d6f2e857":"markdown","903283cd":"markdown","e1720d21":"markdown","1bcdd7cc":"markdown","d926d76e":"markdown","805f14ad":"markdown","f97aeeed":"markdown","1ff483a5":"markdown","2b9120d3":"markdown","7b0ec3b4":"markdown","60bc5bd6":"markdown"},"source":{"d21504c6":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import he_normal, he_uniform, glorot_normal, glorot_uniform\nfrom keras import backend as K\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Bidirectional, CuDNNGRU, GlobalAveragePooling1D\nfrom keras.layers import CuDNNLSTM, GlobalMaxPooling1D, concatenate, Input, Dropout, SpatialDropout1D\nfrom keras.optimizers import Adam\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nimport time\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom statistics import mean, median, stdev\nfrom numpy import amax, amin\n\nimport gc\nimport time\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport operator \nimport re\nimport os\nimport math","7ad3859d":"batch_size = 1024\nepochs = 18\ncurrent_embd = \"Glove\"\nquestion_length =  100\nmax_eval_size = 15000\n#max_features = 50000\n\n# Cesc: Seed utilitzada pel K-Fold\nDATA_SPLIT_SEED = 2018\nK_FOLDS = 5\nK_FOLD_EPOCHS = int(epochs\/K_FOLDS)\n\nseed_nb=14\nnp.random.seed(seed_nb)\ntf.set_random_seed(seed_nb)","d8a73c14":"paraules_prohibides = ['2g1c', '2 girls 1 cup', 'acrotomophilia', 'alabama hot pocket', 'alaskan pipeline', 'anal', 'anilingus', 'anus',\n                       'apeshit', 'arsehole', 'ass', 'asshole', 'assmunch', 'auto erotic', 'autoerotic', 'babeland', 'baby batter',\n                       'baby juice', 'ball gag', 'ball gravy', 'ball kicking', 'ball licking', 'ball sack', 'ball sucking', 'bangbros',\n                       'bareback', 'barely legal', 'barenaked', 'bastard', 'bastardo', 'bastinado', 'bbw', 'bdsm', 'beaner', 'beaners',\n                       'beaver cleaver', 'beaver lips', 'bestiality', 'big black', 'big breasts', 'big knockers', 'big tits', 'bimbos',\n                       'birdlock', 'bitch', 'bitches', 'black cock', 'blonde action', 'blonde on blonde action', 'blowjob', 'blow job',\n                       'blow your load', 'blue waffle', 'blumpkin', 'bollocks', 'bondage', 'boner', 'boob', 'boobs', 'booty call',\n                       'brown showers', 'brunette action', 'bukkake', 'bulldyke', 'bullet vibe', 'bullshit', 'bung hole', 'bunghole',\n                       'busty', 'butt', 'buttcheeks', 'butthole', 'camel toe', 'camgirl', 'camslut', 'camwhore', 'carpet muncher',\n                       'carpetmuncher', 'chocolate rosebuds', 'circlejerk', 'cleveland steamer', 'clit', 'clitoris', 'clover clamps',\n                       'clusterfuck', 'cock', 'cocks', 'coprolagnia', 'coprophilia', 'cornhole', 'coon', 'coons', 'creampie', 'cum',\n                       'cumming', 'cunnilingus', 'cunt', 'darkie', 'date rape', 'daterape', 'deep throat', 'deepthroat', 'dendrophilia',\n                       'dick', 'dildo', 'dingleberry', 'dingleberries', 'dirty pillows', 'dirty sanchez', 'doggie style', 'doggiestyle',\n                       'doggy style', 'doggystyle', 'dog style', 'dolcett', 'domination', 'dominatrix', 'dommes', 'donkey punch',\n                       'double dong', 'double penetration', 'dp action', 'dry hump', 'dvda', 'eat my ass', 'ecchi', 'ejaculation',\n                       'erotic', 'erotism', 'escort', 'eunuch', 'faggot', 'fecal', 'felch', 'fellatio', 'feltch', 'female squirting',\n                       'femdom', 'figging', 'fingerbang', 'fingering', 'fisting', 'foot fetish', 'footjob', 'frotting', 'fuck',\n                       'fuck buttons', 'fuckin', 'fucking', 'fucktards', 'fudge packer', 'fudgepacker', 'futanari', 'gang bang',\n                       'gay sex', 'genitals', 'giant cock', 'girl on', 'girl on top', 'girls gone wild', 'goatcx', 'goatse', 'god damn',\n                       'gokkun', 'golden shower', 'goodpoop', 'goo girl', 'goregasm', 'grope', 'group sex', 'g-spot', 'guro', 'hand job',\n                       'handjob', 'hard core', 'hardcore', 'hentai', 'homoerotic', 'honkey', 'hooker', 'hot carl', 'hot chick', 'how to kill',\n                       'how to murder', 'huge fat', 'humping', 'incest', 'intercourse', 'jack off', 'jail bait', 'jailbait', 'jelly donut',\n                       'jerk off', 'jigaboo', 'jiggaboo', 'jiggerboo', 'jizz', 'juggs', 'kike', 'kinbaku', 'kinkster', 'kinky', 'knobbing',\n                       'leather restraint', 'leather straight jacket', 'lemon party', 'lolita', 'lovemaking', 'make me come', 'male squirting',\n                       'masturbate', 'menage a trois', 'milf', 'missionary position', 'motherfucker', 'mound of venus', 'mr hands', 'muff diver',\n                       'muffdiving', 'nambla', 'nawashi', 'negro', 'neonazi', 'nigga', 'nigger', 'nig nog', 'nimphomania', 'nipple', 'nipples',\n                       'nsfw images', 'nude', 'nudity', 'nympho', 'nymphomania', 'octopussy', 'omorashi', 'one cup two girls', 'one guy one jar',\n                       'orgasm', 'orgy', 'paedophile', 'paki', 'panties', 'panty', 'pedobear', 'pedophile', 'pegging', 'penis', 'phone sex',\n                       'piece of shit', 'pissing', 'piss pig', 'pisspig', 'playboy', 'pleasure chest', 'pole smoker', 'ponyplay', 'poof',\n                       'poon', 'poontang', 'punany', 'poop chute', 'poopchute', 'porn', 'porno', 'pornography', 'prince albert piercing',\n                       'pthc', 'pubes', 'pussy', 'queaf', 'queef', 'quim', 'raghead', 'raging boner', 'rape', 'raping', 'rapist', 'rectum',\n                       'reverse cowgirl', 'rimjob', 'rimming', 'rosy palm', 'rosy palm and her 5 sisters', 'rusty trombone', 'sadism',\n                       'santorum', 'scat', 'schlong', 'scissoring', 'semen', 'sexo', 'sexy', 'shaved beaver', 'shaved pussy',\n                       'shemale', 'shibari', 'shit', 'shitblimp', 'shitty', 'shota', 'shrimping', 'skeet', 'slanteye', 'slut', 's&m',\n                       'smut', 'snatch', 'snowballing', 'sodomize', 'sodomy', 'spic', 'splooge', 'splooge moose', 'spooge', 'spread legs',\n                       'spunk', 'strap on', 'strapon', 'strappado', 'strip club', 'style doggy', 'suck', 'sucks', 'suicide girls', 'sultry women',\n                       'swastika', 'swinger', 'tainted love', 'taste my', 'tea bagging', 'threesome', 'throating', 'tied up', 'tight white',\n                       'tit', 'tits', 'titties', 'titty', 'tongue in a', 'topless', 'tosser', 'towelhead', 'tranny', 'tribadism', 'tub girl',\n                       'tubgirl', 'tushy', 'twat', 'twink', 'twinkie', 'two girls one cup', 'undressing', 'upskirt', 'urethra play', 'urophilia',\n                       'vagina', 'venus mound', 'vibrator', 'violet wand', 'vorarephilia', 'voyeur', 'vulva', 'wank', 'wetback', 'wet dream',\n                       'white power', 'wrapping men', 'wrinkled starfish', 'xx', 'xxx', 'yaoi', 'yellow showers', 'yiffy', 'zoophilia', '\ud83d\udd95']\n","6abbe5d5":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntest['target']=-1\ndf = pd.concat([train ,test])","a07acfc0":"del train, test; gc.collect(); time.sleep(5)","d4f1a01f":"def load_embed(file):\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    \n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file)) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file, encoding='latin')))\n        \n    return embeddings_index","fed3fac6":"embed_glove = load_embed('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt')","d5e1c399":"embed_paragram = load_embed('..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt')","100a47a1":"#embed_fasttext = load_embed('..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec')","a5239725":"my_embedding_matrix = dict()\n\nfor k1,v1 in embed_glove.items():\n    my_val = v1\n    if k1 in embed_paragram.keys():\n            my_val = (v1 + embed_paragram[k1])\/2\n    my_embedding_matrix[k1] = my_val\n\nfor k1,v1 in embed_paragram.items():\n    if k1 not in embed_glove.keys():\n        my_embedding_matrix[k1] = v1","a70e2f22":"print(len(my_embedding_matrix))\nprint(len(embed_glove))\nprint(len(embed_paragram))","af417221":"print(my_embedding_matrix[\"dog\"][0])\nprint(embed_glove[\"dog\"][0])\nprint(embed_paragram[\"dog\"][0])","3ce53726":"#if current_embd == \"Glove\":\n#    glove = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n#    print(\"Extracting GloVe embedding\")\n#    embed_glove = load_embed(glove)\n#elif current_embd == \"Paragram\":\n#    paragram =  '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt'\n#    print(\"Extracting Paragram embedding\")\n#    embed_paragram = load_embed(paragram)\n#elif current_embd == \"FastText\":\n#    wiki_news = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\n#    print(\"Extracting FastText embedding\")\n#    embed_fasttext = load_embed(wiki_news)","b5d0f1f1":"df['size'] = df['question_text'].str.len()\nprint(mean(df['size']))\nprint(median(df['size']))\nprint(stdev(df['size']))\nprint(amax(df['size']))\nprint(amin(df['size']))\ndf = df.drop(['size'], axis=1)","7c5210b0":"# Funci\u00f3 per crear el Vocabulari\ndef build_vocab(texts):\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in sentences:\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab","dbad2f86":"vocab = build_vocab(df['question_text'])","2c6fb988":"print(vocab[\"Apple\"])","95ca85bd":"# Convertir en minuscules les paraules\ndf['question_text'] = df['question_text'].apply(lambda x: x.lower())","a24bb393":"# Afegim les paraules amb minuscules als altres diccionaris\ndef add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1","1fc60667":"print(time.strftime(\"%Y-%m-%d %H:%M\"))","5c9f851e":"df['conte_paraula_prohibida']=0","b4da9ccf":"for index, row in df.iterrows():\n    if (index % 1000) == 0:\n        print(\"index: \",  index)\n    for w in paraules_prohibides:\n        if w in row['question_text']:\n            row['conte_paraula_prohibida']=1\n            break;","ec2dc6f7":"print(time.strftime(\"%Y-%m-%d %H:%M\"))","7b5e31d3":"if \"sexo\" in paraules_prohibides:\n    print(\"OK\")\nif \"casa\" in paraules_prohibides:\n    print(\"OK2\")    ","5ba9030e":"m=0\nfor index, row in df.iterrows():\n    print(row)\n    if row['conte_paraula_prohibida']==\"1\":\n        print(row['question_text'])\n        print(row['target'])\n        m = m+1\n    if m >= 20:\n        break;","71184546":"print(list(df))","034052ad":"for index, row in df.iterrows():\n    if row['conte_paraula_prohibida']==1 & index < 20:\n        print (row['question_text'])","393bacb7":"#print(my_embedding_matrix)\n","baffff5d":"#if current_embd == \"Glove\":\n#    add_lower(embed_glove, vocab)\n#elif current_embd == \"Paragram\":\n#    add_lower(embed_paragram, vocab)\n#elif current_embd == \"FastText\":\n#    add_lower(embed_fasttext, vocab)\n\n#add_lower(embed_glove, vocab)\n#add_lower(embed_paragram, vocab)\nadd_lower(my_embedding_matrix, vocab)","796b745e":"print(len(my_embedding_matrix))\nprint(len(vocab))\nprint(type(my_embedding_matrix))\nprint(type(vocab))","62fbe76c":"my_embedding_matrix = {k: v for k, v in my_embedding_matrix.items() if k in vocab}","600e5986":"print(my_embedding_matrix[\"dog\"][0])","e6032406":"del embed_paragram, embed_glove, vocab; gc.collect(); time.sleep(5)","592ab36b":"# Contraccions\n# Cesc: He afegit canvis en monedes\n\n#contraction_mapping = {\"euros\" : \"eur\", \"dollars\": \"usd\", \"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\n#Ho desfem perqu\u00e8 sembla que no xuta\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }","f0b2ba46":"# Netejar contraccions\ndef clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","84a09966":"df['question_text'] = df['question_text'].apply(lambda x: clean_contractions(x, contraction_mapping))","0690e5c8":"# Caracters especials de puntuaci\u00f3\npunct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n\n# Cesc: Afegim uns quants caracters mes que he trobat#\n#mes_punct = \"[,.:)(-!?|;$&\/[]>%=#*+\\\\\u2022~@\u00a3\u00b7_{}\u00a9^\u00ae`<\u2192\u00b0\u20ac\u2122\u203a\u2665\u2190\u00d7\u00a7\u2033\u2032\u00c2\u2588\u00bd\u00e0\u2026\u201c\u2605\u201d\u2013\u25cf\u00e2\u25ba\u2212\u00a2\u00b2\u00ac\u2591\u00b6\u2191\u00b1\u00bf\u25be\u2550\u00a6\u2551\u2015\u00a5\u2593\u2014\u2039\u2500\u2592\uff1a\u00bc\u2295\u25bc\u25aa\u2020\u25a0\u2019\u2580\u00a8\u2584\u266b\u2606\u00e9\u00af\u2666\u00a4\u25b2\u00e8\u00b8\u00be\u00c3\u22c5\u2018\u221e\u2219\uff09\u2193\u3001\u2502\uff08\u00bb\uff0c\u266a\u2569\u255a\u00b3\u30fb\u2566\u2563\u2554\u2557\u25ac\u2764\u00ef\u00d8\u00b9\u2264\u2021\u221a])\"","b1aeb6db":"df['size'] = df['question_text'].str.len()\nprint(mean(df['size']))\nprint(median(df['size']))\nprint(stdev(df['size']))\nprint(amax(df['size']))\nprint(amin(df['size']))","528a55d6":"# Reemplacament dels caracters especials\n# Cesc: He afegit canvis en monedes\npunct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"eur\",\"$\": \"usd\",  \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }\n#punct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }","8df466b6":"# Neteja dels caracters especials\ndef clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n\n    for p in punct:\n        text = text.replace(p, f' {p} ')\n        #text = text.replace(p, '')\n    specials = {'\\u200b': ' ', '\u2026': ' ... ', '\\ufeff': '', '\u0915\u0930\u0928\u093e': '', '\u0939\u0948': ''}  # Other special characters that I have to deal with in last\n    for s in specials:\n        text = text.replace(s, specials[s])\n    \n    return text","4239272d":"df['question_text'] = df['question_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))","205e51a1":"# Cesc: Ho apliquem tamb\u00e9 amb la nova llista de caracters raros\n#df['question_text'] = df['question_text'].apply(lambda x: clean_special_chars(x, mes_punct, punct_mapping))","585f05ae":"# Reempla\u00e7ament d'errors ortogr\u00e0fics\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}","d6d0ed36":"# Correcci\u00f3 d'errors ortogr\u00e0fics\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","55f4156b":"df['question_text'] = df['question_text'].apply(lambda x: correct_spelling(x, mispell_dict))","bf63c612":"df['size'] = df['question_text'].str.len()\nprint(mean(df['size']))\nprint(median(df['size']))\nprint(stdev(df['size']))\nprint(amax(df['size']))\nprint(amin(df['size']))\ndf = df.drop(['size'],axis=1)","330ff6df":"total_set_qid = df['qid'].values\ntotal_set_X = df[\"question_text\"].fillna(\"_na_\").values\ntotal_set_y = df['target'].values","7b8a79b6":"## Tokenize the sentences\ntokenizer = Tokenizer(num_words=None, filters='')\ntokenizer.fit_on_texts(list(total_set_X))\ntotal_set_X = tokenizer.texts_to_sequences(total_set_X)","bba8958a":"## Pad the sentences \ntotal_set_X = pad_sequences(total_set_X, maxlen=question_length)\n# TODO: provar amb opcio padding='post' per posar els zeros a la dreta","45fd4425":"df = pd.concat([pd.DataFrame(total_set_qid) ,pd.DataFrame(total_set_X), pd.DataFrame(total_set_y)], axis=1, keys=[\"qid\", \"question_text\", \"target\"])","d6ddf620":"def embedding_matrix_creator(embeddings_index):\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    word_index = tokenizer.word_index\n    nb_words = len(word_index) #min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n    for word, i in word_index.items():\n        #if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    return embedding_matrix","5e327310":"print(len(my_embedding_matrix))\n#print(my_embedding_matrix[\"apple\"])\nprint(type(my_embedding_matrix))","8c058681":"#emb_matrix_fasttext = embedding_matrix_creator(embed_fasttext)\n#emb_matrix_glove = embedding_matrix_creator(embed_glove)\n#print(len(emb_matrix_fasttext))\n#print(len(emb_matrix_glove))\nemb_matrix = embedding_matrix_creator(my_embedding_matrix)","1c458576":"#if current_embd == \"Glove\":\n#    emb_matrix = emdedding_matrix_creator(embed_glove)\n#    del embed_glove\n#elif current_embd == \"Paragram\":\n#    emb_matrix = emdedding_matrix_creator(embed_paragram)\n#    del embed_paragram\n#elif current_embd == \"FastText\":\n#    emb_matrix = emdedding_matrix_creator(embed_fasttext)\n#    del embed_fasttext\n\n# Fem la mitjana de tots els 3 embeddings\n#emb_matrix = np.mean([ emdedding_matrix_creator(embed_glove),  \n#                       emdedding_matrix_creator(embed_paragram),\n#                       emdedding_matrix_creator(embed_fasttext)], axis=1)","474ac66f":"#del embed_glove; del embed_paragram; del embed_fasttext; gc.collect(); time.sleep(10)","98ee708f":"# Creem el train set i el eval set\ntrain_df, val_df = train_test_split(df[df.target[0]!=-1], test_size=0.1)\n\ntrain_X = np.array(train_df[\"question_text\"])\ntrain_y = np.array(train_df[\"target\"])\n\nval_X = np.array(val_df[\"question_text\"])\nval_y = np.array(val_df[\"target\"])","cb07e6b4":"# Creem el test set\ntest_df = df[df.target[0]==-1]\n#test_df = test_df.drop('target', axis=1)\n\ntest_X=np.array(test_df[\"question_text\"])\n#print(test_X.shape)","c53e7c32":"def f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    \n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","adb8d3ca":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n    \n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim","2752f1ea":"def make_old_model(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n    inp    = Input(shape=(question_length,))\n    x      = Embedding(embedding_matrix.shape[0], embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x      = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n    x      = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n    avg_pl = GlobalAveragePooling1D()(x)\n    max_pl = GlobalMaxPooling1D()(x)\n    concat = concatenate([avg_pl, max_pl])\n    dense  = Dense(64, activation=\"relu\")(concat)\n    drop   = Dropout(0.1)(concat)\n    output = Dense(1, activation=\"sigmoid\")(concat)\n    \n    model  = Model(inputs=inp, outputs=output)\n    model.compile(loss=loss, optimizer=Adam(lr=0.0001), metrics=['accuracy', f1])\n    return model","997ad288":"#model = make_model(emb_matrix)","7ff81246":"def model_lstm_gru_atten(embedding_matrix, embed_size=300, loss='binary_crossentropy'):\n    inp = Input(shape=(question_length,))\n    x = Embedding(embedding_matrix.shape[0], embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.1,seed=seed_nb)(x)\n    x = Bidirectional(CuDNNLSTM(64, kernel_initializer=glorot_uniform(seed=seed_nb), return_sequences=True))(x)\n    y = Bidirectional(CuDNNGRU(40,kernel_initializer=glorot_uniform(seed=seed_nb), return_sequences=True))(x)\n\n    atten_1 = Attention(question_length)(x) \n    atten_2 = Attention(question_length)(y)\n    avg_pool = GlobalAveragePooling1D()(y)\n    max_pool = GlobalMaxPooling1D()(y)\n\n    conc = concatenate([atten_1, atten_2, avg_pool, max_pool])\n    conc = Dense(16,kernel_initializer=he_uniform(seed=seed_nb),  activation=\"relu\")(conc)\n    conc = Dropout(0.1,seed=seed_nb)(conc)\n    outp = Dense(1,kernel_initializer=he_uniform(seed=seed_nb),  activation=\"sigmoid\")(conc)    \n\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss=loss, optimizer=Adam(lr=0.0001), metrics=['accuracy', f1])\n    return model","517d8bd9":"#model = model_lstm_gru_atten(emb_matrix)\nmodel = make_old_model(emb_matrix)","770ccf68":"model.summary()","97976792":"checkpoints = ModelCheckpoint('weights.hdf5', monitor=\"val_f1\", mode=\"max\", verbose=True, save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=2, verbose=1, min_lr=0.000001)","fcc87bca":"# Trobar el threshold m\u00e9s \u00f3ptim\ndef tweak_threshold(pred, truth):\n    thresholds = []\n    scores = []\n    print(\"Threshold: Valor\")\n    for thresh in np.arange(0.01, 1.01, 0.01):\n        thresh = np.round(thresh, 2)\n        thresholds.append(thresh)\n        score = f1_score(truth, (pred>thresh).astype(int))\n        print(thresh, \": \", score)\n        scores.append(score)\n    return np.max(scores), thresholds[np.argmax(scores)]","d195374a":"#print(history.history)\n#plt.plot(history.history['acc'])\n#plt.plot(history.history['f1'])","0357ec94":"df_X =  df[df.target[0]!=-1][\"question_text\"]\ndf_y =  df[df.target[0]!=-1][\"target\"]\n\n# Cesc: Afegim la separacio de K-Folds\ntrain_meta = np.zeros(df_y.shape)\ntest_meta = np.zeros(test_X.shape[0])\n\n#K_FOLDS = 4\n#K_FOLD_EPOCHS = 1 #int(epochs\/K_FOLDS)\nmy_splits = list(StratifiedKFold(n_splits=K_FOLDS,\n                                  shuffle=True,\n                                  random_state=DATA_SPLIT_SEED).split(df_X, df_y))\n\nfor idx, (train_idx, valid_idx) in enumerate(my_splits):\n    print(\"======== K-FOLD: {0} ==========\".format(idx))\n    train_X = df_X.iloc[train_idx]\n    train_y = df_y.iloc[train_idx]\n    val_X = df_X.iloc[valid_idx]\n    val_y = df_y.iloc[valid_idx]\n#    model = model_lstm_gru_atten(emb_matrix)\n    model = make_old_model(emb_matrix)\n#   pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = 8, callback = [clr,])\n    history = model.fit(train_X, train_y, batch_size=batch_size, epochs=K_FOLD_EPOCHS, validation_data=[val_X, val_y], callbacks=[checkpoints, reduce_lr])\n    model.load_weights('weights.hdf5')\n    eval_pred = model.predict(val_X, batch_size=batch_size, verbose=1)\n    test_pred = model.predict(test_X, batch_size=batch_size, verbose=1)\n   # pred_val_y = model.predict([val_X], batch_size=batch_size, verbose=0)\n    train_meta[valid_idx] = eval_pred#.reshape(-1)\n    test_meta += test_pred.reshape(-1) \/ len(my_splits)","68866cc5":"score_val, best_thresh = tweak_threshold(train_meta, df_y)\nprint(\"=====================================\")\nprint(f\"Scored {round(score_val, 4)} for threshold {best_thresh} with untreated texts on validation data\")","291a8824":"# Imprimim la submission en un fitxer\ny_te = (np.array(test_meta) > best_thresh).astype(np.int)\nqid = test_df[\"qid\"].values\n#submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n#submit_df = pd.concat([pd.DataFrame(qid),pd.DataFrame(y_te)], axis = 1, keys=[\"qid\", \"prediction\"])\nsubmit_df = pd.concat([pd.DataFrame(qid, columns=['qid']),pd.DataFrame(y_te, columns=['prediction'])], axis = 1)\nsubmit_df.to_csv(\"submission_cesc.csv\", index=False)","382cefb2":"# Output","16b79fc7":"# Training","de364f3e":"# Constants i Hiperparametres","1a752e42":"# Model 2","3299d32a":"# Creaci\u00f3 de la matriu d' embeddings","d6f2e857":"# Separaci\u00f3 de Sets","903283cd":"# Neteja de dades","e1720d21":"# Llibreries","1bcdd7cc":"# Callbacks","d926d76e":"# Model 1","805f14ad":"# C\u00e0rrega de dades","f97aeeed":"# Custom Attention Layer","1ff483a5":"# Tokenitzaci\u00f3","2b9120d3":"# Training, Eval Prediction & Test Prediction","7b0ec3b4":"# Carregant embeddings","60bc5bd6":"## F1 Score"}}