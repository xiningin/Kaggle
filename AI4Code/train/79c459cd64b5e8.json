{"cell_type":{"6b138e59":"code","eb6a0711":"code","1f2c3c4a":"code","0918167b":"code","adc2e579":"code","4d5cfe14":"code","5b13c3c2":"code","42130750":"code","a25ce880":"code","b744554d":"code","71176b4d":"markdown","eab868ea":"markdown","7d5ab796":"markdown","ae5a3b65":"markdown","d0c20d77":"markdown"},"source":{"6b138e59":"import pandas as pd\nimport numpy as np\nimport json\nimport math\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras import layers, models, Input, backend as K\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport keras_tuner as kt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nprint(tf.__version__)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = os.path.join(dirname, filename)\n        folder_path = dirname\n        print(file_path)","eb6a0711":"# TPU or GPU detection\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","1f2c3c4a":"# Step 1: Get the credential from the Cloud SDK\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\n\n# Step 2: Set the credentials\nuser_secrets.set_tensorflow_credential(user_credential)\n\n# Step 3: Use a familiar call to get the GCS path of the dataset\nfrom kaggle_datasets import KaggleDatasets\nGCS_DS_PATH = KaggleDatasets().get_gcs_path(\"head-orientation-digested-all-features\")\nprint(GCS_DS_PATH)","0918167b":"config = {\n    \"height\": 720,\n    \"width\": 1280\n}\nBATCH_SIZE = 10000 * REPLICAS # 525081 \/\/ REPLICAS #\nprint(BATCH_SIZE)","adc2e579":"def parse_tfrecord(serialized_example):\n    feature_description = {\n        'input': tf.io.FixedLenFeature((), tf.string),\n        'a_out': tf.io.FixedLenFeature((), tf.int64),\n        'b_out': tf.io.FixedLenFeature((), tf.int64),\n        'c_out': tf.io.FixedLenFeature((), tf.int64),\n        'z_out': tf.io.FixedLenFeature((), tf.int64)\n    }\n\n    example = tf.io.parse_single_example(serialized_example, feature_description)\n\n    return tf.reshape(tf.io.parse_tensor(example['input'], out_type=tf.float32), (10,)), (\n        tf.reshape(example[\"a_out\"], (1,)),\n        tf.reshape(example[\"b_out\"], (1,)),\n        tf.reshape(example[\"c_out\"], (1,)),\n        tf.reshape(example[\"z_out\"], (1,)))\n\ntfrecord_dataset = tf.data.TFRecordDataset(f\"{GCS_DS_PATH}\/tfrecords-digested-all-features.tfrec\")\ndataset = tfrecord_dataset.map(parse_tfrecord).cache()\n\nSAMPLE_COUNT = 525081 # dataset.reduce(np.int64(0), lambda x, _: x + 1)\ntrain_size = int(SAMPLE_COUNT * 0.6)\nval_size = int(SAMPLE_COUNT * 0.2)\ntest_size = int(SAMPLE_COUNT * 0.2)\n\ndataset = dataset.shuffle(SAMPLE_COUNT).cache()\ntrain_dataset = dataset.take(train_size).cache()\nval_dataset = dataset.skip(train_size).take(val_size).cache()\ntest_dataset = dataset.skip(train_size + val_size).take(test_size).cache()\n\nprint(\"There are {} sample in train_dataset\".format(train_dataset.batch(1000).reduce(np.int64(0), lambda x, _: x + 1000)) )\nprint(\"There are {} sample in val_dataset\".format(val_dataset.batch(1000).reduce(np.int64(0), lambda x, _: x + 1000)) )\nprint(\"There are {} sample in test_dataset\".format(test_dataset.batch(1000).reduce(np.int64(0), lambda x, _: x + 1000)) )","4d5cfe14":"class TotalAccuracy(keras.callbacks.Callback):\n    def __init__(self):\n        super(TotalAccuracy, self).__init__()\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_epoch_end(self, epoch, logs={}):\n        #logs['accuracy'] = (logs[\"output_1_accuracy\"] + logs[\"output_2_accuracy\"] + logs[\"output_3_accuracy\"] + logs[\"output_4_accuracy\"]) \/ 4\n        #logs['val_accuracy'] = (logs[\"val_output_1_accuracy\"] + logs[\"val_output_2_accuracy\"] + logs[\"val_output_3_accuracy\"] + logs[\"val_output_4_accuracy\"]) \/ 4\n\n        logs['accuracy'] = (logs[\"a_out_accuracy\"] + logs[\"b_out_accuracy\"] + logs[\"c_out_accuracy\"] + logs[\"z_out_accuracy\"]) \/ 4\n        logs['val_accuracy'] = (logs[\"val_a_out_accuracy\"] + logs[\"val_b_out_accuracy\"] + logs[\"val_c_out_accuracy\"] + logs[\"val_z_out_accuracy\"]) \/ 4\n\n\nclass HelmetOrientation(keras.Model):\n    def __init__(self, hp=None):\n        super(HelmetOrientation, self).__init__()\n\n        if hp:\n            self._layers = []\n            for l in range(hp.Int('layers', 1, 4)):\n                units = hp.Int(f'units_d{l+1}', min_value=64, max_value=8192, step=64)\n                dense_layer = layers.Dense(units, activation='relu', name=f\"dense_{l+1}\")\n                self._layers.append(dense_layer)\n        else:\n            self._layers = [layers.Dense(128, activation='relu', name=\"dense_1\"),\n                            layers.Dense(1024, activation='relu', name=\"dense_2\"),\n                            layers.Dense(5120, activation='relu', name=\"dense_3\")]\n\n        self.a_out = layers.Dense(17, activation='softmax', name='a_out')\n        self.b_out = layers.Dense(27, activation='softmax', name='b_out')\n        self.c_out = layers.Dense(71, activation='softmax', name='c_out')\n        self.d_out = layers.Dense(29, activation='softmax', name='d_out')\n\n        if hp:\n            hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n            self.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                         loss=('sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy'),\n                         metrics=[\"accuracy\"])\n        else:\n            self.compile(optimizer=\"RMSprop\",\n                         loss=('sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy',\n                               'sparse_categorical_crossentropy'),\n                         metrics=[\"accuracy\"])\n\n    def summary(self, **kwargs):\n        x = Input(shape=(10, ))\n        model = keras.Model(inputs=[x], outputs=self.call(x))\n        return model.summary(kwargs)\n\n    def call(self, inputs, training=True, **kwargs):\n        x = inputs\n\n        for l in self._layers:\n            x = l(x)\n\n        a = self.a_out(x)\n        b = self.b_out(x)\n        c = self.c_out(x)\n        d = self.d_out(x)\n\n        return a, b, c, d","5b13c3c2":"def get_model(hp=None):\n    input_layer = Input(shape=(10,), name=\"input\")\n    x = input_layer\n    \n    if hp:\n        for l in range(hp.Int('layers', 1, 4)):\n            units = hp.Int(f'units_d{l+1}', min_value=64, max_value=8192, step=64)\n            x = layers.Dense(units, activation='relu', name=f\"dense_{l+1}\")(x)\n    else:\n        x = layers.Dense(128, activation='relu', name=\"dense_1\")(x)\n        x = layers.Dense(1024, activation='relu', name=\"dense_2\")(x)\n        x = layers.Dense(5120, activation='relu', name=\"dense_3\")(x)\n\n    a_out = layers.Dense(17, activation='softmax', name='a_out')(x)\n    b_out = layers.Dense(27, activation='softmax', name='b_out')(x)\n    c_out = layers.Dense(71, activation='softmax', name='c_out')(x)\n    z_out = layers.Dense(29, activation='softmax', name='z_out')(x)\n\n    model = models.Model(input_layer, [a_out, b_out, c_out, z_out])\n    \n    if hp:\n        hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n        model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                      loss=('sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy'),\n                      metrics=[\"accuracy\"])\n    else:\n        model.compile(optimizer=\"RMSprop\",\n                      loss=('sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy',\n                            'sparse_categorical_crossentropy'),\n                      metrics=[\"accuracy\"])\n    \n    return model","42130750":"def testModel(model, test_dataset):\n    eval_dict = model.evaluate(test_dataset.batch(BATCH_SIZE), return_dict=True)\n    for key, value in eval_dict.items():\n        print(f'{key}: {value}')\n\ndef plotHistory(history):\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","a25ce880":"BATCH_SIZE = 10000\nK.clear_session()\nwith strategy.scope():\n    baseline_model = get_model()\n    baseline_model.load_weights(\"\/kaggle\/input\/head-orientation-model\/head_orientation_estimator.h5\")\n    stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=500)\n    baseline_history = baseline_model.fit(dataset.repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE),\n                                          steps_per_epoch=train_size \/\/ BATCH_SIZE,\n                                          validation_data=val_dataset.repeat().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE),\n                                          validation_steps=val_size \/\/ BATCH_SIZE,\n                                          callbacks=[TotalAccuracy(), stop_early],\n                                          epochs=5000)\n\nbaseline_model.save_weights(\"head_orientation_estimator.h5\")","b744554d":"testModel(baseline_model, test_dataset)\nplotHistory(baseline_history)","71176b4d":"# Dataset\nReading the dataset and parsing the TFRecord samples","eab868ea":"# Configs\nSome simple configs","7d5ab796":"# TPU\nUse a TPU is it is present and accessible. Otherwise, use a fallback strategy.","ae5a3b65":"# GCS\nInisialize accessing the dataset using a GCS bucket","d0c20d77":"# Model and training\nCreating a model and training it using the dataset"}}