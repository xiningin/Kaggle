{"cell_type":{"b7ba1a14":"code","bc1107f5":"code","e5b1be81":"code","2c6ba457":"code","b3a917d8":"code","546ecd14":"code","efb783b4":"code","37bb0714":"code","ea266723":"code","7fc3ddd4":"code","c3d007b9":"code","12945559":"code","c274a7bf":"code","dfcc6bd8":"code","36d90072":"code","cd8aef91":"code","ad2ce790":"code","b1e5a609":"code","2b05e2ad":"code","b2006637":"code","3d774b65":"code","629ecfd5":"code","646b3a57":"code","23d8bd0f":"code","9170f66f":"code","8c3b757f":"code","82ebf78a":"code","5866674d":"code","5d0a923e":"code","4e1eb1a8":"code","894c2285":"code","c61da251":"code","3d62671f":"code","4a6eba65":"code","89bef6eb":"code","716e4e7e":"code","3545b92b":"code","8b8b9303":"code","a20aa250":"code","5ceb896f":"code","b72516b2":"code","f7fb5a1d":"code","45390c25":"code","133631d1":"code","407c4879":"code","0c75e0be":"code","f7c16bc5":"code","51e7f374":"code","feba1f0e":"code","084e89c2":"code","30ba7166":"code","eed6c914":"code","5260ae0d":"code","39d7cf3c":"code","aa06c064":"code","0a8aa843":"code","f1759369":"code","49d37628":"code","fd14a297":"code","1f42138f":"code","a3f482b0":"markdown","edf3c2f2":"markdown","7175bb42":"markdown","f360c80c":"markdown","1bfbf905":"markdown","0e9f14d9":"markdown","e4d33169":"markdown","926078ae":"markdown","7d34abb3":"markdown","d349ae7a":"markdown","028cd158":"markdown"},"source":{"b7ba1a14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc1107f5":"!pip install tensorflow-text\n!pip install bert-tensorflow==1.0.1\n!pip install -q tf-models-official==2.4.0\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","e5b1be81":"import re\nimport nltk\nimport tensorflow_text\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem import PorterStemmer\nimport tensorflow as tf\nlem = WordNetLemmatizer()\nps = PorterStemmer()\nfrom sklearn.model_selection import train_test_split","2c6ba457":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense,Input,GlobalMaxPooling1D\nfrom keras.layers import Conv1D,MaxPooling1D,Embedding,Bidirectional\nfrom keras.layers import LSTM,Dropout\nfrom keras import regularizers\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau, TensorBoard\nfrom sklearn.metrics import roc_auc_score","b3a917d8":"# some configuration\nmax_sequence_length = 100\nmax_vocab_size = 20000\nembedding_dim = 300\nvalidation_split = 0.2\nbatch_size = 128\nepoch = 5\n\nsizes ={\"tiny\":16,\"mini\":32,\"small\":64,\"medium\":128,\"large\":256,\"grand\":512}\nsize = \"tiny\"","546ecd14":"df = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\ndf2 = df.copy()\ndf_test = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')","efb783b4":"df.head()\ndf_test.head()","37bb0714":"df.info()","ea266723":"len(df.comment_text.max())","7fc3ddd4":"df.describe()","c3d007b9":"column_list = [f for f in df.columns if df.dtypes[f] != 'object']\ndfP = pd.DataFrame(columns=column_list)\nfor col in column_list:\n    dfP.loc[0,col] = df[df[col] == 1][col].sum()\ndfP['non_hate'] = df.shape[0] - dfP.sum(axis=1)    ","12945559":"pie, ax = plt.subplots(figsize=[13,10])\nlabels = dfP.keys()\nplt.pie(x=dfP.values[0], autopct=\"%.12f\", explode=[0.05]*len(dfP.values[0]), labels=labels, pctdistance=0.55)\nplt.title(\"Types of Toxic Comments\", fontsize=14);\ndel dfP\ndel column_list","c274a7bf":"def clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'http[s]?:\/\/(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', '', text) # clean url\n    text = re.sub(r'#(\\w+)', '', text)   # clean hashes\n    text = re.sub(r'@(\\w+)', '', text)   # clean @\n    text = re.sub(r'<[^>]+>', '', text)  # clean tags\n    text = re.sub(r'\\d+', '', text)      # clean digits\n    text = re.sub(r'[,!@\\'\\\"?\\.$%_&#*+-:;]', '', text)   # clean punctuation\n    #word_tokens = nltk.word_tokenize(text)\n    #filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    #text = \"\".join(filtered_sentence)\n    text = lem.lemmatize(text)\n    return text\n","dfcc6bd8":"# load in pre-trained vectors\n# loading word vectors by using pre trained glove.6B.txt file\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('..\/input\/glove6b\/glove.6B.%sd.txt' % embedding_dim)) as f:\n    # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float128')\n        word2vec[word] = vec","36d90072":"df.isnull().sum()","cd8aef91":"# prepare text samples and their labels\nprint('loading in comments...')\nclean_sentences = df2['comment_text'].apply(clean_text)\nsentences = clean_sentences\nsentences[0]","ad2ce790":"print('loading in test comments...')\ntest_sentences = df_test['comment_text'].values","b1e5a609":"possible_labels = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\ntrain_labels = df[possible_labels].values","2b05e2ad":"# convert the sentences into tokens\/integers\ntokenizer = Tokenizer(num_words=max_vocab_size)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)","b2006637":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","3d774b65":"# pad sequences so that we get a NxT matrix\ntrain_data = pad_sequences(sequences,maxlen=max_sequence_length)","629ecfd5":"test_data = pad_sequences(test_sequences,maxlen=max_sequence_length)\ntest_data","646b3a57":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(max_vocab_size, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, embedding_dim))\nfor word, i in word2idx.items():\n      if i < max_vocab_size:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n          # words not found in embedding index will be all zeros.\n          embedding_matrix[i] = embedding_vector","23d8bd0f":"import datetime","9170f66f":"from cyclic_lr import CyclicLR\n\nclr_cnn = CyclicLR(base_lr=8e-5,max_lr=4e-4,step_size=4000,mode='triangular2')\n\nclr_lstm = CyclicLR(base_lr=1e-6,max_lr=1e-4,step_size=4000,mode='triangular2')\n\nclr_hybrid = CyclicLR(base_lr=1e-6,max_lr=1e-4,step_size=2000,mode='triangular2')","8c3b757f":"embedding_layer = Embedding(num_words,\n                           embedding_dim,\n                           weights=[embedding_matrix],\n                           input_length= max_sequence_length,\n                           trainable = False,name='Embedding')\n","82ebf78a":"#Create train,validation set, with split of 0.2\nX_train, X_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)","5866674d":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss',patience=2,mode='min',min_delta=0.005)\n \n    \n    \nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                                                     patience=2, min_lr=0.000001)\nfrom lrfinder import LRFinder\nlr_finder = LRFinder(min_lr=1e-7, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=998, \n                                 epochs=5)\n","5d0a923e":"def print_auc_loss(history):\n    history_dict = history.history\n    print(history_dict.keys())\n\n    acc = history_dict['auc']\n    val_acc = history_dict['val_auc']\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n    fig = plt.figure(figsize=(10, 6))\n    plt.subplot(2, 1, 1)\n    # \"bo\" is for \"blue dot\"\n    plt.plot(epochs, loss, 'r', label='Training loss')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n    \n    \n    plt.plot(epochs, acc, 'r', label='Training AUC')\n    plt.plot(epochs, val_acc, 'b', label='Validation AUC')\n    plt.title('Training and validation AUC')\n    plt.xlabel('Epochs')\n    plt.ylabel('AUC')\n    plt.legend(loc='lower right')\n    plt.show()","4e1eb1a8":"model_hybrid = Sequential(name='Hybrid')\nmodel_hybrid.add(embedding_layer)\nmodel_hybrid.add(Bidirectional(LSTM(sizes[size],return_sequences=True),name='Bidirectional'))\nmodel_hybrid.add(Dense(sizes[size],name='Dense1'))\nmodel_hybrid.add(Dropout(0.2,name='Dropout1'))\nmodel_hybrid.add(Conv1D(sizes[size],3))\nmodel_hybrid.add(GlobalMaxPooling1D(name='Pooling'))\nmodel_hybrid.add(Dense(sizes[size],name='Dense2'))\nmodel_hybrid.add(Dropout(0.2,name='Dropout2'))\nmodel_hybrid.add(Dense(6,activation='sigmoid',name='Classifier'))\nmodel_hybrid.summary()\nmodel_hybrid.compile(loss = 'binary_crossentropy', optimizer = Adam(), metrics = ['AUC'])\nhistory_hybrid = model_hybrid.fit(X_train,y_train,validation_data=(X_test,y_test),epochs = epoch, batch_size = batch_size ,callbacks=[clr_hybrid,early_stop])","894c2285":"print_auc_loss(history_hybrid)","c61da251":"#Prints the learning rate finder loss vs lr\n#lr_finder.plot_loss()\n#lr_finder.plot_lr()","3d62671f":"print('Training model')\n\nmodel_cnn = Sequential(name='CNN')\ninput_ = Input(shape=(max_sequence_length,))\nmodel_cnn.add(embedding_layer)\nmodel_cnn.add(Conv1D(sizes[size],3,activation='relu',name='Convolutional1'))\nmodel_cnn.add(MaxPooling1D(3))\nmodel_cnn.add(Conv1D(sizes[size],3,activation='relu',name='Convolutional2'))\nmodel_cnn.add(GlobalMaxPooling1D(name='Pooling'))\nmodel_cnn.add(Dense(sizes[size],name='Dense'))\nmodel_cnn.add(Dropout(0.2))\nmodel_cnn.add(Dense(len(possible_labels),activation='sigmoid',name='Classifier'))\n\nmodel_cnn.compile(loss='binary_crossentropy',\n             optimizer=Adam(),\n             metrics=['AUC'])\n\n\nhistory_cnn = model_cnn.fit(X_train,y_train,batch_size=batch_size, epochs=epoch,validation_data=(X_test,y_test),callbacks=[clr_cnn,early_stop])","4a6eba65":"#model_losses = pd.DataFrame(history_cnn.history)\nprint_auc_loss(history_cnn)","89bef6eb":"#Plots the learning rate when using lr_finder\n#lr_finder.plot_loss()\n#lr_finder.plot_lr()","716e4e7e":"print('Training model')\n\nmodel_lstm = Sequential(name='LSTM')\nmodel_lstm.add(embedding_layer)\nmodel_lstm.add(Bidirectional(LSTM(sizes[size]),name='BidirectionalLSTM'))\nmodel_lstm.add(Dense(sizes[size],name='Dense'))\n#odel_lstm.add(Dropout(0.2))\nmodel_lstm.add(Dense(sizes[size]))\n#odel_lstm.add(Dropout(0.2))\nmodel_lstm.add(Dense(len(possible_labels),activation='sigmoid',name='Classifier'))\n    \nmodel_lstm.compile(loss='binary_crossentropy',\n             optimizer=Adam(),\n             metrics=['AUC','accuracy'])\n\n\nhistory_lstm = model_lstm.fit(X_train,y_train,batch_size=batch_size, epochs=epoch,validation_data=(X_test,y_test),callbacks=[clr_lstm,early_stop])\n\n","3545b92b":"print_auc_loss(history_lstm)","8b8b9303":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix","a20aa250":"total_string = \"\"","5ceb896f":"#LSTM\npL = model_lstm.predict(X_test)\naucsL = []\nfor j in range(6):\n    auc = roc_auc_score(y_test[:,j],pL[:,j])\n    aucsL.append(auc)\nprint(\"auc lstm:\" +str(np.mean(aucsL)))\ntotal_string+=\"auc lstm:\" +str(np.mean(aucsL))+\"\\n\"","b72516b2":"#CNN\npC = model_cnn.predict(X_test)\naucsC = []\nfor j in range(6):\n    auc = roc_auc_score(y_test[:,j],pC[:,j])\n    aucsC.append(auc)\nprint(\"auc cnn:\" +str(np.mean(aucsC)))\ntotal_string+=\"auc cnn:\" +str(np.mean(aucsC))+\"\\n\"","f7fb5a1d":"#Hybrid\npH = model_hybrid.predict(X_test)\naucsH = []\n##precs = []\n#recalls = []\n#f1_scores = []\nfor j in range(6):\n    auc = roc_auc_score(y_test[:,j],pH[:,j])\n    aucsH.append(auc)\n    \nprint(\"auc hybrid:\" +str(np.mean(aucsH)))\ntotal_string+=\"auc hybrid:\" +str(np.mean(aucsH))+\"\\n\"\n","45390c25":"print(\"size is: \"+size+\"\\n\"+\"embedding dimensions: \"+str(embedding_dim)+\"\\n\"+total_string)","133631d1":"#p = model_cnn.predict(test_data)\n#predict = np.hstack((df_test.id[:, np.newaxis], p))\n#subm = pd.DataFrame(predict, columns = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n#subm.to_csv('subm_CNN.csv', index = False)","407c4879":"#p = model_lstm.predict(test_data)\n#predict = np.hstack((df_test.id[:, np.newaxis], p))\n#subm = pd.DataFrame(predict, columns = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n#subm.to_csv('subm_LSTM2.csv', index = False)","0c75e0be":"#p = model_hybrid.predict(test_data)\n#predict = np.hstack((df_test.id[:, np.newaxis], p))\n#subm = pd.DataFrame(predict, columns = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n#subm.to_csv('subm_HYBRID2.csv', index = False)","f7c16bc5":"\"\"\"\"tf.keras.utils.plot_model( model_hybrid, to_file='model_hybrid.png', show_shapes=False, show_dtype=False,\n    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=48)\"\"\"","51e7f374":"#first exp\nx = [16,32,64,128,256]\nc = [0.924,0.934,0.942,0.952,0.957]\nl =[0.948,0.952,0.959,0.964,0.971]\nh =[0.953,0.951,0.959,0.966,0.974]\nplt.plot(x, c, \"-o\")\nplt.plot(x,l, \"-o\")\nplt.plot(x,h,\"-o\")\n\nplt.show()","feba1f0e":"#second exp\nx = [16,32,64,128,256]\nc = [0.928,0.933,0.944,0.951,0.962]\nl =[0.950,0.954,0.955,0.971,0.975]\nh =[0.945,0.951,0.958,0.963,0.973]\nplt.plot(x, c, \"-o\")\nplt.plot(x,l, \"-o\")\nplt.plot(x,h,\"-o\")\n\nplt.show()","084e89c2":"from official import nlp\nfrom transformers import AutoTokenizer,TFAutoModel\nfrom bert_tokenizer_v2 import FullTokenizer\n\nimport tensorflow_hub as hub","30ba7166":"from official.nlp import bert\n# Load the required submodules\nimport official.nlp.bert.bert_models\nimport official.nlp.bert.configs\nimport official.nlp.bert.run_classifier\nimport official.nlp.bert.tokenization\nimport official.nlp.data.classifier_data_lib\nimport official.nlp.modeling.losses\nimport official.nlp.modeling.models\nimport official.nlp.modeling.networks\nfrom official.nlp import optimization","eed6c914":"epochs = 4\nsteps_per_epoch = np.ceil(len(X_train)\/batch_size)\nnum_train_steps = steps_per_epoch * epochs\nnum_warmup_steps = int(0.1*num_train_steps)\n\ninit_lr = 3e-5\noptimizerr = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')","5260ae0d":"bert_model_name = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-128_A-2\/1'\n#bert_model_name = 'https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-2_H-256_A-4\/1'\nbert_preprocess_name = 'https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3'\n\ntfhub_handle_encoder = bert_model_name\ntfhub_handle_preprocess = bert_preprocess_name\n\nprint(f'BERT model selected           : {tfhub_handle_encoder}') \nprint(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')","39d7cf3c":"bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)","aa06c064":"def build_classifier_model():\n  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n  encoder_inputs = preprocessing_layer(text_input)\n  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n  outputs = encoder(encoder_inputs)\n  net = outputs['pooled_output']\n  net = tf.keras.layers.Dropout(0.25,name='Dropout')(net)\n  net = tf.keras.layers.Dense(6, activation='sigmoid', name='classifier')(net)\n  return tf.keras.Model(text_input, net)","0a8aa843":"bert_model = build_classifier_model()\nbert_model.compile(loss='binary_crossentropy',\n             optimizer=optimizerr,\n             metrics=['AUC'])\n\nbert_model.summary()","f1759369":"#The bert training is commented out, because it takes a lot of time. \n\nprint(f'Training model with {tfhub_handle_encoder}')\n\nX_train, X_test, y_train, y_test = train_test_split(sentences, train_labels, test_size=0.2, random_state=42)\n\n#history_bert = bert_model.fit(X_train,y_train,batch_size=batch_size,\n        #                 epochs=epochs,validation_data=(X_test,y_test))","49d37628":"#print_acc_loss(history_bert)","fd14a297":"#test_s = df_test['comment_text']\n#test_s.apply(clean_text)","1f42138f":"\"\"\"p = bert_model.predict(test_s)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(y_test[:,j],p[:,j])\n    aucs.append(auc)\nprint(\"auc bert:\" +str(np.mean(aucs)))\"\"\"","a3f482b0":"## Creating embedding layer","edf3c2f2":"## Setup","7175bb42":"## BERT MODEL IMPLEMENTATION\nFollowing this tutorial\n[Bert google colab](https:\/\/colab.research.google.com\/github\/google-research\/bert\/blob\/master\/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=US_EAnICvP7f)\n","f360c80c":"## Configuration","1bfbf905":"## Preprocessing","0e9f14d9":"Make CSV out of predictions to be submission ready.","e4d33169":"#### Prints the model overview to file.","926078ae":"## Finding the mean AUC score of the models trained.","7d34abb3":"## Hybrid Model","d349ae7a":"## CNN Model","028cd158":"## LSTM Model"}}