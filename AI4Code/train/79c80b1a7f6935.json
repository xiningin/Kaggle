{"cell_type":{"e8442797":"code","d82baf5f":"code","82587675":"code","372021f8":"code","647492c9":"code","74fadf1f":"code","2e28bae8":"code","cece6d7c":"code","bbab40a5":"code","6c42a969":"code","accdf8dd":"code","7a24bb63":"code","e40cd54a":"code","9a9f6d5d":"code","5c503064":"code","6437b277":"code","242e227f":"code","5a3435f4":"code","976f465e":"code","1da6fe80":"code","b229277d":"code","019a7000":"code","9be39acf":"code","4b820f9a":"code","ef969fe3":"code","ef05875f":"code","3f63b8f5":"code","a75c749f":"code","b48259d6":"code","e08ec45c":"code","5d0e09ca":"code","296bcb07":"code","b27f93db":"code","2d31e472":"code","1fed106e":"code","7378d76d":"code","5067b75a":"code","870fbfe5":"code","92592f97":"markdown","c30c7b30":"markdown","06991d7e":"markdown","e29e0048":"markdown","280bb747":"markdown","c8272e5e":"markdown","158a33ff":"markdown","8bb1cfa3":"markdown","56c5b523":"markdown","8b1b6a18":"markdown","5db0571f":"markdown","a22cc85b":"markdown","d373cd02":"markdown","b15ec40e":"markdown","76d9eb5d":"markdown","3955f095":"markdown","f4c39b1e":"markdown","4110ebcd":"markdown","fd5cc376":"markdown","97615533":"markdown"},"source":{"e8442797":"import pandas as pd \nimport numpy as np \nimport numpy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport ipywidgets \nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","d82baf5f":"data=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","82587675":"## check the shape\ndata.shape","372021f8":"## check the info\ndata.info()","647492c9":"## describe the data\ndata.describe()","74fadf1f":"## check for nulls\ndata.isna().sum()","2e28bae8":"fig = make_subplots(rows=2, cols=2)\nfig.add_trace(go.Histogram(x=data['radius_mean'],\n                           histnorm='probability',\n                           marker_color='#EB89B5',name=\"radius mean:Histogram\"),row=1,col=1) \nfig.add_trace(go.Violin(y=data['radius_mean'],\n                        box_visible=True,\n                        meanline_visible=True,\n                        points='all',\n                        jitter=0.05,\n                        marker_color='yellow',name=\"radius mean:Violin\"),row=1,col=2)\nfig.add_trace(go.Histogram(x=data['texture_mean'],\n                           histnorm='probability',\n                           marker_color='#EB89B5',name=\"texture mean:Histogram\"),row=2,col=1) \nfig.add_trace(go.Violin(y=data['texture_mean'],\n                        box_visible=True,\n                        meanline_visible=True,\n                        points='all',\n                        jitter=0.05,\n                        marker_color='yellow',name=\"texture mean:Violin\"),row=2,col=2)\n\nfig.update_xaxes(title_text=\"Radius Mean : Histogram Distribution\", row=1, col=1)\nfig.update_xaxes(title_text=\"Radius Mean : Violin Distribution\", row=1, col=2)\nfig.update_xaxes(title_text=\"Texture Mean : Histogram Distribution\", row=2, col=1)\nfig.update_xaxes(title_text=\"Texture Mean : Violin Distribution\", row=2, col=2)\n\nfig.update_layout(title={'text':\"Distribution Plots:Radius Mean and Texture Mean\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.97},\n                  template='plotly_dark',\n                  height=900,\n                  legend=dict(\n                        orientation='h',\n                        yanchor=\"bottom\",\n                        y=1.01,\n                        xanchor=\"center\",\n                        x=0.5,\n                        bgcolor=\"black\",\n                        bordercolor=\"white\",\n                        borderwidth=2,\n                        font=dict(\n                                family=\"Courier\",\n                                size=10,\n                                color=\"white\"\n                            )),\n                  annotations=[dict(showarrow=True,\n                                x=28,\n                                y=0.0037,\n                                xref='x1',\n                                yref='y1',\n                                text=\"Slightly Right Skewed\",\n                                xanchor=\"left\",\n                                xshift=10,\n                                opacity=0.7,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowhead=4),\n                               dict(showarrow=True,\n                                x=39,\n                                y=0.0018,\n                                xref='x3',\n                                yref='y3',\n                                text=\"Slightly Right Skewed\",\n                                xanchor=\"left\",\n                                xshift=10,\n                                opacity=0.7,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowhead=4)\n                          ])\nfig.show()                                  ","cece6d7c":"col_y = 'diagnosis'\ntargets = [data.loc[data[col_y] == val] for val in data[col_y].unique()]\nfig=go.Figure()\nfig.add_trace(go.Histogram(x=targets[0]['perimeter_mean'],\n                               histnorm='probability',\n                               marker_color='yellow',name=\"Diagnosis:M\"))\nfig.add_trace(go.Histogram(x=targets[1]['perimeter_mean'],\n                               histnorm='probability',\n                               marker_color='red',name=\"Diagnosis:B\"))\nfig.update_xaxes(title_text=\"Perimeter Mean : Histogram Distribution\")\nfig.update_layout(title={'text':\"Distribution :Perimeter Mean Hue on Diagnosis\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.97},\n                      template='plotly_dark',\n                      legend=dict(\n                        orientation='h',\n                        yanchor=\"bottom\",\n                        y=1.01,\n                        xanchor=\"center\",\n                        x=0.5,\n                        bgcolor=\"black\",\n                        bordercolor=\"white\",\n                        borderwidth=2,\n                        font=dict(\n                                family=\"Courier\",\n                                size=10,\n                                color=\"white\"\n                            )),\n                        annotations=[dict(showarrow=True,\n                                x=65,\n                                y=0.14,\n                                text=\" Diagnosis Distribution <br> for Benign\",\n                                xanchor=\"right\",\n                                xshift=10,\n                                opacity=0.9,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowhead=4),\n                                    dict(showarrow=True,\n                                x=139,\n                                y=0.15,\n                                text=\" Diagnosis Distribution <br> for malignant\",\n                                xanchor=\"left\",\n                                xshift=10,\n                                opacity=0.9,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowside=\"end\",\n                                arrowhead=4)],\n                      barmode='overlay')\nfig.update_traces(opacity=0.5)\nfig.show();","bbab40a5":"fig=go.Figure()\nfig.add_trace(go.Box(x=data['diagnosis'],\n                     y=data['compactness_mean'],\n                     marker=dict(\n                                color='limegreen',\n                                outliercolor='red',\n                                line=dict(\n                                    outliercolor='red',\n                                    outlierwidth=2)),\n                     name=\"Diagnosis\",\n                     jitter=0.5,\n                     boxmean='sd',\n                     boxpoints='suspectedoutliers'))\nfig.update_xaxes(title_text=\"Compactness Mean : Boxplot\")\nfig.update_layout(title={'text':\"Boxplot :Compactness Mean Hue on Diagnosis\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.97},\n                      template='plotly_dark',\n                      legend=dict(yanchor=\"bottom\",\n                                  y=0.5,\n                                  xanchor=\"center\",\n                                  x=1.2),\n                      barmode='overlay',\n                      annotations=[dict(showarrow=True,\n                                x=0,\n                                y=0.36,\n                                text=\" Outliers\",\n                                xanchor=\"right\",\n                                xshift=0,\n                                opacity=0.9,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowhead=4)],)\nfig.update_traces(opacity=0.5)\nfig.show();","6c42a969":"fig=go.Figure()\nfig.add_trace(go.Box(x=data['diagnosis'],\n                     y=data['concavity_mean'],\n                     marker=dict(\n                                color='yellow',\n                                outliercolor='red',\n                                line=dict(\n                                    outliercolor='red',\n                                    outlierwidth=2)),\n                     name=\"Diagnosis\",\n                     jitter=0.5,\n                     boxmean='sd',\n                     boxpoints='suspectedoutliers'))\nfig.update_xaxes(title_text=\"Concavity Mean : Boxplot\")\nfig.update_layout(title={'text':\"Boxplot :Concavity Mean Hue on Diagnosis\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.97},\n                      template='plotly_dark',\n                      legend=dict(yanchor=\"bottom\",\n                                  y=0.5,\n                                  xanchor=\"center\",\n                                  x=1.2),\n                      barmode='overlay',\n                      annotations=[dict(showarrow=True,\n                                x=0,\n                                y=0.46,\n                                text=\" Outliers\",\n                                xanchor=\"right\",\n                                xshift=0,\n                                opacity=0.9,\n                                font=dict(color=\"lightgoldenrodyellow\",\n                                          size=12),\n                                arrowcolor=\"lightgoldenrodyellow\",\n                                arrowsize=5,\n                                arrowwidth=0.5,\n                                arrowhead=4)],)\nfig.update_traces(opacity=0.5)\nfig.show();","accdf8dd":"fig=ex.violin(data,\n              y='diagnosis',\n              x='area_mean', \n              color='diagnosis',\n              orientation='h').update_traces(side='positive',width=2)\nfig.update_xaxes(title_text=\"Area Mean : Violin Side Positive Hue on Diagnosis \")\nfig.update_layout(template=\"plotly_dark\",\n                  legend=dict(\n                        orientation='h',\n                        yanchor=\"bottom\",\n                        y=1.01,\n                        xanchor=\"center\",\n                        x=0.5,\n                        bgcolor=\"black\",\n                        bordercolor=\"white\",\n                        borderwidth=2,\n                        font=dict(\n                                family=\"Courier\",\n                                size=10,\n                                color=\"white\"\n                            )\n                          ))","7a24bb63":"fig = ex.scatter(data, x=\"texture_mean\", y='perimeter_mean', \n                 color=\"diagnosis\",\n                 size='texture_worst')\nfig.update_layout(template='plotly_dark',\n                  title={'text':\"Scatter Plot\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.94},\n                  legend=dict(\n                        orientation='h',\n                        yanchor=\"bottom\",\n                        y=-0.3,\n                        xanchor=\"center\",\n                        x=0.5,\n                        bgcolor=\"black\",\n                        bordercolor=\"white\",\n                        borderwidth=2,\n                        font=dict(\n                                family=\"Courier\",\n                                size=10,\n                                color=\"white\"\n                            )\n                          ))\nfig.show()","e40cd54a":"from plotly.graph_objs import *\ncols=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\ntrace1 = {\n  \"type\": \"heatmap\", \n  \"x\": data[cols].corr().columns.tolist(), \n  \"y\": data[cols].corr().columns.tolist(), \n  \"z\": data[cols].corr().values.tolist()\n}\ndf = Data([trace1])\nfig = Figure(data=df)\nfig.update_layout(template='plotly_dark',\n                  title={'text':\"Features Correlation Matrix\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.9})\nfig.show()","9a9f6d5d":"data['diagnosis'].value_counts()","5c503064":"# encoding the Target columns\ndata.drop(columns = ['Unnamed: 32','id'], inplace = True)\ndata['diagnosis'] = data['diagnosis'].map({'M':1, 'B':0})","6437b277":"data['diagnosis'].value_counts()","242e227f":"data.columns","5a3435f4":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\nX = data\nY = X['diagnosis'].values\nX = X.drop('diagnosis', axis = 1)\nX = X[col]","976f465e":"X.shape","1da6fe80":"X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size = 0.1)","b229277d":"import warnings\nwarnings.filterwarnings('ignore')\nall_classifier = []\nall_classifier.append(('LR', Pipeline([('Scaler', StandardScaler()),('LR',LogisticRegression())])))\nall_classifier.append(('KNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier(n_neighbors=5))])))\nall_classifier.append(('DT', Pipeline([('Scaler', StandardScaler()),('DT', DecisionTreeClassifier(criterion='gini', max_depth=9, min_samples_leaf=10, random_state=42))])))\nall_classifier.append(('RF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42))])))\nall_classifier.append(('ADB', Pipeline([('Scaler', StandardScaler()),('ADB', AdaBoostClassifier(n_estimators=500))])))\nall_classifier.append(('BC', Pipeline([('Scaler', StandardScaler()),('BC', BaggingClassifier(n_estimators=500))])))","019a7000":"train_acc = []\ntest_acc = []\ntrain_acc2 = []\ntest_acc2 = []\nfor name, model in all_classifier:\n    kfold = KFold(n_splits=5)\n    cv_results_train = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_results_test = cross_val_score(model, X_test, y_test, cv=kfold, scoring='accuracy')\n    cv_results_train2 = cross_val_score(model, X_train, y_train, cv=kfold, scoring='f1')\n    cv_results_test2 = cross_val_score(model, X_test, y_test, cv=kfold, scoring='f1')\n    train_acc.append(cv_results_train.mean())\n    test_acc.append(cv_results_test.mean())\n    train_acc2.append(cv_results_train2.mean())\n    test_acc2.append(cv_results_test2.mean())","9be39acf":"col={'Train Acc':train_acc,'Test Acc':test_acc, 'Train F1':train_acc2,'Test F1':test_acc2}\nmodels=['Logistic Regression', 'KNN', 'Decision Tree', 'Random Forest','ADA Boost','Bagging']\nrslt=pd.DataFrame(data=col,index=models)\nrslt","4b820f9a":"# rslt.plot(kind='barh',legend=False)\nfig=ex.bar(rslt,barmode=\"group\",\n       template='plotly_dark')\nfig.update_layout(title={'text':\"Results of Baseline Models\",\n                         'xanchor': 'center',\n                         'yanchor': 'top',\n                         'x':0.5,'y':0.96},\n                  legend=dict(\n                        orientation='h',\n                        yanchor=\"bottom\",\n                        y=-0.3,\n                        xanchor=\"center\",\n                        x=0.5,\n                        bgcolor=\"black\",\n                        bordercolor=\"white\",\n                        borderwidth=2,\n                        font=dict(\n                                family=\"Courier\",\n                                size=10,\n                                color=\"white\"\n                            )\n                          ),\n                  xaxis_title=\"Models\",yaxis_title=\"Scores\")","ef969fe3":"# Function to create model, required for KerasClassifier\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","ef05875f":"# fix random seed for reproducibility\nseed = 123\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\n# define the grid search parameters\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","3f63b8f5":"# Function to create model, required for KerasClassifier\ndef create_model(optimizer='adam'):\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","a75c749f":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\nparam_grid = dict(optimizer=optimizer)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","b48259d6":"from keras.optimizers import SGD\n# Function to create model, required for KerasClassifier\ndef create_model(learn_rate=0.01, momentum=0):\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    # Compile model\n    optimizer = SGD(lr=learn_rate, momentum=momentum)\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    return model","e08ec45c":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\nparam_grid = dict(learn_rate=learn_rate, momentum=momentum)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","5d0e09ca":"# Function to create model, required for KerasClassifier\ndef create_model(init_mode='uniform'):\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, kernel_initializer=init_mode, activation='relu'))\n    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","296bcb07":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\ninit_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\nparam_grid = dict(init_mode=init_mode)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","b27f93db":"# Function to create model, required for KerasClassifier\ndef create_model(activation='relu'):\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, kernel_initializer='uniform', activation=activation))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","2d31e472":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nparam_grid = dict(activation=activation)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","1fed106e":"from keras.constraints import maxnorm\nfrom keras.layers import Dropout\n# Function to create model, required for KerasClassifier\ndef create_model(dropout_rate=0.0, weight_constraint=0):\n    # create model\n    model = Sequential()\n    model.add(Dense(15, input_dim=30, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(weight_constraint)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","7378d76d":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nweight_constraint = [1, 2, 3, 4, 5]\ndropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nparam_grid = dict(dropout_rate=dropout_rate, weight_constraint=weight_constraint)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","5067b75a":"# Function to create model, required for KerasClassifier\ndef create_model(neurons=1):\n    # create model\n    model = Sequential()\n    model.add(Dense(neurons, input_dim=30, kernel_initializer='uniform', activation='linear', kernel_constraint=maxnorm(4)))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n    # Compile model\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","870fbfe5":"# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n# create model\nmodel = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n# define the grid search parameters\nneurons = [1, 5, 10, 15, 20, 25, 30]\nparam_grid = dict(neurons=neurons)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\ngrid_result = grid.fit(X, Y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","92592f97":"<a id = \"OV\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white; padding:10px\">Overview<\/span><\/h1>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">About Dataset&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> \n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">About Features&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>\n\nAttribute Information:\n\n* ID number\n* Diagnosis (M = malignant, B = benign)\n\nTen real-valued features are computed for each cell nucleus:\n\n* radius (mean of distances from center to points on the perimeter)\n* texture (standard deviation of gray-scale values)\n* perimeter\n* area\n* smoothness (local variation in radius lengths)\n* compactness (perimeter^2 \/ area - 1.0)\n* concavity (severity of concave portions of the contour)\n* concave points (number of concave portions of the contour)\n* symmetry\n* fractal dimension (\"coastline approximation\" - 1)\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Goal&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> \n\nThe goal is to predict whether it is a maligant or benign","c30c7b30":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune Dropout Regularization<\/span><\/h1>","06991d7e":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune the Neuron Activation Function<\/span><\/h1>","e29e0048":"<a id = \"model\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Model Building<\/span><\/h1>","280bb747":"<a id = \"eda\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">EDA<\/span><\/h1>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Univariate Analysis&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> \n\n\nWe will use countplot to create the univariate count distribution plot of all categorical variables and numerical variables.","c8272e5e":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Heat Map&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","158a33ff":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Read Dataset&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","8bb1cfa3":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Bivariate Analysis&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> \n\nThe target variable is a categorical variable. We can just add the target variable as the hue layer to show the distribution by each target class. If the variable is predictive, we shall see significant distribution across classes of the target variable","56c5b523":"<br>\n<h1 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #EC5555 ; color : white; text-align: center; border-radius: 100px 100px;padding:10px\">Content <\/h1>\n<br>\n<p id=\"toc\"><\/p>\n\n--- \n<h3 style=\"text-indent: 3vw; font-family: Garamond; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; color: black; background-color: white;\"><a href=\"#OV\" style=\"color:red;text-decoration:none\">&nbsp;&nbsp;&nbsp;&nbsp;1.Overview<\/a><\/h3>\n\n---\n<h3 style=\"text-indent: 3vw; font-family: Garamond; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; color: navy; background-color: #ffffff;\"><a href=\"#eda\" style=\"color:red;text-decoration:none\">&nbsp;&nbsp;&nbsp;&nbsp;2.EDA<\/a><\/h3>\n\n---\n<h3 style=\"text-indent: 3vw; font-family: Garamond; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; color: navy; background-color: #ffffff;\"><a href=\"#model\" style=\"color:red;text-decoration:none\">&nbsp;&nbsp;&nbsp;&nbsp;3.Model Building<\/a><\/h3>\n\n---\n<h3 style=\"text-indent: 3vw; font-family: Garamond; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; color: navy; background-color: #ffffff;\"><a href=\"#results\" style=\"color:red;text-decoration:none\">&nbsp;&nbsp;&nbsp;&nbsp;4.Conclusion and Further improvement<\/a><\/h3>\n\n---","8b1b6a18":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Pipelining&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","5db0571f":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune Learning Rate and Momentum<\/span><\/h1>","a22cc85b":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white; padding:10px\">Purpose of Notebook<\/span><\/h1>\n\n* Basis and high level use of Plotly charts and the classes available.\n* EDA to understand the pattern of Data.\n* Baseline prediction to set a benchmark score for further improvement","d373cd02":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune Network Weight Initialization<\/span><\/h1>","b15ec40e":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Training with CV&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>","76d9eb5d":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune the Number of Neurons in the Hidden Layer<\/span><\/h1>","3955f095":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Import Library&nbsp;&nbsp;&nbsp;&nbsp;<\/h1> ","f4c39b1e":"<a id = \"results\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Conclusion and Further Improvement<\/span><\/h1>\n\n* F1 score should be looked at as the data is slightly imbalanced and the objective should be to identify class =\"M\" more accurately than class=\"B\".\n* F1 score with slightly less overfitting model is good for RF and decision tree in. the baseline model.\n* Further improvements can be done using some fine tuning on the base classifiers with additional classifiers.\n* Giving weights to under represented samples.\n* The objective of the Keras section was to give the understanding on how to use hyperparameters for NN in keras. \n\n<h3>PLEASE UPVOTE IF IT IS USEFUL<\/h3>","4110ebcd":"<br>\n<h1 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #EC5555 ; color : white; text-align: center; border-radius: 100px 100px;padding:10px\">Breast Cancer : EDA and Modeling <\/h1>\n<br>\n<center><img src=\"https:\/\/ak.picdn.net\/shutterstock\/videos\/1039044962\/thumb\/5.jpg\"><\/center>\n\n<h1 style=\"font-size:40px; font-family:Garamond ; font-weight : normal\"><b>What is breast cancer?<\/b><\/h1>\n\nCancer occurs when changes called mutations take place in genes that regulate cell growth. The mutations let the cells divide and multiply in an uncontrolled way.Breast cancer is cancer that develops in breast cells. Typically, the cancer forms in either the lobules or the ducts of the breast.\nLobules are the glands that produce milk, and ducts are the pathways that bring the milk from the glands to the nipple. Cancer can also occur in the fatty tissue or the fibrous connective tissue within your breast.\nThe uncontrolled cancer cells often invade other healthy breast tissue and can travel to the lymph nodes under the arms. The lymph nodes are a primary pathway that help the cancer cells move to other parts of the body.\n\nBreast Cancer is the most common cancer and its very highly reporting skin cancer types in recent times. Above infographics give clear idea about this cancer and its impact on current world. In the world of healthcare, <strong>Breast Cancer is a current hot-buttom issue<\/strong> why u ask? our modern and lathergic lifestyle could be main reason. This type of cancer can occur in both men and women, but as per scientific investigation, <strong>Women are 2X susceptible to Breast Cancer<\/strong>  Which is why it is important to diagnose cancer in early stages.\n\nAs most of us know cancer is uncontrolled growth of the cells in a given area,and if that place is breast it causes breast cancer. Based on imaging proceduce called Fine Needle Aspiration proceducre, an expert will classify the cells as malignant,or benign. But How can we diagnosis breast cancer with machine learning? Thats question of the hour.\nWith image processing techniqes or manual measurements, cell charateristics are measured from Fine Needle aspiration images, and this characteistics will be used to classifiy the cells in to Benign and Cancerous. Few more specifics are given with the datasets...\n\n<h1 style=\"font-size:40px; font-family:Garamond ; font-weight : normal\"><b>What stats show?<\/b><\/h1>\n\n<center><img src=\"https:\/\/i0.wp.com\/images-prod.healthline.com\/hlcmsresource\/images\/topic_centers\/2018-8\/9161-Curing_Breast_Cancer_But_at_What_Cost-1296x900-body-image-01.jpg?w=1155&h=1890\"><\/center>\n\n<h1 style=\"font-size:40px; font-family:Garamond ; font-weight : normal\"><b>Treatment<\/b><\/h1>\n\n<center><img src=\"https:\/\/images.ctfassets.net\/yixw23k2v6vo\/64NWnigOlmrrefK18kC1yv\/d4963cfd9a5d6019d682f5f53e019e71\/BREASTCAN_INFO_tx.png\"><\/center>","fd5cc376":"<h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune the Training Optimization Algorithm<\/span><\/h1>","97615533":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\">Training with Keras&nbsp;&nbsp;&nbsp;&nbsp;<\/h1>\n\n<a id = \"results\"><\/a><h1 id=\"Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#EC5555; border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white;padding:10px \">Tune Batch Size and Number of Epochs<\/span><\/h1>\n"}}