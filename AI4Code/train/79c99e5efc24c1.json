{"cell_type":{"af0b0597":"code","36d56722":"code","b1b8fc77":"code","451efd1b":"code","9cbe11c3":"code","c1bbe348":"code","871306a4":"code","fabc262f":"code","14b4582b":"code","a729d676":"code","01937fb5":"code","d5215d74":"code","b3c4f7e7":"code","db89eca0":"code","c0d445c2":"code","0a0e0903":"code","19edad19":"code","0ee92874":"code","ecc4ebf6":"code","a35ec2c7":"code","4270d514":"code","8b5d4d20":"code","967d5e24":"code","07cf7f24":"code","627a1797":"code","3161d40d":"code","f6e3ba86":"markdown","181a841d":"markdown","3f9e0104":"markdown","06193b36":"markdown","95cc2992":"markdown"},"source":{"af0b0597":"# Importing data sets\nimport pandas as pd\ntrain_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv') ","36d56722":"# Training data details\ntrain_data.info()\ntrain_data.head(20)","b1b8fc77":"# Testing data details\ntest_data.info()\ntest_data.head(20)","451efd1b":"# Row counts where missing value is present in Train data\nprint(train_data.notnull().sum())\nprint(train_data.isnull().sum())","9cbe11c3":"train_data.dropna(axis = 0,inplace=True)","c1bbe348":"# Row counts where missing value is present in Test data\nprint(test_data.notnull().sum())\nprint(test_data.isnull().sum())","871306a4":"# plot frequency of positive, negative and neutral sentiments in Train Data\nfrom matplotlib import pyplot as plt\ncount_sentiments = pd.value_counts(train_data['sentiment'], sort=True)\ncount_sentiments.plot(kind='bar', color=(['green','red','orange']), alpha=0.8, rot=0)\nplt.title(\"Distribution of Sentiment Types in Train Data\")\nplt.xticks(range(3), ['positive', 'negative', 'neutral'])\nplt.xlabel(\"Sentiment Type\")\nplt.ylabel(\"Frequency\")\nplt.show()","fabc262f":"# plot frequency of positive, negative and neutral sentiments in Test Data\nfrom matplotlib import pyplot as plt\ncount_sentiments_te = pd.value_counts(test_data['sentiment'], sort=True)\ncount_sentiments_te.plot(kind='bar', color=(['green','red','orange']), alpha=0.8, rot=0)\nplt.title(\"Distribution of Sentiment Types in Test Data\")\nplt.xticks(range(3), ['positive', 'negative', 'neutral'])\nplt.xlabel(\"Sentiment Type\")\nplt.ylabel(\"Frequency\")\nplt.show()","14b4582b":"# Removes punctuation from text. Convert entire text to lower case.\nimport string\ndef remove_punctuation(text):\n    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n    return no_punct\n\ntrain_data['s_text_clean'] = train_data['selected_text'].apply(str).apply(lambda x: remove_punctuation(x.lower()))\ntrain_data.head(20)","a729d676":"# Breaks up entire string into a list of words based on a pattern specified by the Regular Expression\nfrom nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')  \ntrain_data['s_text_tokens'] = train_data['s_text_clean'].apply(str).apply(lambda x: tokenizer.tokenize(x))\ntrain_data.head(20)","01937fb5":"# Remove stopwords\nfrom nltk.corpus import stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if (w not in stopwords.words('english') or w not in 'im')]\n    return words\n\ntrain_data['s_text_tokens_NOTstop'] = train_data['s_text_tokens'].apply(lambda x: remove_stopwords(x))\ntrain_data.head(20)","d5215d74":"# Lemmatization\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text = [lemmatizer.lemmatize(i) for i in text]\n    return lem_text\n\ntrain_data['s_text_lemma'] = train_data['s_text_tokens_NOTstop'].apply(lambda x: word_lemmatizer(x))\ntrain_data.head(20)","b3c4f7e7":"# Stemming\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ndef word_stemmer(text):\n    stem_text = \" \".join([stemmer.stem(i) for i in text])\n    return stem_text\n\ntrain_data['s_text_stem'] = train_data['s_text_lemma'].apply(lambda x: word_stemmer(x))\ntrain_data.head(20)","db89eca0":"from wordcloud import WordCloud, STOPWORDS\nstop_w = set(STOPWORDS)\n\nsentiment=['positive','neutral','negative']\nfig, a = plt.subplots(1,3, figsize=(20,20))\nfor i,s in enumerate(sentiment):   \n    total_token = ''\n    total_token +=' '.join(train_data.loc[train_data['sentiment']==s,'s_text_stem'])\n    if (s == 'positive'):\n        w_cloud = WordCloud(width=1200, height=1200, background_color='green', stopwords = stop_w, min_font_size=12).generate(total_token)\n    if (s == 'neutral'):\n        w_cloud = WordCloud(width=1200, height=1200, background_color='orange', stopwords = stop_w, min_font_size=12).generate(total_token)\n    if (s == 'negative'):\n        w_cloud = WordCloud(width=1200, height=1200, background_color='red', stopwords = stop_w, min_font_size=12).generate(total_token)\n    a[i].imshow(w_cloud, interpolation = 'bilinear')  \n    a[i].set_title(s)\n    a[i].axis('off')","c0d445c2":"import seaborn as sns\n\ndef unique_words_analysis(df):\n    fig,ax = plt.subplots(1,3, figsize=(16,4))\n    for i,s in enumerate(sentiment):\n        new = train_data[train_data['sentiment']==s]['s_text_stem'].map(lambda x: len(set(x.split())))\n        if (s =='positive'):\n            sns.distplot(new.values, ax = ax[i], color='green', rug=True)\n        if (s =='neutral'):\n            sns.distplot(new.values, ax = ax[i], color='orange', rug=True)\n        if (s =='negative'):\n            sns.distplot(new.values, ax = ax[i], color='red', rug=True)\n        ax[i].set_title(s)\n    fig.suptitle('Distribution of number of unique words')\n    fig.show()\n\nunique_words_analysis(train_data)","0a0e0903":"# Segregating positive, negative, neutral sentiment data\npositive_train = train_data[train_data['sentiment']=='positive']\nneutral_train = train_data[train_data['sentiment']=='neutral']\nnegative_train = train_data[train_data['sentiment']=='negative']","19edad19":"# Common Word frequency analysis for positive text\nfrom nltk.probability import FreqDist\nimport pandas as pd\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nfdist_pos = FreqDist(positive_train['s_text_stem'])\ntop_twen_pos = fdist_pos.most_common(20)\n#top_ten_pos\n\ndf1 = pd.DataFrame(top_twen_pos, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', color='green', linecolor='black', title='Top 20 Common Words in positive text',orientation='v')","0ee92874":"# Common Word frequency analysis for neutral text\n\nfdist_neu = FreqDist(neutral_train['s_text_stem'])\ntop_twen_neu = fdist_neu.most_common(20)\n\ndf2 = pd.DataFrame(top_twen_neu, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', color='orange', linecolor='black', title='Top 20 Common Words in neutral text',orientation='v')","ecc4ebf6":"# Common Word frequency analysis for negative text\n\nfdist_neg = FreqDist(negative_train['s_text_stem'])\ntop_twen_neg = fdist_neg.most_common(20)\n\ndf3 = pd.DataFrame(top_twen_neg, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', color='red', linecolor='black', title='Top 20 Common Words in negative text',orientation='v')","a35ec2c7":"#BPE (Byte Pair Encoding) tokenizer is used for tokenizing text\nimport tokenizers\nimport numpy as np\nmax_len = 128\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file = '\/kaggle\/input\/roberta-base\/vocab.json',\n            merges_file = '\/kaggle\/input\/roberta-base\/merges.txt',\n            lowercase =True,\n            add_prefix_space=True\n)\n\nsentiment_id = {'positive':tokenizer.encode('positive').ids[0], \n                'negative':tokenizer.encode('negative').ids[0], \n                'neutral':tokenizer.encode('neutral').ids[0]}\n\ntrain_data.reset_index(inplace=True)\n\n# input data formating for training\ntot_tw = train_data.shape[0]\n\ninput_ids = np.ones((tot_tw, max_len), dtype='int32')\nattention_mask = np.zeros((tot_tw, max_len), dtype='int32')\ntoken_type_ids = np.zeros((tot_tw, max_len), dtype='int32')\nstart_mask = np.zeros((tot_tw, max_len), dtype='int32')\nend_mask = np.zeros((tot_tw, max_len), dtype='int32')\n\nfor i in range(tot_tw):\n    set1 = \" \"+\" \".join(train_data.loc[i,'text'].split())\n    set2 = \" \".join(train_data.loc[i,'selected_text'].split())\n    idx = set1.find(set2)\n    set2_loc = np.zeros((len(set1)))\n    set2_loc[idx:idx+len(set2)]=1\n    if set1[idx-1]==\" \":\n        set2_loc[idx-1]=1\n  \n    enc_set1 = tokenizer.encode(set1)\n\n    selected_text_token_idx=[]\n    for k,(a,b) in enumerate(enc_set1.offsets):\n        sm = np.sum(set2_loc[a:b]) \n        if sm > 0:\n            selected_text_token_idx.append(k)\n\n    senti_token = sentiment_id[train_data.loc[i,'sentiment']]\n    input_ids[i,:len(enc_set1.ids)+5] = [0]+enc_set1.ids+[2,2]+[senti_token]+[2] \n    attention_mask[i,:len(enc_set1.ids)+5]=1\n\n    if len(selected_text_token_idx) > 0:\n        start_mask[i,selected_text_token_idx[0]+1]=1\n        end_mask[i, selected_text_token_idx[-1]+1]=1","4270d514":"# Categorical Cross Entropy with Label Smoothing\n# Label Smoothing is done to enhance accuracy\n\ndef custom_loss(y_true, y_pred):\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits = False, label_smoothing = 0.20)\n    loss = tf.reduce_mean(loss)\n    return loss","8b5d4d20":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nfrom keras.layers import Dense, Flatten, Conv1D, Dropout, Input\nfrom keras.models import Model, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\ndef build_model():\n        ids = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n        att = tf.keras.layers.Input((max_len,), dtype=tf.int32)\n        tok =  tf.keras.layers.Input((max_len,), dtype=tf.int32) \n\n        config_path = RobertaConfig.from_pretrained('\/kaggle\/input\/tf-roberta\/config-roberta-base.json')\n        roberta_model = TFRobertaModel.from_pretrained('\/kaggle\/input\/tf-roberta\/pretrained-roberta-base.h5', config=config_path)\n        x = roberta_model(ids, attention_mask = att, token_type_ids=tok)\n        \n        x1 = tf.keras.layers.Dropout(0.05)(x[0])\n        x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1) \n        #128 is the no. of filters; 2 is the kernel size of each filter\n        x1 = tf.keras.layers.LeakyReLU()(x1)\n        x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n        x1 = tf.keras.layers.Dense(1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n        x2 = tf.keras.layers.Dropout(0.05)(x[0]) \n        x2 = tf.keras.layers.Conv1D(128, 2,padding='same')(x2)\n        x2 = tf.keras.layers.LeakyReLU()(x2)\n        x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n        x2 = tf.keras.layers.Dense(1)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax')(x2)\n\n\n        model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n        optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5) \n        model.compile(loss=custom_loss, optimizer=optimizer)\n\n        return model","967d5e24":"#input data formating for testing\n\ntot_test_tw = test_data.shape[0]\n\ninput_ids_t = np.ones((tot_test_tw,max_len), dtype='int32')\nattention_mask_t = np.zeros((tot_test_tw,max_len), dtype='int32')\ntoken_type_ids_t = np.zeros((tot_test_tw,max_len), dtype='int32')\n\nfor i in range(tot_test_tw):\n    set1 = \" \"+\" \".join(test_data.loc[i,'text'].split())\n    enc_set1 = tokenizer.encode(set1)\n\n    s_token = sentiment_id[test_data.loc[i,'sentiment']]\n    input_ids_t[i,:len(enc_set1.ids)+5]=[0]+enc_set1.ids+[2,2]+[s_token]+[2]\n    attention_mask_t[i,:len(enc_set1.ids)+5]=1","07cf7f24":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom transformers import TFRobertaModel\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\n\npred_start= np.zeros((input_ids_t.shape[0],max_len))\npred_end= np.zeros((input_ids_t.shape[0],max_len))\n\nfor i in range(2):\n    print('--'*20)\n    print('-- MODEL %i --'%(i+1))\n    print('--'*20)\n    K.clear_session()\n    model = build_model()\n    model.load_weights('\/kaggle\/input\/model4\/v4-roberta-%i.h5'%(i+3))\n    pred = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n    pred_start = pred_start + (pred[0]\/2)\n    pred_end = pred_end + (pred[1]\/2)","627a1797":"all = []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(pred_start[k,])\n    b = np.argmax(pred_end[k,])\n    if a>b: \n        st = test_data.loc[k,'text'] \n    else:\n        text1 = \" \"+\" \".join(test_data.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    all.append(st)\ntest_data['selected_text']=all\ntest_data.head(20)","3161d40d":"test_data[['textID','selected_text']].to_csv('submission.csv', index=False)\nprint(\"Submission successful\")","f6e3ba86":"Exploratory data analysis ends here. Now we begin the advanced part where we will transform data set into [RoBERTa](http:\/\/arxiv.org\/pdf\/1907.11692.pdf) format. [BERT](http:\/\/arxiv.org\/pdf\/1810.04805.pdf) is a masked language model. It requires: \"start_tokens\", \"end_tokens\". These tokens pad the inputs. It also uses \"attention_masks\" to avoid performing attention on padding token indices. \"0\" is used for the tokens which are masked; \"1\" is used for the tokens which are not masked.","181a841d":"There is one row in the training data set which has its \"text\" and \"selected text\" missing. We can discard that.","3f9e0104":"We observe that both positive and negative tweets' no. of unique words follow almost the similar pattern of distribution (positively skewed). Though neutral tweets also follow a positively skewed distribution, it has a more wide spread as compared to the spread of other two types.","06193b36":"In both train and test datasets, no. of positive tweets are higher than no. of negative and neutral tweets. ","95cc2992":"In this competition, the aim is to identify\/extract \"selected_text\" from the \"text\" field of the test data set. Only after that, we will be able to cross-check and verify whether sentiment extracted in \"sentiment\" column of the test data set is correct or not. "}}