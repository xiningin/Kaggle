{"cell_type":{"0e0540ae":"code","32fb952a":"code","f4efc08f":"code","6e157727":"code","6afa9563":"code","9ebb351a":"code","e9dfb952":"code","edfd2b2e":"code","26852f30":"code","be4b720a":"code","882a55e5":"code","2256e5c2":"code","bffe9f19":"code","f6f377af":"code","3d97048a":"code","a1d168de":"code","0d046619":"code","6f85f1ba":"code","a000ed77":"code","6fec18df":"code","9579479a":"code","d26e318d":"code","49a90b80":"code","2b1171a3":"code","743629d5":"code","1879ad3f":"code","3894ad60":"code","1c6c0697":"code","9fb01906":"code","dc83cb25":"code","d14870a7":"code","681cc4df":"markdown","11c80146":"markdown","8d0191cf":"markdown","eb705956":"markdown","5e99e968":"markdown","d9c48d53":"markdown","0d2569ff":"markdown","a790b9dc":"markdown","9af0d632":"markdown","9766c42e":"markdown","05c50c81":"markdown","01582058":"markdown","ef8f7e4d":"markdown","2792ad3d":"markdown"},"source":{"0e0540ae":"import numpy as np \nimport pandas as pd\nimport time\nimport lightgbm as lgb\nfrom itertools import product\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.model_selection as skt\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import ElasticNet\nfrom numpy import savetxt\nfrom numpy import loadtxt\nfrom sklearn.linear_model import Ridge\nimport seaborn as sns\nimport gc\nimport os\nimport pickle\nfrom tqdm import tqdm_notebook\nfrom sklearn.model_selection import PredefinedSplit\nimport xgboost as xgb\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n\n\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_rows', 1100)\npd.set_option('display.max_columns', 100)","32fb952a":"DATA_FOLDER  = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\ntrain          = pd.read_csv(os.path.join(DATA_FOLDER, 'sales_train.csv'))\nitems           = pd.read_csv(os.path.join(DATA_FOLDER, 'items.csv'))\nitem_categories = pd.read_csv(os.path.join(DATA_FOLDER, 'item_categories.csv'))\nshops           = pd.read_csv(os.path.join(DATA_FOLDER, 'shops.csv'))\nsample_sub      = pd.read_csv(os.path.join(DATA_FOLDER, 'sample_submission.csv'))\ntest            = pd.read_csv(os.path.join(DATA_FOLDER, 'test.csv'))\n#This is a translated version of cities data frame\ncities          = pd.read_csv('..\/input\/shops-en\/shops-en.csv')","f4efc08f":"#EDA - Detect Outliers\nplt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=train.item_cnt_day)\n#We plot item prices\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price)","6e157727":"#We remove outliers and fix negative price\ntrain = train[train.item_price<100000]\ntrain = train[train.item_cnt_day<1200]\n#Fix negative price for a item 2973\nmedian = np.median(train[train['item_id']==2973].item_price)\ntrain.loc[train['item_price']<0,'item_price'] = median","6afa9563":"#We pre process shops data to extract city information\ncities['city'] = cities['shop_name'].str.split(\" \").map(lambda x: x[1])\n#Manual adjustement for some rows to fix city names\ncities.iloc[20,2] = \"Moscow\"\ncities.iloc[42,2] = \"St. Petersburg\"\ncities.iloc[43,2] = \"St. Petersburg\"","9ebb351a":"#This is a function to reduce memory usage by adjusting column types\ndef optimize_memory(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    integers = ['int8','int16','int32','int64']\n    floats   = ['float32','float64']\n    int_cols  = [c for c in df if df[c].dtype in integers]\n    float_cols  = [c for c in df if df[c].dtype in floats]\n    for i in int_cols:\n        df[i] = pd.to_numeric(df[i], downcast='integer')\n    for i in float_cols:\n        df[i] = pd.to_numeric(df[i], downcast='float')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e9dfb952":"#Data Preparation and Pre- Processing \n#For every pair shop_id\/item_id we create a row for each month (0 - 33)\ngrid = [] \nindex_cols = ['date_block_num','shop_id', 'item_id']\nmonths = train['date_block_num'].unique()\n#We construct a grid of all possible shop_id\/item_id pairs for a given month\nfor mth in months:\n    shop_ids = train[train['date_block_num'] == mth].shop_id.unique()\n    item_ids = train[train['date_block_num'] == mth].item_id.unique()\n    grid.append(np.array(list(product(*[[mth],shop_ids, item_ids])),dtype='int16'))\n\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)  \n\n#We join the grid with the aggregated sales data per month\ngb = train.groupby(index_cols,as_index = False).agg({ 'item_cnt_day':'sum'})\ngb.rename(columns ={'item_cnt_day':'item_cnt_month'},inplace = True)\ndf_sales = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n\n#We add item price\ngb = train.groupby('item_id',as_index = False).agg({ 'item_price':'mean'})\ngb.rename(columns ={'item_price':'avg_item_price'},inplace = True)\ndf_sales = pd.merge(df_sales,gb,how='left',on='item_id').fillna(0)\n\n#Clip target values\ndf_sales['item_cnt_month'] = np.clip(df_sales['item_cnt_month'],0,20)\ndf_sales.sort_values(index_cols, inplace = True)\n\n#Free Memory\ndel grid,gb\ngc.collect()","edfd2b2e":"#We assign next Date Block Num to the test set\ntest['date_block_num'] = 34\n#Concatenate train and test dataframes\ndf_sales = pd.concat([df_sales,test], ignore_index = True)\ndf_sales.drop('ID',axis = 1, inplace = True)\ndf_sales.fillna(0,inplace = True)","26852f30":"# We add the item category to the data frame\ndf_sales = pd.merge(df_sales,items,how = 'left',on = 'item_id').drop('item_name', axis = 1)\n#We add also the city info to the data\ndf_sales = pd.merge(df_sales,cities,how = 'left',on = 'shop_id').drop('shop_name', axis = 1)\n#We assign codes to each city\ndf_sales['city_code'] = LabelEncoder().fit_transform(df_sales['city'])\ndf_sales.drop('city',axis = 1,inplace = True)","be4b720a":"#Add month and days\/month holidays\/month to the dataframe\ndf_sales['month'] =df_sales['date_block_num'] % 12\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\ndf_sales['days'] = df_sales['month'].map(days)\nholidays = pd.Series([8,1,1,0,2,1,0,0,0,0,1,0])\ndf_sales['holidays'] = df_sales['month'].map(holidays)","882a55e5":"\ndef shift_feature(df, lags, feature,drop = True):\n    for step in lags:\n        new_col_name = feature+'_'+str(step)\n        df[new_col_name] = df.groupby(['shop_id','item_id'])[feature].shift(periods = step)\n    if (drop):\n            df.drop(feature,axis = 1,inplace = True)\n    return df\n\n#In my method based on shifting by step, whatever previous month is, we will have a forecast \n#even if the immediat previous month is not available, which has been rewarding in terms of score\ndef add_feature(df,grp_cols,feature):\n        new_df = df.groupby(grp_cols).agg({'item_cnt_month': ['mean']})\n        new_df.columns = [feature]\n        new_df.reset_index(inplace = True)\n        df = pd.merge(df,new_df,on = grp_cols,how='left')\n        return(df)","2256e5c2":"#Mean Encoding Part 1 : We splits parts to free some memory in between\ndf_sales = shift_feature(df_sales,[1,2,3],'item_cnt_month',False)\n\ndf_sales = add_feature(df_sales,['date_block_num', 'item_id'],'date_item_avg_cnt')\ndf_sales = shift_feature(df_sales,[1,2,3],'date_item_avg_cnt')\n\ndf_sales = add_feature(df_sales,['date_block_num', 'shop_id'],'date_shop_avg_cnt')\ndf_sales = shift_feature(df_sales,[1,2,3],'date_shop_avg_cnt')\n\ndf_sales = add_feature(df_sales,['date_block_num', 'item_category_id'],'date_cat_avg_cnt')\ndf_sales = shift_feature(df_sales,[1,2,3],'date_cat_avg_cnt')\n\ndf_sales = add_feature(df_sales,['date_block_num'],'date_avg_item_cnt')\ndf_sales = shift_feature(df_sales,[1,2,3],'date_avg_item_cnt')\n\n#Cleaning works\ndf_sales.fillna(0,inplace = True)\ndf_sales = optimize_memory(df_sales)\ngc.collect()","bffe9f19":"#Mean Encoding Part 2\n\ndf_sales = add_feature(df_sales,['month'],'month_target')\ndf_sales = shift_feature(df_sales,[1,2,3,12],'month_target')\n\ndf_sales = add_feature(df_sales,['month','item_id'],'month_item_target')\ndf_sales = shift_feature(df_sales,[1,2,3],'month_item_target')\n\ndf_sales = add_feature(df_sales,['month','shop_id'],'month_shop_target')\ndf_sales = shift_feature(df_sales,[1,2,3],'month_shop_target')\n\ndf_sales = add_feature(df_sales,['month','item_category_id'],'month_cat_target')\ndf_sales = shift_feature(df_sales,[1,2,3,12],'month_cat_target')\n\n#Cleaning works\ndf_sales.fillna(0,inplace = True)\ndf_sales = optimize_memory(df_sales)\ngc.collect()","f6f377af":"#Mean Encoding Part 3\ndf_sales['date_item_day'] = df_sales['item_cnt_month'] \/ df_sales['days']\ndf_sales = shift_feature(df_sales,[1,2,3,12],'date_item_day')\n\ndf_sales = add_feature(df_sales,['date_block_num','item_id','city_code'],'date_item_city_target')\ndf_sales = shift_feature(df_sales,[1,2,3,12],'date_item_city_target')\n\n#We measure the difference between item price vs average item pricce \ngb = df_sales.groupby(['shop_id','item_id'],as_index = False).agg({ 'avg_item_price':'mean'})\ngb.rename(columns ={'avg_item_price':'avg_pair_price'},inplace = True)\ndf_sales = pd.merge(df_sales,gb,how='left',on=['shop_id','item_id']).fillna(0)\ndf_sales['delta_item_price'] = (df_sales['avg_pair_price'] - df_sales['avg_item_price'])\/df_sales['avg_item_price']\ndf_sales = shift_feature(df_sales,[1,2,3,6],'avg_item_price')\ndf_sales = shift_feature(df_sales,[1,2,3,6],'avg_pair_price')\ndf_sales = shift_feature(df_sales,[1,2,3,6],'delta_item_price')\n\ndf_sales.fillna(0,inplace = True)\ndf_sales = optimize_memory(df_sales)\ndel gb\ngc.collect()","3d97048a":"#Discard the first year\nstart_mem = df_sales.memory_usage().sum() \/ 1024**2\n\ndf_sales = df_sales[df_sales['date_block_num'] > 11]\n\nend_mem = df_sales.memory_usage().sum() \/ 1024**2\nprint('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\ngc.collect()","a1d168de":"#Save the data\ndf_sales.to_pickle('df_sales.pkl')","0d046619":"#Un comment to Load data and avoid re processing the data\n\n#df_sales = pd.read_pickle('..\/input\/predit-sales\/df_sales.pkl')","6f85f1ba":"#Splitting training into training and validation\n#Using gridsearch on XGBoost will take very long time, so I decided to go with a fixed validation set for month 33\n#and use evaluation built in functionnality of XGBoost to determine the best iteration\nX_train = df_sales[df_sales['date_block_num'] < 33].drop('item_cnt_month',axis = 1)\ny_train = df_sales[df_sales['date_block_num'] < 33].item_cnt_month\nX_val = df_sales[df_sales['date_block_num'] == 33].drop('item_cnt_month',axis = 1)\ny_val = df_sales[df_sales['date_block_num'] == 33].item_cnt_month\n#Test Set\nX_test  = df_sales[df_sales['date_block_num'] == 34].drop('item_cnt_month',axis = 1)\n#del df_sales\ngc.collect()","a000ed77":"#Model training and fiting\n#Model is already serialized, uncomment to train the model again\nts = time.time()\n\nxgb = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    subsample=0.8,\n    colsample_bytree=0.8,\n    eta = 0.3,\n    seed=42)\n\nxgb.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, y_train), (X_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","6fec18df":"#Saving the model\npickle.dump(xgb, open(\"xgb.dat\", \"wb\"))\n\n#Uncomment to load pre-saved model\n#xgb = pickle.load(open(\"..\/input\/predit-sales\/xgbmodel.dat\", \"rb\"))","9579479a":"#Prediction and score results for XGBoost model\ny_pred = xgb.predict(X_val).clip(0,20)\ny_pred_tr = xgb.predict(X_train).clip(0,20)\nrmse_tr = mean_squared_error(y_train, y_pred_tr,squared=False)\nrmse_val = mean_squared_error(y_val, y_pred,squared=False)\nprint(\"RMSE Validation: %.5f\" % rmse_val)\nprint(\"RMSE Training: %.5f\" % rmse_tr)","d26e318d":"#We plot the most important features of the model\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(xgb, (10,14))","49a90b80":"#First submission\ny_test = xgb.predict(X_test).clip(0,20)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('submission_1.csv', index=False)","2b1171a3":"#We include now validation set into the full training set, to have better data and improve the model\nX_train = df_sales[df_sales['date_block_num'] < 34].drop('item_cnt_month',axis = 1)\ny_train = df_sales[df_sales['date_block_num'] < 34].item_cnt_month","743629d5":"#Use GridSearch and Timesplit to tune ElasticNet parameters, before creating meta features for ensembling\nparam_search = {\"max_iter\": [1500]}\nelanet = ElasticNet()\ntscv = TimeSeriesSplit(n_splits = 10).split(X_train)\ngsearch = GridSearchCV(elanet, cv=tscv,param_grid=param_search,scoring = 'neg_root_mean_squared_error',n_jobs = 2,verbose = 3)\ngsearch.fit(X_train,y_train)\ngc.collect()","1879ad3f":"#To have the same index for X_test and y_test\ndates_train = X_train['date_block_num']\ndates_train_level2 = dates_train[dates_train.isin([28,29,30,31,32,33])]\n#Test meta features\n\n#Linear Model \npred_lr = gsearch.predict(X_test).clip(0,20)\npred_xgb = xgb.predict(X_test).clip(0,20)\n#Concatenate the two predictions\nX_test_level2 = np.c_[pred_lr,pred_xgb]\n\n#train meta features\ny_train_level2  =    y_train[dates_train.isin([28,29,30,31,32,33])]\nX_train_level2 =     np.zeros([y_train_level2.shape[0], 2])","3894ad60":"del gsearch,pred_lr,pred_xgb\ngc.collect()\n\nts = time.time()\nlr = ElasticNet(max_iter = 1500)\n\nfor cur_block_num in [28,29,30,31,32,33]:\n    \n    print(cur_block_num)\n    X_train_part = X_train[dates_train < cur_block_num]\n    y_train_part = y_train[dates_train < cur_block_num]\n    X_test_part  = X_train[dates_train == cur_block_num]\n    y_test_part  = y_train[dates_train == cur_block_num]\n    print('Linear Regression Model')\n    lr.fit(X_train_part,y_train_part)\n    pred_lr = lr.predict(X_test_part)\n    print('Fitting XGBoost Model')\n    xgb.fit(X_train_part,y_train_part,eval_metric=\"rmse\", eval_set=[(X_train_part, y_train_part), (X_test_part, y_test_part)], \n            verbose=True, early_stopping_rounds = 10)\n    pred_xgb = xgb.predict(X_test_part)\n    X_train_level2[dates_train_level2 == cur_block_num,0] = pred_lr\n    X_train_level2[dates_train_level2 == cur_block_num,1] = pred_xgb\n    \ntime.time() - ts","1c6c0697":"plt.scatter(X_train_level2[:,0],X_train_level2[:,1],marker='o');","9fb01906":"#Save train meta features to avoid repeated processing\npickle.dump(X_train_level2, open(\"X_train_level2.dat\", \"wb\"))","dc83cb25":"#We find the best alpha parameter for pred = alpha * lr + (1-alpha)*xgb\nalphas_to_try = np.linspace(0, 1, 5000)\nrmse_scores = np.array([mean_squared_error(y_train_level2,np.dot(X_train_level2,[alpha,1-alpha]),squared = False) for alpha in alphas_to_try])\nbest_alpha = alphas_to_try[rmse_scores.argmin()]\nrmse_train_simple_mix = rmse_scores.min()\nprint('Best alpha: %f; Corresponding rmse score on train: %f' % (best_alpha, rmse_train_simple_mix))","d14870a7":"y_test = np.dot(X_test_level2,[best_alpha,1-best_alpha]).clip(0,20)\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": y_test\n})\nsubmission.to_csv('submission_2.csv', index=False)","681cc4df":"<a id='data_engineering'><\/a>\n# Features Engineering","11c80146":"<a id='first_submission'><\/a>\n# First Submission","8d0191cf":"<a id='model_training'><\/a>\n# Model Training","eb705956":"<a id='second_submission'><\/a>\n# Second Submission","5e99e968":"<a id='scoring'><\/a>\n# Scores","d9c48d53":"<a id='data_loading'><\/a>\n# Data Loading","0d2569ff":"<a id='features_plot'><\/a>\n# Features Plot","a790b9dc":"<a id='import'><\/a>\n# Importing Required Librairies","9af0d632":"<a id='data_cleaning'><\/a>\n# Exploratory Data Analysis and Outliers Removal","9766c42e":"This notebook is the final project in the Coursera course : \"*How to Win a Data Science Competition : Learn from Top Kagglers\"*.  \n\n**Summary**:  \nI used XGBoost to train the model. For ensembling, I associated XGBoost with ElasticNet. Parameters for ElasticNet have been tuned using Timesplit and GridSeachCV. The most important feature is the previous item sales for a given shop_id\/item_id pair. For the development tool, I used Kaggle Jupyter notebook without acceleration.\n  \n**Pipeline:**\n- [Importing Required Librairies](#import)\n- [Data Loading](#data_loading)\n- [Exploratory Data Analysis and Outliers Removal](#data_cleaning)\n- [Data Pre-Processing](#data_preprocess)\n- [Features Engineering](#data_engineering)\n- [Model Training](#model_training)  \n    - [Scores](#scoring)\n    - [Features Plot](#features_plot)\n- [First Submission](#first_submission)\n- [Ensembling](#model_ensemble)\n    - [GridSearchCV for Linear Regression](#gridsearch_elasticnet)\n    - [Building Test & Train Meta Features](#meta_features)\n- [Second Submission](#second_submission)","05c50c81":"<a id='gridsearch_elasticnet'><\/a>\n# GridSearchCV for Linear Regression","01582058":"<a id='data_preprocess'><\/a>\n# Data Pre-Processing","ef8f7e4d":"<a id='meta_features'><\/a>\n# Building Test and Train Meta Features","2792ad3d":"<a id='model_ensemble'><\/a>\n# Ensembling"}}