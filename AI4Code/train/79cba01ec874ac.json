{"cell_type":{"c6aec378":"code","57f308b6":"code","4272bf33":"code","b709715b":"code","6047ebb3":"code","a3218039":"code","57164d6b":"code","da0f6eb0":"code","28551f89":"code","5fe27cc2":"code","d5778e9d":"code","a17ccc62":"code","2c2b1c64":"code","f73b0291":"code","a4f88b12":"code","1df62f20":"code","8f4d442b":"code","baa4d41c":"code","89382e93":"code","c0cdd78f":"markdown","c68d8709":"markdown","d80aead9":"markdown","ed5d233b":"markdown","d0e43899":"markdown","d2844294":"markdown","72821a59":"markdown","7fb8b804":"markdown","4e1b0b6b":"markdown","1123491a":"markdown","72226a0f":"markdown","2ac4dec1":"markdown","660ac1fc":"markdown","43f35f30":"markdown","15af5bc6":"markdown","7955df2c":"markdown","60ecbe5b":"markdown","05f1a157":"markdown","5022dfd6":"markdown","e6302546":"markdown","c92cadb1":"markdown","5c48c6cf":"markdown","3d2f148f":"markdown","d947d52f":"markdown","6de01cb9":"markdown","65a8f255":"markdown","3aa146ac":"markdown","56707ae1":"markdown","b8c20134":"markdown","35e091e9":"markdown","780ee265":"markdown","876bd918":"markdown","553922ae":"markdown","46e43b6f":"markdown","40409d01":"markdown","440ac459":"markdown","03652a1d":"markdown","d505e722":"markdown","51c830af":"markdown","d7c4e04e":"markdown","e67a7098":"markdown","6efd46f3":"markdown","d34c840a":"markdown","ecfed828":"markdown","5a405751":"markdown","779d3091":"markdown","24030ee8":"markdown","ce061df9":"markdown","8f69b414":"markdown","20876189":"markdown","936a287b":"markdown","85af690e":"markdown","fe45f323":"markdown","a31c9b95":"markdown","61bf498c":"markdown","3a886d70":"markdown","5c6dcba6":"markdown","2e041b0f":"markdown","7f970e10":"markdown","779a33e2":"markdown","e216df41":"markdown","b3aa25fc":"markdown","f8480a81":"markdown","5a63b0b5":"markdown","358ed273":"markdown","2b739899":"markdown","19f302c0":"markdown","42ee38e5":"markdown","51326c35":"markdown","5f56b9bf":"markdown","34060198":"markdown","73dd90d5":"markdown","6a44d15f":"markdown","e303b811":"markdown"},"source":{"c6aec378":"import tensorflow as tf\nprint(tf.__version__)","57f308b6":"# Gather data and preprocess it\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\nX_train, X_test = X_train \/ 255.0, X_test \/ 255.0","4272bf33":"CLASS_NAMES = ['0', '1', '2', '3', '4',\n               '5', '6', '7', '8', '9']","b709715b":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(True)\n    plt.imshow(X_train[i], cmap=plt.cm.binary)\n    plt.xlabel(CLASS_NAMES[y_train[i]])\nplt.show()","6047ebb3":"#!pip install wandb\nimport wandb","a3218039":"from wandb.keras import WandbCallback","57164d6b":"def training(init_scheme):\n    if isinstance(init_scheme, str):\n        wandb.init(project='weight-initialization-tb', sync_tensorboard=True,\n                   id=init_scheme)\n    else:\n        wandb.init(project='weight-initialization-tb', sync_tensorboard=True,\n                   id=str(init_scheme))\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Flatten(input_shape=(28, 28)),\n        tf.keras.layers.Dense(256, activation='relu', kernel_initializer=init_scheme,\n            bias_initializer='zeros'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(128, activation='relu', kernel_initializer=init_scheme,\n            bias_initializer='zeros'),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(10, activation='softmax', kernel_initializer=init_scheme,\n            bias_initializer='zeros')\n    ])\n\n    model.compile(optimizer='adam',\n                loss='sparse_categorical_crossentropy',\n                metrics=['accuracy'])\n    \n    return model, wandb.run.dir","da0f6eb0":"model_w_zeros, run_dir = training(init_scheme='zeros')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nmodel_w_zeros.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","28551f89":"model_w_ones, run_dir = training(init_scheme='ones')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nmodel_w_ones.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","5fe27cc2":"Orthogonal, run_dir = training(init_scheme='Orthogonal')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nOrthogonal.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","d5778e9d":"Identity, run_dir = training(init_scheme='Identity')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nIdentity.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","a17ccc62":"rand_normal, run_dir = training(init_scheme='RandomNormal')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nrand_normal.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","2c2b1c64":"rand_uniform, run_dir = training(init_scheme='RandomUniform')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nrand_uniform.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","f73b0291":"glorot_normal, run_dir = training(init_scheme='glorot_normal')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nglorot_normal.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","a4f88b12":"glorot_uniform, run_dir = training(init_scheme='glorot_uniform')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nglorot_uniform.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","1df62f20":"he_normal, run_dir = training(init_scheme='he_normal')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nhe_normal.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","8f4d442b":"he_uniform, run_dir = training(init_scheme='he_uniform')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nhe_uniform.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","baa4d41c":"lecun_uniform, run_dir = training(init_scheme='lecun_uniform')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nlecun_uniform.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","89382e93":"lecun_normal, run_dir = training(init_scheme='lecun_normal')\n\ntb_callback = tf.keras.callbacks.TensorBoard(log_dir=run_dir, histogram_freq=4, write_images=True)\n\nlecun_normal.fit(X_train, y_train, \n    validation_data=(X_test, y_test), \n    epochs=20, batch_size=128,\n    callbacks=[WandbCallback(data_type='image', labels=CLASS_NAMES, \n                             validation_data=(X_test, y_test)),\n               tb_callback])","c0cdd78f":"In this method, it draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(1 \/ fan_in) where fan_in is the number of input units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.\n","c68d8709":"I have written a blog on it as well: https:\/\/medium.com\/guidona-softpedia\/weight-initialization-methods-in-neural-networks-a3e7a793cee5","d80aead9":"![image.png](attachment:image.png)","ed5d233b":"![image.png](attachment:image.png)","d0e43899":"![image.png](attachment:image.png)","d2844294":"Training a neural network completely depends upon the type of parameters used to initialize the network. If the initialization of parameters is done correctly, the optimization or the result will be achieved in minimal time and if not initialized properly will lead to problems. In simple words, the performance of the neural network depends on how its parameters are initialized when it is starting to train. If we train a neural network with random weights, then the output becomes non-reproducible. On the other hand, if we train the neural network with constantly valued weights, then it would consume a lot of time to converge. In this, we will compare the accuracy of all the keras initializers and also look at the visualization as well. Snippets of the code have been displayed here because the visualization was done on wandb. Adding to it the model was trained for 20 epochs and dataset chosen was MNIST dataset. ","72821a59":"### LeCun Normal","7fb8b804":"![image.png](attachment:image.png)","4e1b0b6b":"![image.png](attachment:image.png)","1123491a":"![image.png](attachment:image.png)","72226a0f":"![image.png](attachment:image.png)","2ac4dec1":"![image.png](attachment:image.png)","660ac1fc":"![image.png](attachment:image.png)","43f35f30":"![image.png](attachment:image.png)","15af5bc6":"In this method, we multiply the random initializations with stddev to get better results. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ fan_in) where fan_in is the number of input units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.","7955df2c":"Consider upvoting if it helps you!","60ecbe5b":"![image.png](attachment:image.png)","05f1a157":"![image.png](attachment:image.png)","5022dfd6":"In this method, the initializer generates tensors with a uniform distribution.<br>\nArguments:\n* minval: A python scalar or a scalar-tensor. Lower bound of the range of random values to generate.\n* maxval: A python scalar or a scalar-tensor. Upper bound of the range of random values to generate. Defaults to 1 for float types.\n* seed: A Python integer. Used to seed the random generator.\n","e6302546":"![image.png](attachment:image.png)","c92cadb1":"### Random Normal ","5c48c6cf":"![image.png](attachment:image.png)","3d2f148f":"![image.png](attachment:image.png)","d947d52f":"### Glorot Normal","6de01cb9":"![image.png](attachment:image.png)","65a8f255":"### Random Uniform ","3aa146ac":"![image.png](attachment:image.png)","56707ae1":"![image.png](attachment:image.png)","b8c20134":"![image.png](attachment:image.png)","35e091e9":"![image.png](attachment:image.png)","780ee265":"Glorot Uniform Initializer is also known as Xavier Uniform Initializer. It is similar to He initializer but it is used for tanh activation functions. It draws samples from a uniform distribution within [-limit, limit] where limit is sqrt(6 \/ (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.","876bd918":"Identity initializer returns a tensor with 0\u2019s everywhere except for 1\u2019s at the diagonal. It is only used for 2D matrices.<br>\nArguments:\n* gain: Multiplicative factor to apply to the identity matrix.\n* dtype: The type of the output.","553922ae":"![image.png](attachment:image.png)","46e43b6f":"![image.png](attachment:image.png)","40409d01":"**References**","440ac459":"![image.png](attachment:image.png)","03652a1d":"![image.png](attachment:image.png)","d505e722":"In this method, all the weights associated with the input are assigned to one but it is still comparatively better than assigning weights to zero because the product of WiXi is not zero as Wi\u2019s are not zero in this method.","51c830af":"In this method, it draws samples from a uniform distribution within [-limit, limit] where the limit is sqrt(6 \/ fan_in) where fan_in is the number of input units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.\n","d7c4e04e":"### Ones","e67a7098":"**Conclusion**","6efd46f3":"![image.png](attachment:image.png)","d34c840a":"![image.png](attachment:image.png)","ecfed828":"![image.png](attachment:image.png)","5a405751":"The model architecture used to train is the following:","779d3091":"![image.png](attachment:image.png)","24030ee8":"![image.png](attachment:image.png)","ce061df9":"Assigning random weight values is better than assigning ones or zeros as weight values as we saw earlier the accuracy of one and zero initializers being reasonably low. On the other hand, if randomly initialized weight values are very high or very low then it may lead to problems known as exploding gradients and vanishing gradients respectively.<br>\nIn this method, the initializer generates tensors with a normal distribution.<br>\nArguments:\n* mean: a python scalar or a scalar-tensor. Mean of the random values to generate.\n* stddev: a python scalar or a scalar-tensor. The standard deviation of the random values to generate.\n* seed: A Python integer. Used to seed the random generator.\n","8f69b414":"![image.png](attachment:image.png)","20876189":"In this method, all the weights associated with the input are assigned to be zero. Hence, the derivative with respective to the loss function is the same for every weight in each iteration. It is then similar to a linear model.","936a287b":"![image.png](attachment:image.png)","85af690e":"Glorot Normal Initializer is also known as Xavier Normal Initializer. It is similar to He initializer but it is used for tanh activation functions. It draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 \/ (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.","fe45f323":"### LeCun Uniform","a31c9b95":"### He Normal ","61bf498c":"![image.png](attachment:image.png)","3a886d70":"![image.png](attachment:image.png)","5c6dcba6":"### Orthogonal","2e041b0f":"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers","7f970e10":"![image.png](attachment:image.png)","779a33e2":"![image.png](attachment:image.png)","e216df41":"The final Dense layer has a shape of (10, ) and a softmax activation function as it would give us the probability of 10 different classes that is from 0\u20139. The model architecture will remain the same for all the initializers which will be used for experimental purposes. Adam and sparse_categorical_crossentropy will be used as optimizer and loss function respectively. The model was trained for a total of 20 epochs.","b3aa25fc":"The weight initialization methods such as He, Glorot, LeCun and are much better than some of the weight initialization methods discussed at the start. Although, the random normal and random uniform initializers have good accuracy they are not reproducible and are the source of vanishing gradient and exploding gradient problems. Some of the new techniques discussed at the end set weights neither too much bigger than 1 nor too much less than 1. Adding to it, the time taken for convergence is also less. Hence, we can conclude that all the initializers have their own significance but the objective of avoiding slow convergence remains the same for all but only a few are able to achieve it.","f8480a81":"### Identity","5a63b0b5":"Orthogonal initialization proves really beneficial in optimizing deep neural networks. It speeds up the convergence relative to the standard Gaussian initialization. For deep neural networks, the width needed for efficient convergence to a global minimum with orthogonal initialization is independent of the depth. It generates a random orthogonal matrix at the time of its execution. Orthogonal initializer returns a tensor that if multiplied by its transpose, gives an identity tensor.<br>\nArguments:\n* gain: Multiplicative factor to apply to the orthogonal matrix.\n* seed: A Python integer. Used to seed the random generator.","358ed273":"### Zeros  ","2b739899":"### He Uniform","19f302c0":"![image.png](attachment:image.png)","42ee38e5":"![image.png](attachment:image.png)","51326c35":"In this method, it draws samples from a uniform distribution within [-limit, limit] where the limit is sqrt(3 \/ fan_in) where fan_in is the number of input units in the weight tensor.<br>\nArguments:\n* seed: A Python integer. Used to seed the random generator.","5f56b9bf":"### Glorot Uniform ","34060198":"![image.png](attachment:image.png)","73dd90d5":"To visualize and understand the performance level of the initializers, I have used the MNIST dataset. The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.","6a44d15f":"**Data and Model**","e303b811":"## Importance of Weight Initialization during training of Neural Networks"}}