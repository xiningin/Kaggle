{"cell_type":{"93625ee7":"code","3f625dd4":"code","514ea8ee":"code","b85ecadc":"code","5a372390":"code","2e0334d1":"code","6deb9864":"code","11b5d4ed":"code","220cdbc3":"code","fa9d0478":"code","c7ec3a23":"code","2888b4ac":"code","48febd68":"code","cd48d7fd":"code","a7d352f6":"code","4323a81e":"code","8eea6dc3":"code","bcc9c236":"code","440c07af":"code","906da065":"code","06af3eeb":"code","cb62fbb3":"code","6def15b5":"code","cbc667eb":"code","b90193be":"code","3c64dc9f":"code","24bc8217":"code","c41eae3b":"code","59ce082f":"code","b7118f11":"code","9c245a40":"code","25135a3f":"code","f8ae0743":"code","9fcdfbeb":"code","fa8ed20a":"code","679e9abb":"code","faf1b115":"code","1b68380d":"code","2c6c10ba":"code","5daf7b5e":"code","be2511ff":"code","fd460834":"code","5fbd3cdb":"code","0fe0831a":"code","9b359328":"code","10403f2d":"code","7d8e5f07":"code","211e7b4c":"code","932efefb":"code","b7d65985":"code","62d04341":"code","e451e16a":"code","7130348d":"code","48e06b65":"code","c783a9b0":"code","3c505c8b":"code","ce99a8bd":"code","5122a039":"code","884609fa":"code","5e1dd224":"code","44522d35":"code","127d9412":"code","f87dc2cc":"code","7f3f3e7b":"code","1e4aa661":"code","cc25ab32":"code","b20871ac":"code","5cb5ef12":"code","6b2022d5":"code","630a99bf":"code","2902be8c":"code","b2c0d572":"code","9e06ad8d":"code","befd8774":"code","aa7c9afe":"code","7710e52d":"code","ab4cddd5":"code","1f92efdd":"code","27a594fa":"code","345e763b":"code","6dc99230":"code","c98b6f3e":"code","f69aaafe":"code","5216e4ae":"code","d44871b5":"code","6d7454a4":"code","3bb5aeea":"code","41365456":"code","31a2284e":"code","c3d44b83":"code","1a9b0903":"code","4f882972":"code","d04b7e7f":"code","7be817ec":"code","61d8c30d":"code","9d448933":"code","7a9ec023":"code","f45eb6d4":"code","f9711894":"code","b47c5751":"markdown","a83899c0":"markdown","bcc9dfd8":"markdown","f73b3c7d":"markdown","6bebe060":"markdown","2a24a9ab":"markdown","b2f24469":"markdown","bc645c83":"markdown","4c0115fc":"markdown","f0816985":"markdown","c6f850ac":"markdown","0f0d40f7":"markdown","e82b6139":"markdown","fb02c50e":"markdown","55be9342":"markdown","5f30be5d":"markdown","7bef996b":"markdown","b6527134":"markdown","07c39f9f":"markdown","4b41ef61":"markdown","d26c589a":"markdown","88188243":"markdown","6b59baea":"markdown","3126775f":"markdown","a19782ed":"markdown","f53085ad":"markdown","1069fb2f":"markdown","f3d3a11c":"markdown","e94d869c":"markdown","bc4839e8":"markdown","97b84bb2":"markdown","3f767c90":"markdown","4bbfd55e":"markdown","1f110e6b":"markdown","0b01689b":"markdown","0901ba02":"markdown","bd6d9ab8":"markdown","3e0e7ae7":"markdown","8e07ff70":"markdown","d9165f2d":"markdown","f532caa9":"markdown","e43ee98f":"markdown","35dc71b5":"markdown","e2554892":"markdown","694d3a8b":"markdown","74267a0a":"markdown","eee246d5":"markdown","6e02c2cf":"markdown","b9b16057":"markdown","d012971e":"markdown","330c2d65":"markdown","71681b8a":"markdown","6dda954b":"markdown","c8f38726":"markdown","9f0cc722":"markdown","9964b6c8":"markdown","068ea59b":"markdown","c8364646":"markdown","0cb3c920":"markdown","11a6da74":"markdown","a5930f0a":"markdown","1eac7148":"markdown","320ae4b9":"markdown","76ccd900":"markdown","e54ed2d5":"markdown","5888ed97":"markdown","b0e6d585":"markdown","6564a146":"markdown","42415e85":"markdown","34f28e40":"markdown","48d6377f":"markdown","297b5c8c":"markdown","9a9d23b6":"markdown","3e53737e":"markdown","676a98f9":"markdown","4f0bd621":"markdown","6be2558e":"markdown"},"source":{"93625ee7":"#Importing necessary libraries\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\npd.options.display.max_columns = 100\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport os","3f625dd4":"#Reading the data set as a dataframe\n\ncars = pd.read_csv('..\/input\/geely-auto\/CarPriceAssignment.csv')","514ea8ee":"#Viewing the cars dataframe\n\ncars.head()","b85ecadc":"#Dimensions of the dataframe\ncars.shape","5a372390":"#Checking the dataframe for any null values\ncars.info()","2e0334d1":"#Getting a statistical view of the numerical variables of the dataframe\ncars.describe()","6deb9864":"# Dropping car_ID variable since it has nothing to do with price\n\ncars.drop('car_ID', axis=1, inplace=True)","11b5d4ed":"cars.head(10)","220cdbc3":"plt.figure(figsize=(10,10))\nsns.pairplot(cars)\nplt.show()","fa9d0478":"#Correlations of price with other numeric variables\n\ncars[cars.columns[1:]].corr()['price'][:].round(2).sort_values(ascending=True)","c7ec3a23":"cars.drop(columns = ['carheight','stroke','compressionratio','peakrpm'], axis=1, inplace=True)","2888b4ac":"cars['symboling'].value_counts()","48febd68":"#Categorising symboling values -3 to 0 as Low Risk and the remaining positive values as High Risk\n\ndef categorise(x):\n    if(-3 <= x <= 0):\n        return \"Low Risk\"\n    else:\n        return \"High Risk\"\n        \ncars['symboling'] = cars['symboling'].apply(lambda x: categorise(x))","cd48d7fd":"cars.head()","a7d352f6":"#Extract the Company name from 'CarName' variable\n\ncars['company'] = cars['CarName'].apply(lambda x: x.split(' ')[0])\ncars['company'] = cars['company'].str.lower()","4323a81e":"#Correcting the incorrect company names\n\ndef compName(x):\n    if (x == \"vw\" or x == \"vokswagen\"):\n        return \"volkswagen\"\n    elif(x == \"toyouta\"):\n        return \"toyota\"\n    elif(x == \"porcshce\"):\n        return \"porsche\"\n    elif(x == \"maxda\"):\n        return \"mazda\"\n    \n    else:\n        return x\n    \ncars['company'] = cars['company'].apply(lambda x: compName(x))","8eea6dc3":"#Dropping the CarName variable\n\ncars.drop('CarName',axis=1,inplace=True)","bcc9c236":"cars.head()","440c07af":"plt.figure(figsize=(25,25))\nfig_num = 0\ndef plot_categorical(var):       #Function to plot boxplots for all categorical variables\n    plt.subplot(3,4, fig_num)\n    sns.boxplot(x = var, y = 'price', data = cars)\n\ncategorical_vars = cars.dtypes[cars.dtypes==object].index\nfor var in categorical_vars:\n    fig_num = fig_num + 1\n    plot_categorical(var)\n\nplt.show()","906da065":"#Dropping doornumber variable from the dataset\n\ncars.drop('doornumber', axis=1, inplace=True)","06af3eeb":"#Moving the price column to the front of the dataframe for better readability\n\ncars = cars[['price','symboling','fueltype','aspiration','carbody','drivewheel','enginelocation','wheelbase','carlength','carwidth',\n 'curbweight','enginetype','cylindernumber','enginesize','fuelsystem','boreratio','horsepower','company']]","cb62fbb3":"cars.head()","6def15b5":"def binary_map(x):\n    cars[x] = cars[x].astype(\"category\").cat.codes\n\nbinary_categorical_vars = ['symboling','fueltype','aspiration','enginelocation']\nfor var in binary_categorical_vars:\n    binary_map(var)","cbc667eb":"cars.head()","b90193be":"print(\"Engine Type\")\nprint(cars['enginetype'].value_counts(normalize=True).round(2))\nprint(\"\\n\")\nprint(\"Drivewheel\")\nprint(cars['drivewheel'].value_counts(normalize=True).round(2))\nprint(\"\\n\")\nprint(\"Carbody\")\nprint(cars['carbody'].value_counts(normalize=True).round(2))\nprint(\"\\n\")\nprint(\"Fuel System\")\nprint(cars['fuelsystem'].value_counts(normalize=True).round(2))","3c64dc9f":"def eng_map(x):\n    if(\"ohc\" in x):\n        return 1\n    else:\n        return 0\n\ncars['enginetype'] = cars.enginetype.apply(lambda x: eng_map(x))","24bc8217":"cars['enginelocation'].value_counts(normalize=True).round(2)","c41eae3b":"cars.drop('enginelocation', axis = 1, inplace=True)","59ce082f":"# Converting \"cylindernumber\" values to its corresponding number\n\ncars['cylindernumber'].replace({\"two\":2,\"three\":3,\"four\":4,\"five\":5, \"six\":6,\"eight\":8,\"twelve\":12}, inplace=True)","b7118f11":"#Get the dummy variables for carbody and store in separate variable \"carbody_dummies\"\n\ncarbody_dummies = pd.get_dummies(cars['carbody'],prefix='carbody',drop_first=True)\ncars = pd.concat([cars,carbody_dummies], axis=1)\ncars.drop('carbody',axis=1,inplace=True)","9c245a40":"#Get the dummy variables for drivewheel and store in separate variable \"drivewheel_dummies\"\n\ndrivewheel_dummies = pd.get_dummies(cars['drivewheel'],prefix='dw',drop_first=True)\ncars = pd.concat([cars,drivewheel_dummies], axis=1)\ncars.drop('drivewheel',axis=1,inplace=True)","25135a3f":"#Get the dummy variables for company and store in separate variable \"company_dummies\"\n\ncompany_dummies = pd.get_dummies(cars['company'],prefix='comp',drop_first=True)\ncars = pd.concat([cars,company_dummies], axis=1)\ncars.drop('company',axis=1,inplace=True)","f8ae0743":"#Get the dummy variables for fuelsystem and store in separate variable \"fuelsys_dummies\"\n\nfuelsys_dummies = pd.get_dummies(cars['fuelsystem'],prefix='dw',drop_first=True)\ncars = pd.concat([cars,fuelsys_dummies], axis=1)\ncars.drop('fuelsystem',axis=1,inplace=True)","9fcdfbeb":"cars.head()","fa8ed20a":"cars.shape","679e9abb":"cars.describe()","faf1b115":"# from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(cars, train_size = 0.7, test_size = 0.3, random_state=100)","1b68380d":"# from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","2c6c10ba":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n\nnum_vars = ['price','wheelbase','carlength','carwidth','curbweight','cylindernumber','enginesize','boreratio','horsepower']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","5daf7b5e":"df_train.head()","be2511ff":"df_train.describe()","fd460834":"y_train = df_train.pop('price')\nX_train = df_train","5fbd3cdb":"#Creating an object of LinearRegression class and using RFE to get the top 20 variables from the dataset\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm,20)\nrfe = rfe.fit(X_train, y_train)","0fe0831a":"#Collecting the 20 variables selected by RFE\n\ncols = X_train.columns[rfe.support_]\ncols","9b359328":"#Selecting these 20 variables from X_train data set and assign to new variable X_train_rfe\n\nX_train_rfe = X_train[cols]","10403f2d":"#Add a constant to X_train_rfe data set using statsmodels.api library as sm\n\nX_train_lm = sm.add_constant(X_train_rfe)","7d8e5f07":"lm = sm.OLS(y_train, X_train_lm).fit()  # Running the linear model\nprint(lm.summary())                     # Viewing the summary of the linear model","211e7b4c":"#Calculating the VIF of the variables using variance_inflation_factor library\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","932efefb":"X_train_new = X_train_rfe.drop('curbweight',axis=1)","b7d65985":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","62d04341":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e451e16a":"X_train_new = X_train_new.drop('comp_peugeot',axis=1)","7130348d":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","48e06b65":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c783a9b0":"X_train_new = X_train_new.drop('comp_isuzu',axis=1)","3c505c8b":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","ce99a8bd":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5122a039":"X_train_new = X_train_new.drop(['enginesize'], axis=1)","884609fa":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","5e1dd224":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","44522d35":"X_train_new = X_train_new.drop('cylindernumber', axis=1)","127d9412":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","f87dc2cc":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7f3f3e7b":"X_train_new = X_train_new.drop('boreratio', axis=1)","1e4aa661":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","cc25ab32":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b20871ac":"X_train_new = X_train_new.drop('enginetype', axis=1)","5cb5ef12":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","6b2022d5":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","630a99bf":"X_train_new = X_train_new.drop('comp_saab', axis=1)","2902be8c":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","b2c0d572":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9e06ad8d":"X_train_new = X_train_new.drop('comp_subaru', axis=1)","befd8774":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","aa7c9afe":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7710e52d":"X_train_new = X_train_new.drop('comp_audi', axis=1)","ab4cddd5":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","1f92efdd":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","27a594fa":"X_train_new = X_train_new.drop('carwidth', axis=1)","345e763b":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","6dc99230":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c98b6f3e":"X_train_new = X_train_new.drop('carbody_sedan', axis=1)","f69aaafe":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","5216e4ae":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d44871b5":"X_train_new = X_train_new.drop('carbody_wagon', axis=1)","6d7454a4":"X_train_lm = sm.add_constant(X_train_new)\nlm = sm.OLS(y_train, X_train_lm).fit()\nprint(lm.summary())","3bb5aeea":"#Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","41365456":"X_train_new.head()","31a2284e":"X_train_lm.head()     # Training set with the constant","c3d44b83":"y_train_pred = lm.predict(X_train_lm)\nerror = y_train - y_train_pred","1a9b0903":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot(error, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18) ","4f882972":"plt.figure(figsize=(5,5))\nsns.regplot(y_train_pred,error)\nplt.xlabel('y_train_pred')\nplt.ylabel('Error')","d04b7e7f":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n\nnum_vars_test = ['price','wheelbase','carlength','carwidth','curbweight','cylindernumber','enginesize','boreratio','horsepower']\n\ndf_test[num_vars_test] = scaler.transform(df_test[num_vars_test])","7be817ec":"df_test.head()","61d8c30d":"y_test = df_test.pop('price')\nX_test = df_test","9d448933":"# Creating X_test_new dataframe by only selecting variables present in the X_train training set\n\nX_test_new = X_test[X_train_new.columns]\n\n# Adding a constant variable\nX_test_new = sm.add_constant(X_test_new)","7a9ec023":"y_test_pred = lm.predict(X_test_new)","f45eb6d4":"fig = plt.figure()\nplt.scatter(y_test,y_test_pred)\nfig.suptitle('y_test vs y_test_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_test_pred', fontsize=16)   ","f9711894":"from sklearn.metrics import r2_score\nr2_score(y_test, y_test_pred)","b47c5751":"Hello there! Welcome to my first kernel where I've designed a Linear Regression model to predict car prices. Since this is my first kernel your suggestions\/feedbacks will be very much appreciated. \nPS: Please upvote if you found it helpful :)","a83899c0":"`comp_audi` is insignificant in presence of other variables. Can be dropped.","bcc9dfd8":"#### Let's check the percentage amount of each level of the categorical variables.","f73b3c7d":"`enginesize` has high VIF. Can be dropped.","6bebe060":"# 4. Splitting the data into training and testing sets\n\nNow that we have prepared our data, we can go ahead and make the train-test split","2a24a9ab":"# 8. Model Evaluation","b2f24469":"**Model 5:<br>\nR-squared value: 0.928<br>\nAdj. R-squared value: 0.919**","bc645c83":"# 1. Reading and Understanding the data","4c0115fc":"**Model 13:<br>\nR-squared value: 0.875<br>\nAdj. R-squared value: 0.867**","f0816985":"![cars.PNG](attachment:cars.PNG)\nSource: shutterstock","c6f850ac":"### Model 13","0f0d40f7":"`comp_peugeot` is insignificant in presence of other variables. Can be dropped.","e82b6139":"**Model 2:<br>\nR-squared value: 0.945<br>\nAdj. R-squared value: 0.936**","fb02c50e":"**Model 3:<br>\nR-squared value: 0.944<br>\nAdj. R-squared value: 0.936**","55be9342":"# Multiple Linear Regression model\n\n## Car Prices Case Study\n\n### Problem Statement:\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts.\n\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market","5f30be5d":"# 9. R2 score","7bef996b":"Therefore, the equation for our best fitted line is:\n\n$ price = 0.72 \\times horsepower - 0.051 \\times carbody\\_hatchback + 0.3 \\times comp\\_bmw + 0.42 \\times comp\\_buick + 0.314 \\times comp\\_jaguar + 0.174 \\times comp\\_porsche + 0.127 \\times comp\\_volvo $","b6527134":"**Model 9:<br>\nR-squared value: 0.922<br>\nAdj. R-squared value: 0.915**","07c39f9f":"From the above plots, we can see there are obvious effects of all categorical variables (except **doornumber**) on `price`. **doornumber** does not seem to have much effect on car prices. Hence, we will drop the **doornumber** variable from the dataset.","4b41ef61":"# 7. Residual analysis for the train data\n\nSo, now to check if the error terms are also normally distributed, let us plot the histogram of the error terms and see what it looks like.","d26c589a":"### Model 9","88188243":"**Model 1:<br>\nR-squared value: 0.948<br>\nAdj. R-squared value: 0.939**","6b59baea":"**Model 14:<br>\nR-squared value: 0.874<br>\nAdj. R-squared value: 0.868**","3126775f":"### Dividing into X and Y sets for the model building","a19782ed":"Since `symbolising` is a categorical variable, lets convert it's numeric values to corresponding categorical values.<br>\nHere the assumption is:<br>\n- symboling value in the range -3 and 0 are considered **Low Risk**<br>\n- symboling values other than this (i.e. greater than 0) are considered **High Risk**","f53085ad":"# 6. Inferences from the final model","1069fb2f":"### Model 5","f3d3a11c":"# 10. Equation of best fitted line","e94d869c":"### Model 2","bc4839e8":"As seen in the above plot, there is no pattern in the error values.","97b84bb2":"`boreratio` is insignificant in presence of other variables and has high VIF. Can be dropped.","3f767c90":"# 2. Visualising the numeric variables","4bbfd55e":"`carbody_sedan` is insignificant in presence of other variables. Can be dropped.","1f110e6b":"### Model 14","0b01689b":"### Model 3","0901ba02":"`comp_subaru` is insignificant in presence of other variables. Can be dropped.","bd6d9ab8":"**Model 7:<br>\nR-squared value: 0.927<br>\nAdj. R-squared value: 0.919**","3e0e7ae7":"# 3. Data Preparation","8e07ff70":"#### We will now process a few numeric and categorical variables","d9165f2d":"# 5. Building the model","f532caa9":"Now let's use our model to make predictions.","e43ee98f":"Thank you for viewing my kernel :)","35dc71b5":"`carwidth` has high VIF. Can be dropped.","e2554892":"`comp_isuzu` is insignificant in presence of other variables. Can be dropped.","694d3a8b":"### Model 10","74267a0a":"From the final model (Model 14), we can conclude that the most important driving factors (variables) for predicting car prices are **horsepower, carbody, and company**.<br>\n\n- **Horsepower** effects car prices the most with a coefficient of 0.72\n- **Carbody** is also another driving factor for car prices, mainly `hatchback` in this case. Geely Auto could focus on Hatchback designs for their cars.\n- **Company** of the car is the third factor, mainly `jaguar`, `porsche`, `bmw`, `buick`, and `volvo`. These car companies may be of interest to Geely Auto in terms of price variations, designs, configurations etc.","eee246d5":"**Model 10:<br>\nR-squared value: 0.921<br>\nAdj. R-squared value: 0.914**","6e02c2cf":"### Model 11","b9b16057":"**Model 8:<br>\nR-squared value: 0.923<br>\nAdj. R-squared value: 0.915**","d012971e":"Here we see that about **72%** of the **`enginetype`** variable is **ohc**. Also, others are also some form of **ohc** engines. Therefore, lets go ahead and mark the ohc type engines as 1 and others as 0. <br>","330c2d65":"#### Let us check the correlations of the numeric variables with price","71681b8a":"As we can see, **`enginelocation`** variable almost only has 0 (front) as the value. So we can drop this variable assuming that almost all cars have engine located at the front.","6dda954b":"`carbody_wagon` is insignificant in presence of other variables. Can be dropped.","c8f38726":"`enginetype` is insignificant in presence of other variables and has high VIF. Can be dropped.","9f0cc722":"### Model 6","9964b6c8":"### Rescaling the features","068ea59b":"**Model 12:<br>\nR-squared value: 0.876<br>\nAdj. R-squared value: 0.867**","c8364646":"`curbweight` is insignificant in presence of other variables and also has high VIF. Can be dropped.","0cb3c920":"### Model 8","11a6da74":"### Model 7","a5930f0a":"We will now create dummies for the variables **`drivewheel`, `carbody`, `company` and `fuelsys`**.","1eac7148":"`cylindernumber` is insignificant in presence of other variables and has high VIF. Can be dropped.","320ae4b9":"We'll now convert the binary categorical variables `symboling`, `fueltype`, `aspiration`, `enginelocation` to numeric.","76ccd900":"### Final training data set","e54ed2d5":"**Model 6:<br>\nR-squared value: 0.928<br>\nAdj. R-squared value: 0.919**","5888ed97":"#### Making the predictions.","b0e6d585":"There are categorical variables in the data set with string values which need to be converted to numeric in order to fit a regression line.","6564a146":"## Visualising the categorical variables","42415e85":"**Model 4:<br>\nR-squared value: 0.942<br>\nAdj. R-squared value: 0.934**","34f28e40":"### Model 12","48d6377f":"`comp_saab` is insignificant in presence of other variables. Can be dropped.","297b5c8c":"### Dividing into X and y sets for the model building","9a9d23b6":"Looking at the above correlations, we can see a few numerical variables that do not quite show a considerably good linear relationship with **`price`**<br>\nThese are namely:<br>\n- carheight (0.12)\n- stroke (0.08)\n- compressionratio (0.07)\n- peakrpm (-0.09)<br>\n\nTherefore, we can go ahead and drop these variables.","3e53737e":"**Model 11:<br>\nR-squared value: 0.919<br>\nAdj. R-squared value: 0.912**","676a98f9":"### Model 4","4f0bd621":"### Model 1","6be2558e":"Let us also extract the company name from the `CarName` variable and assign this new derived metric into a new categorical variable named `company`"}}