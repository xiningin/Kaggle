{"cell_type":{"6920f738":"code","78173fb2":"code","38655a1f":"code","b1f3aaf2":"code","bed65668":"code","fba33898":"code","d9aa1a02":"code","953111db":"code","f7146377":"code","a26e7ceb":"code","9c2b3932":"code","f6061dd9":"code","a1f2001f":"code","154a43b8":"code","4ef58b2c":"code","3e57d852":"code","db48b840":"code","7845164c":"markdown","13ca4db6":"markdown","17e65f20":"markdown","f228d2e8":"markdown","18f2e033":"markdown","a0157a71":"markdown","bfb2bd52":"markdown","9b5af568":"markdown","621c50c4":"markdown","16aae6f4":"markdown","0380fa68":"markdown"},"source":{"6920f738":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nimport random\n\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom pathlib import Path\n\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","78173fb2":"!kaggle competitions download -c journey-springfield","38655a1f":"!cp -r \"..\/input\/journey-springfield\" \".\/\"","b1f3aaf2":"def load_image(file):\n    image = Image.open(file)\n    image.load()\n    return image","bed65668":"TRAIN_DIR = Path('.\/journey-springfield\/train\/simpsons_dataset')\ntrain_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntrain_val_labels = [path.parent.name for path in train_val_files]","fba33898":"def get_count(labels):\n    df = pd.DataFrame(labels, columns=['name'])\n    df['count'] = 1\n    return df.groupby('name').count().sort_values('count')","d9aa1a02":"data = get_count(train_val_labels)","953111db":"def show_data(data, min_level=True, max_level=True):\n    plt.figure(figsize=(15, 9))\n    sns.barplot(x=data.index, y=data['count'], orient='v').set_xticklabels(data.index, rotation=90)\n    if max_level:\n        plt.axhline(y=1500)\n    if min_level:\n        plt.axhline(y=100, color='red')\n    plt.show()","f7146377":"show_data(data)","a26e7ceb":"data['to_add'] = data['count'].apply(lambda x: 1500 - x if x < 1500 else 0)","9c2b3932":"data['one_iteration'] = data.apply(lambda x: math.ceil(x['to_add'] \/ x['count']), axis=1)\ndata","f6061dd9":"augmenters = {\n    'Crop': transforms.Compose([\n                                transforms.Resize(size=300, max_size=301),\n                                transforms.CenterCrop(size=300),\n                                transforms.RandomCrop(250)\n                                ]),\n    'Rotate': transforms.RandomRotation(degrees=(-25, 25)),\n    'HFlip': transforms.RandomHorizontalFlip(p=1)\n}","a1f2001f":"n_examples = 3\n\nfig, ax = plt.subplots(nrows=n_examples, ncols=(len(augmenters) + 1),figsize=(10, 10))\n\nfor i in range(n_examples):\n    random_character = int(np.random.uniform(0, len(train_val_files)))\n    img_orig = load_image(train_val_files[random_character])\n    img_label = train_val_files[random_character].parent.name\n\n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                img_label.split('_')))\n    ax[i][0].imshow(img_orig)\n    ax[i][0].set_title(img_label)\n    ax[i][0].axis('off')\n        \n    for j, (augmenter_name, augmenter) in enumerate(augmenters.items()):\n        img_aug = augmenter(img_orig)\n        ax[i][j + 1].imshow(img_aug)\n        ax[i][j + 1].set_title(augmenter_name)\n        ax[i][j + 1].axis('off')\n","154a43b8":"for image_path in tqdm(train_val_files):\n    path = image_path.parents[0]\n    character = image_path.parent.name\n    img = load_image(image_path)\n    # if we don't need to add any images, we can continue without transformation\n    if data.loc[character]['to_add'] <= 0:\n        continue\n\n    # if number of images to create for one iteration is greater than the rest of images to create,\n    # we should use 'to_add' instead of 'one_iteration'  \n    if data.loc[character]['one_iteration'] > data.loc[character]['to_add']:\n        iter_size = data.loc[character]['to_add']\n    else:\n        iter_size = data.loc[character]['one_iteration']\n    data.loc[character]['to_add'] -= iter_size\n    \n    for i in range(iter_size):\n        augmenter = random.choice(list(augmenters.values()))\n        aug_img = augmenter(img)\n        aug_img.save(f\"{path}\/{image_path.name.split('.')[0]}_{i}.jpg\")","4ef58b2c":"!zip -r \"journey-springfield-augmented.zip\" . > \/dev\/null","3e57d852":"aug_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\naug_labels = [path.parent.name for path in aug_files]\naug_data = get_count(aug_labels)","db48b840":"show_data(aug_data, min_level=False)","7845164c":"As we can see, the data is highly unbalanced.\n\nWe would also recommend to manually add images for classes with less then 100 images, because extending such little number of images only by augmentation may not give desired result.\n\nLet's say we would like to have at least 1500 images per class.","13ca4db6":"Now we can either download the archive to use it in other environment, or proceed to train our model here.","17e65f20":"We will import some methods and variables from the original notebook.","f228d2e8":"![image.png](attachment:f521bfb1-4a40-4077-aaf4-1c88c4dc417e.png)","18f2e033":"# Data Augmentation\n\nIn this notebook we will augment the existing data.","a0157a71":"The `one_iteration` column will show us, how much new images we should make from one original image.","bfb2bd52":"---\n\nLet's run a cycle for all images in the train folder.","9b5af568":"We decided to randomly apply one of the following 3 transforms:\n- `RandomCrop` (`Resize` and `CenterCrop` are used to turn the image into a squared one)\n- `RandomRotation`\n- `RandomHorizontalFlip`\n\nMore types of transformation can be found at `PyTorch` [Illustration of transforms](http:\/\/pytorch.org\/vision\/master\/auto_examples\/plot_transforms.html) ","621c50c4":"Let's see how our data looks like now.","16aae6f4":"We will copy the data to output directory so we would be able to save images to existing directories inside the dataset.","0380fa68":"We can check how augmented data will look like."}}