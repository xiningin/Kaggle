{"cell_type":{"cc9401ad":"code","9e36d051":"code","404d0e8b":"code","88b0f959":"code","154ee129":"code","7f410a27":"code","b4f7cede":"code","b2c43d52":"code","8a188b59":"code","1551494f":"code","0d3613da":"code","211061bd":"code","dbbdf956":"code","5f030ca2":"code","31fc01f0":"code","33857ac6":"code","8299d35f":"code","08f09f2e":"code","97d2db45":"code","c7838892":"code","cee0b66d":"code","a17dbb25":"code","dade89a9":"markdown","a0010bb3":"markdown","fc26ce83":"markdown","b32e72e2":"markdown","6ef20d5a":"markdown","8ebda00e":"markdown","25414b54":"markdown","6421a73b":"markdown","c1d44b0d":"markdown","6031d5b9":"markdown","e767850f":"markdown","d2af0a12":"markdown","cc75137c":"markdown","e079e8af":"markdown","cc0a63b3":"markdown","e02037da":"markdown"},"source":{"cc9401ad":"!find \/ -name \\*.arff","9e36d051":"JAR = '..\/input\/java-jars\/weka.jar'\nARFF = '\/opt\/conda\/lib\/python3.7\/site-packages\/scipy\/io\/arff\/tests\/data\/iris.arff'","404d0e8b":"!head -n 80 {ARFF}","88b0f959":"!which java","154ee129":"!java -version","7f410a27":"!java -cp {JAR} weka.core.Instances -h","b4f7cede":"!java -cp {JAR} weka.core.Instances {ARFF}","b2c43d52":"!java -cp {JAR} weka.core.converters.CSVSaver -h","8a188b59":"!java -cp {JAR} weka.core.converters.CSVSaver -i {ARFF} -o iris.csv","1551494f":"!head iris.csv","0d3613da":"!java -cp {JAR} weka.classifiers.functions.MultilayerPerceptron -h","211061bd":"!java -cp {JAR} weka.classifiers.functions.MultilayerPerceptron -t {ARFF} -d saved_model","dbbdf956":"!file saved_model","5f030ca2":"!strings saved_model | head -n 20","31fc01f0":"!java -cp {JAR} weka.classifiers.trees.J48 -t {ARFF}","33857ac6":"!java -cp {JAR} weka.classifiers.trees.RandomForest -t {ARFF}","8299d35f":"!java -cp {JAR} weka.classifiers.trees.LMT -h","08f09f2e":"!java -cp {JAR} weka.classifiers.trees.LMT -t {ARFF}","97d2db45":"!java -cp {JAR} weka.classifiers.bayes.NaiveBayes -t {ARFF}","c7838892":"!java -cp {JAR} weka.classifiers.lazy.IBk -t {ARFF}","cee0b66d":"!java -cp {JAR} -Xmx15g weka.core.SystemInfo","a17dbb25":"!javac","dade89a9":"# J48 Decision Tree","a0010bb3":"`file` command is not here so let's peek at the contents: it saves a serialised class file","fc26ce83":"Or convert arff to CSV:","b32e72e2":"# K Nearest Neighbours\n\nIs called \"lazy instance-based classifier\" :)","6ef20d5a":"To run weka on the command line you specify the full name of a class as the first argument.\n\nYou can see a list of the utilities and models included in [this index][1].\n\nFor example create an Instances class to inspect the arff data:\n\n[1]: https:\/\/www.kaggle.com\/jtrotman\/java-jars-index","8ebda00e":"# RandomForest\n\nA lot of decision trees...","25414b54":"# Conclusion\n\nWeka tends to be slower than Python libraries on larger datasets, but it is open source and the algorithms are written in a quite straightforward way: with an IDE it's possible to step through the training process, to get a better idea of how each model works.\n\nFor competitions: it is possible to pass in a test set and save test set predictions (`-T` and `-p` options). There are a few models in Weka not yet seen in Python so perhaps one day an old-fashioned tabular competition will launch where a Weka model like LMT could add diversity to an ensemble!\n\nThis notebook could also serve as a template for any other Java you want to run on Kaggle (for example [shared solutions in old Santa optimisation challenges][1]). You will have to upload compiled code; it seems the compiler is not installed:\n\n[1]: https:\/\/www.kaggle.com\/c\/packing-santas-sleigh\/discussion\/6934","6421a73b":"Nice, the Confusion Matrix at the end shows only 2+2=4 errors","c1d44b0d":"Seems nothing beat the MLP we started with!\n\n# SystemInfo\n\nUse this code on enough tasks and you will eventually run into memory problems.\nUse the -Xmx flag to increase memory available to java.\nSystemInfo shows useful debug info - note how `memory.max` is set to 15360MB because of the `-Xmx15g` argument:","6031d5b9":"# Weka Java Demo\n\nDemonstrating the use of Weka machine learning Java library from the command line. There are no `import` statements in this notebook - all code is run in a shell using the `!` magic. The Weka jar library is loaded from [this dataset][1].\n\nFirst find some arff files in the Kaggle Docker image...\n\n[1]: https:\/\/www.kaggle.com\/jtrotman\/java-jars","e767850f":"I spy an \"iris\" ;)\n","d2af0a12":"# Logistic Model Trees\n\nDecision trees with a logistic regression model at each leaf node :)","cc75137c":"Similarly we can train models from the command line: specify the name of the model as the first argument.\nFor example to get help on MultilayerPerceptron:","e079e8af":"# NaiveBayes","cc0a63b3":"Which version of java do we have?","e02037da":"Now try MultilayerPerceptron with the iris data, and default options.\n\nBy default a model trained on the whole training set is printed first, then cross validation is run to generate a better report.\n"}}