{"cell_type":{"d66d1b5f":"code","80bf1975":"code","8728326e":"code","d8442304":"code","a97f5b68":"code","5fd6d6c7":"code","eba63501":"code","f5cce105":"code","8bdffff1":"code","1b982d53":"code","903e02b8":"code","58001c8b":"code","4a474732":"code","8b1e2f59":"code","d9476eec":"code","b7b6a575":"code","2ce635da":"code","1922f7b4":"code","1fbcac3f":"code","414b01fa":"code","c5c99cbc":"code","04747183":"code","86b3faa6":"code","2a76ba6b":"code","fffd1d39":"code","f7147edc":"code","5781d6ee":"markdown","226c2b44":"markdown","b82864d0":"markdown","a0f24fe6":"markdown","d4817a2e":"markdown","51efb435":"markdown","025a5e1b":"markdown","d72f924f":"markdown","1849428f":"markdown","8e4095b2":"markdown","e57b84ec":"markdown","fd4527dc":"markdown","b319f627":"markdown","6d31091c":"markdown"},"source":{"d66d1b5f":"# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\n\npd.set_option(\"display.max_rows\", None,\"display.max_columns\", None)\nwarnings.simplefilter(action='ignore')\nplt.style.use('seaborn')","80bf1975":"df = pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head()","8728326e":"df.info()","d8442304":"df.describe()","a97f5b68":"df.drop('Id',inplace=True,axis=1)","5fd6d6c7":"# Checking distribution of target classes\nsns.countplot(df[\"Species\"])","eba63501":"# Plotting boxplots to check the distribution of numerical columns\ncols = df.columns[:-1].tolist()\nfig,ax = plt.subplots(2,2,figsize=(10,7))\nr = c = 0\nfor col in cols:\n  sns.boxplot(x=col, data=df,ax=ax[r,c])\n  if c == 1:\n    r+=1\n    c = 0\n    continue\n  c+=1","f5cce105":"# Visualizing numerical columns clustered based on species\nsns.pairplot(df,hue='Species')","8bdffff1":"'''\nOne cool more sophisticated technique pandas has available is called Andrews Curves\nAndrews Curves involve using attributes of samples as coefficients for Fourier series\nand then plotting these'''\nfrom pandas.plotting import andrews_curves\nandrews_curves(df, \"Species\")","1b982d53":"'''\nAnother multivariate visualization technique pandas has is parallel_coordinates\nParallel coordinates plots each feature on a separate column & then draws lines\nconnecting the features for each data sample\n'''\nfrom pandas.plotting import parallel_coordinates\nparallel_coordinates(df, \"Species\")","903e02b8":"X = df.drop('Species',axis=1).values\n# Since this ia a multiclass-classification, output labels are one-hot encoded for training the ANN\ny = pd.get_dummies(df['Species']).values","58001c8b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test  = train_test_split(X,y,test_size=0.25,random_state=101)","4a474732":"# Performing min-max scaling\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","8b1e2f59":"# Importing keras related libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","d9476eec":"# input_shape\nX_train_scaled.shape[1:]","b7b6a575":"def build_model(n_hidden = 1, n_neurons=5, learning_rate=3e-3, input_shape=X_train_scaled.shape[1:]):\n  '''\n  Builds a keras ANN for Multi-Class Classification i.e. output classes which are mutually exclusive\n  ''' \n  model = Sequential()\n  options = {\"input_shape\": input_shape}\n  # Adding input and hidden layers\n  for layer in range(n_hidden):\n    model.add(Dense(n_neurons,activation=\"relu\",**options))\n    options = {}\n  # Adding output layer having 3 neurons, 1 per class\n  model.add(Dense(3,activation='softmax'))\n  # Creating instance of adam optimizer\n  opt = Adam(learning_rate=learning_rate)\n  model.compile(optimizer=opt,loss='categorical_crossentropy',metrics='accuracy')\n  return model","2ce635da":"# Applying KerasClassifier Wrapper to neural network\nkeras_cls = KerasClassifier(build_model)","1922f7b4":"from tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import RandomizedSearchCV","1fbcac3f":"param_dict = {\n    \"n_hidden\" : (2,3),\n    \"n_neurons\" : tuple(range(2,7)),\n    \"learning_rate\" : (3e-2,3e-3,3e-4)\n}\n\nmodel_cv = RandomizedSearchCV(keras_cls, param_dict, n_iter=10, cv=3)","414b01fa":"%%time\nmodel_cv.fit(\n    X_train_scaled, y_train, epochs=150,\n    validation_data = (X_test_scaled,y_test),\n    callbacks = [EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=10)],\n    verbose=0\n)","c5c99cbc":"model_cv.best_params_","04747183":"model_cv.best_score_","86b3faa6":"# building model based on best set of parameters obtained from RandomSearchCV\nbest_set = model_cv.best_params_\n\nmodel = build_model(learning_rate= best_set['learning_rate'], \n                    n_hidden= best_set['n_hidden'], n_neurons= best_set['n_neurons'])","2a76ba6b":"model.fit(\n    X_train_scaled, y_train, epochs=100,\n    validation_data = (X_test_scaled,y_test),\n    callbacks = [EarlyStopping(monitor='val_loss', mode='min', patience=10)],\n    verbose=0\n)","fffd1d39":"# Plotting accuracy, loss of train and validation set\npd.DataFrame(model.history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.show()","f7147edc":"from sklearn.metrics import classification_report,confusion_matrix\n\n# Instead of probabilities it provides class labels\npred_classes = model.predict_classes(X_test_scaled)\ny_test_classes = np.argmax(y_test,axis=1)\nprint(classification_report(y_test_classes,pred_classes),\"\\n\\n\")\nprint(confusion_matrix(y_test_classes,pred_classes))","5781d6ee":"For visualizing multi-dimensional variables we can use below mentioned techniques[<a href=\"https:\/\/www.kaggle.com\/benhamner\/python-data-visualizations\">src<\/a>]:\n\n- Andrews Curves\n- Parallel Coordinates","226c2b44":"# **2. Data Preprocessing**","b82864d0":"# **4. Data Preparation**","a0f24fe6":"## **b) Hyperparameter tuning**","d4817a2e":"## **a) Creating model**","51efb435":"## **b) Feature Scaling**","025a5e1b":"## **a) Train-Test Split**","d72f924f":"**Remarks:**\n\nIn this only few parameters are considered for hyperparameter tuning. For better results one can consider various batch_sizes, epochs, etc.","1849428f":"# **3. EDA**","8e4095b2":"## **d) Model evaluation**","e57b84ec":"# **5. Model Creation\/Evaluation**","fd4527dc":"# **1. Getting Familier with dataset**","b319f627":"# **About the dataset**[<a href=\"https:\/\/www.kaggle.com\/uciml\/iris\">src<\/a>]\n\n### **Description**\n\nThe Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the UCI Machine Learning Repository.\n\nIt includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe columns in this dataset are:\n\n    Id\n    SepalLengthCm\n    SepalWidthCm\n    PetalLengthCm\n    PetalWidthCm\n    Species\n\n# **Steps Involved**\n1. Getting familier with dataset\n2. Data Preprocessing\n3. EDA\n4. Data Preparation\n5. Model Creation\/Evaluation","6d31091c":"## **c) Training the model**"}}