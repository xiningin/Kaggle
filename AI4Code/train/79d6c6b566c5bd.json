{"cell_type":{"17c54e8b":"code","4b19b492":"code","0f25f682":"code","cc263969":"code","f102d044":"code","e53ce823":"code","fb535d3d":"code","4ce34756":"code","68233f6e":"code","831256c3":"code","865c803a":"code","935c3ebd":"code","5b721fc4":"code","86ebf887":"code","1fc15498":"code","a512c3d5":"code","36f90261":"code","4176c15f":"code","63e767cf":"markdown","ffbd0e00":"markdown","fe7284b8":"markdown","24892f58":"markdown","500efcd1":"markdown","1c2ace37":"markdown","2384a298":"markdown","2a74cbb8":"markdown"},"source":{"17c54e8b":"#Ignore the warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('always')\n\n#Data visualization and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#configure\n%matplotlib inline\nsns.set(style = 'whitegrid', color_codes = True)\n\nimport nltk\n\n#importing stop-words\nfrom nltk.corpus import stopwords\nstop_words = set(nltk.corpus.stopwords.words('english'))\n\n#Tokenization\nfrom nltk import word_tokenize, sent_tokenize\n\n#Keras\nimport keras\nfrom keras.preprocessing.text import one_hot, Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Embedding, Input\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport tensorflow as tf","4b19b492":"sample_text_1=\"bitty bought a bit of butter\"\nsample_text_2=\"but the bit of butter was a bit bitter\"\nsample_text_3=\"so she bought some better butter to make the bitter butter better\"\n\ncorp = [sample_text_1, sample_text_2, sample_text_3]\nno_docs = len(corp)\nprint(no_docs)","0f25f682":"VOCAB_SIZE = 50\nencod_corp = []\nfor i, doc in enumerate(corp):\n    encod_corp.append(one_hot(doc, VOCAB_SIZE))\n    print('The encoding for document', i+1, 'is : ', one_hot(doc, VOCAB_SIZE))","cc263969":"encod_corp #list of lists","f102d044":"#Finding max_len\nMAX_LEN = -1\nfor doc in corp:\n    tokens = nltk.word_tokenize(doc)\n    if(len(tokens) > MAX_LEN):\n        MAX_LEN = len(tokens)\nprint('The maximum number of unique words in any document is : ', MAX_LEN)","e53ce823":"#How nltk word tokenizes a text\nnltk.word_tokenize(corp[0])","fb535d3d":"#Actual padding\npad_corp = pad_sequences(encod_corp, maxlen=MAX_LEN, padding='post', value=0)\npad_corp","4ce34756":"#Specifying the input shape\ninput = Input(shape=(no_docs, MAX_LEN), dtype='float64')\ninput","68233f6e":"word_input = Input(shape = (MAX_LEN,), dtype = 'float64')","831256c3":"#Creating the embedding\nword_embedding = Embedding(input_dim = VOCAB_SIZE, output_dim = 8, input_length= MAX_LEN)(word_input)","865c803a":"#Flattening the embedding\nword_vec = Flatten()(word_embedding)\nword_vec","935c3ebd":"#combining all into a Keras Model\nembed_model = Model([word_input], word_vec)","5b721fc4":"#Training the model\nembed_model.compile(optimizer = Adam(lr = 1e-3), loss='binary_crossentropy', metrics = ['acc'])","86ebf887":"#Model summary\nprint(embed_model.summary())","1fc15498":"#Getting the embeddings\nembeddings = embed_model.predict(pad_corp)","a512c3d5":"print('Shape of embeddings : ', embeddings.shape)\nprint(embeddings)","36f90261":"#Reshaping embeddings\nembeddings = embeddings.reshape(-1, MAX_LEN, 8)\nprint('Shape of embeddings : ', embeddings.shape)\nprint(embeddings)","4176c15f":"for i, doc in enumerate(embeddings):\n    for j, word in enumerate(doc):\n        print('The encoding for Word', j+1, 'in Document', i+1, ' : ', word)","63e767cf":"Getting encoding for a word in a document","ffbd0e00":"Importing necessary modules and dependencies","fe7284b8":"That's it for Today! Please Upvote if you enjoyed through this kernel like me!","24892f58":"Each word is 8D!","500efcd1":"Integer encoding all texts","1c2ace37":"Creating embeddings","2384a298":"Creating sample texts","2a74cbb8":"Padding texts (to make all texts of the same length)"}}