{"cell_type":{"3c62331d":"code","0b2de877":"code","4fb78097":"code","eee1a233":"code","7398bc28":"code","01e93d15":"code","1d1dc02d":"code","7ecde831":"code","63225d6a":"code","98cc5a77":"code","a4c731c6":"code","3498ae86":"code","8729bece":"code","129162e5":"code","e2fd6321":"markdown","47fa490d":"markdown","a3da0ca5":"markdown","9000aadc":"markdown","2cef227c":"markdown","c5eef187":"markdown"},"source":{"3c62331d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Dropout, Flatten, Dense\n\nfrom sklearn.preprocessing import LabelBinarizer","0b2de877":"# Try to detect TPUs\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","4fb78097":"# Load the data\ntrain_data = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv\")\ntest_data = pd.read_csv(\"..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv\")\n\ntrain_data.head()","eee1a233":"# Join both Training and Testing sets, shuffling them, splitting them, resizing and rescaling them.\n\ndata = pd.concat([train_data, test_data])\ndata = data.sample(frac=1).reset_index(drop=True)\n\nval_split = 1200\nvalid = data[:val_split]\ntrain = data[val_split:]\n\ntrainX = train.drop(['label'], axis=1).values\ntrainY = train['label']\n\nvalidX = valid.drop(['label'], axis=1).values\nvalidY = valid['label']\n\n# Label Binarize\nlb = LabelBinarizer()\n\ntrainY = lb.fit_transform(trainY)\nvalidY = lb.fit_transform(validY)\n\ntrainX = trainX \/ 255.\nvalidX = validX \/ 255.\n\ntrainX = trainX.reshape(-1, 28, 28, 1)\nvalidX = validX.reshape(-1, 28, 28, 1)","7398bc28":"# Visualize a few Images\nf, ax = plt.subplots(2,5) \nf.set_size_inches(10, 10)\nk = 0\nfor i in range(2):\n    for j in range(5):\n        ax[i,j].imshow(trainX[k].reshape(28, 28) , cmap = \"gray\")\n        k += 1\n    plt.tight_layout()  ","01e93d15":"# Data augmentation to prevent overfitting\nwith strategy.scope():\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            featurewise_center=False,  # set input mean to 0 over the dataset\n            samplewise_center=False,  # set each sample mean to 0\n            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n            samplewise_std_normalization=False,  # divide each input by its std\n            zca_whitening=False,  # apply ZCA whitening\n            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n            zoom_range = 0.1, # Randomly zoom image \n            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n            horizontal_flip=False,  # randomly flip images\n            vertical_flip=False)  # randomly flip images\n\n    datagen.fit(trainX)","1d1dc02d":"# Train the Model on GPU\nwith strategy.scope():\n    model = tf.keras.Sequential()\n    model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n    model.add(Flatten())\n    model.add(Dense(units = 512 , activation = 'relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(units = 24 , activation = 'softmax'))\n    model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n    model.summary()\n    learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","7ecde831":"# Plot the Model Architecture\ntf.keras.utils.plot_model(model)","63225d6a":"# Traing the Model\nwith strategy.scope():\n    history = model.fit(datagen.flow(trainX, trainY, batch_size = 128) ,epochs = 10 , validation_data = (validX, validY) , callbacks = [learning_rate_reduction])","98cc5a77":"# Define a few functions for easy visualizations\ndef get_label(input_image, model=model, good=True):\n    if not good:\n        img = input_image.reshape(1, 28, 28, 1)\n    clss = model.predict_classes([img])\n    return clss[0]\n\ndef get_confidence(input_image, model=model, good=False):\n    if not good:\n        input_image = input_image.reshape(1, 28, 28, 1)\n    clss = model.predict_classes([input_image])\n    conf = model.predict_proba([input_image]).max()\n    return clss[0], conf","a4c731c6":"# Visualize a predicted Image Label and it's Confidence value\n# Original Label = Predicted Label = 9\nclss, conf = get_confidence(validX[141], model)\nplt.title(f\"Predicted Label: {clss}\\nConfidence: {conf * 100:.2f}%\")\nplt.imshow(validX[141].reshape(28, 28), cmap='gray')\nplt.axis('off')\nplt.show()","3498ae86":"# Create a Loss Object, and the adversarial Function\n\nloss_obj = tf.keras.losses.CategoricalCrossentropy()\n\ndef create_adversarial_pattern(input_img, input_label):\n    \"\"\"\n    Watch the Gradients of the Loss w.r.t to the input using TF Gradient Tape\n    Then find the needed loss and return the specified gradient matrix signs(only)\n    \"\"\"\n    \n    with tf.GradientTape() as tape:\n        tape.watch(input_img)\n        preds = model(input_img)\n        prediction = tf.reshape(preds, (24,))\n        loss = loss_obj(input_label, prediction)\n    # Gradient of loss wrt to input image\n    gradient = tape.gradient(loss, input_img)\n    \n    # Sign of gradient to get the peturbation\n    sign = tf.sign(gradient)\n    \n    return sign","8729bece":"# Now, test the function and see the peturbation for the image we visualized before. \ntest_img = tf.convert_to_tensor(validX[141].reshape(1, 28, 28, 1))\ntest_label = validY[141]\n\npeturbation = create_adversarial_pattern(test_img, test_label).numpy()\nplt.imshow(peturbation[0].reshape(28,28)*0.5+0.5)\nplt.title(\"Peturbation Matrix\")\nplt.show()","129162e5":"epsilons = [0, 0.01, 0.1, 0.15]\ndescriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input') for eps in epsilons]\n\nfor i, eps in enumerate(epsilons):\n    adversarial_inp = test_img + eps * peturbation\n    adversarial_inp = tf.clip_by_value(adversarial_inp, -1, 1)\n    \n    # Original Label = Predicted Label = 23\n    clss, conf = get_confidence(adversarial_inp, model, good=True)\n    plt.title(f\"Predicted Label: {clss}\\nConfidence: {conf * 100:.2f}%\")\n    plt.imshow(validX[141].reshape(28, 28), cmap='gray')\n    plt.axis('off')\n    plt.show()","e2fd6321":"<h1 style=\"color:blue\"> Fast Gradient Signing Method <\/h1>\n\nNow, let's get on to the juicy part!","47fa490d":"Now comes the moment of truth, we use the above peturbation matrix for an image (in this case, Image number 141 from Validation Set) and multiply it with Epsilon values and then clip it to see the new confidence values (we have essentially attacked the model now!)\n\n**Remember the Actual Label of this Image: \"9\"**","a3da0ca5":"Now that the theory part is over, let's do the coding!\nI will do the following steps (with some comments so you can work your way through):\n\n1. Loading the Libraries, Data, Splitting the Data and Making sure that overall data is in the right format for training.\n2. Augmenting the Data for better results\n3. Making the Model and plotting it's architecture\n4. Training the Model\n5. Displaying a Sample Image with it's predicted label and confidence\n6. Defining the `create_adversarial_pattern()` function for getting the peturbations\n7. Visualizing the peturbations for a given image\n8. Attacking the Network (essentially using the Peturbations to get Wrong Output)","9000aadc":"**As you can clearly see, the model predicted a completely wrong label (14) instead of (9, original label) that too with 100% confidence!**\n\nLadies and Gentlemen, we have successfully fooled our Model!","2cef227c":"Here are the sources, that I took a lot of help from:\n* Adversarial Examples, Interpretable ML: https:\/\/christophm.github.io\/interpretable-ml-book\/adversarial.html\n* Adversarial FGSM using Tensorflow: https:\/\/www.tensorflow.org\/tutorials\/generative\/adversarial_fgsm\n* This Excellent on getting 100% accuracy on this dataset: https:\/\/www.kaggle.com\/madz2000\/cnn-using-keras-100-accuracy","c5eef187":"<h1 style=\"color:blue\">[Tensorflow]: Adversarial Attack using Fast Gradient Sign Method<\/h1>\n\n![Example of Adversarial Attack](https:\/\/miro.medium.com\/max\/4000\/1*PmCgcjO3sr3CPPaCpy5Fgw.png)\n<p style=\"font-size:10px\">Source: <a href=\"https:\/\/arxiv.org\/abs\/1412.6572\">Explaining and Harnessing Adversarial Examples<\/a>, Goodfellow et al, ICLR 2015.<\/p>\n\n<q>\nA self-driving car crashes into another car because it ignores a stop sign. Someone had placed a picture over the sign, which looks like a stop sign with a little dirt for humans, but was designed to look like a parking prohibition sign for the sign recognition software of the car.\n<\/q>\n<br>\n<br>\nOR\n<br>\n<br>\n<q>    \nA spam detector fails to classify an email as spam. The spam mail has been designed to resemble a normal email, but with the intention of cheating the recipient.\n<\/q>\n<br>\n<hr>\nDoes that sound scary, or atleast worrying? If it does than that's because <span style=\"color:red\">Machine Learning Models aren't made full-proof<\/span>. They can be fooled (that too pretty easily).\n<br><br>\n\n<h2 style=\"color:aqua\">But How do we do that?<\/h2>\n\nWe do that with help of our Lord and Savior:- <strong style=\"color:red\">Adversarial Attack (where we use an Adversarial Example)<\/strong>\n\nAdversarial examples are specialised inputs created with the purpose of confusing a neural network, resulting in the misclassification of a given input. These notorious inputs are indistinguishable to the human eye, but cause the network to fail to identify the contents of the image.\n\nAlthough there are many different types of Adversarial Attacks, I am going to be focsuing on probably the most easiest one out there which is: <strong style=\"color:orange\">Fast Gradient Sign Method<\/strong>.\n\nFor example; In the above Image(which is an image of a panda), the attacker adds small perturbations (distortions) to the original image, which results in the model labelling this image as a gibbon, with high confidence. The process of adding these perturbations is the most important part.\n\n<h2 style=\"color:aqua\">And How do we add those Peturbations?<\/h2>\nWell, you *calculate* those peturbations using the below equation;\n<br><br>\n$$x\u2032=x+\u03f5\u22c5sign(\u25bdxJ(\u03b8,x,y))$$\n\nNow's we see who's-who of this equation:<br>\n\n<strong>\n$x\u2032$: The Adversarial Image\n<br><br>    \n$x$: The original Input Image\n<br><br>\n$\u03f5$: Multiplier to ensure the perturbations are small.\n<br><br>    \n$\ud835\udc60\ud835\udc56\ud835\udc54\ud835\udc5b$: The Sign function (it captures the sign (+ or -) of the whatever it's called on)\n<br><br>\n$\u25bdxJ(\u03b8, x, y)$: Gradients (or derivative) of Loss function ($J$) with respect to the Input Image ($x$)\n<\/strong>\n<br><br>\n<br><br>\nNow that you know what all those crazy symbols mean, allow me to quickly run through what all these things do, so we can move towards actually doing stuff;\n<br><br>\nThe fast gradient sign method works by using the gradients of the neural network to create an adversarial example. For an input image, the method uses the gradients of the loss with respect to the input image to create a new image that maximises the loss. This new image is called the adversarial image."}}