{"cell_type":{"704a237e":"code","99cfaaf7":"code","ec7e2736":"code","d5033b81":"code","c938898c":"code","3a683f9e":"code","106b5878":"code","aa6494ce":"code","98d8bab9":"code","e70639bd":"markdown","eb39f99e":"markdown"},"source":{"704a237e":"import numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","99cfaaf7":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","ec7e2736":"from sklearn import datasets\ndf = datasets.load_iris()","d5033b81":"df=pd.DataFrame({\n    'sepal length':df.data[:,0],\n    'sepal width':df.data[:,1],\n    'petal length':df.data[:,2],\n    'petal width':df.data[:,3],\n    'species':df.target\n})\ndf.head()","c938898c":"X=df[['sepal length', 'sepal width', 'petal length', 'petal width']]  # Features\ny=df['species']  # Label","3a683f9e":"# standarize data\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX=scaler.fit_transform(X)","106b5878":"# a simple pipeline for hyperparameter optimization of all the 5 different models\n# (could have wrote a more condense function )\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nfrom sklearn.metrics import classification_report\n\n# Load and split the data, into 70\/30 training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23)\n","aa6494ce":"import warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.simplefilter(action='ignore', category=DataConversionWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)","98d8bab9":"# 1. assign a name to the classifier that you want to use\ncls_name = {0: 'random_forest', 1: 'logistic_regression', 2: 'support_vector'}\n\n# 2. assign the actual classifier name in sklearn\nclassifiers = [\n    RandomForestClassifier(),\n    LogisticRegression(),\n    SVC()\n    ]\n# 3. This is the manual part, specify the corresponding hyperparameters that you want to optimize, hyperparameters are different for each classifer in most cases. Be careful of conditional hyperparameters, you will get an error if not specified correctly\nparameters = [{'clf__criterion': ['gini', 'entropy'],\n 'clf__n_estimators': [20,30],\n'clf__min_samples_leaf': [25,50],\n'clf__max_depth': [3,4,5,6,7]},\n    \n{'clf__C': [0.001, 0.1, 1, 5],\n'clf__class_weight': [None,'balanced',{0:0.25, 1:0.75}],\n'clf__solver': ['lbfgs', 'liblinear']},\n    \n{'clf__C': [0.001, 0.1, 1, 5],\n'clf__class_weight': [None,'balanced']}\n]\n    \n# Fit the grid search objects\nprint('hyperparameters grid search in process... ')\n\n\n#create a placehold for best accuracy and best model parameters, these place must be set within in the loop, not global.\nbest_val_acc = 0\nbest_val_clf = []\nbest_val_gs = []\n    \nbest_test_acc = 0\nbest_test_clf = []\nbest_test_gs = []\n\n# there are only 3 parameters to change, the name of the classifier, the actual classifer in sklearn and the corresponding hyperparameters\nfor idx, classifier, params in zip(cls_name, classifiers, parameters):\n    \n    clf_pipe = Pipeline([\n        ('clf', classifier)\n        ])\n    gs_clf = GridSearchCV(clf_pipe, param_grid=params, n_jobs=-1)\n    \n    print('\\nEstimator: %s' % cls_name[idx])\n    # Fit grid search\n    gs_clf.fit(X_train, y_train)\n    # Best params\n    print('Best params: %s' % gs_clf.best_params_)\n    # Best validation data accuracy\n    print('Mean cross-validated score of the best_estimator: %.3f' % gs_clf.best_score_)\n    # Predict on test data with best params\n    y_pred = gs_clf.predict(X_test)\n    # Test data accuracy of model with best params and print classification report\n    print('Test set accuracy score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n    print (classification_report(y_test, y_pred))\n    \n    # Track best validation accuracy model, default k=5 since v0.22\n    if gs_clf.best_score_ > best_val_acc:\n        best_val_acc = gs_clf.best_score_\n        best_val_gs = gs_clf\n        best_val_clf = cls_name[idx]\n    # Track best test accuracy model\n    if accuracy_score(y_test, y_pred) > best_test_acc:\n        best_test_acc = accuracy_score(y_test, y_pred)\n        best_test_gs = gs_clf\n        best_test_clf = cls_name[idx]\n    # note: Often the best validation is the also the best test accuracy model, however in rare instances that may not be the case. If the results of validation between two classifier are really close, then it can easily have a different best testing accuracy classifer. it's up to the data scientist to investigate further the most suitable model to use.","e70639bd":"## This an automated pipeline hyperparameter optimization search for any classification algorithms in sklearn","eb39f99e":"### step 1. input cls_name(user defined)\n### step 2. input actual classifier name\n### step 3. specify corresponding hyperparameters"}}