{"cell_type":{"5b25bdce":"code","f67e2648":"code","a38179ca":"code","c5331678":"code","709b6658":"code","47d16add":"code","556dd93b":"code","e312c3bd":"code","a35b91ba":"code","1395a19e":"code","9883af94":"code","16f6d266":"code","f6333fad":"code","74693008":"code","238dfcb8":"code","5af0ebfc":"code","22514603":"code","4e2f4104":"code","fb24358c":"code","fc70d7f2":"code","ed9a8ae3":"code","c56bacd4":"code","01c100af":"markdown","346da1f8":"markdown","0f0e15f8":"markdown","fd33928a":"markdown","f913ca1d":"markdown","5ee14b33":"markdown","ace665ef":"markdown","901ce81f":"markdown","f663b3af":"markdown"},"source":{"5b25bdce":"!pip install torchsummary","f67e2648":"import os\nfrom collections import Counter\nfrom tqdm import tqdm\nimport sys\nimport ast\n\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom torch.utils.data import DataLoader\nfrom torchsummary import summary\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\n\nimport greatbarrierreef","a38179ca":"INPUT_DIR = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\nINPUT_DIR_IMG = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/'\nsys.path.append(INPUT_DIR)\nsys.path.append(INPUT_DIR_IMG)","c5331678":"TRAINING_RATIO = 0.8\n\ndata_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'))\n\n# Split the dataset so that no sequence is leaked from the training dataset into the validation dataset.\nsplit_index = int(TRAINING_RATIO * len(data_df))\nwhile data_df.iloc[split_index - 1].sequence == data_df.iloc[split_index].sequence:\n    split_index += 1\n\n# Shuffle both the training and validation datasets.\ntrain_data_df = data_df.iloc[:split_index].sample(frac=1).reset_index(drop=True)\nval_data_df = data_df.iloc[split_index:].sample(frac=1).reset_index(drop=True)\n\ntrain_positive_count = len(train_data_df[train_data_df.annotations != '[]'])\nval_positive_count = len(val_data_df[val_data_df.annotations != '[]'])\n\nprint('Training ratio (all samples):', \n      float(len(train_data_df)) \/ (len(train_data_df) + len(val_data_df)))\nprint('Training ratio (positive samples):', \n      float(train_positive_count) \/ (train_positive_count + val_positive_count))","709b6658":"# Take only the positive images for training and validation\ntrain_data_df = train_data_df[train_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for training:', len(train_data_df))\nval_data_df = val_data_df[val_data_df.annotations != '[]'].reset_index()\nprint('Number of positive images used for validation:', len(val_data_df))\n\ntrain_data_df[\"annotations\"] = train_data_df[\"annotations\"].map(lambda x : ast.literal_eval(x))\nval_data_df[\"annotations\"] = val_data_df[\"annotations\"].map(lambda x : ast.literal_eval(x))\n\ntrain_data_df[\"filepath\"] = train_data_df.apply(lambda x : f\"video_{x.video_id}\/{x.video_frame}.jpg\", axis=1)\nval_data_df[\"filepath\"] = val_data_df.apply(lambda x : f\"video_{x.video_id}\/{x.video_frame}.jpg\", axis=1)\n\ntrain_data_df.head(3)","47d16add":"train_data_df.head(10)","556dd93b":"val_data_df.head(10)","e312c3bd":"HEIGHT, WIDTH = 720, 1280\npresize = 512\nsize = 384\n\ndf = train_data_df\n# df = df.explode(\"annotations\")\n\ndf[\"W\"] = [WIDTH]*len(df)\ndf[\"H\"] = [HEIGHT]*len(df)\ndf[\"label\"] = [1]*len(df)\n\n# df[\"x\"] = df.apply(lambda x : x.annotations[\"x\"]\/1280, axis=1)\n# df[\"y\"] = df.apply(lambda x : x.annotations[\"y\"]\/720, axis=1)\n# df[\"width\"] = df.apply(lambda x : x.annotations[\"width\"]\/1280, axis=1)\n# df[\"height\"] = df.apply(lambda x : x.annotations[\"height\"]\/720, axis=1)\n\n# df.loc[df[\"width\"] > 1, \"width\"] = 1\n# df.loc[df[\"height\"] > 1, \"height\"] = 1\n\ndf = df.drop([\"video_id\",\"sequence\",\"video_frame\",\"sequence_frame\",\n              \"image_id\"], axis=1)\n\ndf = df.reset_index(drop=True)\ndf.head(3)","a35b91ba":"vdf = val_data_df\n# df = df.explode(\"annotations\")\n\nvdf[\"W\"] = [WIDTH]*len(vdf)\nvdf[\"H\"] = [HEIGHT]*len(vdf)\nvdf[\"label\"] = [1]*len(vdf)\n\n# df[\"x\"] = df.apply(lambda x : x.annotations[\"x\"]\/1280, axis=1)\n# df[\"y\"] = df.apply(lambda x : x.annotations[\"y\"]\/720, axis=1)\n# df[\"width\"] = df.apply(lambda x : x.annotations[\"width\"]\/1280, axis=1)\n# df[\"height\"] = df.apply(lambda x : x.annotations[\"height\"]\/720, axis=1)\n\n# df.loc[df[\"width\"] > 1, \"width\"] = 1\n# df.loc[df[\"height\"] > 1, \"height\"] = 1\n\nvdf = vdf.drop([\"video_id\",\"sequence\",\"video_frame\",\"sequence_frame\",\n              \"image_id\"], axis=1)\n\nvdf = vdf.reset_index(drop=True)\nvdf.head(3)","1395a19e":"architecture_config = [\n                       #format: (kernel_size, num_filters, stride, padding)\n                       (7, 64, 2, 3),\n                       \"M\",\n                       (3, 192, 1, 1),\n                       \"M\",\n                       (3, 128, 1, 0),\n                       (1, 256, 1, 1),\n                       (3, 256, 1, 0),\n                       (1, 512, 1, 1),\n                       \"M\",\n                       [(1, 256, 1, 0),(3, 512, 1, 1), 4], # 4 = no. of times these two tuples should be repeated\n                       (1, 512, 1, 0),\n                       (3, 1024, 1, 1),\n                       \"M\",\n                       [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n                       (3, 1024, 1, 1),\n                       (3, 1024, 2, 1),\n                       (3, 1024, 1, 1),\n                       (3, 1024, 1, 1),\n]","9883af94":"class CNNBlock(nn.Module):\n  def __init__(self, in_channels, out_channels, **kwargs):\n    super(CNNBlock, self).__init__()\n    self.conv = nn.Conv2d(in_channels, out_channels, bias = False, **kwargs)\n    self.batchnorm = nn.BatchNorm2d(out_channels)\n    self.leakyrelu = nn.LeakyReLU(0.1)\n\n  def forward(self, x):\n    return self.leakyrelu(self.batchnorm(self.conv(x)))","16f6d266":"class yolov1(nn.Module):\n  def __init__(self, in_channels=3, **kwargs):\n    super(yolov1, self).__init__()\n    self.architecture = architecture_config\n    self.in_channels = in_channels\n    self.darknet = self._create_conv_layers(self.architecture)\n    self.fcs = self._create_fcs(**kwargs)\n\n  def forward(self, x):\n    x = self.darknet(x)\n#     print(x.shape)\n    aap = nn.AdaptiveAvgPool2d((7,7))\n#     print(x.shape)\n    x = aap(x)\n#     print(x.shape)\n    return self.fcs(torch.flatten(x, start_dim=1))\n\n  def _create_conv_layers(self, architecture):\n    layers = []\n    in_channels = self.in_channels\n\n    for x in architecture:\n      if type(x) == tuple:\n        layers += [\n                   CNNBlock(in_channels, \n                            x[1], \n                            kernel_size = x[0], \n                            stride = x[2], \n                            padding = x[3])\n                  ]\n        in_channels = x[1]\n\n      elif type(x) == str:\n        layers += [nn.MaxPool2d(kernel_size=2, stride = 2)]\n      elif type(x) == list:\n        conv_1 = x[0]\n        conv_2 = x[1]\n        num_repeats = x[2]\n\n        for i in range(num_repeats):\n          layers += [\n                     CNNBlock(in_channels,\n                              conv_1[1],\n                              kernel_size = conv_1[0],\n                              stride = conv_1[2],\n                              padding = conv_1[3]),\n                     CNNBlock(conv_1[1],\n                              conv_2[1],\n                              kernel_size = conv_2[0],\n                              stride = conv_2[2],\n                              padding = conv_2[3])\n          ]\n\n          in_channels = conv_2[1]\n    return nn.Sequential(*layers)\n\n  def _create_fcs(self, split_size, num_boxes, num_classes):\n    S, B, C = split_size, num_boxes, num_classes\n    return nn.Sequential(\n        nn.Flatten(),\n        nn.Linear(1024 * S * S, 512),\n        nn.Dropout(0.0),\n        nn.LeakyReLU(0.1),\n        nn.Linear(512, S * S *(C + B * 5)),\n    )","f6333fad":"def test(S = 7, B = 2, C = 1):\n  model = yolov1(split_size = S, num_boxes = B, num_classes = C)\n  x = torch.randn((1, 3, 672, 672))\n  print(model(x).shape)\n\ntest()","74693008":"class yololoss(nn.Module):\n  def __init__(self, S = 7, B = 2, C = 1):\n    super(yololoss, self).__init__()\n    self.mse = nn.MSELoss(reduction=\"sum\")\n    self.S = S\n    self.B = B\n    self.C = C\n    self.lambda_coord = 5\n    self.lambda_noobj = 0.5\n\n  def forward(self, prediction, target):\n    prediction = prediction.reshape(-1, self.S, self.S, self.C + self.B * 5)\n#     print(prediction.shape)\n\n    iou_b1 = intersection_over_union(prediction[...,2:6], target[...,2:6])\n    iou_b2 = intersection_over_union(prediction[...,7:11], target[...,2:6])\n\n    ious = torch.cat([iou_b1.unsqueeze(0),iou_b2.unsqueeze(0)], dim =0)\n    iou_maxes, best_box = torch.max(ious, dim=0)\n    exists_box = target[..., 1].unsqueeze(3) #Iobj_i\n    \n    if torch.sum(best_box) > 1:\n        best_box = 1\n    else:\n        best_box = 0\n    ####  BOX LOSS  ####\n    #Box-Coordinates (mid-point, width & height)\n    box_predictions = exists_box * (\n        (\n            best_box * prediction[...,7:11]\n            + (1 - best_box) * prediction[...,2:6]\n        )\n    )\n\n    box_targets = exists_box * target[...,2:6]\n\n    box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n        torch.abs(box_predictions[..., 2:4] + 1e-6)\n    )\n    \n    #box dimensions: (N, S, S, 6)\n    box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n    #(N, S, S, 4) -> (N*S*S, 4)\n    box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2),\n                        torch.flatten(box_targets, end_dim=-2),\n                        )\n    \n    #### OBJECT LOSS ####\n    pred_box = (\n        best_box * prediction[..., 6:7] + (1 - best_box) * prediction[...,1:2]\n    )\n    # (N,S,S,1) -> (N*S*S*1)\n    object_loss = self.mse(\n        torch.flatten(exists_box * pred_box),\n        torch.flatten(exists_box * target[...,1:2])\n    )\n\n    #### NO-OBJECT LOSS ####\n    #(N, S, S, 1) -> (N, S*S*1)\n    no_object_loss = self.mse(\n        torch.flatten((1 - exists_box) * prediction[..., 1:2], start_dim=1),\n        torch.flatten((1 - exists_box) * target[..., 1:2], start_dim=1),\n    )\n\n    no_object_loss = self.mse(\n        torch.flatten((1 - exists_box) * prediction[..., 6:7], start_dim=1),\n        torch.flatten((1 - exists_box) * target[..., 1:2], start_dim=1),\n    )\n\n    class_loss = self.mse(\n        torch.flatten(exists_box * prediction[...,:1], end_dim = -2),\n        torch.flatten(exists_box * target[...,:1], end_dim = -2)\n    )\n\n    loss = (\n        self.lambda_coord * box_loss \n        + object_loss\n        + self.lambda_noobj * no_object_loss\n        + class_loss\n    )\n\n    return loss","238dfcb8":"class COTSDataset(torch.utils.data.Dataset):\n  def __init__(self, dataframe, img_dir,\n               S=7, B=2, C=1, transform=None):\n    self.annotations = dataframe\n    self.img_dir = img_dir\n    self.transform = transform\n    self.S = S\n    self.C = C\n    self.B = B\n\n  def __len__(self):\n    return len(self.annotations)\n\n  def __getitem__(self, index):\n#     label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n    boxes = []\n#     with open(label_path) as f:\n    for box in self.annotations[\"annotations\"][index]:\n        class_label, x, y, width, height = self.annotations[\"label\"][index], box[\"x\"]\/1280, box[\"y\"]\/720, box[\"width\"]\/1280, box[\"height\"]\/720\n        boxes.append([class_label, x, y, width, height])\n\n    img_path = os.path.join(self.img_dir, self.annotations[\"filepath\"][index])\n    image = Image.open(img_path)\n    boxes = torch.Tensor(boxes)\n\n    if self.transform:\n      image, boxes = self.transform(image, boxes)\n\n    label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n\n    for box in boxes:\n      class_label, x, y, width, height = box.tolist()\n      class_label = int(class_label)\n      i, j = int(self.S * x), int(self.S * y)\n      x_cell, y_cell = self.S * x - i, self.S * y - j\n      width_cell, height_cell = (\n          width * self.S,\n          height *self.S,\n      )\n\n      if label_matrix[i, j, 1] == 0:\n        label_matrix[i, j, 1] = 1 \n        box_coordinates = torch.Tensor(\n            [x_cell, y_cell, width_cell, height_cell]\n        )\n        label_matrix[i,j, 2:6] = box_coordinates\n        label_matrix[i,j, class_label] = 1\n\n      return image, label_matrix\n\n","5af0ebfc":"\ndef intersection_over_union(boxes_pred, boxes_labels, box_format = \"midpoint\"):\n\n  if box_format == \"midpoint\":\n    box1_x1 = boxes_pred[..., 0:1] - boxes_pred[..., 2:3]\/2\n    box1_y1 = boxes_pred[..., 1:2] - boxes_pred[..., 3:4]\/2\n    box1_x2 = boxes_pred[..., 0:1] + boxes_pred[..., 2:3]\/2\n    box1_y2 = boxes_pred[..., 1:2] + boxes_pred[..., 3:4]\/2\n    box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3]\/2\n    box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4]\/2\n    box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3]\/2\n    box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4]\/2\n\n  elif box_format == \"corners\":\n    box1_x1 = boxes_pred[..., 0:1]\n    box1_y1 = boxes_pred[..., 1:2]\n    box1_x2 = boxes_pred[..., 2:3]\n    box1_y2 = boxes_pred[..., 3:4]\n    box2_x1 = boxes_labels[..., 0:1]\n    box2_y1 = boxes_labels[..., 1:2]\n    box2_x2 = boxes_labels[..., 2:3]\n    box2_y2 = boxes_labels[..., 3:4]\n\n  x1 = torch.max(box1_x1, box2_x1)\n  y1 = torch.max(box1_y1, box2_y1)\n  x2 = torch.min(box1_x2, box2_x2)\n  y2 = torch.min(box1_y2, box2_y2)\n\n  intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n#   print(intersection)\n\n  box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n  box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n  union  = box1_area + box2_area -intersection\n\n  return intersection\/(union + 1e-6)\n\n\n\ndef non_max_suppression(\n    bboxes,\n    iou_threshold,\n    prob_threshold,\n    box_format = \"corners\"\n                      ):\n  \n  assert type(bboxes) == list\n\n  bboxes = [box for box in bboxes if box[1] > prob_threshold]\n  bboxes = sorted(bboxes, key=lambda x : x[1], reverse = True)\n  bboxes_aft_nms = []\n\n  while bboxes:\n    chosen_box = bboxes.pop(0)\n\n    bboxes = [\n              box for box in bboxes\n              if box[0] != chosen_box[0]\n              or intersection_over_union(\n                  torch.Tensor(chosen_box[2:]),\n                  torch.Tensor(box[2:]),\n                  box_format = box_format\n              ) < iou_threshold\n    ]\n\n    bboxes_aft_nms.append(chosen_box)\n\n  return bboxes_aft_nms\n\n\n\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold = 0.5, box_format=\"corners\", num_classes=20):\n\n  #pred_boxes_format = [[train_id, class_prob, box_prob, x1, y1, x2, y2], ...]\n  average_precisions = []\n  epsilon = 1e-6\n\n  for c in range(num_classes):\n    detections = []\n    ground_truths = []\n\n    for detection in pred_boxes:\n      if detection[1] == c:\n        detections.append(detection)\n\n    for true_box in true_boxes:\n      if true_box[1] == c:\n        ground_truths.append(true_box)\n\n    amount_boxes = Counter([gt[0] for gt in ground_truths])\n    \n    for key, val in amount_boxes.items():\n      amount_boxes[key] = torch.zeros(val)\n\n    detections.sort(key = lambda x: x[2], reverse = True)\n    TP = torch.zeros((len(detections)))\n    FP = torch.zeros((len(detections)))\n    total_true_bboxes = len(ground_truths)\n\n    for detection_idx, detection in enumerate(detections):\n      ground_truth_img = [\n              bbox for bbox in ground_truths  if bbox[0] == detection[0]\n                   ]\n\n      num_gts = len(ground_truth_img)\n      best_iou = 0\n\n      for idx, gt in enumerate(ground_truth_img):\n        iou = intersection_over_union(\n            torch.Tensor(detection[3:]),\n            torch.Tensor(gt[3:]),\n            box_format = box_format,\n            )\n        \n        if iou > best_iou:\n          best_iou = iou\n          best_gt_idx = idx\n\n      if best_iou > iou_threshold:\n\n        if amount_boxes[detection[0]][best_gt_idx] == 0:\n            TP[detection_idx] = 1\n            amount_boxes[detection[0]][best_gt_idx] = 1\n        else:\n            FP[detection_idx] = 1\n       \n      else:\n        FP[detection_idx] = 1 \n\n    TP_cumsum = torch.cumsum(TP, dim=0)\n    FP_cumsum = torch.cumsum(FP, dim=0)\n    recalls = TP_cumsum \/ (total_true_bboxes + epsilon)\n    precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n    precisions = torch.cat((torch.tensor([1]), recalls))\n    recalls = torch.cat((torch.tensor([0]), recalls))\n    average_precisions.append(torch.trapz(precisions, recalls))\n\n  return sum(average_precisions) \/ len(average_precisions)\n\n\n\ndef convert_cellboxes(predictions, S=7):\n    \"\"\"\n    Converts bounding boxes output from Yolo with\n    an image split size of S into entire image ratios\n    rather than relative to cell ratios. \n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, 7, 7, 11)\n    bboxes1 = predictions[..., 2:6]\n    bboxes2 = predictions[..., 7:11]\n    scores = torch.cat(\n        (predictions[..., 1].unsqueeze(0), predictions[..., 6].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 \/ S * (best_boxes[..., :1] + cell_indices)\n    y = 1 \/ S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 \/ S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :1].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., 1], predictions[..., 6]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes\n\n\n\ndef get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=\"cuda\",\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                prob_threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n\n\n\ndef plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # box[0] is x midpoint, box[2] is width\n    # box[1] is y midpoint, box[3] is height\n\n    # Create a Rectangle potch\n    for box in boxes:\n        box = box[2:]\n        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n        upper_left_x = box[0] - box[2] \/ 2\n        upper_left_y = box[1] - box[3] \/ 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","22514603":"m = nn.AdaptiveAvgPool2d((5,7))\ninp = torch.randn(1, 64, 8, 9)\noutput = m(inp)\noutput.shape","4e2f4104":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nload_path = \"..\/input\/custom-yolov1-from-scratch\/yolov1.pth\"\nmodel = yolov1(split_size = 7, num_boxes=2, num_classes = 1).to(DEVICE)\nstate_dict = torch.load(load_path)\nmodel.load_state_dict(state_dict)\nmodel.fcs = nn.Sequential(\n#     nn.AdaptiveAvgPool2d((9,7)),\n    nn.Flatten(),\n    nn.Linear(1024 * 7 * 7, 512),\n    nn.Dropout(0.0),\n    nn.LeakyReLU(0.1),\n    nn.Linear(512, 7 * 7 *(1 + 2 * 5)),\n)\nmodel.to(DEVICE)\n\nsummary(model, (3, 672, 672))","fb24358c":"seed = 123\ntorch.manual_seed(seed)\nIMG_DIR = \"..\/input\/tensorflow-great-barrier-reef\/train_images\"\nlr = 3e-5\nBATCH_SIZE = 8\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nEPOCHS = 5\nsave_path = \"yolov1_2.pth\"\nfine_tune = True\nload_path = \"..\/input\/custom-yolov1-from-scratch\/yolov1.pth\"\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes):\n        for t in self.transforms:\n            img, bboxes = t(img), bboxes\n\n        return img, bboxes\n\ntransform = Compose([transforms.Resize((672, 672)), transforms.ToTensor(),])\n\n\n\n\ndef train_fn(train_loader, test_loader, model, optimizer, loss_fn):\n  loop = tqdm(train_loader, leave= True)\n  mean_loss = []\n  mean_test_loss = []\n\n  for batch_id, (x,y) in enumerate(loop):\n    x,y = x.to(DEVICE), y.to(DEVICE)\n    out = model(x)\n#     print(out.shape)\n#     print(y.shape)\n    loss = loss_fn(out,y)\n    mean_loss.append(loss.item())\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    loop.set_postfix(loss = loss.item())\n    \n  model.eval()\n  for batch_idx, (x, labels) in enumerate(test_loader):\n        x = x.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.no_grad():\n            predictions = model(x)\n        \n        test_loss = loss_fn(predictions, labels)\n        mean_test_loss.append(test_loss.item())\n        \n  model.train() \n    \n#   scheduler.step()\n\n    \n  print(f\"Mean loss was{sum(mean_loss)\/len(mean_loss)}\")\n  print(f\"Test Mean loss was{sum(mean_test_loss)\/len(mean_test_loss)}\")\n \n\ndef main():\n  model = yolov1(split_size = 7, num_boxes=2, num_classes = 1).to(DEVICE)\n  optimizer = optim.SGD(model.parameters(), lr = lr)\n#   scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, 50)\n  loss_fn = yololoss()\n\n  if fine_tune:\n    state_dict = torch.load(load_path, map_location=DEVICE)\n    model.load_state_dict(state_dict)\n\n  train_dataset = COTSDataset(df, img_dir = IMG_DIR, transform = transform)\n\n  test_dataset = COTSDataset(vdf, img_dir = IMG_DIR, transform = transform)\n\n  train_loader = DataLoader(\n      dataset = train_dataset,\n      batch_size = BATCH_SIZE,\n      shuffle=True,\n      drop_last = True\n  )\n\n  test_loader = DataLoader(\n      dataset = test_dataset,\n      batch_size = 2,\n      shuffle=True,\n      drop_last = True\n  )\n\n\n  for epochs in range(EPOCHS):\n\n\n    pred_boxes, target_boxes = get_bboxes(\n            test_loader, model, iou_threshold=0.5, threshold=0.4\n        )\n\n    mean_avg_prec = mean_average_precision(\n            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n        )\n    print(f\"Valid mAP: {mean_avg_prec}\")\n\n    train_fn(train_loader, test_loader, model, optimizer, loss_fn)\n    \n#     scheduler.step()\n#     print(scheduler.get_lr())\n    \n    torch.save(model.state_dict(), save_path)\n\n    #  if mean_avg_prec > 0.9:\n    #        checkpoint = {\n    #            \"state_dict\": model.state_dict(),\n    #            \"optimizer\": optimizer.state_dict(),\n    #        }\n    #        save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)","fc70d7f2":"main() #uncomment to start training loop","ed9a8ae3":"model = yolov1(split_size = 7, num_boxes=2, num_classes = 1).to(DEVICE)\nstate_dict = torch.load(\".\/yolov1_2.pth\")\nmodel.load_state_dict(state_dict)\n\ntrain_dataset = COTSDataset(df, img_dir = IMG_DIR, transform = transform)\n\ntest_dataset = COTSDataset(vdf, img_dir = IMG_DIR, transform = transform)\n\ntrain_loader = DataLoader(\n  dataset = train_dataset,\n  batch_size = BATCH_SIZE,\n  shuffle=True,\n  drop_last = True\n)\n\npred_boxes, target_boxes = get_bboxes(\n            train_loader, model, iou_threshold=0.5, threshold=0.4\n        )\n\nmean_avg_prec = mean_average_precision(\n        pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n    )\nprint(f\"train mAP: {mean_avg_prec}\")","c56bacd4":"for epoch in range(4):\n    for x, y in train_loader:\n       x = x.to(DEVICE)\n       for idx in range(8):\n           bboxes = cellboxes_to_boxes(model(x))\n           bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, prob_threshold=0.4, box_format=\"midpoint\")\n           plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n\n       import sys\n       sys.exit()","01c100af":"# **Utility Funtions**","346da1f8":"# **Import packages**","0f0e15f8":"# **Training Loop**","fd33928a":"# **Creating PyTorch Dataset Class for COTS**","f913ca1d":"# **Prepare Training dataset**","5ee14b33":"# **YOLO V1 Implementation for COTS dataset**\n\nGoing back to roots of YOLO architectures and implementing the first version from the YOLO family ***first introduced back in 2015*** being tested today to detect COTS.\n\nThis a just of fun implementations (obviously!) of YOLO V1, I've tried to mimic the original paper and architecture to as close as possible with a few tweaks here and there.\nMy main purpose here was to experiment on how the original YOLO v1 would perform on the present day object detection challenge.\nThe original YOLO v1 model was first trained on Image Classification dataset **ImageNet** and then fine-tuned on **COCO** dataset to perform Object detection. However no such pre-training on ImageNet has been done here due constraints, the model has been trained completely from scratch on COTS dataset.\n\n**Any suggestions to further Improve mAP and minimize loss in terms of hyperparameters will be highly appreciated. I would very much like to test the limits of this architecture on this particular dataset. One such Idea would be to make use of modern day training procedures for object detection that promises improvements in accuracy.**\n\n*The implementation is heavily inspired from this amazing [YouTube tutorial by Aladdin Persson](https:\/\/www.youtube.com\/watch?v=n9_XyCGr-MI&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=47)*\n![image.png](attachment:12b26799-4f03-404a-8ec1-18ab105bc94f.png)![image.png](attachment:2b310436-8e8c-45f1-9fe8-b26c4c4dd1ac.png)","ace665ef":"Wrote a few utility fucntions commonly used for object detection from scratch to understand them even better. Definitely, by far it's not the best and the most efficient implementation of these algorithms.","901ce81f":"# **Defining YOLO Architecture**\nSmall add-on to the original architecture is **batch normalization**.\n![image.png](attachment:a17435a5-a00b-45f8-b7b2-adcf329424e0.png)![image.png](attachment:55e308a2-6caf-4443-b29c-3e21a0e9c73b.png)","f663b3af":"# **YOLO Loss**"}}