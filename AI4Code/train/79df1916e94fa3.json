{"cell_type":{"e1856a10":"code","4674616b":"code","74e7edb9":"code","6bb664b6":"code","a874a16c":"code","2b1b6fd4":"code","8edb3fe2":"code","019a9c3f":"code","a118c915":"code","2ba45415":"code","f4297361":"code","dfb28cdb":"code","cacf8e43":"code","4f23ffe1":"code","904261a7":"code","344009c6":"code","70972d04":"code","06317930":"code","b492e9b4":"code","fbc4b583":"code","59ce8a40":"code","02cf575b":"code","aa77b55c":"code","9b827025":"code","6795152e":"code","0697ffdc":"code","ce017e09":"code","4cab53f6":"code","a6a97392":"code","21c650eb":"code","e9fa6e56":"code","1f13292e":"code","90b8c8f7":"code","51f52efb":"code","be086acf":"code","a2910a2d":"code","82d24804":"code","2a218c7c":"code","1ca1606f":"code","9e306580":"code","328341bd":"code","1b3c1125":"code","fa7464f0":"code","932b6fef":"code","12875d39":"code","aec1c52c":"code","a203cbeb":"code","e8e980ef":"code","05caf0f5":"code","364330c0":"code","75f2f048":"code","99a78b9d":"code","c45e28f0":"code","f3f7ee93":"code","f015b3f6":"code","be973787":"code","4fa91879":"code","c6843641":"code","fce561da":"code","add49c11":"code","754c3885":"code","0b249fe5":"code","a6718126":"code","0bb9fc45":"code","6326cfdc":"markdown","8a2e190f":"markdown","8b166fe4":"markdown","1a53d1c1":"markdown","4e5aa9e6":"markdown","f0c8f6f8":"markdown","559128d9":"markdown","1a595f9d":"markdown","2cc190d5":"markdown","2388bec2":"markdown","11f207e8":"markdown","10438b5a":"markdown","7da44981":"markdown","4685446f":"markdown"},"source":{"e1856a10":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4674616b":"# Read csv file\ndf=pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","74e7edb9":"#print top-5 records\ndf.head()","6bb664b6":"# Check how many rows and columns\ndf.shape","a874a16c":"# Check statistic information\ndf.describe()","2b1b6fd4":"# Check any missing values , datatypes all features in the dataset\ndf.info()","8edb3fe2":"# remove 'No' feature becuase it's no so necessary here\ndf.drop('No',axis=1,inplace=True)","019a9c3f":"#print top-5 records\ndf.head()","a118c915":"# Change the columns name (remove X1,X2.. from columns name )\ndf.rename(columns={\"X1 transaction date\":\"transaction_date\",\"X2 house age\":\"house_age\",\"X3 distance to the nearest MRT station\":\"distance_to_nearest_station\",\"X4 number of convenience stores\":\"number_of_convenience_stores\",\"X5 latitude\":\"latitude\",\"X6 longitude\":\"longitude\",\"Y house price of unit area\":\"house_price\"},inplace=True)","2ba45415":"#print top-5 records\ndf.head()","f4297361":"# take only years fro the transaction date\nnew=df['transaction_date'].astype(str).str.split(\".\",n=1,expand=True)","dfb28cdb":"df['transaction_year']=new[0]","cacf8e43":"# check the relation between house price and transaction year\ndata=df.copy()\ndata.groupby('transaction_year')['house_price'].median().plot.bar()\nplt.title('Transaction Year vs House Price')\nplt.xlabel('Transaction Year')\nplt.ylabel('House Price')\nplt.show()","4f23ffe1":"# remove the transaction_date column\ndf.drop('transaction_date',axis=1,inplace=True)","904261a7":"# create the list for numeric features\nnumeric_feature= list(df.select_dtypes(include=['int64','float64']).keys())\nnumeric_feature","344009c6":"import matplotlib.pyplot as plt\nimport seaborn as sns\n# check the distribution of all numeric features\nfor feature in numeric_feature:\n    sns.histplot(data=df,x=feature,kde=True,palette='pastel')\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.ylabel('frequancy')\n    plt.show()\n    ","70972d04":"# Check the outliers\nfor feature in numeric_feature:\n    data=df.copy()\n    if data[feature].unique() is 0:\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(feature)\n        plt.title(feature)\n        plt.ylabel('count')\n        plt.show()","06317930":"sns.pairplot(df,hue='transaction_year')","b492e9b4":"# Transform all skewed data into log normal distribution\n# create a list of skewed features\n\nfor feature in numeric_feature:\n    if 0 in df[feature].unique():\n        pass\n    else:\n        df[feature]=np.log(df[feature])\n        sns.histplot(data=df,x=feature,kde=True,palette='pastel')\n        plt.title(feature)\n        plt.ylabel('count')\n        plt.show()","fbc4b583":"df.head()","59ce8a40":"# Check the outliers and remove outliers\nfeatures=['house_age','distance_to_nearest_station','latitude','house_price']\nfor feature in features:\n    df.boxplot(feature)\n    plt.title(feature)\n    plt.ylabel('count')\n    plt.show()","02cf575b":"# remove outliers\n#crete the function return lower_range and upper_range\ndef outlier_treatment(datacolumn):\n    sorted(datacolumn)\n    Q1,Q3 = np.percentile(datacolumn,[25,75])\n    IQR = Q3-Q1\n    lower_range = Q1 - (1.5 * IQR)\n    upper_range = Q3 + (1.5 * IQR)\n    return lower_range,upper_range","aa77b55c":"# Check percentage outliers in each feature\nfor feature in features:\n        data=df.copy()\n        lowerbound,upperbound = outlier_treatment(data[feature])\n        print(f\"Feature is {feature} and {np.round(data[(data[feature] < lowerbound) | (data[feature] > upperbound)].shape[0]\/ data.shape[0]*100,4)}% outliers\")\n        print(\"------------------------------\")","9b827025":"# remove outliers\nfor feature in features:\n    lowerbound,upperbound = outlier_treatment(df[feature])\n    df.drop(df[(df[feature]>upperbound)|(df[feature]<lowerbound)].index,inplace=True)\n    print(f\"Feature is {feature} and {np.round(df[(df[feature] < lowerbound) | (df[feature] > upperbound)].shape[0]\/ df.shape[0]*100,4)}% outliers\")\n    print(\"---------------------------------\")","6795152e":"# create dummy variables for transaction_year\nstatic=pd.get_dummies(df['transaction_year'],prefix_sep='_',prefix='year')\nstatic","0697ffdc":"# merge the static into main dataframe\ndf=pd.concat([df,static],axis=1)","ce017e09":"# drop the transaction_year column and print top-5 records\ndf.drop('transaction_year',axis=1,inplace=True)\ndf.head()","4cab53f6":"df['number_of_convenience_stores'].value_counts()","a6a97392":"# split the data into independent and dependent variables\nX=df.drop('house_price',axis=1)\ny=df['house_price']","21c650eb":"X.head()","e9fa6e56":"y.head()","1f13292e":"# import minmaxscaler\nfrom sklearn.preprocessing import MinMaxScaler","90b8c8f7":"# initialize and fit the data\nscaler=MinMaxScaler()","51f52efb":"col=X.columns\nx=pd.DataFrame(scaler.fit_transform(X),columns=col)\nx.head()","be086acf":"X_df=x.copy()\ny_df=y.copy()\ndf_1=pd.concat([X_df,y_df],axis=1)\ndf_1.head()","a2910a2d":"# import train_test_split\nfrom sklearn.model_selection import train_test_split","82d24804":"#test train spit\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","2a218c7c":"print(\"shape of X_train:\",X_train.shape)\nprint(\"shape of X_test:\",X_test.shape)\nprint(\"shape of y_train:\",y_train.shape)\nprint(\"shape of y_test:\",y_test.shape)","1ca1606f":"# Importing statsmodels module as sm\nimport statsmodels.api as sm\n\n# Adding a constant column to our X_train dataframe\nX_train = sm.add_constant(X_train)\n\n# create a first fitted model\nmodel=sm.OLS(y_train.values.reshape(-1,1),X_train)\nlm_1 = model.fit()","9e306580":"#Let's see the summary of our first linear model\nprint(lm_1.summary())","328341bd":"# import variance_inflation_factor module\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# Define vif_scores function as stated above\ndef vif_score(X):\n    vif_data=pd.DataFrame()\n    vif_data['Variables']=X.columns\n    vif_data['VIF']=[variance_inflation_factor(X.values,i) for i in range(len(X.columns))]\n    return vif_data\n\n\n\n\n# print vif scores for all current input features\nprint(vif_score(x))","1b3c1125":"# drop the year_21012 and year_2013 columns\nx.drop(columns=['year_2012','year_2013'],axis=1,inplace=True)\nprint(vif_score(x))","fa7464f0":"#test train spit\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","932b6fef":"# Importing statsmodels module as sm\nimport statsmodels.api as sm\n\n# Adding a constant column to our X_train dataframe\nX_train = sm.add_constant(X_train)\n\n# create a first fitted model\nmodel=sm.OLS(y_train.values.reshape(-1,1),X_train)\nlm_2 = model.fit()","12875d39":"#Let's see the summary of our second linear model\nprint(lm_2.summary())","aec1c52c":"# drop  the longtitud and check if r2 values increase or not\nx.drop(columns=['longitude'],axis=1,inplace=True)\nprint(vif_score(x))","a203cbeb":"#test train spit\nX_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","e8e980ef":"# Importing statsmodels module as sm\nimport statsmodels.api as sm\n\n# Adding a constant column to our X_train dataframe\nX_train = sm.add_constant(X_train)\n\n# create a first fitted model\nmodel=sm.OLS(y_train.values.reshape(-1,1),X_train)\nlm_3 = model.fit()","05caf0f5":"#Let's see the summary of our third linear model\nprint(lm_3.summary())","364330c0":"# Adding a constant column to our dataframe\nX_test =  sm.add_constant(X_test)\n\n\n# Adding  constant variable to test dataframe\nX_test = sm.add_constant(X_test)\n\n# create a second fitted model\nlm_3 = sm.OLS(y_test.values.reshape(-1,1),X_test).fit()","75f2f048":"#print model 2 summary\nprint(lm_3.summary())","99a78b9d":"# Making predictions\ny_pred = lm_3.predict(X_test)","c45e28f0":"# Actual vs Predicted graph as below\nc = [i for i in range(1,121,1)]\nfig = plt.figure()\n#Plotting Actual\nplt.plot(c,y_test)\n#Plotting predicted\nplt.plot(c,y_pred,color='red')\n# Plot heading \nplt.title('Actual vs Predicted',fontsize=20)\n# X-label\nplt.xlabel('Index',fontsize=15)\n# Y-label\nplt.ylabel('House price',fontsize=15)\n#showing the plot\nplt.show()","f3f7ee93":"# Plotting y_test and y_pred scatter plot to understand the spread.\nfig = plt.figure()\n#plotting scatter plot between actual and predicted\nplt.scatter(y_test,y_pred)\n# Plot heading \nplt.title('y_test vs y_pred',fontsize=20)\n# X-label\nplt.xlabel('y_test',fontsize=15)\n# Y-label\nplt.ylabel('y_pred',fontsize=15)\n#show plot\nplt.show()","f015b3f6":"# Plotting line chart of Error terms\nfig = plt.figure()\nc = [i for i in range(1,121,1)]\n# line plot between c and error trem\nsns.lineplot(c,y_test.values-y_pred.values,color='blue')\n# Plot heading \nplt.title('error term',fontsize=20)\n# X-label\nplt.xlabel('index',fontsize=15)\n# Y-label\nplt.ylabel('y_test-y_pred',fontsize=15)\n#show plot\nplt.show()","be973787":"# Plotting the error terms as dist plot to understand the distribution.\nfig = plt.figure()\n#plot distplot of error tem\nsns.distplot(y_test.values-y_pred.values,bins=20)\n# Plot heading \nplt.title('error term',fontsize=20)\n# X-label\nplt.xlabel('y_test-y_pred',fontsize=15)\n\n# Y-label\"\nplt.ylabel('index',fontsize=15)\n\n#show plot\nplt.show()","4fa91879":"#import metrics module\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n#calculate and print RMSE\nmse=mean_squared_error(y_test,y_pred)\nprint(\"Root Mean Squared error(RMSE) is: \",np.sqrt(mse))","c6843641":"#calculate and print MSE\nprint(\"Mean squared Error(MSE) is: \",mse)","fce561da":"#calculate and print MAE\nprint(\"Mean Absolute error(MAE) is: \",mean_absolute_error(y_test,y_pred))","add49c11":"print(vif_score(x))","754c3885":"residual=y_test.values-y_pred.values\nsns.distplot(residual)","0b249fe5":"np.mean(residual)","a6718126":"fig,ax=plt.subplots(figsize=(6,2.5))\n_=ax.scatter(y_pred,residual)","0bb9fc45":"import statsmodels.tsa.api as smt\nacf=smt.graphics.plot_acf(residual,lags=40,alpha=0.05)\nacf.show()","6326cfdc":"### Homoscedasticity","8a2e190f":"### Multicollinearity","8b166fe4":"# Exploratory Data Analysis","1a53d1c1":"### Normality of residuals","4e5aa9e6":"# Feature Selection","f0c8f6f8":"# Feature Engineering","559128d9":"## Regression metrics","1a595f9d":"**here most of the features are not follow the normal distribution**","2cc190d5":"**Year_2012 and year_2013 columns highy correlated because vif score is high**","2388bec2":"# Data Preparation","11f207e8":"### No auto correlation of residual","10438b5a":"# Verify Assumption","7da44981":"**here little-bit of linear relationship between house price and other features**","4685446f":"# Model Evaluation"}}