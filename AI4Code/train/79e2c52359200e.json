{"cell_type":{"0421d16d":"code","5b515f49":"code","8fea9ff6":"code","9ff50890":"code","e4095f9b":"code","269d4eb3":"code","4e9007e3":"code","fddca86e":"code","8ecdf4ff":"code","b413c212":"code","4fa27a6f":"code","4b9082e3":"code","f1ddb919":"code","b5ce50e1":"code","00480279":"code","f8dd3838":"code","54cdf32a":"code","1dd81bbe":"code","1ec3cef3":"code","cacadd4f":"code","48db231b":"code","ca58373c":"code","9ac5877b":"code","6626a461":"code","1a9425d8":"code","7ac8c1e1":"code","3dde77de":"code","836bd9e7":"code","7c47871f":"code","79d810c0":"code","5bfe8168":"code","d9fc78be":"code","c7d343f3":"code","1bdaba48":"code","78865411":"code","b1d006e0":"code","2208bdac":"code","f8f4f933":"code","1965d00b":"code","bfa149a8":"code","4fc21ff4":"code","d8f8b704":"code","e564661e":"code","73f412eb":"code","f2d29182":"code","f2c0b1fc":"code","1f2fd7bb":"code","40816ae7":"code","09cd754f":"code","8e585cd4":"code","55f26366":"code","00b79b9a":"code","13b73d0e":"code","34470959":"code","86045dd5":"code","a08f7055":"code","ce374749":"code","014f72c8":"code","b08752a4":"code","f20b0fc1":"code","9d38059d":"code","f4f7c004":"code","7aaa2fae":"code","f458faac":"code","d50b2757":"markdown","f65edee3":"markdown","e0b5022c":"markdown","fec2d91c":"markdown","24e38e7f":"markdown","13ce7fe5":"markdown","a5edcdc3":"markdown","0fe56412":"markdown","2ef39ba7":"markdown","db5f3067":"markdown","ca694cfb":"markdown","bfe0e254":"markdown","6ee1c863":"markdown","ee1c1872":"markdown","10254951":"markdown","f34edaf9":"markdown","fb3c0dc8":"markdown","7cd7fbdd":"markdown","6b98124a":"markdown","8adfbd14":"markdown","28fe747f":"markdown"},"source":{"0421d16d":"# packages\nimport os, time\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\n\n# imbalanced dataset\nimport imblearn\nfrom imblearn.over_sampling import SMOTE \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFECV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import kneighbors_graph, KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, plot_confusion_matrix, classification_report, roc_curve, roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5b515f49":"# loading data\ndata_dir = \"..\/input\/breast-cancer-wisconsin-data\/data.csv\"\ndf  = pd.read_csv(data_dir)\ndf.head()","8fea9ff6":"df.isnull().values.any() # no missing values","9ff50890":"# Every feature variables are float numbers, ID - id of the case\/patient\ndf.diagnosis.value_counts()","e4095f9b":"sns.countplot(x=\"diagnosis\", data=df);","269d4eb3":"# Not very-unbalanced dataset - still their is disparity in the number of Malignant and Benign records\n# converting M and B into 1 and 0: LabelEncoding\ndf.diagnosis = df.diagnosis.map({\"M\": 1, \"B\": 0})","4e9007e3":"#df[\"Unnamed: 32\"].values\n# This seems an empty columns: drop it\n\n# Since we don't need ID for this analysis: not relating any results to actual ID for now\nfor col in [\"id\", \"Unnamed: 32\"]: \n    df = df.drop(columns=[col])\ndf.columns","fddca86e":"df.describe().T","8ecdf4ff":"# the features seem derived from few variables' mean, standard deviation and worst cases\n# Let us plot their distributions into three different groups\n\nmean_features = []\nfor f in df.columns:\n    if \"mean\" in f:\n        mean_features.append(f)\n# plotting mean variables\nplt.figure(figsize = (27, 36))\nfor i in range(10):\n    plt.subplot(10, 5, i+1)\n    sns.distplot(df[mean_features[i]], kde=True)\nplt.show()","b413c212":"se_features = []\nfor f in df.columns:\n    if \"se\" in f:\n        se_features.append(f)\n# plotting se variables\nplt.figure(figsize = (27, 36))\nfor i in range(10):\n    plt.subplot(10, 5, i+1)\n    sns.distplot(df[se_features[i]], kde=True)\nplt.show()","4fa27a6f":"worst_features = []\nfor f in df.columns:\n    if \"se\" in f:\n        worst_features.append(f)\n# plotting worst variables\nplt.figure(figsize = (27, 36))\nfor i in range(10):\n    plt.subplot(10, 5, i+1)\n    sns.distplot(df[worst_features[i]], kde=True)\nplt.show()","4b9082e3":"# Swarmplot: to see the variance\/distribution of each featuress: standardized value : z = (x-m)\/s\n\ndef swarmplot_features(findex_min, findex_max):\n    sns.set_theme() #(style=\"darkgrid\", palette=\"deep\")\n    sns.set_context(\"talk\")\n\n    y = df.diagnosis\n    data = df.drop(columns=[\"diagnosis\"])\n    data_std = (data - data.mean()) \/ (data.std())     # standardization\n    data = pd.concat([y, data_std.iloc[:, findex_min:findex_max]], axis=1)\n    data = pd.melt(data,id_vars=\"diagnosis\",\n                        var_name=\"features\",\n                        value_name='value')\n\n    plt.figure(figsize=(18,8))\n    sns.swarmplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data)\n    plt.xticks(rotation=90)\n    plt.show()","f1ddb919":"swarmplot_features(0, 15)","b5ce50e1":"swarmplot_features(15, 30)","00480279":"# correlation between the features:\n\ncorr = df.corr()\n# mask for upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nplt.figure(figsize=(12, 12))\nsns.heatmap(corr, mask=mask, vmin=-1.0, vmax=1.0, center=0, cmap=cmap,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});","f8dd3838":"# highly correlated features\n\nhighly_correlated_features = []\nfor i, idx in enumerate(corr.index):\n    for j, col in enumerate(corr.columns):\n        if j <= i: continue\n        if abs(corr.loc[idx, col]) > 0.90:\n            print((idx, col), \":--> \", round(corr.loc[idx, col], 3))\n            highly_correlated_features.append((idx, col))\nlen(highly_correlated_features)","54cdf32a":"plt.figure(figsize = (25, 80))\nfor i in range(21):\n    fs = highly_correlated_features[i]\n    plt.subplot(21, 7, i+1)\n    sns.scatterplot(x=fs[0], y=fs[1], hue=\"diagnosis\", data=df, legend=False)\nplt.show()","1dd81bbe":"# Radar plot: separate two classes based on few important variables\ndef radar_plot_class(df):\n    '''\n    In binary classification problem:\n        df: feature dataframe\n        edges: select few most important columns\/features --> plot radar chart to show separation between the two classes\n    '''\n    \n    target = \"diagnosis\"\n    if target not in df.columns:\n        raise ValueError(\"If prediction target variable is named different, name it target!\")\n    \n    corr = df.corr()[target].sort_values(ascending=False)\n\n    edges = list(corr.index)[:9]\n    \n    #edges = highly_correlated_features\n    \n    if target in edges:\n        edges.remove(target)\n    \n    # stadardization of features: for scaled radii as standard deviation of features\n    df_scaled = df[edges]\n    df_scaled = StandardScaler().fit_transform(df_scaled)\n\n    df_scaled = pd.DataFrame(df_scaled)\n    df_scaled.columns = edges\n    df_scaled[target] = list(df[target].values)\n\n    # radius of the chart\n    radii_0 = []\n    radii_1 = []\n    for edge in edges:\n        \n        value1 = df_scaled[df_scaled[target] == 1][edge].mean()\n        value0 = df_scaled[df_scaled[target] == 0][edge].mean()\n        \n        radii_1.append(value1)\n        radii_0.append(value0)\n            \n    edge_labels = [i.upper() for i in edges]\n\n    # plotting\n    fig = go.Figure()\n    fig.add_trace(go.Scatterpolar(\n        r=radii_0,\n        theta=edge_labels,\n        fill='toself',\n        name='Negative'\n    ))\n    fig.add_trace(go.Scatterpolar(\n        r=radii_1,\n        theta=edge_labels,\n        fill='toself',\n        name='Positive'\n    ))\n    fig.update_layout(\n    polar=dict(\n            radialaxis=dict(\n            visible=True,\n            range=[-1, 1.1]\n        )),\n    showlegend=True\n    )\n    fig.show()\n    \nradar_plot_class(df)","1ec3cef3":"y = df.diagnosis\nX = df.drop(columns=[\"diagnosis\"])\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","cacadd4f":"# Dimensionality reduction with PCA\n\npca = PCA(n_components=2, svd_solver='full')\n\npca.fit(X)\nprint(\"total variance by principle components: \", round(pca.explained_variance_ratio_.sum(), 5))\n\nX_red = pca.transform(X)\ntmp = pd.DataFrame(data={\"x1\": X_red[:, 0], \"x2\": X_red[:, 1], \"label\": y})\n\nsns.scatterplot(x=\"x1\", y=\"x2\", hue=\"label\", data=tmp);\nplt.title(\"Scatterplot of principle components\");","48db231b":"# total explained variance by the num of components\nnum_components = []\ntotal_variance = []\nfor n in range(31):\n    pca = PCA(n_components=n, svd_solver=\"full\", random_state=21)\n    pca.fit(X)\n    \n    variance = pca.explained_variance_ratio_.sum()\n    \n    num_components.append(n)\n    total_variance.append(variance)\n\nplt.scatter(num_components, total_variance)\nplt.plot(num_components, total_variance)\nplt.xlabel('# of components')\nplt.ylabel('Total explained variance')\nplt.ylim(0.6, 1.02)\nplt.show()","ca58373c":"pca.explained_variance_ratio_","9ac5877b":"!pip install babyplots\nfrom babyplots import Babyplot","6626a461":"#3D plot with babyplots: 3 PCA components\n\npca = PCA(n_components=3, svd_solver='full')\npca.fit(X)\nprint(\"total variance by principle components: \", round(pca.explained_variance_ratio_.sum(), 3))\n\nX_red = pca.transform(X)\ntmp = pd.DataFrame(data={\"x1\": X_red[:, 0], \"x2\": X_red[:, 1], \"x3\": X_red[:, 2], \"label\": y})\n\ncoords = tmp.iloc[:,0:3].values.tolist()\nclusts = tmp.label.values.tolist()\n\nbp = Babyplot(background_color=\"#000000ff\")\nbp.add_plot(coords, \"pointCloud\", \"categories\", clusts, {\"colorScale\": \"spring\", \"size\": 2, \"intensityMode\": \"gamma\"})\n\nbp","1a9425d8":"y_train.value_counts()","7ac8c1e1":"# Oversampling minority class using SMOTE: Synthetic Minority Oversampling Technique\noversample = SMOTE(sampling_strategy=0.8)\nX_train, y_train = oversample.fit_resample(X_train, y_train)","3dde77de":"y_train.value_counts()  # Both class are equal in number","836bd9e7":"# Baseline model: k-NN\n# This is highly separated data by class in many feature space, so kNN might already be a good model\n# correlated features - non-zero covariates --> feature sclaing is not effective so use Mahalanobis distance\n\nauc_list = []\nn_neighbors_list = []\n\nfor n in range(1, 16):\n    knn = KNeighborsClassifier(n_neighbors=n, weights=\"distance\", metric=\"minkowski\")\n    knn.fit(X_train, y_train)\n    auc_list.append(roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1]).round(3))\n    n_neighbors_list.append(n)\nplt.scatter(n_neighbors_list, auc_list)\nplt.plot(n_neighbors_list, auc_list)\nplt.ylim(0.6, 1.05)\nplt.title(\"k-NN Model\")\nplt.xlabel(\"n_neighbors\")\nplt.ylabel(\"AUC\")\nplt.show();","7c47871f":"knn = KNeighborsClassifier(n_neighbors=9, weights=\"uniform\", metric=\"minkowski\")\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test).ravel()\n\nprint(classification_report(y_test, y_pred))\nprint(\"AUC: \", roc_auc_score(y_test, knn.predict_proba(X_test)[:, 1]).round(3))\n\nconfusion_matrix(y_test, y_pred)","79d810c0":"# pipeline = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=21))  # base model with default parameters\n# As we are only going to use RF, decision tree-based model, we are not using feature stadardization: should be done for other methods like NN, linear models etc.\n\nmodel = RandomForestClassifier(random_state=21)\nmodel.fit(X_train, y_train)","5bfe8168":"y_proba = model.predict_proba(X_test)[:, 1]","d9fc78be":"# Area under curve\nroc_auc_score(y_test, y_proba)","c7d343f3":"cutoff = 0.5\ny_pred = (y_proba > cutoff).astype(int)","1bdaba48":"print(classification_report(y_test, y_pred))","78865411":"# Let us do some hyperparameter tuning to create the model: then we can do few more results analysis to understand the model\n# to find the list of the tunable parameters\nmodel.get_params()  ","b1d006e0":"# Feature selection: Recursive Feature Elimination and cross validation\n\nrf = RandomForestClassifier(random_state=21)\nrfecv = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')\n\nrfecv = rfecv.fit(X_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Top features :', X_train.columns[rfecv.support_])","2208bdac":"# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"CV score\")\nplt.scatter(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","f8f4f933":"# Let us work further only with those selected 10 features\nX_train = X_train[X_train.columns[rfecv.support_].tolist()]\nX_test = X_test[X_test.columns[rfecv.support_].tolist()]\n# y_train and y_test remain the same","1965d00b":"# Hyperparamters tuning with RF model: using selected features\n# parameter space to search the best hyperparameters\nrandom_grid = {  'criterion': [\"gini\", \"entropy\"],\n                 'bootstrap': [True, False],\n                 'max_depth': [3, 4, 5, 6, 7, 9, 11, 13, None],\n                 'max_features': ['auto', 'sqrt'],\n                 'min_samples_leaf': [1, 2, 4],\n                 'min_samples_split': [2, 5, 10],\n                 'n_estimators': [60, 80, 100, 120, 160, 200, 400, 800, 1000]\n              }","bfa149a8":"X.columns[rfecv.support_]","4fc21ff4":"%%time\n\nrf = RandomForestClassifier()\nclf = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=5, verbose=2, random_state=21, n_jobs=-1)\n\n# Fitting on the full dataset: for hyperparameters tuning\nclf.fit(X[X.columns[rfecv.support_].tolist()], y)","d8f8b704":"# best model hyperparameters: clf.best_params_\n# from saved params from hyperparameters tuning\n\nparams =   { 'n_estimators': 100,\n             'min_samples_split': 2,\n             'min_samples_leaf': 1,\n             'max_features': 'auto',\n             'max_depth': 6,\n             'criterion': 'gini',\n             'bootstrap': True\n           }\n\n# creating the best model\nmodel = RandomForestClassifier(**params) \nmodel.fit(X_train, y_train)","e564661e":"y_proba = model.predict_proba(X_test)[:, 1]","73f412eb":"tmp = pd.DataFrame(data={\"predicted_proba\": y_proba, \"true\": y_test})\nsns.histplot(x=\"predicted_proba\", hue=\"true\", data=tmp, bins=20, multiple=\"stack\", );\nplt.vlines(x=0.5, ymin=0, ymax=55, color='r', linestyle=\"--\");","f2d29182":"# Wrong predictions on following cases\nerr_tmp = pd.concat([tmp[(tmp.predicted_proba < 0.5) & (tmp.true == 1)], tmp[(tmp.predicted_proba >= 0.5) & (tmp.true == 0)]])\nerr_tmp","f2c0b1fc":"# wrong predictions on following instances: why is the predictions for these records incorrect?\nX_test[X_test.index.isin(err_tmp.index.tolist())]  ","1f2fd7bb":"cutoff = 0.45\ny_pred = (y_proba > cutoff).astype(int)","40816ae7":"print(classification_report(y_test, y_pred))","09cd754f":"# Feature importance: For tree based ensemble model, we can calculate the feature importance\n\ntmp = pd.DataFrame(data={\"feature\": X_train.columns, \"importance\": model.feature_importances_})\ntmp = tmp.sort_values(by=[\"importance\"], ascending=False)\nplt.figure(figsize=(8, 4))\nsns.barplot(x=\"feature\", y=\"importance\", data=tmp);\nplt.xticks(rotation=90);","8e585cd4":"# Helper function: plot the confusion matrix\ndef plot_confusion_matrix(y_true, y_pred):\n    # confusion matrix\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    data = conf_matrix.transpose()  \n    \n    _, ax = plt.subplots()\n    ax.matshow(data, cmap=\"Blues\")\n    # printing exact numbers\n    for (i, j), z in np.ndenumerate(data):\n        if i == j:\n            ax.text(j, i, '{}'.format(z), ha='center', va='center', c=\"white\")\n        else:\n            ax.text(j, i, '{}'.format(z), ha='center', va='center')\n    # axis formatting \n    plt.xticks([])\n    plt.yticks([])\n    plt.title(\"True label\\n 0  {}     1\\n\".format(\" \"*18), fontsize=14)\n    plt.ylabel(\"Predicted label\\n 1   {}     0\".format(\" \"*18), fontsize=14)\n    \nplot_confusion_matrix(y_test, y_pred);","55f26366":"# helper function: ROC curve\ndef draw_roc_curve(y_true, y_proba):\n    '''\n       Plot and return ROC curve with appropriate labels and legend \n    \n    '''\n    fpr, tpr, _ = roc_curve(y_true, y_proba)\n    _, ax = plt.subplots()\n    \n    ax.plot(fpr, tpr, color='r');\n    ax.plot([0, 1], [0, 1], color='y', linestyle='--')\n    ax.fill_between(fpr, tpr, label=f\"AUC: {round(roc_auc_score(y_true, y_proba), 3)}\")\n    ax.set_aspect(0.90)\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_xlim(-0.02, 1.02);\n    ax.set_ylim(-0.02, 1.02);\n    plt.legend()\n    plt.show()\n\n# plotting roc curve\ndraw_roc_curve(y_test, y_proba)","00b79b9a":"# SHAP values: Even better tool to understand the feature contributions in ensemble models:\nimport shap","13b73d0e":"explainer = shap.Explainer(model, X_train)\nshap.initjs()","34470959":"# Force plot\ntest_instance = X_test.values[0]\n\nshap_values = explainer.shap_values(test_instance)\nshap.force_plot(explainer.expected_value[1], shap_values[1], test_instance)","86045dd5":"# Force plot\ntest_instance = X_test.values[6]\nshap_values = explainer.shap_values(test_instance)\nshap.force_plot(explainer.expected_value[1], shap_values[1], test_instance)","a08f7055":"shap.summary_plot(explainer.shap_values(X_test, check_additivity=False), X_test, plot_type=\"bar\", plot_size=(8, 6))","ce374749":"import tensorflow\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import CSVLogger, EarlyStopping\nfrom tqdm.keras import TqdmCallback\n\n# model\nmodel = tensorflow.keras.Sequential(\n                  [\n                    tensorflow.keras.layers.Dense(units=16, activation=\"relu\", name=\"Input\", input_dim=30),\n                    tensorflow.keras.layers.Dropout(rate=0.1),\n                    tensorflow.keras.layers.Dense(units=16, activation=\"relu\", name=\"Hidden\"),\n                    tensorflow.keras.layers.Dropout(rate=0.1),\n                    tensorflow.keras.layers.Dense(units=1, activation=\"sigmoid\", name=\"Output\"),\n                  ]\n              )","014f72c8":"model.summary()","b08752a4":"# visualaize the keras model\nplot_model(model, show_shapes=True, show_layer_names=True, rankdir=\"VR\", expand_nested=True)","f20b0fc1":"# compiling the model\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])","9d38059d":"y = df.diagnosis\nX = df.drop(columns=[\"diagnosis\"])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n\n# training\ncallbacks = [TqdmCallback(verbose=0), CSVLogger(\"..\/working\/results.csv\")]\nmodel.fit(X, y, validation_split=0.2, batch_size=128, epochs=350, verbose=0, callbacks=callbacks);","f4f7c004":"results = pd.read_csv(\"..\/working\/results.csv\")\nresults.tail()","7aaa2fae":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(8, 6))\nplt.plot(results.epoch.values, results.accuracy.values, label=\"training\");\nplt.plot(results.epoch.values, results.val_accuracy.values, label=\"validation\");\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend();","f458faac":"sns.set_style(\"whitegrid\")\nplt.figure(figsize=(8, 6))\nplt.plot(results.epoch.values, results.loss.values, label=\"training\");\nplt.plot(results.epoch.values, results.val_loss.values, label=\"validation\");\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend();","d50b2757":"### Best Model","f65edee3":"Many of the features are highly correlated to each other. If we want to fit a linear model like Logistic Regression, we must to do careful features selection to remove the strong multi-colinearity in the data. ","e0b5022c":"### **Data**\n\n> - ```data.csv``` -  full data set, we don't have separate train and test dataset in this problem. We will separate the test data from it later.","fec2d91c":"- We can see that: two classes are well separated based on the probability of confidence on the prediction. However, there are few cases where the model is predicting incorrect results. Since this dataset is small, we can't confirm the benefits of tuning the cutoff probability for dichotomous binary prediction. However, this can be used to fine tune the cutoff and acheive the higher value of preferred metric depending upon the problem.","24e38e7f":"### **<span style=\"color:#e76f51;\">Hyperparameters Tuning<\/span>**\n- RandomizedSearchCV is just a cheaper version of GridSearchCV that possibly covers the optimal range of parameter space with less number of runs. However, this method is entirely random and doesn't guarentee the optimized set of hyperparameters - as it doesn't remember anything from the previous run. \n\n- Best approach is to use Bayesian hyperparameters optimization approaches. These algorithms keep track of past evaluation results which they use to form a probabilistic model mapping hyperparameters to a probability of a score on the objective function. As these methods choose the next set of hyperparameters in the informed manner, they quickly reaches the best set of parameters. Examples: **<span style=\"color:#e76f51;\">HYPEROPT<\/span>**, **<span style=\"color:#e76f51;\">OPTUNA<\/span>** packages etc. ","13ce7fe5":"\ud83c\udfaf We can already see a pretty good separation of the benign and malignant groups in the 2D-plane.","a5edcdc3":"# **<span style=\"color:#e76f51;\">Model Interpretation<\/span>**\n\n> Feature importance analysis of the best model with Shapley value","0fe56412":"### Model Performance Evaluation","2ef39ba7":"### Random Forest Algorithm","db5f3067":"- Minority class data oversampling with SMOTE","ca694cfb":"\ud83d\udccc This 3D plot with Babyplot looks cool. Feel free to interact with the plot and see the even better separation from different angle.","bfe0e254":"## **<span style=\"color:#e76f51;\">EDA<\/span>**\n\nLet us investigate the data in order to understand its basic properties.","6ee1c863":"# **<span style=\"color:#e76f51;\">Dimensionality Reduction<\/span>**\n\nAs there are strongly correlated features in the dataset, let us reduce the dimension of the dataset into 2 and see the clustering of two target groups in a simple two dimensional plane\n\n### **<span style=\"color:#e76f51;\">PCA<\/span>**","ee1c1872":"# **<span style=\"color:#e76f51;\">Prediction Modelling<\/span>**\n\n### K-Nearest Neighbors Algorithm","10254951":"# **<span style=\"color:#e76f51;\">Dense Neural Network<\/span>**\n> Building a Neural Network (MLP) in Keras","f34edaf9":"# **<span style=\"color:#e76f51;\">Breast Cancer Diagnosis (Wisconsin Data Set)<\/span>**\n\n![Breast Cancer](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/180\/384\/3da2510581f9d3b902307ff8d06fe327\/dataset-cover.jpg)\n\n## **<span style=\"color:#e76f51;\">Problem<\/span>**\nThe goal of this problem is creat a classification model to identify benign vs malignant breast cancer from the features that are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image in the 3-dimensional space.\n\n\n- Let us start with importing some of the packages required for our analysis and modelling.","fb3c0dc8":" Let us further see how much better separation among the target groups we can see in the three dimension.\n \n ### **<span style=\"color:#e76f51;\">Visualization in 3D<\/span>**\n Plotting 3 PCA components in 3-dimension with Babyplot (interactive).","7cd7fbdd":"- We see strong correlation among the features as well as that of features with the target variable - which is a categorical variable. \n- We understand the correlation between the continuous and categorical (diagnois) varibles by: point-bisereal correlation, logistic regression etc. ","6b98124a":"- We can see that these highly correlated features are also already very important features: that can separate the records into the two target classes","8adfbd14":"### Feature selection: \n\n- Recursive Feature Elimination Technique","28fe747f":"### **<span style=\"color:#e76f51;\">Done!<\/span>**\n\n> \ud83d\udccc Please consider upvoting if you find it useful!\n\n> Thanks!"}}