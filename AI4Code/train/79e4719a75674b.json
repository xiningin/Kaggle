{"cell_type":{"d5d876c1":"code","564388cd":"code","3b8daf0d":"code","31b30ce5":"code","5163c7e1":"code","ac233d0b":"code","1db1a677":"code","9d41093a":"code","f120dbae":"code","39f28025":"code","ab7ea994":"code","4ff50ffe":"code","fb33fec2":"code","b1c7b287":"code","2b13b447":"code","fb8c03ec":"code","51334159":"code","78310df6":"code","23025d56":"code","f27972d4":"code","ae378795":"code","2054cbda":"code","da44c2a0":"code","cc74b001":"code","6c3e12cb":"code","9be1698d":"code","a3c4f607":"code","693a1386":"code","cdb3cc3c":"code","3c693922":"code","7a18b561":"code","8be693a6":"code","912d1772":"code","8c171c29":"code","c9620c73":"code","c4353f22":"code","cd201129":"code","2f43c92f":"code","652a00a8":"code","eddb603b":"code","55cb5430":"code","3352b047":"code","3607b4af":"code","5d1c0523":"code","cada4081":"code","79e9aa0c":"code","0820f713":"code","fe0825a2":"code","7e71aa7c":"code","885666b9":"code","91a54643":"code","9a25adbe":"markdown","3b3c7150":"markdown","b4e3bca4":"markdown","3b584b70":"markdown","5daee797":"markdown","1a784744":"markdown","7c026dc2":"markdown","b5246251":"markdown","c73f9c2a":"markdown","7acded61":"markdown","ac84873c":"markdown","7d8cbe76":"markdown","3e7e9607":"markdown","f9becc39":"markdown","d547b55c":"markdown","cb2a9902":"markdown","2b358d0a":"markdown","549063b9":"markdown","40344887":"markdown","427456f7":"markdown","7deb9be6":"markdown","a29c9816":"markdown","3200e69a":"markdown","7d14016e":"markdown","5e744519":"markdown","f9df81cd":"markdown","200921eb":"markdown","8b13e2dc":"markdown","f65a643f":"markdown","cb805a87":"markdown","0f5b1c93":"markdown","e5f87d88":"markdown","140590de":"markdown","c05875a1":"markdown","8c879899":"markdown"},"source":{"d5d876c1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\nimport missingno as msno\n%matplotlib inline  \n\nfrom scipy import stats\nfrom sklearn import preprocessing\nfrom sklearn import feature_selection\nimport warnings\nwarnings.filterwarnings('ignore')\nSEED = 42\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor","564388cd":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:1459], all_data.loc[1460:]\n\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ny_train = df_train.SalePrice\nid_val = df_train.Id\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_all = concat_df(df_train, df_test).drop(['SalePrice', 'Id'], axis=1)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint(f'Number of Training Examples = {df_train.shape[0]}')\nprint(f'Number of Test Examples = {df_test.shape[0]}\\n')\nprint(f'Training X Shape = {df_train.shape}')\nprint(f'Training y Shape = {df_train[\"SalePrice\"].shape[0]}\\n')\nprint(f'Test X Shape = {df_test.shape}')\nprint(f'Test y Shape = {df_test.shape[0]}\\n')\nprint(df_train.columns)","3b8daf0d":"def score_dataset(X, y, model=XGBRegressor(random_state=SEED)):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n","31b30ce5":"null_features = df_all.isnull().sum()\n\n# For features having smaller than 100 missing values\nnull_100 = df_all.columns[list((null_features < 100) & (null_features != 0))]\nnum = df_all[null_100].select_dtypes(include=np.number).columns\nnon_num = df_all[null_100].select_dtypes(include='object').columns\n# Numerous features\ndf_all[num] = df_all[num].apply(lambda x: x.fillna(x.median()))\n# Object features\ndf_all[non_num] = df_all[non_num].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\n# For features having larger than 1000 missing values\nnull_1000 = df_all.columns[list(null_features > 1000)]\ndf_all.drop(null_1000, axis=1, inplace=True)\ndf_all.drop(['GarageYrBlt', 'LotFrontage'], axis=1, inplace=True)","5163c7e1":"# For other features having missing values\n# GarageCond\ndf_all['GarageCond'] = df_all['GarageCond'].fillna('Null')\n# GarageFinish\ndf_all['GarageFinish'] = df_all['GarageFinish'].fillna('Null')\n# GarageQual\ndf_all['GarageQual'] = df_all['GarageQual'].fillna('Null')\n# GarageType\ndf_all['GarageType'] = df_all['GarageType'].fillna('Null')","ac233d0b":"\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\ndf_train = pd.concat([df_train, y_train], axis=1)\n\n# Checking existing missing value or not\nprint('If the result is zero means not exist any missing values in dataset')\nprint(df_all.isnull().any().sum())","1db1a677":"qualFeatures = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond']\n\n# Ex\tExcellent (100+ inches)\t\n# Gd\tGood (90-99 inches)\n# TA\tTypical (80-89 inches)\n# Fa\tFair (70-79 inches)\n# Po\tPoor (<70 inches\n# NA\tNo Basement\n\nQualityMapping = { 'Ex': 5, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0, 'NA': 0, 'Null': 0 }\n\nfor feat in qualFeatures:\n    df_all[feat] = df_all[feat].map(QualityMapping)\n\n","9d41093a":"def make_categorical ( df, feature ):\n    df[feature] = pd.Categorical(df[feature])\n    df[feature] = df[feature].cat.codes\n    \nfor col in df_all.select_dtypes(include=['object', 'category']).columns:\n    make_categorical ( df_all, col )","f120dbae":"# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","39f28025":"# Clean Year Sold in the test set few yrsold are older that year built and RemodAdd\n\ndef clean_year_sold ( row ):\n    if ( row.YearBuilt > row.YrSold ):\n        row.YrSold = row.YearBuilt\n    if ( row.YearRemodAdd > row.YrSold ):\n        row.YrSold = row.YearRemodAdd\n    return row\n\ndf_all = df_all.apply( clean_year_sold, axis=1 )\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","ab7ea994":"# EC added Feature\n\ndf_all['AgeWhenSold'] =  df_all['YrSold'] - df_all['YearBuilt']\n# No Improvments\n# df_all['AgeOfRemodWhenSold'] =  df_all['YrSold'] - df_all['YearRemodAdd']\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","4ff50ffe":"# Generating new features\n# Total square foot\ndf_all['TotalSF'] = df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']\n\n# Total number of bathroom\ndf_all['TotalBath'] = ( df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                        df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\n\ndf_all['TotalBsmtbath'] = df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath'])\n\n# Total square feet of porch in a house\ndf_all['TotalPorchSF'] = ( df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n                           df_all['EnclosedPorch'] + df_all['ScreenPorch'] + df_all['WoodDeckSF'])\n\n# Check the exist of each infrastructure (Ex: basement, bath,...) in a house\n\ndf_all['IsRemodel']    = df_all[['YearBuilt', 'YearRemodAdd']].apply(lambda x: 1 if x[0] != x[1] else 0, axis=1)\ndf_all['HasPool']      = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['Has2ndFloor']  = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasGarage']    = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasBsmt']      = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['HasFireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","fb33fec2":"# Ratio\n\ndf_all[\"LivLotRatio\"]    = df_all[\"GrLivArea\"] \/ df_all[\"LotArea\"]\ndf_all[\"Spaciousness\"]   = ( df_all[\"1stFlrSF\"] + df_all[\"2ndFlrSF\"] ) \/ df_all[\"TotRmsAbvGrd\"]\n\n# Allready in TotalPorchSF\n# df_all[\"TotalOutsideSF\"] = df_all[['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].sum(axis=1)\n\n# BldgType multiplied by GrLivArea\n\nX_2 = pd.get_dummies(df_all['BldgType'], prefix=\"Bldg\")\nX_2 = X_2.mul(df_all['GrLivArea'], axis=0)\ndf_all = pd.concat ( [X_2, df_all], axis=1 )\n\n# Porch Type\ndf_all[\"PorchTypes\"] = df_all[['WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']].gt(0).sum(axis=1)\n\n# Too long todo - need to convert cat code to numbers...\n\n# X_4 = pd.DataFrame()\n# X_4['MSClass'] = df_all['MSSubClass'].str.split(\"_\")\n# df_all['MSSubClass']\n\ndf_all[\"MedNhbdArea\"] = df_all.groupby('Neighborhood')['GrLivArea'].transform('median')\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","b1c7b287":"# Quality Sum\n\nqualFeatures = qualFeatures + ['OverallQual', 'OverallCond']\ndf_all[\"QualitySum\"] = df_all[qualFeatures].sum(axis=1)\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","2b13b447":"\n# Square feet price per neighboorhood\n\ny_SqFeetPrice = y_train \/ df_train['TotalSF']\n\n# Create the average AvgSqFeetPrice\n\nencoder = MEstimateEncoder(cols=['Neighborhood'], m=5.0)\n\nencoder.fit(df_train, y_SqFeetPrice)\n\n# Encode the training split\ndf_all = encoder.transform(df_all)\n\n# df_train[\"AvgSqFeetPrice\"] = df_train.groupby(\"Neighborhood\")[\"SqFeetPrice\"].transform(\"mean\")\n# df_all = df_all.merge( df_train[[\"Neighborhood\", \"AvgSqFeetPrice\"]].drop_duplicates(),\n#              on=\"Neighborhood\", how=\"left\" )\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","fb8c03ec":"from category_encoders import MEstimateEncoder\n\ndf_train, df_test = divide_df(df_all)\n\nencoder = MEstimateEncoder(cols=[\"Neighborhood\", \"SaleType\", \"MSSubClass\", \"QualitySum\"], m=5.0)\n\nencoder.fit(df_train, y_train)\n\n# Encode the training split\ndf_all = encoder.transform(df_all)\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","51334159":"# cluster_features = [ \"BldgType\", \"HouseStyle\", \"RoofStyle\", \"LandSlope\", 'Exterior1st' ]\ncluster_features = [ \"Heating\", \"HeatingQC\", \"CentralAir\", \"Electrical\" ]\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=SEED)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=SEED)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd\n\ndf_all['Cluster'] = cluster_labels (df_all, cluster_features, 5)\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","78310df6":"Xy = df_all.copy()\nXy[\"Cluster\"] = Xy.Cluster.astype(\"category\")\nXy[\"SalePrice\"] = y_train\nsns.relplot(\n    x=\"value\", y=\"SalePrice\", hue=\"Cluster\", col=\"variable\",\n    height=4, aspect=1, facet_kws={'sharex': False}, col_wrap=3,\n    data=Xy.melt(\n        value_vars=cluster_features, id_vars=[\"SalePrice\", \"Cluster\"],\n    ),\n);","23025d56":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","f27972d4":"low_impact_features = ['Electrical', 'PavedDrive', 'ExterCond', 'BldgType', 'TotalBsmtbath',\n       'BsmtCond', 'HasBsmt', 'Bldg_3.0', 'LotConfig', 'Bldg_2.0', 'Heating',\n       'RoofStyle', 'LandContour', 'IsRemodel', 'Bldg_1.0', 'Has2ndFloor',\n       'Functional', 'LowQualFinSF', 'KitchenAbvGr', 'Condition1', 'LandSlope',\n       'RoofMatl', 'PoolArea', 'HasPool', 'YrSold', 'Utilities', 'MiscVal',\n       'Condition2', 'BsmtFinType2', 'Street', 'MoSold']\n\nX = df_all.copy()\nX = X.loc[:, low_impact_features]\n\n# `apply_pca`, defined above, reproduces the code from the tutorial\npca, X_pca, loadings = apply_pca(X)\nplot_variance(pca)","ae378795":"X_pca.head()","2054cbda":"# Keep the first components to replace the low impact feature\n# Decrease the score\n\n# df_all = pd.concat ( [df_all, X_pca.iloc[:,0:8]], axis=1 )\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","da44c2a0":"# Drop all the recipe features\nremove_cols = ['BsmtFinSF1', 'BsmtFinSF2', 'FullBath', 'HalfBath', 'BsmtFullBath',\n              'BsmtHalfBath', 'OpenPorchSF', '3SsnPorch', 'EnclosedPorch', 'ScreenPorch', 'WoodDeckSF']\n\ndf_all.drop(remove_cols, axis=1, inplace=True)\n\n# Update training & testing data\ndf_train, df_test = divide_df(df_all)\nscore_dataset ( df_train, y_train )","cc74b001":"y_train = np.log1p(y_train)","6c3e12cb":"df_train, df_test = divide_df(df_all)","9be1698d":"from sklearn.feature_selection import mutual_info_regression\n\n# Utility functions from Tutorial\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","a3c4f607":"mi_scores = make_mi_scores ( df_train, y_train )\n\n# print ( mi_scores.head ( 42 ) )\n\nplt.figure(dpi=100, figsize=(16, 10))\nplot_mi_scores(mi_scores [ mi_scores.values > 0.05 ])","693a1386":"chosen_cols = mi_scores [ mi_scores.values > 0.1 ].index\nchosen_cols","cdb3cc3c":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n\nrf = RandomForestClassifier(n_jobs=-1, random_state=42, oob_score=True )\n# %prun \nrf.fit(df_train, y_train.astype('int'))\n\nimportances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\nforest_importances = pd.DataFrame ( importances, index=df_train.columns )\n\nforest_importances = forest_importances.sort_values (forest_importances.columns[0],ascending=False)\nforest_importances.head(42).plot.bar(figsize=(16,10))","3c693922":"forest_importances","7a18b561":"# chosen_cols = forest_importances [ forest_importances.values > 0.01 ].index","8be693a6":"# \"Electrical\" features\ndf_train['Electrical'].loc[df_train['Electrical']=='Mix'] = 'SBrkr'\n\n# \"Exterior2nd\" features\ndf_train['Exterior2nd'].loc[df_train['Exterior2nd']=='Other'] = 'VinylSd'\n\n# \"Heating\" features\ndf_train['Heating'].loc[df_train['Heating']=='OthW'] = 'GasA'\ndf_train['Heating'].loc[df_train['Heating']=='Floor'] = 'GasA'\n\n# \"HouseStyle\" features\ndf_train['HouseStyle'].loc[df_train['HouseStyle']=='2.5Fin'] = '1.5Fin'\n\n# \"MSSubClass\" features\ndf_test['MSSubClass'].loc[df_test['MSSubClass']=='150'] = '160'\n\n# \"Condition2\" feature\ntemp = [True if ((val=='RRNn') | (val=='RRAn') | (val=='RRAe')) else False\n        for val in df_train['Condition2']]\ndf_train['Condition2'].loc[temp] = 'Norm'\n\n# \"Utilities\" is a constant-value feature --> Delete it\n# \"RoofMatl\" has high number of different values --> Delete it\ncate_drop = ['Utilities', 'RoofMatl']\ndf_train.drop(cate_drop, axis=1, inplace=True)\ndf_test.drop(cate_drop, axis=1, inplace=True)\n","912d1772":"df_train = df_train[chosen_cols]\ndf_test = df_test[chosen_cols]\ndf_all = concat_df(df_train, df_test)","8c171c29":"chosen_cols","c9620c73":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnum_features = df_all.select_dtypes(include=numerics).columns","c4353f22":"# Normalize skewness feature using Log function\nskew_features = df_all[num_features].apply(lambda x: stats.skew(x)).sort_values(ascending=False)\nskew_features = skew_features[abs(skew_features) > 0.75]\nprint(skew_features)           \n\n# Apply Box cox for skewness > 0.75\nfor feat in skew_features.index:\n    df_all[feat] = np.log1p(df_all[feat])","cd201129":"df_train, df_test = divide_df(df_all)","2f43c92f":"print(df_train.shape, df_test.shape)","652a00a8":"df_train.head()","eddb603b":"from sklearn.model_selection import KFold # for repeated K-fold cross validation\nfrom sklearn.model_selection import cross_val_score # score evaluation\nfrom sklearn.model_selection import cross_val_predict # prediction\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.ensemble import StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport time\nSEED = 42","55cb5430":"# Repeated K-fold cross validation\nkfolds = KFold(n_splits=10, shuffle=True, random_state=SEED)\n\n# Return root mean square error of model prediction (Used for test prediction)\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n# Return root mean square error applied cross validation (Used for training prediction)\ndef evaluate_model_cv(model, X, y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","3352b047":"def construct_models():\n    # Initialize parameters for models\n    alphas_ridge = [0.005, 0.01, 0.1, 1, 5, 10, 15]\n    alphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n    e_alphas_elas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n    e_l1ratio_elas = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n    \n    # Constructing the models\n    models = dict()\n    \n    models['ridge'] = RidgeCV(alphas=alphas_ridge, cv=kfolds)\n    \n    models['lasso'] = LassoCV(alphas=alphas_lasso, random_state=SEED, cv=kfolds)\n    \n    models['elasticnet'] = ElasticNetCV(alphas=e_alphas_elas, cv=kfolds, l1_ratio=e_l1ratio_elas)\n    \n    models['svr'] = SVR(C = 20, epsilon = 0.008, gamma =0.0003)\n    \n    models['gbr'] = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, \n                                              max_depth=4, max_features='sqrt',\n                                              min_samples_leaf=15, min_samples_split=10, \n                                              loss='huber',random_state =SEED) \n    \n    models['lgbm'] = LGBMRegressor(objective='regression', num_leaves=4,\n                                   learning_rate=0.01, n_estimators=5000,\n                                   max_bin=200, bagging_fraction=0.75,\n                                   bagging_freq=5, bagging_seed=7,\n                                   feature_fraction=0.2,\n                                   feature_fraction_seed=7, verbose=-1,\n                                  colsample_bytree=None, subsample=None, subsample_freq=None)\n    \n    models['xgboost'] = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7, verbosity = 0,\n                                     objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1, seed=SEED, reg_alpha=0.00006)\n    return models\n\n# Construct the set of model\nmodels = construct_models()","3607b4af":"from sklearn.model_selection import cross_validate\n\nfor name, model in models.items():\n    # Start counting time\n    start = time.perf_counter()\n    \n    model = model.fit(np.array(df_train), np.array(y_train))\n    \n    rmse_result = rmse ( y_train, model.predict ( np.array(df_train ) ) )\n    print(f'{name}\\'s rmse after training: {rmse_result}')\n    \n    # Compute time for executing each algo\n    run = time.perf_counter() - start\n    print(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')","5d1c0523":"'''\ncv_rmse_result = dict()\ncv_rmse_mean = dict()\ncv_rmse_std = dict()\n\nfor name, model in models.items():\n    # Start counting time\n    start = time.perf_counter()\n    \n    cv_rmse_result[name] = evaluate_model_cv(model, np.array(df_train), np.array(y_train))\n    cv_rmse_mean[name] = cv_rmse_result[name].mean()\n    cv_rmse_std[name] = cv_rmse_result[name].std()\n    print(f'Finish {name}\\'s model')\n    \n    # Compute time for executing each algo\n    run = time.perf_counter() - start\n    print(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')\n'''","cada4081":"'''\nML_cv = pd.DataFrame({'cv_rsme_mean' : cv_rmse_mean, 'cv_rmse_std' : cv_rmse_std})\nML_cv\n'''","79e9aa0c":"# Type 1 stacking model\nstack_model = StackingCVRegressor(regressors=(models['ridge'], models['lasso'], models['xgboost'],\n                                              models['elasticnet'], models['gbr'], models['lgbm']),\n                                  meta_regressor=models['xgboost'], use_features_in_secondary=True)","0820f713":"# Time performance counter\nstart = time.perf_counter()\n\nstack_model = stack_model.fit(np.array(df_train), np.array(y_train))\nprint('Finish training')\n\n# Compute rmse with cross-validation technique\n# rmse_stack_cv = evaluate_model_cv(stack_model, np.array(df_train), y_train)\n# print(f'stack_model\\'s rmse (using cv) after training: {rmse_stack_cv.mean()}')\n\n# Compute rmse without cross-validation technique\nrmse_stack = rmse(y_train, stack_model.predict(np.array(df_train)))\nprint(f'stack_model\\'s rmse (using cv) after training: {rmse_stack}')\n\n# Compute time for executing each algo\nrun = time.perf_counter() - start\nprint(f'Computational runtime of this algo: {round(run, 2)} seconds\\n')","fe0825a2":"# Get the id feature from testing dataset\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_id = test['Id']","7e71aa7c":"df_test['MSSubClass'] = df_test['MSSubClass'].astype ( 'float32' )\ndf_test['Exterior2nd'] = df_test['Exterior2nd'].astype ( 'int32' )","885666b9":"# Submission set\nsubmit = pd.concat((test_id, pd.Series(np.exp(stack_model.predict(df_test)), \n                                       name='SalePrice')), axis=1)\nsubmit.to_csv('submission.csv', index=False)","91a54643":"print (\"That's All Folks !\")","9a25adbe":"<a name='2.3.0.0'><\/a>\n### 2.3.0.0 Mutual Information","3b3c7150":"## k-Means Clustering on comfort features","b4e3bca4":"<a name='2.2'><\/a>\n## 2.2 Construct new useful features\n- There are some features that we can concatunate them together to get more useful features\n- After constructing new features, all the recipe features might be removed b.c these features and the new one both represent the same type of infomation. Therefore, they would not be more effective to be together than to be alone","3b584b70":"EC : \n* Initial Score => 0.12076\n* if yearbin is removed => 0.12102\n* With df_all['AgeWhenSold'] =>  df_all['YrSold'] - df_all['YearBuilt'] => 0.12065\n* Mutual information => 0.12739\n* Feature infformation => 0.13340","5daee797":"## Squarre feet price feature","1a784744":"<a name='2.3'><\/a>\n## 2.3 Feature selection","7c026dc2":"Now the number of features in training and testing dataset are the same & all the preprocessing steps are finished. The data is ready for training!!","b5246251":"Great!! All the mismatched values are fixed. Now let's move to selecting the continuous features","c73f9c2a":"<a name='2.4'><\/a>\n## 2.4 Features transformation","7acded61":"Next we'll visualize the year features after applying the binning technique","ac84873c":"## Features from exercice","7d8cbe76":"### Normalize Dependant variable","3e7e9607":"<a name='2.3.0.1'><\/a>\n### 2.3.0.1 Feature Importance","f9becc39":"## Addind Age feature","d547b55c":"<a name='3'><\/a>\n# 3. Modeling","cb2a9902":"- For dealing with mismatched values, I'll replace them by the values having the highest frequency in each feature. \n- With some features having high number of different values, I think removing them might be a good choice","2b358d0a":"### PCA on low impact feature","549063b9":"<a name='2.4.1'><\/a>\n### 2.4.1 Highly skewed numeric features\n- Highly skewed numeric features are the heavy-tail features like our target features\n- We decide whether a feature is skewness or not based on the value of \"skewness\" statistics measurement\n- All skewed features will be normalize by Box-cox normalization technique","40344887":"And then transform some numeric features that are actually the categorical feature","427456f7":"<a name='2.1'><\/a>\n## 2.1 Feature enginnering on Year\n- Using \"Bin\" technique to all features representing \"year\" value (Ex: 2000, 1999,...)\n- And after that encoding them into continuous features","7deb9be6":"## MEstimate from Exercice","a29c9816":"<a name='2.3.2'><\/a>\n### 2.3.2 Mismatched value between train & test set in categorical features\n- In some case, some columns in train dataset contained values which do not exist in testing dataset, we called them **missmatched data** between train and test\n- This would be an serious problem if we plan to perform one-hot encoding for categorical features in the future because of the different number of features between train and test data\n- Below are an example of missmatched data in \"Electrical\" feature. The value \"mix\" exist in training set but not in testing set","3200e69a":"- Target feature is a heavy-tailed distribution --> So Box-Cox transformation should be useful to bring the target from heavy-tailed to normal distribution","7d14016e":"<a name='3.2'><\/a>\n## 3.2 Stacking model\n- In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than the learning algorithm alone. **Stacking model** is an ensemble one\n- It uses a meta-learning algorithm to learn how to best combine the predictions from two or more base machine learning algorithms.\n\nReference link: [Stacking model](https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/)","5e744519":"After design and construct the based model, we need to fit the training data to the model and compute the root mean square error (rmse) result to validate models after the training\n- **Note that:** numpy array is recommended as an input to the training model instead of Dataframe. Because numpy has a hugh benefit relating to the time consuming than pandas. Link for more infomation [Here](https:\/\/towardsdatascience.com\/speed-testing-pandas-vs-numpy-ffbf80070ee7)","f9df81cd":"# Feature Selection Testing\n\nThis notebook was created starting with the work of https:\/\/www.kaggle.com\/hoangphamviet\n* https:\/\/www.kaggle.com\/hoangphamviet\/house-price-stacked-regression-top-3\n\nThe notebook was deeply cleaned to help feature analysis.","200921eb":"- For features having missing value smaller than 100 -> I'll fill numeric features with the corresponding median & categorical features with the corresponding most frequent values\n- For features having missing value larger than 1000 -> Removing these features might be a good choice\n- For other features having missing value -> I'll fill them with \"Null\" value (b.c all the other features are object features)","8b13e2dc":"<a name='1'><\/a>\n# 1. Load the data\n","f65a643f":"<a name='3.1'><\/a>\n## 3.1 Base models\nWe'll plan to construct these below based model\n- **Ridge** is regression model applying l2 regularization technique. ([Link here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html))\n- **Lasso** stands for Least Absolute Shrinkage and Selection Operator that is a linear regression model applied l1 regularization technique ([Link here](https:\/\/www.statisticshowto.com\/lasso-regression\/))\n- **elasticnet** is a penalized linear regression model that includes both the L1 and L2 penalties during training ([Link here](https:\/\/machinelearningmastery.com\/elastic-net-regression-in-python\/))\n- **svr** stands for Support Vector Regression is a type of \"SVM\" model using for regression problem ([Link here](https:\/\/towardsdatascience.com\/an-introduction-to-support-vector-regression-svr-a3ebc1672c2))\n- **gbr** is gradient boosting model for regression problem ([Link here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html))\n- **lgbm** is a fast, distributed, high-performance gradient boosting framework that uses a tree-based learning algorithm ([Link here](https:\/\/machinelearningmastery.com\/light-gradient-boosted-machine-lightgbm-ensemble\/))\n- **xgboost** is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework ([Link here](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d#:~:text=What%20is%20XGBoost%3F,all%20other%20algorithms%20or%20frameworks.))","cb805a87":"## Quality Sum Feature","0f5b1c93":"<a name='2'><\/a>\n# 2. Feature engineering","e5f87d88":"## Technics used in this notebook\n* Compute AgeWhenSold and clean when YearSold > YearBuilt (few in the test set)\n* Construct features from exercice (LivLotRatio, Spaciousness, PorchTypes, MedNhbdArea, BldgType x GrLivArea)\n* M Estimate from Exercice (Neighborhood, SaleType, MSSubClass)\n* Construct Quality_sum feature from qualities features\n* Add a Neighborhood Square feet price\n* Cluster on comfort equipment (Heating, HeatingQC, CentralAir, Electrical)\n* PCA on low impact feature (ie features with a low score on Mutual Information)\n\nfeature selection:\n* One try based on Mutual Information\n* One try based on Feature Importance\n\nModel:\n* The blending has been removed as it confused us about the effects of our feature engineering.","140590de":"<a name='1.1'><\/a>\n## 1.1 Missing values","c05875a1":"<a name='4'><\/a>\n# 4 Submision","8c879899":"- \"Overfitting\" problem happens when the model overlearns the detail of training dataset so that it'll negatively impact the performance of model on the testing dataset. In short, when the performance on training dataset is much more higher than performance on testing dataset\n- Computing rmse applying cross validation technique is effective to prevent the \"Overfitting\" problem"}}