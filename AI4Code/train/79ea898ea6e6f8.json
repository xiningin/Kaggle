{"cell_type":{"5314156e":"code","72c94ae6":"code","306d18a6":"code","b6ef533b":"code","7196c2e0":"code","83d80f05":"code","32c64501":"code","13415aab":"code","8535fa7d":"code","ace9a763":"code","49d0dafe":"code","cfa1cd0a":"code","0b8387e8":"code","fd8d5680":"code","5127f2a3":"code","edd8c77d":"code","625dbe97":"code","4de90cb0":"code","0924105b":"code","3ca17616":"code","9db517be":"code","238bc88f":"code","2443741d":"code","47e2821d":"code","7b663885":"code","db459516":"code","144380fd":"code","b6316789":"code","2c0cf6bd":"code","5fd1d0e0":"code","9fb73155":"code","1ffe7f4c":"code","34897c43":"code","4b5064ec":"code","8e3f3626":"code","5634795c":"code","995e94d6":"code","fe1830c2":"code","80f95a6b":"code","64e39ee4":"markdown","924d53d5":"markdown","966e9ea2":"markdown","c0caa8b5":"markdown","db7f7dbc":"markdown","3547f2a8":"markdown","fb55b826":"markdown","a8e6334a":"markdown","94a70a6f":"markdown","5f2d45f5":"markdown","b93878cf":"markdown","5de0a573":"markdown","36406920":"markdown","e0a48002":"markdown","b8570c3b":"markdown","678fa582":"markdown","a07769de":"markdown","5d031abe":"markdown","55690629":"markdown","fd4221f2":"markdown","c2afde2a":"markdown","be467dd2":"markdown","6a042ba3":"markdown","e71663c9":"markdown","e382f2d2":"markdown"},"source":{"5314156e":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport datetime\nimport os\nimport time\n!pip install ta\nimport ta\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow_datasets as tfds\n!pip install tensorflow-addons\nimport tensorflow_addons as tfa\nfrom sklearn.preprocessing import MinMaxScaler","72c94ae6":"complete_stock_data_path = keras.utils.get_file(\"all_stocks_data_JAN_2006_to_JAN_2018.csv\",\n                                                \"https:\/\/raw.githubusercontent.com\/prikmm\/Stock_predictor\/main\/all_stocks_2006-01-01_to_2018-01-01.csv\")\n\nstock_data_2017_path = keras.utils.get_file(\"all_stocks_data_JAN_2017_to_JAN_2018.csv\",\n                                            \"https:\/\/raw.githubusercontent.com\/prikmm\/Stock_predictor\/main\/all_stocks_2017-01-01_to_2018-01-01.csv\")","306d18a6":"print(complete_stock_data_path)\nprint(stock_data_2017_path)","b6ef533b":"complete_stock_df = pd.read_csv(complete_stock_data_path)\ncomplete_stock_df.head()","7196c2e0":"complete_stock_df.info()","83d80f05":"grouped_stock_df = complete_stock_df.set_index([\"Name\", \"Date\"])\ngrouped_stock_df","32c64501":"stock_names = complete_stock_df['Name'].unique()\nstock_names","13415aab":"ibm_df = grouped_stock_df.loc['IBM'].copy()\nibm_df","8535fa7d":"temp_ibm_df = ibm_df.reset_index()\nmasker = temp_ibm_df['Date'].str.split('-').str[0].astype(int)\n\n# Train Set\ntrain_df = temp_ibm_df[masker < 2016].copy()\ntrain_df = train_df.set_index(\"Date\")\n\n\n# Valid Set\nvalid_df = temp_ibm_df[(masker >= 2016) & (masker < 2017)].copy()\nvalid_df = valid_df.set_index(\"Date\")\n\n\n# Test Set\ntest_df = temp_ibm_df[masker >= 2017].copy()\ntest_df = test_df.set_index(\"Date\")\n\n\ndatasets_list = (('Train', train_df), \n                 ('Valid', valid_df), \n                 ('Test', test_df))\n\nfor Name, data in datasets_list:\n    print(f\"{'-'*20}{Name}{'-'*20}\")\n    print(\"Data---\", data.shape)\n    print(data.head())\n    print(data.tail())\n    print(f\"End{'-'*60}\"+\"\\n\")","ace9a763":"plt.figure(figsize=(40, 10))\nplt.plot(train_df.index.get_level_values(0), train_df[\"Close\"], label=\"Train\")\nplt.plot(valid_df.index.get_level_values(0), valid_df[\"Close\"], label=\"Valid\")\nplt.plot(test_df.index.get_level_values(0), test_df[\"Close\"], label=\"Test\")\nplt.legend()\nplt.show()","49d0dafe":"data_2017 = pd.read_csv(stock_data_2017_path).set_index([\"Name\", \"Date\"]).drop(['IBM'])\ndata_2017","cfa1cd0a":"extra_stock_names = stock_names[stock_names!='IBM']\nextra_stock_names","0b8387e8":"fig = px.line(data_2017, x=data_2017.index.get_level_values(1), y=\"Close\",\n              color=data_2017.index.get_level_values(0),\n              color_discrete_sequence=px.colors.qualitative.Alphabet)\nfig.show()","fd8d5680":"extra_data = grouped_stock_df.loc[extra_stock_names]\n\nfig = px.line(extra_data, x=extra_data.index.get_level_values(1), y=\"Close\",\n              color=extra_data.index.get_level_values(0),\n              color_discrete_sequence=px.colors.qualitative.Alphabet)\nfig.show()","5127f2a3":"def complete_scaled_dataset_creator(data, window_size=30, batch_size=30):\n   \n    # Using a library known as \"Technical Analysis\" we will add Technical \n    # indicators, which the stock analysts use to predict\n    # different properties of stock.\n    y_scaler = MinMaxScaler()\n    y_scaler.fit(data)\n    temp_data = ta.add_all_ta_features(data, open=\"Open\", high=\"High\", \n                                       low=\"Low\", close=\"Close\", \n                                       volume=\"Volume\", fillna=True)\n    \n    x_scaler = MinMaxScaler()\n    temp_data = x_scaler.fit_transform(temp_data)        \n    dataset = tf.data.Dataset.from_tensor_slices((temp_data))\n    dataset = dataset.window(window_size+1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(lambda window: (window[:, :-1], window[:, -1, :5]))\n    return dataset.prefetch(1), y_scaler\n\n\nclass Callbacks():\n    def __init__(self, model_name, log_path=\".\/log\"):\n        self.model_name = model_name\n        self.log_path = log_path\n        self.model_log_path = f'{self.log_path}\/{self.model_name}'\n        os.makedirs(self.model_log_path, exist_ok=True)\n        self.model_run = 0\n\n    def generate_log_name(self):\n        self.model_run += 1\n        return f'{self.model_log_path}\/run_{self.model_run}'\n\n    def get_callbacks(self, patience=5,):\n        callbacks = [keras.callbacks.ModelCheckpoint(f'{self.model_name}.h5'),\n                     keras.callbacks.EarlyStopping(patience=patience),\n                     keras.callbacks.TensorBoard(self.generate_log_name())]\n        \n        return callbacks\n\n\nclass LastTimeStepMSE(keras.metrics.Metric):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.mse_history = tf.TensorArray(tf.float32, size=0,\n                                          dynamic_size=True,\n                                          clear_after_read=False)\n        self.count = 0\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        metric = keras.metrics.mean_squared_error(y_true[:, -1], y_pred[:, -1])\n        self.mse_history = self.mse_history.write(self.count, metric)\n        self.count += 1\n    \n    def result(self):\n        return self.mse_history.read(self.count-1)\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config}","edd8c77d":"window_size=20\nbatch_size=30\n\n# We will copy the dataset as, the function\n# which adds Technical Indicator performs inplace operations\ntemp_train_df = train_df.copy()  \ntemp_valid_df = valid_df.copy()\ntemp_test_df = test_df.copy()\ntrain_set, train_scaler = complete_scaled_dataset_creator(temp_train_df,\n                                                          window_size=window_size,\n                                                          batch_size=batch_size)\nvalid_set, valid_scaler = complete_scaled_dataset_creator(temp_valid_df,\n                                                          window_size=window_size,\n                                                          batch_size=batch_size)\ntest_set, test_scaler = complete_scaled_dataset_creator(temp_test_df,\n                                                        window_size=window_size,\n                                                        batch_size=batch_size)\n\nfor item in train_set.take(1):\n    print(item)","625dbe97":"train_scaler.data_max_","4de90cb0":"# Updated the funciton to handle EncoderDecoder and Transformer model\ndef predictor_plotter_func(model_name, train_set=train_set, valid_set=valid_set, \n                           train_scaler=train_scaler, valid_scaler=valid_scaler,\n                           ed_architecture=False, ed_model=None, which_next_day=0,\n                           transformer_architecture=False,\n                           testing=False, test_set=test_set, test_scaler=test_scaler):\n    # loading the best model and predicting the values\n    if transformer_architecture:\n        model = keras.models.load_model(model_name, custom_objects={'Time2Vector': Time2Vector,\n                                                                    'TransformerEncoder': TransformerEncoder})\n    elif ed_architecture:\n        #weights = keras.models.load_weights(model_name)\n        ed_model.load_weights(model_name)\n        model = ed_model\n    else:    \n        model = keras.models.load_model(model_name, custom_objects={'LastTimeStepMSE': LastTimeStepMSE})\n    y_train_pred = model.predict(train_set)\n    y_valid_pred = model.predict(valid_set)\n    print(\"Train Prediction Shape:\", y_train_pred.shape)\n    print(\"Valid Prediction Shape:\", y_valid_pred.shape)\n    if testing:\n        y_test_pred = model.predict(test_set)\n        print(\"Test Prediction Shape:\", y_test_pred.shape)\n\n    # making predictions fit for inverse_transform by making \n    # the dimensions of predictions same as those used for fitting the scaler\n    if ed_architecture:\n        # only the last sequence output is holds the next 10 values and\n        # we will only use a particular days(next 10 days) values (i.e first 5 feature values)\n        which_next_day = which_next_day  # 0-9\n        start = which_next_day * 5\n        end = (which_next_day + 1) * 5\n        y_train_pred = y_train_pred[:, -1, start:end]    \n        y_valid_pred = y_valid_pred[:, -1, start:end]\n        if testing:\n            y_test_pred = y_test_pred[:, -1, start:end]\n\n    # inversing the scaled value to obtain the original values\n    y_train_pred = train_scaler.inverse_transform(y_train_pred)\n    y_valid_pred = valid_scaler.inverse_transform(y_valid_pred)\n    if testing:\n        y_test_pred = test_scaler.inverse_transform(y_test_pred)\n\n    # plotting the graph between original and predicted values\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=train_df.index.get_level_values(0)[1:],\n                             y=train_df[\"Close\"][1:],\n                             mode=\"lines\",\n                             name=\"Train\")) \n    fig.add_trace(go.Scatter(x=train_df.index.get_level_values(0)[1:len(y_train_pred[:, 3])+1],\n                             y=y_train_pred[:, 3],\n                             mode=\"lines\",\n                             name=\"Train_Prediction\"))\n    fig.add_trace(go.Scatter(x=valid_df.index.get_level_values(0)[1:],\n                             y=valid_df[\"Close\"][1:],\n                             mode=\"lines\",\n                             name=\"Valid\"))\n    fig.add_trace(go.Scatter(x=valid_df.index.get_level_values(0)[1:len(y_valid_pred[:, 3])+1],\n                             y=y_valid_pred[:, 3],\n                             mode=\"lines\",\n                             name=\"Valid_Prediction\"))\n    if testing:\n        fig.add_trace(go.Scatter(x=test_df.index.get_level_values(0)[1:],\n                             y=test_df[\"Close\"][1:],\n                             mode=\"lines\",\n                             name=\"Test\"))\n        fig.add_trace(go.Scatter(x=test_df.index.get_level_values(0)[1:],\n                             y=y_test_pred[:, 3],\n                             mode=\"lines\",\n                             name=\"Test Prediction\"))\n    fig.show()\n\n    if testing:\n        return {\"model\": model,\n                \"y_train_pred\": y_train_pred,\n                \"y_valid_pred\": y_valid_pred,\n                \"y_test_pred\": y_test_pred}\n    else:\n        return {\"model\": model,\n                \"y_train_pred\": y_train_pred,\n                \"y_valid_pred\": y_valid_pred}","0924105b":"np.random.seed(42)\ntf.random.set_seed(42)\nlstm_nonst_callbacks = Callbacks(\"LSTM\", \".\/logs\/non_stationary\/\")\n\nlstm_nonst_model = keras.models.Sequential([\n    keras.layers.LSTM(50, input_shape=[None, 88], return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.LSTM(50, return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.LSTM(50, return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.LSTM(50),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(5)\n])\n\nlstm_nonst_model.compile(loss=\"mse\",\n                   optimizer=keras.optimizers.Nadam(),\n                   metrics=[LastTimeStepMSE()])\nlstm_nonst_history = lstm_nonst_model.fit(train_set, epochs=50,\n                                          validation_data=valid_set,\n                                          callbacks=lstm_nonst_callbacks.get_callbacks(10))","3ca17616":"lstm_nonst_details = predictor_plotter_func(\"LSTM.h5\")","9db517be":"np.random.seed(42)\ntf.random.set_seed(42)\ngru_nonst_callbacks = Callbacks(\"GRU\", \".\/logs\/non_stationary\/\")\n\ngru_nonst_model = keras.models.Sequential([\n    keras.layers.GRU(50, input_shape=[None, 88], return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.GRU(50, return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.GRU(50, return_sequences=True),\n    keras.layers.Dropout(0.2),\n    keras.layers.GRU(50),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(5)\n])\n\ngru_nonst_model.compile(loss=\"mse\",\n                        optimizer=keras.optimizers.Nadam(),\n                        metrics=[LastTimeStepMSE()])\ngru_nonst_history = gru_nonst_model.fit(train_set, epochs=50,\n                                         validation_data=valid_set,\n                                         callbacks=gru_nonst_callbacks.get_callbacks(10))","238bc88f":"gru_nonst_details = predictor_plotter_func(\"GRU.h5\")","2443741d":"np.random.seed(42)\ntf.random.set_seed(42)\nwavenet_nonst_callbacks = Callbacks(\"Wavenet\", \".\/logs\/non_stationary\/\")\n\nwavenet_nonst_model = keras.models.Sequential()\nwavenet_nonst_model.add(keras.layers.InputLayer(input_shape=[None, 88]))\nwavenet_nonst_model.add(keras.layers.LSTM(50, return_sequences=True))\nwavenet_nonst_model.add(keras.layers.Dropout(0.2))\nfor rate in (1, 2, 4, 8) * 2:\n    wavenet_nonst_model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\", \n                                          activation=\"relu\", dilation_rate=rate))\nwavenet_nonst_model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\nwavenet_nonst_model.add(keras.layers.GRU(50))\nwavenet_nonst_model.add(keras.layers.Dropout(0.2))\nwavenet_nonst_model.add(keras.layers.Dense(5))\n\nwavenet_nonst_model.compile(loss=\"mse\",\n                            optimizer=keras.optimizers.Nadam(),\n                            metrics=[LastTimeStepMSE()])\nwavenet_nonst_history = wavenet_nonst_model.fit(train_set, epochs=50,\n                                                validation_data=valid_set,\n                                                callbacks=wavenet_nonst_callbacks.get_callbacks(10))","47e2821d":"wavenet_nonst_details = predictor_plotter_func(\"Wavenet.h5\")","7b663885":"def ed_dataset_creator(data, window_size=30, batch_size=30,\n                       each_output_size=10, seq_len=False):\n    \n    # Creating Targets, each target is a 10D vector, which\n    # contains values of 10 future timesteps (10 x 5(no.of features) = 50)\n    y_scaler = MinMaxScaler()\n    y = y_scaler.fit_transform(data)\n    y = np.float32(y)\n    y = tf.data.Dataset.from_tensor_slices(y[1:])\n    y = y.window(each_output_size, shift=1, drop_remainder=True)\n    y = y.flat_map(lambda window: window.batch(each_output_size))\n    y = y.map(lambda window: tf.reshape(window, [-1]))\n\n    # What we have now is dataset containing 50 columns per row,\n    # Now, we will convert this into a dataset of dimension\n    # (batch_size, timesteps, features), where features=50\n    # Hence, dataset for seq2seq model is ready and it will output\n    # 50 predictions per timestep\n    y = y.window(window_size, shift=1, drop_remainder=True)\n    y = y.flat_map(lambda window: window.batch(window_size))\n    y = y.batch(batch_size, drop_remainder=True)\n\n\n    # Creating Sequence lengths dataset which indicates the size of a sample\n    if seq_len:\n        seqlens = np.full(data.shape[0], window_size)\n        seqlens = np.int32(seqlens)\n        seqlens = tf.data.Dataset.from_tensor_slices(seqlens)\n        seqlens = seqlens.window(window_size)\n        seqlens = seqlens.flat_map(lambda windows: windows.batch(window_size))\n        #seqlens = seqlens.batch(batch_size)\n\n    # Using a library known as \"Technical Analysis\" we will add Technical \n    # indicators, which the stock analysts use to predict\n    # different properties of stock.\n    X = ta.add_all_ta_features(data, open=\"Open\", high=\"High\", \n                               low=\"Low\", close=\"Close\", \n                               volume=\"Volume\", fillna=True)\n    X_scaler = MinMaxScaler()\n    X = X_scaler.fit_transform(X)  \n    X = np.float32(X)   \n    X = tf.data.Dataset.from_tensor_slices(X)\n    X = X.window(window_size, shift=1, drop_remainder=True)\n    X = X.flat_map(lambda window: window.batch(window_size))\n    X = X.batch(batch_size, drop_remainder=True)\n\n    # using \"zip\" to get the final dataset\n    if seq_len:\n        dataset = tf.data.Dataset.zip((X, y, seqlens, y))\n    else:\n        dataset = tf.data.Dataset.zip((X, y))\n    return dataset.prefetch(1), y_scaler\n","db459516":"window_size=30\nbatch_size=30\ntemp_train_df = train_df.copy()\ntemp_valid_df = valid_df.copy()\ntemp_test_df = test_df.copy()\ntrain_set, train_scaler = ed_dataset_creator(temp_train_df,\n                                             window_size=window_size,\n                                             batch_size=batch_size,\n                                             seq_len=True)\nvalid_set, valid_scaler = ed_dataset_creator(temp_valid_df,\n                                             window_size=window_size,\n                                             batch_size=batch_size,\n                                             seq_len=True)\ntest_set, test_scaler = ed_dataset_creator(temp_test_df,\n                                           window_size=window_size,\n                                           batch_size=batch_size,\n                                           seq_len=True)","144380fd":"def get_encdec_model(input_features, output_features,\n                     encoder_cell_units=50, decoder_cell_units=50):\n    encoder_inputs = keras.layers.Input(shape=[None, input_features], dtype=tf.float32, name=\"encoder\")\n    decoder_inputs = keras.layers.Input(shape=[None, output_features], dtype=tf.float32, name=\"decoder\")\n    sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32, name=\"seqlens\")\n\n    encoder = keras.layers.LSTM(encoder_cell_units, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_state = [state_h, state_c]\n\n    sampler = tfa.seq2seq.sampler.TrainingSampler()\n\n    decoder_cell = keras.layers.LSTMCell(decoder_cell_units)\n    output_layer = keras.layers.Dense(output_features)\n    decoder = tfa.seq2seq.BasicDecoder(decoder_cell, sampler,\n                                    output_layer=output_layer)\n    final_outputs, final_state, final_sequence_lenghts = decoder(\n        decoder_inputs, initial_state=encoder_state,\n        sequence_length=sequence_lengths,\n    )\n\n    model = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n                                outputs=[final_outputs.rnn_output])\n    return model","b6316789":"encdec_nonst_callbacks = Callbacks(\"EncoderDecoder\", \".\/logs\/non_stationary\/\")\nencdec_nonst_model = get_encdec_model(88, 50)\nencdec_nonst_model.compile(loss=\"mse\",\n                           optimizer=keras.optimizers.Nadam(),\n                           metrics=[LastTimeStepMSE()])\n\ntf.keras.utils.plot_model(\n    encdec_nonst_model,\n    to_file=\"IBM_encoder_decoder.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    expand_nested=True,\n    dpi=96,)","2c0cf6bd":"def input_mapper(encoder_input, decoder_input, seqlens_input, labels):\n    return {\n        \"encoder\": encoder_input,\n        \"decoder\": decoder_input,\n        \"seqlens\": seqlens_input, \n    }, labels","5fd1d0e0":"np.random.seed(42)\ntf.random.set_seed(42)\nmapped_train_set = train_set.map(input_mapper)\nmapped_valid_set = valid_set.map(input_mapper)\nencdec_nonst_history = encdec_nonst_model.fit(mapped_train_set, epochs=50,\n                                              validation_data=mapped_valid_set,\n                                              callbacks=encdec_nonst_callbacks.get_callbacks(10))","9fb73155":"encdec_nonst_details = predictor_plotter_func(\"EncoderDecoder.h5\",\n                                              train_set=mapped_train_set,\n                                              valid_set=mapped_valid_set,\n                                              ed_architecture=True,\n                                              ed_model=get_encdec_model(88, 50))","1ffe7f4c":"window_size = 128\nbatch_size = 50","34897c43":"class Time2Vector(keras.layers.Layer):\n    def __init__(self, seq_len, **kwargs):\n        super().__init__(**kwargs)\n        self.seq_len = seq_len\n\n    def build(self, input_shape):\n        '''Initialize weights and biases with shape (batch, seq_len)'''\n        self.weights_linear = self.add_weight(name=\"weight_linear\",\n                                              shape=(int(self.seq_len),),\n                                              initializer='uniform',\n                                              trainable=True)\n        self.bias_linear = self.add_weight(name=\"bias_linear\",\n                                           shape=(int(self.seq_len),),\n                                           initializer='uniform',\n                                           trainable=True)\n        self.weight_periodic = self.add_weight(name=\"weight_periodic\",\n                                                shape=(int(self.seq_len),),\n                                                initializer='uniform',\n                                                trainable=True)\n        self.bias_periodic = self.add_weight(name=\"bias_periodic\",\n                                              shape=(int(self.seq_len),),\n                                              initializer='uniform',\n                                              trainable=True)\n        \n    def call(self, x):\n        '''Calculate linear and periodic time features'''\n        x = tf.math.reduce_mean(x[:, :, :4], axis=-1)\n        time_linear = self.weights_linear * x + self.bias_linear\n        time_linear = tf.expand_dims(time_linear, axis=-1)\n\n        time_periodic = tf.math.sin(tf.multiply(x, self.weight_periodic) + self.bias_periodic)\n        time_periodic = tf.expand_dims(time_periodic, axis=-1)\n        return tf.concat([time_linear, time_periodic], axis=-1)\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config,\n                'seq_len': self.seq_len}\n\n\n\nclass TransformerEncoder(keras.layers.Layer):\n    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n        super().__init__(**kwargs)\n        self.d_k = d_k\n        self.d_v = d_v\n        self.n_heads = n_heads\n        self.ff_dim = ff_dim\n        self.dropout_rate = dropout\n\n    def build(self, input_shape):\n        #print(input_shape)\n        self.attn_multi = keras.layers.MultiHeadAttention(self.n_heads, self.d_k, self.d_v)\n        self.attn_dropout = keras.layers.Dropout(rate=self.dropout_rate)\n        self.attn_normalize = keras.layers.LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n        self.ff_conv1D_1 = keras.layers.Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n        self.ff_conv1D_2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1)\n        self.ff_dropout = keras.layers.Dropout(self.dropout_rate)\n        self.ff_normalize = keras.layers.LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n    def call(self, queries, values):\n        attn_layer = self.attn_multi(queries, values)\n        attn_layer = self.attn_dropout(attn_layer)\n        attn_layer = self.attn_normalize(queries + attn_layer)\n        \n        ff_layer = self.ff_conv1D_1(attn_layer)\n        ff_layer = self.ff_conv1D_2(ff_layer)\n        ff_layer = self.ff_dropout(ff_layer)\n        ff_layer = self.ff_normalize(queries + ff_layer)\n        return ff_layer\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config,\n                \"d_k\": self.d_k,\n                \"d_v\": self.d_v,\n                \"n_heads\": self.n_heads,\n                \"ff_dim\": self.ff_dim,\n                \"dropout\": self.dropout_rate}\n\n\n\ndef create_transformer_model(batch_size=30, \n                             seq_len = 30,\n                             num_features = 88, \n                             d_k = 60, \n                             d_v = 60,\n                             n_heads = 12, \n                             ff_dims = 60):\n    \n    '''Initialize time and transformer layers'''\n    time_embedding = Time2Vector(seq_len)\n    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dims)\n    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dims)\n    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dims)\n\n    '''Construct Model'''\n    in_seq = keras.layers.Input(shape=(seq_len, num_features))\n    x = time_embedding(in_seq)\n    x = keras.layers.Concatenate(axis=-1)([in_seq, x])\n    x = attn_layer1(x, x)\n    x = attn_layer2(x, x)\n    x = attn_layer3(x, x)\n    x = keras.layers.GlobalAveragePooling1D(data_format='channels_first')(x)\n    x = keras.layers.Dropout(rate=0.1)(x)\n    x = keras.layers.Dense(64, activation='relu')(x)\n    x = keras.layers.Dropout(rate=0.1)(x)\n    out = keras.layers.Dense(5, activation='linear')(x)\n\n    model = keras.Model(inputs=in_seq, outputs=out)\n    model.compile(loss=\"mse\",\n                  optimizer=keras.optimizers.Nadam(),\n                  metrics=['mae'])\n    return model","4b5064ec":"np.random.seed(42)\ntf.random.set_seed(42)\ntransformer_model = create_transformer_model(batch_size=batch_size,\n                                             seq_len=window_size,\n                                             d_k=256,\n                                             d_v=256,\n                                             ff_dims=256)\ntransformer_model.summary()","8e3f3626":"tf.keras.utils.plot_model(\n    transformer_model,\n    to_file=\"Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    expand_nested=True,\n    dpi=96,)","5634795c":"temp_train_df = train_df.copy()  \ntemp_valid_df = valid_df.copy()\ntemp_test_df = test_df.copy()\ntransformer_train_set, train_scaler = complete_scaled_dataset_creator(temp_train_df,\n                                                                      window_size=window_size,\n                                                                      batch_size=batch_size)\ntransformer_valid_set, valid_scaler = complete_scaled_dataset_creator(temp_valid_df,\n                                                                      window_size=window_size,\n                                                                      batch_size=batch_size)\ntransformer_test_set, test_scaler = complete_scaled_dataset_creator(temp_test_df,\n                                                                    window_size=window_size,\n                                                                    batch_size=batch_size)\nfor item in transformer_train_set.take(1):\n    print(item)","995e94d6":"transformer_callbacks = Callbacks(\"Transformer\", \".\/logs\/non_stationary\/\")\ntransformer_model.fit(transformer_train_set, epochs=50,\n                      validation_data=transformer_valid_set,\n                      callbacks=transformer_callbacks.get_callbacks(5))","fe1830c2":"transformer_details = predictor_plotter_func(\"Transformer.h5\",\n                                              train_set=transformer_train_set,\n                                              valid_set=transformer_valid_set,\n                                              transformer_architecture=True)","80f95a6b":"mapped_train_set = train_set.map(input_mapper)\nmapped_valid_set = valid_set.map(input_mapper)\nmapped_test_set = test_set.map(input_mapper)\nbest_model_details = predictor_plotter_func(\"EncoderDecoder.h5\",\n                                            train_set=mapped_train_set,\n                                            valid_set=mapped_valid_set,\n                                            ed_architecture=True,\n                                            ed_model=get_encdec_model(88, 50),\n                                            test_set=mapped_test_set,\n                                            testing=True)","64e39ee4":"Transformer implementation is heavily inspired by **Jan Schmitz's Blog in Towards Data Science**: https:\/\/towardsdatascience.com\/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6","924d53d5":"### Creating Train, Valid and Test datasets from IBM data:","966e9ea2":"***A company's stock price is not only dependent on the company's past     performance, but also on the performance of its competitors and many other factors like governmental policies, mergers etc. But, it is extremely difficult to make a model that can take into account all the possible factors. We will first make a model which will take into account a company's past performance, then TRY to create a model which will try to learn the dependency between different companies.***","c0caa8b5":"#### Visualizing data from 2006 to 2018 of all stocks except IBM:","db7f7dbc":"### WORK IN PROGRESS!! WOULD LOVE TO HEAR YOUR SUGGESTIONS!","3547f2a8":"Sadly, tensorboard are not enabled in kaggle notebooks as they make notebook boot-ups slow:\nhttps:\/\/www.kaggle.com\/product-feedback\/89671\n\n> %load_ext tensorboard<br>\n> %tensorboard --logdir=.\/logs\/non_stationary --port=6006","fb55b826":"Defining **Time2Vector Layer**, & **TransformerEncoder Layer**:","a8e6334a":"### a) Using Non-Stationary Data:","94a70a6f":"### Goal is to predict the price of IBM stock for year 2017!","5f2d45f5":"#### Normal GRU Architecture:","b93878cf":"# Modelling:","5de0a573":"Haven't worked out exacty how to get the transformer to learn better, :)","36406920":"## 1. Model using past performance:","e0a48002":"### Encoder-Decoder Architecture:","b8570c3b":"#### Visualizing data of year 2017 of all stocks except \"IBM\":","678fa582":"<a href=\"https:\/\/colab.research.google.com\/github\/prikmm\/Stock_predictor\/blob\/main\/Stock_predictor.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","a07769de":"### Wavenet Architecture:","5d031abe":"### Normal LSTM architecture:","55690629":"## Visualization:","fd4221f2":"Ran Wavenet model many times and very rarely did it even come close to performing the same as LSTM or GRU!! ","c2afde2a":"Given below is a function used for mapping input datasets to their respective layers:","be467dd2":"So, the best model till now is EncoderDecoder, I still feel getting transformer will work better due its its attentions mechanism, well will try making it work!!","6a042ba3":"### Visualizing IBM data:","e71663c9":"Each batch consists of samples and each sample consists of timesteps (i.e days).","e382f2d2":"### Transformer Architecture:"}}