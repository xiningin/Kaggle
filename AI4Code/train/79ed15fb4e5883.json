{"cell_type":{"942c9d7e":"code","90eb6eb0":"code","468f3569":"code","90d1e870":"code","0ef53d2b":"code","a38a653c":"code","5e7112c4":"code","6f08f8f0":"code","22eb5939":"code","a7360668":"code","b09e1ed2":"code","13f5fbe1":"code","4a1bd2c7":"code","1a00b549":"code","1d8581af":"code","1299a8c9":"code","32bbe71d":"code","fd47b1db":"code","827fcf60":"code","f68111a0":"code","77e747a9":"code","e784be65":"code","a1f4d35d":"code","267ed5ad":"code","0bd0eaee":"code","3ee70f14":"code","dd9063f3":"code","0ef2e043":"code","515a7af4":"code","4ea6e7a3":"code","cf5387b6":"code","7cd4365b":"code","e7385e52":"code","29308f62":"code","5876ee53":"code","217a0350":"code","b2044dee":"code","6cb90740":"code","ac4a4146":"code","d33fb065":"code","103be24f":"code","10284939":"code","fc31fe97":"code","353d0479":"markdown","f9ab8566":"markdown","3a8701cb":"markdown","c8008d2e":"markdown","37667337":"markdown","30c012b9":"markdown","5e9a423e":"markdown","1c9566b0":"markdown","e6bf5382":"markdown","70768328":"markdown"},"source":{"942c9d7e":"from IPython.display import clear_output","90eb6eb0":"!pip install contractions\n!pip install scikit-learn  -U\n!pip install nltk  -U\n!pip install spacy  -U\n!pip install emoji -U\n!pip install transformers -U\nclear_output()","468f3569":"!python -m spacy download en_core_web_md\nclear_output()","90d1e870":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy\",\n    u\":\\)\":\"Happy\",\n    u\":-\\]\":\"Happy\",\n    u\":\\]\":\"Happy\",\n    u\":-3\":\"Happy\",\n    u\":3\":\"Happy\",\n    u\":->\":\"Happy\",\n    u\":>\":\"Happy\",\n    u\"8-\\)\":\"Happy\",\n    u\":o\\)\":\"Happy\",\n    u\":-\\}\":\"Happy\",\n    u\":\\}\":\"Happy\",\n    u\":-\\)\":\"Happy\",\n    u\":c\\)\":\"Happy\",\n    u\":\\^\\)\":\"Happy\",\n    u\"=\\]\":\"Happy\",\n    u\"=\\)\":\"Happy\",\n    u\":\u2011D\":\"Laughing\",\n    u\":D\":\"Laughing\",\n    u\"8\u2011D\":\"Laughing\",\n    u\"8D\":\"Laughing\",\n    u\"X\u2011D\":\"Laughing\",\n    u\"XD\":\"Laughing\",\n    u\"=D\":\"Laughing\",\n    u\"=3\":\"Laughing\",\n    u\"B\\^D\":\"Laughing\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"sad\",\n    u\":-\\(\":\"sad\",\n    u\":\\(\":\"sad\",\n    u\":\u2011c\":\"sad\",\n    u\":c\":\"sad,\",\n    u\":\u2011<\":\"sad,\",\n    u\":<\":\"sad,\",\n    u\":\u2011\\[\":\"sad\",\n    u\":\\[\":\"sad\",\n    u\":-\\|\\|\":\"sad\",\n    u\">:\\[\":\"sad\",\n    u\":\\{\":\"sad\",\n    u\":@\":\"sad\",\n    u\">:\\(\":\"sad\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"happiness\",\n    u\":'\\)\":\"happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"dismay\",\n    u\"D;\":\"dismay\",\n    u\"D=\":\"dismay\",\n    u\"DX\":\"dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink\",\n    u\";\\)\":\"Wink\",\n    u\"\\*-\\)\":\"Wink\",\n    u\"\\*\\)\":\"Wink\",\n    u\";\u2011\\]\":\"Wink\",\n    u\";\\]\":\"Wink\",\n    u\";\\^\\)\":\"Wink\",\n    u\":\u2011,\":\"Wink\",\n    u\";D\":\"Wink\",\n    u\":\u2011P\":\"playful\",\n    u\":P\":\"playful\",\n    u\"X\u2011P\":\"playful\",\n    u\"XP\":\"playful\",\n    u\":\u2011\u00de\":\"playful\",\n    u\":\u00de\":\"playful\",\n    u\":b\":\"playful\",\n    u\"d:\":\"playful\",\n    u\"=p\":\"playful\",\n    u\">:P\":\"playful\",\n    u\":\u2011\/\":\"uneasy\",\n    u\":\/\":\"uneasy\",\n    u\":-[.]\":\"uneasy\",\n    u\">:[(\\\\\\)]\":\"uneasy\",\n    u\">:\/\":\"uneasy\",\n    u\":[(\\\\\\)]\":\"uneasy\",\n    u\"=\/\":\"uneasy\",\n    u\"=[(\\\\\\)]\":\"uneasy\",\n    u\":L\":\"uneasy\",\n    u\"=L\":\"uneasy\",\n    u\":S\":\"uneasy\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed\",\n    u\":\u2011x\":\"Sealed lips\",\n    u\":x\":\"Sealed lips\",\n    u\":\u2011#\":\"Sealed lips\",\n    u\":#\":\"Sealed lips\",\n    u\":\u2011&\":\"Sealed lips\",\n    u\":&\":\"Sealed lips\",\n    u\"O:\u2011\\)\":\"innocent\",\n    u\"O:\\)\":\"innocent\",\n    u\"0:\u20113\":\"innocent\",\n    u\"0:3\":\"innocent\",\n    u\"0:\u2011\\)\":\"innocent\",\n    u\"0:\\)\":\"innocent\",\n    u\":\u2011b\":\"playful\",\n    u\"0;\\^\\)\":\"innocent\",\n    u\">:\u2011\\)\":\"Evil\",\n    u\">:\\)\":\"Evil\",\n    u\"\\}:\u2011\\)\":\"Evil\",\n    u\"\\}:\\)\":\"Evil\",\n    u\"3:\u2011\\)\":\"Evil\",\n    u\"3:\\)\":\"Evil\",\n    u\">;\\)\":\"Evil\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Embarrassed\",\n    u\"\\(\\^_\\^;\\)\":\"Embarrassed\",\n    u\"\\(-_-;\\)\":\"Embarrassed\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Embarrassed\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"respect\",\n    u\"_\\(\\._\\.\\)_\":\"respect\",\n    u\"<\\(_ _\\)>\":\"respect\",\n    u\"<m\\(__\\)m>\":\"respect\",\n    u\"m\\(__\\)m\":\"respect\",\n    u\"m\\(_ _\\)m\":\"respect\",\n    u\"\\('_'\\)\":\"Sad\",\n    u\"\\(\/_;\\)\":\"Sad\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad\",\n    u\"\\(;_;\":\"Sad\",\n    u\"\\(;_:\\)\":\"Sad\",\n    u\"\\(;O;\\)\":\"Sad\",\n    u\"\\(:_;\\)\":\"Sad\",\n    u\"\\(ToT\\)\":\"Sad\",\n    u\";_;\":\"Sad\",\n    u\";-;\":\"Sad\",\n    u\";n;\":\"Sad\",\n    u\";;\":\"Sad\",\n    u\"Q\\.Q\":\"Sad\",\n    u\"T\\.T\":\"Sad\",\n    u\"QQ\":\"Sad\",\n    u\"Q_Q\":\"Sad\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Laugh\",\n    u\"<\\^!\\^>\":\"Laugh\",\n    u\"\\^\/\\^\":\"Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Laugh\",\n    u\"\\(^\\^\\)\":\"Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Laugh\",\n    u\"\\(\\^\\^\\)\":\"Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"amazed\",\n    u\"\\(\\*_\\*;\":\"amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Deflated\"\n}\nchat_words_str = \"\"\"AFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime Anywhere Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It is Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My Ass Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The Ass\nPRT=Party\nPRW=Parents Are Watching\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My Ass Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age Sex Location\nTHX=Thank You\nTTFN=Ta Ta For Now\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The Fuck\nWTG=Way To Go\nWUF=Where Are You From\nW8=Wait\n7K=Sick:-D Laugher\"\"\"","0ef53d2b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom tqdm import tqdm\nimport emoji\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, roc_auc_score\nfrom wordcloud import WordCloud\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# For BERT\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tokenizers import BertWordPieceTokenizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport contractions as ct\nimport spacy\nimport string\nfrom tqdm import tqdm, tqdm_notebook\nfrom gensim.models import KeyedVectors, fasttext\n\ntqdm.pandas()\nnlp = spacy.load('en_core_web_md')\nnltk.download(\"stopwords\")\nstops = set(stopwords.words(\"english\"))\nct_dict = ct.contractions_dict","a38a653c":"stops = stops.union(set([\"hi\", \"hello\"]));","5e7112c4":"# Freeze layers from 0 to start and unfreeze next\ndef set_trainable(model, start):\n    layers_number = len(model.layers)\n    for i in range(0,start):\n        model.layers[i].trainable = False\n    for i in range(start, layers_number):\n        model.layers[i].trainable = True\n\n# Create callback to save best model\ndef save_best_callback(path, monitor=\"val_loss\", mode=\"min\"): \n    return tf.keras.callbacks.ModelCheckpoint(filepath=path, save_weights_only=True, monitor=monitor, mode=mode, save_best_only=True)\n\n# Plot history of .fit\ndef plot_history(history, metrics=None):\n    labels = []\n    df = pd.DataFrame(history)\n    ncols = len(df.columns)\n    fig, axs = plt.subplots(ncols=ncols \/\/ 2, nrows=1, figsize=(35,10))\n    if metrics is None:\n        metrics = set([i.replace(\"val_\", \"\") for i in history.keys()])\n    for metric in metrics:\n        labels.append([metric, f\"val_{metric}\"])\n    if ncols == 2:\n        sns.lineplot(data=df, ax=axs)\n        plt.xlabel(\"epoch\")\n    else:\n        for pair_ind in range(len(labels)):\n            sns.lineplot(data=df[labels[pair_ind]], ax=axs[pair_ind % ncols])\n            axs[pair_ind % ncols].set_xlabel(\"epoch\")\n    plt.show()\n\ndef plot_wordcloud(text, figsize=(15,15), **kwargs):\n    _, ax = plt.subplots(figsize=(15,15))\n    wordcloud = WordCloud(**kwargs).generate(text)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\");\n\ndef expand_contractions(text):\n    return \" \".join([ct_dict[w] if w in ct_dict else w for w in text.split()])","6f08f8f0":"test_labels = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/test_labels.csv\")","22eb5939":"toxic_comments_raw_data = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\nunintended_bias_raw_data = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\nvalidation = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv\")\ntest_raw_data = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\")","a7360668":"#training_data_es = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-es.csv\", index_col=\"id\")\n#training_data_fr = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-fr.csv\", index_col=\"id\")\n#training_data_it = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-it.csv\", index_col=\"id\")\n#training_data_pt = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-pt.csv\", index_col=\"id\")\n#training_data_ru = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-ru.csv\", index_col=\"id\")\n#training_data_tr = pd.read_csv(\"..\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-tr.csv\", index_col=\"id\")","b09e1ed2":"translated_test_data = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_test_translated.csv\", index_col=\"id\")\ntranslated_valid_data = pd.read_csv(\"..\/input\/jigsaw-multilingual-toxic-test-translated\/jigsaw_miltilingual_valid_translated.csv\", index_col=\"id\")","13f5fbe1":"unintended_bias_raw_data[\"toxic\"] = (unintended_bias_raw_data[\"toxic\"] > 0.5).astype(\"int32\")","4a1bd2c7":"toxic_comments_raw_data = toxic_comments_raw_data[[\"id\", \"comment_text\", \"toxic\"]]\nunintended_bias_raw_data = toxic_comments_raw_data[[\"id\", \"comment_text\", \"toxic\"]]","1a00b549":"print(f\"Number of rows in toxic ds: {toxic_comments_raw_data.shape[0]}\")\nprint(f\"Number of rows in unintended bias ds: {unintended_bias_raw_data.shape[0]}\")","1d8581af":"toxic_comments_raw_data[toxic_comments_raw_data[\"comment_text\"].str.len() < 4]","1299a8c9":"_, axs = plt.subplots(ncols=2, figsize=(15,5))\nsns.histplot(toxic_comments_raw_data[\"toxic\"], ax=axs[0]);\nsns.histplot(toxic_comments_raw_data[\"toxic\"], ax=axs[1]);\n\naxs[0].set_title(\"Toxic comments data\")\naxs[1].set_title(\"Unintended bias data\");","32bbe71d":"big_data = pd.concat([toxic_comments_raw_data, unintended_bias_raw_data])","fd47b1db":"plot_wordcloud(\" \".join(big_data[big_data[\"toxic\"] == 0].sample(100000)[\"comment_text\"].to_numpy()), width=2560, height=1440, figsize=(25, 25), max_words=300)","827fcf60":"plot_wordcloud(\" \".join(big_data[big_data[\"toxic\"] == 1][\"comment_text\"].to_numpy()), width=2560, height=1440, figsize=(25, 25), max_words=300)","f68111a0":"big_data[\"comment_text\"] = big_data[\"comment_text\"].progress_apply(lambda x: expand_contractions(x.lower()))\nbig_data[\"comment_text\"] = big_data[\"comment_text\"].progress_apply(lambda x: ' '.join([word for word in x.split() if word not in (stops)]))","77e747a9":"html_pattern = re.compile(r\"<[^<]+?>\")\ndate_pattern = re.compile(r\"\\d\\d[-\\.\\\\\\\/]\\d\\d[-\\.\\\\\\\/]\\d{0,4}\")\nemoticon_pattern = re.compile((f\"({'|'.join(EMOTICONS.keys())})\"))\nurl_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+') \nchat_words_map_dict = {pair[0]:pair[1] for pair in [line.split(\"=\") for line in chat_words_str.lower().split(\"\\n\")]}\nchat_words_set = set(chat_words_map_dict.keys())\n\ndef remove_html(text):\n    return html_pattern.sub(\"\", text)\n\ndef remove_dates(text):\n    return date_pattern.sub(\"\", text)\n\ndef remove_punctuation(text):\n    return \"\".join(filter(lambda x: x not in string.punctuation, text))\n\ndef remove_nums(text):\n    return \"\".join(filter(lambda x: not x.isdigit(), text))\n\ndef replace_emoticons(text):\n    return emoticon_pattern.sub(\"\", text)\n\ndef chat_words_conversion(text):\n    words_to_replace = set(text.split()).intersection(chat_words_set)\n    return \" \".join(chat_words_map_dict.get(word, word) for word in text.split())\n                               \ndef replace_emoji_with_word(text):\n    return emoji.demojize(text)\n\ndef clear_text(text):\n    return chat_words_conversion(remove_punctuation(replace_emoji_with_word(replace_emoticons(remove_nums(remove_dates(remove_html(text)))))))","e784be65":"#cleared_big_data = big_data.copy()\n#cleared_big_data[\"comment_text\"] = big_data[\"comment_text\"].progress_apply(clear_text)\n#cleared_big_data.to_csv(\"cleared_data.csv\")","a1f4d35d":"#lemmantized_big_data = cleared_big_data.copy()\n#result = nlp.pipe(lemmantized_big_data[\"comment_text\"], batch_size=32, n_process=4, disable=[\"parser\", \"ner\"])\n#lemmantized_big_data[\"comment_text\"] = [\" \".join([tok.lemma_ for tok in doc]) for doc in result]\n#lemmantized_big_data.to_csv(\"lemmantized_cleared_data.csv\")","267ed5ad":"#X_several_language_data = np.concatenate([\n#    training_data_es[\"comment_text\"].astype(str), training_data_fr[\"comment_text\"].astype(str), training_data_it[\"comment_text\"].astype(str),\n#    training_data_pt[\"comment_text\"].astype(str), training_data_ru[\"comment_text\"].astype(str), training_data_tr[\"comment_text\"].astype(str),\n#])\n#y_several_language_data = np.concatenate([\n#    training_data_es[\"toxic\"], training_data_fr[\"toxic\"], training_data_it[\"toxic\"],\n#    training_data_pt[\"toxic\"], training_data_ru[\"toxic\"], training_data_tr[\"toxic\"],\n#])","0bd0eaee":"cleared_big_data = pd.read_csv(\"..\/input\/jigsaw-preprocessed\/cleared_data.csv\")\nlemmantized_big_data = pd.read_csv(\"..\/input\/jigsaw-preprocessed\/lemmantized_cleared_data.csv\", index_col=\"id\")\nvalidation_data = pd.read_csv(\"..\/input\/jigsaw-preprocessed\/valid_translated_lemmantized.csv\", index_col=\"id\")\ntest_data = pd.read_csv(\"..\/input\/jigsaw-preprocessed\/test_translated_lemmantized.csv\", index_col=\"id\")","3ee70f14":"#X_very_big = np.concatenate([X_several_language_data, cleared_big_data[\"comment_text\"].to_numpy()])\n#y_very_big = np.concatenate([y_several_language_data, cleared_big_data[\"toxic\"].to_numpy()])","dd9063f3":"#translated_valid_data[\"comment_text\"] = translated_valid_data[\"comment_text\"].progress_apply(lambda x: expand_contractions(x.lower()))\n#translated_valid_data[\"comment_text\"] = translated_valid_data[\"comment_text\"].progress_apply(lambda x: ' '.join([word for word in x.split() if word not in (stops)]))\n\n#translated_test_data[\"content\"] = translated_test_data[\"content\"].progress_apply(lambda x: expand_contractions(x.lower()))\n#translated_test_data[\"content\"] = translated_test_data[\"content\"].progress_apply(lambda x: ' '.join([word for word in x.split() if word not in (stops)]))","0ef2e043":"#result = nlp.pipe(translated_valid_data[\"translated\"], batch_size=32, n_process=4, disable=[\"parser\", \"ner\"])\n#translated_valid_data[\"translated\"] = [\" \".join([tok.lemma_ for tok in doc]) for doc in result]\n\n#result = nlp.pipe(translated_test_data[\"translated\"], batch_size=32, n_process=4, disable=[\"parser\", \"ner\"])\n#translated_test_data[\"translated\"] = [\" \".join([tok.lemma_ for tok in doc]) for doc in result]\n\n#translated_valid_data.to_csv(\"valid_translated_lemmantized.csv\")\n#translated_test_data.to_csv(\"test_translated_lemmantized.csv\")","515a7af4":"lemmantized_big_data = lemmantized_big_data.drop(\"Unnamed: 0\", axis=1)\nlemmantized_big_data = lemmantized_big_data.dropna()","4ea6e7a3":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","cf5387b6":"AUTO = tf.data.experimental.AUTOTUNE\nEPOCHS = 1\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync\nMAX_LENGTH = 128","7cd4365b":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\nfrom transformers import DataCollatorWithPadding\nfrom transformers import create_optimizer\n# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')","e7385e52":"def tokenize(dataset):\n    return tokenizer(dataset, padding='max_length', max_length=MAX_LENGTH, \n                     truncation=True, return_tensors=\"tf\")","29308f62":"import gc\ngc.collect()","5876ee53":"x_train = tokenize(lemmantized_big_data[\"comment_text\"].astype(str).tolist())\nx_valid = tokenize(validation_data[\"translated\"].astype(str).to_list())\nx_test = tokenize(test_data[\"translated\"].astype(str).to_list())\n\ny_train = lemmantized_big_data[\"toxic\"]\ny_valid = validation_data[\"toxic\"]\ny_test = test_labels[\"toxic\"]","217a0350":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((dict(x_train), y_train))\n    .shuffle(10000, seed=42)\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .cache()\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((dict(x_valid), y_valid))\n    .batch(BATCH_SIZE, drop_remainder=True)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(dict(x_test))\n    .batch(BATCH_SIZE)\n)","b2044dee":"def build_model_new():\n    checkpoint = \"bert-base-multilingual-uncased\"\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    metrics = [tf.keras.metrics.AUC(name=\"AUC\")]\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-5, 1000, 0.98)\n    optimizer = Adam(learning_rate=learning_rate)\n    \n    input_ids_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='input_ids', dtype='int32')\n    input_masks_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='attention_mask', dtype='int32') \n    input_token_types_in = tf.keras.layers.Input(shape=(MAX_LENGTH,), name='token_type_ids', dtype='int32') \n    \n    bert_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n    \n    embedding_layer = bert_model(input_ids_in, token_type_ids=input_token_types_in, attention_mask=input_masks_in)[0]\n    cls_token = embedding_layer[:, 0, :]\n    \n    X = tf.keras.layers.Dense(50, activation='relu')(cls_token)\n    X = tf.keras.layers.Dropout(0.2)(X)\n    X = tf.keras.layers.Dense(4, activation='sigmoid')(X)\n    \n    model = tf.keras.Model(inputs=[input_ids_in, input_masks_in, input_token_types_in], outputs = X)\n    model.compile(optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\ndef build_model():\n    checkpoint = \"bert-base-multilingual-uncased\"\n    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n    metrics = [tf.keras.metrics.AUC(name=\"AUC\")]\n    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-5, 1000, 0.98)\n    optimizer = Adam(learning_rate=learning_rate)\n    \n    model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=1)\n    model.compile(optimizer, loss=loss, metrics=metrics)\n    \n    return model","6cb90740":"%%time\nwith strategy.scope():\n    model = build_model()\nmodel.summary()","ac4a4146":"pred = model(tokenize([\"What the hell\", \"Really? Are you dumb?\"]))\nlogits = pred.logits\ntf.math.sigmoid(logits)","d33fb065":"train_history_1 = model.fit(train_dataset, epochs=1, validation_data=valid_dataset, batch_size=BATCH_SIZE)\n#train_history_1 = model.fit(x_train_ids, np.stack([y_train == 0, y_train == 1], axis=1).astype(int), \n#                            epochs=EPOCHS, \n#                            validation_data=(x_valid_ids, np.stack([y_valid == 0, y_valid == 1], axis=1).astype(int)), \n#                            batch_size=BATCH_SIZE)\n#train_history_1 = model.fit(train_dataset, epochs=EPOCHS, validation_data=valid_dataset, batch_size=BATCH_SIZE)","103be24f":"train_history_2 = model.fit(valid_dataset, epochs=2, batch_size=BATCH_SIZE)","10284939":"bert_pred = model.predict(dict(x_test))\nlogits = bert_pred.logits\nprob = tf.math.sigmoid(logits)","fc31fe97":"print(f\"ROC AUC: {roc_auc_score(y_test, prob)}\")","353d0479":"<font size=4>Auxiliary functions<\/font>","f9ab8566":"<font size=4>Data Preprocessing<\/font>","3a8701cb":"<font size=4>Imports<\/font>","c8008d2e":"<font size=4>Declare dictionaries with emoticons and abbreviation<\/font>","37667337":"<font size=4>BERT<\/font>","30c012b9":"<font size=4>Load preprocessed data<\/font>","5e9a423e":"<font size=4>Load all data<\/font>","1c9566b0":"# <font size=4>Set up notebook<\/font>","e6bf5382":"<font size=4>Check for packages update<\/font>","70768328":"<font size=4>Set up TPU<\/font>"}}