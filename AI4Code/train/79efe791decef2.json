{"cell_type":{"1ca4e21f":"code","918956f0":"code","2a852e3d":"code","c317d652":"code","26aa7fc3":"code","50178d79":"code","e41d5f5f":"code","1b995c63":"code","19a8c30c":"code","389885cd":"code","18aa1f8e":"code","1a58dcef":"code","e6eeedc1":"code","d545877d":"code","778e90ad":"code","12ccc8aa":"code","24b68b4f":"code","f73b2a3e":"code","7d8c7620":"code","2722d98e":"code","2d755588":"code","c4fb5cb5":"code","8817670f":"code","64726f32":"code","95db1ae1":"code","7633fd9f":"code","b8e1bdd4":"code","fc2deb3a":"code","df7d77ee":"code","df4be253":"code","aebb7466":"code","5a08834b":"code","a90d02b5":"code","4ec18d96":"code","4b445abf":"code","f01e9682":"code","87bf6d29":"code","03946002":"code","6d110a19":"code","f0f02836":"code","5098bba2":"code","c2ee428f":"code","33796c6a":"code","014dd1c3":"code","9ea5bdc0":"code","510df7d5":"code","b81609ca":"code","0a248bce":"code","3f44a94c":"code","bcaafdb2":"code","4bc9103c":"code","ba2764a8":"code","dc565074":"code","1bdb26a2":"code","d5567fab":"code","c2b74bfb":"code","8fd93927":"code","3bb1ee4e":"code","1ab50104":"code","365aedf1":"code","44ae5360":"code","12492598":"markdown","e42d87e4":"markdown","8a053b19":"markdown","c8b314d0":"markdown","1c0630b2":"markdown","3b5d012c":"markdown","158628a0":"markdown","dd8cb4cb":"markdown","3e263820":"markdown","d1009ff0":"markdown","80ec1bf4":"markdown","f910c5e8":"markdown","a1ad0567":"markdown","10ab36c9":"markdown","a5bf434e":"markdown","74308af9":"markdown"},"source":{"1ca4e21f":"#Importing Required Library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#SMOTE to balance the Imbalance Data\nfrom imblearn.over_sampling import SMOTE\n\n#for Spliting Data and Hyperparameter Tuning \nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n#Importing Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\n\n#Bagging Algo\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n#To tranform data\nfrom sklearn import preprocessing\n\n#statistical Tools\nfrom sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, plot_confusion_matrix\n\n#Setting Format\npd.options.display.float_format = '{:.5f}'.format\npd.options.display.max_columns = None\npd.options.display.max_rows = None","918956f0":"data = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","2a852e3d":"data.shape","c317d652":"data.head()","26aa7fc3":"data.describe(include='all').T","50178d79":"(data['Churn'].value_counts()\/data.shape[0]).plot(kind='bar')","e41d5f5f":"#Lets Convert Churn column into Numerical data\ndata['Churn'] = data['Churn'].map({'Yes':1, 'No':0})","1b995c63":"data.head()","19a8c30c":"data.info()","389885cd":"data['Churn'] = data['Churn'].astype('int')","18aa1f8e":"data.isna().sum()","1a58dcef":"data[data==0].sum()","e6eeedc1":"for i in data.columns:\n    print(i,'(', data[i].dtype, ')' ,\": Distinct Values\")\n    print(data[i].nunique(), \"Total Unique Values\")\n    print(data.shape[0], \"Total Values\")\n    print(data[i].unique())\n    print(\"-\"*30)\n    print(\"\")","d545877d":"# Senior Citizen should be int dtype lets change it to object\n#This Columns means whether or not customer is Senior so it should be Object Type \ndata['SeniorCitizen'] = data['SeniorCitizen'].astype(object)","778e90ad":"#Total Charges should be numeric not object\n\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')","12ccc8aa":"data.info()","24b68b4f":"data.isna().sum()","f73b2a3e":"data.head()","7d8c7620":"data[data.isna().any(axis=1)]","2722d98e":"df = data[data.isna().any(axis=1)]\ndf.shape","2d755588":"df['TotalCharges'] = df['MonthlyCharges']","c4fb5cb5":"df.head()","8817670f":"data = pd.concat([data,df],ignore_index=True)","64726f32":"data.dropna(inplace=True)\ndata.shape","95db1ae1":"sns.countplot(x=data['SeniorCitizen'])","7633fd9f":"for i in data.select_dtypes(include='O'):\n    sns.countplot(data[i])\n    plt.xticks(rotation = 90)\n    plt.show()","b8e1bdd4":"sns.pairplot(data)","fc2deb3a":"for i in data.select_dtypes(exclude='O'):\n    sns.distplot(data[i], bins=10)\n    plt.show()","df7d77ee":"data['tenure'] = np.log1p(data['tenure'])\ndata['MonthlyCharges'] = np.log1p(data['MonthlyCharges'])\ndata['TotalCharges'] = np.log1p(data['TotalCharges'])","df4be253":"for i in data.select_dtypes(exclude='O'):\n    sns.distplot(data[i], bins=10)\n    plt.show()","aebb7466":"data.head()","5a08834b":"data.drop(['customerID'], axis=1, inplace=True)","a90d02b5":"cat_col = data.select_dtypes(include=\"O\").columns\ncat_col","4ec18d96":"df = data.copy()","4b445abf":"df = pd.get_dummies(df)","f01e9682":"df.head()","87bf6d29":"plt.figure(figsize=(24,12))\nsns.heatmap(df.corr())","03946002":"df.corr()['Churn'].sort_values(ascending = False)","6d110a19":"X = df.drop(['Churn'], axis=1)\ny = df['Churn']","f0f02836":"sm = SMOTE(random_state=50)","5098bba2":"X_tf,y_tf = sm.fit_resample(X,y)","c2ee428f":"scaler = preprocessing.RobustScaler()\ndf_x = scaler.fit_transform(X_tf)","33796c6a":"# Using Skicit-learn to split data into training and testing sets \n# Split the data into training and testing sets \nx_train,x_test,y_train,y_test = train_test_split(df_x,y_tf,test_size=.2, random_state = 100)","014dd1c3":"lr = LogisticRegression(C=5.0)\nknn = KNeighborsClassifier(weights='distance', algorithm='auto', n_neighbors=15)\nrfc = RandomForestClassifier(n_estimators=200,criterion='gini', n_jobs=-1)\ndtc = DecisionTreeClassifier()\nbnb = BernoulliNB()\nxgb = XGBClassifier(n_jobs=-1)\ncat = CatBoostClassifier(verbose=0)\nada = AdaBoostClassifier()\ngbc = GradientBoostingClassifier()\nsvc = svm.SVC(kernel = 'poly', C=4, gamma='scale', degree = 2)","9ea5bdc0":"def train_model(model):\n    # Checking accuracy\n    model = model.fit(x_train, y_train)\n    pred = model.predict(x_test)\n    print('accuracy_score',accuracy_score(y_test, pred)*100)\n    print('precision_score',precision_score(y_test, pred)*100)\n    print('recall_score',recall_score(y_test, pred)*100)\n    print('f1_score',f1_score(y_test, pred)*100)\n    print('roc_auc_score',roc_auc_score(y_test, pred)*100)\n    # confusion matrix\n    print('confusion_matrix')\n    print(pd.DataFrame(confusion_matrix(y_test, pred)))\n    fpr, tpr, threshold = roc_curve(y_test, pred)\n    roc_auc = auc(fpr, tpr)*100\n\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","510df7d5":"train_model(lr)","b81609ca":"train_model(dtc)","0a248bce":"train_model(rfc)","3f44a94c":"train_model(xgb)","bcaafdb2":"train_model(ada)","4bc9103c":"train_model(gbc)","ba2764a8":"train_model(cat)","dc565074":"train_model(bnb)","1bdb26a2":"train_model(knn)","d5567fab":"mlp = MLPClassifier()\ntrain_model(mlp)","c2b74bfb":"train_model(svc)","8fd93927":"# Predicted values\ny_head_lr = lr.predict(x_test)\ny_head_rfc = rfc.predict(x_test)\ny_head_xgb = xgb.predict(x_test)\ny_head_ada = ada.predict(x_test)\ny_head_dtc = dtc.predict(x_test)\ny_head_gbc = gbc.predict(x_test)\ny_head_cat = cat.predict(x_test)\ny_head_knn = knn.predict(x_test)\ny_head_nb = bnb.predict(x_test)\ny_head_mlp = mlp.predict(x_test)\ny_head_svm = svc.predict(x_test)","3bb1ee4e":"\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_rfc = confusion_matrix(y_test,y_head_rfc)\ncm_xgb = confusion_matrix(y_test,y_head_xgb)\ncm_ada = confusion_matrix(y_test,y_head_ada)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_gbc = confusion_matrix(y_test,y_head_gbc)\ncm_cat = confusion_matrix(y_test,y_head_cat)\n\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_mlp = confusion_matrix(y_test,y_head_mlp)\ncm_svm = confusion_matrix(y_test,y_head_svm)","1ab50104":"plt.figure(figsize=(30,20))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(4,3,5)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,6)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,2)\nplt.title(\"XGB Confusion Matrix\")\nsns.heatmap(cm_xgb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\nplt.subplot(4,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,11)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,1)\nplt.title(\"Random Forest Gini Confusion Matrix\")\nsns.heatmap(cm_rfc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,7)\nplt.title(\"CatBooost Confusion Matrix\")\nsns.heatmap(cm_cat,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\nplt.subplot(4,3,8)\nplt.title(\"Ada Boost Confusion Matrix\")\nsns.heatmap(cm_ada,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,9)\nplt.title(\"Gradient boost Classifier Confusion Matrix\")\nsns.heatmap(cm_gbc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,10)\nplt.title(\"MLP CLassifier Confusion Matrix\")\nsns.heatmap(cm_mlp,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(4,3,3)\nplt.title(\"Support Vector CLassifier Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","365aedf1":"'''\nmax_feature = ['auto', 'sqrt']\n\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [1, 2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\ngrid = GridSearchCV(rfc, random_grid, cv=3, verbose=True, n_jobs=-1)\ngrid.fit(x_train, y_train)'''","44ae5360":"rfc_1 = RandomForestClassifier(bootstrap=False, max_depth=90, max_features='auto',\n                              min_samples_leaf=2, min_samples_split=5, n_estimators=600, )\n\ntrain_model(rfc_1)","12492598":"# Let's Transform Our data","e42d87e4":"# Data Description","8a053b19":"We will use Random Forest Classifer as it is giving Good Accuracy and it gives less error rate as it has 171 False Positive and 123 False Negative which is less than all other Alogorithms","c8b314d0":"# Spliting our data into Train and Validation set","1c0630b2":"All the data with nan value does not leave our company, are dependents,they are not senior citizens and doesn't have any tenure.\n\nLets fill them with Monthly Charges","3b5d012c":"Now after converting TotalCharges to Numerical value we now have 11 Nan Values lets dive into it","158628a0":"# Importing all the Libraries","dd8cb4cb":"# Confusion Matrix's","3e263820":"# Let check for missing values or nan","d1009ff0":"Lets drop customerID as it will always be Unique and Doesn't add any value to our Model","80ec1bf4":"# Importing ML Models","f910c5e8":"# HyperParameter Tuning","a1ad0567":"# Importing Data","10ab36c9":"# Lets get some Insight in our data ","a5bf434e":"# Visualization","74308af9":"# Let's balance our Imbalance Dataset using SMOTE"}}