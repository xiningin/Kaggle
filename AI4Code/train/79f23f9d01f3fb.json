{"cell_type":{"4854a404":"code","ee845066":"code","1a1186f1":"code","57eb760b":"code","6e9e68cb":"code","23bdc376":"code","1e068e7a":"code","498849b4":"code","d0be6f60":"code","d11e7e24":"code","f01a1035":"code","b4e2d19b":"code","33f82964":"code","f83ca850":"code","cd61f5b5":"code","bf257b9e":"code","ceb08bdd":"code","512ea865":"code","5b6e29a5":"code","e8e4af48":"code","c5e218c1":"code","fa1f70f1":"code","ac46dd46":"code","694b6408":"code","1ab2022a":"code","3b67196b":"code","fa655173":"code","484d3f8f":"code","f8a8de7d":"code","a1f32071":"code","82162ecd":"code","cb4e3da8":"code","f242a5e7":"code","9726f9fc":"code","0b29fc81":"code","16d0b03f":"code","09b8327b":"code","6f5e14fd":"code","b10bf3bc":"code","24c188bf":"code","454cb14b":"code","ef3f8613":"code","88405bca":"code","4a3f7180":"code","5d8a62ae":"code","50896d05":"markdown","524a69e5":"markdown","cebe2771":"markdown","693c7e86":"markdown","84f7aca1":"markdown","dbbcdd83":"markdown","e66abfe7":"markdown","c454d340":"markdown","1115fc70":"markdown","0df7c455":"markdown","f6fbae87":"markdown","f7f1cf6c":"markdown","21c02466":"markdown"},"source":{"4854a404":"# Display figure in the notebook\n%matplotlib inline","ee845066":"# Load packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","1a1186f1":"# Define some functions\ndef plot_mnist(data, index, label=None):\n    \"\"\"Plot one image from the mnist dataset.\"\"\"\n    fig = plt.figure(figsize=(3, 3))\n    if type(data) == pd.DataFrame:\n        plt.imshow(np.asarray(data.iloc[index, 1:]).reshape((HEIGHT, WIDTH)),\n                   cmap=plt.cm.gray_r,\n                   interpolation='nearest')\n        plt.title(f\"Image label: {data.loc[index, 'label']}\")\n    else:\n        plt.imshow(data[index].reshape((HEIGHT, WIDTH)),\n                   cmap=plt.cm.gray_r,\n                   interpolation='nearest')\n        plt.title(f\"Image label: {label}\")\n    \n    plt.axis('off')\n    return fig","57eb760b":"# Define a constant tensor\na = tf.constant(3)\na","6e9e68cb":"# Define a Variable tensor\nc = tf.Variable(0)\n\nb = tf.constant(2)\n\n# Sum of two tensors\nc = a + b\nc","23bdc376":"# Define a constant tensor in 2 dimensions\nA = tf.constant([[0, 1], [2, 3]], dtype=tf.float32)\nA","1e068e7a":"# Convert tf.Tensor as numpy array\nA.numpy()","498849b4":"# Define a Variable tensor\nb = tf.Variable([1, 2], dtype=tf.float32)\nb","d0be6f60":"# Reshape a Variable tensor\ntf.reshape(b, (-1, 1))","d11e7e24":"# Perform matrix multiplication\ntf.matmul(A, tf.reshape(b, (-1, 1)))","f01a1035":"def squared_norm(x):\n    return tf.reduce_sum(x ** 2)","b4e2d19b":"x = tf.Variable([1, -4], dtype=tf.float32)\nx","33f82964":"squared_norm(x)","f83ca850":"squared_norm(x).numpy()","cd61f5b5":"with tf.GradientTape() as tape:\n    result = squared_norm(x)\n    \nvariables = [x]\ngradients = tape.gradient(result, variables)\ngradients","bf257b9e":"x.assign_sub(0.1 * gradients[0])\nx.numpy()","ceb08bdd":"with tf.GradientTape() as tape:\n    objective = squared_norm(x)\n\nx.assign_sub(0.1 * tape.gradient(objective, [x])[0])\nprint(f\"Objective = {objective.numpy():e}\")\nprint(f\"x = {x.numpy()}\")","512ea865":"# On CPU\nwith tf.device(\"CPU:0\"):\n    x_cpu = tf.constant(3)\nx_cpu.device\n\n# On GPU\n#with tf.device(\"GPU:0\"):\n#    x_gpu = tf.constant(3)\n#x_gpu.device","5b6e29a5":"# Load the data\ndigits_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndigits_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","e8e4af48":"# Define some global parameters\nHEIGHT = 28 # Height of an image\nWIDTH = 28 # Width of an image\nPIXEL_NUMBER = 784 # Number of pixels in an image\nPIXEL_VALUE = 255 # Maximum pixel value in an image","c5e218c1":"# Print an image\nsample_index = 42\n\nplot_mnist(digits_train, sample_index)\nplt.show()","fa1f70f1":"# Extract and convert the pixel as numpy array with dtype='float32'\ntrain = np.asarray(digits_train.iloc[:, 1:], dtype='float32')\ntest = np.asarray(digits_test, dtype='float32')\n\ntrain_target = np.asarray(digits_train.loc[:, 'label'], dtype='int32')","ac46dd46":"# Split and scale the data\nX_train, X_val, y_train, y_val = train_test_split(\n    train, train_target, test_size=0.15, random_state=42)\n\nscaler = preprocessing.StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\nX_test = scaler.transform(test)","694b6408":"def gen_dataset(x, y, batch_size=128):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y)) # Create the dataset\n    dataset = dataset.shuffle(buffer_size=10000, seed=42) # Shuffle the dataset\n    dataset = dataset.batch(batch_size=batch_size) # Combine consecutive elements of dataset into batches.\n    return dataset","1ab2022a":"# Create the dataset\ndataset = gen_dataset(X_train, y_train)\ndataset","3b67196b":"# Get the first batch\nbatch_x, batch_y = next(iter(dataset))\nprint(f\"Size batch_x: {batch_x.shape} \/ Size batch_y: {batch_y.shape}\")","fa655173":"# Helper function\ndef init_weights(shape):\n    return tf.Variable(tf.random.normal(shape, stddev=0.01))\n\ndef accuracy(y_pred, y):\n    return np.mean(np.argmax(y_pred, axis=1) == y)\n\ndef test_model(model, x, y):\n    dataset = gen_dataset(x, y)\n    preds, targets = [], []\n    \n    for batch_x, batch_y in dataset:\n        preds.append(model(batch_x).numpy())\n        targets.append(batch_y.numpy())\n        \n    preds, targets = np.concatenate(preds), np.concatenate(targets)\n    return accuracy(preds, targets)","484d3f8f":"# Hyperparameters\nBATCH_SIZE = 32\nHIDDEN_SIZE = 15\nLEARNING_RATE = 0.5\nNUM_EPOCHS = 10\nINPUT_SIZE = X_train.shape[1]\nOUTPUT_SIZE = 10\nLAMBDA = 10e-4\nGAMMA = 0.9\n\n# Build the model\nclass MyModel:\n    def __init__(self, input_size, hidden_size, output_size):\n        self.W_h = init_weights(shape=[input_size, hidden_size])\n        self.b_h = init_weights([hidden_size])\n        self.W_o = init_weights(shape=[hidden_size, output_size])\n        self.b_o = init_weights([output_size])\n        \n    def __call__(self, inputs):\n        h = tf.sigmoid(tf.matmul(inputs, self.W_h) + self.b_h)\n        return tf.matmul(h, self.W_o) + self.b_o","f8a8de7d":"# Define the model\nmodel = MyModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)","a1f32071":"# Run the model on the validation set\nprint(f\"Accuracy on the validation set on the untrained model: {test_model(model, X_val, y_val)}\")","82162ecd":"losses = []\ntrain_acc = [test_model(model, X_train, y_train)]\ntest_acc = [test_model(model, X_val, y_val)]\n# Loop over the epochs\nfor e in range(NUM_EPOCHS):\n    train_dataset = gen_dataset(X_train, y_train, batch_size=BATCH_SIZE)\n    \n    # Loop over the batches\n    for batch_x, batch_y in train_dataset:\n        # tf.GradientTape records the activation to compute the gradients.\n        with tf.GradientTape() as tape:\n            logits = model(batch_x)\n            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits))\n            losses.append(loss.numpy())\n        \n        # Compute the gradient of the loss with respect to W_h, b_h, W_o and b_o\n        dW_h, db_h, dW_o, db_o = tape.gradient(\n            loss, [model.W_h, model.b_h, model.W_o, model.b_o])\n        \n        # Update the weights as a SGB would do\n        model.W_h.assign_sub(LEARNING_RATE * dW_h)\n        model.b_h.assign_sub(LEARNING_RATE * db_h)\n        model.W_o.assign_sub(LEARNING_RATE * dW_o)\n        model.b_o.assign_sub(LEARNING_RATE * db_o)\n    \n    train_acc_e = test_model(model, X_train, y_train)\n    test_acc_e = test_model(model, X_val, y_val)\n    train_acc.append(train_acc_e)\n    test_acc.append(test_acc_e)\n    print(f\"Epoch {e}: train accuracy = {round(train_acc_e, 4)}, test accuracy = {round(test_acc_e, 4)}\")","cb4e3da8":"# Plot of the losses\nplt.plot(losses)\nplt.show()","f242a5e7":"# Plot of the accuracy\nfig, ax = plt.subplots()\nax.plot(train_acc, label='Train')\nax.plot(test_acc, label='Test')\nax.legend()\nplt.show()","9726f9fc":"# Define the model\nmodel = MyModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)","0b29fc81":"# Run the model on the validation set\nprint(f\"Accuracy on the validation set on the untrained model: {test_model(model, X_val, y_val)}\")","16d0b03f":"losses = []\ntrain_acc = [test_model(model, X_train, y_train)]\ntest_acc = [test_model(model, X_val, y_val)]\n# Loop over the epochs\nfor e in range(NUM_EPOCHS):\n    train_dataset = gen_dataset(X_train, y_train, batch_size=BATCH_SIZE)\n    \n    # Loop over the batches\n    for batch_x, batch_y in train_dataset:\n        # tf.GradientTape records the activation to compute the gradients.\n        with tf.GradientTape() as tape:\n            logits = model(batch_x)\n            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits)) +\\\n                    LAMBDA * (tf.nn.l2_loss(model.W_h) + tf.nn.l2_loss(model.W_o))\n            losses.append(loss.numpy())\n        \n        # Compute the gradient of the loss with respect to W_h, b_h, W_o and b_o\n        dW_h, db_h, dW_o, db_o = tape.gradient(\n            loss, [model.W_h, model.b_h, model.W_o, model.b_o])\n        \n        # Update the weights as a SGB would do\n        model.W_h.assign_sub(LEARNING_RATE * dW_h)\n        model.b_h.assign_sub(LEARNING_RATE * db_h)\n        model.W_o.assign_sub(LEARNING_RATE * dW_o)\n        model.b_o.assign_sub(LEARNING_RATE * db_o)\n    \n    train_acc_e = test_model(model, X_train, y_train)\n    test_acc_e = test_model(model, X_val, y_val)\n    train_acc.append(train_acc_e)\n    test_acc.append(test_acc_e)\n    print(f\"Epoch {e}: train accuracy = {round(train_acc_e, 4)}, test accuracy = {round(test_acc_e, 4)}\")","09b8327b":"# Plot of the losses\nplt.plot(losses)\nplt.show()","6f5e14fd":"# Plot of the accuracy\nfig, ax = plt.subplots()\nax.plot(train_acc, label='Train')\nax.plot(test_acc, label='Test')\nax.legend()\nplt.show()","b10bf3bc":"# Do prediction on the test set\npred = np.argmax(model(X_test).numpy(), axis=1)","24c188bf":"# Submision file\nsub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsub.loc[:,'Label'] = pred\n\nsub.to_csv('submission.csv', index=False)","454cb14b":"# Define the model\nmodel = MyModel(INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)","ef3f8613":"# Run the model on the validation set\nprint(f\"Accuracy on the validation set on the untrained model: {test_model(model, X_val, y_val)}\")","88405bca":"losses = []\ntrain_acc = [test_model(model, X_train, y_train)]\ntest_acc = [test_model(model, X_val, y_val)]\n\n# Define the momentum\nm_W_h = np.zeros((INPUT_SIZE, HIDDEN_SIZE))\nm_W_o = np.zeros((HIDDEN_SIZE, OUTPUT_SIZE))\n# Loop over the epochs\nfor e in range(NUM_EPOCHS):\n    train_dataset = gen_dataset(X_train, y_train, batch_size=BATCH_SIZE)\n\n    # Loop over the batches\n    for batch_x, batch_y in train_dataset:\n        # tf.GradientTape records the activation to compute the gradients.\n        with tf.GradientTape() as tape:\n            logits = model(batch_x)\n            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(batch_y, logits)) +\\\n                    LAMBDA * (tf.nn.l2_loss(model.W_h) + tf.nn.l2_loss(model.W_o))\n            losses.append(loss.numpy())\n        \n        # Compute the gradient of the loss with respect to W_h, b_h, W_o and b_o\n        dW_h, db_h, dW_o, db_o = tape.gradient(\n            loss, [model.W_h, model.b_h, model.W_o, model.b_o])\n        \n        # Update the momentum\n        m_W_h = GAMMA * m_W_h + LEARNING_RATE * dW_h\n        m_W_o = GAMMA * m_W_o + LEARNING_RATE * dW_o\n        \n        # Update the weights as a SGB would do\n        model.W_h.assign_sub(m_W_h)\n        model.b_h.assign_sub(LEARNING_RATE * db_h)\n        model.W_o.assign_sub(m_W_o)\n        model.b_o.assign_sub(LEARNING_RATE * db_o)\n    \n    train_acc_e = test_model(model, X_train, y_train)\n    test_acc_e = test_model(model, X_val, y_val)\n    train_acc.append(train_acc_e)\n    test_acc.append(test_acc_e)\n    print(f\"Epoch {e}: train accuracy = {round(train_acc_e, 4)}, test accuracy = {round(test_acc_e, 4)}\")","4a3f7180":"# Plot of the losses\nplt.plot(losses)\nplt.show()","5d8a62ae":"# Plot of the accuracy\nfig, ax = plt.subplots()\nax.plot(train_acc, label='Train')\nax.plot(test_acc, label='Test')\nax.legend()\nplt.show()","50896d05":"The following implements a training loop in Python. Note the use of `tf.GradientTape` to automatically compute the gradients of the loss with respect to the different parameters of the model.","524a69e5":"`Tensorflow` provides dataset abstraction which makes possible to iterate over the data batch by batch.","cebe2771":"# Multilayer Neural Network in Tensorflow\n\nThis notebook is based on the Deep Learning course from the Master Datascience Paris Saclay. Materials of the course can be found [here](https:\/\/github.com\/m2dsupsdlclass\/lectures-labs).\n\n### Goal of the notebook\n\n* Introduce the basics of `Tensorflow`.\n* Computation  of auto-differentiation with `Tensorflow`.\n* Implement the digit classifier using the low level `Tensorflow` API without Keras abstraction.\n\n## Introduction to Tensorflow\n\n`Tensorflow` is a dynamic graph computation engine that allows differentiation of each node. This library is the default computational backend of the `Keras` library. It can also be used directly from Python to build deep learning models. Check out this link:\n* https:\/\/www.tensorflow.org\/\n* https:\/\/www.tensorflow.org\/tutorials\/quickstart\/advanced\n\nIt builds on nodes where nodes may be:\n* **constant**: constants tensors, such as training data;\n* **Variable**: any tensor tht is meant to be updated when training, such as parameters of the models.\n\n**Note:** we are going to use the version 2.0 of `Tensorflow`. This version cleaned the old cluttered API and uses by default dynamic graph of operations to make it natural to design a model interactively in Jupyter. Previously, you defined the graph statically once, and then needed to evaluate it by feeding it some data. Now, it is dynamically defined when executing imperative Python instructions which means that you can `print` any tensor at any moment or even use `pdb.set_trace()` to inspect intermediary values.","693c7e86":"### Build a model using Tensorflow\n\n* Using `Tensorflow`, build a MLP with one hidden layer.\n* The input will be a batch coming from `X_train`, and the output will be a batch of integer.\n* The output do not need be normalized as probabilities, the softmax will be moved to the loss function.","84f7aca1":"* Implementation of the momentum\n\nThis idea of the momentum is to accumulate gradients across successive updates:\n$$m_t = \\gamma m_{t-1} + \\eta \\nabla_{\\theta}L_{B_t}(\\theta_{t-1})$$\n$$\\theta_t = \\theta_{t-1} - m_t$$\n$\\gamma$ is typically set to $0.9$.","dbbcdd83":"### Autodifferentiation and Gradient Descent","e66abfe7":"Write a function that computes the squared Euclidean norm of an 1D tensor input `x`:\n* Use element wise arithmetic operations `(+, -, *, \/, **)`.\n* Use `tf.reduce_sum` to compute the sum of the element of a Tensor.","c454d340":"Execute the following gradient descent step many times consecutively to watch the decrease of the objective function and the values of `x` converging to the minimum of the `squared_norm` function.","1115fc70":"We can apply a gradient step to modify `x` in place by taking one step of gradient descent.","0df7c455":"* Add $L_2$ regularization with $\\lambda = 10^{-4}$.\n\nWith the regularization, the cost function is the negative likelihood of the model computed on the full training set (for i.i.d. samples):\n$$L_S(\\theta) = -\\frac{1}{\\lvert S \\rvert}\\sum_{s \\in S}\\log f(x^s; \\theta)_{y^s} + \\lambda\\Omega(\\theta)$$\nwhere $\\Omega(\\theta) = \\| W^h \\|^2 + \\| W^o \\|^2$.","f6fbae87":"### Preprocessing\n\n* Normalization\n* Train \/ Validation split","f7f1cf6c":"### Device-aware Memory Allocation\n\nTo explicitely place tensors on a device, we use context managers.","21c02466":"## Building a digits classifier in Tensorflow\n\nDataset:\n * The MNIST dataset ([Kaggle link](https:\/\/www.kaggle.com\/c\/digit-recognizer\/overview))"}}