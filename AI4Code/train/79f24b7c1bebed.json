{"cell_type":{"cbbf6e9c":"code","fabe6bd3":"code","de12db72":"code","2d9ddd53":"code","fde2b032":"code","ec9a5751":"code","75971cb9":"code","57e2568f":"code","ae0ef24d":"code","32b3d7d5":"code","e80e5f47":"code","213ffa96":"code","7adcfddf":"code","9c02e546":"code","fa33137c":"code","54c1469a":"code","c50184f1":"code","fda8758f":"code","d6295b65":"code","996e420b":"code","053d4679":"code","4896a65f":"code","bc17f993":"code","067cde9e":"code","7ffc191d":"code","4d78246f":"code","f05cef85":"code","962a86b4":"code","2461a820":"code","58a97d75":"code","da3e118c":"code","1093b089":"code","86c77454":"code","7d5a70e4":"code","5e4e7b2c":"code","1c414ad0":"code","5cc7cd83":"code","1f1a628d":"code","ef95e444":"code","ea690dbe":"code","9bead735":"code","24b655f4":"code","d9d06930":"code","146f5802":"code","699631fa":"code","a621e45f":"code","8d3e4eaa":"code","d89e38b1":"code","ed3655d6":"code","813a31eb":"code","2106629e":"code","8493956a":"code","f1328102":"code","74c02dc7":"code","016385cd":"code","b33d8feb":"code","eb7f1fdf":"code","d4457c02":"code","7e288d35":"code","cb3d6ef2":"code","3aa0965b":"code","09887808":"code","ede8ab0b":"code","05a09aa1":"code","430f2ddd":"code","3a1d3ca9":"code","9a0ae532":"code","af673a29":"code","f39734e6":"code","d1b5e128":"code","99083eba":"markdown","bbe9a9f4":"markdown","76df0978":"markdown","7088e086":"markdown","52b33e04":"markdown","0b5d4499":"markdown","8c14f8f7":"markdown","54667d5b":"markdown","0fdd8f91":"markdown","bc57be9b":"markdown","29bd4c53":"markdown","4ddd4008":"markdown","10c483e1":"markdown","9b73aa28":"markdown","61482c5d":"markdown","9f8bdcf1":"markdown","4d7c9385":"markdown","fbd2f1e1":"markdown","2e030a0f":"markdown","289a1d46":"markdown","0a0e25e9":"markdown"},"source":{"cbbf6e9c":"from typing import Any, Union\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nimport re\nfrom statistics import mode\nfrom pandas import DataFrame, Series\nfrom pandas.io.parsers import TextFileReader\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_selector, make_column_transformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n","fabe6bd3":"# dataset\ndataset = pd.read_csv('..\/input\/titanic\/train.csv')\n\n# submission data\nX_test_submission = pd.read_csv('..\/input\/titanic\/test.csv')\n","de12db72":"dataset.info()","2d9ddd53":"X_test_submission.info()","fde2b032":"dataset.isnull().sum()","ec9a5751":"X_test_submission.isnull().sum()","75971cb9":"dataset.loc[dataset.Age.isnull(), 'Age'] = dataset.groupby(\"Pclass\").Age.transform('median')\ndataset.isnull().sum()","57e2568f":"X_test_submission.loc[X_test_submission.Age.isnull(), 'Age'] = dataset.groupby(\"Pclass\").Age.transform('median')\nX_test_submission.isnull().sum()","ae0ef24d":"dataset['Cabin'] = dataset['Cabin'].fillna('U')\ndataset.isnull().sum()","32b3d7d5":"X_test_submission['Cabin'] = X_test_submission['Cabin'].fillna('U')\nX_test_submission.isnull().sum()","e80e5f47":"dataset.Embarked.value_counts()","213ffa96":"X_test_submission.Embarked.value_counts()","7adcfddf":"dataset['Embarked'] = dataset['Embarked'].fillna(mode(dataset['Embarked']))\ndataset.isnull().sum()","9c02e546":"X_test_submission['Embarked'] = X_test_submission['Embarked'].fillna(mode(X_test_submission['Embarked']))\nX_test_submission['Fare'] = X_test_submission['Fare'].fillna(mode(X_test_submission['Fare']))\n\nX_test_submission.isnull().sum()","fa33137c":"def display_heatmap_na(df):\n    print(df.isnull().sum())\n    plt.style.use('seaborn')\n    plt.figure()\n    sns.heatmap(df.isnull(), yticklabels = False, cmap='plasma')\n    plt.title('Null Values in Training Set')\n\ndisplay_heatmap_na(dataset)\n","54c1469a":"display_heatmap_na(X_test_submission)","c50184f1":"# dataset['Fare']  = dataset.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","fda8758f":"dataset.Cabin.value_counts()","d6295b65":"X_test_submission.Cabin.value_counts()","996e420b":"# dataset['Cabin'] = dataset['Cabin'].fillna('U')","053d4679":"# display_heatmap_na(dataset)","4896a65f":"dataset.Sex.unique()","bc17f993":"X_test_submission.Sex.unique()","067cde9e":"dataset['Sex'][dataset['Sex'] == 'male'] = 0\ndataset['Sex'][dataset['Sex'] == 'female'] = 1","7ffc191d":"X_test_submission['Sex'][X_test_submission['Sex'] == 'male'] = 0\nX_test_submission['Sex'][X_test_submission['Sex'] == 'female'] = 1","4d78246f":"dataset.Embarked.unique()","f05cef85":"X_test_submission.Embarked.unique()","962a86b4":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(dataset[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\ndataset = dataset.join(temp)\ndataset.drop(columns='Embarked', inplace=True)\n\n# display_heatmap_na(dataset)","2461a820":"encoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(X_test_submission[['Embarked']]).toarray(), columns=['S', 'C', 'Q'])\nX_test_submission = X_test_submission.join(temp)\nX_test_submission.drop(columns='Embarked', inplace=True)\n\n# display_heatmap_na(dataset)","58a97d75":"# deleting outliers\ndataset= dataset[dataset['Age'] < 70]\ndataset= dataset[dataset['Fare'] < 500]\n\ndataset.columns","da3e118c":"dataset.Cabin.tolist()[0:20]","1093b089":"X_test_submission.Cabin.tolist()[0:20]","86c77454":"dataset['Cabin'] = dataset['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ndataset.Cabin.unique()","7d5a70e4":"X_test_submission['Cabin'] = X_test_submission['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\nX_test_submission.Cabin.unique()","5e4e7b2c":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ndataset['Cabin'] = dataset['Cabin'].map(cabin_category)\ndataset.Cabin.unique()","1c414ad0":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\nX_test_submission['Cabin'] = X_test_submission['Cabin'].map(cabin_category)\nX_test_submission.Cabin.unique()","5cc7cd83":"dataset.Name","1f1a628d":"X_test_submission.Name","ef95e444":"dataset['Name'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\ndataset['Name'].unique().tolist()","ea690dbe":"X_test_submission['Name'] = X_test_submission.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\nX_test_submission['Name'].unique().tolist()","9bead735":"dataset.rename(columns={'Name' : 'Title'}, inplace=True)\ndataset['Title'] = dataset['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess',\n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n\ndataset['Title'].value_counts(normalize = True) * 100","24b655f4":"X_test_submission.rename(columns={'Name' : 'Title'}, inplace=True)\nX_test_submission['Title'] = X_test_submission['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess',\n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')\n\nX_test_submission['Title'].value_counts(normalize = True) * 100","d9d06930":"dataset = dataset.reset_index(drop=True)\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(dataset[['Title']]).toarray())\ndataset = dataset.join(temp)\ndataset.drop(columns='Title', inplace=True)\n\n# display_heatmap_na(dataset)","146f5802":"X_test_submission = X_test_submission.reset_index(drop=True)\n\nencoder = OneHotEncoder()\ntemp = pd.DataFrame(encoder.fit_transform(X_test_submission[['Title']]).toarray())\nX_test_submission = X_test_submission.join(temp)\nX_test_submission.drop(columns='Title', inplace=True)\n\n# display_heatmap_na(dataset)","699631fa":"dataset['familySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n","a621e45f":"X_test_submission['familySize'] = X_test_submission['SibSp'] + X_test_submission['Parch'] + 1","8d3e4eaa":"fig = plt.figure(figsize = (15,4))\n\nax1 = fig.add_subplot(121)\nax = sns.countplot(dataset['familySize'], ax = ax1)\n\n# calculate passengers for each category\nlabels = (dataset['familySize'].value_counts())\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+6, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n\nplt.title('Passengers distribution by family size')\nplt.ylabel('Number of passengers')\n\nax2 = fig.add_subplot(122)\nd = dataset.groupby('familySize')['Survived'].value_counts(normalize = True).unstack()\nd.plot(kind='bar', color=[\"#3f3e6fd1\", \"#85c6a9\"], stacked='True', ax = ax2)\nplt.title('Proportion of survived\/drowned passengers by family size (train data)')\nplt.legend(( 'Drowned', 'Survived'), loc=(1.04,0))\nplt.xticks(rotation = False)\n\nplt.tight_layout()","d89e38b1":"# drop redundant features\ndataset = dataset.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\n\n# drop unused columns\nsurvived = dataset['Survived']\ndataset = dataset.drop(['PassengerId', 'Survived'], axis = 1)\n\ndataset.head()","ed3655d6":"# drop redundant features\nX_test_submission = X_test_submission.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)\n\n# drop unused columns\nX_test_submission = X_test_submission.drop(['PassengerId'], axis = 1)\n\ndataset.head()","813a31eb":"from sklearn.model_selection import train_test_split, learning_curve\n\nX_train, X_test, y_train, y_test = train_test_split(dataset, survived, test_size = 0.2, random_state=2)\n\n","2106629e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\n\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)","8493956a":"X_test_submission_scaled = scaler.transform(X_test_submission)","f1328102":"from imblearn.over_sampling import SMOTE\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import make_pipeline\n\nX_train_scaled, y_train = SMOTE(k_neighbors=(np.unique(y_train, return_counts=True)[1][1] - 1)).fit_resample(X_train_scaled, y_train)\n","74c02dc7":"def evaluation(model, scoring='accuracy'):\n\n    model.fit(X_train_scaled, y_train)\n    ypred = model.predict(X_test_scaled)\n\n    print(confusion_matrix(y_test, ypred))\n    print(classification_report(y_test, ypred))\n\n    N, train_score, val_score = learning_curve(model,\n                                                X_train_scaled,\n                                                y_train,\n                                                cv=4,\n                                                scoring=scoring,\n                                                train_sizes=np.linspace(0.1, 1, 10))\n\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(N, train_score.mean(axis=1), label='train score')\n    plt.plot(N, val_score.mean(axis=1), label='validation score')\n    plt.legend()","016385cd":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n","b33d8feb":"# delete infinite values\n\ndef assert_all_finite(X):\n    \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n    X = np.asanyarray(X)\n    # First try an O(n) time, O(1) space solution for the common case that\n    # everything is finite; fall back to O(n) space np.isfinite to prevent\n    # false positives from overflow in sum method.\n    if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n            and not np.isfinite(X).all()):\n        raise ValueError(\"Input contains NaN, infinity\"\n                         \" or a value too large for %r.\" % X.dtype)\n\ndef remove_infinite_values(matrix):\n    # find min and max values for each column, ignoring nan, -inf, and inf\n    mins = [np.nanmin(matrix[:, i][matrix[:, i] != -np.inf]) for i in range(matrix.shape[1])]\n    maxs = [np.nanmax(matrix[:, i][matrix[:, i] != np.inf]) for i in range(matrix.shape[1])]\n\n    # go through matrix one column at a time and replace  + and -infinity \n    # with the max or min for that column\n    for i in range(log_train_arr.shape[1]):\n        matrix[:, i][matrix[:, i] == -np.inf] = mins[i]\n        matrix[:, i][matrix[:, i] == np.inf] = maxs[i]\n\n# assert_all_finite(X_train)\n# assert_all_finite(y_train)\n\n# remove_infinite_values(X_train)\n# remove_infinite_values(y_train)","eb7f1fdf":"\nrandom_forest = RandomForestClassifier().fit(X_train_scaled, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(random_forest.score(X_train_scaled, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(random_forest.score(X_test_scaled, y_test)))","d4457c02":"\n\npreprocessor = make_pipeline(PolynomialFeatures(2, include_bias=False), SelectKBest(f_classif, k=10))\n\nrandom_forest_pipeline = make_pipeline(preprocessor, RandomForestClassifier())\n\nevaluation(random_forest_pipeline, scoring='accuracy')","7e288d35":"random_forest_pipeline.get_params()","cb3d6ef2":"\n# Set our parameter grid\nparam_grid = {\n    'randomforestclassifier__criterion' : ['gini', 'entropy'],\n    'randomforestclassifier__n_estimators': [20, 50, 100, 300, 500, 1000],\n    'randomforestclassifier__max_features': ['auto', 'log2', 'sqrt'],\n    'randomforestclassifier__max_depth' : [3, 4, 5, 6, 7, 10],\n    'randomforestclassifier__min_samples_leaf': [5, 8, 10, 20, 50],\n    'randomforestclassifier__min_samples_split': [1, 2, 3, 4, 5, 8, 10],\n    'randomforestclassifier__n_jobs': [5, 8, 10, 20, 50],\n    'pipeline__selectkbest__k': [3, 4, 5, 8, 10],\n    'pipeline__polynomialfeatures__degree': [2, 3, 4, 5, 8]\n}\n\n","3aa0965b":"def randomized_search_cv():\n    # n_iter -> combien de fois l'algorithme va-t-il tester les combinaisons\n    grid = RandomizedSearchCV(random_forest_pipeline, param_grid, scoring='recall', cv=4,\n                          n_iter=60)\n    grid.fit(X_train_scaled, y_train)\n    print(grid.best_params_)\n    y_pred = grid.predict(X_test_scaled)\n    print(classification_report(y_test, y_pred))\n\n    return grid\n\n\n# grid = randomized_search_cv()\n","09887808":"\n# Model Accuracy, how often is the classifier correct?\n# print(\"Accuracy:\", accuracy_score(y_test, y_pred))","ede8ab0b":"# evaluation(grid.best_estimator_, scoring='accuracy')","05a09aa1":"'''\n{'randomforestclassifier__n_jobs': 10, \n'randomforestclassifier__n_estimators': 500, \n'randomforestclassifier__min_samples_split': 2, \n'randomforestclassifier__min_samples_leaf': 8, \n'randomforestclassifier__max_features': 'log2', \n'randomforestclassifier__max_depth': 3, \n'randomforestclassifier__criterion': 'gini', \n'pipeline__selectkbest__k': 3, \n'pipeline__polynomialfeatures__degree': 4}\n'''\n\nrandom_forest = RandomForestClassifier(n_jobs= 10,\n                                        criterion = 'gini',\n                                        max_depth = 3,\n                                        max_features = 'log2',\n                                        min_samples_leaf = 8,\n                                        min_samples_split = 2,\n                                        n_estimators = 500)\n\nrandom_forest.fit(X_train_scaled, y_train)\ny_pred = random_forest.predict(X_test_scaled)\n","430f2ddd":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred) * 100","3a1d3ca9":"evaluation(random_forest, scoring='accuracy')","9a0ae532":"preprocessor = make_pipeline(PolynomialFeatures(4, include_bias=False), SelectKBest(f_classif, k=3))\n\nrandom_forest_pipeline = make_pipeline(preprocessor, RandomForestClassifier())\n\nevaluation(random_forest_pipeline, scoring='accuracy')","af673a29":"model = random_forest.fit(X_train_scaled, y_train)\ny_pred = model.predict(X_test_scaled)\n\nevaluation(model, scoring='f1')","f39734e6":"y_pred_submission = model.predict(X_test_submission_scaled)","d1b5e128":"\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\nsubmission = pd.DataFrame({\n        'PassengerId': gender_submission['PassengerId'],\n        'Survived': y_pred_submission\n    })\n\nsubmission.to_csv('titanic_submission.csv', index=False)\n\nprint('It\\'s done')\n\n\n\n","99083eba":"**We can get the alphabets by running regular expression**","bbe9a9f4":"**So many different values let's place missing values with U as \"Unknown\"**","76df0978":"# Feature Engineering","7088e086":"# Random Forest\n\nSecondly, this part introduces one of the most popular algorithms for classification (but also regression, etc), Random Forest! In a nutshell, Random Forest is an ensembling learning algorithm which combines decision trees in order to increase performance and avoid overfitting.","52b33e04":"## Handle Missing Values and outliers","0b5d4499":"## SMOTE","8c14f8f7":"**Let's encode with OneHotEncoder technique**\n","54667d5b":"### What is in the name?\n\nEach passenger Name value contains the title of the passenger which we can extract and discover.\nTo create new variable \"Title\":\n\n- Using method 'split' by comma to divide Name in two parts and save the second part\n- Splitting saved part by dot and save first part of the result\n- To remove spaces around the title, using 'split' method\n- To visualize, how many passengers hold each title, chosing countplot.","0fdd8f91":"## Deleting outliers","bc57be9b":"**So many different values let's place missing values with U as \"Unknown\"**","29bd4c53":"## Magic Weapon#1: **Let's Scale our data and re-train the model**","4ddd4008":"## Magic Weapon #3: Hyperparameter Tuning\n\nBelow we set the hyperparameter grid of values with 4 lists of values:\n\n- 'criterion' : A function which measures the quality of a split.\n- 'n_estimators' : The number of trees of our random forest.\n- 'max_features' : The number of features to choose when looking for the best way of splitting.\n- 'max_depth' : the maximum depth of a decision tree.","10c483e1":"**Also, corr(Fare, Pclass) is the highest correlation in absolute numbers for 'Fare', so we'll use Pclass again to impute the missing values!**","9b73aa28":"# SUBMITING","61482c5d":"**As maximum values in dataset set is S let's replace the null values by this letter**","9f8bdcf1":"**Hmmm... but we know from part 2 that Sibsp is the number of siblings \/ spouses aboard the Titanic, and Parch is the number of parents \/ children aboard the Titanic... So, what is another straightforward feature to engineer? Yes, it is the size of each family aboard!**","4d7c9385":"# MinMaxScaler","fbd2f1e1":"**Wohh that's lot's of title. So, let's bundle them**","2e030a0f":"**Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class**","289a1d46":"**Sex is categorical data so we can replace male to 0 and femail to 1**","0a0e25e9":"**Better! let's convert to numeric**"}}