{"cell_type":{"4ce4ccc5":"code","b03c34eb":"code","93b1833f":"code","557a0ec7":"code","3738a552":"code","8ff66bd2":"code","afc839b8":"code","8b922708":"code","d73fbd2c":"code","27f41f0c":"code","8f1824e9":"code","49fec699":"code","00912961":"code","3200387a":"code","6df8855f":"code","d92b317f":"code","dff51b2c":"code","14bab4c7":"code","ea490085":"code","a4478c23":"code","407a839c":"code","b44e8454":"code","84f8656c":"code","97958988":"code","8f069c37":"code","ac6f33cd":"code","6c4accec":"code","ac9ef930":"code","e2553107":"code","bb849b26":"code","d2d19378":"code","d634640f":"code","7b60a47c":"code","f904d4cf":"code","4e8c0ebc":"code","13b5cc79":"code","c7ee2d3c":"code","3f6520d6":"code","7a453243":"code","b8e5ec61":"code","a27bc326":"code","f2739a6a":"code","9e47beb3":"code","6d3f1b9f":"code","fa026c22":"code","60071630":"code","b7656338":"code","a44de46b":"code","46a29b27":"code","9c94fa14":"code","31c80221":"code","49996ec4":"markdown","fbd6f25e":"markdown","1be3962f":"markdown","3a1e735a":"markdown","62a9a9a5":"markdown","e12907f7":"markdown"},"source":{"4ce4ccc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b03c34eb":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Dense, Flatten, Dropout, SeparableConv1D\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport librosa\n\nimport soundfile as sf\n\nimport librosa.display\nfrom os import listdir\nfrom os.path import isfile, join\nfrom tensorflow.keras.utils import plot_model,to_categorical\nfrom IPython.display import Audio\nfrom scipy.io import wavfile\nfrom pydub import AudioSegment\nimport IPython\nfrom IPython.display import Audio, Javascript\n\nfrom scipy.io import wavfile\nfrom base64 import b64decode\n\nfrom pydub import AudioSegment\n\n\n","93b1833f":"diagnosis_df = pd.read_csv('..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/patient_diagnosis.csv', names=['Patient ID', 'Diagnosis'])\ndiagnosis_df['Binary_diagnosis'] = diagnosis_df['Diagnosis'].apply(lambda x: 'Healthy' if x =='Healthy'  else 'Unhealthy')\ndiagnosis_df.head()","557a0ec7":"df_no_diagnosis = pd.read_csv('..\/input\/respiratory-sound-database\/demographic_info.txt', names = \n                 ['Patient ID', 'Age', 'Sex' , 'Adult BMI (kg\/m2)', 'Child Weight (kg)' , 'Child Height (cm)'],\n                 delimiter = ' ')\ndf_no_diagnosis.head(5)","3738a552":"df =  df_no_diagnosis.join(diagnosis_df.set_index('Patient ID'), on = 'Patient ID', how = 'left')\ndf.head()","8ff66bd2":"root = '..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/audio_and_txt_files\/'\n\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\n\n\ndef extract_annotation_data(file_name, root):\n    \n    tokens = file_name.split('_')\n    \n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient ID', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    \n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    \n    return (recording_info, recording_annotations)\n\ni_list = []\nrec_annotations = []\nrec_annotations_dict = {}\n\nfor s in filenames:\n    \n    (i,a) = extract_annotation_data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\n    \nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.head()","afc839b8":"len(rec_annotations_dict)","8b922708":"def slice_audio(audiodata,samplerate,start,end):\n\n     start = samplerate * start \n     end   = samplerate * end\n\n     return audiodata[start:end]\n","d73fbd2c":"class Diagnosis():\n    def __init__ (self, id, diagnosis, file_path):\n        self.id = id\n        self.diagnosis = diagnosis \n        self.file_path = file_path","27f41f0c":"def get_wav_files():\n    \n    audio_path = '..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/audio_and_txt_files\/'\n    \n    files = [f for f in listdir(audio_path) if isfile(join(audio_path, f))]  #Gets all files in dir\n    wav_files = [f for f in files if f.endswith('.wav')]  # Gets wav files \n    wav_files = sorted(wav_files)\n    return wav_files, audio_path","8f1824e9":"def diagnosis_data():\n     \n    diagnosis = pd.read_csv('..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/patient_diagnosis.csv')\n  \n    wav_files, audio_path = get_wav_files()\n    diag_dict = { 101 : \"URTI\"}  \n    diagnosis_list = []\n  \n    for index , row in diagnosis.iterrows():\n        diag_dict[row[0]] = row[1]     \n\n    c = 0\n    audio_data_list=[]\n    for f in wav_files:\n        diagnosis_list.append(Diagnosis(c, diag_dict[int(f[:3])], audio_path+f))\n        \n        #wav,s_rate = librosa.load(audio_path+f)\n        \n        \n        if diag_dict[int(f[:3])] == \"Healthy\":\n            \n            binary_classification_label=\"Healthy\"\n            \n        else:\n            \n            binary_classification_label=\"UnHealthy\"\n            \n        \n        audio_data_list.append({'id':c,'diagnosis':diag_dict[int(f[:3])],'binary_diagnosis':binary_classification_label,'filename':f,'file_path':audio_path+f})\n        c+=1  \n\n    return diagnosis_list,pd.DataFrame(audio_data_list)","49fec699":"dataset_obj,dataset_df=diagnosis_data()","00912961":"print(dataset_df['diagnosis'].unique())\nplt.figure(figsize=(10,5))\nsns.countplot(dataset_df['diagnosis'])","3200387a":"dataset_df.to_csv(\"DIagnosis.csv\",index=False)","6df8855f":"processed_dataset_df = dataset_df[ (dataset_df['diagnosis'] !='Asthma') &  (dataset_df['diagnosis'] !='LRTI') ]","d92b317f":"plt.figure(figsize=(10,5))\nsns.countplot(processed_dataset_df['diagnosis'])","dff51b2c":"processed_dataset_df.head()","14bab4c7":"audio_file, samplerate = librosa.core.load(processed_dataset_df.file_path[0])","ea490085":"import tensorflow as tf\nfrom tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nimport os\nfrom scipy.io import wavfile\nimport pandas as pd\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom glob import glob\nimport argparse\nimport warnings\nimport wavio\nfrom librosa.core import resample, to_mono\n\n# Dependencies\nimport numpy as np \nimport pandas as pd\nimport os\nimport librosa\nimport matplotlib.pyplot as plt\nimport gc\nimport time\nfrom tqdm import tqdm, tqdm_notebook; tqdm.pandas() # Progress bar\nfrom sklearn.metrics import label_ranking_average_precision_score\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.layers import Input, LSTM, Dense, TimeDistributed, Activation, BatchNormalization, Dropout, Bidirectional\nfrom keras.models import Sequential\nfrom keras.utils import Sequence\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n","a4478c23":"# Preprocessing parameters\n\nsr = 44100 # Sampling rate\nduration = 10\nhop_length = 347 # To make time steps 128\nfmin = 20\n\nfmax = sr \/\/ 2\nn_mels = 128\n\nn_fft = n_mels * 20\nsamples = sr * duration","407a839c":"def downsample_mono(path, sr):\n    \n    obj = wavio.read(path)\n    wav = obj.data.astype(np.float32, order='F')\n    rate = obj.rate\n    \n    try:\n        channel = wav.shape[1]\n        if channel == 2:\n            wav = to_mono(wav.T)\n        elif channel == 1:\n            wav = to_mono(wav.reshape(-1))\n    except IndexError:\n        wav = to_mono(wav.reshape(-1))\n        pass\n    except Exception as exc:\n        raise exc\n    wav = resample(wav, rate, sr)\n    wav = wav.astype(np.int16)\n    return sr, wav\n\n\ndef read_audio(path):\n    '''\n    Reads in the audio file and returns\n    an array that we can turn into a melspectogram\n    \n    set Audio clip limit and Down sampling rates\n    \n    '''\n    y, sr = librosa.core.load(path, sr=16000,duration=duration)\n\n    return y\n\ndef audio_to_melspectrogram(audio,s_r,n_mel_val):\n    '''\n    Convert to melspectrogram after audio is read in\n    '''\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=s_r,\n                                                 n_mels=n_mel_val,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_mel_val*20,\n                                                 fmin=20,\n                                                 fmax=s_r\/\/2)\n    \n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram(path):\n    '''\n    Convert audio into a melspectrogram \n    so we can use machine learning\n    '''\n    mels = audio_to_melspectrogram(read_audio(path))\n    return mels\n\n\ndef audio_to_melspectrogram2(audio,s_r,n_mel_val):\n    '''\n    Convert to melspectrogram after audio is read in\n    '''\n    spectrogram = librosa.feature.melspectrogram(audio, \n                                                 sr=s_r,\n                                                 n_mels=n_mel_val,\n                                                 hop_length=hop_length,\n                                                 n_fft=n_mel_val*20,\n                                                 fmin=20,\n                                                 fmax=s_r\/\/2)\n    \n    return librosa.power_to_db(spectrogram).astype(np.float32)\n\ndef read_as_melspectrogram2(path):\n    '''\n    Convert audio into a melspectrogram \n    so we can use machine learning\n    '''\n    mels = audio_to_melspectrogram2(read_audio(path))\n    return mels\n\ndef convert_wav_to_image(df):\n    \n    X_mel_spec = []\n    X_mel_spec2 = []\n    for _,row in tqdm(df.iterrows()):\n        x_mel1 = read_as_melspectrogram(row.file_path)\n        x_mel2 = read_as_melspectrogram2(row.file_path)\n        X_mel_spec.append(x_mel1.transpose(),)\n        X_mel_spec2.append(x_mel2.transpose())\n\n        \n    return X_mel_spec,X_mel_spec2\n\n\ndef convert_wav_to_mfcc(df):\n\n    X_mfcc = []\n    \n    for _,row in tqdm(df.iterrows()):\n\n        x_mfcc =generate_mfcc_feature(row.file_path)\n        X_mfcc.append(x_mfcc.transpose())\n        \n    return X_mfcc\n\ndef normalize(img):\n    '''\n    Normalizes an array \n    (subtract mean and divide by standard deviation)\n    '''\n    eps = 0.001\n    if np.std(img) != 0:\n        img = (img - np.mean(img)) \/ np.std(img)\n    else:\n        img = (img - np.mean(img)) \/ eps\n    return img\n\ndef normalize_dataset(X):\n    '''\n    Normalizes list of arrays\n    (subtract mean and divide by standard deviation)\n    '''\n    normalized_dataset = []\n    for img in X:\n        normalized = normalize(img)\n        normalized_dataset.append(normalized)\n    return normalized_dataset","b44e8454":"import numpy as np\nfrom numpy.lib.stride_tricks import as_strided\nfrom typing import Tuple\nimport librosa\nimport numpy as np\n\ndef get_audio(file_path: str) -> Tuple[np.ndarray, int]:\n    audio_data, sr = librosa.core.load(file_path)\n    length = len(audio_data) \/ sr\n\n    return audio_data, sr, length\n\nclass AudioMovingWindowPreProcessor():\n\n    def __init__(self) -> None:\n        \"\"\"\n         init class\n        \"\"\"\n        pass\n\n    def get_audio_windows(self, audio: np.ndarray, sr: int, length: int, window_size: int, stride: int) -> np.ndarray:\n        \"\"\"\n           Generate audio frames using  sliding window with stride -memory safe\n        \"\"\"\n        no_frames = int((length - window_size) \/ stride) + 1\n        window_size = window_size * sr\n        stride = stride * sr\n\n        audio_frames = []\n\n        for index in range(no_frames):\n\n            if (stride * index + window_size) < len(audio):\n                frame = audio[stride * index:(stride * index + window_size)]\n                audio_frames.append(frame)\n\n            else:\n                break\n\n        return np.array(audio_frames)\n\n    def get_audio_windows_numpy_vectorized(self, audio: np.ndarray, sr: int, length: int, window_size: int,\n                                           stride: int) -> np.ndarray:\n        \"\"\"\n           Generate audio frames using  sliding window with stride Numpy - non-memory safe\n        \"\"\"\n        no_frames = int((length - window_size) \/ stride) + 1\n        audio_frames = as_strided(audio, shape=(no_frames, window_size * sr), strides=(stride * sr, 1))\n\n        audio_frames = audio_frames[:-2]\n\n        return audio_frames\n","84f8656c":"import pandas as pd\nimport numpy as np\nfrom scipy import signal\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\n\nfrom skimage.restoration import denoise_wavelet\n\n\nclass Wavelet_Filter:\n\n    def wavelet_filter(self, filteredSignal, samplerate):\n        x_den = denoise_wavelet(filteredSignal, method='VisuShrink', mode='soft', wavelet_levels=5, wavelet='coif2',\n                                rescale_sigma='True')\n\n        return x_den, samplerate\n\n\nclass Filter_BW_HP:\n\n    def __init__(self, high_pass):\n        self.high_pass = high_pass\n\n    def BW_highpass(self, newdata, samplerate):\n        b, a = signal.butter(4, 100 \/ (22050 \/ 2), btype='highpass')\n\n        filteredSignal = signal.lfilter(b, a, newdata)\n\n        return filteredSignal, samplerate\n\n\nclass FIlter_BW_LP:\n\n    def __init__(self, low_pass):\n        self.low_pass = low_pass\n\n    def BW_lowpass(self, filteredSignal, samplerate):\n        c, d = signal.butter(4, 2000 \/ (22050 \/ 2), btype='lowpass')\n        newFilteredSignal = signal.lfilter(c, d, filteredSignal)\n\n        return newFilteredSignal, samplerate\n\n\nclass FilterPipeline():\n\n    def __init__(self, low_pass, high_pass):\n\n        self.low_pass = low_pass\n        self.high_pass = high_pass\n\n        self.lp_filter = FIlter_BW_LP(low_pass)\n        self.hp_filter = Filter_BW_HP(high_pass)\n        self.wavelet = Wavelet_Filter()\n\n    def filters(self, audio_signal, sample_rate):\n        filtered_output, sr = self.lp_filter.BW_lowpass(audio_signal, sample_rate)\n        filtered_output, sr = self.hp_filter.BW_highpass(filtered_output, sr)\n        filtered_output, sr = self.wavelet.wavelet_filter(filtered_output, sr)\n\n        return filtered_output, sr\n\n","97958988":"\nimport math\n\n\nimport librosa\nimport numpy as np\nfrom typing import Tuple\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Filter Config\nLOW_PASS_FREQUENCY = 100\nHIGH_PASS_FREQUENCY = 2000\n\n# Mel-Spectral Config\nHOG_LENGTH = 347\n\n# sizes in seconds\nMOVING_WINDOW_SIZE = 5\nAUDIO_STRIDE_SIZE = 5\n\n\nclass AudioPreProcessor(AudioMovingWindowPreProcessor, FilterPipeline):\n\n    def __init__(self):\n        AudioMovingWindowPreProcessor.__init__(self)\n        FilterPipeline.__init__(self, LOW_PASS_FREQUENCY, HIGH_PASS_FREQUENCY)\n        super().__init__()\n\n    def pre_process_audio(self, audio_path: str, sample_rate: int) -> np.ndarray:\n        \"\"\"\n\n        Generate Audio frames using moving window\n\n        Parameters\n        ----------\n        audio_path\n        sample_rate\n\n        Returns audio_frames\n        -------\n\n        \"\"\"\n        audio_data, sample_rate, length = get_audio(audio_path, sample_rate)\n        filtered_audio, sample_rate = self.filters(audio_data, sample_rate)\n        audio_frames = self.get_audio_windows(filtered_audio, sample_rate, length, MOVING_WINDOW_SIZE,\n                                              AUDIO_STRIDE_SIZE)\n\n        return audio_frames\n\n    def audio_to_mel_spectrogram(self, audio_data: np.ndarray, s_r: int, n_mel_val: int) -> np.ndarray:\n        \"\"\"\n                Convert audio_frames into mel-spectrogram\n\n        Parameters\n        ----------\n        s_r\n        n_mel_val\n\n        Returns mel-spectralgrams\n        -------\n\n        \"\"\"\n        spectrogram = librosa.feature.melspectrogram(audio_data,\n                                                     sr=s_r,\n                                                     n_mels=n_mel_val,\n                                                     hop_length=HOG_LENGTH,\n                                                     n_fft=n_mel_val * 20,\n                                                     fmin=20,\n                                                     fmax=s_r \/\/ 2)\n\n        return librosa.power_to_db(spectrogram).astype(np.float32)\n\n","8f069c37":"# sizes in seconds\nMOVING_WINDOW_SIZE = 5\nAUDIO_STRIDE_SIZE = 4\nSAMPLE_RATE=16000\n\nfrom tqdm import tqdm\n#!mkdir breathcycles\n\nX_mel_spec = []\nX_mel_spec2 = []\nY_labels = []\n\ndurations_spec=[]\n\nfor  indx, df_row in tqdm( processed_dataset_df.iterrows() ):\n    \n    \n     head, audio_file = os.path.split(df_row.file_path)\n     audio_file_name = audio_file.split('.wav')[0]   \n     diagnosis_type =df_row.diagnosis\n    \n     breathing_cycles_df = rec_annotations_dict.get(audio_file_name)\n\n     audio_data, sample_rate,length = get_audio(df_row.file_path)\n\n        \n        \n     no_frames = int((length - MOVING_WINDOW_SIZE) \/ AUDIO_STRIDE_SIZE) + 1\n     window_size = MOVING_WINDOW_SIZE * sample_rate\n     stride = AUDIO_STRIDE_SIZE * sample_rate\n\n     audio_frames = []\n\n     for index in range(no_frames):\n\n            if (stride * index + window_size) < len(audio_data):\n                \n               frame = audio_data[stride * index:(stride * index + window_size)]\n               x_mel1 = audio_to_melspectrogram(frame,s_r=sample_rate,n_mel_val=128)\n               x_mel2 = audio_to_melspectrogram2(frame,s_r=sample_rate,n_mel_val=64)\n               X_mel_spec.append(x_mel1.transpose(),)\n               X_mel_spec2.append(x_mel2.transpose())\n            \n               Y_labels.append(str(diagnosis_type))\n\n            else:\n                break\n\n\n    \n    \n","ac6f33cd":"X_mel1 = np.array(X_mel_spec)\nX_mel2 = np.array(X_mel_spec2)\n#X = normalize_dataset(X)\nX_array_mel1=np.array(X_mel1)\nX_array_mel2=np.array(X_mel2)","6c4accec":"val_spae=[]\nfor x in X_mel1:\n    #print(x.shape[0])\n    val_spae.append(x.shape[0])","ac9ef930":"def Average(lst):\n    return sum(lst) \/ len(lst)\n  \n# Driver Code\n\naverage = Average(val_spae)\n# Printing average of the list\nprint(\"Average of the list =\", round(average, 2))","e2553107":"import seaborn as sns\n\nsns.distplot(val_spae)","bb849b26":"max_audio_lenght = max([len(i) for i in X_array_mel1])\n\nmax_audio_lenght2 = max([len(i) for i in X_array_mel2])\n\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n#raw data processing-padding\nX_array_mel1=pad_sequences(X_array_mel1, padding=\"post\", dtype='float32')\nX_array_mel2=pad_sequences(X_array_mel2, padding=\"post\", dtype='float32')\n","d2d19378":"X_array_mel1[0].shape","d634640f":"# Visualize an melspectogram example\nplt.figure(figsize=(15,10))\nplt.title('Visualization of audio file', weight='bold')\nplt.imshow(X_array_mel1[1110]);","7b60a47c":"X_array_mel1.shape[0]","f904d4cf":"shape_lenght=X_array_mel1.shape[0]\nnew_Xtrain = np.empty((shape_lenght,2), dtype=np.object)\nfor idx in range(shape_lenght):\n    \n    new_Xtrain[idx,]=([X_array_mel1[idx],X_array_mel2[idx]])\n\n","4e8c0ebc":"from tensorflow import keras \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import LSTM, Dense,TimeDistributed, LayerNormalization,Masking\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\n\n\n\n#!pip install kapre\n#import kapre\n#from kapre.composed import get_melspectrogram_layer\nimport tensorflow as tf\nimport os\n\n!pip install -U tensorflow-addons\n!pip install -q \"tqdm>=4.36.1\"\n\nimport tensorflow_addons as tfa\nfrom tqdm import tqdm\nfrom tqdm.keras import TqdmCallback","13b5cc79":"from tensorflow.keras.utils import to_categorical\n\nle = LabelEncoder()\ndiagnosis_classes= list(set(Y_labels))\nle.fit(diagnosis_classes)\ny = le.transform(Y_labels)\nY=to_categorical(y, num_classes=len(diagnosis_classes))\nnp.save('diagnosis_classes.npy', le.classes_)","c7ee2d3c":"audio_time_span=new_Xtrain[0][0].shape[0]\nno_mel_sp_features =new_Xtrain[0][0].shape[1]\nNO_CLASSES=6 # Binary classification -> 2","3f6520d6":"def LSTM_Model_Dual(N_CLASSES,max_audio_length,no_features):\n    \n    i1=Input(shape=(max_audio_length,no_features), dtype=\"float32\",name='mel-spectral-input')\n    x1 = LayerNormalization(axis=2, name='batch_norm')(i1)\n    x1 = TimeDistributed(layers.Reshape((-1,)), name='reshape')(x1)\n    s1 = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh')(x1)\n    #masking\n    x1 = layers.Bidirectional(layers.LSTM(128, return_sequences=True),name='LSTM_1-128')(s1)\n    x1 = layers.Bidirectional(layers.LSTM(256, return_sequences=True),name='LSTM_1-256')(x1)\n    x1 = layers.Bidirectional(layers.LSTM(64, return_sequences=True),name='LSTM_1-64')(x1)\n    #x1 = layers.Bidirectional(layers.LSTM(64),name='LSTM_2-64')(x1)\n    \n    \n    x1 = layers.concatenate([s1, x1], axis=2, name='skip_connection')\n    x1 = layers.Dense(64, activation='relu', name='dense_1_relu')(x1)\n    x1 = layers.MaxPooling1D(name='max_pool_1d')(x1)\n    x1 = layers.Dense(32, activation='relu', name='dense_2_relu')(x1)\n    x1 = layers.Flatten(name='flatten')(x1)\n    x1 = layers.Dropout(rate=0.2, name='dropout')(x1)\n    x1 = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu')(x1)\n    \n    \n    i2=Input(shape=(max_audio_length,64), dtype=\"float32\",name='mel-spectral2-input')\n    x2 = LayerNormalization(axis=2, name='batch_norm_mel-spectral2')(i2)\n    x2 = TimeDistributed(layers.Reshape((-1,)), name='reshape_mel-spectral2')(x2)\n    s2 = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh_mel-spectral2')(x2)\n    #masking\n    x2 = layers.Bidirectional(layers.LSTM(128, return_sequences=True),name='mel-2-LSTM_1-128')(s2)\n    x2 = layers.Bidirectional(layers.LSTM(256, return_sequences=True),name='mel-2-LSTM_1-256')(x2)\n    x2 = layers.Bidirectional(layers.LSTM(64, return_sequences=True),name='mel-2-LSTM_1-64')(x2)\n    \n    x2 = layers.concatenate([s2, x2], axis=2, name='skip_connection_mel-spectral2')\n    x2 = layers.Dense(64, activation='relu', name='dense_1_relu_mel-spectral2')(x2)\n    x2 = layers.MaxPooling1D(name='max_pool_1d_mfcc')(x2)\n    x2 = layers.Dense(32, activation='relu', name='dense_2_relu_mel-spectral2')(x2)\n    x2 = layers.Flatten(name='flatten_mel-spectral2')(x2)\n    x2 = layers.Dropout(rate=0.2, name='dropout_mel-spectral2')(x2)\n    x2 = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu_mel-spectral2')(x2)\n    \n    \n    x = layers.concatenate([x1, x2], name='mel-mfcc_connection')\n    \n    o = layers.Dense(N_CLASSES, activation='softmax', name='softmax')(x)\n    model = Model(inputs=[i1,i2], outputs=o, name='dual_convolutional_long_short_term_memory')\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = LSTM_Model_Dual(NO_CLASSES,audio_time_span,no_mel_sp_features)\nmodel.summary()","7a453243":"def LSTM_model_V2(N_CLASSES,max_audio_length,no_features):\n    \n    i=Input(shape=(max_audio_length,no_features), dtype=\"float32\")\n    x = LayerNormalization(axis=2, name='batch_norm')(i)\n    x = TimeDistributed(layers.Reshape((-1,)), name='reshape')(x)\n    s = TimeDistributed(layers.Dense(64, activation='tanh'),\n                        name='td_dense_tanh')(x)\n    #masking\n    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True),\n                             name='bidirectional_lstm')(s)\n    x = layers.concatenate([s, x], axis=2, name='skip_connection')\n    x = layers.Dense(64, activation='relu', name='dense_1_relu')(x)\n    x = layers.MaxPooling1D(name='max_pool_1d')(x)\n    x = layers.Dense(32, activation='relu', name='dense_2_relu')(x)\n    x = layers.Flatten(name='flatten')(x)\n    x = layers.Dropout(rate=0.2, name='dropout')(x)\n    x = layers.Dense(32, activation='relu',\n                         activity_regularizer=l2(0.001),\n                         name='dense_3_relu')(x)\n    o = layers.Dense(N_CLASSES, activation='softmax', name='softmax')(x)\n    model = Model(inputs=i, outputs=o, name='long_short_term_memory')\n    model.compile(optimizer='adam',\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = LSTM_model_V2(NO_CLASSES,audio_time_span,no_mel_sp_features)\nmodel.summary()\n","b8e5ec61":"# initialize tqdm callback with default parameters\ntqdm_callback = tfa.callbacks.TQDMProgressBar()","a27bc326":"from sklearn.model_selection import KFold,StratifiedKFold\nnum_folds = 5\n\n# Define per-fold score containers\nacc_per_fold = []\nloss_per_fold = []\n\n# Merge inputs and targets\n#inputs = np.concatenate((X_train, X_val), axis=0)\n#targets = np.concatenate((Y_train, Y_val), axis=0)\n\n# Define the K-fold Cross Validator\nStrftkfold = StratifiedKFold(n_splits=num_folds, shuffle=True)\n\n# K-fold Cross Validation model evaluation\nfold_no = 1\n\nk_fold_history=[]\n\n#Custom callback\ncheckpoint_filepath = 'training_lstm_v2\/cp.ckpt'\ncheckpoint_filepath = os.path.dirname(checkpoint_filepath)\nmodel_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    monitor='val_accuracy',\n    verbose=0,\n    mode='max',\n    save_best_only=True)\n\nprogressbar=keras.callbacks.ProgbarLogger(count_mode=\"samples\", stateful_metrics=['acc','val_loss'])\n\n#Keras callback\nkeras_callbacks = [model_checkpoint_callback,tqdm_callback]\n\n#Setup Model\n#model = build_lstm_model()\nmodel = LSTM_Model_Dual(NO_CLASSES,audio_time_span,no_mel_sp_features)\nclass_labels = np.argmax(Y, axis=1)\n\n# Loop through the indices the split() method returns\nfor index, (train_indices, val_indices) in enumerate(Strftkfold.split(new_Xtrain, class_labels)):\n\n    # Generate batches from indices\n    xtrain, xval = new_Xtrain[train_indices], new_Xtrain[val_indices]\n    ytrain, yval = Y[train_indices], Y[val_indices]\n    \n    \n    x_mel1= np.empty( ( int(xtrain.shape[0]) ,int(xtrain[0][0].shape[0]) , int(xtrain[0][0].shape[1]) ), dtype=np.float32)\n    x_mel2= np.empty( ( int(xtrain.shape[0]) ,int(xtrain[0][1].shape[0]) , int(xtrain[0][1].shape[1]) ), dtype=np.float32)\n    \n    x_mel1_val= np.empty( ( int(xval.shape[0]) ,int(xval[0][0].shape[0]) , int(xval[0][0].shape[1]) ), dtype=np.float32)\n    x_mel2_val= np.empty( ( int(xval.shape[0]) ,int(xval[0][1].shape[0]) , int(xval[0][1].shape[1]) ), dtype=np.float32)\n    \n    # Exctracting 2 features into Numpy array- Training set\n    for i,x in enumerate(xtrain):\n        \n        x_mel1[i,]=x[0]\n        x_mel2[i,]=x[1]\n\n   # Exctracting 2 features into Numpy array validation set\n    for i,x in enumerate(xval):\n        \n        x_mel1_val[i,]=x[0]\n        x_mel2_val[i,]=x[1]\n    \n    \n    #create a batch\n\n    #train the model \n    history = model.fit({'mel-spectral-input':x_mel1,'mel-spectral2-input':x_mel2}, ytrain,batch_size=32,epochs=50,verbose=0,callbacks=keras_callbacks,validation_split=0.2)\n    k_fold_history.append(history)\n    \n    # Generate generalization metrics\n    scores = model.evaluate({'mel-spectral-input':x_mel1_val,'mel-spectral2-input':x_mel2_val}, yval, verbose=0)\n    print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%' )\n    acc_per_fold.append(scores[1] * 100)\n    loss_per_fold.append(scores[0])  ","f2739a6a":"# Average scores ==\nprint('------------------------------------------------------------------------')\nprint('Validation Score per fold')\nfor i in range(0, len(acc_per_fold)):\n  print('------------------------------------------------------------------------')\n  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\nprint('------------------------------------------------------------------------')\nprint('Average scores for all folds:')\nprint(f'> Val Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\nprint(f'> Val Loss: {np.mean(loss_per_fold)}')\nprint('------------------------------------------------------------------------')","9e47beb3":"fig, axes = plt.subplots(num_folds, 2,sharex=True,figsize=(30,20))\nfig.suptitle('Cross-Validation Learning Curve')\n\nindex_sub_plot=1\n\nfor history in k_fold_history:\n    \n    axes[index_sub_plot-1,0].set_title('K-fold: {} Acc'.format(index_sub_plot))\n    axes[index_sub_plot-1,0].plot(history.history['accuracy'])\n    axes[index_sub_plot-1,0].plot(history.history['val_accuracy'])\n    axes[index_sub_plot-1,0].set_ylabel('accuracy')\n    axes[index_sub_plot-1,0].set_xlabel('epoch')\n    axes[index_sub_plot-1,0].legend(['train', 'test'], loc='upper left')\n    \n    \n    axes[index_sub_plot-1,1].set_title('K-fold: {} Loss'.format(index_sub_plot))\n    axes[index_sub_plot-1,1].plot(history.history['loss'])\n    axes[index_sub_plot-1,1].plot(history.history['val_loss'])\n    axes[index_sub_plot-1,1].set_ylabel('loss')\n    axes[index_sub_plot-1,1].set_xlabel('epoch')\n    axes[index_sub_plot-1,1].legend(['train', 'test'], loc='upper left')\n\n    \n    index_sub_plot+=1\n    \n\n#","6d3f1b9f":"!ls {checkpoint_filepath}","fa026c22":"checkpoint_path = 'training_lstm_v2\/saved_model.pb' \ncheckpoint_dir = os.path.dirname(checkpoint_path)\noptimal_model = tf.keras.models.load_model(checkpoint_dir)","60071630":"optimal_model.save(\"my_model\")\n","b7656338":"# Calling `save('my_model.h5')` creates a h5 file `my_model.h5`.\noptimal_model.save(\"my_h5_model.h5\")\n!commit","a44de46b":"from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nmatrix_index = le.classes_\n\npreds = optimal_model.predict({'mel-spectral-input':x_mel1_val,'mel-spectral2-input':x_mel2_val})\n#preds = optimal_model.predict(x_mel1_val)\nclasspreds = np.argmax(preds, axis=1) # predicted classes \ny_testclass = np.argmax(yval, axis=1) # true classes\n\ncm = confusion_matrix(y_testclass, classpreds)\nprint(classification_report(y_testclass, classpreds, target_names=matrix_index))\n\n# Get percentage value for each element of the matrix\ncm_sum = np.sum(cm, axis=1, keepdims=True)\ncm_perc = cm \/ cm_sum.astype(float) * 100\nannot = np.empty_like(cm).astype(str)\nnrows, ncols = cm.shape\nfor i in range(nrows):\n    for j in range(ncols):\n        c = cm[i, j]\n        p = cm_perc[i, j]\n        if i == j:\n            s = cm_sum[i]\n            annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n        elif c == 0:\n            annot[i, j] = ''\n        else:\n            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n\n\n# Display confusion matrix \ndf_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nfig, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(df_cm, annot=annot, fmt='')\n","46a29b27":"le.classes_","9c94fa14":"dftest = pd.DataFrame(preds, columns = le.classes_)","31c80221":"dftest.to_csv(\"Test_Predictions.csv\",index=False)","49996ec4":"#Libraries","fbd6f25e":"# Data-Set","1be3962f":"# Clean unbalnce classes","3a1e735a":"Pre-process audio files","62a9a9a5":"Label Audio Data","e12907f7":"Data Preparing"}}