{"cell_type":{"682f4aef":"code","a358b1fd":"code","81cfae63":"code","3ac16594":"code","32343164":"code","e7d6b0e6":"code","5bf71a5b":"code","4ec606c0":"code","995bfc3c":"code","00f85681":"code","e7512b8a":"code","8d5ac573":"code","b033bba4":"code","92c6f9ce":"code","15323a93":"code","6c1e282e":"code","d73b4b85":"code","4f8e8768":"markdown","265813b0":"markdown","36cfa1eb":"markdown","2ad5c666":"markdown","6ab0acd3":"markdown","9bf9637d":"markdown","bfe41d53":"markdown","fae82351":"markdown","b86fb1f6":"markdown","511dc57d":"markdown"},"source":{"682f4aef":"import numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,RandomizedSearchCV\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport re\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor","a358b1fd":"df=pd.read_csv('..\/input\/airbnb-dataset-for-internship-project-showcase\/newyork_airbnb.csv')","81cfae63":"# We create an array with the features we want to keep.\nselected_features = ['name', 'neighbourhood_cleansed', 'room_type', 'guests_included', 'minimum_nights',\n                     'number_of_reviews', 'review_scores_rating', 'amenities', 'property_type',\n                     'accommodates', 'bathrooms', 'bedrooms', 'beds', 'bed_type', 'price']\nselected_df= df.copy()[selected_features]\nselected_df.rename(columns = {'neighbourhood_cleansed':'neighbourhood'}, inplace = True)","3ac16594":"selected_df['price']=selected_df['price'].apply(lambda x: x.replace('$',''))\nselected_df['price']=selected_df['price'].apply(lambda x: float(x.replace(',','')))","32343164":"\n##%%time\nregex = r\"{([^}]*)}\"\nregex2 = r\"translation.\\w\\D+..\"\nlistings_cp = selected_df.copy()\nlistings_cp['amenities'] = selected_df['amenities'].map(lambda amns: re.search(regex, amns).group(1))\nlistings_cp['amenities'] = selected_df['amenities'].map(lambda amns: re.sub(regex2, '', amns))\nlistings_cp['amenities'] = selected_df['amenities'].map(lambda amns: amns.replace(\"\\\"\", \"\"))","e7d6b0e6":"\n# The code for adding the amenities colums is currently commented for practicality\n#amenity_ohe = listings_cp.amenities.str.get_dummies(sep = \",\")\n#amenities_cols = amenity_ohe.columns.values\n# dataset = pd.concat([listings_cp, amenity_ohe], axis=1)\ndataset = selected_df.query('price <= 500')\ndataset = dataset.drop('amenities', axis=1)\ndataset = dataset.drop('name', axis=1)","5bf71a5b":"dataset.isnull().sum()","4ec606c0":"dataset.dropna(inplace=True)\n","995bfc3c":"# The numeric features that present null values are: review_scores_rating, bathrooms, bedrooms\n# and beds from previous notebook https:\/\/github.com\/Summi-bhai\/Airbnb_dataset\/blob\/master\/Analysis_Part_1.ipynb\n\ndataset['review_scores_rating'] = dataset['review_scores_rating'].fillna(dataset['review_scores_rating'].median())\ndataset['bathrooms'] = dataset['bathrooms'].fillna(dataset['bathrooms'].median())\ndataset['bedrooms'] = dataset['bedrooms'].fillna(dataset['bedrooms'].median())\ndataset['beds'] = dataset['beds'].fillna(dataset['beds'].median())","00f85681":"dataset.dtypes","e7512b8a":"\n# it was showing error with bed_type that's why we will be handling it by our hands\ncat_columns = ['neighbourhood', 'room_type', 'property_type','bed_type']\nencoder=LabelEncoder()\nfor col in cat_columns:\n    dataset[col]=encoder.fit_transform(dataset[col])\n\n# converting features to float\ndataset = dataset.astype(float)\ndataset.dtypes","8d5ac573":"y=dataset['price']\ndataset.drop('price',axis=1,inplace=True)\n\nX=dataset\n\n# now we have training,validation and test data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.1)\nX_train,X_val,y_train,y_val=train_test_split(X_train,y_train,test_size=0.2)\n\n","b033bba4":"\ndef fit_model(model,X_train, y_train, X_val, y_val, cross_val=False, cv_folds=5):\n    \n    model.fit(X_train,y_train)\n    #Predict values:\n    training_predictions = model.predict(X_train)\n    validation_predictions = model.predict(X_val)\n    model_report(X_train,y_train, X_val,y_val, training_predictions, validation_predictions)\n\n    if cross_val:\n        evaluate_cross_validation(model, X_train, y_train, cv_folds)\n        \ndef model_report(X_train,y_train, X_val,y_val, training_predictions, validation_predictions):\n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Training\")\n    print(\"Mean Absolute Error : {}\".format(mean_absolute_error(y_train, training_predictions)))\n    print(\"Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y_train, training_predictions))))\n    print(\"R2 Score: {}\".format(r2_score(y_train, training_predictions)))\n    print(\"\\n\")\n    print(\"Validation\")\n    print(\"Mean Absolute Error : {}\".format(mean_absolute_error(y_val, validation_predictions)))\n    print(\"Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y_val, validation_predictions))))\n    print(\"R2 Score: {}\".format(r2_score(y_val, validation_predictions)))\n    \ndef evaluate_cross_validation(model, X_train, y_train, K):\n    \n    cv = KFold(n_splits=K, shuffle=True, random_state=2)\n    scores = cross_val_score(model, X_train, y_train, cv=cv)\n    print()\n    print(scores)\n    print(\"Mean score: {} max is {} and min {}\".format(scores.mean(), max(scores), min(scores)))   \n    \ndef random_search(model, param_grid, X_train, y_train, X_val, y_val, cv):\n    \n    random_search = RandomizedSearchCV(model, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=2, cv=cv)\n    random_result = random_search.fit(X_train, y_train)\n\n    # summarize results\n    print(\"Best: %f using %s\" % (random_result.best_score_, random_result.best_params_))\n    means = random_result.cv_results_['mean_test_score']\n    stds = random_result.cv_results_['std_test_score']\n    params = random_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n    validation_predictions = random_result.predict(X_val)\n\n    print(\"Validation\")\n    print(\"Mean Absolute Error : {}\".format(mean_absolute_error(y_val, validation_predictions)))\n    print(\"Root Mean Squared Error : {}\".format(np.sqrt(mean_squared_error(y_val, validation_predictions))))\n    print(\"R2 Score: {}\".format(r2_score(y_val, validation_predictions)))\n","92c6f9ce":"model=DecisionTreeRegressor(random_state=8)\nfit_model(model,X_train,y_train,X_val,y_val,cross_val=True)","15323a93":"model=RandomForestRegressor()\nfit_model(model,X_train,y_train,X_val,y_val,cross_val=True)","6c1e282e":"# Tunning Parameters\nmax_depth = [5, 10, 15, 20]\nmax_features = ['log2', 'sqrt', 'auto']\nmin_samples_leaf = [10, 50, 70, 100]\nparam_grid = dict(max_depth=max_depth, max_features=max_features, \n                  min_samples_leaf=min_samples_leaf)\nkfold = KFold(n_splits=5, shuffle=True, random_state=2)\n\nrandom_search(DecisionTreeRegressor(random_state=8), param_grid, X_train, y_train, X_val, y_val, kfold)","d73b4b85":"model = DecisionTreeRegressor(max_depth=20, min_samples_leaf=50, random_state=8)\nfit_model(model,X_train,y_train,X_val,y_val,cross_val=True)","4f8e8768":"Now we are going to define .fit() and cross-validation methods then we will fit different different models of regression from sklearn library and then finally use randomized search cv for better hyper-parameter choice ","265813b0":"# Handling Null Values  \n\nSince there was many empty columns in our dataset so we have to treat them before fitting to the model","36cfa1eb":"now we will convert amenities column ie. we will extract every single values and then join them to our dataset","2ad5c666":"# Importing Packages","6ab0acd3":"#  Conclusion\n\nWe have a model which have maximum accuracy of 57% in and min accuracy of 56% terms of R2_score.\n\n","9bf9637d":"# Converting categorical features to float\n","bfe41d53":"# ML Modelling\n\n* The main thing to see in this notebook are the functions whhich will be printing a systematic report for you machine learning algorithms\n\n* If your'e reading it means you are a begginer and came here in search of a project for your internship or placement, So the codes are self-explanatory and it is kind of a well documented code which you can use for companies in analytics domain since it is kind of a analysis based project.\n\n* From here you can have a good start in Data Science and please after repeating this notebook don't forget to upload your notebook in this notebook section so it will boost your confidence in showing your work to the community.\n\n* for hindi speakers - \"upvote karna mat bhoolna tabhi to bhai ki notebook chalegi\"\n\n* for others - \"please Don't forgot to upvote\"","fae82351":"# Model selection and training\n\n For now we will fit are dataset on Decision tree and Random forest.","b86fb1f6":"# Hyperparameter Tuning\n\nNow we had dry runned our model over default parameters but however unfortunately our model overfitted the data so now we will tune our parameters with the help of random search cv we will only using random search for decision tree.","511dc57d":"# Splitting the data \n"}}