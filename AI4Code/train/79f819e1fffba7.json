{"cell_type":{"728a8051":"code","29803970":"code","39d21c7b":"code","a5ee9c09":"code","8b5b22b7":"code","6f6d7b74":"code","988053eb":"code","73b4d14e":"code","6c05cf98":"code","1d700f44":"code","a9df0eef":"code","13876db0":"code","e5a9eac5":"code","9da994b0":"code","4837e80f":"code","e7a2f549":"code","742048bf":"code","7d35bc6d":"code","22cd2e6e":"code","7a3c0fcc":"code","7574aecd":"code","fa5bf941":"code","70060319":"code","e5be0d9b":"code","635e1f8d":"code","8476b332":"code","ba397319":"code","060e8d35":"code","41a4618b":"code","8c184a6d":"markdown","4647f195":"markdown","1468e62d":"markdown","c23ab0c4":"markdown","320c3cfc":"markdown","390fa56f":"markdown","5d3ace19":"markdown","89861ee4":"markdown","5b45128b":"markdown","ebd06570":"markdown","fee0fec1":"markdown","b061cc88":"markdown","fe5fa336":"markdown","a17a856f":"markdown"},"source":{"728a8051":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29803970":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","39d21c7b":"test.info(memory_usage=\"deep\")","a5ee9c09":"train.head(10)","8b5b22b7":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","6f6d7b74":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","988053eb":"train.describe().T","73b4d14e":"train.isna().sum().sum(), test.isna().sum().sum()","6c05cf98":"train[\"loss\"].value_counts()","1d700f44":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.bar(train[\"loss\"].value_counts().sort_index().index,\n              train[\"loss\"].value_counts().sort_index().values,\n              color=colors,\n              edgecolor=\"black\")\nax.set_title(\"Loss (target) distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Loss (target) value\", fontsize=14, labelpad=10)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"loss\"].value_counts().sort_index().values\/(len(train)\/100)],\n                 padding=5, fontsize=10, rotation=90)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","a9df0eef":"df = pd.concat([train.drop([\"id\", \"loss\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","13876db0":"train.nunique().sort_values().head()","e5a9eac5":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","9da994b0":"df[(df[\"loss\"]>-0.001) & (df[\"loss\"]<0.001)][\"loss\"]","4837e80f":"columns = train.drop([\"id\", \"loss\"], axis=1).columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"loss\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n#plt.suptitle(\"Features vs loss\", y=0.99)\nplt.show();","e7a2f549":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(train[\"loss\"], bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(train[\"loss\"], target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","742048bf":"# # Scaling data to [0, 1] range\n# x_scaler = MinMaxScaler()\n# X = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\n# X_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n# y_scaler = MinMaxScaler()\n# y = pd.Series(y_scaler.fit_transform(np.array(train[\"loss\"].copy()).reshape(-1,1)).flatten())\n# # y = train[\"loss\"].copy()","7d35bc6d":"# Scaling data\nx_scaler = StandardScaler()\nX = pd.DataFrame(x_scaler.fit_transform(train.drop([\"id\", \"loss\"], axis=1)), columns=train.drop([\"id\", \"loss\"], axis=1).columns)\nX_test = pd.DataFrame(x_scaler.transform(test.drop(\"id\", axis=1)), columns=test.drop([\"id\"], axis=1).columns)\n\ny = train[\"loss\"].copy()","22cd2e6e":"# def add_new_features(df):\n#     \"\"\"\n#     Adds custom features to a given dataframe\n#     \"\"\"\n#     df_copy = df.copy()\n#     df[\"custom_feat_0\"] = df_copy.sum(axis=1)\n#     df[\"custom_feat_1\"] = df_copy.mean(axis=1)  \n#     df[\"custom_feat_2\"] = df_copy.min(axis=1)\n#     df[\"custom_feat_3\"] = df_copy.max(axis=1)\n#     df[\"custom_feat_4\"] = df_copy.median(axis=1)\n\n# #     df[\"custom_feat_5\"] = (df_copy > 0).astype(int).sum(axis=1)\n# #     df[\"custom_feat_6\"] = (df_copy < 0).astype(int).sum(axis=1)\n# #     df[\"custom_feat_7\"] = df_copy.kurt(axis=1)\n# #     df[\"custom_feat_8\"] = df_copy.mad(axis=1)\n# #     df[\"custom_feat_9\"] = df_copy.var(axis=1)\n# #     df[\"custom_feat_10\"] = df_copy.std(axis=1)\n# #     df[\"custom_feat_11\"] = df_copy.skew(axis=1)\n# #     df[\"custom_feat_12\"] = df_copy.abs().sum(axis=1)\n    \n#     return df","7a3c0fcc":"# # Adding new features to the datasets\n# X = add_new_features(X.copy())\n# X_test = add_new_features(X_test.copy())","7574aecd":"X.describe()","fa5bf941":"X_test.describe()","70060319":"y.min(), y.max()","e5be0d9b":"def train_model_optuna(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n        \n    #A set of hyperparameters to optimize by optuna\n   \n    cb_params = {\n             \"iterations\": trial.suggest_categorical('iterations', [10000]),\n             \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 1.0),\n             \"loss_function\": 'RMSE',\n             \"eval_metric\": 'RMSE',\n             \"l2_leaf_reg\": trial.suggest_float('l2_leaf_reg', 0.00001, 10),\n             \"bagging_temperature\": trial.suggest_float('bagging_temperature', 0.0, 10.0),\n             \"random_strength\": trial.suggest_float('random_strength', 1.0, 2.0),\n             \"depth\": trial.suggest_int('depth', 1, 16),\n             \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"]),\n             \"leaf_estimation_method\": trial.suggest_categorical(\"leaf_estimation_method\", [\"Newton\", \"Gradient\"]),#, \"Exact\"]),\n             \"od_type\": \"Iter\",\n             \"early_stopping_rounds\": 100,\n             \"border_count\": 254,\n             \"use_best_model\": True,\n\n# #                  \"max_leaves\": trial.suggest_int('max_leaves', 1, 64),\n# #                  \"task_type\": \"GPU\",\n                }\n\n\n    model = CatBoostRegressor(**cb_params, random_state=42, thread_count=4)\n    model.fit(\n                X_train, y_train,\n                eval_set=(X_valid, y_valid),\n                verbose=False,\n            )\n    print(f\"Trees: {model.tree_count_}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","635e1f8d":"# %%time\n# # Splitting data into train and valid folds using target bins for stratification\n# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n# for train_idx, valid_idx in split.split(X, target_bins):\n#     X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#     y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n# # Setting optuna verbosity to show only warning messages\n# # If the line is uncommeted each iteration results will be shown\n# # optuna.logging.set_verbosity(optuna.logging.WARNING)\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(lambda trial: train_model_optuna(trial, X_train, X_valid,\n#                                                     y_train, y_valid),\n#                n_trials = 100)\n\n# # Showing optimization results\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial parameters:', study.best_trial.params)\n# print('Best score:', study.best_value)","8476b332":"# Hyperparameters optimized by Optuna\n\n\ncb_params = {'iterations': 10000,\n             'learning_rate': 0.00929418593306697,\n             'l2_leaf_reg': 7.1557353016594405,\n             'bagging_temperature': 9.924411556321033,\n             'random_strength': 1.5202844820405794,\n             'depth': 7,\n             'grow_policy': 'Depthwise',\n             'leaf_estimation_method': 'Newton'}\n# cb_params = {'iterations': 10000,\n#              'learning_rate': 0.010526847803225213,\n#              'l2_leaf_reg': 7.578784014838337,\n#              'bagging_temperature': 0.5813008056988401,\n#              'random_strength': 1.7705601193056997,\n#              'depth': 7,\n#              'grow_policy': 'Depthwise',\n#              'leaf_estimation_method': 'Gradient'}\n# cb_params = {'iterations': 10000,\n#              'learning_rate': 0.030108080370377578,\n#              'l2_leaf_reg': 4.606620127979116,\n#              'bagging_temperature': 7.518949583881732,\n#              'random_strength': 1.060436568484918,\n#              'depth': 2,\n#              'grow_policy': 'Depthwise',\n#              'leaf_estimation_method': 'Gradient'}","ba397319":"%%time\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    model = CatBoostRegressor(random_state=42,\n                             thread_count=4,\n                             verbose=False,\n                             loss_function='RMSE',\n                             eval_metric='RMSE',\n                             od_type=\"Iter\",\n                             early_stopping_rounds=500,\n                             use_best_model=True,\n                             **cb_params)\n    model.fit(X_train, y_train,\n              eval_set=(X_valid, y_valid),\n              verbose=False)\n    preds += model.predict(X_test) \/ splits\n    model_fi += model.feature_importances_\n    oof_preds[valid_idx] = model.predict(X_valid)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse \/ splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")    ","060e8d35":"df = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model_fi \/ model_fi.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)\n\nx = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","41a4618b":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"loss\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","8c184a6d":"The code below is commented in order to save runtime.","4647f195":"# **Hyperparameters optimization**","1468e62d":"# **EDA**","c23ab0c4":"There are no missing value in the both datasets.\n\nLets check target distribution.","320c3cfc":"# **Data preparation**","390fa56f":"## **Feature importances**","5d3ace19":"## **Submission**","89861ee4":"Lets check feature values distribution in the both datasets.","5b45128b":"# **Model training**","ebd06570":"As you can see, the correlation is between ~0.03 and ~0.03 which is pretty small. So the features are weakly correlated. \n\nThere are some features with relatively low correlation with target value even comparing with other features:","fee0fec1":"Lets visualize each feature vs loss.","b061cc88":"The datasets are pretty well balanced.","fe5fa336":"## **Data import**","a17a856f":"As you can see, f1 feature has the smallest amount of unique values - 289. So I don't think any feature should be treated as categorical.\n\nLets look at feature correlation."}}