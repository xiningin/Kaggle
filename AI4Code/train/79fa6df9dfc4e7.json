{"cell_type":{"88ec8b6e":"code","48fb52cc":"code","52b5246e":"code","8363c17e":"code","967b45e7":"code","e1955a67":"code","154a4bf8":"code","141db7b5":"code","47953b14":"code","c2202adf":"code","551ece44":"code","3e5c20bf":"code","873c763c":"code","d23c6064":"code","82e17a9e":"code","90611c38":"code","b52a61ea":"code","a0e0dc6f":"code","8915f804":"code","a0e6c4d6":"code","24ac9f1a":"code","e0ddd1d5":"code","d2344974":"code","64d13f66":"code","f9d4c3e8":"code","362d02de":"code","84aeb414":"code","28c28a04":"code","bc8bee1b":"code","3672c3c9":"code","28ae4668":"code","44e340af":"code","d7d9738d":"code","c54ebdb6":"code","606a5b7c":"code","9e1e840a":"code","fdaea86c":"code","3a1eb23c":"code","fffcb8f6":"code","d0254020":"code","e5fa56a1":"code","34f299c4":"code","1a3b5912":"code","91241fa9":"code","a9b9588a":"code","cc046a89":"code","cd7f05a6":"code","9795a933":"code","3e0ac4b1":"code","c5d33fca":"code","fb84539f":"code","d7cdf750":"code","1d57bfca":"code","dc7b19ea":"code","5ad3eee4":"code","ffce80ee":"code","49596328":"code","76707da6":"code","96e675a5":"code","186dbdb3":"code","8d468d2e":"code","e45db998":"code","073f7bc0":"code","eda37b35":"code","5735c6f0":"code","39b014e6":"code","265f5767":"code","1daf273f":"code","021a5482":"code","1062b53a":"code","5d5d63bd":"code","76486196":"code","4daa1955":"markdown","c226d84a":"markdown","d00a6b1e":"markdown","3757d424":"markdown","f5ea8fec":"markdown","5bbedc9b":"markdown","2dad98c1":"markdown","94c3c6ee":"markdown","0e875595":"markdown","b9ab2287":"markdown","233d5784":"markdown","32f09e74":"markdown","3223b9fd":"markdown","fc81bf05":"markdown","6ff13b62":"markdown","7f056932":"markdown","1e216731":"markdown"},"source":{"88ec8b6e":"!pip uninstall keras -y\n!pip install git+https:\/\/github.com\/qubvel\/segmentation_models\n!git clone https:\/\/github.com\/SlinkoIgor\/ImageDataAugmentor.git\n    ","48fb52cc":"import nibabel as nib\nimport skimage.io as io\nimport numpy as np\nimg_tr=nib.load('\/kaggle\/input\/segmentation-data\/tr_im.nii')\nimg_tr_arr=img_tr.get_fdata()\nimg_tr_arr=np.squeeze(img_tr_arr)\nimg_tr_arr.shape","52b5246e":"import matplotlib.pyplot as plt","8363c17e":"plt.imshow(img_tr_arr[:,:,12])","967b45e7":"import nibabel as nib\nimport skimage.io as io\nimport numpy as np\nimg_mask=nib.load('\/kaggle\/input\/segmentation-data\/tr_mask.nii')\nimg_mask_arr=img_mask.get_fdata()\nimg_mask_arr=np.squeeze(img_mask_arr)\nimg_mask_arr.shape","e1955a67":"plt.imshow(img_mask_arr[:,:,12])","154a4bf8":"import json","141db7b5":"j = open('\/kaggle\/input\/jsonnew\/959830.json')\ndata = json.load(j)","47953b14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c2202adf":"import os \nimport numpy as np\nimport pandas as pd","551ece44":"prefix = '\/kaggle\/input\/covid-segmentation\/'\n\nimages_radiopedia = np.load(os.path.join(prefix, 'images_radiopedia.npy')).astype(np.float32)\nmasks_radiopedia = np.load(os.path.join(prefix, 'masks_radiopedia.npy')).astype(np.int8)\nimages_medseg = np.load(os.path.join(prefix, 'images_medseg.npy')).astype(np.float32)\nmasks_medseg = np.load(os.path.join(prefix, 'masks_medseg.npy')).astype(np.int8)\n\ntest_images_medseg = np.load(os.path.join(prefix, 'test_images_medseg.npy')).astype(np.float32)","3e5c20bf":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef visualize(image_batch, mask_batch=None, pred_batch=None, num_samples=2):\n    num_classes = mask_batch.shape[-1] if mask_batch is not None else 0\n    fix, ax = plt.subplots(num_classes + 1, num_samples, figsize=(num_samples * 2, (num_classes + 1) * 2))\n\n    for i in range(num_samples):\n        ax_image = ax[0, i] if num_classes > 0 else ax[i]\n        ax_image.imshow(image_batch[i,:,:,0], cmap='Greys')\n        ax_image.set_xticks([]) \n        ax_image.set_yticks([])\n        \n        if mask_batch is not None:\n            for j in range(num_classes):\n                if pred_batch is None:\n                    mask_to_show = mask_batch[i,:,:,j]\n                else:\n                    mask_to_show = np.zeros(shape=(*mask_batch.shape[1:-1], 3)) \n                    mask_to_show[..., 0] = pred_batch[i,:,:,j] > 0.5\n                    mask_to_show[..., 1] = mask_batch[i,:,:,j]\n                ax[j + 1, i].imshow(mask_to_show, vmin=0, vmax=1)\n                ax[j + 1, i].set_xticks([]) \n                ax[j + 1, i].set_yticks([]) \n\n    plt.tight_layout()\n    plt.show()","873c763c":"images_radiopedia.shape","d23c6064":"plt.imshow(images_radiopedia[100,:,:,0])","82e17a9e":"visualize(images_radiopedia[20:], masks_radiopedia[20:])","90611c38":"visualize(images_medseg, masks_medseg)","b52a61ea":"visualize(test_images_medseg)","a0e0dc6f":"plt.imshow(test_images_medseg[9,:,:,0])","8915f804":"def plot_hists(images1, images2=None):\n    plt.hist(images1.ravel(), bins=100, density=True, color='b', alpha=1 if images2 is None else 0.5)\n    if images2 is not None:\n        plt.hist(images2.ravel(), bins=100, density=True, alpha=0.5, color='orange')\n    plt.show();","a0e6c4d6":"plot_hists(images_radiopedia, images_medseg)","24ac9f1a":"plot_hists(test_images_medseg, images_medseg)","e0ddd1d5":"def preprocess_images(images_arr, mean_std=None):\n    images_arr[images_arr > 500] = 500\n    images_arr[images_arr < -1500] = -1500\n    min_perc, max_perc = np.percentile(images_arr, 5), np.percentile(images_arr, 95)\n    images_arr_valid = images_arr[(images_arr > min_perc) & (images_arr < max_perc)]\n    mean, std = (images_arr_valid.mean(), images_arr_valid.std()) if mean_std is None else mean_std\n    images_arr = (images_arr - mean) \/ std\n    print(f'mean {mean}, std {std}')\n    return images_arr, (mean, std)\n\nimages_radiopedia, mean_std = preprocess_images(images_radiopedia)\nimages_medseg, _ = preprocess_images(images_medseg, mean_std)\ntest_images_medseg, _ = preprocess_images(test_images_medseg, mean_std)","d2344974":"images_radiopedia.shape","64d13f66":"images_medseg.shape","f9d4c3e8":"test_images_medseg.shape","362d02de":"plot_hists(images_radiopedia, images_medseg)","84aeb414":"plot_hists(test_images_medseg, images_medseg)","28c28a04":"val_indexes, train_indexes = list(range(24)), list(range(24, 100))\n\ntrain_images = np.concatenate((images_medseg[train_indexes], images_radiopedia))\ntrain_masks = np.concatenate((masks_medseg[train_indexes], masks_radiopedia))\nval_images = images_medseg[val_indexes]\nval_masks = masks_medseg[val_indexes]\n\nbatch_size = len(val_masks)\n\ndel images_radiopedia\ndel masks_radiopedia\ndel images_medseg\ndel masks_medseg","bc8bee1b":"train_images.shape","3672c3c9":"train_masks.shape","28ae4668":"val_images.shape","44e340af":"plt.imshow(val_images[0,:,:,0])","d7d9738d":"val_masks.shape","c54ebdb6":"plt.imshow(val_masks[0,:,:,0])","606a5b7c":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Display\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nimport numpy as np\nimport os\nimport time\nimport pickle\nimport cv2\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import applications\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential, Model, load_model \nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Input\nfrom tensorflow.keras import backend as k \nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\nfrom skimage.transform import resize\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.resnet50 import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.vgg19 import VGG19\nimport os\nfrom xgboost import XGBClassifier\n#os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\nimport pickle\nfrom datetime import datetime\nimport cv2\nimport math\nimport numpy as np\nimport os\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport PIL\nimport torch\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.optim import lr_scheduler\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.nn import init\nfrom torch.autograd import Variable\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch import nn\nimport time, copy, argparse\nimport multiprocessing\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom torch import FloatTensor\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score,recall_score,precision_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import svm\nimport random\nimport functools\nimport joblib\nimport pickle\nimport os\nimport cv2\nfrom glob import glob\nfrom random import shuffle\n","9e1e840a":"def data_prep():\n  root_folder = '\/kaggle\/input\/large-covid19-ct-slice-dataset\/curated_data\/curated_data'\n  diction = {}\n\n  \n\n  dir_images = []\n  dir_label = []\n  \n  count_covid = 0\n  count_non_covid = 0\n    \n  for subdir in os.listdir(root_folder)[1:]:\n          print('subdir',os.path.join(root_folder, subdir))\n          for file_count, file_name in enumerate( sorted(glob(os.path.join(root_folder, subdir)+ '\/*'),key=len) ):\n              \n              img = cv2.imread(file_name)\n              img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n              dir_images.append(img)\n              dir_label.append(subdir)\n              if subdir =='1NonCOVID':\n                  count_non_covid +=1\n                  if count_non_covid ==1500:\n                    break\n              elif subdir == '2COVID':\n                  count_covid +=1\n                  if count_covid == 1500:\n                    break\n              \n\n  diction_train = {}\n  diction_test = {}\n  dir_label_shuf_train = []\n  dir_images_shuf_train = []\n  index_shuf = list(range(len(dir_label)))\n  shuffle(index_shuf)\n\n  train_shuf = index_shuf[0:int(len(index_shuf))]\n  \n\n  for i in train_shuf:\n      dir_label_shuf_train.append(dir_label[i])\n      dir_images_shuf_train.append(dir_images[i])    \n          \n  diction_train['X_tr'] = dir_images_shuf_train\n  diction_train['y_tr'] = dir_label_shuf_train\n\n\n\n        \n        \n  print(len(diction_train['X_tr']))\n  ##print(diction_train['y_tr'])\n  print(diction_train['X_tr'][0].shape)\n\n\n  \n\n  with open('training.pickle', 'wb') as handle:\n      pickle.dump(diction_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n  \n\n  nrows=224\n  ncolumns=224\n  channels=1\n\n  batch_size=16\n  epochs=1\n  # Number of classes\n  num_cpu = multiprocessing.cpu_count()\n  num_classes = 2\n  torch.manual_seed(8)\n  torch.cuda.manual_seed(8)\n  np.random.seed(8)\n  random.seed(8)\n\n\n\n\n  device = torch.device(\"cpu\")\n\n  dbfile = open('training.pickle', 'rb')      \n  db = pickle.load(dbfile) \n  X_train,y_train = db['X_tr'],db['y_tr']\n\n  X = []\n  #X_train=np.reshape(np.array(X_train),[len(X_train),])\n  for img in list(range(0,len(X_train))):\n    if X_train[img].ndim>=3:\n        X.append(np.moveaxis(cv2.resize(X_train[img][:,:,:3], (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n    else:\n        smimg= cv2.cvtColor(X_train[img],cv2.COLOR_GRAY2RGB)\n        X.append(np.moveaxis(cv2.resize(smimg, (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n    \n    if y_train[img]=='2COVID':\n        y_train[img]=1\n    elif y_train[img]=='1NonCOVID' :\n        y_train[img]=0\n    else:\n        continue\n\n  x = np.array(X)\n  y_train = np.array(y_train)\n\n\n  outputs_all = []\n  labels_all = []\n\n  X_train, X_val, y_train, y_val = train_test_split(x, y_train, test_size=0.2, random_state=2)\n\n  #base_model = tf.keras.applications.VGG19(weights = 'imagenet')\n  #model = Model(inputs=base_model.input,outputs = base_model.get_layer('block5_pool').output)\n  base_model = tf.keras.applications.ResNet50(weights = 'imagenet')\n  model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n  #base_model = tf.keras.applications.DenseNet121(weights='imagenet')\n  #model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n  #base_model = tf.keras.applications.InceptionResNetV2(weights='imagenet')\n  #model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n\n  feature_dict={0:[],1:[]}\n  feature_dict_val={0:[],1:[]}\n  classes=['0','1']\n  for j in range(len(X_train)):\n    xx = X_train[j].transpose(2,1,0)\n    xx = np.expand_dims(xx,axis=0)\n  \n    feature_dict[y_train[j]].append(model.predict(xx))\n\n\n  for j in range(len(X_val)):\n    xx = X_val[j].transpose(2,1,0)\n    xx = np.expand_dims(xx,axis=0)\n  \n    feature_dict_val[y_val[j]].append(model.predict(xx))\n\n\n  pickle.dump(feature_dict, open('features_till_flatten.pkl', \"wb\" ))\n  pickle.dump(feature_dict_val, open('features_till_flatten_val.pkl', \"wb\" ))\n\n\n\n  labels = []\n  feature_list = []\n  feature_dict=pickle.load(open('features_till_flatten.pkl','rb'))\n  names = [0,1]\n  for cat,features  in feature_dict.items():\n      labels.extend([int(cat)] * len(features))\n      feature_list.append(features)\n  feature_list = np.concatenate(feature_list)\n  feature_list1=[]\n  for i in range(0,len(feature_list)):\n      feature_list1.append(feature_list[i].ravel())\n  \n\n  labels2 = []\n  feature_list_val = []\n  feature_dict_val=pickle.load(open('features_till_flatten_val.pkl','rb'))\n  names = [0,1]\n  for cat,features  in feature_dict_val.items():\n      labels2.extend([int(cat)] * len(features))\n      feature_list_val.append(features)\n  feature_list_val = np.concatenate(feature_list_val)\n  feature_list2=[]\n  for i in range(0,len(feature_list_val)):\n      feature_list2.append(feature_list_val[i].ravel())\n\n  scaler = StandardScaler().fit(feature_list1)\n  rescaledX = scaler.transform(feature_list1)\n  rescaledX_validation = scaler.transform(feature_list2)\n  \n  return rescaledX, rescaledX_validation, labels, labels2 \n","fdaea86c":"rescaledX, rescaledX_validation, labels,labels2 = data_prep()","3a1eb23c":"root_folder = '\/kaggle\/input\/large-covid19-ct-slice-dataset\/curated_data\/curated_data'\ndiction = {}\n\n\n\ndir_images = []\ndir_label = []\n\ncount_covid = 0\ncount_non_covid = 0\n\nfor subdir in os.listdir(root_folder)[1:]:\n      print('subdir',os.path.join(root_folder, subdir))\n      for file_count, file_name in enumerate( sorted(glob(os.path.join(root_folder, subdir)+ '\/*'),key=len) ):\n\n          img = cv2.imread(file_name)\n          img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n          dir_images.append(img)\n          dir_label.append(subdir)\n          if subdir =='1NonCOVID':\n              count_non_covid +=1\n              if count_non_covid ==1500:\n                break\n          elif subdir == '2COVID':\n              count_covid +=1\n              if count_covid == 1500:\n                break\n\n\ndiction_train = {}\ndiction_test = {}\ndir_label_shuf_train = []\ndir_images_shuf_train = []\nindex_shuf = list(range(len(dir_label)))\nshuffle(index_shuf)\n\ntrain_shuf = index_shuf[0:int(len(index_shuf))]\n\n\nfor i in train_shuf:\n  dir_label_shuf_train.append(dir_label[i])\n  dir_images_shuf_train.append(dir_images[i])    \n\ndiction_train['X_tr'] = dir_images_shuf_train\ndiction_train['y_tr'] = dir_label_shuf_train\n\n\n\n\n\nprint(len(diction_train['X_tr']))\n##print(diction_train['y_tr'])\nprint(diction_train['X_tr'][0].shape)\n\n\n\n\nwith open('training.pickle', 'wb') as handle:\n  pickle.dump(diction_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n\n\nnrows=224\nncolumns=224\nchannels=1\n\nbatch_size=16\nepochs=1\n# Number of classes\nnum_cpu = multiprocessing.cpu_count()\nnum_classes = 2\ntorch.manual_seed(8)\ntorch.cuda.manual_seed(8)\nnp.random.seed(8)\nrandom.seed(8)\n\n\n\n\ndevice = torch.device(\"cpu\")\n\ndbfile = open('training.pickle', 'rb')      \ndb = pickle.load(dbfile) \nX_train,y_train = db['X_tr'],db['y_tr']\n\nX = []\n#X_train=np.reshape(np.array(X_train),[len(X_train),])\nfor img in list(range(0,len(X_train))):\n    if X_train[img].ndim>=3:\n        X.append(np.moveaxis(cv2.resize(X_train[img][:,:,:3], (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n    else:\n        smimg= cv2.cvtColor(X_train[img],cv2.COLOR_GRAY2RGB)\n        X.append(np.moveaxis(cv2.resize(smimg, (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n\n    if y_train[img]=='2COVID':\n        y_train[img]=1\n    elif y_train[img]=='1NonCOVID' :\n        y_train[img]=0\n    else:\n        continue\n\nx = np.array(X)\ny_train = np.array(y_train)\n\n\noutputs_all = []\nlabels_all = []\n\nX_train, X_val, y_train, y_val = train_test_split(x, y_train, test_size=0.2, random_state=2)\n\n#base_model = tf.keras.applications.VGG19(weights = 'imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('block5_pool').output)\nbase_model = tf.keras.applications.ResNet50(weights = 'imagenet')\nmodel = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n#base_model = tf.keras.applications.DenseNet121(weights='imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n#base_model = tf.keras.applications.InceptionResNetV2(weights='imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n\nfeature_dict={0:[],1:[]}\nfeature_dict_val={0:[],1:[]}\nclasses=['0','1']\nfor j in range(len(X_train)):\n    xx = X_train[j].transpose(2,1,0)\n    xx = np.expand_dims(xx,axis=0)\n\n    feature_dict[y_train[j]].append(model.predict(xx))\n\n\nfor j in range(len(X_val)):\n    xx = X_val[j].transpose(2,1,0)\n    xx = np.expand_dims(xx,axis=0)\n\n    feature_dict_val[y_val[j]].append(model.predict(xx))\n\n\npickle.dump(feature_dict, open('features_till_flatten.pkl', \"wb\" ))\npickle.dump(feature_dict_val, open('features_till_flatten_val.pkl', \"wb\" ))\n\n\n\nlabels = []\nfeature_list = []\nfeature_dict=pickle.load(open('features_till_flatten.pkl','rb'))\nnames = [0,1]\nfor cat,features  in feature_dict.items():\n  labels.extend([int(cat)] * len(features))\n  feature_list.append(features)\nfeature_list = np.concatenate(feature_list)\nfeature_list1=[]\nfor i in range(0,len(feature_list)):\n  feature_list1.append(feature_list[i].ravel())\n\n\nlabels2 = []\nfeature_list_val = []\nfeature_dict_val=pickle.load(open('features_till_flatten_val.pkl','rb'))\nnames = [0,1]\nfor cat,features  in feature_dict_val.items():\n  labels2.extend([int(cat)] * len(features))\n  feature_list_val.append(features)\nfeature_list_val = np.concatenate(feature_list_val)\nfeature_list2=[]\nfor i in range(0,len(feature_list_val)):\n  feature_list2.append(feature_list_val[i].ravel())\n\nscaler = StandardScaler().fit(feature_list1)\nrescaledX = scaler.transform(feature_list1)\nrescaledX_validation = scaler.transform(feature_list2)","fffcb8f6":"train_img = []\nfor i,j in enumerate(train_images):\n    xx = j.transpose(2,0,1)\n    train_img.append(xx)\ntrain_img = np.array(train_img)\n\nnrows = 224\nncolumns = 224\nX = []\n#X_train=np.reshape(np.array(X_train),[len(X_train),])\nfor img in list(range(0,len(train_img))):\n    if train_img[img].ndim>=3:\n        X.append(np.moveaxis(cv2.resize(train_img[img][:,:,:3], (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n    else:\n        smimg= cv2.cvtColor(train_img[img],cv2.COLOR_GRAY2RGB)\n        X.append(np.moveaxis(cv2.resize(smimg, (nrows,ncolumns),interpolation=cv2.INTER_CUBIC), -1, 0))\n\n   \n\nx = np.array(X)\nX_train = x\n\n\noutputs_all = []\nlabels_all = []\n\n\n\n#base_model = tf.keras.applications.VGG19(weights = 'imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('block5_pool').output)\nbase_model = tf.keras.applications.ResNet50(weights = 'imagenet')\nmodel = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n#base_model = tf.keras.applications.DenseNet121(weights='imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n#base_model = tf.keras.applications.InceptionResNetV2(weights='imagenet')\n#model = Model(inputs=base_model.input,outputs = base_model.get_layer('avg_pool').output)\n\nfeature_dict={0:[]}\n\n\nfor j in range(len(X_train)):\n    xx = X_train[j].transpose(2,1,0)\n    xx = np.expand_dims(xx,axis=0)\n\n    feature_dict[0].append(model.predict(xx))\n\n\n\n\npickle.dump(feature_dict, open('features_till_flatten.pkl', \"wb\" ))\n\n\n\n\n\nfeature_list = []\nfeature_dict=pickle.load(open('features_till_flatten.pkl','rb'))\n\nfor cat,features  in feature_dict.items():\n  #labels.extend([int(cat)] * len(features))\n  feature_list.append(features)\nfeature_list = np.concatenate(feature_list)\nfeature_list1=[]\nfor i in range(0,len(feature_list)):\n  feature_list1.append(feature_list[i].ravel())\n\n\n\n\nscaler = StandardScaler().fit(feature_list1)\nrescaledX_train = scaler.transform(feature_list1)\n\n\n","d0254020":"rescaledX_train.shape","e5fa56a1":"from sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier","34f299c4":"clf_model = XGBClassifier()","1a3b5912":"clf_model.fit(rescaledX,labels)","91241fa9":"preds_train = clf_model.predict(rescaledX_train)","a9b9588a":"np.savetxt(\"results.csv\", preds_train, delimiter=\",\")","cc046a89":"train_images.shape","cd7f05a6":"import gc\ngc.collect()","9795a933":"preds_train.shape","3e0ac4b1":"import tensorflow\n\nimport albumentations\nimport cv2\n\nSOURCE_SIZE = 512\nTARGET_SIZE = 256\n\ntrain_augs = albumentations.Compose([\n    albumentations.Rotate(limit=360, p=0.9, border_mode=cv2.BORDER_REPLICATE),\n    albumentations.RandomSizedCrop((int(SOURCE_SIZE * 0.75), SOURCE_SIZE), \n                                   TARGET_SIZE, \n                                   TARGET_SIZE, \n                                   interpolation=cv2.INTER_NEAREST),\n    albumentations.HorizontalFlip(p=0.5),\n\n])\n\nval_augs = albumentations.Compose([\n    albumentations.Resize(TARGET_SIZE, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n])","c5d33fca":"class Dataset:   \n    def __init__(\n            self, \n            images, \n            masks,\n            augmentations=None\n    ):\n        self.images = images\n        self.masks = masks\n        self.augmentations = augmentations\n    \n    def __getitem__(self, i):\n        image = self.images[i]\n        mask = self.masks[i]\n        \n        if self.augmentations:\n            sample = self.augmentations(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        return image, mask\n        \n    def __len__(self):\n        return len(self.images)\n    \n    \nclass Dataloder(tensorflow.keras.utils.Sequence):\n    \"\"\"Load data from dataset and form batches\n    \n    Args:\n        dataset: instance of Dataset class for image loading and preprocessing.\n        batch_size: Integet number of images in batch.\n        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size=1, shuffle=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n\n        self.on_epoch_end()\n\n    def __getitem__(self, i):\n        \n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        images = []\n        masks = []\n        for j in range(start, stop):\n            image, mask = self.dataset[self.indexes[j]]\n            images.append(image)\n            masks.append(mask)\n        \n        images = np.stack(images, axis=0)\n        masks = np.stack(masks, axis=0).astype(np.float32)\n        \n        return (images, masks)\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return len(self.indexes) \/\/ self.batch_size\n    \n    def on_epoch_end(self):\n        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)\n            \ntrain_dataset = Dataset(train_images, train_masks, train_augs)\nval_dataset = Dataset(val_images, val_masks, val_augs)\n\ntrain_dataloader = Dataloder(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = Dataloder(val_dataset, batch_size=batch_size, shuffle=False)","fb84539f":"assert train_dataloader[0][0].shape == (batch_size, TARGET_SIZE, TARGET_SIZE, 1)\nassert train_dataloader[0][1].shape == (batch_size, TARGET_SIZE, TARGET_SIZE, 4)","d7cdf750":"visualize(*next(iter(train_dataloader)))\nvisualize(*next(iter(val_dataloader)))","1d57bfca":"def fscore_glass(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 0:1], \n                               y_pred[..., 0:1])\n    \ndef fscore_consolidation(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 1:2], \n                               y_pred[..., 1:2])\n\ndef fscore_lungs_other(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 2:3], \n                               y_pred[..., 2:3])\n\ndef fscore_glass_and_consolidation(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., :2], \n                               y_pred[..., :2])","dc7b19ea":"from segmentation_models import Unet\nimport segmentation_models as sm\n\nfrom tensorflow.keras.layers import Input, Conv2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nnp.random.seed(0)\n\nbase_model = Unet(backbone_name='efficientnetb1',\n                  encoder_weights='imagenet',\n                  classes=4, \n                  activation='softmax')\n\nmodel = Sequential([Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n                    Conv2D(3, (1, 1)),  # map N channels data to 3 channels\n                    base_model])\n    \nmodel.compile(Adam(learning_rate=0.001, amsgrad=True),\n              loss=sm.losses.categorical_crossentropy,\n              metrics=[fscore_glass, fscore_consolidation, fscore_lungs_other, fscore_glass_and_consolidation])\n\ncheckpoint_callback = ModelCheckpoint('best_model',\n                                      monitor='fscore_glass_and_consolidation',\n                                      mode='max',\n                                      save_best_only=True)\n\nmodel.fit(\n    train_dataloader,\n    steps_per_epoch=len(train_dataloader) * 6,\n    epochs=20,\n    validation_data=val_dataloader,\n    validation_steps=len(val_dataloader),\n    callbacks=[checkpoint_callback],\n    workers=4)","5ad3eee4":"TARGET_SIZE","ffce80ee":"from segmentation_models import Unet\nimport segmentation_models as sm\n\nfrom tensorflow.keras.layers import Input, Conv2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nnp.random.seed(0)","49596328":"del train_images\ndel train_masks","76707da6":"model = tensorflow.keras.models.load_model('best_model\/',\n                                           compile=False,\n                                           custom_objects={\n                                                'categorical_crossentropy': sm.losses.categorical_crossentropy,\n                                                'fscore_consolidation': fscore_consolidation, \n                                                'fscore_glass': fscore_glass, \n                                                'fscore_lungs_other': fscore_lungs_other,\n                                                'fscore_glass_and_consolidation': fscore_glass_and_consolidation})\n\nmodel.compile(Adam(learning_rate=0.001, amsgrad=True),\n              loss=sm.losses.jaccard_loss)","96e675a5":"import gc\ngc.collect()","186dbdb3":"print(tf.__version__)","8d468d2e":"!pip install tensorflow==2.5.0\n","e45db998":"import tensorflow as tf\nfrom tensorflow import keras","073f7bc0":"\nfrom keras.utils import get_custom_objects\nfrom keras import backend as K\nfrom keras.layers import Activation\n\ndef swish_activation(x):\n        return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish_activation': Activation(swish_activation)})","eda37b35":"from tensorflow import keras","5735c6f0":"from keras.backend import sigmoid\n  \ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))","39b014e6":"import os\nfrom tensorflow.keras.preprocessing import image\nfrom tf.keras.activations import swish\n\npb_model_dir = \"best_model\/\"\nh5_model = \"mymodel.h5\"\n\n# Loading the Tensorflow Saved Model (PB)\ninput_shape = (1,256,256,1)\n\nmodel = tf.keras.models.load_model(pb_model_dir)\nmodel.build(input_shape)\nprint(model.summary())\n\n# Saving the Model in H5 Format\ntf.keras.models.save_model(model, h5_model)\n\n# Loading the H5 Saved Model\nloaded_model_from_h5 = tf.keras.models.load_model(h5_model)\nprint(loaded_model_from_h5.summary())","265f5767":"input = val_dataloader[0]\nimage_batch, mask_batch = input\n\npreds = model.predict_on_batch(image_batch)\nvisualize(image_batch, mask_batch, pred_batch=preds)\n\n# yellow is TP, red is FP, green is FN","1daf273f":"image_batch = np.stack([val_augs(image=img)['image'] for img in test_images_medseg], axis=0)\ntest_preds = model.predict_on_batch(image_batch)\ntest_masks_prediction = test_preds > 0.5\nvisualize(image_batch, test_masks_prediction, num_samples=len(test_images_medseg))","021a5482":"plt.imshow(test_images_medseg[2,:,:,0])","1062b53a":"plt.imshow(test_preds[2,:,:,0])","5d5d63bd":"import scipy\ntest_masks_prediction_original_size = scipy.ndimage.zoom(test_masks_prediction[..., :-2], (1, 2, 2, 1), order=0)\ntest_masks_prediction_original_size.shape","76486196":"import pandas as pd\n\npd.DataFrame(\n             data=np.stack((np.arange(len(test_masks_prediction_original_size.ravel())), \n                            test_masks_prediction_original_size.ravel().astype(int)),\n                            axis=-1), \n             columns=['Id', 'Predicted'])\\\n.set_index('Id').to_csv('submission.csv')","4daa1955":"# training images}","c226d84a":"## Split train \/ val:","d00a6b1e":"## Test preds:","3757d424":"### Preprocess images:","f5ea8fec":"### Images from medseg are individual axial slices:\nClasses are same as in radiopedia part","5bbedc9b":"## Data generator and augmentations:","2dad98c1":"### Images from radiopedia are full CT volumes:\nClass 0 is \"ground glass\"<br>\nClass 1 is \"consolidations\"<br>\nClass 2 is \"lungs other\" \u2013 it doesn't mean that it is healthy lungs (you don't need to predict this class)<br>\nClass 3 is \"background\" \u2013 not lungs (you don't need to predict this class)<br>","94c3c6ee":"### Test images from medseg are individual axial slices:\nYou should make predictions for class 0 \"ground glass\" and class 1 \"consolidation\"","0e875595":"## Resize prediction to original size:","b9ab2287":"### Plot images hists:\nHU (Hounsfield scale) of radiopedia data (blue) vs medseg data (orange):","233d5784":"# Training masks","32f09e74":"HU (Hounsfield scale) of test medseg data (blue) vs medseg data (orange):","3223b9fd":"### Images from radiopedia are full CT volumes:\nClass 0 is \"ground glass\"<br>\nClass 1 is \"consolidations\"<br>\nClass 2 is \"lungs other\" \u2013 it doesn't mean that it is healthy lungs (you don't need to predict this class)<br>\nClass 3 is \"background\" \u2013 not lungs (you don't need to predict this class)<br>","fc81bf05":"## Load best model and visualize predicions on val:","6ff13b62":"## Metrics:","7f056932":"\nNormalized values of test medseg data (blue) vs medseg data (orange):","1e216731":"Normalized values of radiopedia data (blue) vs medseg data (orange):"}}