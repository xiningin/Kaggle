{"cell_type":{"08d2b46f":"code","534ad16e":"code","eb90c4ad":"code","6aa65650":"code","78884eda":"code","29409268":"code","a860c096":"code","9676a3a6":"code","544d56f6":"code","34a0b458":"code","f7f0aba2":"code","5e3e0402":"code","a78c7171":"code","a198fc27":"code","5acd9b25":"markdown"},"source":{"08d2b46f":"import pandas as pd\nimport numpy as np\nimport pandas_datareader as pdr\n#import talib\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nimport tensorflow as tf\nimport random as python_random\n\n\n#kaggle\uc5d0\uc11c \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\ndf = pd.read_csv('..\/input\/dataframe\/df_0.csv',\n                 index_col = 'Date',\n                 parse_dates = True)\ndf = df.fillna(method='ffill')\ndf\n","534ad16e":"# \ud2b9\uc131\ubaa9\ub85d\nfeature1_list = ['Open','High','Low','Adj Close','Volume','log_return']\nfeature2_list = ['RASD5','RASD10','ub','lb','CCI','ATR',\n                 'MACD','MA5','MA10','MTM1','MTM3','ROC','WPR']\nfeature3_list = ['KOSPI', 'SOX']\nfeature4_list = ['next_rtn']\nall_features = feature1_list + feature2_list + feature3_list + feature4_list\n\n#train, validation, test set \ntrain_from = '2021-01-04'\ntrain_to = '2021-05-21'\n\nval_from = '2021-05-24'\nval_to = '2021-07-05'\n\ntest_from = '2021-07-06'\ntest_to = '2021-08-20'\n\ntrain_df = df.loc[train_from:train_to,all_features].copy()\nval_df = df.loc[val_from:val_to,all_features].copy()\ntest_df = df.loc[test_from:test_to,all_features].copy()\n\n#\uc815\uaddc\ud654\ndef min_max_normal(tmp_df):\n    eng_list = []\n    sample_df = tmp_df.copy()\n    for x in all_features:\n        if x in feature4_list :\n            continue\n        series = sample_df[x].copy()\n        values = series.values\n        values = values.reshape((len(values), 1))\n        # train the normalization\n        scaler = MinMaxScaler(feature_range=(0, 1))\n        scaler = scaler.fit(values)\n#         print('columns : %s , Min: %f, Max: %f' % (x, scaler.data_min_, scaler.data_max_))\n        # normalize the dataset and print\n        normalized = scaler.transform(values)\n        new_feature = '{}_normal'.format(x)\n        eng_list.append(new_feature)\n        sample_df[new_feature] = normalized\n    return sample_df, eng_list\n\ntrain_sample_df, eng_list =  min_max_normal(train_df)\nval_sample_df, eng_list =  min_max_normal(val_df)\ntest_sample_df, eng_list = min_max_normal(test_df)\n\ntrain_sample_df.head()\nval_sample_df.head()\ntest_sample_df.head()\n\n#\ud559\uc2b5 \ub370\uc774\ud130\uc640 \ub808\uc774\ube14 \ub370\uc774\ud130 \ubd84\ub9ac\nnum_step = 5\nnum_unit = 200\ndef create_dateset_binary(data, feature_list, step, n):\n    train_xdata = np.array(data[feature_list[0:n]])\n\n    m = np.arange(len(train_xdata) - step)\n    x, y = [], []\n    for i in m:\n        a = train_xdata[i:(i+step)]\n        x.append(a)\n    x_batch = np.reshape(np.array(x), (len(m), step, n))\n\n    train_ydata = np.array(data[[feature_list[n]]])\n    for i in m + step :\n        next_rtn = train_ydata[i][0]\n        if next_rtn > 0 :\n            label = 1\n        else :\n            label = 0\n        y.append(label)\n    y_batch = np.reshape(np.array(y), (-1,1))\n    return x_batch, y_batch\n\n    eng_list = eng_list + feature4_list\nn_feature = len(eng_list)-1\n\nx_train, y_train = create_dateset_binary(train_sample_df[eng_list], eng_list, num_step, n_feature)\nx_val, y_val = create_dateset_binary(val_sample_df[eng_list], eng_list, num_step, n_feature)\nx_test, y_test = create_dateset_binary(test_sample_df[eng_list], eng_list, num_step, n_feature)\n\nfrom tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train, 2)\ny_val = to_categorical(y_val, 2)\ny_test = to_categorical(y_test, 2)\n\n\nprint(pd.DataFrame(y_train).sum())\nprint(pd.DataFrame(y_val).sum())\nprint(pd.DataFrame(y_test).sum())\n\nx_train.shape[1]","eb90c4ad":"x_train.shape","6aa65650":"# LSTM \ubaa8\ub378 \uc0dd\uc131\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.layers import Input, Dense, LSTM\nfrom tensorflow.keras.layers import Activation, BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import regularizers\n\n\nK.clear_session()\ninput_layer = Input(batch_shape=(None, x_train.shape[1], x_train.shape[2]))\nlayer_lstm_1 = LSTM(num_unit, return_sequences = True, \n                    recurrent_regularizer = regularizers.l2(0.01))(input_layer)\nlayer_lstm_1 = BatchNormalization()(layer_lstm_1)\nlayer_lstm_2 = LSTM(num_unit, return_sequences = True, \n                    recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_1)\nlayer_lstm_2 = Dropout(0.25)(layer_lstm_2)\nlayer_lstm_3 = LSTM(num_unit, return_sequences = True, \n                    recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_2)\nlayer_lstm_3 = BatchNormalization()(layer_lstm_3)\nlayer_lstm_4 = LSTM(num_unit, return_sequences = True, \n                    recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_3)\nlayer_lstm_4 = Dropout(0.25)(layer_lstm_4)\nlayer_lstm_5 = LSTM(num_unit , recurrent_regularizer = regularizers.l2(0.01))(layer_lstm_4)\nlayer_lstm_5 = BatchNormalization()(layer_lstm_5)\noutput_layer = Dense(2, activation='sigmoid')(layer_lstm_5)\n\nmodel = Model(input_layer, output_layer)\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","78884eda":"#\ubaa8\ub378\ud559\uc2b5\ny_val.shape\n\nhistory = model.fit(x_train,\n                    y_train,epochs=20, \n                    batch_size=10, \n                    validation_data=(x_val, y_val))\n\ndef plot_history(history):\n    plt.figure(figsize=(15, 5))\n    ax = plt.subplot(1, 2, 1)\n    plt.plot(history.history[\"loss\"])\n    plt.title(\"Train loss\")\n    ax = plt.subplot(1, 2, 2)\n    plt.plot(history.history[\"val_loss\"])\n    plt.title(\"Test loss\")\n    plt.savefig('sample.png')\n\nplot_history(history)","29409268":"#\uacb0\uacfc\uc608\uce21\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\npredicted = model.predict(x_test)\ny_pred = np.argmax(predicted, axis=1)\nY_test = np.argmax(y_test, axis=1)\ncm = confusion_matrix(Y_test, y_pred)\nreport = classification_report(Y_test, y_pred,labels=np.unique(y_pred)) \n\ny_pred\n\ntn = cm[0][0]\nfn = cm[1][0]\ntp = cm[1][1]\nfp = cm[0][1]\nif tp == 0:\n    tp = 1\nif tn == 0:\n    tn = 1\nif fp == 0:\n    fp = 1\nif fn == 0:\n    fn = 1\nTPR = float(tp)\/(float(tp)+float(fn))\nFPR = float(fp)\/(float(fp)+float(tn))\naccuracy = round((float(tp) + float(tn))\/(float(tp) +\n                                          float(fp) + float(fn) + float(tn)), 3)\nspecitivity = round(float(tn)\/(float(tn) + float(fp)), 3)\nsensitivity = round(float(tp)\/(float(tp) + float(fn)), 3)\nmcc = round((float(tp)*float(tn) - float(fp)*float(fn))\/math.sqrt(\n    (float(tp)+float(fp))\n    * (float(tp)+float(fn))\n    * (float(tn)+float(fp))\n    * (float(tn)+float(fn))\n), 3)","a860c096":"cm","9676a3a6":"TPR\n","544d56f6":"FPR\n","34a0b458":"accuracy\n","f7f0aba2":"specitivity\n","5e3e0402":"sensitivity\n","a78c7171":"mcc","a198fc27":"#ROC curve\n#%matplotlib inline\nfrom sklearn.metrics import roc_curve, auc\n\n# Plot a confusion matrix.\n# cm is the confusion matrix, names are the names of the classes.\ndef plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(names))\n    plt.xticks(tick_marks, names, rotation=45)\n    plt.yticks(tick_marks, names)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\n# Plot an ROC. pred - the predictions, y - the expected output.\ndef plot_roc(pred,y):\n    fpr, tpr, _ = roc_curve(y, pred)\n    roc_auc = auc(fpr, tpr)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC)')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \nplot_roc(y_pred,Y_test)\nfrom sklearn.metrics import roc_auc_score\nroc_score = roc_auc_score(Y_test,y_pred)\nprint('ROC AUC \uac12 : {0:.4f}'.format(roc_score))\n\n#Drop & Batch\ny_pred\ntn = cm[0][0]\nfn = cm[1][0]\ntp = cm[1][1]\nfp = cm[0][1]\nif tp == 0:\n    tp = 1\nif tn == 0:\n    tn = 1\nif fp == 0:\n    fp = 1\nif fn == 0:\n    fn = 1\nTPR = float(tp)\/(float(tp)+float(fn))\nFPR = float(fp)\/(float(fp)+float(tn))\naccuracy = round((float(tp) + float(tn))\/(float(tp) +\n                                          float(fp) + float(fn) + float(tn)), 3)\nspecitivity = round(float(tn)\/(float(tn) + float(fp)), 3)\nsensitivity = round(float(tp)\/(float(tp) + float(fn)), 3)\nmcc = round((float(tp)*float(tn) - float(fp)*float(fn))\/math.sqrt(\n    (float(tp)+float(fp))\n    * (float(tp)+float(fn))\n    * (float(tn)+float(fp))\n    * (float(tn)+float(fn))\n), 3)\n\nf_output = open('binary_lstm_open_close_phase3_dropout_batch_Normal_3\ub2e8\uacc4 test.txt', 'a')\nf_output.write('=======\\n')\nf_output.write('{}epochs_{}batch\\n'.format(\n    20, 10))\nf_output.write('TN: {}\\n'.format(tn))\nf_output.write('FN: {}\\n'.format(fn))\nf_output.write('TP: {}\\n'.format(tp))\nf_output.write('FP: {}\\n'.format(fp))\nf_output.write('TPR: {}\\n'.format(TPR))\nf_output.write('FPR: {}\\n'.format(FPR))\nf_output.write('accuracy: {}\\n'.format(accuracy))\nf_output.write('specitivity: {}\\n'.format(specitivity))\nf_output.write(\"sensitivity : {}\\n\".format(sensitivity))\nf_output.write(\"mcc : {}\\n\".format(mcc))\nf_output.write(\"{}\".format(report))\nf_output.write('=======\\n')\nf_output.close()\n\n# \uc608\uce21\n# y_hat = model.predict(x_test, batch_size = 1)\nprint(len(y_test))\nprint(len(y_pred))\n\ntrain_sample_df['Adj Close'].plot()\ntest_sample_df['Adj Close'].plot()\n\nlstm_book_df = test_sample_df[['Adj Close','next_rtn']].copy()\nt1 = pd.DataFrame(data = y_pred,columns=['position'],index = lstm_book_df.index[5:])\nlstm_book_df = lstm_book_df.join(t1,how='left')\nlstm_book_df.fillna(0,inplace=True)\nlstm_book_df['ret'] = lstm_book_df['Adj Close'].pct_change()\nlstm_book_df['lstm_ret'] = lstm_book_df['next_rtn'] * lstm_book_df['position'].shift(1)\nlstm_book_df['lstm_cumret'] = (lstm_book_df['lstm_ret'] + 1).cumprod()\nlstm_book_df['bm_cumret'] = (lstm_book_df['ret'] + 1).cumprod()\n\nlstm_book_df[['lstm_cumret','bm_cumret']].plot()\n\n#\ubc31\ud14c\uc2a4\ud305\nhistorical_max = lstm_book_df['Adj Close'].cummax()\ndaily_drawdown = lstm_book_df['Adj Close'] \/ historical_max - 1.0\nhistorical_dd = daily_drawdown.cummin()\nhistorical_dd.plot()\n\n#BM\nCAGR = lstm_book_df.loc[lstm_book_df.index[-1],'bm_cumret'] ** (252.\/len(lstm_book_df.index)) -1\nSharpe = np.mean(lstm_book_df['ret']) \/ np.std(lstm_book_df['ret']) * np.sqrt(252.)\nVOL = np.std(lstm_book_df['ret']) * np.sqrt(252.)\nMDD = historical_dd.min()\nprint('CAGR : ',round(CAGR*100,2),'%')\nprint('Sharpe : ',round(Sharpe,2))\nprint('VOL : ',round(VOL*100,2),'%')\nprint('MDD : ',round(-1*MDD*100,2),'%')\n\n#LSTM\nCAGR = lstm_book_df.loc[lstm_book_df.index[-1],'lstm_cumret'] ** (252.\/len(lstm_book_df.index)) -1\nSharpe = np.mean(lstm_book_df['lstm_ret']) \/ np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\nVOL = np.std(lstm_book_df['lstm_ret']) * np.sqrt(252.)\nMDD = historical_dd.min()\nprint('CAGR : ',round(CAGR*100,2),'%')\nprint('Sharpe : ',round(Sharpe,2))\nprint('VOL : ',round(VOL*100,2),'%')\nprint('MDD : ',round(-1*MDD*100,2),'%')","5acd9b25":"**RNN(LSTM)\uc744 \ud65c\uc6a9\ud55c \uc8fc\uac00 \ubc29\ud5a5\uc131 \ubd84\ub958 \uc608\uce21**\n\n* Reference\n\ud000\ud2b8 \uc804\ub7b5\uc744 \uc704\ud55c \uc778\uacf5\uc9c0\ub2a5 \ud2b8\ub808\uc774\ub529 (\uae40\ud0dc\ud5cc \uc678, \ud55c\ube5b\ucd9c\ud310\uc0ac, 2020) ;\nApplication of Deep Learning to Algorithm Trading (2017) ;\nhttps:\/\/github.com\/quant4junior\/algoTrade\n\n"}}