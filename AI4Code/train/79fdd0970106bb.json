{"cell_type":{"cc34dcb5":"code","fd947f2f":"code","651b798a":"code","fd9b929b":"code","8aceb6e8":"code","44aceb7f":"code","dae145be":"code","ab6dadf8":"code","51f73bbb":"code","8c09295d":"code","309c8403":"code","fd91ebd7":"code","316d7abd":"code","1bb5f65e":"code","44514da3":"code","38afb7e3":"code","0e0e4839":"code","ce0a0ef9":"code","abb21c3c":"code","f7f53b25":"code","77758435":"code","66750e3a":"code","5e744fb1":"code","30eb7455":"code","9bfc3e88":"code","ae95089a":"code","1111178c":"code","58582913":"code","bdd0136d":"code","86020a84":"markdown","ed564499":"markdown"},"source":{"cc34dcb5":"import numpy as np \nimport pandas as pd  \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport nltk\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.cuda import amp\nimport transformers\nfrom transformers import BertTokenizer, BertModel, BertConfig\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nfrom collections import defaultdict\nfrom loguru import logger\nnltk.download('stopwords')\nstop_words = stopwords.words('english')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fd947f2f":"trtdata=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntstdata=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","651b798a":"trtdata.head(4)","fd9b929b":"trtdata.shape","8aceb6e8":"trtdata.isnull().sum() * 100 \/ len(trtdata)","44aceb7f":"#for test data element\ntstdata.head(7)","dae145be":"import missingno as msno\n%matplotlib inline\nmsno.matrix(trtdata,figsize=(10,5),fontsize=12,sort=\"ascending\",color=(0.50, 0.50, 0.50))","ab6dadf8":"fig, ax = plt.subplots(1, 1, figsize=(10, 6))\nsns.distplot(trtdata['target'], ax=ax,color=(0.20, 0.20, 0.20))\nplt.title(\"Target Distribution\",font=\"Serif\",size=\"18\",color=(0.70, 0.70, 0.50))\nplt.show()","51f73bbb":"fig, ax = plt.subplots(1, 1, figsize=(8, 6))\nsns.distplot(trtdata['standard_error'], ax=ax,color=(0.10, 0.10, 0.20))\nplt.title(\"Standard_error Distribution\",font=\"Serif\",size=\"18\")\nplt.show()","8c09295d":"sns.jointplot(x=trtdata['target'], y=trtdata['standard_error'], kind='hex',height=8,color=(0.30, 0.30, 0.30))\nplt.suptitle(\"Target vs Standard error \",font=\"Serif\",size=\"12\")\nplt.subplots_adjust(top=0.94)\nplt.show()","309c8403":"#before cleaning text data \nprint(trtdata.excerpt.min())","fd91ebd7":"text_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\ndef preprocess(x,stem=False):\n    x=re.sub(text_cleaning_re,'  ',str(x).lower()).strip()\n    tokens=[]\n    for token in x.split('\\n'):\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n        return '  '.join(tokens)\ntrtdata.excerpt=trtdata.excerpt.apply(lambda x:preprocess(x))","316d7abd":"#After cleaning text data|\nprint(trtdata.excerpt.min())","1bb5f65e":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntest_s = trtdata[\"excerpt\"].iloc[0]\ntest_s","44514da3":"result1 = tokenizer.encode_plus(test_s)\nresult1","38afb7e3":"from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold","0e0e4839":"from sklearn.metrics import mean_squared_error\nfrom sklearn import model_selection","ce0a0ef9":"def create_folds(data, num_splits):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)\n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n# return dataframe with folds\n    return data\ndf = create_folds(trtdata, num_splits=5)\ndf.head()","abb21c3c":"class CONFIG:\n    seed = 42\n    max_len = 205\n    train_batch_size = 64\n    valid_batch_size = 64\n    epochs = 10\n    learning_rate = 1e-4\n    n_accumulate = 1\n    folds = 10\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n    tokenizer.save_pretrained('.\/tokenizer')\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","f7f53b25":"class BERTDataset(Dataset):\n    def __init__(self, train, tokenizer, max_len):\n        self.text = train['excerpt'].values\n        self.target =train['target'].values\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    def __len__(self):\n        return len(self.text)\n    def __getitem__(self, index):\n        text = self.text[index]\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True)\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'target': torch.tensor(self.target[index], dtype=torch.float)}        ","77758435":"def criterion(outputs, targets):\n    return nn.MSELoss()(outputs, targets)","66750e3a":"class BERTClass(nn.Module):\n    def __init__(self):\n        super(BERTClass, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.fc = nn.Linear(768, 1)\n        self.dropout = nn.Dropout(p=0.3)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask = mask, \n                              token_type_ids = token_type_ids, \n                              return_dict=False)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n\nmodel = BERTClass()\nmodel.to(CONFIG.device);","5e744fb1":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    scaler = amp.GradScaler()\n    dataset_size = 0\n    running_loss = 0.0\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        batch_size = ids.size(0)\n        \n        with amp.autocast(enabled=True):\n            outputs = model(ids, mask, token_type_ids)\n            loss = criterion(outputs, targets)\n            loss = loss \/ CONFIG.n_accumulate\n            \n        scaler.scale(loss).backward()\n        \n        if (step + 1) % CONFIG.n_accumulate == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # zero the parameter gradients\n            optimizer.zero_grad()\n            \n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss\/dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    return epoch_loss","30eb7455":"@torch.no_grad()\ndef valid_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.eval()\n    dataset_size = 0\n    running_loss = 0.0\n    TARGETS = []\n    PREDS = []\n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype = torch.float)\n        batch_size = ids.size(0)\n        outputs = model(ids, mask, token_type_ids)\n        loss = criterion(outputs, targets)\n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        epoch_loss = running_loss\/dataset_size\n        PREDS.extend(outputs.cpu().detach().numpy().tolist())\n        TARGETS.extend(targets.cpu().detach().numpy().tolist())\n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n    gc.collect()\n    return epoch_loss, val_rmse","9bfc3e88":"#validation function\n@logger.catch\ndef run(model, optimizer, scheduler, device, num_epochs):    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_rmse = np.inf\n    history = defaultdict(list)\n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG.device, epoch=epoch)\n        valid_epoch_loss, valid_epoch_rmse = valid_one_epoch(model, optimizer, scheduler,\n                                                             dataloader=valid_loader, \n                                                             device=CONFIG.device, epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(valid_epoch_loss)\n        history['Valid RMSE'].append(valid_epoch_rmse)\n        \n        print(f'Valid RMSE: {valid_epoch_rmse}')\n        \n        # deep copy the model\n        if valid_epoch_rmse <= best_epoch_rmse:\n            print(f\"Validation RMSE Improved ({best_epoch_rmse} ---> {valid_epoch_rmse})\")\n            best_epoch_rmse = valid_epoch_rmse\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = \"Loss{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n            torch.save(model.state_dict(), PATH)\n            print(\"Model Saved\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_rmse))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history\ndef prepare_data(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = BERTDataset(df_train, CONFIG.tokenizer, CONFIG.max_len)\n    valid_dataset = BERTDataset(df_valid, CONFIG.tokenizer, CONFIG.max_len)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG.train_batch_size, \n                              num_workers=4, shuffle=True, pin_memory=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG.valid_batch_size, \n                              num_workers=4, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","ae95089a":"train_loader, valid_loader = prepare_data(fold=0)","1111178c":"# Defining Optimizer with weight decay to params other than bias and layer norms\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0001},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n     'weight_decay': 0.0}\n    ]  \n\noptimizer = AdamW(optimizer_parameters, lr=CONFIG.learning_rate)\n\n# Defining LR Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=len(train_loader)*CONFIG.epochs\n)\n# scheduler = None\n","58582913":"lrs = []\nfor epoch in range(1, CONFIG.epochs + 1):\n    if scheduler is not None:\n        scheduler.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\nplt.plot(lrs);","bdd0136d":"from datetime import time\nmodel, history = run(model, optimizer, scheduler=scheduler, device=CONFIG.device, num_epochs=CONFIG.epochs)","86020a84":"# Cleaning and preprocessing of the text data","ed564499":"# EDA of the text data"}}