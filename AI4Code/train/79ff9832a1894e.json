{"cell_type":{"72e57336":"code","e69158cc":"code","c01fef25":"code","dc9f9c4d":"code","8dad39b1":"code","2c7efaba":"code","215e24ac":"code","c163ded7":"code","0a2f0c7f":"code","86d80bb1":"code","9893fc9a":"code","ea6c1403":"code","14db2827":"code","66aa6c0a":"code","0ad46dcf":"code","0dc48c68":"code","ea997491":"code","cce78282":"code","029fb0ee":"code","9b1402cc":"code","e1c4903a":"code","5e23007a":"code","fec80e63":"code","2beb1e5f":"code","503e5397":"code","fd2de32b":"code","43c43957":"markdown","4d66d6fc":"markdown","e8d2aa63":"markdown","cb6630da":"markdown","81e92403":"markdown","6e36b4d6":"markdown","f8849cb6":"markdown","c3e49a52":"markdown","e80eb474":"markdown","69ca4d68":"markdown","7fc6a4b0":"markdown","ab714c34":"markdown","0af91c0d":"markdown","74e8149c":"markdown","9166667f":"markdown","677e6a81":"markdown","51d92ff7":"markdown","5890441e":"markdown","91157e23":"markdown","06d3bd6d":"markdown","f16ce0be":"markdown"},"source":{"72e57336":"!pip install tensorflow==1.14","e69158cc":"import tensorflow as tf","c01fef25":"tf.__version__","dc9f9c4d":"import numpy as np\nimport gym\nfrom tensorflow.contrib.layers import flatten, conv2d, fully_connected\nfrom collections import deque, Counter\nimport random\nimport datetime","8dad39b1":"color = np.array([210, 164, 74]).mean()\n\n\n#prepro (210, 160, 3) uint8 frame into 7040 (88x80) 1D float vector \n\ndef preprocess_observation(obs):\n\n    # Crop and resize the image\n    img = obs[25:201:2, ::2]\n\n    # Convert the image to greyscale\n    img = img.mean(axis=2)\n\n    # Improve image contrast\n    img[img==color] = 0\n\n    # Next we normalize the image from -1 to +1\n    img = (img - 128) \/ 128 - 1\n\n    return img.reshape(88,80)\n","2c7efaba":"pip install gym[atari]","215e24ac":"env = gym.make(\"SpaceInvaders-v0\")\nn_outputs = env.action_space.n\nprint(n_outputs)\nprint(env.env.get_action_meanings())\n\nobservation = env.reset()\n\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n\nfor i in range(22):\n  \n  if i > 20:\n    plt.imshow(observation)\n    plt.show()\n\n  observation, _, _, _ = env.step(1)\n  ","c163ded7":"#Run this cell for more observations.\nfor i in range(22):\n  \n  if i > 20:\n    plt.imshow(observation)\n    plt.show()\n\n  observation, _, _, _ = env.step(1)\n  ","0a2f0c7f":"#Take a look at the preprocessed inputs in greyscale\n#Let's compare the original and preprocessed tensors.\n\nobs_preprocessed = preprocess_observation(observation).reshape(88,80)\nplt.imshow(obs_preprocessed)\nplt.show()\nprint(observation.shape)\nprint(obs_preprocessed.shape)","86d80bb1":"#Frame Stacking, used to supply another minibatch within a minibatch, in order to judge motion. orginally in deepmind paper\n#Basically, as long as we always use stacks for all steps, and are consistent, there will not be any significant gaps in frequency\n\n\"\"\"\nWe have all these frames, but what gets fed as input to the \u201cQ-Network\u201d? Paper used\n a sequence of four game frames stacked together, making the data dimension (4,84,84). \n The idea is that the action agents choose depends on the prior sequence of game frames. \n Imagine playing Breakout, for instance. Is the ball moving up or down? \n If the ball is moving down, you better get the paddle in position to bounce it back up. \n If the thatball is moving up, you can wait a little longer or try to move in the opposite direction as needed if you think the ball will eventually reach there.\n\nDue to the way that Atari renders screens, every other frame may not aactually be rendered.\nThis is negatively affecting our performance, so instead, we take Deepmind's approach of element wise maxima\nCreate stack of 4, with 2 images combined via elementwise-maxima\n\n\"\"\"\n\n\nstack_size = 4 # We stack 4 composite frames in total\n\n# Initialize deque with zero-images one array for each image. Deque is a special kind of queue that deletes last entry when new entry comes in\nstacked_frames  =  deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n\ndef stack_frames(stacked_frames, state, is_new_episode):\n    # Preprocess frame\n    frame = preprocess_observation(state)\n    \n    if is_new_episode:\n        # Clear our stacked_frames\n        stacked_frames = deque([np.zeros((88,80), dtype=np.int) for i in range(stack_size)], maxlen=4)\n        \n        # Because we're in a new episode, copy the same frame 4x, apply elementwise maxima\n        maxframe = np.maximum(frame,frame)\n        stacked_frames.append(maxframe)\n        stacked_frames.append(maxframe)\n        stacked_frames.append(maxframe)\n        stacked_frames.append(maxframe)\n        \n        \n        \n        # Stack the frames\n        stacked_state = np.stack(stacked_frames, axis=2)\n        \n    else:\n        #Since deque append adds t right, we can fetch rightmost element\n        maxframe=np.maximum(stacked_frames[-1],frame)\n        # Append frame to deque, automatically removes the oldest frame\n        stacked_frames.append(maxframe)\n\n        # Build the stacked state (first dimension specifies different frames)\n        stacked_state = np.stack(stacked_frames, axis=2) \n    \n    return stacked_state, stacked_frames","9893fc9a":"tf.compat.v1.reset_default_graph()\n#Reset is technically not necessary if variables done  in TF2\n#https:\/\/github.com\/ageron\/tf2_course\/issues\/8\n\ndef q_network(X, name_scope):\n    \n    # Initialize layers\n    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=2.0)\n\n    with tf.compat.v1.variable_scope(name_scope) as scope: \n\n\n        # initialize the convolutional layers\n        layer_1 = conv2d(X, num_outputs=32, kernel_size=(8,8), stride=4, padding='SAME', weights_initializer=initializer) \n        tf.compat.v1.summary.histogram('layer_1',layer_1)\n        \n        layer_2 = conv2d(layer_1, num_outputs=64, kernel_size=(4,4), stride=2, padding='SAME', weights_initializer=initializer)\n        tf.compat.v1.summary.histogram('layer_2',layer_2)\n        \n        layer_3 = conv2d(layer_2, num_outputs=64, kernel_size=(3,3), stride=1, padding='SAME', weights_initializer=initializer)\n        tf.compat.v1.summary.histogram('layer_3',layer_3)\n        \n        # Flatten the result of layer_3 before feeding to the fully connected layer\n        flat = flatten(layer_3)\n        # Insert fully connected layer\n        fc = fully_connected(flat, num_outputs=128, weights_initializer=initializer)\n        tf.compat.v1.summary.histogram('fc',fc)\n        #Add final output layer\n        output = fully_connected(fc, num_outputs=n_outputs, activation_fn=None, weights_initializer=initializer)\n        tf.compat.v1.summary.histogram('output',output)\n        \n\n        # Vars will store the parameters of the network such as weights\n        vars = {v.name[len(scope.name):]: v for v in tf.compat.v1.get_collection(key=tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)} \n        #Return both variables and outputs together\n        return vars, output","ea6c1403":"epsilon = 0.5\neps_min = 0.05\neps_max = 1.0\neps_decay_steps = 500000\n\n#\ndef epsilon_greedy(action, step):\n    p = np.random.random(1).squeeze() #1D entries returned using squeeze\n    epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step\/eps_decay_steps) #Decaying policy with more steps\n    if p< epsilon:\n        return np.random.randint(n_outputs)\n    else:\n        return action","14db2827":"buffer_len = 20000\n#Buffer is made from a deque - double ended queue\nexp_buffer = deque(maxlen=buffer_len)","66aa6c0a":"def sample_memories(batch_size):\n    perm_batch = np.random.permutation(len(exp_buffer))[:batch_size]\n    mem = np.array(exp_buffer)[perm_batch]\n    return mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n","0ad46dcf":"num_episodes = 5  #1000+ is good but 2000 is preffered.\nbatch_size = 48\n\ninput_shape = (None, 88, 80, 1)\n#Recall shape is img.reshape(88,80,1)\nlearning_rate = 0.001\n#Modified for composite stacked frames\nX_shape = (None, 88, 80, 4)\ndiscount_factor = 0.97\n\nglobal_step = 0\ncopy_steps = 100\nsteps_train = 4\nstart_steps = 2000","0dc48c68":"logdir = 'logs'\ntf.compat.v1.reset_default_graph()\n\n# Now we define the placeholder for our input i.e game state\nX = tf.compat.v1.placeholder(tf.float32, shape=X_shape)\n\n# we define a boolean called in_training_model to toggle the training\nin_training_mode = tf.compat.v1.placeholder(tf.bool)","ea997491":"# we build our Q network, which takes the input X and generates Q values for all the actions in the state\nmainQ, mainQ_outputs = q_network(X, 'mainQ')\n\n# similarly we build our target Q network\ntargetQ, targetQ_outputs = q_network(X, 'targetQ')\n","cce78282":"# define the placeholder for our action values\n\nX_action = tf.compat.v1.placeholder(tf.int32, shape=(None,))\nQ_action = tf.reduce_sum(input_tensor=targetQ_outputs * tf.one_hot(X_action, n_outputs), axis=-1, keepdims=True)\n","029fb0ee":"copy_op = [tf.compat.v1.assign(main_name, targetQ[var_name]) for var_name, main_name in mainQ.items()]\ncopy_target_to_main = tf.group(*copy_op)\n","9b1402cc":"# define a placeholder for our output i.e action\ny = tf.compat.v1.placeholder(tf.float32, shape=(None,1))\n\n# now we calculate the loss which is the difference between actual value and predicted value\nloss = tf.reduce_mean(input_tensor=tf.square(y - Q_action))\n\n# we use adam optimizer for minimizing the loss\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\ntraining_op = optimizer.minimize(loss)\n\ninit = tf.compat.v1.global_variables_initializer()\n\nloss_summary = tf.compat.v1.summary.scalar('LOSS', loss)\nmerge_summary = tf.compat.v1.summary.merge_all()\nfile_writer = tf.compat.v1.summary.FileWriter(logdir, tf.compat.v1.get_default_graph())\n","e1c4903a":"with tf.compat.v1.Session() as sess:\n    init.run()\n    \n    # for each episode\n    history = []\n    for i in range(num_episodes):\n        done = False\n        obs = env.reset()\n        epoch = 0\n        episodic_reward = 0\n        actions_counter = Counter() \n        episodic_loss = []\n        #First step, begin stacking frames\n        \n        obs,stacked_frames= stack_frames(stacked_frames,obs,True)\n\n        # while the state is not the terminal state\n        while not done:\n\n           #Data generation using the untrained network\n        \n            \n\n            # feed the game screen and get the Q values for each action,  FEED THE NETWORK BY CALLING THE OUTPUT LAYER\n            \n            actions = mainQ_outputs.eval(feed_dict={X:[obs], in_training_mode:False})\n\n            # get the action\n            action = np.argmax(actions, axis=-1)\n            actions_counter[str(action)] += 1 \n\n            # select the action using epsilon greedy policy\n            action = epsilon_greedy(action, global_step)\n            \n            # now perform the action and move to the next state, next_obs, receive reward\n            next_obs, reward, done, _ = env.step(action)\n\n            #Begin stacking intra-episode code\n            next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n\n            # Store this transistion as an experience in the replay buffer! Quite important\n            exp_buffer.append([obs, action, next_obs, reward, done])\n            \n            # After certain steps, we train our Q network with samples from the experience replay buffer\n            if global_step % steps_train == 0 and global_step > start_steps:\n                #Our buffer should already contain everything preprocessed and stacked\n                # sample experience, mem[:,0], mem[:,1], mem[:,2], mem[:,3], mem[:,4]\n                o_obs, o_act, o_next_obs, o_rew, o_done = sample_memories(batch_size)\n\n                # states\n                o_obs = [x for x in o_obs]\n\n                # next states\n                o_next_obs = [x for x in o_next_obs]\n\n                # next actions\n                next_act = mainQ_outputs.eval(feed_dict={X:o_next_obs, in_training_mode:False})\n\n\n                # discounted reward: these are our Y-values\n                y_batch = o_rew + discount_factor * np.max(next_act, axis=-1) * (1-o_done) \n\n                # merge all summaries and write to the file\n                mrg_summary = merge_summary.eval(feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:False})\n                file_writer.add_summary(mrg_summary, global_step)\n\n                # To calculate the loss, we run the previously defined functions mentioned while feeding inputs\n                train_loss, _ = sess.run([loss, training_op], feed_dict={X:o_obs, y:np.expand_dims(y_batch, axis=-1), X_action:o_act, in_training_mode:True})\n                episodic_loss.append(train_loss)\n            \n            # after some interval we copy our main Q network weights to target Q network\n            if (global_step+1) % copy_steps == 0 and global_step > start_steps:\n                copy_target_to_main.run()\n                \n            obs = next_obs\n            epoch += 1\n            global_step += 1\n            episodic_reward += reward\n\n        next_obs=np.zeros(obs.shape)\n        exp_buffer.append([obs, action, next_obs, reward, done])\n        obs= env.reset()\n        obs,stacked_frames= stack_frames(stacked_frames,obs,True) \n        \n        history.append(episodic_reward)\n        print('Epochs per episode:', epoch, 'Episode Reward:', episodic_reward,\"Episode number:\", len(history))\n    \n\n","5e23007a":"plt.plot(history)\nplt.show()","fec80e63":"#Visualization cobe for running within Colab\n\n# Install dependencies first for graphics visualization within Colaboratory\n\n#remove \" > \/dev\/null 2>&1\" to see what is going on under the hood\n!pip install gym pyvirtualdisplay > \/dev\/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg > \/dev\/null 2>&1\n\n!apt-get update > \/dev\/null 2>&1\n!apt-get install cmake > \/dev\/null 2>&1\n!pip install --upgrade setuptools 2>&1\n!pip install ez_setup > \/dev\/null 2>&1\n\n","2beb1e5f":"#To Evaluate model on OpenAI gym, we will record a video via Ipython display\n\n\n\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\ngymlogger.set_level(40) #error only\n\nimport numpy as np\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\n\nfrom IPython import display as ipythondisplay\n\n\nfrom pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()","503e5397":"\"\"\"\nUtility functions to enable video recording of gym environment and displaying it\nTo enable video, just do \"env = wrap_env(env)\"\"\n\"\"\"\n\ndef show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","fd2de32b":"#Evaluate model on openAi GYM\n\n\nenvironment = wrap_env(gym.make('SpaceInvaders-v0'))\ndone = False\nobservation = environment.reset()\nnew_observation = observation\n\nprev_input = None\n\nwith tf.compat.v1.Session() as sess:\n    init.run()\n    observation, stacked_frames = stack_frames(stacked_frames, observation, True)\n    while True:\n       \n    \n        #set input to network to be difference image\n  \n        \n        #print(observation.shape)\n        \n        \n\n        # feed the game screen and get the Q values for each action\n        actions = mainQ_outputs.eval(feed_dict={X:[observation], in_training_mode:False})\n\n        # get the action\n        action = np.argmax(actions, axis=-1)\n        actions_counter[str(action)] += 1 \n\n        # select the action using epsilon greedy policy\n        action = epsilon_greedy(action, global_step)\n        environment.render()\n        new_observation, stacked_frames = stack_frames(stacked_frames, new_observation, False)\n        \n        observation = new_observation        \n        # now perform the action and move to the next state, next_obs, receive reward\n        new_observation, reward, done, _ = environment.step(action)\n        \n        if done: \n          #observation = env.reset()\n          break\n      \n    environment.close()\n    show_video()","43c43957":"Compute and optimize loss using gradient descent optimizer\n","4d66d6fc":"We are going to train our model. This is going to train for over 2000 episodes as it was found to be the optimal number. Feel free to stop anywhere in the middle by pressing STOP\n","e8d2aa63":"Now we define a preprocessing function for our input game screens. We crop the image size convert the image into greyscale 1D tensor. We do this in order to make our algorithm faster. This removes all the unnecessary information which will apply less load on our GPU.\n\nWe are going to convert our 210 * 160 * 3 sized array into a 2-Dimensional 80 * 80 array. We also convert our images from RGB to normal grayscale pictures as we dont need have any use for the colors.\n","cb6630da":"Next we define a function called epsilon_greedy for performing epsilon greedy policy.\n\nIn epsilon greedy policy we either select the best action with probability [1 - epsilon] or a random action with\nprobability [epsilon].\n\n\nWe use decaying epsilon greedy policy where value of epsilon will be decaying over time as we don't want to explore\nforever. So over time our policy will be exploiting only good actions.\n\n\n","81e92403":"This is the output after running the code for over 1000 episodes.\n\n![](https:\/\/media.giphy.com\/media\/kEj04tJ6wETz6hHDRK\/giphy.gif)","6e36b4d6":"## Evaluation\n\nFinally, let's visualize our agent's performance, and play a game within the gym environment itself.","f8849cb6":"Let's plot our reward distribution across the incremental episodes. Remember to interrupt the training process first.","c3e49a52":"**Enjoy the code and feel free the play around with it.\n**","e80eb474":"Now, we initialize our  buffer of length 20000 which holds the gameplay information in SARSA.\n\nWe store all the agent's experience i.e (state, action, rewards) in the experience replay buffer\nand  we sample from this minibatch of experience for generating the y-values for the update function, and hence train the network.\n\n","69ca4d68":"Importing all the required tools.","7fc6a4b0":" Now we start the tensorflow session and run the model,\n\n1. First, we preprocess and feed the game screen (state s) to our DQN, which will\nreturn the Q values of all possible actions in the state.\n2. Now we select an action using the epsilon-greedy policy: with the probability\nepsilon, we select a random action a and with probability 1-epsilon, we select an\naction that has a maximum Q value, such as .\n3. After selecting the action a, we perform this action in a state s and move to a new\nstate s' and receive a reward. The next state, s', is the preprocessed image of the\nnext game screen.\n4. We store this transition in our replay buffer as <s,a,r,s'>.\n5. Next, we sample some random batches of transitions from the replay buffer and\ncalculate the loss.\n\n6. We know that the loss is defined as the squared\ndifference between target Q and predicted Q.\n\n7. We perform gradient descent with respect to our actual network parameters in\norder to minimize this loss.\n\n8. Copy weights of training network to actual network\n9. Repeat for M steps\n\nEpochs here exist because we are suplying data in batches.","ab714c34":"Change the number of episodes to train it quicker, but the model won't be perfect.","0af91c0d":"Copy the primary Q network parameters to the target  Q network","74e8149c":"Let us initialize our gym environment, and take a look at some observations. Let's also inspect the size and type of action space in this game.\n\nInstalling atari environments that are used as part of OpenAI gym.","9166667f":"# **Optimizing Deep Q-learning for Automated Atari Space Invaders**\nSpace Invaders is a classic Atari game that we all have played once upon a time in our childhood. Today we are going to automate the game using the famous Reinforcement Learning algortihm **Deep Q learning**. \n\nThe process for acomplishing the above task is: \n1. We define our Deep Q-learning neural network. This is a CNN that takes in-game screen images and outputs the probabilities of each of the actions, or Q-values, in the Ms-Pacman gamespace. To acquire a tensor of probabilitieses, we do not include any activation function in our final layer.\n2. As Q-learning require us to have knowledge of both the current and next states, we need to start with data generation. We feed preprocessed input images of the game space, representing initial states s, into the network, and acquire the initial probability distribution of actions, or Q-values. Before training, these values will appear random and sub-optimal. Note that our preprocessing now includes stacking and composition as well.\n3. With our tensor of probabilities, we then select the action with the current highest probability using the argmax() function, and use it to build an epsilon greedy policy.\n4. Using our policy, we'll then select the action a, and evaluate our decision in the gym environment to receive information on the new state s', the reward r, and whether the episode has been finished.\n5. We store this combination of information in a buffer in the list form <s,a,r,s',d>, and repeat steps 2\u20134 for a preset number of times to build up a large enough buffer dataset.\n6. Once step 5 has finished,we move to generate our target y-values, R' and A', that are required for the loss calculation. While the former is simply discounted from R, we obtain the A' by feeding S' into our network. With all of our components in place, we can then calculate the loss to train our network.\n7. Once training has finished, we'll evaluate the performance of our agent graphically and through a demonstration\n\n\n","677e6a81":"This code is run using google colab as it has a lot of computational power and is free to use for every user. Google colab is similar to jupyter notebooks and offers many line to line interactive tools. Google colab,kaggle have default tensorflow version of TensorFlow 2.0 but this code was written using TensorFlow 1.0. For this reason we change the default to Tensotflow 1.0. \n","51d92ff7":" Now let us build our primary and target Q network We have two networks to allow for training and data generation to occur concurrently.\n Note that the network returns the weights, as well as network outputs. ","5890441e":"Next, we define a function called sample_memories for sampling experiences from the memory according to batches. Batch size is the number of experience sampled\nfrom the memory.\n","91157e23":"Okay, Now we define a function called q_network for building our Deep Q network.  <br>\nWe build Q network with three convolutional layers with same padding followed by a flattening, and a fully connected layer. ","06d3bd6d":"Now we define our network hyperparameters,","f16ce0be":"Run the above block again and again to see different outputs. \nThe longer you train the model the better your scores."}}