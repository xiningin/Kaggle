{"cell_type":{"3942a090":"code","f0a7d78a":"code","95adf19d":"code","006ea569":"code","2f7242bb":"code","72a3b73c":"code","437f92af":"code","3b39724f":"code","c840ff49":"code","6e27edfc":"code","710085d6":"code","8f5e047f":"code","c709db59":"code","cd39cc01":"code","60f29ecd":"code","7dd740f9":"code","39c23563":"code","f44cee49":"code","eabe1628":"code","9c5cc7e4":"code","95774ee9":"code","2b4e3a05":"code","7c76148b":"code","31131916":"code","002de32b":"code","278968ab":"code","48fc0a0c":"code","845f5d25":"code","daed656e":"code","cf63c499":"code","5bfd9d19":"code","8d89b288":"code","f2c1ab34":"code","2c6cc763":"code","fdcb22f9":"markdown","6cc70d2f":"markdown","48d7b6c4":"markdown","e3071e4b":"markdown","67e62245":"markdown","d2c02e51":"markdown","a440b0c8":"markdown","af08b45b":"markdown","030b6320":"markdown","5df171be":"markdown","92361619":"markdown","0ba85107":"markdown","bf0f2647":"markdown","69481486":"markdown","247f2599":"markdown","e236d7b3":"markdown","68d3242a":"markdown","0003be95":"markdown","de13e80a":"markdown"},"source":{"3942a090":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n#from pandas_profiling import ProfileReport\n#from autoviz.AutoViz_Class import AutoViz_Class as AVC\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score","f0a7d78a":"raw_data = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\nraw_data.head()","95adf19d":"raw_data.describe(include='all')","006ea569":"raw_data.isnull().sum()","2f7242bb":"raw_data['status'].value_counts()","72a3b73c":"raw_data['salary'] = raw_data['salary'].fillna(0)\nraw_data['salary'].isnull().sum()","437f92af":"raw_data['salary'].skew()","3b39724f":"sns.boxplot(raw_data['salary'])","c840ff49":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= raw_data, x = 'status')\nplt.xlabel('Status', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Placement Rate', fontsize = 18)\nplt.show()","6e27edfc":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= raw_data, hue = 'workex', x = 'status')\nplt.xlabel('Status', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Placement Rate based on Work Experience', fontsize = 18)\nplt.show()","710085d6":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= raw_data, hue = 'gender', x = 'status')\nplt.xlabel('Status', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Placement Rate by Gender', fontsize = 18)\nplt.show()","8f5e047f":"fig = plt.figure(figsize=(8,6))\nsns.violinplot(data= raw_data, x = 'gender', y = 'salary')\nplt.xlabel('Gender', fontsize = 14)\nplt.ylabel('Salary', fontsize = 14)\nplt.title('Salary distribution based on Gender', fontsize = 18)\nplt.show()","c709db59":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= raw_data, hue = 'degree_t', x = 'status')\nplt.xlabel('Status', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Placement Rate by Field of Degree', fontsize = 18)\nplt.show()","cd39cc01":"fig = plt.figure(figsize=(8,6))\nsns.violinplot(data= raw_data, x = 'degree_t', y = 'salary')\nplt.xlabel('Gender', fontsize = 14)\nplt.ylabel('Salary', fontsize = 14)\nplt.title('Salary distribution by on Field of Degree', fontsize = 18)\nplt.show()","60f29ecd":"fig = plt.figure(figsize=(8,6))\nsns.countplot(data= raw_data, hue = 'specialisation', x = 'status')\nplt.xlabel('Status', fontsize = 14)\nplt.ylabel('Count', fontsize = 14)\nplt.title('Placement Rate by MBA Specialisation', fontsize = 18)\nplt.show()","7dd740f9":"fig = plt.figure(figsize=(8,6))\nsns.violinplot(data= raw_data, x = 'specialisation', y = 'salary')\nplt.xlabel('Gender', fontsize = 14)\nplt.ylabel('Salary', fontsize = 14)\nplt.title('Salary distribution based on MBA specialisation', fontsize = 18)\nplt.show()","39c23563":"placement_data = raw_data.copy()\nplacement_data.head()","f44cee49":"X = placement_data.drop(['sl_no','status','salary'], axis = 1)\ny = placement_data['status']","eabe1628":"#Machine Learning models take just numbers so any string values we have in our data will have to be converted to numbers.\n\n#Using Column Transformer and One Hot Encoder rather than Label Encoder and One Hot Encoder as both give the same results.\n#Using this method is however more effcient since i use just two lines of code.\n\n#One Hot Encoder sorts the values for each column in ascending order and encodes each category based on this order. Eg male and \n#female, female will have a value of 1, 0 and male 0, 1. The output from One Hot Encoding puts the encoded columns first and \n#then the other columns that were not encoded.\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0, 2, 4, 5, 7, 8, 10])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))","9c5cc7e4":"print(X[:1])","95774ee9":"lab_enc = LabelEncoder()\ny = lab_enc.fit_transform(y)","2b4e3a05":"print(y[:5])","7c76148b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","31131916":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","002de32b":"models = [LogisticRegression(max_iter = 1500), \n          KNeighborsClassifier(),\n          SVC(kernel = 'linear'), \n          SVC(kernel = 'rbf'), \n          GaussianNB(), \n          DecisionTreeClassifier(), \n          RandomForestClassifier(), \n          XGBClassifier(),\n          LGBMClassifier(),\n          ExtraTreesClassifier()]\n\na, b, c, d = [], [], [], []\n\nfor i in models:\n    model = i.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    a.append(accuracy_score(y_test, y_pred))\n    b.append(f1_score(y_test, y_pred))\n    c.append(precision_score(y_test, y_pred))\n    d.append(recall_score(y_test, y_pred))\n    \nclass_metrics = pd.DataFrame([a, b, c, d], index = ['Accuracy','F1 Score','Precision','Recall'], \n                             columns = ['Logistic Reg','KNN','SVM','KSVM','Naive Bayes','Decision Tree','Random Forest', \n                                        'XGBoost','LGBM','Extra Trees'])\n\nclass_metrics.transpose().sort_values(by='Accuracy', ascending=False)","278968ab":"log_classifier = LogisticRegression(max_iter = 1500)\nlog_classifier.fit(X_train, y_train)","48fc0a0c":"log_pred = log_classifier.predict(X_test)","845f5d25":"log_cm = confusion_matrix(y_test, log_pred)\nprint(log_cm)\naccuracy_score(y_test, log_pred)","daed656e":"print(classification_report(y_test, log_pred))","cf63c499":"accuracies = cross_val_score(estimator = log_classifier, X = X_train, y = y_train, cv = 10)\n\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))","5bfd9d19":"log_classifier.get_params()","8d89b288":"importance = log_classifier.coef_[0]\nfor i, v in enumerate(importance):\n    print('Feature: %0d, Score:%.5f' % (i, v))\n#plotting feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","f2c1ab34":"placement_data.head(1)","2c6cc763":"print(X[:1])","fdcb22f9":"A lot more people were placed with companies. The ratio of the placement rate (being placed compared to not being placed) is about 2:1.","6cc70d2f":"Men were offered higher salaries than women. This is a prevalent issue in our society. Men are generally offered higher salaries than women.","48d7b6c4":"## Cross Validation","e3071e4b":"The positive scores indicate a feature that predicts class 1 while a negative score indicates a feature that predicts class 0.","67e62245":"Individuals with a degree in field of communications & management had a significant chance of getting placed compared to individuals with a degree in other fields (Science & Technology and Others). They also had a higher salary distribution as seen in the chart below.\nIndividuals with a degree in 'Others' field had the lowest chance of not getting placed as well as the lowest salary distribution.","d2c02e51":"People with an MBA specialisation in marketing & finance had a significant placement rate than people with an MBA specialisation in Marketing & HR. \nPeople with an MBA specialisation in Marketing & HR also had a higher chance of not being placed with companies compared to those with an MBA specialisation in marketing & finance. If at this time period, companies had a much higher demand for people with a degree in marketing & finance, it could lead to such individuals having a higher placement rate than those with a degree in marketing & HR. These individuals also had a much higher salary distribution than those with a specialisation in marketing & HR.","a440b0c8":"The skewness lies between -0.5 and 0.5, so the distribution of salary is approximately symmetric","af08b45b":"## Examining Feature Importance","030b6320":"The logistic regression and SVM models have the same values across all four metrics. They also have high accuracy, f1, precison and recall scores. I will be using Logistic regression on the dataset.","5df171be":"## Exploratory Data Analysis","92361619":"In addition to accuracy, f1-score, precison and recall can also be used to measure the classification model\nPrecison measure the ability of the model to not label positive values as negative.\nRecall is the ability of the model to find positive vales.\nF1-score is the weighted mean of precision and recall. The closer to 1 these values are, the better.","0ba85107":"The cross validation accuracy is close to the accuracy predicted by the logistic regression. This shows that the model did not overfit or underfit the data.","bf0f2647":"The boxplot shows that a lot of individuals were offered salaries in the range of 0k - 200k, with 0k representing those who were not placed with companies. ","69481486":"Work experience doesnt seem to have an impact on placement rate. Individuals with or without work experience were still placed with companies. However, individuals without work experience had  a higher chance of not being placed compared to those with work experience. Generally, a lot of other factors are often considered in addition to having work experience. For example, cultural fit, performance at the interview stage etc. ","247f2599":"#To get visualizations about the data. output is a variation of different charts eg scatter plots, histograms etc\navc = AVC()\ndata_viz = avc.AutoViz('C:\\\\Users\\\\Fabulous\\\\Downloads\\\\Data\\\\Placement_Data_Full_Class.csv')\ndata_viz","e236d7b3":"#To generate a quick indepth EDA analysis\n\nprofile = ProfileReport(raw_data, title = 'Pandas Profiling Report')\nprofile","68d3242a":"The variables that contribute heavily in the model are work experience (workex), employability test percentage (etest_p), mba post graduation specialisation (specialisation) and mba percentage (mba_p)","0003be95":"Men had a much significant placement rate with companies than women.","de13e80a":"1. The sl_no and salary columns were dropped for the machine learning models and one hot encoding was carried out on categorical columns\n2. Features 0 and 1 - the two gender categories (M,F)\n3. Feature 2 - ssc_p\n4. Features 3 and 4 - the two ssc_b categories (Others, Central)\n5. Feature 5 - hsc_p\n6. Features 6 and 7 - the two hsc_b categories (Others, Central)\n7. Features 8, 9 and 10 - the three hsc_s categories (Commerce, Science, Arts)\n8. Feature 11 - degree_p\n9. Features 12, 13 and 14 - three categories of degree_t (Comm & Mgmt, Sci & Tech, Others)\n10. Features 15 and 16 - two categories of workex (No, Yes)\n11. Feature 17 - etest_p\n12. Features 18 and 19 - two categories of specialisation ( Mkt & HR, Mkt & Fin)\n13. Feature 20 - mba_p"}}