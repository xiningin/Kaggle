{"cell_type":{"c7e113a7":"code","f08cae84":"code","e6180419":"code","6c57f16b":"code","d20411f1":"code","df6b3f1e":"code","109e9b75":"code","170a0b06":"code","012f519b":"code","86e7ef27":"code","e74eb312":"code","ea451e9b":"code","4cdc0680":"code","965df79c":"code","4381b1e3":"code","d8c62f8a":"code","05fa874c":"code","d37d349c":"code","ddab7c85":"code","235e9ab6":"code","521bf8cb":"code","c1700b55":"code","c049ee80":"code","f02532b1":"code","b72da55d":"code","0a65cb0b":"code","7369257b":"code","06eac73b":"code","be2ae3c1":"code","96fe213e":"code","d18e5bb8":"code","d2f9c440":"code","6cf62478":"code","b186ff5c":"code","5bc7d328":"code","352b2b0b":"code","b6df8305":"code","ac97c68d":"code","c7c91180":"code","b8060a1f":"code","6e86f3ad":"code","7608ca5d":"code","1f5d0390":"code","1617cccb":"code","cb1e3578":"code","ad638083":"code","93091813":"code","a6f5f1df":"code","9a2189ba":"code","4955291c":"code","22a91d4f":"code","8e3db47b":"code","ff108206":"code","4e9f6f4a":"code","0b502616":"code","e3f95913":"code","db6045a6":"markdown","f1f69643":"markdown","049215c6":"markdown","9455e906":"markdown","c1e3d527":"markdown","505b4423":"markdown","45fa7030":"markdown","6a97f3e8":"markdown","f82aef74":"markdown","a1df4ad0":"markdown","0456f9b7":"markdown","100e8998":"markdown","0cf7ffb6":"markdown"},"source":{"c7e113a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f08cae84":"\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head()","e6180419":"train[\"Survived\"].shape","6c57f16b":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","d20411f1":"print(train.shape)\n\nprint(test.shape)","df6b3f1e":"#Checking null values present on each column so that we can remove the unnecessary column\n\ntrain.isnull().sum()","109e9b75":"#Checking null values present on each column so that we can remove the unnecessary column\n\ntest.isnull().sum()","170a0b06":"#Removing column \"Cabin\" since it has many null values\n\ntrain.drop(['Cabin'], axis=1,inplace=True )\n\ntest.drop(['Cabin'], axis=1,inplace=True )","012f519b":"#Finding out missing values of \"Embarked\" and then filling them up by S\n\ntrain['Embarked'].value_counts()\ntrain['Embarked'].fillna('S', inplace=True)\n\n#test['Embarked'].value_counts()\n#test['Embarked'].fillna('S', inplace=True)","86e7ef27":"#Finding out missing values of \"Fair\" and fiiling them up by mean values\n\n#train['Fare'].fillna(train['Fare'].mean(),inplace=True)\n\ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)","e74eb312":"#Finding out missing values for \"Age\" of train\n#Creating a variable called train_age and storing the random values from  mean and std\n\ntrain_age=np.random.randint(train['Age'].mean() - train['Age'].std() , train['Age'].mean() + train['Age'].std(), 177)\n\ntrain_age","ea451e9b":"#Checking null values present in age of train\ntrain['Age'][train['Age'].isnull()]","4cdc0680":"#Replacing these null values by train_age\ntrain['Age'][train['Age'].isnull()]=train_age","965df79c":"train.isnull().sum()","4381b1e3":"#Finding out missing values for \"Age\" of test \n#Creating a variable called test_age and storing the random values from  mean and std\n\ntest_age=np.random.randint(train['Age'].mean() - train['Age'].std() , train['Age'].mean() + train['Age'].std(), 86)\n\ntest_age","d8c62f8a":"#Checking null values present in age of test\ntest['Age'][test['Age'].isnull()]","05fa874c":"#Replacing these null values by train_age\ntest['Age'][test['Age'].isnull()]=test_age","d37d349c":"test.isnull().sum()","ddab7c85":"#for Pclass\n\ntrain.groupby(['Pclass'])['Survived'].mean() #Therefore, Pclass can not be removed.","235e9ab6":"#For Sex\n\ntrain.groupby(['Sex'])['Survived'].mean() #Therefore, Sex matters too","521bf8cb":"#For Embarked\n\ntrain.groupby(['Embarked'])['Survived'].mean() #Therefore, Embarked matters too","c1700b55":"#For Age since its numerical data so we are plotting the graph \n\nsns.distplot(train['Age'][train['Survived']==0])\nsns.distplot(train['Age'][train['Survived']==1])\n\n#Therefore, Age matters too","c049ee80":"#For Fare\n\nsns.distplot(train['Fare'][train['Survived']==0])\nsns.distplot(train['Fare'][train['Survived']==1])\n\n#Therefore, Fair matters too","f02532b1":"#For Ticket, we have to remove it since it doesn't matter\n\ntrain.drop(['Ticket'], axis=1, inplace=True)\ntest.drop(['Ticket'], axis=1, inplace=True)","b72da55d":"test","0a65cb0b":"#For \"SibSp\" and \"Parch\", we are going to add these two columns to a new column called \"Family\" for both train and test dataset\n\ntrain['Family']=train['SibSp'] + train['Parch'] + 1\ntest['Family']=test['SibSp'] + test['Parch'] + 1\n\ntrain['Family'].value_counts()\ntest['Family'].value_counts()","7369257b":"#For Family\n\ntrain.groupby(['Family'])['Survived'].mean() #Therefore, Family matters too","06eac73b":"#Creating a separate column for people travelling alone, with 2 or more than 2 & less than 4 and with more than 11\n\ndef cal1(number):\n    if number==1:\n        return\"Alone\"\n    elif number>1 & number<5:\n        return\"Medium\"\n    else:\n        return\"Large\"\n    \ntrain['Family_size']=train['Family'].apply(cal1)\ntrain","be2ae3c1":"test","96fe213e":"#Creating a separate column for people travelling alone, with 2 or more than 2 & less than 4 and with more than 11\n\ndef cal2(number):\n    if number==1:\n        return\"Alone\"\n    elif number>1 & number<5:\n        return\"Medium\"\n    else:\n        return\"Large\"\n    \ntest['Family_size']=test['Family'].apply(cal2)\ntest","d18e5bb8":"#Removing columns like \"SibSp\", \"Parch\" and \"Family\"\n\ntrain.drop(['SibSp','Parch', 'Family'], axis=1, inplace=True)\ntest.drop(['SibSp','Parch', 'Family'], axis=1, inplace=True)","d2f9c440":"#We need \"PassengerId\" for test so storing it in \"passengerid\"\n\npassengerid=test['PassengerId'].values","6cf62478":"#We don't need passenger id for training the dataset we only need it while testing so removing \"PassengerId\" and \"Name\"\n\ntrain.drop(['PassengerId','Name'], axis=1, inplace=True)\ntest.drop(['PassengerId','Name'], axis=1, inplace=True)","b186ff5c":"train","5bc7d328":"test","352b2b0b":"#Converting categorical values into numerical values for train\n\ntrain=pd.get_dummies(columns=['Pclass','Sex','Embarked','Family_size'], drop_first=True, data=train)\ntrain","b6df8305":"#Converting categorical values into numerical values for train\n\ntest=pd.get_dummies(columns=['Pclass','Sex','Embarked','Family_size'], drop_first=True, data=test)\ntest","ac97c68d":"train.shape","c7c91180":"test.shape","b8060a1f":"X=train.iloc[:,1:].values\nprint(\"The shape of X:\",X.shape)\n\nY=train.iloc[:,0].values\nprint(\"The shape of Y:\",Y.shape)","6e86f3ad":"#Splitting\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2)\n\nprint(\"The shape of X_train:\",X_train.shape)\nprint(\"The shape of Y_train:\",Y_train.shape)\nprint(Y_test.shape)","7608ca5d":"classifier1=DecisionTreeClassifier() \nclassifier1.fit(X_train, Y_train)","1f5d0390":"#Predicting\nY_predict=classifier1.predict(X_test)\nprint(Y_predict.shape)\n\n#Accuracy\nAS1=accuracy_score(Y_test,Y_predict) \nprint(\"The accuracy score using decision tree classifier:\", AS1)","1617cccb":"# Using Grid-Search-CV class and Training the model\n#Creating Variable\nparam_dist={\"criterion\":[\"gini\",\"entropy\"],\n            \"max_depth\":[1,2,3,4,5,6,7,8,None],\n            \"max_features\":[1,2,3,4,5,6,7,None],\n            \"random_state\":[0,1,2,3,4,5,6,7,8,9,None],\n            \"max_leaf_nodes\":[0,1,2,3,4,5,6,7,8,9,None],\n            \"max_features\" : [\"auto\",\"sqrt\",\"log2\",None],\n            \"min_samples_leaf\" : [1,2,3,4,5,6,7,8,None],\n            \"min_samples_split\" : [1,2,3,4,5,6,7,8,None]}\n\n#Applying Grid-Search-CV\ngrid=GridSearchCV(classifier1, param_grid=param_dist, cv=10, n_jobs=-1)\n\n#Training the model after applying Grid-Search-CV\ngrid.fit(X_train,Y_train)\n","cb1e3578":"OHV=grid.best_params_ \nprint(\"The values of Optimal Hyperparameters are\",OHV)","ad638083":"Acc=grid.best_score_\nprint(\"The Accuracy Score is\",Acc)\nprint(\"Accuracy using DecisionTreeClassifier:\", Acc*100,\"%\")","93091813":"grid.best_estimator_","a6f5f1df":"classifier2=DecisionTreeClassifier(criterion= 'gini', max_depth= 7, max_features= 'auto', max_leaf_nodes= None, min_samples_leaf= 8, min_samples_split= 2, random_state= 4)\n\nclassifier2.fit(X_train, Y_train)","9a2189ba":"# Predicting\nY_predict=classifier2.predict(X_test)\nprint(Y_predict.shape)\n\n# Accuracy\nAS2=accuracy_score(Y_test,Y_predict) \nprint(\"The accuracy score using decision tree classifier:\", AS2)","4955291c":"X_test=test.iloc[:,:].values","22a91d4f":"Y_test=classifier2.predict(X_test)","8e3db47b":"Y_test.shape","ff108206":"passengerid.shape","4e9f6f4a":"#Creating an empty dataframe since passenger_id and Y_test have same number of rows\nFinal = pd.DataFrame()\nFinal","0b502616":"#Adding these 2 columns \"passengerid\" and \"survived\" then passing Y_test value in survived column\n\nFinal['passengerid'] = passengerid\nFinal['survived'] = Y_test\nFinal","e3f95913":"#Converting it into csv file\n\nFinal.to_csv('submission.csv', index=False)","db6045a6":"# Predicting using classifier and finding Accuracy","f1f69643":"# Finding out missing values of each column and filling them up\n","049215c6":"# For column \"Age\" of test dataset","9455e906":"# Training the model by applying Decision Tree Classifier and passing X_train and Y_train to it.","c1e3d527":"# Checking for each and every column of Train dataset if they are needed or not by checking the number of people survived or died","505b4423":"# Extracting all columns of test","45fa7030":"# Extracting X and Y for train","6a97f3e8":"# Calculating Accuracy","f82aef74":"# Removing Columns that are not important","a1df4ad0":"# for column \"Age\" of train dataset","0456f9b7":"# Applying Train and Test split","100e8998":"# Using Grid-Search-CV class and Training the model","0cf7ffb6":"# Finding Optimal Hyperparameter Value"}}