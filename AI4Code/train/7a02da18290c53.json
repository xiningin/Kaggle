{"cell_type":{"4b3ec13a":"code","69df2c53":"code","29bc8868":"code","a1223e3e":"code","386a7c8e":"code","009e4b57":"code","65214496":"code","ed23cf17":"code","64921dcd":"code","0948dadc":"code","f96dbf30":"code","8f8273f4":"code","a4256432":"code","c663dded":"code","972af342":"code","cb01cadd":"code","d1465ad6":"code","1e5bdfcf":"code","0701a293":"code","19c3f6cb":"code","c98584a4":"code","bc327b08":"code","a17bbde1":"code","d27d3d1b":"code","a9b0cbc5":"code","27ef9b91":"code","2a02f700":"code","512ca2e7":"code","70a52d18":"code","3276a991":"code","d1c4a0f5":"code","eb9d2738":"code","3341c179":"code","deda749b":"code","798bc61d":"code","64d6f2c9":"code","fb83a9bd":"code","62ca1fab":"code","acd58a4d":"code","4c668d66":"code","82036a1c":"code","53f99680":"code","7f1c3008":"code","7c5b6a4c":"code","39742ace":"code","99fd778c":"code","4d188f47":"code","744c3318":"code","5c48ab54":"code","cb9a2292":"code","f567ff48":"code","1cbd23c7":"code","dcd4ba8c":"code","9c39d697":"code","c07129af":"code","e581d84a":"code","cc1a6122":"code","49431ff5":"code","6ecdd244":"code","b0f3870e":"code","cb43df64":"code","02c02f02":"code","6ff473b0":"code","3f5a6a0b":"code","251d4a3d":"code","d6affe34":"code","947ae83b":"code","8f606615":"code","523df596":"code","2335d60f":"code","b6389e3a":"code","84800c03":"code","b401df9a":"code","ae9f96ba":"code","7fd66a22":"code","95fde14b":"code","28d31522":"code","3359337f":"code","979687e1":"code","bd958065":"code","7bc73d98":"code","59bdd12b":"code","b00913e0":"code","d78f94ce":"code","fa4ab132":"code","6edd4e6c":"code","07313ab9":"code","b5e90688":"markdown","1f231d07":"markdown","3c4f5ef0":"markdown","6564dd90":"markdown","31b16ff0":"markdown","32d2c0ad":"markdown","c0fdaaa9":"markdown","76e9cbaa":"markdown","a07c4bfc":"markdown","fbc3cb1e":"markdown","875ef733":"markdown","ece30656":"markdown","fb8e6f0a":"markdown","7da28094":"markdown"},"source":{"4b3ec13a":"import keras\nimport tensorflow as tf\nimport glob\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pickle\nfrom tqdm import tqdm\nimport pandas as pd\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, Activation, Flatten\nfrom keras.layers import merge\nfrom keras.optimizers  import Adam, RMSprop\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nimport nltk\nimport os","69df2c53":"token = '..\/input\/img-caption-data\/Flickr8k_text\/Flickr8k.token.txt'\ncaptions = open(token, 'r').read().strip().split('\\n')","29bc8868":"d = {}\nfor i, row in enumerate(captions):\n    row = row.split('\\t')\n    row[0] = row[0][:len(row[0])-2]\n    if row[0] in d:\n        d[row[0]].append(row[1])\n    else:\n        d[row[0]] = [row[1]]\n        \nd['1000268201_693b08cb0e.jpg']","a1223e3e":"images = '..\/input\/img-caption-data\/Flickr8k_Dataset\/Flicker8k_Dataset\/'\n# Contains all the images\nimg = glob.glob(images+'*.jpg')\nimg[:5]","386a7c8e":"def split_data(l):\n    temp = []\n    for i in img:\n        if i[len(images):] in l:\n            temp.append(i)\n    return temp","009e4b57":"train_images_file = '..\/input\/img-caption-data\/Flickr8k_text\/Flickr_8k.trainImages.txt'\ntrain_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n# Getting the training images from all the images\ntrain_img = split_data(train_images)\nlen(train_img)","65214496":"val_images_file = '..\/input\/img-caption-data\/Flickr8k_text\/Flickr_8k.devImages.txt'\nval_images = set(open(val_images_file, 'r').read().strip().split('\\n'))\n# Getting the validation images from all the images\nval_img = split_data(val_images)\nlen(val_img)","ed23cf17":"test_images_file = '..\/input\/img-caption-data\/Flickr8k_text\/Flickr_8k.testImages.txt'\ntest_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n# Getting the testing images from all the images\ntest_img = split_data(test_images)\nlen(test_img)","64921dcd":"Image.open(train_img[0])","0948dadc":"def preprocess_input(x):\n    x \/= 255.\n    x -= 0.5\n    x *= 2.\n    return x\n\ndef preprocess(image_path):\n    img = image.load_img(image_path, target_size=(299, 299))\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n\n    x = preprocess_input(x)\n    return x","f96dbf30":"plt.imshow(np.squeeze(preprocess(train_img[0])))","8f8273f4":"model = InceptionV3(weights='imagenet')","a4256432":"from keras.models import Model\n\nnew_input = model.input\nhidden_layer = model.layers[-2].output\n\nmodel_new = Model(new_input, hidden_layer)","c663dded":"tryi = model_new.predict(preprocess(train_img[0]))\ntryi.shape","972af342":"def encode(image):\n    image = preprocess(image)\n    temp_enc = model_new.predict(image)\n    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n    return temp_enc","cb01cadd":"encoding_train = {}\nfor img in tqdm(train_img):\n    encoding_train[img[len(images):]] = encode(img)","d1465ad6":"with open(\"encoded_images_inceptionV3.p\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_train, encoded_pickle) ","1e5bdfcf":"encoding_train = pickle.load(open('encoded_images_inceptionV3.p', 'rb'))\nencoding_train['3556792157_d09d42bef7.jpg'].shape","0701a293":"encoding_test = {}\nfor img in tqdm(test_img):\n    encoding_test[img[len(images):]] = encode(img)","19c3f6cb":"with open(\"encoded_images_test_inceptionV3.p\", \"wb\") as encoded_pickle:\n    pickle.dump(encoding_test, encoded_pickle) ","c98584a4":"encoding_test = pickle.load(open('encoded_images_test_inceptionV3.p', 'rb'))\nencoding_test[test_img[0][len(images):]].shape","bc327b08":"train_d = {}\nfor i in train_img:\n    if i[len(images):] in d:\n        train_d[i] = d[i[len(images):]]\n\nprint(\"Length of train_d: \",len(train_d))\n\nval_d = {}\nfor i in val_img:\n    if i[len(images):] in d:\n        val_d[i] = d[i[len(images):]]\n        \nprint(\"Length of val_d: \",len(val_d))\n\ntest_d = {}\nfor i in test_img:\n    if i[len(images):] in d:\n        test_d[i] = d[i[len(images):]]\n        \nprint(\"Length of test_d: \",len(test_d))","a17bbde1":"train_d[images+'3556792157_d09d42bef7.jpg']","d27d3d1b":"caps = []\nfor key, val in train_d.items():\n    for i in val:\n        caps.append('<start> ' + i + ' <end>')","a9b0cbc5":"words = [i.split() for i in caps]\nunique = []\nfor i in words:\n    unique.extend(i)\n    \nunique = list(set(unique))","27ef9b91":"with open(\"unique.p\", \"wb\") as pickle_d:\n    pickle.dump(unique, pickle_d) ","2a02f700":"unique = pickle.load(open('unique.p', 'rb'))\nlen(unique)","512ca2e7":"word2idx = {val:index for index, val in enumerate(unique)}\nword2idx['<start>']","70a52d18":"idx2word = {index:val for index, val in enumerate(unique)}\nidx2word[5553]","3276a991":"max_len = 0\nfor c in caps:\n    c = c.split()\n    if len(c) > max_len:\n        max_len = len(c)\nmax_len","d1c4a0f5":"len(unique), max_len","eb9d2738":"vocab_size = len(unique)\nvocab_size","3341c179":"f = open('flickr8k_training_dataset.txt', 'w')\nf.write(\"image_id\\tcaptions\\n\")","deda749b":"for key, val in train_d.items():\n    for i in val:\n        f.write(key[len(images):] + \"\\t\" + \"<start> \" + i +\" <end>\" + \"\\n\")\n\nf.close()","798bc61d":"df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\nlen(df)","64d6f2c9":"c = [i for i in df['captions']]\nlen(c)","fb83a9bd":"imgs = [i for i in df['image_id']]\na = c[-1]\na, imgs[-1]\n","62ca1fab":"for i in a.split():\n    print (i, \"=>\", word2idx[i])","acd58a4d":"samples_per_epoch = 0\nfor ca in caps:\n    samples_per_epoch += len(ca.split())-1\n    \nsamples_per_epoch","4c668d66":"def data_generator(batch_size = 32):\n        partial_caps = []\n        next_words = []\n        images = []\n        \n        df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\n        df = df.sample(frac=1)\n        iter = df.iterrows()\n        c = []\n        imgs = []\n        for i in range(df.shape[0]):\n            x = next(iter)\n            c.append(x[1][1])\n            imgs.append(x[1][0])\n\n\n        count = 0\n        while True:\n            for j, text in enumerate(c):\n                current_image = encoding_train[imgs[j]]\n                for i in range(len(text.split())-1):\n                    count+=1\n                    \n                    partial = [word2idx[txt] for txt in text.split()[:i+1]]\n                    partial_caps.append(partial)\n                    \n                    # Initializing with zeros to create a one-hot encoding matrix\n                    # This is what we have to predict\n                    # Hence initializing it with vocab_size length\n                    n = np.zeros(vocab_size)\n                    # Setting the next word to 1 in the one-hot encoded matrix\n                    n[word2idx[text.split()[i+1]]] = 1\n                    next_words.append(n)\n                    \n                    images.append(current_image)\n\n                    if count>=batch_size:\n                        next_words = np.asarray(next_words)\n                        images = np.asarray(images)\n                        partial_caps = sequence.pad_sequences(partial_caps, maxlen=max_len, padding='post')\n                        yield ([images, partial_caps], next_words)\n                        partial_caps = []\n                        next_words = []\n                        images = []\n                        count = 0","82036a1c":"embedding_size = 300","53f99680":"image_model = Sequential([\n        Dense(embedding_size, input_shape=(2048,), activation='relu'),\n        RepeatVector(max_len)\n    ])","7f1c3008":"caption_model = Sequential([\n        Embedding(vocab_size, embedding_size, input_length=max_len),\n        LSTM(256, return_sequences=True),\n        TimeDistributed(Dense(300))\n    ])","7c5b6a4c":"type(image_model)","39742ace":"# max_len = 40\nimage_in = keras.Input(shape=(2048,))\ncaption_in = keras.Input(shape=(max_len,))","99fd778c":"# image_in = Input(shape=(2048,))\n# caption_in = keras.Input(shape=(max_len, vocab_size))\ncaption_in = keras.Input(shape=(max_len,))\nmerged = keras.layers.concatenate([image_model(image_in), caption_model(caption_in)],axis=1)\nlatent = Bidirectional(LSTM(256, return_sequences=False))(merged)\nout = Dense(vocab_size, activation='softmax')(latent)\nfinal_model = Model([image_in, caption_in], out)","4d188f47":"final_model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])","744c3318":"final_model.summary()","5c48ab54":"steps_per_epoch = samples_per_epoch\/128","cb9a2292":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=1)","f567ff48":"final_model.load_weights('..\/input\/earlyweightsimagecaption\/time_inceptionV3_7_loss_3.2604.h5')","1cbd23c7":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","dcd4ba8c":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","9c39d697":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","c07129af":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","e581d84a":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","cc1a6122":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","49431ff5":"# final_model.optimizer.lr = 1e-4\n# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=1)","6ecdd244":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","b0f3870e":"# final_model.save_weights('time_inceptionV3_7_loss_3.2604.h5')","cb43df64":"# final_model.load_weights('time_inceptionV3_7_loss_3.2604.h5')","02c02f02":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","6ff473b0":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","3f5a6a0b":"# final_model.save_weights('time_inceptionV3_3.21_loss.h5')","251d4a3d":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","d6affe34":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","947ae83b":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","8f606615":"# final_model.save_weights('time_inceptionV3_3.15_loss.h5')","523df596":"# final_model.fit(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch, epochs=1, verbose=2)","2335d60f":"# final_model.load_weights('time_inceptionV3_1.5987_loss.h5')","b6389e3a":"def predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word2idx[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        e = encoding_test[image[len(images):]]\n        preds = final_model.predict([np.array([e]), np.array(par_caps)])\n        word_pred = idx2word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])","84800c03":"def beam_search_predictions(image, beam_index = 3):\n    start = [word2idx[\"<start>\"]]\n    \n    start_word = [[start, 0.0]]\n    \n    while len(start_word[0][0]) < max_len:\n        temp = []\n        for s in start_word:\n            par_caps = sequence.pad_sequences([s[0]], maxlen=max_len, padding='post')\n            e = encoding_test[image[len(images):]]\n            preds = final_model.predict([np.array([e]), np.array(par_caps)])\n            \n            word_preds = np.argsort(preds[0])[-beam_index:]\n            \n            # Getting the top <beam_index>(n) predictions and creating a \n            # new list so as to put them via the model again\n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n                prob += preds[0][w]\n                temp.append([next_cap, prob])\n                    \n        start_word = temp\n        # Sorting according to the probabilities\n        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n        # Getting the top words\n        start_word = start_word[-beam_index:]\n    \n    start_word = start_word[-1][0]\n    intermediate_caption = [idx2word[i] for i in start_word]\n\n    final_caption = []\n    \n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n        else:\n            break\n    \n    final_caption = ' '.join(final_caption[1:])\n    return final_caption","b401df9a":"try_image = test_img[0]\nImage.open(try_image)","ae9f96ba":"print ('Normal Max search:', predict_captions(try_image)) \nprint ('Beam Search, k=3:', beam_search_predictions(try_image, beam_index=3))\nprint ('Beam Search, k=5:', beam_search_predictions(try_image, beam_index=5))\nprint ('Beam Search, k=7:', beam_search_predictions(try_image, beam_index=7))","7fd66a22":"try_image2 = test_img[7]\nImage.open(try_image2)","95fde14b":"print ('Normal Max search:', predict_captions(try_image2)) \nprint ('Beam Search, k=3:', beam_search_predictions(try_image2, beam_index=3))\nprint ('Beam Search, k=5:', beam_search_predictions(try_image2, beam_index=5))\nprint ('Beam Search, k=7:', beam_search_predictions(try_image2, beam_index=7))","28d31522":"# try_image3 = test_img[851]\n# Image.open(try_image3)","3359337f":"# print ('Normal Max search:', predict_captions(try_image3)) \n# print ('Beam Search, k=3:', beam_search_predictions(try_image3, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(try_image3, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(try_image3, beam_index=7))","979687e1":"# try_image4 = 'Flickr8k_Dataset\/Flicker8k_Dataset\/136552115_6dc3e7231c.jpg'\n# print ('Normal Max search:', predict_captions(try_image4))\n# print ('Beam Search, k=3:', beam_search_predictions(try_image4, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(try_image4, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(try_image4, beam_index=7))\n# Image.open(try_image4)","bd958065":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/1674612291_7154c5ab61.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","7bc73d98":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/384577800_fc325af410.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","59bdd12b":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/3631986552_944ea208fc.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","b00913e0":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/3320032226_63390d74a6.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","d78f94ce":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/3316725440_9ccd9b5417.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","fa4ab132":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/2306674172_dc07c7f847.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","6edd4e6c":"# im = 'Flickr8k_Dataset\/Flicker8k_Dataset\/2542662402_d781dd7f7c.jpg'\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","07313ab9":"# im = test_img[int(np.random.randint(0, 1000, size=1))]\n# print (im)\n# print ('Normal Max search:', predict_captions(im))\n# print ('Beam Search, k=3:', beam_search_predictions(im, beam_index=3))\n# print ('Beam Search, k=5:', beam_search_predictions(im, beam_index=5))\n# print ('Beam Search, k=7:', beam_search_predictions(im, beam_index=7))\n# Image.open(im)","b5e90688":"Merging the models and creating a softmax classifier","1f231d07":"## Predict funtion","3c4f5ef0":"Calculating the unique words in the vocabulary.","6564dd90":"## Generator \n\nWe will use the encoding of an image and use a start word to predict the next word.\nAfter that, we will again use the same image and use the predicted word \nto predict the next word.\nSo, the image will be used at every iteration for the entire caption. \nThis is how we will generate the caption for an image. Hence, we need to create \na custom generator for that.\n\nThe CS231n lecture by Andrej Karpathy explains this concept very clearly and beautifully.\nLink for the lecture:- https:\/\/youtu.be\/cO0a0QYmFm8?t=32m25s","31b16ff0":"Adding <start> and <end> to all the captions to indicate the starting and ending of a sentence. This will be used while we predict the caption of an image","32d2c0ad":"We will feed these images to VGG-16 to get the encoded images. Hence we need to preprocess the images as the authors of VGG-16 did. The last layer of VGG-16 is the softmax classifier(FC layer with 1000 hidden neurons) which returns the probability of a class. This layer should be removed so as to get a feature representation of an image. We will use the last Dense layer(4096 hidden neurons) after popping the classifier layer. Hence the shape of the encoded image will be (1, 4096)","c0fdaaa9":"Uncomment the below cells for better results. Only one epoch was done for testing purpose.","76e9cbaa":"## Creating a dictionary containing all the captions of the images","a07c4bfc":"Input dimension is 4096 since we will feed it the encoded version of the image.","fbc3cb1e":"## Let's create the model","875ef733":"Since we are going to predict the next word using the previous words(length of previous words changes with every iteration over the caption), we have to set return_sequences = True.","ece30656":"Mapping the unique words to indices and vice-versa","fb8e6f0a":"As even one epoch takes around 7 hours time or more, let's load the weights trained before","7da28094":"Calculating the maximum length among all the captions"}}