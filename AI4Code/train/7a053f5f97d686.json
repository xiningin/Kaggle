{"cell_type":{"d5199999":"code","18c1b68b":"code","e72d6247":"code","e951aa1a":"code","adb95ad2":"code","afbfc52e":"code","056d1f5f":"code","c40410b9":"code","c79123cc":"code","27596ab6":"code","7bd6adf3":"code","2db87806":"code","57ed7583":"code","0a80bdd4":"code","e5377654":"code","eb4d8459":"code","5f1cdb1b":"code","c54d68e8":"code","3e71f108":"code","5a2e31b6":"code","cfc2cd92":"code","ac7d8dad":"code","6420ec2e":"code","712ae4d4":"code","4029a3c2":"code","7de0bd51":"code","f420cb04":"code","21f43464":"code","c25f2bab":"markdown","5b0d4e2b":"markdown","32d02d63":"markdown","e05ad90f":"markdown"},"source":{"d5199999":"import numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import LSTM, Bidirectional,  GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import GRU\nfrom keras.preprocessing import text, sequence\nfrom gensim.models import KeyedVectors\nfrom keras.callbacks import LearningRateScheduler","18c1b68b":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","e72d6247":"train.shape","e951aa1a":"train.head(10)","adb95ad2":"def preprocess(data):\n    '''\n    Adapted from https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n    '''\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    def clean_special_chars(text, punct):\n        for p in punct:\n            text = text.replace(p, ' ')\n        return text\n\n    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n    return data","afbfc52e":"x_train = preprocess(train['comment_text'])\ny_train = np.where(train['target'] >= 0.5, 1, 0)","056d1f5f":"y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nx_test = preprocess(test['comment_text'])","c40410b9":"MAX_LEN = 220 # length of each comment after converting to a number","c79123cc":"tokenizer = text.Tokenizer()\ntokenizer.fit_on_texts(list(x_train) + list(x_test))","27596ab6":"# tokenize the train and test dataframes\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","7bd6adf3":"x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","2db87806":"# free some memory\nimport gc\ndel train\ngc.collect()","57ed7583":"!wget http:\/\/nlp.stanford.edu\/data\/glove.840B.300d.zip\n!unzip glove.840B.300d.zip","0a80bdd4":"EMBEDDING_FILES = ['glove.840B.300d.txt']","e5377654":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)","eb4d8459":"def build_matrix(word_index, path):\n    \"\"\"\n    path: a path that contains embedding matrix\n    word_index is a dict of the form ('apple': 123, 'banana': 349, etc)\n    \n    we will construct an embedding_matrix for the words in word_index\n    using pre-trained embedding word vectors from 'path'\n    \"\"\"\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","5f1cdb1b":"embedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","c54d68e8":"del tokenizer\ngc.collect()","3e71f108":"NUM_MODELS = 2\nMAX_FEATURES = 100000 # maximum number of different words to keep in the original texts\nBATCH_SIZE = 512 # the number of training sample to put in the model in each step\nLSTM_UNITS = 128 # the dimension of the output vector of each LSTM cell.\nGRU_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 5","5a2e31b6":"from keras.models import Sequential\n\nLSTM_model = Sequential()\nLSTM_model.add(Embedding(*embedding_matrix.shape, weights = [embedding_matrix], \n                               input_length = MAX_LEN, trainable = False))\nLSTM_model.add(LSTM(units = LSTM_UNITS, return_sequences = True))\nLSTM_model.add(GlobalAveragePooling1D())\nLSTM_model.add(Dense(units = 1, activation = 'sigmoid'))","cfc2cd92":"LSTM_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nLSTM_model.fit(x_train, [y_train, y_aux_train], epochs=EPOCHS, \n               batch_size = BATCH_SIZE)","ac7d8dad":"LSTM_pred = LSTM_model.predict(x_test, batch_size=2048)","6420ec2e":"GRU_model = Sequential()\nGRU_model.add(Embedding(*embedding_matrix.shape, weights = [embedding_matrix], \n                           input_length = MAX_LEN, trainable = False))\nGRU_model.add(GRU(units = GRU_UNITS, return_sequences = True))\nGRU_model.add(GlobalAveragePooling1D())\nGRU_model.add(Dense(units = 1, activation = 'sigmoid'))","712ae4d4":"GRU_model.compile(optimizer = 'adam', loss = 'binary_crossentropy')\n\nGRU_model.fit(x_train, [y_train, y_aux_train], epochs=EPOCHS, \n               batch_size = BATCH_SIZE)","4029a3c2":"GRU_pred = GRU_model.predict(x_test, batch_size=2048)","7de0bd51":"len(GRU_pred)","f420cb04":"test.shape","21f43464":"# submission = pd.DataFrame.from_dict({\n#     'id': test['id'],\n#     'prediction': GRU_pred\n# })\n\n# submission.to_csv('submission.csv', index=False)","c25f2bab":"#### Tokenization","5b0d4e2b":"# LSTM Model","32d02d63":"# GRU Model","e05ad90f":"#### Embedding"}}