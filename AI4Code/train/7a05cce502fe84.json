{"cell_type":{"b93aefec":"code","f52bf8bb":"code","5804784a":"code","e787f125":"code","76e60431":"code","e33b9630":"code","9d770896":"code","b31b915e":"code","92348a7a":"code","e0c27e33":"code","5cb225da":"code","486325a3":"code","e5217918":"code","b0aadb7c":"code","d1b2569e":"code","de4b3b95":"code","00f2611f":"code","d3f48a74":"code","7933ab77":"code","95b2dceb":"code","240fd31a":"code","9916af7f":"code","6329e743":"code","51f204d9":"code","73579119":"code","040fb71f":"code","af1bb865":"markdown"},"source":{"b93aefec":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split","f52bf8bb":"# Load data\npath = '\/kaggle\/input\/digit-recognizer\/'\ndata = pd.read_csv(path +'train.csv')\nprint(data.shape)","5804784a":"# Prepare input data\n# split data into label and features\nlabel = tf.keras.utils.to_categorical(data['label'].values) # convert labels to categorical\nX = data.drop('label',axis=1).values\n# Note: you can convert to one-hot-encoding as above and use categorical_crossentropy as loss function\n# OR you can keep data['label'] (without converting) and use sparse_categorical_loss function instead\n\n# get shapes\nnum_images = X.shape[0] # examples\/images\nm = X.shape[1] # pixels\nmax_pixel = 255 # maximum pixel value for rescaling (improves convergence rate in training)\nimg_dim = np.sqrt(m).astype(int) #image dimensions\n#out_dim = label.nunique() #shape[1]\nout_dim = label.shape[1]\nprint('Number of different labels\/output dimension ' + str(out_dim))\n\n#reshape and normalize features\nX = X.reshape((num_images,img_dim,img_dim,1)) \/ max_pixel\nprint('Shape of input data X ' + str(X.shape))","e787f125":"# Let's print the first couple of images\nplt.figure(figsize=(4,4))\nfor i in range(4*4):\n    plt.subplot(4,4,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(X[i].reshape(img_dim,img_dim))\n    plt.xlabel(data.iloc[i,:].label)\nplt.subplots_adjust(hspace=0.5)\nplt.show()","76e60431":"# We want to use the TPU to speed up training\n# Let's set it up \n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","e33b9630":"# Let's write a function for our model\ndef create_model():\n    my_model = Sequential()\n    my_model.add(Conv2D(32,\n                        kernel_size = (3,3),\n                        activation = 'relu',\n                        input_shape = (img_dim,img_dim,1)))\n    my_model.add(Dropout(0.5))\n    my_model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n    my_model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))\n    my_model.add(Dropout(0.5))\n    my_model.add(MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n    my_model.add(Flatten())\n    my_model.add(Dense(units = 500))\n    my_model.add(Dense(out_dim, activation = 'softmax'))\n    \n    return my_model","9d770896":"# Set up the CNN architecture and compile the model\n# The layers of the CNN are 2x(Conv2D > Dropout > MaxPooling2D) > Flatten > Dense > Dense output layer\n# We use 'relu' activation in the hidden layers with 'softmax' in the output layer \n# Instantiating the model in the strategy scope creates the model on the TPU\n\nwith tpu_strategy.scope():\n    my_model = create_model()    \n    # compile model\n    my_model.compile(optimizer = 'adam',\n                    loss = 'categorical_crossentropy',\n                    metrics = ['accuracy'])","b31b915e":"my_model.summary()","92348a7a":"# Split into train and validation data\n# > I used the built in validation_split parameter before. \n# > This turned out to take more than 3x times as long to train per epoch than splitting it with train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, label, test_size = 0.1)","e0c27e33":"# Set the parameter for our model and then fit it\nBATCH_SIZE = 128 * tpu_strategy.num_replicas_in_sync # 16 with TPU off and 128 with TPU on\nEPOCHS = 100\nSTEPS_PER_EPOCH = num_images \/\/ BATCH_SIZE","5cb225da":"#train model\nhistory = my_model.fit(X_train, y_train ,\n#                         X, data['label'],\n                         epochs = EPOCHS,\n                         steps_per_epoch = STEPS_PER_EPOCH,\n                         validation_data = (X_val, y_val))","486325a3":"# Plot training and validation accuracy curves\nimport matplotlib.pyplot as plt \n\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nepochs = range(0,len(train_loss))\n\nfig, (ax1,ax2) = plt.subplots(1,2)\nfig.suptitle('Training progress')\n\nax1.plot(epochs,train_loss,label = 'train')\nax1.plot(epochs,val_loss,label = 'validation')\nax1.set_ylabel('Loss')\nax2.plot(epochs,train_acc,label = 'train')\nax2.plot(epochs,val_acc,label = 'validation')\nax2.set_ylabel('Accuracy')\nax1.legend()\nax2.legend()\n\nplt.show()","e5217918":"# load test data \ndata_test  = pd.read_csv(path+'test.csv').values\nnum_test = data_test.shape[0]\ndata_test = data_test.reshape((num_test,img_dim,img_dim,1)) \/ max_pixel","b0aadb7c":"# Let's try data augmentation and see if this leads to a better generalisation\/ better performance on the validation set","d1b2569e":"datagen = ImageDataGenerator(rotation_range=20,  \n                             zoom_range = 0.20,\n                             width_shift_range=0.15,\n                             height_shift_range=0.15)","de4b3b95":"# Loop through the ImageDataGenerator to retrieve augmented data set\n# (since Keras doesnt currently accept ImageDataGenerator as an input)\nTrain_x, Train_y = None, None\nbatch = 0\nfor x_batch, y_batch in datagen.flow(X_train, y_train, \n                                     batch_size=BATCH_SIZE, shuffle=False):\n    if batch == 0:\n        Train_x, Train_y = x_batch, y_batch\n    elif batch >= len(datagen.flow(X_train, y_train, \n                                     batch_size=BATCH_SIZE)): # X.shape[0] \/\/ BATCH_SIZE:\n        break\n    else:\n        Train_x = np.concatenate((Train_x, x_batch))\n        Train_y = np.concatenate((Train_y, y_batch))\n    batch += 1","00f2611f":"# Let's compare the originals to the augmented images\nplt.figure(figsize=(10,4))\nfor i in range(10):\n    plt.subplot(2,10,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(X_train[i].reshape(img_dim,img_dim))\nfor i in range(10):\n    plt.subplot(2,10,10+i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(Train_x[i].reshape(img_dim,img_dim))\n#plt.subplots_adjust(hspace=0.2)\nplt.show()","d3f48a74":"# Looks like it worked, the bottom row looks a bit more wonky than the top row :)\n# Let's see what it does to our model training","7933ab77":"# Create model on TPU\nwith tpu_strategy.scope():\n    my_model_aug = create_model()\n    # compile model\n    my_model_aug.compile(optimizer = 'adam',\n                    loss = 'categorical_crossentropy',\n                    metrics = ['accuracy'])   ","95b2dceb":"# Training\nhistory_aug = my_model_aug.fit(Train_x, Train_y, \n                    epochs = EPOCHS,\n                    steps_per_epoch = STEPS_PER_EPOCH,\n                    validation_data = (X_val, y_val))","240fd31a":"# Plot training and validation accuracy curves for both the original model and the model trained on augmented data\nimport matplotlib.pyplot as plt \n\ntrain_loss_aug = history_aug.history['loss']\nval_loss_aug = history_aug.history['val_loss']\ntrain_acc_aug = history_aug.history['accuracy']\nval_acc_aug = history_aug.history['val_accuracy']\n\nepochs = range(0,len(train_loss))\n\nfig, (ax1,ax2) = plt.subplots(1,2, figsize = (15,5))\nfig.suptitle('Training progress')\n\nax1.plot(epochs,train_loss,label = 'train')\nax1.plot(epochs,val_loss,label = 'validation')\nax1.plot(epochs,train_loss_aug,label = 'train augmented')\nax1.plot(epochs,val_loss_aug,label = 'validation augmented')\nax1.set_ylabel('Loss')\nax2.plot(epochs,train_acc,label = 'train')\nax2.plot(epochs,val_acc,label = 'validation')\nax2.plot(epochs,train_acc_aug,label = 'train augmented')\nax2.plot(epochs,val_acc_aug,label = 'validation augmented')\nax2.set_ylabel('Accuracy')\n# axes.set_ylim([ymin,ymax])\nax1.legend()\nax2.legend()\n\nplt.show()","9916af7f":"# I find it hard to see if data augmentation actually improves generalisation \u00af\\_(\u30c4)_\/\u00af","6329e743":"# predict with trained model\nmy_predictions = my_model.predict(data_test)\nimageId = np.arange(len(my_predictions))\nresults = my_predictions.argmax(axis=1)","51f204d9":"submission = pd.DataFrame(np.array([imageId + 1,results]).transpose(), columns = ['ImageId','Label'])\nsubmission.to_csv('submission.csv', index = False)","73579119":"# predict with model trained on augmented data\nmy_predictions_aug = my_model_aug.predict(data_test)\nimageId = np.arange(len(my_predictions_aug))\nresults_aug = my_predictions_aug.argmax(axis=1)","040fb71f":"submission_aug = pd.DataFrame(np.array([imageId + 1,results_aug]).transpose(), columns = ['ImageId','Label'])\nsubmission_aug.to_csv('submission_aug.csv', index = False)","af1bb865":"# Convolutional Neural Network for Digit Recognition\n\nWe use Keras to build and train a convolutional network.\nThe below uses TPU to speed up the training process, the set up is taken from https:\/\/www.kaggle.com\/c\/tpu-getting-started. This gives just above 99% accuracy if you run for about 300 epochs (below is done with 100 only).  \nEDIT: In an effort to improve generalisation of the model (and higher accuracy on the test set), we try data augmentation, i.e. apply random rotations, shifts, etc. of our training images to the training set (see https:\/\/www.kaggle.com\/alewicka\/mnist-digit-recognizer-cnn-in-keras-99-57 for a nice example using Keras ImageDataGenerator).   \nTPU doesn't currently work with Keras ImageDataGenerator (I believe this will be added in a future version though) but we can loop through the ImageDataGenerator manually to retrieve the augmented images as suggested here (https:\/\/www.kaggle.com\/wahyusetianto\/cnn-keras-cv-0-996-tpu)\nWe compare the performance for the model trained on original data vs augmented data "}}