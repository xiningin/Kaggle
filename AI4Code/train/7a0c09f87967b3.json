{"cell_type":{"d0ec313e":"code","d5e2b9d5":"code","1bf4a623":"code","6272a1f4":"code","c7fbd587":"code","535b214b":"code","35134362":"code","e4cf4dd3":"code","9c00c776":"code","3ca6d860":"code","0f5228a4":"code","2496aab5":"code","738b29ed":"code","6b90bbd7":"code","49562b6f":"code","8e5d4ea9":"code","815d020a":"code","e34e9e34":"code","b75a11b1":"code","1a4ea18c":"code","3a069d29":"code","299c3623":"code","9ab841a9":"code","f093f32e":"code","f08101dc":"code","c4458338":"code","af72f0bd":"code","9f7fb15a":"code","43c14e16":"code","26c7cb59":"code","284776c4":"code","d090f257":"code","d74070a5":"code","fc7d2bf4":"code","f51964fd":"code","fdc72604":"code","de0f16e6":"code","027f77ba":"code","f07d4bca":"code","20fd1a31":"code","424ad794":"code","1535013a":"code","d2e67eb7":"code","25457244":"code","ca9da12f":"code","b5982cac":"code","f0c0f721":"code","d10eff74":"code","f0e8d08f":"code","935f1ee8":"code","0b37b2bd":"code","7e2ef4f2":"code","1020aba4":"code","576d0a94":"code","249471cd":"code","69a8729b":"code","d355338f":"code","93a85a8c":"code","b3d16258":"code","aa07cdf3":"code","13ae74a3":"code","307f9119":"code","0998be1c":"code","320eaa9f":"code","931eed83":"code","3709d09a":"code","c833a679":"markdown","e29715e7":"markdown","9af38bdc":"markdown","7290a761":"markdown","e2d3cc83":"markdown","e0db509e":"markdown","223a599b":"markdown","5dc05306":"markdown","cd0091fb":"markdown","9537045d":"markdown","3d9815f0":"markdown","1b3b928b":"markdown","85ecd35a":"markdown","8d6fe365":"markdown","e5fdcabe":"markdown","772ed984":"markdown","ac55b184":"markdown","81101cb0":"markdown","883388ee":"markdown","d03a9518":"markdown","d6db4f6b":"markdown","39de8180":"markdown","764f6332":"markdown","347e514b":"markdown","278e4a2d":"markdown","57c0a580":"markdown","c12802e7":"markdown","751d2f55":"markdown","c7f24fdd":"markdown","772e205e":"markdown","0ffd8607":"markdown","c5ff6739":"markdown","310dc641":"markdown","34580003":"markdown","d5539819":"markdown","fcd075c1":"markdown","c33f7794":"markdown","29f5dfa7":"markdown","e4cd06f0":"markdown","83cb878b":"markdown","2b577378":"markdown","c31ef7f7":"markdown","6b0a73aa":"markdown","663ceffc":"markdown","a5b48ff8":"markdown","be11ea94":"markdown","d5377f40":"markdown","94a46809":"markdown","4f72590e":"markdown","d9b662fa":"markdown","5d202d92":"markdown","8e5c2424":"markdown","a4dad8dd":"markdown","e63fb557":"markdown","58f50994":"markdown","28585826":"markdown","d575f37f":"markdown","f1335627":"markdown","814facb3":"markdown","9fd2ee23":"markdown","1ee07bec":"markdown","2fb327fe":"markdown","510a27bf":"markdown","aec1f6be":"markdown","6fc50808":"markdown","25a4ebdf":"markdown","21a27d78":"markdown","e380573d":"markdown","bc74e9aa":"markdown","81babafc":"markdown","7cc6c02c":"markdown","b968766f":"markdown","44e4f5e9":"markdown","513e5f52":"markdown","3caa8dec":"markdown","a35ed198":"markdown","17b312ee":"markdown","17410a2e":"markdown","1e529839":"markdown","31b0cf3e":"markdown","29f93f8c":"markdown","36d58285":"markdown","a9670431":"markdown","6834811d":"markdown","78f598da":"markdown","5c810eea":"markdown","359f9536":"markdown","4ee0bb0d":"markdown","169350a1":"markdown","09db375e":"markdown","479919cb":"markdown","9d60b129":"markdown","df5a6785":"markdown","c9295d19":"markdown","7dbe62f2":"markdown","d14c1ccd":"markdown","bf14fbfd":"markdown","ff0752ad":"markdown","cb0f5b50":"markdown","3378b7ce":"markdown","3bb42bff":"markdown","afa43f10":"markdown","ac26b294":"markdown","1083dc06":"markdown","4f32670b":"markdown","9c2ada0c":"markdown","081830f1":"markdown"},"source":{"d0ec313e":"from IPython.display import Image\nImage(\"..\/input\/concrete-cubes\/concrete_cubes.jpg\")","d5e2b9d5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler, PowerTransformer\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\nsns.set_theme()","1bf4a623":"# Function used to add value labels to vertical barplots\n\ndef add_val_labels(plot, spacing=0, color=\"black\"):\n    for p in plot.patches:\n        if not math.isnan(p.get_height()):\n            x = p.get_x() + p.get_width() - 0.5\n            y = p.get_height() - plot.patches[0].get_height()*spacing\n            value = round(p.get_height(), 3)\n            plot.text(x, y, value, ha=\"left\", color=color, size='large')\n\n# Function used to add value labels to horizontal barplots\n\ndef add_val_labels_horizontal(plot, spacing=0, color=\"black\"):\n    for p in plot.patches:\n        if not math.isnan(p.get_width()):\n            x = p.get_width() - plot.patches[0].get_width()*spacing\n            y = p.get_y() + p.get_height() - 0.3\n            value = round(p.get_width(), 3)\n            plot.text(x, y, value, ha=\"left\", color=color, size='large')","6272a1f4":"df = pd.read_csv(\"..\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv\")","c7fbd587":"df.shape","535b214b":"df.columns","35134362":"df.head(-10)","e4cf4dd3":"df.info()","9c00c776":"df.describe()","3ca6d860":"skewness = round(df[\"csMPa\"].skew(), 2)\nkurtosis = round(df[\"csMPa\"].kurt(), 2)\n\nplt.figure(figsize=(16, 6))\nsns.histplot(x=\"csMPa\", kde=True, bins=30, data=df)\nplt.title(\"csMPa distribution, skewness= \" + str(skewness) +\n          \", kurtosis= \" + str(kurtosis), fontsize=20)\nplt.xlabel(\"csMPa\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\n\nplt.show()","0f5228a4":"plt.figure(figsize=(16, 4))\nsns.boxplot(x=\"csMPa\", data=df)\nplt.title(\"csMPa boxplot\", fontsize=20)\nplt.xlabel(\"csMPa\", fontsize=16)\n\nplt.show()","2496aab5":"plt.figure(figsize=(6, 4))\nres = stats.probplot(df['csMPa'], plot=plt)\nplt.title(\"csMPa Probability Plot\", fontsize=20)\nplt.xlabel(\"Theoretical quantiles\", fontsize=16)\nplt.ylabel(\"Ordered Values\", fontsize=16)\nplt.show()","738b29ed":"df_vars_num = df.drop('csMPa', axis=1)\n\nfor col_name in df_vars_num:\n\n    skewness = round(df_vars_num[col_name].skew(), 2)\n    kurtosis = round(df_vars_num[col_name].kurt(), 2)\n\n    plt.figure(figsize=(16, 6))\n    sns.histplot(x=col_name, kde=True, bins=30, data=df_vars_num)\n    plt.title(col_name + \" distribution, skewness= \"+str(skewness) +\n              \", kurtosis= \" + str(kurtosis), fontsize=20)\n    plt.xlabel(col_name, fontsize=16)\n    plt.ylabel(\"Count\", fontsize=16)\n    plt.show()","6b90bbd7":"for col_name in df_vars_num:\n    plt.figure(figsize=(16, 3))\n    sns.boxplot(x=df_vars_num[col_name])\n    plt.title(col_name + \" boxplot\", fontsize=20)\n    plt.xlabel(col_name, fontsize=16)\n    plt.show()","49562b6f":"fig = plt.figure(figsize=(20, 20))\n\nfor i in range(len(df_vars_num.columns)):\n    ax = plt.subplot(4, 3, i+1)\n    stats.probplot(df[df_vars_num.columns[i]], plot=plt)\n    plt.title(df_vars_num.columns[i], fontsize=20)\n    plt.xlabel(\"Theoretical quantiles\", fontsize=20)\n    plt.ylabel(\"Ordered Values\", fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\nfig.suptitle('Variable QQ-plots', fontsize=25, y=1.01)\nfig.tight_layout()\nplt.show()","8e5d4ea9":"df[\"age\"].unique()","815d020a":"plt.figure(figsize=(20, 10))\nplot = sns.countplot(x=\"age\", data=df,\n                     order=df['age'].value_counts().index, palette=\"Accent\")\nplt.title(\"Age distribution\", fontsize=20)\nplt.xlabel(\"age\", fontsize=16)\nplt.ylabel(\"count\", fontsize=16)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nadd_val_labels(plot, 0)\nplt.show()","e34e9e34":"plt.figure(figsize=(16, 8))\nsns.boxplot(x=\"csMPa\", y=\"age\", data=df, orient=\"h\", palette=\"Accent\")\nplt.title(\"csMPa distribution per value of age\", fontsize=20)\nplt.xlabel(\"csMPa\", fontsize=16)\nplt.ylabel(\"age\", fontsize=16)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","b75a11b1":"correlations = df.corr()\nplt.figure(figsize=(16, 12))\nax = sns.heatmap(correlations, square=True, annot=True,\n                 fmt='.2f', linecolor='white', cmap=\"vlag\")\nax.set_xticklabels(ax.get_xticklabels(), rotation=30, fontsize=12)\nax.set_yticklabels(ax.get_yticklabels(), rotation=30, fontsize=12)\nplt.title(\"Pearson's correlation heatmap\", fontsize=20)\nplt.show()","1a4ea18c":"df_top_corr = df[[\"cement\", \"water\", \"superplasticizer\", \"age\", \"csMPa\"]]\nplt.figure(figsize=(16, 16))\nsns.pairplot(df_top_corr, kind='reg', diag_kind='hist', plot_kws={'line_kws': {'color': '#eb4034'},\n                                                                  'scatter_kws': {'alpha': 0.5,\n                                                                                  'color': '#002aff'}})\nplt.show()","3a069d29":"df[\"total_density\"] = df.apply(lambda x: sum([x[\"cement\"], x[\"slag\"], x[\"flyash\"],\n                              x[\"water\"], x[\"superplasticizer\"], x[\"coarseaggregate\"], x[\"fineaggregate\"]]), axis=1)","299c3623":"df.head()","9ab841a9":"skewness = round(df[\"total_density\"].skew(), 2)\nkurtosis = round(df[\"total_density\"].kurt(), 2)\n\nplt.figure(figsize=(16, 6))\nsns.histplot(x=\"total_density\", kde=True, bins=30, data=df)\nplt.title(\"total_density distribution, skewness= \" + str(skewness) +\n          \", kurtosis= \" + str(kurtosis), fontsize=20)\nplt.xlabel(\"total_density\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","f093f32e":"plt.figure(figsize=(6, 4))\nres = stats.probplot(df['total_density'], plot=plt)\nplt.title(\"total_weight QQ-plot\", fontsize=20)\nplt.xlabel(\"Theoretical quantiles\", fontsize=16)\nplt.ylabel(\"Ordered Values\", fontsize=16)\nplt.show()","f08101dc":"weight_vars = [\"cement\", \"slag\", \"flyash\", \"water\",\n               \"superplasticizer\", \"coarseaggregate\", \"fineaggregate\"]\n\ndf_2 = df.copy()\n\nfor var in weight_vars:\n    df_2[var + \" %\"] = df_2.apply(lambda x: x[var]\/x[\"total_density\"], axis=1)","c4458338":"df_2.head()","af72f0bd":"y = df_2[\"csMPa\"]\nX = df_2.drop([\"csMPa\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=2)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","9f7fb15a":"pt = PowerTransformer()\n\nX_train = pd.DataFrame(pt.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(pt.transform(X_test), columns=X_test.columns)","43c14e16":"feature_scaler = MinMaxScaler()\nX_train = pd.DataFrame(feature_scaler.fit_transform(\n    X_train), columns=X_train.columns)\nX_test = pd.DataFrame(feature_scaler.transform(X_test),\n                      columns=X_train.columns)","26c7cb59":"X_train.describe()","284776c4":"# Lists for storing validation scores of tested models\nr2_scores = {}\nmae_scores = {}\nrmse_scores = {}","d090f257":"lin_reg = LinearRegression()\n\nrfe = RFE(lin_reg)","d74070a5":"params = [{'n_features_to_select': list(range(1, 17))}]","fc7d2bf4":"%%time\ncv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\ncv = GridSearchCV(estimator=rfe, param_grid=params,\n                  scoring='neg_root_mean_squared_error', cv=cv_scheme, return_train_score=True)\n\ncv.fit(X_train, y_train)","f51964fd":"print(\"Best parameters for Linear Regression:\")\nprint(cv.best_params_)\nprint(\"Best mean cross validation score:\")\nprint(cv.best_score_)","fdc72604":"n_features = cv.best_params_['n_features_to_select']\n\nlin_reg = LinearRegression()\nrfe = RFE(estimator=lin_reg, n_features_to_select=n_features)\nrfe.fit(X_train, y_train)\n\ny_pred = rfe.predict(X_test)\nlin_r2 = r2_score(y_test, y_pred)\nlin_mae = mean_absolute_error(y_test, y_pred)\nlin_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"---LINEAR REGRESSION---\")\nprint(\"R2 score: \"+str(lin_r2))\nprint(\"Mean Absolute Error: \"+str(lin_mae))\nprint(\"Root Mean Squared Error: \"+str(lin_rmse))\n\nr2_scores[\"linear_regression\"] = lin_r2\nmae_scores[\"linear_regression\"] = lin_mae\nrmse_scores[\"linear_regression\"] = lin_rmse","de0f16e6":"ridge_reg = Ridge()","027f77ba":"params = [{'alpha': [0.0004, 0.0003, 0.0002, 0.0001, 0.00009, 0.00008, 0.00007, 0.00006],\n           \"solver\": ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}]","f07d4bca":"cv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\ncv = GridSearchCV(estimator=ridge_reg, param_grid=params,\n                  scoring='neg_root_mean_squared_error', cv=cv_scheme, return_train_score=True, n_jobs=-1)\n\ncv.fit(X_train, y_train)","20fd1a31":"print(\"Best parameters for Ridge Regression:\")\nprint(cv.best_params_)\nprint(\"Best mean cross validation score:\")\nprint(cv.best_score_)","424ad794":"alpha = cv.best_params_['alpha']\nsolver = cv.best_params_['solver']\n\nridge_reg = Ridge(alpha=alpha, solver=solver)\nridge_reg.fit(X_train, y_train)\n\ny_pred = ridge_reg.predict(X_test)\nridge_r2 = r2_score(y_test, y_pred)\nridge_mae = mean_absolute_error(y_test, y_pred)\nridge_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"---RIDGE REGRESSION---\")\nprint(\"R2 score: \"+str(ridge_r2))\nprint(\"Mean Absolute Error: \"+str(ridge_mae))\nprint(\"Root Mean Squared Error: \"+str(ridge_rmse))\n\nr2_scores[\"ridge_regression\"] = ridge_r2\nmae_scores[\"ridge_regression\"] = ridge_mae\nrmse_scores[\"ridge_regression\"] = ridge_rmse","1535013a":"# I needed to increase the max_iter value because otherwise the model wouldn't converge\nlasso_reg = Lasso(max_iter=1000000)","d2e67eb7":"params = [{'alpha': [0.0009, 0.0008, 0.0007, 0.0006, 0.0005, 0.0004]}]","25457244":"%%time\ncv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\ncv = GridSearchCV(estimator=lasso_reg, param_grid=params,\n                  scoring='neg_root_mean_squared_error', cv=cv_scheme, return_train_score=True, n_jobs=-1)\n\ncv.fit(X_train, y_train)","ca9da12f":"print(\"Best parameters for Lasso Regression:\")\nprint(cv.best_params_)\nprint(\"Best mean cross validation score:\")\nprint(cv.best_score_)","b5982cac":"alpha = cv.best_params_['alpha']\n\nlasso_reg = Lasso(max_iter=1000000, alpha=alpha)\nlasso_reg.fit(X_train, y_train)\n\ny_pred = lasso_reg.predict(X_test)\nlasso_r2 = r2_score(y_test, y_pred)\nlasso_mae = mean_absolute_error(y_test, y_pred)\nlasso_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"---LASSO REGRESSION---\")\nprint(\"R2 score: \"+str(lasso_r2))\nprint(\"Mean Absolute Error: \"+str(lasso_mae))\nprint(\"Root Mean Squared Error: \"+str(lasso_rmse))\n\nr2_scores[\"lasso_regression\"] = lasso_r2\nmae_scores[\"lasso_regression\"] = lasso_mae\nrmse_scores[\"lasso_regression\"] = lasso_rmse","f0c0f721":"rforest_reg = RandomForestRegressor()","d10eff74":"params = [{'n_estimators': [500, 600, 800],\n           'min_samples_split': [2, 5, 10],\n           'min_samples_leaf': [1, 2, 5],\n           'max_depth': [20, 25, 30, 35, 40]}]","f0e8d08f":"%%time\ncv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\ncv = GridSearchCV(estimator=rforest_reg, param_grid=params, scoring='neg_root_mean_squared_error',\n                  cv=cv_scheme, return_train_score=True, n_jobs=-1)\n\ncv.fit(X_train, y_train)","935f1ee8":"print(\"Best parameters for Random Forest Regression:\")\nprint(cv.best_params_)\nprint(\"Best mean cross validation score:\")\nprint(cv.best_score_)","0b37b2bd":"n_estimators = cv.best_params_['n_estimators']\nmin_samples_split = cv.best_params_['min_samples_split']\nmin_samples_leaf = cv.best_params_['min_samples_leaf']\nmax_depth = cv.best_params_['max_depth']\n\nrforest_reg = RandomForestRegressor(\n    n_estimators=n_estimators, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_depth=max_depth, random_state=1)\nrforest_reg.fit(X_train, y_train)\n\ny_pred = rforest_reg.predict(X_test)\n\nrforest_r2 = r2_score(y_test, y_pred)\nrforest_mae = mean_absolute_error(y_test, y_pred)\nrforest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(\"---RANDOM FOREST REGRESSOR---\")\nprint(\"R2 score: \"+str(rforest_r2))\nprint(\"Mean Absolute Error: \"+str(rforest_mae))\nprint(\"Root Mean Squared Error: \"+str(rforest_rmse))\n\nr2_scores[\"random_forest\"] = rforest_r2\nmae_scores[\"random_forest\"] = rforest_mae\nrmse_scores[\"random_forest\"] = rforest_rmse","7e2ef4f2":"gboost_reg = GradientBoostingRegressor(random_state=1)","1020aba4":"params = [{'n_estimators': [900, 1000, 1100, 1300],\n           'learning_rate': [0.05, 0.1, 0.5],\n           'min_samples_split': [8, 12],\n           'min_samples_leaf': [2, 3, 4],\n           'max_depth': [2, 3, 4]}]","576d0a94":"%%time\ncv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\ncv = GridSearchCV(estimator=gboost_reg, param_grid=params, scoring='neg_root_mean_squared_error',\n                  cv=cv_scheme, return_train_score=True, n_jobs=-1)\n\ncv.fit(X_train, y_train)","249471cd":"print(\"Best parameters for GradientBoosting:\")\nprint(cv.best_params_)\nprint(\"Best mean cross validation score:\")\nprint(cv.best_score_)","69a8729b":"n_estimators = cv.best_params_['n_estimators']\nlearning_rate = cv.best_params_['learning_rate']\nmin_samples_split = cv.best_params_['min_samples_split']\nmin_samples_leaf = cv.best_params_['min_samples_leaf']\nmax_depth = cv.best_params_['max_depth']\n\ngboost_reg = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate,\n                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, max_depth=max_depth, random_state=1)\ngboost_reg.fit(X_train, y_train)\n\ny_pred = gboost_reg.predict(X_test)\n\ngboost_r2 = r2_score(y_test, y_pred)\ngboost_mae = mean_absolute_error(y_test, y_pred)\ngboost_rmse = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(\"---GRADIENT BOOSTING REGRESSOR---\")\nprint(\"R2 score: \"+str(gboost_r2))\nprint(\"Mean Absolute Error: \"+str(gboost_mae))\nprint(\"Root Mean Squared Error: \"+str(gboost_rmse))\n\nr2_scores[\"gradient_boosting\"] = gboost_r2\nmae_scores[\"gradient_boosting\"] = gboost_mae\nrmse_scores[\"gradient_boosting\"] = gboost_rmse","d355338f":"#xgb_reg = XGBRegressor(random_state=1)","93a85a8c":"\"\"\"\nparams = [{'eta': [0.001, 0.005, 0.01, 0.05, 0.1],\n           'gamma': [0, 0.001, 0.01, 0.1, 1, 2, 5],\n           'max_depth': [2, 4, 6, 8, 10, None],\n           'min_child_weight':[0, 1, 2],\n           'subsample': [0.7, 0.8, 0.9, 1],\n           'colsample_bytree':[0.7, 0.8, 0.9, 1],\n           'reg_lambda': [1, 1.5, 2, 3, 4],\n           'alpha': [0, 0.25, 0.5, 0.75, 1, 1.25, 1.5],\n           'n_estimators': [400, 600, 800, 1000, 1200, 1400]\n           }]\n\"\"\"","b3d16258":"\"\"\"\n%%time\ncv_scheme = KFold(n_splits=5, shuffle=True, random_state=1)\n\ncv = RandomizedSearchCV(estimator=xgb_reg, param_distributions=params, scoring='neg_root_mean_squared_error',\n                        cv=cv_scheme, return_train_score=True, n_jobs=-1, n_iter=2000, random_state=1)\n\ncv.fit(X_train, y_train)\n\"\"\"","aa07cdf3":"\"\"\"\nprint(\"Best parameters for XGBoost:\")\nprint(cv.best_params_)\nprint(\"Best validation RMSE:\")\nprint(cv.best_score_)\n\"\"\"","13ae74a3":"\"\"\"\neta = cv.best_params_['eta']\ngamma = cv.best_params_['gamma']\nmax_depth = cv.best_params_['max_depth']\nmin_child_weight = cv.best_params_['min_child_weight']\nsubsample = cv.best_params_['subsample']\ncolsample_bytree = cv.best_params_['colsample_bytree']\nreg_lambda = cv.best_params_['reg_lambda']\nalpha = cv.best_params_['alpha']\nn_estimators = cv.best_params_['n_estimators']\n\"\"\"\neta = 0.05\ngamma = 0.1\nmax_depth = 4\nmin_child_weight = 1\nsubsample = 0.8\ncolsample_bytree = 0.8\nreg_lambda = 4\nalpha = 0\nn_estimators = 1400\n\nxgb_reg = XGBRegressor(eta=eta, gamma=gamma, max_depth=max_depth, min_child_weight=min_child_weight,\n                       subsample=subsample, colsample_bytree=colsample_bytree, reg_lambda=reg_lambda, alpha=alpha, n_estimators=n_estimators, random_state=1)\nxgb_reg.fit(X_train, y_train)\n\ny_pred = xgb_reg.predict(X_test)\nxgb_r2 = r2_score(y_test, y_pred)\nxgb_mae = mean_absolute_error(y_test, y_pred)\nxgb_rmse = mean_squared_error(y_test, y_pred, squared=False)\n\nprint(\"---EXTREME GRADIENT BOOSTING REGRESSOR---\")\nprint(\"R2 score: \"+str(xgb_r2))\nprint(\"Mean Absolute Error: \"+str(xgb_mae))\nprint(\"Root Mean Squared Error: \"+str(xgb_rmse))\n\nr2_scores[\"xgboost\"] = xgb_r2\nmae_scores[\"xgboost\"] = xgb_mae\nrmse_scores[\"xgboost\"] = xgb_rmse","307f9119":"scores = [r2_scores, mae_scores, rmse_scores]\ntitles = ['R2 Score', 'Mean Absolute Error', 'Root Mean Squared Error']\n\nfor i in range(len(scores)):\n    plt.figure(figsize=(16, 4))\n    plot = sns.barplot(x=list(scores[i].values()), y=list(\n        scores[i].keys()), palette=\"Set2\")\n    plt.title(titles[i], fontsize=20)\n    plt.xlabel(\"Value\", fontsize=16)\n    plt.ylabel(\"Model\", fontsize=16)\n    \n    add_val_labels_horizontal(plot, 0.06, \"white\")\n    plt.show()","0998be1c":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_test, y=y_pred, alpha=0.8)\nline_X = np.linspace(0, 80, 100)\nline_Y = line_X\nplt.plot(line_X, line_Y, 'r--')\nplt.title(\"Predicted vs real values\", fontsize=20)\nplt.xlabel('Real value', fontsize=16)\nplt.ylabel('Predicted value', fontsize=16)\nplt.show()","320eaa9f":"residuals = y_test-y_pred\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x=y_pred, y=residuals, alpha=0.8)\nline_X = np.linspace(0, 80, 100)\nline_Y = line_X*0\nplt.plot(line_X, line_Y, 'r--')\nplt.title(\"Residuals\", fontsize=20)\nplt.xlabel('Predicted value', fontsize=16)\nplt.ylabel('Residual', fontsize=16)\n\nplt.show()","931eed83":"feature_importance = pd.Series(\n    xgb_reg.feature_importances_, index=X.columns).sort_values(ascending=False)\n\nplt.figure(figsize=(16, 10))\nplot = sns.barplot(x=feature_importance,\n                   y=feature_importance.index, palette=\"CMRmap\")\nplt.title(\"Feature importance\", fontsize=20)\nplt.xlabel(\"Mean decrease in impurity\", fontsize=16)\nplt.ylabel(\"Feature\", fontsize=16)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nadd_val_labels_horizontal(plot, 0)\nplt.show()","3709d09a":"Image(\"..\/input\/concrete-cubes\/concrete_cubes.jpg\")","c833a679":"#### Validation score on full dataset","e29715e7":"#### Results","9af38bdc":"#### Boxplots","7290a761":"- **Analyzed dataset contains information about 1030 different concrete recipes paired with compressive strengths of resulting mixtures**\n\n\n- **Distributions of feature variables have varying levels of skewness and kurtosis, but none of them are extremely skewed. Target variable csMPa has distribution which is relatively similar to normal**\n\n\n- **No features are strongly correlated with the target, although there is also no suspicion of multicollinearity**\n\n\n- **Several new variables were created which improved accuracies of all tested models. These new variables were \"total_density\" and additional percentage density variables for each of pre-existing density variables**\n\n\n- **Additionally feature variables were transformed using Yeo-Johnson transformation to increase normality and scaled using MinMaxScaler - Those transformations also had a positive effect on accuracies of all models**\n\n\n- **For compressive strength regression I have tested 3 different linear regression (Linear+RFE, Ridge and Lasso) and 3 different non-linear regression (RandomForest, GradientBoosting, XGBRegressor) models and spent some time optimizing their hyperparameters using GridSearchCV and RandomSearchCV**\n\n\n- **Non-linear models performed significantly better than linear models**\n\n\n- **<ins>The best model - XGBoostRegressor has achieved MAE of <span style=\"color:green; font-weight:bold\">2.43<\/span> and RMSE of <span style=\"color:green; font-weight:bold\">3.7<\/span> on testing data<\/ins>**","e2d3cc83":"### Dataset","e0db509e":"#### Results","223a599b":"## Introduction","5dc05306":"#### Hyperparameters for grid-search","cd0091fb":"#### Boxplot of \"age\" per value of \"csMPa\"","9537045d":"## Feature engineering","3d9815f0":"#### Grid search with cross validation","1b3b928b":"### Linear regression with Recursive feature selection","85ecd35a":"### GradientBoosting regressor","8d6fe365":"- As cumultative density of mixtures varies, so does their ratio of each ingredient's density to total density\n- Therefore, I'm going to create a new variable for each ingredient that contains information about it's density expressed in percentages \n- Presence of those columns as well as the **total_density** column turned out to have a positive effect on accuracy of all tested models","e5fdcabe":"**Observations:**\n- Judging from above information, variables slag, flyash, superplasticizer and age may contain outliers and\/or have high skew","772ed984":"#### Hyperparameters for grid-search","ac55b184":"#### Validation score on full dataset","81101cb0":"#### Unique values of \"age\"","883388ee":"#### Feature scaling","d03a9518":"#### Hyperparameters for grid-search","d6db4f6b":"**Observations:**\n- XGBoost was able to accurately capture most of relationships in provided data. Vast majority of all predictions have MAE of 5 or less\n- 2 of the predictions were especially inaccurate - Value of absolute error for them was higher than 15","39de8180":"#### Validation score on full dataset","764f6332":"#### Histogram, skewness and kurtosis of \"total_weight\" ","347e514b":"**Observations:**\n- There is no null values\n- All columns contain numerical, quantitative variables\n- All of the variables have type float64 except age which is expressed in int64","278e4a2d":"### Accuracy comparison","57c0a580":"### Variable correlations","c12802e7":"**Observations:**\n- **XGBoostRegressor** is a clear **winner** in all metrics\n- Linear models perform significantly worse than non-linear models\n- There isn't much difference in performance between different non-linear models","751d2f55":"#### Grid search with cross validation","c7f24fdd":"**Observations:**\n- Despite being a quantitive variable, **age** has only 14 unique values","772e205e":"#### QQ-plot of total_weight","0ffd8607":"**Observations:**\n- There seems to be a light positive correlation between **age** and **csMPa**\n- Notably, all recipes with **age** >=56 days have strength of at least 20 **csMPa**, therefore it seems that **age** of cement mixture plays an important role","c5ff6739":"### Target variable exploration","310dc641":"#### Randomized search with cross validation","34580003":"#### Boxplot","d5539819":"#### Count of each \"age\" value","fcd075c1":"**Observations:**\n- About 50% of **slag**, **flyash**, **superplasticizer** distributions are comprised of  0-values","c33f7794":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Dataset<\/a><\/span><\/li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Objectives<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Imports-and-function-definitions\" data-toc-modified-id=\"Imports-and-function-definitions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Imports and function definitions<\/a><\/span><\/li><li><span><a href=\"#Data-loading-and-basic-exploration\" data-toc-modified-id=\"Data-loading-and-basic-exploration-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Data loading and basic exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Reading-data-into-DataFrame\" data-toc-modified-id=\"Reading-data-into-DataFrame-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Reading data into DataFrame<\/a><\/span><\/li><li><span><a href=\"#Basic-Information\" data-toc-modified-id=\"Basic-Information-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Basic Information<\/a><\/span><\/li><li><span><a href=\"#Null-value-and-type-checking\" data-toc-modified-id=\"Null-value-and-type-checking-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Null-value and type checking<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Variable-exploration\" data-toc-modified-id=\"Variable-exploration-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Variable exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Basic-statistical-information\" data-toc-modified-id=\"Basic-statistical-information-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Basic statistical information<\/a><\/span><\/li><li><span><a href=\"#Target-variable-exploration\" data-toc-modified-id=\"Target-variable-exploration-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Target variable exploration<\/a><\/span><\/li><li><span><a href=\"#Feature-variable-exploration\" data-toc-modified-id=\"Feature-variable-exploration-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Feature variable exploration<\/a><\/span><\/li><li><span><a href=\"#Variable-correlations\" data-toc-modified-id=\"Variable-correlations-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Variable correlations<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-engineering\" data-toc-modified-id=\"Feature-engineering-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Feature engineering<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Total-weight-variable\" data-toc-modified-id=\"Total-weight-variable-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Total weight variable<\/a><\/span><\/li><li><span><a href=\"#Values-of-percentage\" data-toc-modified-id=\"Values-of-percentage-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Values of percentage<\/a><\/span><\/li><li><span><a href=\"#Split-into-train-and-test-sets\" data-toc-modified-id=\"Split-into-train-and-test-sets-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Split into train and test sets<\/a><\/span><\/li><li><span><a href=\"#Feature-normalization-&amp;-scaling\" data-toc-modified-id=\"Feature-normalization-&amp;-scaling-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Feature normalization &amp; scaling<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-training\" data-toc-modified-id=\"Model-training-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Model training<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Linear-regression-with-Recursive-feature-selection\" data-toc-modified-id=\"Linear-regression-with-Recursive-feature-selection-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Linear regression with Recursive feature selection<\/a><\/span><\/li><li><span><a href=\"#Ridge-regression\" data-toc-modified-id=\"Ridge-regression-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Ridge regression<\/a><\/span><\/li><li><span><a href=\"#Lasso-regression\" data-toc-modified-id=\"Lasso-regression-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Lasso regression<\/a><\/span><\/li><li><span><a href=\"#Random-Forest-regression\" data-toc-modified-id=\"Random-Forest-regression-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Random Forest regression<\/a><\/span><\/li><li><span><a href=\"#GradientBoosting-regressor\" data-toc-modified-id=\"GradientBoosting-regressor-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;<\/span>GradientBoosting regressor<\/a><\/span><\/li><li><span><a href=\"#XGBoost\" data-toc-modified-id=\"XGBoost-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;<\/span>XGBoost<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-exploration\" data-toc-modified-id=\"Model-exploration-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Model exploration<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy-comparison\" data-toc-modified-id=\"Accuracy-comparison-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Accuracy comparison<\/a><\/span><\/li><li><span><a href=\"#XGBoost---Prediction-scatterplot\" data-toc-modified-id=\"XGBoost---Prediction-scatterplot-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>XGBoost - Prediction scatterplot<\/a><\/span><\/li><li><span><a href=\"#XGBoost---Residual-plot\" data-toc-modified-id=\"XGBoost---Residual-plot-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>XGBoost - Residual plot<\/a><\/span><\/li><li><span><a href=\"#XGBoost---Feature-importance\" data-toc-modified-id=\"XGBoost---Feature-importance-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>XGBoost - Feature importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><li><span><a href=\"#References\" data-toc-modified-id=\"References-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>References<\/a><\/span><\/li><\/ul><\/div>","29f5dfa7":"### Basic Information","e4cd06f0":"#### Hyperparameters for grid-search","83cb878b":"#### Validation score on full dataset","2b577378":"#### Yeo-Johnson transformation","c31ef7f7":"### Feature normalization & scaling","6b0a73aa":"#### QQ-plots","663ceffc":"[1] I-Cheng Yeh, \"Modeling of strength of high performance concrete using artificial neural networks,\" Cement and Concrete Research, Vol. 28, No. 12, pp. 1797-1808 (1998)\n\n[2] https:\/\/archive.ics.uci.edu\/ml\/datasets\/Concrete+Compressive+Strength","a5b48ff8":"#### Results","be11ea94":"## Data loading and basic exploration","d5377f40":"#### Validation score on full dataset","94a46809":"## Imports and function definitions","4f72590e":"#### Hyperparamters for grid-search","d9b662fa":"## Summary","5d202d92":"### Objectives","8e5c2424":"#### Hyperparameters for grid-search","a4dad8dd":"\"Concrete is the most important material in civil engineering.\nThe concrete compressive strength is a highly nonlinear function of age and ingredients.\" [2]\n\nThis notebook contains a brief **Exploratory Data Analysis** of the [\"Revisiting a Concrete Strength regression\" Dataset](https:\/\/www.kaggle.com\/maajdl\/yeh-concret-data) and a selection of models trained to predict concrete mixture's compressive strength based on it's ingredients. \n","e63fb557":"#### Creating  new \"total_density\" column","58f50994":"As the last preprocessing step, I have used the MinMaxScaler to scale all variables into the same scale - this turned out to have positive effects on accuracy of linear models.","28585826":"**Column definitions:**\n- cement - Cement in mixture: (kg\/m<sup>3<\/sup>)\n- slag - Blast furnace slag in mixture: (kg\/m<sup>3<\/sup>)\n- flyash - Fly ash in mixture: (kg\/m<sup>3<\/sup>)\n- water - Water in mixture: (kg\/m<sup>3<\/sup>)\n- superplasticizer - Superplasticizer in mixture: (kg\/m<sup>3<\/sup>)\n- coarseaggregate - Coarse aggregate in mixture: (kg\/m<sup>3<\/sup>)\n- fineaggregate - Fine Aggregate in mixture: (kg\/m<sup>3<\/sup>)\n- age - Age of the mixture: (days)\n- csMPa - Concrete compressive strength: (MPa) ","d575f37f":"### Basic statistical information ","f1335627":"#### Histograms, skewness and kurtosis","814facb3":"**Observations:**\n- Majority of rows have **age** value of 28 days\n- There's only a handful of rows with values of 360, 120, 1","9fd2ee23":"#### Model","1ee07bec":"#### Results","2fb327fe":"**Observations:**\n- **slag**, **water**, **superplasticizer** contain a low amount of outliers - shouldn't be a problem\n- **age** contains a potentially large amount of outliers - this should be further investigated as linear models tend to be very sensitive to outliers in data","510a27bf":"#### Creating new columns with values of percentage","aec1f6be":"## Model exploration","6fc50808":"### Split into train and test sets","25a4ebdf":"### Total weight variable  ","21a27d78":"#### Model and RFE estimator ","e380573d":"### XGBoost","bc74e9aa":"**Observations:**\n- **total_density** has a relatively good distribution with acceptable values of skewness and kurtosis\n- Interestingly, one of the recipes has **total_density** which is considerably higher than the rest - around 2550 kg\/m<sup>3<\/sup>","81babafc":"### XGBoost - Prediction scatterplot","7cc6c02c":"#### Correlation pairplot of variables with highest correlations to target","b968766f":"**Observations:**\n- **cement** is slightly positively correlated with the target variable\n- **superplasticizer** and **age** are very lightly correlated with the target variable\n- **water** is very lightly negatively correlated with the target variable\n- No predictors are highly correlated with eachother - **There's no multicollinearity**","44e4f5e9":"- Cumultative weights of all ingredients in each cement mixture vary and that difference may have an impact on compressive strength\n- Therefore, I'm going to create a new \"total_density\" column and evaluate whether its presence improves model accuracy","513e5f52":"**Observations:**\n\n- QQ-plot confirms our earlier findings: csMPa follows normal distribution relatively well","3caa8dec":"### Reading data into DataFrame","a35ed198":"### Lasso regression","17b312ee":"**EDA**\n\nThe objective of this notebook is to perform a brief **data analysis** in order to identify properties of variables and relationships between them. \n\n\n**Data cleaning and variable engineering**\n\nAcquired information will then be used to **clean the data** and **engineer variables** in a way that should improve model performance. \n\n\n**Model training**\n\nNext, I will train **6 regression models**: \n- LinearRegression\n- Ridge \n- Lasso\n- RandomForestRegressor\n- GradientBoostingRegressor\n- XGBRegressor\n\nusing **GridSearchCV** and **RandomizedSearchCV** to optimize their parameters. This balanced selection of linear and non-linear models should hopefully provide us with an accurate solution for this problem.\n\n\n**Model exploration**\n\nLastly, I will **analyze the information** provided by the best performing model to gain deeper insights about the data as well as the model and try to find out which variables have the largest impact on concrete's compressive strength.","17410a2e":"### XGBoost - Feature importance","1e529839":"#### Model","31b0cf3e":"**Observations:**\n\n- There's only a few outliers, so that shouldn't impact model accuracy in a significant way","29f93f8c":"### Ridge regression","36d58285":"**Observations:**\n- **total_density** is the most important variable, providing over 20% of mean decrease in impurity\n- **total_density**, **age** and **cement** together provide around 50% of mean decrease in impurity\n- **flyash**, **slag %**, **coarseaggregate**, **flyash %** were the least important, each of them provdided less than 3% of decrease in impurity","a9670431":"**Observations:**\n- Variables **slag**, **superplasticizer**, **age** have moderatly high skewness (absolute value larger than 0.7)\n- Variables **flyash**, **superplasticizer** have moderatly high kurtosis (absolute value larger than 1)\n- Variable **age** has very high kurtosis - **12.17**\n- Normalizing those variables will most likely raise model accuracy\n- **slag**, **flyash**, **superplasticizer** contain a large amount of 0-values","6834811d":"## Model training","78f598da":"As the dataset contains a moderate amount of skewed predictors, some of which have 0-values, I have used the Yeo-Johnson transformation to normalize all columns.\n\nI have tried transforming only highly skewed variables but it worked worse than simply transforming everything.","5c810eea":"### Null-value and type checking","359f9536":"#### Pearson's correlation heatmap","4ee0bb0d":"### Feature variable exploration","169350a1":"#### Validation score on full dataset","09db375e":"#### Histogram, skewness and kurtosis","479919cb":"<h1 style=\"color:black; background-color:#a3a3a3; font-family:Helvetica;font-size:200%;text-align:center\">Thank you for your attention! <\/h1>","9d60b129":"### Random Forest regression","df5a6785":"### Values of percentage","c9295d19":"In this section I'm going to train 6 different models 3 of them are linear (Linear+RFE, Ridge and Lasso) and 3 are non-linear (RandomForest, GradientBoosting, XGBRegressor).\n\nOn each of the models I'll perform hyperparameter optimization using 5-fold cross validation with GridSearchCV on all models except XGBRegressor.\n\nXGBRegressor has too many customizable parameters for GridSearchCV to finish in a reasonable amount of time, so I will use RandomSearchCV for that model.","7dbe62f2":"#### Grid search with cross validation","d14c1ccd":"#### Grid search with cross validation","bf14fbfd":"**Observations:**\n\n- Dataset contains 1030 rows and 9 columns\n","ff0752ad":"## References","cb0f5b50":"<h1 style=\"color:black; background-color:#a3a3a3; font-family:Helvetica;font-size:400%;text-align:center\">Concrete strength - Regression<\/h1>","3378b7ce":"#### Results","3bb42bff":"#### Grid search with cross validation","afa43f10":"\nThe [\"Revisiting a Concrete Strength regression\" Dataset](https:\/\/www.kaggle.com\/maajdl\/yeh-concret-data) contains information about different concrete recipes and compressive strength of each of them. ","ac26b294":"### XGBoost - Residual plot","1083dc06":"**Observations:**\n\n- Target variable csMPa has a decently normal distribution \n- Relatively low skew and kurtosis values","4f32670b":"#### QQ-plot","9c2ada0c":"#### Model","081830f1":"## Variable exploration"}}