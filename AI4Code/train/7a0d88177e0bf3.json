{"cell_type":{"bccf16d0":"code","5705632c":"code","06e1c384":"code","2caa27a6":"code","96847308":"code","01f08991":"code","e63dc675":"code","3fe57dec":"code","669a0e0d":"code","0775dda1":"code","5ad8e960":"code","3c369eb6":"code","65a458c7":"code","d9e57984":"code","8372ee72":"code","953fb6e4":"code","09506906":"code","1c16072f":"code","33c046aa":"code","72781563":"code","8bb922df":"code","2cd9c55c":"code","8cc9aded":"code","ff1aace6":"code","77c001b7":"code","a1b90678":"code","0273686a":"code","25b4207a":"code","878ccf30":"code","80d73096":"code","88285369":"code","9b74097b":"code","023ce58a":"code","9f6b5e0e":"code","03008081":"code","30ea5d44":"code","b5e4f979":"code","2402cfea":"code","6d7c9ee5":"code","e0304754":"code","f9b3462a":"code","c089b1e2":"code","5a17e66f":"code","912432f5":"code","441c5cb1":"code","144e7064":"code","c23ca7f5":"code","2b5618f4":"code","5ca41b48":"code","f59786c5":"code","5ac50c71":"code","2ad0ccff":"code","73244133":"code","025c80eb":"code","2ae99c71":"code","483cd7b8":"code","fea1eafd":"code","7c85b7e1":"code","83d42f69":"code","d7deb99d":"code","ae4358a4":"code","fb97016a":"code","2f3347bb":"code","a8e4e2b7":"code","23545c77":"code","114adecc":"code","ac9e28f1":"code","3abe7e32":"code","edcaa02a":"code","597a289b":"code","54c488e5":"code","960b328f":"code","1338fdff":"code","46fd29a1":"code","445b75ca":"code","03a3932c":"code","c1f46d10":"code","4141cf63":"code","8617658d":"code","69c609ad":"code","7327cc8a":"code","41b34f75":"code","f58569d8":"code","82635ed1":"code","cd2969a5":"code","0b469562":"code","86b06694":"code","25f6ab3f":"code","c77809a8":"code","a1c37f2a":"code","f7ae4bd2":"code","75f7f208":"code","80d3ceb0":"code","4c7b5f84":"code","e407f9b9":"code","bd8c6d01":"code","5c364964":"code","7fc2e24e":"code","394cd9be":"code","96ebff20":"code","a3d6a4ad":"code","19cd57be":"code","fd6735f4":"code","c1f74240":"code","be14b9f9":"code","487224a8":"code","d445c061":"code","90968da4":"markdown","f221a50e":"markdown","258d5e13":"markdown","a89a8c27":"markdown","ba51e1c5":"markdown","d05ef157":"markdown","a3e68986":"markdown","52b1a5de":"markdown","1aebea82":"markdown","e7b3c635":"markdown","e2826a6a":"markdown","cf40ec0f":"markdown","96b36cff":"markdown","c175e326":"markdown","141d836d":"markdown","20cdd63e":"markdown","01b473c9":"markdown","a25a6428":"markdown","5b65553c":"markdown","93a5e6e1":"markdown","0cebbc48":"markdown","1e2cd872":"markdown","872627a6":"markdown","9fa3ae34":"markdown","b37a3289":"markdown","fa22aaac":"markdown","6de55257":"markdown","0ad9a53e":"markdown","59b64552":"markdown","765d48be":"markdown","bfb34687":"markdown","652d6870":"markdown","61c920cf":"markdown","f2b531c5":"markdown","e35f5ff7":"markdown","2b37e14f":"markdown","bdd9f8ce":"markdown","48fc5d94":"markdown","ec91d8ee":"markdown","616c4cb4":"markdown","e80253fc":"markdown","c7f86611":"markdown","7b2e9e59":"markdown","7de2d8b0":"markdown","e40849d1":"markdown","f7bd36b9":"markdown","8e6fda9e":"markdown","0eca7dc4":"markdown","b3d8b6fd":"markdown","4e0baa74":"markdown","91c661b3":"markdown","210e37d6":"markdown","8c3664e2":"markdown","2aae6447":"markdown","dc14ef1b":"markdown","98e2831c":"markdown","0a95b23e":"markdown","b71ea004":"markdown","333428cc":"markdown","79a991c7":"markdown","328b402a":"markdown","2b01d93c":"markdown","b9bd5e66":"markdown","61581e87":"markdown","fa8da4c6":"markdown","91cc5db6":"markdown","c41150d1":"markdown","534628ca":"markdown","0d6dfdc2":"markdown","6f8ca682":"markdown","8ccdd7f3":"markdown"},"source":{"bccf16d0":"#Imports\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.datasets import make_biclusters\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats('retina')","5705632c":"\n\n# # Kaggle reading data sets.\n\n\ntrain_or = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_or = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ng_s_or = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ng_s = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\n","06e1c384":"train.head() # display","2caa27a6":"test.head() # display","96847308":"train.shape\ntest.shape","01f08991":"train.isna().sum() # get the numbers of null values.\n# Age, Cabin and Embarked columns have null values.","e63dc675":"test.isna().sum() # get the numbers of null values.\n# Age, Fare, and Cabin columns have null values.","3fe57dec":"# Another way of visualize null values.\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","669a0e0d":"train.dtypes","0775dda1":"test.dtypes","5ad8e960":"# renaming train columns\ntrain = train.rename(columns={'PassengerId':\"id\",\n                                    'Pclass':'p_class',\n                                    'Name':\"name\",\n                                    \"Sex\":\"sex\",\n                                    \"Age\":\"age\",\n                           \"SibSp\":\"sib_sp\",\n                           \"Parch\":\"parch\",\n                           \"Ticket\":\"ticket\",\n                           \"Fare\":\"fare\",\n                           \"Cabin\":\"cabin\",\n                           \"Embarked\":\"embarked\",\n                          \"Survived\":\"survived\"})\ntrain.head() # checking","3c369eb6":"# renaming test columns\ntest = test.rename(columns={'PassengerId':\"id\",\n                                    'Pclass':'p_class',\n                                    'Name':\"name\",\n                                    \"Sex\":\"sex\",\n                                    \"Age\":\"age\",\n                           \"SibSp\":\"sib_sp\",\n                           \"Parch\":\"parch\",\n                           \"Ticket\":\"ticket\",\n                           \"Fare\":\"fare\",\n                           \"Cabin\":\"cabin\",\n                           \"Embarked\":\"embarked\"})\n\ntest.head() # checking","65a458c7":"# displying how many ports are in Embarked column?(train data)\ntrain.embarked.value_counts()","d9e57984":"# code.\n# # S manually .\n# train['embarked']= train.embarked.replace(np.nan,'S')\n# train.embarked.value_counts()\n\n# S auto.\nmax_em=train.loc[train.embarked.value_counts().max()]['embarked']\ntrain['embarked']= train.embarked.replace(np.nan,max_em)\n\ntrain.embarked.value_counts() # to check.\n","8372ee72":"test[test.fare.isnull()]","953fb6e4":"x_mean= np.mean(test[test['p_class']==3]['fare'])\nprint('The mean fare for the Pclass (for missing fare data) is: ', x_mean)","09506906":"# only one data point has a null value in the Fare column. \n# The data point has 3 in Pclass= 3 column.\n# Therefore, replacing the null value with the average of fares in class 3. \n\ntest['fare']= test.fare.replace(np.nan,x_mean)\n\ntest[test['id']==1044]","1c16072f":"test.isna().sum() # to check null values in Fare column.","33c046aa":"list_a={}\nfor i in sorted(train['p_class'].unique()): # for each class.\n# saving it as dictionary type and use it for the following questions.\n    list_a[i]= np.mean(train[train['p_class']==i]['age']) # Calculating the fare average for each class.\nlist_a    ","72781563":"# manually:\n\n# train.loc[(train['class']==1) & (train['age'].isna()),'age']= list_a[1]\n# train.loc[(train['class']==2) & (train['age'].isna()),'age']= list_a[2]\n# train.loc[(train['class']==3) & (train['age'].isna()),'age']= list_a[3]\n\n# using function\n# defining a function 'impute_age'\nnu_classes=[]\nfor i in train['p_class'].unique(): # get the unique values of Pclass column.  \n    nu_classes.append(i)\nnu_classes\n\ndef impute_age(age_pclass): # train or test\n    # for each class, filling missing value in Age with it's ava.\n    for i in nu_classes: # 1,2,3\n        x_mean= list_a[i] # get ava.\n        # update and chane null values with ava.        \n        age_pclass.loc[(age_pclass['p_class']==i) & (age_pclass['age'].isna()),'age']= list_a[i]\n        \n# impute_age(train)\n# impute_age(test)\n# #train.head(32) # to check\n","8bb922df":"# call the function to update train and test dataframes.\n# (for train) grab age and apply the impute_age, our custom function\nimpute_age(train)\n# (for test) grab age and apply the impute_age, our custom function \nimpute_age(test)","2cd9c55c":"train.isna().sum() # checking null values.","8cc9aded":"test.isna().sum() # checking null values.","ff1aace6":"# Updating Cabin column in train data frame.\ntrain.loc[pd.notna(train['cabin']), 'cabin'] = 1 # not null with 1\ntrain.loc[pd.isna(train['cabin']), 'cabin'] = 0 # null with 0\n\ntrain['cabin'].unique() # to check","77c001b7":"# # Updating Cabin column in test data frame.\ntest.loc[pd.notna(test['cabin']), 'cabin'] = 1 # not null with 1\ntest.loc[pd.isna(test['cabin']), 'cabin'] = 0 # null with 0\n\ntest['cabin'].unique() #to check","a1b90678":"train.isna().sum() # to check","0273686a":"test.isna().sum() # to check","25b4207a":"# re-visualization of null values.\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","878ccf30":"# Dummy the `Sex` and `Embarked` columns.\n\n\ntrain_final=train.copy()\ntrain_final= pd.get_dummies(train_final, columns=['sex'],drop_first=True)\ntrain_final= pd.get_dummies(train_final, columns=['embarked'],drop_first=True)\n\n\ntrain= pd.get_dummies(train, columns=['embarked']) # to get categorical column. \ntrain= pd.get_dummies(train, columns=['sex'])  # to get categorical column. \n\n","80d73096":"train.head() # to check.","88285369":"\n\ntest_final=test.copy()\ntest_final= pd.get_dummies(test_final, columns=['sex'],drop_first=True)\ntest_final= pd.get_dummies(test_final, columns=['embarked'],drop_first=True)\n\n\n\ntest = pd.get_dummies(test, columns=['embarked'])  # to get categorical column. \ntest = pd.get_dummies(test, columns=['sex'])  # to get categorical column. \n\ntest.head() # to check.","9b74097b":"fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 6))\n\n# train data \nsns.heatmap(train.isnull(), yticklabels=False, ax = ax[0], cbar=False, cmap='viridis')\nax[0].set_title('Train data')\n\n# test data\nsns.heatmap(test.isnull(), yticklabels=False, ax = ax[1], cbar=False, cmap='viridis')\nax[1].set_title('Test data');","023ce58a":"train.describe() # statics ","9f6b5e0e":"test.describe() # statics ","03008081":"# plotting heatmap to identify correlations\nfig, ax = plt.subplots(figsize = (16,16))\nsns.heatmap(train.corr(),annot = True, cmap = 'coolwarm')\nax.set_title('Correlation Between Features');","30ea5d44":"# function to have similar min\/max values\ndef var_standardized(var):\n    var_stand = (var-var.mean())\/var.std()\n    return var_stand","b5e4f979":"# plotting boxplot to investigate outliers\nmerge_df_stand = var_standardized(\n    train.select_dtypes(exclude=['object']).drop(['id'], axis=1)) # only numeric columns and drop year. \n\nfig,ax=plt.subplots(figsize=(16,8)) # create figure with specific size  \nsns.boxplot(data=merge_df_stand, orient='h', fliersize=5, \n                 linewidth=3, notch=True, saturation=0.5, ax=ax) # plot \nplt.title('This plot is from standardized merged data frame'); # title.\n","2402cfea":"# plotting Survived vs. none-survived\nax= sns.factorplot('survived', data=train, kind='count')\nplt.title('Survived vs. none-survived', size=16)","6d7c9ee5":"# plotting Survival by gender\nax2 =train.select_dtypes(exclude=['object']).drop(['id','fare','age','sib_sp','parch','embarked_Q','embarked_C','embarked_S', 'p_class'], axis=1).groupby('survived').mean().plot.bar()\nax2.legend(loc=3) # move legend to have clear plot. \nax2.legend(bbox_to_anchor=(1.1, 1.05))\nax2.set_title('Survival by gender',fontsize=20); # title.","e0304754":"# plotting Survival by class\nax= sns.countplot('p_class',hue='survived',data=train)\nax.set_title('Survival by class')\n\nplt.show()","f9b3462a":"# plotting Gender by Class\nax2 =train.select_dtypes(exclude=['object']).drop(['id','fare','age','sib_sp','parch','embarked_Q','embarked_C','embarked_S','survived'], axis=1).groupby('p_class').mean().plot.bar()\nax2.legend(loc=3) # move legend to have clear plot. \nax2.legend(bbox_to_anchor=(1.1, 1.05))\nax2.set_title('Gender by Class',fontsize=20); # title.","c089b1e2":"# plotting survival by embarked \nax2 =train.select_dtypes(exclude=['object']).drop(['id','fare','age','sib_sp','parch','sex_male', 'sex_female', 'p_class'], axis=1).groupby('survived').mean().plot.bar()\nax2.legend(loc=3) # move legend to have clear plot. \nax2.legend(bbox_to_anchor=(1.1, 1.05))\nax2.set_title('Survival by Embarked',fontsize=20); # title.","5a17e66f":"# plotting Embarked by class\nax2 =train.select_dtypes(exclude=['object']).drop(['id','fare','age','sib_sp','parch','sex_male', 'sex_female', 'survived'], axis=1).groupby('p_class').mean().plot.bar()\nax2.legend(loc=3) # move legend to have clear plot. \nax2.legend(bbox_to_anchor=(1.1, 1.05))\nax2.set_title('Embarked by class',fontsize=20); # title.","912432f5":"# plotting distribution of age for each class\ntrain.hist(column='age', by='p_class');","441c5cb1":"# plotting the distribution of Age for survived and none-survived\ntrain.hist(column='age', by='survived');\n# survived: 1, none-survived: 0","144e7064":"# plotting the distribution of age for each gender\n\ntrain.hist(column='age', by= 'sex_male');\n# male: 1, female: 0","c23ca7f5":"# Model Prep: Creating `X` and `y` variables\nfeatures_drop = ['id','name', 'ticket', 'survived']","2b5618f4":"# a list comprehension to select features of inetrest\nselected_features = [feature for feature in train.columns if feature not in features_drop]\nselected_features","5ca41b48":"# geting the columns to check.\ntrain[selected_features].columns  ","f59786c5":"# (train data) separating the selected_column in `X_train` and `Survived` in `y_train`\nX_train = train[selected_features] \ny_train = train['survived'] ","5ac50c71":"X_train.head() # to check","2ad0ccff":"y_train.value_counts() # this is our target for train data","73244133":"# Baseline accuracy (train data)\nbaseline_acc = 1. - y_train[y_train == 0].shape[0]\/y_train.shape[0]\n\nprint('0: ', y_train[y_train == 0].shape[0]\/y_train.shape[0])\nprint('1: ', 1- y_train[y_train == 0].shape[0]\/y_train.shape[0])","025c80eb":"X_test = test[selected_features] # # Separating the test data using a list comprehension as the train data.\ny_test = g_s_or['Survived']  # from Kaggle file (gender_submission.csv)","2ae99c71":"y_test.head() # to check","483cd7b8":"X_test.head() # to check","fea1eafd":"# RF instance with 100 trees \nrf = RandomForestClassifier(n_estimators=100) \n\n# fitting rf classifier\nrf = rf.fit(X_train,y_train)\n\n# Obtaining the predictions for rf on test data.\npred_rf = rf.predict(X_test) \npred_rf[:5] # predictions of survival for X_test using random forest","7c85b7e1":"# RF confusion matrix and classification reports.\n\n# Based on Kaggle file (gender_submission.csv).\ny_true = y_test # get y_test which is the true labels on the test set.\n\ny_pred= pred_rf # get predicted labels on the test set.  \nprint(f\"Confusion Matrix for RF:\\n{confusion_matrix(y_true,y_pred)}\")\n\n# Classification Report RF\n\ntarget_names = ['class 0', 'class 1']\nprint(f\"Classification Report for RF:\\n{classification_report(y_true, y_pred, target_names=target_names)}\")\n","83d42f69":"# Random Forest Classifier and grid search\nrf_g = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False) \n\n# Runing the GridSearch with CV = 5**\n\ngs = GridSearchCV(estimator= rf_g, cv=5, n_jobs=None,\n       param_grid={'max_depth': [1, 2, 3, 4, 5, 8]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=1) \n\n# Fiting grid search\ngs.fit(X_train, y_train)\ngs.get_params()","d7deb99d":"# the score after GridSearch on test data\ngs.score(X_test, y_test) # y_test == gender submission file from Kaggle.com","ae4358a4":"# the score of your trained model after GridSearch on train data\ngs.score(X_train, y_train)","fb97016a":"#geting the best parameters.\ngs.best_params_","2f3347bb":"# best parameter of gridsearch\nrf_bset= RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=gs.best_params_, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\n# gird search for random forest with best parameters\ngs_rf_bset = GridSearchCV(estimator= rf_bset , cv=5, n_jobs=None,\n       param_grid={'max_depth': [1, 2, 3, 4, 5, 8]},\n       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n       scoring=None, verbose=1) \n\ngs_rf_bset.get_params() # displaying parameters","a8e4e2b7":"# Fitting train data using best model.\ngs_rf_bset.fit(X_train, y_train) # fit.\n\n# Getting prediction for the best model\npred_rf_bset= gs_rf_bset.predict(X_test)\npred_rf_bset[:5] # displaying sample\n\ngs_rf_bset.score(X_train, y_train)","23545c77":"#  confusion matrix\n#  pred_rf_bset confusion matrix and classification reports for the best model.\ny_true = y_test # get y_test which is the true labels on the test set.\ny_pred= pred_rf_bset # get predicted labels on the test set.  \nprint(f\"Confusion Matrix:\\n{confusion_matrix(y_true,y_pred)}\")\ncon_mat= confusion_matrix (y_true,y_pred,labels=[1,0])\n\n# Classification Report best model after gridsearch\n\n\ntarget_names = ['class 0', 'class 1']\nprint(f\"Classification Report:\\n{classification_report(y_true, y_pred, target_names=target_names)}\")","114adecc":"baseline_acc = 1. - y_pred[y_pred == 0].shape[0]\/y_pred.shape[0]\n# print('Baseline:',baseline_acc) #not needed\n\nprint('0: ', y_pred[y_pred == 0].shape[0]\/y_pred.shape[0])\nprint('1: ', 1- y_pred[y_pred == 0].shape[0]\/y_pred.shape[0])\n\n# Kaggle Submission\n\npred = gs_rf_bset.predict(X_test)\ng_s_or['Survived']= pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_best_rf.csv',index=False)","ac9e28f1":"# bagging classifiers with the above best grid search. \nmodel_gs = BaggingClassifier(base_estimator=gs_rf_bset, verbose=1, n_jobs=-1)\nmodel_gs.fit(X_train, y_train)","3abe7e32":"model_gs.score(X_train, y_train)","edcaa02a":"# Kaggle Submission , # PassengerId\n\npred = model_gs.predict(X_test)\ng_s_or['Survived']= pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_rf_bagging_best_grid_search.csv',index=False)","597a289b":"#ET instance witn 100 trees\net = ExtraTreesClassifier(n_estimators=100)\net= et.fit(X_train,y_train) # fitting et classifier\n\n# Obtaining the predictions for et on test data.\npred_et = et.predict(X_test)\npred_et[:5] # predictions of survival for X_test using extraTrees ","54c488e5":"# ET confusion matrix and classification reports.\n\ny_true = y_test # get y_test which is the true labels on the test set.\n\ny_pred= pred_et # get predicted labels on the test set.  \nprint(f\"Confusion Matrix for ET:\\n{confusion_matrix(y_true,y_pred)}\")\n\n# Classification Report ET\n\ntarget_names = ['class 0', 'class 1']\nprint(f\"Classification Report for ET:\\n{classification_report(y_true, y_pred, target_names=target_names)}\")","960b328f":"# our grid\nex_g = ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False) \n\nex_gs = GridSearchCV(estimator= ex_g, cv=5, n_jobs=None,\n                  iid='warn',\n                  param_grid={'max_depth': [1, 2, 3, 4, 5, 8]},\n                  pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n                  scoring=None, verbose=1)  \n\n# fitting grid search with extra tree\nex_gs.fit(X_train, y_train) \nex_gs.get_params()","1338fdff":"ex_gs.score(X_test, y_test) # test score","46fd29a1":"ex_gs.score(X_train, y_train) # train score","445b75ca":"ex_gs.best_params_ ","03a3932c":"ex_bset= ExtraTreesClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth= ex_gs.best_params_, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\ngs_ex_bset = GridSearchCV(estimator= ex_bset,\n                          cv=5, param_grid={'max_depth': [1, 2, 3, 4, 5, 8]}, n_jobs=None,scoring=None, verbose=1) \ngs_ex_bset.get_params()","c1f46d10":"gs_ex_bset.fit(X_train, y_train)\n\npred_ex_bset= gs_ex_bset.predict(X_test)\npred_ex_bset[:5] # displaying sample","4141cf63":"pred = gs_ex_bset.predict(X_test)\n\ng_s_or['Survived'] = pred\n\n# Saving the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_et_gs_best_model.csv',index=False)","8617658d":"# Grid search with decision tree \nparm_grid2 = {'max_depth':[1,2,3,4,5,6,7,8]}\ngrid2 = GridSearchCV(DecisionTreeClassifier(), parm_grid2, cv=5, verbose = 1)\ngrid2.fit(X_train, y_train)\n# kaggle submission\npred = grid2.predict(X_test) \ng_s_or['Survived'] = pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_gs_dt.csv',index=False)\n\ngrid2.best_score_    ","69c609ad":"# bagging classifer with grid search\nparam = { 'max_features': [0.6,0.8, 1],\n        'n_estimators': [50, 200], \n         'base_estimator__max_depth': [ 5, 20],\n         'max_samples':[1,0.5,0.7,0.8],\n         'bootstrap':[True,False]\n       \n        }\n\n\nmodel = BaggingClassifier(base_estimator=DecisionTreeClassifier())\nmodel_gs = GridSearchCV(model,param, cv=4, verbose=1, n_jobs=-1 )\nmodel_gs.fit(X_train, y_train)\n\n\nmodel_gs.best_score_\n\nmodel_gs.score(X_train, y_train)\n\n# Kaggle submission. # PassengerId\npred = model_gs.predict(X_test)\ng_s_or['Survived'] = pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_dt_gs_bagging.csv',index=False)","7327cc8a":"# selecting X features Age and p_class\nX = train[['age', 'p_class']]\ny = train.survived # target\n\nss = StandardScaler() # instantiating scaler\nXs_train = ss.fit_transform(X) # fitting scaler\nXs_test = ss.transform(X_test[['age', 'p_class']]) # fitting scaler\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(Xs_train,y)\ncross_val_score(knn, Xs_train, y, cv=5).mean()\n\nknn.predict(Xs_test)\nknn.score(Xs_train, y)\n\n# kaggle submission\npred = knn.predict(Xs_test)\ng_s_or['Survived'] = pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_KNN_st.csv',index=False)","41b34f75":"# Logistic Regression with all features + relative , ie. parch + sib_sp\n# making new feature ['relative'] that takes the number of the relativs\n\nX_train_lr= X_train.copy() \nX_train_lr['reltv'] = X_train_lr['sib_sp'] + X_train_lr['parch'] \nX_train_lr = X_train_lr[['p_class','sex_male','age','embarked_C','embarked_Q','embarked_S','reltv']]\nX_train_ss= ss.fit_transform(X_train_lr) # standardization\n\n\ntest_lr= X_test.copy()\ntest_lr['reltv'] = test_lr['sib_sp'] + test_lr['parch']\ntest_lr = test_lr[['p_class','sex_male','age','embarked_C','embarked_Q','embarked_S','reltv']]\ntest_lr_ss= ss.transform(test_lr)\n\n\nlog_modal = LogisticRegression().fit(X_train_ss,y_train)\n# cross_val_score(log_modal,X_train_lr,y_train,cv= 5).mean()\n\npred = log_modal.predict(test_lr_ss)\ng_s_or['Survived'] = pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_lg_st_reltv.csv',index=False)\n","f58569d8":"# Logistic Regression with relative , ie. parch + sib_sp\n\nX_train_lr= X_train.copy()\nX_train_lr['reltv'] = X_train_lr['sib_sp'] + X_train_lr['parch']\n\nX_train_ss= ss.fit_transform(X_train_lr)\n\n\ntest_lr= X_test.copy()\ntest_lr['reltv'] = test_lr['sib_sp'] + test_lr['parch']\n\ntest_lr_ss= ss.transform(test_lr)\n\n\nlog_modal = LogisticRegression().fit(X_train_ss,y_train)\n# cross_val_score(log_modal,X_train_lr,y_train,cv= 5).mean()\n\npred = log_modal.predict(test_lr_ss)\ng_s_or['Survived'] = pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_lg_st_reltv_only.csv',index=False)","82635ed1":"# Support vector machine model\nss = StandardScaler() # initiating standard scaler\nXs_train = ss.fit_transform(X_train)\nXs_test = ss.transform(X_test)\n\nsvm_l = svm.SVC(kernel='linear').fit(Xs_train, y_train)\nsvm_l.fit(Xs_train, y_train)\ncross_val_score(svm_l, Xs_train, y_train, cv=5).mean()","cd2969a5":"# kaggle submission\npred = svm_l.predict(X_test)\ng_s_or['Survived'] = pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_SVM_st.csv',index=False)","0b469562":"# boosting classifier model\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, \n                                 max_depth=1, random_state=0).fit(X_train, y_train)\n\npred = clf.predict(X_test)\nclf.score(X_train, y_train)","86b06694":"clf.get_params()","25f6ab3f":"# kaggle submission\ng_s_or['Survived']= pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_GradientBoosting.csv',index=False)","c77809a8":"# here, we removed the cabin column since it originally contained 687 nulls to investigate whether\n# removing the column will improve model performance \n\n# deleting cabin column\ntemp1= train.drop(['id', 'survived','cabin'], axis=1).select_dtypes(exclude=['object'])\ntemp1 # checking","a1c37f2a":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(temp1, y_train)\ny_pred = logistic_regression.predict(X_test.drop(['cabin'], axis=1).select_dtypes(exclude=['object']))\nlogistic_regression.score(temp1, y_train)","f7ae4bd2":"# kaggle submission\ng_s_or['Survived']= pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_LogisticRegression_without_cabin.csv',index=False)","75f7f208":"ss = StandardScaler() # instantiating scaler\nXs_train = ss.fit_transform(temp1) # fitting scaler\nXs_test = ss.transform(X_test.drop(['cabin'], axis=1).select_dtypes(exclude=['object'])) # fitting scaler\n\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(Xs_train, y)\ny_pred = knn.predict(Xs_test)\nknn.score(Xs_train,y) ","80d3ceb0":"# kaggle submission\ng_s_or['Survived']= pred\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_knn_without_cabin.csv',index=False)","4c7b5f84":"# Random forest classifier without cabin\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(temp1, y_train)\ny_pred = random_forest.predict(X_test.drop(['cabin'], axis=1).select_dtypes(exclude=['object']))\nrandom_forest.score(temp1, y_train)","e407f9b9":"g_s_or['Survived']= y_pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_rf_remove_cabin.csv',index=False)","bd8c6d01":"# calculate interquartile range:\nnum_colu= train.select_dtypes(include=[np.number])\nlower_range={} # create dict to save lower_range for each column\nupper_range={} # create dict to save lower_range for each column\nfor i in num_colu: # for each column.\n    # Calculate Q1(first Quarter- 25) and Q3(third Quartile- 75)\n    q75, q25 = np.percentile(train[i], [75 ,25]) \n    iqr = q75 - q25 # Calculate IQR = (Q3 - Q1).\n    lower_range[i] = q25 -(1.5 * iqr) # lower range for a column\n    upper_range[i] = q75 + (1.5 * iqr) # upper range for a column\n\nlower_range,upper_range # can be used to find the outliers as shown in next line.\n\n","5c364964":"# finding outliers\np_class_out= train[(train.p_class < lower_range['p_class']) | (train.p_class > upper_range['p_class'])] # no Outliers\nage_out= train[(train.age < lower_range['age']) | (train.age > upper_range['age'])] # 26 outliers\nsib_sp_out=train[(train.sib_sp < lower_range['sib_sp']) | (train.sib_sp > upper_range['sib_sp'])] # 46 outliers\nparch_out = train[(train.parch < lower_range['parch']) | (train.parch > upper_range['parch'])] # 213 outliers\nfare_out = train[(train.fare < lower_range['fare']) | (train.fare > upper_range['fare'])] # 116 outliers\nembarked_C_out = train[(train.embarked_C < lower_range['embarked_C']) | (train.embarked_C > upper_range['embarked_C'])] # 170 outliers\nembarked_Q_out = train[(train.embarked_Q < lower_range['embarked_Q']) | (train.embarked_Q > upper_range['embarked_Q'])] # 77 outliers\nembarked_S_out = train[(train.embarked_S < lower_range['embarked_S']) | (train.embarked_S > upper_range['embarked_S'])] # no Outliers\nsex_female_out = train[(train.sex_female < lower_range['sex_female']) | (train.sex_female > upper_range['sex_female'])] # no Outliers\nsex_male_out = train[(train.sex_male < lower_range['sex_male']) | (train.sex_male > upper_range['sex_male'])] # no Outliers\np_class_out= train[(train.p_class < lower_range['p_class']) | (train.p_class > upper_range['p_class'])] # no Outliers\n\n# printing the total number of outliers in each column\nprint('The total number of outliers in parch equals to {}'.format(len(parch_out)))\nprint('The total number of outliers in fare equals to {}'.format(len(fare_out)))\nprint('The total number of outliers in age equals to {}'.format(len(age_out)))\nprint('The total number of outliers in sib_sp equals to {}'.format(len(sib_sp_out)))\nprint('The total number of outliers in embarked_C equals to {}'.format(len(embarked_C_out)))\nprint('The total number of outliers in embarked_Q equals to {}'.format(len(embarked_Q_out)))\n\nprint('The total number of outliers in p_class equals to {}'.format(len(p_class_out)))\nprint('The total number of outliers in p_class equals to {}'.format(len(p_class_out)))\nprint('The total number of outliers in embarked_S equals to {}'.format(len(embarked_S_out)))\nprint('The total number of outliers in sex_female equals to {}'.format(len(sex_female_out)))\nprint('The total number of outliers in sex_male equals to {}'.format(len(sex_male_out)))\n","7fc2e24e":"# here, we removed embarked column since it contained 170 outliers to see whether it will affect model performance\n# Random forest classifier without cabin\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train.drop(['embarked_C'], axis=1).select_dtypes(exclude=['object']), y_train)\ny_pred = random_forest.predict(X_test.drop(['embarked_C'], axis=1).select_dtypes(exclude=['object']))\nrandom_forest.score(X_train.drop(['embarked_C'], axis=1).select_dtypes(exclude=['object']), y_train)","394cd9be":"g_s_or['Survived']= y_pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_rf_remove_embarked_C.csv',index=False)","96ebff20":"print (train.embarked_C.value_counts()) # to check the value count of embarked_C before apply dummy again. ","a3d6a4ad":"train_final.info() # to check the columns after applying dummy with drop first.","19cd57be":"# Calculate the outliers again after updating dummy with drop_first= True.  \n\n# calculate interquartile range:\nnum_colu= train_final.select_dtypes(include=[np.number])\nlower_range={} # create dict to save lower_range for each column\nupper_range={} # create dict to save lower_range for each column\nfor i in num_colu: # for each column.\n    # Calculate Q1(first Quarter- 25) and Q3(third Quartile- 75)\n    q75, q25 = np.percentile(train_final[i], [75 ,25]) \n    iqr = q75 - q25 # Calculate IQR = (Q3 - Q1).\n    lower_range[i] = q25 -(1.5 * iqr) # lower range for a column\n    upper_range[i] = q75 + (1.5 * iqr) # upper range for a column\n\nlower_range,upper_range # can be used to find the outliers as shown in next line.\n\n","fd6735f4":"# finding outliers again after updating dummy with drop_first= True. \np_class_out= train_final[(train_final.p_class < lower_range['p_class']) | (train_final.p_class > upper_range['p_class'])] # no Outliers\nage_out= train_final[(train_final.age < lower_range['age']) | (train_final.age > upper_range['age'])] # 26 outliers\nsib_sp_out=train_final[(train_final.sib_sp < lower_range['sib_sp']) | (train_final.sib_sp > upper_range['sib_sp'])] # 46 outliers\n\n\nparch_out = train_final[(train_final.parch < lower_range['parch']) | (train_final.parch > upper_range['parch'])] # 213 outliers\nfare_out = train_final[(train_final.fare < lower_range['fare']) | (train_final.fare > upper_range['fare'])] # 116 outliers\n\nembarked_Q_out = train_final[(train_final.embarked_Q < lower_range['embarked_Q']) | (train_final.embarked_Q > upper_range['embarked_Q'])] # 77 outliers\nembarked_S_out = train_final[(train_final.embarked_S < lower_range['embarked_S']) | (train_final.embarked_S > upper_range['embarked_S'])] # no Outliers\n\nsex_male_out = train_final[(train_final.sex_male < lower_range['sex_male']) | (train_final.sex_male > upper_range['sex_male'])] # no Outliers\np_class_out= train_final[(train_final.p_class < lower_range['p_class']) | (train_final.p_class > upper_range['p_class'])] # no Outliers\n\n# # printing the total number of outliers in each column\nprint('All the columns after updating dummy with drop_first=True:\\n')\nprint('The total number of outliers in parch equals to {}'.format(len(parch_out)))\nprint('The total number of outliers in fare equals to {}'.format(len(fare_out)))\nprint('The total number of outliers in age equals to {}'.format(len(age_out)))\nprint('The total number of outliers in sib_sp equals to {}'.format(len(sib_sp_out)))\nprint('The total number of outliers in embarked_Q equals to {}'.format(len(embarked_Q_out)))\n\nprint('The total number of outliers in p_class equals to {}'.format(len(p_class_out)))\nprint('The total number of outliers in embarked_S equals to {}'.format(len(embarked_S_out)))\nprint('The total number of outliers in sex_male equals to {}'.format(len(sex_male_out)))\n\n\n# The only differences are the following columns which are related to the outliers before updating dummy.\n# embarked_C and sex_female are included before the update while after the update they are not included\n\n# The total number of outliers in embarked_C equals to 170\n# The total number of outliers in sex_female equals to 0","c1f74240":"# Try RandomForestClassifier with removing embarked_Q which has 77 outliers for the updated train and test to make sure. \n\nfeatures_drop_last = ['id','name', 'ticket', 'survived']\n# a list comprehension to select features of inetrest\nselected_features_last = [feature for feature in train_final.columns if feature not in features_drop]\nselected_features_last\n\n\nX_train_final= train_final[selected_features_last]   \nX_test_final= test_final[selected_features_last]\n# Random forest classifier without cabin\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train_final.drop(['embarked_Q'], axis=1).select_dtypes(exclude=['object']), y_train)\n\ny_pred = random_forest.predict(X_test_final.drop(['embarked_Q'], axis=1).select_dtypes(exclude=['object']))\n\nrandom_forest.score(X_train_final.drop(['embarked_Q'], axis=1).select_dtypes(exclude=['object']), y_train)","be14b9f9":"g_s_or['Survived']= y_pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_rf_remove_embarked_Q_dummy_update.csv',index=False)","487224a8":"# Try RandomForestClassifier with removing parch which has 213 outliers for the updated train and test to make sure. \n\nfeatures_drop_last = ['id','name', 'ticket', 'survived']\n# a list comprehension to select features of inetrest\nselected_features_last = [feature for feature in train_final.columns if feature not in features_drop]\nselected_features_last\n\n\nX_train_final= train_final[selected_features_last]   \nX_test_final= test_final[selected_features_last]\n# Random forest classifier without cabin\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train_final.drop(['parch'], axis=1).select_dtypes(exclude=['object']), y_train)\n\ny_pred = random_forest.predict(X_test_final.drop(['parch'], axis=1).select_dtypes(exclude=['object']))\n\nrandom_forest.score(X_train_final.drop(['parch'], axis=1).select_dtypes(exclude=['object']), y_train)","d445c061":"g_s_or['Survived']= y_pred\n# Save the prediction into a file that matches the Kaggle submission requirements\ng_s_or[['PassengerId', 'Survived']].to_csv('submission_rf_remove_parch_dummy_update.csv',index=False)","90968da4":"### submitting above prediction to Kaggle resulted in score = 0.75358","f221a50e":"## Random Forest without Embarked\n","258d5e13":"### Model instantiation\n\n### Random forest classifier","a89a8c27":"#### Grid Search for Random Forest\n\n","ba51e1c5":"## Without Outliers","d05ef157":"\n* id -- PassengerId\n* survived -- 0 = No, 1 = Yes\n* p_class -- Ticket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n* name -- Passenger name \n* sex -- male \/ female \t\n* age -- age in years\t\n* sib_sp -- no. of siblings \/ spouses aboard the Titanic\t\n* parch -- no. of parents \/ children aboard the Titanic\t\n* ticket -- Ticket number\t\n* fare -- Passenger fare\t\n* cabin -- Cabin number\t\n* embarked -- Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n\t\t\t\t\t\t","a3e68986":" - There are many outliers in most columns, we can not delete all of them as deleting the outliers will result in the removal of big parts of the data\n - As a future improvement, it is recommended to find a way to deal with the outliers without deleting them","52b1a5de":"## outliers:\n- Fare, age, parch, sib_sp. The higest outliers are related to fare column. it might be a correlation between fare and survived.\n- It might be a correlatioin between class or age with survived.\n- We can remove one outlier from embarked_Q column.\n- all the above columns are not normal distribution.\n","1aebea82":" ### submitting above prediction to Kaggle resulted in score = 0.75837","e7b3c635":"### Feature Engineering.\n\n\n\n####  Performing Feature Engineering on  `Cabin` column \n\n- If there originally was a value for `Cabin` -- putting 1\n- If the value is missing\/null -- putting 0","e2826a6a":"###  Investigating data types\nDisplaying the data types of each feature. ","cf40ec0f":"#### Separating the test data into X_test and y_test. Using same features and target as above for the train data.","96b36cff":"**Using the above function for both datasets (train and test) and filling the missing data in `Age` column accordingly.**","c175e326":"**From displayed dataframes below, outliers can be noticed in different columns. More investigation is needed**","141d836d":"**(test data) What is the `Pclass` of missing fare in test dataset. Printing the complete row here.**","20cdd63e":" ### submitting above prediction to Kaggle resulted in score = 0.76555","01b473c9":" ### submitting above prediction to Kaggle resulted in score = 0.74880 ","a25a6428":"#### Tuning the bagging classifiers with grid search","5b65553c":"##  Data Cleaning and Data Exploration\n###  Displaying data","93a5e6e1":"## KNN without cabin","0cebbc48":" ### submitting above prediction to Kaggle resulted in score = 0.76315","1e2cd872":" ### submitting above prediction to Kaggle resulted in score = 0.76315","872627a6":"## Visualization","9fa3ae34":"# Classification Techniques - Titanic Passenger Information Dataset\n\n## Problem Identification\n\nThe sinking of the Titanic is the most famous shipwrecks in history. Unfortunately, more than 1500 people didn\u2019t survive. \n\nIn this challenge, we built a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\nAfter clearing the noise in the data, we proceeded by performing mulltiple classification models including Random Forest, Extra Tree, Support Vector Machine, KNN, Logistic Regression and Decision Tree to get the best score. Furthermore, we used Ensemble and tuning techniques in an attempt to increase the accuracy of our models.\n\n\n##  Model  Accuracy Table\n|Model|Score|Specific setting|\n|:-|:------|:---------------------|\n|KNN|0.50956|KNN K-fold = 5, cross validation|\n|Decision Tree|0.74880|Grid search|\n|Random Forest|0.75358|drop(column= \u2018embarked_C\u2019)|\n|Random Forest|0.75358|dummy(drop_first=True),drop(column= 'embarked_Q')|\n|Random Forest|0.75837|dummy(drop_first=True),drop(column= 'parch')|\n|Logistic Regression|0.76315|drop(column= \u2018cabin\u2019)|\n|KNN|0.76315|drop(column= \u2018cabin\u2019)|\n|Random Forest|0.76315|drop(column= \u2018cabin\u2019)|\n|Gradient Boosting Classifier|0.76315|n_estimators=100, learning_rate=1.0,                             max_depth=1, random_state=0|\n|Extra Tree|0.76555|Grid search, best model|\n|Logistic regression|0.76555|with new feature 'relative'|\n|SVM|0.76555|K-fold = 5, cross validation|\n|Random Forest|0.77272|Grid search, best model|\n|Random Forest|0.77272|Grid search, best model,bagging classifier|\n|Decision Tree|0.77272|Grid search, bagging classifier|\n\n\n\n\n##  Summary\n\nIn Model accuracy table above, muiltple models of different settings were applied to predict the survival of passengers in aboard Titanic ship. The best prediction accuracy for survival was 0.77272. Imporoving the model accuracy was challenging since multiple models of different settings resulted in similar scores. \n \nSuggestions to improve the model accuracy include, dealing with outliers without necessarily removing them. Furthermore, the majority of null values were in the cabin column. More analysis is required to predict the correct values of missing values. Creating new data through feature engineering might further improve the model performance.\n\n\n--- \n**Team members (Group 9)**\n - Fahdah Alalyan - fahdah.a15@gmail.com   \n - Amjaad Alsubaie - amjaad_636@hotmail.com  \n - Ahmed Adam - am4ma@hotmail.com\n \n---\n\n\n##  Data Wrangling\n\nLet's read the data.\n","b37a3289":"###  Renaming columns","fa22aaac":"- Most passengers were males aged between 20 - 40","6de55257":"## Random Forest without cabin\n","0ad9a53e":"**Displying the shape of datasets** ","59b64552":"## Gradient Boosting Classifier","765d48be":"## More investigation related to outliers:\n\n- We noticed that embarked_C has outliers equals to 170 eventhoght embarked_C is a catogrical column. \n- embarked_C column has only 0 or 1 values, which does not make sense.\n- After getting the value count for embarked_C column, there are 721 values of 0 while 170 values of 1.  \n- As a result, all the data points that contain 1 in embarked_C column are considered as outliers.\n- We have tried to apply dummy again, but with drop first= True, and check the outliers again to make sure whether updated dummy is going to affect the model performance.\n\n","bfb34687":"- Passengers from embarked S are the highest per each class\n- Passengers from embarked Q are the lowest for class 1 and 2","652d6870":"### Extra-trees classifier","61c920cf":"#### Excellent, looks all good. No more missing data!\n\n###  Dummies","f2b531c5":" ### submitting above prediction to Kaggle resulted in score = 0.76555 ","e35f5ff7":"# Decision Tree Classification","2b37e14f":"##  Modeling","bdd9f8ce":"- The higher the age the lower the class number\n- The lower the age the higher the class number\n- There is a negative correlation between age and class","48fc5d94":"# SVM","ec91d8ee":" ### submitting above prediction to Kaggle resulted in score = 0.50956  ","616c4cb4":"### Treating the `Age` column\n\n**computing mean age of each Pclass in the train data**","e80253fc":"- The number of males is higher than females in each class.\n- Class 3 has the highest number of male passengers\n- Class 3 has the lowest number of female passengers","c7f86611":"- The highest survival rates in passengers who assigned to class 1\n- The lowest survival rates in passengers who assigned to class 2.\n---\n- The highest mortality rates were recorded in class 3\n- The lowest mortality reates were recorded in class 1","7b2e9e59":"## Random Forest without Embarked_Q\n","7de2d8b0":"- The highest survival rates were in passengers who embarked from S.\n\n- The lowsed survival rates were in passengers who embarked from port Q.","e40849d1":" ### submitting above prediction to Kaggle resulted in score = 0.77272  ","f7bd36b9":" ### submitting above prediction to Kaggle resulted in score = 0.77272, same as rf with gs","8e6fda9e":"####  Computing the average of `Fare` of the missing `Pclass` (test data) ","0eca7dc4":"### Creating a data dictionary","b3d8b6fd":"**computing baseline accuracy for train data**","4e0baa74":" ### submitting above prediction to Kaggle resulted in score = 0.76315 , same as above score","91c661b3":"## Random Forest without parch\n","210e37d6":"**Writing a function `impute_age` to fill the mean age with respect to each `Pclass`.**<br>","8c3664e2":"**Getting the mean fare, and filling the missing value of `Fare` with everyone from the same `Pclass` (test data)**","2aae6447":"####  - Statistical Measures\n","dc14ef1b":"#### Tuning the bagging classifiers with grid search","98e2831c":" ### submitting above prediction to Kaggle resulted in score = 0.76315  ","0a95b23e":" ### submitting above prediction to Kaggle resulted in score = 0.76315","b71ea004":"- The highest mortiality rates were in younger passengers\n\n- As a result from both plots above, yonger people (aged 20-35) were mostly in class three which is the cheapest class (economy class), most of which did not survive.\n","333428cc":" ### submitting above prediction to Kaggle resulted in score = 0.76315","79a991c7":"## logistic regression without cabin","328b402a":" ### submitting above prediction to Kaggle resulted in score = 0.75358\n","2b01d93c":"###  How complete is the data?\n\nInvestigating missing values etc.","b9bd5e66":"### Grid Search for Extra Tree","61581e87":"### Other ways to improve the accuracy of modeling (removing cabin)","fa8da4c6":"# Logistic Regression","91cc5db6":"**Using heatmap to compare the difference between above and below heatmaps, to see new columns after dummies**","c41150d1":"More female passengers survived compared to males.","534628ca":"Overall survival. More people did not survived vs. survived.","0d6dfdc2":"###  Filling missing data.\n\n**filling missing data with the port of highest embarkation**","6f8ca682":" ### submitting above prediction to Kaggle resulted in score = 0.77272 ","8ccdd7f3":"# KNN"}}