{"cell_type":{"96be1c3b":"code","f1186b29":"code","db33f80f":"code","c3fc6e60":"code","e7bc8079":"code","1f3ea788":"code","1a11e9a4":"code","511ffb7d":"code","b0137ef3":"code","f4aecc62":"code","2619766f":"code","cd4dd390":"code","90571871":"code","90fb38a5":"code","f7bccf55":"code","66c5062c":"code","56d35c49":"code","8e8e9a45":"code","07c941ec":"code","1bb5437f":"code","f77fffda":"code","40200fdb":"markdown","fc1ebbd8":"markdown","8fd9996e":"markdown","f60bc4b2":"markdown","bb14204e":"markdown","b43f0a9e":"markdown"},"source":{"96be1c3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f1186b29":"import shutil\n\nfrom allennlp.common.params import Params\nfrom allennlp.common.util import prepare_environment, dump_metrics\nfrom allennlp.data.iterators import BasicIterator, BucketIterator\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers import PretrainedBertIndexer\nfrom allennlp.data.tokenizers import WordTokenizer\nfrom allennlp.data.tokenizers.word_splitter import BertBasicWordSplitter\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper\nfrom allennlp.modules.seq2vec_encoders import BagOfEmbeddingsEncoder\nfrom allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\nfrom allennlp.modules.token_embedders import Embedding\nfrom allennlp.training import Trainer\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\n\n# from utility script\nfrom swemencoder import SWEMEncoder\nfrom toxiccommentclassificationreader import ToxicCommentClassificationReader\nfrom toxiccommentpredictor import ToxicCommentPredictor\nfrom toxicbaseclassifier import ToxicBaseClassifier\nfrom toxicbertclassifier import ToxicBertClassifier","db33f80f":"# from https:\/\/github.com\/LiyuanLucasLiu\/RAdam\/blob\/master\/radam.py\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss","c3fc6e60":"# from https:\/\/github.com\/lonePatient\/lookahead_pytorch\/blob\/master\/optimizer.py\nimport itertools as it\nfrom torch.optim import Optimizer\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer,alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        self.optimizer = base_optimizer\n        self.param_groups = self.optimizer.param_groups\n        self.alpha = alpha\n        self.k = k\n        for group in self.param_groups:\n            group[\"step_counter\"] = 0\n        self.slow_weights = [[p.clone().detach() for p in group['params']]\n                                for group in self.param_groups]\n\n        for w in it.chain(*self.slow_weights):\n            w.requires_grad = False\n            \n        self.state = base_optimizer.state\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n        loss = self.optimizer.step()\n        for group,slow_weights in zip(self.param_groups,self.slow_weights):\n            group['step_counter'] += 1\n            if group['step_counter'] % self.k != 0:\n                continue\n            for p,q in zip(group['params'],slow_weights):\n                if p.grad is None:\n                    continue\n                q.data.add_(self.alpha,p.data - q.data)\n                p.data.copy_(q.data)\n                self.state = self.optimizer.state\n        return loss","e7bc8079":"params = Params({})\nprepare_environment(params)","1f3ea788":"reader = ToxicCommentClassificationReader(token_indexers={\n    \"tokens1\": SingleIdTokenIndexer(),\n    \"tokens2\": SingleIdTokenIndexer(),\n})\nall_dataset = reader.read('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntrain_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)\nvocab = Vocabulary.from_instances(train_dataset, min_count={'tokens': 3})\niterator = BucketIterator(\n    batch_size=512,\n    sorting_keys=[(\"tokens\", \"num_tokens\")],\n)\niterator.index_with(vocab)","1a11e9a4":"glove_params = Params({\n    'pretrained_file': '..\/input\/glove-stanford\/glove.twitter.27B.200d.txt',\n    'embedding_dim': 200,\n    'trainable': False\n})\nfasttext_params = Params({\n    'pretrained_file': '..\/input\/fatsttext-common-crawl\/crawl-300d-2M\/crawl-300d-2M.vec',\n    'embedding_dim': 300,\n    'trainable': False\n})\nglove_embedding = Embedding.from_params(vocab, glove_params)\nfasttext_embedding = Embedding.from_params(vocab, fasttext_params)\nword_embeddings = BasicTextFieldEmbedder({\"tokens1\": glove_embedding, \"tokens2\": fasttext_embedding})\nseq2vec_encoder = BagOfEmbeddingsEncoder(embedding_dim=word_embeddings.get_output_dim())","511ffb7d":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=None,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","b0137ef3":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","f4aecc62":"seq2vec_encoder = SWEMEncoder(embedding_dim=word_embeddings.get_output_dim())","2619766f":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=None,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","cd4dd390":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","90571871":"lstm = torch.nn.LSTM(\n    bidirectional=True,\n    input_size=word_embeddings.get_output_dim(),\n    hidden_size=40,\n    num_layers=2,\n    batch_first=True\n)\nseq2seq_encoder = PytorchSeq2SeqWrapper(lstm)\nseq2vec_encoder = SWEMEncoder(embedding_dim=seq2seq_encoder.get_output_dim())","90fb38a5":"model = ToxicBaseClassifier(\n    text_field_embedder=word_embeddings,\n    seq2seq_encoder=seq2seq_encoder,\n    seq2vec_encoder=seq2vec_encoder,\n    dropout=0.5,\n    num_labels=6,\n    vocab=vocab\n)\nmodel.cuda()\n\ntrainer = Trainer(\n    model=model,\n    optimizer=Lookahead(RAdam(model.parameters())),\n    iterator=iterator,\n    train_dataset=train_dataset,\n    validation_dataset=validation_dataset,\n    cuda_device=0,\n    num_epochs=1000,\n    grad_norm=5.0,\n    grad_clipping=1.0,\n    patience=10\n)\nmetrics = trainer.train()","f7bccf55":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","66c5062c":"BERT_MODEL_PATH = '..\/input\/bertpretrained\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\nWORK_DIR = \"..\/working\/\"\nconvert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n    BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin'\n)\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","56d35c49":"token_indexer = PretrainedBertIndexer(\n    pretrained_model=BERT_MODEL_PATH,\n    max_pieces=128,\n    do_lowercase=True,\n)\n\ntokenizer = WordTokenizer(word_splitter=BertBasicWordSplitter())\n\nreader = ToxicCommentClassificationReader(\n    tokenizer=tokenizer,\n    token_indexers={\"bert\": token_indexer}\n)\n\nall_dataset = reader.read('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntrain_dataset, validation_dataset = train_test_split(all_dataset, test_size=0.2, random_state=11)\n\niterator = BucketIterator(\n    batch_size=64,\n    sorting_keys=[(\"tokens\", \"num_tokens\")],\n)\n\nvocab = Vocabulary()\niterator.index_with(vocab)\n\nmodel = ToxicBertClassifier(\n    vocab=vocab,\n    bert_model=WORK_DIR,\n    num_labels=6\n)","8e8e9a45":"model.cuda()\n\ntrainer = Trainer(model=model,\n                  optimizer=Lookahead(RAdam(model.parameters())),\n                  iterator=iterator,\n                  train_dataset=train_dataset,\n                  validation_dataset=validation_dataset,\n                  cuda_device=0,\n                  num_epochs=1000,\n                  grad_norm=5.0,\n                  grad_clipping=1.0,\n                  patience=3)\nmetrics = trainer.train()","07c941ec":"print('metrics: {}'.format(metrics))\nprint('best_validation_loss: {}'.format(metrics['best_validation_loss']))","1bb5437f":"test_dataset = reader.read('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')\nseq_iterator = BasicIterator(batch_size=64)\nseq_iterator.index_with(vocab)","f77fffda":"predictor = ToxicCommentPredictor(model, seq_iterator, cuda_device=0)\ntest_preds = predictor.predict(test_dataset)\nsubmission = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv')\nsubmission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = test_preds\nsubmission.to_csv('submission.csv', index=False)","40200fdb":"Try simple bag of embedding model","fc1ebbd8":"seq2vec_encoder: BagOfEmbeddingsEncoder -> SWEMEncoder","8fd9996e":"Try BERT base model","f60bc4b2":"### reference\n* \"Writing code for NLP Research\" Tutorial at EMNLP 2018: https:\/\/docs.google.com\/presentation\/d\/17NoJY2SnC2UMbVegaRCWA7Oca7UCZ3vHnMqBV4SUayc\/\n\n* \u201cAn In-Depth Tutorial to AllenNLP (From Basics to ELMo and BERT)\u201d: http:\/\/mlexplained.com\/2019\/01\/30\/an-in-depth-tutorial-to-allennlp-from-basics-to-elmo-and-bert\/\n\n* \"About my 0.9872 single model\": https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/52644\n\n* \"Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms\": https:\/\/arxiv.org\/abs\/1805.09843 ACL 2018\n\n* \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\": https:\/\/arxiv.org\/abs\/1810.04805 NAACL 2019","bb14204e":"Sets random seeds for reproducible experiments","b43f0a9e":"seq2seq_encoder: None -> Bidirectional LSTM"}}