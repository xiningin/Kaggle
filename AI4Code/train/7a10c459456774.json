{"cell_type":{"4146c386":"code","5657c989":"code","3266e27c":"code","88b41c91":"code","268581fb":"code","3db802af":"code","757d6761":"code","eef6fb48":"code","bb965a2f":"code","52816147":"code","67e5614d":"code","d9a4e2b7":"code","aaecc8f5":"code","bbfa240d":"code","38a38595":"code","4c6d4a41":"code","0edb0407":"code","f6926bd1":"code","7a91707c":"code","61b1b588":"code","0911b5ef":"code","28d840f4":"code","40daeb4d":"code","3eb6b710":"code","155f33c3":"code","8bc85e4d":"code","0429258c":"code","ddfb6804":"code","482e87b7":"code","824d5d32":"code","27b572c2":"code","31b675da":"code","7d60ba2c":"code","c8dfdbe3":"code","bb67e3cf":"code","b9de173c":"code","b0d51c45":"code","b4327adc":"code","7c49320f":"markdown","cb47c13f":"markdown","975e0133":"markdown","cddf2020":"markdown","59045e46":"markdown","789d6230":"markdown","7e9cd401":"markdown","c74229ef":"markdown","4920a4cd":"markdown","6141dbbb":"markdown","bdbf269d":"markdown"},"source":{"4146c386":"import pandas as pd","5657c989":"#Loading the datasets\n\ntrue_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\nfake_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')","3266e27c":"true_df.head()","88b41c91":"fake_df.head()","268581fb":"#Creating 'check' on both dfs that will be the target feature.\n\ntrue_df['check'] = 'TRUE'\nfake_df['check'] = 'FAKE'","3db802af":"true_df.head()","757d6761":"fake_df.head()","eef6fb48":"true_df.describe()","bb965a2f":"fake_df.describe()","52816147":"#We will combine both dfs.\n\ndf_news = pd.concat([true_df, fake_df])","67e5614d":"df_news.head(30)","d9a4e2b7":"df_news.info()","aaecc8f5":"#Shuffling to see some Fakes\n\ndf_news.sample(frac = 1)","bbfa240d":"#Searching for null values.\n\ndf_news.isna().sum()","38a38595":"#We will join title, text and subject to create the article feature\ndf_news['article'] = df_news['title']+\"\"+df_news['text']+\"\"+['subject']","4c6d4a41":"#Creating the final Dataframe with article and check.\n\ndf = df_news[['article','check']]","0edb0407":"#Converting to lower case\n\ndf['article'] = df['article'].apply(lambda x: x.lower())","f6926bd1":"df['article'].head()","7a91707c":"#Removing punctuation\n\nimport string\n\ndef punctuation_removal(messy_str):\n    clean_list = [char for char in messy_str if char not in string.punctuation]\n    clean_str = ''.join(clean_list)\n    return clean_str","61b1b588":"df['article'] = df['article'].apply(punctuation_removal)\ndf['article'].head()","0911b5ef":"#Removing stopwords\n\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndf['article'].apply(lambda x: [item for item in x if item not in stop])","28d840f4":"df['article']","40daeb4d":"%matplotlib inline\n\nfrom wordcloud import WordCloud\n\nall_words = ' '.join([text for text in df.article])\n\nwordcloud = WordCloud(width= 800, height= 500,\n                          max_font_size = 110,\n                          collocations = False).generate(all_words)","3eb6b710":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,7))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","155f33c3":"#Function to generate wordcloud to True news.\n\ndef wordcloud_true(text, column_text):\n    true_text = text.query(\"check == 'TRUE'\")\n    all_words = ' '.join([text for text in true_text[column_text]])\n\n    wordcloud = WordCloud(width= 800, height= 500,\n                              max_font_size = 110,\n                              collocations = False).generate(all_words)\n    plt.figure(figsize=(10,7))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","8bc85e4d":"#Function to generate wordcloud to Fake news.\n\ndef wordcloud_fake(text, column_text):\n    fake_text = text.query(\"check == 'FAKE'\")\n    all_words = ' '.join([text for text in fake_text[column_text]])\n\n    wordcloud = WordCloud(width= 800, height= 500,\n                              max_font_size = 110,\n                              collocations = False).generate(all_words)\n    plt.figure(figsize=(10,7))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","0429258c":"#Wordcloud of the true news.\n\nwordcloud_true(df, \"article\")","ddfb6804":"#Wordcloud of the fake news.\n\nwordcloud_fake(df, \"article\")","482e87b7":"from nltk import tokenize\n\ntoken_space = tokenize.WhitespaceTokenizer()","824d5d32":"import seaborn as sns\nimport nltk\n    \ndef pareto(text, column_text, quantity):\n    all_words = ' '.join([text for text in text[column_text]])\n    token_phrase = token_space.tokenize(all_words)\n    frequency = nltk.FreqDist(token_phrase)\n    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n                                   \"Frequency\": list(frequency.values())})\n    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n    plt.figure(figsize=(12,8))\n    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n    ax.set(ylabel = \"Count\")\n    plt.show()","27b572c2":"#The 20 more frequent words.\n\npareto(df, \"article\", 20)","31b675da":"# #Lemmatization\n\n# from nltk.stem import WordNetLemmatizer \n\n# lemmatizer = WordNetLemmatizer()\n\n# def lemmatize_text(text):\n#     return [lemmatizer.lemmatize(w) for w in df[\"article\"]]\n\n# df['article'] = df[\"article\"].apply(lemmatize_text)","7d60ba2c":"from sklearn.feature_extraction.text import CountVectorizer\n\n#Creating the bag of words\nbow_article = CountVectorizer().fit(df['article'])\n\narticle_vect = bow_article.transform(df['article'])","c8dfdbe3":"#TF-IDF\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(article_vect)\nnews_tfidf = tfidf_transformer.transform(article_vect)\nprint(news_tfidf.shape)","bb67e3cf":"#We will use 20% of the data to test the model.\n\nfrom sklearn.model_selection import train_test_split\nX = news_tfidf\ny = df['check']\n\n\nX_train, X_test, Y_train,Y_test= train_test_split(X, y, test_size=0.2)","b9de173c":"from sklearn.linear_model import SGDClassifier\n\nfake_detector_svc = SGDClassifier().fit(X_train, Y_train)","b0d51c45":"prediction_svc = fake_detector_svc.predict(X_test)","b4327adc":"print (classification_report(Y_test, prediction_svc))","7c49320f":"Visualizing the data with Wordcloud.","cb47c13f":"Now we will plot the pareto chart to better visualize the frequencies of the words.","975e0133":"We will divide the data in to fake and true. ","cddf2020":"Now we will combine both dataset and in the next step we will check it shuffled.","59045e46":"Now We will train our data using Suport Vector Machine.","789d6230":"Lemmatization taking several hours to execute so I decided to comment it.","7e9cd401":"The objective of this work is to use Machine learning to predict if the news is True or Fake.","c74229ef":"Support Vector Machine.","4920a4cd":"# Fake news detector","6141dbbb":"We will create the BOW and the TF-IDF.","bdbf269d":"Let's begin importing pandas."}}