{"cell_type":{"d00123fe":"code","ea7d5ed2":"code","068a3f57":"code","fe3ddef5":"code","3f674609":"code","87bb5067":"code","90b471b5":"code","0abeaee8":"code","d519c0bb":"code","d8bddae3":"code","93042537":"code","071d510b":"code","910c988a":"code","ff35785b":"code","e6be3799":"code","f97a9267":"code","874d187a":"code","829f88e6":"code","617c3edf":"code","ec69074c":"code","e1267d0b":"code","af1a63ad":"code","8953ed86":"code","391d84df":"code","82dad8d4":"code","4ad4e3a8":"code","5b29fc80":"code","dc8cbbfa":"code","6687c4a8":"code","4b9a5c97":"code","4824a546":"code","f1716bbf":"code","ec73b83f":"code","13735b47":"code","8cb781a4":"code","f5c2a587":"code","80bd92d0":"code","f98eb9b4":"code","16921bd5":"code","07030d2d":"code","3361b381":"code","fdd73973":"code","cf9bb832":"code","5974835f":"code","510b9607":"code","b3bbd0e1":"code","1091ac75":"code","f066451a":"code","ad4578af":"code","66ba04bd":"code","8d408ddc":"code","48a8cb92":"code","30efa745":"code","6384ead8":"code","0c8e2a61":"markdown","9521a965":"markdown","0b2ecc2c":"markdown","ad5edab2":"markdown","751d77e3":"markdown","c97932a4":"markdown","717b47ea":"markdown","1f0eb46c":"markdown","8e387370":"markdown","855c248d":"markdown","9effab49":"markdown","a396c4a7":"markdown","3c8a9e6b":"markdown","b2866bb2":"markdown","9caa0ac8":"markdown","cb23e798":"markdown","1c03acee":"markdown","93f127a6":"markdown","0eccb91c":"markdown","59e0b1e0":"markdown","bbc5e63c":"markdown","b89be392":"markdown","5d6e6323":"markdown","62a015d5":"markdown","570bbf05":"markdown","a155168a":"markdown","13c41f9a":"markdown","c55eab79":"markdown","b2bcf435":"markdown","61ea8f3f":"markdown","7425bca4":"markdown","4cf26040":"markdown","7744457e":"markdown","503d6bf7":"markdown","eab582be":"markdown","bafd1adb":"markdown","02952028":"markdown","b23495e1":"markdown","1fcb90b8":"markdown","17f870e5":"markdown","81e8ea9b":"markdown","c7c15115":"markdown","acc0e4e9":"markdown","f8772e26":"markdown","357e6945":"markdown","f1501a92":"markdown","e03cb13a":"markdown","68121f6d":"markdown","92f94b54":"markdown","e72eed6c":"markdown","1adcb732":"markdown","9b865a12":"markdown"},"source":{"d00123fe":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n# Library to work with Regular Expressions\nimport re\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 10,8","ea7d5ed2":"# Importing train dataset\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\n\n# Importing test dataset\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","068a3f57":"#Looking data format and types\nprint(df_train.info())\n\n# printing test info()\nprint(df_test.info())","fe3ddef5":"#Some Statistics\ndf_train.describe()","3f674609":"#Take a look at the data\nprint(df_train.head())","87bb5067":"#Looking how the data is and searching for a re patterns\ndf_train[\"Name\"].head()","90b471b5":"#GettingLooking the prefix of all Passengers\ndf_train['Title'] = df_train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\n#defining the figure size of our graphic\nplt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=df_train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()","0abeaee8":"#Doing the same on df_test with regular expressions\ndf_test['Title'] = df_test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))","d519c0bb":"#Now, I will identify the social status of each title\n\nTitle_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n    \n# we map each title to correct category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","d8bddae3":"#printing the chance to survive by each title\nprint(\"Chances to survive based on titles: \") \nprint(df_train.groupby(\"Title\")[\"Survived\"].mean())\n\n# figure size\nplt.figure(figsize=(12,5))\n\n#Plotting the count of title by Survived or not category\nsns.countplot(x='Title', data=df_train, palette=\"hls\",\n              hue=\"Survived\")\nplt.xlabel(\"Titles\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.title(\"Title Grouped Count\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()","93042537":"#First I will look my distribuition without NaN's\n#I will create a df to look distribuition \nage_high_zero_died = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 0)]\nage_high_zero_surv = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 1)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the two\nsns.distplot(age_high_zero_surv[\"Age\"], bins=24, color='g')\nsns.distplot(age_high_zero_died[\"Age\"], bins=24, color='r')\nplt.title(\"Distribuition and density by Age\",fontsize=20)\nplt.xlabel(\"Age\",fontsize=15)\nplt.ylabel(\"Distribuition Died and Survived\",fontsize=15)\nplt.show()","071d510b":"#Let's group the median age by sex, pclass and title, to have any idea and maybe input in Age NAN's\nage_group = df_train.groupby([\"Sex\",\"Pclass\",\"Title\"])[\"Age\"]\n\n#printing the variabe that we created by median\nprint(age_group.median())","910c988a":"#inputing the values on Age Na's \n# using the groupby to transform this variables\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')\n\n# printing the total of nulls in Age Feature\nprint(df_train[\"Age\"].isnull().sum())","ff35785b":"#Let's see the result of the inputation\n\n#seting the figure size\nplt.figure(figsize=(12,5))\n\n#ploting again the Age Distribuition after the transformation in our dataset\nsns.distplot(df_train[\"Age\"], bins=24)\nplt.title(\"Distribuition and density by Age\")\nplt.xlabel(\"Age\")\nplt.show()","e6be3799":"#separate by survivors or not\n\n# figure size\nplt.figure(figsize=(12,5))\n\n# using facetgrid that is a great way to get information of our dataset\ng = sns.FacetGrid(df_train, col='Survived',size=5)\ng = g.map(sns.distplot, \"Age\")\nplt.show()","f97a9267":"#df_train.Age = df_train.Age.fillna(-0.5)\n\n#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 25, 35, 60, 120) \n\n#Seting the names that we want use to the categorys\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval, labels=cats)\n\n# Printing the new Category\ndf_train[\"Age_cat\"].head()","874d187a":"#Do the same to test dataset \ninterval = (0, 5, 12, 18, 25, 35, 60, 120)\n\n#same as the other df train\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# same that we used above in df train\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval, labels=cats)","829f88e6":"#Describe of categorical Age\n\n# Using pd.crosstab to understand the Survived rate by Age Category's\nprint(pd.crosstab(df_train.Age_cat, df_train.Survived))\n\n#Seting the figure size\nplt.figure(figsize=(12,10))\n\n#Plotting the result\nplt.subplot(2,1,1)\nsns.countplot(\"Age_cat\",data=df_train,hue=\"Survived\", palette=\"hls\")\nplt.ylabel(\"Count\", fontsize=18)\nplt.xlabel(\"Age Categorys\", fontsize=18)\nplt.title(\"Age Distribution \", fontsize=20)\n\nplt.subplot(2,1,2)\nsns.swarmplot(x='Age_cat',y=\"Fare\",data=df_train,\n              hue=\"Survived\", palette=\"hls\", )\nplt.ylabel(\"Fare Distribution\", fontsize=18)\nplt.xlabel(\"Age Categorys\", fontsize=18)\nplt.title(\"Fare Distribution by Age Categorys \", fontsize=20)\n\nplt.subplots_adjust(hspace = 0.5, top = 0.9)\n\nplt.show()","617c3edf":"Age_fare = ['Pclass', 'Age_cat'] #seting the desired \n\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df_train[Age_fare[0]], df_train[Age_fare[1]], \n            values=df_train['Fare'], aggfunc=['mean']).style.background_gradient(cmap = cm)\n","ec69074c":"# Seting the figure size\nplt.figure(figsize=(12,5))\n\n# Understanding the Fare Distribuition \nsns.distplot(df_train[df_train.Survived == 0][\"Fare\"], \n             bins=50, color='r')\nsns.distplot(df_train[df_train.Survived == 1][\"Fare\"], \n             bins=50, color='g')\nplt.title(\"Fare Distribuition by Survived\", fontsize=20)\nplt.xlabel(\"Fare\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\nplt.show()","e1267d0b":"#Filling the NA's with -0.5\ndf_train.Fare = df_train.Fare.fillna(-0.5)\n\n#intervals to categorize\nquant = (-1, 0, 8, 15, 31, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\n#doing the cut in fare and puting in a new column\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\n\n#Description of transformation\nprint(pd.crosstab(df_train.Fare_cat, df_train.Survived))\n\nplt.figure(figsize=(12,5))\n\n#Plotting the new feature\nsns.countplot(x=\"Fare_cat\", hue=\"Survived\", data=df_train, palette=\"hls\")\nplt.title(\"Count of survived x Fare expending\",fontsize=20)\nplt.xlabel(\"Fare Cat\",fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\n\nplt.show()","af1a63ad":"# Replicate the same to df_test\ndf_test.Fare = df_test.Fare.fillna(-0.5)\n\nquant = (-1, 0, 8, 15, 31, 1000)\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","8953ed86":"#Now lets drop the variable Fare, Age and ticket that is irrelevant now\ndel df_train[\"Fare\"]\ndel df_train[\"Ticket\"]\ndel df_train[\"Age\"]\ndel df_train[\"Cabin\"]\ndel df_train[\"Name\"]\n\n#same in df_test\ndel df_test[\"Fare\"]\ndel df_test[\"Ticket\"]\ndel df_test[\"Age\"]\ndel df_test[\"Cabin\"]\ndel df_test[\"Name\"]","391d84df":"#Looking the result of transformations\ndf_train.head()","82dad8d4":"# Let see how many people die or survived\nprint(\"Total of Survived or not: \")\nprint(df_train.groupby(\"Survived\")[\"PassengerId\"].count())\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Survived\", data=df_train,palette=\"hls\")\nplt.title('Total Distribuition by survived or not', fontsize=22)\nplt.xlabel('Target Distribuition', fontsize=18)\nplt.ylabel('Count', fontsize=18)\n\nplt.show()","4ad4e3a8":"print(pd.crosstab(df_train.Survived, df_train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()","5b29fc80":"# Distribuition by class\nprint(pd.crosstab(df_train.Pclass, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Pclass\",palette=\"hls\")\nplt.title('Embarked x Pclass Count', fontsize=20)\nplt.xlabel('Embarked with PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()","dc8cbbfa":"#lets input the NA's with the highest frequency\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","6687c4a8":"# Exploring Survivors vs Embarked\nprint(pd.crosstab(df_train.Survived, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=17)\nplt.ylabel('Count', fontsize=17)\n\nplt.show()","4b9a5c97":"# Exploring Survivors vs Pclass\nprint(pd.crosstab(df_train.Survived, df_train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=17)\nplt.ylabel('Count', fontsize=17)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()","4824a546":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,\n                   kind=\"bar\", height = 5, aspect= 1.6, palette = \"hls\")\ng.set_ylabels(\"Probability(Survive)\", fontsize=15)\ng.set_xlabels(\"SibSp Number\", fontsize=15)\n\nplt.show()\n","f1716bbf":"f","ec73b83f":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=df_train, kind=\"bar\", size = 6,palette = \"hls\")\ng = g.set_ylabels(\"survival probability\")","13735b47":"#Create a new column and sum the Parch + SibSp + 1 that refers the people self\ndf_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\n\ndf_test[\"FSize\"] = df_test[\"Parch\"] + df_test[\"SibSp\"] + 1","8cb781a4":"print(pd.crosstab(df_train.FSize, df_train.Survived))\nsns.factorplot(x=\"FSize\",y=\"Survived\", data=df_train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()","f5c2a587":"del df_train[\"SibSp\"]\ndel df_train[\"Parch\"]\n\ndel df_test[\"SibSp\"]\ndel df_test[\"Parch\"]","80bd92d0":"df_train.head()","f98eb9b4":"df_train = pd.get_dummies(df_train, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                          prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)\n\ndf_test = pd.get_dummies(df_test, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                         prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)","16921bd5":"#Finallt, lets look the correlation of df_train\nplt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(df_train.astype(float).corr(),vmax=1.0,  annot=True)\nplt.show()","07030d2d":"df_train.shape","3361b381":"train = df_train.drop([\"Survived\",\"PassengerId\"],axis=1)\ntrain_ = df_train[\"Survived\"]\n\ntest_ = df_test.drop([\"PassengerId\"],axis=1)\n\nX_train = train.values\ny_train = train_.values\n\nX_test = test_.values\nX_test = X_test.astype(np.float64, copy=False)","fdd73973":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","cf9bb832":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz","5974835f":"# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(18, \n                activation='relu',  \n                input_dim=20,\n                kernel_initializer='uniform'))\n#The argument being passed to each Dense layer (18) is the number of hidden units of the layer. \n# A hidden unit is a dimension in the representation space of the layer.\n\n#Stacks of Dense layers with relu activations can solve a wide range of problems\n#(including sentiment classification), and you\u2019ll likely use them frequently.\n\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.50))\n\n#adding second hidden layer \nmodel.add(Dense(60,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#With such a scalar sigmoid output on a binary classification problem, the loss\n#function you should use is binary_crossentropy\n\n#Visualizing the model\nmodel.summary()","510b9607":"#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.01, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, \n               batch_size = 60, \n               epochs = 30, verbose=2)","b3bbd0e1":"y_preds = model.predict(X_test)\n\nsubmission = pd.read_csv(\"..\/input\/gender_submission.csv\", index_col='PassengerId')\nsubmission['Survived'] = y_preds.astype(int)\nsubmission.to_csv('TitanicKNN.csv')","1091ac75":"scores = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","f066451a":"# Fit the model\nhistory = model.fit(X_train, y_train, validation_split=0.20, \n                    epochs=180, batch_size=10, verbose=0)\n\n# list all data in history\nprint(history.history.keys())","ad4578af":"# summarizing historical accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","66ba04bd":"\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","8d408ddc":"y_pred = model.predict(X_test)","48a8cb92":"# Trying to implementing the TensorBoard to evaluate the model\n\ncallbacks = [\n    keras.callbacks.TensorBoard(log_dir='my_log_dir',\n                                histogram_freq=1,\n                                embeddings_freq=1,\n                               )\n]","30efa745":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","6384ead8":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 10\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","0c8e2a61":"Because you\u2019re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it\u2019s best to use the <i>binary_crossentropy<\/i> loss.","9521a965":"## Grouping some titles and ploting the results","0b2ecc2c":"<br>\nDescription of Fare variable<br>\n- Min: 0<br>\n- Median: 14.45<br>\n- Mean: 32.20<br>\n- Max: 512.32<br> \n- Std: 49.69<br>\n\n<h3>I will create a categorical variable to treat the Fare expend<\/h3><br>\nI will use the same technique used in Age but now I will use the quantiles to binning\n\n","ad5edab2":"Now, lets start explore the data","751d77e3":"<b>Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive<\/b>","c97932a4":"Not bad result to a simple model! Let's now verify the validation of our model, to see and understand the learning curve","717b47ea":"<h1>Anatomy of a neural network: <\/h1>\n\nAs you saw in the previous chapters, training a neural network revolves around the following\nobjects:\n- Layers, which are combined into a network (or model)\n- The input data and corresponding targets\n- The loss function, which defines the feedback signal used for learning\n- The optimizer, which determines how learning proceeds\n\n\n\n\n<h2> Layers: the building blocks of deep learning<\/h2>\nfrom keras import layers<br>\nlayer = layers.Dense(32, input_dim=data_dimension)) \n\n- We can think of layers as the LEGO bricks of deep learning, a metaphor that is\nmade explicit by frameworks like Keras. Building deep-learning models in Keras is\ndone by clipping together compatible layers to form useful data-transformation pipelines.\n\n\n<h2>What are activation functions, and why are they necessary?<\/h2>\nWithout an activation function like relu (also called a non-linearity), the Dense layer would consist of two linear operations\u2014a dot product and an addition: <br><br>\n<i>output = dot(W, input) + b<\/i><br><br>\n\nSo the layer could only learn linear transformations (affine transformations) of the\ninput data: the hypothesis space of the layer would be the set of all possible linear\ntransformations of the input data into a 16-dimensional space. \n\n\n<h2>Loss functions and optimizers:<br>\nkeys to configuring the learning process<\/h2>\nOnce the network architecture is defined, you still have to choose two more things:\n- <b>Loss function (objective function) <\/b>- The quantity that will be minimized during\ntraining. It represents a measure of success for the task at hand.\n- <b>Optimizer<\/b> - Determines how the network will be updated based on the loss function.\nIt implements a specific variant of stochastic gradient descent (SGD).","1f0eb46c":"<h2>Competition Description: <\/h2>\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","8e387370":"<h1> Welcome to my Titanic Kernel! <\/h1>\n<h2>This kernel will provide a analysis through the Titanic Disaster to understand the Survivors patterns<\/h2><br>\n\nI will handle with data (<i>transform, missings, manipulation<\/i>), explore the data (<i>descritive and visual<\/i>) and also create a Deep Learning model","855c248d":"So to Finish our exploration I will create a new column to with familiees size","9effab49":"<a id=\"Librarys\"><\/a> <br> \n# **2. Librarys:** ","a396c4a7":"Why this occurs and how to solve this problem in graph? it's a overffiting? ","3c8a9e6b":"<h1>Now, lets do some exploration in Pclass and Embarked to see if might have some information to build the model","b2866bb2":"## Title grouped","9caa0ac8":"<a id=\"Introduction\"><\/a> <br> \n# **1. Introduction:** \n<h3> The data have 891 entries on train dataset and 418 on test dataset<\/h3>\n- 10 columns in train_csv and 9 columns in train_test\n","cb23e798":"<h1>Evaluating the model<\/h1>","1c03acee":"<h3>Data Dictionary<\/h3><br>\nVariable\tDefinition\tKey<br>\n<b>survival<\/b>\tSurvival\t0 = No, 1 = Yes<br>\n<b>pclass<\/b>\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd<br>\n<b>sex<\/b>\tSex\t<br>\n<b>Age<\/b>\tAge in years\t<br>\n<b>sibsp<\/b>\t# of siblings \/ spouses aboard the Titanic\t<br>\n<b>parch<\/b>\t# of parents \/ children aboard the Titanic\t<br>\n<b>ticket<\/b>\tTicket number\t<br>\n<b>fare<\/b>\tPassenger fare\t<br>\n<b>cabin<\/b>\tCabin number\t<br>\n<b>embarked\t<\/b>Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton<br>\n<h3>Variable Notes<\/h3><br>\n<b>pclass: <\/b>A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n<b>age: <\/b>Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n<b>sibsp:<\/b> The dataset defines family relations in this way...<br>\n- <b>Sibling <\/b>= brother, sister, stepbrother, stepsister<br>\n- <b>Spouse <\/b>= husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n\n<b>parch: <\/b>The dataset defines family relations in this way...<br>\n- <b>Parent<\/b> = mother, father<br>\n- <b>Child <\/b>= daughter, son, stepdaughter, stepson<br>\n\nSome children travelled only with a nanny, therefore parch=0 for them.<br>","93f127a6":"<a id=\"Known\"><\/a> <br> \n# **4. Exploring the data:** ","0eccb91c":"Let's look this keys values further","59e0b1e0":"<a id=\"Model\"><\/a> <br> \n# **6. Modelling : ** ","bbc5e63c":"Interesting. With 1 or 2 siblings\/spouses have more chance to survived the disaster","b89be392":"<i>*I'm from Brazil, so english is not my first language, sorry about some mistakes<\/i>","5d6e6323":"This might show us a better way to input the NAN's \n\n<b>For example: <\/b> an male in 2 class that is a Officer the median Age is 42. <br>\nAnd we will use that to complete the missing data\n","62a015d5":"<h2>To try a new approach in the data, I will start the data analysis by the Name column","570bbf05":"<h2>We can look that % dies to mens are much higher than female","a155168a":"<a id=\"Known\"><\/a> <br> \n# **3. First look at the data:** ","13c41f9a":"I am using the beapproachs as possible but if you think I can do anything another best way, please, let me know.","c55eab79":"- Very interesting. We can see that babies has the highest mean value. ","b2bcf435":"<a id=\"Validation\"><\/a> <br> \n# **7. Validation: ** ","61ea8f3f":"# Stay tuned and don't forget to votesup this kernel","7425bca4":"<a id=\"Preprocess\"><\/a> <br> \n# **5. Preprocessing :** ","4cf26040":"OK, its might be enough to start with the preprocess and builting the model\n","7744457e":"Finally, we need to choose a loss function and an optimizer. ","503d6bf7":"<h2>To complete this part, I will now work on \"Names\"","eab582be":"## Let's cross our Pclass with the Age_cat \nWe will aggregate than to get the mean of Fare by each category pair\n","bafd1adb":"## Predicting X_test","02952028":"Now we might have information enough to think about the model structure","b23495e1":"We can see a high standard deviation in the survival with 3 parents\/children person's <br>\nAlso that small families (1~2) have more chance to survival than single or big families","1fcb90b8":"To finish the analysis I let's look the Sibsp and Parch variables","17f870e5":"It's interesting... Children's and ladys first, huh?","81e8ea9b":"<h1>It's looking ok","c7c15115":"<h1>It's my first Deep Learning implementation... I am studying about this and I will continue editing this Kernel to improve the results<\/h1>","acc0e4e9":"### Looking the Fare distribuition to survivors and not survivors\n","f8772e26":"I will start looking the type and informations of the datasets","357e6945":"Are you looking for another interesting Kernels? <a href=\"https:\/\/www.kaggle.com\/kabure\/kernels\">CLICK HERE<\/a> <br>\nGive me your feedback and if yo like this kernel, votes up","f1501a92":"Give me your feedback how can I increase this model =) ","e03cb13a":"Stacks of Dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you\u2019ll likely use them frequently.","68121f6d":"<h1> Now I will handle the Age variable that has a high number of NaN's, using some columns to correctly input he missing Age's","92f94b54":"Now it look's better and clearly","e72eed6c":"# Table of Contents:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Librarys](#Librarys)** <br>\n**3. [Knowning the data](#Known)** <br>\n**4. [Exploring some Variables](#Explorations)** <br>\n**5. [Preprocessing](#Prepocess)** <br>\n**6. [Modelling](#Model)** <br>\n**7. [Validation](#Validation)** <br>\n","1adcb732":"<h3>Titanic survivors prediction: <br>\na binary classification example<\/h3>\nTwo-class classification, or binary classification, may be the most widely applied kind of machine-learning problem.","9b865a12":"Now let's categorize them "}}