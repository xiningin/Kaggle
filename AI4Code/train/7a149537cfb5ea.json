{"cell_type":{"e100b15e":"code","f2c10469":"code","26022865":"code","223af1c9":"code","15d61894":"code","3e779770":"code","e15b0b1e":"code","1cc641d5":"code","e21f4dfc":"code","ceb86091":"code","f88fea22":"code","6e9f6ac9":"code","ba44de09":"code","817530a4":"code","af55b370":"code","19334d20":"code","064678f0":"code","45bef1f1":"code","4e49e8ea":"code","c1774700":"code","93bc669c":"code","bb27d93c":"code","f5252995":"code","59f0df75":"code","20a9963a":"code","5628eb1f":"code","d932e31f":"code","d4647fe8":"code","acaf69f4":"code","15e97014":"code","d89922b7":"code","d8956304":"code","28b95f32":"code","573d5bd9":"code","c3b41042":"code","ba737f99":"code","c699b3c1":"code","b8381dbc":"code","c96fe0a2":"code","244da6dd":"code","c7c259b1":"code","b31b1443":"markdown","2d4344f5":"markdown","d4e49dbe":"markdown","58511d67":"markdown","2e678385":"markdown","9356b320":"markdown","141b3126":"markdown","9a9b9c7b":"markdown","74bbf2ad":"markdown","0167ce85":"markdown","6dcfe0e6":"markdown","35d0061c":"markdown","1334013a":"markdown","ab938cd2":"markdown","bc713e5b":"markdown","2242f3d7":"markdown","7fc8fec8":"markdown","f8bd6186":"markdown","10a4eea7":"markdown","037cd2a0":"markdown","7ec6f08c":"markdown","a971413d":"markdown","f8f84304":"markdown","4e8f4db8":"markdown"},"source":{"e100b15e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n# Deep Learning Libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout\n\n# Misc. Libraries\nfrom imblearn.datasets import fetch_datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f2c10469":"data=pd.read_csv(\"..\/input\/heart.csv\")","26022865":"print('Heart Disease', round(data['target'].value_counts()[1]\/len(data) * 100,2), '% of the target ')\nprint('No heart Disease', round(data['target'].value_counts()[0]\/len(data) * 100,2), '% of the target')","223af1c9":"sns.countplot('target', data=data, palette=\"winter\")\nplt.title('Class Distributions \\n 0: No Disease || 1: Disease', fontsize=14)\n","15d61894":"data.head()","3e779770":"sns.set(style=\"white\", palette=\"PuBuGn_d\", color_codes=True)","e15b0b1e":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nage = data['age'].values\nsex = data['sex'].values\nsns.distplot(age, ax=ax[0], color='purple')\nax[0].set_title('Distribution of age', fontsize=14)\nax[0].set_xlim([min(age), max(age)])\nsns.distplot(sex, ax=ax[1], color='b')\nax[1].set_title('Distribution of sex', fontsize=14)\nax[1].set_xlim([min(sex), max(sex)])\nplt.show()\n","1cc641d5":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\ncp = data['cp'].values\ntrestbps = data['trestbps'].values\nsns.distplot(cp, ax=ax[0], color='green')\nax[0].set_title('Distribution of chest pain', fontsize=14)\nax[0].set_xlim([min(cp), max(cp)])\nsns.distplot(trestbps, ax=ax[1], color='orange')\nax[1].set_title('Distribution of trestbps', fontsize=14)\nax[1].set_xlim([min(trestbps), max(trestbps)])\nplt.show()\n","e21f4dfc":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nchol = data['chol'].values\nfbs = data['fbs'].values\nsns.distplot(chol, ax=ax[0], color='brown')\nax[0].set_title('Distribution of cholestrol', fontsize=14)\nax[0].set_xlim([min(chol), max(chol)])\nsns.distplot(fbs, ax=ax[1], color='blue')\nax[1].set_title('Distribution of fasting blood sugar', fontsize=14)\nax[1].set_xlim([min(fbs), max(fbs)])\nplt.show()\n","ceb86091":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nrestecg = data['restecg'].values\nthalach = data['thalach'].values\nsns.distplot(restecg,ax=ax[0], color='r')\nax[0].set_title('Distribution of ecg resting electrode', fontsize=14)\nax[0].set_xlim([min(restecg), max(restecg)])\nsns.distplot(thalach, ax=ax[1], color='b')\nax[1].set_title('Distribution of thalach', fontsize=14)\nax[1].set_xlim([min(thalach), max(thalach)])\nplt.show()\n","f88fea22":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nexang = data['exang'].values\noldpeak = data['oldpeak'].values\nsns.distplot(exang,ax=ax[0], color='yellow')\nax[0].set_title('Distribution of exang', fontsize=14)\nax[0].set_xlim([min(exang), max(exang)])\nsns.distplot(oldpeak, ax=ax[1], color='b')\nax[1].set_title('Distribution of oldpeak', fontsize=14)\nax[1].set_xlim([min(oldpeak), max(oldpeak)])\nplt.show()\n","6e9f6ac9":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nslope = data['slope'].values\nca = data['ca'].values\nsns.distplot(slope,ax=ax[0], color='red')\nax[0].set_title('Distribution of slope', fontsize=14)\nax[0].set_xlim([min(slope), max(slope)])\nsns.distplot(ca, ax=ax[1], color='green')\nax[1].set_title('Distribution of ca', fontsize=14)\nax[1].set_xlim([min(ca), max(ca)])\nplt.show()\n","ba44de09":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\nthal = data['thal'].values\ntarget = data['target'].values\nsns.distplot(thal,ax=ax[0], color='blue')\nax[0].set_title('Distribution of thal', fontsize=14)\nax[0].set_xlim([min(thal), max(thal)])\nsns.distplot(target, ax=ax[1], color='green')\nax[1].set_title('Distribution of target', fontsize=14)\nax[1].set_xlim([min(target), max(target)])\nplt.show()\n","817530a4":"data.describe()","af55b370":"data.isnull().any()","19334d20":"data.target.value_counts()","064678f0":"data[data.duplicated()==True]","45bef1f1":"data.drop_duplicates(inplace=True)\ndata[data.duplicated()==True]","4e49e8ea":"cmap = sns.diverging_palette(250, 15, s=75, l=40,n=9, center=\"dark\")","c1774700":"data = data.sample(frac=1)\n\n# total heart disease data classes 164 rows.\nnon_hd_data = data.loc[data['target'] == 0]\nhd_data = data.loc[data['target'] == 1][:138]\n\nb_data = pd.concat([non_hd_data,hd_data])\n\n# Shuffle dataframe rows\nb_data = b_data.sample(frac=1, random_state=7)\n\nb_data.head()","93bc669c":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = data.corr()\nsns.heatmap(corr, cmap=cmap, annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = b_data.corr()\nsns.heatmap(sub_sample_corr, cmap=cmap, annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","bb27d93c":"print('Distribution of the Classes in the new balanced dataset')\nprint(b_data['target'].value_counts()\/len(b_data))\nsns.countplot('target', data=b_data, palette='winter')\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","f5252995":"fig, axes = plt.subplots(ncols=6,figsize=(20,4))\nsns.boxplot(x='target',y='age',data=b_data, palette='Oranges', ax=axes[0])\naxes[0].set_title('Age vs Target distribution')\n\nsns.boxplot(x='target',y='sex' ,data=b_data, palette='Oranges', ax=axes[1])\naxes[1].set_title(\"Sex vs Target distribution\")\n\nsns.boxplot(x='target',y='exang' ,data=b_data, palette='Oranges', ax=axes[2])\naxes[2].set_title(\"exang vs Target distribution\")\n\nsns.boxplot(x='target',y='oldpeak' ,data=b_data, palette='Oranges', ax=axes[3])\naxes[3].set_title(\"Oldpeak vs Target distribution\")\n\nsns.boxplot(x='target',y='ca' ,data=b_data, palette='Oranges', ax=axes[4])\naxes[4].set_title(\"CA vs Target distribution\")\n\nsns.boxplot(x='target',y='trestbps',data=b_data, palette='Oranges',ax=axes[5])\naxes[5].set_title(\"trestbps vs Target distribution\")\n","59f0df75":"fig, axes = plt.subplots(ncols=7,figsize=(20,4))\nsns.boxplot(x='target',y='cp',data=b_data, palette='winter', ax=axes[0])\naxes[0].set_title('Chest Pain vs Target distribution')\n\nsns.boxplot(x='target',y='fbs' ,data=b_data, palette='winter', ax=axes[1])\naxes[1].set_title(\"fbs vs Target \")\n\nsns.boxplot(x='target',y='restecg' ,data=b_data, palette='winter', ax=axes[2])\naxes[2].set_title(\"restecg vs Target distribution\")\n\nsns.boxplot(x='target',y='thalach' ,data=b_data, palette='winter', ax=axes[3])\naxes[3].set_title(\"thalach vs Target \")\n\nsns.boxplot(x='target',y='slope' ,data=b_data, palette='winter', ax=axes[4])\naxes[4].set_title(\"slope vs Target distribution\")\n\nsns.boxplot(x='target',y='chol' ,data=b_data, palette='winter', ax=axes[5])\naxes[5].set_title(\"chol vs Target \")\n\nsns.boxplot(x='target',y='thal' ,data=b_data, palette='winter', ax=axes[6])\naxes[6].set_title(\"thal vs Target \")\n","20a9963a":"from scipy.stats import norm\n\nf,(ax1,ax2,ax3,ax4)=plt.subplots(1,4,figsize=(20,4))\nage_d=b_data['age'].loc[b_data['target']==0].values\nsns.distplot(age_d,ax=ax1,fit=norm,color='g')\nax1.set_title(\"Age Distirbution \\n (Non Heart Disease)\", fontsize='14')\n\nsex_d=b_data['sex'].loc[b_data['target']==0].values\nsns.distplot(sex_d,ax=ax2,fit=norm,color='red')\nax2.set_title(\"Sex Distribution \\n (Non Heart Disease)\", fontsize='14')\n\nexang_d=b_data['exang'].loc[b_data['target']==1].values\nsns.distplot(sex_d,ax=ax3,fit=norm,color='blue')\nax3.set_title(\"exang Distribution \\n (Heart Disease)\", fontsize='14')\n\n'''oldpeak_d=b_data['oldpeak'].loc[b_data['target']==1].values\nsns.distplot(oldpeak_d,ax=ax4,fit=norm,color='blue')\nax4.set_title(\"oldpeak Distribution \\n (Heart Disease)\", fontsize='14')'''\n\noldpeak_d=b_data['oldpeak'].values\nsns.distplot(oldpeak_d,ax=ax4,fit=norm,color='blue')\nax4.set_title(\"oldpeak Distribution \\n (Non Heart Disease)\", fontsize='14')\n\n\n","5628eb1f":"f,(ax1,ax2,ax3,ax4,ax5)=plt.subplots(1,5,figsize=(20,4))\nca_d=b_data['ca'].loc[b_data['target']==1].values\nsns.distplot(ca_d,ax=ax1,fit=norm,color='y')\nax1.set_title(\"CA Distirbution \\n (Heart Disease)\", fontsize='14')\n\ntrestbps_d=b_data['trestbps'].loc[b_data['target']==0].values\nsns.distplot(trestbps_d,ax=ax2,fit=norm,color='red')\nax2.set_title(\"trestbps Distribution \\n (Non Heart Disease)\", fontsize='14')\n\ncp_d=b_data['cp'].loc[b_data['target']==1].values\nsns.distplot(cp_d,ax=ax3,fit=norm,color='orange')\nax3.set_title(\"cp Distribution \\n (Heart Disease)\", fontsize='14')\n\nfbs_d=b_data['oldpeak'].values\nsns.distplot(fbs_d,ax=ax4,fit=norm,color='purple')\nax4.set_title(\"fbs Distribution \\n (Heart Disease)\", fontsize='14')\n\nthalach_d=b_data['thalach'].loc[b_data['target']==1].values\nsns.distplot(thalach_d,ax=ax5,fit=norm,color='cyan')\nax5.set_title(\"thalach Distribution \\n (Non Heart Disease)\", fontsize='14')\n\n\n","d932e31f":"X=b_data.iloc[:,:-1]\ny=b_data.iloc[:,-1]","d4647fe8":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=20)\n","acaf69f4":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=100,max_depth=5)\nmodel.fit(X_train, Y_train)","15e97014":"Y_pred=model.predict(X_test)\naccuracy=accuracy_score(Y_test,Y_pred)\naccuracy","d89922b7":"from sklearn.model_selection import GridSearchCV\nparameters = {'n_estimators':[1,500], 'max_depth':[1, 15]}\nclf = GridSearchCV(model, parameters, cv=5)\nclf.fit(X_train,Y_train)\nprint(clf.best_params_)","d8956304":"model = RandomForestClassifier(n_estimators=500,max_depth=1)\nmodel.fit(X_train, Y_train)\nY_pred=model.predict(X_test)\ncm=confusion_matrix(Y_test.tolist(),Y_pred.tolist())\nprint(\"Accuracy:\",accuracy_score(Y_test,Y_pred))\nprint(\"Precision Score :\",precision_score(Y_test,Y_pred))\nprint(\"f1 Score :\",f1_score(Y_test,Y_pred))\nprint(\"Confusion Matrix: \\n\",cm)\n","28b95f32":"total=sum(sum(cm))\nsensitivity = cm[0,0]\/(cm[0,0]+cm[1,0])\nprint('Sensitivity : ', sensitivity )\nspecificity = cm[1,1]\/(cm[1,1]+cm[0,1])\nprint('Specificity : ', specificity)","573d5bd9":"classifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}\n","c3b41042":"from sklearn.model_selection import cross_val_score\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, Y_train)\n    training_score = cross_val_score(classifier, X_train, Y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n","ba737f99":"# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, Y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, Y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, Y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, Y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","c699b3c1":"log_reg_score = cross_val_score(log_reg, X_train, Y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, Y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, Y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, Y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","b8381dbc":"X_train.shape","c96fe0a2":"model=Sequential()\nmodel.add(Dense(128, init=\"uniform\", input_dim=13, activation='relu'))\nmodel.add(Dense(64, init =\"uniform\", activation=\"relu\"))\nmodel.add(Dense(1, init=\"uniform\", activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\", metrics=['accuracy'], optimizer='adam')\nmodel.summary()\nhistory=model.fit(X_train,Y_train, epochs=100, batch_size=100)","244da6dd":"plt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","c7c259b1":"model.evaluate(X_test,Y_test)","b31b1443":"**Checking for null values**","2d4344f5":"<b>There are three broad reasons for computing a correlation matrix.<\/b>\n\n<li>To summarize a large amount of data where the goal is to see patterns. In our example above, the observable pattern is that all the variables highly correlate with each other.<\/li>\n<li>To input into other analyses. For example, people commonly use correlation matrixes as inputs for exploratory factor analysis, confirmatory factor analysis, structural equation models, and linear regression when excluding missing values pairwise. <\/li>\n<li>As a diagnostic when checking other analyses. For example, with linear regression a high amount of correlations suggests that the linear regression\u2019s estimates will be unreliable.<\/li>\n\nHere, we check how each values contribute to the target values","d4e49dbe":"Using Random Forest and then using GridSearch CV to find the best parameter\n","58511d67":"<b>Distribution:<\/b> Checking how the attribute values are distributed and determining their skewness","2e678385":"This new dataset comprises of equal number of heart disease and non heart disease . This, thus, helps the model to idenitfy the pattern which causes the disease.\nWe won't be testing the result on this new datatset but on original dataset.","9356b320":"No null values","141b3126":"<h1> Attributes and what does they resemble <\/h1>\n* **age**: Age of patient\n* **sex**:Sex, 1 for male\n* **cp**:chest pain\n* **trestbps**:resting blood pressure,more than 120 over 80 and less than 140 over 90 (120\/80-140\/90): You have a normal blood pressure reading but it is a little higher than it should be, and you should try to lower it. Make healthy changes to your lifestyle.\n* **chol**:serum cholesterol,shows the amount of triglycerides present. Triglycerides are another lipid that can be measured in the blood. \n* **fbs**:fasting blood sugar larger 120mg\/dl (1 true),less than 100 mg\/dL (5.6 mmol\/L) is normal,100 to 125 mg\/dL (5.6 to 6.9 mmol\/L) is considered prediabetes\n* **rest**:ecg resting electrode.\n* **thalach**:maximum heart rate achieved, maximum heart rate is 220 minus your age.\n* **exang**:exercise induced angina (1 yes),Angina is a type of chest pain caused by reduced blood flow to the heart.Angina is a symptom of coronary artery disease.\n* **oldpeak**: ST depression induced by exercise relative to rest\n* **slope**:slope of peak exercise ST\n* **ca**:number of major vessel\n* **thal**:no explanation provided, but probably thalassemia (3 normal; 6 fixed defect; 7 reversable defect)\n* **result**:(1 anomality)\tnum\tdiagnosis of heart disease (angiographic disease status)","9a9b9c7b":"Loading the dataset","74bbf2ad":"Correleation Matrices","0167ce85":"\nUsing the parameter achieved from the GridSearch to ","6dcfe0e6":"<b> Loading common dependencies <\/b>","35d0061c":"From the correlation matrix, we analyize that , Chest Pain,fbs,restecg, thalach, slope contribute more to the heart disease and hence, are <b>positive correleations <\/b> .\n\nAge, Sex, Exang, oldpeak and ca show <b> negative correleation <\/b>.","1334013a":"<h1 align=\"center\">Introduction<\/h1>In this kernel, i have tried to introduce various machine learning models and their implementation. The sole perpose of this kernel is to correctly identify the presence of any heart disease based on the given input. We proceed by giving a general introduction towards the disease followed by dataset's attributes and what they resemble. As we descend down the kernel , i have used various Machine Learning and Deep Learning approach. \n\n\nNote:- This Kernel is subject to get updated as soon as i find something which can be revelant to the context.\n<b> Please Upvote if you like the Kernel <\/b>","ab938cd2":"<h1> Creating a Normal Distributed Dataset <\/h1>","bc713e5b":"Checking for the existence of any duplicate values.\nThe duplicates should be tackled down safely,otherwise would affect in generalization of the model.There might be a chance if duplicates are not dealt properly, they might show up in the test dataset which are also in the training dataset. ","2242f3d7":"<h1> Getting Insight <h1>","7fc8fec8":">Researchers found that throughout life, men were about twice as likely as women to have a heart attack. That higher risk persisted even after they accounted for traditional risk factors for heart disease, including high cholesterol, high blood pressure, diabetes, body mass index, and physical activity.\nhttps:\/\/www.health.harvard.edu\/heart-health\/throughout-life-heart-attacks-are-twice-as-common-in-men-than-women","f8bd6186":"<b> From the boxplot, we analyize the outliers which have been impacting the correleation matrices, this way we can eliminate the outliers and thus we can improve the accuracy. <\/b>\n","10a4eea7":"<li><b>Going through the dataset, first impression <\/b><\/li>","037cd2a0":"We see that the Heart disease occured 54.3 % of the times in the dataset, whilst the 45.7% were the no heart disease. So, we need to balance the dataset otherwise it might get overfit. This will help the model to find pattern in the dataset which contributes to the heart disease and which doesn't.","7ec6f08c":"<h1>About Heart Disease<\/h1>Heart disease is a general term that means that the heart is not working normally. Babies can be born with heart disease. This is called congenital heart disease. If people get heart disease later, it is called acquired heart disease. Most heart disease is acquired.\nThe three most common types of acquired heart disease are:\n\n<li><b>Coronary Artery Disease (acronym CAD)<\/b><\/li> \n<li><b>Congestive Heart Failure (CHF)<\/b><\/li>\n<li><b>Bad Heart Rhythms<\/b><\/li>\n\n<li><h3>DEATHS<\/h3><\/li>\n<p>Heart disease is the biggest killer of both men and women in the United States, England, Wales, and Canada. For example, heart disease causes 4 out of every 10 deaths in the United States.This is more than all kinds of cancer put together. Also, one person dies of heart disease about every minute in the United States alone.<\/p>","a971413d":"**The Model**","f8f84304":"<h1> Using Grid Search CV to find the best parameters ","4e8f4db8":"Counting the total target values in each class"}}