{"cell_type":{"1bc4ddb5":"code","8e7a3021":"code","a526a9be":"code","2e318e02":"code","b162e009":"code","a96d09f0":"code","9f5a9bc2":"code","6878bbfb":"code","3c856afb":"code","f9a1b093":"code","02069720":"code","b40b0555":"code","e6a878fa":"code","7c69f337":"code","a6e733cf":"code","93284a6e":"code","54cd0ed6":"code","f12881d6":"code","ca25d3d4":"code","6aec72d3":"code","a3c893c9":"code","c01fcdcf":"code","c023de41":"code","dcfdbbee":"code","c8f7580e":"code","f40d0c50":"code","1154caeb":"code","1579880f":"code","052c5844":"code","e060b58e":"code","6c4ff26e":"code","0fe562e3":"code","3dad194f":"code","bf84b998":"code","d3b04c1d":"code","90fa6d8a":"code","2640ee25":"code","173353fc":"code","8f5ef3b1":"markdown","436e0815":"markdown","d56e1e2c":"markdown","e0101d27":"markdown"},"source":{"1bc4ddb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e7a3021":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nstyle.use('seaborn-colorblind')\n","a526a9be":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsub=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n\ndataset =  pd.concat(objs=[train,test], axis=0,sort=False).reset_index(drop=True)","2e318e02":"dataset.head()","b162e009":"#exploring the data\ndataset.columns","a96d09f0":"print(dataset.shape)","9f5a9bc2":"dataset.describe()\n#So we get the range,mean,std of all the numericale columns.","6878bbfb":"dataset.tail()","3c856afb":"#Analyse Sales price\ndataset['SalePrice'].describe()","f9a1b093":"#fig=plt.figure(figsize=(18,12))\n#grid = GridSpec(ncols=2, nrows=1, figure=fig)\nf, axes = plt.subplots(1, 2, figsize=(25, 15), sharex=True)\nsns.despine(left=True)\n#fig,axs= plt.subplot(2,1,1)\nsns.distplot(dataset['SalePrice'],ax=axes[0])\nsns.boxplot(dataset['SalePrice'],orient='h',ax=axes[1])\nplt.tight_layout()","02069720":"print(dataset['SalePrice'].skew())\nprint(dataset['SalePrice'].kurt())","b40b0555":"dataset[\"SalePrice\"] = np.log1p(dataset[\"SalePrice\"])\ndataset[\"SalePrice\"] ","e6a878fa":"print(dataset['SalePrice'].skew())\nprint(dataset['SalePrice'].kurt())","7c69f337":"f, axes = plt.subplots(1, 2, figsize=(25, 15), sharex=True)\nsns.despine(left=True)\nsns.distplot(dataset['SalePrice'],ax=axes[0])\nsns.boxplot(dataset['SalePrice'],orient='h',ax=axes[1])\nplt.tight_layout()","a6e733cf":"#Realtionship b\/w some columns and saleprice\ncols= [\"LotArea\",\"Street\",\"LandSlope\",\"GrLivArea\", \"TotalBsmtSF\",'LotFrontage']\nn_rows= 2\nn_col= 3\nfig,axs= plt.subplots(2,3,figsize=(20,16))\n\nfor r in range(0,n_rows):\n    for c in range(0,n_col):\n        i= r*n_col + c\n        ax=axs[r][c]\n        sns.scatterplot(dataset[cols[i]], dataset['SalePrice'],ax=ax)","93284a6e":"#CHaecking Null values\ndataset.isnull().sum()","54cd0ed6":"plt.figure(figsize=(16,10))\nsns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.tight_layout()","f12881d6":"cat=dataset.select_dtypes(\"object\")\nfor column in cat:\n    dataset[column].fillna(dataset[column].mode()[0], inplace=True)\n    #dataset[column].fillna(\"NA\", inplace=True)\n\n\nfl=dataset.select_dtypes([\"float64\",\"int64\"]).drop(\"SalePrice\",axis=1)\nfor column in fl:\n    dataset[column].fillna(dataset[column].median(), inplace=True)\n    #dataset[column].fillna(0, inplace=True)","ca25d3d4":"#checking through heatmap\nplt.figure(figsize=(16,10))\nsns.heatmap(dataset.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.tight_layout()","6aec72d3":"#Auto Detect Outliers\n\ntrain_o=dataset[dataset[\"SalePrice\"].notnull()]\noutliers = [30, 88, 462, 523, 632, 1298, 1324]\nfrom sklearn.neighbors import LocalOutlierFactor\ndef detect_outliers(x, y, top=5, plot=True):\n    lof = LocalOutlierFactor(n_neighbors=40, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n\nouts = detect_outliers(train_o['GrLivArea'], train_o['SalePrice'],top=5) #got 1298,523\nouts\nplt.show()","a3c893c9":"#Detect and Remove outliers\n\nfrom collections import Counter\nall_outliers=[]\nnumeric_features = train_o.dtypes[train_o.dtypes != 'object'].index\nfor feature in numeric_features:\n    try:\n        outs = detect_outliers(train_o[feature], train_o['SalePrice'],top=5, plot=False)\n    except:\n        continue\n    all_outliers.extend(outs)\n\nprint(Counter(all_outliers).most_common())\nfor i in outliers:\n    if i in all_outliers:\n        print(i)\ntrain_o = train_o.drop(train_o.index[outliers])\ntest_o=dataset[dataset[\"SalePrice\"].isna()]\ndataset =  pd.concat(objs=[train_o, test_o], axis=0,sort=False).reset_index(drop=True)","c01fcdcf":"dataset.isnull().sum()","c023de41":"train_heat=dataset[dataset[\"SalePrice\"].notnull()]\ntrain_heat=train_heat.drop([\"Id\"],axis=1)\n#style.use('ggplot')\n#sns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_heat.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(train_heat.corr(), \n            cmap=sns.diverging_palette(2, 132, l=60, n=6),#colours \n            mask=mask,\n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","dcfdbbee":"train_heat.corr().abs()","c8f7580e":"feature_corr = train_heat.corr().abs()\ntarget_corr=dataset.corr()[\"SalePrice\"].abs()\ntarget_corr=pd.DataFrame(target_corr)\ntarget_corr=target_corr.reset_index()\nfeature_corr_unstack= feature_corr.unstack()\ndf_fc=pd.DataFrame(feature_corr_unstack,columns=[\"corr\"])\ndf_fc=df_fc[(df_fc[\"corr\"]>=.80)&(df_fc[\"corr\"]<1)].sort_values(by=\"corr\",ascending=False)\ndf_dc=df_fc.reset_index()\n\ntarget_corr=df_dc.merge(target_corr, left_on='level_1', right_on='index',\n          suffixes=('_left', '_right'))","f40d0c50":"target_corr","1154caeb":"#dataset=dataset.drop([\"GarageArea\",\"TotRmsAbvGrd\",\"TotalBsmtSF\",\"PoolArea\",\"hasgarage\",\"has2ndfloor\",\"YearBuilt\",\"Fireplaces\"],axis=1)\ndataset=dataset.drop([\"GarageArea\",\"TotRmsAbvGrd\"],axis=1)","1579880f":"dataset=pd.get_dummies(dataset,columns=cat.columns)","052c5844":"train=dataset[dataset[\"SalePrice\"].notnull()]\ntest=dataset[dataset[\"SalePrice\"].isna()]","e060b58e":"n_f= dataset.shape[1]\nk = n_f # if you change it 10 model uses most 10 correlated features\ncorrmat=abs(dataset.corr())\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ntrain_x=train[cols].drop(\"SalePrice\",axis=1)\ntrain_y=train[\"SalePrice\"]\nX_test=test[cols].drop(\"SalePrice\",axis=1)","6c4ff26e":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=0.20, random_state=r_s)","0fe562e3":"# from sklearn.linear_model import LinearRegression\n# from sklearn.metrics import mean_squared_error,mean_absolute_error\n# from sklearn.metrics import accuracy_score\n\n\n\n\n# reg = LinearRegression().fit(x_train, y_train)\n# y_pred=reg.predict(x_test)\n\n\n\n# acc_logreg = accuracy_score(y_pred, y_test)\n# #print(y_pred)\n# #reg.score(x_train, y_train)\n# #print(mean_absolute_error(y_test,y_pred))\n# #mean_squared_error(y_test,y_pred))\n# #np.sqrt(mean_squared_error(y_test,y_pred)))","3dad194f":"r_s=42  \nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Ridge,RidgeCV,BayesianRidge,LinearRegression,Lasso,LassoCV,ElasticNet,RANSACRegressor,HuberRegressor,PassiveAggressiveRegressor,ElasticNetCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.neural_network import MLPRegressor\n\n\n\nmy_regressors=[ \n               ElasticNet(alpha=0.01),\n               ElasticNetCV(),\n               CatBoostRegressor(logging_level='Silent',random_state=r_s),\n               GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber',random_state =r_s),\n               LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       random_state=r_s\n                                       ),\n               RandomForestRegressor(random_state=r_s),\n               AdaBoostRegressor(random_state=r_s),\n               ExtraTreesRegressor(random_state=r_s),\n               SVR(C= 20, epsilon= 0.008, gamma=0.0003),\n               Ridge(alpha=13),\n               RidgeCV(),\n               BayesianRidge(),\n               DecisionTreeRegressor(),\n               LinearRegression(),\n               KNeighborsRegressor(),\n               Lasso(alpha=0.00047,random_state=r_s),\n               LassoCV(),\n               KernelRidge(),\n               CCA(),\n               MLPRegressor(random_state=r_s),\n               HuberRegressor(),\n               RANSACRegressor(random_state=r_s),\n               PassiveAggressiveRegressor(random_state=r_s)\n               #XGBRegressor(random_state=r_s)\n              ]\n\nregressors=[]\n\nfor my_regressor in my_regressors:\n    regressors.append(my_regressor)\n\n\nscores_val=[]\nscores_train=[]\nMAE=[]\nMSE=[]\nRMSE=[]\n\n\nfor regressor in regressors:\n    scores_val.append(regressor.fit(X_train,y_train).score(X_val,y_val))\n    scores_train.append(regressor.fit(X_train,y_train).score(X_train,y_train))\n    y_pred=regressor.predict(X_val)\n    MAE.append(mean_absolute_error(y_val,y_pred))\n    MSE.append(mean_squared_error(y_val,y_pred))\n    RMSE.append(np.sqrt(mean_squared_error(y_val,y_pred)))\n\n    \nresults=zip(scores_val,scores_train,MAE,MSE,RMSE)\nresults=list(results)\nresults_score_val=[item[0] for item in results]\nresults_score_train=[item[1] for item in results]\nresults_MAE=[item[2] for item in results]\nresults_MSE=[item[3] for item in results]\nresults_RMSE=[item[4] for item in results]\n\n\ndf_results=pd.DataFrame({\"Algorithms\":my_regressors,\"Training Score\":results_score_train,\"Validation Score\":results_score_val,\"MAE\":results_MAE,\"MSE\":results_MSE,\"RMSE\":results_RMSE})\ndf_results","bf84b998":"best_models=df_results.sort_values(by=\"RMSE\")\nbest_model=best_models.iloc[0][0]\nbest_stack=best_models[\"Algorithms\"].values\nbest_models","d3b04c1d":"best_model.fit(X_train,y_train)\ny_test=best_model.predict(X_test)\ntest_Id=test['Id']\nmy_submission = pd.DataFrame({'Id': test_Id, 'SalePrice': np.expm1(y_test)})\nmy_submission.to_csv('submission_bm.csv', index=False)\n# print(\"Model Name: \"+str(best_model))\n# print(best_model.score(X_val,y_val))\n# y_pred=best_model.predict(X_val)\n# print(\"RMSE: \"+str(np.sqrt(mean_squared_error(y_val,y_pred))))","90fa6d8a":"my_submission.to_csv('submission1.csv', index=False)","2640ee25":"sub=pd.read_csv('.\/submission_bm.csv')","173353fc":"sub","8f5ef3b1":"We have skewed target so we need to transofmation. I ll use log.","436e0815":"Create regression models and compare the accuracy to our best regressor","d56e1e2c":"Above visual shows that\n\nThere is positive skewness The plot is not normally distributed","e0101d27":"Fill null values with Mode\/Median (for categorical features -Mode and for numbers-Median)"}}