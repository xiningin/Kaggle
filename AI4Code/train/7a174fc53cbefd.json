{"cell_type":{"6903bfb9":"code","ad13827a":"code","8b36f6f3":"code","a66b3791":"code","855fcdc7":"code","a92c5544":"code","657dd98c":"code","50a92bc5":"code","95141ff1":"code","06ad49f1":"code","5bba6897":"code","3e6b3de1":"code","e311e258":"code","a6830ab8":"code","4b2a1c56":"code","450d42fd":"code","de1a8951":"code","aa9c592a":"code","79907e4e":"code","ca33e003":"code","716df348":"code","d52a8b18":"code","3eb08446":"code","f17a01f1":"code","da099a5c":"code","d04b46bc":"code","7038af51":"code","07159b99":"code","20c489aa":"code","27302fbe":"code","939d6c48":"code","fa8ba8e1":"code","8786f58c":"code","48943d5a":"code","a501f305":"code","b16a22ac":"code","36ebe173":"code","fde75672":"code","27cab8fb":"code","69a1225a":"code","0c93ac66":"code","38481e92":"code","bae2f216":"code","aa56b141":"code","cbe5ba2b":"code","ff029fff":"code","59f74718":"code","0a8b7dfa":"markdown","25b753c1":"markdown","98d45499":"markdown","3d375406":"markdown","bb22b43d":"markdown","41f72479":"markdown","2d39f387":"markdown","b4605a6c":"markdown","eff0e1ff":"markdown","d045ed37":"markdown","5834db75":"markdown","f30f1654":"markdown","b4a9663f":"markdown","5eb88011":"markdown","1739085c":"markdown","e38e1bad":"markdown","b16ccfe3":"markdown","a7b15a13":"markdown","14754fa9":"markdown","f21b41b8":"markdown","cbb904b8":"markdown","e0378e3a":"markdown","be0a539b":"markdown"},"source":{"6903bfb9":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom ast import literal_eval\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nfrom surprise import Reader, Dataset, accuracy\nfrom surprise import SVD, KNNBasic \nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\n\nimport warnings; warnings.simplefilter('ignore')\n%matplotlib inline","ad13827a":"md = pd. read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv')\nlinks_small = pd.read_csv('..\/input\/the-movies-dataset\/links_small.csv')\ncredits = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\nkeywords = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')\nratings = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')","8b36f6f3":"features = ['id', 'imdb_id', 'title', \n            'genres', 'overview', 'tagline', 'release_date', \n            'popularity', 'vote_average', 'vote_count']\nmd = md[features]\nmd[:1]","a66b3791":"features = ['id', 'cast', 'crew']\ncredits = credits[features]\ncredits[:1]","855fcdc7":"keywords[:1]","a92c5544":"features = ['tmdbId', 'movieId', 'imdbId']\nlinks_small = links_small[features]\nlinks_small.columns = ['id', 'movieId', 'imdbId']\nlinks_small[:1]","657dd98c":"ratings[:1]","50a92bc5":"def clean_id(x):\n    try:\n        x = int(x)\n    except:\n        x = np.NaN\n    return x","95141ff1":"md['id'] = md['id'].apply(clean_id)\nmd.dropna(subset=['id'], inplace=True)","06ad49f1":"df = pd.merge(md, credits, how='inner', on='id')\ndf = pd.merge(df, keywords, how='inner', on='id')","5bba6897":"df[:1]","3e6b3de1":"df['tagline'] = df['tagline'].fillna('')\ndf['overview'] = df['overview'].fillna('')\ndf['description'] = df['overview'] + df['tagline']","e311e258":"df['genres'] = df['genres'].fillna('[]')\ndf['genres'] = df['genres'].apply(literal_eval)\ndf['genres'] = df['genres'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])","a6830ab8":"def genres_list(df_genres):\n    genres = set()\n        \n    for genres_list in df_genres:\n        try:\n            genres.update(genres_list)\n        except AttributeError:\n            pass\n    \n    return genres\n    \ngenres = genres_list(df['genres'])\nprint(genres)","4b2a1c56":"def split_genres(val):\n    try:\n        if gene in val:\n            return 1\n        else:\n            return 0\n    except AttributeError:\n        return 0\n\n# Apply function for each genre\nfor gene in genres:        \n    df[gene] = df['genres'].apply(split_genres)","450d42fd":"def clean_year(x):\n    if x != np.nan:\n        year = str(x).split('-')[0]\n        return year\n    else:\n        return np.NaN\n    \ndf.dropna(subset=['release_date'], inplace=True)\ndf['year'] = df['release_date'].apply(clean_year)\ndf = df.drop(['release_date'], axis=1)","de1a8951":"df.dropna(subset=['year'], inplace=True)\ndf['year'] = df['year'].astype(int)","aa9c592a":"def add_movie_year_period(movie_year):\n    if movie_year < 1900:\n        return '1800'\n    elif movie_year < 2000:\n        return '1900'\n    elif movie_year < 2010:\n        return '2000'\n    else :\n        return '2010'","79907e4e":"df['year_period'] = df['year'].apply(add_movie_year_period)","ca33e003":"df['cast'] = df['cast'].apply(literal_eval)\ndf['crew'] = df['crew'].apply(literal_eval)\ndf['keywords'] = df['keywords'].apply(literal_eval)","716df348":"def get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.NaN","d52a8b18":"df['director'] = df['crew'].apply(get_director)\ndf['director'] = df['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\ndf['director'] = df['director'].apply(lambda x: [x, x, x])\n\ndf['cast'] = df['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\ndf['cast'] = df['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)\ndf['cast'] = df['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])","3eb08446":"df['keywords'] = df['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])","f17a01f1":"dictionary = {}\n\ndef count_words(word_list):\n    for word in word_list:\n        if dictionary.get(word) == None:\n            dictionary[word] = 1\n        else:\n            dictionary[word] += 1\n\n\ndf['keywords'].apply(count_words)\n\n\ndictionary_copy = dictionary.copy()\nfor key, value in dictionary_copy.items():\n    if value == 1:\n        dictionary.pop(key)","da099a5c":"def filter_keywords(word_list):\n    words = []\n    for word in word_list:\n        if dictionary.get(word):\n            words.append(word)\n    return words","d04b46bc":"stemmer = SnowballStemmer('english')\nstemmer.stem('Dogs')","7038af51":"df['keywords'] = df['keywords'].apply(filter_keywords)\ndf['keywords'] = df['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\ndf['keywords'] = df['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])","07159b99":"def weighted_rating(x, C, m):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)\n\ndef build_chart(genre, percentile=0.85):\n    df_genre = df[df[genre] == 1]\n    \n    vote_counts = df_genre[df_genre['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages = df_genre[df_genre['vote_average'].notnull()]['vote_average'].astype('int')\n    C = vote_averages.mean()\n    m = vote_counts.quantile(percentile)\n    \n    features = ['title', 'year', 'vote_count', 'vote_average', 'popularity']\n    qualified = df_genre[(df_genre['vote_count'] >= m) & (df_genre['vote_count'].notnull()) & (df_genre['vote_average'].notnull())][features]\n    \n    # qualified['wr'] = qualified.apply(lambda x: (x['vote_count']\/(x['vote_count']+m) * x['vote_average']) + (m\/(m+x['vote_count']) * C), axis=1)\n    qualified['wr'] = qualified.apply(lambda x: weighted_rating(x, C, m), axis=1)\n    qualified = qualified.sort_values('wr', ascending=False)[:50]\n    \n    return qualified","20c489aa":"build_chart('Romance').head(5)","27302fbe":"df = pd.merge(df, links_small, how='inner', on='id')\n\nfeatures = ['id', 'movieId','imdb_id', 'title', 'genres', 'overview', 'tagline', 'description','popularity',\n       'vote_average', 'vote_count', 'year', 'year_period', 'cast', 'crew','director', 'keywords',\n       'History', 'Horror', 'Foreign', 'Action', 'Documentary',\n       'Science Fiction', 'Music', 'Romance', 'Animation', 'TV Movie',\n       'Fantasy', 'War', 'Comedy', 'Family', 'Drama', 'Western', 'Crime',\n       'Adventure', 'Mystery', 'Thriller']\ndf = df[features]\ndf[:1]","939d6c48":"tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ntfidf_matrix = tf.fit_transform(df['description'])\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n\ndf = df.reset_index()\ndf[:1]","fa8ba8e1":"indices_to_titles = df['title'] # titles\ntitle_to_index = pd.Series(data=df.index, index=df['title']) # indices","8786f58c":"def get_recommendations(title):\n    idx = title_to_index[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    # return df.iloc[movie_indices]\n    return indices_to_titles.iloc[movie_indices] ","48943d5a":"get_recommendations('The Dark Knight').head(5)","a501f305":"df['soup'] = df['cast'] + df['director'] + df['keywords'] + df['genres']\ndf['soup'] = df['soup'].apply(lambda x: ' '.join(x))\ndf['soup'] = df['soup'] + ' ' + df['year_period']\ndf['soup'][:1]","b16a22ac":"count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(df['soup'])\n\ncosine_sim = cosine_similarity(count_matrix, count_matrix)","36ebe173":"get_recommendations('The Dark Knight').head(5)","fde75672":"def improved_recommendations(title):\n    idx = title_to_index[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = df.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]\n    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n    C = vote_averages.mean()\n    m = vote_counts.quantile(0.60)\n    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n    qualified['vote_count'] = qualified['vote_count'].astype('int')\n    qualified['vote_average'] = qualified['vote_average'].astype('int')\n\n    # qualified['wr'] = qualified.apply(lambda x: (x['vote_count']\/(x['vote_count']+m) * x['vote_average']) + (m\/(m+x['vote_count']) * C), axis=1)\n    qualified['wr'] = qualified.apply(lambda x: weighted_rating(x, C, m), axis=1)    \n    qualified = qualified.sort_values('wr', ascending=False).head(10)\n    return qualified","27cab8fb":"improved_recommendations('The Dark Knight').head(5)","69a1225a":"reader = Reader()\ndata = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)","0c93ac66":"#knn = KNNBasic()\nsvd = SVD()\n\n# Run 5-fold cross-validation and then print results\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)","38481e92":"# test set is made of 25% of the ratings.\ntrainset, testset = train_test_split(data, test_size=.25)","bae2f216":"svd.fit(trainset)\n\npredictions = svd.test(testset)\n\n# Then compute RMSE\naccuracy.rmse(predictions)","aa56b141":"user_id = 514\nmovie_id = 505\nreal_rating = 3\nsvd.predict(user_id, movie_id, real_rating, verbose=True)","cbe5ba2b":"indices_map = df.set_index('id')\nindices_map.head(1)","ff029fff":"def hybrid(userId, title):\n    idx = title_to_index[title]\n    \n    sim_scores = list(enumerate(cosine_sim[int(idx)]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = df.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year', 'id']]\n    movies['est'] = movies['id'].apply(lambda x: svd.predict(userId, indices_map.loc[x]['movieId']).est)\n    movies = movies.sort_values('est', ascending=False)\n    return movies.head(10)","59f74718":"hybrid(1, 'Avatar')","0a8b7dfa":"#### Clean id","25b753c1":"### **Single Value Decomposition**\nOne way to handle the scalability and sparsity issue created by CF is to leverage a **latent factor model** to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as how good we are in predicting the rating for items given a user. One common metric is Root Mean Square Error (RMSE). **The lower the RMSE, the better the performance**.\n\nNow talking about latent factor you might be wondering what is it ?It is a broad idea which describes a property or concept that a user or an item have. For instance, for music, latent factor can refer to the genre that the music belongs to. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable. The below figure illustrates this idea.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*GUw90kG2ltTd2k_iv3Vo0Q.png)","98d45499":"# 3. DATA EXPLORATION","3d375406":"#### Clean cast, crew, keywords","bb22b43d":"# INTRODUCTION\n\n<a href=\"https:\/\/ibb.co\/3RKmCvX\"><img src=\"https:\/\/i.ibb.co\/Vw6Yqxy\/netflix.jpg\" alt=\"netflix\" border=\"0\"><\/a>\n\nThe rapid growth of data collection has led to a new era of information. Data is being used to create more efficient systems and this is where Recommendation Systems come into play. Recommendation Systems are a type of information filtering systems as they improve the quality of search results and provides items that are more relevant to the search item or are realted to the search history of the user.\nThey are used to predict the rating or preference that a user would give to an item. Amazon uses it to suggest products to customers, Facebook uses it to recommend pages to like and people to follow. Moreover YouTube, Netflix, Spotify use it to decide which video to play next on autoplay.\n\n\n### Let's explore the overview of the recommender systems\nThere are basically three types of recommender systems:\n\n> *  **Demographic Filtering**- They offer generalized recommendations to every user, based on movie popularity and\/or genre. The System recommends the same movies to users with similar demographic features. Since each user is different , this approach is considered to be too simple. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience.\n\n> *  **Content Based Filtering**- They suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it.\n\n> *  **Collaborative Filtering**- This system matches persons with similar interests and provides recommendations based on this matching. Collaborative filters do not require item metadata like its content-based counterparts.","41f72479":"## 5.1 Demographic Recommender \/ Simple Recommender\n\n   Before getting started with this  -\n* we need a metric to score or rate movie \n* Calculate the score for every movie \n* Sort the scores and recommend the best rated movie to the users.\n\nWe can use the average ratings of the movie as the score but using this won't be fair enough since a movie with 8.9 average rating and only 3 votes cannot be considered better than the movie with 7.8 as as average rating but 40 votes.\nSo, I'll be using IMDB's weighted rating (wr) which is given as :-\n\n![](https:\/\/image.ibb.co\/jYWZp9\/wr.png)\nwhere,\n* v is the number of votes for the movie;\n* m is the minimum votes required to be listed in the chart;\n* R is the average rating of the movie; And\n* C is the mean vote across the whole report\n\nWe already have v(**vote_count**) and R (**vote_average**) and C can be calculated as ","2d39f387":"# 5. RECOMMENDER SYSTEM TECHNIQUES","b4605a6c":"#### Clean year","eff0e1ff":"### Content Based Rec - Plot Description Recs - Cleaning","d045ed37":"## 5.4 Collaborative Filtering\n\nOur content based engine suffers from some severe limitations. It is only capable of suggesting movies which are close to a certain movie. That is, it is not capable of capturing tastes and providing recommendations across genres.\n\nAlso, the engine that we built is not really personal in that it doesn't capture the personal tastes and biases of a user. Anyone querying our engine for recommendations based on a movie will receive the same recommendations for that movie, regardless of who she\/he is.\n\nTherefore, in this section, we will use a technique called Collaborative Filtering to make recommendations to Movie Watchers.\nIt is basically of two types:-\n\n*  **User based filtering**-  These systems recommend products to a user that similar users have liked. For measuring the similarity between two users we can either use pearson correlation or cosine similarity.\nThis filtering technique can be illustrated with an example. In the following matrixes, each row represents a user, while the columns correspond to different movies except the last one which records the similarity between that user and the target user. Each cell represents the rating that the user gives to that movie. Assume user E is the target.\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*9NBFo4AUQABKfoUOpE3F8Q.png)\n\nIt suffers from several problems. One main issue is that users\u2019 preference can change over time. It indicates that precomputing the matrix based on their neighboring users may lead to bad performance. To tackle this problem, we can apply item-based CF.\n\n* **Item Based Collaborative Filtering** - Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated. Likewise, the similarity can be computed with Pearson Correlation or Cosine Similarity. The major difference is that, with item-based collaborative filtering, we fill in the blank vertically, as oppose to the horizontal manner that user-based CF does. The following table shows how to do so for the movie Me Before You.\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*LqFnWb-cm92HoMYBL840Ew.png)\n\n\nHowever, several problems remain for this method. First, the main issue is ***scalability***. The computation grows with both the customer and the product. The worst case complexity is O(mn) with m users and n items. In addition, ***sparsity*** is another concern. Take a look at the above table again. Although there is only one user that rated both Matrix and Titanic rated, the similarity between them is 1. In extreme cases, we can have millions of users and the similarity between two fairly different movies could be very high simply because they have similar rank for the only user who ranked them both.","5834db75":"## 5.5 Hybrid Recommender: Metadata Based and Collaborative Filtering\n\nIn this section, I will try to build a simple hybrid recommender that brings together techniques we have implemented in the content based and collaborative filter based engines. This is how it will work:\n- Input: User ID and the Title of a Movie\n- Output: Similar movies sorted on the basis of expected ratings by that particular user.","f30f1654":"## 5.3 Simple Recommender and Metadata Based Recommender","b4a9663f":"We are now in a good position to define our recommendation function. These are the following steps we'll follow :-\n* Get the index of the movie given its title.\n* Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n* Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n* Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n* Return the titles corresponding to the indices of the top elements.","5eb88011":"# CREDITS\n\nThis work is inspired by multiple great sources done before:\n- https:\/\/www.kaggle.com\/rounakbanik\/movie-recommender-systems\n- https:\/\/www.kaggle.com\/ibtesama\/getting-started-with-a-movie-recommendation-system\n- https:\/\/www.udacity.com\/course\/data-scientist-nanodegree--nd025","1739085c":"#### Clean genres","e38e1bad":"# MOVIE RECOMMENDATION\n\n## Author: Max Duong\n\n### Date: April, 2021","b16ccfe3":"# 1. LIBRARY","a7b15a13":"### Content Based Rec - Metadata Recs - Cleaning\nThe next steps are the same as what we did with our plot description based recommender. One important difference is that we use the CountVectorizer() instead of TF-IDF. This is because we do not want to down-weight the presence of an actor\/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense.","14754fa9":"# 4. DATA MANIPULATION","f21b41b8":"## 5.2 Content Based: \nIn this recommender system the content of the movie (overview, cast, crew, keyword, tagline etc) is used to find its similarity with other movies. Then the movies that are most likely to be similar are recommended.\n\n\n### 5.2.1 Movie Description Based Recommender\n\n![](https:\/\/image.ibb.co\/f6mDXU\/conten.png)\n\nWe will compute pairwise similarity scores for all movies based on their plot descriptions and recommend movies based on that similarity score.\n\n\n**TF * IDF** is used to indicate the overall importance of each word to the documents. This will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each row represents a movie, as before.This is done to reduce the importance of words that occur frequently in plot overviews and therefore, their significance in computing the final similarity score.\n\nWe will be using the cosine similarity to calculate a numeric quantity that denotes the similarity between two movies. We use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate. Mathematically, it is defined as follows:\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/nB1x1MG\/4.png\" alt=\"4\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/imgbb.com\/'>image upload<\/a><br \/>","cbb904b8":"# 2. DATA EXTRACTION","e0378e3a":"For user_id of 514 rating movie_id of 505, we get an estimated prediction of 3.11 where the true rating is 3. One startling feature of this recommender system is that it doesn't care what the movie is (or what it contains). It works purely on the basis of an assigned movie ID and tries to predict ratings based on how the other users have predicted the movie.","be0a539b":"### 5.2.2 Metadata Based Recommender\n- It goes without saying that the quality of our recommender would be increased with the usage of better metadata. We are going to build a recommender based on the following metadata: the actors, the director, related genres and the movie plot keywords, year.\n- The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that our vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same.\n- Creating our \"metadata soup\", which is a string that contains all the metadata that we want to feed to our vectorizer (namely actors, director and keywords)."}}