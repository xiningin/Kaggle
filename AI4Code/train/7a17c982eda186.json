{"cell_type":{"24dedfb6":"code","d5e69bb4":"code","1616dcc1":"code","5e3c56b9":"code","18901ec7":"code","906c1b52":"code","f18a4ad6":"code","aaa5d53e":"code","af92bfd5":"code","0debc26a":"code","1d2c2fac":"code","b729eeac":"code","86a9aadc":"code","bed5c8a0":"code","1e80f20c":"code","d5e0ad0e":"code","32514970":"code","b98f5d47":"code","c39e8d99":"code","3f60eb87":"code","8071c44d":"code","fabc066b":"code","22353c0a":"code","8c20f1aa":"code","f8ddafcb":"code","e4145951":"code","812065c1":"code","50dc4100":"code","4263597c":"code","73daa7cb":"code","cccbbfea":"code","f39c5c3f":"code","e9fc9a60":"code","fc51efce":"code","a45c868b":"code","0fa6eb58":"code","3bd7209f":"code","b9166c62":"code","69e3e6f4":"code","81eb706f":"code","b4b89fd9":"code","c74a58b6":"code","8794a3ab":"code","eca67a84":"code","3a54d0a8":"code","5abea084":"code","ca5b8993":"code","c7317075":"code","8e92d38c":"code","8f66f615":"code","9b0e027a":"code","d4f81e4f":"code","85dcd86e":"code","aa66a25c":"markdown","3a18452f":"markdown","2084de47":"markdown","0fc3cd98":"markdown","b3faadfe":"markdown","347dcf46":"markdown","547cd7ec":"markdown","1d158222":"markdown","d27682d0":"markdown","86d5d2c2":"markdown","48b2a7ba":"markdown","3d641792":"markdown","757c3008":"markdown","ccc404e9":"markdown","0cd255b7":"markdown","39742c48":"markdown","2482ddb4":"markdown","5ad35d68":"markdown","cde43ff7":"markdown","9c59a908":"markdown","49ae138f":"markdown","3b66db1d":"markdown","209f1f67":"markdown","369f13f2":"markdown","111ed6bd":"markdown","1b4b3052":"markdown","06f7dbaf":"markdown","efec7eaf":"markdown","baaa50e9":"markdown","f98552ae":"markdown","cf56dcf2":"markdown","1aa5881b":"markdown","5207854a":"markdown","1865122f":"markdown","435d6e38":"markdown","8fb563c9":"markdown","9c3f2a4d":"markdown","df8ae629":"markdown"},"source":{"24dedfb6":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd","d5e69bb4":"dataset = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","1616dcc1":"dataset.head()","5e3c56b9":"dataset.shape","18901ec7":"#checking correlation between features\nplt.figure(figsize=(8, 6))\nsns.heatmap(dataset.corr(), annot=True, linewidths=2)\nplt.show()","906c1b52":"#check for null values\ndataset.isna().any()","f18a4ad6":"# getting overview of columns\ndataset.info()","aaa5d53e":"#summary statistics\ndataset.describe()","af92bfd5":"#replace zero values with np.nan to visualize them\nzero_not_accepted = [\"Glucose\", \"BloodPressure\", \"SkinThickness\",\n                    \"Insulin\", \"BMI\"]\nfor col in zero_not_accepted:\n    dataset[col].replace(0, np.nan, inplace=True)\n\n#checkf if zeros were replaced in required columns    \ndataset.head(n=10)","0debc26a":"#Visualizing Null values\nplt.figure(figsize=(9,5))\nax = sns.barplot(x=dataset.isna().sum(),\n           y=dataset.columns, orient='h')\nfor p in ax.patches:\n    ax.annotate(text=f\"{p.get_width():.0f}\", \n                xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n                xytext=(5, 0), textcoords='offset points', \n                ha=\"left\", va=\"center\",\n               )\nplt.grid(False)\nplt.show()","1d2c2fac":"#imputing mean instead of null values\nfor col in zero_not_accepted:\n    dataset[col].replace(np.nan, dataset[col].mean(), inplace=True)","b729eeac":"dataset.describe()","86a9aadc":"#Plot pairwise relationships in a dataset\nplt.figure(figsize=(20,20))\nsns.pairplot(data=dataset, hue=\"Outcome\", diag_kind=\"hist\")\nplt.show()","bed5c8a0":"#distribution of outcomes\ndataset[\"Outcome\"].value_counts()","1e80f20c":"#extracting input and output features\nX = dataset.iloc[:, :-1].to_numpy()\ny = dataset.iloc[:, -1].to_numpy()","d5e0ad0e":"print(X)","32514970":"print(y)","b98f5d47":"from imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state=0)\nX_r, y_r = smk.fit_resample(X, y)","c39e8d99":"# from imblearn.over_sampling import RandomOverSampler\n# om = RandomOverSampler(random_state=0)\n# X_r, y_r = om.fit_resample(X, y)","3f60eb87":"#Check if over sampling worked\nfrom collections import Counter\nprint(f\"Initial counts: {Counter(y)}\")\nprint(f\"Resampled Counts: {Counter(y_r)}\")","8071c44d":"print(X_r.shape, y_r.shape)","fabc066b":"#updating the input and output features for further exploration\nX = X_r\ny = y_r","22353c0a":"#split the dataset in Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                   test_size=0.2,\n                                                   random_state=0)","8c20f1aa":"print(X_train.shape)\nprint(X_train)","f8ddafcb":"print(y_train.shape)\nprint(y_train)","e4145951":"print(X_test.shape)\nprint(X_test)","812065c1":"print(y_test.shape)\nprint(y_test)","50dc4100":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4263597c":"#Notice the mean ~ 0 and std ~ 1 for all the input features\npd.DataFrame(X_train, columns=dataset.columns[:-1]).describe()","73daa7cb":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score","cccbbfea":"#Helper functions to judge different classifiers\n\n#function to display an exquisite confusion matrix\ndef disp_cm(y_test, y_pred)->float:\n    \"\"\"Displays the confusion matrix in the form of heatmap.\n    \n    Parameters:\n    y_test (array-like): list of true labels\n    y_pred (array-like): list of predicted labels\n    \n    Returns:\n    acc_score (float): Accuracy score \n    \"\"\"\n    acc_score = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\"d\")\n    plt.title(f\"Accuracy: {acc_score:0.3f}\")\n    plt.xlabel(\"Predicted labels\")\n    plt.ylabel(\"Actual labels\")\n    plt.show()\n    return acc_score\n#function to generate performance report of a classifer\ndef judge_clf(classifier, X_train=X_train, y_train=y_train,\n              X_test=X_test, y_test=y_test)->float:\n    \"\"\"Fits the `classifier` to `X_train`, `y_train` and generate an elegant \n    classification report using `X_test` and `y_test`.\n    \n    Parameters:\n    classifer : classifier obj implementing 'fit' method.\n    X_train (array-like): 2D-array of input features of Training Set.\n    y_train (array-like): list of target features of Training Set.\n    X_test  (array-like): 2D-array of input features of Testing Set.\n    y_test  (array-like): list of target features of Testing Set.\n    \n    Returns:\n    acc_score (float): Accuracy score \n    \"\"\"\n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    acc_score = disp_cm(y_test, y_pred)\n    print(classification_report(y_test, y_pred))\n    return acc_score","f39c5c3f":"from sklearn.linear_model import LogisticRegression\nlog_clf = LogisticRegression(random_state=0)\nlog_acc = judge_clf(log_clf)","e9fc9a60":"from sklearn.neighbors import KNeighborsClassifier\nknn_clf = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\nknn_acc = judge_clf(knn_clf)","fc51efce":"from sklearn.svm import SVC\nlsvm = SVC(kernel=\"linear\", random_state=0)\nlsvm_acc = judge_clf(lsvm)","a45c868b":"ksvm = SVC(kernel=\"rbf\", random_state=0)\nksvm_acc = judge_clf(ksvm)","0fa6eb58":"from sklearn.naive_bayes import GaussianNB\nnb_clf = GaussianNB()\nnaiveb_acc = judge_clf(nb_clf)","3bd7209f":"from sklearn.tree import DecisionTreeClassifier\ndtree_clf = DecisionTreeClassifier(criterion=\"entropy\",\n                                             random_state=0\n                                  )\ndtree_acc = judge_clf(dtree_clf)","b9166c62":"from sklearn.ensemble import RandomForestClassifier\nrfor_clf = RandomForestClassifier(n_estimators=100,\n                                            criterion=\"entropy\",\n                                            random_state=0)\nrfor_acc = judge_clf(rfor_clf)","69e3e6f4":"from xgboost import XGBClassifier\nxgb_clf = XGBClassifier(use_label_encoder=False,\n                       verbosity=0)\nxgb_acc = judge_clf(xgb_clf)","81eb706f":"import warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)","b4b89fd9":"from sklearn.neural_network import MLPClassifier\nmlp_clf = MLPClassifier(random_state=0)\nmlp_acc = judge_clf(mlp_clf)","c74a58b6":"models = pd.DataFrame({\n    'Model': [\"Logistic Regr\", \"KNN\", \"SVM-Linear\", \"SVM-RBF\", \n             \"Naive Bayes\", \"Decision-Tree\", \"Radom Forest\", \"XGB\",\n             \"MLP\"],\n    'Accuracy Score': [log_acc, knn_acc, lsvm_acc, ksvm_acc, \n                       naiveb_acc, dtree_acc, rfor_acc, xgb_acc, \n                       mlp_acc]\n})\n\nmodels.sort_values(by = 'Accuracy Score', ascending = False, ignore_index=True)","8794a3ab":"from sklearn.model_selection import cross_val_score\ndef perform_kfold(clf, X_train=X_train, y_train=y_train)->(float, float):\n    \"\"\"Performs k-fold cross validation on given data(X_train, y_train) using \n    the `clf` (aka classifier)\n    \n    Parameters:\n    classifer : classifier obj implementing 'fit' method.\n    X_train (array-like): 2D-array of input features of Training Set.\n    y_train (array-like): list of target features of Training Set.\n    \n    Returns:\n    mean_score (float): Mean of Accuracy scores after operation.\n    std_score  (float): Standard Deviation of Accuracy scores.\n    \"\"\"\n    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, \n                            cv=10 #number of folds\n                            )\n    \n    mean_score = scores.mean()\n    std_score = scores.std()\n    print(f\"Mean Accuracy: {mean_score*100:0.3f} %\")\n    print(f\"Standard Deviation: {std_score*100:0.3f} %\")\n    \n    return mean_score, std_score\n    ","eca67a84":"log_macc, log_std = perform_kfold(log_clf)","3a54d0a8":"knn_macc, knn_std = perform_kfold(knn_clf)","5abea084":"lsvm_macc, lsvm_std = perform_kfold(lsvm)","ca5b8993":"ksvm_macc, ksvm_std = perform_kfold(ksvm)","c7317075":"naiveb_macc, naiveb_std = perform_kfold(nb_clf)","8e92d38c":"dtree_macc, dtree_std = perform_kfold(dtree_clf)","8f66f615":"rfor_macc, rfor_std = perform_kfold(rfor_clf)","9b0e027a":"xgb_macc, xgb_std = perform_kfold(xgb_clf)","d4f81e4f":"mlp_macc, mlp_std = perform_kfold(mlp_clf)","85dcd86e":"models_1 = pd.DataFrame({\n    \"Model\": [\"Logistic Regr\", \"KNN\", \"SVM-Linear\", \"SVM-RBF\", \n             \"Naive Bayes\", \"Decision-Tree\", \"Radom Forest\", \"XGB\",\n             \"MLP\"],\n    \"Mean Accuracy Score\": [log_macc, knn_macc, lsvm_macc, ksvm_macc, \n                       naiveb_macc, dtree_macc, rfor_macc, xgb_macc, \n                       mlp_macc],\n    \"Stadard Dev\": [log_std, knn_std, lsvm_std, ksvm_std, \n                       naiveb_std, dtree_std, rfor_std, xgb_std, \n                       mlp_std]\n})\n\nmodels_1.sort_values(by = 'Mean Accuracy Score', ascending = False,\n                     ignore_index=True)","aa66a25c":"### Logistic Regresion","3a18452f":"### Comparing Models","2084de47":"- It seems like `Logistic Regression` performs best!\n- But before we jump into any conclusions let's perform K-fold cross validation.","0fc3cd98":"### SVM with Non-Linear kernel\n- Kernel used: RadialBasisFunction","b3faadfe":"### Decision Trees","347dcf46":"## K-Fold Cross Validation\n- we'll apply k-fold cross validation on our _training_ set.","547cd7ec":"- We have 768 rows and 9 columns(features)","1d158222":"### XGBoost","d27682d0":"### Splitting the Dataset into Training and Test Set","86d5d2c2":"### Feature Scaling\n\nAs all the input features are numerical values, we will perform Standarization(Z-score normalization): \n\n> $x_{i} = (x_{i} - \\mu_{i})\\div\\sigma $\n\n$\\mu$ - mean, $\\sigma$ - standard deviation\n","48b2a7ba":"### Logistic Regression","3d641792":"# Diabetes Prediction \n## [Youtube Explanation Video](https:\/\/youtu.be\/TtHrmEeMKeY)\n","757c3008":"## Handling Class Imbalance Problem using Over Sampling","ccc404e9":"## Importing Dataset","0cd255b7":"### Naive Bayes","39742c48":"## Training Various Classification models from `sklearn`","2482ddb4":"### Naive Bayes","5ad35d68":"### MLP (Multi-Layer-Perceptron) classifier of `sklearn`","cde43ff7":"### Decision Tree","9c59a908":"- So, it turns out `MLPClassifier` and `Random forest` are the winners after K-fold Crossvalidation since they both have similar accuracy.\n\n>Note: The standard deviation has been reduced after oversampling!","49ae138f":"- Important Observation(s):\n    - It seems like null values are present in the form of `zeros`.\n    - It's impossible to have `Glucose`, `BloodPressure`, `SkinThickness`, `Insulin`, `BMI` to be _zero_. So, we have to handle this.\n- Let's get a sense of how many zero value are present in each column.    ","3b66db1d":"- We can see there are lot of null values in `SkinThickness` and `Insulin` column.\n- So, after Imputation the `mean` will change drastically.","209f1f67":"### Kernel SVM - Kernel Support Vector Machine\n- Kernel used: RadialBasisFunction","369f13f2":"## Exploratory Data Analysis","111ed6bd":"### SVM with Linear kernel","1b4b3052":"## Data Preprocessing\n- There are mainly 3 things to be performed\n    1. Extracting input and output features.\n    2. Splitting the dataset into Training and Testing set.\n    3. Feature Scaling.","06f7dbaf":"### Random Forest","efec7eaf":"### KNN ","baaa50e9":"### Linear SVM - Linear Support Vector Machine","f98552ae":"- And as we thought, new _mean_ of `Insulin` and `SkinThickness` has increased drastically.","cf56dcf2":"### Extracting input(independent) and output(dependent) feature","1aa5881b":"### Random Forest","5207854a":"### XGBoost","1865122f":"## Importing Libraries","435d6e38":"### KNN - KNearestNeighbours","8fb563c9":"- `Glucose`, `BMI`, `Age`, `Pregnancies` have noticeable positive correlation with `Outcome`","9c3f2a4d":"### Comparing Models after K-fold cross validation","df8ae629":"### MLP Classifer"}}