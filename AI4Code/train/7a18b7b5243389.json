{"cell_type":{"4df16b17":"code","d3c8fa8c":"code","87789806":"code","ed8296a5":"code","4dd82af2":"code","7bf09770":"code","e4c8a7b7":"code","d2f3b4f1":"code","f6e41bee":"code","9d517229":"code","bfee326e":"code","f81a7ab4":"code","2831b928":"code","8482bc6b":"code","b29bac7e":"code","c1137d79":"code","74df6470":"code","ddc73ac2":"code","9a53f3d0":"code","974dfb97":"code","75add58f":"code","b3b8921b":"code","eca55d94":"code","c01093ec":"code","bf59eb88":"code","f2131224":"code","7a1ec186":"code","14777b99":"code","5ce90124":"code","1bee763e":"code","d7d84b24":"code","d69283ef":"code","7b119fda":"code","884ef9c0":"code","ec7121c3":"code","64cd3bee":"code","e832b8bd":"code","0cfd8123":"code","3fca0f7f":"code","048698a3":"code","b1fe7049":"code","63602212":"code","506d339b":"code","8dcf3085":"code","f3d83b5c":"code","6ecfb4e2":"code","55b15e69":"code","413d7aff":"markdown","1c72f7a2":"markdown","aa940546":"markdown","d399ea7c":"markdown","92ffa570":"markdown","670bc859":"markdown","7428e594":"markdown","44e7e2a4":"markdown","20464cc6":"markdown","47b6442e":"markdown","24900a79":"markdown","4aa62561":"markdown","04a8a4fe":"markdown","6ac9e4d1":"markdown","26a2cfd8":"markdown","a7bd40ff":"markdown","6d37c9d5":"markdown","aea3c040":"markdown","748c3eef":"markdown","01585121":"markdown","eaad1a2b":"markdown","f5703903":"markdown","ee727196":"markdown","2bf627d4":"markdown","fa226156":"markdown","6bc8de48":"markdown","f11d6c0a":"markdown","5ecffd38":"markdown","2134c6b4":"markdown","949e614e":"markdown","a97c3041":"markdown","0e7015b5":"markdown","02a23dc5":"markdown","80dc4629":"markdown","15a2764f":"markdown","e782a67e":"markdown","91f74a8b":"markdown"},"source":{"4df16b17":"#Import libraries.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.feature_extraction import DictVectorizer\nimport statsmodels.regression.linear_model as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n%matplotlib notebook","d3c8fa8c":"#Import dataset.\ntrain = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', delimiter=',')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', delimiter=',')\n\ntrain.set_index('Id', inplace=True)\ntest.set_index('Id', inplace=True)\n\nprint('train set size : {}'.format(train.shape))\nprint('test set size : {}'.format(test.shape))","87789806":"train.columns[train.dtypes != 'object']","ed8296a5":"numerical = train.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\nnumerical","4dd82af2":"numerical.shape","7bf09770":"# #Repartition of values for each numerical features.\n# for elt in numerical.columns:\n#     print(\"{} -> {}\".format(elt, numerical[elt].unique()), end='\\n\\n')","e4c8a7b7":"# Histogram.\nfig = plt.figure(figsize=(12,18))\n\nfor i in range(len(numerical.columns)):\n   fig.add_subplot(9,4,i+1)\n   sns.distplot(numerical.iloc[:,i].dropna(), kde=False)\n   plt.xlabel(numerical.columns[i])\n\nplt.tight_layout()","d2f3b4f1":"train.columns[train.dtypes == 'object']","f6e41bee":"categorical = train.select_dtypes(include='object').copy()\ncategorical","9d517229":"categorical.shape","bfee326e":"#Repartition of values for each categorical features.\nfor elt in categorical.columns:\n    print(\"{} -> {}\".format(elt, categorical[elt].unique()), end='\\n\\n')","f81a7ab4":"#Histogram.\nfig = plt.figure(figsize=(15,25))\n\nfor i in range(len(categorical.columns)):\n   fig.add_subplot(9,5,i+1)\n   ax = sns.countplot(categorical.iloc[:,i].dropna())\n   ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"right\", fontsize=7)\n   plt.xlabel(categorical.columns[i])\n\nplt.tight_layout()","2831b928":"train['Utilities'].value_counts()","8482bc6b":"#plt.title('Distribution of SalePrice')\n#sns.distplot(train['SalePrice'])\n\n#fig = plt.figure()\n#stats.probplot(train['SalePrice'], plot=plt)\n\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","b29bac7e":"f = plt.figure(figsize=(12,20))\n\nfor i in range(len(numerical.columns)):\n   f.add_subplot(9, 4, i+1)\n   sns.scatterplot(numerical.iloc[:,i], train['SalePrice'])\n   \nplt.tight_layout()","c1137d79":"correlation_mat = train.corr()\n\nf, ax = plt.subplots(figsize=(12,9))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation_mat, vmin=0.2, vmax=0.8, square=True, cmap='BuPu')\nplt.show()","74df6470":"correlation_mat['SalePrice'].sort_values(ascending=False)","ddc73ac2":"#Numerical\npercentage_missing = numerical.isna().sum() \/ len(train) * 100\npercentage_missing.sort_values(ascending=False)","9a53f3d0":"#Categorical\npercentage_missing = categorical.isna().sum() \/ len(train) * 100\npercentage_missing.sort_values(ascending=False)","974dfb97":"train.PoolQC.value_counts()","75add58f":"#Remove outliers in LotFrontage.\ntrain.drop(train[train['LotFrontage'] > 200].index, inplace=True)\n\n#Remove outliers in LotArea.\ntrain.drop(train[train['LotArea'] > 100000].index, inplace=True)\n\n#Remove outliers in BsmtFinSF1.\ntrain.drop(train[train['BsmtFinSF1'] > 4000].index, inplace=True)\n\n#No need to remove outliers in TotalBsmtSF because we will delete this feature further (high-correlated).\n\n#Remove outliers in 1stFlrSF.\ntrain.drop(train[train['1stFlrSF'] > 4000].index, inplace=True)\n\n#Remove outliers in GrLivArea.\ntrain.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace=True)\n\n#Remove outliers in LowQualFinSF.\ntrain.drop(train[train['LowQualFinSF'] > 550].index, inplace=True)\n\n\n#fig = plt.figure()\nplt.title('Probability plot with kurtosis fixed')\nstats.probplot(train['SalePrice'], plot=plt)\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","b3b8921b":"train['SalePrice'] = np.log(train['SalePrice'])","eca55d94":"fig = plt.figure()\nplt.title('Distribution of SalePrice without skewness')\nsns.distplot(train['SalePrice'])\nprint(\"Skewness: %f\" % train['SalePrice'].skew())","c01093ec":"#Concat train\/test set.\nall_data = pd.concat([train.drop('SalePrice', axis=1), test], sort=False)","bf59eb88":"all_data.shape","f2131224":"#Numerical missing values.\nall_data[numerical.columns].isna().sum().sort_values(ascending=False).head(15)","7a1ec186":"tmp = ['LotFrontage', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF']\n\nall_data[tmp] = all_data[tmp].fillna(0)\nall_data[numerical.columns] = all_data[numerical.columns].fillna(all_data[numerical.columns].mean())","14777b99":"#Categorical missing values.\nall_data[categorical.columns].isna().sum().sort_values(ascending=False).head(15)","5ce90124":"tmp = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageQual', 'GarageFinish',\n       'GarageType', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\nall_data[tmp] = all_data[tmp].fillna('None')\n\nall_data[categorical.columns] = all_data[categorical.columns].fillna(all_data[categorical.columns].mode().iloc[0, :])","1bee763e":"all_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","d7d84b24":"all_data['TotalBathroom'] = all_data['FullBath'] + all_data['HalfBath']\nall_data.drop(['FullBath', 'HalfBath'],axis=1,inplace=True)","d69283ef":"features_to_drop = ['GarageArea', 'TotalBsmtSF', 'GarageYrBlt', 'TotRmsAbvGrd']\nall_data.drop(columns=features_to_drop, inplace=True)","7b119fda":"all_data.drop(columns='Utilities', inplace=True)","884ef9c0":"all_data = pd.get_dummies(all_data)","ec7121c3":"all_data.shape","64cd3bee":"nTrain = train.SalePrice.shape[0]","e832b8bd":"train_transf = all_data[:nTrain]\ntest_transf = all_data[nTrain:]","0cfd8123":"#Split dataset into training and validation set.\n#X_train, X_val, y_train, y_val = train_test_split(train_transf, train['SalePrice'], random_state=1)","3fca0f7f":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","048698a3":"lin_reg = LinearRegression()\nlasso = Lasso(alpha =0.0005, random_state=1)\nridge = Ridge(alpha =0.0005, random_state=1)\nEnet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nmodel_xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_lgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nbase_models = [lin_reg, lasso, ridge, Enet, GBoost, model_xgb, model_lgb]","b1fe7049":"# header_list = ['y_lin_reg', 'y_lasso', 'y_ridge', 'y_Enet', 'y_GBoost', 'y_model_xgb', 'y_model_lgb', 'y_cb']\nheader_list = ['y_lin_reg', 'y_lasso', 'y_ridge', 'y_Enet', 'y_GBoost', 'y_model_xgb', 'y_model_lgb']\nnew_train_dataset = pd.DataFrame(columns=header_list)\nnew_test_dataset = pd.DataFrame()\n#Enable us to pick the meta model.\nmae_compare = pd.Series(index=header_list)","63602212":"kfold = KFold(n_splits=6, random_state=42)\n\n#For each model.\nfor model, header in zip(base_models, header_list):\n    #Fit 80% of the training set (train_index) and predict on the remaining 20% (oof_index). \n    for train_index, oof_index in kfold.split(train_transf, train['SalePrice']):\n        X_train, X_val = train_transf.iloc[train_index, :], train_transf.iloc[oof_index, :]\n        y_train, y_val = train.SalePrice.iloc[train_index], train.SalePrice.iloc[oof_index]\n    \n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        new_train_dataset[header] = y_pred\n        \n        mae_compare[header] = mean_absolute_error(np.exp(y_val), np.exp(y_pred))\n    \n    #Create new_test_set at the same time.\n    print(header)\n    new_test_dataset[header] =  model.predict(test_transf)\n\n# #Add y_val to new_train_dataset.\n# #If we don't drop the ID, we will get NaN if ID doesn't match with new_train_dataset index.\nnew_train_dataset['y_val'] = y_val.reset_index(drop=True)","506d339b":"#Pick the meta-model.\nmae_compare.sort_values(ascending=True)","8dcf3085":"#Train meta-model on new_train_dataset.\nlasso.fit(new_train_dataset.iloc[:, :-1], new_train_dataset.iloc[:, -1])","f3d83b5c":"#Apply train meta-model to new_test_dataset.\ny_meta_pred = lasso.predict(new_test_dataset)","6ecfb4e2":"print('{}: {}'.format('train_transf',train_transf.shape))\nprint('{}: {}'.format('test_transf',test_transf.shape))\nprint('{}: {}'.format('X_train',X_val.shape))\nprint('{}: {}'.format('y_train',y_val.shape))\nprint('{}: {}'.format('X_val',X_val.shape))\nprint('{}: {}'.format('y_val',y_val.shape))\nprint('{}: {}'.format('new_train_dataset',new_train_dataset.shape))\nprint('{}: {}'.format('new_test_dataset',new_test_dataset.shape))\nprint('{}: {}'.format('y_meta_pred',y_meta_pred.shape))","55b15e69":"# #Submission\n# output = pd.DataFrame({'Id': test.index,\n#                        'SalePrice': (np.exp(y_meta_pred))})\n# output.to_csv('stacking_submission.csv', index=False)","413d7aff":"### b) Creating new features","1c72f7a2":"### d) Correlation Matrix","aa940546":"Note:\n\n- Could it be possible for a missing value 'NaN' to have another meaning ? \n    - PoolQC -> pool quality. It is described by \"Ed\/Fa\/Ex\". 'NaN' can mean here that there is no pool at all.","d399ea7c":"### a) Hyperparameter tuning","92ffa570":"### c) Concatenate train\/test set","670bc859":"We need to remove highly correlated features.","7428e594":"Certains numerical features are in fact categorical variable:\n\n- MSSubClass","44e7e2a4":"### c) Adressing highly-correlated independant features ","20464cc6":"# 4) Build a model","47b6442e":"### d) Adressing useless features","24900a79":"### e) Missing values check","4aa62561":"### d) Adressing skewness (independant variable)","04a8a4fe":"### b) Categorical variables","6ac9e4d1":"Note:\n\nBased on a first viewing of the scatter plots against SalePrice, there appears to be:\n\n- A few outliers on the LotFrontage (say, >200) and LotArea (>100000) data.\n- BsmtFinSF1 (>4000) and TotalBsmtSF (>6000)\n- 1stFlrSF (>4000)\n- GrLivArea (>4000 AND SalePrice <300000)\n- LowQualFinSF (>550) ","26a2cfd8":"### a) Convert numerical variable to categorical (resp)","a7bd40ff":"#### ii) Numerical columns and target variable","6d37c9d5":"No assumptions required us to normalize our independant variables. If we want to do so, we are not \"normalizing\" but rather making it easier to detect \"multicolinearity\" (one of the assumption).","aea3c040":"### a) Adressing kurtosis (outliers)","748c3eef":"# 2) Data Cleaning & Preprocessing","01585121":"### f) Split all_data into train\/test set ","eaad1a2b":"### a) Numerical variables ","f5703903":"### e) Dealing with missing values","ee727196":"Distribution of categorical variable.","2bf627d4":"### c) Target variable analysis","fa226156":"Objectives:\n\n- Create numerical\/categorical variables DataFrame.\n- Repartition of values for each numerical\/categorical features.\n- Check distplot\/countplot to see which feature can be dropped.\n- Check if can transform numerical feature into categorical one (resp)","6bc8de48":"### e) Encoding categorical values","f11d6c0a":"Note:\n\nWe can remove 'Utilities' because it is filled only of AllPub, we can remove it.\n\nUtilities: Type of utilities available\n\t\t\n       AllPub\tAll public Utilities (E,G,W,& S)\t\n       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n       NoSeWa\tElectricity and Gas Only\n       ELO\tElectricity only\t\n","5ecffd38":"Note:\n\n- Perform log transformation on target variable to fix skewness.\n- Remove outliers to fix kurtosis.","2134c6b4":"# 3) Feature selection & Engineering","949e614e":"### b) Adressing skewness (target variable)","a97c3041":"Bivariate analysis - scatter plots for target versus numerical attributes (Find outliers)","0e7015b5":"To perform regression, we have to follow some assumptions. Two of them are \"Normality of error\" (Multivariate Normality) and  \"Homoscedasticity\" (Constant Error Variance).\n\n- Normality of error: Error (difference between predicted value and exact value) has to follow a normal distribution.\n\n- Homoscedasticity: Error has a constant variance.\n\nWhen you apply log function on y, you are normalizing it which means that y will have a mean = 0 and a variance = 1.\n(\"Normality of error\" assumption checked). Since variance = 1 which is a constant, error has a constant variance (\"Homoscedasticity\" assumption checked).","02a23dc5":"Note:\n\nThose features are highly-correlated (multicollinearity). Remove the one that is less-correlated with target variable, denoted with (x).\n  \n- GarageArea (x) \/ GarageCars\n- 1stFlrSF \/ TotalBsmtSF (x)\n- GarageYrBlt (x) \/ YearBuilt\n- TotRmsAbvGrd (x) \/ GrLivArea\n","80dc4629":"#### i) skewness & kurtosis","15a2764f":"### b) Stacking ","e782a67e":"Distribution of numerical variable.","91f74a8b":"#  1) Exploratory Data Analysis"}}