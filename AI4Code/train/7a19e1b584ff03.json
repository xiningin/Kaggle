{"cell_type":{"cda0c45e":"code","c9011314":"code","b6026045":"code","196b53df":"code","f442d8a4":"code","5c8893ec":"code","95845d38":"code","42b20604":"code","a9dcee2f":"code","f765fc68":"code","8ce280b9":"code","2f7cec93":"code","3aec01b3":"code","afee3d3d":"code","938a5c5c":"code","934d811b":"markdown","8d01fa74":"markdown","866d996a":"markdown","85f5f7f0":"markdown","da1fc5b6":"markdown","b2ec6c4e":"markdown","b138890a":"markdown"},"source":{"cda0c45e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n \n# Importing the libraries\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n    \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c9011314":"data = pd.read_csv('..\/input\/international-airline-passengers.csv',skipfooter=5, engine='python') # skipfooter: son 5 satiri at\ndata.head()","b6026045":"dataset = data.iloc[:,1].values\nplt.plot(dataset)\nplt.xlabel(\"time\")\nplt.ylabel(\"Number of Passenger\")\nplt.title(\"international airline passenger\")\nplt.show()","196b53df":"dataset = dataset.reshape(-1,1)\ndataset = dataset.astype(\"float32\")\ndataset.shape","f442d8a4":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0, 1))\ndataset_scaled = scaler.fit_transform(dataset)","5c8893ec":"plt.plot(dataset_scaled)\nplt.show()","95845d38":"train_size = int(len(dataset) * 0.60)\ntest_size = len(dataset) - train_size\ntrain = dataset_scaled[0:train_size,:]\ntest = dataset_scaled[train_size:len(dataset),:]\nprint(\"train size: {}, test size: {} \".format(len(train), len(test)))","42b20604":"time_stemp = 10\ndataX = []\ndataY = []\nfor i in range(len(train)-time_stemp-1):\n    a = train[i:(i+time_stemp), 0]\n    dataX.append(a)\n    dataY.append(train[i + time_stemp, 0])\ntrainX = np.array(dataX)\ntrainY = np.array(dataY)  ","a9dcee2f":"dataX = []\ndataY = []\nfor i in range(len(test)-time_stemp-1):\n    a = test[i:(i+time_stemp), 0]\n    dataX.append(a)\n    dataY.append(test[i + time_stemp, 0])\ntestX = np.array(dataX)\ntestY = np.array(dataY) ","f765fc68":"trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\ntestX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\nprint('shape of trainX: ', trainX.shape)\nprint('shape of testX: ', testX.shape)","8ce280b9":"# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the first RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 100,activation='relu', return_sequences = True, input_shape = (trainX.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n# Adding a second RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 80,activation='relu', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 50,activation='relu', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 30,activation='relu', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fifth RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 20))\nregressor.add(Dropout(0.2))\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(trainX, trainY, epochs = 250, batch_size = 50)","2f7cec93":"import math \nfrom sklearn.metrics import mean_squared_error\n\ntrainPredict = regressor.predict(trainX)\ntestPredict = regressor.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","3aec01b3":"plt.plot(trainPredict)","afee3d3d":"plt.plot(testPredict)\n","938a5c5c":"# shifting train\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[time_stemp:len(trainPredict)+time_stemp, :] = trainPredict\n# shifting test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(time_stemp*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset_scaled), color='b')\nplt.plot(trainPredictPlot, color='r')\nplt.plot(testPredictPlot, color='g')\nplt.show()","934d811b":"REFERENCES\n* Tutor: Kaan Can udemy course (deep learning)","8d01fa74":"<a id=\"6\"><\/a>\n## Conclusion\n* Sequence Models\n* Recurrent Neural Network (RNN)\nImplementing Recurrent Neural Network with Keras\n    * Loading and Preprocessing Data\n    * Create RNN Model\n    * Predictions and Visualising RNN Model\n* Long Short Term Memory (LSTMs)\n* Implementing Long Short Term Memory with Keras\n    * Loading and Visualizing Data\n    * Preprocessing Data\n    * Create LSTM Model\n    * Predictions and Visualising RNN Model\n    * Implementing Long Short Term Memory","866d996a":"<a id=\"43\"><\/a>\n### Create RNN Model","85f5f7f0":"<a id=\"41\"><\/a>\n### Loading and Visualizing Data","da1fc5b6":"# Recurrent Neural Network (RNN)\n\n## Content\n* [Sequence Models](#1)\n* [Recurrent Neural Network (RNN)](#2)\n* [Implementing Recurrent Neural Network with Keras](#3)\n    * [Loading and Preprocessing Data](#31)\n    * [Create RNN Model](#32)\n    * [Predictions and Visualising RNN Model](#33)\n* [Long Short Term Memory (LSTMs)](#4)\n* [Implementing Long Short Term Memory with Keras](#99)\n    * [Loading and Visualizing Data](#41)\n    * [Preprocessing Data](#42)\n    * [Create LSTM Model](#43)\n    * [Predictions and Visualising LSTM Model](#44)\n* [Conclusion](#6)\n\n<a id=\"1\"><\/a>\n## Sequence Models\n* Sequence models plays an over time. \n* Speech recognition, natural language process (NLP), music generation\n* Apples Siri and Google's voice search\n* Sentiment classification (duygu s\u0131n\u0131fland\u0131rma) Mesela \"bu ders bu dunyadaki en guzel ders\" yada \"sacma sapan ders cekmissin hocaaa\"  \n\n\n<a id=\"2\"><\/a>\n## Recurrent Neural Network\n* RNN\u2019s are able to remember important things about the input they received, which enables them to be very precise in predicting what\u2019s coming next.\n* This is the reason why they are the preferred algorithm for sequential data like time series, speech, text, financial data, audio, video, weather and much more because they can form a much deeper understanding of a sequence and its context, compared to other algorithms.\n* Not only feeds output but also gives feed backs into itself. Because RNN has internal memory\n* temporal loop = zamansal d\u00f6ng\u00fc. Kendini besler.\n* ![title](temporal loop.jpg)\n* Belle\u011fe sahipler short term memory bir \u00f6nceki node da olanlar\u0131 hat\u0131rlarlar. Eskiyi hat\u0131rlar.\n* Mesela ge\u00e7mi\u015fi hat\u0131rlamak neden \u00f6nemli biz yapt\u0131klar\u0131m\u0131zdan bir \u015feyler \u00f6\u011freniriz ve yeni \u00f6\u011frenilen \u015feyleri de eski \u00f6\u011frendiklerimizi \u00fczerine kurar\u0131z. RNN'i de ayn\u0131 mant\u0131kta d\u00fc\u015f\u00fcnebilirsiniz. Film \u00f6rne\u011finde oldu\u011fu gibi.\n* ![title](new world image.jpg)\n* \u00d6rnek RNN yap\u0131lar\u0131na bakal\u0131m\n* One to Many\n    * Input bir resim output o resimde yap\u0131lan c\u00fcmle yani \"Adam surf yap\u0131yor\"\n    * ![title](one to many.jpg)\n* Many to One\n    * Input bir c\u00fcmle output bir duygu mesela iyimser ne\u015feli gibi.\n    * ![title](many to one.jpg)\n* Many to Many\n    * Mesela google translate kullanarak \u0130ngilizceden bir c\u00fcmleyi T\u00fcrk\u00e7e'ye translate etmek\n    * ![title](many to many.jpg)\n* RNN short term memory'e sahip ama LSTM long term memory'e de sahip olabiliyor.\n* RNN'i ANN yada CNN'den ay\u0131ran daha \u00f6nce de belirtti\u011fimiz gibi *memory*. Mesela \"DATAI\" diye bir stringimiz var ve biz 4. harfe geldik yani \"A\" harfine. ANN' e sordu\u011fumuz zaman 4. harfi A olan bir kelimenin 5. harfi ne olabilir diye. ANN bilemez \u00e7\u00fcnk\u00fc memory olmad\u0131\u011f\u0131 i\u00e7in ge\u00e7mi\u015f harfleri yani \"DAT\" harflerini bilip \"A\" ile birle\u015ftirip daha sonra 5. harf \"I\" olabilir diyemez. Ama RNN tam olarak bunu s\u00f6yleyebilir.\n* Exploiding Gradients: Gradient'in \u00e7ok b\u00fcy\u00fck olmas\u0131 durumu. Gereksiz yere belli weightlere \u00f6nem kazand\u0131r\u0131r.\n* Vanishing Gradients: Gradient'in \u00e7ok k\u00fc\u00e7\u00fck olmas\u0131 durumu. Yava\u015f \u00f6\u011frenir.\n* Gradient neydi arkada\u015flar costa g\u00f6re weightlerde ki de\u011fi\u015fim.","b2ec6c4e":"Predictions and Visualising RNN Model","b138890a":"<a id=\"42\"><\/a>\n### Preprocessing Data\n* reshape\n* change type\n* scaling\n* train test split\n* Create dataset"}}