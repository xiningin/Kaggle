{"cell_type":{"22ef1397":"code","5f8ea20d":"code","1739b7f4":"code","cc4d7f20":"code","15bd12f5":"code","8f48e815":"code","f305afa6":"code","f77bc331":"code","b7ada96b":"code","abe4ae9b":"code","c1f55acb":"code","86e95028":"code","2d25a796":"code","f11c7adc":"code","c7306352":"code","a513df9a":"code","c317a40d":"code","2e0f8d59":"code","4773e8ff":"code","212b14b4":"code","f8e92ab1":"code","fd275f4a":"code","d7dbc80b":"code","f8f73ef4":"code","30d722ff":"code","84bff599":"code","49d6c134":"code","c86d96df":"code","e1f21dab":"code","6e968e32":"code","5c0969fb":"code","86c0f0d6":"code","d5a2ad2c":"code","288b31b9":"code","13997b41":"code","1968323e":"code","a0fe4683":"code","8f45645a":"code","4b78f995":"code","b19957d2":"code","a67d4296":"code","52b8c6b9":"code","bc91176f":"code","58150a08":"code","2cbc3e22":"code","fa80fb35":"markdown","39318bc4":"markdown","15fa86d3":"markdown","03525b98":"markdown","fb3e55d1":"markdown","c5fcb0e0":"markdown","15ea0d9c":"markdown","ed9ecc84":"markdown","7d0e400a":"markdown","a85774e7":"markdown","8892e247":"markdown","1c377f06":"markdown","0955dc71":"markdown","638fb996":"markdown","4bcef26a":"markdown","0aa65fda":"markdown","d164c096":"markdown","34fa79ae":"markdown","085d42f9":"markdown","7b40b4d3":"markdown","91ccb2d6":"markdown","8dbdaabe":"markdown","98144efc":"markdown","6167e867":"markdown","e21a1e8c":"markdown","eeb84dc5":"markdown","f92ffe3e":"markdown","71dace06":"markdown","0fd2ea83":"markdown","fbfc3e3d":"markdown","82d59a36":"markdown","76abcaf1":"markdown","892a09b7":"markdown","f461f12f":"markdown","359689e7":"markdown","cfc6919d":"markdown","c1d345af":"markdown","913794a2":"markdown","2f4478fa":"markdown","e106824f":"markdown","87aa0f82":"markdown","68738beb":"markdown","5f1531c9":"markdown","56014e5e":"markdown","f39f399c":"markdown","b5b6d7f4":"markdown","770aeab4":"markdown","658f66fa":"markdown","e9c6a0f1":"markdown","ad2605c1":"markdown","7621a20e":"markdown","ec0521cd":"markdown"},"source":{"22ef1397":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom colorama import Fore, Back, Style\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","5f8ea20d":"train = pd.read_csv(\"\/kaggle\/input\/insurance-premium-prediction\/insurance.csv\")\ntrain.head()","1739b7f4":"train.shape","cc4d7f20":"train.isnull().sum()","15bd12f5":"train.info()","8f48e815":"for col in train.columns:\n    print(Fore.BLUE + \"Feature Name:\",col,Style.RESET_ALL)\n    print(Fore.YELLOW + \"\",train[col].describe(),Style.RESET_ALL)\n    print(Fore.RED + \"-------------------------------------------------\")\n    print(\"-------------------------------------------------\",Style.RESET_ALL)","f305afa6":"num_cols = [\"age\",'bmi','expenses','children']\n\nplt.figure(figsize = (15,10))\n\nfor idx,col in enumerate(num_cols):\n    plt.subplot(2,2,idx+1)\n    sns.distplot(train[col])","f77bc331":"# helper function to perform univariate analysis\ndef univariate(feat):\n    count = train[feat].value_counts()\n       \n    print(\"--------------------\" + feat + \" Distribution (counts)------------------\")\n    print(count)\n\n    print(\"--------------------\" + feat + \" Distribution (in %)------------------\")    \n    if feat == \"region\":\n        val1 = count.values[0]\n        val2 = count.values[1]\n        val3 = count.values[2]\n        val4 = count.values[3]\n    else:\n        val1 = count.values[0]\n        val2 = count.values[1]\n\n    \n    if feat == \"region\":\n        print(\"% of \" + count.index[0] + \":\", (val1\/train.shape[0]) * 100)\n        print(\"% of \" + count.index[1] + \":\", (val2\/train.shape[0]) * 100)\n        print(\"% of \" + count.index[2] + \":\", (val3\/train.shape[0]) * 100)\n        print(\"% of \" + count.index[3] + \":\", (val4\/train.shape[0]) * 100)\n    else:\n        print(\"% of \" + count.index[0] + \":\", (val1\/train.shape[0]) * 100)\n        print(\"% of \" + count.index[1] + \":\", (val2\/train.shape[0]) * 100)\n\n    # lets visualize what we discussed above\n    plt.figure(figsize = (8,6))\n    if feat == \"region\":\n        plt.bar(count.index, count.values,color = ['g','b','y','r'])\n    else:\n        plt.bar(count.index, count.values,color = ['g','b'])\n        \n    plt.show()","b7ada96b":"univariate(\"sex\")","abe4ae9b":"univariate(\"smoker\")","c1f55acb":"univariate(\"region\")","86e95028":"sns.countplot(train.sex,hue=train.smoker)","2d25a796":"sns.pairplot(train)","f11c7adc":"sns.pairplot(train,hue=\"smoker\")","c7306352":"sns.pairplot(train,hue=\"sex\")","a513df9a":"corrMatrix = train.corr()\nmask = np.triu(corrMatrix)\nsns.heatmap(corrMatrix,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',            \n            mask=mask,\n            linewidths=1,\n            cbar=False)\nplt.show()","c317a40d":"train.sex = train.sex.astype(\"category\")\ntrain.sex = train.sex.cat.codes + 1","2e0f8d59":"train.region = train.region.astype(\"category\")\ntrain.region = train.region.cat.codes + 1","4773e8ff":"train.smoker = train.smoker.astype(\"category\")\ntrain.smoker = train.smoker.cat.codes + 1","212b14b4":"# scale numeric features\nscale = ['age','bmi','children']\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Apply scaler() to all the columns\ntrain[scale] = scaler.fit_transform(train[scale])","f8e92ab1":"train.describe()","fd275f4a":"X = train.drop('expenses',1)\ny = train.expenses","d7dbc80b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 , random_state=100)","f8f73ef4":"# import LinearRegression from sklearn\nfrom sklearn.linear_model import LinearRegression\n# Representing LinearRegression as lr(Creating LinearRegression Object)\nlr = LinearRegression()\n#You don't need to specify an object to save the result because 'lr' will take the results of the fitted model.\nlr.fit(X_train, y_train)","30d722ff":"# Print the intercept and coefficients\nprint(lr.intercept_)\nprint(lr.coef_)","84bff599":"# Making predictions on the testing set\ny_pred = lr.predict(X_test)","49d6c134":"# Actual vs Predicted\nplt.figure(figsize = (15,10))\n\nc = [i for i in range(1,len(X_test.index)+1,1)] # generating index\n\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.plot(c,y_pred, color=\"red\", linewidth=2.5, linestyle=\"-\")\nplt.suptitle('Actual and Predicted', fontsize=20) \nplt.xlabel('Index', fontsize=18) ","c86d96df":"# Error terms\nplt.figure(figsize = (15,10))\n\nc = [i for i in range(1,len(X_test.index)+1,1)] # generating index\n\nplt.plot(c,y_test-y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.suptitle('Error Terms', fontsize=20)\nplt.xlabel('Index', fontsize=18)\nplt.ylabel('ytest-ypred', fontsize=16) ","e1f21dab":"from sklearn.metrics import mean_squared_error, r2_score\nmse = mean_squared_error(y_test, y_pred)\nr_squared = r2_score(y_test, y_pred)\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","6e968e32":"import statsmodels.api as sm","5c0969fb":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()","86c0f0d6":"# Print the parameters, i.e. the intercept and the slope of the regression line fitted\nlr.params","d5a2ad2c":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","288b31b9":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm.columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm.values, i) for i in range(X_train_sm.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","13997b41":"X_train_sm.drop(\"sex\",1,inplace=True)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()\n\nprint(lr.summary())","1968323e":"# Making predictions on the testing set\ny_pred = lr.predict(X_test) # this should ideally be X_test_sm, but i want negative r-squared thats why kept X_test\n\nmse = mean_squared_error(y_test, y_pred)\nr_squared = r2_score(y_test, y_pred)\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","a0fe4683":"# lets create boxplots to detect outliars detection \nplt.figure(figsize=(16,48))\nfor idx,col in enumerate(['age','bmi','children','region']):\n    plt.subplot(11,2,idx+1)\n    plt.boxplot(train[col])\n    plt.xlabel(col)","8f45645a":"# lets handle the outliers\nq3 = train.bmi.quantile(0.75)\ntrain = train[train.bmi <= q3]","4b78f995":"X = train.drop(['expenses','sex'],1)\ny = train.expenses","b19957d2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8 , random_state=100)","a67d4296":"# Add a constant to get an intercept\nX_train_sm = sm.add_constant(X_train)\nX_test_sm = sm.add_constant(X_test)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y_train, X_train_sm).fit()","52b8c6b9":"# Performing a summary operation lists out all the different parameters of the regression line fitted\nprint(lr.summary())","bc91176f":"# Making predictions on the testing set\ny_pred = lr.predict(X_test_sm)\n\nmse = mean_squared_error(y_test, y_pred)\nr_squared = r2_score(y_test, y_pred)\nprint('Mean_Squared_Error :' ,mse)\nprint('r_square_value :',r_squared)","58150a08":"res = (y_test - y_pred)\n\nfig = plt.figure()\nsns.distplot(res, bins = 15)\nfig.suptitle('Error Terms', fontsize = 15)                  # Plot heading \nplt.xlabel('Actual - Predicted', fontsize = 15)         # X-label\nplt.show()","2cbc3e22":"plt.scatter(y_test,res)\nplt.show()","fa80fb35":"Error terms are not following normal distribution, which is not a good indication!\n\nLet's look at the patter in the the error terms now.","39318bc4":"# EDA\n\n### Univariate Analysis","15fa86d3":"Above, we built a model with all the variables, we may not require all the variables to build the model, few of them may be insignificant. let rebuild the model using statsmodel.api so that we can see some important properties of the model built","03525b98":"So, we have been provided with age, sex, bmi, no. of children, somke, region and expenses.\n\nHere, expenses is our target variable.","fb3e55d1":"**Importance of RSS\/TSS:**\n\nThink about it for a second. If you know nothing about linear regression and still have to draw a line to represent those points, the least you can do is have a line pass through the mean of all the points as shown below.\n\nThis is the worst possible approximation that you can do. TSS gives us the deviation of all the points from the mean line\n![image.png](attachment:image.png)","c5fcb0e0":"Now, we have a fair idea about regression metrices, lets now check MSE & R-Squared values for the models that we have built.","15ea0d9c":"# Build Model using OLS from statsmodel.api","ed9ecc84":"### Let'check\/remove outliers and rebuilt the model","7d0e400a":"There are almost equal no. of males and females in the given data","a85774e7":"### Visualize Error Terms(Actual - Predicted)","8892e247":"**TSS (Total sum of squares):**\n\nIt is the sum of errors of the data points from mean of response variable. Mathematically, TSS is:\n![image.png](attachment:image.png)","1c377f06":"### Visualize Actual Vs Predicted","0955dc71":"### Root Mean Squared Error: \n\nRMSE is the most widely used metric for regression tasks and is the square root of the averaged squared difference between the target value and the value predicted by the model. \n\nIt is preferred more in some cases because the errors are first squared before averaging which poses a high penalty on large errors.\n\nThis implies that RMSE is useful when large errors are undesired.\n\n<img src = \"https:\/\/miro.medium.com\/max\/650\/0*TO7BkvQwtnvVzkK4.png\">","638fb996":"There are more non-smokers than smokers!","4bcef26a":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<a id=\"10\"><\/a>\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Linear Regression!\n <\/center><\/h2>\n\n<img src = \"https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2019\/03\/Linear-Regression.jpg\" >   ","0aa65fda":"### Example - Simple Linear Regression\n\nAssume we have a dataset which contains information about relationship between `number of hours studied` and `marks obtained`. \n\nMany students have been observed and their hours of study and grade are recorded. \n\nOur goal is to design a model that can predict marks if given the number of hours studied. \n\nUsing the training data, a regression line is obtained which will give minimum error. \n\nThis linear equation is then used for any new data. That is, if we give number of hours studied by a student as an input, our model should predict their mark with minimum error.\n\n**Y(pred) = b0 + b1*x**\n\n**The values b0(intercept) and b1(slope) must be chosen so that they minimize the error.**\n\nAs discussed earlier, the goal is to find the `best fit line` which can be achieved by minimising the expression of **RSS (Residual Sum of Squares)** which is equal to the sum of squares of the residual for each data point in the plot. \n\nResiduals for any data point is found by subtracting predicted value of dependent variable from actual value of dependent variable, see the image below:\n\n![image.png](attachment:image.png)","d164c096":"# Let's Build the Model","34fa79ae":"Age and BMI seems to have a good relation with the target variable. \nLet's check co-relation between variables now!","085d42f9":"# Assumptions of Linear Regression\n\n### Distribution of the error terms(should follow normal distribution)\n\nWe need to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","7b40b4d3":"### R\u00b2 Error: \n\nCoefficient of Determination or R\u00b2 is another metric used for evaluating the performance of a regression model. \n\nThe metric helps us to compare our current model with a constant baseline and tells us how much our model is better. \n\nThe constant baseline is chosen by taking the mean of the data and drawing a line at the mean. \n\nR\u00b2 is a scale-free score that implies it doesn't matter whether the values are too large or too small, the R\u00b2 will always be less than or equal to 1.\n\n<img src = \"https:\/\/miro.medium.com\/max\/783\/0*_Bk3m941thWlveS3.png\">\n\nR\u00b2 Can also be represented as:\n`R\u00b2 = 1 - (RSS \/ TSS)`\n![image.png](attachment:image.png)","91ccb2d6":"**Purpose of this kernel will cover all the basic concepts related to Linear Regression.**\n\n`Linear regression` is used for finding linear relationship between `target` and one or more `predictors`. \n\nThere are two types of **Linear Regression**:\n\n1. Simple Linear Regression \n2. Multiple Linear Regression.\n\nThe core idea is to obtain a line that `best fits` the data. The best fit line is the one for which total prediction error (all data points) are as small as possible. \n\n`Error` is the distance between the point to the regression line.\n\nExample of Linear Regression below:\n\n<img src = \"https:\/\/miro.medium.com\/max\/875\/1*dToo8pNrhBmYfwmPLp6WrQ.png\">\n\nLet's check `Simple Linear Regression` in detail!","8dbdaabe":"# Multiple Linear Regression\n\nMultiple linear regression is a statistical technique to understand the relationship between one dependent\nvariable and several independent variables (explanatory variables).\n\nThe objective of multiple regression is to find a linear equation that can best determine the value of\ndependent variable Y for different values independent variables in X.\n\nConsider another example where we need to make sales prediction using TV Marketing budget. \n\nIn real life scenario, the marketing head would want to look into the dependency of sales on the budget allocated to different\nmarketing sources. Here, we have considered three different marketing sources, i.e. TV marketing, Radio\nmarketing, and Newspaper marketing.\n\nThe simple linear regression model is built on a straight line which has the following formula:\n**Y(pred) = b0 + b1*x**\n\nMultiple linear regression also uses a linear model that can be formulated in a very similar way.\n\nThus, the equation of multiple linear regression would be as follows: \n![image.png](attachment:image.png)","98144efc":"Trying to reinforce this understanding of R2 visually, you can look at the 4 graphs of marketing data and compare the corresponding R2 values.\n\n* In Graph 1: All the points lie on the line and the R2 value is a perfect 1\n* In Graph 2: Some points deviate from the line and the error is represented by the lower R2 value of 0.70\n* In Graph 3: The deviation further increases and the R2 value further goes down to 0.36\n* In Graph 4: The deviation is further higher with a very low R2 value of 0.05\n\n![image.png](attachment:image.png)","6167e867":"## Let's check VIF","e21a1e8c":"Only bmi is normally distributed!","eeb84dc5":"**Important note about R-Squared & Adjusted R-Squared**\n\nIn multiple variable regression, adjusted R-squared is a better metric than R-squared to assess how good the model fits the data. \n\nR-squared always increases if additional variables are added into the model, even if they are not related to the dependent variable. R-squared thus is not a reliable metric for model accuracy. \n\nAdjusted R-squared, on the other hand, penalises R-squared for unnecessary addition of variables. So, if the variable added does not increase the accuracy adequately, adjusted R-squared decreases although R-squared might increase.","f92ffe3e":"The r square value is about 80% which means our model is able to explain 80% of the variance which is good.\nWe will learn more about r-square later in this notebook.","71dace06":"### Adjusted R\u00b2: \n\nAdjusted R\u00b2 depicts the same meaning as R\u00b2 but is an improvement of it.\n\nR\u00b2 suffers from the problem that the scores improve on increasing terms even though the model is not improving which may misguide the researcher. \n\nAdjusted R\u00b2 is always lower than R\u00b2 as it adjusts for the increasing predictors and only shows improvement if there is a real improvement.\n\n<img src = \"https:\/\/miro.medium.com\/max\/495\/0*WkdWEm2993yhYvUA.png\">","0fd2ea83":"# Simple Linear Regression\n\nSimple linear regression is useful for finding relationship between two continuous variables. \n\nOne is `predictor` or `independent variable` and other is `response` or `dependent` variable. \n\nIt looks for `statistical relationship` but not `deterministic relationship`. \n\nRelationship between two variables is said to be deterministic if one variable can be accurately expressed by the other. \n\nFor example, using temperature in degree Celsius it is possible to accurately predict Fahrenheit. \n\nStatistical relationship is not accurate in determining relationship between two variables. For example, relationship between height and weight.","fbfc3e3d":"### Let's now check relation of various features with the target variable","82d59a36":"There is good amount of representation from all the four areas!","76abcaf1":"All the p-values are good now, let's rebuild the model","892a09b7":"# Model Evaluation (Visualization)\n\nWe have built the model, now its time to evaluate its peformance.\n\nWe will use MSE & R-Squared metrices to evaluate the performance of our model. But, before we do that, let's visualize our actual vs predicted values! ","f461f12f":"### Mean Absolute Error: \n\nMAE is the absolute difference between the target value and the value predicted by the model.\n\nThe MAE is more robust to outliers and does not penalize the errors as extremely as mse. \n\nMAE is a linear score which means all the individual differences are weighted equally. \n\nIt is not suitable for applications where you want to pay more attention to the outliers.\n\n<img src = \"https:\/\/miro.medium.com\/proxy\/0*zX9jlpZ8k0CuEpFE.jpg\">","359689e7":"we can now make predictions using this model as we did earlier!","cfc6919d":"The variance of residuals increasing with X indicates that there is significant variation that this model is unable to explain.","c1d345af":"### Let's try to understand few of the statistics shown above\n\n`coef(Coefficient)` - You can see the estimated value of the intercept is around -1.408e+04, and the estimated coefficients of Age is 3707.94, Sex is 13.28, bmi is 1882, childern is 572, smoker is 2.348e+04 and region is -384.\n\n`standard error`. It measures the variability in the estimate for these coefficients. \n\nA lower value of standard deviation is good but it is somewhat relative to the value of the coefficient. \n\nE.g. you can check the standard error of the intercept is about 909.559, whereas its estimate is -1.408e+04, So, it can be interpreted that the variability of the intercept is from -1.408e+04\u00b1909.559. \n\nNote that standard error is absolute in nature and so many a times, it is difficult to judge whether the model is good or not.\n\n`t-value` It is the ratio of the estimated coefficients to the standard deviation of the estimated coefficients. \n\nIt measures whether or not the coefficient for this variable is meaningful for the model. Though you may not use this value itself, you should know that it is used to calculate the p-value and the significance levels which are used for building the final model.\n\n`p-value` - A very important parameter of this analysis is the p-value, it is used for hypothesis testing. \n\nHere, in regression model building, the `null hypothesis` corresponding to each p-value is that the corresponding independent variable does not impact the dependent variable. \n\n`The alternate hypothesis is that the corresponding independent variable impacts the response.` \n\nNow, p-value indicates the probability that the null hypothesis is true. Therefore, a low p-value, i.e. less than 0.05,\nindicates that you can reject the null hypothesis. \n![image.png](attachment:image.png)","913794a2":"Almost equal no. of male\/female are smokers\/non-smokers!","2f4478fa":"By default, the statsmodels library fits a line on the dataset which passes through the origin. But in order to have an intercept, you need to manually use the add_constant attribute of statsmodels.\n\nAnd once you've added the constant to your X_train dataset, you can go ahead and fit a regression line using the OLS (Ordinary Least Squares) attribute of statsmodels as shown below","e106824f":"VIF is good for all the variables, but p-value of \"sex\" is greater than 0.05, this variable becomes insignificant and we can drop this variable.","87aa0f82":"A large value in this matrix would indicate a pair of highly correlated variables. \n\nUnfortunately, not all collinearity problems can be detected by the inspection of the correlation matrix. It is possible for\ncollinearity to exist between three or more variables even if no pair of variables has a high correlation.\n\nThis situation is called `multicollinearity`.\n\nA better way to assess multicollinearity is to compute the `variance inflation factor (VIF)`.\n\nSince one of the major goals of linear regression is identifying the important explanatory variables, it is important to assess the impact of each and then keep those which have a significant impact on the outcome. \n\nThis is the major issue with multicollinearity. Multicollinearity makes it difficult to assess the effect of individual predictors. \n\nA variable with a high VIF means it can be largely explained by other independent variables. Thus, you have to check and remove variables with a high VIF after checking for pvalues, implying that their impact on the outcome can largely be explained by other variables. Thus, removing the variable with a high VIF would make it easier to assess the impact of other variables, while\nmaking little difference to the predicted outcome.\n\nThe higher the VIF, the higher the multicollinearity. But remember \u2014 variables with a high VIF or multicollinearity may be statistically significant p<0.05, in which case you will first have to check for other insignificant variables before removing the variables with a higher VIF and lower p-values. You can took VIF = 2 as the threshold, but in real business scenarios, it will depend on the case requirements","68738beb":"### Let's check some statistics","5f1531c9":"**We will now try to build a Linear Regression Model and will asses it's performance!**","56014e5e":"### Multicollinearity\n\nIt may be that some variables could have some relation amongst themselves; \n\nIn other word, the variables may be highly collinear to each other. A simple way to detect collinearity is to look at the correlation matrix of the independent variables as shown.","f39f399c":"# Model Evaluation(Regression Metrics)\n\nThis is the more formal way to evaluate the performance of a linear regresion model rather than plotting error terms etc.\n\nThere are various metrics used to evaluate the performance of the model :\n\n* Mean Squared Error(MSE)\n* Root-Mean-Squared-Error(RMSE).\n* Mean-Absolute-Error(MAE).\n* R\u00b2 or Coefficient of Determination.\n* Adjusted R\u00b2\n\nLet's discuss them one by one!","b5b6d7f4":"Even though R squared is lesser than our earlier model, but this model may be a better choice because we have removed an insignificant variable.","770aeab4":"**Dataset we are going to use here has data related to some customers who have opted for an Insurance, we need to predict the Insurance Premium based on the dependent features. let's check the dataset first**","658f66fa":"\n**Let's check RSS(MSE) and TSS(MSE(Model Baseline) in more detail:**\n\n**RSS (Residual Sum of Squares):** In statistics, it is defined as the total sum of error across the whole sample.\n\nIt is the measure of the difference between the expected and the actual output. A small RSS indicates a tight fit of the model to the data. It is also defined as follows:\n\n![image.png](attachment:image.png)","e9c6a0f1":"### Bi-Variate Analysis\n#### Let's check how many smokers are male\/female","ad2605c1":"### Mean Squared Error: \n\n`MSE or Mean Squared Error` is one of the most preferred metrics for regression tasks. \n\nIt is simply the average of the squared difference between the target value and the value predicted by the regression model. \n\nAs it squares the differences, it penalizes even a small error which leads to over-estimation of how bad the model is. \n\nIt is preferred more than other metrics because it is differentiable and hence can be optimized better.\n\n<img src = \"https:\/\/miro.medium.com\/max\/875\/0*aTUPK_ILg7-n0znw.jpg\">","7621a20e":"By looking at the above numbers, we can say that the data is in good shape and does not require any pre-processing.\nWe will still check for outliers in a while!\n\nNow, lets check how numeric data is distributed!","ec0521cd":"### Why is R\u00b2 Negative?\n\nThere is a misconception among people that R\u00b2 score ranges from 0 to 1 but actually it ranges from -\u221e to 1. \n\nDue to this misconception, they are sometimes scared why the R\u00b2 is negative which is not a possibility according to them.\n\n**The main reasons for R\u00b2 to be negative are the following:**\n\n1) One of the main reason for R\u00b2 to be negative is that the chosen model does not follow the trend of the data causing the R\u00b2 to be negative. This causes the mse of the chosen model(numerator) to be more than the mse for constant baseline(denominator) resulting in negative R\u00b2.\n\n<img src=\"https:\/\/miro.medium.com\/max\/531\/0*fLHizx3eMNBiV1od.png\">\n\n2) Maybe there are a large number of outliers in the data that causes the mse of the model to be more than mse of the baseline causing the R\u00b2 to be negative(i.e the numerator is greater than the denominator).\n\n3) Sometimes while coding the regression algorithm, the researcher might forget to add the intercept to the regressor which will also lead to R\u00b2 being negative. \n\nThis is because, without the benefit of an intercept, the regression could do worse than the sample mean(baseline) in terms of tracking the dependent variable (i.e., the numerator could be greater than the denominator). \n\nHowever, most of the standard machine learning library like scikit-learn include the intercept by default but if you are using stats-model library then you have to add the intercept manually.\n\nReason why we got negative R-square above is, we delibiretly missed adding constant to X_test."}}