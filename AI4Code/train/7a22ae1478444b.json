{"cell_type":{"ff969b3b":"code","bdc37800":"code","a7fcef50":"code","ef6a150a":"code","aedfa875":"code","e5ce7b0b":"code","810ffd27":"code","995c969d":"code","191d5e03":"code","77f1c625":"code","c3e17668":"code","54705edc":"code","efe5d848":"code","9035d617":"code","1270894d":"code","45993401":"code","601e0c79":"code","6f92097a":"code","82e2a4ca":"code","7b810b19":"code","fadec105":"code","579aa9f6":"code","54c46c64":"code","032d8144":"code","a4799d59":"code","59c3e0b3":"code","5cc2d4ed":"code","0cd6d9f2":"code","4d3b65c5":"code","48461449":"code","a5095976":"code","223bf410":"code","5861b339":"code","cd9b9c29":"code","2cc54526":"code","cb668106":"code","092b6b96":"code","512a3869":"code","04fad5db":"code","d63844e4":"code","57b2520f":"code","d9752e52":"code","8463ec8b":"code","43884dc0":"markdown","b7a7ec37":"markdown","1194ebb2":"markdown","7d656ecd":"markdown","4bece95a":"markdown","30a65f59":"markdown","5de2a20d":"markdown","3251cd14":"markdown","7a7eeebf":"markdown","b2ffd9e8":"markdown","5915c888":"markdown","c4e446d8":"markdown","a10ed202":"markdown","ca9b4f44":"markdown","a30665cf":"markdown","0dc07340":"markdown","09fef07d":"markdown","59b00c70":"markdown","7b42465c":"markdown","e4cd9e82":"markdown","3f84208f":"markdown","83b82c9f":"markdown","961dd4b9":"markdown","40a41e6f":"markdown","9ef1edee":"markdown","8b384847":"markdown","52d2dc8e":"markdown","60c7b61f":"markdown","7ca9c1c1":"markdown","0cfc314a":"markdown","ecded6dc":"markdown","6c2bb5ba":"markdown","68328a09":"markdown","5b1853ac":"markdown","14034f5b":"markdown","d2658010":"markdown","ab680837":"markdown","bb390ece":"markdown","31ec9115":"markdown","df9ab12d":"markdown"},"source":{"ff969b3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bdc37800":"!pip install pmdarima","a7fcef50":"import warnings; warnings.filterwarnings('ignore')\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pylab\nfrom pylab import rcParams\n\n\nplt.style.use('dark_background')\nplt.rcParams['figure.figsize'] = 18,8\npd.set_option('display.max_columns', None)\n\n\nfrom sklearn.metrics import mean_squared_error\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport pmdarima as pm\n\n","ef6a150a":"df_holi = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\ndf_oil = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv')\ndf_stores = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ndf_trans = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv')\n\ndf_train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv')\ndf_test = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv')","aedfa875":"df_train1 = df_train.merge(df_holi, on = 'date', how='left')\ndf_train1 = df_train1.merge(df_oil, on = 'date', how='left')\ndf_train1 = df_train1.merge(df_stores, on = 'store_nbr', how='left')\ndf_train1 = df_train1.merge(df_trans, on = ['date', 'store_nbr'], how='left')\ndf_train1 = df_train1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ndf_train1['date'] = pd.to_datetime(df_train1['date'])\ndf_train1['year'] = df_train1['date'].dt.year\ndf_train1['month'] = df_train1['date'].dt.month\ndf_train1['week'] = df_train1['date'].dt.isocalendar().week\ndf_train1['quarter'] = df_train1['date'].dt.quarter\ndf_train1['day_of_week'] = df_train1['date'].dt.day_name()","e5ce7b0b":"df_train1.head(2)","810ffd27":"df = df_train1 #copying the original df into a temp df which we can work on,just incase we dont mess up with the original one","995c969d":"print(f'The merged dataframe has {df.shape[0]} rows and {df.shape[1]} columns')","191d5e03":"df.info()","77f1c625":"df.describe()","c3e17668":"print(f'The data is available from {df.date.max()} to {df.date.min()}')","54705edc":"df['date'] = df['date'].dt.floor('d')","efe5d848":"df = df[['date', 'sales']]","9035d617":"df = df.set_index(df.date)\ndf.drop('date', axis=1, inplace=True)","1270894d":"df.isna().sum()","45993401":"df = df.resample('M').mean()","601e0c79":"y = df['sales']\nfig, ax = plt.subplots(figsize=(18, 8))\nax.plot(y,marker='o', markersize=8, linestyle='-', label='Monthly Mean Resample', color='fuchsia')\nax.set_ylabel('sales')\nax.set_title('Average sales per month')\nax.set_xlabel('years')\nax.grid(axis='x')\nax.legend();","6f92097a":"df_dec=seasonal_decompose(df,model='additive', extrapolate_trend='freq')\ndf_dec.plot();","82e2a4ca":"sales_mean = df.sales.rolling(window=12).mean()\nsales_std = df.sales.rolling(window=12).std()","7b810b19":"fig, ax = plt.subplots(figsize=(18, 8))\nax.plot(df.sales,marker='x', markersize=8, label='Sales',color='cyan')\nax.plot(sales_mean,marker='o', markersize=4, label='rolling mean', color='red');\nax.plot(sales_std,marker='*', markersize=4, label='rolling std', color='royalblue');\nax.legend()\nplt.xlabel('Years')\nplt.ylabel('Avg sales')\nplt.title('Check of stationarity')\nax.grid(axis='x')","fadec105":"dftest = adfuller(df.dropna(), autolag='AIC')\nprint('Test statistic = {:.3f}'.format(dftest[0]))\nprint('P-value = {:.3f}'.format(dftest[1]))\nprint('Critical values :')\nfor k, v in dftest[4].items():\n    print('\\t{}: {} - The data is {} stationary with {}% confidence'.format(k, v, 'not' if v<dftest[0] else '', 100-int(k[:-1])))","579aa9f6":"df_lag12 = df.sales - df.sales.shift(12)","54c46c64":"df_lag12.tail(5)","032d8144":"y = df_lag12\nfig, ax = plt.subplots(figsize=(18, 8))\nax.plot(y,marker='o', markersize=8, linestyle='-', label='Monthly Mean Resample', color='fuchsia')\nax.set_ylabel('sales')\nax.set_title('Average sales per month')\nax.set_xlabel('years')\nax.grid(axis='x')\nax.legend();","a4799d59":"dftest = adfuller(df_lag12.dropna(), autolag='AIC')\nprint('Test statistic = {:.3f}'.format(dftest[0]))\nprint('P-value = {:.3f}'.format(dftest[1]))\nprint('Critical values :')\nfor k, v in dftest[4].items():\n    print('\\t{}: {} - The data is {} stationary with {}% confidence'.format(k, v, 'not' if v<dftest[0] else '', 100-int(k[:-1])))","59c3e0b3":"df_dec_lag12 =seasonal_decompose(df_lag12.dropna(),model='additive', extrapolate_trend='freq')\ndf_dec.plot();","5cc2d4ed":"df['shifted_sales'] = df.sales.shift(1)","0cd6d9f2":"df.head(4)","4d3b65c5":"fig, ax = plt.subplots(figsize=(18, 8))\nax.plot(df.sales,marker='*', markersize=8, label='Sales',color='white')\nax.plot(df.shifted_sales,marker='o', markersize=4, label='Shifted Sales', color='cyan');\nplt.xlabel('Year')\nplt.ylabel('avg sales')\nplt.title('Naive model')\nax.legend()\nplt.show()","48461449":"df = df.dropna()","a5095976":"mse = mean_squared_error(df.sales, df.shifted_sales)\n\nrmse = np.sqrt(mse)\n\nprint(f'The root mean square error of the naive model is : {rmse}')","223bf410":"df = df.drop('shifted_sales',axis=1) # 'shifter_sales' not needed for holt-winters model","5861b339":"hw = ExponentialSmoothing(df, seasonal_periods=12,trend='add', seasonal='add').fit()","cd9b9c29":"hw_preds = hw.forecast(steps=24)","2cc54526":"fig, ax = plt.subplots(figsize=(18, 8))\nax.plot(df,marker='*', markersize=8, label='Sales',color='fuchsia')\nax.plot(hw_preds,marker='*', markersize=8, label='Forecasted sales', color='orange');\nplt.xlabel('Year')\nplt.ylabel('avg sales')\nplt.title('Holt-Winters model' )\nax.grid(axis='x')\nax.legend()\nplt.show()","cb668106":"hw_resid = hw.resid\nhw_resid_sqr = np.square(hw_resid)\nhw_resid_mse = hw_resid_sqr.sum()\/len(hw_resid_sqr)\nhw_resid_rmse = np.sqrt(hw_resid_mse)\n\n\nprint(f'The rmse value of Holt-Winters model is {hw_resid_rmse}')","092b6b96":"plot_acf(df);","512a3869":"plot_pacf(df);","04fad5db":"dftrain = df.iloc[:38]\ndftest = df.iloc[38:]","d63844e4":"arima_mod = ARIMA(dftrain, order=(5,1,3)) #p=5, d=1, q=3\narima_mod= arima_mod.fit(disp=0)\narima_preds = arima_mod.forecast(steps = 17)","57b2520f":"mse = mean_squared_error(dftest, arima_preds[0])\nprint(f'The root mean square error of ARIMA with (5,1,3) is {np.sqrt(mse)}')","d9752e52":"arima_preds = pd.Series(arima_preds[0], index=dftest.index)","8463ec8b":"fig, ax = plt.subplots(figsize=(18, 8))\nax.plot(dftest,marker='*', markersize=8, label='Test sales', color='fuchsia');\nax.plot(arima_preds,marker='*', markersize=8, label='Arima_forecasting', color='grey');\nplt.xlabel('Year')\nplt.ylabel('avg sales')\nplt.title('ARIMA' )\nax.grid(axis='x')\nax.legend()\nplt.show()","43884dc0":"*Now ARIMA takes in 3 components to work and hose are*\n* Auto-Regressive (p)\n* Integrated (differencing, d)\n* Moving Average (q)\n\n*And these values has to be found before giving it to the model, it can be done using **ACF** and **PACF** plots which will give the right values for the p and q values respectively*\n   \n*We can even use grid-seach technique to find the best values of p, q and d inorder to have least AIC value (AIC is Akike Information Criteria, it is a measure of the goodness of the time series model, lower the value of AIC better the model is)*\n","b7a7ec37":"# DECOMPOSE THE DATA","1194ebb2":"*The augmented Dickey-Fuller test is a statistical test which is used to check for the stationarity of a time series, this test gives a p-value, test-statistic value and the critical values with confidence intervals stating the stationarity and based on the p-value obtained by the adf test we can reject or accpet the null hypothesis.*\n* **H<sub>0<\/sub>  : 'The time series is not stationary'**\n* **H<sub>a<\/sub>  : 'The time series is stationary'**","7d656ecd":"\n*There are no missing values in our series, that means that the data is present for all of 2013 to 2017*\n*The data is captured on daily basis, let's resample it to monthly basis to make analysis easier*","4bece95a":"\n\n*Looking at the figure above, by the eye-judgement we can see an upward trend in the avg sales and there seems to be some seasonality, but later on in this notebook we will verify these attributes*\n","30a65f59":"*The most two common methods used to check for stationarity are*\n* ***Visualization***\n* ***Augmented Dickey-Fuller test (ADF test)*** ","5de2a20d":"# **MAKING THE DATA STATIONARY**","3251cd14":"*Looking at the four components above, we can say that the our avg sales per month has an upward trend overall and there is seasonality throughout, based on these factors, we can use suitable time-series forecasting model later*","7a7eeebf":"*For the next step we have to check for the stationarity of the trend, a series is said to be stationary when*\n* ***The mean doesnt change over time***\n* ***The variance of the series doesnt change over time***\n\n*and we have to make our data stationary if it is not stationary, because the time-series forecasting models make use of the assumption that there is no change in mean or variance in the data*","b2ffd9e8":"\n*Chesking if there are any missing values in our series, if there is any missing values then our time series analysis wont work*","5915c888":"**PLOTTING ACF AND PACF PLOT**","c4e446d8":"# **CHECK FOR STATIONARITY**","a10ed202":"2. **Augmented Dickey-Fuller test (ADF test)**","ca9b4f44":"*The naive forecasting model gave us a root mean square error of **57**, that means on an average a monthly sales prediction value might differ by a value of <u>+<\/u> ***57*** from the actual prediction*","a30665cf":"\n*For time-series analysis, let's only consider the 'data' and the 'sales' columns*","0dc07340":"# **ARIMA (Auto-Regressive Integrated Moving Average)**","09fef07d":"1. **DIFFERENCING**","59b00c70":"# **NAIVE MODEL IN TIME-SERIES FORECASTING**","7b42465c":"*Let's us check the time period of the data which is available*","e4cd9e82":"*for simplicity lets convert the date column values with only dates discarding the info about the hours, minutes and seconds*","3f84208f":"# **VISUALIZE THE DATA**","83b82c9f":"# **IMPORTING ESSENTIALS**","961dd4b9":"*As expected from the figure and also tested by the adf test for stationarity, we can confidently say that the original data is not stationary*","40a41e6f":"*On comparing the Naie model, Holt-winter's model and ARIMA model, we can make out that the Holt-Winter's model performs well compared to all other models*","9ef1edee":"*The shaded region is the critical value and the spikes is the correlation value between the 1st value and the next corresponding values, the spike which is in the shaded region is the optimal value of q*","8b384847":"# **READ AND MERGE THE DATA INTO A SINGLE DATAFRAME**","52d2dc8e":"**NOTE :**\n*The 'sales' column had many outliers, instead of treating them seperately, we can make use of 'median' while resampling instead of 'mean' to neutralize the effects of outliers,*\n*Now the above series is an account of the almost average sales per month from 2013 to aug-2017*","60c7b61f":"# EDA","7ca9c1c1":"*Differencing is a method of inducing a lag in the data which will remove the seasonal or cyclical patterns from the data*","0cfc314a":"*Now that the data is stationary, we can go ahead and build a forecasting model*\n*There are many forecasting models in time-series forecasting but depending on the data components we can pick the suitable model and build it*\n\n* **Exponential Smoothing model is best for data without trend or seasonality**\n* **Holt\u2019s Method for data with a trend but no seasonality**\n* **Holt-Winters for data with trend and\/or seasonality**\n* **ARIMA for data with trend and\/or seasonality**\n","ecded6dc":"**HOLT-WINTER's FORECASTING MODEL FTW**","6c2bb5ba":"# **BUILDING A FORECASTING MODEL**","68328a09":"*Now that we know about the stationarity of the data, we have to make it stationary, there are many ways to make a series stationary viz.*\n* **Differencing**\n* **Rolling window**\n* **Transformation**","5b1853ac":"# **HOLT-WINTER'S MODEL FOR FORECASTING**","14034f5b":"1. **Visualization**","d2658010":"*By looking at the graph of sales data above, we can see a general increasing trend with no clear pattern of seasonal or cyclical changes. The next step is to decompose the data to view more of the complexity behind the linear visualization, once decomposed, we can clearly get :*\n* *Observed (actual series)*\n* *Trend*\n* *Seasonality*\n* *Residuals (irrgularities)*","ab680837":"*In the above figure, we are smoothening the series to remove the abrupt changes of the original data which can be helpful to judge the smoothened mean and std line of the data.*\n*We can see that the mean of the data is not same through out but where as the standard deviation is almost constant.*\n\n*We cannot confidently conclude that the data is stationary just by seeing the above insights, lets use adf test to check for the stationarity of the data next*","bb390ece":"*The shaded region is the critical value and the spikes is the correlation value between the 1st value and the next corresponding values, the spike which is in the shaded region is the optimal value of p*","31ec9115":"<h2 style=\"color:fuchsia;font-size:55px;font-family:Space mono;text-align:center;\"><strong>Time Series Analysis \ud83d\udcc8\ud83d\udcc8  <\/strong><\/h2> \n","df9ab12d":"*Comparing the rmse values of the Naive model and the holt-winters model, holt-winters model performs better with lesser rmse value, let's check with the final model  i.e **ARIMA** and compare how it performs other than these two*"}}