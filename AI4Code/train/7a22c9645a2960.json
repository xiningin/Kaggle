{"cell_type":{"5ee95824":"code","89872680":"code","fba02a23":"code","e13bb3c2":"code","f545f1a9":"code","e5d132bc":"code","0d631c7c":"code","c4f7cb80":"code","a68d0e06":"code","eaca44f1":"code","373c08e3":"code","05358980":"code","93233b43":"code","bc1832e9":"code","4e96c365":"code","286a357d":"code","042f1514":"code","66f8711b":"code","317f6a09":"markdown","9ea7bebd":"markdown","8bf85d44":"markdown","6de97003":"markdown","a0887081":"markdown","aaa40266":"markdown","2c337642":"markdown","da0a203c":"markdown","51283bcc":"markdown","e74ccf08":"markdown","12b1661d":"markdown","669f1353":"markdown","6f02c69d":"markdown","0ed5a2b7":"markdown","401c3aaf":"markdown","d88ee79f":"markdown","787e833f":"markdown"},"source":{"5ee95824":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will \n#list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as \n#output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89872680":"#Load the training and the test data\ntrain_filepath = \"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest_filepath = \"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\"\ntrain_data = pd.read_csv(train_filepath)\ntest_data = pd.read_csv(test_filepath)","fba02a23":"#Display the feautres of train and test datasets\nprint(train_data.info())","e13bb3c2":"#Display few lines of the train and the test datasets\nprint(train_data.head())\n#print(test_data.head())","f545f1a9":"#Summarize the datasets of training and test datasets\nprint(train_data.describe())\nprint(test_data.describe())","e5d132bc":"#Lets visulaize the variation of target variable in the train data\nimport matplotlib.pyplot as plt\ntrain_data['claim'].value_counts().plot.bar()\nplt.xlabel('claim')\nplt.ylabel('frequency')\nplt.title('Distribution of claim paid and unpaid')","0d631c7c":"print(\"Train:\")\nprint(train_data.isnull().sum())\nprint()\n\nprint(\"Test:\")\nprint(test_data.isnull().sum())\nprint()","c4f7cb80":"trainData = train_data.dropna(axis=0)\ntestData = test_data.dropna(axis = 0)","a68d0e06":"print(\"Modified Train:\")\nprint(trainData.isnull().sum())\nprint()\n\nprint(\"Modified Test:\")\nprint(testData.isnull().sum())\nprint()","eaca44f1":"import numpy as np\nprint(np.shape(trainData))\nprint(np.shape(testData))","373c08e3":"#We modify the training data set\ny = train_data['claim']\nX = train_data.drop(\"claim\", axis=1)","05358980":"#Split the data into train and validation data\nfrom sklearn.model_selection import train_test_split\nx_train, x_valid, y_train, y_valid = train_test_split(X,y,test_size=0.2, random_state=0)","93233b43":"#from xgboost import XGBRegressor\n#from sklearn.metrics import mean_absolute_error\n#from sklearn.metrics import mean_squared_error\n\n#XGB_modelR = XGBRegressor(base_score=0.5, booster='gbtree', eval_metric='mlogloss',\n #             gamma=0, tree_method = 'gpu_hist', gpu_id=-1,learning_rate=0.01,\n  #            max_delta_step=0, max_depth=6, objective = 'reg:squarederror', n_estimators=60, random_state=0,\n  #            reg_alpha=0, reg_lambda=1, use_label_encoder=False,\n   #           validate_parameters=1,verbosity=None) # \n#XGB_modelR.fit(x_train, y_train)\n#y_predict4 = XGB_modelR.predict(x_valid)\n#print(\"Mean Absolute Error\", mean_absolute_error(y_valid, y_predict4))\n#print(\"Mean Squared Error\", mean_squared_error(y_valid, y_predict4))","bc1832e9":"#Using Model 2: XGBoost \n#y_testXGB_R = XGB_modelR.predict(test_data)#.drop(\"id\", axis = 1))\n#print(\"predicted values:\", y_testXGB_R)\n\n#import matplotlib.pyplot as plt\n#plt.hist(y_testXGB_R, 20, ec = 'black')","4e96c365":"#Build the light LGBM model\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nmodel_LGB = lgb.LGBMRegressor(boosting_type='gbdt',device = \"gpu\", num_leaves= 31, max_depth=20,\n                  learning_rate=0.1, objective= 'regression', n_estimators=60, n_jobs=-1,\n                              subsample_for_bin=200000)\n\nmodel_LGB.fit(x_train, y_train)\ny_predict_lgb = model_LGB.predict(x_valid)\nprint(\"Mean Absolute Error\", mean_absolute_error(y_valid, y_predict_lgb))\nprint(\"Mean Squared Error\", mean_squared_error(y_valid, y_predict_lgb))","286a357d":"#Using Model 3: LGBM \ny_test_lgb= model_LGB.predict(test_data)\nprint(\"predicted values:\", y_test_lgb)\n\nimport matplotlib.pyplot as plt\nplt.hist(y_test_lgb, 100, ec = 'black')\nplt.xlabel('predicted claim')\nplt.ylabel('count')\nplt.show()","042f1514":"print(\"predicted values:\", y_test_lgb)","66f8711b":"my_submission = pd.DataFrame({'id': test_data.id, 'claim': y_test_lgb})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","317f6a09":"**From the bar plots it is apparent that the there is an equal number of claims that are paid and unpaid.**<br>\nThe input variables are of unkonown type, therefore in order to viualize them it is better to see correlation between these variables.","9ea7bebd":"Lets now check the modified set of data\n","8bf85d44":"**NB:**<br>\nTo expedite the program I use gpu server, and I used cuML <br>\nThe API for the cuML is in the given reference <br>\nhttps:\/\/docs.rapids.ai\/api\/cuml\/stable\/api.html#logistic-regression","6de97003":"Therefore, there are in total 118 variables with dtype float, while the id and claim is integer.","a0887081":"##### Lets play with the training data set \"trainData\"","aaa40266":"## 4. Data Preprocessing\n\nSegregate the target and train variables from the training data and drop all the rows with null values.","2c337642":"## 3. Check for the missing variables\n\nHere we will check the values in the columns, and in particular check the missing value, or catgorical variables.","da0a203c":"### Description about the data\nThere is a set of variables F = {f1, f2, f3, ...f118} and the target variable \"claim\". The value of a target variable is digital i.e., it's value is 0 or 1. It is a decisional statement implying that whether the claim will be made or not. The claim = 0, implies that it will not be paid while the claim = 1, implies, that the claim will be made.\n\n**Our job is to predict the claim for the given set of {f1, f2, f3, ...f118}**","51283bcc":"From the above set of outputs, it is evident that there are significant number of null values in both training and test data set i,e., several values are missing. Therefore, we will **preprocess the data** to remove all the rows with missing values. \n##### Lets just take few columns\/rows for out analysis","e74ccf08":"## 7. Prepare the submission file","12b1661d":"## 1. Loading the Data","669f1353":"## 5. Model Verification","6f02c69d":"## 6. Prediction ","0ed5a2b7":"## 2. Data Visualization","401c3aaf":"Now all the null values are removed in traing and test dataset","d88ee79f":"#### Split the data into train and validation data","787e833f":"#### Model 2: XGBoost Rgressor"}}