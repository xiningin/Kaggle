{"cell_type":{"a08daf29":"code","059b826c":"code","e2aaaf17":"code","718a8926":"code","3664161f":"code","a7be4b25":"code","8754b01d":"code","02e39bdd":"code","baf10839":"code","4d0275f1":"code","ce31930f":"code","99722b04":"code","2d538cc1":"code","0b6a1161":"code","c71a4363":"code","dfe58afc":"code","45e56646":"code","fe1be792":"code","b887bb45":"code","8601ac7a":"code","c95776cd":"code","1396f776":"code","dcdb2ae8":"code","1bd6e0ea":"code","fc5f2c6a":"code","9bff5c5d":"code","8cfd5847":"code","4de2476a":"code","488fb104":"code","102d2390":"code","2324e7af":"code","59acf045":"code","d39d5aae":"code","64271ff6":"code","f9a5f704":"code","17eecfb8":"code","10bc5c58":"code","df6ddafa":"code","293ff10b":"code","068392a0":"code","30b169e8":"code","650c1433":"code","122473de":"code","5e68d71e":"code","ddae8aa8":"code","f0ecc50c":"code","2f095e4f":"code","c654c063":"code","92fe07e4":"code","c1ce81af":"code","08011ebb":"code","b1c9adab":"code","ffde769b":"code","83ff892c":"markdown","aafa5293":"markdown","4c632758":"markdown","bd4fce74":"markdown","7b0047e9":"markdown","3e75950c":"markdown","2561ef48":"markdown","5b0cf3b5":"markdown","6eeeacba":"markdown","c7c06a74":"markdown","ca810b24":"markdown","d9707dfd":"markdown","14d52903":"markdown","a06d70c8":"markdown","f9aec431":"markdown","12d7789f":"markdown","bed59a95":"markdown","d237634c":"markdown","e069e343":"markdown","192f26e1":"markdown","b58f2433":"markdown","12d0485f":"markdown","f9fc1c09":"markdown","eb85fdd2":"markdown","4f01a48b":"markdown","c1b9ce83":"markdown","92909d97":"markdown","03bb93de":"markdown","10e87e04":"markdown","5e02f11c":"markdown","5daf2298":"markdown","824c17eb":"markdown","f7b39b79":"markdown","f0f1dc58":"markdown","71624cfc":"markdown","cf5fe7b7":"markdown","6038ef8b":"markdown","f26df253":"markdown","287d7829":"markdown","944ad8d7":"markdown","21d01b5a":"markdown","2d2b45eb":"markdown","471e9307":"markdown","2d695f06":"markdown","c1d9bc7e":"markdown","1a3b5b76":"markdown","5a5f7bcd":"markdown","54443dcc":"markdown","615f21be":"markdown"},"source":{"a08daf29":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.datasets import load_boston\nfrom sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Lasso, Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression","059b826c":"boston_dataset = load_boston()","e2aaaf17":"#create a dataframe of the boston datasets\ndf_boston = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)\ndf_boston['MEDV'] = boston_dataset.target\ndf_boston.head()","718a8926":"df_boston.columns","3664161f":"null = df_boston[df_boston.columns[df_boston.isnull().any()]]\nnull = list(null)\nprint('There are', len(null), 'columns with null values')","a7be4b25":"X = df_boston.drop('MEDV', axis = 1)\nY = df_boston['MEDV']\n\nfor k in np.arange(1, 6, 1):\n    X_new = SelectKBest(score_func = f_regression, k = k)\n    X_new.fit_transform(X, Y)\n    features = X.columns[X_new.get_support()]\n    print('---------------For k =', k, '---------------')\n    display(features)","8754b01d":"corr = df_boston.corr()\ncorr = corr['MEDV'].abs()\ncorr.sort_values(ascending = False)","02e39bdd":"df_boston_pred = df_boston[['LSTAT', 'RM', 'PTRATIO', 'MEDV']]","baf10839":"plt.figure()\nsns.pairplot(data = df_boston_pred)\nplt.show()","4d0275f1":"print('LSTA skewness:', df_boston_pred['LSTAT'].skew())\nprint('RM skewness:', df_boston_pred['RM'].skew())\nprint('PTRATIO skewness:', df_boston_pred['PTRATIO'].skew())\nprint('MEDV skewness:', df_boston_pred['MEDV'].skew())","ce31930f":"#LSTAT\nq25, q50, q75 = np.percentile(df_boston_pred['LSTAT'], [25, 50, 75])\n#get the interquartile\niqr = q75 - q25\n#detect the min and max that will be considered has the limit of the outliers\nmini = q25 - 1.5 * iqr\nmaxi = q75 + 1.5 * iqr\nprint('The minimum limit is', mini)\nprint ('The maximum limit is', maxi)\n\n#boxplot\nfig = plt.subplots(figsize = (10,7))\nsns.boxplot(data = df_boston_pred, x = 'LSTAT')\nplt.show()","99722b04":"#RM\nq25, q50, q75 = np.percentile(df_boston_pred['RM'], [25, 50, 75])\n#get the interquartile\niqr = q75 - q25\n#detect the min and max that will be considered has the limit of the outliers\nmini = q25 - 1.5 * iqr\nmaxi = q75 + 1.5 * iqr\nprint('The minimum limit is', mini)\nprint ('The maximum limit is', maxi)\n\n#boxplot\nfig = plt.subplots(figsize = (10,7))\nsns.boxplot(data = df_boston_pred, x = 'LSTAT')\nplt.show()","2d538cc1":"#PTRATIO\nq25, q50, q75 = np.percentile(df_boston_pred['PTRATIO'], [25, 50, 75])\n#get the interquartile\niqr = q75 - q25\n#detect the min and max that will be considered has the limit of the outliers\nmini = q25 - 1.5 * iqr\nmaxi = q75 + 1.5 * iqr\nprint('The minimum limit is', mini)\nprint ('The maximum limit is', maxi)\n\n#boxplot\nfig = plt.subplots(figsize = (10,7))\nsns.boxplot(data = df_boston_pred, x = 'LSTAT')\nplt.show()","0b6a1161":"skew = df_boston_pred.skew()\nskew = skew.abs()\nprint('The absolute skewed values are:')\ndisplay(skew)","c71a4363":"df_boston_pred[['LSTAT', 'PTRATIO', 'MEDV']] = np.log1p(df_boston_pred[['LSTAT', 'PTRATIO', 'MEDV']])","dfe58afc":"X = df_boston_pred.drop('MEDV', axis = 1)\ny = df_boston_pred['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","45e56646":"X_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","fe1be792":"lr = LinearRegression()\nlr.fit(X_train, y_train)","b887bb45":"y_test_predicted = lr.predict(X_test)\ny_train_predicted = lr.predict(X_train)","8601ac7a":"MSE_test_lr = mean_squared_error(y_test, y_test_predicted)\nRMSE_test_lr = np.sqrt(MSE_test_lr)\nRS_test_lr = lr.score(X_test, y_test)\nprint('----------------------Test set----------------------')\nprint('This is the RMSE:', RMSE_test_lr)\nprint('This is the RS:', RS_test_lr)\n\nMSE_train_lr = mean_squared_error(y_train, y_train_predicted)\nRMSE_train_lr = np.sqrt(MSE_train_lr)\nRS_train_lr = lr.score(X_train, y_train)\nprint('----------------------Train set----------------------')\nprint('This is the RMSE:', RMSE_train_lr)\nprint('This is the RS:', RS_train_lr)","c95776cd":"#plot predictions\nplt.figure(figsize = (10,7))\nplt.scatter(y_train_predicted, y_train, label = 'Training data')\nplt.scatter(y_test_predicted, y_test, label = 'Validation data')\nplt.legend()\nplt.show()","1396f776":"#create the function that runs the polynomial preprocessing\ndef poly_regr(degree, model):\n    #choose the degree of the polynomial equation\n    poly_features = PolynomialFeatures(degree = degree)\n    #transform the X_train in a polynomial feature\n    X_train_poly = poly_features.fit_transform(X_train)\n    X_test_poly = poly_features.fit_transform(X_test)\n    #instantiate the model\n    poly = model\n    #fit the model\n    poly.fit(X_train_poly, y_train)\n    y_test_predicted_poly = poly.predict(X_test_poly)\n    y_train_predicted_poly = poly.predict(X_train_poly)\n    #evaluating the model\n    MSE_test = mean_squared_error(y_test, y_test_predicted_poly)\n    RMSE_test = np.sqrt(MSE_test)\n    RS_test = poly.score(X_test_poly, y_test)\n    print('------Test set')\n    print('The R-squared score is:', RS_test)\n    print('The Root Mean Squared Error is:', RMSE_test)\n    MSE_train = mean_squared_error(y_train, y_train_predicted_poly)\n    RMSE_train = np.sqrt(MSE_train)\n    RS_train = poly.score(X_train_poly, y_train)\n    print('------Train set')\n    print('The R-squared score is:', RS_train)\n    print('The Root Mean Squared Error is:', RMSE_train)","dcdb2ae8":"for degree in range(1,7):\n    print('-----------------Order', degree, '-----------------')\n    poly_regr(degree, LinearRegression())","1bd6e0ea":"poly_features = PolynomialFeatures(degree = 3)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\npoly = LinearRegression()\npoly.fit(X_train_poly, y_train)\ny_test_predicted_poly = poly.predict(X_test_poly)\ny_train_predicted_poly = poly.predict(X_train_poly)\nMSE_test_plr = mean_squared_error(y_test, y_test_predicted_poly)\nRMSE_test_plr = np.sqrt(MSE_test_plr)\nRS_test_plr = poly.score(X_test_poly, y_test)\n\n#plot predictions\nplt.figure(figsize = (10,7))\nplt.scatter(y_train_predicted_poly, y_train, label = 'Training data')\nplt.scatter(y_test_predicted_poly, y_test, label = 'Validation data')\nplt.legend()\nplt.show()","fc5f2c6a":"rr = RidgeCV(alphas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30])\nrr.fit(X_train, y_train)\nalpha_rr = rr.alpha_\nprint('The Best alpha is', alpha_rr)","9bff5c5d":"rr = RidgeCV(alphas = [alpha_rr * 0.6, alpha_rr * .65, alpha_rr * .7, alpha_rr * .75, alpha_rr * .8, alpha_rr * .85, \n                          alpha_rr * .9, alpha_rr * .95, alpha_rr, alpha_rr * 1.05, alpha_rr * 1.1, alpha_rr * 1.15,\n                          alpha_rr * 1.25, alpha_rr * 1.3, alpha_rr * 1.35, alpha_rr * 1.4], cv = 10)\nrr.fit(X_train, y_train)\nalpha_rr = rr.alpha_\nprint('The Best alpha is', alpha_rr)","8cfd5847":"for degree in range(1,7):\n    print('-----------------Order', degree, '-----------------')\n    poly_regr(degree, Ridge(alpha = 1.4))","4de2476a":"poly_features = PolynomialFeatures(degree = 5)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\npoly_rr = Ridge(alpha = 1.4)\npoly_rr.fit(X_train_poly, y_train)\ny_test_predicted_rr = poly_rr.predict(X_test_poly)\ny_train_predicted_rr = poly_rr.predict(X_train_poly)\nMSE_test_rr = mean_squared_error(y_test, y_test_predicted_rr)\nRMSE_test_rr = np.sqrt(MSE_test_rr)\nRS_test_rr = poly_rr.score(X_test_poly, y_test)\n\n#plot predictions\nplt.figure(figsize = (10,7))\nplt.scatter(y_train_predicted_rr, y_train, label = 'Training data')\nplt.scatter(y_test_predicted_rr, y_test, label = 'Validation data')\nplt.legend()\nplt.show()","488fb104":"lar = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], max_iter = 50000, cv = 10)\nlar.fit(X_train, y_train)\nalpha_lar = lar.alpha_\nprint('The Best alpha is:', alpha_lar)","102d2390":"lar = LassoCV(alphas = [alpha_lar * 0.6, alpha_lar * .65, alpha_lar * .7, alpha_lar * .75, alpha_lar * .8, alpha_lar * .85, \n                          alpha_lar * .9, alpha_lar * .95, alpha_lar, alpha_lar * 1.05, alpha_lar * 1.1, alpha_lar * 1.15,\n                          alpha_lar * 1.25, alpha_lar * 1.3, alpha_lar * 1.35, alpha_lar * 1.4],\n              max_iter = 50000, cv = 10)\nlar.fit(X_train, y_train)\nalpha_lar = lar.alpha_\nprint('The Best alpha is', alpha_lar)","2324e7af":"for degree in range(1,7):\n    print('-----------------Order', degree, '-----------------')\n    poly_regr(degree, Lasso(alpha = 0.00042, max_iter = 50000))","59acf045":"poly_features = PolynomialFeatures(degree = 5)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.fit_transform(X_test)\npoly_lar = Lasso(alpha = 0.00042, max_iter = 50000)\npoly_lar.fit(X_train_poly, y_train)\ny_test_predicted_lar = poly_lar.predict(X_test_poly)\ny_train_predicted_lar = poly_lar.predict(X_train_poly)\nMSE_test_lar = mean_squared_error(y_test, y_test_predicted_lar)\nRMSE_test_lar = np.sqrt(MSE_test_lar)\nRS_test_lar = poly_lar.score(X_test_poly, y_test)\n\n#plot predictions\nplt.figure(figsize = (10,7))\nplt.scatter(y_train_predicted_lar, y_train, label = 'Training data')\nplt.scatter(y_test_predicted_lar, y_test, label = 'Validation data')\nplt.legend()\nplt.show()","d39d5aae":"models_comp = pd.DataFrame({\n   'Model': ['Linear Regression', 'LR poly preprocessing', 'Ridge poly preprocessing', 'Lasso poly preprocessing'],\n    'Accuracy score': [RS_test_lr, RS_test_plr, RS_test_rr, RS_test_lar], \n    'RMSE score': [RMSE_test_lr, RMSE_test_plr, RMSE_test_rr, RMSE_test_lar],\n})\ndisplay(models_comp.sort_values(by = 'RMSE score', ascending = True))","64271ff6":"#define the skewness\nskew = df_boston.skew()\nskew = skew.abs()\nprint('The absolute skewed values are:')\ndisplay(skew.sort_values(ascending = False))","f9a5f704":"#skew below 0.5 are transformed --> log\ndf_boston[['CRIM', 'CHAS', 'B', 'ZN', 'DIS', 'RAD', 'LSTAT', 'PTRATIO', 'NOX', 'TAX', 'AGE', 'MEDV']] = np.log1p(df_boston[['CRIM', 'CHAS', 'B', 'ZN', 'DIS', 'RAD', 'LSTAT', 'PTRATIO', 'NOX', 'TAX', 'AGE', 'MEDV']])","17eecfb8":"X = df_boston.drop('MEDV', axis = 1)\ny = df_boston['MEDV']\n\nfrom sklearn.feature_selection import RFECV\n\ndef rfecv(estimator):\n    rfecv = RFECV(estimator = estimator, step = 1, cv = 10)\n    rfecv.fit(X, y)\n    print(\"Optimal number of features: %d\" % rfecv.n_features_)\n    print('Selected features:', list(X.columns[rfecv.support_]))","10bc5c58":"model = [LinearRegression(), RandomForestRegressor(), GradientBoostingRegressor()]\nfor estimator in model:\n    print('-------------------', estimator, '-------------------')\n    rfecv(estimator)","df6ddafa":"rfecv = RFECV(estimator = LinearRegression(), step = 1, cv = 10)\nrfecv.fit(X, y)\ncols = list(X.columns[rfecv.support_])\ndf_lr = df_boston[cols]\ndf_lr.columns","293ff10b":"X = df_lr\ny = df_boston['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","068392a0":"X_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","30b169e8":"lr_2 = LinearRegression()\nlr_2.fit(X_train, y_train)\ny_test_predicted_2 = lr_2.predict(X_test)\ny_train_predicted_2 = lr_2.predict(X_train)\nMSE_test_lr_2 = mean_squared_error(y_test, y_test_predicted_2)\nRMSE_test_lr_2 = np.sqrt(MSE_test_lr_2)\nRS_test_lr_2 = lr_2.score(X_test, y_test)\nprint('----------------------Test set----------------------')\nprint('This is the RMSE:', RMSE_test_lr_2)\nprint('This is the RS:', RS_test_lr_2)\n\nMSE_train_lr_2 = mean_squared_error(y_train, y_train_predicted_2)\nRMSE_train_lr_2 = np.sqrt(MSE_train_lr_2)\nRS_train_lr_2 = lr_2.score(X_train, y_train)\nprint('----------------------Train set----------------------')\nprint('This is the RMSE:', RMSE_train_lr_2)\nprint('This is the RS:', RS_train_lr_2)\n","650c1433":"models_comp = pd.DataFrame({\n   'Model': ['LR 3 features (method 1)', 'LR 6 features (method 2)'],\n    'Accuracy score': [RS_test_lr,RS_test_lr_2], \n    'RMSE score': [RMSE_test_lr,RMSE_test_lr_2],\n})\ndisplay(models_comp.sort_values(by = 'RMSE score', ascending = True))","122473de":"X = df_boston.drop('MEDV', axis = 1)\ny = df_boston['MEDV']\n\nrfecv = RFECV(estimator = RandomForestRegressor(), step = 1, cv = 10)\nrfecv.fit(X, y)\ncols = list(X.columns[rfecv.support_])","5e68d71e":"#using the rfecv, the new dataframe is:\ndf_rf = df_boston[cols]\ndf_rf.columns","ddae8aa8":"X = df_rf\ny = df_boston['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","f0ecc50c":"X_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","2f095e4f":"rf = RandomForestRegressor(max_depth = 10)\nrf.fit(X_train, y_train)\ny_test_predicted_rf = rf.predict(X_test)\ny_train_predicted_rf = rf.predict(X_train)\nMSE_test_rf = mean_squared_error(y_test, y_test_predicted_rf)\nRMSE_test_rf = np.sqrt(MSE_test_rf)\nRS_test_rf = rf.score(X_test, y_test)\nprint('----------------------Test set----------------------')\nprint('This is the RMSE:', RMSE_test_rf)\nprint('This is the RS:', RS_test_rf)\n\nMSE_train_rf = mean_squared_error(y_train, y_train_predicted_rf)\nRMSE_train_rf = np.sqrt(MSE_train_rf)\nRS_train_rf = rf.score(X_train, y_train)\nprint('----------------------Train set----------------------')\nprint('This is the RMSE:', RMSE_train_rf)\nprint('This is the RS:', RS_train_rf)","c654c063":"X = df_boston.drop('MEDV', axis = 1)\ny = df_boston['MEDV']\n\nrfecv = RFECV(estimator = GradientBoostingRegressor(), step = 1, cv = 10)\nrfecv.fit(X, y)\ncols = list(X.columns[rfecv.support_])","92fe07e4":"#using the rfecv, the new dataframe is:\ndf_gb = df_boston[cols]\ndf_gb.columns","c1ce81af":"X = df_gb\ny = df_boston['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","08011ebb":"X_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","b1c9adab":"gb = GradientBoostingRegressor()\ngb.fit(X_train, y_train)\ny_test_predicted_gb = gb.predict(X_test)\ny_train_predicted_gb = gb.predict(X_train)\n\nMSE_test_gb = mean_squared_error(y_test, y_test_predicted_gb)\nRMSE_test_gb = np.sqrt(MSE_test_gb)\nRS_test_gb = gb.score(X_test, y_test)\nprint('----------------------Test set----------------------')\nprint('This is the RMSE:', RMSE_test_gb)\nprint('This is the RS:', RS_test_gb)\n\nMSE_train_gb = mean_squared_error(y_train, y_train_predicted_gb)\nRMSE_train_gb = np.sqrt(MSE_train_gb)\nRS_train_gb = gb.score(X_train, y_train)\nprint('----------------------Train set----------------------')\nprint('This is the RMSE:', RMSE_train_gb)\nprint('This is the RS:', RS_train_gb)","ffde769b":"models_comp = pd.DataFrame({\n   'Model': ['Linear Regression', 'Random Forest Regression', 'Gradient Boosting Regression'],\n    'Accuracy score': [RS_test_lr_2, RS_test_rf, RS_test_gb], \n    'RMSE score': [RMSE_test_lr_2, RMSE_test_rf, RMSE_test_gb],\n})\ndisplay(models_comp.sort_values(by = 'RMSE score', ascending = True))","83ff892c":"# Conclusion","aafa5293":"Since a polynomial regression can take different order, I'll run a for loop to get the best order.\n<br>\n<br>\nA polynomial preprocessing with an order 3 improve both the caccuracy score and the RMSE score: Accuracy = 0.8105 and RMSE = 0.1714","4c632758":"-----------------------------------------------------------------------------","bd4fce74":"-----------------------------------------------------------------------------","7b0047e9":"So, the first goal is to select the right features to implement in our model.\n<br>\nA feature selection can be used to improve the accuracy scores of estimators. It reduces the feature selection on sample sets. (https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html)\n<br>\nIn our case, we have 13 features. Using a feature selection, we're going to choose the best feature for our future model.","3e75950c":"The Lasso Regression with a polynomial preprocessing give both the best accuracy (0.830239) and the best RMSE (0.162322).\n<br>\n<br>\nWhen the scatter plot for each model are plotted, we can visualize some outliers that are probably the cause of this accuarcy and RMSE. In other words, these scores can be improved if that outliers are better treated. ","2561ef48":"Check the skewness and deal with it with a log transformation.","5b0cf3b5":"## Prediction","6eeeacba":"## Pairplot","c7c06a74":"#### Gradient Boosting Regression","ca810b24":"For the same reason given with the linear regression, I'll directly perform a polynomial preprocessing","d9707dfd":"Then, I narrow down the alpha value to get a more accurate alpha.","14d52903":"It's a good introduction to a regression problem. \n<br>\nWe saw how to improve the accuracy and rmse scores using different model and different feature selection method.\n<br>\nWe could improve both scores by working on the features, i.e feature engineering. This will be in another episode.\n<br>\n##### To be continued...","a06d70c8":"## Discover the dataset","f9aec431":"#### Linear Regression\nThe accuracy score is 0.738 for the test set. This can be improved.\n<br>\nThe RMSE score is 0.2 which is not bad at all (as a comparison, the RMSE score without treating the outliers is about 5.01).","12d7789f":"----------------------------------------------------------------------------","bed59a95":"Let's compare the Kbest feature selection with the correlation for each feature. \n<br>\nThe correlation tells if the feature has a big impact on the target or no.\n<br>\nThe closer the correlation is to -1 or 1, the greater the impact is. That is why I set all the value with their absolute, so I can rank them. ","d237634c":"# Method #1","e069e343":"## Data Cleaning","192f26e1":"With 2 other models, I successfully improved the accuracy score and the RMSE score compare to a classical Linear regression model. This score can be improved if I perform a hyperparameter selection for the models.\n\n<b>To be continued...<b>","b58f2433":"Standardize the numerical values","12d0485f":"One of the straight forward feature engineering with a linear regression problem is to use the log transform of the skewed features. So the outliers will have less impact on the prediction model.\n<br>\n<br>\nAs a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed.\n<br>\n<br>\nLSTAT, PTRATIO, and MEDV will be log transformed.","f9fc1c09":"On the pairplot, we can visualize some outilers that could skew the prediction model. Let's deal with them. \n<br>\n<br>\nFirst, we'll detect the outliers for each feature. Using the interquartile.","eb85fdd2":"RFECV tells us what the optimal number of features is for a specific prediction model.\n<br>\nLet's try with a simple Linear regression and compare the accuracy and rmse with the previous method (3 features --> LSTAT, RM, PTRATIO)","4f01a48b":"#### Random Forest Regressor\n<b>Reminder<\/b>: The Random Forest Algorithm merges the output of multiple Decision Trees to generate the final output. Random Forest model is mostly known for classification problems, but can be used for regression problem like this one.\n<br>\nThe Random Forest model is also very fast and robust than other regression models.","c1b9ce83":"# Method #2","92909d97":"## Feature engineering","03bb93de":"With Ridge regression, we use the least-squared method but we add a penalty to it.\n<br>\nThis will minimize the square of w entries --> L2 penalty\n<br>\n<br>\n$${LS} = \\sum\\limits _{i = 1} ^{N} (y_{i} - (w * x_{i} + b))^2 $$\n<br>\n$${RSS} = \\sum\\limits _{i = 1} ^{N} (y_{i} - (w * x_{i} + b))^2 + {\\alpha} \\sum\\limits _{j = 1} ^{p} w _{j} ^2 $$\n<br>\n<br>\nAt first, the RMSE was extremely close to the one I had in the Linear regression model. We saw that with a polynomial preprocessing, the RMSE was better.\n<br>\n<br>\nThe Ridge regression with a polynomial preprocessing of degree 5 gives an accuracy of 0.8278 and a RMSE score of 0.16344","10e87e04":"## Comparison with the feature selection RFECV","5e02f11c":"Let's keep only features with a correlation greater than 0.5 -> LSTAT, RM, and PTRATIO. ","5daf2298":"#### Lasso Regression\nThe lasso regression will minimize the sum of the absolute values of the coeeficient --> L1 penalty\n<br>\n<br>\n$${LR} = \\sum\\limits _{i = 1} ^{N} (y_{i} - (w * x_{i} + b))^2 + {\\alpha} \\sum\\limits _{j = 1} ^{p} \\lvert w _{j} \\rvert $$","824c17eb":"#### Ridge Regression","f7b39b79":"## Set the environment","f0f1dc58":"Split the test and train sets","71624cfc":"## Outliers","cf5fe7b7":"## With feature selection --> Recursive feature elimination with cross-validation","6038ef8b":"-----------------------------------------------------------------------------","f26df253":"# Objective\nIn this notebook, I take the boston housing datasets to practice on a regression problem.\n<br>\n<br>\nI compare 4 prediction models:\n<ul>\n    <li>Linear Regression<\/li>\n    <li>Linear Regression with Polynomial Preprocessing<\/li>\n    <li>Ridge Regression with Polynomial Preprocessing<\/li>\n    <li>Lasso Regression with Polynomial Preprocessing<\/li>\n<\/ul>\nThen, I quickly introduce the Recursive feature elimination with cross-validation, for a better feature selection method.\n<br>\n<br>\n<i><b>This notebook is under progress (better feature engineering, improve accuracy score, improve rmse score).<\/b><\/i>\n<br>\nThis is an introduction and practice for the competition: House Prices - Advanced Regression Techniques.","287d7829":"-----------------------------------------------------------------------------","944ad8d7":"LSTAS and PTRATIO seems to be skewed. While RM seems symetrical. Let's check that to be sure.\n<br>\nAs expected, LSTAT and PTRATIO are respectively positively skewed and negatively skewed. RM is as well a little bit positively skewed.","21d01b5a":"I first test different alpha values to get the best alpha for this particular model.","2d2b45eb":"#### Linear regression","471e9307":"## Conclusion","2d695f06":"### Polynomial preprocessing","c1d9bc7e":"#### Comparison results","1a3b5b76":"As we can see, the method 2 with 6 features gives a better accuracy and a better RMSE score than the method one.\n<br>\n<br>\nYes, we have to be careful with feature selection. But more features doesn't necessarly means worst prediction.\n<br>\nRFECV is a good method to choose the right number of features.","5a5f7bcd":"## Feature selection","54443dcc":"Seeing the scatter plot above, we can see that a polynomial regression can be a good solution to improve the accuracy score of the Linear Regression model. Let's see if the assumption is correct.","615f21be":"## Comparing the results"}}