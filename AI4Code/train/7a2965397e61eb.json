{"cell_type":{"c5bfc334":"code","205f6931":"code","6b43451e":"code","ee263356":"code","4800958e":"code","ad94ed45":"code","c211a65c":"code","6ef12c5d":"code","9c978618":"code","a4b591ff":"code","1897875e":"code","a5692618":"code","37cbc5bd":"code","2c0d6b30":"code","b4a60048":"code","6bd868a6":"code","4670cf75":"code","98ba803d":"code","40bc1915":"code","c8a27c57":"code","0a1c45a4":"code","727c0ad8":"code","45bf51a2":"code","5c20049b":"code","04466bc9":"code","9c6b2fc7":"code","8791e217":"code","b233cc40":"code","5a650c5d":"code","4c87cd0d":"code","9560c614":"code","884a2110":"code","2303142d":"code","e361384d":"code","9d37a67b":"code","ac6a4d75":"code","bc7c7073":"code","c4ffd0d5":"code","c9ff5dba":"code","b9739f3c":"code","1ad8f01a":"code","2bded663":"code","8569c84c":"code","64b1b5d6":"code","b1df0172":"code","3bc10194":"code","894f5419":"code","fc0dce6d":"code","39c4ab28":"code","ba7ad743":"code","f33b6c00":"code","2ee20a5d":"code","6f4ebd6e":"code","844c8f22":"code","5e42344d":"code","684594e6":"code","70564396":"code","4c52995f":"code","31da09d0":"code","1ecaf25a":"markdown","fb59078c":"markdown","2052e0c4":"markdown","f1f2531c":"markdown","eaba22e4":"markdown","46e8e9a6":"markdown","287d110f":"markdown","b4cdd2b8":"markdown","5d6c9d52":"markdown","fcbda91c":"markdown","79bc3dec":"markdown","a7f72232":"markdown","ccd606d2":"markdown","474f35b0":"markdown","73d74ff3":"markdown","a460ff5e":"markdown","f5c4dffc":"markdown","5b3ba1da":"markdown","468b6bcc":"markdown","9b5f55e6":"markdown","7b830364":"markdown","7e785afc":"markdown","df9e4c97":"markdown","6f924cf2":"markdown","3c0e1152":"markdown","972d7a49":"markdown","ac9e3352":"markdown","d5143a1e":"markdown","32505432":"markdown","565a6e9f":"markdown","6f1fd30a":"markdown","5dfbc444":"markdown","ecb5af34":"markdown","ae788def":"markdown","91c0fb77":"markdown","30a8c545":"markdown","db1f74de":"markdown","cd233881":"markdown","97d357b9":"markdown","2a7f80a0":"markdown","aa9fadb4":"markdown","3deb64e5":"markdown","dc7a63cd":"markdown","84e63248":"markdown","d578ce75":"markdown","5b7f2aef":"markdown","cf51900d":"markdown"},"source":{"c5bfc334":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import RANSACRegressor\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom scipy.special import boxcox1p\nfrom sklearn.svm import LinearSVR\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport statsmodels.api as sm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/house-prices-advanced-regression-techniques\"))\n\n# Any results you write to the current directory are saved as output.","205f6931":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndata_desc = open(\"..\/input\/house-prices-advanced-regression-techniques\/data_description.txt\")\nprint(data_desc.read())","6b43451e":"test_data.shape","ee263356":"sample_submission","4800958e":"train_data.head()","ad94ed45":"train_data.info()","c211a65c":"train_data.describe()","6ef12c5d":"data_null = train_data.isnull().sum()\/len(train_data) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nmissing_data.head(10)","9c978618":"plt.subplots(figsize=(8,6))\nplt.xticks(rotation='90')\nsns.barplot(data_null.index, data_null)\nplt.xlabel('Features', fontsize=12)\nplt.ylabel('Missing rate', fontsize=12)","a4b591ff":"plt.subplots(figsize=(8,8))\ncorr_data = train_data.corr()\nsns.heatmap(corr_data, square=True)","1897875e":"k = 10 # number of variables for heatmap\ncols = corr_data.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_data[cols].values.T)\nplt.subplots(figsize=(8,8))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","a5692618":"corr_data[\"SalePrice\"].sort_values(ascending=False)","37cbc5bd":"var = 'OverallQual'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","2c0d6b30":"missing_data","b4a60048":"train_data.select_dtypes(include=['object']).columns","6bd868a6":"train_data.select_dtypes(exclude=['object']).columns","4670cf75":"y_train = train_data[\"SalePrice\"]\ndata = train_data.drop([\"SalePrice\"], axis=1)","98ba803d":"fig, ax = plt.subplots()\nax.scatter(x = train_data['GrLivArea'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","40bc1915":"train_data.sort_values(by = 'GrLivArea', ascending = False)[:2]","c8a27c57":"train_data = train_data.drop(train_data[train_data['Id'] == 1299].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 524].index)","0a1c45a4":"fig, ax = plt.subplots()\nax.scatter(x = train_data['GrLivArea'], y = train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","727c0ad8":"def find_outliers(model, X, y, sigma=3):\n\n    # predict y values using model\n    try:\n        y_pred = pd.Series(model.predict(X), index=y.index)\n    # if predicting fails, try fitting the model first\n    except:\n        model.fit(X,y)\n        y_pred = pd.Series(model.predict(X), index=y.index)\n        \n    # calculate residuals between the model prediction and true y values\n    resid = y - y_pred\n    mean_resid = resid.mean()\n    std_resid = resid.std()\n\n    # calculate z statistic, define outliers to be where |z|>sigma\n    z = (resid - mean_resid)\/std_resid    \n    outliers = z[abs(z)>sigma].index\n    \n    return outliers","45bf51a2":"def changeSeqCat(data):\n    cols_ExGd = ['ExterQual','ExterCond','BsmtQual','BsmtCond',\n                 'HeatingQC','KitchenQual','FireplaceQu','GarageQual',\n                'GarageCond','PoolQC']\n\n    dict_ExGd = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'None':0}\n\n    for col in cols_ExGd:\n        data[col].replace(dict_ExGd, inplace=True)  \n\n    # Remaining columns\n    data['BsmtExposure'].replace({'Gd':4,'Av':3,'Mn':2,'No':1,'None':0}, inplace=True)\n\n    data['CentralAir'].replace({'Y':1,'N':0}, inplace=True)\n\n    data['Functional'].replace({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0}, inplace=True)\n\n    data['GarageFinish'].replace({'Fin':3,'RFn':2,'Unf':1,'None':0}, inplace=True)\n\n    data['LotShape'].replace({'Reg':3,'IR1':2,'IR2':1,'IR3':0}, inplace=True)\n\n    data['Utilities'].replace({'AllPub':3,'NoSewr':2,'NoSeWa':1,'ELO':0}, inplace=True)\n\n    data['LandSlope'].replace({'Gtl':2,'Mod':1,'Sev':0}, inplace=True)\n    \n    return data","5c20049b":"def setMissingData(data):\n    # MSZoning NA in pred. filling with most popular values\n    data['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\n\n    # LotFrontage  NA in all. I suppose NA means 0\n    data['LotFrontage'] = data['LotFrontage'].fillna(data['LotFrontage'].mean())\n    # Converting OverallCond to str\n    data.OverallCond = data.OverallCond.astype(str)\n    \n    data['Fence'] = data['Fence'].fillna('NA')\n    data['Alley'] = data['Alley'].fillna('NA')\n    data['Functional'] = data['Functional'].fillna('Typ')\n\n    # MasVnrType NA in all. filling with most popular values\n    data['MasVnrType'] = data['MasVnrType'].fillna(data['MasVnrType'].mode()[0])\n\n    # BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2\n    # NA in all. NA means No basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        data[col] = data[col].fillna('None')\n\n    # TotalBsmtSF  NA in pred. I suppose NA means 0\n    data['TotalBsmtSF'] = data['TotalBsmtSF'].fillna(0)\n\n    # KitchenQual NA in pred. filling with most popular values\n    data['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\n\n    # FireplaceQu  NA in all. NA means No Fireplace\n    data['FireplaceQu'] = data['FireplaceQu'].fillna('None')\n\n    # GarageType, GarageFinish, GarageQual  NA in all. NA means No Garage\n    for col in ('GarageType', 'GarageFinish', 'GarageQual'):\n        data[col] = data[col].fillna('None')\n\n    # GarageCars  NA in pred. I suppose NA means 0\n    data['GarageCars'] = data['GarageCars'].fillna(0.0)\n\n    # SaleType NA in pred. filling with most popular values\n    data['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])\n\n    # Year and Month to categorical\n    data['YrSold'] = data['YrSold'].astype(str)\n    data['MoSold'] = data['MoSold'].astype(str)\n\n    # Adding total sqfootage feature and removing Basement, 1st and 2nd floor features\n    data['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n    \n    data['YrBltAndRemod']=data['YearBuilt']+data['YearRemodAdd']\n\n    data['Total_sqr_footage'] = (data['BsmtFinSF1'] + data['BsmtFinSF2'] +\n                                     data['1stFlrSF'] + data['2ndFlrSF'])\n\n    data['Total_Bathrooms'] = (data['FullBath'] + (0.5 * data['HalfBath']) +\n                                   data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']))\n\n    data['Total_porch_sf'] = (data['OpenPorchSF'] + data['3SsnPorch'] +\n                                  data['EnclosedPorch'] + data['ScreenPorch'] +\n                                  data['WoodDeckSF'])\n    \n    data['haspool'] = data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['has2ndfloor'] = data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasgarage'] = data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasbsmt'] = data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    data['hasfireplace'] = data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    \n    return data","04466bc9":"from scipy.special import boxcox1p\nfrom scipy.stats import norm, skew #for some statistics\n\ndef handleSkew(all_data):\n    numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    print(\"\\nSkew in numerical features: \\n\")\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    skewness.head(10)\n    skewness = skewness[abs(skewness) > 0.75]\n    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\n    skewed_features = skewness.index\n    lam = 0.15\n    for feat in skewed_features:\n        #all_data[feat] += 1\n        all_data[feat] = boxcox1p(all_data[feat], lam)\n    return all_data","9c6b2fc7":"class LabelBinarizerPipelineFriendly(MultiLabelBinarizer):\n    def fit(self, X, y=None):\n        super(LabelBinarizerPipelineFriendly,self).fit(X)\n    def transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).transform(X)\n    def fit_transform(self, X, y=None):\n        return super(LabelBinarizerPipelineFriendly, self).fit(X).transform(X)","8791e217":"class DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attr):\n        self.attributes = attr\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attributes].values","b233cc40":"class LabelEncoderCat(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X = pd.DataFrame(X)\n        X = X.apply(LabelEncoder().fit_transform)\n        return X","5a650c5d":"id_test = test_data['Id']\ndata = setMissingData(data)\ndata = changeSeqCat(data)\ndata = handleSkew(data)","4c87cd0d":"cat_attr = ['MSZoning', 'LandContour', 'Neighborhood', 'Condition1',\n       'BldgType', 'RoofMatl', 'Exterior1st', 'LotConfig', 'Alley',\n       'MasVnrType', 'BsmtFinType1', 'BsmtFinType2', 'Electrical',\n       'GarageType', 'SaleCondition', 'BsmtFinType1', 'RoofStyle']\n\nnum_attr = ['LotFrontage', 'LotArea', 'OverallQual', 'TotalSF', 'CentralAir',\n        'YearBuilt', 'BsmtFinSF1', 'BsmtUnfSF', 'BsmtExposure', 'GarageArea',\n        'GrLivArea', 'BsmtFullBath', 'FullBath', 'TotRmsAbvGrd', 'GarageCond',\n        'Fireplaces', 'WoodDeckSF','OpenPorchSF', 'GarageArea', 'GarageCars',\n        'haspool', 'has2ndfloor', 'hasgarage', 'hasbsmt', 'hasfireplace', \n        'YrBltAndRemod', 'Total_sqr_footage', 'Total_Bathrooms', 'Total_porch_sf',\n        'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'CentralAir',\n        'BsmtCond', 'BsmtQual', 'OverallCond', 'Functional', 'ExterQual']\n        \n\n# cat_seq_attr = ['HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual',\n#                 'BsmtCond', 'BsmtQual', 'OverallCond']\n\nnum_pipeline = Pipeline([\n        ('selector', DataFrameSelector(num_attr)),\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('scaler', StandardScaler()),\n    ]) \n\ncat_pipeline = Pipeline([\n        ('selector', DataFrameSelector(cat_attr)),\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('label_binarizer', LabelBinarizerPipelineFriendly()),\n#         ('scaler', StandardScaler()),\n    ])\n\n\nfull_pipeline = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('cat_pipeline', cat_pipeline),\n])","9560c614":"n_folds = 5\n\ndef rmse_cv(model, X_data, y_data):\n    rmse= np.sqrt(-cross_val_score(model, X_data, y_data, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","884a2110":"X_prepared = pd.DataFrame(full_pipeline.fit_transform(data))\nX_prepared = pd.DataFrame(X_prepared)","2303142d":"# from sklearn.feature_selection import f_regression\n\n# all_cat = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities',\n#        'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n#        'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n#        'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n#        'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n#        'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual',\n#        'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n#        'PavedDrive', 'Fence', 'SaleType', 'SaleCondition']\n\n# not_evaluated = (list(set(all_cat) - set(cat_attr)))","e361384d":"data_null = data[cat_attr].isnull().sum()\/len(data[cat_attr]) * 100\ndata_null = data_null.drop(data_null[data_null == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio': data_null})\nmissing_data","9d37a67b":"from sklearn.feature_selection import f_regression\n\nfinal_data = pd.get_dummies(data[cat_attr]).reset_index(drop=True)\nF, p_value = f_regression(final_data, y_train)\nnp.array(final_data.columns) + \" = \" + (p_value < 0.05).astype(str)","ac6a4d75":"\noutliers = find_outliers(Ridge(), X_prepared, y_train)\nX_prepared = X_prepared.drop(outliers)\ny_train = y_train.drop(outliers)\n\ny_train = np.log1p(y_train)","bc7c7073":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nscore = rmse_cv(lin_reg, X_prepared, y_train)\nscore.mean()","c4ffd0d5":"ridge_reg = Ridge(alpha=0.001, solver=\"cholesky\")\nscore = rmse_cv(ridge_reg, X_prepared, y_train)\nscore.mean()","c9ff5dba":"lasso_reg = Lasso(alpha=0.0003)\nscore = rmse_cv(lasso_reg, X_prepared, y_train)\nscore.mean()","b9739f3c":"elastic_net = ElasticNet(alpha=0.0005, l1_ratio=0.65)\nscore = rmse_cv(elastic_net, X_prepared, y_train)\nscore.mean()","1ad8f01a":"ransac_reg = RANSACRegressor(lasso_reg)\nscore = rmse_cv(ridge_reg, X_prepared, y_train)\nscore.mean()","2bded663":"random_reg = RandomForestRegressor(max_depth=10, random_state=42, n_estimators=200)\nscore = rmse_cv(random_reg, X_prepared, y_train)\nscore.mean()","8569c84c":"huber_reg = HuberRegressor(epsilon=5)\nscore = rmse_cv(huber_reg, X_prepared, y_train)\nscore.mean()","64b1b5d6":"svm_reg = LinearSVR(epsilon=0.001)\nscore = rmse_cv(svm_reg, X_prepared, y_train)\nscore.mean()","b1df0172":"adf_clf = AdaBoostRegressor(DecisionTreeRegressor(max_depth=3), n_estimators=3000, learning_rate=0.05, \n                            loss='square')\n\nscore = rmse_cv(adf_clf, X_prepared, y_train)\nscore.mean()","3bc10194":"from sklearn.metrics import mean_squared_error\n\ngbrt = GradientBoostingRegressor(max_depth=3, n_estimators=3000, learning_rate=0.05,\n                                 max_features='sqrt', loss='huber')\ngbrt.fit(X_prepared, y_train)\nerrors = [mean_squared_error(y_train, y_pred) for y_pred in gbrt.staged_predict(X_prepared)]\nbst_n_estimators = np.argmin(errors)\nbst_n_estimators","894f5419":"gbrt = GradientBoostingRegressor(max_depth=3, n_estimators=bst_n_estimators, learning_rate=0.05, max_features='sqrt',\n                                 loss='huber')\nscore = rmse_cv(gbrt, X_prepared, y_train)\nscore.mean()","fc0dce6d":"adf_clf.fit(X_prepared, y_train)\ngbrt.fit(X_prepared, y_train)\nlasso_reg.fit(X_prepared, y_train)\nelastic_net.fit(X_prepared, y_train)\nrandom_reg.fit(X_prepared, y_train)\nridge_reg.fit(X_prepared, y_train)\nsvm_reg.fit(X_prepared, y_train)\nransac_reg.fit(X_prepared, y_train)","39c4ab28":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg = VotingRegressor([\n            (\"gradient_boost\", gbrt),\n            (\"lasso_reg\", lasso_reg),\n            (\"random_reg\", random_reg),\n#             (\"ridge_reg\", ridge_reg),\n#             (\"svm_reg\", svm_reg),\n            (\"elastic_net\", elastic_net),\n            ]) \n\nscore = rmse_cv(voting_reg, X_prepared, y_train)\nscore.mean()","ba7ad743":"voting_reg.fit(X_prepared, y_train)","f33b6c00":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n    def fit(self, X, y=None):\n        X = X.values\n        y = y.values\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    \n    def get_metafeatures(self, X):\n        return np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n    \n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","2ee20a5d":"stacked_averaged_models = StackingAveragedModels(base_models = (elastic_net, gbrt, ridge_reg, random_reg),\n                                                 meta_model = lasso_reg)\n\nscore = rmse_cv(stacked_averaged_models, X_prepared, y_train)\nscore.mean()","6f4ebd6e":"X_prepared.head()","844c8f22":"test_data = setMissingData(test_data)\ntest_data = changeSeqCat(test_data)\ntest_data = handleSkew(test_data)","5e42344d":"test_prepared = full_pipeline.transform(test_data)\ntest_prepared = pd.DataFrame(test_prepared)","684594e6":"stacked_averaged_models.fit(X_prepared, y_train)\nstack_y_pred = stacked_averaged_models.predict(test_prepared)\n\nvoting_reg_y_pred = voting_reg.predict(test_prepared)","70564396":"y_pred = np.expm1(stack_y_pred) * 0.75 + np.expm1(voting_reg_y_pred) * 0.25","4c52995f":"submission = pd.DataFrame({\n        \"Id\": id_test,\n        \"SalePrice\": y_pred\n    })\n\nsubmission","31da09d0":"submission.to_csv('prediction.csv', index=False)","1ecaf25a":"# Handling Missing Data","fb59078c":"* ### Huber Regressor","2052e0c4":"If a value is missing it is probably becuase that attribute does not exist in that house. For instance if attributes regarding garage are missing, it is likely that the house does not have a garage. So, it is not wise to always fill null values with median, etc. strategies.","f1f2531c":"# House Price Prediction","eaba22e4":"### P-value","46e8e9a6":"Let's look at which attributes are categorical.","287d110f":"## Change the Sequential Categories","b4cdd2b8":"### SalePrice Correlation Matrix","5d6c9d52":"SalePrice seems to increase with OverallQual.","fcbda91c":"Some of the categorical values such as Excellent, Good, etc. are values which can be sorted and have meaning in relation to each other. So, it is not wise to use 1-hot-encoding to handle them.","79bc3dec":"* ### Elastic Net","a7f72232":"# Pipeline","ccd606d2":"First, we read the train data, test data, sample submission, and data description.\n\nData description is printed to give us better view of the data.","474f35b0":"* ### Ridge Regression","73d74ff3":"I consider deleting the columns with more than 20% of missing data as thay are highly incomplete. \nSo we will delete the columns below:\n* PoolQc \n* MiscFeature\n* Alley\n* Fence","a460ff5e":"# Train Model","f5c4dffc":"### Set Missing Data to None-Existant","5b3ba1da":"# Prepare Data for Train","468b6bcc":"# Test","9b5f55e6":"It looks like that SalePrice is highly correlated with OverallQual, GrLivArea, Garage Capacity, and Basement Size.\nThere are also many other attributes affecting the SalePrice.","7b830364":"* ### Lasso Regression","7e785afc":"* ### Random Forest","df9e4c97":"Let's have a closer look at missing data.","6f924cf2":"The rest can be replaced by strategies such as median.","3c0e1152":"Relationship with \"OverallQual\" attribute:","972d7a49":"It looks like that the pair of \n* GarageCars\n* GarageArea\n\nand also the pair of \n* TotalBsmtSF\n* 1stFlrSF\n\nare highly correlated and one of them can be dropped.","ac9e3352":"# Read Required Files","d5143a1e":"* ### Gradient Boosting","32505432":"# Recognize Missing Data","565a6e9f":"There are so many feautures and we need to gain a better insight about what will be useful here.","6f1fd30a":"* ### Stacking","5dfbc444":"The attributes with p-value less than 0.5 are useful. Note that if one of the classes of a categorical attribute has a desirable p-value, you should keep all the classes corresponding to that categorical attribute.","ecb5af34":"## Handle Skewed Data","ae788def":"# Delete Outliers","91c0fb77":"We can see that there are a few obvious outliers which have a large area but low price. We can safely remove them.","30a8c545":"* ### SVM","db1f74de":"Generally, \n[\"OverallQual\", \"GrLivArea\", \"GarageCars\", \"TotalBsmtSF\", \"FullBath\", \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\", \"MasVnrArea\", \"Fireplaces\"] \nseem to be the most important factors.","cd233881":"Outliers can be easily misleading. We nee","97d357b9":"![](https:\/\/vmcdn.ca\/f\/files\/kamloopsmatters\/images\/stock-photos\/housing\/money-for-house.jpg;w=630)","2a7f80a0":"As seen above, many features are not complete and there exists a lot of null values. We will handle the incomplete data in this section.","aa9fadb4":"* ### Voting Regressor","3deb64e5":"# Correlation","dc7a63cd":"* ### Linear Regression","84e63248":"* ### Ada Boost","d578ce75":"We will now calculate the rmse using cross validation.","5b7f2aef":"* ### RANSAC Regressor","cf51900d":"# RMSE Calculator"}}