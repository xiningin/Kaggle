{"cell_type":{"e71ddee9":"code","bb003808":"code","0234a2e1":"code","ec69fba6":"code","e3c74aff":"code","00a5846d":"code","e126bd79":"code","6cd66a59":"code","5629fc35":"code","458763d5":"code","e560c9ed":"code","48be384c":"code","47d90662":"code","16a498e4":"code","0b64b24a":"code","d6b45621":"code","4f39ea5b":"code","14948ddf":"code","5a0d2365":"code","0f1b6666":"code","f6661239":"code","fd2e5d27":"code","bdca66d1":"code","36c3191e":"code","4acb7297":"code","5ea43077":"code","6bd28d33":"code","13dd33c0":"code","9b5514b2":"code","5457ecdd":"code","0c53a6f3":"code","11cf6071":"code","dec29fbd":"code","84577d9b":"code","9f45c50e":"code","625eb382":"code","91e8f8b3":"code","a69976e5":"code","3a885322":"code","9e2bc0fb":"code","1c55de8b":"code","646b43d5":"code","c131b514":"code","c5af9e09":"code","09f677e7":"code","f161f8f0":"code","c0a0fff6":"code","fd2ff014":"code","d6b0f2ca":"code","4af4abdf":"code","c4dc3956":"code","874e4005":"code","9fd337c2":"code","e57682ab":"code","6652faa3":"code","0836ca72":"code","88aa334f":"code","fa33e1a0":"code","a73bb275":"code","b49947ca":"code","9f8e0fa9":"code","7d2ad441":"code","6c2f0ef8":"code","c1c37db3":"code","739d4573":"code","a27e522a":"code","7cf3c6ac":"code","864a52aa":"code","97ea780e":"code","c4e955b5":"code","98b9a7ff":"code","91dcd0cf":"code","79bb25f3":"code","4581ae31":"code","059518fa":"code","4f015df0":"code","9b779a7b":"code","75298614":"code","00f5131f":"code","44ef3d58":"code","a2e06f20":"code","a7ca2112":"code","999cff78":"code","3ba29d9d":"code","338c0759":"code","bdf63d53":"code","c9574cf0":"code","5e03d286":"code","b0a2b2e2":"code","b67d06f0":"code","1bcf170e":"code","5c75734c":"code","a6e77c39":"code","663d008e":"code","be07826c":"code","c07ddb22":"code","f1baa078":"code","238b9fe7":"code","e8f3faca":"code","61787def":"code","e9fc836f":"code","30124a7d":"code","da4d0caf":"code","37374566":"code","82e4c996":"code","7337debb":"code","122ad542":"code","1cc87172":"code","4299b206":"code","2a6cd6dc":"code","8a79a0c7":"code","dd57de02":"code","b1350f19":"code","cf2b285e":"code","eb8d8bb6":"code","7d2f574b":"code","f8f98dfe":"code","d17c585d":"code","13bb939e":"code","c09cb918":"code","a1e8e8d3":"code","e11e00c1":"code","909886d8":"code","f18b78ae":"code","c332735a":"code","4396cfce":"code","b6304b3b":"code","83fde6eb":"code","3e2da607":"code","e50b6e30":"code","20bfe4ea":"code","48a93655":"code","91086a3c":"code","269a1341":"code","e7fc664f":"code","1cb4dc33":"code","3f6c29bf":"code","8d6c86cb":"code","24dbb59b":"code","1fbae692":"code","499e3f0b":"code","e667813c":"code","cf632c05":"code","af416f7e":"code","5a51c838":"code","71c86d64":"code","d2faad18":"code","734afbab":"code","5f9674d5":"markdown","86eef43a":"markdown","f0aecf1d":"markdown","0a946311":"markdown","36a1f7c8":"markdown","1baac98e":"markdown","000fc6a8":"markdown","62e20d24":"markdown","c024c201":"markdown","422e2b46":"markdown","879fdf9a":"markdown","d4d9e2ec":"markdown","11b35c52":"markdown","a2c8c9be":"markdown","6c18c6ec":"markdown","a0931712":"markdown","1e931f6b":"markdown","19814b6d":"markdown","4f491cd8":"markdown","3d296588":"markdown","a5893ce9":"markdown","fca45bd5":"markdown","76bd163a":"markdown","c8d6605d":"markdown","69d2ef07":"markdown","70bf507b":"markdown","58c536a2":"markdown","fc3d1ff1":"markdown","670ea796":"markdown","8097f6a8":"markdown","962972da":"markdown","c428794d":"markdown","8e98011f":"markdown","3f89a8f1":"markdown","5a10e44b":"markdown","b5386d0a":"markdown"},"source":{"e71ddee9":"# suppress display of warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# 'Pandas' is used for data manipulation and analysis\nimport pandas as pd \n\n# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\nimport numpy as np\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\nimport seaborn as sns\n\n# import 'is_string_dtype' to check if the type of input is string  \nfrom pandas.api.types import is_string_dtype\n\n# import various functions to perform classification\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import SVC\n\n# import various functions to perform regression\nfrom sklearn.linear_model import SGDClassifier\nimport statsmodels\nimport statsmodels.api as sm\nfrom sklearn import linear_model\n\n#importing library for scaling data\nfrom sklearn.preprocessing import MinMaxScaler\n\n# display all columns of the dataframe\npd.options.display.max_columns = None","bb003808":"#setting the plot size using rcParams\nplt.rcParams['figure.figsize'] = [15,8]","0234a2e1":"#importing data set for training the models\ndf=pd.read_csv(\"..\/input\/share-market-prediction\/Training_.csv\")","ec69fba6":"#printing the first 5 records from the training data set\ndf.head()","e3c74aff":"#checking the number of rows and columns in the training data set\ndf.shape","00a5846d":"#\"info\" gives us the column names and their data types along with null values if any in the columns\ndf.info()","e126bd79":"# the describe() returns the statistical summary of the numeric variables\ndf.describe()","6cd66a59":"#As the problem statement states that any day when the closing value is 2% higher than the opening price\n#that day is considered to be the buy day and rest don't buy day for intraday traders\n#Hence calculating the closing price is how much greater tha the opening price and the percent of it\n#Then storing both the values in two new columns\ndf['greater']=(df['Close']-df['Open'])\/df['Open']\ndf['percent']=df['greater']*100","5629fc35":"#defining the target variable\n#Setting the values in target variable as 1 where the percent is greater than 2\n#rest of the values are set as 0\ndf['target'] = np.where(df['percent']>=2, 1, 0)\n   ","458763d5":"#plotting the countplot for the target column\nsns.countplot(df['target'],palette='rainbow')\nplt.title(\"Countplot of the target column\")\nplt.show()","e560c9ed":"# plot the histogram of numeric variables\n# the hist() function considers the numeric variables only, by default\ndf.hist(xrot = 20, )\n\n# adjust the subplots\nplt.tight_layout()\n\n# display the plot\nplt.show()  ","48be384c":"# Pairplot of numeric variables\n\n# select the columns for the pairplot\ncolumns= [\"Open\", \"Close\", \"High\", \"Low\", \"Volume\"]\n\n# draw the pairplot such that the diagonal should be density plot and the other graphs should be scatter plot\nsns.pairplot(df[columns], size=2, kind= \"scatter\", diag_kind=\"kde\")\n\n# display the plot\nplt.show()","47d90662":"# draw the boxplot for target and the opening price\nsns.boxplot(y=\"Open\", x=\"target\", data= df)\n\n# set the title of the plot and the fontsize\nplt.title(\"Open price versus target variable\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Target\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Open\", fontsize=15)\n\n# display the plot\nplt.show()","16a498e4":"# draw the boxplot for target and the opening price\nsns.boxplot(y=\"Close\", x=\"target\", data= df)\n\n# set the title of the plot and the fontsize\nplt.title(\"Close price versus target variable\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Target\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Close\", fontsize=15)\n\n# display the plot\nplt.show()","0b64b24a":"sns.boxplot(y=\"High\", x=\"target\", data= df)\n\n# set the title of the plot and the fontsize\nplt.title(\"High price versus target variable\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Target\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"High\", fontsize=15)\n\n# display the plot\nplt.show()","d6b45621":"sns.boxplot(y=\"Low\", x=\"target\", data= df)\n\n# set the title of the plot and the fontsize\nplt.title(\"Low price versus target variable\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Target\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"Low\", fontsize=15)\n\n# display the plot\nplt.show()","4f39ea5b":"# draw the boxplot for greater and the opening price\nsns.boxplot(y=\"greater\", x=\"target\", data= df)\n\n# set the title of the plot and the fontsize\nplt.title(\"Open price versus target variable\", fontsize=15)\n\n# set the xlabel and the fontsize\nplt.xlabel(\"Target\", fontsize=15)\n\n# set the ylabel and the fontsize\nplt.ylabel(\"greater\", fontsize=15)\n\n# display the plot\nplt.show()","14948ddf":"#Checking employees who require treatment are from which gender\nsns.boxplot(df.Open)\nplt.show()","5a0d2365":"#Checking employees who require treatment are from which gender\nsns.boxplot(df.Close)\nplt.show()","0f1b6666":"sns.boxplot(df.Low)\nplt.show()","f6661239":"sns.boxplot(df.High)\nplt.show()","fd2e5d27":"sns.boxplot(df['Adj Close'])\nplt.show()","bdca66d1":"sns.boxplot(df['Volume'])\nplt.show()","36c3191e":"sns.boxplot(df.greater)\nplt.show()","4acb7297":"#Putting all features in one variable df_features\ndf_features=df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume','greater', 'percent','target']]","5ea43077":"from scipy.stats.mstats import winsorize","6bd28d33":"df['Open']=winsorize(df['Open'],(0.01,0.1))\ndf['Close']=winsorize(df['Close'],(0.01,0.1))\ndf['Low']=winsorize(df['Low'],(0.01,0.1))\ndf['High']=winsorize(df['High'],(0.01,0.1))\ndf['Adj Close']=winsorize(df['Adj Close'],(0.01,0.1))\ndf['Volume']=winsorize(df['Volume'],(0.01,0.1))","13dd33c0":"#Checking for outliers after handling outliers \nsns.boxplot(df['Open'])\nplt.show()","9b5514b2":"#Checking for outliers after handling outliers \nsns.boxplot(df['Close'])\nplt.show()","5457ecdd":"#Checking for outliers after handling outliers \nsns.boxplot(df['Low'])\nplt.show()","0c53a6f3":"#Checking for outliers after handling outliers \nsns.boxplot(df['High'])\nplt.show()","11cf6071":"#Checking for outliers after handling outliers \nsns.boxplot(df['Volume'])\nplt.show()","dec29fbd":"#Checking for outliers after handling outliers \nsns.boxplot(df['greater'])\nplt.show()","84577d9b":"# sort the variables on the basis of total null values in the variable\n# 'isnull().sum()' returns the number of missing values in each variable\nTotal = df.isnull().sum().sort_values(ascending = False)          \n\n# calculate the percentage of missing values\nPercent = ((Total*100)\/df.isnull().count()).sort_values(ascending = False)   \n\n# concat the 'Total' and 'Percent' columns using 'concat' function\nmissing_data = pd.concat([Total, Percent], axis = 1, keys = ['Total', 'Percentage of Missing Values'])    \nmissing_data","9f45c50e":"# plot heatmap to check null values\n# 'cbar = False' does not show the color axis \nsns.heatmap(df.isnull(), cbar=False)\n\n# display the plot\nplt.show()","625eb382":"#deleting the rows with missing data\ndf=df.dropna(axis=0)\n","91e8f8b3":"# plot heatmap to check null values\n# 'cbar = False' does not show the color axis \nsns.heatmap(df.isnull(), cbar=False)\n\n# display the plot\nplt.show()","a69976e5":"#Creating a dataframe X which contains all the features\nX=df[['Open', 'High', 'Low', 'Close', 'Adj Close','percent','greater','Volume']]","3a885322":"scaler = MinMaxScaler()\nscaler.fit(X)","9e2bc0fb":"X = pd.DataFrame(scaler.fit_transform(X))","1c55de8b":"#creating another dataframe to store the target variable\ny=pd.DataFrame(df['target'])","646b43d5":"# let us now split the dataset into train & test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=10)\n\n# print the shape of 'x_train'\nprint(\"X_train \",X_train.shape)\n\n# print the shape of 'x_test'\nprint(\"X_test \",X_test.shape)\n\n# print the shape of 'y_train'\nprint(\"y_train \",y_train.shape)\n\n# print the shape of 'y_test'\nprint(\"y_test \",y_test.shape)","c131b514":"# create a generalized function to calculate the metrics values for test set\ndef get_test_report(model):\n    \n    # return the performace measures on test set\n    return(classification_report(y_test, y_pred))","c5af9e09":"# create a generalized function to calculate the metrics values for test set\ndef kappa_score(model):\n    \n    # return the kappa score on test set\n    return(cohen_kappa_score(y_test, y_pred))","09f677e7":"# define a to plot a confusion matrix for the model\ndef plot_confusion_matrix(model):\n    \n    # create a confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n    # plot a heatmap to visualize the confusion matrix\n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n\n    # set the font size of x-axis ticks using 'fontsize'\n    plt.xticks(fontsize = 20)\n\n    # set the font size of y-axis ticks using 'fontsize'\n    plt.yticks(fontsize = 20)\n\n    # display the plot\n    plt.show()","f161f8f0":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc(model):\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC Curve', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred),4)))\n\n    # plot the grid\n    plt.grid(True)","c0a0fff6":"# create an empty dataframe to store the scores for various classification algorithms\nscore_card = pd.DataFrame(columns=['Model', 'AUC Score', 'Precision Score', 'Recall Score', 'Accuracy Score',\n                                   'Kappa Score', 'f1-score'])\n\ndef update_score_card(model_name):\n    \n    # assign 'score_card' as global variable\n    global score_card\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card = score_card.append({'Model': model_name,\n                                    'AUC Score' : roc_auc_score(y_test, y_pred),\n                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa Score': cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)\n    return(score_card)","fd2ff014":"# instantiate the 'SGDClassifier' to build model using SGD\n# to perform logistic regression, consider the log-loss function \n# set 'random_state' to generate the same dataset each time you run the code \nSGD = SGDClassifier(loss = 'log', random_state = 10)\n\n# fit the model on scaled training data\nlogreg_with_SGD = SGD.fit(X_train, y_train)","d6b0f2ca":"# use predict() to predict the class labels of target variable\ny_pred = logreg_with_SGD.predict(X_test)","4af4abdf":"# call the function to plot the confusion matrix\nplot_confusion_matrix(logreg_with_SGD)","c4dc3956":"# compute the performance measures on test data\ntest_report = get_test_report(logreg_with_SGD)\n\n# print the performace measures\nprint(test_report)","874e4005":"# compute kappa score on test set\nkappa_value = kappa_score(logreg_with_SGD)\n\n# print the kappa value\nprint(kappa_value)","9fd337c2":"# call the function 'plot_roc' to plot the ROC curve\nplot_roc(logreg_with_SGD)","e57682ab":"# use the function 'update_score_card' to store the performance measures\nupdate_score_card(model_name = 'Logistic Regression (SGD)')","6652faa3":"# build the model\nsvclassifier = SVC(kernel = 'linear')\n\n# fit the model\nsvc_model=svclassifier.fit(X_train, y_train)\n","0836ca72":"# predict the values\ny_pred = svclassifier.predict(X_test)","88aa334f":"# call the function to plot the confusion matrix\nplot_confusion_matrix(svc_model)","fa33e1a0":"# compute the performance measures on test data\ntest_report = get_test_report(svc_model)\n\n# print the performace measures\nprint(test_report)","a73bb275":"# compute kappa score on test set\nkappa_value = kappa_score(svc_model)\n\n# print the kappa value\nprint(kappa_value)","b49947ca":"plot_roc(svc_model)","9f8e0fa9":"update_score_card(model_name='SVM')","7d2ad441":"# build the model\nsvclassifier = SVC(kernel='rbf')\n# fit the model\nsvm_rbf=svclassifier.fit(X_train, y_train)","6c2f0ef8":"# predict the values\ny_pred= svclassifier.predict(X_test)\n","c1c37db3":"plot_confusion_matrix(svm_rbf)","739d4573":"# compute kappa score on test set\nkappa_value = kappa_score(svm_rbf)\n\n# print the kappa value\nprint(kappa_value)","a27e522a":"update_score_card(model_name='SVM with rbf')","7cf3c6ac":"# build the model\nsvclassifier = SVC(kernel='sigmoid')\n# fit the model\nsvm_sigmoid=svclassifier.fit(X_train, y_train)","864a52aa":"# predict the values\ny_pred  = svclassifier.predict(X_test)","97ea780e":"# call the function to plot the confusion matrix\nplot_confusion_matrix(svm_sigmoid)","c4e955b5":"# compute the performance measures on test data\ntest_report = get_test_report(svm_sigmoid)\n\n# print the performace measures\nprint(test_report)","98b9a7ff":"# compute kappa score on test set\nkappa_value = kappa_score(svm_sigmoid)\n\n# print the kappa value\nprint(kappa_value)","91dcd0cf":"plot_roc(svm_sigmoid)","79bb25f3":"update_score_card(model_name='SVM Sigmoid')","4581ae31":"# build the model\nsvclassifier = SVC(kernel='poly')\n# fit the model\nsvm_poly=svclassifier.fit(X_train, y_train)\n","059518fa":"# predict the values\ny_pred  = svclassifier.predict(X_test)","4f015df0":"# call the function to plot the confusion matrix\nplot_confusion_matrix(svm_poly)","9b779a7b":"# compute the performance measures on test data\ntest_report = get_test_report(svm_poly)\n\n# print the performace measures\nprint(test_report)","75298614":"# compute kappa score on test set\nkappa_value = kappa_score(svm_poly)\n\n# print the kappa value\nprint(kappa_value)","00f5131f":"plot_roc(svm_poly)","44ef3d58":"update_score_card(model_name='SVM using polynomial kernel')","a2e06f20":"# build the model\nsvclassifier_Poly = SVC(kernel='poly', degree = 2, gamma = 'auto')\n# fit the model\nsvm=svclassifier_Poly.fit(X_train, y_train)","a7ca2112":"# predict the values\ny_pred  = svclassifier_Poly.predict(X_test)","999cff78":"plot_confusion_matrix(svm)","3ba29d9d":"test_report=get_test_report(svm)\nprint(test_report)","338c0759":"# compute kappa score on test set\nkappa_value = kappa_score(svm)\n\n# print the kappa value\nprint(kappa_value)","bdf63d53":"plot_roc(svm)","c9574cf0":"update_score_card(model_name='SVM with kernel(ploynomial) with degree 2')","5e03d286":"# degree: Degree of the polynomial\n# C: value of C parameter or regularisation parameter\n# gamma:\nparam_grid = { \n    'degree': [2,4,6,8,10], \n    'gamma' : ['auto','scale' ],\n    'C': [0.5, 1,2,2.5]\n}","b0a2b2e2":"CV_rfc = GridSearchCV(estimator= svclassifier_Poly, param_grid=param_grid, scoring='accuracy', cv= 5)\n# fit the model\nCV_rfc.fit(X_train, y_train)\n","b67d06f0":"# find the best parameters\nCV_rfc.best_params_","1bcf170e":"# build the model with best parameters obtained from above code\nsvclassifier_Poly_Grid = SVC(kernel='poly', \n                            degree = 6, \n                            gamma = 'scale',\n                           C = 2.5 )\n# fit the model\nsvm1=svclassifier_Poly_Grid.fit(X_train, y_train)","5c75734c":"# predict the values\ny_pred= svclassifier_Poly_Grid.predict(X_test)","a6e77c39":"plot_confusion_matrix(svm1)","663d008e":"test_report=get_test_report(svm1)\nprint(test_report)","be07826c":"# compute kappa score on test set\nkappa_value = kappa_score(svm1)\n\n# print the kappa value\nprint(kappa_value)","c07ddb22":"plot_roc(svm1)","f1baa078":"update_score_card(model_name='SVM with grid search CV ')","238b9fe7":"# instantiate the 'DecisionTreeClassifier' object using 'entropy' criterion\ndecision_tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 10)\n\n# fit the model using fit() on train data\ndecision_tree_model = decision_tree.fit(X_train, y_train)","e8f3faca":"labels=X_train.columns\n\n#plot the decisin tree\nfig=plt.figure(figsize=(20,20))\nz=tree.plot_tree(decision_tree_model,\n                feature_names=labels,\n                class_names=['0','1'],\n                filled=True)","61787def":"y_pred=decision_tree_model.predict(X_test)","e9fc836f":"plot_confusion_matrix(decision_tree_model)","30124a7d":"kappa_score(decision_tree_model)","da4d0caf":"test_report=get_test_report(decision_tree_model)\nprint(test_report)","37374566":"plot_roc(decision_tree)","82e4c996":"update_score_card(model_name='Decision Tree')","7337debb":"classifier=KNeighborsClassifier(n_neighbors=5)\nKNN=classifier.fit(X_train,y_train)","122ad542":"y_pred=classifier.predict(X_test)","1cc87172":"plot_confusion_matrix(KNN)","4299b206":"kappa_score(KNN)","2a6cd6dc":"test_report=get_test_report(KNN)\nprint(test_report)","8a79a0c7":"plot_roc(KNN)","dd57de02":"update_score_card(model_name='KNN')","b1350f19":"#loading the test data set\ndf_test=pd.read_csv(\"..\/input\/share-market-prediction\/Test_.csv\")","cf2b285e":"#displaying the first 5 records of the test dataset\ndf_test.head()","eb8d8bb6":"df.head()","7d2f574b":"df1=df.drop(['greater','percent','target'],axis=1)","f8f98dfe":"# filter the numerical features in the dataset using select_dtypes()\n# include=np.number: selects the numeric features\ndf_numeric_features = df1.select_dtypes(include=np.number)\n\n# display the numeric features\ndf_numeric_features.drop(['S.No','Close'],axis=1,inplace=True)","d17c585d":"df_numeric_features = sm.add_constant(df_numeric_features)\n# separate the independent and dependent variables\nX = df_numeric_features\n\n# extract the target variable from the data set\ny = df1['Close']\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","13bb939e":"# build a full model using OLS()\nlinreg_full_model = sm.OLS(y_train, X_train).fit()\n\n","c09cb918":"# predict the 'log_Property_Sale_Price' using predict()\nlinreg_full_model_predictions = linreg_full_model.predict(X_test)","a1e8e8d3":"# take the exponential of predictions using np.exp()\npredicted=linreg_full_model_predictions \n\n# extract the 'Property_Sale_Price' values from the test data\nactual= y_test","e11e00c1":"from statsmodels.tools.eval_measures import rmse\n# calculate rmse using rmse()\nlinreg_full_model_withlog_rmse = rmse(actual, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_withlog_rsquared = linreg_full_model.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_withlog_rsquared_adj = linreg_full_model.rsquared_adj ","909886d8":"# create the result table for all accuracy scores\n# accuracy measures considered for model comparision are RMSE, R-squared value and Adjusted R-squared value\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n# create a empty dataframe of the colums\n# columns: specifies the columns to be selected\nresult_tabulation = pd.DataFrame(columns = cols)\n\n# compile the required information\nlinreg_full_model_withlog_metrics = pd.Series({'Model': \"Linreg full model with log of target variable \",\n                     'RMSE':linreg_full_model_withlog_rmse,\n                     'R-Squared': linreg_full_model_withlog_rsquared,\n                     'Adj. R-Squared': linreg_full_model_withlog_rsquared_adj     \n                   })\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_withlog_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","f18b78ae":"df1['log']=np.log(df1['Close'])","c332735a":"df_numeric_features = sm.add_constant(df_numeric_features)\n# separate the independent and dependent variables\nX = df_numeric_features\n\n# extract the target variable from the data set\ny = df1['log']\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","4396cfce":"# build a full model using OLS()\nlinreg_full_model_withlog = sm.OLS(y_train, X_train).fit()","b6304b3b":"# predict the 'log_Property_Sale_Price' using predict()\nlinreg_full_model_withlog_predictions = linreg_full_model_withlog.predict(X_test)","83fde6eb":"# take the exponential of predictions using np.exp()\npredicted=np.exp(linreg_full_model_withlog_predictions) \n\n# extract the 'Property_Sale_Price' values from the test data\nactual= y_test","3e2da607":"# calculate rmse using rmse()\nlinreg_full_model_withlog_rmse = rmse(actual, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_withlog_rsquared = linreg_full_model_withlog.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_withlog_rsquared_adj = linreg_full_model_withlog.rsquared_adj ","e50b6e30":"# create the result table for all accuracy scores\n# accuracy measures considered for model comparision are RMSE, R-squared value and Adjusted R-squared value\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n# create a empty dataframe of the colums\n# columns: specifies the columns to be selected\nresult_tabulation = pd.DataFrame(columns = cols)\n\n# compile the required information\nlinreg_full_model_withlog_metrics = pd.Series({'Model': \"Linreg full model with log of target variable \",\n                     'RMSE':linreg_full_model_withlog_rmse,\n                     'R-Squared': linreg_full_model_withlog_rsquared,\n                     'Adj. R-Squared': linreg_full_model_withlog_rsquared_adj     \n                   })\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_withlog_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","20bfe4ea":"#preparing the dataset\ndf_test1=df_test.drop('S.No',axis=1)","48a93655":"#adding constant as we need to do so for statsmodels library\ndf_test1=sm.add_constant(df_test1)","91086a3c":"predict=linreg_full_model.predict(df_test1)","269a1341":"#converting the predicted values into list\nv=np.array(predict).tolist()","e7fc664f":"#inserting the predicted values in the dataframe under the column name treatment\ndf_test.insert(2,column=\"Close\",value=v)\ndf_test.head()","1cb4dc33":"#Finding the rows in which volume=0\n#making a dataframe out of it\ndf3=df_test[df_test['Volume']==0]","3f6c29bf":"df3.shape","8d6c86cb":"#On the days when volume is 0 the opening and closing price should be same as no one traded on that day\n#using \"np.where\" function to set the closing value price of such days when volume=0\ndf_test['Close'] = np.where(df_test['Volume'] == 0,df_test['Open'], df_test['Close'])","24dbb59b":"#creating the greater and percent column in test data as its required for model prediction\ndf_test['greater']=(df_test['Close']-df_test['Open'])\/df_test['Open']\ndf_test['percent']=df_test['greater']*100","1fbae692":"df_test_features=df_test.drop(['S.No'],axis=1)","499e3f0b":"scaler = MinMaxScaler()\nscaler.fit(df_test_features)","e667813c":"df_test_features = pd.DataFrame(scaler.fit_transform(df_test_features))","cf632c05":"#predicting using the best model\ndf_test_predict=svm1.predict(df_test_features)","af416f7e":"#converting the predicted values into list\nv1=np.array(df_test_predict).tolist()","5a51c838":"#inserting the predicted values in the dataframe under the column name treatment\ndf_test.insert(2,column=\"Flag\",value=v1)\n","71c86d64":"#Converting the data in treatment column replacing 1 with yes and 0 with no\ndf_test['Flag']=df_test['Flag'].replace(1,'Buy')\ndf_test['Flag']=df_test['Flag'].replace(0,\"Don't Buy\")\n","d2faad18":"upload=df_test.drop([ 'Open', 'Close', 'High', 'Low', 'Adj Close', 'Volume',\n       'greater', 'percent'],axis=1)\nupload.head()","734afbab":"upload.to_csv(\"predicted_flag9.csv\",index=False)","5f9674d5":"We can see from the plots that open,close,high and low are directly proportional to each other.","86eef43a":"We can see from the above graphs that open,high,low,close,adj close and volume all columns are right skewed.The greater and percent columns are normally distributed and the target column is categorical in nature.","f0aecf1d":"# Finding the missing values","0a946311":"We can see that the days which are considered good for intraday traders are very less as compared to the days considered bad  for intraday traders","36a1f7c8":"# Creating generalised functions","1baac98e":"# Model 3(Support vector machine using kernel(rbf))","000fc6a8":"# Model 4 (Support vector machine using kernel(sigmoid))","62e20d24":"# Model 9(K-nearest neighbour)","c024c201":"\n# Importing libraries","422e2b46":"# Understanding the data set","879fdf9a":"# DATA PREPARATION FOR MODEL BUILDING","d4d9e2ec":"# Viewing the columns after adjusting the outliers ","11b35c52":"# Predicting the closing price of the test data","a2c8c9be":"# Model 2(Support vector machine)","6c18c6ec":"On loading the test data we found that the close price column is missing.Hence we need to predict that column first in order to predict whether an intraday trader should invest or not on a particular day.","a0931712":"# Model 5(Support vector machine using kernel(polynomial))","1e931f6b":"There are total 4378 rows and 7 columns in the training dataset","19814b6d":" # Creating model to find closing price","4f491cd8":"From the above output we can see that their are 6 values missing in each column,except \"S.No\".We need to handle these missing values.","3d296588":"Since open,close,high and low prices are directly proportional,therefore we see that the boxplot is same for all the variables against the target variable.","a5893ce9":"From the plot we can see that the day which is not good for trading the greater value(closing price versus opening price calculation) has got many outliers below the minimum value, whereas the day which is good for trading the outliers lies above maximum value in the boxplot.","fca45bd5":"# Model 1(Logistic regression)","76bd163a":"# Finding Outliers","c8d6605d":"Total 9 models have been built to predict whether it is a good day for an intraday trader or not.Out of all the models decision tree model is the best with 100% accuracy.Even the F1 score which is the harmonic mean between the precision and recall is 1.\n\nWe can even see that the kappa score of decision tree model is 1 which means that the predicted data and the actual data totally agree with each other.\n\nThe AUC score i.e. the area under the curve of decision tree model is 1.We even use this metrics to choose between different models.As the metric value is 1 and it is the best among other models we choose this model to do prediction on the test data.\n\nIf we see the confusion matrix then the type 1 error and type 2 error both are 0,it confirms that the model has got no errors while predicting the day on which the intraday traders should trade or not.\n\nBut while doing prediction on the test dataset it was observed that support vector machine using gridCv which is the model number 6 in our result table gave better output. It gave an accuracy of 81.6% whereas when prediction was done on the test data using decision tree model the accuracy was 81.2%.\n\nWe used the support vector machine with gridsearch Cv model to do prediction  because it is the second best model having accuracy of 99.65% and F1 score of 98.08%.\n\nEven the AUC score is 99.23% which is better than rest of the models. If we see the confusion matrix then we can observe that in this model the type 1 and type 2 errors are 2 and 1 respectively, which is quiet good.Even when we see the kappa score it is 0.9790 which means that the predicted data agrees 97.9% with the actual data.\n\nHence, this model was used to do prediction on the test data.\n\nReason why support vector machine worked better than decision tree model on test data even after decision tree model having better metrics when model was trained on training data, can be overfitting.","69d2ef07":"We can see that the day which is good for intraday traders for trading the opening price is between the range 11000 to 28000 approximately.","70bf507b":"# Working with the test data set","58c536a2":"# Model 8(Decision tree model)","fc3d1ff1":"From the plot we can see that the day which is not good for trading versus the day that is good for intraday trading,the mean closing price lies above 20000 and below 20000 respectively","670ea796":"# Conclusion:","8097f6a8":"The statistical summary contains information about the mean,number of rows(count),standard deviation,minimum value,quartiles and maximum value in a column.The column named close contains 4372 values with the maximum value of 91000,mean value of 26466 and the minimum value of 8040.","962972da":"# Preparing the data for model building","c428794d":"We can see from the graph above that the data missing from each column is from the same rows.Hence we delete those rows,as the missing data count is not high.","8e98011f":"# Model 6(Support vector machine using kernel with degree 2)","3f89a8f1":"Hence we can see from the plots that the outliers have been handled to great extent.","5a10e44b":"# Model 7(Support vector machine using grid search)","b5386d0a":"# Exploratory Data Analysis"}}